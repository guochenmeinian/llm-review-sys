# Have any third parties imposed IP-based or other restrictions on the data associated with the instances?

[MISSING_PAGE_EMPTY:1]

aspects of video generation. **(2) Lack of reliable automatic evaluation metrics.** In text-to-image generation, it has been found that the existing automatic evaluation metrics are inconsistent with human judgment [6; 27; 28]. However, in T2V generation, this problem is still under-explored.

To address these issues, we introduce a benchmark called **FETV** for the **F**ine-grained **E**valuation of **T**ext-to-**V**ideo generation, which offers a comprehensive categorization system for T2V evaluation. FETV includes a diverse set of text prompts, categorized based on three orthogonal aspects: major content, attribute control, and prompt complexity (see Figure 1). We also propose several temporal categories specifically designed for video generation, encompassing both spatial and temporal content. As shown in Table 1, compared with existing benchmarks, FETV highlights the most comprehensive categorization for fine-grained evaluation of T2V generation. To efficiently categorize the text prompts, we employ a two-step annotation process involving an automatic assignment of category labels followed by manual review. On this basis, we collect 619 categorized text prompts from existing open-domain text-video datasets [48; 1] and manually-written unusual prompts.

Using FETV, we perform comprehensive manual evaluations for four representative T2V models, enabling fine-grained analysis of their strengths and weaknesses across different prompt categories. The results highlight the weaknesses of existing T2V models in terms of (1) poor video quality for action and kinetic motion related videos and (2) inability to control quantity, motion direction and event order (see Section 3.1 for detailed definition of these categories). Furthermore, FETV can serve as a testbed for assessing the reliability of automatic evaluation metrics through correlation analysis with manual evaluations. Our experiments reveal that the widely used automatic metrics of video quality (FID  and FVD ) and video-text alignment (CLIPScore ) correlate poorly with human evaluation. In response to this problem, we explore several solutions to improve CLIPScore and FVD. Based on UMT , an advanced vision-language model (VLM), we develop FVD-UMT for video quality and UMTScore for video-text alignment, which exhibit much higher correlation with humans than existing metrics.

The contributions of this work are as follows: **(1)** We propose the FETV for fine-grained evaluation of open-domain T2V generation. Compared with existing benchmarks, FETV provides a more comprehensive view of T2V models' capabilities from multiple aspects. **(2)** Based on FETV, we conduct a comprehensive manual evaluation of four representative open T2V models and reveal their pros and cons from different aspects. **(3)** We extend FETV as a testbed to evaluate the reliability of automatic T2V metrics and develop two automatic metrics that exhibit significant higher correlation with humans than existing metrics.

## 2 Related Work

Benchmarks for Text-to-Video Generation.Most existing studies on T2V generation adopt datasets from other related tasks to evaluate their models. UCF-101  and Kinetics-400/600/700 [16; 4; 36] are initially proposed for the task of human action recognition, which cannot cover the diversity of open-domain video generation. MSR-VTT , which is commonly used for video captioning and video-text retrieval, consists of text-video pairs covering 20 categories with diverse video contents. However, its categorization only considers the topic of videos (e.g., "music", "sports" and "news") while ignoring other important aspects in text-to-video generation, e.g., the temporal

    &  & Open &  & Attribute Control &  \\  & &  & spatial & temporal & spatial & temporal & Complexity \\   & DrawBench & ✓ & ✗ & ✗ & ✓ & ✗ & ✗ \\  & PartiPrompts & ✓ & ✓ & ✗ & ✓ & ✗ & ✓ \\   & UCF-101 & ✗ & ✗ & ✓ & ✗ & ✗ & ✗ \\  & Kinetics & ✗ & ✓ & ✗ & ✗ & ✗ \\   & MSR-VTT & ✓ & ✓ & ✗ & ✗ & ✗ \\   & Make-a-Video-Eval & ✓ & ✓ & ✗ & ✗ & ✗ \\   & FETV (Ours) & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of text-to-image/video generation benchmarks. ✗ and ✓ denote whether the benchmark contains the corresponding aspect (column) of categorization or whether it is open-domain. FETV is open-domain, multi-aspect and temporal-aware, compared with other benchmarks.

contents and the attributes a text prompt aims to control. In text-to-image generation, DrawBench  and PartiPrompts  introduce another aspect of categorization concerning the type of challenge a prompt involves. However, their prompts lack descriptions of temporal content, which are not suitable for T2V generation. Make-a-Video-Eval  contains 300 text prompts specially composed for T2V generation. However, their categorization (i.e., animals, fantasy, people, nature and scenes, food and beverage) only considers the spatial content.

Automatic Metrics for Text-to-Video Generation.There are two types of automatic open T2V metrics, which evaluate the video quality and video-text alignment, respectively. For video quality, the widely used metrics are the Inception Score (IS) , Frechet Inception Distance (FID) , and Frechet Video Distance (FVD) . Based on a classification model, IS assesses whether every single generated video contains identifiable objects and whether all the videos are diverse in the generated objects. FID and FVD compute the distribution similarity between real and generated videos in the latent space of a deep neural network. In terms of video-text alignment, existing works on open T2V mainly adopt the CLIPScore . CLIPScore is initially proposed to measure the similarity between machine-generated captions and the given image, using the pre-trained CLIP model . In the task of text-to-image generation, it has been found that the automatic image quality and image-text alignment metrics are inconsistent with human judgment [6; 27; 28]. When it comes to open T2V, this problem remains under-explored.

Models for Text-to-Video Generation.Existing text-to-video generation models can be divided into the Transformer-based models [45; 46; 15; 43] and the Diffusion-based models [14; 13; 34; 51; 17; 25; 2; 9; 44]. The former first converts the videos into discrete visual tokens and then trains the Transformer model to generate these visual tokens. The latter generates videos using the Diffusion Models [37; 12; 49], either on the original pixel space or a latent space. In this paper, We evaluate three models that are open-sourced up to the time of writing this paper, namely CogVideo , Text2Video-zero  and ModelScopeT2V , as well as a more recent model ZeroScope . Note that text-driven video editing [26; 47], where the input is a text prompt and a source video, may also be referred to as text-to-video generation. In this paper, we focus on the typical kind of text-to-video generation, where the input condition is only a piece of text prompt.

## 3 FETV Benchmark

FETV is a dataset of text prompts divided into different categories from multiple aspects for fine-grained evaluation of T2V generation. In this section, we will introduce the multi-aspect categorization, how to collect the prompts and category labels and the application scope of FETV.

Figure 1: Illustration of our multi-aspect categorization (a-b), based on which we can realize fine-grained evaluation (c). Details of each category are shown in Figure 2 and Appendix B.1.

### Multi-Aspect Categorization

As shown in Figure 1, each prompt is categorized based on three aspects, namely the major content it describes, the attribute it aims to control in the generation of videos, and its complexity. The "major content" aspect is further divided into spatial and temporal categories, the former focus on the type of objects described in the prompt, while the latter focus on the type of temporal content. Similarly, the "attribute control" aspect is comprised of both spatial attributes and temporal attributes. The "prompt complexity" aspect has three levels of complexity, namely "simple", "medium", and "complex", depending on the number of non-stop words contained in a prompt. With the multi-aspect categorization of every single prompt, the entire FETV benchmark can be divided into different subsets (Figure 1 (c)), therefore enabling fine-grained evaluation. Figure 2 presents the descriptions and corresponding examples of the temporal categories. The complete description of all categories can be found in Appendix B.1.

### Data Collection

Prompt Sources.The prompts of FETV come from two kinds of sources. **(1)** we reuse the texts from existing datasets of open-domain text-video pairs, which guarantees the diversity of prompts. Specifically, we use the MSR-VTT  test set and WebVid  validation set, which contain 59,800 and 4,999 text-video pairs, respectively. **(2)** we manually write prompts describing scenarios that are unusual in the real world, which cannot be found in existing text-video datasets. These "unusual prompts" are inspired by the "Imagination" category in PartiPrompts  and "Conflicting" category in DrawBench , which aims to test the generalization ability of the generation models. To write the unusual prompts, we first define several types of unusual cases, namely "object-action", "object-object", "object-color", "object-quantity", "object-direction", "object-speed" and "event order". For example, the prompt "A cat is driving a car." belongs to the case of unusual "object-action". Then, for each case, we enumerate several specific objects and attributes and write the prompts.

Prompt Categorization and SelectionFor the prompts from both sources, we assign category labels to them based on our multi-aspect categorization structure. This is achieved in two steps: **In the first step**, we define a list of keyphrases, WordNet synsets and some hand-crafted matching patterns for each category and automatically categorize the prompts. For example, to determine

Figure 2: Descriptions and examples of the temporal categories.

whether a prompt belongs to the "animals" category, we iterate over the words in the prompt and see whether any of them belong to the animal-related WordNet synset. Details of the automatic matching rules for each category are presented in Appendix G. **In the second step**, we manually select prompts for each category and make some revisions to the category labels to ensure that they are correct. Specifically, we enumerate combinations of \((c_{i},c_{j})\), where \(c_{i}\) belongs to the six "attribute control" categories and \(c_{j}\) belongs to the four "temporal major content" categories. For each combination, we select roughly 20 prompts that belong to both \(c_{i}\) and \(c_{j}\), according to the first-step categorization, and then revise the incorrect category labels. For the text prompts from existing datasets, we also revise the ones that do not align well with the reference video.

### Dataset Statistics and Data Format

We collect a total of 619 prompts with corresponding categorizations, among which 541 prompts are from existing datasets and 78 unusual prompts are written by us. Such data scale is comparable with existing text-to-image/video benchmarks, e.g., DrawBench (200 prompts), PartiPrompts (1,600 prompts) and Make-a-Video-Eval (300 prompts), which are mainly proposed for manual evaluation. The data distributions over categories under different aspects are illustrated in Figure 4 and Figure 4, which show that FETV contains enough data to cover the diverse range of categories. We also compare FETV with MSR-VTT and WebVid in terms of the data distribution over categories, which can be found in Appendix B.2.

Each data sample in FETV consists of three elements: the prompt \(t\), the reference video \(v^{}\) (not available for the unusual prompts) and the categorization labels \(\{^{a}\}_{a}\). \(\) is the collection of the three aspects, \(^{a}\) is the categorization labels of \(t\) under aspect \(a\). Taking the prompt in Figure 1 as an example, when the aspect \(a\) is "attribute control", \(^{a}=\{\}\). Note that except for the complexity aspect, we allow each prompt to be classified into multiple categories under the same aspect. This setting is more realistic compared with existing benchmarks that only classify each prompt into a single category. More examples of FETV data can be found in Appendix B.3.

### Application Scope

Theoretically, FETV can be used for both manual and automatic evaluation. However, considering that existing automatic T2V metrics correlate poorly with humans (see Section 5), we mainly rely on manual evaluation to obtain more accurate results. In addition to evaluating T2V models, FETV can also be extended to diagnose automatic metrics, using the manual evaluation as a reference.

## 4 Manual Evaluation of Open T2V Models

### Models

We select four representative T2V models, which are open-sourced, for evaluation. We briefly describe them as follows and defer the details to Appendix C.

**CogVideo** is based on the Transformer architecture . It first transforms the video frames into discrete visual tokens using VQVAE  and then utilizes two Transformer models to hierarchically generate the visual tokens from a given text prompt.

**Text2Video-zero** is based on the Diffusion Probabilistic Model . It generates videos by enhancing the latent codes of a text-to-image Stable Diffusion model 1 with motion dynamics, without training the model on any video data.

**ModelScopeT2V** is another diffusion-based model. It extends the T2I Stable Diffusion model with temporal convolution and attention blocks and conducts training using both image-text and video-text datasets.

**ZeroScope** is a watermark-free T2V generation model trained from the weights of ModelScopeT2V. It is optimized for 16:9 compositions.

### Evaluation Setups

Evaluation Criteria.We evaluate the generated videos mainly from four perspectives: **Static quality** focuses on the visual quality of single video frames. **Temporal quality** focuses on the temporal coherence of video frames. **Overall alignment** measures the overall alignment between a video and the given text prompt. **Fine-grained alignment2** measures the video-text alignment regarding specific attributes. For the first three perspectives, we score the videos based on a 1-5 Likert-scale judgement. For fine-grained alignment, we adopt 1-3 Likert-scale judgement. To facilitate the consistency between human evaluators, we carefully design the wording of questions and descriptions of each rating level. The agreement between our human evaluators is high, with Krippendorff's \(\) of 0.711, 0.770, 0.638 and 0.653 in the above four perspectives. In Appendix E, we summarize the details of our instruction document for manual evaluation and the annotation interface.

Implementation Details.We generate three videos for each prompt using the three models respectively. For each generated video and each of the above four perspectives, we collect three rating scores from graduate students in our laboratory and report the average rating. For the generated videos, we obtain 619 samples \(\) 3 models \(\) 3 perspectives \(\) 3 humans =16,713 ratings for the first three perspectives and 5,184 ratings of fine-grained alignment perspective (note that not every prompt involves an attribute for evaluating fine-grained alignment). We also evaluate the 541 reference videos, which produce 6,219 ratings for the four perspectives. In total, we collect 28,116 ratings in our manual evaluation. To obtain the performance under a specific category, we report the average rating of all videos whose prompts belong to this category.

### Results and Analysis

In this section, we present and analyse the results from a quantitative perspective (some qualitative analyses are shown in Appendix F.5). Specifically, we are interested in the following questions: (1) how video quality varies when generating different major contents, (2) how video-text alignment varies when controlling different attributes or facing different prompt complexity. We also investigate (3) how well the models generalize to unusual prompts in Appendix F.4.

The results of video quality are presented in Figure 5. We can observe that: (1) In terms of the spatial categories, **the generated videos containing "people" and "animals" have poor quality, while the videos containing "plants" and "scenery & natural objects" are of good quality**. This is because videos of the former two categories involve many high-frequency details (e.g., human faces and fingers) which are more difficult to learn than the low-frequency information in the latter two categories (e.g., water and clouds). (2) When it comes to the temporal categories, **the videos about "actions" and "kinetic motions" are of worse quality**, since the corresponding temporal content is of higher frequency. Although there is a correlation between some spatial and temporal categories(e.g., "actions" is related to "people"), our temporal categorization provides a more direct view of the model performance from the temporal content perspective. (3) **None of the four models achieve comparable video quality with real videos**, especially in the aforementioned poor-quality categories. (4) Text2Video-zero performs the best in static quality while lagging far behind in temporal quality. This is because Text2Video-zero inherits the image generation power of Stable Diffusion, but the absence of video training prevents it from generating temporally coherent content. (5) ZeroScope, which is trained from ModelScopeT2V, is comparable with ModelScopeT2V in temporal quality

Figure 5: Manual evaluation of static and temporal video quality.

Figure 6: Manual evaluation of video-text alignment.

while performing worse in static quality. We conjecture that this is because the ZeroScope is primarily optimized for the 16:9 composition and watermark-free videos, instead of for better visual quality and video-text alignment.

The results of video-text alignment are shown in Figure 6. We can see that: (1) **Existing T2V models can already control "color" and "camera view" in most cases.** (2) **All four model struggle to accurately control "quantity", "motion direction" and "event order".** However, ModelScopeT2V exhibits some preliminary ability to control "speed" and "motion direction", which is not observed in CogVideo and Text2Video-zero. (3) ZeroScope slightly underperforms the original ModelScopeT2V in video-text alignment. We attribute this to the same underlying cause as explained for the video quality performance. (4) The relative performance among different attributes and T2V models is similar between fine-grained and overall alignment, with a few exceptions. This is because the overall alignment may inevitably be affected by other attributes. Therefore, **the fine-grained alignment is more accurate when evaluating controllability over specific attributes, but the overall alignment is an acceptable approximation**. (5) While video-text alignment is slightly lower for the "complex" prompts, **we cannot observe a clear correlation between prompt complexity and video-text alignment.**

In addition to the fine-grained evaluating results, we also summarize the leaderboard on the entire FETV benchmark in Figure 7.

## 5 Diagnosis of Automatic Evaluation Metrics

In this section, we perform automatic evaluations on FETV and diagnose their correlation with manual evaluations.

### Metrics

For video-text alignment, we investigate five metrics. **(1) CLIPScore** is widely used to evaluate T2V/I generation models. CLIPScore is based on CLIP , a vision-language model (VLM) pre-trained on large-scale text-image pairs. **(2) CLIPScore-ft** is based on a CLIP model fine-tuned on the video-text retrieval task using MSR-VTT data. **(3) BLIPScore** and **(4) UMTScore** replace CLIP with more advanced VLMs, i.e., BLIP  and UMT . BLIP adopts bootstrapping language-image pre-training. UMT is pre-trained large-scale video-text data and we adopt the fine-tuned version on MSR-VTT. Then, we compute BLIPScore in the same way as CLIPScore. For UMT, we utilize the cross-modal decoder's video-text matching result and refer to this metric as UMTScore. We also adopt a Video Large Language Models (LLMs), i.e., Otter , to evaluate video-text alignment by formulating it as a Video QA task. Specifically, we first extract the key elements from the text prompt and generate corresponding yes-no questions via the Vicuna model . Then, we feed the generated (or ground-truth) videos and questions into the Video LLM. The average number of questions that receive a positive answer is defined as the alignment score for a specific video-text pair. We name the Video LLM-based metric as **(5) Otter-VQA**.

For video quality metrics, we investigate **FID** and **FVD**. The original FVD adopts the I3D model  as a video encoder. We introduce FVD-UMT, an enhanced version of FVD that leverages the UMT vision encoder for video feature extraction. More details of these metrics and our implementations can be found in Appendix D.

Figure 7: Leaderboard on FETV benchmark based on manual evaluation.

### Evaluation Setups

Video-Text Alignment Metrics.We use the same generated and reference videos for manual evaluation (one video for a text prompt using one model) and compute the video-text alignment for each video-text pair. Then, we measure the correlation between automatic and manual evaluations, using Kendall's \(_{c}\) coefficient and Spearman's \(\) coefficient. To obtain the correlation under a specific category, we collect the manual and automatic evaluation results of video-text pairs that belong to this category and compute the coefficients.

Video Quality Metrics.Different from the video-text alignment metrics, FID and FVD evaluate the quality of a group of generated videos instead of a single video. To evaluate the fine-grained video quality in specific categories, for each open T2V model, we generate 300 videos for each category under the "major content" aspect, based on FETV prompts. In this case, each prompt can be used multiple times but the generated videos are different due to the randomness in the diffusion process and visual token sampling. We also sample 300 reference videos for each category. This is achieved by using the automatic categorization results of MSR-VTT and WebVid prompts (introduced in Section 3.2) and collecting the corresponding videos. To compute the overall FID and FVD for a T2V model, we sample 1,024 generated and reference videos (the impact of video sample number is analyzed in Appendix F.3). The results are averaged across 4 runs with different random seeds.

### Results and Analysis

Video-Text Alignment Metrics.Table 2 shows the correlation between automatic and manual evaluations. We can see that: (1) Although the widely-used CLIPScore is positively correlated with manual evaluations, the overall correlation degree is very weak compared with inter-human correlations. (2) Fine-tuning CLIP on video-text retrieval and replacing CLIP with BLIP are beneficial, which promote the correlation across almost all categories. However, CLIPScore-ft and BLIPScore are still inconsistent with the human ranking of videos generated from different models, as shown in Figure 8. Therefore, we cannot rely on CLIPScore and BLIPScore to accurately evaluate open T2V models. (3) UMTScore exhibits the strongest correlation with humans when measured by Kendall's \(_{c}\) and Spearman's \(\). More importantly, it consistently aligns with human judgments in the T2V model rankings. The performance of UMTScore suggests that training VLMs with video data is important and the evaluation of open T2V models can benefit from the development of stronger VLMs. (4) The correlation varies across different "attribute control" categories. It is interesting to see that for UMTScore, the correlation in the challenging categories (for open T2V models), i.e., "Quantity" and "Motions Direction", is higher than in the relatively simple "Color" and "Camera

    & **Color** & **Quantity** &  **Camera** \\ **View** \\  &  **Speed** \\  &  **Motion** \\ **Direction** \\  & 
 **Event** \\ **Order** \\  & **All** \\  CLIPScore & 0.157/0.218 & 0.147/0.202 & 0.254/0.345 & 0.178/0.246 & 0.141/0.194 & 0.177/0.248 & 0.177/0.243 \\ CLIPScore-ft & 0.206/0.287 & 0.250/0.340 & 0.293/0.402 & 0.203/0.280 & 0.254/0.348 & 0.157/0.221 & 0.224/0.309 \\ BLIPScore & 0.222/0.307 & 0.207/0.282 & 0.285/0.394 & 0.195/0.266 & 0.223/0.305 & 0.180/0.250 & 0.235/0.322 \\ Otter-VQA & 0.049/0.070 & 0.134/0.188 & 0.027/0.038 & 0.051/0.073 & 0.119/0.166 & 0.146/0.206 & 0.081/0.114 \\ UMTScore & **0.304/0.420** & **0.394/0.528** & **0.300/0.415** & **0.296/0.407** & **0.356/0.476** & **0.295/0.406** & **0.309/0.425** \\  Human & 0.547/0.702 & 0.647/0.784 & 0.447/0.595 & 0.539/0.683 & 0.619/0.747 & 0.517/0.680 & 0.576/0.719 \\   

Table 2: Correlation between automatic and manual evaluations of video-text alignment, measured by Kendall’s \(_{c}\) (left) and Spearman’s \(\) (right) coefficients. We also report the average inter-human correlation in the final row as a reference.

Figure 8: Automatic and human ranking of the T2V models in terms of video-text alignment. Results on specific categories are reported in Appendix F.1.

View". We conjecture that this is because, in the simpler categories, the performance of open T2V models is more similar and it is more difficult to distinguish the difference, and vice versa.

Figure 9 presents a case study of video-text alignment scores. We can see that the results measured by CLIPScore and Otter-VQA strongly differ from human evaluation. CLIPScore-ft and BLIPScore correlate better with humans, while still mistakenly assigning lower score to the ground-truth video. In comparison, UMTScore's ranking of video-text alignment is most consistent with humans.

Video Quality Metrics.For FID and FVD, we cannot calculate the correlation coefficients because they are not sample-wise metrics. To investigate their correlation with humans, we focus on the system-wise ranking of open T2V models. As we can see in Figure 10, the FID and the original FVD with I3D video encoder are loosely coupled with the human's perception of the video quality. This result reveals the critical problem of using FID and FVD as a measurement of video quality, which is common in previous works. In comparison, the FVD-UMT metric, which is enhanced with the UMT model as video feature extractor, generates consistent rankings with humans.

## 6 Conclusions

In this paper, we propose the FETV benchmark for fine-grained evaluation of open-domain text-to-video generation. Compared with existing benchmarks, FETV is both multi-aspect and temporal-aware, which can provide more specific information on the performance of T2V models. Based on FETV, we conduct a comprehensive manual evaluation of representative T2V models and reveal their pros and cons in different categories of text prompts from different aspects. Furthermore, we extend FETV to diagnose the reliability of automatic T2V metrics, revealing the poor correlation of existing metrics with humans. To address this problem, we develop two automatic metrics using an advanced VLM called UMT, which can produce a consistent ranking of T2V models with humans.

## 7 Limitations and Future Work

The limitations of this work can be viewed from two perspectives. First, while the proposed UMT-based metrics are more reliable than existing automatic metrics, they still have room for improvement to better align with humans. Second, due to the lack of open-sourced open T2V models, the number of models evaluated in this work is limited. We invite follow-up studies to evaluate their models on our benchmark and encourage more projects to open-source their models.

Figure 10: Automatic and human ranking of T2V models in terms of video quality (averaged static and temporal quality). Results on specific categories are reported in Appendix F.2.

Figure 9: Video examples and alignment scores measured by different metrics. The left three videos are generated and the rightmost video is the ground-truth. More examples are shown in Appendix F.6.