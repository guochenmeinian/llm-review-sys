# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

Zou et al., 2019; Huang et al., 2018; Dong et al., 2021), where the sampling for the whole layer is done collectively. On the other hand subgraph sampling methods (Chiang et al., 2019; Zeng et al., 2020; Hu et al., 2020; Zeng et al., 2021; Fey et al., 2021; Shi et al., 2023) do not use the recursive layer by layer sampling scheme used in the node- and layer-based sampling methods and instead tend to use the same subgraph for all of the layers. Some of these sampling methods take the magnitudes of embeddings into account (Liu et al., 2020; Zhang et al., 2021; Huang et al., 2018), while others, such as Chen et al. (2018); Cong et al. (2021); Fey et al. (2021); Shi et al. (2023), cache the historical embeddings to reduce the variance of the computed approximate embeddings. There are methods of sampling from a vertex cache Dong et al. (2021) filled with popular vertices. Most of these approaches are orthogonal to each other and they can be incorporated into other sampling algorithms.

Node-based sampling methods suffer the most from the NEP but they guarantee a good approximation for each embedding by ensuring each vertex gets \(k\) neighbors which is the only hyperparameter of the sampling algorithm. Layer-based sampling methods do not suffer as much from the NEP because the number of vertices sampled is a hyperparameter but they can not guarantee that each vertex approximation is good enough and also their hyperparameters are hard to reason with, the number of nodes to sample at each layer depends highly on the graph structure (as the numbers in Table 2 show). Moreover, Table 4 in Chen et al. (2023) shows that they can be extremely sensitive to the chosen hyperparameters and final task performance can differ by up to 15% between choosing a fixed number or doubling the number of sampled vertices at each layer. It is also unclear why one should settle with doubling; tripling, or quadrupling could potentially improve the performance of the final trained model even further. Subgraph sampling methods usually sample sparser subgraphs compared to their node- and layer-based counterparts. Hence, in this paper, we focus on the node- and layer-based sampling methods and combine their advantages. The major contributions of this work can be listed as follows:

* We propose the use of Poisson Sampling for GNNs, taking advantage of its lower variance and computational efficiency against sampling without replacement. Applying it to the existing layer sampling method LADIES, we get the superior PLADIES method outperforming the former by up to 2% in terms of F1-score. Moreover, PLADIES is linear time, improving upon the recently proposed quadratic time debiasing technique in Chen et al. (2023).
* We propose a new sampling algorithm called LABOR, combining the advantages of neighbor and layer sampling approaches using Poisson Sampling. LABOR correlates the sampling procedures of the given set of seed nodes so that the sampled vertices from different seeds have a lot of overlap, resulting in a \(7\) and \(4\) reduction in the number of vertices and edges sampled compared to NS, respectively. Furthermore, LABOR can sample up to \(13\) fewer edges compared to LADIES.
* We experimentally verify our findings and show that our proposed sampling algorithm LABOR outperforms both neighbor and layer sampling approaches. LABOR can enjoy a batch size of up to \(112\) larger than NS while sampling the same number of vertices.

## 2 Background

Graph Neural Networks:Given a directed graph \(=(V,E)\), where \(V\) and \(E V V\) are vertex and edge sets respectively, \((t s) E\) denotes an edge from a source vertex \(t V\) to a destination vertex \(s V\), and \(A_{ts}\) denotes the corresponding edge weight if provided. If we have a batch of seed vertices \(S V\), let us define \(l\)-hop neighborhood \(N^{l}(S)\) for the incoming edges as follows:

\[N(s)=\{t|(t s) E\},N^{1}(S)=N(S)=_{s S}N(s),N^{l}(S)=N(N^{(l-1)} (S)) \]

Let us also define the degree \(d_{s}\) of vertex \(s\) as \(d_{s}=|N(s)|\). To simplify, let's assume uniform edge weights, \(A_{ts}=1,(t s) E\). Then, our goal is to estimate the following for each vertex \(s S\), where \(H_{t}^{(l-1)}\) is defined as the embedding of the vertex \(t\) at layer \(l-1\), and \(W^{(l-1)}\) is the trainable weight matrix at layer \(l-1\), and \(\) is the nonlinear activation function (Hamilton et al., 2017):

\[Z_{s}^{l}=}_{t s}H_{t}^{(l-1)}W^{(l-1)},\ H_{s}^{l}= (Z_{s}^{l}) \]

Exact Stochastic Gradient Descent:If we have a node prediction task and \(V_{t} V\) is the set of training vertices, \(y_{s},s V_{t}\) are the labels of the prediction task, and \(\) is the loss function for the prediction task, then our goal is to minimize the following loss function: \(|}_{s V_{t}}(y_{s},Z_{s}^{})\). Replacing \(V_{t}\) in the loss function with \(S V_{t}\) for each iteration of gradient descent, we get stochastic gradient descent for GNNs. However with \(l\) layers, the computation dependency is on \(N^{l}(S)\), which reaches large portion of the real world graphs, i.e. \(|N^{l}(S)||V|\), making each iteration costly both in terms of computation and memory.

Neighbor Sampling:Neighbor sampling approach was proposed by Hamilton et al. (2017) to approximate \(Z_{s}^{(l)}\) for each \(s S\) with a subset of \(N^{l}(S)\). Given a fanout hyperparameter \(k\), this subset is computed recursively by randomly picking \(k\) neighbors for each \(s S\) from \(N(s)\) to form the next layer \(S^{1}\), that is a subset of \(N^{1}(S)\). If \(d_{s} k\), then the exact neighborhood \(N(s)\) is used. For the next layer, \(S^{1}\) is treated as the new set of seed vertices and this procedure is applied recursively.

Revisiting LADIES, Dependent Layer-based Sampling:From now on, we will drop the layer notation and focus on a single layer, and also ignore the nonlinearities. Let us define \(M_{t}=H_{t}W\) as a shorthand notation. Then our goal is to approximate:

\[H_{s}=}_{t s}M_{t} \]

If we assign probabilities \(_{t}>0, t N(S)\) and normalize it so that \(_{t N(S)}_{t}=1\), then use sampling with replacement to sample \(T N(S)\) with \(|T|=n\), where \(n\) is the number of vertices to sample given as input to the LADIES algorithm and \(T\) is a multi-set possibly with multiple copies of the same vertices, and let \(_{s}=|T N(s)|\) which is the number of sampled vertices for a given vertex \(s\), we get the following two possible estimators for each vertex \(s S\):

\[H_{s}^{}=}_{t T N(s)}}{_{t}} )} H_{s}^{}=}{_{t}}}{ _{t T N(s)}}} \]

Note that \(H_{s}^{}\) in (4a) is the Thompson-Horvitz estimator and the \(H_{s}^{}\) in (4b) is the Hajek estimator. For a comparison between the two and how to get an even better estimator by combining them, see Khan and Ugander (2021). The formulation in the LADIES paper uses \(H_{s}^{}\), but it proposes to row-normalize the sampled adjacency matrix, meaning they use \(H_{s}^{}\) in their implementation. However, analyzing the variance of the Thompson-Horvitz estimator is simpler and its variance serves as an upper bound for the variance of the Hajek estimator when \(|M_{t}|\) and \(_{t}\) are uncorrelated Khan and Ugander (2021); Dorfman (1997), which we assume to be true in our case. Note that the variance analysis is simplified to be element-wise for all vectors involved.

\[(H_{s}^{})(H_{s}^{} )=_{s}d_{s}^{2}}_{t s}_{t}_{t^{} s} (M_{t^{}})}{_{t^{}}} \]

Since we do not have access to the computed embeddings and to simplify the analysis, we assume that \((M_{t})=1\) from now on. One can see that \((H_{s}^{})\) is minimized when \(_{t}=p, t s\) under the constraint \(_{t s}_{t} pd_{s}\) for some constant \(p\), hence any deviation from uniformity increases the variance. The variance is also smaller the larger \(_{s}\) is. However, in theory and in practice, there is no guarantee that each vertex \(s S\) will get any neighbors in \(T\), not to mention equal numbers of neighbors. Some vertices will have pretty good estimators with thousands of samples and very low variances, while others might not even get a single neighbor sampled. For this reason, we designed LABOR so that every vertex in \(S\) will sample enough neighbors in expectation.

While LADIES is optimal from an approximate matrix multiplication perspective Chen et al. (2023), it is far from optimal in the case of nonlinearities and multiple layers. Even if there is a single layer, then the used loss functions are nonlinear. Moreover, the existence of nonlinearities in-between layers and the fact that there are multiple layers exacerbates this issue and necessitates that each vertex gets a good enough estimator with low enough variance. Also, LADIES gives a formulation using sampling with replacement instead of without replacement and that is sub-optimal from the variance perspective while its implementation uses sampling without replacement without taking care of the bias created thereby. In the next section, we will show how all of these problems are addressed by our newly proposed Poisson sampling framework and LABOR sampling.

Proposed Layer Sampling Methods

Node-based sampling methods suffer from sampling too shallow subgraphs leading to NEP in just a few hops (e.g., see Table 2). Layer sampling methods Zou et al. (2019) attempt to fix this by sampling a fixed number of vertices in each layer, however they can not ensure that the estimators for the vertices are of high quality, and it is hard to reason how to choose the number of vertices to sample in each layer. LADIES Zou et al. (2019) proposes using the same number for each layer while papers evaluating it found it is better to sample an increasing number of vertices in each layer Liu et al. (2020); Chen et al. (2023). There is no systematic way to choose how many vertices to sample in each layer for the LADIES method, and since each graph has different density and connectivity structure, this choice highly depends on the graph in question. Therefore, due to its simplicity and high quality results, Neighbor Sampling currently seems to be the most popular sampling approach and there exists high quality implementations on both CPUs and GPUs in the popular GNN frameworks Wang et al. (2019); Fey and Lenssen (2019).

We propose a new approach that combines the advantages of layer and neighbor sampling approaches using a vertex-centric variance-based framework, reducing the number of sampled vertices drastically while ensuring the training quality does not suffer and matches the quality of neighbor sampling. Another advantage of our method is that the user only needs to choose the batch size and the fanout hyperparameters as in the Neighbor Sampling approach, the algorithm itself then samples the minimum number of vertices in the later layers in an unbiased way while ensuring each vertex gets enough neighbors and a good approximation.

We achieve all the previously mentioned good properties with the help of Poisson Sampling. So, the next section will demonstrate applying Poisson Sampling to Layer Sampling, and then we will show how the advantages of Layer and Neighbor Sampling methods can be combined into LABOR while getting rid of their cons altogether.

### Poisson Layer Sampling (PLADIES)

In layer sampling, the main idea can be summarized as individual vertices making correlated decisions while sampling their neighbors, because in the end if a vertex \(t\) is sampled, all edges into the seed vertices \(S\), i.e., \(t s\), \(s S\), are added to the sampled subgraph. This can be interpreted as vertices in \(S\) making a collective decision on whether to sample \(t\), or not.

The other thing to keep in mind is that the existing layer sampling methods use sampling with replacement when doing importance sampling with unequal probabilities, because it is nontrivial to compute the inclusion probabilities in the without replacement case. The Hajek estimator in the without replacement case with equal probabilities becomes:

\[H^{}_{s}=}{_{t}}}{_{ t T N(s)}}}=M_{t}|N(S)|}{ _{t T N(s)}|N(S)|}=}}_{t T N(s)}M_{t} \]

and it has the variance:

\[(H^{}_{s})=-_{s}}{d_{s}-1} {_{s}} \]

Let us show how one can do layer sampling using Poisson sampling (PLADIES). Given \(n^{+}\) and unnormalized probabilities \(_{t}[0,), t N(S)\), we can ensure that \(_{t N(S)}_{t}=n\) by finding a solution of \(_{t N(S)}(1,c_{t})=n\) for \(c^{+}\) in linear time and redefining \(_{t}(1,c_{t})\) so that \(_{t}\). Then we include \(t N(S)\) in our sample \(T\) with probability \(_{t}\) by flipping a coin for it, i.e., we sample \(r_{t} U(0,1)\) and include \(t T\) if \(r_{t}_{t}\). In the end, \(E[|T|]=n\) and we can still use the Hajek estimator \(H^{}_{s}\) or the Horvitz Thomson estimator \(H^{}_{s}\) to estimate \(H_{s}\). Doing layer sampling this way is unbiased by construction and achieves the same goal in linear time in contrast to the quadratic time debiasing approach explained in Chen et al. (2023), further discussion can be found in Appendix A.7. The variance then approximately becomes (Williams et al., 1998), see Appendix A.1 for a derivation:

\[(H^{}_{s})(H^{}_{s})=^{2}}_{t s}}-} \]One can notice that the minus term \(}\) enables the variance to converge to \(0\), if all \(_{t}=1\) and we get the exact result. However, in the sampling with replacement case, the variance goes to \(0\) only as the sample size goes to infinity.

### LABOR: Layer Neighbor Sampling

The design philosophy of LABOR Sampling is to create a direct alternative to Neighbor Sampling while incorporating the advantages of layer sampling. Mimicking Layer Sampling with Poisson Sampling in Section 3.1 still has the disadvantage that \(_{s}\) varies wildly for different \(s\). To overcome this and mimic Neighbor Sampling where \(E[_{s}]=(d_{s},k)\), where \(k\) is a given fanout hyperparameter, we proceed as follows: for given \(_{t} 0, t N(S)\) denoting unnormalized probabilities, for a given \(s\), let us define \(c_{s}\) as the quantity satisfying the following equality if \(k<d_{s}\), otherwise \(c_{s}=_{t s}}\):

\[^{2}}_{t s}_{t})}-} =-} \]

Note that \(-}\) is the variance when \(_{t}=}, t N(s)\) so that \(E[_{s}]=k\). Also note that:

\[}{d_{s}-1}(-})--k}{d_{s}-1} =-k}{k(d_{s}-1)}--k}{d_{s}-1}=0 \]

meaning that the variance target we set through (9) is equal to Neighbor Sampling's variance in (7) after calibrating with \(}{d_{s}-1}\)Ohlsson (1998) and it will result in \(E[_{s}] k\) with strict equality in the uniform probability case. Then each vertex \(s S\) samples \(t s\) with probability \(c_{s}_{t}\). To keep the collective decision making, we sample \(r_{t} U(0,1), t N(S)\) and vertex \(s\) samples vertex \(t\) if and only if \(r_{t} c_{s}_{t}\). Note that if we use a uniform random variable for each edge \(r_{ts}\) instead of each vertex \(r_{t}\), and if \(\) is uniformly initialized, then we get the same behavior as Neighbor Sampling.

#### 3.2.1 Importance Sampling

Given the sampling procedure above, one wonders how different choices of \( 0\) will affect \(|T|\), the total number of unique vertices sampled. We can compute its expected value as follows:

\[E[|T|] =_{t N(S)}(t T) \] \[=_{t N(S)}(1,_{t}_{t s}c_{s})\]

In particular, we need to minimize \(E[|T|]\):

\[^{*}=*{arg\,min}_{ 0}_{t N(S)}(1,_{t} _{t s}c_{s}) \]

Note that for any given \( 0\), \(E[|T|]\) is the same for any vector multiple \(x,x^{+}\), meaning the objective function is homogeneous of degree \(0\).

#### 3.2.2 Computing \(c\) and \(^{*}\)

\(c_{s}\) was defined to be the scalar satisfying the following equality involving the variance of the estimator of \(H_{s}\):

\[^{2}}_{t s}_{t})}- }=-} \]

If we rearrange the terms, we get:

\[_{t s}_{t})}=^{2}}{k} \]One can see that the left-hand side of the equality is monotonically decreasing with respect to \(c_{s} 0\). Thus one can use binary search to find the \(c_{s}\) satisfying the above equality to any precision needed. But we opt to use the following iterative algorithm to compute it:

\[v_{s}^{(0)}=0,c_{s}^{(0)}=^{2}}_{t s}} \]

\[c_{s}^{(i+1)}=^{(i)}}{^{2}}{k}-v_{s}^{(i)}}-v_{s} ^{(i)}+_{t s}^{(i)}_{t})} \]

\[v_{s}^{(i+1)}=_{t s}[c_{s}^{(i+1)}_{t} 1] \]

This iterative algorithm converges in at most \(d_{s}\) steps and the convergence is exact and monotonic from below. One can also implement it in linear time \((d_{s})\) if \(\{_{t} t s\}\) is sorted and making use of precomputed prefix sum arrays. Note that \(c=c()\), meaning that \(c\) is a function of the given probability vector \(\). To compute \(^{*}\), we use a similar fixed point iteration as follows:

\[^{(0)}=1, t N(S):_{t}^{(i+1)}=_{t}^{(i)}_{t s}c_{s}( ^{(i)}) \]

Thus, we alternate between computing \(c=c()\), meaning \(c\) is computed with the current \(\), and updating \(\) with the computed \(c\) values, see the lines 5-10 in Algorithm 1. Each step of this iteration is guaranteed to lower the objective function value in (12) until convergence to a fixed point, see Appendix A.3. Modified formulation for a given weight matrix \(A_{ts}\) is discussed in Appendix A.5. Finally, Algorithm 1 summarizes LABOR including the actual sampling (lines 17-19) and computation of the edge weights (lines 20-22) due to the use of importance sampling.

```
1:Input: seed vertices \(S\), # tiers \(i\), fanout \(k\)
2:Output: sampled edges \(E^{}\) and weights \(A^{}\)
3:\(T\{t t N(S)\}\)
4:\(c_{s}=}, s S\)
5:\(_{t} 1, t T\)
6:while\(i>0\)do
7:for all\(s S\)do
8: Solve (14) for \(c_{s}\)
9:for all\(t T\)do
10:\(_{t}_{t}_{t s}c_{s}\)
11:\(i i-1\)
12:\(r_{t} U(0,1), t T\)
13:\(E^{}\) [ ]
14:\(A^{}\) [ ]
15:for all\(s S\)do
16:\(w 0\)
17:for all\(t N(s)\)do
18:if\(r_{t} c_{s}_{t}\)then
19:\(E^{}\).append(\(t s\))
20:\(w w+_{t})}\)
21:for all\(t N(s)\)do
22:if\(r_{t} c_{s}_{t}\)then
23:\(A^{}\).append(\(_{t})w}\))
```

**Algorithm 1** LABOR-i for uniform edge weights

### Choosing how many neighbors to sample

The variance of Poisson Sampling when \(_{t}=}\) is \(-}\). One might question why we are trying to match the variance of Neighbor Sampling and choose to use a fixed fanout for all the seed vertices. In the uniform probability case, if we have already sampled some set of edges for the vertices in \(S\), and want to sample one more edge, then we should sample the new edge for the vertex \(s\), whose variance would decrease the most. If vertex \(s\) currently has \(_{s}\) sampled edges, then sampling one more edge for it would improve its variance from \(}-}\) to \(}-}\). Since the derivative of the variance with respect to \(_{s}\) is monotonic, we can reason about the marginal improvements by comparing their derivatives, which is:

\[_{s}}_{s}}-}=-_{s}^{2}} \]

Notice that the derivative does not depend on the degree \(d_{s}\) of the vertex \(s\) at all, and the greater the magnitude of the derivative, the more the variance of a vertex improves by sampling one more edge. Thus, choosing any vertex \(s\) with least number of edges sampled would work for us, that is \(s=_{s^{} S}_{s^{}}\). In light of this observation, one can see that it is optimal to sample an equal number of edges for each vertex in \(S\). This

### Sampling a fixed number of neighbors

To make LABOR-0 even more similar to Neighbor Sampling, one can easily resort to Sequential Poisson Sampling by Ohlsson (1998) if one wants \(_{s}=(k,d_{s})\) instead of \(E[_{s}]=(k,d_{s})\). Given \(_{t}\), \(c_{s}\) and \(r_{t}\), we pick the \(_{s}=(k,d_{s})\) smallest vertices \(t s\) with respect to \(}{c_{s}_{t}}\), which can be computed in expected linear time by using the quickselect algorithm (Hoare, 1961).4

## 4 Experiments

In this section, we empirically evaluate the performance of each method in the node-prediction setting on the following datasets: reddit (Hamilton et al., 2017), products (Hu et al., 2020), yelp, flickr (Zeng et al., 2020). Details about these datasets are given in Table 1. We compare our proposed LABOR variants LABOR-0, LABOR-1 and LABOR-*, where \(0,1,*\) stand for the number of fixed point iterations applied to optimize (12), respectively, together with PLADIES (see Section 3.1), against the baseline sampling methods: Neighbor Sampling (NS) and LADIES. We do not include Fast-GCN in our comparisons as it is super-seeded by the LADIES paper. The works of Liu et al. (2020); Zhang et al. (2021); Huang et al. (2018); Cong et al. (2021); Dong et al. (2021) are not included in the comparisons because they either take into account additional information such as historical embeddings or their magnitudes or they have an additional sampling structure such as a vertex cache to sample from. Also the techniques in these papers are mostly orthogonal to the sampling problem and algorithms discussed in this paper, hence our proposed LABOR and PLADIES methods can be used together with them. However, we leave this investigation to future work.

We evaluate all the methods on the GCN model in (2) with 3 layers, with 256 hidden dimension and residual skip connections enabled. We use the Adam optimizer (Kingma and Ba, 2014) with \(0.001\) learning rate. We implemented LABOR variants and PLADIES in DGL5(Wang et al., 2019), and carried out our experiments using DGL with the Pytorch backend (Paszke et al., 2019), URLs are provided in Appendix A.8. Experiments were repeated 100 times and averages are presented.

### Baseline Comparison of Vertex/Edge Efficiency

LABOR comparison against NS:In this experiment, we set the batch size to 1,000 and the fanout \(k=10\) for LABOR and NS methods to see the difference in the sizes of the sampled subgraphs and also how vertex/edge efficient they are. In Figure 1, we can see that LABOR variants outperform NS in both vertex and edge efficiency metrics. Table 2 shows the difference of the sampled subgraph sizes in each layer. One can see that on reddit, LABOR-* samples \(6.9\) fewer vertices in the 3rd layer leading much faster convergence in terms of cumulative number of sampled vertices. On the flickr dataset however, LABOR-* samples only \(1.3\) fewer vertices. The amount of difference depends on two factors. The first is the amount of overlap of neighbors among the vertices in \(S\). If the neighbors of vertices in \(S\) did not overlap at all, then one obviously can not do better than NS. The second is the average degree of the graph. With a fanout of \(10\), both NS and LABOR has to copy the whole neighborhood of a vertex \(s\) with degree \(d_{s} 10\). Thus for such graphs, it is expected that the difference will be insignificant. The average degree of the flickr is \(10.09\) (Table 1), and thus there is only a small difference between LABOR and NS.

  
**Dataset** & \(||\) & \(||\) & \(|}{||}\) & **\# Reats.** & \(|}|\) & **budget** & **train - val - test (\%)** \\  reddit & 233K & 115M & 493.56 & 602 & 60k & 66 - 10 - 24 \\ products & 2.45M & 61.9M & 25.26 & 100 & 400k & 8 - 2 - 90 \\ yelp & 717K & 14.0M & 19.52 & 300 & 200k & 75 - 10 - 15 \\ flickr & 89.2K & 900K & 10.09 & 500 & 70k & 50 - 25 - 25 \\   

Table 1: Properties of the datasets used in experiments: numbers of vertices (\(|V|\)), edges (\(|E|\)), avg. degree (\(\)), number of features, sampling budget used, training, validation and test vertex split.

As seen in Figure 1 and Table 2, LABOR-0 reduces both the number of vertices and edges sampled. On the other hand, when importance sampling is enabled, the number of vertices sampled goes down while number of edges sampled goes up. This is because when importance sampling is used, inclusion probabilities become nonuniform and it takes more edges per seed vertex to get a good approximation (see Equation (9)). Thus, LABOR-0 leads the pack when it comes to edge efficiency.

Layer sampling algorithm comparison:The hyperparameters of LADIES and PLADIES were picked to match LABOR-* so that all methods have the same vertex sampling budget in each layer (see Table 2). Figure 1 shows that, LADIES and PLADIES perform almost identically in terms of convergence, on all but the flickr dataset where there is a big difference between the two in favor of PLADIES. We also see that LABOR variants outperform LADIES variants on products, yelp, and flickr in terms of vertex efficiency with the exception of reddit, where the best-performing algorithm is PLADIES proposed by us. Regarding edge efficiency, however, we see that LADIES variants are inefficient and all LABOR variants outperform LADIES variants across all the datasets by up to \(13\).

Comparing LABOR variants:Looking at Table 2, we can see that LABOR-0 has the best runtime performance across all datasets. This is due to 1) sampling fewer edges, 2) not having the fixed point iteration overhead, compared to the other LABOR variants. By design, all LABOR variants should

Figure 1: The validation F1-score and training loss curves on different datasets with same batch size. The x-axis stands for cumulative number of vertices and edges used during training, respectively. The soft edges represent the confidence interval. Number of sampled vertices and edges can be found in Table 2.

have the same convergence curves, as seen in Appendix A.2 in Figure 3. Then, the decision of which variant to use depends on one factor: feature access speed. If vertex features were stored on a slow storage medium (such as, on host memory accessed over PCI-E), then minimizing number of sampled vertices would become the highest priority, in which case, one should pick LABOR-*. Depending on the relative vertex feature access performance and the performance of the training processor, one can choose to use LABOR-\(j\), the faster feature access, the lower the \(j\).

### LABOR vs NS under vertex sampling budgets

In this experiment, we set a limit on the number of sampled vertices and modify the batch size to match the given vertex budget. The budgets used were picked around the same magnitude with numbers in the Table 2 in the \(|V_{3}|\) column and can be found in Table 1. Figure 2 displays the result of this experiment. Table 3 shows that the more vertex efficient the sampling method is, the larger batch size it can use during training. Number of sampled vertices is not a function of the batch size for the LADIES algorithm so we do not include it in this comparison. All of the experiments were repeated 100 times and their averages were plotted, that is why our convergence plots are smooth and differences are clear. The most striking result in this experiment is that there can be up to \(112\) difference in batch sizes of LABOR-* and NS algorithms on the reddit dataset, which translates into faster convergence as the training loss and validation F1-score curves in Figure 2 show.

### Importance Sampling, Fixed Point Iterations

In this section, we look at the convergence behavior of the fixed point iterations described in Section 3.2.2. Table 4 shows the number of sampled vertices in the last layer with respect to the number of fixed point iterations applied. In this table, the * stands for applying the fixed point iterations until

  
**Dataset** & **Algo.** & \(|}|\) & \(|}|\) & \(|}|\) & \(|}|\) & \(|}|\) & \(|}|\) & \(|}|\) & **it/s** & **test F1-score** \\   & PLADIES & 24 & 2390 & 14.1 & 927 & 6.0 & 33.2 & 1 & 1.7 & 96.21 \(\) 0.06 \\  & LADIES & 25 & 2270 & 14.5 & 852 & 6.0 & 32.5 & 1 & 1.8 & 96.20 \(\) 0.05 \\  & LABOR-* & 24 & 1070 & 13.7 & 435 & 6.0 & 26.9 & 1 & 4.1 & 96.23 \(\) 0.05 \\  & LABOR-1 & 27 & 261 & 14.4 & 116 & 6.1 & 16.7 & 1 & 24.8 & 96.23 \(\) 0.06 \\  & LABOR-0 & 36 & 177 & 17.8 & 67 & 6.8 & 9.6 & 1 & 37.6 & 96.25 \(\) 0.05 \\  & NS & 167 & 682 & 68.3 & 100 & 10.1 & 9.7 & 1 & 14.2 & 96.24 \(\) 0.05 \\   & PLADIES & 160 & 2380 & 51.2 & 293 & 9.7 & 11.7 & 1 & 4.1 & 78.44 \(\) 0.24 \\  & LADIES & 165 & 2230 & 51.8 & 270 & 9.7 & 11.5 & 1 & 4.2 & 78.59 \(\) 0.22 \\  & LABOR-* & 166 & 1250 & 51.8 & 167 & 9.8 & 10.6 & 1 & 6.2 & 78.59 \(\) 0.34 \\  & LABOR-1 & 178 & 799 & 53.4 & 136 & 9.8 & 10.5 & 1 & 21.3 & 78.47 \(\) 0.26 \\  & LABOR-0 & 237 & 615 & 62.4 & 100 & 10.1 & 9.9 & 1 & 32.5 & 78.76 \(\) 0.26 \\  & NS & 513 & 944 & 95.4 & 106 & 10.6 & 9.9 & 1 & 24.6 & 78.48 \(\) 0.29 \\   & PLADIES & 100 & 1300 & 29.5 & 183 & 6.2 & 6.9 & 1 & 5.1 & 61.55 \(\) 0.87 \\  & LADIES & 102 & 1280 & 29.7 & 182 & 6.2 & 6.9 & 1 & 5.3 & 61.89 \(\) 0.66 \\  & LABOR-* & 105 & 991 & 30.7 & 158 & 6.1 & 6.8 & 1 & 13.3 & 61.57 \(\) 0.67 \\  & LABOR-1 & 109 & 447 & 31.0 & 96 & 6.2 & 6.8 & 1 & 27.3 & 61.71 \(\) 0.70 \\  & LABOR-0 & 138 & 318 & 35.1 & 54 & 6.2 & 6.3 & 1 & 27.2 & 61.55 \(\) 0.85 \\  & NS & 188 & 392 & 42.5 & 55 & 6.3 & 6.3 & 1 & 23.0 & 61.50 \(\) 0.66 \\   & PLADIES & 55 & 309 & 24.9 & 85 & 6.2 & 6.9 & 1 & 10.2 & 51.52 \(\) 0.26 \\  & LADIES & 56 & 308 & 25.1 & 85 & 6.2 & 6.9 & 1 & 10.5 & 50.79 \(\) 0.29 \\   & LABOR-* & 57 & 308 & 25.6 & 85 & 6.3 & 6.9 & 1 & 20.3 & 51.67 \(\) 0.27 \\   & LABOR-1 & 58 & 242 & 25.9 & 73 & 6.3 & 6.9 & 1 & 32.7 & 51.66 \(\) 0.24 \\   & LABOR-0 & 66 & 219 & 29.1 & 52 & 6.4 & 6.7 & 1 & 33.3 & 51.65 \(\) 0.26 \\   & NS & 73 & 244 & 32.8 & 52 & 6.4 & 6.7 & 1 & 31.7 & 51.70 \(\) 0.23 \\   

Table 2: Average number of vertices and edges sampled in different layers (All the numbers are in thousands, lower is better). Last two columns show iterations (mini-batches) per second (it/s) and test F1-score, for both, higher is better. The hyperparameters of LADIES and PLADIES were picked to roughly match the number of vertices sampled by the LABOR-* to get a fair comparison. The convergence curves can be found in Figure 1. The timing information was measured on an NVIDIA T4 GPU. Green stands for best, red stands for worst results, with a \(5\%\) cutoff.

convergence, and convergence occurs in at most 15 iterations in practice before the relative change in the objective function is less than \(10^{-4}\). One can see that most of the reduction in the objective function (12) occurs after the first iteration, and the remaining iterations have diminishing returns. Full convergence can save from \(14\% 33\%\) depending on the dataset. The monotonically decreasing numbers provide empirical evidence for the presented proof in Appendix A.3.

## 5 Conclusions

In this paper, we introduced LABOR sampling, a novel way to combine layer and neighbor sampling approaches using a vertex-variance centric framework. We then transform the sampling problem into an optimization problem where the constraint is to match neighbor sampling variance for each vertex while sampling the fewest number of vertices. We show how to minimize this new objective function via fixed-point iterations. On datasets with dense graphs like Reddit, we show that our approach can sample a subgraph with \(7\) fewer vertices without degrading the batch quality. We also show that compared to LADIES, LABOR converges faster with same sampling budget.