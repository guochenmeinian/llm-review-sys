# Convergence of Alternating Gradient Descent for Matrix Factorization

Rachel Ward

University of Texas

Austin, TX

rward@math.utexas.edu

&Tamara G. Kolda

MathSci.ai

Dublin, CA

tammy.kolda@mathsci.ai

###### Abstract

We consider alternating gradient descent (AGD) with fixed step size applied to the asymmetric matrix factorization objective. We show that, for a rank-\(r\) matrix \(^{m n}\), \(T=C()}{_{r}()}^{2} (1/)\) iterations of alternating gradient descent suffice to reach an \(\)-optimal factorization \(\|-_{T}_{T}^{}\|_{}^{2} \|\|_{}^{2}\) with high probability starting from an atypical random initialization. The factors have rank \(d r\) so that \(_{T}^{m d}\) and \(_{T}^{n d}\), and mild overparameterization suffices for the constant \(C\) in the iteration complexity \(T\) to be an absolute constant. Experiments suggest that our proposed initialization is not merely of theoretical benefit, but rather significantly improves the convergence rate of gradient descent in practice. Our proof is conceptually simple: a uniform Polyak-Lojasiewicz (PL) inequality and uniform Lipschitz smoothness constant are guaranteed for a sufficient number of iterations, starting from our random initialization. Our proof method should be useful for extending and simplifying convergence analyses for a broader class of nonconvex low-rank factorization problems.

## 1 Introduction

This paper focuses on the convergence behavior of alternating gradient descent (AGD) on the low-rank matrix factorization objective

\[ f(,)\|^{ }-\|_{}^{2} ^{m d},^{n d}.\] (1)

Here, we assume \(m,n d r=()\). While there are a multitude of more efficient algorithms for low-rank matrix approximation, this serves as a simple prototype and special case of more complicated nonlinear optimization problems where gradient descent (or stochastic gradient descent) is the method of choice but not well-understood theoretically. Such problems include low-rank tensor factorization using the GCP algorithm descent , a stochastic gradient variant of the GCP algorithm , as well as deep learning optimization.

Surprisingly, the convergence behavior of gradient descent for low-rank matrix factorization is still not completely understood, in the sense that there is a large gap between theoretical guarantees and empirical performance. We take a step in closing this gap, providing a sharp linear convergence rate from a simple asymmetric random initialization. Precisely, we show that if \(\) is rank-\(r\), then a number of iterations \(T=C-)^{2}}^{2}()}{_ {2}^{2}()}(1/)\) suffices to obtain an \(\)-optimal factorization with high probability. Here, \(_{k}()\) denotes the \(k\)th singular value of \(\) and \(C>0\) is a numerical constant. To the authors' knowledge, this improves on the state-of-art convergence result in theliterature , which provides an iteration complexity \(T=C()}{_{r}()}^{3} (1/)\) for gradient descent to reach an \(\)-approximate rank-\(r\) approximation1.

Our improved convergence analysis is facilitated by our choice of initialization of \(_{0},_{0}\), which appears to be new in the literature and is distinct from the standard Gaussian initialization. Specifically, for \(_{1}\) and \(_{2}\) independent Gaussian matrices, we consider an "unbalanced" random initialization of the form \(_{0}}_{1}\) and \(_{0}_{2}\), where \(>0\) is the step-size used in (alternating) gradient descent. A crucial feature of this initialization is that the columns of \(_{0}\) are in the column span of \(\), and thus by invariance of the alternating gradient update steps, the columns of \(_{t}\) remain in the column span of \(\) throughout the optimization. Because of this, a positive \(r\)th singular value of \(_{t}\) provides a Polyak-Lojasiewicz (PL) inequality for the region of the loss landscape on which the trajectory of alternating gradient descent is guaranteed to be confined to, even though the matrix factorization loss function \(f\) in (1) does not satisfy a PL-inequality globally.

By Gaussian concentration, the pseudo-condition numbers \((_{0})}{_{r}(_{0})}()}{_{r}()}\) are comparable with high probability2; for a range of step-size \(\) and the unbalanced initialization \(_{0}}_{1}\) and \(_{0}_{2}\), we show that \((_{t})}{_{r}(_{t})}\) is guaranteed to remain comparable to \(()}{_{r}()}\) for a sufficiently large number of iterations \(t\) that we are guaranteed a linear rate of convergence with rate \((_{0})}{_{1}(_{0})}^{2}\).

The unbalanced initialization, with the particular re-scaling of \(_{0}\) and \(_{0}\) by \(}\) and \(\), respectively, is not a theoretical artifact but crucial in practice for achieving a faster convergence rate compared to a standard Gaussian initialization, as illustrated in Fig. 1. Also in Fig. 1, we compare empirical convergence rates to the theoretical rates derived in Theorem 3.1 below, indicating that our rates are sharp and made possible only by our particular choice of initialization. The derived convergence rate of (alternating) gradient descent starting from the particular asymmetric initialization where the columns of \(_{0}\) are in the column span of \(\) can be explained intuitively as follows: in this regime, the \(_{t}\) updates remain sufficiently small with respect to the initial scale of \(_{0}\), while the \(_{t}\) updates change sufficiently quickly with respect to the initial scale \(_{0}\), that the resulting alternating gradient descent dynamics on matrix factorization follow the dynamics of gradient descent on the linear regression problem \( g()=\|_{0}^{T}-\|_{F}^{2}\) where \(_{0}\) is held fixed at its initialization.

We acknowledge that our unbalanced initialization of \(_{0}\) and \(_{0}\) is different from the standard Gaussian random initialization in neural network training, which is a leading motivation for studying gradient descent as an algorithm for matrix factorization. The unbalanced initialization should not be viewed as at odds with the implicit bias of gradient descent towards a balanced factorization , which have been linked to better generalization performance in various neural network settings. An interesting direction of future research is to compare the properties of the factorizations obtained by (alternating) gradient descent, starting from various (balanced versus unbalanced) initializations.

## 2 Preliminaries

Throughout, for an \(m n\) matrix \(\), \(\|\|\) refers to the spectral norm and \(\|\|_{F}\) refers to the Frobenius norm.

Consider the square loss applied to the matrix factorization problem (1). The gradients are

\[_{\!}f(,) =(^{}-),\] (2a) \[_{\!}f(,) =(^{}-)^{}.\] (2b)

We will analyze alternating gradient descent, defined as follows.

_Assumption 1_ (Alternating Gradient Descent).: For fixed stepsize \(>0\) and initial condition \((_{0},_{0})\), the update is

\[_{t+1} =_{t}-_{}f(_{t}, _{t}),\] (A1a) \[_{t+1} =_{t}-_{}f(_{t+1},_{t}).\] (A1b)

We assume that the iterations are initialized in an asymmetric way, which depends on the step size \(\) and assumes a known upper bound on the spectral norm of \(\). The matrix factorization is of rank \(d>r\), and we also make assumptions about the relationship of \(d\), \(r\), and quantities \(s\), \(\), and \(\) that will impact the bounds on the probability of finding and \(\)-optimal factorization.

_Assumption 2_ (Initialization and key quantities).: Draw random matrices \(_{1},_{2}^{n d}\) with i.i.d. \((0,1/d)\) and \((0,1/n)\) entries, respectively. Fix \(C 1\), \(<1,\) and \(D\), and let

\[_{0}=\,C\,_{1}()}\, _{1},_{0}=^{1/2}\,D\,_{1} ()\,_{2}.\] (A2a) The factor matrices each have \[d r\] columns.

For \(>0\), define

\[=(1-}{})\] (A2b) The number of iterations for convergence to \[\] -optimal factorization will ultimately be shown to depend on \[=_{r}^{2}()}{C^{2}_{1}^{2}()}.\] (A2c) The probability of finding this \[\] -optimal factorization will depend on \[=(C_{1})^{d-r+1}+e^{-C_{2}d}+e^{-r/2}+e^{-d/2}\] (A2d) where \[C_{1},C_{2}>0\] are the universal constants in A.1.

Observe that the initialization of \(_{0}\) ensures its columns are in the column span of \(\).

_Remark 2.1_.: The quantity \(f(_{0},_{0})\) does not depend on the step size \(\) or \(_{1}()\) in Assumption 2 since

\[_{0}_{0}^{}-=(_{1}_{2}^{}-).\]

Figure 1: Alternating gradient descent for \(^{100 100}\) with \(()=5\) and factors of size \(100 10\), The plot shows five runs each of our proposed initialization and compared with the standard random initialization. The title of each plot shows the condition and step length.

Main results

Our first main result gives a sharp guarantee on the number of iterations necessary for alternating gradient descent to be guaranteed to produce an \(\)-optimal factorization.

**Theorem 3.1** (Main result, informal).: _For a rank-\(r\) matrix \(^{m n}\), set \(d r\) and consider \(_{0},_{0}\) randomly initialized as in Assumption 2. For any \(>0\), there is an explicit step-size \(=()>0\) for alternating gradient descent as in Assumption 1 such that_

\[\|-_{T}_{T}^{}\|_{}^{2}  T C^{2}()}{ _{r}^{2}()}}\|_{}^{2}}{}\]

_with probability \(1-\) with respect to the draw of \(_{0}\) and \(_{0}\) where \(\) is defined in (A2d). Here, \(C>0\) is an explicit numerical constant._

For more complete theorem statements, see Corollary 5.2 and Corollary 5.3 below.

We highlight a few points below.

1. The iteration complexity in Theorem 3.1 is independent of the ambient dimensions \(n,m\). In the edge case \(d=r\), \(}=O(r^{2})\), so the iteration complexity scales quadratically with \(r\). With mild multiplicative overparameterization \(d=(1+)r\), \(}=-1)^{2}}\), and the iteration complexity is essentially dimension-free. This is a direct result of the dramatic improvement in the condition number of a \((1+)r r\) Gaussian random matrix compared to the condition number of a square \(r r\) Gaussian random matrix.
2. Experiments illustrate that initializing \(_{0}\) in the column span of \(\), and especially re-scaling \(_{0}\) and \(_{0}\) by \(}\) and \(\), respectively, is crucial in practice for improving the convergence rate of gradient descent. See Figs. 2 to 4.
3. The iteration complexity in Theorem 3.1 is conservative. In experiments, the convergence rate often follows a dependence on \(()}{_{1}()}\) rather than \(^{2}()}{_{1}^{2}()}\) for the first several iterations.

### Our contribution and prior work

The seminal work of Burer and Monteiro  advocated for the general approach of using simple algorithms such as gradient descent directly applied to low-rank factor matrices for solving non-convex optimization problems with low-rank matrix solutions. Initial theoretical work on gradient descent for low-rank factorization problems such as , , , ,  did not prove global convergence of gradient descent, but rather local convergence of gradient descent starting from a spectral initialization (that is, an initialization involving SVD computations). In almost all cases, the spectral initialization is the dominant computation, and thus a more global convergence analysis for gradient descent is desirable.

Global convergence for gradient descent for matrix factorization problems without additional explicit regularization was first derived in the symmetric setting, where \(^{n n}\) is positive semi-definite, and \(f()=\|-^{}\|_{}^ {2}\), see for example .

For overparameterized symmetric matrix factorization, the convergence behavior and implicit bias towards particular solutions for gradient descent with small step-size and from small initialization was analyzed in the work .

The paper  initiated a study of gradient descent with fixed step-size in the more challenging setting of _asymmetric_ matrix factorization, where \(^{m n}\) is rank-\(r\) and the objective is \(\|-^{}\|_{}^{2}\). This work improved on previous work in the setting of gradient flow and gradient descent with decreasing step-size . The paper  proved an iteration complexity of \(T=nd()}{_{1}()}^{4}(1/)\) for reaching an \(\)-approximate matrix factorization, starting from small i.i.d. Gaussian initialization for the factors \(_{0},_{0}\). More recently,  studied gradient descent for asymmetric matrix factorization, and proved an iteration complexity \(T=C_{d}()}{_{r}( )}^{3}(1/)\) to reach an \(\)-optimal factorization, starting from small i.i.d. Gaussian initialization.

We improve on previous analysis of gradient descent applied to objectives of the form (1), providing an improved iteration complexity \(T=()}{_{r}()} ^{2}(1/)\) to reach an \(\)-approximate factorization. There is no dependence on the matrix dimensions in our bound, and the dependence on the rank \(r\) disappears if the optimization is mildly over-parameterized, i.e., \(d=(1+)r.\) We do note that our results are not directly comparable to previous work as we analyze alternating gradient descent rather than full gradient descent. Our method of proof is conceptually simpler than previous works; in particular, because our initialization \(_{0}\) is in the column span of \(\), we do not require a two-stage analysis and instead can prove a fast linear convergence from the initial iteration.

## 4 Preliminary lemmas

[Bounding sum of norms of gradients] Consider alternating gradient descent as in Assumption 1. If \(\|_{t}\|^{2}\), then

\[\|_{}f(_{t},_{t})\|_{}^{2} f(_{t},_{t})-f(_{t+1}, _{t}).\] (3)

If moreover \(\|_{t}\|^{2},\) then \(f(_{t},_{t}) f(_{t},_{t-1})\). Consequently, if \(\|_{t}\|^{2}\) for all \(t=0,,T,\) and \(\|_{t}\|^{2}\) for all \(t=0,,T,\) then \(_{t=0}^{T}\|_{}f(_{t},_{t})\|_{ }^{2}f(_{0},_{0})\) Likewise, if \(\|_{t+1}\|^{2}\), then

\[\|_{Y}f(_{t+1},_{t})\|_{}^{2}(f(_{t+1},_{t})-f(_{t+1},_{t+1})).\] (4)

and if \(\|_{t}\|^{2},\) then \(f(_{t+1},_{t}) f(_{t},_{t})\), and so if \(\|_{t+1}\|^{2}\) for all \(t=0,,T,\) and \(\|_{t}\|^{2}\) for all \(t=0,,T,\) then \(_{t=0}^{T}\|_{}f(_{t+1},_{t})\|_{ }^{2}f(_{0},_{0})\).

Proof.: The proof of Lemma 4 is a direct calculation:

\[f(_{t+1},_{t}) =\|-_{t+1}_{t}^{ }\|_{}^{2}\] \[=\|-(_{t}-_{}f (_{t},_{t}))_{t}^{}\|_{}^{2}\] \[=\|-_{t}_{t}^{}+ _{}f(_{t},_{t})_{t}^{ }\|_{}^{2}\] \[=\|-_{t}_{t}^{} \|_{}^{2}+\|_{}f(_{t}, _{t})_{t}^{}\|_{}^{2}-[(_{t}_{t}^{}-)_{t}( _{}f(_{t},_{t}))^{}}_{_{ }f(_{t},_{t})})^{}]\] \[=f(_{t},_{t})+\|_{ }f(_{t},_{t})_{t}^{}\|_{ }^{2}-\|_{}f(_{t},_{t})\|_{ }^{2}\] \[ f(_{t},_{t})+}{2}\|_{ }f(_{t},_{t})\|_{}^{2}\|_{t} \|^{2}-\|_{}f(_{t},_{t})\|_{ }^{2}\] \[ f(_{t},_{t})-\|_{ }f(_{t},_{t})\|_{}^{2}.\]

**Proposition 4.2** (Bounding singular values of iterates).: _Consider alternating gradient descent as in Assumption 1. Set \(f_{0}:=f(_{0},_{0})\). Set \(T_{*}=f_{0}}.\) Suppose \(_{1}^{2}(_{0}),_{1}^{2}(_{ 0}).\) Then for all \(0 T T_{*},\)_

1. \(\|_{T}\|}\|_{T }\|}\)_,_
2. \(_{r}(_{0})-}_{r}(_{T}) _{1}(_{T})_{1}(_{0})+}\)_._
3. \(_{r}(_{0})-}_{r}(_{T}) _{1}(_{T})_{1}(_{0})+}\)_._

The proof of Proposition 4.2 is in the supplement Appendix B.

**Proposition 4.3** (Initialization).: _Assume \(_{0}\) and \(_{0}\) are initialized as in Assumption 2, which fixes \(C 1\), \(<1\), and \(D\), and consider alternating gradient descent as in Assumption 1. Then with probability at least \(1-\), with respect to the random initialization and \(\) defined in (A2d),_

1. \(}()}{_{1}( )}_{r}(_{0}),_{1}(_{0}) },_{1}(_{0})}{3}\)_,_
2. \(}\|\|_{F}^{2} f(_{0},_{ 0})(1+)^{2}\|\|_{}^{2}\)_._

The proof of Proposition 4.3 is in the supplement Appendix C.

Combining the previous two propositions gives the following.

**Corollary 4.4**.: _Assume \(_{0}\) and \(_{0}\) are initialized as in Assumption 2, with the stronger assumption that \(C 4\). Consider alternating gradient descent as in Assumption 1 with \(()}\). With \(\) as in (A2c) and \(f_{0}=f(_{0},_{0})\), set \(T=f_{0}}\). With probability at least \(1-\), with respect to the random initialization and \(\) defined in (A2d), the following hold for all \(t=1,,T\):_

\[_{r}(_{t})},_{1}(_{t}),_{1}(_{t})}+ }\]

Proof.: By Proposition 4.3, we have the following event occurring with the stated probability:

\[_{r}^{2}()}{C^{2}_{1}^{2}() }_{r}^{2}(_{0})_{1}^{2}(_{0}) \]

where the upper bound uses that \(C 4\). Moreover, using that \(()}\), \(_{1}^{2}(_{0}) 9 C^{2}_{1}()^{2}\). For \(\) as in (A2c), note that \(T=f_{0}} {1}{32^{2}f_{0}},\) which means that we can apply Proposition 4.2 up to iteration \(T\), resulting in the bound \(_{r}(_{t})_{r}(_{0})-} }\). Similarly, \(_{1}(_{t}),_{1}(_{t})}+}\). 

Finally, we use a couple crucial lemmas which apply to our initialization of \(_{0}\) and \(_{0}\).

**Lemma 4.5**.: _Consider alternating gradient descent as in Assumption 1. If \((_{0})(),\) then \((_{t})()\) for all \(t\)._

Proof.: Suppose \((_{t})()\). Then \((_{t}_{t}^{}_{t}) (_{t})()\) and by the update of Assumption 1,

\[(_{t+1}) =(_{t}+_{t}- _{t}_{t}^{}_{t})\] \[(_{t})(_{t})(_{t}_{t}^{} _{t})\] \[().\]

**Lemma 4.6**.: _If \(\) is rank \(r\), \((_{t})()\), and \(_{r}(_{t})>0\) then_

\[\|_{\!}}f(_{t},_{t-1})\|_{}^{ 2} 2_{r}^{2}(_{t})f(_{t},_{t-1}).\] (5)

Proof.: If \(\) is rank \(r\), \((_{t})()\), and \(_{r}(_{t})>0,\) then \(_{t}\) is rank-\(r\); thus, \((_{t})=().\) In this case, each column of \((_{t}_{t}^{}-)\) is in the row span of \(_{t}^{}\), and so

\[\|_{\!}}f(_{t},_{t-1}) \|_{}^{2}=\|(_{t}_{t-1}^{}- )^{}_{t}\|_{}^{2}\\ =\|_{t}^{}(_{t}_{t-1}^{ }-)\|_{}^{2}_{r}^{2}(_{t})\ \|_{t}_{t-1}^{}-\|_{}^{2}. \]

_Remark 4.7_.: While Lemmas 4.5 and 4.6 are straightforward to prove in the setting we consider where \(\) is exactly rank-\(r\), these lemmas no longer hold beyond the exactly rank-\(r\) setting (while the rest of the theorems we use do extend). Numerical experiments such as Figure 4 illustrate that the proposed algorithm does extend to finding best low-rank approximations to general matrices, but the theory for these experiments will require a careful reworking of Lemmas 4.5 and 4.6.

## 5 Main results

We are now ready to prove the main results.

**Theorem 5.1**.: _Assume \(_{0}\) and \(_{0}\) are initialized as in Assumption 2, with the stronger assumption that \(C 4\). Consider alternating gradient descent as in Assumption 1 with_

\[()}.\]

_With \(\) as in (A2c) and \(f_{0}=f(_{0},_{0})\), set_

\[T=f_{0}}.\]

_Then with probability at least \(1-\), with respect to the random initialization and \(\) defined in (A2d), the following hold for all \(t=1,,T\):_

\[\|-_{t}_{t}^{}\|_{}^{2}  2(- t/4)f_{0}\] (6) \[(- t/4)(1+)^{2}\| \|_{}^{2}.\]

Proof.: Corollary 4.4 implies that \(_{r}(_{t})^{2}\) for \(t=1,,T\). Lemmas 4.5 and 4.6 imply since \(_{0}\) is initialized in the column space of \(\), \(_{t}\) remains in the column space of \(\) for all \(t\), and

\[\|_{\!}}f(_{t+1},_{t})\|_ {}^{2} =\|(^{}-_{t}_{t+1}^{ })_{t+1}\|_{}^{2}_{r}(_{t+1 })^{2}\|(^{}-_{t}_{t+1}^{})\|_ {}^{2}\] (7) \[\|-_{t+1}_{t }^{}\|_{}^{2}=f(_{t+1},_{t}).\]

That is, a lower bound on \(_{r}(_{t})^{2}\) implies that the gradient step with respect to \(\) satisfies the Polyak-Lojasiewicz (PL)-equality3.

We can combine this PL inequality with the Lipschitz bound from Lemma 4.1 to derive the linear convergence rate. Indeed, by (3),

\[f(_{t+1},_{t+1})-f(_{t+1},_{t})- \|_{\!}}f(_{t+1},_{t}) \|_{}^{2}-f(_{t+1},_{t}).\]where the final inequality is (7). Consequently, using Proposition 4.3,

\[f(_{T},_{T})(1-/4)f(_{T-1}, _{T-1})(1-/4)^{T}\,f(_{0},_{0})\\ (- T/4)f(_{0},_{0}).\]

**Corollary 5.2**.: _Assume \(_{0}\) and \(_{0}\) are initialized as in Assumption 2, with the stronger assumptions that \(C 4\) and \(\). Consider alternating gradient descent as in Assumption 1 with_

\[(2f_{0}/)}},\] (8)

_where \(\) is defined in (2c) and \(f_{0}=f(_{0},_{0})\). Then with probability at least \(1-\), with respect to the random initialization and \(\) defined in (2d), it holds_

\[\|-_{T}_{T}^{}\|_{}^{2}  T=f_{0}} .\]

_Here \(\) is defined in (2b). Using the upper bound for \(\) in (8), the iteration complexity to reach an \(\)-optimal loss value is_

\[T=((()}{_{r}( )})^{2}}(\|_{}^{2} }{})).\]

This corollary follows from Theorem 5.1 by solving for \(\) so that the RHS of (6) is at most \(,\) and then noting that \((2f_{0}/)}}\) implies that \(()}\) when \(,\) using the lower bound on \(f_{0}\) from Proposition 4.3.

Using this corollary recursively, we can prove that the loss value remains small for \(T^{}f_{0}}\), provided we increase the lower bound on \(C\) by a factor of 2. The proof is in supplementary section D.

**Corollary 5.3**.: _Assume \(_{0}\) and \(_{0}\) are initialized as in Assumption 2, with the stronger assumptions that \(C 8\) and \(\). Fix \(<1/16\), and consider alternating gradient descent as in Assumption 1 with_

\[(1/)}},\] (9)

_where \(\) is defined in (2c) and \(f_{0}=f(_{0},_{0})\). Then with probability at least \(1-\), with respect to the random initialization and \(\) defined in (2d), it holds for any \(k\) that_

\[\|-_{T}_{T}^{}\|_{}^{2} ^{k}\|-_{0}_{0}^{}\|_{}^{2} T_{=0}^{k-1}( )^{}f_{0}}.\]

## 6 Numerical experiments

We perform an illustrative numerical experiment to demonstrate both the theoretical and practical benefits of the proposed initialization. We use gradient descent _without_ alternating to demonstrate that this theoretical assumption makes little difference in practice. We factorize a rank-5 (\(r=5\)) matrix of size \(100 100\). The matrix is constructed as \(=^{}\) with \(\) and \(\) random \(100 5\) orthonormal matrices and singular value ratio \(_{r}()/_{1}()=0.9\). The same matrix is used for each set of experiments. We compare four initializations:

\[ _{0} =C_{1}}_{(n  d)} _{0} =D_{1}}{}_{(n d)}\] \[()\] \[ _{0} =}_{(m d)} _{0} =}_{(n d)}\] \[ _{0} =}}_{(m d )} _{0} =}_{(n d)}\]

Here, \(\) denotes a random matrix with independent entries from \((0,1)\). The random initialization is what is commonly used and analyzed. We include ColSpan(\(\)) to understand the impact of starting in the column space of \(\) for \(_{0}\). Our proposed initialization (Assumption 2) combines this with an asymmetric scaling. In all experiments we use the defaults \(C=4\) and \(D=C/9\) with \(=1e{-}10\) for computing the proposed initialization as well as the theoretical step size. We assume \(_{1}=_{1}()\) is known in these cases. (In practice, a misestimate of \(_{1}\) can be compensated with a different value for \(C\).)

Figure 2 shows how the proposed method performs using the theoretical step size and compared to its theory. We also compare our initialization to three other initializations with the same step size. We consider two different levels of over-factoring, choosing \(d=10\) (i.e, \(2r\)) and \(d=6\) (i.e., \(r+1\)). All methods perform better for larger \(d=10\). The theory for the proposed initialization underestimates its performance (there may be room for further improvement) but still shows a stark and consistent advantage compared to the performance of the standard initialization, as well as compared to initializing in the column span of \(\) but not asymmetrically, or initializing asymmetrically but not in the column span of \(\).

Figure 3 shows how the three initializations compare with different step lengths. The advantage of the proposed initialization persists even with step lengths that are larger than that proposed by the theory, up until the step length is too large for any method to converge (\(=1\)). We emphasize that we are showing standard gradient descent, not alternating gradient descent. (There is no major difference between the two in our experience.)

Although the theory requires that \(\) be exactly rank-\(r\), Fig. 4 shows that the proposed initialization still maintains its advantage for noisy problems that are only approximately rank-\(r\).