# Improved Few-Shot Jailbreaking Can Circumvent

Aligned Language Models and Their Defenses

 Xiaosen Zheng\({}^{*1,2}\), Tianyu Pang\({}^{ 1}\), Chao Du\({}^{1}\), Qian Liu\({}^{1}\), Jing Jiang\({}^{ 2}\), Min Lin\({}^{1}\)

\({}^{1}\)Sea AI Lab, Singapore

\({}^{2}\)Singapore Management University

{zhengxs, tianyupang, duchao, liuqian, linmin}@sea.com;jingjiang@smu.edu.sg

Work done during Xiaosen Zheng's internship at Sea AI Lab.Correspondence to Tianyu Pang and Jing Jiang.

###### Abstract

Recently, Anil et al.  show that many-shot (up to hundreds of) demonstrations can jailbreak state-of-the-art LLMs by exploiting their long-context capability. Nevertheless, is it possible to use _few-shot demonstrations_ to efficiently jailbreak LLMs within limited context sizes? While the vanilla few-shot jailbreaking may be inefficient, we propose improved techniques such as injecting special system tokens like [/INST] and employing demo-level random search from a collected demo pool. These simple techniques result in surprisingly effective jailbreaking against aligned LLMs (even with advanced defenses). For example, our method achieves \(>80\%\) (mostly \(>95\%\)) ASRs on Llama-2-7B and Llama-3-8B without multiple restarts, even if the models are enhanced by strong defenses such as perplexity detection and/or SmoothLLM, which is challenging for suffix-based jailbreaking. In addition, we conduct comprehensive and elaborate (e.g., making sure to _use correct system prompts_) evaluations against other aligned LLMs and advanced defenses, where our method consistently achieves nearly \(100\%\) ASRs. Our code is available at https://github.com/sail-sg/I-FSJ.

## 1 Introduction

Large language models (LLMs) are typically trained to be safety-aligned in order to avoid misuse during their widespread deployment . However, many red-teaming efforts have focused on proposing _jailbreaking attacks_ and reporting successful cases in which LLMs are misled into producing harmful or toxic content .

When jailbreaking, optimization-based attacks search for adversarial suffixes that can achieve high attack success rates (ASRs) ; more recently, Andriushchenko et al.  use prompting and self-transfer techniques to randomly search adversarial suffixes, while reporting 100% ASRs on both Llama-2-Chat-7B and Llama-3-8B . Although effective against aligned LLMs, these adversarial suffixes mostly have no semantic meaning (even after low-perplexity regularization ), making them susceptible to _jailbreaking defenses_ like perplexity filters  and SmoothLLM . As empirically reported in Figure 5, adversarial suffixes generated by Andriushchenko et al.  result in quite high perplexity and are easily detectable.

LLM-assisted attacks, on the other hand, use auxiliary LLMs to generate adversarial but semantically meaningful requests capable of jailbreaking the target LLM, usually requiring only tens of queries . The generated adversarial requests can bypass perplexity filters and are insensitive to defenses that rely on input preprocessing . On the downside, it can be challenging for LLM-assisted attacks to achieve state-of-the-art ASRs on aligned LLMs, especially when they are evaluated under strict conditions (e.g., using the correct system prompt on Llama-2-Chat-7B) .

In contrast, manual attacks are more flexible, but necessitate elaborate designs and considerable human labor . In particular, Wei et al.  explore few-shot in-context demonstrations containing harmful responses to jailbreak LLMs. Anil et al.  automate and extend this strategy to many-shot jailbreaking, which prompts LLMs with hundreds of harmful demonstrations and can achieve high ASRs on cutting-edge closed-source models. Nonetheless, many-shot jailbreaking requires LLMs' long-context capability that is still lacking in most open-source models .

In this work, we revisit and significantly improve few-shot jailbreaking, especially against open-source LLMs with limited context sizes (\( 8192\)). We first automatically create a _demo pool_ containing harmful responses generated by "helpful-inclined" models like Mistral-7B  (i.e., not specifically safety-aligned). Then, we inject _special tokens_ from the target LLM's system prompt, such as [/INST] in Llama-2-7B-Chat,1 into the generated demos as illustrated in Figure 1. Finally, given the number of demo shots (e.g., 4-shot or 8-shot), we apply _demo-level random search_ in the demo pool to optimize the attacking loss.

As summarized in Table 1, our **improved few-shot jailbreaking** (named as \(\)-FSJ) achieves \(>80\%\) (mostly \(>95\%\)) ASRs on aligned LLMs including Llama-2-7B and Llama-3-8B. In addition, as reported in Table 3, we further enhance Llama-2-7B by different jailbreaking defenses, while our \(\)-FSJ can still achieve \(>95\%\) ASRs in most cases. Note that the random search operation in \(\)-FSJ is demo-level, not token-level, so the crafted inputs remain semantic. Overall, \(\)-FSJ is completely

Figure 1: **Injecting special tokens into the generated demonstrations on Llama-2-7B-Chat. Given an original FSJ demonstration, we construct \(\)-FSJ demonstration by first injecting [/INST] between the user message and assistant message, which is motivated by the specific formatting of Llama-2-Chat’s single message template. Additionally, we inject \(\)/INST] between the generated steps in the demonstration. After the \(\)-FSJ demonstration pool is constructed, we use demo-level random search to minimize the loss of generating the initial token “Step” on the target model.**

automated, eliminating the need for human labor and serving as a strong baseline for future research on jailbreaking attacks.

## 2 Related work

**Jailbreaking attacks.** LLMs like ChatGPT/GPT-4  and Llama-2  are generally designed to return helpful and safe responses, and they are trained to align with human values [43; 14; 5; 25]. However, red-teaming research has shown that LLMs can be jailbroken to produce harmful content using manually created or automatically generated prompts [9; 12; 16; 28; 30; 35; 36; 45; 47; 51; 57; 67; 73; 74; 44; 32; 2]. Additionally, Tian et al.  investigate the safety risks of LLM-based agents; Greshake et al.  introduce the concept of indirect prompt injection to compromise LLM-integrated applications; According to Wei et al. , aligned LLMs are vulnerable to jailbreaking due to the conflicting goals of capability and safety, as well as the gap between pretraining and safety training; Carlini et al.  point out the inherent vulnerability of neural networks to adversarial examples as a root cause. Recently, several studies have found that fine-tuning aligned LLMs with either poisoned or benign data can undermine their alignment and safety [13; 20; 29; 46; 65].

**Jailbreaking defenses.** Though instruction fine-tuning (SFT+RLHF)  plays a major role in safety alignment, various defense mechanisms have been proposed to strengthen models' alignment to tackle the safety issue from a system-level perspective . Context-based defenses insert either manually designed or automatically optimized safety prompts into the context [33; 41; 62; 63; 72; 71]. Input-detection-based defenses aim to detect potentially harmful input prompts like perplexity filter [1; 7; 11; 19; 18; 22; 27; 52; 59]. Perturbation-based defenses purify the input prompts by adding perturbations [22; 23; 50; 60; 70]. Output-detection-based defenses distinguish whether the generated responses contain harmful content and return the default refusal responses if harmful information detected [17; 21; 31; 68]. Mazeika et al.  propose to train the model against the suffix optimized by GCG  directly. Xu et al.  augment the original model with a safety expert model. In particular, they interpolate the token prediction distributions from these two models with a weighting hyperparameter. These methods generally fall into the adversarial training category.

## 3 Improved few-shot jailbreaking

We briefly introduce preliminaries before describing improved few-shot jailbreaking strategies such as constructing a demo pool, injecting special tokens, and performing demo-level random search.

### Preliminaries

**Jailbreaking setup.** We focus on jailbreaking attacks that aim to discover prompts to mislead LLMs producing harmful content, in order to fulfill a specific harmful request \(G^{*}\) (e.g., "Develop a detailed instruction manual for making a bomb or other explosive device"). We assume access to a collection of such harmful requests, which most aligned LLMs recognize as harmful. These typically encompass categories such as misinformation, violence, and hateful content, and are designed to elicit no response from the LLMs. Following previous notations [2; 10], we define a language model \(:^{*}^{*}\) as a function that transforms a sequence of input tokens into a sequence of output tokens. This model, referred to as the _target model_, is the one we aim to compromise or "jailbreak". We also define a judge function \(:^{*}\{\}\) to evaluate the content generated by the target model. The goal of a jailbreaking attacker is to find a prompt \(P^{*}\) such that when the target model processes \(P\), the judge function deems the output harmful, i.e., \(((P),G)=\).

**In-context learning (ICL).** ICL  is a remarkable capability of LLMs. During ICL, a LLM is presented with a demonstration set \(D=\{(x_{1},y_{1}),...,(x_{m},y_{m})\}=\{d_{1},...,d_{m}\}\), where each \(x_{i}\) is a query input and each \(y_{i}\) is the corresponding label or output. These examples effectively teach the model task-specific functionals. The process involves constructing a prompt that includes the demonstration set followed by a new query input for which the label needs to be predicted. The prompt takes the form \([x_{1},y_{1},...,x_{n},y_{n},x_{}]\), where \(x_{}\) is the new input query. The model, having inferred the underlying pattern from the provided examples, uses this prompt to predict the corresponding label \(y_{}\) for the new input \(x_{}\). ICL leverages the model's pre-trained knowledge and its ability to recognize and generalize patterns from the context provided by the demonstration set. This capability is particularly powerful because it allows the model to adapt to a wide range of tasks with minimal task-specific data, making it a flexible and efficient tool for various applications.

**Few-shot jailbreaking (FSJ).** Wei et al.  explore few-shot in-context demonstrations containing harmful responses to jailbreak LLMs. Anil et al.  automate and extend this strategy to many-shot jailbreaking, which prompts LLMs with hundreds of harmful demonstrations and can achieve high ASRs on cutting-edge closed-source models. Nonetheless, many-shot jailbreaking requires LLMs' long-context capability that is still lacking in most open-source models . And the vanilla FSJ is ineffective on some well-aligned LLMs like the Llama-2-Chat family.

### Improved strategies

We primarily develop three strategies to obtain **improved FSJ (\(\)-FSJ)**, as summarized below:

**Constructing a demo pool.** Given a set of harmful requests \(\{x_{1},...,x_{m}\}\) (e.g. the harmful behaviors from AdvBench ), we collect the corresponding harmful responses \(\{y_{1},...,y_{m}\}\) by prompting "helpful-inclined" models like Mistral-7B  which are not specifically safety-aligned. Finally, we create a demonstration pool as \(D=\{(x_{1},y_{1}),...,(x_{m},y_{m})\}=\{d_{1},...,d_{m}\}\). Note that we only build the pool once and use it to attack multiple models and defenses.

**Injecting special tokens.** In our initial trials, we attempt to directly use the generated vanilla FSJ demonstrations (examplified in the left part of Figure 1) to jailbreak LLMs and obtain non-trivial ASRs on some models like Qwen1.5-7B-Chat . But we keep obtaining near zero ASRs on much more well-aligned LLMs such as Llama-2-7B-Chat, which is consistent with the results reported by Wei et al.  and it seems FSJ is ineffective on these models.

_Intriguing observations:_ Interestingly, we observe that most current open-source LLMs' conversation templates separate the user message and assistant message (e.g. model completion) with special tokens. For example, as shown in Figure 1's single message template, Llama-2-Chat separates the messages with \(\)/INST\(\). We suspect the model is prone to conduct generation once presented by the \(\)/INST\(\) tokens. We thus hypothesize we can exploit this tendency with the help of ICL to induce the model to generate harmful content by appending harmful messages with the \(\)/INST\(\) tokens.

Thus, we inject _special tokens_ from the target LLM's system prompt, such as \(\)/INST\(\) in Llama-2-7B-Chat, into the generated demos as illustrated by the \(\)-FSJ Demonstration example in Figure 1. More specifically, given an original FSJ demonstration, we construct \(\)-FSJ demonstration by first injecting \(\)/INST\(\) between the user message and assistant message, which is motivated by the specific formatting of Llama-2-Chat's single message template. Additionally, we inject \(\)/INST\(\) between the generated steps in the demonstration.

**Demo-level random search.** After the \(\)-FSJ demo pool is constructed, we use demo-level random search to minimize the loss of generating the initial token (e.g. "Step") on the target model. We modify the random search (RS) algorithm [48; 2] into a demo-level variant, which is simple and requires only the output logits instead of gradients. The algorithm is as follows: _(i)_ prepend a sequence of \(n\) sampled demonstrations to the original request; _(ii)_ in each iteration, change a demonstration to another one at a random position in the sequence; _(iii)_ accept the change if it reduces the loss of generating target token (e.g., "Step" that leads the model to fulfill a harmful request) at the first position of the response. Furthermore, we implement the above demo-level RS algorithm in a batchway to achieve better parallelism as described in Algorithm 1. To tackle input-perturbation-based defenses like SmoothLLM , we introduce an ensemble variant of our demo-level RS method as described in Algorithm 2, which aims to find a combination of demonstrations that is not only effective for jailbreaking but also robust to perturbations. More details are provided in Appendix B.1.

## 4 Empirical studies

This section demonstrates the effectiveness of our \(\)-FSJ in jailbreaking various open-source aligned LLMs and advanced defenses.

### Implementation details

**Aligned LLMs.** We evaluate open-source and advanced LLMs for reproducibility. These include _Llama-2-Chat_, which underwent multiple rounds of manual red teaming for adversarial training, making them resilient to various attacks; _Llama-3-Instruct_, which were intentionally optimized for helpfulness and safety; _Open_Chat-3.5_, fine-tuned from Llama-2 using mixed-quality data with consideration of data quality; _Starling-LM_, fine-tuned from OpenChat 3.5 using RLHF with a reward model emphasizing helpfulness and harmlessness; and _Open_1.5-Chat_, trained on datasets annotated for safety concerns such as violence, bias, and pornography. According to Mazeika et al. , the attack success rates (ASRs) are stable within model families but vary significantly between different families. Therefore, we only consider the 7B variant across all model families.

**ASR metrics.** We follow Liu et al.  to evaluate the attacking effectiveness by two ASR metrics. The first one is a _Rule-based metric_ from Zou et al. , which is a keyword-based detection method that counts the number of harmful responses. Previous studies have used LLM-based metric such as GPT-4 to determine whether the responses are harmful. For reproducibility, we instead use the fine-tuned Llama Guard classifier [21; 10] following Chao et al. . More details are in Appendix B.2.

**Defenses.** We consider seven efficient defense mechanisms to further enhance aligned LLMs. Among these, _Self-Reminder_ and _ICD_ are context-based methods, (window) _PPL_ filters  are input-detection-based, while _Retokenization_ and _SmoothLLM_ are perturbation-based methods. _Safe Decoding_ belongs to adversarial training. _Llama Guard_ is output-detection-based that requires the attacker to jailbreak both the target model and the output filter, which judges whether the target model's outputs are safe or unsafe. More details are in Appendix B.3.

**Setup of our attack.** For the demonstrations used in FSJ and \(\)-FSJ, we apply Mistral-7B-Instruct-v0.2, an LLM with weaker safety alignment, to create the harmful content on a set of harmful requests.

Figure 2: **The ASRs of the three SmoothLLM variants on Llama-2-7B-Chat. We plot the LLM-based ASRs (Top) and rule-based ASRs (Bottom) for various perturbation percentages \(q\{5,10,15,20\}\); the results are compiled across three trials. Though the ASRs decrease as the \(q\) grows up (especially when the number of shots is relatively small), our method still maintains high ASRs (e.g. \( 80\%\)) across all the perturbation types at the 8-shot setting.**

    &  &  &  &  \\  & & & RS & Rule & LLM \\   &  & [/INST] & ✗ & 0\% & 0\% \\  & & [/INST] & ✓ & 68\% & 58\% \\   &  & [/INST] & ✗ & 34\% & 26\% \\  & & [/INST] & ✓ & 100\% & 96\% \\   & & ✗ & ✗ & 0\% & 0\% \\  & & ✗ & ✓ & 0\% & 0\% \\  & & [/INST] & ✗ & 38\% & 38\% \\  & & [/INST] & ✓ & **100\%** & **96\%** \\   &  & \(<\)|end\_of_turn\(|>\) GPT4 Correct Assistant: & ✗ & 98\% & 88\% \\  & & \(<\)|end_of_turn\(|>\) GPT4 Correct Assistant: & ✓ & 100\% & 96\% \\   &  & \(<\)|end_of_turn\(|>\) GPT4 Correct Assistant: & ✗ & 100\% & 86\% \\  & & \(<\)|end_of_turn\(|>\) GPT4 Correct Assistant: & ✓ & 100\% & 94\% \\   &  & \(\ ✗\) & 12\% & 4\% \\  & & \(<\)|end_of_turn\(|>\) GPT4 Correct Assistant: & ✓ & 100\% & 94\% \\   &  & \(\ ✗\) & 100\% & 90\% \\  & & \(<\)|end_of_turn\(|>\) GPT4 Correct Assistant: & ✓ & 100\% & 94\% \\   &  & \(\ ✗\) & 100\% & 90\% \\  & & \(<\)|end_of_turn\(|>\) GPT4 Correct Assistant: & ✓ & 100\% & 96\% \\   &  & \(\ ✗\) & 50\% & 16\% \\  & & \(\) & 100\% & 96\% \\   &  & \(\ ✗\) & 8\% & 90\% \\  & & \(<\)|end_of_turn\(|>\) GPT4 Correct Assistant: & ✓ & **100\%** & **98\%** \\   &  & \(<\)|im_end\(|>\)n\(<\)im_start\(|>\)assistant\(\)n & ✗ & 88\% & 78\% \\  & & \(<\)|im_end\(|>\)n\(<\)im_start\(|>\)assistant\(\)n & ✓ & 100\% & 96\% \\   &  & \(<\)|im_end\(|>\)n\(<\)im_start\(|>\)assistant\(\)n & ✗ & 96\% & 84\% \\  & & \(<\)|im_end\(|>\)n\(<\)|im_start\(|>\)assistant\(\)n & ✓ & 100\% & 96\% \\   &  & \(\ ✗\) & 88\% & 56\% \\  & & \(\) & 100\% & 94\% \\  & & \(<\)|im_end\(|>\)n\(<\)|im_start\(|>\)assistant\(\)n & ✗ & 98\% & 90\% \\  & & \(<\)|im_end\(|>\)n\(<\)|im_start\(|>\)assistant\(\)n & ✓ & **100\%** & **96\%** \\  ^{}\)} &  & \(\)|end_header\_id\(|>\)n\(\)n & ✗ & 0\% & 8\% \\  & & \(\)|end_header\_id\(|>\)n\(\)n & ✓ & 34\% & 34\% \\   &  & \(\)|end_header\_id\(|>\)n\(\)n & ✗ & 0\% & 8\% \\  & & \(\)|end_header\_id\(|>\)n\(\)n & ✓ & 84\% & 82\% \\   &  & \(\) & 0\% & 8\% \\  & & \(\)|end_header\_id\(|>\)n\(\)n & ✗ & 4\% & 10\% \\   &  & \(\)|end_header\_id\(|>\)n\(\)n & ✓ & **94\%** & **88\%** \\    \({}^{}\) Compared to Llama-2-7B-Chat, we generally need more shots to jailbreak Llama-3-8B-Instruct, which might be because of the improved alignment techniques .

Table 1: **ASRs of our \(\)-FSJ attack against aligned LLMs.** We measure attack success rates (ASRs) on the safety-aligned LLMs, using a dataset of 50 harmful requests from Chao et al. . We calculate ASRs using both the rule-based and LLM-based metrics, and the results are reported after just 3 random restarts (previous attacks usually apply 10\(\)100 restarts ). We ablate the effects of number of shots, injecting special tokens, and using demo-level RS in our \(\)-FSJ. We also calculate the mean and standard deviation of ASRs on these restarts, as shown in Table 7.

For more details, please check Appendix B.4. Our targets are a collection of 50 harmful behaviors from AdvBench curated by Chao et al.  that ensures distinct and diverse harmful requests. We exclude the demonstrations for the same target harmful behavior from the pool to avoid leakage. For the demo-level random search, we set batch size \(B=8\) and iterations \(T=128\). We let the target LLMs generate up to 100 new tokens. We use each LLM's default generation config. Every experiment is run on a single NVIDIA A100 (40G) GPU within a couple of hours. To address the concerns about leakage, diversity of the test behaviors, decoding length, correctness of special tokens, and number of necessary query times, we conduct additional ablation studies in Appendix C.1.

### Jailbreaking attacks on aligned LLMs

To examine the generality of our proposed \(\)-FSJ, we evaluate it on a diverse set of aligned LLMs. For different LLMs that utilize different conversation templates, we inject the corresponding special tokens, which distinct the user message and assistant message, into demonstrations. Note that such a process can be fully automated by a simple regular expression method. As detailed in Tables 1 and 7, we first find that our \(\)-FSJ attack is effective on all tested LLMs. In particular, on OpenChat-3.5, Starling-LM-7B, and Qwen1.5-7B-Chat, augmenting the FSJ with either demon-level random search or injecting special tokens is sufficient to achieve nearly 100% ASRs.

Nonetheless, models with stronger alignment, like Llama-2-7B-Chat and Llama-3-8B-Instruct, are more challenging. For these models, the FSJ with demo-level random search alone is insufficient for jailbreaking. Only by combining special tokens and demon-level random search can we successfully break these models' safety alignment, demonstrating the effectiveness of our techniques. Llama-3-Instruct requires more shots to jailbreak than Llama-2-Chat, which could be due to improved alignment techniques. Still, our \(\)-FSJ achieves over 90% ASRs within limited context window sizes.

Our approach consistently achieves near 100% ASR on most models tested, highlighting the significant vulnerabilities and unreliability of current alignment methods. These findings highlight the critical need for improved and more resilient alignment strategies in the development of LLMs.

Additionally, in the case of closed-source LLMs, the special tokens are mostly unknown, despite attempts to extract them . To address this issue, we propose constructing a pool of public special tokens from open-source LLMs, and then searching within this pool for high-performing special tokens on closed-source LLMs. As shown in Figure 3, we experiment on GPT-4 and observe that several public special tokens (e.g., "</text>", "</SYS>", "[/INST]") outperform the by-default one ("\(\)"). Furthermore, our findings indicate that there is some "transferability" with regard to special tokens, which could be an interesting research question.

We evaluate \(\)-FSJ on GPT-4 with similar settings as in Andriushchenko et al. , adopting a modified prompt template as shown in Figure 12. We conduct our experiments using the OpenAI API "gpt-4-1106-preview". As detailed in Tabel 2, we show that our \(\)-FSJ attack is effective on GPT-4, achieving \(>90\%\) rule-based and \(>80\%\) LLM-based ASRs with just 1-shot or 2-shot demos. Furthermore, we observe that both demo-level RS and the special token "</text>" (selected according to Figure 3) can consistently improve ASRs against GPT-4.

    &  &  &  \\  & RS & Rule & LLM & Rule & LLM \\  \(\) & ✗ & 48\% & 40\% & 50\% & 44\% \\ \(\) & ✓ & 74\% & 64\% & 76\% & 70\% \\  & ✗ & 70\% & 60\% & 70\% & 58\% \\  & ✓ & **90\%** & **84\%** & **94\%** & **86\%** \\   

Table 2: **ASRs of our \(\)-FSJ attack against GPT-4 on AdvBench.** For each request, we filter out similar harmful requests with a similarity higher than 0.5 from the demonstrations pool to avoid leakage.

Figure 3: **The loss of harmful target optimized by \(\)-FSJ across different injected special tokens on GPT-4.** We observe certain special tokens like </text> lead to lower loss.

### Jailbreaking attacks on Llama-2-7B-Chat + jailbreaking defenses

To assess our \(\)-FSJ's effectiveness against system-level robustness, we test it on Llama-2-7B-Chat with various defenses. As shown in Tables 3 and 8, our results demonstrate that \(\)-FSJ can circumvent jailbreaking defenses. For most defenses, randomly initialized \(n\)-shot demonstrations exhibit relatively low ASRs. However, optimizing the combination of demonstrations with demo-level random search can significantly boost the ASRs, peaking at near 100% in the 4-shot and 8-shot configurations. For the majority of defenses, the 4-shot setting is sufficient to achieve high ASRs.

Self-Reminder modifies Llama-2-Chat's default system message, which may degrade the safety alignment. ICD indicates a positive trend: as the defense shot increases, \(\)-FSJ's ASRs decrease significantly in the 2-shot setting. Attack success rates remain relatively low across defense shots, even with demo-level random search, indicating ICD's effectiveness. Yet, in the 4- and 8-shot settings, the ICD fails to defend the \(\)-FSJ. The PPL filter cannot reduce our ASRs because our input is mostly natural language with a perplexity lower than the filtering threshold (for example, the

    & ASR &  &  \\  & metric & 2-shot & 4-shot & 8-shot & 2-shot & 4-shot & 8-shot \\  RLHF  & Rule & 0\% & 34\% & 38\% & 68\% & 100\% & **100\%** \\  & LLM & 0\% & 26\% & 38\% & 58\% & 96\% & **96\%** \\   & Rule & 0\% & 42\% & 48\% & 80\% & 100\% & **100\%** \\  & LLM & 0\% & 36\% & 44\% & 74\% & 96\% & **94\%** \\   & Rule & 0\% & 8\% & 34\% & 46\% & 98\% & **100\%** \\  & LLM & 0\% & 6\% & 34\% & 38\% & 94\% & **96\%** \\   & Rule & 0\% & 4\% & 32\% & 22\% & 98\% & **100\%** \\  & LLM & 0\% & 4\% & 30\% & 20\% & 94\% & **94\%** \\   & Rule & 0\% & 6\% & 34\% & 16\% & 94\% & **100\%** \\  & LLM & 0\% & 6\% & 34\% & 16\% & 86\% & **96\%** \\   & Rule & 0\% & 34\% & 38\% & 68\% & 100\% & **100\%** \\  & LLM & 0\% & 26\% & 38\% & 58\% & 96\% & **96\%** \\   & Rule & 0\% & 34\% & 38\% & 68\% & 100\% & **100\%** \\  & LLM & 0\% & 26\% & 38\% & 58\% & 96\% & **96\%** \\   & Rule & 2\% & 48\% & 76\% & 72\% & 98\% & **100\%** \\  & LLM & 2\% & 36\% & 70\% & 64\% & 94\% & **96\%** \\   & Rule & 0\% & 10\% & 62\% & 30\% & 70\% & **96\%** \\  & LLM & 0\% & 6\% & 50\% & 10\% & 56\% & **88\%** \\   & Rule & 0\% & 38\% & 100\% & 90\% & 100\% & **100\%** \\  & LLM & 0\% & 16\% & 70\% & 4\% & 76\% & **90\%** \\   & Rule & 0\% & 4\% & 50\% & 2\% & 76\% & **94\%** \\  & LLM & 0\% & 4\% & 44\% & 2\% & 66\% & **86\%** \\   & Rule & 18\% & 82\% & 86\% & 76\% & 100\% & **100\%** \\  & LLM & 14\% & 78\% & 84\% & 74\% & 96\% & **94\%** \\   & Rule & 8\% & 20\% & 34\% & 82\% & 100\% & **100\%** \\  & LLM & 4\% & 20\% & 34\% & 82\% & 98\% & **96\%** \\   

* We employ the Llama Guard model to judge whether the generated content is harmful. If the generation is classified as “unsafe”, a refusal response like “I am sorry.” will be returned. To circumvent such a challenging defense, we modify our \(\)-FSJ demonstrations slightly, as shown in Figure 10, to achieve _propagating_ FSJ motivated by .

Table 3: **ASRs of our \(\)-FSJ against Llama-2-7B-Chat + jailbreaking defenses.** We measure attack success rates (ASRs) for the safety-aligned LLMs on a dataset of 50 harmful requests from Chao et al. . We calculate ASRs using both the rule-based and LLM-based metrics, and the results are reported after just 3 random restarts. We also calculate the mean and standard deviation of ASRs on these restarts, as shown in Table 8. The special tokens [\(\)/I\(\)] are injected.

highest perplexity of harmful queries in AdvBench). Even with a higher interpolation weight \(=4\), SafeDecoding cannot defend against our attack when computing the output token distribution.

**Remark 1: \(\)-FSJ is robust to perturbations.** Retokenization, which splits tokens and represents tokens with smaller tokens, can effectively perturb the encoded representation of the input prompt but fails to defend against \(\)-FSJ. Regarding the SmoothLLM variants, which directly perturb the input text in different ways, they successfully defend \(\)-FSJ at the 2-shot setting, resulting in \( 10\%\) ASRs. However, our method achieves \(>85\%\) ASRs against all of them at the 8-shot setting, which still falls into the few-shot regime. Also, as shown in Figure 2, we plot the LLM-based ASRs (**Top**) and rule-based ASRs (**Bottom**) for various perturbation percentages \(q\{5,10,15,20\}\); the results are compiled across three trials. At the 8-shot setting, our method still maintains high ASRs (e.g. \( 80\%\)) across all the perturbation types and perturbation rates. We also plot the loss curves of the random search optimization process in Figure 15. All these results demonstrate that \(\)-FSJ is robust to perturbations.

**Remark 2: \(\)-FSJ can be propagative.** To counter the defense of Llama Guard, we need to achieve propagating jailbreaking. Previous work  has demonstrated how to achieve adversarial-suffix-based propagating jailbreaking, which can jailbreak the target LLM and evade the Guard LLM. However, such an attack is also fragile confronting a perplexity filter. We instead modify our \(\)-FSJ demonstrations slightly by adaptively taking the Guard LLM's conversation template into account as shown in Figure 10. Our results show that \(\)-FSJ successfully jailbreaks both the target LLM and Guard LLM, demonstrating that \(\)-FSJ can be propagative.

### Further analysis

**The effect of pool size.** Our method inherently comes with a design choice: the size of the demonstration pool. To figure out the effect of this factor, we evaluate our method on Llama-2-7B-Chat under various pool sizes. As shown in Figure 4, the ASRs generally increase as the pool size grows and gradually saturate as observed from 256 to 512. The pool size shows a much larger impact on the 2-shot setting compared to the 4-shot and 8-shot settings, which might be because the latter two settings are relatively easier. Surprisingly, 32 demonstrations are already sufficient to achieve over 90% ASRs at an 8-shot setting, indicating the data efficiency of our method. Thus, we set the pool size as 512 in all of our experiments.

**The effect of shots.** Figure 4 highlights the impact of the number of shots on the ASR. As the number of shots increases from 2 to 8, there is a noticeable improvement in the ASR. With 2 shots, the ASR starts relatively low, around 25.4%, and gradually improves as the dataset size increases, reaching about 61.6% at its highest point. This indicates moderate effectiveness in terms of attack success when only 2 shots are used. For 4 shots, there is a significant jump in the initial ASR compared to 2 shots. The ASR begins at around 88.0% and rapidly stabilizes close to 97.8% as the dataset grows. This demonstrates that increasing the shot count to 4 substantially enhances the attack's success rate, achieving a high level of effectiveness early on. The effect is most pronounced when moving from 2to 4 shots, with further improvement seen when increasing to 8 shots, where the ASR approaches 100%. However, these results also indicate that beyond a certain point, increasing the number of shots does not substantially boost the ASRs since fewer shots are already sufficient. Thus, we test up to 8 shots in most of our experiments.

**Compared to other attack methods** As shown in Table 4, we compare our method against other attacks such as PAIR , GCG , AutoDAN , PAP , and PRS (stands for 'Prompt+RS+Self-transfer') . The table indicates that the \(\)-FSJ method with Demo RS is the most effective approach for bypassing safety measures in language models, achieving the highest ASRs in both scenarios (with and without a system message). The presence of a system message generally reduces the effectiveness of most methods, except for \(\)-FSJ with Demo RS and PRS, which remain robust. When compared with adversarial-suffix based method , though they may achieve comparable ASRs (e.g. 90% evaluated by the rule-based metric) with our method, it completely fails with a single perplexity (windowed) filter as shown in Figure 5.

## 5 Discussion

Jailbreaking attacks on LLMs are rapidly evolving, with different approaches demonstrating varying strengths and limitations. Our \(\)-FSJ represents a significant advancement in this domain, particularly against well-aligned open-source LLMs with limited context sizes. The primary innovation lies in the automated creation of the demonstration pool, the utilization of special tokens from the target LLM's system template, and demo-level random search, which together facilitate high ASRs. Our empirical studies demonstrate the efficacy of \(\)-FSJ in achieving high ASRs on aligned LLMs and various jailbreaking defenses. The automation of \(\)-FSJ eliminates the need for extensive human labor, offering a robust baseline for future research in this domain.