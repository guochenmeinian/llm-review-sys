# Any2Graph: Deep End-To-End Supervised Graph Prediction With An Optimal Transport Loss

Paul Krzakala

LTCI & CMAP, Telecom paris, IP Paris

&Remi Flamary

CMAP, Ecole polytechnique, IP Paris

&Junjie Yang

LTCI, Telecom paris, IP Paris

&Charlotte Laclau

LTCI, Telecom paris, IP Paris

&Matthieu Labeau

LTCI, Telecom paris, IP Paris

###### Abstract

We propose Any2Graph, a generic framework for end-to-end Supervised Graph Prediction (SGP) i.e. a deep learning model that predicts an entire graph for any kind of input. The framework is built on a novel Optimal Transport loss, the Partially-Masked Fused Gromov-Wasserstein, that exhibits all necessary properties (permutation invariance, differentiability) and is designed to handle any-sized graphs. Numerical experiments showcase the versatility of the approach that outperforms existing competitors on a novel challenging synthetic dataset and a variety of real-world tasks such as map construction from satellite image (Sat2Graph) or molecule prediction from fingerprint (Fingerprint2Graph). 1

## 1 Introduction

This work focuses on the problem of Supervised Graph Prediction (SGP), at the crossroads of Graph-based Learning and Structured Prediction. In contrast to node and graph classification or link prediction widely covered in recent literature by graph neural networks, the target variable in SGP is a graph and no particular assumption is made about the input variable. Emblenatic applications of SGP include knowledge graph extraction  or dependency parsing  in natural language processing, conditional graph scene generation in computer vision , , or molecule identification in chemistry , to name but a few. Moreover, close to SGP is the unsupervised task of graph generation notably motivated by _de novo_ drug design .

SGP raises some specific issues related to the complexity of the output space and the absence of widely accepted loss functions. First, the non-Euclidean nature of the output to be predicted makes both inference and learning challenging while the size of the output space is extremely large. Second, the arbitrary size of the output variable to predict requires a model with a flexible expressive power in the output space. Third, graphs are characterized by the absence of natural or ground-truth ordering of their nodes, making comparison and prediction difficult. This particular issue calls for a node permutation invariant distance to predict graphs. Scrutinizing the literature through the lens of these issues, we note that existing methodologies circumvent the difficulty of handling output graphs in various ways. A first body of work avoids end-to-end learning by relying on some relaxations. For instance, energy-based models (see for instance ) convert the problem into the learning of anenergy function of input and output while surrogate regression methods  implicitly embed output graphs into a given Hilbert space where the learning task boils down to vector-valued regression. Note that these two families of approaches generally involve a rather expensive decoding step at inference time. In what follows, we focus on methods that directly output graphs or close relaxations, enabling end-to-end learning.

One strategy to overcome the need for a permutation invariant loss is to exploit the nature of the input data to determine a node ordering, with the consequence that application to new types of data requires similar engineering. For instance, in _de novo_ drug generation SMILES representations  are generally used to determine atom ordering. In semantic parsing, the target graph is a tree that can be serialized  while in text-to-knowledge-graph, the task is re-framed into a sequence-to-sequence problem, often addressed with large autoregressive models. Finally, for road map extraction from satellite images, one can leverage the spatial positions of the nodes to define a unique ordering .

Another line of research proposes to address this problem more directly by seeking to solve a graph-matching problem, i.e., finding the one-to-one correspondence between nodes of the graphs. Among approaches in this category, we note methods dedicated to molecule generation  where the invariant loss is based on a characterization of graphs, ad-hoc to the molecule application. While being fully differentiable their loss does not generalize to other applications. In the similar topic of graph generation, Simonovsky and Komodakis  propose a more generic definition of the similarity between graphs by considering both feature and structural matching. However, they solve the problem using a two-step approach by using first a smooth matching approximation followed by a rounding step using the Hungarian algorithm to obtain a proper one-to-one matching, which comes with a high computational cost and introduces a non-differentiable step. For graph scene generation, Relationformer  is based on a bipartite object matching approach solved using a Hungarian matcher . The main shortcoming of this approach is that it fails to consider structural information in the matching process. The same problem is encountered by Melnyk et al. . We discuss Relationformer in more detail later in the article.

Finally, another way to approach end-to-end learning is to leverage the notion of graph barycenter to define the predicted graph. Relying on the Implicit Loss Embedding (ILE) property of surrogate regression, Brogat-Motto et al.  have exemplified this idea by exploiting an Optimal Transport loss, the Fused Gromov-Wasserstein (FGW) distance  for which barycenters can be computed efficiently . They proposed two variants, a non-parametric kernel-based one and a neural network-based one, referred to as FGW-Bary and FGW-BaryNN, respectively. However, to calculate the barycenter, the size must be known upstream, leaving the challenge of arbitrary size unresolved. In addition, prediction accuracy is highly dependent on the expressiveness of the barycenter, i.e. the nature and number of graph templates, resulting in high training and inference costs.

In contrast to existing works, our goal is to address the problem of supervised graph prediction in an end-to-end fashion, for different types of input modalities and for output graphs whose size and node ordering can be arbitrary.

Main contributionsThis paper presents Any2Graph, a versatile framework for end-to-end SGP. Any2Graph leverages a novel, fully differentiable, OT-based loss that satisfies all the previously mentioned properties, i.e., size agnostic and invariant to node permutation. In addition, the encoder part of Any2Graph allows us to leverage inputs of various types, such as images or sets of tokens. We complete our framework with a novel challenging synthetic dataset which we demonstrate to be suited for benchmarking SGP models.

The rest of the paper is organized as follows. After a reminder and a discussion about the relation between graph matching and optimal transport (Section 2), we introduce in Section 3, a _size-agnostic_ graph representation and an associated _differentiable_ and _node permutation invariant_ loss. This loss, denoted as **Partially Masked Fused Gromov Wasserstein** (PMFGW) is a novel and necessary adaptation of the FGW distance . This loss is then integrated into Any2Graph, an end-to-end learning framework depicted in Figure 1 and presented in Section 4. We express the whole framework objective as an ERM problem and highlight the adaptations necessary for extending existing deep learning architectures  to more general input modalities.

Section 5, presents a thorough empirical study of Any2Graph on various datasets. We evaluate our approach on four real-world problems with different input modalities as well as _Coloring_, a novel synthetic dataset. As none of the existing approaches could cover the range of input modalities, nor scale to very large-sized datasets, we adapted them for the purpose of fair comparison. The numerical results showcase the state-of-the-art performances of the proposed method in terms of prediction accuracy and ability to retrieve the right size of target graphs as well as computational efficiency.

## 2 Background on graph matching and optimal transport

Graph representation and notationsAn attributed graph \(g\) with \(m\) nodes can be represented by a tuple \((,)\) where \(=[_{1},,_{m}]^{}^{m d}\) encodes node features with \(_{i}^{d}\) labeling each node indexed by \(i\), \(^{m m}\) is a symmetric pairwise distance matrix that describes the graph relationships between the nodes such as the adjacency matrix or the shortest path matrix. Further, we denote \(_{m}\) the set of attributed graphs of \(m\) nodes and \(=_{m=1}^{M}_{m}\), the set of attributed graphs of size up to \(M\), where the size refers to the number of nodes in a graph and the largest size \(M\) is an important hyperparameter. In the following, \(_{m}^{m}\) is the all one vector and we denote \(_{m}=\{\{0,1\}^{m m}_{m}= _{m},^{T}_{m}=_{m}\}\) the set of permutation matrices.

Graph IsomorphismTwo graphs \(g_{1}=(_{1},_{1}),g_{2}=(_{2},_{2}) _{m}\) are said to be isomorphic whenever there exists \(_{m}\) such that \((_{1},_{1})=(_{2}, _{2}^{T})\), in which case we denote \(g_{1} g_{2}\). In this work, we consider all graphs to be unordered, meaning that all operations should be invariant by Graph Isomorphism (GI).

Comparing graphs of the same sizeDesigning a discrepancy to compare graphs is challenging, for instance, even for two graphs of the same size \(=(},})\), \(g=(,)\), one cannot simply compute a point-wise comparison as it would not satisfy GI invariance. A solution is to solve a Graph Matching (GM) problem, i.e., to find the optimal matching between the graphs and compute the pairwise errors between matched nodes and edges. This problem can be written as the following

\[(,g)=_{_{m}}_{i,j=1}^{m}_ {i,j}_{F}(}_{i},_{j})+_{i,j,k,l=1}^{m}_{i,j}_{k,l}_{A}(_{i,k},A_{j,l}). \]

In particular, with the proper choice of ground metrics \(_{f}\) and \(_{A}\), this is equivalent to the popular Graph Edit Distance (GED) . The minimization problem however is a Quadratic Assignment Problem (QAP) which is known to be one of the most difficult problems in the NP-Hard class . To mitigate this computational complexity, Aflalo et al.  suggested to replace the space of permutation matrices with a convex relaxation. The Birkhoff polytope (doubly stochastic matrices) \(_{m}=\{^{m m}_{m}=_{m},^{T}_{m}=_{m}\}\) is the tightest of those relaxations as it is exactly the convex hull of \(_{m}\) which makes it a suitable choice . Interestingly, the resulting metric is known in OT  field as a special case of the (Fused) Gromov-Wasserstein (FGW) distance proposed by .

\[(,g)=_{_{m}}_{i,j=1}^{m}_{i,j}_{F}(}_{i},_{j})+_{i,j,k,l=1}^{m}_{i,j}_{k,l}_{A}(_{i,k},A_{j,l}) \]

However, these two points of view differ in their interpretation of the FGW metric. From the GM perspective, FGW is cast as an approximation of the original problem, and the optimal transport plan is typically projected back to the space of permutation via Hungarian Matching . From the OT perspective, FGW is used as a metric between distributions with interesting topological properties . This raises the question of the tightness of the relaxation between GM and FGW. In the linear case, i.e., when \(_{A}=0\), the relaxation is tight and this phenomenon is known in the OT literature as the equivalence between Monge and Kantorovitch formulation . The quadratic case, however, is much more complex, and sufficient conditions under which the tightness holds have been studied in both fields .

As seen above, both OT and GM perspectives offer ways to characterize the same objects. In the remainder of this paper, we adopt the OT terminology, e.g., we use the term _transport plan_ in place of _doubly stochastic matrix_. We provide a quantitative analysis of the effect of the relaxation in F.2.

Numerical solverComputing the FGW distance requires solving the optimization problem presented in Equation (2) whose objective rewrites \(,+, \) where \(\) is a fixed matrix, \(\) a fixed tensor and \(\) the tensor matrix product. A standard way of solving this problem  is to use a conditional gradient (CG) algorithm which iteratively solves a linearization of the problem. Each step of the algorithm requires solving a linear OT/Matching problem of cost \(,^{(k)}\) where the linear cost \(^{(k)}=+^{(k)}\) is updated at each iteration. The linear problem can be solved with a Hungarian solver with cost \((M^{3})\) while the overall complexity of computing the tensor product \(^{(k)}\) is theoretically \((M^{4})\). Fortunately, this bottleneck can be avoided thanks to a \((M^{3})\) factorization proposed originally by Peyre et al. .

Comparing graphs of arbitrary sizeThe metrics defined above cannot directly be used to compare graphs of different sizes. To overcome this problem, Vayer et al.  proposed a more general formulation that fully leverages OT to model weights on graph nodes and can be used to compare graphs of different sizes as long as they have the same total mass. However, this approach raises specific issues. In scenarios where masses are uniform, nodes in larger graphs receive lower mass which might not be suitable for practical applications. Conversely, employing non-uniform masses complicates interpretation, as decoding a discrete object from a weighted one becomes less straightforward. Those issues can be mitigated by leveraging Unbalanced Optimal Transport (UOT) , which relaxes marginal constraints, allowing for different total masses in the graphs. Unfortunately, UOT introduces several additional regularization parameters that are difficult to tune, especially in scenarios like SGP, where model predictions exhibit wide variability during training.

Another close line of work is Partial Matching (PM) , which consists in matching a small graph \(g\) to a subgraph of the larger graph \(\). In practice, this can be done by adding dummy nodes to \(g\) through some padding operator \(\) after which one can directly compute \((,g)=(,(g))\). However, PM is not suited to train a model as the learned model would only be able to predict a graph that includes the target graph. Partial Matching and its relationship with our proposed loss is discussed in more detail in Appendix B.3.

## 3 Optimal Transport loss for Supervised Graph Prediction

A size-agnostic representation for graphsOur first step toward building an end-to-end SGP framework is to introduce a space \(}\) to represent any graph of size up to \(M\).

\[}=\{y=(,,) ^{M},^{M d},^{M M}\}. \]

We refer to the elements of \(}\) as continuous graphs, in opposition with discrete graphs of \(\). Here \(h_{i}\) (resp. \(A_{i,j}\)) should be interpreted as the probability of the existence of node \(i\) (resp. edge \([i,j]\) ). Any graph of \(\) can be embedded into \(}\) with a padding operator \(\) defined as

\[(g)=(_{m}\\ _{M-m},_{m}\\ _{M-m},_{m}&_{m,M-m} \\ _{M-m,m}&_{M-m,M-m}),g=(_{m}, _{m})_{m}. \]

We denote \(=()}\) the space of padded graphs. For any padded graph in \(\), the padding operator can be inverted to recover a discrete graph \(^{-1}:\). Besides, any continuous graph \(}\) can be projected back to padded graphs \(\) by a threshold operator \(:}\). Note that \(}\) is **convex** and of **fixed dimension** which makes it ideal for parametrization with a neural network. Hence, the core idea of our work is to use a neural network to make a prediction \(}\) and to compare it to a target \(g\) through some loss \((,(g))\). This calls for the design of an asymmetric loss \(:}_{+}\).

An Asymmetric loss for SGPThe Partially Masked Fused Gromov Wasserstein (PMFGW) is a loss between a padded target graph \((g)=(,,)\) with real size \(m=\|\|_{1} M\) and a continuous prediction \(=(},},})}\). We define \((,(g))\) as:

\[_{_{M}}}}{M}_{i,j}T_{i,j}_{h }(_{i},h_{j})+}}{m}_{i,j}T_{i,j}_{f}( {}_{i},_{j})h_{j}+}}{m^{2}}_{i, j,k,l}T_{i,j}T_{k,l}_{A}(_{i,k},A_{j,l})h_{j}h_{l}. \]

Let us decompose this loss function to understand the extend to which it simultaneously takes into account each property of the graph. The first term ensures that the padding of a node is well predicted. In particular, this requires the model to predict correctly the number of nodes in the target graph. The second term ensures that the features of all non-padding nodes (\(h_{i}=1\)) are well predicted. Similarly, the last term ensures that the pairwise relationships between non-padded nodes (\(h_{i}=h_{j}=1\)) are well predicted. The normalizations in front of the sums ensure that each term is a weighted average of its internal losses as \( T_{i,j}=M\), \( T_{i,j}h_{j}=m\) and \( T_{i,j}T_{k,l}h_{j}h_{l}=m^{2}\). Finally \(=[_{},_{},_{ }]_{3}\) is a triplet of hyperparameters on the simplex balancing the relative scale of the different terms. For \(_{A}\) and \(_{h}\) we use the cross-entropy between the predicted value after a sigmoid and the actual binary value in the target. This is equivalent to a logistic regression loss after the OT plan has matched the nodes. For \(_{f}\) we use the squared \(_{2}\) or the cross-entropy loss when the node features are continuous or discrete, respectively.

A key feature of this loss is its flexibility. Not only other ground losses can be considered but it is also straightforward to introduce richer spectral representations of the graph . For instance, in Section 5, we explore the benefits of leveraging a diffused version of the nodes features.

Finally, PMFGW translates all the good properties of FGW to the new size-agnostic representation.

**Proposition 1** (Complexity).: _The objective of the inner optimization can be evaluated in \((M^{3})\)._

**Proposition 2** (GI Invariance).: _If \(^{}\) and \(g g^{}\) then \((,(g))=(^{}, (g^{}))\)._

**Proposition 3** (Positivity).: \((,(g)) 0\) _with equality if and only if \((g)\)._

See Appendix A for a toy example illustrating the behavior of the loss and Appendix B.1 and B.2 for formal statements and proofs of Proposition 1, 2 and 3.

Relation to existing metricsPMFGW is an asymmetric extension of FGW  suited for comparing a continuous predicted graph with a padded target. The extension is achieved by adding (1) a novel term to quantify the prediction of node padding, and (2) the partial masking of the components of the second and third terms to reflect padding. It should be noted that in contrast to what is usually done in OT, the node masking vectors (\(\) and \(}\)) are not used as a marginal distribution but directly integrated into the loss. In that sense, the additional node masking term is very similar to the one of \(_{p}\) that proposed to use uniform marginal weight and move the part that measures the similarity between the distribution weights in an additional linear term. However, \(_{p}\) is restricted to linear OT problems and does not use the marginal distributions as a masking for other terms as in PMFGW.

PMFGW also relates to Partial GM/GW Chapel et al.  as both metrics compare graphs by padding the smallest one with zero-cost dummy nodes. The critical difference lies in the new vector \(}\) which predicts which sub-graphs are activated, i.e., should be matched to the target. The exact relationship between Partial Fused Gromov Wasserstein (PFGW) and PMFGW is summarized below

**Proposition 4**.: _If \(l_{h}\) is set to a constant value, PMFGW is equal to PFGW (up to a constant)._

_Remark 1_.: In that case, the vector \(}\) disappears from the loss and cannot be trained. In particular, this would prevent the model from learning to predict the size of the target graph.

The formal definition of PFGW and the proof of Proposition 4 are provided in Appendix B.3.

## 4 Any2Graph: a framework for end-to-end SGP

### Any2Graph problem formulation

The goal of Supervised Graph Prediction (SGP) is to learn a function \(f:\) using the training samples \(\{(x_{i},g_{i})\}_{i=1}^{n}()^{n}\). In Any2Graph, we relax the output space and learn a function \(:}\) that predicts a continuous graph \(:=f(x)\) as defined in the previous section. Assuming \(\) is a parametric model (in this work, a deep neural network) completely determined by a parameter \(\), the Any2Graph objective writes as the following empirical risk minimization problem:

\[_{}_{i=1}^{n}(_{}(x_{i }),(g_{i})). \]

At inference time, we recover a discrete prediction by a straightforward decoding \(f(x)=^{-1}()\), where \(\) is the thresholding operator with threshold \(}{{2}}\) on the edges and nodes and \(^{-1}\) is the inverse padding defined in the previous section. In other words, the full decoding pipeline \(^{-1}\) removes the nodes \(i\) (resp. edges \((i,j)\)) whose predicted probability is smaller than \(}{{2}}\) i.e. \(_{i}<}{{2}}\) (resp. \(_{i,j}<}{{2}}\))). Unlike surrogate regression methods, this decoding step is very efficient.

### Neural network architecture

The model \(_{}:}\) (left part of Figure 1) is composed of three modules, namely the **encoder** that extracts features from the input, the **transformer** that convert these features into \(M\) nodes embeddings, that are expected to capture both feature and structure information, and the **graph decoder** that predicts the properties of our output graph, i.e., \((},},})\). As we will discuss later, the proposed architecture draws heavily on that of Relationformer  since the latter has been shown to yield to state-of-the-art results on the Image2Graph task.

EncoderThe encoder extracts \(k\) feature vectors in \(^{d_{e}}\) from the input. Note that \(k\) is not fixed a priori and can depend on the input (for instance sequence length in case of text input). This is critical for encoding structures as complex as graphs and the subsequent transformer is particularly apt at treating this kind of representation. By properly designing the encoder, we can accommodate different types of input data. In Appendix C, we describe how to handle images, text, graphs, and vectors and provide general guidelines to address other input modalities.

TransformerThis module takes as input a set of feature vectors and outputs a fixed number of \(M\) node embeddings. This resembles the approach taken in machine translation, and we used an architecture based on a stack of transformer encoder-decoders, akin to Shit et al. .

Graph decoderThis module decodes a graph from the set of node embeddings \(=[_{1},,_{M}]^{T}\) using the following equation:

\[_{i}=(_{m}(_{i})),_{i}=_{f}(_{i}),_{i,j}=(_{s}^{2}( _{s}^{1}(_{i})+_{s}^{1}(_{j}))) \]

where \(\) is the sigmoid function and \(_{m}\), \(_{f}\), \(_{s}^{k}\) are multi-layer perceptrons heads corresponding to each component of the graph (mask, features, structure). The adjacency matrix is expected to be symmetric which motivate us to parameterize it as suggested by Zaheer et al. .

Positioning with RelationformerAs discussed above, the architecture is similar to the one proposed in Relationformer , with two modifications: (1) we use a symmetric operation with a sum to compute the adjacency matrix while Relationformer uses a concatenation that is not symmetric; (2) we investigate more general encoders to enable graph prediction from data other than images. However, as stated in the previous section, the main originality of our framework lies in the design of the PMFGW loss. Interestingly Relationformer uses a loss that presents similarities with FGW but where the matching is done on the node features only, before computing a quadratic-linear loss similar to PMFGW. In other words, they solve a bi-level optimization problem, where the plan is computed on only part of the information, leading to potentially suboptimal results on heterophilic graphs as demonstrated in the next section.

Figure 1: Illustration of the architecture for a target graph of size 3 and \(M=4\).

Numerical experiments

This section is organized as follows. First, we describe the experimental setting (5.1) including baselines. Next, we showcase the state-of-the-art performances of Any2Graph for a wide range of metrics and datasets (5.2). Finally, we provide an empirical analysis of the key hyperparameters of Any2Graph (5.3). The code for Any2Graph and all the experiments will be released on GitHub.

### Experimental setting

DatasetsWe consider 5 datasets thus covering a wide spectrum of different input modalities, graph types, and sizes. The first one, _Coloring_, is a new synthetic dataset that we proposed, inspired by the four-color theorem. The input is a noisy image partitioned into regions of colors and the goal is to predict the graph representing the regions as nodes (4 color classes) and their connectivity in the image. An example is provided in Figure 5 and more details are in Appendix D. Then, we consider four real-world benchmarks. _Toulouse_ is Sat2Graph datasets where the goal is to extract the road network from binarized satellite images of a city. _USCities_ is also a Sat2Graph dataset but features larger and more convoluted graphs. Note that we leave aside the more complex RGB version of _USCities_ as it was shown to require complex multi-level attention architecture , which is beyond the scope of this paper. Finally, following Ucak et al. , we address the Fingerprint2Graph task where the goal is to reconstruct a molecule from its fingerprint representation (list of tokens). We consider two widely different datasets for this tasks: _QM9_, a scarce dataset of tiny molecules (up to 9 nodes) and _GBD13_Blum and Reymond , a large dataset 2 featuring molecules with up to 13 heavy atoms. Additional details concerning the datasets (e.g. dataset size, number of edges, number of nodes) are provided in Appendix E.1.

Compared methodsWe compare Any2Graph, to our direct end-to-end competitor Relationformer  that has shown to be the state-of-the-art method for Image2Graph. For a fair comparison, we use the same architecture (presented in Figure 1) for both approaches so that the only difference is the loss. We conjecture that Any2Graph and Relationformer might benefit from feature diffusion, that is replacing the node feature vector \(\) by the concatenation \([,]\) before training. We denote by "+FD" the addition of feature diffusion before training. Moreover, we also compare with a surrogate regression approach (FGW-Bary) based on FGW barycenters . We test both the end-to-end parametric variant, FGWBary-NN, where weight functions \(_{k}\), as well as \(K=10\) templates, are learned by a neural network and the non-parametric variant, FGWBary-ILE, where the templates are training samples and \(_{k}\) are learned by sketched kernel ridge regression [54; 18] with gaussian kernel. Both have been implemented using the codes provided by Brogat-Motte et al. , modified to incorporate sketching. Hyperparameters regarding architectures and optimization are provided in Appendix E.2.

Performance metricsThe heterogeneity of our datasets, calls for task-agnostic metrics focusing on different fine-grained levels of the graph. At the graph level, we report the PMFGW loss between continuous prediction \(\) and padded target \((g)\) and the graph edit distance Gao et al.  between predicted graph \(^{-1}\) and target \(g\). We also report the Graph Isomorphism Accuracy (GI Acc), a metric that computes the proportion of perfectly predicted graphs. At the edge level, we treat the adjacency matrix prediction as a binary prediction problem and report the Precision and Recall. Finally, at the node level, we report node, the fraction of well-predicted node features, and size a metric reporting the accuracy of the predicted number of nodes. See Appendix E.3 for more details.

### Comparison with existing SGP methods on diverse modalities

Prediction PerformancesTable 1 shows the performances of the different methods on the five datasets. First, we observe that Any2Graph achieves state-of-the-art performances for all datasets and graph level metrics. On the Sat2Graph tasks (_Toulouse_ and _USCities_) we note that Relationformer performs very close to Any2Graph. In fact, the features (2D positions) are enough to uniquely identify the nodes, making Relationformer's Hungarian matching sufficient. Moreover, both methods highly benefit from feature diffusion on Fingerprint2Graph tasks which we discuss further in Appendix F. Note that both barycenter methods struggle on _Toulouse_, possibly due to a lack of expressivity. On

[MISSING_PAGE_FAIL:8]

terms of best edit distance) and illustrate the flexibility of _Coloring_ by bridging this complexity gap with the more challenging variation described in D.

Scalability to larger graphsAs stated in property 1, each iteration of the PMFGW solver scales with \((M^{3})\). Denoting \(k(M)\) the average number of iterations required for convergence, this means that the actual cost of computing the loss scales with \((k(M)M^{3})\). We provide an empirical estimation of \(k(M)\) in figure 2 which we obtain by computing PMFGW(\((g_{1}),(g_{2})\)) for pairs of graphs \(g_{1},g_{2}\) sampled from the _Coloring_ dataset. We observe that \(k(M)\) seems linear but can be made sub-linear using feature diffusion (FD). Still, the cubic cost prevents Any2Graph from scaling beyond a few tens of nodes.

Choice of maximal graph size \(M\)The default value of \(M\) is the size of the largest graph in the train set. We explore whether or not overparametrizing the model with higher values bring substantial benefits. To this end, we train our model on _Coloring_ for \(M\) between \(10\) (default value) and \(25\) and report the (test) edit distance. To quantify the effective number of nodes used by the model, we also record the number of active nodes, i.e., that are masked less than 99% of the time (see Figure 3). Interestingly, we observe that performances are robust w.r.t. the choice of \(M\) which can be explained by the number of useful nodes reaching a plateau. This suggests the model automatically learns the number of nodes it needs to achieve the required expressiveness.

Sensitivity to the weights \(\)We investigate the sensitivity of the proposed loss to the triplet of weight hyperparameters \(\). To this end, we train our model on _Coloring_ for different values \(\) on the simplex and report the (test) edit distance on Figure 4. We observe that the performance is optimal for uniform \(\) and robust to other choices as long as there is not too much weight on the structure loss term (corner \(_{}=1\)). Indeed, the quadratic term of the loss being the hardest, putting too much emphasis on it might slow down the training. This explains the efficiency of feature diffusion, as it moves parts of the structure prediction to the linear term. Further evidence backing this intuitive explanation is provided in F.1.

Figure 5: (Left): a sample of predictions made by Any2Graph and Relationformer for a given input and ground truth target. Many more are provided in G. (Right): we truncate the train datasets to provide an overview of Any2Graph training curves (test performances against train set size).

Figure 3: Effect of \(M\) on test edit distance and number of active nodes for _Coloring_.

Figure 2: Average number of solver iterations required for computing PMFGW loss.

Conclusion and limitations

We present Any2Graph, a novel deep learning approach to Supervised Graph Prediction (SGP) leveraging an original asymmetric Partially-Masked Fused Gromov-Wasserstein loss. To the best of our knowledge, Any2Graph stands as the first end-to-end and versatile framework to consistently achieve state-of-the-art performances across a wide range of graph prediction tasks and input modalities. Notably, we obtain excellent results in both accuracy and computational efficiency. Finally, we illustrate the adaptability of the proposed loss (using diffused features), and the robustness of the method to sensitive parameters such as maximum graph size and weights of the different terms in PMFGW.

The main limitation of Any2Graph is its scalability to graphs of larger size. Considering the tasks at hand, in this paper, we limit ourselves to relatively small graphs of up to 20 nodes.

For the future, we envision two working directions to address this issue. First, given the promising results with feature diffusion, we plan to introduce more tools from spectral graph theory [14; 20; 4] in Any2Graph, e.g., using diffusion on the adjacency matrix to capture higher-order interactions that may occur in large graphs. More generally, this extension could be useful even for smaller graphs. Secondly, we expect that the solver computing the optimal transport plan can be accelerated using approximation. In particular, the entropic regularization  might unlock the possibility of fully parallelizing the optimization on a GPU while low-rank OT solvers Scetbon et al.  could allow Any2Graph to scale to large output graphs.