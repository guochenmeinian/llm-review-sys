# The Monte Carlo Method and New Device and Architectural Techniques for Accelerating It

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Computing systems interacting with real-world processes must safely and reliably process uncertain data. The Monte Carlo method is a popular approach for computing with such uncertain values. This article introduces a framework for describing the Monte Carlo method and highlights two advances in the domain of physics-based non-uniform random variate generators (PPRVGs) to overcome common limitations of traditional Monte Carlo sampling. This article also highlights recent advances in architectural techniques that eliminate the need to use the Monte Carlo method by leveraging distributional microarchitectural state to natively compute on probability distributions. Unlike Monte Carlo methods, uncertainty-tracking processor architectures can be said to be _convergence-oblivious_.

## 1 Introduction

Uncertainty arises when systems carry out computations based on _measurements_ (aleatoric uncertainty) or _limited knowledge_ (epistemic uncertainty). Uncertainty introduces risk to actions taken based on measurements or limited knowledge. Studying and quantifying how uncertainty propagates through computations is a requirement when making principled decisions about the suitability of an uncertain system for an application.

Despite the importance of quantifying and understanding uncertainty, computer architectures and circuit implementations lack numerically-robust and computationally-efficient methods to programmatically process and reason about uncertainty. State-of-the-art techniques often employ the Monte Carlo method [1; 2; 3; 4] to estimate the effect of long sequences of arithmetic operations on inputs that are uncertain when closed-form propagation of uncertainty is not possible. Monte-Carlo-based methods can be sample-inefficient: the variance in the result of Monte Carlo integration using \(n\) samples scales as \(}\). This means that if we wanted to halve the variance, we would need to _quadruple_ the number of samples.

This article presents a framework for describing Monte-Carlo-based methods (Section 2). The framework poses them as the application of three steps: sampling, evaluation, and post-processing. In Section 3 we describe recent advances in physics-based programmable non-uniform random variate generators (PPRVGs) which can improve the sampling phase of Monte Carlo methods. Section 4 shows how a novel uncertainty-tracking microarchitecture, Laplace [5; 6], can provide a more efficient way to represent and compute on uncertain variables. Section 6 compares the performance of Laplace to the traditional Monte Carlo method.

The Monte Carlo Method

The phrase _Monte Carlo method_ refers to a wide class of computational methods that sample from random variables to calculate solutions to computational problems. The earliest example of the use of a Monte Carlo method is attributed to Georges-Louis LeClerc , Comte de Buffon, who, in the eighteenth century, simulated a value of \(\) by dropping needles onto a lined background. He showed that when the needle has the same length as the distance between parallel lines, the probability that a randomly-thrown needle will overlap with a line is \(\). Therefore, \(\) can be estimated by throwing a large number of needles and averaging the number of times they overlap with a line.

### The Monte Carlo Method: Sampling, Evaluation, and Post-Processing

The Monte Carlo method approximates a desired numerical property of the outcome of transformations of random variables. Practitioners use the Monte Carlo method when the desired property is not available analytically or because the analytical solution is computationally expensive. The desired property could be the expectation of the resulting random variable (_Monte Carlo integration_), a sample from it (_Monte Carlo sampling_), or its probability density function (_Monte Carlo simulation_).

Suppose that we want to obtain a property from the random variable \(Y\) that is defined by transforming the random variable \(X\) using the transformation \(f:X Y\) (i.e., \(Y=f(X)\)). We summarize the steps of the Monte Carlo method to approximate the desired numerical properties as follows:

1. **Sampling:** The Monte Carlo method first generates i.i.d. samples from \(X\). Let \(n\) denote the number of samples of the random variable \(X\) in the set \(\{x_{i}\}_{i=1}^{n}\). This step typically uses a random number generator program running on a computer that can generate pseudo-random numbers from a uniform distribution. Samples from more complex random variables are generated using _Monte Carlo sampling_, where the Monte Carlo method itself is used to generate samples by transforming the uniform random variates. Examples of Monte Carlo sampling include the Box-Muller method  for generating standard Gaussian samples, inverse transform sampling for sampling from random variables for which an inverse cumulative distribution function (ICDF) exists1, and Markov Chain Monte Carlo (MCMC) for more complex random variables . As an alternative to Monte Carlo sampling, we can use physical hardware to efficiently sample from a non-uniform random variable. Section 3 presents several such methods from the research literature which can provide large-batch single-shot convergence-oblivious random variate generation by exploiting physical processes that generate _non-uniform_ entropy and can be sampled in parallel. 2. **Evaluation:** The second step of the Monte Carlo method then evaluates the transformation \(f\) on the set of samples \(\{x_{i}\}_{i=1}^{n}\) to obtain a set \(\{y_{i}\}_{i=1}^{n}\) of \(n\) samples of \(Y\), where each \(y_{i}=f(x_{i})\). This step is called _Monte Carlo evaluation_. In the Monte Carlo method, the evaluation step is carried out on each sample \(x_{i}\), one at a time. Section 4 presents recent research on computer architectures that can process compact representations of entire distributions at once, rather than one sample at a time as is the case for the traditional Monte Carlo method.
3. **Post-processing:** In the third and final step, the Monte Carlo method approximates the desired numerical property from the samples \(\{y_{i}\}_{i=1}^{n}\) by applying an operation on their set. For example, taking their average (as in the case of Monte Carlo integration), applying the identity function (as in Monte Carlo sampling), or generating a representation of the probability density function, such as a histogram (as in Monte Carlo simulation).

## 3 Physics-Based Programmable Non-Uniform Random Variate Generation

Section 2 described the sampling of (possibly non-uniform) random variables as the first step of the Monte Carlo method. Most computing systems use pseudo-random number generators to generate uniform random variates. Computers generate samples from non-uniform random variables by using Monte Carlo sampling (Section 2). Since Monte Carlo methods could require large numbers of samples, these methods can be computationally-expensive and can lead to a significant overhead.

Two recent methods of generating non-uniform random variates from physical processes, Spot  and Grappa  have the following key features:

* They can _efficiently_ generate _non-uniform_ random variates: Spot, for example, can generate Gaussian random variables \(260\) faster than the Box-Muller transformation running on an ARM Cortex-M0+ microcontroller, while dissipating less power than such a microcontroller.
* They are _physics-based_: Spot generates random variates using electron tunneling noise, while Grappa exploits the transfer characteristics of Graphene field-effect transistors.
* They are programmable: The distributions from which they can sample from are not fixed; their host systems can dynamically and digitally configure them to produce samples from a required probability distribution.

Due to these features, we call methods such as Spot and Grappa physics-based programmable non-uniform random variate generators (PPRVGs).

Spot:Spot is a method for generating random numbers by sampling a one-dimensional distribution associated with a Gaussian voltage noise source . Using an analog-to-digital converter (ADC), Spot takes measurements of a physical process that generates Gaussian noise. Spot then maps this physically-generated univariate Gaussian to any other univariate Gaussian using only two operations: a multiplication and an addition . Samples from any other non-uniform random variable are generated by creating a mixture of Gaussians.

Grappa:Grappa is a Graphene Field-Effect Transistor (GFET)-based programmable analog function approximation architecture . Grappa relies on the non-linear transfer characteristics of GFETs to transform a uniform random sample into a non-uniform random sample .

Grappa implements a linear least-squares Galerkin approximation  to approximate the ICDF of a target distribution and carry out inverse transform sampling. The required orthonormal basis functions are obtained from the GFET transfer characteristics using the Gram-Schmidt process .

Tye _et al_. showed that Monte Carlo integration using samples generated by Grappa is at least 1.26x faster than using a C++ lognormal random number generator. Subsequent work  demonstrated an average speedup of up to 2x compared to MATLAB for lognormal, exponential, generalized Pareto, and Gaussian mixture distributions, with the execution time independent of the target distribution.

## 4 Beyond the Monte Carlo Method

Let \(X\) be a random variable and \(f:X Y\) be a transformation of \(X\). Denoting the resulting random variable as \(Y=f(X)\), from the change of variable formula for random variables (Theorem 1 in Appendix A), we obtain probability density function \(p_{Y}\) of \(Y\). If \(p_{X}\) is the probability density function of \(X\), then the probability density function of \(p_{Y}\) of \(Y\) is,

\[p_{Y}(y)=p_{X} f^{-1}(y)| f^{-1}(y)|,\] (1)

where \(y Y\), and \(_{y}f^{-1}\) is the Jacobian matrix. Using the change of variables technique of integrals, we obtain

\[_{p_{X}}[f(X)] =_{X}f(x)p_{X}(x)\;x\] by Equation 7 in Appendix A \[=_{Y}yp_{Y} f^{-1}(y)| f^{-1}(y)|\;y\] by change of variables (integration) \[=_{Y}yp_{Y}(y)\;y\] by Theorem 1 in Appendix A \[=_{p_{Y}}[Y].\]

Thus, if we had access to \(p_{Y}\) of \(Y\), we can evaluate \(_{p_{X}}[f(X)]\) by taking the expectation of the random variable \(Y\) with respect to the \(p_{Y}\). When \(p_{Y}\) isn't directly accessible, we usually obtain the expectation of \(Y\) by using Monte Carlo integration. However, having access to \(p_{Y}\) would eliminate the need to use the Monte Carlo method completely.

Laplace [5; 6] is a computer microarchitecture that is capable of directly computing \(p_{Y}\) by representing distributional information in its microarchitectural state and tracking how these distributions evolve under arithmetic operations, transparently to the applications running on it. Laplace provides a representation for the distribution (see Definition 3 in Appendix A) of random variables, and carries out _deterministic computations on this distribution_.

Laplace's in-processor distribution representation has an associated _representation size_ that describes the _precision_ at which the probability distribution is represented. Higher values of the representation size result in a more accurate representation. A useful analogy is the IEEE-754 standard for representing the uncountable infinite set of real numbers as floating-point numbers [15; 16] on a finite-precision computer.

Computer architectures such as Laplace eliminate the need for using the Monte Carlo method and can therefore have far-reaching consequences in areas where the Monte Carlo method is used. For example, to approximate the predictive Gaussian Process posterior distribution with an uncertain input, Deisenroth _et al_[17; 18] used moment-matching; Laplace could compute the posterior exactly, up to the precision of the representation.

## 5 Methods

The remaining text compares and evaluates _Monte Carlo methods_ and _Laplace-based methods_. Both methods were evaluated on single-threaded applications written in the C programming language.

Monte Carlo method:We use the standard Monte Carlo method that we described in Section 2. We use the pseudo-random number generator _rand_ from the Standard C Library  to sample from uniform distributions and use the modified Box-Muller method  as implemented by the gsl_ran_gaussian_ziggwrat function in the GNU Scientific Library . We compile our code using clang, the C family front-end to LLVM , with optimization set to \( 03\)2.

Laplace:We use Laplace as a replacement for the Monte Carlo method, as described in Section 4. In our experiments, we exclusively use Laplace's Telescope Torques Representation (TTR)  as provided by a commercial implementation of Laplace , release 2.6.

We compare these methods by empirically measuring and reporting the average _run time_ and the average _Wasserstein distance  of the output to a ground truth_ in two different applications of Monte Carlo simulation. We change the number of samples (for Monte-Carlo-based methods), or the representation size (for Laplace-based methods) to observe the trade-offs between accuracy and run time. For each configuration of number of samples or representation size, we repeat the experiments 30 times to account for variation in the process of sampling3. See Appendix C for more detail on our methods. Figure 1 summarizes our results.

### Applications

We carry out the experiments described above on two applications of Monte Carlo simulation.

Monte Carlo Convergence Challenge Example:Let \(X^{}\) be the initial random variable that we sample from, with its probability density function \(p_{X^{}}\) being a Gaussian mixture. given by:

\[p_{X^{}}(x)=0.6(}e^{-2(x-2)^{2}}) +0.4(}e^{}{2}}).\] (2)

For the Monte Carlo evaluation step of Section 2, we define a function \(f^{}\) as a sigmoidal function:

\[f^{}(x)=}.\] (3)For the Traditional Monte Carlo method, we evaluated on \(n\{4,256,1152,2048,4096,8192,\)\(16000,32000,128000,256000\}\). For Laplace, we evaluated on \(r\{16,32,64,256,2048\}\).

Poiseuille's Law for Blood TransfusionAs a real-world application, we use Poiseuille's Law, a mathematical model from fluid dynamics used to calculate the rate of laminar flow, \(Q\), of a viscous fluid through a pipe of constant radius . This model is used in medicine as a simple method for approximating the rate of flow of fluids, such as blood, during transfusion . We look at Poiseuille's Law applied to the case of blood transfusion using a pump with the following parameters:

* Pressure difference created by the pump, where \( P(5500000\,,36000^{2})\).
* Viscosity of the fluid, where \((3.88\,,4.12\,)\).
* Length of the tube from the cannula to the pump, where \(l(6.95\,,7.05\,)\).
* Radius of the cannula, where \(r(0.0845\,,0.0855\,)\).

We assume the cannula to have a gauge of 14 (a radius of \(0.85\,\)) and the viscosity of blood to be \(4\,\).  reported that for porcine blood, the uncertainty of using a ventricular assist device to measure blood viscosity in real time was \( 0.12\,\); we use this as the uncertainty of the viscosity.

The flow rate \(Q\) is therefore measured in \(^{3}/\). Using these parameters, we can calculate the flow rate using Poiseuille's Law:

\[Q= P}{8 l}.\] (4)

For the Traditional Monte Carlo method, we evaluated on \(n\{4,256,1152,4096,8192,32000,\)\(128000,256000,512000,640000\}\). For Laplace, we evaluated on \(r\{16,32,64,128,256,2048\}\).

## 6 Results

Figure 1 shows Pareto plots of the mean run time against the Wasserstein distance from the ground truth for both applications. A key observation is that the variance of the Laplace-based methods is more or less constant as we increase the representation size. Laplace carries out _deterministic computations on probability distributions_; this variance is caused by using a finite number of samples from Laplace's representation of the output distribution to calculate the Wasserstein distance. It is possible to calculate the Wasserstein distance directly from the Laplace processor representation but we did not do so at the time of writing. This calculation would be deterministic since it only depends on the representation of the distribution. In contrast, each run of the Monte Carlo method results in a different output distribution; to reduce this variance we need to increase the number of samples. In this way, Laplace is _convergence-oblivious_ to the number of samples.

Increasing the representation size larger than \(r=32\) provides a worse trade-off with the run time for both applications. Table 1 shows that for the accuracy obtained by Laplace, the equivalent Monte Carlo simulation is \(113.85\) (for the Monte Carlo Convergence Challenge example) and \(51.53\) (for the Poiseuille's Law for Blood Transfusion application) slower. If much better accuracy is required, then the Monte Carlo method will need to be used. However, if the accuracy provided by Laplace is sufficient, it provides a potentially-orders-of-magnitude-faster alternative that is also _consistent outputs across repetitions_.

Tables with the numerical results are in Appendix F. Appendix F also compares histograms of the resulting distributions and provides additional discussion.

## 7 Conclusions

The Monte Carlo method is a powerful and historically-significant tool for solving complex problems that might otherwise be intractable. It involves three simple steps: sampling, evaluating and post-processing. Despite its versatility, the Monte Carlo method can suffer from inefficiencies. One of these is that generating samples for the first step of the Monte Carlo method is inefficient when samples are required from non-uniform probability distributions. Recent advances in physics-based random number generators, namely Spot  and Grappa  address these challenges.

Techniques such as Laplace [5; 6] represent probability distributions in a computing system using an approximate fixed-bit-width representation in a manner analogous to how traditional computer architectures approximately represent real-valued numbers using fixed-bit-width representations such as the IEEE-754 floating-point [15; 16] representation. The computations of Laplace are approximations of explicit Monte Carlo methods in much the same way that computations on floating-point are approximations of arithmetic on real numbers. Laplace does not require iterative and repeated processing of samples until convergence to a target distribution is achieved, nor does it suffer from the high variance observed across Monte Carlo runs. Methods like Laplace are therefore _convergence-oblivious_.