# Instruction-tuned LLMs with World Knowledge are More Aligned to the Human Brain

Khai Loong Aw, Syrielle Montariol, Badr AlKhamissi, Martin Schrimpf, Antoine Bosselut 

EPFL

{khai.aw,syrielle.montariol,badr.alkhamissi,martin.schrimpf,antoine.bosselut}@epfl.ch

Equal contributionEqual supervision / senior authors

###### Abstract

Instruction-tuning is a widely adopted method of finetuning that enables large language models (LLMs) to generate output that more closely resembles human responses to natural language queries, in many cases leading to human-level performance on diverse testbeds. However, it remains unclear whether instruction-tuning truly makes LLMs more similar to how humans process language. We investigate the effect of instruction-tuning on _brain alignment_, the similarity of LLM internal representations to neural activity in the human language system. We assess 25 vanilla and instruction-tuned LLMs across three datasets involving humans reading naturalistic stories and sentences, and discover that instruction-tuning generally enhances brain alignment by an average of \(6\%\). To identify the factors underlying LLM-brain alignment, we compute the correlation between the brain alignment of LLMs and various model properties, such as model size, performance ability on problem-solving benchmarks, and ability on benchmarks requiring world knowledge spanning various domains. Notably, we find a strong positive correlation between brain alignment and model size (r = 0.95), as well as performance on tasks requiring world knowledge (r = 0.81). Our results demonstrate that instruction-tuning LLMs improves both world knowledge representations and human brain alignment, suggesting that mechanisms that encode world knowledge in LLMs also improve representational alignment to the human brain.

## 1 Introduction

Instruction-tuning is a widely adopted method for finetuning large language models (LLMs) on datasets containing task-specific instructions. This approach enhances their ability to generalize effectively to previously unseen tasks by learning to follow provided instructions . Instruction-tuning often costs only a small fraction of compute relative to pretraining , yet propels pretrained LLMs to incredible performance leaps on reasoning and problem-solving benchmarks. This transformation has enabled LLMs to approach human performance on many tasks, despite using only few (or zero) training examples, and tackle open-world reasoning tasks previously only achievable by humans .

In addition to teaching LLMs to understand and follow human instructions, instruction-tuning also improves the ability of LLMs to mimic the ground-truth outputs (often human-written) of the training data. This property allows them to produce more controllable and predictable output that is deemed (1) more desirable by human evaluators on various metrics [27; 5; 24], (2) more aligned to human values , and (3) more stylistically similar to human outputs [6; 15].

Consequently, instruction-tuning yields LLMs that are more similar to humans in both capability and output similarity. From a neuroscience perspective, these observations beg the question: **Does**

**instruction-tuning make LLMs more similar to the human language system?** Previous work has shown that models with high performance on next-word prediction tasks are well-aligned to the human language system [16; 8; 3], and, on some datasets, even hit the estimated noise ceiling.3 However, there has been no similar study on how instruction-tuning, the training method that enabled powerful LLMs such as ChatGPT, affects alignment to the human language system.

In this work, we explore the impact of instruction-tuning on _brain alignment_, how closely LLMs' internal representations match neural activity in the human language system. Both LLMs and human participants are presented with the same language stimuli comprised of naturalistic stories and sentences. For LLMs, we analyze their internal representations, while for humans, we use previously collected brain activity data from functional magnetic resonance imaging (fMRI) experiments.

To measure brain alignment, we use the Brain-Score  linear predictivity metric, assessing how well LLM representations predict human brain activity in response to the same language stimuli [10; 21; 16; 11], using three neural datasets: Pereira2018 , Blank2014 , and Wehbe2014 . As models vary in brain alignment across different architectures and training objectives , we estimate the effect of instruction-tuning across 17 instruction-tuned and 8 vanilla LLMs, and report a significant increase in brain alignment by instruction-tuned LLMs compared to vanilla ones.

To investigate _why_ instruction-tuning increases alignment to human brain activity, we then estimate the contribution of various LLM properties towards brain alignment. Specifically, we compute Pearson correlations between an LLM's brain alignment and its properties, including next-word prediction (NWP) ability, model size, a range of problem-solving abilities, and world knowledge spanning different domains. The latter two properties are evaluated with the Big-Bench Hard benchmark (BBH)  and the Massive Multi-task Language Understanding benchmark (MMLU) , respectively.

We report two major findings:

1. Instruction-tuning generally improves the alignment of LLM representations to brain activity, increasing by \(6.2\)% on average for the LLMs and neural datasets we tested (Figure 1).
2. Investigating the factors underlying LLM-brain alignment, we find that world knowledge and model size are strongly correlated with brain alignment (r = \(0.81\) and \(0.95\) for instruction-tuned models, respectively; Figure 2).

## 2 Language Models

We evaluate the brain alignment of 25 large language models (LLMs) from two model families: T5  and LLaMa . T5 models are encoder-decoder LLMs pre-trained on the Colossal Common Crawl Corpus (C4), a corpus of 356 billion tokens, using a masked infilling objective, and then further finetuned on multi-task mixture of unsupervised and supervised tasks converted into a text-to-text format. We use all five T5 models with sizes between 77M to 11B parameters. LLaMA models  are decoder-only LLMs trained on 1.6 trillion tokens from a mixture of corpora including C4, English CommonCrawl, Wikipedia, Github, and more. For LLaMA, we use the 7B, 13B, and 33B parameter versions in our study.

## 3 Brain Alignment

Brain alignment refers to the method of evaluating the representational similarity between LLMs and human brain activity (Figure 1). This assessment relies on fMRI recordings obtained from human subjects while they read specific language stimuli on potentially any topic [here: 12; 2; 26]. In brain alignment studies, these same language stimuli from prior brain recordings are provided as input to LLMs, whose intermediate layer activations are recorded to extract model representations of the language stimuli. To study the alignment of LLM and human data, we follow a general approach previously used in several works [17; 16; 10; 21; 11; 1]. Specifically, we use the linear predictivity metric implemented in Brain-Score [18, Figure 1], first training a linear function to predict fMRI voxels associated with the human language system using LLM representations as input features. We then apply this linear function to held-out brain activity data from the original corpus of recordings,and evaluate the brain alignment of the LLM as the Pearson correlation between the predicted and actual brain activity data.

**Datasets** We use three fMRI datasets to measure the brain alignment of LLMs. Each neural dataset includes the brain activity of a different set of human participants, and uses a different set of language stimuli involving naturalistic stories and sentences. (1) Pereira2018 (experiments 2 and 3 from 12): In experiment 2, nine participants read 384 sentences organized into 96 text passages. In experiment 3, six participants read 243 sentences in 72 text passages. Each sentence was displayed for four seconds on a screen. (2) Blank2014 : The data consists of fMRI recordings of 5 human participants listening to naturalistic stories from the Natural Stories Corpus . Participants listened to stories presented auditorily. (3) Webbe2014 : The data includes fMRI recordings of 8 human participants reading chapter 9 of the book _Harry Potter and the Sorceror's Stone_. Participants read the chapter at a fixed interval of one word every 0.5 seconds.

**Finding 1: Instruction-tuning aligns LLM representations to human brain activity** First, we study the effect of instruction-tuning on brain alignment of LLMs. We compute each LLM's average brain alignment as the mean of its brain alignment scores on the 3 neural datasets and find that instruction-tuning improves alignment by an average of 6.2% across all tested LLMs (Figure 1B). We elaborate additional findings in Figure 1.

Figure 1: **Instruction-tuning aligns LLM representations to human brain activity.****(A)** Method of brain alignment. **(B)** Instruction-tuning improves average brain alignment by 6.2%. We compute each LLM’s average brain alignment using the mean of its brain alignment on the 3 neural datasets. Then, we compare the brain alignment of each instruction-tuned LLM against its vanilla counterpart. Each point above the identity line represents an instruction-tuned LLM that has greater brain alignment than its vanilla counterpart. Error bars (here and elsewhere) represent median absolute deviation over human participants. **(C)** Instruction-tuning generally improves brain alignment on all three neural datasets. **(D)** We instruction-tune LLaMA-7B using the Alpaca dataset. We also train an ablation model with the same process and training data, but remove the instruction portion from each training sample. This experiment demonstrates that improvements in brain alignment from instruction-tuning are due to both (1) training data (present in both models) and (2) the process of training LLMs to understand and follow instructions (present only in original model).

**Finding 2: World Knowledge and Model Size are key factors underlying LLM-brain alignment** To identify factors underlying the representational similarity between LLMs and human brains, we compute the Pearson correlation between LLM brain alignment and various properties of LLMs: performance on benchmarks involving different reasoning abilities (BBH benchmark; 19), performance on benchmarks requiring domain-specific world knowledge (MMLU; 9), language modeling ability, and model size. We elaborate additional findings in Figure 2 and Table 1. Finally, we provide our full results, as well as details about our models and code repositories, in the Appendix.

## 4 Conclusions

We investigate whether instruction-tuning improves the alignment of LLMs to the human language system. We evaluate 25 LLMs with parameter sizes ranging from 77 million to 33 billion, across three neural datasets of humans reading naturalistic stories and sentences, and find that instruction-tuning generally improves the alignment of LLM representations to brain activity. Delving into the factors underlying LLM-brain alignment, we discover that world knowledge and model size are

  
**Task category** &  **Brain Alignment** \\ **Correlation** (_r_) \\  &  **corrected** \\ _p-value_ \\  &  **Number** \\ **of tasks** \\  & 
 **Average Model** \\ **Performance** \\  \\   MMLU – Overall Score & **0.809** & **0.000329** & 57 & 0.36 \\ MMLU – STEM & **0.792** & **0.000343** & 18 & 0.28 \\ MMLU – Humanities & **0.791** & **0.000343** & 13 & 0.34 \\ MMLU – Social Sciences & **0.807** & **0.000329** & 12 & 0.41 \\ MMLU – Others & **0.809** & **0.000329** & 14 & 0.40 \\  BBH – Overall score & 0.384 & 0.177 & 23 & 0.28 \\ BBH – Algorithmic reasoning & 0.194 & 0.558 & 8 & 0.22 \\ BBH – Language understanding & 0.163 & 0.585 & 3 & 0.43 \\ BBH – World knowledge & **0.679** & **0.005** & 5 & 0.36 \\ BBH – Multilingual reasoning & -0.035 & 0.895 & 1 & 0.19 \\ BBH – Others & 0.478 & 0.083 & 6 & 0.27 \\   

Table 1: **Brain alignment strongly correlates with world knowledge across all subject domains in MMLU, and the world knowledge problem-solving category in BBH. At the same time, brain alignment is not significantly correlated with all other types of problem-solving abilities in BBH (e.g., algorithmic or multilingual reasoning). We obtain the p-value after false discovery correction.**

Figure 2: **World knowledge and model size are important factors underlying LLM-brain alignment.** Insets display results on individual datasets with stars reflecting statistical significance (n.s. = p \(>\) 0.05, * = p \(<\) 0.05, ** = p \(<\) 0.005, etc.) **(A)** Brain alignment is significantly and strongly correlated with world knowledge as evaluated by the MMLU Overall score (r = 0.81). This score reports the mean performance across all world knowledge subject domains on MMLU. **(B)** Brain alignment is significantly and strongly correlated with the world knowledge category on the BBH benchmark (r = 0.68). This score reports the mean performance on tasks included in the BBH world knowledge category. **(C)** Brain alignment is significantly and strongly correlated with model size (logarithm of the number of model parameters) (r = 0.95).

key determinants of brain alignment. This correlation suggests that world knowledge helps shape representations in the human language system, and highlights the significance of integrating world knowledge in the development of future LLMs.