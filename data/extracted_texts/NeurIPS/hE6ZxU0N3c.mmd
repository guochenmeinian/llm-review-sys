# Understanding Multi-Granularity

for Open-Vocabulary Part Segmentation

Jiho Choi\({}^{1}\)1

Seonho Lee\({}^{1}\)1

Seungho Lee\({}^{2}\)

Minhyun Lee\({}^{2}\)

Hyunjung Shim\({}^{1}\)2

\({}^{1}\)Graduate School of Artificial Intelligence, KAIST, Republic of Korea

\({}^{2}\)School of Integrated Technology, Yonsei University, Republic of Korea

{jihochoi, glanceyes, kateshim}@kaist.ac.kr, {seungholee, lln315}@yonsei.ac.kr

###### Abstract

Open-vocabulary part segmentation (OVPS) is an emerging research area focused on segmenting fine-grained entities using diverse and previously unseen vocabularies. Our study highlights the inherent complexities of part segmentation due to intricate boundaries and diverse granularity, reflecting the knowledge-based nature of part identification. To address these challenges, we propose PartCLIPSeg, a novel framework utilizing generalized parts and object-level contexts to mitigate the lack of generalization in fine-grained parts. PartCLIPSeg integrates competitive part relationships and attention control, alleviating ambiguous boundaries and underrepresented parts. Experimental results demonstrate that PartCLIPSeg outperforms existing state-of-the-art OVPS methods, offering refined segmentation and an advanced understanding of part relationships within images. Through extensive experiments, our model demonstrated a significant improvement over the state-of-the-art models on the Pascal-Part-116, ADE20K-Part-234, and PartImageNet datasets. Our code is available at https://github.com/kaist-cvml/part-clipseg.

Figure 1: Prediction results of our PartCLIPSeg for unseen categories in the Pascal-Part-116  validation set. A “dog” is **unseen** during training. The final prediction of PartCLIPSeg utilizes (b) object-level context and (c) generalized parts, incorporating disjoint activation among (e)–(i) parts, and enhancing activation for smaller parts (e.g., (h) “nose”).

Introduction

The pursuit of understanding parts and multi-granularity in computer vision [7; 13; 21] mirrors the innate complexities of animal instincts. For example, a "cheetah" instinctively targets an "impala's neck" during a hunt, demonstrating its ability to distinguish specific parts. This ability extends to applications such as robot commands , fine-grained controls on image editing , and more sophisticated image generation . Part segmentation aims to mimic this ability by recognizing intricate details (e.g., parts) within objects, going beyond simple object-level segmentation to achieve detailed and diverse entity recognition.

Recognizing parts is more challenging than recognizing whole objects due to their complexity and diversity. Parts often have ambiguous boundaries not only defined by visual cues but also require a broader spectrum of contextual information, reflecting their knowledge-based nature. For example, the "head" of a "dog" may include only the "face" or also the "neck" depending on the annotators' perspective [7; 21].

To address difficulties in part segmentation, Open-Vocabulary Part Segmentation (OVPS) [40; 44; 46] has evolved by leveraging the knowledge of powerful Vision-Language Models (VLMs) like CLIP  or ALIGN . Especially, it aims to achieve adaptive recognition and processing of previously unseen categories with the aid of pre-trained VLMs, pushing the boundaries of vocabularies in traditional part segmentation. By utilizing Oracle supervision of base classes during training, recent studies in OVPS exploit part-level knowledge of base classes to generalize to novel classes. Recently, VLPart  uses DINO  features to map correspondences between base and novel classes and creates pseudo labels for the novel categories. OV-PARTS  addresses the ambiguity of part boundaries by introducing object mask prompts and transferring knowledge of base class through a few-shot approach. These methods successfully extract knowledge from VLMs and extend it to novel classes, achieving significant performance improvements in open-vocabulary settings.

However, through empirical analysis of existing OVPS methods, we observed several common limitations in Figure 2. (Lack of generalization in (a)) Despite understanding part-level information, they often misidentify parts at the object level, e.g., a "dog's leg" as a "cat's leg". Also, part-level misclassification occurs as the knowledge of parts in the base class fails to generalize to a novel class, e.g., "dog's tail" as a "sheep's ear". (Ambiguous boundaries of parts in (b)) They fail to maintain non-overlapping relationships between parts, frequently resulting in overlaps, e.g., an "airplane's wing" overlapping with its "body" or the presence of empty spaces where no part is predicted. (Missing underrepresented parts in (c)) They ignore small and less frequent parts, causing prediction bias based on part size.

To overcome these limitations, we propose a novel framework called PartCLIPSeg, which consists of three main components. First, we devise generalized parts with object-level contexts to address the lack of generalization issue as the upper side of Figure 1. It explicitly obtains object-level and part-level pseudo-labels from VLMs and trains the OVPS model to satisfy both types of supervision. This guides the model to learn object boundaries while recognizing both part and object-level classes. Then, we suggest an attention control for minimizing the overlap between predicted parts, ensuring that parts are clearly separated as the lower side of Figure 1. In this way, we effectively leverage

Figure 2: Limitations of existing OVPS methods in predicting unseen categories. (a) Lack of generalization: Classification of a “dog’s parts” involving categories like “cats” and “sheep”, “dog’s tail” misclassified as “sheep’s ear”. (VLPart ) (b) Ambiguous boundaries: Vague boundary output of “aeroplane’s body”. (c) Missing underrepresented parts: Neglecting parts such as “beak” and “leg”. (CLIPSeg [32; 46]).

internal part information to learn ambiguous part boundaries. Finally, we enhance the activation related to certain parts by normalizing the activation scale of CLIP's self-attention information. It prevents small and less frequent areas from being ignored in pseudo-labels. This strategy ensures that the smallest granularity levels are retained in the final prediction. Through these three modules, PartCLIPSeg effectively addresses the challenges of existing OVPS methods and achieves robust multi-granularity segmentation. As a result, the proposed method achieves significant improvements in mIoU for both unseen and the harmonic mean when compared to previous state-of-the-art methods on Pascal-Part-116, ADE20K-Part-234, and PartImageNet in both Pred-All and Oracle-Obj settings.

## 2 Related Work

**Open-Vocabulary Semantic Segmentation.** Open-vocabulary [19; 55] semantic segmentation (OVSS) goes beyond traditional semantic segmentation, which is restricted to predefined categories, by enabling predictions for unseen classes. Pioneering works focused on aligning predefined text embeddings with pixel-level visual features [4; 48; 56]. By leveraging large-scale Vision-Language Models (VLMs) like CLIP  and ALIGN , OVSS enables zero-shot segmentation through rich multi-modal features learned from extensive image-text pairs. MaskCLIP  modified CLIP's image encoder to directly handle visual and text features for segmenting novel classes. Some works proposed two-stage strategy [15; 16; 18; 20; 29; 30; 51; 52]: first, models generate class-agnostic mask proposals [9; 10]; then, a pre-trained VLM predicts the category for each region. Some studies have introduced diffusion models to improve mask generation quality  or fine-tuned CLIP to enhance classification capabilities [20; 29]. Other studies have adopted a single-stage framework [11; 27; 32; 49; 54; 59]. They use pre-trained CLIP models to align pixel-level visual features with text features. CLIPSeg  adds a transformer-based pixel decoder with a FiLM  module to fuse multi-modal features. ZegCLIP  enhances segmentation by incorporating learnable tokens. SAN  adopted a side adapter network for a CLIP-aware end-to-end approach to predict proposal-wise classification. FC-CLIP  uses a frozen convolutional CLIP to predict class-agnostic masks and classifies using mask-pooled features . CAT-Seg  and SED  generate pixel-level cost maps and refine them for segmentation.

**Part Segmentation.** Part segmentation aims to identify the individual parts of objects, a task that is more complex and costly due to the smaller and more diverse nature of parts compared to whole objects. To tackle this, various datasets like Pascal-Part , PartImageNet , ADE20k-Part , Cityscapes-Panoptic-Part , and PACO  provide diverse and detailed part annotations. Earlier studies [7; 12; 22; 23; 43] used self-supervised constraints and contrastive settings for effective part-level entity segmentation. Recent studies extended this to open-vocabulary scenarios [35; 40; 46], opening new avenues for handling diverse parts. By leveraging class-agnostic detectors  and Vision-Language Models like CLIP [40; 46], part segmentation has extended its generalization ability to unseen parts. Our work builds upon and extends methodologies from these studies.

## 3 Methodology

As illustrated in Figure 2, we identified three primary challenges of open-vocabulary part segmentation (OVPS): lack of generalization, overlapping parts, and missing underrepresented parts. Recognizing object-specific parts (such as "dog's torso") cannot be determined solely by looking at each part in isolation; it is imperative to consider both generalized part information and the overall context of the object. Furthermore, some parts may have overlapping meanings across different granularity labels (e.g., "eye", "face", and "head"). This implies that predictions should consider direct guidance for each part as well as the relationships between different parts. These intricate spatial and functional dependencies between parts are crucial for achieving a holistic understanding and precise predictions in fine-grained entity segmentation tasks.

Based on this motivation, we propose a novel OVPS method, PartCLIPSeg. This method leverages _generalized part_ information combined with _object-level context_ to tackle the lack of generalization problem (see Section 3.2). Also, we directly minimize the overlap among part predictions to improve the part boundaries (see Section 3.3.1). Finally, we normalize the scale of attention activation from various parts for handling missing underrepresented parts (see Section 3.3.2). The overall architecture of our method is shown in Figure 3.

### Preliminary

OVPS aims to segment an image into a set of object-specific part categories \(^{}_{}\) (e.g., "dog's head," "car's front") in the _test_ set, where the image is \(^{H W 3}\), and \(H\) and \(W\) are the height and width. During training, image-mask pairs \(\{(_{k},_{k})\}\) are used, consisting of images \(_{k}\) and corresponding ground-truth mask \(_{k}\) which only contains the object-specific part categories \(^{}_{}\) (e.g., "cat's head," "bus's front") in the _train_ set.

**Zero-Shot Part Segmentation.** Open-vocabulary is a generalized zero-shot task, allowing the zero-shot segmentation protocol to evaluate zero-shot part segmentation performance. In this setting, _train_ and _test_ category names are divided into _seen_ (base) and _unseen_ (novel) sets, respectively, with disjoint object-specific category names; \(\{^{}_{}^{}_{}=\}\).

**Cross-Dataset Part Segmentation.** In this setting, the model is trained on one dataset and evaluated on another without fine-tuning. This means that the category names of the _train_ and _test_ sets come from different datasets, denoted as \(^{}_{}^{}_{ }\). Considering the domain gap between the datasets, such as differences in granularity, this setting is more challenging.

### Generalized Parts with Object-level Contexts

To address the problem of a lack of generalization, we propose leveraging generalized parts with object-level contexts. The concept of generalized parts involves identifying and utilizing common structural components that are shared across different object-level categories. For instance, many animals have parts like "head" or "torso" which, although functionally and visually distinct, may share certain underlying characteristics. By introducing generalized parts from object-specific parts, our PartCLIPSeg can efficiently recognize and segment these object-specific parts across diverse object classes, significantly enhancing the model's ability to generalize from seen to unseen categories.

Although generalized parts help distinguish the part-level categories, the visual information of a part may not suffice for accurately classifying their object-level categories. For instance, predicting the "leg" part of an animal can be challenging to identify when solely examining the part as it may not clearly indicate to which animal it belongs. For this reason, there have been attempts to incorporate object-level guidance [33; 40; 46] in part segmentation. However, object-level guidance without a generalized part may lose contextual information and miss hierarchical relationships.

By integrating object contexts with generalized parts, PartCLIPSeg employs object-level guidance that captures the holistic essence of the object to which parts belong. This integration allows for a more precise understanding and classification of parts, improving the overall performance of OVPS.

**Object and Part Embedding Generation.** We modified the architecture of CLIPSeg [32; 46], which adopted CLIP  encoder-decoder architecture for semantic segmentation. However, it is worth

Figure 3: **The overall architecture of PartCLIPSeg. The embeddings derived from the object category name and the part category name are conditioned using the FiLM operation. Each embedding, modified through attention control, is subsequently reconstructed to predict the final object-specific part parts.**noting that our approach of utilizing generalized parts with object-level context is orthogonal to other previously proposed object-level segmentation methods .

The proposed approach begins by parsing an object-specific part category name, \(_{}_{}\), into separate components: an object category name (\(_{}\)) and a generalized part category name (\(_{}\)), e.g., "cat" and "torso". Then, the CLIP text encoder, \(^{}_{}()\), is used to transform these category names into their respective CLIP embeddings (\(^{}_{}\) and \(^{}_{}\)). It will condition the image features, \(^{}\), derived from the CLIP image encoder, \(^{}_{}()\) as:

\[^{}_{}=^{}_{}(_{}),^{}=^{ }_{}(),\] (1)

where \({}^{}\) denotes frozen pre-trained models. By using Feature-wise Linear Modulation (FiLM) , each category name embeddings respectively modulate the image features as:

\[^{}_{}=^{} (^{}_{}),\] (2)

where \(\) is an element-wise sum. FiLM is an adaptive affine transformation widely used for multimodal or conditional tasks. It helps retrieve adequate conditioning for the image features. The modulated image features, \(^{}_{}\), corresponding to each object and part category name, pass through a decoder module. The decoder module will be discussed in detail in Section 3.3. They then proceed through a transposed convolution model. Finally, the output mask of the object \(^{o}\) and part \(^{p}\) are evaluated with ground-truth mask of objects, \(s^{o}\), and parts, \(s^{p}\). Oracle supervision for the object and parts mask is simply computed from a combination of object-specific parts annotations: \(s\).

**Object-specific Part Construction.** We utilize previously computed generalized part embeddings (\(^{}_{}\), \(^{}_{}\)) and object embeddings (\(^{}_{}\), \(^{}_{}\)) to reconstruct object-specific part embeddings. This process involves separate operations on modulated image features and category name embeddings.

Initially, we project the concatenated results of the object category name with the generalized part category name. This is to synthesize the embeddings for the target object-specific part category name. The approach ensures that the resultant embeddings are highly representative of parts and contextually relevant. The equivalent operation is applied to both object-level image features and part-level image features to generate object-specific image features as:

\[^{[|]}_{}=( ^{[|]}_{}^{[|]}_{}).\] (3)

The resulting object-specific part embeddings are further refined by a FiLM process. Combined with the respective object-specific image features, final modulated object-specific part embeddings, \(_{}\) is computed as:

\[_{}=^{}_{} (^{}_{}).\] (4)

These embeddings are then processed through a deconvolution layer to produce the final segmentation masks \(s\). This step ensures that the embeddings are precisely aligned to enhance the definition and accuracy of the object-specific part masks. It effectively bridges the gap between object and part-level categorical information with object-specific parts information.

**Object, Part, and Object-specific Part Mask Supervision.** The mask supervision is provided for three distinct categories: object-specific parts, objects, and generalized parts. This multi-faceted supervision enables our model to effectively disentangle generalized parts from objects, thereby facilitating a more nuanced learning process for OVPS. This disentanglement is crucial for the model to accurately recognize and differentiate between various object categories and their corresponding parts. It enhances the model's ability to handle complex segmentation tasks with unseen object-specific parts. The overall mask guidance loss can be defined as follows:

\[_{}=_{i=1}^{|_{}|+1} {(s_{i},_{i})}_{}+_{} _{i=1}^{|_{}|+1}(s^{o}_{i},^{o}_{i})}_{}+_{}_{i=1}^{|_{}|}(s^{p}_{i},^{p}_{i})}_{},\] (5)

where \(|_{}|+1\) and \(|_{}|+1\) are for uncategory (or background) prediction. The disentangled object and part generalization with object-specific parts guidance provides a clue to the lack of generalization problem.

### Attention Control for Ambiguity and Omission

In this subsection, we address the previously mentioned challenges: (1) ambiguity in part boundaries and (2) omission of small or infrequently appearing parts. The main reason for these challenges is the incomplete guidance from knowledge-based, multi-granularity characteristics of parts. To overcome these, we adopt unsupervised methods traditionally used in fine-grained recognition and part discovery studies [7; 12; 43]. Specifically, we utilize approaches for adjusting self-attention activation inspired by the recent diffusion methods [6; 41; 25].

We assume that the distribution of self-attention activation maps for visual tokens belonging to the same object-specific part mask should exhibit inter-similarity characteristics , implying similar distributions. To this end, we first compute the average self-attention map \(_{_{}}\) for each object-specific part mask \(_{}\), where \(_{}\) represents an object-specific part category. This is done by summing the self-attention activation maps from channels specifically corresponding to object \(_{}\) and part \(_{}\), across all spatial tokens \((h,w)\) within the mask, as follows:

\[_{_{}}=_{}|}_{ (h,w)_{}}(_{_{}}[h,w,:,:]+_{_{}}[h,w,:,:]).\] (6)

Subsequently, the self-attention map \(_{_{}}\) for the object-specific part mask is refined through min-max normalization, followed by the application of a Gaussian filter to smooth the initial activation as in [6; 50]. Therefore, the dimensions of both the original and normalized self-attention maps for the object-specific part masks are as follows: \(_{_{}},_{_{}}^{ }^{H W}\).

#### 3.3.1 Minimizing Part Overlaps for Ambiguity

In the self-attention of the decoder layers, competition between object-specific parts helps define boundaries that cannot be sufficiently established by supervision alone. Using the previously obtained normalized attention map, our method generates parts with minimized intersections, inspired by [1; 3; 25; 37; 47]. This approach effectively mitigates the ambiguity issue in part boundaries. Specifically, the normalized attention activation map \(_{_{}}^{}\) is first binarized based on an arbitrary threshold \(\) as:

\[_{_{}}(h,w)=_{\{_{_{}}^{}(h,w)\}},\] (7)

where \(_{_{}}\) denotes binarized attention map for part mask \(_{}\). From now on, \(_{}\) is simply denoted as \(\). The separation loss \(_{}\), which indicates the degree of intersection between object-specific parts, is as follows:

\[_{}=|}|}_{_{}}(h,w)>1 \}}{\{(h,w)_{}_{_{}}(h,w) 1\}}|,\] (8)

where separating activation mitigates the challenge of ambiguous boundaries between parts.

#### 3.3.2 Enhancing Part Activation for Omission

To address the omission problem, we employ a method inspired by attention controls in modern diffusion-based approaches [3; 6]. This approach enhances the activation within the self-attention

Figure 4: Example of attention control using separation and enhance losses. The proposed method manipulates attention maps to accurately identify and segment small parts.

activation map to enhance underrepresented parts before normalization. Specifically, for each object-specific part mask, the maximum value within the attention map is identified. Subsequently, among all object-specific parts, the minimum activation of the part with the maximum value is enhanced as:

\[_{}=1-_{}(_{(h,w) _{}}_{_{}}[h,w]),\] (9)

thereby boosting its representational efficacy. In this way, the enhancement loss \(_{}\) provides sufficient guidance for small or infrequently occurring parts, effectively mitigating the omission problem.

The training objective for PartCLIPSeg integrates three key loss components as:

\[_{}=_{}+_{}_{}+_{}_{},\] (10)

where (1) \(_{}\) for generalized parts with object-level context, (2) \(_{}\) for addressing ambiguous boundaries, (3) \(_{}\) for handling missing underrepresented parts, and \(_{}\) and \(_{}\) are hyperparmeters.

## 4 Experiments

### Experimental Setups

**Datasets.** We evaluate our method on three part segmentation datasets: Pascal-Part-116 [7; 46], ADE20K-Part-234 [46; 57], and PartImageNet . Pascal-Part-116 [7; 46] consists of 8,431 training images and 850 test images. It is a modified version of PascalPart  by removing direction indicators for certain part classes and merging them to avoid overly complex part definitions. This dataset contains a total of 116 object part classes across 17 object categories. ADE20K-Part-234 [46; 57] consists of 7,347 training images and 1,016 validation images. It provides instance-level object mask annotations along with their corresponding part mask annotations, including 44 objects and 234 parts. PartImageNet  contains 16k training images and 2.9k validation images, segmented into 158 object classes from ImageNet  and organizes them into 11 super-categories. For this study, we select 40 object classes that represent common categories to assess cross-dataset performance effectively. More details about the datasets can be found in the supplementary materials.

**Evaluation Protocols.** We use two evaluation protocols for the performance of OVPS: (1) **Pred-All** setting, where the ground truth object-level mask and object class are not provided, and (2) **Oracle-Obj** setting, where the ground truth object-level mask and object class are known. In particular, the **Pred-Obj** setting in OV-PARTS  uses predicted masks from the off-the-shelf segmentation model. In contrast, our **Pred-All** setting is a more challenging and practical setting because it does not rely on additional predicted masks or foundation models but solely uses the predicted object masks from the proposed model. For both evaluation protocols, we used mean Intersection over Union (mIoU) as an evaluation metric, which is widely used to measure segmentation performance. Additionally, we utilized the harmonic mean of the results from the seen and unseen categories as the final evaluation metric.

**Implementation Details.** We build upon CLIPSeg [32; 46], a CLIP-based encoder-decoder model. The implementation details can be found in the supplementary material.

### Performance Evaluation

**Zero-Shot Part Segmentation.** We compare our PartCLIPSeg to previous methods [11; 32; 46; 52] on three OVPS benchmarks [7; 57]. As shown in Table 1, PartCLIPSeg consistently outperforms previous approaches by significant margins on Pascal-Part-116, demonstrating its zero-shot ability, with performance improvements of 3.94% in the Pred-All setting and 3.55% in the Oracle-Obj setting. The more challenging ADE20K-Part-234 dataset, which is a fine-grained segmentation dataset, further highlights the effectiveness of PartCLIPSeg. As shown in Table 2, PartCLIPSeg achieves a harmonic mean mIoU of 11.38% in the Pred-All setting, outperforming the best-performing baseline by 7.85%. In the Oracle-Obj setting, it achieves 38.60%, which is 4.45% higher than the best baseline. Notably, PartCLIPSeg shows significant performance improvement in unseen categories, demonstrating its strong generalizability. Considering that performance in unseen categories is crucial in a zero-shotscenario, these results are significant despite some performance degradation in seen categories. We also evaluated PartCLIPSeg on PartImageNet. According to Table 3, PartCLIPSeg shows a notable improvement over CLIPSeg.

We present the segmentation results of PartCLIPSeg in comparison to state-of-the-art open-vocabulary part segmentation methods  on Pascal-Part-116. Specifically, we focus on **qualitative performance** on unseen categories such as "dog", "sheep", "car", and "bird". As shown in Figure 5 for the Pred-All and Figure 6 for the Oracle-Obj setting, the proposed method effectively segments target parts regardless of the need for predefined masks during inference. Notably, PartCLIPSeg excels at identifying smaller, often overlooked part classes such as "eye", "tail", and "headlight". Additionally, our method effectively segments multiple objects and their respective parts, a challenge for other methods, demonstrating the effectiveness of PartCLIPSeg in zero-shot part segmentation. Its improved performance on unseen categories and higher accuracy in challenging environments highlight the robustness and generalization capabilities of PartCLIPSeg. Consistent improvements on Pascal-Part-116, ADE20K-Part-234, and PartImageNet demonstrate that PartCLIPSeg sets a new standard in open-vocabulary part segmentation.

**Cross-Dataset Part Segmentation.**

Table 4 validated the efficacy of our approach in a cross-dataset setting, where category names, annotation style, and granularity of mask may vary. Additionally, unlike zero-shot situations within the same dataset, there are differences in the types and diversity of parts. Initially, we trained our model on PartImageNet and ADE20K-Part-234 respectively. Subsequent tests on Pascal-Part-116  showed that PartCLIPSeg outperforms CLIPSeg in both the Pred-All and Oracle-Obj settings, confirming our method's superiority on generalization in different datasets.

    &  &  &  \\   & & Seen & Unseen & Harmonic & Seen & Unseen & Harmonic \\  ZSSeg+  & ResNet-50 & 38.05 & 3.38 & 6.20 & **54.43** & 19.04 & 28.21 \\ VLPart  & ResNet-50 & 35.21 & 9.04 & 14.39 & 42.61 & 18.70 & 25.99 \\ CLIPSeg  & ViT-B/16 & 27.79 & 13.27 & 17.96 & 48.91 & 27.54 & 35.24 \\ CAT-Seg  & ViT-B/16 & 28.17 & **25.42** & 26.72 & 36.20 & 28.72 & 32.03 \\  PartCLIPSeg (Ours) & ViT-B/16 & **43.91\({}_{ 0.45}\)** & 23.56\({}_{ 0.21}\) & **30.67\({}_{ 0.09}\)** & 50.02\({}_{ 0.51}\) & **31.67\({}_{ 0.29}\)** & **38.79\({}_{ 0.13}\)** \\   

* The best score is **bold** and the second-best score is underlined. The standard error of an average of 5 results is reported. These are the same for all experiments.
* Comparison of zero-shot performance with state-of-the-art methods on ADE20K-Part-234.

    &  &  &  \\   & & Seen & Unseen & Harmonic & Seen & Unseen & Harmonic \\  CLIPSeg  & ViT-B/16 & 32.39 & 12.27 & 17.80 & 53.91 & 37.17 & 44.00 \\ PartCLIPSeg (Ours) & ViT-B/16 & **38.82\({}_{ 0.74}\)** & **19.47\({}_{ 0.45}\)** & **25.94\({}_{ 0.42}\)** & **56.26\({}_{ 0.29}\)** & **51.65\({}_{ 0.62}\)** & **53.85\({}_{ 0.37}\)** \\   

* The best score is **bold** and the second-best score is underlined. The standard error of an average of 5 results is reported. These are the same for all experiments.
* Comparison of zero-shot performance with state-of-the-art methods on ADE20K-Part-234.

    &  &  &  \\   & & Seen & Unseen & Harmonic & Seen & Unseen & Harmonic \\  CLIPSeg  & ViT-B/16 & 32.39 & 12.27 & 17.80 & 53.91 & 37.17 & 44.00 \\ PartCLIPSeg (Ours) & ViT-B/16 & **38.82\({}_{ 0.74}\)** & **19.47\({}_{ 0.45}\)** & **25.94\({}_{ 0.42}\)** & **56.26\({}_{ 0.29}\)** & **51.65\({}_{ 0.62}\)** & **53.85\({}_{ 0.37}\)** \\   

* ADE20K-Part-234 \({}_{}\) Pascal-Part-116
* CLIPSeg  & 5.41 & 17.82 &  ADE20K-Part-234 \\ 
*}
* PartCLIPSeg (Ours) & **10.37** & **17.94** & & & & & \\   & (+0.12) & (+0.12) & (+0.12) & & & & & & \\   

* The best score is **bold** and the second-best score is underlined. The standard error of an average of 5 results is reported. These are the same for all experiments.
* Comparison of zero-shot performance with state-of-the-art methods on ADE20K-Part-234.

Table 1: Comparison of zero-shot performance with state-of-the-art methods on Pascal-Part-116.

    &  &  &  \\   & & Seen & Unseen & Harmonic & Seen & Unseen & Harmonic \\  CLIPSeg  & ViT-B/16 & 32.39 & 12.27 & 17.80 & 53.91 & 37.17 & 44.00 \\ PartCLIPSeg (Ours) & ViT-B/16 & **38.82\({}_{ 0.74}\)** & **19.47\({}_{ 0.45}\)** & **25.94\({}_{ 0.42}\)** & **56.26\({}_{ 0.29}\)** & **51.65\({}_{ 0.62}\)** & **53.85\({}_{ 0.37}\)** \\   

* ADE20K-Part-234 \({}_{}\) Pascal-Part-116
* CLIPSeg  & 5.41 & 17.82 &  ADE20K-Part-234 \\ 
*}
* PartCLIPSeg (Ours) & **10.37** & **17.94** & & & & & & \\   & (+0.12) & (+0.12) & & & & & & & \\   

* The best score is **bold** and the second-best score is underlined. The standard error of an average of 5 results is reported. These are the same for all experiments.

Table 2: Comparison of zero-shot performance with state-of-the-art methods on ADE20K-Part-234.

    &  &  &  \\   & & Seen & Unseen & Harmonic & Seen & Unseen & Harmonic \\  ZSSeg+  & ResNet-50 & **32.20** & 0.89 & 1.74 & **43.19** & 27.84 & 33.85 \\ CLIPSeg  & ViT-B/16 & 3.14 & 0.55 & 0.93 & 38.15 & 30.92 & 34.15 \\ CAT-Seg  & ViT-B/16 & 7.02 & 2.36 & 3.53 & 33.80 & 25.93 & 29.34 \\  PartCLIPSeg (Ours) & ViT-B/16 & 14.15\({}_{ 0.51}\) & **9.52\({}_{ 0.13}\)** & **11.38\({}_{ 0.10}\)** & 38.37\({}_{ 0.14}\)** & **38.82\({}_{ 0.13}\)** & **38.60\({}_{ 0.08}\)** \\  & & & & & & & & & \\   & (+4.5) & & & & & & & & \\  \

### Ablation Study

In this section, we analyze the impact of each training loss on PartCLIPSeg. We focus on the roles of the separation and enhancement losses, examining how they contribute to improved segmentation accuracy.

**Separation & Enhancement Losses.** We conducted an ablation study to investigate the effect of the separation loss \(_{}\) and the enhancement loss \(_{}\) on the performance of PartCLIPSeg in Table 5. On Pascal-Part-116, eliminating both losses resulted in a lower harmonic mean of 29.20 in Pred-All and a harmonic mean of 38.20 in Oracle-Obj. Introducing \(_{}\) without \(_{}\) improved the harmonic mean in both Pred-All and Oracle-Obj setups. Using both losses led to the highest harmonic means of 30.67 and 38.79, respectively. Similarly, for ADE20K-Part-234, employing both losses resulted in the

   Method & Seen & Unseen & Harmonic \\  ZSSeg+  & 33.01 & 26.76 & 29.56 \\ CLIPSeg  & 34.67 & 32.20 & 33.39 \\ CAT-Seg  & 34.17 & 30.14 & 32.03 \\ PartCLIPSeg (Ours) & **36.15** & **39.07** & **37.55** \\   

Table 6: Performance on mean Boundary IoU (\(\)) on Pascal-Part-116 in Oracle-Obj setting.

  
**Part:** **\%ey** & bird & cat & cow & dog & sheep & person \\  CLIPSeg  & **3.33** & 18.77 & 3.65 & 16.05 & 0.00 & 15.30 \\ PartCLIPSeg (Ours) & 1.95 & **31.01** & **28.16** & **32.79** & **0.67** & **29.16** \\ 
**Part:** **neck** & bird & cat & cow & dog & sheep & person \\  CLIPSeg  & 19.09 & 6.57 & 0.78 & 8.12 & 8.47 & 30.93 \\ PartCLIPSeg (Ours) & **32.51** & **12.00** & **2.75** & **16.37** & **18.80** & **50.71** \\ 
**Part:** **“leg** & bird & cat & cow & dog & sheep & person \\  CLIPSeg  & 19.61 & 38.62 & 27.85 & 39.34 & 52.63 & 52.67 \\ PartCLIPSeg (Ours) & **31.12** & **44.82** & **63.78** & **41.55** & **54.73** & **55.35** \\   

Table 7: Impact of PartCLIPSeg for small parts on Pascal-Part-116 in Oracle-Obj setting. (mIoU)

Figure 5: Qualitative results of zero-shot part segmentation on Pascal-Part-116 in **Pred-All** setting. Annotations for unseen categories (bird, car, dog, sheep, etc.) are not included in the train set.

best performance, with harmonic means of 19.63 in Pred-All and 38.60 in Oracle-Obj. These results highlight the importance of both separation and enhancement losses in improving performance.

To verify the effectiveness of boundary creation of PartCLIPSeg, we examined an additional qualitative metric, Boundary IoU . The results demonstrated high Boundary IoU performance, confirming that PartCLIPSeg effectively resolves ambiguous boundary issues as shown in Table 6.

**Impact of PartCLIPSeg for Underrepresented Parts.** We investigate the effect of the enhancement loss \(_{}\) on OVPS model performance, especially with respect to underrepresented parts. In Table 7, we compare our PartCLIPSeg with CLIPSeg  on small parts such as "eye", "neck", and "leg" of animals in Pascal-Part-116. As shown in the table, PartCLIPSeg consistently outperforms CLIPSeg with significant improvements in most cases. Notably, there is an impressive performance increase of 35.93%p for "cow's leg". These improvements highlight the effectiveness of the enhancement loss in accurately segmenting small and intricate parts, demonstrating its crucial role in improving overall performance.

## 5 Conclusion

In this study, we introduced PartCLIPSeg, a state-of-the-art OVPS method that addresses three primary challenges in OVPS. PartCLIPSeg utilizes generalized parts and object-level guidance to effectively solve identification issues. Then, it separates parts by minimizing their overlaps in attention maps, thus learning ambiguous part boundaries. Additionally, we implemented an enhancement loss function to improve the detection of underrepresented parts. Through extensive experimentation, we have confirmed the superior performance of PartCLIPSeg.

Figure 6: Qualitative results of zero-shot part segmentation on Pascal-Part-116 in **Oracle-Obj** setting.