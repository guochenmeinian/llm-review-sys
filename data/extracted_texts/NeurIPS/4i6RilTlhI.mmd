# Self-Supervised Bisimulation Action Chunk Representation for Efficient RL

Lei Shi, Jianye Hao, Hongyao Tang, Zibin Dong, Yan Zheng

College of Intelligence and Computing, Tianjin University

{leishi, jianye.hao, bluecontra, yanzheng}@tju.edu.cn

zibindong@outlook.com

###### Abstract

Action chunking in reinforcement learning is a promising approach, as it significantly reduces decision-making frequency and leads to more consistent behavior. However, due to inherent differences between the action chunk space and the original action space, uncovering its underlying structure is crucial. Previous works cope with this challenge of single-step action space through action representation methods, but directly applying these methods to action chunk space fails to capture the semantic information of multi-step behaviors. In this paper, we introduce **A**ction **C**hunk **R**epresentation (**ACR**), a self-supervised representation learning framework for uncovering the underlying structure of the action chunk space to achieve efficient RL. To build the framework, we propose the action chunk bisimulation metric to measure the principled distance between action chunks. With this metric, ACR encodes action chunks with a Transformer that extracts the temporal structure and learns a latent representation space where action churns with similar bisimulation behavior semantics are close to each other. The latent policy is then trained in the representation space, and the selected latent action chunk is decoded back into the original space to interact with the environment. We flexibly integrate ACR with various DRL algorithms and evaluate it on a range of continuous manipulation and navigation tasks. Experiments show that ACR surpasses existing action representation baselines in terms of both learning efficiency and performance.

## 1 Introduction

Action chunking reinforcement learning (RL) agents learn a policy that produces a sequence of actions at each step to solve tasks, which has been proven to reduce interaction frequency and lead to more consistent behavior [1; 2; 3; 4; 5; 6]. A common approach is to use the Cartesian product of the original action space as the action chunk space . While straightforward, this approach leads to an exponential increase in dimensionality and introduces meaningless action combinations , making policy optimization challenging. Since only a few action combinations yield meaningful behaviors, learning the underlying structure of the action chunk space is crucial. While action representation methods  have shown promise in handling complex action spaces, they often focus on single-step action space settings, constructing representation spaces through forward prediction [9; 10; 11] or reconstruction [12; 7].

Figure 1: **Overview. The latent policy is trained in the ACR action chunk representation space guided by the bisimulation metric. The selected latent action chunk is decoded back into the original space to interact with the environment.**Unfortunately, directly using single-step representation methods in the action chunk space still introduces the curse of dimensionality and fails to capture the semantic information of multi-step behavior, largely due to the inherent underlying structure. Therefore, we aim to develop a behavior-centric self-supervised representation method that is inherently suitable for action chunk space.

In this paper, we propose a novel self-supervised representation learning framework for action chunk space, called **A**ction **C**hunk **R**epresentation (**ACR**), which constructs a compact and decodable action chunk representation space, enabling efficient policy learning and interaction with the environment. An overview of ACR is shown in Fig.1. In contrast to commonly used forward prediction or reconstruction objectives, we introduce a novel action chunk bisimulation metric as the distance metric for constructing the ACR representation space, thereby regularizing the learned representation to focus on the behavior influence of action chunks. Moreover, ACR utilizes a Causal Transformer  encoder to capture the temporal information within action chunks, which is crucial for constructing a semantically rich representation space. In principle, ACR is algorithm-agnostic, allowing integration with any continuous control DRL algorithm. Our experiments on nine manipulation and navigation control tasks demonstrate the significant performance advantage of ACR compared to other action representation learning methods.

## 2 ACR: Action Chunk Representation

ACR considers solving a multi-step MDP, \(=(,,k,,},,,)\), where \(\) is the state space, \(\) is the action space, \(k\) is the length of action, \(=_{k}\) is the action chunk space, \(}\) is the representation space, \(\) and \(\) are the lifted transition function and reward function to accept action chunks, respectively. An action chunk can be defined as a multi-step action \(c=\{a_{t},a_{t+1},...,a_{t+k-1}\}\). The main idea of ACR is to learn an encoder \(E_{}(|c)\) that constructs a compact representation space for action chunks. The latent policy \((|s)\) explores and learns in this space. It selects latent action chunks, which can be decoded to the original action space by the ACR decoder \(D_{}(c|)\) to interact with the environment. The overview is shown in Fig.1, and the training process of ACR can be divided into two parts:

**Learning the representation space with Action Chunk Bisimulation Metric.** Our insight is that _action chunks with similar behaviors should cluster closely in the representation space, indicating those leading to the same transitions and rewards are equivalent in the associated multi-step MDP_. We extend the bisimulation metric  to measure the principled distance between action chunks:

**Definition 1** (Action Chunk Bisimulation Metric).: Given a multi-step MDP \(\) and a state \(s_{t}\), a state-conditional action chunk bisimulation metric is a function \(d_{c}\)1: \(_{ 0}\) such that:

\[d(c_{i},c_{j}|s_{t})=|_{s_{t}}^{c_{i}}-_{s_{t}}^{c _{j}}|+ W_{2}(_{s_{t}}^{c_{i}},_{ s_{t}}^{c_{j}};d_{c})\] (1)

Figure 2: **The training process of ACR. Left:** We train the Transformer encoder to construct the action chunk representation space, ensuring that the \(_{1}\) distance between any two latent action chunks equals the distance measured by the ACB Metric. During training, we sample pairs \((s_{t},c_{i},_{s_{t}}^{c_{i}})\) and additional chunks \(c_{j}\) from random samples. The encoder first generates latent action chunks \(_{i}\) and \(_{j}\), after which the transition model and reward model predict the transitions for \((s_{t},c_{i})\) and \((s_{t},c_{j})\), and the cumulative reward for \((s_{t},c_{j})\). **Right:** We train the action chunk decoder once the encoder is fully trained. The decoded action chunk \(c^{*}\) is then passed through the fixed encoder to obtain \(^{*}\).

where \(W_{2}\) is the \(2^{nd}\) Wasserstein distance between two distributions. Here, \(_{s_{t}}^{c}\) represents the sum of cumulative discounted reward, i.e. \(_{i=0}^{k-1}^{i}_{s_{t+i}}^{a_{t+i}}\). \(_{s_{t}}^{c}\) represents the distribution of \(s_{t+k}\) after executing \(c\) starting from \(s_{t}\). The training process of ACR is shown in Fig. 2. To ensure action chunks in representation space satisfy the property \(d(c_{i},c_{j}|s_{t}):=|_{i}-_{j}|_{1}\), we train a Causal Transformer encoder \(E_{}(|c)\) to learn the representation space. We minimize self-supervised loss:

\[J()=_{s_{t},c_{i},c_{j},_{s_{t}}^{c}}, (\|_{i}-_{j}\|_{1}-(c_{i},c_{j} s_{t}))^{2},\] (2)

\[(c_{i},c_{j}|s_{t})=|_{s_{t}}^{c_{i}}-}(s_{t},_{j})|+ W_{2}(}(|s_ {t},_{i}),}(|s_{t},_{j})).\] (3)

\((c_{i},c_{j}|s_{t})\) represents an estimate of \(d(c_{i},c_{j}|s_{t})\), \(}\) and \(}\) is the reward model and transition model, which are trained with the encoder separately. Please note that all inputs to the transition and reward model are stop-gradient. During the training of the decoder \(D_{}\), we fix the trained Transformer encoder and minimize the decoder objective function:

\[J()=_{c}[\|E_{}(D_{}(E_{}(c)))- E_{}(c)\|_{2}^{2}+\|D_{}(E_{}(c))\|_{2}^{2}]\] (4)

The first term ensure that \(D_{}\) serves as a one-sided inverse of \(E_{}\), which means \(E_{}(D_{}()=\) but \(D_{}(E_{}(c)) c\). The second term guarantees that \(D_{}\) is the minimum-norm one-sided inverse of \(E_{}\), ensuring the validity of the decoded action chunk in the original action space.

**Optimizing the policy over the learned representation space.** ACR is algorithm-agnostic, allowing for integration with any continuous control DRL algorithm through minor modifications. Here, we use TD3  as an example. The latent policy \(_{}\) is trained in the representation space and should fully utilize the temporal information within the action chunk. Therefore, the TD3 double critic networks \(Q_{_{m=1,2}}\) additionally take as input the current step \(i[0,k)\) within the executed action chunk. With a buffer of collected transition sample \(b=(s_{t:t+k},,r_{t:t+k-1},i_{0:k-1})\), the critics are trained by Clipped Double Q-Learning:

\[ L_{}(_{m})=_{b }[(y-Q_{_{m}}(s_{t+i},,i)],\\ y=_{j=0}^{k-i-1}(^{j}r_{t+j})+^{k-i}_{n=1,2} Q_{_{n}}(s_{t+k-i},_{}(s_{t+k-i}),0)\] (5)

\(_{n=1,2}\), \(\) are the the target network parameters. The actor considers the action chunk generated based on the initial state \(s_{t}\) of each sample. We then update the actor as follows:

\[_{}J()=_{s_{t}}[_{}_{}(s_{t}) _{_{}(s_{t})}Q_{_{1}}(s_{t},,0)|_{=_{ }(s_{t})}]\] (6)

Please note that as the latent policy improves the outdated representation may no longer reflect the same behavioral effects . We propose two mechanisms to ensure the validity of the representation space: **Periodic Update** and **Adaptive Constraint**. Details are provided in Appendix A.

## 3 Experimental results

We evaluate ACR across nine continuous control tasks spanning three domains: 2DoF Arm Control , 7DoF Arm Control , and Maze Navigation  (See Appendix B.1 for more details), and aim to answer the following research questions (**RQ**s): 1) Can combining ACR with DRL algorithms improve learning efficiency and performance? 2) Compared to other self-supervised action representation learning methods, what advantages does ACR offer? 3) What components and design choices of ACR contribute to the improved performance in our experiments?

**RQ1:** We combine ACR with three widely used DRL algorithms: DDPG, TD3, and SAC [20; 15; 21]. For a fair comparison, we use the default hyperparameters and architectures of them. As shown in Fig. 3 (Left), we observe significant performance improvements and faster convergence when ACR is combined with all three algorithms, especially in the challenging 7DoF robotic arm control tasks and sparse reward Maze navigation tasks. Notably, in Striker and Thrower, the original algorithms struggle to explore effectively due to the complexity of the action space, leading to unstable performance or even collapse. However, with ACR, the algorithms are able to learn stably. This evidence suggests that the action chunk representation space constructed by ACR captures the underlying structure of the original action space, allowing the latent policy to explore and make decisions more effectively, thereby simplifying the tasks and significantly improving learning efficiency and performance.

**RQ2:** We combine ACR and other self-supervised action representation methods with TD3, training latent policies in their respective action representation spaces. These methods include: 1) Reconstruction: **PLAS**, 2) Forward Prediction: **CVAE**, **LASER** and **DynE**, 3) Inverse Prediction: **PGRA**, 4) Bisimulation: **MERLION** (see Appendix B.2.2 for details). As shown in Fig. 3 (Right), we observe that as task difficulty increases, other action representation learning methods suffer from performance collapse and unstable learning, while ACR maintains stable performance, especially in 7DoF arm control tasks. Additionally, we find that constructing action chunk representation space outperformed single-step action representation space, with ACR achieving significantly better performance and faster learning in the first two domains. These results demonstrate the intrinsic value of constructing an action chunk representation space.

**RQ3:** As shown in Fig. 4, we conduct ablation studies on 3 tasks: 1) **w/o ACB metric**: Removing the self-supervised learning objective for measuring action chunks distance (Equation 2) degrades ACR to a structure similar to conditional VAE, resulting in poor performance, indicating that the ACB metric is crucial for the representation space. 2) **w/o Transformer**: Replacing the Transformer encoder with an MLP leads to performance decline, highlighting the importance of temporal information within action chunks for high-quality representation. 3) **w/o Periodic Update**: The representation space is not continuously updated during policy training. 4) **w/o Adaptive Constraint**: The representation space range is fixed, similar to . The experimental results show the absence of these two mechanisms harms learning performance and stability.

## 4 Conclusion

In this paper, we introduce ACR, a novel framework that uses the action chunk bisimulation metric as the self-supervised learning objective to construct a compact, low-dimensional, and decodable action chunk representation space for multi-step action, effectively capturing the semantic information of multi-step behavior. ACR is algorithm-agnostic, enabling integration with any continuous control DRL algorithm to enhance performance and learning efficiency. Our experiments demonstrate that ACR significantly outperforms other action representation learning methods, highlighting that the Action Chunk Bisimulation Metric captures richer semantic information. Additionally, the representation space constructed by ACR incorporates temporal information within action chunks, which is beneficial for uncovering the underlying structure of the action chunking space.

Figure 4: **Ablation studies.** The performance of ACR-TD3 relative to the version without the corresponding component or mechanism.

Figure 3: **Experimental results. Left:** Comparing ACR applied to three DRL algorithms against directly training them on the original action spaces. **Right:** Comparing the performance of ACR against different self-supervised action representation learning methods, each combined with TD3. The curve and shade denote the mean and a standard deviation over 5 random seeds.