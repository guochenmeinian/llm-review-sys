# Not Just Object, But State: Compositional Incremental Learning without Forgetting

Yanyi Zhang \({}^{1}\), Binglin Qiu \({}^{1}\), Qi Jia \({}^{1}\), Yu Liu \({}^{4}\), Ran He \({}^{2}\)

\({}^{1}\) International School of Information Science & Engineering, Dalian University of Technology

\({}^{2}\) MAIS&CRIPAC, Institute of Automation, Chinese Academy of Sciences

yanyi.zhang@mail.dlut.edu.cn, mlandy@mail.dlut.edu.cn

jiaqi@dlut.edu.cn, liuyu8824@dlut.edu.cn, rhe@nlpr.ia.ac.cn

corresponding author

###### Abstract

Most incremental learners excessively prioritize coarse classes of objects while neglecting various kinds of states (_e.g_. color and material) attached to the objects. As a result, they are limited in the ability to reason fine-grained compositionality of state-object pairs. To remedy this limitation, we propose a novel task called **Compositional Incremental Learning** (composition-IL), enabling the model to recognize state-object compositions as a whole in an incremental learning fashion. Since the lack of suitable benchmarks, we re-organize two existing datasets and make them tailored for composition-IL. Then, we propose a prompt-based **Composition** Incremental **L**earner (**CompILer**), to overcome the ambiguous composition boundary problem which challenges composition-IL largely. Specifically, we exploit multi-pool prompt learning, which is regularized by inter-pool prompt discrepancy and intra-pool prompt diversity. Besides, we devise object-injected state prompting by using object prompts to guide the selection of state prompts. Furthermore, we fuse the selected prompts by a generalized-mean strategy, to eliminate irrelevant information learned in the prompts. Extensive experiments on two datasets exhibit state-of-the-art performance achieved by CompILer. Code and datasets are available at: https://github.com/Yanyi-Zhang/CompILer.

## 1 Introduction

Class Incremental Learning (class-IL)  gathers increasing attention due to its ability to make the models learn new tasks rapidly, without forgetting previously acquired knowledge. Yet, traditional class-IL sets a strict limit on the old classes such that they should not recur in newly incoming tasks. To break such a strict limitation, recent studies develop a new setting mostly called Blurry Incremental Learning (blur-IL) , where the incremental sessions allow the recurrence of previous classes, resulting in a more realistic and flexible scenario. Despite such empirical progresses on incremental learning, they aim to improve object classification only, while overlooking fine-grained states attached to the objects. For instance, analyzing how the clothing styles (akin to states) have changed over time is important for forecasting the future trends that will emerge.

To simultaneously model objects and their states, some efforts are dedicated to Compositional Learning whose aim is how to equip the models with _compositionality_. The core of compositional learning lies in the structure of class labels, which conceptualizes a state-object pair (_e.g_. "Brown Pants" and "Yellow Dress") as a whole, rather than a lonely object label. In this way, the model can dissect and reassemble learned knowledge, achieving a more fine-grained understanding about the objects. However, existing works are mainly focused on zero-shot generalization from seencompositions to unseen ones [26; 27; 5; 15], whereas none of them consider the challenging fact that the model must deal with a significantly larger number of composition classes than object classes. As a result, it is hardly feasible to learn all compositions by training the model once.

To remedy the limitations inherent in incremental learning and compositional learning, we conceive a novel task named **Compositional Incremental Learning** (composition-IL), enabling the model to continually learn new state-object compositions in an incremental fashion. As compared in Fig. 1, we can see that composition-IL integrates the characteristics of class-IL and blur-IL. Although the composition classes are disjoint across incremental tasks, the primitive classes (_i.e_. objects and states) encountered in old tasks are allowed to reappear in new tasks. Unfortunately, existing incremental learning approaches are challenged by such a compositional scenario, because their models excessively prioritize the object primitives while neglecting the state primitives. Consequently, the compositions with the same object but with different states become ambiguous and indistinguishable.

To tackle the problem, we propose a rehearsal-free and prompt-based **Com**positional **I**ncremental **L**earner (**CmpILer**). Specifically, our model comprises of three primary components: multi-pool prompt learning, object-injected state prompting, and generalized-mean prompt fusion. Firstly, we construct three prompt pools for learning the states, objects and compositions individually. Upon that, we add extra restrictions to regularize the inter-pool prompt discrepancy and intra-pool prompt diversity. This multi-pool prompt learning paradigm strengthens the fine-grained understanding and reasoning towards primitive concepts and their compositions. In addition, as the state classes are more difficult to distinguish than the object ones, we propose object-injected state prompting which incorporates object prompts to guide the selection of state prompts. Furthermore, we fuse the selected prompts by a generalized-mean fusion manner, which helps to adaptively eliminate irrelevant information learned in the prompts. Last but not least, we also leverage symmetric cross-entropy loss to alleviate the impact of noisy data during training.

In summary, the main contributions in this work are encapsulated as follows: (1) We devise a new task coined compositional incremental learning (composition-IL). It enables learning fine-grained state-object compositions continually while the isolated primitive concepts can randomly recur in incremental tasks. (2) To address the lack of datasets, we re-organize two existing datasets such that they are tailored specifically for composition-IL. For the two new datasets (Split-Clothing and Split-UT-Zappos), we split them into 5 and 10 incremental tasks for evaluating the methods. (3) We propose a novel learning-to-prompt model for composition-IL, namely CompILer. Our state-of-the-art results on Split-Clothing and Split-UT-Zappos validate the effectiveness of CompILer for incrementally learning new compositions without forgetting old ones.

## 2 Related Work

**Incremental Learning.** The approaches to addressing catastrophic forgetting for incremental learning can be broadly grouped into four categories: regularization based methods [6; 39] aim to protect

Figure 1: Differences between Class Incremental Learning (class-IL), Blurry Incremental Learning (blur-IL), and Compositional Incremental Learning (composition-IL). The object classes are not allowed to recur in the class-IL scenario, whereas they may recur randomly in the blur-IL scenario. Different from them, the classes in composition-IL involve state-object compositions apart from the object classes. Besides, the compositions do not reoccur, but the primitives (states or objects) may randomly reappear across incremental sessions.

influential weights of old experiences from updating; knowledge distillation based methods [16; 34] distill knowledge from the model trained on the previous tasks and adapt it to new tasks; rehearsal based methods [32; 48; 20] require a memory buffer to store some old data, so as to make the network remember previous tasks; parameter isolation methods allocates different model parameters to each task, to prevent any possible interference. Different from the methods, L2P  proposes an innovative learning-to-prompt paradigm, which incorporates plasticity and stability through adapting a set of learnable prompt tokens on top of a frozen pre-trained backbone. Inspired by L2P , more recent works [42; 36; 2; 28] take full advantage of various prompt tuning strategies, achieving new state-of-the-art performance for incremental learning. However, such methods take into account object classes solely, while neglecting various kinds of state classes associated with the objects. To this end, our work proposes compositional incremental learning with the purpose to continually identifying the composition classes of state-object pairs. Note that, Liao, _et al_ conduct an initial study toward the compositionality in incremental learning, whereas their attention is on the composition of multiple object classes (_e.g._ "Car" and "Person") in one image, rather than the state-object compositions in this work.

**Compositional Learning.** A major line of compositional learning research focuses on Compositional Zero-Shot Learning (CZSL) , which aims to infer unseen state-object compositions by acquiring knowledge from seen ones. Subsequent approaches building upon the CZSL setting further incorporate graph neural networks to model the dependency between primitives and compositions , and employ cosine classifiers to avoid being overly biased toward seen compositions . Other approaches [13; 14; 12] propose training two classifiers to identify states and objects separately. The latest works [19; 3; 38; 8; 47] model both composition and primitives simultaneously, achieving state-of-the-art results. Albeit the numerous attempts made in compositional learning, they fail to consider an incremental learning paradigm given the increasing number of composition classes in open-world scenarios. Besides, directly applying CZSL methods to composition-IL might lead to a stale and decaying performance on forgetting. By contrast, our proposed CompILer markedly bypasses catastrophic forgetting with the help of multi-pool prompt learning.

## 3 Preliminaries

In this section, we firstly define the task of composition-IL, and then introduce two datasets we construct for the task, followed by revealing the ambiguous composition boundary problem.

### Problem Definition

For composition-IL, a model sequentially learns \(N\) tasks \(=\{_{1},_{2},_{N}\}\) corresponding to a set of composition classes \(=\{_{1},_{2},_{N}\}\). We note that the composition classes between incremental tasks are always disjoint, which means \(_{i}_{j}=\) for any \(i j\). Different from the composition classes, the primitive classes are allowed to recur in different tasks. That means it allows the tasks to share some primitive concepts of objects and states. Therefore, we can define the set of all state and object classes with \(=\{s_{1},s_{2},,s_{n}\}\) and \(=\{o_{1},o_{2},,o_{m}\}\), respectively. Given each image \(x\), it has a composition label \(c\) which is constructed with a state label \(s\) and an object label \(o\), _i.e._\(c=<s,o>\), where \(c\), \(s\) and \(o\). We take the example of "red shirt", where "red" is denoted with \(s\), "shirt" corresponds to \(o\), and "red shirt" is expressed with \(c\).

Figure 2: Data Statistics of Split-Clothing and Split-UT-Zappos for tasking composition-IL. Split-Clothing is divided into a 5-task scenario, while Split-UT-Zappos includes both 5-task and 10-task scenarios. In all settings, the number of images per task has been balanced properly.

### Dataset Construction

As there are no existing datasets suitable for composition-IL, we re-organize the data in Clothing16K  and UT-Zappos50K , and construct two new datasets tailored for composition-IL, namely **Split-Clothing** and **Split-UT-Zappos**. To be more specific, we firstly sort the composition classes based on the number of their images, and then select the foremost 35 compositions from Clothing16K and the top 80 from UT-Zappos50K, so as to construct Split-Clothing and Split-UT-Zappos, respectively. In this way, Split-Clothing encompasses 9 states and 8 objects while Split-UT-Zappos consists of 15 states and 12 objects in total. For Split-Clothing, we randomly partition the compositions into 5 tasks. Regarding Split-UT-Zappos, the compositions are sorted by count and are evenly divided into 5 and 10 tasks. The image distribution for each task is shown in Fig. 2. Note that, we elaborate more details on both datasets in the following technical appendix.

### Revealing the Ambiguous Composition Boundary

The main stumbling block in composition-IL is the ambiguous composition boundary. Although the composition label consists of two primitives (_i.e_. object and state), we note that the model excessively prioritizes the object primitive while neglecting the state primitive. Consequently, the compositions with the same object but with different states become ambiguous and indistinguishable. To prove that, we apply L2P  to composition-IL, whereas it is challenged by significant ambiguities in composition classification. As illustrated in Fig. 3 (a), the _t_-SNE visualization showcases the entanglement among the compositions like "white dress", "black dress" and "blue dress". We conjecture that this ambiguous problem tends to become more severe when more tasks are arriving incrementally. To address it, we propose a new model namely CompILer, which disentangles compositions and primitives via a multi-pool prompt learning. Advantageously, our method promotes the learning on the states and establishes clearer composition boundaries, as shown in Fig. 3 (b).

## 4 Methodology

**Overview.** We leverage the learning-to-prompt paradigm  and develop a novel compositional incremental learner (CompILer) tailored specifically for composition-IL. As depicted in Fig. 4, CompILer comprises three primary components: multi-pool prompt learning, object-guided state prompting, and generalized-mean prompt fusion. Firstly, we initialize three prompt pools dedicated to learning and storing visual information related to states, objects and their compositions. In order to differentiate the knowledge learned across and within prompt pools, we define inter-pool discrepant loss and intra-pool diversified loss jointly. We then employ object prompts to guide the selection of state prompts, thereby improving the state representation learning. Moreover, we utilize a generalized-mean fusion to integrate the selected prompts in a learnable manner. Ultimately, we optimize the classification objective with symmetric cross-entropy loss, to alleviate the effect of noisy data.

### Multi-pool Prompt Learning

The learning-to-prompt paradigm [43; 35], especially suitable for large pre-trained backbones, has opened up a new path for incremental learning. It has proven to incorporate plasticity and stability

Figure 3: _t_-SNE feature distributions of seven compositions from the Split-Clothing benchmark. For the compositions with the same object but with different states, our CompILer achieves more distinguishable boundaries than the L2P baseline.

better through adapting a set of learnable tokens in a prompt pool to a frozen pre-trained backbone. Nevertheless, existing prompt-based approaches are initially designed for class-IL, thereby building a single prompt pool for object classification solely. when dealing with state-object composition classification, they tend to excessively prioritize the object primitive while neglecting the state primitive. To this end, we propose to construct three discrepant and diversified prompt pools \(_{s}\), \(_{o}\) and \(_{c}\), which serve to learn visual information related to states, objects and their compositions, respectively. Besides, each pool is associated with a set of learnable keys \(_{}\) for query-key prompt selection. The three prompt pools and their keys are defined as:

\[_{}=\{P_{}^{1},P_{}^{2}, P_{}^{M} \},_{}=\{K_{}^{1},K_{}^{2}, K_{ }^{M}\},\{s,o,c\},\] (1)

where \(P_{}^{i}^{L D}\) is a single prompt with token length \(L\) and embedding dimension \(D\). \(K_{}^{i}^{D}\), the key of \(P_{}^{i}\), is a learnable token with the same size. \(M\) is the number of prompts in each pool.

One important concern in such multi-pool prompt learning is how to enrich the prompts with the avoidance of identical pools. To achieve it, we consider integrating **inter-pool prompt discrepancy** and **intra-pool prompt diversity** jointly. On the one hand, the inter-pool prompts should be discrepant as the visual information about states, objects, and compositions should be different. One the other hand, within each pool, the intra-pool prompts should be diversified so to capture more comprehensive features from all the classes.

In practice, we formulate a unified objective to regularize both inter-pool discrepancy and intra-pool diversity, by leveraging a simple and effective directional decoupled loss used in . The directional decoupled (dd) loss between any two pools (_e.g_. \(P_{i}\) and \(P_{j}\)) is formulated as:

\[_{dd}^{(i,j)}=_{n=1}^{M}_{m=1}^{M} (0,_{}-_{nm}),\] (2)

\[_{nm}=^{-1}^{n})^{}P_{j}^{m}}{(\|P_{ i}^{n}\|_{2},)(\|P_{j}^{m}\|_{2},)},\] (3)

where \(_{nm}\) measures the angle between any two prompts, \(n\) and \(m\); \(\) is a scalar to avoid division by zero. Note that, \(_{dd}^{(i,j)}\) encourages the angles between each prompt to be at least \(_{}\) degrees. Since \((i,j)\) is unordered Cartesian product of \(\), _i.e_. \((i,j)\{(i,j) i j\}\), the inter-pool prompt discrepancy loss for the three pools can be expressed with \(_{inter}=_{dd}^{(s,o)}+_{dd}^{(s,c)}+ _{dd}^{(o,c)}\), and the intra-pool prompt diversity loss becomes \(_{intra}=_{dd}^{(s,s)}+_{dd}^{(o,o)}+ _{dd}^{(c,c)}\). As opposed to \(_{inter}\), \(_{intra}\) computes the angle between any two prompts within the same pool. Thus, it contains the case when \(n=m\), for which we set \(_{}-_{nm}=0\).

Figure 4: Overall architecture of our composition incremental learner (CompILer), which comprises multi-pool prompt learning, object-injected state prompting, and generalized-mean prompt fusion. The multi-pool prompt learning mechanism captures information related to states, objects, and their compositions, each through a dedicated pool. The object-injected state prompting utilizes the object prompt to promote the state representation learning. Moreover, the generalized-mean prompt fusion is used to prioritize the useful prompts and diminish the irrelevant ones.

### Object-injected State Prompting

Akin to the query-key matching mechanism in other work [43; 42; 40], we utilize a fixed feature extractor \(f()\) to obtain a query feature \(q(x)=f(x)[0,]\), determining which prompts in the pool to be selected. However, pre-trained backbones are typically trained for object classification, thus under-performing for state representation learning. In addition, it is more difficult to predict the state classes due to their more abstract and fine-grained characteristics. To tackle this problem, we strategically inject object prompts to guide the selection of state prompts. Intuitively, once we have learned knowledge about the object class, it may be easier to predict the correct state class and avoid mistaken results. For instance, given an object is "heels", we can expect that the corresponding state is unlikely to be "canvas" or "plastic". To summarize, we select object and composition prompts in each pool based on the original query feature, which means \(q_{o}(x)=q_{c}(x)=q(x)\); but for the selection of state prompts, we propose object-injected state prompting to ameliorate the query feature as shown in Fig. 5.

Specifically, we employ the fused object prompt \(_{o}\) (see Sec. 4.3) to perform cross attention on the query feature \(q(x)\), resulting in object-injected query feature \(q_{s}(x)\) for the state prompt selection:

\[q_{s}(x)=(q(x),_{o})=( _{o}W^{K}}{})_{o}W^{V},\] (4)

where \(W^{Q}\), \(W^{K}\) and \(W^{V}\) are learnable projections. To establish alignment between the query and the selected prompts, we optimize a surrogate loss for state, object and composition prompting jointly:

\[_{sur}=_{}_{q_{}}(f_{}(x),K_{ }^{s_{i}}),\;\{s,o,c\},\] (5)

where \((,)\) denotes cosine similarity, \(_{}\) represents the subset of top-k keys selected from \(_{}\), and \(\{s_{i}\}_{i=1}^{k}\) is a subset of top-k indices from \([1,M]\) (prompt number). Despite the simplicity of the object-injected state prompting, it facilitates more judicious prompt selection within the state prompt pool, alleviating the hurdles posed by state learning.

### Generalized-mean Prompt Fusion

After obtaining the selected top-k prompts \(\{P_{}^{s_{i}}\}_{i=1}^{k}\), the next step is fusing these prompts into a single prompt. It is general to utilize a simple mean pooling whereas it overlooks the relative importance of each prompt. Besides, when the prompts contain information that is unrelated or contradictory to current task, it is critical to strengthen useful prompts and eliminate irrelevant ones. To this end, we draw inspiration from generalized-mean pooling  and exploit generalized-mean (GeM) prompt fusion which is given by:

\[_{}=_{}(P_{}^{s_{1}},P_{}^{s_{2}}, ,P_{}^{s_{k}})=(_{i=1}^{k}P_{}^{s_{i}, })^{},\{s,o,c\},\] (6)

where \(\) is a learnable parameter. When \(=1\), GeM becomes mean pooling; as \(\) approaches infinity (\(\)), it converges to max pooling. By taking over mean and max pooling, GeM learns to achieve an optimal fusion, mitigating the influence of irrelevant information present in the prompts.

### Training and Inference

**Classification Objective.** We prepend three fused prompts (_i.e_. \(_{s}\), \(_{o}\) and \(_{e}\)) with \(x_{e}\), which is the output from a ViT embedding layer \(f_{e}()\). The extended token sequence is \(x_{p}=[_{c};_{s};_{o};x_{e}]\)

Figure 5: Architecture of object-injected state prompting. Query feature serves as Q, while fused object prompt serves as both K and V.

Then, we feed \(x_{p}\) to a transformer encoder layer \(f_{r}()\) and achieve \(_{s}^{r}\), \(_{o}^{r}\) and \(_{c}^{r}\) for classifying state, object and composition classes, respectively. We estimate the probability via a classifier \(_{}()\): \(p( x)=_{}(_{}^{r})\). For each image \(x\), we denote its ground-truth distribution over labels with \(q( x)\). When \(\) is consistent with the ground truth, then \(q( x)=1\); otherwise, \(q( x)=0\). As a result, the cross entropy (CE) loss used for classification objective is:

\[_{CE}^{}=-_{=1}^{}q( x) p(  x),[||,||, ||],\] (7)

where \(\) represents the number of classes. However, the model optimized with a standard CE loss is easily affected by noisy samples during training. Instead, we advocate using a symmetric cross entropy loss (SCE) , which incorporates an additional term called reverse cross entropy (RCE), to mitigate the impact of noisy data. Contrary to CE, the formula for RCE loss is defined as:

\[_{RCE}^{}=-_{=1}^{}p( x) q(  x),\{s,o,c\}.\] (8)

Then, the SCE loss combines two loss terms by \(_{SCE}^{}=_{CE}^{}+_{RCE}^ {}\), where \(\) is a hyper-parameter that controls the weight of the RCE term. As a result, the whole SCE loss becomes \(_{SCE}=_{SCE}^{c}+(_{SCE}^{s}+_{SCE}^{o})\), where \(\) adjusts the weights between primitives and compositions.

**Total Loss.** The total loss for training the whole CompILer model is:

\[_{total}=_{1}_{inter}+_{2}_{intra }+_{3}_{sur}+_{SCE},\] (9)

where \(_{1}\), \(_{2}\), \(_{3}\) are hyper-parameters balancing different terms.

**Inference.** During inference, we incorporate the primitive probabilities to aid the composition probability. Hence, the final probability for composition classification is expressed with:

\[p_{}(c x)=p(c x)+(p(s x)+p(o x)),\] (10)

where \(\) adjusts the probabilities.

## 5 Experiments

### Datasets and Metrics

We conduct experiments on two newly split datasets: Split-Clothing and Split-UT-Zappos as elucidated in Section 3.2. We assess the overall performance on compositions using Average Accuracy (**Avg Acc**) and Forgetting (**FTT**). A higher Avg Acc signifies stronger recognition abilities, while a lower FTT indicates improved resilience against forgetting. Additionally, we provide individual Average Accuracy scores on states and objects, denoted as **State** and **Object** for simplicity. These metrics imply the ability to recognize fine-grained primitives. Furthermore, we calculate the Harmonic Mean (**HM**) between State and Object, _i.e_. \(HM=2\). We provide more emphasis to Avg Acc and HM due to their more comprehensive assessment. Avg Acc encompasses the plasticity and stability [33; 2] and HM provides a holistic evaluation on both state and object.

   Datasets &  &  &  \\  Metrics & Avg Acc(\(\)) & FTT(\(\)) & Avg Acc(\(\)) & FTT(\(\)) & Avg Acc(\(\)) & FTT(\(\)) \\  Upper Bound & 97.02\(\)0.10 & - & 86.71\(\)0.41 & - & - & 68.71\(\)0.41 & - \\  EWC  & 47.89\(\)0.87 & 52.75\(\)0.44 & 37.59\(\)2.06 & 55.70\(\)2.76 & 24.63\(\)0.94 & 61.31\(\)2.29 \\ LwF  & 49.96\(\)0.68 & 44.22\(\)0.53 & 40.15\(\)0.43 & 49.61\(\)0.68 & 30.38\(\)1.41 & 58.15\(\)0.20 \\ iCaRL  & 68.65\(\)0.41 & 31.74\(\)1.89 & 37.78\(\)2.14 & 55.06\(\)3.50 & 31.40\(\)1.96 & 59.65\(\)2.40 \\  L2P  & 80.22\(\)0.41 & 14.23\(\)0.44 & 42.20\(\)2.18 & 50.24\(\)2.76 & 31.65\(\)0.16 & 31.02\(\)6.2 \\ Deep L2P++[43; 33] & 80.55\(\)0.45 & 12.60\(\)1.90 & 42.37\(\)0.65 & 30.10\(\)1.56 & 30.68\(\)0.35 & 32.02\(\)1.96 \\ Dual-Prompt  & 87.87\(\)0.63 & 7.71\(\)0.25 & 43.30\(\)0.19 & 19.41\(\)2.80 & 33.01\(\)1.65 & 24.61\(\)1.11 \\ CODA-Prompt  & 86.35\(\)0.20 & 8.99\(\)0.71 & 43.35\(\)0.29 & 21.76\(\)2.45 & 31.40\(\)0.36 & 30.54\(\)2.63 \\ LGCL  & 87.32\(\)0.10 & 7.58\(\)0.06 & - & - & 33.56\(\)0.31 & **24.37\(\)**0.56 \\ 
**Sim-CompILer** & 88.38\(\)0.08 & 8.01\(\)0.42 & 45.70\(\)0.68 & 20.06\(\)0.62 & 33.30\(\)0.10 & -30.31\(\)0.03 \\
**CompILer** & **89.21\(\)**0.24 & **7.26\(\)**0.60 & **46.48\(\)**0.26 & **19.27\(\)**0.75 & **34.43\(\)**0.07 & 28.69\(\)0.82 \\  

Table 1: Avg Acc and FTT results on Split-Clothing (5 tasks) and Split-UT-Zappos (5 and 10 tasks). The best results are marked in **bold**. All results with standard deviations are averaged over three runs.

### Implementation Details

For a fair comparison with previous works [43; 42; 2; 33], we also employ ViT B/16  pretrained on the ImageNet 1K dataset as the feature extractor and backbone. For multi-pool prompt learning, the size of each pool is set to \(20\), and each prompt has \(5\) tokens. We select top-5 prompts from each pool and generate a fused prompt. During training, we utilize the Adam optimizer  with a batch size of \(16\). The whole ComplLer undergoes training for \(25\) epochs on the Split-Clothing, for \(10\) epochs on the 5-task Split-UT-Zappos, and for \(3\) epochs on the 10-task Split-UT-Zappos. For the Split-Clothing and the 10-task Split-UT-Zappos, we set the learning rate to \(0.03\), while we use a learning rate of \(0.02\) for the 5-task Split-UT-Zappos. Note that, for all the methods, their results are averaged over **three runs** with the corresponding standard deviations reported to mitigate the influence of random factors.

As there are a few hyper-parameters in the model, we conduct a rigorous tuning on them. For instance, we set \(_{}\) to \(\) for all settings. For Split-Clothing, the loss weights \(_{1}\) and \(_{3}\) are set to \(0.1\); \(_{2}\) is set to \(10^{-7}\); \(\) and \(\) for SCE loss are \(0.006\) and \(0.3\), and the parameter \(\) during inference is \(0.5\). For 5-task Split-UT-Zappos, \(_{1}\), \(_{2}\), \(_{3}\), \(\), \(\) and \(\) are set to \(1.0\), \(3 10^{-6}\), \(0.7\), \(0.01\), \(0.7\) and \(0.02\), respectively. For 10-task Split-UT-Zappos, \(_{1}\), \(_{2}\), \(_{3}\), \(\), \(\) and \(\) are set to \(0.5\), \(10^{-7}\), \(0.1\), \(0.05\), \(0.4\) and \(0.03\). We elaborate more details on hyper-parameter analysis in the appendix.

### Compared Baselines

To demonstrate the effectiveness of the proposed method, we compare CompILer with state-of-the-art incremental learning methods, including prompt-free approaches [10; 16; 32] and prompt-based methods [43; 42; 33; 7]. All the methods arehearsal-free except iCaRL . Note that, due to LGCL  relying on CLIP  to achieve language guidance at the task level, it is limited by the length of class names per task. Thereby, LGCL fails to operate on the 5-task Split-UT-Zappos since the total length of class names exceeds the limitation.

To streamline our CompILer, we further implement a simplified version named **Sim-CompILer** and report its results. Sim-CompILer is optimized using cross entropy loss and is comprised solely of multi-pool prompt learning and generalized-mean prompt fusion. In other words, we exclude the object-injected prompting, directional decoupled loss, and reverse cross entropy loss, resulting in a large reduction of hyperparameters to only \(\), \(_{3}\), and \(\).

### Comparison with the State-of-the-arts

The compared results on Avg Acc and FTT are reported in Table 1. Overall, CompILer consistently outperforms all competitors on Avg Acc by a significant margin. For FTT scores, CompILer excels previous methods with 0.32% on the 5-task Split-Clothing and with 0.14% on the 5-task Split-UT-Zappos, while falling behind Dual-Prompt  and LGCL  for the 10-task Split-UT-Zappos. We notice that, the main reason is these methods sacrifice more plasticity for lower forgetting rates. Besides, the number of model parameters in these methods dynamically increases along with more incremental tasks arriving, whereas our CompILer does not rely on imposing task-specific parameters to reduce the forgetting.

We also report the primitives accuracy and their HM in Table 2 and Table 3. Likewise, our method surpasses other methods considerably in terms of State and HM. Interestingly, the prompt-free methods [16; 10; 32] achieve higher accuracy in state prediction than object prediction for Split-Clothing, which is contrary to other results. This is because the states in Split-Clothing are color-related descriptions, which are easier to capture with the help of parameter fine-tuning. The prompt-based methods do not exhibit this phenomenon because their pre-trained backbones are initially

  Datasets &  \\  Metrics & State & Object & HM \\  Upper Bound & 97.44\(\)0.08 & 97.09\(\)0.10 & 97.26\(\)0.08 \\ EWC  & 86.49\(\)0.97 & 52.72\(\)1.30 & 675.50\(\)0.97 \\ LwF  & 87.11\(\)0.66 & 54.57\(\)0.69 & 67.10\(\)0.33 \\ iCaRL  & 91.21\(\)1.05 & 71.70\(\)0.99 & 80.28\(\)0.74 \\ \(\) & 83.03\(\)0.42 & 95.56\(\)0.57 & 88.85\(\)0.16 \\ Dual-Prompt  & 90.77\(\)0.25 & 94.18\(\)0.31 & 92.45\(\)0.20 \\ LGCL  & 91.45\(\)0.20 & 94.87\(\)0.33 & 93.13\(\)0.10 \\ 
**Sim-CompILer** & 91.15\(\)0.10 & 96.32\(\)0.02 & 93.66\(\)0.02 \\
**CompILer** & **91.81\(\)**0.23 & **96.67\(\)**0.01 & **94.18\(\)**0.06 \\  

Table 2: State, Object and HM results on Split-Clothing. The best results are marked in **bold**.

trained for object classification, and are frozen across incremental sessions. As the performance improvements are mainly attributed to the accuracy of state recognition, it suggests that our model enhances the understanding on fine-grained compositionality.

### Ablation Study and Analysis

**Effect of multi-pool prompt learning.** This experiment aims to delineate the contribution of three pools in CompILer. We firstly implement a baseline model with composition prompt pool only. Building upon the baseline, we develop two additional models, which incorporate either object or state prompt pool. As reported in Table 4, the inclusion of primitive prompt pool yields consistent gains over the baseline. Furthermore, the best results are achieved when the model integrates all three pools simultaneously. This experiment signifies the significant necessity of exploiting multiple prompt pools for composition-IL.

**Effect of object-injected state prompting.** To provide insights into object-injected state prompting, we compare three models: None (vanilla model), S\(\)O (state-injected object prompting) and O\(\)S (object-injected state prompting). As shown in Table 4(a), compared to the None model, the S\(\)O exhibits a decrease in all metrics, implying that state prompts may interfere with the selection of object prompts. On the contrary, O\(\)S outperforms the None model as we expect. This phenomenon validates our motivation that state recognition is harder than object recognition, and thereby the former cannot help the latter easily. Yet, it is a promising direction for future research.

**Effect of generalized-mean prompt fusion.** This study aims to study the impact of prompt fusion on CompILer. As shown in Table 4(b), GeM performs better than both max and mean pooling across various metrics. It validates the benefit of GeM on mitigating irrelevant information in the selected prompts. as it may hamper the model's attention on image tokens.

**Effect of loss functions.** As shown in Table 6, we investigate the influence of loss functions used in our model, including directional decoupled loss (\(_{inter}\) and \(_{intra}\)) and symmetric cross entropy loss (\(_{CE}\) and \(_{RCE}\)). The baseline model (the first row) includes all modules but is trained by cross entropy loss only. By adding the RCE loss, the model is equivalent to training with the SCE loss, which help to improve the robustness to noisy labels. The use of either \(_{inter}\) or \(_{intra}\) improves the performance on both datasets, and synchronously applying them witnesses all-around improvements

   &  \\  C & S & O & Avg Acc & FTT(\(\)) & HM \\   ✓ & & & 80.22\(\)0.41 & 14.23\(\)0.44 & 88.85\(\)0.16 \\ ✓ & ✓ & ✓ & 88.10\(\)0.11 & 7.79\(\)0.04 & 93.55\(\)0.04 \\ ✓ & ✓ & & 88.09\(\)0.50 & **7.26\(\)**0.54 & 93.52\(\)0.13 \\ ✓ & ✓ & ✓ & **88.38\(\)**0.08 & 8.01\(\)0.42 & **93.66\(\)**0.02 \\  

Table 4: Ablation study on multi-pool prompt learning with Split-Clothing dataset.

   &  &  \\  Metrics & State & Object & HM & State & Object & HM \\  Upper Bound & 75.10\(\)0.10 & 88.13\(\)0.03 & 81.90\(\)0.06 & 75.10\(\)0.10 & 88.13\(\)0.03 & 81.90\(\)0.06 \\ EWC  & 47.95\(\)1.26 & 76.53\(\)0.91 & 58.90\(\)0.53 & 39.29\(\)2.69 & 67.64\(\)1.97 & 49.69\(\)2.30 \\ LwF  & 53.13\(\)1.08 & 75.48\(\)0.82 & 62.35\(\)0.31 & 38.70\(\)2.33 & 68.90\(\)1.97 & 49.54\(\)1.30 \\ iCaRL  & 51.71\(\)0.95 & 75.03\(\)0.49 & 61.22\(\)0.78 & 38.94\(\)2.01 & 67.10\(\)1.05 & 49.27\(\)1.58 \\ L2P  & 52.20\(\)2.92 & 79.05\(\)0.01 & 62.87\(\)1.61 & 42.66\(\)0.87 & 76.60\(\)0.03 & 54.80\(\)0.55 \\ Dual-Prompt  & 52.25\(\)0.77 & 77.46\(\)0.05 & 62.40\(\)0.34 & 44.34\(\)1.61 & 77.92\(\)0.37 & 56.51\(\)1.11 \\ LGCL  & - & - & - & - & 43.44\(\)0.79 & **78.64\(\)**0.64 & 55.96\(\)0.43 \\
**Sim-CompILer** & 55.93\(\)1.23 & **79.69\(\)**0.06 & 65.72\(\)0.53 & 45.88\(\)0.38 & 75.72\(\)0.67 & 57.14\(\)0.06 \\
**CompILer** & **56.85\(\)**0.34 & 79.56\(\)0.04 & **66.31\(\)**0.15 & **46.27\(\)**1.56 & 76.65\(\)1.19 & **57.69\(\)**0.42 \\  

Table 3: State, Object and HM results on Split-UT-Zappos (5 tasks) and Split-UT-Zappos (10 tasks).

  Dataset &  &  &  \\  Metrics & Avg Acc & FTT(\(\)) & HM \\  None & 88.45\(\)0.10 & 7.93\(\)0.11 & 93.70\(\)0.03 & Max & 84.70\(\)0.64 & 12.24\(\)2.25 & 91.54\(\)0.30 \\ S\(\)O & 88.27\(\)0.02 & 7.99\(\)0.05 & 93.67\(\)0.01 & Mean & 87.80\(\)0.12 & 7.82\(\)0.01 & 93.38\(\)0.03 \\ O\(\)S & **89.21\(\)**0.24 & **7.26\(\)**0.60 & **94.18\(\)**0.06 & GeM & **89.21\(\)**0.24 & **7.26\(\)**0.60 & **94.18\(\)**0.06 \\  

Table 5: Ablative experiments for (a) object-injected state prompting, (b) prompt fusion method.

compared to the baseline. Eventually, we achieve the best results when combing all the loss terms during training.

### Additional Results and Analysis

In order to study the repeatability characteristic in composition-IL, we exhibit more results on Split-Clothing in Fig. 6: in (a), it shows a decreasing trend in composition accuracy along with the introduction of new tasks; however, the green rectangles in (b) and (c) showcase that the accuracy occasionally increases as more tasks are learned. We conjecture the reason is mostly attributed to the re-occurrence of primitive concepts. This forward transfer is critical for incremental learners. We compare the composition predictions between CompILer and L2P  in Fig. 6 (d). CompILer predicts all the images correctly, while L2P makes some mistakes, particularly for state labels. This limitation arises from an excessive focus on the dominant object primitive, while weakening the attention toward state primitive. Fortunately, CompILer relieves the bias toward object classes, and enhances the perception on state classes.

## 6 Conclusion

In this paper, we have proposed a novel task coined compositional incremental learning (composition-IL), which is stumbled by ambiguous composition boundary. To tackle it, we develop a learning-to-prompt model, namely CompILer. Our model exploits multi-pool prompt learning to model composition and primitive concepts, object-injected state prompting to improve the selection of state prompts, and generalized-mean prompt fusion to eliminate irrelevant information. Extensive experiments on two tailored datasets show that CompILer achieves state-of-the-art performance. In the future, it is challenging yet potential to consider reasoning multiple state classes per object.

## 7 Acknowledgments

This work was supported in part by the National Natural Science Foundation of China under Grant Numbers 62102061, 62272083 and 62472066, and in part by the Open Projects Program of State Key Laboratory of Multimodal Artificial Intelligence Systems.

   &  &  \\  \(_{CE}\) & \(_{RCE}\) & \(_{inter}\) & \(_{intra}\) & Avg Acc & FTT(\(\)) & Avg Acc & FTT(\(\)) \\  ✓ & & & & 88.17\(\)0.08 & 8.08\(\)0.27 & 44.83\(\)0.15 & 19.49\(\)2.93 \\ ✓ & ✓ & & & & 88.36\(\)0.37 & 8.33\(\)0.11 & 45.47\(\)0.07 & 20.14\(\)0.43 \\ ✓ & & ✓ & & & 88.32\(\)0.56 & 7.82\(\)0.64 & 45.58\(\)0.04 & 19.64\(\)0.37 \\ ✓ & & & ✓ & & 88.42\(\)0.30 & 8.23\(\)0.06 & 45.62\(\)0.13 & 20.13\(\)0.14 \\ ✓ & & ✓ & ✓ & & 88.61\(\)0.61 & 7.72\(\)0.87 & 46.01\(\)0.69 & 19.50\(\)0.86 \\ ✓ & ✓ & ✓ & ✓ & & **89.21\(\)**0.24 & **7.26\(\)**0.60 & **46.48\(\)**0.26 & **19.27\(\)**0.75 \\  

Table 6: Ablate the loss functions on Split-Clothing and Split-UT-Zappos.

Figure 6: Results and analysis. (a) to (c) show accuracy of CompILer on composition, state, and object for each task in Split-Clothing. The x-axis represents the test stream, and the y-axis denotes the status after training the \(T_{k}\) task. Darker background color indicates higher accuracy. (d) displays some images and their predictions: top row is GT, middle row is CompILer prediction, and bottom row is L2P  prediction. Green indicates correct predictions, while red indicates incorrect predictions.