# NetworkGym: Reinforcement Learning Environments for Multi-Access Traffic Management

in Network Simulation

Momin Haider

UC, Santa Barbara

&Ming Yin

Princeton University

&Menglei Zhang

Intel Labs

&Arpit Gupta

UC, Santa Barbara

&Jing Zhu

Intel Labs

&Yu-Xiang Wang

UC, San Diego

Rest in peace.Most of the work was done while Ming Yin was at UCSB. Correspondence to: my0049@princeton.edu, jing.zhu.ietf@gmail.com, and yuxiangw@ucsd.edu.

###### Abstract

Mobile devices such as smartphones, laptops, and tablets can often connect to multiple access networks (e.g., Wi-Fi, LTE, and 5G) simultaneously. Recent advancements facilitate seamless integration of these connections below the transport layer, enhancing the experience for apps that lack inherent multi-path support. This optimization hinges on dynamically determining the traffic distribution across networks for each device, a process referred to as _multi-access traffic splitting_. This paper introduces _NetworkGym_, a high-fidelity network environment simulator that facilitates generating multiple network traffic flows and multi-access traffic splitting. This simulator facilitates training and evaluating different RL-based solutions for the multi-access traffic splitting problem. Our initial explorations demonstrate that the majority of existing state-of-the-art offline RL algorithms (e.g. CQL) fail to outperform certain hand-crafted heuristic policies on average. This illustrates the urgent need to evaluate offline RL algorithms against a broader range of benchmarks, rather than relying solely on popular ones such as D4RL. We also propose an extension to the TD3+BC algorithm, named Pessimic TD3 (PTD3), and demonstrate that it outperforms many state-of-the-art offline RL algorithms. PTD3's behavioral constraint mechanism, which relies on value-function pessimism, is theoretically motivated and relatively simple to implement.

## 1 Introduction

There exists a general lack of standardized benchmarks for reinforcement learning (RL) in the domain of computer networking. Whereas RL has shown promise in addressing various challenges in computer networking, such as congestion control, routing, and resource allocation, the field lacks widely accepted benchmarks that would facilitate systematic evaluation and comparison of different RL approaches. Hence, we propose _NetworkGym_, a high-fidelity, end-to-end, full-stack network Simulation-as-a-Service framework that leverages open-source network simulation tools, such as ns-3 Henderson et al. (2008). Furthermore, NetworkGym offers a closed-loop machine learning (ML) algorithm development and training pipeline via open-source gym-like APIs. The components of NetworkGym achieve the following objectives:* **Open APIs for ML Training and Data Collection**: The Agent is fully customizable and controlled by the developer. The network simulation Environment is hosted in the cloud. By utilizing the open-source NetworkGym Client and APIs, an Agent can interact with an Environment to collect measurement data and take actions that allow training for the desired use case.
* **Flexibility of Programming Language**: The separation of Agent and Environment provides the freedom to employ different programming languages for the ML algorithm and network simulation. For instance, a Python-based Agent can smoothly interact with a C++ (ns-3) based simulation Environment. This is a critical aspect of our framework, as previous networking frameworks would have required modern ML algorithms to be coded in the same language(s) as the simulation environment.
* **Independent and Modular Deployment**: Such separation also allows the Agent and Environment to be deployed on different machines or platforms, optimized for specific workloads. For example, when training online on-policy algorithms, such as PPO Schulman et al. (2017) and SAC Haarnoja et al. (2018, 2018), it is often critical to parallelize environment instances to accelerate training and improve generalization capability Wijmans et al. (2019); Makoviychuk et al. (2021). This would be difficult to accomplish if the Agent and Environment were coupled. They can also be developed and maintained by different entities. Access to the Environment is controlled through NetworkGym APIs to hide the details of how a network function or feature is implemented from developers.

**Motivation from Computer Networking.** As the mobile industry evolves toward 6G, it is becoming clear that no single access technology will be able to meet the great variety of requirements for human and machine communications. Multi-access traffic management for integrating multiple heterogeneous wireless networks, e.g., Wi-Fi, cellular, satellite, etc., into a virtualized and unified network becomes vital for addressing today's ever-increasing performance requirements and future applications. Recently, the Generic Multi-Access (GMA) protocol has been proposed in the Internet Engineering Task Force (IETF) to address this need Zhu and Zhang (2024), and the 3rd Generation Partnership Project (3GPP) has also developed the access traffic steering, switching, and splitting (ATSSS) feature, which enables simultaneous use of one 3GPP and one non-3GPP connection to deliver data flows ats (2018). We defer more technical details of these protocols to Appendix A.

One effective method for managing multi-access traffic is through traffic splitting between different network types. Specifically, for each user equipment (UE), traffic is allocated between a 3GPP connection (e.g., LTE) and a non-3GPP connection (e.g., Wi-Fi), with the ratio adjusted at frequent intervals, as in Figure 1. It is natural to consider using RL for learning adaptive and data-driven decision policies on the traffic-splitting ratios.

Figure 1: GMA Protocol. A UE interfaces with the GMA gateway over UDP. ”APP” refers to the application layer at the client or server level, ”IP” refers to the Internet Protocol layer, facilitating the addressing and routing of packets, and ”PHY” refers to the physical layer in the network responsible for the actual transmission of data over the network medium. The GMA gateway handles multi-access traffic splitting at the edge.

Applying RL, however, is notoriously hard. One may run _online RL_ on real networks, but the initial decisions made by the algorithms can be suboptimal, leading to poor network traffic splitting and diminished user experience. Notably, in applications such as robotic control over networks, it is critical to ensure high reliability and low packet-loss ratios to maintain operational effectiveness. An alternative is to run _offline RL_ on the logged data from real networks, but data coverage is a big issue. Even if the learned policy from offline RL improves over the baseline, one cannot know for sure until testing it online with real network traffic. Moreover, the networking environment is not static and most challenging scenarios occur in the long tail of the data distribution.

NetworkGym is timely as it allows us to not only evaluate any learned RL policies, but also stress-test them in challenging scenarios. One could also use NetworkGym to simulate the entire workflow of offline RL for policy improvement before deploying the workflow on real networks.

**Frictionless Reproducibility for ML Researchers interested in Computer Networking.** The intended use of NetworkGym is to allow machine learning (especially RL) researchers to evaluate their algorithms on a faithfully simulated environment in computer networking without having to understand the intricate networking protocols and their interactions in a multi-access traffic splitting system. To facilitate "frictionless reproducibility" Donoho (2024), we conduct preliminary experiments on NetworkGym with popular offline RL algorithms and make the code to setup such experiments available. Our results provide the following take-home messages:

* **Offline RL for Policy Improvement.** Offline RL algorithms can effectively improve the performance in networking systems using data collected from three behavioral policies.
* **Transferability of Scientific Advances.** Methods that work well on standard OpenAI gym environments may not work well on networking problems. Comparative advantages of State-of-the-Art algorithms on D4RL Fu et al. (2020) do not _transfer_ to NetworkGym.
* **Details matter.** Seemingly arbitrary choices in the parameterization and state/action representation (e.g., normalization) have more substantial impact than the choice of RL algorithms.
* **Success of principles.** "Pessimism" in offline RL works for networking problems. A more theory-inspired pessimistic bonus is more effective than the popular Behavioral Cloning (BC).

We hope NetworkGym lowers the entry-barrier into computer networking research and enables new collaboration in the emerging research area of machine learning for networking across academia and industry.

## 2 Related Work

**RL-based Network Optimization.** RL has been used for network optimization in a variety of contexts Yang et al. (2024); Mao et al. (2017); Jay et al. (2019); Jamil et al. (2022); Zhang et al. (2023); Xia et al. (2022); Gilad et al. (2019); Boyan and Littman (1993); Wei et al. (2022); He et al. (2017); Liang et al. (2019); Sadeghi et al. (2017). For example, Yang et al. (2024) use offline RL on a mixture of datasets from different behavior policies to maximize throughput via radio resource management. Additionally, Mao et al. (2017) construct a system that can generate adaptive bitrate algorithms to maximize user quality of experience by training a deep RL model on client video player observations. Jay et al. (2019) employ deep RL to solve the congestion control problem, whereas Jamil et al. Jamil et al. (2022) use deep RL to dynamically determine the optimal number of TCP streams in parallel to maximize throughput while avoiding network congestion. Despite the existing works, our use of offline RL for multi-access traffic splitting is novel and the first of its kind.

**RL Benchmarks.** A wide variety of online and offline RL benchmarks have been proposed in the research community in order to properly evaluate the performance and generalization of RL algorithms. Popular online RL benchmarks include the OpenAI Gym Brockman et al. (2016); Atari 2600 games bel (2013), and Mujoco Todorov et al. (2012). These sets of environments offer a diverse selection of tasks to choose from, mostly involving classic control, continuous control of multi-joint bodies, or video game playing with high-dimensional input spaces. Common offline RL benchmarks include D4RL Fu et al. (2020) and RL Unplugged Gulcehre et al. (2020), which provide similar environments to those in the referenced online RL benchmarks. Recent efforts have been made to consolidate these offline RL benchmarks and have also reinforced the finding that the success of offline RL methods strongly depends on the training data distribution Kang et al. (2023). Additionally, Voloshin et al. (2019) introduce the COBS off-policy evaluation benchmarking suite to comprise a much wider variety of environments than simply the Mujoco-style or Atari-style ones. However, none of these benchmarks contains environments that focus on computer networking applications.

**Offline RL.** Most approaches to offline RL involve some form of behavioral constraint or policy regularization to ensure that the actions chosen by the policy don't stray too far from the actions in the dataset for corresponding states Levine et al. (2020); Kostrikov et al. (2021); Kumar et al. (2020); Kostrikov et al. (2021); Yin and Wang (2021); Li et al. (2023). This is used to mitigate the distribution shift between training and testing states. Certain algorithms seek to avoid off-policy evaluation (OPE) altogether, due to the inherent associated high variance which is compounded on each training iteration Brandfonbrener et al. (2021). Other algorithms use a form of divergence constraint to control the resulting behavior policy. For example, Conservative \(Q\)-Learning (CQL) modifies the actor-critic framework by selecting a policy whose expected value under a \(Q\)-function lower-bounds its true value Kumar et al. (2020). Implicit \(Q\)-Learning (IQL) seeks to avoid policy evaluation on unseen actions and instead treats the state value function as a random variable; the value of the best actions at a state can then be estimated by taking the upper expectile of the value function conditioned on the state Kostrikov et al. (2021).

**Offline RL Using Online Algorithms.** Other offline RL methods take advantage of the empirical success of state-of-the-art online RL methods; we include a discussion of some of these algorithms in Appendix B. Additionally, Fujimoto et al. Fujimoto and Gu (2021) propose a minimal extension to the popular online RL algorithm TD3 by augmenting the policy improvement step with a simple behavioral cloning term. We note that while behavioral cloning is one way to prevent learned policies from excessively favoring out-of-distribution (OOD) actions, another possibility is to incorporate some form of pessimism into the \(Q\)-value estimates for these OOD actions. In particular, our work is inspired by that of Yin et al. Yin et al. (2023) in which the authors analyze the Pessimistic Fitted \(Q\)-Learning (PFQL) algorithm and show that in the finite-horizon case, it is provably sample efficient under certain assumptions. Their approach involves computing the Fisher information matrix on the offline dataset with respect to the \(Q\)-function approximator and using that matrix to estimate the uncertainty of any state-action pair. In this way, they are able to compute policies that maximize a lower-bound estimate of the state-action value function, improving the performance of the algorithm in the offline RL setting. In our work, we incorporate this idea of introducing pessimism into the \(Q\)-value estimates of TD3 in a similar way that Fujimoto et al. introduce behavioral cloning to TD3. Specifically, we adjust the policy improvement step to account for uncertainties present in the \(Q\)-values of specific state-action pairs and produce a resulting algorithm we denote as Pessimistic TD3 (PTD3). We introduce PTD3 in Appendix C.

## 3 Problem Setup

**Markov Decision processes.** Let \((,,,p,)\) define a Markov decision process (MDP) where \(\) is the state space, \(\) is the action space, \(:\) is the scalar reward function, \(p:^{}\) is the transition dynamics model where \(^{}\) is a set of probability distributions over \(\), and \(\) is the discount factor. \(\) and \(\) can both potentially be infinite or continuous. Typically, an RL agent chooses actions via a deterministic policy \(:\) or a stochastic policy \(:^{}\) where \(^{}\) is a set of probability distributions over \(\). The goal of an RL agent is to find a policy that maximizes the expected discounted return \(_{}[_{t=0}^{}^{t}r_{t}|s_{0}=s]\) from the starting state distribution. We denote the state-action value function with respect to policy \(\) as \(Q^{}(s,a)=_{}[_{t=0}^{}^{t}r_{t}|s_{0}=s,a _{0}=a]\).

**Offline RL.** In the offline RL setting, we assume that the agent does not have the ability to interact with the environment and instead has access to an offline dataset \(=\{(s_{k},a_{k},r_{k},s^{}_{k})\}_{k=1}^{K}\) collected by some unknown data-generating process (for example, a collection of different behavior policies). This makes the offline RL setting more challenging than the online RL setting. An online RL agent that overestimates the \(Q\)-values at specific state-action pairs can quickly adapt after being punished for taking those actions in the environment, but an offline RL agent does not have the ability to interact with the environment. This leads to the resulting problem of distribution shift in offline RL, which occurs due to extrapolation error in the \(Q\)-function approximators on state-action pairs that are poorly represented by those in the offline dataset.

**Multi-Access Traffic Splitting Environment.** In the Multi-Access Traffic Splitting environment, a predetermined number of UEs are randomly distributed on a 2-dimensional grid. When the environment is first instantiated, each UE is connected to a single LTE base station and the nearest Wi-Fi access point. The location range of the UEs and the locations of the base station and access points may be specified at environment initialization. If the RSSI-based handover is enabled in the NetworkGym environment configuration, then the Wi-Fi access point for each UE will dynamically change during the simulation to whichever has the highest received signal. Each time step in the environment consists of a time interval of 0.1 seconds. During this time interval, traffic-related measurements are taken, such as the one-way-delay and output traffic throughput. The goal of a centralized traffic splitting agent is to strategically split traffic over the Wi-Fi and LTE links, aiming to achieve high throughput and low latency. Within the NetworkGym environment configuration, it is possible to specify parameters that control the nature of the UEs' movement and whether or not they follow a random or deterministic walk.

**Observation Space.** An observation at time \(t\) is \(s(t)=[s_{1}(t),s_{2}(t),...,s_{N_{u}}(t)]\) for \(N_{u}\) users where \(s_{j}(t)\) is a tuple of values for the \(j\)-th UE in the following form: \((l_{},l_{},tp_{},tp_{},tp_{},ovd_{},ovd_{},ovd_{},owd_{},id_{},sr_{},\)

\(sr_{},x,y)\), \(l_{}\) is the UE's link capacity for channel k, \(tp_{}\) is the UE's input traffic throughput, \(tp_{}\) is the UE's output traffic throughput across channel k, \(owd_{}\) is the UE's one-way-delay across channel k, \(owd_{}\) is the UE's maximum one-way-delay across channel k, \(id_{}\) is the UE's current Wi-Fi access point ID, \(sr_{}\) is the UE's splitting ratio for channel k, \(x\) is the x-location of the user, and \(y\) is the y-location of the user.

**Action Space.** An action at time \(t\) takes the form \(a(t)=[a_{1}(t),a_{2}(t),...,a_{N_{u}}(t)]\) for \(N_{u}\) users where \(a_{j}(t)\{,,...,,\}\) is the desired Wi-Fi splitting ratio for the \(j\)-th UE during the next time interval.

**Reward Function.** The immediate reward at time \(t\) is computed as in Equation 1 where \(tp_{i}\) and \(dy_{i}\) are the output traffic throughput and one-way-delay across both channels for the \(i\)-th user during the current time interval, respectively. Furthermore, \(tp_{i,}\) is the sum of link capacities across both channels for the \(i\)-th user during this time interval and we take \(dy_{}\) to be 1000ms, after which a packet is treated as lost. In this way, we normalize the reward function to be invariant to unit-translation and incentivize a learning agent to maximize the average throughput while simultaneously minimizing the average delay across channels. Although this reward function is admittedly somewhat arbitrary, a different reward function can be easily specified by the network administrators in order to satisfy different QoS requirements.

\[r(t)=(}_{i=1}^{N_{u}}}{tp_{i,}})-(}_{i=1}^{N_{u}}}{dy_{ }})\] (1)

## 4 Experiments

**Experimental Setup.** We test PTD3 and other state-of-the-art offline RL algorithms on a simplified configuration of the NetworkGym multi-access traffic splitting environment. The relevant environment configuration file is included in Appendix D. At initialization of each environment, four UEs are randomly stationed 1.5 meters above the \(x\)-axis between \(x=0\) and \(x=80\) meters. From there, they begin to bounce back and forth in the \(x\)-direction at 1 m/s for the entire duration of an episode. The Wi-Fi access points are stationed at \((x,z)=(30,3)\) and \((x,z)=(50,3)\), respectively while the LTE base station lies at \((x,z)=(40,3)\). Figure 2 illustrates this environment setting. Although this setup is deceptively simple and unrealistic due to the relative locations between UEs and access points as well as the degenerate movement of the UEs, it provides a simple enough testing ground for offline RL on the GMA traffic splitting protocol while still containing some amount of dynamic behavior and resource competition between UEs.

Since the multi-access gateway is connected to all four UEs, the gateway can send traffic splitting command messages to each of the UEs in a centralized manner via the GMA protocol while taking into account network information across all UEs. Therefore, we represent the state as a \(14 4\) matrix where we have 14 network measurement values for each user from the previous time interval and we represent the action as a \(1 4\) row-vector, where each element represents the desired traffic splittingratio during the next time interval for a specific user. Although the traffic splitting ratio for each user can only be one of 33 discrete values \((,,...,)\), we treat each element in the action as a continuous real number between 0 and 1 and map it to the closest corresponding discrete value.

**Heuristic Policies.** NetworkGym provides three heuristic policies for traffic splitting and offline data collection, which we denote throughput_argmax, system_default, and utility_logistic. All of these policies operate independently on each UE, without considering coupled interactions between them. For each user, throughput_argmax examines the previous Wi-Fi and LTE link capacities and chooses the traffic splitting ratio to completely favor whichever channel previously had the highest link capacity. For the system_default algorithm, if the UE-specific difference in delay among the Wi-Fi and LTE links exceeds a threshold, traffic over the link with lower delay is gradually increased. If the delay difference among both links is small but packet loss is detected, traffic over the link with a lower packet loss rate is incrementally increased. The final heuristic policy, utility_logistic, computes a Wi-Fi utility \(u_{i,}=(1+tp_{i,})-(1+dy_{i, })\) and the corresponding LTE utility \(u_{i,}\) for each user and then computes the desired traffic splitting ratio for said user as \((u_{i,}-u_{i,})\) where \(()\) is the logistic function. In this way, the traffic splitting ratio favors channels that indicated higher utility during the previous time interval.

**Offline Datasets.** For each heuristic policy, we collect an offline dataset over 64 episodes, each with a different starting configuaration of UEs. Each episode consists of 10,000 steps. We evaluate the offline dataset coverages for each algorithm by computing the minimum eigenvalue of the feature covariance matrix \(=_{s,a}[(s,a)(s,a)^{}]\) where \((s,a)\) is the featurization of the state-action pair Jin et al. (2020, 2021); Zanette et al. (2021); Yin et al. (2022); Nguyen-Tang et al. (2023). We featurize state-action pairs by simply concatenating the flattened state and action vectors together.

The minimum eigenvalue and condition number for each population covariance matrix are illustrated in Table 1. The offline dataset coverage is highest for the utility_logistic algorithm and lowest for the throughput_argmax algorithm. In practice, the throughput_argmax algorithm sends all traffic through the Wi-Fi channel over 99% of the time for each user, with occasional bursts over LTE while the utility_logistic algorithm thrashes back and forth for each user between sending traffic over Wi-Fi and LTE, resulting in a dataset with much higher coverage. The results in Table 1 present a wide range of dataset coverage values, spanning multiple orders of magnitude. This diverse set of benchmarks ensures that offline RL algorithms can be appropriately evaluated. For instance, algorithms trained on datasets with low coverage are expected to adhere closely to the behavior policy due to limited data variety, while those trained on high-coverage datasets have the potential for greater improvement over the behavior policy due to more diverse experiences to learn from.

In Table 3, we evaluate the performance of three heuristic policies, several offline RL algorithms trained on different datasets, and two state-of-the-art online RL algorithms (PPO and SAC) in this environment setting. The online RL algorithms establish a soft upper bound on the returns achievable by offline RL algorithms in our NetworkGym environment setting. For each of the algorithms, we evaluate its performance on 40 evaluation episodes, each of which is 3200 steps. The total return

Figure 2: Environment configuration for offline RL testing (not-to-scale). Here, we randomly initialize four UE’s 1.5 meters above the \(x\)-axis and they move back and forth in the \(x\)-direction between \(x=0\) meters and \(x=80\) meters. The Wi-Fi access point locations are \((x,z)=(30,3)\) and \((x,z)=(50,3)\) while the LTE base station location is \((x,z)=(40,3)\).

per step across all episodes is then averaged and reported; the error bar indicates a 95% confidence interval centered around the mean. The performance across different datasets is then averaged again to produce an average performance across all datasets in the rightmost column. Finally, in Table 5, we examine the performance of the PTD3 algorithm across the different datasets and different values of \(\) where \(=1.0\). We find that setting \(=1.0\) results in the least variance in performance across values of \(\).

## 5 Discussion

**Offline RL Algorithm Performance.** First, we note that of the 7 off-the-shelf offline RL algorithms tested in our NetworkGym environment setting, only 2 of them were able to significantly outperform the average performance of the heuristic baseline algorithms. Furthermore, in the case of both of these algorithms, they were only able to do so when we disabled state normalization based on the offline dataset, a feature that is included by default when training these offline algorithms. Therefore, using the default hyperparameters for every tested off-the-shelf offline RL algorithm, _none_ of these algorithms could significantly outperform the heuristic baseline algorithms on average. Furthermore, in the case of a few of these algorithms, such as EDAC and LB-SAC, the performance across different datasets is erratic, resulting in a significantly _lower_ average performance overall, compared to the heuristic baseline algorithms. While these algorithms are known to exhibit state-of-the-art performance on D4RL-like tasks, it has been noted that the performance of these algorithms in practice is unstable across environments of varying characteristics Tarasov et al. (2024). These

  
**dataset-generating algorithm** & \(_{}()\) & \(()\) \\ 
**throughput\_argmax** & \(4.4 10^{-6}\) & 4,949,213 \\
**system\_default** & \(1.5 10^{-4}\) & 220,682 \\
**utility\_logistic** & \(4.8 10^{-4}\) & 34,827 \\   

Table 1: Offline Dataset Coverage Measurements among Heuristic Policies. \(_{}()\) and \(()\) are the minimum eigenvalue and condition number of the feature covariance matrix, respectively.

    & **thrpt\_argmax** & **system\_default** & **utility\_logistic** & **Average** \\ 
**baseline** & 0.747 \(\) 0.049 & 0.555 \(\) 0.052 & 0.949 \(\) 0.039 & 0.750 \(\) 0.047 \\
**BC (norm)** & 0.749 \(\) 0.047 & 0.825 \(\) 0.042 & 0.749 \(\) 0.047 & 0.774 \(\) 0.045 \\
**BC (no norm)** & 0.751 \(\) 0.049 & 0.433 \(\) 0.054 & 0.946 \(\) 0.039 & 0.710 \(\) 0.047 \\
**CQL (norm)** & 0.749 \(\) 0.047 & 0.749 \(\) 0.047 & 0.749 \(\) 0.047 & 0.749 \(\) 0.047 \\
**CQL (no norm)** & **0.998 \(\) 0.043** & 0.381 \(\) 0.082 & 0.957 \(\) 0.044 & 0.779 \(\) 0.056 \\
**IQL (norm)** & 0.770 \(\) 0.051 & 0.818 \(\) 0.042 & 0.748 \(\) 0.048 & 0.779 \(\) 0.047 \\
**IQL (no norm)** & 0.749 \(\) 0.049 & 0.846 \(\) 0.042 & 0.948 \(\) 0.036 & 0.848 \(\) 0.042 \\
**TD3+BC (norm)** & 0.749 \(\) 0.047 & 0.034 \(\) 0.046 & 0.749 \(\) 0.047 & 0.511 \(\) 0.047 \\
**TD3+BC (no norm)** & 0.778 \(\) 0.047 & 0.906 \(\) 0.049 & 0.863 \(\) 0.038 & 0.849 \(\) 0.045 \\
**EDAC** & 0.336 \(\) 0.285 & -0.888 \(\) 0.034 & 0.913 \(\) 0.027 & 0.120 \(\) 0.115 \\
**LB-SAC** & 0.902 \(\) 0.046 & -0.204 \(\) 0.072 & **1.150 \(\) 0.033** & 0.616 \(\) 0.050 \\
**SAC-N** & 0.838 \(\) 0.052 & 0.817 \(\) 0.035 & 0.699 \(\) 0.026 & 0.785 \(\) 0.038 \\
**PTD3** & 0.746 \(\) 0.050 & **1.013 \(\) 0.039** & **1.079 \(\) 0.040** & **0.946 \(\) 0.043** \\ 
**PPO** & \(\) & \(\) & \(\) & 1.214 \(\) 0.037 \\
**SAC** & \(\) & \(\) & \(\) & 1.104 \(\) 0.037 \\   

Table 2: Offline and Online RL Algorithm Performance Across Multiple Offline Datasets. Each of the first three column headers indicates the baseline algorithm that collected the offline dataset where ”thrpt_argmax” is an alias for throughput_argmax. Each row header (except ”baseline”, ”PPO”, and ”SAC”) is an offline RL algorithm trained on one of three offline datasets. ”baseline” refers to the performance of the original baseline heuristic policies without any offline data collection. ”(norm)” indicates that the algorithm implements state-normalization based on the offline dataset while ”(no norm)” indicates that the algorithm does not. If not specified, the algorithm does not implement state normalization. We use \(=1.0\) and \(=10.0\) in our evaluation of PTD3.

findings strongly suggest that it would be imprudent to deploy such algorithms trained on a similar task into the real world, even if they were trained on datasets collected from _real_ interactions.

Since our implementation of PTD3 is, on average, able to significantly outperform not only the heuristic baseline policies, but also several existing state-of-the-art offline RL algorithms, this suggests that the poor performance across existing algorithms is not due to a lack of coverage across datasets, but rather the lack of diversity and breadth of testing environments for these algorithms. While the D4RL benchmark has become a standard for assessing offline RL performance, it is essential to recognize its limitations. Algorithms that are touted as state-of-the-art based on their performance on D4RL may not generalize well to other, perhaps more complex or varied, scenarios. We have shown that many advanced offline RL algorithms have the potential to fail catastrophically when deployed in different contexts or faced with unfamiliar environments. Therefore, to ensure robustness and reliability, it is crucial to test offline RL algorithms across a wider array of datasets and environments. This broader testing approach helps to uncover potential weaknesses and provides a more comprehensive understanding of an algorithm's capabilities and limitations. We hope that by expanding the scope of testing beyond popular benchmarks like D4RL and RL Unplugged,

    &  \\   & **throughput\_argmax** & **system\_default** & **utility\_logistic** \\ 
0.1 & \(0.744 0.056\) & \(0.878 0.048\) & \(0.816 0.045\) \\
0.3 & \(0.744 0.056\) & \(\) & \(1.044 0.029\) \\
1.0 & \(0.744 0.056\) & \(\) & \(1.015 0.045\) \\
3.0 & \(0.744 0.056\) & \(\) & \(1.022 0.041\) \\
10.0 & \(0.744 0.057\) & \(\) & \(1.083 0.047\) \\
30.0 & \(0.744 0.056\) & \(0.679 0.044\) & \(\) \\
100.0 & \(0.744 0.056\) & \(0.213 0.070\) & \(\) \\
300.0 & \(0.744 0.056\) & \(0.243 0.059\) & \(\) \\   

Table 4: PTD3 Performance where \(=1.0\). For each of the algorithms, we evaluate its performance on 32 evaluation episodes, each of which is 3200 steps. We avoid bolding the throughput_argmax runs, as they all have roughly the same performance.

    & **thrpt\_default** & **delay\_default** & **utility\_default** & **Average** \\ 
**baseline** & 0.747 \(\) 0.049 & 0.555 \(\) 0.052 & 0.949 \(\) 0.039 & 0.750 \(\) 0.047 \\
**BC (norm)** & 0.749 \(\) 0.047 & 0.825 \(\) 0.042 & 0.749 \(\) 0.047 & 0.774 \(\) 0.045 \\
**BC (no norm)** & 0.751 \(\) 0.049 & 0.433 \(\) 0.054 & 0.946 \(\) 0.039 & 0.710 \(\) 0.047 \\
**CQL (norm)** & 0.749 \(\) 0.047 & 0.749 \(\) 0.047 & 0.749 \(\) 0.047 & 0.749 \(\) 0.047 \\
**CQL (no norm)** & **0.998 \(\) 0.043** & 0.381 \(\) 0.082 & 0.957 \(\) 0.044 & 0.779 \(\) 0.056 \\
**IQL (norm)** & 0.770 \(\) 0.051 & 0.818 \(\) 0.042 & 0.748 \(\) 0.048 & 0.779 \(\) 0.047 \\
**IQL (no norm)** & 0.749 \(\) 0.049 & 0.846 \(\) 0.042 & 0.948 \(\) 0.036 & 0.848 \(\) 0.042 \\
**TD3+BC (norm)** & 0.749 \(\) 0.047 & 0.034 \(\) 0.046 & 0.749 \(\) 0.047 & 0.511 \(\) 0.047 \\
**TD3+BC (no norm)** & 0.778 \(\) 0.047 & 0.906 \(\) 0.049 & 0.863 \(\) 0.038 & 0.849 \(\) 0.045 \\
**EDAC** & 0.336 \(\) 0.285 & -0.888 \(\) 0.034 & 0.913 \(\) 0.027 & 0.120 \(\) 0.115 \\
**LB-SAC** & 0.902 \(\) 0.046 & -0.204 \(\) 0.072 & **1.150 \(\) 0.033** & 0.616 \(\) 0.050 \\
**SAC-N** & 0.838 \(\) 0.052 & 0.817 \(\) 0.035 & 0.699 \(\) 0.026 & 0.785 \(\) 0.038 \\
**PTD3** & 0.746 \(\) 0.050 & **1.013 \(\) 0.039** & **1.079 \(\) 0.040** & **0.946 \(\) 0.043** \\ 
**PPO** & \(\) & \(\) & \(\) & 1.214 \(\) 0.037 \\
**SAC** & \(\) & \(\) & \(\) & 1.104 \(\) 0.037 \\   

Table 3: Offline and Online RL Algorithm Performance Across Multiple Offline Datasets. Each of the first three column headers indicates the baseline algorithm that collected the offline dataset where ”thrpt_argmax” is an alias for throughput_argmax. Each row header (except ”baseline”, “PPO”, and ”SAC”) is an offline RL algorithm trained on one of three offline datasets. ”baseline” refers to the performance of the original baseline heuristic policies without any offline data collection. ”(norm)” indicates that the algorithm implements state-normalization based on the offline dataset while ”(no norm)” indicates that the algorithm does not. If not specified, the algorithm does not implement state normalization. We use \(=1.0\) and \(=10.0\) in our evaluation of PTD3.

researchers and practitioners can better gauge the true potential and practicality of their offline RL solutions.

Behavioral Cloning vs. State-Action Value Function Pessimism.In testing PTD3 (\(=1.0\)) on all three offline datasets with varying values of \(\), we note that while the performance on the system_default dataset improves up to a point as \(\) increases then drops off, the performance on the utility_logistic dataset improves substantially even up to values as high as \(=300.0\). In fact, for large enough \(\), the performance of PTD3 on the utility_logistic dataset is comparable to that of the best performing _online_ deep RL algorithm, PPO. This behavior leads us to question whether or not \(Q\)-function pessimism as we've defined it in this work is always comparable to behavioral cloning. To further test this, we compare the performance of behavioral cloning (without offline dataset feature normalization) with PTD3 where the \(Q\)-value maximization term is removed from the policy-update step. In other words, this implementation of PTD3 would be performing pure minimization of the \(Q\)-value uncertainty estimates with respect to state-action pairs, without any regard for how high those \(Q\)-values are. The results are illustrated in Table 6. Interestingly, we note that the performance of this purely pessimistic PTD3 implementation is significantly higher than that of behavioral cloning when both are trained on the utility_logistic offline dataset, while the two implementations are comparable in the case of the other two datasets. This is reflective of a fundamental difference between behavioral cloning and \(Q\)-value uncertainty minimization: while the objective of a behavioral cloning agent is to pointwise match the agent's actions to those chosen by the behavior policy, the objective of the uncertainty-minimizing agent is to choose actions that minimize the uncertainty in the \(Q\)-function estimates by considering the associated variance in \(Q\)-values.

Limitations.Several limitations exist in our current approach. First, the NetworkGym environment simulation setup that we use in all experiments assumes a fixed number of UEs. Consequently, the addition or removal of even a single UE necessitates retraining the online or offline algorithms from scratch, given the current formulation of the MDP. Additionally, our simulation setting incorporates unrealistic degenerate movement patterns for UEs, which may not accurately reflect real-world dynamics. Finally, one of the major limitations of PTD3 is the constraint on the parameter size of the \(Q\)-networks: if the \(Q\)-networks each have \(d\) parameters, then \(d d\) space is required to store \(}_{t}\) in memory. As a result, in order for gradient-related computations to fit on our 12 GB NVIDIA

  
**Offline Dataset** & **BC (no norm)** & **PTD3** (\(=``"\)) \\ 
**throughput\_argmax** & \(0.751 0.055\) & \(0.770 0.055\) \\
**system\_default** & \(0.432 0.063\) & \(0.547 0.059\) \\
**utility\_logistic** & \(0.948 0.042\) & **1.252 \(\) 0.079** \\   

Table 6: Comparison between Behavioral Cloning and \(Q\)-function pessimism. We evaluate each offline RL algorithm across 32 evaluation episodes, each of which is 3200 steps. In this implementation of PTD3, we set \(=1.0\), set \(=1.0\), and manually remove the \(Q\)-value maximization term from the policy-update step to simulate \(=\).

    &  \\   & **throughput\_default** & **delay\_default** & **utility\_default** \\ 
0.1 & \(0.744 0.056\) & \(0.878 0.048\) & \(0.816 0.045\) \\
0.3 & \(0.744 0.056\) & \(\) & \(1.044 0.029\) \\
1.0 & \(0.744 0.056\) & \(\) & \(1.015 0.045\) \\
3.0 & \(0.744 0.056\) & \(\) & \(1.022 0.041\) \\
10.0 & \(0.744 0.057\) & \(\) & \(1.083 0.047\) \\
30.0 & \(0.744 0.056\) & \(0.679 0.044\) & \(\) \\
100.0 & \(0.744 0.056\) & \(0.213 0.070\) & \(\) \\
300.0 & \(0.744 0.056\) & \(0.243 0.059\) & \(\) \\   

Table 5: PTD3 Performance where \(=1.0\). For each of the algorithms, we evaluate its performance on 32 evaluation episodes, each of which is 3200 steps. We avoid bolding the throughput_argmax runs, as they all have roughly the same performance.

TITAN Xp, we were required to use MLP critics with two hidden layers of 64 neurons each instead of hidden layers of 400 and 300 neurons as TD3+BC uses.

**Future Work.** In future work, we plan to explore methods to appropriately featurize multiple UEs to allow for dynamic changes in their number without requiring retraining any algorithms. This will involve rethinking the current MDP formulation to accommodate a variable number of UEs more flexibly. We also aim to incorporate more complex movement patterns for UEs, such as random walks, to gain a better understanding of how our tested algorithms generalize to these settings. In addition to the previously mentioned areas, potential opportunities exist to enhance the performance of our PTD3 algorithm. The use of GPUs with larger memory capacities would enable us to use larger critic network architectures for PTD3. Additionally, in this work, we primarily explore estimating \(_{t}\) using an exponentially-weighted moving sum with low variance (\(=1.0\)); this is because as \(\) changes, training with a high variance estimator of the Fisher information matrix across timesteps makes it difficult to properly evaluate the effect of \(\).

## 6 Conclusion

In this work, we present NetworkGym, a high-fidelity gym-like network environment simulator that facilitates multi-access traffic splitting. NetworkGym seamlessly aids in training and evaluating online and offline RL algorithms on the multi-access traffic splitting task in simulation. In our simulated experiments, we demonstrate that existing state-of-the-art offline RL algorithms fail to significantly outperform heuristic policies on this task. This highlights the critical need for a broader range of benchmarks across multiple domains for offline RL algorithm evaluation. On the other hand, our proposed PTD3 algorithm significantly outperforms not only heuristic policies, but also many state-of-the-art offline RL algorithms trained on heuristic-generated datasets. These findings pave the way for more effective offline RL algorithms and demonstrate the potential of PTD3 as a strong contender among existing solutions. Future research should consider evaluating offline RL algorithms on networking-specific tasks alongside other benchmarks to foster the development of more robust and versatile solutions.