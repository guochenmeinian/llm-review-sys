# Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders

Jan Dubinski

\({}^{1,2,}\)1

 Stanislaw Pawak

\({}^{1}\)

 Franzziska Boenisch

\({}^{4}\)2

**Tomasz Trzcinski**

\({}^{1,2,3}\)

**Adam Dziedzic**

\({}^{4}\)3

\({}^{1}\)Warsaw University of Technology

\({}^{2}\)IDEAS NCBR

\({}^{3}\)Tooploox

\({}^{4}\)CISPA Helmholtz Center for Information Security

###### Abstract

Machine Learning as a Service (MLaaS) APIs provide ready-to-use and high-utility encoders that generate vector representations for given inputs. Since these encoders are very costly to train, they become lucrative targets for model stealing attacks during which an adversary leverages query access to the API to replicate the encoder locally at a fraction of the original training costs. We propose _Bucks for Buckets (B4B)_, the first _active defense_ that prevents stealing while the attack is happening without degrading representation quality for legitimate API users. Our defense relies on the observation that the representations returned to adversaries who try to steal the encoder's functionality cover a significantly larger fraction of the embedding space than representations of legitimate users who utilize the encoder to solve a particular downstream task. B4B leverages this to adaptively adjust the utility of the returned representations according to a user's coverage of the embedding space. To prevent adaptive adversaries from eluding our defense by simply creating multiple user accounts (sybils), B4B also individually transforms each user's representations. This prevents the adversary from directly aggregating representations over multiple accounts to create their stolen encoder copy. Our active defense opens a new path towards securely sharing and democratizing encoders over public APIs.5

## 1 Introduction

In model stealing attacks, adversaries extract a machine learning model exposed via a public API by repeatedly querying it and updating their own stolen copy based on the obtained responses. Model stealing was shown to be one of the main threats to the security of machine learning models in practice . Also in research, since the introduction of the first extraction attack against classifiers , a lot of work on improving stealing , extending it to different model types , and proposing adequate defenses  has been put forward. With the recent shift in learning paradigms from supervised to self supervised learning (SSL), especially the need for new defenses becomes increasingly pressing. From an academic viewpoint, the urge arises because it was shown that SSL models (_encoders_) are even more vulnerable to model stealing  than their supervised counterparts. This is because whereas supervised models' output is low dimensional, _e.g.,_ per-class probabilities or pure labels, SSL encoders output high-dimensional representation vectors that encode a larger amount of information and thereby facilitate stealing. In addition, from a practical industry's viewpoint, defenses are required since many popular API providers, such as Coherence, OpenAI, or Clarify  already expose their high-value SSL encoders via APIs to a broad range of users.

Most of the current defenses against encoder stealing are _reactive, i.e.,_ they do not actively prevent the stealing but rather aim at detecting it by adding watermarks to the encoder  or performing dataset inference to identify stolen copies . Since at the point of detection, the damage of stealing has already been inflicted, we argue that reactive defenses intervene too late and we advocate for _active_ defenses that prevent stealing while it is happening. Yet, active defenses are challenging to implement because they not only need to prevent stealing but also should preserve the utility of representations for legitimate users. The only existing active defense against encoder stealing  falls short on this latter aspect since it significantly degrades the quality of representations for all users.

To close the gap between required and existing defenses, we propose _Bucks for Buckets (B4B)_, the first active defense against encoder stealing that does not harm utility for legitimate users. B4B leverages the observation that the representations returned to adversaries who try to steal the encoder's functionality cover a significantly larger fraction of the full embedding space than representations of legitimate users who utilize the encoder to solve a particular downstream task. To turn this observation into a practical defense, B4B is equipped with three modular building blocks: (1) The first building block is a tracking mechanism that continuously estimates the fraction of the embedding space covered by the representations returned to each user. The intuition why this is relevant is that by covering large fractions of the embedding space, the representations will suffice for an adversary to reproduce the encoder's functionality, _i.e.,_ to successfully steal it. (2) B4B's second building block consists of a cost function to translate the covered fraction of the embedding space into a concrete penalty. We require this cost function to significantly penalize adversaries trying to steal the model while having only a minimal effect on legitimate users. (3) The third building block contains transformations that can be applied to the representations on a per-user basis to prevent adaptive attackers from circumventing our defense by creating multiple user accounts (sybils) and distributing their queries over these accounts such that they minimize the overall cost. We present the different building blocks of B4B in Figure 1.

While B4B's modularity enables different instantiations of the three building blocks, we propose a concrete end-to-end instantiation to showcase the practicability of our approach. To implement tracking of the covered embedding space, we employ _local sensitive hashing_ that maps any representation returned to a given user into a set of hash _buckets_. We base our cost function (_i.e.,_ the _"**bucks**"_) on utility and make B4B add noise to the representations with a magnitude that increases with the number of buckets occupied by the given user. While the scale of noise added to legitimate users'

Figure 1: **Overview of B4B. In the upper part, we present our B4B framework that consists of three modular building blocks: (1) A coverage estimation to track the fraction of embedding space covered by the representations returned to each user, (2) a cost function that serves to map the coverage to a concrete penalty to prevent stealing, and (3) per-user transformations that are applied to the returned representations to prevent sybil attacks. In the lower part, we present a concrete instantiation of B4B and the operation flow of our defense: The API calculates representations for the incoming queries. We instantiate the coverage estimation with local sensitive hashing and estimate the covered space as the fraction of _hash buckets_ occupied. We calibrate the costs by adding noise to the representations according to the coverage. We apply a set of transformations on a per-user basis. The noised and transformed representations are returned to the user.**

representations does not harm their downstream performance due to their small embedding space coverage, the representations returned to an adversary become increasingly noisy--significantly degrading the performance of their stolen encoder. Finally, we rely on a set of transformations (_e.g._, affine transformations, shuffling, padding) that preserve downstream utility . While, as a consequence, legitimate users remain unaffected by these transformations, adversaries cannot directly combine the representations obtained through different sybil accounts anymore to train their stolen copy of the encoder. Instead, they first have to remap all representations into the same embedding space, which we show causes both query and computation overhead and still reduces the performance of the stolen encoder.

In summary, we make the following contributions:

1. We present B4B, the first active defense against encoder stealing that does not harm legitimate users' downstream performance. B4B's three building blocks enable penalizing adversaries whose returned representations cover large fractions of the embedding space and prevent sybil attacks.
2. We propose a concrete instantiation of B4B that relies on local sensitive hashing and decreases the quality of representations returned to a user once their representations fill too many hash buckets.
3. We provide an end-to-end evaluation of our defense to highlight its effectiveness in offering high utility representations for legitimate users and degrading the performance of stolen encoders in both the single and the sybil-accounts setup.

## 2 Related Work

**Model Extraction Attacks.** The goal of the model extraction attacks is to replicate the functionality of a victim model \(f_{v}\) trained on a dataset \(D_{v}\). An attacker has a black box access to the victim model and uses a stealing dataset \(D_{s}=\{q_{i},f_{v}(q_{i})\}_{i=1}^{n}\), consisting of queries \(q_{i}\) and the corresponding outputs \(f_{v}(q_{i})\) returned by the victim model, to train a stolen model \(f_{s}\). Model extraction attacks have been shown against various types of models including classifiers [24; 40] and encoders [16; 36].

**Self Supervised Learning and Encoders.** SSL is an increasingly popular machine learning paradigm. It trains encoder models to generate representations from complex inputs without relying on explicit labels. These representations encode useful features of a given input, enabling efficient learning for multiple downstream tasks. Many SSL frameworks have been proposed [9; 10; 12; 22; 23; 44]. In our work, we focus on the two popular SSL vision encoders, namely SimSiam  and DINO , which return high-quality representations that achieve state-of-the-art performance on downstream tasks when assessed by training a linear classifier directly on representations. SimSiam trains with two Siamese encoders with directly shared weights. A prediction MLP head is applied to one of the encoders \(f_{1}\), and the other encoder \(f_{2}\) has a stop-gradient, where both operations are used for avoiding collapsing solutions. In contrast, DINO shares only architecture (not weights) between a student \(f_{1}\) and a teacher model \(f_{2}\), also with the stop-gradient operation, but not the prediction head. While SimSiam uses convolutional neural networks (CNNs), DINO also employs vision transformers (ViTs). Both frameworks use a symmetrized loss of the form \(g(f_{1}(x_{1}),f_{2}(x_{2}))+g(f_{1}(x_{2}),f_{2}(x_{1}))\) in their optimization objectives, where \(g(,)\) is negative cosine similarity for SimSiam and cross-entropy for DINO. SimSiam and DINO's similarities and differences demonstrate our method's broad applicability across SSL frameworks. More details can be found in Appendix E.

**Stealing Encoders.** The stealing of SSL encoders was shown to be extremely effective [16; 29; 36]. The goal of extracting encoders is to maximize the similarity of the outputs from the stolen local copy and the original representations output by the victim encoder. Therefore, while training the stolen copy, the adversary either imitates a self-supervised training using a contrastive loss function, _e.g._, InfoNCE  or SoftNN  or directly matches both models' representations via the Mean Squared Error (MSE) loss. To reduce the number of queries sent to the victim encoder, the attack proposed in  leverages the key observation that the victim encoder returns similar representations for any image and its augmented versions. Therefore, a given image can be sent to the victim while the stolen copy is trained using many augmentations of this image, where the representation of a given augmented image is approximated as the one of the original image produced by the victim encoder.

**Defending Encoders.** Recently, watermarking [7; 25; 42] methods have been proposed to detect stolen encoders [14; 16; 43]. Many of these approaches use downstream tasks to check if a watermark embedded into a victim encoder is present in a suspect encoder. Dataset inference  is another type of encoder ownership resolution. It uses the victim's training dataset as a unique signature, leveraging the following observation: for a victim encoder trained on its private data as well as for its stolen copies, the distribution of the representations generated from the victim's training data differs from the distribution of the representations generated on the test data. In contrast, for an independently trained encoder, these two distributions cannot be distinguished, allowing the detection of stolen copies . However, all the previous methods are _reactive_ and aim at detecting the stolen encoder instead of _actively_ preventing the attack. The only preliminary active defenses for encoders were proposed by [16; 29]. They either perturb or truncate the answers to poison the training objective of an attacker. These operations were shown to harm substantially the performance of legitimate users, which renders the defense impractical. In contrast, our B4B has negligible impact on the quality of representations returned to legitimate users.

## 3 Actively Defending against Model Stealing with B4B

B4B aims at actively preventing model stealing while preserving high-utility representations for legitimate users. Before introducing the three main building blocks of B4B, namely (1) the estimation of embedding space coverage, (2) the cost function, and (3) the transformation of representations (see Figure 1), we detail our threat model and the observation on embedding space coverage that represents the intuition behind our approach.

### Threat Model and Intuition

Our setup and the resulting threat model are inspired by public APIs, such as Cohere, OpenAI, or Clarify [1; 2; 3] that expose encoders to users through a pre-defined interface. These encoders are trained using SSL on large amounts of unlabeled data, often crawled from the internet, and therefore from diverse distributions. We notice that to provide rich representations to multiple users, the training dataset of the encoder needs to be significantly more diverse than the individual downstream tasks that the users query for representations. For instance, if the encoder behind the API is trained on the ImageNet dataset, then the legitimate users are expected to query the API for downstream tasks, such as CIFAR10 or SVHN. Similarly, if the encoder is trained on CIFAR10, the expected downstream tasks are MNIST or Fashion MNIST. Yet, in the design of our defense, we consider adversaries who can query the encoder with arbitrary inputs to obtain high-dimensional representation vectors from the encoder. Our defense is independent of the protected encoder's architecture and does not rely on any assumption about the adversary's data and query strategy.

We argue that even in this restricted setup, our defense can distinguish between adversaries and legitimate users by analyzing the distribution of representations returned to them. In Figure 2, by using PCA to project representations for different datasets to a two-dimensional space, we visualize that representations for different downstream tasks cluster in _disjoint_ and _small sub-spaces_ of the full embedding space. The representations were obtained from a SimSiam encoder originally trained on ImageNet (we observe similar clustering for DINO shown in Appendix F). As a result, legitimate users can be characterized by their representations' small coverage of the embedding space. In contrast, the adversary does not aim at solving a particular downstream task. They instead would want to obtain representations that cover large fractions of the embedding space. This enables reproducing the overall functionality of the encoder (instead of only learning some local task-specific behavior). Indeed, it has been empirically shown by prior work, such as , that stealing with multiple distributions, _e.g.,_ by relying on the complex ImageNet dataset, yields higher performance of the stolen encoder on various downstream tasks than stealing with a downstream dataset, such as CIFAR10. As a result, intuitively, we can identify and penalize adversaries based on their coverage of the embedding space, which will be significantly larger than the coverage of legitimate users. We leverage this intuition to build our B4B defense and present our three main building blocks in the following sections.

Figure 2: **Representations from Different Tasks Occupy Different Sub-Spaces of the Embedding Space. Presented for Fashion-MNIST, SVHN, CIFAR10, and STL10.**

### Building Block 1: Coverage Estimation of the Embedding Space

The first building block of our B4B serves to estimate and continuously keep track of the fraction of the embedding space occupied by any given user. Let \(\) denote our embedding space of dimension \(s\), further, let \(U\) be a user with a query dataset \(D=q_{1},,q_{n}\) and let \(f_{v}:\) be our protected victim encoder that maps data points from the input to the embedding space. Assume user \(U\) has, so far, queried a subset of their data points \(q_{1},,q_{j}\) with \(j n\) to the encoder and obtained the representations \(r_{1},,r_{j}\) with each \(r_{i}^{s}\). We aim to estimate the true fraction of the embedding space \(^{U}_{f}\) that is covered by all returned representations \(r_{1},,r_{j}\) to user \(U\) and denote our estimate by \(}^{U}_{f}\).

Local Sensitive Hashing.One of the methods to approximate the occupied space by representations returned to a given user is via Local Sensitive Hashing (LSH) . We rely on this approach for the concrete instantiation of our B4B and use it to track per-user coverage of the embedding space. Standard (cryptographic) hash functions are characterized by high dispersion such that hash collisions are minimized. In contrast, LSH hashes similar data points into the same or proximal, so-called _hash buckets_. This functionality is desired when dealing with searches in high-dimensional spaces or with a large number of data points. Formally, an LSH function \(\) is defined for a metric space \(=(M,d)\), where \(d\) is a distance metric in space \(M\), with a given threshold \(T>0\), approximation factors \(f>1\), and probabilities \(P_{1}\) and \(P_{2}\), where \(P_{1} P_{2}\). \(\) maps elements of the metric space to buckets \(b B\) and satisfies the following conditions for any two points \(q_{1},q_{2} M\): (1) If \(d(q_{1},q_{2}) T\), then \((q_{1})=(q_{2})\) (_i.e.,_\(q_{1}\) and \(q_{2}\) collide in the same bucket \(b\)) with probability at least \(P_{1}\). (2) If \(d(q_{1},q_{2}) fT\), then \((q_{1})=(q_{2})\) with probability at most \(P_{2}\).

### Building Block 2: Cost Function Design

Once we can estimate the coverage of an embedding space for a given user \(U\) as \(}^{U}_{f}\), we need to design a cost function \(:^{+}^{+}\) that maps from the estimated coverage to a cost. The cost function needs to be designed such that it does not significantly penalize legitimate users while imposing a severe penalty on adversaries to effectively prevent the encoder from being stolen. The semantics of the cost function's range depend on the type of costs that the defender wants to enforce. We discuss a broad range of options in Appendix C. These include monetary cost functions to adaptively charge users on a batch-query basis depending on their current coverage, costs in the form of additional computation that users need to perform in order to obtain their representations, similar to the proof of work in , costs in the form of delay in the interaction with the encoder behind the API , or costs in form of disk space that needs to be reserved by the user (similar to a proof of space [19; 20]). Which type of cost function is most adequate depends on the defender's objective and setup.

Exponential Cost Functions to Adjust Utility of Representations.In the concrete instantiation of B4B that we present in this work, we rely on costs in the form of the utility of the returned representations. We choose this concrete instantiation because it is intuitive, effective, and can be directly experimentally assessed. Moreover, it is even suitable for public APIs where, for example, no monetary costs are applicable. We adjust utility by adding Gaussian noise with different standard deviation \(\) to the returned representations. Since we do not want to penalize legitimate users with small coverage but make relating for adversaries with growing coverage increasingly prohibitive, we instantiate an exponential cost function that maps from the fraction of hash buckets occupied by the user to a value for \(\). We choose the general form of this function as

\[f_{,,}(}^{U}_{f})=(^{ }^{U}_{f}^{-1}}-1)\] (1)

where \(<1\) compresses the curve of the function to obtain low function values for small fractions of occupied buckets, and then we set a target penalty \(\) for our cost function at a specified fraction of filled buckets \(\). For instance, if we want to enforce a \(\) of \(5\) at \(90\%\) of filled buckets (_i.e.,_ for \(}^{U}_{f}=0.9\)), we would need to set \(=5\) and \(=0.9\).

### Building Block 3: Per-User Representation Transformations against Sybil Attacks

Given that our defense discourages users from querying with a wide variety of data points from different distributions, an adversary could create multiple fake user accounts (sybils) and querydifferent data subsets with more uniform representations from each of these accounts. By combining all the obtained representations and using them to train a stolen copy, the adversary could overcome the increased costs of stealing. To defend against such sybil attacks, we propose individually transforming the representations on a per-user level before returning them. As a result, the adversary would first have to map all the representations to one single unified space before being able to jointly leverage the representations from different accounts for their stolen copy.

Formally, for a given query \(q_{i}\), the protected victim encoder produces a representation \(r_{i}=f_{v}(q_{i})\), which is transformed by a user-specific transformation \(T_{U}(r_{i})\) before being returned to the querying user \(U\). For a new user \(U\), the defense randomly selects the transformation \(T_{U}\) from all possible choices. Note that the randomness is also added on a per-transformation basis, instead of only on the level of selecting the transformations. For example, a permutation of the elements in the output representations should be different for each user.

We formulate two concrete requirements for the transformations. First, they should preserve utility for legitimate users on their downstream tasks, and second, they should be costly to reverse for an adversary.

Utility Preserving Transformations.As a concrete instantiation for our B4B, we propose a set of transformations that fulfill the above-mentioned two requirements: (1) _Affine_ where we apply affine transformations to representations, (2) _Pad_ where we pad representations with constant values, (3) _Add_ where we add constant values at random positions within representations, (4) _Shuffle_ where we shuffle the elements in the representation vectors, and (5) _Binary_ where the original representations are mapped to binary vectors relying on a random partitioning of the representation space. To preserve the full amount of information contained in the original representations, in our binary transformations, we tune the length of binary representations. We visualize the operation of each of these transformations in Appendix C. All these transformations can additionally be combined with each other, which further increases the possible set of transformations applied per user. This renders it impossible for an adversary to correctly guess and reverse the applied representation. Instead, the adversary has to remap the representations over all accounts into a single embedding space in order to unify them and leverage them for training of their stolen encoder copy. We present an exhaustive list of strategies that adversaries can apply for the remapping in Appendix D. All the attack methods reduce to the minimum of remapping between representations of a pair of users, _i.e.,_ they are at least as complex as mapping between two separate accounts. In the next section, we show that our defense already impedes stealing for an adversary with two accounts.

## 4 Empirical Evaluation

We first empirically evaluate our instantiation of B4B's three building blocks and show how to calibrate each of them for our defense. Finally, we provide an end-to-end evaluation that highlights B4B's effectiveness in preserving downstream utility for legitimate users while successfully preventing the stealing by adversaries.

Experimental Setup.We conduct experiments on various kinds of downstream tasks and two popular SSL encoders. To test our defense, we use FashionMNIST, SVHN, STL10, and CIFAR10 as our downstream datasets, each with standard train and test splits. For stealing, we utilize training data from ImageNet and LAION-5B. We rely on encoder models from the SimSiam  and the DINO  SSL frameworks. As our victim encoders, we use the publicly available ResNet50 model from SimSiam trained for 100 epochs on ImageNet and the ViT Small DINO encoder trained for 800 epochs on ImageNet, each using batch size 256. The ViT architecture takes as input a grid of non-overlapping contiguous image patches of resolution \(N\)x\(N\). In this paper, we typically use \(N=16\). The Simsiam encoder has an output representation dimension of 2048, while DINO returns 1536 dimensional representations. We examine the utility of downstream classifiers using SimSiam's or DINO's representations obtained for the respective downstream datasets. To implement LSH, we rely on random projections  that we implement from scratch. For a detailed description of our stealing and downstream training parameters, we refer to Appendix F.

### Local Sensitive Hashing for Coverage Estimation

We first observe that the choice of the total number of hash buckets in the LSH influences the effectiveness of our method. In the extreme, if we have a too large number of buckets, the number of buckets filled will correspond to the number of queries posed by a user which fails to capture that similar representations cover similar sub-spaces of the embedding space, and hence does not serve to approximate the total fraction of the embedding space covered. However, if we have too few buckets, even the representations for simple downstream tasks will fill large fractions of buckets, making it impossible to calibrate the cost function such that it only penalizes adversaries. We experimentally find that for our evaluated encoders, \(2^{12}\) buckets represent a good trade-off. In Appendix F.6, we present an ablation study on the effect of the number of total buckets.

Our evaluation of the LSH to track coverage of the embedding space is presented in Figure 3. We observe that representations returned for standard downstream tasks (FashionMNIST, SVHN, CIFAR10) occupy a significantly smaller fraction of the total number of buckets than complex data from multiple distributions (ImageNet, LAION-5B). We present additional experimental results on measuring the coverage of the representation space in Appendix F.5. Specifically, we show that our method of measuring the embedding space coverage has broad applicability across various encoders and datasets used for pretraining. We further observe that the fraction of buckets occupied by the representations saturates over time. These findings highlight that LSH is successful in capturing the differences between legitimate users and adversaries--even in a low-query regime. Finally, we note that our total number of buckets (\(2^{12}\)) is well calibrated since, over all datasets, it successfully maps multiple representations to the same hash bucket while still filling various fractions of the total number of buckets.

### Calibrating the Cost Function

We experiment with different sets of hyperparameters to instantiate the cost function from Equation (1) in the previous section (3.3). As described there, we can calibrate the function (as shown in Figure 4) such that a desired penalty (in the form of a specific \(\)) will be assigned at a certain fraction of buckets occupied. For B4B, we aim at penalizing high embedding space coverage severely. Therefore, we need to identify and optimize for two components: 1) which value of \(\) leads to significant performance drops, and 2) for what

Figure 4: **Cost Function Calibration.**

Figure 3: **Estimating Embedding Space Coverage through LSH on SimSiam Encoder.** We present the fraction of buckets occupied by representations of different datasets as a function of the number of queries posed to the encoder _(left)_. We observe that representations for the downstream datasets (FashionMNIST, SVHN, CIFAR10) occupy a smaller fraction of buckets than representations from the complex ImageNet dataset. Our evaluation of the number of queries whose representations are mapped to the same bucket _(right)_ indicates that our total number of buckets (\(2^{12}\)) is well calibrated for the estimation of covered representation space: over all datasets, we experience hash collisions, _i.e.,_ queries whose representations are mapped to the same buckets. This indicates that our LSH is capable of representing similarities in the representations.

fraction of coverage do we want to impose this significant drop. We base both components on empirical observations. Our first observation is that for our four downstream tasks (FashionMNIST, SVHN, STL10, and CIFAR10), performance drops to 10% (_i.e.,_ random guessing) at roughly \(=0.5\). In Figure 3, we further see that with 50k queries, the downstream tasks occupy \(<30\%\) of the buckets. Ultimately, setting \(\) and \(\) are design choices that an API provider needs to make in order to specify what type of query behavior they want to penalize. As very loose bounds (weak defense), based on our observation, we consider \(=1\) as a high penalty, which leads to \(=1\), and select \(=0.8\). This \(\) corresponds to considering 80% of buckets filled as a too-large coverage of the embedding space. We empirically observe that coverage of 80% of buckets occurs, for example, after around 100k of ImageNet queries. By choosing our target \(\) so loose, _i.e.,_ significantly larger than the observed \(30\%\) for downstream tasks, we offer flexibility for the API to also provide good representations for more complex downstream tasks. Finally, to obtain a flat cost curve close to the origin--which serves to map small fractions of covered buckets to small costs--we find that we can set \(=10^{-6}\). In the Appendix, we evaluate our defense end-to-end with differently parameterized cost functions.

### Assessing the Effect of Transformations

Transformations Do Not Harm Utility for Legitimate Users.We evaluate the downstream accuracy for transformed representations based on training a linear classifier on top of them. To separate the effect of the noise added by our defense from the effect of the transformations, we perform the experiments in this subsection without adding noise to the returned representations. For example, on the CIFAR10 dataset and a SimSiam encoder pre-trained on ImageNet, without any transformations applied, we obtain a downstream accuracy of 90.41% (\(\) 0.02), while, with transformations, we obtain 90.24% (\(\) 0.11) for Affine, 90.40% (\(\) 0.05) for Pad+Shuffle, 90.18% (\(\) 0.06) for Affine+Pad+Shuffle, and 88.78% (\(\) 0.2) for Binary. This highlights that the transformations preserve utility for legitimate users. This holds over all datasets we evaluate as we show in Appendix F.

Adversaries Cannot Perfectly Remap Representations over Multiple Sybil Accounts.To understand the impact of our per-user account transformations on sybil-attack based encoder stealing, we evaluate the difficulty of remapping representations between different sybil accounts. For simplicity, and since we established in Section 3.4 that multi-account attacks reduce to a two-account setup, we assume an adversary who queries from two sybil accounts and aims at learning to map the transformed representations from account #2 to the representation space of account #1. Using more accounts for the adversary causes a larger query overhead and potentially more performance loss from remapping. Our evaluation here, hence, represents a lower bound on the overhead caused to the adversary through our transformations.

We learn the mapping between different accounts' representations by training a linear model on overlapping representations between the accounts. We assess the fidelity of remapped representations as a function of the number of overlapping queries between the accounts. As a fidelity metric for our remapping, we compare the cosine distance between representations (\(a\) and \(b\) defined as: \(1-b}{\|(a\|a\|^{2}\|b\|_{2})}\)). Once the remapping is trained, we evaluate by querying 10k data points from the test dataset through account #1 and then again through account #2. Then, we apply the learned remapping to the latter one and compute the pairwise cosine distances between the representations from account #1 and their remapped counterparts from account #2. Our results are depicted in Figure 5. We show that the largest cosine distance is achieved with the binary transformations, making them the most protective against the adversary since they best prevent perfect remapping, even with an overlap of as many as 10k queries between both accounts. However, these binary transformations also incur the highest drop in accuracy for legitimate users. The defender has the possibility of selecting their preferred types of transformations between representations taking into account the trade-offs between the effectiveness of the defense and the negative impact on legitimate users.

Figure 5: **Quality of Remappings.**

### End-to-End Stealing of an Encoder under our Defense

We perform an end-to-end study to showcase how our B4B defense affects legitimate users vs adversaries. The hyperparameters for B4B are chosen according to the empirical evaluation of the previous sections with \(2^{12}\) as the number of buckets, \(=1,=0.8,=10^{-6}\) as the hyperparameter of the cost function, and different random affine transformations per-user account. Our main results are presented in Table 1. We observe that instantiating our framework with B4B has a negligible impact on legitimate users while substantially lowering the performance of stolen encoders in the case of single-user and sybil attackers.

**Legitimate Users.** We compare the accuracy of downstream classifiers trained on top of unprotected vs defended encoders. The victim encoder achieves high accuracy on the downstream tasks when no defense is employed. With B4B in place, we observe that across all the downstream tasks, the drop in performance is below 1%. For example, there is only a slight decrease in the accuracy of CIFAR10 from 90.41\( 0.02\)% to 90.24\( 0.11\)%. B4B's small effect on legitimate users stems from the fact that their downstream representations cover a relatively small part of the representations space. This results in a very low amount of noise added to their representations which preserves performance.

**Adversaries.** For adversaries who create a stolen copy of the victim encoder, we make two main observations. The most crucial one is that when our B4B is in place, the performance of the stolen copies over all downstream tasks significantly drops in comparison to when the victim encoder is unprotected (grey rows in Table 1). This highlights that our B4B effectively prevents stealing. Our next key observation concerns the number of stealing queries used by the adversary: When no defense is applied, the more queries are issued against the API (_e.g.,_ 100k instead of 50k), the higher performance of the stolen encoder on downstream tasks (_e.g.,_ CIFAR10 or FashionMNIST). In contrast, with B4B implemented as a defense, the performance decreases when using more stealing queries from a single account. This is because with more queries issued, the coverage of embedding space grows which renders the returned representations increasingly noisy and harms stealing performance.

   User & Defense & \# Queries & Dataset & Type & CIFAR10 & STL10 & SVHN & F-MNIST \\  Legit & None & All & Task & Query & 90.41 \( 0.02\) & 95.08\( 0.13\) & 75.47\( 0.04\) & 91.22\( 0.11\) \\ Legit & B4B & All & Task & Query & 90.24\( 0.11\) & 95.05\( 0.1\) & 74.96\( 0.13\) & 91.71\( 0.01\) \\ Attack & None & 50K & Imgnet & Steal & 65.2\( 0.03\) & 64.9\( 0.01\) & 63.1\( 0.01\) & 88.5 \( 0.01\) \\ Attack & B4B & 50K & Imgnet & Steal & **35.72\( 0.04\)** & **31.54\( 0.02\)** & **19.74\( 0.02\)** & **70.01\( 0.01\)** \\ Attack & None & 100K & Imgnet & Steal & 66.1 \( 0.03\) & 63.1 \( 0.01\) & 61.5 \( 0.01\) & 89.0 \( 0.07\) \\ Attack & B4B & 100K & Imgnet & Steal & 12.01\( 0.07\) & 13.94\( 0.05\) & **19.96\( 0.03\)** & **69.63\( 0.07\)** \\ Attack & None & 100K & Ldison & Steal & 64.92\( 0.03\) & 62.51\( 0.03\) & 59.02\( 0.02\) & 84.54\( 0.01\) \\ Attack & B4B & 100K & LAION & Steal & 40.96\( 0.03\) & **40.69\( 0.05\)** & **34.43\( 0.01\)** & 72.92\( 0.01\) \\  Sybil & B4B & 2x50K & Imgnet & Steal & 39.56\( 0.06\) & 38.50\( 0.04\) & 23.41\( 0.02\) & 77.01\( 0.08\) \\ Sybil & B4B & 3x33.3k & Imgnet & Steal & 33.87\( 0.05\) & 38.57\( 0.06\) & 21.16\( 0.01\) & 72.95\( 0.05\) \\ Sybil & B4B & 4x25k & Imgnet & Steal & 33.98\( 0.04\) & 34.52\( 0.08\) & 21.21\( 0.02\) & 70.71\( 0.05\) \\ Sybil & B4B & 5x20K & Imgnet & Steal & 32.65\( 0.05\) & 32.45\( 0.05\) & 29.63\( 0.01\) & 70.12\( 0.08\) \\ Sybil & B4B & 6x16.7k & Imgnet & Steal & 26.62\( 0.04\) & 26.85\( 0.05\) & 24.32\( 0.02\) & 70.51\( 0.04\) \\   

Table 1: **Stealing and Using Encoders With and Without our Defense.** The _USER_ column represents the type of the APIs’ user, where LEGIT denotes a legitimate user, ATTACKER stands for a standard single-account adversary, and SYBIL represents an adversary using two sybil accounts. We use InfoNCE loss for encoder extraction. # Queries stands for the number of queries used for stealing with ALL denoting that the entire downstream dataset was used. The _TYPE_ column expresses how the dataset is used. We follow the stealing setup from . In the first row, we present the undefended victim encoder’s performance as the accuracy for downstream tasks trained on the encoder’s returned representations. In the following row, we show downstream utility for legitimate users when the victim encoder is defended by our B4B. Finally, (in the remaining rows) we assess the performance of stolen encoders on the downstream tasks. Our results highlight that while the performance of the encoder for legitimate users stays high, our B4B renders stealing inefficient with the stolen encoders obtaining significantly worse performance on downstream tasks.

Moreover, we show that B4B can also prevent model stealing attacks with data from a different distribution than the victim encoder's training set. We highlight this in Table 1 where we also use the LAION-5B dataset to steal an ImageNet pre-trained encoder. Our results highlight first that without any defense in place, the LAION dataset is highly effective to extract the ImageNet pre-trained encoder. Second, B4B effectively defends against such attacks, and yields a significant drop in downstream accuracy (on average above 20%) of the stolen encoder.

We also show that this performance drop cannot be prevented by sybil attacks. Therefore, we first consider an adversary who queries from two sybil accounts with 50k queries issued per account and the first 10k queries of both accounts used to learn the remapping of representations between them. When the adversary trains their stolen encoder copy on all the remapped representations, they increase downstream performance over querying from a single account. Yet, their performance is still significantly smaller than the performance of the victim encoder for legitimate users, or the encoder stolen from an undefended victim. Moreover, using more than two sybil accounts further reduces the attack performance as remapping complications accumulate. With ten sybils, remapping leaves no more usable data for training the stolen encoder. This demonstrates our method's advantage: increasing the number of sybil accounts makes encoder extraction impractical due to the growing remapping overhead. Overall, the results highlight that our B4B also successfully prevents sybil attacks.

### Baseline Comparison

Finally, we compare our B4B against the current state-of-the-art baseline defense, namely a static addition of noise to all the returned representations (as proposed in  (Section A.4),). For direct comparability, we evaluate the defense using the our end-to-end experiment setup from the previous section. We present our results in Table 6 in Appendix F.4. Confirming the findings from  our results also show that defenses that rely on static noise have the great disadvantage to harm legitimate users and attackers equally. When adding noise with a small standard deviation of \(=0.1\), we observe a negligible (<1%) performance drop for both attackers and legitimate users. Adding noise with a large standard deviation of, for example, \(=10\), we observe that both legitimate users' and attackers' performance drops between 15% and >40%. In summary, these defenses can either effectively defend stealing (but harm legitimate users), or keep utility for legitimate users high (but not defend well against stealing). In contrast, our B4B is able to provide high performance for legitimate users while effectively defending the encoder against stealing attacks.

## 5 Conclusions

We design B4B a new and modular active defense framework against stealing SSL encoders. All the previous approaches were either reactive, acting after the attack happened to detect stolen encoders, or lowered the quality of outputs substantially also for legitimate users which rendered such mechanisms impractical. We show that B4B successfully distinguishes between legitimate users and adversaries by tracking the embedding space coverage of users' obtained representations. B4B then leverages this tracking to apply a cost function that penalizes users based on the current space coverage, for instance, by lowering the quality of their outputs. Finally, B4B prevents sybil attacks by implementing per-user transformations for the returned representations. Through our experimental evaluation, we show that our defense indeed renders encoder stealing inefficient while preserving downstream utility for legitimate users. Our B4B is therefore a valuable contribution to a safer sharing and democratization of high-utility encoders over public APIs.