# Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers

Haifeng Huang\({}^{1,2}\)\({}^{}\) Yilun Chen\({}^{2}\)\({}^{}\) Zehan Wang\({}^{1}\)\({}^{}\) Rongjie Huang\({}^{1}\) Runsen Xu\({}^{2}\) Tai Wang\({}^{2}\)

**Luping Liu\({}^{1}\) Xize Cheng\({}^{1}\) Yang Zhao\({}^{3}\) Jiangmiao Pang\({}^{2}\) Zhou Zhao\({}^{1,2}\)\({}^{*}\)**

\({}^{1}\)Zhejiang University \({}^{2}\)Shanghai AI Laboratory \({}^{3}\)Bytedance Inc.

{huanghaifeng}@zju.edu.cn

 Equal contribution.

\({}^{*}\) Corresponding author.

###### Abstract

Recent advancements in 3D Large Language Models (LLMs) have demonstrated promising capabilities for 3D scene understanding. However, previous methods exhibit deficiencies in general referencing and grounding capabilities for intricate scene comprehension. In this paper, we introduce the use of object identifiers and object-centric representations to interact with scenes at the object level. Specifically, we decompose the input 3D scene into a set of object proposals, each assigned a unique identifier token, which enables efficient object referencing and grounding during user-assistant interactions. Given the scarcity of scene-language data, we model the scene embeddings as a sequence of explicit object-level embeddings, derived from semantic-rich 2D or 3D representations. By employing object identifiers, we transform diverse 3D scene-language tasks into a unified question-answering format, facilitating joint training without the need for additional task-specific heads. With minimal fine-tuning on all downstream tasks, our model significantly outperforms existing methods on benchmarks including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D. Code has been released at https://github.com/ZZZZCHS/Chat-Scene.

## 1 Introduction

Recent advancements in Large Language Models (LLMs)  have established language as a universal interface for creating general-purpose assistants. This breakthrough has been instrumental in the development of Multi-modal LLMs (MLLMs), which effectively tackle a broad spectrum of multi-modal tasks. While significant strides have been made in 2D MLLMs , current 3D MLLMs still face significant challenges that must be overcome to achieve a general-purpose assistant for 3D scene understanding.

Object referencing and grounding are essential for advanced scene understanding. Object referencing involves a model's precise comprehension of the semantics associated with a user-specified object, while object grounding requires the model's ability to localize a target object within the scene. These capabilities are vital for various 3D scene-language tasks such as dense captioning  and visual grounding . However, current 3D MLLMs lack general referencing and grounding capabilities, often failing in tasks that necessitate precise object referencing or grounding--contrary to the objectives of addressing general-purpose tasks.

Regarding the object referencing capability, several 3D MLLMs  employ additional prompt encoders to comprehend user-specified objects, but they still lack grounding capabilities. The 3DLLM  incorporates location tokens to enable object grounding, a technique validated in the 2D domain . However, this approach underperforms on the 3D grounding benchmark, ScanRefer , compared to traditional expert models. The ineffectiveness of location tokens in the 3D domain primarily arises from the significant data scarcity in the scene-language area. Current 3D scene-language datasets  contain only tens of thousands of grounding instances, a scale much smaller than the million-level datasets used for training 2D MLLMs . Given the exponentially greater complexity of 3D spaces compared to 2D spaces, robust training of location tokens for 3D MLLMs may require substantially more data than is currently used for 2D MLLMs. Therefore, our objective is to explore more efficient methods for object referencing and grounding and to mitigate the impact of data scarcity.

We observe that most existing 3D MLLMs convert the 3D scene into hidden 3D scene embeddings, employing either a Q-Former-based module  or direct projection methods . Such architectures inherently lack the capability to efficiently interpret individual object instances. To address this limitation, we propose a novel approach for **representing and interacting with 3D scenes at the object level** within the language model. This method incorporates two principal designs: (i) referencing 3D scene using object identifiers, and (ii) representing 3D scene using well-trained object-centric representations. The first component offers a unified format for object referencing and grounding, while the second alleviates the requirement for extensive scene-language datasets.

**Reference 3D scene using object identifiers.** Objects play a crucial role in defining and interpreting a scene, as their organization shapes the entire 3D landscape. This intuition is evident in most 3D scene understanding benchmarks, including 3D grounding, VQA, and dense captioning, all of which annotate at the object level. To effectively model scene embeddings at the object level, the entire 3D scene can be decomposed into a set of object proposals via reliable 3D detectors . Importantly, we assign objects with object identifiers--a set of learnable identifier tokens {<OBJ&>}\({}_{k=1,..}\)--to distinguish them during language modeling. This design allows the LLM to reference respective objects using discrete identifier tokens. As the example shown in Figure 1, the chair and the two trash cans are labeled as "<OBJ013>", "<OBJ023>", and "<OBJ032>", respectively. This avoids the text ambiguity that arises from subjective viewing words like "rightmost". Besides, the lengthy description like "the chair located at the southwest corner of the rightmost table" often complicates user-assistant interaction. These identifiers enable efficient object referencing and grounding during user-assistant interactions. By using these identifiers, we convert diverse 3D scene-language tasks into a unified format of question-answering pairs, facilitating joint training without any additional task-specific heads.

**Represent 3D scene using well-trained object-centric representations.** The scene-level representation requires a large amount of paired scene-language data for training, which is generally unaffordable and labor-intensive due to its complex real-world scenarios. To address this challenge, our model represents the scenes using a set of object-level embeddings, which obtain object-centric representations from well-trained 2D and 3D models. Specifically, after obtaining object proposals from prior detectors (either 2D or 3D), we extract the object features using well-trained 3D object-centric representations  or 2D representations . Due to the million-level pre-training, these representations contain abundant semantic and visual cues. Through simple linear layers, we project them into the embedding space of the language model. Combined with the object identifier, the sequences of object-level embeddings are thus constructed into scene embeddings and fed into the LLM. With merely _two_ epochs of fine-tuning on all downstream tasks, extensive experiments on either 3D, 3D+2D, or 2D-only settings demonstrate the effectiveness of our model in various downstream 3D scene understanding tasks.

We perform comprehensive experiments across five representative 3D scene-language datasets, including ScanRefer , Multi3DRefer , Scan2Cap , ScanQA , and SQA3D . Our model consistantly enhances state-of-the-art performance across all these datasets without fine-tuning

Figure 1: An example of using object identifiers during the conversation.

on specific tasks. Notably, it surpasses previous methods by 3.7% (Acc@0.5) on ScanRefer, 14.0% (F1@0.5) on Multi3DRefer, 8.7% (CIDEr@0.5) on Scan2Cap, and 7.7% (CIDEr) on ScanQA.

Our contributions are summarized as follows:

* We propose an enhanced 3D MLLM which models and interacts with 3D scenes at the object level.
* We introduce object identifiers to enable efficient referencing and grounding within 3D scenes. By leveraging these identifiers, we convert diverse 3D scene-language tasks into a unified question-answering format, facilitating joint training without necessitating additional task-specific heads.
* We effectively represent the 3D scene through a sequence of multi-modal object-centric representations derived from well-trained foundation models, which alleviate the impact of scene-language data scarcity.
* Our model significantly enhances state-of-the-art performance across various 3D scene-language datasets without fine-tuning for specific tasks. Extensive experiments on either 3D, 3D+2D, or 2D-only settings demonstrate the effectiveness of our model for 3D indoor scene understanding.

## 2 Related Work

**3D Scene-language Understanding** In the rapidly evolving field of 3D scene understanding, there is an increasing focus on using language to provide both contextual knowledge and query conditions, thus enabling precise interpretation of user intentions. This process, known as "3D scene-language understanding", leverages language to more effectively grasp the intricacies of 3D environments in a manner consistent with human cognition. The primary tasks in this domain include: 1) 3D Visual Grounding [4; 1; 25; 75; 7; 52; 53; 48], which involves locating specified objects within a 3D scene based on textual queries; 2) 3D Dense Captioning [12; 69; 28; 8; 9], which demands proficiency in both localizing and captioning objects densely in the scene; 3) 3D Visual Question Answering [2; 43; 38], which focuses on general scene question answering. Initial efforts concentrated on specialized tasks, resulting in limited generalizability across different 3D scene understanding tasks. Recent initiatives such as 3DJCG  and D3Net  have aimed to unify tasks like 3D visual grounding and dense captioning, leveraging their synergistic benefits to enhance overall model performance. Advances like 3D-VisTA  and 3D-VLP  are working to develop a more general 3D visual-language framework through pre-training techniques for better scene-language alignment. However, despite these models' adeptness at handling various 3D scene tasks, their reliance on task-specific heads limits their adaptability for broader user-assistant interactions.

**Multi-modal Large Language Models.** Recent advancements in large language models (LLMs) have exhibited impressive capabilities in intricate reasoning and interactive dialogues with humans [14; 39; 47; 15]. There is a growing interest in enhancing the scope of LLMs to encompass additional modalities [31; 33; 35; 77; 79; 20; 19; 21; 54; 65; 32; 61; 23; 10]. In the 3D realm, PointLLM  directly maps point clouds into the embedding space of the LLM. Both ImageNet-LLM  and Point-LLM  integrate the 3D modality into LLMs by establishing a joint embedding space among 3D point clouds, images, audio, and text. These models perform well at the object level but encounter difficulties when interpreting complex spatial relationships in 3D scenes. To improve scene understanding, 3D-LLM  incorporates positional embedding and learns location tokens. Nevertheless, it projects 3D features into the input space of pre-trained 2D Vision-Language Models (VLMs). Involving 2D encoders make it difficult to grasp the 3D spatial structure and intricate relationships among objects. Chat-3D  tackles this limitation by directly utilizing 3D scene-text data to align the 3D scene with the LLM, overcoming the challenge of limited data availability through a pre-alignment phase. However, the architectural design of this model limits its focus on specific target objects during interactions. Current 3D MLLMs face challenges in precise object referencing and grounding, limiting their functionality to straightforward tasks. By incorporating object identifiers into the LLM, we significantly enhance the object referencing and grounding capabilities of 3D MLLMs, thereby showing potential for complex real-world applications.

**3D Representation Learning** Recently, numerous efforts have been made to learn discriminative and robust representations for 3D point clouds, which serve as a fundamental visual modality. Approaches such as PointBERT , Point-MAE , Transformer-OcCo , and Point-m2ae  employ self-supervised learning techniques to extract meaningful representations of 3D objects from unlabeled point cloud data. Another set of works [62; 36; 74; 17; 58; 56; 55; 57] seeks to extend representation from other modalities to the 3D domain. For example, ULIP  and OpenShape  construct 3D-image-text triplets to align point clouds within the CLIP  representation space. C-MCR  and Ex-MCR  learn contrastive representations between various modalities, including 3D point clouds. They leverage knowledge from existing MCR spaces to tackle to challenge of lacking paired data. These robust 3D representations effectively capture detailed information about 3D objects. Our approach involves segmenting the 3D scene at the instance level and extracting a set of object features to represent the entire scene.

## 3 Method

### Overview

Our motivation is to facilitate object referencing and grounding for 3D MLLMs while simultaneously addressing the scarcity of scene-language data. We propose representing 3D scenes at the object level by using object identifiers for referencing and employing well-trained, object-centric representations for scene depiction. Section 3.2 delineates the model architecture, which primarily consists of generating a sequence of object-level embeddings to represent the entire scene. Section 3.3 provides illustrations of the prompt template through examples. Lastly, Section 3.4 details the training methodology of our model.

### Model Architecture

As illustrated in Figure 2, our method processes a 3D scene's point cloud by decomposing it into object proposals using a pre-trained detector. We then employ pre-trained 3D and 2D encoders to derive object-centric representations from point clouds and multi-view images, respectively. These representations are subsequently mapped into the token embedding space of a language model. By incorporating \(n\) object identifiers into the language model's vocabulary, we link these identifiers to the corresponding object proposals, thereby facilitating efficient object referencing and grounding during user-assistant interactions. Finally, the scene embeddings, composed of a sequence of object-level embeddings, are input into the LLM.

Figure 2: **Overall model architecture. The model processes a 3D scene’s point cloud input by decomposing it into object proposals via a pre-trained detector. Subsequently, the 3D and 2D encoders are employed to extract object-centric representations. After projection layers, they are combined with object identifiers to form the scene embeddings as a sequence of object-level embeddings, which are then fed into the LLM. The assigned unique identifiers enable efficient object referencing in subsequent interactions.**

**Object Detector.** Given a point cloud from a 3D scene, we decompose it into \(n\) objects using the pre-trained detector Mask3D . Compared to object detection models [11; 46], the instance segmentation model is preferred in this work due to due to its capability to generate accurate masks, which are essential for subsequent projection into 2D masks on multi-view images. The point cloud of the \(i\)-th object is denoted as \(_{i}^{m_{i} 6}\), where \(m_{i}\) represents the number of points in the \(i\)-th object, and the 6D information for each point comprises its 3D coordinates and RGB colors.

**Object Identifiers.** To achieve a localized understanding of 3D scenes, we introduce a set of learnable identifier tokens \(\{\}_{i=1..n}\), designated as object identifiers, into the token vocabulary of the language model. These identifiers are processed by the tokenizer to produce their respective token embeddings \(\{_{i}\}_{i=1..n}\). The identifier tokens are then integrated with the object tokens to establish one-to-one correspondences, thereby enabling object referencing and grounding using identifiers in subsequent interactions.

**Object-level Embeddings.** After extracting object proposals from the 3D scene, we derive object features using well-trained 3D and 2D object representations. Owing to the million-level scale of pre-training, these representations are rich in semantic and visual cues. By employing simple linear layers, we project them into the embedding space of the language model. Together with identifier token embeddings, this process yields object-level embeddings for each object.

_3D Encoder._ The 3D encoder excels in extracting spatial and shape attributes from point clouds. We employ a pre-trained 3D encoder Uni3D  to derive object-centric 3D representations. This embedding processes each object's point cloud \(_{i}\), outputting the feature \(_{i}^{p}\) for each object.

_2D Encoder._ The 2D encoder adeptly extracts semantically rich features from 2D images. We project the point clouds for each object onto multi-view images, creating a sequence of 2D masks. Utilizing a pre-trained DINOv2 , we extract and aggregate local features from all masked regions across the multi-view images of each object, taking into account both mask areas and multi-view information. We opted for DINOv2 over the more common CLIP  due to its superior handling of local features within images. The 2D encoder processes the multi-view images and their corresponding projected masks from each object's point cloud \(_{i}\), generating the visual feature \(_{i}^{v}\) for each object. Details are provided in Appendix A.

_Visual-Language Projectors._ To align the extracted object representations with the language model, we employ a 3D-language projector \(f_{p}()\) and a 2D-language projector \(f_{v}()\) to map the 3D point cloud features and 2D visual features into the token embedding space of the language model. For the \(i\)-th object, these features are represented as token embeddings \(_{i}^{p}\) and \(_{i}^{v}\).

\[_{i}^{p}=f_{p}(_{i}^{p});_{i}^{v}=f_{v}( _{i}^{v}).\] (1)

**Scene Embeddings.** Following the process described above, we obtain an object identifier token embedding \(_{i}\), a 3D object token embedding \(_{i}^{p}\), and a 2D object token embedding \(_{i}^{v}\) for each object. We combine the identifier token embeddings and object token embeddings in an one-to-one correspondence manner to formulate a sequence of object-level embeddings, which represents the constructed scene embeddings and then be fed into the LLM to represent the whole scene.

### Prompt Template

Despite variations in task formulations, both referencing and grounding are unified using object identifiers. As illustrated in Table 1, the system message encodes object information in the scene as a sequence of "<OBJi> <object>", where <OBJi> denotes the identifier token for the \(i\)-th proposal, and <object> serves as the placeholder for object tokens. The language tokenizer converts <OBJi> into its token embedding \(_{i}\) and <object> into the combined object token features \(_{i}^{p}\) and \(_{i}^{v}\). As illustrated by the following interaction, the user can directly employ identifier tokens to reference specific objects, while the assistant uses these tokens in responses to precisely ground target objects.

### Training Strategy

Most existing MLLMs [18; 35; 23] adopt a two-stage training approach, comprising an initial alignment phase to train the projector exclusively, followed by a fine-tuning phase for both the projector and the language model. This method not only demands extra data and extended training duration for alignment but also complicates determining the optimal duration for the initial phase. Consequently, we opt for a single-stage process, concurrently training both the projectors and thelanguage model. In our experiments, we observe that this jointly trained model already exhibits superior performance without the necessity of fine-tuning for specific downstream tasks.

**Training Data** We aggregate essential training data for downstream tasks and standardize it into uniform instruction formats. The downstream tasks encompass 3D visual grounding (ScanRefer & Multi3DRef), 3D dense captioning (Scan2Cap), and 3D visual question answering (ScanQA & SQA3D). We incorporate the training sets from these datasets into our training corpus. Each task is adaptable to a single-turn user-assistant interaction, as illustrated in Figure 3.

**Training Objective** We have unified all tasks into a consistent user-assistant interaction format, and as a result, the sole training loss in the joint-training phase is the Cross-Entropy loss of the language model. The training objective is to optimize the trainable parameters, denoted by \(\), aiming to minimize the negative log-likelihood of the target response sequence \(s^{}\) generated by the assistant. Specifically, given the input prefix sequence \(s^{}\), which encompasses both system messages and user instructions, the loss function is expressed as follows:

\[()=-_{i=1}^{k} P(s_{i}^{}|s_{[1,...,i-1]}^ {},s^{}),\] (2)

where \(k\) is the number of tokens in the response sequence, and \(s_{[1,...,i-1]}^{}\) denotes the sequence of the previous \(i-1\) tokens in the response. The set of trainable parameters \(\) includes two vision-language projectors, newly added \(n\) token embeddings for object identifiers, and the language model itself.

|} 
**System**: A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user’s questions. The conversation centers around an indoor scene: [OBJ001<object> <OBJ002<object>... <OBJ\(n>\) <object>]. \\
**User:** Find the closest trash bin to OBJ013. \\
**Assistant:** There are two trash bins, OBJ023 and OBJ032, both located near the chair. \\  

Table 1: **Prompt template for the language model.**

Figure 3: **Examples of various 3D scene-language understanding tasks**. All the tasks are unified to single-turn question-answering pairs without extra task heads. Object identifiers are used to reference and ground the object during the conversation.

[MISSING_PAGE_FAIL:7]

point cloud data directly, responding to textual instructions and visual prompts. LEO  pioneers an embodied generalist approach by incorporating action tokens. Scene-LLM  merges scene-level and egocentric 3D information, enhancing understanding and reasoning in 3D environments. However, LL3DA, LEO, and Scene-LLM lack grounding capabilities.

**Analysis.** As shown in Table 2, our model surpasses previous methods across almost all metrics without task-specific fine-tuning, suggesting a promising unified framework for 3D scene understanding. For visual grounding tasks, our model boosts the state-of-the-art performance by 3.7% (Acc@0.5) on ScanRefer and 14.0% (F1@0.5) on Multi3DRefer, demonstrating excellent grounding capabilities. For the dense captioning task, we improve the SOTA performance by 8.7% (CIDEr@0.5) on Scan2Cap, highlighting strong object referring and captioning ability. For VQA tasks on ScanQA and SQA3D, which do not require object referencing and grounding, we still achieve consistent performance enhancement, demonstrating improved overall 3D scene understanding and reasoning.

### Ablation Study

**Object Identifiers.** Table 3 shows that the format of object identifiers affects both performance and token cost. For comparing token costs, we consider scenes with hundreds of objects. A straightforward approach is to use plain text for object identifiers such as "Obj001", which is tokenized into four tokens ("Obj", "0", "0", "1"). Including two object tokens (3D & 2D), representing a single object requires six tokens in total. This high token cost makes the approach impractical for real-world scenarios. Thus, we explored using a single token per identifier by adding new tokens to the language's vocabulary. We assess two strategies: employing fixed random Gaussian embeddings ("Gaussian") and using learnable tokens ("Learnable"). The results show that learnable tokens enhance performance and reduce token costs simultaneously. Lowering token costs from \(6N\) to \(3N\) significantly reduces memory usage and accelerates training/inference when handling 3D scenes with numerous objects.

**Multi-modal Object-centric Representations.** We evaluate various methods for retrieving object features and combining features from different sources (3D and 2D), as shown in Table 4. As described in Section 3.2, the 3D and 2D features are derived from the 3D encoder and 2D encoder, respectively. We assess two methods of extracting 2D features: one from a single-view image ("Single") and another from multi-view images ("Multi").

First, we evaluate the performance of using either a single 3D feature or a single 2D feature for the object token. The results show that using a 2D feature derived from multi-view images yields

   Fusion Method & Token Cost & 3D & 2D &  ScanRefer \\ Acc@0.5 \\  &  Multi3DRefer \\ F1@0.5 \\  &  Scan2Cap \\ C@0.5 \\  &  ScanQA \\ CIDEr \\  & 
 SQA3D \\ EM \\  \\   &  & ✓ & – & 41.2 & 43.8 & 64.9 & 80.3 & 53.4 \\  & & ✗ & Single & 32.9 & 37.2 & 65.9 & 83.7 & 53.2 \\  & & ✗ & Multi & 45.8 & 49.1 & 75.7 & **88.2** & 54.4 \\   &  & ✓ & Single & 45.7 & 49.1 & 73.8 & 86.5 & 53.2 \\  & & ✓ & Multi & **50.2** & **52.4** & **77.1** & 87.7 & **54.6** \\   &  & ✓ & Single & 42.4 & 46.8 & 70.9 & 85.9 & 52.9 \\  & & ✓ & Multi & 46.9 & 50.2 & 74.4 & 88.0 & 53.9 \\   

Table 4: **Ablation studies on multi-modal object-centric representations. “Early Fusion” merges object features before language model input, whereas “Separate Token” keeps them distinct. “Single” denotes using a single image to extract 2D feature of an object, while “Multi” uses multi-view images.**

   Identifier Token Type & Token Cost &  ScanRefer \\ Acc@0.5 \\  &  Multi3DRefer \\ F1@0.5 \\  &  Scan2Cap \\ C@0.5 \\  &  ScanQA \\ CIDEr \\  & 
 SQA3D \\ EM \\  \\  Plain Text & 6\(N\) & 47.2 & 49.6 & 73.1 & 84.9 & 53.7 \\ Gaussian & 3\(N\) & 46.1 & 49.4 & 71.7 & 82.5 & 53.4 \\ Learnable & 3\(N\) & **50.2** & **52.4** & **77.1** & **87.7** & **54.6** \\   

Table 3: **Ablation studies on object identifiers. “Plain Text” employs plain text for object numbers, “Gaussian” uses fixed Gaussian embeddings, and “Learnable” learns new identifier tokens. “Token Cost” denotes the total tokens for \(N\) objects, including object identifiers.**better performance than using a 3D feature. This suggests that semantic information from 2D visual contexts is more crucial than spatial information from 3D point clouds. It may also indicate that the pre-trained 2D encoder is more reliable than the pre-trained 3D encoder due to the abundance of 2D image-text data compared to 3D-text data for pre-training.

Next, we assess the combination of both 3D and 2D features using two fusion methods. The first method, named "Separate Token" in the table, involves using two separate object tokens (3D and 2D object tokens) to represent object information. The second method, termed "Early Fusion", combines the 3D and 2D features into a single token for each object. The results indicate that combining 3D and 2D features consistently improves performance compared to using a single 3D/2D feature, highlighting the importance of utilizing both modalities. Fusing 3D and 2D tokens reduces the token cost, while it results in a slight performance drop. This provides an option for situations where the token limit is tight, suggesting that combining multi-modal features into one token is acceptable.

### Experiments with 2D Video Input

In practical applications of 3D scene understanding, acquiring indoor RGB (video) scan is simpler than obtaining a processed 3D point cloud from RGB-D images. We examine our model's ability to adapt to video input (without depth) for 3D indoor scenes based on the ScanNet  dataset. For video input, we use a tracking-based video detector DEVA  to extract object proposals. This process involves detecting objects in each frame and merging these proposals across frames via the tracking module. After extracting objects from the video, we perform the same operations as for the 3D tasks. The grounding results can then be evaluated on video frames with 2D masks.

**Tasks and Metrics.** We assess video grounding and VQA tasks for video input. For video grounding, we use descriptions annotated in ScanRefer and project the ground-truth object's point cloud to 2D masks in video frames, allowing us to compute the IoU between the predicted masks and GT masks in 2D images. Given a video with a frame length of \(L\), the predicted masks are a series of 2D masks denoted as \(\{_{i}^{}^{H W}\}_{i=1 L}\) and the GT masks denoted as \(\{_{i}^{}^{H W}\}_{i=1 L}\), where \(H\) and \(W\) are the height and width of an image, respectively. We concatenate these masks along the temporal axis to obtain a predicted spatial-temporal mask \(}^{}^{H W L}\) and a GT spatial-temporal mask \(}^{}^{H W L}\). We propose calculating the Spatial-Temporal IoU (ST-IoU) between the predicted mask \(}^{}\) and the GT mask \(}^{}\). Thus, similar to the metrics for the grounding task on ScanRefer, we use Acc@0.25 and Acc@0.5 to measure accuracy based on the ST-IoU threshold. For VQA tasks, we use the annotations of ScanQA and SQA3D along with their respective metrics for evaluation.

Figure 4: **Visualization results of video grounding for video input. “GT” denotes the projected 2D masks derived from the ground-truth 3D point cloud mask.**

   &  &  &  \\  & Acc@0.25 & Acc@0.5 & CIDEF & EM \\  Random & 1.4 & 0.5 & – & – \\ Ours & **22.8** & **10.8** & **85.6** & **52.9** \\ Upperbound & 54.9 & 22.2 & – & – \\   

Table 5: **Evaluation results for video input.**

**Performance Analysis.** Table 5 presents the evaluation results of video grounding and VQA tasks. For video grounding, we compute upper bound results to assess the quality of object masks extracted by the video detector. Compared to the upper bound and random results, our method demonstrates strong grounding ability. Visualization results are provided in Figure 4. The second example shows that the tracking-based video detector might lose track of an object after it has been out of sight for a prolonged period. This is a primary reason for the low quality of the extracted object masks. Missing parts of the frames that contain the target object leads to the low Acc@0.5 result of the upper bound. For VQA tasks, we achieve comparable results to those using objects extracted from 3D inputs. This indicates that despite the lower quality of extracted objects from video input, our approach of constructing scene embeddings from sequences of object-level embeddings efficiently enhances overall scene comprehension, thereby improving QA performance on ScanQA and SQA3D.

## 5 Conclusion

To enable efficient object referencing and grounding abilities in 3D MLLMs, this paper proposes modeling and interacting with 3D scenes at the object level. It decomposes the input 3D scene into a set of object proposals assigned with object identifiers. Given the scarcity of scene-language data, we model the scene embeddings as a sequence of explicit object-level embeddings, derived from semantic-rich 2D and 3D representations. By using object identifiers, we transform diverse 3D scene-language tasks into a unified question-answering format. With minimal fine-tuning on all downstream tasks, our model significantly outperforms existing methods across various benchmarks.