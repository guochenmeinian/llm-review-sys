# Class Concept Representation from Contextual Texts for Training-Free Multi-Label Recognition

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The power of large vision-language models (VLMs) has been demonstrated for diverse vision tasks including multi-label recognition with training-free approach or prompt tuning by measuring the cosine similarity between the text features related to class names and the visual features of images. Prior works usually formed the class-related text features by averaging simple hand-crafted text prompts with _class names_ (e.g., _"a photo of [class name]"_). However, they may not fully exploit the capability of VLMs considering how humans form the concepts on words using rich contexts with the patterns of co-occurrence with other words. Inspired by that, we propose _class concept_ representation for zero-shot multi-label recognition to better exploit rich contexts in the massive descriptions on images (e.g., captions from MS-COCO) using large VLMs. Then, for better aligning visual features of VLMs to our class concept representation, we propose context-guided visual representation that is in the same linear space as class concept representation. Experimental results on diverse benchmarks show that our proposed methods substantially improved the performance of zero-shot methods like Zero-Shot CLIP and yielded better performance than zero-shot prompt tunings that require additional training like Tal-DPT. In addition, our proposed methods can _synergetically_ work with existing prompt tuning methods, consistently improving the performance of DualCoOp and Tal-DPT in a training-free manner with negligible increase in inference time.

## 1 Introduction

The goal of multi-label image recognition is to assign all semantic labels (or class names) within an image [10; 44; 48; 11; 27; 33; 31]. Differing from single-label recognition, multi-label recognition addresses a broader range of practical applications such as image retrieval [36; 39], recommendation systems [52; 8], medical diagnosis recognition  and retail checkout recognition [17; 45]. However, one of the challenges in multi-label recognition is the difficulty of collecting full label annotations, which is laborious and prone to missing. To alleviate it, recent works have investigated training with incomplete labels such as partial labels [37; 6; 31; 15; 9] or a single positive label [13; 46].

Recent advances of large vision-language models (VLMs) [32; 2; 22; 25; 47; 49] has demonstrated their strong transferability on various downstream tasks with great performance. Contrastive Language-Image Pretraining (CLIP) achieved impressive performance in zero-shot classification by measuring the cosine similarity between images and class-related hand-crafted text prompts . Fine-tuning VLMs for adapting desired downstream datasets  can further improve performance for targeted tasks, but tuning millions of parameters is usually undesirable due to computation burden and possible forgetting. Prompt tuning has been investigated as an efficient and low-cost training paradigm [54; 53], learning only a few context tokens of VLMs for a given task. In multi-label recognition, prompt tuning with CLIP has been investigated for distinguishing multiple objects in animage [37; 18; 41], mitigating the difficulty of acquiring annotated samples. However, prompt tuning inherently requires labeled data with additional training and may be susceptible to overfitting for context tokens, hindering generalization. The capability of VLMs for label-free and/or training-free classification has been exploited using prompt engineering [32; 34; 50; 4]. However, prompt ensembles by averaging text features from simple hand-crafted prompts (_e.g., "a sketch of [class name]"_) yielded marginal improvements and struggled with multi-label recognition. Thus, the approach of prior works on zero-shot or prompt-tuning based multi-label recognition using _class names_ to obtain class-related text features from VLMs may not use the full capacity of VLMs properly.

Humans form concepts on words from past experience, especially using their patterns of co-occurrence with _other words_[5; 29; 20]. Inspired by this perspective in cognitive neuroscience, we propose a novel approach of exploiting VLMs for multi-label recognition by replacing single _class name_-related hand-crafted prompts with our proposed _class concept_ representation using text descriptions such as "A **person** holding a large pair of scissors," capturing rich contextual information with target class names (e.g., person) as well as related words (e.g., holding, scissors). Our class concept will be constructed from rich contextual descriptions on classes that may contain diverse and realistic patterns of co-occurrence with target class name and other related class names. Then, this novel text features with class concept representation requires aligned visual features with them for multi-label recognition to properly match them with our class concepts. Thus, we propose context-guided visual features to bring VLM's visual features to the same representation domain as our class concept representation by using our sequential attention. See Fig. 1 for the differences of performing multi-label recognition using (a) prior zero-shot approach (ZS-CLIP), (b) our proposed class concepts from text descriptions and (c) our proposed context-guided visual features on the same space as the class concepts. We demonstrated that our proposed methods achieved improved performance on multiple benchmark datasets without additional training (tuning), without additional labels (text-image pairs) and with negligible increase in inference time. Here is the summary of the contributions:

* Proposing a novel class concept representation for training-free multi-label recognition tasks using VLMs from massive text descriptions inspired by how human forms concept on words.
* Proposing a context-guided visual feature, transformed onto the same text feature space as class concepts using sequential attention for better aligning multi-modal features.
* Demonstrating that our methods synergetically improve the performance of ZSCLIP and other state-of-the-art prompt tuning methods with a negligible increase in inference time.

## 2 Related Works

**Multi-label image recognition with CLIP.** Multi-Label Recognition (MLR) aims to identify all semantic labels within an image. However, it is difficult to collect the annotation of multi-label images which involve complex scenes and diverse objects. Recently, prompt tuning with the pre-trained vision

Figure 1: Illustration of our methods applied to zero-shot CLIP (ZSCLIP) . (a\(\)b) Class concept is formed from the text descriptions that contain rich contextual information with relevant class names and other related words, yielding substantially improved performance without aligning with visual features yet. (b\(\)c) Context-guided visual feature is transformed from visual feature so that it is in the same linear space as class concept representation, yielding significantly improved performance.

language model CLIP has been developed to address the high labeling costs of multi-label images in incomplete label setting. Among them, DualCoOp  proposed a novel prompt tuning approach that trains positive and negative learnable contexts with class names in the partially labeled setting. For mitigating data-limited or label-limited issues, Tal-DPT  proposed effective dual-grained prompt tuning method using easily accessible text descriptions. It is worth noting that Tal-DPT used the same text descriptions as ours not for performing training-free multi-label recognition itself, but for label-free prompt tuning by replacing the image features with the contextual text features (text as image) under the conventional framework of multi-label recognition with class name. SCPNet  is designed to leverage the structured semantic prior from CLIP to complement deficiency of label supervision for MLR with incomplete labels. CDUL  proposed unsupervised multi-label recognition through pseudo-labeling using CLIP, alleviating the annotation burden. Even though recent works has demonstrated outstanding performance of multi-label recognition task, they still require tuning costs or labeled dataset to adapt pre-trained CLIP to various downstream tasks. In this work, our method enables training-free and label-free adaptation of CLIP into downstream tasks, utilizing the text descriptions.

**Training-free enhancement with CLIP.** For single-label recognition, recent works has developed the training-free enhancement of CLIP. ZPE  weighted-averaged many prompts by automatically scoring the importance of each prompt in zero-shot manner for improving prompt ensemble technique. CALIP  designed a simple parameter-free attention module for zero-shot enhancement over CLIP without any tuning of model parameter. With few-shot samples, Tip-Adapter  proposed training-free approach for fast adaptation to target task, obtaining the weights of adapter using few-shot samples during inference. Since these methods were originally developed for single-label recognition, it is difficult to be directly applied to multi-label recognition. In multi-label recognition, our method enables training-free enhancement and demonstrated its effectiveness on the benchmark dataset.

## 3 Method

First of all, we propose _class concept_ representation as a training-free approach for multi-label recognition instead of _class name_ by exploiting pre-trained VLM and rich contextual text descriptions. Secondly, we also propose context-guided visual feature that can enhance the alignment of the visual feature of VLM with our novel class concept. Our proposed methods are label-free as well as training-free so that they can be applicable _synergetically_ for most existing VLM-based multi-label recognition methods. The overall pipeline of our method is illustrated in Figure 2.

### Class Concept Representation

Humans form concepts on words from past experience, often using their patterns of co-occurrence with _other words_. For example, the word "apple" does not exist alone, but often comes with the verb "eat" or the noun "basket." However, it may not well associate with other words such as "fly" or "space." Fortunately, we can easily obtain rich contextual text descriptions from various public sources, including captions from benchmark datasets , web crawling and large language models . These text descriptions do not only contain _class names_, but also include _other words_ like class-related verbs and nouns in real-world contexts.

Assume that rich contextual text descriptions were gathered from the public sources that include one or multiple class names. We denote the set of text descriptions as \(Z^{all}=\{z_{1},z_{2},...,z_{M}\}\) where \(z_{i}\) refers to an individual text description. \(M\) denotes the total number of text descriptions across all classes. Note that \(M\) can be dynamically changed at inference since our proposed method does not require additional training, thus can be seen as test-time adaptation. Assuming that the target task uses the class names of person, scissors, clock, building and cake, the examples of the contextual text descriptions from \(Z^{all}\) are as follows:

"**A person** holding a large pair of **scissors**."

"**A clock** mounted on top of a **building** in the city."

"Half of a white **cake** with coconuts on top."

TaI-DPT  used these descriptions with rich contextual information as a surrogate for images to propose a label-free prompt tuning. In this work, we propose to use these descriptions to form concepts on class names to compare with images, so that ways of using them are completely different.

We define the class concept as a vector in the space constructed by the text descriptions as follows. Firstly, the linear space \(\) can be constructed by spanning the VLM's text features from all text descriptions \(z_{i}\) in \(Z^{all}\) using the VLM's text encoder \(_{}(z_{i}) R^{1 D}\), leading to \(=\{_{}(z_{1}),_{ }(z_{2}),,_{}(z_{M})\}\). Secondly, we propose the class concept for a target class name \(c\) as a vector \(t_{c}^{concept}\) in the space \(\) by defining it as follows:

\[t_{c}^{concept}=_{i=1}^{M}w_{c,i}_{c}(z_{i})_{}(z_{i}) R^{1 D}\] (1)

where \(_{c}(z_{i})\) an indicator function such that \(_{c}(z_{i})=1\) if the text description \(z_{i}\) contains the class name \(c\) and \(_{c}(z_{i})=0\) otherwise. The weight \(w_{c,i}\) is assigned to the text feature of each text description within a class \(c\) and it is assumed to be normalized within the class. In this work, we set \(w_{c,i}=1/_{j}^{M}_{c}(z_{j})\) for \( i\), thus will be the same for all \(i\) for each class, which was guided by the prior work on prompt ensembling , demonstrated that the prompt ensembling with equal weights achieved significant performance gains that were comparable to weighted ensembling for single-label recognition. Each class concept can be stored individually or together as a matrix.

Our class concept representation thus consists of various text features including diverse contextual information related to the target class name. For instance, the descriptions for the class name "dog" should contain the target class name as the following examples of the text descriptions:

"**A dog** greets a sheep that is in a sheep pen."

"A woman walks her **dog** on a city sidewalk."

"A **dog** with goggles is in a motorcycle side car."

Note that the descriptions include the target class name (bold) as well as other related words in class-related contexts (underline) as intended. We expect that our novel class concepts will be beneficial for multi-label recognition due to other nouns (other class names) as well as other verbs to better explain the context where the target class name is used. In this work, we obtain the texts from two sources to collect the sufficient contextual text descriptions. The first source is the MS-COCO dataset  that is publicly available and the second source is large language model(_i.e._, GPT-3.5) that can generate the several sentences quickly if the set of class names related to the target task were provided.

### Context-Guided Visual Feature

Our novel class concept representation forms new vectors for diverse class names in the linear space \(\) instead of the embedding space of the VLM where the text and image encoders were relatively well-aligned. Thus, it is expected that the class concept representation and the VLM's visual feature

Figure 2: (a) Overall pipeline of our method. 1) Class concept representation: VLM’s text features from the rich contextual descriptions associated with each class name are used to construct the class concept. 2) Context-guided visual features: VLM’s visual features are sequentially transformed onto the class concept representation space using (b) sequential attention mechanism.

may not be aligned well. Here, we propose context-guided visual feature by transforming the visual features of the VLM onto the same space as the class concept representation \(\) by using our sequential attention with the text descriptions \(Z^{all}\) that were used for class concept construction.

For the target image \(q\) and the VLM's visual encoder \(_{}(q)\), the L2-normalized global visual feature \(f\) is obtained by using \(_{}(q) R^{1 D}\) and the flatten local visual feature \(F R^{HW D}\) is also constructed by using \(_{}(P_{i,j}(q))\) where \(P_{i,j}()\) is an extractor of the \((i,j)\)th patch of the input image. Then, we aim to transform both the global visual feature vector \(f\) and the local visual feature matrix \(F\) onto the same linear space \(\) as our class concept representation. One easy way is to "project" these visual features \(f\) and \(F\) onto the space \(\) by computing the cosine similarity between visual features (\(f\) and the column vectors of \(F\)) and all the text features \(t_{i}=_{}(z_{i}) R^{1 D},i=1,,M\) that constructed \(\). Unfortunately, when the softmax function is applied to the cosine similarity values, they tend to become similar, thus weigh both relevant and irrelevant texts almost equally as illustrated in Figure 3 (a). To address this challenge, we propose sequential attention, applying the softmax function to part of the cosine similarity values by dividing them into \(G\) groups. For the text feature matrix \(T=[t_{1}\;\;t_{2}\;\;\;t_{M}] R^{M D}\), let us determine \(M_{i}\) for \(i=1,,G\) such that \(M=_{i=1}^{G}M_{i}\) and reshape the text feature matrix to be \(T R^{M_{1} M_{G} D}\). Then, propose to sequentially apply the following attention process for \(G\) iterations for estimating both global and local context-guided visual features \(v^{(k)}\) and \(V^{(k)}\), respectively, at the \(k\)th iteration:

\[v^{(k)}=T&k=0,\\ _{_{k}}()^{t}}{_{f}} )v^{(k-1)}&k>0,\] (2)

\[V^{(k)}=T&k=0,\\ _{_{k}}()^{t}}{_{F}})V^{(k-1 )}&k>0,\] (3)

where \(_{f}\) and \(_{F}\) denote the modulation parameters, \(_{M_{k}}\) refers to the softmax operation applied along the dimension corresponding to \(M_{k}\). In this work, we utilize \(v^{(3)}\) and \(V_{(3)}\) to compute classification score. The sequential attention process is illustrated in Figure 2 (b). Figure 3 further demonstrates that our sequential attention is particularly effective in handling massive text descriptions. Without sequential attention, weighted averaging essentially becomes equal averaging.

### Multi-Label Recognition with Class Concepts

**Architecture of model.** Two encoders of CLIP are denoted as \(_{}\) and \(_{}\) for the visual encoder and text encoder, respectively. Following Tal-DPT , we adopt the structure of double-grained prompts (DPT), which has been shown effective for enhancing zero-shot multi-label recognition performance. To obtain visual representations at both coarse-grained and fine-grained levels, we

Figure 3: Softmax values can be used to weigh the relevance with the given image. However, (a) naive attention mechanisms yielded almost equal softmax values, thus may include texts with low relevance. The proposed sequential attention method focuses on a subset of texts most relevant to the test image, thus can transforms visual features to context-guided visual features for multi-label recognition by assigning very high softmax value to the relevant text at index 0 while very low softmax value to the irrelevant text at index 5000.

extract the local visual feature map \(F=_{}(x) R^{HW D}\) is extracted before attention pooling layer, where \(H\) and \(W\) are spatial dimension of visual feature. After attention pooling layer, we obtain the global visual feature \(f R^{1 D}\). Similarly, text features \(t=_{}(z) R^{1 D}\)are obtained by projecting the End-of-Sentence (EOS) token of the text prompt. Thus, we leverage both global and local visual features for multi-label recognition.

**Inference.** Through our sequential attention, we obtain the context-guided visual features \(v^{(G)}\) and \(V^{(G)}\) at both global and local levels, respectively. The similarity score \(S^{glo}\) and \(S^{loc}\) are calculated between the transformed context-guided visual features \(v^{(G)},V^{(G)}\) and the class concepts \(t_{c}^{concept}\) using the cosine similarity \((,)\) as follows:

\[S_{c}^{tot}=S_{c}^{glo}+S_{c}^{loc}=(v^{(G)},t_{c}^{concept})+_{j=1}^{ HW}(s_{c,j}^{loc}) s_{c,j}^{loc}\] (4)

where \(S_{c}^{tot}\) is the classification score for the class \(c\) and \(s_{c,j}^{loc}=([V^{(G)}]_{j},t_{c}^{concept})\) for the class \(c\). For obtaining \(S_{c}^{loc}\), we employ the spatial aggregation over \(HW\).

Finally, we combined ZSCLIP and other prompt tuning methods with our training-free approach through simple logit ensemble. In our experiments, we demonstrate the effectiveness of integrating of our method with existing methods, thereby boosting the performance of multi-label recognition.

## 4 Experiments

### Implementation Details

**Architecture.** We employ CLIP ResNet-50 in the Table. 2 and Table. 3 and ResNet-101 in other experiments as the visual encoders and the CLIP transformer as the text encoder for ZSCLIP, Tal-DPT , DualCoOP  and our method in the paper. In addition, ZSCLIP, Tal-DPT  and our method are based on the double-grained prompt  for both global and local features1.

**Datasets.** For evaluation, we performed multi-label recognition experiments on 3 benchmark datasets. MS-COCO  consists of 80 classes with 82,081 images for training and 40,504 images for test. VOC2007 consists of 20 object classes with 5,011 image for training and 4,952 images for test. NUS-WIDE consists of 81 concepts with 161,789 image for training and 107,859 image for test. For MS-COCO  and VOC2007 , text description source is from MS-COCO . For NUS-WIDE, we gathered the text descriptions from GPT-3.5. Note that there is example of text template for extracting sentence from GPT-3.5 in supplementary.

**Inference Details.** In the paper, we set the total number of text descriptions, denoted as \(M\), for the MSCOCO, VOC2007, and NUS-WIDE at 40,000, 64,000, and 57,600, respectively. Note that we prepared the text embeddings of every text descriptions from CLIP text encoder in advance. We set values of modulation parameter \(\) via validation.

### Evaluation on Limited Data Setting

To evaluate our method, we conducted the experiments in limited data scenarios, including zero-shot and few-shot settings for data-limited cases and partially labeled setting for label-limited cases. Note that only our method provides training-free enhancement of CLIP without tuning cost for multi-label recognition. Therefore, our method can be easily combined with existing methods to improve their performance.

**Evaluation on Zero-Shot Setting.** We performed comparison studies for different zero-shot and fully supervised methods in multi-label image recognition. To evaluate the effectiveness of our method which, we combined our method with existing zero-shot methods, ZSCLIP and Tal-DPT , for zero-shot setting, as shown in Table 1. Additionally, we utilized the fully supervised method, DualCoOp with our method, for zero-shot learning setting (ZSL) as presented in Table 2.

Table 1 summarizes the results of the zero-shot experiment on benchmark datasets. In MS-COCO  and VOC2007 , Tai-DPT  and our method utilized the public language data from MSCOCO . By applying our method to ZSCLIP and Tai-DPT  during inference, we yield performance improvements without tuning costs. Especially, the performance of ZSCLIP with

our method is notably enhanced, achieving better and comparable performance to TaI-DPT , which requires mild tuning. In NUSWIDE , we incorporate contextual text descriptions from a large language model (GPT-3.5) to validate the potential of utilizing generated texts instead of well-curated caption data. With provided class name of NUSWIDE , we readily gathered the massive set of text descriptions within a short amount of time. TaI-DPT  is trained with the public caption data from OpenImages. Our method exceeds the performance of ZSCLIP and TaI-DPT  by a large margin, with improvements of 9.3 mAP and 2.6 mAP, respectively.

Table 2 shows the results of the zero-shot learning setting for unseen classes. In MS-COCO , we follow the DualCoOp and split the dataset into 48 seen classes and 17 unseen classes. The evaluation is conducted in both zero-shot setting (ZSL, recognizing only unseen classes) and generalized zero-shot setting (GZSL, recognizing both seen and unseen classes). Based on prompt tuning, DualCoOp trains learnable context tokens on 48 seen classes and achieves the state-of-the-art performance on both ZSL and GZSL. Our method was originally designed to handle novel classes (unseen classes) by leveraging text descriptions. As a result, our method significantly improved the ZSL and GZSL performance of the supervised DualCoOp by providing complementary information. Table 1 and Table 2 demonstrate the effectiveness of our method performing training-free enhancement of CLIP with only text descriptions that are easily obtained.

**Evaluation on Few-Shot Setting.** We performed comparison study with few-shot methods in multi-label recognition. In TaI-DPT , they have investigate to confirm the effectiveness of their zero-shot method. Here, we further validate our method, which is zero-shot test-time task adaption without tuning costs.

Table 3 summarizes the results of the few-shot methods on MS-COCO dataset , especially using 1 and 5 shot samples for all classes. While existing few-shot methods  demonstrated the performance enhancements with an increase of labeled samples, TaI-DPT  and our method are performed within the zero-shot setting. By applying our method with existing zero-shot methods (ZSCLIP and TaI-DPT ), we consistently enhance performance, as already demonstrated in a zero-shot setting. In the absence of labeled samples and tuning, we achieved comparable performance with ML-FSL and better results than other few-shot methods utilizing 5-shot samples.

**Evaluation on Partially Labeled Setting.** Due to high costs of annotation in multi-label image recognition, training with partially labeled samples  has been studied. Following DualCoOp , we performed the evaluation of partially labeled setting. As shown in Table 4, our method supplements the decreased performance of DualCoOp  caused by partially labeled samples by providing complementary information during inference. Through zero-shot test time task adaptation without tuning costs, we consistently enhance the the performance of DualCoOp  on

   Training-free & Methods & MS-COCO & VOC2007 & NUS-WIDE \\  ✓ & ZSCLIP & 57.4 & 82.8 & 37.3 \\ ✓ & +Ours & 70.0 (+12.6) & 89.2 (+6.4) & 46.6 (+9.3) \\ ✗ & TaI-DPT & 68.0 & 88.9 & 46.5 \\ ✓ & +Ours & 70.9 (+2.9) & 90.1 (+1.2) & 49.1 (+2.6) \\   

Table 1: Multi-label recognition with zero-shot methods on MS-COCO , VOC2007  and NUS-WIDE . Without training, our method significantly enhances the performance of existing zero-shot methods. The evaluation is based on mAP.

    &  &  \\  & ZSL & GZSL & ZSL & GZSL \\  DualCoOp & 78.2 & 70.2 & 82.9 & 74.9 \\ +Ours & 82.9 (+4.7) & 73.2 (+3.0) & 87.6 (+4.7) & 78.0 (+3.1) \\   

Table 2: Multi-label recognition with 17 unseen classes on MS-COCO . In zero-shot learning (ZSL, recognizing only unseen classes) and generalized ZSL (GZSL, recognizing both seen and unseen classes), our method effectively supplements the complementary information of unseen classes to the supervised DualCoOp on 48 seen classes. The evaluation is based on mAP.

all benchmark dataset. Furthermore, we achieved the performance of DualCoOp  trained with 90% labels by applying our method with DualCoOp trained with 60% labels from MS-COCO , 50% labels from VOC2007 , and 70% labels from NUSWIDE .

### Ablation Study and Analysis

#### 4.3.1 Effectiveness of our method

To verify the effectiveness of components of our method, we conducted an ablation study for analyzing our method. As shown in Table 5, we first proposed a novel class concept representation with text descriptions by class to ZSCLIP. Since the text descriptions contain the semantic meaning among multiple class names and contextual information for multi-label recognition, the alignment between visual features of test image and text features are improved compared to the hand-crafted prompts as shown in the Fig.1. Thus, the performance is increased by 4.1 mAP and 1.1 mAP on MS-COCO  and VOC2007 , respectively. Then, we performed the context-guided visual feature using a large set of text descriptions, \(Z^{all}\). Transforming the visual features into same text feature space as our class concept representation is essential to minimize the gap between visual feature from task-agnostic visual encoder and text features for each class. Constructing context-guided visual feature, our method yield remarkable performance gain by 8.5 mAP and 5.3 mAP on MS-COCO  and VOC2007 , respectively. Thus, we effectively designed our method that improves the alignment between visual and text features.

#### 4.3.2 The Number of Text Descriptions

We investigate the effect of the number of text descriptions for our method. As shown in Table 6, we evaluated performance by increasing the number of randomly selected text descriptions from 1K to 32K texts. With only 1K text descriptions, our method enhances performance by approximately

    &  &  \\   & & 10\% & 20\% & 30\% & 40\% & 50\% & 60\% & 70\% & 80\% & 90\% & **Avg.** \\  MS-COCO & SARB & 71.2 & 75.0 & 77.1 & 78.3 & 79.6 & 79.6 & 80.5 & 80.5 & 80.5 & 77.9 \\  & DualCoOp & 80.8 & 82.2 & 82.8 & 83.0 & 83.5 & 83.8 & 83.9 & 84.1 & 84.2 & 82.7 \\  & DualCoOp+Ours & **81.5** & **82.8** & **83.3** & **83.5** & **84.0** & **84.2** & **84.4** & **84.5** & **84.6** & **83.6** \\  VOC2007 & SARB & 83.5 & 88.6 & 90.7 & 91.4 & 91.9 & 92.2 & 92.6 & 92.8 & 92.9 & 90.7 \\  & DualCoOp & 91.6 & 93.3 & 93.7 & 94.3 & 94.5 & 94.7 & 94.8 & 94.9 & 94.8 & 94.0 \\  & DualCoOp+Ours & **92.5** & **93.9** & **94.3** & **94.7** & **94.9** & **95.0** & **95.1** & **95.2** & **95.1** & **94.5** \\  NUS-WIDE & DualCoOp & 54.0 & 56.1 & 56.9 & 57.4 & 57.9 & 57.8 & 58.0 & 58.4 & 58.8 & 57.3 \\  & DualCoOp+Ours & **55.0** & **56.9** & **57.7** & **58.2** & **58.6** & **58.6** & **58.8** & **59.2** & **59.5** & **58.1** \\   

Table 4: Performance of multi-label recognition based on the partially labeled dataset . Without training and labeled samples, our method consistently enhanced the performance of supervised DualCoOp  over all partial label ratio. DualCoOp  is reproduced and the evaluation is based on mAP.

   Training-free & Methods & 0-shot & 1-shot & 5-shot \\  ✗ & LaSO & - & 45.3 & 58.1 \\ ✗ & ML-FSL & - & **54.4** & **63.6** \\ ✗ & CoOp & - & 46.9 & 55.6 \\ ✓ & Tip-Adapter & - & 53.8 & 59.7 \\  ✓ & ZSCLIP & 49.7 & - & - \\ ✓ & +Ours & 58.5 (+8.8) & - & - \\ ✗ & Tal-DPT & 59.2 & - & - \\ ✓ & +Ours & **61.4** (+2.2) & - & - \\   

Table 3: Comparison with few-shot methods on MS-COCO . The evaluation is based on mAP with 16 novel classes. For each shot, we highlighted the best performance in bold.

8 mAP on MS-COCO  and 5 mAP on VOC2007 , respectively. As the number of text descriptions ranges from 1K to 32K, the text embeddings of \(Z^{all}\) can cover the wider range of test dataset, resulting in increased performance gains. For adapting to novel classes during inference, our method not only achieves a significant performance improvement with only 1K texts but also further enhances performance as the quantity of texts increases.

#### 4.3.3 Analysis of Inference Time

We analyzed the inference time of our method depending on the number of text descriptions. When extracting text embeddings from the text descriptions in advance, we measure the inference time as the number of text descriptions increases. ZSCLIP, as the baseline model, processes each sample for classification in 7.2ms. When the number of texts increases from 1K to 32K, integrating ZSCLIP with our method only increases the inference time by 0.4-0.5ms, with tests conducted on the RTX3090. In addition, Our method (6.8GB) requires slightly more memory than ZSCLIP (6.5GB) on VOC2007 . Therefore, our method presents a simple and efficient approach for training-free enhancement approach at inference.

## 5 Conclusion

In this paper, we propose a novel class concept representation from massive text descriptions for training-free multi-label recognition tasks. Inspired by how humans form concepts based on words, as studied in cognitive neuroscience, we replace single class name prompts with the class concept representation that capture various patterns of co-occurrence with other words. To further enhance alignment between multi-modal features of VLMs, we propose a context-guided visual representation that is transformed onto the same linear space as the class concept representation. Remarkably, our proposed method outperforms zero-shot prompt tuning methods such as TaI-DPT and achieves significant enhancements over ZSCLIP and other state-of-the-art prompt tuning methods without requiring parameter tuning or labeled samples, and with minimal inference time overhead.

**Limitations.** While our method achieved impressive results with training-free enhancement of CLIP, it exhibits limitations. First, a significant performance gap exists compared to prompt tuning methods with full samples, like DualCoOp . Second, the computational memory demands of our method grow at a faster rate than ZSCLIP as the batch size increases.

    &  \\   & 1K & 2K & 4K & 8K & 16K & 32K \\  MS-COCO  & 65.8 & 68.4 & 68.5 & 69.1 & 69.6 & 69.9 \\ VOC2007  & 88.1 & 88.5 & 88.8 & 88.9 & 89.0 & 89.1 \\   

Table 6: Ablation studies in terms of the number of the text descriptions. As increasing the number of texts, we measured the performance of ZSCLIP with our method in mAP on MS-COCO  and VOC2007 . Note that ZSCLIP achieves 57.4 mAP and 82.8 mAP for MS-COCO  and VOC2007 , respectively.

  
**Method** & MS-COCO  & VOC2007  \\  Baseline (ZSCLIP) & 57.4 & 82.8 \\ +Class concept representation & 61.5(+4.1) & 83.9(+1.1) \\ +Context-guided visual feature & 70.0(+8.5) & 89.2(+5.3) \\   

Table 5: Effectiveness of our method on MS-COCO  and VOC2007 . Each component of our method consistently improves performance, with significant enhancements achieved particularly in context-guided visual feature through narrowing the gap between visual and text features. The evaluation is based on mAP.