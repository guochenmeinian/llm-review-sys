# MambaTree: Tree Topology

is All You Need in State Space Model

 Yicheng Xiao\({}^{1}\)\({}^{*}\),  Lin Song\({}^{2,358}\),

**Shaoli Huang\({}^{3}\), Jiangshan Wang\({}^{1}\), Siyu Song\({}^{4}\), Yixiao Ge\({}^{2,3}\), Xiu Li\({}^{1}\)\({}^{}}\), Ying Shan\({}^{2,3}\)**

\({}^{1}\)Tsinghua Shenzhen International Graduate School, Tsinghua University

\({}^{2}\)ARC Lab, Tencent PCG \({}^{3}\)Tencent AI Lab \({}^{4}\)South China Normal University

xiaoyc23@mails.tsinghua.edu.cn ronrysong@tencent.com

Equal contribution. Work done during an internship at Tencent. Corresponding author.

###### Abstract

The state space models, employing recursively propagated features, demonstrate strong representation capabilities comparable to Transformer models and superior efficiency. However, constrained by the inherent geometric constraints of sequences, it still falls short in modeling long-range dependencies. To address this issue, we propose the MambaTree network, which first dynamically generates a tree topology based on spatial relationships and input features. Then, feature propagation is performed based on this graph, thereby breaking the original sequence constraints to achieve stronger representation capabilities. Additionally, we introduce a linear complexity dynamic programming algorithm to enhance long-range interactions without increasing computational cost. MambaTree is a versatile multimodal framework that can be applied to both visual and textual tasks. Extensive experiments demonstrate that our method significantly outperforms existing structured state space models on image classification, object detection and segmentation. Besides, by fine-tuning large language models, our approach achieves consistent improvements in multiple textual tasks at minor training cost. Code is available at https://github.com/EasonXiao-888/GrootVL.

## 1 Introduction

Mainstream fundamental models are primarily based on CNN  and Transformer architectures , which dominate in visual and language tasks. However, the small receptive field of CNNs and the high complexity of Transformers make it challenging to strike a good balance between effectiveness and efficiency. The state space models (SSMs)  attempt to disrupt this impasse, which model sequences in a recurrent form. Different from the previous recurrent neural networks , these approaches draw inspiration from control systems, leveraging structural parameter initialization to attain stable optimization and superior computing performance. Nevertheless, it remains susceptible to the intrinsic flaw shared by recurrent neural networks, \(i.e.\), a deficiency in capturing long-range dependencies.

Recently, an improved selection mechanism known as Mamba  is proposed to mitigate the challenges of SSMs. This approach introduces weight modulation during the propagation process, which substantially enlarges the effective receptive field and achieves impressive performance in NLP tasks. Besides, numerous studies aim to extend Mamba into computer vision, by employing various pre-defined strategies to map 2D image features into 1D sequences. ViM  and VMamba  utilize a multi-directional raster-scanning strategy, while LocalMamba  further confines itspropagation range within a local window. They have successfully adapted Mampa to image inputs. Nevertheless, as shown in Fig. 1(a), both raster-scanning and local-scanning strategies introduce spatial discontinuities between adjacent pixels, and feature transformations in Mampa rely on the feature relationships, thereby impeding the effective information flow in a sequence. Additionally, PlainMampa  introduces a continuous scanning strategy, aiming to alleviate this issue by simply adjusting the propagation direction at discontinuous positions. However, all these methods rely on fixed propagation trajectories, which ignore the inherent spatial structure and cannot dynamically adjust the topology based on input. Therefore, this paper endeavors to explore a new perspective: _introducing an input-aware topological network for feature propagation in state space models_.

To achieve it, we develop a tree state space model and propose a new framework, termed MampaTree, which adaptively generates a tree topology based on the input feature and then performs feature propagation on it. Specifically, two sub-networks, MampaTreeV and MampaTreeL, are designed for visual and language tasks respectively, which are illustrated in Fig. 1(b) and Fig. 1(d). For visual tasks, motivated by [71; 54], we first utilize the dissimilarity between adjacent features to construct a minimum spanning tree on a four-connected planner graph. This process can adaptively encode the spatial and semantic information into a tree graph [71; 54]. Then, we iteratively traverse each pixel, considering it as the root vertex, and aggregate the features of other pixels using the state transition function of Mampa. Intuitively, this operation requires two levels of traversal across the entire pixel set, resulting in an unacceptable quadratic complexity relative to the number of pixels. However, given that the tree graph is acyclic, we propose a dynamic programming algorithm to achieve linear complexity propagation. With such an input-aware tree topology, our approach enables more effective long-range interactions while maintaining consistent linear complexity with Mampa. Furthermore, our method can also be applied to language tasks by constructing a tree topology based on the dissimilarity between token features, which overcomes the geometrical constraints of the text sequence. Using a similar aggregation process as MampaTreeV, MampaTreeL can significantly enhance the language representation of a pre-trained Large Language Model .

We conduct extensive experiments to validate the effectiveness of MampaTreeV on multiple visual benchmarks, \(i.e.\) image classification on ImageNet , object detection and instance segmentation on MSCOCO  as well as semantic segmentation on ADE20K . Results show that our method notably outperforms existing SSM-based methods for all benchmarks and achieves competitive performance with CNN and Transformer-based approaches. Moreover, with LoRA finetuning , MampaTreeL demonstrates consistent improvements for a pre-trained large language model at minor training cost.

## 2 Related Work

### Conventional Vision Foundation Models

The evolution of deep neural networks has been a significant catalyst in machine vision perception. CNN-based models [30; 51; 35; 25; 61; 72; 38; 55; 73] firstly emerge as pivotal landmarks, with ResNet  notably standing out for its inventive residual connection module, garnering widespread adoption across diverse domains of visual recognition. Furthermore, more efficient convolution operations are formulated, such as depth-wise convolutions introduced by MobileNet , paving the way for lightweight models. Additionally, deformable convolution  has been proposed to enhance the receptive field. Subsequently, ViT  has significantly improved the vision recognition paradigm. It reformulates the architecture design and training mechanism by combining transformer architecture in natural language processing, aiming to improve computational efficiency and broaden the scope of applications. After research discourse is centred on hierarchical ViTs [43; 42; 11; 63; 14; 56; 5] which design networks by decreasing feature resolution across the backbone gradually. Furthermore, recent research built on CNN serves to re-emphasize the capabilities of convolutional networks. For example, InterImage  presents a large model based on deformable CNN, while UniRepLKNet  exhibits significant performance through large kernel convolution.

### Explorations about State Space Models

State space models (SSMs) have emerged as a novel class of models within the deep learning paradigm, showing significant potential for sequence transforming [23; 22; 52]. These methods have attracted significant attention due to their linear scalability with sequence length. The early method,LSSL , draws inspiration from continuous state space models in control systems and attempts to address the long-range dependency problem through a combination with HIPPO  initialization. S4  proposes to normalize the parameters into a diagonal matrix, prompting a subsequent series of research on structured SSMs [24; 21; 26; 19]. Recently, the Selective State Space Model , known as Mamba, strikes a balance between effectiveness and efficiency through the design of an input-dependent parameter initialization strategy, which has emerged as a formidable competitor to both transformer and CNN structures. In addition to showcasing superior outcomes in sequence modeling, Mamba has been seamlessly incorporated into the visual domain [77; 41; 34; 69; 68]. These studies often rely on handcrafted fixed scanning mechanisms to mitigate the execution bias of the selective state space model on 2D non-causal images. However, such simplistic approaches cannot effectively capture spatial relationships in an input-dependent paradigm. To address this limitation, we propose an effective framework MambaTree in this work to enhance long-range modeling for both vision and language tasks by introducing an input-aware tree-based topological structure.

## 3 Method

In this section, we first revisit the selective state space model  and then elaborate on our input-aware topology scanning algorithm for state space modeling. With this superior algorithm, we develop a tree SSM and propose a novel framework called MambaTree, which consists of two sub-networks: MambaTreeV for visual tasks and MambaTreeL for fine-tuning a pre-trained language model .

### Revisiting Selective State Space Model

State Space Models (SSMs) are commonly regarded as continuous linear time-invariant systems  that map input stimulation \(x(t)^{1 D}\) to output signal \(y(t)^{1 D}\) through a state vector \(h(t)^{1 N}\), where \(t\), \(D\) and \(N\) indicate the time step, channel number of the signal and state size, respectively. These models can be formulated as the following linear ordinary differential equations:

\[h^{}(t)=h(t)+x(t), y(t)=h(t)+x(t),\] (1)

where \(^{N N}\), \(^{N D}\), \(^{N D}\) and feedthrough coefficient \(^{D}\).

Discretization.Although SSM serves as a powerful tool in systems and control engineering, its time-continuous nature poses challenges for integration into deep learning architectures. To alleviate this issue, most methods utilize the zero-order hold rule  to discretize the continuous system described by Eq. (1) and convert continuous variables (\(\), \(\), \(\), \(\)) into corresponding discrete

Figure 1: **Comparison of different propagation strategies for multi-modal tasks. For visual tasks, the previous strategies (a) are based on fixed patterns, while our method can adaptively generate the propagation topology according to input features. For textual tasks, compared to previous methods (c), our approach (d) can break the inherent constraints of text sequences, facilitating the effective transmission of long-range information.**

parameters (\(}\), \(}\), \(}\), \(}\)) over the specified sampling time-scale \(^{D}\):

\[}=e^{},}=(e^{ }-I)^{-1},}=,}=\] (2)

In addition, many improved methods  use an approximation of \(}\) based on the first-order Taylor Series:

\[}=(e^{}-I)^{-1}()()^{-1}= \] (3)

Selective Mechanism.Previous SSMs store information through finite states and inherent time-invariance, which limits their effectiveness. Therefore, Mamba  introduces a dynamic mechanism to selectively filter out input into a sequential state. Specifically, it utilizes Linear Projection to calculate the parameters \(\{_{i}\}_{i=1}^{L}\), \(\{_{i}\}_{i=1}^{L}\) and \(\{_{i}\}_{i=1}^{L}\) from the input sequence \(\{x_{i}\}_{i=1}^{L}\) with \(x_{i}^{1 D}\) directly to improve the context-aware ability. Then the output sequence \(\{y_{i}\}_{i=1}^{L}\) can be computed with those input-adaptive discretized parameters as follows:

\[h_{i}=}_{i}h_{i-1}+}_{i}x_{i}, y_{i}= _{i}h_{i}+x_{i}\] (4)

### Tree State Space Model

Mamba  has showcased remarkable performance in modeling the dependencies of consecutive words in a sequence. However, its applicability in long-context tasks, especially visual modeling, still poses certain challenges. For visual tasks, many methods attempt to address this problem by employing fixed scanning strategies, such as multi-directional raster scan , local scan , and continuous scan . However, these handcrafted scanning methods fail to effectively preserve the 2D structural information of images.

Following the design in Mamba , we construct a transform block as a tree state space model, which is presented in Fig. 2. The only difference between our block and Mamba lies in the replacement of the structured state space block with the proposed tree scanning algorithm. In the tree scanning algorithm, we generate a tree topology and then propagate the state of each vertex along the topological path to obtain strong feature representations. In addition, our algorithm can effectively enhance language representations by incorporating such a tree topology during text processing, which overcomes the geometrical constraints of text sequences. In the following, we elaborate on the proposed tree scanning algorithm and its applications for multi-modal tasks.

Tree Scanning Algorithm.Given an input feature \(X=\{x_{i}\}_{i=1}^{L}\) where \(L\) is the sequence length (or the number of input pixels), we construct an undirected \(m\)-connected graph \(G=(V,E)\) for the

Figure 2: **Illustration of Tree State Space Model. With an image feature map \(x\), we perform Tree Scanning Algorithm (TSA) to construct a \(4\)-connected graph with edge weights measured by dissimilarity between pixels. Then, we obtain an MST with vertices set \(\) through a pruning algorithm and perform the state transition for each vertex in this topology (detailed in Sec. 3.2). Red arrows describe the propagation source of vertex \(i\).**

feature. \(m\) is a hyper-parameter that indicates the number of adjacent tokens. Following , we set \(m=4\) for visual tasks, meaning each pixel is connected to its four neighboring pixels. For language tasks, we set \(m=3\) by default, meaning each token is connected to the previous three tokens. In addition, the vertices \(V\) represent the pixel (or token) embeddings, and the \(E\) indicates the edges of the graph. The edge weight is calculated by the feature dissimilarity between adjacent vertices. Besides, the metric of dissimilarity uses cosine distance by default, and the comparison with other metrics refers to Table 5.

We use the Contractive Boruvka algorithm  to prune the edges with significant dissimilarity, which generates a minimum spanning tree (MST) \(_{T}\) whose sum of dissimilarity weights is minimum out of all spanning trees. In the propagation process, we iteratively traverse each vertex, treating it as the root, and aggregate the features of the remaining vertices. Intuitively, applying state propagation within such a geometric configuration makes its preferential interactions among vertices with small spatial and feature distances. Following the Mampa, we employ the data-dependent transition matrix for state propagation. For a vertex \(k\), we denote the transition matrix with its parent as \(}_{k}\). Furthermore, following the Eq. (4), the state aggregation process for the \(i\)-th vertex can be formulated as:

\[h_{i}=_{ j}S(E_{ij})}_{j}x_{j}, S(E_{ ij})=_{k N_{ij}}}_{k},\] (5)

where \(\) denotes the index set of all vertices in the tree. \(S(E_{ij})\) represents the path weight of hyperedge \(E_{ij}\) traced from \(j\)-th vertex to \(i\)-th vertex in the tree \(_{T}\), and \(N_{ij}\) indicates the index set of all vertices on this hyperedge. For visual tasks, we iterate over each vertex, treating it as the root of the spanning tree \(_{T}\), and aggregate the states from the other vertices, thereby obtaining the transformed states \(\{h_{i}\}_{i=1}^{L}\). For textual tasks, because of the causal prediction manner in large language models, we only take the last token as root and aggregate from other tokens. To achieve end-to-end training, we derive the derivative of the output hidden state \(h_{i}\) to the input variables \(}_{k}\), \(}_{j}\) and \(x_{j}\) as follows:

\[}{ x_{j}}=S(E_{ij})}_{j },}{}_{j}}=S(E_{ij} )x_{j}\] (6)

\[}{}_{k}}=_{ j C_{k}^ {i}}}_{j}x_{j}S(E_{kj})S(E_{in}),\] (7)

where \(C_{k}^{i}\) indicates the children of vertex \(k\) in tree \(_{T}\) whose root is the vertex \(i\), and \(n\) denotes the parent of vertex \(k\) in Eq. (7). Finally, the output feature \(Y\) can be formulated as:

\[Y= Norm(H)+ X,\] (8)

where \(Y\), \(H\) and \(X\) indicate the stack of \(\{y_{i}\}_{i=1}^{L}\), \(\{h_{i}\}_{i=1}^{L}\) and \(\{x\}_{i=1}^{L}\) respectively. \(\) denotes the element-wise multiplication.

Efficient Implementation for Multi-Modality.For visual tasks, the tree scanning algorithm requires two levels of traversal across the entire pixel set, resulting in an unacceptable quadratic complexity relative to the number of pixels \((L^{2})\). To alleviate this issue, we utilize a dynamic

Figure 3: **Overview of MampaTreeV.** LN means LayerNorm and FFN is a feed-forward network in the basic block. S2 and P1 denote stride of \(2\) and padding size of \(1\) in convolution, respectively.

programming procedure to accelerate the inference and training processes as elaborated in Algorithm 1, which results in linear complexity \((L)\). For textual tasks, we perform a unidirectional aggregation approach (shown in Algorithm 2 of Appendix B) in adherence to the causal nature of language. Moreover, we provide the back-propagation process for both Vision Tree Scanning and Language Tree Scanning processes, whose detailed proofs refer to Appendix C.

```
0: Input feature \(\{x_{i}\}_{i=1}^{L}\); Input matrix \(\{}_{i}\}_{i=1}^{L}\); State matrix \(\{}_{i}\}_{i=1}^{L}\); Gradient of loss to hidden states \(\{}\}_{i=1}^{L}\); Minimum Spanning Tree \(_{T}\).
0:\(Root,,Leaf BFS(_{T})\)\(\)Breadth-first topological order of \(_{T}\) Forward:  Initialization: \(\{_{i}\}_{i=1}^{L}\{x_{i}\}_{i=1}^{L}\)
2:for\(i Leaf\) to \(Root\)do\(_{i}=}_{i}x_{i}+_{ j\{t|(t)=i\}} _{j}}_{j}\)
4:endforforfor\(i Root\)to\(Leaf\)do
6:if\(i\) is \(Root\)then\(h_{i}=_{i}\)
7:else\(h_{i}=}_{i}(h_{(i)}-}_{i}_{i})+ _{i}=(1-}_{i}^{2})_{i}+}_{i}h_{( i)}\)
8:endif
9:endfor
10:endfor
11:Initialization: \(\{_{i}\}_{i=1}^{L}\{}\}_{i=1}^ {L}\)for\(i Leaf\) to \(Root\)do
12:\(_{i}=}_{i}}+_{  j\{t|(t)=i\}}_{j}}_{j}\)
13:endfor
14:for\(i Root\)to\(Leaf\)doif\(i\) is \(Root\)then
15:\(}=_{i}}_{i}\,, }_{i}}=_{i}x_{i}\,, { Loss}{}_{i}}=0\) else
16:\(}=(1-}_{i}^{2})_{i} }_{i}+}_{i}(i)}}}_{i}\,,}_{i}}=(1-}_{i}^{2})_{i}x_{i}+}_{i }}_{(i)}}x_{i}\)
17:\(}_{i}}=_{i}*(h_{i}-}_{i}_{i})+_{i}*(}-}_{i}_{i})=_{i}h_{i}+_{i}}-2_{i}_{i}}_{i}\)
18:endif
19:endfor
20: Hidden states \(\{h_{i}\}_{i=1}^{L}\); Grad. of loss to input feature \(\{}\}_{i=1}^{L}\); Grad. of loss to input matrix \(\{}_{i}}\}_{i=1}^{L}\); Grad. of loss to state matrix \(\{}_{i}}\}_{i=1}^{L}\). ```

**Algorithm 1** Vision Tree Scanning

### Application for Vision and Language

MambaTreeVGiven an image with a shape of \(H W 3\), our goal is to obtain high-quality visual features for downstream tasks. To this end, we propose an effective vision architecture MambaTreeV which consists of a stem module, several basic blocks and downsampling layers to generate hierarchical representations illustrated in Fig. 3. Overall, our MambaTreeV comprises four stages similar to previous general vision backbones [44; 43; 62; 41]. We integrate the stem module before the first stage to decrease the resolution of the input image signal by a factor of \(4\), resulting in a feature map with a shape \( C\). It includes two convolutions, two Layer Normalization (LN) layers and one GELU activation function. The kernel size for both convolutions is \(3\) with a stride of \(2\) and padding of \(1\). Similarly, a downsampling layer consists of a \(3 3\) convolution with a stride of \(2\) and padding of \(1\) and an LN layer. Positioned between two stages, it serves to downsample the input feature map by a factor of \(2\). Motivated by [62; 41], we devise a residual block with skip connections to integrate our fundamental Tree State Space Model in Sec. 3.2. In detail, we first normalize the input features with LN layer. Spatial priors and long-range dependencies are then obtained through our tree scanning algorithm with residual connections established alongside the input features. Finally, a feedforward neural network is utilized to project the normalized features to output signals as shown in Fig. 3. Based on the above origin components, we develop our MambaTreeV in three scales, \(i.e.\), MambaTreeV-Tiny, MambaTreeV-Small and MambaTreeV-Base.

MambaTreeLRecurrent neural networks rely on fixed memory to preserve past information, which poses limitations when handling long contexts where relevant words are distant from the current moment. While Mamba  employs a selection mechanism to enhance context awareness, its fixed memory size cannot expand over time, resulting in restricted state space. Therefore, the ability to extrapolate decreases during scrolling as the prompt extends. To mitigate this issue, we propose an effective fine-tuning paradigm. Specifically, the tree-based topology branch is built upon one-way scrolling with a scaling factor, enabling state transitions within such a structure. This arrangement facilitates the preferential interaction of semantically related tokens. It is noteworthy that this paradigm does not introduce any additional training parameters. Instead, it utilizes pretrained state transformation parameters to conduct semantic aggregation by incorporating topological structures. Experimental results demonstrate the effectiveness of our approach.

## 4 Experiments

We conduct extensive experiments to evaluate the effectiveness of MambaTreeV and compare it with advanced CNN-based, Transformer-based, and SSM-based models covering various downstream tasks, including image classification, object detection and semantic segmentation. Furthermore, we validate the capability of MambaTreeL in the field of natural language understanding.

### Image Classification

Settings.We assess the classification performance of MambaTreeV on the ImageNet-1k dataset . Following previous practices [43; 44; 62; 41], all MambaTreeV models are trained for \(300\) epochs from scratch using AdamW optimizer with a warm-up strategy of \(20\) epochs. During training, we utilize a Cosine Scheduler with an initial learning rate of \(1 10^{-3}\) and weight decay of \(0.05\). In addition, the exponential moving average (EMA) is also applied.

Results.The comparison results summarized in Table 1 show MambaTreeV leading all SSM-based models and competitive with advanced CNNs and Transformers across tiny, small, and base scales. Specifically, MambaTreeV-T achieves \(83.4\%\) Top-1 Acc. boosting ViM-S by \(2.9\%\), LocalVim-S by \(2.2\%\), PlainMamba-L2 by \(1.8\%\) and VMamba-T by \(0.9\%\) with similar FLOPs. Additionally, it surpasses ConvNeXt-T by \(1.3\%\) and Swin-T by \(2.2\%\), demonstrating the effectiveness of our method.

   &  &  Top-1 \\ Acc. \\  }} \\     &  & 22M & 4.6G & 79.9 \\ Swin-T  &  & 28M & 4.6G & 81.3 \\ CoAtNet-0  &  & 25M & 4.0G & 81.6 \\ SG-Former-S  &  & 23M & 4.8G & 83.2 \\ ConvNeXt-T  &  & 29M & 4.5G & 82.1 \\ SLaK-T  &  & 30M & 5.0G & 82.5 \\ UniRepLKNet-T  &  & 31M & 4.9G & 83.2 \\ InterImage-T  &  & 30M & 5.0G & 83.5 \\ ViM-S  &  & 26M & 5.1G & 80.5 \\ LocalViM-S  &  & 28M & 4.8G & 81.2 \\ PlainMamba-L2  &  & 25M & 8.1G & 81.6 \\ Mamba-2D-S  &  & 24M & - & 81.7 \\ S4ND-ConvNeXt-T  &  & 30M & - & 82.2 \\ VMamba-T  &  & 31M & 4.

### Object Detection

Settings.We verify the detection performance of MambaTreeV on the MSCOCO 2017 dataset  with MMDetection library . We follow previous works [41; 62; 43; 34; 53; 55; 74; 70; 6] to validate object detection and instance segmentation tasks with Mask-RCNN . Specifically, We adopt the AdamW optimizer with a learning rate of \(1 10^{-4}\) and batch size of \(16\) to optimize the model built upon our pre-trained classification backbones on ImageNet-1K. The training schedules include \(1\) (\(12\) epochs) and \(3\) (\(36\) epochs) with multi-scale data augmentation.

Results.As depicted in Table 8 (in Appendix A.), our method outperforms existing methods on most evaluation metrics, especially for instance segmentation. Under \(1\) schedule, MambaTreeV-T achieves \(47.0\) in box mAP (AP\({}^{b}\)), which is \(1.1\) points higher than ViM-S and \(0.5\) points higher than VMamba-T. It is worth noting that MambaTreeV-T outperforms ViM-S by \(1.7\) points with \(1\) schedule and LocalV Mamba-T by \(0.4\) points with \(3\) schedule in mask mAP (AP\({}^{m}\)). Moreover, the best AP\({}^{b}\)\(50.1\) and AP\({}^{m}\)\(44.6\) are obtained by MambaTreeV-S in \(3\) schedule with multi-scale training.

### Semantic Segmentation

Settings.To evaluate the semantic segmentation performance of our MambaTreeV series, we train our models with UperNet  initialized by pre-trained classification weights on ADE20K for 160k iterations, following common practices without additional augmentations for fair comparison.

Results.Our method performs exceptionally well on segmentation tasks shown in Table 2. MambaTreeV-T yields a clear improvement of \(+3.6\) in single-scale mIoU compared to ViM-S and \(+1.9\) in multi-scale mIoU compared to LocalViM-S. Furthermore, MambaTreeV-S boosts InternImage-S by \(0.6\) and \(0.8\) in single-scale and multi-scale respectively. We consider the preservation of intricate structural details through tree topology scanning to be particularly advantageous for segmentation tasks that require pixel-level perception.

    &  &  &  \\  & &  &  \\  Swin-T  & T & 945G & 44.5 & 45.8 \\ ConvNeXt-T  & C & 939G & 46.0 & 46.7 \\ SLAK-T  & C & 936G & 47.6 & - \\ InterImage-T  & C & 944G & 47.9 & 48.1 \\ UniRepLKNet-T  & C & 946G & 48.6 & 49.1 \\ ViM-S  & S & - & 44.9 & - \\ LocalViM-S  & S & 297G & 46.4 & 47.5 \\ PlainMamba-L2  & S & 285G & 46.8 & - \\ VMamba-T  & S & 964G & 47.3 & 48.3 \\ LocalVAMba-T  & S & 970G & 47.9 & 49.1 \\ MambaTreeV-T (Ours) & S & 941G & **48.5** & **49.4** \\  Swin-S  & T & 1038G & 47.6 & 49.5 \\ ConvNeXt-S  & C & 1027G & 48.7 & 49.6 \\ SLAK-S  & C & 1028G & 49.4 & - \\ InterImage-S  & C & 1017G & 50.1 & 50.9 \\ UniRepLKNet-S  & C & 1036G & 50.5 & 51.0 \\ PlainMamba-L3  & S & 419G & 49.1 & - \\ VMamba-S  & S & 1081G & 49.5 & 50.5 \\ LocalVAMamba-S  & S & 1095G & 50.0 & 51.0 \\ MambaTreeV-S (Ours) & S & 1019G & **50.7** & **51.7** \\   

Table 2: **Semantic segmentation performance on ADE20K val set.** The crop size is all set to \(512^{2}\). SS and MS denote single-scale and multi-scale testing, respectively.

Figure 4: **Visualization of affinity maps in the specific position.** The Location is marked by the red cross in each input (a). TP is our tree topology scanning algorithm (b), which captures more detailed structural information and has a larger receptive field compared to raster scanning (c).

### Language Understanding

We regard Mamba  with \(130\)M parameters as the base model. To verify the effectiveness of our MambaTreeL in nature language understanding, we first fine-tune pre-trained Mamba via LoRA  and MambaTreeL under the same setting with the Alpaca data , which contains \(52000\) instruction tuning data for supervised fine-tuning. Then we utilize popular language benchmarks provided in the open-sourced lm-evaluation-harness project  for evaluation, including PIQA , AI2-ARC , SST , WinoGrande, LAMBADA , Race  and Openbookqa . The results in Table 3 demonstrate that our MambaTreeL provides a benefit of \(+1.1\%\) in average Acc. compared to LoRA. Since the short prompt length of WinoGrande dataset, the performance degrades with a marginal gap.

### Ablation Study & Qualitative Results

In this section, we conduct analysis experiments on ImageNet-1K dataset and present some visual results to illustrate the effectiveness of our algorithm.

Scanning Strategy.We conduct a head-to-head comparison of different scanning strategies, as shown in Table 4. The tree topology scanning outperforms previous strategies by \(0.8\%\) and \(0.3\%\), highlighting the superiority of our algorithm in vision recognition.

Distance Metric.Before generating a minimum spanning tree from a connected graph, it is important to measure the edge weights between vertices. Therefore, we validate several distance metrics as illustrated in Table 5. The results indicate that \(Cosine\) distance most effectively represents the relationship between vertices, performing \(0.5\%\) better than \(Manhattan\) and \(0.2\%\) better than \(Euclidean\).

Root Setting.We traverse all vertices, treating each as a root, and perform state transitions along the topological path from the other vertices toward the root. This traversal ensures that each vertex captures long-range dependencies. To verify the effectiveness of this operation, we consider only the first and last vertices as the root in Table 6. The results show reductions of \(0.5\%\) and \(0.4\%\), respectively.

Inference speed comparison.As shown in Table 7, we report the inference throughputs of our method on an Nvidia V100 GPU. MambaTreeV-T\({}^{*}\) refers to each stage sharing the same tree topology structure, which enhances efficiency without compromising accuracy. To achieve better practical inference speed, we also introduce a cuda implementation optimized for GPUs. Compared with other counterparts, our approach exhibits superior effectiveness and faster inference speed.

Qualitative Results.To better illustrate the superiority of our scanning strategy, we visualize the affinity maps of different positions marked by the red cross in each input image. For example, we

       Scanning Strategy \\  & Acc \\  
 Raster Scan \\ Cross Scan \\  & 82.6 \\ Tree Topology Scan & **83.4** \\   

Table 4: **Effectiveness of our algorithm.**

    Method \\  & PIQA \(\) & Arc-E \(\) & SST \(\) & WG \(\) & L-ppl \(\) & Race \(\) & BQA \(\) & 
 Average \\ Acc. \\  \\  Mamba  & 64.5 & 48.0 & 65.6 & 51.8 & 16.1 & 27.4 & 16.8 & 45.7 \\ + LoRA  & 64.7 & 48.3 & 65.1 & **52.2** & 17.7 & 28.6 & 17.8 & 46.1 \\ + MambaTreeL (Ours) & **65.0** & **49.8** & **69.5** & 51.1 & **15.9** & **28.9** & **19.2** & **47.2** \\   

Table 3: **Evaluation on language model benchmarks. Arc-E, WG, L-ppl and BQA indicate Arc-easy , WinoGrande, LAMBADA  and Openbookqa  benchmark, respectively.**set the anchor point in the upper left corner of the sky as shown in the second row of in Fig. 4(a). Our method can easily identify white houses, flagpoles, and the sky, which raster scanning fails to achieve. This demonstrates the capability of our algorithm to preserve detailed structural information. More comparisons can be seen in Fig. 6 (in Appendix D.)

## 5 Conclusion & Limitations

In this paper, we propose a tree state space model to perform feature propagation on an input-aware topology. Besides, we introduce a linear complexity dynamic programming algorithm to enhance long-range interactions without increasing computational cost. With the proposed techniques, we establish the general multi-modal networks to break the original sequence constraints and achieve stronger representation capabilities. Extensive experiments demonstrate the effectiveness of our method in both visual and language tasks. The limitation of our method is that the tree structure is not a common paradigm, and it needs to be specifically optimized according to the hardware device.