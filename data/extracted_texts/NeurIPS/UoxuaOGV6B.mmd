# Spectral Adapter: Fine-Tuning in Spectral Space

Fangzhao Zhang

Electrical Engineering

Stanford University

zfzhao@stanford.edu &Mert Pilanci

Electrical Engineering

Stanford University

pilanci@stanford.edu

###### Abstract

Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for pretrained deep neural networks have captured widespread interest. In this work, we study the enhancement of current PEFT methods by incorporating the spectral information of pretrained weight matrices into the fine-tuning procedure. We investigate two spectral adaptation mechanisms, namely additive tuning and orthogonal rotation of the top singular vectors, both are done via first carrying out Singular Value Decomposition (SVD) of pretrained weights and then fine-tuning the top spectral space. We provide a theoretical analysis of spectral fine-tuning and show that our approach improves the rank capacity of low-rank adapters given a fixed trainable parameter budget. We show through extensive experiments that the proposed fine-tuning model enables better parameter efficiency and tuning performance as well as benefits multi-adapter fusion. Code is released at https://github.com/pilancilab/spectral_adapter.

## 1 Introduction

Size of language and vision model undergoes a drastic explosion in recent days and results in billions of parameters up to date. While fine-tuning has been used a lot for adapting pretrained large models to various downstream tasks, fine-tuning tasks become increasingly hard with current size of pretrained models due to the huge demand of computing resource. Meanwhile, exchange and storing of fine-tuned models are also expensive given their enormous size. To alleviate these rising problems for fine-tuning large pretrained models, a recent line of research has digged into the Parameter-Efficient Fine-Tuning (PEFT) model family and harnessed great attention. A high-level philosophy behind those PEFT methods is to train a reduced number of parameters compared to full fine-tuning, which instantly saves computing resource and enables light-weight fine-tuned model exchange. Among all PEFT methods, Low-Rank Adaptation (LoRA)  model is a huge success attributed to its simplicity and effectiveness. Specifically, LoRA proposes to tune an additive trainable low-rank matrix and brings zero inference latency after merging the adapter into pretrained model weights. Since its emergence, numerous variants of LoRA have been developed. For instance, AdaLoRA , IncreLoRA , and DyLoRA  propose to dynamically adjust LoRA rank distribution for improving tuning efficiency, QLoRA  combines LoRA with model quantization to further save computing resource, LoRA+  and PrecLoRA  study the optimization landscape of LoRA training, and more recent variant DoRA  decomposes pretrained weights into magnitude and direction components and applies LoRA for direction tuning, see Apppendix A for a more comprehensive review of different LoRA variants. Other PEFT methods such as Orthogonal Fine-Tuning (OFT) proposes to multiply pretrained weights by tunable orthogonal matrices for preservation of hypersphere energy between pretrained neurons. Though these different PEFT methods focus on improving fine-tuning efficiency with reduced parameters, rare attention has been paid to utilize pretrained model weights' information beyond its magnitude in the fine-tuning procedure.

Prior research in statistical machine learning such as  has studied the Empirical Spectral Distribution (ESD) of deep models' weight matrices and found that the ESDs for larger model weights are usually more structured and contain indicative information to distinguish between different training stages. More recent work such as  investigates the "dark matter" effect of bottom spectral space of model weights and recognizes its critical role in attention sink phenomenon observed in . Both work contributes to decrypting spectral information of model weights and sheds light on building insightful understanding of the connection between weight matrices' spectral information and model performance. In this work, we explore further the value of model weights' spectral pattern and unravel its effectiveness in enhancing fine-tuning tasks. We showcase via extensive empirical observation that integration of spectral information of pretrained model weights improves current PEFT methods' parameter efficiency, tuning effect, and arises as a natural solution to multi-adapter fusion problems. Moreover, the suggested fine-tuning model maintains better practicality compared to prior spectral tuning models, which will be investigated further below.

Though any technique for weight fine-tuning can be directly applied to fine-tune singular vector matrices of pretrained model weights, we investigate two specific forms of such extension, namely additive tuning and orthogonal rotating the top singular vector space, which we address as Spectral Adapter\({}^{A}\) and Spectral Adapter\({}^{R}\) respectively in later content. The spectral adaptation mechanisms being considered are formally depicted in Section 2. As a warmup, to show that incorporating spectral information is indeed helpful, Figure 1 displays the training loss of fine-tuning Llama3 8B model on HuggingFace Orca Math dataset and validation score on GSM8K benchmark, from which it can be clearly observed that Spectral Adapter\({}^{A}\) performs superior to recent variants of PEFT methods and behaves closest to full fine-tuning, here we follow experimental setup in , see Appendix F.1 for details and more investigation. In below, we first introduce the fine-tuning model being studied in Section 2 and we then provide some theoretic insights in Section 3. After that, we detail the advantage of our spectral adapter in enhancing fine-tuning result, improving model's parameter efficiency, and helping with multi-adapter fusion as well as address any concern with respect to practicality issues in Section 4. Conclusion and future work is discussed in Section 5. For sake of page limitation, literature review is deferred to Appendix A.

To summarize, the proposed spectral adaptation mechanism demonstrates the first attempt to fine-tune spectral space of pretrained model weights in a parameter-efficient and storage-economic way which improves current PEFT methods from aspects involving tuning results, parameter efficiency, and multi-adapter fusion. We hope this work serves as a building block and motivates further and deeper insightful investigation for exploring spectral structure of pretrained model weights, which becomes increasingly meaningful especially in current large model regime.

## 2 Spectral Adapter: Incorporating Spectral Information into Fine-Tuning

Motivated by the intrinsic low-rank of weight shifts in fine-tuning procedure studied in , LoRA  proposes to add a low-rank factorized trainable matrix to pretrained model weights and tune only these additive parameters for downstream task adaptation, which usually injects far fewer trainable

Figure 1: Training loss of fine-tuning Llama3 8B model with Orca Math dataset  and evaluation score on GSM8K benchmark . We follow experimental setup in , see Appendix F.1 for details. All methods except full fine-tuning maintain approximately \(0.23\%\) trainable parameters.

parameters compared to full fine-tuning and results in light-weight tuned adapters. LoRA serves as an outstanding representative of PEFT family and is now widely-used for different fine-tuning tasks. Inspired by the parameter efficiency of LoRA and the close connection between matrix rank and its spectral representation, here we study two spectral fine-tuning mechanisms, both are completed via first carrying out Singular Value Decomposition (SVD) of pretrained model weights and then fine-tuning the top columns of singular vector matrices obtained via the SVD. More precisely, consider a pretrained weight matrix with its spectral representation of form \(W=USV^{T}\), we define additive spectral adapter as

\[Adapter}^{A}(W):=[U_{1}+A_{U}~{}U_{2}]S[V_{1}+A_{V}~{}V_{2}],\]

and correspondingly the rotational version

\[Adapter}^{R}(W):=[U_{1}R_{U}~{}U_{2}]S[V_{1}R_{V}~{}V_{2}],\]

where \(U_{1},V_{1}\) denote the top-\(r\) columns of \(U\) and \(V\) and \(U_{2},V_{2}\) denote the rest of the columns. \(A=(A_{U},A_{V})\) consists of trainable matrices of shape same as \((U_{1},V_{1})\) and \(R=(R_{U},R_{V})\) consists of two trainable orthogonal matrices of shape \(r\) by \(r\) such that \(R_{U}^{T}R_{U}=R_{V}^{T}R_{V}=I\). As we show in later sections, the orthogonality constraint is efficiently handled with the Cayley parameterization, see Section 4.3 for details. The proposed fine-tuning model architecture can be visualized from Figure 2. Here Spectral Adapter\({}^{A}\) more resembles LoRA as it is of additive form while Spectral Adapter\({}^{R}\) more resembles prior Orthogonal Fine-Tuning (OFT) method which we compare further in Section 4. To ensure zero initialization as often done for PEFT methods, we initialize \(A_{U}\) and \(A_{V}\) both at zero. For rotational spectral adapter, we initialize \(R_{U}\) and \(R_{V}\) as identity matrices.

A more thorough literature review suggests that prior work considering tuning model weights' spectral representation (FSGAN, SVDDiff ) has been proposed for alleviating overfitting when fine-tuning different vision models. These methods only look at tuning the singular values of flattened CNN weights and thus have fixed amount of trainable parameters. Moreover, these methods require storing all \(U,S\) and \(V\) during training while only the diagonal vector of \(S\) is tuned, which nearly doubles the storage requirement compared to pretraining when fine-tuning on downstream tasks. Contrarily, we consider incorporating spectral information in generic fine-tuning procedure for different layers (flattened CNN weights, dense linear weights, etc.) and our method enables flexible parameter budget choices by varying values of \(r\). Methodology-wise, we consider tuning the top-\(r\) columns of \(U\) and \(V\) by additive and rotational tuning, both requiring only these top columns to be stored additionally and the left part can be merged into a single weight matrix. See Section 4.4 for more investigation on practicality of the proposed method.

## 3 Theoretical Insights

After introducing the model architecture of spectral adapter we consider, the main question now remains whether tuning the spectral representation of pretrained weights is indeed an improvement over existing PEFT methods. Before we step into our empirical observations, we first provide

Figure 2: Compared to LoRA which proposes to add low-rank trainable matrices to pretrained weights, we study two types of spectral adapters: \(Adapter}^{A}\) considers additively tuning the top columns of singular vector matrices and \(Adapter}^{R}\) considers orthogonally rotating the top columns of singular vector matrices.

some theoretical insights for the proposed spectral adaptation mechanism. In this section, we show advantage of our spectral adapter method compared to LoRA from two theoretic perspectives by analyzing both the rank capacity of the adapters (Section 3.1) and the subspace alignment of pretrained weight matrices (Section 3.2). Specifically, we will see that Spectral Adapter\({}^{A}\) has larger rank capacity than LoRA adapter, which indicates the tuned weight has more adaptation freedom and thus is more desirable. Moreover, the dominant spectral direction of pretrained weight matrix identifies more ideal neuron alignment under the setting we consider in Section 3.2, which justifies the robustness of tuning top singular vectors in our spectral adapter. In Appendix D, we show that Spectral Adapter\({}^{A}\) is approximately equivalent to DoRA  for vector-form weights.

### Adapter Rank Capacity

For any pretrained weight matrix \(W\), suppose that the adapter is given by the parameterization \(f_{}(W)\) where \(\) represents trainable weights. For instance with LoRA adapter, \(f_{}(W)=W+AB^{T}\), where \(=\{A,B\}\) is trainable. We define the _rank capacity_ of an adapter \(f_{}(W)\) as follows:

\[(f_{};W):=_{}(f_{}(W))-_{ }(f_{}(W)),\]

which describes the range of matrix ranks the tuned weight can achieve given a specific adapter form. Then, the following lemma shows that Spectral Adapter\({}^{A}\) has twice the rank capacity of LoRA adapter under an equal number of trainable parameters.

**Lemma 3.1**.: _Suppose that \(W^{n m}\) is an arbitrary full row-rank matrix and \(n m\) without loss of generality. Consider rank-r LoRA and rank-r additive spectral adapter, which have an equal number of trainable parameters. We have_

\[(;W) =r,\] \[(Adapter}^{A};W) =2r.\]

See Appendix B for proof. Therefore when pretrained model weight matrix is close to full row-rank, as what has been observed in , Spectral Adapter\({}^{A}\) has nearly double rank capacity compared to LoRA adapter. Furthermore, some prior work explicitly imposes low-rank constraint when training original NNs [50; 43; 66; 22; 68; 24; 9]. Using LoRA adapter to fine-tune such pretrained model weights would destroy their rank constraints while applying spectral adapter preserves the constraints.

Next we proceed to show that top spectral space of pretrained weight matrices is more aligned with ideal neuron direction under a simple setting via subspace decomposition analysis of pretrained model weights. This observation corroborates our choice of tuning top singular vectors in our proposed spectral adaptation mechanism. Empirically, we observe that tuning top directions performs superior to tuning bottom ones, see Appendix F.3 and F.5.1 for related experiments.

### Weight Subspace Alignment

Consider two-layer ReLU network with \(m\) hidden nodes and univariate output. For squared loss objective, we can write out the training problem explicitly as

\[_{W^{(1)},W^{(2)}}(XW^{(1)})_{+},W^{(2)}-y_{2}^{2}+( \|W^{(1)}\|_{F}^{2}+\|W^{(2)}\|_{2}^{2}),\]

where \(X^{n d}\) is the data matrix, \((W^{(1)}^{d m},W^{(2)}^{m})\) are first and second layer weights respectively and \(y^{n}\) is the label vector. For better visualization, we take \(d=3\). Consider the case that all data points lie on \(xy\)-plane, which mimics the usual observation that data points occupy a low-dimensional manifold. Then we can decompose each first layer neuron \(W^{(1)}_{j}^{d}\) into \(W^{(1)}_{j}=w_{j1}+w_{j2}\) where \(w_{j1}(X),w_{j2}(X)\). With simple algebra, for non-zero weight decay which is often the default setting for current deep learning optimizers, one can derive \(w_{j2}=0\) and thus \(W^{(1)}_{j}=w_{j1}(X)\). Therefore all optimal neurons lie also in \(xy\)-plane. However, due to optimization errors, some of the trained neurons might be slightly deviated from \(xy\)-plane, as illustrated in Figure 3, where \(u_{i}\) indicates pretrained neuron directions, though most of them lie in \(xy\)-plane, some might deviate (i.e., \(u_{4}\)). \(u^{}\) indicates the top singular vector direction of pretrained weight \(W^{(1)}\) which here recognizes the \(xy\)-plane orientation, and thus fine-tuning \(u^{}\) is noiseless and is expected to be more robust.

Figure 3: Top singular vector of pretrained weight recognizes more ideal neuron direction. Illustration plot for Section 3.2.

## 4 Empirical Results: The Impact of Spectral Information

We experiment our proposed spectral adapter with fine-tuning large language models and diffusion models and compare against various recent PEFT methods. From language model experiments, we observe that Spectral Adapter\({}^{A}\) performs superior to various PEFT baselines and harnesses higher scores on different benchmarks, which again verifies the effectiveness of incorporating spectral information into the fine-tuning procedure, see Section 4.1 for details. For diffusion model experiments, we will see that the advantage of spectral adapter comes in two-fold: Spectral Adapter\({}^{A}\) offers a natural solution to existing problems in multi-adapter fusion procedure and Spectral Adapter\({}^{R}\) manifests finer-grained parameter budgets as well as better parameter efficiency, see Section 4.2 and 4.3 respectively. For a fair comparison with all baselines, we use their official implementation and follow hyperparameter setting in their original reports as long as available. See each individual section for corresponding experimental details. All experiments are done with NVIDIA RTX A6000 GPU.

### Language Model Fine-Tuning: Enhancing Fine-Tuning Results with Spectral Adapter\({}^{A}\)

For large language model experiments, we present experimental results for fine-tuning DeBERTaV3-base model (185M) and Mistral model (7B) on GLUE and GSM8K tasks respectively. Our Spectral Adapter\({}^{A}\) method achieves superior tuning results compared to various recent PEFT methods in most experiments.

**DeBERTaV3-base Experiment.** Table 1 shows fine-tuning results of DeBERTaV3-base model on GLUE benchmarks with various PEFT methods. For a fair comparison, we use official implementations for LoRA, DoRA, OFT and AdaLoRA in HuggingFace PEFT library, with hyperparameter setting for LoRA  and AdaLoRA  following their original reports. We use same hyperparameter setting as LoRA for DoRA and follow the setting used in BOFT , a variant of OFT, for OFT experiments. We abbreviate Spectral Adapter\({}^{A}\) as Spectral\({}^{A}\) for presentation simplicity and we tune hyperparameters for Spectral Adapter\({}^{A}\). See Appendix F.2 for hyperparameter details and F.3 for loss/validation plot comparison. We fine-tune all \(q,k,v\) matrices in attention layers. Our Spectral Adapter\({}^{A}\) achieves highest average score and best scores for most tasks with fewest trainable parameters.

**Mistral 7B Experiment.** We experiment our Spectral Adapter\({}^{A}\) with Mistral 7B model  fine-tuned for GSM8K task . Since all baseline model reports include no fine-tuning tasks with the Mistral family, we use official implementations of all baseline methods for comparison and we fix learning rate to be \(2.5e-5\) for all methods following . We take \(r=8\) for LoRA, DoRA and Spectral Adapter\({}^{A}\) to maintain approximately same number of trainable parameters for all methods. Table 2 presents the accuracy comparison where Spectral\({}^{A}\) stands for Spectral Adapter\({}^{A}\). From the result, we observe that our Spectral Adapter\({}^{A}\) scores higher than both LoRA and DoRA by a large margin and increases the pretrained model baseline significantly, which verifies the effectiveness of the proposed spectral adaptation mechanism. See Appendix F.4 for more about experimental details. Note for a different learning rate, DoRA performs better than LoRA while still worse than our method, see also Appendix F.4 for details.

    &  &  \\  & & **MNLI** & **SST-2** & **MRPC** & **CoLA** & **QNLI** & **QQP** & **RTE** & **STS-B** & **Avg.** \\  LoRA\({}_{r=24}\) & \(0.72\%\) & 88.87 & 95.06 & 87.00 & 65.84 & 91.87 & 91.45 & 81.22 & 90.43 & 86.47 \\ DoRA\({}_{r=24}\) & \(0.73\%\) & 88.91 & 95.29 & 88.72 & 65.84 & 92.01 & 91.51 & 80.14 & 90.10 & 86.57 \\ OFT\({}_{r=4}\) & \(0.72\%\) & 89.16 & 95.06 & 87.74 & 66.75 & 93.28 & 91.33 & 78.70 & 89.72 & 86.47 \\ AdaLoRA\({}_{r=24}\) & \(1.07\%\) & 89.44 & 94.95 & 89.70 & 63.06 & 93.17 & 91.48 & **83.75** & **91.22** & 87.10 \\  Spectral\({}^{A}_{r=24}\) & \(0.72\%\) & **89.79** & **95.75** & **90.19** & **69.44** & **93.35** & **91.65** & 83.39 & 90.64 & **88.03** \\   

Table 1: Accuracy comparison of fine-tuning DeBERTaV3-base with various PEFT methods on GLUE benchmarks. Spectral\({}^{A}\) is abbreviation for Spectral Adapter\({}^{A}\). See Section 4.1 for experimental details.

  
**Method** & \(\#\)**Param** & **GSM8K** \\  Pre-Trained & – & \(37.91 1.34\) \\ LoRA\({}_{r=8}\) & \(0.16\%\) & \(44.81 1.37\) \\ DoRA\({}_{r=8}\) & \(0.17\%\) & \(43.82 1.37\) \\  Spectral\({}^{A}_{r=8}\) & \(0.16\%\) & \(49.73 1.38\) \\   

Table 2: Accuracy comparison of fine-tuning Mistral 7B model with different PEFT methods on GSM8K benchmark. See Section 4.1 for experimental details.

### Diffusion Model Fusion: Improving Multi-Object Fine-Tuning with Spectral Adapter\({}^{A}\)

Multi-adapter fusion is a current bottleneck in diffusion model fine-tuning tasks with LoRA adapters. Simply adding different LoRA adapters tuned for distinct objects will result in problems involving identity loss and concept binding . To tackle this toughness, different methods emerge such as Gradient Fusion  and Orthogonal Adaptation . Specifically, Orthogonal Adaptation method proposes to fix LoRA parameter \(B\) to have orthogonal basis and train \(A\) solely. Experiments there show that merging LoRA weights with such orthogonal basis helps preserving individual object characteristics compared to its non-orthogonal counterpart. In Orthogonal Adaptation , the authors maintain \(B\) by manually keeping large orthogonal matrices for different layer sizes and sample \(r\) columns from corresponding orthogonal matrix to form \(B\) for each LoRA adapter. With knowledge from random matrix theory, such sampled matrices are likely to have orthogonal basis.

Notably, our Spectral Adapter\({}^{A}\) naturally operates on orthogonal singular vectors and thus introduces an elegant solution to multi-adapter fusion problems by distributing different concept tunings along different columns of singular vector matrices, which maps to wireless communications where the signals are distributed over non-overlapping frequencies. A subtlety here lies in the choice of column space for different fine-tuning tasks: (1) Sample-based methods can be adopted if data privacy is considered and different tuning tasks are done independently. In Appendix F.5, we show that tuning top columns manifests better generation quality compared to both tuning bottom columns and sampling random orthogonal basis as what has been done in Orthogonal Adaptation . Thus there is a trade-off between high-quality generation and concept collapsing, i.e., sampling from top singular vectors is more encouraged while column overlapping between concepts happens more often compared to sampling from the whole set. (2) On the other hand, if fine-tuning tasks are not isolated and can collaborate on the column scheduling, then more deliberate tuning scheduling can be adopted, for example in a two-concept tuning task with \(r=4\), the first concept can allocate first to fourth columns and the second concept then claims fifth to eighth columns. Figure 4 demonstrates steps for the same method for three-concept tuning task. Since we expect fine-tuned weights to stay close to original weights, though both row space and column space are tuned in spectral adapter, this adaptation mechanism approximates orthogonal-basis tuning for different objects and thus we expect it helps improving identity preservation for multi-adapter fusion. In this section, we investigate this effect via extensive diffusion model experiments.

Our experiments follow  and build on  which studies multi-LoRA fusion. We experiment with multi-object tuning and face generation tasks. Due to space limitation, we present some multi-object tuning results below and we leave the rest to Appendix F.5. For all tasks, we compare against baselines including Gradient Fusion , Orthogonal Adaptation , and FedAvg . We start with a simple review for these baseline methods.

#### Baseline Review

To merge different LoRA adapters, say we have a set of LoRA parameters \(\{_{1},,_{n}\}\) where \(_{i}=A_{i}B_{i}^{T}\) and pretrained parameter \(_{0}\), FedAvg  proposes to merge them in to a single parameter by taking a weighted average as \(_{}=_{0}+_{i}_{i}_{i},\) where \(_{i}\) is the weight attached to parameter \(_{i}\) and is usually taken to satisfy \(_{i}_{i}=1\), i.e., \(_{}\) is a convex combination of individual adapters. Gradient Fusion  instead considers solving an auxiliary optimization problem of form \(_{}=_{}_{i=1}^{n}\|(_{0}+ _{i})X_{i}- X_{i}\|_{F}^{2}\) where \(X_{i}\) represents the input activation of the \(i\)-th concept. Orthogonal Adaptation  follows FedAvg method and replaces original LoRA

Figure 4: Distributing different concept tunings along different spectral space helps with identity preservation in multi-adapter fusion, see Section 4.2 for details.

parameters with orthogonal-based LoRA adapters. For our method, to merge different spectral adapters, let \(_{0}=U_{0}S_{0}V_{0}^{T}\) denote the spectral representation of pretrained model weight. Given a set of spectral adapters \(\{(U_{i},V_{i}),,(U_{n},V_{n})\}\) with zero-padding to make the shape the same as \((U_{0},V_{0})\), we follow FedAvg and compute \(_{}=(U_{0}+_{i}_{i}U_{i})S_{0}(V_{0}+_{i} _{i}V_{i})^{T}\). In the following experiments, we take \(_{i}=1/n\) as in  for all FedAvg, Orthogonal Adaptation, and our Spectral Adapter\({}^{A}\) fusion. Notably, all FedAvg, Orthogonal Adaptation, and our Spectral Adapter\({}^{A}\) fusion can be done approximately instantly while Gradient Fusion usually takes around \(10 15\) minutes for solving its auxiliary optimization problems for all concept adapters.

#### Multi-Object Generation

We follow default training setting in  and fine-tune the Chilloutmix diffusion model  on three custom animal concepts, see original animals in "reference" in Figure 5. For better spatial alignment, we adopt T2I-Adapter  with sketch condition and we set guidance equal to one, see also "reference" in Figure 5 for the sketch condition being used. LoRA rank \(r=8\) is adopted. For baseline comparisons, we use original code for Gradient Fusion  and Orthogonal Adaptation . We adapt code of Gradient Fusion for FedAvg method since there is no official implementation available. Custom animal name is replaced with special token \(<\)V\({}_{}\)> for fine-tuning. For our Spectral Adapter\({}^{A}\), we follow the method depicted in Figure 4 and tune first, second, and third top eighth columns of singular vector matrices for different animal concepts. Figure 5 shows the generation results with different methods for selected prompts. Notably, baseline methods sometimes fail to capture the custom animal concepts while Spectral Adapter\({}^{A}\) recognizes all custom animals and generates visually satisfactory images. For better measurement, we also compute the alignment scores for each generated image with both reference images and prompt texts. It can be witnessed that our method achieves better alignment scores compared to baselines. See Appendix F.7 for details on alignment score computation.

### Diffusion Model Expressiveness: Improving Parameter Efficiency with Spectral Adapter\({}^{R}\)

Spectral Adapter\({}^{R}\) is closely connected to prior Orthogonal Fine-Tuning (OFT )  method which proposes to multiply the pretrained model weights by trainable orthogonal matrices in the fine-tuning procedure. Motivation behind OFT is to preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. Unlike OFT which orthogonally rotates neurons, Spectral Adapter\({}^{R}\) multiplies the top-\(r\) columns of singular vector space \(U\) and \(V\) by orthogonal trainable matrices. For our implementation, several options are available for maintaining a trainable orthogonal matrix such as adding an orthogonality penalty in the objective function considered in  or via Cayley parameterization considered in . We follow  and adopt Cayley parameterization which is supported by Pytorch . Specifically, the orthogonal matrix \(R\) is constructed via \(R=(I+Q)(I-Q)^{-1}\) with a skew-symmetric matrix \(Q\) maintained as \((A-A^{T})/2\)

Figure 5: Generation results of Chilloutmix diffusion model  with different fused adapters tuned on three custom animal concepts. See Section 4.2 for details.

where \(A\) is our trainable parameter. Compared to adding an auxiliary orthogonality penalty, this parametrization is exact and thus the SVD form is preserved after tuning with Spectral Adapter\({}^{R}\) and can be adopted directly for subsequent fine-tuning tasks, which we state formally as a lemma below:

**Lemma 4.1**.: _With the Cayley parametrization, Spectral Adapter\({}^{R}\) is an exact rotation operation and thus preserves the structure of the SVD of the fine-tuned weight. Subsequent fine-tunings can be applied consequently without recomputing the SVD each time._

See Appendix C for the proof of above lemma. Unlike LoRA which requires number of trainable parameters to scale with weight size, when tuning top-\(r\) columns of \(U\) an \(V\), Spectral Adapter\({}^{R}\) only requires two trainable matrices of size \(r r\) and thus can be more parameter-efficient especially for large pretrained weight. For common weight size such as \(W^{1024 1024}\), LoRA with only \(r=1\) introduces same number of trainable parameters as Spectral Adapter\({}^{R}\) with \(r=32\). For a thorough analysis on parameter efficiency improvement brought by Spectral Adapter\({}^{R}\), we here also compare with different variants of LoRA which are proposed for trainable parameter savings. We review all baselines in detail below.

#### Baseline Review

We compare our Spectral Adapter\({}^{R}\) with LoRA , SVDiff , LiDB , OFT , and VeRA . Though the other methods are proposed for vision model tuning, VeRA is originally proposed for LLM tuning and we extend it here to diffusion model tuning due to its parameter efficiency. Consider a pretrained weight \(W^{n n}\), SVDiff originally proposes to tune all singular values of flattened CNN weights, here we extend it to tune all singular values of text encoder and U-Net weights for our comparison, thus trainable parameter attached to \(W\) will be of size \(n\) and is nonadjustable. LiDB stands for Lightweight Dreambooth and proposes to cut down trainable parameter budget by introducing auxiliary frozen matrix \(A_{}^{n a}\) and \(B_{}^{b n}\), then it mimics LoRA but uses \(A_{}AB^{T}B_{}\) in replace of \(AB^{T}\) with trainable (\(A^{a r},B^{b r}\)). Thus with \(a,b<n\), LiDB requires \((a+b)r<2nr\) trainable parameters. In below, we use \(a=50,b=100\) as default in . OFT multiplies the weight matrix by a trainable orthogonal matrix via Cayley parametrization discussed above, thus its complete version requires \(n^{2}\) trainable parameters. For parameter efficiency, OFT proposes to use block-diagonal trainable matrix with all diagonal blocks being orthogonal. Thus with \(r\) diagonal blocks, the number of trainable parameter will be \(r(n/r)^{2}\). Further reduction of trainable parameter is achieved via sharing the diagonal blocks, which demands only (\(n/r\))\({}^{2}\) parameters. In below comparison, we use this shared block-diagonal version for best parameter efficiency of OFT. VeRA proposes to use \(_{a}A_{b}B^{T}\) in replace of \(AB^{T}\) where \(_{a}\) and \(_{b}\) are diagonal matrices of size \(n n\) and \(r r\) respectively. Thus the total number of trainable parameters by VeRA is \((n+r) n\). Table 3 compares different properties across all methods, where \(n\) represents weight size and \(r\) represents rank for all methods except for OFT, where \(r\) denotes number of diagonal blocks.

#### Parameter Efficiency

We fine-tune the Chilloumix diffusion model  with various PEFT methods on custom vase concept and present the generation results for prompt "a <\(V_{}\)>" in Figure 6 for various trainable parameter budgets, where grey dash denotes that the corresponding parameter budget is unobtainable with a given adapter no matter how the hyperparameter is chosen and empty entry without grey dash

 
**Method** & **Granularity** & \(\#\)**Param** & **Auxiliary Param** \\  LoRA & \(\) & \(\) & \(2nr n\) & \(\) \\ SVDff & \(\) & \(1\) & \(n n\) & \(\) \\ LiDB & \(\) & \(\) & \((a+b)r r\) & **yes** \\ OFT & \(\) & \(\#\) factors of a \({}^{1}\) & \((n/r)^{2}\) & \(\) \\ VRA & \(\) & \(\) & \(n+r n\) & **yes** \\ Spectral Adapter\({}^{R}\) & \(\) & \(n\) & \(2r^{2} r\) & \(\) \\   \({}^{1}\) Ceiling operation is ignored for this count.

Table 3: Baseline methods comparison for parameter efficiency. Granularity indicates number of trainable parameter budgets available. See Section 4.3 for details.

represents that there is a way to achieve the corresponding parameter budget though the generation result is skipped for better visualization. We follow default LoRA implementation in  for LoRA baseline and adjust it for all other methods. From Figure 6, it can be observed that LoRA, OFT, and LiDAR start to generate vase close to custom vase with at least \(200k\) trainable parameters. SVDiff and VeRA are unable to generate ideal vase images even if scaled to large parameter budget. On the contrary, Spectral Adapter\({}^{R}\) starts to recognize the custom vase concept with only \(20k\) trainable parameters and has finer-grained parameter choices compared to other methods, i.e., notably Spectral Adapter\({}^{R}\) can have as few as \(1k\) parameters while other methods start with at least tens of thousands of trainable parameters. In a word, Spectral Adapter\({}^{R}\) enjoys finer-grained parameter budget choices and manifests better visual quality with fewer parameters, thus achieves enhanced parameter efficiency compared to various other PEFT methods.

Figure 7 below presents generation results of Chilloutmix diffusion model  tuned on custom chair concept with different methods under various parameter budgets. The prompt used is "a yellow <\(V_{}\)>". See "reference" in Figure 7 for original chair images. From the generation results, it can be observed that LoRA generates reasonable chairs for all rank \(r=1,2,3\) though it already induces \(273k\) parameters even if rank is set to \(1\). OFT and VeRA start to recognize custom chair with \(>100k\) parameters. SVDiff has a single fixed trainable parameter budget of size around \(100k\).

LiDB forms a competitive candidate and generates satisfactory images with smallest trainable parameter budget among all baseline methods. However, our Spectral Adapter\({}^{R}\) still generates images better aligned to

Figure 6: Generation results for prompt “a <\(V_{}\)> on a table” after fine-tuning Chilloutmix diffusion model  on custom vase images with different PEFT methods. See Section 4.3 for details.

Figure 7: Generation results for prompt “a yellow <\(V_{}\)>” after fine-tuning Chilloutmix diffusion model  on custom chair images with different PEFT methods. Spectral\({}^{R}\) is abbreviation for Spectral Adapter\({}^{R}\). See Section 4.3 for details.

reference images with as few as \(20k\) trainable parameters and has finer-grained parameter budget choices compared to LiDB. See Appendix F.6 for hyperparameter setting and Appendix F.7 for alignment score computation details.

### Final Note: A Closer Look at SVD Cost

To alleviate the concerns with respect to online training cost and show that our proposed method is very practical, we provide runtime and GPU storage cost bar plot in Figure 8, which shows runtime and GPU storage cost for LoRA and for our Spectral Adapter\({}^{A}\) when used for fine-tuning diffusion model in Section 4.2 and Mistral 7B model in Section 4.1. Here we adopt rank \(r=8\) for both LoRA and Spectral Adapter\({}^{A}\). It can be observed that our Spectral Adapter\({}^{A}\) introduces negligible runtime and storage overhead for current large model size. Modern numerical tools such as randomized SVD  can also be exploited for further runtime reduction and the SVD procedure can be parallelized when multiple machines are available. See Appendix E for further investigation.

## 5 Conclusion and Limitations

In this work, we investigate the incorporation of spectral information of pretrained model weights into current PEFT models by introducing a spectral adaptation mechanism which updates only the top singular vectors of pretrained weights. We investigate the additive and rotational variants of such spectral adaptation mechanism. Theoretically, we show the motivation of tuning top singular vectors by comparing the rank capacity of different fine-tuning models and carrying out weight decomposition of pretrained model layers. Empirically, we verify the superiority of our proposed spectral adaptation method compared to various recent PEFT methods from different aspects via extensive experiments. To our best knowledge, this is the first work considering incorporating spectral information as a practical generic paradigm for fine-tuning tasks and enhances fine-tuning results, parameter efficiency, as well as benefits multi-adapter fusion of existing PEFT methods. For future work, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Other PEFT methods such as AdaLoRA  can also be dynamically combined with spectral adaptation.

A limitation of the current work remains in the choice of tuning top spectral space. Though its validity has been theoretically verified under simple settings, further investigation on tuning different columns of singular vector matrices is critical to understanding the role of spectral information in fine-tuning procedure. Besides, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Moreover, the time consumption of singular value decomposition procedure increases as model grows larger and thus faster singular value decomposition method also benefits.

Figure 8: Runtime and GPU storage cost plot. See Section 4.4 for details.

Acknowledgements

This work was supported in part by the National Science Foundation (NSF) under Grant DMS-2134248; in part by the NSF CAREER Award under Grant CCF-2236829; in part by the U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; and in part by the Office of Naval Research under Grant N00014-24-1-2164.