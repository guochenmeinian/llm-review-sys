# Practical Sharpness-Aware Minimization

Cannot Converge All the Way to Optima

 Dongkuk Si  Chulhee Yun

Korea Advanced Institute of Science and Technology (KAIST)

{dongkuksi, chulhee.yun}@kaist.ac.kr

###### Abstract

Sharpness-Aware Minimization (SAM) is an optimizer that takes a descent step based on the gradient at a perturbation \(_{t}=_{t}+_{t})}{\| f(_{t})\|}\) of the current point \(_{t}\). Existing studies prove convergence of SAM for smooth functions, but they do so by assuming decaying perturbation size \(\) and/or no gradient normalization in \(_{t}\), which is detached from practice. To address this gap, we study deterministic/stochastic versions of SAM with practical configurations (i.e., constant \(\) and gradient normalization in \(_{t}\)) and explore their convergence properties on smooth functions with (non)convexity assumptions. Perhaps surprisingly, in many scenarios, we find out that SAM has _limited_ capability to converge to global minima or stationary points. For smooth strongly convex functions, we show that while deterministic SAM enjoys tight global convergence rates of \((})\), the convergence bound of stochastic SAM suffers an _inevitable_ additive term \((^{2})\), indicating convergence only up to _neighborhoods_ of optima. In fact, such \((^{2})\) factors arise for stochastic SAM in all the settings we consider, and also for deterministic SAM in nonconvex cases; importantly, we prove by examples that such terms are _unavoidable_. Our results highlight vastly different characteristics of SAM with vs. without decaying perturbation size or gradient normalization, and suggest that the intuitions gained from one version may not apply to the other.

## 1 Introduction

Modern neural networks are armed with a large number of layers and parameters, having a risk to overfit to training data. In order to make accurate predictions on unseen data, generalization performance has been considered as the most important factor in deep learning models. Based on the widely accepted belief that geometric properties of loss landscape are correlated with generalization performance, studies have proved theoretical and empirical results regarding the relation between _sharpness_ measures and generalization [14; 17; 19; 21; 29]. Here, _sharpness_ of a loss at a point generally refers to the degree to which the loss varies in the small neighborhood of the point.

Motivated by prior studies that show flat minima have better generalization performance [19; 21], Foret et al.  propose an optimization method referred to as _Sharpness-Aware Minimization (SAM)_. A single iteration of SAM consists of one ascent step (perturbation) and one descent step. Starting from the current iterate \(_{t}\), SAM first takes an ascent step \(_{t}=_{t}+_{t})}{\| f(_{t})\|}\) to (approximately) maximize the loss value in the \(\)-neighborhood of \(_{t}\), and then uses the gradient at \(_{t}\) to update \(_{t}\) to \(_{t+1}\). This two-step procedure gives SAM a special characteristic: the tendency to search for a flat minimum, i.e., a minimum whose neighboring points also return low loss value. Empirical results [3; 9; 16; 20; 25] show that SAM demonstrates an exceptional ability to perform well on different models and tasks with high generalization performance. Following the success of SAM,numerous extensions of SAM have been proposed in the literature [6; 15; 18; 23; 24; 26; 27; 28; 30; 31; 32; 34; 35].

On the theoretical side, various studies have demonstrated different characteristics of SAM [1; 2; 4; 5; 12; 13; 22; 33]. However, comprehending the global convergence properties of practical SAM on a theoretical level still remains elusive. In fact, several recent results [2; 18; 27; 31; 35] attempt to prove the convergence guarantees for SAM and its variants. While these results provide convergence guarantees of SAM on smooth functions, they are somewhat detached to the practical implementations of SAM and its variants. They either (1) assume _decaying_ or _sufficiently small_ perturbation size \(\)[18; 27; 31; 35], whereas \(\) is set to constant in practice; or (2) assume ascent steps _without gradient normalization_, whereas practical implementations of SAM use normalization when calculating the ascent step \(_{t}\).

### Summary of Our Contributions

To address the aforementioned limitations of existing results, we investigate convergence properties of SAM using _gradient normalization_ in ascent steps and arbitrary _constant perturbation size_\(\). We note that to the best of our knowledge, the convergence analysis of SAM have not been carried out under the two practical implementation choices. Our analyses mainly focus on smooth functions, with different levels of convexity assumptions ranging from strong convexity to nonconvexity. We summarize our contributions below; a summary of our convergence results (upper and lower bounds) can also be found in Table 1.

* For deterministic SAM, we prove convergence to global minima of smooth strongly convex functions, and show the tightness of convergence rate in terms of \(T\). Furthermore, we establish the convergence of SAM to stationary points of smooth convex functions. For smooth nonconvex functions, we prove that SAM guarantees convergence to stationary points up to an additive factor \((^{2})\). We provide a worst-case example that always suffers a matching squared gradient norm \((^{2})\), showing that the additive factor is unavoidable and tight in terms of \(\).
* For stochastic settings, we analyze two versions of stochastic SAM (\(n\)-SAM and \(m\)-SAM) on smooth strongly convex, smooth convex, and smooth (Lipschitz) nonconvex functions. We provide convergence guarantees to global minima or stationary points up to additive factors \((^{2})\). In case of \(m\)-SAM, we demonstrate that these factors are inevitable and tight in terms of \(\), by providing worst-case examples where SAM fails to converge properly.

### Related Works: Convergence Analysis of SAM

Recent results [18; 27; 31; 35] prove convergence of stochastic SAM and its variants to stationary points for smooth nonconvex functions. However, these convergence analyses are limited to only _smooth nonconvex_ functions and do not address convex functions. Also, as already pointed out, all the proofs require one crucial assumption detached from practice: _decaying_ (or _sufficiently small_) perturbation size \(\). If \(\) becomes sufficiently small, then the difference between \(f^{}()=_{\|\|}f(+)\) and \(f()\) becomes negligible, which means that they undergo approximately the same updates. Due to this reason, especially in later iterates, SAM with negligible perturbation size

   Optimizer & Function Class & Convergence Upper/Lower Bounds & Reference \\  Deterministic SAM & **C1.C2** & \(_{\{0,...,T\}}f(_{t})-f^{*}=( (-T)+})\) & Theorem 3.1 \\ Deterministic SAM & **C1.C2** & \(_{\{0,...,T\}}f(_{t})-f^{*}=(})\) & Theorem 3.2 \\ Deterministic SAM & **C1.C3** & \(_{t=0}^{T-1}\| f(_{t})\|^{2}=(+})\) & Theorem 3.3 \\ Deterministic SAM & **C1** & \(_{t=0}^{T-1}\| f(_{t})\|^{2}( )+^{2}^{2}\) & Theorem 3.4 \\ Stochastic SAM & **C1.C2.C5** & \((_{T})-f^{*}((-T)+-^{2}^{2}|_{+}}{T})+^{2}}{}\) & Theorem 4.1 \\ Stochastic SAM & **C1.C3.C5** & \(_{t=0}^{T-1}\| f(_{t})\|^{2} (+})+^{2}^{2}\) & Theorem 4.3 \\ Stochastic \(n\)-SAM & **C1.C4.C5** & \(_{t=0}^{T-1}\|(\| f(_{t})\|-)^{2} (+})+^{2}^{2}\) & Theorem 4.5 \\ Stochastic \(m\)-SAM & **C1.C4.C5** & \(_{t=0}^{T-1}\|(\| f(_{t})\|-)^{2} (+})+5^{2}^{2}\) & Theorem 4.6 \\   

Table 1: Convergence of SAM with constant perturbation size \(\) after \(T\) steps. **C1**, **C2**, **C3**, and **C4** indicate \(\)-smoothness, \(\)-strong convexity, convexity, and Lipschitzness, respectively. **C5** indicates the bounded variance of the gradient oracle. See Section 2.3 for definitions of **C1â€“C5**.

would become almost identical to gradient descent (GD), resulting in a significant difference in the convergence behavior compared to constant \(\).

As for existing proofs that do not require decaying \(\), Andriushchenko and Flammarion  prove convergence guarantees for _deterministic_ SAM with _constant_\(\) on smooth nonconvex, smooth Polyak-Lojasiewicz, smooth convex, and smooth strongly convex functions. Moreover, the authors prove convergence guarantees of _stochastic_ SAM, this time with _decaying_\(\), on smooth nonconvex and smooth Polyak-Lojasiewicz functions. However, their analyses are also crucially different from the practical implementations of SAM because their proofs are for a variant of SAM _without gradient normalization_ in ascent steps, with updates in the form of \(_{t+1}=_{t}- f(_{t}+ f(_{t}))\). This form can be considered as vanilla SAM with an "effective" perturbation size \(\| f(_{t})\|\), which indicates that even with constant \(\), the effective perturbation size will become smaller and smaller as the algorithm converges towards a stationary point. As in the case of decaying \(\) discussed above, this can make a huge difference in the convergence behavior.

Figure 1 illustrates a comparison between SAM with and without gradient normalization, highlighting the disparity between their convergence points. As we predicted above, it is evident that they indeed reach entirely distinct global minima, exhibiting different convergence characteristics.

Discussions on the related works on the relationship between sharpness and generalization, as well as other theoretical properties of SAM, are provided in Appendix A.

### Notation

The euclidean norm of a vector \(\) is denoted as \(\|\|\). We let \(^{*}_{+}\{x x>0\}\), and use \([]_{+}\) to define \(\{,0\}\). We use \(()\) and \(()\) to hide numerical constants in our upper and lower bounds, respectively. We use \(}()\) to additionally hide logarithmic factors. When discussing rates, we sometimes use these symbols to hide everything except \(T\), the number of SAM iterations. When discussing additive factors that arise in our convergence bounds, we use these symbols to hide all other variables except the perturbation size \(\). For an objective function \(f\) and initialization \(_{0}\) of SAM, we define \(f^{*}_{}f()\) and \( f(_{0})-f^{*}\).

## 2 Sharpness-Aware Minimization: Preliminaries and Intuitions

Before presenting the main results, we first introduce deterministic and stochastic SAM, and present definitions of function classes considered in this paper. We next introduce _virtual gradient map_ and _virtual loss_ that shed light on our intuitive understanding of SAM's convergence behavior.

### Sharpness-Aware Minimization (SAM)

We focus on minimizing a function \(f:^{d}\), where the optimization variable is represented by \(^{d}\), using the Sharpness-Aware Minimization (SAM) optimizer. Instead of minimizing the vanilla objective function \(f()\), SAM  aims to minimize the _SAM objective_\(f^{}()\), where

\[_{}f^{}()=_{}_{\|\|}f( +).\] (1)

For \(^{*}_{+}\), the SAM loss \(f^{}()\) outputs the worst function value in a \(\)-ball \(\{^{d}\|-\|\}\) around \(\). Assuming "sufficiently small" \(\), the inner maximization of (1) can be solved by Taylor-approximating the objective:

\[_{\|\|}f(+)_{\| \|}f()+, f()=)}{\| f()\|}}().\]

Figure 1: Trajectory plot for a function \(f(x,y)=(xy-1)^{2}\). The green line indicates global minima of \(f\). SAM and USAM indicates SAM with and without gradient normalization, respectively, and GD indicates gradient descent. When USAM approaches the green line, it converges to a nearby global minimum, which is similar to GD. In contrast, SAM travels along the green line, towards the flattest minimum \((1,1)\).

Using this approximate solution \(}()\), we can define the _approximate SAM objective_\(^{}\) as \(^{}() f(+}())\). In order to run gradient descent (GD) on \(^{}()\), one needs to calculate its gradient; however, from the definition of \(}()\), we can realize that \(^{}()\) has terms that involve the Hessian of \(f\). Here, SAM makes another approximation, by ignoring the Hessian term:

\[^{}() f()|_{+}()}.\]

From these approximations, one iteration of SAM is defined as a set of two-step update equations:

\[_{t}=_{t}+_{t})}{\| f (_{t})\|},\\ _{t+1}=_{t}- f(_{t}).\] (2)

As seen in (2), we use \(_{t}\) to denote the iterate of SAM at the \(t\)-th step. We use \(T\) to denote the number of SAM iterations. We refer to the hyperparameter \(^{*}_{+}\) as the _perturbation size_ and \(^{*}_{+}\) as the _step size_. Note that in (2), the perturbation size \(\) is a time-invariant constant; in practice, it is common to fix \(\) as a constant throughout training [3; 9; 16].

According to the SAM update in (2), the update cannot be defined when \(\| f()\|=0\). In practice, we add a small numerical constant (e.g., \(10^{-12}\)) to the denominator in order to prevent numerical instability. In this paper, we ignore this constant and treat \(_{t})}{\| f(_{t})\|}\) as \(0\) whenever \(\| f(_{t})\|=0\).

### SAM under Stochastic Settings

To analyze stochastic SAM, we suppose that the objective is given as \(f()=_{}[l(;)]\), where \(\) is a stochastic parameter (e.g., data sample) and \(l(;)\) indicates the loss at point \(\) with a random sample \(\). Based on the SAM update in (2), we can define stochastic SAM under this setting:

\[_{t}=_{t}+_{t})}{\|g(_{t})\|},\\ _{t+1}=_{t}-(_{t}).\] (3)

We define \(g()=_{}l(;)\) and \(()=_{}l(;)\). Here, \(\) and \(\) are stochastic parameters, queried from any distribution(s) which satisfies \(_{}l(;)=_{}l(;)=f()\).

There are two popular variants of stochastic SAM, introduced in Andriushchenko and Flammarion . Stochastic \(n\)-SAM algorithm refers to the update equation (3) when \(\) and \(\) are independent. In contrast, practical SAM algorithm in Foret et al.  employs stochastic \(m\)-SAM algorithm, which follows the update equation (3) where \(\) is equal to \(\). We will consider both versions in our theorems.

### Function Classes

We state definitions and assumptions of function classes of interest, which are fairly standard.

**Definition 2.1** (Convexity).: A function \(f:^{d}\) is convex if, for any \(,^{d}\) and \(\), it satisfies \(f(+(1-)) f()+(1-)f()\).

**Definition 2.2** (\(\)-Strong Convexity).: A differentiable function \(f:^{d}\) is \(\)-strongly convex if there exists \(>0\) such that \(f()\!\!f()\!+\! f(),-+ }{2}\|-\|^{2}\), for all \(,^{d}\).

**Definition 2.3** (\(L\)-Lipschitz Continuity).: A function \(f:^{d}\) is \(L\)-Lipschitz continuous if there exists \(L 0\) such that \(\|f()-f()\| L\|-\|\), for all \(,^{d}\).

**Definition 2.4** (\(\)-Smoothness).: A differentiable function \(f:^{d}\) is \(\)-smooth if there exists \( 0\) such that \(\| f()- f()\|\|-\|\), for all \(,^{d}\).

Next we define an assumption considered in the analysis of stochastic SAM (3).

**Assumption 2.5** (Bounded Variance).: The gradient oracle of a differentiable function \(f:^{d}\) has bounded variance if there exists \( 0\) such that

\[_{}\| f()- l(;)\|^{2}^{2}, _{}\| f()- l(;)\|^{2} ^{2},^{d}.\]

### SAM as GD on Virtual Loss

Convergence analysis of SAM is challenging due to the presence of normalized ascent steps. In order to provide intuitive explanations of SAM's convergence properties, we develop tools referred to as _virtual gradient map_ and _virtual loss_ in this section. These tools can provide useful intuitions when we discuss our main results.

In order to define the new tools, we first need to define Clarke subdifferential , whose definition below is from Theorem 6.2.5 of Borwein and Lewis .

**Definition 2.6** (Clarke Subdifferential).: Suppose that a function \(f:^{d}\) is locally Lipschitz around a point \(\) and differentiable on \(^{d} W\) where \(W\) is a set of measure zero. Then, the Clarke subdifferential of \(f\) at \(\) is \( f()\{_{t} f (_{t})_{t},_{t} W\}\), where \(\) denotes the convex hull of a set.

Clarke subdifferential \( f()\) is a convex hull of all possible limits of \( f(_{t})\) as \(_{t}\) approaches \(\). It can be thought of as an extension of gradient to a nonsmooth function. It is also known that for a convex function \(f\), the Clarke subdifferential of \(f\) is equal to the subgradient of \(f\).

Using the definition of Clarke differential, we now define virtual gradient map and virtual loss of \(f\), which can be employed for understanding the convergence of SAM.

**Definition 2.7** (Virtual Gradient Map/Loss).: For a differentiable function \(f:^{d}\), we define the _virtual gradient map_\(G_{f}:^{d}^{d}\) of \(f\) to be \(G_{f}() f+)}{  f()})\). Additionally, if there exists a function \(J_{f}:^{d}\) whose Clarke subdifferential is well-defined and \( J_{f}() G_{f}()\) for all \(\), then we call \(J_{f}\) a _virtual loss_ of \(f\).

If a virtual loss \(J_{f}\) is well-defined for \(f\), the update of SAM (2) on \(f\) is equivalent to a (sub)gradient descent update on the virtual loss \(J_{f}()\), which means, \(_{t+1}=_{t}- G_{f}(_{t})\). The reason why we have to use Clarke subdifferential to define the virtual loss is because even for a differentiable and smooth function \(f\), there are cases where the virtual gradient map \(G_{f}\) is discontinuous and the virtual loss \(J_{f}\) (if exists) is nonsmooth; see the discussion below Theorem 3.1.

Note that if the differentiable function \(f\) is one-dimensional (\(d=1\)), the virtual loss \(J_{f}(x)\) is always _well-defined_ because it can be obtained by simply (Lebesgue) integrating \(G_{f}(x)\). However, in case of multi-dimensional functions (\(d>1\)), there is no such guarantee, although \(G_{f}\) is always well-defined. We emphasize that the virtual gradient map \(G_{f}\) and virtual loss \(J_{f}\) are mainly used for a better intuitive understanding of our (non-)convergence results. In formal proofs, we use them for analysis only if \(J_{f}\) is well-defined.

Lastly, we note that Bartlett et al.  also employ a similar idea of virtual loss. In case of convex quadratic objective function \(f()\), the authors define \(_{t}=(-1)^{t}_{t}\) and formulate a (different) virtual loss \(_{f}()\) such that a SAM update on \(f(_{t})\) is equivalent to a GD update on \(_{f}(_{t})\).

## 3 Convergence Analysis Under Deterministic Settings

In this section, we present the main results on the (non-)convergence of deterministic SAM with _constant_ perturbation size \(\) and gradient normalization. We study four function classes: smooth strongly convex, smooth convex, smooth nonconvex, and nonsmooth Lipschitz convex functions.

### Smooth and Strongly Convex Functions

For smooth strongly convex functions, we prove a global convergence guarantee for the best iterate.

**Theorem 3.1**.: _Consider a \(\)-smooth and \(\)-strongly convex function \(f\). If we run deterministic SAM starting at \(_{0}\) with any perturbation size \(>0\) and step size \(=\{\{1,( T^{ 2}}{^{3}^{2}})\},\}\) to minimize \(f\), we have_

\[_{t\{0,,T\}}f(_{t})-f^{*}=}((- )+^{2}}{^{5}T^{2}}).\]

The proof of Theorem 3.1 can be found in Appendix B.2. As for relevent existing studies, Theorem 1 of Dai et al.  can be adapted to establish the convergence guarantee \(f(_{T})-f^{*}=}()\), by selecting the step size \(=\{\{1,( T}{^{3}^{2}} )\},\}\) and employing a similar proof technique as ours. We highlight that Theorem 3.1 achieves a faster convergence rate compared to the concurrent bound by Dai et al. . Moreover, Theorem 11 of Andriushchenko and Flammarion  proves the convergence guarantee for this function class: \(\|_{T}-^{*}\|^{2}=((-T))\), but assuming SAM _without gradient normalization_, and the boundedness of \(\). Here, we get a slower convergence rate of \(}((-T)+})\), but with any \(>0\) and with normalization.

By viewing SAM as GD on virtual loss, we can get an intuition why SAM cannot achieve exponential convergence. Consider a smooth and strongly convex function: \(f(x)=x^{2}\). One possible virtual loss of \(f\) is \(J_{f}(x)=(x+(x))^{2}\), which is a _nonsmooth_ and strongly convex function, for which the exponential convergence of GD is impossible .

Indeed, we can show that the sublinear convergence rate in Theorem 3.1 is not an artifact of our analysis. Interestingly, the \(}((-T)+})\) rate given in Theorem 3.1 is _tight_ in terms of \(T\), up to logarithmic factors. Our next theorem provides a lower bound for smooth strongly convex functions.

**Theorem 3.2**.: _Suppose \( 2\). For any choice of perturbation size \(\), step size \(\), and initialization \(_{0}\), there exists a differentiable, \(\)-smooth, and \(\)-strongly convex function \(f\) such that_

\[_{t\{0,,T\}}f(_{t})-f^{*}=(^{ 2}}{^{2}T^{2}})\]

_holds for deterministic SAM iterates._

For the proof of Theorem 3.2, refer to Appendix B.3. By comparing the rates in Theorems 3.1 and 3.2, we can see that the two bounds are tight in terms of \(T\) and \(\). The bounds are a bit loose in terms of \(\) and \(\), but we believe that this may be partly due to our construction; in proving lower bounds, we used one-dimensional quadratic functions as worst-case examples. A more sophisticated construction may improve the tightness of the lower bound, which is left for future work.

### Smooth and Convex Functions

For smooth and convex functions, proving convergence of the function value to global optimum becomes more challenging, due to the absence of strong convexity. In fact, we can instead prove that the gradient norm converges to zero.

**Theorem 3.3**.: _Consider a \(\)-smooth and convex function \(f\). If we run deterministic SAM starting at \(_{0}\) with any perturbation size \(>0\) and step size \(=\{}{^{2}}T},\}\) to minimize \(f\), we have_

\[_{t=0}^{T-1}\| f(_{t})\|^{2}= +^{2}}}{} .\]

The proof of Theorem 3.3 is given in Appendix B.4. As for relevant existing studies, Theorem 11 of Andriushchenko and Flammarion  proves the convergence guarantee for this function class: \(f(})-f^{*}=()\), where \(}\) indicates the averaged \(\) over \(T\) iterates, while assuming SAM _without gradient normalization_ and bounded \(\). Here, we prove a weaker result: a convergence rate of \((})\) to _stationary points_, albeit for any \(>0\) and with normalization.

One might expect that a reasonably good optimizer should converge to global minima of smooth convex functions. However, it turns out that both showing convergence to global minima and finding a non-convergence example are quite challenging in this function class.

Indeed, we later show non-convergence examples for other relevant settings. For stochastic SAM, we provide a non-convergence example (Theorem 4.4) in the same smooth and convex function class, showing that the suboptimality gap in terms of function values cannot have an upper bound, and therefore rendering convergence to global minima impossible. For deterministic SAM in nonsmooth Lipschitz convex functions, we show an example (Theorem 3.6) where convergence to the global minimum is possible only up to a certain distance proportional to \(\).

Given these examples, we suspect that there may also exist a non-convergence example for the determinstic SAM in this smooth convex setting. We leave settling this puzzle to future work.

### Smooth and Nonconvex Functions

Existing studies prove that SAM (and its variants) with decaying or sufficiently small perturbation size \(\) converges to stationary points for smooth nonconvex functions . Unfortunately, with constant perturbation size \(\), SAM exhibits a different convergence behavior: it does not converge all the way to stationary points.

**Theorem 3.4**.: _Consider a \(\)-smooth function \(f\) satisfying \(f^{*}=_{}f()>-\). If we run deterministic SAM starting at \(_{0}\) with any perturbation size \(>0\) and step size \(=\) to minimize \(f\), we have_

\[_{t=0}^{T-1}\| f(_{t})\|^{2}()+^{2}^{2}.\]

Refer to the Appendix B.5 for the proof of Theorem 3.4. For a comparison, Theorem 9 of Andriushchenko and Hammarion  proves the convergence for this function class: \(_{t=0}^{T}\| f(_{t})\|^{2}=()\), but again assuming SAM _without gradient normalization_, and boundedness of \(\). Our Theorem 3.4 guarantees \(()\) convergence up to an additive factor \(^{2}^{2}\).

One might speculate that the undesirable additive factor \(^{2}^{2}\) is an artifact of our analysis. The next theorem presents an example which proves that this extra term is in fact unavoidable.

**Theorem 3.5**.: _For any \(>0\) and \(\), there exists a \(\)-smooth and \(()\)-Lipschitz continuous function such that, if deterministic SAM is initialized at a point \(_{0}\) sampled from a continuous probability distribution, then deterministic SAM converges to a nonstationary point, located at a distance of \(()\) from a stationary point, with probability \(1\)._

Here we present a brief outline for the proof of Theorem 3.5. For a given \(\), consider a one-dimensional function \(f(x)=}{25^{2}}(x)\). Figure 2(a) demonstrates the virtual loss for this example. By examining the virtual loss \(J_{f}\) and its stationary points, we observe that SAM iterates \(x_{t}\) converge to a non-stationary point, located at a distance of \(()\) from the stationary point. This means that the limit point of SAM will have gradient norm of order \(()\), thereby proving that the additive factor in Theorem 3.4 is tight in terms of \(\). A detailed analysis of Theorem 3.5 is provided in Appendix B.6.

Remark: why do we ignore \(=()\)?As the reader may have noticed, Theorem 3.5 only considers the case \(=()\), and hence does not show that the additive factor is inevitable for _any_ choice of \(>0\). However, one can notice that if \(=()\), then we can consider a one-dimensional \(()\)-strongly convex quadratic function and show that the SAM iterates blow up to infinity. For the same reason, in our other non-convergence results (Theorems 4.2 and 4.4), we only focus on \(=()\).

### Nonsmooth Lipschitz Convex Functions

Previous theorems study convergence of SAM assuming smoothness. The next theorem shows an example where SAM on a nonsmooth convex function converges only up to \(()\) distance from the global minimum \(^{*}\). This means that for constant perturbation size \(\), there exist _convex_ functions that prevent SAM from converging to global minima. In Appendix B.7, we prove the following:

**Theorem 3.6**.: _For any \(>0\) and \(<\), there exists a nonsmooth Lipschitz convex function \(f\) such that for some initialization, deterministic SAM converges to suboptimal points located at a distance of \(()\) from the global minimum._

## 4 Convergence Analysis Under Stochastic Settings

In this section, we present the main results on the convergence analysis of stochastic SAM, again with _time-invariant (constant)_ perturbation size \(\) and _gradient normalization_. We consider both types of stochastic SAM: \(n\)-SAM and \(m\)-SAM, defined in Section 2.2. We study four types of function classes: smooth strongly convex, smooth convex, smooth nonconvex, and smooth Lipschitz nonconvex functions, under the assumption that the gradient oracle has bounded variance of \(^{2}\) (Assumption 2.5). Our results in this section reveal that stochastic SAM exhibits different convergence properties compared to deterministic SAM.

### Smooth and Strongly Convex Functions

Theorem 3.1 shows the convergence of deterministic SAM to global optima, for smooth strongly convex functions. Unlike this result, we find that under stochasticity and constant perturbation size \(\), both \(n\)-SAM and \(m\)-SAM ensure convergence _only up to an additive factor \((^{2})\)_.

**Theorem 4.1**.: _Consider a \(\)-smooth, \(\)-strongly convex function \(f\), and assume Assumption 2.5. Under \(n\)-SAM, starting at \(x_{0}\) with any perturbation size \(>0\) and step size\[\{\{1,(  T}{[^{2}-^{2}^{2}]_{+}})\},\}f\] \[f(_{T})-f^{*}}( (-)+-^{2}^ {2}]_{+}}{^{2}T})+^{2}}{}.\]

_Under \(m\)-SAM, additionally assuming \(l(,)\) is \(\)-smooth for any \(\), the inequality continues to hold._

For the proof, please refer to Appendix C.2. Theorem 4.1 provides a convergence rate of \(}()\) to global minima, but only up to suboptimality gap \(^{2}}{}\). If \(\), then Theorem 4.1 becomes:

\[f(_{T})-f^{*}}((-))+^{2}}{},\]

thereby showing a convergence rate of \(((-T))\) modulo the additive factor.

For relevant existing studies, Theorem 2 of Andriushchenko and Flammation  proves the convergence guarantee for smooth Polyak-Lojasiewicz functions: \(f(_{T})-f^{*}=()\), but assuming stochastic SAM _without gradient normalization_, and perturbation size \(\) decaying with \(t\). In contrast, our analysis shows that the convergence property can be different with normalization and constant \(\).

The reader might be curious if the additional \((^{2})\) term can be removed. Our next theorem proves that in the case of high gradient noise (\(>\)), \(m\)-SAM with constant perturbation size \(\) cannot converge to global optima beyond a suboptimality gap \((^{2})\), when the component function \(l(;)\) is smooth for any \(\). Hence, the additional term in Theorem 4.1 is _unavoidable_, at least for the more practical version \(m\)-SAM.

**Theorem 4.2**.: _For any \(>0,>0,>\) and \(\), there exists a \(\)-smooth and \(\)-strongly convex function \(f\) satisfying the following. (1) The function \(f\) satisfies Assumption 2.5. (2) The component functions \(l(;)\) of \(f\) are \(\)-smooth for any \(\). (3) If we run \(m\)-SAM on \(f\) initialized inside

Figure 2: Examples of virtual loss plot for deterministic and stochastic SAM. The graph drawn in green indicates \(f\), and the graph drawn in blue indicates \(J_{f}\) (or \(J_{f}\)). Red arrows indicate the (expected) directions of SAM update. The (expected) updates are directed to red stars. (a) \(f\) and \(J_{f}\) in Theorem 3.5. (b) \(f\) and its component functions \(f^{(1)}\), \(f^{(2)}\) in Theorem 4.2. (c) \(J_{f}\) and its component functions \(J_{f^{(1)}}\), \(J_{f^{(2)}}\) in Theorem 4.2. (d) \(f\) and its component functions \(f^{(1)}\), \(f^{(2)}\) in Theorem 4.4. (e) \(J_{f}\) and its component functions \(J_{f^{(1)}}\), \(J_{f^{(2)}}\) in Theorem 4.4. For the simulation results of SAM trajectories on these functions, refer to Appendix D.

a certain interval, then any arbitrary weighted average \(}\) of the iterates \(_{0},_{1},\) must satisfy \([f(})-f^{*}](^{2})\)._

Here we provide a brief outline for Theorem 4.2. The in-depth analysis is provided in Appendix C.3. Given \(>0\), \(>0\), \(>\), we consider a one-dimensional quadratic function \(f(x)\) whose component function \(l(x;)\) is carefully chosen to satisfy the following for \(x[,]\):

\[f(x)=[l(x;)]=x^{2}, l(x;)=- (x-)^{2},&\\ x^{2}+(x-)^{2},& \]

For values of \(x\) outside this interval \([,]\), each component function \(l(x;)\) takes the form of a strongly convex quadratic function. Figures 2(b) and 2(c) illustrate the original and virtual loss function plots of \(l(x;)\). The _local concavity_ of component function plays a crucial role in making an attracting basin in the virtual loss, and this leads to an interval \([,]\) bounded away from the global minimum \(0\), from which \(m\)-SAM iterates cannot escape.

For this scenario, we obtain \(f(x_{t})-f^{*}=(^{2})\) and \(\| f(x_{t})\|^{2}=(^{2})\) for all iterates. From this, we can realize that the additive factor in Theorem 4.1 for \(m\)-SAM is unavoidable in the \(>\) regime, and tight in terms of \(\). Moreover, the proof of Theorem 4.2 in Appendix C.3 reveals that even in the \(\) case, an analogous example gives \(\|x_{t}-x^{*}\|=()\) for all iterates; hence, \(m\)-SAM fails to converge all the way to the global minimum in the small \(\) regime as well.

### Smooth and Convex Functions

We now move on to smooth convex functions and investigate the convergence guarantees of stochastic SAM for this function class. As can be guessed from Theorem 3.3, our convergence analysis in this section focuses on finding stationary points. Below, we provide a bound that ensures convergence to stationary points up to an additive factor.

**Theorem 4.3**.: _Consider a \(\)-smooth, convex function \(f\), and assume Assumption 2.5. Under \(n\)-SAM, starting at \(x_{0}\) with any perturbation size \(>0\) and step size \(=\{}{-^{2}^{2 }]_{+}T}},\}\) to minimize \(f\), we have_

\[_{t=0}^{T-1}\| f(_{t})\|^{2}= +-^{ 2}^{2}]_{+}}}{}+4^{2}^{2}.\]

_Under \(m\)-SAM, additionally assuming \(l(,)\) is \(\)-smooth for any \(\), the inequality continues to hold._

The proof of Theorem 4.3 can be found in Appendix C.4. Theorem 4.3 obtains a bound of \((})\) modulo an additive factor \(4^{2}^{2}\). Similar to Theorem 4.1, if \(\), then Theorem 4.3 reads

\[_{t=0}^{T-1}\| f(_{t})\|^{2}= +4^{2}^{2},\]

hence showing a convergence rate of \(()\) modulo the additive factor. Since the non-convergence example in Theorem 4.2 provides a scenario that \(\| f(x_{t})\|^{2}=(^{2})\) for all \(t\), we can see that the extra term is inevitable and also tight in terms of \(\).

Theorem 4.3 sounds quite weak, as it only proves convergence to a stationary point only up to an extra term. One could anticipate that stochastic SAM may actually converge to global minima of smooth convex functions modulo the unavoidable additive factor. However, as briefly mentioned in Section 3.2, the next theorem presents a counterexample illustrating that ensuring convergence to global minima, even up to an additive factor, is impossible for \(m\)-SAM.

**Theorem 4.4**.: _For any \(>0\), \(>0\), \(>0\), and \(\), there exists a \(\)-smooth and convex function \(f\) satisfying the following. (1) The function \(f\) satisfies Assumption 2.5. (2) The component functions \(l(;)\) of \(f\) are \(\)-smooth for any \(\). (3) If we run \(m\)-SAM on \(f\) initialized inside a certain interval, then any arbitrary weighted average \(}\) of the iterates \(_{0},_{1},\) must satisfy \([f(})-f^{*}] C\), and the suboptimality gap \(C\) can be made arbitrarily large and independent of the parameter \(\)._

Here, we present the intuitions of the proof for Theorem 4.4. As demonstrated in Figures 2(d) and 2(e), the _local concavity_ of the component function significantly influences the formation of attractingbasins in its virtual loss, thereby creating a region from which the \(m\)-SAM updates get stuck inside the basin forever.

Also note that we can construct the function to form the basin at any point with arbitrary large function value (and hence large suboptimality gap). Therefore, establishing an upper bound on the convergence of function value becomes impossible in smooth convex functions. A detailed analysis for the non-convergence example is presented in Appendix C.5.

### Smooth and Nonconvex Functions

We now study smooth nonconvex functions. Extending Theorem 3.4, we can show the following bound for stochastic \(n\)-SAM.

**Theorem 4.5**.: _Consider a \(\)-smooth function \(f\) satisfying \(f^{*}=_{x}f()>-\), and assume Assumption 2.5. Under \(n\)-SAM, starting at \(_{0}\) with any perturbation size \(>0\) and step size \(=\{,}{T}}\}\) to minimize \(f\), we have_

\[_{t=0}^{T-1}\| f(_{t})\|^{2} (+} }{})+^{2}^{2}.\]

The proof of Theorem 4.5 is provided in Appendix C.6. Notice that the non-convergence example presented in Theorem 3.5 (already) illustrates a scenario where \(\| f(x_{t})\|^{2}=(^{2})\), thereby confirming the tightness of additive factor in terms of \(\).

The scope of applicability for Theorem 4.5 is limited to \(n\)-SAM. Compared to \(n\)-SAM, \(m\)-SAM employs a stronger assumption where \(=\). By imposing an additional Lipschitzness condition on the function \(f\), \(m\)-SAM leads to a similar but different convergence result.

**Theorem 4.6**.: _Consider a \(\)-smooth, \(L\)-Lipschitz continuous function \(f\) satisfying \(f^{*}=_{x}f()>-\), and assume Assumption 2.5. Additionally assume \(l(,)\) is \(\)-smooth for any \(\). Under \(m\)-SAM, starting at \(_{0}\) with any perturbation size \(>0\) and step size \(=}{+L^{2})T}}\) to minimize \(f\), we have_

\[_{t=0}^{T-1}[(\| f(_{t})\| -)^{2}](+L^{2})}}{})+5^{2}^{2}.\]

**Corollary 4.7**.: _Under the setting of Theorem 4.6, we get_

\[_{t\{0,,T\}}\{\| f(_{t})\|\} (+L^{2}))^{1/4}}{T^{1/4}})+ (1+).\]

The proofs for Theorem 4.6 and Corollary 4.7 are given in Appendix C.7. Since Theorem 3.5 presents an example where \(\| f(_{t})\|=()\) and \((\| f(_{t})\|-)^{2}=(^{2})\), we can verify that the additive factors in Theorem 4.6 and Corollary 4.7 are tight in terms of \(\).

As for previous studies, Theorems 2, 12, and 18 of Andriushchenko and Flammarion  prove the convergence of \(n\),\(m\)-SAM for smooth nonconvex functions: \(_{t=0}^{T}\| f(_{t})\|^{2}=( })\), but assuming SAM _without gradient normalization_, and _sufficiently small_\(\). For \(m\)-SAM on smooth Lipschitz nonconvex functions, Theorem 1 of Mi et al.  proves \(_{t=0}^{T}\| f(_{t})\|^{2}=}(})\), while assuming _decaying_\(\). From our results, we demonstrate that such full convergence results are impossible for practical versions of stochastic SAM.

## 5 Conclusions

This paper studies the convergence properties of SAM, under constant \(\) and with gradient normalization. We establish convergence guarantees of deterministic SAM for smooth and (strongly) convex functions. To our surprise, we discover scenarios in which deterministic SAM (for smooth nonconvex functions) and stochastic \(m\)-SAM (for all function class considered) converge only up to _unavoidable_ additive factors proportional to \(^{2}\). Our findings emphasize the drastically different characteristics of SAM with vs. without decaying perturbation size. Establishing tighter bounds in terms of \(\) and \(\), or searching for a non-convergence example that applies to \(n\)-SAM might be interesting future research directions.

#### Acknowledgments

This paper was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant (No. 2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)) funded by the Korea government (MSIT), two National Research Foundation of Korea (NRF) grants (No. NRF-2019R1A5A1028324, RS-2023-00211352) funded by the Korea government (MSIT), and a grant funded by Samsung Electronics Co., Ltd.