# What Makes CLIP More Robust to

Long-Tailed Pre-Training Data?

A Controlled Study for Transferable Insights

 Xin Wen\({}^{1}\) Bingchen Zhao\({}^{2}\) Yilun Chen\({}^{3}\) Jiangmiao Pang\({}^{3}\) Xiaojuan Qi\({}^{1}\)

\({}^{1}\)The University of Hong Kong \({}^{2}\)University of Edinburgh \({}^{3}\)Shanghai AI Laboratory

{wenxin, xjqi}@eee.hku.hk pangjiangmiao@pjlab.org.cn

Corresponding author.

###### Abstract

Severe data imbalance naturally exists among web-scale vision-language datasets. Despite this, we find CLIP pre-trained thereupon exhibits notable robustness to the data imbalance compared to supervised learning, and demonstrates significant effectiveness in learning generalizable representations. With an aim to investigate the reasons behind this finding, we conduct controlled experiments to study various underlying factors, and reveal that CLIP's pretext task forms a dynamic classification problem wherein only a subset of classes is present in training. This isolates the bias from dominant classes and implicitly balances the learning signal. Furthermore, the robustness and discriminability of CLIP improve with more descriptive language supervision, larger data scale, and broader open-world concepts, which are inaccessible to supervised learning. Our study not only uncovers the mechanisms behind CLIP's generalizability beyond data imbalance but also provides transferable insights for the research community. The findings are validated in both supervised and self-supervised learning, enabling models trained on imbalanced data to achieve CLIP-level performance on diverse recognition tasks. Code and data are available at: https://github.com/CVMI-Lab/clip-beyond-tail.

## 1 Introduction

The development of contrastive language-image pre-training (CLIP) [36; 44; 57; 68; 93] has demonstrated unprecedented success in learning generalizable representations, empowering zero-shot vision tasks and robustness to natural distributional shifts. This success can be primarily attributed to the effective use of large-scale uncurated image captioning datasets collected from the web. A recent trend involves delving into the distribution of these datasets and explicitly introducing interventions to the curation process to create better data for training [29; 91]. However, limited research has been conducted on analyzing the distribution of concepts/classes in these datasets and the behavior of CLIP under varying distributions. This work thus starts by presenting a _concept-centric_ analysis of existing web-scale image-text datasets and models pre-trained accordingly (Fig. 1).

**Motivation.** Our motivation for this study arises from an intriguing observation of CLIP's zero-shot performance on ImageNet: CLIP is notably more robust to pre-trained data imbalance than supervised learning. We examine various vision-language datasets at different scales, and analyze their distribution with respect to ImageNet classes. We find that image-text datasets share an extremely imbalanced class distribution (Fig. 1a). Interestingly, we find that the zero-shot classification performance of trained CLIP models is more robust to this imbalance, especially compared to models obtained by supervised learning. This is evidenced by a weaker correlation between a class's performance and itsfrequency (Fig. 0(b)). This trend is consistent across CLIP models and pre-training datasets and even holds true for smaller-scale datasets like CC-12M . This phenomenon inspires us to study the underlying causes for CLIP's relative robustness toward data imbalance and what we can learn from.

**Our study and findings.** To answer the question above, we conduct controlled experiments to analyze factors including supervision signal and pretext task (Fig. 3), data distribution (Fig. 4), scale (Fig. 5), and open-world concepts (Fig. 6). Our extensive studies have led us to the following findings:

* Language supervision, particularly the texts with increased descriptiveness (informativeness), enhances both the robustness and discriminability of CLIP, and preserves more feature variation.
* CLIP's pretext task forms dynamic classification problems, wherein only a subset of classes is present during training, effectively isolates biases to dominant classes, and balances learning signal.
* Severe data imbalance in web datasets increases the risk of bias in models. However, distribution shift and higher data diversity in them can enhance robustness, albeit a trade-off in data efficiency.
* CLIP's robustness and discriminability improve together with data scaling, benefitting from its ability to utilize open-world data, a privilege not accessible to supervised learning.

**Applications.** Inspired by the findings of our study, we found that this robustness to data imbalance can be transferred to supervised and self-supervised learning models with simple techniques by making the classification task dynamic during training. Under extremely imbalanced data scenarios, we show that a vanilla classification model can also generalize well to tail (or even open-world) classes as well as CLIP via 1) fixing the classifier with class prototypes from pre-trained CLIP text encoder, and 2) training with randomly subsampled vocabulary (results in Fig. 8, and analysis in Fig. 9). Beyond classification, we also show improved transferability on DINO  pre-trained on uncurated web data by simply randomly subsampling the prototypes in training (Fig. 10).

**Summary.** Our study is one of the pioneering efforts to explore CLIP's robustness in the context of imbalanced data distributions. Our exploration provides a comprehensive analysis that uncovers the mechanisms contributing to CLIP's robustness against data imbalance. As we will demonstrate in this paper, the insights gained from our research are transferable to other domains, including supervised and self-supervised learning frameworks.

Figure 1: Per-class statistics of image-text datasets and models trained on top. (a) A highly imbalanced class distribution is _shared_ across datasets.\({}^{}\)(b) Compared to supervised learning (\(\) SL), CLIP’s performance (measured by \(\) accuracy) is _less biased_ by data frequency, and the classifier is notably uncorrelated (measured by model’s number of \(\) prediction per class). Besides, the correlation narrows as data scales up. Both aspects indicate implicit re-balancing mechanisms exist in CLIP.

## 2 Related work

**CLIP's distributional robustness.** The debut of CLIP not only set the state-of-the-art performance on conventional image classification benchmarks but also demonstrated unprecedented robustness to challenging distribution shifts. Studies have shown that this robustness stems from the diverse training distributions CLIP has seen during training time [27; 69]. Also, it is shown that the data quality plays an important role in enhancing the distributional robustness of CLIP . It may seem that CLIP obtains the improvement distributional robustness due to the similarity of pretraining data to the distribution shifted data, but  shows that it is not the case where even after pruning similar data, CLIP still obtains strong robustness, indicating generalizable representations are learned.

**Learning from uncurated data.** Apart from robustness to distribution shifts, previous works have also delved into the nature of uncurated large-scale datasets [35; 49; 77; 91]. Studies have shown that self-supervised learning can produce more robust models than supervised learning on uncurated data [35; 49]. Moreover, focusing on learning of subsets of the entire dataset [9; 82] has shown to further enhance self-supervised learning from uncurated data. On the learning on uncurated data, the language information has shown to help learn good representations . Balancing the concept distribution of uncurated data has shown to be a scalable way of learning good models . However, the uncurated data is not all harmful for performance, the lower intra-class similarity of the data is shown to help preserve information/variation in representations , but at low data efficiency .

**Generalization of vision models.** One of the main themes of computer vision research in the era of deep learning is the search for more generalizable models. Works have focused on self-supervised pretraining with only images, among which contrastive learning  and self-distillation [11; 61] are shown to be effective. With the introduction of large-scale image-text datasets [73; 74], there is a huge interest in learning more generalizable vision representations from additional language supervision. While techniques for incorporating language supervision have been proposed [19; 36; 68; 72; 94], further exploration of how semantic grounding help improves the generalization is needed . To fully utilize language supervision, using synthetic data from large language models to improve language supervision is a newly emerged research area [25; 26].

## 3 What makes CLIP more robust to long-tailed pre-training data?

In the following, we conduct a series of controlled experiments to systematically evaluate the role of various factors on the robustness of CLIP to data imbalance. These factors include supervision signal (Sec. 3.2), pretext task (Sec. 3.3), data distribution (Sec. 3.4), data scale (Sec. 3.5), and open-world concepts (Sec. 3.6). Moreover, we also provide some insights on CLIP's feature space in Sec. 3.7.

Figure 2: Curation process and distribution of datasets used in our controlled study. Top: IN-Caps  augments train images of ImageNet with texts by querying Flickr with image URLs. The texts include title, description, and tags. Bottom: LAIONet  is a filtered subset of LAION-400M , obtained by matching ImageNet classes with captions and filtering by CLIP text encoder for disambiguation.

### Setting

**Datasets.** Experiments in this study are conducted on variants of two image-text datasets: ImageNet-Captions  and LAIONet  to allow better data-centric control. An overview is shown in Fig. 2. Both datasets provide images with their paired captions, and class labels on ImageNet. The captions of ImageNet-Captions are in the format of title, description, and tags (some can be missing for a specific image), which allows control of captions' descriptiveness. Images of LAIONet are drawn from LAION, which has a higher intra-class variability and is extremely imbalanced across classes. This makes it more challenging to train on and allows isolating the effect of data distribution.

**Models.** We consider both CLIP and supervised learning (SL) with ResNet-50 as the backbone. Given that CNNs are generally considered less robust than ViTs , this choice also enables us to infer the robustness of other models. For SL, we align most details with CLIP  to rule out the effect of irrelevant factors. _E.g._, we use the same weak data augmentation as CLIP, adopt a prototypical classification head (_i.e._, \(_{2}\)-normalizing both features and classifier weights), and apply a learnable temperature to logits. The training schedules of CLIP and SL follow  and , respectively. Models are fully trained from scratch by default. More details are provided in Appx. C.

**Metrics.** We compute Spearman correlation coefficients  between class frequency and models' statistics (class-wise top-1 accuracy and number of samples predicted as each class). Besides, we also consider metrics from neural collapse literature [32; 63] for analyzing feature distribution. Formally, defining the global feature mean \(_{G}=_{i,c}_{i,c}\), class-level means \(_{c}=_{i}_{i,c}\), within-class covariance \(_{W}=_{i,c}(_{i,c}- _{c})(_{i,c}-_{c})^{}\), and between-class covariance \(_{B}=_{c}(_{c}- _{G})(_{c}-_{G})^{}\), the metrics are defined as:

\[=_{W} _{B}^{}/C,= _{c,c^{}}_{c}^{} _{c^{}}}{\|_{c}\|\|_{c^ {}}\|}+,\] (1)

where \(\) denotes the Moore-Penrose pseudoinverse, \(_{i,c}\) is the feature of the \(i\)-th example in class \(c\), and \(C\) is the total number of classes. Intuitively, NC1 and NC2 measure the compactness and separation of clusters, respectively. NC1 approaches zero when the within-class variation of features becomes negligible, and NC2 converges to zero when classifiers reach maximal and equal margins (_i.e._, ETF structure) . Note that these two metrics are originally defined as an average across classes, and it is simple to obtain per-class NC1 and NC2 metrics, measuring the variability of _a specific class_ or its average margin to all other classes.

### (Descriptive) language as supervision signal

**Setting.** We start by examining the impact of language supervision, the primary distinction between CLIP and other contrastive learning approaches. This is done by creating _texts with roughly monotonic

Figure 3: Results on IN-Caps about \(\) text descriptiveness and \(\) vocabulary size. 1) Increasing \(\) text descriptiveness improves both robustness (a) and discriminability (b) of CLIP, but the tendency varies if using \(\) less descriptive (template-based) supervision. 2) The gap between SL and CLIP (a) implies CLIP re-balances predictions, which is replicable by \(\) subsampling the vocabulary SL trains with.

increasing descriptiveness_ given metadata of ImageNet-Captions. For the low-diversity texts, we create \(\) synthetic class-centric texts using classification templates from CLIP  given class names or synset . The \(\) natural language-based texts are created by concatenating different types of captions (see Fig. 2), and the descriptiveness of language supervision is controlled by the number of text types used. More details are available in Appx. C.2.

**Results.** Fig. 3 provide a comprehensive comparison between model variants from different perspectives. Restricting our view to CLIP models in the first two subfigures, \(\) higher text descriptiveness results in improvements in both robustness and discriminability of CLIP, as shown by lower correlation (Fig. 2(a)) and higher overall accuracy (Fig. 2(b), \(y\)-axis). On the other hand, \(\) relatively less descriptive texts show weaker results that are close to results of \(\) templated-based CLIP (Fig. 2(a), \(x\)-axis). We see this as less descriptive texts could collapse to class-centric supervision without much additional variance. Despite this, predictions of \(\) template-based CLIP are still notably less biased by pre-training data than \(\) SL (Fig. 2(b)), indicating other factors may re-balance CLIP's predictions.

### Dynamic classification (using subsampled vocabulary) as pretext task

**Setting.** We note that the pretext of \(\) template-based CLIP still differs from \(\) SL. Although both formed as discrimination tasks, the vocabulary (classes in a mini-batch) of CLIP is much smaller than SL (all classes). Take using a batch size of 1024 for instance, after deduplication, the vocabulary only contains around 600 classes (for ImageNet-Captions). If negative samples are not shared across devices, the vocabulary received by each GPU can be even smaller. In contrast, the vocabulary of SL is consistent: 1000 classes for ImageNet. Considering CLIP sees far more than 1000 classes from a web-crawled dataset, the _portion_ that CLIP's training vocabulary takes is even smaller. To isolate the influence of training vocabulary, we experiment by forming dynamic classifiers during SL training. This is done by randomly subsampling the vocabulary (candidate classes) to a smaller size during training, thus forming dynamic classification tasks similar to CLIP (see details in Appx. C.3).

**Results.** As shown in Fig. 2(a), sampling a \(}\) smaller vocabulary notably reduces SL's prediction bias, and obtains robustness similar to \(}\) template-based CLIP. Regarding the favorable vocabulary size, smaller ones are more effective in reducing prediction bias (Fig. 2(a)), and intermediate ones also improve accuracy (Fig. 2(b)). The preferred vocabulary size for ImageNet-Captions is around 100.

**Discussion.** Our intuition of the phenomena above is that dynamic classification in some way achieves class-level re-balancing. When the ground truth (GT) corresponds to a tail class, a small vocabulary isolates the negative impact of most head classes, avoiding bias towards them and enabling the model to focus on classifying the tail class itself. Besides, it is worth noting that as demonstrated in [32; 50], optimization continues after the model's predictions reach zero error, and seeks minimum intra-class variability and maximum inter-class margin (especially larger margin around head classes). Thus when the GT is a head class, this approach limits the number of negative classes and could prevent the model from excessively distorting the representations of them through over-optimization.

### Data distribution (level of imbalance, web distribution shift, and intra-class diversity)

**Motivation.** Motivated by the findings of  regarding the impact of image distribution on CLIP's robustness to natural distribution shifts, our study also examines its influence on robustness to data imbalance. As shown in , a higher filter threshold leads to a more condensed image distribution, a result that is confirmed in Fig. 3(a). We thus create LAIONet variants of different intra-class variations by adjusting this threshold. All variants in this section keep the data scale the same as ImageNet-Captions (0.45M). In addition, due to the disparity in class distribution between LAIONet and ImageNet-Captions, we also create a variant that aligns with the class frequencies of ImageNet-Captions ('=freq') while preserving web image distribution. This variant is sampled from the full version (3.26M) that uses a threshold of 0.7. More details about datasets are provided in Appx. C.5.

**Results.** A comparison between models trained on the aforementioned datasets is present in Fig. 3(b). We find that web data is not naturally friendly for de-biasing, but could have made models more biased due to extreme data imbalance (comparing '=freq' with other columns). The distribution shift of web data could improve robustness if a \(\) pre-trained text head is available (circles _vs._ squares, last column). If not, scaling may help. Moreover, results with smaller thresholds also turn out to be more robust, indicating that higher intra-class data diversity (smaller threshold) improves robustness.

### Data scaling (also achievable via language pre-training)

**Motivation.** We note that the correlations of CLIP in Fig. 2(a) (\(x\)-axis) are still higher than that of open-source models in Fig. 0(b). One key remaining factor is the scale of pre-training data (see Fig. 0(b) for large-scale results). Given that ImageNet-Captions is small-scaled (see Fig. 2), experiments following are conducted on LAIONet. See Appxs. C.4 and C.5 for more details about the setting.

**Results.** Fig. 5 presents the results obtained from uniformly subsampled subsets of LAIONet. These findings extend the scaling law: as data scales, ImageNet zero-shot accuracy (Fig. 4(a)) and models' robustness to data imbalance (Fig. 4(b)) improve simultaneously. We also provide a comparison between text encoders: \(\) training from scratch, initializing with \(\) pre-trained CLIP (frozen) or \(\) frozen RoBERTa , or \(\) fine-tuning the text encoder together. \(\) Frozen CLIP language head enables the vision model to leverage a well-established feature space as supervision, achieving better data efficiency (Fig. 4(a)) and robustness to data imbalance (Fig. 4(b)). \(\) Fine-tuning CLIP text head results in over-fitting (similar results with \(\) training from scratch), and \(\) RoBERTa does not suit the contrastive task and adversarially affects performance. Further investigation through NC-based metrics shows \(\)\(\) frozen heads effectively preserves intra-class variation (Fig. 4(c)), which is at risk of being lost when \(\) fine-tuned. Both \(\) frozen and \(\) fine-tuned heads contribute to inter-class margins (Fig. 4(d)), and if \(\) randomly initialized, scaling training data still can achieve improved margins. Compared to \(\) SL, CLIP can better utilize web-crawled data and pre-trained text encoder (Fig. 4(a)). But note that when evaluating close-set accuracy, the data efficiency of CLIP is still much lower than SL trained on classification datasets (_e.g._, ImageNet).

Figure 4: Results on LAIONet about data distribution (level of data imbalance, distribution shift, and data diversity). 1) Extreme data imbalance makes models more prone to bias (last column _vs._ others). 2) Distribution shift (\(\)_vs._mm, last column) harms discriminability but could improve robustness if pre-trained text head is used. 3) Higher data diversity (smaller threshold) also improves robustness.

Figure 5: Results on LAIONet subsets about data scale and text encoder. 1) CLIP’s discriminability (a) and robustness (b) co-improve as data scales up, and can be boosted by pre-trained heads. 2) A frozen head helps CLIP preserve intra-class variation (c) while not harming margins (d), which can be lost if fine-tuned. It is also unattainable by SL even using the same head. 3) Language pre-training using CLIP is more favorable for image-text tasks than pure language modeling (_e.g._, RoBERTa ).

### Utilization of open-world concepts

**Motivation.** One overlooked factor in Sec. 3.5 (on 1K ImageNet classes) is the existence of massive open-world concepts in web-crawled datasets. CLIP only requires weak image-text supervision and is thus not bound by a pre-defined vocabulary. The open-world concepts may share useful information with close-set ones and generalization could happen when data scales up. This section presents experiments on ImageNet-Captions and YFCC-15M subsets that reveal scaling effects of the number of concepts/classes. Results are shown in Fig. 6 and details of datasets can be found in Appx. C.5.

**Results.** We present results on ImageNet-Captions subsets (evaluate on 100 classes) and YFCC-15M subsets (evaluate on 1K classes) in Fig. 6 to validate this. IN-Caps-100 stands for a 100-class subset of ImageNet-Captions, and IN-Caps (10%) denote a 1K-class subset at the same scale as IN-Caps-100. In Fig. 5(a), both SL and CLIP attain additional robustness from the scaling of concept and data. However, expanding the vocabulary for SL is label-expensive in practice. Thus concepts other than ImageNet classes in YFCC-15M do not benefit SL in Fig. 5(b).

### Understanding the feature distribution of CLIP pre-trained at scale

**Setting.** The results above have shown that the discriminability and robustness to data imbalance improve simultaneously as pre-training data scales up (Sec. 3.5). Then if pre-trained on sufficient data, when does CLIP fail (Fig. 6(a).1), what does data imbalance affect (Fig. 6(a).2), and how are they reflected in the feature space (Fig. 6(b))? To answer these questions, we consider 3 vision feature-related metrics (\(\) NC1, \(\) NC2\({}_{M}\), \(\) NC2\({}_{M}^{nn}\)) and 2 text feature-related metrics (\(\) NC2\({}_{W}\), \(\) NC2\({}_{W}^{nn}\)). NC2\({}_{M}\) uses vision feature centers, and NC2\({}_{W}\) takes CLIP's text classifier as feature centers. Margins are computed as average over all other classes for NC2, and that to the nearest neighbor for NC2\({}^{nn}\).

**Results.** Cluster compactness (\(\) NC1) does not show a strong correlation with CLIP's failures (Fig. 6(a).1), and the frequent classes of LAION models tend to preserve more intra-class variation (Fig. 6(b).2). Besides, there are some implications from the margin between class centers (\(\)\(\) NC2).

Figure 7: Inspecting CLIP’s failures and effects of data imbalance from NC-based metrics. 1) Fail classes of smaller-scale models (12/15M) are hardly discriminative to most classes, while larger-scale models (\(\) 400M) only fail on some nearest-neighbor classes. 2) Data imbalance is weakly correlated with most feature statistics except NC2\({}_{W}\), denoting denser head and coarser tail classes in text space.

Figure 6: CLIP can benefit from open-world concepts. (a) Train on IN-Caps variants, and evaluate on 100 classes. (b) Train on YFCC-15M variants, and evaluate on 1K classes.

For example, Fig. 6(a).1 shows that the fail classes of smaller-scale models (12/15M) are hardly discriminative to most classes (\(\) NC2\({}_{M}\)), while larger-scale models (\(\) 400M) only fail on some nearest-neighbor classes (\(\) NC2\({}_{M}^{nn}\)). This indicates that the failing classes already have good separation from most other classes, and the confusion primarily comes from very few hard classes. Regarding the effects of data imbalance on CLIP (Fig. 6(a).2), we find a strong connection to \(\) NC2\({}_{W}\), denoting denser head and coarser tail classes in text space. t-SNE  of the class centers is provided in Fig. 6(b) for reference, and more visualizations of vision features can be found in Fig. 20.

**Discussions.** Though weakly correlated to the class frequency, CLIP's performance is still highly biased . If data imbalance is not the main cause, then what are other suspect of CLIP's failures? We hypothesize that ImageNet is intrinsically biased. The classes are not of equal difficulty  and some are even ambiguous , _e.g._, "sunglass" _vs._ "sunglasses". In this case, it is possible for a model trained on the balanced ImageNet to be biased , and some errors are unsolvable no matter how much training data is added. Besides, CLIP leverages open-world concepts in training, which are not counted for frequency but still could affect close-set performance. Moreover, such biases might be connected with CLIP's hallucination . We believe these are valuable questions to be explored. In supplement to this discussion, we also discuss CLIP's bias measured on broader sets of concepts in Appx. A.2 and the effects of data imbalance on CLIP in Appx. A.5.

## 4 Acquiring CLIP-level generalization

This section shows findings from CLIP's underlying mechanisms can be applied to both supervised learning (Sec. 4.1) and self-supervised learning (Sec. 4.2) under severe data imbalance.

### Data-imbalanced learning: an extreme case

In quest of the limit of CLIP's robustness to pre-training data imbalance, we create an extreme case based on ImageNet-Captions: trimming the tail classes to only one shot, or even completely zero shot (_i.e._, an open-world setting). We then train models on this trimmed dataset, and evaluate performance on ImageNet regarding tail/other classes. As shown in Fig. 8, at the scale of ImageNet-Captions (\(\)0.45M), \(\) CLIP trained from scratch also fails on tail classes when trained under severe data imbalance. Despite this, by adopting a \(\) pre-trained text encoder following Sec. 3.5, CLIP acquires open-world knowledge and demonstrates superior generalization on tail (and open-world) classes. Then how much can an SL model acquire such generalization? Surprisingly, we find training it with \(\) frozen class prototypes produced by CLIP text head is not effective. Instead, also \(\) subsampling the vocabulary during training is necessary to achieve a similar level of generalization as CLIP.

To understand the underlying mechanisms, we present a case study on the affinity matrix between classifiers, and tail class accuracies under the zero-shot tail (50 classes) setting in Fig. 9. The affinity matrices of the classification head (see Fig. 8(a), we subsample 100 classes for visualization) demonstrate that the learned tail prototypes collapse to singularity, while the class prototypes from

Figure 8: An extreme case: we train SL models on IN-Caps variants that have tail classes trimmed to only one shot (a & b) or even zero shot (c & d), and evaluate the accuracy on the tail and other classes. \(\) CLIP with a frozen pre-trained text encoder shows superior generalization, which can be acquired by a \(\) SL model with \(\) fixed class prototypes from CLIP and \(\) vocabulary subsampling.

CLIP maintain a healthier structure. Replacing the learned head with frozen CLIP prototypes alleviates classifier bias. However, per-class accuracies (see Fig. 8(b)) show that using this head alone is merely effective, only small improvements are observed in very few classes, indicating that the representations are still biased. Additionally, applying vocabulary subsampling overcomes the hidden bias in supervision, allows the representations to fit the manifold encoded by CLIP text embeddings, and generalizes to open classes that CLIP has seen in pre-training. We note that this setting shares similarities with open-vocabulary recognition. Surprisingly, we indeed find a similar technique (termed federated loss) used in open-vocabulary object detection (OVOD) , but few explorations exist in the relevant literature. Our study provides a thorough analysis of this technique from another perspective, and we hope it can motivate future applications in this field.

### Empowering self-supervised learning in-the-wild at scale

To show the universality of the aforementioned techniques, we also explore the application in improving self-supervised learning when pre-trained on imbalanced data. As discussed in [3; 61], DINO's performance is sensitive to the imbalance in web-crawled pre-training data, and thus data deduplication is a crucial process in DINOv2 . As discussed by a recent study , the learnable prototypes of DINO (akin to the classifier of SL) may be biased to imbalanced data and many collapses (like Fig. 8(a)). We hypothesize that applying subsampling to the prototypes may alleviate this phenomenon. Our intuition is that the operation resembles dropout and could encourage better utilization of the online-learned prototypes of DINO, thus improving representations learned from uncurated web data. Based on vanilla DINO , we randomly subsample prototypes (instead of using them all) during the calculation of the self-distillation loss (see details in Appx. D). All models are pre-trained for 100 epochs on LAIONet, and evaluated on the transfer learning benchmark of .

Figure 10: Transfer learning results of DINO variants pre-trained on LAIONet _vs_. vanilla DINO trained on ImageNet. Extreme data imbalance makes LAIONet much harder for DINO to learn transferrable representations. The \(\) vocabulary subsampling strategy effectively helps \(\) DINO alleviate such defects and generally match ImageNet-pretrained performance.

Figure 9: A case study of SL under the zero-shot tail setting. (a) SL models seek maximal margins between classifiers, and tail prototypes collapse together. Instead, CLIP has a healthier structure. (b) Using CLIP head solely is less effective, and voc. subsampling is needed for CLIP-like generalization.

Results in Fig. 10 and Tab. 2 show that compared to pre-training on ImageNet, \(\) vanilla DINO's performance drops notably among 11 datasets out of 12. Instead, \(\) vocabulary-subsampling narrows the gap by a large margin, highlighting this technique's effectiveness on large-scale data in the wild. To rule out the influence of total vocabulary size (number of prototypes), we also train \(\) vanilla DINO with reduced vocabulary (16384). This model is notably weaker than that trained with \(\) subsampling (16384 for each training iter, 65536 in total), and supports the improvement's effectiveness.

## 5 Limitations, future work, and broader impacts

**Limitations.** Our study has focused on the robustness of CLIP-type models in relation to the data imbalance naturally raised from web data sources. We have demonstrated that our findings are transferable to the supervised and self-supervised learning setting for classification tasks. However, we acknowledge that our estimation of image-text datasets' concept frequency is based on a simple rule-based pipeline, which could be prone to caption noise, multi-label, and ambiguity. Besides, CLIP models are not only employed for classification tasks, the study of leveraging CLIP for open-world detection or segmentation is the area our study does not cover. Additionally, given the nature of the web-based data sources used in our study, we acknowledge that the data may contain implicit bias or harmful information. We provide more discussions in Appx. A.

**Future work.** Our findings cover insights in language supervision, pretext task, data scaling, and concept scaling, but only a small portion are validated in application. One direction for future work is to explore the use of language supervision and open-world data in recognition models. Besides, a recent work  finds Adam optimizer to outperform (stochastic) gradient descent on heavy-tailed data, which can be another factor in CLIP's robustness and is worth further exploration. On the other hand, we are interested in extending our discovery to the open-world detection and segmentation tasks to see if our findings still hold under these more challenging scenarios.

Furthermore, as we have analyzed in our study, language supervision plays an important role in achieving such robustness to the data imbalance, thus we are also interested in studying whether or not similar traces of generalization exist in (multi-modal) large language models (_e.g._, Llama , BLIP-2 , LLaVA , _etc._). However, despite being trained on large-scale data with language supervision, recent works show that LLM/MLLMs still suffer from long-tailed training data [37; 46], and their performance is highly correlated with the frequency that corresponding knowledge appeared in training [1; 95]. This indicates that generative models might be intrinsically more prone to long-tailed data than contrastive models like CLIP, and injecting rebalancing mechanisms into the generative process could be valuable for future explorations.

**Broader impacts.** We provide an in-depth analysis of CLIP's robustness to data imbalance, which helps understand the effectiveness of CLIP. The techniques here are also shown to be effective for other domains (supervised learning and self-supervised learning) to overcome biases in tail under-represented classes. Thus, we expect our work not to pose potential negative societal consequences but rather to improve society's overall equality and inclusiveness.

## 6 Concluding remarks

Our work starts with the observation that although web-crawled datasets share an extremely imbalanced data distribution, CLIP is relatively more robust to it. Extensive studies on 1) language supervision, 2) pretext task, 3) web data distribution, 4) data scaling, and 5) open-world concepts reveal significant findings about the underlying mechanisms of this robustness. We have also demonstrated that these findings can be transferred to classification and self-supervised learning methods, yielding improved generalization under pre-training data imbalance. Our study uncovers key factors of CLIP's robustness to pre-training data imbalance, and provides new perspectives to understand its generalizability. The insights learned are validated on tasks from extremely long-tailed supervised learning to self-supervised learning on web-crawled data. While CLIP has been a game changer in these research fields, it has long been utilized as is. Our study, instead, delved into the mechanisms behind CLIP, providing an opportunity to improve downstream tasks by leveraging the underlying mechanisms rather than relying solely on the model itself, with greater flexibility and adaptability.