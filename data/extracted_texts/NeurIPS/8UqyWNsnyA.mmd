# An Autoencoder-Like Nonnegative Matrix

Co-Factorization for Improved Student Cognitive Modeling

 Shenbao Yu \({}^{1}\)  Yinghui Pan \({}^{2}\)1  Yifeng Zeng \({}^{3}\)  Prashant Doshi \({}^{4}\)

**Guoquan Liu \({}^{5}\)  Kim-Leng Poh \({}^{6}\)  Mingwei Lin \({}^{1}\)**

\({}^{1}\) College of Computer and Cyber Security, Fujian Normal University, China

\({}^{2}\) National Engineering Laboratory for Big Data System Computing Technology,

Shenzhen University, China

\({}^{3}\) Department of Computer and Information Sciences, Northumbria University, UK

\({}^{4}\) Intelligent Thought and Action Lab, School of Computing, University of Georgia, USA

\({}^{5}\) Financial Technology Research Institute, Fudan University, China

\({}^{6}\) College of Design and Engineering, National University of Singapore, Singapore

{yushenbao,lmwfjnu}@fjnu.edu.cn, panyinghui@szu.edu.cn

yifeng.zeng@northumbria.ac.uk, pdoshi@uga.edu

liugq@fudan.edu.cn, pohkimleng@nus.edu.sg

###### Abstract

Student cognitive modeling (SCM) is a fundamental task in intelligent education, with applications ranging from personalized learning to educational resource allocation. By exploiting students' response logs, SCM aims to predict their exercise performance as well as estimate knowledge proficiency in a subject. Data mining approaches such as matrix factorization can obtain high accuracy in predicting student performance on exercises, but the knowledge proficiency is unknown or poorly estimated. The situation is further exacerbated if only sparse interactions exist between exercises and students (or knowledge concepts). To solve this dilemma, we root monotonicity (a fundamental psychometric theory on educational assessments) in a co-factorization framework and present an autoencoder-like nonnegative matrix co-factorization (AE-NMCF), which improves the accuracy of estimating the student's knowledge proficiency via an encoder-decoder learning pipeline. The resulting estimation problem is nonconvex with nonnegative constraints. We introduce a projected gradient method based on block coordinate descent with _Lipschitz_ constants and guarantee the method's theoretical convergence. Experiments on several real-world data sets demonstrate the efficacy of our approach in terms of both performance prediction accuracy and knowledge estimation ability, when compared with existing student cognitive models.

## 1 Introduction

With the explosion of open educational resources, student cognitive modeling is receiving growing attention. As illustrated in Figure 1, given a set of exercises (could be recommended by a learning platform) with the expert-annotated knowledge concepts in a subject domain, a student is required to finish the exercises and leaves the responses. Based on the response log, cognitive modeling aims to(\(a\)) estimate the student's cognitive levels on the knowledge concepts (i.e., cognitive diagnosis) and (\(b\)) predict some exercise performance. With a comprehensive understanding of students, cognitive modeling is fruitful in applications such as computerized adaptive testing  and exercise recommendations . To profile students' cognitive status, much progress has been made in the educational psychology area, where one popular avenue is to use cognitive diagnosis models (CDMs) . While most CDMs provide detailed insights into students' cognitive states, the subjective handcraft features (e.g., the slip and guess of an exercise) may only partially capture the nuances of actual cognitive functioning, triggering cascading errors in predicting student performance .

In a fresh direction, several studies focus on applying data mining techniques to model students' learning status, of which the cornerstone is matrix factorization (MF) . By transforming students' response logs into a scoring matrix, MF-based models directly predict the missing response values via latent factors, thereby reducing cascading errors. In contrast to CDMs, MF-based models enjoy high prediction accuracy and are inexpensive to deploy . On the other hand, the latent factors uncovered from the factorization techniques, which encode students' implicit learning ability, are unexplainable, i.e., the true knowledge components of the students remain unknown in the latent vectors. Recognizing this problem, a follow-up scalable nonnegative matrix co-factorization (SNMCF) model  utilizes a point coverage function to learn students' proficiency levels via pre-trained latent factors. However, SNMCF solves the two learning tasks separately, i.e., the generation of latent features is aimed at performance prediction without considering the target of improving cognitive diagnosis, thereby compromising the identification of student cognitive levels. As such, the fundamental issue of identifying students' knowledge proficiency remains an open problem.

In this paper, we envision a reliable and interpretable data mining-based cognitive model with interlocking learning components. Learning latent factors that help pinpoint students' responses to exercises can guide the assessment of their knowledge proficiency, and the corresponding latent knowledge features, in turn, enable their success or failure on the exercises. To this end, several challenges exist: How can we specify and assess the students' knowledge proficiency since the ground truth of the cognitive levels is unknown ? How can we frame the two learning tasks as the building blocks of an optimization framework while reducing cascading errors?

To mitigate these challenges, we leverage the known monotonicity  to sidestep the issue of unknown knowledge proficiency. The monotonicity states that a student's knowledge proficiency has a monotonic relationship with the probability of the right responses to related exercises. Furthermore, by investigating the form of an autoencoder, our key observation reveals that its self-reconstruction principle, which aims to reconstruct input data from the learned low-dimensional representations, is amenable to the requirement of the monotonic constraint. Leveraging this observation, we root the monotonicity in a co-factorization framework via the autoencoder mechanism. Consequently, an autoencoder-like nonnegative matrix co-factorization (AE-NMCF) is presented, which enables an iterative link between students' knowledge proficiency and exercise performance, thereby enhancing prediction accuracy and diagnostic ability. As the resulting optimization problem is not convex and

Figure 1: A schematic illustration of the student cognitive modeling problem. On the **left** is a set of exercises with the expert-labeled knowledge concepts. The **middle** is a student’s binary-value response log with missing values (e.g., Ex\({}_{2}\) is missing) that is input to the modeling, and the **top right** illustrates the two cognitive tasks, which are the output of the modeling.

has nonnegative constraints - which makes the complexity acute by an inverse link function (often called response functions in the case of general linear models ) - we develop a projected gradient method based on block coordinate descent with _Lipschitz_ constants and guarantee its theoretical convergence. The main contributions are:

* We introduce a data mining-based model (AE-NMCF) for improved student cognitive modeling, which provides an end-to-end and data-driven way of specifying and assessing students' understanding of a set of given knowledge concepts. This new model exploits the monotonicity in educational MF-based approaches for the first time.
* To learn the model, we present a novel projected gradient method based on block coordinate descent with _Lipschitz_ constants, for which theoretical convergence is guaranteed. This method accounts for the non-convexity of the optimization function with nonnegative constraints and the complexity of the inverse link function.
* AE-NMCF provides a good fit to the students' knowledge proficiency while maintaining student performance prediction that is comparable to other student cognitive models.

These contributions will potentially improve the automated comprehensive understanding of students' knowledge learning and benefit numerous intelligent educational tools.

## 2 Related Works

Student cognitive modeling has generally proceeded along two tracks: cognitive diagnostic models (CDMs) and data mining approaches.CDMs are of two types: continuous CDMs, an example of which is item response theory (IRT) ), and discrete CDMs such as the deterministic inputs, noisy "and" gate (DINA) ). IRT predicts a student's exercise performance based on a single learning trait and exercise difficulty levels. In contrast, DINA probes a student's binary cognitive status in different knowledge concepts, which assumes that a student can answer correctly if she has mastered all the required knowledge concepts. Traditional CDMs engender a plethora of advanced models. For example, Cheng _et al_.  extend IRT using deep learning to enhance the diagnostic process. Noting the importance of the relation among knowledge concepts, Gao _et al_.  proposed a deep diagnosis framework that considers both the importance of and the interactions between knowledge concepts. Furthermore, Yang _et al_.  recently presented a relationship-based CDM to explore implicit knowledge-exercise relations that educators ignore.

Along the data mining approach, MF has proven to be effective in understanding students' response processes , especially toward student performance prediction. In this study, classic models (e.g., nonnegative MF (NMF) ) and their variants such as the regularized NMF  were successfully applied. Because the latent trait of MF is not interpretable for knowledge estimation, Yu _et al_.  proposed SNMCF that utilizes a coverage function to model students' knowledge states, thereby taking an important stride in data mining-based student cognitive modeling. But, the coverage function often gives binary cognitive levels, failing to discern the nuance between knowledge proficiencies.

Matrix co-factorization (MCF)  benefits from jointly exploiting multiple data sources.It is well established for many applications such as convolutive source separation , data sparsity [18; 19], and decision support systems . Given a domain task, MCF improves performance by incorporating knowledge in additional matrices (e.g., trust relationship for social recommendation ), which share latent factors with the original one. This sharing mechanism facilitates entity-relation learning . It motivates us to develop an in-depth understanding of students, exercises, and knowledge concepts, and facilitates an effective solution for the two learning tasks, which differs from existing MCF-based approaches that aim at performance boost only.

Recently, the autoencoder architecture is being explored in dimensionality reduction , classification , and anomaly detection [25; 26].For example, Wang _et al_.  proposed a deep version of autoencoder to explore manifold data structures. Gong _et al_.  augmented the autoencoder with a memory module to mitigate anomaly reconstruction problems. For student cognitive modeling, given the reconstruction ability of autoencoder, our work is the first attempt to exploit this mechanism in MF-based approaches to estimate student knowledge proficiency.

AE-NMCF Model and Method

Given \(\) students and \(\) exercises, all students' responses to the exercises are recorded in a binary scoring matrix \([0|1]^{}\), where \(_{nm}\) denotes student \(_{m}\)'s answer on exercise \(_{n}\). In addition, given \(\) knowledge concepts, we have an expert-labeled Q-matrix \([0|1]^{}\), where \(_{nk}=1\) if \(_{n}\) relates to knowledge concept \(_{k}\), otherwise \(_{nk}=0\). With \(\) and \(\) in hand, we aim to \((a)\) learn students' proficiency in knowledge concepts from the responses, and \((b)\) predict students' performance on exercises that they have never done.

### Model Formulation

Figure 2 (from left to right) offers an overview of the approach, which includes an encoder and a decoder. The encoder and decoder specify and diagnose students' cognitive levels, thereby enabling monotonicity. Specifically, the new framework receives the student scoring matrix (\(\)) and the Q-matrix (\(\)). In the encoder process, we introduce the exercise-knowledge association matrix (\(\)) and then jointly decompose \(\) and \(\) to obtain three low-dimensional nonnegative matrices: the student proficiency matrix (\(\)), the exercise characteristic matrix (\(\)), and the knowledge requirement matrix (\(\)). Note that the shared matrix \(\) places \(\) on the same scale as \(\), which shapes a pathway to specify the students' knowledge proficiency (\(\)). In the decoder process, we introduce the exercise difficulty vector (\(\)), which is combined with \(\) and \(\) to form cognitive factors. By re-fitting \(\), the decoder process ensures that students' knowledge proficiency is monotonic with the probability of the correct exercise responses, which embodies our desire to maintain the monotonicity.

**The encoder process.** Given \([0|1]^{}\) and \([0|1]^{}\), we start the encoder process with optimization problem (1), where we have three low-dimensional nonnegative matrices: \(^{}\), \(^{}\), and \(^{}\), each of which consists of \(\) latent factors. The latent factors can be loosely viewed as a series of topic skills denoting high-level knowledge in a subject area, such as "spatial imagination" and "abstract summarization" in mathematics.

\[_{,,,} \|(-)\|_{}^{2}+\| (-)\|_{}^{2}\\ ,,,,\] (1)

Figure 2: The end-to-end pipeline of AE-NMCF. We start from the scoring matrix (\(\)), which is also the ending module. The question marks (’?’) in \(\) denote the absent responses that the students have never visited the exercises before. Here, we use the cell shadings to highlight the nonnegative constraints on the matrix blocks, wherein the dotted lines impose the sparse constraints. In addition, the solid and chain-dotted lines denote the decomposing and composing processes, respectively.

For the first term in problem (1), we use \(\) and \(\) to approximate \(\) through the _Frobenius_ norm and introduce a weighted matrix \([0|1]^{}\) to focus on the observed entries in \(\) via the Hadama product (\(\)). In the second term, considering that \(\) only stores the linkage between exercises and knowledge concepts with either true or false relations (failing to uncover their strength), we introduce the nonnegative matrix \(^{}\), where \(_{nk}\) is the degree to which exercise \(_{n}\) involves knowledge concept \(_{k}\), with larger values denoting stronger involvement of the knowledge concept. Similarly, we use \(\) and \(\) to approximate \(\), where the sparsity is imposed by \(\).

In problem (1), \(\) and \(\) share the matrix \(\), which bridges the gap between students and knowledge concepts. In reality, given \(\), the approximation for \(_{:m}\) is a linear accumulation of the columns of \(\), weighted by the components of \(_{:m}\), and so does \(_{:k}\): we project the two nonnegative vectors \(_{:m}\) and \(_{:k}\) into the new basis \(\). Since the latent factors are considered topic skills , we define \(_{tm}\) as the _topic knowledge_ of student \(_{m}\) on \(t\)-th topic skill, as well as \(_{tk}\) as the _topic requirement_ of \(_{k}\) accordingly. Based on \(_{tm}\) and \(_{tk}\), we specify students' knowledge proficiency via the matrix \(=^{}^{}\), where \(_{km}\) is the cognitive level of \(_{m}\) on \(_{k}\).

**The decoder process.** Recall that the matrix \(\) specified by problem (1) does not give an off-the-shelf diagnostic solution due to the ignorance of monotonic constraints. We remedy this void by reconstructing the scoring matrix \(\). Specifically, we first assume that exercise \(_{n}\) has an intrinsic difficulty level \(_{n}\), which are stacked into a column vector \(=[_{1},_{2},,_{}]^{}\). Armed with \(\), \(\), and \(\), the probability that \(_{m}\) answers \(_{n}\) correctly is

\[(_{nm})=_{-}^{_{nm}}(t)t=}_{-}^{_{nm}}^{-t^{2}/2}t,\] (2)

where \(_{nm}=_{n:}_{:m}+_{n}\) indicates that \(_{m}\)'s response to \(_{n}\) is generated by a linear accumulation of required knowledge concepts. In addition, we use an _inverse link function_\((x)\), which is often a response function in generalized linear models, to map \(_{nm}\) to the success probability of the binary response \(_{nm}\). \((x)\) can be any monotonic differentiable function. Here, we focus on the commonly used _probit link function_ with the probability density of the standard Gaussian distribution.

Given Eq. (2), we can maximize the likelihood of the observed data \(_{nm}\) as

\[(_{nm})=(_{nm})^{_{nm}}[1-(_{ nm})]^{(1-_{nm})},\] (3)

and the likelihood finally yields the following optimization problem

\[_{_{n:}_{m},_{n}: n,m}-+_{n=1}^{}\|_{n:}\|_{2}^{2},\] (4)

where \(=_{(n,m)_{}}(_{nm})\) is the log-likelihood term, and \(_{}\{1,,\}\{1,,\}\) contains indices of the observed responses in \(\). In addition, since one can arbitrarily increase the scale of the vector \(_{n:}\) while decreasing the scale of the vector \(_{:m}\) (or \(^{}_{:m}\)) accordingly (and vice versa) without changing the likelihood, we gauge the vector \(_{n:}\) using the regularization term \(_{n=1}^{}\|_{n:}\|_{2}^{2}\) with the regularization parameter \(>0\). To illustrate the encoder-decoder process further, we provide an example in Appendix A.

**Objective function.** By combining the encoder and decoder, the objective function (\(_{}\)) is

\[_{,,,, }&_{}=-+\|( -)\|_{}^{2}+\|( -)\|_{}^{2}+_{n=1}^{} \|_{n:}\|_{2}^{2},\\ &, ,,.\] (5)

It is worth taking a few moments to study the form of problem (5) as it enables the monotonicity from two viewpoints. First, the monotonicity is achieved by the monotonic formulation in Eq. (2); Second, the monotonicity is optimized by problem (4). They jointly guarantee that a large value of knowledge proficiency corresponds to a better chance of success on related exercises.

### Model Solution

In problem (5), the first term \(-\) is convex for the probit link function . The second and third terms are convex in either \(\), \(\), \(\), or \(\) only, but they are not convex in all the variables together.

Given the nonnegative constraints, we employ the projected gradient (PG) method . Furthermore, concerning blocks of \(_{n:}\) and \(_{:m}\), we apply the PG via a block coordinate descent (PG-BCD) approach. Hence, problem (5) can be expressed in a block fashion as

\[_{,,,,} _{}=-+_{m=1}^{}\|_{ :m}(_{:m}-_{:m})\|_{2}^{2}+_{n=1}^{ }\|_{n:}(_{n:}-_{n:})\|_{ 2}^{2}\] \[+_{n=1}^{}\|_{n:}\|_{2}^{2},\ \ ,,, .\]

Accordingly, the subproblems of \(_{n:}\), \(_{:m}\), \(\), \(\), and \(_{n}\) constitute the iterations of PG-BCD for AE-NMCF. Next, we show the parameter solution for \(_{n:}\) in problem (6) below. For further details on the remaining parameters, refer to Appendix B.

\[_{_{n:}}\ _{}(_{n:})= _{m}-(_{nm})+\|_{n:}\|_{2}^{2}+ \|_{n:}(_{n:}-_{n:})\|_{2}^{2}.\] (6)

To solve problem (6), we note that second-order methods do not scale well to high-dimensional problems due to the necessary computation of the Hessian, making explicit calculation difficult for the probit link function. Thus, we build our learning algorithm on first-order methods. To do so, we first derive the gradients of \(_{}(_{n:})\) as

\[_{}(_{n:})=-_{m}_{nm}[_{ nm}-(_{nm})]_{:m}^{}+2[_{n:} _{n:}-_{n:}(_{n:})]+_{n:},\]

where \(_{nm}=(_{nm})}{(_{nm})[1-(_{nm})]}\), and we can employ the gradients above to search for the optimum point. In each iteration \(l=1,2,\), the gradient step is

\[_{n:}^{(l+1)}[_{n:}^{(l)}-_{ _{n:}}^{(l)}_{}^{(l)}(_{n:})]_{+},\] (7)

where the half-wave rectifier \([x]_{+}=max(,x),\ =10^{-15}\) ensures the nonnegativity , and \(_{_{n:}}^{(l)}\) is a suitable step size. For Eq. (7), a key issue is to choose the appropriate step size \(_{_{n:}}^{(l)}\), and a simple strategy is "_Armijo_ rule along the projection arc" . Although the convergence is guaranteed, it is time-consuming to search for feasible values. Motivated by Lan _et al._, we determine the appropriate step sizes by _Lipschitz_ constants . A common approach that guarantees convergence of a function \(f\) is to set \(^{(l)}=1/L\), where \(L\) is the _Lipschitz_ constant of \( f\).

### Algorithm and Theoretical Analysis

We start with Lemma 1 to analyze the _Lipschitz_ constant for problem (6). After that, we present the parameter learning algorithm for problem (5) and conclude with its theoretical analysis.

**Lemma 1**.: _Let \(g(x)=(x)}{(x)},\ x\), where \((x)\) is the probit link function. Then, for \(y,z\), we have \(|g(y)-g(x)| L_{p}|y-z|\). Here, \(L_{p}=1\) is the scalar Lipschitz constant of \(g(x)\)._

Since Eq. (3) can be rewritten as \((_{nm})=((2_{nm}-1)(_{n:}^ {}_{:m}+_{n}))\) for \(()\), we derive the following theorem which serves as a bound on the (vector) _Lipschitz_ constant for problem (6), using the result in Lemma 1.

**Theorem 1**.: _For a given \(n\), substituting \((_{nm})\) in problem (6) with the right hand side expression above yields the following_

\[_{}(_{n:})= _{m}-(_{nm})+\|_{n:} \|_{2}^{2}+\|_{n:}(_{n:}-_{n:})\| _{2}^{2},\]

_where \(_{nm}=(2_{nm}-1)(_{n:}^{}_{ :m}+_{n})\). For any vectors \(,\), we have_

\[\|_{}()-_{}( )\|_{2}[L_{p}_{1}^{2}(^{})+2 (_{k=1}^{}_{nk}^{2})^{}+ ]\|-\|_{2}.\]To prove Theorem 1, we first derive the gradient of \(_{}(_{n:})\) based on the element-wise operation of \(()\) and \(()\). After that, we establish the upper bound of the \(_{2}\)-norm of the gradient difference given two arbitrary points \(\) and \(\). The derivation details refer to Appendix D. By comparing with Theorem 1, we obtain the _Lipschitz_ constant as \(L_{p}_{1}^{2}(^{})+2(_{k=1}^{} _{nk}^{2})^{}+\), where \(_{1}()\) denotes the corresponding maximum singular value.

Armed with Eq. (7) with the step sizes determined by Theorem 1, Algorithm 1 outlines the optimization process for AE-NMCF, named PG-BCD+_Lipschitz_. In Algorithm 1, we first initialize all parameters with random entries (line 1) and then optimize \(_{}\) in an alternating fashion. Each outer iteration solves the inner subproblems (lines 3 - 9). For each subproblem, we optimize the target parameter and hold others constant. For example, we hold \(,,\), and M constant and separately optimize each block of variables in \(\). The update order in the block case is \(_{1:}_{2:} _{:}\). The outer loop is terminated if the decrease in \(_{}\) is smaller than a threshold \(\) (lines 6 - 8).

```
0:\(\), \(\), and \(\).
0:\(_{n:}\), \(_{:m}\), \(\), \(\), and \(_{n}\) (1\(\)\(n\)\(\)\(,1\)\(\)\(m\)\(\)M).
1: Initialize \(_{n:}^{(0)}\), \(_{:m}^{(0)}\), \(^{(0)}\), \(^{(0)}\), and \(_{n}^{(0)}\) (1\(\)\(n\)\(\)\(,1\)\(\)\(m\)\(\)M).
2: Calculate \(_{}^{(0)}\).
3:for\(l=0,1,\)do
4: Update: \(_{n:}^{(l+1)}\) (1\(\)\(n\)\(\)\(\)); \(_{:m}^{(l+1)}\) (1\(\)\(m\)\(\)\(\)); \(^{(l+1)}\); \(^{(l+1)}\); \(_{n}^{(l+1)}\) (1\(\)\(n\)\(\)\(\)).
5: Calculate \(_{}^{(l+1)}\).
6:if\(|_{}^{(l+1)}-_{}^{(l)}|\)then
7: Return \(_{n:}^{(l+1)}\), \(_{:m}^{(l+1)}\), \(^{(l+1)}\), \(^{(l+1)}\), and \(_{n}^{(l+1)}\).
8:endif
9:endfor ```

**Algorithm 1** PG-BCD+_Lipschitz_

We now establish the convergence guarantees of PG-BCD+_Lipschitz_. In fact, the development of rigorous statements for the convergence of \(\), \(\), \(\), \(\), and \(\) to an optimum is not trivial, due to the block multi-convex nature. Nevertheless, we can establish the convergence of PG-BCD+_Lipschitz_ based on a prior analysis of BCD for multiconvex optimization . To achieve this, for the sake of convenience, let \(=(,,,,)\), then we have the following theorem.

**Theorem 2**.: _Given any start point \(^{(0)}\), let \(\{^{(l)}\}\) be the sequence of the factors from PG-BCD+Lipschitz, where \(l=1,2,\) are the outer iteration numbers, then the sequence \(\{^{(l)}\}\) converges to the finite the critical point of problem (5). In particular, if \(^{(0)}\) is close to the global point of problem (5), PG-BCD+_Lipschitz_ converges to the global optimum._

Since minimizing AE-NMCF follows multi-block coordinate descent solutions, which correspond to BCDs with the update (1.3a) in , we can use the results laid by Xu and Yin [33, Lemma 2.6, Corollary 2.7, and Theorem 2.8] to prove Theorem 2, and the proof details refer to Appendix E. Note that we can not guarantee the global optimum convergence of PG-BCD+_Lipschitz_ from an arbitrary point due to the multi-convex, but the use of multiple randomized initialization attempts can increase the change to reach the global optimal solution.

## 4 Experiments

**Data set description.** We use real-world students' response data with different sparsities and knowledge-exercise relations, which are from diversified academic subjects, including \((a)\)_Math_ (FrCsub, Junyi-s, and Quanlang-s), \((b)\)_Biology_ (SLP-Bio-s), \((c)\)_History_ (SLP-His-s), and \((d)\)_English_ (SLP-Eng). FrCsub comprises of the fraction subtraction problem scores of 536 middle school students . Junyi-s includes problem logs from an e-learning website based on the open-source code released by Khan Academy . The private Quanlang-s data set is collected from mathematical exams given to junior schools supplied by QUANLANG education company. 2 Others include SLPBio-s, -His-s, and -Eng, which provide unit test results of K-12 learners compiled by an online learning platform (smart learning partner, SLP) . Statistics of the data set are summarized in Table III of Appendix F.

**Baseline approaches.** The baselines include data mining approaches and cognitive diagnosis models. The former uses the well-known **NMF**, **MCF**, **GNMF**, **NMMF**, and the advanced **SNMCF**. For the latter, we consider the following competing models: \((i)\)**DINA**, a classic CDM that models students' knowledge levels by a binary attribute vector with the slip and guess factors of exercises; \((ii)\)**DIRT**, an extended IRT model incorporating the deep learning technique to enhance the diagnostic process; \((iii)\)**DeepCDF**, a deep learning-based CDM that considers the importance and relationships of knowledge concepts; and \((iv)\)**QRCDM**, which integrates the implicit knowledge-exercise relations into CDMs. DIRT and DeepCDF are modified by excluding text information. Our code is available at https://github.com/ShenbaYu/AE-NMCF.

### Results

In this section, we evaluate the effectiveness of AE-NMCF in the two learning tasks. Additional experiments, such as cognitive case studies, can be found in Appendix F.

We first compare student performance prediction.The evaluation metrics are the commonly used ACC and RMSE , which are calculated based on the ground truth of students' responses to exercises and corresponding predicted ones. Table 1 shows the prediction results with the best performances highlighted in boldface, the top 2 results are shaded, and we use '\(\)' to denote the standard deviations. The last column lists the average ranks of all models from the Friedman test (a rank-based method to validate the performance of multiple models on multiple datasets) . In Table 1, we observe that the data mining methods (especially SNMCF and AE-NMCF) perform well, and AE-NMCF lies in the top-2 performers on all data sets except for FrCub and SLP-Eng in terms of ACC and RMSE, respectively. Its rank of 1.33 on ACC and 1.67 on RMSE also confirm the competitiveness of AE-NMCF. The results indicate that our model can not only handle the students' response data that yields varied degrees of sparsity but also do so for diversified subject domains.

We proceed to discuss AE-NMCF's ability to estimate students' knowledge proficiency, which is our major concern.Since the ground truth of students' cognitive levels is unknown, we take cues from the _area under curve_ and use a ranking-based metric (_knowledge-response consistency coefficient_, KRC) as an alternative way to evaluate the diagnostic results. Specifically, for knowledge concept \(_{k}\), we first extract the pair set \(=\{(_{n},_{m}),n[0,],m[0,]\}\) from the testing set \(\)

    &  &  &  \\    & & FrCub & Janyi-s & & & & & \\   & NMF & 0.7564\(\)0.0093 & 0.6186\(\)0.0223 & 0.6312\(\)0.0075 & 0.6752\(\)0.0103 & 0.7169\(\)0.0094 & 0.7222\(\)0.0114 & 5.83 \\  & MCF-Gral & 0.5727\(\)0.0126 & 0.5046\(\)0.0219 & 0.5679\(\)0.0249 & 0.5515\(\)0.0199 & 0.5828\(\)0.0090 & 0.5640\(\)0.0101 & 10.17 \\  & MCF-New2 & 0.7066\(\)0.0076 & 0.5327\(\)0.0174 & 0.5743\(\)0.0093 & 0.5543\(\)0.0096 & 0.5824\(\)0.0043 & 0.5696\(\)0.0083 & 9.17 \\  & GNMF & 0.7516\(\)0.0112 & 0.6429\(\)0.0305 & 0.5894\(\)0.0109 & 0.6090\(\)0.0143 & 0.6209\(\)0.0036 & 0.6034\(\)0.0077 & 7.00 \\  & NMMMF & 0.7750\(\)0.0055 & 0.6729\(\)0.0264 & 0.6477\(\)0.0194 & 0.6780\(\)0.0084 & 0.7020\(\)0.0041 & 0.7007\(\)0.0176 & 4.83 \\  & SNMCF & **0.8584\(\)0.0043** & 0.6878\(\)0.0092 & 0.7417\(\)0.0087 & 0.7351\(\)0.0018 & 0.8051\(\)0.0032 & 0.7485\(\)0.0091 & 1.83 \\  & DINA & 0.8156\(\)0.0037 & 0.5209\(\)0.0078 & 0.6000\(\)0.0143 & 0.4988\(\)0.0066 & 0.5814\(\)0.0051 & 0.5950\(\)0.0123 & 8.50 \\  & DIIR & 0.6154\(\)0.0076 & 0.5741\(\)0.0028 & 0.6420\(\)0.0100 & 0.5226\(\)0.0222 & 0.5929\(\)0.0150 & 0.5823\(\)0.0181 & 8.33 \\  & DeepCDF & 0.8115\(\)0.0081 & 0.4717\(\)0.0055 & 0.6956\(\)0.0189 & 0.6763\(\)0.0069 & 0.7850\(\)0.0022 & 0.6676\(\)0.0113 & 5.67 \\  & QRCDM & 0.8308\(\)0.0079 & 0.6406\(\)0.0226 & 0.6611\(\)0.0133 & 0.6996\(\)0.0091 & 0.8016\(\)0.0035 & 0.7396\(\)0.0208 & 3.33 \\  & AE-NMCF & 0.8267\(\)0.0048 & **0.7065\(\)0.0285** & **0.7531\(\)0.0064** & **0.7553\(\)0.0101** & **0.8072\(\)0.0019** & **0.7632\(\)0.0062** & 1.33 \\   & NMF & 0.4102\(\)0.0057 & 0.5192\(\)0.0105 & 0.5412\(\)0.0025 & 0.4558\(\)0.0099 & 0.4421\(\)0.0065 & 0.4696\(\)0.0093 & 5.67 \\  & MCF-Gra & 0.5670\(\)0.0067 & 0.6076\(\)0.0329 & 0.5393\(\)0.0096 & 0.5487\(\)0.0110 & 0.6621\(\)0.0263 & 0.6675\(\)0.0179 & 10.50 \\   & MC-New & 0.4738\(\)0.0128 & 0.5906\(\)0.0241 & 0.5602\(\)0.0128 & 0.5674\(\)0.0038 & 0.5879\(\)0.0133 & 0.6478\(\)0.0097 & 9.83 \\   & GNMF & 0.4153\(\)0.0066 & 0.4890\(\)0.0198 & 0.5012\(\)0.0056 & 0.5175\(\)0.0064 & 0.5294\(\)0.0018 & 0.5468\(\)0.0060 & 7.83 \\   & NMMF & 0.3966\(\)0.0032 & 0.4704\(\)0.0402 & 0.4749\(\)0.0076 & 0.4564\(\)0.0083 & 0.4555\(\)0.0062 & 0.4724\(\)0.0226 & 5.33 \\   & SNMCF & **0.3343\(\)0.0029** & 0.4537\(\)0.0159 & 0.4216\(\)0.0076 & 0.4260\(\)0.0082 & 0.3741\(\)0.0095 & 0.5845\(\)0.0867 & 3.17 \\   & DINA & 0.3927\(\)0.0035 & 0.6179\(\)0.0055 & 0.5756\(\)0.0119 & 0.5312\(\)0.0040 & 0.5281\(\)0.0029 & 0.5876\(\)0.0105 & 8.67 \\   & DIIRT & 0.4811\(\)0.00For each pair \((_{n},_{m})\), we record student \(_{m}\)'s proficiency level of \(_{k}\) and the true response score on exercise \(_{n}\). Then, the KRC result on \(_{k}\) is \((_{k})=(-^{+}(^{+}+1) }{2})/(^{+}^{-})\), where \(=_{_{n}^{*}=1}(n,m)\), and \((n,m)\) is the reordered position of the pair \((_{n},_{m})\) based on the proficiency level. \(^{+}\) (\(^{-}\)) denotes the number of records with correct (wrong) answers in \(\). Finally, we average the KRC values of all the knowledge concepts and denote the average as \(r_{c}\). Higher values of \(r_{c}\) indicate better performance.

Figure 3 illustrates the results of estimating students' knowledge proficiency. We exclude NMF, MCF, GNMF, and NMMF due to their known poor capability of cognitive diagnosis. From the results, we have: \((a)\) the \(r_{c}\) values in \(\) surpass the other data sets as expected, which is mainly due to the strong and consistent connection between exercises and knowledge concepts. \((b)\) Regardless of the relationship types, AE-NMCF delivers comparable or slightly improved performance w.r.t. CDM-based approaches and rises well above SNMCF on all data sets. The resulting \(p\)-value given by a Wilcoxon-signed rank test between AE-NMCF and SNMCF is 0.031, which also confirms the improvement. \((c)\) While QRCDM shows good diagnostic results, its predicting performance suffers for multiple data sets (see Table 1). This is mainly due to the knowledge-exercise relationship being one-to-one (or one-to-many), which may impede the discovery of implicit correlations.

Furthermore, Figure 4 compares the model performance in balancing the two learning tasks, where we use a bubble's horizontal (vertical) position to note the ACC (\(r_{c}\)) value for a model. In spired by \(F_{1}\) score, we further visualize the bubble size based on the harmonic mean of ACC and \(r_{c}\). Hence, the closer to the upper right corner with a larger bubble size, the better the balance achieved. As shown in Figure 4, SNMCF excels at student performance prediction but is inadequate in knowledge cognitive estimation. In addition, the comparatively low prediction performance of QRCDM compromises

Figure 4: Model comparison in balancing the two learning tasks via bubble visualizations. The x(y)-axis denotes the prediction (estimation) performance in terms of ACC (\(r_{c}\)), and the bubble size measures the harmonic mean of ACC and \(r_{c}\). The dash lines locate the models’ average performance.

Figure 3: Students’ knowledge proficiency estimations.

its balance ability, especially on Junyi-s and Quanlang-s. AE-NMCF, in contrast, is well above the model average (indicated by dash lines) on all data sets, which achieves the best balance between prediction accuracy and diagnostic ability and works with multiple relation cases.

**We close with the demonstration of the effectiveness of our encoder-decoder learning pipeline.** As shown in Table 2, we conduct the ablation study by the use of two variants of AE-NMCF, i.e., AE-NMCF _w/o_ Decoder (Encoder) that removes the decoder (encoder) module. The optimization approach is also PG-BCD with appropriate _Lipschitz_ constants. According to Table 2, the ignorance of the encoder (or decoder) process leads to a degradation in predicting and estimating performances, and the performance losses of AE-NMCF _w/o_ Encoder are lower than those of the variant that removes the decoder. The positive results not only suggest the performance boost of the decoder module but also prove the efficacy of the proposed encoder-decoder architecture, which aligns with our expectations to achieve the monotonicity.

### Discussion on the Results

We summarize the key findings. First, AE-NMCF improves on competing approaches on two learning tasks across subject domains, data sparsities, and knowledge-exercise relationships. Notably, the better estimation accuracy of knowledge proficiency benefits from the explicit encoding of the knowledge level for each student, which is then iteratively improved by the novel autoencoder machine that guarantees that knowledge proficiency can cumulatively cause success in exercises. Second, our purely data-driven model estimates interpretable factors to pinpoint a student's strengths and weaknesses, which is helpful for decision-making as we may tailor learning resources.

However, AE-NMCF's improved prediction and estimation accuracy over the baselines (SNMCF in particular) comes at a price of higher computational complexity (e.g., see Table VI in Appendix F). Nevertheless, AE-NMCF is well-suited to scale-based tests, which are common scenarios in the real world because students are often evaluated for a small set of knowledge concepts, and the need for confidence statistics is one of the critical factors. In addition, we observe that the diagnostic results for some knowledge concepts tend to be overoptimistic due to ignorance of the prerequisite structure (e.g., see Figure VI in Appendix F), and one part of ongoing work is exploiting the knowledge prerequisite structure for AE-NMCF to attenuate this problem.

## 5 Conclusion

This paper studies student cognitive modeling from a data mining perspective, in which students' knowledge proficiency estimation is our primary concern. To tackle this problem, we propose the AE-NMCF model. Specifically, we root monotonicity in a co-factorization via the carefully crafted encoder-decoder framework. It achieves the assessment of students' knowledge proficiency end-to-end. Considering the nonconvex nature of the objective function with nonnegative constraints, we develop a projected gradient method based on block coordinate descent with _Lipschitz_ to facilitate model learning, in which theoretical convergence is guaranteed. Experiments on real-world data sets show that AE-NMCF embraces the merit of satisfactory ability to measure students' knowledge proficiency while retaining good performance prediction accuracy. The future work is two-fold: (1) Considering the learning dependency of knowledge concepts; (2) Investigating other efficient parameter learning methods and exploring their scalability.

    &  &  \\   & & & FrCsub & Junyi-s & Quanlang-s & SLP-Bio-s & SLP-His-s & SLP-Eng \\   & AE-NMCF w/o Decoder & 0.7523\(\)0.0118 & 0.6261\(\)0.0542 & 0.6187\(\)0.0278 & 0.6582\(\)0.0099 & 0.724\(\)0.0062 & 0.5511\(\)0.0184 \\  & AE-NMCF w/o Encoder & 0.8156\(\)0.0060 & 0.6504\(\)0.0141 & 0.7269\(\)0.0066 & 0.7418\(\)0.0010 & 0.7743\(\)0.0011 & 0.743\(\)0.0126 \\  & AE-NMCF & **0.8267\(\)0.0048** & **0.7065\(\)0.0285** & **0.7531\(\)0.0064** & **0.7553\(\)0.0101** & **0.8072\(\)0.0019** & **0.7632\(\)0.0062** \\   & AE-NMCF w/o Decoder & 0.4197\(\)0.0042 & 0.4953\(\)0.0141 & 0.4782\(\)0.0081 & 0.4585\(\)0.0014 & 0.4243\(\)0.0025 & 0.5608\(\)0.0135 \\  & AE-NMCF w/o Encoder & 0.3668\(\)0.0061 & 0.5076\(\)0.0138 & 0.4262\(\)0.0070 & 0.4160\(\)0.0021 & 0.4193\(\)0.0014 & 0.4494\(\)0.0145 \\   & AE-NMCF & **0.3476\(\)0.0088** & **0.4514\(\)0.0193** & **0.4067\(\)0.0047** & **0.3996\(\)0.0060** & **0.3665\(\)0.0024** & **0.4262\(\)0.0034** \\  \)} & AE-NMCF w/o Decoder & 0.7022\(\)0.0089 & 0.6362\(\)0.0053 & 0.5594\(\)0.0153 & 0.5730\(\)0.0020 & 0.5493\(\)0.0150 & 0.5279\(\)0.0460 \\  & AE-NMCF w/o Encoder & 0.8137\(\)0.0140 & 0.6286\(\)0.0028 & 0.5665\(\)0.0163 & 0.5891\(\)0.0171 & 0.5653\(\)0.0091 & 0.4756\(\)0.0225 \\   & AE-NMCF & **0.8738\(\)0.0147** & **0.7249\(\)0.0380** & **0.6456\(\)0.0167** & **0.6875\(\)0.0109** & **0.6393\(\)0.0116** & **0.7663\(\)0.0578** \\   

Table 2: Ablation analysis of AE-NMCF in student cognitive modeling