# 2D-OOB: Attributing Data Contribution Through

Joint Valuation Framework

 Yifan Sun

University of Illinois Urbana-Champaign

yifan50@illinois.edu

&Jingyan Shen

Columbia University

js5544@columbia.edu

&Yongchan Kwon

Columbia University

yk3012@columbia.edu

Equal contribution.Corresponding author.

###### Abstract

Data valuation has emerged as a powerful framework for quantifying each datum's contribution to the training of a machine learning model. However, it is crucial to recognize that the quality of _cells_ within a single data point can vary greatly in practice. For example, even in the case of an abnormal data point, not all cells are necessarily noisy. The single scalar score assigned by existing data valuation methods blurs the distinction between noisy and clean cells of a data point, making it challenging to interpret the data values. In this paper, we propose 2D-OOB, an out-of-bag estimation framework for jointly determining helpful (or detrimental) samples as well as the particular cells that drive them. Our comprehensive experiments demonstrate that 2D-OOB achieves state-of-the-art performance across multiple use cases while being exponentially faster. Specifically, 2D-OOB shows promising results in detecting and rectifying fine-grained outliers at the cell level, and localizing backdoor triggers in data poisoning attacks.

## 1 Introduction

From customer behavior prediction and medical image analysis to autonomous driving and policy making, machine learning (ML) systems process ever increasing amounts of data. In such data-rich regimes, a fraction of the samples is often noisy, incorrect annotations are likely to occur, and uniform data quality standards become difficult to enforce. To address these challenges, data valuation emerges as a research field receiving increasing attention, focusing on properly assessing the contribution of each datum to ML training . These methods have proven useful in identifying low-quality samples that can be detrimental to model performance, as well as selecting subsets of data that are representative of enhanced model performance . Furthermore, they are widely applicable in data marketplace for fair revenue allocation and incentive design .

Nevertheless, existing data valuation methods assign a scalar score to each datum, thereby failing to account for the varied roles of individual cells. This leaves the valuation rationale unclear and can be unsatisfactory and sub-optimal in various practical scenarios. Firstly, whenever a score is assigned to a data point by a particular data valuation method, it is crucial to understand the underlying justifications to ensure transparency and reliability, especially in high-stakes decision making . Secondly, it is important to recognize the fact that even if a data point is of low quality, it is rarely the case that all the cells within this data point are noisy . The absence of detailed insightsinto how individual cells contribute to ML training inevitably leads to discarding the entire data point. This can result in substantial data waste, particularly when only a few cells are noisy and data acquisition is expensive. Finally, in data markets, different cells within a data point may originate from different data sellers [3; 11]. Consequently, a singular valuation for the entire point fails to offer equitable compensation to all contributing parties.

Our contributionsIn this paper, we propose 2D-OOB, a powerful and efficient joint valuation framework that can attribute a data point's value to its features. 2D-OOB quantifies the importance of each _cell_ in a dataset, as illustrated in Figure 1, providing interpretable insights into which cells are associated with influential data points. Our method is computationally efficient as well as theoretically supported by its connections with Data-OOB. Moreover, our extensive empirical experiments demonstrate the practical effectiveness of 2D-OOB in various use cases. 2D-OOB accurately identifies cell outliers and pinpoints which cells to fix to improve model performance. Additionally, 2D-OOB enables inspection of data poisoning attacks by precisely localizing the backdoor trigger, an artifact inserted into a training sample to induce malicious model behavior [14; 6]. On average, 2D-OOB is \(200\) times faster than state-of-the-art methods across all datasets examined.

## 2 Preliminaries

NotationsThroughout this paper, we focus on supervised learning settings. For \(d\), we denote an input space and an output space by \(^{d}\) and \(\), respectively. We denote a training dataset with \(n\) data points by \(=\{(x_{i},y_{i})\}_{i=1}^{n}\) where \((x_{i},y_{i})\) is the \(i\)-th pair of the input covariates \(x_{i}\) and its output label \(y_{i}\). For an event \(A\), an indicator function \((A)\) is \(1\) if \(A\) is true, otherwise \(0\). For \(j\), we set \([j]:=\{1,,j\}\). For a set \(S\), we denote its power set by \(2^{S}\) and its cardinality by \(|S|\).

DataShapleyThe primary goal of data valuation is to quantify the contribution of individual data points to a model's performance. Leveraging the Shapley value in cooperative game theory , DataShapley measures the average change in a utility function \(U:2^{}\) over all possible subsets of the dataset that either include or exclude the data point. For \(i[n]\), DataShapley of \(i\)-th datum is defined as follows.

\[_{i}^{}:=_{k=1}^{n}} _{S_{k}^{(i)}}[U(S\{(x_{i},y_{i})\})-U(S)],\] (1)

Figure 1: **Comparison of data valuation and joint valuation. (a) Data valuation evaluates the quality of individual data points, whereas (b) joint valuation evaluates the quality of individual cells. Both panels illustrate the same hypothetical dataset, while darker colors indicate higher quality or importance. As illustrated in panel (a), data valuation can only identify that the third and fifth data points are of low quality, but it lacks further feature-level attribution. This limitation may result in discarding the entire data point, even when only certain cells are problematic. In contrast, joint valuation provides a finer level of attribution than data valuation and aims to reveal how individual features contribute to data values. As shown in panel (b), the joint valuation framework can identify outlier cells (highlighted by blue boxes), such as \(-1\) in “Income” and \(100\) in “Education”, providing detailed interpretations of data values.**

where \(_{i}^{(i)}:=\{S|(x_{i},y_{i}) S,|S|=k-1\}\). DataShapley\(_{i}^{}\) in (1) considers every set \(S_{k}^{(i)}\) and computes the average difference in utility \(U(S\{(x_{i},y_{i})\})-U(S)\). It characterizes the impact of a data point, but its computation requires evaluating \(U\) for all possible subsets of \(\), rendering precise calculations infeasible. Many efficient computation algorithms have been proposed [16; 28; 54], and in these studies, Shapley-based methods have demonstrated better effectiveness in detecting low-quality samples than standard attribution approaches, such as leave-one-out and influence function methods [22; 10].

Data-OOBAs an alternative efficient data valuation method, Kwon and Zou  propose Data-OOB, which leverages a bagging model and measures the similarity between the nominal label and weak learners' predictions. To be more specific, consider a bagging model consisting of \(B\) weak learners, where for \(b[B]\), the \(b\)-th weak learner \(_{b}\) is given as a minimizer of the weighted empirical risk,

\[_{b}:=_{h}_{i=1}^{n}w_{bi}(y_{i},h(x_{i})),\]

where \(:\) is a loss function and \(w_{bi}\) is the number of times the \(i\)-th datum \((x_{i},y_{i})\) is selected by the \(b\)-th bootstrap dataset. Let \(_{b}\) be a weight vector \(_{b}:=(w_{b1},,w_{bn})\) for all \(b[B]\). For \(i[n]\) and \(\{(_{b},_{b})\}_{b=1}^{B}\), Data-OOB of the \(i\)-th datum is defined as follows.

\[_{i}^{}:=^{B}(w_{bi}=0)T(y_{i},_{b}(x_{i}))}{_{b=1}^{B}(w_{bi}=0)},\] (2)

where \(T(y_{i},_{b}(x_{i}))\) is a score function evaluated at \((x_{i},y_{i})\). We assume that the higher \(T\), the better the prediction. In classification settings, a common choice for \(T\) is \((y_{i}=_{b}(x_{i}))\), and in this case, Data-OOB\(_{i}^{}\) measures the average similarity between a nominal label \(y_{i}\) and weak learners' predictions \(_{b}(x_{i})\) when a datum \((x_{i},y_{i})\) is _not_ sampled in a bootstrap dataset. It intuitively captures the quality of a data point. For instance, when \((x_{i},y_{i})\) is a mislabeled sample or an outlier, the label \(y_{i}\) is likely to differ from \(_{b}(x_{i})\), resulting in \(_{i}^{}\) being close to zero.

It is noteworthy that Data-OOB in (2) can be computed by training a single bagging model, making it computationally efficient. Kwon and Zou  show that Data-OOB can easily scale to millions of data points, but for DataShapley this is often very impractical. In addition, Data-OOB is typically comparable to or even more effective than DataShapley in detecting mislabeled data points and selecting helpful data points [26; 17].

## 3 Attributing Data Contribution through Joint Valuation Framework

Data valuation quantifies the utility of data points, however, it fails to identify which features contribute to these data values and to what extent. For instance, in anomaly detection tasks, data valuation methods can be deployed to detect anomalous data points but cannot explain why they are considered abnormal, which is generally not desirable in practice. To address this challenge, we introduce a joint valuation framework that assigns _a cell score_ to each feature of a data point. Here, a cell score is designed to quantify how each feature affects the value of an individual data point, thereby attributing the data value to specific features.

To the best of the author's knowledge, Liu et al.  were the first to consider the concept of joint valuation in the literature, proposing 2D-Shapley as a means to quantitatively interpret DataShapley. To formalize this, we denote a 2D utility function by \(u:[n][d]\), which takes as input a subset of data points \(S[n]\) and a subset of features \(F[d]\), measuring the utility of a fragment of the given dataset consisting of cells \(\{(i,j)\}_{i S,j F}\), where a tuple \((i,j)\) denotes a cell at the \(i\)-th datum and the \(j\)-th column. Then, 2D-Shapley is defined as

\[_{ij}^{}:=_{k=1}^{n}_{l=1}^{d} {}_{(S,F)_{k,l}^{(i,j)}}M_{u} ^{i,j}(S,F)\] (3)

where \(_{k,l}^{(i,j)}:=\{(S,F)|S[n]\{i\},F[d] \{j\},|S|=k-1,|F|=l-1\}\) and

\[M_{u}^{i,j}(S,F)=u(S\{i\},F\{j\})+u(S,F)-u(S\{i\},F)-u(S,F\{j \}).\]The function \(M_{u}^{i,j}\) allows us to quantify how much removing a specific cell at \((i,j)\) from a given set \((S\{i\},F\{j\})\) affects the overall utility, and \(Shapley}\)\(_{ij}^{}\) evaluates the average \(M_{u}^{i,j}\) across all possible data fragments \((S,F)_{k,l}^{(i,j)}\).

Similar to DataShapley, the permutation of all rows and columns required for exact \(Shapley}\) calculations presents significant computational challenges. To address this, Liu et al.  develop \(KNN}\), which utilizes \(k\)-nearest-neighbors models as surrogates to approximate \(Shapley}\) values. However, the approximation methods can compromise the accuracy of valuations [26; 17]. Additionally, \(KNN}\) still faces challenges scaling to large-scale datasets and high-dimensional settings.

To address these limitations, we propose \(OOB}\), an _efficient_ and _model-agnostic_ joint valuation framework that leverages out-of-bag estimation to attribute data contribution. We also illustrate how \(OOB}\) is connected to Data-OOB, thereby facilitating sample-wise feature-level interpretation for data valuation, as discussed in Section 3.2.

### 2D-OOB: an efficient joint valuation framework

Our idea builds upon the subset bagging model , which is well recognized as an earlier version of Breiman's random forest model . A key distinction from a standard bagging model is that a weak learner in a subset bagging model is trained on a randomly selected subset of features. For \(b[B]\), we denote the \(b\)-th random feature subset by \(S_{b}[d]\). Then, the \(b\)-th weak learner of a subset bagging model is given as follows.

\[_{b}:=*{argmin}_{f}_{i=1}^{n}w_{bi}(y_{i},f(x_{i,S_ {b}})),\]

where \(x_{i,S_{b}}\) is a subvector of \(x_{i}\) that only takes elements in a subset \(S_{b}\). This difference enables us to assess the impact of which features are more influential: if \(S_{b}\) includes a helpful (or detrimental) feature, we can expect the out-of-bag prediction \((x_{i,S_{b}})\) to be good (or poor). We formalize this intuition and propose \(OOB}\). For \(i[n]\), \(j[d]\) and \(\{(_{b},S_{b},_{b})\}_{b=1}^{B}\), the \(OOB}\) for the \(j\)-th cell of the \(i\)-th data point is defined as follows,

\[_{ij}^{}:=^{B}(w_{bi}=0,j S_{b})T( y_{i},_{b}(x_{i,S_{b}}))}{_{b=1}^{B}(w_{bi}=0,j S_{b})},\] (4)

where \(T:\) is a utility function that scores the performance of the weak learner \(_{b}(x_{i,S_{b}})\) on the \(i\)-th datum \((x_{i},y_{i})\). Specifically, for binary or multi-class classification problems, we can adopt \(T(y_{i},_{b}(x_{i,S_{b}}))=(y_{i}=_{b}(x_{i,S_{b}}))\). In this case, \(OOB}\) measures the average accuracy score of out-of-bag predictions (specifically, when the \(i\)-th data point is out-of-bag) if the \(j\)-th feature is used in training \(_{b}\). For regression problems, we can use the negative squared error loss function, defined as \(T(y_{i},_{b}(x_{i,S_{b}}))=-(y_{i}-_{b}(x_{i,S_{b}}))^{2}\). In practice, \(\) could also be incorporated into \(T\) to suit the specific use case.

While Data-OOB in (2) aims to assess the impact of the \(i\)-th datum, \(OOB}\) in (4) further provides interpretable insights by evaluating the data point with various combinations of features, revealing which cells are influential to model performance. By leveraging the subset bagging scheme, \(OOB}\) only requires a single training of the bagging model, making it computationally efficient.

### Connection to Data-OOB

We now present interpretable expressions of how \(OOB}\) connects to Data-OOB in the following proposition. To begin with, we denote a set of subsets of \([d]\) by \(:=\{S[d]\}\). With \(\{(_{b},_{b})\}_{b=1}^{B}\), we define the \(i\)-th Data-OOB when a particular subset \(S\) is used as follows and denote it by \(_{i}^{}(S)\).

\[_{i}^{}(S):=^{B}(w_{bi}=0)T(y_{i},_{b}(x_{i,S}))}{_{b=1}^{B}(w_{bi}=0)}.\]

**Proposition 3.1**.: _For all \(i[n]\) and \(j[d]\), \(_{ij}^{}\) can be expressed as follows._

\[_{ij}^{}=_{_{S}}[_{i}^{}(S) j  S],\]_where \(_{S}\) is an empirical distribution with respect to \(S\) induced by the sampling process._

A proof is given in Appendix C. Proposition 3.1 shows that \(_{ij}^{}\) can be expressed as a conditional empirical expectation of \(\) provided that the \(j\)-th feature is used in \(\) computation. It provides intuitive interpretations: for a fixed \(i\) and \(j k\), \(_{ij}^{}>_{ik}^{}\) implies that the cell \(x_{ij}\) is more helpful to achieve a higher OOB score than the cell \(x_{ik}\), where the OOB score serves as an indicator of model performance. By distinguishing the contributions of individual cells, \(\) effectively realizes joint valuation, providing a finer granularity of analysis that links feature-level importance to individual data quality.

## 4 Experiments

In this section, we empirically show the effectiveness of \(\) across multiple use cases of the joint valuation: _cell-level outlier detection_, _cell fixation_, and _backdoor trigger detection_. To the best of our knowledge, the latter two use cases are introduced here for the first time within the joint valuation framework. As a summary, \(\) can precisely identify anomalous cells that should be prioritized for examination and subsequent fixation to improve model performance. In the context of backdoor trigger detection, \(\) demonstrates its efficacy by accurately identifying different types of triggers within poisoned data, showcasing its proficiency in detecting non-random, targeted anomalies. Our method also exhibits high computational efficiency through run-time comparison.

Throughout all of our experiments, \(\) uses a subset bagging model with \(B=1000\) decision trees. We randomly select a fixed ratio of features to build each decision tree. Unless otherwise specified, we utilize half of the features for each weak learner and set \(T(y_{i},(x_{i,S_{0}}))=(y_{i}=(x_{i,S_{0}}))\). The run time is measured on a single Intel Xeon Gold 6226 2.9 GHz CPU processor. We provide a detailed ablation study of key hyperparameters in Section 4.4.

### Cell-level outlier detection

Experimental settingIn practical situations, even when dealing with abnormal data points, it is not always the case that all cells are noisy [40; 32; 23]. To simulate more realistic settings, we introduce noise to certain _cells_ in the following two-step process: First, we randomly select \(20\%\) rows for each dataset. We then select \(20\%\) columns uniformly at random, allowing each selected row to have a different set of perturbed cells. We inject noises sampled from the low-probability region into these cells, following Du et al.  and Liu et al. . Details on the outlier injection process can be found in Appendix A.3.

We use \(12\) publicly accessible binary classification datasets from OpenML, encompassing a range of both low and high-dimensional datasets, which have been widely used in the literature [13; 25; 26]. Details on these datasets are presented in Appendix A.1. For each dataset, \(1000\) and \(3000\) data points are randomly sampled for training and test datasets, respectively. For the baseline method, we consider \(\), a fast and performant variant of \(\). We incorporate a distance regularization term in the utility function \(T\) for enhanced performance.

ResultsWe calculate the valuations for each cell using our joint valuation framework. Ideally, the outlier cells should receive a low valuation. We then arrange the cell valuations in _ascending_ order and inspect those cells with the lowest values first.

The detection rate curve of inserted outlier is shown in Figure 2. For all datasets, \(\) successfully identifies over \(90\%\) of the outlier cells by inspecting only \(30\%\) of the bottom cells. In comparison, \(\) requires examining nearly \(90\%\) of the cells to achieve the same detection level.

We also evaluate the area under the curve (AUC) as a quantitative metric and measure the run-time. As Table 1 shows, \(\) achieves an average AUC of \(0.83\) across \(12\) datasets, compared to \(0.67\) for \(\), while being significantly faster. For high-dimensional datasets such as the musk dataset, which comprises \(166\) features, \(\) would take more than an hour to process, while \(\) can finish in seconds. Furthermore, we present additional results on **multi-class classification** datasets in Appendix B.1, demonstrating the consistently superior performance and efficiency of \(\).

### Cell fixation experiment

Experimental settingA naive strategy to handle cell-level outliers is to eliminate data points that contain outliers. This method, however, risks substantial data loss, particularly when outliers are scattered and data points are costly to collect. We instead consider a cell fixation experiment, where we assume that the ground-truth annotations of outlier cells can be restored with external expert knowledge. At each step, we "fix" a certain number of cells by substituting them with their ground-truth annotations, prioritizing cells that have the lowest valuations. Then we fit a logistic model3 and evaluate the model's performance with a test set of \(3000\) samples. It is important to note that correcting normal cells has no effect, whereas fixing outlier cells is expected to enhance the model's performance. We adopt the same datasets and implementations as in Section 4.1.

    &  &  \\  & 2D-OOB (ours) & 2D-KNN & 2D-OOB (ours) & 2D-KNN \\  lawschool & **0.88\(\) 0.0027** & 0.75\(\) 0.0011 & **3.33 \(\) 0.06** & 177.56 \(\) 1.92 \\ electricity & **0.77\(\) 0.0072** & 0.68\(\) 0.0014 & **3.39 \(\) 0.07** & 191.38 \(\) 2.60 \\ fired & **0.91\(\) 0.0015** & 0.61\(\) 0.0005 & **3.97 \(\) 0.10** & 322.79 \(\) 2.98 \\
2dplanes & **0.87\(\) 0.0015** & 0.62\(\) 0.0005 & **3.46 \(\) 0.05** & 295.25 \(\) 2.37 \\ creditcard & **0.72\(\) 0.0028** & 0.69\(\) 0.0011 & **4.56 \(\) 0.10** & 662.34 \(\) 7.12 \\ pol & **0.82\(\) 0.0014** & 0.67\(\) 0.0006 & **4.34 \(\) 0.05** & 759.33 \(\) 4.37 \\ MiniBooNE & **0.77\(\) 0.0058** & 0.63\(\) 0.0019 & **7.46 \(\) 0.06** & 1507.83 \(\) 14.50 \\ jannis & **0.83\(\) 0.0042** & 0.55\(\) 0.0004 & **7.98 \(\) 0.07** & 1753.01 \(\) 12.35 \\ nomao & **0.79\(\) 0.0021** & 0.67\(\) 0.0009 & **7.69 \(\) 0.11** & 2564.58 \(\) 23.11 \\ vehicle\_sensIT & **0.81\(\) 0.0014** & 0.61\(\) 0.0005 & **9.87 \(\) 0.08** & 3113.65 \(\) 24.54 \\ gas\_drift & **0.86\(\) 0.0010** & 0.84\(\) 0.0017 & **11.28 \(\) 0.10** & 3878.31 \(\) 40.72 \\ musk & **0.88\(\) 0.0008** & 0.71\(\) 0.0006 & **14.09 \(\) 0.11** & 4415.45 \(\) 22.96 \\  Average & **0.83** & 0.67 & **6.78** & 1636.80 \\   

Table 1: **Cell-level outlier detection results.** AUC and run-time comparison between 2D-OOB and 2D-KNN across twelve binary classification datasets. The average and standard error of the AUC and run-time (in seconds) based on \(30\) independent experiments are denoted by “average \(\) standard error”. Bold numbers denote the best method. The AUC value for the Random method consistently remains at \(0.5\) across all datasets. In every dataset, 2D-OOB achieves a significantly higher AUC while being orders of magnitude faster than 2D-KNN.

Figure 2: **Cell-level outlier detection rate curves for 2D-OOB, 2D-KNN, and Random. The x-axis represents the percentage of inspected cells. The y-axis represents the detection rate, defined as the ratio of the number of detected outlier cells to the total number of outlier cells present in a dataset. The error bars show a \(95\%\) confidence interval based on \(30\) independent experiments. We examine the cells in ascending order, starting from those with the lowest values, and thus a curve closer to the left-top corner indicates better performance. 2D-OOB efficiently detects the majority of outlier cells by examining only a small fraction of the total cells, while 2D-KNN and Random require scanning nearly all the cells.**ResultsFigure 3 illustrates the anticipated trend in the performance of 2D-OOB, validating our method's capability to accurately identify and prioritize the most impactful outliers for correction. As cells with the lowest valuations are progressively fixed, 2D-OOB demonstrates a consistent improvement in model accuracy. In contrast, when applying the same procedure with 2D-KNN, such notable performance enhancements are not observed.

Additionally, we investigate a scenario where ground-truth annotations remain unavailable. We adopt the setup from Liu et al. , where we replace the outlier cells with the average of other cells in the same feature column. 2D-OOB uniformly demonstrates significant superiority over its counterparts. Results are provided in Appendix B.2.

### Backdoor trigger detection

A common strategy of data poisoning attacks involves inserting a predefined trigger (e.g., a specific pixel pattern in an image) into a subset of the training data [14; 6; 31]. These malicious manipulations can be challenging to detect as they only infect specific, targeted samples. Even when poisoned data are present, it could be difficult to discern the root cause of the attacks, since manually reviewing the images is expensive and time-consuming. In this experiment, we introduce a novel task enabled by the joint valuation framework: localizing backdoor triggers in data poisoning attacks. Distinct from the random outliers investigated previously, this type of cell contamination is targeted and deliberate.

Figure 4: **Backdoor trigger detection rate curves for 2D-OOB, 2D-KNN, and Random. Panels A (top) and B (bottom) correspond to the Trojan square and BadNets square, respectively. We inspect the cells within each poisoned sample in descending order of their valuation scores. The detection rate curve shows the average detection rate across all poisoned samples, with error bars representing a \(95\%\) confidence interval based on \(15\) independent runs. 2D-OOB demonstrates superior performance in detecting the cells implanted with triggers.**

Figure 3: **Cell fixation experiment results (test accuracy curves) for 2D-OOB, 2D-KNN, and Random. We replace cells with their ground-truth annotations, starting with those cells assigned the lowest valuations. The results for \(6\) datasets are presented, and additional results for other datasets are available in Appendix B.2. We conduct \(30\) independent trials and report the average results. A higher curve indicates better performance. 2D-OOB demonstrates a superior capability in accurately identifying and rectifying cell-level outliers.**

We consider two popular backdoor attack algorithms: BadNets  and Trojan Attack . During training, the poisoned samples, relabeled as the adversarial target class, are mixed up with clean data. As a result, the model learns to incorrectly associate the trigger with the target class. At test time, inputs containing the trigger are misclassified to the target class. In this context, our goal is to effectively pinpoint the triggers by recognizing them as influential features through our joint valuation framework.

**Experimental setting** We select 5 pairs of classes from CIFAR-10 . For each pair, one class is designated as the target attack class, while the other serves as the source class. The training dataset comprises \(1000\) images. For each attack, we contaminate \(15\%\) of the training samples from the source class and relabel them to the target class. Two types of attack triggers are implemented: the Trojan square and the BadNets square . These triggers are placed in the lower right corner of the original images to minimize occlusion. Further details about these attacks are available in Appendix A.4. In our experiment, the ratio of poisoned cells is approximately \(1\%\), and each weak learner in the subset bagging model is built by sampling \(25\)% of the features.

ResultsWe adopt the same detection scheme and baseline methods as in Section 4.1. Ideally, the poisoned cells should receive high valuation scores given that such data points have been relabeled. We present the detection rate curves for five datasets in Figure 4. 2D-OOB significantly outperforms 2D-KNN in detecting both types of triggers. Overall, 2D-OOB achieves an average detection AUC of \(0.95\) across all datasets and attack types, compared to \(0.83\) for 2D-KNN. It is worth noting that conventional data valuation methods are fundamentally unable to localize backdoor triggers; at most, they can only identify poisoned data points.

Qualitative examplesFigure 5 displays the heatmaps for poisoned samples based on cell valuations of 2D-OOB. Areas with higher cell valuations (marked in dark red) precisely indicate the trigger locations within these samples, demonstrating the effectiveness of our joint valuation framework. Additional examples can be found in Appendix B.3.

### Ablation study

We conduct ablation studies on the cell-level outlier detection task, as outlined in Section 4.1, to examine the impact of key hyperparameters on 2D-OOB estimations, including the selection and number of weak learners, as well as the feature subset ratio.

Selection of weak learnersAlthough our study primarily employs decision trees as weak learners, it is important to note that 2D-OOB is **model-agnostic**, enabling the use of any class of machine learning models as weak learners. Specifically, we examine decision trees, logistic regression, a single-layer MLP with \(64\) units, and a two-layer MLP with \(64\) and \(32\) units, respectively, as weak learners to compare their performance.

Table 2 presents a comparison of detection AUC across \(12\) datasets with different choices of weak learners, indicating that 2D-OOB is not model-free. The selection of weak learners _slightly_ affects the

Figure 5: **Qualitative examples for 2D-OOB in the backdoor trigger detection task. Each pair of images consists of a poisoned image and its corresponding cell valuation heatmap. The color of the heatmap indicates importance, with red cells representing higher importance and blue cells representing lower importance. In the first two pairs, the class ‘bird” is relabeled as “cat”, while in the latter two pairs, the class “deer” is relabeled as “cat”. The heatmaps clearly show that higher cell valuations are predominantly concentrated in the regions containing triggers, while areas featuring actual objects receive lower valuations. This pattern suggests that 2D-OOB effectively captures the triggers as the impactful features responsible for the misclassification of the poisoned samples.**

valuation results, with more complex models generally yielding better performance. Nonetheless, all variations of 2D-OOB outperform 2D-KNN, highlighting the clear advantages of the 2D-OOB approach.

Number of weak learnersIncreasing the number of weak learners allows for a greater number of data-feature subset pairs to be explored, potentially leading to more accurate estimates. However, as shown in Table (a)a, when we increase the number of base learners from \(500\) to \(3000\), the detection AUC for each dataset remains relatively unchanged, suggesting convergence of the estimation beyond a certain threshold. Typically, \(1000\) base learners are sufficient to achieve an equitable joint valuation.

Feature subset ratio \(K/d\)The feature subset ratio \(K/d\) refers to the fraction of the total number of features \(d\) that are randomly selected to build each weak learner, where \(K\) is the number of selected features. In previous experiments, we used a fixed ratio of \(0.50\) (unless otherwise specified). To further investigate the impact of this ratio, we now test two additional values: \(0.25\) and \(0.75\). The results in Table (b)b suggest that in general, the joint valuation capacity of our method is robust to the choice of feature subset ratio.

Apart from the experiments discussed above, we showcase that marginalization of 2D-OOB can either match or surpass state-of-the-art data valuation methods on standard benchmarks in Appendix D.

   Dataset & Decision Tree & Logistic Regression & MLP (single-layer) & MLP (two-layer) & 2D-KNN (Baseline) \\  lawschool & **0.88 \(\) 0.0027** & 0.81 \(\) 0.0014 & 0.83 \(\) 0.0023 & 0.86 \(\) 0.0049 & 0.75 \(\) 0.0011 \\ electricity & **0.77 \(\) 0.0072** & 0.75 \(\) 0.0029 & 0.75 \(\) 0.0039 & 0.74 \(\) 0.0064 & 0.68 \(\) 0.0014 \\ fried & **0.91 \(\) 0.0015** & 0.82 \(\) 0.0023 & 0.85 \(\) 0.0020 & 0.88 \(\) 0.0027 & 0.61 \(\) 0.0005 \\ Zdplanes & 0.87 \(\) 0.0015 & 0.82 \(\) 0.0026 & 0.86 \(\) 0.0026 & **0.88 \(\) 0.0037** & 0.62 \(\) 0.0005 \\ creditcard & 0.72 \(\) 0.0028 & **0.74 \(\) 0.0023** & **0.74 \(\) 0.0026** & **0.74 \(\) 0.0071** & 0.69 \(\) 0.0011 \\ pol & 0.82 \(\) 0.0014 & 0.79 \(\) 0.0029 & 0.85 \(\) 0.0014 & **0.86 \(\) 0.0019** & 0.67 \(\) 0.0006 \\ MiniBoeNE & 0.77 \(\) 0.0058 & 0.77 \(\) 0.0059 & 0.80 \(\) 0.0057 & **0.81 \(\) 0.0119** & 0.63 \(\) 0.0019 \\ jannis & **0.83 \(\) 0.0042** & 0.76 \(\) 0.0040 & 0.79 \(\) 0.0048 & 0.80 \(\) 0.0108 & 0.55 \(\) 0.0004 \\ nomao & 0.79 \(\) 0.0021 & 0.82 \(\) 0.0012 & **0.83 \(\) 0.0010** & **0.83 \(\) 0.0017** & 0.67 \(\) 0.0009 \\ vehicle-sensIT & 0.81 \(\) 0.0014 & 0.81 \(\) 0.0026 & 0.80 \(\) 0.0025 & **0.82 \(\) 0.0037** & 0.61 \(\) 0.0005 \\ gas-drift & 0.86 \(\) 0.0010 & **0.89 \(\) 0.0005** & 0.88 \(\) 0.0005 & 0.88 \(\) 0.0006 & 0.84 \(\) 0.0017 \\ musk & 0.88 \(\) 0.0008 & 0.87 \(\) 0.0005 & **0.88 \(\) 0.0005** & **0.88 \(\) 0.0008** & 0.71 \(\) 0.0006 \\  Average & **0.83** & 0.80 & 0.82 & **0.83** & 0.67 \\   

Table 2: **Ablation study results of weak learner types. The average and standard error of the detection AUC based on \(30\) independent experiments are denoted by “average \(\) standard error”. Results from 2D-KNN are included for comparison. The choice of weak learner leads to variations in cell values, yet the performance of the detection task remains robust.**

   Dataset &  \\  & \(B=500\) & \(B=1000\) & \(B=3000\) \\  lawschool & 0.86 \(\) 0.0035 & 0.88 \(\) 0.0027 & 0.88 \(\) 0.0026 \\ electricity & 0.77 \(\) 0.0025 & 0.77 \(\) 0.0072 & 0.77 \(\) 0.0070 \\ fried & 0.87 \(\) 0.0022 & 0.91 \(\) 0.0015 & 0.91 \(\) 0.0014 \\ \(2d\)planes & 0.87 \(\) 0.0016 & 0.87 \(\) 0.0015 & 0.87 \(\) 0.0015 \\ creditcard & 0.72 \(\) 0.0025 & 0.72 \(\) 0.0028 & 0.72 \(\) 0.0028 \\ pol & 0.87 \(\) 0.0022 & 0.85 \(\) 0.0014 & 0.82 \(\) 0.0014 \\ MiniBoeNE & 0.77 \(\) 0.0042 & 0.77 \(\) 0.0058 & 0.77 \(\) 0.0058 \\ jannis & 0.78 \(\) 0.0045 & 0.83 \(\) 0.0042 & 0.83 \(\) 0.0039 \\ nomao & 0.79 \(\) 0.0018 & 0.79 \(\) 0.0021 & 0.79 \(\) 0.0020 \\ vehicle\_sensIT & 0.80 \(\) 0.0021 & 0.81 \(\) 0.0014 & 0.81 \(\) 0.0014 \\ gas\_drift & 0.86 \(\) 0.0007 & 0.86 \(\) 0.0010 & 0.86 \(\) 0.0010 \\ musk & 0.88 \(\) 0.0008 & 0.88 \(\) 0.0008 & 0.88 \(\) 0.0008 \\  Average & 0.81 & **0.83** & 0.82 \\   

Table 3: **Ablation study results of (a) the number of base learners \(B\) and (b) the feature subset ratio \(K/d\). The average and standard error of the detection AUC based on \(30\) independent runs are denoted by “average \(\) standard error.” (a) Increasing the number of base learners from \(1000\) to \(3000\) does not yield a notable performance improvement. (b) Our method’s joint valuation capacity remains relatively stable regardless of the selected feature subset ratio**Related work

Data contribution estimationIn addition to the marginal contribution-based methods discussed in Section 2, many other approaches are emerging in the area of data valuation. Just et al.  develop a non-conventional class-wise Wasserstein distance between the training and validation sets and use the gradient information to evaluate each data point, an approach that has also been applied to data selection . Wu et al.  extend data valuation to deep neural networks, introducing a training-free data valuation framework based on neural tangent kernel theory. Yoon et al.  leverage reinforcement learning techniques to automatically learn data valuation scores by training a regression model. However, all these data valuation methods do not assign importance scores to cells, whereas our method provides additional insights into how individual cells contribute to the data valuations.

Feature attributionFeature attribution is a pivotal research domain in explainable machine learning that primarily aims to provide insights into how individual features influence model predictions. Various effective methods have been proposed, including SHAP-based explanation [33; 34; 27; 8; 7], counterfactual explanation [47; 18; 39; 35; 36], and backpropagation-based explanation [1; 2; 45; 44; 53]. Among these methods, the SHAP-based explanation stands out as the most widely adopted approach, utilizing cooperative game theory principles to compute the Shapley value . While feature attribution offers a potential method to attribute data valuation scores across individual cells, our empirical experiments in Appendix B.1 reveal that this two-stage scheme falls short in efficacy compared to our proposed joint valuation paradigm, which integrates data valuation and feature attribution in a simultaneous process.

## 6 Conclusion

We propose 2D-OOB, an efficient joint valuation framework that assigns a score to each cell in a dataset, thereby facilitating finer attribution of data contributions and enabling a deeper understanding of the dataset. Through comprehensive experiments, we show that 2D-OOB is computationally efficient and competitive over state-of-the-art methods in multiple joint valuation use cases.

DiscussionWe emphasize that the primary objective of the joint valuation framework is to evaluate the quality of cells within the dataset, rather than to optimize model performance. The model used serves as a proxy for this evaluation, and it is important to note that a high-performing machine learning method does not necessarily ensure a justified valuation framework.

Limitation and future workWhile our study primarily explores random forest models applied to tabular datasets and simple image datasets, the potential application of neural network models within the 2D-OOB framework for more complex vision and language tasks presents a promising avenue for future investigation. For instance, in text datasets, tokens or words can be treated as cells. 2D-OOB can be easily integrated into any bagging training scheme that uses language models.

Overall, we believe that our work will inspire further exploration in the field of joint valuation, with the broader goal of improving the transparency and interpretability of machine learning, as well as developing an equitable incentive mechanism for data sharing.