# Physics-Driven ML-Based Modelling for Correcting Inverse Estimation

Ruiyuan Kang

Bayanat AI

Abu Dhabi, UAE

ruiyuan.kang@bayanat.ai

&Tingting Mu

University of Manchester

Manchester, M13 9PL, United Kingdom

tingting.mu@manchester.ac.uk

Panos Liatsis

Khalifa Univeristy

Abu Dhabi, UAE

panos.liatsis@ku.ac.ae

&Dimitrios C. Kyritsis

Khalifa Univeristy

Abu Dhabi, UAE

dimitrios.kyritsis@ku.ac.ae

The work is done during Ruiyuan's PhD and Postdoc at Khalifa Univeristy

###### Abstract

When deploying machine learning estimators in science and engineering (SAE) domains, it is critical to avoid failed estimations that can have disastrous consequences, e.g., in aero engine design. This work focuses on detecting and correcting failed state estimations before adopting them in SAE inverse problems, by utilizing simulations and performance metrics guided by physical laws. We suggest to flag a machine learning estimation when its physical model error exceeds a feasible threshold, and propose a novel approach, GEESE, to correct it through optimization, aiming at delivering both low error and high efficiency. The key designs of GEESE include (1) a hybrid surrogate error model to provide fast error estimations to reduce simulation cost and to enable gradient based backpropagation of error feedback, and (2) two generative models to approximate the probability distributions of the candidate states for simulating the exploitation and exploration behaviours. All three models are constructed as neural networks. GEESE is tested on three real-world SAE inverse problems and compared to a number of state-of-the-art optimization/search approaches. Results show that it fails the least number of times in terms of finding a feasible state correction, and requires physical evaluations less frequently in general.

## 1 Introduction

Many estimation problems in science and engineering (SAE) are fundamentally inverse problem, where the goal is to estimate the state \(\) of a system from its observation \(\). Examples include estimating the temperature state from the observed spectrum in combustion diagnostics , and discovering design parameters (state) of aero engine according to a group of performance parameters (observation) . Traditional physics-driven inverse solvers are supported by rigorous physical laws, which vary depending on the application, e.g., the two-colour method for spectrum estimation , and cycle analysis for aero engine design . Recent advances take advantage of machine learning (ML) techniques, constructing mapping functions \(F\) to directly estimate the state from the observation, i.e., \(}=F()\)[5; 6; 7]. Such ML solutions are more straightforward to develop, moreover, efficient and easy to use. However, ML-based state estimates can sometimes be erroneous, while SAE applications have very low error tolerance. One can imagine the disastrousconsequences of providing unqualified aero engine design parameters. Therefore, it is critical to detect and correct failed ML estimations before adopting them.

This leads to a special SAE requirement of evaluating the estimation correctness in the deployment process of an ML estimator. Since the ground truth state is unknown at this stage, indirect evaluation has to be performed. Such evaluations can be based on physical forward models and performance metrics [8; 9]. A common practice is to combine multiple evaluations to obtain an accumulated physical error, enforcing quality control from different aspects.

When the physical error exceeds a feasibility threshold, one has to remediate the concerned ML estimation. One practice for finding a better estimation is to directly minimize the physical error in state space . This requires solving a black-box optimization problem, for which it is challenging to find its global optimum, iterative approaches are used to find a near-optimal solution [11; 12]. In each iteration, a set of states are selected to collect their physical errors, then error feedback is used to generate better state(s) until a near-optimal state is found. Physical error collection involves time-consuming simulations[13; 14], e.g., a spectrum simulation which, despite taking just several minutes for each run , can become costly if queried many times. Consequently, the optimization process becomes time-consuming. Therefore, in addition to searching a satisfactory state with as small as possible physical error, it is also vital to decrease the query times to the physical evaluation.

Our work herein is focused on developing an efficient algorithm for remediating the concerned ML estimation in deployment. We propose a novel correction algorithm, **G**enerative **E**xploitation and **E**xploration guided by hybrid **S**urrogate **E**rror (GEESE), building upon black-box optimization. It aims at finding a qualified state within an error tolerance threshold after querying the physical evaluations as few times as possible. The key design elements of GEESE include: (1) A hybrid surrogate error model, which comprises an ensemble of multiple base neural networks, to provide fast estimation of the physical error and to enable informative gradient-based backpropagation of error feedback in model training. (2) A generative twin state selection approach, which consists of two generative neural networks for characterizing the distributions of candidate states, to effectively simulate the exploitation and exploration behaviours. We conduct thorough experiments to test the proposed algorithm and compare it with a series of state-of-the-art optimization/search techniques, based on three real-world inverse problems. Results show that, among the compared methods, GEESE is able to find a qualified state after failing the least number of times and needing to query the physical evaluations less times.

## 2 Related Work

**Optimization in SAE:** Development of SAE solutions often requires to formulate and solve optimization problems [16; 17; 18]. They are often black-box optimization due to the SAE nature. For instance, when the objective function is characterized through physical evaluations and solving partial differential equations (PDEs) , it is not given in a closed form. Typical black-box optimization techniques include Bayesian Optimization , Genetic Algorithm (GA) , and Particle Swarm Optimization (PSO) , etc. They often require a massive number of queries to the objective function in order to infer search directions for finding a near-optimal solution, which is time-consuming and expensive in SAE applications.

Instead, differentiable objective functions are constructed, and the problem is reduced to standard optimization, referred to as white-box optimization to be in contrast with black-box. A rich amount of well established solvers are developed for this, e.g., utilizing first-order and second-order gradient information . Some recent developments use neural networks to optimize differentiable physical model evaluations, e.g., Optnet  and iterative neural networks . However, physics-driven objective functions cannot always be formulated in a differential form, e.g., errors evaluated by the physical forward model in aero engine simulation, which is a mixture of database data, map information and PDEs . A grey-box setting is thus more suitable in practice, where one does not overwrap the evaluations as a black box or oversimplify them as a white box, but a mixture of both.

**Surrogate Model in Black-box Optimization:** To reduce the cost of querying objective function values in black-box optimization, recent approaches construct surrogate models to obtain efficient and cheap estimation of the objective function. This practice has been used in SAE optimization, where the objective functions are mostly based on physical evaluations. The most popular technique for constructing surrogate models is machine learning (ML), including neural networks and Gaussian pro cess models [27; 28; 29]. The associated surrogate model is then incorporated within an optimization process, guided by, for instance, GA and Bayesian optimization, which generate states and interact with it [30; 29], or neural networks that work with differentiable surrogate models [31; 32; 12]. To avoid overfitting, recent effort has been invested to develop surrogate models consistent with some pre-collected data, aiming at obtaining more reliable near-optimal solutions [33; 34; 35; 36; 37]. Nevertheless, there is no guarantee that a surrogate model can well approximate a physical model consistently. Indeed, this is the motivation for the proposed method, where surrogate models are used to speed up the querying process, while the decision in regards to the suitability of the solution is based on the actual physical evaluation.

**Reinforcement Learning for Inverse Problems:** In addition to black-box optimization based approaches, Reinforcement Learning (RL) [38; 39] serves as an alternative framework for solving inverse problems [40; 41; 42]. In an RL-based solution framework, physical evaluations are wrapped as a black-box environment outputting scalar reward, and the actions are the states to estimate according to the observation. The behaviour of the environment is simulated by training a world/critic model [43; 44], which is equivalent to a surrogate model of the physical evaluations. Different from black-box optimization based approaches, RL does not intend to search a feasible state estimation for the given observation, but to learn an authoritative agent/policy model [45; 46] to provide state estimations, while the policy training is guided by optimizing an accumulated scalar reward or error [47; 48]. Because of the desire of training a powerful policy model and the statistical nature of the reward, RL often requires many physical evaluations to collect diverse samples and validate training performance [49; 50]. This can be time-consuming when there is limited computing resource.

## 3 Proposed Method

We firstly explain the notation convention: Ordinary letters, such as \(x\) or \(X\), represent scalars or functions with scalar output. Bold letters, such as \(\) or \(\), represent vectors or functions with vector output. The \(i\)-th element of \(\) is denoted by \(x_{i}\), while the first \(k\) elements of \(\) by \(x_{1:k}\). We use \(||\), \(\|\|_{1}\) and \(\|\|_{2}\) to denote the dimension, \(l_{1}\)-norm and \(l_{2}\)-norm of the vector \(\). An integer set is defined by \([n]=\{1,2 n\}\).

Without loss of generality, an estimated state \(}\) is assessed by multiple physical models and/or metrics \(\{P_{i}\}_{i=1}^{h}\), resulting to an \(h\)-dimensional error vector, denoted by

\[(},)=[E_{P_{1}}(}, ),E_{P_{2}}(},),,E_{P_{h}}(},)].\] (1)

Each concerned ML estimation obtained from an observation \(\) is remediated independently, so \(\) acts as a constant in the algorithm, which enables simplifying the error notation to \((})\) and \(E_{P_{i}}(})\). A better state estimation is sought by minimizing the following accumulated physical error as

\[_{}}e(})=_{i=1}^{h}w_{i}E _{P_{i}}(}),\] (2)

where the error weights are priorly identified by domain experts according to the targeted SAE application. For our problem of interest, the goal is to find a state correction that is within a desired error tolerance, e.g., \(e(})\) where \(>0\) is a feasibility threshold, determined by domain experts. Thus it is not necessary to find a global optimal solution, instead a feasible solution suffices. A typical iterative framework for black-box optimization can be used for this. For instance, at each iteration \(t\), a set of selected states \(\{}_{i}^{(t)}\}_{i=1}^{n_{t}}\) are queried to collect their physical errors resulting in a set of state-error pairs \(\{(}_{i}^{(t)},_{i})\}_{i=1}^{n _{t}}\). A state analysis is then performed according to the error feedback. In the next iteration, a new set of states \(\{}_{i}^{(t+1)}\}_{i=1}^{n_{t+1}}\) are selected to query. This process is repeated until the feasible state \(}^{*}\) that satisfies \(e(}^{*})\) is found. When designing such a framework, the objective is to find a feasible state \(}^{*}\) by querying the physical errors as less times as possible because it is time-consuming to collect the errors.

We challenge the difficult setting of choosing only two states to query at each iteration. To ease the explanation, we first present a sketch of our proposed GESE approach in Algorithm 1. It starts from an initial set of randomly selected and queried states \(\{(}_{i}^{(0)},_{i})\}_{i=1}^{N}\). After this, at each iteration, only two new states are selected to query based on a novel _twin state selection_ approach that we propose, which selects a potentially near-optimal state for exploitation and a potentially informative state for exploration, resulting in \((}_{}^{(t)},_{}^{(t)})\) and \((}_{}^{(t)},_{}^{(t)})\). The selection requires to perform error analysis for a large set of candidate states, involving both the errors and their gradients. To ease and enable such computation, we develop a differentiable surrogate error model \((,)\) to rapidly approximate those error elements that are expensive to evaluate or in need of gradient calculation, and also provide informative gradient guidance with the assistance of error structure. The weights \(\) of the surrogate model are trained using the queried state-error pairs, which start as \(\{(}_{i}^{(0)},_{i})\}_{i=1}^{N}\) and then expand by including \((}_{}^{(t)},_{}^{(t)})\) and \((}_{}^{(t)},_{}^{(t)})\) at each iteration till the algorithm terminates by satisfying the feasibility condition \(e(}^{*})\). Below, we first explain the process of constructing the surrogate model for error approximation, followed by the twin state selection for characterizing the probability distributions of the candidate states and collecting errors, and finally, the implementation of the complete algorithm.

```
1:Randomly select \(n_{0}\) states and query their physical errors to obtain \(D_{0}=\{}_{i}^{(0)},_{i}\}_{i=1}^{n_{0}}\)
2:Train the surrogate error model \((,^{(0)})\) with \(D_{0}\)
3:for\(t T\)do
4: Select a query state \(}_{}^{(t)}\) by exploitation and collect the state-error pair \((}_{}^{(t)},_{}^{(t)})\)
5: Update the feasible state \(}^{*}=}_{}^{(t)}\)
6: Stop the algorithm if \((}^{*})\)
7: Select a query state \(}_{}^{(t)}\) by exploration and collect the state-error pair \((}_{}^{(t)},_{}^{(t)})\)
8: Expand the training data \(D_{t}\) using the two pairs \((}_{}^{(t)},_{}^{(t)})\) and \((}_{}^{(t)},_{}^{(t)})\)
9: Keep training the surrogate error model using \(D_{t}\), resulting in \((,^{(t)})\)
10:endfor
11:output \(}^{*}\) ```

**Algorithm 1** Sketch of GEESE

### Hybrid Neural Surrogate Error Models

We start from an informal definition of implicit and explicit errors. Among the set of \(h\) error elements in Eq. (1 ), those that are expensive to collect or to perform gradient calculation are referred to as _implicit errors_. These can include cases where the system is too complicated and needs much more time to calculate the gradient than that of network backpropagation; or where the system is indifferentiable, such as the physical model of spectroscopy  and aeroengine  containing database or map. In addition to implicit errors, the remaining are _explicit errors_. We order these error elements so that the first \(k\) elements \(\{E_{P_{i}}(})\}_{i=1}^{k}\) are implicit while the remaining \(\{E_{P_{i}}(})\}_{i=k+1}^{n}\) are explicit. Our strategy is to develop a surrogate for each implicit error element, while directly calculate each explicit error.

Taking advantage of the robustness of ensemble learning [51; 52], we propose to estimate the implicit errors by an ensemble of multiple base neural networks. Each base neural network is fully connected with a mapping function \((,):^{D}^{| |}^{k}\), taking the \(D\)-dimensional state space \(^{D}\) as its input space, while returning the approximation of the \(k\) implicit errors by its \(k\) output neurons. An example of such a state space is a space of \(D=2\) dimensions, where the two dimensions correspond to the temperature and concentration states from spectroscopy. Another example is a state space of \(D=11\) dimensions with each state corresponding to a design parameter for aeroengine design, for which we provide more details in Section 4 and Appendix A. The network weights are stored in the vector \(\). We train \(L\) individual base networks sharing the same architecture, while obtain the final prediction using an average combiner. As a result, given a state estimation \(}\), the estimate of the implicit error vector is computed by

\[}_{}(},\{_{i}\} _{i=1}^{L})=_{i=1}^{L}(},_{i}),\] (3)and thus, the accumulated physical error is approximated by

\[(},\{_{i}\}_{i=1}^{L})= ^{k}w_{j}(_{i=1}^{L}_{j}( },_{i}))}_{}+^{h}w_{j}E_{P_{j}}(})}_{}.\] (4)

We refer to Eq. (4) as a hybrid surrogate error model including both approximated and true error evaluation.

The weights of the base neural networks \(\{_{i}\}_{i=1}^{L}\) are trained using a set of collected state-error pairs, e.g., \(D=\{(}_{i},_{i})\}_{i=1}^{N}\). In our implementation, bootstrapping sampling  is adopted to train each base neural network independently, by minimizing a distance loss between the estimated and collected implicit errors, as

\[_{_{i}}_{(},) D}[ ((},_{i}),_{1:k})].\] (5)

A typical example of the distance function is \((},)=\|}-\|_{2}^ {2}\).

Here, we choose to estimate each element of the implicit error vector, rather than to estimate a scalar value of the weighted error sum, because the structural information of the error vector can directly contribute in training through its associated gradient information. When estimating the weighted sum directly, it is in a way to restrict the training loss to a form loosely like \((()-\|\|_{1})^{2}\), which negatively affects the information content of the gradient information. We have observed empirically that, the proposed individual error estimation leads to improvements in training the exploitation generator, compared to using the weighted error sum, see ablation study (1) in Table 2.

### Twin State Selection

A selection strategy, i.e., twin state selection (TSS), for querying two individual states at each iteration is proposed, one for exploration and one for exploitation, respectively. The objective of TSS is to substantially reduce the cost associated with physical error collection. In turn, this translates to the formidable challenge of designing a selection process, which maximizes the informativeness of the associated physical error collection subject to minimizing query times. It is obviously impractical and inaccurate to adopt the naive approach of choosing directly one state by searching the whole space. Instead, we target at a two-folded task, researching (1) which candidate set of states to select from and (2) how to select.

By taking advantage of developments in generative AI, we construct generative neural networks to sample the candidate states. Specifically, we employ a latent variable \(^{d}\), which follows a simple distribution, e.g., uniform distribution \( U([-a,a]^{d})\), and a neural network \((,):^{d}^{| |}^{D}\). The transformed distribution \(p((,))\) is then used to model the distribution of a candidate set. Thus, the task of candidate selection is transformed into determining the neural network weights \(\) for the generator \(\).

In general, exploitation attempts to select states close to the optimal one, whereas exploration attempts to select more informative states to enhance the error estimation. There are various ways to simulate the exploitation and exploration behaviours. For instance, in conventional black-box optimization, e.g., Bayesian optimization and GA, exploitation and exploration are integrated within a single state selection process , while in reinforcement learning, a balance trade-off approach is pursued [55; 39]. Our method treats them as two separate tasks with distinct strategies for constructing generators and selecting states.

**ExploITation:** To simulate the exploitation behaviour, the exploitation generator \(_{}\) is trained at each iteration by minimizing the expectation of the physical error estimate, using the hybrid surrogate error model

\[_{_{}}^{(t)}=_{^{d}}_{ U([-a,a]^{d})}[ (_{}(,),\{ _{i}^{(t-1)}\}_{i=1}^{L})],\] (6)

where the base networks from the last iteration are used and we add the subscript \(t-1\) to the weights of the error network for emphasizing. Finally, among the candidates generated by \(_{}\) with its trained weights \(_{_{}}^{(t)}\), we select the following state

\[}_{}^{(t)}=_{} p (}|_{_{}}^{(t)})}(},\{_{i}^{(t-1)}\}_{i=1}^{L}),\] (7)

to query its physical error by Eq. (1), resulting in the state-error pair \((}_{}^{(t)},_{}^{(t)})\). If the queried error is less than the feasibility threshold, i.e., \(_{}^{(t)}\), this selected state is considered acceptable and the iteration is terminated. Otherwise, it is used to keep improving the training of the surrogate error model in the next iteration.

**ExploRation:** To simulate the exploration behaviour, a state that does not appear optimal but has the potential to complement the surrogate error model should be selected. We use an exploration generator \(_{}\) to generate candidates. To encourage diversity so as to facilitate exploration, we assign the generator random weights sampled from a simple distribution, e.g.,

\[_{_{}}^{(t)} N(0,^{|_ {_{}}|}).\] (8)

We do not intend to train the exploration generator \(_{}\), because any training loss that encourages exploration and diversity can overly drive the base networks to shift focus in the state space and cause instability in the integrated algorithm. Such an instability phenomenon, caused by training \(_{}\), is demonstrated in the ablation study (2) in Table 2.

By adopting the idea of active exploration via disagreement [56; 57], we consider the state, for which the base networks are the least confident about to estimate the implicit errors, as more informative. Since we use an ensemble of base neural networks to estimate the error, the standard deviations of the base network predictions serve as natural confidence measures , which are stored in a \(k\)-dimensional vector:

\[(},\{_{i}^{(t-1)} \}_{i=1}^{L})=[_{1}(\{_{1}( },_{i})\}_{i=1}^{L}),,_ {k}(\{_{k}(},_{i}) \}_{i=1}^{L})].\] (9)

The state maximizing disagreement, i.e., an accumulated standard deviation, between the base networks, is selected given as

\[}_{}^{(t)}=_{} p (}|_{_{}}^{(t)})}(},\{_{i}^{(t-1)}\}_{i=1}^{L} )_{k}^{T},\] (10)

where the row vector \(_{k}=[w_{1},w_{2},,w_{k}]\) stores the implicit error weights. The state-error pair \((}_{}^{(t)},_{}^{(t)})\) is obtained after error collection.

**Surrogate Model Update:** To initialize the algorithm, we priorly collect a set of state-error pairs \(D_{0}=\{_{i},_{i}\}_{i=1}^{N}\) for randomly selected states. Next, at each iteration \(t\), two new states are selected and their physical errors are calculated, thus resulting in two new training examples to update the surrogate error model, and an expanded training set \(D_{t}=D_{t-1}(}_{}^{(t)},_{}^{(t)})(}_{}^{(t)},_{}^{(t)})\). In our implementation, the base neural network weights \(_{i}^{(t-1)}\) obtained from the previous iteration are further fine tuned using the two added examples \((}_{}^{(t)},_{}^{(t)})\) and \((}_{}^{(t)},_{}^{(t)})\), as well as the \(N\) examples sampled from the previous training set \(D_{t-1}\).

### Remediation System and Implementation

Given an ML estimation \(}\), the remediation system collects its physical error vector as in Eq. (1), then calculates the accumulated error from the objective function of Eq. (2) and compares it to the feasibility threshold \(>0\). When the error exceeds the threshold, the GEESE algorithm is activated to search a feasible estimation \(}^{*}\) such that \(e(}^{*})\) by querying the physical error as few times as possible. Algorithm 2 outlines the pseudocode of GEESE2, while Fig.1 illustrates its system architecture. Our key implementation practice is summarized below.

**Empirical Estimation:** Eqs. (6), (7) and (10) require operations performed over probability distributions. In practice, we approximate these by Monte Carlo sampling. For Eq. (6), we minimize instead the average over the sampled latent variables \(Z_{}=\{_{i}\}_{i=1}^{N_{}}\) with \(_{i} U([-a_{},a_{}]^{d})\), and this is fixed in all iterations. The search space of Eq. (7) is approximated by a state set computed from \(Z_{}\) using the trained generator, i.e., \(X_{}^{(t)}=\{_{}(_{i},_{}^{(t)})\}_{i=1}^{N_{}}\). Similarly, the search space of Eq. (10) is approximated by a state sample \(X_{}^{(t)}=\{_{}(_{i},_{}^{(t)})\}_{i=1}^{N_{}}\) where \(_{i} U([-a_{},a_{}]^{d})\).

**Early Stopping:** When training the base neural networks for implicit error estimation, in addition to the maximum iteration number \(T_{e}\), early stopping of the training is enforced when the training loss in Eq. (5) is smaller than a preidentified threshold \(_{e}\). As a result, a higher number \(n_{e}\) of early stopped base neural networks indicates a potentially more accurate error estimation. This strengthens the confidence in training the generator \(_{}\) by Eq. (6) that uses the trained base neural network from the previous iteration. In other words, when the base neural network are not sufficiently well trained, it is not recommended to put much effort in training the generator, which relies on the estimation quality. Therefore, we set the maximum iteration number \(T_{G}\) for training \(_{}\) in proportional to \(n_{e}\), i.e., \(T_{G}=_{G}}{L}+1\), where \(_{G}\) is training frequency coefficient.

**Failed Exploitation Exclusion:** The state selection motivated by exploitation aims at choosing an \(}_{}^{(t)}\) with comparatively low physical error. To encourage this, a focus coefficient \(c\) is introduced,which, together with the feasibility error threshold \(>0\), is used to exclude a potentially failed state with a high estimated error, i.e., \((},\{_{i}\}_{i=1}^{L})>c\), to avoid an unnecessary query.

## 4 Experiments and Results

We test the proposed approach GESE on three real-world engineering inverse problems, including aero engine design , electro-mechanical actuator design  and pulse-width modulation of 13-level inverters . The first problem is to find eleven design parameters (state) of an aero engine to satisfy the thrust and fuel consumption requirement (observation). The second problem is to find 20 design parameters (state) of an electro-mechanical actuator to satisfy requirements for overall cost and safety factor (observation). And the third problem is to find a group of 30 control parameters (state) of a 13-level inverter to satisfy the requirements for distortion factor and nonlinear factor (observation). Details of these problems along with their physical models and metrics for evaluation are explained in supplementary material (Section A). We compare it with a set of classical and state-of-the-art black-box optimization techniques, including Bayesian Optimization with Gaussian Process (BOGP), GA , PSO , CMAES , ISRES , NSGA2 , and UNSGA3 , as well as the recently proposed work SVPEN , which employs RL in solving SAE inverse problems. These techniques are chosen because they are effective at seeking solutions with the assist of actual physical evaluations.

In practice, different types of simulators exist for the same problem. For instance, for problem 1, a simulator mentioned in  with high fidelity takes 10.3 seconds \(\) 309 iterations = 53 minutes to obtain a converged simulation, while another simulator in  with a much lower fidelity can realize real-time simulation. Since we aim at a research scenario that attempts to search a feasible state without querying too much data and without setting a high standard on data quality, we choose to use faster simulators with lower but reasonable fidelity in our experiments. For the three studied problems, each simulator takes no more than five seconds to run. Since the computing time varies when changing simulators, we report the performance by query times instead of actual computing time. Accordingly, we adopt two metrics to compare performance. First, we set a maximum budget of \(T=1,000\) query times for all studied problems and compared methods, and test each method on each problem individually with 100 experimental cases, each case corresponds to a concerned ML state estimation. The setup of the experimental cases is described in Appendix A of supplementary material. we measure the number of experiments out of 100 where a method fails to correct the concerned estimation when reaching the maximum query budget, and refer to it as the failure times \(N_{}\). Also, the average number of queries that a method requires before finding a feasible state in an experiment, is reported over 100 experiments, and referred to as average query times \(N_{}\). A more competitive algorithm expects smaller \(N_{}\) and \(N_{}\).

We report the adopted hyper-parameter and model setting for GESE: The common hyperparameter settings shared between all three studied problems include \(T_{e}=40\), \(_{e}=1e^{-4}\) and \(N=64\), and the learning rates of \(1e^{-2}\) and \(1e^{-4}\), for training the exploitation generator and base neural networks, respectively. Different focus coefficients of \(c=1.5,2\) and \(5\) (set in an increasing fashion) are used for problems 1, 2 and 3, respectively, due to an increased problem complexity in relation to their

Figure 1: The workflow of GESE: If the estimation from an ML estimator fails the physical evaluation \(E_{P}\), GESE is activated. The error estimated by hybrid surrogate error model is used to train the exploitation generator \(}}\). Two candidate state sets are generated by \(}}\) and exploration generator \(}}\), and finally, two states \(}^{*}=_{}}\) and \(_{}}\) are selected by the surrogate error model and fed into \(E_{P}\) for evaluation and data collection. The process is terminated till \(e(}^{*})\).

increasing dimensions of the state space. Similarly, an increasing training frequency coefficient \(_{G}=1,1\) and \(7\) is used for problems 1, 2 and 3, respectively, because the problem requires more training iterations as it involves more complex patterns from higher-dimensional state space. The ensemble surrogate model for estimating the implicit errors is constructed as an average of 4 multi-layer perceptrons (MLPs) each with three hidden layers consisting of 1024, 2028 and 1024 hidden neurons. The exploration generator \(}\) is constructed as a single layer perceptron (SLP) and its one-dimensional input is sampled from \(U([-5,5])\). For problems 1 and 2 that are relatively less complex from an engineering point of view, we directly set the latent space \(\) as the state space \(\) without using any neural network to transform in between. Then, we directly sample initial state set \(X_{}^{(0)}\). The exploitation state is directly optimized iteratively, e.g., by a gradient descent approach based on Eq.(7) to obtain state set \(X_{}^{(t)}\), as shown in Eq. (11). The one with the smallest objective function value is selected as the exploitation state, i.e.,

\[}_{}^{(t)}=_{}} (},\{_{i}^{(t-1)}\}_{i=1}^{L }).\] (11)

Problem 3 involves a special state pattern, requiring an increasing state value over the dimension, i.e., \(_{i}-_{i+1}<0\). To enable the latent variables to capture this, we construct the exploitation generator \(}\) as an MLP with three hidden layers consisting of 256, 512 and 256 hidden neurons. Also, to avoid generation collapse  in problem 3, a regularization term has been added to the training loss in Eq. (6), resulting in the following revised training to encourage state diversity, as

\[_{}}^{(t)}=_{ ^{30}}_{ U([-5,5]^{30})}[ (}(,),\{_{i}^{(t-1)}\}_{i=1}^{L})+(0.0288-_{1}( ,),0)],\] (12)

where \(_{1}(,)\) denotes the standard deviation of the first state element generated by \(}\). We encourage it to shift away from the collapsed point but not overly spread, by bounding \(_{1}\) with a portion of the standard deviation of a uniform distribution, e.g., \(0.288\), and the portion \(=0.0288\) is observed empirically effective. The spread control is only needed for the first state as the remaining states follow by \(_{i}-_{i+1}<0\). Configurations of the competing methods, together with extra information on GEESE, are provided in Appendix B of supplementary material.

### Results and Comparative Analysis

Table 1 summarizes the results of the compared methods for the three problems, obtained with a feasibility threshold of \(=0.075\), which reflects high challenge with low error tolerance. It can be observed that GEESE has the least failure times \(N_{}\) on all three problems. In problem 3, especially, GEESE succeeds with no failure while most other methods have more than 10 failures. This is a highly desired characteristic for a remediation system with low error tolerance. In addition, GEESE also has the least query times \(N_{}\) in all three problems, indicating the best efficiency. We report additional results in Appendix C of supplementary material by varying the feasibility threshold \(\) and the initial sample size \(N\), where GEESE also achieves satisfactory performance in general, while outperforming other methods in handling higher-dimensional problems with lower error tolerance. SVPEN  cannot return a feasible correction in 1000 queries in all experiments, as its core supporting RL requires a lot more queries than other optimization based techniques.

   &  &  &  \\  &  &  &  \\  &  &  &  &  \\  BOGP & 0 & 3.29 \(\)1.51 & 97 & 973.76 \(\)144.28 & 4 & 112.66 \(\)229.98 \\ GA & 0 & 64.00 \(\)0.00 & 0 & 130.56 \(\)3.31 & 13 & 231.76 \(\)339.71 \\ PSO & 0 & 64.00 \(\)0.00 & 0 & 64.00 \(\)0.00 & 12 & 244.16\(\)343.71 \\ CMAES & 0 & 55.67 \(\)3.28 & 0 & 119.44 \(\)41.80 & 12 & 227.42 \(\)312.17 \\ ISRES & 0 & 65.00\(\)0.00 & 0 & 177.64 \(\)80.51 & 16 & 250.05 \(\)350.16 \\ NSGA2 & 0 & 64.00 \(\)0.00 & 0 & 139.52 \(\)68.56 & 13 & 232.40 \(\)359.4 \\ UNSGA3 & 0 & 64.00 \(\)0.00 & 0 & 140.80 \(\)79.94 & 12 & 227.52 \(\)330.07 \\ SVPEN & 100 & 1000.00\(\)0.00 & 100 & 1000.00\(\)0.00 & 100 & 1000.00\(\)0.00 \\ GEESE (Ours) & 0 & **3.18 \(\)1.98** & 0 & **51.65 \(\)33.01** & **0** & **43.56 \(\)65.28** \\  

Table 1: Performance comparison of the compared methods, where the best is shown in **bold**, while the second best is underlined

### Ablation Studies and Sensitivity Analysis

To examine the effectiveness of the key design elements of GEESEE, we perform a set of ablation studies and report the results in Table 2 using problem 1 with a small feasibility threshold \(=0.05\) indicating low error tolerance. The studies include the following altered designs: (1) Estimate directly the implicit error sum using an MLP with the same hidden layers but one single output neuron. (2) Train the exploration generator \(}\) by using an approach suggested by . (3) Remove the early stopping design. (4) Remove the focus coefficient.

Results show that estimating the implicit error sum worsens the performance. As explained earlier in Section 3.1, this is because the structural information in gradient is lost in error sum estimation, causing ambiguous update when training \(}\), and consequently requires GEESE to make more error queries. Also training \(}\) worsens the performance as compared to just assigning random network weights to \(}\) without training. As previously explained in Section 3.2, this is because training \(}\) can frequently shift the focus of the surrogate error model and, thus, impact on the stability of the optimization process. Both early stopping and focus coefficient play an important role in GEESE, where the former prevents GEESE from overfitting and the latter helps avoid unnecessary queries. Additional results on hyperparameter sensitivity analysis for GEESE are provided in Appendix D of supplementary material. The results show that GEESE is not very sensitive to hyperparameter changes and allows a wide range of values with satisfactory performance, which makes GEESE easy to be tuned and used in practice.

## 5 Discussion and Conclusion

We have proposed a novel physics-driven optimization algorithm GEESE to correct ML estimation failures in SAE inverse problems. To query less frequently expensive physical evaluations, GEESE uses a cheaper hybrid surrogate error model, mixing an ensemble of base neural networks for implicit error approximation and analytical expressions of exact explicit errors. To effectively model the probability distribution of candidate states, two generative neural networks are constructed to simulate the exploration and exploitation behaviours. In each iteration, the exploitation generator is trained to find the most promising state with the smallest error, while the exploration generator is randomly sampled to find the most informative state to improve the surrogate error model. These two types of selection are separately guided by the approximated error by the ensemble and the disagreement between its base neural networks. The element-wise error approximation promotes a more effective interaction between the surrogate error model and the two generators. Being tested on three real-world engineering inverse problems, GEESE outperforms all the compared methods, finding a feasible state with the least query number with no failure under the low error tolerance setup.

However, there are still challenges to address in the future, particularly for very high-dimensional inverse problems. Such problems are in need of larger and more complex model architecture to accommodate their more complex underlying patterns, and thus impose challenge on training time and data requirement. Computation expense should not only consider the query cost of physical evaluations but also the learning cost of such models. Flexible neural network architectures that allow for embedding domain specific or induced knowledge in addition to simulation data and their training, as well as their interaction with the main solution model, e.g., an ML estimator for inverse problems, are interesting directions to pursue.

    \\  Surrogate Error Model & Query times & Standard deviation \\  Estimate error elements & **20.20** & **16.37** \\ Estimate error sum & 23.26 & 21.18 \\   \\  Schedule & Query times & Standard deviation \\  with earlystop & **20.20** & **16.37** \\ w/o earlystop & 32.80 & 17.84 \\    
   \\  Exploration style & Query times & Standard deviation \\  w/o training & **32.64** & **22.82** \\ with training & 41.32 & 97.15 \\   

Table 2: Results of ablation studies reported on problem 1, where a better performance is highlighted in **bold**.