# Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation

Wangbo Zhao\({}^{1,2}\)1 &Jiasheng Tang\({}^{2,3}\)2,3 &Yizeng Han\({}^{2,4}\) &Yibing Song\({}^{2,3}\) &Kai Wang\({}^{1}\) &Gao Huang\({}^{4}\) &Fan Wang\({}^{2}\) &Yang You\({}^{1}\)2\({}^{1}\)National University of Singapore &\({}^{2}\)DAMO Academy, Alibaba Group 3Hupan Laboratory &\({}^{4}\)Tsinghua University &

Code: https://github.com/NUS-HPC-AI-Lab/Dynamic-Tuning

###### Abstract

Existing parameter-efficient fine-tuning (PEFT) methods have achieved significant success on vision transformers (ViTs) adaptation by improving parameter efficiency. However, the exploration of enhancing inference efficiency during adaptation remains underexplored. This limits the broader application of pre-trained ViT models, especially when the model is computationally extensive. In this paper, we propose **D**ynamic **T**uning (DyT), a novel approach to improve both parameter and inference efficiency for ViT adaptation. Specifically, besides using the lightweight adapter modules, we propose a _token dispatcher_ to distinguish informative tokens from less important ones, allowing the latter to _dynamically_ skip the original block, thereby reducing the redundant computation during inference. Additionally, we explore multiple design variants to find the best practice of DyT. Finally, inspired by the mixture-of-experts (MoE) mechanism, we introduce an enhanced adapter to further boost the adaptation performance. We validate DyT across various tasks, including image/video recognition and semantic segmentation. For instance, DyT achieves superior performance compared to existing PEFT methods while evoking only \(71\%\) of their FLOPs on the VTAB-1K benchmark.

## 1 Introduction

With the remarkable success of Vision Transformers (ViTs) , fine-tuning pre-trained ViT on other data domains  or task applications  is becoming a common practice. However, as model sizes increase , the associated adaptation costs become prohibitive due to the burden of fine-tuning and inference on the target task. Parameter-efficient fine-tuning (PEFT) methods (_e.g_. AdaptFormer , LoRA , and VPT ), are proposed to tackle the tuning problem by reducing tunable model parameters. They usually update a small amount of parameters while keeping the original model fixed, which reduces learnable parameters effectively while maintaining fine-tuning accuracy.

Despite the extensive research in parameter efficiency, the _inference efficiency_ on target tasks is less explored. We numerically demonstrate the inference efficiency of three representative PEFT methods in Figure. 1(a), revealing that none of them reduces the computation during inference compared to full tuning. This limitation poses challenges for adapting pre-trained ViT to downstream tasks, particularly when the model is computationally extensive. To this end, we aim to unify both parameter and inference perspectives for efficient ViT adaptations.

Dynamic networks [83; 67; 48] demonstrate that token pruning helps to reduce model inference costs. However, these designs primarily focus on pre-training from scratch or full tuning on the same dataset, without considering data domain transfer. Furthermore, the token pruning operation constraints these methods on visual recognition scenarios, limiting their further applications _e.g_. dense prediction . Our motivation thus arises from how to develop dynamic modules together with PEFT methods to enhance both parameter and inference efficiency during ViT adaptation, which should also suit a wide range of visual tasks.

In this paper, we introduce **D**ynamic **T**uning (DyT), an efficient approach for ViTs adaptation that achieves both parameter and inference efficiency. Specifically, we design a token dispatcher for each transformer block learning to dynamically determine whether a token should be activated or deactivated. Only those activated tokens will traverse the entire block and an extra lightweight adapter module, while the remaining tokens skip the original block and will solely be processed by the adapter. DyT does not reduce the total number of tokens, making it suitable for both visual recognition and dense prediction scenarios. Additionally, since the impact of token skipping during adaptation has not been explored, we propose four model variants to determine the best practice for DyT. Finally, as the adapter module is set to process all the visual tokens, we propose a mixture-of-expert(MoE)-based adapter design that further enhances token processing with negligible additional FLOPs. Through these designs, our DyT fulfills both parameter and inference efficiency for ViTs adaptation.

Comprehensive evaluations for DyT are conducted from multiple perspectives. In the image domain, as shown in Figure 1(b). DyT surpasses existing PEFT methods while consuming only \(71\%\) of the ViT-B FLOPs on the VTAB-1K benchmark . When visual tokens are scaled up from images to videos, our DyT shows superior generalization ability on action recognition benchmarks, _e.g_. K400  and SSV2 , with a reduction of 37GFLOPs. In the scenario where labels are scaled up from recognition to segmentation, our DyT even outperforms full tuning on ADE20K  with 21GFLOPs reduction. These experimental results indicate that the proposed DyT is efficient in both parameter and inference across various visual tasks and directs a promising field for efficient model adaptation.

## 2 Related Works

Parameter-efficient fine-tuning.Parameter-efficient fine-tuning (PEFT) is designed to adapt a pre-trained model to downstream tasks by only tuning a small part of the parameters. Existing PEFT methods can broadly be categorized into three groups: adapter-based methods, re-parametrization-based methods, and prompt-based methods. Adapter-based methods [12; 33; 62; 38; 14] insert some tiny modules into the original model and only these inserted modules are updated during fine-tuning. Re-parametrization approaches [87; 34; 47; 21; 18] usually cooperate with reparameterization techniques, directly modifying specific parameters in the pre-trained model. Prompt-based methods [36; 2; 95; 94; 40; 9; 8] involve appending a small number of learnable tokens to input sequences, thereby adapting intermediate features in frozen layers. Please refer to [84; 46] to find more comprehensive reviews.

However, PEFT methods primarily focus on improving parameter efficiency during fine-tuning while overlooking inference cost reduction. In this paper, we propose to improve the parameter efficiency and address inference hosts simultaneously for efficient ViT adaptation.

Figure 1: **FLOPs and Accuracy of ViT-B/16  on VTAB-1K . “Full tuning” denotes that all parameters are fine-tuned. AdaptFormer , LoRA  and VPT  are typical PEFT methods.**

Dynamic neural networks.Dynamic neural networks[28; 63; 92] can dynamically adjust their architectures based on the input data, which enables them to control the computational redundancy based on input data. Existing methods have explored dynamic layer depth [71; 4; 81; 30; 27; 86], dynamic channel width [32; 29; 82; 43] and dynamic routing  in convolutional neural networks. When it comes to the era of vision transformer, many works [77; 76; 69; 67; 48; 74; 57; 59; 58; 64] attempt to improve the inference efficiency by reducing the token redundancy. For instance, Liang _et al_.  identify and consolidate inattentive tokens into only one token in each transformer layer. Rao _et al_.  progressively discard less informative tokens between layers. Wang _et al_.  adjust the number of tokens to represent images. A more detailed literature review can be found in [28; 75]. Zhou _et al_.  propose assessing token significance with a ReLU function to effectively tackle point cloud analysis. Although these approaches have shown significant success in vision tasks, they require training a model from scratch or fine-tuning all parameters on the same dataset used in pre-training, making them unsuitable for efficient ViT adaptation.

In contrast to these methods, the proposed method can adapt pre-trained knowledge to diverse downstream datasets and tasks, while reducing the computational cost during inference by introducing negligible additional parameters.

## 3 ViTs Adaptation with Dynamic Tuning

In the following, we first introduce the vision transformer and the adapter architecture in Section 3.1. Subsequently, we present the proposed dynamic tuning (DyT) and explore its four variants in Section 3.2 and Section 3.3, respectively. Furthermore, we build an MoE-adapter, which effectively enhances the adaptation performance without introducing additional computational cost, as elaborated in Section 3.4. Finally, we introduce the loss functions of our method in Section 3.5.

### Preliminary

Vision Transformer (ViT) consists of a stack of layers. The multi-head self-attention block denoted as \(\), and multi-layer perceptron block, termed as \(\) are the main components in each layer. The input of each layer can be denoted as \(=[_{cls},_{1},_{2},..._{N}] ^{(N+1) C}\), containing \(N\) image tokens and a classification token \(_{cls}\).

The adapter architectures have been widely explored in previous PEFT methods [12; 38; 33]. An adapter is generally designed as a bottleneck module, which consists of a project layer \(_{down}^{C d}\) to squeeze the channel dimension and another project layer \(_{up}^{d C}\) for dimension restoration. Here, \(d\) denotes the bottleneck dimension and \(d C\) ensures the efficiency of the adapter. Given an input feature \(\), the adapter operation is formulated as:

\[_{adapt}=()=(_ {down})_{up},\] (1)

where \(\) denotes a nonlinear function _e.g_. ReLU. In this study, we follow this general architecture to build the adapter within dynamic tuning. We place the adapter in parallel with an \(\) block, \(\), or an entire transformer layer, which will be presented in detail in Section 3.3.

### Dynamic Tuning

In Vision Transformers (ViTs), the computational burden primarily resides in the transformer layers. Specifically, the operations in \(\) and \(\) account for approximately \(35.8\%\) and \(63.5\%\) of total FLOPs in ViT-B/16 . Previous works [67; 48] have revealed that there exists a token-redundancy issue in ViTs and found that some tokens can be discarded without sacrificing the performance. Inspired by this, we propose an efficient ViT adaptation approach, named **D**ynamic **T**uning (DyT), which not only maintains parameter efficiency during fine-tuning but also reduces redundant computation during inference. The core idea is dynamically selecting tokens for processing within transformer blocks. Given the input tokens \(\) of a block, DyT can be formulated as:

\[^{}=(())+( ),\] (2)

where \(\) can represent an multi-head self-attention block \(\), a multi-layer perceptron \(\), or an entire transformer layer. The proposed token dispatcher (\(\)) learns to selectively activate or deactivate tokens. Only the activated tokens are input into the \(\), while all tokens are processed by the \(\).

Token dispatter.The key point in DyT is to select partial tokens which will be passed through \(\) and/or \(\). A straightforward approach is randomly selecting tokens with a predefined probability and adapting the model to conduct downstream tasks with these selected tokens. However, this simplistic strategy risks discarding informative tokens while retaining less important tokens, potentially hindering the adaptation performance. To tackle this problem, we propose a token dispatcher (TD), which learns to select tokens during the adaptation. Specifically, given the input tokens \(^{(N+1) C}\), \(\) learns to selectively activate a sequential of tokens \(_{s}^{K C}\), where \(K\) represents the number of activated tokens. To achieve this, it should obtain a mask \(^{N+1}\), which indicates whether a token should be activated or deactivated.

To obtain \(\), we adopt a projection layer \(_{g}^{C 1}\) followed by a sigmoid function to predict the activation probability \(^{N+1}\). Then, we set 0.5 as the threshold to determine the activation of each token. This can be formulated as:

\[=(_{g}),\ _{n}=1& _{n} 0.5\\ 0&_{n}<0.5.\] (3)

\(_{n}=1\) denotes that the \(n\)-th token is activated and will subsequently undergo the process of \(\). Conversely, if \(_{n}=0\), the token will be deactivated and skip the \(\). In practice, we always set the mask value of the classification token \(_{cls}\) to 1, allowing it to traverse the entire network. Notably, the number of additional parameters introduced in \(\) is negligible, with only \(C\) parameters in the linear layer \(_{g}\).

Fine-tuning stage.However, directly using threshold makes \(\) a discrete decision, resulting in a non-differentiable problem during fine-tuning. To address this, we incorporate the Gumbel Noise  into sigmoid to replace the original sigmoid function _during fine-tuning_. It can be formulated as:

\[}=(_{g})= (_{g}+_{1}-_{2 }}{}),\] (4)

where \(_{1}\), \(_{2}(0,1)\). \(\) represents the temperature and is set to 5.0 by default. Further details of Gumbel-Sigmoid are provided in the Appendix A.7. Subsequently, we can obtain \(}\) using the same operation in Equation.3. The Gumbel Noise makes the sampling of \(}\) stochastic during training and we adopt \(}\) as a differentiable approximation of \(}\). Both of these help \(\) to be trained in an end-to-end manner. The calculation of forward and backward propagations during training can be formulated as:

\[}_{s}=() {}&\\ ()}& ,\] (5)

The Equation 2 can be reformulated into:

\[^{}=+}_{s}+( ),\] (6)

From Equation 5, only the block outputs, \(()\), of those activated tokens are retained, while others are masked out by \(}\). As shown in Figure 2 (a), during the fine-tuning stage, all tokens within \(\) still need to traverse the \(\).

Figure 2: **Overview of Dynamic Tuning.** (a) In the fine-tuning stage, we adopt Gumbel Noise to enable end-to-end training. (b)In the inference stage, \(\) selects \(K\) activated tokens \(_{s}\) from \(\) based on the mask \(\), which saves the computations on those deactivated tokens in \(\). \(\) can represent a \(\) block, a \(\) block, or an entire transformer layer.

Inference stage.During inference, we can directly adopt Equation 3 to generate the dispatch mask \(\) and obtain activated tokens \(_{s}^{K C}\) in \(\). Then, we can only feed them into \(\). Only processing \(K\) tokens effectively reduces the computational cost because \(K<N\). In practice, we add padding to the output from the \(\) to maintain tensor shape. This results in Equation 2. See Figure 2 (b) for a detailed illustration of the inference stage.

### Model Variants

The \(\) in Equation 2 can be instantiated into any blocks in the original ViT, such as a multi-head self-attention block \(\), a multilayer perceptron block \(\), or even a complete transformer layer in ViT. Since the impact of skipping tokens in these blocks during the adaptation fine-tuning remains non-trivial to estimate and has not been explored in previous works, we propose four model variants and conduct experiments to determine the best practice.

* Attention Dispatch: Considering the quadratic computational complexity of the \(\) block with respect to the token numbers, skipping tokens before applying Attn can significantly reduce computation. In this design, multi-head self-attention is exclusively performed between activated tokens, while other tokens are bypassed, which may hurt the interaction between tokens.
* MLP Dispatch: Based on the analysis in Section 3.2, we observe that \(\) takes \(\)63.5% FLOPs in ViT-B/16 and propose to skip tokens only before \(\). It enables that the interaction between tokens in \(\) is not affected.
* Attention-MLP Dispatch: An alternative strategy is skipping tokens before both self-attention and MLP blocks. This design permits a higher activation rate in both \(\) and \(\) while maintaining similar computational efficiency comparable to "Attention Dispatch" and "MLP Dispatch". However, it costs double the additional parameters in adapters.
* Layer Dispatch: Inspired by "Attention-MLP Dispatch", we can dispatch tokens before a transformer layer. Specifically, tokens are identified by one \(\) to be activated or deactivated in the subsequent entire layer. With the same activation rate, its computation is similar to "Attention-MLP Dispatch" while requiring fewer parameters to build adapters.

We demonstrate the architecture variants in Figure 3. The experimental results and analyses of these variants are presented in Section 4.2.

### MoE-Adapter

In DyT, the adapter is responsible for processing all tokens, requiring it to have enough capability, particularly when the downstream tasks _e.g_. semantic segmentation are challenging to adapt. To tackle this problem, we propose a MoE-adapter, inspired by mixture-of-experts [68; 80]. It effectively enhances the capability of the adapter with introducing negligible computation cost.

A MoE-adapter comprises a routing layer \(_{r}^{C N}\) and \(N\) adapter experts, denoted as \(\{_{down}^{i}^{C d},_{up}^{i} ^{d C}\}_{i=1}^{N}\). The routing layer generates a series of scalar as the weights for the experts based on input features. The features from different images may yield distinct expert weights. Specifically, we first conduct average pooling across all tokens to generate a feature as the global representation of them. Subsequently, this representation is fed into the routing layer to generate weight scalars \(\{^{1},^{2},...^{N}\}\). Finally, tokens are processed by each expert independently and combined with the corresponding weight. We demonstrate this in Figure 4.

However, this increases the computational cost of the adapter in proportion to the number of experts \(N\). To address this problem, we adopt its mathematical equivalence to process \(\) in practice, which

Figure 3: **Model variants. For brevity, we omit the LayerNorm  in \(\) and \(\) blocks. “DyT” denotes the dynamic tuning presented in Figure 2.**

can be formulated as:

\[_{adapt}=(_{down}^{moe})_{up}^{moe},\] (7)

where \(_{down}^{moe}=_{i=1}^{N}^{i}_{down}^{i}\) and \(_{up}^{moe}=_{i=1}^{N}^{i}_{up}^{i}\). The design endows the MoE-adapter with the same capacity as \(N\) individual adapters, while maintaining computational efficiency equivalent to that of a single adapter (the computational cost in the routing layer is negligible).

### Loss Functions

For an image \(I\), we calculate the cross-entropy loss \(_{cls}=-(})\) between the predicted probability \(}\) and its corresponding one-hot label \(\), to supervise the learning of classification. To control the average activation rate \(r\) in the entire model, we add a loss term to constrain the number of activated tokens in the dynamic tuning, which can be formulated as: \(_{rate}=(_{l=1}^{L}_{n=1}^{N}_{n}^{l}-r)^{2}\), where \(L\) denotes the number of layers in ViT. \(_{n}^{l}\) represents the mask generated in \(\) from the \(l\)-th layer. Additionally, we employ a loss \(_{cls}^{}=-(^{})\) to supervise the adaptation of the complete model, where \(^{}\) is the output probability without employing the token dispatcher. Thus, this complete model can also act as a teacher to enhance the dynamic tuning during adaption by a distillation loss \(_{distill}=(y^{},y)\), where \(\) represents the Kullback-Leibler divergence loss. Therefore, the overall loss function is defined as \(=_{cls}+_{cls}^{}+_{distill }+_{rate}\), where \(\) serves as the weight of the activation rate loss and is set to 2.0 by default. Note that, DyT can also achieve competitive performance without the distillation loss (see Appendix A.6).

## 4 Experiments

### Experiment Settings

Datasets.To evaluate the adaptation performance, we conduct experiments on VTAB-1K  benchmark. The training data in this benchmark is extremely limited, with only 1,000 training samples for each task. Different from existing PEFT works [12; 38; 39], which mainly focusing on VTAB-1K, we also conduct experiments on three image classification datasets with complete training sets, including CIFAR-100 , SVHN , Food-101 . Additionally, we adopt two video datasets, Kinetic-400 (K400)  and Something-Something V2 (SSv2) , to evaluate the performance when the number of tokens scaled up. All images or frames are resize into 224 \(\) 224. For the dense prediction task, we evaluate our method on two widely recognized semantic segmentation datasets, AED20K  and COCO-stuff . The results of semantic segmentation are presented in the Appendix A.4. We run each task three times and report the averaged results. The error bars are small (\(<0.1\)) and omitted for simplification.

Implementation Details.We conduct all experiments based on the ViT-B/16 , which is pre-trained on ImageNet21K  with full supervision, unless otherwise specified. Results based on ViT-L are presented in the Appendix A.10. The bottleneck dimension \(d\) is set to 64 by default. We adopt the same training schedule as . Detailed hyperparameters for each experiment can be found in the Appendix A.8. The default setting in experiments is marked in \(}\).

### Analysis

Model variants.In Table 1, we compare the performance of four model variants across both image and video datasets. We set the activation rate \(r\) in "Attention Dispatch" and "MLP Dispatch" variants to 0.5, and to 0.7 for "Attention-MLP Dispatch" and "Layer Dispatch" variants, and train each respectively. This results in similar average FLOPs for four variants. We observe that the default

Figure 4: **The architecture of the MoE-adapter.** It is consist of \(N\) adapter experts.

setting "MLP Dispatch" achieves superior performance across five datasets while maintaining the lowest computational cost. Although "Attention-MLP Dispatch" and "Layer Dispatch" also exhibit good performance on K400, the former incurs double additional parameters while the latter lacks generalization capability on other datasets. The comparison between "MLP Dispatch" and other variants proves that only skipping tokens in MLP blocks is a better design. More investigations on model variants can be found in our Appendix A.3.

Effectiveness of MoE-adapter.We conduct experiments to explore the effectiveness of the MoE-adapter and the results are illustrated in Table 2. The MoE-adapter design ensures that the FLOPs will theoretically remain the same as the ordinary adapter, with the computational cost from the routing function being negligible. However, in practical scenarios, the computational cost is also influenced by the learned token dispatch strategy within the Token Dispatcher (TD), leading to slightly varying FLOPs across different models in Table 2.

We observe that the performance on image datasets drops when we increase the expert number in MoE-adapter. This phenomenon can be attributed to the simplicity of image datasets and the model does not require too many parameters to adapt. In contrast, for video datasets, such as K400 and SSv2, the best accuracy is achieved 4 experts. The reason behind this is that the domain gap between the pre-training dataset and video datasets is large and the model needs sufficient adapter capacity to learn the adaptation. This proves that we can introduce the MoE-adapter when the target dataset is difficult to adapt.

Visualization of token activation rate in each layer.In Figure 5, we visualize the token activation rates across different layers in ViT-B/16 . We observe that the model tends to activate more tokens in lower layers while deactivating tokens in higher layers. This phenomenon can be attributed to that the model tends to take more general knowledge from the lower layers of the pre-trained model and learn more task-specific knowledge in higher levels. Additionally, the activation results vary across different datasets. For instance, SSv2 demonstrates increased token activation rate in Layer 5 and Layer 6 compared to other datasets, whereas SVHN experiences substantial token deactivation in Layer 6, 7, and 8. This discrepancy arises from that the model needs different knowledge from the pre-trained weights to address dataset-specific challenges.

It is noteworthy that nearly all tokens are deactivated in the final layer across five datasets, especially CIFAR-100, SVHN, and K400, where the activation rates are exactly 0%. This indicates that on these datasets, we can _directly drop_ the original MLP block in Layer11 without hurting the performance, which further reduces about 4.7M *, 5.4% of the ViT-B total parameters.

    &  &  &  &  \\  & & & CIFAR-100 & SVHN & Food-101 & K400 & SSv2 \\  Attention Dispatch & **1.19** & 14.77 & 84.58 & 96.55 & 86.67 & 69.67 & 41.56 \\ 
**MLP Dispatch** & **1.19** & **12.21** & **91.37** & **97.08** & **90.32** & **75.28** & **45.43** \\  Attention-MLP Dispatch & 2.38 & 12.93 & 90.03 & 96.66 & 88.61 & 74.62 & 41.83 \\ Layer Dispatch & **1.19** & 13.08 & 89.38 & 96.57 & 89.05 & 74.72 & 43.94 \\   

Table 1: **Comparison of model variants.** “Params. (M)” indicates the additional parameters in backbones. “FLOPs (G)” denotes the average FLOPs on CIFAR-100.

    &  &  &  &  \\  & & & CIFAR-100 & SVHN & Food-101 & K400 & SSv2 \\ 
**DyT** & 1.19 & 12.21 & **91.37** & **97.08** & **90.32** & **75.28** & **45.43** \\   DyT\( N=2\) & 2.40 & 12.58 & 91.07 & 96.09 & 89.98 & 74.55 & 45.78 \\ DyT\( N=4\) & 4.80 & 12.29 & 91.01 & 96.90 & 89.77 & **75.43** & **46.56** \\ DyT\( N=8\) & 9.60 & 12.44 & 90.45 & 96.84 & 89.53 & 75.34 & 46.06 \\ DyT\( N=12\) & 14.40 & 12.43 & 90.31 & 96.72 & 89.32 & 75.17 & 44.97 \\   

Table 2: **Effectiveness of MoE-Adapter.** DyT\(\) denotes the DyT with MoE-adapters. Standard adapter is enough to handle image datasets while MoE-adapter is more suitable for challenging scenarios, such as video datasets. It theoretically does not increase extra computational cost, but the FLOPs slightly vary in different models since the learned token dispatch strategy in the TD is different. \(N\) represents the number of experts.

Visualization of activated tokens.In Figure 6, we visualize two representative samples from K400. We can observe that the model tends to deactivate those tokens that are less informative _e.g_. tokens of the sky in (a) and tokens of grass in (b). In higher layers, such as layer7 and layer10, only those tokens from the primary objects are activated. This further proves the the existence of token redundancy problems in ViT and provides validation for the rationality behind our approach. Additional visualizations are available in Appendix A.11.

### VTAB-1K Results

Comparison with PEFT methods.To evaluate the adaptation performance when the training data is limited, we adapt the VTAB-1K  benchmark, which a widely employed to evaluate the performance of adaptation tasks. Following exiting works, we reduce the bottleneck dimension \(d\) to 8, resulting in only 0.16M extra parameters. We vary the activation rate \(r\) in DyT in the range [0.5, 0.7, 0.9] and conduct fine-tuning, obtaining three models with different inference costs.

The results are presented in Table 3. Although previous methods, such as ConvPass  and ResTuning , achieve satisfactory performance, they do not improve the efficiency during inference and even introduce additional FLOPs compared with full fine-tuning. In contrast, with only 12.54 GFLOPs, about 71% of the cost in original ViT-B, our method has outperformed previous methods _e.g_. LoRA  and VPT , by a large margin. Remarkably, the DyT model with \(r=0.9\) does not surpass the performance of the DyT model with \(r=0.7\). This observation suggests the presence of computational redundancy in the transformer after adaptation, which further validates the significance of dynnamic tuning. These experimental results validate that DyT effectively improves parameter efficiency and inference efficiency while maintaining the adaptation performance.

Dynamic tuning achieves actual acceleration.As a hardware-agnostic metric, FLOPs is not the most suitable choice to evaluate the inference efficiency across diverse platforms. The real inference speed has usually been ignored in previous PEFT methods. Here, we adopt two GPUs (Tesla V100 and Tesla T4) and a CPU Xeon(R) Platinum 8163 to comprehensively evaluate the efficiency of our methods and three representative PEFT methods, including LoRA , AdaptFormer , and VPT . The batch size during inference is set to 512 and 32 for GPUs and the CPU, respectively. The results, as summarized in Table 4, reveal that our method achieves better performance while effectively accelerating inference speed compared to existing PEFT methods on different platforms.

Figure 5: **Token activation rate in different layers.** We visualize the token activation rates in ViT-B/16. “Overall” denotes the mean activation rate in the whole model, which arrives at around 50% when \(r\) is set to 0.5. “Layer0” and “Layer11” denote the lowest and highest level, respectively. Notably, the activation rate in the last layer is exactly 0% on CIFAR-100, SVHN, and K400 datasets.

Figure 6: **Visualization of activated tokens.** We present two representative samples from the K400 dataset. Blue patches represent the tokens activated in token dispatcher (Detailed in Section 3.2). Results verify that the token dispatcher has learned to identify informative tokens during fine-tuning.

[MISSING_PAGE_FAIL:9]

and Car . The results are demonstrated in Table 6. We further explore a straightforward approach, "Dynamic-Full", which has a token dispatcher as the same in DyT and is fine-tuned with all parameters. We observe that its performance becomes unstable and drops significantly on some datasets _e.g._ CIFAR-100 and Food-101. This phenomenon may arise due to the potential adverse impact of dynamic dispatch on the pre-trained parameters during full-tuning adaptation, thereby validating the importance of DyT. In this data-sufficient scenario, although our method achieves performance slightly below that of full tuning and AdaptFormer, it brings a significant reduction in FLOPs.

Scaling token counts from images to videos.We conduct experiments on video datasets to show the performance when the number of tokens scaled up. The number of input frames is set to 8. For video tasks, similar to [85; 13], we employ a cross-attention layer and a query token to aggregate features from different frames. The video classification is conducted on the query token. Additional implementation details are provided in the Appendix A.8. We demonstrate the results in Table 6. Although DyT achieves slightly lower performance than AdaptFormer and LoRA, it costs obviously fewer FLOPs. DyT\(\) containing four experts can achieve the best average accuracy with only cost 12.29 GFLOPs, which further verifies the superiority of our design.

## 5 Discussion and Conclusion

Previous methods for ViT adaptation primarily focus on improving the efficiency _during the adaptation_, thus aiming to reduce additional parameters. However, with the increasing size and computational cost of ViT models, the inference cost _after the adaptation_ is becoming a heavy burden. In this paper, we unify both of these two problems into the efficiency problem for ViT adaptation and propose dynamic tuning (DyT) to tackle them simultaneously. We validate its performance and generalization across various tasks and datasets.

Limitations and future works.DyT is currently designed for vision tasks. We believe it would be interesting to combine vision backbones with large language models _e.g._ to build efficient large multi-modal models _e.g._[50; 97]. DyT could be further developed to be compatible with multi-modal models, which would be our future direction.

    &  Params. \(\) \\ (G) \\  } &  &  \\   & & FLOPs\(\) &  &  &  &  \\  & & (G) & CIFAR-100 & SVHN & Food-101 & Air & Pct & Car & Avg. & (G) & K400 & SV2 & Avg. \\   \\  Full tuning & 85.80 & 17.58 & 90.91 & 97.29 & 90.69 & **80.53** & 93.11 & **88.71** & **90.21** & 142.53 & 57.48 & 45.22 & 60.35 \\ Linear & **0** & 17.58 & 85.87 & 56.29 & 88.07 & 40.51 & 92.78 & 52.85 & 69.00 & 142.53 & 69.04 & 27.64 & 48.34 \\ Dynamic-Full & 85.80 & 12.24 & 83.43 & 96.90 & 84.46 & 62.50 & 75.16 & 70.48 & 78.82 & 170.67 & 74.63 & 44.94 & 59.79 \\   \\ AdaptFormer  & 1.19 & 17.81 & **92.03** & 97.23 & **90.84** & 78.23 & **94.46** & 87.33 & 90.02 & 144.39 & **75.53** & 45.36 & 60.45 \\ LaRA  & 1.19 & 17.58 & 91.42 & **97.36** & 90.48 & 76.32 & 93.62 & 87.00 & 89.36 & 142.53 & 57.48 & 45.62 & 60.55 \\ VPT  & 0.07 & 18.32 & 91.46 & 95.72 & 90.41 & 68.91 & 93.92 & 80.72 & 86.88 & 148.44 & 73.46 & 38.17 & 55.82 \\   \\  DyT & 1.19 & **12.21** & 91.37 & 97.08 & 90.32 & 78.89 & 94.45 & 87.80 & 89.99 & 108.31 & 75.28 & 45.43 & 60.36 \\ DyT & \(N=4\) & 4.80 & 12.29 & 91.01 & 96.50 & 89.72 & 78.27 & 93.85 & 87.61 & 89.56 & **105.45** & 75.43 & **46.56** & **60.98** \\   

Table 6: **Results on image and video tasks.** “Avg.” denotes the average results from the corresponding task. The FLOPs are evaluated on CIFAR-100 and K400.

    &  &  & Param. (M) \(\) \\  & Accuracy \(\) \\  DynamicViT  & 60.10 & 14.05 & 88.70 & 1010.40 \\ DynamicViT+AdaptFormer & 75.48 & 14.23 & 3.10 & 954.82 \\ EViT  & 67.42 & 11.62 & 85.80 & 1233.45 \\ EViT+AdaptFormer & 74.63 & 11.77 & 0.16 & 1152.38 \\  Full tuning + ToMe  & 68.68 & 13.12 & 85.80 & 989.29 \\ AdaptFormer  + ToMe  & 74.30 & 13.29 & 0.16 & 941.70 \\  DyT \(r=0.5\) & 77.14 & 12.54 & 0.16 & 912.39 \\ DyT \(r=0.5\) + ToMe  & 76.60 & 9.85 & 0.16 & 1114.70 \\   

Table 5: **Comparison with efficient transformers.** The throughput is measured on a Tesla V100 GPU. “Params. (M) \(\) ” denotes the number of trainable parameters in **backbones**.