# Stochastic contextual bandits with graph feedback:

from independence number to MAS number

 Yuxiao Wen\({}^{}\) Yanjun Han\({}^{}\) Zhengyuan Zhou\({}^{,*}\)

New York University\({}^{}\) Arena Technologies\({}^{*}\)

{yuxiaowen, yanjunhan}@nyu.edu zz26@stern.nyu.edu

###### Abstract

We consider contextual bandits with graph feedback, a class of interactive learning problems with richer structures than vanilla contextual bandits, where taking an action reveals the rewards for all neighboring actions in the feedback graph under all contexts. Unlike the multi-armed bandits setting where a growing literature has painted a near-complete understanding of graph feedback, much remains unexplored in the contextual bandits counterpart. In this paper, we make inroads into this inquiry by establishing a regret lower bound \(((G)T})\), where \(M\) is the number of contexts, \(G\) is the feedback graph, and \(_{M}(G)\) is our proposed graph-theoretic quantity that characterizes the fundamental learning limit for this class of problems. Interestingly, \(_{M}(G)\) interpolates between \((G)\) (the independence number of the graph) and \((G)\) (the maximum acyclic subgraph (MAS) number of the graph) as the number of contexts \(M\) varies. We also provide algorithms that achieve near-optimal regret for important classes of context sequences and/or feedback graphs, such as transitively closed graphs that find applications in auctions and inventory control. In particular, with many contexts, our results show that the MAS number essentially characterizes the statistical complexity for contextual bandits, as opposed to the independence number in multi-armed bandits.

## 1 Introduction

Contextual bandits encode a rich class of sequential decision making problems in reality, including clinical trials, personalized healthcare, dynamic pricing, recommendation systems (Bouneffouf et al., 2020). However, due to the exploration-exploitation trade-off and a potentially large context space, the pace of learning for contextual bandits could be slow, and the statistical complexity of learning could be costly for application scenarios with bandit feedback (Agarwal et al., 2012). There are two common approaches to alleviate the burden of sample complexity, either by exploiting the function class structure for the reward (Zhu and Mineiro, 2022), or by utilizing additional feedback available during exploration.

In this paper we focus on the second approach, and aim to exploit the feedback structure efficiently in contextual bandits. The framework of formulating the feedback structure as feedback graphs in bandits has a long history (Mannor and Shamir, 2011; Alon et al., 2015, 2017; Lykouris et al., 2020), where a direct edge between two actions indicates choosing one action provides the reward information for the other. Such settings have also been generalized to contextual cases (Balseiro et al., 2019; Dann et al., 2020; Han et al., 2024), where counterfactual rewards could be available under different contexts. Typical results in these settings are that, the statistical complexity of bandits with feedback graphs is characterized by some graph-theoretic quantities, such as the independence number or the maximum acyclic subgraph (MAS) number of the feedback graph.

To understand the influence of the presence of contexts on the statistical complexity of learning and to compare with multi-armed bandits, we focus on the _tabular_ setting where the contexts are treated asgeneral variables determining the rewards. A widely studied alternative is the _structured_ setting that leverages certain structures in the dependence on the context. Examples of the latter include linear contextual bandits (Auer, 2002; Agrawal and Goyal, 2013), which assume a linear reward function on the contexts, and their variants (Chu et al., 2011; Li et al., 2017; Agrawal and Devanur, 2016).

Despite the existing results, especially in multi-armed bandits where a near-complete characterization of the optimal regret is available (Alon et al., 2015; Kocak and Carpentier, 2023; Eldowa et al., 2024), the statistical complexity of contextual bandits with feedback graphs is much less understood. For example, consider the case where there is a feedback graph \(G\) across the actions and a complete feedback graph across the contexts (termed as _complete cross-learning_ in (Balseiro et al., 2019)). In this case, for a long time horizon \(T\), the optimal regret scales as \(()\) when there is only one context (Alon et al., 2015), but only an upper bound \(((G)T})\) is known regardless of the number of contexts (Dann et al., 2020). Here \((G)\) and \((G)\) denote the independence number and the MAS number of the graph \(G\), respectively; we refer to Section 1.1 for the precise definitions. While \((G)=(G)\) for all undirected graphs, for directed graphs their gap could be significant. It is open if the change from \((G)\) to \((G)\) is essential with the increasing number of contexts, not to mention the precise dependence on the number of contexts.

### Notations

For \(n\), let \([n]:=\{1,,n\}\). For two probability measures \(P\) and \(Q\) over the same space, let \((P,Q)=|P-Q|/2\) be the total variation (TV) distance, and \((P\|Q)=P(P/Q)\) be the Kullback-Leibler (KL) divergence. We use the standard asymptotic notations \(O,,\), as well as \(,,\) to denote respective meanings within polylogarithmic factors.

We use the following graph-theoretic notations. For a directed graph \(G=(V,E)\), let \(u v\) denote that \((u,v) E\). For \(u V\), let \(N_{}(u)=\{v V:u v\}\) be the set of out-neighbors of \(u\) (including \(u\) itself). We will also use \(N_{}(A)=_{v A}N_{}(v)\) to denote the set of all out-neighbors of vertices in \(A\). The _independence number_\((G)\), _dominating number_\((G)\), and _maximum acyclic subgraph (MAS) number_\((G)\) are defined as

\[(G) =\{|I|:I Vu v, u v I\},\] \[(G) =\{|J|:J VN_{}(J)=V\},\] \[(G) =\{|D|:D VG\},\]

respectively. It is easy to show that \(\{(G),(G)\}(G)\), with a possibly unbounded gap, and a probabilistic argument also shows that \((G)=O((G)|V|)\) (cf. Lemma A.1).

### Our results

In this paper we focus on contextual bandits with both feedback graphs across actions and complete cross-learning across contexts. This setting was proposed in (Han et al., 2024), with applications to bidding in first-price auctions. As opposed to an arbitrary feedback graph across all context-action pairs in (Dann et al., 2020), we assume a complete cross-learning because of two reasons. First, in many scenarios the contexts encode different states which only play roles in the reward function; in other words, the counterfactual rewards for all contexts can be observed by plugging different contexts into the reward function. Such examples include bidding in auctions (Balseiro et al., 2019; Han et al., 2024) and sleeping bandits modeled in (Schneider and Zimmert, 2024). Second, this scenario is representative and sufficient to reflect the main ideas and findings of this paper. Discussion and results under more general settings is left to Section 4.1.

Throughout this paper we consider the following stochastic contextual bandits. At the beginning of each round \(t[T]\) during the time horizon \(T\), an oblivious adversary chooses a context \(c_{t}[M]\) and reveals it to the learner, and the learner chooses an action \(a_{t}[K]\). There is a strongly observable1 directed feedback graph \(G=([K],E)\) across the actions such that all rewards in \((r_{t,c,a})_{c[M],(a_{t},a) E}\) are observable, where we assume no structure in the rewards except that \(r_{t,c,a}\). In our stochastic environment, the mean reward \([r_{t,c,a}]=_{c,a}\) is unknown but invariant with time. We are interested in the characterization of the minimax regret achieved by the learner:

\[_{T}^{}(G,M) =_{^{T}}_{T}(^{T};G,M)\] \[=_{^{T}}_{c^{T}}_{^{K M}}[_{t=1}^{T}(_{^{}(c_{t})[K]}_{c_{1},a^{ }(c_{t})}-_{c_{1},_{t}(c_{t})})],\] (1)

where the infimum is over all admissible policies based on the available observations. In the sequel we might also constrain the class of context sequences to \(c^{T}\), and we will use \(_{T}^{}(G,M,)\) and \(_{T}(^{T};G,M,)\) to denote the respective meanings by taking the supremum over \(c^{T}\).

Our first result concerns a new lower bound on the minimax regret.

**Theorem 1.1** (Minimax lower bound).: _For \(T_{M}(G)^{3}\), it holds that \(_{T}^{}(G,M)=((G)T})\), where the graph-theoretic quantity \(_{M}(G)\) is given by_

\[_{M}(G)=\{_{c=1}^{M}|I_{c}|:I_{1},,I_{M} [K],I_{i} I_{j}i<j\},\] (2)

_and \(I_{i} I_{j}\) means that \(u v\) whenever \(u I_{i}\) and \(v I_{j}\)._

Theorem 1.1 provides a minimax lower bound on the optimal regret, depending on both the number of contexts \(M\) and the feedback graph \(G\). Note that the independent subsets \(I_{1},,I_{M}\) are allowed to be empty if needed. It is clear that \(_{1}(G)=(G)\) is the independence number, and \(_{M}(G)=(G)\) whenever \(M(G)\). This leads to the following corollary.

**Corollary 1.2** (Tightness of MAS number).: _For any graph \(G\), if \(M(G)\) and \(T(G)^{3}\), one has \(_{T}^{}(G,M)=((G)T})\)._

Corollary 1.2 shows that, the regret change from \(()\) in multi-armed bandits to \(((G)T})\) in contextual bandits (Dann et al., 2020) is in fact not superfluous when there are many contexts. In other words, although the _independence number_ determines the statistical complexity of multi-armed bandits with graph feedback, the statistical complexity in contextual bandits with many contexts is completely characterized by the _MAS number_.

For intermediate values of \(M(1,(G))\), the next result shows that the quantity \(_{M}(G)\) is tight for a special class \(_{}\) of context sequence called _self-avoiding contexts_. A context sequence \((c_{1},,c_{T})\) is called self-avoiding iff \(c_{s}=c_{t}\) for \(s<t\) implies \(c_{s}=c_{s+1}==c_{t}\) (or in other words, contexts do not jump back). For example, \(113222\) is self-avoiding, but \(12231\) is not. This assumption is reasonable when contexts model a nonstationary environment changing slowly, e.g. the environment changes from season to season.

**Theorem 1.3** (Upper bound for self-avoiding contexts).: _For self-avoiding contexts, there is a policy \(\) achieving \(_{T}(;G,M,_{})=((G)T})\). This policy can be implemented in polynomial-time, and does not need to know the context sequence in advance._

As the minimax lower bound in Theorem 1.1 is actually shown under \(_{}\), for large \(T\), Theorem 1.3 establishes a tight regret bound for stochastic contextual bandits with graph feedback and self-avoiding contexts. The policy used in Theorem 1.3 is based on arm elimination, where a central step of exploration is to solve a sequential game in general graphs which has minimax value \((_{M}(G))\) and could be of independent interest.

For general context sequences, we have a different sequential game in which we do not have a tight characterization of the minimax value in general. Instead, we have the following upper bound, which exhibits a gap compared with Theorem 1.1.

**Theorem 1.4** (Upper bound for general contexts).: _For general contexts, there is a policy \(\) achieving_

\[_{T}(;G,M)=(_{ M}(G),(G)\}T}),\]

_where_

\[_{M}(G)=\{_{c=1}^{M}|I_{c}|:I_{1}, ,I_{M}[K]\}.\] (3)Fortunately, additional assumptions on the feedback graph \(G\) can be leveraged to recover the tight regret bound:

**Corollary 1.5** (Upper bound for transtively closed or undirected feedback).: _For any undirected or transitively closed graph \(G\), the policy \(\) in Theorem 1.4 achieves a near-optimal regret \(_{T}(;G,M)=((G)T})\)._

A directed graph \(G\) is called _transitively closed_ if \(u v\) and \(v w\) imply that \(u w\). In reality directed feedback graphs are often transitively closed, for a directed structure of the feedback typically indicates a partial order over the actions. Examples include bidding in auctions (Zhao and Chen, 2019; Han et al., 2024) and inventory control (Huh and Rusmevichientong, 2009), both of which exhibit the one-sided feedback structure \(i j\) for \(i j\). For general graphs, Theorem 1.4 gives another graph-theoretic quantity \(_{M}(G)\). Note that \(_{M}(G)_{M}(G)\) as there is no acyclic requirement between \(I_{c}\)'s in (3), which in turn is due to a technical difficulty of non-self-avoiding contexts. Further discussions on this gap are deferred to Section 4.3.

Interestingly, the upper bound quantities \(_{M}(G)\) and \(_{M}(G)\) are not explicitly linear in \(M\) and are always no larger than \((G)M\). Hence our results partially answer an open problem in (Hao et al., 2022, Remark 5.11) that if the dependence of regret bound \(O()\) on \(M\) can be improved.

### Related work

The study of bandits with feedback graphs has a long history dating back to (Mannor and Shamir, 2011). For (both adversarial and stochastic) multi-armed bandits, a celebrated result in (Alon et al., 2015, 2017) shows that the optimal regret scales as \(()\) if \(T(G)^{3}\); the case of smaller \(T\) was settled in (Kocak and Carpentier, 2023), where the optimal regret is a mixture of \(\) and \(T^{2/3}\) rates. For stochastic bandits, simpler algorithms based on arm elimination or upper confidence bound (UCB) are also proposed (Lykouris et al., 2020; Han et al., 2024), while the UCB algorithm is only known to achieve an upper bound of \(((G)T})\).2 In addition to strongly observable graphs we primarily focus on, weakly observable graphs have also drawn vast interest (Alon et al., 2015; Chen et al., 2021) where the optimal regret is characterized by the dominating number \((G)\). There exploration plays a more significant role due to weaker observability of certain nodes, leading to an optimal regret \(((G)^{1/3}T^{2/3})\). We will briefly discuss the regret characterization of our contextual setting with weakly observable graphs in Section 4.1 and 4.2.

Recently, the graph feedback was also extended to contextual bandits under the name of "cross-learning" (Balseiro et al., 2019; Schneider and Zimmert, 2024). The work (Balseiro et al., 2019) considered both complete and partial cross-learning, and showed that the optimal regret for stochastic bandits with complete cross learning is \(()\). Motivated by bidding in first-price auctions, (Han et al., 2024) generalized the setting to general graph feedback across actions and complete cross-learning across contexts, a setting used in the current paper. The finding in (Han et al., 2024) is that the effects of graph feedback and cross-learning could be "decoupled": a regret upper bound \((T})\) is shown, which is tight only for a special choice of the feedback graph \(G\). The work (Dann et al., 2020) considered a tabular reinforcement learning setting with adversarial initial states, so that their setting with episode length \(H=1\) coincides with our problem with a general feedback graph \(G\) across all context-action pairs. They showed that the UCB algorithm achieves a regret upper bound \(((G)T})\); however, their lower bound was only \(()\) when \(T(G)^{3}\). Therefore, tight lower bounds that work for general graphs \(G\) are still underexplored in the literature, and our regret upper bounds in Theorems 1.3 and 1.4 also improve or generalize the existing results.

The problem of bandits with feedback is also closely related to partial monitoring games (Bartok et al., 2014). Although this is a more general setting which subsumes bandits with graph feedback, the results in the literature (Bartok et al., 2014; Lattimore, 2022; Foster et al., 2023) typically have tight dependence on \(T\), but often not on other parameters such as the dimensionality. Similar issues also applied to the recent line of work (Foster et al., 2021, 2023) aiming to provide a unified complexity measure based on the decision-estimation coefficient (DEC); the nature of the two-point lower bound used there often leaves a gap. We also point to some recent work (Zhang et al., 2024) which adopted the DEC framework and established regret bounds for contextual bandits with graph feedback, but no cross-learning across contexts, based on regression oracles.

## 2 Hard instance and the regret lower bound

In this section we sketch the proof of the minimax lower bound \(^{*}_{T}(G,M,_{})=((G)T})\) for \(T_{M}(G)^{3}\) and general \((G,M)\), implying Theorem 1.1. We first identify a hard instance that corresponds to the graph-theoretic quantity \(_{M}(G)\), and then present the core exploration-exploitation tradeoff in the proof to arrive at the fundamental limit of learning under this instance. This approach has been widely adopted in the bandit literature. The complete proof is deferred to Appendix B.

The proof uses the definition (2) of \(_{M}(G)\) to construct \(M\) independent sets \(I_{1},,I_{M}\) such that \(I_{i} I_{j}\) for \(i<j\); by definition, the independent sets \(I_{1},,I_{M}\) are disjoint. We then construct a hard instance where the best action under context \(c[M]\) is distributed uniformly over \(I_{c}\); since \(I_{c}\) is an independent set, this ensures that the learner must essentially explore all actions in \(I_{c}\) under context \(c\). Moreover, the context sequence \(c^{T}\) is set to be \(11 122 2 M\), i.e. never goes back to previous contexts. This order ensures that the exploration in \(I_{c_{1}}\) during earlier rounds provides no information to the exploration in \(I_{c_{2}}\) during later rounds, whenever \(c_{1}<c_{2}\). Naively, if the learner only explores in each \(I_{c}\) under context \(c\), then learning under each context \(c\) becomes a multi-armed bandit problem (because \(I_{c}\) is itself an independent set), and we can show lower bound \(^{M}|I_{c}|}\) with appropriate context sequence \(c^{T}\). Maximizing over all possible constructions gives the desired result.

It is possible, however, for the learner to choose actions outside \(I_{c}\) to obtain information for the later rounds. To address this challenge, we use a delicate exploration-exploitation tradeoff argument to show that this pure exploration must incur a too large regret to be informative when \(T_{M}(G)^{3}\). Specifically, consider the regret incurred by this pure exploration:

\[_{}=_{c=1}^{M}_{t T_{c}}[1(a_{t}  I_{c})]\]

where \(T_{c}=\{t[T]:c_{t}=c\}\). Then for some absolute constants \(c_{1}\) and \(c_{2}\), the tradeoff can be formulated as two lower bounds of the regret \(_{T}\):

\[_{T} c_{1}(G)T}(-_{M}(G)_{ }/T)_{T} c_{2}_{ }.\]

The first bound is decreasing in the amount of pure exploration, while the second one is increasing. Balancing this tradeoff gives the desired lower bound \(_{T}=(G)T}\) for \(T_{M}(G)^{3}\).

In summary, the key structure used in the proof is that \(I_{i} I_{j}\) for \(i<j\); we remark that this does not preclude the possibility that \(I_{j} I_{i}\) for \(j>i\), which underlies the change from \((G)\) to \((G)\) as the number of context increases.

## 3 Algorithms achieving the regret upper bounds

This section provides algorithms that achieve the claimed regret upper bounds in Theorems 1.3 and 1.4. The crux of these algorithms is to exploit the structure of the feedback graph and choose a small number of actions to explore. Depending on whether the context sequence is self-avoiding or not, the above problem can be reduced to two different kinds of sequential games on the feedback graph. Given solutions to the sequential games, Sections 3.2 and 3.3 will rely on the layering technique to use these solutions on each layer, and propose the final learning algorithms via arm elimination.

### Two sequential games on graphs

In this section we introduce two sequential games on graphs which are purely combinatorial and independent of the learning process. We begin with the first sequential game.

**Definition 1** (Sequential game I).: _Given a directed graph \(G=(V,E)\) and a positive integer \(M\), the sequential game consists of \(M\) steps, where at each step \(c=1,,M\):_

1. _the adversary chooses a strongly observable subset_ \(A_{c} V\) _disjoint from_ \(N_{}(_{c^{}<c}D_{c^{}})\)_;_
2. _the learner chooses_ \(D_{c} A_{c}\) _such that_ \(D_{c}\) _dominates_ \(A_{c}\)_, i.e._ \(A_{c} N_{}(D_{c})\)_._3__ 
_The learner's goal is to minimize the total size \(_{c=1}^{M}|D_{c}|\) of the sets \(D_{c}\)._

The above sequential game is motivated by bandit learning under self-avoiding contexts. Consider a self-avoiding context sequence in the order of \(1,2,,M\). For \(c[M]\), the set \(A_{c}\) represents the "active set" of actions, i.e. the set of all probably good actions, yet to be explored when context \(c\) first occurs. Thanks to the self-avoiding structure, "yet to be explored" means that \(A_{c}\) must be disjoint from \(N_{}(_{c^{}<c}D_{c^{}})\). The learner then plays a set of actions \(D_{c} A_{c}\) to ensure that all actions in \(A_{c}\) have been explored at least once; we note that a good choice of \(D_{c}\) not only aims to observe all of \(A_{c}\), but also tries to observe as many actions as possible outside \(A_{c}\) and make the complement of \(N_{}(_{c^{} c}D_{c^{}})\) small. The final cost \(_{c=1}^{M}|D_{c}|\) characterizes the overall sample complexity to explore every active action once over all contexts.

It is clear that the minimax value of this sequential game is given by

\[U_{1}^{}(G,M)=_{A_{1} V}_{D_{1}  A_{1}\\ A_{1} N_{}(D_{1})}_{ {c}A_{M} V\\ _{c=1}^{M-1}D_{c}_{M}}_{D_{M } A_{M}\\ A_{M} N_{}(D_{M})}_{c=1}^{M}|D_{c}|.\] (4)

The following lemma characterizes the quantity \(U_{1}^{}(G,M)\) up to an \(O(|V|)\) factor.

**Lemma 3.1** (Minimax value of sequential game I).: _There exists an absolute constant \(C>0\) that_

\[_{M}(G) U_{1}^{}(G,M) C_{M}(G)|V|.\]

_Moreover, the learner can achieve a slightly larger upper bound \(O(_{M}(G)^{2}|V|)\) using a polynomial-time algorithm._

The second sequential game is motivated by bandit learning with an arbitrary context sequence.

**Definition 2** (Sequential game II).: _Given a directed graph \(G=(V,E)\) and a positive integer \(M\), the sequential game starts with an empty set \(D_{0}=\), and at time \(t=1,2,\):_

1. _the adversary chooses an integer_ \(c_{t}[M]\) _(and a set_ \(A_{c_{t}} V\) _if_ \(c_{t}\) _does not appear before). The adversary must ensure that_ \(A_{c_{t}} N_{}(D_{t-1})\) _is non-empty;_
2. _the learner picks a vertex_ \(v_{t} A_{c_{t}}\) _and updates_ \(D_{t} D_{t-1}\{v_{t}\}\)_._

_The game terminates at time \(T\) whenever the adversary has no further move (i.e. \(_{c}A_{c} N_{}(D_{T})\)), and the learner's goal is to minimize the duration \(T\) of the game._

The new sequential game reflects the case where the context sequence might not be self-avoiding, so instead of taking a set of actions at once, the learner now needs to take actions non-consecutively. Clearly the sequential game II is more difficult for the learner as it subsumes the sequential game I when the context sequence is self-avoiding: the set \(D_{c}\) in Definition 1 is simply the collection of \(v_{t}\)'s in Definition 2 whenever \(c_{t}=c\). Consequently, the minimax values satisfy \(U_{2}^{}(G,M) U_{1}^{}(G,M)\). The following lemma proves an upper bound on \(U_{2}^{}(G,M)\).

**Lemma 3.2** (Minimax value of sequential game II).: _There exists a polynomial-time algorithm for the learner which achieves_

\[U_{2}^{}(G,M)_{}(G,M)\{(G),C _{M}(G)^{2}|V|\},\]

_where \(C>0\) is an absolute constant, \(_{M}(G)\) is given in (3), and_

\[_{}(G,M)= _{c=1}^{M}|B_{c}|:_{c}B_{c}B_{c} V_{c}V_{c}\] \[V_{1},,V_{M} V,|B_{c}|(V_{c})(1+|V|).}\]

### Learning under self-avoiding contexts

Given a learner's algorithm for the first sequential game, we are ready to provide an algorithm for bandit learning under any self-avoiding context sequence. The algorithm relies on the well-known idea of arm elimination (Even-Dar et al., 2006): for each context \(c[M]\), we maintain an active set \(A_{c}\) consisting of all probably good actions so far under this context based on usual confidence bounds of the rewards. To embed the sequential games into the algorithm, we further make use of the _layering technique_ in (Lykouris et al., 2020; Dann et al., 2020): for \(\), we construct the set \(A_{c,}\) as the active set on layer \(\) such that all actions in \(A_{c,-1}\) have been taken for at least \(-1\) times. In other words, the active set \(A_{c,}\) is formed based on \(-1\) reward observations of all currently active actions. As higher layer indicates higher estimation accuracy, the learner now aims to minimize the duration of each layer \(\), which is precisely the place we will play an independent sequential game.

``` Input: time horizon \(T\), action set \([K]\), context set \([M]\), feedback graph \(G\), a subroutine \(\) for the sequential game I, failure probability \((0,1)\). Initialize: active sets \(A_{c,1}[K]\) for all contexts \(c[M]\) on layer \(1\). for\(c=1\)to\(M\)do for\(=1,2,\)do  compute \(D_{c,} A_{c,} N_{}(_{c^{}<c} D_{c^{},})\) according to the subroutine \(\), based on past  plays \[(A_{c^{},} N_{}(_{i<c^{}}D_{i,}) )_{c^{} c}(D_{c^{},})_{c^{}<c};\]  choose each action in \(D_{c,}\) once (break the loop if \(c_{t} c\) or \(t>T\) during this process),  and update \(t\) accordingly;  compute the empirical rewards \(_{c,a}\) for all actions based on all historic reward observations;  choose the following active set on the next layer: \[A_{c,+1}\{a A_{c,}:_{c,a}_{a^{}  A_{c,}}_{c,a^{}}-2} \};\] (5) move to the next layer \(+1\).  end for  end for ```

**Algorithm 1**Arm elimination algorithm for self-avoiding contexts

The description of the algorithm is summarized in Algorithm 1, and we assume without loss of generality that the self-avoiding contexts comes in the order of \(1,,M\) (the duration of some contexts might be zero). During each context, Algorithm 1 sequentially constructs a shrinking sequence of active sets \(A_{c,1} A_{c,2}\), and on each layer \(\), the algorithm plays the sequential game I based on the current status (past plays \((A_{c^{},})_{c^{} c}\), or equivalently \((A_{c^{},} N_{}(_{i<c^{}}D_{i,} ))_{c^{} c}\), of the adversary, and past plays \((D_{c^{},})_{c^{}<c}\) of the learner).4 After the rewards of all actions of \(A_{c,}\) have been observed once, the algorithm constructs the active set \(A_{c,+1}\) for the next layer based on the confidence bound (5) and sample size \(\).

The following theorem summarizes the performance of the algorithm.

**Theorem 3.3** (Regret upper bound of Algorithm 1).: _Let the subroutine \(\) for the sequential game I be the polynomial-time algorithm given by Lemma 3.1. Then with probability at least \(1-\), the regret of Algorithm 1 is upper bounded by_

\[_{T}(\;1;G,M,_{})=O((G)^{2}(K)(MKT/)}).\]

On a high level, by classical confidence bound arguments, each action chosen on layer \(\) suffers from an instantaneous regret \((1/)\). Moreover, Lemma 3.1 shows that the number of actions chosen on a given layer is at most \((_{M}(G))\). A combination of these two observations leads to the \(((G)T})\) upper bound in Theorem 3.3, and a full proof is provided in Appendix C.

### Learning under general contexts

The learning algorithm under a general context sequence is described in Algorithm 2. Similar to Algorithm 1, for each context \(c\) we break the learning process into different layers, construct active sets \(A_{c,}\) for each layer, and move to the next layer whenever all actions in \(A_{c,}\) have been observed once on layer \(\). The only difference lies in the choice of actions on layer \(\), where the plays from the sequential game II are now used. The following theorem summarizes the performance of Algorithm 2, whose proof is very similar to Theorem 3.3 and deferred to Appendix C.

``` Input: time horizon \(T\), action set \([K]\), context set \([M]\), feedback graph \(G\), a subroutine \(\) for the sequential game II, failure probability \((0,1)\). Initialize: active sets \(A_{c,}[K]\) for all contexts \(c[M]\) and layers \( 1\); set of actions \(D_{}\) chosen on layer \(\); the current layer index \((c) 1\) for all \(c[M]\). for\(t=1\)to\(T\)do  receive the context \(c_{t}\), and compute the current layer index \(_{t}=(c_{t})\);  according to subroutine \(\), choose an action \(a_{t} A_{c_{t},_{t}}\) based on the active sets \((A_{c,_{t}})_{c[M]}\) and previously taken actions \(D_{_{t}}\) on the current layer;  update the set of actions on layer \(_{t}\) via \(D_{_{t}} D_{_{t}}\{a_{t}\}\); for\(c[M]\)do  compute the new layer index \(_{}(c)=\{:A_{c,} N_{}(D_{})\}\); if\(_{}(c)>(c)\)then  compute the empirical rewards \(_{c,a}\) for all actions based on all historic observations;  choose the following active set on the new layer: \[A_{c,_{}(c)}\{a A_{c,(c)}:_{c,a} _{a^{} A_{c,(c)}}_{c,a^{}}-2}(c)-1}}\};\]  update the layer index \((c)_{}(c)\).  end if  end for  end for ```

**Algorithm 2**Arm elimination under general contexts

**Theorem 3.4** (Regret upper bound of Algorithm 2).: _Let the subroutine \(\) for the sequential game II be the polynomial-time algorithm given by Lemma 3.2. Then with probability at least \(1-\), the regret of Algorithm 2 is upper bounded by_

\[_{T}(\ 2;G,M)=O(}(G,M) (MKT/)}).\]

By the second inequality in Lemma 3.2, Theorem 3.4 implies Theorem 1.4. Corollary 1.5 then follows from the following result.

**Lemma 3.5**.: _For undirected or transitively closed graph \(G\), it holds that \(_{}(G,M)=O(_{M}(G)|V|)\)._

## 4 Discussions

### Weakly observable feedback graphs

Naturally, we may ask what results we would get under a weaker assumption / a more general feedback structure. If the feedback graph \(G\) is instead weakly observable5, then under complete cross-learning, an explore-then-commit (ETC) policy can achieve regret \(((G)^{1/3}T^{2/3})\) by first exploring the minimum dominating set6 uniformly for time \((G)^{1/3}T^{2/3}\), and then committing to the empirically best action that has suboptimality bounded by \(((G)^{1/3}T^{-1/3})\) with high probability. This matches the existing lower bound in (Alon et al., 2015) and is hence near-optimal.

### Incomplete cross-learning

It is possible to further relax the assumption of complete cross-learning. Suppose the feedback across contexts is characterized by another directed graph \(G_{[M]}\) (and denote \(G_{[K]}\) across actions respectively), and consider a product feedback graph \(G_{[K]} G_{[M]}\) over the context-action pairs such that \((a_{1},c_{1})(a_{2},c_{2})\) if \(a_{1} a_{2}\) in \(G_{[K]}\) and \(c_{1} c_{2}\) in \(G_{[M]}\). Then we can get the following generalized results.

#### 4.2.1 Weakly observable feedback graphs on actions

When the feedback graph \(G_{[K]}\) is weakly observable, following the argument in Section 4.1, we can achieve regret \((G_{[K]})(G_{[M]})^{1/3}T^{ 2/3}\) by running an ETC subroutine for each context as follows: for every context \(c[M]\), we keep an "exploration" counter \(n_{c}\). At each time \(t\) with context \(c_{t}\), if \(n_{c_{t}}(G_{[K]})^{1/3}(G_{[M]})^{-2/3}T^{2/3}\), we are in the "exploration" stage and continue to uniformly explore the minimum dominating set of \(G_{[K]}\). Then we increase the counter for all observed contexts, i.e. \(n_{c} n_{c}+1\) for all \(c N_{}(c_{t})\) in \(G_{[M]}\). Otherwise, we "commit" to the empirically best action that has suboptimality bounded by \((G_{[K]})(G_{[M]})^{1/3}T^{ -1/3}\) with high probability.

The key observation is that the number of times we are in the "exploration" stage is \((G_{[K]})(G_{[M]})^{1/3}T^{ 2/3}\). This can be seen from a layering argument, similar to the one in Section 3.2, that the number of actually played contexts on each layer is at most \((G_{[M]})\). Together with the bounded rewards and the bounded suboptimality in the "commit" stage, this proves the regret upper bound.

Combining the context sequence construction in Section 2 and the lower bound argument in (Alon et al., 2015), one can also prove a matching lower bound \((G_{[K]})(G_{[M]})^{1/3}T^{2/3} \).

#### 4.2.2 Strongly observable feedback graphs on actions

When \(G_{[K]}\) is strongly observable, it is straightforward to generalize our upper (for self-avoiding contexts) and lower bounds in Theorem 1.1 and 1.3 with \(_{M}(G_{[K]})\) replaced by

\[_{M}(G_{[K]} G_{[M]})= _{c=1}^{M}|I_{c}|:I_{c}[K]\{c\},I_{c} I_{c^{}}c<c^{}}\]

and \(_{}(G_{[K]})\) in Theorem 3.4 by

\[_{}(G_{[K]} G_{[M]})=_{c=1}^{M}|B_{c} |:_{c}B_{c}G_{[K]} G_{[M]},\]

\[B_{c}(1+ K)V_{c} G_{[K]}\{c\}}\]

where the new graph quantities are defined on the product graph \(G_{[K]} G_{[M]}\). For general contexts, this gives a tight upper bound \((G_{[K]} G_{[M]})}\) when \(G_{[K]}\) and \(G_{[M]}\) are either both _undirected_ or both _transitively closed_7. Most generally, we have a loose upper bound \((G_{[K]} G_{[M]}),(G_ {[K]})M\}}\).

### Gap between upper and lower bounds

Although we provide tight upper and lower bounds for specific classes of context sequences (self-avoiding in Theorem 1.3) or feedback graphs (undirected or transitively closed in Corollary 1.5), in general the quantities \(_{M}(G)\) in Theorem 1.1 and \(\{_{M}(G),(G)\}\) in Theorem 1.4 exhibit a gap. The following lemma gives an upper bound on this gap.

**Lemma 4.1**.: _For any graph \(G\), it holds that_

\[_{M}(G)\{_{M}(G),(G)\}\{ ,1\}_{M}(G),\]

_where \((G)\) denotes the length of the longest path in \(G\)._

Lemma 4.1 shows that if \(G\) does not contain long paths or \(M\) is large, the gap between \(_{M}(G)\) and \(_{M}(G)\) is not significant. We also comment on the challenge of closing this gap. First, we do not know a tight characterization of the minimax value of the sequential game II (cf. Definition 2), and the upper bound \(_{}(G,M)\) in Lemma 3.2 could be loose, as shown in the following example.

**Example 1**.: _Consider an acyclic graph \(G=(V,E)\) with \(KM\) vertices \(\{(i,j)\}_{i[K],j[M]}\) and edges \((i,j)(i^{},j^{})\) if either \(i<i^{}\) and \(j j^{}\), or \(i=i^{}\) and \(j<j^{}\), and all self-loops. By choosing \(B_{c}=V_{c}=\{(i,c):i[K]\}\) in the definition of \(_{}(G,M)\) in Lemma 3.2, it is clear that \(_{}(G,M)=KM\). However, we show that the minimax value is \(U_{2}^{*}(G,M)=K+M-1\). The lower bound follows from \(U_{2}^{*}(G,M) U_{1}^{*}(G,M)_{M}(G) K+M-1\), as \(I_{1}=\{(i,M):i[K]\}\) and \(I_{c}=\{(1,M+1-c)\}\) for \(2 c M\) satisfy the constraints in the definition of \(_{M}(G)\) in (2). For the upper bound, we consider the following strategy for the learner in the sequential game II: \(v_{t}=(i_{t},j_{t})\) is the smallest element (under the lexicographic order over pairs) in \(A_{c_{t}} N_{}(D_{t-1})\). To show why \(U_{2}^{*}(G,M) K+M-1\), let \(D_{c}\) be the final set of vertices chosen by the learner under context \(c\). By the lexicographic order and the structure of \(G\), each \(D_{c}\) can only consist of vertices in one column. Moreover, for different \(c c^{}\), the row indices of \(D_{c}\) must be entirely no smaller or entirely no larger than the row indices of \(D_{c^{}}\). These constraints ensure that \(_{c=1}^{M}|D_{c}| K+M-1\)._

_This example shows the importance of non-greedy approaches when choosing \(v_{t}\). In the special case where \(A_{c}=\{(i,c):i[K]\}\) is the \(c\)-th column, within \(A_{c}\) this is an independent set, so any greedy approach that does not look outside \(A_{c}\) will treat the vertices in \(A_{c}\) indifferently. In contrast, the above approach makes use of the global structure of the graph \(G\)._

The second challenge lies in the proof of the lower bound. Instead of the sequential game where the adversary and the learner take turns to play actions, the current lower bound argument assumes that the adversary tells all his plays to the learner ahead of time. We expect the sequential structure to be equally important for the lower bounds, and it is interesting to work out an argument for the minimax lower bound to arrive at a sequential quantity like \(U_{2}^{*}(G,M)\).

### Other open problems

Performance of the UCB algorithm.The UCB algorithm under feedback graphs has been analyzed for both multi-armed (Lykouris et al., 2020) and contextual bandits (Dann et al., 2020). However, both results only show a regret upper bound \(((G)T})\), even in the case of multi-armed bandits (i.e. \(M=1\)). It is interesting to understand for algorithms without forced exploration (such as UCB), if the MAS number \((G)\) (rather than \((G)\) or \(_{M}(G)\)) turns out to be fundamental.

Regret for small \(T\).Note that our upper bounds hold for all values of \(T\), but our lower bound requires \(T_{M}(G)^{3}\). This is not an artifact of the analysis, as the optimal regret becomes fundamentally different for smaller \(T\). The case of multi-armed bandits has been solved completely in a recent work (Kocak and Carpentier, 2023), where the regret is a mixture of \(\) and \(T^{2/3}\) terms. We anticipate the same behavior for contextual bandits, but the exact form is unknown.

Stochastic contexts.In this paper we assume that the contexts are generated adversarially, but the case of stochastic contexts also draws some recent attention (Balseiro et al., 2019; Schneider and Zimmer, 2024), and sometimes there is a fundamental gap between the optimal performances under stochastic and adversarial contexts (Han et al., 2024). It is an interesting question whether this is the case for contextual bandits with graph feedback.