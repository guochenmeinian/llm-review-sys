# Prior-itizing Privacy: A Bayesian Approach to Setting the Privacy Budget in Differential Privacy

Zeki Kazan

Department of Statistical Science

Duke University

Durham, NC 27708

zekican.kazan@duke.edu

&Jerome P. Reiter

Department of Statistical Science

Duke University

Durham, NC 27708

jreiter@duke.edu

###### Abstract

When releasing outputs from confidential data, agencies need to balance the analytical usefulness of the released data with the obligation to protect data subjects' confidentiality. For releases satisfying differential privacy, this balance is reflected by the privacy budget, \(\). We provide a framework for setting \(\) based on its relationship with Bayesian posterior probabilities of disclosure. The agency responsible for the data release decides how much posterior risk it is willing to accept at various levels of prior risk, which implies a unique \(\). Agencies can evaluate different risk profiles to determine one that leads to an acceptable trade-off in risk and utility.

## 1 Introduction

Differential privacy (DP)  is a gold standard definition of what it means for data curators, henceforth referred to as agencies, to protect individuals' confidentiality when releasing sensitive data. The confidentiality guarantee of DP is determined principally by a parameter typically referred to as the privacy budget \(\). Smaller values of \(\) generally imply greater confidentiality protection at the cost of injecting more noise into the released data. Thus, agencies must choose \(\) to balance confidentiality protection with analytical usefulness. This balancing act has resulted in a wide range of values of \(\) in practice. For example, early advice in the field recommends \(\) of "0.01, 0.1, or in some cases, \((2)\) or \((3)\)" , whereas recent large-scale implementations use \(=8.6\) in OnTheMap , \(=14\) in Apple's use of local DP for iOS 10.1.1 , and an equivalent of \(=17.14\) in the 2020 decennial census redistricting data release .

Some decision makers may find interpreting and selecting \(\) difficult . For example, a recent study of practitioners using DP Creator  found that users wished for more explanation about how to select privacy parameters and better understanding of the effects of this choice. One potential path for providing guidance is to convert the task of selecting \(\) to setting bounds on the allowable probabilities of adversaries learning sensitive information from the released data, as in the classical disclosure limitation literature . There is precedent for this approach: prior work in the literature suggests that "privacy semantics in terms of Bayesian inference can shed more light on a privacy definition than privacy semantics in terms of indistinguishable pairs of neighboring databases"  and prevailing advice for setting \(\) in \((,)\)-DP originates from such Bayesian semantics .

We propose that agencies utilize relationships between DP and Bayesian semantics  to select values of \(\) that accord with their desired confidentiality guarantees. The basic idea is as follows. First, the agency constructs a function that summarizes the maximum posterior probability of disclosure permitted for any prior probability of disclosure. For example, for a prior risk of 0.001, the agency may be comfortable with a ten-fold (or more) increase in the ratio of posterior risk to prior risk, whereas for a prior risk of 0.4, the agency may require the increase not exceed, say, 1.2. Second, for each prior risk value, the agency converts the posterior-to-prior ratio into the largest\(\) that still ensures the ratio is satisfied. Third, the agency selects the smallest \(\) among these values, using that value for the data release. Importantly, the agency does not use the confidential data in these computations--they are theoretical and data free--so that they do not use up part of the overall privacy budget. Our main contributions include:

* We propose a framework for selecting \(\) under certain conditions (see Section 3) that applies to any DP mechanism, does not use additional privacy budget, and can account for disclosure risk from both an individual's inclusion and the sensitivity of values in the data.
* We enable agencies to tune the choice of \(\) to achieve their desired posterior-to-prior risk profile. This can avoid setting \(\) unnecessarily small if, for example, the agency tolerates larger posterior-to-prior ratios for certain prior risks. In turn, this can help agencies better manage trade-offs in disclosure risk and data utility.
* We give theoretic justification for the framework and derive closed-form solutions for the \(\) implied by several risk profiles. For more complex risk profiles, we also provide a general form for \(\) as a minimization problem.

To streamline the discussion, we focus on the release of discrete-valued statistics computed on discrete-valued data. Extension to continuous-valued statistics and data can be accomplished by replacing sums with integrals and PMFs with PDFs throughout the theorem statements and proofs.

## 2 Background and Motivation

We first describe some aspects of DP and Bayesian probabilities of disclosure relevant for our approach. We summarize all notation and definitions in Tables 2 and 3 in Appendix A.1.

### Differential Privacy

Let \(\) represent a population of individuals. The agency has a subset of \(\), which we call \(\), comprising \(n\) individuals measured on \(d\) variables. For any individual \(i\), let \(Y_{i}\) be the length-\(d\) vector of values corresponding to individual \(i\), and let \(I_{i}=1\) when individual \(i\) is in \(\) and \(I_{i}=0\) otherwise. For all \(i\) such that \(I_{i}=1\), let \(_{-i}\) be the \((n-1) d\) matrix of values for the \(n-1\) individuals in \(\) excluding individual \(i\).1 The agency wishes to release some function of the data, \(T()\). We assume \(\) and \(T()\) each have discrete support but may be many-dimensional. The agency turns to DP and will release \(T^{*}()\), a noisy version of \(T()\) under \(\)-DP.

Our work focuses on unbounded DP as defined in , which involves the removal or addition of one individual's information.2 If a function \(T^{*}()\) with discrete support satisfies unbounded \(\)-DP, then for all \(i\), all \(y\) in the support of \(Y_{i}\), all \(_{-i}\) in the support of \(_{-i}\), and all \(t^{*}\) in the support of \(T^{*}()\),

\[e^{-}()=t^{*} Y_{i}=y,I_{i}=1, _{-i}=_{-i}]}{P[T^{*}(_{-i})=t^{*} I_{i}=0, _{-i}=_{-i}]} e^{}.\] (1)

In settings where the statistic of interest is a count, a commonly used algorithm to satisfy DP is the geometric mechanism .

**Definition 1** (Geometric Mechanism).: _Let \(T()\) be a count statistic and suppose we wish to release a noisy count \(T^{*}()\) satisfying \(\)-DP. The geometric mechanism produces a count centered at \(T()\) with noise from a two-sided geometric distribution with parameter \(e^{-}\). That is,_

\[P[T^{*}()=t^{*} T()=t]=}{1+e ^{-}}e^{- t^{*}-t}, t^{*}.\] (2)

Under the geometric mechanism, the variance of \(T^{*}()\) is \(2e^{-}/(1-e^{-})^{2}\). The variance increases as \(\) decreases, reflecting the potential loss in data usefulness when setting \(\) to a small value.

Of course, data usefulness is only one side of the coin. Agencies also need to assess the implications of the choice of \(\) for disclosure risks [7; 42; 43]. Prior works on setting \(\) focus on settings where (i)the data have yet to be collected, and the goal is to simultaneously select \(\) and determine how much to compensate individuals for their loss in privacy [6; 15; 23; 30], (ii) the population is already public information, and the goal is to protect which subset of individuals is included in a release [29; 35], or (iii) data holders can utilize representative test data or the confidential data set itself [4; 17; 20; 35]. We focus on the common setting where data already have been collected, the population they are drawn from is not public information, and no data are available for tuning \(\).

### Bayesian Measures of Disclosure Risk

Consider an adversary who desires to learn about some particular individual \(i\) in \(\) using the release of \(T^{*}()\). We suppose that the adversary has a model, \(\), for making predictions about the components of \(Y_{i}\) that they do not already know.3 For example, the adversary could know some demographic information about individual \(i\) but not some other sensitive variable. The adversary might predict this sensitive variable from the demographic information using a model estimated with proprietary information or data from sources like administrative records. We assume that the release mechanism for \(T^{*}()\) is known to the adversary, that the DP release mechanism does not depend on \(\), and that, under \(\), the observations are independent but not necessarily identically distributed. These conditions are formalized in Section 3. For our ultimate purpose, i.e., helping agencies set \(\), the exact form of the adversary's \(\) is immaterial. In fact, as we shall discuss, we are not concerned whether the adversary's predictions from \(\) are highly accurate or completely awful.

On a technical note, we make the distinction that the agency views \(\) and \(I_{i}\) as fixed quantities, since it knows which rows are in the collected data and what values are associated to each row. The adversary, however, views \(\) and \(I_{i}\) as random variables, and thus probabilistic statements about these quantities are well defined from the adversary's perspective. Notationally, we signify that a probabilistic statement is from the adversary's perspective via the subscript \(\).

Let \(\) be the subset of the support of \(Y_{i}\) that the agency considers a privacy violation. For example, if \(d=1\) and \(\) is income data, then \(\) may be the set of possible incomes within \(5,\!000\) or within \(5\%\) of the true income for individual \(i\). Alternatively, if \(d=1\) and \(\) is binary, then \(\) is a subset of \(\{0,1\}\). The selection of \(\) must not depend on \(\), as this might constitute a privacy violation.

The agency may be concerned about the risk that the adversary determines individual \(i\) is in \(\) or the risk that the adversary makes a disclosure for individual \(i\); that is, \(I_{i}=1\) and \(Y_{i}\), respectively. Assuming that the adversary's model puts nonzero probability mass on these events, we can express their relevant prior probabilities as follows.

\[P_{}[I_{i}=1]=p_{i}, P_{}[Y_{i} I _{i}=1]=q_{i}.\] (3)

For fixed \(p_{i}\) and \(q_{i}\), we can measure the risk of disclosure for individual \(i\) in a number of ways. Drawing from , one measure is the relative disclosure risk, \(r_{i}(p_{i},q_{i},t^{*})\). Writing the noisy statistic as \(T^{*}\) and suppressing the dependence on \(\) or \(_{-i}\), this is defined as follows.

**Definition 2** (Relative Disclosure Risk).: _For fixed data \(\), individual \(i\), adversary's model \(\), and released \(T^{*}=t^{*}\), the relative disclosure risk is the posterior-to-prior risk ratio,_

\[r_{i}(p_{i},q_{i},t^{*})=}[Y_{i},I_{i}=1  T^{*}=t^{*}]}{P_{}[Y_{i},I_{i}=1]}.\] (4)

The relative risk can be decomposed into the posterior-to-prior ratio from inclusion (\(I_{i}\)) and the posterior-to-prior ratio from the values (\(Y_{i}\)). We have

\[r_{i}(p_{i},q_{i},t^{*})=}[Y_{i} I_{i}=1,T^{*}=t^{*}]}{P_{}[Y_{i} I_{i}=1]}}[I_{i}=1 T^{*}=t^{*}]}{P_{}[I_{i}=1]}.\] (5)

The relative risk, however, does not tell the full story. As discussed in , the data holder also may care about absolute disclosure risks, \(a_{i}(p_{i},q_{i},t^{*})\).

**Definition 3** (Absolute Disclosure Risk).: _For fixed data \(\), individual \(i\), adversary's model \(\), and released \(T^{*}=t^{*}\), the absolute disclosure risk is the posterior probability,_

\[a_{i}(p_{i},q_{i},t^{*})=P_{}[Y_{i},I_{i}=1 T^{*} =t^{*}].\] (6)

Since \(r_{i}(p_{i},q_{i},t^{*})=a_{i}(p_{i},q_{i},t^{*})/(p_{i}q_{i})\), we can convert between these risk measures.

### Risk Profiles

The quantities from Section 2.2 can inform the choice of \(\). For example, DP implies that

\[r_{i}(p_{i},q_{i},t^{*}) e^{2}\] (7)

for all \((p_{i},q_{i},t^{*})\). 4 The inequality in (7) implies a naive strategy for setting \(\): select a desired bound on the relative risk, \(r^{*}\), and set \(=(r^{*})/2\). Practically, however, this strategy suffers from two drawbacks that could result in a smaller recommended \(\) than necessary. First, for any particular \(p_{i}\) and \(q_{i}\), the bound in (7) need not be tight. In fact, this bound is actually quite loose across a wide range of values. Second, this strategy does not account for agencies willing to tolerate different relative risks for different prior probabilities. For example, if \(p_{i}q_{i}=0.25\), an agency may wish to limit the adversary's posterior to \(a_{i}(p_{i},q_{i},t^{*}) 2 0.25=0.5\), but for \(p_{i}q_{i}=10^{-6}\), the same agency may find a limit of \(a_{i}(p_{i},q_{i},t^{*}) 2 10^{-6}\) unnecessarily restrictive.

Rather than restricting to a constant bound, the agency can consider tolerable relative risks as a function of a hypothetical adversary's prior probabilities. We refer to this function as the agency's risk profile, denoted \(r^{*}(p_{i},q_{i})\). Thus, the agency establishes a \(r^{*}(p_{i},q_{i})\) so that, for all \(p_{i}\), \(q_{i}\), and \(t^{*}\),

\[r_{i}(p_{i},q_{i},t^{*}) r^{*}(p_{i},q_{i}).\] (8)

As the following results show, the requirement in (8) translates to a maximum value of \(\).

## 3 Theoretical Results

In this section, we describe our main theoretical results. To devote more space to examples and applications, we describe supporting results in Appendix B.1 and provide all proofs in Appendix E.

We begin by formalizing the assumptions on the release mechanism for \(T^{*}\) and the adversary's model, \(\). We emphasize that \(P[T^{*}()=t^{*} Y_{i}=y,_{-i}=_{-i},I_{i}=1]\) and \(P[T^{*}(_{-i})=t^{*}_{-i}=_{-i},I_{i}=0]\) are probabilities under the actual DP mechanism used for the release of \(T^{*}()\) when \(i\) is and is not included, respectively. We use these probabilities in two assumptions as follows.

**Assumption 1**.: _For all \(y\) in the support of \(Y_{i}\), \(_{-i}\) in the support of \(_{-i}\), and \(t^{*}\) in the support of \(T^{*}\),_

\[P_{}[T^{*}()=t^{*} Y_{i}=y, _{-i}=_{-i},I_{i}=1]=P[T^{*}()=t^{*} Y_{i}= y,_{-i}=_{-i},I_{i}=1]\\ P_{}[T^{*}(_{-i})=t^{*}_{-i}=_{-i },I_{i}=0]=P[T^{*}(_{-i})=t^{*}_{-i}=_{-i},I _{i}=0].\]

Assumption 1 implies that the mechanism for releasing \(T^{*}\) given \(\) is known to the adversary and that the adversary uses the actual release mechanism. An adversary who uses something other than the actual release mechanism to compute probabilities is unlikely in general to find more success than the adversary who does. Hence, Assumption 1 assures that the agency's computations are principled and cover a type of "worst-case" analysis.

**Assumption 2**.: _For all \(y\) in the support of \(Y_{i}\) and \(_{-i}\) in the support of \(_{-i}\),_

\[P_{}[_{-i}=_{-i} I_{i}=1,Y_{i}=y]=P_{ }[_{-i}=_{-i} I_{i}=0].\] (9)

Assumption 2 implies that the adversary's beliefs about the distribution of \(_{-i}\) do not change whether individual \(i\) is included in the data or not, nor do they depend on individual \(i\)'s confidential values. This assumption is similar to one explored in , who consider the case where "the presence/absence/record-value of each individual is independent of the presence/absence/record-values of other individuals," although we do not require that (9) holds for all \(i\). Since DP is designed to capture the effect that a single atomic unit of data has on the output distribution, assuming this type of independence structure ensures one atomic unit is a single data point.5 We believe these two assumptions are weaker than those in related work on choosing \(\), which we discuss in Section 6.

Under Assumption 1 and Assumption 2, we can relate \(\) to the distribution of \(T^{*}\) unconditional on \(_{-i}\) via the following lemma. This result is similar, but not identical to, Theorem 6.1 in .

**Lemma 1**.: _Under Assumption 1 and Assumption 2, if the release of \(T^{*}=t^{*}\) satisfies \(\)-DP, then for any subset \(\) of the domain of \(Y_{i}\), we have_

\[e^{-}}[T^{*}=t^{*} Y_{i} ,I_{i}=1]}{P_{}[T^{*}=t^{*} I_{i}=0]} e^{ }, e^{-2}}[T^{*}=t^{*}  Y_{i},I_{i}=1]}{P_{}[T^{*}=t^{*} Y_{i} ,I_{i}=1]} e^{2}.\] (10)

We emphasize that Lemma 1 and all subsequent results require Assumption 2. Without it, generalizable knowledge from the release can lead to arbitrarily large relative risk, as demonstrated by examples in [25; 27].

For a given function \(r^{*}\) selected by the data holder, we can determine the \(\) that should be used for the release. This is due to the following result relating the relative risk to \(\).

**Theorem 1**.: _Under Assumption 1 and Assumption 2, if the release of \(T^{*}=t^{*}\) satisfies \(\)-DP, then_

\[r_{i}(p_{i},q_{i},t^{*})p_{i}+e^{-2}(1-q_{i})p_{ i}+e^{-}(1-p_{i})}.\] (11)

One can solve for \(e^{-}\) in (11) to determine the recommended \(\), which is given by Theorem 2.

**Theorem 2**.: _For any individual \(i\), fix the prior probabilities, \(p_{i}\) and \(q_{i}\), and a desired bound on the relative disclosure risk, \(r^{*}(p_{i},q_{i})\). Define \(_{i}(p_{i},q_{i})\) to be the function_

\[_{i}(p_{i},q_{i})=((1-q_{i})}{ )^{2}+4p_{i}(1-q_{i})((p_{i},q_{i })}-p_{i}q_{i})-(1-p_{i})}}),&0<q_{i}<1;\\ (}{(p_{i},1)}-p_{i}}),&q_{i}=1.\] (12)

_Under the conditions of Theorem 1, any statistic \(T^{*}=t^{*}\) released under \(\)-DP with \(_{i}(p_{i},q_{i})\) will satisfy \(r_{i}(p_{i},q_{i},t^{*}) r^{*}(p_{i},q_{i})\)._

By Theorem 2, to achieve \(r_{i}(p_{i},q_{i},t^{*}) r^{*}(p_{i},q_{i})\) for all \((p_{i},q_{i})\), the agency should set \(\) via the following minimization problem:

\[=_{p_{i},q_{i}(0,1]}_{i}(p_{i},q_{i}).\] (13)

Closed forms for the \(\) resulting from a few specific risk profiles are included in the appendix.

## 4 Using Posterior-to-prior Risks for Setting \(\)

In this section, we illustrate how an agency can use the results of Section 3 to select \(\). Notationally, for a given quantity \(x\), we use \(\) to indicate the agency has fixed \(x\) to a particular constant.

Given \(\), the agency must select a form for \(r^{*}(p_{i},q_{i})\). A default choice, equivalent to the naive strategy using (7) discussed above, is to set the bound to a constant \(>1\), i.e.,

\[r^{*}(p_{i},q_{i})=.\] (14)

As we prove in Theorem 3 in the appendix, the bound in (14) implies the agency should set \(=()/2\). While a constant bound on relative risk is simple, agencies that tolerate different risk profiles may be able to set \(\) to larger values, as we now illustrate.

Consider an agency that seeks to bound the relative risks for high prior probabilities and bound the absolute disclosure risk for low prior probabilities. For example, the agency may not want adversaries whose prior probabilities are low to use \(t^{*}\) to increase those probabilities beyond 0.10. Simultaneously, the agency may want to ensure adversaries with prior probabilities around, for example, 0.20 cannot use \(t^{*}\) to triple their posterior probability. Such an agency can specify a risk profile that requires either the relative risk be less than some \(>1\) or the absolute risk be less than some \(<1\), as we now illustrate via the following two examples.

The first example is a setting inspired by a case study in .

**Example 1**.: _A healthcare provider possesses a data set comprising demographic information about individuals diagnosed with a particular form of cancer in a region of interest. They plan to release the count of individuals diagnosed with this form of cancer in various demographic groups via the geometric mechanism, but are concerned this release, if insufficient noise is added, could reveal which individuals in the community have cancer. They wish to choose \(\) appropriately._

In this example, the primary concern is with respect to inclusion in the data set. That is, for a given individual \(i\), the adversary's prior probability \(p_{i}\) is the key quantity, whereas the \(q_{i}\) is not as important for any given \(\). The agency can fix some \((0,1]\), for example \(=1\) to focus on adversaries who already know individual \(i\)'s demographic information. To simplify the analysis, we set the risk profile to \(\) for adversaries with \( 1\), which indicates these adversaries are not considered. A reasonable risk profile for this agency then might be of the form, for some \(>1\) and \(<1\),6

\[r^{*}(p_{i},q_{i})=\{}{p_{i}}, \},&q_{i}=;\\ ,&q_{i}.=}{p _{i}},&q_{i}=0<p_{i}<}{ };\\ ,&q_{i}=}{ } p_{i} 1;\\ ,&q_{i}.\] (15)

An example visualization of this risk function is presented in the first column of Figure 1. The upper plot displays the risk profile as a function of \(p_{i}\) when \(q_{i}=\), and the lower plot displays the maximal \(\) for which the relative risk bound holds for each \(p_{i}\). We can derive a closed form for \(\) under this risk profile; see Table 4 in Appendix A.2 for a summary and Theorem 5 in Appendix B.1 for details.

Suppose the agency is generally willing to accept a maximum absolute disclosure risk of \(=0.25\) for adversaries with small prior probabilities. Table 1 presents the maximal \(\) which satisfies the desired risk profile for three such agencies. Agency A is risk averse, agency C is utility seeking, and agency

 Agency & \(\) & \(\) & \(\) Recommendation & Noise Std. Dev. & Prob. Exact \\  A & \(1.5\) & \(0.25\) & \(0.51\) & \(2.74\) & 25\% \\ B & \(3\) & \(0.25\) & \(1.30\) & \(1.02\) & 57\% \\ C & \(6\) & \(0.25\) & \(2.04\) & \(0.59\) & 77\% \\ 

Table 1: The \(\) recommended by our framework for three risk profiles of the form in (15). Also included are the standard deviations of the noise distribution and the probabilities that the exact value is released, i.e., the mass at zero, for geometric mechanisms that satisfy the corresponding \(\)-DP.

Figure 1: Each column corresponds to a particular hypothetical agency. The first row presents the agency’s risk profile and the second row presents the profile’s implied maximal allowable \(\) at each point on the curve. Agency 1’s risk profile is given by (15) with \(=0.1\), \(=1\), and \(=3\), while Agency 2’s risk profile is given by (16) with \(=0.1\), \(=0.05\), and \(=3\). For Agency 2, \(r^{*}\) and \(_{i}\) are very large for small \(q_{i}\); large values are truncated for readability.

B sits in between in terms of risk and utility. For adversaries with high prior probabilities, agencies A, B, and C bound the relative disclosure risk at \(=1.5\), \(=3\), and \(=6\), respectively. Figure 3 in Appendix C.1 plots the forms of each agency's risk profile. To provide intuition on the amount of noise implied by these \(\), in Table 1 we display the standard deviation of the noise distribution for each statistic under the geometric mechanism and the probability that the geometric mechanism will output the exact value of each statistic. The risk averse agency is recommended an \(\) that results in a release with a high standard deviation and fairly low probability of releasing the exact value of the statistic. The utility seeking agency is recommended an \(\) that results in a release with a fairly low standard deviation and high probability of releasing the exact value of the statistic.

The recommendations from these risk profiles, which are tailored to the setting and agency preferences, are higher than the recommendations from a corresponding simple risk profile of \(r^{*}(p_{i},q_{i})=\) for all prior probabilities. This simple profile yields \( 0.20\), \( 0.55\), and \( 0.90\) for \(=1.5\), \(=3\), and \(=6\), respectively, less than half the corresponding \(\) values in Table 1.

We now consider a second example, inspired by Example 16 in , that alters Example 1.

**Example 2**.: _A survey is performed on a sample of individuals in the region of interest. 5% of the region is surveyed, and respondents are asked whether they have this particular form of cancer along with a series of demographic questions. The agency plans to release the counts of surveyed individuals who have and do not have the cancer in various demographic groups via the geometric mechanism, but is concerned this release, if insufficient noise is added, could reveal which individuals in the community have cancer. The agency wishes to choose \(\) appropriately._

In this example, the primary concern is with respect to the values in the data set. For ease of notation, let \(Y_{i}\) be a \(d\)-vector of binary values, and let the first element, \(Y_{1i}\), be an indicator for whether individual \(i\) has this form of cancer. Set \(\) to be the subset of the support of \(Y_{i}\) for which individual \(i\) has the cancer, i.e., \(=\{y\{0,1\}^{d}:y_{1}=1\}\). For individual \(i\), the adversary's \(q_{i}\) is the key quantity, and their \(p_{i}\) is not of interest. The agency can fix some \((0,1]\); for example, if the agency is most concerned about adversaries whose only prior knowledge is that individual \(i\) is in the population, but not whether they were surveyed, they might set \(=0.05\). Alternatively, they might set \(=1\) to imply an adversary that knows a priori that individual \(i\) is included in \(\). A reasonable risk profile for this agency might be of the form, for some \(<1\) and \(>1\), 7

\[r^{*}(p_{i},q_{i})=\{}{q_{i}}, \},&p_{i}=;\\ ,&p_{i}.=}{q_{i}},&p_{i}=0<q_{i}<}{ };\\ ,&p_{i}=}{ } q_{i} 1;\\ ,&p_{i}.\] (16)

An example visualization of this risk function is presented in the second column of Figure 1. The upper plot displays the risk profile as a function of \(q_{i}\) when \(p_{i}=\), and the lower plot displays the maximal \(\) for which the relative risk bound holds for each \(p_{i}\). The agency can select the smallest \(\) on this curve for their release. We can derive a closed form the minimum; see Table 4 in Appendix A.2 for a summary and Theorem 4 in Appendix B.1 for details. Notably, it can be shown that the \(\) selected under these two profiles is bounded below by \(()/2\) and may be much larger. We provide further exploration and visualizations for this example in Appendix C.2.

As demonstrated by these examples, the gap between the baseline strategy of (14) and our recommendation can be large. To further illustrate, suppose an agency has a simple, "point" risk profile, where \(r^{*}(0.5,1)=r^{}\) for \(r^{}<2\) (and \(\) elsewhere). The baseline recommends \(=(r^{})/2<0.35\), while the recommendation from Theorem 2 can be shown to have the form \(=(r^{}/(2-r^{}))\), which diverges as \(r^{} 2\). Thus, the gap between the recommendations can be arbitrarily large. While this likely does not represent a realistic disclosure risk profile of a real-world agency, it is instructive. In general, the baseline's recommendation is smaller because it is enforcing a relative risk of \(r^{}\) for adversaries with small priors. If \(r^{}<2\), then an adversary with prior probability \(p_{i}q_{i}=0.001\) is restricted to a posterior probability at most 0.002, leading to a small \(\) recommendation. If adversaries with small priors are ignored or allowed to have large relative risks, the recommendation from our method will outperform the baseline, and possibly by a substantial amount.

A particular agency's risk profile may not be characterized by one of the forms described above. For example, an agency may be equally concerned about \(p_{i}\) and \(q_{i}\), rather than focusing on one; see Appendix C.3 for an example of this setting. Or, as argued in , in some settings, agencies might be most concerned about the difference between the posterior and prior probabilities (rather than their ratio).8 In such settings, it is straightforward to write down the corresponding risk function, \(r^{*}(p_{i},q_{i})\), and the optimal \(\) can be determined by numerically solving the minimization problem in (13).9

Regardless of the agency's desiderata for a risk profile, we recommend that they keep the following in mind when setting its functional form. First, for any region where \(r^{*}(p_{i},q_{i})>1/(p_{i}q_{i})\), the risk profile generates a bound on the posterior probability that exceeds \(1\). Of course, the posterior probabilities themselves cannot exceed \(1\); thus, in these regions, the risk profile effectively does not bound the posterior risk. For example, an agency that sets \(r^{*}(p_{i},1)=3\) in the region where \(p_{i} 1/3\) (as in the left column of Figure 1) implicitly is willing to accept an unbounded \(\) for prior probabilities \(p_{i} 1/3\). Second, when bounding the absolute disclosure risk below some \(\) in some region of \((p_{i},q_{i})\), the agency should require \(p_{i}q_{i}<\) in that region. When \(p_{i}q_{i}=\), the recommended \(=0\) since the data holder requires \(T^{*}\) to offer no information about \(Y_{i}\). This also suggests that an agency bounding absolute disclosure risk in a region of \((p_{i},q_{i})\) that sets \(\) close to some value of \(p_{i}q_{i}\) in the region is willing to accept only small \(\) values.

## 5 Managing the Trade-off in Privacy and Utility

Agencies can use the framework to manage trade-offs in disclosure risk and data utility. In particular, the agency can evaluate potential impacts of the DP algorithm using data quality metrics under different risk profiles, choosing an \(\) that leads to a satisfactory trade-off. We demonstrate this in the below example, using data on infant mortality in Durham county, N.C. We note that these data may not require the use of DP in reality, but they do allow us to illustrate the process with public data.

**Example 3**.: _Suppose that an agency in Durham county, N.C., wishes to release a differentially private version of the number of infant deaths the county experienced in the year 2020. The agency plans to use the geometric mechanism to release this value. Of particular interest is whether the county infant death rate meets the U. S. Department of Health and Human Services' Healthy People 2020 target of \(6.0\) or fewer deaths per \(1{,}000\) live births . The agency wishes to minimize the probability that they release a noisy count that changes whether or not the \(6.0\) target is met; their goal is to ensure this probability is below 10%. It is public information that there were \(4{,}012\) live births in Durham county in 2020 ._

The primary concern in Example 3 is with respect to inclusion in the data. Thus, the agency focuses on adversaries with \(q_{i}=1\) and varying \(p_{i}\). The agency is generally willing to incur large relative risks if \(p_{i}\) is small and small relative risks if \(p_{i}\) is large. Furthermore, due to possible policy ramifications of appearing not to meet the target rate, the agency is open to accepting greater privacy risks for greater accuracy in the released count, within reason as determined by agency decision makers.

The agency's privacy experts determine that they can characterize the agency's risk profiles reasonably well using the following general class of risk profiles.

\[r^{*}(p_{i},q_{i})=\{}{p_{i}}, \},&q_{i}=1;\\ ,&q_{i} 1.\] (17)

To allow for consideration of the effects on accuracy of different specifications of (17), the agency considers combinations of \(\{1.2,2,5\}\) and \(\{0.1,0.25,0.5\}\). They also examine \(=0\), which corresponds to a constant risk profile \(r^{*}(p_{i},q_{i})=\). The agency could consider other combinations of parameters or risk profiles classes should these not lead to a satisfactory trade-off in risk and utility.

Figure 2 summarizes an analysis of the risk/utility trade-offs. For all \(\), the recommended \(\) is larger when \(>0\) than under the corresponding constant risk profile. Similar increases in \(\) also are evident for the risk profiles of Section 4; see Appendix C. Naturally, increases in \(\) and \(\) decrease the RMSEs10 of the noisy counts due to larger \(\). The switching probabilities in the top panel do not use

the true count of infant deaths; rather, they consider hypothetical deviations of \(0.25\), \(0.5\), \(1\), and \(2\) in the true rate from the target rate of \(6.0\). In this way, the agency avoids additional privacy leakage. The probability differs substantially for each of the four deviations, with the deviation of \(0.25\) leading to the largest probability of a switch. To ensure the probability is below \(10\%\) for the deviation of \(0.25\), the agency would need to use the most aggressive of the risk profiles, which allows absolute disclosure risk of \(=0.5\) or relative disclosure risk of \(=5\). This leads to \( 2.20\) with RMSE \( 0.53\). If they are willing to allow the probability to exceed \(10\%\) or to place priority on deviations exceeding \(0.25\), the agency could select a less aggressive risk profile that yields a smaller \(\).

In actuality, there were \(6.2\) infant deaths per \(1{,}000\) live births in Durham County in 2020 . As the truth is close to the \(6.0\) target, the agency would need to use a large \(\) to achieve their goal.

## 6 Relationship to Prior Work

The most similar work we are aware of is Lee and Clifton 's "How Much is Enough? Choosing \(\) for Differential Privacy," which also uses a Bayesian approach to set \(\). The authors focus on settings where the population is public information and the adversary's goal is to determine which subset of individuals in the population was used for a differentially private release of a statistic. Their method for selecting \(\) is tailored to the setting where only disclosure of an individual's inclusion in the data is of concern and the values of the entire population can be used to inform the choice of \(\).  focuses on the setting where the Laplace mechanism is used and the population size is known; their proposal was extended to any DP mechanism by  and settings where the population size is unknown by . In Appendix D.1, we compare the recommended \(\) from the proposal in  and our framework in several examples. We find that the approach of , for particular statistics and populations, can recommend a larger \(\) than the one resulting from our approach. However, our framework may be used in settings where the values of the entire population are not public and settings where the privacy of the values in the release is of primary concern.

Figure 2: The top panel presents the probability that the differentially private algorithm switches whether Durham county’s released rate is above or below the 6.0 target—e.g., the added noise makes the released rate 6.5 but the actual rate is 5.5. Each color represents a different hypothetical absolute difference between the true and target rate. The middle panel presents RMSEs of the noisy count of infant deaths. The bottom panel presents the implied \(\). Each bar corresponds to a different risk profile of the form in (17).

Another series of related work involves using Bayesian semantics to communicate the meaning of \(\) to potential study participants. For an overview of recent work in this area, see , , and citations therein. We highlight an example in  (corrected in ), which considers an individual deciding whether or not to participate in a survey for which results will be released via DP with a particular \(\). The authors suggest that the individual consider a bound on a posterior-to-posterior ratio similar to the bound in (11) to make an informed decision about whether to participate in the survey. We describe the advice of  using our notation in Appendix D.2. While their and our bounds have similar expressions, the goal in  differs from ours. They use the bound to characterize the individual's disclosure risks for a fixed \(\), whereas we establish the agency's disclosure risk profile in order to set \(\).

There are a number of other works that examine Bayesian semantics of \(\)-DP, which we now briefly summarize. In their seminal work proposing DP,  showed that, under some conditions, bounded DP is equivalent to a bound on the relative risk when all but one data point is fixed.  showed that bounded DP implies a bound on the total variation distance between the posterior distribution with all data points included and the posterior distribution with one data point removed.  showed that both bounded and unbounded DP are equivalent to a bound on the posterior-to-prior odds ratio exactly when the presence/absence/record-value of each individual is independent of that of other individuals. Finally,  showed that both bounded and unbounded DP imply bounds on the ratio of the posterior distribution with all data points included to the posterior with one observation replaced with a draw from the posterior with their observation removed. These works do not discuss how these semantic characterizations of DP can be used to select \(\).

## 7 Commentary

In this article, we propose a framework for selecting \(\) for DP by establishing disclosure risk profiles. Essentially, we provide a method for agencies to trade the problem of selecting \(\) for a release for the problem of specifying their desired disclosure risk profile. This process involves focusing on particular classes of adversaries the agency is most concerned about--represented by the set \(\)--and tuning \(\) to ensure the risk from these adversaries is sufficiently low. We emphasize that, once applied, DP will protect against all attacks with the guarantee of DP, not just the attack used to tune \(\).

We primarily focus on the risk from one class of adversary attacks on one individual, assuming all individuals are exchangeable. If agencies assign different risk profiles for different classes of adversaries, they could repeat the analysis for each class For example, the agency might assign different risk profiles to adversaries targeting different characteristics, e.g., they may consider whether an individual has a disease more sensitive than their age. Similarly, if an agency does not treat individuals in the data as exchangeable--for example, if an agency seeks to ensure lesser disclosure risks for groups with some characteristics than for not-necessarily-disjoint groups without those characteristics --the agency could repeat this analysis for each group. In both of these settings, the agency has a decision problem on its hands. As with designing DP solutions in general, the agency must prioritize some risks over others, e.g., using decision-theoretic criteria. This is an important topic for future work and raises ethical questions regarding how assigning different risk profiles for individuals with different characteristics could affect data equity.

One avenue for future work involves incorporating a version of this framework into differentially private data analysis tools. Recently developed interfaces for DP data releases such as  and  use the framework of  to guide users in setting \(\). Our framework could be similarly incorporated to provide guidance in settings that do not satisfy the assumption of  that the values of the population are public. Another avenue for future work involves examining how an agency can set the risk profile appropriate for a particular release. We envision agencies could do so analogously to the elicitation of utility functions in decision theory, e.g., by considering a series of bets . Other future extensions involve examining whether similar results follow under weaker assumptions. This includes settings with multiple differentially private releases via existing results that relate the relative risk to DP composition theorems (e.g., S5 of ). This also includes extension to variants of DP--such as zero-concentrated DP  or approximate DP --by deriving Bayesian semantics analogous to Theorem 1 or leveraging existing results [25; 27]. Additionally, it may be possible to extend the results in this article from posterior-to-prior risks to the sorts of posterior-to-posterior risks discussed in Section 6.