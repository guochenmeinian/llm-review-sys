# Recommender Systems with Generative Retrieval

Shashank Rajput

University of Wisconsin-Madison

&Nikhil Mehta

Google DeepMind

&Anima Singh

Google DeepMind

&Raghunandan Keshavan

Google

&Trung Vu

Google

&Lukasz Heldt

Google

&Lichan Hong

Google DeepMind

&Yi Tay

Google DeepMind

&Vinh Q. Tran

Google

&Jonah Samost

Google

&Maciej Kula

Google DeepMind

&Ed H. Chi

Google DeepMind

&Maheswaran Sathiamoorthy

Google DeepMind

###### Abstract

Modern recommender systems perform large-scale retrieval by embedding queries and item candidates in the same unified space, followed by approximate nearest neighbor search to select top candidates given a query embedding. In this paper, we propose a novel generative retrieval approach, where the retrieval model autoregressively decodes the identifiers of the target candidates. To that end, we create semantically meaningful tuple of codewords to serve as a Semantic ID for each item. Given Semantic IDs for items in a user session, a Transformer-based sequence-to-sequence model is trained to predict the Semantic ID of the next item that the user will interact with. We show that recommender systems trained with the proposed paradigm significantly outperform the current SOTA models on various datasets. In addition, we show that incorporating Semantic IDs into the sequence-to-sequence model enhances its ability to generalize, as evidenced by the improved retrieval performance observed for items with no prior interaction history.

## 1 Introduction

Recommender systems help users discover content of interest and are ubiquitous in various recommendation domains such as videos , apps , products , and music . Modern recommender systems adopt a retrieve-and-rank strategy, where a set of viable candidates are selected in the retrieval stage, which are then ranked using a ranker model. Since the ranker model works only on the candidates it receives, it is desired that the retrieval stage emits highly relevant candidates.

Figure 1: Overview of the _Transformer Index for GEnerative Recommenders_ (TIGER) framework. With TIGER, sequential recommendation is expressed as a generative retrieval task by representing each item as a tuple of discrete semantic tokens.

There are standard and well-established methods for building retrieval models. Matrix factorization  learns query and candidate embeddings in the same space. In order to better capture the non-linearities in the data, dual-encoder architectures  (i.e., one tower for the query and another for the candidate) employing inner-product to embed the query and candidate embeddings in the same space have become popular in recent years. To use these models during inference, an index that stores the embeddings for all items is created using the candidate tower. For a given query, its embedding is obtained using the query tower, and an Approximate Nearest Neighbors (ANN) algorithm is used for retrieval. In recent years, the dual encoders architectures have also been extended for sequential recommendations [11; 24; 41; 17; 32; 6; 44] that explicitly take into account the order of user-item interactions.

We propose a new paradigm of building generative retrieval models for sequential recommendation. Instead of traditional query-candidate matching approaches, our method uses an end-to-end generative model that predicts the candidate IDs directly. We propose to leverage the Transformer  memory (parameters) as an end-to-end index for retrieval in recommendation systems, reminiscent of Tay et al.  that used Transformer memory for document retrieval. We refer to our method as _Transformer Index for GEnerative Recommenders_ (TIGER). A high-level overview of TIGER is shown in Figure 1. TIGER is uniquely characterized by a novel semantic representation of items called "Semantic ID" - a sequence of tokens derived from each item's content information. Concretely, given an item's text features, we use a pre-trained text encoder (e.g., SentenceT5 ) to generate dense content embeddings. A quantization scheme is then applied on the embedding of an item to form a set of ordered tokens/codewords, which we refer to as the Semantic ID of the item. Ultimately, these Semantic IDs are used to train the Transformer model on the sequential recommendation task.

Representing items as a sequence of semantic tokens has many advantages. Training the transformer memory on semantically meaningful data allows knowledge sharing across similar items. This allows us to dispense away with the atomic and random item IDs that have been previously used [33; 42; 11; 8] as item features in recommendation models. With semantic token representations for items, the model is less prone to the inherent feedback loop [1; 26; 39] in recommendation systems, allowing the model to generalize to newly added items to the corpus. Furthermore, using a sequence of tokens for item representation helps alleviate the challenges associated with the scale of the item corpus; the number of items that can be represented using tokens is the product of the cardinality of each token in the sequence. Typically, the item corpus size can be in the order of billions and learning a unique embedding for each item can be memory-intensive. While random hashing-based techniques  can be adopted to reduce the item representation space, in this work, we show that using semantically meaningful tokens for item representation is an appealing alternative. The main contributions of this work are summarized below:

1. We propose TIGER, a novel generative retrieval-based recommendation framework that assigns Semantic IDs to each item, and trains a retrieval model to predict the Semantic ID of an item that a given user may engage with.
2. We show that TIGER outperforms existing SOTA recommender systems on multiple datasets as measured by recall and NDCG metrics.
3. We find that this new paradigm of generative retrieval leads to two additional capabilities in sequential recommender systems: 1. Ability to recommend new and infrequent items, thus improving cold-start recommendations, and 2. Ability to generate diverse recommendations using a tunable parameter.

Paper Overview.In Section 2, we provide a brief literature survey of recommender systems, generative retrieval, and the Semantic ID generation techniques we use in this paper. In Section 3, we explain our proposed framework, and outline the various techniques we use for Semantic ID generation. We present the result of our experiments in Section 4, and conclude the paper in Section 5.

## 2 Related Work

Sequential Recommenders.Using deep sequential models in recommender systems has developed into a rich literature. GRU4REC  was the first to use GRU based RNNs for sequential recommendations. Li et al.  proposed Neural Attentive Session-based Recommendation (NARM), where an attention mechanism along with a GRU layer is used to track long term intent of the user.

AttRec  proposed by Zhang et al. used self-attention mechanism to model the user's intent in the current session, and personalization is ensured by modeling user-item affinity with metric learning. Concurrently, Kang et al. also proposed SASRec , which used self-attention similar to decoder-only transformer models. Inspired by the success of masked language modeling in language tasks, BERT4Rec  and Transformers4Rec  utilize transformer models with masking strategies for sequential recommendation tasks. S\({}^{3}\)-Rec  goes beyond just masking by pre-training on four self-supervised tasks to improve data representation. The models described above learn a high-dimensional embedding for each item and perform an ANN in a Maximum Inner Product Search (MIPS) space to predict the next item. In contrast, our proposed technique, TIGER, uses Generative Retrieval to directly predict the Semantic ID of the next item.

P5  fine-tunes a pre-trained large language models for _multi-task_ recommender systems. The P5 model relies on the LLM tokenizer (SentencePiece tokenizer ) to generate tokens from randomly-assigned item IDs. Whereas, we use Semantic ID representation of items may are learned based on the content information of the items. In our experiments (Table 2), we demonstrate that recommendation systems based on Semantic ID representation of items yield much better results than using random codes.

Semantic IDs.Hou _et al._ proposed VQ-Rec  to generate "codes" (analogous to Semantic IDs) using content information for item representation. However, their focus is on building transferable recommender systems, and do not use the codes in a generative manner for retrieval. While they also use product quantization  to generate the codes, we use RQ-VAE to generate Semantic IDs, which leads to hierarchical representation of items (Section 4.2). In a concurrent work to us, Singh et al.  show that hierarchical Semantic IDs can be used to replace item IDs for ranking models in large scale recommender systems improves model generalization.

Generative Retrieval.While techniques for learning search indices have been proposed in the past , generative retrieval is a recently developed approach for document retrieval, where the task is to return a set of relevant documents from a database. Some examples include GENRE , DSI , NCI , and CGR . A more detailed coverage of the related work is in Appendix A. To the best of our knowledge, we are the first to propose generative retrieval for recommendation systems using Semantic ID representation of items.

## 3 Proposed Framework

Our proposed framework consists of two stages:

1. _Semantic ID generation using content features_. This involves encoding the item content features to embedding vectors and quantizing the embedding into a tuple of semantic codewords. The resulting tuple of codewords is referred to as the item's Semantic ID.
2. _Training a generative recommender system on Semantic IDs._ A Transformer model is trained on the sequential recommendation task using sequences of Semantic IDs.

Figure 2: An overview of the modeling approach used in TIGER.

### Semantic ID Generation

In this section, we describe the Semantic ID generation process for the items in the recommendation corpus. We assume that each item has associated content features that capture useful semantic information (_e.g._ titles or descriptions or images). Moreover, we assume that we have access to a pre-trained content encoder to generate a semantic embedding \(^{d}\). For example, general-purpose pre-trained text encoders such as Sentence-T5  and BERT  can be used to convert an item's text features to obtain a semantic embedding. The semantic embeddings are then quantized to generate a Semantic ID for each item. Figure 1(a) gives a high-level overview of the process.

We define a Semantic ID to be a tuple of codewords of length \(m\). Each codeword in the tuple comes from a different codebook. The number of items that the Semantic IDs can represent uniquely is thus equal to the product of the codebook sizes. While different techniques to generate Semantic IDs result in the IDs having different semantic properties, we want them to at least have the following property: _Similar items (items with similar content features or whose semantic embeddings are close) should have overlapping Semantic IDs._ For example, an item with Semantic ID \((10,21,35)\) should be more similar to one with Semantic ID \((10,21,40)\), than an item with ID \((10,23,32)\). Next, we discuss the quantization schemes which we use for Semantic ID generation.

**RQ-VAE for Semantic IDs.** Residual-Quantized Variational AutoEncoder (RQ-VAE)  is a multi-level vector quantizer that applies quantization on residuals to generate a tuple of codewords (aka Semantic IDs). The Autoencoder is jointly trained by updating the quantization codebook and the DNN encoder-decoder parameters. Fig. 3 illustrates the process of generating Semantic IDs through residual quantization.

RQ-VAE first encodes the input \(\) via an encoder \(\) to learn a latent representation \(:=()\). At the zero-th level (\(d=0\)), the initial residual is simply defined as \(_{0}:=\). At each level \(d\), we have a codebook \(_{d}:=\{_{k}\}_{k=1}^{K}\), where \(K\) is the codebook size. Then, \(_{0}\) is quantized by mapping it to the nearest embedding from that level's codebook. The index of the closest embedding \(_{c_{d}}\) at \(d=0\), i.e., \(c_{0}=_{i}|_{0}-_{k}||\), represents the zero-th codeword. For the next level \(d=1\), the residual is defined as \(_{1}:=_{0}-_{c_{0}}\). Then, similar to the zero-th level, the code for the first level is computed by finding the embedding in the codebook for the first level which is nearest to \(_{1}\). This process is repeated recursively \(m\) times to get a tuple of \(m\) codewords that represent the Semantic ID. This recursive approach approximates the input from a coarse-to-fine granularity. Note that we chose to use a separate codebook of size \(K\) for each of the \(m\) levels, instead of using a single, \(mK\)-sized codebook. This was done because the norm of residuals tends to decrease with increasing levels, hence allowing for different granularities for different levels.

Figure 3: RQ-VAE: In the figure, the vector output by the DNN Encoder, say \(_{0}\) (represented by the blue bar), is fed to the quantizer, which works iteratively. First, the closest vector to \(_{0}\) is found in the first level codebook. Let this closest vector be \(_{c_{0}}\) (represented by the red bar). Then, the residual error is computed as \(_{1}:=_{0}-_{c_{0}}\). This is fed into the second level of the quantizer, and the process is repeated: The closest vector to \(_{1}\) is found in the second level, say \(_{c_{1}}\) (represented by the green bar), and then the second level residual error is computed as \(_{2}=_{1}-^{}_{c_{1}}\). Then, the process is repeated for a third time on \(_{2}\). The semantic codes are computed as the indices of \(_{c_{0}},_{c_{1}}\), and \(_{c_{2}}\) in their respective codebooks. In the example shown in the figure, this results in the code \((7,1,4)\).

Once we have the Semantic ID \((c_{0},,c_{m-1})\), a quantized representation of \(\) is computed as \(}:=_{m=0}^{m-1}_{c_{i}}\). Then \(}\) is passed to the decoder, which tries to recreate the input \(\) using \(}\). The RQ-VAE loss is defined as \(():=_{}+_{}\), where \(_{}:=\|-}\|^{2}\), and \(_{}:=_{d=0}^{m-1}\|[_{i}]-_{c_ {i}}\|^{2}+\|_{i}-[_{c_{i}}]\|^{2}\). Here \(}\) is the output of the decoder, and sg is the stop-gradient operation . This loss jointly trains the encoder, decoder, and the codebook.

As proposed in , to prevent RQ-VAE from a codebook collapse, where most of the input gets mapped to only a few codebook vectors, we use k-means clustering-based initialization for the codebook. Specifically, we apply the k-means algorithm on the first training batch and use the centroids as initialization.

**Other alternatives for quantization.** A simple alternative to generating Semantic IDs is to use Locality Sensitive Hashing (LSH). We perform an ablation study in Subsection 4.2 where we find that RQ-VAE indeed works better than LSH. Another option is to use k-means clustering hierarchically , but it loses semantic meaning between different clusters . We also tried VQ-VAE, and while it performs similarly to RQ-VAE for generating the candidates during retrieval, it loses the hierarchical nature of the IDs which confers many new capabilities that are discussed in Section 4.3.

**Handling Collisions.** Depending on the distribution of semantic embeddings, the choice of codebook size, and the length of codewords, semantic collisions can occur (\(i.e.\), multiple items can map to the same Semantic ID). To remove the collisions, we append an extra token at the end of the ordered semantic codes to make them unique. For example, if two items share the Semantic ID \((12,24,52)\), we append additional tokens to differentiate them, representing the two items as \((12,24,52,0)\) and \((12,24,52,1)\). To detect collisions, we maintain a lookup table that maps Semantic IDs to corresponding items. Note that collision detection and fixing is done only once after the RQ-VAE model is trained. Furthermore, since Semantic IDs are integer tuples, the lookup table is efficient in terms of storage in comparison to high dimensional embeddings.

### Generative Retrieval with Semantic IDs

We construct item sequences for every user by sorting chronologically the items they have interacted with. Then, given a sequence of the form \((_{1},,_{n})\), the recommender system's task is to predict the next item \(_{n+1}\). We propose a generative approach that directly predicts the Semantic ID of the next item. Formally, let \((c_{i,0},,c_{i,m-1})\) be the \(m\)-length Semantic ID for \(_{i}\). Then, we convert the item sequence to the sequence \((c_{1,0},,c_{1,m-1},c_{2,0},,c_{2,m-1},,c_{n,0},,c_{n, m-1})\). The sequence-to-sequence model is then trained to predict the Semantic ID of \(_{n+1}\), which is \((c_{n+1,0},,c_{n+1,m-1})\). Given the generative nature of our framework, it is possible that a generated Semantic ID from the decoder does not match an item in the recommendation corpus. However, as we show in appendix (Fig. 6) the probability of such an event occurring is low. We further discuss how such events can be handled in appendix E.

## 4 Experiments

**Datasets.** We evaluate the proposed framework on three public real-world benchmarks from the Amazon Product Reviews dataset , containing user reviews and item metadata from May 1996 to July 2014. In particular, we use three categories of the Amazon Product Reviews dataset for the sequential recommendation task: "Beauty", "Sports and Outdoors", and "Toys and Games". We discuss the dataset statistics and pre-processing in Appendix C.

**Evaluation Metrics.** We use top-k Recall (Recall@K) and Normalized Discounted Cumulative Gain (NDCG@K) with \(K=5,10\) to evaluate the recommendation performance.

**RQ-VAE Implementation Details.** As discussed in section 3.1, RQ-VAE is used to quantize the semantic embedding of an item. We use the pre-trained Sentence-T5  model to obtain the semantic embedding of each item in the dataset. In particular, we use item's content features such as title, price, brand, and category to construct a sentence, which is then passed to the pre-trained Sentence-T5 model to obtain the item's semantic embedding of 768 dimension.

The RQ-VAE model consists of three components: a DNN encoder that encodes the input semantic embedding into a latent representation, residual quantizer which outputs a quantized representation, and a DNN decoder that decodes the quantized representation back to the semantic input embeddingspace. The encoder has three intermediate layers of size 512, 256 and 128 with ReLU activation, with a final latent representation dimension of \(32\). To quantize this representation, three levels of residual quantization is done. For each level, a codebook of cardinality \(256\) is maintained, where each vector in the codebook has a dimension of \(32\). When computing the total loss, we use \(=0.25\). The RQ-VAE model is trained for 20k epochs to ensure high codebook usage (\( 80\%\)). We use Adagrad optimizer with a learning rate of 0.4 and a batch size of 1024. Upon training, we use the learned encoder and the quantization component to generate a 3-tuple Semantic ID for each item. To avoid multiple items being mapped to the same Semantic ID, we add a unique \(4^{th}\) code for items that share the same first three codewords, \(i.e.\) two items associated with a tuple (7, 1, 4) are assigned (7, 1, 4, 0) and (7, 1, 4, 1) respectively (if there are no collisions, we still assign 0 as the fourth codeword). This results in a unique Semantic ID of length 4 for each item in the recommendation corpus.

**Sequence-to-Sequence Model Implementation Details.** We use the open-sourced T5X framework  to implement our transformer based encoder-decoder architecture. To allow the model to process the input for the sequential recommendation task, the vocabulary of the sequence-to-sequence model contains the tokens for each semantic codeword. In particular, the vocabulary contains 1024 (\(256 4\)) tokens to represent items in the corpus. In addition to the semantic codewords for items, we add user-specific tokens to the vocabulary. To keep the vocabulary size limited, we only add 2000 tokens for user IDs. We use the Hashing Trick  to map the raw user ID to one of the 2000 user ID tokens. We construct the input sequence as the user Id token followed by the sequence of Semantic ID tokens corresponding to a given user's item interaction history. We found that adding user ID to the input, allows the model to personalize the items retrieved.

We use 4 layers each for the transformer-based encoder and decoder models with 6 self-attention heads of dimension 64 in each layer. We used the ReLU activation function for all the layers. The MLP and the input dimension was set as 1024 and 128, respectively. We used a dropout of 0.1. Overall, the model has around 13 million parameters. We train this model for 200k steps for the "Beauty" and "Sports and Outdoors" dataset. Due to the smaller size of the "Toys and Games" dataset, it is trained only for 100k steps. We use a batch size of 256. The learning rate is \(0.01\) for the first 10k steps and then follows an inverse square root decay schedule.

### Performance on Sequential Recommendation

In this section, we compare our proposed framework for generative retrieval with the following sequential recommendation methods (which are described briefly in Appendix B): GRU4Rec , Caser , HGN , SASRec , BERT4Rec , FDSA , S\({}^{3}\)-Rec , and P5 . Notably

Figure 4: Qualitative study of RQ-VAE Semantic IDs (\(c_{1},c_{2},c_{3},c_{4}\)) on the Amazon Beauty dataset. We show that the ground-truth categories are distributed across different Semantic tokens. Moreover, the RQVAE semantic IDs form a hierarchy of items, where the first semantic token (\(c_{1}\)) corresponds to coarse-level category, while second/third semantic token (\(c_{2}\)/\(c_{3}\)) correspond to fine-grained categories.

[MISSING_PAGE_FAIL:7]

items have overlapping codewords, which allows the model to effectively share knowledge from semantically similar items in the dataset.

**Hashing vs. RQ-VAE Semantic IDs.** We study the importance of RQ-VAE in our framework by comparing RQ-VAE against Locality Sensitive Hashing (LSH) [14; 13; 2] for Semantic ID generation. LSH is a popular hashing technique that can be easily adapted to work for our setting. To generate LSH Semantic IDs, we use \(h\) random hyperplanes \(_{1},,_{h}\) to perform a random projection of the embedding vector \(\) and compute the following binary vector: \((1_{_{1}^{}>0},,1_{_{h}^{}>0})\). This vector is converted into an integer code as \(c_{0}=_{i=1}^{h}2^{i-1}1_{_{i}^{}>0}\). This process is repeated \(m\) times using an independent set of random hyperplanes, resulting in \(m\) codewords \((c_{0},c_{1},,c_{m-1})\), which we refer to as the LSH Semantic ID.

In Table 2, we compare the performance of LSH Semantic ID with our proposed RQ-VAE Semantic ID. In this experiment, for LSH Semantic IDs, we used \(h=8\) random hyperplanes and set \(m=4\) to ensure comparable cardinality with the RQ-VAE. The parameters for the hyperplanes are randomly sampled from a standard normal distribution, which ensures that the hyperplanes are spherically symmetric. Our results show that RQ-VAE consistently outperforms LSH. This illustrates that learning Semantic IDs via a non-linear, Deep Neural Network (DNN) architecture yields better quantization than using random projections, given the same content-based semantic embedding.

**Random ID vs. Semantic ID.** We also compare the importance of Semantic IDs in our generative retrieval recommender system. In particular, we compare randomly generated IDs with the Semantic IDs. To generate the Random ID baseline, we assign \(m\) random codewords to each item. A Random ID of length \(m\) for an item is simply \((c_{1},,c_{m})\), where \(c_{i}\) is sampled uniformly at random from \(\{1,2,,K\}\). We set \(m=4\), and \(K=255\) for the Random ID baseline to make the cardinality similar to RQ-VAE Semantic IDs. A comparison of Random ID against RQ-VAE and LSH Semantic IDs is shown in Table 2. We see that Semantic IDs consistently outperform Random ID baseline, highlighting the importance of leveraging content-based semantic information.

### New Capabilities

We describe two new capabilities that directly follow from our proposed generative retrieval framework, namely cold-start recommendations and recommendation diversity. We refer to these capabilities as "new" since existing sequential recommendation models (See the baselines in section 4.1) cannot be directly used to satisfy these real-world use cases. These capabilities result from a synergy between RQ-VAE based Semantic IDs and the generative retrieval approach of our framework. We discuss how TIGER is used in these settings in the following sections.

    &  &  &  \\   & Recall & NDCG & Recall & NDCG & Recall & NDCG & Recall & NDCG & Recall & NDCG \\  & @5 & @5 & @10 & @10 & @5 & @5 & @10 & @10 & @5 & @5 & @10 & @10 \\  Random ID & 0.007 & 0.005 & 0.0116 & 0.0063 & 0.0296 & 0.0205 & 0.0434 & 0.0250 & 0.0362 & 0.0270 & 0.0448 & 0.0298 \\ LSH SID & 0.0215 & 0.0146 & 0.0321 & 0.0180 & 0.0379 & 0.0259 & 0.0533 & 0.0309 & 0.0412 & 0.0299 & 0.0566 & 0.0349 \\ RQ-VAE SID & **0.0264** & **0.0181** & **0.0400** & **0.0225** & **0.0454** & **0.0321** & **0.0648** & **0.0384** & **0.0521** & **0.0371** & **0.0712** & **0.0432** \\   

Table 2: Ablation study for different ID generation techniques for generative retrieval. We show that RQ-VAE Semantic ID (SID) perform significantly better compared to hashing SIDs and Random IDs.

Figure 5: Performance in the cold-start retrieval setting.

**Cold-Start Recommendation.** In this section, we study the cold-start recommendation capability of our proposed framework. Due to the fast-changing nature of the real-world recommendation corpus, new items are constantly introduced. Since newly added items lack user impressions in the training corpus, existing recommendation models that use a random atomic ID for item representation fail to retrieve new items as potential candidates. In contrast, the TIGER framework can easily perform cold-start recommendations since it leverages item semantics when predicting the next item.

For this analysis, we consider the Beauty dataset from Amazon Reviews. To simulate newly added items, we remove 5% of test items from the training data split. We refer to these removed items as _unseen items_. Removing the items from the training split ensures there is no data leakage with respect to the unseen items. As before, we use Semantic ID of length 4 to represent the items, where the first 3 tokens are generated using RQ-VAE and the \(4^{th}\) token is used to ensure a unique ID exists for all the seen items. We train the RQ-VAE quantizer and the sequence-to-sequence model on the training split. Once trained, we use the RQ-VAE model to generate the Semantic IDs for all the items in the dataset, including any unseen items in the item corpus.

Given a Semantic ID \((c_{1},c_{2},c_{3},c_{4})\) predicted by the model, we retrieve the seen item having the same corresponding ID. Note that by definition, each Semantic ID predicted by the model can match at most one item in the training dataset. Additionally, unseen items having the same first three semantic tokens, \(i.e.\)\((c_{1},c_{2},c_{3})\) are included to the list of retrieved candidates. Finally, when retrieving a set of top-K candidates, we introduce a hyperparameter \(\) which specifies the maximum proportion of unseen items chosen by our framework.

We compare the performance of TIGER with the k-Nearest Neighbors (KNN) approach on the cold-start recommendation setting in Fig. 5. For KNN, we use the semantic representation space to perform the nearest-neighbor search. We refer to the KNN-based baseline as Semantic_KNN. Fig. 4(a) shows that our framework with \(=0.1\) consistently outperforms Semantic_KNN for all Recall@K metrics. In Fig. 4(b), we provide a comparison between our method and Semantic_KNN for various values of \(\). For all settings of \( 0.1\), our method outperforms the baseline.

**Recommendation diversity.** While Recall and NDCG are the primary metrics used to evaluate a recommendation system, diversity of predictions is another critical objective of interest. A recommender system with poor diversity can be detrimental to the long-term engagement of users. Here, we discuss how our generative retrieval framework can be used to predict diverse items. We show that temperature-based sampling during the decoding process can be effectively used to control the diversity of model predictions. While temperature-based sampling can be applied to any existing recommendation model, TIGER allows sampling across various levels of hierarchy owing to the properties of RQ-VAE Semantic IDs. For instance, sampling the first token of the Semantic ID allows retrieving items from coarse-level categories, while sampling a token from second/third token allows sampling items within a category.

We quantitatively measure the diversity of predictions using Entropy@K metric, where the entropy is calculated for the distribution of the ground-truth categories of the top-K items predicted by the model. We report the Entropy@K for various temperature values in Table 3. We observe that temperature-sampling in the decoding stage can be effectively used to increase the diversity in the ground-truth categories of the items. We also perform a qualitative analysis in Table 4.

   Target Category &  \\   &  & T = 2.0 \\  Hair Styling Products & Hair Styling Products & Hair Styling Tools, Skin Face & Tosh Nail & Tools Nail, Malebow Nails \\ Malebow Nails & Malebow Nails & Malebow Nails, Skin Tabards \& Nail, Tools Nail \\ Skin Eyes & Skin Spec & Hair Retakers, Skin Face, Hair Styling Products, Skin Eyes \\ Malebow Face & Tools Malebow Brushes,Malebow Face & Tools Malebow Brushes, Malebow Face, Skin Face, Malebow Sets, Hair Styling Tools \\ Hair Loss Products & Hair Loss Products, Skin Face, Skin Body & Skin Face, Hair Loss Products, Hair Stampenov,Hair \& Scably Treatments, Hair Conditioners \\   

Table 4: Recommendation diversity with temperature-based decoding.

   Temperature & Entropy@10 & Entropy@20 & Entropy@50 \\  T = 1.0 & 0.76 & 1.14 & 1.70 \\ T = 1.5 & 1.14 & 1.52 & 2.06 \\ T = 2.0 & 1.38 & 1.76 & 2.28 \\   

Table 3: The entropy of the category distribution predicted by the model for the Beauty dataset. A higher entropy corresponds more diverse items predicted by the model.

### Ablation Study

We measure the effect of varying the number of layers in the sequence-to-sequence model in Table 5. We see that the metrics improve slightly as we make the network bigger. We also measure the effect of providing user information, the results for which are provided in Table 8 in the Appendix.

### Invalid IDs

Since the model decodes the codewords of the target Semantic ID autoregressively, it is possible that the model may predict invalid IDs (i.e., IDs that do not map to any item in the recommendation dataset). In our experiments, we used semantic IDs of length \(4\) with each codeword having a cardinality of \(256\) (i.e., codebook size = 256 for each level). The number of possible IDs spanned by this combination \(=256^{4}\), which is approximately 4 trillion. On the other hand, the number of items in the datasets we consider is 10K-20K (See Table 6). Even though the number of valid IDs is only a fraction of all complete ID space, we observe that the model almost always predicts valid IDs. We visualize the fraction of invalid IDs produced by TIGER as a function of the number of retrieved items \(K\) in Figure 6. For top-10 predictions, the fraction of invalid IDs varies from \( 0.1\%-1.6\%\) for the three datasets. To counter the effect of invalid IDs and to always get top-10 valid IDs, we can increase the beam size and filter the invalid IDs.

It is important to note that, despite generating invalid IDs, TIGER achieves state-of-the-art performance when compared to other popular methods used for sequential recommendations. One extension to handle invalid tokens could be to do prefix matching when invalid tokens are generated by the model. Prefix matching of Semantic IDs would allow retrieving items that have similar semantic meaning as the tokens generated by the model. Given the hierarchical nature of our RQ-VAE tokens, prefix matching can be thought of as model predicting item category as opposed to the item index. Note that such an extension could improve the recall/NDCG metrics even further. We leave such an extension as a future work.

## 5 Conclusion

This paper proposes a novel paradigm, called TIGER, to retrieve candidates in recommender systems using a generative model. Underpinning this method is a novel semantic ID representation for items, which uses a hierarchical quantizer (RQ-VAE) on content embeddings to generate tokens that form the semantic IDs. Our framework provides results in a model that can be used to train and serve without creating an index -- the transformer memory acts as a semantic index for items. We note that the cardinality of our embedding table does not grow linearly with the cardinality of item space, which works in our favor compared to systems that need to create large embedding tables during training or generate an index for every single item. Through experiments on three datasets, we show that our model can achieve SOTA retrieval performance, while generalizing to new and unseen items.