# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

TDE-GNN, however, is limited to utilizing integer-order ODEs and does not account for the non-local memory effects inherent in fractional-order differential operators. These operators  have been developed to overcome the limitations of their traditional integer-order counterparts when modeling complex real-world dynamics. The key difference between fractional and integer operators can be grasped from a microscopic random walk perspective as shown in . For instance, traditional integer-order diffusion PDEs, which model diffusive transport in homogeneous porous media, typically ignore the waiting times between particle movements. However, these models struggle when applied to solute diffusion in heterogeneous porous media, prompting the introduction of fractional-order operators to better handle these complexities . In fractional scenarios, particles may remain at their current position, delaying jumps to subsequent locations with fading waiting times and leading to a non-Markovian process. In contrast, traditional integer-order differential equations are typically used to model Markovian movement of particles, as the derivative \(f(t)}{t}=_{ t 0}\) captures the local rate of function changes. On the other hand, although FROND utilizes a fractional-order \(\) and demonstrates performance improvement, its capacity for feature updating dynamics remains constrained by limited temporal dependencies with a single \(\). Moreover, the optimized performance of FROND is achieved through extensive fine-tuning of \(\) across various graph datasets. Observations from Fig. 2 indicate that performance can fluctuate significantly as the value of the fractional order varies from 0 to 1.

The distributed-order fractional differential operator has gained recognition in fractional calculus for its capacity to model complex dynamics that traditional differential equations with integer or single fractional orders cannot sufficiently capture . Inspired by this advancement, we introduce a novel continuous GNN framework named the _Distributed-order fRActional Graph Operating Network (DRAGON)_, which extends beyond existing frameworks like TDE-GNN and FROND. Rather than designating a single, constant \(\) with extensive fine-tuning, DRAGON employs a learnable measure \(\) over a range \([a,b]\) for \(\). The foundation of our framework is the distributed-order fractional differential operator :

\[_{a}^{b}D^{}f(t)\,(),\] (1)

which can be perceived as the limiting case of \(_{i}w(_{i})D^{_{i}}f(t)\), a weighted summation over derivatives of multiple orders with weight \(w()\) (we employ this more common notation \(D^{}\) instead of \(\,^{}/\,t^{}\) henceforth). Notably, unlike TDE-GNN, which restricts \(_{i}\) to integer values, DRAGON allows for a continuous range of values, significantly broadening its application scope and flexibility in modeling. This operator also addresses the limitations of the single fractional-order operator \(D^{}\) employed in FROND, which still has a restricted capacity to model the intricacies of feature updating dynamics. From the perspective of a random walk in a diffusion process, a single \(D^{}\) dictates that the waiting time between particle jumps follows a fixed power-law distribution \( t^{--1}\) for \(0<<1\). In contrast, DRAGON adopts a more flexible approach, enabling a broader range of waiting times across multiple temporal scales. In this paper, we demonstrate the efficacy of the DRAGON framework in modeling more intricate non-Markovian node feature updating dynamics in graph-based data. We provide evidence that DRAGON can approximate any given waiting time probability distribution pertinent to graph random walks, thus showcasing its advanced capability in capturing complex feature dynamics.

**Main contributions.** Our objective is to develop a general continuous GNN framework that enhances flexibility in graph feature updating dynamics. Our key contributions are summarized as follows:

* We propose a generalized continuous GNN framework that incorporates distributed-order fractional derivatives, extending previous continuous GNN models into a unified approach. Specifically, our framework treats these models as special cases with \(()\) taking a single positive real value for  or multiple integer values . Our approach facilitates flexible and learnable node feature updating dynamics stemming from the superposition of dynamics across various derivative orders.
* From a theoretical standpoint, we present the non-Markovian graph random walk with flexible waiting time for DRAGON, presuming that the feature updating dynamics adhere to a diffusion principle. This exposition elucidates the rationale behind the flexible feature updating dynamics.
* Through empirical assessments, we test the DRAGON-enhanced versions of several prominent continuous GNN models. Our findings consistently demonstrate their outperformance. This under scores the DRAGON framework's potential as an augmentation to amplify the effectiveness of a range of continuous GNN models.

## 2 Preliminaries and Related Work

This paper focuses on developing a new GNN framework centered around distributed-order fractional dynamic processes. In this section, we provide a concise introduction to the key concepts in fractional calculus. Throughout the paper, we adopt certain standard assumptions to ensure problem well-posedness. For instance, the well-definedness of integrations, the existence and uniqueness of the differential equation solution , and the allowance for interchange between summation and limit via the monotone or dominated convergence theorem  are all assumed.

### Fractional Derivative

The single fractional-order operator \(D^{}\) in the distributed-order fractional operator in (1) can assume various definitions. In this study, we start off with the _Marchaud-Weyl_ fractional derivative \({}_{}D^{}\), recognized for its efficacy in elucidating the fading memory phenomena , which we will discuss further in Sections 2.1.1 and 3.2.

**Remark 1**.: _However, in practical engineering implementations, the Caputo fractional derivative \({}_{}D^{}\) is more commonly utilized . Due to space limitations, the introduction of Caputo's derivative is deferred to the Appendix B and will be subsequently employed in Section 3.3 to solve DRAGON. The Marchaud-Weyl and Caputo definitions are equivalent under certain constraints ._

For any \((0,1)\), the Marchaud-Weyl \(\)-order derivative of a function \(f\), defined over the real line, at a specified point \(t\) is defined as :

\[{}_{}D^{}f(t)=_{0}^{ }}\,,\] (2)

where \(()\) is the Gamma function. For sufficiently smooth functions, according to , we have

\[_{ 1^{-}}\,{}_{}D^{}f(t)=f(t)}{ t}=_{ t 0}.\] (3)

It is evident from (2) that the Marchaud-Weyl fractional derivative is a nonlocal operator and accounts for the past values of \(f\) within the \((-,t)\) range, indicative of its memory effect. In terms of probability, the related non-Markovian processes for fractional systems are characterized by state evolution that depends not just on the current state, but also on historical states . As \( 1^{-}\) in (3), the operator reverts to the traditional first-order derivative, representing the local change rate of the function with respect to time.

#### 2.1.1 Non-Markovian Random Walk Interpretation

We elucidate fractional-order derivatives by linking them to one-dimensional heat diffusion and memory-decaying non-Markovian random walks . Assuming a random walker moves along an axis with infinitesimal intervals of space \( x>0\) and time \(>0\), the walker moves a distance of \( x\) from the current point \(x\) in either direction with equal probability and waits at each location for a random period of time, a positive integer multiple of \(\). This introduces randomness in the waiting times between steps. We aim to compute \(u(x,t)\), the probability of the walker arriving at position \(x\) at time \(t\). The waiting time distribution, \(_{}(n)\), is given by a power-law function \(d_{}n^{-(1+)}\) with \(d_{}>0\) chosen to ensure \(_{n=1}^{}_{}(n)=1\). The law of total probability is expressed as:

\[u(x,t)=_{n=1}^{}u(x- x,t-n)\,+ \,u(x+ x,t-n)\,_{}(n).\]

Here, the terms within brackets denote the probability of arriving at \(x\) from either neighboring points, \(x- x\) or \(x+ x\), each with probability \(1/2\). The sum over \(n\) accounts for the possibility that the walker could have remained stationary for an extended period \(n\) with a waiting time probability \(_{}(n)\). After subtracting \(_{n=1}^{}_{}(n)u(x,t-n)\) from both sides and rearranging, we obtain:

\[_{n=1}^{}} ()=}{2d_{}()^{}}_{n=1} ^{}_{2}u(x,t-n)_{}(n).\] (4)where the second-order incremental quotient is defined as:

\[_{2}u(x,t)=}.\]

In the limit as \( x, 0\) and assuming that \(}{d_{}()^{}} k_{}|(- )|\) for a positive \(k_{}\), we obtain the time-fractional diffusion equation:

\[{}_{}D^{}u=}{2}u_{xx},\] (5)

where the summations on the left-hand side of (4) converge to the integration (2). As \( 1^{-}\), (5) reverts to the standard heat diffusion equation:

\[=}{2}u_{xx}.\] (6)

Consequently, the aforementioned non-Markovian random walk with fading memory simplifies to the Markovian random walk, thereby eliminating the memory effects.

### Integer-Order Continuous GNN Models

We denote an undirected graph as \(=(,)\), where \(\) is the set of \(||=N\) nodes and \(=([_{1}]^{},,[_{N}]^{ })^{}^{N d}\) consists of rows \(_{i}^{1 d}\) as node feature vectors. The \(N N\) adjacency matrix \(:=(W_{ij})\) has elements \(W_{ij}\) indicating the edge weight between the \(i\)-th and \(j\)-th nodes with \(W_{ij}=W_{ji}\). In the subsequent GNNs inspired by dynamic processes, we let \((t)=([_{1}(t)]^{},,[_{N}(t )]^{})^{}^{N d}\) be the features at time \(t\) with \((0)=\) serving as the initial condition. The time \(t\) here acts as an analog to the layer index [7; 30]. Typically, these dynamics can be described by:

\[(t)}{t}=(, (t)).\] (7)

The function \(\) is specifically tailored for graph dynamics as illustrated in Appendix F. For instance, in the GRAND model, \(\) is defined as follows:

**GRAND**: Drawing from the standard heat diffusion equation, GRAND formulates the following feature updating dynamics:

\[(t)}{t}=(((t))- )(t),\] (8)

where \(((t))\) is a learnable attention or fixed normalized matrix, and \(\) is an identity matrix.

### Fractional-Order Continuous GNN Models

Recently, the paper  introduces FROND, extending traditional integer-order graph neural differential equations such as (8), (40) and (42) to fractional-order equations. The framework is formalized as

\[D^{}(t)=(,(t)),>0,\] (9)

where \(\) represents the graph dynamics. Further, the study in  explores the robustness of FROND, demonstrating its ability to enhance the resilience of integer-order continuous GNNs under perturbations. This underscores the potential applications of FROND in various domains.

### Motivation: Advanced Dynamics Modeling Capability

To intuitively understand the versatility and efficacy of the DRAGON framework in learning dynamics, we consider three classical stress-strain constitutive models for viscoelastic solids: the single-order Maxwell model , the multi-order Zener model , and the distributed-order Kelvin-Voigt model . Using the FROND and DRAGON frameworks, we develop Neural Network(NN) methods to predict future states based on current observations.

  Model & FROND-NN & DRAGON-NN \\ 
**Maxwell** & \(2.0 10^{-4}\) & \(5.6 10^{-5}\) \\
**Zener** & \(3.6 10^{-2}\) & \(3.5 10^{-3}\) \\
**Kelvin-Voigt** & \(3.3 10^{-3}\) & \(1.4 10^{-4}\) \\  

Table 1: Comparison of MSE for the Maxwell, Zener, and Kelvin-Voigt models using FROND-NN and DRAGON-NN frameworks.

The detailed descriptions and implementation specifics can be found in Appendix G.1. The results presented in Table 1 demonstrate that the DRAGON framework excels in fitting not only the multi-order model but also in capturing the dynamics of single-order and distributed-order models. We observe that the DRAGON framework achieves a Mean Squared Error (MSE) that is ten times smaller than that of the FROND method across all three models. This highlights the DRAGON framework's exceptional ability to effectively learn and adapt to a diverse range of dynamics, surpassing the capabilities of FROND.

## 3 DRAGON Framework

In this section, we introduce our general DRAGON framework for GNNs, with a random walk interpretation that elucidates the underlying mechanics when a specific diffusion-inspired system is utilized. Subsequently, we discuss numerical techniques for solving DRAGON. The versatility of our framework is highlighted by its capacity to encapsulate a broad spectrum of existing continuous GNN architectures, while simultaneously nurturing the development of more flexible continuous GNN designs within the research community in the future.

### Framework

DRAGON generalizes the current integer-order and fractional-order continuous GNNs as it uses a learnable probability distribution over a range of real numbers for the fractional derivative orders. Consider a graph \(=(,)\) composed of \(||=N\) nodes with \(\) being adjacency matrix as defined in Section 2.2. Similar to the approach used in integer-order continuous GNN models  as presented in Section 2.2, we apply a preliminary learnable encoder function \(:^{d}\) that maps each node to a feature vector. After stacking all these feature vectors, we obtain \(^{N d}\). Employing the distributed-order fractional derivative outlined in (1), the feature dynamics in DRAGON are characterized by the following graph dynamic equation:

\[_{a}^{b}D^{}(t)\,()=( ,(t)),\] (10)

where \([a,b]\) denotes the range of the order \(\), \(\) is a learnable measure of \(\), and \(\) is a dynamic operator on the graph as illustrated in Appendix F.

**Remark 2**.: _In practical engineering settings, the Caputo fractional derivative, represented by \({}_{}D^{}\), is commonly used . When leveraging the Caputo definition for the fractional derivative, as detailed in Section 3.3, the initial condition for (10) is given by \(^{[n]}(0)=\), where \(^{[n]}(0)\) denotes the \(n\)-th order derivative at \(t=0\), encompassing the initial node features for all integers \(n[0,[b]]\). Here, \(\) is the ceiling function, and this setup ensures a unique solution . For instance, when \([a,b]=\), we define the initial condition as \((0)=\)._

This framework generalizes prior continuous GNN models, encompassing them as special instances. Specifically, with \(()=(-1)\), where \(\) is the Dirac delta function, (10) simplifies to a local first-order differential equation like . When \([a,b]=\), we may obtain a distributed-order fractional wave propagation GNN model , which generalizes the second-order GraphCON model (40). When \(()=(-_{o})\) for \(_{o}^{+}\), (10) reduces to the FROND framework (9). Additionally, when \(\) adopts a discrete distribution over multiple integers, the model corresponds to TDE-GNN .

Following previous works, we set an integration time parameter \(T\) to obtain \((T)\). The final node embeddings, employed for subsequent downstream tasks, can be decoded as \(((T))\), where \(\) symbolizes a learnable decoder function.

### Non-Markovian Graph Random Walk with Flexible Memory

In this subsection, we provide a non-Markovian graph random walk interpretation for DRAGON under a specific anomalous diffusion setting, where the dynamic operator \((,(t))\) in (10) is set as \((((t))-)(t)\) in (8) with a fixed constant matrix \(\). More specifically, we obtain the following linear distributed-order FDE:

\[_{0}^{1}{}_{}D^{}(t)\,()= (t),\] (11)where we set \(=^{-1}\) and \(:=^{-1}-\) is the random walk Laplacian. Here, \(\) is a diagonal matrix with \(D_{ii}=d_{i}\), the degree of node \(i\). For clarity, without loss of generality, similar to the approach in Section 2.1.1, we interpret \((t)\) as a \(N\)-dimensional probability or concentration vector \((t)\) over the graph nodes \(\) at time \(t\). The Marchaud-Weyl\({}_{}D^{}\) employed in (11) helps expedite the exposition of the subsequent random walk, drawing an analogy from the one-dimensional random walk discussed in Section 2.1.1.

For every individual value \(_{o}(0,1)\), we consider a random walker navigating over graph \(\) with an infinitesimal interval of time \(>0\). We assume that there is no self-loop in the graph topology. The dynamics of the random walk are characterized as follows:

1. The walker is expected to wait at the current location for a random period of time. The distribution of waiting times, \(_{_{o}}(n)\), is given by a power-law function \(d_{_{o}}n^{-(1+_{o})}\) with \(d_{_{o}}>0\) chosen to ensure \(_{n=1}^{_{o}}_{_{o}}(n)=1\).
2. Upon deciding to make a jump, the walker can either move from the current node \(i\) to a neighboring node \(j\) with a probability of \(()^{_{o}}d_{_{o}}|(-_{o})|}{d_{ i}}\) if \(i j\). Alternatively, with a probability of \(1-()^{_{o}}d_{_{o}}|(-_{o})|\), it will remain at the current node \(i\).

We denote \(_{j}(t;_{o})\), the probability of the walker being at node \(j\) at time \(t\) with a specific order \(_{o}\) and \(()=(-_{o})\). The law of total probability is expressed as:

\[_{j}(t;_{o}) =_{n=1}^{}_{i \\ i j}_{i}(t-n;_{o})()^{ _{o}}d_{_{o}}|(-_{o})|}{d_{i}}\] (12) \[+_{j}(t-n;_{o})1-( )^{_{o}}d_{_{o}}|(-_{o})|_{ _{o}}(n).\]

In this equation, the summation over \(n\) accounts for the possibility that the walker may have remained stationary for a period of \(n\), with a waiting time probability of \(_{_{o}}(n)\). Fig. 1 provides a visualization of the non-Markovian graph random walk. For more explanation of the non-Markovian random walk on graphs, please refer to Appendix E. From (12), we can derive Theorem 1.

**Theorem 1**.: _Given \(()=(-_{o})\) where \(_{o}(0,1)\) and \( 0\), we establish that \((t;_{o})\), the probability vector whose \(j\)-th element is \(_{j}(t;_{o})\), solves (11). That is to say, we have_

\[_{0}^{1}{}_{}D^{}(t;_{o})\,( )=(t;_{o}).\] (13)

Figure 1: Visualization of the Non-Markovian Graph Random Walk. The diagram illustrates the walker’s decision-making process during the walk. After waiting for a random duration \(n\), the walker may either remain on the current node or proceed to a neighborhood node. This reflects the flexible, memory-influenced dynamics of the walker’s movement.

**Remark 3**.: _In Theorem 1, we present the graph random walk interpretation for the fractional anomalous diffusion equation (11) under the condition that \(=(-_{})\). This condition represents a single-term fractional scenario similar to FROND. At its core, this type of random walk is non-Markovian, underscoring the importance of the entire walk history._

From the discussion above, for a specific \(_{}\), the waiting time is steered by the power-law distribution \( n^{-(_{}+1)}\). Moreover, the distributed-order fractional operator can be interpreted as a flexible superposition of the dynamics behaviors embodied by individual fractional-order operators. This generalization reframes the interpretation of graph random walk and enables more nuanced dynamics that accommodate diverse waiting times. Although it is feasible to formulate a random walk interpretation where the waiting time is linked to the measure \(\) and converges to the solution of (11), this approach relies on the intricate stopping time technique [Sec 7.5] and may sacrifice flexibility in waiting time insights. Instead, we propose a more modest conclusion, demonstrating that a weighted sum of \(_{_{i}}(n)\) can _approximate any waiting time_, highlighting the capability of our framework in comparison to FROND.

**Theorem 2**.: _Let \(C_{0}()\) be the space of functions on the natural numbers \(\) vanishing at \(\), i.e., \(f C_{0}()\) if and only \(_{n}f(n)=0\). Assume the sequence \((_{m})_{m=1}^{}\) is strict increasing in \(\), then the span of \(\{_{_{m}},m 1\}\) is dense in \(C_{0}()\) in the sense of uniform convergence._

**Remark 4**.: _Theorem 2 demonstrates the DRAGON framework's ability to approximate any waiting time distribution for graph random walkers, offering flexibility in modeling feature updating dynamics with varying extents of memory incorporation. This highlights the advantage of using DRAGON for deploying learnable and flexible feature updating dynamics. In contrast, FROND is confined to a fixed waiting time distribution, limiting its adaptability in modeling feature updating over time._

### Solving DRAGON

Previous continuous GNNs have leveraged neural ODE solvers  when \(=(-1)\). For example, in the explicit Euler scheme, neural ODEs are effectively reduced to residual networks with shared hidden layers . Addressing the challenge of solving the distributed-order FDE (10) given by DRAGON, the standard approach involves discretizing it into a multi-term FDE. This is achieved by using a quadrature formula to approximate the integral term . As articulated in Sections 2.1 and 3.1, we follow the convention in the fractional calculus literature for real-world applications and employ the Caputo definition \({}_{}D^{}\) in this section. This choice is intuitive, as it seamlessly incorporates initial conditions into the problem as previously discussed under (10). The initial step is to approximate (10) as follows:

\[_{j=0}^{n}w_{j{}}D^{_{j}}(t)=( ,(t))\] (14)

where \(_{j}[a,b]\), \(j=0,1,,n\), are distinct interpolation points and \(w_{j}\) are weights associated with the measure \(\). Reflecting the learnable nature of \(\), \(w_{j}\) is directly set to be a learnable parameter in our implementation.

The next step is to solve the multi-term FDE presented in (14). According to the approach outlined in [17, Theorem 8.1], the multi-term FDE can be transformed into a system of single-order equations \({}_{}D^{}\), where \( 1/M\) and \(M\) is the least common multiple of the denominators of \(_{0},_{1},,_{n}\) when these coefficients are rational numbers. The classical fractional Adams-Bashforth-Moulton method can then be applied to solve the resulting system of single-order equations . This method is a generalization of the Euler scheme for ODEs to fractional scenarios (see Appendix C.3 for a detailed explanation).

An alternative approach involves directly approximating the fractional derivative operators as demonstrated in . This discretization method can then be used to derive iterative methods for solving the multi-term FDE given in (14). Detailed procedures for this method are provided in Appendix C.4. Additionally, the approximation error analysis of the numerical solvers is discussed in Appendix D.

### DRAGON GNNs

In Section 2.2 and Appendix F, several continuous GNNs, such as (8), (40) and (42), which employ integer-order derivatives, are introduced. We now extend these dynamical systems to operate underour proposed DRAGON framework, which generalizes the scenarios to involve distributed-order fractional derivatives. More specifically, we present the following GNNs, which will be utilized in Section 4 to show the advantages of our framework over various graph benchmarks.

1. **D-GRAND:** By extending (8), we get \[_{0}^{1}D^{}(t)\,()=(((t))-)(t).\] (15)
2. **D-GraphCON:** By extending (40), we get \[_{0}^{2}D^{}(t)\,()=(_{ }((t),t))-(t).\] (16)
3. **D-CDE:** By extending (42), we get \[_{0}^{1}D^{}(t)\,()=(( (t))-)(t)+((t)(t)),\] (17) where \(((t)(t))\) is given in (43) and (44).

Depending on the method used to compute the matrix \(\) in (15), the D-GRAND model can be categorized into two versions: linear (D-GRAND-l) and non-linear (D-GRAND-nl). Similarly, based on the computation of \(_{}\) in (16), the D-GraphCON model also has two versions: linear (D-GraphCON-l) and non-linear (D-GraphCON-nl). Detailed explanations are provided in Appendix F.1.

## 4 Experiments

Our approach aims to enhance the capabilities of continuous GNN models by flexibly combining graph dynamics across different derivative orders. To achieve this, we have integrated DRAGON into several existing continuous GNN models and assessed their performance. Specifically, we conduct experiments on our proposed D-GRAND (15), D-GraphCON (16), and D-CDE (17) in this section, as well as D-GREAD and D-GRAND++ in Appendix I.3 and Appendix I.4.

### Implementation Details

In our approach, we employ a fully connected (FC) layer as the encoder, \(:^{d}\), to determine the initial values for DRAGON. Subsequently, another FC layer \(\) serves as the decoder, transforming the output of DRAGON for downstream tasks. Most existing continuous GNNs are first-order or can be transformed into first-order representations of certain dynamic processes across graphs . The FROND framework also restricts the fractional order to the range \(\), maintaining identical initial conditions to those utilized in the original models. Given these considerations, we mainly restrict \(_{j}\) values between  in our implementation, while also balancing computational costs. The parameter \(_{j}\) is selected to evenly divide the entire range, aiming to comprehensively cover values between \(\). Typically, we set the number of \(_{j}\) in (14) to 10. We also explore the empirical results when \(_{j}\) exceeds 1, as shown in Appendix I.5. For a sensitivity analysis of the number and value of \(_{j}\), we refer the readers to Appendix I.6. Details on the datasets used can be found in Appendix G.2.

### Long Range Graph Benchmark

As illustrated in Remark 4, the DRAGON framework exhibits a distinctive intrinsic property: its ability to capture flexible memory effects, which is crucial for modeling long-range dependencies

  
**Method** & **Peptides-func** & **Peptides-Struct** \\  & Test AP \(\) & Test MAE \(\) \\  GCN  & 0.5930\(\)0.0023 & 0.3496\(\)0.0013 \\ GCN  & 0.5543\(\)0.0078 & 0.3471\(\)0.0010 \\ GINE  & 0.5498\(\)0.0079 & 0.3447\(\)0.0045 \\ GatedGCN  & 0.5864\(\)0.0077 & 0.3420\(\)0.0013 \\  Transformer+LapPE  & 0.6326\(\)0.0126 & 0.2529\(\)0.0016 \\ SAN+Lapple  & 0.6384\(\)0.0121 & 0.2683\(\)0.0043 \\ SAN+RWE  & 0.6439\(\)0.0075 & 0.2545\(\)0.0012 \\ GCN+Drew  & 0.6996\(\)0.0076 & 0.2781\(\)0.0028 \\ PathN  & 0.6816\(\)0.0026 & 0.2545\(\)0.0032 \\ DRGAN  & 0.6586\(\)0.0042 & 0.2495\(\)0.015 \\  GRAND-l & 0.6962\(\)0.0015 & 0.2867\(\)0.0009 \\ F-GRAND-l & 0.7126\(\)0.0024 & 0.2677\(\)0.0014 \\ D-GRAND-l & **0.7571\(\)0.0014** & **0.2461\(\)0.0014** \\   

Table 2: Numerical results for various methods on LRGB tests.

in graph data . To empirically validate this capability, we conduct experiments using the Long-Range Graph Benchmark (LRGB) . Specifically, we focus on the Peptides molecular graphs dataset, performing _graph classification_ on the Peptides-func dataset and _graph regression_ based on the 3D structure of peptides in the Peptides-struct dataset. The performance metrics used are Average Precision (AP) for classification and Mean Absolute Error (MAE) for regression tasks. From Table 2, it is evident that the DRAGON framework outperforms the other methods on these two long-range graph datasets, even when compared to state-of-the-art (SOTA) techniques. Notably, DRAGON achieves an improvement of approximately 4-6% over traditional continuous GNNs like GRAND-l and F-GRAND-l. This demonstrates DRAGON's capability to effectively capture long-range dependencies in graph data.

### Node Classification

#### 4.3.1 Homophilic Graph Datasets

In our evaluation on homophilic datasets, we leverage a diverse set of datasets including citation networks (Cora , Citeseer , Pubmed ), tree-structured datasets (Disease and Airport ), as well as coauthor and co-purchasing graphs (CoauthorCS , Computer and Photo ). For the Disease and Airport datasets, we follow the data partitioning and preprocessing procedures as described in . For all other datasets, we adopt random splits for the largest connected component (LCC), in line with the approach detailed in .

   Method & Roman-empire & Wiki-coco & Minesweeper & Questions & Workers & Amazon-ratings \\ \(h_{}\) & -0.05 & -0.03 & 0.01 & 0.02 & 0.09 & 0.14 \\  ResNet  & 65.71\(\)0.44 & 89.36\(\)0.71 & 50.95\(\)1.12 & 70.10\(\)0.75 & 73.08\(\)1.28 & 45.70\(\)0.69 \\  H2GCN  & 68.09\(\)0.29 & 89.24\(\)0.32 & 89.95\(\)0.38 & 66.66\(\)1.84 & 81.76\(\)0.68 & 41.36\(\)0.47 \\ CPGNN  & 63.78\(\)0.50 & 84.84\(\)0.66 & 71.27\(\)1.14 & 67.09\(\)2.63 & 72.44\(\)0.80 & 44.36\(\)0.35 \\ GPR-RNN  & 73.37\(\)0.68 & 91.90\(\)0.78 & 81.79\(\)0.98 & 73.41\(\)1.24 & 70.59\(\)1.15 & 43.90\(\)0.48 \\ GloGNN  & 63.85\(\)0.49 & 88.49\(\)0.45 & 62.53\(\)1.34 & 67.15\(\)1.92 & 73.90\(\)0.95 & 37.28\(\)0.66 \\ FAGCN  & 70.53\(\)0.99 & 91.88\(\)0.37 & 89.69\(\)0.60 & **74.14\(\)**5.86 & 81.87\(\)0.94 & 46.32\(\)2.50 \\ GBR-RNN  & 75.87\(\)0.43 & 97.81\(\)0.32 & 83.56\(\)0.34 & 72.98\(\)1.05 & 78.06\(\)0.91 & 43.47\(\)0.51 \\ ACM-GCN  & 68.35\(\)1.95 & 87.48\(\)1.06 & 90.47\(\)0.57 & 0OM & 78.25\(\)0.78 & 38.51\(\)3.38 \\  GRAND  & 71.60\(\)0.58 & 92.03\(\)0.46 & 76.67\(\)0.98 & 70.67\(\)1.28 & 75.33\(\)0.84 & 45.05\(\)0.65 \\ GraphBell  & 69.47\(\)0.37 & 90.30\(\)0.50 & 76.51\(\)1.03 & 70.99\(\)0.99 & 73.02\(\)0.92 & 43.63\(\)0.42 \\ Diag-NSD  & 77.50\(\)0.67 & 92.06\(\)0.40 & 89.59\(\)0.61 & 69.25\(\)1.15 & 79.81\(\)0.99 & 37.96\(\)0.20 \\ ACMP  & 71.27\(\)0.59 & 92.68\(\)0.37 & 76.15\(\)1.12 & 71.18\(\)1.03 & 75.03\(\)0.92 & 44.76\(\)0.52 \\ TDE-GNN  & 64.29\(\)0.58 & 84.95\(\)0.78 & 61.15\(\)2.24 & 68.94\(\)1.69 & 75.13\(\)0.81 & 40.33\(\)1.37 \\  CDE  & 91.64\(\)0.28 & 97.99\(\)0.38 & 95.50\(\)5.23 & 75.17\(\)0.99 & 80.70\(\)1.04 & 47.63\(\)0.43 \\ F-CDE  & 93.66\(\)0.55 & **98.73\(\)1.68** & **96.94\(\)0.25** & 75.17\(\)0.99 & 82.68\(\)0.86 & **49.01\(\)0.56**

#### 4.3.2 Heterophilic Graph Datasets

For evaluating performance on heterophilic datasets, we utilize six datasets introduced in , with details provided in Appendix G.2. As highlighted in , these datasets are characterized by lower adjusted homophily \(h_{}\), indicating a higher degree of heterophily. In our experimental setup with these heterophilic datasets, we follow the data splitting strategy described in , dividing the data into 50% for training, 25% for validation, and 25% for testing.

#### 4.3.3 Performance of DRAGON framework

As shown in Table 3, for homophilic datasets such as citation networks, coauthor networks, and co-purchasing networks, our DRAGON framework enhances the performance of continuous backbones like GRAND and GraphCON. This demonstrates the ability of our DRAGON framework to seamlessly integrate with existing continuous GNNs and improve their performance. Notably, on tree-structured datasets, our DRAGON framework significantly boosts the performance of both GRAND and GraphCON. In particular, on the Airport dataset, our DRAGON framework excels, achieving a 7% performance increase compared to the GIL model specifically designed for this type of tree-like dataset. Compared to FROND, our DRAGON framework shows improvements on most datasets. The results of the graph node classification on heterophilic datasets are presented in Table 4. As indicated in Table 4, the proposed D-CDE model with our DRAGON framework improves the performance of the original CDE and F-CDE models on five out of the six datasets. This underscores the ability of DRAGON to capture flexible memory effects as proved in Theorem 2, highlighting its enhanced capability in modeling complex feature updating dynamics.

### Model Complexity

For the Adams-Bashforth-Moulton method (25), the numerical solution is computed iteratively for \(E T/h\) time steps, where \(h\) represents the discretization size and \(T\) the integration time. This process involves repeated computation of \((,_{j})\) for each iteration. By storing intermediate function evaluation values \(\{(,_{j})\}_{j}\), we can express the total computational time complexity across the process as \(_{k=0}^{E}(C+O(k))\), where \(O(k)\) indicates the computational overhead from summing and weighting the \(k\) terms at each step. Here, \(C\) represents the complexity of computing \(\). This yields a total cost of \(O(EC+E^{2})\). If a fast algorithm for the convolution computations is available, we typically require \(O(E E)\) for the convolution , resulting in \(O(EC+E E)\). If the cost of weighted summing is minimal, the complexity is reduced to \(O(EC)\). For the Grunwald-Letnikov method (32), the computational complexity is the same as that of the method (25).

The term \(C\) denotes the computational complexity of the function \(\). For instance, setting \(\) to the GRAND model results in \(C=||d\), where \(||\) represents the edge set size and \(d\) the dimensionality of the features . Alternatively, using the GREAD model results in \(C=O((||+|_{2}|)d+||d_{})\), where \(|_{2}|\) accounts for the number of two-hop edges, and \(d_{}\) is the maximum degree among nodes . More details of the computation cost can be found in Appendix H.

## 5 Conclusion

We introduce the DRAGON framework, which incorporates distributed-order fractional derivatives into continuous GNNs. DRAGON advances the field by employing a learnable distribution of fractional derivative orders, surpassing the constraints of existing continuous GNN models. This approach eliminates the need for fine-tuning the fractional order, as required in FROND, and enriches the dynamics and representational capacity of existing continuous GNN models. We also provide a flexible random walk interpretation. Through rigorous empirical testing, DRAGON has demonstrated not only its adaptability but also its consistent outperformance compared to other continuous GNN models. Consequently, DRAGON establishes itself as a powerful framework for advancing graph-related tasks.