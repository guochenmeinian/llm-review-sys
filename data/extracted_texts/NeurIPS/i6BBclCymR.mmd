# How to Use Diffusion Priors under Sparse Views?

Qisen Wang, Yifan Zhao, Jiawei Ma, Jia Li

State Key Laboratory of Virtual Reality Technology and Systems, SCSE

Beihang University

{wangqisen, zhaoyf, majiawei, jiali}@buaa.edu.cn

Correspondence should be addressed to Yifan Zhao and Jia Li. Website: [https://cvteam.buaa.edu.cn](https://cvteam.buaa.edu.cn)

###### Abstract

Novel view synthesis under sparse views has been a long-term important challenge in 3D reconstruction. Existing works mainly rely on introducing external semantic or depth priors to supervise the optimization of 3D representations. However, the diffusion model, as an external prior that can directly provide visual supervision, has always underperformed in sparse-view 3D reconstruction using Score Distillation Sampling (SDS) due to the low information entropy of sparse views compared to text, leading to optimization challenges caused by mode deviation. To this end, we present a thorough analysis of SDS from the mode-seeking perspective and propose Inline Prior Guided Score Matching (IPSM), which leverages visual inline priors provided by pose relationships between viewpoints to rectify the rendered image distribution and decomposes the original optimization objective of SDS, thereby offering effective diffusion visual guidance without any fine-tuning or pre-training. Furthermore, we propose the IPSM-Gaussian pipeline, which adopts 3D Gaussian Splatting as the backbone and supplements depth and geometry consistency regularization based on IPSM to further improve inline priors and rectified distribution. Experimental results on different public datasets show that our method achieves state-of-the-art reconstruction quality. The code is released at [https://github.com/iCVTEAM/IPSM](https://github.com/iCVTEAM/IPSM).

## 1 Introduction

Novel View Synthesis (NVS) , _e.g._Neural Radiance Fields (NeRF)  and recently emerged 3D Gaussian Splatting (3DGS) , requires dense training viewpoints for optimization, as demonstrated in prevailing works . Indeed, NVS under sparse views has been an important and challenging task . Due to the scarcity of viewpoints, most methods of 3D representation reconstruction often fall into over-fitting with sparse views, and cannot synthesize satisfactory novel views . To address the optimization over-fitting problem under the sparse-view condition, current methods introduce external priors to supervise the optimization of reconstruction like CLIP  semantic information , monocular depth , and diffusion visual priors . However, although the diffusion model  as an external prior can provide stronger visual supervision than semantic and depth information, it often requires a significant amount of computational resources for _fine-tuning the diffusion prior_ or _pre-training encoders_ with external data. A few works have no fine-tuning and pre-training, but it is difficult to straightly extract diffusion prior knowledge to effectively supplement the missing visual information of sparse views .

Interestingly, although the diffusion model shows great potential in 3D generation tasks, _e.g._text-to-3D , which benefit from the recent rapid development of score distillation techniques , Score Distillation Sampling (SDS)  shows little visual information guidance ability of the diffusion prior under sparse views and even takes an inhibitory effect on the baseline performance when theinput views increase, as shown in Fig. 1. **The SDS dilemma** highlights that score distillation exhibits distinctive optimization characteristics across sparse input views. Consequently, SDS is NOT readily applicable for lifting visual supervision from diffusion priors under sparse views.

With the curiosity of the SDS dilemma in our mind, it can be recognized that the difference between _sparse views_ and _text prompts_ lies in the inline constraints sparse views bring. For the unsupervised invisible views, unlike text prompts, the ideal rendered image supervision information is not completely absent. Due to the consistency of the 3D geometry and structure, the information exists in the given sparse views, which we refer to the **inline priors**. Some researchers  attempt to implicitly encode the given input sparse views to guide the sampling trajectory of the diffusion model, thereby introducing inline priors. Nonetheless, owing to domain shifts between specific scenes and the diffusion prior, a significant amount of external 3D annotated data and computational resources are frequently necessitated for domain rectification . To this end, a potentially viable approach is exploring the feasibility of adjusting the optimization objective of SDS by incorporating inline priors to facilitate efficient domain rectification without fine-tuning and pre-training.

In this paper, we conduct a comprehensive analysis of SDS from the perspective of mode-seeking. Intuitively, the optimization objective of SDS is to align the rendered image distribution with the target mode in the diffusion prior. However, due to the inherent suboptimality of the rendered image distribution under sparse views, SDS tends to deviate from the target mode, resulting in the SDS dilemma. To tackle this challenge, we present Inline Prior Guided Score Matching (IPSM), a method that rectifies the rendered image distribution by utilizing inline priors. IPSM leverages the rectified distribution to divide the optimization objective of SDS into two sub-objectives. The rectified distribution, as an intermediate state of the optimization objective, plays a role in controlling the mode-seeking direction, thereby suppressing mode deviation and promoting improvements in reconstruction. Moreover, we propose the pipeline IPSM-Gaussian, which combines IPSM with the efficient explicit 3D representation 3DGS for sparse-view 3D reconstruction. In addition to IPSM, IPSM-Gaussian integrates depth regularization to support inline priors and geometric consistency regularization to narrow the discrepancy between the rendered image distribution and the rectified distribution at the pixel level. Experimental results demonstrate that IPSM effectively leverages visual knowledge from the diffusion priors to improve sparse-view 3D reconstruction. The presented method achieves superior performance on publicly available datasets.

Overall, our contributions can be summarized as:

* _Analysis of SDS from mode-seeking perspective_. We present a comprehensive analysis of SDS optimization characteristics under sparse views, revealing that the mode deviation of SDS results in the optimization dilemma.
* _Rectified score distillation method for sparse views_. We propose Inline Prior Guided Score Matching (IPSM), which utilizes inline priors provided by sparse views to rectify rendered image distribution for controlling the direction of seeking the target mode.
* _Pipeline using IPSM based on 3DGS_. We present IPSM-Gaussian, a pipeline for sparse-view 3D reconstruction, which adopts IPSM for diffusion guidance, as well as depth and geometry regularization to boost the performance of IPSM. The experiments show that IPSM-Gaussian achieves state-of-the-art reconstruction quality on public datasets.

Figure 1: **Dilemma of SDS**. Average PSNR\(\), SSIM\(\), and LPIPS\(\) of each iteration on the LLFF test dataset  with Base (without SDS), SDS (CFG=7.5), and SDS (CFG=100). The prior-added period starts from the 2K iteration and ends at the 9.5K iteration. The opacity is also reset at 2K. The details and final training results of SDS are shown in Sec. 4.4.

Related Works

**Novel View Synthesis**. Novel View Synthesis [27; 1; 2; 28; 29; 30; 31] aims to synthesize invisible novel views given a set of images at seen viewpoints while preserving the geometric structure and appearance of the original 3D scene [32; 33; 34; 35; 36; 37]. NeRF [1; 3; 4], as an implicit 3D representation, adopts volume rendering to establish an implicit mapping relationship from the positions and ray directions to colors using a Multi-Layer Perception (MLP). Although NeRF can achieve photographic-realistic rendering quality compared to traditional methods, its required training time and rendering speed are not satisfactory [1; 28]. Recently, 3DGS [2; 5; 6] has garnered attention from researchers by achieving high training speeds and real-time rendering capabilities through explicit modeling of 3D scenes using Gaussian point clouds and rasterization rendering [38; 39; 40; 41; 42]. To this end, we choose 3DGS instead of NeRF as the backbone of 3D representations and adopt it in subsequent experiments.

**Sparse-view Novel View Synthesis**. Although current training-based NVS techniques, _i.e._NeRF  and 3DGS , can achieve satisfactory rendering quality in scenarios with dense input views, the quality of novel view synthesis significantly decreases under sparse views due to overfitting [43; 12; 44; 7; 11; 45]. To tackle this challenge, Yang _et al._ leverage the optimization properties of MLP and employ annealing strategies for positional encoding  tailored to the characteristics of NeRF, but this cannot be directly applied to 3DGS. More broadly, some works [46; 47] leverage the intrinsic relationships between sparse views to augment the data required for model optimization, but this does not address the established condition of information deficiency. More works involve introducing external pre-trained priors as optimization guidance to supervise sparse-view 3D reconstruction. Jain _et al._ introduce CLIP  to provide semantic guidance. Li _et al._ propose global-local depth regularization with DPT  for geometric structure guidance. However, the aforementioned prior information cannot directly provide visual supervision for sparse-view NVS like diffusion priors.

**Sparse-view Novel View Synthesis with Diffusion Priors**. Although diffusion priors can provide more direct visual guidance, current works are limited by the mode deviation with using diffusion priors directly. Liu _et al._ leverage diffusion models to progressively generate pseudo-observations at unseen views. Wu _et al._ use PixelNeRF  to encode sparse inputs for guiding the trajectory of diffusion priors. Unlike score distillation techniques, these works either require fine-tuning the diffusion model for narrowing the mode range , or pre-training image encoders for guiding the direction of the target mode , both of which consume many resources [16; 15]. Xiong _et al._ attempt to directly use SDS to extract the external visual prior of the diffusion model, but have to suppress its weighting, thus achieving limited effects. Although view-conditioned diffusion priors [50; 51] have emerged recently, different to helpness for 3D generation [50; 52], their guidance is still limited for sparse-view reconstruction, which is detailedly discussed in the Appendix. Therefore, _how to use diffusion priors_ and _how to use score distillation_ under sparse views without fine-tuning, pre-training, and the optimization dilemma shown in Fig. 1 have become crucial issues.

## 3 Method

With the phenomenon of the SDS dilemma shown in Fig. 1 in our mind, we have realized that SDS that works for text prompts does not work equally well for sparse views. Therefore, we attempt to analyze the disadvantages of SDS under sparse views and introduce inline constraints for effectively extracting visual guidance of diffusion priors without fine-tuning and pre-training. We start with the overview of 3DGS and also define the main symbols.

### Overview of 3D Gaussian Splatting

**Representation**. The 3DGS models the 3D structure with a set of Gaussian points with positions \(_{n}\), covariance matrix \(_{n}\), color \(c_{n}\) represented by Spherical Harmonic (SH) coefficients and opacity \(_{n}\). For each Gaussian point \(n\), its 3D position follows

\[G(x)=e^{-(x-_{n})^{}_{n}^{-1}(x-_{n})}, \]

where \(_{n}\) can be represented by the scaling matrix \(S_{n}\) and the rotation matrix \(R_{n}\)

\[_{n}=R_{n}S_{n}S_{n}^{}R_{n}^{}. \]

**Rendering**. For the 3D representation \(=\{_{n},_{n},c_{n},_{n}\}\), we can optimize the trainable parameters \(\) through the following differentiable rendering function

\[x_{0}()=_{n=1}^{N}c_{n}_{n}_{m=1}^{n-1}(1- _{m}), \]

where \(x_{0}()\) is the rendering color at pixel \(\) of rendered image \(_{0}\), and \(_{n}\) are computed from the projected 2D Gaussians.

### IPSM: Inline Prior Guided Score Matching

**Review of Score Distillation Sampling**. Intuitively, SDS tends to drive the rendered image distribution denoted with red color seeking the nearest mode of diffusion distribution denoted with blue color guided by text prompts. Specifically, we denote the rendered image at viewpoint \(^{j}\) as \(_{0}^{j}=g(,^{j})\), where \(g(,)\) is rendering function and \(\) is the 3D representation needed optimization. Without elaborating text prompts on the conditions for brevity, the posterior noisy distribution of rendered images is defined as

\[q_{t}^{}(_{t}^{j})(_{t}^{j};_{t}}_{0}^{j},(1-_{t})). \]

The prevailing score distillation works start from minimizing the reverse KL divergence between the distribution of the noisy rendered images \(q_{t}^{}(_{t}^{j})\) and the noisy real-world distribution \(p_{t}^{*}(_{t}^{j})\) represented by the pre-trained diffusion models, namely

\[_{}_{t,_{j}}(t)D_{KL}(q_{t}^{ }(_{t}^{j})\|p_{t}^{*}(_{t}^{j})), \]

which indicates the gradient of score distillation that

\[_{}_{}()_{t,,,^{j}}(t)(_{*}(_ {t}^{j},t)-)^{j})}{ }=_{t,,^{j}}(_{0}^{j}-}_{0}^{j;*})^{j})}{}, \]

where \((t)=_{t}}}{_{t}}}\) and \(_{0}^{j} q_{0}^{}(_{0}^{j}),\ }_{0}^{j;*}  p_{0}^{*}(_{0}^{j})\). That is, for the given new viewpoints \(_{j}\), the gradient \(_{}_{}()\) considers the rendered image distribution of the 3D representation and drives it closer to the pre-trained diffusion prior.

Following , we provide a further discussion of SDS. The optimization objective of Eq. 5 derives \(q_{t}^{}(_{t}^{j})\) to the high-density region of \(p_{t}^{*}(_{t}^{j})\). Considering samples \(^{},^{}\) from two modes of \(p_{t}^{*}(_{t}^{j})\), where \(^{}\) is from target mode and \(^{}\) is from failure mode. \(^{}\) is harmless for text-to-3D tasks due to the high information entropy properties of text prompts. However, for sparse-view 3D reconstruction, this leads the optimized 3D representation to be inconsistent with the given sparse images, thus causing optimization difficulties as shown in Fig. 2. Specifically, we denote the L2 distance of two samples as \((,)\). We want \(_{t}}_{0}^{j}_{t}} ^{}\) for any \(t\), but the gap between two modes is unclear when \(t\) increases, _i.e._\((_{t}}_{0}^{j},_{t}} ^{})(_{t}}_{0 }^{j},_{t}}^{})\), since \(|(_{0}^{j},^{})-(_{0}^{j },^{})|\) is not large enough for a small \(_{t}}\). This results in the mode aliasing for optimization and further affects the optimizing direction during training. To this end, the

Figure 2: **Comparison of SDS and IPSM. Left:** Tending to seek nearest mode, causing mode deviation. **Right:** Rectifying distribution to seek the target mode.

distribution of rendered images is not constrained to seeking the target mode, causing mode deviation. Therefore, we aim to construct a rectified distribution excluded failure mode using the inline prior from sparse views, whose sample \(^{}\) provides \(_{0}^{j}\) stable optimization guidance and amplifies the gap \(|(^{},^{})-(^{ },^{})|\) so that \((_{t}}^{},_{t} }^{})(_{t}}^{},_{t}}^{})\), and the rectified distribution is served as the bridge between \(_{0}^{j}\) and \(^{}\) to control the mode-seeking direction.

**Inline Prior**. Different from text-to-3D tasks, sparse views can achieve geometry consistency guidance of novel views through camera pose transformation, namely the inline prior we mentioned in Sec. 1. Therefore, we aim to utilize the additional visual information of sparse views compared to text prompts to correct the erroneous tendency of SDS optimization. Specifically, we sample a set of random pseudo viewpoints \(^{j}\) around the seen views \(^{i}\). Given the ground-truth image \(_{0}^{i}\) at the seen viewpoint \(^{i}\), we formulate the transforming function \((_{0}^{i};^{j},^{j i})\) which inversely warps image \(_{0}^{i}\) from viewpoint \(^{i}\) to \(^{j}\). \(^{j i}\) represents the relative pose transformation between two viewpoints, and \(^{j}\) is the alpha-blending rendered depth at viewpoint \(^{j}\) following

\[D^{j}()=_{n=1}^{N}d_{n}_{n}_{m=1}^{N-1}(1- _{m}), \]

where \(d_{n}\) is the z-buffer of the \(n\)-th Gaussian. During transformation, each pixel location \(^{j}\) at the pseudo viewpoint \(^{j}\) is warped to the pixel location \(^{j i}\) at the seen viewpoint \(^{i}\), and \(^{j i}\) can be represented by

\[^{j i}^{j i}D^{j}(^{j})^{-1 }^{j}, \]

where \(\) is the camera intrinsic parameter. Then, we can obtain the warped image \(I_{0}^{i j}(^{j})\) using inverse warping with the nearest sampling operator

\[I_{0}^{i j}(^{j})=(_{0}^{i}, ^{j i}). \]

However, this direct inverse warping may lead to warping distortion due to erroneous geometry. Following , we tackle it through the generated consistency mask with an error threshold \(\)

\[M^{i j}(^{j})=(\|D^{j}(^{j})-D^{i j}( ^{j})\|_{1}<), \]

where \(D^{i j}(^{j})=(^{i},^{j i})\) like Eq. 9. Eq. 10 ensures the filterability of erroneous geometry using the difference between the warped depth of the seen viewpoint and the depth of the pseudo viewpoint. In practice, the warped image \(_{0}^{i j}\) and its accompanying mask \(^{i j}\) are served

Figure 3: **IPSM-Gaussian** obtains the inline prior within sparse views through inversely warping seen views to unseen pseudo views, thus modifying the rendered image distribution to the rectified distribution. Consequently taking the rectified distribution as the intermediate state, two sub-optimization objectives are utilized for controlling the optimization direction.

as the _inline geometry consistency prior_ to guide external diffusion prior scene specialization. The intuitive explanation of inline priors can be found in Appendix B.7.

**Inline Prior Guided Score Matching**. Using score distillation directly in the case of sparse views overlooks the inline geometry consistency prior within the sparse views themselves, which is fundamentally different from text-to-3D. To this end, we rectify the distribution denoted with green color from \(q_{0}^{}(_{0}^{j})\) to \(_{0}^{,}(_{0}^{j}|^{i j}) _{0}^{i j},^{i j})\) using the inline prior. As shown in Fig. 3, we utilize the warped masked image \(_{0}^{i j}\) from the seen viewpoints to guide the sampling trajectory of \(}_{0}^{j;}_{0}^{,}(_{0}^{j}| ^{i j}_{0}^{i j},^{i j})\), thus introducing the inline geometry consistency prior to the score distillation. So our optimization objective is changed to minimizing (1) the KL divergence between the noisy rendered image distribution \(q_{t}^{}(_{t}^{j})\) and the noisy rectified distribution \(_{t}^{,}(_{t}^{j})\); (2) the KL divergence between the noisy rectified distribution \(_{t}^{,}(_{t}^{j})\) and the noisy diffusion prior distribution \(p_{}^{*}(_{t}^{j})\) represented by the pre-trained diffusion models, namely

\[_{}\{_{r}_{t,c}(t)D_{KL}(q_{t}^{ }(_{t}^{j})\|_{t}^{,}(_{t}^{j})) +_{t,c}(t)D_{KL}(_{t}^{,}( _{t}^{j})\|_{0}^{*}(_{t}^{j}))\}, \]

where \(_{r}\) is the adjustment parameter of the two sub-optimization objectives. In practice, we introduce an inpainting diffusion model \(_{}(_{t}^{j},t,^{i j} _{0}^{i j},^{i j})\), which shares the same VAE-feature domain with the pre-trained diffusion model \(_{*}(_{t}^{j},t)\) representing the real data distribution. So we have the rectified gradient of score distillation

\[_{}_{}() _{r}_{t,,^{j}} (_{0}^{j}-}_{0}^{j;}) ^{j})}{}+ _{t,,^{j}}(}_{0}^{j;}-}_{0}^{j;*})^{j})}{}\] \[= _{r}_{t,,^{j}} (t)(_{}(_{t}^{j},t,^{i j} _{0}^{i j},^{i j})-) _{j})}{}\] \[+_{t,,^{j}} (t)(_{*}(_{t}^{j},t)-_{}(_{t }^{j},t,^{i j}_{0}^{i j},^{i  j}))^{j})}{}. \]

Consequently, the IPSM regularization can be represented as

\[_{}=_{r}_{t,, ^{j}}\|(t)(_{}- )\|_{2}^{2}}_{_{}^{_{} }}+_{t,,^{j}}\| (t)(_{*}-_{})\|_{2}^{2}}_{_{}^{_{}}}. \]

### Training Details

**Depth Regularization**. In the warping process, it can be observed that the rendered depth influences pixel mapping relations, which is detailed in Sec. 4. Therefore, it is necessary to incorporate monocular depth estimation prior to supervising rendered depth, thus providing the correct inline prior. We use the Pearson Correlation to provide depth regularization, which can be represented as

\[(_{r},_{m})=(_{r },_{m})}{(_{r})(_{ m})}}. \]

Given the rendered depth \(_{r}^{i}\), monocular depth \(_{m}^{i}\) from the input view \(_{0}^{i}\) at the seen view \(^{i}\), and the rendered depth \(_{r}^{j}\), monocular depth \(_{m}^{j}\) from the rendered image \(_{0}^{j}\) at the unseen view \(^{j}\), we take the depth regularization as

\[_{}=_{d}\|(_{r}^{i},_{m}^{i})\|_{1}+\|(_{r}^{j},_{m}^{j})\|_{1}, \]

where \(_{d}\) serves as the weight to balance the supervision of seen views and pseudo-unseen views.

**Geometry Consistency Regularization**. In Eq. 13, we introduce \(_{}^{_{}}\) for providing guidance to minimize the reverse KL divergence between the rendered image and rectified distribution. In practice, we not only supervise from the diffusion feature domain but also provide stronger guidance by directly adding masked L1 loss of \(_{0}^{j}\) and \(_{0}^{i j}\), which is denoted as the geometry consistency regularization and can be represented as

\[_{}=\|^{i j}(_{0}^{j}- _{0}^{i j})\|_{1}. \]

**Total Training Objectives**. Overall, our training objectives can be divided into three parts: 1) The direct supervision \(_{1}\) and \(_{}\) of the sparse input views, which are inherited from the vanilla 3DGS; 2) The supervision \(_{}\) provided by diffusion priors using IPSM; 3) The supervision of depth and vision information \(_{}\) and \(_{}\) to support the inline priors and provide low-level inline guidance. The total training loss function can be summarized as

\[=_{1}_{1}+_{}_{ }+_{}_{}+_{ }_{}+_{}_{ }. \]

More training details are shown in the Appendix A.2.

## 4 Experiments

### Experiments Settings

**Datasets and Metrics**. We evaluate our method on the LLFF  and DTU dataset . The LLFF dataset involves 8 forward-facing scenes and we select 3 training views following prevailing works [8; 7]. On the DTU dataset, we choose the 15 testing scenes, and 3 training views whose IDs are 25, 22, and 28, following RegNeRF . Following prevailing works [7; 8; 9] to focus on the object-of-interest for the DTU dataset, we also remove the background with the mask of objects when evaluating. Aligning with the protocol of baselines, we apply the downsampling rate of 8 and 4 on the LLFF and DTU datasets respectively. We evaluate the reconstruction quality using SSIM , LPIPS , and PSNR. Following DNGaussian  and FreeNeRF , we also report AVGE for a comprehensive evaluation of the reconstruction quality. The AVGE is calculated by the geometric mean of \(}\), LPIPS, and \(=10^{-/10}\). The experiments are conducted 3 times and we report the mean and standard deviation. More details about datasets, _e.g._the sparsity of training views and train-test split protocols, can be found in Appendix A.1.

**Implementation details**. Our method is built on 3DGS instead of NeRF due to the advantages of 3DGS on high training speed and real-time rendering. Following prevailing works [8; 9], the camera poses are known before optimization. The initialized point clouds are estimated by Structure from Motion (SfM)  only using the given sparse input views. The total training process involves 10K iterations for experiments on all datasets. The guidance of pseudo views starts from 2K iteration and ends at 9.5K iteration. Following FSGS, we introduce the proximity-guided Gaussian unpooling operation  and retain the high tolerance for large Gaussian points without size thresholds. For the score distillation methods, we randomly select one of 3 training views to generate BLIP-based

    &  &  &  \\   & & & SSIM\(\) & LPIPS\(\) & PSNR\(\) & AVGE\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & AVGE\(\) \\   SRF  \\ PixelNeRF  \\ MVSNeRF  \\  } &  Trained on \\ DTU \\  } & 0.250 & 0.591 & 12.34 & 0.313 & 0.671 & 0.304 & 15.32 & 0.171 \\  & & 0.272 & 0.682 & 7.93 & 0.461 & 0.695 & 0.270 & 16.82 & 0.147 \\  & & 0.557 & 0.356 & 17.25 & 0.171 & 0.769 & 0.197 & 18.63 & 0.113 \\   SRF ft.  \\ PixelNeRF ft.  \\ MVSNeRF ft.  \\  } &  Fine-tuned \\ per Scene \\  } & 0.436 & 0.529 & 17.07 & 0.203 & 0.698 & 0.281 & 15.68 & 0.162 \\  & & 0.438 & 0.512 & 16.17 & 0.217 & 0.170 & 0.269 & 18.95 & 0.125 \\  & & 0.584 & 0.327 & 17.88 & 0.157 & 0.769 & 0.197 & 18.54 & 0.113 \\   Mip-NeRF  \\ DietNeRF  \\ RegNeRF  \\ FreeNeRF  \\ SparseNeRF  \\  } &  Based on \\ NeRF \\ Optimized \\ per Scene \\  } & 0.351 & 0.495 & 14.62 & 0.246 & 0.571 & 0.353 & 8.68 & 0.323 \\  & & 0.370 & 0.496 & 14.94 & 0.240 & 0.633 & 0.314 & 11.85 & 0.243 \\  & & 0.587 & 0.336 & 19.08 & 0.149 & 0.745 & 0.190 & 18.89 & 0.112 \\  & & 0.612 & 0.308 & 19.63 & 0.134 & 0.787 & 0.182 & 19.92 & 0.098 \\  & & 0.624 & 0.328 & 19.86 & 0.127 & 0.769 & 0.201 & 19.55 & 0.102 \\   3DGS  \\ FSGS  \\ DNGaussian  \\  } &  ResNet \\ Optimized \\ per Scene \\  } & 0.456 & 0.385 & 14.97 & 0.208 & 0.795 & 0.178 & 15.06 & 0.136 \\  & & 0.682 & 0.248 & 20.43 & 0.108 & 0.825 & 0.145 & 17.69 & 0.101 \\  & & 0.591 & 0.294 & 19.12 & 0.132 & 0.790 & 0.176 & 18.91 & 0.102 \\   & & 0.687 & 0.228 & 19.94 & 0.109 & - & - & - & - \\   & & 0.702 & 0.207 & 20.44 & 0.101 & 0.856 & 0.121 & 19.99 & 0.077 \\   & & \(\)0.001 & \(\)0.001 & \(\)0.08 & \(\)0.001 & \(\)0.001 & \(\)0.001 & \(\)0.10 & \(\)0.001 \\    \(\): Using SfM initialization same as 3DGS, FSGS and Ours for fair comparisons.

Table 1: **Quantitative comparisons with other methods.** text prompts. Background priors are introduced on DTU for accurately reconstructing the object-of-interest. All experimental results are obtained on a single RTX 3090. More training details and experimental environments can be found in Appendix A.2 and A.3.

**Baselines**. Following prevailing works, we compare our method with the state-of-the-art methods, _i.e._SRF , PixelNeRF , MVSNeRF , Mip-NeRF , DietNeRF , RegNeRF , FreeNeRF , SparseNeRF , the vanilla 3DGS , FSGS  and DNGaussian  as our baselines. Except for the reproduced results of the 3DGS  on the LLFF dataset, and 3DGS  and FSGS  on the DTU dataset, the rest are based on the values reported. Since the original DNGaussian uses random initialization, while other 3DGS methods use SfM , we also report the provided LLFF results of using SfM . Reproduction details can be found in the Appendix A.4.

### Comparison with Other Methods

**LLFF**. The quantitative results on the LLFF dataset  are shown in Tab. 1. Our method shows significant improvement and achieves the best reconstruction quality among state-of-the-art methods under multi-metric evaluation. For the NeRF-based methods, SSIM of our method is improved by \(+12.5\%\) compared to SparseNeRF , and LPIPS is improved by \(+32.79\%\) compared to FreeNeRF , which are the state-of-the-art in the NeRF-based methods respectively. For the 3DGS-based methods, the AVGE of our method is improved by \(+6.48\%\) and \(+7.34\%\) compared to the state-of-the-art FSGS  and DNGaussian \(\) respectively. Note that the vanilla DNGaussian uses random initialization, but the 3DGS, FSGS, and our method use SfM initialization. Thus, we also report the provided results of SfM-initialized DNGaussian which is denoted by \(\). The qualitative results are shown in Fig. 4. Due to the lack of external priors, 3DGS  and FreeNeRF  show the optimization tendencies of 3D representations themselves, which are high-frequency artifacts and low-frequency smoothness respectively. Although DNGaussian  using external depth prior can suppress artifacts, it only uses coarse-grained depth guidance and lacks fine-grained visual guidance, so the rendered image lacks high-frequency information. Our approach achieves improvements in both visual and geometric quality.

**DTU**. Similar performances of the quantitative results on the DTU dataset  are shown in Tab. 1. The AVGE of our method is improved by \(+23.76\%\) compared to FSGS  and \(+21.43\%\) compared to FreeNeRF . Note that DNGaussian  does not provide the corresponding parameter settings for using SfM  initialization on the DTU dataset . The qualitative results are shown in Fig. 5. SparseNeRF  and DNGaussian , which only use depth priors, cannot obtain guidance on visual texture details, causing optimization difficulties. Our IPSM-Gaussian using diffusion priors can obtain textured details of reconstruction close to the Ground Truth.

Details of reported experimental results are shown in Appendix B.3. More rendered novel views and qualitative comparisons can be found in the Appendix B.8.

Figure 4: **Qualitative comparison on the LLFF dataset**.

Figure 5: **Qualitative comparison on DTU**.

### Ablation Study

We conduct detailed ablations of regularization terms on the LLFF dataset  shown in Tab. 2. We can notice that the first two regularization terms, _i.e._IPSM and depth, provide significant improvements. The first three lines demonstrate the promoting effect of our proposed IPSM on the reconstruction quality of 3D representations, _e.g._using IPSM boosts \(9.8\%\) on the LPIPS and \(9.6\%\) on the AVGE compared to the Base. It is worth noting that since the inline prior requires an accurate rendering depth from the unseen perspective shown in Eq. 8. The impact of depth error on inline priors is shown in Fig. 6 (a). However, the diffusion priors, as a kind of visual supervision, cannot provide direct depth geometry guidance, so an additional external depth prior needs to be introduced, which can support the accuracy of inline prior to further provide performance improvements. In Fig. 6 (b), we show the visual and geometry improvements of IPSM and depth regularization. The last line in Tab. 2 introduces the geometry consistency regularization for providing pixel-wise guidance, which shows a steady improvement. More additional ablations are detailed in the Appendix B.4.

### Comparison to SDS

As shown in Fig. 1, SDS guidance is hard to provide effective supervision but tends to hinder reconstruction due to the mode deviation we have analyzed. Due to the too-strong semantic visual supervision of SDS(CFG=100), the performance increases significantly in the final 500 iterations after the 2K-9.5K prior-added period instead. In this section, we report the final evaluated performance comparison of _Base_ (without any regularization), _w/ SDS(CFG=7.5), w/ SDS(CFG=100)_, and _w/ IPSM(CFG=7.5)_ in Tab 3. Except for SDS (CFG=7.5), which can provide a limited improvement in structural similarity compared to the Base, the other performances show a downward trend, which is colored by blue. However, IPSM can provide considerable improvements in multiple metrics which are colored by red. It is supposed to be noted that all the experiments of SDS shown in Fig. 1 and Tab. 3 are under the same experimental setting. We also present the qualitative comparison of SDS. As shown in Fig. 7 (a), the guidance of SDS will produce the imaginary reconstruction caused by mode deviation when using the diffusion prior directly. This property is reasonable and acceptable in text-to-3D generation tasks, but it fails in specific scene reconstructions limited by sparse views. As shown in Fig. 7 (b), we can observe that SDS will also produce large floaters during optimization,

which indicates the characteristic of its training instability since SDS overlooks the inline prior of sparse views and is hard to provide stable guidance towards target mode.

The experiments are conducted 3 times reporting the average results, and use the weight of 2.0 and the VAE encoder same as IPSM for fair comparisons. Since the feature domains of Stable Diffusion and Stable Diffusion Inpainting are identical, using the original VAE of Stable Diffusion shows similar performance, which is reported in the Appendix B.2. We have also analyzed the training instability of SDS additionally in Appendix B.1. Furthermore, we discuss the effects of using view-conditioned diffusion prior for SDS in Appendix B.6.

## 5 Conclusions and Limitations

In this paper, we start by revisiting the phenomenon where SDS not only fails to improve optimization in sparse-view 3D reconstruction but degrades performance. We present a comprehended analysis of SDS from a mode-seeking perspective. Based on these observations and analyses, we propose Inline Prior Guided Score Matching (IPSM), which utilizes the sparse-view input as the inline prior to rectifying the rendered image distribution. IPSM utilizes the rectified distribution as an intermediate state to decompose the mode-seeking optimization objective of SDS for controlling the optimization direction of mode-seeking to suppress mode deviation. We further propose the pipeline IPSM-Gaussian, which selects 3DGS as the backbone and incorporates IPSM with depth and geometry regularization for boosting IPSM. Experimental results on different public datasets show that our method achieves state-of-the-art reconstruction quality compared to other current methods.

The limitation of our method is that the rectified distribution needs to match the same feature space as the diffusion prior, which restricts the range of inpainting models used for the rectified distribution, thereby limiting the scalability and performance of our method. An alternative improvement could be substituting the pre-trained inpainting models with fine-tuning the diffusion prior like VSD. However, it would further increase the computational complexity of the method. We leave it as our future work.