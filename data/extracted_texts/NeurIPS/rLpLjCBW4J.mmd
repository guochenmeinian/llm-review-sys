# Acknowledgements

Preconditioning Matters: Fast Global Convergence of Non-convex Matrix Factorization via Scaled Gradient Descent

Xixi Jia\({}^{1}\) **Hailin Wang\({}^{2}\)**Jiangjun Peng\({}^{2}\)**Xiangchu Feng\({}^{1}\)1**Deyu Meng\({}^{2,3}\)

\({}^{1}\)School of Mathematics and Statistics, Xidian University

\({}^{2}\)School of Mathematics and Statistics, Xi'an Jiaotong University

\({}^{3}\) Macao Institute of Systems Engineering, Macau University of Science and Technology

{hsijiaxidian, andrew.pengjj}@gmail.com; wanghailin97@163.com

xcfeng@mail.xidian.edu.cn; dymeng@xjtu.edu.cn

###### Abstract

Low-rank matrix factorization (LRMF) is a canonical problem in non-convex optimization, the objective function to be minimized is non-convex and even non-smooth, which makes the global convergence guarantee of gradient-based algorithm quite challenging. Recent work made a breakthrough on proving that standard gradient descent converges to the \(\)-global minima after \(O(}{^{2}}}{}+}{^{2}}}{})\) iterations from small initialization with a very small learning rate (both are related to the small constant \(\)). While the dependence of the convergence on the _condition number_\(\) and _small learning rate_ makes it not practical especially for ill-conditioned LRMF problem.

In this paper, we show that precondition helps in accelerating the convergence and prove that the scaled gradient descent (ScaledGD) and its variant, alternating scaled gradient descent (AltScaledGD) converge to an \(\)-global minima after \(O(+)\) iterations from general random initialization. Meanwhile, for small initialization as in gradient descent, both ScaledGD and AltScaledGD converge to \(\)-global minima after only \(O()\) iterations. Furthermore, we prove that as a proximity to the alternating minimization, AltScaledGD converges faster than ScaledGD, its global convergence does not rely on small learning rate and small initialization, which certificates the advantages of AltScaledGD in LRMF.

## 1 Introduction

Low-rank matrix factorization aims to approximate a given rank \(d\) matrix \(^{m n}\) by the product of two factor matrices \(^{m d},^{n d}\), which plays a fundamental and essential role in low-rank matrix recovery such as matrix completion Jain et al. (2013); Ge et al. (2016); Sun and Luo (2016), matrix sensing Chi et al. (2019); Zhao et al. (2015); Charisopoulos et al. (2021), robust principal component analysis Candes et al. (2011); Cai et al. (2021), and the theoretical analysis of deep neural network Du et al. (2018). Meanwhile, low-rank matrix factorization is also viewed as a canonical problem in non-convex optimization as the objective function to be minimized is non-convex and even non-smooth. Mathematically, we are to solve

\[_{^{n d},^{n d}}f(, ):=\|^{}-\|_{F}^{2},\] (1)where \(d(m,n)\). Though problem (1) is not difficult to solve, the study of this problem has great significance to the gradient-based algorithm for low-rank matrix recovery Hou et al. (2020); Li et al. (2019); Chen et al. (2019); Tong et al. (2021), as it is exactly the population loss of low-rank matrix recovery models. Meanwhile, from the perspective of non-convex optimization, problem (1) is an ideal test bed for the theoretical analysis of the asymptotic convergence of gradient-based algorithm for non-convex optimization.

The theoretical guarantee for the global convergence of gradient-based algorithm for problem (1) is challenging, which is due to the following reasons: 1) the problem is non-convex with respect to the variables \(\) and \(\), and there are infinitely many local minima and saddle points. Specifically, if \(^{*}\) and \(^{*}\) is an optimal solution of problem (1), then \(^{*}\) and \(^{*}^{-}\) is also an optimal solution for any invertible matrix \(\); 2) the problem is non-smooth with respect to the variables \(\) and \(\) and is not coercive due to \(f(^{})=f(^{})\) where the scalar \(\) can be arbitrarily large or small. In theory, gradient-based algorithm is only able to find critical points, while practically, gradient descent algorithm has been verified to converge to the global minima of problem (1) efficiently.

To close the gap between theory and practice, Li et al. (2019); Ge et al. (2017); Chi et al. (2019) proved that even-though the loss in Eq. (1) is non-convex its loss landscape has some nice property: all local minima are global optima and all the saddle points are strict saddles. Therefore gradient descent algorithms can be guarantee to converge to the global minima. To help escape the strict saddles, Jin et al. (2017) proposed perturbed gradient descent by adding isotropic noise to the gradient at each iteration, they prove that with high probability, perturbed gradient descent converges to the global minima from random initialization at a linear rate. While as analyzed in Ye and Du (2021) and verified by experiments that the gradient perturbation is not really necessary for problem (1).

In contrast to the perturbed gradient descent, Du et al. (2018) studied the naive gradient descent for problem (1), they exploited the balancedness of the two factors \(\|\|_{F}^{2}-\|\|_{F}^{2}\) maintained by gradient flow and proved polynomial convergence rate of gradient descent for problem (1) when \(d=1\). Furthermore, Ye and Du (2021) improved the results of Du et al. (2018) to rank \(d\) case, they proved that gradient descent converges to the \(\)-global minima of problem (1) after \(O(}{^{2}}}{}+}{^{2}}}{})\) iterations. Ye and Du (2021) divided the convergence process into two stages: warm-up phase which takes \(O(}{^{2}}}{})\) iterations and local convergence phase which takes \(O(}{^{2}}}{})\) iterations. The warm-up phase in Ye and Du (2021) is actually the saddle avoid phase after which the gradient descent escapes all the saddle regions as shown in Fig. 1.

It can be seen from Fig. 1 and proved by Ye and Du (2021) that both the saddle avoid phase and local convergence phase of gradient descent highly rely on the condition number \(\) of the matrix \(\). If the condition number \(\) is large, then gradient descent takes long time to escape the saddle regions and also converges slowly. It is therefore very important to know _can we improve the gradient-based algorithm such that the global convergence is independent of the condition number?_ Besides, the global convergence of both Du et al. (2018) and Ye and Du (2021) require very small learning rate (related to the small constant \(\)), which seriously limits the application of gradient descent algorithm for ill-conditioned LRMF problem.

Recently, the scaled gradient descent algorithm (ScaledGD) Apuroop (2012); Mishra and Sepulchre (2016); Tanner and Wei (2016), has been proved by Tong et al. (2021); Tong (2022) to converge very fast from specialized initialization (spectral initialization) to the global minima of problem (1) and the convergence rate is independent of the condition number (Theorem 5 in Tong et al. (2021)). Yet the convergence result provided by Tong et al. (2021) is only local, whether the scaled gradient algorithm can escape saddle regions efficiently for the non-convex problem (1) is still not clear.

In this paper, we are the first to prove that ScaledGD as well as AltScaledGD converge to the global minima of problem (1) from general random Gaussian initialization, and the convergence rate is _independent of the condition number_ of the matrix \(\). Moreover, we show that the global convergence

Figure 1: Illustration of the global convergence of GD and AltScaledGD from random initialization.

results of ScaledGD and AltScaledGD _do not rely on small initialization_, the global convergence of AltScaledGD _does not even require a small learning rate_, which significantly improves the result of Ye and Du (2021). To sum up, the contributions of this paper are three-fold:

1. We provide a very simple proof framework for the convergence of ScaledGD and AltScaledGD from general random initialization. Specifically, we divide the optimization process into three phases: **initial phase, saddle avoid phase and linear convergence phase**. We prove that the loss decreases at rate \((1-)^{2k}\) in the initial phase, and further decreases linearly as \((1-_{k})^{k}\) in the saddle avoid phase and the linear convergence phase, where \(\) and \(_{k}\) are independent of the condition number \(\) and \(_{k}\) is monotonically increasing from \(}{(2-)^{2}}\) to \(\);
2. We prove that if the scale of the random initialization is smaller than a given constant (small initialization), then the loss decrease linearly as \((1-)^{k}\) from such small initialization to the global minima for both ScaledGD and AltScaledGD.
3. We show that AltScaledGD is a significant improvement of the ScaledGD in that it converges fast with large learning rate up to \(1\). While in contrast the learning rate of ScaledGD should be smaller than a constant \(c_{}\) that is much smaller than \(1\).

The organization of this paper is as follow. In Section 2, we introduce the related works on ScaledGD and AltScaledGD. In Section 3, we present our main results, then we give more detailed theoretical analysis on the proof sketch in Section 4. Finally, we conclude this work in Section 5.

## 2 Related work

In this section, we introduce the ScaledGD and the AltScaledGD as specified in Apuroop (2012), Mishra and Sepulchre (2016), Tanner and Wei (2016), Tong et al. (2021). We show that our work is a significant improvement to these existing works on the global convergence analysis.

### Scaled gradient descent

Different to the gradient descent algorithm which takes the negative gradient direction as the descent direction, scaled gradient descent is designed to accelerate the convergence process by scaled the gradient with a preconditioning matrix. Specifically, the ScaledGD updates the variables \(_{k}\), \(_{k}\) as

\[_{k+1}=_{k}-_{_{k}}f(_{k},_{k})(_{k}^{}_{k})^{-1}\\ _{k+1}=_{k}-_{_{k}}f(_{k},_{k})( _{k}^{}_{k})^{-1}\] (2)

where \(\) is the learning rate, \(_{k}^{}_{k}\) and \(_{k}^{}_{k}\) are matrices of \(d d\), and \(d(m,n)\) therefore the computation of ScaledGD is comparable to that of gradient descent. The inverse matrices \((_{k}^{}_{k})^{-1}\) and \((_{k}^{}_{k})^{-1}\) is the preconditioning for the gradient descent. If we denote \(=(,)\), then Eq. (2) corresponds to

\[_{k+1}=_{k}- f_{_{k}}(_{k})_{k}\] (3)

where \(_{k}=(_{k}^{}_{k})^{-1}&0\\ 0&(_{k}^{}_{k})^{-1}\).

Apuroop (2012), Mishra and Sepulchre (2016) proved that ScaledGD is derived by imposing a new metric on the tangent space of the Riemannian manifold. They verified empirically that ScaledGD converges much faster than gradient descent while there is no rigous convergence rate analysis. Recently, Tong et al. (2021) is the first to prove the linear convergence property of Eq. (2) for problem (1), while their proof relies on specialized initialization that \((_{0},_{*}) 0.1_{d}()\), where \(^{*}=(^{*},^{*})\) and \(^{*}{^{*}}^{}=\)(Theorem 5 Tong et al. (2021)). The local convergence guarantee is far from satisfactory to understand the convergence of ScaledGD for the non-convex optimization problem (1).

### Alternating scaled gradient descent

The Gaussian-Seidel version of ScaledGD is the following alternating scaled gradient descent which writes

\[_{k+1}=_{k}-_{_{k}}f(_{k},_{ k})(_{k}^{}_{k})^{-1}\\ _{k+1}=_{k}-_{_{k}}f(_{k+1},_{k})( _{k+1}^{}_{k+1})^{-1}\] (4)

Eq. (4) was studied as scaled alternating steepest descent algorithm in Tanner and Wei (2016), and it is also closely related to the alternating minimization algorithm for the minimization problem (1) when \(=1\), and other matrix recovery problem as Wen et al. (2012); Jain et al. (2013); Chandrasekher et al. (2022). Different to alternating minimization, the AltScaledGD presented in Eq. (4) can be broadly used in lots of low-rank matrix recovery problem where alternating minimization is computationally prohibitive, such as matrix completion Zilber and Nadler (2022); Sun and Luo (2016), matrix sensing Ma et al. (2021). Existing works for Eq. (4) only proved convergence to critical point as Wen et al. (2012); Tanner and Wei (2016), yet global convergence analysis of AltScaledGD is still vague.

In this paper, we provide rigorous proofs for the global convergence of ScaledGD Eq. (2) and AltScaledGD Eq. (4), and we show that both ScaledGD and AltScaledGD converge linearly for random Gaussian initialization after saddle avoid phase. Meanwhile, we show that AltScaledGD is robust to the learning rate \(\) which can be set as large as \(1\), while large \(\) can seriously deteriorates the convergence property of ScaledGD as illustrated by Fig. 6, which sheds light on the superiority of AltScaledGD Eq. (4) over ScaledGD Eq. (2) on problem (1) as well as more low-rank matrix recovery problem.

## 3 Main results

In this section, we present our main theorems on the convergence of ScaledGD and AltScaledGD for two different random initialization: general random initialization and small initialization. These two initializations are both random Gaussian initialization with zero mean but different variances. Small initialization is widely used in the convergence analysis of low rank matrix factorization problem Stoger and Soltanolkotabi (2021); Ye and Du (2021); Ma and Fattahi (2022), while small initialization is skin to spectral initialization which does not help us fully understand the global convergence of the non-convex problem. In this paper, we provide both the global convergence analysis of general random initialization and small initialization.

### Global convergence of ScaledGD

If the matrix \(\) is rank one, i.e., \(d\) in Eq. (1) is \(1\), then Eq. (2) is exactly gradient descent with adaptive step-size. We show that such specialized gradient descent for \(d 1\) converges linearly to the global minima after an initial decreasing phase and the convergence rate is independent of the singular value of \(\).

**Theorem 1** (General random initialization).: _Let \(_{0}^{m d}\) and \(_{0}^{n d}\) be random Gaussian that follow \((0,)\) for \(>c_{}\)\((c_{}\) is a positive constant), and \(_{k}\), \(_{k}\) are updated by Eq. (2). If \( c_{}<1\) for small constant \(c_{}\), we have that the objective function of problem (1) decreases linearly after \(T_{1}=O()\) iterations, namely_

\[\|_{k+T_{1}}_{k+T_{1}}^{}-\|_{F}_{1}(1- _{k+T_{1}})^{k}\|\|_{F}, k 0\] (5)

_where \(_{k+T_{1}}\) is monotonically increasing from \(}{(2-)^{3}}\) to \(\), \(\) is a sufficiently small constant, \(_{1}\) is a constant._

The Theorem 1 indicates that the global convergence of ScaledGD can be divided into three phases: the **initial phase** that lasts \(T_{1}\) iterations, the **saddle avoid phase** in which \(_{k+T_{1}}\) increases from \(\) to \(\) and the final **linear convergence phase** with convergence rate \(1-\). While if the scale of the initialization \(_{0}\) and \(_{0}\) are very small (with small \(\)), then the following theorem shows that the ScaledGD converges linearly without entering the saddle regions.

**Theorem 2** (Small initialization).: _Let \(_{0}^{m d}\) and \(_{0}^{n d}\) be random Gaussian that follow \((0,)\), with \( c_{}\) and \(_{k}\), \(_{k}\) are updated by Eq. (2). If \( c_{}<1\) for small constant \(c_{}\), we have that the objective function of problem (1) decreases linearly, namely_

\[\|_{k}_{k}^{}-\|_{F}_{2}(1-)^{k }\|\|_{F}\] (6)_where \(c_{}\) is a small constant and \(_{2}\) is a constant._

### Global convergence of AltScaledGD

We now present the main convergence results of AltScaledGD.

**Theorem 3** (General random initialization).: _Let \(_{0}^{m d}\) and \(_{0}^{n d}\) be random Gaussian that follow \((0,)\) for \(>c_{}\), \(_{k}\), \(_{k}\) are updated by Eq. (4), we have that the objective function of problem (1) decreases linearly after \(T_{1}=O()\) iterations, namely_

\[\|_{k+T_{1}}_{k+T_{1}}^{}-\|_{F}_{1}(1- _{k+T_{1}})^{k}\|\|_{F}\] (7)

_where \(_{k+T_{1}}\) is monotonically increasing from \(}{(2-)^{2}}\) to \(\), \(0< 1\) and \(_{1}\) is a constant._

**Theorem 4** (Small initialization).: _Let \(_{0}^{m d}\) and \(_{0}^{n d}\) be random Gaussian that follow \((0,)\), with \( c_{}\), \(_{k}\), \(_{k}\) are updated by Eq. (4) then we have that the objective function of problem (1) decreases linearly, namely_

\[\|_{k}_{k}^{}-\|_{F}_{2}(1-)^{ k}\|\|_{F}\] (8)

_where \(0< 1\) is the step size, \(_{2}\) is a constant and \(c_{}\) is a small constant._

The convergence results of ScaledGD and AltScaledGD are almost the same with differences in that for ScaledGD the learning \(\) should be smaller than a constant \(c_{}\) which is much less than \(1\). The small constant \(c_{}\) greatly restricts the convergence rate of ScaledGD. While for AltScaledGD the learning rate \(\) can be as large as \(1\), which indicates the superiority of AltScaledGD over ScaledGD in convergence as \(\) is crucial to the convergence rate. From the above theorems, it can be easily deduced that both ScaledGD and AltScaledGD converge to an \(\)-global minima after \(O(+)\) iterations from general random initialization. While for small initialization, these two algorithms only need \(O()\) iterations to converge to an \(\)-global minima. More detailed analysis and proofs are provided in Section 4 the proof sketch part and the supplementary materials.

### Why does preconditioning help?

In previous works, preconditioning has been used to improve the condition of the optimization problem Saad (2003); Zhang et al. (2021). In this paper, we analyze how does the preconditioning help improving the convergence by analyzing the effect of condition number \(\) on the learning rate \(\) as the convergence rate highly depends on the learning rate. For simplicity, we take one step of the AltScaledGD as example to analyze the effect of the preconditioning. Since \(f(,)\) is quadratic on \(\), then it can be verified

\[f(_{k+1},_{k}) f(_{k},_{k})+ f_{},+\|\|_{_{k}}^{2}\] (9)

where \(=_{k+1}-_{k}\) and \(\|\|_{_{k}}\) is a local norm defined by \(\|\|_{_{k}}=_{k}^{}_{k}, \).

For gradient descent, we take \(=- f_{}(_{k},_{k})\), then Eq. (9) becomes

\[ f(_{k+1},_{k})& f( _{k},_{k})-\| f_{}(_{k},_{k})\|_{F}^{ 2}+_{1}^{2}(_{k})}{2}\| f_{}(_{k}, _{k})\|_{F}^{2}\\ & f(_{k},_{k})-2_{d}^{2}(_{k})f( _{k},_{k})+^{2}_{1}^{4}(_{k})f(_{k},_{ k})\\ &(1-(2_{d}^{2}(_{k})-^{2}_{1} ^{4}(_{k})))f(_{k},_{k})\] (10)

Similarly, it holds

\[f(_{k+1},_{k+1})(1-(2_{d}^{2}(_{k+1})- ^{2}_{1}^{4}(_{k+1})))f(_{k+1},_{k})\] (11)

Therefore, we have

\[f(_{k+1},_{k+1})(1-(2_{d}^{2}(_{k+1})- ^{2}_{1}^{4}(_{k+1})))(1-(2_{d}^{2}(_ {k})-^{2}_{1}^{4}(_{k})))f(_{k},_{k})\] (12)To guarantee the linear convergence of gradient descent, it is required that \(2_{r}^{2}(_{k})^{2}_{1}^{4}(_{k})\) and \(2_{r}^{2}(_{k+1})^{2}_{1}^{4}(_{k+1})\) which implies \(\{^{2}(_{k})}(_{k})^{2}, {2}{_{1}^{2}(_{k+1})}(_{k+1})^{2}\}\)2. In contrast, if we take \(=- f_{}(_{k},_{k})(_{k}^{}_{k })^{-1}\), then

\[f(_{k+1},_{k})  f(_{k},_{k})-(-}{2}) (_{k}_{k}^{}-)_{k}_{k}^{},( _{k}_{k}^{}-)\] \[ f(_{k},_{k})-(-}{2})_{r} (_{E}^{}_{k})f(_{k},_{k}).\] (13) \[=(1-(-}{2})_{r}(_{E }^{}_{k}))f(_{k},_{k})\]

and similarly

\[f(_{k+1},_{k+1})  f(_{k+1},_{k})-(-}{2}) (_{k}_{k+1}^{}-^{})_{k+1} _{k+1}^{},(_{k+1}_{k}^{}-)\] \[ f(_{k+1},_{k})-(-}{2})_{r }(_{E}^{}_{k+1})f(_{k+1},_{k})\] \[=(1-(-}{2})_{r}(_{E }^{}_{k+1}))f(_{k+1},_{k})\] (14)

Thus

\[f(_{k+1},_{k+1})(1-(-}{2})_{r} (_{E}^{}_{k}))(1-(- {^{2}}{2})_{r}(_{E}^{}_{k+1}) )f(_{k},_{k}).\] (15)

To guarantee the linear convergence, we only need \(0<<2\). \(_{r}(_{E}^{}_{k+1})\) as well as \(_{r}(_{E}^{}_{k})\) is strictly larger than \(0\) (\(_{E}\) is the orthogonal row subspace of \(_{k}_{k}^{}-\) and \(_{k}\) is the orthogonal subspace of \(_{k}\)), which indicates that the linear convergence rate is independent of the condition number of matrix \(\).

We show in Fig. 2 that the convergence of ScaledGD and AltScaledGD are independent of the condition number \(\) of the matrix \(\) with general random initialization and small initialization. In Fig. 2, we set the rank of the matrix \(\) as \(5\), with condition number \(\) ranging from \(10,100,200\). It can be seen from the left subfigure of Fig. 2 that for general random initialization, the error curves of ScaledGD with different \(\) are the exactly the same, and the error curves of ScaledGD also coincide with that of the AltScaledGD. These results are also true for small initialization as shown in the right subfigure of Fig. 2. These observations certificate that preconditioning in Eq. (2) and Eq. (4) indeed help accelerating the convergence such that the convergence rate is independent of the condition number of the matrix \(\).

Figure 2: Illustration of convergence of ScaledGD and AltScaledGD under different condition \(\) and different initialization.

Theoretical analysis - proof sketch

In this section, we provide the proof sketch of our results in Section 3. For simplicity, we present the theoretical analysis for rank one matrix factorization where \(^{m 1}\) and \(^{n 1}\). More detailed proofs for the main theorems are provided in the supplementary material.

### Convergence of ScaledGD

It can be deduced that the objective function of problem (1) is upper bounded by four terms as

\[\|_{k+1}_{k+1}^{}-\|_{F} \|_{k}_{k}^{}- \|_{F}}_{}+(1-)\|_{F}\|_{* }^{}_{k}\|_{2}}_{}\] \[+(1-)\|_{F}\|_{*}^{ }_{k}\|_{2}}_{}+^{2}\|_{F}| 1-\|_{F}_{k}^{}_{k}^{}}{\|_{k}_{k}^{}\|_{F}}|}_{}\] (16)

where \(_{k}\) and \(_{k}\) correspond to the orthogonal basis of the column space of \(_{k}\) and \(_{k}\), \(_{*}\) and \(_{*}\) are the orthogonal complements of the left and right singular vector matrices of \(\) (i.e. \(_{*}\), \(_{*}\)), and \(_{k}^{}\) is cosine value of the angle between the vectors \(_{k}\) and \(_{*}\), \(_{k}^{}\) is cosine value of the angle between the vectors \(_{k}\) and \(_{*}\). The upper-bound depicts the differences between \(_{k+1}_{k+1}^{}\) and \(\) in two aspects

* The angle between the subspace of \(_{k}_{k}^{}\) and \(\): \(\|_{*}^{}_{k}\|_{2}\), \(\|_{*}^{}_{k}\|_{2}\);
* The difference of the length (norm) between \(_{k}_{k}^{}\) and \(\): \(\|\|_{F}-\|_{k}_{k}^{}\|_{F}\).

The term 2 and 3 are related to the angle between the subspace of \(\) and \(_{k}_{k}^{}\), the term 4 is related to the difference between the norm of \(\) and \(_{k}_{k}^{}\), as given by the following lemma.

**Lemma 1**.: _If \(,_{k}_{k}^{}\|_{k} {V}_{k}^{}\|_{F}^{2}\), then there is constant \(C_{u} 0\) such that_

\[|1-\|_{F}_{k}^{} {}_{k}^{}}{\|_{k}_{k}^{}\|_{F}}| C_{u }(\|\|_{F}-\|_{k}_{k}^{}\|_{F})\] (17)

According to Eq. (16), we know that the decrease of the objective function in problem (1) is decided by the decrease of the distance between the subspace (2 and 3) and the difference between the norm of \(\) and \(_{k}_{k}^{}\) (4). The following lemma further reveals that the distance between the subspace of \(_{k}_{k}^{}\) and \(\) decreases.

**Lemma 2**.: (Convergence of the distance between subspaces) _For the ScaledGD (2), if \(\|\|_{F}_{k}^{}_{k}^{ }\|_{k}_{k}^{}\|_{F}\), then the following holds_

\[\|_{*}^{}_{k+1}\|_{2}(1-)\|_ {*}^{}_{k}\|_{2},\ \ \|_{*}^{}_{k+1}\|_{2}(1-)\|_{* }^{}_{k}\|_{2}\] (18)

The Lemma 2 indicates that the term 2 and 3 in Eq. (16) decrease linearly if the norm of \(\|_{k}_{k}^{}\|_{F}\) is smaller than norm of the projection of \(\) onto the column and row spaces of \(_{k}_{k}^{}\). At the mean time, the condition \(\|\|_{F}_{k}^{}_{k}^{ }\|_{k}_{k}^{}\|_{F}\) also guarantees the linear convergence of the differences between the norm of \(_{k}_{k}^{}\) and \(\).

**Theorem 5**.: (Convergence of the matrix norm) _For the ScaledGD (2), if \(\|\|_{F}_{k}^{}_{k}^{ }\|_{k}_{k}^{}\|_{F}\) for all \(k 0\), then we have_

\[\|\|_{F}-\|_{k+1}_{k+1}^{}\|_{F}(1-)^{2k}kC_{}\] (19)

_where \(C_{}\) is a constant and \(\) is the step length \(0<1\)._

Both Lemma 2 and Theorem 5 are built on the condition that \(\|\|_{F}_{k}^{}_{k}^{ }\|_{k}_{k}^{}\|_{F}\) for all \(k 0\), while this is not a trivial condition for ScaledGD. The following lemma guarantees that the condition can be satisfied if the step length \(\) is smaller than a constant.

**Lemma 3**.: _Let \( c_{}<1\) with \(c_{}\) a small constant, if \(\|\|_{F}_{0}^{}_{0}^{ }\|_{0}_{0}^{}\|_{F}\) then the following is true_

\[\|\|_{F}_{k}^{}_{k}^{ }\|_{k}_{k}^{}\|_{F}, k>0.\] (20)The above results guarantee the local linear convergence of the term 2, 3 and 4 on the condition that

\[\|\|_{F}_{0}^{u}_{0}^{v} \|_{0}_{0}^{}\|_{F}\] (21)

which is critical for our analysis on random Gaussian initialization and small initialization.

#### 4.1.1 Small initialization

In practice, the condition \(\|\|_{F}_{0}^{u}_{0}^{v} \|_{0}_{0}^{}\|_{F}\) can be easily satisfied by very small (near zero) initialization. According to the random matrix theory Theorem 2.7.5 in Tao (2012), for Gaussian initialization there exists \(>0\) such that with high probability \(_{0}^{u}\) and \(_{0}^{v}\) is lower bounded by constant \(1/\), therefore one can simply set the norm of \(_{0}\) and \(_{0}\) to be sufficiently small such that the inequality (21) holds. In consequence small initialization can guarantee the global linear convergence of ScaledGD, as shown in Fig. 3. While small initialization is very special. it can not helps us fully understand the global convergence property of ScaledGD from arbitrary initialization for the non-convex objective (1), even though small initialization has been widely used in the global convergence analysis of gradient descent algorithms Stoger and Soltanolkotabi (2021); Ye and Du (2021); Ma and Fattahi (2022) and ScaledGD for symmetric low rank matrix recovery problems Xu et al. (2023); Zhang et al. (2021).

#### 4.1.2 General random initialization

In order to understand the optimization path of ScaledGD for the non-convex objective (1), we present the theoretical analysis of ScaledGD from random Gaussian initialization that may not satisfy the condition in Eq. (21). As shown in Fig. 3, when initialized with \(=1\) ScaledGD iterations are also attracted by the saddle point thus enter the saddle region (zoomed region marked by red rectangle), while it can escape saddle region very fast. To rigorously characterize the saddle avoid phase, we first show and prove that the norm of matrices \(_{k}\) and \(_{k}\) decrease if \(\|\|_{F}\{_{k}^{u},_{ k}^{v}\}<\|_{k}_{k}^{}\|_{F}\) as given by the following lemma and shown in Fig. 4.

**Lemma 4**.: _If the condition \(\|\|_{F}\{_{k}^{u},_{ k}^{v}\}<\|_{k}_{k}^{}\|_{F}\) is satisfied then we have_

\[\|_{k+1}\|_{F}<\|_{k}\|_{F}\|_{k+1}\|_ {F}<\|_{k}\|_{F}.\] (22)

_Furthermore, if the condition \(\|\|_{F}_{k}^{u}_{k}^{v} \|_{k}_{k}^{}\|_{F}\) is satisfied then we have_

\[\|_{k+1}\|_{F}\|_{k}\|_{F}\|_{k+1 }\|_{F}\|_{k}\|_{F}.\] (23)

In general, if we initialize the matrices \(\) and \(\) as \((0,)\) with large \(\), then in the initial phase, with high probability we have \(\|\|_{F}\{_{0}^{u},_{ 0}^{v}\}<\|_{0}_{0}^{}\|_{F}\). According to Lemma 3 and Lemma 4, we know that the norm of the matrices \(_{k}\) and \(_{k}\) decreases with the increase of \(k\) until it reaches the condition \(\|\|_{F}_{k}^{u}_{k}^{v} \|_{k}_{k}^{}\|_{F}\), which is also illustrated in Fig. 4. In Fig. 4, we plot the changes of the norm of matrices \(\) and \(\), the nested subfigure illustrates the the matrix norm in log scale. It is very interesting to study the changes of the matrix norm with respect to the optimization path. Generally, if \(\|_{0}\|_{F}\) and \(\|_{0}\|_{F}\) is initialized very large, then the decrease of the norm will decrease the objective function (1). Meanwhile, \(=\) and \(=\) is a saddle point of the objective function (1), the results in Lemma 4 thus indicate that the matrices \(_{k}\) and \(_{k}\) are updated toward the saddle point zero. While interestingly, as shown in Fig. 4 the matrix norm decreases to a magnitude which is strictly larger than zero, then the matrix norm begins to increase. These observation indicates that ScaledGD can escape from the saddle point zero, the saddle avoid phase is also illustrated in Fig. 5.

**Analysis on the entire iteration process.** It can be easily deduced from Eq. (16) that

\[\|_{k}_{k}^{}-\|_{F}<(1-)^{2k}\|_{0}_{0} ^{}-\|_{F}+\|\|_{F},\] (24)

Figure 3: Global convergence of small initialization and general random initialization.

therefore after \(T_{1}=O()\) iterations (for sufficiently small \(\))3, we have

\[\|_{k}_{k}^{}-\|_{F}\|\|_{F},\; k T_{ 1},\] (25)

which indicates that \(\|\|_{F}{ cos}_{k}^{n}{ cos}_{k}^{n} {1}{2}\|_{k}_{k}^{}\|_{F}\). We term this period of time the **initial phase**. The following lemma tells that after \(T_{1}\) iterations the term 2, 3 and 4 decrease linearly.

**Lemma 5**.: _After \(T_{1}\) iterations of ScaledGD, the following inequalities hold \( k T_{1}\)_

\[\|_{*}^{}_{k+1}\|_{2}(1-_{k} )\|_{*}^{}_{k}\|_{2}\] (26)

\[\|_{*}^{}_{k+1}\|_{2}(1-_{k} )\|_{*}^{}_{k}\|_{2}\] (27)

\[1-{ cos}_{k+1}^{n}{ cos}_{k+1}^{v}(1- _{k})^{2}(1-{ cos}_{k}^{n}{ cos}_{k}^{v})\] (28)

_where \(_{k}=}{1-(1-_{k})}<1\) and \(_{k}=\|_{F}{ cos}_{k}^{n}{ cos}_{ k}^{n}}{\|_{k}_{k}^{}\|_{F}}[1/2,1]\)._

If \(\|_{k}_{k}^{}\|_{F}\|\|_{F}{ cos}_{k}^{n}{ cos}_{k}^{n}\|_{k}_{k}^{}\| _{F}\) and \(\|\|_{F}\|_{k}_{k}^{}\|_{F}\), we have that the term 4 in Eq. (16) is upper bounded by \(1-{ cos}_{k}^{n}{ cos}_{k}^{v}\). Thus the above Lemma 5 indicates that after \(T_{1}\) iterations, the objective function decreases at rate \(1-_{k}\). Meanwhile, \({ cos}_{k}^{n}\) and \({ cos}_{k}^{v}\) are increasing, and \(\|_{k}_{k}^{}\|_{F}\) continues to decrease until \(\|\|_{F}{ cos}_{k}^{n}{ cos}_{k}^{n}\| {U}_{k}_{k}^{}\|_{F}\), which means the value \(_{k}=\|_{F}{ cos}_{k}^{n}{ cos}_ {k}^{n}}{\|_{k}_{k}^{}\|_{F}}\) is monotonically increasing with the increase of \(k\) until up to \(1\). In consequence, the \(_{k}\) is monotonically increasing from \(\) to \(\). We name the period in which \(_{k}\) increases from \(\) to \(\) the **saddle avoid phase** as shown in Fig. 54. The Lemma 5 also indicates that the ScaledGD escapes saddle points exponentially fast. After the saddle avoid phase, the ScaledGD converges to the global minima at rate \(1-\) according to the analysis in Section 4.1.1, since \(\|\|_{F}{ cos}_{k}^{n}{ cos}_{k}^{n}\| {U}_{k}_{k}^{}\|_{F}\), we name this period the **linear convergence phase** as shown in Fig. 5.

### Convergence of AltScaledGD

The convergence analysis of the AltScaledGD is similar to that of ScaledGD, while different to ScaledGD, the objective function (1) is upper-bounded by three terms in AltScaledGD as

\[\|_{k+1}_{k+1}^{}-\|_{F} \|_{k}_{k}^{}-\|_{F}}_{}}+(-^{ 2})\|_{F}\|_{*}^{}_{k}\|_{2 }}_{}}+\|_{F}\|_{* }^{}_{k+1}\|_{2}}_{}}\] (29)therefore, the analysis of AltScaledGD for (1) is much easier than that of the ScaledGD in Eq. (16). Specifically, we only need to guarantee that the distance between subspaces of \(_{k}\) and \(_{*}\) (\(\|_{*}^{}_{k}\|_{2}\)), \(_{k}\) and \(_{*}\) (\(\|_{*}^{}_{k}\|_{2}\)) decrease linearly. The Lemma 2 also holds for AltScaledGD as

**Lemma 6**.: (Convergence of the distance between subspaces) _For AltScaledGD (4), if \(\|\|_{F}_{k}^{u}_{k}^{v} \|_{k}_{k}^{}\|_{F}\) and \(0< 1\), then the following holds_

\[\|_{*}^{}_{k+1}\|_{2}(1-)\| _{*}^{}_{k}\|_{2},\ \ \|_{*}^{}_{k+1}\|_{2}(1-)\|_{* }^{}_{k}\|_{2}.\] (30)

The condition in Lemma 6 can be satisfied \( k\) if \(\|\|_{F}_{0}^{u}_{0}^{v} \|_{0}_{0}^{}\|_{F}\) and \(0< 1\) as specified by the following lemma, the condition is mild compared to the condition in Lemma 3.

**Lemma 7**.: _For AltScaledGD (4), if \(\|\|_{F}_{0}^{u}_{0}^{v} \|_{0}_{0}^{}\|_{F}\) and \(0< 1\), then the following is true_

\[\|\|_{F}_{k}^{u}_{k}^{v} \|_{k}_{k}^{}\|_{F}, k>0.\] (31)

The convergence analysis of AltScaledGD is the same as that of the ScaledGD in Section 4.1 with small initialization and general Gaussian initialization (the three phases convergence). The main difference between ScaledGD and AltScaledGD is that \(\) in ScaledGD should be small such that \( c_{}<1\), while for AltScaledGD \(\) can be as large as \(1\). It can be seen from Eq. (29) that if \(=1\), the the AltScaledGD Eq. (4) converges to the global minima in just one iteration. We also illustrate the convergence of ScaledGD Eq. (2) and AltScaledGD Eq. (4) with respect to the learning rate \(\) in Fig. 6. It can be seen from Fig. 6 that for small learning rate \(=0.1\), the convergence property (the loss curve) of AltScaledGD is almost exactly the same as ScaledGD, while for large learning rate \(=0.8\), the AltScaledGD converges very fast, in contrast the ScaledGD does not converge as the condition \(\) is not satisfied according to Lemma 3. These results certificates the superiority of AltScaledGD over ScaledGD, since both ScaledGD and AltScaledGD converges fast with large \(\), while the learning rate \(\) is upper-bounded by a small constant \(c_{}\) in ScaledGD.

## 5 Conclusion

In this work, we are the first to rigorously prove the global convergence of ScaledGD and AltScaledGD for the non-convex low rank matrix factorization problem and show that thanks to the preconditioning matrices the global convergence rate of ScaledGD and AltScaledGD are independent of the condition number of the matrix \(\), thus they converge faster than gradient descent algorithm for ill-conditioned problem. We further prove that ScaledGD and AltScaledGD converges linearly from both small initialization and general random initialization, which is in contrast to the existing global convergence analysis that are only applicable to small initialization. Meanwhile, we show that compared to ScaledGD, AltScaledGD is more practical as it enables larger learning rate thus converges fast.

**Limitations.** This paper concerns low-rank matrix factorization which is the population loss of the more general low-rank matrix recovery problem, such as matrix completion and matrix sensing. While the empirical loss is different to the population loss in that the number of the samples is limited, therefore our results can not directly applied to general low-rank matrix recovery. Our further work is to study the empirical loss with the help of RIP condition for matrix sensing and the sampling lower-bound for matrix completion.