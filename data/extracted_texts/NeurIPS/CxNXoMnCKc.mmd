# PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action

Yijia Shao

Stanford University

shaoyj@stanford.edu

&Tianshi Li

Northeastern University

tia.li@northeastern.edu

&Weiyan Shi

Northeastern University

we.shi@northeastern.edu

&Yanchen Liu

Harvard University

yanchenliu@g.harvard.edu

&Diyi Yang

Stanford University

diyiy@stanford.edu

###### Abstract

As language models (LMs) are widely utilized in personalized communication scenarios (_e.g._, sending emails, writing social media posts) and endowed with a certain level of agency, ensuring they act in accordance with the contextual privacy norms becomes increasingly critical. However, quantifying the privacy norm awareness of LMs and the emerging privacy risk in LM-mediated communication is challenging due to (1) the contextual and long-tailed nature of privacy-sensitive cases, and (2) the lack of evaluation approaches that capture realistic application scenarios. To address these challenges, we propose PrivacyLens, a novel framework designed to extend privacy-sensitive seeds into expressive vignettes and further into agent trajectories, enabling multi-level evaluation of privacy leakage in LM agents' actions. We instantiate PrivacyLens with a collection of privacy norms grounded in privacy literature and crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM performance in answering probing questions and their actual behavior when executing user instructions in an agent setup. State-of-the-art LMs, like GPT-4 and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even when prompted with privacy-enhancing instructions. We also demonstrate the dynamic nature of PrivacyLens by extending each seed into multiple trajectories to red-team LM privacy leakage risk. Dataset and code are available at https://github.com/SALT-NLP/PrivacyLens.

## 1 Introduction

Recent advancements in language models (LMs) have led to new applications, such as LM agents  that can assist users in handling everyday communication tasks (_e.g._, sending emails, making social media posts, _etc._). Equipped with tool use or retrieval-augmented generation capabilities, LMs can access sensitive data **at inference time**. Consequently, their unawareness of the privacy norms, _i.e._, the appropriateness of the data flow in the data sharing context , could lead to unintentional privacy leakage, even **without malicious attackers** involved. For example, as illustrated in Figure 1, it is undesirable for an LM agent to share the information that John is _"talking to a few companies about switching jobs"_ when assisting John in sending an "email" to "John's manager" without John's explicit consent. It is challenging to balance the LM's **agency** with users' privacy expectations, because the privacy management process may involve respecting privacy norms in context and taking into account individual preferences and knowledge . This raises anemerging privacy risk that differs from widely studied risk models with intentional attackers, such as training data extraction [6; 37] and membership inference attacks (MIA) [31; 10; 27].

Recent efforts to evaluate the privacy reasoning capabilities of LMs involve probing them with targeted questions [33; 52; 25]. While such evaluation setups are straightforward to implement and offer essential insights, such as LMs' sensitivity to privacy-related words and their ability to determine information accessibility, a growing amount of work has highlighted a potential disconnection between LMs' performance on these probing tasks and their actual behavior in applications [20; 29; 11; 42]. We focus on evaluating LMs' privacy norm awareness **in action** by grounding our setup in a critical family of applications, _i.e._, LM agents that directly interact with tools such as users' calendar or email. Compared to single-turn probing questions, such evaluation requires collecting agent _trajectories_, which demands expert construction due to the complex logic involved . It is even more challenging to evaluate them from a privacy perspective because it focuses on worst-case scenarios that may be very rare but consequential . Moreover, privacy sensitivity is context-dependent. In Figure 1's example, the data flow would turn acceptable if the information comes from a virtual meeting transcript where the manager is also present, rather than from the user's personal calendar without the user's explicit consent. Generating such contexts can be difficult, as they are inherently unstructured and subject to subtle changes .

We address these challenges by proposing **PrivacyLens**, a procedural data construction and multi-level evaluation framework to evaluate privacy norm awareness of LMs _in action_. PrivacyLens starts with collecting privacy norms using a generic schema informed by the _Contextual Integrity_ theory . This theoretical framework helps characterize privacy norms with nuanced consideration of who the information is about, the social relationship between the sender and the recipient, and the method of information transmission . To evaluate LMs in action, we use these norms as _privacy-sensitive seeds_ (Figure 1 Left) and employ a template-based generation method to expand them into expressive _vignettes_ describing scenarios where the sensitive data transmission could happen. Finally, we build a simulated sandbox environment where the LM agent can interact with a set of tools (_e.g._, email, calendar, personal notebook, _etc._) to further obtain agent trajectories from the seed and vignette (Figure 1 Right).

We initiate PrivacyLens by collecting privacy-sensitive seeds from U.S. privacy regulations, privacy research literature on vulnerable groups, and crowdsourcing. In total, we gather 493 seeds and extend them into 493 expressive vignettes and 493 trajectories. We evaluate a series of LMs using QA probing with 1,479 questions and an LM agent setup with the trajectories. While closed-source LMs (_e.g._, GPT-4) perform well in the probing evaluation, GPT-4 still leaks information in 25.68% of cases in the action-based evaluation, even with privacy-enhancing prompt engineering. This leakage rate is concerning, as we focus on worst-case evaluation and privacy leakage may lead to consequential

Figure 1: **Risk Model of PrivacyLens. PrivacyLens quantifies an emerging LM privacy risk where LMs unintentionally leak private information when assisting human communication. The risk model involves three primary actors: (1) a sender, who is a daily user instructing an LM to assist in communication; (2) a recipient, who is specified in the user instruction; (3) an LM agent, who gets access to sensitive information through tool use (_e.g._, reading events from the user’s personal calendar). The privacy leakage arises when the LM agent shares a piece of information (_e.g._, “lunch with TechAdvance Recruiter”) in its final action, and the information flow violates a privacy norm.**

outcome . Furthermore, we demonstrate the dynamic nature of PrivacyLens by introducing variations to the vignette generation process, yielding more vignettes and trajectories. This approach holds the potential to mitigate data contamination and support comprehensive red-teaming.

## 2 Related Work

Language Model PrivacyAs shown in Table 1, previous research on evaluating LM privacy has focused on whether these models memorize training data and if malicious attackers can extract sensitive information from them [6; 26; 60; 10]. However, privacy risks go beyond memorization . As LMs are increasingly applied to complex everyday tasks, private information can be easily exposed at inference time. These models may share such information in their generated texts, potentially violating social norms specific to the context . Accordingly, prior work has focused on testing attribute inference or privacy-sensitive prompt injection, yet lacks systematic studies of LM privacy risks [48; 52]. The most relevant work is ConfAIde , which evaluates whether LMs can reason about contextual privacy. However, ConfAIde primarily employs probing questions and only covers a single application of meeting summary and action-item generation with 20 test cases. Thus, it remains unclear whether LMs could unintentionally leak sensitive information presented at inference time in agentic applications. In this work, we propose PrivacyLens to study this emerging LM privacy leakage risk.

Evaluating Language Model AgentsRecent advancements in LMs have led to their rapid expansion in agent-based applications. Current LM agent benchmarks typically evaluate their capabilities across various domains, including web environments [55; 58; 9], game playing , coding [23; 28; 45], social interactions , _etc._[19; 30; 44]. However, in addition to high task completion rates, an ideal LM agent should also consider the consequences of its actions when completing tasks on behalf of the user. To this end, Naihin et al.  and Yuan et al.  manually craft risky agent trajectories to assess whether LMs can be used to monitor or judge unsafe actions of LM agents. This manual approach is labor-intensive and prone to becoming outdated due to issues of data contamination. Addressing this, Ruan et al.  proposes ToolEmu, an LM-based emulation framework designed to evaluate tool-use LM agents. Despite these developments, to the best of our knowledge, no existing research focuses on evaluating LM agent actions from the privacy perspective.

Language Model Assisted EvaluationGiven the high costs and limited coverage of human-annotated datasets, previous studies have leveraged the instruction-following ability of LMs to generate test cases for assisting the evaluation of LMs themselves [17; 12; 15]. More recent work further develops data construction framework using LMs to discover novel test cases , facilitate red teaming , and understand social reasoning in LMs . Drawing inspiration from these advancements, our work introduces a procedural data construction pipeline that utilizes LMs to construct vignettes and LM agent trajectories from privacy-sensitive seeds.

## 3 PrivacyLens

In this section, we define the risk model that serves as the focus of our evaluation (SS3.1), the PrivacyLens framework, which comprises a procedural data construction pipeline (SS3.2) and multi-level evaluation of LM privacy norm awareness (SS3.3). Appendix F documents prompts in PrivacyLens.

### Risk Model behind PrivacyLens

PrivacyLens focuses on the emerging **unintentional LM privacy leakage** risk caused by the **privacy norm unawareness** of LMs. Our risk model (depicted in Figure 1) involves three primary actors: (1)

    & **Exposure** & **Leakage** & **Action-Based** \\  & **Time** & **Type** & **Eval7** \\  Evaluating MIA on LM  & Training & Intentional & No \\ LM Extraction Benchmark  & Training & Intentional & No \\ Decoding Trust  & 8.1 & Training & Intentional & No \\ Decoding Trust  & 88.2,8.3 & Inference & Intentional & No \\ ConfAIde  & Inference & Unintentional & No \\  PrivacyLens (Ours) & Inference & Unintentional & Yes \\   

Table 1: Comparison of PrivacyLens with previous work on evaluating LM privacy. “Unintentional Leakage” refers to data leaks without malicious attackers. “Action-Based Eval” refers to evaluating the actions performed by LM agents.

a sender, who provides an instruction \(i\) that involves sharing information with a recipient (_e.g._, "Help me reply to an email"), but may underspecify the appropriateness of sharing certain information; (2) a recipient, who the sender specifies to receive the information; (3) an LM agent, which executes \(i\) through a sequence of tool use actions \(a\), represented as \(\{a_{1},o_{1},,a_{n-1},o_{n-1},a_{n}\}\), where \(o\) denotes the observations from the environment. Notably, unlike traditional risk models, our risk model does not involve malicious actors. The privacy leakage arises when a piece of information gathered in the agent trajectory, \(\{a_{1},o_{1},,a_{n-1},o_{n-1}\}\), is shared with the recipient in the agent's final action \(a_{n}\), and the information flow violates privacy norms.

### Data Construction Pipeline

Collecting Contextual Privacy SeedsTo quantify the privacy norm awareness of LMs in action, we start with privacy-sensitive seeds that encapsulate a privacy-norm-violating scenario . Drawing from the _Contextual Integrity_ theory, we define the privacy-sensitive seed \(\) with a 5-tuple: (1) _data type_, the attribute or information type; (2) _data subject_, the subject of the information that is being transferred; (3) _data sender_, the sender of the information; (4) _data recipient_, the recipient of the information; (5) _transmission principle_, the information transmission method or condition imposed. The seed \(\) delineates potentially inappropriate information transmissions and specifying all five elements makes the seed contextual, as altering any single element could shift the expected privacy norms (see Appendix B.2 for examples).

Extending Contextual Seed into VignetteAlthough our theory-based schema enables the privacy-sensitive seeds to be contextual, these seeds have limited details. For instance, the seed in Figure 1 does not specify the circumstances under which John emails his manager. To evaluate whether LMs can identify potentially sensitive data transmissions in detailed scenarios, we extend the seed into a vignette \(\), _i.e._, a short story, using a template-based generation method with GPT-4. The vignette (Figure 2 (B)) includes more details to reflect the real-world complexity.

Since vignettes are extended from privacy-sensitive seeds, direct generations from GPT-4 often include terms explicitly indicating sensitivity, _e.g._, "sensitive", "private", "confidential". However, private issues in daily communication are typically implicit and nuanced. To mirror such subtleties, we require that the generated vignettes exclude these restricted words. To achieve this, we introduce a _Surgery Kit module_ that refines model outputs to meet specific criteria established by unit tests. As outlined in Algorithm 1, this module takes in the initial output alongside a set of unit tests, and uses

Figure 2: **Data construction pipeline in PrivacyLens.** PrivacyLens starts with contextual privacy-sensitive seeds (A). It extends each seed into a vignette (B) with more details through template-based generation. The seed and vignette will be used to guide the emulator in sandbox simulation to get an LM agent trajectory (C). We employ the Surgery Kit module to improve the vignette and trajectory quality based on unit tests and LM refinement.

an LM to refine the text based on the repair instruction associated with the failed test. In vignette generation, we define a deterministic keyword detection function as the unit test and instruct the LM to remove these keywords when the test fails.

Constructing Executable Agent TrajectoryTo collect agent trajectories at scale, we develop a sandbox environment leveraging ToolEmu . Within the sandbox, the LM agent can interact with a suite of tools, _e.g._, calendar, email, social media, personal notebook, _etc._ (see Appendix C for details), by generating a series of actions \(a\) towards fulfilling the user instruction \(i\). The observations \(o\) from these tool interactions are generated by a GPT-4 emulator, eliminating the need to rely on real user data within the sandbox. After multiple interactions between the LM agent and the sandbox emulator, we obtain an agent trajectory \(\{a_{1},o_{1},,a_{n-1},o_{n-1},a_{n}\}\), which ultimately completes \(i\) with the final action \(a_{n}\). To prepare this trajectory for evaluation across different LMs, we remove the final action \(a_{n}\) (_e.g._, sending an email or message) where data transfer occurs. The whole process results in an excutable trajectory \(=\{a_{1},o_{1},,a_{n-1},o_{n-1}\}\) as a component of each data point in PrivacyLens.

To enhance worst-case evaluation efficiency, we incorporate risk-triggering elements into trajectory construction. Based on our risk model (SS3.1), we use GPT-4 to generate an underspecified user instruction \(i\) given each privacy-sensitive seed \(\) and its corresponding vignette \(\). The sandbox emulator is instructed to generate observations based on \(\) and \(\), thus creating adversarial scenarios that are likely to expose privacy risks. To ensure these agent trajectories do not have placeholders and reflect the intended privacy-sensitive contexts, we again utilize the Surgery Kit module. This module edits each \(o\) locally to eliminate placeholders and globally refines \(\) to enhance its alignment with \(\).

### Multi-Level Evaluation

PrivacyLens evaluates LM privacy awareness in two ways, with \(,\) and \(\) in each data point:

``` Input: Original output \(x\), model \(\), unit tests \(=\{u_{1},,u_{k}\}\), corresponding refining instructions \(=\{i_{1},,i_{k}\}\), maximum iterations \(n\) Output: Refined output \(\), success flag \(s\)
1for\(i=1\) to \(n\)do
2\(p true\)for\(j\) = 1 to \(k\)do
3ifnot\(u_{j}(x)\)then
4\(p false\)\(x(,x,i_{j})\)
5if\(p\)then
6return\(x\), \(true\)
7
8for\(j=1\) to \(k\)do
9ifnot\(u_{j}(x)\)then
10return\(x\), \(false\)
11
12
13return\(x\), \(true\) ```

**Algorithm 1**Surgery Kit Module

Probing in Question-Answering FormatWe first assess LMs on their ability to identify sensitive data transmission through probing in QA format. For \(\) and \(\) of each data point, we prompt the LM to answer _"Is it acceptable for [data sender] to share [data subject]'s [data type] with [data recipient] by [transmission principle]?"_ This probing question is taken from Shvartzshnaider et al. , which was originally used to survey human privacy expectations. For the agent trajectory \(\), we formulate the probing question by emphasizing the agent is sending information on behalf of the user, as exemplified in Figure 3. Since \(\) is privacy-sensitive and \(,\) are constructed from it, we expect the LM to answer "No" for all these probing questions. We use _accuracy_ as the metric.

Evaluating LMs in Action with Agent SetupNext, we evaluate LMs in action with agent trajectories. For each trajectory \(\), we prompt the LM to generate the final action, denoted as \(a_{}\), to complete the corresponding user instruction. We evaluate whether \(a_{}\) leaks any information about the data type in \(\). To streamline the evaluation process, we extract a list of sensitive information items, \((,)=\{i_{1},,i_{m}\}\), from trajectory \(\) based on its corresponding seed \(\). We build an LM-based few-shot classifier \(f\) to judge whether each information item can be inferred from the action. \(a_{}\) leaks the information if \( t\{1,,m\}\), \(i_{t}\) can be inferred from \(a_{}\). The _leakage rate_ (**LR**) on the evaluation dataset \(\) is defined as the percentage of \(a_{}\) that leaks the information:

\[=,,)} \{ f(i_{t},a_{}))|t=1,,m\}}{||}.\] (1)

## 4 Instantiating PrivacyLens

We now apply PrivacyLens to curate an evaluation dataset. We collect privacy norms in common interpersonal communication in the U.S. from different sources, spanning from legal norms to social norms, and from specialized community norms to those of the general public (Figure 2 (A)). Concretely, we use GPT-4 to extract privacy-sensitive seeds from 15 U.S. privacy regulations covering rules that govern specific types of data (_e.g._, HIPAA, FERPA, GLBA) and various occupations (_e.g._, AMA Code of Medical Ethics) to collect legal norms, as well as from privacy research papers curated in Sannon and Forte  that focus on vulnerable groups.

To scale up the seed collection, we conduct crowdsourcing through Prolific. Specifically, we pre-fill the transmission principle with online communication activities and enumerate different social relationships2 and occupations3 for the data sender and recipient fields. Participants are then tasked to brainstem data types and data subjects that would make the seed violate privacy norms. More details about the seed collection process are included in Appendix B. After gathering seeds from various sources, we conduct a validation phase (Appendix B.3) where annotators remove unclear seeds and label whether each clearly described seed represents a privacy-sensitive case. Each seed receives three annotations, and we consider it valid if at least two annotators label it as privacy-sensitive. The inter-annotator agreement, measured by Fleiss' Kappa, is 0.79, indicating substantial agreement. Through the whole process, we collect a total of 493 valid privacy-sensitive seeds. We then use PrivacyLens to extend each seed into one vignette and trajectory. Notably, PrivacyLens' dynamic nature allows mapping a single seed to multiple vignettes and trajectories--a capability that we will explore in SS5.3. For cases where the Surgery Kit module returns a false success flag \(s\) when setting the maximum iterations \(n\) to 2, the authors manually fix the generated vignettes and trajectories. The curated dataset has been manually reviewed by the authors to ensure the data quality.

ExtensibilityWhile we use this dataset for our main evaluation experiments, as an extensible framework, PrivacyLens can also be instantiated with other seed collections. In Appendix D, we demonstrate this extensibility by applying PrivacyLens to convert seeds from a privacy-related dataset  and a cultural knowledge base  into contextualized data points. Our experimental results on them further expose significant LM privacy leakage across scenarios collected in prior datasets.

Figure 3: An example of the multi-level evaluation of PrivacyLens.

Experiments

### Evaluation Setup

Evaluated ModelsWe test four closed-source LMs: ChatGPT-3.5 (gpt-3.5-turbo-1106), GPT-4 (gpt-4-1106-preview) , Claude-3-Haiku (claude-3-haiku-20240307), Claude-3-Sonnet (claude-3-sonnet-20240229) ; and five open-source LMs, Mistral-7B-Instruct-v0.2 , Mistral-8x7B-Instruct-v0.1 , zephyr-7b-beta , Meta-Llama-3-8B-Instruct, Meta-Llama-3-70B-Instruct. All models are used with a temperature of 0. We apply the chat template to the open-source LMs and run inferences with 4 A100 80GB GPUs.

Prompting DetailsFor all the probing questions, we use zero shot prompts as shown in Figure 3. For evaluating LMs in action, we implement LM agents with ReAct  which requires the LM to output "thought" before generating the action and use the system prompt in ToolEmu  (denoted as "Basic Prompt"). To evaluate the effectiveness of prompt engineering, we also experiment with another system prompt that directly instructs the LM agent to preserve privacy (denoted as "Privacy-Enhancing Prompt"). All the prompts we use are documented in Appendix F.

Details of Evaluating LMs in ActionWe use leakage rate (\(\), see Eq. 1) to quantify the LM privacy leakage in action. We obtain \((,)\) by instructing Mistral-7B-Instruct-v0.2 to extract facts from the trajectory \(\) that are related to the data type in \(\). The authors manually ensure the quality of \((,)\), as this computation only needs to be performed once. To determine whether each information item in \((,)\) can be inferred from the final action \(a_{}\), we build a few-shot classifier \(f\) using the same Mistral model. We validate \(f\) by randomly sampling 50 final actions from different LMs we test and having 4 annotators label whether an action leaks certain information on 153 pairs in total. The Fleiss' Kappa between \(f\) and the human majority vote is 0.82; using the human majority vote as ground truth, the model's accuracy in judging whether \(a_{}\) leaks information is 0.92.

Adjusting Leakage Rate to Consider Safety-Helpfulness Trade-offThere is a trade-off between safety and helpfulness, and \(\) alone may favor models that perform poorly in executing user instructions. To address this, we use the same Mistral model to assign a _helpfulness_ score to \(a_{}\), assessing whether the action achieves the user instruction. We use the same rubric as ToolEmu , where scores of 0 (Poor) and 1 (Unsatisfactory) correspond to a _negative_ case, and scores of 2 (Good) and 3 (Excellent) correspond to a _positive_ case. On the same set of 50 final actions, the agreement between the model's judgment and the human majority vote in terms of the binary label is 0.56 with Fleiss' Kappa. We also report the adjusted leakage rate \(_{h}=}{\#}\).

### Results

We present the evaluation results of QA probing in Figure 4 and LM agent actions in Table 2.

QA probing at different levelsWhen we move from seeds to LM agent trajectories, the probing accuracy of weaker models drops significantly (_e.g._, Mistral-7B 94.32% \(\) 63.29%, Llama-3-8B 88.84% \(\) 31.44%). This may be due to the complexity of grasping relevant context from the trajectory and processing long sequences. Stronger models (_e.g._, GPT-4, Claude-3, Llama-3-70B) perform consistently well on QA probing evaluation at all three levels.

Discrepancy between probing accuracy and action-based evaluationWhile strong LMs generally perform well in QA probing evaluation, a huge discrepancy exists between how they judge data transmission appropriateness and their actual behavior in the LM agent setup. Comparing probing accuracy and leakage rates in Table 2, models like GPT-4 and Claude-3-Sonnet answer almost all trajectory-level probing questions correctly yet leak the sensitive information on 27.23% and 38.83%

Figure 4: Probing accuracy with 95% confidence intervals.

[MISSING_PAGE_EMPTY:8]

Effect of scalingWhile increasing model size is effective for improving performance on many tasks, comparing the results between Claude-3-Haiku and Claude-3-Sonnet, as well as Llama-3-8B-Instruct and Llama-3-70B-Instruct, we find that larger models can perform better on probing evaluation _but not on the action-based evaluation_. Larger models still tend to leak sensitive information in the final action without properly considering privacy norms.

Effect of prompt engineeringWe evaluate two prompt types, "Basic" and "Privacy-Enhancing", at the trajectory level (see Appendix F.2.1, F.2.2 for the full prompts). While the privacy-enhancing prompt improves probing results, it does not significantly boost performance in action-based evaluation. Since LM agents implemented with ReAct output "thoughts" before actions, we analyze these thoughts and observe that privacy-enhancing instructions are more effective in prompting stronger LMs (_e.g._, GPT-4, Claude-3-Sonnet) to output privacy-related content in the "thought". However, LMs may still leak information, despite considering privacy in their thought process.

### Dynamic Nature of PrivacyLens

One advantage of PrivacyLens is the easy extension of each seed from the current dataset to _multiple_ vignettes and trajectories with the data construction pipeline, which expands the evaluation dataset dynamically. Besides sampling multiple times with a non-zero temperature, we can add additional conditions (_e.g._, "The data receiver provides a legitimate need to access the data.") into the vignette generation process (detailed in Appendix F.1.1). This approach allows us to expand each seed into diverse vignettes, and subsequently develop them into trajectories.

As a proof of concept, we select 10 seeds from regulations and use five different conditions (reciprocal disclosure, legitimate reasons, close relationships, excitement, and perceived benefits) inspired by human information leakage [24; 7] to convert each seed into five distinct vignettes and trajectories. We evaluate LMs using these 50 trajectories, and the results in Table 3 demonstrate that the expanded data points remain useful for evaluating the privacy awareness of LMs in action. Given that one seed maps to multiple trajectories, we also compute the percentage of seeds triggering at least one leakage in their trajectories, \(p_{L}=}{}\). The results indicate that expanding a seed into multiple trajectories has the potential to support more holistic red-teaming and assessment of unintentional LM privacy leakage.

## 6 Discussion

We introduce PrivacyLens, a novel and extensible framework to evaluate the privacy norm awareness of LMs and quantify unintentional LM privacy leakage in action. Using our curated dataset, we demonstrate that even though state-of-the-art LMs perform well in answering probing questions, they still often leak information when executing user instructions in an agent setup. As scaling and prompt engineering are not effective in addressing this issue, we hope this work encourages further study on improving the privacy norm awareness of LMs.

LimitationsWe consider our work a first step in exploring privacy norm awareness of LMs and recognize the following limitations. First, our data only covers general privacy norms in the United States. Inherently, privacy concerns can differ across individuals and different culture groups. Leveraging PrivacyLens to democratize LM privacy evaluation for individuals is a meaningful direction for future work. Second, PrivacyLens evaluates LMs in action by collecting trajectories with the GPT-4 agent and instructing different LMs to conduct the final action only. This may affect

    &  &  \\  &  &  \\  & \)} & _{h}\)} & \(p_{L}\) & =p_{L}\)} \\  ChatGPT-3.5 & 14.00 & 14.58 & 0.5 & 0.2 \\ GPT-4 & 14.00 & 14.58 & 0.4 & 0.3 \\ Claude-3-Haiku & 28.00 & 28.57 & 0.6 & 0.5 \\ Claude-3-Sonnet & 18.00 & 18.27 & 0.5 & 0.4 \\  Mistral-7B-Instruct-v0.2 & **10.00** & **10.20** & 0.2 & 0.3 \\ Mistral-8x7B-Instruct-v0.1 & 30.00 & 33.33 & 0.6 & 0.4 \\ zephyr-7b-beta & 18.00 & 22.50 & 0.6 & 0.2 \\ Llama-3-8B-Instruct & 14.00 & 16.28 & 0.4 & 0.1 \\ Llama-3-70B-Instruct & 14.00 & 14.00 & 0.4 & 0.3 \\   

Table 3: Results on 50 trajectories extended from 10 privacy-sensitive seeds. The reported results use the “Privacy-Enhancing Prompt”. \(\) denotes the leakage rate; \(_{h}\) denotes the adjusted leakage rate; \(p_{L}\) denotes the percentage of seeds triggering leakage in their corresponding trajectories.

the validity of assessing other LMs. Third, our setup focuses on LM-mediated communication. Unintentional LM privacy leakage can occur in other scenarios (_e.g._, web agents interacting with websites). We leave exploring different scenarios for future work.

Broader ImpactsPrivacy norms exhibit substantial diversity, varying across cultures, communities, and individuals. In addition to releasing our dataset, we provide the implementation of our data generation pipeline. We encourage the research community to build upon PrivacyLens to create more comprehensive privacy evaluations that reflect the complex and evolving nature of privacy norms. Moreover, our framework has the potential to empower individual users to audit LM agents by providing seeds that align with their specific concerns. Users can then obtain agent trajectories to observe or evaluate LMs in action before use, thereby gaining a better understanding of the potential privacy risks associated with LM agents.