# Information Maximization Perspective of Orthogonal Matching Pursuit with Applications to Explainable AI

Information Maximization Perspective of Orthogonal Matching Pursuit with Applications to Explainable AI

 Aditya Chattopadhyay\({}^{}\)\({}^{}\) Ryan Pilgrim\({}^{}\)\({}^{}\) Rene Vidal\({}^{}\)

\({}^{}\)Johns Hopkins University, USA, {achatto1, rpi1gri1}@jhu.edu

\({}^{}\)University of Pennsylvania, USA, vidalr@upenn.edu

Equal contribution

###### Abstract

Information Pursuit (IP) is a classical active testing algorithm for predicting an output by sequentially and greedily querying the input in order of _information gain_. However, IP is computationally intensive since it involves estimating mutual information in high-dimensional spaces. This paper explores Orthogonal Matching Pursuit (OMP) as an alternative to IP for greedily selecting the queries. OMP is a classical signal processing algorithm for sequentially encoding a signal in terms of dictionary atoms chosen in order of _correlation gain_. In each iteration, OMP selects the atom that is most correlated with the signal residual (the signal minus its reconstruction thus far). Our first contribution is to establish a fundamental connection between IP and OMP, where we prove that IP with random projections of dictionary atoms as queries "almost" reduces to OMP, with the difference being that IP selects atoms in order of _normalized correlation gain_. We call this version IP-OMP and present simulations indicating that this difference does not have any appreciable effect on the sparse code recovery rate of IP-OMP compared to that of OMP for random Gaussian dictionaries. Inspired by this connection, our second contribution is to explore the utility of IP-OMP for generating explainable predictions, an area in which IP has recently gained traction. More specifically, we propose a simple explainable AI algorithm which encodes an image as a sparse combination of semantically meaningful dictionary atoms that are defined as text embeddings of interpretable concepts. The final prediction is made using the weights of this sparse combination, which serve as an explanation. Empirically, our proposed algorithm is not only competitive with existing explainability methods but also computationally less expensive.

## 1 Introduction

**Information Pursuit** (IP), first proposed by Geman and Jedynak , is a classical algorithm for active testing: Given a set of queries whose answers are informative about some target variable, IP proceeds by adaptively selecting queries in order of _information gain_. Specifically, in each iteration, IP selects the query whose answer has maximum mutual information about the target variable given the history of query-answers observed so far. IP has found numerous applications in machine learning such as tracking roads in satellite images , face detection & localization , detecting & tracking surgical instruments , and scene interpretation . More recently, IP has been touted as a framework for explainable AI . Despite its utility, a major limitation for IP is its reliance on mutual information, which is challenging to compute in high dimensions . In light of this, we ask the question, are there alternative sequential algorithms that could replace IP by employing a simpler objective in their selection step? One such algorithm is Orthogonal Matching Pursuit (OMP), which we describe next.

**OMP**[7; 8] is a classical algorithm for sparse coding: given a dictionary of atoms and an observed signal, OMP proceeds by adaptively selecting atoms in order of _correlation gain_. Specifically, in each iteration, OMP selects the atom that has maximum correlation with the current residual (the unexplained part of the observed signal given the previously selected atoms). OMP has been successfully applied to many signal processing applications such as image denoising using over-complete dictionaries , image super-resolution [10; 11], and matrix completion .

**Similarities between IP and OMP.** Although the two algorithms were discovered independently in two different communities, IP in the active testing and OMP in the signal processing community, they share many similarities. Both algorithms are illustrated in Figure 1.

* Both seek a parsimonious representation. IP tries to make a prediction by selecting the minumum number of queries on average. OMP tries to represent the observed signal by selecting the minimum number of atoms.
* Both are greedy algorithms used as efficient approximations to the true optimal solution.2 In each iteration, IP uses conditional mutual information to decide the next most _informative_ query about the target variable \(Y\). Similarly, in each iteration, OMP uses the dot product between a dictionary atom and the residual (the observed signal's projection onto the orthogonal complement of the span of all the atoms selected so far) to select the next most _representative_ atom to reconstruct the observed signal. The conditioning (in IP) and residual (in OMP) are mechanisms to account for previous decisions made by each algorithm. * IP terminates when the posterior \(P(Y)\) is sufficiently peaked, indicating \(Y\) can be predicted with low error. Similarly, OMP terminates once the mean squared error between the observed signal and its reconstruction is sufficiently low, indicating that the observed signal can be reconstructed from the selected atoms with high fidelity.

**Differences between IP and OMP.** Given these similarities, one may be tempted to conjecture that one of the algorithms is a particular case of the other. However, establishing such a connection is not trivial at all. The reason is that there are fundamental differences in the manner in which these two algorithms operate, despite their apparent similarities. In IP, the target variable and query answers are all random variables. An illustrative example of this is given by the popular parlor game "twenty questions", where a player asks queries about attributes of some entity \(Y\) (that another player has thought of) and would like to identify \(Y\) by asking the minimum number of questions. In this example, \(Y\) is a random variable, and as a result, the corresponding query answer which depends on \(Y\) is also a random variable. In OMP, on the other hand, one typically observes some fixed signal and then proceeds to encode it using the dictionary atoms. Consequently, IP's objective to select the next query requires estimating mutual information, which, as already stated, is difficult in high dimensions. OMP, on the other hand, only relies on dot products and least-squares involving the fixed signal and small numbers of dictionary atoms, which is computationally a far more tractable objective. Therefore, it is indeed surprising that one can derive a connection between the two algorithms.

**Is OMP a particular case of IP?** Formally establishing a connection between IP and OMP requires answering two key questions: (i) "what would the queries and their corresponding answers be?" and (ii) "what would the target variable be?" As IP operates on random variables and OMP on fixed observed signals, we need to inject the observed signal into the definition of the queries and/or target variable. We achieve this by taking queries as dot products of the dictionary atoms with a standard normal variable \(Z\), and the target variable as the dot product between between the observed signal \(x\) and \(Z\). The intuition is that since \(Z\) has a radially symmetric distribution, in order to accurately

Figure 1: **IP and OMP. Both are greedy sequential algorithms. IP selects the next query using conditional mutual information \(I(\,;)\) while OMP selects the next dictionary atom based on dot products \(,\). Previous choices are incorporated via the history \(_{1:k}(x)\) of previous query answers in IP, and the projection matrix \(_{D_{k}}^{}\), which we define in ยง 2, in OMP.**reconstruct the target dot product, IP would need to encode the observed signal into the selected queries (scalar projections of the dictionary atoms). We formally show this in SS3. In particular, we show that IP selects the atom in each iteration whose projection (onto the orthogonal complement of the span of the atoms selected so far) has the maximum normalized dot product with the residual. This contrasts with OMP which uses the unnormalized dot product as its selection criterion. We call this IP version of sparse coding _IP-OMP_. We then show via simulations over random Gaussian dictionaries and different sparsity levels that despite this algorithmic difference, IP-OMP and OMP have almost identical sparse recovery success rates. This empirical observation is further complemented by theoretical work by Soussen et al.  who prove that if for a given dictionary, exact recovery for all \(s\)-sparse signals is possible after \(s\) iterations of OMP, then the set of atoms selected after termination (\(s\) iterations) would be the same for both IP-OMP and OMP.3

**Implications of this connection.** As alluded to before, computing mutual information is often intractable in high dimensions. Previous approaches tackle this issue by either explicitly learning a generative model for data [5; 14] or by learning the most informative next query from data using deep networks and stochastic objectives [15; 16]. However, IP-OMP presents a much simpler alternative which does not involve training any deep networks or learning data distributions using generative models. This begs the question of whether IP-OMP can be used as a cheap surrogate to IP in certain applications. In this paper, we show one such application to explainable AI, where IP has been recently proposed as a method for making explainable predictions by design . More specifically,  proposes a framework called Variational Information Pursuit (V-IP) where the query set \(\) consists of interpretable queries about the data. Using \(\), V-IP explains its prediction in terms of the selected interpretable queries. Similarly, we construct a dictionary comprised of interpretable atoms from text embeddings. We then propose to use IP-OMP to represent the image in terms of a sparse combination of dictionary atoms (the sparse code). The final prediction is made by training a linear classifier on top of the sparse code. In this way, we can make predictions explainable--in the sense that humans can understand predictions in terms of the sparse interpretable inputs to the classifier and their corresponding weights--without computing mutual information. This, however, presents a challenge: images will typically not be represented well as linear combination of text embeddings since they are very different modalities. To remedy this, we propose to use CLIP , a recently introduced large Vision-Language Model, to encode both images and text into a shared latent space.

**Paper contributions.** To summarize, our main contributions are:

* We formally establish a connection between IP and OMP. By choosing the parameters of IP appropriately, we obtain an algorithm, IP-OMP, which is equivalent to OMP up to a normalization in the objective.
* Inspired by this connection, we propose a simple algorithm using IP-OMP and CLIP for making explainable predictions in visual classification tasks. We show empirically on multiple image classification datasets that the performance of our algorithm is competitive with, if not better than, state-of-the-art methods for explainable AI.

## 2 Preliminaries

### Information Pursuit

In this paper, random variables are defined over a common sample space \(\) and are denoted by capital letters. Their realizations are denoted by the corresponding lowercase letter. Let \(Y\) be a target variable of interest. Let \(\) be a set of queries, where every query \(Q\) is a random variable. We call the realization of \(Q\), written \(q\), its answer. Given \(\), one is often interested in predicting \(Y\) by sequentially asking queries from \(\) such that the average number of queries needed is minimized [18; 19; 20; 14; 5; 16]. Information pursuit (IP)  greedily approximates the solution to this problem. In particular, a single IP run proceeds as follows,

\[Q_{1}=*{arg\,max}_{Q}I(Q;Y); Q_{k+1}= *{arg\,max}_{Q}I(Q;Y_{1:k}),\] (1)

where \(Q_{k+1}\) is the query selected in iteration \(k+1\), \(I(\,;)\) denotes mutual information and \(_{1:k}\) is the history of query-answer pairs observed after the first \(k\) iterations. More precisely, \(_{1:k}\) is defined as the event \(\{:Q_{1}()=q_{1},,Q_{k}()=q_{k}\}\). The algorithm terminates after \(\) iterations if the entropy (or differential entropy if \(Y\) is a continuous random variable) of the posterior \(P(Y_{1:})\) is below a user-defined threshold . Alternatively, one could employ a fixed budget of \(\) iterations and terminate . After termination, IP's prediction for \(Y\) is given by \(_{Y}P(Y H_{1:})\).

### Orthogonal Matching Pursuit

Given a matrix \(D^{m n}\) with \(m n\) and an observed signal \(x^{m}\), the goal of sparse coding is to find a sparse vector \(^{n}\) that reconstructs the signal \(x\). More precisely, the goal is to solve

\[_{^{n}}\|\|_{0} x=D,\] (2)

where \(\|\|_{0}\) refers to the \(_{0}\) pseudo-norm. Equation 2 has been of significant interest over the past few decades due to its applications to compressed sensing [22, Chapter 1] and sparse representation theory . We will call \(D\) the dictionary, \(x\) the observed signal and \(\) the sparse code. We will refer to the columns of \(D\) as atoms, where \(d^{j}\) refers to the \(j^{}\) atom.

Since the problem in equation 2 is NP-Hard [22, SS2.3], approximations are employed. Orthogonal Matching Pursuit (OMP)  is a popular greedy algorithm known for its computational efficiency  compared to alternatives like Basis Pursuit . Given \(x\) and \(D\), the OMP algorithm first \(_{2}\)-normalizes all the atoms in \(D\). Subsequently, it initializes its estimate of the sparse code as \(_{0}=0\) and its estimate of the support of \(\) (set of non-zero indices in \(\)) as the empty set \(S_{0}=\). OMP then proceeds iteratively. Its \((k+1)^{}\) iteration (\(k 0\)) is defined as,

\[ j_{k+1}&=*{arg\,max}_{j \{1,,n\}}| d^{j},x-D_{k}|; S_{k+1}=S_{ k}\{j_{k+1}\};\\ _{k+1}&=*{arg\,min}_{u ^{n}}\{\|x-Du\|_{2}\ :\ (u) S_{k+1}\},\] (3)

where \(x-D_{k}\) is called the residual at iteration \(k\)[22, SS3.2]. There are multiple choices for the termination criteria, including (i) to run some fixed number of iterations \(\), or (ii) to terminate once the norm of the residual is below some user-defined threshold \( 0\), that is, \(\|x-D_{}\|_{2}\).

We now rewrite the atom selection step in equation 3 into a form that is more amenable to comparison with the IP version of OMP called IP-OMP that we will derive next. For a particular run of the OMP algorithm, let \(D_{k}\) be a sub-matrix of \(D\) composed of the first \(k\) atoms selected by the OMP algorithm and let \(_{D_{k}}\) be the projection to the range of \(D_{k}\). From the update of \(_{k}\), it is clear that \(D_{k}\) is a projection of \(x\) onto the range of \(D_{k}\). Similarly, the residual \(x-D_{k}\) is the projection of \(x\) onto the orthogonal complement . Using this observation, we rephrase the atom selection in equation 3 as

\[j_{k+1}=*{arg\,max}_{j\{1,,n\}}| d^{j},_{D_{k} }^{}x|=*{arg\,max}_{j\{1,,n\}}|_{D _{k}}^{}d^{j},_{D_{k}}^{}x|.\] (4)

## 3 IP-OMP: Information Pursuit for Sparse Coding

Recall from SS1 that to establish a connection between IP and OMP, we need to specify a set of queries and a target variable, all of which are random variables. Moreover, since OMP operates on deterministic inputs (the observed signal), we have to inject the observed signal into the definition of the queries and/or the target variable. Inspired from the Bayesian perspective of compressed sensing  one natural attempt could be to consider a sparsity-inducing prior over the sparse code, like the Laplace distribution . This induces a distribution over the observed signal \(X=D+\) (capitalized to emphasize that \(X\) is now a random variable). Since the goal is to sparse-code the signal, a natural choice for the target variable could be \(X\) itself. Finally, since we want to recover the atom selection step in OMP (equation 4) using IP, which is obtained by maximizing the absolute dot product between the atom and the residual, a reasonable choice for the query set \(Q\) could be atoms of the dictionary where the answer to a query is the corresponding atom's dot product with \(X\). Given this setup, the first query selected by IP is4

\[Q_{1}=*{arg\,max}_{Q^{j}}I(Q^{j};X)=*{ arg\,max}_{d^{j} D}I( d^{j},X;X).\] (5)It is immediately clear from equation 5 that the first query would be independent of the observed signal \(X=x\). This is because IP decides what is the most informative query solely based on mutual information, which is a property of the distribution, and does not involve observing the realization \(x\). This is in sharp contrast to OMP, where the selection of the first atom is driven by the observed signal (see equation 3). Thus, assuming a generative model for the signal \(X\) would not result in the OMP algorithm and calls for a more "non-standard" way of defining the queries and/or the target variable.

**What is the right query set and target variable?** How can we make the queries and/or the target variable depend on a realization of \(X=x\)? Inspired by a measure called sliced mutual information, which defines the dependence between two random variables as the average mutual information between their one-dimensional projections onto random directions , we propose the following query set and target variable.

* Define \(Z\) to be a standard normal variable in \(^{m}\). Let \(z^{0}\) be a sample from \(P_{Z}=(0,I_{m})\).
* Take \(\) to contain all the atoms in \(D\) as queries, such that query \(Q^{j}\) corresponds to the random projection \( d^{j},Z\) of atom \(d^{j} D\), and the corresponding answer is \(q^{j}:= d^{j},z^{0}\).
* Take the target random variable of interest as \( x,Z\). Thus, given \(z^{0}\), the value of the target variable that needs to be predicted is \( x,Z= x,z^{0}\).

The rationale behind these choices is as follows. Since \(Z\) is a random vector with a radially symmetric density, to accurately predict the target variable \( x,Z\) for all realizations of \(Z\), IP needs to query atoms that would in a way _code_ for \(x\) itself. This intuition is formalized by the following lemma.

**Lemma 1**.: _Let \(z^{0}\) be a realization of \(Z\). Given a subset \(\) of queries from \(\) (or equivalently, atoms selected from \(D\)), the conditional entropy of the target \( x,Z\) given the query answers observed is_

\[h x,Z\{Q= d_{Q},z^{0}:Q \}=(2 e\|_{D_{}}^{}x\|_{2}^{2}),\] (6)

_where \(h\) is the conditional differential entropy, \(d_{Q}\) denotes the atom used to construct the random variable \(Q\), \(D_{}\) is the sub-matrix of \(D\) consisting of atoms corresponding to queries in \(\) and \(_{D_{}}^{}\) is the projection matrix onto the orthogonal complement of the range of \(D_{}\)._

The proof of this lemma relies on the joint Gaussianity of all the query answers and the target variable since they are all dot products with the same normal vector \(Z\). Proof in Appendix SSA.1.

Notice that the history of query answers observed, after a finite number of iterations of IP, is just some subset of queries selected from \(\) along with their corresponding answers. Since IP greedily selects queries until the conditional entropy of the target (given history) is minimized (SS2.1), Lemma 1 indicates that IP would terminate when \(\|_{D_{}}^{}x\|_{2}^{2} 0\), that is, when \(x\) can be "almost exactly" reconstructed from the atoms selected by IP.

**The IP-OMP algorithm.** Given this choice of query set and target variable, we now state our main result (Theorem 1) proving that IP not only chooses atoms that _code_ for \(x\), but also uses exactly the same query selection strategy as the atom selection strategy of OMP up to a normalization factor.

**Theorem 1**.: _Given an observed signal \(x\), a realization \(z^{0}\) of \(Z\), and the choice of \(\) and the target variable as described in this section, IP proceeds as follows for \(k 1\),_

\[ Q_{1}&=*{arg\,max}_{Q^{j} }I(Q^{j}; x,Z)=*{arg\,max}_{d^{j}  D},x|}{\|d^{j}\|_{2}\|x\|_{2}}}^{}d^{j},_{D_{k}}^{}x|}{\|_{D_{k}}^{}d^{j }\|_{2}\|_{D_{k}}^{}x\|_{2}},\\ \] (IP-OMP) \[ Q_{k+1}&=*{arg\,max}_{Q^{ j}}I(Q^{j}; x,Z_{1:k})\!=\! *{arg\,max}_{d^{j} D,\|_{D_{k}}^{}d^{j}\|_{2} 0} }^{}d^{j},_{D_{k}}^{}x|}{\|_{D_ {k}}^{}d^{j}\|_{2}\|_{D_{k}}^{}x\|_{2}},\]

_where \(_{1:k}:=\{Q_{1}= d_{1},z^{0},Q_{2}= d_{2},z^{0 },,Q_{k}= d_{k},z^{0}\}\), \(d_{i}\) is the atom (query) selected by IP in the \(i^{}\) iteration, \(D_{k}\) is the sub-matrix of \(D\) consisting of atoms corresponding to queries selected by IP in the first \(k\) iterations and \(_{D_{k}}^{}\) is the projection matrix onto the orthogonal complement of the range of \(D_{k}\)._

The above result uses the joint Gaussianity of all the query answers and the target variable. A full derivation can be found in Appendix SSA.2. Comparing equation 4 and IP-OMP, it is clear that the two algorithms differ by a normalization. To distinguish from the OMP algorithm, we call the iterative algorithm described above as the IP-OMP algorithm. If one uses the entropy of the posterior \(P( x,Z_{1:})\) criterion for termination after \(\) iterations, then according to Lemma 1 the algorithm would terminate once \(\|_{D_{}}^{}x\|_{2}^{2}\). This is reminiscent of the norm of the residual termination criterion commonly employed by OMP.

**Sparse coding using the IP-OMP algorithm.** In the discussion so far between the equivalence of OMP and IP-OMP, we swept the issue of recovering the sparse code under the rug. To remind the reader, the goal of sparse coding is to find a code \(_{}\) such that \(x D_{}\). However, the IP-OMP updates do not explicitly provide a way of doing so. However, by analyzing IP-OMP's prediction of the target \( x,Z\) we can derive \(_{}\).

Recall from SS2.1 that after termination, IP's prediction is the mode of the posterior distribution. Thus after termination IP-OMP's prediction would be given by

\[_{Z}P( x,Z_{1:})= D_{}D_{ }^{}x,z^{0},\] (7)

where \(\) is the number of iterations before IP-OMP terminated, \(D_{}\) is the sub-matrix of \(D\) consisting of the atoms selected in the first \(\) iterations, \({}^{}\) represents the pseudo-inverse, and \(z^{0}\) is the realization of \(Z\) that determines the outcomes of all the query answers and the target (which is \( x,Z= x,z^{0}\)). Refer to Appendix SSA.3 for a proof of equation 7. Next, notice that the query selection in IP-OMP is independent of the realization of \(Z\), which implies that the sequence of queries selected in the first \(\) iterations would depend solely on \(x\). This implies the following lemma,

**Lemma 2**.: _For all \(z^{m}\), IP-OMP's prediction of the target is given by \( D_{}D_{}^{}x,z\)._

Refer to Appendix SSA.4 for a proof. Taking \(z=e_{i}\), the \(i^{}\) canonical basis vector in \(^{m}\), we observe that IP-OMP's prediction of the \(i^{}\) component of \(x\), denoted as \(x_{i}\), is \((D_{}D_{}^{}x)_{i}\). Hence, we conclude

\[x D_{}D_{}^{}x=\ D_{-} ]}^{D}^{}x)}^{_{}}_{}=D_{}^{}x\\ 0,\] (8)

where \(D_{}\) is the matrix comprised of atoms of \(D\) not selected by IP-OMP in the first \(\) iterations. Comparing with equation 3, we see that this is analogous to the sparse code estimate OMP obtains after \(\) iterations, with the \(D_{}\) matrix now replaced with the atoms selected by OMP.

**IP-OMP\(\) OMP.** Recall the only difference between IP-OMP and OMP is the normalization step after the dot product. We carry out synthetic experiments to investigate the effect of this normalization factor on the practical performance of IP-OMP vs. OMP in sparse code recovery. We first consider the noiseless case (equation 2) and later investigate the scenario where signals are corrupted by noise.

For the noiseless case, we conduct the following experiment. Fix \(n\), the dimension of the sparse code, \(m\) the dimension of the observed signal, and \(s\) the sparsity level. Generate a large number of random dictionaries, where each atom (column) is sampled from the uniform distribution on the unit sphere in \(^{m}\). For every dictionary, pick a subset of \(s\) (out of \(n\)) atoms uniformly at random and simulate the coefficients for each of the selected atoms from the standard normal distribution. Set all the remaining coefficients to \(0\). These coefficients constitute the sampled sparse code. For each simulation, construct the observed signal \(x_{o}=D_{o}_{o}\), where \(D_{o}\) is the sampled random dictionary and \(_{o}\) is the sampled sparse code. For every \(x_{o}\), we carry out both OMP and IP-OMP and measure their performance in terms of the fraction of simulations in which the algorithm exactly recovered the sparse code \(_{o}\). We define exact recovery when the normalized mean-squared error (NMSE)

\[(_{},_{o})=\|_{}-_{o}\|_{2}^{2}/\|_{o}\|_{2}^{2}\] (9)

Figure 2: Performance of OMP (solid curves) and IP-OMP (overlaid circles) in the noiseless case as a function of the number of measurements \(m\) for a fixed sparsity level \(s\) and a number of dictionary atoms \(n\). โProbability of exact recoveryโ reports the fraction of times (over 1000 simulations) the algorithm exactly recovered the sparse code for a particular setting of \((m,n,s)\).

is approximately zero (\(<10^{-14}\)), where the subscript \({}_{}\) indicates the algorithm (OMP or IP-OMP). This experimental setup was inspired by . Further details of the experiment can be found in Appendix SSB. Figure 2 reports these results which indicate that OMP and IP-OMP achieve _almost_ the same exact recovery rate over a variety of different values of \((m,n,s)\). In particular, both IP-OMP and OMP degrade similarly when the sparsity rate \( 1\) or the measurement rate \( 0\).

For the noisy case, we extend equation 2 with additive Gaussian noise. Specifically, we consider the measurement model \(=D_{o}_{o}+e\), where \(D_{o}\) and \(_{o}\) are sampled as described in the previous paragraph, \(\) is the noisy observed signal and \(e(0,^{2}I)\). The noise variance \(^{2}\) was set to achieve particular values of the expected signal-to-noise ratio (SNR)

\[[]=_{D_{o},_{o},e}[ _{o}\|_{2}^{2}}{\|e\|_{2}^{2}}].\] (10)

Details on this calculation are provided in Appendix SSB.3. We measure the quality of the estimates produced by OMP and IP-OMP by reporting the mean NMSE of their estimates, \(_{}\) (taken over all the trials). Further details can be found in Appendix SSB. Figure 3 shows that the the empirical equivalence between OMP and IP-OMP is maintained over different values of \((m,n,s,[])\).

More results can be found in Appendix SSB. Figure 2 and Figure 3 show that despite the extra normalization, IP-OMP and OMP have practically the same sparse code recovery rates for random Gaussian dictionaries, which are commonly studied in compressed sensing (see, e.g., [22, Ch. 9]).

**Connection to Orthogonal Least Squares (OLS).** Interestingly, the IP-OMP selection criterion (equation IP-OMP) results in greedily selecting the atom that maximally reduces the residual error . Specifically, let \(D_{k}\) indicate the set of atoms from \(D\) obtained after \(k\) iterations of IP-OMP. Then, the atom \(d^{*}\) solving the following optimization problem

\[(d^{*},^{*})=*{arg\,min}_{d D, ^{k+1}}x-[D_{k}\ \ d]_{2}^{2},\] (11)

is equal to the \(*{arg\,max}\) of the IP-OMP objective at iteration \(k+1\). Using this insight, multiple authors have proposed to modify OMP by replacing its atom selection criterion with equation 11 and have re-discovered this algorithm under different names  with OLS being one of them. In this paper, we coin the name IP-OMP to emphasize its derivation from information-theoretic principles.

Our results on the empirical equivalence between IP-OMP and OMP are complemented by prior investigations into the OLS algorithm which report similar conclusions using random Gaussian dictionaries . Theoretically, Soussen et al.  showed that both the algorithms share the same exact recovery conditions. This is made precise in Theorem 2 (a restatement of results in ).

**Theorem 2**.: _Given a dictionary \(D\) and support \(\), define \(=\{^{n}:*{supp}() \}\). Then IP-OMP recovers all \(\) in at most \(s=||\) iterations if and only if the same holds for OMP._

These results further strengthen our claim that for practical scenarios, the normalization factor in IP-OMP does not have a significant impact. We note in passing that, for certain "special" dictionaries in which the dictionary atoms are strongly correlated with each other, IP-OMP has been reported to outperform OMP in its ability to recover the sparse code .

Figure 3: Performance of OMP (solid curves) and IP-OMP (overlaid circles) in the case where signals are corrupted by noise. On the \(y\)-axis we report the mean NMSE, as defined in equation 9, averaged over 1000 trials. On the \(x\)-axis, we report the number of measurements (signal dimension) \(m\). Each sub-figure corresponds to a fixed number of dictionary atoms \(n\) and a fixed expected signal-to-noise ratio \([]\). The top row shows results for small \(n\) (256), while the bottom shows results for large \(n\) (1024).

IP-OMP for Explainable Predictions for Visual Classification Tasks

Inspired by the recent application of IP  and its variant V-IP  to explainable AI, here we propose a simple explainable AI algorithm for visual classification tasks which uses IP-OMP to ask queries about the image. In SS4.1 we briefly describe the V-IP framework, propose our CLIP-IP-OMP algorithm, and discuss the differences with V-IP. Then, in SS4.2 we present experiments showing that CLIP-IP-OMP is computationally much cheaper than V-IP, while achieving a similar classification accuracy. Refer to Appendix SSC for a review of prior work on explainable AI.

### The CLIP-IP-OMP Algorithm

**The V-IP algorithm.** In V-IP, a user first specifies a query set, which is comprised of interpretable functions of the input. For example, if the task is animal classification, the questions could be about various animal attributes. V-IP then adaptively generates short query-answer chains that are sufficient for prediction. This is done by learning a querier and a classifier network simultaneously from data. The querier learns to select the most informative query given the history of query-answer pairs obtained so far, while the classifier learns to make predictions based on the same history. However, the number of possible histories (subsets of query-answer pairs) the querier and classifier have to learn is exponential in the size of the query set, making the training process very slow. In this paper, we propose an alternative algorithm which is computationally much cheaper.

**The CLIP-IP-OMP algorithm.** Analogously to V-IP's interpretable query sets, we propose to use dictionaries of interpretable atoms. For example, every atom of the dictionary could correspond to a text embedding  of a semantic concept relevant for the classification task. This poses a challenge, since images and text embeddings live in very different spaces (pixel space vs. the latent space of some deep network). To address this mismatch, we leverage CLIP , which learns to encode images and text into a shared latent space such that the dot products of their respective embeddings are reflective of how well the text describes the image contents. We propose the following algorithm for making explainable predictions for visual classification tasks:

1. Given a set of semantic concepts relevant to the classification task, define each atom of the dictionary \(D\) as the CLIP embedding of one of the concepts obtained using CLIP's text encoder.
2. For a given input image, define the observed signal \(x\) as the CLIP embedding of the image.
3. Using IP-OMP as the querier, encode the image's CLIP embedding as a sparse combination of dictionary atoms with weights \(_{}\). While both OMP and IP-OMP recover the sparse code similarly well, we choose to use IP-OMP in our experiments due to its nice information-theoretic interpretation (SS3).
4. Train a linear classifier to predict the class label from the sparse codes \(_{}\). This choice is made to enhance interpretability. By inspecting the weights of the linear layer, one can identify which concepts (atoms) are most important for the prediction.

For a given image, the linear network predicts the class label based on the sparse code. The explanation of the prediction, then, is given in terms of the non-zero coefficients of \(_{}\) and their corresponding atoms. Because these atoms are text embeddings for concepts, they are semantically meaningful. This is in stark contrast to predictions made by deep networks trained on raw pixel data, where the reasoning behind network predictions is opaque.

**Differences between CLIP-IP-OMP and V-IP.** A significant difference between CLIP-IP-OMP and V-IP is the target variable in question. In V-IP, this is the class label \(Y\), whereas in IP-OMP this is a projection of \(x\) (the observed image's CLIP embedding) onto a random direction \(Z\). Moreover, the query answers in both algorithms are different. In V-IP, the image is considered a random variable and answers are functions of the image. In IP-OMP, the answers are simply dot products between a query (dictionary atom) and the random direction \(Z\). Despite these differences, the label prediction in both V-IP and CLIP-IP-OMP is explained using a small set of concepts selected before termination. In V-IP, this is the set of query-answer pairs selected. In CLIP-IP-OMP, this is the sparse code \(_{}\), which encodes the set of selected concepts and their weights.

### Experiments

Having described our algorithm, we now show its efficacy at generating explainable predictions on 5 different vision datasets: ImageNet , Places365 , CIFAR-{10, 100} , and CUB-200 . Specific details about architecture and training protocols used can be found in Appendix SSD.2. Our code is available at https://github.com/r-zip/ip-omp.

**Semantic dictionaries.** To get a set of semantically relevant concepts, we follow prior work , which uses GPT-3  to extract relevant concepts for every class in the dataset by asking prompts like "List the most important features for recognizing something as a class". For more details, we direct the reader to . In this work, we directly use their extracted concepts . For each concept, we obtain their corresponding CLIP embedding by directly using the concept name as input to the text encoder. Some example concepts for Imagenet are shown in col. 2 of Figures 3(a) and 3(b) (y-axis).

**Explainable predictions using CLIP-IP-OMP.** We show example runs of the CLIP-IP-OMP algorithm on two images taken from the ImageNet dataset in Figure 4. In particular, given an image, we run IP-OMP on its CLIP embedding for a fixed number of iterations \(\), which is the desired explanation length. Then, we make a prediction using the linear classifier, which was trained to learn the mapping between sparse codes of sparsity level \(\) (number of non-zero elements) and the class label. All of the CLIP embeddings (text and image) are \(_{2}\)-normalized. The signed magnitude of the coefficient in the sparse code indicates how relevant the corresponding atom (text concept) is for the contents of the image. This claim is motivated by CLIP's training loss which incentivizes aligned image-text embedding pairs (that is, the text describes the image contents) to have a high dot product (closer to +1) and unaligned image-text embedding pairs (that is, the text does not correspond to the image contents) to have a small dot product (closer to -1). We also visualize the weights of the trained linear classifier for the predicted class. In Figure 3(a), we see that the selected atoms with positive coefficients in the sparse code correspond to characteristics of a tiger like "black and white striped fur" and "prey animal". Similarly, atoms with negative coefficients correspond to concepts not found in a tiger like "salty flavor" and "witch". Although the image is not of a Savannah habitat, we suspect CLIP assigns a positive association between the text embedding for "a Savannah" and the image, since tigers are found in savannah habitats. Interestingly, the sign of the weights the linear classifier assigns to each of the concepts seems to be positively correlated with their sign in the sparse code. Similar observations are made for Figure 3(b). Although the concept of "originated in Sussex, England" has a large coefficient in the sparse code, the linear classifier assigns a very low weight to it, indicating a diminished effect of that concept on the prediction "Aircraft Carrier".

**Baseline comparisons.** We compare our method with V-IP and Label-free Concept Bottleneck Models (Lf-CBM) , a recently proposed explainable AI algorithm which is similar in spirit to ours. Lf-CBM represents a given image in terms of a concept vector where every dimension

Figure 4: **Explainable predictions using CLIP-IP-OMP.** Two example runs of our algorithm on two images from the ImageNet test set, with number of iterations \(=9\). For both images, our model predicted the correct class with \(68.73\%\) & \(72.21\%\) confidence resp. The weights of the linear classifier correspond to the weights for the predicted class. More examples in Appendix SSE.6.

corresponds to a semantic concept and the corresponding feature is the dot product between CLIP's embedding of the image and the text concept. The final prediction is then made by training a sparse linear layer (using elastic net regularization) to map from concept features to labels. Note that their model is sparse in the network weights, whereas ours is sparse in the input features.

For a fair comparison, we use the same set of concepts for all methods. For V-IP, the query is CLIP's text embedding and the answer is its corresponding dot product with the CLIP embedding (since the dot product indicates whether the concept is present in the image). We compare all methods by examining the tradeoff between explanation length and classification accuracy. For V-IP, CLIP-IP-OMP, and Lf-CBM respectively, the explanation lengths are the number of queries selected, the number of dictionary atoms selected, and the number of non-zero weights in the linear classifier for the predicted class. For prediction accuracy, we use V-IP's learned classifier, whereas for our method, we train a separate linear classifier for every sparsity level (the number of iterations of IP-OMP before termination). Unlike CLIP-IP-OMP and V-IP, Lf-CBM does not have any sequential selection strategy. Hence, to compare, we report the average number of non-zero weights per class in the concepts-to-label linear mapping. The results are shown in Figure 5. On most datasets, CLIP-IP-OMP has a better trade-off (greater area-under-the-curve) than V-IP, except CIFAR-100 (where they are competitive) and CUB-200. Notice that for all datasets, in the initial iterations, atoms selected by CLIP-IP-OMP are more predictive for the class (higher test accuracy) than V-IP. This is because the atom selection in IP-OMP explicitly depends on the image content, whereas, the query selection in V-IP (equation 1) solely depends on the answers to the previous queries and the data distribution. On all datasets except CUB and ImageNet (where it is competitive), our method achieves similar accuracy to Lf-CBM, but with a much smaller explanation length (Lf-CBM's explanation length (accuracy) is indicated by the red star (red dashed line)).

**Computational efficiency.** Complementary to being performant at explaining prediction, we would like to stress the simplicity of our method compared to V-IP and Lf-CBM, two contemporary explainable AI methods. Both V-IP and Lf-CBM require costly optimization routines. In V-IP, this is training a querier network to learn what to ask next from data. In Lf-CBM, this is optimizing the sparse linear classifier using the elastic-net regularizer. Finding the right sparsity level (number of non-zero weights in the classifier) in Lf-CBM requires hyperparameter tuning by training multiple models. In comparison, our method uses IP-OMP, which requires no training, to select dictionary atoms. Moreover, the number of iterations of the algorithm controls the explanation lengths.

The main bottleneck of IP-OMP is in extracting the sparse codes since this involves computing projection matrices. For example, our implementation, based on Cholesky decomposition , takes \(\!40\) hours to extract sparse codes of support size 50 (the highest considered in our experiments) for all the 1.28 million images in the ImageNet training set on an NVIDIA RTX A5000 GPU. This will be much faster for smaller support sizes, for example at level 10, it takes \(\!6\) hours. For comparison, training Lf-CBM  on ImageNet for _one set of hyperparameters_ takes about 50 hours on the same GPU. Similarly, training the V-IP framework  on ImageNet takes about 4 days!

## 5 Conclusions and Limitations

In this work, we show that Orthogonal Matching Pursuit (OMP) is a particular case of Information Pursuit (IP) (modulo a normalization factor) with a suitably chosen set of queries and target variable. We call this IP-derived algorithm for sparse coding IP-OMP. We then present a simple algorithm, called CLIP-IP-OMP, which uses CLIP and IP-OMP to make explainable predictions for visual classification tasks. CLIP-IP-OMP has two limitations. First, its application is restricted to image datasets since it relies on CLIP to encode images and text concepts into a shared space. Second, the atom selection procedure using IP-OMP is independent of the class label; as a result an auxiliary classifier needs to be trained. Future work would aim to address these limitations.

Figure 5: Trade-off between accuracy and explanation length for different methods (best viewed in colour).