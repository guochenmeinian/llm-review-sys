# Enhancing Efficiency of Safe Reinforcement Learning via Sample Manipulation

Shangding Gu\({}^{1,3}\),Laixi Shi\({}^{2}\), Yuhao Ding\({}^{1}\), Alois Knoll\({}^{3}\), Costas Spanos\({}^{1}\), Adam Wierman\({}^{2}\),

**Ming Jin\({}^{4}\)**

\({}^{1}\)University of California, Berkeley, USA

\({}^{2}\)California Institute of Technology, USA

\({}^{3}\)Technical University of Munich, Germany

\({}^{4}\)Virginia Tech, USA

Equal contribution.

###### Abstract

Safe reinforcement learning (RL) is crucial for deploying RL agents in real-world applications, as it aims to maximize long-term rewards while satisfying safety constraints. However, safe RL often suffers from sample inefficiency, requiring extensive interactions with the environment to learn a safe policy. We propose Efficient Safe Policy Optimization (ESPO), a novel approach that enhances the efficiency of safe RL through _sample manipulation_. ESPO employs an optimization framework with three modes: maximizing rewards, minimizing costs, and balancing the trade-off between the two. By dynamically adjusting the sampling process based on the observed conflict between reward and safety gradients, ESPO theoretically guarantees convergence, optimization stability, and improved sample complexity bounds. Experiments on the _Safety-MuJoCo_ and _Omnisafe_ benchmarks demonstrate that ESPO significantly outperforms existing primal-based and primal-dual-based baselines in terms of reward maximization and constraint satisfaction. Moreover, ESPO achieves substantial gains in sample efficiency, requiring 25-29% fewer samples than baselines, and reduces training time by 21-38%.

## 1 Introduction

Reinforcement learning (RL)  has demonstrated powerful capabilities in various domains . However, ensuring safety in RL, particularly in real-world applications such as autonomous driving, robotics, and power grids, is crucial . Safe RL aims to maximize long-term cumulative rewards while adhering to additional safety cost constraints.

Most state-of-the-art (SOTA) safe RL methods, including primal-based baselines (e.g., CRPO , PCRPO ) and primal-dual-based methods (e.g., CUP , PPOLag ), optimize cost and reward with a predetermined sample size for each iteration. However, this approach may lead to _sample inefficiency_ due to two main reasons:

* Wasted samples and computational resources in simple scenarios, where the (computational/physical) cost of obtaining these samples may outweigh their learning benefits.
* Insufficient exploration in complex cases with high uncertainty or conflicting objectives, potentially hindering the learning of a safe and optimal policy.

A key insight from optimization literature suggests that the selection of sample size is worthwhile but a delicate issue, as it may vary depending on the optimization stage and landscape . However, this insight remains largely unexplored in the context of safe RL, where the considerationof safety adds complexity. The presence of safety constraints can create regions with high conflict between reward and safety objectives, requiring careful balancing and potentially more samples to resolve. Therefore, an unresolved question in safe RL is: **Can we enhance sample efficiency by dynamically adapting the sample size, while simultaneously improving reward performance and guaranteeing safety?**

To address this question, we focus on primal-based approaches, which do not require fine-tuning of dual parameters or heavily rely on initialization, unlike primal-dual-based optimization [30; 53]. The key to effectively enhancing sample efficiency is to establish reliable criteria for determining sample size requirements. Inspired by insights from multi-objective optimization/RL [40; 37; 30], we use _gradient conflict between rewards and costs_ as an effective signal for adjusting sample size in each iteration. Intuitively, when gradient conflict occurs, balancing reward and safety optimization with a uniform sample size becomes challenging; conversely, when there is gradient alignment, optimizing with fewer samples is more straightforward. This motivates us to adopt a three-mode optimization framework: 1) optimizing cost exclusively upon a safety violation; 2) simultaneously optimizing both reward and cost during a soft constraint violation; 3) optimizing only the reward when no violations are present. This allows tailored sample size adjustment based on the optimization regime. We increase the sample size in situations of gradient conflict to incorporate more informative samples and reduce it in cases of gradient alignment to prevent unnecessary costs and training time. This sampling adjustment is effective in each policy learning mode (cost only, simultaneous reward and cost, and reward only), enabling the search for improved policies that prioritize safety, rewards, or a balance of both.

This study makes three key contributions emphasizing sample manipulation for safe RL:

1 We propose Efficient Safe Policy Optimization (ESPO), an algorithm that depart from prior arts by incorporating sample manipulation by leveraging gradient conflict signals as criteria to enhance sample efficiency and reduce unnecessary interactions with the environments, 2 We provide a comprehensive theoretical analysis of ESPO, including convergence rates, the advantages of reducing optimization oscillation, and provable sample efficiency. The theoretical results inspire ESPO's sample manipulation approach and could be of independent interest for broad RL applicability. 3 We evaluate ESPO through comparative and ablation experiments on two benchmarks: _Safety-MuJoCo_ and _Omnisafe_. The results demonstrate that ESPO improves reward performance and safety compared to SOTA primal-based and primal-dual-based baselines. Notably, ESPO significantly reduces the number of samples used during policy learning and minimizes training costs while ensuring safety and achieving superior reward performance.

## 2 Related Works

Various methodologies have been developed to enhance safety in RL [12; 31], including constrained optimization-based methods, control-based methods [18; 19; 33; 29], and formal methods . Among these, constrained optimization-based methods have gained notable popularity due to their ease of use and reduced dependency on external knowledge .

Constrained optimization-based methods can be categorized into primal-dual (e.g., CPO , PCPO , CUP ) and primal approaches. Primal-dual methods face challenges in tuning dual multipliers, ensuring feasible initialization, and sensitivity to learning rates [53; 30]. Primal methods offer a distinct advantage by eliminating the need for dual multipliers. A prominent primal-based method is CRPO , which focuses on directly optimizing the primal problem. When safety violations occur, CRPO exclusively improves the violated constraints. However, it encounters significant challenges with conflicting gradients between optimizing rewards and constraints, which can impact ensuring both performance and ongoing safety compliance. PCRPO  addresses this issue by balancing the trade-offs between reward and safety performance through strategic gradient manipulation. However, it lacks comprehensive convergence and sample complexity analysis and faces computational challenges due to the need to compute reward and safety gradients in each gradient handling step.

Figure 1: Oscillation analysis compared our method with existing safe RL methods in three modes of optimization.

Several efficient safe RL methods have been recently proposed [16; 21; 22; 23; 24; 36; 42; 47; 50], including offline  and off-policy settings [36; 42]. Our model-free, on-policy approach is distinguished by its dynamic calibration of sampling based on the interplay between reward maximization and safety assurance. Closely related works are  and .  employs symbolic reasoning for safety but relies on external knowledge, potentially limiting applicability.  proposes a non-stationary safe RL approach with regret bounds using linear function approximation but may struggle with complex tasks and inherits issues common in primal-dual safe RL [22; 23]. Our primal-based method circumvents these drawbacks.

Adaptive sampling methods in optimization can be categorized into prescribed (e.g., geometric) sample size increase [13; 26][13; 8], gradient approximation test [14; 7; 13; 8; 15; 10; 5], and derivative-free [45; 9] and simulation-based methods  (see  for a review). These methods focus on controlling the variance of gradient approximations or function evaluations (e.g., through inner product  or norm tests [14; 15]) to balance computational efficiency and sample complexity. Adaptive sampling methods have also been applied to constrained stochastic optimization problems with convex feasible sets [4; 52]. A recent work  extends adaptive sampling to a multi-objective setting, but their criteria are still based on variance. Our research introduces a novel perspective by focusing on conflict-aware updates based on safety and performance gradients in safe RL, making it the first adaptive sampling method for this important domain.

## 3 Problem Formulation

A Constrained Markov Decision Process (CMDP)  is often used to model safe RL problems. A CMDP is denoted as \((,,P,r,c,b,)\), where \(\) is the state space, \(\) is the action space, \(P:\) is the transition probability function, \(r:\) is the reward function, and \(\) is the discount factor. To encode safety, \(c=(c_{1},,c_{n}):^{n}\) is the cost function assigning costs to state-action pairs, with higher costs indicating higher risks, \(b=(b_{1},,b_{n})^{n}\) contains safety thresholds for each constraint. This CMDP framework searches for a safe policy \(\) in the stochastic Markov policy set \(\), balancing rewards and safety constraints.

The expected cumulative reward values are defined as \(V_{r}^{}(s)=[_{t=0}^{}^{t}r(s_{t},a_{t })|,s_{0}=s.]\) and \(Q_{r}^{}(s,a)=[_{t=0}^{}^{t}r(s_{t},a_ {t})|,s_{0}=s,a_{0}=a.]\) for states and state-action pairs, respectively. Similarly, safety is quantified using the cost state values \(V_{c}^{}(s)\) and cost state-action values \(Q_{c}^{}(s,a)\). The primary objective in safe RL is to maximize the accumulative reward while ensuring safety, under an initial state distribution \(\):

\[_{}\;V_{r}^{}()_{s}[V_{r }^{}(s)],\;\;\;V_{c}^{}()_{s }[V_{c}^{}(s)] b.\] (1)

However, conflicts often arise in safe RL between the reward gradient \(_{r}= V_{r}^{}()\) and negative cost gradient \(_{c}=- V_{c}^{}()\). These conflicts can lead to unstable policy updates that cause experiences violating safety constraints, forcing reversion to prior policies and wasting samples. Such unstable dynamics further impede efficient exploration, risking premature convergence and squandering of computational resources. This study aims to efficiently search for a safe policy by manipulating samples to reduce waste and improve safe RL efficiency.

## 4 Algorithm Design and Analysis

### Three-Mode Optimization

To improve learning efficiency and mitigate oscillations, we leverage PCRPO  and categorize performance optimization into three distinct strategies: focusing on reward, on both reward and cost simultaneously, or solely on cost. Two essential parameters are introduced to construct a soft constraint region -- \(h^{-}\) on the lower side and \(h^{+}\) on the upper side. With \(h^{-},h^{+}\) in hand,  divides the optimization process into three modes as below. Throughout the paper, we parameterize the policy \(\) by \(w\).

\(\)**1) Safety Violations.** When the cost values \(V_{c}^{}()>(h^{+}+b)\), we apply (2) to update the policy parameter \(w_{t}\) with learning rate \(\). In such mode, since the constraints are violated, we prioritizesafety and choose to minimize the cost objective to achieve compliance with safety standards.

\[w_{t+1}=w_{t}+_{c}.\] (2)

\(\)**2) Soft Constraint Violations.** When \(V_{c}^{}()[h^{-}+b,h^{+}+b]\), we leverage (3) and (4) for simultaneous optimization of reward and safety performance. Specifically, when within the soft constraint region, the _conflict_ between the reward and cost gradients is characterized by the angle \(_{r,c}\) between the reward gradient \(_{r}\) and the cost gradient \(_{c}\). When \(_{r,c}>90^{}\), it indicates the directions that optimize the reward and the safety performance are in conflict, and the update rule is (3).

\[w_{t+1}=\{ & w_{t}+[x_{t}^{r}( _{r}-_{r}_{c}}{\|_{c}\|^{2 }}_{c})+x_{t}^{c}(_{c}-_{c} _{r}}{\|_{r}\|^{2}}_{r})],\\ & w_{t}+[x_{t}^{r}_{r}+x_{t}^{c}_{c} ],.\] (3)

where \(x_{t}^{r},x_{t}^{c} 0\) and \(x_{t}^{r}+x_{t}^{c}=1\) for all \(t T\). It employs gradient projection techniques [30; 58], projecting reward and cost gradients onto their normal planes and ensuring that the policy adjustment balances the conflicting objectives of maximizing rewards and minimizing costs. In contrast, when \(_{r,c} 90^{}\), namely, the directions for maximizing rewards and minimizing costs are aligned or do not significantly oppose each other, we use the update rule (4). In this scenario, the gradient for the update is computed based on the weight of the reward and cost gradients. This method leverages the synergistic potential between reward maximization and cost minimization, aiming for a policy update that harmoniously improves both aspects.

\(\)**3) No Violations.** When \(V_{c}^{}()<(h^{-}+b)\), the update rule in (5) is applied to optimize the policy:

\[w_{t+1}=w_{t}+_{r}.\] (5)

In other words, given that the policy adheres to all specified constraints, only the reward objective is considered.

### Sample Size Manipulation

As introduced above, PCRPO  allows for adaptive optimization updates based on different conditions. However, PCRPO and other existing safe RL methods usually apply an identical sample size during the learning process, resulting in potentially unnecessary computation cost for simpler tasks and inadequate exploration for more complex tasks. Furthermore, there is no existing theoretical analysis for PCRPO, leaving the performance guarantees of it somewhat uncharted. To address the above challenges, we propose a method called ESPO based on a crucial sample manipulation approach that will be introduced momentarily. A comprehensive theoretical analysis of ESPO is provided in Section 4.4.

Throughout the framework of three-mode optimization, our proposed method dynamically adjusts the number of samples utilized at each iteration based on the criteria of gradient conflict, to meet specific demands of reducing unnecessary samples in simpler scenarios and increasing exploration in more complex situations. Specifically, we consider the three-mode optimization classified by the gradient-conflict criteria respectively. **2)(a)**_Soft Constraint Violations with Gradient Conflict_, where \(_{r,c}>90^{}\) (cf. (6)): the cases with slight safe constraint violation and gradient conflict between reward and safety objectives. In this scenario, adjusting the sample size becomes crucial for sufficiently exploring the environments to identify a careful balanced update direction. We increase the sample size in (6) to enhance the likelihood of achieving a near-optimal balance between the reward and cost objectives. **2)(b)_Soft Constraint Violations without Gradient Conflict_**, where \(_{r,c} 90^{}\) (cf. (7)): the cases with slight safe constraint violation and gradient alignment between reward and safe objectives. Considering it is easier to search for a update direction that benefits the aligned reward and cost objectives, we reduce the sample size in (7) to achieve efficient learning. **1) and 3)**_Safety Violations_ and _No Violations_: only reward or cost objective is considered. It indicates that there is no gradient conflict since only one objective is targeted, where we also employ the update rule in (7).

For more details, we dynamically adjust the sample size \(X_{t}\) (\(X\) denote a default fixed sample size), with \(_{t}^{+}\) and \(_{t}^{-}\) representing some sample size adjustment parameters.

\[X_{t+1}=\{ & X+X_{t}^{+},\ _{r,c}>90^{},\\ & X+X_{t}^{-},\ _{r,c} 90^{}. .\] (6)This gradient-conflict-based sample manipulation is a crucial feature of our proposed method, which enables adaptively sample size tailored to the specific nature of the joint reward-safety objective landscape at each update iteration.

### Efficient Safe Policy Optimization (ESPO)

Building upon the above two modules -- three-mode optimization and sample size manipulation, we have formulated a practical algorithm. The details of this algorithm are summarized in Algorithm 1 in Appendix B. This algorithm encompasses a strategic approach to sample size adjustment and policy updates under various conditions: **1)**_Safety Violations_: When a safety violation occurs, we adjust the sample size \(X_{t}\) using Equation (7). Simultaneously, the policy \(_{w_{t}}\) is updated to ensure safety, as dictated by Equation (2). **2)(a)**_Soft Constraint Violations with Gradient Angle \( 90^{}\)_: In modes of soft region violation where the angle \(_{r,c}\) between gradients \(_{r}\) and \(_{c}\) is less than or equal to \(90^{}\), we adjust the sample size \(X_{t}\) using Equation (7). The policy \(_{w_{t}}\) is then updated in accordance with Equation (3). **2)(b)**_Soft Constraint Violations with Gradient Angle \(>90^{}\)_: Conversely, if the soft region violation occurs with a gradient angle \(_{r,c}\) exceeding \(90^{}\), the sample size \(X_{t}\) is adjusted via Equation (6). Policy updates are made using Equation (4). **3)**_No Violations_: In the absence of any violations, the sample size \(X_{t}\) is altered using Equation (7). The policy \(_{w_{t}}\) is then updated to maximize the reward \(V_{r}^{}()\), following Equation (5). This practical algorithm reflects an insightful analysis of the interplay between reward maximization and safety assurance in safe RL, tailoring the learning process to the specific demands of each scenario.

### Theoretical analysis of ESPO

In this section, we provide theoretical guarantees for the proposed ESPO, including the convergence rate guarantee and provable optimization stability and sample complexity advancements.

Tabular setting with softmax policy class.In this paper, we focus on a fundamental tabular setting with finite state and action space. We consider the class of policies with the softmax parameterization which is complete including all stochastic policies. Specifically, a policy \(_{w}\) associated with \(w^{||||}\) is defined as

\[(s,a):_{w}(a|s)}(w(s,a^{}))}.\] (8)

Before proceeding, we introduce some useful notations. When executing ESPO (cf. Algorithm 1), let \(_{r}\), \(_{}\), and \(_{c}\) denote the set of iterations using _Safety Violation Response_ (mode 1), _Soft Constraint Violation Response_ (mode 2), and _No Violation Response_ (mode 3) in Section 4.3, respectively.

I: Provable convergence of ESPO.First, we present the convergence rate of our proposed ESPO in terms of both the optimal reward and the constraint requirements in the following theorem; the proof is given in Appendix A.3.

**Theorem 4.1**.: _Consider tabular setting with policy class defined in (8), and any \((0,1)\). For Algorithm 1, applying \(T_{}=|| |}{1-)^{3}||||}}{(1-)^{3}||| |}^{2}\) iterations for each policy evaluation step, set tolerance \(h^{+}=|||}}{(1- )^{1.5}}\) and the learning rate of NPG update \(=(1-)^{1.5}/|||T}\). Then, the output \(\) of Algorithm 1 satisfies that with probability at least \(1-\),_

\[V_{r}^{^{*}}()-[V_{r}^{}()](|||}{(1-)^{3}T}}),\ \ [V_{c}^{}()]-V_{c}^{^{*}}() (|||}{(1-)^{3}T}}).\]

_Here, the expectation is taken with respect to the randomness of the output \(\), which is randomly selected from \(\{_{w_{t}}\}_{1 i T}\) with a certain probability distribution (specified in Appendix (30))._

Theorem 4.1 demonstrates that taking the output policy \(\) as a random one selected from \(\{_{w_{t}}\}_{1 i T}\) following some distribution, the proposed ESPO algorithm achieves convergence to a globally optimal policy \(^{}\) within the feasible safe set, following the convergence rate of \((}})\). The convergence rate for constraint violations towards \(0\) is also \((}})\). While note that the implementation of Algorithm 1 in practice only need to output the final \(=_{w_{T}}\) for simplicity. The randomized procedure is only used for theoretical analysis.

We observe that ESPO enjoys the same convergence rate as the well-known primal safe RL algorithm -- CRPO . In addition, Theorem (4.1) directly indicates the same convergence rate guarantee for PCRPO  -- the three-mode optimization framework that our ESPO refer to, which closes the gap between practice and theoretical guarantees for PCRPO . Technically, to handle the variation in ESPO's update rules across a three-mode optimization process compared to CRPO, deriving the results necessitates to overcome additional challenges by tailoring a new distribution probability for the algorithm that is used to randomly select policies from \(\{_{w_{t}}\}_{1 i T}\).

Besides the efficient convergence, in the following, we present two advantages of ESPO in terms of both optimization benefits and sample efficiency; the proof are provided in Appendix A.4 and A.5 respectively.

II: Efficient optimization with reduced oscillation.Shown qualitatively in Figure 1, compared to other primal safe RL algorithms (such as CRPO), our proposed ESPO can significantly increase the ratios of iterations for maximizing the reward objective within the (relaxed) soft safe region by reducing oscillation across the safe region boundary. We provide a rigorous quantitative analysis for such advancement as below:

**Proposition 4.2**.: _Suppose CRPO  and ESPO (ours) are initialized at an identical point \(w_{0}^{[||]}\). Denote the set of iterations that CRPO updates according to the reward objective as \(_{r}^{}\). Then by adaptively choosing the parameters (\(x_{t}^{r},x_{t}^{c}\)) of Algorithm 1, if there exist iteration \(i_{}<T\) such that \(t_{r}_{}\), one has_

\[ t_{} t T: t_{r} _{},\] (9a) \[|_{r}|+|_{}|=T-t_{ }_{r}^{}.\] (9b)

In words, (9a) shows that as long as ESPO (cf. Algorithm 1) enters the safe region that the constraint is violated at most \(h^{+}\), it will stay and always (at least partially) optimizes the reward objective without oscillation across the safe region boundary. In addition, (9b) indicates that the proposed ESPO enables more iterations to maximize the reward objective inside the safe region with comparison to CRPO, accelerating the optimization towards the global optimal policy. These two theoretical guarantees are further corroborated by the phenomena in practice (shown in Table 3): ESPO spends more iterations (\(99.4\%\) steps) on optimizing the reward objective inside the safe region compared to CRPO (\(35.6\%\) steps), while only a few on solely cost objective.

III: Sample efficiency with sample size manipulation.Besides the efficient optimization of ESPO, the following proposition presents the provable sample efficiency of ESPO.

**Proposition 4.3**.: _Consider any \(0_{1},_{2}\). To meet the following goals of performance gaps_

\[V_{r}^{^{}}()-[V_{r}^{}()] _{1},\ [V_{c}^{}()]-V_{c}^{^{}}() _{2},\] (10)

_ESPO (Algorithm 1) needs fewer number of samples than that without the sample manipulation in Section 4.2._

The result demonstrates that, considering the accuracy level/constraint violation requirements, the sample manipulation module contributes to a more sample-efficient algorithm ESPO (Algorithm 1). Additionally, the _conflict_ between reward and cost gradients emerges as an effective metric for determining sample size requirements.

## 5 Experiments and Evaluation

To evaluate the effectiveness of our algorithm, we compare it with two key paradigms in safe RL frameworks. The first paradigm is based on the primal framework, including PCRPO  and CRPO  as the representative baselines. The second paradigm includes methods that leverage the primal-dual framework, with PCPO , CUP , and PPOLag  serving as representative methodologies. Our algorithm is developed within the primal framework, thereby highlighting the importance of comparing it against these paradigmatic safe RL algorithms to clearly demonstrate its performance. Experiments are conducted using both primal and primal-dual benchmarks. The _Omnisafe3_ benchmark is leveraged for primal-dual based methods, where representative techniques such as PCPO , CUP , and PPOLag  generally exhibit stronger performance compared to existing primal methods like CRPO , a finding discussed in . Additionally, we use the _Safety-MuJoCo4_ benchmark for primal-based methods. This benchmark, developed in 2024, is relatively new and primarily supports primal-based methods due to the specific implementation efforts involved. The detailed experimental settings are provided in Appendix D. Furthermore, to thoroughly evaluate the effectiveness of our method, we conduct a series of ablation experiments regarding different cost limits and sample manipulation techniques. In particular, we provide performance update analysis in terms of constraint violations. These experiments are specifically designed to dissect and understand the impact of various factors integral to our approach.

### Experiments of Comparison with Primal-Based Methods

We deploy our algorithm on the _Safety-MuJoCo_ benchmark and carry out experiments compared with representative primal algorithms, PCRPO  and CRPO . Specifically, we conduct experiments on a set of challenging tasks, namely, _SafetyReacher-v4_, _SafetyWalker-v4_, _SafetyHumanoidStandup-v4_.

In the experiments conducted on the _SafetyReacher-v4_ task, as depicted in Figures 2(a)-(c), our method demonstrates superior performance compared to SOTA primal baselines, CRPO and PCRPO. For instance, our method achieves better reward performance than CRPO and PCRPO. Another notable aspect of ESPO's performance is its training efficiency, which is largely attributed to sample manipulation. Specifically, as depicted in Table 1, while CRPO and PCRPO utilize 8 million

  TaskAlgorithm & ESPO (Ours) & CRPO & PCRPO \\  _SafetyReacher-v4_ & **5.7 M** & 8 M & 8 M \\ _SafetyWalker-v4_ & **6.2 M** & 8 M & 8 M \\ _SafetyHumanoidStandup-v4_ & **5.1 M** & 8 M & 8 M \\  

Table 1: Comparison of sampling steps with primal-based methods (The lower, the better). M denotes one million.

Figure 2: Compare our algorithm (ESPO) with PCRPO  and CRPO  on the _Safety-MuJoCo_ benchmark. Our algorithm consistently and remarkably outperforms the SOTA baseline across multiple performance metrics, including reward maximization, safety assurance, and learning efficiency.

samples for the _SafetyReacher-v4_ task, our method requires only 5.7 million samples for the same task. Crucially, our method improves reward and efficiency performance without sacrificing safety. However, CRPO and PCRPO are struggling to ensure safety during policy learning. Ensuring safety is a pivotal aspect of RL in safety-critical environments. The experiment results indicate that our method's ability to balance safety with other performance metrics is a significant improvement. As illustrated in Figures 2(d)-(f), our comparison experiments on the challenging _SafetyWalker-v4_ task, yielding findings consistent with those observed in _SafetyReacher-v4_ tasks. Due to space limits, additional experiments on _SafetyHumanoidStandup-v4_ are postponed to Appendix D.

### Experiments of Comparison with Primal-Dual-Based Methods

The _Omnisafe_ Benchmark is a popular platform for evaluating the performance of safe RL algorithms. To further examine the effectiveness of our method, we have implemented our algorithm within the _Omnisafe_ framework and conducted an extensive series of experiments compared with SOTA primal-dual-based baselines, e.g., PPOLag , CUP  and PCPO , focusing mainly on challenging tasks such as _SafetyHopperVelocity-v1_ and _SafetyAntVelocity-v1_.

The efficacy of our algorithm, ESPO, is demonstrated in Figures 3(a)-(c), where it is benchmarked against SOTA baselines on the _SafetyHopperVelocity-v1_ tasks. Firstly, ESPO is remarkably able to achieve better reward performance than the SOTA primal-dual-based baselines. Secondly, a critical aspect of our algorithm is its capability to ensure safety. It is particularly significant considering that some of the compared baselines, such as CUP  and PPOLag , struggle to maintain safety within the same task parameters. Thirdly, an outstanding feature of ESPO is its efficiency, as evidenced by approximately half the training time required compared to the SOTA baselines like CUP and PPOLag. This efficiency in training time demonstrates ESPO's practicality for use in various applications, especially where computational resources and time are constraints. Moreover, while PCPO  manages to ensure safety, its reward performance is inferior to ESPO's. PCPO also requires more training time than ESPO, underscoring our algorithm's reward, safety performance, and training efficiency advantages. Particularly, as illustrated in Table 2, across the entire training period, all the benchmark baselines, including PCPO, CUP, and PPOLag, utilized 10 million samples for tasks on _SafetyHopperVelocity-v1_. In contrast, our method required only 7.3 million samples for the _SafetyHopperVelocity-v1_ task. The trends observed in the performance of our algorithm on the

   Task & Algorithm & ESPO (Ours) & PCPO & CUP & PPOLag \\  _SafetyHopperVelocity-v1_ & **7.3 M** & 10 M & 10 M & 10 M \\ _SafetyAntVelocity-v1_ & **7.6 M** & 10 M & 10 M & 10 M \\   

Table 2: Comparison of sampling steps with primal-dual based methods (The lower, the better). M denotes one million samples.

Figure 3: Compare our algorithm (ESPO) with PCPO , CUP  and PPOLag  on the _Omnisafe_ benchmark. Our algorithm performs significantly better than the SOTA baselines regarding reward, safety, and efficiency performance.

_SafetyHopperVelocity-v1_ task are similarly reflected in the results presented in Figures 3(d)-(f), about the _SafetyAntVelocity-v1_ task. These findings further prove the effectiveness of ESPO in various tasks. Note that the reduction in samples may not equate to a corresponding reduction in training time, as this can vary depending on the characteristics of the benchmarks and the algorithms applied to different tasks. Factors such as the action space of the task and the settings of parallel processing supported by the benchmark can influence the overall training time.

These results on _Omnisafe_ tasks further highlight the strengths of ESPO in improving reward performance with safety assurance while maintaining greater efficiency in training. The ability of ESPO validates its potential as an effective solution for further exploration and application in real-world environments.

### Ablation Experiments

We conducted ablation studies focusing on various cost limits, sample sizes, learning rates, gradient weights, and update styles to further assess our method's effectiveness. These studies are crucial for gaining deeper insights into our method, highlighting its strengths, and identifying potential areas for improvement. Through this evaluation, we aim to demonstrate the adaptability of our method, confirming its applicability and efficacy across a broad spectrum of safe RL scenarios. Due to space limits, details of the ablation studies are provided in Appendix C.

## 6 Conclusion

In the study, we improved the efficiency of safe RL through a three-mode optimization scheme employing sample manipulation. We provide an in-depth theoretical analysis of convergence, stability, and sample complexity. These theoretical insights inform a practical algorithm for safety-critical control. Extensive experiments on two major benchmarks, _Safety-MuJoCo_ and _Omnisafe_, indicate that our method not only surpasses the SOTA baselines in terms of efficiency but also achieves higher reward performance while maintaining safety. Moving forward, we plan to assess our method's capabilities in real world control applications to further expand its influential reach into safety-critical domains. Impact and limitation statements are provided in Appendix E.