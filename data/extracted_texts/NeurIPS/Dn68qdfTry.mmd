# Almost Surely Asymptotically Constant Graph Neural Networks

Sam Adam-Day

Corresponding author. Email: me@samadamday.com

Michael Benedikt

Ismail Ilkan Ceylan

Ben Finkelshtein

Department of Computer Science

University of Oxford

Oxford, UK

###### Abstract

We present a new angle on the expressive power of graph neural networks (GNNs) by studying how the predictions of real-valued GNN classifiers, such as those classifying graphs probabilistically, evolve as we apply them on larger graphs drawn from some random graph model. We show that the output converges to a _constant_ function, which upper-bounds what these classifiers can uniformly express. This strong convergence phenomenon applies to a very wide class of GNNs, including state of the art models, with aggregates including mean and the attention-based mechanism of graph transformers. Our results apply to a broad class of random graph models, including sparse and dense variants of the Erdos-Renyi model, the stochastic block model, and the Barabasi-Albert model. We empirically validate these findings, observing that the convergence phenomenon appears not only on random graphs but also on some real-world graphs.

## 1 Introduction

Graph neural networks (GNNs)  have become prominent for graph machine learning with applications in domains such as life sciences . Their empirical success motivated work investigating their theoretical properties, pertaining to their expressive power , generalization capabilities , and convergence properties .

We consider GNNs outputting real-valued vectors, such as those which classify graphs probabilistically, and ask: _how do the outputs of these GNNs evolve as we apply them on larger graphs drawn from a random graph model?_

Our study provides a surprising answer to this question: the output of many GNNs eventually become _independent_ of their inputs - each model eventually outputs the same values on all graphs - as graph sizes increase (Figure 1). This "almost sure convergence" to a _constant distribution_ is much stronger than convergence to _some_ limit object . The immediate consequence of this strong convergence phenomenon is to upper bound the uniform expressiveness of the considered model architectures: _these architectures can uniformly express only the classifiers that are almost

Figure 1: The output of the considered GNNs eventually become constant as the graph sizes increase.

surely asymptotically constant_. In other words, our results provide impossibility results for what tasks are _in principle_ learnable by GNNs. While our top-level results are for graph classification, in the process we provide strong limitations on what node- and edge-classification can be performed by GNNs on random graphs: see, for example Theorem 5.3.

**Scope of the result.** The core approach in graph machine learning is based on iteratively updating node representations of an input graph by an _aggregate_ of messages flowing from the node's neighbours . This approach can be extended to global aggregates . Our main result holds for all architectures that use _weighted mean_ as an aggregation function and it is extremely robust in the following two dimensions:

1. _Model architectures_: Our result is very general and abstracts away from low-level architectural design choices. To achieve this, we introduce an _aggregate term language_ using weighted mean aggregation and provide an "almost sure optimization" result for this language: our result states that every term in the language can be simplified to Lipschitz functions for most inputs. Thus, any architecture that can be expressed in this language follows the same convergence law. This includes _graph attention networks_ (GATs) , as well as popular (graph) transformers, such as the _General, Powerful, Scalable Graph Transformer_ (GPS) with random walk encodings . The term language can seamlessly capture common design choices, such as skip and jumping knowledge connections , or global aggregation schemes .
2. _Random graph models_: All results apply to a wide class of random graph models, including Erdos-Renyi models of various sparsity levels, the Barabasi-Albert preferential attachment model, and the stochastic block model. The sparse models are more realistic than their dense counterparts which makes it typically harder to obtain results for them. This is also reflected in our study, as the results for sparse and dense models require very different proofs.

**Contributions.** The key contributions of this paper are as follows:

* We introduce a flexible aggregate term language with attractive closure properties (Section 4) and prove an "almost sure convergence" result for this language relative to a wide class of random graph models (Section 5). This result is of independent interest since it pushes the envelope for convergence results from classical logical languages  to include aggregation.
* We show that a diverse class of architectures acting as real-valued graph classifiers can be expressed in the term language (Section 4). In Section 5 we present "almost sure convergence" results for our term language, from which we derive results about GNNs (Corollary 5.2). The results are robust to many practical architectural design choices and even hold for architectures using mixtures of layers from different architectures. We also show strong convergence results for real-valued node classifiers in many graph models.
* We validate these results empirically, showing the convergence of these graph classifiers in practice (Section 6) on graphs drawn from the random models studied. In addition, we probe the real-world significance of our results by testing for convergence on a dataset with varying size dataset splits. Across all experiments we observe rapid convergence to a constant distribution. Interestingly, we note some distinctions between the convergence of the sparse and non-sparse Erdos-Renyi model, which we can relate to the proof strategies for our convergence laws.

## 2 Related work

**Uniform expressiveness.** The expressive power of MPNNs is studied from different angles, including their power in terms of graph distinguishability . The seminal results of Xu et al. , Morris et al.  show that MPNNs are upper bounded by the _1-dimensional Weisfeiler Leman graph isomorphism test (1-WL)_ in terms of graph distinguishability. WL-style expressiveness results are inherently non-uniform, i.e., the model construction is dependent on the graph size. There are also recent studies that focus on uniform expressiveness . In particular, Adam-Day et al.  investigate the uniform expressive power of GNNs with randomized node features, which are known to be more expressive in the non-uniform setting . They show that for classical Erdos-Renyi graphs, GNN binary classifiers display a zero-one law, assuming certain restrictions on GNN weights and the random graph model. We focus on real-valued classifiers, where their results do not apply, while dealing with a wider class of random graph models, subsuming popular architectures such as graph transformers.

**Convergence laws for languages.** Our work situates GNNs within a rich term language built up from graph and node primitives via real-valued functions and aggregates. Thus it relates to convergence laws for logic-based languages on random structures, dating back to the zero-one law of Fagin , including . We are not aware of _any_ prior convergence laws for languages with aggregates; the only work on numerical term languages is by Gradel et al. , which deals with a variant of first-order logic in general semi-rings.

**Other notions of convergence on random graphs.** The works of Cordonnier et al. , Keriven et al. , Maskey et al. , Levie  consider convergence to continuous analogues of GNNs, often working within metrics on a function space. The results often focus on dense random graph models, such as graphons . Our approach is fundamentally different in that we can use the standard notion of asymptotic convergence in Euclidean space, comparable to traditional language-based convergence results outlined above, such as those by Fagin  and Lynch . The key point is that a.a.s. constancy is a _very_ strong notion of convergence and it does not follow from convergence in the senses above. In fact, obtaining such a strong convergence result depends heavily on the details of the term language, as well as the parameters that control the random graph: see Section 7 for further discussion of the line between a.a.s. convergence and divergence. Our study gives particular emphasis to sparse random graph models, like Barabasi-Albert, which are closer to graphs arising in practice.

## 3 Preliminaries

### Featured random graphs and convergence

**Random graphs.** We consider simple, undirected graphs \(G=(V_{G},E_{G},H_{G})\) where each node is associated with a vector of _node features_ given by \(H_{G}:V_{G}^{d}\). We refer to this as a _featured graph_. We are interested in _random graph models_, specifying for each number \(n\) a distribution \(_{n}\) on graphs with \(n\) nodes, along with _random graph feature models_, where we have a distribution \(_{n}\) on featured graphs with \(n\) nodes. Given a random graph model and a distribution \(\) over \(^{d}\), we get a random graph feature model by letting the node features be chosen independently of the graph structure via \(\).

**Erdos-Renyi and the stochastic block model.** The most basic random graph model we deal with is the _Erdos-Renyi distribution_\((n,p(n))\), where an edge is included in the graph with \(n\) nodes with probability \(p(n)\). The classical case is when \(p(n)\) is independent of the graph size \(n\), which we refer as the _dense_ ER distribution. We also consider the _stochastic block model_\((n_{1},,n_{M},P)\), which contains \(m\) communities of sizes \(n_{1},,n_{M}\) and an edge probability matrix between communities \(^{M M}\). A community \(i\) is sampled from the Erdos-Renyi distribution \((n,p(n)=_{i,i})\) and an edge to a node in another community \(j\) is included with probability \(_{i,j}\).

**The Barabasi-Albert preferential attachment model.** Many graphs encountered in the real world obey a _power law_, in which a few vertices are far more connected than the rest . The _Barabasi-Albert distribution_ was developed to model this phenomenon . It is parametrised by a single integer \(m\), and the \(n\)-vertex graph \((n,m)\) is generated sequentially, beginning with a fully connected \(m\)-vertex graph. Nodes are added one at a time and get connected via \(m\) new edges to previous nodes, where the probability of attaching to a node is proportional to its degree.

**Almost sure convergence.** Given any function \(F\) from featured graphs to real vectors and a random featured graph model \((_{n})_{n}\), we say \(F\)_converges asymptotically almost surely_ (converges a.a.s.) to a vector \(\) with respect to \(\) if for all \(,>0\) there is \(N\) such that for all \(n N\), with probability at least \(1-\) when drawing featured graphs \(G\) from \(_{n}\), we have that \(\|F(G)-\|<\).

### Graph neural networks and graph transformers

We first briefly introduce _message passing neural networks_ (MPNNs) , which include the vast majority of graph neural networks, as well as (graph) transformers.

**Message passing neural networks.** Given a featured graph \(G=(V_{G},E_{G},H_{G})\), an MPNN sets the initial features \(_{v}^{(0)}=H_{G}(v)\) and iteratively updates the feature \(_{v}^{(0)}\) of each node \(v\), for\(0 L-1\), based on the node's state and the state of its neighbors \((v)\) by defining \(_{v}^{(+1)}\) as:

\[^{()}(_{v}^{()},^{()}_{v}^{()},\{\!\!\{_{u}^{()} u(v)\}\!\!\} ),\]

where \(\{\!\!\{\}\!\}\) denotes a multiset and \(^{()}\) and \(^{()}\) are differentiable _update_ and _aggregation_ functions, respectively. The final node embeddings are pooled to form a graph embedding vector \(_{G}^{(L)}\) to predict properties of entire graphs. A MeanGNN is an MPNN where the aggregate is mean.

**GATs.** One class of MPNNs are graph attention networks (GATs) , where each node is updated with a weighted average of its neighbours' representations, letting \(_{v}^{(+1)}\) be:

\[_{u(v)}(_{v} ^{()},_{u}^{()}))}{_{w(v)} ((_{v}^{()},_{w}^{()} ))}^{()}_{u}^{()},\]

where \(\) is a certain learnable Lipschitz function.

**Graph transformers.** Beyond traditional MPNNs, _graph transformers_ extend the well-known transformer architecture to the graph domain. The key ingredient in transformers is the self-attention mechanism. Given a featured graph, a single attention head computes a new representation for every (query) node \(v\), in every layer \(>0\) as follows:

\[(v)=_{u V_{G}}(_{v}^{()},_{u}^{()}))}{_{w  V_{G}}((_{v}^{()},_{w}^{ ()}))}^{()}_{u}^{()}\]

where \(\) is another learnable Lipschitz function (the scaled dot-product).

The vanilla transformer architecture ignores the graph structure. Graph transformer architectures  address this by explicitly encoding graph inductive biases, most typically in the form of positional encodings (PEs). In their simplest form, these encodings are additional features \(_{v}\) for every node \(v\) that encode a node property (e.g., node degree) which are concatenated to the node features \(_{v}\). The random walk positional encoding (RW)  of each node \(v\) is given by:

\[_{v}=[_{v,1},_{v,2},_{v,k}],\]

where \(_{v,i}\) is the probability of an \(i\)-length random walk that starts at \(v\) to end at \(v\).

The GPS architecture  is a representative graph transformer, which applies a parallel computation in every layer: a transformer layer (with or without PEs) and an MPNN layer are applied in parallel and their outputs are summed to yield the node representations. By including a standard MPNN in this way, a GPS layer can take advantage of the graph topology even when there is no positional encoding. In the context of this paper, we write GPS to refer to a GPS architecture that uses an MPNN with _mean_ aggregation, and GPS+RW if the architecture additionally uses a random-walk PE.

**Probabilistic classifiers.** We are looking at models that produce a vector of reals on each graph. All of these models can be used as probabilistic graph classifiers. We only need to ensure that the final layer is a softmax or sigmoid applied to pooled representations.

## 4 Model architectures via term languages

We demonstrate the robustness and generality of the convergence phenomenon by defining a term language consisting of compositions of operators on graphs. Terms are formal sequences of symbols which when interpreted in a given graph yield a real-valued function on the graph nodes.

**Definition 4.1** (Term language).: \([]\) is a term language which contains node variables \(x,y,z,\) and terms defined inductively:2

* The _basic terms_ are of the form \((x)\), representing the features of the node \(x\), and constants \(\).

* Let \(h^{d}(0,)^{d}\) be a function which is Lipschitz continuous on every compact domain. Given terms \(\) and \(\), the _local \(h\)-weighted mean_ for node variable \(x\) is: \[_{y(x)}(y) h((y))\] The interpretation of \(\) will be defined below. The _global \(h\)-weighted mean_ is the term: \[_{y}(y) h((y))\]
* Terms are closed under applying a function symbol for each Lipschitz continuous \(F^{d k}^{d}\) for any \(k^{+}\).

The weighted mean operator takes a weighted average of the values returned by \(\). It uses \(\) to perform a weighting, normalizing the values of \(\) using \(h\) to ensure that we are not dividing by zero (see below for the precise definition).

To avoid notational clutter, we keep the dimension of each term fixed at \(d\). It is possible to simulate terms with different dimensions by letting \(d\) be the maximum dimension and padding the vectors with zeros, noting that the padding operation is Lipschitz continuous.

We make the interpretation of the terms precise as follows. See Figure 2 for a graphical example of evaluating a term on a graph.

**Definition 4.2**.: Let \(G=(V_{G},E_{G},H_{G})\) be a featured graph. Let \(\) be a term with free variables \(x_{1},,x_{k}\) and \(=(u_{1},,u_{k})\) a tuple of nodes. The _interpretation_\([\![()]\!]_{G}\) of term \(\) graph \(G\) for tuple \(\) is defined recursively:

* \([\![()]\!]_{G}=\) for any constant \(\).
* \([\![}(x_{i})()]\!]_{G}=H_{G}(u_{i})\) for the \(i^{}\) node's features.
* \([\![F(_{1},,_{k})()]\!]_{G}=F([\![_{1}()]\!]_ {G},,[\![_{k}()]\!]_{G})\) for any function symbol \(F\).
* For any term composed using \(\): \[_{y(x_{i})} h()() _{G}=\{(u_{i})}[\![ (,v)]\!]_{G}h([\![(,v)]\!]_{G})}{_{v (u_{i})}h([\![(,v)]\!]_{G})}&(u_{i}) $};\\ &(u_{i})=$}..\] The semantics of global weighted mean is defined analogously and omitted for brevity.

A _closed_ term has all node variables bound by a weighted mean operator: so the implicit input is just a featured graph.

**Definition 4.3**.: We augment the term language to \([,]\) by adding the random walk operator \((x)\). The interpretation of \((x_{i})\) given a graph \(G\) and a tuple of nodes \(\), is:

\[[\![(x_{i})()]\!]_{G}=[\![_{u_{i},1},, _{u_{i},d}]\]

Figure 2: Evaluation of the term \(_{x(y)}(2(x)+2) 1.0\) on a small graph with scalar features. The term computes the mean of \(z 2z+2\) on each of a nodeâ€™s neighbours. As each sub-term has one free variable, we can represent the intermediate results as scalar values for each node.

### How powerful is the term language?

Various architectures can be described using this term language. The core idea is always the same: we show that all basic building blocks of the architecture can be captured in the term language and applying this inductively yields the desired result. Let us first note that all linear functions and all commonly used activation functions are Lipschitz continuous, and therefore included in the language.

**MPNNs with mean aggregation.** Consider an \(L\)-layer MPNN with mean aggregation, update functions \(^{()}\) consisting of an activation function applied to a linear transformation, with mean pooling at the end. First, note that mean aggregation can be expressed as:

\[_{y}(y)_{y}(y) 1.\]

For each layer \(0<L\), we define a term \(^{()}(x)\) which will compute the representation of a node at layer \(\) of the MPNN:

* **Initialization**. Let \(^{(0)}(x)(x)\). Then the value \(^{(0)}(x)(u)_{G}\) at a node \(u\) is the initial node representation \(H(u)\).
* **Layers**. For \(1<L\), define \(^{(+1)}(x)^{()}(x,_{y (x)}^{()}(y))\). Then the value at node \(u\) is the following, which conforms with the inductive construction of the MPNN: \[^{(+1)}(x)(u)_{G}=^{()}( ^{()}(x)(u)_{G},(u)|}_{v (u)}^{()}(x)(v)_{G})\]
* **Final mean pooling**. The final graph representation is computed as \(_{x}^{(L)}(x)\).

The idea is similar for the other architectures, where the difference lies in the aggregation functions. Thus below we only present how the term language captures their respective aggregation functions.

**Graph transformers.** We can express the self-attention mechanism of transformers using the following aggregator:

\[_{y}(y)_{y}(y)(((x), (y)))\]

The function \(((x),(y))\) is a term in the language, since scaled dot product attention is a Lipschitz function. To see how graph transformer architectures such as GPS can be expressed, it suffices to note that we can express both self-attention layers and MPNN layers with mean aggregation, since the term language is closed under addition. The random walk positional encoding can also be expressed using the rw operator.

**Graph attention networks.** The attention mechanism of GAT is local to a node's neighbours and can be expressed in our term language using similar ideas, except using the local aggregate terms.

**Additional architectural features.** Because the term language allows the arbitrary combination of graph operations, it can robustly capture many common architectural choices used in graph learning architectures. For example, a _skip connection_ or _residual connection_ from layer \(_{1}\) to layer \(_{2}\) can be expressed by including a copy of the term for layer \(_{1}\) in the term for the layer \(_{2}\). _Global readout_ can be captured using a global mean aggregation . Attention _conditioned on computed node or node-pair representations_ can be captured by including the term which computes these representations in the mean weight .

**Capturing probabilistic classification.** Our term language defines bounded vector-valued functions over graphs. Standard normalization functions, like softmax and sigmoid, are easily expressible in our term language, so probabilistic classifiers are subsumed.

**Graph convolutional networks.** In Appendix A we show how to extend the term language to incorporate graph convolutional networks (GCNs)  by adding a new aggregator.

## 5 Convergence theorems

We start by presenting the convergence theorem.

**Theorem 5.1**.: _Consider \((_{n})_{n}\) sampling a graph \(G\) from any of the following models and node features independently from i.i.d. bounded distributions on \(d\) features._

1. _The Erdos-Renyi distribution_ \((n,p(n))\) _where_ \(p\) _satisfies any of the following properties._ * _Density._ \(p\) _converges to_ \(>0\)_._ * _Root growth._ _For some_ \(K>0\) _and_ \(0<<1\) _we have:_ \(p(n)=Kn^{-}\)_._ * _Logarithmic growth._ _For some_ \(K>0\) _we have:_ \(p(n)=K\)_._ * _Sparsity._ _For some_ \(K>0\) _we have:_ \(p(n)=Kn^{-1}\)_._
2. _The Barabasi-Albert model_ \((n,m)\) _for any_ \(m 1\)_._
3. _The stochastic block model_ \((n_{1}(n),,n_{m}(n),)\) _where_ \(n_{1},,n_{m}\) _are such that_ \(n_{1}(n)++n_{m}(n)=n\) _and each_ \(}{n}\) _converges, and_ \(\) _is any symmetric_ \(m m\) _edge probability matrix._

_Then every \([,]\) term converges a.a.s. to a constant with respect to \((_{n})_{n}\).3_

Concretely, this result shows that for any probabilistic classifier, or other real-valued classifier, which can be expressed within the term language, when drawing graphs from any of these distributions, eventually the output of the classifier will be the same regardless of the input graph, asymptotically almost surely. Thus, the only probabilistic classifiers which can be expressed by such models are those which are asymptotically constant.

**Corollary 5.2**.: _For any of the random graph featured models above, for any MeanGNN, GAT, or GPS + RW, there is a distribution \(\) on the classes such that the class probabilities converge asymptotically almost surely to \(\)._

We now discuss briefly how the results are proven. The cases divide into two groups: the denser cases (the first three \(\) distributions and the \(\)) and the sparser cases (the fourth \(\) distribution and the \(\) model). Each is proved with a different strategy.

### Overview of the technical constructions for the denser cases

While the theorem is about closed terms, naturally we need to prove it inductively on the term language, which requires consideration of terms with free variables. We show that each open term in some sense degenerates to a Lipschitz function almost surely. The only caveat is that we may need to distinguish based on the "type" of the node - for example, nodes \(u_{1},u_{2},u_{3}\) that form a triangle may require a different function from nodes that do not. Formally, for node variables \(\), an \(\)_graph type_ is a conjunction of expressions \(E(x_{i},x_{j})\) and their negations. The _graph type of tuple \(\) in a graph_, denoted \(()\) is the set of all edge relations and their negations that hold between elements of \(\). A \((,d)\)_feature-type controller_ is a Lipschitz function taking as input pairs consisting of \(d\)-dimensional real vector and an \(\) graph type.

The key theorem below shows that to each term \(\) we can associate a feature-type controller \(e_{}\) which captures the asymptotic behaviour of \(\), in the sense that with high probability, for most of the tuples \(\) the value of \(e_{}(H_{G}(),())\) is close to \(()\).

**Theorem 5.3** (Aggregate Elimination for Non-Sparse Graphs).: _For all terms \(()\) over featured graphs with \(d\) features, there is a \((,d)\) feature-type controller \(e_{}\) such that for every \(,,>0\), there is \(N\) such that for all \(n N\), with probability at least \(1-\) in the space of graphs of size \(n\), out of all the tuples \(\) at least \(1-\) satisfy that \( e_{}(H_{G}(),())- ()<\)._

This can be seen as a kind of "almost sure quantifier elimination" (thinking of aggregates as quantifiers), in the spirit of Kaila , Keisler and Lotfallah . It is proven by induction on term depth, with the log neighbourhood bound playing a critical role in the induction step for weighted mean.

Theorem 5.3 highlights an advantage of working with a term language having nice closure properties, rather than directly with GNNs: it allows us to use induction on term construction, which may be more natural and more powerful than induction on layers. Theorem 5.3 also gives strong limitations on _node and link classification_ using GNNs: on most nodes in (non-sparse) random graphs, GNNs can only classify based on the features of a node, they cannot make use of any graph structure.

### Overview of the technical constructions for the sparser cases

In the sparser cases, the analysis is a bit more involved. Instead of graph types over \(\), which only specify graph relations among the \(\), we require descriptions of local neighbourhoods of \(\).

**Definition 5.4**.: Let \(G\) be a graph, \(\) a tuple of nodes in \(G\) and \(\). The _\(\)-neighbourhood of \(\) in \(G\)_, denoted \(_{}()\) is the subgraph of \(G\) induced by the nodes of distance at most \(\) from some node in \(\).

The "types" are now graphs \(T\) with \(k\) distinguished elements \(\), which we call _\(k\)-rooted graphs_. Two \(k\)-rooted graphs \((T,)\) and \((U,)\) are _isomorphic_ is there is a structure-preserving bijection \(T U\) which maps \(\) to \(\). The combinatorial tool here is a fact known in the literature as 'weak local convergence': the percentage of local neighbourhoods of any given type converges.

**Lemma 5.5** (Weak local convergence).: _Consider sampling a graph \(G\) from either the sparse \(\) or \(\) distributions. Let \((T,)\) be a \(k\)-rooted graph and take \(\). There is \(q_{T}\) such that for all \(,>0\) there is \(N\) such that for all \(n N\) with probability at least \(1-\) we have that:_

\[|_{}()(T, )\}|}{n}-q_{T}|<\]

Our "aggregate elimination", analogous to Theorem 5.3, states that every term can be approximated by a Lipschitz function of the features and the neighbourhood type.

**Theorem 5.6** (Aggregate Elimination for Sparser Graphs).: _For every \(()\), letting \(\) be the maximum aggregator nesting depth in \(\), for all \(k\)-rooted graphs \((T,)\) there is a Lipschitz function \(e_{}^{T}^{|T| d}^{d}\) such that for each \(,>0\), there is \(N\) such that for all \(n N\) with probability at least \(1-\) in the space of graphs of size \(n\), for every \(k\)-tuple of nodes \(\) in the graph such that \(_{}()(T,)\) we have that \(\|e_{}^{T}(H_{G}(_{}()))-[\![()]\!]\|<\)._

Compared to Theorem 5.3 the result is much less limiting in what node-classifying GNNs can express. Although combining the sparse and non-sparse conditions covers many possible growth rates, it is not true that one gets convergence for Erdos-Renyi with _arbitrary_ growth functions:

**Theorem 5.7**.: _There are functions \(p(n)\) converging to zero and a term \(\) in our language such that \(\) does not converge even in distribution (and hence does not converge a.a.s.) over ER random graphs with growth rate \(p\)._

Proof.: Let \(p\) alternate between \(\) on even \(n\) and \(\) on odd \(n\). Consider \(_{1}(x)\) that returns \(0\) if \(x\) has a neighbour and \(1\) otherwise, and let \(\) be the global average of \(_{1}\). So \(\) is the percentage of isolated nodes in the graph. Then \(\) clearly goes to zero in the non-sparse case. However the probability that a particular node is not isolated in a random sparse graph of size \(n\) is \((1-)^{n-1}\), which goes to \(<1\). Thus \([\![]\!]\) diverges on \((n,p(n))\). 

## 6 Experimental evaluation

We first empirically verify our findings on random graphs and then on a real-world graph to answer the following questions: **Q1**. Do we empirically observe convergence? **Q2**. What is the impact of the different weighted mean aggregations on the convergence? **Q3**. What is the impact of the graph distribution on the convergence? **Q4**. Can these phenomena arise within large real-world graphs?

All our experiments were run on a single NVidia GTX V100 GPU. We made our codebase available online at [https://github.com/benfinkelshtein/GNN-Asymptotically-Constant](https://github.com/benfinkelshtein/GNN-Asymptotically-Constant).

**Setup.** We report experiments for the architectures MeanGNN, GAT , and GPS+RW  with random walks of length up to 5. Our setup is carefully designed to eliminate confounding factors:

* We consider five models with the same architecture, each having randomly initialized weights, utilizing a \(\) non-linearity, and applying a softmax function to their outputs. Each model uses a hidden dimension of \(128\), \(3\) layers and an output dimension of \(5\).
* We experiment with distributions \((n,p(n)=0.1)\), \((n,p(n)=)\), \((n,p(n)=)\), \((n,m=5)\). We also experiment with an SBM of 10 communities with equal size, where an edge between nodes within the same community is included with probability \(0.7\) and an edge between nodes of different communities is included with probability \(0.1\).

* We draw graphs of sizes up to 10,000, where we take 100 samples of each graph size. Node features are independently drawn from \(U\) and the initial feature dimension is \(128\).

To understand the behaviour of the respective models, we will draw larger and larger graphs from the graph distributions. We use five different models to ensure this is not a model-specific behaviour. Further experimental results are reported in Appendix F.

### Empirical results on random graph models

In Figure 3, a single model initialization of the MeanGNN, GAT and GPS+RW architectures is used with \((n,p(n)=0.1)\), \((n,p(n)=)\) and \((n,p(n)=)\). Each curve in the plots corresponds to a different class probability, depicting the average of 100 samples for each graph size along with the standard deviation shown in lower opacity.

The convergence of class probabilities is apparent across all models and graph distributions, as illustrated in Figure 3, in accordance with our main theorems (**Q1**). The key differences between the plots are the convergence time, the standard deviation and the converged values.

One striking feature of Figure 3 is that the eventual constant output of each model is the same for the dense and logarithmic growth distributions, but not for the sparse distribution (**Q3**). We can relate this to the distinct proof strategy employed in the sparse case, which uses convergence of the proportions of local isomorphism types. There are many local isomorphism types, and the experiments show that what we converge to depends the proportion of these. In all other cases the neighbourhood sizes are unbounded, so there is asymptotically almost surely one 'local graph type'.

We observe that attention-based models such as GAT and GPS+RW exhibit delayed convergence and greater standard deviation in comparison to MeanGNN (**Q2**). A possible explanation is that because some nodes are weighted more than others, the attention aggregation has a higher variance than regular mean aggregation. For instance, if half of the nodes have weights close to \(0\), then the

Figure 3: Each plot shows the five mean class probabilities (in different colours) with standard deviations of a single model initialization over \((n,p(n)=0.1)\), \((n,p(n)=)\), and \((n,p(n)=)\), as we draw graphs of increasing size.

attention aggregation effectively takes a mean over half of the available nodes. Eventually, however, the attention weights themselves converge, and thus convergence cannot be postponed indefinitely.

Figure 4 depicts the outcomes of the GPS+RW architecture for various graph distributions. The analysis involves calculating the Euclidean distance between the class probabilities of the GPS+RW architecture and the mean class probabilities over the different graph samples. The standard deviation across the different graph samples is then derived for each of the 5 different model initializations and presented in Figure 4. The decrease of standard deviation across the different model initializations in Figure 4 indicates that all class probabilities converge across the different model initializations, empirically verifying the phenomenon for varying model initializations (**Q1** and **Q3**).

### Empirical results on large real-world graphs

Towards **Q4**, we investigated a large real-world dataset. Many commonly studied graph datasets (e.g. ZINC , QM9 ) do not exhibit sufficient graph size variance and provide no obvious means to add scaling. We used the _TIGER-Alaska_ dataset  of geographic faces. The original dataset has 93366 nodes, while Dimitrov et al.  extracted smaller datasets with graphs having 1K, 5K, 10K, 25K and 90K nodes. We chose the modified dataset as it is split by graph size, and consists of graphs differing from our random models (in particular all graphs are planar). Figure 5 shows the results of applying the same five MeanGNNs to graphs of increasing sizes. Strikingly, we again observe a convergence phenomenon, but at a slower pace.

## 7 Discussion and limitations

We have demonstrated a wide convergence phenomenon for real-valued classifiers expressed even in very advanced GNN architectures, and it applies to a great variety of random graph models. Rather than having separate proof techniques per GNN model, our paper introduces a broad language where such models can be situated, and provides techniques at the level of term languages. Although our top-level theorems deal with graph-level tasks, along the way we provide strong limitative results on what can be achieved on random graphs for node- or edge-level real-valued tasks: see Theorem 5.3.

The principal limitations of our work come our assumptions. In particular, we assume that the initial node embeddings are i.i.d. This assumption is used in the application concentration inequalities throughout the proofs, so loosening it would require careful consideration.

Our main results show that many GNN architectures cannot distinguish large graphs. To overcome this limitation, one could consider moving beyond our term language. For example, if we add _sum aggregation_, the term values clearly diverge, and similarly if we allow non-smooth functions, such as linear inequalities. Further we emphasize that a.a.s. convergence is not universal for our term language, and it does not hold even for ER with _arbitrary_\(p(n)\) going to \(0\): see Theorem 5.7.

Figure 4: Each plot depicts the standard deviation of Euclidean distances between class probabilities and their respective means across various samples of each graph size for GPS+RW.

Figure 5: Standard deviation of distances between class probabilities and their means across TIGER-Alaska graph sizes for MeanGNN.