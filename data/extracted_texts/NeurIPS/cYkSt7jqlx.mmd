# Context-guided Embedding Adaptation for

Effective Topic Modeling in Low-Resource Regimes

 Yishi Xu, Jianqiao Sun, Yudi Su, Xinyang Liu, Zhibin Duan, Bo Chen,

National Key Laboratory of Radar Signal Processing, Xidian University, Xi'an, China, 710071

{xuyishi, jianqiaosun}@stu.xidian.edu.cn, bchen@mail.xidian.edu.cn

&Mingyuan Zhou

McCombs School of Business, The University of Texas at Austin, TX 78712, USA

mingyuan.zhou@mcombs.utexas.edu

Equal contributionCorresponding author

###### Abstract

Embedding-based neural topic models have turned out to be a superior option for low-resourced topic modeling. However, current approaches consider static word embeddings learnt from source tasks as _general knowledge_ that can be transferred directly to the target task, discounting the dynamically changing nature of word meanings in different contexts, thus typically leading to sub-optimal results when adapting to new tasks with unfamiliar contexts. To settle this issue, we provide an effective method that centers on adaptively generating semantically tailored word embeddings for each task by fully exploiting contextual information. Specifically, we first condense the contextual syntactic dependencies of words into a semantic graph for each task, which is then modeled by a _Variational Graph Auto-Encoder_ to produce task-specific word representations. On this basis, we further impose a learnable Gaussian mixture prior on the latent space of words to efficiently learn topic representations from a clustering perspective, which contributes to diverse topic discovery and fast adaptation to novel tasks. We have conducted a wealth of quantitative and qualitative experiments, and the results show that our approach comprehensively outperforms established topic models.

## 1 Introduction

The last two decades have witnessed the enduring popularity of topic models along with their many successful applications in a range of fields [1; 2; 3; 4; 5; 6; 7]. And this is predominantly attributed to their ability to reveal the underlying semantic structure from large volumes of textual data. By identifying a group of salient themes, topic models represent each document as a mixture of them, providing an intuitive understanding of the target corpus. Although conventional probabilistic topic models [8; 9; 10; 11; 12; 13; 14] have been widely used, new variants continue to spring up in the era dominated by deep neural networks.

Among the proliferation of new methodologies come both the neural topic models (NTMs) [15; 16; 17; 18] resulting from the development of variational autoencoders (VAEs) and autoencoding variational Inference (AVI) [19; 20], and the contextualized topic models (CTMs) [21; 22; 23; 24] benefiting from the flourishing of pre-trained language models [25; 26]. However, these recently developed approaches essentially maintain the assumption of sufficient resources, _i.e._, with a plethora of documents being available. Comparatively, little attention has been paid to topic modeling under resource-limited or resource-poor conditions , which plays a significant role in real-world applications.

A case in point occurs in personalized recommendation systems where users' preferences are judged based on only a small amount of their historical data, such as past purchases or online behaviors . Another example arises from crisis management. Since being able to quickly identify and monitor emerging topics during a crisis, _e.g._, a public health emergency, could substantially support a government or organization in responding appropriately to rapidly changing situations .

While there have also been some beneficial attempts to study few-shot learning for topic modeling, they more or less exhibit certain limitations. For instance, Iwata  first proposed a straightforward method that aims to learn good, _i.e._, task-specific priors for latent Dirichlet allocation (LDA)  by using neural networks. However, this approach suffers from a lack of expressiveness when dealing with difficult novel tasks. Later on, Duan et al.  claimed that embedding-based NTMs  are naturally superior in generalizing to new and unseen tasks. By considering word embeddings learned from the training task pool as transferable knowledge, they have shown that effective generalization can be achieved by only learning topic embeddings adaptively. Nevertheless, as the word semantics inevitably change with contexts, the learned static word embeddings may not adapt well to the target task with alien contexts. On the other hand, we have experimentally found that the performance of two representative CTMs [21; 22] is also not competitive enough under resource-poor conditions.

To address the above issues, we propose to learn adaptive word embeddings suitable for each task by fully exploiting the contextual grammar information. Concretely, we first construct a task-specific semantic graph between words using well-established dependency parsing tools3. This graph helps depict the precise meaning of each word in the given task, which we then model with a created variational graph autoencoder (VGAE) , and the resulting latent representations of words merge the contextual information and are thus semantically tailored to the given task. Furthermore, by imposing a Gaussian mixture prior on the latent space of word, we offer a perspective on learning topics through clustering, _i.e._, each component of the Gaussian mixture can be viewed as the representation of a topic. Consequently, the adaptive word embeddings tend to be reasonably covered by several clusters, facilitating the discovery of more interpretable topics closely related to the given task.

Our main contributions can be summarized as follows:

* To solve the problem of few-shot learning for topic modeling, we propose to learn adaptive word embeddings that are semantically matched to the given task.
* To generate adaptive word embeddings for each task, we innovatively introduce a variational graph autoencoder to learn the latent representations of words from the task-specific semantic graph that captures the contextual syntactic dependencies.
* To mine more interpretable topics related to the given task, we offer a perspective on learning topics through clustering by imposing a Gaussian mixture prior on the word latent space. Besides, we also develop an efficient variational inference scheme to approximate the posteriors of latent variables.

## 2 Meta-CETM

### Problem Formulation

To clarify the problem definition of few-shot learning for topic modeling, we assume that there are \(C\) training corpora \(\{_{c}\}_{c=1}^{C}\) from different domains, with the goal of using them to learn a topic model that generalizes to the test corpus \(_{test}\) in a new domain, _i.e._, the resulting topic model is supposed to quickly adapt to a given test task with a few documents and mine topics related to the new domain. Further, we adopt an episodic training strategy as is conventional in most few-shot learning literature. Specifically, we construct a batch of training tasks \(\{^{(i)}\}_{i=1}^{M}\) to mimic the scenario of giving tasks at test time, so that each task contains only several documents from an arbitrary training corpus. And we denote the bag-of-words (BoW) representations of the task documents as \(^{(i)}^{V J}\), where \(V\) is the vocabulary size and \(J\) is the number of documents in each task. In addition, our method also builds a task-specific dependency graph whose adjacency matrix is represented as \(^{(i)}^{V V}\). To understand the concepts of corpus, task, and document more clearly, see the example in Appendix C.

### Generative Model

In this section, we present a **C**ontextualized **E**mbedded **T**opic **M**odel, dubbed as **Meta-CETM**, to cope with the problem of topic modeling under resource-poor conditions, the essence of which is to make extra use of the contextual information of given documents to learn dynamic word embeddings that are well adapted to the current task. Concretely, for any given task \(^{(i)}\), in addition to obtaining the BoW representations \(^{(i)}\) of its documents, which only imply the word co-occurrence patterns, we also build a semantic graph between words based on the contextual dependency grammars. Thus the corresponding adjacency matrix \(^{(i)}\) can be viewed as complementary information on the word semantics. With \(^{(i)}\) and \(^{(i)}\), then the goal of the generative model is to model these two types of observations jointly, whose key lies in how to establish a bridge connecting the two.

Inspired by the recently developed VGAEs  and embedded topic models (ETMs) [18; 31], we create our generative model based on the compact assumption, _i.e._, there exists a shared latent space of words that can be mapped to the observation spaces of BoW and semantic graph, respectively. Following this conception, we portray a well-structured generative process via a specific task \(^{(i)}\). First, the latent representations of words \(^{(i)}^{D V}\) are sampled from a prior distribution. Acting as the suitable connection, they are responsible for generating both the adjacency matrix \(^{(i)}\) and the BoW \(^{(i)}\). To produce \(^{(i)}\), we use a typical inner product graph decoder formulated as

\[^{(i)}((^{(i)}{}^{}^{ (i)}))\] (1)

As for the generation of BoW \(^{(i)}\), the latent representations \(^{(i)}\) (or word embeddings) are mainly used to derive the topic-word matrix. Unlike previous ETMs that usually decompose the topic-word matrix into the inner product of learnable word embeddings and topic embeddings, here we provide a Bayesian perspective on interpreting the topic-word matrix by imposing a Gaussian mixture prior to the word latent space. More precisely, we regard each component of the Gaussian mixture prior as the representation of a topic, such that the word embeddings are naturally generated from different clusters (topics). And the prior probability for the \(v^{th}\) word embedding is given by

\[p(_{v}^{(i)})=_{k=1}^{K}_{k}(_{v}^ {(i)}|_{k},_{k}),\] (2)

where \(_{k}\) is the prior mixing coefficient of the \(k^{th}\) topic and \(_{k}_{k}=1\). As such, each topic can be represented as a Gaussian distribution in the word latent space, with a probability density assigned to each word embedding. This provides a natural choice to define the topic-word matrix \(^{(i)}^{V K}\).

Figure 1: Overview of the proposed method. The top branch establishes a standard neural topic modeling pipeline, and the bottom branch creates a graph VAE to learn contextualized word embeddings, with a Gaussian mixture prior imposed on the latent space to yield task-specific topic representations. Note that the topic-word matrix is derived based on the probability density assigned to the adaptive word embeddings.

Hence, the distribution of topic \(k\) over the vocabulary \(_{k}^{(i)}^{V}\) can be derived by

\[_{k}^{(i)}=(p(}^{(i)}|_{k},_{k})),\] (3)

In this way, we expect that semantically similar words will be close in the latent space and generated from the same topic. Moreover, we also posit a context variable \(^{(i)}\) that summarizes the task-level information about the topic proportion and thus serves as a prior for generating the topic proportion \(_{j}^{(i)}\) of each document. Formally, the complete generation process of our model can be formulated as

1. Generate the adjacency matrix of semantic graph using Eq. 1;
2. For the \(v^{th}\) word appeared in the task \(^{(i)}\): 1. Draw its topic assignment \(y_{v}^{(i)}()\); 2. Draw its word embedding \(_{v}^{(i)}(_{y_{v}^{(i)}},_{y_{v} ^{(i)}})\);
3. Compute task-specific topic-word matrix \(^{(i)}\) based on Eq. 3;
4. Draw task-level context variable \(^{(i)}(0,a^{2})\);
5. For the \(j^{th}\) document in the task \(^{(i)}\): 1. Draw its topic proportion \(_{j}^{(i)}(^{(i)},b^{2})\); 2. For the \(n^{th}\) word in the \(j^{th}\) document: 1. Draw its topic assignment \(e_{jn}^{(i)}(_{j}^{(i)})\); 2. Generate the word count \(x_{jn}^{(i)}(_{e_{jn}^{(i)}}^{(i)})\),

where \(()\), \(()\), \(()\), and \(()\) denote categorical, Gaussian, Bernoulli, and logistic-normal distributions, respectively. \(\) is the \(()\) operation4. \(a\) and \(b\) are both hyper-parameters. Table 4 in Appendix B gives a list of key notations used in this paper.

### Variational Inference Algorithm

Observing a task \(^{(i)}\) with data \(}^{(i)}\), the goal of inference network is to approximate the posterior distributions over the latent variables, \(\{_{j}^{(i)},^{(i)},}^{(i)}\}\), and the parameters of GMM, \(\{_{k}^{(i)},_{k}^{(i)},_{k}^{(i)}\}\).

**Document-specific latent variable inference.** To model the uncertainty of topic proportions, we define the variational posterior distribution for \(_{j}^{(i)}\), and a residual multi-layer perception (MLP) is employed to learn distribution parameters. To be specific,

\[q(_{j}^{(i)}|_{j}^{(i)},^{(i)}) =(_{_{j}^{(i)}},_{ {}_{j}^{(i)}})\] (4) \[_{_{j}^{(i)}},_{_{j}^{( i)}} =(_{j}^{(i)}+^{(i)})\] \[_{j}^{(i)} =_{}(_{j}^{(i)})\]

where \(_{_{j}^{(i)}}\) and \(_{_{j}^{(i)}}\) are logistic Gaussian distribution deterministic parameters depending on the latent mean \(^{(i)}\) and the document latent representation \(_{j}^{(i)}\) of \(_{j}^{(i)}\).

**Task-specific latent variable inference.** For topic proportion mean \(^{(i)}\), we apply Gaussian distribution to approximate the variational posterior, whose parameters mean vectors and covariance matrices are derived through encoding all documents in task \(^{(i)}\) with attention mechanism , written as

\[q(^{(i)}|}^{(i)})=(_{^{(i)}},_{^{(i)}});_{^{(i)}},_{^{(i)}}= (}^{(i)});}^{(i)}=(}^{(i)}),\] (5)

where \(}^{(i)}\) is the latent indication of the task, and Attn() is the attention mechanism to capture relationships of \(J\) documents in task \(^{(i)}\).

Additionally, to ensure the modeling flexibility, we design a simple inference network consisting of a two-layer GCN  to infer the latent representations \(^{(i)}\) of words following :

\[ q(^{(i)}|^{(i)},^{(i)} )&=(^{(i)}|^{(i)}_{},^{(i)}_{})\\ ^{(i)}_{},^{(i)}_{}& =(^{(i)},^{(i)})\] (6)

where \(^{(i)}\) is the initialized word features and \(^{(i)}\) is derived through a neural parser on \(^{(i)}\). Figure 1 illustrates an overview of our approach, including the variational inference network.

In this paper, for simplicity, we denote \(\) as network parameters of both the encoder and the decoder.

**Expectation Maximization for solving \(^{(i)}_{k}\)** and \(^{(i)}_{k}\). As discussed before, \(^{(i)}_{k}\) and \(^{(i)}_{k}\) are task-specific parameters of Gaussian mixture distribution, which are not learnable variables, and we do not have analytic solutions for Maximum Likelihood Estimation (MLE) of GMM containing the non-differentiable sampling process . To approximate the posterior, we resort to Expectation Maximization (EM)  algorithm to optimize the parameters, formulated as:

\[& Q^{(i)}_{v}:\;=p(y^{(i )}_{v}=k|^{(i)}_{v})=_{k}(^{(i)} _{v};^{(i)}_{k},^{(i)}_{k})}{_{k}^{(i)}_{k} (^{(i)}_{v};^{(i)}_{k},^{(i)}_{k})}\\ &^{(i)}_{k}:\;=Q ^{(i)}_{v}^{(i)}_{v}}{_{v}Q^{(i)}_{v}}\\ &^{(i)}_{k}:\;=Q^{(i)}_{v}(^{( i)}_{v}-^{(i)}_{k})(^{(i)}_{v}-^{(i)}_{k})^{T}}{_{v}Q^{( i)}_{v}}\\ &^{(i)}_{k}:\;=Q^{(i)}_{v}}{_{k}_{v}Q^{( i)}_{v}}.\] (7)

Since topic \(k\) is sampled from the Uniform distribution for each task, the mixing coefficients are initialized as \(\). We initialize \(^{(i)}_{k}\) and \(^{(i)}_{k}\) as the average of latent variables \(^{(i)}\) and the identity matrix \(\), respectively. Here, we only display the final updating formulas. The detailed derivation processes for E-step and M-step are presented in Appendix E.2.

### Training Objective and Optimization

By Jensen's inequality, the evidence lower bound (ELBO) of each task can be derived as

\[_{ELBO}&=_{j=1}^{J} _{Q}[ p(^{(i)}_{j}^{(i)}_{j}, ^{(i)})]+_{j=1}^{J}_{Q}[^{(i)}_{j}^{(i)})}{q(^{(i)}_{j}^{( i)}_{j},^{(i)})}]\\ &+_{Q}[^{(i)})}{q(^{(i)} ^{(i)})}]+_{Q}[ p(^{(i)} ^{(i)})]+_{Q}[^{(i)})}{q( ^{(i)}^{(i)},^{(i)})}]\] (8)

where

\[Q=_{j=1}^{J}q(^{(i)}_{j}^{(i)}, ^{(i)})q(^{(i)}^{(i)})q(^{(i)}^{( i)},^{(i)})\] (9)

is the variational joint distribution. The first and the fourth terms in Eq. 8 are the reconstruction errors for document BoW and the graph adjacency matrix, respectively. The remaining three terms are all Kullback-Leibler (KL) divergence to constrain the distance between the prior distribution and the variational posterior distribution. Owing to the space limit, we only present the final formulas for \(_{ELBO}\) here. The detailed derivations, the training algorithm, and the meta-testing algorithm can be found in Appendix E.1, Alg. 1, and Alg. 2, respectively.

## 3 Experiments and Analysis

### Experimental setup

**Datasets.** We conducted experiments on four widely used textual benchmark datasets, specifically _20Newsgroups_ (**20NG**) , _Yahoo Answers Topics_ (**Yahoo**) , _DBpedia_ (**DB14**) , and _Web of Science_ (**WOS**) .

**Baseline methods.** Our model is compared with exemplary baseline methods, including probabilistic topic models, state-of-the-art NTMs and CTMs. Specifically, we conduct experiments of the following methodologies under the document-limited setting: 1) **LDA**; 2) **PFA**; 3) LDA with Products of Experts (**ProdLDA**) , which replaces the mixture model in LDA with products of experts and updates parameters using AVI; 4) Embedded Topic Model (**ETM**) , an NTM incorporating word embeddings and learning with AVI. For a fair comparison, we also consider the variations of ProdLDA and ETM under the few-shot setting, referred to 5) **MAML-ProdLDA** and 6) **MAML-ETM**, where the parameters are optimized through MAML algorithm . In addition, we include a model-based hierarchical NTM, 7) **Meta-SawETM**, but we only apply their single-layer model. Moreover, another two CTMs 8) **CombinedTM** and 9) **ZeroShotTM** are also compared, both of which contain contextualized Sentence BERT  embeddings as the model input.

### Experimental results and analysis

In this section, we evaluate the predictive performance, topic quality, and classification performance of our model through an extensive series of experiments. Note that in all tables, we have highlighted the best and runner-up results in boldface and with an underline, respectively. Our code is available at https://github.com/NoviceStone/Meta-CETM.

#### 3.2.1 Per-holdout-word perplexity

Following the practice in Meta-SawETM , we adopt the per-holdout-word perplexity (PPL)  to measure the predictive performance of our model. Specifically, for each task composed of several documents, \(80\%\) of the tokens in the BoWs are randomly chosen to form a support set \(D_{test}^{S}\), which is used to adapt to a task-specific topic-word matrix \(\), and the remaining \(20\%\) word tokens are held out to form the query set \(D_{test}^{Q}\) with data \(\). Then, the PPL can be calculated as

\[\{-}_{v=1}^{V}_{n=1}^{N}y_{ vn}^{S}_{u=1}^{K}_{u}^{s}_{u}^{s}}{_{s=1}^{S} _{v=1}^{K}_{v}^{s}_{u}^{s}}\},\] (10)

where \(S\) is the total number of collected samples and \(y_{}=_{v=1}^{V}_{n=1}^{N}y_{vn}\).

**Model settings.** For all compared methods, we set the number of topics as 10. And for all NTMs, the hidden layers size of the encoder is set to 300. For all embedding-based topic models, _i.e.,_ ETM, MAML-ETM, Meta-SawETM and our Meta-CETM, we load pretrained GloVe word embeddings  as the initialization for a fair comparison. Finally, We train our model using the Adam optimizer  with a learning rate of \(1 10^{-2}\) for 10 epochs on an NVIDIA GeForce RTX 3090 graphics card.

**Results.** In Table 1, we list the PPL of ten compared methods on four datasets. It can be noticed although LDA and PFA are both traditional probabilistic topic models, PFA presents better results than LDA. By utilizing MAML to learn parameter initializations, MAML-ProdLDA performs better than ProdLDA while MAML-ETM exhibits poorer results than ETM. This can be attributed to that ETM possesses more parameters than ProdLDA, and calculating the gradients in a high-dimensional space with only a few documents is difficult for MAML-ETM. Applying Weibull distribution to model the latent representation for documents and employing task-specific variable designs for both

    &  &  &  &  \\   & 5 & 10 & 5 & 10 & 5 & 10 & 5 & 10 \\  LDA  & 4021\(\)1528 & 3502\(\)1277 & 4476\(\)1544 & 4028\(\)1097 & 4410\(\)1918 & 3697\(\)1747 & 3439\(\)671 & 3246\(\)461 \\ PFA  & 3463\(\)1452 & 3150\(\)1119 & 3257\(\)1328 & 3212\(\)1040 & 3443\(\)1937 & 3170\(\)1562 & 3131\(\)819 & 3431\(\)830 \\ ProDLDA  & 8531\(\)1034 & 4523\(\)1817 & 5765\(\)1101 & 5378\(\)1846 & 5297\(\)740 & 4311\(\)469 & 4220\(\)392 \\ ETM  & 3192\(\)895 & 3107\(\)671 & 2868\(\)909 & 2817\(\)620 & 3217\(\)1960 & 3054\(\)1539 & 3135\(\)704 & 3310\(\)455 \\  MAML-ProdLDA* & 4292\(\)1123 & 4355\(\)997 & 4354\(\)1369 & 4250\(\)919 & 4844\(\)1337 & 4678\(\)1119 & 4117\(\)462 & 4068\(\)332 \\ MAML-ETM* & 3849\(\)1064 & 3725\(\)841 & 3653\(\)1081 & 3642\(\)776 & 4448\(\)2737 & 4279\(\)2301 & 3483\(\)4044 & 3277\(\)644 \\ Meta-SawETM  & 2872\(\)869 & 2844\(\)740 & 2365\(\)934 & 287\(\)756 & 2047\(\)1374 & 1941\(\)1009 & 3213\(\)445 & 2253\(\)315 \\ CombinedTM  & 2660\(\)659 & 2595\(\)625 & 2700\(\)590 & 2674\(\)575 &topic proportions and topic matrices, Meta-SawETM acts superior to other topic models. Moreover, incorporating contextualized embeddings as input enables CombinedTM and ZeroShotTM to present competitive performance. Furthermore, it is noteworthy that Meta-CETM achieves the lowest PPL among all these methods, indicating the excellent predictive performance of our model.

#### 3.2.2 Topic quality

In this part, we evaluate the topic quality of different methods in terms of the topic diversity (**TD**) , defined as the percentage of unique words in the top 10 words of all topics, and the topic coherence (**TC**), which measures the average Normalized Pointwise Mutual Information (NPMI) [50; 51] values to count word co-occurrences5. The experimental settings are the same as in Sec. 3.2.1. For each task, the number of documents is 10 for all datasets. The results are displayed in Fig. 2 and it can be notably and interestingly observed that MAML-ProdLDA achieves fairly "perfect" TD results, but it shows the worst TC performances among compared methods. Such inconsistency is brought by MAML-ProdLDA's concentration on a large amount of universal and frequently-occurring words when training on the base data, hindering it from extracting informative topics given the meta-test task. Embobying the embedding design for words and topics, MAML-ETM and Meta-SawETM are more likely to mine context-related topics than MAML-ProdLDA. Besides, as the representatives of CTM, CombinedTM and ZeroShotTM equipped with BERT embeddings present more comprehensive performances than previous topic models. From the TC perspective, our Meta-CETM yields the most favorable results among the six methods, indicating the fast adaptability in discovering interpretable topics with limited documents.

#### 3.2.3 Few-shot document classification

To further validate that our model is capable of learning topics which are highly adapted to the task, we undertake experiments on few-shot document classification.

**Model settings.** As in Meta-SawETM , we compare our Meta-CETM with classical meta-learning algorithms under different architectures. Specifically, we design a three-layer feedforward network as **MLP** structure and three-layer 1-dimensional convolutions followed by batch normalization, \(()\) activation and max-pooling operation as **CNN**. For meta-learning methods, we apply **MAML** to learn parameter initializations, and prototypical network (**PROTO**)  to learn an embedding space and minimize the distance between the clustering centroids and the samples. Besides, we adopt two fine-tuning manners, named **FT** and **FT***, to update the classifier parameters and all parameters of the model, respectively. Single-layer HNS-SawETM and single-layer Meta-SawETM in  are compared as well. Additionally, we investigate the CombinedTM  and ZeroShotTM  to evaluate their few-shot classification performance. Different from PPL evaluation in Sec. 3.2.1, the support set and the query set for classification are sourced from two batches of documents.

Figure 2: Topic diversity results (top row) and topic coherence results (bottom row) on four datasets of six compared methods. The number of documents for each task is 10.

**Classification strategy.** For typical few-shot learning algorithms, we follow the convention to train parameters of the feature extractor in a supervised manner. At meta-test stage, we use the support set (_i.e.,_ a few labeled examples) to adapt to a task-specific classifier and compute the accuracy on the query set. As for topic models, where no dedicated classifiers are available, we first use the support set to learn a group of class-specific topic-word matrices, then for each document in the query set, we calculate its reconstruction error as the basis for classification.

**Results.** The classification results6 are listed in the Table 2. As we can see, the CNN architecture outperforms MLP under the same algorithms, which can be attributed to CNN's unique inductive bias of the locality. Furthermore, PROTO surpasses MAML by a large margin in most cases, indicating that a good embedding space is more useful for classification than favorable parameter initializations. For the CNN architecture, fine-tuning the classifier only (FT) and fine-tuning all parameters (FT*) make slight differences; but for MLP, we observe a considerable performance boost by updating all parameters of the network over the FT algorithm. We postulate that the classification results of MLP are more susceptible to the variation of feature extractor parameters due to its linear structure. Additionally, CombinedTM and ZeroShotTM, incorporating contextualized representations, outperforms HNS-SawETM and Meta-SawETM by learning class-specific topic-word matrices more effectively. With the design of adaptive word embeddings, our Meta-CETM not only achieves much better results than previous topic models, but also is comparable to few-shot learning algorithms particularly designed for supervised learning.

#### 3.2.4 Embedding space visualization

    &  &  &  &  \\  Rep. & Alg. & 5 shot & 10 shot & 5 shot & 10 shot & 5 shot & 10 shot & 5 shot & 10 shot \\   & MAML  & 32.01 & 36.20 & 50.20 & 60.30 & 45.42 & 51.00 & 37.77 & 40.43 \\  & PROTO  & 35.20 & 38.30 & 54.13 & 57.16 & 50.01 & 56.16 & 39.61 & 41.46 \\  & FT  & 29.70 & 33.04 & 51.11 & 53.83 & 48.59 & 53.06 & 36.52 & 37.22 \\  & FT* & 38.87 & 48.52 & 71.12 & 77.94 & 50.73 & 56.74 & 45.02 & 51.20 \\   & MAML  & 34.08 & 45.40 & 66.28 & 75.96 & 48.81 & 56.50 & 47.28 & 57.32 \\  & PROTO  & 39.86 & 49.71 & **78.58** & **81.01** & 53.16 & 63.66 & 59.05 & **67.75** \\  & FT  & 45.70 & 53.63 & 74.68 & 80.75 & 56.78 & 66.04 & 54.68 & 63.39 \\  & FT* & 44.53 & 51.92 & 72.49 & 80.07 & 53.28 & 52.56 & 51.42 & 61.98 \\  HNS-SawETM  & 39.37 & 43.78 & 65.93 & 71.08 & 52.35 & 57.86 & 42.09 & 56.91 \\ Meta-SawETM  & 39.19 & 45.83 & 67.20 & 72.31 & 52.45 & 60.58 & 43.39 & 57.44 \\ CombinedTM  & 46.17 & 52.73 & 68.42 & 73.26 & 57.94 & 64.75 & 56.16 & 65.97 \\ ZeroShotTM  & 46.65 & 52.08 & 71.93 & 76.09 & **53.82** & 66.21 & 58.50 & 66.10 \\ Meta-CETM & **50.57** & **58.47** & 76.85 & 79.34 & **63.84** & **72.67** & **61.47** & 67.62 \\   

Table 2: 5-way 5-shot and 5-way 10-shot few-shot document classification results on all four datasets. *denotes all parameters of the model are fine-tuned.

Figure 3: Visualization of the adapted embedding space for (a) MAML-ETM, (b) Meta-SawETM and (c) Meta-CETM. The small grey points represent word embeddings, and the big blue points denote topic embeddings for MAML-ETM, topic means for Meta-SawETM and our Meta-CETM. For Meta-SawETM and our Meta-CETM, the ellipse coverages represent topic covariances. For MAML-ETM, the ellipse coverages are the areas of top words. The target task is sampled from the sub-topic “rec.sport.hockey” of 20NG dataset.

In addition to quantitative results, we also qualitatively analyzed the effectiveness of our model by visualizing the adapted embedding space, as shown in Figure 3. It can be seen that both MAML-ETM and Meta-SawETM learn topics that are not highly relevant to the target task, as most of their top words (_e.g., windows_ and _dos_) are inherited from the base training corpora, while those informative words (_e.g., players_ and _hockey_) associated with the target task are away from the topic embeddings. By contrast, in the embedding space learned by Meta-CETM, the adapted Gaussian distributions, _i.e.,_ topics reasonably cover almost all words closely related to the target task, indicating that our model can achieve successful adaptation and discover more interpretable topics.

Furthermore, we investigated whether the adaptive word embeddings generated by our model effectively reflect the contextual information of each given task. As illustrated in Figure 4, the word embedding of "apple" adapted from a task related to _company_ is surrounded by words like "mobile" and "amazon", whereas in another task concerning _plant_, the embedding of "apple" is closer to the words such as "fruit" and "trees". This phenomenon suggests that Meta-CETM produces word embeddings that are semantically well-matched to the context of the target task.

### Ablation study

To explore the effectiveness of key designs in our model, we conduct a series of ablation experiments on all four datasets by removing each designed module separately. The numerical results of perplexity, topic diversity, and topic coherence are listed in Table 3, where ETM  is chosen as the baseline method. From the results presented below, we have the following observations:

_i)_ Despite achieving the best TD results, ETM obtains the worst PPL and TC scores, which can be attributed to its tendency to extract frequently occurring themes from training, hindering its fast adaptability to meta-test sets with only several documents. _ii)_ To capture the relations of texts and model the task information, in Section 2.2, we posit a context-specific variable \(}\) as the prior of document-level topic proportion \(_{j}^{(i)}\). It can be found with \(}\), our Meta-CETM achieves much lower PPL and higher TC, demonstrating the efficacy of task-specific variables. _iii)_ For ablation results without the **Graph VAE** module, we replace it with a **Graph AE** rather than discarding it completely. Besides, for Meta-CETM without the **GMM prior** design, we replace it with the **standard Gaussian distribution prior** rather than imposing no prior completely. From the quantitative results shown

Figure 4: The adaptive contextual word embeddings learned by our Meta-CETM on DB14 dataset . Left: The local embedding space of a task from the “Company” domain, Right: The local embedding space of a task in the “Plant” domain.

    & context variable & Graph VAE & GMM prior &  & DB14 \\   & & & & PPL & TD & TC & PPL & TD & TC \\  ETM & & & & & 3107 & **0.8395** & -0.8437 & 3054 & **0.8106** & -0.8719 \\  & & ✓ & ✓ & 1964 & 0.8031 & -0.4301 & 1682 & 0.7441 & -0.4917 \\  & ✓ & & ✓ & & 1255 & 0.7983 & -0.4169 & 1131 & 0.7210 & -0.5025 \\  & ✓ & ✓ & & & 1361 & 0.6538 & -0.4688 & 1276 & 0.6562 & -0.6677 \\ 
**Meta-CETM** & ✓ & ✓ & ✓ & **1170** & 0.8154 & **-0.3701** & **1084** & 0.7475 & **-0.4783** \\    & context variable & Graph VAE & GMM prior &  & WOS \\   & & & & PPL & TD & TC & PPL & TD & TC \\  ETM & & & & 2817 & **0.8851** & -0.8913 & 3310 & **0.9286** & -0.9785 \\  & & ✓ & ✓ & 1906 & 0.7243 & -0.5097 & 2023 & 0.9141 & -0.5420 \\  & ✓ & & ✓ & 1316 & 0.7612 & -0.4860 & 1304 & 0.8387 & -0.5262 \\  & ✓ & ✓ & & 1271 & 0.5847 & -0.5503 & 1389 & 0.7018 & -0.5587 \\ 
**Meta-CETM** & ✓ & ✓ & ✓ & **1219** & 0.7886 & **-0.4639** & **1293** & 0.8667 & **-0.5177** \\   

Table 3: Ablation study on four datasets. The number of texts in each task is 10. “✓” means we add the corresponding design into ETM , which is chosen as the baseline.

in the table, it can be found compared with Gaussian prior in vanilla VAE model, GMM priors encourage the semantically coherent words to be allocated to the same topic, leading to higher TC scores and lower PPL. Further, we visualize the embedding space in Figure 5 to demonstrate the benefits of GMM prior.

## 4 Related Work

To discover a group of topics from a batch of documents, probabilistic topic models have been developed in recent years, including hierarchical topic models [54; 14; 46; 55], NTMs [56; 15],embedded topic models [18; 31], and CTMs [22; 21]. Besides, some works are proposed to incorporate knowledge into the modeling process [57; 58; 59] and some researchers apply optimal transport to measure the distances between topics and words or documents [60; 61]. From the perspective of modelling word embeddings with Gaussian distribution, Vilnis _etc._ proposed a novel density-based mapping method  and achieved promising performances. Recently, some works [30; 27] aimed at topic modelling under the few-shot setting and proposed to obtain a group of task embeddings to adaptively explore topics within only a few documents in a model-based meta-learning fashion. However, their method cannot address the multiple meanings of one word issue, a prevalent issue in practical document analysis, which is addressed through the introduction of dependency graph and the GMM prior distribution in our work.

## 5 Conclusion

In this paper, we propose a novel NTM, Meta-CETM, to address the the fast adaption problem in document analysis under low-resource regimes. Specifically, we construct a task-specific graph to obtain context-related word embeddings. Then we introduce the graph VAE with Gaussian mixture prior to model the word representations and topic embeddings, which are optimized through the EM algorithm. We also propose the task-specific prior for topic proportions. Through extensive experiments and illustrations, we demonstrate the superior performance of our model in solving the adaptation problem in topic modeling.

## 6 Acknowledgements

This work was supported in part by the National Natural Science Foundation of China under Grant U21B2006; in part by Shaanxi Youth Innovation Team Project; in part by the Fundamental Research Funds for the Central Universities QTZX23037 and QTZX22160; in part by the 111 Project under Grant B18039; in part by the Fundamental Research Funds for the Central Universities; in part by the Innovation Fund of Xidian University under Grant YJSJ23016.

Figure 5: Illustration of the advantage of using a GMM prior. The adapted embedding space of our model by using (a) no prior, _i.e._, Graph VAE is replaced with a vanilla graph AE, and the topic embeddings are learnable point vectors, (b) a standard normal prior, _i.e._, the topic embeddings are learnable Gaussian distributions which are constrained by KL divergence with the standard normal distribution, (c) a GMM prior. The target task is sampled from the sub-topic ”rec.sport.hockey” of the 20NG dataset. Room in for better visual effects.