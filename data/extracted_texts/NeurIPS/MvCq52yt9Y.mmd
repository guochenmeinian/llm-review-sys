# Yifei Zhang\({}^{\dagger}\), Hao Zhu\({}^{\dagger,\lx@sectionsign}\), Yankai Chen\({}^{\dagger}\), Zixing Song\({}^{\dagger}\), Piotr Koniusz\({}^{\star,\lx@sectionsign,\lx@sectionsign,\ddagger}\), Irwin King\({}^{\dagger}\)

Mitigating the Popularity Bias of Graph Collaborative Filtering: _A Dimensional Collapse Perspective_

 Yifei Zhang\({}^{}\), Hao Zhu\({}^{}\), Yankai Chen\({}^{}\), Zixing Song\({}^{}\), Piotr Koniusz\({}^{}\),\({}^{@sectionsign,}\), Irwin King\({}^{}\)

\({}^{}\)The Chinese University of Hong Kong

\({}^{}\)Australian National University, \({}^{@sectionsign}\)Data6\({}^{}\)CSIRO

{yfzhang, ykchen, zxsong, king}@cse.cuhk.edu.hk

allenhaozhu@gmail.com, piotr.koniusz@data61.csiro.au

###### Abstract

Graph Collaborative Filtering (GCF) is widely used in personalized recommendation systems. However, GCF suffers from a fundamental problem where features tend to occupy the embedding space inefficiently (by spanning only a low-dimensional subspace). Such an effect is characterized in GCF by the embedding space being dominated by a few of popular items with the user embeddings highly concentrated around them. This enhances the so-called Matthew effect of the popularity bias where popular items are highly recommend whereas remaining items are ignored. In this paper, we analyze the above effect in GCF and reveal that the simplified graph convolution operation (typically used in GCF) shrinks the singular space of the feature matrix. As typical approaches (_i.e_., optimizing the uniformity term) fail to prevent the embedding space degradation, we propose a decorrelation-enhanced GCF objective that promotes feature diversity by leveraging the so-called principle of redundancy reduction in embeddings. However, unlike conventional methods that use the Euclidean geometry to relax hard constraints for decorrelation, we exploit non-Euclidean geometry. Such a choice helps maintain the range space of the matrix and obtain small condition number, which prevents the embedding space degradation. Our method outperforms contrastive-based GCF models on several benchmark datasets and improves the performance for unpopular items.

## 1 Introduction

Collaborative Filtering (CF) has emerged as an effective technique for personalized recommendations . By leveraging embedding and optimizing with a suitable loss function, CF can effectively capture complex patterns in user-item interactions . However, user-item interactions usually neglect the potential of the higher-hop connections. To this end, Graph Neural Networks (GNN)  have been used to model high-hop connections in CF. Graph Collaborative Filtering (GCF) has also been proposed based on CF .

Graph Contrastive Learning (GCL), a powerful tool for unsupervised learning, has recently been applied on a variety of different tasks . Many methods also demonstrate that GCL improves the performance of GCF. The basic idea of GCL in GCF is to enhance training data with various graph augmentations and maximize the alignment between different data views . However, the above methods ignore the disadvantages of GCL (_e.g_., dimensional collapse) and the negative impact on user and item embeddings (_e.g_., the Matthew effect) . Dimensional Collapse (DC)  can be characterized by high similarity of features which are confined to a low-dimensional subspace or are contrated on certain region of latent space( Figure 0(b)). DC limits the representational power of high-dimensional spaces by restricting the diversity of information that can be learned .

Although DC has been identified and tackled in a variety of domains , DC poses issues for GCL in the Graph Collaborative Filtering where it exacerbates the Matthew effect of popularity bias (as explained next).

A significant issue that sets GCL in GCF apart from other domains is the highly skewed data distribution over the graph where most users interact sporadically with a very small set of items within a vast interaction space comprising millions or even billions of items . This results in extremely sparse user-item interactions and a long-tail item distribution following the Power Law. Propagating highly skewed data through the sparse bipartite graph via message passing enhances the Matthew effect of the popularity bias . This means that several popular items may dominate the entire learning process, while other items are rarely recommended. Thus, a collapse mode may occur, where the user embedding collapses around several popular items, as shown in Figure 0(a).

To migrate this issue, previous research argues that the so-called uniformity term included in the auxiliary contrastive loss can solve DC and so the uniformity term was adopted it in CF [45; 46; 38]. Particularly, Yu _et al_.  provide an empirical evidence that the features learned by the typical GCF model are non-uniformly distributed on the sphere, which causes the collapse. Wang _et al_.  have further investigated how optimizing the Bayesian Personalized Ranking (BPR) loss in CF can promote both the alignment and uniformity, and suggested to directly optimize them. However, while optimizing the uniformity of both the user and item embeddings in CF can refine the em bedding space, it does not guarantee the elimination of dimensional collapse. Thus, a fundamental question arises: is there any provable remedy for DC?

To this end, we analyze the connection between dimensional collapse and GCF by demonstrating that the simplified graph convolution operation for collaborative filtering does shrink the singular space of the feature matrix. Although previous methods promote the feature uniformity, it does not prevent the dimensional collapse. Thus, we propose a new objective, Decorrelation-Enhanced GCF, that employs geometry of the LogDet divergence to relax orthogonality constraints and encourage the feature diversity. Our method maintains the range space of the feature matrix with a small condition number (well-conditioned), providing theoretical guarantees against dimensional collapse. Our experimental results demonstrate the effectiveness of the proposed approach in addressing feature collapse. We outperform contrastive-based CF models on several benchmark datasets. Notably, our approach improves the performance for unpopular items.

We summarize our contributions below:

1. We demonstrate that embeddings learned from existing GCF models suffer from dimensional collapse. Our theoretical analysis reveals that the dimensional collapse in GCF arises from propagation, and that the current uniformity-based objective does not guarantee to alleviate it.
2. To mitigate the issue of dimensional collapse, we propose a decorrelation-enhanced objective following the principle of redundancy reduction  and LogDet geometry.
3. Instead of using the Frobenius norm (the Euclidean geometry) to relax the conditional optimization problem, we propose using the LogDet divergence, a Bregman divergence with theoretical guarantees, to preserve the range space of the covariance matrix.

Our experiments show that our proposed method effectively mitigates dimensional collapse and outperforms various contrastive learning-based CF models. Notably, it achieves improved the performance for unpopular items, thus it mitigates the Matthew effect.

## 2 Preliminaries

Collaborative Filtering.Let \(\) and \(\) denote the user and item set, respectively. Given a set of observed user-item interactions \(=\{(u,i) ui\}\), CF methods infer the score \(s(u,i)\) for each unobserved user-item pair indicating how likely the user \(u\) is to interact with the item \(i\). Then, items with the highest scores for each user will be recommended based on the predictions . Most CF methods use an encoder network \(f()\) that maps each user and item into a low-dimensional representation \(f(u),f(i)^{d}\) (where \(d\) is the dimensionality of the latent space). For example, the encoder in matrix factorization models is usually an embedding table, which directly maps each user and item to a latent vector based on their IDs. The encoder in graph-based models uses the neighborhood information. The predicted score is defined as the similarity between the user/item representations, _e.g._, dot product, \(s(u,i)=f(u)^{}f(i)\). Most studies adopt the pairwise BPR  loss to train the model:

\[_{bpr}=-|}_{(u,i)} s(u,i)-s(u,i^{-})\,,\] (1)

where \(i^{-}\) is a randomly sampled negative item that the user has not interacted with. Such a loss maximizes sigmoid scores: target user-item scores become higher than user-negative item scores.

LightGCN.A linear graph neural network backbone, LightGCN , is one of the most successful backbones in Graph Collaborative Filtering. It leverages neighborhood information at different levels of locality, and combines outputs of different layers by the sum aggregation.

To facilitate the analysis, we present the matrix from of the LightGCN. Let \(^{M N}\) be the matrix representation of set \(\) where \(R_{ui}=1\) if \((u,i)\) (else \(R_{ui}=0\)). Let \(M\) and \(N\) be the numbers of users and items, and \(N^{}\!\!=\!M\!+\!N\). We then obtain the adjacency matrix of the user-item graph as:

\[=(&\\ ^{}&)^{N^{} N ^{}}.\] (2)

Let the embedding matrix of input layer be \(^{N^{} d}\), where \(d\) is the embedding size. Then the \(K\)-th layer of LightGCN in the matrix form is defined as:

\[=f(;},K)=_{1}} +_{2}}^{2}++_{K} }^{K}=_{k=1}^{K}_{k}}^ {k},\] (3)

where \(_{1},_{2},,_{K}\) are weights, and \(_{k=1}^{K}_{k}=1\). Moreover, \(}=^{-}^{-}\) is a normalized adjacency matrix and the degree matrix \(\) is an \(N^{} N^{}\) diagonal matrix in which each entry \(D_{ii}\) denotes the number of non-zero entries in the \(i\)-th row vector of the adjacency matrix \(\).

Dimensional Collapse (DC).Often referred to as spectral collapse , dimensional collapse is a prevalent phenomenon in representation learning. It occurs when the embedding space is dominated by a small number of large singular values, while the remaining singular values decay significantly as the training progresses. This phenomenon limits the representation power of high-dimensional spaces by restricting the diversity of information that can be learned.

## 3 Popularity Bias from a Perspective of Dimensional Collapse

### Popularity Bias _vs._ Dimensional Collapse in Graph Collaborative Filtering

_Dimensionality collapse_ and _Popularity Bias_ in CF are two sides of the same coin. The prevalence of popularity bias in GCF can be attributed to the significant imbalance within the data distribution across the recommendation bipartite graph . This distribution often features numerous users engaging with a small subset of items, within a much larger interaction space encompassing millions or billions of items. Consequently, during the process of message passing within GCF, a few popular items can disproportionately influence learning, leaving out other items. This leads to the dimensional collapse in the embedding space where most user embeddings are around very few popular item embeddings.

We illustrate this by showing the 2D latent embedding space of the user and item in Figure 2:

1. Figure 2 (top) shows that both user and item embedding are clustered in a few regions, the gaps in the unpopular item embedding cause _effective rank_ drop. We refer to this effect as the _dimensional collapse_. As the unpopular items are clustered in just one cluster, and the corresponding user cluster is non-dense so unpopular items do not get recommended often (_popularity bias_).

2. If only the embeddings were spread uniformly on the surface of the hypersphere, it would align with dense user clusters which would reduce the popularity bias. Figure 2 (bottom) shows that thanks to more uniformly spread unpopular items, they form now 3 clusters, and 2 of them align well with dense user clusters, which means the popularity bias has been reduced.

### Dimensional Collapse in Graph Collaborative Filtering

The previous section reveals that the occurrence of dimensional collapse is closely related to the presence of popularity bias, as depicted in Figure 2. Below, we show how the typical backbone of GCL in GCF (_i.e._, LightGCN) potentially causes the DC in the learned embedding space, and why optimizing uniformity  (the best current solution) cannot guarantee to prevent DC.

**Lemma 1** (Equivalent reformulation of LightGCN.): _The propagation process of LightGCN in Equation (3) can be viewed as a result of optimization of the following objective:_

\[^{*}=*{arg\,min}_{}\ \|-}\|_{F}^{2}+(^{} ),\] (4)

_where \(=-}\) is the normalized symmetric positive semi-definite graph Laplacian matrix. \(\) Proof of Lemma 4 is in Appendix A.1._

Lemma 1 shows that the propagation process of LightGCN can be derived from the optimization formulation which uses the Graph Laplacian regularization. This implies that the output embedding \(\) is smooth due to the Graph Laplacian regularization in Equation (4). Similar derivations for the graph neural network with learnable weights were shown by Ma _et al_.  and Zhu _et al_. .

To understand the dynamics of the embedding space, we denote the state of the embedding matrix and the output of the LightGCN at time \(t\) as \((t)\) and the initial state \((0)\) is randomly initialized. Let \(\{_{n}^{}\}_{n=1}^{N^{}}\) denote eigenvalues of \(\) (descending order)

**Lemma 2** (Shrinking singular space of feature matrix.): _Consider minimizing the objective in Lemma 1 with the Laplacian regularization term \((^{})\). The relative value of the ratio

Figure 2: Mitigating Popualrity Bias. (_top_) We applied 2D PCA and the \(_{2}\) normalization on embeddings of LightGCN (Yelp2018 dataset). Notice that when only a non-dense user cluster aligns with the unpopular item cluster, these items will not be recommended frequently (✗). (_bottom_) With our LogDet-based loss, the unpopular-item cluster is spread into three clusters across surface of hypersphere. Newly formed clusters align well with dense user clusters (✓) so the unpopular items will be recommended more often. The dimensional Collapse manifests itself with gaps on the surface of hypersphere. Each plot was renormalized by its maximum density (darkest color). The 3D ball effect is to remind reader the embeddings exist on the surface hypersphere but in our simulation we use in fact a 2D sphere.

[MISSING_PAGE_FAIL:5]

be too strong. Equation (8) may be solved by applying Lagrangian multipliers or the Stiefel manifold (GeoTorch ) or by converting hard constraints into soft constraints:

\[_{}\,_{align}()\!+\!\!\!_{ soft}_{soft}\!=\!_{i}f_{ }(u)f_{}(u)^{}\!-\!_{F}^{2}\!\!+\! _{i}f_{}(i)f_{}(i)^{}\!-\!_{F }^{2},\] (9)

However, relaxation \(_{soft}\) in Equation (9) suffers from the same problem as \(_{uni}\) in Equation (6). Specifically, these both regularization terms yield a finite penalty for any singular values \(_{i}=0\) leading to dimensional collapse if other loss terms yield a higher reward than the penalty is. Thus, we explore the application of Bregman divergence. Our study indicates that the proposed approach preserves the range space of the covariance matrix, promoting a full-rank embedding matrix.

Decorrelation by the LogDet Divergence.Figure 3 and Table 1 summarizes properties of different decorrelation terms. While \(_{uni}\) and \(_{soft}\) encourage soft decorrelation (approximate isotropy), they may fail to impose full rank on the embedding matrix. Stiefel-based optimization enjoys hard constraints but full decorrelation (exact isotropy) may be too restrictive. In contrast, LogDet divergence based \(_{logdet}\) encourages isotropy in the soft manner and imposes full rank on embedding matrix as shown below. Firstly, we review the Bregman divergence and introduce our LogDet based decorrelation penalty.

**Definition 1** (**Bregman Divergence on Matrices.**): _Let \(,^{d}\) be two symmetric matrices. Let \(\!:\!^{n}\) be a strictly convex and differentiable matrix function. The Bregman matrix divergence is defined as:_

\[D_{}(,)=()-()- {tr}(())^{}(-)\,.\] (10)

The Bregman divergence is a measure of the nearness between matrices \(\) and \(\). It is computed by using the first-order Taylor approximation of a convex generating function \(()\). The Bregman divergence is always non-negative, and it becomes zero only when the two matrices are equal.

As \(_{}\!=\!_{u}f_{}(u)f_{ }(u)^{}\!^{d}_{+}\) and \(_{}\!=\!_{i}f_{}(i)f_{ }(i)^{}\!^{d}_{+}\) are symmetric, and at least positive semi-definite (PSD) matrices but we desire them to be strictly positive definite (PD) matrices (\(^{d}_{++}\)) to prevent the dimensional collapse, we firstly generalize \(_{soft}\) in Equation (9) to its Bregman divergence based variant:

\[_{breg}=D_{}(_{},)+D_{ }(_{},).\] (11)

Different \(()\) lead to different measures of nearness. For example, \(_{F}()\!=\!\|\|_{F}^{2}\) yields the Frobenius-based \(D_{_{F}}(_{},)\!+\!D_{_{F}}( _{},)\!=\!\|_{} \!-\!\|_{F}^{2}\) re-\(\|_{}\!-\!\|_{F}^{2}\) penalty \(_{soft}\) in Equation (9).

Let \(=^{}\) be an eigenvalue decomposition of \(\), and \(\!=\!([_{1},,_{d}])\). Choosing \(_{ld}()\!=\!-\!=\!-_{i}_{i}\) leads to the LogDet divergence \(D_{_{ld}}(,)=^{-1}-^{-1}-d\) where \(,^{d}_{++}\). Then generalizing \(_{soft}\) in Equation (9) to its LogDet divergence variant by computing \(D_{_{ld}}(_{},)+D_{_{ld}}( _{},)\) and removing \(-2d\) constant yields:

\[_{logdet}=_{}+ _{}-_{ }_{}\!\!\!2d.\] (12)

Our LogDet decorrelation-enhanced GCF objective becomes:

\[=_{(i,u)}\,\|f_{}(u) -f_{}(i)\|_{2}^{2}+\!\! {tr}_{}+_{}- _{}_{} \,.\] (13)

In contrast to the Frobenius-based \(_{soft}\), the LogDet-based penalty \(_{logdet}\) maintains the range space of the covariance matrices and upper bounds the condition number of the matrices. This ensures the user/item embedding matrices are of full rank which prevents the dimensional collapse.

Figure 3: The spectrum (eigenvalue distribution) of the covariance matrix under various loss penalties. Various penalties realize some composition of {isotropic, non-isotropic}\(\){full rank, rank-deficient}, _e.g._, our LogDet formulation promotes the (isotropic, full rank) case.

Theoretical Analysis.

To use the LogDet divergence based penalty \(_{logdet}\) in GCF, we discuss important to our work theoretical properties of the LogDet divergence and \(_{logdet}\) for rank-deficient matrices.

LogDet Divergence and \(_{logdet}\) for Rank-deficient Matrices.LogDet divergence \(D_{_{ld}}(,)\) is finite iff \(()=()\), which implies that \(_{logdet}=D_{_{ld}}(_{},)+D _{_{ld}}(_{},)=\) if \((_{})<()=d\ \ (_{ })<()=d\).

Let \(_{},_{}_{++}^ {d}\). Note that \(_{soft}=D_{_{F}}(_{},)+D_{ _{F}}(_{},)\) is finite if both \(\|_{}\|_{2}\) and \(\|_{}\|_{2}\) are finite. There exist cases where some infinitesimal change \(_{}\) and/or \(_{}\) will make \(_{}+_{}\) and/or \(_{}+_{}\) positive semi-definite (PSD). Define the gain of the Bregman divergence based decorrelation penalty as \(g_{}(_{reg})=D_{}(_{},) +D_{}(_{},)-[D_{}(_ {}+_{},)+D_{}( _{}+_{},)]\). Notice, in case of change from PD to SPD matrix, gain \(g_{}(_{reg})<0\). Let the gain of the alignment loss be \(g(_{align})\). Then one may easily construct examples where \(g(_{align})-g_{_{F}}(_{soft})>0\). That is to say, the net loss \(-g_{_{F}}(_{soft})\) in case of covariance degradation to SPD is lesser than the net gain \(g(_{align})\) due to increased user-item alignment. In contrast, from Corollary 1, \(g(_{align})-g_{_{ld}}(_{logdet})=-\) in case of covariance degradation to SPD, which prevents the dimensional collapse.

Proof of Corollary 2 is in Appendix A.3.

**Corollary 3**.: _Expression \(-(_{}_{})\) from Equation (12) might fail to yield \(\) if \(_{}_{}\) is indefinite. However, covariance matrices are at least SPD by definition, and so is their matrix product._

Corollaries 1, 2 and 3 demonstrate that \(_{logdet}\) naturally maintains the full-rank constraints on covariance matrices of user and item embeddings.

Minimizing \(D_{_{ld}}(,)\) Reduces the Condition Number of \(\).Feature decorrelation is closely related to the condition number. If the spectrum of a matrix is dominated by the leading eigenvalue, the matrix is ill-conditioned and has a large condition number. Let \(\{_{i}\}_{i=0}^{d}\) be the eigenvalues of the matrix \(\). The condition number is defined as \(()=}{_{}}\). The ratio between the largest eigenvalue and the smallest eigenvalue quantifies the flatness (isotropy) of the spectrum. For example, \(()=1\) implies a fully balanced spectrum with \(_{i}=_{j}\) for all \(i j\) (_e.g._, spectrum balancing ). Below we show that our proposed method minimizes the upper bound of \(()\).

**Lemma 3**.: _The condition number of \(\), \(()=}{_{}}\), is upper bounded by \(D_{_{ld}}(,)\) as: \(() 4(D_{_{ld}}(,))\)._

Proof of Lemma 3 is in Appendix A.4.

Lemma 3 implies that the sum of condition numbers of \(_{}\) and \(_{}\) is upper bounded by the LogDet divergence based penalty \(_{logdet}\). Minimizing that penalty implies the flattening of spectrum implies decorrelation/redundancy reduction of embedding.

## 6 Experiments

Datasets.To evaluate the effectiveness of our GCF with the LogDet divergence based penalty \(_{logdet}\), denoted as GCF\({}_{logdet}\), we conduct experiments on three large-scale public datasets: Yelp2018 , Amazon-kindle , and Alibaba-iFashion . See statistics of these datasets in Appendix A.6. The datasets were split into the training, validation, and test set, with the ratio of \(7:1:2\). As per the methodology suggested by , we first identified the best hyperparameters on the validation set and then merged the training and validation sets to train the model. Finally, the test set was used to evaluate the model using the relevancy-based metric Recall@\(20\) and the ranking-aware metric NDCG@\(20\).

Baselines.Apart from the typical GCF models, _i.e._, **NGCF** and **LightGCN**, we also compare GCF\({}_{logdet}\) with recent data CL-based recommendation models. **SGL** and DNN+SSL  adopt CL as an auxiliary task and conduct feature masking for contrastive learning (CL). **BUIR** has a two-branch architecture with a target network and an online network, utilizing only positive examples for self-supervised recommendation. **MixGCL** introduces the hop mixing technique for synthesizing hard negatives for GCF through embedding interpolation. **NCL** is a recent contrastive model that designs a prototypical contrastive objective to capture the correlations between a user/item and its context. **SimGCL** is a CL-based model for recommendation that does not rely on graph augmentation. **DirectAU** proposes optimizing the alignment and uniformity of both user and item directly for recommendation.

Implementation.We referred to the best hyperparameter settings reported in the original papers of the baselines and then fine-tuned them with the grid search. Detailed settings are in Appendix A.5.

### Main Results

Decorrelation _vs._Uniformity.Within this section, we compare the performance of our proposed method, GCF\({}_{logdet}\), with contrastive-based methods (_i.e_., SGL, DirectAU) that optimize uniformity to prevent the dimensional collapse in CF. Furthermore, we demonstrate the effectiveness of our method by varying the number of encoder layers. Table 3 shows the performance of different baseline CF methods and our GCF\({}_{logdet}\). In the vast majority of cases, contrastive-based methods (_e.g_., SGL, DirectAU, SimGCL) significantly outperform LightGCN. The largest improvements are observed on Alibaba-iFashion, where the performance of GCF\({}_{logdet}\) surpasses that of LightGCN under 1 and 3 layers settings. This observation supports our conclusion that LightGCN is susceptible to the dimensional collapse, leading to poor results. Additionally, we observe that DirectAU outperforms SGL, indicating that graph augmentation is not the primary factor for enhancing the performance of the CL-based recommendation model. Instead, our findings demonstrate the importance of designing appropriate loss functions over sophisticated augmentation. Finally, GCF\({}_{logdet}\) achieves the best results on the all three datasets. Specifically, GCF\({}_{logdet}\) outperforms DirectAU and SGL, highlighting that decorrelation is more effective than optimizing uniformity for preventing dimensional collapse.

    &  &  &  \\   & **Recall@20** & **NDCG@20** & **Recall@20** & **NDCG@20** & **Recall@20** & **NDCG@20** \\   & LightGCN & 0.0590 & 0.0484 & 0.1871 & 0.1186 & 0.0845 & 0.0390 \\  & SGL-ED & 0.0637 & 0.0526 & 0.1936 & 0.1231 & 0.0932 & 0.0442 \\  & SimGCL & 0.0689 & 0.0572 & 0.2087 & **0.1361** & 0.1036 & 0.0505 \\  & DirectAU & 0.0674 & 0.0522 & 0.1971 & 0.1239 & 0.0969 & 0.0426 \\   & GCF\({}_{logdet}\) & **0.0694** & **0.0577** & **0.2103** & 0.1321 & **0.1060** & **0.0510** \\   & LightGCN & 0.0622 & 0.0504 & 0.2033 & 0.1284 & 0.1053 & 0.0505 \\  & SGL-ED & 0.0668 & 0.0549 & 0.2084 & 0.1341 & 0.1062 & 0.0514 \\  & SimGCL & 0.0719 & 0.0601 & 0.2071 & 0.1341 & 0.1119 & 0.0548 \\  & DirectAU & 0.0702 & 0.0584 & 0.2034 & 0.1352 & 0.1043 & 0.0549 \\   & GCF\({}_{logdet}\) & **0.0720** & **0.0607** & **0.2143** & **0.1381** & **0.1180** & **0.0557** \\   & LightGCN & 0.0639 & 0.0525 & 0.2057 & 0.1315 & 0.0955 & 0.0461 \\  & SGL-ED & 0.0675 & 0.0555 & 0.2090 & 0.1352 & 0.1093 & 0.0531 \\  & SimGCL & 0.0721 & 0.0601 & 0.2104 & 0.1374 & 0.1151 & 0.0567 \\  & DirectAU & 0.0713 & 0.0602 & 0.2047 & 0.1385 & 0.1136 & 0.0586 \\   & GCF\({}_{logdet}\) & **0.0725** & **0.0612** & **0.2151** & **0.1418** & **0.1210** & **0.0601** \\   &  & & & & & & \\ 

Table 2: Performance comparison for different contrastive GCF model that optimize uniformity.

    &  &  &  \\   & **Recall@20** & **NDCG@20** & **Recall@20** & **NDCG@20** & **Recall@20** & **NDCG@20** \\  NGCF & 0.0579 & 0.0477 & 0.1954 & 0.1263 & 0.1043 & 0.0486 \\ LightGCN & 0.0639 & 0.0525 & 0.2057 & 0.1315 & 0.1053 & 0.0505 \\ NCL & 0.0670 & 0.0562 & 0.2090 & 0.1348 & 0.1088 & 0.0528 \\ BUIR & 0.0487 & 0.0404 & 0.0922 & 0.0528 & 0.0830 & 0.0384 \\ DNN+SSL & 0.0483 & 0.0382 & 0.1520 & 0.0989 & 0.0818 & 0.0375 \\ MixGCF & 0.0713 & 0.0589 & 0.2098 & 0.1355 & 0.1085 & 0.0520 \\  SGL-ED & 0.0675 & 0.0555 & 0.2090 & 0.1352 & 0.1093 & 0.0531 \\ SimGCL & 0.0721 & 0.0601 & 0.2104 & 0.1374 & 0.1151 & 0.0567 \\ DirectAU & 0.0713 & 0.0602 & 0.2047 & 0.1385 & 0.1136 & 0.0586 \\ 
**GCF\({}_{logdet}\)** & **0.0725** & **0.0612** & **0.2151** & **0.1418** & **0.1210** & **0.0601** \\   

Table 3: Performance comparison with other models.

Performance Comparison with the State of the Art.To further demonstrate the exceptional performance of GCF\({}_{logdet}\), we conduct a comparative analysis with several recently proposed recommendation models that are based on augmentation or contrastive learning techniques. As illustrated in Table 3, GCF\({}_{logdet}\) outperforms the other models by a significant margin, achieving the best performances, respectively. Additionally, NCL and MixGCF, which utilize LightGCN as their backbone, exhibit competitive performance. In contrast, DNN+SSL and BUIR fall short of expectations and are not comparable to LightGCN. We attribute their suboptimal performance to the fact that DNNs have been proven to be effective only when abundant user/item features are available. In our datasets, however, such features are unavailable, and self-supervision signals are created by masking item embeddings, which makes it difficult for these models to perform well in this context. We also report performance on Recall@K and NDCG@K with \(K=5,10\) in Table 9.

### Quantitative Analysis

Measuring the Dimensional Collapse.The evidence of dimensional collapse was identified in contrastive learning by observing the spectrum of representations . The rank of the matrix corresponds to the number of dimensions retained by the transformation (_i.e._, the dimension of its range) but reveals nothing about the shape of the spectrum. The effective rank  (see the definition in the Appendix A.7) quantifies how balanced the spectrum is by means of the spectral entropy. Thus, to show the dimensional collapse tendency when training the model, instead of showing the distribution of eigenvalues, we use the effective rank. Figure 4 shows that the effective rank increases as the training progresses, indicating an increasingly balanced eigenvalue distribution. GCF\({}_{logdet}\) consistently achieves higher effective rank compared to other methods. This finding not only verifies the existence of the dimensional collapse in CF but also indicates the effectiveness of our GCF\({}_{logdet}\). Figure 3(d) further shows that the condition number decreases as more batches are processed, thus balancing the spectrum of embedding matrices.

Overcoming the Popularity Bias.Below we investigate whether our GCF\({}_{logdet}\) can reduce the popularity bias by promoting more uniform representations and alleviating the problem of dimensional collapse. We partition the test set into three subsets based on item popularity. Specifically, we label 80% of the items with the fewest clicks/purchases as "Unpopular," 5% of the most clicked/purchased items as "Popular," and the remaining items as "Normal." We then conduct experiments to measure Recall@20 contributed by each group, with the overall Recall@20 value being the sum of the values

    &  &  &  \\ 
**Encoder** & **Loss** & **Recall@20** & **NDCG@20** & **Recall@20** & **NDCG@20** & **Recall@20** & **NDCG@20** \\   & \(_{uni}\) & 0.0713 & 0.0602 & 0.2107 & 0.1385 & 0.1136 & 0.0586 \\  & \(_{soft}\) & 0.0709 & 0.0592 & 0.1982 & 0.1351 & 0.1112 & 0.0580 \\  & Stiefel & 0.0703 & 0.0582 & 0.1882 & 0.1352 & 0.1071 & 0.0528 \\    & \(_{logdet}\) & **0.0725** & **0.0612** & **0.2151** & **0.1418** & **0.1210** & **0.0601** \\    & \(_{uni}\) & 0.0702 & 0.0589 & 0.1960 & 0.1325 & 0.1085 & 0.0580 \\   & \(_{soft}\) & 0.0699 & 0.0582 & 0.2060 & 0.0989 & 0.1018 & 0.0595 \\   & Stiefel & 0.0691 & 0.0579 & 0.1852 & 0.1191 & 0.0958 & 0.0523 \\    MLP \\  } & \(_{logdet}\) & **0.0721** & **0.0601** & **0.2154** & **0.1414** & **0.1151** & **0.0607** \\   

Table 4: Performance comparison with other models.

Figure 4: The effective rank (higher is better) w.r.t. epoch number and condition number in Yelp2018.

from all three groups. The results are presented in Figure 5 and Table 5, which show that our method significantly improves the ability to recommend unpopular items. In contrast, LightGCN tends to recommend popular items and achieves the highest recall value on Normal and Popular items.

**Ablations on Different Decorrelation Penalties.** Table 4 compares the performance on LightGCN and MLP equipped with different decorrelation penalties. \(_{logdet}\) appears to consistently outperform other penalties, thus verifying its ability to deal with the dimensional collapse and balancing the spectrum of embeddings. Appendix A.8 compares the KL Matrix Divergence and the Riemannian metric in terms of speed. Appendix A.9 compares different distance types.

## 7 Related Works

**Graph Contrastive Learning** applies CL to the graph domain. By adapting DeepInfoMax  to graph representation learning, DGI  learns embedding by maximizing the mutual information to discriminate between nodes of original and corrupted graphs. REFINE  uses a simple negative sampling term inspired by skip-gram models. COLES  and GLEN  link the GCL to the form of Laplacian Eigenmap with negative sampling, extended also to image domain by EASE . GRACE /GraphCL  create views via graph augmentation for node/graph-level task. COSTA  and SFA  create graph views via feature augmentations.

**Graph Collaborative Filtering.** Graph Neural Networks (GNNs) have proven to be powerful architectures for modeling recommendation data, replacing MLP-based models and improving the performance of neural recommender systems [20; 15]. GNNs are a foundation for several state-of-the-art recommendation models. The most common GNN variant is GCN, which has driven the development of graph neural recommendation models, such as GCMC, NGCF, and LightGCN [3; 41; 15]. These GCN-based models refine embeddings and perform graph reasoning by gathering neighborhood information in the user-item graph. Among such models, LightGCN stands out for its effectiveness and simplicity by eliminating transformation matrices and non-linear activation functions. The success of LightGCN has inspired subsequent CL-based recommendation models, such as SGL and SimGCL [43; 46].

**Graph Contrastive Learning for Recommendation.** Inspired by the success of CL in other fields [11; 5; 35], CL has been combined with recommendation systems. Yao _et al_.  proposed a feature dropout based two-tower architecture for large-scale item recommendation. NCL  designed a prototypical contrastive objective to capture the correlations between a user/item and its context. SimGCL  adds noise to the embedding space to perform feature augmentation to enhance the performance of recommendations. DirectAU  proposes optimizing the alignment and uniformity of both user and item during training. The most widely used model, SGL , applies edge/node dropout to augment the graph data. Although these methods have demonstrated their effectiveness, they pay little attention to why CL can enhance recommendations.

## 8 Conclusions

We have investigated the popularity bias in Graph Collaborative Filtering, and showed it results from the dimensional collapse of the embedding space. We have demonstrated that the commonly used backbone in GCF, LightGCN, is prone to pose the dimensional collapse. To this end, we have proposed the LogDet divergence based decorrelation penalty for the GCF. We have showed that GCF\({}_{logdet}\) promotes full-rank embedding matrices and yields small condition number with a theoretical guarantee. For these reasons, GCF\({}_{logdet}\) has outperformed other contrastive-based GCF models on several benchmark datasets and improved the performance for unpopular items.

Figure 5: LogDet _vs_. other baselines. LightGCN is the backbone used in all methods.