# Real-world Image Dehazing with Coherence-based Pseudo Labeling and Cooperative Unfolding Network

Chengyu Fang\({}^{1,}\)

Equal Contribution, \(\) Corresponding Author, \(\) Email Address

Chunming He\({}^{1,3,}\)

Equal Contribution, \(\) Corresponding Author, \(\) Email Address

Fengyang Xiao\({}^{1,2}\)

Equal Contribution, \(\) Corresponding Author, \(\) Email Address

Yulun Zhang\({}^{4,}\)

Equal Contribution, \(\) Corresponding Author, \(\) Email Address

Longxiang Tang\({}^{1}\)

Equal Contribution, \(\) Corresponding Author, \(\) Email Address

Yuelin Zhang\({}^{5}\)

Equal Contribution, \(\) Email Address

Kai Li\({}^{6}\)

Equal Contribution, \(\) Corresponding Author, \(\) Email Address

Xiu Li\({}^{1,}\)

Equal Contribution, \(\) Corresponding Author, \(\) Email Address

\({}^{1}\)Shenzhen International Graduate School, Tsinghua University, \({}^{2}\)Sun Yat-sen University,

\({}^{3}\)Duke University, \({}^{4}\)Shanghai Jiao Tong University,

\({}^{5}\)The Chinese University of Hong Kong, \({}^{6}\)Meta Reality Labs

\(\) _chengyufang.thu@gmail.com_

###### Abstract

Real-world Image Dehazing (RID) aims to alleviate haze-induced degradation in real-world settings. This task remains challenging due to the complexities in accurately modeling real haze distributions and the scarcity of paired real-world data. To address these challenges, we first introduce a cooperative unfolding network that jointly models atmospheric scattering and image scenes, effectively integrating physical knowledge into deep networks to restore haze-contaminated details. Additionally, we propose the first RID-oriented iterative mean-teacher framework, termed the Coherence-based Label Generator, to generate high-quality pseudo labels for network training. Specifically, we provide an optimal label pool to store the best pseudo-labels during network training, leveraging both global and local coherence to select high-quality candidates and assign weights to prioritize haze-free regions. We verify the effectiveness of our method, with experiments demonstrating that it achieves state-of-the-art performance on RID tasks. Code will be available at https://github.com/cnyvfang/CORUN-Colabator.

## 1 Introduction

Real-world image dehazing (RID) is a challenging task that aims to restore images affected by complex haze in real-world scenarios. The goal is to generate visual-appealing results while enhancing the performance of downstream tasks . The atmospheric scattering model (ASM), providing a physical framework for real-world dehazing, is formulated as follows:

\[P(x)=J(x)t(x)+A(1-t(x)),\] (1)

where \(P(x)\) and \(J(x)\) are the hazy image and the haze-free counterpart. \(A\) signifies the global atmospheric light. \(t(x)\) characterizes the transmission map reflecting varying degrees of haze visibility across different regions.

Figure 1: Results of cutting-edge methods. Our CORUN better restores hazy-contaminated details. Furthermore, techniques optimized by our Colabator framework, indicated by “\(+\)” suffix, exhibit strong generalization in haze removal and color correction.

Conventional methods [3; 4] are limited by fixed feature extractors, which struggle to handle the complexities of real haze. Although existing deep learning-based methods [5; 6; 7; 8; 9] demonstrate improved performance, they face two significant challenges: (1) These methods do not accurately model the complex distribution of haze, leading to color distortion, as illustrated in fig. 1 DGUN . (2) Real-world settings lack sufficient paired data for network training while optimizing the network with synthesized data brings a domain gap, limiting the generalizability of the models.

To overcome the first challenge, PDN  first introduces unfolding network [12; 13] to the RID field. In specific, PDN unfolds the iterative optimization steps of an ASM-based solution into a deep network for end-to-end training, incorporating physical information into the deep network. However, PDN does not effectively leverage the complementary information between the dehazed image and the transmission map, bringing overfitting problems and resulting in detail blurring (see fig. 1).

In this paper, we introduce the COopeRative Unfolding Network (CORUN), also derived from the ASM-based function, to address PDN's limitations and better model real hazy distribution. CORUN cooperatively models the atmospheric scattering and image scene by incorporating Transmission and Scene Gradient Descent Modules at each stage, corresponding to each iteration of the traditional optimization algorithm. To prevent overfitting, we introduce a global coherence loss, which constrains the entire pipeline to adhere to physical laws while alleviating constraints on the intermediate layers. These design choices collectively ensure that CORUN effectively integrates physical information into deep networks, thereby excelling in restoring haze-contaminated details, as depicted in fig. 1.

To enhance generalizability in real-world scenarios, we introduce the first RID-oriented iterative mean-teacher framework, named Coherence-based label generator (Colabator), designed to generate high-quality dehazed images as pseudo labels for training dehazing methods. Specifically, Colabator employs a teacher network, a dehazing network pretrained on synthesized datasets, to generate dehazed images on label-free real-world datasets. These restored images are stored in a dynamically updated label pool as pseudo labels for training the student network, which shares the same structure as the teacher network but with distinct weights. During network training, the teacher network generates multiple pseudo labels for a single real-world hazy image. We propose selecting the best labels to store in the label pool based on visual fidelity and dehazing performance.

To achieve this, we design a compound image quality assessment strategy tailored to the dehazing task, evaluating the global coherence of the dehazed images and selecting the most visually appealing ones without distortions for inclusion in the label pool. Additionally, we propose a patch-level certainty map to encourage the network to focus on well-restored regions of the dehazed pseudo labels, effectively constraining the local coherence between the outputs of the student model and the teacher model. As shown in fig. 1, Colabator, generating high-quality pseudo labels for network training, enhances the student dehazing network's capacity for haze removal and color correction.

Our contributions are summarized as follows:

(1) We propose a novel dehazing method, CORUN, to cooperatively model the atmospheric scattering and image scene, effectively integrating physical information into deep networks.

(2) We propose the first iterative mean-teacher framework, Colabator, to generate high-quality pseudo labels for network training, enhancing the network's generalization in haze removal.

(3) We evaluate our CORUN with the Colabator framework on real-world dehazing tasks. Abundant experiments demonstrate that our method achieves state-of-the-art performance.

## 2 Related Works

### Real-world Image Dehazing

The dissonance between synthetic and real haze distributions often hinders existing Learning-based dehazing methods [14; 15; 16; 17; 18] from effectively dehazing real-world images. Consequently, there's a growing emphasis on tackling challenges specific to real-world dehazing [19; 20; 21; 22; 23].

Given the characteristics of real haze, RIDCP  and Wang _et al._ proposed novel haze synthesis pipelines. However, relying solely on synthetic data limits models' robustness in real-world dehazing scenarios. Recognizing the distributional disparities between synthetic and real haze, methods like CDD-GAN , D4 , Shao _et al._, and Li _et al._ have utilized CycleGAN  for dehazing. Despite this, the challenges inherent in GAN  training often result in artifacts. Some approaches combine synthetic and real-world data, applying unsupervised loss to supervise real-world dehazing learning . However, these losses lack sufficient precision, leading to suboptimal results. Other methods leverage pseudo-labels [31; 32], but the erroneous pseudo-labels cause degrade quality.

To address these challenges, we introduce a coherence-based pseudo labeling method termed Colabator. Our approach selectively identifies and prioritizes high-quality regions within pseudo labels, leading to enhanced robustness and superior generation quality for real-world image dehazing.

### Deep Unfolding Image Restoration

Deep Unfolding Networks (DUNs) integrate model-based and learning-based approaches [33; 34] and thus offer enhanced interpretability and flexibility compared to traditional learning-based methods. Increasingly, DUNs are being utilized for various image tasks, including image super-resolution , compressive sensing , and hyperspectral image reconstruction . DGUN  proposes a general form of proximal gradient descent to learn degradation. However, it fails to decouple prior knowledge, relying solely on single-path DUN to model degradation and construct mappings, posing challenges in comprehending complex degradation. Yang and Sun first introduced DUNs to the image dehazing field and proposed PDN . However, PDN does not exploit the complementary information between the dehazed image and the transmission map, resulting in detail blurring. Our CORUN optimizes the atmospheric scattering model and the image scene feature through dual proximal gradient descent, thus preventing overfitting and facilitating detail restoration.

## 3 Methodology

### Cooperative Unfolding Network

We propose the Cooperative Unfolding Network (CORUN), the first Deep Unfolding Network (DUN) method utilizing Proximal Gradient Descent (PGD) to optimize image dehazing performance by leveraging the Atmospheric Scattering Model (ASM) and neural image reconstruction in a cooperative manner. Each stage of CORUN includes Transmission and Scene Gradient Descent Modules (T&SGDM) paired with Cooperative Proximal Mapping Modules (T&S-CPMM). These modules work together to model atmospheric scattering and image scene features, enabling the adaptive capture and restoration of global composite features within the scene.

According to eq. (1), given a hazy image \(^{H W 3}\), we initialize a transmission map \(^{H W 1}\). In gradient descent, we simplify the atmospheric light \(A^{3}\) and implicitly estimate it in the CORUN pipeline to focus on the detailed characterization of the scene and the relationship between volumetric haze and scene. Hence, eq. (1) can be rewrite as

\[=+-,\] (2)

Figure 2: The architecture of the proposed CORUN with the details at \(k^{th}\) stage.

Where \(\) means the clear image without hazy, \(\) is the all-one matrix. Based on eq.2, we can define our cooperative dehazing energy function like

\[L(,)=\|-+ -\|_{2}^{2}+()+(),\] (3)

where \(()\) and \(()\) are regularization terms on \(\) and \(\). We introduce two auxiliary variables \(}\) and \(}\) to approximate \(\) and \(\), respectively. This leads to the following minimization problem:

\[\{},}\}=*{arg\,min}_{, }L(,).\] (4)

**Transmission optimization.** Give the estimated coarse transmission map \(\) and dehazed image \(}_{k-1}\) at iteration \(k-1\), the variable \(\) can be updated as:

\[_{k}=*{arg\,min}_{}\|-}_{k-1}+-\|_{2}^{2}+ ().\] (5)

We construct the proximal mapping between \(}\) and \(\) by a encoder-decoder like neural network which we named T-CPMM and denoted as \(*{prox}_{}\):

\[_{k}=*{prox}_{}(_{k-1},}_ {k}),\] (6)

the auxiliary variables \(}\), which we calculate by our proposed TGDM can be formulated as:

\[}_{k}=_{c\{R,G,B\}}(-}_{k-1}^{ c}+}{(-}_{k-1})^{}})^{-1}( -^{c}+_{k-1}}{(- {}_{k-1}^{c})^{}}).\] (7)

The variable \(_{k}\) is a learnable parameter, we enable CORUN to learn this parameter at each stage during the end-to-end learning process, allowing the network to adaptively control the updates in iteration.

**Scene optimization.** Give \(}_{k}\) and \(\), the variable \(\) can be updated as:

\[_{k}=*{arg\,min}_{}\|- }_{k}+}_{k}-\|_{2}^{2} +().\] (8)

Same as the proximal mapping process in the transmission optimization, S-CPMM has the similar structure as T-CPMM but different inputs, we denote S-CPMM as \(*{prox}_{}\):

\[_{k}=*{prox}_{}(}_{k},}_{k}),\] (9)

where the \(}_{k}\) we process by our SGDM can be presented as:

\[}_{k}=(}_{k}^{}}_{k}+_{k} )^{-1}(}_{k}^{}+}_ {k}^{}}_{k}-}_{k}^{}+_{k}_ {k-1}),\] (10)

as the \(_{k}\) in transmission optimization, \(_{k}\) is also a learnable parameter to bring more generalization capabilities to the network.

**Details about CPMM.** T-CPMM and S-CPMM share the same structure for improved mapping quality. Each CPMM block uses a 4-channel convolution to embed \(\) and \(\) into a 30-dimensional feature map. The distinction between T-CPMM and S-CPMM lies in their outputs: T-CPMM produces a 1-channel result to aid TGDM in predicting a scene-compliant transmission map, whereas S-CPMM generates a 3-channel RGB image. This enables S-CPMM to learn additional scene feature information, such as atmospheric light and blur, assisting SGDM in generating higher-quality dehazed results with more details. For more efficient computation, each CPMM comprises only 3 layers with \(\) blocks, doubling the dimensions with increasing depth.

### Coherence-based Pseudo Labeling by Colabator

We generate and select pseudo labels using our proposed plug-and-play coherence-based label generator, Colabator. Colabator consists of a teacher network with weights \(_{tea}\) shared with the student network \(_{stu}\) via exponential moving average (EMA). It employs a tailored mean-teacher strategy with a trust weighting process and an optimal label pool to generate high-quality pseudo labels, addressing the scarcity of real-world data. Figure3 illustrates the pipeline of our Colabator.

[MISSING_PAGE_FAIL:5]

### Semi-supervised Real-world Image Dehazing

To achieve success in real-world dehazing, we designed several loss functions for our CORUN and Colabator to constrain their learning process. We introduce a reconstruction loss using the \(L_{1}\) norm \(\|\|_{1}\). To enhance visual perception, we employ contrastive and common perceptual regularization to ensure the consistency of the reconstruction results with the ground truth in terms of features at different levels. The perceptual loss is defined as follows:

\[L_{Rec}^{common}(_{HQ},_{GT})=\|_{GT},_{HQ}\|_{1}+_{c}_{i=1}^{n}_{i}\|_{i}(_{GT}), _{i}(_{HQ})\|_{1}\] (15)

\[L_{Rec}^{contra}(_{LQ},_{HQ},_{GT})=\|_{GT},_{HQ}\|_{1}+_{c}_{i=1}^{n}_{i}(_{GT}),_{i}(_{HQ})\|_{1}}{\|_{i}(_{LQ}),_{i}(_{HQ})\|_{1}},\] (16)

where \(_{HQ}\) is the dehazed result, \(_{i}()\) means the \(i_{th}\) hidden layer of pre-trained VGG-19 , \(_{i}\) is the weight coefficient. Besides, to constrain the entire pipeline to obey physical laws while alleviating constraints on the intermediate layers, and prevent overfitting, we introduce a global coherence loss:

\[L_{Coh}(_{LQ},_{HQ},_{HQ})=\|(_{HQ} _{HQ}+(-_{HQ}))-_{LQ}\|_{1},\] (17)

where \(\) is the Hadamard product, \(\) means the all-ones matrix as the same size of \(_{LQ}^{S}\). The global coherence loss ensures that CORUN can more efficiently integrate physical information into the deep network to facilitate the recovery of more physically consistent details. In addition, we introduce a density loss \(L_{dens}\) based on \(()\) to score and constraint the model to dehaze in the semantic domain:

\[L_{Dens}()=().\] (18)

**Pre-training phase.** To ensure the capacity in dehazing and transmission map estimation, we pre-trained CORUN on synthetic paired datasets which contained clear image \(_{GT}^{S}^{H W 3}\) and synthetic hazy image \(_{LQ}^{S}^{H W 3}\). Setting \(_{LQ}^{S}\) as input, we can get the result by

\[_{HQ}^{S},_{HQ}^{S}=f_{_{stu}}(_{w}( _{LQ}^{S})),\] (19)

where \(_{w}\) means weakly geometric data augment, \(_{HQ}^{S}\) means the dehazed result of synthetic hazy image, and \(_{HQ}^{S}\) is the corresponding transmission map. In the pre-training phase, our CORUN isoptimized end-to-end using two supervised loss functions. The overall loss of the pre-training phase:

\[L_{pre}= _{r}L_{Rec}^{contrna}(_{w}(_{LQ}^{S}),_{HQ}^{S},_{GT}^{S})\] (20) \[+_{c}L_{Coh}(_{w}(_{LQ}^{S}),_{ HQ}^{S},_{HQ}^{S})+L_{Dens}(_{HQ}^{S}),\]

where \(_{r}\) is the trade-off weight of \(L_{Rec}^{contrna}\), \(_{c}\) is the trade-off weight of \(L_{Coh}\).

**Fine-tuning phase.** In fine-tuning phase, we adapt our CORUN pre-trained on synthetic data to the real-world domain by our Colabator framework. For more steady learning, in this phase, we train with both synthetic and real-world data. As eq. (13), we generate \(_{HQ}^{R},_{HQ}^{R},_{Sec}^{R},_{Pse}^ {R},w_{pse}\) from \(_{LQ}^{R}\), and we get \(_{HQ}^{S},_{HQ}^{S}\) use the eq. (19). The overall loss of the fine-tuning phase:

\[L_{fine}= w_{r}L_{Rec}^{contrna}(_{s}(_{LQ}^{R}), _{HQ}^{R},_{Sec}^{R})+_{r}L_{Rec}^{common}(_ {HQ}^{S},_{GT}^{S})\] (21) \[+w_{c}L_{Coh}(_{s}(_{LQ}^{R}),_{HQ}^{R},_{HQ}^{R})+L_{Dens}(_{HQ}^{S})+wL_{Dens}( _{HQ}^{R}).\]

## 4 Experiments

### Experimental Setup

**Data Preparation.** We use RIDCP500  dataset, comprising 500 clear images with depth maps estimated by , and follow the same way of RIDCP  for generating paired data. During the fine-tuning phase, we incorporate the URHI subset of RESIDE dataset , which only consists of 4,807 real hazy images, for generating pseudo-labels and fine-tuning the network. We evaluate our framework qualitatively and quantitatively on the RTTS subset, which comprises over 4,000 real hazy images featuring diverse scenes, resolutions, and degradation. Fattal's dataset , comprising 31 classic real hazy cases, serves as a supplementary source for cross-dataset visual comparison.

**Implementation Details.** Our framework is implemented using PyTorch  and trained on four NVIDIA RTX 4090 GPUs. During the pre-training phase, we train the network for 30K iterations,

   Metrics & Hazy & PDN  & MBDN  & DH  & DAD  & PSD  & D4  & RIDCP  & DGUN  & Ours \\  FADE\(\) & 2.484 & **0.876** & 1.363 & 1.895 & 1.130 & 0.920 & 1.358 & 0.944 & 1.111 & **0.824** \\ BRISQUE\(\) & 36.642 & 30.811 & 27.672 & 33.862 & 32.241 & 27.713 & 33.210 & **17.293** & 27.968 & **11.956** \\ NIMA\(\) & 4.483 & 4.464 & 4.529 & 4.522 & 4.312 & 4.598 & 4.484 & **4.965** & 4.653 & **5.342** \\   

Table 1: Quantitative results on RTTS dataset. **Red** and **blue** indicate the best and the second best.

Figure 4: Visual comparison on RTTS. Please zoom in for a better view.

optimizing it with AdamW  using momentum parameters (\(_{1}=0.9,_{2}=0.999\)) and an initial learning rate of \(2 10^{-4}\), gradually reduced to \(1 10^{-6}\) with cosine annealing. In Colabator, the initial learning rate is set to \(5 10^{-5}\) with only 5K iterations. Following , we employ random crop and flip for synthetic data augmentation. We use DA-CLIP  as our haze density evaluator and MUSIQ  as the image quality evaluator. Our CORUN consists of 4 stages and the trade-off parameters in the loss are set to \(_{c},_{r},_{c}\) are set to \(0.2,5,10^{-2}\), respectively.

**Metrics.** We utilize the Fog Aware Density Evaluator (FADE)  to assess the haze density in various methods. However, FADE focuses on haze density exclusively, overlooking other crucial image characteristics such as color, brightness, and detail. To address this limitation, we also employ Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) , and Neural Image Assessment(NIMA)  for a more comprehensive evaluation of image quality and aesthetic. Higher NIMA scores, along with lower FADE and BRISQUE scores, indicate better performance. We use PyIQA  for BRISQUE and NIMA calculations, and the official MATLAB code for FADE calculations. All of these metrics are non-reference because there is no ground-truth in RTTS .

### Comparative Evaluation

We compare our method with 8 state-of-the-art methods: PDN , MBDN , DH , DAD , PSD , D4 , RIDCP , DGUN . The quantitative results, presented in table 1, show that our method achieved the highest performance, outperforming the second-best method (RIDCP) by \(17.0\%\). Specifically, our method improved FADE, BRISQUE, and NIMA scores by \(12.7\%\), \(30.8\%\), and \(7.6\%\), respectively. This demonstrates that our method surpasses current state-of-the-art techniques in both dehazing capability and the quality, and aesthetics of the generated images.

The visual comparisons of our proposed method and state-of-the-art algorithms are shown in figs. 4 and 5. We can observe that these methods have demonstrated some effectiveness in real-world dehazing tasks, but when images containing white objects, sky, or extreme haze, the results from PDN, DAD, PSD, and RIDCP exhibited varying degrees of dark patches and contrast inconsistencies. Conversely, D4 caused an overall reduction in brightness, leading to detail loss in darker areas. Under these conditions, DGUN produced relatively aesthetically pleasing results but lost significant local detail, impairing overall visual quality. Notably, PSD achieved higher brightness but suffered from

   Datasets & Metrics &  w/o Colabator \\ teacher \\  &  w/o Trusted \\ weight \\  &  w/o Optimal \\ label pool \\  \\  RTTS &  FADE\(\) \\ BRISQUE\(\) \\ NIMA\(\) \\  &  0.912 \\ 15.728 \\ 4.921 \\ 4.921 \\  &  0.827 \\ 15.606 \\ 4.867 \\ 4.867 \\  &  0.846 \\ 15.707 \\ 5.285 \\  &  0.878 \\ 15.520 \\ 5.281 \\  &  0.808 \\ 11.956 \\ 5.342 \\  &  0.824 \\ 0.839 \\ 16.227 \\ 5.187 \\  & 
 0.839 \\ 16.227 \\ 5.342 \\  \\   

Table 3: Module’s Effect of our Colabator.

Figure 5: Visual comparison on Fattal’s data.

   Datasets & Metrics &  w/o Colabator \\ teacher \\  &  w/o Trusted \\ weight \\  &  w/o Optimal \\ label pool \\  \\  RTTS &  FADE\(\) \\ BRISQUE\(\) \\ NIMA\(\) \\  &  0.912 \\ 15.728 \\ 4.921 \\ 4.921 \\  &  0.827 \\ 15.707 \\ 4.867 \\ 4.867 \\  &  0.846 \\ 15.707 \\ 5.285 \\  &  0.878 \\ 15.520 \\ 5.281 \\  &  0.808 \\ 11.956 \\ 5.342 \\  & 
 0.824 \\ 0.839 \\ 16.227 \\ 5.187 \\  \\   

Table 2: Generalization and Effect of our Colabator.

severe oversaturation. CORUN+ consistently outperforms others by producing clearer images with natural colors and better contrast, effectively removing haze while preserving image details.

### Ablation Study

**Generalization and Effect of Colabator.** We evaluates the performance and the impact of our proposed Colabator framework across different metrics. As shown in table 2, removing the fine-tuning phase of Colabator led to significant performance drops, highlighting its critical role in the dehazing process. To evaluate the generalizability of Colabator, we conducted additional experiments by replacing our CORUN with the DGUN , while maintaining consistent training settings. Results in table 2 and fig. 1 indicate that Colabator substantially enhances DGUN's performance, demonstrating its effectiveness as a plug-and-play paradigm with strong generalization capabilities.

**Effect of Colabator.** We validate the effect of our Colabator. In table 3, we systematically removed critical components, such as mean-teacher, trusted weighting, and the optimal label pool, from the model architecture. The outcomes indicate the performance deteriorates when these components are removed, highlighting their essential role in the system.

**Ablations on stage number.** The number of stages in a deep unfolding network significantly impacts its efficiency and performance. To investigate this, we experimented with different stage numbers for CORUN+, specifically choosing \(k\) values from the set \(\{1,2,4,6\}\). The results detailed in table 4, indicate that CORUN+ achieves high-quality dehazing with 4 stages. Notably, increasing the number of stages does not necessarily improve outcomes. Excessive stages can increase the network's complexity, hinder convergence, and potentially introduce errors in the results.

### User Study and Downstream Task

**User Study.** We conducted a user study to evaluate the human subjective visual perception of our proposed method against other methods. We invited five experts with an image processing background and 16 naive observers as testers. These testers were instructed to focus on three primary aspects: (i) Haze density compared to the original hazy image, (ii) Clarity of details in the dehazed image, and (iii) Color and aesthetic quality of the dehazed image. The results for each method, along with the corresponding hazy images, were presented to the testers anonymously. They scored each method on a scale from 1 (worst) to 10 (best). The hazy images were selected randomly, with a total of 225 images from RTTS and 54 images from Fattal's dataset. The user study scores are reported in table 5, showing that our method achieved the highest average score.

   Class(AP) & Hazy & PDN  & MBDN  & DH  & DAD  & PSD  & D4  & RIDCP  & DGUN  & Ours \\   Bicycle & 0.51 & 0.55 & 0.54 & 0.47 & 0.52 & 0.52 & 0.54 & **0.57** & 0.55 & **0.59** \\ Bus & 0.25 & 0.29 & 0.27 & 0.23 & 0.29 & 0.25 & 0.28 & **0.32** & **0.31** & **0.31** \\ Car & 0.61 & 0.65 & 0.63 & 0.51 & 0.65 & 0.63 & 0.64 & 0.67 & 0.66 & **0.68** \\ Motor & 0.38 & 0.45 & 0.43 & 0.37 & 0.38 & 0.42 & 0.42 & 0.47 & 0.46 & **0.49** \\ Person & 0.73 & **0.76** & 0.75 & 0.69 & 0.74 & 0.74 & 0.75 & **0.76** & **0.76** & **0.77** \\   Mean & 0.50 & 0.54 & 0.52 & 0.45 & 0.52 & 0.51 & 0.53 & **0.56** & 0.55 & **0.57** \\   

Table 6: Object detection results on RTTS.

Figure 6: Visual comparison of object detection on RTTS .

   Dataset & PDN  & MBDN  & DH  & DAD  & PSD  & D4  & RIDCP  & DGUN  & Ours \\   RTTS & 4.52 & 3.47 & 3.23 & 4.35 & 3.90 & 4.66 & **7.14** & 6.04 & **7.76** \\ Fattal’s & 4.85 & 3.33 & 3.19 & 4.80 & 4.28 & 4.38 & **7.28** & 6.33 & **8.04** \\   

Table 5: User study scores on RTTS and Fattal’s.

**Downstream Task Evaluation.** The performance of high-level vision tasks, _e.g._ object detection and semantic segmentation, is greatly affected by image quality, with severely degraded images often leading to erroneous results [52; 53]. To address this performance degradation, some methods have incorporated image restoration as a preprocessing step for high-level vision tasks. To validate the effectiveness of our approach for high-level vision, we utilized pretrained YOLOv3 , and tested it on the RTTS  dataset, and evaluated the results using the mean Average Precision (mAP) metric. As shown in table 6 and fig. 6, our method demonstrates a substantial advantage over existing methods, verifying our efficacy in facilitating high-level vision understanding.

## 5 Limitations and Future Work

In fig. 7, our CORUN+ model struggles to maintain result quality and preserve texture details when dealing with severely degraded inputs, such as strong compression and extreme high-density haze. This challenge persists across existing methods and remains unresolved. We attribute this difficulty to the model's struggle in reconstructing scenes from dense haze, where information is often severely lacking or entirely lost, affecting the reconstruction of both haze-free and low haze density areas. Moreover, the model solely focuses on dehazing and lacks the capability to address other image degradations, such as image deblurring and low-light image enhancement [56; 57], limiting its ability to achieve high-quality reconstruction results from complex degraded images. To address this limitation in future research, we propose not only focusing on environmental degradation but also considering additional information about image degradation when solving real-world dehazing problems. In addition to this, we can integrate robust generative methods to improve the network's ability to restore dense haze regions [58; 59; 60; 61; 62], synthesize haze that matches real-world distributions [63; 64; 65; 66], and introduce more modalities as supplements to RGB images, enhancing the model's ability to effectively recover details .

## 6 Conclusions

In this paper, we introduce CORUN to cooperatively model atmospheric scattering and image scenes and thus incorporate physical information into deep networks. Furthermore, we propose Colabator, an iterative mean-teacher framework, to generate high-quality pseudo-labels by storing the best-ever results with global and local coherence in a dynamic label pool. Experiments demonstrate that our method achieves state-of-the-art performance in real-world image dehazing tasks, with Colabator also improving the generalization of other dehazing methods. The code will be released.