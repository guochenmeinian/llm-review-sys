# On the Exploration of Local Significant Differences

For Two-Sample Test

 Zhi-Jian Zhou, Jie Ni, Jia-He Yao, Wei Gao

National Key Laboratory for Novel Software Technology, Nanjing University, China

School of Artificial Intelligence, Nanjing University, China

{zhouzj, nij, yaojh, gaow}@lamda.nju.edu.cn

###### Abstract

Recent years have witnessed increasing attentions on two-sample test with diverse real applications, while this work takes one more step on the exploration of local significant differences for two-sample test. We propose the \(_{}\), an effective test for two-sample testing, and the basic idea is to exploit local information by multiple Mahalanobis kernels and introduce bi-directional hypothesis for testing. On the exploration of local significant differences, we first partition the embedding space into several rectangle regions via a new splitting criterion, which is relevant to test power and data correlation. We then explore local significant differences based on our bi-directional masked \(p\)-value together with the \(_{}\) test. Theoretically, we present the asymptotic distribution and lower bounds of test power for our \(_{}\) test, and control the familywise error rate on the exploration of local significant differences. We finally conduct extensive experiments to validate the effectiveness of our proposed methods on two-sample test and the exploration of local significant differences.

## 1 Introduction

Two-sample test has attracted much attention with diverse applications such as cancer detection , distribution-shift detection , generative modeling [3; 4], etc. The basic problem is to assess whether two i.i.d. samples are drawn from the same distribution. Various kernel-based methods have been developed for two-sample test such as Maximum Mean Discrepancy (MMD) [5; 6; 7; 8] and Mean Embedding (ME) [9; 10; 11]. Another relevant approach is to construct a binary classifier and assess two samples according to classification performance [12; 13; 14; 15; 16; 17; 18; 19; 20]. For an overview of two-sample test, we refer to a survey [21, and references therein].

In many real applications, however, it is necessary to take one more step to explore and understand local significant differences, rather than only two-sample test. For example, a scientific problem in galaxy morphology is to identify some local regions of significant differences between two kinds of galaxies, which is important to discover galaxy formation and evolution history . On the analysis of mass cytometry data in cell biology, researchers are always interested in finding local regions of significantly different abundance between disease and healthy samples .

Several attempts have been made to explore local significant differences in the past years. A feasible solution is to partition space into several regions, and identify significantly different regions according to their cardinalities of samples [24; 25; 26; 27]. Another relevant work is to identify local significant differences by estimating kernel densities [28; 29] and conditional probabilities . Generally, it is not easy to deal with complex data by simply counting cardinalities of samples, regardless of data intrinsic correlations, and it is also difficult to make accurate estimation of kernel densities and conditional probabilities without sufficient data, especially for many regions with finite samples.

This work presents a new two-sample test from local and directional information, and further explore local significant differences. The main contributions can be summarized as follows:

* We propose the effective ME\({}_{}\) test for two-sample testing, and the basic idea is to exploit local information by multiple Mahalanobis kernels and introduce bi-directional hypothesis for testing. Intuitively, Mahalanobis kernels are more flexible to exploit local differences from neighborhoods and feature maps, and the bi-directional hypothesis is beneficial to improve the sensitivity of two-sample test with proper parameter adaptation.
* We partition the embedding space into several rectangle regions based on a new splitting criterion, which is relevant to test power and data correlation. We introduce the bi-directional masked \(p\)-value for each rectangle region, and finally explore local regions with significant difference based on our bi-directional masked \(p\)-value together with the ME\({}_{}\) test.
* We present theoretical guarantees for our ME\({}_{}\) test via the asymptotic distribution, as well as the lower bounds on the test power for our test. We also present the upper bounds on familywise error rate for our exploration of local significant differences.
* We conduct extensive experiments to validate the effectiveness and efficiency of our methods. Specifically, our methods achieve better performance on most datasets for two-sample test and exploring local significant differences, along with comparable or smaller running time.

The rest of this work is organized as follows: Section 2 presents our ME\({}_{}\) test. Section 3 explores local significant differences. Section 4 conducts extensive experiments, and Section 5 concludes with future work. All technical proofs are given in Appendix A.

## 2 Our ME\({}_{}\) Test for Two-Sample Testing

Let \(\) and \(\) denote two (unknown) Borel probability measures over an instance space \(^{d}\), and \(X=\{_{i}\}_{i=1}^{m}\) and \(Y=\{_{j}\}_{j=1}^{n}\) are two i.i.d. samples from \(\) and \(\), respectively. The goal of two-sample test is to assess whether \(X\) and \(Y\) are drawn from the same distribution; in other words, we aim to assess whether \(=\) from two samples \(X\) and \(Y\).

We introduce some necessary notations used in this work. Write \([]=\{1,2,,\}\) for integer \( 2\), and \(|A|\) denotes the cardinality of set \(A\). Let \(_{d}\) be the identity matrix of size \(d d\). For a vector \(=[a_{1},a_{2},,a_{d}]\), denote by \(()=[(a_{1}),(a_{2}),,(a_{ d})]\) with \((a_{i})=a_{i}/|a_{i}|\) for \(a_{i} 0\); otherwise, \((a_{i})=0\). Let \(_{}^{2}\) be the \(^{2}\) distribution with \(\) degree of freedom, as well as the \(p\)-value function \(_{}^{2}()\). Denote by \(_{,}^{2}\) the \(\)-quantile of distribution \(_{}^{2}\) for \((0,1)\).

### Learning multiple Mahalanobis kernels via maximizing test power in training

Following ME test [9; 10], we begin with a set of test locations \(=\{_{1},_{2},,_{}\}\) to construct discriminative features. For every \(_{i}\), we introduce a Mahalanobis kernel as follows:

\[_{i}(,_{i})=(-(-_{i})^{}M_{i}(-_{i})/2_{i}^{2})\ \ \ \ _{i}>0\ \ M_{i}.\] (1)

Here, we propose multiple Mahalanobis kernels for two-sample test, which is motivated from multiple kernel learning [31; 32] and Mahalanobis distance [33; 34; 35]. The advantage of multiple Mahalanobis kernels is to exploit intrinsic structures and correlations from different directions and regions, and adjust geometrical distribution of data so as to enlarge the distance between different samples [36; 37].

Figure 1: An illustration of different contours of Mahalanobis and Gaussian kernels for two-sample test.

This is different from previous Gaussian kernel \(_{i}(,_{i})=(-\|-_{i}\|^{2}/2^{2})\), which deals with every direction isotropically without difference. Figure 1 presents an illustration of different contours of Mahalanobis and Gaussian kernels for two-sample test. As we can see, Mahalanobis kernels are more flexible to exploit different directional information than Gaussian kernels. Our work is also different from previous deep kernel approaches , which train single one deep neural network combined with Gaussian kernel for variations in distribution smoothness and shape.

We then embed each element in \(X=\{_{i}\}_{i=1}^{m}\) and \(Y=\{_{j}\}_{j=1}^{n}\) into an \(\)-dimensional space as

\[}_{i}=(_{1}(_{i},_{1}),,_{}(_{i},_{}))^{}}_{j}=(_{1}(_{j},_{1}),,_{}(_{j},_{}))^{},.\] (2)

Denote by \(=\{}_{i}\}_{i=1}^{m}\) and \(=\{}_{j}\}_{j=1}^{n}\). We define the _pooled covariance matrix_ as

\[_{,}=_{i=1}^{m}}_{i}-_{})(}_{i}-_{})^{}}{m+n-2}+_{i=1}^{n}}_{i}-_{})(}_{i}-_{})^{}} {m+n-2}+_{d}\,\] (3)

where \(_{}=_{i=1}^{m}}_{i}/m\) and \(_{}=_{j=1}^{n}}_{i}/n\), and \(_{d}\) is introduced to guarantee the positive definiteness for small constant \(>0\). We consider the _Hotelling \(T^{2}\) statistic_, as in ,

\[(,)=mn(_{}-_{})^{} _{,}^{-1}(_{}-_{})(m+n)\.\] (4)

Test power is the probability of correctly identifying two different samples. Maximizing \((,)\) is essentially equivalent to maximizing a lower bound of test power , and we learn test locations and Mahalanobis kernels as follows:

\[\{,M_{1},,M_{},_{1},,_{}\} \{(,)\}\.\] (5)

We take gradient method  to solve the above optimization, as done by Jitkrittum et al. , and the details are presented in Appendix B.

We decompose \(_{,}=\) via the Schur method  to remove feature correlations, and it follows

\[(,)=(_{}-_{} )^{}_{,}^{-1}(_{}-_{})= {mn}{m+n}\|^{-1}_{}-^{-1}_{}\| _{2}^{2}\.\]

Hence, \((,)\) essentially measures the difference between two samples via the \(L_{2}\)-norm of vector \(^{-1}_{}-^{-1}_{}\). We further exploit their _inference direction_, defined by

\[=(^{-1}_{}-^{-1}_{} )\{-1,0,+1\}^{}\.\] (6)

#### Bi-directional hypothesis for testing

Let \(}\) and \(}\) be the corresponding embedding distributions from the original \(\) and \(\), respectively. Denote by \(_{}}=E_{}^{}}[}^{}]\) and \(_{}}=E_{}^{}}[}^{}]\). We consider the following null hypothesis

\[H_{0}_{}}=_{}}\.\]

The null hypothesis \(H_{0}\) can be used to test whether \(=\) by the following lemma:

**Lemma 1**.: _We have \(_{}}=_{}}\) iff \(=\), for bounded Mahalanobis kernels \(\{_{j}\}_{j=1}^{}\) and for test locations \(\{_{j}\}_{j=1}^{}\) drawn i.i.d. from a absolutely-continuous distribution w.r.t. Lebesgue measure._

Let \(^{}=\{}^{}_{i}\}_{i=1}^{m^{}}\) and \(^{}=\{}^{}_{j}\}_{j=1}^{n^{}}\) denote two embedding testing samples. We make similar Schur decomposition \(_{^{},^{}}=^{}^{}\), and calculate testing statistic \((^{},^{})\) according to Eqn. (4). Based on Lemma 1, we can present the asymptotic distribution of statistic \((^{},^{})\) as follows:

**Theorem 2**.: _The testing statistic \((^{},^{})\) is almost surely asymptotically distributed as \(_{}^{2}\) if \(=\); otherwise, \(_{}^{2}((^{},^{})) 0\) as \(m^{}n^{}/(m^{}+n^{})\)._

From Theorem 2, we propose the _bi-directional hypothesis_, by considering inference direction,

\[h(^{},^{})=[\,_{}^{2} (^{},^{}) ]&^{}^{-1}(_{^{}} -_{^{}}) 0\\ [\,_{}^{2}(^{},^{ })(2-)]&^{}^{ -1}(_{^{}}-_{^{}})<0,\] (7)where \((0,1)\) is the significance level of hypothesis test, and \(\) is an adaptive parameter.

Our bi-directional hypothesis is essentially about designing a rejection region of null hypothesis . How to design an efficient rejection region is an interesting problem from the early work , and some techniques has been developed for selecting rejection regions . We consider the most discriminative directions \(\) and \(-\) in our bi-directional hypothesis, which could improve the sensitivity for two-sample test by selecting appropriate parameters according to different datasets.

Our bi-directional hypothesis can be viewed as a generalization of previous hypotheses, that is,

* By setting \(=1\), our test has been the non-directional hypothesis  regardless of direction information, which is also referred to as two-sided/tailed hypothesis ;
* By setting \(=2\) and \(=(1,1,,1)^{}\), our test has become one-directional hypothesis , which is also referred to as one-sided/tailed hypothesis .

Notice that previous non/one-directional hypotheses fix the structures of rejection region for a given significance level \(\), whereas our bi-directional hypothesis could adjust rejection region according to inference direction \(\) w.r.t. different datasets, which can be illustrated in Figure 2. Here, we consider an illustrative dataset, and our bi-directional hypothesis gives the rejection region adaptive to dataset, which could yield higher test power in two-sample test, as shown in Figure 5 (in Section 4).

We can also present some distribution and probability information for \(^{}^{-1}(_{^{}}-_{^{ }}) 0\) in Eqn. (7). Denote by \(_{},}}=E_{^{}}^{},^{}}^{}}| {L}^{}|\) and \(=[^{}^{-1}(_{^{}}-_{^{}}) 0]\). We have

**Lemma 3**.: _For inference direction \(\) in Eqn. (6) and for embedding samples \(^{}\) and \(^{}\), we have_

\[^{-1}(_{^{}}-_{^{}}) _{,}}^{-1}(_{}}-_{}}),^{-1}_{} }\ \ \ \ (^{}^{-1}(_{^{}}-_{ ^{}}))()\]

_with \(=m^{}n^{}/(m^{}+n^{})\) and \(=1-(-^{}_{, }}^{-1}(_{}}-_{}}))\). Here, \(()\) is the distribution function of standard Gaussian, and \(()\) denotes a distribution over \(\{-1,+1\}\) with probability \(\) on the selection of \(+1\)._

The selection of parameter \(\) is highly positive-relevant to the probability \(\). This is because a larger \(\) implies larger difference between two samples in the inference direction \(\), and we should select a larger \(\) to enlarge the rejection region and improve the sensitivity of dataset. Figure 6 (in Section 4) shows such positive relevance between the optimal parameter \(\) and probability \(\) empirically.

We finally present theoretical analysis on test power and type-I error of our bi-directional hypothesis. Let \(f(x n_{1},n_{2},)\) be the density function of noncentral \(F\)-distribution with \(n_{1}\) and \(n_{2}\) degrees of freedom and non-centrality parameter \(\), and denote by \(F_{n_{1},n_{2},}\) the \(\)-quantile of central \(F\)-distribution with \(n_{1}\) and \(n_{2}\) degrees of freedom for \((0,1)\). We define the following probability, from the work of ,

\[q(n_{1},n_{2},,)=_{F_{n_{1},n_{2},}}^{}f(x  n_{1},n_{2},)dx\.\] (8)

**Theorem 4**.: _For our bi-directional hypothesis \(h(^{},^{})\), the test power can be lower bounded by_

\[q(,-,,)+(-(_{,(2-) }^{2})^{1/2}-(/)^{1/2}^{}_{,}}^{-1}(_{}}-_{}}))\]

Figure 2: An illustration of different rejection regions on two samples for our bi-directional hypothesis and previous non/one-directional hypothesis. Our rejection region can be adaptive according to different datasets.

_if \(^{}_{},}}^{-1}_{}}>^{}_{},}}^{-1}_{}}\); and the test power can also be lower bounded by_

\[q(,-,,(2-))(1-)+1-((_{, }^{2})^{1/2}-(/)^{1/2}^{}_{}, }}^{-1}(_{}}-_{} }))\]

_if \(^{}_{},}}^{-1}_{}}<^{}_{},}}^{-1}_{}}\); and the type-I error rate is equal to \(\) if \(_{}}=_{}}\). Here, \(=m^{}n^{}/(m^{}+n^{})\), \(=m^{}+n^{}-1\) and \(=\|_{},}}^{-1}(_{ }}-_{}})\|_{2}^{2}\)._

This theorem presents lower bounds on test power, and the performance of the statistical test is maintained under the general condition. Notice that the type-I error in our hypothesis test is controlled only by the significant level \(\), regardless of different \(\) and \(\).

We call our test as \(_{}\) test because of multiple Mahalanobis kernels in training and our bi-directional hypothesis in testing.

## 3 Explore Local Significant Differences for Two-sample Test

On the exploration of local significant difference, most previous studies [24; 25; 26; 27; 28] partition the instance space into several regions, and then exploit the difference on each region from two samples. Motivated from polya tree method [26; 58], we first partition the embedding instance space with a new splitting criterion, which is relevant to test power and data correlations. We then exploit local regions (i.e., leaves nodes of partition tree) with significant difference.

### Partition of the embedding instance space

Our partition tree is constructed iteratively as follows: We initiate the tree root with embedding space \((0,1]^{}\). In each iteration, each node is associated with a rectangle region, and all leaves constitute a partition of embedding instance space. The following procedure is repeated \(s-1\) iterations (\(s 2\)):

* Randomly select a leaf node, denote by \(\), uniformly over leaf nodes of the largest \(||\).
* Let \(_{j}=\{}_{i,j}}_{i} \}\) for \(j[]\). We select the best splitting feature \[j^{*}_{j[]}\{(_{^{j}_{l}},_{^{j}_{l}})(_{^{j}_{l}},_{^{j}_{l}})\}\,\] with \(_{^{j}_{l}}=^{j}_{l}\), \(_{^{j}_{r}}=^{j}_{r}\), \(_{^{j}_{l}}=^{j}_{l}\) and \(_{^{j}_{r}}=^{j}_{r}\). Here, \(^{j}_{l}\) and \(^{j}_{r}\) are left and right children of \(\) w.r.t. the \(j\)-th splitting feature and splitting position \(_{j}\), respectively, and \((,)\) is defined by Eqn. (4).
* Select the splitting position \(_{j^{*}}=\{}_{i,j^{*}}}_{i} \}\).

We finally get the partition tree with \(s\) leaf nodes, associated with \(s\) rectangle regions \(_{1},_{2},,_{s}\). Algorithm 1 presents the detailed description on tree construction and rectangle region splitting.

We take the statistic \((,)\) as a splitting criterion, relevant to test power and data correlations, and it is helpful to exploit local significant difference directly. We also adopt the median splitting position with equal probabilities on partitioned regions, i.e., balanced examples for each partition region, and this could yield better performance than regular grids, as shown empirically in .

Our partition tree is different from previous \(p\)-value histogram based on Chi-square test , where the difference is measured by cardinalities of elements in two samples over a local rectangle region. Our splitting criterion is also different from that of previous decision trees , which consider some information-theoretic criterions such as entropy, Gini index, information gain, etc. In comparisons, our statistic \((,)\) is more essential to reflect the test power for two-sample test.

For each rectangle region \(_{i}\), let \(_{_{_{i}}}\) and \(_{_{_{i}}}\) be the means of \(_{_{i}}=_{i}\) and \(_{_{i}}=_{i}\), respectively. We make similar Schur decomposition \(_{_{i}}_{_{i}}=_{_{_{i}},_{_{i}}}\) for covariance matrix \(_{_{_{i}},_{_{i}}}\), and introduce the local inference direction for each \(_{i}\) as follow:

\[_{_{i}}=(_{_{i}}^{-1}_{ _{_{i}}}-_{_{i}}^{-1}_{_{ _{i}}})\{-1,0,+1\}^{}\;.\] (9)

For different rectangle regions, we could have different or even contrary inference directions, which is helpful to exploit local differences from distributional shapes of two samples.

#### Exploration of local significant differences

For testing embedding samples \(^{}\) and \(^{}\), we denote by \(^{}_{_{i}}=_{i}^{}\) and \(^{}_{_{i}}=_{i}^{}\) with their respective means \(_{^{}_{_{i}}}\) and \(_{^{}_{_{i}}}\) for each rectangle region \(_{i}\), and calculate testing statistic \(_{_{i}}=(^{}_{_{i}}, ^{}_{_{i}})\) by Eqn. (4).

We propose the new _bi-directional masked \(p\)-value_ for each rectangle region \(_{i}\) as follows:

\[g(^{}_{_{i}},^{}_{_{i}})= \{_{}(_{_{i}})}{ },(1-^{2}_{}(_{_{i}})) }{1- p_{*}}\}&\ \ ^{}_{_{i}}^{-1}_{_{i}}(_{ ^{}_{_{i}}}-_{^{}_{_{i}}}) 0\\ \{_{}(_{_{i}})}{2-}, (1-^{2}_{}(_{_{i}}))}{1-(2 -)p_{*}}\}&\ \ ^{}_{_{i}}^{-1}_{_{i}}(_{ ^{}_{_{i}}}-_{^{}_{_{i}}})<0\;, \] (10)

where \(^{-1}_{_{i}}\) is from Schur decomposition \(_{^{}_{_{i}},^{}_{_{i}}}= ^{-1}_{_{i}}^{-1}_{_{i}}\), \(p_{*}(0,1)\) is a parameter on significance level, and \(\) is an adaptive parameter. Here, we also consider two discriminative directions \(_{_{i}}\) and \(-_{_{i}}\) on each rectangle region \(_{i}\), which is different from previous masked \(p\)-value  without directional information.

Our bi-directional masked \(p\)-value directly reflects the significant level of local difference when there is a significant difference in local \(_{i}\), similarly to . The smaller the bi-directional masked \(p\)-value, the more significant the local difference. On the other hand, the bi-directional masked \(p\)-value is a random number with uniform distribution over \((0,p_{*})\) when there is no significant difference in \(_{i}\), since \(^{2}_{}(_{_{i}})\) follows a uniform distribution in such case .

Based on such recognition, we resort rectangle regions as \(_{ 1},_{ 2},, _{ s}\) according to their bi-directional masked \(p\)-value, i.e.,

\[g(^{}_{_{ 1}},^{}_{ _{ 1}}) g(^{}_{_{ 2 }},^{}_{_{ 2}}) g(^{}_{ _{ s}},^{}_{_{ s }})\;.\]

  Dataset & \# Inst. & \# Feat. & Dataset & \# Inst. & \# Feat. & Dataset & \# Inst. & \# Feat. & Dataset & \# Inst. & \# Feat. \\  dna & 3,186 & 180 & krypt & 27,705 & 6 & santan & 200,000 & 200 & adult & 1,000,000 & 14 \\  agnos & 3,468 & 970 & diamon & 53,940 & 9 & codma & 487,867 & 8 & labor & 1,000,000 & 16 \\  topo21 & 8,885 & 266 & cifar10 & 60,000 & 3072 & blob & 1,000,000 & 2 & poker & 1,025,010 & 10 \\  har & 10,299 & 561 & mnist & 70,000 & 784 & sea50 & 1,000,000 & 3 & higgs & 11,000,000 & 4 \\  

Table 1: DatasetsWe then take our bi-directional hypothesis \(h(^{}_{_{i}},^{}_{_{i}})\) with parameter \(\) and \(=p_{*}\) as in Eqn. (7), and finally get the local regions with significant differences as

\[\{_{ i}\;\;i t^{*}\;\;\;\;h( ^{}_{_{ i}},^{}_{_{ i }})=1\}\;,\]

where

\[t^{*}=*{arg\,max}_{t[s]}\{t-|\{i[t] h(^{}_{_{ i}},^{}_{_{ i }})=1\}|+1)}{(1-p_{*})}\}\;.\] (11)

Here, \(p_{*}\) is selected as in Eqn. (10), and \(_{*}[p_{*},1)\) is a parameter to control the probability of mis-identifying at least one rectangle region without significant difference, also called _familywise error rate_. We present theoretical analysis for familywise error rate as follows:

**Theorem 5**.: _For our exploration, the familywise error rate is upper bounded by \(_{*}\), if 1) the \(p\)-values of local regions without differences are mutually independent; and 2) the \(p\)-values of local regions with differences are independent to those \(p\)-values of local regions without differences._

Our method is different from previous space partition methods of trees or clusters , where the splitting criterion is taken as the cardinalities of samples in each region. Duong  partitioned and searched local regions from the estimated density function, and Kim et al.  identified local regions by clustering data samples from the estimated conditional probabilities. It is not easy to make accurate estimation for density and conditional probabilities without sufficient data, particularly for multiple small regions. Other relevant studies detected local differences implicitly based on interactive rank test  or a learned classifier .

## 4 Experiments

We conduct experiments on 16 datasets1 as summarized in Table 1. Most dataset have been studied in previous two-sample test, and features have been scaled to \(\) for all datasets. All experiments are performed with Python on nodes of a computational cluster with a single CPU (Intel Core i9-10900X 3.7GHz) and a single GPU (GeForce RTX 2080 Ti), running Ubuntu with 128GB main memory.

and other datasets are downloaded from _www.openml.org_.

#### Experimental comparisons for two-sample test

We compare our ME\({}_{}\) with the state-of-the-art approaches on two-sample test as follows:

  Dataset & Our ME\({}_{}\) & ME & MMDAgg & MMD-D & C2ST-L & C2ST-S & AutoMLST \\  blob & **.985\(\).009** &.823\(\).000 &.935\(\).012 &.963\(\).010 &.972\(\).078 &.946\(\).037 &.980\(\).029 \\ dna & **.717\(\).068** &.536\(\).059 &.659\(\).070 &.628\(\).006 &.699\(\).028 &.505\(\).044 &.603\(\).085 \\ agnos & **.812\(\).018** &.602\(\).033 &.779\(\).046 &.734\(\).006 &.742\(\).012 &.679\(\).051 &.632\(\).077 \\ topo21 & **.692\(\).006** &.526\(\).058 &.605\(\).077 &.633\(\).062 &.679\(\).046 &.517\(\).046 &.591\(\).006 \\ har & **.858\(\).065** &.816\(\).015 &.814\(\).026 &.728\(\).064 &.761\(\).093 &.738\(\).063 &.740\(\).058 \\ kropt & **.992\(\).012** &.875\(\).027 &.971\(\).024 &.916\(\).066 &.946\(\).013 &.929\(\).031 &.971\(\).026 \\ diamond & **.837\(\).066** &.697\(\).068 &.676\(\).047 &.755\(\).056 &.747\(\).086 &.727\(\).076 &.831\(\).062 \\ cifar & **.893\(\).022** &.859\(\).075 &.866\(\).091 &.878\(\).090 &.834\(\).099 &.798\(\).019 &.882\(\).086 \\ mnist & **.985\(\).017** &.926\(\).056 &.932\(\).068 &.972\(\).051 &.969\(\).042 &.930\(\).029 &.963\(\).074 \\ samtan & **1.00\(\).000** &.896\(\).060 & **1.00\(\).000** &.887\(\).021 &.911\(\).084 &.850\(\).021 &.954\(\).012 \\ codrma & **1.00\(\).000** &.946\(\).085 &.926\(\).037 &.914\(\).076 & **1.00\(\).000** & **1.00\(\).000** &.876\(\).067 \\ sea50 & **.993\(\).018** & **.993\(\).018** &.982\(\).012 & **.993\(\).018** & **.993\(\).018** &.970\(\).053 &.989\(\).029 \\ adult & **.996\(\).002** &.875\(\).034 &.967\(\).029 &.908\(\).072 &.761\(\).091 &.854\(\).058 &.992\(\).006 \\ labor &.992\(\).012 &.807\(\).078 &.988\(\).010 &.930\(\).093 &.756\(\).059 &.791\(\).031 & **1.00\(\).000** \\ poker &.821\(\).079 &.719\(\).096 &.712\(\).033 &.701\(\).056 &.743\(\).039 &.731\(\).052 & **.832\(\).048** \\ higgs & **.979\(\).024** &.818\(\).090 &.938\(\).047 &.953\(\).055 &.968\(\).043 &.933\(\).013 &.969\(\).030 \\  Average & **.909\(\).026** &.795\(\).053 &.859\(\).039 &.843\(\).050 &.842\(\).052 &.806\(\).039 &.863\(\).043 \\  

Table 2: Comparisons of test powers (mean\(\)std) on two-sample test. Bold denotes the highest mean in per row.

* ME: Mean Embeddings over multiple test locations and a single Gaussian kernel [9; 10];
* MMD-D: Maximum Mean Discrepancy based on a Deep kernel ;
* MMDAgg: Maximum Mean Discrepancy with Aggregating of multiple Gaussian kernels ;
* C2ST-S: Train a binary classification network and test its accuracy on a hold-out set ;
* C2ST-L: Train a binary classification network with a statistic about class probabilities [14; 18];
* AutoMLTST: Train a binary classifier based on AutoML method with a statistic as C2ST-L .

Following [39; 76], we train on a subset of each available data, and test on 100 random subsets from the remaining dataset, and the ratio is set as \(4:1\) for training and testing. We repeat such process 10 times for each dataset. More details are given in Appendix C. For our ME\({}_{}\), we set \(=0.05\) and take 5-fold cross validation to select \([1:0.2:2]\). We limit the cardinality of test locations within \(20\) for ME and ME\({}_{}\) as in [9; 10; 11], and optimization parameters of Eqn. (5) is presented in Appendix C. We take parameter settings for other methods as in their respective inferences.

Table 2 summarizes the average of test powers and standard deviations. It is evident that our ME\({}_{}\) takes better performance than ME and MMDAgg, because they both take Gaussian kernels with isotropic scale, and ignore the distributional differences from different directions. Our method is still better than MMD-D with a deep kernel, and a reason is that multiple Mahalanobis kernels are more flexible than a deep kernel to capture local difference from multiple neighborhoods and directions.

From Table 2, it is also observed that our ME\({}_{}\) outperforms three classifier-based methods C2ST-S, C2ST-L and AutoML expect for datasets labor and poker, since those methods focus merely on the prediction information from outputs of classifiers, rather than local and directional information among data samples. For datasets labor and poker, AutoML generates new features automatically from the original mixture of continuous and symbolic features, and thus achieves better performance.

We further compare the average running time (in seconds) for different methods on two-sample test, as shown in Figure 3. As expected, ME takes the least running time since it considers only one Gaussian kernel, yet with the smallest average of test powers in Table 2. Our ME\({}_{}\) method takes smaller and comparable running time in contrast to other methods since our method takes relatively smaller time on training Mahalanobis kernels without permutation test in the testing process.

#### Experiments on the exploration of local significant differences

We compare with the state-of-the-art approaches on exploring local significant difference as follows:

* FDG: Partition space by probability binning and compare cardinalities of two samples ;
* K-PRIM: Partition space by patient rule induction and estimate kernel density differences ;
* MRS: Partition space by polya tree and measure difference via Binomial distributions ;
* TEAM: Partition space by data variance and measure difference via Binomial distributions ;
* BTLDD: Estimate conditional probabilities of two samples and cluster data via difference ;
* MMDT: Partition space into equal grids and test density difference via Welch's statistic .

  Dataset & Our method & FDG & KPRIM & MRS & MMDT & BTLDD & TEAM \\  blob & **.945\(\).082** &.902\(\).075 &.879\(\).045 &.849\(\).075 &.909\(\).091 &.932\(\).046 &.877\(\).067 \\ diamon & **.974\(\).054** &.852\(\).010 &.895\(\).089 &.876\(\).066 &.867\(\).104 &.951\(\).070 &.947\(\).022 \\ codrna & **.969\(\).026** &.936\(\).037 &.876\(\).061 &.966\(\).045 &.884\(\).105 &.905\(\).050 &.863\(\).036 \\ sea50 &.977\(\).056 &.975\(\).062 &.933\(\).074 &.928\(\).058 & **.985\(\).045** &.892\(\).059 &.944\(\).065 \\ adult & **.953\(\).048** &.862\(\).044 &.838\(\).109 &.911\(\).085 &.927\(\).101 &.875\(\).074 &.880\(\).070 \\ labor & **.959\(\).062** & **.894\(\).080** &.905\(\).016 &.900\(\).037 &.911\(\).093 &.922\(\).048 &.932\(\).067 \\ poker & **.945\(\).030** &.901\(\).030 &.882\(\).027 &.925\(\).023 &.927\(\).057 &.894\(\).028 &.884\(\).064 \\ higgs & **.946\(\).016** &.932\(\).011 &.918\(\).001 &.940\(\).000 &.927\(\).026 &.926\(\).027 &.937\(\).002 \\  Average & **.959\(\)**.047 &.907\(\).044 &.891\(\).053 &.912\(\).049 &.917\(\).078 &.912\(\).050 &.908\(\).049 \\  

Table 3: Comparisons of density differences (mean\(\)std) on the exploration of local significant differences, and the bold denotes the highest mean in per row.

[MISSING_PAGE_FAIL:9]

Figure 5 illustrates the test power versus sample size for our bi-directional hypothesis, non-directional hypothesis and one-directional hypothesis. As can be seen, our bi-directional hypothesis achieves higher test power by considering the inference and its contrary direction and adaptive parameter selection. Figure 6 exploits the relationship between the optimal parameter \(\) and the probability \(=[^{}}^{-1}(_{^{}}-_ {^{}}) 0]\) for our ME\({}_{}\) method. We can easily find the positive relevance between \(\) and \(\): the larger the probability \(\), the larger the optimal parameter \(\).

Figure 7 indicates that the type-I error is limited about \(=0.05\) for different \(\) in our experiments, as shown in Theorem 4, and thus our method could effectively control the rate of falsely reject the null hypothesis, which empirically verify the trustworthiness of our ME\({}_{}\) test. Figure 8 empirically shows the familywise error rate is limited about \(_{*}=0.05\) for different number of local regions \(s\); therefore, our exploring method could control the rate of incorrectly exploiting the local regions with significant difference, and this is nicely in accordance with Theorem 5.

## 5 Conclusion

This work takes one more step on the exploration of local significant differences. We propose the ME\({}_{}\) test by exploiting local information from multiple Mahalanobis kernels and introducing bi-directional hypothesis for testing. We partition embedding space via a new splitting criterion, and then identify local significant differences based on our bi-directional masked \(p\)-value and ME\({}_{}\) test. We verify the effectiveness of our proposed methods both theoretically and empirically. An interesting work is to explore other local and directional information for local significant differences.