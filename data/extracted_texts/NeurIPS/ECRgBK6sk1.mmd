# Prototype-based Aleatoric Uncertainty Quantification

for Cross-modal Retrieval

 Hao Li

18th.leolee@gmail.com

Jingkuan Song

18th.leolee@gmail.com

&Lianli Gao

jingkuan.song@gmail.com

&Xiaosu Zhu

xiaosu.zhu@outlook.com

&Heng Tao Shen

shenhengtao@hotmail.com

Center for Future Media,

University of Electronic Science and Technology of China

Corresponding author.

###### Abstract

Cross-modal Retrieval methods build similarity relations between vision and language modalities by jointly learning a common representation space. However, the predictions are often unreliable due to the _Aleatoric_ uncertainty, which is induced by low-quality data, _e.g._, corrupt images, fast-paced videos, and non-detailed texts. In this paper, we propose a novel Prototype-based Aleatoric Uncertainty Quantification (PAU) framework to provide trustworthy predictions by quantifying the uncertainty arisen from the inherent data ambiguity. Concretely, we first construct a set of various learnable prototypes for each modality to represent the entire semantics subspace. Then _Dempster-Shafer Theory_ and _Subjective Logic Theory_ are utilized to build an evidential theoretical framework by associating evidence with Dirichlet Distribution parameters. The PAU model induces accurate uncertainty and reliable predictions for cross-modal retrieval. Extensive experiments are performed on four major benchmark datasets of MSR-VTT, MSVD, DiDeMo, and MS-COCO, demonstrating the effectiveness of our method. The code is accessible at [https://github.com/leolee99/PAU](https://github.com/leolee99/PAU).

## 1 Introduction

Cross-modal Retrieval, devoted to searching a gallery of related samples in vision/language modality given a query in another, has attracted increasing interest as the wellspring development of multimedia platforms. However, the heterogeneity between modalities casts daunting challenges for this task. To tackle these challenges, most existing methods  map the query and retrieval items from distinct modalities into a joint latent space to make similarity calculation feasible.

Some early works  corroborate the powerful capabilities of deep neural networks to extract high-level features. Chen _et al._ construct a hierarchical graph structure for videos and texts to build the relationships between the local and the global features. PVSE  utilizes the multi-head self-attention module to learn multiple and diverse representations of videos and texts for the polysemous problem. Recently, a large body of transformer-based pre-training models  have shown remarkable superiority with excellent generalization and performance.

However, prior approaches routinely assume the data qualities are absolutely stable and view them equally. They estimate the relevance of vision-language pairs relying only on the similarity scoresgenerated by complex neural networks, but ignoring the confidence of these predictions, which should be taken seriously. Low-quality data like corrupt images , fast-paced videos , non-detailed sentences , _etc._, inevitably lead to unreliable results. The model therefore not only needs to predict the relevance of vision-language pairs, but also capably answer the question "_How confident is the prediction?_". Namely, it is necessary to quantify the uncertainty of each data to guide more reliable vision-language pair similarity.

In the literature, uncertainty can be classified into _Aleatoric Uncertainty_ and _Epistemic Uncertainty_ based on causation . The former is induced by the inherent data defects, while the latter is inherent to the model. This work mainly focuses on the aleatoric uncertainty (_i.e._, data uncertainty), defined as "the uncertainty caused by ambiguous multi-modal data, such as fast-paced videos and non-detailed texts". Next, we will answer the question "_Why do these ambiguous data lead to uncertainty?_". We first assume the possible semantics of each modal subspace are finite with \(K\) categories. Afterward, the fast-paced videos (multi-scene videos) and the non-detailed texts are generally highly related to multiple semantics, misleading models into confused matching, shown in the Figure 1. These alternative matches will bring uncertainty, which can be explained through _Information Entropy_. Concretely, Information Entropy is the average uncertainty level in the random variable's possible outcomes, which can be denoted as Eq. 1 with the possible semantics as the random variable \(X\).

\[(X)=[- p(x_{i})]=-_{i=1}^{K}p(x_{i}) p(x_{i}), \]

where \(_{i=1}^{K}p(x_{i})=1\), \(p(x_{i})\) is the \(i^{th}\) semantics probability derived from semantic cosine similarity through a softmax layer. The ambiguous data highly associated with multiple semantics have a low variance of the probability distribution, resulting in high information entropy and high uncertainty due to "_The Maximum Entropy Principle_" (proof in Appendix. E). Information entropy is a seemingly reasonable criterion to estimate the data uncertainty, but unfortunately, it is powerless to distinguish the instances with the similarity distribution of \([0.8,0.8,0.8,0.8]\) and the distribution of \([0.2,0.2,0.2,0.2]\). After a softmax function, both distributions turn into \([0.25,0.25,0.25,0.25]\) and obtain the same information entropy. In practice, we hope the first one gets a higher uncertainty score than the other because the front of the retrieval sequence with highly similar instances is more notable for the retrieval task. Thus, the _Dempster-Shafer Theory of Evidence_ (DST) , a generalization of the Bayesian theory to subjective probabilities, which has developed into a general framework for uncertainty modeling, is leveraged to quantify multi-modal aleatoric uncertainty.

To precisely quantify the aleatoric uncertainty, in this work, we propose an effective Prototype-based Aleatoric Uncertainty Quantification (PAU) framework to provide trustworthy predictions by quantifying the uncertainty arisen from inherent data ambiguity. To be specific, we first construct a series of diverse prototypes for each modality to capture the overall \(K\) semantics of the subspace. Each prototype represents an individual semantics. Afterward, the variations of cosine similarities

Figure 1: **Illustration of confused matching in fast-paced videos and non-detailed texts.** Assuming the possible semantics of each modal subspace are finite with \(K\) categories. (a) A single-scene _Video A_ can only match one semantics of “_talking_”. By contrast, a multi-scene _Video B_ can match to 3 semantics of “_talking_”, “_shadow_”, and “_cave_”. (b) _Text A_ can only match the left video, while _Text B_ with some details removed (in red) matches both videos.

between the instance and the prototypes from another modality are deployed as the belief masses used in DST  to model uncertainty. Finally, a re-ranking practice is utilized to further strengthen prediction reliability by regarding each data uncertainty as a similarity weight.

The main contributions of our paper are three folds. 1) We give a clear definition of the inherent aleatoric uncertainty in multi-modal data. 2) A powerful aleatoric uncertainty quantification framework PAU is built to accurately estimate the data uncertainty and effectively mitigate the negative impact on retrieval. 3) We also verify the superiority of our proposed PAU on multiple challenging benchmarks. Massive insightful ablation studies further reveal its effectiveness and generalization.

## 2 Related work

### Cross-modal Retrieval

Cross-modal retrieval [9; 25; 67; 39; 17; 60; 54; 49; 22; 18; 66; 26; 64; 37], as one of the most popular tasks in multi-modal learning, has attracted increasing attention with the aim of searching similar semantic samples from different modalities. Early works [25; 39] build a joint embedding space to make query-positive pairs closer by canonical correlation analysis (CCA) . Faghri _et al_.  first use a triplet loss to improve the model performance by focusing only on the hardest negative. PVSE  proposes a multi-candidate representation function to capture the diversity and build one-to-many relationships in different modalities. Recently, CLIP  has shown vital power and generalization in various multi-modal tasks. A rich body of CLIP-based approaches [49; 22; 18; 66; 26] have got significant achievements in cross-modal retrieval. CLIP4Clip  transfers the rich knowledge from image-text pre-trained model CLIP to video-text retrieval. CenterCLIP  proposes a multi-segment token clustering algorithm to keep the most representative tokens in consecutive frames. In this paper, we follow the setting of the above work, which inherits the knowledge from CLIP.

### Uncertainty Quantification

Generally, there are two paradigms among existing uncertainty estimation approaches. The first one is the Bayesian-based paradigm [5; 51; 40], which estimates uncertainty by approximating the moments of the posterior predictive distribution. Subsequently, some algorithms focus on the variety of Bayesian such as Laplace approximation , Markov Chain Monte Carlo , and variational techniques [55; 6]. Recent work  leverages dropout  in the test phase, reducing the computational cost of uncertainty estimation. Nonetheless, limited by the increment of model parameters and poor convergence, these approaches always have expensive training costs and slow inference time. The second paradigm focuses on the non-Bayesian approach [41; 57; 27]. Lakshminarayanan _et al_.  propose an alternative to Bayesian Neural Networks (BNN) which is simple to train and produce reliable uncertainty estimates. Wang _et al_.  replace the parameter set of a categorical distribution with the parameters of a Dirichlet density and represent the uncertainties with a single abnormal class. Recently, uncertainty-based algorithms have been used in multi-view classification for trusted decision-making . Although uncertainty-based methods have achieved impressive progress, they are limited to working with particular classification and segmentation tasks.

## 3 Method

In this section, we present our **Prototype-based Aleatoric Uncertainty Quantification (PAU)** framework (Figure 2). We first define the cross-modal retrieval task and introduce a commonly used pipeline in Section 3.1. Next, we detail the uncertainty theory, as well as the approach to quantify the uncertainty in Section 3.2.

### Task Definition

Let \(=(,)\) denote a vision and language dataset, where \(\) is a set of images or videos, and \(\) is a set of texts. The goal of cross-modal retrieval is to rank the relevance between a visual query \(v\) (respectively a textual query \(t\)) and textual set \(\) (respectively visual set \(\)). Recent works [54; 49; 22; 18; 66; 26] have shown CLIP's strong performance and generalization in various downstream tasks, inspiring us to employ CLIP as our backbone.

To be specific, given a visual instance \(v\) and a text instance \(t\), the CLIP encodes them into a latent embedding space \(^{D}\) through a visual encoder \(_{v}()\) and a textual encoder \(_{t}()\), respectively. Then the relevance between them can be computed using a cosine similarity function as:

\[(v,t)=(_{v}(v),_{t}(t))=(v)}{\| _{v}(v)\|}(t)}{\|_{t}(t)\|} \]

### Uncertainty Quantification

**Prototype construction.** Although the cosine similarity function has shown effectiveness in most existing works [25; 60; 54; 49], some unreliable results will inevitably be produced due to the low inherent data quality. These ambiguous data are generally highly related to multiple semantics, leading to confusing matching (see Figure 1).

To assess these data uncertainty and provide trustworthy predictions, we first assume the semantic space is finite with \(K\) semantic categories. Then, \(K\) learnable prototypes \(^{K D}\) are constructed to represent the overall latent semantic subspace for each modality, making data semantic ambiguity evaluable. Each prototype represents an individual semantics. Finally, DST  is employed to build the uncertainty quantification framework by assigning the variations of similarities between an instance and the prototypes from another modality as the belief masses.

**The theory of uncertainty.** The _Dempster-Shafer Theory of Evidence_ (DST) , a generalization of the Bayesian theory to subjective probabilities , builds a general uncertainty modeling framework by assigning belief masses to the set of mutually exclusive possible states with one state expressing "_I am not sure_". Following EDL , we adopt _Subjective Logic_ (SL)  to associate the belief distribution in DST with the parameters of the Dirichlet distribution. More precisely, SL provides an evidential theoretical framework to quantify the belief masses \(b_{k}\) of different categories and overall uncertainty mass \(\) for the \(K\) classification task. These \(K+1\) mass values are all non-negative and sum up to one:

\[+_{k=1}^{K}b_{k}=1, \]

where \(b_{k} 0\) and \( 0\) denote the belief mass of the \(k^{th}\) semantic category (\(k=1,,K\)) and overall uncertainty mass, respectively. In our setting, \(\) here means the uncertainty mass of the

Figure 2: **The Framework of PAU.** The visual encoder \(_{v}\) and textual encoder \(_{t}\) separately map the visual and textual instances into a joint embedding space to calculate the similarity matrix \(\). A dot product function is used to build a set of similarity vector \(^{N K}\) between \(N\) instances and \(K\) prototypes, afterward modeling the uncertainty. An uncertainty loss forces the prototypes into learning the rich semantics of subspace to realize accurate uncertainty quantification. Besides, \(\) diversity loss is introduced to keep prototypes diverse. \(\) means cosine similarity.

event "the data is ambiguous", which is negatively related to the degree of data ambiguity. Thus, the \(u=1-=_{k=1}^{K}b_{k}\) is designed to express the aleatoric uncertainty defined in Section 1. In brief, the data with lower \(\) is more ambiguous, which should be a high aleatoric uncertainty score as higher \(u\). A belief mass \(b_{k}\) for the \(k^{th}\) semantic category is computed using the evidence for the category. We term the _evidence_ as the amount of data supports for being classified into a certain category. Let \(=[e_{1},,e_{k}]\) be an evidence vector derived from similarity vector \(=[p_{1},,p_{k}]\) between an instance and \(K\) prototypes (evidence generation shown in Appendix. B), then the belief \(b_{k}\) and the uncertainty \(u\) are signified as:

\[b_{k}=}{S}=-1}{S} u=1-= _{k=1}^{K}b_{k}=1-, \]

where \(_{k}=e_{k}+1\) denote the Dirichlet parameters associated with a belief mass assignment in SL, and \(S=_{i=1}^{K}_{k}\) is referred to as the Dirichlet strength. \(e_{k}\) is the evidence of a visual (or textual) instance matching the \(k^{th}\) semantics category. Intuitively, massive evidence indicates considerably ambiguous data, resulting in high uncertainty, which brings out fallible predictions.

**Uncertainty learning.** After uncertainty modeling, how to force the model to learn uncertainty from data is worth considering (see Figure 2). The key is enriching prototypes' meaningful semantics to represent overall subspace rigorously. We first give a normalized initialization proposed by Xavier  to all prototypes. Subsequently, the cosine similarity matrix \(^{N N}\) between \(N\) visual and textual instances is calculated by Eq. 2 within a batch. Furthermore, \(N\) instances and \(K\) prototypes from different modalities build a set of similarity vectors \(^{N K}\) to derive the evidence vector set and compute the uncertainty using Eq. 4. The data uncertainty sets of \(N\) visual and textual instances can be symbolized as \(_{v}=[u^{v}_{1},,u^{v}_{N}]\) and \(_{t}=[u^{t}_{1},,u^{t}_{N}]\) in parallel.

Next, an objective function is required to force prototypes to learn rich semantic knowledge. As the prototypes are expected to represent the overall semantics of each modality, the instance with high overall similarity to these prototypes should also have high overall similarity with other instances from another modality. According to Eq. 4, the aleatoric uncertainty \(u\), denoted as the sum of belief masses, should be positively related to the mean similarities of each modality within a batch. Thereby, we compute the mean of similarity matrix \(\) along the row and the column, symbolized as \(_{v}=[h^{v}_{1},,h^{v}_{N}]\) and \(_{t}=[h^{t}_{1},,h^{t}_{N}]\), respectively. After that, prototypes can be optimized by employing Mean-Squared (MSE) Loss to keep the \(_{v}\) (\(_{t}\)) and \(_{v}\) (\(_{t}\)) consistent, where \(\) is a scaling coefficient to keep the same value range of \(u\) and \(h\) (Eq. 5).

\[^{v}_{uct}&= _{i=1}^{N}(u^{v}_{i}- h^{v}_{i})^{2}\\ ^{t}_{uct}&=_{i=1}^{N}(u^{ t}_{i}- h^{t}_{i})^{2} \]

**Diversity learning.** To employ the DST uncertainty framework, the mutual exclusion among possible states needs to be satisfied. Precisely, prototypes should avoid attending to the same or similar semantics. Therefore, a diversity loss is proposed to help the prototypes focus on diversified independent semantics. The formula is as:

\[^{v}_{div}&= }_{i=1}^{K}_{j=1}^{K}((^{v}_{i},^{v}_{j}))^{2} \\ ^{t}_{div}&=}_{i=1}^{K} _{j=1}^{K}((^{t}_{i},^{t}_{j}))^{2} \]

where \(^{v}_{i}\) and \(^{t}_{i}\) denote \(i^{th}\) prototypes of visual and textual modality. Intuitively, the semantic similarities between different prototypes are expected to be 0.

**Re-ranking.** After training, the uncertainty of each instance could be precisely assessed using Eq. 4. To further alleviate the impact of the uncertainty on prediction, we re-rank the retrieval sequences by weighting the predicted matrix based on the uncertainty vector \(_{v}\) and \(_{t}\) by Eq. 7.

\[^{}=((-_{1}^{T}_{v})(-_ {2}_{t}))^{} \]where \(_{1}\) and \(_{2}\) are learnable parameters used to control the impact degree of re-ranking on prediction. \(}\) and \(}\) separately signify the similarity matrices between all visual and textual instances in test processing before and after re-ranking.

## 4 Experiments

In this section, we first introduce the datasets and metrics used for our experiments, as well as the implementation details. Later, we compare PAU with the previous state of the arts on several tasks and public benchmarks. In the end, rich ablation studies are carried out to verify our method's effectiveness and generalization.

### Datasets and Metrics

We report our experimental results on three public video-text retrieval datasets containing MSR-VTT , MSVD , DiDeMo , and a public image-text retrieval dataset of MS-COCO . Following , Recall@1 (R@1), Recall@5 (R@5), Recall@10 (R@10), Median Rank (MdR), and Mean Rank (MnR) are used to evaluate the performance.

**MSR-VTT** is a commonly used benchmark in video-text retrieval, containing 10,000 Youtube videos with 20 captions for each. Following , we employ 9,000 videos for training and 1,000 selected video-text pairs for testing.

**MSVD** contains 1,970 videos ranging in length from 1 to 62 seconds, with approximately 40 sentences per video. There are 1,200, 100, and 670 videos used for training, validation, and testing in the standard split.

**DiDeMo** consists of 10,611 Flickr videos with approximately 40,000 annotations. We follow [46; 42; 3] and concatenate all captions of the same video into a paragraph, which will be regarded as a single item in a paragraph-to-video retrieval task.

**MS-COCO** is a widely used dataset for image-text retrieval, containing 123,287 images with five captions per image. We follow the split in  with 113,287 images for training, 5,000 for validation, and 5,000 for testing. The results are reported on 1K and 5K test sets.

    &  &  \\  & R@1 & R@5 & R@10 & MdR & MnR & R@1 & R@5 & R@10 & MdR & MnR \\  CLIP4Clip-meanP  & 43.1 & 70.4 & 80.8 & 2.0 & 16.2 & 43.1 & 70.5 & 81.2 & 2.0 & 12.4 \\ CLIP4Clip-seqTransf  & 44.5 & 71.4 & 81.6 & 2.0 & 15.3 & 42.7 & 70.9 & 80.6 & 2.0 & 11.6 \\ CenterCLIP  & 44.2 & 71.6 & 82.1 & 2.0 & 15.1 & 42.8 & 71.7 & 82.2 & 2.0 & 10.9 \\ MILES  & 44.3 & 71.1 & 80.2 & 2.0 & 14.7 & - & - & - & - & - \\ CLIP2Video  & 45.6 & 72.6 & 81.7 & 2.0 & 14.6 & 43.5 & 72.3 & 82.1 & 2.0 & 10.2 \\ CLIP2TV  & 46.1 & 72.5 & **82.9** & 2.0 & 15.2 & 43.9 & 73.0 & 82.8 & 2.0 & 11.1 \\ XPool  & 46.9 & 72.8 & 82.2 & 2.0 & 14.3 & 44.4 & **73.3** & **84.0** & 2.0 & **9.0** \\ 
**Baseline** & 45.6 & **72.9** & 81.5 & 2.0 & 14.5 & 45.2 & 71.6 & 81.5 & 2.0 & 10.9 \\
**ours** & **48.5** & 72.7 & 82.5 & **2.0** & **14.0** & **48.3** & 73.0 & 83.2 & **2.0** & 9.7 \\   

Table 1: Retrieval Performance on MSR-VTT

    &  &  \\  & R@1 & R@5 & R@10 & MdR & MnR & R@1 & R@5 & R@10 & MdR & MnR \\  CLIP4Clip-meanP  & 46.2 & 76.1 & 84.6 & 2.0 & 10.0 & 56.6 & 79.7 & 84.3 & 1.0 & 7.6 \\ CLIP4Clip-seqTransf  & 45.2 & 75.5 & 84.3 & 2.0 & 10.0 & 62.0 & 87.3 & 92.6 & 1.0 & 4.3 \\ CenterCLIP  & 47.3 & 76.9 & **86.0** & 2.0 & 9.7 & 63.5 & 86.4 & 92.6 & 1.0 & 3.8 \\ CLIP2Video  & 47.0 & 76.8 & 85.9 & 2.0 & 9.6 & 58.7 & 85.6 & 91.6 & 1.0 & 4.3 \\ CLIP2TV  & 47.0 & 76.5 & 85.1 & 2.0 & 10.1 & - & - & - & - & - \\ XPool  & 47.2 & 77.4 & 86.0 & 2.0 & **9.3** & 66.4 & 90.0 & 94.2 & 1.0 & 3.3 \\ 
**Baseline** & 46.3 & 76.8 & 84.9 & 2.0 & 10.0 & 60.1 & 82.8 & 87.3 & 1.0 & 7.5 \\
**ours** & **47.3** & **77.4** & 85.5 & **2.0** & 9.6 & **68.9** & **93.1** & **97.1** & **1.0** & **2.4** \\   

Table 2: Retrieval Performance on MSVD

### Implementation Details

We use CLIP's  visual-textual encoder (ViT-B/32) as the backbone and initialize all parameters from pre-trained CLIP weights. The max token and frame length are both set consistently with . Specifically, the dimension of common embedding space is set to 512. In video-text retrieval, the Adam optimizer  is employed with the initial learning rate of 1e-7 for the visual encoder and the textual encoder, as well as the initial learning rate of 1e-4 for other layers. As for image-text retrieval, the AdamW optimizer  updates the parameters of all modules with the initial learning rate of 1e-5. The learning rate decays complying with the cosine schedule strategy  in 5, 3, 20, and 5 epochs trained on MSR-VTT, MSVD, DiDeMo, and MS-COCO. Besides, the number of prototypes \(K\) is set as 8 according to Table 8. All experiments are conducted on 1 to 4 RTX3090.

### Comparison with State of the Arts

**Video-text retrieval.** We compare our model with other state-of-the-art methods  and our baseline (model structure see Appendix. A) on multiple datasets. For fair comparisons, all methods are CLIP-based, symbolizing the most advanced techniques of video-text retrieval. The performance of baseline trained without PAU is also given. Table 1, 2, 3 show the results of video-text retrieval on MSR-VTT, MSVD, and DiDeMo, respectively.

We can observe that PAU brings sizeable improvements on all benchmarks. For text-to-video retrieval (t2v), PAU boosts the R@1 performance of 2.9%, 1.0%, and 1.6% on MSR-VTT, MSVD, and DiDeMo. Moreover, 3.1%, 8.8%, and 1.2% improvements of R@1 are obtained on video-to-text retrieval (v2t). Notably, we find that the performance improvement of v2t on MSVD is more significant than other benchmarks. The reason is that MSVD is a small dataset with few videos (only 1,200 for training) but massive annotations (about 40 annotations per video). Meanwhile, CLIP simply utilizes cross-entropy loss, which only regards the diagonal pairs as the positives. Nonetheless, for a batch with a typical size of 256 , the probability of any two texts coming from separate videos is approximately \(3.49 10^{-13}\) (see Appendix. D), bringing innumerable noises and performance damage for the training on video-to-text retrieval. Thus, the significant improvement indicates the ability of PAU to capture and rectify misleading data to support robust learning.

**Image-text retrieval.** We also test our method on image-text retrieval task. To be specific, CLIP  is utilized as the baseline. We compare our model with several previous state-of-the-art approaches, including PVSE , SGRAF , NAAF , DAA , VSE\(\), PCME , and PCME++ . Table 4 shows the results of image-text retrieval on MS-COCO 1K and MS-COCO 5K, respectively.

    & & &  &  \\  & & & i2t & & & i2t & & & i2t & & i2t & \\ Method & Backbone & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\  PVSE  & & 69.2 & 91.6 & 96.6 & 55.2 & 86.5 & 93.7 & 45.2 & 74.3 & 84.5 & 32.4 & 63.0 & 75.0 \\ SGRAF  & ResNet-101 & 79.6 & 96.2 & 98.5 & 63.2 & 90.7 & 96.1 & 57.8 & - & 91.6 & 41.9 & - & 81.3 \\ NAAF  & & 80.5 & 96.5 & 98.8 & 64.1 & 90.7 & 96.5 & 58.9 & 85.2 & 92.0 & 42.5 & 70.9 & 81.4 \\ DAA  & & 80.2 & 96.4 & 98.8 & 65.0 & 90.7 & 95.8 & 60.0 & 86.4 & 92.4 & 43.5 & 72.3 & 82.5 \\  VSE\(\) & & **82.0**We can observe that the overall performance improvement of image-text retrieval is smaller than that of video-text retrieval. The phenomenon reflects that video information is more complex than image information, inducing immense uncertainty in video-text retrieval. Furthermore, we also validate the robustness of PAU training in noisy datasets. The detailed results can be found in Appendix. C.4.

### Comparison with Probabilistic Model

PCME  is a probabilistic-based approach to represent instance diversity, which can also represent uncertainty. The more diverse the instance, the more uncertain the instance. Thus, an instance's uncertainty and diversity should be positively correlated. To verify the superiority of our SL-based method over than probabilistic-based method, we conduct two experiments, _pearson correlation analysis_ and _uncertain data analysis_.

**Pearson correlation analysis.** The pearson correlation coefficient (**PCC**) \(r_{xy}\) (\(r_{xy}[-1,1]\)) is a metric to estimate the strength of the correlation between variable \(x\) and variable \(y\). Generally, \(r_{xy}>0.5\) means that two variables are strongly positively correlated. As mentioned before, the uncertainty and the diversity of an instance should be positively correlated for PCME. If an instance is diverse, it will have high mean similarity \(h\) with the instances from the other modality. This means that the uncertainty score \(u\) and mean similarity \(h\) should also be positively correlated for an instance. Thus, we compute the pearson correlation coefficient between \(u\) and \(h\) to explore whether PAU correctly estimates uncertainty. The results are shown in Table 5.

In this Table, we compute the PCCs of PCME and PAU on MSCOCO, as well as PCCs of PAU on MSR-VTT. PCC\({}_{V}\) and PCC\({}_{T}\) separately indicate the instance's PCC in vision or textual modality. The low PCCs of PCME mean current probabilistic models have limited abilities to reflect uncertainty and diversity. We argue the reason arises from the strong prior distribution assumption, _i.e._, gaussian or other simple distributions are powerless to fit complex and diverse relationships in high-dimension space. By contrast, PAU obtains high PCCs in all settings, proving our approach indeed leads to proper uncertainty estimations.

**Uncertain data analysis**. To further prove the superiority of SL-based aleatoric uncertainty estimation method than probabilistic-based method, we compare the changes of R@1 after removing top-r instances with highest uncertainty scores predicted by PCME and PAU, respectively. To fairly compare, we employ removal on both predictions arised from PAU and PCME. The results with removed ratio from 0% to 20% on MS-COCO 5K is shown in Figure 3. We can observe that PAU outperforms PCME on all directions and predictions. This means that the uncertain data found by PAU is more precise than PCME. All experiments indicate the SL-based approach (PAU) works better than probabilistic-based method (PCME) on representing aleatoric uncertainty.

   Method & PCC\({}_{V}\) & PCC\({}_{T}\) & Dataset \\  PCME & -0.071 & -0.219 & COCO \\ PAU & 0.886 & 0.786 & COCO \\ PAU & 0.917 & 0.939 & MSR-VTT \\   

Table 5: PCCs between \(u\) and \(h\)

Figure 3: **The performance changes comparison after removing top-r instances with the highest uncertainty scores quantified by PCME and PAU on MS-COCO.** To fairly compare, we employ the removal on both predictions arising from CLIP and PCME. (a) and (b) show the performance changes on CLIP predictions. (c) and (d) show the performance changes on PCME predictions. In i2t, text instances are removed. In t2i, image instances are removed.

### Ablation Studies

**The effect of different components.** To understand the effect of each component, we exhaustively ablate three main components of PAU, including Uncertainty Loss, Diversity Loss, and Re-ranking. As shown in Table 6, all three components effectively improve the model performance. The comparison between No. 1 and No. 3 shows that uncertainty loss gives significant R@1 improvements of 1.4% on t2v and 2.2% on v2t. The experiment No. 5 outperforms the experiment No. 3 by a clear margin, demonstrating the necessity of diversity loss. Furthermore, the experiment No. 2 re-ranks the baseline prediction using uncertainty vectors from PAU. It's worth noting that R@1 performance still boosts on both t2v and v2t, reflecting the strong generalization of PAU.

**The impact of uncertain data.** To demonstrate the uncertain data found by PAU indeed affect the retrieval, a trial of the retrieval subsets with varying reliability levels is carried out on MSR-VTT. Specifically, the samples are first ranked according to the uncertainty calculated by PAU in descending order. Then, \(r\) samples with the highest uncertainty will be removed from the test set to produce an uncertainty-based removal subset. Meanwhile, \(r\) random samples will be removed from the test set to produce a random-based removal subset. The baseline model is tested on both subsets above and reports the results in Figure 4.

It's worth noting that the model tested on the subsets removing uncertain data always outperforms the ones removing randomly. Furthermore, the performance gap reaches a peak of 14.2 when \(r=600\)

   No & w/ \(_{uct}\) & w/ \(_{div}\) &  &  &  &  &  &  &  \\ 
1 & & & & & 45.6 & 72.9 & 81.5 & 45.2 & 71.6 & 81.5 \\
2 & & & & ✓ & 45.9 & 72.8 & 81.5 & 46.0 & 72.4 & 81.7 \\ 
3 & ✓ & & & & 47.0 & 72.4 & 82.8 & 47.4 & 71.8 & 82.2 \\
4 & ✓ & & & ✓ & 47.8 & 72.7 & 82.5 & 47.9 & 72.2 & 83.0 \\ 
5 & ✓ & ✓ & & 48.4 & 72.4 & 82.2 & 47.7 & 72.6 & 82.6 \\
6 & ✓ & ✓ & ✓ & 48.5 & 72.7 & 82.5 & 48.3 & 73.0 & 83.2 \\   

Table 6: Ablation study to investigate the effect of different components on MSR-VTT

Figure 4: **The performance comparison against data removal number on MSR-VTT.**

Figure 5: **The performance comparison against uncertain data number on MSR-VTT.**for text-to-video retrieval and a peak of 12.3 when \(r=700\) for video-to-text retrieval, respectively. The above phenomena prove PAU can indeed find uncertain data, which easily misleading the model into incorrect predictions, resulting in extremely unreliable retrieval results.

**Is the prediction more trustworthy?** To validate the effectiveness of PAU in providing trustworthy predictions, the performances of PAU and baseline are compared in the MSR-VTT's subsets under a series of different uncertain degrees. Precisely, \(r\) samples with the highest uncertainty are selected to form the uncertain test subsets. As shown in Figure 5, PAU consistently outperforms the baseline with the largest gap of 8.6 (5.8) while \(r=70\) (\(r=120\)) for t2v (v2t). The smaller the \(r\) is, the higher the total uncertainty of the subset is, so the results confirm the ability of PAU to decrease the performance loss caused by uncertain data. Furthermore, PAU can provide trustworthy predictions even in high uncertainty sets.

**Visualization of data uncertainty.** Three captions and three videos with their uncertainty scores are shown in Figure 6. Obviously, the caption with high uncertainty is more semantically ambiguous, such as "A person is explaining something". By contrast, the caption with low uncertainty is more detailed, such as "Multi colored horses in a barn and outside in the snow". Moreover, the first video (complete video shown in Appendix. F) is more difficult to summarize due to the varied scenes, inducing high uncertainty. By contrast, the third video with low uncertainty is relatively simple.

## 5 Conclusion

In this work, we clearly define the inherent aleatoric uncertainty in multi-modal data. A Prototype-based Aleatoric Uncertainty Quantification (PAU) framework is designed to excavate the uncertain data and further provide trustworthy predictions. Uncertainty loss and diversity loss are proposed to support the uncertainty training by encouraging the learnable prototypes to represent the semantic subspace of varied modalities. Plemitful empirical experiments on a range of tasks and public benchmarks demonstrate the effectiveness and generalization of PAU.

**Broader impacts statement.** Cross-modal matching has become a common paradigm in large--scale multi-modal pretraining, such as CLIP . However, the enormous data demand brings a huge challenge to pre-trained models. If we can accurately select high-quality data, which typically make greater contributions to the performance of pre-trained models, the pretraining process would become much more efficient. This work could be one of the first work to be aware of the aleatoric uncertainty in cross-modal retrieval. By quantifying uncertainty, higher quality data and more reliable results are provided, which could perhaps be extended to other multi-modal tasks in the future [2; 44; 63].