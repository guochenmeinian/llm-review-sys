# Cost-efficient Knowledge-based Question Answering

with Large Language Models

Junnan Dong\({}^{1}\), Qinggang Zhang\({}^{1}\), Chuang Zhou\({}^{1}\), Hao Chen\({}^{1}\), Daochen Zha\({}^{2}\), Xiao Huang\({}^{1}\)

\({}^{1}\) The Hong Kong Polytechnic University

\({}^{2}\) Rice University

{hanson.dong, qinggangg.zhang, chuang-qqzj.zhou}@connect.polyu.hk,

sundaychenhao@gmail.com,daochen.zha@rice.edu, xiaohuang@polyu.edu.hk

###### Abstract

Knowledge-based question answering (KBQA) is widely used in many scenarios that necessitate domain knowledge. Large language models (LLMs) bring opportunities to KBQA, while their costs are significantly higher and absence of domain-specific knowledge during pre-training. We are motivated to combine LLMs and prior small models on knowledge graphs (KGMs) for both inferential accuracy and cost saving. However, it remains challenging since accuracy and cost are not readily combined in the optimization as two distinct metrics. It is also laborious for model selection since different models excel in diverse knowledge. To this end, we propose Coke, a novel cost-efficient strategy for KBQA with LLMs, modeled as a tailored multi-armed bandit problem to minimize calls to LLMs within limited budgets. We first formulate the _accuracy expectation_ with a cluster-level Thompson Sampling for either KGMs or LLMs. A context-aware policy is optimized to further distinguish the expert model subject to the question semantics. The overall decision is bounded by the _cost regret_ according to historical expenditure on failures. Extensive experiments showcase the superior performance of Coke, which moves the Pareto frontier with up to 20.89% saving of GPT-4 fees while achieving a 2.74% higher accuracy on the benchmark datasets.

+
Footnote â€ : \(\) Corresponding Author.

## 1 Introduction

Knowledge-based question answering (KBQA) has gained significant attention across various specialized domains, e.g., education and medicine . Given a question, the model is required to make inferences based on necessary reasoning background . Inspired by the effectiveness of knowledge graphs (KGs), e.g., ConceptNet , where real-world entities are represented in the structural form as (_head entity_, _relation_, _tail entity_) , KG-based models (KGMs) have been proposed to leverage KGs for reasoning. Based on the hypothesis that answers could be located multiple hops away from the question concepts  in KGs, former studies are mainly dedicated to tracing the trajectory from questions to answers to model the structural information , or utilizing graph neural networks (GNNs) to learn the question-specific subgraph from KGs .

With the emergence of large language models (LLMs), e.g., ChatGPT and GPT-4 , They have shown remarkable performance benefited from the injected knowledge during pre-training . However, it is challenging to adopt LLMs in practice. First, either calling the API or deploying the open-source LLMs with cloud service is prohibitive . GPT-4 is estimated to cost at least thousands of dollars for pilot-scale customer service  while Llama3 70B requires unaffordablecomputation resources for small business, e.g., 40G graphics memory, let alone the high throughput scenarios like online e-commerce . Second, LLMs may struggle to identify the correct answer for certain questions due to the lack of particular knowledge that is not covered in their pre-training corpus . They are considered unreliable to assist pedagogical or medical purposes since they could generate hallucination  and misleading responses .

We illustrate the sketched overview of LLMs and KGMs methods in Figure 1 (a) where LLMs are used in a zero-shot setting with direct prompts, e.g., [Question:Choices], and KGMs require the external domain KG as reasoning background. In Figure 1 (b), we visualize the accuracy/parameter size of four types of models. In general, KGMs are far more lightweight considering model size but tend to underperform LLMs overall. Conversely, LLMs are more computationally expensive and may struggle with specific questions that demand knowledge not covered in the pre-training corpus. We are thereby motivated to combine the strengths of LLMs and KGMs through question-specific model selection to improve the Pareto frontier, achieving higher inferential accuracy and lower costs.

However, it is nontrivial for two major challenges. First, inferential accuracy and cost saving are two distinct metrics. It is hard to combine them in the optimization simultaneously. Since more parameters will lead to higher costs, it indicates the importance of a careful selection that balances both exploration and exploitation. Second, selecting the most suitable model for particular questions is laborious. In Figure 1 (c), we showcase the overlaps among several representative models on the benchmark OpenBookQA  dataset. While different models focus on various considerations, they may consequently excel in diverse knowledge and question types. For example, HamQA  focuses on hypernymy in the questions, i.e., cats and mammals. This makes it expensive to incorporate expertise according to the specialty of models to answer diverse real-world domain-specific questions.

To this end, we present a novel cost-efficient strategy to leverage LLMs for KBQA, i.e., Coke. It is modeled as a tailored multi-armed bandit (MAB) problem, which is trained to assign the most promising model for each question within a considerably limited budget. Specifically, we assemble two sets of base models, i.e., LLMs and KGMs to balance both inferential accuracy and cost saving. \((i)\) we first model the preliminary selection with a cluster-level Thompson Sampling. It suggests the _accuracy expectation_ of choosing either LLMs or KGMs based on historical success and failure at Beta distribution. \((ii)\) A context-aware policy is learned to further distinguish the most suitable model. This could effectively assign the corresponding expert for the given question semantics. \((iii)\) The overall decision making is bounded by the _cost regret_. It indicatively constrains the selection based on the cumulative expenses incurred from failures of the current model.

**Contributions**.

* We formally define the task of optimizing both inferential accuracy and cost for KBQA with LLMs.
* A novel cost-efficient strategy, Coke, is presented to automatically assign the most promising model for particular questions. It could effectively make reliable decisions considering both _inferential accuracy_ and _cost saving_, making a balance of _exploration_ and _exploitation_ during selections.
* Extensive experiments over three domain-specific benchmark datasets demonstrate the superiority of our proposed framework, moving the Pareto frontier to achieve higher accuracy and lower costs.

Figure 1: A sketched overview of LLMs and small KGMs in (a) We visualize the Acc./Param size of both pipelines of models in (b) The overlaps among different model predictions are shown in (c).

Problem Formulation

### Task Definition

We adopt lowercase alphabets for scalars (e.g., \(m\)), boldface lowercase letters for vectors(e.g., \(\)) and boldface uppercase ones for matrices (e.g., \(\)). The decorated letters are used for sets, e.g., \(\). We assemble two clusters of models, i.e., \(=\{c_{L},c_{K}\}\) for sets of LLMs and KGMs, respectively. All the candidate models in \(\) are denoted as \(=\{m_{1},m_{2}..m_{n}\}\). The knowledge graph used in KGMs is expressed as \(\) where real-world entities \(e\) are represented in the form of triples as \((e_{h},r,e_{t})\).

Given domain-specific questions \(Q=\{q_{1},q_{2}..q_{u}\}\), and several candidate models \(\), we aim to optimize the framework to identify the most promising model within a limited budget \(\) to invoke LLMs. The overall performance is evaluated by both inferential accuracy and cost saving. We aim to maximize the accuracy comparing prediction \(_{i}\) and ground truth \(y_{i}\), i.e., \(max(f_{acc}(_{i}|y_{i}))\), and minimize the costs, i.e., \(min(f_{cost}(p(m)|q_{i}))\) where \(p(m)\) is the unit cost of model \(m\).

### Performance Evaluation

For fair comparisons, we divide the overall evaluation of Coke against state-of-the-art baselines into two parts considering both _inference accuracy_ and _cost saving_.

**Inferential accuracy:** The performance of KBQA task itself is evaluated by the overall accuracy of the model prediction compared with ground truths, i.e., \(\{0,\,1\}\) for each question indicates wrong/correct. The accuracy is expected to be as high as possible to correctly answer more questions.

**Cost Saving:** The cost of using particular models is often case by case. It could involve many aspects, e.g., power consumption, GPU and graphics memory costs, cloud service charges and token-level API expenses, etc. In this paper, we instantiate this metric for Coke in two ways; one can define the cost in other ways under our framework. \((i)\)_APIfees_ (\(\$\)). This intuitively indicates the cost of money particularly for comparing with a series of LLMs from OpenAI, e.g., GPT3.5 and GPT4. \((ii)\)_Calls_ (times). We generalize the evaluation to open-source LLMs like Llama and ChatGLM. It indicates the number of times that we invoke the LLMs. Since KGMs are considered far more cheaper for local and cloud implementation, the metric of calls is also expected to be as few as possible.

## 3 Approach: Coke

To achieve an effective model selection, we mainly aim to answer two research questions: \((i)\)_How can we balance the exploration and exploitation to find the best model for both accuracy and cost saving?_ Real-world questions are difficult to manually identify to apply LLMs or KGMs. We usually value the prior knowledge to select the most accurate and inexpensive model at present greedily, i.e., _exploitation_. However, we also wish to obtain sufficient posterior experiences from under-explored models so far, i.e., _exploration_ to find more models with a superior performance-to-price ratio. \((ii)\)_How can we automatically distinguish the most promising expert models for different questions?_ The particular types of required knowledge vary among questions. This suggests a careful selection to leverage the specialized expertise from different models to make correct inferences. To this end, we design a novel cost-efficient framework, Coke, to automatically select question-specific models.

We formulate the automatic model selection as a multi-armed bandit (MAB) problem. Specifically, in each iteration \(k\), our policy is presented with the choice of selecting one model \(m\) out of \(\) candidate models, referred to as \(\)_arms_. Let \(r^{k}_{m}\{0,1\}\) denote the reward obtained by selecting the model \(m\) at the iteration \(k\), where \(m\). For each given question \(q\), if the chosen model \(m\) can correctly answer it, the policy will receive a positive reward of 1, and 0 vice versa. We aim to maximize the cumulative rewards in \(\) iterations and formulate the objective as follows:

\[_{k=1}^{}r^{k}_{m}.\] (1)

In each iteration, the selection is inherently associated with an expectation function \(()\) to capture the implicit linear correlation between the given question \(q\) and the potential reward \(r\), indicating the likelihood of success by choosing one particular model \(m\).

### Accuracy-Cost Trade-off for KBQA with LLMs

To answer the two aforementioned questions, we decompose the expectation \((r_{m}^{k}|q)\) into three aspects. First, we calculate the _accuracy expectation_ of one cluster, i.e., LLMs or KGMs, for better inferential performance(*). Second, we design a context-aware expert distinguishing to present the question embedding to the policy and find the best arm with implicit expertise (**), which further increases the probability of achieving higher accuracy for different question contexts. Finally, we constrain the decision-making with a _cost regret_. This is introduced to punish the model which wastes more on failures (***). The overall expectation is correspondingly formulated as follows.

\[(r_{m}^{k}|q_{k})=_{c}(r_{c}|_{c})}_{(* )}+_{a}(r_{a}|q_{k})}_{(**)}-_{a}(,p(a),q_{i})}_{(***)},\] (2)

where \(r_{m}^{k}\{0,1\}\), \(r_{c}\) and \(r_{a}\) are the decomposed rewards for cluster sampling in terms of accuracy and arm selection considering knowledge expertise for particular questions. \(\) indicates the limited budget allocated to invoke LLMs. Details are described hereunder.

### Accuracy-Encouraged Cluster Expectation (*)

To encourage the policy to select more promising models, we first establish a higher-level expectation concerning the overall accuracy performance of KGMs and LLMs. Inspired by the traditional Thompson Sampling, which iteratively samples the best arm based on historical information, i.e., success and failure, we thereby design a tailored cluster-level Thompson Sampling to evaluate the expectation of choosing one particular cluster \(c\). It presents a dynamic approach where the selections evolve over iterations based on success and failures, gradually converging towards optimal selections as more questions are encountered. This could also effectively embody the _exploration-exploitation_ trade-off inherent in our model selection scenarios for the sake of accuracy.

Specifically, it is established based on the prior knowledge as _(success, failure)_ of each cluster, instantiated by a Beta distribution of \(Beta(,)\). We define this pair of conjugate prior below.

**Definition: conjugate prior of \(\) and \(\).** Given a cluster \(c\{c_{L},c_{K}\}\), let \(_{c}\) and \(_{c}\) denote the success and failure prior respectively for \(c\). The pair of \((_{c},_{c})\) forms a conjugate prior. It facilitates the efficient update of fundamental beliefs about the performance of each cluster \(c\) during selections.

In general, we consider our cluster-level accuracy expectation \(_{c}\) as a likelihood of success for each cluster by randomly sampling an indicator \(_{c}^{k}\) in iteration \(k\) from the distribution of \(Beta(_{c}^{k-1},_{c}^{k-1})\) based on the observation of success and failure in the former \((k-1)\) rounds.

\[_{c}(r_{c}^{k}|_{c})_{c}(_{c}^{k-1}, _{c}^{k-1}),\] (3)

where \(_{c}\) is distributed approximately uniformly across the entire interval, resulting in a uniform _exploration_ when a particular cluster has not been extensively sampled. Otherwise, if cluster \(c\) has been sufficiently selected and the performance turns out to be satisfying with more success times, i.e., larger \(_{c}\), the corresponding expectation associated with \(_{c}\) will be more likely to be higher to facilitate a reliable _exploitation_. When \(k=0\), we value the prior knowledge of cluster expectation before any observations have been made. In our paper, we utilize the average reported performance of each arm within the cluster as the prior for \(_{c}\). This ensures an empirically grounded initial belief about the success probability of the cluster, as well as for subsequent Bayesian inference.

\[prior(_{c}^{k})(_{c}^{k-1}+_{c}^{k-1})} {(_{c}^{k-1})(_{c}^{k-1})}_{ c}^{_{c}-1}(1-_{c})^{_{c}^{k-1}-1},\] (4)

where \(()\) is the Gamma function. Based on this, we consider the cluster with the largest \(_{c}^{k}\) in iteration \(k\) as the best cluster \(c^{*}\) for current question \(q\), where the arm models within this particular cluster will have higher chances of answering this question. A reward \(r_{c}^{k}\{1,0\}\) will then be given if \(c^{*}\) can/cannot make the correct prediction. Correspondingly, the posterior distributions for all the historically selected clusters will be updated when \(0<k\). The history of cluster sampling and the posterior updating is formulated as follows, respectively.

\[&_{c}^{k-1}=\{c_{n}^{},_{c}^{k-1}, _{c}^{k-1},_{c}^{k-1},=1,2...k-1,n=1,2...N\}\\ & posterior(_{c}^{k})(_{c}^{k-1 }+_{c}^{k-1}+1)}{(_{c}^{k-1}+r_{c}^{k-1})(_{c}^{k-1}+1-r_{c}^{k-1})}(_{c}^{k})^{_{c}^{k-1 }+r_{c}^{k-1}-1}(1-_{c}^{k})^{_{c}^{k-1}-r_{c}^{k-1}}.\] (5)While the exact sequence of cluster selections may vary between runs, the overall behavior will eventually converge to optimal cluster selections over time in our modeled MAB problem, especially as more questions are presented and more historical success/failure information is observed.

**Definition: Selection Regret.**_In iteration \(k\), we denote the real-selected arm as \(a_{k}\), the annotated best arm as \(a_{*}^{*}\), which can answer the question with the lowest costs. We refer to the expectation differences between \(a_{k}\) and \(a_{k}^{*}\) as the selection regret for current iteration \(k\)._

Specifically, we give the overall selection regret bounds for the cluster-level Thompson Sampling as follows. Given the selected arm \(a_{k}\) and the historical information \(H_{k}\) up to iteration \(k\), \(_{c}\) is bounded as follows: (The detailed proof of confidence bound is provided in the **Appendix** Section A)

\[SR() 2+2_{k=1}^{}[r(a_{k},H_{k-1})].\] (6)

In general, the bounds are obtained from the following derivations. Initially, we establish the upper and lower confidence bounds as explicit functions of the arm \(a_{k}\) and history \(H_{k-1}\) respectively, denoted as \(U(a_{k},H_{k-1})\) and \(L(a_{k},H_{k-1})\). For some \(>0\) and the number of arms \(\), the specific form of functions \(U\) and \(L\) is irrelevant as long as they satisfy the following properties:

\[ a,\,k,& U(a,H_{k-1})-(a)^{-} },\\  a,\,k,&(a)- L(a,H_{k-1})^{-}}. \] (7)

### Context-Aware Expert Distinguishing (**)

To answer the second question, we further deepen our MAB problem as a contextual variant. In this part, the expectation is highly related to the question semantics. We aim to automatically learn from the vector representation of questions, e.g., \(^{d}\), \(d\) for dimension, and effectively identify the corresponding expert model to answer it. The embedding is uniformly obtained by applying a lightweight pre-trained language model RoberTa . To this end, we design the expectation function \(_{a}(r_{a}^{k}|q^{k})\) to effectively capture the linear correlation between \(\) and \(r_{a}\) in iteration \(k\).

\[_{a}(r_{a}^{k}|q^{k})=^{k}_{a}^{k-1} +_{a}^{k-1},\] (8)

where \(_{a}^{k-1}^{1 d}\) is a learned vectored parameter associated with each arm \(a\) in \(k-1\) steps. We introduce \(_{a}^{k-1}\) as a trainable noise at Gaussian distribution \(}(0,(^{(n)})^{2})\) to balance the _exploration_ and _exploitation_. We maximize \(^{k}_{a}^{k-1}\) to encourage the exploitation . The tight correlations are established among the given question, the history information, i.e., \(_{a}^{k-1}\), including all the questions \(_{a}^{k-1}^{(k-1) d}\) answered by arm \(a\) and the rewards received \(_{a}^{k-1}^{1(k-1)}\) in k-1 iterations. We could observe the history \(\) for each arm for abundant information for reference as:

\[_{a}^{k-1}=\{a_{n}^{},_{a}^{k-1},_{a}^{k-1},=1,2...k-1,n=1,2...N\}\] (9)

Specifically, we update \(_{a}^{k}\) based on the \(_{a}^{k-1}\) with a typical ridge regression \(f()\).

\[ f(_{a}^{k-1},_{a}^{k-1})=_ {k=1}^{K}(_{a}^{k-1}-_{a}^{k-1}_{a}^{k})^ {2}+_{a}^{k}_{2}^{2}\\  f(_{a}^{k})=(_{a}^{k-1}-_{a}^ {k-1}(_{a}^{k})^{}(_{a}^{k-1}-_{a}^{k-1 }_{a}^{k})+^{b}(_{a}^{k})^{} _{a}^{k},\] (10)

where we introduce the L2 normalization to ensure the reversibility of \(_{a}^{k-1}\) in addition to the original ordinary least square loss. \(\) solves the over-fitting problem by adopting a suitable \(\). To find the optimal value of \(_{a}^{k}\) that minimizes the cost function, we differentiate the formula with respect to \(_{a}^{k}\), set the derivative equal to zero, and solve it. This yields the following update equation:

\[_{a}^{k}=(_{a}^{k-1})^{}_{a}^{k -1}+_{a}^{-1}\,(_{a}^{k-1})^{}\, _{a}^{k-1},\] (11)

where \(^{d d}\) is an identity matrix. To facilitate exploration on less explored arms, we adopt an upper confidence bound for exploration-exploitation trade-off by introducing \(_{a}^{k-1}\). For any \(>0\) with the probability at least \((1-)\), the expectation \(_{a}(r_{a}^{k}|^{k})\) is bounded by a confidence interval:

\[^{k}a^{k-1}- f(_{a}^{k-1 })_{a}^{k}a^{k-1}+ f (_{a}^{k-1}),\] (12)where \(\) is a constant value, i.e., \(=1+\). Also, we can derive the correlation term \(f(a^{k-1})\) as \(f(_{a}^{k-1})^{k}((_{a }^{k-1})^{}_{a}^{k-1}+)^{-1}(^{k })^{}}\). Hence, through learning from the current question \(^{k}^{d}\) and all historically assigned questions \(_{a}^{k-1}^{(k-1) d}\) for arm \(a\), we can simply derive the equation to appropriately update the noise term \(_{a}^{k-1}\) for next iteration.

\[_{a}^{k}= ^{k}((_{a}^{k-1})^{ }_{a}^{k-1}+_{a})^{-1}(^{k})^{ }}.\] (13)

When an arm \(a\) with larger \(^{k}_{a}^{k-1}\) is selected, this reflects an _exploitation_ process. Similarly, when the model chooses an arm with larger \(_{a}^{k-1}\) learned in previous iterations, this variance shows an _exploration_ process since the model performs few selections of the current arm. Thus, jointly maximizing \((^{k}_{a}^{k-1}+_{a}^{k-1})\) helps us for more promising expert models subject to question \(q^{k}\).

In conclusion, guided by the objective of maximizing cumulative rewards \(r_{m}^{k}\), we concentrate on the sub-target of finding contextually best arm as the expert to correctly answer the question by prioritizing the arm with higher expectation \(_{a}(r_{a}^{k}|q^{k})\) to obtain \(r_{a}^{k}\).

### Cost Regret Constraint (***)

Since the budget is limited, we aim to make the best use of the chances for both accuracy improvement and cost savings. In this part, we introduce a penalty term _cost regret_, denoted as \(_{a}\), to measure the proportion of costs incurred by incorrect predictions given by the arm \(a\) within a budget constraint.

\[&_{a}=^{}}p(a_ {k})\|q_{a}^{}\|}{_{q\{Q_{a}^{} Q_{a}^{}\}}p(a_{k}) \|q_{a}\|}\\ & s.t._{q\{Q_{a}^{}-1\}}p(a_{k})\|q_{a}\|+p(a_{k})\|Q _{a}^{k}\|; a.\] (14)

where \(q_{a}^{}\) and \(Q_{a}^{}\) indicate the failure, i.e., the question and historically assigned questions for arm \(a\) that are wrongly answered. \(p(a)\) is the unit cost for invoking the LLMs. For black-box LLMs, we calculate \(p(a)\|q_{a}\) as the token-level fees; while for open-sourced LLMs, it will be replaced by the times that we call LLMs. The numerator of \(_{a}\) represents the total cost incurred by incorrect answers given by arm \(a\), while the denominator calculates the total cost of all questions answered by arm \(a\). The constraint could effectively ensure that the total cost of selecting arm \(a\) for answering questions in the previous \(k-1\) iterations and the current iteration \(k\) does not exceed a predefined budget \(\). To control the impacts of over-constraint from _cost regret_ which may penalize the model for the costs on necessary trials, we introduce \(\) as a hyperparameter to control the trade-off between maximizing rewards and minimizing cost regret for a reasonable _exploration_ and _exploitation_.

## 4 Experiments

We conduct experiments on three domain-specific datasets: \((i)\) Commonsense knowledge domain: CommonsenseQA ; \((ii)\) Scientific Openbook domain: OpenbookQA ; \((iii)\) Medical Domain: MedQA-USMLE . To compare the performance of Coke, we include the baselines from three aspects, i.e., fine-tuned Language Models, KGMs and both API series and local series of LLMs. Additionally, our framework is efficiently runnable on one CPU. To accelerate the matrix computation, we adopt Torch to boost the selection on an NVIDIA GeForce RTX 4090 GPU.

### Datasets

**CommonsenseQA** (abbreviated as _CSQA_) is a prominent dataset in the commonsense knowledge domain that demands extensive real-world commonsense knowledge. It encompasses 12,102 questions, and ConceptNet , one of the largest commonsense knowledge graphs (KG), is frequently employed by existing KGMs for reasoning. Due to the official test set being reserved for leaderboard evaluations, we assess model performance using the in-house (IH) data split as implemented in . **OpenBookQA** (referred to as _OBQA_) is a scientific domain dataset that comprises 5,957 multiple-choice questions from open book exams, each with four options. Answering _OBQA_ questions necessitates a comprehensive understanding of fundamental science facts and their applications, which involves understanding scientific principles and applying them to novel situations.

**MedQA-USMLE**, i.e., _MedQA_, serves as a domain-specific question-answering benchmark focused on medical knowledge. This dataset is derived from the United States Medical Licensing Examination, which is a comprehensive and challenging assessment used to evaluate the competence of prospective medical professionals. MedQA includes a variety of question types that test clinical knowledge, diagnostic reasoning, and medical science applications. The dataset benchmarks the performance in understanding and applying medical knowledge in both healthcare and medicine.

### Baselines

**Fine-tuned Language Models** abbreviated as LMs. We evaluate our method against standard fine-tuned language models, specifically using Bert-base, Bert-large , and RoBerta-large .

**KGMs** We include off-the-shelf small KGMs that integrate KGs for KBQA, including MHGRN , QA-GNN , HamQA , JointLK , GreaseLM , and GrapeQA .

**LLMs** Our comparison includes two categories of LLMs: the API series (GPT-3, GPT-3.5, and GPT-4) and local series (ChatGLM, Baichuan-7B, Llama2 (7b), and Llama3 (8b).

### Main Results

The overall performance is compared and shown in Table 1. To compare with API series of LLMs, we assemble HamQA, GPT3.5 and GPT-4 as the arms and our proposed Coke has outperformed all four categories of baselines in terms of inferential accuracy. Specifically, we could achieve 2.03%, 0.67% and 1.03% improvements over GPT-4 on three datasets. For the local series of LLMs, we

    &  &  &  \\   & IIHdev-Acc. & IIHtest-Acc. & Dev-Acc. & Test-Acc. & Dev-Acc. & Test-Acc. \\   \\  Bert-base  & 0.573 & 0.535 & 0.588 & 0.566 & 0.359 & 0.344 \\ Bert-large  & 0.611 & 0.554 & 0.626 & 0.602 & 0.373 & 0.367 \\ RoBerta-large  & 0.731 & 0.687 & 0.668 & 0.648 & 0.369 & 0.361 \\   \\  MHGRN  & 0.745 & 0.713 & 0.786 & 0.806 & - & - \\ QA-GNN  & 0.765 & 0.733 & 0.836 & 0.828 & 0.394 & 0.381 \\ HamQA  & 0.769 & 0.739 & 0.858 & 0.846 & 0.396 & 0.385 \\ JointLK  & 0.777 & 0.744 & 0.864 & 0.856 & 0.411 & 0.403 \\ GreaseLM  & **0.785** & 0.742 & 0.857 & 0.848 & 0.400 & 0.385 \\ GrapeQA  & 0.782 & 0.749 & 0.849 & 0.824 & 0.401 & 0.395 \\    \\  ChatGLM & 0.473 & 0.469 & 0.352 & 0.360 & 0.346 & 0.366 \\ Baichuan-7B & 0.491 & 0.476 & 0.411 & 0.395 & 0.334 & 0.319 \\ Llama2 (7b) & 0.561 & 0.547 & 0.526 & 0.466 & 0.302 & 0.299 \\ Llama3 (8b) & 0.754 & 0.720 & 0.762 & 0.756 & 0.622 & 0.691 \\   \\  Acc. Imp.\% vs. Llama3 (8b) & - & - & - & - & **0.627** & **0.692** \\ Cost Sav. (calls) \% vs. Llama3 (8b) & - & - & - & - & **+ 0.81\%** & **+ 0.16\%** \\    \\  GPT3 & 0.539 & 0.520 & 0.420 & 0.482 & 0.312 & 0.289 \\ GPT3.5 & 0.735 & 0.710 & 0.598 & 0.600 & 0.484 & 0.487 \\ GPT-4 & 0.782 & 0.802 & 0.898 & 0.902 & 0.739 & 0.770 \\   \\   \\  Acc. Imp.\% vs. GPT-4 & **+ 2.56\%** & **+ 2.74\%** & **+ 1.12\%** & **+ 0.67\%** & **+ 0.95\%** & **+ 1.03\%** \\ Cost Sav. (\$) \% vs. GPT-4 & **- 15.14\%** & **- 20.89\%** & **- 5.33\%** & **- 11.02\%** & **- 2.11\%** & **- 4.32\%** \\   

Table 1: Performance comparison among state-of-the-art baselines and Coke on three benchmark datasets in terms of both inferential accuracy and cost saving ($ API fees).

adopt HamQA, Llama2 (7b) and Llama3 (8b). Since Llama2 (7b) and Llama3 (8b) underperform the traditional KGMs with lagging performance on both CSQA and OBQA, we merely conduct the experiments on MedQA with the arm models, i.e., HamQA, Llama2 (7B) and Llama3 (8B). We conclude our observations hereunder. The performance gap among different arms plays a vital role in balancing the accuracy and costs. For example, on CSQA and OBQA, the accuracy of state-of-the-art KGMs are very close to GPT-4 and much better that both GPT 3.5 and local series of LLMs. This facilitates a big improvements on cost saving by invoking more KGMs, where we achieve higher accuracy with much lower costs, i.e., 20.89% and 11.02%. However, on MedQA, where the questions are over-complicated and open-ended for KGMs to infer, there exists a huge performance gap between the best KGM and all the LLMs, especially when compared with GPT-4. Our policy has to rely on sufficient calls of LLMs to ensure accuracy, which indeed increases the costs.

### Hyperparameter Analysis

In this subsection, we conduct a detailed analysis of the important hyperparameters, i.e., \(\) and \(\).

#### 4.4.1 Budget Study

The Pareto frontier for both inferential accuracy and cost saving is shown in Figure 2. For clear illustration, we replace the accuracy with the error rate which reversely shows the direction of performance changes. To observe when the accuracy and cost reach the balance, we decrease the budget from 1 to 0.5 until \(\) has a higher error rate than GPT-4 \(\){0.5,0.6,0.7...,1}. In this figure, nodes in the lower left positions imply better performance considering both higher accuracy and lower costs. Specifically, our proposed \(\) could achieve comparable performance to GPT-4 within around 60% treated on both CSQA and OBQA datasets. On MedQA, the slope rapidly drops before the budget ratio reaches 95% and finally outperforms GPT-4 after \( 0.95\). Since budget \(\) strictly constrains the opportunities to call LLMs, intuitively, more budgets for LLMs will lead to higher accuracy in most scenarios. However, this does not practically stand for KBQA since the domain knowledge in LLMs is limited. Moreover, purely relying on LLMs will also lose the opportunities to leverage KGMs to further boost the accuracy. On the other hand, more budget will inevitably increase the token-level costs. Consequently, our proposed \(\) has outperformed GPT-4 on all the datasets with higher accuracy as \(\) increases. It effectively moves the Pareto frontier for KBQA with LLMs.

### Observations on search of \(\)

The hyperparameter \(\) is essential for controlling the constraint from cost regret within our framework. We face a dilemma to cautiously decide the value of \(\). On one hand, we aim to adopt a large value to strictly penalize models that allocate more resources to failed attempts. On the other hand, an over-penalty will damage the exploration during selection, which means the penalized models will no longer be invoked. Thus, we carefully adjust \(\) within \(\{0.001,0.1,1,10,100\}\) to modulate the extent to which we minimize cost regret versus maximizing accuracy. The visualization of performance, i.e., inferential accuracy and cost saving, is shown in Figure 3. Consequently, efficient decision-making, potentially leading to more conservative model selections. Conversely, a lower \(\) value may prioritize accuracy over cost savings, resulting in a higher tolerance for resource expenditure on exploration. We adopt the wave peak value of 1 for the final reported performance.

### Ablation Studies

In this part, we investigate the importance of each decomposed expectation in our overall objective of optimization, i.e., \(_{c}\) and \(_{a}\), as well as the constraint from _cost regret_\(_{a}\). The performance of

Figure 3: Performance changes based on the search of \(\).

Figure 2: A visualization of Pareto frontier of both inferential accuracy and cost saving as budget \(\) increases on three datasets.

inferential accuracy and cost saving by removing one component are shown in Table 2. Removing \(_{c}\) leads to significant reductions in cost savings across all datasets. Specifically, cost savings decrease by 47.59% for CSQA, 32.10% for OBQA, and 30.51% for MedQA. While The exclusion of \(_{a}\) results in noticeable reductions in accuracy, especially for MedQA, where accuracy falls to 0.760. Finally, removing \(_{a}\) brings decreases in both accuracy and cost savings across all datasets. The observations suggest the unique contribution of each component to the model's overall performance.

### Selection Regret Analysis

Additionally to the proof of the expectation bounds of both \(_{c}\) and \(_{a}\), we comprehensively evaluate our model selection by visualizing the selection regret in a 3D figure across three datasets in Figure 4. For a clear demonstration of the performance changes, we instantiate the expectation gap \([(a_{k}^{*})-(a_{k})]\) between best arm \(a_{k}^{*}\) and the selected arm \(a_{k}\) with a toy example as {-1,0} which indicate the regret of choosing \(a_{k}\). It sheds light on an intuitive understanding of how regret evolves and converges quickly over iterations, offering valuable insights into the model performance and the correctness of our selection strategy, highlighting the strengths of our proposed method.

### Case Studies

To provide insights into the effectiveness of \(\) on balancing inferential accuracy and cost saving, we visualize the distribution of model selection on CSQA, OBQA and MedQA in Figure 5, respectively. We could clearly observe the _exploration_ process when the color of cubes in the heatmap changes from deep to shallow, e.g., \(\{250,500\}\) intervals on CSQA and MedQA for GPT-4. This makes trials and spends necessary costs on the under-explored model. While the color changes from shallow to deep, e.g., \(\{250,500\}\) intervals on CSQA for HamQA, \(\{500,750\}\) intervals on CSQA and MedQA, \(\{100,200\}\) on OBQA for GPT-4 and \(\{750,1000\}\) intervals for ChatGPT, indicating the _exploitation_ process that leverages the best model. The case study sheds light on our superior ability to balance the selection for more accurate and cost-saving candidate models.

## 5 Related Work

Contemporary research has been focused on generating answers through deductive processes applied to knowledge graphs, as outlined in surveys and studies on the subject [24; 22]. Existing methods predominantly leverage techniques rooted in semantic parsing and information retrieval [31; 26]. For instance, KagNet  introduces a schema graph that adeptly captures relational paths among pivotal entities. Nonetheless, these methodologies often delineate the processes of question interpretation and knowledge graph (KG) inference as distinct stages, thereby exposing the models to potential pitfalls associated with the nuanced or implicit facets of query phrasing, including negations and

    &  &  &  \\   & Accuracy & Cost Saving (\$) & Accuracy & Cost Saving (\$) & Accuracy & Cost Saving (\$) \\  w/o \(_{c}\) & 0.750 & **- 47.59\%** & 0.855 & **- 32.10\%** & 0.607 & **- 30.51\%** \\ w/o \(_{a}\) & 0.801 & **- 15.42\%** & 0.880 & **- 21.16\%** & 0.760 & **- 1.07\%** \\ w/o \(_{a}\) & 0.800 & **- 6.26\%** & 0.898 & **- 3.31\%** & 0.684 & **- 15.98\%** \\  \(\) & **0.824** & **- 20.89\%** & **0.908** & **- 11.02\%** & **0.778** & **- 4.32\%** \\   

Table 2: Verification of the importance of \(_{c}\), \(_{a}\) and \(_{a}\) on three datasets.

Figure 4: A 3D toy visualization of the selection regret on three datasets as iteration \(k\) goes.

contextual constraints. Recognizing this limitation, recent innovations have pivoted towards a more integrated approach to reasoning. MHGRN  exemplifies this trend by dynamically refining the embeddings of the question context in tandem with the reasoning process, utilizing graph neural networks. This fusion of path encoders with GNNs not only enhances the model's interpretability but also its ability to scale. Building on this, the QA-GNN framework  goes further by crafting a work graph wherein the context itself is instantiated as an entity and interlinked with relevant entities through cooccurrence, thereby streamlining the reasoning process. GreaseLM  establishes a tighter connection between language models and KGs with a joint training mechanism. While HamQA  focuses on combining question understanding and knowledge reasoning, it excels in answering hierarchical questions containing hyponymys.

## 6 Limitation

Our study faces a major limitation of the distinct performance gaps among existing models considering both inferential accuracy and cost savings, e.g., KGMs and LLMs. This disparity from the underperformance of certain models hinders the overall efficiency of model selection. For example, on MedQA dataset, the gap between the best KGM and GPT-4 is a remarkable 36.50%. This forces our policy to rely on GPT-4 to maintain accuracy, bringing higher costs. However, as a fast-pluggable policy, Coke can easily address this concern when more robust models appear to boost the selection.

## 7 Conclusion

In this paper, we present Coke, a novel cost-efficient strategy for LLMs in KBQA while balancing inferential accuracy and cost saving. Our work first formally defines the problem of trading off accuracy and cost for KBQA with LLMs and provides a practical solution for utilizing LLMs in resource-constrained and domain knowledge-required scenarios.Coke could effectively integrate two sets of off-the-shelf models, i.e., LLMs and KGMs, and efficiently assign the most promising model for each question within a limited budget by employing a tailored cluster-based Thompson Sampling and a contextual multi-armed bandit. The former models the preliminary selection between LLMs and KGMs based on historical performance, while the latter identifies the best model within a cluster according to question semantics. The overall decision-making is bounded by the cost regret, constraining the selection based on cumulative expenses incurred from model failures. Extensive experiments on three domain-specific benchmark datasets demonstrate the superiority of Coke in terms of both inferential accuracy and cost-effectiveness. Our proposed framework could also offer a significantly promising direction for efficient integration of LLMs in various knowledge-based tasks.