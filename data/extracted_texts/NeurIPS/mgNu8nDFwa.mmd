# Beyond Average Return in Markov Decision Processes

Alexandre Marthe

UMPA

ENS de Lyon

Lyon, France

alexandre.marthe@ens-lyon.fr &Aurelien Garivier

UMPA UMR 5669 and LIP UMR 5668

Univ. Lyon, ENS de Lyon

46 allee d'Italie F-69364 Lyon cedex 07, France

aurelien.garivier@ens-lyon.fr &Claire Vernade

University of Tuebingen

Tuebingen, Germany

claire.vernade@uni-tuebingen.de

###### Abstract

What are the functionals of the reward that can be computed and optimized exactly in Markov Decision Processes? In the finite-horizon, undiscounted setting, Dynamic Programming (DP) can only handle these operations efficiently for certain classes of statistics. We summarize the characterization of these classes for policy evaluation, and give a new answer for the planning problem. Interestingly, we prove that only generalized means can be optimized exactly, even in the more general framework of Distributional Reinforcement Learning (DistRL). DistRL permits, however, to evaluate other functionals approximately. We provide error bounds on the resulting estimators, and discuss the potential of this approach as well as its limitations. These results contribute to advancing the theory of Markov Decision Processes by examining overall characteristics of the return, and particularly risk-conscious strategies.

## 1 Introduction

Reinforcement Learning (RL) has emerged as a flourishing field of study, delivering significant practical applications ranging from robot control and game solving to drug discovery or hardware design (Lazic et al., 2018; Popova et al., 2018; Volk et al., 2023; Mirhoseini et al., 2020). The cornerstone of RL is the "return" value, a sum of successive rewards. Conventionally, the focus is on computing and optimizing its expected value on Markov Decision Process (MDP). The remarkable efficiency of MDPs comes from their ability to be solved through dynamic programming with the Bellman equations (Sutton and Barto, 2018; Szepesvari, 2010). RL theory has seen considerable expansion, with a renewed interest for the consideration of more rich descriptions of a policy's behavior than the sole average return. At the other end of the spectrum, the so-called _Distributional Reinforcement Learning_ (DistRL) approach aims at studying and optimizing the entire return distribution, leading to impressive practical results (Bellemare et al., 2017; Hessel et al., 2018; Wurman et al., 2022; Fawzi et al., 2022). Between the expectation and the entire distribution, the efficient handling of other statistical functionals of the reward appears also particularly relevant for risk-sensitive contexts (Bernhard et al., 2019; Mowbray et al., 2022).

Despite recent progress, the full understanding of the abilities and limitations of DistRL to compute other functionals remains incomplete, with the underlying theory yet to be fully understood. Historically, the theory of RL has been established for discounted MDPs, see e.g. (Sutton and Barto, 2018; Watkins and Dayan, 1992; Szepesvari, 2010; Bellemare et al., 2023) for modern reference textbooks. Recently more attention was drawn to the undiscounted, finite-horizon setting (Auer, 2002,Osband et al., 2013; Jin et al., 2018; Ghavamzadeh et al., 2020), for which fundamental questions remain open. In this paper, we explore policy evaluation, planning and exact learning algorithms for undiscounted MDPs for the optimization problem of general functionals of the reward. We explicitly delimit the possibilities offered by dynamic programming as well as DistRL.

Our paper specifically addresses two questions:

1. How accurately can we evaluate statistical functionals by using DistRL?
2. Which functionals can be exactly optimized through dynamic programming?

We first recall the fundamental results in dynamic programming and Distributional RL. Addressing question (i), we refer to Rowland et al. (2019)'s results on Bellman closedness and provide their adaptation to undiscounted MDPs. We then prove upper bounds on the approximation error of Policy Evaluation with DistRL and corroborate these bounds with practical experiments. For question (ii), we draw a connection between Bellman closedness and planning. We then utilize the DistRL framework to identify two key properties held by _optimizable_ functionals.

Our main contribution is a characterization of the families of utilities that verify these two properties (Theorem 1). This result gives a comprehensive answer to question (ii) and closes an important open issue in the theory of MDP. It shows in particular that DistRL does not extend the class of functionals for which planing is possible beyond what is already allowed by classical dynamic programming.

## 2 Background

We introduce the classical RL framework in finite-horizon tabular Markov Decisions Processes (MDPs). We write \(()\) the space of probability distributions on \(\). A finite-horizon tabular MDP is a tuple \(=(,,p,R,H)\), where \(\) is a finite state space, \(\) is a finite action space, \(H\) is the horizon, for each \(h[H]\), \(p_{h}(x,a,)\) is a transition probability law and \(R_{h}(x,a)\) is a reward random variable with distribution \(_{h}\). The parameters \((p_{h})\) and \((R_{h})\) define the _model_ of dynamics. A deterministic policy on \(\) is a sequence \(=(_{1},,_{H})\) of functions \(_{h}:\).

Reinforcement Learning traditionally focuses on learning policies optimizing the expected return. For a given policy \(\), the \(Q\)-function maps a state-action pair to its expected return under \(\):

\[Q_{h}^{}(x,a)=_{_{h}}[R_{h}(x,a)]+_{s^{ }}p_{h}(x,a,x^{})Q_{h+1}^{}(x^{},_{h+1}(x^{})),  Q_{H+1}^{}(x,a)=0\.\] (1)

When the model is known, the \(Q\)-function of a policy \(\) can be computed by doing a backward recursion, also called dynamic programming. This is referred to as _Policy Evaluation_. Similarly, an optimal policy can be found by solving the optimal Bellman equation:

\[Q_{h}^{*}(x,a)=_{_{h}}[R_{h}(x,a)]+_{x^{ }}p_{h}(x,a,x^{})_{a^{}}Q_{h+1}^{*}(x^{},a^{}),  Q_{H+1}^{*}(x,a)=0\.\] (2)

Solving this equation when the model is known is also called _Planning_. When it is unknown, _reinforcement learning_ aims at finding the optimal policy from sample runs of the MDP. But evaluating and optimizing the _expectation_ of the return in the definition of the Q-function above is just one choice of statistical functional. We now introduce Distributional RL and then discuss other statistical functionals that generalize the expected setting discussed so far.

### Distributional RL

Distributional RL (DistRL) refers to the approach that tracks not just a statistic of the return for each state but its _entire distribution_. We introduce here the most important basic concepts and refer the reader to the recent comprehensive survey by Bellemare et al. (2023) for more details. The main idea is to use the full distributions to estimate and optimize various metrics over the returns ranging from the mere expectation (Bellemare et al., 2017) to more complex metrics (Rowland et al., 2019; Dabney et al., 2018; Liang and Luo, 2022).

At state-action \((x,a)\), let \(Z_{h}^{}(x,a)\) denote the future sum of rewards when following policy \(\) and starting at step \(h\), also called _return_. It verifies the simple recursive formula \(Z_{h}^{}(x,a)=R_{h}(x,a)+Z_{h+1}^{}(X^{},_{h+1}(X^{}))\) where \(X^{} p_{h+1}(x,a,)\). Its distribution is \(=(_{h}^{}(x,a))_{(x,a,h)[H]}\) and is often referred to as the _Q-value distribution_. One can easily derive the recursive law of the return as a convolution: for any two measures \(_{1},_{2}()\), we denote their convolution by \(_{1}*_{2}(t)=_{}_{1}()_{2}(t-)d\). For any two independent random variables \(X\) and \(Y\), the distribution of the sum \(Z=X+Y\) is the convolution of their distributions: \(_{Z}=_{X}*_{Y}\). Thus, the law of \(Z_{h}^{}(x,a)\) is

\[ x,a,h,_{h}^{}(x,a)=_{h}(x,a)*_{x^{}}p_{h} (x,a,x^{})_{h}^{}(x^{},_{h+1}(x^{}))\.\] (3)

This equation is a distributional equivalent to Eq. (1) and thus defines a _distributional Bellman operator_\(_{h}^{}=_{h}^{}_{h+1}^{}\).

Obviously, from a practical point of view, distributions form a non-parametric family that is not computationally tractable. It is necessary to choose a parametric (thus incomplete) family to represent them. Even the restriction to discrete reward distributions is not tractable, since the number of atoms in the distributions may grow exponentially with the number of steps1[Achab and Neu, 2021]: approximations are unavoidable. The most natural solution is to use projections of the obtained distribution on the parametric family, at each step of the Bellman operator. This process is called _parameterization_. The practical equivalent to Eq. (1) in DistRL hence writes

\[ x,a,h,_{h}^{}(x,a)=(_{h}(x,a)*_{ x^{}}p_{h}(x,a,x^{})_{h+1}^{}(x^{},_{h+1}(x^{ })))\,\] (4)

where \(\) is the projector operator on the parametric family. The full policy evaluation algorithm in DistRL is summarized in Alg.1.

``` Input: model \(p\), reward distributions \(_{h}\), policy \(\) to evaluated, \(\) projection. Data:\(^{H||||N}\) \( x,a,_{H}(x,a)=_{0}\) for\(h=H-1 0\)do \(_{h}(x,a)=_{h}(x,a)*_{x^{}}p_{h}(x,a,x^{})_{h+1}( x^{},_{h+1}(x^{})) x,a\) \(_{h}(x,a)=(_{h}(x,a)) x,a \) endfor Output:\(_{h}(x,a) x,a,h\) ```

**Algorithm 1** Policy Evaluation (Dynamic Programming) for Distributional RL

Distribution ParametrizationThe most commonly used parametrization is the so-called _quantile projection_. It put Diracs (atoms) with fixed weights at locations that correspond to the quantiles of the source distribution. One main benefit is that it does not require a previous knowledge of the support of the distribution, and allows for unbounded distributions.

The quantile projection is defined as

\[_{Q}=_{i=0}^{N-1}_{z_{i}},\ (z_{i})_{i}\ \ F_{}(z_{i})=\,\] (5)

which corresponds to a minimal \(W_{1}\) distance: \(_{Q}_{=_{i}_{i}/N}\,W_{1}(,)\), where \(W_{1}(.,.)\) is the Wasserstein distance defined for any distributions \(_{1},_{2}\) as \(W_{1}(_{1},_{2})=_{0}^{1}|F_{_{1}}^{-1}(u)-F_{_{2}}^{-1}( u)|\,u\). Note that this parametrization might admit several solutions and thus the projection may not be unique. For simplicity, we overload the notation to \(_{Q}=(_{Q}(x,a))_{(x,a)}\)For a Q-value distribution \(\) with support of length \(_{}\), and parametrization of resolution \(N\), Rowland et al. (2019) prove that the projection error is bounded by

\[_{(x,a)}\;W_{1}(_{Q}(x,a),(x,a)) }{2N}\;.\] (6)

In Section 3, we extend this result to the full iterative Policy Evaluation process and bound the error on the returned statistical functional in the finite-horizon setting. Note that other studied parametrizations exist but are less practical. For completeness, we discuss the Categorical Projection (Bellemare et al., 2017)(Rowland et al., 2018)(Bellemare et al., 2023) in Appendix B.

### Beyond expected returns

The expected value is an important functional of a probability distribution, but it is not the only one of interest in decision theory - especially when a control of the risk is important. We discuss two concepts that have received considerable attention: _utilities_, defined as expected values of functions of the return, and _distorted means_ which place emphasis on certain quantiles.

**Expected Utilities** are of the form \([f(Z)]\), or \( f\;\), where \(Z\) is the return of distribution \(\) and \(f\) is an increasing function. For instance, when \(f\) is a power function, we obtain the different moments of the return. The case of exponential functions plays a particularly important role: the resulting utility is referred to as _exponential utility_, _exponential risk measure_, or _generalized mean_ according to the context:

\[U_{}()=( X)  X\;.\] (7)

This family of utilities has a variety of applications in finance, economics, and decision making under uncertainty (Follmer and Schied, 2016). They can be considered as a risk-aware generalization of the expectation, with benefits such as accommodating a wide range of behaviors (Shen et al., 2014) from risk-seeking when \(>0\), to risk-averse when \(<0\) (the limit \( 0\) is exactly the expectation). To fix ideas, \(U_{}(,^{2})=+^{2}\): each \(\) captures a certain quantile of the Gaussian distribution.

**Distorted means**, on the other hand, involve taking the mean of a random variable, but with a different weighting scheme (Dabney et al., 2018). The goal is to place more emphasis on certain quantiles, which can be achieved by considering the quantile function \(F^{-1}\) of the random variable and a continuous increasing function \(:\). By applying \(\) to a uniform variable \(\) on \(\) and evaluating \(F^{-1}\) at the resulting value \(()\), we obtain a new random variable that takes the same values as the original variable, but with different probabilities. The distorted mean is then calculated as the mean of this new random variable, given by the formula \(^{}()F^{-1}()d\). If \(\) is the identity function, the result is the classical mean. When \(\) is \((/,1)\), we get the \(\)-Conditional Value at Risk (CVaR\(()\)) of the return, a risk measure widely used in risk evaluation (Rockafellar et al., 2000).

## 3 Policy Evaluation

The theory of MDPs is particularly developed for estimating and optimizing the mean of the return of a policy. But other values associated to the return can be computed the same way, by dynamic programming. This includes for instance the variance of the return, or more generally, any moment of order \(p 2\), as was already noticed in the 1980's (Sobel, 1982). Recently, Rowland et al. (2019) showed that for utilities in discounted MDPs, this is essentially all that can be done. More precisely, they introduce the notion of _Bellman closedness_ (recalled below for completeness) that characterizes a finite set of statistics that can efficiently be computed by dynamic programming.

**Definition 1** (Bellman closedness (Rowland et al., 2019)).: _A set of statistical functionals \(\{s_{1}, s_{K}\}\) is said to be Bellman closed if for each \((x,a)\), the statistics \(s_{1:K}(_{n}^{}(x,a))\) can be expressed in closed form in terms of the random variables \(R_{h}(x,a)\) and \((s_{1:K}(_{h+1}^{}(X^{},A^{})),\;A^{}(x),\;X ^{} p_{h}(x,A^{},)\), independently of the MDP._

Importantly, in the undiscounted setting, Rowland et al. (2019)(Appendix B, Theorem 4.3) show that the only families of utilities that are Bellman closed are of the form \(\{x x^{}( x)|0 L\}\)for some \(L<\). Thus, all utilities and statistics of the form of (or linear combinations of) moments and exponential utilities can easily be computed by classic linear dynamic programming and do not require distributional RL (see Appendix A.3).

Some important metrics such as the CVaR or the quantiles are not known to belong to any Bellman-closed set and hence cannot be easily computed. For this kind of function of the return, the knowledge of the transitions and the values in following steps is insufficient to compute the value on a specific step. In general, it requires the knowledge of the whole distribution of each reward in each state. Hence, techniques developed in distributional RL come in handy: for a choice of parametrization, one can use the projected dynamic programming step Eq. (4) to propagate a finite set of values along the MDP and approximate the distribution of the return. In the episodic setting, following the line of Rowland et al. (2019) (see Eq.(6)), we prove that the Wasserstein distance error between the exact and approximate distribution of the Q-values of a policy is bounded.

**Proposition 1**.: _Let \(\) be a policy and \(^{}\) the associated Q-value distributions. Assume the return is bounded on a interval of length \(_{} H_{R}\), where \(_{R}\) is the support size of the reward distribution. Let \(^{}\) be the Q-value distributions obtained by dynamic programming (Algorithm 1) using the quantile projection \(_{Q}\) with resolution \(N\). Then,_

\[_{(x,a,h)(,,[H])}W_{1}(^{}_{h}(x,a),^{}_{h}(x,a)) H}{2N} H^{2}}{2N}\;.\]

This result shows that the loss of information due to the parametrization may only grow quadraticly with the horizon. The proof consists of summing the projection bound in (6) at each projection step, and using the non-expansion property of the Bellman operator (Bellemare et al., 2017). The details can be found in Appendix C

The key question is then to understand how such error translates into our estimation problem when we apply the function of interest to the approximate distribution. We provide a first bound on this error for the family of statistics that are either utilities or distorted means.

First, we prove that the utility is Lipschitz on the set of return distributions.

**Lemma 1**.: _Let \(s\) be either an utility or a distorted mean and let \(L\) be the Lipschitz coefficient of its characteristic function. Let \(_{1},_{2}\) be return distributions. Then:_

\[|s(_{1})-s(_{2})| LW_{1}(_{1},_{2})\;.\]

Both family of functionals are treated separately, but lays a similar bound. The utility bound is the direct application of the Kantorovitch-Rubenstein duality, while the distorted mean one is a direct majoration in the integral. Again, the details are provided in the Appendix.

This property allows us to prove a maximal upper bound on the estimation error for those two families.

**Theorem 1**.: _Let \(\) be a policy. Let \(^{}\) be the Q-value return distribution associated to \(\) with the return bounded on a interval of length \(_{} H_{R}\) where \(_{R}\) is the support size of the reward distribution. Let \(^{}\) be the approximated return distribution computed with Algorithm 1, for the projection \(_{Q}\) with resolution \(N\). Let \(s\) be either an expected utility or a distorted mean, and \(L\) the Lipschitz coefficient of its characteristic function. Then:_

\[_{x,a,h}|s(^{}_{h}(x,a))-s(^{}_{h}(x,a))| LH }{2N} LH^{2}}{2N}\;.\]

Note that depending on the choice of utilities, the Lipschitz coefficient \(L\) may also depend on \(H\) and \( R\). For instance, in a stationary MDP, the Lipschitz constant of the exponential utility depends exponentially on \(_{}\). For the CVaR\(()\), however, \(L\) is constant and only depends on \((0,1)\).

Experiment: empirical validation of the bounds on a simple MDPWe consider a simple Chain MDP environment of length \(H=70\) equal to the horizon (see Figure 1 (right)) (Rowland et al., 2019), with a single action leading to the same discrete reward distribution for every step. We consider a Bernouilli reward distribution \((0.5)\) for each state so that the number of atoms for the return only grows linearly2 with the number of steps, which allows to compute the exact distribution easily.

We compare the distributions obtained with exact dynamic programming and the approximate distribution obtained by Alg 1, with a quantile projection with resolution \(N=1000\). Note that even at early stages, when the true distribution has less atoms than the resolution, the exact and approximate distributions differ due to the weights of the atoms in the quantile projection. Figure 2 (Right) reports the Wasserstein distance between the two distributions: the cumulative projection approximation error (dashed blue), the true error between the current exact and approximate distributions (solid blue) and the theoretical bound (red). Fundamentally, the proof of Prop. 1 upper bounds the distance between distributions by the cumulative projection error so we plot this quantity to help validating it.

We also empirically validate Theorem 1 by computing the CVaR(\(\)) for \(\{0.1,0.25\},\) corresponding respectively to distorted means with Lipschitz constants \(L=\{10,4\}.\) We compute these statistics for both distributions and report the maximal error together with the theoretical bound, re-scaled3 by a factor 5. Figure 2 (Left) shows an impressive correspondence of the theory and the empirical results despite a constant multiplicative gap.

## 4 Planning

Planning refers to the problem of returning a policy that optimizes our objective for a given model and reward function (or distribution in DistRL). It shares with policy evaluation the property to be grounded on a Bellman equation: see Eq. (2) for the classical expected return, which leads to efficient computations by dynamic programming.

For other statistical functionals of the cumulated reward, however, can the optimal policy be computed efficiently? The main result of this section characterizes the family of functionals that can be exactly and efficiently optimized by Dynamic Programming.

In the previous section, we recalled that Bellman-closed sets of utilities can be efficiently computed by DP as long as all the values of the utilities in the Bellman-closed family are computed together. For the planning problem, however, we only want to optimize one utility so we cannot consider families as previously. Under this constraint, only exponential and linear expected utilities are Bellman closed and thus can verify a Bellman Equation. In fact, for the exponential utilities, such Bellman Equation

Figure 1: A Chain MDP of length \(H\) with deterministic transition and identical reward distribution for each state.

Figure 2: Left: Validation of Theorem 1 on CVaR(\(\)) together with the scaled upper bound (see main text for discussion): the quadratic dependence in \(H\) is verified. Right: Validation of Proposition 1: The cumulative projection error (dashed blue) is the sum of the projection errors at every time step, and matches the true approximation error (solid blue). The theoretical upper bound (dashed red) differs only by a factor 2.

exists and allows for the planning problem to be solved efficiently [Howard and Matheson, 1972]:

\[Q_{h}^{}(x,a)=U_{}^{}(R_{h}(x,a))+[_{x^{}}p_{h}(x,a,x^{})(_{a ^{}}Q_{h+1}^{}(x^{},a^{}))]\] (8) \[Q_{H+1}^{}(x,a)=0\.\]

However, the question of the existence of Optimal Bellman Equations for non-utility functionals remains open (e.g. quantiles). More generally, an efficient planning strategy is not known. To address these questions, we consider the most general framework, DistRL, and recall the theoretical DP equations for any statistical functional \(s\) in the Pseudo4-Algorithm 2. DistRL offers the most comprehensive, or 'lossless', approach, so if a statistical functional cannot be optimized with Alg. 2, then there cannot exist a Bellman Operator to perform exact planning.

```
1:Input: model \(p\), reward \(R\), statistical functional \(s\)
2:Data:\(^{H||||},^{ H||N}\)
3:\( x,_{H+1}^{x}=_{0}\)
4:for\(h=H 1\)do
5:\(_{h}(x,a)=_{h}^{(x,a)}*_{x^{}}p_{h}^{a}(x,x^{}) _{h+1}^{x^{}} x,a\)
6:\(_{h}^{x}=_{h}(x,a^{*})\, a^{*}_{a}s(_{h}(x,a))  x\)
7:endfor
8:Output:\(_{h}(x,a)\  x,a,h\) ```

**Algorithm 2** Pseudo-Algorithm: Exact Planning with Distributional RL

We formalize this idea with the new concept of _Bellman Optimizable_ statistical functionals:

**Definition 2** (Bellman Optimizable statistical functional).: _A statistical functional \(s\) is called Bellman optimizable if, for any MDP \(\), the Pseudo-Algorithm 2 outputs an optimal return distribution \(=^{*}\) that verifies:_

\[ x,a,h, s(_{h}^{*}(x,a))=_{}s(_{h}^{ }(x,a))\.\] (9)

**Remark**.: _This definition is equivalent to the satisfaction of an Optimal Distributionnal Bellman equation. Indeed, a statistical functional \(s\) is Bellman Optimizable if and only if, for any \((,,,p,)\), \(s\) verifies_

\[_{a_{x^{}}}s(*_{x^{}}p(x^{} )(x^{},a_{x^{}}))=s(*_{x^{}}p(x^{ })(x^{},a_{x^{}}^{*}))\]

_with \(a_{x}^{*}_{a}s((x,a))\)_

We can now state our main results that characterizes all the _Bellman optimizable_ statistical functionals. First, we prove that such a functional must satisfy two important properties.

**Lemma 2**.: _A Bellman optimizable functional \(s\) satifies the two following properties:_

* _Independence Property: If_ \(_{1},_{2}()\) _are such that_ \(s(_{1}) s(_{2})\)_, then_ \[_{3}(),, s( _{1}+(1-)_{3}) s(_{2}+(1-)_{3}))\.\]
* _Translation Property: Let_ \(_{c}\) _denote the translation on the set of distributions:_ \(_{c}_{x}=_{x+c}\)_. If_ \(_{1},_{2}()\) _are such that_ \(s(_{1}) s(_{2})\)_, then_ \[ c, s(_{c}_{1}) s(_{c} _{2})\.\]

Indeed, the expectation and the exponential utility both satisfy these properties (see Appendix A.2). Each property is implied by an aspect of the Distributional Bellman Equation (Alg. 2, line 5) and the proof (in Appendix E) unveils these important consequences of the recursion identities.

Fundamentally, they follow from the Markovian nature of policies optimized this way: the choice of the action in each state should be independent of other states and rely only on the knowledge of the next-state value distribution.

The Independence property states that, for Bellman optimizable functionals, the value of each next state should not depend on that of any other value in the convex combination in the rightmost term of the convolution. In turn, the Translation property is associated to the leftmost term, the reward, and it imposes that, for Bellman optimizable functionals, the decision on the best action is independent of the previously accumulated reward.

The Independence property is related to expected utilities (von Neumann et al., 1944). Any expected utility verifies this property (Appendix A.2) but, most importantly, the Expected Utility Theorem (also known as the Von Neumann Morgenstein theorem) implies that any continuous statistical functional \(s\) verifying the Independence property can be reduced to an expected utility. This means that for any such statistical functional \(s\), there exists \(f\) continuous such that \(_{1},_{2}()\), we have \(s(_{1})>s(_{2}) U_{f}(_{1})>U_{f}(_{2})\)(von Neumann et al., 1944, Grandmont, 1972).

This result directly narrows down the family of Bellman optimizable functionals to utilities. Indeed, although other functionals might potentially be optimized using the Bellman equation, addressing the problem on utilities is adequate to characterize all possible behaviors. For instance, moment-optimal policies that can be found through dynamic programming, can also be found by optimizing an exponential utility function. The next task is therefore to identify all the utilities that satisfy the second property. We demonstrate that, apart from the mean and exponential utilities, no other \(W_{1}\)-continuous functional satisfies this property.

**Theorem 2**.: _Let \(\) be a return distribution. The only \(W_{1}\)-continuous Bellman Optimizable statistical functionals of the cumulated return are exponential utilities \(U_{}()=_{}[( R )]\) for \(\), with the special case of the expectation \(_{}[R]\) when \(=0\)._

Of course, if \(U()\) is a utility and \(\) is a monotonous scalar mapping, \((U())\) is an equivalent utility: one should understand in the previous theorem that a _\(W_{1}\)-continuous_ Bellman Optimizable statistical functional is equivalent to \(U_{}()\) for some \(\). We chose here to define \(U_{}()=_{}[( R )]\) with the \(\) and the factor \(1/\) since it results in a normalized utility that tends to the expectation when \(\) goes to \(0\). The full proof of Theorem 2 is provided in Appendix E.

We make a few important observations. First, this result shows that algorithms using Bellman updates to optimize any continuous functionals other than the exponential utility cannot guarantee optimality. The theorem does not apply to non-continuous functionals, but Lemma 2 still does. For instance, the quantiles are not \(W_{1}\)-continuous so Theorem 2 does not apply, but it is easy to prove that they do not verify the Independence Property and thus are not Bellman Optimizable. Also, there might also exist other optimizable functionals, like moments, but they must first be reduced to exponential or linear utilities.

Most importantly, while in theory, DistRL provides the most general framework for optimizing policies via dynamic programming, our result shows that in fact, the only utilities that can be exactly and efficiently optimized do not require to resort to DistRL. This certainly does not question the very purpose of DistRL, which has been shown to play important roles in practice to regularize or stabilize policies and to perform deeper exploration (Bellemare et al., 2017; Hessel et al., 2018). Some advantages of learning the distribution lies in the enhanced _robustness_ offered in the richer information learned (Rowland et al., 2023), particularly when utilizing neural networks for function approximation (Dabney et al., 2018; Barth-Maron et al., 2018; Lyle et al., 2019).

## 5 Q-Learning Exponential Utilities

The previous sections consider the the model, i.e. the reward and transition functions, be known. Yet in most practical situations, those are either approximated or learnt5. After addressing policy evaluation (Section 3) and planning (Section 4), we conclude here the argument of this paper by addressing the question of learning the statistical functionals of Theorem 2. In fact, we simply highlight a lesser known version of Q-Learning Watkins and Dayan (1992) that extend this celebrated algorithm to exponential utilities. We provide the pseudo-code for the algorithm proposed by Borkar with the relevant utility-based updates in Alg. 3. We refer to these seminal works for convergence proofs. Linear utility updates (line 8) differ only slightly from classical ones for expected return optimization, which have been shown to lead to the optimal value asymptotically [Watkins and Dayan, 1992].

```
1:Input:\((_{t})_{t}\), transition and reward generator. \(Q_{h}(x,a) H\),\((x,a,h)[H]\)
2:Utilities: Linear (\(Z Z+b\)) or Exponential (\(Z(( Z))/\))
3:for episode \(K=1,,K\)do
4: Observe \(x_{1}\)
5:for step \(h=1,,H\)do
6: Choose action \(a_{h}_{a}\,Q_{h}(x_{h},a)\)
7: Observe reward \(r_{h}\) and transition \(x_{h+1}\) and update for chosen objective:
8: Linear Util.: \(Q_{h}(x_{h},a_{h})(1-_{k})Q_{h}(x_{h},a_{h})+_{k}[ (r_{h}+_{a^{}}\,Q_{h+1}(x_{h+1},a^{}))+b]\)
9: Exponential Util.: \(Q_{h}(x_{h},a_{h})[(1-_{k})e^{  Q_{h}(x_{h},a_{h})}+_{k}e^{ r_{h}+_{a^{}}\,Q_{h}(x_{h+1},a^{})}]\)
10:endfor
11:endfor
12:Output:\(Q_{h}(x,a) x,a\) ```

**Algorithm 3** Q-Learning for Linear and Exponential Utilities

## 6 Discussions and Related Work

The Discounted FrameworkWe focused in this article on undiscounted MDPs, and it is important to note that the results differ for discounted scenarios. The crucial difference is that the family of exponential utilities no longer retains Bellman Closed or Bellman Optimizable properties due to the introduction of the discount factor \(\)[Rowland et al., 2019]. When it comes to Bellman Optimization, the necessary translation property becomes an affine property : \( c,,\ s(_{c}^{}_{1})>s(_{c}^{}_{2})\) where \(_{c}^{}\) is the affine operator such that \(_{c}^{}_{x}=_{ x+c}\). This property is not upheld by the exponential utility. Nonetheless, there exists a method to optimize the exponential utility through dynamic programming in discounted MDPs [Chung and Sobel, 1987]. This approach requires modifying the functional to optimize at each step (the step \(h\) is optimized with the utility \(x(^{-h}\ x)\)), but it also implies a loss of policy stationarity, property usually obtained in dynamic programming for discounted finite-horizon MDPs [Sutton and Barto, 2018].

Utilizing functionals to optimize expected return.DistRL has also been used in Deep Reinforcement learning to optimize non-Bellman-optimizable functionals such as distorted means[Ma et al., 2020, Dabney et al., 2018a]. While, as we proved so, such algorithms cannot lead to optimal policies in terms of these functionals, experiments show that in some contexts they can lead to better expected return and faster convergence in practice. The change of functional can be interpreted as a change in the exploration process, and the resulting risk-sensitive behaviors seem to be relevant in adequate environments.

Dynamic programming for the optimization of other functionalsTo optimize other statistical functionals such as CVaR and other utilities such as moments with Dynamic Programming, Bauerle and Ott  and Bauerle and Rieder  propose to extend the state space of the original MDP to \(^{}=\) by theoretically adding a continuous dimension to store the current cumulative rewards. This idea does not contradict our results, and the resulting algorithms remain empirically much more expensive.

Another recent thread of ideas to optimize functionals of the reward revolve around the dual formulation of RL through the empirical state distribution [Hazan et al., 2019]. Algorithms can be derived by noticing that utilities like the CVar are equivalent to solving a convex RL problem [Mutti et al., 2023].

## 7 Conclusion

Our work closes an important open problem in the theory of MDPs: we exactly characterize the families of statistical functionals that can be evaluated and optimized by dynamic programming.

We also put into perspective the DistRL framework: the only functionals of the return that can be optimized with DistRL can actually be handled exactly by dynamic programming. Its benefit lies elsewhere, and notably in the improved stability of behavioral properties it allows. We believe that, by narrowing down the avenues to explain its empirical successes, our work can contribute to clarify the further research to conduct on the theory of DistRL.