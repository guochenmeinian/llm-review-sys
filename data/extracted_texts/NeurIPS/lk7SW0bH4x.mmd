# ProbTS: Benchmarking Point and Distributional Forecasting across Diverse Prediction Horizons

Jiawen Zhang

HKUST (GZ)

Guangzhou, China

jiawe.zh@gmail.com

&Xumeng Wen

Microsoft Research Asia

Beijing, China

xumengwen@microsoft.com

&Zhenwei Zhang

Tsinghua University

Beijing, China

zzw20@mails.tsinghua.edu.cn

Shun Zheng

Microsoft Research Asia

Beijing, China

shun.zheng@microsoft.com

&Jia Li

HKUST (GZ)

Guangzhou, China

jialee@ust.hk

&Jiang Bian

Microsoft Research Asia

Beijing, China

jiang.bian@microsoft.com

This work was done during the internship at Microsoft Research Asia.

###### Abstract

Delivering precise point and distributional forecasts across a spectrum of prediction horizons represents a significant and enduring challenge in the application of time-series forecasting within various industries. Prior research on developing deep learning models for time-series forecasting has often concentrated on isolated aspects, such as long-term point forecasting or short-term probabilistic estimations. This narrow focus may result in skewed methodological choices and hinder the adaptability of these models to uncharted scenarios. While there is a rising trend in developing universal forecasting models, a thorough understanding of their advantages and drawbacks, especially regarding essential forecasting needs like point and distributional forecasts across short and long horizons, is still lacking. In this paper, we present ProbTS, a benchmark tool designed as a unified platform to evaluate these fundamental forecasting needs and to conduct a rigorous comparative analysis of numerous cutting-edge studies from recent years. We dissect the distinctive data characteristics arising from disparate forecasting requirements and elucidate how these characteristics can skew methodological preferences in typical research trajectories, which often fail to fully accommodate essential forecasting needs. Building on this, we examine the latest models for universal time-series forecasting and discover that our analyses of methodological strengths and weaknesses are also applicable to these universal models. Finally, we outline the limitations inherent in current research and underscore several avenues for future exploration. 1

## 1 Introduction

Time-series forecasting has extensive applications in various industries, including traffic flow forecasting , renewable energy forecasting , and diverse forecasting demands in retail , finance , physical system , and climate . It is crucial to provide forecasts across different prediction horizons, addressing both short- and long-term planning needs . Moreover, modern decision-making processes typically require not only point forecasts to quantify planning efficiency but also robust distributional estimations to manage uncertainty effectively . The fundamental need to produce accurate point and distributional forecasts across various horizons presents significant challenges to existing forecasting approaches.

Nevertheless, much of the previous research on developing deep learning models for time-series forecasting has often focused on isolated aspects, such as long-term point forecasting or short-term distribution estimations. This narrow focus may result in skewed methodological choices and hinder the adaptability of these models to rarely evaluated scenarios. For example, studies such as  have primarily explored neural architecture designs tailored for long-term point forecasting with strong trending and seasonal patterns. However, it remains unclear how these advancements can be effectively extended to capture complicated distributions and whether these designs maintain their effectiveness in short-term scenarios. Conversely, research such as  adapts deep generative models  for probabilistic forecasting, specializing in characterizing complex data distributions. Yet, these models have mainly been developed and evaluated in short-term scenarios, leaving questions about their effectiveness in long-term forecasting and their ability to preserve point forecasting performance.

Despite the recent surge in building time-series foundation models over the past year , our understanding of their advantages and limitations, especially regarding essential forecasting needs like point and distributional forecasts across various horizons, is still limited. Many of these models claim to support arbitrary prediction horizons, employing different mechanisms that come with their own set of advantages and drawbacks. Among them, a select few offer capabilities for distributional forecasting, which, however, are typically confined to predefined closed-form distributions  or discrete distributions with value quantization . The emergence of these foundation models has brought about unprecedented zero-shot forecasting capabilities. Consequently, it is both timely and crucial to delve into an evaluation of their strengths and weaknesses, especially in relation to the fundamental forecasting needs mentioned earlier.

In this study, we present ProbTS, a benchmark tool crafted to serve as a comprehensive platform for assessing those key forecasting needs and for performing a detailed comparison of several state-of-the-art models developed in recent years. To address the core forecasting requirements, ProbTS includes a broad array of datasets and spans various forecasting horizons. It also utilizes both point and distributional metrics to facilitate a thorough performance evaluation.

Our research reveals that the specific data characteristics inherent to different forecasting requirements often play a crucial role in guiding the selection of model designs. As a result, it is crucial to have a comprehensive view of the essential forecasting needs. To aid in the analysis and interpretation of performance, we measure three essential data characteristics in ProbTS: the strength of trends and seasonality, and the complexity of the data distribution. Moreover, we have explicitly distinguished three fundamental methodological aspects within ProbTS that differentiate the existing forecasting models, largely influencing their pros and cons. The first aspect involves the approach to distributional forecasting, ranging from models focused on point forecasts  to those using pre-defined distribution heads based on specific data assumptions . The second aspect is the decoding scheme used to generate multi-step forecasts, which can be either autoregressive (AR) or non-autoregressive (NAR). The third aspect pertains to the normalization choice, where the long-term point forecasting models typically employ reversible instance normalization (RevIN)  while short-term probabilistic ones often use mean scaling strategies .

By utilizing ProbTS, we conduct a systematic comparison between studies that focus on long-term point forecasting and those aimed at short-term distributional estimation, employing various forecasting horizons and evaluation metrics. Our overarching finding is that the strengths of these methods tend to diminish in scenarios they are rarely evaluated in, highlighting several important but unresolved research questions. Notably, while recent probabilistic forecasting approaches have shown proficiency in short-term distribution estimation, we find that long-term distributional forecasting remains a significant challenge. This challenge stems from achieving distribution estimation that remains both efficient and effective as the prediction horizon extends--a topic that has not been thoroughly investigated in existing literature. Additionally, our analysis uncovers a clear divide in the choice of decoding schemes: most long-term point forecasting methods opt for NAR, whereas choices in short-term forecasting studies are more evenly split. Further investigation suggests that the preference for NAR methods stems from existing AR models' difficulty in managing error accumulation, particularly over extended horizons with strong trends. However, we observe that a proper normalization strategy can significantly improve AR models in long-term forecasting, opening new possibilities for AR-based approaches. Moreover, AR decoding performs better in scenarios with pronounced seasonality, indicating potential for refining these strategies, particularly for long term forecasts. Given the inefficiency of existing NAR-based probabilistic methods like CSDI, our comparison highlights the need for further exploration of decoding strategies in future research.

Furthermore, we have expanded the analytical framework of ProbTS to include an examination of several very recently developed time-series foundation models, which has allowed us to re-validate some of our earlier findings. Interestingly, there appears to be a relatively even split in their preference for AR and NAR decoding schemes. Our analysis reaffirms the limitation of AR in handling time-series data, as we observe that AR-based foundation models tend to excel at shorter horizons. However, their performance advantages often significantly diminish over longer forecasting periods. This underscores the critical need for future research to focus on addressing the issue of error accumulation in AR-based foundation models. Besides, our exploration reveals that current probabilistic foundation models may face challenges when dealing with complex data distributions. This observation suggests that the integration of more sophisticated distribution estimation techniques could enhance the development of time-series foundation models.

In summary, we have made the following contributions.

* Introduction of ProbTS, a benchmark tool designed for a thorough evaluation of essential forecasting needs, towards precise point and probabilistic forecasting across varied horizons.
* Comprehensive analysis of methodological variations within forecasting models, especially regarding distributional estimation methods and decoding schemes (AR vs. NAR), which illuminates significant yet previously underexplored research challenges.
* Extension of our analytical framework to include the latest time-series foundation models, providing insights into the implications of their methodological choices and underscoring important directions for future research endeavors.

## 2 Related Work

Classical Time-series Forecasting ModelsIn recent years, classical research in time-series forecasting has bifurcated into two distinct but complementary streams. The first stream has concentrated on refining neural architecture designs for long-term forecasting, primarily employing non-autoregressive decoding schemes to address scenarios with pronounced trend and seasonality. This stream has evolved from enhancing multi-layer perceptrons [53; 76] to developing specialized recurrent or convolutional neural networks [34; 37], and introducing Transformer-based models [66; 49; 40]. Despite achieving advancements in point forecasts, these efforts mainly capture average future changes, with only a few adopting approaches like quantile regression to partially overcome this limitation [69; 36]. On the other hand, the second stream, probabilistic time-series forecasting specializes in capturing the intricate data distribution of future time series. It encompasses a spectrum of techniques, from utilizing predefined likelihood functions [55; 60] and Gaussian copulas [59; 19] to exploring advanced deep generative models [58; 7]. Unlike the first stream, this branch employs both AR [58; 57] and NAR decoding schemes [62; 7; 33], often utilizing standard neural network architectures to represent time series [16; 58; 57; 7; 19], though some studies propose customized designs [62; 35; 5]. Together, these streams highlight the diverse approaches to forecasting, ranging from point predictions focusing on the mean future variations to probabilistic forecasts that capture the full distribution of future values. In Appendix A.1, we summarize a comparison of these models on the coverage of essential forecasting needs and their methodological preferences.

Universal Time-series Foundation ModelsOver the past year, the development of time-series foundation models has greatly accelerated, driven by the success of language foundation models . This wave has seen models such as Lag-Llama , TimesFM , Timer , and Chronos  adopting the decoder-only Transformer architecture with an AR decoding scheme. Conversely, models like Forecast PFN , MOIRAI , TTM , and UniTS  employ the NAR decoding, often using variable-length placeholders to indicate prediction positions for different horizons. Probabilistic forecasting is less common, with MOIRAI and Lag-Llama integrating pre-defined distribution heads (Student-t for Lag-Llama and a mixture for MOIRAI) while Chronos uses quantized bins to accommodate time-series values and adopts Softmax outputs for distribution approximation. The strategic choice between AR and NAR decoding and the method for distributional estimation highlight distinct trade-offs. For an extensive comparison, see Appendix A.2.

Toolkits for Time-series Forecasting.We observe a plethora of toolkits that have been developed for time-series forecasting. These range from those primarily designed for point forecasting, such as Prophet , sktime , tasi , and TSIibb , to others that incorporate probabilistic forecasting, including GluonTS , PyTorchTS , PyTorchForecasting2, and NeuralForecast3. In creating ProbTS, we built upon the foundations laid by tools like PyTorchTS, GluonTS, and TSIibb. Our unique contribution is a detailed approach that supports both precise point and probabilistic forecasting over various horizons, and examines methodological differences in forecasting models, especially regarding distributional estimation and decoding schemes (AR vs. NAR). Additionally, ProbTS integrates cutting-edge time-series foundation models, making it a comprehensive benchmark tool for tackling current and future challenges in time-series forecasting. A comparison of ProbTS with existing toolkits, focusing on functionalities and features, is provided in Appendix A.3.

## 3 The ProbTS Tool

This section offers a concise overview of the ProbTS tool's design and implementation. The core modules and the primary pipeline of ProbTS are depicted in Figure 1.

DataWe aggregate publicly accessible datasets used for both short-term and long-term forecasting. Initial data visualization analyses reveal that the data domains and forecasting horizons significantly influence specific data characteristics within a given forecasting horizon. For instance, many long-term forecasting scenarios exhibit clear trend and seasonality patterns within a forecasting window, while numerous short-term forecasting cases display irregular variations within a short sliding window. Consequently, we have developed quantified indicators, such as trend and seasonality strengths, along with _non-Gaussianity_ to indicate the complexity of data distribution within a forecasting window. Detailed information about dataset statistics, visualization analyses, and quantified measures can be found in Appendix B.1.1, B.1.2, B.1.3, and B.1.4. The quantified measurements for all forecasting scenarios are compiled in Table 1.

MetricsProbTS incorporates a broad range of evaluation metrics to enable a thorough assessment of both point and distributional forecasts. These metrics are elaborated in detail in Appendix B.2. In this paper, we primarily use the normalized mean absolute error (NMAE) for point forecasts and the continuous ranked probability score (CRPS) for distributional forecasts to succinctly communicate the critical insights discovered. It is noteworthy that some methods reproduced in ProbTS, their original papers reported certain point forecast metrics before de-normalizing forecasts to the initial scale [75; 71; 49] or primarily reveal aggregated distributional metrics over all time-series variates, namely CRPS-sum [59; 57; 58]. We have verified our reproduced results align with their reported results and utilized the unified metrics in this study to offer a comprehensive and fair comparison of these studies from different research threads.

Figure 1: An overview of ProbTS.

ModelThe model module in ProbTS explicitly differentiates critical methodological decisions, especially the decoding scheme (AR vs NAR) and the distributional estimation approach. Specifically, we employ the following mathematical formulation. We denote an element of a multivariate time series as \(x_{t}^{k}\), where \(k\) represents the variate index and \(t\) denotes the time index. At time step \(t\), we have a multivariate vector \(_{t}^{K}\). Each \(x_{t}^{k}\) is associated with covariates \(_{t}^{k}^{N}\), which encapsulates auxiliary information about the observations. Given a length-\(T\) forecast horizon, a length-\(L\) observation history \(_{t-L:t}\) and corresponding covariates \(_{t-L:t}\), the objective in time series forecasting is to generate the vector of future values \(_{t+1:t+T}\). Based on established conventions, we categorize forecast as short-term if the horizon \(T\)[57; 62], and long-term if \(T\)[75; 49; 40], where \(\) represents the primary periodicity of the data (e.g., 24 for hourly frequency). To represent point and distributional forecasting in a unified way, here we divide a model into an encoder \(f_{}\) and a forecaster \(p_{}\). An encoder is tasked with generating expressive hidden states \(^{D}\). Under _autoregressive_ decoding scheme, encoder forecasts variates using their past values: \(_{t}=f_{}(_{t-1},_{t},_{t-1})\). Under the _non-autoregressive_ scheme, the encoder generates all the forecasts in one step: \(_{t+1:t+T}=f_{}(_{t-L:t},_{t-L:t+T})\). A forecaster \(p_{}\) is employed either to directly estimate _point forecasts_ as \(}_{t}=p_{}(_{t})\), or to perform sampling based on the estimated _probabilistic distributions_ as \(}_{t} p_{}(_{t}|_{t})\). In addition, the normalization choices utilized by different research branches vary, with a detailed analysis provided in Appendix D.1.

## 4 Results and Analyses

Utilizing ProbTS, we conducted a comprehensive benchmarking and analysis of a diverse range of state-of-the-art models from different strands of research. We mainly assessed these models using NAME and CRPS metrics across multiple forecasting horizons, repeating each experiment five times with different seeds to ensure result reliability.

Selected Models for Comparison.Our selection criteria for models focused on a balance of performance, reproducibility, and simplicity. For long-term point forecasting, we included models like iTransformer , PatchTST , TimesNet , N-HiTS , and LTSF-Linear . Probabilistic forecasting methods selected include GRU NVP, GRU MAF, Trans MAF , TimeGrad , and CSDI . Additionally, general architectures like Linear, GRU , and Transformer , along with simple non-parametric baselines, were evaluated as a reference. For foundation models, reproducible methods such as Lag-Llama , TimesFM , Timer , MOIRAI , Chronos , and UniTS  were included. Detailed implementation specifics are in Appendix B.3.

Due to space constraints, comprehensive comparison results are placed in Appendix C, with detailed results for short-term and long-term forecasting in Tables 9 and 10, respectively. Zero-shot evaluations of pre-trained time-series foundation models are detailed in Tables 11 and 12. Our evaluation highlights the critical relationship between forecasting requirements, data properties, and modeling strategies. It aims to shed light on the strengths and limitations of current approaches, paving the way for uncovering novel research avenues.

  
**Dataset-Horizon** & **Exchange-S** & **Solar-S** & **Electricity-S** & **Traffic-S** & **Wikipedia-S** & **ETTm1-L** & **ETTm2-L** \\  Trend \(F_{T}\) & 0.9982 & 0.1688 & 0.6443 & 0.2880 & 0.5253 & 0.9462 & 0.9770 \\ Seasonality \(F_{S}\) & 0.1256 & 0.8592 & 0.8323 & 0.6656 & 0.2234 & 0.0105 & 0.0612 \\  Non-Gaussianity & 0.2967 & 0.5004 & 0.3579 & 0.2991 & 0.2751 & 0.0833 & 0.1701 \\  
**Dataset-Horizon** & **ETTm1-L** & **ETTm2-L** & **Electricity-L** & **Traffic-L** & **Weather-L** & **Exchange-L** & **ILI-L** \\  Trend \(F_{T}\) & 0.7728 & 0.9412 & 0.6476 & 0.1632 & 0.9612 & 0.9978 & 0.5438 \\ Seasonality \(F_{S}\) & 0.4772 & 0.3608 & 0.8344 & 0.6798 & 0.2657 & 0.1349 & 0.6075 \\  Non-Gaussianity & 0.0719 & 0.1422 & 0.1533 & 0.1378 & 0.1727 & 0.1082 & 0.1112 \\   

Table 1: This table includes a quantitative assessment of the inherent characteristics for all forecasting scenarios, each corresponding to a dataset with a specific forecasting horizon. We use the suffixes ”-S” and ”-L” to differentiate between short-term and long-term scenarios. Quantified indicators encompass trend and seasonality strengths, as well as non-Gaussianity, where a higher value signifies a greater deviation from a Gaussian distribution.

### Analyzing Classical Models for Time-series Forecasting

We examine traditional non-universal time-series models from distinct research branches: one branch focuses on developing customized neural architectures tailored for long-term point forecasting, while the other branch concentrates on creating advanced probabilistic methods for short-term distributional forecasting. Our investigation confirms the effectiveness of these models for their intended purposes. However, we observe a notable trend: the strengths of these methods tend to diminish in scenarios where they are seldom tested.

**Diminishing Advantages of Customized Architectures in Short-term Forecasting Scenarios** The comparative analysis presented in Figures 1(a) and 1(c) showcases the performance of point and probabilistic forecasting methods with respect to the NMAE metric. These figures also illustrate how NMAE values correlate with non-Gaussianity, a measure we employ to evaluate the complexity of data distributions. It becomes evident that customized architectures, originally crafted for long-term forecasting, tend to lose their competitive performance in short-term scenarios. This phenomenon could be attributed to the increased importance of accurately characterizing complex data distributions within shorter forecasting windows, where higher non-Gaussianity scores are indicative of this necessity. A closer look at Figures 1(c) and 1(d) further reveals that the performance disparity measured with CRPS becomes even more pronounced for datasets characterized by significant non-Gaussianity, such as Solar-S. This observation underscores the critical need for incorporating short-term patterns and distributional estimation capabilities into the design of new forecasting architectures.

**Significant Performance Degradation for Existing Probabilistic Methods in Long-term Distribution Forecasting** The performance of current probabilistic forecasting models in long-term scenarios, even when assessed using distributional metrics such as CRPS, reveals notable limitations. This is highlighted by the comparison between Figures 1(b) and 1(d), which shows a significant drop in performance for models like TimeGrad on ETTm1-L, GRU NVP on ETTh2-L, and Weather-L datasets. The decline in performance can be attributed to the fact that these probabilistic models were not specifically designed with the unique challenges of long-term forecasting in mind. This oversight has mixed influences. On the positive side, the design of these methods has led to a more balanced approach in the choice between AR and NAR decoding schemes, providing a versatile

Figure 2: We present a comprehensive comparison between classical models designed for long-term point forecasting and short-term distributional forecasting across various prediction horizons. It utilizes a non-Gaussianity score to highlight the complexity of the data distribution across different datasets. The aggregated performance metrics are derived from Tables 9 and 10.

foundation for probabilistic forecasting. However, the downside is more significant: no matter using which decoding schemes, existing probabilistic models face considerable challenges when applied to long-term distributional forecasting. We will dive deeper into the specific challenges associated with each decoding scheme next. Here the performance gap underscores the need for future research to systematically investigate long-term distributional forecasting.

Different Decoding Schemes & Challenges in Long-term Distributional ForecastingExisting probabilistic forecasting methods exhibit a balanced preference for both AR and NAR decoding schemes. For instance, TimeGrad employs an AR decoding scheme, whereas CSDI utilizes an NAR approach. This contrasts starkly with the aforementioned customized architectures, which solely opt for NAR decoding. These two types of decoding schemes, however, confront distinctive challenges when applied to long-term probabilistic forecasting. With original normalization strategy, AR probabilistic models like TimeGrad struggle with error accumulation, particularly as the forecast horizon extends or trends strengthen, the performance gap widens, as shown in Figures 2(a) and 2(c). On the other hand, NAR models such as CSDI encounter memory constraints in long-term forecasts (detailed in Appendix D.7). Moreover, Table 10 reveals that even on smaller datasets, such as ETTm2 and ETTh1, CSDI's performance in long-term scenarios is less than optimal, indicating reduced learning efficiency by the extension of the forecasting horizon.

The Unexpected Superiority of AR Decoding in Addressing Strong SeasonalityDespite its drawbacks, AR-based models, as used in TimeGrad, excels in capturing strong seasonality, outperforming models like PatchTST in scenarios such as the Traffic dataset (Table 10). This advantage is further analyzed in Figures 2(c) and 2(d), showing AR's increasing benefit with stronger seasonal patterns. This suggests AR's potential in long-term forecasting could be revitalized with solutions to its error accumulation challenge in long horizons.

Figure 3: We explore the challenges faced by current models in conducting long-term distributional forecasting, with insights drawn from Table 10 and Table 16. Subplot (a) shows significant error increases in AR-based models, averaged across all datasets except Traffic. Subplot (b) demonstrates how the instance-level normalization impacts performance in long-term forecasting. Subplots (c) examine how trends and seasonality impact performance across all long-term forecasting datasets and horizons.

RevIN's Effectiveness in Long-term Forecasting with Exceptions.RevIN significantly enhances AR-based models in long-term forecasting, as shown in Figures 2(b) and 7. Notably, on the ETTh1 dataset, GRU NVP (w/ RevIN) even outperforms PatchTST (w/ RevIN). While RevIN offers substantial improvements for most models across most datasets, it brings negative impact on the Traffic dataset. The Traffic dataset features strong seasonality but minimal trends, thus we speculate that the major distribution shift addressed by RevIN is related to normalizing the effect of trending. These findings indicate that normalizing the trending effect could be a direction to alleviate error accumulation of AR-based models in long-term forecasting. However, we also observe that RevIN does not seem to be an ideal match for the NAR probabilistic model. For instance, CSDI (w/ RevIN) performs worse than CSDI (w/ Scaling) on the Weather dataset. Further research in developing effective normalization strategies for NAR probabilistic models is necessary.

No Dominating Normalization Strategies in Short-term Forecasting.While RevIN is effective for long-term scenarios, it does not adequately address the challenges faced by short-term probabilistic models. As shown in Table 14, RevIN fails to consistently deliver significant improvements for models such as CSDI, TimeGrad, and GRU NVP in short-term forecasting. The mean scaling strategy has proven to be the most reliable option for these models, explaining its widespread use. Although omitting instance-level normalization is occasionally acceptable, it can lead to significant issues, as seen with TimeGrad (without normalization) on the Wikipedia and Solar datasets, and GRU NVP (without normalization) on Electricity. We provide detailed analysis in Appendix D.1.

### Analyzing Foundation Models for Universal Time-series Forecasting

We next explore the capabilities of recent foundation models in universal time-series forecasting, focusing on their performance across different prediction horizons and their ability to estimate distributions, especially regarding their zero-shot transfer capabilities on unseen datasets. Table 11 showcases their significant progress, sometimes outperforming traditional models without re-training. Using the analytic framework of ProbTS, we delve into their methodological pros and cons.

Navigating the AR Decoding Challenge over Extended Forecasting HorizonsFigure 3(a) illustrates how the performance of various time-series foundation models evolves in relation to expanding forecasting horizons. It is evident that for shorter horizons, AR-based foundation models such as TimesFM and Timer exhibit highly competitive performance, on par with NAR-based models like MOIRAI. However, the advantage of NAR-based decoding becomes increasingly apparent as the forecasting horizon lengthens, as demonstrated by the widening performance gap between TimesFM and MOIRAI. This trend is consistent with our earlier observation that without proper normalization strategies, AR-based methods could suffer from significant error accumulation when applied to long-term time-series forecasting. Given the inherent strengths of AR decoding, such as its superiority at capturing strong seasonality and its robust performance in certain short-term forecasting

Figure 4: We evaluate the efficacy of time-series foundation models for various forecasting horizons and distributional estimation. Subplot (a), derived from Table 11 and excluding results from the Electricity dataset, demonstrates the short-term forecasting capabilities and long-term error accumulation of AR-based models. Subplot (b), draw from Table 12, investigates short-term distributional estimation, highlighting the performance challenges of foundation models compared to CSDI in handling complex data distributions. Note that we include MOIRAI with two different context lengths, 96 and 5000, as context length significantly affects its transfer performance.

scenarios, it is clear that further research is warranted to explore ways to overcome its limitations in long-term forecasting contexts. This could potentially unlock new avenues for enhancing the versatility and effectiveness of AR-based time-series foundation models across a broader range of forecasting horizons.

The Critical Role of Addressing Complex Data DistributionsFigure 3(a) illustrates the incremental changes in CRPS among leading probabilistic time-series foundation models, such as MOIRAI and Chronos, compared to the best-performing short-term probabilistic model, CSDI. It becomes apparent that in scenarios characterized by complex data distributions, indicated by higher non-Gaussianity scores, the performance decline of MOIRAI in relation to CSDI becomes notably more pronounced. This phenomenon may be attributed to MOIRAI's approach to supporting distributional forecasting, which involves utilizing a mixture of predefined distribution heads. While this method is efficient and effective for certain applications, it may lack the expressiveness required to accurately model more complex data distributions. Furthermore, these observations underscore that, in specific contexts, foundation models might not be able to fully replace traditional models that have been specifically tailored and trained for particular domains. Additionally, the prospect of fine-tuning these foundation models as a remedy is less economically viable, primarily due to their significantly larger size. This highlights the importance of not only continuing to refine foundation models to enhance their adaptability and performance across a wide spectrum of data distributions but also recognizing the continued relevance of domain-specific models, especially for handling intricate data distributions where a more nuanced approach may be necessary.

## 5 Conclusion

In this study, we introduced ProbTS, a benchmark tool tailored for evaluating essential forecasting needs, which facilitates a detailed comparison of various state-of-the-art models in the context of time-series forecasting. Through our comprehensive analysis, we identified significant challenges and opportunities in the realm of time-series forecasting, particularly highlighting the need for models that can effectively address both point and probabilistic forecasting across diverse horizons.

LimitationsWhile our study represents a significant step forward in understanding and evaluating time-series forecasting models, it does come with many limitations. A predominant focus of our work is on empirical analysis, relying heavily on intuitions and experimental observations, which may lack the depth that theoretical foundations could provide. Additionally, our exploration, though extensive, might not encompass all the nuanced factors that influence model performance. By concentrating on major methodological decisions such as AR versus NAR decoding schemes, we may inadvertently overlook other critical aspects that could play a decisive role in forecasting accuracy. Moreover, the datasets employed for evaluation, despite their diversity and relevance to current research threads, may not fully capture the vast spectrum of real-world forecasting challenges. This limitation is particularly pronounced when comparing different foundation models, as their pre-training often involves an even broader array of data, potentially skewing the comparative analysis.

Future DirectionsThe insights derived from our study open the door to numerous promising research directions. Addressing the shortcomings of AR and NAR decoding schemes, especially in their application across varying forecasting horizons, emerges as a critical area for future exploration. Innovating effective architecture designs that can navigate the intricacies of short-term forecasting challenges and devising efficient methods for long-term probabilistic forecasting stand out as urgent needs. For AR-based models, reducing error accumulation remains essential, with ReVIN-style normalization showing potential for improving long-term forecasting. Additionally, exploring effective normalization strategies for NAR-based probabilistic models is an underdeveloped yet promising area. Equally important is the enhancement of models' abilities to characterize complex data distributions, which could significantly improve the adaptability and effectiveness of foundation models. Beyond these technical endeavors, expanding the scope of datasets used for evaluation to encompass a wider range of real-world scenarios will be crucial for validating the robustness and versatility of future forecasting models. Lastly, integrating theoretical insights with empirical findings could provide a more holistic understanding of model behaviors, contributing to the development of more sophisticated and nuanced forecasting solutions.