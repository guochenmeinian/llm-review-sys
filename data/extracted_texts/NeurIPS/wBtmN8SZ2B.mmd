# Learning Structured Representations

with Hyperbolic Embeddings

 Aditya Sinha

University of Illinois, Urbana-Champaign

as146@illinois.edu

&Siqi Zeng

University of Illinois, Urbana-Champaign

siqi6@illinois.edu

&Makoto Yamada

Okinawa Institute of Science and Technology

makoto.yamada@oist.jp

Authors contributed equally.

###### Abstract

Most real-world datasets consist of a natural hierarchy between classes or an inherent label structure that is either already available or can be constructed cheaply. However, most existing representation learning methods ignore this hierarchy, treating labels as permutation invariant. Recent work  proposes using this structured information explicitly, but the use of Euclidean distance may distort the underlying semantic context . In this work, motivated by the advantage of hyperbolic spaces in modeling hierarchical relationships, we propose a novel approach HypStructure: a Hyperbolic Structured regularization approach to accurately embed the label hierarchy into the learned representations. HypStructure is a simple-yet-effective regularizer that consists of a hyperbolic tree-based representation loss along with a centering loss. It can be combined with any standard task loss to learn _hierarchy-informed_ features. Extensive experiments on several large-scale vision benchmarks demonstrate the efficacy of HypStructure in reducing distortion and boosting generalization performance, especially under low-dimensional scenarios. For a better understanding of structured representation, we perform an eigenvalue analysis that links the representation geometry to improved Out-of-Distribution (OOD) detection performance seen empirically. The code is available at https://github.com/uiuctml/HypStructure.

## 1 Introduction

Real-world datasets, such as ImageNet  and CIFAR , often exhibit a natural hierarchy or an inherent label structure that describes a structured relationship between different classes in the data. In the absence of an existing hierarchy, it is often possible to cheaply construct or infer this hierarchy from the label space directly . However, the majority of existing representation learning methods  treat the labels as permutation invariant, ignoring this semantically-rich hierarchical label information. Recently, Zeng et al.  offer a promising approach to embed the tree-hierarchy explicitly in representation learning using a tree-metric-based regularizer, leading to improvements in generalization performance. The approach uses a computation of shortest paths between two classes in the tree hierarchy to enforce the same structure in the feature space, by means of a **C**ophenetic **C**orrelation **C**officient (CPCC)  based regularizer. However, their approach uses the \(_{2}\) distance in the Euclidean space, distorting the parent-child representations in the hierarchy  owing to the bounded dimensionality of the Euclidean space .

Hyperbolic geometry has recently gained growing interest in the field of representation learning [66; 67]. Hyperbolic spaces can be viewed as the continuous analog of a tree, allowing for embedding tree-like data in finite dimensions with minimal distortion [44; 73; 75; 24]. Unlike Euclidean spaces with zero curvature and spherical spaces with positive curvature, the hyperbolic spaces have negative curvature enabling the length to grow exponentially with its radius. Owing to these advantages, hyperbolic geometry has been used for various applications such as natural language processing [52; 73; 16], image classification [40; 103; 18], object detection [46; 21], action retrieval , and hierarchical clustering . However, the aim of using hyperbolic geometry in these approaches is often to _implicitly_ leverage the hierarchical nature of the data.

In this work, given a label hierarchy, we argue that accurately and _explicitly_ embedding the hierarchical information into the representation space has several benefits, and for this purpose, we propose HypStructure, a hyperbolic label-structure based regularization approach that extends the proposed methodology in  for semantically structured learning in the hyperbolic space. HypStructure can be easily combined with any standard task loss for optimization, and enables the learning of discriminative and _hierarchy-informed_ features. In summary, our contributions are as follows:

* We propose HypStructure and demonstrate its effectiveness in the supervised hierarchical classification tasks on three real-world vision benchmark datasets, and show that our proposed approach is effective in both training from scratch, or fine-tuning if there are resource constraints.
* We qualitatively and quantitatively assess the nature of the learned representations and demonstrate that along with the performance gains, using HypStructure as a regularizer leads to more interpretable as well as tree-like representations as a side benefit. The low-dimensional representative capacity of hyperbolic geometry is well-known , and interestingly, we observe that training with HypStructure allows for learning extremely low-dimensional representations with distortion values lower than even their corresponding high-dimensional Euclidean counterparts.
* We argue that representations learned with an underlying hierarchical structure are beneficial not only for the in-distribution (ID) classification tasks but also for Out-of-distribution (OOD) detection tasks. We empirically demonstrate that learning ID representations with HypStructure leads to improved OOD detection on 9 real-world OOD datasets without sacrificing ID accuracy .
* Inspired by the improvements in OOD detection, we provide a formal analysis of the eigenspectrum of the in-distribution _hierarchy-informed_ features learned with CPCC-style structured regularization methods, thus leading to a better understanding of the behavior of structured representations in general.

## 2 Preliminaries

In this section, we first provide a background of structured representation learning and then discuss the limited representation capacity of the Euclidean space for hierarchical information, which serves as the primary motivation for our work.

### Background

Structured representation learning  breaks the permutation invariance of flat representation learning by incorporating a hierarchical regularization term with a standard classification loss. The regularization term is specifically designed to enforce class-conditioned grouping or partitioning in the feature space, based on a given hierarchy.

More specifically, given a weighted tree \(=(V,E,e)\) with vertices \(V\), edges \(E\) and edge weights \(e\), let us compute a tree metric \(d_{}\) for any pair of nodes \(v,v^{} V\), as the weighted length of the shortest path in \(\) between \(v\) and \(v^{}\). For a real world dataset \(=\{(_{i},y_{i})\}_{i=1}^{N}\), we can specify a _label tree_\(\) where a node \(v_{i} V\), \(v_{i}\) corresponds to a subset of classes, and \(_{i}\) denote the subset of data points with class label \(v_{i}\). We denote dataset distance between \(_{i}\) and \(_{j}\) as \((v_{i},v_{j})=d(_{i},_{j})\), where \(d(,)\) is any distance metric in the feature space, varied by design.

With a collection of tree metric \(d_{}\) and dataset distances \(\), we can use the Cophenetic Correlation Coefficient (CPCC) , inherently a Pearson's correlation coefficient, to evaluate the correspondence between the nodes of the tree, and the features in the representation space. Let \(}},\) denote the mean of the collection of distances, then CPCC is defined as

\[(d_{},):=(d_{}(v_{i},v_{j} )-}})((v_{i},v_{j})-)}{(_{i<j} (d_{}(v_{i},v_{j})-}})^{2})^{1/2}(_{i<j }((v_{i},v_{j})-)^{2})^{1/2}}.\] (1)

For the supervised classification task, we consider the training set \(_{}^{}=\{(_{i},y_{i})\}_{i=1}^{N}\) and we aim to learn the network parameter \(\) for a feature encoder \(f_{}:\), where \(^{d}\) denotes the representation/feature space. For structured representation learning, the feature encoder is usually followed by a classifier \(g_{w}\), and the parameters \(,w\) are learnt by minimizing \(\) along with a standard _flat_ (non-hierarchical) classification loss, for instance, Cross-Entropy (CE) or Supervised Contrastive (SupCon)  loss, with the structured regularization term as:

\[()=_{(,y)}_{}( {x},y,,w)-(d_{},).\] (2)

Using a composite objective as defined in Equation (2), we can enforce the distance relationship between a pair of representations in the feature space, to behave similarly to the tree metric between the same vertices. For instance, consider a simple label tree with a root node, a coarse level, and a fine level, where subsets of fine classes share the same coarse parent. For this hierarchy, we would expect the fine classes of the same parents (e.g., _apple_ and _banana_ are _fruits_) to have closer representations in the feature space, whereas fine classes with different coarse parents (e.g., an _apple_ is a _fruit_ and a _tulip_ is a _flower_) should be further apart. The learned structure-informed representations reflect these hierarchical relationships and lead to interpretable features with better generalization .

### \(_{2}\)-Cpcc

Equation (1) offers the flexibility of designing a metric to measure the similarity between two data subsets, and  define the _Euclidean dataset distance_ as \(_{_{2}}(_{i},_{j}):=\|_{i} |}_{_{i}}f()-_{j}|}_{ ^{}_{j}}f(^{})\|_{2}\). The distance between datasets is thus the \(_{2}\) distance between two Euclidean _centroids_ of their class-conditioned representations, which is unsuitable for modeling _tree-like_ data . Additionally, this regularization approach in  is applied only to the leaf nodes of \(\) for efficiency. However, this leaf-only formulation of the CPCC offers an _approximation_ of the structured information, since the distance between non-leaf nodes is not restricted explicitly by the regularization. This approximation, therefore, leads to a loss of information contained in the original hierarchy \(\). Actually, it is impossible to embed \(d_{}\) into \(_{2}\) exactly. Or more formally, there exists no bijection \(\) such that \(d_{}((_{i}),(_{j}))=\|_{i}-_{j }\|_{2}\) irrespective of how large the feature dimension \(d\) is. We provide two such examples for a toy label tree in Figure 1, below.

**Example 1.** We intend to embed all nodes in \(\), including purple internal nodes. Notice that \(G,C,D,E\) is a star graph centered at \(G\). Since \(CG=DG=1,CD=2\), by triangle inequality \(C,D,G\) must be on the same line where \(G\) is the center of \(CD\). Similarly, \(G\) must be at the center of \(DE\). Hence, the location of \(E\) must be at \(C\), which contradicts the uniqueness of all nodes in \(\).

**Example 2.** As an easier problem, let us only embed leaf nodes into the Euclidean space as shown in Figure 1. Since \(CD=DE=CE=2\), they must be on a plane with an equilateral triangle \(_{CDE}\) in Euclidean geometry. Then all the green classes have the same distance \(4\) to each yellow class. Therefore, \(A,B\) must be on the line the plane with \(O\), which is the barycenter of \(_{CDE}\). Due to the uniqueness and symmetry of \(A,B\), we must have \(AO=BO=1\) to satisfy \(AB=2\). \(AO=1,OE=}{3},AE=4\) which contradicts the Pythagorean Theorem.

Since we cannot embed an arbitrary tree \(\) into \(_{2}\) without distortion, it would also affect the optimization of the \(_{2}\)-CPCC in a classification problem, where the tree weights encode knowledge

Figure 1: (left) An unweighted label tree with two coarse nodes: \(F\), \(G\). \(F\) contains two fine classes \(A,B\) and \(G\) contains three fine classes \(C,D,E\). We cannot embed this in \(_{2}\) exactly (right).

of class similarity. To verify our claims, we consider the optimization of \(512\)-dimensional \(_{2}\)-CPCC structured representations for CIFAR10 . The CIFAR10 dataset consists of a small label hierarchy as shown in Figure 2 (left). The optimal CPCC is achieved when each tree metric value corresponds to a single \(_{_{2}}\). However, in Figure 2 (right), even with an optimization of the \(_{2}\)-CPCC loss for the entire tree, we observe a sub-optimal train CPCC less than \(1\), where the distance between two coarse nodes, _transportation_ and _animal_, is far away from the desired solution. Furthermore, optimization of the CPCC loss for only the leaf nodes, leads to an even larger distortion of the tree metrics.

## 3 Methodology

Hyperbolic spaces are more suitable for embedding hierarchical relationships with low distortion  and low dimensions. Hence, motivated by the aforementioned challenges, we propose a **Hyperbolic Structured** regularizer for _hierarchy-informed_ representation learning. We begin with the basics of hyperbolic geometry, followed by the detailed methodology of our proposed approach.

### Hyperbolic Geometry

Hyperbolic spaces are non-Euclidean spaces with negative curvature where given a fixed point and a line, there exist infinitely many parallel lines that can pass through this point. There are several commonly used isometric hyperbolic models . For this work, we mainly use the Poincare Ball model.

**Definition 3.1** (Manifold).: A _manifold_\(\) is a set of points \(\) that are locally Euclidean. Every point \(\) of the manifold \(\) is attached to a _tangent space_\(_{}\), which is a vector space over the reals of the same dimensionality as \(\) that contain all the possible directions that can tangentially pass through \(\).

**Definition 3.2** (Poincare Ball Model).: Given \(c\) as a constant, the Poincare ball model \((_{c}^{d},_{B})\) is defined by a manifold of an open ball \(_{c}^{d}=\{^{d}:c\|\|^{2}<1\}\) and metric tensor \(_{B}\) that defines an inner product of \(_{}_{c}^{d}\). The model is equipped with the distance  as

\[d_{_{c}}(_{1},_{2})=}^{-1}( \|_{1},_{2}+c \|_{2}\|^{2})_{1}+(1-c\|_{1}\|^{2})_{2}}{1-2c _{1},_{2}+c^{2}\|_{1}\|^{2}\|_{2}\|^{2}} \|).\] (3)

For \(c 0\), we can recover the properties of the Euclidean geometry since \(_{c 0}d_{_{c}}(_{1},_{2})=2\|_{1}-_{2}\|\). Since \(_{}_{c}^{d}\) is isomorphic to \(^{d}\), we can connect vectors in Euclidean space and hyperbolic space with the bijection between \(_{}_{c}^{d}\) and \(_{c}^{d}\). For \(=\), the _exponential map_\(_{}^{c}:_{}_{c}^{d}_{c}^{d}\) and _logarithm map_\(_{}^{c}:_{c}^{d}_{}_{c}^{d}\) have the closed form of

\[_{}^{c}()=(\|\|)}{ \|\|},_{}^{c}()=}^{-1 }(\|\|)}{\|\|}.\] (4)

Alternatively, to guarantee the correctness of Poincare distance computation, we can also process any Euclidean vector with a clipping module 

\[^{c}()=,&\|\|<1/\\ (}-)}{\|\|},&,\] (5)

so the clipped vector is within the Poincare disk. We set \(\) as a small positive number in practice.

Figure 3: Lines on different models for \(2\)-dimensional hyperbolic space.

Figure 2: Using \(_{2}\)-CPCC for structured representation on CIFAR10. CIFAR10 hierarchy (left) has a three level structure with \(13\) vertices. For a \(512\)-dimensional embedding, we apply \(_{2}\)-CPCC either for the full tree (middle) or the leaf nodes only (right) and plot the ground truth tree metric against pairwise Euclidean centroid distances of the learnt representation. The optimal train CPCC is \(1\).

**Definition 3.3** (Klein Model).: Klein model \((_{c}^{d},_{K})\) consists of an \(1/\)-radius open ball \(_{c}^{d}=\{^{d}:c\|\|^{2}<1\}\) and a metric tensor \(_{K}\) different from \(_{B}\). Similar to the mean computation in Euclidean space, let \(_{i}=1/_{i}\|^{2}}\), the Einstein midpoint of a group of Klein vectors \(_{1},_{n}_{c}^{d}\) has a simple expression of a weighted average

\[_{K}(_{1},_{n})=_{i=1}^{n}_{i}_ {i}_{i=1}^{n}_{i}.\] (6)

We illustrate the relationship between the different hyperbolic models in Figure 3. The hyperboloid space models \(d\)-dimensional hyperbolic geometry on a \(d+1\)-dimensional space. When \(d=2\), the Klein model is the tangent plane of the hyperboloid model at \((0,0,1)\), and the Poincare disk shares the same support as the Klein disk, although shifted downwards and centered at the origin. Given a triangle on the hyperboloid model, its projection on the Klein model preserves the straight sides, but the projection of a line on the Poincare model is a part of a circular arc or the diameter of the disk. Let \(_{},_{}\) be coordinates of \(\) under Poincare and Klein model respectively, the prototype operations on \(_{c}^{d}\) require transformations between \(_{c}^{d}\) and \(_{c}^{d}\) as

\[_{}=_{}}{1+_{}\|^{2}}},_{}=_{}}{1+c\|_{ }\|^{2}}.\] (7)

For example, in Figure 3, \(O^{}\) is the HypAve\({}_{K}\) of \(A^{},B^{},C^{}\), and can be mapped back to the Poincare disk to get the Poincare prototype (HypAve\({}_{}\)) \(O\) of points \(A,B,C\) by a change of coordinates.

### HypStructure: Hyperbolic Structured Regularization

At a high level, HypStructure uses a combination of two losses: a Hyperbolic Cophenetic Correlation Coefficient Loss (HypCPCC)), and a Hyperbolic centering loss (HypCenter) for embedding the hierarchy in the representation space. Below we describe the two components of HypStructure. The pseudocode of HypStructure is shown in Algorithm 1 in Appendix B.2.

**HypCPCC (Hyperbolic Cophenetic Correlation Coefficient):** We extend the \(_{2}\)-CPCC methodology in  to the hyperbolic space in HypCPCC. Three major steps of HypCPCC are (i) map Euclidean vectors to Poincare space (ii) compute class prototypes (iii) use Poincare distance for CPCC. Specifically, we first project each \(_{i}^{d}\) to \(_{c}^{d}\), and compute the Poincare centroid for each vertex of \(\) using hyperbolic averaging as shown in Equation (6) and Equation (7). Alternatively, we can also compute Euclidean centroids \(}=_{i}|}_{_{i}} \) for each vertex, and project each \(}^{d}\) to \(_{c}^{d}\) either by \(_{0}^{}\) or clip\({}^{c}\). After the computation of hyperbolic centroids, we use the pairwise distances between all vertex pairs in \(\) in the Poincare ball, to compute the HypCPCC loss using Equation (1) by setting \(=d_{_{c}}\).

**HypCenter (Hyperbolic Centering):** Inspired by Sarkar's low-distortion construction  that places the root node of a tree at the origin, we propose a centering loss for this positioning, that enforces the representation of the root node to be close to the center of the Poincare disk, and the representations of its children to be closer to the border of Poincare disk. We enforce this constraint by minimizing the norm of the hyperbolic representation of the root node as \(_{}=\|_{B}(_{0}^{}(_{1}), ,_{0}^{}(_{N}))\|\). Alternatively, for centroids computed in the Euclidean space and mapped to the Poincare disk, we minimize \(_{}=\|1/N_{i=1}^{N}f_{}(_{i})\|\) directly due to the monotonicity of \(()\) in the exponential map.

Finally, for \(,>0\), we can learn the _hierarchy-informed_ representations by minimizing

\[()=_{(,y)}_{}( {x},y,)-(d_{},d_{_{c}})+ _{}(,),\] (8)

where \(_{}\) is a standard classification loss, such as the CE loss or the SupCon loss.

**Time Complexity:** In a batch computation setting with a batch size \(b\) and the number of classes (leaf nodes) as \(k\), the computational complexity for a HypStructure computation to embed the full tree will still be \(O(d\{b^{2},k^{2}\})\), which is the same as the complexity of a Euclidean _leaf-only_ CPCC. The additional knowledge gained from internal nodes allows us to reason about the relationship between higher-level concepts, and the hyperbolic representations help in achieving a low distortion of hierarchical information for better performance in downstream tasks.

Experiments

We conduct extensive experiments on several large-scale image benchmark datasets to evaluate the performance of HypStructure as compared to the Flat and \(_{2}\)-CPCC baselines for hierarchy embedding, classification, and OOD detection tasks.

Datasets and SetupFollowing the common benchmarks in the literature, we consider three real-world vision datasets, namely CIFAR10, CIFAR100  and ImageNet100  for training, which vary in scale, number of classes, and number of images per class. We construct the ImageNet100 dataset by sampling 100 classes from the ImageNet-1k  dataset following . For CIFAR100, a three-level hierarchy is available with the dataset release . Since no hierarchy is available for the CIFAR10 and ImageNet100 datasets, we construct a hierarchy for CIFAR10 manually in Figure 2. For ImageNet100, we create a subtree from the WordNet  given 100 classes as leaves. More details regarding the datasets, network, training and setup are provided in the Appendix B.4.

### Quality of Hierarchical Information

First, to assess the _tree-likeness_ of the learnt representations, we measure the Gromov's hyperbolicity \(_{rel}\) of the features in Table 1. Lower \(_{rel}\) indicates higher tree-likeness and a perfect tree metric space has \(_{rel}=0\) (more details in Appendix B.5). To also evaluate the correspondence of the feature distances with ground truth tree metrics, we compute CPCC on test sets. We observe that HypStructure reduces distortion of hierarchical information over Flat by upto **59.4%** and over \(_{2}\)-CPCC by upto **45.4%**, while also consistently improving the test CPCC for most datasets.

We also perform a qualitative analysis of the learnt representations from HypStructure on the CIFAR10 dataset, and visualize them in a Poincare disk using UMAP  in Figure 4(a). We can observe clearly that the samples for fine classes arrange themselves in the Poincare disk based on the hierarchy tree as seen in Figure 2, being closer to the classes which share a _coarse_ class parent.

To examine the impact of feature dimension on the representative capacity of the hyperbolic space, we vary the feature dimension for HypStructure and compute the \(_{rel}\) for each learnt feature. Comparing the distortion of features with the Flat and \(_{2}\)-CPCC settings in Figure 4, we observe that \(_{rel}\) decreases consistently with increasing dimensions, implying that high dimension features using HypStructure are more tree-like, and better than Flat and \(_{2}\)-CPCCs' \(512\)-dimension baselines.

### Classification

Following , we treat leaf nodes in the hierarchy as _fine_ classes and their parent nodes as _coarse_ classes. To evaluate the quality of the learnt representations, we perform a classification task on the fine and coarse classes using a kNN-classifier following  and report the performance on the three datasets in Table 1. We observe that HypStructure leads to upto **2.2%** improvements over Flat and upto **0.8%** improvements over \(_{2}\)-CPCC on both fine and coarse accuracy. We also visualize the learnt test features from Flat vs HypStructure on the CIFAR100 dataset using Euclidean t-SNE  and show the visualizations in Figure 4(b) and Figure 4(c) respectively. We observe that HypStructure leads to sharper and more discriminative representations in Euclidean space. Additionally, we see that the fine classes belonging to a coarse class (the same shades of colors)

  
**Dataset** & **Method** &  &  \\ (**Backbone**) & & \(_{rel}\) (\(\)) & **CPCC (\(\))** & **Fine (\(\))** & **Coarse (\(\))** \\   & Flat & 0.232 (0.001) & 0.573 (0.002) & 94.64 (0.12) & 99.16 (0.04) \\  & \(_{2}\)-CPCC & 0.174 (0.002) & 0.966 (0.001) & 94.72 (0.13) & 98.91 (0.02) \\  & HypStructure & **0.094** (0.003) & **0.929** (0.001) & **94.729** (0.104) & **99.918** (0.04) \\   & Flat & 0.209 (0.002) & 0.534 (0.119) & 74.96 (0.14) & 84.15 (0.19) \\  & \(_{2}\)-CPCC & 0.213 (0.006) & **0.779** (0.002) & 76.70 (0.19) & 85.28 (0.32) \\  & HypStructure & **0.127** (0.016) & **0.065** (0.007) & **76.68** (0.022) & **86.60** (0.13) \\   & Flat & 0.168 (0.003) & 0.429 (0.002) & 90.01 (0.07) & 90.77 (0.11) \\  & \(_{2}\)-CPCC & 0.213 (0.009) & 0.834 (0.002) & 89.57 (0.38) & 90.34 (0.28) \\   & HypStructure & **0.134** (0.001) & **0.841** (0.001) & **90.12** (0.01) & **90.84** (0.02) \\   &  & & & & \\ 

Table 1: Evaluation of hierarchical information distortion and classification accuracy using SupCon  as \(_{}\). All metrics are reported as mean (standard deviation) over 3 seeds.

Figure 4: Evaluation of distortion vs feature dimensions for HypStructure.

which are semantically closer in the label hierarchy, are grouped closer and more compactly in the feature space as well, as compared to Flat. We also perform evaluations using the linear evaluation protocol  and observe an identical trend in the accuracy, we report these results in Appendix C.1.

### OOD Detection

In addition to leveraging the hierarchy explicitly for the purpose of learning tree-like ID representations, we argue that a structured separation of features in the hyperbolic space as enforced by HypStructure is helpful for the OOD detection task as well. To verify our claim, we perform an exhaustive evaluation on 9 real-world OOD datasets and demonstrate that HypStructure leads to improvements in the OOD detection AUROC. We share more details below.

#### 4.3.1 Problem Setting

Out-of-distribution (OOD) data refers to samples that do not belong to the in-distribution (ID) and whose label set is disjoint from \(^{}\) and therefore should not be predicted by the model. Therefore the goal of the OOD detection task is to design a methodology that can solve a binary problem of whether an incoming sample \(\) is from \(P_{}\) i.e. \(y^{}\) (ID) or \(y^{}\) (OOD).

**OOD datasets** We evaluate on 5 OOD image datasets when CIFAR10 and CIFAR100 are used as the ID datasets, namely SVHN, Places365, Textures, LSUN, and iSUN, and 4 large scale OOD test datasets, specifically SUN, Places365, Textures and iNaturalist when ImageNet100 is used as the ID dataset. This subset of datasets is prepared by  and is created with overlapping classes from ImageNet-1k removed from these datasets to ensure there is no overlap in the distributions.

**OOD detection scores** While several scores have been proposed for the task of OOD detection, we evaluate our proposed method using the Mahalanobis score , computed by estimating the mean

Figure 6: Left: Hyperbolic UMAP visualization of CIFAR10’s HypStructure representation on Poincaré disk. Middle and Right: t-SNE visualization of learnt representations on CIFAR100.

Figure 7: Left: OOD detection score across various datasets on the CIFAR100 ID dataset. Right: Hyperbolic UMAP of the CIFAR100 (ID) test vs SVHN (OOD) test features learnt from HypStructure with a clear separation in the Poincaré disk.

[MISSING_PAGE_FAIL:8]

As seen in Figure 6(a), we observe a significant improvement in the OOD detection performance using HypStructure with the Mahalanobis score eq. (9). After training a composite loss with CPCC till convergence, let us denote the matrix of the normalized in-distribution trained feature as \(Z^{n d}\). Naturally, we inspect the eigenvalue properties of \(\) (i.e, \(Z\)), and observe that \(K=ZZ^{}^{n n}\) exhibits a hierarchical block structure (Figure 7(a)) where the diagonal blocks have a significantly higher correlation than other off-diagonal values, leading us to the following theorem.

**Theorem 5.1** (**Eigenspectrum of Structured Representation with Balanced Label Tree**).: _Let \(\) be a balanced tree with height \(H\), such that each level has \(C_{h}\) nodes, \(h[0,H]\). Let us denote each entry of \(K\) as \(r^{h}\) where \(h\) is the height of the lowest common ancestor of the row and the column sample. If \(r^{h} 0, h\), then: (i) For \(h=0\), we have \(C_{0}-C_{1}\) eigenvalues \(_{0}=1-r^{1}\). (ii) For \(0<h H-1\), we have \(C_{h}-C_{h+1}\) eigenvalues \(_{h}=_{h-1}+(r^{h}-r^{h+1})}{C_{h}}\). (iii) The last eigenvalue is \(_{H}=_{H-1}+C_{0}r^{H}\)._

We defer the eigenspectrum analysis for an arbitrary label tree to Appendix A. Theorem 5.1 implies a **phase transition pattern** in the eigenspectrum. There always exists a significant gap in the eigenvalues representing each level of nodes in the hierarchy, and the eigenvalues corresponding to the coarsest level are the highest in magnitude. CIFAR100 has a balanced three-level label hierarchy where each coarse label has five fine labels as its children. In Figure 7(b), we visualize the eigenspectrum of CIFAR100 for HypStructure, \(_{2}\)-CPCC and the Flat objective. We observe a significant drop in the eigenvalues for features learnt from two hierarchical regularization approaches, \(_{2}\)-CPCC and HypStructure, at approximately the 20th largest eigenvector (which corresponds to the number of coarse classes), whereas these phase transitions do not appear for standard flat features. We also observe that the magnitude of coarse eigenvalues are approximately at the same scale.

In summary, Theorem 5.1 helps us to formally characterize the difference between flat and structured representations. CPCC style (eq. (1)) regularization methods can also be treated as dimensionality reduction techniques, where the structured features can be explained mostly by the coarser level features. For the OOD detection setting, this property differentiates the ID and OOD samples at the coarse level itself using a lower number of dimensions, and makes the OOD detection task easier. We visualize the OOD detection AUROC on SVHN (OOD) corresponding to the CIFAR100 (ID) features with the top\(-k\) principal component for different methods, in Figure 7(c). We observe that for features learnt using HypStructure, accurately embedding the hierarchical information leads to the top \(20\) eigenvectors (corresponding to the coarse classes) being the most informative for OOD detection. Recall that CIDER  is a state-of-the-art method proposed specifically for improving OOD detection by increasing inter-class dispersion and intra-class compactness. We note that CPCC style (eq. (1)) methods can be seen as a generalization of CIDER on higher-level concepts, where the same rules are applied for coarse labels as well, along with the fine classes. When the ID and OOD distributions are far enough, using coarse level feature might be sufficient for OOD detection.

## 6 Related Work

Learning with Label Hierarchy.Several recent works have explored how to leverage hierarchical information between classes for various purposes such as relational consistency , designing specific hierarchical classification architectures [101; 25; 68], hierarchical conditioning of the logits , learning order preserving embeddings , and improving classification accuracy [91; 86; 48; 49; 108; 34]. The proposed structural regularization framework in  offers an interesting approach to embed a tree metric to learn structured representations through an _explicit_ objective term, although they rely on the \(_{2}\) distance, which is less than ideal for learning hierarchies.

Hyperbolic Geometry. first proposed using the hyperbolic space to learn hierarchical representations of symbolic data such as text and graphs by embedding them into a Poincare ball. Since then, the use of hyperbolic geometry has been explored in several different applications.  proposed a hyperbolic image embedding for few-shot learning and person re-identification.  proposed hyperbolic neural network layers, enabling the development of hybrid architectures such as hyperbolic convolutional neural networks , graph convolutional networks , hyperbolic variational autoencoders  and hyperbolic attention networks . Additionally, these hybrid architectures have also been explored for different tasks such as deep metric learning [18; 100], object detection  and natural language processing . There have also been several investigations into the properties of hyperbolic spaces and models such as low distortion , small generalization error  and representation capacity . However, none of these works have leveraged hyperbolic geometry for _explicitly_ embedding a hierarchy in the representation space via structured regularization, and usually attempt to leverage the underlying hierarchy _implicitly_ using hyperbolic models.

## 7 Discussion and Future Work

In this work, we introduce HypStructure, a simple-yet-effective structural regularization framework for incorporating the label hierarchy into the representation space using hyperbolic geometry. In particular, we demonstrate that accurately embedding the hierarchical relationships leads to empirical improvements in both classification as well as the OOD detection tasks, while also learning _hierarchy-informed_ features that are more interpretable and exhibit less distortion with respect to the label hierarchy. We are also the first to formally characterize properties of _hierarchy-informed_ features via an eigenvalue analysis, and also relate it to the OOD detection task, to the best of our knowledge. We acknowledge that our proposed method depends on the availability or construction of an external hierarchy for computing the HypCPCC objective. If the hierarchy is unavailable or contains noise, this could present challenges. Therefore, it is important to evaluate how injecting noisy hierarchies into CPCC-based methods impacts downstream tasks. While the current work uses the Poincare ball model, exploring the representation trade-offs and empirical performances using other models of hyperbolic geometry in HypStructure, such as the Lorentz model  is an interesting future direction. Further theoretical investigation into establishing the error bounds of CPCC style structured regularization objectives is of interest as well.