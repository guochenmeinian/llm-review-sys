# A Unified Algorithm Framework for

Unsupervised Discovery of Skills based on

Determinantal Point Process

 Jiayu Chen

Purdue University

West Lafayette, IN 47907

chen3686@purdue.edu &Vaneet Aggarwal

Purdue University

West Lafayette, IN 47907

vaneet@purdue.edu &Tian Lan

The George Washington University

Washington, DC 20052

tlan@gwu.edu

###### Abstract

Learning rich skills under the option framework without supervision of external rewards is at the frontier of reinforcement learning research. Existing works mainly fall into two distinctive categories: variational option discovery that maximizes the diversity of the options through a mutual information loss (while ignoring coverage) and Laplacian-based methods that focus on improving the coverage of options by increasing connectivity of the state space (while ignoring diversity). In this paper, we show that diversity and coverage in unsupervised option discovery can indeed be unified under the same mathematical framework. To be specific, we explicitly quantify the diversity and coverage of the learned options through a novel use of Determinantal Point Process (DPP) and optimize these objectives to discover options with both superior diversity and coverage. Our proposed algorithm, ODPP, has undergone extensive evaluation on challenging tasks created with Mujoco and Atari. The results demonstrate that our algorithm outperforms state-of-the-art baselines in both diversity- and coverage-driven categories.

## 1 Introduction

Reinforcement Learning (RL) has achieved impressive performance in a variety of scenarios, such as games , robotic control , and transportation . However, most of its applications rely on carefully-crafted, task-specific rewards to drive exploration and learning, limiting its use in real-life scenarios often with sparse or no rewards. To this end, utilizing unsupervised option discovery - acquiring rich skills without supervision of environmental rewards by building temporal action-sequence abstractions (denoted as options), to support efficient learning can be essential. The acquired skills are not specific to a single task and thus can be utilized to solve multiple downstream tasks by implementing a corresponding meta-controller that operates hierarchically on these skills.

Existing unsupervised option discovery approaches broadly fall into two categories: (1) Variational option discovery, e.g., , which aims to improve diversity of discovered options by maximizing the mutual information  between the options and trajectories they generate. It tends to reinforce already discovered behaviors for improved diversity rather than exploring (e.g., visiting poorly-connected states) to discover new ones, so the learned options may have limited coverage of the state space. (2) Laplacian-based option discovery, e.g., , which clusters the state space using a Laplacian spectrum embedding of the state transition graph, and then learns options to connect different clusters. This approach is shown to improve the algebraic connectivity of the state space  and reduce expected covering time during exploration. However, the discovered options focus on improving connectivity between certain clusters and thus could be homogeneous and lack diversity. Note that coverage in this paper is defined as a property with respect to a single option, which can be measured as the number of state clusters traversed by an option trajectory. By maximizing coverageof each single option and diversity among different options in the meanwhile, the overall span of all options can be maximized. However, diversity and coverage may not go hand-in-hand in option discovery, as visualized in Figure 1(a)(b). Attempts such as [14; 15] have been made to address this gap, but they rely on expert datasets that contain diverse trajectories spanning the whole state spaces and lack an analytical framework for diversity and coverage of the discovered options.

This paper introduces a novel framework for unsupervised option discovery by utilizing Determinantal Point Process (DPP) to quantify and optimize both diversity and (single-option) coverage of the learned options. A DPP establishes a probability measure for subsets of a set of items. The expected cardinality, representing the average size of random subsets drawn according to the DPP, serves as a diversity measure, as it reflects the likelihood of sampling diverse items and the number of distinct items in the set. **First**, to enhance option diversity, we apply a DPP to the set of trajectories for different options. Maximizing the expected cardinality under this DPP encourages agents to explore diverse trajectories under various options. **Second**, we create another DPP for the set of visited states in a trajectory and maximize its expected cardinality. This prompts the agent to visit distant states from distinct state clusters through each trajectory, leading to higher single-option coverage. As shown in Figure 1(d), these objectives effectively measure diversity and coverage of the options. **Lastly**, to establish the option-policy mapping and so learn multiple options simultaneously, we maximize the mutual information between the option choice and related trajectories, as in variational option discovery. Rather than using the whole trajectory  or only the goal state , our solution extract critical landmark states from a trajectory via a maximum a posteriori (MAP) inference of a DPP, allowing noise mitigation while retaining vital information. **In conclusion**, our proposed framework unifies diversity and coverage in option discovery using a mathematical modeling tool--DPP. Our **key contributions** are as follows. (1) To the best of our knowledge, this is the first work to adopt DPP for option discovery. (2) The proposed unified framework enables explicit maximization of option diversity and coverage, capturing advantages of both variational and Laplacian-based methods. (3) Empirical results on a series of challenging RL tasks demonstrates the superiority of our algorithm over state-of-the-art (SOTA) baselines.

## 2 Background and Related Works

### Unsupervised Option Discovery

As proposed in , the option framework consists of three components: an intra-option policy \(:\), a termination condition \(:\{0,1\}\), and an initiation set \(I\). An option \(<I,,>\) is available in state \(s\) if and only if \(s I\). If the option is taken, actions are selected according to \(\) until it terminates according to \(\) (i.e., \(=1\)). The option framework enables learning and planning at multiple temporal levels and has been widely adopted in RL. Multiple research areas centered on this framework have been developed. Unsupervised Option Discovery aims at discovering skills that are diverse and efficient for downstream task learning without supervision from rewards, for which algorithms have been proposed for both single-agent and multi-agent scenarios [18; 19; 20; 21]. Hierarchical Reinforcement Learning [22; 23] and Hierarchical Imitation Learning [24; 25; 26], on the other hand, aim at directly learning a hierarchical policy incorporated with skills, either from interactions with the environment or expert demonstrations.

Figure 1: Illustrative example. (a) Options from variational methods have good diversity but poor (single-option) coverage of the corridors. (b) Each option from Laplacian-based methods aims to improve its coverage by visiting poorly-connected corner states but tends to be homogeneous as the others. (c) Options from our proposed method have both superior diversity and coverage. (d) The coverage and diversity measure of the options in (a)-(c) defined with DPP (i.e., Eq.(7) and (9)).

We focus on single-agent unsupervised option discovery. One primary research direction in this field is variational option discovery, which aims to learn a latent-conditioned option policy \((a|s,c)\), where \(c\) represents the option latent. Diverse options can be learned by maximizing the mutual information between the option choice \(c\) and the trajectory generated by the policy conditioned on \(c\). As shown in , this objective is equivalent to a Variational Autoencoder (VAE) . Variational option discovery can be categorized into two groups based on the structure of the VAE used: **(1)** Trajectory-first methods, such as EDL  and OPAL , follow the structure \(c\). Here, \(\) represents the agent's trajectory, while \(D\) and \(E\) denote the VAE's decoder and encoder, respectively. In this case, the policy network \((a|s,c)\) serves as the decoder \(D\). The effectiveness of these methods rely on the quality of the expert trajectory set used as the encoder's input. For instance, OPAL employed for Offline RL  assumes access to a trajectory dataset generated by a mix of diverse policies starting from various initial states. Similarly, in EDL, highly-efficient exploration strategies such as State Marginal Matching  are utilized to simulate perfect exploration and obtain a set of trajectories (\(\)) with good state space coverage for option learning. **(2)** Option-first methods, such as VIC , DIAYN , VALOR , and DADS , follow the structure \(cc\) and learn \((a|s,c)\) as the encoder \(E\). Unlike EDL and OPAL learning from "exploration experts", these methods start from a random policy to discover diverse options. However, due to challenges in exploration, they fail to expand the set of states visited by the random policy, leading to poor state space coverage. For instance, in Figure 1(a), though trajectories of different options are diverse, none of them explore the corridor. There have been more notable advancements in variational option discovery methods, as cited in [30; 31; 32; 33; 34; 35]. A detailed comparison between these methods and our algorithm, highlighting our contributions, can be found in Appendix B.

In this paper, we tackle a more challenging scenario where the agent must learn to identify diverse options and thoroughly explore the environment, starting from a random policy. This approach supports self-driven learning and exploration, rather than relying on expert policy/trajectories like trajectory-first methods. We achieve this by integrating the variational method with another key branch in unsupervised option discovery--Laplacian-based option discovery [11; 12]. This approach is based on the Laplacian spectrum of the state transition graph, which can be estimated using state transitions in the replay buffer. The state transition graph and its Laplacian matrix are formally defined in Appendix A.1. Specifically, the approach presented in  first estimates the Fiedler vector - the eigenvector associated with the second smallest eigenvalue of the Laplacian. Options are then trained to connect states with high and low values in the Fiedler vector. These states are loosely connected by "bottleneck" states. By connecting them with options, the algebraic connectivity of the state space is enhanced, and thus the exploration can be accelerated . However, these options may be homogeneous. For instance, in Figure 1(b), multiple options are found to explore the right corridor, but they all follow the same direction given by the Fiedler vector. Inspired by variational and Laplacian-based methods, we propose a new option discovery framework that unifies the optimization of diversity and coverage, through a novel use of DPP. As shown in Figure 1(c), multiple diverse options are discovered, most of which traverse the "bottleneck" states to enhance coverage.

### Determinantal Point Process

According to , given a set of items \(=\{w_{1},,w_{N}\}\), a point process \(\) on \(\) is a probability measure on the set of all the subsets of \(\). \(\) is called a Determinantal Point Process (DPP) if a random subset \(\) drawn according to \(\) has probability:

\[_{L()}(=W)=)}{_{W^{ }}(L_{W^{}})}=)}{(L+I)}\] (1)

\(I^{N N}\) is the identity matrix. \(L=L()^{N N}\) is the DPP kernel matrix, which should be symmetric and positive semidefinite. \(L_{W}^{|W||W|}\) is the sub-matrix of \(L\) indexed by elements in \(W\). Specifically, \(P_{L}(=\{w_{i}\}) L_{ii}\) and \(P_{L}(=\{w_{i},w_{j}\}) L_{ii}L_{jj}-L_{ij}^{2}\) where \(L_{ij}\) measures the similarity between item \(i\) and \(j\). Since the inclusion of one item reduces the probability of including similar items, sets consisting of diverse items are more likely to be sampled by a DPP.

The DPP kernel matrix \(L\) can be constructed as a Gram Matrix : \(L=^{T}=Diag() B^{T}B Diag ()\). \(=[q_{1},,q_{N}]^{N}\) with \(q_{i} 0\) denotes the quality measure. \(B=[}}]^{D N}\) is the stacked feature matrix where \(}^{D}\) is the feature vector corresponding to \(w_{i}\). The inner product of feature vectors, e.g., \(}T}\), is used as the similarity measure between items in \(\). From Eq. (1), we can see that \(_{L}(=W)\) is propotional to the squared \(|W|\)-dimension volume of the parallelepiped spanned by the columns of \(\) corresponding to the elements in \(W\). Diverse sets have feature vectors that are more orthogonal and span larger volumes, making them more probable.

The expected cardinality of samples from a DPP is an effective measure of the diversity of \(\) and reflects the number of modes in \(\). We provide detailed reasons why we choose the expected cardinality instead of the likelihood in Eq. (1) as the diversity measure in Appendix C.1. According to , the expected cardinality of a set sampled from a DPP can be calculated with Eq. (2), where \(_{i}^{}\) is the \(i\)-th eigenvalue of \(L()\). DPPs have found applications across a wide array of domains to promote diversity due to their unique ability to model diverse subsets, such as information retrieval [39; 40], computer vision , natural language processing [41; 42], and reinforcement learning [43; 44; 45]. This motivates us to employ DPPs in skill discovery for diversity enhancement.

\[}_{ P_{L()}}[||]=_{ i=1}^{N}^{}}{_{i}^{}+1}\] (2)

## 3 Proposed Approach

In this section, we propose a DPP-based framework that unifies the variational and Laplacian-based option discovery to get options with both superior diversity and coverage. As discussed in Section 2.1, defining an option requires relating it to its initial states \(s_{0}\), specifying its termination condition, and learning the intra-option policy. Our algorithm learns a prior network \(P_{}(c|s_{0})\) to determine the option choice \(c\) at an initial state, and an intra-option policy network \(_{}(a|s,c)\) to interact with the environment using a certain option for a fixed number of steps (i.e., the termination condition). With \(P_{}\) and \(_{}\), we can collect trajectories \(=(s_{0},a_{0},,s_{T})\) corresponding to different options.

In Section 3.1, we propose \(^{IB}\), a lower bound for the mutual information between the option and the landmark states in its trajectory, inspired by variational option discovery. By introducing the conditional variable \(c\) and maximizing \(^{IB}\), we can establish the option-policy mapping and learn multiple options simultaneously. Each option can generate specific trajectories. However, the mutual information objective only implicitly measures option diversity as the difficulty of distinguishing them via the variational decoder, and does not model the coverage. Thus, in Section 3.2, we additionally introduce explicit coverage and diversity measures based on DPP as objectives. In particular, \(^{DPP}_{1}\) measures single-agent coverage as the number of landmark states traversed by an option trajectory. Maximizing this metric encourages each option to cover a broader area in the state space. Notably, \(^{DPP}_{1}\) generalizes the Laplacian-based option discovery objectives by employing the Laplacian spectrum as the feature vector to construct the DPP kernel matrix. \(^{DPP}_{2}\) measures the diversity among trajectories of the same option. By minimizing it, the consistency these trajectories can be enhanced. \(^{DPP}_{3}\) measures the diversity among trajectories corresponding to different options, which should be maximized to improve the diversity among various options. The rationale behind introducing \(^{DPP}_{1:3}\) is to maximize both the coverage of each individual option and the diversity among various options. By doing so, we can maximize the overall span of all options, thereby fostering the formation of diverse skills that fully explore the state space.

### Landmark-based Mutual Information Maximization

Previous works tried to improve the diversity of learned options by maximizing the mutual information between the option choice \(c\) and trajectory \(\) or goal state \(s_{T}\) generated by the corresponding intra-option policy. However, the whole trajectory contains noise and the goal state only cannot sufficiently represent the option policy. In this paper, we propose to maximize the mutual information between \(c\) and landmark states \(G\) in \(\) instead. \(G\) is a set of distinct, representative states. Specifically, after clustering all states according to their features, a diverse set of landmarks with varied feature embeddings is identified to represent each different cluster. Notably, \(G\) can be extracted from \(\) through the maximum a posteriori (MAP) inference of a DPP , shown as Eq. (3) where \(\) denotes the set of states in \(\). The intuition is that these landmarks should constitute the most diverse subset of \(\) and thus should be the most probable under this DPP.

\[G=*{arg\,max}_{X}_{L()} (=X)=*{arg\,max}_{X}( =X|=L())=*{arg\,max}_{X })}{(L+I)}\] (3)In order to maximize the mutual information between \(G\) and \(c\) while filtering out the redundant information for option discovery in \(\). According to the Information Bottleneck framework , this can be realized through Eq. (4), where \(()\) is the initial state distribution, \(I()\) denotes the mutual information, and \(I_{ct}\) is the information constraint.

\[_{,}}_{s_{0}()}I(c,G|s_{0}; ,),\ s.t.,\ I(c,|s_{0};,) I_{ct}\] (4)

Equivalently, with the introduction of a Lagrange multiplier \( 0\), we can optimize:

\[_{,}}_{s_{0}()}[I(c,G|s_{0}; ,)- I(c,|s_{0};,)]\] (5)

It is infeasible to directly compute and optimize Eq. (5), so we have the following proposition.

**Proposition 1**.: _The optimization problem as Eq. (5) can be solved by maximizing \(^{IB}(,,)\):_

\[(|)+}_{s_{0},c,} [P^{DPP}(G|) P_{}(c|s_{0},G)]-} _{s_{0},c}[D_{KL}(P_{}(|s_{0},c)||Unif(|s_{0}))]\] (6)

_Here, \(s_{0}(),c P_{}(|s_{0}), P_{}(|s _{0},c)\). \((|)\) represents the entropy associated with the option choice distribution at a given state. \(P^{DPP}(G|)=(L_{G})/(L+I)\) is the probability of extracting \(G\) from \(\), under a DPP on the set of states in \(\). \(P_{}(c|s_{0},G)\) serves as a variational estimation of the posterior term \(P(c|s_{0},G)\) in \(I(c,G|s_{0})\). \(Unif(|s_{0})\) denotes the probability of sampling trajectory \(\) given \(s_{0}\) under a uniformly random walk policy._

The derivation can be found in Appendix A.2. **(1)** The first two terms of Eq. (6) constitute a lower bound of the first term in Eq. (5). \((|)\) and \(P_{}(c|s_{0},G)\) can be estimated using the output from the prior network and variational posterior network, respectively. **(2)** The third term in Eq. (6) corresponds to a lower bound of the second term in Eq. (5). Instead of directly calculating \(- I(c,|s_{0})\) which is implausible, we introduce \(Unif(|s_{0})\) to convert it to a regularization term as in Eq. (6). Specifically, by minimizing \(D_{KL}(P_{}(|s_{0},c)||Unif(|s_{0}))\), which is the KL Divergence  between the distribution of trajectories under our policy and a random walk policy, exploration of the trained policy \(_{}\) can be improved. Note that \(P_{}(|s_{0},c)=_{t=0}^{T-1}_{}(a_{t}|s_{t},c)P(s_{t+1 }|s_{t},a_{t})\), where \(P(s_{t+1}|s_{t},a_{t})\) is the transition function in the MDP.

Another challenge in calculating \(^{IB}\) is to infer the landmarks \(G\) with Eq. (3). (\(L\) is the kernel matrix of the DPP, of which the construction is further introduced in the next section.) The MAP inference problem related to a DPP is NP-hard , and greedy algorithms have shown to be promising as a solution. In this work, we adopt the fast greedy method proposed in  for the MAP inference, which is further introduced in Appendix C.2 with its pseudo code and complexity analysis.

### Quantifying Diversity and Coverage via DPP

In this section, we propose three optimization terms defined with DPP to explicitly model the coverage and diversity of the learned options. Jointly optimizing these terms with Eq. (6), the discovered options are expected to exhibit improved state space coverage and enhanced diversity.

The first term relates to improving the coverage of each option by maximizing:

\[^{DPP}_{1}(,)=}_{s_{0}}[_{c,}P_{}(c|s_{0})P_{}(|s_{0},c)f()],\ f()= }_{ P_{L()}}[||]=_{i =1}^{T+1}^{}}{_{i}^{}+1}\] (7)

where \(\) is the set of states in \(\), \(L()\) is the kernel matrix built with feature vectors of states in \(\), \(f()\) is the expected number of modes (landmark states) covered in a trajectory \(\). Thus, \(^{DPP}_{1}\) measures single-option coverage, by maximizing which we can enhance exploration of each option.

As for the kernel matrix, according to Section 2.2, we can construct \(L()\) by defining the quality measure \(\) and normalized feature vector for each state in \(\). States with higher expected returns should be visited more frequently and thus be assigned with higher quality values. However, given that there is no prior knowledge on the reward function, we assign equal quality measure to each state as 1. As for the features of each state, we define them using the Laplacian spectrum, i.e., eigenvectors corresponding to the \(D\)-smallest eigenvalues of the Laplacian matrix of the state transition graph, denoted as \([},,}]\). To be specific, for each state \(s\), its normalized feature is defined as: \((s)=[}(s),,}( s)]/^{D}(}(s))^{2}}\). The reasons for this feature design are as follows: **(1)** As shown in Spectral Clustering , states with high similarity in this feature embedding fall in the same cluster. Under a DPP with this feature embedding, the sampled states with distinct features should belong to different clusters, i.e., the landmarks. Then, by maximizing \(_{1}^{DPP}\) (i.e., the expected number of landmarks covered in \(\)), the agent is encouraged to traverse multiple clusters within the state space. **(2)** With this feature design, our algorithm generalizes of the Laplacian-based option discovery . In , they set a threshold to partition the state space into two parts - the set of states with higher values in the Fiedler vector (i.e., \(}\)) than the threshold is used as the initiation set and the other states are used as the termination set, then the option policy is trained to connect states within these two areas. We note that, as a special case of our algorithm, when setting the feature dimension \(D=2\), we can get similar options with the Laplacian-based methods through maximizing \(_{1}^{DPP}\). Given that the eigenvector corresponding to the smallest eigenvalue of a Laplacian matrix is \(}=\), states with diverse feature embeddings encoded by \([},}]\) (i.e., \(D=2\)) tend to differ in \(}\). By maximizing \(_{1}^{DPP}\), the options are trained to visit the states that are as distinct in \(}\) as possible in a trajectory, which is similar with the ones learned in . We empirically demonstrate this in Section 4. Note that the Laplacian spectrum \([},,}]\) for infinite-scale state space can be learned as a neural network through representation learning  which is introduced in Appendix C.3. Thus, our algorithm can be applied to tasks with large-scale state spaces.

Next, we expect the set of sampled trajectories related to the same option \(c\) and starting from the same state \(s_{0}\), i.e., \(_{(s_{0},c)}\), to be consistent and thus hard to be distinguished by a DPP, which is important given the stochasticity of the policy output. This is realized by minimizing the expected mode number in each \(_{(s_{0},c)}\): \((s_{0}(),\ c P_{}(|s_{0}),\ _{(s_{0},c)} P_{}(|s_{0},c))\)

\[_{2}^{DPP}(,)=,c,_{(s_{0},c)}}{}[g(_{(s_{0},c)})], \ g(_{(s_{0},c)})= P_{L()}}{}[||]=_{i=1}^{M}^{}}{ _{i}^{}+1}\] (8)

where \(\) is the set of \(M\) trajectories related to \(c\) starting at \(s_{0}\) (i.e., \(\{_{1},,_{M}\}=_{(s_{0},c)}\)), \(L()\) is the kernel matrix built with the feature vectors of each trajectory, i.e., \()}\). The feature vector of a trajectory can be obtained based on the ones of its landmark states \(G_{i}\) through the Structured DPP framework : \()}=_{s G_{i}}\). (\(\) is defined above.) Alternatively, we can use the hidden layer output of the decoder \(P_{}(c|G,s_{0})\), which embeds the information contained in \(G\). This design is commonly adopted in DPP-related works [52; 53]. We will compare these two choices in Section 4. Notably, our kernel matrix design, i.e, \(L()\) and \(L()\), is task-irrelevant and domain-knowledge-free.

Last, the set of sampled trajectories subject to different options should be diverse. This is achieved by maximizing its expected mode number (i.e., Eq. (9)). Here, \(\) is the union set of \(\) related to different options, i.e., \(_{c^{}}_{(s_{0},c^{})}\), and \(L()\) is the kernel matrix corresponding to trajectories in \(\).

\[_{3}^{DPP}(,)=,c,_{(s_{0},c)}}{}[h(}{} _{(s_{0},c^{})})],\ h(_{c^{}} _{(s_{0},c^{})})= P_{L( )}}{}[||]=_{i=1}^{K}^{ }}{_{i}^{}+1}\] (9)

### Overall Algorithm Framework

The overall objective is to maximize Eq. (10). The hyperparameters \(_{1:3} 0\) (provided in Appendix C.6) are the weights for each DPP term and can be fine-tuned to enable a tradeoff between coverage and diversity of the learned options.

\[(,,)=^{IB}(,,)+_{1 }_{1}^{DPP}(,)-_{2}_{2}^{DPP}(, )+_{3}_{3}^{DPP}(,)\] (10)Based on Eq. (10), we can calculate the gradients with respect to \(,,\), i.e., the parameters of the prior network, policy network and variational decoder, and then apply corresponding algorithms for optimization. First, \(_{}=_{}^{IB}\), so \(P_{}\) can be optimized as a standard likelihood maximization problem with SGD . Next, regarding \(\) and \(\), we have Proposition 2, which is proved in Appendix A.3. Notably, we select PPO  to update \(\) and \(\). The overall algorithm (ODPP) is summarized as Algorithm 1, where the main learning outcome is the intra-option policy \(_{}\). When applied to downstream tasks, \(_{}(a|s,c)\) can be fixed and we only need to learn a high-level policy \(P_{}(c|s)\) to select among options. We note that our algorithm is highly salable and only slightly increases the time complexity compared with previous algorithms in this field. Detailed discussion on the complexity and scalability are provided in Appendix C.4 and C.5.

**Proposition 2**.: _The gradients of the overall objective \((,,)\) with respect to \(\) and \(\) can be unbiasedly estimated using Eq. (11). Here, \(Q^{P_{}}\) and \(Q_{m}^{_{}}\) are the Q-functions for the prior and policy networks, respectively. Consequently, both networks can be trained using reinforcement learning._

\[_{}=}_{s_{0},c}[ _{} P_{}(c|s_{0})Q^{P_{}}],\;_{ }=}_{s_{0},c,}[_ {m=1}^{M}_{t=0}^{T-1}_{}_{}(a_{t}^{m}|s_{t}^{m}, c)Q_{m}^{_{}}]\] (11) \[Q^{P_{}}(c,s_{0})= - P_{}(c|s_{0})+}_{}[_{m=1}^{M}Q_{m}^{_{}}(,s_{0},c )]\] (12) \[Q_{m}^{_{}}(,c,s_{0})= (G_{m}|_{m}) P_{}(c|s_{0},G_{m})}{M}- _{t=0}^{T-1}_{}(a_{t}^{m}|s_{t}^{m},c)\] (13) \[+}{M}f(_{m})-_{2}g(_{(s_{0},c)})+_{3}h(_{c^{}}_{(s_{0},c^{})})\]

## 4 Evaluation and Main Results

In this section, we compare ODPP with SOTA baselines on a series of RL tasks. **(1)** For intuitive visualization, we test these algorithms on maze tasks built with Mujoco . In particular, we select the Point and Ant as training agents, and put them in complex 3D Mujoco Maze environments (Figure 4(a) and 4(d)). We evaluate the diversity and coverage of the options learned with different algorithms, through visualizing corresponding trajectories. Then, we provide a quantitative study to see if these options can aid learning in downstream tasks - goal-achieving or exploration tasks in the maze environments. Both tasks are long-horizon with an episode length of 500. **(2)** To show the applicability of our algorithm on a wide range of tasks, we test it on 3D Ant locomotion tasks (Figure 5(a)) and Atari video games. In this part, we focus on evaluating if the agent can learn effective skills without supervision of task-specific rewards.

As mentioned in Section 2.1, our algorithm follows the option-first variational option discovery, which starts from a random policy rather than an efficient exploration policy like the trajectory-first methods. To keep it fair, we compare our algorithm with several SOTA option-first methods: VIC , DIAYN , VALOR , DADS , and APS . Further, our algorithm integrates the variational and Laplacian-based option discovery, so we compare it with a SOTA Laplacian-based method as well: DCO . The codes are available at https://github.com/LucasCJYSDL/ODPP.

### Ablation Study

In Figure 2, we visualize trajectories of options learned with different algorithms in the Point Room task, which start from the same initial state and have the same horizon (i.e., 50 steps). Note that the visualizations in Figure 1-3 are aerial views of the 3D Mujoco Room/Corridor. As mentioned in Section 3.2, the objective of DCO is to train options that can connect states with high and low values in the Fielder vector of the state transition graph. Due to its algorithm design , we can only learn one option with DCO at a time. While, with the others, we can learn multiple options simultaneously. **(1)** From (a)-(c), we can see that variational option discovery methods can discover diverse options, but these options can hardly approach the "bottleneck" states in the environment which restricts their coverage of the state space. On the contrary, in (d), options trained with DCO can go through two "bottleneck" states but lack diversity, since they stick to the direction given by the Fiedler vector. While, as shown in (h), options learnt with our algorithm have both superior diversity and coverage. **(2)** In (e), we can already get significant better options than the baselines by only using \(^{IB}\) as the objective. From (e) to (f), it can be observed that additionally introducing the objective term \(_{1}^{DPP}\) can encourage options to cover more landmark states. From (f) to (h), we further add \(_{2}^{DPP}\) and \(_{3}^{DPP}\), which makes the option trajectories more distinguishable from the view of DPP. Also, in (g) and (h), we adopt different designs of the trajectory feature. It shows that using trajectory features defined with the Structured DPP framework (introduced in Section 3.2) is better than using the hidden layer output of the decoder \(P_{}\). These results show the effectiveness of each component in our algorithm design and demonstrate that our algorithm can construct options with higher diversity and coverage than baselines. In Appendix D.3, we provide quantitative results to further show the improvement brought by each objective term of ODPP.

Next, as claimed in Section 3.2, if setting the feature dimension \(D=2\), ODPP is expected to learn similar options with DCO, thus ODPP generalizes the Laplacian-based method. In Figure 3, we visualize the value of each state in the Fiedler vector as the background color (the darker the higher), and the options learned with DCO and ODPP in the Point Corridor task starting from different locations. We can observe from (a) and (b) that most of the learned options are similar both in direction and length. Further, if we adopt the normal setting, where \(D=30\) and the number of options to learn at a time is 10, we can get diverse options shown as (c), which can be beneficial for various downstream tasks. In this case, ODPP can be viewed as a generalization and extension of DCO through variational tools (e.g., \(^{IB}\)) to learn multiple diverse options at a time.

### Evaluation in Downstream Tasks

In Figure 4, we evaluate the options learned with different algorithms on a series of downstream tasks. These options are trained without task-specific rewards, and thus potentially applicable to different downstream tasks in the environment. Firstly, we test the options in Point Room/Corridor goal-achieving tasks where a point agent is trained to achieve a certain goal (i.e., red points in Figure 4(a) and 4(d)). This task is quite challenging since: (1) The location of the goal is not included in the observation. (2) The agent will get a positive reward only when it reaches the goal area; otherwise, it will receive a penalty. Hence, the reward setting is highly sparse and delayed, and the agent needs to fully explore the environment for the goal state. In Figure 4(b)-4(c), we compare the mean and standard deviation of the performance (i.e., mean return of a decision step) of different algorithms

Figure 3: (a) Options learned with DCO; (b) Options learned with ODPP, when setting the feature dimension as 2; (c) Options learned with ODPP in the normal setting.

Figure 2: (a) VIC, (b) DIAYN, (c) VALOR, (d) DCO, (e) ODPP (\(^{IB}\)), (f) ODPP (\(^{IB},_{1}^{DPP}\)), (g) ODPP using trajectory features defined by hidden layer output, (h) ODPP using trajectory features defined with the Structured DPP framework.

in the training process, which is repeated four times (each time with a different goal). It can be observed that, with options learned by ODPP, the convergence speed and return value can be much higher. Note that the pretrained options are fixed for downstream tasks and we only need to learn an option selector \(P_{}(c|s)\) which gives out option choice \(c\) at state \(s\). In this way, we can simplify a continuous control task to a discrete task, and the advantage can be shown through the comparison with using PPO (i.e., "Base RL") directly on the task. To keep it fair, the PPO agent is pretrained for the same number of episodes as the option learning. Moreover, we evaluate these algorithms in Ant Room/Corridor exploration tasks where an Ant agent is trained to explore the areas as far from the start point (center) as possible. The reward for a trajectory is defined with the largest distance that the agent has ever reached during this training episode. In Figure 4(e)-4(f), we present the change of the trajectory return during the training process of these algorithms (repeated five times with different random seeds). The options trained with ODPP provide a good initialization of the policy for this exploration task. With this policy, the Ant agent can explore a much larger area in the state space.

As mentioned in Section 3, we learn a prior network \(P_{}\) together with the option policy network \(_{}\). This design is different from previous algorithms which choose to fix the prior distribution. In Appendix D.1, we provide a detailed discussion on this and empirically show that we can get a further performance improvement in the downstream task by initializing the option selector \(P_{}\) with \(P_{}\).

### Performance of Unsupervised Skill Discovery Across Various Benchmarks

In the 3D Locomotion task, an Ant agent needs to coordinate its four legs to move. In Figure 5(a), we visualize two of the controlling behaviors, learned by ODPP without supervision of any extrinsic rewards. Please refer to Appendix D.2 for more visualizations. The picture above shows that the Ant rolls to the right by running on Leg 1 first and then Leg 4, which is a more speedy way to move ahead. While, the picture below shows that it learns how to walk to the right by stepping on its front legs (2&3) and back legs (1&4) by turn. These complex behaviors are learned with only intrinsic objectives on diversity and coverage, which is quite meaningful. Further, we show numeric results to evaluate the learned behaviors. We use the reward defined in  as the metric, which is designed to encourage the Ant agent to move as fast as possible at the least control cost. In Figure 5(b), we take the average of five different random seeds, and the result shows that options trained by ODPP outperform the baselines. The reward drops during training for some baselines, which is reasonable since this is not the reward function used for option learning. Finally, to see whether we can learn a large number of options in the meantime with ODPP, we test the performance of the discovered options when setting the number of options to learn as 10, 20, 40, 60. From Figure 5(c), it can be observed that even when learning a large number of options at the same time, we can still get options with high quality (mean) and diversity (standard deviation) which increase during the training.

Figure 4: (a)(d) Mujoco Maze tasks. (b)(c) Applying the options to goal-achieving tasks in the Point Room/Corridor where the agent needs to achieve one of the four goals (red points). (e)(f) Applying options to exploration tasks in the Ant Room/Corridor where the agent needs to explore as far as possible to get higher reward.

To further demonstrate the applicability of ODPP, we compare it with SOTA baselines on more general OpenAI Gym and Atari tasks . Notably, we adopt two more advanced skill discovery algorithms as baselines: DADS and APS. Comparisons among these two and previous baselines are in Appendix D.4. Skills discovered with different algorithms are evaluated with reward functions carefully-crafted for each task, provided by OpenAI Gym. For each algorithm, we learn 10 skills, of which the average cumulative rewards (i.e., sum of rewards within a skill duration) in the training process are shown in Figure 6. The skill duration is set as 100 for CartPole and 50 for the other two. Note that the complete episode horizon is 200 for CartPole and 10000 for AirRaid and RiverRaid. Thus, it would be unfair to compare the cumulative reward of a skill with the one of a whole episode. Our algorithm performs the best in all the three tasks, and the improvement becomes more significant as the task difficulty increases. When relying solely on MI-based objectives, the agent tends to reinforce already discovered behaviors for improved diversity rather than exploration. The explicit incorporation of coverage and diversity objectives in our algorithm proves beneficial in this case.

## 5 Conclusion and Discussion

ODPP is a novel unsupervised option discovery framework based on DPP, which unifies variational and Laplacian-based option discovery methods. Building upon the information-theoretic objectives in prior variational research, we propose three DPP-related measures to explicitly quantify and optimize diversity and coverage of the discovered options. Through a novel design of the DPP kernel matrix based on the Laplacian spectrum of the state transition graph, ODPP generalizes SOTA Laplacian-based option discovery algorithms. We demonstrate the superior performance of ODPP over SOTA baselines using variational and Laplacian-based methods on a series of challenging benchmarks. Regarding limitations of ODPP, the primary one is to assign suitable weights for each objective term: \(_{1:3}\) in Eq. (10) and \(\) in Eq. (6). These hyperparameters are crucial, yet conducting a grid search for their joint selection would be exhaustive. Instead, we employ a sequential, greedy method to select hyperparameters in accordance with our ablation study's procedure, further detailed in Appendix C.6.