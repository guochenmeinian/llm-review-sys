# Dual Defense: Enhancing Privacy and Mitigating Poisoning Attacks in Federated Learning

Runhua Xu

Beihang University

runhua@buaa.edu.cn

&Shiqi Gao

Beihang University

gaoshiqi@buaa.edu.cn

&Chao Li

Beijing Jiaotong University

li.chao@bjtu.edu.cn

&James Joshi

University of Pittsburgh

jjoshi@pitt.edu

&Jianxin Li

Beihang University and Zhongguancun Laboratory

lijx@buaa.edu.cn

corresponding author

###### Abstract

Federated learning (FL) is inherently susceptible to privacy breaches and poisoning attacks. To tackle these challenges, researchers have separately devised secure aggregation mechanisms to protect data privacy and robust aggregation methods that withstand poisoning attacks. However, simultaneously addressing both concerns is challenging; secure aggregation facilitates poisoning attacks as most anomaly detection techniques require access to unencrypted local model updates, which are obscured by secure aggregation. Few recent efforts to simultaneously tackle both challenges often depend on impractical assumption of non-colluding two-server setups that disrupt FL's topology, or three-party computation which introduces scalability issues, complicating deployment and application. To overcome this dilemma, this paper introduce a **D**ual **D**efense **F**ederated learning (_DDFed_) framework. _DDFed_ simultaneously boosts privacy protection and mitigates poisoning attacks, without introducing new participant roles or disrupting the existing FL topology. _DDFed_ initially leverages cutting-edge fully homomorphic encryption (FHE) to securely aggregate model updates, without the impractical requirement for non-colluding two-server setups and ensures strong privacy protection. Additionally, we proposes a unique two-phase anomaly detection mechanism for encrypted model updates, featuring secure similarity computation and feedback-driven collaborative selection, with additional measures to prevent potential privacy breaches from Byzantine clients incorporated into the detection process. We conducted extensive experiments on various model poisoning attacks and FL scenarios, including both cross-device and cross-silo FL. Experiments on publicly available datasets demonstrate that _DDFed_ successfully protects model privacy and effectively defends against model poisoning threats.

## 1 Introduction

Federated learning (FL) is gaining popularity as a collaborative model training paradigm that provides primary privacy protection by eliminating the need of sharing private training data. Based on the participants' scale, FL is typically divided into two categories: cross-silo FL and cross-device FL. Cross-device FL typically involves numerous similar devices, while cross-silo FL usually includes fewer participants like organizations. Recent studies show that FL mainly confronts two types of threats: privacy risks from curious adversaries attempting to compromise data privacythrough methods like membership inference and model inversion attacks, and security risks from Byzantine adversaries looking to damage the final model's integrity with backdoors or by lowering its accuracy [2; 24; 14; 11; 3; 1; 34].

To mitigate privacy risks in FL, researchers have developed a range of techniques to bolster privacy. These encompass differential privacy-based aggregation , as well as secure aggregation approaches using homomorphic encryption, functional encryption, and secure multi-party computation[6; 43]. Aside from privacy concerns, many studies have proposed strategies to identify and mitigate potentially harmful updates during the model aggregation phase, thereby safeguarding the global model against adversarial attacks. Notable Byzantine-resistant aggregation mechanisms encompass the Krum fusion method, cosine defense aggregation mechanism[29; 38], and median/mean-based strategies like clipping median and trimmed mean strategies . Research in these two areas has been conducted separately, and addressing both issues at once continues to be challenging. This difficulty arises because secure aggregation makes it easier for adversarial attacks to occur, as most anomaly detection methods need access to "unencrypted" local model updates that secure aggregation protects.

Few recent efforts [39; 13; 16; 43; 15; 23; 9; 20] to tackle both challenges simultaneously often depend on differential privacy techniques [39; 13; 16; 22; 12], which can degrade model performance due to added noise, or rely on impractical non-colluded two-server assumption that disrupts FL's topology[43; 15; 23; 9; 20], complicating its deployment and application. In light of these limitations, a critical yet overlooked question is _how to create a straightforward dual defense strategy that simultaneously strengthens privacy protection and mitigates poisoning attacks without introducing new participant roles or altering the single-server multiple-clients structure?_

To address this dilemma, this paper proposes a **D**ual **D**efense approach that simultaneously enhances privacy protection and combats poisoning attacks in **F**ederated learning (_DDFed_), without changing the structure of current FL frameworks. _DDFed_ initially leverages cutting-edge cryptographic technology, specifically fully homomorphic encryption (FHE), to securely aggregate model updates without the impractical assumption of non-colluding two-server setups and ensures strong privacy protection by permitting only the aggregation server to perform secure aggregation in the dark. To tackle the challenge of detecting malicious models within encrypted model updates, _DDFed_ introduces a novel two-phase anomaly detection mechanism. This approach enables cosine similarity computation over encrypted models and incorporates a feedback-driven collaborative selection process, with additional measures to prevent potential privacy breaches from Byzantine clients incorporated into the detection mechanism. Our main contributions are summarized as follows:

* We introduce a dual defense strategy that simultaneously boosts privacy and combats poisoning attacks in federated learning. This is achieved by integrating FHE-based secure aggregation with a mechanism for detecting malicious encrypted models based on similarity.
* To effectively detect malicious models in encrypted updates, we propose a novel two-phase anomaly detection mechanism with extra safeguards against potential privacy breaches by Byzantine clients during the detection process. Additionally, we introduce a clipping technique to bolster defenses against diverse poisoning attacks.
* We carried out comprehensive experiments on multiple model poisoning attacks and federated learning scenarios, covering both cross-device FL and cross-silo FL. Our experiments with publicly accessible datasets demonstrate _DDFed_'s effectiveness in safeguarding model privacy and robustly defending against model poisoning threats.

## 2 Related Works

Privacy Risks and Countermeasures in FLThe fundamental design of FL ensures that all training data stays with its owner, offering basic privacy. However, it still exposes vulnerabilities to inference attacks, which allow adversaries to extract information about the training data used by each party [24; 27; 2; 24; 14; 11]. In some cases, the risk of private information leakage may be unacceptable. Therefore, several defenses have been suggested to mitigate these risks, including differential privacy (DP) and secure aggregation (SA), based on various cryptographic primitives such as (partial) homomorphic encryption [21; 41], threshold Paillier , functional encryption , and pairwise masking protocols .

Poisoning Risks and Countermeasures in FL.Besides privacy inference attacks, FL is also susceptible to poisoning attacks, where adversaries can compromise certain clients and manipulate their data or models to intentionally worsen the global model's performance by introducing corrupted updates during training. This paper focuses on untargeted model attacks, whose goal is to significantly diminish the effectiveness of the global model through methods such as Inner Product Manipulation (IPM) attack , scaling attack, and "a little is enough" (ALIE) attack . Several strategies have been developed to counteract the impact of attacks, ensuring they don't compromise model performance. These strategies fall into two categories: client-side and server-side defenses. Client-side defenses adjust the local training algorithm with a focus on secure client updates, whereas server-side defenses [5; 29; 38; 40] either reduce the influence of updates from malicious clients through adjusted aggregation weights or use clustering techniques to aggregate updates from trustworthy clients only. However, these defense strategies typically operate under the assumption that model updates are not encrypted, which contradicts the objectives of privacy-focused secure aggregation defense strategies.

Private and Robust Federated Learning.In privacy-preserving FL, identifying poisoning attacks is harder because of the need to balance local model privacy with the detection of harmful models. Only a few existing studies like those mentioned in [39; 13; 16; 15] employ Byzantine-resilient aggregation through differential-privacy techniques. This approach necessitates a compromise between privacy and model accuracy. Additionally, recent initiatives have been launched to address this problem through diverse methods by using various secure computation technologies. These include 3PC, which faces scalability limitations; an oblivious random grouping method constrained by its design for partial parameter disclosure; and both additive secret sharing and two-trapdoor homomorphic encryption, which depend on the impractical assumption of non-colluding dual servers.

## 3 Dual Defense Federated Learning Framework

### Formulation and Assumption

Formulation.A typical FL framework involves \(m\) clients, \(_{1},...,_{m}\), and a single aggregation server \(\). Each client \(_{i}\) possesses its own dataset \(D_{i}\). The overarching goal in FL across these \(m\) clients is to minimize the global objective function:

\[_{}_{1},...,}_{m}}_{i=1}^{m}|}{_{i=1}^{m}|D_{i}|}L_{i}(}_{i};D_{i}).\] (1)

Here, \(L_{i}\) represents the local loss function for each client's data, and \(}_{i}\) are the local model parameters specific to client \(_{i}\). The term \(D_{i}\) refers to the private dataset of client \(i\), with \(|D_{i}|\) indicating its size in terms of sample count. In short, the goal of general FL is to learn an optimal global model \(}_{G}\) across \(m\) clients. This is achieved by periodically synchronizing the model parameters from all clients using specified fusion algorithms like _FedAvg_ and its variants, with the aggregation server \(\) over several training rounds.

Due to various malicious activities, including inference attacks that aim to steal private information from legitimate clients and poisoning attacks designed to undermine model integrity by degrading its performance, existing privacy-preserving FL often relies on a secure aggregation mechanism[21; 41; 30; 36; 6]. Typically, without loss of generality, during the \(t\)-th federated learning training round, each client \(_{i}\) secures its local model update \(}_{i}\) - referred to as \([}_{i}]\) throughout this paper - before transmitting it to the aggregation server. This is achieved by using various privacy-enhancing technologies such as homomorphic encryption and secure multi-party computation.

Threat Assumption._DDFed_ tolerates an adversary, capable of corrupting any subset of local clients at a specified ratio \(r_{},s.t.,r_{}<0.5\), to carry out model poisoning attacks that degrade the global model's performance. Additionally, we assume the aggregation server \(\) is semi-honest (honest-but-curious), meaning it adheres to the protocol but seeks to glean as much private information as possible. Similarly, the compromised clients \(_{i}^{}\) can conduct privacy inference attacks like those performed by \(\). In summary, regarding privacy preservation, both the inquisitive \(\) and the corrupted client subset aim to extract private information from benign clients; however,only the corrupted client subset will also initiate model poisoning attacks to undermine the global model.

### Framework Details

Objective of DDFed._DDFed_ is designed to bolster privacy protection and mitigate model poisoning attacks seamlessly within the existing FL framework. Unlike existing private and robust approaches [39; 13; 16; 43; 15; 23; 9; 20] that add new participant roles or depend on differential privacy, which may compromise model performance, _DDFed_ maintains effectiveness efficiently. _DDFed_ introduces a dual defense strategy that combines fully homomorphic encryption (FHE) for secure data aggregation with an optimized similarity-based mechanism to detect malicious models, ensuring unparalleled privacy protection and security against model poisoning attacks.

Similarity-based methods are commonly used in existing studies for anomaly detection models [29; 38]. Specifically, it computes the cosine similarity between each local model update of training round \(t\) and the global model from the previous round \(t-1\):

\[cos(_{i})=_{i}^{(t)},_{G}^{(t-1)}}{ \|_{i}^{(t)}\|_{2}\|_{G}^{(t-1)}\|_{2}},\] (2)

where \(_{i}\) denotes the angle between global model weights \(_{G}^{(t-1)}\) and local model update \(_{i}^{(t)}\) of client \(_{i}\). However, existing similarity-based mechanisms [29; 38] offer no privacy protection for local model updates, and integrating FHE into them poses significant challenges. These challenges arise from FHE's limitations in performing division and comparison operations, which are essential for identifying benign clients in these methods.

Framework Overview and Training Process.Figure 1 provides an overview of _DDFed_ framework, which includes several clients \(_{1},...,_{m}\) and a single aggregation server \(\), consistent with the architecture of most existing FL frameworks. In the following section, we demonstrate the _DDFed_ training process. Due to space limitations, the formal algorithm pseudocode is provided solely in Appendix A.1.

Before the FL training begins, each client \(_{i}\) is equipped with an FHE key pair \((,)\). During the FL training phase, let's assume that in the \(t\)-th round, each client \(_{i}\) trains a local model \(_{i}^{(t)}\) (\(\)) and performs the normalization and encryption as \([}_{i}^{(t)}]=_{}(_{i}^{(t)})\) with public key pk (\(}\)). Upon receiving encrypted local models, \(\{[_{i}^{(t)}]\}_{i[1,...,m]}\), \(\) starts to detect anomaly model updates over all encrypted local models. Specifically, \(\) first extracts the last layer, denoted as \(\{[_{i}^{(t)}]\}_{i[1,...,m]}\), which remains encrypted (\(\)), and adds a perturbation \(^{(t)}\) to safeguard against

Figure 1: Overview of _DDFed_ framework and illustration of a single round _DDFed_ training.

potential privacy attacks by malicious clients. Next, it retrieves the last layer of the encrypted global model from the previous training round (\(^{{}^{}(t-1)}_{G}\)), The method for adding perturbations will be discussed in Section 3.2. Then, \(\) performs secure inner-product between each perturbed \(^{{}^{}(t)}_{i_{s}}+^{(t)}\) and \(^{{}^{}(t-1)}_{G}\) to derive encrypted similarity score, denoted as \(^{{}^{}(t)}=( s^{{}^{}(t)}_ {1},..., s^{(t)}_{m})\), and query each client (1). After receiving \(^{{}^{}(t)}\), each client \(_{i}\) decrypts it to obtain the plaintext scores \(^{{}^{}(t)}_{i}\). Subsequently, each client submits their list of similarity scores (1). It's important to note that at this stage, malicious clients may tamper with their similarity scores in an attempt to prevent detection of their compromised models. Since a benign client will honestly and accurately decrypt and select trustworthy clients group via threshold-based filter, and hence their results should be consistent. Therefore, \(\) uses a majority voting strategy to acquire the final client score list, i.e., the voted \(^{(t)}\) (2). Next, \(\) normalizes \(^{(t)}\) and generates the fusion weight (1). Here, _DDFed_ employs _FedAvg_'s approach by weighting the aggregation according to dataset size proportions in current training round (1). Finally, each client \(_{i}\) receives the aggregated global model \(^{(t)}_{G}\), decrypts it, and initiates the \((t+1)\)-th round of _DDFed_ training (1).

Private and Robust Malicious Model Detection.As observed in , the distribution of local data labels can be more effectively represented in the weights of the last layer than in other layers. Consequently, _DDFed_ employs a similar approach to enhance the efficiency of detecting anomalies, as it requires performing similarity computation on encrypted model updates. Given that FHE supports only basic mathematical operations, and the similarity-based anomaly model detection mechanism needs complex operations like division (as shown in equation 2), comparison and sorting operations, _DDFed_ breaks it down into two stages: _secure similarity computation_ and _feedback-driven collaborative selection_. In the rest of the paper and during our experimental evaluation, we adhere to the layer section settings described in . However, _DDFed_ can be easily extended to support strategies for detecting malicious models using full layers. Additional experiments are detailed in Appendix A.2.4 to demonstrate the impact of layer sections on the _DDFed_ framework.

Secure Similarity Computation.To circumvent division operations, _DDFed_ necessitates that all clients pre-process their inputs for normalization and shifts the task of comparing similarity scores to the client side. This is because clients possess the FHE private key, allowing them to obtain the similarity score in plaintext. Formally, we have the following:

\[ cos(_{i})=^{(t)}_{i}],[ ^{(t-1)}_{G}]}{\|[^{(t)}_{i}]\|_{2}\|[ {W}^{(t-1)}_{G}]\|_{2}}=[^{(t)}_{i}}{\|^{(t)}_ {i}\|_{2}}],[^{(t-1)}_{G}}{\|^{(t-1)}_{G}\|_{2}}],\] (3)

where each client \(_{i}\) prepares the \(^{(t)}_{i}}{\|^{(t)}_{i}\|_{2}}\) and \(^{(t-1)}_{G}}{\|^{(t-1)}_{G}\|_{2}}\) in advance, and then encrypts them using FHE encryption algorithm. Next, the aggregation server \(\) verifies received \(^{(t-1)}_{G}}{\|^{(t-1)}_{G}\|_{2}}\) and perturbs local inputs and conducts secure inner-product computation as follows:

\[^{{}^{}(t)}=[^{(t) }_{i}}{\|^{(t)}_{i}\|_{2}}]+^{(t)},[^{(t-1)}_{ G}}{\|^{(t-1)}_{G}\|_{2}}].\] (4)

Motivation of Similarity Score Perturbation._DDFed_ aims to simultaneously address privacy and poisoning risks. This means it not only considers model poisoning attacks but also prevents adversarial clients from inferring private information from other benign clients by exploiting decrypted similarity scores and previous global models. To mitigate this privacy risk, _DDFed_ improves secure inner-product computation by introducing perturbations into each normalized and encrypted model update. Specifically, _DDFed_ uses \((,)\)-differential privacy with a Gaussian mechanism as its method of perturbation, \(^{(t)}=(0,^{2}),=}{}\), where \((,)\) represents the parameters of the DP mechanism and \( f\) denotes sensitivity.

It's important to note that our perturbation affects only the anomaly detection phase and does not change the encrypted model updates that are to be aggregated. Consequently, the final aggregated model retains its accuracy, just as it would with a standard aggregation mechanism. Furthermore, our experiments indicate that the perturbation noise does not affect the effectiveness of anomaly detection.

Even at \(=0.01\), which offers strong privacy protection, _DDFed_ still performs well and delivers good model performance.

Feedback-driven Collaborative Selection.As shown in the threat model, _DDFed_ tolerates less than 50% malicious clients, indicating that over half of the clients are benign and will execute the steps honestly and correctly as designed. _DDFed_ employs a feedback-driven collaborative selection approach to filter out potentially malicious models. Specifically, upon receiving the encrypted \([^{{}^{}(t)}]\), each client \(_{i}\) first decrypts to acquire \(_{i}^{{}^{}(t)}\) using the FHE private key sk. Next, each client \(_{i}\) independently decrypts the similarity scores, sorts them, and selects trustworthy clients \(_{i}^{(t)}\) for the current training round based on a threshold. _DDFed_ uses only the mean value of similarity scores as its filtering threshold. Subsequent experiments have demonstrated its effectiveness. Additionally, _DDFed_ is open and compatible with alternative methods for setting thresholds. After each client returns their decision on the group of benign clients (\(_{i}^{(t)}\)), the aggregation server uses a majority of vote strategy to decide the final aggregation group (\(^{(t)}\)) for the current training round. Next, similar to _FedAvg_, _DDFed_ applies a data size-based fusion weight strategy to calculate each client's fusion weight \(_{^{(t)}}^{^{(t)}}\) in the aggregation group, where \(f_{j}^{(t)}=|}{_{j^{(t)}}|D_{j}|}\).

FHE-based Secure Aggregation with Clipping._DDFed_'s secure aggregation leverages the FHE cryptosystem, specifically the CKKS instance, which excels in arithmetic operations on encrypted real or complex numbers and stands as one of the most efficient methods for computing with encrypted data. Formally, the aggregation server performs secure aggregation as \([^{(t)}_{G}]=[^{(t)}],_{^{(t)}}^{^{(t)}}\). Once receiving the aggregated global model \([^{(t)}_{G}]\), each client \(_{i}\) uses their private key to decrypt it, obtaining the final global model \(^{(t)}_{G}\) in plaintext via the FHE decryption algorithm. In contrast to current approaches in private and robust FL, _DDFed_ uniquely enables each benign client to execute a clipping operation before the next training round. This enhancement is designed to counteract more sophisticated model poisoning attacks that conventional similarity-based methods [29; 38] fail to address, as will be shown in the experiments section.

### Analysis on Privacy and Robustness

Based on the threat model discussed earlier, _DDFed_ prevents an honest-but-curious aggregation server from potentially inferring private information from accessible model updates. Additionally, it also withstands a subset of local clients, compromised by an adversary, to launch model poisoning attacks and attempt to infer private information from other benign clients during the anomaly model detection phase.

In terms of privacy risks, _DDFed_ utilizes FHE primitives to ensure cryptographic-level privacy protection. This means the aggregation server processes each operation without any insight into the model update (in the dark), eliminating any chance of inferring private information from local model updates. Furthermore, to counter potential inferences by corrupted clients exploiting decrypted similarity scores, _DDFed_ incorporates a perturbation method where DP noise is added during the secure similarity computation phase. Due to space limitations, the formal DP-enhanced perturbation analysis is provided solely in Appendix A.3.

Regarding the risk of poisoning attacks, _DDFed_ adopts similarity-based anomaly detection technologies with additional optimizations such as perturbation-based similarity computation and post-aggregation clipping. These enhancements bolster the robustness of its aggregation mechanism. Our experiments demonstrate that _DDFed_ effectively resists a range of continuous poisoning attacks, including IPM, SCALING, and ALIE attacks, which will be elaborated in Section 4.

## 4 Experiments

### Experimental Setup

Datasets and Implementation.We assessed our proposed _DDFed_ framework using publicly available benchmark datasets: MNIST, a collection of handwritten digits, and Fashion-MNIST (FMNIST), which includes images of various clothing items, offering a more challenging and diverse dataset for federated learning tasks. We create non-iid partitions for all datasets based on previous research [38; 43], using a default \(q\) value of 0.5, where a higher \(q\) reflects greater degrees of non-iid. We assess the framework's performance using a nine-layer CNN model with 225k parameters, secured by the FHE cryptosystem in each training round. This secure aggregation is implemented through TenSEAL library . The experimental _DDFed_ is available on the GitHub repository.

Baselines and Default Setting.We compare our proposed method _DDFed_ with well-known FL fusion algorithms and robust aggregation methods, including _Krum_, _Cos Defense_, and median/mean-based approaches like _Median_, _Clipping Median_, and _Trimmed Mean_ strategies. We exclude baselines such as FLTrust or RFFL because they require server-side validation data or are incompatible with client sampling, making them impractical for real-world applications. Additionally, we omit secure robust approaches[9; 20; 23; 43] that depend on complex secure aggregation techniques due to their requirement for additional non-colluding participants, which alters the original structure of the federated learning framework. Note that the core contribution of this paper is not to propose new model poisoning defense approaches, but to enhance existing popular defenses with privacy features--specifically, server-side similarity-based defenses. Therefore, the experiments aim to evaluate how these privacy-preserving features affect the original defense methods, rather than defending against recent attack techniques and strategies as shown in works like [26; 31; 42; 10; 25].

To assess defense performance, we evaluated the proposed work against popular model poisoning attacks: Inner Product Manipulation (_IPM_) attack , _scaling_ attack, and the "a little is enough" (_ALIE_) attack. Unless otherwise mentioned, we assume a default attacker ratio of 0.3 among all participants as malicious clients. The attacks commence at the 50th round and persist until training ends. The default FL training involves 10 clients randomly chosen from 100 for each communication round. Furthermore, we employ a batch size of 64 with each client conducting local training over three epochs per round using an SGD optimizer with a momentum of 0.9 and a learning rate of 0.01. Our _DDFed_ implementation's default epsilon (\(\)) value is set to 0.01 unless specified differently.

### Performance Evaluation

Performance of Defense Effectiveness under Various Attacks.Figure 2 demonstrates the effectiveness of our _DDFed_ method compared to baseline methods in countering three prevalent model poisoning attacks, with an attacker ratio set at 0.3. The attack commences at the 50th round and continues until training concludes. Under the IPM attack scenario, aside from FedAvg, Trimmed Mean, and Clipping Median mechanisms, our approach along with other defense strategies performs well (nearly as model accuracy as without any model poisoning attack) in defending against the IPM attack. The same conclusion also holds true in the ALIE attack. However, only _DDFed_ and Clip Median successfully withstand SCALING attacks with minor and acceptable losses in model

Figure 2: Comparison of defense effectiveness across various defense approaches, evaluated on MNIST (top) and FMNIST(bottom), under IPM attack (left), ALIE attack (middle), and SCALING attack (right).

performance. Note that _DDFed_ remains robust even when attackers target the system from the start of training. Due to space constraints, we present the defense effectiveness against various cold-start attacks in Appendix A.2.3. In summary, our _DDFed_ method achieves the best comprehensive defense performance.

Impact of Attacker Ratio.To further investigate the impact of attacker ratio in the _DDFed_ framework, we conducted experiments with various attacker ratio settings. It's important to note that _DDFed_ operates under the security assumption that at least half of the participants must be benign (i.e., \(r_{attacker}<0.5\)), therefore, in our experiments, the attacker ratio setting is ranged from 0.1 to 0.4. As shown in Figure 3, the proportion of attackers among all clients does not significantly affect our proposed _DDFed_ method. This suggests that it can effectively counter three types of model poisoning attacks. Additionally, we observed that under an ALIE attack scenario, our method may require approximately 10-20 training rounds to recover from the continuous attack, depending on the dataset evaluated.

Compatibility with Cross-device and Cross-silo FL Scenarios.To explore how the number of clients affects our _DDFed_ framework and to confirm its compatibility with two common federated learning scenarios, i.e., cross-device and cross-silo, we conducted multiple experiments. These

Figure 4: Comparison of _DDFed_ effectiveness across different client numbers, evaluated on MNIST (top) and FMNIST (bottom), under IPM attack (left), ALIE attack (middle), and SCALING attack (right).

Figure 3: Comparison of _DDFed_ effectiveness across different attack ratios, evaluated on MNIST (top) and FMNIST (bottom), under IPM attack (left), ALIE attack (middle), and SCALING attack (right).

experiments had an attacker ratio fixed at 0.3, with client counts varying from 10 to 100. In cross-silo FL, client numbers are typically small, often ranging from a few to several dozen; however, for simulating the cross-device FL scenario in our study, we used 100 clients due to their generally larger population. As illustrated in Figure 4, our _DDFed_ framework effectively defends against all three attacks across various client number settings. This suggests that the performance of _DDFed_ is not significantly affected by the number of clients, indicating its suitability for both cross-silo and cross-device FL scenarios. Furthermore, a higher number of client settings may result in relatively large fluctuations during training rounds immediately following the attack; however, the model training ultimately converges steadily, unaffected by the continuous attack.

Time Cost of Secure Aggregation.To assess the additional time cost incurred by integrating FHE-based secure similarity computation and secure aggregation into _DDFed_, we measured the time cost of each training round and compared it with the baseline methods mentioned earlier. All experiments were carried out using the default settings described above. Due to space constraints, we only present the defense approach's time cost per training round when under an IPM attack and have included further results in Appendix A.2.2.

As shown in Table 1, compared to other robust aggregation mechanisms that lack privacy-preserving features, our _DDFed_ solution incurs additional time costs due to the integration of FHE-based secure similarity computation and secure aggregation. Across experiments on various datasets and under different attacks, our _DDFed_ generally requires an extra 2 seconds compared to the usual 10-second training round, resulting in a 20% increase in time per training round. However, our _DDFed_ is capable of defending against model poisoning attacks while also offering strong privacy guarantees. Note that the time-related experiments were conducted on a MacOS platform with an Apple M2 Max chip and 96GB of memory.

Impact of Epsilon Setting.To better understand the effect of the hyperparameter \(\) setting on _DDFed_'s perturbation-based secure similarity computation phase, we conducted several experiments with different \(\) settings, ranging from 0.01 to 0.1. Here, we only demonstrate the results from 10 clients here, with additional results in Appendix A.2.1.

As shown in Figure 5, the \(\) setting has a negligible impact on performance with the MNIST dataset. However, higher \(\) values, which indicate stronger DP protection, cause relatively larger fluctuations in performance on the FMNIST dataset. Therefore, we believe that the optimal \(\) setting depends on the specific task at hand and leave it as an open question for future research.

   Approaches & MNIST, IPM attack &  \\   & avg (s) & var (s) & avg (s) & var (s) \\  FedAvg & 10.26 & 0.07 & 10.47 & 0.01 \\ Krum & 10.32 & 0.03 & 10.26 & 0.01 \\ Median & 10.32 & 0.01 & 10.28 & 0.02 \\ Clipping Median & 10.31 & 0.01 & 10.32 & 0.01 \\ Trimmed Mean & 10.32 & 0.02 & 10.30 & 0.01 \\ Cos Defense & 10.25 & 0.01 & 10.26 & 0.02 \\ DDFed (Our Work) & 12.43 & 0.01 & 12.14 & 0.01 \\   

Table 1: Time cost per training round of various defense approaches.

Figure 5: Impact of hyper-parameter \(\) of differential privacy based perturbation at secure similarity computation phase, evaluated on MNIST (left) and FMNIST (right), under IPM attack.

### Discussion and Limitation

To the best of our knowledge, _DDFed_ offers a dual defense strategy that simultaneously boosts privacy protection and fights against poisoning attacks in FL, without altering the existing FL framework's architecture. _DDFed_ utilizes FHE for top-notch privacy, enabling the aggregation server to perform similarity calculations and aggregation without directly accessing model updates. Additionally, _DDFed_ introduces perturbation techniques to block attempts by malicious clients to infer information from similarity scores. It further employs similarity-based anomaly detection, enhanced with strategies like perturbation and post-aggregation clipping, to protect against various types of poisoning attacks. However, _DDFed_ has not fully explored two related questions: how can we relax the attacker ratio restriction (i.e., \(r_{}<0.5\)) while still ensuring effective dual defense? And how can we adapt _DDFed_ to more complex FL scenarios, such as dropout and dynamic participant groups? We leave these questions open for future research. Currently, _DDFed_ only enhances existing popular defenses, such as similarity-based strategies with privacy features. Extending _DDFed_ to support other or more recent defense strategies remains an open question.

## 5 Conclusion

To tackle the dual challenges of privacy risks and model poisoning in federated learning, we introduce _DDFed_, a comprehensive approach that strengthens privacy protections and counters model poisoning attacks. _DDFed_ enhances privacy by using an FHE-based secure aggregation mechanism and addresses encrypted poisoned model detection through an innovative secure similarity-based anomaly filtering method. This method includes secure similarity computation with perturbation and feedback-driven selection process to distinguish safe model updates from potentially harmful ones. Our approach has been rigorously tested against well-known attacks on diverse datasets, demonstrating its effectiveness. We believe our work sets a solid foundation for future advancements in secure and robust federated learning.