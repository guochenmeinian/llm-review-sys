# Can Graph Neural Networks Expose Training Data Properties? An Efficient Risk Assessment Approach

Hanyang Yuan\({}^{1,2,4}\), Jiarong Xu\({}^{2}\), Renhong Huang\({}^{1}\)

Mingli Song\({}^{1,4}\), Chunping Wang\({}^{3}\), Yang Yang\({}^{1}\)

\({}^{1}\)Zhejiang University, \({}^{2}\)Fudan University, \({}^{3}\)Firvolution Group

\({}^{4}\)Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security

{yuanhanyang, renh2, brooksong, yangya}@zju.edu.cn

jiarongxu@fudan.edu.cn, wangchunping02@xinye.com

This work was done when the author was a visiting student at Fudan University.State Key Laboratory of Blockchain and Security, Zhejiang University.Corresponding author.

###### Abstract

Graph neural networks (GNNs) have attracted considerable attention due to their diverse applications. However, the scarcity and quality limitations of graph data present challenges to their training process in practical settings. To facilitate the development of effective GNNs, companies and researchers often seek external collaboration. Yet, directly sharing data raises privacy concerns, motivating data owners to train GNNs on their private graphs and share the trained models. Unfortunately, these models may still inadvertently disclose sensitive properties of their training graphs (_e.g._, average default rate in a transaction network), leading to severe consequences for data owners. In this work, we study graph property inference attack to identify the risk of sensitive property information leakage from shared models. Existing approaches typically train numerous shadow models for developing such attack, which is computationally intensive and impractical. To address this issue, we propose an efficient graph property inference attack by leveraging model approximation techniques. Our method only requires training a small set of models on graphs, while generating a sufficient number of approximated shadow models for attacks. To enhance diversity while reducing errors in the approximated models, we apply edit distance to quantify the diversity within a group of approximated models and introduce a theoretically guaranteed criterion to evaluate each model's error. Subsequently, we propose a novel selection mechanism to ensure that the retained approximated models achieve high diversity and low error. Extensive experiments across six real-world scenarios demonstrate our method's substantial improvement, with average increases of 2.7% in attack accuracy and 4.1% in ROC-AUC, while being 6.5\(\) faster compared to the best baseline.

## 1 Introduction

Graph data, encapsulating relationships between entities across various domains such as social networks, molecular networks, and transaction networks, holds immense value [1; 2; 3]. Graph neural networks (GNNs) have proven effective in modeling graph data [4; 5; 6], yielding promising results across diverse applications, including recommender systems , molecular prediction [8; 9], and anomaly detection . While training high-quality GNN models may necessitate a substantial amount of data, graphs may be scarce or of low quality in practice [11; 12], prompting companies and researchers to seek additional data from external sources .

However, directly obtaining data from other sources is often difficult due to privacy concerns . As an alternative, sharing models rather than raw data has become increasingly common . Typically, _data owners_ train a model on their own data and subsequently release it to the community or collaborators . For instance, a larger bank may train a fraud detection model on its extensive transaction network and share it with partners, allowing them to use their own customer data to identify risks.

Despite the benefits, this model-sharing strategy sometimes remains vulnerable to data leakage risks. Given access to the released model, one may infer sensitive properties of the data owner's graph, which are not intended to be shared. In the context of releasing a fraud detection model, if an adversarial bank can determine the average default rate of all customers in the transaction network, the data owner bank's financial status can potentially be revealed. Another example is releasing a recommendation model trained on a company's product network . If a competitor can infer the distribution of co-purchase links between different products, he may determine which items are frequently promoted together and deduce the company's marketing tactics. Such attacks are possible because released models may inadvertently retain and expose sensitive information from the training data . We refer to such sensitive information related to the global distribution in a graph as _graph sensitive properties_, and we aim to investigate the problem of _graph property inference attack_.

Previous property inference attacks  primarily focus on text or image data, assuming models trained on different properties exhibit differences in parameters or outputs. For GNNs modeling graph data, the inherent relationships and message-passing mechanisms can magnify distribution bias , making them more vulnerable to attacks. Although a few studies extend property inference to graphs and GNNs , they typically involve creating shadow models that replicate the released model's architecture and are trained on shadow graphs with varying sensitive properties. The parameters or outputs of shadow models are used to train an attack model to classify the property of the data owner's graph. A major limitation of these attacks is the need to train a large number of shadow models (_e.g._, 4,096 models , 1,600 models ), resulting in significant computational cost and low efficiency.

In this paper, we explore the feasibility of avoiding the training of numerous shadow models by designing an _efficient yet effective_ graph property inference attack. Our key insight is to train only a small set of models and then generate sufficient approximated shadow models to support the attack. To this end, we leverage and extend model approximation techniques. For a given dataset and a model trained on it, when the training data changes (_e.g._, removing a sample), model approximation allows the efficient estimation of new model parameters for the updated dataset without retraining. This technique, often called unlearning , enables the efficient generation of multiple approximated shadow models from a single trained model. Specifically, given a small set of graphs and their corresponding trained models, we perturb each graph to alter sensitive properties (_e.g._, changing the number of nodes corresponding to high default rate users) and then apply model approximation to produce a sufficient number of approximated models corresponding to the perturbations, thereby reducing the total attack cost. Figure 1 illustrates our approach compared to the traditional attack.

Nevertheless, achieving this goal presents several challenges. The first challenge is to ensure the diversity of approximated models, which provides a broader range of training samples for the attack model and enhances its generalization capability. To tackle this, we develop structure-aware random walk sampling graphs from distinctive communities and introduce edit distance to quantify the diversity of a set of approximated models. The second challenge is to ensure that the errors in the approximated shadow models are sufficiently small. Otherwise, these models may fail to accurately reflect differences in graph properties, thereby diminishing attack performance. To address this, we establish that different graph perturbations can lead to varying approximation errors, which offers a theoretical-guaranteed criterion for assessing the errors of each approximated model. Finally, we propose a novel selection mechanism to reduce errors while enhancing the diversity of approximated models, formulated as an efficiently solvable programming problem.

* We propose an efficient and effective graph property inference attack that requires training only a few models to generate sufficient approximated models for the attack.
* We propose a novel selection mechanism to retain approximated models with high diversity and low error, using edit distance to measure the diversity of approximated models and a theoretical criterion for assessing the errors of each. This diversity-error optimization is formulated as an efficiently solvable programming problem.

* Experiments on six real-world scenarios demonstrate the efficiency and effectiveness of our proposed attack method. On average, existing attacks require training 700 shadow models to achieve 67.3% accuracy and 63.6% ROC-AUC, whereas our method trains only 66.7 models and obtains others by approximation, achieving 69.0% attack accuracy and 66.4% ROC-AUC.

## 2 Problem Definition

The scenario of a graph property inference attack first involves a data owner who trains a GNN model using his graph data, referred to as the _target model_ and _target graph_ in the following text. Once trained, the target model's parameters or output posterior probabilities can be released in communities [16; 18]. For example, the data owner can upload the pre-trained parameters to GitHub [17; 19] to facilitate downstream tasks. Or he may upload the posterior probabilities from a recommender system to a third-party online optimization solver , such as Gurobi *. Collaborative machine learning is another potential attack scenario . For instance, consider two banks aiming to collaboratively train a fraud detection model. While sharing raw transaction networks poses risks to privacy and commercial confidentiality, they can employ model-sharing strategies .

Footnote *: https://www.gurobi.com

With access to the target model, curious users may launch inference attacks to obtain some sensitive properties of the target graph, which can reveal secrets not intended to be shared. For example, the ratio of co-purchase links between particular products in a product network may relate to the promoting tactics of a sales company, or the average default rate in a transaction network may reveal the financial status of a bank. Such confidential information may further impact business competition.

In the rest of this section, we first define the privacy, _i.e._, the sensitive properties referred to in this work. Then, we introduce the knowledge of the attack. Finally, we formulate the problem of property inference attacks.

Graph sensitive property.In this paper, we consider an attributed target graph where nodes are associated with multiple attributes. The sensitive property is defined based on one specific type of attribute, called the _property attribute_. Specifically, the sensitive property is defined as a certain statistical value of the property attribute's distribution. We consider two types of properties that the attacker may infer: (1) node properties, specified by the ratio of nodes with a particular property attribute value, and (2) link properties, specified by the ratio of links where the end nodes have particular property attribute values.

Note that the property attribute can be either discrete or continuous. For instance, a node property defined on the discrete category attribute in a product network can be the ratio of co-purchase links between luxury items. An edge property defined on the continuous default rate attribute in a transaction network can be the average default rate of all customers. The inferred sensitive properties may reveal data owner's secrets such as commercial strategies; see [23; 32] for further discussion.

Attacker's knowledge.The attacker's background knowledge is assumed to be as follows:

* Auxiliary graph: We assume the attacker has an auxiliary graph from the same domain as the target graph but does not necessarily intersect with the latter. In practice, the auxiliary graph can be sourced from publicly available data or derived directly from the adversary's own knowledge.
* Target model: We consider two types of knowledge on the target model: the white-box setting, where the adversary knows the architecture and parameters of the target GNN, and the black-box setting, where the adversary only knows the target GNN's output posterior probabilities.

Property inference attack.Formally, let \(G^{}\) denote the target graph. And let \((G^{})\) denote the property value of \(G^{}\). Note that \(\) can represent either node properties or link properties. Given that the attacker has an auxiliary graph \(G^{}\) from the same domain as \(G^{}\), we define graph property inference attack as follows:

**Problem 1** (Graph property inference attack): _Given the auxiliary graph \(G^{}\), and assume the attacker has either the white-box knowledge of the target GNN parameters or the black-box knowledge of target GNN's output posterior probabilities, the objective of the graph property inference attack is to infer the property \((G^{})\) without access to it._

## 3 Methodology

This section provides a detailed description of the proposed graph property inference attack. We start with an overview of our method and then delve into the technical aspects of model approximation. Finally, we describe how to ensure the diversity of approximated models while reducing their errors. The overall algorithm and complexity analysis are summarized in Appendix A.3.

### Overview

As shown in Figure 1(a), given the auxiliary graph, the conventional approach is to first sample numerous shadow graphs, ensuring that graphs with different properties are adequately represented. Each is then used to train a shadow model with the same structure as the target model. Once trained, parameters (white-box) or output posterior probabilities (black-box) of shadow models are collected, along with the corresponding properties of shadow graphs. Finally, an attack model (_e.g._ linear classifier) is trained to classify properties based on parameters or posteriors. Since the number of shadow models is usually hundreds or thousands, their training can be computationally expensive.

To mitigate this issue, our proposed attack method utilizes model approximation techniques as a substitute. We provide a illustration of our method in Figure 1(b):

(1) Instead of numerous shadow graphs, we first sample only a few _reference graphs_.

(2) On each reference graph, we train a _reference model_ with the same architecture as the target model, and generate multiple augmented graphs by removing different nodes and edges.

(3) By efficient model approximation, we obtain approximated models _w.r.t_ to the augmented graphs.

(4) We collect parameters or posteriors of all approximated models and train the attack model in a similar manner as previous attacks.

Here, we mainly face two challenges: ensuring that the approximate error associated with augmentations is relatively small, and ensuring that the approximated models are sufficiently diverse. To address them, we derive a theoretical criterion for calculating approximation errors across different augmented graphs (see SS 3.2) and design a diversity enhancement strategy in SS 3.3.

### Model approximation and error analysis

We proceed by introducing the techniques of model approximation, which include generating augmented graphs, obtaining approximated models, and conducting theoretical analysis for error criterion.

Generating augmented graphs and identifying influenced nodes.First, we aim to ensure that multiple perturbations produce distinctive augmented graphs. This is essential because highly similar augmentations reduce the distinction in the corresponding graph properties and model features, providing minimal benefit to the overall attack. For this purpose, we propose removing both nodes and edges from the reference graph. Formally, Let the reference graph be denoted by \(G^{}=(V,E)\) sampled from \(G^{}\), where \(V\) is the node set, \(E V V\) is the edge set. For one perturbation, we

Figure 1: Illustrations of (a) conventional graph property inference attacks and (b) the proposed attack, with yellow shading indicating model training, the main source of computational cost.

remove \(V^{} V\) and \(E^{} E\) to obtain the augmented graph \(G^{}\). In GNNs, the neighborhood aggregation makes the removal inevitably influence the state of other remaining nodes. Given a \(l\)-layer GNN, the influenced nodes of removing a single node \(v V^{R}\) is the \(l\)-hop neighborhood of \(v\), denote as \(_{l}(v)\). And the influenced nodes of removing a single edge \(e E^{}\), connecting nodes \(v\) and \(u\), is denoted as \(_{l}(e)=_{l-1}(v)_{l-1}(u)\{v,u\}\). With these in mind, we next define the total influenced nodes for removing \(V^{}\) and \(E^{}\).

**Definition 1** (Influenced nodes ): _Given the removed nodes \(V^{}\), removed edges \(E^{}\) and a \(l\)-layer GNN, the total influenced nodes \(V^{1}\) is defined as_

\[V^{1}=_{e E^{}}_{l}(e)_{v V^{ }}_{l}(v).\] (1)

_Note that \(V^{1}\) is exclusive of \(V^{}\); we omit the set difference for simplicity._

Generating Approximated Models.Subsequently, we generate the approximated model based on the perturbation. While existing graph unlearning  may offer potential solutions, they are either limited to specific model architectures or only support the removal of nodes or edges individually, making them unsuitable for direct application. To address this, we extend their mechanisms to suit our scenario. Let the reference model be parameterized by \(^{}^{m}\). In this paper, we consider cross-entropy loss as the loss function, and \(^{}\) is obtained as follows:

\[^{}=_{}_{v V}(;v,E).\] (2)

After removing \(V^{}\) and \(E^{}\), directly retraining on \(G^{}\) could yield a new model parameter \(^{}\):

\[^{}=_{}_{v V/V^{}}( ;v,E/E^{}).\] (3)

To avoid training from scratch, we derive the approximation of \(^{}\) by the following theorem.

**Theorem 3.1** (GNN model approximation): _Given the GNN parameter \(^{}\) on \(G^{}\), the removed nodes \(V^{}\), removed edges \(E^{}\) and influenced nodes \(V^{1}\). Assume \(\) is twice-differentiable everywhere and convex, we have_

\[^{}^{}+(^{2}( ^{};G^{}))^{-1}(_{v V^{1} V^{ }}(^{};v,E)-_{v V^{1}}(^{ };v,E/E^{})),\] (4)

_where \(\) denote gradient, and \(^{2}\) denote Hessian. \((^{};G^{})=_{v V/V^{}}(^{};v,E/E^{})\). The detailed derivation can be found in Appendix A.2._

In practice, the Hessian may be non-invertible due to the non-convexity of GNNs. We address this by adding a damping term to the Hessian .To reduce computation, we also follow  to convert the inverse Hessian calculation into quadratic minimization. See Appendix A.3 for complexity analysis.

Analyzing the approximation error.Eventually, we aim to quantitatively assess the error in the approximated model, as this directly determines whether graph properties can be effectively reflected, thereby influencing the attack. To achieve this, we investigate how specific removal choices of \(V^{}\) and \(E^{}\) affect the approximation error in Eq. (4). Note that \(_{v V/V^{}}(;v,E/E^{})=0\) only when \(^{}\) is the exact minimizer, thus the gradient norm \(\|_{v V/V^{}}(;v,E/E^{})\|_{2}\) can reflect the approximation error. The following theorem provides an upper bound on this gradient norm.

**Theorem 3.2** (Approximation error bound): _Assume \(\) is twice-differentiable everywhere and convex, \(\|\|_{2} c_{1}\), \(^{2}_{v V/V^{}}(;v,E/E^{})\) is \(_{1}\)-Lipschitz, the approximation error bound is given by:_

\[\|_{v V/V^{}}(^{};v,E/E^{ })\|_{2} C(|V^{}|+2|V^{1}|)^{2}=C(V^{},E^{}),\] (5)

_where \(||\) denotes the cardinality of a set, and \((,)\) denotes the square of the number of nodes removed and influenced, given \(V^{}\) and \(E^{}\). \(C\) denotes a constant depending on the GNN model, see Appendix A.2 for detail proof._Theorem 3.2 indicates that the error bound for the approximation is related to both the number of removed nodes and influenced nodes. Next, we demonstrate how this can serve as an _error criterion_ to select augmented graphs that result in minimal approximation errors.

### Diversity enhancement

Following the above, we detail the designed diversity enhancement strategy. To develop a well-generalized attack model capable of distinguishing different sensitive properties, we first apply a structure-aware random walk for sampling diverse reference graphs. We then propose a novel selection mechanism to ensure that multiple perturbations on the reference graphs further enhance diversity while considering the reduction of approximation error.

Sampling diverse reference graphs.Inspired by community detection, where diverse communities are identified on a graph, we design a structure-aware random walk for sampling reference graphs. Specifically, we incorporate Louvain community detection  to partition the auxiliary graph into several similarly sized communities. During random walks, the starting nodes are chosen from different communities. And we assign different weights to neighboring nodes: \(w\) for those within the same community and \(1-w\) for those from different communities, where \(w\) is a hyper-parameter. The transition probabilities are then obtained by normalizing these weights. This strategy encourages sampling reference graphs within distinct communities, thus boosting their diversity.

Ensuring diverse augmented graphs.To ensure perturbations on reference graphs can enhance the diversity, we further design a perturbation selector. Based on SS 3.2, it is easy to see that each approximated model can be considered as a result of the specific perturbation. Thus, improving the diversity of approximated models is essentially improving the diversity of augmented graphs. Formally, for each reference graph we generate \(k\) augmented graphs \(^{}=\{G_{1}^{},G_{2}^{}, ,G_{k}^{}\}\) by randomly removing \(k\) different sets of nodes and edges. The diversity for \(^{}\) is defined as:

**Definition 2** (Diversity for \(^{}\)): _Given a set of \(k\) graphs \(^{}=\{G_{1}^{},,G_{k}^{}\}\), and a graph metric \(d(G_{i}^{},G_{j}^{})\) that measures the distance between \(G_{i}^{}\) and \(G_{j}^{}\). The diversity of \(^{}\) is defined as the sum of all pair-wise graph distances in \(^{}\), that is, \(_{i=1}^{k}_{j=1}^{k}d(G_{i}^{},G_{j}^{})\)._

Since stochastic augmentations may not all contribute to total diversity, our objective is to select a diverse subset of \(^{}\), namely, a subset of diverse perturbations to enhance the diversity of augmented models. However, it is important to note that solely maximizing diversity may lead to relatively large approximation errors, which may worsen the attack performance. Fortunately, utilizing the error criterion from Eq. (5), we can ensure that augmentations enhance diversity while minimizing total approximation error, which can be formulated as a quadratic integer programming task.

Given \(k\) available perturbations, we aim to select \(q\) of them, such that the diversity among these selected is maximized while keeping the approximation error minimal. We here introduce decision variables \(x_{i}\{0,1\}\) to represent whether the \(i\)-th augmentation is selected. Let \(_{i}\) represent the approximation error in the \(i\)-th augmentation (_cf._ Eq. (5)). The optimization problem is as follows:

\[_{i=1}^{k}_{j=1}^{k}d(G_{i}^{},G_{j}^{})x_{i}x_{j},(1)_{i=1}^{k}x_{i}=q,(2)_{i=1}^{k}_{i}x_{i},\] (6)

where \(\) is a constant that imposes the budget on the total approximation error of the selected \(q\) augmentations, ensuring that it does not exceed \(\). Here, we select graph edit distance as the distance metric, which can be efficiently calculated since all \(k\) augmented graphs \(^{}\) are derived from one reference \(^{}\). We utilize Gurobi Optimizer , a state-of-the-art solver, to solve this quadratic integer programming problem, which is known for its efficiency and effectiveness.

### Overall algorithm

We summarize the overall algorithm of our attack in Algorithm 1. Steps 1-2 outline the structure-aware random walk for sampling reference graphs, steps 5-12 detail the perturbation selector, with step 10 calculating the error criterion in Eq. (5). Finally, in step 16 we train the attack model.

## 4 Experiments

In this section, we evaluate the performance of the proposed attack method by addressing the following three research questions:

* **RQ1**: How efficient and effective is our method on various graph datasets?
* **RQ2**: How do different factors influence the performance of our method?
* **RQ3**: How applicable is our method in different scenarios?

### Experimental setup

Datasets and sensitive properties.We conduct property inferences on three real world datasets: Facebook , Pubmed , and Pokec . Appendix A.4 details the datasets and properties.

* Facebook and Pokec are social networks where nodes represent users and edges denote friendships. Following , we select gender as the property attribute, set node property as whether the male nodes are dominant, and edge property as whether the same-gender edges are dominant.
* Pubmed is a citation network where nodes are publications and edges are citations. We select the keyword "Insulin" (IS) as the property attribute. Node property is whether publications with "IS" are dominant. Edge property is whether citations between publications with "IS" are dominant. All used properties are summarized in Table 1.

Training and testing data.For fairness, we evaluate our method and baselines on the same target graphs. To ensure there is no overlap between the target graph and the auxiliary graph, for each dataset we first use Louvain community detection to split the original graph into two similarly sized parts. One part is used as the auxiliary graph, and the other part is used to sample multiple target graphs. Sizes and numbers of reference graphs (our method), shadow graphs (baselines), and target graphs are provided in Appendix A.4.

Target GNN.For target GNN, We use a widely recognized GNN model, GraphSAGE , configured as per  with 2 layers, 64 hidden sizes, and 1,500 training epochs with an early stop tolerance of 50. The Adam optimizer is used with a learning rate of 1e-4 and a weight decay of 5e-4.

Implementation details.For the attack model, We use a linear classifier with the deepest trick ; For hyper-parameter settings, we perform grid searches of reference graphs' numbers in (0, 100) (step size 25), and augmented graphs' numbers in (0, 10) (step size 2), across all datasets. Experiments are repeated 5 times to report the averages with standard deviations. See appendix A.4 for more details. Our codes are available at https://github.com/zjunet/GPIA_NIPS.

Baselines.We adopt four state-of-the-art baseline models to compare against the proposed attack model: (1) GPIA : An attack method designed for graphs and GNNs in both white-box and black-box settings, following the traditional attack framework. (2) PIR-S/PIR-D : Two permutation equivalence methods designed for white-box attacks, PIR-S using neuron sorting and PIR-D using set-based representation. (3) AIA : Property inference method based on attribute inference attack, which first predicts the property attribute based on embeddings/posteriors and then predicts property, suitable for both white-box/black-box attacks. See Appendix A.4 for details.

  
**Type** & **Dataset** & **Property attribute** & **Property description** \\   & Pokec & Gender & \# male users \(>\) \# female users \\  & Facebook & Gender & \# male users \(>\) \# female users \\  & Pubmed & Keyword “IS” & \# publications with “IS” \(>\) \# publications w/o “IS” \\   & Pokec & Gender & \# same-gender edges \(>\) \# diff-gender edges \\  & Facebook & Gender & \# same-gender edges \(>\) \# diff-gender edges \\   & Pubmed & Keyword “IS” & \# edges between papers with “IS” \(>\) \# other edges \\   

Table 1: Properties to be attacked, # indicates number of nodes or edges.

### Evaluation of efficiency and effectiveness (RQ1)

We first focus on white-box settings and evaluate the accuracy and ROC-AUC for effectiveness and runtime for efficiency. Note that the reported runtime throughout this work encompasses the entire attack process for both the proposed method and baselines, starting from sampling the reference (shadow) graphs to inferring the properties of the target graphs. Table 2 presents the average accuracy and runtime of the proposed attack method compared to other baseline methods on the six aforementioned sensitive properties. We provide the corresponding standard deviations and ROC-AUCs results in Appendix A.5. The results reveal several key insights: (1) Traditional attacks incur significantly high runtime. The slight differences mainly depend on the different strategies in their attack models. (2) PIR-D achieves better accuracy among the baselines, possibly due to their consideration of permutation equivalence. AIA shows lower performance, which may be because of their limited ability to conduct attribute inference, thus affecting the classification of properties. (3) The proposed attack model outperforms all baseline methods across all datasets, achieving an average increase of 2.7% in accuracy and being 6.5\(\) faster compared to the best baseline, demonstrating its remarkable efficiency and efficacy. The significant margin by which our method outperforms the baselines is primarily due to our specific mechanisms that ensure diversity in both reference and augmented graphs, which are essential for training a robust attack model. In contrast, conventional attacks lack such designs for shadow graph diversity, resulting in sub-optimal performance.

### Evaluation of influencing factors (RQ2)

Ablation study.To ensure effectiveness, our method includes two main mechanisms: sampling diverse reference graphs and selecting diverse augmented graphs. Here, we conduct ablation studies to demonstrate their necessity, including four variants: (1) w/o structure: We discard structure-aware sampling and use simple random walks to sample reference graphs. (2) w/o selector: We discard the augmentation selector and use random removal to obtain augmented graphs. (3) w/o error: In the augmentation selector (_cf._ Eq. (6)), we ignore the approximation error and only select augmentations that maximize diversity. (4) w/o diversity: We ignore diversity in the augmentation selector (_cf._ Eq. (6)) and only select augmentations that minimize the approximation error. Figure 2 (a) shows the attack results on Facebook's node property. Notably, the complete model consistently surpasses the performance of all variants, showing the effectiveness and necessity of simultaneously sampling diverse reference graphs and selecting diverse augmented graphs.

Hyper-parameter analysis.We next evaluate the impact of two important hyper-parameters on our method: (1) the number of reference graphs and (2) the number of selected augmented graphs. Both directly affect the diversity of approximated models. We tune the number of reference graphs among {25, 50, 75, 100} and the number of selected augmented graphs among {2, 4, 6, 8, 10}. The results in Figure 2 (b) and 2 (c) show that as both hyper-parameters increase, the attack performance initially improves and then stabilizes. This indicates that a relatively small number of reference graphs and augmented graphs are sufficient to ensure diversity, thereby maintaining good attack performance.

### Evaluation in different scenarios (RQ3)

    &  &  &  \\   &  &  &  &  &  &  &  &  \\  & Acc. & Rt. & Acc. & Rt. & Acc. & Rt. & Acc. & Rt. & Acc. & Rt. & Acc. & Rt. \\  GPIA & 60.7 & 1791 & 56.1 & 1591 & 82.9 & 3028 & 73.8 & 2901 & 58.6 & 1176 & 54.7 & 1314 \\ PIR-D & 67.8 & 1733 & 58.1 & 1563 & 83.9 & 3022 & 73.9 & 2895 & 60.5 & 1187 & 56.2 & 1325 \\ PIR-S & 60.2 & 1762 & 57.7 & 1576 & 80.5 & 3032 & 72.6 & 2912 & 59.6 & 1181 & 57.9 & 1318 \\ AIA & 64.3 & 1741 & 56.7 & 1553 & 67.3 & 3026 & 75.5 & 2896 & 58.3 & 1215 & 56.5 & 1353 \\  Ours & **71.4** & **254** & **61.5** & **222** & **85.2** & **550** & **75.9** & **432** & **61.4** & **242** & **58.8** & **159** \\   

Table 2: Average accuracy and runtime (seconds) comparison on different properties in white-box setting. “Node” and “Link” denote node and link properties, respectively. The best results are in bold.

To test the applicability of our method, we evaluate its performance under various conditions, including scenarios with black-box adversary knowledge, on different types of GNN models, on large-scale graph datasets, and when the target and auxiliary graphs are distinct.

Performance on black-box knowledge.In the black-box setting, we use model outputs, specifically posterior probabilities, to train attack models for our method and baselines. Since PIR-D and PIR-S only support white-box settings, we included another state-of-the-art black-box attack, PIA-MP , as detailed in Appendix A.4. The results on Facebook's node property in Figure 2 (d) show that our method improves accuracy by 11.5% compared to the best baselines while being 7.3\(\) faster.

Performance on other GNNs.We conduct property inference attacks on other three fundamental GNNs: GCN , GAT , and SGC . For GCN and GAT, hyper-parameters are configured according to , while for SGC, we set the number of hops to 2. We report the attack accuracy and runtime of our method alongside other baselines on Facebook's node property, as illustrated in Figure 3 (a)-(c). It is observed that the overall attack accuracy for SGC is comparatively lower, potentially due to the SGC model's inherent limitations in capturing property information effectively. Moreover, our method consistently achieves the highest accuracy, also demonstrating a runtime that is 4.4\(\) faster on GCN, 4.0\(\) faster on GAT, and 4.3\(\) faster on SGC compared to the best baseline.

Performance on scalability.We further conducted property inference attacks on a large-scale graph dataset, Pokec-100M, which contains 1,027,956 nodes and 27,718,416 edges. This graph is sampled from the original dataset  by retaining nodes with relatively complete features. We targeted the same node property as in the Pokec dataset, with the number of nodes in the reference graphs, shadow graphs, and target graphs set to 52,600, 50,000, and 50,000, respectively. All other settings remain consistent with previous experiments. We compare the attack accuracy and runtime of our method against other baselines. As shown in Figure 3 (d), conventional attacks incur significant computational costs on this dataset, whereas our method is 10.0\(\) faster. Additionally, our attack accuracy is significantly higher than those of the baselines.

Performance with distinct target and auxiliary graphs.In the above experiments, the target and auxiliary graphs are splits of the same original graph. However, in real-world scenarios, this assumption may not hold. Therefore, we evaluate the performance of our attack under a more practical condition, where distinct graphs (from the same domain) are used as the target and auxiliary graphs. Specifically, we select Facebook and Pokec, as they are both social networks, and consider two cases: using Facebook as the target and Pokec as the auxiliary graph, and vice versa. Since the feature dimensions of these two datasets differ, the parameters of the approximated model and the target model are not directly compatible, so we apply PCA dimension reduction to align the parameters.

    &  &  \\   &  &  &  &  \\  & Acc. & Rt. & Acc. & Rt. & Acc. & Rt. & Acc. & Rt. \\  GPIA & 56.7 & 1732 & 54.6 & 1569 & 60.2 & 1244 & 57.5 & 1296 \\ PIR-D & 54.5 & 1794 & 56.8 & 1648 & 62.4 & 1297 & 55.9 & 1385 \\ PIR-S & 57.6 & 1777 & 52.1 & 1609 & 60.3 & 1262 & 56.1 & 1342 \\ AIA & 53.4 & 1729 & 53.0 & 1535 & 64.6 & 1237 & 55.6 & 1321 \\  Ours & **58.3** & **267** & **57.3** & **236** & **65.7** & **233** & **59.3** & **177** \\   

Table 3: Attack comparison using distinct graphs for the target and auxiliary graphs. The arrows indicate the auxiliary graph on the left and the target graph on the right. The best results are bolded.

Figure 2: (a) Evaluation of the necessity of considering diversity while minimizing the approximation error. (b) and (c) Impact of the number of augmented graphs (per reference graph) and reference graphs on attack accuracy, respectively. (d) Accuracy and runtime comparison in black-box settings.

## 5 Literature Review

Property inference attack.The concept of property inference attack is first introduced by , demonstrating the leakage of sensitive properties from hidden Markov models and support vector machines in systems like speech-to-text. Building on this, attacks on various machine learning models are studied, including feed-forward neural networks, convolutional neural networks, and generative adversarial networks [22; 25; 32; 46]. Some works also consider multi-party collaborative learning scenarios [20; 24] or incorporate data poisoning [32; 47]. Specifically,  proposes an efficient attack based on distinguishing tests, achieving faster performance than traditional shadow training. Their setting differs from ours by the additional adversarial capability of data poisoning. Recently, with the increasing use of graphs and GNNs, security and privacy concerns are emerging [48; 49; 50; 51]. While efforts have been made to investigate property inference attacks on GNNs [20; 21; 27], they follow the shadow training framework, which requires training a relatively large number of shadow GNN models, leading to high computational costs and reduced feasibility .  assumes access to the embedding of whole graphs and targets at graph-level properties, which is beyond our scope.

GNN model approximation.GNN model approximations are primarily based on the influence function [29; 31; 53; 54] or Newton update [30; 33]. Except for , these methods are utilized in the context of graph unlearning. Studies [30; 33; 54] explore model approximation for edge or node removal and analyze the corresponding approximation error bounds, yet they are limited to specific model architectures (_e.g._, simple graph convolution, graph scattering transform). Further efforts [29; 31] extend model approximation to generic GNNs.  introduces a framework for edge unlearning, while  proposes a general unlearning framework for removing either nodes, edges, or features individually. Our model approximation differs from above by enabling the simultaneous removal of nodes and edges across generic GNN architecture. A concurrent work  addresses a similar model approximation as our attack; however, the additional theoretical assumptions could fail when removing a combination of nodes and edges, and their corresponding solution may significantly compromise the efficiency. Other studies [55; 56] also employ the graph shard approach. However, they may have poor efficiency in batch removal, which involves multiple retraining of sub-models.

## 6 Conclusion

In this paper, we focus on the problem of graph property inference attacks. We utilize model approximation to efficiently generate approximated models after initially training a small set of models, which replaces the costly shadow training in traditional attacks. To overcome the challenge of ensuring the diversity of approximated models while reducing the approximation error, we first derive a theoretical criterion to quantify the impact of different augmentations on approximation error. Next, we propose a diversity enhancement strategy, including a structure-aware random walk for sampling diverse reference graphs and a selection mechanism to retain optimal approximated models, utilizing edit distance to measure diversity and the theoretical criterion to assess approximation error. The retained approximated models are finally used to train an attack classifier. Extensive experiments across six real-world scenarios demonstrate our attack's outstanding efficiency and effectiveness.

Figure 3: Comparison of average attack accuracy and runtime (seconds) on: (a)-(c) other GNNs, including GAT, GCN, and SGC; (d) a large-scale dataset, Pokec-100M.