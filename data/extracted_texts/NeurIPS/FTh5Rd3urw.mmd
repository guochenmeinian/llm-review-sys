# PI-FL: Personalized and Incentivized Federated Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Existing incentive solutions for traditional Federated Learning (FL) only consider individual clients' contributions to a single global model. They are unsuitable for clustered personalization, where multiple cluster-level models can exist. Moreover, they focus solely on providing monetary incentives and fail to address the need for personalized FL, overlooking the importance of enhancing the personalized model's appeal to individual clients as a motivating factor for consistent participation. In this paper, we first propose to treat incentivization and personalization as interrelated challenges and solve them with an incentive mechanism that fost personalized learning. Second, unlike existing approaches that rely on the aggregator to perform client clustering, we propose to involve clients by allowing them to provide incentive-driven preferences for joining clusters based on their data distributions. Our approach enhances the personalized and cluster-level model appeal for self-aware clients with high-quality data leading to their active and consistent participation. Through evaluation, we show that we achieve an 8-45% test accuracy improvement of the cluster models, 3-38% improvement in personalized model appeal, and 31-100% increase in the participation rate, compared to a wide range of FL modeling approaches, including those that tackle data heterogeneity and learn personalized models.

## 1 Introduction

Training high-quality models using traditional distributed machine learning requires massive data transfer from the data sources to a central location, which raises various communication, computation, and privacy challenges. In response, Federated Learning (FL)  has emerged as a solution to train models at the source, reducing privacy issues and addressing the need for high-quality models. However, the success of FL relies on resolving various new challenges related to statistical heterogeneity , scheduling , and incentive distribution . Recent works have focused on training personalized models  to overcome data heterogeneity challenges.

Among personalized Federated Learning (pFL) techniques, similarity-based approaches that use clustering of clients at the aggregator have gained popularity . These personalization solutions fulfill the primary goal of overcoming data heterogeneity for specific cases. However, existing pFL solutions do not include any incentive mechanism, which is crucial in FL to motivate participants to contribute their data and computation resources. Existing incentive mechanisms  for traditional FL cannot be applied to pFL techniques because they only consider the performance contribution of clients towards training a single objective. In contrast, clients in pFL can be contributing towards multiple objectives simultaneously . Furthermore, traditional incentive solutions only provide monetary benefits and do not consider increasing personalized models' appeal as an incentive for encouraging active and reliable participation of clients. Withoutincentives, participants may provide low-quality data  or opt-out from participation1, leading to poorly performing pFL models , as shown with empirical evaluations in the later section. Collaboration fairness  can also be ensured by appropriately rewarding contributions and accounting for data heterogeneity .

In addition, since existing pFL techniques assume voluntary and consistent participation from clients, the aggregator controls the client selection and training with limited knowledge of clients' training capacity, availability, frequency of new incoming data, clustering preferences, and performance requirements from the trained personalized models. These factors can directly influence the motivation of self-conscious clients to participate consistently. Our evaluation shows that this causes frequent opt-outs from uninterested clients due to uninformed clustering decisions by the server and low personalized model appeal (PMA)2, which leads to reduced pFL performance. We also show that solving personalization and incentivization as interrelated challenges yield better outcomes for pFL than solving them as separate problems. However, this requires new paradigms for clustered pFL using data distribution information available to clients via their preferences and designing incentive mechanisms for increasing pFL appeal to reduce client opt-outs.

In this paper, we propose PI-FL that combines clustering-based pFL with token-based incentivization. Unlike previous works that control clustering from the server side, PI-FL allows clients to estimate the importance of each cluster and send their preferences for joining them to the aggregator as bids. To identify a cluster's importance to a client we use the importance weight of the cluster model as defined by FedSoft . Clients also use the importance weights to perform weighted local aggregation for single-shot personalization. This client-driven clustering approach results in accurate clustering because clients can attain a global perspective from their own local dataset which is only accessible to them and the importance weights information of each cluster. This allows them to make informed decisions that the server cannot make, resulting in improved PMA and reduced opt-outs. To incentivize clients for consistent participation, PI-FL motivates clients to join clusters with the clients that are most similar to them, maximizing their contribution to the cluster and, in turn, their rewards. Good quality cluster-level models then produce more appealing personalized models for each client. The incentive mechanism treats clients as both providers and consumers. As a consumer, the client tries to attain a certain level of personalized model appeal, so it pays the provider to spend resources to participate in training for the said model in each round. Whereas as a provider, the client earns a profit based on its marginal contribution to training the cluster models. The marginal contribution is calculated with a Shapley Value approximation due to the large computational overhead of the original algorithm .

**Contributions.** Existing pFL solutions fail to include PMA as an incentive to maintain consistent participation, resulting in increased opt-outs. To address this issue, we propose PI-FL as the first contribution, which provides contribution-based incentives to achieve collaborative fairness and maintain the cluster-level and personalized models' appeal for clients to prevent opt-outs. Additionally, PI-FL has the added advantage of creating personalized models for unseen clients with unknown data distributions that perform similarly to seen clients without the need for training. Secondly, we provide theoretical analysis and empirical verification of the benefits of including incentives with personalization. Lastly, we empirically evaluate the performance of PI-FL and other pFL models.

## 2 Related work

**Cluster-based pFL:** Among the cluster-based pFL works most related to PI-FL are FedSoft , FedGroup , and . FedSoft utilizes soft clustering on the basis of matching data distributions in clients with cluster models while FedGroup quantifies the similarities between clients' gradients by calculating the Euclidean distance of decomposed cosine similarity metric and  finds the optimal personalization-generalization trade-off from the cluster model by solving a bi-level optimization problem. This work incurs clustering overhead at each iteration and does not consider the overlap of distribution between clients wherein each client is restricted to one cluster for each training round. Other cluster-based pFL models include IFCA  which proposes a framework for loss-based clustering of clients and  which proposes three approaches for personalization using clustering, data interpolation, and model interpolation.

**Other pFL models:** Some pFL models propose meta-learning techniques that provide methods for rapid training of a personalized model. These include fine-tuning methods such as Per-FedAvg and regularization of local models . Others works  including FedALA , Ditto  and pFedMe  propose multi-task learning and model-interpolation  pFL models. FedFomo  suggests an adaptive local aggregation approach for personalization. FedProx  proposes a proximal term to improve the stability of FL. As per our knowledge, all of these pFL works lack qualities for attracting or sustaining long-term participation from self-conscious clients leading to an increase in opt-outs and low PMA. Moreover, most of these works require either require further training or re-clustering to adapt the personalized models for new incoming clients.

**Incentivized FL:** FAIR  integrates a quality-aware incentive mechanism with model aggregation to improve global model quality and encourage the participation of high-quality learning clients. FedFAIM  proposes a fairness-based incentive mechanism to prevent free-riding and reward fairness with Shapley value-based client contribution calculation.  proposes an approach based on reputation and reverse auction theory which selects and rewards participants by combining the reputation and bids of the participants under a limited budget.  proposes an approach where clients decide whether to participate based on their own utilities (reward minus cost) modeled as a minority game with incomplete information. Other incentivized FL works include . All of these works propose standalone solutions to attract clients, however, none of them fulfill the design requirements to be used with any pFL models.

**Why existing incentive mechanisms cannot be applied directly to pFL frameworks?**

Existing FL incentivization schemes designed for motivating clients to contribute to a single global goal  may not be applicable to pFL frameworks due to the multi-dimensional goals and objectives involved. In pFL frameworks, multiple objectives must be optimized simultaneously, such as cluster and personalized models per client in cluster-based pFL  or global and local models per client in multi-task learning . To encourage clients to contribute towards the multiple objectives in pFL frameworks, new incentive mechanisms need to be developed that are specifically tailored to their multi-objective nature. PI-FL uses clustering for pFL wherein the clusters memberships are changed after every \(R\) training rounds. PI-FL is different from these as it forms clear boundaries between multiple cluster models and improves shared learning between cluster similarities through multiple participation at the client level. PI-FL incorporates maintaining PMA for consistent client participation with an incentive mechanism that directly motivates personalized training on the basis of Individual Rationality (IR) constraint of game theory .

## 3 Proposed Methodology

In this section, we introduce PI-FL, which has three main modules: the profiler, the token manager, and the scheduler as shown by the architecture diagram in Figure 1. The profiler calculates and maintains the history of client contributions using Shapley Values approximation (lines 24-27) of Algorithm 1. The profiler also aids the scheduler in forming clusters using two different modes further explained in section 3.1. The token manager orchestrates transactions, holds auctions, deducts payments, and distributes rewards as given in lines (13 and 14). The scheduler selects clients based on bids and contributions, grouping them for improved homogeneity shown in lines (20 and 27-29). Individual clients calculate the importance weights of each aggregated cluster model and send their preference bids to the Token Manager for joining clusters as shown in lines (23-28) in Algorithm 2. Clients also generate a single-shot personalized model, shown in line 29. We assume that each client will look to maximize their profits according to the principle of Individual Rationality (IR)  and this will lead them to choose clusters in which they can contribute the most for maximum reward.

### Profiler

At the start of pFL training, the scheduler module forms the initial clusters by randomly assigning clients. Then for each round, clients train the cluster-level model on their local data and calculate the importance weight of each aggregated cluster model \(M_{k}\) on their local dataset via Equation 1. Here \(_{ck}\) is the normalized sum of correctly predicted data points \(n_{ck}\) on local dataset \(D_{c}\) of client \(c\). The importance weights are used to generate a single-shot personalized model through the weighted aggregation of cluster-level models using Equation 2.

\[_{ck}=n_{ck}/n_{k} k[K]\] (1)

Figure 1: PI-FL design

\[P_{ck}=_{k=1}^{K}v_{ck}(_{k})\] (2)

Here the \(P_{ck}\) is the personalized model of client \(c\) in cluster \(k\) and \(_{k}\) is the weight vector of \(k\) cluster model. Using this, clients generate single-shot personalized models offline according to their dynamic data needs. The client-centric clustering and participation method enhances the appeal of pFL for clients and in doing so also provides them the opportunity to customize their personalized model offline in case their requirements which are unknown to the server change during training. Clients can also make informed decisions on participating in training clusters based on their budget and past rewards, using importance weights and knowledge of previous rounds. They convey their preferences to the aggregator by submitting bids for the cluster they wish to participate in for the next training round.

The profiler calculates the marginal contributions of each client after every round using Shapley Values approximation (Algorithm 3), aiding scheduling by providing data quality information to the scheduler. The Shapley Value approximation derivation from Appendix is used to avoid the computational expense of calculating Shapley Values for multiple clients. PI-FL also includes a mode to facilitate clients to form well-defined initial clusters. So the clients can avoid the decision-making process in the beginning and streamline their spending when the client contributions and cluster distributions are unclear. For this, the profiler and the scheduler module facilitate forming the initial clusters by training for some pre-training rounds. This is done as client contributions and similarity metrics that the clients use among other metrics to make decisions about joining clusters are initially unknown. After pre-training, the profiler calculates per-class F1-Scores \(\) of all client local models on an IID test dataset . Then the profiler with the help of scheduler clusters clients for the next training round using the K-Means clustering  algorithm with the most varying F1-scores \(V_{F1}\) from \(C\) total classes. Equation 3 shows the calculation of \(V_{F1}\) where \(C\) is the number of total classes and \(N\) is the number of all available clients.

\[V_{F1}=var(_{i})[1,C] i N\] (3)We perform all our evaluations for PI-FL without this feature, but this is an added feature that PI-FL includes for faster convergence and to save clients' costs. We also realize the constraints in choosing all the clients for training, which is why clients that reply within threshold time in pre-training rounds are used to calculate F1 scores. The remaining clients are considered unexplored and assigned to clusters randomly, they can later settle into appropriate clusters through preference and contribution selection.

``` Input:\(T_{h}\): Importance weight threshold, \(K\): Number of clusters, \(M_{k}\): Cluster-level model of cluster \(k K\), \(D\): Local dataset of client,
2FunctionClientPreferences(\(M_{1},...,M_{k}\))
3for each cluster \(k K\)do
4foreach data point \(d D\)do
5 The client computes \(_{k}\) importance weight of \(M_{k}\) model for each data point \(d\) via Eqn. 1
6if\(_{k}>T_{h}\)then
7 Client adds cluster \(k\) to client's preference bids list \(_{i}^{*}\)
8
9 The client generates personalized model \(P_{ck}\) via Eqn. 2 return\(_{i}^{*}\) ```

**Algorithm 2** PI-FL (Client)

### Token Manager

The token manager acts as a bank to orchestrate and keep track of transactions between different clients. At the start of each training round the token manager holds an auction for each cluster, and the clients that want to participate in that cluster place their bids using tokens. The token manager forwards the list of willing clients to the scheduler to select clients for training. It also deducts payments from the willing clients/consumers via Equation 4. Here \(_{i}\) is the tokens owned by client \(i\), \(_{k}\) are the clients willing to join cluster \(k\), and \(_{p}\) in this and all following Equations is the per round bid amount to be paid by each client for participation.

\[_{i}=_{i}-_{p} i_{k}^{*}\] (4)

The tokens collected as payments from clients/consumers are then added to the available pool of tokens at the Token Manager as shown in Equation 5. Here \(_{ar}\) are the total available pool of tokens at the Token Manager. The term \(N_{p}\) is the number of clients selected on basis of performance and \(N_{r}\) is the number of clients selected randomly. The significance of using \(N_{p}\) and \(N_{r}\) is explained in section 3.3.

\[_{ar}=_{ar}+(Np+N_{r})_{p} r[1,R]\] (5)

The token manager handles the distribution of reimbursement and rewards to each provider/client. Reimbursement penalizes degradation in the performance of providers and depends on the utility function. The utility is calculated as the percentage of average accuracy improvement of the cluster model \(M_{k}\) over the maximum achieved accuracy in past rounds on the local data of clients in cluster \(k\). The utility function is given in Equation 6 and reimbursement calculation is given in Equation 7, both metrics are calculated at the profiler which assists the token manager in reimbursement.

\[=-Acc_{max} ))}{Acc_{max}})))}{},\] (6)

\[_{i}=_{i}-_{ar}[0,], i[ N], r[1,R]\] (7)

In Equation 6, \(Acc_{kr}\) is the cluster-level model accuracy in the current round \(r\) and \(Acc_{kmax}\) is the maximum cluster-level model accuracy achieved until the current round \(r\). The term \(\) represents the maximum portion of tokens that can be returned and \(\) represents the maximum accuracy improvement that leads to the use of one full token. In Equation 7, \(_{ar}\) are the total number of tokens collected from consumers/clients for \(r\) training round. We have used a similar approach to , however, they use the accuracy of the FedAvg model on an IID dataset. It is not practical to assume the presence of an IID dataset that can correspond to the data distribution of clients within a cluster which is why we rely on the local dataset of clients within that cluster to gather this information.

\[_{i}=_{i}+sort(_{ki},_{ki})}{N_{r} +1)}{2}} k[K], i[N], r[R]\] (8)After reimbursement, the token manager uses the marginal contributions calculated by the profiler and sorts providers/clients by their contributions and participation record in Equation 8. Here \(_{ki}\) represents the marginal contributions and \(_{ki}\) represents the participation records of all clients \(N\) in \(K\) clusters. The term \(\) is a normalizing term from Equation 8 in which \(N_{r}\) are the number of providers selected for participation in round \(r\). Using the ranks \(\) of providers from sorting and the normalization term \(\), the remaining available tokens are distributed between these providers in Equation 8. Here \(_{i}\) represents the tokens owned by provider/client \(i\) and \(_{ar}\) are the tokens available for incentive distribution at the token manager. Through reimbursements to consumers and payments to providers, the Token Manager ensures that each client receives an incentive according to their contributions in training the pFL models. By doing so, PI-FL incentivizes improvement in personalized learning, resulting in an enhancement of PMA and a decrease in opt-outs.

### Scheduler

The scheduler selects clients for each round \(r\) by the \(SelectClients(r)\) function given in Algorithm 1. The scheduler receives the preference bids \(_{i}\) from the token manager, the marginal contributions \(_{ki}\) from the profiler for each client \(i N\) in cluster \(k K\), where \(N\) is the total number of clients and \(K\) are the total number of clusters. Using this information scheduler groups clients with similar preference bids and then sorts those clients by their marginal contributions. Then the scheduler selects \(N_{p}\) number of clients from the sorted clients and \(N_{r}\) number of clients randomly. Both \(N_{p}\) and \(N_{r}\) are tunable parameters. To reduce bias, a small portion of clients \(N_{r}\) are selected randomly which is a technique adopted from previous works . By grouping clients with similar preferences the scheduler reduces the within-cluster bias improving the within-cluster homogeneity and a cluster model is produced that accurately represents the clients within it. Section 4 gives a theoretical analysis of how this is an important factor in improving the PMA.

## 4 Theoretical Analysis

We study the following particular case to develop insights. Suppose there are \(m\) clients in total, each observing a set of independent Gaussian observations \(z_{i,j}(_{i},^{2}),j=1,,n_{i}\), with a personalized task of estimating its unknown mean \(\). The quality of the learning result, denoted by \(\), will be assessed by the mean squared error \(_{i}(-)^{2}\), where the expectation \(_{i}\) is taken with respect to the distribution of client \(i\).

It is conceivable that if clients' underlying parameters \(_{i}\)'s are arbitrarily given, personalized FL may not boost the local learning result. To highlight the potential benefit of cluster-based modeling, we suppose that the \(m\) clients can be partitioned into two subsets: one with \(m_{1}\) clients, say \(T_{1}=\{1,,m_{1}\}\), and the other with \(m_{2}\) clients, say \(T_{2}=\{m_{1}+1,,m\}\), whose underlying parameters are randomly generated in the following way:

\[_{i}(_{1},^{2})\ | i T_{1},_{i} (_{2},^{2})\ | i T_{2}.\] (9)

Here, \(_{1}\) and \(_{2}\) can be treated as the root cause of two underlying clusters. We will study how the values of sample size \(n_{i}\), data variation \(\), within-cluster similarity as quantified by \(\), and cross-cluster similarity as quantified by \(|_{1}-_{2}|\) will influence the gain of a client in personalized learning. To simplify the discussion, we will assess the learning quality (based on the mean squared error) of any particular client \(i\) in the following three procedures:

**Local training**: Client \(i\) only performs local learning by minimizing the local loss \(L_{i}()=_{j=1}^{n_{i}}(-z_{i,j})^{2}\), and obtains \(_{i}=n_{i}^{-1}_{j=1}^{n_{i}}z_{i,j}\). Thus, the corresponding error is

\[e(_{i})=_{i}(_{i}-_{1})^{2}=}{n _{i}}.\] (10)

**Federated training**: Suppose the FL converges to the global minimum of the loss, \(_{i=1}^{m}}{n}L_{i}(), n}{{=}}_{i=1}^{m}n_{i}\), which can be calculated to be \(_{}=_{i=1}^{m}}{n}_{i}\). Consider any particular client \(i\). Without loss of generality, suppose it belongs to cluster 1, namely \(i T_{1}\). From the client \(i\)'s angle, conditional on its local \(_{i}\) and assuming a flat prior on \(_{1}\) and \(_{2}\), client \(j\)'s \(_{j}\) follows \(_{j}\ |\ _{i}(_{1},2^{2})\) for \(j T_{1}\) and \(j i\), and \(_{j}\ |\ _{i}(_{1}+_{2}-_{1},2^{2})\) for \(j T_{2}\). Then, the corresponding error is

\[e(_{}) =_{i}(_{}-_{1})^{2}\] \[=_{j T_{2}}}{n}(_{2}-_{1}) }^{2}+_{j=1,,m,j i}}{n}^{2} }{n_{j}}+2^{2}+}{n} ^{2}}{n_{i}}.\] (11)It can be seen that compared with (10), the above FL error can be non-vanishing if \(_{j T_{1}}}{n}(_{2}-_{1})\) is away from zero, even if sample sizes go to infinity. In other words, in the presence of a significant difference between the two clusters, the FL may not bring additional gain compared with local learning.

**Cluster-based personalized FL**: Suppose our algorithm allows both clusters to be correctly identified upon convergence. Consider any particular client \(i\). Suppose it belongs to Cluster 1 and will use a weighted average of Cluster-specific models. Specifically, the Cluster 1 model will be the minimum of the loss \(_{j T_{1}}}{n_{}}L_{j}(), n_{} }{{=}}_{j T_{1}}n_{j}\), which can be calculated to be \(_{}=_{j T_{1}}}{n_{}}_ {i}\). By a similar argument as in the derivation of (11), we can calculate

\[e(_{})=_{j T_{1},j i}(}{n_{}})^{2}(}{n_{j}}+2^{2})+(}{n _{}})^{2}}{n_{i}}.\] (12)

The above value can be smaller than that in (10). To see this, let us suppose the sample sizes \(n_{i}\)'s are all equal to, say \(n_{0}\), for simplicity. Then, we have

\[e(_{}) =-1}{m_{1}^{2}}(}{n_{0}}+2^{ 2})+}}{n_{0}}=-1}{m_{1}^{2}} (}{n_{0}}+2^{2})+^{2}}}{n_{0}}\] \[=}}{n_{0}}+-1}{m_{1}^{2} }2^{2},\]

which is smaller than (10) if and only if

\[^{2}<^{2}}{2n_{0}}.\] (13)

We derive the following intuitions from this analysis: **R1**. If the within-cluster bias is relatively small, the number of cluster-specific clients is large, and data noise is large, a client will have personalized gain from collaborating with others in the same cluster. **R2.** PI-FL's incentive algorithm rewards accuracy improvement reflected in PMA, which directly correlates with reducing within-cluster bias as per Equation 13. **R3.** By association, the incentive algorithm motivates clients to join similar clusters which increases cluster homogeneity and reduces the within-cluster bias. We show the impact of change in performance with an ablation study of PI-FL incentive in section 5.5.

## 5 Experimental Study

### Experimental Setup

We use NVIDIA GeForce RTX 3070 GPUs for all our experiments. To evaluate the performance of PI-FL with other pFL models we use four datasets. A simple CNN model (32x64x64 convolutional and 3136x128 linear layer parameters) is used that can be trained on client devices with limited system resources to map Cross-Device FL settings  for all pFL methods.

**CIFAR10 Data.** For comparison with FedSoft  we use the same CIFAR10 dataset provided in their repository. This image dataset has images of dimension 32 x 32 x 3 and 10 output classes. We copy different data heterogeneity conditions from , namely 10:90, 30:70, linear, and random. The data classes are divided into two clusters \(D_{A}\) and \(D_{B}\). In the **10:90** partition, \(50\) clients have \(90\%\) training data from \(D_{A}\) and \(10\%\) from \(D_{B}\), while the other \(50\) have \(10\%\) training data from \(D_{A}\) and \(90\%\) from \(D_{B}\). The **30:70** partition is similar to 10:90 except that the distribution ratios are \(30\%\) and \(70\%\).

**EMNIST Data.** This image dataset has images of dimension 28 x 28 and 52 output classes where 26 classes are lower case letters and 26 classes are upper case letters. Same as CIFAR10 data, we use the 10:90 and 30:70 data partitions and also include linear, and random partitions. In **linear** partition, client k has \((0.5+k)\%\) training and testing data from \(D_{A}\) and \((99.5-k)\%\) training data and testing data from \(D_{B}\). In the **random** partition, client k is assigned a mixture vector generated randomly by dividing the \(\) range into S segments with \(S-1\) points drawn from \(Uniform(0,1)\). The training and testing data are then assigned based on this vector from \(D_{A}\) and \(D_{B}\). Similar to , we also divide the EMNIST dataset into \(K\) clusters, where \(K=}{C_{p}}\), \(C_{t}\) are total classes and \(C_{p}\) are the classes owned per party with no overlap of data between clusters.

**Synthetic CIFAR10.** This is a synthetic dataset created from the CIFAR10 dataset and contains the same heterogenous partitions of 10:90, 30:70, linear, and random. The only difference is that the training and testing data distributions are different to simulate dynamic data at the clients. For example, in **10:90**\(50\) clients have \(90\%\) training data with \(10\%\) testing data from \(D_{A}\) and \(10\%\) training data with \(90\%\) testing data from \(D_{B}\) and vice versa. Similar to this, all the other partitions also have inverse training and testing data distributions. The reason for separate training and testing data distributions are explained in further depth in Appendix.

### Focus of Experimental Study

First, we compare the clustering ability of PI-FL with a recent clustering-based pFL algorithm . Second, we show how PI-FL compares with other non-clustering pFL models with a simple test accuracy comparison. Taking it one step further, we provide a comparison of PI-FL and other clustering and non-clustering pFL models in terms of reduction in opt-outs and PMA maintenance in section 5.3. Lastly, in section 5.5 we show that including client preferences while clustering yields better personalization results because clients can make decisions based on knowledge restricted to the aggregator server.

### Test Accuracy performance study.

**Effectiveness of clustering.** We evaluate the performance of cluster-level models using holdout datasets sampled from the corresponding cluster distributions (\(D_{A}\) and \(D_{B}\)). To demonstrate the effectiveness of our proposed PI-FL method, we compare it with a recent cluster-based pFL algorithm called FedSoft using CIFAR10 data. We use the same parameters as in , with \(N=100\) clients, batch size 128, and learning rate \(=0.01\), and perform training for 300 rounds. Table 1 presents the test accuracy for the **10:90** and **30:70** partitions with PI-FL. We observe that PI-FL performs better for the 10:90 partition, where each cluster dominates one of the distributions. With PI-FL, clients that have a greater portion of data from \(_{0}\) prefer to train in cluster \(c_{0}\), achieving \(63.68\%\) accuracy, while clients with a greater portion of data from \(_{1}\) prefer to train in cluster \(c_{1}\), achieving \(63.82\%\) accuracy. FedSoft cluster-level models, on the other hand, achieve \(50.7\%\) and \(49.6\%\) for 10:90 data. It is worth noting that FedSoft is unable to cater to different partitions of data through its clustering mechanism, and the performance is adversely impacted by increased heterogeneity. Moreover, cluster-level models in FedSoft are unable to dominate a single distribution of data. As expected, the performance for the 30:70 partition is not as good as it is a less heterogeneous partition than the 10:90 partition. Neither cluster dominates a single distribution, and the clients with different distributions are not clearly differentiated for training with different clusters. Additionally, the cluster-level models \(c_{0}\) and \(c_{1}\) have similar performance with either distribution (\(_{0}\) and \(_{1}\)), as FedSoft promotes personalizing models when clients have a greater percentage of shared data. This generates cluster-level models that cannot represent a single distribution and do not perform as well as PI-FL with non-IID data.

**Comparison with non-clustering pFL models.** Table 2 shows a test accuracy comparison of PI-FL with other recent pFL algorithms. This table shows that some pFL models are able to perform well for individual partitions such as Ditto for 10:90, FedProx and FedALA for Linear, and PerFedAvg for Random, however, PI-FL is able to maintain its performance for all partitions.

### Effectiveness of PI-FL in opt-outs reduction and PMA maintenance.

Each client's natural aim is to create a model that maximizes its test accuracy. Clients can have different thresholds of how much should be the least accuracy gain for it to participate in pFL, and we define this self-defined threshold as \(_{i}\), \(i[N]\). Since each client can have its own definition of the threshold requirement, we define \(_{i}\) as the test accuracy achieved by client \(i\) if it used FedAvg. So \(_{i}\) shows the gain in performance from pFL compared to vanilla FL using FedAvg for client \(i\) in \(N\). PMA is similar to GMA from , however, creating a single global model may not be appealing for all clients as we show in section 4 and verify in section 5.4. We formally define PMA and opt-outs in Equation 14 and 15 respectively, where \(f_{i}(w_{k})\) is the test accuracy achieved by pFL.

\[_{i}=f_{i}(w_{k})-_{i} i[N],k[K]\] (14)

\[=_{i=1}^{N}f_{i}(w_{k})<_{i} i[N],k [K]\] (15)

Figure 2 shows the empirical Cumulative Distribution Function (CDF) plot of PMA for all clients with CIFAR10 data using FedSoft and PIFL and with EMNIST dataset for all other pFL models. PI-FL particularly outperforms for the 10:90 partition in terms of PMA as this is the most heterogeneous data partition as can be seen in Figure 1(a). The EMNIST dataset is less heterogeneous as it has more classes per client compared to CIFAR10 which is why FedAvg is able to perform relatively well and there is less room for improvement with personalizing. PI-FL maintains the PMA and also improves it, particularly for the 10:90 and 30:70 partitions where other pFL solutions lack. We also test on a more heterogenous case where the dataset is divided into 52 clusters and each client owns 4 maximum classes. Figure 1(g) shows that while other pFL solutions perform better than FedAvg only Ditto and FedProto come relatively close to PI-FL, however, PI-FL outperforms them both by approximately \(15\%\) in terms of PMA. The FedProx, FedALA, and PerFedAvg opt-out ratios are 0.64, 0.31, and 0.68, respectively. Ditto, FedFomo, and PI-FL have no opt-outs. This goes to show that PI-FL is not only able to reduce the opt-outs but also improves the PMA under all data heterogeneity conditions.

### Advantages of including client preferences in pFL.

We show that PI-FL can maintain the test accuracy of personalized models even in case of dynamic data at the client or a new unseen client accidentally being added to the wrong cluster. Figure 3 shows the CDF of clients' personalized model test accuracy after training for 500 rounds. PI-FL is robust to variations in clients' local data, while FedSoft is less effective due to its clustering approach being based on the server's perspective, which lacks access to clients' private data and limits its ability to make accurate clustering decisions.

**Ablatian study with Incentive in PI-FL.** To measure the impact of incentive provision on personalized model generation we test PI-FL with incentives enabled and disabled. Figure 4 shows the CDF of clients' personalized model test accuracy with the Synthetic CIFAR10 dataset. Except for the 30:70 partition, the accuracy for all other partitions is higher with the incentive enabled. We argue that the test accuracy for 30:70 is low in this case because it is a less heterogeneous data case and PI-FL performs best in cases where data is highly heterogeneous and requires personalized learning. Further details of the experimental setup and impact of incentive on clustering are discussed in the Appendix.

## 6 Conclusion

In this paper, we proposed PI-FL to address the challenges of incentive provision in pFL for increasing consistent participation by providing appealing personalized models to clients. PI-FL client-centric clustering approach ensures accurate clustering and improved performance even in case of dynamic data distribution shift of the client's local data or inadvertently mistaken clustering decision by the client. Unlike prior works that consider incentivizing and personalization as separate problems, PI-FL solves them as interrelated challenges yielding improvement in pFL performance. Extensive empirical evaluation shows its promising performance compared to other state-of-the-art works.

Figure 4: PI-FL with and without incentive (I/NI)

Figure 3: PI-FL and FedSoft with Synthetic CIFAR10 data

Figure 2: CDF of clientsâ€™ PMA for different datasets and methods