# Data Measurements for Decentralized Data Markets

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Decentralized data markets can provide more equitable forms of data acquisition for machine learning. However, to realize practical marketplaces, efficient techniques for seller selection need to be developed. We propose and benchmark federated data measurements to allow a data buyer to find sellers with relevant and diverse datasets. Diversity and relevance measures enable a buyer to make relative comparisons between sellers without requiring intermediate brokers and training task-dependent models.

## 1 Introduction

Massive training datasets have proved foundational to AI breakthroughs, from earlier deep learning breakthroughs in computer vision to large language models (LLM) . However, AI companies face increasing scrutiny and backlash for their data collection practices, resulting in lawsuits from data owners such as artists, software developers, and journalists . As AI applications continue to be developed and deployed, more equitable and transparent means of data acquisition must be designed and implemented . Recently, data markets have been proposed to incentivize greater data sharing and access for data-restricted domains . As the ethical challenges and legal risks of acquiring data increase, data market platforms will be crucial to address the ethical and economic challenges in training AI models.

To facilitate practical data market platforms, we investigate the challenge of _seller selection_ for a data buyer using a framework based on federated data measurements. We benchmark several proposed heuristic measures of _diversity_ and _relevance_, which can be used by the buyer to compare the relative value of different sellers. The advantage of this federated data measurement framework is that it does not require direct access to the seller's data, is training-free, and is task-agnostic. These attributes are desirable for a decentralized marketplace to enable scalable seller selection for many different buyers. The three main steps of the data measurement framework are depicted in Figure 1. We evaluate several definitions of diversity and relevance on multiple computer vision datasets by benchmarking each data measurement for its ability to rank sellers, correlation with classification performance, and robustness to duplicate and noisy data. In summary, we show that federated data measurements allow private and lightweight seller discovery that can lower search costs for a data buyer in a decentralized data marketplace.

## 2 Decentralized Data Markets

Current data brokers are highly centralized and aggregate vast amounts of data, often without a user's knowledge, consent, or compensation . This massive centralization of data has led to increaseddata breaches, the erosion of privacy, and harmful data misuse. For example, the 2017 Equifax data breach exposed the private records of more than 150 million people . In contrast, decentralized data markets may present a more equitable and efficient approach to data acquisition [53; 55; 36].

On a decentralized marketplace platform, buyers can transact directly with sellers, bypassing intermediate data brokers by utilizing decentralized and privacy-enhancing technologies such as smart contracts and trusted execution environments [28; 6]. Bypassing data brokers may result in lower transaction costs and greater market efficiency by allowing data owners to capture more of the revenue generated from their data. In addition, whereas data brokers indiscriminately acquire data and sell bundled datapoints wholesale, data marketplaces could take a more targeted approach to data acquisition. by only paying for the most valuable datapoints, lowering the overall privacy incursion . Lastly, compensating data owners may incentivize greater data access from a more diverse range of individual data producers, which may decrease bias in data acquisition by increasing participation from smaller, more heterogeneous data sources.

However, to fully realize this paradigm shift to decentralized data marketplaces, scalable methods are needed to match buyers with relevant data sellers. A survey of data market participants found that finding relevant sellers was a major source of friction and recommended lowering search costs for the data buyer . In a centralized one-sided marketplace, this process can be facilitated by a data broker. However, in the absence of brokers in a decentralized marketplace, we need federated techniques to signal the value of data sellers to different buyers, each of whom may have different preferences and goals for data acquisition. This problem of seller selection is related to client selection in federated learning . Without new federated methods to lower search costs, market platforms will struggle to attract enough participants to attain the scale and network effects for a sustainable marketplace.

Most current work in data valuation, such as Data Shapley , assumes a centralized setting where all data is fully accessible to train models to estimate data value. In a decentralized setting, a seller would not permit data access before payment since data is easily copied. However, a buyer would be reluctant to pay a fair price for data if they cannot be assured of its value. Therefore, a fundamental asymmetry arises between the buyer and seller, related to Arrow's Information Paradox , resulting in increased search costs and fewer transactions taking place. New methods must be developed for the decentralized data market setting taking into account only limited, "white-box" data access .

To allow a buyer to search for the most promising sellers in a decentralized marketplace, we evaluate _federated data measurements_, which have the advantage of being computationally cheap to compute, task-agnostic, and only require indirect data access. Many different data measurements have been developed to quantify intrinsic, task-agnostic characteristics [48; 40; 43]. Data measurements can be general-purpose, such as central tendency (e.g., mean, median) and "distance" (e.g., Euclidean distance, KL divergence) or modality-specific, such as Frechet Inception Distance  and lexical diversity . Recent work proposed to use conditional diversity and relevance measurements to value data without requiring model training or validation data evaluation . We incorporate their work by evaluating several other definitions of diversity and relevance in the context of private and federated data valuation on medical imaging datasets.

Figure 1: **Steps of data measurements framework. A buyer embeds their data through some embedding model and sends a private query of matrix projections to each seller. Each seller responds with data measurements that allow the buyer to compare and transact with sellers that have the most relevant data.**

Federated Data Measurements

Instead of directly attempting to measure the contribution of each datapoint in the seller's dataset, we measure inherent properties of the seller's aggregate dataset through data measurements. These _data measurements_, \(\), can be used by the buyer to compare between data sellers. For instance, a seller \(j\) with measurement \(_{j}_{i}\) would be deemed to have more valuable data than seller \(i\).

Many data measurements have been developed to quantify intrinsic, task-agnostic characteristics . Data measurements can be general-purpose, such as central tendency and distance metrics, or modality-specific, such as Frechet Inception Distance  and lexical diversity . Many data quality measures have been developed for structured relational data, such as completeness, consistency, and accuracy; however, data quality becomes more complicated for unstructured data .

Before measuring the seller's data, a buyer sends a personalized _query_, \(\), to each seller. We assume that a buyer has a small sample of reference data, \(X_{i}^{}^{}\), from the desired distribution to create the query. The buyer communicates this query to the seller, and the seller uses this query to transform their data, calculate the data measurements, and return the measurements to the buyer. The query can be any matrix projection to measure the seller's data. For instance, this basis can be chosen to maximize variance (PCA), independence (ICA), or class separability (LDA) . Empirically, we found PCA with 10 principal directions appropriate for most datasets as most of the variance is captured in the first few components (see Figure 11).

Another common preprocessing step is to embed data into a low-dimensional representation using a deep learning model . The choice of embedding, \(f:^{d}\), can incorporate domain-specific knowledge and has become popular for retrieval augmented generation (RAG) and vector databases . For our benchmark, we use a pretrained CLIP (ViT-16) model -- due to its good performance for zero-shot capabilities across a wide range of image domains -- to precompute 512-dimensional embedding vectors for each dataset . We envision that more application-specific platforms could use multiple choices of embeddings, such as medical foundation models .

First, buyer \(i\) sends seller \(j\) their query, \(=_{k}(f(^{}))\), where \(_{k}:^{n d}^{k d}\) computes the first \(k\) principal directions using the buyer's reference data. Then, the seller uses this query to transform their data and returns certain information to the buyer to calculate a specified data measurement. The measurement function, \(g:^{k d}^{k d}\), takes in the projected data from the seller and buyer to produce a scalar data measurement \(_{ij}\), \(_{ij}=g(^{},^{ })\), where \( f()^{}f()\) is the covariance matrix of the embedded data.

In prior work, \(g\) has been defined as measuring heuristic notions of _relevance_ and _diversity_. For our benchmark, we evaluate the four different definitions of relevance and four definitions of diversity for our decentralized data market setting. Intuitively, relevance should capture the similarity between the buyer and seller. For example, if the buyer has chest X-ray (CXR) images with COVID-19, then a seller with similar COVID-19 CXR images would be more relevant than CXR from normal patients. Likewise, CXR data should be more relevant than MRI data or photography images. We evaluate four definitions of relevance for seller selection.

1. **Negative Euclidean (L2) distance** between the mean vectors of the buyer and seller: \(-\|}^{}-}^{} \|_{2}\), where \(}_{i=1}^{k}_{i}_{i}\).
2. **Cosine similarity** between mean vectors: \((}^{}}^{})/ \|}^{}\|_{2}\|}^ {}\|_{2}\).
3. **Correlation** between mean vectors: \(}}{{}^{}}},}^{}}}{{}(}^{ })(}^{})}}\).
4. **Overlap** between principal components : \(^{k}(_{i}^{},_{i}^{})}/(_{i}^{},_{i}^{})}}}{{\ _{i}\|_{i}_{i}\|_{2}}\) is the magnitude of the projected vector.

For many machine learning applications, using only relevance measures is insufficient to guarantee useful training data. For example, a seller's data may be highly relevant but have duplicate data or imbalanced classes that result in brittle, low-performing models. Intuitively, a seller with X-ray images from 1,000 unique patients contains more non-redundant information than 1,000 X-rays from a single patient. Then, training on the more diverse seller should lead to better model generalization on unseen test data as more of the input space has been learned [70; 20]. We evaluate four definitions of diversity.

1. **Volume** of the projected covariance : \(((^{}))\)
2. **Vendi score**, defined as the exponential of negative entropy of eigenvalues of the covariance: \((-(^{} ^{}))\).
3. **Dispersion** of the features, measured as the geometric mean of standard deviations : \([k]{(_{i=1}^{k}(^{ }^{})_{i})}\)
4. **Difference** in the normalized magnitude between principal components : \([k]{_{i=1}^{k}|_{i}^{}-_{i}^{ }|/(_{i}^{},_{i}^{ })},\) where \(_{i}\|_{i}_{i}\|_{2}\).

These data measurements of diversity and relevance are computationally efficient to compute, even for large datasets (>100,000 datapoints), and only require indirect data access from each seller. Additionally, leveraging deep embeddings allows high-dimensional, multi-modal data such as images and text to be measured in a task-agnostic and training-free manner.

## 4 Experiments

Ranking Sellers with MeasurementsWe first evaluate each data measurement in correctly ranking the seller with data IID with the buyer's distribution. For example, when the buyer has reference data from ImageNet, the seller with ImageNet data should have the largest data measurement (see Figure 8). A common metric to evaluate ranking quality in information retrieval is discounted cumulative gain (DCG) . For simplicity, we assume that the IID seller has a maximum gain of \(1\) and non-IID sellers have a gain of \(0\). In Table 1, we report the mean rank of the IID seller and DCG over 10 random trials using 20 computer vision datasets (listed in Appendix A). For all experiments, we use 100 datapoints for the buyer query and 10,000 datapoints for each seller unless otherwise specified.

Overall, we find that relevance measurements, such as L2 distance and the "overlap" measure, are better than diversity measurements at ranking the IID seller. This reflects the intuition that relevance directly compares distributional information between buyer and seller. On the other hand, most diversity measures only consider information from the buyer through the query projection step. Among all data measurements, the "difference" measure had the lowest DCG, often ranking the IID seller very low (see Figure 8 for an example).

Correlation with Downstream Classifier PerformanceNext, we evaluate how useful each data measurement is as a proxy for training data quality. In this experiment, we assume that the buyer wants to use the seller's data to train a model to predict a held-out test set, which is IID with the buyer's query data. We train a model for each seller using their data as a training set and correlate the

    & Avg. ranking \(\) & Avg. DCG \(\) \\   & L2 & \(1.25 0.85\) & \(0.94 0.15\) \\  & Cosine & \(1.28 0.99\) & \(0.94 0.16\) \\  & Correlation & \(1.34 1.16\) & \(0.93 0.17\) \\  & Overlap & \(\) & \(\) \\   & Volume & \(3.64 5.28\) & \(0.79 0.30\) \\  & Vendi & \(3.38 2.87\) & \(0.69 0.31\) \\   & Dispersion & \(2.73 2.87\) & \(0.80 0.29\) \\   & Difference & \(19.47 1.04\) & \(0.23 0.0\) \\   

Table 1: Performance of data measurements for seller rankingresulting model's test performance with the data measurements for that seller. In this way, a seller with a high data measurement value should ideally have test performance for a particular buyer than a seller with a lower data measurement value.

We use four medical imaging datasets (BloodMNIST, OrganMNIST, PathMNIST, and TissueMNIST) from the MedMNIST benchmark (see Figure 6 for example images) . To introduce heterogeneity between sellers, we sample classes from a Dirichlet distribution as standard practice in federated learning to simulate non-IID clients . For each dataset, we evaluate three different prediction task scenarios: binary classification with logistic regression, multiclass classification with a random forest classifier, and K-means clustering. For each data buyer, we randomly sample a subset of classes for multiclass classification and evaluate the accuracy score as the performance metric. For binary classification, we consider the selected subset of classes as "positive" and the other classes as "negative" and evaluate accuracy. For clustering, we set the number of clusters equal to the total number of classes for each dataset and evaluate homogeneity score, a common clustering metric, as the performance metric .

For another baseline, we also evaluate two centralized data valuation, KNN Shapley  and LAVA , using the OpenDataVal framework . We selected these two valuation methods for

   Prediction Task &  &  \\  & & Blood & Organ & Path & Tissue & Avg. \\   & L2 & -0.02 & 0.04 & 0.03 & 0.10 & 0.04 \\  & Cosine & 0.16 & 0.09 & 0.13 & 0.20 & 0.15 \\  & Correlation & 0.13 & 0.07 & 0.13 & 0.21 & 0.14 \\  & Overlap & 0.04 & -0.02 & 0.01 & 0.06 & 0.02 \\   & Volume & **0.28** & **0.29** & **0.31** & **0.28** & **0.29** \\  & Vendi & 0.19 & 0.19 & 0.22 & 0.18 & 0.20 \\  & Dispersion & 0.17 & 0.18 & 0.18 & 0.14 & 0.17 \\  & Difference & -0.03 & 0.02 & 0.03 & -0.09 & -0.02 \\   & KNN Shapley & 0.10 & 0.07 & 0.05 & 0.08 & 0.08 \\  & LAVA & -0.02 & -0.02 & 0.02 & 0.01 & 0.00 \\   & L2 & 0.22 & 0.15 & 0.19 & 0.22 & 0.20 \\  & Cosine & 0.23 & 0.14 & 0.12 & 0.18 & 0.17 \\  & Correlation & 0.24 & 0.15 & 0.12 & 0.19 & 0.18 \\  & Overlap & 0.27 & 0.19 & 0.19 & 0.24 & 0.22 \\   & Volume & **0.42** & **0.35** & **0.32** & **0.36** & **0.36** \\  & Vendi & 0.30 & 0.23 & 0.19 & 0.22 & 0.24 \\  & Dispersion & 0.22 & 0.20 & 0.12 & 0.18 & 0.18 \\  & Difference & -0.23 & -0.14 & -0.14 & -0.18 & -0.17 \\   & KNN Shapley & 0.09 & 0.12 & 0.07 & 0.12 & 0.10 \\  & LAVA & -0.01 & 0.00 & -0.02 & 0.00 & -0.01 \\   & L2 & 0.22 & 0.23 & 0.20 & 0.19 & 0.21 \\  & Cosine & 0.29 & 0.28 & 0.31 & 0.26 & 0.29 \\  & Correlation & 0.29 & 0.29 & 0.31 & 0.26 & 0.29 \\  & Overlap & 0.31 & 0.35 & 0.36 & 0.32 & 0.34 \\   & Volume & **0.55** & **0.54** & **0.52** & **0.55** & **0.54** \\  & Vendi & 0.45 & 0.45 & 0.49 & 0.48 & 0.47 \\  & Dispersion & 0.35 & 0.38 & 0.32 & 0.36 & 0.25 \\  & Difference & -0.22 & -0.27 & -0.29 & -0.25 & -0.26 \\   & KNN Shapley & 0.01 & 0.05 & 0.02 & -0.01 & 0.02 \\  & LAVA & 0.01 & 0.00 & -0.03 & 0.02 & 0.00 \\   

Table 2: Correlation test performance across three tasks on four MedMNIST datasetstheir efficient runtime. We split the seller's data into 20% for training and used the other 80% as a validation set. To aggregate a value for each seller, we take the average data value of the validation datapoints. In Table 2, we report these correlations between data measurement and test accuracy for 500 sellers, each with 5,000 datapoints, and average correlations over 10 buyers for each dataset.

Intuitively, we expect that sellers with more similar data as the buyer will learn higher-performing classifiers and be associated with larger data measurement values. For several of the diversity measures (volume, Vendi score), we find a moderate-strong correlation to test performance across datasets and prediction tasks. See Figure 9 for an example of strong correlations between volume measurements and test prediction accuracy. Compared to diversity measures, relevance measures and the centralized data valuation methods (KNN Shapley, LAVA) had a weak correlation with downstream classification performance. These results support that a seller with higher diversity measurements is more likely to have training data that is more useful for a particular, even without specifying the exact prediction task or model architecture. Similar observations between generalization performance and data diversity are reported in determinantal point processes .

Detecting Seller Misreporting with Multiple QueriesOne practical challenge that arises with a decentralized marketplace is ensuring that the seller is not able to "cheat" by artificially inflating the value of their data measurements. In the case of relevance measures, a malicious seller would aim to report mean vectors similar to those of the buyer, but a buyer could avoid sending their own mean vectors to prevent this. However, this strategy would not work for diversity measures, which are independent of the buyer's data given the query.

To counteract this, a buyer could send multiple queries containing "false" directions that may be computed using non-relevant data or even random directions in addition to their actual data (see Figure 10. Then, the buyer could discount sellers with large data measurements in these false directions while only considering sellers with high value using the real query. We evaluate each data measurement's ability to discriminate between data measurements using the real query and false queries with the following ratio

\[(\%)=}}{(\{_{ }^{(i)}\}_{i}^{m},\%)},\] (1)

which is simply the ratio of the data measurement using the real query \(_{}\) over the \(\%\)-quantile of measurement using false queries. In our experiment, we compute false queries using 20 non-IID datasets and consider three quantile threshold ratios: \(50\%\), \(75\%\), and \(90\%\). The \(50\%\) ratio corresponds to the real IID measurement divided by the median measurement when using buyer queries from the 19 other non-IID datasets.

In Table 3, we report measurement ratios and find that most data measurements of relevance and diversity have high ratios, implying that sending multiple queries can be an effective strategy to deal with adversarial sellers that misreport their measurements. This will incentivize the sellers to honestly report their true data measurements as they do not know which queries are real or fake. Sending

    &  \\  & & 50\% & 75\% & 90\% \\   & L2 & \(1.02\) & \(0.93\) & \(0.89\) \\  & Cosine & \(\) & \(1.57\) & \(1.25\) \\  & Correlation & \(2.83\) & \(1.53\) & \(1.18\) \\  & Overlap & \(2.88\) & \(\) & \(\) \\   & Volume & \(1.39\) & \(1.31\) & \(1.24\) \\  & Vendi Score & \(2.20\) & \(1.92\) & \(\) \\   & Dispersion & \(1.91\) & \(1.73\) & \(1.58\) \\   & Difference & \(0.38\) & \(0.30\) & \(0.27\) \\   

Table 3: Ratio of measurement using real query over measurements of false queriesadditional queries increases communication overhead, but this may be tolerable since each query is cheap -- being only a \(\) matrix, where \(\). For instance, each of our queries is \(10 512\) in our experiments.

Robustness to Duplicate DataBecause there is no cost to copying data, an adversarial seller may duplicate portions of their data to try to obtain higher measurement values. In Figure 2, we vary the amount of duplicate data to observe the effect on each data measurement when both the seller and buyer have IID data. We note that the implementation of the considered volume method  explicitly quantizes the data into a \(d\)-dimensional hypercube to achieve robustness to duplicate data. Therefore, increasing the amount of duplicated data has a negative effect on volume. For all other data measurements, the value is relatively consistent until falling off for extreme numbers of duplicates, e.g., each datapoint is duplicated 200 times, leaving only \(}{{200}}=50\) unique datapoints. Exploring duplicate-robust versions of data measurements would be interesting for future work.

Effect of Noisy and Corrupted DataIn this experiment, we utilize the ImageNet-C benchmark dataset  to study the effect of 19 different types of noise corruptions (blurring, intensity changes,

Figure 3: Effect of different types of noise corruptions on each data measurement. See Figure 7 for example images on the ImageNet-C dataset.

Figure 2: Effect of duplicate data on data measurements. Each seller has 10,000 total datapoints, and a subset of datapoints are duplicated, keeping the total number of datapoints the same. Each colored dotted line represents an individual dataset, and the solid black line represents the average of all datasets. Errors bars represent one standard deviation.

[MISSING_PAGE_FAIL:8]

Figure 5. We find that data measurements were relatively stable for most datasets after around 100 query datapoints.

## 5 Discussion

As observed in the experiments, both diversity and relevance measures capture important aspects of data value for a buyer. Relevance measures allow a buyer to filter out irrelevant data and identify sellers with in-domain data distributions. On the other hand, diversity measures, such as volume, reveal which sellers have the most informative and useful data (correlated with test performance, non-duplicated data). As shown with the corruption experiments using ImageNet-C, both diversity and relevance are associated with data quality as noisier and more corrupted data have lower data measurements.

In contrast with prior work , we find their "difference" definition of diversity to underperform in most experiments compared to other definitions of diversity. Subjectively, we observe that "difference" measurements tend to be the inverse of "overlap" measurements and thus redundant in terms of information. On the other hand, volume has additional nice properties, such as being robust to data duplication and increasing with the number of seller datapoints. Based on our benchmark experiments, we conclude that cosine similarity and "overlap" are appropriate relevance measures and that the volume-based definition of diversity is well-suited for seller selection.

Advantages of Federated Data MeasurementsUnlike centralized and training-based approaches to data valuation, using federated data measurements is a lightweight and private way to match a buyer with relevant sellers in a decentralized marketplace with millions of participants. Measuring a seller's data is agnostic to the modeling task and model architecture. This approach allows a buyer to compare the value of multiple sellers relatively without requiring direct access to the seller's data, which would not be allowed before payment. Different choices of embedding functions could be precomputed to serve different types of modalities and domains. In summary, this decentralized data valuation scheme allows private and scalable seller discovery to lower search costs for a data buyer, enabling more efficient markets and lower transaction costs.

LimitationsWhile our work presents an initial benchmark of different data measurements, it is limited in several ways. Firstly, while our data measurements framework can accommodate other types of data modalities such as text and tabular data, we only consider common computer vision datasets for our benchmark. Future work would extend the experiments and embeddings for other domains such as natural language and graphical data. Another limitation is the lack of formal privacy guarantees. While the federated nature of the query and measurement step should prevent reconstruction attacks, techniques such as differential privacy  and homomorphic encryption  could be employed to provide explicit guarantees. Additionally, further work could incorporate incentive mechanisms to study adversarial seller behavior.

## 6 Conclusion

Reimagining a new decentralized model of data acquisition where individual data producers are fairly compensated for sharing data could help redistribute the economic benefits from AI technology to those whose data enables AI research and development . Decentralized data markets may address issues with current centralized settings by providing a more equitable and efficient exchange of data resources, as well as enabling more collective data governance .

In this paper, we presented federated data measurements for decentralized data marketplaces. These measurements allow a buyer to perform seller selection without direct access to the seller's data and are more scalable than current data valuation approaches. We benchmark several properties of data measurements on computer vision datasets and find that a combination of relevance and diversity performs well for several practical data marketplace considerations.