# Smoothing the Landscape Boosts the Signal for SGD Optimal Sample Complexity for Learning Single Index Models

Smoothing the Landscape Boosts the Signal for SGD Optimal Sample Complexity for Learning Single Index Models

 Alex Damian

Princeton University

ad27@princeton.edu

Eshaan Nichani

Princeton University

eshnich@princeton.edu

Rong Ge

Duke University

rongge@cs.duke.edu

Jason D. Lee

Princeton University

jasonlee@princeton.edu

###### Abstract

We focus on the task of learning a single index model \((w^{} x)\) with respect to the isotropic Gaussian distribution in \(d\) dimensions. Prior work has shown that the sample complexity of learning \(w^{}\) is governed by the _information exponent_\(k^{}\) of the link function \(\), which is defined as the index of the first nonzero Hermite coefficient of \(\). Ben Arous et al.  showed that \(n d^{k^{}-1}\) samples suffice for learning \(w^{}\) and that this is tight for online SGD. However, the CSQ lower bound for gradient based methods only shows that \(n d^{k^{}/2}\) samples are necessary. In this work, we close the gap between the upper and lower bounds by showing that online SGD on a smoothed loss learns \(w^{}\) with \(n d^{k^{}/2}\) samples. We also draw connections to statistical analyses of tensor PCA and to the implicit regularization effects of minibatch SGD on empirical losses.

## 1 Introduction

Gradient descent-based algorithms are popular for deriving computational and statistical guarantees for a number of high-dimensional statistical learning problems [2; 3; 1; 4; 5; 6]. Despite the fact that the empirical loss is nonconvex and in the worst case computationally intractible to optimize, for a number of statistical learning tasks gradient-based methods still converge to good solutions with polynomial runtime and sample complexity. Analyses in these settings typically study properties of the empirical loss landscape , and in particular the number of samples needed for the signal of the gradient arising from the population loss to overpower the noise in some uniform sense. The sample complexity for learning with gradient descent is determined by the landscape of the empirical loss.

One setting in which the empirical loss landscape showcases rich behavior is that of learning a _single-index model_. Single index models are target functions of the form \(f^{*}(x)=(w^{} x)\), where \(w^{} S^{d-1}\) is the unknown relevant direction and \(\) is the known link function. When the covariates are drawn from the standard \(d\)-dimensional Gaussian distribution, the shape of the loss landscape is governed by the _information exponent_\(k^{*}\) of the link function \(\), which characterizes the curvature of the loss landscape around the origin. Ben Arous et al.  show that online stochastic gradient descent on the empirical loss can recover \(w^{*}\) with \(n d^{(1,k^{*}-1)}\) samples; furthermore, they present a lower bound showing that for a class of online SGD algorithms, \(d^{(1,k^{*}-1)}\) samples are indeed necessary.

However, gradient descent can be suboptimal for various statistical learning problems, as it only relies on local information in the loss landscape and is thus prone to getting stuck in local minima. For learning a single index model, the Correlational Statistical Query (CSQ) lower bound only requires\(d^{(1,k^{*}/2)}\) samples to recover \(w^{}\)[6; 4], which is far fewer than the number of samples required by online SGD. This gap between gradient-based methods and the CSQ lower bound is also present in the Tensor PCA problem ; for recovering a rank 1 \(k\)-tensor in \(d\) dimensions, both gradient descent and the power method require \(d^{(1,k-1)}\) samples, whereas more sophisticated spectral algorithms can match the computational lower bound of \(d^{(1,k/2)}\) samples.

In light of the lower bound from , it seems hopeless for a gradient-based algorithm to match the CSQ lower bound for learning single-index models.  considers the regime in which SGD is simply a discretization of gradient flow, in which case the poor properties of the loss landscape with insufficient samples imply a lower bound. However, recent work has shown that SGD is not just a discretization to gradient flow, but rather that it has an additional implicit regularization effect. Specifically, [9; 10; 11] show that over short periods of time, SGD converges to a quasi-stationary distribution \(N(, S)\) where \(\) is an initial reference point, \(S\) is a matrix depending on the Hessian and the noise covariance and \(=\) measures the strength of the noise where \(\) is the learning rate and \(B\) is the batch size. The resulting long term dynamics therefore follow the _smoothed gradient_\(L()=_{z N(0,S)}[ L(+ z)]\) which has the effect of regularizing the trace of the Hessian.

This implicit regularization effect of minibatch SGD has been shown to drastically improve generalization and reduce the number of samples necessary for supervised learning tasks [12; 13; 14]. However, the connection between the smoothed landscape and the resulting sample complexity is poorly understood. Towards closing this gap, we consider directly smoothing the loss landscape in order to efficiently learn single index models. Our main result, Theorem 1, shows that for \(k^{}>2\), online SGD on the smoothed loss learns \(w^{}\) in \(n d^{k^{}/2}\) samples, which matches the correlation statistical query (CSQ) lower bound. This improves over the \(n d^{k^{}-1}\) lower bound for online SGD on the unsmoothed loss from Ben Arous et al. . Key to our analysis is the observation that smoothing the loss landscape boosts the signal-to-noise ratio in a region around the initialization, which allows the iterates to avoid the poor local minima for the unsmoothed empirical loss. Our analysis is inspired by the implicit regularization effect of minibatch SGD, along with the partial trace algorithm for Tensor PCA which achieves the optimal \(d^{k/2}\) sample complexity for computationally efficient algorithms.

The outline of our paper is as follows. In Section 3 we formalize the specific statistical learning setup, define the information exponent \(k^{}\), and describe our algorithm. Section 4 contains our main theorem, and Section 5 presents a heuristic derivation for how smoothing the loss landscape increases the signal-to-noise ratio. We present empirical verification in Section 6, and in Section 7 we detail connections to tensor PCA had minibatch SGD.

## 2 Related Work

There is a rich literature on learning single index models. Kakade et al.  showed that gradient descent can learn single index models when the link function is Lipschitz and monotonic and designed an alternative algorithm to handle the case when the link function is unknown. Soltanolkotabi  focused on learning single index models where the link function is \((x):=(0,x)\) which has information exponent \(k^{}=1\). The phase-retrieval problem is a special case of the single index model in which the link function is \((x)=x^{2}\) or \((x)=|x|\); this corresponds to \(k^{}=2\), and solving phase retrieval via gradient descent has been well studied [17; 18; 19; 20]. Dudeja and Hsu  constructed an algorithm which explicitly uses the harmonic structure of Hermite polynomials to identify the information exponent. Ben Arous et al.  provided matching upper and lower bounds that show that \(n d^{(1,k^{}-1)}\) samples are necessary and sufficient for online SGD to recover \(w^{}\).

Going beyond gradient-based algorithms, Chen and Meka  provide an algorithm that can learn polynomials of few relevant dimensions with \(n d\) samples, including single index models with polynomial link functions. Their estimator is based on the structure of the filtered PCA matrix \(_{x,y}[_{|y|}xx^{T}]\), which relies on the heavy tails of polynomials. In particular, this upper bound does not apply to bounded link functions. Furthermore, while their result achieves the information-theoretically optimal \(d\) dependence it is not a CSQ algorithm, whereas our Algorithm 1 achieves the optimal sample complexity over the class of CSQ algorithms (which contains gradient descent).

Recent work has also studied the ability of neural networks to learn single or multi-index models [5; 6; 23; 24; 4]. Bietti et al.  showed that two layer neural networks are able to adapt to unknown link functions with \(n d^{k^{}}\) samples. Damian et al.  consider multi-index models with polynomial link function, and under a nondegeneracy assumption which corresponds to the \(k^{}=2\) case, show that SGD on a two-layer neural network requires \(n d^{2}+r^{p}\) samples. Abbe et al. [24; 4] provide a generalization of the information exponent called the _leap_. They prove that in some settings, SGD can learn low dimensional target functions with \(n d^{-1}\) samples. However, they conjecture that the optimal rate is \(n d^{/2}\) and that this can be achieved by ERM rather than online SGD.

The problem of learning single index models with information exponent \(k\) is strongly related to the order \(k\) Tensor PCA problem (see Section 7.1), which was introduced by Richard and Montanari . They conjectured the existence of a _computational-statistical gap_ for Tensor PCA as the information-theoretic threshold for the problem is \(n d\), but all known computationally efficient algorithms require \(n d^{k/2}\). Furthermore, simple iterative estimators including tensor power method, gradient descent, and AMP are suboptimal and require \(n d^{k-1}\) samples. Hopkins et al.  introduced the partial trace estimator which succeeds with \(n d^{[k/2]}\) samples. Anandkumar et al.  extended this result to show that gradient descent on a smoothed landscape could achieve \(d^{k/2}\) sample complexity when \(k=3\) and Biroli et al.  heuristically extended this result to larger \(k\). The success of smoothing the landscape for Tensor PCA is one of the inspirations for Algorithm 1.

## 3 Setting

### Data distribution and target function

Our goal is to efficiently learn single index models of the form \(f^{}(x)=(w^{} x)\) where \(w^{} S^{d-1}\), the \(d\)-dimensional unit sphere. We assume that \(\) is normalized so that \(_{x N(0,1)}[(x)^{2}]=1\). We will also assume that \(\) is differentiable and that \(^{}\) has polynomial tails:

**Assumption 1**.: _There exist constants \(C_{1},C_{2}\) such that \(|^{}(x)| C_{1}(1+x^{2})^{C_{2}}\)._

Our goal is to recover \(w^{}\) given \(n\) samples \((x_{1},y_{1}),,(x_{n},y_{n})\) sampled i.i.d from

\[x_{i} N(0,I_{d}), y_{i}=f^{}(x_{i})+z_{i} z _{i} N(0,^{2}).\]

For simplicity of exposition, we assume that \(\) is known and we take our model class to be

\[f(w,x):=(w x) w S^{d-1}.\]

### Algorithm: online SGD on a smoothed landscape

As \(w S^{d-1}\) we will let \(_{w}\) denote the spherical gradient with respect to \(w\). That is, for a function \(g:^{d}\), let \(_{w}g(w)=(I-ww^{T}) g(z)_{z=w}\) where \(\) is the standard Euclidean gradient.

To compute the loss on a sample \((x,y)\), we use the correlation loss:

\[L(w;x;y):=1-f(w,x)y.\]

Furthermore, when the sample is omitted we refer to the population loss:

\[L(w):=_{x,y}[L(w;x;y)]\]

Our primary contribution is that SGD on a _smoothed_ loss achieves the optimal sample complexity for this problem. First, we define the smoothing operator \(_{}\):

**Definition 1**.: _Let \(g:S^{d-1}\). We define the smoothing operator \(_{}\) by_

\[(_{}g)(w):=_{z_{w}}[g]\]

_where \(_{w}\) is the uniform distribution over \(S^{d-1}\) conditioned on being perpendicular to \(w\)._

This choice of smoothing is natural for spherical gradient descent and can be directly related1 to the Riemannian exponential map on \(S^{d-1}\). We will often abuse notation and write \(_{}(g(w))\) rather than \((_{}g)(w)\). The smoothed empirical loss \(L_{}(w;x;y)\) and the population loss \(L_{}(w)\) are defined by:

\[L_{}(w;x;y):=_{}(L(w;x;y)) L_{ }(w):=_{}(L(w)).\]

### Hermite polynomials and information exponent

The sample complexity of Algorithm 1 depends on the Hermite coefficients of \(\):

**Definition 2** (Hermite Polynomials).: _The \(k\)th Hermite polynomial \(He_{k}:\) is the degree \(k\), monic polynomial defined by_

\[He_{k}(x)=(-1)^{k}(x)}{(x)},\]

_where \((x):=}{2}}}{}\) is the PDF of a standard Gaussian._

The first few Hermite polynomials are \(He_{0}(x)=1,He_{1}(x)=x,He_{2}(x)=x^{2}-1,He_{3}(x)=x^{3}-3x\). For further discussion on the Hermite polynomials and their properties, refer to Appendix A.2. The Hermite polynomials form an orthogonal basis of \(L^{2}()\) so any function in \(L^{2}()\) admits a Hermite expansion. We let \(\{c_{k}\}_{k 0}\) denote the Hermite coefficients of the link function \(\):

**Definition 3** (Hermite Expansion of \(\)).: _Let \(\{c_{k}\}_{k 0}\) be the Hermite coefficients of \(\), i.e._

\[(x)=_{k 0}}{k!}He_{k}(x) c_{k} =_{x N(0,1)}[(x)He_{k}(x)].\]

The critical quantity of interest is the _information exponent_ of \(\):

**Definition 4** (Information Exponent).: \(k^{}=k^{}()\) _is the first index \(k 1\) such that \(c_{k} 0\)._

**Example 1**.: _Below are some example link functions and their information exponents:_

* \((x)=x\) _and_ \((x)=(x):=(0,x)\) _have information exponents_ \(k^{}=1\)_._
* \((x)=x^{2}\) _and_ \((x)=|x|\) _have information exponents_ \(k^{}=2\)_._
* \((x)=x^{3}-3x\) _has information exponent_ \(k^{}=3\)_. More generally,_ \((x)=He_{k}(x)\) _has information exponent_ \(k^{}=k\)_._

Throughout our main results we focus on the case \(k^{} 3\) as when \(k^{}=1,2\), online SGD without smoothing already achieves the optimal sample complexity of \(n d\) samples (up to log factors) .

## 4 Main Results

Our main result is a sample complexity guarantee for Algorithm 1:

**Theorem 1**.: _Assume \(w_{0} w^{} d^{-1/2}\) and \([1,d^{1/4}]\). Then there exists a choice of \(T_{1},\) satisfying \(T_{1}=d^{k^{}-1}^{-2k^{}+4}\) and \(=(d^{-k^{}/2}^{2k^{}-2})\) such that if we run Algorithm 1 with \(_{t}=\) and \(_{t}=\) for \(t T_{1}\) and \(_{t}=0\) and \(_{t}=O(d+t-T_{1})^{-1}\) for \(t>T_{1}\), we have that with high probability, after \(T=T_{1}+T_{2}\) steps, the final iterate \(w_{T}\) satisfies \(L(w_{T}) O(})\)._

Note that the condition that \(w_{0} w^{} d^{1/2}\) can be guaranteed with probability \(1/2\). Theorem 1 uses large smoothing (up to \(=d^{1/4}\)) to rapidly escape the regime in which \(w w^{} d^{-1/2}\). This first stage continues until \(w w^{}=1-o_{d}(1)\) which takes \(T_{1}=(d^{k^{}/2})\) steps when \(=d^{1/4}\). The second stage, in which \(=0\) and the learning rate decays linearly, lasts for an additional \(T_{2}=d/\)steps where \(\) is the target accuracy. Because Algorithm 1 uses each sample exactly once, this gives the sample complexity

\[n d^{k^{}-1}^{-2k^{}+4}+d/\]

to reach population loss \(L(w_{T})\). Setting \(=O(1)\) is equivalent to zero smoothing and gives a sample complexity of \(n d^{k^{}-1}+d/\), which matches the results of Ben Arous et al. . On the other hand, setting \(\) to the maximal allowable value of \(d^{1/4}\) gives:

\[n}{2}}}_{}+_{}\]

which matches the sum of the CSQ lower bound, which is \(d^{}{2}}\), and the information-theoretic lower bound, which is \(d/\), up to poly-logarithmic factors.

To complement Theorem 1, we replicate the CSQ lower bound in  for the specific function class \((w x)\) where \(w S^{d-1}\). Statistical query learners are a family of learners that can query values \(q(x,y)\) and receive outputs \(\) with \(|-_{x,y}[q(x,y)]|\) where \(\) denotes the query tolerance [28; 29]. An important class of statistical query learners is that of correlational/inner product statistical queries (CSQ) of the form \(q(x,y)=yh(x)\). This includes a wide class of algorithms including gradient descent with square loss and correlation loss. For example, if the model is \(f_{}(x)\), then the gradient with square loss can be written as

\[ L(w;x,y)=(f_{}(x)-_{}) f_{ }(x).\]

Note that the other term in the gradient only depends on the distribution of \(x N(0,I_{d})\) which we assume is known. However, this connection with gradient descent is only heuristic as the errors in GD are random while the errors in the SQ/CSQ framework are adversarial. However, SQ/CSQ lower bounds are frequently used to argue the existence of statistical-computational gaps in statistical learning problems [30; 31; 32; 33]. Theorem 2 measures the tolerance needed by CSQ learners to learn \(w^{}\).

**Theorem 2** (CSQ Lower Bound).: _Consider the function class \(_{}:=\{(w x):w^{d-1}\}\). Any CSQ algorithm using \(q\) queries requires a tolerance \(\) of at most_

\[()^{k^{}/4}\]

_to output an \(f_{}\) with population loss less than \(1/2\)._

Using the standard \( n^{-1/2}\) heuristic which comes from concentration, this implies that \(n d^{}{2}}\) samples are necessary to learn \((w x)\) unless the algorithm makes exponentially many queries. In the context of gradient descent, this is equivalent to either requiring exponentially many parameters or exponentially many steps of gradient descent.

## 5 Proof Sketch

In this section we highlight the key ideas of the proof of Theorem 1. The full proof is deferred to Appendix B. The proof sketch is broken into three parts. First, we conduct a general analysis on online SGD to show how the signal-to-noise ratio (SNR) affects the sample complexity. Next, we compute the SNR for the unsmoothed objective (\(=0\)) to heuristically rederive the \(d^{k^{}-1}\) sample complexity in Ben Arous et al. . Finally, we show how smoothing boosts the SNR and leads to an improved sample complexity of \(d^{k^{}/2}\) when \(=d^{1/4}\).

### Online SGD Analysis

To begin, we will analyze a single step of online SGD. We define \(_{t}:=w_{t} w^{}\) so that \(_{t}[-1,1]\) measures our current progress. Furthermore, let \(v_{t}:=- L_{_{t}}(w_{t};x_{t};y_{t})\). Recall that the online SGD update is:

\[w_{t+1}=+_{t}v_{t}}{\|w_{t}+_{t}v_{t}\|}_{t+ 1}=+_{t}(v_{t} w^{})}{\|w_{t}+_{t}v_{t}\|}.\]Using the fact that \(v_{t} w_{t}\) and \(}} 1-}{2}\) we can Taylor expand the update for \(_{t+1}\):

\[_{t+1}=+_{t}(v_{t} w^{})}{^ {2}\|v_{t}\|^{2}}}_{t}+_{t}(v_{t} w^{})-^{2}\|v_{t}\|^{2}_{t}}{2}+\]

As in Ben Arous et al. , we decompose this update into a drift term and a martingale term. Let \(_{t}=\{(x_{0},y_{0}),,(x_{t-1},y_{t-1})\}\) be the natural filtration. We focus on the drift term as the martingale term can be handled with standard concentration arguments. Taking expectations with respect to the fresh batch \((x_{t},y_{t})\) gives:

\[[_{t+1}|_{t}]_{t}+_{t}\,[v_{t} w^{}|_{t}]-_{t}^{2}\,[\|v_{t}\|^{2 }|_{t}]_{t}/2\]

so to guarantee a positive drift, we need to set \(_{t}[v_{t} w^{}|_{t}]}{[\|v_{t}\|^{2}|_{t}]_{t}}\) which gives us the value of \(_{t}\) used in Theorem 1 for \(t T_{1}\). However, to simplify the proof sketch we can assume knowledge of \([v_{t} w^{}|_{t}]\) and \([\|v_{t}\|^{2}|_{t}]\) and optimize over \(_{t}\) to get a maximum drift of

\[[_{t+1}|w_{t}]_{t}+} [v_{t} w^{}|_{t}]^{2}}{ [\|v_{t}\|^{2}|_{t}]}}_{}.\]

The numerator measures the correlation of the population gradient with \(w^{}\) while the denominator measures the norm of the noisy gradient. Their ratio thus has a natural interpretation as the signal-to-noise ratio (SNR). Note that the SNR is a local property, i.e. the SNR can vary for different \(w_{t}\). When the SNR can be written as a function of \(_{t}=w_{t} w^{}\), the SNR directly dictates the rate of optimization through the ODE approximation: \(^{}/\). As online SGD uses each sample exactly once, the sample complexity for online SGD can be approximated by the time it takes this ODE to reach \( 1\) from \(_{0} d^{-1/2}\). The remainder of the proof sketch will therefore focus on analyzing the SNR of the minibatch gradient \( L_{}(w;x;y)\).

### Computing the Rate with Zero Smoothing

When \(=0\), the signal and noise terms can easily be calculated. The key property we need is:

**Property 1** (Orthogonality Property).: _Let \(w,w^{} S^{d-1}\) and let \(=w w^{}\). Then:_

\[_{x N(0,I_{d})}[He_{j}(w x)He_{k}(w^{} x)]= _{jk}k!^{k}.\]

Using Property 1 and the Hermite expansion of \(\) (Definition 3) we can directly compute the population loss and gradient. Letting \(P_{w}^{}:=I-ww^{T}\) denote the projection onto the subspace orthogonal to \(w\) we have:

\[L(w)=_{k 0}^{2}}{k!}[1-^{k}]  L(w)=-(P_{w}^{}w^{})_{k 0}^{2}}{(k-1)!} ^{k-1}.\]

As \( 1\) throughout most of the trajectory, the gradient is dominated by the first nonzero Hermite coefficient so up to constants: \([v w^{}]=- L(w) w^{}^{k^{ }-1}\). Similarly, a standard concentration argument shows that because \(v\) is a random vector in \(d\) dimensions where each coordinate is \(O(1)\), \([\|v\|]^{2} d\). Therefore the SNR is equal to \(^{2(k^{}-1)}/d\) so with an optimal learning rate schedule,

\[[_{t+1}|_{t}]_{t}+_{t}^{2k^{ }-3}/d.\]

This can be approximated by the ODE \(^{}=^{2k^{}-3}/d\). Solving this ODE with the initial \(_{0} d^{-1/2}\) gives that the escape time is proportional to \(d_{0}^{-2(k^{}-1)}=d^{k^{}-1}\) which heuristically re-derives the result of Ben Arous et al. .

### How Smoothing boosts the SNR

Smoothing improves the sample complexity of online SGD by boosting the SNR of the stochastic gradient \( L(w;x;y)\).

Computing the SignalRecall that the population loss was approximately equal to \(1-^{2}}{k^{*}!}^{k^{*}}\) where \(k^{*}\) is the first nonzero Hermite coefficient of \(\). Isolating the dominant \(^{k^{*}}\) term and applying the smoothing operator \(_{}\), we get:

\[_{}(^{k^{*}})=_{z_{w}}[(  w^{})^{k^{*}}].\]

Because \(z w\) and \(\|z\|=1\) we have that \(\|w+ z\|=}\). Therefore,

\[_{}(^{k^{*}})^{-k^{*}}\,_{z _{w}}[(+(z w^{}))^{k^{*}}]= ^{-k^{*}}_{j=0}^{k}}{j}^{k^{*}-j}^{j} \,_{z_{w}}[(z w^{})^{j}].\]

Now because \(z-z\), the terms where \(j\) is odd disappear. Furthermore, for a random \(z\), \(|z w^{}|=(d^{-1/2})\). Therefore reindexing and ignoring all constants we have that

\[L_{}(w) 1-_{}(^{k^{*}}) 1- ^{-k^{*}}_{j=0}^{}{2}}^{k^{*}-2j} ^{2}/d^{j}.\]

Differentiating gives that

\[[v w^{}]-w^{}_{w}_{ }(^{k^{*}})^{-1}_{j=0}^{-1}{ 2}}()^{k^{*}-1}\!(}{^{2}d})^{j}.\]

As this is a geometric series, it is either dominated by the first or the last term depending on whether \( d^{-1/2}\) or \( d^{-1/2}\). Furthermore, the last term is either \(d^{--1}{2}}\) if \(k^{*}\) is odd or \(d^{--2}{2}}\) if \(k^{*}\) is even. Therefore the signal term is:

\[[v w^{}]^{-1}k^{*}-1}{}& d^{-1/2}\\ d^{--1}{2}}& d^{-1/2}k^{*}\\ d^{--2}{2}}& d^{-1/2}k^{*}.\]

Computing the NoiseRecall that \(L_{}(w;x,y)=1-y(w x)\). Differentiating through the smoothing operator gives:

\[_{w}L_{}(w;x,y)=-y_{w}\,_{}((w  x))^{-1}yx_{}(^{}(w x )).\]

We know that with high probability, \(y=(1)\) and \(\|x\|=O()\) so it suffices to bound \(_{}(^{}(w x))\). The variance of this term is equal to:

\[_{x}[_{}(^{}(w x))^{2}]=_{x}[_{z_{w}}[^{}(}} x)]^{2}].\]

To compute this expectation, we will create an i.i.d. copy \(z^{}\) of \(z\) and rewrite this expectation as:

\[_{x}[_{}(^{}(w x))^{2}]=_{x}[_{z,z^{}_{w}}[^{}(}} x)^{}(}{}} x)]].\]Now we can swap the expectations and compute the expectation with respect to \(x\) first using the Hermite expansion of \(\). As the first nonzero Hermite coefficient of \(^{}\) is \(k^{}-1\), this variance is approximately equal to the correlation between \(}}\) and \(}{}}\) raised to the \(k^{}-1\) power:

\[_{x}[_{}(^{}(w x))^{2}] _{z,z^{}_{w}}[(}}}{}})^{k^ {}-1}]=_{z,z^{}_{w}}[(z z^{}}{1+^{2}})^{k^{}-1}].\]

As \(z,z^{}\) are random unit vectors, their inner product is of order \(d^{-1/2}\). Therefore when \( d^{1/4}\), the first term in the numerator is dominant and when \( d^{1/4}\), the second term is dominant. Combining these two regimes gives that the variance is of order \((,d^{1/4})^{k^{}-1}\) which motivates our choice of \(=d^{1/4}\). Combining this with \(y=(1)\) and \(\|x\|=O()\) gives that for \( d^{1/4}\), the variance of the noise is bounded by \([\|v\|^{2}] d^{-2k^{}}\). Note that in the high signal regime (\( d^{-1/2}\)), both the signal and the noise are smaller by factors of \(^{k^{}}\) which cancel when computing the SNR. However, when \( d^{-1/2}\) the smoothing shrinks the noise faster than it shrinks the signal, resulting in an overall larger SNR. Explicitly,

\[:=[v w^{}]^{2}}{[\|v\|^{2}]} ^{2(k^{}-1)}& d^{- 1/2}\\ (^{2}/d)^{k^{}-1}& d^{-1/2}k^{}\\ ^{2}(^{2}/d)^{k^{}-2}& d^{-1/2}k^{}.\]

For \( d^{-1/2}\), smoothing does not affect the SNR. However, when \( d^{-1/2}\), smoothing greatly increases the SNR (see Figure 1).

Solving the ODE: \(^{}=/\) gives that it takes \(T d^{k^{}-1}^{-2k^{}+4}\) steps to converge to \( 1\) from \( d^{-1/2}\). Once \( 1\), the problem is locally strongly convex, so we can decay the learning rate and use classical analysis of strongly-convex functions to show that \( 1-\) with an additional \(d/\) steps, from which Theorem 1 follows.

## 6 Experiments

For \(k^{}=3,4,5\) and \(d=2^{8},,2^{13}\) we ran a minibatch variant of Algorithm 1 with batch size \(B\) when \((x)=}(x)}{}\), the normalized \(k^{}\)th Hermite polynomial. We set:

\[=d^{1/4},=/2}(1+^{2})^{k^{}-1} }{1000k^{}!}, B=0.1d^{k^{}/2}(1+^{2})^{-2k^{ }+4},8192.\]

We computed the number of samples required for Algorithm 1 to reach \(^{2}=0.5\) from \(=d^{-1/2}\) and we report the min, mean, and max over \(10\) random seeds. For each \(k\) we fit a power law of the form \(n=c_{1}d^{c_{2}}\) in order to measure how the sample complexity scales with \(d\). For all values of \(k^{}\), we find that \(c_{2} k^{}/2\) which matches Theorem 1. The results can be found in Figure 2 and additional experimental details can be found in Appendix E.

## 7 Discussion

### Tensor PCA

We next outline connections to the Tensor PCA problem. Introduced in , the goal of Tensor PCA is to recover the hidden direction \(w^{*}^{d-1}\) from the noisy \(k\)-tensor \(T_{n}(^{d})^{ k}\) given by2

\[T_{n}=(w^{*})^{ k}+}Z,\]

where \(Z(^{d})^{ k}\) is a Gaussian noise tensor with each entry drawn i.i.d from \((0,1)\).

The Tensor PCA problem has garnered significant interest as it exhibits a _statistical-computational gap_. \(w^{*}\) is information theoretically recoverable when \(n d\). However, the best polynomial-time algorithms require \(n d^{k/2}\); this lower bound has been shown to be tight for various notions of hardness such as CSQ or SoS lower bounds . Tensor PCA also exhibits a gap between spectral methods and iterative algorithms. Algorithms that work in the \(n d^{k/2}\) regime rely on unfolding or contracting the tensor \(X\), or on semidefinite programming relaxations . On the other hand, iterative algorithms including gradient descent, power method, and AMP require a much larger sample complexity of \(n d^{k-}\). The suboptimality of iterative algorithms is believed to be due to bad properties of the landscape of the Tensor PCA objective in the region around the initialization. Specifically  argue that there are exponentially many local minima near the equator in the \(n d^{k-1}\) regime. To overcome this, prior works have considered "smoothed" versions of gradient descent, and show that smoothing recovers the computationally optimal SNR in the \(k=3\) case  and heuristically for larger \(k\).

#### 7.1.1 The Partial Trace Algorithm

The smoothing algorithms above are inspired by the following _partial trace_ algorithm for Tensor PCA , which can be viewed as Algorithm 1 in the limit as \(\). Let \(T_{n}=(w^{})^{ k}+}Z\). Then we will consider iteratively contracting indices of \(T\) until all that remains is a vector (if \(k\) is odd) or a matrix (if \(k\) is even). Explicitly, we define the partial trace tensor by

\[M_{n}:=T_{n}I_{d}^{} ^{d d}&\\ ^{d}&\]

When \(k^{}\) is odd, we can directly return \(M_{n}\) as our estimate for \(w^{}\) and when \(k^{}\) is even we return the top eigenvector of \(M_{n}\). A standard concentration argument shows that this succeeds when \(n d^{ k/2}\). Furthermore, this can be strengthened to \(d^{k/2}\) by using the partial trace vector as a warm start for gradient descent or tensor power method when \(k\) is odd .

#### 7.1.2 The Connection Between Single Index Models and Tensor PCA

For both tensor PCA and learning single index models, gradient descent succeeds when the sample complexity is \(n=d^{k-1}\). On the other hand, the smoothing algorithms for Tensor PCA  succeed with the computationally optimal sample complexity of \(n=d^{k/2}\). Our Theorem 1 shows that this smoothing analysis can indeed be transferred to the single-index model setting.

In fact, one can make a direct connection between learning single-index models with Gaussian covariates and Tensor PCA. Consider learning a single-index model when \((x)=(x)}{}\), the normalized \(k\)th Hermite polynomial. Then minimizing the correlation loss is equivalent to maximizing

Figure 2: For \(k^{}=3,4,5\), Algorithm 1 finds \(w^{}\) with \(n d^{k^{}/2}\) samples. The solid lines and the shaded areas represent the mean and min/max values over 10 random seeds. For each curve, we also fit a power law \(n=c_{1}d^{c_{2}}\) represented by the dashed lines. The value of \(c_{2}\) is reported in the legend.

the loss function:

\[L_{n}(w)=_{i=1}^{n}y_{i}(w x_{i})}{}=  w^{ k},T_{n} T_{n}:= _{i=1}^{n}y_{i}_{k}(x_{i})}{}.\]

Here \(_{k}(x_{i})(^{d})^{ k}\) denotes the \(k\)th Hermite tensor (see Appendix A.2 for background on Hermite polynomials and Hermite tensors). In addition, by the orthogonality of the Hermite tensors, \(_{x,y}[T_{n}]=(w^{})^{ k}\) so we can decompose \(T_{n}=(w^{})^{ k}+Z_{n}\) where by standard concentration, each entry of \(Z_{n}\) is order \(n^{-1/2}\). We can therefore directly apply algorithms for Tensor PCA to this problem. We remark that this connection is a heuristic, as the structure of the noise in Tensor PCA and our single index model setting are different.

### Empirical Risk Minimization on the Smoothed Landscape

Our main sample complexity guarantee, Theorem 1, is based on a tight analysis of online SGD (Algorithm 1) in which each sample is used exactly once. One might expect that if the algorithm were allowed to reuse samples, as is standard practice in deep learning, that the algorithm could succeed with fewer samples. In particular, Abbe et al.  conjectured that gradient descent on the empirical loss \(L_{n}(w):=_{i=1}^{n}L(w;x_{i};y_{i})\) would succeed with \(n d^{k^{}/2}\) samples.

Our smoothing algorithm Algorithm 1 can be directly translated to the ERM setting to learn \(w^{}\) with \(n d^{k^{}/2}\) samples. We can then Taylor expand the smoothed loss in the large \(\) limit:

\[_{}(L_{n}(w))_{z(S^{d-1}) }[L_{n}(z)+^{-1}w L_{n}(z)+}{2} w ^{T}^{2}L_{n}(z)w]+O(^{-3}).\]

As \(\), gradient descent on this smoothed loss will converge to \(_{z(S^{d-1})}[ L_{n}(z)]\) which is equivalent to the partial trace estimator for \(k^{}\) odd (see Section 7.1). If \(k^{}\) is even, this first term is zero in expectation and gradient descent will converge to the top eigenvector of \(_{z(S^{d-1})}[^{2}L_{n}(z)]\), which corresponds to the partial trace estimator for \(k^{}\) even. Mirroring the calculation for the partial trace estimator, this succeeds with \(n d^{ k^{}/2}\) samples. When \(k^{}\) is odd, this can be further improved to \(d^{k^{}/2}\) by using this estimator as a warm start from which to run gradient descent with \(=0\) as in Anandkumar et al. , Biroli et al. .

### Connection to Minibatch SGD

A recent line of works has studied the implicit regularization effect of stochastic gradient descent . The key idea is that over short timescales, the iterates converge to a quasi-stationary distribution \(N(, S)\) where \(S I\) depends on the Hessian and the noise covariance at \(\) and \(\) is proportional to the ratio of the learning rate and batch size. As a result, over longer periods of time SGD follows the _smoothed gradient_ of the empirical loss:

\[_{n}(w)=_{z N(0,S)}[L_{n}(w+ z)].\]

We therefore conjecture that minibatch SGD is also able to achieve the optimal \(n d^{k^{}/2}\) sample complexity _without explicit smoothing_ if the learning rate and batch size are properly tuned.