# Conformalized Time Series with Semantic Features

Baiting Chen

Department of Statistics and Data Science

UCLA

brantchen@g.ucla.edu &Zhimei Ren

Department of Statistics and Data Science

University of Pennsylvania

zren@wharton.upenn.edu &Lu Cheng

Department of Computer Science

University of Illinois Chicago

lucheng@uic.edu

###### Abstract

Conformal prediction is a powerful tool for uncertainty quantification, but its application to time-series data is constrained by the violation of the exchangeability assumption. Current solutions for time-series prediction typically operate in the output space and rely on manually selected weights to address distribution drift, leading to overly conservative predictions. To enable dynamic weight learning in the semantically rich latent space, we introduce a novel approach called Conformalized Time Series with Semantic Features (CT-SSF). CT-SSF utilizes the inductive bias in deep representation learning to dynamically adjust weights, prioritiing semantic features relevant to the current prediction. Theoretically, we show that CT-SSF surpasses previous methods defined in the output space. Experiments on synthetic and benchmark datasets demonstrate that CT-SSF significantly outperforms existing state-of-the-art (SOTA) conformal prediction techniques in terms of prediction efficiency while maintaining a valid coverage guarantee.

## 1 Introduction

Uncertainty quantification is essential for reliable predictions in time series data [34; 45]. The emergence of 'black-box' models has intensified interest in conformal prediction (CP), a technique valued for its model-agnostic and distribution-free properties [44; 57]. Under the assumption of data exchangeability, the confidence bands provided by CP are theoretically guaranteed . This reliability has led to CP's promising performance across various domains, e.g., image classification , graph neural networks , anomaly detection , and natural language processing [37; 16].

The fundamental assumption of exchangeability in CP is often compromised in time series data due to inherent temporal dependencies, as highlighted in studies such as  and . This violation poses significant challenges in directly applying CP to time series forecasting. Existing adaptations of CP tailored for time series (e.g., ) tend to produce overly conservative prediction sets or intervals too wide to provide practical insight, thus limiting their practical utility. These adaptations often necessitate manually selected weights, which curtail their generalizability across various datasets. Moreover, the practice of calculating non-conformity scores based on output space does not take full advantage of the rich representational data available in the latent space. This oversight is particularly critical given the prevalent use of neural networks (NN) in contemporary time series forecasting, which are capable of capturing deep, complex patterns in data as discussed in  and .

The latent space in NN offers enhanced model interpretability and the ability to capture complex, nonlinear relationships in the data that are often invisible in the original input space (). By mapping input data into a latent feature space through the feature function, the resulting representation not only supports the model in learning essential temporal patterns but also in generalizing better to unseen data by focusing on the most informative features. To overcome the inherent challenges in applying CP to time series data, our work presents a novel approach--_Conformalized Time Series with Semantic Features (CT-SSF)_--which leverages latent semantic features of NN for time series. Central to our methodology is the use of weighted errors for constructing non-conformity scores in the latent space, drawing upon rich semantic features. The weights are dynamically adjusted to prioritize semantic features more pertinent to the current prediction task. This approach ensures that the weights adapt to the changing importance of different data aspects over time. By shifting CP into the latent space of NN, our non-conformity scores achieve a deeper insight into the temporal dynamics. This not only maintains the requisite coverage of predictive intervals but also significantly improves their prediction efficiency, offering a refined tool for time series forecasting that aligns with the complexities of modern data structures.

Our contributions are summarized as follows:

* We propose CP in the latent space of the underlying time-series prediction model, enabling the construction of non-conformity scores that encapsulate a more comprehensive understanding of the data and deliver tight prediction sets.
* We propose an adaptive weight adjustment scheme to refine the weight learning process in the latent space, accounting for distribution shifts in time-series data by emphasizing data points more relevant to the current time step.
* We conduct extensive experiments under both synthetic and realistic settings to corroborate the effectiveness of the proposed algorithm, showing a 10%-20% improvement over the SOTA approach for real-world data.

## 2 Related Work

**CP with exchangeable data** CP, pioneered by , has become a cornerstone in uncertainty quantification due to its model-free and distribution-free property . The work in this domain can be summarized of CP into two branches: improve the efficiency of CP ([33; 13; 51; 36; 6; 27] ) and generalize CP to different settings, such as quantile regression , decision tree , random forest ), survival analysis , k-nearest neighbor , online learning  and auction design . Comprehensive insights into CP and its theoretical underpinnings are provided by .

**Beyond Exchangeability** The foundational works of  and the introductory reviews by  describe the evolution of CP to accommodate contexts "beyond exchangeability". Notable extensions include  application to covariate shift, and innovative adaptations for dealing with label shift by . The work of  and  address calibration and test set shifts by reweighting data points.  show how reweighting can be extended to causal inference setups for predictive inference on individual treatment effects, and  show how to apply these ideas in the context of censored outcomes in survival analysis.

**CP for time series data** CP for time series can be divided into two primary trends that deviate from the traditional assumption of exchangeability. The first trend focuses on adaptively adjusting the error rate \(\) during the testing phase to enhance coverage accuracy. This approach is demonstrated in works such as ([2; 3; 8; 14; 53; 26; 46; 52; 15; 38; 58]). The second trend (e.g., [41; 48; 49; 7; 23; 24; 47]) emphasizes manually assigning weights to historical non-conformity scores, giving greater importance to those that reflect the current scenario more accurately. To address the limitation of manual selection, the advanced HopCPT model, developed by , leverages the Modern Hopfield Network to capture temporal structure and learn the weights. The work closest to ours is the HopCPT (). We advance HopCPT in developing the non-conformity score in the latent space with rich semantic features, enabling a deeper analysis and utilization of the inherent temporal patterns in the data. Alternative probabilistic frameworks like Mixture Density Networks  and Gaussian Processes  offer robust methodologies for time series forecasting beyond CP. However, these methods frequently grapple with computational limitations and lack robust theoretical backing.

## 3 Background

**Split Conformal Prediction** One of the most common approaches in CP is the _split conformal prediction_ method , which starts with a trained model and assesses its efficacy on a calibration set of paired examples \(\{(X_{i},Y_{i})\}_{i=1}^{n}\). Central to split CP is the foundational assumption that the data are exchangeable, ensuring that the errors observed in the test set will be consistent with those from the calibration set. This allows for reliable empirical estimation of quantiles across both datasets. Define a non-conformity score function \(s(x,y)\) that quantifies the disagreement between predicted and ground-truth values. The coverage guarantee of CP is defined as follows:

\[P(Y_{n+1} C(X_{n+1})) 1-, C(X_{n+1})=\{y:s(X_{n+1},y) \}.\] (1)

where \((X_{n+1},Y_{n+1})\) is a new data point, \(\) is the \((1-)(n+1)\)-th smallest elements among \(\{s(X_{i},Y_{i})\}_{i=1}^{n}\) and \(\) signifies a predefined miscoverage rate.

**Non-exchangeable Conformal Prediction** Real-world settings encounter challenges such as data drift and inter-dependencies, necessitating further adaptations of CP to these non-exchangeable settings. One common solution is to use reweighting [7; 5]. For example, given a set of pre-specified weights \(\{w_{i}\}_{i=1}^{n}\), \(w_{i}\), the coverage guarantee for seminal work NexCP  is articulated as follows:

\[P(Y_{n+1} C(X_{n+1})) 1--_{i=1}^{n}_{i}d_{}(Z,Z^{i}),_{i}=}{1+_{i=1}^{N}w_{i}},\] (2)

where \(Z=(X_{1},Y_{1}),,(X_{n},Y_{n}),(X_{n+1},Y_{n+1})\) represents a sequence of \(n\) calibration examples along with a subsequent test example and \(_{i}\) is the normalized weight. The term \(Z^{i}\) denotes the sequence \(Z\) after the \(i\)-th pair \((X_{i},Y_{i})\) is swapped with the test example \((X_{n+1},Y_{n+1})\), and \(d_{}(Z,Z^{i})\) quantifies the dissimilarity introduced by this swap.

To construct prediction sets, NexCP determines the quantile threshold \(\) by the following equation:

\[=\{q:_{i=1}^{n}_{i}1\{s_{i} q\} 1- \}.\] (3)

Eq. 3 is consistent with the standard CP when all weights \(\{_{i}\}_{i=1}^{n}=1\). Intuitively, closer alignment to exchangeable data minimizes the calibration terms \(d_{}(Z,Z^{i})\), allowing for more precise calibration. Strategic weight allocation--such as assigning larger weights to calibration points \((x_{i},y_{i})\) that have similar distributions in \(Z\) and \(Z^{i}\) and smaller weights where distributions diverge--can yield tighter bounds on the prediction sets. For time series data, this suggests that greater weights should be assigned to more recent observations to reflect their increased relevance.

## 4 Methodology

In this section, we first formally define CP for time-series prediction and then propose our framework that leverages rich semantic features in latent space modeling and employs a dynamic weighting mechanism that adjusts to temporal dependencies based on these semantic features, simultaneously achieving valid coverage and high efficiency.

### Problem Definition

Given a multivariate time series \(_{t}=(_{t},y_{t})\) for \(t=1,,T\), where each \(_{t}^{m}\) is a feature vector with \(m\) dimensions and \(y_{t}\) is the corresponding real-valued target. The prediction model \(\) utilizes these feature vectors to produce point predictions \(_{t}=(_{t})\). The goal of CP is to construct a prediction interval \(C^{t}_{}(_{t+1})\) that contain the true value \(y_{t+1}\) with a confidence level of \(1-\):

\[P(Y_{t+1} C^{t}_{}(_{t+1})) 1-,\] (4)

where the probability is over the randomness of \(\{_{s}\}_{s t}\) and \(_{t+1}\). This interval aims to balance reliability with informativeness--beyond ensuring the coverage, it is ideal that the prediction interval is of short length.

A common choice of non-conformity score is the absolute errors \(|y_{t}-(_{t})|\) between observed values and the model's predictions [43; 32]. The prediction interval is then calculated based on the empirical \(1-\) quantile \(Q_{1-}\) of the non-conformity scores:

\[C^{t}_{}(_{t+1})=(_{t+1}) Q_{1-} \{|y_{i}-(_{i})|\}_{i=1}^{t}\{\}.\] (5)

### Non-conformity Score in the Latent Space

Given the rich semantic information in the latent space, we investigate the effectiveness of using semantic features for CP with time series data. Our base model architecture \(\) is a deep neural network (e.g., RNN) which is structured around two key sub-neural networks comprised of linear layers \(=g f\): The feature function \(f\) maps input data into a latent feature space, while the prediction head \(g\) transforms these features into the forecasted outputs.

In typical supervised learning for time series, ground truth labels are available only in the output space. The latent space, by contrast, is designed to capture abstract, non-obvious patterns in the data. These features help the model's learning but do not correspond directly to observable labels (). To address the lack of ground-truth labels in the latent space, we apply the surrogate feature  to replace the ground-truth term when constructing the non-conformity score. Concretely, we define the nonconformity score to be

\[s(X,Y,)=_{v V:(v)=Y}\|v-(X)\|,\] (6)

where \(\) and \(\) are approximate \(f\) and \(g\). This score measures the minimal discrepancy between any latent representation \(v\) that could correctly predict \(Y\) when processed by \(\) and the actual latent representation \((X)\) produced by the model from input \(X\).

It is usually complicated to calculate the score in Equation 6 due to the infimum operator. One solution is to directly apply the gradient descent mechanism in : \(u u-((u)-Y)^{2}\), however, this approach overlooks the characteristics of time series data and the adaptability of the update process. Therefore, we propose a weighted gradient descent mechanism

\[u u-((u)-Y)^{2},\] (7)

where \(\) represents a vector of weights specifically designed to enhance the adaptability of the update process. These weights adjust the influence of each component of the gradient, enabling the latent vector \(u\) to more effectively minimize the squared error between the predicted \((u)\) and actual targets \(Y\). With learning rate \(\), these weighted adjustments prevent inefficient updates, fostering better convergence and ensuring well-defined non-conformity scores in the semantic feature space.

### Adaptive Weight Adjustment

To ensure precise calibration of our model for time series data, it is imperative to meticulously define the weight terms \(\), thereby enabling the algorithm to concentrate on minimizing errors where they are most critical. We propose an adaptive weight adjustment scheme in the latent space, fundamentally inspired by the intuition that weights should be dynamically adjusted to prioritize semantic features more pertinent to the current prediction task. To effectively quantify this pertinence, we employ the principle of proximity in error terms, where smaller discrepancies between predicted and ground-truth values suggest higher relevance . For time series data, the underlying processes often exhibit consistent behaviors or trends. Similar errors reflect this consistency, making them reliable indicators for future occurrences. As a result, we expect that similar errors should work best to predict the current error. Note that these error terms are measured in the semantic feature space, allowing us to leverage the rich representations of the base model. Our strategy to handle non-exchangeable conditions in time series is to replace the quantile of in standard CP with a weighted quantile:

\[Q_{1-}(_{i=1}^{t}_{i}_{s_{i}}+_{t+ 1}_{}).\] (8)

Here, \(_{i}\) are weights assigned based on the similarity of their error terms to the current prediction error, \(s_{i}\) are the non-conformity score calculated in the latent space, \(t\) represents the size of calibration data, and \(_{a}\) denotes a point mass at \(a\) which represents a discrete distribution:

\[P(X=x)=1&x=,\\ 0&x.\] (9)

In Eq. 8, we assign larger weights to more relevant data points which can significantly enhance the lower bound of empirical coverage (as shown in Equation 2). This insight guides our adaptation of the attention mechanism to assess and quantify the relevance of training data points to agiven test instance. The attention mechanism is designed to calculate weights based on the similarity of error terms, which are then used to adjust the influence of each data point in the final prediction model (). The attention weights here are: \(((u),Y)\), and AttentionWeights\(((u),Y)\) represents the attention weights calculated by the Transformer model, reflecting the similarity between the predicted output \((u)\) and \(Y\). Accordingly, the attention mechanism in our approach actively assigns higher weights to these similar errors, thereby optimizing the inference coverage. The adaptive weight adjustment bolsters our method's capacity to consistently meet the significant level, providing a robust guarantee for its performance.

### Weighted Conformal Prediction with Semantic Features

Since the non-conformity score is initially constructed in the semantic space, it is necessary to translate the prediction intervals to the output space for practical application. Our method incorporates band estimation techniques  to estimate confidence or prediction intervals around the forecasts. This approach allows for uniform uncertainty levels in the latent space, while effectively reflecting varied levels of uncertainty in the output space based on the non-linear transformation. In this work, we apply linear relaxation-based perturbation analysis (LiPRA) () to tackle this problem under deep neural network regimes. LiPRA transforms the certification problem as a linear programming problem, and solves it accordingly. Following , we model the Band Estimation problem as a perturbation analysis one, where we regard the surrogate feature as a perturbation of the trained feature and analyze the output bounds of the prediction head. Since LiPRA results in a relatively looser interval than the actual band, this method would give an upper bound estimation of the exact band length.

We summarize our framework in Algorithm 1. First, it calculates non-conformity scores within the latent space by splitting the base model into a feature function and a prediction head in step 3. By doing so, our method can utilize the latent features to generate precise and efficient prediction sets that are tailored to the specific dynamics of the data. During the calibration phase, the algorithm integrates the attention mechanism in steps 7-10 to refine the learning process of weights. This integration is crucial for dynamically prioritizing and adjusting the influence of specific features based on their relevance to the prediction task. Lastly, step 14 applies a structured process to achieve comprehensive coverage and maintain competitive intervals in our prediction sets. As a result, our method can dynamically adjust weights in response to changes in data patterns.

```
0: Dataset \(\{(X_{t},Y_{t})\}_{t=1}^{T}\), test feature \(\{X_{i}\}_{i I_{te}}\), \(\), \(M\), \(\), \(\);
1: Randomly split the dataset \(D\) into training \(D_{tr}=\{(X_{i},Y_{i})\}_{i I_{tr}}\) and calibration \(D_{ca}=\{(X_{i},Y_{i})\}_{i I_{te}}\);
2: Training with \(D_{tr}\)
3: Train a base deep NN model for time series \(=()\) using the training fold \(D_{tr}\);
4: Adaptive weight adjustment
5:\(u\{(X_{i})\}_{i I_{ca}U_{te}}\); \(m 0\); \(n|I_{ca}|+|I_{te}|\); \([1/n,,1/n]\)
6:while\(m<M\)do
7: predict \((u)-\{Y_{i}\}_{i I_{te}}\)
8: update \(\)
9:\(u u-(\|(u)-\{Y_{i}\}_{i I_{ca}U_{te }}\|^{2})\)
10:\(m m+1\)
11:endwhile
12:\(s(X,Y,)=\|u-(X)\|\).
13:for\(i I_{te}\)do
14: Calibration with \(D_{ca}\)
15: Calculate the \((1-)\)-th quantile \(Q_{1-}\) of the distribution \(|+1}(_{i=1}_{i}_{s_{i}}+ _{n+1}_{})\).
16: Prediction
17: Apply LiPRA on \((X_{i})\) with perturbation \(Q_{1-}\) and prediction head \(\), which returns \(C_{1-}(X_{i})\);
18:\(D_{ca} X_{i}\)
19:endfor
20:\(C_{1-}(X_{i})\) for each test input. ```

**Algorithm 1** Conformalized Time Series with Semantic Features

### Theoretical Guarantee

This section outlines the theoretical guarantees for CT-SSF with respect to coverage and prediction interval length. Based on Theorem 4 in , we demonstrate that constructing non-conformity scores in the latent space allows CT-SSF to surpass CP for time series in the output space (e.g., NexCP) in terms of average prediction interval length under the following mild assumption:

**Assumption 1**.: _In the output space, we define \(H(v,X)\) as the length associated with sample \(X\), derived from the length \(v\) in the feature space. This is represented as \(H(v,X)=\{g(u):\|u-(X)\|\}\). We assume a Holder condition for \(H\), stipulating that for any \(X\), the inequality \(|H(v,X)-H(u,X)| L|v-u|^{}\) holds, where \(>0\) and \(L>0\) are constants._

The Holder condition for \(H(u,X)\) ensures that small changes in the input lengths \(u\) or \(v\) result in small and predictable changes in the length \(H\). This is characterized by the constants \(\) and \(L\), which control the sensitivity of \(H\) (). In the context of CT-SSF, this condition helps maintain consistent lengths when transforming from the feature space to the output space (length preservation), ensures that differences between individual lengths and their quantiles are amplified (expansion), and guarantees that the prediction intervals remain stable and reliable across both spaces for a given calibration set (quantile stability). Below is an informal statement of the theoretical guarantee of CT-SSF. The complete version of the theorem is in Appendix C.1.

**Theorem 1**.: _Under Assumption 1, if the following cubic conditions hold:_

1. _Length Preservation__._ _CT-SSF method does not cost much loss in feature space._
2. _Expansion__._ _The Band Estimation operator expands the differences between individual length and their quantiles._
3. _Quantile Stability__._ _The band length is stable in both the feature space and the output space for a given calibration set._

_Let \(_{i}\) denote the weights learned from Algorithm 1. Suppose that \(_{i}\)'s are independent of the calibration and test nonconformity scores. Then CT-SSF outperforms CP approaches for time series in the output space in terms of average band length while maintaining a valid coverage guarantee._

Here, length preservation ensures that the transformation from feature space to output space maintains consistent interval lengths, minimizing efficiency loss. Expansion plays a key role in reducing inefficiency, as the latent space exhibits smaller distances between individual non-conformity scores and their quantiles, reducing the computational cost of the quantile operation. Lastly, quantile stability ensures that the interval is generalizable from the calibration set to the test samples. Since these conditions primarily emphasize the properties of quantiles and transformation steps rather than the reweighting, extending this framework to incorporate a weighted setting is a logical and justified progression. Note that in our theoretical results (for both efficiency and coverage) essentially assumes the weights to be fixed, where in implementation, the weights are learned from the data. It remains an interesting question to establish a condescending theory for data-drive weights. The detailed proofs can be found in Appendix C.1.

## 5 Experiments

### Experimental Setup

**Prediction Models for Time Series**. CT-SSF is model agnostic, therefore, any NN-based prediction models for time series.prediction models can be used as the base models. To better show the advantage of our proposed method, we utilize a Recurrent Neural Network (RNN) model, which can be replaced with more advanced models like Transformers ().

**Datasets.** Our experiments encompass evaluations on both synthetic and four real-world benchmark datasets, allowing for assessment under controlled and natural conditions. They are electricity, stock of Amazon, weather, and wind data. The details of these datasets can be found in Appendix B.

**Calibration Details.** During calibration, to get the best value for the number of steps \(M\), we take a subset (e.g., one-fifth) of the calibration set as the additional validation set. We calculate the non-conformity score on the rest of the calibration set with various values of step \(M\) and then evaluatethe validation set to get the best \(M\) whose coverage is right above \(1-\). The final trained surrogate feature \(v\) is close to the true feature because \((v)\) is sufficiently close to the ground truth \(Y\). In practice, the surrogate feature after optimization satisfies

\[(v)-Y\|^{2}}{\|Y\|^{2}}<1\%.\] (10)

Eq. 10 indicates that the normalized error of predictions with the surrogate feature is less than 1%. Therefore, the surrogate feature approximates ground truth feature in the latent space.

**Compared Approaches.** Baselines include state-of-the-art (SOTA) methods for CP under distribution drift and SOTA CP for time series: (1) **Standard Split CP** (). This is the standard split CP method outlined in Section 3; (2) **NexCP** (). A refined CP method for handling distribution drifts using manually selected weights. Details are outlined in Section 3; (3) **FeatureCP (FCP)** (). Implementing CP within a latent feature space under the assumption of exchangeability; (4) **HopCPT** (). The current state-of-the-art CP for time series. HopCPT leverages Modern Hopfield Networks to build prediction intervals by identifying and utilizing historical events with similar error distributions.

**Metrics.** Our analysis employs two widely used metrics to assess the effectiveness of each CP method (): _Empirical Coverage Rate (Coverage):_ Measures the effectiveness of CP in achieving the theoretically guaranteed coverage. _Average Prediction Set Size (Width):_ Evaluates the efficiency of CP, reflecting the compactness of the prediction intervals.

All the standard deviations are obtained over five repeated runs with different random seeds under the same base model. Code and data are available at https://github.com/baiting0522/CT-SSF.

### Simulations

We adapt the simulation method in  and generate time series datasets that incorporate manually designed temporal dependency and heteroskedasticity. We generate \(n=1,000\) data points \((X_{i},Y_{i})^{M}^{N}\) by sampling \(X_{i}\) from a Gaussian distribution, \(X_{i}(0,I_{M})\). We set \(Y_{i} AX_{i}+BX_{i-1}+CX_{i-2}+_{i}\). The coefficient matrix \(A,B,C\) are set to the identity matrix \(I_{M}\) and \(_{i}(0,)\) where \(\) is a diagonal matrix and its elements are given by \(0.5+0.1i\).

Table 1 presents the performance of the compared CP algorithms across three different levels of miscoverage error (\(\)): 0.05, 0.1, and 0.15. We observe from the results that CT-SSF consistently outperforms other baselines across all tested scenarios. Specifically, at a miscoverage level of 0.05, CT-SSF shows a 4.61% reduction in interval width compared to the SOTA approach, HopCPT. The smallest width indicates that our method can produce the most informative prediction intervals while maintaining valid empirical coverage. This efficiency is attributed to the innovative construction of the non-conformity score within the latent space with adaptive weight adjustment, which enhances the ability to leverage the underlying information of the base model. Meanwhile, the superior performance of FCP compared to CP and NexCP aligns with the experimental results reported in . However, since FCP operates under the assumption of exchangeability, it may occasionally result in under-coverage, e.g., \(=0.05\) and \(=0.10\). On the other hand, NexCP tends to produce overly conservative results, though it maintains robust coverage. This underscores the need for our adaptive weight adjustments with semantic features to optimize performance.

**Ablation Study.** To further investigate the effectiveness of using semantic features and adaptive weight adjustment in CT-SSF, we compare it with its three variants that use: (1) **Manually selected

   \\ Methods &  &  &  \\  & Cov & Width & Cov & Width & Cov & Width \\  CP & 94.80\(\)1.83 & 10.39\(\)0.49 & 90.90\(\)1.50 & 9.30\(\)0.11 & 84.90\(\)2.83 & 8.24\(\)0.40 \\ FCP & 94.20\(\)0.68 & 1.77\(\)0.27 & 89.70\(\)2.41 & 1.72\(\)0.21 & 85.10\(\)1.32 & 1.40\(\)0.21 \\ NexCP & 95.30\(\)1.57 & 9.60\(\)0.44 & 91.50\(\)0.63 & 9.10\(\)0.40 & 85.50\(\)2.90 & 7.66\(\)0.39 \\ HopCPT & 95.10\(\)3.27 & 1.54\(\)0.87 & 91.28\(\)3.40 & 1.43\(\)0.78 & 85.10\(\)4.04 & 1.10\(\)0.72 \\ CT-SSF & 96.50\(\)0.84 & **1.45\(\)**0.23 & 90.70\(\)1.57 & **1.23\(\)**0.15 & 85.70\(\)2.14 & **1.04\(\)**0.34 \\  

Table 1: Performance of the evaluated CP algorithms for the simulations dataset.

weights in the Semantic space (CT-MS), (2) Manually selected weights in the Output space (CT-MO). This variant reduces to the NexCP method and (3) Learned weights in the Output space (CT-LO). The weights are learned using the same learning scheme in CT-SSF.

Figure 1 demonstrates the effectiveness of the CT-SSF method compared to its variants across varied miscoverage risks from 0.05 to 0.3. CT-SSF consistently outperforms the other methods in terms of prediction interval width, indicating its superior efficiency in generating compact prediction sets while maintaining robust empirical coverage. Relative to CT-MO and CT-LO, our method achieves more than a 50% reduction in the width of the prediction intervals, a benefit derived from exploiting the rich latent information in the feature space. Additionally, in comparison with CT-MS, our method shows an approximate 10% reduction in interval width. These results demonstrate the significant impact of adaptive weight adjustments on the efficiency of prediction intervals, further establishing CT-SSF as a highly effective CP method for time series forecasting.

CT-SSF's performance can be attributed to its specialized approach to learning weights in the latent space. Unlike methods such as NexCP and CT-MS, which apply predefined weight schemes, CT-SSF's dynamic adaptation to the underlying data structure allows for more precise adaptation of the model to the specific data characteristics. Unlike HopCPT, which learns the weights but constructs non-conformity scores in the output space, CT-SSF builds these scores directly within the latent space to utilize underlying latent features effectively and better capture the data variability and structural dependencies, resulting in more reliable and informative prediction intervals.

### Real-world Data

Real-world experiments are conducted using benchmark time series data including _electricity_ (), _stock market_ ([14; 2]), _weather_ ([28; 54]), and _wind speed forecasting_ ().

    & & CT-SSF & HopCPT & NexCP & FCP & CP \\   & Cov & 90.59\(\)1.74 & 90.88\(\)2.39 & 91.67\(\)0.79 & 91.26\(\)0.99 & 91.26\(\)1.15 \\  & Width & **0.21\(\)**0.03 & 0.23\(\)\(0.05\) & 0.74\(\)0.17 & 0.28\(\)0.05 & 0.83\(\)0.01 \\   & Cov & 91.58\(\)2.23 & 90.79\(\)3.06 & 91.03\(\)2.37 & 91.58\(\)2.07 & 91.74\(\)1.98 \\  & Width & **0.19\(\)**0.03 & 0.23\(\)0.04 & 1.43\(\)0.12 & 0.29\(\)0.10 & 1.51\(\)0.17 \\   & Cov & 90.12\(\)0.26 & 90.11\(\)0.68 & 90.14\(\)0.26 & 90.06\(\)0.22 & 90.13\(\)0.26 \\  & Width & **0.012\(\)**0.002 & 0.017\(\)0.004 & 0.034\(\)0.006 & 0.023\(\)0.004 & 0.034\(\)0.007 \\   & Cov & 90.32\(\)1.51 & 90.06\(\)2.49 & 90.06\(\)2.08 & 90.19\(\)1.75 & 89.54\(\)1.75 \\  & width & **0.52\(\)**0.03 & 0.54\(\)0.04 & 2.76\(\)0.11 & 0.59\(\)0.13 & 2.65\(\)0.14 \\  

Table 2: Performance of the evaluated CP algorithms for the real data. The specified miscoverage level is \(=0.1\) for all experiments. Results for different \(\) can be found in Appendix B.

Figure 1: Comparisons of the variants of CT-SSF. The blue bar chart represents Width and the gray bar represents Coverage, the blue line represents target coverage.

Results in Table 2 reveal that CT-SSF maintains valid coverage rates while excelling in minimizing prediction interval widths. Across all datasets, CT-SSF shows a 10%-20% reduction in prediction intervals compared to the SOTA HopCPT. While NexCP is designed for non-exchangeable settings, the need for manually selected weights limits its practical application. For instance, in the _wind_ dataset, improperly chosen weights result in larger prediction intervals compared to standard CP, despite meeting theoretical coverage guarantees. We further conduct an ablation study to compare the variants of CT-SSF, similar to that for synthetic data. Results in Appendix B show that CT-SSF outperforms other variants. This demonstrates the effectiveness of utilizing semantic features and implementing adaptive weight adjustments. While our experiments are based on RNN, we show in Appendix B that using a different base model can lead to similar findings.

**The impact of choosing \(f\) and \(g\).** Finally, we analyze the potential impact of the selection of the semantic feature space on the performance of CT-SSF. Our base model is an 8-layer RNN, and we conduct experiments on semantic spaces ranging from the 2nd to the 6th layer, i.e., using the first 2-6 layers as \(f\) and the rest as \(g\). Results in Table 3 indicate that the performance of CT-SSF is influenced by the configuration of \(f\) and \(g\), as evidenced by the variation in the length of prediction intervals. A key observation is that optimal performance typically arises from the middle layers (specifically, the 3rd and 4th layers). This suggests that \(f\) with too few layers may not capture enough semantic information, and \(g\) with a limited number of layers lacks adequate predictive strength. Additionally, the coverage and interval length results are similar for the 5th and 6th layers across all four datasets. This similarity may be due to the comparable level of information interpreted in these feature spaces. Given all these observations, it is important to perform cross-validation to identify the best semantic space for the highest efficiency when deployed in various applications.

## 6 Conclusions and Limitations

In this work, we introduce CT-SSF, a novel approach designed to overcome the challenges of applying CP directly to time-series data. Our method shifts the calculation of non-conformity scores to the semantic feature space, effectively capturing the complex, nonlinear relationships in time-series data that are often overlooked in the output space. By employing a reweighting scheme and dynamically adjusting weights based on the importance of various semantic features, CT-SSF achieves valid coverage with high efficiency. We provide theoretical analyses demonstrating that CT-SSF outperforms CP methods that operate in the output space. Experimental results show that CT-SSF significantly outperforms existing SOTA CP methods for time-series data.

Our approach to CP for time series forecasting presents certain limitations that warrant further exploration and mitigation. Firstly, its reliance on NNs excludes simpler models like ridge regression or random forests. This limitation, while justified by the prevalent use of NN in time series forecasting, narrows the scope of our methodology's wide applicability. Secondly, the robustness of our prediction set lengths is sensitive to the configuration of the latent space to construct \(f\) and \(g\). Our current solution uses a validation set to help separate \(f\) and \(g\), rendering less data for calibration. Future enhancements could include the development of adaptive methods that dynamically refine the latent space to enhance the stability of the predictions across diverse data scenarios, thereby addressing the current limitations and expanding the utility of our approach.

   & & 2nd & 3rd & 4th & 5th & 6th \\   & Cov & 89.75\(\)1.96 & 90.94\(\)1.02 & 90.59\(\)1.74 & 90.22\(\)2.02 & 90.21\(\)1.92 \\  & Width & 0.23\(\)0.09 & 0.29\(\)0.02 & **0.21\(\)**0.03 & 0.28\(\)0.02 & 0.30\(\)0.02 \\   & Cov & 93.50\(\)1.52 & 88.65\(\)2.45 & 91.58\(\)2.23 & 88.50\(\)2.42 & 88.41\(\)2.60 \\  & Width & 0.39\(\)0.12 & 0.26\(\)0.09 & **0.19\(\)**0.03 & 0.21\(\)0.07 & 0.21\(\)0.07 \\   & Cov & 90.12\(\)0.31 & 90.07\(\)0.34 & 90.12\(\)0.26 & 90.06\(\)0.56 & 90.06\(\)0.54 \\  & Width & 0.018\(\)0.004 & 0.034\(\)0.010 & **0.012\(\)**0.002 & 0.014\(\)0.002 & 0.014\(\)0.002 \\   & Cov & 89.67\(\)2.42 & 90.32\(\)1.51 & 89.28\(\)1.88 & 88.63\(\)1.90 & 88.90\(\)1.80 \\  & Width & 1.00\(\)0.023 & **0.52\(\)**0.03 & 0.54\(\)0.12 & 0.59\(\)0.03 & 0.65\(\)0.02 \\  

Table 3: Performance of the CT-SSF with different \(f\) and \(g\) configurations. The specified miscoverage level is \(=0.1\) for all experiments.