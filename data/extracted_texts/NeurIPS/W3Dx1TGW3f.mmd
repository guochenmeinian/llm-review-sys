# Contextual Bilevel Reinforcement Learning for

Incentive Alignment

Vinzenz Thoma

ETH AI Center

vinzenz.thoma@ai.ethz.ch

&Barna Pasztor

ETH AI Center

barna.pasztor@ai.ethz.ch &Andreas Krause

ETH Zurich

krausea@ethz.ch &Giorgia Ramponi

University of Zurich

giorgia.ramponi@uzh.ch &Yifan Hu

EPFL & ETH Zurich

yifan.hu@epfl.ch

Equal Contribution

###### Abstract

The optimal policy in various real-world strategic decision-making problems depends both on the environmental configuration and exogenous events. For these settings, we introduce _Contextual Bilevel Reinforcement Learning_ (CB-RL), a stochastic bilevel decision-making model, where the lower level consists of solving a contextual Markov Decision Process (CMDP). CB-RL can be viewed as a Stackelberg Game where the leader and a random context beyond the leader's control together decide the setup of many MDPs that potentially multiple followers best respond to. This framework extends beyond traditional bilevel optimization and finds relevance in diverse fields such as RLHF, tax design, reward shaping, contract theory and mechanism design. We propose a stochastic _Hyper Policy Gradient Descent (HPGD)_ algorithm to solve CB-RL, and demonstrate its convergence. Notably, HPGD uses stochastic hypergradient estimates, based on observations of the followers trajectories. Therefore, it allows followers to use any training procedure and the leader to be agnostic of the specific algorithm, which aligns with various real-world scenarios. We further consider the setting when the leader can influence the training of followers and propose an accelerated algorithm. We empirically demonstrate the performance of our algorithm for reward shaping and tax design.

## 1 Introduction

In Reinforcement Learning (RL), Markov Decision Processes (MDPs)  provide a versatile framework for capturing sequential decision-making problems across various domains such as health care , energy systems , economics , and finance . A considerable amount of work has been devoted to solving standard MDPs . However, in many applications, MDPs can be configured on purpose or affected by exogenous events, both of which can significantly impact the corresponding optimal policies. For example, in a simplified economic framework, the optimal decision of an individual household depends both on public policies and economic uncertainties . The policy maker in turn has to make decisions, anticipating the best response of differently-minded individual agents to its policies and exogenous events outside the policy maker's control.

To study such problems, we introduce Contextual Bilevel Reinforcement Learning (CB-RL), a hierarchical decision-making framework where followers solve a contextual Markov decisionprocesses (CMDP) , configured by the leader. CB-RL is formalized as:

\[_{x}& F(x):=_{}[f(x, ^{*}_{x,},)]\\ &^{*}_{x,}=*{argmax}_ {}J_{,x,}(),\] (1)

where \(x\) represents the model configuration of the CMDP chosen by the leader, \(\) represents the context that followers encounter, and the function \(J_{,x,}\) denotes the (entropy-regularized) reward of the CMDP for a policy \(\), a given model parameters \(x\), a context \(\), and a regularization parameter \(\).

In our framework, the leader chooses \(x\) to configure various aspects of the CMDP, such as state transitions, the initial state distribution, and the followers' reward functions. Modeling the problem as a CMDP instead of a standard MDP is essential when modeling situations where the environment is influenced by side information or personal preferences. For example, the context \(\) can capture a wide range of real-world scenarios such as: (1) there is one follower, who responds optimally not only to the leader's chosen \(x\) but also to a side information \(\), such as weather or season, (2) there are multiple followers, each aiming to maximize their own utility, in which case \(\) represents different possible follower preferences, and (3) there are multiple followers, each facing an uncertain contextual variable, i.e., \(=(i,)\) represents the \(i\)-th follower encountering a specific context \(_{}\).

The proposed framework extends the concept of contextual bilevel optimization , where the follower engages in static contextual stochastic optimization rather than sequential decision-making. It also expands upon traditional MDP model design [78; 15] and configurable MDPs [50; 55], where typically only one follower attempts to solve an MDP, as opposed to CMDPs. We defer a full discussion of the related works to Appendix A. Beyond these fields, our framework finds various applications in Principal-Agent problems , RLHF [13; 58], dynamic Stackelberg games [27; 65], Security Games [60; 46], dynamic mechanism design , contract theory [41; 69], and tax design [19; 82; 34]. See Appendix B for the concrete CB-RL formulations of these applications.

Despite its wide applicability, to the best of our knowledge, there are no algorithms specifically designed for CB-RL. The closest is an algorithm from model design for MDPs , which can be adapted to our setting after some modifications. However,  requires the follower to solve the MDP deterministically using soft value iteration. Moreover, the full hypergradient is computed in each iteration, as part of which the leader requires access to the lower-level computations. Both of these aspects significantly restrict how well the method can scale to larger settings.

Instead, in this work, we propose a stochastic _Hyper Policy Gradient Descent (HPGD)_ algorithm for the leader that solely relies on trajectory data from followers. The followers can use a variety of possibly stochastic learning algorithms to find an approximately optimal policy for the lower-level CMDPs. The leader in turn is agnostic of the exact algorithm used, as the hypergradient is estimated using only trajectory samples generated from the follower policy. The fact that both the lower-level and the hypergradient computation are stochastic makes HPGD salable to large problem settings.

We show non-asymptotic convergence of HPGD to a stationary point and validate these findings through experimental evidence. In scenarios where followers grant the leader full control over their training procedure, as posited in prior work , we present an accelerated HPGD algorithm, designed to minimize the number of lower-level iterations.

**Our Contributions**

* We introduce _Contextual Bilevel Reinforcement Learning (CB-RL)_ that captures a wide range of important applications (Sec. 2). It is the first bilevel reinforcement learning framework that through the context \(\) allows multiple followers and side information. We summarize the key differences to the previous literature in Table 1.
* We propose a stochastic _Hyper Policy Gradient Descent (HPGD)_ algorithm that performs stochastic gradient descent on the upper-level objective (Sec. 3.2). Importantly, we are the first to estimate the hypergradient from lower-level trajectory samples instead of computing it exactly, while further providing convergence guarantees. Furthermore, our approach _is agnostic of the learning dynamics of the agent_, enabling followers to utilize a wide range of algorithms to solve the lower-level CMDPs. We only assume the leader can sample lower-level trajectories from an inexact oracle. For several widely-used RL algorithms, we explicitly show how to use them to build the inexact oracle needed by HPGD. Noteably, we are the first to consider stochastic lower-level learning algorithms, such as soft Q-learning.

* We establish the non-asymptotic convergence rate of HPGD to a stationary point of the overall objective--despite the nonconvex lower-level problem (Sec. 3.2). When assuming full hypergradient information, i.e., deterministic updates, the outer iteration complexity of HPGD reduces to \((^{-2})\), recovering previous results. Moreover, we discuss how to estimate the hypergradient if the upper-level loss function admits a specific form of cumulative costs (Sec. 3.3).
* When the leader is allowed to control the follwers' learning dynamics (Sec 4), we propose a stochastic accelerated algorithm denoted as HPGD RT-Q (Alg 8). It greatly reduces the number of lower-level soft Q-learning iterations from \(}(^{-2})\) to \(((^{-1}))\), such that we recover the rate for deterministic lower-level updates. For this result we leverage several techniques, such as mini-batches, multilevel Monte Carlo [29; 37], and importance sampling.
* We demonstrate the performance of HPGD for principal agent reward shaping and tax design (Sec. 5). In certain cases, the stochastic updates of HPGD are beneficial as they avoid local minima. Moreover, HPGD needs fewer outer iterations compared to benchmark algorithms.

## 2 Problem Formulation

We consider a bilevel optimization problem, where the follower solves a Contextual Markov Decision Process (CMPD) and the leader controls the configuration of the CMDP. In particular, the leader chooses a parameter \(x X^{d}\) and nature chooses a random context \(\) according to a distribution \(_{}\). Together \((x,)\) parameterises an MDP \(_{x,}\), which the follower aims to solve. \(_{x,}\) is defined by a tuple \((,,r_{x,},P_{x,},_{x,},)\), where \(\) denotes the state space, \(\) denotes the action space, \(r_{x,}(,):\) is the reward function, \(P_{x,}(;,): \) denotes the transition kernel, \(_{x,}\) indicates the initial state distribution, and \(\) is the discount factor. The subscript \(x,\) implies that rewards, transitions, and initial state distribution depend on the leader's decision \(x\) and the context \(\). Connecting to previous works, for a fixed \(x\), \(_{x,}\) is a _contextual MDP_ with respect to \(\). For a fixed \(\), \(_{x,}\) generalizes a _configurable MDP_. Given \(_{x,}\), the follower maximizes an entropy-regularized objective by choosing a policy \(_{x,}\), where \(_{x,}(a;s)\) denotes the probability of choosing action \(a\) in state \(s\).

\[_{}J_{,x,}()=_{s_{0}}[V^{}_{,x, }(s_{0})]=_{s_{0}}[^{}_{P_{x,}}[ _{t=0}^{}^{t}(r_{x,}(s_{t},a_{t})+ H(;s_{t} ))]],\] (2)

where \(s_{0}_{x,}\), \(a_{t}(;s_{t}),s_{t+1} P_{x,}(;s_{t},a_{t})\) and \(H(;s)=-_{a}(a;s)(a;s)\). We call \(>0\) the regularization parameter and \(V^{}_{,x,}\) the value function. As standard in RL literature,

    &  & \)} & \)} & \)} &  &  &  &  \\  & & & & & & Control & & & Iterations & Iterations & Method \\ 
 & & & & & & Control & & Deter & \((^{-2})\)* & \(((^{-1}))\) & Soft-VI \\ 
 & & & & & & Agnostic & & Deter & \((^{-2})\) & \(((^{-1}))\) & PG \\ 
 & & & & & & & Agnostic & & Deter & \((^{-2})\) & \(((^{-1}))\) & PMG \\ 
 & & & & & & & Agnostic & & Deter & \((^{-2})\) & \(((^{-1}))\) & PMG \\    & & & & & & & & & & \(((^{-1}))\) & Soft-VI \\ HPGD & & & & & & & & & \((^{-4})\) & \(((^{-1}))\) & NPG \\  & & & & & & & & & & \(}(^{-2})\) & Soft-Q \\  HPGD & & & & & & & Control & Stoch & \((^{-4})\) & \(((^{-1}))\) & RT-Q \\    
   Multi: Multiple followers. Side Info: Side information. Context: CMDP instead of MDP. \(r_{x}\), \(P_{x}\), and \(_{x}\) denote the dependence of rewards, transitions, and initial state distribution on \(x\). Agnostic vs. Control: Whether leader can influence the training of the follower(s). Deter vs. Stoch: Requiring full knowledge of hypergradient or estimating it from samples. Complexity is based on \(\| F(x)\|^{2}^{2}\) instead of \(\| F(x)\|^{2}\). * assumes convexity of \(F\) in \(x\) and considers \(F(x)- F(x)\). VI: Value Iteration. PMG: Policy Mirror Gradient. PI: Policy Iteration. NPG: Natural Policy Gradient. Q: Q-learning. RT-Q: Randomly Truncated Soft Q-learning.

Table 1: Summary of Related Works in Bilevel Reinforcement Learning.

we define the related Q and advantage functions as:

\[Q_{,x,}^{}(s,a) =r_{x,}(s,a)+_{s^{} P_{x,}(:s,a)} [V_{,x,}^{}(s^{})]\] \[A_{,x,}^{}(s,a) =Q_{,x,}^{}(s,a)-V_{,x,}^{}(s)=Q_{ ,x,}^{}(s,a)-_{a^{}}(a^{};s)Q_{,x,}^ {}(s,a^{}).\] (3)

The unique optimal policy for (2) is given by \(_{x,}^{*}(s;a)(Q_{,x,}^{*}(s,a)/)\), i.e., the softmax of the optimal Q-function .2 Given \(x,_{x,}^{*},\), the leader in turn incurs a loss \(f(x,_{x,}^{*},)\), which it wants to minimize in expectation over \(_{}\). To do so, it needs to choose \(x\) to align the follower's policy \(_{x,}^{*}\) with the leader's objective. CB-RL can thus be formulated as the following stochastic bilevel optimization problem:

\[_{x}& F(x):=_{}[f(x, _{x,}^{*},)]\\ &_{x,}^{*}=*{argmax }_{}J_{,x,}().\] (4)

Equation (4) is well-defined due to entropy regularization ensuring the uniqueness of \(_{x,}^{*}\). It further ensures \(_{x,}^{*}\) is differentiable, stabilizes learning and appears in previous works . Moreover, the difference between the regularized and unregularized problem generally vanishes as \( 0\)[15; 26].

CB-RL captures many important real-world problems, including RLHF, dynamic mechanism design, tax design, and general principal-agent problems. We discuss these in detail in Appendix B.

## 3 Hyper Policy Gradient Descent Algorithm for CB-RL

In this section, we derive a simple expression for the hypergradient of CB-RL. We present HPGD and prove non-asymptotic convergence. HPGD can be combined with a large class of lower-level MDP solvers satisfying a mild inexact oracle assumption. We show this is the case for several popular RL algorithms. Furthermore, we present results for two important special cases of our problem: (1) when the upper-level objective decomposes as a discounted sum of rewards over the lower-level trajectories, and (2) when the leader can direct the lower-level algorithm. We defer all proofs to Appendix E. and make the following standard assumptions on how \(x\) and \(\) influence the setup of the CMDP.

**Assumption 3.1**.: _We assume the following conditions:_

* \(f\) _is_ \(L_{f}\)_-Lipschitz continuous and_ \(S_{f}\)_-smooth in_ \(x\) _and_ \(\)_, uniformly for all_ \(\)_, i.e._ \[\|f(x_{1},_{1},)-f(x_{2},_{2},)\|_{}  L_{f}(\|x_{1}-x_{2}\|_{}+\|_{1}-_{2}\|_{})\] \[\|_{x}f(x_{1},_{1},)-_{x}f(x_{2},_{2}, )\|_{}  S_{f}(\|x_{1}-x_{2}\|_{}+\|_{1}-_{2}\|_{})\] \[\|_{}f(x_{1},_{1},)-_{}f(x_{2},_{ 2},)\|_{}  S_{f}(\|x_{1}-x_{2}\|_{}+\|_{1}-_{2}\|_{})\]
* \( x,:|r_{x,}(s,a)|<\), \(\|_{x} P_{x,}(s^{};s,a)\|_{}<K_{1}\), \(\|_{x}r_{x,}(s,a)\|_{}<K_{2}\).

### Hypergradient derivation

The leader's loss \(f\) depends on \(x\) directly and indirectly through \(_{x,}^{*}\). Therefore, the derivative of \(f\) with respect to \(x\) is commonly referred to as the _hypergradient_ to highlight this nested dependency. It is possible to obtain a closed-form expression of the hypergradient, using the implicit function theorem . However, this involves computing and inverting the Hessian of the follower's value function, which can be computationally expensive and unstable [24; 47]. Instead, we leverage the fact that \(_{x,}^{*}\) is a softmax function to explicitly compute its derivative with respect to \(x\), which is given by \(^{*}(s,a)}{dx}=_{x,}^{*}(a;s) _{x}A_{,x,}^{_{x,}^{*}}(s)\), where \(_{x}A=(_{x_{1}}A,,_{x_{d}}A)\). Applying the Dominated Convergence Theorem to switch derivative and expectation, we arrive at Theorem 1.

**Theorem 1**.: _Under Assumption 3.1, \(F\) is differentiable and the hypergradient is given by_

\[=_{}[f(x,_{x,}^{*}, )}{ x}+_{s,a_{x,}^{*}}[f(x,_{x,}^{*},)}{_{x,}^ {*}(a;s)}_{x}A_{,x,}^{_{x,}^{*}}(s,a)]],\] (5)

_where \(\) is any sampling distribution with full support on the state space \(\)._The first term captures the direct influence of \(x\) on \(f\), and the second the indirect influence through \(^{*}_{x,}\). For now we assume the leader knows \(_{1}f(,,)\) and \(_{2}f(x,,)\). It remains to compute \(_{x}A^{^{*}_{x,}}_{,x,}(s,a)\), i.e. the partial derivative with respect to \(x\) evaluated for a given policy. For this, cf. (3), we need \(_{x}_{,x,}^{^{*}_{x,}}(s,a)\). We derive an expression for the latter in Theorem 2. The proof adapts the analysis of the policy gradient theorem to account for the dependence of \(P_{x,},_{x,}\) and \(r_{x,}\) on \(x\).

**Theorem 2**.: _For given \(,x,\), it holds that:_

\[_{x}Q^{}_{,x,}(s_{0},a_{0})=_{s,a}^{}[ _{t=0}^{}^{t}(s_{t},a_{t})}{dx}+^{t+1} (s_{t+1};s_{t},a_{t})}{dx}V^{}_{,x,}(s_{t+1} )].\]

Note, Theorems 1 and 2 generalize existing results in model design for MDPs to CMDPs [15; 78].

### HPGD Algorithm and Convergence Analysis

Computing the exact hypergradient is computationally expensive and thus infeasible in larger settings. Instead, to minimize \(F(x)\), one would ideally sample unbiased estimates of the hypergradient in Equation (5) and run stochastic gradient descent (SGD). However, the leader does not have access to \(^{*}_{x,}\) and generally no control over the training procedure of the lower level. Instead, we assume the follower adapts any preferred algorithm to solve the MDP up to a certain precision \(\) and the leader can observe trajectories from the follower's policy. Such a setting is well-motivated by economic applications.

**Assumption 3.2**.: _For any \(_{x,}\), the leader has access to an oracle \(o\), which returns trajectories sampled from a policy \(^{o}_{x,}\) such that \( x,:_{o}[\|^{*}_{x,}-^{o}_{x, }\|_{}^{2}]^{2}\)._

We will show that Assumption 3.2 is relatively mild and holds for a variety of RL algorithms. Given access to trajectories generated by \(^{o}_{x,}\), the leader can construct an estimator of \(_{x}A^{^{o}_{x,}}_{,x,}(s,a)\) by rolling out \(^{o}_{x,}\) for \(T\) steps, where \(T(1-)\). We defer the construction (Algorithm 2) and proof of unbiasedness (Proposition 3) to the Appendix. Using this estimator, we introduce HPGD in Algorithm 1. As \(F\) is generally nonconvex due to the bilevel structure , we demonstrate non-asymptotic convergence to a stationary point of \(F\), which matches the lower bound for solving stochastic smooth nonconvex optimization .

**Theorem 3**.: _Using HPGD, under Assumptions 3.1 and 3.2, for \( 1/(2S_{f})\), we have the following:_

\[\|_{T})}{dx}\|^{2}= ++.\] (6)

_For \(=(1/)\) and \(=(1/)\), HPGD converges to a stationary point at rate \((1/)\)._

Proof sketch.: Using the smoothness of \(F\) and the fact that \(_{T}\) is uniformly sampled from all iterates, we upper bound the left side of (6) by the sum of three terms. The first is \(|F(x_{0})-_{x}F(x)|/( T)\). The second depends on the bias of our gradient estimate, which we show is linear in \(\). The last term depends on \(\) times the variance of our estimator, which is bounded.

To the best of our knowledge, Theorem 3 is the first result that shows convergence when using stochastic estimates for the hypergradient. When the hypergradient can be computed exactly, the last \(()\) term vanishes and we recover the deterministic convergence rates of previous works [15; 13; 58].

Another major advantage of HPGD is that the follower can use any (possibly stochastic) algorithm satisfying Assumption 3.2 to solve the lower-level MDP, while the leader only needs access to generated trajectories. While Assumption 3.2 certainly holds if the follower solves the MDP exactly, for example with an LP-solver, we are interested in verifying it for common RL algorithms, which can scale to larger state and action spaces. In Appendix E.8, we prove non-asymptotic convergence to \(^{*}_{x,}\) for Soft Value Iteration, which converges at rate \(( 1/)\) (Proposition 5); Q-learning, which converges at rate of \(((1/)/^{2})\) (Proposition 6) and Natural Policy Gradient, which converges at rate of \(( 1/)\) (Proposition 8). Additionaly, we show Vanilla Policy Gradient converges asymptotically in Proposition 7. All these Algorithms thus satisfy Assumption 3.2, which makes HPGD scalable and widely applicable to settings where followers might use a variety of model-free or model-based algorithms.

### Upper-Level Discounted Reward Objective

So far we assumed the leader knows \(_{1}f(,,)\) and \(_{2}f(x,,)\). In this subsection, instead, we assume \(f\) can be written as the negative expected sum of discounted rewards over the lower-level trajectories and show how to estimate the hypergradient from trajectory samples without explicit knowledge of \(_{1}f(,,)\) and \(_{2}f(x,,)\). In many practical applications, such as reward shaping, or dynamic mechanism design (cf. Appendix B), the loss \(f\) satisfies:

\[f(x,^{*}_{x;},)=-_{s_{0}}^{^{*}_{x,}} _{t}^{t}_{x,}(s_{t},a_{t}).\] (7)

Here \(_{x,}\) represents the reward of the leader, which is generally distinct from the follower's reward. The expectation is taken over trajectories induced by the lower-level \(^{*}_{x,}\). In this case, the leader does not know the partial derivatives of \(f\) but can still estimate the hypergradient from trajectory samples. The following proposition follows from a similar analysis as the policy gradient theorem.

**Proposition 1**.: _If \(f\) decomposes as in Equation (7), then \(\) can be expressed as follows:_

\[=_{}_{s_{0} _{x,}}^{^{*}_{x,}} _{t=0}^{}^{t} _{x}A_{,x,}^{^{*}_{x,}}(s_{t},a_{t})_{x, }(s_{t},a_{t})\] (8) \[+_{x,}(s_{t},a_{t})}{dx}+_{x} P_{x, }(s_{t};s_{t-1},a_{t-1})_{x,}(s_{t}),\]

_where for compactness, we slightly abuse notation to express \(_{x,}(s_{0})\) as \(P_{x,}(s_{0},a_{-1},s_{-1})\)._

Here \(_{x,},_{x,}\) are the (unregularized) value and state action value functions with respect to \(_{x,}\). Comparing to Theorem 1, note that the expectation is over trajectories with starting states distributed according to the actual initial distribution \(_{x,}\) instead of some \(\). We discuss how to construct estimators for (8) in Algorithm 5 (Appendix D) and prove unbiasedness in Proposition 4 (Appendix E.7). A special case of Equation (8) appeared in , where they consider model design for MDPs, that does not take into account contextual uncertainty or the possibility of multiple followers, i.e when the support of \(\) is a singleton.

## 4 Accelerated HPGD with Full Lower-Level Access

Previously, we assumed that the leader does not know the solver used in the lower level and queries trajectories from an oracle. However, in certain settings, such as model design , and dynamic mechanism design , the leader can additionally influence how the followers solve the CMDP. In this section, we focus on the case when the followers use a stochastic tranning procedure, which usually require a polynomial number of steps in terms of \(^{-1}\), to learn the optimal lower-level policy. We argue that if the leader has influence on the followers' training procedure, we can greatly reduce the number of lower-level iterations.

Let us assume that the lower level is solved using vanilla soft Q-learning (Algorithm 4 in Appendix D). According to Proposition 6 (Appendix E.7), the follower needs to run \(T=(K2^{K})\) iterationsto ensure that \(\|_{x,}^{T}-_{x,}^{*}\|_{}^{2} 2^{-K}\) and thus \(\|[-}{dx}}]\| _{}=(2^{-K/2}),\) where \(^{T}\) denotes the learned policy after running \(T\)-th Q-learning iterations and \(}{dx}}\) denotes the corresponding hypergradient estimator.

To reduce the lower-level iteration complexity, we propose a randomized early stopping scheme over the lower-level soft Q-learning iterations, denoted as randomly-truncated soft Q-learning (RT-Q). The pseudocode is given in Algorithm 8 (Appendix E.5). We illustrate the high-level idea below.

Without loss of generality, consider a subsequence \(t_{k}:=(k2^{k})\) such that \(t_{K}:=T\). Let \(F_{T}\) denote the hypergradient estimator, based on the \(T\)-th policy iterate \(^{T}\). It holds that:

\[F_{T}=F_{t_{K}}=F_{t_{1}}+_{k=1}^{K-1} (F_{t_{k+1}}-F_{t_{k}})=F_{t_{ 1}}+_{k p_{k}}[F_{t_{k+1}}-F _{t_{k}}}{p_{k}}],\]

where \(p_{k}\) denotes a truncated geometric distribution, such that \(p_{k} 2^{-k}\). The above shows that \(F_{t_{1}}+p_{k}^{-1}F_{t_{k+1}}-F_ {t_{k}}\) with \(k p_{k}\) is an unbiased estimator of \(F_{T}\). Using this estimator, the follower does not need to run \((K2^{K})\) soft Q-learning iterations but in expectation only \(_{k=1}^{K-1}p_{k}t_{k}=(K^{2})\) iterations. This implies that if the leader can direct how the followers learn and observe behaviors sampled from their learned policies, we can generate a hypergradient estimator with the same bias as \(F_{T}\) but a much smaller lower-level iteration complexity. We formalize our results in the following Theorem.

**Theorem 4** (Improved iteration complexity using RT-Q).: _Using Randomly Truncated soft Q-learning (RT-Q) instead of vanilla soft Q-learning to estimate the hypergradient, we achieve the bias, variance, and lower-level iteration complexity results summarized in Table 2._

The idea has been previously studied for contextual bilevel optimization under the name randomly truncated multilevel Monte-Carlo [29; 36; 37]. The reduction in the iteration complexity generally comes at the expense of an increased variance of the hypergradient estimator. In , this increase is logarithmic as the lower-level problem is a static optimization problem and samples generated to estimate the hypergradient are independent from the lower-level decision variable. This structure is crucial for controlling the increased variance of the hypergradient estimator. However, for CB-RL, rollouts generated from \(_{x,}^{t}\) are used to estimate the hypergradient. These trajectory samples thus depend on the lower-level decision and it is not possible to control the variance as in . To address this issue, we notice that the major source of randomness in our hypergradient estimators stems from the estimator \(A^{_{k}}}(s,a)\) computed by GradientEstimator (Algorithm 2). We control this randomness by sampling multiple trajectories with a random length, which is in expectation \((1)\).

We further sample an action \(a\) once from \(^{t_{k+1}}\) and then use it to compute both \(A^{^{t_{k+1}}}}(s,a)\) and \(A^{^{t_{k}}}}(s,a)\), using importance sampling. Combining all these tricks with multi-level Monte Carlo, RT-Q achieves a variance of \((K)\), where \(K=((^{-1}))\).

## 5 Numerical Experiments

We illustrate the performance of HPGD in the Four-Rooms environment and on the Tax Design for Macroeconomic Model problem [34; 15] that we extend to multiple households with diverse preferences. We use Adaptive Model Design (AMD)  and a zeroth-order gradient approximation algorithm for comparison. Details on the zeroth-order algorithm are deferred to Appendix F.1.2. We note that AMD is not directly applicable to CB-RL as it was designed for solving an MDP without

    & Vanilla & RT-Q \\  Bias & \((2^{-K/2})\) & \((2^{-K/2})\) \\ Variance & \((1)\) & \((K)\) \\ Complexity & \((K2^{K})\) & \((K^{2})\) \\   

Table 2: Bias, variance and lower-level iteration complexity of hypergradient estimators when using vanilla soft Q-learning and RT-Q.

context. We apply it with modifications described in Appendix F.1.1. To our knowledge, such a zeroth-order gradient method is also the first of its kind for CB-RL. The main distinction between the algorithms is the zeroth-order algorithm requires two oracle queries for each gradient calculation while HPGD and AMD require only one. However, the zeroth-order method only needs to observe the function value of the upper level while the latter two require first-order information about the lower-level contextual MDP. In particular, AMD assumes complete access to the MDP to calculate the exact hypergradient while HPGD relies only on trajectory samples. Technical details about the implementation 3 are deferred to Appendix (F.2).

### Four-Rooms Environment

Figure ((a)a) depicts the lower-level CMDP for the Four-Rooms environment. \(S\) denotes the initial position while \(G^{1}\) and \(G^{2}\) are goal states. We consider the two goal states as separate tasks and define \(\) in Equation (4) to be the uniform distribution over the set of tasks, i.e., \((\{1,2\})\). We denote the goal state in each task by \(G^{}\). The state space \(\) is defined by the cells of the grid world while the actions are the movements in the four directions. In each step \(t\), with probability \(2/3\), the agent moves to \(s_{t+1}\) following the chosen direction \(a_{t}\) while it takes a random movement with probability \(1/3\). The reward is always zero except when \(s_{t}=G^{}\) where \(r(s_{t},a_{t})=1\), and the episode resets. To incentivize taking the shortest path, we set the discount factor as \(=0.99\).

For the upper level, we let \(x\) parameterize an additive penalty function \(_{x}:[-0.2,0.0]\)4, such that the follower receives a reward of \(r+_{x}\), as in the Principal-Agent problem . The goal of the leader is to steer the followers through the cell marked with \(+1\) in Figure (a)a, denoted by \(s^{+1}\), while keeping the penalties allocated to states to their minimum. We define \(\) in Equation (7) as

\[_{x,}(s_{t},a_{t})=_{\{s_{t}=s^{+1}\}}-_{\{s_{t}=G^{}\}}_{s,a}_{x}(s,a),\]

where \(\) is the indicator function and the second term defines the cost associated with implementing the penalties for the lower level. Note that there is a trade-off between the terms in \(\) depending on the context variable \(\). If \(=2\), the desired change in the follower's policy can be achieved with small interventions since the shortest path from \(S\) to \(G^{2}\) is already going through the bottom-left room. When \(=1\), the leader must completely block the shortest path from \(S\) to \(G^{1}\) to divert the follower through the desired state. An efficient algorithm for this CB-RL problem therefore must avoid the local optimum of setting \(=0\) and find the balance between the follower visiting state \(s^{+1}\) and implementing too large penalties in the CMDP.

Figure ((b)b) depicts the upper-level's objective function over the learning iterations \(t\) with hyperparameters \(=0.001\) and \(=1.0\). HPGD outperforms both AMD and the Zero-Order algorithms

Figure 1: Four-Rooms State Space and Performance. **Left**: \(S\) denotes the start state while \(G^{1}\) and \(G^{2}\) denote goal states that are considered separate tasks. \(+1\) denotes the target cell to which the upper-level aims to steer the lower-level MDP. **Right:** HPGD escapes local optima achieving higher performance than comparison algorithms.

in this instance in terms of overall performance. The major difference in their performances is that HPGD successfully escapes the local optimum of \(=0\) after about \(5000\) steps and assigns all the additive penalty budget to states in the gridworld. On the contrary, AMD and Zero-Order converge to the local optimum of minimizing the implementation penalty term in \(\). They only utilize \(38\%\) and \(26\%\) of the available budget of \(-0.2\) to divert the follower when \(=2\) but neglect the goal state \(G^{1}\).

Figure 2 shows the value of additive penalties \(\) in the state space with the highest probability paths for the goal states. HPGD successfully blocks the follower when \(=1\) and diverts its shortest path from \(S\) to \(G^{1}\) along the other rooms, while AMD and Zero-Order fail to assign sufficient penalty to the upper corridor to cause the same effect. All algorithms are successful in ensuring that the shortest path through the bottom-left room is going through the marked state.

The parameters \(\) and \(\) were chosen for demonstration purposes to highlight the capability of HPGD to escape local minima, as has been observed for SGD . However, we emphasize that in the majority of the cases, the three algorithms perform equally as shown in Table 3. We provide the figures for the remaining hyperparameters in Appendix F.2.3. The slightly higher performance of AMD and low standard error among initializations is expected since this algorithm calculates the gradient of \(f\) deterministically while HPGD and Zero-Order rely on stochastic estimates yielding more variations, especially for the Zero-Order approach.

### Tax Design for Macroeconomic Models

We test HPGD on a bilevel macroeconomic model evaluating taxation schemes based on [34; 15]. The economic model consists of a finite set of consumption goods \(i\{1,,M\}\) each with price \(p_{i}\). We choose \(M=3\) with unit prices. On the lower-level, at each time step \(t\), \(s_{t}\) denotes the accumulated assets of a household which in turn chooses the number of hours worked

   Parameters &  \\ \(\) & \(\) & HPGD & AMD & Zero-Order \\  \(0.001\) & \(1\) & \( 0.088\) & \(0.58 0.000\) & \(0.59 0.059\) \\ \(0.001\) & \(3\) & \(0.51 0.006\) & \(0.51 0.000\) & \(0.50 0.005\) \\ \(0.001\) & \(5\) & \(0.46 0.006\) & \(0.46 0.003\) & \(0.46 0.007\) \\ \(0.003\) & \(1\) & \(0.95 0.002\) & \(1.00 0.000\) & \(0.91 0.048\) \\ \(0.003\) & \(3\) & \( 0.001\) & \(0.39 0.000\) & \(0.40 0.028\) \\ \(0.003\) & \(5\) & \(0.29 0.003\) & \(0.32 0.000\) & \(0.32 0.002\) \\ \(0.005\) & \(1\) & \(1.17 0.011\) & \(1.28 0.003\) & \(1.15 0.026\) \\ \(0.005\) & \(3\) & \(1.01 0.002\) & \(1.13 0.004\) & \(1.02 0.027\) \\ \(0.005\) & \(5\) & \(0.87 0.003\) & \(0.97 0.009\) & \(0.79 0.027\) \\   

Table 3: Performance over hyperparameters \(\) and \(\) for the Four Rooms Problem averaged over \(10\) random seeds with standard errors. Algorithms perform on-par for most hyperparameters while HPGD outperforms others in few. AMD enjoys low variance due to the non-stochastic gradient updates while Zero-Order suffers from the most variation.

Figure 2: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively. HPGD efficiently steers the lower-level MDP when the task is to reach \(G^{1}\) while others are only successful in the case of \(G^{2}\).

and consumption \(c_{i,t}\). The environment updates the accumulated assets of the household as \(s_{t+1}=s_{t}+(1-x)wn_{t}-_{i=1}^{M}c_{i,t}+\) where \(\) is sampled from a normal distribution with mean \(0\) and standard deviation \(\). We clip the accumulated assets to the range \([-100,100]\) for numerical stability. The utility of a household is given by \(u_{t}=(s_{t})- n_{t}^{2}+_{i=1}^{M}(c_{i,t}/(p(1+y_{i})))^{ _{i}}\) where the product-of-consumption term corresponds to the Cobb-Douglas function  and \(()\) is the value of accumulated assets. We define \(_{}\) as the distribution of households representing different socio-economic groups in the economy. In particular, we define two equal-sized groups with \(=(0.6,0.3,0.1)\) and \((0.1,0.7,0.2)\) that model their different preferences over the goods in the economy and optimize their consumption behavior using regularized deep Q-learning  with \(=0.2\). On the upper-level, a tax designer is optimizing the discounted sum of social welfare by setting the income tax \(x\) and value-added tax \(y_{i}\) for \(i\{1,,M\}\). In each time step \(t\), the social welfare is defined as \(v_{t}=(s_{t})+_{i=1}^{M}c_{i,t}/(1+y_{i})+(_{i=1}^{M}c_ {i,t}y_{i}/(1+y_{i})+wxn_{t})\) where \(()\) is the utility of accumulated assets and \(\) is a positive hyperparameter.

Figure 2(a) demonstrates that HPGD can successfully optimize the hyperparameters even in continuous complex problems. Benefiting from first-order information, HPGD converges faster than the Zero-Order algorithm and increases the social-welfare by about \(25\%\). Additionally, HPGD shows better qualitative results for the optimised tax rates in Figure 2(b). HPGD swiftly increases the income tax to improve its objective by increasing tax revenues while sets value-added tax rates according to the preferences of the household groups. Households on average prefer the second good the most, therefore, \(y_{2}\) is set low to increase consumption and therefore maximize the second term in \(v_{t}\) while the third good is the least preferred for which the tax rate is increased5 since consumption is already low. While Zero-Order shows equivalent performance on Figure 2(a) to HPGD it fails to distinguish goods when setting value-added tax rates. We defer further details on the implementation, hyperparameter choices, and additional results to Appendix F.3.1.

## 6 Conclusion

We introduce CB-RL, a class of stochastic bilevel optimization problems with lower-level contextual MDPs that capture a wide range of important applications, where a leader aims to design environments and incentive structures that align the followers' policies with the leader's upper-level objective. We propose HPGD, an oracle-based algorithmic framework, and analyze its convergence. Importantly, HPGD works with any existing algorithm that solves the lower-level CMDP to near-optimality, making it suitable in various regimes when the leader can only observe trajectories of followers. Moreover, HPGD is the first provably convergent algorithm in this area, which uses stochastic estimates of the hypergradient. We further propose RT-Q, a more efficient algorithm and study its bias, variance, and cost when the leader can fully control the followers' training. Numerical results further validate the expressiveness of the proposed model and the performance of our algorithm. Future directions include 1) applying HPGD in various real-world applications, 2) studying the setting when the lower-level problem is a game, and 3) studying single-loop algorithms for bilevel reinforcement learning with when the lower-level is just an MDP.