# Vision Transformer Neural Architecture Search for Out-of-Distribution Generalization: Benchmark and Insights

Vision Transformer Neural Architecture Search for Out-of-Distribution Generalization: Benchmark and Insights

Sy-Tuyen Ho &Tuan Van Vo &Somayeh Ebrahimkhani &Ngai-Man Cheung

Singapore University of Technology and Design (SUTD)

sytuyen_ho@mymail.sutd.edu.sg,

{vovan_tuan,somayeh_ebrahimkhani,ngaiman_cheung}@sutd.edu.sg

Equal ContributionCorresponding Author

###### Abstract

While Vision Transformer (ViT) have achieved success across various machine learning tasks, deploying them in real-world scenarios faces a critical challenge: generalizing under Out-of-Distribution (OoD) shifts. A crucial research gap remains in understanding how to design ViT architectures - both manually and automatically - to excel in OoD generalization. **To address this gap**, we introduce OoD-ViT-NAS, the first systematic benchmark for ViT Neural Architecture Search (NAS) focused on OoD generalization. This comprehensive benchmark includes \(3,000\) ViT architectures of varying model computational budgets evaluated on \(8\) common large-scale OoD datasets. With this comprehensive benchmark at hand, we analyze the factors that contribute to the OoD generalization of ViT architecture. Our analysis uncovers several key insights. Firstly, we show that ViT architecture designs have a considerable impact on OoD generalization. Secondly, we observe that In-Distribution (ID) accuracy might not be a very good indicator of OoD accuracy. This underscores the risk that ViT architectures optimized for ID accuracy might not perform well under OoD shifts. Thirdly, we conduct the first study to explore NAS for ViT's OoD robustness. Specifically, we study \(9\) Training-free NAS for their OoD generalization performance on our benchmark. We observe that existing Training-free NAS are largely ineffective in predicting OoD accuracy despite their effectiveness at predicting ID accuracy. Moreover, simple proxies like #Param or #Flop surprisingly outperform more complex Training-free NAS in predicting ViTs OoD accuracy. Finally, we study how ViT architectural attributes impact OoD generalization. We discover that increasing embedding dimensions of a ViT architecture generally can improve the OoD generalization. We show that ViT architectures in our benchmark exhibit a wide range of OoD accuracy, with up to \(11.85\%\) for some OoD shift, prompting the importance to study ViT architecture design for OoD. We firmly believe that our OoD-ViT-NAS benchmark and our analysis can catalyze and streamline important research on understanding how ViT architecture designs influence OoD generalization. **Our OoD-NAS-ViT benchmark and code are available at https://hosytuyen.github.io/projects/OoD-ViT-NAS**

## 1 Introduction

Vision Transformers (ViT)  have recently achieved impressive results and become a major area of research in computer vision, with significant efforts towards understanding how ViT works. These efforts have led to the proposal of both manually designed architectures [1; 2; 3; 4; 5] or automated-searched architectures [6; 7; 8; 9; 10; 11; 12] to advance ViT architectures.

**Research Gap.** Existing research on ViT architectures focuses on maximizing In-Distribution (ID) accuracy, while studies on the impact of ViT architectures on Out-of-Distribution (OoD) generalization are limited. Initial works , , and  evaluate sets of \(3\), \(10\) and \(22\) human-designed ViT architectures under OoD settings, respectively, and provide coarse insights into which models exhibit better OoD generalization. However, with very limited ViT architectures studied in previous works, the influence of ViT structural attributes (e.g., embedding dimension, number of heads, MLP ratio, number of layers) on OoD generalization remains unclear. Besides, in the context of ViT Neural Architecture Search (NAS), while there are various ViT NAS for ID accuracy [6; 7; 16; 8; 10; 11; 17; 18; 19; 20; 9], there is no study on ViT NAS for OoD generalization.

**In this paper,** we address existing research gaps by introducing OoD-ViT-NAS, the first comprehensive benchmark specifically designed for ViT's OoD generalization. Building NAS benchmarks is notoriously time-consuming and expensive due to the need to train and evaluate every candidate architecture. This challenge is particularly acute for ViT, known for its high computational demands and memory usage [21; 22; 23]. To overcome this bottleneck, we propose leveraging One-Shot NAS, specifically AutoFormer , a widely used ViT search space.We sample a diverse set of sub-architectures (models) to populate our benchmark. Importantly, these subnets inherit the weights from the pre-trained supernets, and _their performance has been shown to be comparable to, or even superior to, that of architectures trained alone_[20; 6] This approach enables us to efficiently acquire a large pool of ViT architectures for OoD generalization analysis. Using OoD-ViT-NAS, we conduct extensive OoD generalization analysis and gain several key insights. Additionally, with our benchmark, our work is the first to explore (training-free) NAS for ViT's OoD generalization. Our contributions are summarized below:

* We introduce OoD-ViT-NAS, the first comprehensive benchmark designed for NAS research on ViT's OOD generalization. This benchmark includes \(3,000\) diverse ViT architectures sampled from the widely used ViT search space . These architectures span a wide range of computational budgets. To thoroughly benchmark OoD generalization, these architectures are evaluated on the \(8\) most common and state-of-the-art (SOTA) OoD datasets: ImageNet-C , ImageNet-A , ImageNet-O , ImageNet-P , ImageNet-D , ImageNet-R , ImageNet-Sketch , and Stylized ImageNet  (Sec. 3)

Figure 1: **We propose, OoD-ViT-NAS, the first comprehensive benchmark for NAS on OoD generalization of ViT architectures.** Then, we comprehensively investigate OoD generalization for ViT. The detailed of \(8\) OoD datasets in our investigation can be found in Tab. 1. In this figure, we show the Kendall \(\) ranking correlation between OoD accuracy of different datasets on the left and different quantities at the bottom. Our analysis uncovers several key insights. **(a) ID as an indicator for ViT OoD Generalization (Sec. 4.2)** We show that the correlation between ID accuracy and OoD accuracy is not very high. This suggests that current architectural insights based on ID accuracy might not translate well to OoD generalization. **(b) Training-free NAS for ViT OoD Generalization. (Sec. 4.3)** We conduct the first study of NAS for ViT’s OoD generalization, showing that their effectiveness significantly weakens in predicting OoD accuracy. **(c) OoD Generalization ViT Architectural Attributes. (Sec. 4.4)** Our first study on the impact of ViT architectural attributes on OoD generalization shows that the embedding dimension generally has the highest correlation with OoD accuracy among ViT architectural attributes. Additional results can be found in the Appx.

* Our analysis demonstrates the significant influence of ViT architectural designs on OoD accuracy. This observation encourages future research to focus more on ViT architecture research for OoD generalization (Sec. 4.1)
* We show that high In-Distribution (ID) accuracy is not a very good indicator of OoD accuracy. This suggests that current architectural insights based on ID accuracy might not translate well to OoD generalization (Sec. 4.2)
* We conduct the first study to explore NAS for ViT's OoD generalization. We study \(9\) Training-free NAS for their OOD generalization performance on our benchmark. We observe that despite their prediction accuracy for ID, their effectiveness significantly weakens when predicting OoD accuracy. Furthermore, simple proxies such as the number of parameters (#Param) or the number of floating point operations (#Flop) surprisingly outperform more complex Training-free NAS in predicting ViTs OoD accuracy (Sec. 4.3)
* We study the impact of ViT architecture design on OoD generalization and demonstrate that careful design of ViT architectures can significantly improve OoD generalization. Specifically, increasing the embedding dimensions of a ViT architecture generally can improve its OoD generalization. (Sec. 4.4) We show that architectures with comparable ID accuracy (within an averaging range of \(1.39\%\)) exhibit a wider range of OoD accuracy, averaging \(3.80\%\) and reaching highs of \(11.85\%\), being comparable or even outperforming state-of-the-art (SOTA) training OoD generalization method, such as those based on domain invariant representation learning [30; 31]. For example, under the same OoD setting, the SOTA method  shows an improvement of \(1.9\%\) OoD accuracy.

## 2 Related Work

**Out-of-Distribution (OoD) Generalization**. Addressing Out-of-distribution (OoD) generalization is a challenge, particularly in computer vision. Various approaches have been proposed to tackle this issue. A common strategy focuses on learning features that remain consistent across different domains, thereby promoting generalizability [31; 32; 33; 34; 35; 36; 37]. Other directions explore distributional robustness [38; 39], model ensembles [40; 41], test-time adaptation [42; 43], data augmentation techniques [44; 45; 46; 47; 48; 49; 50], and meta learning [51; 52] for OoD generalization. From an architectural perspective, a few attempts investigate the impact of network architecture on OoD generalization. Early work  shows that over-parameterized networks can hinder OoD performance due to overfitting. This raises an intriguing question: can sub-networks within such architectures achieve better OoD performance? Inspired by the Lottery Ticket Hypothesis (LTH) , the Functional LTH has been explored and shown that over-parameterized networks harbor sub-networks with better OoD performance. Techniques like Modular Risk Minimization  and Debiased Contrastive Weight Pruning  aim to identify these winning tickets. Another direction [57; 14] leverages Neural Architecture Search (NAS) to analyze the OoD robust architectures. However, these studies primarily focus on CNNs. While ViTs have achieved success in various visual recognition, investigations into their OoD generalization are limited. Initial works [13; 14], and  evaluate sets of \(3\), \(10\), and \(22\) human-designed ViT architectures respectively, under OoD settings. Their results provide coarse insights into which models exhibit better OoD performance. _However, the influence of ViT architectural attributes on OoD robustness remains unclear._

**Neural Architecture Search (NAS)**. NAS is a promising approach that has achieved remarkable success in automatically searching efficient and effective architectures for ID performance [58; 20; 59; 60]. Recently, NAS has been explored in the context of adversarial robustness for CNNs as well [61; 62; 63]. With the rise of Vision Transformers (ViTs), several NAS approaches have been applied to improve ViT architectures, including Autoformer , S3 , ViTAS , ElasticViT , DSS , Auto-Prox  and GLiT . Additionally, hybrid CNN-ViT architectures like HR-NAS , UniNet , and NASViT  have also been explored. These efforts have shown promising results in terms of ID accuracy. _However, there has not been any work on NAS for ViT architectures specifically for OoD generalization_.

## 3 OoD-ViT-NAS: NAS Benchmark for ViT's OoD Generalization

In this section, we describe the construction of our OoD-ViT-NAS benchmark with details on the search spaces, datasets, evaluation metrics, and protocol. Our comprehensive benchmark includes\(3,000\) ViT architectures of varying sizes evaluated on \(8\) widely used large-scale, high-resolution, and SOTA OoD datasets. Our OoD-ViT-NAS benchmark is summarized in the Tab. 1

**Search Space.** We construct our benchmark based on Autoormer  search space. This search space is currently a widely used search space in the ViT NAS community for ID data . Autoormer search space is a large vision transformer search space including five architectural attributes that define the building block. _Embedding Dimension:_ This determines the input feature representation size and is typically consistent across layers in ViT architectures. _Q-K-V Dimension:_ This specifies the size of the query, key, and value vectors used in the attention mechanism. _Number of Heads:_ This defines the number of parallel attention computations performed within a single attention block. _MLP Ratio:_ This controls the dimensionality of the feed-forward network within each transformer block. Unlike embedding dimension, in Autoormer search space, Q-K-V Dimension, Number of Heads, and MLP Ratio can be varied across layers. _Network Depth:_ This refers to the total number of transformer layers stacked in the architecture. It is important to note that Autoormer maintains a fixed ratio between the Q-K-V dimension and the number of heads in each block. This ensures that the scaling factor in the attention calculation remains constant. This helps stabilize the gradients of different heads during the training . We strictly follow Autoormer search space. The details can be found in the Appx. E.1.

**Dataset.** Our benchmark consists of the evaluation on large-scale, high-resolution, and most SOTA OoD datasets, including ImageNet-1k , ImageNet-C , ImageNet-P , ImageNet-A , ImageNet-O , ImageNet-R , ImageNet-Sketch , Stylized ImageNet , and ImageNet-D . These datasets capture a comprehensive range of OoD shifts such as common corruptions (blur, noise, digital, weather), Stable-Diffusion-based OoD shifts, and natural OoD shifts. A detailed description of these datasets can be found in the Appx. E.2.

**Metrics.** Following the previous OoD generalization methods , we employ three metrics to construct our benchmark:

Figure 2: **Our analysis of the OoD accuracy range highlights the significant influence of ViT architectural designs on OoD accuracy. (Sec. 4.1) The numbers within each violin plot for each sub-figure (e.g., IN-D \(9.79\) (\(1.06\)), \(9.65\) (\(2.25\)), and \(7.99\) (\(0.56\))) denote the corresponding OoD (ID) accuracy range of architectures sampled from Autoormer-Tiny/Small/Base search space, respectively. See Appx. G for additional plots and results on other OoD shifts. For a fair comparison, we fix the same range for the x-axis across all sub-figures. We include the ID accuracy range in the top-left sub-figure for reference. On average, the OoD accuracy across all shifts is \(3.8\%/4.86\%/2.74\%\) for the search spaces in our OoD-ViT-NAS benchmark. This range is comparable to and even surpasses the current SOTA method based on domain-invariant representation learning , which achieved a \(1.9\%\) improvement in OoD accuracy under similar settings.**

* _ID Classification Accuracy (ID Acc)_: This metric measures the model performance on In-Distribution (ID) data, typically the data it was trained on (e.g., ImageNet). A higher ID Acc indicates the model's ability to learn training data's distribution.
* _OoD Classification Accuracy (OoD Acc)_: This metric measures the model performance on Out-of-Distribution (OoD) data, which could differ significantly from the training data. A higher OoD Acc indicates a better generalization of the model to handle the OoD shifts.
* For the specific case of ImageNet-O, , we use the Area Under the Precision-Recall Curve (AUPR) metric. A higher AUPR indicates a better generalization of the model to handle the OoD detection.

**Protocol.** Neural Architecture Search (NAS) is notorious for its computationally expensive nature, requiring the training and evaluation of numerous candidate architectures. To address this challenge and efficiently obtain the large number of architectures needed for our benchmark (i.e., \(3,000\)), we make use of the One-Shot NAS approach [58; 20; 59; 12; 6; 7].

In One-shot NAS, a single supernet is first constructed. This supernet contains all possible architectures within the defined search space and is trained only once. Then, during evaluation, various architectures (i.e., subnets) can be efficiently extracted from the supernet. Importantly, these subnets inherit the weights from the pre-trained supernet, and their performance has been shown to be comparable or even superior to that of architectures trained alone [20; 6].

To support a wide range of model sizes, we leverage three supernets: Autoformer-Tiny/Small/Base, which were previously proposed for ID accuracy . We randomly sample \(1,000\) architectures from each supernet, resulting in a total of \(3,000\) architectures in our OoD-ViT-NAS benchmark. Once obtained, these architectures are evaluated on 8 aforementioned OoD datasets.

## 4 Investigation on Out-of-Distribution Generalization of ViT

In this section, we provide the first comprehensive investigation of how ViT architectures affect OoD generalization using our OoD-ViT-NAS benchmark. In Sec. 4.1, we first demonstrate that ViT architectures considerably impact OoD accuracy. In Sec. 4.2, while existing works [2; 3; 6; 7; 8; 9; 12; 18] have made significant strides in improving ViT's ID accuracy, their findings could not be applicable for ViT's OoD generalization due to the not very high correlation between ViT's ID and OoD accuracy. In Sec. 4.3, we conduct the first study to explore NAS for ViT's OoD generalization. Specifically, we study \(9\) Training-free NAS based on their OoD generalization performance on our benchmark. Finally, in Sec. 4.4, we analyze the influence of individual ViT architectural attributes (i.e., embedding dimension, number of heads, MLP ratio, number of layers) on OoD generalization. Additional results of these analysis can be found in Appx. G, H, I, J, L

### ViT architecture designs have a considerable impact on OoD generalization

In this section, we highlight that ViT architectures considerably impact OoD accuracy. This observation encourages future research to put more focus on ViT architecture research for OoD generalization.

 
**Search Space** & **Dataset** & **\#Classes** & **\#Images** & **\#OoD Shifts** & **OoD Shifts** & **Metrics** \\   & ImageNet-1k (NI-1K)  & 1K & 50K & - & - & - \\   & ImageNet-C (N-C)  & 1K & 3.73M & 15 & & \\   & ImageNet-P (N-P)  & 1K & 1M & 9 & & \\   & ImageNet-D (N-D)  & 113 & 4.8K & 3 & & & \\   & Syvizd ImageNet (Sylylized BN)  & 1K & 50K & 1 & & & OoD Acc \\   & ImageNet-B (N-B)  & 200 & 30K & 1 & & & \\   & ImageNet-B (N-Sketch)  & 1K & 50K & 1 & & \\   & ImageNet-A (N-A)  & 200 & 7.5K & 1 & & & \\   & ImageNet-0 (N-A)  & 200 & 2K & 1 & & & \\   &  &  & & & & \\   &  &  & & & & \\ 

Table 1: **An overview of comprehensive setups to construct our OoD-ViT-NAS benchmark. We utilize the widely used ViT NAS search space, Autoformer , which includes three different search spaces Autoformer-Tiny/Small/Base to cover a broad range of model sizes. We randomly sample \(3,000\)**architectures** from these search spaces to populate our benchmark. To ensure comprehensiveness, we evaluate these architectures across 8 of the most common OOTA OoD datasets. Following prior OoD generalization works [57; 30; 31; 15], we employ three metrics for our benchmark: ID Accuracy, OoD Accuracy, and Area Under the Precision-Recall Curve (AUPR).

**Experimental Setups.** To show how ViT architecture designs impact OoD generalization, we compute the range of OoD accuracy for each search space on an OoD dataset. This range reflects the variation in OoD performance for different architectures within a search space. For example, in Fig. 2, each sub-plot represents the range of OoD accuracy for three different search spaces in our OoD-ViT-NAS benchmark on one OoD dataset. We compute the average OoD accuracy range across all datasets as general statistics. For reference, the range of ID accuracy is also included.

**Results.** The results are shown in Fig. 2. Additional results can be found in the Appx.G. The average range of OoD accuracy across the three search spaces in our benchmark is \(3.81\%\)/\(4.86\%\)/\(2.74\%\), which is comparable to or even outperforming state-of-the-art (SOTA) training OoD generalization method, such as those based on domain invariant representation learning [30; 31]. For example, under a similar OoD setting, the current SOTA  shows an improvement of \(1.9\%\) OoD accuracy. This observation highlights the significant influence of ViT architectural designs on OoD accuracy. By carefully designing ViT architecture, the OoD accuracy could improve significantly.

We further explore how the severity of the OoD shift affects the range of ViT's OoD accuracy. We conduct similar experimental setups as before, analyzing \(1,000\) architectures from the Autoformer-Small search space within our OoD-ViT-NAS benchmark for \(1,000\) architectures in Autoformer-Small search space within our benchmark on IN-C. The results are visualized in Fig. 3. We observe that the range of OoD accuracy widens as the severity of the OoD shift increases. This suggests that under stronger OoD shifts, the architecture design becomes even more critical for OoD generalization.

When visualizing OoD accuracy, we observe a bimodal distribution. We figure out that the embedding dimension, as the primary ViT structural attribute, influences this bimodality. For example, among architectures from the Autoformer-Small search space of our benchmark, most architectures with a lower embedding dimension (\(320\)) fall within the lower OoD accuracy mode, while those with higher dimensions (\(384\) and \(448\)) tend to reside in the higher accuracy mode. This observation will be further discussed in detail in Sec. 4.4.

### Can ID accuracy serve as a good indication for OoD accuracy?

While existing works [2; 3; 6; 7; 8; 9; 12; 18] study the impact of ViT architectures to ID accuracy, studies on OoD accuracy are limited. To what extent can we directly apply existing findings of ViT architecture insights for ID to OoD accuracy? To answer this question, we investigate the relationship between ViT ID and ViT OoD accuracy.

Several studies [69; 70; 71; 72] investigates the relationship between ID and OoD accuracy for the CNNs model. However, there is no work on such study particularly for ViT. Utilizing our OoD-ViT-NAS benchmark, we provide the first comprehensive study on the relationship between ViT ID and OoD accuracy. Through our investigation, we find that the correlation between ViT ID and ViT OoD accuracy is not very high. This suggests that architectural insights optimized for ViT ID accuracy, as presented in previous work [2; 3; 6; 7; 8; 9; 12; 18] may not be applicable for ViT OoD generalization.

**Experimental Setup.** Following previous work , we use Kendall's \(\) rank correlation coefficient to compute the correlation between OoD and ID accuracy of all \(1,000\) architectures from a search space on one OoD dataset. Our examination comprehensively computes the correlations across all \(8\) OoD datasets, \(3\) search spaces, and \(3,000\) architectures within our OoD-ViT-NAS benchmark. We compute the average correlations across search spaces and datasets as general statistics.

Figure 3: Visualization of OoD accuracy range across OoD shift severity. We conduct the analysis on \(1,000\) architectures in Autoformer-Small search space within our OoD-NAS-ViT benchmarks. Level \(0\) denotes the clean examples. All corruptions can be found in Fig. G.6, in Appx. G. **We generally observe that the range of OoD accuracy widens as the severity of the OoD shift increases.**Besides the investigation of the correlation of various architectures, we further study the relationship between ViT ID and ViT OoD accuracy for Pareto architectures, representing the top-performing architectures for a certain model size. As shown in Fig. 4-a, the red dots represent Pareto architectures for ID accuracy In this study, we analyze \(1,000\) architectures from the Autofomer-Small search space within our OoD-ViT-NAS benchmark. To identify Pareto architectures for ID accuracy, we divide the total parameter budget into \(30\) equal intervals and select the architecture with the best ID performance within each interval.

**Results.** The correlation results are illustrated in Fig. 1-a. The individual correlations can be found in the Appx. H. We show that the correlation between ID and OoD accuracy is generally not very high. This suggests that current architectural insights based solely on ID accuracy might not effectively translate to OoD generalization.

Among all OoD datasets, the IN-P dataset exhibits the strongest correlation with ID accuracy. This can be attributed to its weaker OoD shift compared to other datasets (see the visualization in Appx. E). As a result, the OoD examples in IN-P are not very different from ID examples, leading to a relatively high correlation between OoD and ID performance. For the remaining seven datasets with stronger OoD shifts, the correlations remain relatively low.

The results of the Pareto architectures analysis are illustrated in Fig. 4. Additional results can be found in the Appx. I. We observe that Pareto architectures for ID accuracy generally perform sub-optimally under the OoD shift. This observation further supports our previous finding that ID accuracy might not be a very good indicator of OoD accuracy.

### Explore Training-free NAS for OoD Generalization

Recently, there has been a new research focus on Training-free NAS, aimed at identifying high-performing architectures without the computational expense of training each candidate. To do so, [10; 11; 17; 62; 60; 73; 74] propose zero-cost proxies to predict the performance of candidate architectures in the initialization or the first training iteration, significantly accelerating NAS. While these works focus on ID accuracy, a few attempts have been made in searching for architectures robust against adversarial attacks [62; 75]. However, there is no work to explore Training-free NAS for ViT for OoD generalization.

**Experimental Setup.** To address this gap, we comprehensively explore the existing \(9\) Training-free NAS for OoD generalization on \(3,000\) ViT architectures within our OoD-ViT-NAS benchmark. Our study includes common and SOTA Training-free NAS originally proposed for CNNs for ID Acc (Grasp , SNIP , MeCo ), ViTs for ID Acc (DSS , AutoProx ), and CNNs for adversarial robustness (Jacobian , CroZe ). We complement this study on Training-free NAS to our OoD-ViT-NAS benchmark to equip the NAS research community with valuable tools to develop more effective Training-free NAS for OoD generalization.

**Results.** Our exploration provides several practical insights for designing a Training-free NAS for ViT for OoD generalization. The results of Kendall \(\) ranking correlation between the OoD accuracy and Training-free NAS proxies on \(8\) common large OoD datasets are illustrated in Tab. 2. The average OoD accuracy is computed across OoD datasets and search spaces. Detailed results can be found in Fig. 1 and Appx. J.

Figure 4: Analysis of OoD Generalization Performance of Pareto Architectures for ID accuracy. Blue dots represent architectures in the search space, while red dots represent the ID Pareto architectures. See Appx. I for additional results. **We find that Pareto architectures for ID accuracy generally perform sub-optimally under OoD shift.**

We observe that existing Training-free NAS are largely ineffective in predicting OoD accuracy. Even recent Training-free NAS designed for ViT (i.e., DSS  and AutoProx-A  ) or Training-free NAS designed adversarial robustness  struggle with predicting OoD accuracy.

Surprisingly, simple zero-cost proxies such as #Param or #Flops outperform all existing, more complex proxies in predicting both OoD accuracy for ViTs. This finding poses a challenge to the Training-free NAS research community: to devise a Training-free NAS that surpasses #Params or #Flops in OoD Acc prediction for ViT.

From Fig. 1-b, we observe that all Training-free NAS methods consistently fail to predict IN-D performance. This is due to the IN-D dataset's unique generation process using Stable Diffusion, which creates images labelled with object names and varying nuisances like background, texture, and material variations. Only the most challenging images are retained, resulting in highly difficult examples, such as distorted images and unrealistic object-background placements (see Fig. E.2). These examples degrade ViT model performance significantly  and cause unpredictable behaviour.

Our investigation into ID accuracy for ViTs also reveals a surprising observation. While proposals for Training-free NAS designed for ViTs (i.e., DSS  and AutoProx-A ), improve the prediction of ID accuracy compared to counterparts designed for CNNs. Our study marks the first attempt to explore simple Training-free NAS like #Param or #Flops. The ID prediction of such simple proxies surprisingly outperforms SOTA Training-free NAS designed for predicting ID accuracy for ViT.

ViT Structural Attributes on OoD Generalization: Increasing Embedding Dimension is Generally Helpful

Our OoD-ViT-NAS benchmark with \(3,000\) ViT architectures covers diverse design choices in ViT structural attributes, including embedding dimension (Embed_Dim), network depth, number of heads (#Heads), and MLP ratio (MLP_Ratio), which allows for finding a wide range of ViT with different structures and complexities. Utilizing our comprehensive benchmark, we are the first to provide an analysis of the impact of these ViT structural attributes. We investigate which structural attributes in ViTs could lead to better OoD generalization. _Through our analysis, we find that increasing the embedding dimension of a ViT architecture can generally improve OoD generalization._ The additional analysis on another search space  further confirms our finding. The details for this additional analysis can be found in Appx.

Experimental Setup.To verify the effectiveness of ViT architectural attributes on our OoD generalization benchmark, we present the results from two perspectives: (1) an analysis of rank correlation for our OoD-ViT-NAS benchmark and (2) a comparison of OoD accuracy across different embedding dimensions. To gain insights into the relationship between embedding dimension (Embed_Dim) and OoD performance, we created the visualizations for all architectures in our OoD-ViT-NAS benchmark. These visualizations compare the average OoD accuracies across different ViT architectures with varying embedding dimensions and depths. Examples of these visualizations are shown in Fig. 5. Additional results can be found in the App.K.

Results.In Fig. 1-c, we find that the embedding dimension generally has the highest correlation with OoD accuracy among all ViT architectural attributes. This positive correlation indicates that _Em

    &  &  &  \\    & **Performance** & & & \\  Grasp  & ID Acc & CNNs & 0.1409 \(\) 0.1951 & 0.1207 \(\) 0.1575 \\ SNIP  & ID Acc & CNNs & 0.3750 \(\) 0.3023 & 0.2889 \(\) 0.2274 \\ McC  & ID Acc & CNNs & 0.1440 \(\) 0.2371 & 0.0975 \(\) 0.0819 \\ Croz\(\&\) & Adv Robustness & CNNs & 0.3823 \(\) 0.3046 & 0.2951 \(\) 0.2223 \\ Jacobian  & Adv Robustness & CNNs & 0.1053 \(\) 0.1509 & 0.0841 \(\) 0.1232 \\ DSS  & ID Acc & ViTs & 0.4165 \(\) 0.3461 & 0.3421 \(\) 0.2365 \\ AutoProx-A  & ID Acc & ViTs & 0.4023 \(\) 0.3827 & 0.3303 \(\) 0.2384 \\ 
**Param** & - & - & 0.4607 \(\) 0.3318 & 0.3600 \(\) 0.2321 \\ 
**\#Flops** & - & - & **0.4705 \(\) 0.3391** & **0.3537 \(\) 0.2327** \\   

Table 2: Comparison of Kendall \(\) ranking correlation between the OoD accuracies and the Training-free NAS proxies values on 8 common large OoD datasets using our OoD-ViT-NAS benchmark. **Bold** and underline stand for the best and second, respectively. We show that existing Training-free NAS’s predictability in ViT OoD accuracy is limited, E.g., the very recently proposed Auto-Prox only achieves \(0.3303\) correlation. Furthermore, we make the first observation that simple proxies like #Param or #Flops outperform other more complex proxies in predicting both ViT OoD/ID accuracy.

bed_Dim could play a crucial role in achieving OoD generalization performance._ Our comprehensive OoD-ViT-NAS benchmark sheds light on a previously unknown relationship: the potential impact of embedding dimension (Embed_Dim) on OoD generalization in ViTs. This trend holds across most OoD shifts in our benchmark (Fig. 5), suggesting that among other architectural attributes, the design choice of Embed_Dim might significantly influence a model's OoD generalization.

Our experiments yield several intriguing phenomena. Based on Fig. 1-c, we observe that _network depth has a slight impact on overall OoD generalization performance (correlation: \(0.19\))_. Also, as shown in Fig. 5, for a given embedding dimension (represented by a distinct colour), we report the mean OoD accuracy, showing how the mean OoD accuracy changes among ViT architectures of varying depths, which aligns with our empirical insight. Fig. 5 shows that while increasing depth can be beneficial for improving ViT's OoD generalization in some cases, there exist shallower models that tend to perform better in terms of OoD accuracy compared to those with deeper models.

It is evident from Fig. 1-c, where both the MLP ratio (0.09) and the number of heads (\(0.07\)) exhibit very low correlation values with overall OoD performance. These findings highlight that increasing the MLP ratio and the number of heads may not substantially enhance a model's robustness to OoD data. Due to space constraints, we defer additional experiments to the Appx. L, showing that the network depth, MLP ratio, and #Heads might have non-obvious impacts on OoD generalization.

**Increasing Embedding Dimension help ViT learn more high-frequency patterns, leading to improve OOD generalization.** In this section, we design a frequency study to understand our finding: why increasing ViT Embedding Dimension can generally improve ViT's OOD generalization. In the literature, the models obtain higher performance on preserving High-Frequency-Component (HFC) samples tend to learn more HFC [77; 78]. By learning more HFC, the models improve OOD generalization [77; 79]. Our hypothesis is that Increasing embedding dimension helps ViTs learn more HFC resulting in improving OOD generalization. We adapt the experiment from  to verify our hypothesis. The details on this experimental setup can be found in the Appx. D. In a nutshell, we filter HFC by hyper-parameter radius r, where the higher the r, the lesser HFC. As shown in Fig. 6, we observe that when increasing Embedding Dimension, the performances obtained on filtering-HFC samples are improved. This observation holds true across setups varying radius r, supporting that increasing Embedding Dimension helps ViT learn more HFC. In contrast, increasing other ViT structural attributes does not help improve ViT learn more HFC.

**Robust ViT architectures designed by our finding.** Our study provides significant insights for guiding the design of ViT architectures. Specifically, among ViT structural attributes, increasing embedding dimension can generally improve OoD generalisation of ViT architectures. Our insight leads to a simple method which can achieve ViT architectures that can outperform well-established human-designed. We demonstrate the superiority of ViT based on our insights in Tab. 3. Scaling up ViT architecture (e.g., from ViT-B-32 to ViT-L-32) by humans typically involves compound scaling of various ViT structural attributes. However, our findings suggest that not all ViT structural attributes need to be increased to benefit OoD generalisation. Among these attributes, increasing the embedding dimension is the most crucial factor for improving OoD generalisation. By only increasing the

Figure 5: The effect of #Embed_Dim on robustness generalization of ViTs. The numbers denote the mean OoD accuracy across ViT architectures with specific colour-coded embedding dimensions and depths. The data points with blue \(\), orange \(\), and green \(\) colours represent ViT architectures with an embedding dimension of \(320\), \(384\), and \(448\), respectively. **Generally, a higher OoD accuracy is obtained when the embedding dimension of ViT architectures increases for most OoD shifts.** See Fig. 15 and 16 in Appx. K for additional plots and results on other OoD shifts.

embedding dimension, ours ViT architectures (e.g., Increasing embedding dimension of ViT-B-32) are significantly more efficient and outperform compound scaling architectures (e.g., ViT-L-32).

## 5 Conclusion

In this work, we introduce OoD-ViT-NAS, the first comprehensive benchmark for NAS on OoD generalization of ViT architectures. Using this benchmark, we conduct a comprehensive investigation on OoD generalization for ViT. Firstly, we show that ViT architecture design significantly impacts OoD accuracy. Secondly, we show that the architectural findings from existing works for ID performance could not apply to OoD generalization due to the low correlation between ID and OoD accuracy. Thirdly, we conduct the first study of NAS for ViT's OoD generalization and show that existing Training-free NAS methods struggle with OoD prediction. Surprisingly, simple proxies like #Param or #Flops outperform other complex Training-free NAS. Finally, we conduct the first study on the impact of ViT architectural attributes on OoD generalization. Our study reveals that increasing a ViT architecture's embedding dimensions can generally improve OoD generalization. We believe our benchmark OoD-ViT-NAS and comprehensive analysis will catalyze and streamline future research on understanding how ViT architecture design influences OoD generalization.

**Acknowledgement.** This research is supported by the National Research Foundation, Singapore under its AI Singapore Programmes (AISG Award No.: AISG2-TC-2022-007); The Agency for Science, Technology and Research (A*STAR) under its MTC Programmatic Funds (Grant No. M23L7b0021). This research is supported by the National Research Foundation, Singapore and Infocomm Media Development Authority under its Trust Tech Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore and Infocomm Media Development Authority.

   &  **Embed** \\ **Dim** \\  & **Depth** & **\#Head** &  **MLP** \\ **Ratio** \\  &  **Latency** \\ **(ms)** \\  &  **\#Param** \\ **(ms)** \\  & 
 **N-R** \\ **OoD Acc** \\  & \(\)**wt Latency** \(\) & \(\)**wt \#Param**\(\) \\  ViT-B-32 & 768 & 12 & 12 & 4.0 & 105.00 & 87.53 & 41.58 & - & - \\ Ours & 840 & 12 & 12 & 4.0 & 113.32 & 98.64 & **48.28** & **0.8054** & **0.6032** \\ ViT-L-32  & 1024 & 16 & 24 & 4.0 & 258.83 & 305.61 & 44.33 & 0.0179 & 0.0126 \\  Swin-T & 96 & 12 & 32 & 4.0 & 100.31 & 28.29 & 46.22 & - & - \\ Ours & 128 & 12 & 32 & 4.0 & 165.80 & 49.91 & **48.28** & **0.0315** & **0.0954** \\ Swin-S  & 96 & 24 & 32 & 4.0 & 184.48 & 49.60 & 47.77 & 0.0184 & 0.0725 \\  

Table 3: Comparison of ViT architectures designed based on our insights (**Embedding Dimension**) and well-established ViT designed by humans in .

Figure 6: Following setting in , the ViTs which were trained on original ID data, are now tested on high frequency components (HFC) of OoD samples, with r as the radius for frequency filtering. The higher the OoD accuracy, the more HFC learned in the model.