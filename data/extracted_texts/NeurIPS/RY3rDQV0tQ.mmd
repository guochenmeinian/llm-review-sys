# Optical Diffusion Models for Image Generation

Ilker Oguz\({}^{1}\) Niyazi Ulas Dinc\({}^{1}\) Mustafa Yildirim\({}^{1}\) Junjie Ke\({}^{2}\) Innfarin Yoo\({}^{2}\)

Qifei Wang\({}^{3}\) Feng Yang\({}^{2}\) Christophe Moser\({}^{1}\) Demetri Psaltis\({}^{1}\)

\({}^{1}\) Ecole Polytechnique Federale de Lausanne \({}^{2}\) Google Research \({}^{3}\) Google

{ilker.oguz,niyazi.dinc,mustafa.yildirim,christophe.moser,demetri.psaltis}@epfl.ch

{junjiek,innfarin,qfwang,fengyang}@google.com

Equal advising.

###### Abstract

Diffusion models generate new samples by progressively decreasing the noise from the initially provided random distribution. This inference procedure generally utilizes a trained neural network numerous times to obtain the final output, creating significant latency and energy consumption on digital electronic hardware such as GPUs. In this study, we demonstrate that the propagation of a light beam through a semi-transparent medium can be programmed to implement a denoising diffusion model on image samples. This framework projects noisy image patterns through passive diffractive optical layers, which collectively only transmit the predicted noise term in the image. The optical transparent layers, which are trained with an online training approach, backpropagating the error to the analytical model of the system, are passive and kept the same across different steps of denoising. Hence this method enables high-speed image generation with minimal power consumption, benefiting from the bandwidth and energy efficiency of optical information processing.

## 1 Introduction

Diffusion models create new samples that resemble their training sets by gradually undoing the diffusion process, which requires the learned reverse process to be applied numerous times . While this method demonstrated unprecedented capabilities by producing highly realistic samples , it is also highly time-consuming and expensive in terms of energy consumption and computing resources since a large number of steps are required for generating each sample . This prolonged processing time not only limits accessibility but also contributes to a significant environmental footprint.

Currently, generating new samples with diffusion models relies on electronic, general-purpose computing hardware such as GPUs or TPUs. However, due to the repetitive nature of the reversal process required in this task, deploying specialized hardware instead of general-purpose ones could significantly enhance the efficiency of sampling. For instance, the use of ASICs in cryptocurrency mining for hashing algorithms has demonstrated substantial improvements in computational speed and energy efficiency . However, both GPUs and ASICs, among other electronic digital computers, face the same challenges like heat dissipation, energy consumption, and the diminishing returns of Moore's Law, as transistors shrink and encounter quantum effects and physical limits that hinder further gains . Therefore, exploring alternative computing modalities, such as optical computing--which offers high bandwidth and low loss--is increasingly important . Optical computing has already shown promise in various applications, including high-speed data transmission and real-time signal processing. Optical computing addresses the inefficiencies of electronic hardware by leveraging the inherent parallelism that light allows for the simultaneous processing of multiple data channels, significantly speeding up the computational process. Several optical neural networks have beenreported to perform complex calculations at reduced latency and energy consumption compared to traditional electronic systems .

In this study, we demonstrate that optical wave propagation can be programmed to act as a computing engine specifically designed for implementing denoising diffusion models. As light passes through specially engineered transparent layers, features related to the original distribution are filtered out without any additional power consumption or computing latency, as depicted in Figure 1. This is due to the passive nature of the transparent layers, which are minimally absorptive and do not require active components or external power to function. These layers are designed to manipulate the light solely through their physical structure, which allows for the noise prediction to exit the system efficiently. Near zero energy consumption of passive optical components reduces the overall power requirements, making the system more energy-efficient.

Through iterative noise prediction and removal, the Optical Denoising Unit (ODU) can generate new images using a minimal number of these passive optical modulation layers. Since these layers do not need power or active control, they do not introduce any latency or energy overhead. Constrained only by optoelectronic input and readout hardware, this approach has the potential to significantly reduce the computational time and energy consumption of diffusion models, specifically performing inference in more sustainable and scalable ways.

The main contributions of this study are:

* The propagation of light through multiple modulation layers is programmed to perform denoising diffusion image generation by predicting and transmitting the noise term in the input images. The system uses only a single modulation plane and multiple reflections.
* A time-aware denoising policy is specifically designed for analog optical computing hardware. This policy facilitates the use of passive building blocks to achieve multi-step computing at low power, translating the time-embedding in digital Denoising Diffusion Probabilistic Models (DDPMs) into optical hardware.
* An online learning algorithm is introduced for training ODUs in real-life scenarios, where alignment and calibration errors exist. The algorithm tracks and alleviates experimental discrepancies with constant updates to a digital twin (DT) during training time.

## 2 Related Work

Diffusion models have become popular, with their superior image generation performance compared to Generative Adversarial Networks (GANs) . High-resolution, guided diffusion process is

Figure 1: Comparison between conventional and proposed methods of image generation based on diffusion models. The conventional method runs on digital electronics based computing units such as GPUs or TPUs. The proposed method utilizes an optical denoising unit that is formed by passive optical layers. The image to be denoised is sent to the system with a modulator and the output is read out with a detector.

currently widely utilized for on-demand image generation [6; 7; 14]. One of the main concerns with these highly capable models is the significant time taken for generating new samples, which can exceed 10 seconds for each high-resolution image . Different methods have been proposed to alleviate this condition and improve the efficiency of diffusion models. While Latent Diffusion Models  work on a lower dimensional representation of images to decrease the computational load, Denoising Diffusion Implicit Models  introduce a deterministic and non-Markovian sampling process to reduce the number of required steps. Similarly, FastDPM , uses domain-specific conditional information for faster sampling with diffusion models. Another approach is to distill multiple denoising steps to a single one with a teacher-student setting . As these methods aim to decrease iterative denoising steps required for sampling through algorithmic innovations, the potential improvements obtained by exploiting the repetitive nature of these models on the computing hardware side remain to be seen.

Optical processors have shown substantial energy efficiency improvements, particularly with larger model sizes, potentially outperforming current digital systems . They can be implemented through various architectures, each leveraging different aspects of optical technology to perform computations. Free-space optical networks use spatial light modulators or fixed modulation layers for performing matrix multiplications and convolutions as light propagates [19; 20; 21], making them highly efficient for image processing tasks. These applications include super-resolution , noise removal , and implementation of convolutional neural network layers , which are shown to competitively perform different downstream tasks such as segmentation . On the other hand, photonic integrated circuits utilize optical components like Mach-Zehnder interferometers, microring resonators, and waveguides on a single chip, enabling compact vector-matrix multipliers [26; 27]. Together, these developments highlight the transformative potential of optical computing in enhancing the performance and efficiency of computationally intensive tasks.

Considering the significant computational demands of denoising tasks, there is a clear need for specialized hardware to scale these operations effectively. Despite the advancements in optics, deep learning, and optical image processing, the realization of an optical diffusion denoiser remains a gap in current research. Bridging this gap could leverage the synergy between these fields to develop highly efficient and scalable solutions for denoising diffusion models.

In addition to their wide range of advantages, optical computing systems also have disadvantages related to the energy cost of modulation and detection of light, its limited programmability, and experimental precision. Calculating the gradients of the experimental loss through the optical wave propagation model allows for a close match between optical experiments and computational models . Similarly, a pre-trained neural network-based emulator of a physical system can also be used for the same purpose . Moreover, it is crucial to perform as many computations as possible with the data while in the optical domain to avoid energy and time expenditure of optoelectronic devices.

## 3 Description of the Study

### Denoising Diffusion Models

DDPMs progressively corrupt data with Gaussian noise in a forward process and subsequently learn to reverse this corruption through a denoising process. This way they can generate new data samples that closely resemble the training data distribution. The forward diffusion process involves the sequential corruption of a data sample \(r_{0} q(r_{0})\) through the addition of Gaussian noise over \(T\) timesteps. At each timestep \(t\), the data sample \(r_{t-1}\) is perturbed to produce \(r_{t}\), \(r_{t}=}r_{t-1}+}_{t}\), where \(_{t}(0,1)\) is a variance schedule that determines the amount of noise added and \(_{t}(0,)\) is standard Gaussian noise. This process transforms the original data into nearly pure noise by timestep \(T\).

The reverse denoising process in DDPMs aims to reconstruct the original data from a highly noisy sample. Starting from completely Gaussian noise \(r_{T}(0,)\), the sample is iteratively denoised by removing the prediction of \(_{t}\) in the image, \(_{}(r_{t},t)\), which is provided by a trained neural network:

\[r_{t-1}=}}(r_{t}-_{t}_{}(r_{t},t )),\] (1)

The training objective of the neural network can be simplified to minimize the mean squared error (MSE) between the true noise \(_{t}\) and the predicted noise \(_{}(r_{t},t),=_{t,r_{0},_{t}} [\|_{t}-_{}(r_{t},t)\|^{2}]\)where \(t\) is uniformly sampled from \(\{1,,T\}\). Finally, to generate new data samples, the model starts with a sample \(r_{T}(0,)\) and applies the learned reverse transitions iteratively.

### Propagation of Modulated Light Beams

In this study, a denoising framework is presented by combining the modulation of a light beam with consequent transparent or reflective patterns and its propagation in free space (environments such as vacuum or air, where the refractive index of light is approximately 1), as shown in Figure 2. This process can be explained by the Fresnel diffraction theory since the features on the layers are not only larger than the optical wavelengths but only sufficiently smaller than the distance between different modulation layers . According to this formalism, the electromagnetic field after propagating a distance \(z\) in free space, \(U(x,y,z)\), can be calculated from its distribution at \(z=0\) by convolution with "the impulse response of free space", \(h(x,y)\):

\[U(x,y,z)=U(x,y,0)*h(x,y),h(x,y)=}{j z} [(x^{2}+y^{2})]\] (2)

Here, \(k\) denotes the wavenumber of the field, and \(\) is the wavelength. In other words, the field's value at the plane of \(z=z_{0}\), at a given location \((x,y)\), is the weighted sum of the values at \(z=0\), where the weight of each location is determined by the response function. Being complex numbers, all of these weights have the same magnitude but their phase depends on the location. In the frequency domain, the transfer function of free space becomes

\[H(f_{X},f_{Y})=[j2)^{2}-( f_{Y})^{2}}].\] (3)

This indicates that for spatial frequencies larger than \(1/\), the magnitude of the transfer function decays to zero exponentially. Hence, only features that are larger than the wavelength of the light can propagate to the far field. Moreover, frequency domain expression of diffraction, Eqn. 3, allows for also the efficient digital simulation of the propagation of light in free space with the utilization of Fast Fourier Transforms (FFTs) in a parallelized manner. Later on, we will benefit from this fact for GPU accelerated training of the diffractive modulation layers.

The proposed method applies trainable weights to the light beam at consequent planes with thin modulation layers. The interaction between layers and light can be represented as a point-wise multiplication between the incident field and the layer, which is followed by the propagation of the field in free space until the next layer,

Figure 2: The main operation principle of ODU. Consequent modulation and free space propagation events can be represented with multiplication and convolution operations. When the input beam \(U_{0}(x,y)\), which is patterned with noisy input images, \(r_{t}\), is introduced to the ODU, the output intensity pattern \(\|U_{f}(x,y)\|^{2}\) corresponds to the trained optical systemâ€™s prediction of the noise component in the input pattern, \(_{}(r_{t})\).

\[U_{n}(x,y)=[U_{n-1}(x,y)*h(x,y)]L_{n}(x,y),\] (4)

where \(U_{n}(x,y)\) is the field distribution right before reaching the modulation layer \(n\), and \(L_{n}(x,y)\) is the complex modulation coefficient distribution of the trained modulation layers or the weights of the optical diffusion model. \(L(x,y)=|L(x,y)|e^{i(x,y)}\) can be only a real number(\((x,y)=0\)), just phase modulation (\(|L(x,y)|=1\)) or an arbitrary complex number depending on the implemented modulation principle. In this paper, we demonstrate our approach with a phase-only liquid crystal spatial light modulator (SLM), which can set \((x,y)\) to any value in the range of \([0,2]\) electronically, and \(|L(x,y)| 1\) everywhere.

### Training of Optical Modulation Layers

As described in section 3.2, propagation of light can be analytically explained in a succinct manner for the scale considered in this study (\(z_{layer}>>d_{pixel}>\)). This allows for defining some free variables in this representation, such as refractive index distribution \(L(x,y)\) or input wavefront distribution \(U_{0}(x,y)\), and optimizing these variables for minimizing a cost function. The gradients of these variables can be found with either manual calculation  or automatic differentiation packages . In this study, our first goal is to find optimal modulation layers, or refractive index distributions, such that after the light beam encoded with the noisy images propagates through them only the predicted noise term reaches the detector, as shown in Figure 2. Moreover, the denoising network should be aware of the given timestep in the diffusion process while predicting the noise, \(_{}(x_{t},t)\), so that it would have _a priori_ information about the variance of the noise term. Since noise level awareness is a crucial aspect of successful sample generation, most of the current implementations of diffusion models utilize time-embedding layers to modify activations of the neural network across different layers depending on the diffusion time step. Instead, the proposed method divides the diffusion timeline consisting of \(T\) timesteps into \(M\) subsets, and for each subset of time frames \(\{S_{m}\}_{m=1}^{M}\), trains a separate set of modulation layers \(\{L_{n}^{m}\}_{n=1}^{N}\) each containing \(N\) layers. Then, for \(t S_{m}\), the noise prediction,\(_{_{m}}(x)\) becomes only a function of \(x\). In this scheme, the training objective for each time step is

\[_{t}=_{x_{0},}[\|- _{_{m}}(x_{t})\|^{2}]\] (5)

where total loss is the sum over all ranges:

\[=_{i=m}^{M}_{t S_{m}}_{t}.\] (6)

This decoupling of denoising at different timesteps by removing time-embedding layers also eliminates the necessity for digital computations to modifications at different layers. By circumventing this problem we perform denoising all-optically. Moreover, a fixed optical modulation pattern performs denoising at multiple consequent timesteps. For instance, we later demonstrate that for \(T=1000,M=10\) creates optimal results. So, a single layer set can process 100 timesteps and the entire sampling workflow can be operated with only 10 fixed parallel devices, or with only 10 updates to the SLM.

After defining the forward calculation of the system with the analytical explanation of light propagation and the loss function as the mean square error of noise prediction (Eqn. 6), the trainable parameters of the system \(\{L_{n}^{m}\}_{n=1}^{N}(x,y)\) are optimized by automatic differentiation .

## 4 Results

### All-Optical Denoising based Image Generation

Following the same experimental settings with the initial DDPM study , we set \(T=1000\) and \(\) values to be in the linear range between \(_{1}=10^{-4}\) to \(_{T}=0.02\). The results in Figure 3 are reported with the beam propagation model (Eqn. 3) of the optical system designed to have \(300 300\) pixels per layer and four modulation layers. The number of layer sets (\(M\)) is 10. Several intermediate results alongside final outputs at \(T=1000\) are reported in Figure 3 for 3 classes of the MNIST digits , Fashion-MNIST  and the clock category of the Quick, Draw! datasets . Furthermore, the evaluation of image generation quality metrics, Inception Score (IS) and Frechet Inception Distance (FID), which are detailed in Appendix A.6, across different generation timesteps captures the improved realism of images with the optical diffusion procedure.

### Effects of Optical Model's Dimensionality on the Image Denoising and Generation Performance

This section provides further analysis with different output dimensions and parameter counts along with the comparisons with purely digital implementations to quantify Optical Diffusion's scalability to

Figure 4: Scaling of the denoising capabilities (left) and generation performance (right) of Optical Diffusion, and pure digital convolutional U-Net and fully connected networks with the output image resolution.

Figure 3: Images generated by the Optical Diffusion Model at different timesteps and when trained with various datasets. The generated images and their corresponding Inception and FID scores are calculated between timesteps \(T=10\) to \(T=950\) are acquired after training with the MNIST digits dataset. Final outputs at time \(T=1000\), acquired from ODUs trained for the MNIST digits samples have FID = \(206.6\), for Fashion MNIST, FID = \(227.7\) and for Quick, Draw!, FID = \(131.4\)

large-scale diffusion problems. The first investigation is into the performance with higher resolution datasets; two digital architectures of similar performance with the ODU, one being fully connected and the other convolutional U-Net , are trained for the same tasks with the ODU, generating images with the MNIST digits dataset at different resolutions. Their architecture is detailed in Appendix Table 1. The results shown in Figure 4 indicate that ODU consistently outperforms the two digital neural networks, and all three scale in a similar manner both in terms of denoising and generation performances when the generated image dimension is increased while the model sizes are kept constant.

Secondly, the scaling of Optical Diffusion's performance with respect to the number of total parameters is probed through three hyperparameters: layer resolution, layer count, and timestep sets. The effects are tracked with different metrics, MSE for denoising, FID, IS and Kernel Inception Distance (KID) for the generation quality. In Figure 5, we observe that, as in digital neural networks, there is a clear tendency to perform better with a larger number of trainable parameters when layer resolution and layer count are increased.

On the other hand, having a larger number of denoising layer/timestep sets improves the results only until they reach a certain level. Afterward, increasing the number of sets is detrimental as shown in Figure 5. As the total number of training steps is fixed in this experiment, increasing the number of timestep sets decreases the training sample count per layer set, hence potentially deteriorating the performance after a particular threshold, which is found to be \(M=10\).

Through the aggregation of data points acquired with different layer resolutions and counts in Figure 5, the relationship between the total trainable parameter count of ODUs and their image generation performance is depicted in Figure 6. This relationship remarkably follows the same widely accepted, power-law trend of digital generative models . Most significantly, when the optical implementation is fitted to a power-law equation, the exponential of the power law (\(-0.15\)) is approximately the same as the reported value (\(-0.16\)) for large-scale image generation networks in . This fit parameter gives the slope of the line in the logarithmic plot, indicating how fast the generation performance scales with the number of parameters, in this case showing that ODU improves its performance at a similar speed with large-scale digital image generation networks while its parameter count is increased. The single outlier in this trend is the case where there is only a single modulation layer, which does not benefit from the multiple optical modulations aspect of the proposed architecture.

Figure 5: The dependency of denoising performance (MSE) and generation quality scores(FID, KID and Inception score), on the hyperparameters of the ODUs (number of pixels of optical modulation layers, number of modulation layers and number of denoising layer sets (\(M\))).

### Higher Experimental Fidelity with the Online Learning Algorithm

To address the challenge of training an optical system with imperfect calibration, as faced in many other analog computing paradigms, we propose an online learning algorithm that updates and leverages a DT during training. During inference, the DT does not incur any additional overhead. The DT (\(_{_{},_{}}\)) again utilizes Fresnel diffraction based model of light propagation, as a surrogate to compute gradients and guide the optimization of the system's trainable parameters. However, matching the DT's parameters (\(_{}\)), for instance, input beam angle, precise locations of the layers, and their angles, perfectly to the experimental conditions of the physical system is a challenging task. Therefore, during each iteration of training, the output of the experiment (\(f_{_{}}\)) and DT (\(_{_{},_{}}\)) is compared and the DT's parameters are updated accordingly, as shown in Algorithm 1.

```  Initialize physical system \(f_{_{}}\) with parameters \(_{}\)  Initialize DT \(_{_{},_{}}\) with parameters \(_{}\), \(_{}\) while not converged do  Forward Pass:  Input data \(\) into the physical system \(f_{_{}}\)  Obtain physical system output \(_{f}=f_{_{}}()\)  Compute error \(E=(_{f},_{})\) Backward Pass:  Compute Jacobian of DT at \(,=_{f}}{_{}}\)  Compute gradients \(_{_{}}=^{T}_{f}}\)  Update physical system parameters \(_{}_{}-_{_{ }}\) DT Refinement:  Obtain DT output \(_{}\)  Compute MSE between DT and physical system outputs \(L=(_{},_{f})\)  Compute gradients \(_{_{}}=}}\)  Update DT alignment parameters \(_{}_{}-_{ _{}}\) endwhile ```

**Algorithm 1** Online Learning Algorithm

In parallel, the DT is also employed to compute the gradients of the trainable parameters of the experiment, with respect to the output(\(_{f}}{_{}}\)). These gradients are then utilized to update the physical system's parameters through backpropagation, informed by the error obtained from the physical system. Concurrently, the DT is refined using the latest inputs and outputs from the physical system to better approximate its behavior, despite the initial parameter mismatches. This iterative process of

Figure 6: The relationship between the total number of parameters in an ODU and its generation performance in terms of FID scores.

forward and backward passes, coupled with the continuous refinement of the DT, enables the physical system to progressively improve its performance and align more closely with the desired outcomes.

In Figure 7, we explore the efficiency of the proposed algorithm by modeling a possible experiment scenario. In this scenario, we define two different optical models, while actually both of them are simulations of the optical wave propagation for an exact insight into the algorithm, we designate the first one as the "optical experiment" by configuring it with the calibration angles obtained from the physical experiment. These four angles account for the slight misalignment of the experiment and define the input angle of the beam to the cavity and the angle between the mirror and the SLM, in x and y axes, all being in the range of a few milliradians and their measurement details being provided in Appendix A.1. The second model, considered as the DT, is initialized with their calibration angles \(20\%\) higher. We used 3 different algorithms, offline, experimental error backpropagation , and the proposed online training schemes. During training, MSE (training loss), and the discrepancy between the DT and the experiment's outputs are tracked. The discrepancy, \(D\), is inversely related to cross-correlation, \(C\), of the experimental and the DT's normalized outputs, \(O_{exp}\) and \(O_{dt}\) respectively, \(D=1-C\), where \(C=_{x}_{y}O_{}(x,y)O_{}(x,y)\).

Offline training improves the loss function when evaluated with the DT, but when evaluated in the experimental setting, it has a higher MSE, as shown with a star in Figure 7. When the online training method is used, the DT is aligned with the actual experiment swiftly and the experimental loss approximates the MSE of perfect calibration case, the results of this approach with the optical experiment are also provided in Appendix Figure 10. Backpropagating the experimental loss, MSE does not decrease significantly. However, for smaller misalignment, this method was also demonstrated to converge. This experiment implements multiple modulation layers on a single device,

Figure 7: The upper block illustrates the online training scheme. The forward pass is calculated with the experiment (blue), while the gradients of the prediction error are backpropagated using a DT of the experiment (green) and updating the physical trainable parameters. The difference between the outputs of the experimental setup and the DT continuously refines the DT (red). The lower graph block compares offline and online training methods. Offline training relies on a pre-trained DT for both the forward and backward passes, with the experimental performance of this method indicated by the star. Experimental backpropagation executes the physical forward pass but does not incorporate DT refinement.

with a single phase-only SLM and a mirror in parallel to resource-efficiently prototype the proposed computing method, as shown in Figure 7 and detailed in Appendix A.3.

## 5 Conclusion

In this study, we introduced an optical diffusion denoising framework for image generation, utilizing a time-aware denoising strategy that enables optical low-power realization. By exploiting light propagation through transparent media, ODU effectively reduces noise in images, with a much smaller energy budget compared to electronics since optical wave propagation has a very small intrinsic loss while acquiring comparable, or better quality. This is especially interesting because diffusion models are currently one of the most costly generative artificial intelligence models due to their repetitive denoising process, with a correspondingly large environmental impact .

The integration of a time-aware policy enables Optical Diffusion to adjust light modulation dynamically according to different stages of the denoising process, thus improving image quality with a minimal number of changes to the modulation layers or parallel optical processing units. Looking ahead, the incorporation of larger modulation layers with more parameters and the exploration of nonlinear optical effects could enhance the functionality of the system. Scaling analyses also show evidence for the ODU to improve its performance at the same rate as digital models while increasing its size. These potential improvements suggest promising directions for future research in expanding the range of applications for this technique.

The proposed method can utilize off-the-shelf consumer electronics such as digital micromirror devices that can be found in portable projectors for input modulation and CMOS cameras for recording output prediction. The online learning algorithm accounts for variations between these devices and closes the gap between the analytical and experimental realization of the ODU. On the other hand, as analyzed in detail in Appendix A.4, these devices have the potential of implementing denoising steps on the order of microsecond latencies while consuming a few Watts only. With the utilization of high-speed light modulation technologies, million-frames-per-second-level denoising can be achieved again with Watt level energy consumption, while still utilizing the proposed approach for predicting noise in provided images .