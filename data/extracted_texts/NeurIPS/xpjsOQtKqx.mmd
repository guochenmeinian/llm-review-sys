# StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners

Yonglong Tian\({}^{1,*}\)  Lijie Fan\({}^{1,2,*}\)  Phillip Isola\({}^{2}\)  Huiwen Chang\({}^{1}\)  Dilip Krishnan\({}^{1}\)

\({}^{1}\)Google Research, \({}^{2}\)MIT CSAIL, \({}^{*}\)equal contribution

Code: https://github.com/google-research/syn-rep-learn

###### Abstract

We investigate the potential of learning visual representations using synthetic images generated by text-to-image models. This is a natural question in the light of the excellent performance of such models in generating high-quality images. We consider specifically the _Stable Diffusion_, one of the leading open source text-to-image models. We show that (1) when the generative model is configured with proper classifier-free guidance scale, training self-supervised methods on synthetic images can match or beat the real image counterpart; (2) by treating the multiple images generated from the same text prompt as positives for each other, we develop a multi-positive contrastive learning method, which we call StableRep. With _solely synthetic_ images, the representations learned by StableRep surpass the performance of representations learned by SimCLR and CLIP using the same set of text prompts and corresponding _real_ images, on large scale datasets. When we further add language supervision, StableRep trained with 20M _synthetic_ images achieves better accuracy than CLIP trained with 50M _real_ images.

## 1 Introduction

_Data_ has assumed a paramount role as the key component for the success of modern machine learning systems. Such systems, especially foundation models in various domains, heavily rely on vast and diverse datasets to acquire knowledge, make accurate predictions, and generate content. The quality, quantity, and diversity of the data significantly impacts the performance and effectiveness of these

Figure 1: **Left:** traditional visual representation learning relies on a dataset of real images to train an image embedding function. **Right:** we view generative models as datasets that allow us to sample images from the data distribution. In our study, we leverage text-to-image models (Stable Diffusion ) and treat multiple images synthesized from the same prompt as positives for contrastive representation learning.

models, as they learn from the collective information encapsulated within the data. In this data-centric era, a central question is: how can we collect such large amounts of varied data to train AI models?

As an example, suppose we are trying to solve a new computer vision problem, and need to collect data (images) for it. An ideal situation is to place a camera anywhere in the wold and capture whatever we need. But in reality, collecting data is historically not easy. In the 1990s, researchers needed to take photos by themselves to create datasets for objects  and faces [68; 24]. To collect data in the 2000s, people crawled the Internet . Noisy, uncurated data collected in such a manner can exhibit domain gaps with the real world problem and reflect imbalances due to societal bias. Removing or reducing such imperfection in data of high volume by human labeling is costly and can be prohibitive.

However, what if data collection could be simplified to the utterance of a natural language command, specifying what you want? What if, for hardly any cost, you could take a photo every few milliseconds? This sounds fanciful, but modern text-to-image generative models are approaching this vision. It has long been a dream that someday we could use these as our data sources, rather than taking photos [75; 30; 35]. In this paper, we study if this is now a practical option in the context of large scale visual representation learning.

To achieve this, we choose to work with Stable Diffusion , one of the leading open source text-to-image models. We synthesize images by prompting Stable Diffusion with text from large scale image-text datasets, such as CC12M  and RedCaps . Surprisingly, our investigation reveals that when the classifier-free guidance scale is properly configured for Stable Diffusion, it is able to synthesize images on which training self-supervised methods can perform _at par with or better than_ training on real images of the same sample size. Inspired by the idea of contrastive self-supervised learning, which promotes intra-image invariance, we develop a representation learning approach that promotes intra-caption invariance. We achieve this by treating the multiple images generated from the same text prompt as positives for each other and use them in a multi-positive contrastive loss (see Figure 1). Despite training with solely synthetic images, this approach, called StableRep, even outperforms state-of-the-art methods such as CLIP  using the same text set, but with corresponding real images, on various representation evaluation benchmarks.

Intuitively, one reason that synthetic data can be better than real data is because we are able to achieve a greater degree of control in the sampling, such as via the guidance scale in Stable Diffusion, or via text prompts and latent noise variables. Furthermore, generative models have the potential to generalize beyond their training data and therefore provide a richer (synthetic) training set than the corresponding real data alone. Our key contributions are:

1. We discover that training modern self-supervised methods on synthetic images from Stable Diffusion can be surprisingly effective. The learned representations are often better than representations learned from real images of the same sample size.
2. We develop StableRep, a novel representation learning approach by capturing invariance between images generated from the same text prompt, and propose a multi-positive contrastive loss.
3. With StableRep, we are able to achieve 76.7% linear accuracy on ImageNet with ViT-B/16, using solely synthetic images.
4. When coupled with language supervision, our StableRep trained with 20M _synthetic_ images (10M captions) achieves better accuracy than CLIP trained with 50M _real_ images (50M captions).

## 2 Standard Self-supervised Learning on Synthetic Images

A typical visual representation learning algorithm takes an image _dataset_\(\{_{i}\}_{i=1}^{N}\) as input, and yields an image encoder \(F:\), which embeds an image \(\) into a vector \(\). In this paper, we instead try to produce a good \(F\) by using a _generative model_\(G\) rather than a real image _dataset_. Specifically, we focus on text-to-image generative models \(G:(,)\), which maps a pair of text \(\) and latent noise \(\) to an image \(\). While there are several top performing text-to-image models [59; 67; 88; 7; 36; 3], we conduct our exploration with the Stable Diffusion  since it is publicly available and widely used. The version we used is v1-5.

### Synthetic images from Stable diffusion

Stable diffusion  is a denoising diffusion probabilistic model [73; 31] that runs the diffusion process in the latent space of an autoencoder. It improves the sample quality and text-image alignment via classifier-free guidance , which linearly combines conditional score estimate \((,_{})\) and unconditional estimate \((_{})\) with the guidance scale \(w\) at each step \(\):

\[}(,_{})=w(,_{})+(1-w)(_{})\] (1)

The Stable Diffusion model \(G_{sd}\) relies on text sources to generate images. Instead of collecting a corpus of captions from scratch, we use the text part of existing _uncurated_ image-text pair datasets, such as CC3M  and CC12M . Formally, given an image caption dataset \(\{_{i}\}_{i=1}^{N}\), we generate one image per caption, forming a synthetic image dataset of the same size.

### Self-supervised learning on synthetic images

Recent representative self-supervised learning algorithms are mostly from two families: (1) contrastive learning which encourages invariance between embeddings of different augmentations of the same image ; (2) masked image modeling where model uses unmasked patches to predict masked patches (although there are other methods that fall into neither category, such as BYOL  and DINO ). For our study, we choose SimCLR  from the former family and MAE  from the latter due to their simplicity and strong performance. We focus on the Vision Transformer architecture , and use captions from CC3M  except when noted.

**SimCLR .** We directly train SimCLR with ViT-B/16 on the synthetic image dataset, and measure the representation quality by linear probing evaluation on ImageNet 1. One factor to consider is the classifier-free guidance scale \(w\), as it trades off between diversity and quality of the synthesized images and thus can affect the learned representations. To study this, for each \(w\) in the set \(\{2,3,4,6,8,10,12\}\), we generate a copy of size \(N\) (one image per caption) to train SimCLR. Figure 2(left) visualizes the influence of \(w\). The optimal \(w\) is around 8 (both 8 and 10 give an accuracy of 62.0\(\%\)). This is different from the FID metric where \(w=2\) is the optimal.

The captions \(\{_{i}\}_{i=1}^{N}\) used to generate synthetic images are also paired with \(N\) real images. We train a SimCLR model with these real images. This model achieves 60.4\(\%\) accuracy, experiencing a 13\(\%\) drop in linear accuracy compared to pre-training on ImageNet. Such gap has been generally observed for uncurated pre-training data . However, both interestingly and surprisingly, synthetic images with \(w=8\) have 1.6\(\%\) higher accuracy than real images (62.0\(\%\) v.s. 60.4\(\%\)).

**MAE .** Following the default hyperparameters in MAE , we train a ViT-B/16 model for each guidance scale \(w\). Figure 2(right) reports the linear probing results. The accuracy of synthetic images increases quickly with \(w\) after 2, and gradually drops when \(w\) is large, e.g., \(w 10\). The optimal guidance scale for MAE is 6, and this is different from SimCLR where the accuracy peaks at 8 or 10. This suggests that different methods may require different \(w\). With \(w=6\), synthetic images have a 4.2\(\%\) better accuracy than real images.

While the linear probing accuracy of MAE is lower than that of contrastive methods, its effectiveness often comes with fine-tuning. When fine-tuning pre-trained MAE models on ImageNet, we found synthetic images are still able to outperform real images. For instance, synthetic images with \(w=6\) is 0.3\(\%\) higher than real images (82.9% v.s. 82.6%).

Figure 2: Performance of linear probes on ImageNet as a function of the guidance scale of Stable Diffusion generation. **Left**: using SimCLR as pre-training; **Right**: using MAE as pre-training. In both cases, we see pre-training on synthetic images that are generated by Stable Diffusion with a guidance scale between 6 and 8, gives a significant boost over training only on real images. We used the CC3M dataset for these experiments.

**Other SSL methods.** To test if synthetic images can be generically applied to different self-supervised learning methods, we try three more representative approaches: BYOL , MoCo-v3 , and DINO . We do not tune \(w\) for each method, and instead apply the optimal \(w\) (\(=8\)) discovered for SimCLR. The results on CC3M and CC12M are visualized in Figure 3. Synthetic images significantly improve over real for MAE, DINO, and SimCLR, and performs on par with real for BYOL, and slightly worse for MoCo-v3 (which could be attributed to not tuning the guidance scale \(w\)).

## 3 Multi-Positive Contrastive Learning with Synthetic Images

Text-to-image generative models offer a new way to compose positive samples for contrastive learning. Given an image caption, we can create multiple diverse samples by starting the reverse diffusion process with different latent noise \(\). Since these images are produced using the same prompt, they possess similar visual semantics, making them suitable for use as multiple positive samples for each other in contrastive learning. This property is unique to generative models, since collecting multiple images for each caption in large scale is infeasible. Figure 4 compares our StableRep pipeline with that of SimCLR and CLIP.

**Multi-positive contrastive loss.** We describe multi-positive contrastive learning as a matching problem. Consider an encoded anchor sample \(\), and a set of encoded candidates \(\{_{1},_{2},...,_{K}\}\).

Figure 4: We compare our pipeline (C) to that of (A) SimCLR; (B) CLIP. In SimCLR, the real image is augmented to give two views which are contrasted against each other through the same encoder. For CLIP, a real image and corresponding real caption are passed into image and text encoder, the image is augmented (usually more weakly than for SimCLR) followed by a contrastive loss. In our pipeline, each real caption is passed into Stable Diffusion (SD) to generate a number of synthetic images. These synthetic images are then augmented as in SimCLR, and treated as positives for each other in a multi-positive contrastive loss.

Figure 3: Training self-supervised methods on synthetic images can be better than, or on par with, real images of the same sample size. **Left**: CC3M dataset; **Right**: CC12M datasetWe compute a contrastive categorical distribution \(\) that describes how likely \(\) is to match each \(\):

\[_{i}=_{i}/)}{_{j=1}^{K} (_{j}/)}\] (2)

where \(_{+}\) is the scalar temperature hyper-parameter, and \(\) and all \(\) have been \(_{2}\) normalized. Intuitively, this is a \(K\)-way softmax classification distribution over all encoded candidates. Assume there is at least one candidate that the anchor \(\) matches. Then we know the ground-truth categorical distribution \(\) is:

\[_{i}=_{(,_{i} )}}{_{j=1}^{K}_{(,_{j})}}\] (3)

where the indicator function \(_{(,)}\) indicates whether the anchor and candiate match. Then the multi-positive contrastive loss is the cross-entropy between the ground-truth distribution \(\) and the contrastive distribution \(\):

\[=H(,)=-_{i=1}^{K}_{ i}_{i}\] (4)

This is a generalized form of the widely-used single-positive contrastive loss , where \(\) reduces to a one-hot vector. This loss is closely related to that in , but a key distinction here is that we have no image class labels, and only assume images generated from the same caption are matched.

The PyTorch-like pseudocode of the batched multi-positive contrastive learning algorithm is described in Algo. 1. Each batch consists of \(n*m\) images, meaning that we sample \(m\) images for each of the \(n\) captions. Here we still apply data augmentation, even though images from the same caption are different. This is to reduce overfitting since we perform many epochs of training over pre-generated synthetic images. However, if in the future the image generator is capable of producing images fast enough, then we can draw batches online and data augmentation may not be necessary. The multi-positive contrastive learning algorithm is also generic such that SimCLR can also be described by it - we begin by randomly selecting a set of \(n\) images and subsequently apply \(m\) (set as 2) crops to each of the chosen images. However, in our StableRep we only utilize a single crop from each image.

``` #f:encoder:backbone+projmlp #tau:temperature #minibatchx:[n,m,3,h,w] #ncaptions,mimagespercaptionforknloader: x=augment(x) x =cat(unbind(x,dim=1))#[n*m,3,h,w]  h=f(x) #computeground-truthdistributionp label=range(n*m)%n p=(label.view(-1,1))==label.view(1,-1)) p.fill_diagonal(0)#selfmasking p/=p.sum(1) #computecontrastivedistributionq logits=h*h.T/tau logits.fill_diagonal(-1e9)#selfmasking q=softmax(logits,dim=1) H(p,q).backward() defH(p,q):#cross-entropy return-(p*log(q)).sum(1).mean() ```

**Notes**: h.T is h's transpose. The \(_{2}\) normalization operator is included in the encoder f.

## 4 Experiments

We perform StableRep pre-training on synthetic images synthesized from texts in the CC3M (2.7 million samples) , CC12M (10 million) , or RedCaps datasets (11.6 million) . We then evaluate the frozen representations by (1) linear probing on ImageNet-1k and other smaller scale image classification benchmark, and (2) few-shot image recognition that measures the generalization ability of the representations.

**Backbone.** We use ViT models  as the backbone for our approach StableRep. On top of the CLS token, we apply a 3-layer MLP projection head with hidden layers of 4096 dimensions and an output of 256 dimensions. Batch Normalization  is used in this projection head.

**Training.** In most of our experiments, we adopt a batch size of 8192 images (i.e. \(m*n=8192\)). This way the computation of each batch is equivalent to SimCLR with a batch size of 4096, because each image in SimCLR has two crops. We use AdamW optimizer  with a learning rate of 0.0032 and weight decay of 0.1, and set \(_{1},_{2}\) as \(0.9,0.98\) respectively. We pre-generate \(10\) images for each text prompt. In each iteration, we randomly sample \(6\) out of the \(10\) for each sampled caption to form the training batch, i.e., \(m=6\) in Algo. 1. Recall that for SimCLR \(m=2\). As a result, one epoch training of StableRep is computationally equivalent to 3 epochs of SimCLR. To provide easy comparison, we report SimCLR-equivalent epochs for StableRep in all of our analysis.

### Main results on CC12M and RedCaps

In this section, we perform StableRep on images synthesized by either CC12M or RedCaps. For StableRep, we first removed duplicate captions from each dataset, resulting in a reduced number of captions: from 10.0M to 8.3M for CC12M and from 11.7M to 10.5M for RedCaps. We compared StableRep to SimCLR, which was trained on either synthetic or original real images. We also included CLIP with a synthetic and a real version 2. For SimCLR and CLIP, we did not perform de-duplication for either real or synthetic setting. We train for 35 epochs for all methods using ViT-B/16 (for StableRep, this refers to 35 SimCLR-equivalent epochs). We observed that CLIP started to overfit around 30 epochs. But StableRep did not overfit with this schedule (see Table (c)c for results with longer training). For StableRep, we additionally apply random downsample augmentation (see Appendix A.1 for details and how such downsample affects different methods).

**ImageNet.** Table 1 presents the results of linear probing on ImageNet. For StableRep, we prepend a BatchNorm layer without affine transformation to the linear classifier (see Appendix A.5 for more details). We observed that training SimCLR on synthetic images yields an improvement of 2.2% top-1 accuracy on CC12M and 1.0% on RedCaps when compared to real images. However, the accuracy of CLIP drops by 2.6% on CC12M and 2.7% on RedCaps when trained on synthetic images (see Section 5 for more discussion). On the other hand, our method StableRep outperforms CLIP trained on real images, with improvements of 3.2% and 2.6% for CC12M and RedCaps, respectively.

**Linear classification on more datasets.** We followed the approach of SimCLR  and BYOL  to assess the generality of our learned representations across different image domains. Specifically, we performed linear classification on 11 image classification datasets introduced by . The results are reported in Table 2, and the relative performance is consistent with that on ImageNet. Notably, our proposed method, StableRep, achieves the highest accuracy on all of the 11 datasets.

Table 1: Comparison under the linear probing protocol on ImageNet ; measuring top-1 accuracy on a frozen pre-trained backbone. We compare our StableRep with SimCLR  and CLIP  with either synthetic or real images, on CC12M  and RedCaps . All models are pre-trained with 35 epochs using ViT-B/16 .

Table 2: Linear probing experiments on image datasets from various domains. Pre-training is conduceted on CC12M, with either synthetic or real images. Best results for each dataset are highlighted with **bold**.

[MISSING_PAGE_FAIL:7]

of representations. To study this trade-off, we vary the sampling parameter \(m\) from 2 to 10 while keeping \(n=C/m\). As shown in Table 4(b), The linear probing accuracy are similar between \(m=4\) and \(m=10\) (peak at \(m=8\) with 69.8% accuracy), showing the robustness of StableRep w.r.t. \(m\). We choose \(m=6\) as our default setup. We "abuse" \(m=1\) to represent SimCLR.

After the above study, we continue to ablate the following factors on CC12M and RedCaps.

Guidance score for training.We consider three configurations for the classifier free guidance scale \(w\): (1) _large scale \(-w\{8,10\}\)_; (2) _small scale \(-w\{2,3\}\)_; (3) _mixed scale \(-w\{2,3,4,5,6,8,10,12\}\)_. As shown in Table 5(a), small scale gives the best linear transfer accuracy on ImageNet and fine-grained classification datasets. This is possibly because smaller \(w\) leads to larger intra-caption variation between generated images, which enforces StableRep to learn stronger invariance. This is different from SimCLR which requires larger \(w\) (recall Section 2.1), as SimCLR only models intra-image invariance and thus higher image quality (larger \(w\)) helps more.

**Model scale.** We switch the backbone architecture to ViT-L/16. Table 5(b) presents the results. The accuracy improves by 1.9% on ImageNet linear probing and 0.7% on the average over fine-grained classification datasets. We found that pre-training with ViT-L was unstable. The loss kept exploding to NaN, and we resumed from the checkpoint before NaN. But this led to a higher convergent loss than ViT-B (ViT-L loss is lower before exploding). This may partly be due to the usage of BatchNorm.

**Longer training.** To investigate the scaling behavior of StableRep w.r.t. training compute, we further increase the pre-training computation budget to 2x and 3x epochs, and report the linear probing accuracy on ImageNet in Table 5(c). The results indicate that StableRep scales well with longer training, e.g., improving by 2.2 for 2x and 2.9 for 3x on CC12M pre-training, and by 2.6 for 2x and 3.0 for 3x on RedCaps pre-training.

## 5 Adding Language Supervision

How would training CLIP using synthetic images work? We study this question by generating a copy (one image per caption) for each guidance scale \(w\) in \(\{1,2,3,4,6,8,10\}\) and training CLIP using

Table 6: **Ablation experiments** by pre-training on CC12M or RedCaps. We report linear probing accuracy on ImageNet (IN) and/or average accuracy over the 11 fine-grained classification datasets (avg.). The colored cell indicates the default setup on each dataset: ViT-B/16 trained for 35 epochs with small guidance scale \(w\).

each copy. Figure 5 plots the zero-shot ImageNet accuracy. Contrary to SSL methods, CLIP favors lower \(w\). With the optimal \(w=2\), CLIP achieves 34.9% zero-shot accuracy. This is 5.4% lower than training on real images (40.2%). Such gap may be explained by misalignment between the generated images and the input text, shown in Figure 7. This is especially true for fine-grained classes.

We can add language supervision to StableRep by adding \(0.5*(_{i2t}+_{t2i})\) to StableRep loss, where \(_{i2t}\), \(_{t2i}\) are image-to-text and text-to-image contrastive losses described by Eq. 4. Adding supervision improves StableRep from 72.8% to 74.4% on CC12M and from 73.7% to 75.4% on RedCaps for ImageNet linear probing. We term it as StableRep+. We then further scale StableRep+ to a randomly selected 50M subset of LAION-400M . For this experiment, we only generate 2 images per caption with \(w=2\), and train CLIP with _real_ images and StableRep+ with _synthetic_ images using different scales of random subsets of the 50M data. We plot the results in Figure 6. StableRep+ consistently achieves better accuracy than CLIP. Noteably, StableRep+ with 10M captions outperforms CLIP with 50M captions, yielding a 5x time caption efficiency (2.5x image efficiency).

### Fairness and compositionality

We further study the fairness and compositional understanding of the learned models on FairFace  and ARO  benchmarks, respectively. The results are presented in Table 7.

**Fairness.** We perform zero-shot classificaton on FairFace. We jointly classify both races and genders, e.g., treating _Black male_, _Black female_, _Indian female_, and so on as different classes at the same time. For cc12m models, CLIP with real data only achieved 0.3% accuracy with _Southeast Asian male_ class, and CLIP wth synthetic data improves this class to 3.1%, while our StableRep+ furthers it to 27.2%. For redcaps models, real CLIP only has 0.4% accuracy for _East Asian Male_, while StableRep+ improves this class to 22.8%. In summary, training with synthetic data is able to improve the worst class accuracy. However, a obvious geographic bias still exists in all models.

**Compositionality.** The results of compositionality evaluation are less clear. While training with synthetic data on cc12m slightly improves the relational understanding, an accuracy drop is observed in models trained with synthetic data on redcaps. An in-depth investigation may be further needed.

## 6 Related Work

**Text-to-Image generative models.** Text-to-image models trained on large image and text pairs have recently enabled the creation of rich and diverse images encompassing many genres and themes [7; 61; 67; 88]. The resulting creations have become a sensation, with Stable Diffusion having

  & & & &  &  \\  & & pre-train data & mean acc. & best-class acc. & worst-class acc. & relation acc. \\  _{i2t}\)} & CLIP & Real & 28.2 & 60.2 & 0.3 & 46.4 \\  & & Syn & 30.4 & 64.0 & 3.1 & **50.0** \\  & StableRep+ & Syn & **37.2** & **74.9** & **10.0** & 47.3 \\  _{i2t}\)} & CLIP & Real & 9.3 & 31.1 & 0.4 & **59.0** \\  & & Syn & 22.3 & 52.4 & 1.0 & 56.0 \\   & StableRep+ & Syn & **27.3** & **64.4** & **2.1** & 52.3 \\ 

Table 7: Results of fairness and compositionality evaluation.

Figure 7: Examples of misalignment between input text and synthesized image, which can lead to suboptimal performance for CLIP trained on synthetic images. **Upper:** require headhammer shark but Stable Diffusion often generates sharks without headhammer; **Lower:** “Andrex Puppies” is a brand of toilet rolls.

millions of downloads and many tools for image manipulation built on top [66; 38; 90]. Most of these models are built on denoising diffusion models [31; 73] with some notable exceptions [8; 7]. In this paper, we leverage this latest generation of diffusion-based pre-trained generative models for the task of representation learning.

**Visual representation learning.** Early approaches for visual representation learning often relied on pretext tasks such as inpainting  to train image encoders. More recent advancements have shown that mask image modeling, a form of self-supervised training, can be highly effective. In particular, Masked Autoencoder (MAE)  has demonstrated significant improvements in downstream fine-tuning performance. Another line of research focuses on contrastive learning, which aims to learn visual representations by maximizing agreement between two augmented views of the same image while distinguishing it from negative examples [10; 78; 54; 84; 27; 79]. Meanwhile CLIP  and its subsequent works  leverage contrastive learning to train image representations using language supervision, leading to impressive transferability across various tasks.

**Learning from synthetic data.** It has been common to train machine learning models with synthetic data in different domains [72; 81; 14; 63; 44; 64; 50; 43; 76; 87; 29; 49]. In computer vision, synthetic images have been used as a source for training models, such as optical flow [48; 23], autonomous driving , semantic segmentation [12; 62], object detection [65; 57], human pose estimation [82; 34] or classification [2; 69; 28]. The closest set of work are the ones that conduct representation learning on synthetic images [60; 45; 4; 35]. In , a model is trained to perform multi-task learning on synthetic images. The main method in [45; 4; 35] is to manipulate the latent variable of deep generative models [45; 4; 35] or image generation procedures , to form meaningful synthetic images for their representation learning methods. Our method falls into this category, but we use text-to-image diffusion models, which have also been explored by [2; 28; 69]. The key difference is that they conducted supervised learning while we use synthetic data for pre-training representations.

## 7 Conclusion, Limitations and Broader Impact

We have shown that solely synthetic data generated from state of the art text-to-image models can be used to train powerful visual representations. By harnessing the stochastic nature of Stable Diffusion in combination with a multi-positive contrastive loss, our approach yields a representation that surpasses the performance achieved through training on real data alone. Through a series of experiments, we establish that pre-training with synthetic datasets of varying scales yields impressive results across different downstream tasks, including linear probing and few-shot classification. Interestingly, we discover that even vanilla self-supervised methods trained on synthetic data can either outperform or achieve comparable results to those trained on real data.

Despite demonstrating the potential of training with synthetic data, this paper acknowledges its limitations. Firstly, we have yet to comprehend the reasons behind the effectiveness of training self-supervised methods on synthetic images compared to an equal amount of real images. It is possible that this observation is confined to our particular evaluation methodology. Furthermore, the current image generation process remains slow, with approximately 0.8s per image on a A100 GPU or 2.2s per image on a V100 GPU while xFormers is enabled. Consequently, we are not able to train StableRep models with non-repetitive images synthesized online. Additionally, we have not addressed the issue of semantic mismatch between the input prompts and the generated images, which may impact the quality and usefulness of the synthetic data. Moreover, synthetic data has the potential to exacerbate biases due to mode collapse and a predisposition to output "prototypical" images. Lastly, image attribution becomes a challenge when working with synthetic data.

**Broader impacts.** This paper focuses on the fundamentals of visual representation learning, and we believe it will be beneficial to the practice of this field. Our method presents an immediate application by reducing the reliance on collecting a vast amount of real images for learning representations. This approach brings potential benefits in terms of cost-effectiveness and minimizing biases introduced through human collection and curation processes. However, it is important to acknowledge that our method relies on text-to-image generative models trained on large-scale, uncurated web data. Such data may conceal social biases and errors that would have been exposed through human curation. Additionally, we must recognize that the text prompts we employed are not completely bias-free; the selection of prompts influences the synthesized images. Thus, the choice of prompts assumes a role similar to the selection of real images for self-supervised visual representation learning.