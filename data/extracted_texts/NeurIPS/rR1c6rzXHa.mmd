# RD-Suite: A Benchmark for Ranking Distillation

Zhen Qin, Rolf Jagerman, Rama Pasumarthi, Honglei Zhuang, He Zhang, Aijun Bai,

Kai Hui, Le Yan, Xuanhui Wang

Google

Mountain View, CA, US 94043

Correspond to Zhen Qin <zhenqin@google.com>.

###### Abstract

The distillation of ranking models has become an important topic in both academia and industry. In recent years, several advanced methods have been proposed to tackle this problem, often leveraging _ranking_ information from teacher rankers that is absent in traditional classification settings. To date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide range of tasks and datasets make it difficult to assess or invigorate advances in this field. This paper first examines representative prior arts on ranking distillation, and raises three questions to be answered around methodology and reproducibility. To that end, we propose a systematic and unified benchmark, Ranking Distillation Suite (RD-Suite), which is a suite of tasks with 4 large real-world datasets, encompassing two major modalities (textual and numeric) and two applications (standard distillation and distillation transfer). RD-Suite consists of benchmark results that challenge some of the common wisdom in the field, and the release of datasets with teacher scores and evaluation scripts for future research. RD-Suite paves the way towards better understanding of ranking distillation, facilities more research in this direction, and presents new challenges.

## 1 Introduction

Ranking models have become the interface between users and many applications, such as search engines and recommender systems . In recent years, due to the popularity of large neural ranking models , as well as the interest in model serving in practical scenarios, such as on mobile devices , the intersection of learning to rank (LTR)  and knowledge distillation  has drawn much attention in both academia and industry .

Examining the evaluation and experimental setup of many of these papers, we found that there is no unified consensus on what makes an acceptable test bed for benchmarking ranking distillation methods. There is also a large diversity in the types of tasks and datasets adopted, as well as the configurations of teacher and student rankers. These make comparison of different methods as well as an assessment of their relative strengths and weaknesses difficult.

In this paper, we propose a new benchmark, Ranking Distillation Suite (RD-Suite), for the purpose of benchmarking ranking distillation methods. We design a benchmark suite comprised of two major modalities in the ranking literature, sensible teacher and student model architectures, and both standard distillation and a novel distillation transfer setting. The datasets and evaluation scripts are publicly available2 and the experimental configurations are made clear to encourage fair comparisons for future research on the important topic.

While the focus of this benchmark is an empirical comparison, we are also fundamentally interested in understanding the open questions and limitations of existing methods on ranking distillation. By examining the general formulation and prior art, we raise three questions around methodology and reproducibility, and provide discussions that challenge some of the common wisdom in the field, shine light on the empirical results, rationalize our design choices, and open up research challenges to tackle. We believe such a side-by-side performance benchmark will be valuable to the community, providing deeper insight on the practical effectiveness of different methods. The contributions and resulted artifacts of this work include:

* A systematic ranking distillation benchmark covering a variety of datasets, modalities, and methods, based on design choices around generality, simplicity, and reproducibility.
* The raise and analysis of key issues that challenge prior art and pave the way towards better understanding of ranking distillation.
* The release of easily accessible datasets and evaluation scripts to facilitate future research.

## 2 Preliminaries

We describe the general formulation of ranking distillation and representative existing arts.

### The General Formulation

For _regular LTR_, the training data can be represented as a set \(=\{()^{n}^{n})\}\), where **x** is a list of \(n\) items \(x_{i}\) and **y** is a list of \(n\) relevance labels \(y_{i}\) for \(1 i n\). We use \(\) as the universe of all items. The objective is to learn a function that produces an ordering of items in **x** so that the utility of the ordered list is maximized. Most LTR algorithms formulate the problem as learning a ranking function to score and sort the items in a list. As such, the goal of LTR boils down to finding a parameterized ranking function \(f(;):^{n}^{n}\), where \(\) denotes the set of trainable parameters, to minimize the empirical loss:

\[(f(;))=_{()}l_{ rel}(\,f(;)),\] (1)

where \(l_{rel}()\) is the loss function on a single list with relevance labels.

In tabular LTR [19; 23; 30], each \(x_{i}\) corresponds to a query-item pair represented as a numeric feature vector, and \(\) is usually a linear, tree, or multilayer perception (MLP) model. For text ranking [22; 45], each \(x_{i}\) represents query and document text strings and transformer-based models, such as BERT  and T5 , are the norm for \(\).

For _ranking distillation_, in addition to the original training data, it is assumed that there is a teacher ranker \(f(;^{t})\) producing teacher scores (or distillation labels) \(^{t}=g(f(;^{t}))\), where \(g()\) is an optional (ranking preserving) transform function on teacher scores. The goal of ranking distillation is to train a student model \(f(;^{s})\), where \(^{s}\) usually has significantly smaller capacity than \(^{t}\), with the following loss:

\[(f(;^{s})) = _{()}\, \,l_{rel}(\,f(;^{s}))\,+\,(1\,-\,)\,\,l _{distill}(^{t},f(;^{s})),\] (2)

where \(\) is a weighting factor and \(l_{distill}\) is the additional distillation loss.

### Existing Art

Regular LTR research has proposed a rich set of \(l_{rel}()\), including pointwise, pairwise, and listwise losses . Here we describe two recent most representative ranking distillation work (see more discussions in Sec. 7) that focus on \(l_{distill}\).

**RD ** treats top \(K\) ranked items by the teacher ranker as positive examples (no negatives are considered), and apply a _pointwise_ sigmoid cross-entropy loss:

\[l_{}(^{t},f(;^{s}))=-_{i=1}^{K} f (x_{(i)};^{s}),\] (3)where \((i)\) is the \(i\)-th document in the ordering \(\) induced by teacher ranker.

**RankDistil ** can be treated as a listwise generalization of RD, trying to preserve _ordering_ of the top \(K\) items induced by the teacher ranker:

\[l_{}(^{t},f(;^{s}))=E_{ P _{}}[- P_{s}(,f(;^{s}))],\] (4)

where

\[P_{s}(,f(;^{s}))=_{j=1}^{K};^{s}))}{_{l=j}^{L}(f(x_{(l)};^{s}))}\] (5)

is the Plackett-Luce probability model, \(L\) is the length of the ranking list, and \((j)\) is the \(j\)-th document from a _sampled_ ordering \(\). Essentially, it samples several length-\(K\) permutations from teacher ranker score distribution (\(P_{^{}}\)) and tries to maximize the log likelihood of the induced permutations from student scores of the corresponding documents.

## 3 Three Questions on Ranking Distillation

By examining the general formulation and prior art, we raise three questions about the current literature on ranking distillation.

### Q1: Where is the knowledge in ranking distillation?

An implicit assumption of RD, RankDistil, and related methods is, the knowledge for distillation  is in the (pointwise or listwise) top \(K\)_orderings_.

There are three concerns of this assumption: First, they largely ignore the teacher score _values_3. In contrast, in classification distillation, the logit values of all classes are considered . Importantly, ordering can be derived from score values, but not the other way. Is it possible that some knowledge is lost by only considering the ordering? Second, the top \(K\) is a hyperparameter. Besides more tuning efforts, it is counter-intuitive that it should be the same for all lists in a dataset, due to the potentially huge variance in queries and their corresponding documents in practice. Third, documents outside of top \(K\) may be useful and contribute to the knowledge. The fact that prior work mainly compared under this assumption (e.g., RankDistil mainly compared with RD) makes the answer to the questions unclear.

### Q2: How to handle teacher ranker scores?

As we argued to potentially leverage teacher score values above, the teacher scores need closer examination. In general, the teacher ranker scores may not be constrained - the ranks of candidate documents are invariant to any transformation that preserves the order of the scores. An example is the scale invariance of many advanced loss functions:

* The pairwise logistic loss in RankNet : \[l_{}(,)=-_{i j}_{y_{i} >y_{j}}-s_{j})}{1+(s_{i}-s_{j})}.\] (6)
* The listwise Softmax loss [5; 3]: \[l_{}(,)=-_{i}y_{i}) }{_{j}(s_{j})}=-_{i}y_{i}(s_{j}-s_{i})},\]

where we use \(=f(;)\) for conciseness. In these popular ranking losses, scores always appear in the _paired_ form of \(s_{i}-s_{j}\) or \(s_{j}-s_{i}\). So when a constant is added to each score, the losses stay invariant. Many other ranking losses, such as Approximate NDCG loss [27; 2], also have the same translation-invariant property.

This is in contrast to classification problems, where the logits have clear probabilistic meanings over all classes. The potentially unconstrained teacher scores need careful treatment. For example, many loss functions (such as cross-entropy based ones) are ill-defined with negative labels, potentially leading to devastating downstream distillation performance.

### Q3: How to ensure reproducible and fair evaluations?

We note several complexities to ensure reproducible and fair evaluation for ranking distillation research. First, for datasets with large corpus, a retrieval stage is performed before ranking, which changes the data distribution for the ranking stage. Second, it is clear that the teacher model has significant effect on the downstream distillation task. For example, as we discussed, teacher ranker scores have large freedom and are difficult to reproduce. Third, researchers should be careful about what to ablate to focus on progress on distillation methods. In Eq. 2, we can see that (ranking) distillation is multi-objective. Though classification problems almost universally use the softmax cross-entropy loss for \(l_{}\), ranking problems have more flexibility for this term. In , the two loss terms always change together - it is unclear if the performance differences are from a better relevance loss or distillation method. Last but not least, there is no consensus on how to calculate ranking metrics. For example, queries without any relevant documents may get perfect score , 0 , or ignored . Different metric definition makes cross-examination among papers difficult.

## 4 RD-Suite

We describe RD-Suite, starting with a set of desiderata motivated by , followed by the actions we take, and how they link to the desiderata and questions raised in the previous section.

### Desiderata

For creating the RD-Suite, we established a set of desiderata:

D1. Generality: Given the long history of learning to rank research on tabular datasets, as well as the more recent interest in text ranking, RD-Suite includes tasks in both modalities. Also, we focus on distillation objectives in this paper, instead of model architecture specific distillation techniques (see Sec. 7), though the latter should benefit from better objective functions and easily compare against the benchmark with appropriate ablations.

D2. Simplicity: The tasks should have a simple setup. All factors that make comparisons difficult should be removed. This encourages simple models instead of cumbersome pipelined approaches. For instance, we consider pretraining to be out of scope of this benchmark. The student rankers are either simple to implement or initiated from publicly available checkpoints.

D3. Challenging: The tasks should be difficult enough for current models to ensure that there is room for improvement to encourage future research in this direction. We leverage state-of-the-art teacher rankers that still have good advantage over existing methods. We also introduce a novel distillation transfer task with encouraging results but considerable headroom.

D4. Non-resource intensive and accessible: The benchmarks should be deliberately designed to be lightweight so as to be accessible to researchers without industry-grade computing resources. We consider linear students for tabular ranking and regular public BERT models for text ranking. We handle the more expensive teacher ranker training and inference, and directly expose the distillation dataset to the community.

D5. Fairness. We note that it is non-trivial and almost impossible to conduct a perfectly fair evaluation of all models. The large search space motivates us to follow a set of fixed hyperparameters for most models (discussed in Sec. 4.6 with few exceptions). The best performance and relative order of the models may change if we aggressively tune hyperparameters for all models. Hence, the results provided in this paper are not meant to be a final authoritative document on which method is the best. Instead, we provide a starting point for future research and strive to be as fair as possible with the tuning process clearly laid out.

### Tasks

RD-Suite has four tasks, differing in terms of data modality, teacher model domain, and availability of relevance label. They are described in Table 1. Specifically, T1 (Text Ranking Distillation) and T4 (Tabular Ranking Distillation) are standard ranking distillation tasks on two common modalities in the LTR literature. T2 (Distillation Transfer) and T3 (Distillation Transfer Zeroshot) are motivated by the popularity of domain adaptation research where knowledge is transferred from source to target domains [31; 18]. We believe the tasks are comprehensive and reflect real-world applications: T1 and T4 are standard ranking distillation tasks, T2 is applicable when target domain data is not sufficient to tune the teacher model, or the teacher model is not accessible for tuning. T3 is applicable when there are no labels for the target domain, which is the zeroshot learning scenario . Note that domain adaptation is not applicable to tabular LTR since different datasets are in different feature spaces.

The tasks are designed to cover wide application scenarios (D1) while maintaining simplicity (D2) - for example, T1, T2, and T3 can use the same implementation by changing teacher label and \(\) (T3 simply has \(=0\)). The novel ranking distillation transfer setting is quite challenging (D3) and not well studied in the literature.

### Teacher and Student Configurations

We make the teacher and student ranker configurations clear. For text datasets, the teacher rankers are from RankT5 , which use encoder-decoder T5  and listwise softmax ranking losses. For tabular datasets, the teacher rankers are from , which use MLP and a novel LambdaLoss that will discussed below. To our knowledge, they are arguably the state-of-the-art in their respective tasks and we get the teacher models from the authors. As for student rankers, we use the publicly available BERT-base  checkpoint for text datasets and linear model for tabular datasets.

The advanced teacher rankers provide reasonable headroom (D3) and the student configurations follow D2 and D5 - they are easily accessible or easy to implement, and can be trained with accessible computing resources.

### Datasets

All datasets in RD-Suite are popular public datasets. For text ranking, we use MSMARCO  and NQ , two of the most popular datasets for text ranking. Both datasets have large document corpus and require a retrieval stage before ranking. We leverage recent neural retrievers  to retrieve the top 50 candidates for each query.

For tabular ranking, we use Web30K  and Istella , two of the most popular datasets for tabular ranking. Retrieval is not needed for these datasets.

RD-Suite will release the teacher scores and retrieved document ids (on MSMARCO and NQ) for each query, to hide the potentially expensive teacher model inference and retrieval from ranking distillation research (D4) while guaranteeing fairness (D5, Q3).

### The Ranking Loss Family for Distillation

As we discussed in Q1 (Where is the knowledge in ranking distillation?), there are several concerns around existing ranking distillation methods, namely they do not consider score values, has a top \(K\) hyperparameter, and largely ignore documents outside of the top \(K\).

Interestingly, we find that many _ranking_ losses naturally have different behaviors around the concerns, but are not well studied in the distillation context. Therefore, we include the "ranking loss family"

   Tasks & Modality & Teacher Domain & Relevance Label \\  T1 & Text & In-domain & Available \\ T2 & Text & Out-domain & Available \\ T3 & Text & Out-domain & Not Available \\ T4 & Tabular & In-domain & Available \\   

Table 1: The tasks of RD-Suite. In-domain means teacher was trained on the same dataset as the student and out-domain otherwise.

for distillation in RD-Suite to study their effects and gain better understanding of assumptions of existing methods. We include six ranking losses that are some of the most representative methods in the literature, including the pointwise mean squared error (MSE), pairwise logistic loss (PairLog in Eq. 6) and mean squared error (PairMSE):

\[l_{}(,)=_{i j}((s_{i}-s_{j})-(y_{ i}-y_{j}))^{2},\] (7)

as well as the listwise Softmax (Eq. 7), Gumbel Approximate NDCG loss (GumbelNDCG) :

\[l_{}(,)=-}}_{i}}-1}{_{2}(1+r_{i})},\] (8)

where \(}\) is the ideal DCG metric, \(r_{i}\) is the approximate rank of document \(i\) given \(\) plus Gumbel noise, and LambdaLoss :

\[l_{}(,)=-_{i j}_{y_{ i}>y_{j}}_{ij}-s_{j})}{1+(s_{i}-s_{j})},\] (9)

where \(_{ij}\) is the LambdaWeight as defined in Eq.11 of .

### Model Tuning

We fix \(l_{}\) for all methods on each modality to focus on the distillation component (Q3). To study Q2, we find that the Softmax transformation is naturally order preserving and ensures positive labels, motivated by the classification setting 4 and the RankDistil work  :

\[g(f(;^{t}))=;^{t})/T)}{_{i} (f(x_{i};^{t})/T)},\] (10)

where \(T\) is the temperature. However, we note that some losses do not require positive labels (such as MSE), so we have the option to not use it (see more discussion in Sec. 5.5). We list the hyperparameter search space in Tbl. 2.

AdamW is the default optimizer for BERT. The key principle of parameter search space is, all methods share the same tuning budget (D5). The only exception is RD and RankDistill: since they have an additional hyperparameter \(K\), we sweep \(K\{1,5,10\}\) and thus they have _advantage_ in terms of tuning budgets.

All methods are implemented in the same open-source framework TF-Ranking . All methods in the ranking loss family have already been implemented in TF-Ranking. We implemented RD due to its simplicity. We got the RankDistil implementation from the authors and had several communications to make sure of its correctness. Using open-source implementations encourage simplicity and fairness (D2, D5).

   Data & Text & Tabular \\  \(l_{rel}\) & Softmax & LambdaLoss \\ Optimizer & AdamW & Adagrad \\ Learning Rate & 1e-4, 1e-5, 1e-6 & 1e-1, 1, 10 \\ Batch Size (of Lists) & 32 & 128 \\ Training Steps & 100K & 200K \\ Sequence Length & 128 & n/a \\  Head Weight \(\) & 0, 0.25, 0.5, 0.75, 1 & \\ Label Softmax Transform & on, off & \\ Temperature T & 0.1, 1.0, 2.0, 5.0, 10.0 & \\   

Table 2: Hyperparameter search space.

## 5 Results

We report results and observations on the four tasks in RD-Suite. We use some of the most popular ranking metrics, i.e., NDCG and MRR, on the relevance labels. The numbers are timed by 100 which is common in the literature. The graded labels in Web30K and Istella are binarized at 3 when computing MRR that requires binary labels. We first discuss the results on each task specifically and summarize more findings on all tasks in Sec. 5.4. All evaluation scripts are made available to facilitate cross paper comparisons (Q3).

### Results on Text Ranking Distillation (T1)

Tbl. 3 shows the ranking performance on MSMARCO and NQ datasets. We have the following observations: (1) The effectiveness of existing methods (RD and RankDistil) is verified. RankDistil significantly outperforms the Relevance Only baseline in both datasets while RD is effective on NQ. (2) However, the performance of RD and RankDistil are considerably worse compared with the ranking loss family. (3) The listwise Softmax loss tends to be the most robust method. It is encouraging to see some student models can even outperform the teacher on NQ NDCG@1.

### Results on Distillation Transfer (T2 and T3)

Tbl. 4 shows the ranking performance on the NQ dataset when transferring knowledge from the teacher rankers trained on MSMARCO (the same MSMARCO teacher in T1). For NQ-Zeroshot, since the relevance labels are not available, the Relevance Only baseline is not applicable. RD is also not applicable since it has to use relevance labels (all labels are simply positive in the distillation objective).

We have the following observations: (1) Results on the distillation transfer task are interesting and encouraging: even if the teacher ranker does not perform well, all distillation methods can get significant help from it and outperform the Relevance Only baseline. Note that the "poor teacher helps distillation" effect has drawn some attention in the classification setting  but is not well understood for ranking problems. (2) Results on NQ-Zeroshot are expected - as no relevance labels are available for the target domain, the teacher model becomes the headroom. There is still considerate performance variance among different methods, showcasing their capabilities in a "distillation only" setting. (3) Existing methods (RD and RankDistil) are still not competitive. (4) Listwise approaches such as Softmax and GumbelNDCG tend to be effective.

### Results on Tabular Ranking Distillation (T4)

Tbl. 5 shows the ranking performance on the Web30K and Istella datasets. Despite data modality and model configurations, one major difference is tabular LTR datasets have dense relevance labels - multiple documents with non-zero relevance, while for MSMARCO and NQ there is usually one labelled positive document for each query.

   &  &  \\  & MRR@10 & MRR & NDCG@1 & NDCG@5 & NDCG & MRR@10 & MRR & NDCG@1 & NDCG@5 & NDCG \\  Teacher & 43.63 & 44.46 & 29.91 & 46.84 & 54.22 & 60.08 & 60.36 & 46.95 & 63.89 & 66.98 \\  Relevance Only & 40.03 & 41.03 & 27.15 & 42.85 & 51.29 & 52.67 & 53.50 & 43.47 & 55.07 & 60.98 \\ RD & 40.25 & 41.25 & 27.51 & 42.92 & 51.45 & 57.09\({}^{}\) & 57.64\({}^{}\) & 46.72\({}^{}\) & 59.80\({}^{}\) & 64.50\({}^{}\) \\ RankDistil & 41.29\({}^{}\) & 42.21\({}^{}\) & 28.27\({}^{}\) & 44.04\({}^{}\) & 52.28\({}^{}\) & 55.04\({}^{}\) & 55.80\({}^{}\) & 45.71\({}^{}\) & 57.53\({}^{}\) & 62.86\({}^{}\) \\ MSE & 42.06\({}^{}\) & 42.96\({}^{}\) & 28.54\({}^{}\) & 45.12\({}^{}\) & 52.96\({}^{}\) & 58.49 & 58.95\({}^{}\) & 47.51\({}^{}\) & 61.34\({}^{}\) & 65.64\({}^{}\) \\ PairLog & 41.95\({}^{}\) & 42.87\({}^{}\) & 28.21 & 45.00\({}^{}\) & 52.86\({}^{}\) & 58.53\({}^{}\) & 59.00\({}^{}\) & 47.95\({}^{}\) & 61.38\({}^{}\) & 65.66\({}^{}\) \\ PairMSE & 42.27\({}^{}\) & 43.20\({}^{}\) & 28.78\({}^{}\) & 45.19\({}^{}\) & 53.11\({}^{}\) & 58.34\({}^{}\) & 58.79\({}^{}\) & 47.05\({}^{}\) & 61.44\({}^{}\) & 65.55\({}^{}\) \\ GumbelNDCG & 41.85\({}^{}\) & 42.76\({}^{}\) & 28.32\({}^{}\) & 44.81\({}^{}\) & 52.75\({}^{}\) & 57.90\({}^{}\) & 58.39\({}^{}\) & 47.26\({}^{}\) & 60.70\({}^{}\) & 65.14\({}^{}\) \\ Softmax & **42.37\({}^{}\)** & **43.27\({}^{}\)** & **28.87\({}^{}\)** & 45.33\({}^{}\) & **53.17\({}^{}\)** & **59.08\({}^{}\)** & **59.58\({}^{}\)** & **48.84\({}^{}\)** & **61.87\({}^{}\)** & **66.08\({}^{}\)** \\ LambdaLoss & 42.30\({}^{}\) & 43.21\({}^{}\) & 28.75\({}^{}\) & **45.38\({}^{}\)** & 53.14\({}^{}\) & 58.36\({}^{}\) & 58.85\({}^{}\) & 47.87\({}^{}\) & 61.13\({}^{}\) & 65.51\({}^{}\) \\  

Table 3: Ranking distillation results on the MSMARCO and NQ datasets. \({}^{}\) means significantly better result, performance against “Relevance Only” at the \(p<0.01\) level using a two-tailed \(t\)-test. Best model is in boldface and second best is underlined for each metric (except for the teacher).

We have the following observations: (1) There are generally fewer effective distillation methods on these tabular datasets than the text datasets, especially on Web30K. We hypothesize it is because of the dense relevance labels, so the benefits from teacher scores are less clear. (2) Listwise ranking losses such as Softmax and LambdaLoss are the most competitive.

### Overall Results

To get an overview of distillation methods, in Tbl. 6, we calculate their performance ranks on the 6 configurations, based on the NDCG@5 metric.

There are several trends: first, listwise ranking losses (Softmax, LambdaLoss, GumbelNDCG) that consider the entire list of documents and teacher score values are most competitive. PairMSE and MSE being next shows the importance of teacher score values over PairLog. Existing methods (RD and RankDistil) are not competitive despite they are effective distillation methods in general (outperforming no distillation baseline in most cases.)

   Model & Best Rank & Worst Rank & Mean Rank \\  Softmax & 1 & 3 & 1.8 \\ LambdaLoss & 1 & 5 & 2.7 \\ GumbelNDCG & 1 & 6 & 3.7 \\ PairMSE & 1 & 8 & 3.8 \\ MSE & 3 & 6 & 4.8 \\ PairLog & 4 & 7 & 5.0 \\ RankDistil & 4 & 8 & 6.7 \\ RD & 6 & 8 & 7.2 \\   

Table 6: The performance ranks of different distillation methods on the 6 task configurations. Smaller rank indicates better performance.

    &  &  \\  & MRR@10 & MRR & NDCG@1 & NDCG@5 & NDCG & MRR@10 & MRR & NDCG@1 & NDCG@5 & NDCG \\  Teacher & 45.27 & 46.02 & 31.08 & 48.94 & 55.49 & 45.27 & 46.02 & 31.08 & 48.94 & 55.49 \\  Relevance Only & 52.67 & 53.50 & 43.47 & 55.07 & 60.98 & n/a & n/a & n/a & n/a & n/a \\ RD & 54.66\({}^{}\) & 55.25\({}^{}\) & 44.25 & 57.34\({}^{}\) & 62.57\({}^{}\) & n/a & n/a & n/a & n/a & n/a \\ RankDistil & 54.17\({}^{}\) & 54.95\({}^{}\) & 44.19 & 56.88\({}^{}\) & 62.28\({}^{}\) & 40.01 & 41.10 & 27.67 & 42.95 & 51.25 \\ MSE & 56.03\({}^{}\) & 56.64\({}^{}\) & 46.12\({}^{}\) & 58.66\({}^{}\) & 63.64\({}^{}\) & 43.20 & 44.09 & 29.49 & 46.76 & 53.84 \\ PairLog & 56.23\({}^{}\) & 56.88\({}^{}\) & 46.58\({}^{}\) & 58.76\({}^{}\) & 63.79\({}^{}\) & 43.37 & 44.25 & 29.72 & 46.84 & 53.94 \\ PairMSE & 56.25\({}^{}\) & 56.89\({}^{}\) & 46.58\({}^{}\) & 58.91\({}^{}\) & 63.84\({}^{}\) & 43.45 & 44.35 & 29.82 & 47.05 & 54.05 \\ GumbelNDCG & 56.03\({}^{}\) & 56.70\({}^{}\) & 46.43\({}^{}\) & 58.72\({}^{}\) & 63.62\({}^{}\) & **44.00** & **44.87** & **30.85** & **47.38** & **54.39** \\ Softmax & **56.72\({}^{}\)** & **57.33\({}^{}\)** & **47.09\({}^{}\)** & **59.30\({}^{}\)** & **64.16\({}^{}\)** & 43.80 & 44.66 & 30.13 & 47.28 & 54.27 \\ LambdaLoss & 56.41\({}^{}\) & 57.06\({}^{}\) & 46.98\({}^{}\) & 58.80\({}^{}\) & 63.93\({}^{}\) & 43.78 & 44.61 & 30.22 & 47.26 & 54.23 \\   

Table 4: Distillation transfer results on the NQ dataset, using Teacher model fine-tuned on MS-MARCO. NQ-Zeroshot means relevance label on NQ is not available. \({}^{}\) means significantly better result, performanceed against “Relevance Only” at the \(p<0.01\) level using a two-tailed \(t\)-test. Best model is in boldface and second best is underlined for each metric (except for the teacher).

    &  &  \\  & MRR@10 & MRR & NDCG@1 & NDCG@5 & NDCG & MRR@10 & MRR & NDCG@1 & NDCG@5 & NDCG \\  Teacher & 34.43 & 35.05 & 48.39 & 47.33 & 71.62 & 84.54 & 84.59 & 71.69 & 67.92 & 81.69 \\  Relevance Only & 27.81 & 28.54 & 41.43 & 41.07 & **68.37** & 76.97 & 77.12 & 61.09 & 57.18 & 74.23 \\ RD & 27.44 & 28.19 & 41.17 & 40.92 & 68.32 & 77.01 & 77.16 & 61.36 & 57.35\({}^{}\) & 74.32\({}^{}\) \\ RankDistil & 27.82 & 28.58 & 41.47 & 41.03 & 68.33 & 77.15 & 77.30\({}^{}\) & 61.61\({}^{}\) & 57.41\({}^{}\) & 74.36\({}^{}\) \\ MSE & 27.44 & 28.19 & 41.37 & 40.92 & 68.31 & 77.31\({}^{}\) & 77.46\({}^{}\) & 61.89\({}^{}\) & 57.52\({}^{}\) & **74.44\({}^{}\)** \\ PairLog & 27.51 & 28.25 & 41.10 & 40.98 & 68.28 & 77.04 & 77.19 & 61.34 & 57.37\({}^{}\) & 74.32\({}^{}\) \\ PairMSE & 26.97 & 27.72 & 40.70 & 40.90 & 68.24 & 77.17 & 77.32 & 61.66\({}^{}\) & 57.51\({}^{}\) & 74.37\({}^{}\) \\ GumbelNDCG & 28.31\({}^{}\) & 29.05\({}^{}\) & 41.79 & **41.29** & 68.32 & 77.44\({}^{}\) & 77.63\({}^{}\) & **62.96\({}^{}\)** & 57.53\({}^{}\) & 74.06 \\ Softmax & **29.54\({}^{}\)** & **30.25\({}^{}\)** & **42.08** & 41.11 & 68.22 & 77.49\({}^{}\) & 77.64\({}^{}\) & 62.47\({}^{}\) & 57.69\({}^{}\) & 74.40\({}^{}\) \\ LambdaLoss & 29.31\({}^{}\) & 30.02\({}^{}\) & 42.03 & 41.13 & 68.30 & **77.59\({}^{}\)** & **77.75\({}^{}\)** & 62.56\({}^{}\) & **57.75\({}^{}\)** & **74.41\({}^{}\)** \\   

Table 5: Ranking distillation results on the Web30K and Istella datasets. \({}^{}\) means significantly better result, performanceed against “Relevance Only” at the \(p<0.01\) level using a two-tailed \(t\)-test. Best model is in boldface and second best is underlined for each metric (except for the teacher).

### The Role of Softmax Transformation

To better understand Q2 (How to handle teacher ranker scores?), in experiments we have a switch on whether to apply softmax transformation and report best performing configurations in the results. Here we peek into if softmax label transformation is enabled for each method.

We examine the MSMARCO and NQ ranking distillation experiments (T1) and observe the same behavior: softmax label transformation was selected by GumbelNDCG, Softmax, and LambdaLoss, but was _not_ selected for MSE and PairMSE. Note that PairLog and RD are indifferent to such order preserving transformation since they do not look at teacher score values, and Rankdistill has to use softmax transformation to sample distributions as done in the original paper.

Some observations are intuitive: cross-entropy based losses such as Softmax are ill-behaved with negative labels; GumbelNDCG and LambdaLoss consist of DCG computation that usually assumes positive labels. The behaviors of MSE and PairMSE are interesting - it indicates methods depending heavily on score values can be less effective with certain transformations. In fact, the performance gap is quite significant: the MRR@10 of MSE on NQ is 54.87 vs 58.49 with softmax transformation turned on and off, a difference between being highly competitive (3rd best result for that task) and not so. We provide more discussions in Appendix. Therefore, researchers should be cautious about label transformations on teacher ranker scores that have much more freedom that those in classification problems, and we provide more discussions in Sec. 6.

### The Learning Dynamics

We draw the ranking performance on validation data during model training to gain insight in the learning dynamics in Fig. 1. We can see that when compared against relevance labels that we care most, the Relevance Only baseline saturates fast, while distillation denoted as Softmax (using Softmax as the distill loss) performance keeps increasing with the help of distillation. Distillation can also get good ranking performance against teacher scores, but it is interesting that it does not have to keep increasing performance on teacher scores to increase performance on relevance labels. Relevance Only model's behavior on teacher scores is also interesting: the performance can even get worse during training (note the Relevance Only model does not have access to teacher scores). Our understanding is this indicates some overfitting issues that can not be caught by relevance labels.

## 6 Summary and Discussions

We provide a brief summary that answers the three questions we raised in Sec. 3. We then discuss topics that could be important for ranking distillation, but are not comprehensively covered in the current benchmark due to scope of this work.

**Summary**. For Q1: Where is the knowledge in ranking distillation, we performed comprehensive benchmark and found that the knowledge can exist in the score values from teacher rankers, challenging common assumptions made in the literature. We hope this can inspire future work in this direction. For Q2: How to handle teacher ranker scores, we showed the freedom of practical teacher rankers that is not well studied, and the effect of label transformations that were either ignored or implicitly considered in the literature. This problem also needs more future work considering the

Figure 1: The performance of Relevance Only and Softmax on relevance label and teacher scores for the NQ ranking distillation task. NDCG is used since teacher scores are real-valued.

importance of teacher score values in Q1. For Q3, we discussed drawbacks of existing efforts, and strive to provide a standardized criterion, by providing retrieved documents, teacher scores, student architectures, and the evaluation scripts.

**The role of teacher ranker**. We assume teacher rankers are given, which is the common setting in the literature, motivating us to release the teacher scores. However, it is still meaningful to investigate different teacher rankers and their effect in the distillation process. Some teacher ranker may not be state-of-the-art itself but may produce very competitive student ranker (as hinted in the distillation transfer experiments). Recently, there are studies around this in the classification setting .

**The role of teacher label transformation**. We formalized the importance of properly handling teacher ranker labels. We studied Softmax transformation motivated by traditional knowledge distillation and some existing work, but also argued that such transformation may not always be useful. An open question is if there exist better teacher label transformations.

**What tasks need more attention?** RD-Suite consists of a diverse set of tasks. However, we encourage researchers to focus more on the text ranking tasks (T1, T2, T3) for the following reasons: (1) The setting on tabular datasets can be useful from a research perspective but less from a practical perspective - MLP models may not be expensive to serve, and the community has yet to find ways to train large effective models on tabular datasets . Other models such as Gradient Boosted Decision Trees (GBDTs) are also highly effective alternatives . (2) Tabular datasets have dense relevance labels that may make distillation less useful. Also, having human annotated dense relevance labels is costly and not practical. (3) We can not study interesting topics such as distillation transfer since each tabular dataset has different manually designed numeric features.

On the other side, we believe the text ranking tasks are more realistic by using giant teacher models, allowing knowledge transfer, and benefiting more from distillation given sparse relevance labels. For future work, we are interested in extending RD-Suite to Large Language Model based rankers .

## 7 Related Work

Ranking distillation has drawn much attention in the community from different perspectives. The most relevant existing work are RD  and RankDistil  that we discussed in the paper, focusing on general distillation objectives, that can be applied to any model architectures and ranking applications.

Some work focus on architecture specific distillation or model compression. For example,  focused on BERT-based models and perform distillation on language model tasks (i.e., predictions on all tokens in the vocabulary). Both teacher and student rankers are BERT models, which is not very general.  distilled intermediate representations in BERT models, and simply assumed the teacher ranker was trained using pointwise loss and applied pointwise logistic loss for distillation. There is also a body of work on BERT specific distillation in general, not limited to ranking .

There is a suite of work on distillation for recommender systems that rank items for users, potentially to be served on mobile devices. These work either apply RD or RankDistil as a core component, or take a route specific to the recommendation problem.  was a parallel work as RankDistil and performed order preserving distillation.  studied student to teacher distillation while the distillation component followed RD. [34; 37] concerned about distillation of sequential recommender systems and also assumed simple teacher configurations and distillation methods (e.g., pointwise losses for both teacher and distillation).  mitigated popularity bias for distillation that is specific to recommender systems.

## 8 Conclusion

We proposed Ranking Distillation Suite (RD-Suite), a new benchmark for evaluating progress on ranking distillation research. Our new benchmark is challenging and probes at model capabilities in dealing with diverse data types and both regular ranking distillation and distillation transfer tasks. For the first time, we conduct an extensive side-by-side comparison of teachers, baseline without distillation, and eight distillation methods. The experimental results challenge common wisdom of prior art, show promises for certain methods, and leaves room for improvements. The benchmark also includes datasets with teacher labels and evaluation scripts to facilitate future benchmarking, research, and model development.