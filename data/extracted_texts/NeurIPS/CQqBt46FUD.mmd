# Unbiased Learning of Deep Generative Models

with Structured Discrete Representations

 Harry Bendekgey, Gabriel Hope, Erik B. Sudderth

{hbendekg, hopej, sudderth}@uci.edu

Department of Computer Science, University of California, Irvine

###### Abstract

By composing graphical models with deep learning architectures, we learn generative models with the strengths of both frameworks. The structured variational autoencoder (SVAE) inherits structure and interpretability from graphical models, and flexible likelihoods for high-dimensional data from deep learning, but poses substantial optimization challenges. We propose novel algorithms for learning SVAEs, and are the first to demonstrate the SVAE's ability to handle multimodal uncertainty when data is missing by incorporating discrete latent variables. Our memory-efficient implicit differentiation scheme makes the SVAE tractable to learn via gradient descent, while demonstrating robustness to incomplete optimization. To more rapidly learn accurate graphical model parameters, we derive a method for computing natural gradients without manual derivations, which avoids biases found in prior work. These optimization innovations enable the first comparisons of the SVAE to state-of-the-art time series models, where the SVAE performs competitively while learning interpretable and structured discrete data representations.

## 1 Introduction

Advances in deep learning have dramatically increased the expressivity of machine learning models at great cost to their interpretability. This trade-off can be seen in deep generative models that produce remarkably accurate synthetic data, but often fail to illuminate the data's underlying factors of variation, and cannot easily incorporate domain knowledge. The _structured variational autoencoder_ (SVAE, Johnson et al. ) aims to elegantly address these issues by combining probabilistic graphical models  with the VAE , gaining both flexibility and interpretability. But since its 2016 introduction, SVAEs have seen few applications because their expressivity leads to optimization challenges. This work proposes three key fixes that enable efficient training of general SVAEs.

SVAE inference requires iterative optimization  of variational parameters for latent variables associated with every observation. Johnson et al.  backpropagate gradients through this multi-stage optimization, incurring prohibitive memory cost. We resolve this issue via an implicit differentiation scheme that shows empirical robustness even when inference has not fully converged. Prior work  also identifies natural gradients  as an important accelerator of optimization convergence, but apply natural gradients in a manner that requires dropping parts of the SVAE loss, yielding biased learning updates. We instead derive unbiased natural gradient updates that are easily and efficiently implemented for any SVAE model via automatic differentiation.

Basic VAEs require carefully tuned continuous relaxations  for discrete latent variables, but SVAEs can utilize them seamlessly. We incorporate adaptive variational inference algorithms  to robustly avoid local optima when learning SVAEs with discrete structure, enabling data clustering. SVAE inference easily accommodates missing data, leading to accurate and multimodal imputations. We further improve training speed by generalizing prior work on parallel Kalman smoothers .

We begin in Sec. 2 and 3 by linking variational inference in graphical models and VAEs. Our optimization innovations (implicit differentiation in Sec. 4, unbiased natural gradients in Sec. 5, variational inference advances in Sec. 6) then enable SVAE models to be efficiently trained to their full potential. Although SVAEs may incorporate any latent graphical structure, we focus on temporal data. In Sec. 8, we are the first to compare SVAE performance to state-of-the-art recurrent neural network- and transformer-based architectures on time series benchmarks , and the first to demonstrate that SVAEs provide a principled method for multimodal interpolation of missing data.

## 2 Background: Graphical Models and Variational Inference

We learn generative models that produce complex data \(x\) via lower-dimensional latent variables \(z\). The distribution \(p(z|)\) is defined by a graphical model (as in Fig. 2) with parameters \(\), and \(z\) is processed by a (deep) neural network with weights \(\) to compute the data likelihood \(p_{}(x|z)\).

Exact evaluation or simulation of the posterior \(p_{}(z,|x)\) is intractable due to the neural network likelihood. _Variational inference_ (VI ) defines a family of approximate posteriors, and finds the distribution that best matches the true posterior by optimizing the _evidence lower bound_ (ELBO):

\[[q(;)q(z;),]=_{q(;)q(z; )}(x|z)}{q(;) q(z;)} p_{}(x).\] (1)

Here, \(q(;)q(z;) p_{}(z,|x)\) are known as _variational factors_. We parameterize these distributions via arbitrary exponential families with _natural parameters_\(,\). This implies that

\[q(z;)=\{,t(z)- Z()\},  28.452756ptZ()=_{z}\{,t(z)\}\;dz.\] (2)

An exponential family is log-linear in its sufficient statistics \(t(z)\), where the normalizing constant \(Z()\) ensures it is a proper distribution (see App. C.1 for properties of exponential families). For models where \(p_{}(x|z)\) has a restricted conjugate form (rather than a deep neural network), we can maximize Eq. (1) by alternating optimization of \(,\); these coordinate ascent updates have a closed form . _Stochastic VI_ improves scalability (for models with exponential-family likelihoods) by sampling batches of data \(x\), fitting a locally-optimal \(q(z;)\) to the latent variables in that batch, and updating \(q(;)\) by the resulting (natural) gradient.

**Amortized VI.** Because it is costly to optimize Eq. (1) with respect to \(\) for each batch of data, VAEs employ _amortized VI_ to approximate the parameters of the optimal \(q(z;)\) via a neural network _encoding_ of \(x\). The inference network weights \(\) for this approximate posterior \(q_{}(z|x)\) are jointly trained with the generative model. A potentially substantial _amortization gap_ exists : the inference network does not globally optimize the ELBO of Eq. (1) for all \(x\).

**Structured VAEs.** For a fixed \(q()\), the true optimizer (across all probability distributions) of Eq. (1) is given by \(q(z) p_{0}(z;)p_{}(x|z)\), where \(p_{0}(z;)\{_{q(;)}[ p(z|)]\}\) is an expected prior on \(z\) (see App. C.2 for derivations). This simple product of expected prior and likelihood cannot be normalized because of the neural network parameterization of \(p_{}(x|z)\). Rather than approximating

Figure 1: The SVAE-SLDS segments each sequence of human motion, which we display as a sequence of discrete colors. _Discrete variables are interpretable:_ Below each segmentation, we show five segmentations _of other_ subjects performing the same action, noting similarity across semantically similar series. _Discrete variables are compact representations:_ Drawing multiple samples from the generative model conditioned on ground-truth segmentations yields the stick figures in grey, which track closely with the observed data.

this whole posterior as in amortized VI, the SVAE  only approximates the likelihood function, and explicitly multiplies it by the known expected prior to obtain an approximate posterior.

In more detail, the SVAE approximates the likelihood \((z)=p_{}(x|z)\) with a function parameterized by a neural network \(_{}(z|x)\). The optimal posterior _given this approximation_ is then equal to

\[q_{}(z|x;)=*{arg\,max}_{q(z)}}[q(; )q(z),]=*{arg\,max}_{q(z)}_{q(;)q(z)} _{}(z|x)}{q(;)q( z)}.\] (3)

The posterior that optimizes the _surrogate loss_\(}[,]\) of Eq. (3) is \(q_{}(z|x;) p_{0}(z;)_{}(z|x)\). If \(_{}(z|x)\) is chosen to be conjugate to \(p(z|)\), this multiplication and normalization is easy for many exponential families. Note that \(_{}(z|x)\) does not need to be normalized, as any multiplicative factors would disappear when the posterior is normalized. The overall ELBO is then \([q(;)q_{}(z|x;),]\).

If the encoder's approximate likelihood \(_{}\) is (up to normalization) close to the true likelihood \(\), the surrogate loss \(}\) will closely approximate the true loss \(\), and there will be little amortization gap. SVAE inference has the advantage that \(q_{}(z|x;)\) depends on the learned posterior \(q(;)\) of graphical model parameters, so as learning improves the graphical generative model, the coupled inference model remains closely aligned with the generative process. The ladder VAE  and related hierarchical VAEs  also incorporate generative parameters in amortized variational inference, but impose a restricted generative hierarchy that is less flexible and interpretable than the SVAE.

**Example 1: Standard Normal.** For a basic VAE, \(\) is fixed and \(z(0,I)\). The SVAE inference network outputs a Gaussian \(_{}(z=(x;),=(x;))(z ;,(^{-1}))\) with mean \(\) and inverse-variance (precision) \(\). The product of this Gaussian with the standard normal prior has a simple form: \(q_{}(z|x)=(z;,((+1)^{-1}))\). This reparameterization of the standard VAE posterior imposes the (useful) constraint that posterior variances must be smaller than prior variances.

**Example 2: Linear Dynamical System (LDS).** A LDS model for temporal data assumes the latent _state_ variables evolve linearly with Gaussian noise: \(z_{t}(A_{t}z_{t-1}+b_{t},Q_{t})\), \(z_{1}(_{1},_{1})\). In this case, we expand the exponential family distribution \(q(z;)\) as a sum across time steps:

\[q(z;)=q(z_{1})_{t=2}^{T}q(z_{t}|z_{t-1})=_ {1},t(z_{1})+_{t=2}^{T}_{t},t(z_{t-1},z_{t})-  Z()},\] (4)

where \(=(_{t})\) and the prior \(p(z|)\) belongs to this family. The prior induces temporal dependence between the \(z_{t}\), but we assume the likelihood factorizes as \(p_{}(x|z)=_{t}p_{}(x_{t}|z_{t})\).

For models like the LDS, approximating the likelihood function with conjugate, time-independent Gaussian distributions is a much simpler task than approximating the temporally-coupled posterior. In addition, as the generative parameters \(A_{t},b_{t},Q_{t}\) of \(p(z|)\) are learned through optimization of \(q()\), the inference routine shares those parameters, improving accuracy. These advantages were not present in the standard normal VAE, where the prior on \(z\) lacks structure and is not learned. Inference, normalization, and sampling of \(q_{}(z|x;)\) in the LDS model is feasible via a Kalman smoothing algorithm  that efficiently (with cost linear in \(T\)) aggregates information across time steps.

Figure 2: _Left:_ Generative (above) and inference (below) graphical models for SVAE-LDS and SVAE-SLDS. For the SLDS, we show the prior and posterior as factor graphs . \(q_{}(z|x)\) combines potentials from the inference network with the true prior. Structured variational inference separates continuous from discrete latent variables for tractability, and mean field messages are propagated across residual edges between disentangled factors. _Right:_ General form of iterative mean-field (MF) and belief-propagation (BP) updates for SVAE inference in a model where \(q(z)\) factorizes into \(M\) groups of latent variables. For the SVAE-SLDS, \(M=2\).

## 3 Structured Variational Inference

For complex graphical models, the distributions \(p(z|)\) and \(q(z)\) typically factorize across subsets of the latent variables \(z\), as illustrated in Fig. 2. We thus generalize Eq. (4) by partitioning \(z\) into local variable groups, and representing the dependencies between them via a set of factors \(\):

\[q(z;)=_{f}_{f},t(z_{f}) - Z()}.\] (5)

For certain factor graphs, we can efficiently compute marginals and draw samples via the _belief propagation_ (BP) algorithm [48; 38]. However, exact inference is intractable for many important graphical models, making it impossible to compute marginals or normalize the \(q_{}(z|x;)\) defined in Sec. 2. SVAE training addresses this challenge via _structured_ variational inference [20; 63; 62], which optimizes the surrogate loss across a restricted family of tractable distributions. We connect structured VI to SVAEs in this section, and provide detailed proofs in App. C.2.

### Background: Block Coordinate Ascent for Mean Field Variational Inference

Let \(\{z_{m}\}_{m=1}^{M}\) be a partition of the variables in the graphical model, chosen so that inference within each \(z_{m}\) is tractable. We infer factorized (approximate) marginals \(q_{}(z_{m}|x;)\) for each mean field cluster by maximizing \(}[q(;)_{m}q(z_{m}),]\). The optimal \(q_{}(z_{m}|x;)\) inherit the structure of the joint optimizer \(q_{}(z|x;)\), replacing any factors which cross cluster boundaries with factorized approximations (see Fig. 2). The optimal parameters for these disentangled factors are a linear function of the expected statistics of clusters connected to \(m\) via residual edges. These expectations in turn depend on their clusters' parameters, defining a stationary condition for the optimal \(\):

\[_{m}=}(_{-m};),_{m}=}(_{ m};,,x).\] (6)

Here, BP is a belief propagation algorithm which computes expected statistics \(_{m}\) for cluster \(m\), and the linear _mean field_ function MF updates parameters of cluster \(m\) given the expectations of _other_ clusters \(_{-m}\) along residual edges. We solve this optimization problem via the _block updating_ coordinate ascent in Alg. 1, which is guaranteed to converge to a local optimum of \(}[q()_{m}q(z_{m})]\).

### Reparameterization and Discrete Latent Variables

While optimizing \(q_{}(z_{m}|x;)\) at inference time requires some computational overhead, it allows us to bypass the typical obstacles to training VAEs with discrete latent variables. To learn the parameters \(\) of the inference network, conventional VAE training backpropagates through samples of latent variables via a smooth reparameterization , which is impossible for discrete variables. Many alternatives either produce biased gradients  or extremely high-variance gradient estimates [50; 28]. Continuous relaxations of discrete variables [44; 27; 7] produce biased approximations of the true discrete ELBO, and are sensitive to annealing schedules for temperature hyperparameters.

SVAE training only requires reparameterized samples of those latent variables which are direct inputs to the generative network \(p_{}(x|z)\). By restricting these inputs to continuous variables, and using other discrete latent variables to capture their dependencies, discrete variables are marginalized via structured VI _without_ any need for biased relaxations. With a slight abuse of notation, we will denote continuous variables in \(z\) by \(z_{m}\), and discrete variables by \(k_{m}\).

Example 3: Gaussian Mixture.Consider a generalized VAE where the state is sampled from a mixture model: \(k()\), \(z(_{k},_{k})\). The likelihood \(p_{}(x|z)\) directly conditions on only the continuous latent variable \(z\). Variational inference produces disentangled factors \(q_{}(z|x;)q_{}(k|x;)\), and we evaluate likelihoods by decoding samples from \(q_{}(z|x;)\), without sampling \(q_{}(k|x)\).

Example 4: Switching Linear Dynamical System (SLDS).Consider a set of discrete states which evolve according to a Markov chain \(k_{1}(_{0}),k_{t}(_{k_{t-1}})\), and a continuous state evolving according to switching linear dynamics: \(z_{0}(_{0},_{0}),z_{t}(A_{k_{t}}z_{t-1} +b_{k_{t}},Q_{k_{t}})\). The transition matrix, offset, and noise at step \(t\) depends on \(k_{t}\). Exact inference in SLDS is intractable , but structured VI  learns a partially factorized posterior \(q_{}(z|x;)q_{}(k|x;)\) that exactly captures dependencies _within_ the continuous and discrete Markov chains.

BP for SLDS uses variational extensions [5; 4] of the Kalman smoother to compute means and variances of continuous states, and forward-backward message-passing to compute marginals of discrete states (see App. D). Let \(k_{tj}=1\) if the SLDS is in discrete state \(j\) at time \(t\), \(k_{tj}=0\) otherwise,and \(_{j}=_{q(;]}[_{j}]\) be the expected (natural) parameters of the LDS for discrete state \(j\). Structured VI updates the natural parameters of discrete states \(_{k_{tj}}\), and continuous states \(_{z_{t},z_{t+1}}\), as follows:

\[_{z_{t},z_{t+1}}=_{j}_{q}[k_{tj}]_{j}, _{k_{tj}}=_{j},_{q}[t(z_{t-1},z_{t})].\] (7)

## 4 Stable and Memory-Efficient Learning via Implicit Gradients

When \(q_{}(z|x)\) is computed via closed-form inference, gradients of the SVAE ELBO may be obtained via automatic differentiation. This requires backpropagating through the encoder and decoder networks, as well as through reparameterized sampling \(z q_{}(z|x;)\) from the variational posterior.

For more complex models where structured VI approximations are required, gradients of the loss become difficult to compute because we must backpropagate through Alg. 1. For the SLDS this _unrolled_ gradient computation must backpropagate through repeated application of the Kalman smoother and discrete BP, which often has prohibitive memory cost (see Table 1).

We instead apply the _implicit function theorem_ (IFT ) to compute implicit gradients \(\), \(\) without storing intermediate states. We focus on gradients with respect to \(\) for compactness, but gradients with respect to \(\) are computed similarly. Let \(^{(1)},,^{(L)}\) be the sequence of \(\) values produced during the "forward" pass of block coordinate ascent, where \(^{(L)}\) are the optimized structured VI parameters. The IFT expresses gradients via the solution of a set of linear equations:

\[}{}=^{-1}, g()=-((; ,,x);).\] (8)

Here we apply the BP and MF updates in _parallel_ for all variable blocks \(m\), rather than sequentially as in Eq. (6). At a VI fixed point, these parallel updates leave parameters unchanged and \(g()=0\).

For an SLDS with latent dimension \(D\) and \(K\) discrete states, \(\) has \(O(K+D^{2})\) parameters at each time step. Over \(T\) time steps, \(\) is thus a matrix with \(O(T(D^{2}+K))\) rows/columns and \(O(T^{2}D^{2}K)\) non-zero elements. For even moderate-sized models, this is infeasible to explicitly construct or solve.

We numerically solve Eq. (8) via a Richardson iteration [54; 64] that repeatedly evaluates matrix-vector products \((I-A)v^{}\) to solve \(A^{-1}v\). Such numerical methods have been previously used for other tasks, like hyperparameter optimization  and meta-learning , but not for the training of SVAEs. The resulting algorithm resembles unrolled gradient estimation, but we repeatedly backpropagate through updates at the _endpoint_ of optimization instead of along the optimization trajectory.

\[}{} -_{j=0}^{J}I-; ,,x)}{}^{j};, ,x)}{}.\] (9) \[ }{} -_{=0}^{L}_{i=}^{L}I- ;,,x)}{} ;,,x)}{}.\] (10)

Lorraine et al.  tune the number of Richardson steps \(J\) to balance speed and accuracy. However, there is another reason to limit the number of iterations: when the forward pass is not iterated until convergence, \(^{(L)}\) is not a stationary point of \(g()\) and therefore Eq. (9) is not guaranteed to converge as \(J\). For batch learning, waiting for _all_ VI routines to converge to a (local) optimum might be prohibitively slow, so we might halt VI before \(^{(L)}\) converges to numerical precision.

Seeking robustness even when the forward pass has not converged, we propose a _capped implicit_ gradient estimator that runs one Richardson iteration for every step of forward optimization, so that

   &  \\  & \(B=1\) & \(B=32\) & \(B=64\) & \(B=128\) \\  Implicit + Parallel & 603 & 922 & 1290 & 2060 \\ Unrolled + Parallel & 659 & 1080 & n/a & n/a \\  Implicit + Sequential & 2560 & 3160 & 3290 & 3530 \\ Unrolled + Sequential & 2660 & 3290 & 3980 & n/a \\  

Table 1: Time of ELBO backpropagation in an SVAE-SLDS with \(K=50\) discrete states, dimension \(D=16\), and \(T=250\) time steps. For varying batch sizes \(B\), we compare _capped implicit_ gradients to unrolled gradients for \(L=10\) block updates of two inference algorithms: standard sequential BP, and our parallel extension. For large batch sizes, unrolled gradients crashed because it attempted to allocate more than 48GB of GPU memory.

\(J=L\). In this regime, implicit gradient computation has a one-to-one correspondence to unrolled gradient computation, while requiring a small fraction of the memory. This can be thought of as a form of gradient regularization: if we take very few steps in the forward pass, we should have low confidence in the optimality of our end-point and compute fewer terms of the Neumann series (9).

**Experiments.** In Fig. 3 (left, middle) we compute the accuracy of different approximate gradient estimators for training a SVAE-SLDS as in Fig. 2. To our knowledge, we are the first to investigate the quality of implicit gradients evaluated away from an optimum, and we compare our _capped implicit_ proposal to other gradient estimators. Ground truth gradients are computed as the implicit gradient at the optimum, and we compare the root-mean-squared-error (rMSE) of various gradient estimators to that of the naive _No-Solve_ solution, which replaces the inverted matrix in Eq. (8) with Identity.

We consider two models: one with randomly initialized parameters, and one that has been trained for 20 epochs. The newly-initialized model requires more forward steps for the block updating routine to converge. We compare the memory-intensive unrolled estimator (_Unrolled_) to three versions of the implicit gradient estimator. First, an uncapped version (_Implicit_) always performs \(J=50\) Richardson iterations regardless of the number of forward iterations, thus incurring high computation time. Note that evaluating implicit gradient far from an optimum can produce high error; in the newly-initialized model, many of these iterations diverge to infinity when fewer than 20 forward steps are taken. Second, we consider a capped implicit estimator (_Implicit+Cap_) which sets \(J=L\) to match the number of forward steps. Finally, we consider a capped implicit estimator which also includes a threshold (_Implicit+Cap+Threshold_): if the forward pass has not converged in the specified number of steps, the thresholded estimator simply returns the _No-Solve_ solution. This gradient is stable in all regimes while retaining desirable asymptotic properties . Our remaining experiments therefore use this method for computing gradients for SVAE training.

**Prior work.** Johnson et al.  consider implicit differentiation, but only very narrowly. They derive implicit gradients by hand in cases (like the LDS) where exact inference is tractable, so the linear solve in Eq. (8) cancels with other terms, and gradients may be evaluated via standard automatic differentiation. For models requiring structured VI (like the SLDS),  instead computes _unrolled_ gradients for inference network weights \(\), suffering high memory overhead. They compute neither unrolled nor implicit gradients with respect to generative model parameters \(\); in practice they set the gradient of the inner optimization to 0, yielding a biased training signal. Our innovations instead enable memory-efficient and unbiased gradient estimates for all parameters, for all graphical models.

## 5 Rapid Learning via Unbiased Natural Gradients

SVAE training must optimize the parameters of probability distributions. Gradient descent implicitly uses Euclidean distance as its notion of distance between parameter vectors, which is often a poor indicator of the divergence between two distributions. The natural gradient  resolves this issue by rescaling the gradient by the Fisher information matrix \(F_{}\) of \(q(;)\), given by:

\[F_{}=_{q(;)}_{}q(;) _{}q(;)^{T}.\] (11)

Figure 3: We compare implicit gradient estimators’ stability (left, middle), and gradient conditioning methods’ loss trajectory (right), on human motion capture data (Sec. 8). _Stability:_ Gradient estimate rMSE relative to the _No-Solve_ estimator (smaller is better) for various numbers of VI block updates \(L\), and SVAE-SLDS models taken from the start of training (left) and after 20 epochs (middle). Solid lines show median rMSE ratio across a batch of 128 data points, and dashed lines show \(90^{}\) percentiles. _Conditioning:_ Convergence of SVAE-LDS negative-ELBO loss versus number of optimization steps (log-scale) for conventional (non-natural) gradients, biased natural gradients , and unbiased natural gradients computed via automatic differentiation (Sec. 5).

Johnson et al.  demonstrate the advantages of natural gradients for the SVAE, drawing parallels to the natural gradients of stochastic VI (SVI ). SVI extends the variational EM algorithm to mini-batch learning: similar to the SVAE, it fits \(q(z)\) in an inner optimization loop and learns \(q(;)\) in an outer loop by natural gradient descent. The key difference between SVI and the SVAE is that SVI's inner optimization is done with respect to the true loss function \(\), whereas the SVAE uses a surrogate \(}\). SVI can only do this inner optimization by restricting all distributions to be conjugate exponential family members, giving up the flexibility provided by neural networks in the SVAE.

Let \(_{}\) be the expected sufficient statistics of \(q(;)\). Exponential family theory tells us that \(=F_{}\)[30; 45], allowing Johnson et al.  to derive the natural gradients of the SVAE loss:

\[}{}F_{}^{-1}=}{}F_{}^{- 1}=}{},}{}=+_{q_{}(z|x;)}[t(z)]-}^{}+}{}}^{}\.\] (12)

This gradient differs from the SVI gradient by the final term: because SVI's inner loop optimizes \(\) with respect to the true loss \(\), \(}{}=0\) for conjugate models. Johnson et al.  train their SVAE by dropping the correction term and optimizing via the SVI update equation, yielding biased gradients.

There are two challenges to computing unbiased gradients in the SVAE. First, in the structured mean field case \(\) involves computing an implicit or unrolled gradient, as addressed by our numerical methods in Sec. 4. Second, including the correction term in the gradient costs us a desirable property of the SVI natural gradient: for step size less than 1, any constraints on the distribution's natural parameters are guaranteed to be preserved, such as positivity or positive-definiteness.

We resolve this issue by reparameterizing \(\) into an unconstrained space, and computing natural gradients with respect to those new parameters. Letting \(\) be an unconstrained reparameterization of \(\), such as \(=\{\}=(1+e^{})\) for a positive precision parameter, we have:

\[}{}F_{}^{-1}=}{}}^{-T}=(}{}_{} )^{T}.\] (13)

See App. F for proof. This differs from the non-natural gradient in two ways. First, the Jacobian of the \(\) map is dropped, as before. Unlike Johnson et al. , we do not hand-derive the solution; we employ a _straight-through gradient estimator_ to replace this Jacobian with the identity. Then, the Jacobian of the \(\) map is replaced by its inverse-transpose. This new gradient can be computed without any matrix arithmetic by noting that the inverse of a Jacobian is the Jacobian of the inverse function. Thus Eq. (13) can be computed by replacing the reverse-mode backpropagation through the \(\) map with a forward-mode differentiation through the inverse \(\) map.

In Fig. 3 (right) we show the performance benefits of our novel unbiased natural gradients with stochastic gradient descent, compared to regular gradients with an Adam optimizer , and stochastic gradient descent via biased natural gradients  that drop the correction term. Results are shown for an SVAE-LDS model whose pre-trained encoder and decoder are fixed.

## 6 Adapting Graphical Model Innovations

Efficient implementations of BP inference, parameter initializations that avoid poor local optima, and principled handling of missing data are well-established advantages of the graphical model framework. We incorporate all of these to make SVAE training more efficient and robust.

Parallel inference.The BP algorithm processes temporal data sequentially, making it poorly suited for large-scale learning of SVAEs on modern GPUs. Sarkka & Garcia-Fernandez  developed a method to parallelize the usually-sequential Kalman smoother algorithm across time for jointly Gaussian data. Their algorithm is not directly applicable to our VI setting where we take expectations over \(q()\) instead of having fixed parameters \(\), but we derive an analogous parallelization of variational BP in App. D.2. We demonstrate large speeds gains from this adaptation in Table 1.

Initialization.Poor initialization of discrete clusters can cause SLDS training to collapse to a single discrete state. This problem becomes worse when the graphical model is trained on the output of a neural network encoder, which when untrained produces outputs which do not capture meaningful statistics of the high-dimensional data. We therefore propose a three-stage training routine: a basic VAE is trained to initialize \(p_{}(x|z)\), and then the output of the corresponding inference network is used for variational learning of graphical model parameters . Once the deep network and graphical model are sensibly initialized, we refine them via joint optimization while avoiding collapse. For details of this initialization scheme, see App. A.5.

Missing data.The structure provided by the SVAE graphical model allows us to solve marginal and conditional inference queries not seen at training time. In particular, we explore the ability of a trained SVAE to impute data that is missing for an extended interval of time. By simply setting \(_{}(z_{t}|x_{t};)\) to be uniform at a particular timestep \(t\), our posterior estimate of \(z_{t}\) is only guided by the prior, which aggregates information across time to produce a smooth estimate of the posterior on \(z_{t}\).

While discriminative methods may be explicitly trained to impute time series, we use imputation performance as a measure of generative model quality, so do not compare to these approaches. Unlike discriminative methods, SVAE imputation does _not_ require training data with aligned missing-ness.

## 7 Related Work

Dynamical VAEs.Girin et al. (2018) provide a comprehensive survey of dynamical VAEs (DVAEs) for time series data, which use recurrent neural networks to model temporal dependencies. The _Stochastic Recurrent Neural Network_ (SRNN (Krizhevsky et al., 2012)), which has similar structure to the _Variational Recurrent Neural Network_ (VRNN (Krizhevsky et al., 2012)), is the highest-performing model in their survey; it models data via one-step-ahead prediction, producing probabilities \(p(x_{t}|z,x_{t-1})\). This model therefore reconstructs \(x\) using more information than is encoded in \(z\) by skipping over the latent state and directly connecting ground truth to reconstruction, reducing the problem of sequence generation to a series of very-local one-step predictions. On the other hand, the _Deep Kalman Smoother_ (DKS (Kalman and Silver, 2015)) extension of the Deep Kalman Filter (Kalman and Silver, 2015) is the best-performing model which generates observations independently across time, given only information stored in the latent encoding \(z\).

RNNs lack principled options for handling missing data. Heuristics such as constructing a dummy neural network input of all-zeros for unobserved time steps, or interpolating with exponentially decayed observations (Han et al., 2017), effectively require training to learn these imputation heuristics. RNNs must thus be trained on missing-ness that is similar to test missing-ness, unlike the SVAE.

Transformers (Srivastava et al., 2015) have achieved state-of-the-art generative performance on sequential language modeling. However, Zeng et al. (2016) argue that their permutation-invariance results in weak performance for time-series data where each observation carries low semantic meaning. Unlike text, many time series models are characterized by their temporal dynamics rather than a collection of partially-permutable tokens. Lin et al. (2017) propose a dynamical VAE with encoder \(q(z_{t}|x_{1:T})\), decoder \(p(x_{t+1}|x_{1:t},z_{1:t+1})\), and latent dynamics \(p(z_{t+1}|x_{1:t},z_{1:t})\) parameterized by transformers.

Structured VAEs.We, as in Johnson et al. (2017), only consider SVAEs where the inference network output factorizes across (temporal) latent variables. Orthogonal to our contributions, Yu et al. (2017) investigate the advantages of taking the SVAE beyond this restriction, and building models where the recognition network outputs proxy-likelihood functions on _groups_ of latent variables.

In recent independent work, Zhao & Linderman (2017) also revisit the capabilities of the SVAE. However, their work differs from ours in a few key respects. First, because their experiments are restricted to the LDS graphical model (which requires no mean field factorization nor block updating), they do not need implicit differentiation, and do not explore the capacity of the SVAE to include discrete latent variables. Second, because they optimize point-estimates \(\) of parameters instead of variational factors \(q()\), they do not make use of natural gradients. In this point-estimate formulation,

Figure 4: Unconstrained generation of 513-dim. speech spectrogram data over \(T=500\) time-steps (horizontal; models are trained on data with \(T=50\)). An example sequence of real speech data is shown. For models which use discrete latent variables, the sequence of discrete states is shown as a changing colorbar beneath the generation, with a solid colorbar meaning a constant discrete state for the entire sequence.

they apply the parallel Kalman smoother  off-the-shelf, whereas we derive a novel extension for our variational setting. Finally, their experimental results are confined to toy and synthetic data sets.

The most directly comparable model to the SVAE is the _Stochastic Inference Network_ (SIN ), which employs a graphical model prior \(p(z|)\) but estimates \(q(z)\) through traditional amortized inference; a parameterized function that shares no parameters with the graphical model produces variational posteriors. The authors consider discrete latent variable models like the SLDS, but due to the intractability of discrete reparameterization, their inference routine fits the continuous latent variables with a vanilla LDS. Thus training pushes the SIN to reconstruct, and therefore model, the data without the use of switching states. (Experiments in  consider only the LDS, not the SLDS.)

The Graphical Generative Adversarial Network  integrates graphical models with GANs for structured data. Experiments in  solely used image data; we compared to their implementation, but it performed poorly on our time-series data, generating unrecognizable samples with huge FID.

Vector Quantization.Vector-Quantized VAEs (VQ-VAEs ) are another family of deep generative models that make use of discrete latent representations. Rather than directly outputting an estimated posterior, the encoder of a VQ-VAE outputs a point in an embedding space which is _quantized_ to one of \(K\) learnable quantization points. The encoder is trained using a straight-through estimated . While discrete SVAE-SLDS states switch among multiple continuous modes, the VQ-VAE representation is purely discrete, limiting information encoded by a latent variable to \(_{2}K\) bits. In order to generate plausible and diverse samples, VQ-VAEs require large values of \(K\), structured collections of discrete variables, and/or post-hoc training of autoregressive priors [51; 52; 16; 15].

## 8 Experiments

We compare models via their test likelihoods, the quality of generated data, and the quality of interpolations. We consider joint positions from human motion capture data (MOCAP [10; 26]) and audio spectrograms from recordings of people reading Wall Street Journal headlines (WSJ0 ); see Table 2. MOCAP has \(84\)-dimensional data and training sequences of length \(T=250\). WSJ0 has \(513\)-dimensional data and training sequences of length \(T=50\). See App. A.2 for further details.

Generation quality is judged via a modified _Frechet inception distance_ (FID ) metric. We replace the InceptionV3 network with appropriate classifiers for motion capture and speech data (see App. A.3). SVAE-SLDS-Bias runs the SVAE as presented by Johnson et al. , with unrolled gradients, dropped correction term, sequential Kalman smoother, and no pre-training scheme. We match encoder-decoder architectures for all SVAE and SIN models using small networks (about 100,000 total parameters for motion data). The DKS, SRNN, and Transformer DVAE have approximately 300,000, 500,000, and 1.4 million parameters each; see App. A.5 for details.

To demonstrate the SVAE's capacity to handle discrete latent variables in a principled manner, we compare to two DVAE baselines which incorporate discrete variables via biased gradients: the straight-through estimator  and the concrete (Gumbel-softmax) distribution [27; 44]. To our knowledge, no one has successfully integrated either method into dynamical VAEs for temporal data. Thus to make comparison possible, we have devised a new model which adds discrete latent variables to the generative process of the DKS. Specifications for this model is provided in App. A.4. We evaluated this model with both gradient estimators, and reported the results in Table 2.

Figure 5: Interpolations of human motion capture data. Red figures (covering 150 time steps) are generated by each model to interpolate between the black figures four times. We see that our SVAE-SLDS provides varied and plausible imputations with corresponding segmentations (colors shared with Fig. 1). During training the SIN-SLDS  collapses to only use a single discrete state, and thus cannot produce diverse imputations. The SRNN  produces varied sequences, but autoregressive generation is sometimes unstable and unrealistic, and its inability to account for future observations prevents smooth interpolation with the observed sequence end.

Interpretability.In Fig. 1 we show the SVAE-SLDS's learned discrete encoding of several joint tracking sequences. While "sitting" sequences are dominated by a single dynamic mode, walking is governed by a cyclic rotation of states. "Posing" and "Taking photo" contain many discrete modalities which are shared with other actions. The discrete sequences provide easily-readable insight into sequences, while also compactly encoding the high-dimensional data.

Generation.In Fig. 4 we show example generated sequences from each model of audio data. Like true speech, the SVAE-SLDS moves between discrete structures over time representing individual sounds. In contrast, the SIN-SLDS  and DKS+Concrete baselines collapse to a single discrete modality, blending together continuous dynamics. While the DKS+Straight-Through model does not collapse, it uses discrete states too coarsely to inform the high-frequency dynamics of speech.

Interpolation.Amortized VI cannot easily infer \(q(z_{t})\) at time steps where the observations \(x_{t}\) are missing. Thus, given observations at a subset of times \(x_{}\), we can encode to obtain \(q(z_{})\) and infer the missing latent variables by drawing from the generative prior \(p(z_{}|z_{},)\). Because baseline models parameterize \(p(z|)\) by one-directional neural networks, they can only condition \(z_{}\) on \(z_{}\) at _previous_ time steps, leading to discontinuity at the end of the missing window. For further specifications and for details of the SVAE approach described in Sec. 6, see App. A.6. An alternative approach of in-filling missing data with zeros causes models to reconstruct the zeros; see App. Fig. 7.

In Fig. 5, we see the SVAE-SLDS uses discrete states to sample variable interpolations, while the SRNN's one-step-ahead prediction scheme cannot incorporate future information in imputation, producing discontinuities. We also note that despite achieving the highest test likelihoods, the SRNN produces some degenerate sequences when we iterate next-step prediction, and has inferior FID (see Table 2). The SIN-SLDS collapses to a single discrete state in training, resulting in uniform imputations that lack diversity. Example imputations for all models are provided in App. Fig. 6.

Transformers for time series.The permutation-invariance of transformers is visible in its varied performance on these two tasks. A lack of sequential modeling can lead to discontinuities in the data sequence which are naturally present in speech. For MOCAP, joint locations are continuous across time, making transformer-generated samples unrealistic (see Fig. 6 and Table 2).

## 9 Discussion

The SVAE is uniquely situated at the intersection of flexible, high-dimensional modeling and interpretable data clustering, enabling models which both generate data and help us understand it. Our optimization innovations leverage automatic differentiation for broad applicability, and provide the foundation for learning SVAEs with rich, non-temporal graphical structure in other domains.

   &  &  \\  & & Sample FID (L) & 0.0-0.8 & 0.2-1.0 & 0.2-0.8 \\   \\  SVAE-SLDS & 2.39 & \(\) & \(\) & \(\) & \(\) \\ SVAE-SLDS-Bias  & 2.36 & \(34.6 0.7\) & \(28.8 0.2\) & \(25.8 0.3\) & \(6.71 0.12\) \\ SVAE-LDS & 2.28 & \(34.0 0.3\) & \(19.3 0.2\) & \(21.9 0.2\) & \(7.90 0.13\) \\  SIN-SLDS  & 2.36 & \(33.7 0.4\) & \(12.38 0.12\) & \(89.7 0.08\) & \(3.27 0.05\) \\ SIN-LDS  & 2.33 & \(65.2 1.4\) & \(18.3 0.2\) & \(15.5 0.2\) & \(6.24 0.09\) \\  Transformer  & 2.82 & \(421 11\) & \(234 9\) & \(228 5\) & \(113 5\) \\ SRNN  & **2.94** & \(62.7 0.7\) & \(43.5 0.7\) & \(24.2 0.6\) & \(14.2 0.3\) \\ DKS  & 2.31 & \(136 6\) & \(46.7 1.7\) & \(33.3 1.1\) & \(9.0 0.3\) \\ DKS+Concrete & 1.70 & \(144 3\) & \(88 3\) & \(89 2\) & \(34.0 1.4\) \\ DKS+Straight-Through & 2.09 & \(\) & \(22.6 0.3\) & \(17.15 0.14\) & \(13.8 0.2\) \\   \\  SVAE-SLDS & 1.54 & \(\) & \(\) & \(\) & \(\) \\ SVAE-SLDS-Bias & 1.45 & \(18.6 0.2\) & \(15.0 0.2\) & \(15.2 0.2\) & \(7.6 0.12\) \\ SVAE-LDS & 1.56 & \(19.1 0.3\) & \(17.9 0.2\) & \(16.6 0.3\) & \(7.2 0.3\) \\  SIN-SLDS & 1.53 & \(20.0 0.4\) & \(17.2 0.3\) & \(14.9 0.3\) & \(9.5 0.2\) \\ SIN-LDS & 1.54 & \(17.8 0.2\) & \(17.21 0.11\) & \(13.2 0.4\) & \(10.1 0.2\) \\  Transformer & 1.88 & \(10.0 0.2\) & \(12.0 0.3\) & \(\) & \(5.7 0.4\) \\ SRNN & **1.94** & \(23.6 0.3\) & \(19.4 0.5\) & \(17.4 0.3\) & \(12.7 0.4\) \\ DKS & 1.55 & \(12.9 0.2\) & \(10.8 0.2\) & \(10.8 0.14\) & \(7.7 0.05\) \\ DKS+Concrete & 1.45 & \(16.6 0.2\) & \(12.8 0.2\) & \(11.3 0.2\) & \(8.2 0.2\) \\ DKS+Straight-Through & 1.48 & \(15.51 0.13\) & \(10.07 0.11\) & \(9.02 0.18\) & \(6.29 0.13\) \\  

Table 2: Comparison of model performance on log-likelihood (higher is better), FIDs of unconditionally generated samples (lower is better), and FIDs of interpolations on augmented human motion capture and audio spectrogram data. Each interpolation column corresponds to a masking regime where the shown range of percentiles of the data is masked, e.g. 0.0-0.8 means the first 80% of time steps are masked.