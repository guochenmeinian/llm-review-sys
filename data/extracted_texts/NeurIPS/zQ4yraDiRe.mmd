# Multi-scale Diffusion Denoised Smoothing

Jongheon Jeong Jinwoo Shin

Korea Advanced Institute of Science and Technology (KAIST)

Daejeon, South Korea

{jongheonj,jinwoos}@kaist.ac.kr

###### Abstract

Along with recent diffusion models, randomized smoothing has become one of a few tangible approaches that offers adversarial robustness to models at scale, _e.g._, those of large pre-trained models. Specifically, one can perform randomized smoothing on any classifier via a simple "denoise-and-classify" pipeline, so-called _denoised smoothing_, given that an accurate denoiser is available - such as diffusion model. In this paper, we present scalable methods to address the current trade-off between certified robustness and accuracy in denoised smoothing. Our key idea is to "selectively" apply smoothing among multiple noise scales, coined _multi-scale smoothing_, which can be efficiently implemented with a single diffusion model. This approach also suggests a new objective to compare the _collective_ robustness of multi-scale smoothed classifiers, and questions which representation of diffusion model would maximize the objective. To address this, we propose to further fine-tune diffusion model (a) to perform consistent denoising whenever the original image is recoverable, but (b) to generate rather diverse outputs otherwise. Our experiments show that the proposed multi-scale smoothing scheme, combined with diffusion fine-tuning, not only allows strong certified robustness at high noise scales but also maintains accuracy close to non-smoothed classifiers. Code is available at https://github.com/jh-jeong/smoothing-multiscale.

## 1 Introduction

Arguably, one of the important lessons in modern deep learning is the effectiveness of massive data and model scaling , which enabled many breakthroughs in recent years . Even with the largest amount of data available on the web and computational budgets, however, the _worst-case_ behaviors of deep learning models are still challenging to regularize. For example, large language models often leak private information in training data , and one can completely fool superhuman-level Go agents with few meaningless moves . Such unintentional behaviors can be of critical concerns in deploying deep learning into real-world systems, and the risk has been increasing as the capability of deep learning continues to expand.

In this context, _adversarial robustness_ has been a seemingly-close milestone towards reliable deep learning. Specifically, neural networks are even fragile to small, "imperceptible" scales of noise when it comes to the worst-case, and consequently there have been many efforts to obtain neural networks that are robust to such noise . Although it is a reasonable premise that we humans have an inherent mechanism to correct against adversarial examples , yet so far the threat still persists in the context of deep learning, _e.g._, even in recent large vision-language models , and is known as hard to avoid without a significant reduction in model performance .

_Randomized smoothing_, the focus in this paper, is currently one of a few techniques that has been successful in obtaining adversarial robustness from neural networks. Specifically, it constructs a _smoothed classifier_ by taking a majority vote from a "base" classifier, _e.g._, a neural network, over random Gaussian input noise. The technique is notable for its _provable_ guarantees on adversarialrobustness, _i.e._, it is a _certified defense_, and its scalability to arbitrary model architectures. For example, it was the first certified defense that could offer adversarial robustness on ImageNet . The scalablity of randomized smoothing has further expanded by Salman et al. , which observed that randomized smoothing can be applied to any pre-trained classifiers by prepending a denoiser model, dubbed _denoised smoothing_. Combined with the recent _diffusion-based models_, denoised smoothing could provide the current state-of-the-arts in \(_{2}\)-certified robustness .

Despite its desirable properties, randomized smoothing in practice is also at odds with model accuracy , similarly to other defenses , which makes it burdening to be applied in the real-world. For example, the variance of Gaussian noise, or _smoothing factor_, is currently a crucial hyperparameter to increase certified robustness at the cost of accuracy. The fundamental trade-off between accuracy and adversarial robustness has been well-evidenced in the literature , but it has been relatively under-explored whether they may or may not be applicable in the context of randomized smoothing. For example, Tramer et al.  demonstrate that pursuing \(\)-uniform adversarial robustness in neural networks may increase their vulnerability against _invariance attacks_, _i.e._, some semantics-altering perturbations possible inside the \(\)-ball: yet, it is still unclear whether such a vulnerability would be inevitable at non-uniformly robust models such as smoothed classifiers.

In attempt to understand the accuracy-robustness trade-off in randomized smoothing, efforts have been made to increase the certified robustness a given smoothed classifier can provide, _e.g._, through a better training method . For example, Salman et al.  have employed adversarial training  in the context of randomized smoothing, and Jeong and Shin  have proposed to train a classifier with consistency regularization. Motivated by the trade-off among different smoothing factors, some works have alternatively proposed to perform smoothing with _input-dependent_ factors : unfortunately, subsequent works  have later shown that such schemes do not provide valid robustness certificates, reflecting its brittleness in overcoming the trade-off.

Contribution.In this paper, we develop a practical method to overcome the current frontier of accuracy-robustness trade-off in randomized smoothing, particularly upon the architectural benefits that the recent _diffusion denoised smoothing_ can offer. Specifically, we first propose to aggregate multiple smoothed classifiers of different smoothing factors to obtain their collective robustness (see Figure 1(a)), which leverages the scale-free nature of diffusion denoised smoothing. In this way, the model can decide which smoothed classifier to use for each input, maintaining the overall accuracy. Next, we fine-tune a diffusion model for randomized smoothing through the "denoise-and-classify" pipeline (see Figure 1(b)), as an efficient alternative of the full fine-tuning of (potentially larger) pre-trained classifiers. Here, we identify the _over-confidence_ of diffusion models, _e.g._, to a specific class given certain backgrounds, as a major challenge towards accurate-yet-robust randomized smoothing, and design a regularization objective to mitigate the issue.

In our experiments, we evaluate our proposed schemes on CIFAR-10  and ImageNet , two of standard benchmarks for certified \(_{2}\)-robustness, particularly considering practical scenarios of applying diffusion denoised smoothing to large pre-trained models such as CLIP . Overall, the

Figure 1: An overview of the proposed approaches, (a) _cascaded randomized smoothing_ (Section 3.2 and (b) _diffusion calibration_ (Section 3.3) to attain a better trade-off between accuracy and certified robustness in randomized smoothing, upon on the recent _diffusion denoised smoothing_ scheme .

results consistently highlight that (a) the proposed multi-scale smoothing scheme upon the recent diffusion denoised smoothing  can significantly improve the accuracy of smoothed inferences while maintaining their certified robustness at larger radii, and (b) our fine-tuning scheme of diffusion models can additively improve the results - not only in certified robustness but also in accuracy - by simply replacing the denoiser model without any further adaptation. For example, we could improve certifications from a diffusion denoised smoothing based classifier by \(30.6\% 72.5\%\) in clean accuracy, while also improving its certified robustness at \(=2.0\) by \(12.6\% 14.1\%\). We observe that the collective robustness from our proposed multi-scale smoothing does not always correspond to their individual certified robustness, which has been a major evaluation in the literature, but rather to their "calibration" across models: which opens up a new direction to pursue for a practical use of randomized smoothing.

## 2 Preliminaries

**Adversarial robustness and randomized smoothing.** For a given classifier \(f:\), where \(^{d}\) and \(y:=\{1,,K\}\), _adversarial robustness_ refers to the behavior of \(f\) in making consistent predictions at the _worst-case_ perturbations under semantic-preserving restrictions. Specifically, for samples from a data distribution \((,y) p_{}(,y)\), it requires \(f(+)=y\) for _every_ perturbation \(\) that a threat model defines, _e.g._, an \(_{2}\)-ball \(\|\|_{2}\). One of ways to quantify adversarial robustness is to measure the following _average minimum-distance_ of adversarial perturbations , _i.e._, \(R(f;p_{}):=_{(,y) p_{}} [_{f(^{}) y}\|^{}-\|_ {2}].\)

The essential challenge in achieving adversarial robustness stems from that evaluating this (and further optimizing on it) is usually infeasible. _Randomized smoothing_[41; 15] bypasses this difficulty by constructing a new classifier \(\) from \(f\) instead of letting \(f\) to directly model the robustness: specifically, it transforms the base classifier \(f\) with a certain _smoothing measure_, where in this paper we focus on the case of Gaussian distributions \((0,^{2})\):

\[():=*{arg\,max}_{c}_{ (0,^{2})}[f(+ )=c].\] (1)

Then, the robustness of \(\) at \((,y)\), namely \(R(;,y)\), can be lower-bounded in terms of the _certified radius_\((,,y)\), _e.g._, Cohen et al.  showed that the following bound holds, which is tight for \(_{2}\)-adversarial threat models:

\[R(;,y)^{-1}(p_{}(,y))=: (,,y), p_{}( ,y):=_{}[f(+)=y],\] (2)

provided that \(()=y\), otherwise \(R(;,y):=0\).1 Here, we remark that the formula for certified radius is essentially a function of \(p_{}\), which is the _accuracy_ of \(f(+)\) over \(\).

**Denoised smoothing.** Essentially, randomized smoothing requires \(f\) to make accurate classification of Gaussian-corrupted inputs. A possible design of \(f\) in this regard is to concatenate a Gaussian denoiser, say \(()\), with any standard classifier \(f_{}\), so-called _denoised smoothing_:

\[f(+):=f_{}((+)).\] (3)

Under this design, an ideal denoiser \(()\) should "accurately" recover \(\) from \(+\), _i.e._, \((+)\) (in terms of their semantics to perform classification) with high probability of \((0,^{2})\). Denoised smoothing offers a more scalable framework for randomized smoothing, considering that (a) standard classifiers (rather than those specialized to Gaussian noise) are nowadays easier to obtain in the paradigm of large pre-trained models, and (b) the recent developments in diffusion models  has supplied denoisers strong enough for the framework. In particular, Lee  has firstly explored the connection between diffusion models and randomized smoothing; Carlini et al.  has further observed that latest diffusion models combined with a pre-trained classifier provides a state-of-the-art design of randomized smoothing.

**Diffusion models.** In principle, _diffusion models_[59; 31] aims to generate a given data distribution \(p_{}()\) via an iterative denoising process from a Gaussian noise \(}_{T}(0,T^{2})\) for a certain \(T>0\). Specifically, it first assumes the following diffusion process which maps \(p_{}\) to \((0,T^{2})\): \(_{t}=(_{t},t)t+ (t)_{t}\), where \(t[0,T]\), and \(_{t}\) denotes the standard Brownian motion. Basedon this, diffusion models first train a _score model_\(_{}(,t) p_{t}()\) via score matching , and use the model to solve the probabilistic flow from \(}_{T}(0,T^{2})\) to \(_{0}\) for sampling. The score estimator \(_{}(,t)\) is often parametrized by a _denoiser_\(D(;(t))\) in practice, _viz._, \( p_{t}()=(D(;(t))-)/(t)^{2}\), which establishes its close relationship with denoised smoothing.

## 3 Method

Consider a classification task from \(\) to \(\) where training data \(=\{(_{i},y_{i})\} p_{}(,y)\) is available, and let \(f:\) be a classifier. We denote \(_{}\) to be the smoothed classifier of \(f\) with respect to the smoothing factor \(>0\), as defined by (1). In this work, we aim to better understand how the _accuracy-robustness trade-off_ of \(_{}\) occurs, with a particular consideration of the recent denoised smoothing scheme. Generally speaking, the trade-off implies the following: for a given model, there exists a sample that the model gets "wrong" as it is optimized for (adversarial) robustness on another sample. In this respect, we start by taking a closer look at what it means by a model gets wrong, particularly when it is from randomized smoothing.

### Over-smoothing and over-confidence in randomized smoothing

Consider a smoothed classifier \(_{}\), and suppose there exists a sample \((,y)\) where \(_{}\) makes an error: _i.e._, \(_{}() y\). Our intuition here is to separate possible scenarios of \(_{}()\) making an error into two distinct cases, based on the _prediction confidence_ of \(_{}()\). Specifically, we define the _confidence_ of a smoothed classifier \(_{}\) at \(\) based on the definition of randomized smoothing (1) and (2):

\[p_{_{}}():=_{y}\ p_{_{}}(,y) =_{y}\ _{(0,^{2})}[f( +)=y].\] (4)

Intuitively, this notion of "smoothed" confidence measures how _consistent_ the base classifier \(f\) is in classifying \(+\) over Gaussian noise \((0,^{2})\). In cases when \(f\) is modeled by denoised smoothing, achieving high confidence requires the denoiser \(D\) to accurately "bounce-back" a given noisy image \(+\) into one that falls into class \(y\) with high probability over \(\).

Given a smoothed confidence \(p:=p_{_{}}()\), we propose to distinguish two cases of model errors, which are both peculiar to randomized smoothing, by considering a certain threshold \(p_{0}\) on \(p\). Namely, we interpret an error of \(_{}\) at \(\) either as (a) \(p p_{0}\): the model is _over-smoothing_, or (b) \(p>p_{0}\): the model is having an _over-confidence_ on the input:

1. **Over-smoothing (\(p p_{0}\)):** On one hand, it is unavoidable that the mutual information \(I(+;y)\) between input and its class label absolutely decrease from smoothing with larger variance \(^{2}\), although using larger \(\) can increase the maximum certifiable radius of \(_{}\) in practice (2). Here, a "well-calibrated" smoothed classifier should output a prediction close to the uniform distribution across \(\), leading to a low prediction confidence. In terms of denoised smoothing, the expectation is clearer: as \(+\) gets closer to the pure Gaussian noise, the denoiser \(\) should generate more diverse outputs hence in \(\) as well. Essentially, this corresponds to an accuracy-robustness trade-off from choosing a specific \(\) for a given (_e.g._, information-theoretic) capacity of data.
2. **Over-confidence (\(p>p_{0}\)):** On the other hand, it is also possible for a model \(_{}\) to be incorrect but with a _high confidence_. Compared to the over-smoothing case, this scenario rather signals a "miscalibration" and corresponds to a trade-off from _model biases_: even across smoothed models with a fixed \(\), the balance between accuracy and robustness can be different depending on how each model assigns robustness in its decision boundary per-sample basis. When viewed in terms of denoised smoothing, this occurrence can reveal an implicit bias of the denoiser function \(D\), _e.g._, that of diffusion models. For example, a denoiser might be trained to adapt to some spurious cues in training data, _e.g._, their backgrounds, as also illustrated in Figure 1(b).

In the subsequent sections, Section 3.2 and 3.3, we introduce two methods to exhibit better accuracy-robustness trade-off in randomized smoothing, each of which focuses on the individual scenarios of over-smoothing and over-confidence, respectively. Specifically, Section 3.2 proposes to use a _cascaded inference_ of multiple smoothed classifiers across different smoothing factors to mitigate the limit of using a single smoothing factor. Next, in Section 3.3, we propose to calibrate diffusion models to reduce its over-confidence particularly in denoised smoothing.

### Cascaded randomized smoothing

To overcome the trade-off between accuracy and certified robustness from _over-smoothing_, _i.e._, from choosing a specific \(\), we propose to combine _multiple_ smoothed classifiers with different \(\)'s. In a nutshell, we design a pipeline of smoothed inferences that each input (possibly with different noise resilience) can adaptively select which model to use for its prediction. Here, the primary challenge is to make it "correct", so that the proposed pipeline does not break the existing statistical guarantees on certified robustness that each smoothed classifier makes.

Specifically, we now assume \(K\) distinct smoothing factors, say \(0<_{1}<<_{K}\), and their corresponding smoothed classifiers of \(f\), namely \(_{_{1}},,_{_{K}}\). For a given input \(\), our desiderata is (a) to maximize robustness certification at \(\) based on the smoothed inferences available from the individual models, say \(p_{_{_{1}}}(),,p_{_{_{K}}}()\), while (b) minimizing the access to each of the models those require a separate Monte Calro integration in practice. In these respects, we propose a simple "predict-or-abstain" policy, coined _cascaded randomized smoothing_:2

\[(;\{_{_{i}}\}_{i=1}^{K}):= _{_{K}}()&\ p_{_{_{K}}}()>p_{0},\\ (;\{_{_{i}}\}_{i=1}^{K-1})&\ p_{_{ _{K}}}() p_{0}K>1,\] (5)

where \(\) denotes an artificial class to indicate "undecidable". Intuitively, the pipeline starts from computing \(_{_{K}}()\), the model with highest \(\), but takes its output only if its (smoothed) confidence \(p_{_{_{K}}}()\) exceeds a certain threshold \(p_{0}\):3 otherwise, it tries a smaller noise scale, say \(_{K-1}\) and so on, applying the same abstention policy of \(p_{_{}}() p_{0}\). In this way, it can early-stop the computation at higher \(\) if it is confident enough, so it can maintain higher certified robustness, while avoiding unnecessary accesses to other models of smaller \(\).

Next, we ask whether this pipeline can indeed provide a robustness certification: _i.e._, how much one can ensure \((+)=()\) in its neighborhood \(\). Theorem 3.1 below shows that one can indeed enjoy the most certified radius from \(_{_{k}}\) where \(()=:\) halts, as long as the preceding models are either keep abstaining or output \(\) over \(\):4

**Theorem 3.1**.: _Let \(_{_{1}},,_{_{K}}:\) be smoothed classifiers with \(0<_{1}<<_{K}\). Suppose \((;\{_{_{i}}\}_{i=1}^{K})=: \) halts at \(_{_{k}}\) for some \(k\). Consider any \(\) and \(_{i,c}\) that satisfy the following:_ **(a)**_\( p_{_{_{k}}}(,)\), and_ **(b)**_\(_{k^{},c} p_{_{_{k}}}(,c)\) for \(k^{}\!\!>k\) and \(c\). Then, it holds that \((+;\{_{_{i}}\}_{i=1}^{K })=\) for any \(\|\|<R\), where:_

\[R:=\{_{k}^{-1},_{ y\\ k^{}>k}\{_{k^{}}^{-1}(1- _{k^{},y})\}\}.\] (6)

Overall, the proposed multi-scale smoothing scheme (and Theorem 3.1) raise the importance of "abstaining well" in randomized smoothing: if a smoothed classifier can perfectly detect and abstain its potential errors, one could overcome the trade-off between accuracy and certified robustness by joining a more accurate model afterward. The option to abstain in randomized smoothing was originally adopted to make its statistical guarantees correct in practice. Here, we extend this usage to also rule out less-confident predictions for a more conservative decision making. As discussed in Section 3.1, now the _over-confidence_ becomes a major challenge in this matter: _e.g._, such samples can potentially bypass the abstention policy of cascaded smoothing, which motivates our fine-tuning scheme presented in Section 3.3.

**Certification.** We implement our proposed cascaded smoothing to make "statistically consistent" predictions across different noise samples, considering a certain _significance level_\(\) (_e.g._, \(=0.001\)): in a similar fashion as Cohen et al. . Roughly speaking, for a given input \(\), it makes predictions only when the \((1-)\)-confidence interval of \(p_{}()\) does not overlap with \(p_{0}\) upon \(n\)_i.i.d._ noise samples (otherwise it abstains). The more details can be found in Appendix D.2.

### Calibrating diffusion models through smoothing

Next, we move on to the _over-confidence_ issue in randomized smoothing: _viz._, \(_{}\) often makes errors with a high confidence to wrong classes. We propose to fine-tune a given \(_{}\) to make rather _diverse_ outputs when it misclassifies, _i.e._, towards a more "calibrated" \(_{}\). By doing so, we aim to cast the issue of over-confidence as that of _over-smoothing_: which can be easier to handle in practice, _e.g._, by abstaining. In this work, we particularly focus on fine-tuning only the _denoiser model_\(D\) in the context of denoised smoothing, _i.e._, \(f:=f_{} D\) for a standard classifier \(f_{}\): this offers a more scalable approach to improve certified robustness of pre-trained models, given that fine-tuning the entire classifier model in denoised smoothing can be computationally prohibitive in practice.

Specifically, given a base classifier \(f:=f_{} D\) and training data \(\), we aim to fine-tune \(D\) to improve the certified robustness of \(_{}\). To this end, we leverage the _confidence_ information of the backbone classifier \(f_{}\) fairly assuming it as an "oracle" - which became somewhat reasonable given the recent off-the-shelf models available - and apply different losses depending on the confidence information per-sample basis. We propose two losses in this matter: (a) _Brier loss_ for either correct or under-confident samples, and (b) _anti-consistency loss_ for incorrect, over-confident samples.

Brier loss.For a given training sample \((,y)\), we adopt the Brier (or "squared") loss  to regularize the denoiser function \(D\) to promote the confidence of \(f_{}(D(+))\) towards \(y\), which can be beneficial to increase the smoothed confidence \(p_{_{}}()\) that impacts the certified robustness at \(\). Compared to the cross-entropy (or "log") loss, a more widely-used form in such purpose, we observe that the Brier loss can be favorable in such fine-tuning of \(D\) through \(f_{}\), in a sense that the loss is less prone to "over-optimize" the confidence at values closer to \(1\).

Here, an important detail is that we do not apply the regularization to incorrect-yet-confident samples: _i.e._, whenever \(f_{}(D(+)) y\) and \(p_{}():=_{c}F_{,c}(D(+))>p_{0}\), where \(F_{}\) is the soft prediction of \(f_{}\). This corresponds to the case when \(D(+)\) rather outputs a "realistic" off-class sample, which will be handled by the anti-consistency loss we propose. Overall, we have:

\[L_{}(,y):=_{}[ [_{}=y\;\;\;\;p_{}( {}) p_{0}]\|F_{}(D(+) )-_{y}\|^{2}],\] (7)

where we denote \(_{}:=f(+)\) and \(_{y}\) is the \(y\)-th unit vector in \(^{||}\).

Anti-consistency loss.On the other hand, the anti-consistency loss aims to detect whether the sample is over-confident, and penalizes it accordingly. The challenge here is that identifying over-confidence in a smoothed classifier requires checking for \(p_{_{}}()>p_{0}\) (4), which can be infeasible during training. We instead propose a simpler condition to this end, which only takes two independent Gaussian noise, say \(_{1},_{2}(0,^{2} )\). Specifically, we identify \((,y)\) as over-confident whenever (a) \(_{1}:=f(+_{1})\) and \(_{2}:=f(+_{2})\) match, while (b) they are incorrect, _i.e._, \(_{1} y\). Intuitively, such a case signals that the denoiser \(D\) is often making a complete flip to the semantics of \(+\) (see Figure 1(b) for an example), which we aim to penalize. A care should be taken, however, considering the possibility that \(D(+)\) indeed generates an in-distribution sample that falls into a different class: in this case, penalizing it may result in a decreased robustness of that sample. In these respects, our design of anti-consistency loss forces the two samples simply to have different predictions, by keeping at least one prediction as the original, while penalizing the counterpart. Denoting \(_{1}:=F_{}(D(+_{1}))\) and \(_{2}:=F_{}(D(+_{2}))\), we again apply the squared loss on \(_{1}\) and \(_{2}\) to implement the loss design, as the following:

\[L_{}(,y):=[_{1}=_{2}\; \;_{1} y](\|_{1}-(_{1})\|^{2}+ \|_{2}\|^{2}),\] (8)

where \(()\) denotes the stopping gradient operation.

Overall objective.Combining the two proposed losses, _i.e._, the Brier loss and anti-consistency loss, defines a new regularization objective to add upon any pre-training objective for the denoiser \(D\):

\[L(D):=L_{}+(L_{}+ L_{ }),\] (9)

where \(,>0\) are hyperparameters. Here, \(\) denotes the relative strength of \(L_{}\) over \(L_{}\) in its regularization.5 Remark that increasing \(\) would give more penalty on over-confident samples, which would lead the model to make more abstentions: therefore, this results in an increased accuracy particularly in cascaded smoothing (Section 3.2).

## 4 Experiments

We verify the effectiveness of our proposed schemes, (a) _cascaded smoothing_ and (b) _diffusion calibration_, mainly on CIFAR-10  and ImageNet : two standard datasets for an evaluation of certified \(_{2}\)-robustness. We provide the detailed experimental setups, _e.g_., training, datasets, hyperparameters, computes, _etc_., in Appendix B.

**Baselines.** Our evaluation mainly compares with _diffusion denoised smoothing_, the current state-of-the-art methodology in randomized smoothing. We additionally compare with two other training baselines from the literature, by considering models with the same classifier architecture but without the denoising step of Carlini et al. . Specifically, we consider (a) _Gaussian training_, which trains a classifier with Gaussian augmentation; and (b) _Consistency_, which additionally regularizes the variance of predictions over Gaussian noise in training. We select the baselines assuming practical scenarios where the training cost of the classifier side is crucial: other existing methods for smoothed classifiers often require much more costs, _e.g_., \(8\) times over Gaussian [57; 73].

**Setups.** We follow Carlini et al.  for the choice of diffusion models: specifically, we use the 50M-parameter \(32 32\) diffusion model from Nichol and Dhariwal  for CIFAR-10, and the 552M-parameter \(256 256\) unconditional model from Dhariwal and Nichol  for ImageNet. For the classifier side, we use ViT-B/16  pre-trained via CLIP  throughout our experiments. For uses we fine-tune the model on each of CIFAR-10 and ImageNet via FT-CLIP , resulting in classifiers that achieve 98.1% and 85.2% in top-1 accuracy on CIFAR-10 and ImageNet, respectively. Following the prior works, we mainly consider \(\{0.25,0.50,1.00\}\) for smoothing in our experiments.

**Evaluation metrics.** We consider two popular metrics in the literature when evaluating certified robustness of smoothed classifiers: (a) the _approximate certified test accuracy_ at \(r\): the fraction of the test set which Certify classifies correctly with the radius larger than \(r\) without abstaining,and (b) the _average certified radius_ (ACR) : the average of certified radii on the test set \(_{}\) while assigning incorrect samples as 0: _viz._, \(:=_{}|}_{(,y) _{}}[(f,,)_{ _{()=y}}]\), where \(()\) denotes the certified radius that Certify returns. Throughout our experiments, we use \(n=10,000\) noise samples to certify robustness for both CIFAR-10 and ImageNet. We follow  for the other hyperparameters to run Certify, namely by \(n_{0}=100\), and \(=0.001\). In addition to certified accuracy, we also compare the _empirical accuracy_ of smoothed classifiers. Here, we define empirical accuracy by the fraction of test samples those are either (a) certifiably correct, or (b) abstained but correct in the _clean_ classifier: which can be a natural alternative especially at denoised smoothing. For this comparison, we use \(n=100\) to evaluate empirical accuracy.

Unlike the evaluation of Carlini et al. , however, we do not compare cascaded smoothing with the _envelop accuracy curve_ over multiple smoothed classifiers at \(\{0.25,0.50,1.00\}\): although the envelope curve can be a succinct proxy to compare methods, it can be somewhat misleading and unfair to compare the curve directly with an individual smoothed classifier. This is because the curve does not really construct a concrete classifier on its own: it additionally assumes that each test sample has prior knowledge on the value of \(\{0.25,0.5,1.0\}\) to apply, which is itself challenging to infer. This is indeed what our proposal of cascaded smoothing addresses.

### Results

**Cascaded smoothing.** Figure 2 visualizes the effect of cascaded smoothing we propose in the plots of certified accuracy, and the detailed results are summarized in Table 1 and 2 as "+ Cascading". Overall, we observe that the certified robustness that cascaded smoothing offers can be highly desirable over the considered single-scale smoothed classifiers. Compared to the single-scale classifiers at highest \(\), _e.g._, \(=1.0\) in Table 1 and 2, our cascaded classifiers across \(\{0.25,0.50,1.00\}\) absolutely improve the certified accuracy at all the range of \(\) by incorporating more accurate predictions from classifiers, _e.g._, of lower \(\{0.25,0.50\}\). On the opposite side, _e.g._, compared to Carlini et al.  at \(=0.25\), the cascaded classifiers provide competitive certified clean accuracy (\(=0\)), _e.g._, 89.5% _vs._ 85.1% on CIFAR-10, while being capable of offering a wider range of robustness certificates. Those considerations are indeed reflected quantitatively in terms of the improvements in ACRs. The existing gaps in the clean accuracy are in principle due to the errors in higher-\(\) classifiers: _i.e._, a better calibration, to let them better abstain, could potentially reduce the gaps.

**Diffusion calibration.** Next, we evaluate the effectiveness of our proposed diffusion fine-tuning scheme: "+ Calibration" in Table 1 and 2 report the results on CIFAR-10 and ImageNet, respectively, and Figure 3 plots the CIFAR-10 results for each of \(\{0.25,0.50,1.00\}\). Overall, on both CIFAR-10 and ImageNet, we observe that the proposed fine-tuning scheme could _uniformly_ improve certified accuracy across the range considered, even including the clean accuracy. This confirms that the tuning could essentially improve the accuracy-robustness trade-off rather than simply moving along itself. As provided in Table 3, we remark that simply pursuing only the Brier loss (7) may achieve a better ACR in overall, but with a decreased accuracy: it is the role of the anti-consistency loss (8) to balance between the two, consequently to achieve a better trade-off afterwards.

**Empirical accuracy.** In practical scenarios of adopting smoothed classifiers for inference, it is up to users to decide how to deal with the "abstained" inputs. Here, we consider a possible candidate of simply outputting the _standard_ prediction instead for such inputs: in this way, the output could be noted as an "uncertified" prediction, while possibly being more accurate, _e.g._, for in-distribution

    &  &  \\   & \(\) & IN-1K & IN-R & IN-A & 0.0 & 0.5 & 1.0 & 1.5 & 2.0 & ACR \\  Carlini et al.  & 0.25 & 80.9 & 69.3 & 35.3 & 78.8 & 61.4 & & & 0.517 \\  Carlini et al.  & 0.50 & 79.5 & 67.5 & 32.3 & 65.8 & 52.2 & 38.6 & 23.6 & 0.703 \\ **+ Cascading (ours)** & (0.25-0.50) & **83.5** & **69.6** & **41.3** & **75.0** & **52.6** & **39.0** & 22.8 & **0.720** \\ **+ Calibration (ours)** & (0.25-0.50) & **83.8** & **69.8** & **41.7** & **76.6** & **54.6** & **39.8** & 23.0 & **0.743** \\  Carlini et al.  & 1.00 & 77.2 & 64.3 & 32.0 & 30.6 & 25.8 & 20.6 & 17.0 & 12.6 & 0.538 \\ **+ Cascading (ours)** & (0.25-1.00) & **82.6** & **69.8** & **40.5** & **69.0** & **42.4** & **26.6** & **19.0** & **14.6** & **0.752** \\ **+ Calibration (ours)** & (0.25-1.00) & **83.2** & **69.5** & **40.8** & **72.5** & **44.0** & **27.5** & **19.9** & **14.1** & **0.775** \\   

Table 2: Comparison of (a) certified accuracy, (b) empricial accuracy, and (c) average certified radius (ACR) on ImageNet. We set our result bold-faced whenever it achieves the best upon baselines.

inputs. Specifically, in Table 1 and 2, we consider a fixed standard classifier of CLIP-finetuned ViT-B/16 on CIFAR-10 (or ImageNet), and compare the empirical accuracy of smoothed classifiers on CIFAR-10, -10-C  and -10.1  (or ImageNet, -R , and -A ). Overall, the results show that our smoothed models could consistently outperform even in terms of empirical accuracy while maintaining high certified accuracy, _i.e_., they abstain only when necessary. We additionally observe in Appendix C.2 that the empirical accuracy of our smoothed model can be further improved by considering an "ensemble" with the clean classifier: _e.g_., the ensemble improves the accuracy of our cascaded classifier (\(\{0.25,0.5\}\)) on CIFAR-10-C by \(88.8\% 95.0\%\), even outperforming the accuracy of the standard classifier of \(93.4\%\).

### Ablation study

In Table 3, we compare our result with other training baselines as well as some ablations for a component-wise analysis, particularly focusing on their performance in cascaded smoothing across \(\{0.25,0.50,1.00\}\) on CIFAR-10. Here, we highlight several remarks from the results, and provide the more detailed study, _e.g_., the effect of \(p_{0}\) (5), in Appendix C.3.

**Cascading from other training.** "Gaussian" and "Consistency" in Table 3 report the certified robustness of cascaded classifiers where each of single-scale models is individually trained by the method. Even while their (certified) clean accuracy of \(=0.25\) models are competitive with those of denoising-based models, their collective accuracy significantly degraded after cascading, and interestingly the drop is much more significant on "Consistency", although it did provide more robustness at larger \(\). Essentially, for a high clean accuracy in cascaded smoothing the individual classifiers should make consistent predictions although their confidence may differ: the results imply that individual training of classifiers, without a shared denoiser, may break this consistency. This supports an architectural benefit of denoised smoothing for a use as cascaded smoothing.

**Cross-entropy _vs_. Brier.** "+ Cross-entropy" in Table 3 considers an ablation of the Brier loss (7) where the loss is replaced by the standard cross-entropy: although it indeed improves ACR compared to Carlini et al.  and achieves the similar clean accuracy with "+ Brier loss", its gain in overall certified robustness is significantly inferior to that of Brier loss as also reflected in the worse ACR. The superiority of the "squared" loss instead of the "log" loss suggests that it may not be necessary to optimize the confidence of individual denoised image toward a value strictly close to 1, which is reasonable considering the severity of noise usually considered for randomized smoothing.

    \\  Model (\(=1.0\)) & \(p p_{0}\) & \(p>p_{0}\) & ACR (\(\)) \\  Carlini et al.  & 43.8 & **7.6** & 0.498 \\ 
**Cascading** & 5.0 & 14.5 & 0.579 \\
**+ Anti-consist.** & 4.7 & 11.4 & 0.566 \\
**+ Brier loss** & **3.6** & 16.8 & **0.645** \\   

Table 4: Comparison of ACR and certified accuracy rates on CIFAR-10 decomposed into (a) over-smoothing (\(p p_{0}\)) and (b) over-confidence (\(p>p_{0}\)), also with ACR.

   \)} &  \\  Training & ACR & 0.00 & 0.25 & 0.50 & 0.75 & 1.00 & 1.25 & 1.50 & 1.75 \\  Gaussian  & 0.470 & 70.7 & 46.9 & 31.7 & 23.7 & 16.8 & 12.4 & 9.2 & 6.5 \\ Consistency  & 0.548 & 57.6 & 42.7 & 32.3 & 26.3 & 22.4 & 18.1 & 14.6 & 11.1 \\  Carlini et al.  & 0.579 & 80.5 & 52.8 & 37.4 & 28.0 & 21.7 & 16.8 & 11.8 & 8.5 \\
**+ Cross-entropy** & 0.605 & 77.9 & 52.8 & 39.6 & 29.1 & 22.2 & 18.7 & 13.5 & 9.5 \\
**+ Brier loss** & **0.666** & 77.6 & **55.7** & **41.7** & **32.7** & **26.8** & **21.7** & **16.3** & **11.9** \\
**+ Anti-consist.** (\(=1.0\)) & 0.645 & 79.6 & 53.5 & 40.5 & 31.2 & 25.0 & 20.1 & 16.2 & 11.4 \\
**+ Anti-consist.** (\(=2.0\)) & 0.625 & 80.1 & 54.6 & 39.2 & 30.6 & 24.0 & 18.5 & 14.5 & 9.9 \\
**+ Anti-consist.** (\(=1.0\)) & 0.566 & **83.9** & 55.1 & 37.8 & 25.8 & 19.8 & 14.5 & 11.5 & 8.1 \\   

Table 3: Comparison of ACR and certified accuracy of cascaded smooth classifiers on CIFAR-10 over different training and ablations. We use \(\{0.25,0.5,1.0\}\) for cascaded smoothing. Bold and underline indicate the best and runner-up, respectively. Model reported in Table 1 is marked as grey.

Anti-consistency loss.The results marked as "+ Anti-consist." in Table 3, on the other hand, validates the effectiveness of the anti-consistency loss (8). Compared to "+ Brier loss", which is equivalent to the case when \(=0.0\) in (9), we observe that adding anti-consistency results in a slight decrease in ACR but with an increase in clean accuracy. The increased accuracy of cascaded smoothing indicates that the fine-tuning could successfully reduce over-confidence, and this tendency continues at larger \(=2.0\). Even with the decrease in ACR over the Brier loss, the overall ACRs attained are still superior to others, confirming the effectiveness of our proposed loss in total (9).

Classifier fine-tuning.In Table 4, we compare our proposed diffusion fine-tuning (Section 3.3) with another possible scheme of _classifier_ fine-tuning, of which the effectiveness has been shown by Carlini et al. . Overall, we observe that fine-tuning both classifiers and denoiser can bring their complementary effects to improve denoised smoothing in ACR, and the diffusion fine-tuning itself could obtain a comparable gain to the classifier fine-tuning. The (somewhat counter-intuitive) effectiveness of diffusion fine-tuning confirms that denoising process can be also biased as well as classifiers to make over-confident errors. We further remark two aspects where classifier fine-tuning can be less practical, especially when it comes with the cascaded smoothing pipeline we propose:

1. To perform cascaded smoothing at multiple noise scales, classifier fine-tuning would require separate runs of training for optimal models per scale, also resulting in multiple different models to load - which can be less scalable in terms of memory efficiency.
2. In a wider context of denoised smoothing, the classifier part is often assumed to be a model at scale, even covering cases when it is a "black-box" model, _e.g._, public APIs. Classifier fine-tuning, in such cases, can become prohibitive or even impossible.

With respect to (a) and (b), denoiser fine-tuning we propose offers a more efficient inference architecture: it can handle multiple noise scales jointly with a single diffusion model, while also being applicable to the extreme scenario when the classifier model is black-box.

Over-smoothing and over-confidence.As proposed in Section 3.1, errors from smoothed classifiers can be decomposed into two: _i.e._, \(p p_{0}\) for over-smoothing, and \(p>p_{0}\) over-confidence, respectively. In Table 5, we further report a detailed breakdown of the model errors on CIFAR-10, assuming \(=1.0\). Overall, we observe that over-smoothing can be the major source of errors especially at such a high noise level, and our proposed cascading dramatically reduces the error. Although we find cascading could increase over-confidence from accumulating errors through multiple inferences, our diffusion fine-tuning could alleviate the errors jointly reducing the over-smoothing as well.

## 5 Conclusion and Discussion

Randomized smoothing has been traditionally viewed as a somewhat less-practical approach, perhaps due to its cost in inference time and impact on accuracy. In another perspective, to our knowledge, it is currently one of a few existing approaches that is prominent in pursuing _adversarial robustness at scale_, _e.g._, in a paradigm where training cost scales faster than computing power. This work aims to make randomized smoothing more practical, particularly concerning on scalable scenarios of robustness large pre-trained classifiers. We believe our proposals in this respect, _i.e._, _cascaded smoothing_ and _diffusion calibration_, can be a useful step towards building safer AI-based systems.

Limitation.A practical downside of randomized smoothing is in its increased inference cost, mainly from the majority voting procedure per inference. Our method also essentially possesses this practical bottleneck, and the proposed multi-scale smoothing scheme may further increase the cost from taking multiple smoothed inferences. Yet, we note that randomized smoothing itself is equipped with many practical axes to reduce its inference cost by compensating with abstention: for example, one can reduce the number of noise samples, _e.g._, to \(n=100\). It would be an important future direction to explore practices for a better trade-off between the inference cost and robustness of smoothed classifiers, which could eventually open up a feasible way to obtain adversarial robustness at scale.

Broader impact.Deploying deep learning based systems into the real-world, especially when they are of security-concerned , poses risks for both companies and customers, and we researchers are responsible to make this technology more reliable through research towards _AI safety_. _Adversarial robustness_, that we focus on in this work, is one of the central parts of this direction, but one should also recognize that adversarial robustness is still a bare minimum requirement for reliable deep learning. The future research should also explore more diverse notions of AI Safety to establish a realistic sense of security for practitioners, _e.g._, monitoring and alignment research, to name a few.