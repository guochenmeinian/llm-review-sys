# Memory-Constrained Algorithms for Convex Optimization

Moise Blanchard

Operations Research Center

MIT

Cambridge, MA 02139

moiseb@mit.edu

Junhui Zhang

Operations Research Center

MIT

Cambridge, MA 02139

junhuiz@mit.edu

Patrick Jaillet

Department of Electrical Engineering and Computer Science

MIT

Cambridge, MA 02139

jaillet@mit.edu

###### Abstract

We propose a family of recursive cutting-plane algorithms to solve feasibility problems with constrained memory, which can also be used for first-order convex optimization. Precisely, in order to find a point within a ball of radius \(\) with a separation oracle in dimension \(d\)--or to minimize \(1\)-Lipschitz convex functions to accuracy \(\) over the unit ball--our algorithms use \((}{p})\) bits of memory, and make \(((C)^{p})\) oracle calls, for some universal constant \(C 1\). The family is parametrized by \(p[d]\) and provides an oracle-complexity/memory trade-off in the sub-polynomial regime \( d\). While several works gave lower-bound trade-offs (impossibility results) [31; 5]--we explicit here their dependence with \(\), showing that these also hold in any sub-polynomial regime--to the best of our knowledge this is the first class of algorithms that provides a positive trade-off between gradient descent and cutting-plane methods in any regime with \( 1/\). The algorithms divide the \(d\) variables into \(p\) blocks and optimize over blocks sequentially, with approximate separation vectors constructed using a variant of Vaidya's method. In the regime \( d^{-(d)}\), our algorithm with \(p=d\) achieves the information-theoretic optimal memory usage and improves the oracle-complexity of gradient descent.

## 1 Introduction

Optimization algorithms are ubiquitous in machine learning, from solving simple regressions to training neural networks. Their essential roles have motivated numerous studies on their efficiencies, which are usually analyzed through the lens of oracle-complexity: given an oracle (such as function value, or subgradient oracle), how many calls to the oracle are needed for an algorithm to output an approximate optimal solution? . However, ever-growing problem sizes have shown an inadequacy in considering only the oracle-complexity, and have motivated the study of the trade-off between oracle-complexity and other resources such as memory [52; 31; 5] and communication[25; 40; 42; 45; 33; 53; 51; 50].

In this work, we study the oracle-complexity/memory trade-off for first-order non-smooth convex optimization, and the closely related feasibility problem, with a focus on developing memory efficient(deterministic) algorithms. Since  formally posed as open problem the question of characterizing this trade-off, there have been exciting results showing what is impossible: for convex optimization in \(^{d}\),  shows that any randomized algorithm with \(d^{1.25-}\) bits of memory needs at least \((d^{1+4/3})\) queries, and this has later been improved for deterministic algorithms to \(d^{1-}\) bits of memory or \((d^{1+/3})\) queries by ; in addition  shows that for the feasibility problem with a separation oracle, any algorithm which uses \(d^{2-}\) bits of memory needs at least \((d^{1+})\) queries.

Despite these recent results on the lower bounds, all known first-order convex optimization algorithms that output an \(\)-suboptimal point fall into two categories: those that have quadratic memory in the dimension \(d\) but can potentially achieve the optimal \((d)\) query complexity, as represented by the center-of-mass method, and those that have \((})\) query complexity but only need the optimal \((d)\) bits of memory, as represented by the classical gradient descent . In addition, the above-mentioned memory bounds apply only between queries, and in particular the center-of-mass method  is allowed to use infinite memory during computations.

We propose a family of memory-constrained algorithms for the stronger feasibility problem in which one aims to find a point within a set \(Q\) containing a ball of radius \(\), with access to a separation oracle. In particular, this can be used for convex optimization since the subgradient information provides a separation vector. Our algorithms use \((}{p})\) bits of memory (including during computations) and \(((C)^{p})\) queries for some universal constant \(C 1\), and a parameter \(p[d]\) that can be chosen by the user. Intuitively, in the context of convex optimization, the algorithms are based on the idea that for any function \(f(,)\) convex in the pair \((,)\), the partial minimum \(_{}f(,)\) as a function of \(\) is still convex and, using a variant of Vaidya's method proposed in , our algorithm can approximate subgradients for that function \(_{}f(,)\), thereby turning an optimization problem with variables \((,)\) to one with just \(\). This idea, applied recursively with the variables divided into \(p\) blocks, gives our family of algorithms and the above-mentioned memory and query complexity. The main algorithmic contribution is in how we design the recursive dimension reduction procedure: a technical step of the design and analysis is to ensure that the necessary precision for recursive computations can be achieved using low memory. Last, our algorithms account for memory usage throughout computations, as opposed to simply between calls to the gradient oracle, which was the traditional approach in the literature.

When \(p=1\), our algorithm is a memory-constrained version of Vaidya's method [48; 27], and improves over the center-of-mass  method by a factor of \(\) in terms of memory while having optimal oracle-complexity. The improvements provided by our algorithms are more significant in regimes when \(\) is very small in the dimension \(d\): increasing the parameter \(p\) can further reduce the memory usage of Vaidya's method (\(p=1\)) by a factor \(/ d\), while still improving over the oracle-complexity of gradient descent. In particular, in a regime \(=( d)\), these memory improvements are only in terms of \( d\) factors. However, in sub-polynomial regimes with potentially \(=d^{c}\) for some constant \(c>0\), these provide polynomial improvements to the memory of standard cutting-plane methods.

As a summary, this paper makes the following contributions.

* Our class of algorithms provides a trade-off between memory-usage and oracle-complexity whenever \( d\). Further, taking \(p=1\) improves the memory-usage from center-of-mass  by a factor \(\), while preserving the optimal oracle-complexity.
* For \((d d)\), our algorithm with \(p=d\) is the first known algorithm that outperforms gradient descent in terms of the oracle-complexity, but still maintains the optimal \((d)\) memory usage.
* We show how to obtain a \(\) dependence in the known lower-bound trade-offs [31; 5], confirming that the oracle-complexity/memory trade-off is necessary for any regime \(}\).

## 2 Setup and Preliminaries

In this section, we precise the formal setup for our results. We follow the framework introduced in , to define the memory constraint on algorithms with access to an oracle \(:\) which takes as input a query \(q\) and outputs a response \((q)\). Here, the algorithm is constrained to update an internal \(M\)-bit memory between queries to the oracle.

**Definition 2.1** (\(M\)-bit memory-constrained algorithm [52; 31; 5]).: _Let \(:\) be an oracle. An \(M\)-bit memory-constrained algorithm is specified by a query function \(_{query}:\{0,1\}^{M}\) and an update function \(_{update}:\{0,1\}^{M}\{0,1\}^{M}\). The algorithm starts with the memory state \(_{0}=0^{M}\) and iteratively makes queries to the oracle. At iteration \(t\), it makes the query \(q_{t}=_{query}(_{t-1})\) to the oracle, receives the response \(r_{t}=(q_{t})\) then updates its memory \(_{t}=_{update}(_{t-1},q_{t},r_{t})\)._

The algorithm can stop at any iteration and the last query is its final output. Importantly, this model does not enforce constraints on the memory usage during the computation of \(_{update}\) and \(_{query}\). This is ensured in the stronger notion of a memory-constrained algorithm with computations. These are precisely algorithms that have constrained memory including for computations, with the only specificity that they need a decoder function \(\) to make queries to the oracle from their bit memory, and a discretization function \(\) to write a discretized response into the algorithm's memory.

**Definition 2.2** (\(M\)-bit memory-constrained algorithm with computations).: _Let \(:\) be an oracle. We suppose that we are given a decoding function \(:\{0,1\}^{}\) and a discretization function \(:\{0,1\}^{}\) such that \((r,n)\{0,1\}^{n}\) for all \(r\). An \(M\)-bit memory-constrained algorithm with computations is only allowed to use an \(M\)-bit memory in \(\{0,1\}^{M}\) even during computations. The algorithm has three special memory placements \(Q,N,R\). Say the contents of \(Q\) and \(N\) are \(q\) and \(n\) respectively. To make a query, \(R\) must contain at least \(n\) bits. The algorithm submits \(q\) to the encoder which then submits the query \((q)\) to the oracle. If \(r=((q))\) is the oracle response, the discretization function then writes \((r,n)\) in the placement \(R\)._

Feasibility problem.In this problem, the goal is to find a point \( Q\), where \(Q_{d}:=[-1,1]^{d}\) is a convex set. We choose the cube \([-1,1]^{d}\) as prior bound for convenience in our later algorithms, but the choice of norm for this prior ball can be arbitrary and does not affect our results. The algorithm has access to a _separation oracle_\(O_{S}:_{d}\{\}^{d}\), such that for a query \(^{d}\) either returns \(\) if \( Q\), or a separating hyperplane \(^{d}\), i.e., such that \(^{}<^{}^{}\) for any \(^{} Q\). We suppose that the separating hyperplanes are normalized, \(\|\|_{2}=1\). An algorithm solves the feasibility problem with accuracy \(\) if the algorithm is successful for any feasibility problem such that \(Q\) contains an \(\)-ball \(B_{d}(^{},)\) for \(^{}_{d}\).

As an important remark, this formulation asks that the separation oracle is consistent over time: when queried at the exact same point \(\), the oracle always returns the same separation vector. In this context, we can use the natural decoding function \(\) which takes as input \(d\) sequences of bits and outputs the vector with coordinates given by the sequences interpreted in base 2. Similarly, the natural discretization function \(\) takes as input the separation hyperplane \(\) and outputs a discretized version up to the desired accuracy. From now, we can omit these implementation details and consider that the algorithm can query the oracle for discretized queries \(\), up to specified rounding errors.

**Remark 2.1**.: _An algorithm for the feasibility problem with accuracy \(/(2)\) can be used for first-order convex optimization. Suppose one aims to minimize a \(1\)-Lipschitz convex function \(f\) over the unit ball, and output an \(\)-suboptimal solution, i.e., find a point \(\) such that \(f()_{ B_{d}(0,1)}f()+\). A separation oracle for \(Q=\{:f()_{ B_{d}(0,1)}f()+\}\) is given at a query \(\) by the subgradient information from the first-order oracle: \()}{\|()\|}\). Its computation can also be carried out memory-efficiently up to rounding errors since if \(\| f()\|/(2)\), the algorithm can return \(\) and already has the guarantee that \(\) is an \(\)-suboptimal solution (\(_{d}\) has diameter \(2\)). Notice that because \(f\) is \(1\)-Lipschitz, \(Q\) contains a ball of radius \(/(2)\) (the factor \(1/(2)\) is due to potential boundary issues). Hence, it suffices to run the algorithm for the feasibility problem while keeping in memory the queried point with best function value._

### Known trade-offs between oracle-complexity and memory

Known lower-bound trade-offs.All known lower bounds apply to the more general class of memory-constrained algorithms without computational constraints given in Definition 2.1.  first showed that \((d)\) queries are needed for solving convex optimization to ensure that one finds an \(\)-suboptimal solution. Further, \((d)\) bits of memory are needed even just to output a solution in the unit ball with \(\) accuracy . These historical lower bounds apply in particular to the feasibility problem and are represented in the pictures of Fig. 1 as the dashed pink region.

More recently,  showed that achieving both optimal oracle-complexity and optimal memory is impossible for convex optimization. They show that a possibly randomized algorithm with \(d^{1.25-}\) bits of memory makes at least \((d^{1+4/3})\) queries. This result was extended for deterministic algorithms in  which shows that a deterministic algorithm with \(d^{1-}\) bits of memory makes \((d^{1+/3})\) queries. For the feasibility problem, they give an improved trade-off: any deterministic algorithm with \(d^{2-}\) bits of memory makes \((d^{1+})\) queries. These trade-offs are represented in the left picture of Fig. 1 as the pink, red, and purple solid region, respectively. Using a clever and more careful analysis, showed that similar lower bounds can be carried out for deterministic algorithms as well.

Known upper-bound trade-offs.Prior to this work, to the best of our knowledge only two algorithms were known in the oracle-complexity/memory landscape. First, cutting-plane algorithms achieve the optimal oracle-complexity \((d)\) but use quadratic memory. The memory-constrained (MC) center-of-mass method analyzed in  uses in particular \((d^{2}^{2})\) memory. Instead, if one uses Vaidya's method which only needs to store \((d)\) cuts instead of \((d)\), we show that one can achieve \((d^{2})\) memory. These algorithms only use the separation oracle and hence apply to both convex optimization and the feasibility problem. On the other hand, the memory-constrained gradient descent for convex optimization  uses the optimal \((d)\) memory but makes \((})\) iterations. While the analysis in  is only carried for convex optimization, we can give a modified proof showing that gradient descent can also be used for the feasibility problem.

### Other related works

Vaidya's method  and the variant  that we use in our algorithms, belong to the family of cutting-plane methods. Perhaps the simplest example of an algorithm in this family is the center-of-mass method, which achieves the optimal \((d)\) oracle-complexity but is computationally intractable, and the only known random walk-based implementation  has computational complexity \((d^{7})\). Another example is the ellipsoid method, which has suboptimal \((d^{2})\) query complexity, but has an improved computational complexity \((d^{4})\).  pointed out that Vaidya's method achieves the best of both worlds by sharing the \((d)\) optimal query complexity of the center-of-mass, and achieving a computational complexity of \((d^{1+})\)1. In a major breakthrough, this computational complexity was improved to \((d^{3}^{3})\) in , then to \((d^{3})\) in . We refer to  for more detailed comparisons of these algorithms.

Another popular convex optimization algorithm that requires quadratic memory is the Broyden-Fletcher- Goldfarb- Shanno (BFGS) algorithm , which stores an approximated inverse Hessian matrix as gradient preconditioner. Several works aimed to reduce the memory usage of BFGS; in particular, the limited memory BFGS (L-BFGS) stores a few vectors instead of the entire approximated inverse Hessian matrix . However, it is still an open question whether even the original BFGS converges for non-smooth convex objectives .

Lying at the other extreme of the oracle-complexity/memory trade-off is gradient descent, which achieves the optimal memory usage but requires significantly more queries than center-of-mass or Vaidya's method in the regime \(}\). There is a rich literature of works aiming to speed up gradient descent, such as the optimized gradient method , Nesterov's Acceleration , the triple momentum method , geometric descent , quadratic averaging , the information-theoretic exact method , or Big-Step-Little-Step method . Interested readers can find a comprehensive survey on acceleration methods in . However, these acceleration methods usually require additional smoothness or strong convexity assumptions (or both) on the objective function, due to the known \((})\) query lower bound in the large-scale regime \(}\) for any first order method where the query points lie in the span of the subgradients of previous query points .

Besides accelerating gradient descent, researchers have investigated more efficient ways to leverage subgradients obtained in previous iterations. Of interest are bundle methods , that have found a wide range of applications [47; 26]. In their simplest form, they minimize the sum of the maximum of linear lower bounds constructed using past oracle queries, and a regularization term penalizing the distance from the current iteration variable. Although the theoretical convergence rate of the bundle method is the same as that of gradient descent, in practice, bundle methods can benefit from previous information and substantially outperform gradient descent .

Our works are focused on high-accuracy regimes, when the accuracy \(\) is sub-polynomial. We note that for their lower-bound result on randomized algorithms,  also required sub-polynomial accuracies, which raises the question whether this is a general phenomenon for the study of memory-constrained algorithms in convex optimization. This also relates our work to the study of low-dimensional problems--or even constant dimension--which has been investigated in the literature [49; 10].

Last, the increasing size of optimization problems has also motivated the development of communication-efficient optimization algorithms in distributed settings such as [25; 40; 42; 45; 33; 53; 51; 50]. Moreover, recent works have explored the trade-off between sample complexity and memory/communication complexity for learning problems under the streaming model, with notable contributions including [6; 13; 14; 39; 44; 32].

## 3 Main results

We first check that the memory-constrained gradient descent method solves feasibility problems. This was known for convex optimization  and the same algorithm with a modified proof gives the following result. For completeness, the proof is given in Appendix D.

**Proposition 3.1**.: _The memory-constrained gradient descent algorithm solves the feasibility problem with accuracy \(}\) using \((d)\) bits of memory and \((})\) separation oracle calls._

Our main contribution is a class of algorithms based on Vaidya's cutting-plane method that provide a query-complexity / memory tradeoff for optimization in \(^{d}\). More precisely, we show the following, where \(<2.373\) is the exponent of matrix multiplication, such that multiplying two \(n n\) matrices runs in \((n^{})\) time.

**Theorem 3.2**.: _For any \(1 p d\), there is a deterministic first-order algorithm that solves the feasibility problem in dimension \(d\) for accuracy \(}\), using \((}{p})\) bits of memory (including during computations), with \(((C)^{p})\) calls to the separation oracle, and computational complexity \(((C()^{1+})^{p})\), where \(C 1\) is a universal constant._

For simplicity, in Section 4, we describe algorithms that achieve this trade-off without computation concerns (Definition 2.1), which already provide the main elements of our method. The proof of oracle-complexity and memory usage is given in Appendix A. In Appendix B, we consider computational constraints and give corresponding algorithms using the cutting-plane method of .

To better understand the implications of Theorem 3.2, it is useful to compare the provided class of algorithms to the two algorithms known in the oracle-complexity/memory tradeoff landscape: the memory-constrained center-of-mass method and the memory-constrained gradient descent .

For \(p=1\), our resulting procedure, which is essentially a memory-constrained Vaidya's algorithm, has optimal oracle-complexity \((d)\) and uses \((d^{2})\) bits of memory. This improves by a \(\) factor the memory usage of the center-of-mass-based algorithm provided in , which used \((d^{2}^{2})\) memory and had the same optimal oracle-complexity.

Next, we recall that the memory-constrained gradient descent method used the optimal number \((d)\) bits of memory (including for computations), and a sub-optimal \((})\) oracle-complexity. While the memory of our algorithms decreases with \(p\), their oracle-complexity is exponential in \(p\). This significantly restricts the values of \(p\) for which the oracle-complexity is improved over that of gradient descent. The range of application of Theorem 3.2 is given in the next result, where \(\) and \(\) represent maximum and minimum respectively.

**Corollary 3.1**.: _The algorithms given in Theorem 3.2 effectively provide a tradeoff for \(p(}{ d} d)\). Precisely, this provides a tradeoff between_

* _using_ \((d^{2})\) _memory with optimal_ \((d)\) _oracle-complexity, and_
* _using \((d^{2} d d)\) memory with \((}(C)^{d})\) oracle-complexity._

Importantly, for \(}\), taking \(p=d\) yields an algorithm that uses the optimal memory \((d)\) and has an improved query complexity over gradient descent. In this regime of small (virtually constant) dimension, for the same memory usage, gradient descent has a query complexity that is polynomial in \(\), \((})\), while our algorithm has poly-logarithmic dependence in \(\), \(_{d}(^{d})\), where \(_{d}\) hides an exponential constant in \(d\). It remains open whether this \(^{d}\) dependence in the oracle-complexity is necessary. To the best of our knowledge, this is the first example of an algorithm that improves over gradient descent while keeping its optimal memory usage in any regime where \(}\).

While this improvement holds only in the exponential regime \((d)}}\), Theorem 3.2 still provides a non-trivial trade-off whenever \( d\), and improves over the known memory-constrained center-of-mass in the standard regime \(}\). Fig. 1 depicts the trade-offs in the two regimes mentioned earlier.

Last, we note that the lower-bound trade-offs presented in  do not show a dependence in the accuracy \(\). Especially in the regime when \( d\), this yields sub-optimal lower bounds (in fact even in the regime \(=1/(d)\), our more careful analysis improves the lower bound on the memory by a \( d\) factor). We show with simple arguments that one can extend their results to include a \(\) factor for both memory and query complexity. Fig. 1 presented these improved lower bounds.

Figure 1: Trade-offs between available memory and first-order oracle-complexity for the feasibility problem over the unit ball. MC=Memory-constrained. GD=Gradient Descent. The left picture corresponds to the regime \( d^{-(d)}\) and \( 1/(d)\) and the right picture represents the regime \( d^{-(d)}\). For both figures, the dashed pink “L” (resp. green inverted “L”) region corresponds to historical lower (resp. upper) bounds for randomized algorithms. The solid pink (resp. red) lower bound tradeoff is due to  (resp. ) for randomized algorithms (resp. deterministic algorithms). The purple region is a lower bound tradeoff for the feasibility problem for accuracy \(\) and deterministic algorithms . All these lower-bound trade-offs are represented with their \(\) dependence (Theorem 3.3). We use memory-constrained Vaidya’s method to gain a factor \(\) in memory compared to memory-constrained center-of-mass , which gives the light green region, and a class of algorithms represented in dark green, that allows trading query-complexity for an extra \(/ d\) factor saved in memory (Theorem 3.2). The dark green dashed region in the left figure emphasizes that the area covered by our class of algorithms depends highly on the regime for the accuracy \(\): the resulting improvement in memory is more significant as \(\) is smaller. In the regime when \( d^{-(d)}\) (right figure), our class of algorithms improves over the oracle-complexity of gradient descent while keeping the optimal memory \((d)\).

**Theorem 3.3**.: _For \( 1/poly(d)\) and any \(\) (the notation \(\) hides \(^{(1)}d\) factors),_

1. _any (randomized) algorithm guaranteed to minimize_ \(1\)_-Lipschitz convex functions over the unit ball with accuracy_ \(\) _uses_ \(d^{5/4-}\) _bits of memory or makes_ \((d^{1+4/3})\) _queries,_
2. _any deterministic algorithm guaranteed to minimize_ \(1\)_-Lipschitz convex functions over the unit ball with accuracy_ \(\) _uses_ \(d^{2-}\) _bits of memory or makes_ \((d^{1+/3})\) _queries,_
3. _any deterministic algorithm guaranteed to solve the feasibility problem over the unit ball with accuracy_ \(\) _uses_ \(d^{2-}\) _bits of memory or makes_ \((d^{1+})\) _queries._

The proof is given in Appendix C and the arguments therein could readily be used to exhibit the \(\) dependence of potential future works improving over these lower bounds trade-offs.

Sketch of proof.At a high level, [31; 5] use a barrier term \(\|\|_{}\) where \(\) has \((d)\) rows: if an algorithm does not have enough memory, \(\) cannot be fully stored which in turn incurs a sub-optimal oracle-complexity. To achieve a \(\) improvement in memory (Appendix C.1), we modify the sampling of rows of \(\), from uniform on vertices of the hypercube to uniform in an \(\)-net. The proof can then be adapted accordingly. Last, one can improve the oracle-complexity by a \(/ d\) factor (Appendix C.2) using a standard rescaling argument .

## 4 Memory-constrained feasibility problem without computation

In this section, we present a class of algorithms that are memory-constrained according to Definition 2.1 and achieve the desired memory and oracle-complexity bounds. We emphasize that the memory constraint is only applied between calls to the oracle and as a result, the algorithm is allowed infinite computation memory and computation power between calls to the oracle.

We start by defining discretization functions that will be used in our algorithms. For \(>0\) and \(x[-1,1]\), we pose \(_{1}(x,)=sign(x) x/\). Next, we define the discretization \(_{d}\) for general dimensions \(d 1\). For any \( C\) and \(>0\),

\[_{d}(,)=(_{1}(x_{1}, /),,_{1}(x_{d},/ )).\]

### Memory-constrained Vaidya's method

Our algorithm recursively uses Vaidya's cutting-plane method  and subsequent works expanding on this method. We briefly describe the method. Given a polyhedron \(=\{:\}\), we define \(s_{i}()=_{i}^{}-b_{i}\) and \(_{x}=diag(s_{i}(x),i[d])\). We will also use the shorthand \(_{x}=_{x}^{-1}\). The volumetric barrier is defined as

\[V_{,}()=(_{x}^{}_{x}).\]

At each step, Vaidya's method queries the volumetric center of the polyhedron, which is the point minimizing the volumetric barrier. For convenience, we denote by \(\) this function, i.e., for any \(^{m d}\) and \(^{d}\) defining a non-empty polyhedron \(=\{:\}\),

\[(,)=_{:>}V_{ ,}().\]

When the polyhedron is unbounded, we can for instance take \((,)=\). Vaidya's method makes use of leverage scores for each constraint \(i\) of the polyhedron, defined as \(_{i}=(_{x}^{-1}_{x}^{})_{i,i}\), where \(=_{x}^{}_{x}\). We are now ready to define the update procedure for the polyhedron considered by Vaidya's volumetric method. We denote by \(_{t}\) the polyhedron stored in memory after making \(t\) queries. The method keeps in memory the constraints defining the current polyhedron and the iteration index \(k\) when these constraints were added, which will be necessary for our next procedures. Hence, the polyhedron will be stored in the form \(_{t}=\{(k_{i},_{i},b_{i}),i[m]\}\), and the associated constraints are given via \(\{:\}\) where \(^{}=[_{1},,_{m}]\) and \(^{}=[b_{1},,b_{m}]\). By abuse of notation, we will write \(()\) for the volumetric center of the polyhedron \((,)\) where \(\) and \(\) define the constraints stored in \(\).

Initially, the polyhedron is simply \(_{d}\), these constraints are given \(-1\) index for convenience, and they will not play a role in the next steps. At each iteration, if the constraint \(i[m]\) with minimum leverage score \(_{i}\) falls below a given threshold \(_{min}\), it is removed from the polyhedron. Otherwise, we query the volumetric center of the current polyhedron and add the separation hyperplane as a constraint to the polyhedron. We bound the number of iterations of the procedure by

\[T(,d)= c d(1.4+2 d+2(1+1/ _{min})),\]

where \(_{min}\) and \(c\) are parameters that will be fixed shortly. Instead of making a call directly to the oracle \(O_{S}\), we instead suppose that one has access to an oracle \(O:_{d}^{d}\) where \(_{d}=(^{d+1})^{}\) has exactly the shape of the memory storing the information from the polyhedron. This form of oracle is used in our recursive calls to Vaidya's method. For example, such an oracle can simply be \(O:_{d} O_{S}(( ))\). Last, in our recursive method, we will not assume that oracle responses are normalized. As a result, we specify that if the norm of the response is too small, we can stop the algorithm. We assume however that the oracle already returns discretized vectors, which will be ensured in the following procedures. The cutting-plane algorithm is formally described in Algorithm 1. With an appropriate choice of parameters, this procedure finds an approximate solution of feasibility problems. We base the constants from .

``` Input:\(O:_{d}^{d}\), \(\), \((0,1)\)
1 Let \(T_{max}=T(,d)\) and initialize \(_{0}:=\{(-1,_{i},-1),(-1,-_{i},-1),\ i[d]\}\)
2for\(t=0,,T_{max}\)do
3if\(\{:\}=\)thenreturn\(_{t}\);
4if\(_{i[m]}_{i}<_{min}\)then
5\(_{t+1}=_{t}\{(k_{j},_{j},b_{j})\}\) where \(j_{i[m]}_{i}\)
6elseif\(:=(_{t})_{d}\)then
7\(_{t+1}=_{t}\{(-1,-sign(_{j})_{j},-1)\}\) where \(j[d]\) has \(|_{j}|>1\)
8else
9\(=O(_{t})\) and \(b=^{}}{}\), where \(=(_{t})\)
10\(_{t+1}=_{t}\{(t,,b)\}\)
11if\(\|\|\)thenreturn\(_{t+1}\) ;
12
13 end if return\(_{T_{max}+1}\) ```

**Algorithm 1**Memory-constrained Vaidya's volumetric method

**Lemma 4.1**.: _Fix \(_{min}=0.04\) and \(c= 715\). Let \(,(0,1)\) and \(O:_{d}^{d}\). Write \(=\{(k_{i},_{i},b_{i}),i[m]\}\) as the output of Algorithm 1 run with \(O\), \(\) and \(\). Then,_

\[_{_{i} 0,\ i[m],\\ _{i[m]}_{i}=1}_{_{d}} _{i=1}^{m}_{i}(_{i}^{}-b_{i})=_{ _{d}}_{i[m]}_{i}^{}-b_{i}).\]

From now, we use the parameters \(_{min}=0.04\) and \(c=1/0.0014\) as in Lemma 4.1. Since the memory of both Vaidya's method and center-of-mass consists primarily of the constraints, we recall an important feature of Vaidya's method that the number of constraints at any time is \((d)\).

**Lemma 4.2** ([48; 1, 2]).: _At any time while running Algorithm 1, the number of constraints of the current polyhedron is at most \(}+1\)._

### A recursive algorithm

We write \(_{m+n}=_{m}_{n}\) and aim to apply Vaidya's method to the first \(m\) coordinates. To do so, we need to approximate a separation oracle on these \(m\) coordinates only, which corresponds to giving separation hyperplanes with small values for the last \(n\) coordinates. This can be achieved using the following auxiliary linear program. For \(_{n}\), we define

\[_{_{i} 0,\ i[m],\\ _{i[m]}_{i}=1}_{_{n}} _{i=1}^{m}_{i}(_{i}^{}-b_{i}), m=|| (_{aux}())\]where as before, \(\) and \(\) define the constraints stored in \(\). The procedure to obtain an approximate separation oracle on the first \(n\) coordinates \(_{n}\) is given in Algorithm 2 and using Lemma 4.1 we can show that this procedure provides approximate separation vectors for the first \(n\) coordinates.

``` Input:\(\), \(\), \(O_{x}:_{n}^{m}\) and \(O_{y}:_{n}^{n}\)
1 Run Algorithm 1 with \(,\) and \(O_{y}\) to obtain polyhedron \(^{}\)
2 Solve \(_{aux}(^{})\) to get a solution \(^{}\)
3 Store \(^{}=(k_{i},i[m])\) where \(m=|^{}|\), and \(^{}(^{},)\)
4 Initialize \(_{0}:=\{(-1,_{i},-1),(-1-_{i},-1),\;i[d]\}\) and \(=^{m}\)
5for\(t=0,1,,n_{}k_{i}\)do
6if\(t=k_{i}^{}\)for some \(i[m]\)then
7\(_{x}=O_{x}(_{t})\)
8\(_{m}(+_{i}^{}_{x},)\)
9 Update \(_{t}\) to get \(_{t+1}\) as in Algorithm 1
10
11 end if return\(\) ```

**Algorithm 2**\(_{,}(O_{x},O_{y})\)

The next step involves using this approximation recursively. We write \(d=_{i=1}^{p}k_{i}\), and interpret \(_{d}\) as \(_{k_{1}}_{k_{p}}\). In particular, for \(_{d}\), we write \(=(_{1},,_{p})\) where \(_{i}_{k_{i}}\) for \(i[p]\). Applying Algorithm 2 recursively, we can obtain an approximate separation oracle for the first \(i\) coordinates \(_{k_{1}}_{k_{i}}\). However, storing such separation vectors would be too memory-expensive, e.g., for \(i=p\), that would correspond to storing the separation hyperplanes from the oracle \(O_{S}\) directly. Instead, given \(j[i]\), Algorithm 3 recursively computes the \(_{j}\) component of an approximate separation oracle for the first \(i\) variables \((_{1},,_{i})\), via the procedure \((i,j)\).

``` Input:\(\), \(\), \(1 j i p\), \(^{(r)}_{k_{r}}\) for \(r[i]\), \(O_{S}:_{d}^{d}\)
1if\(i=p\)then
2\(_{r}=(_{r},_{r})\) where \((_{r},_{r})\) defines the constraints stored in \(^{(r)}\) for \(r[p]\)
3\((_{1},,_{p})=O_{S}(_{1},,_{p})\)
4return\(_{k_{j}}(_{j},)\)
5 end if
6 Define \(O_{x}:_{k_{i+1}}^{k_{j}}\) as \(_{,,_{f}}(i+1,j,^{(1)},, ,^{(i)},)\)
7 Define \(O_{y}:_{k_{i+1}}^{k_{i+1}}\) as \(_{,,_{f}}(i+1,i+1,^{(1)}, ,^{(i)},)\)
8return\(_{,,O_{S}}(O_{x},O_{y})\) ```

**Algorithm 3**\(_{,,O_{S}}(i,j,^{(1)},,^{( i)})\)

We can then use \(_{,,O_{S}}(1,1,)\) to solve the original problem with the memory-constrained Vaidya's method. In Appendix A, we show that taking \(=\) and \(=}{32d^{3/2}}\) achieves the desired oracle-complexity and memory usage. The final algorithm is given in Algorithm 4.

``` Input:\(\), \(\), and \(_{S}:_{d}^{d}\) a separation oracle
1 Check:Throughout the algorithm, if \(O_{S}\) returned \(\) to a query \(\), return\(\)
2 Run Algorithm 1 with parameters \(\) and \(\) and oracle \(_{,,O_{S}}(1,1,)\) ```

**Algorithm 4**Memory-constrained algorithm for convex optimization

A geometric illustration of the recursive step.In Figure 2, we give a 2-dimensional feasibility problem with target \(^{}=(p_{1}^{},p_{2}^{})\) and two blocks (i.e. \(p=2\)) as an illustration of our recursive approach (Algorithm 2) to construct an approximate separating hyperplane for a "reduced" problem.

Suppose at a step of the Algorithm 4, the current value of the \(x_{1}\) coordinate is \(c\). We aim to find an approximate separating hyperplane between \(x_{1}=p_{1}^{}\) and \(x_{1}=c\). Algorithm 2 first runs Algorithm 1 (i.e. the memory-constrained Vaidya) to find two separating hyperplanes (the two blue hyperplanes). Lemma 4.1 then guarantees the existence of a convex combination of the 2 blue hyperplanes - the red hyperplane- which is approximately parallel to the \(x_{2}\)-axis and thus can serve as an approximate separating hyperplane between \(x_{1}=p_{1}^{*}\) and \(x_{1}=c\).

Sketch of proof.At the high level, the algorithm recursively runs Vaidya's method Algorithm 1 for each level of computation \(i[p]\). Since each run of Algorithm 4 requires \(()\) queries, the total number of calls to the oracle, which is exponential in the number of levels, is \((()^{p})\). As for the memory usage, the algorithm mainly needs to keep in memory the constraints defining the polyhedrons at each level \(i[p]\). From Lemma 4.2, each polyhedron only requires \(()\) constraints that each require \(()\) bits of memory. Hence, the total memory needed is \((}{p})\). The main difficulty lies in showing that the algorithm is successful. To do so, we need to show that the precision in the successive approximated separation oracles Algorithm 2 is sufficient. To avoid an exponential dependence of the approximation error in \(p\)--which would be prohibitive for the memory usage of our method--each run of Vaidya's method Algorithm 1 is run for more iterations than the precision of the separation vectors would classically allow. To give intuition, if the separation oracle came from a convex optimization subgradient oracle for a function \(f\), the iterates at a level \(i\) do not converge to the true "minimizer" of \(_{_{i}}f^{(i)}(_{1},,_{i})\), where \(f^{(i)}()=_{_{i+1},,_{p}}f(,_{i+1}, ,_{p})\), but instead converge to a close enough point while still providing meaningful approximate subgradients at the higher level \(i-1\) (in Algorithm 2).

## 5 Discussion and Conclusion

To the best of our knowledge, this work is the first to provide some positive trade-off between oracle-complexity and memory-usage for convex optimization or the feasibility problem, as opposed to lower-bound impossibility results [31; 5]. Our trade-offs are more significant in a high accuracy regime: when \( d^{}\), for \(c>0\) our trade-offs are polynomial, while the improvements when \(=( d)\) are only in \( d\) factors. A natural open direction  is whether there exist algorithms with polynomial trade-offs in that case. We also show that in the exponential regime \((d d)\), gradient descent is not Pareto-optimal. Instead, one can keep the optimal memory and decrease the dependence in \(\) of the oracle-complexity from \(}\) to \(()^{d}\). The question of whether the exponential dependence in \(d\) is necessary is left open. Last, our algorithms rely on the consistency of the oracle, which allows re-computations. While this is a classical assumption, gradient descent and classical cutting-plane methods do not need it; removing this assumption could be an interesting research direction (potentially, this could also yield stronger lower bounds).

Figure 2: Intuition for the recursive procedure in Algorithm 4. Using the separation hyperplanes (blue) found by Algorithm 1, i.e., the memory-constrained Vaidya, it constructs an approximate separation hyperplane (red) between \(x_{1}=c\) and the target \(x_{1}=p_{1}^{*}\).