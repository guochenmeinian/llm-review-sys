# Instance-Optimal Private Density Estimation in the Wasserstein Distance

Vitaly Feldman

Apple

&Audra McMillan

Apple

&Satchit Sivakumar\({}^{*}\)

Boston University

&Kunal Talwar

Apple

###### Abstract

Estimating the density of a distribution from samples is a fundamental problem in statistics. In many practical settings, the Wasserstein distance is an appropriate error metric for density estimation. For example, when estimating population densities in a geographic region, a small Wasserstein distance means that the estimate is able to capture roughly where the population mass is. In this work we study differentially private density estimation in the Wasserstein distance. We design and analyze instance-optimal algorithms for this problem that can adapt to easy instances.

For distributions \(P\) over \(\), we consider a strong notion of instance-optimality: an algorithm that uniformly achieves the instance-optimal estimation rate is competitive with an algorithm that is told that the distribution is either \(P\) or \(Q_{P}\) for some distribution \(Q_{P}\) whose probability density function (pdf) is within a factor of 2 of the pdf of \(P\). For distributions over \(^{2}\), we use a different notion of instance optimality. We say that an algorithm is instance-optimal if it is competitive with an algorithm that is given a constant-factor multiplicative approximation of the density of the distribution. We characterize the instance-optimal estimation rates in both these settings and show that they are uniformly achievable (up to polylogarithmic factors). Our approach for \(^{2}\) extends to arbitrary metric spaces as it goes via hierarchically separated trees. As a special case our results lead to instance-optimal private learning in TV distance for discrete distributions.

## 1 Introduction

Distribution estimation is a fundamental problem in statistics. In this work, we focus on the problem of learning the density of a distribution over a low-dimensional real space. Our motivation for studying this problem comes from practical problems such as estimating the population density in a geographical area (defined by bounded two dimensional space, for e.g. \([0,]^{2}\)), learning the distribution of accuracy of a machine learning model (i.e. a distribution over \(\)), estimating the average temperature across latitude, longitude, and altitude (i.e. a distribution over \([0,]^{3}\)) etc.

In this work, we are interested in the _non-parametric_ version of this question, where we make no assumptions on the form of the distribution we are learning. This is frequently of interest in practice, where population densities for example may change over time (become more or less concentrated), and it is difficult to specify a meaningful parametric class that will simultaneously capture all densities of interest. Given estimation is often done using sensitive data (for e.g. health data), our interest in this question is in, and consequently all our results are for, the differentially private version of this question. While we believe our results in the non-private setting are also novel and interesting, we view the private results as our main contribution.

Any statistical algorithm learning from samples is inexact. The appropriate gauge to measure the (in)accuracy of a density estimation algorithm depends on how this density estimate is used. In this work, we focus on the _Wasserstein_ distance between the original distribution and the learntdistribution as our measure of accuracy. Known by many names (Earthmover distance, Kantorovich distance, Optimal Transport distance), this distance is defined over any distance metric \(d\) as the minimum over all couplings \(\) from \(P\) to \(Q\) of the quantity \(_{x P}[d(x,(x))]\). It is arguably one of the most natural ways to define distances between distributions over a metric space and has been extensively studied (see Appendix C). We note that Wasserstein distance is particularly salient in many practical applications of density estimation where the geometry of the space is significant. As a simple example, when creating population density estimates, if the population is concentrated in a few cities, then outputting a distribution concentrated close to these cities (even if not exactly at the cities) is intuitively better than outputting a distribution that is more spread out. Metrics such as TV distance that do not incorporate the geometry of the space do not capture this nuance. Additionally, Wasserstein distance is versatile and can be adapted to the setting of interest by varying the metric. In the case of the metric being a discrete metric with \(d(x,y)=(x y)\), it reduces to the commonly used total variation \(()\) distance. Our focus in this work is on the case of Euclidean distance metric on \(\) or \(^{2}\), though our results apply to both to higher-dimensional Euclidean space as well as to any finite metric. In the \(\) case (with the standard Euclidean metric), the Wasserstein distance is equivalent to the total area between the cumulative distribution functions.

The problem of learning a distribution under Wasserstein distance has a long history, starting with  proving worst-case bounds on the rate of convergence of the Wasserstein distance between the empirical distribution \(_{n}\) and the target distribution \(P\) over \(^{d}\). Similarly, this question for the case of the discrete metric (\(d(x,y)=(x y)\)) has been very well studied. However, most known results for this problem look at it from the point of view of worst-case analysis. This can paint a rather pessimistic picture. For example, the minimax rate of \(\)-privately learning a discrete distribution over \(\{0,,k\}\) in TV distance (i.e. Wasserstein with the discrete metric described above) scales linearly with \(k\), which can be prohibitive for large support size \(k\). For Wasserstein distance with \(_{2}\) norm, the rate of convergence of the empirical distribution suffers a curse of dimensionality, with the worst-case error between the distribution and the empirical distribution being \((n^{-})\) for distributions over \(^{d}\). For the differentially private version of this question, recent works  have shown that the optimal Wasserstein minimax error between the sample and the private estimate is \((( n)^{-})\). This worst-case analysis viewpoint fails to distinguish between algorithms that perform very differently on the types of instances one may see in practice. In particular, many practical distributions may be more feasible to estimate than suggested by the minimax rate. As an example, Figure 1 shows the cumulative distribution function of a bimodal distribution on \(\) with very sparse support, and the cdf learnt by a minimax optimal algorithm, as well as an algorithm we present in this work (See Appendix F for details on this experiment). As is clear from the figure, the minimax optimal algorithm is easily outperformed. This phenomenon only gets worse in higher dimensions. Similarly, the distribution in \(^{d}\) lies on a \(k\)-dimensional subspace, the worst-case error scaling with \((( n)^{-})\) is significantly larger than our algorithm's scaling of \((( n)^{-})\).

This motivates the problem of viewing this question through the lens of _instance optimality_. 2 Briefly, instance optimal algorithms are those that on any given instance of the problem, are able to perform competitively with what any algorithm can do on this instance. Let \(\) be a class of algorithms of interest (e.g. all \((,)\)-differentially private algorithms) and \(cost(,P)\) be a cost measure for an instance \(P\). In our setting, we have a distribution \(P\) over a metric space, and given a set \(_{n}\) of \(n\) samples from \(P\), we want to learn an estimate \((_{n})\) for the distribution. Our measure of performance is the Wasserstein distance \(\), so \(cost(,P)=[(P,(_{n}))]\). We would ideally like to say that an algorithm \(\) is \(\)-instance optimal in a class \(\) if for all instances \(P\), and all \(^{} M\),

\[cost((_{n})),P) cost(^{}( _{n})),P).\] (InstanceOptimality-Ideal)

The reader would have noticed that this definition is however impossible to achieve except for trivial classes \(\). The algorithm \(^{}\) that ignores its input and always outputs \(P\) makes the right hand side 0. However, this algorithm performs poorly on any distributions far from \(P\) and so is not a reasonable benchmark. A common approach in many works is to measure the performance of the competing algorithm \(^{}\) not just on the given instance, but on a small neighborhood around it. Thus we say that that an algorithm \(\) is \(\)-instance optimal amongst a class \(\) with respect to a neighborhood function \(\) if for all instances \(P\), and all \(^{} M\)

\[cost((_{n})),P)_{P^{} (P)}cost(^{}(^{}{}_{n})),P^{}).\]

In other words, the benchmark we evaluate against is the cost of the best algorithm for a neighborhood \((P)\)_that knows this neighborhood_. We would like our algorithm \(\), that is not tailor-made for \((P)\), to nevertheless be competitive against this benchmark.

This definition is general, and captures most notions of instance optimality that have been studied in the literature. The set \((P)\) must be carefully defined for this notion to be meaningful; we can always define \((P)\) to be the set of all instances whence this notion reduces to worst-case analysis. In many previous works, this neighborhood map has been defined to capture the belief that any natural algorithm must not have significantly different performance on different members of \((P)\). For example,  include in \((P)\) appropriate renamings of \(P\) to capture some kind of permutation invariance of natural algorithms. In statistics, one often enforces that the cardinality of \((P)\) is \(2\), often called the _hardest one-dimensional subproblem_. Some recent works in privacy  have defined instance optimality w.r.t. neighboring datasets obtained by deleting a small number of data points. Any reasonable definition of instance optimality for a problem must justify its choice of the neighborhood map; similar choices must be justifiable in every other notion of beyond worst case analysis . In instance-optimality definitions, this choice of neighborhood is what encapsulates what class of domain-specific algorithms our algorithm competes against. A good definition thus depends on the context and on the kind of domain knowledge we imagine an expert designing a custom algorithm for an application may have. Ideally, the definition is broad (i.e. the neighborhoods \((P)\) are sufficiently contained) so that in a large class of applications, we expect the domain knowledge to not be enough to rule out any member of \((P)\). We discuss this general definition of instance optimality further in Appendix B. We remark that for reasonable neighborhood maps, this is an extremely strong requirement: an instance-optimal algorithm must simultaneously do well on every single input, in fact as well as any other algorithm that is given this neighborhood \((P)\) in advance!

Instance optimality guarantees are most useful when there is a big difference between achievable utility guarantees for typical cases and the worst-case utility guarantees. Wasserstein estimation is an example of such a problem. We will see that achievable utility bounds for, for example, concentrated distributions are a lot better than worse case distributions. Our definition of instance optimality is particularly suitable for metric spaces, and our notion of neighborhood allows the target utility bound to adapt to the distribution. We note that for estimation in Wasserstein distance with practically important metrics such as \(_{1}\) and \(_{2}\) norms, it is unclear if existing instance optimality

Figure 1: (Left) A sparsely supported distribution on integers  (pdf). (Right) CDF for the same distribution (green, solid line), along with a (non-private) minimax optimal learnt distribution (blue, dashed line), as well as 1-DP instance-optimal algorithm (red, dotted), both learnt from the same 1600 samples. The \(W_{1}\) error for the minimax optimal algorithm is 13.4, whereas the DP estimated distribution has \(W_{1}\) error of \(0.86\). While this example is artificial, it demonstrates the large potential gap between minimax optimal and instance optimal algorithms on specific instances.

definitions (using notions of neighborhood discussed above) capture this. For example, for discrete distributions, setting the neighborhood to be all permutations of the distribution destroys all structure of the distribution (for e.g. concentration), and hence performance on this neighborhood may not capture the relative ease of estimation of a concentrated distribution. Similar problems apply to other previously studied definitions of instance optimality, which are not well-suited to density estimation with error metrics that incorporate the geometry of the metric space. See Appendix B and Appendix C for further discussion on the inadequacy of existing instance optimality definitions for our setting of interest.

Our notion of neighborhood will correspond to small balls in one of the strictest notions of distance between distributions. Recall that for distributions \(P,Q\) on \(X\), \(D_{}(P,Q)\) is defined as \(_{x X}(,)\). Our neighborhood map \(\) will have the property that for all \(P\), and for all \(Q(P)\), \(D_{}(P,Q) 2\). This corresponds to the benchmark algorithm \(^{}\) being given as auxiliary input a multiplicative constant factor approximation to the probability density function \(P(x)\) (and we can replace the constant \(2\) by any constant). In particular, an algorithm that knows the support of the distribution \(P\) will not be able to do much better than our algorithm that gets no such information. Notice that this implicitly implies that our algorithm is able to exploit sparsity in the data distribution since it is competitive with an algorithm that is told the support. In the one-dimensional real case we can achieve an even stronger notion of instance-optimality. In this case \((P)\) is defined to be \(\{P,Q\}\) where \(Q\) is a distribution with \(D_{}(P,Q) 2\). This is a strengthening of the rate defined by the _hardest one-dimensional subproblem_.

We also give a definition that captures another aspect of instance optimality, related to the notion of super efficiency, that we term local minimality in Appendix B. Informally, local minimality says that if any comparator algorithm does better than \(\) on \(P\), then there is a distribution \(Q\) in the neighborhood of \(P\) where \(\) does better than the comparator. Approximate local minimality relaxes the latter condition to being better than some constant times the comparator. The two definitions of approximate local minimality and instance optimality are in general incomparable (see Appendix B) but for suitable smooth algorithms, we show that these definitions are equivalent. Our algorithms, both for the 1-dimensional and the case of general metric spaces approximately satisfy both these definitions.

In order to show that the instance optimality definition is achievable, we give both algorithmic upper bounds and matching, up to logarithmic factors, theoretical lower bounds. The algorithms we use in our upper bounds are built largely from ingredients previously used for similar problems. We see this as an asset since these algorithms are implementable in practice. A key ingredient that we do introduce is the use of randomised HST approximation of finite metric spaces. This replaces deterministic hierarchical decompositions that were used in prior work, allowing us to gain tighter utility guarantees. Our main conceptual contribution is to introduce what we believe to the right notion of instance optimality for this problem, including the definition of a meaningful neighbourhood function. The main technical challenge is in the lower bounds, which require carefully building nets of distributions within each neighborhood \((P)\) that allow us to use a slight generalisation of DP Assoud's Lemma to give a lower bound on the target estimation rate for each distribution \(P\).

Preliminaries:First, we define differential privacy. Further discussion on differential privacy can be found in Appendix A.

**Definition 1.1** (Differential Privacy ).: _A randomized algorithm \(:^{n}\) is \((,)\)-differentially private if for every pair of datasets \(,^{}^{n}\) that differ in at-most one data entry, and for all events \(Y\),_

\[[() Y] e^{}[( ^{}) Y]+.\]

Given an estimation algorithm \(:^{n}\), the estimation rate of \(\) for distribution \(P\) is:

\[R_{,n}(P)=_{t}\{t: 0.75 P^{n}_{d}((),P) t\}.\] (1)

### Our Results

We start by stating an informal version of our result in the one-dimensional real case.

**Theorem 1.2** (Informal 1-dimensional result).: _Let \(,(0,1]\). There is an \(\)-differentially private algorithm \(\) such that, for all distributions \(P\) supported in \(\), for all natural numbers \(n>^{1/}}{}\), there exists a distribution \(Q\) (with \(D_{}(P,Q) 2\)) such that the following is satisfied._

_For any \(\)-DP algorithm \(^{}\), with probability at least \(0.75\) over the randomness of \( P^{n}\) and additional randomness of the algorithm,_

\[((_{n}),P)n _{P^{}\{P,Q\}}R_{^{},n^{}}(P^{})+\]

_where \(n^{}n/}\)_

In this one-dimensional case, our algorithm is based on DP quantile estimation. The additive \(\) term can be made polynomially small. The lower bound is based on (differentially private) simple hypothesis testing where for each distribution \(P\), we find a distribution in \((P)\) that is indistinguishable from \(P\) given \(n\) samples but also sufficiently far from \(P\) in Wasserstein distance.

Extending the quantiles based approach from the one dimensional setting to even the two dimensional setting is challenging, as there is no "right" way to generalize quantiles to dimensions 2 or beyond. Several previous works on Wasserstein density estimation (e.g. ) have used a hierarchical decomposition approach to address this question. A hierarchical approach has also been used in various more practical works on private density estimation (e.g. ). These works focus on practical performance and do not offer tight theoretical bounds. A hierarchical approach was also used by , who proved theoretical bounds for a related problem, but not through the lens of instance optimality. We compare our results to theirs in more detail later in this section.

The use of deterministic hierarchical decompositions in all these papers means that some points that are very close (but on opposite sides of the boundaries of the hierarchical decomposition) get mapped to relatively far points, resulting in high distortion factors that are not appropriate for instance optimality.

Inspired by the above approaches but noting their constraints, we use a randomized embedding into hierarchically separated trees instead of a deterministic one. We define our algorithm on any hierarchically separated tree metric and use the fact that there is a randomized embedding of \(^{2}\) on a hierarchically separated tree metric space with low distortion. This, along with some other important technical modifications (such as truncating low values to \(0\)), allows us to analyze a variant of the above practical algorithms theoretically and show that it satisfies our strong notion of instance optimality, up to polylogarithmic factors in the number of samples.

**Theorem 1.3** (Informal two-dimensional result).: _There is a polynomial time \(\)-differentially private algorithm \(\) that for any distribution \(P\) on \(^{2}\), any integer \(n\), and any \(\)-DP algorithm \(^{}\) with probability at least 0.75, satisfies_

\[_{2}((_{n}),P)( n)^{O(1)}_{P^{ }:D_{}(P,P^{}) 2}[_{2}(^{ }(}_{n^{}}),P^{})],\]

_where \(n^{}n}\). Here, the expectation is taken over the internal coin tosses of \(\) as well as over the choice of the i.i.d. samples \(_{n}\)._

In fact, since our algorithm is defined on any hierarchically separated tree metric space, it has the added bonus of giving instance optimality results for any finite metric space (since powerful results  show that any finite metric space can be embedded in a hierarchically separated tree metric space with a distortion factor at most logarithmic in the size of the metric space).

**Theorem 1.4** (Informal finite metric result).: _Let \((,d)\) be an arbitrary metric space with diameter \(1\). There is a polynomial time \(\)-differentially private algorithm \(\) such that for any distribution \(P\) on \(X\) any integer \(n\) and any \(\)-DP algorithm \(^{}\) with probability at least 0.75, satisfies_

\[((_{n}),P)(|| n)^{O( 1)}_{P^{}:D_{}(P,P^{}) 2}[( ^{}(}_{n^{}}),P^{})],\]

_where \(n^{}n}\). Here, the expectation is taken over the internal coin tosses of \(\) as well as over the choice of the i.i.d. samples \(_{n}\)._Our lower bound result is actually slightly stronger than stated in Theorem 1.4 since it holds not only for \(\)-DP, but also for \((,)\)-DP. At this point, we also compare specifically to the paper of [GHK\({}^{+}\)23] who give an algorithm for obtaining two-dimensional heatmaps and analyze it theoretically. They focus on the empirical version of a variant of this problem as opposed to the population version, and aim to compete with the best \(k\)-sparse distribution. Their algorithm takes the sparsity parameter \(k\) as input in order to set parameters and achieves additive error \(/n\) (and a constant multiplicative factor). On the other hand, our algorithm also performs better for sparse distributions but is _automatically adaptive_ to the sparsity (and hence doesn't need to take it as an input). Additionally the additive term in our work can be made polynomially small (for any polynomial) in \(n\) at a logarithmic cost to the multiplicative error (regardless of the sparsity of the distribution). On the other hand, for large \(k\) their results have additive error that scales with \(1/\). Their use of a deterministic hierarchical decomposition makes their algorithm unsuitable for our notion of instance optimality (as discussed earlier), and it is unclear if their algorithm can be directly extended to all finite metric spaces.

Note that instance optimality for all finite metric spaces implies instance optimality results for a wide variety of applications not addressed in prior work. For example, our results immediately extend to other low-dimensional real spaces with arbitrary metrics (for e.g. \(_{p}\) norms). They also give non-trivial improvements on worst-case analysis for higher-dimensional spaces that are not the main focus of our work (for \(^{d}\), we can use a fine grid of size \((1/(/))^{d}\) at an additive cost of \(\) in the Wasserstein distance in order to create a finite metric space to apply our result on. Since the dependence on \(||\) in the result above is logarithmic, this translates to a \(d\) multiplicative overhead term replacing the \(||\) factor above. While this is still a significant overhead, all previous results on density estimation in the Wasserstein distance (in both the private and non-private literature) are worst case, where the sample complexity is exponential in \(d\). Since our results only have a polynomial dependence in \(d\) over the optimal error, this is a non-trivial improvement over worst-case error, even when \(d\) is large.

Another immediate application of our results is to give (to the best of our knowledge) new bounds for private estimation of discrete distributions in TV distance. Generally, for learning a discrete distribution defined by probabilities \(\{p_{1},,p_{k}\}\), our results lead to a rate (up to polylogarithmic factors) of \(_{i}\{p_{i}(1-p_{i}),(1-p_{i})}{n}}\}+ _{i}(p_{i},(1-p_{i}),).\)

This can give significant improvements over the worst case bounds for practically important distributions. The minimax rate is linear in the support size \(k\), namely \((k/ n)\) (for sufficiently small \(\)). Now, consider the following power-law distribution over support size \(k\): \(p(i) i^{-2}\). (Power law distributions arise frequently in practice for e.g. frequencies of family names, sizes of power outages etc. all follow power law distributions.) Applying our result above gives a bound that is \((\{,}\})\), which is much better than the worst case bound for large support distributions.

Our result also applies to other practically important settings such as building lists of popular sequences such as n-grams over words. We leave open the questions of designing instance-optimal algorithms for other practically important questions in private learning and statistics, and of designing better instance optimal algorithms for higher dimensional spaces. We also leave open the question of removing the polylogarithmic factors in our instance optimality bounds.

### Techniques

#### 1.2.1 Distributions over \(\):

We start by describing the rate we obtain for distributions \(P\) over \(\).In order to state the rate, we will use \(q_{}\) to represent the \(\)-quantile of the distribution \(P\) and use \(P|_{a,b}\) to define a certain restricted distribution described below. The rate consists of three terms and roughly looks as follows-- we suppress logarithmic factors in \(n\).

\[R_{,n}(P)=([(P, _{n})]+(q_{1-} -q_{})+(P,P|_{q_{},q_{1-}})),\]

The first term is \([(P,_{n})]\), the expected Wasserstein distance between the true distribution and the empirical distribution over \(n\) samples, and is the non-private term. The remaining two termsrepresent the cost of privacy- the first is a specific interquantile distance, roughly \((q_{1-}-q_{})\), and the second can be thought of as capturing the weight of the tails- represented by the Wasserstein distance between \(P\) and a'restricted' version of \(P\) with its tails chopped off (i.e. the cumulative distribution function is \(0\) below \(q_{1/ n}\) and \(1\) above \(q_{1-1/ n}\) and identical to \(P\) otherwise). Observe that all \(3\) of the terms above are smaller for distributions with small support or greater concentration, and hence the rate adapts to the hardness of the distributions.

**Upper Bounds:** The upper bound involves estimating roughly \( n\) equally spaced quantiles of the empirical distribution differentially privately (using a known private CDF estimation algorithm), and placing roughly \(1/ n\) mass at each of the estimated quantile points. For the analysis, the intuition for each of the terms is as follows: since we only have access to the empirical distribution, the non-private term \([(P,_{n})]\) comes from that. Next, if the quantile estimates are good, then the pointwise CDF differences between the empirical distribution and the estimated distribution are at most \(1/ n\) (due to the discretization), and so we will pay \(1/ n\) multiplied by the interquantile distance of the empirical distribution. This aligns with the accuracy of state-of-the-art DP quantile estimation algorithms. Finally, since the distribution is restricted to the estimated quantiles, the distribution is \(0\) before the first estimated quantile and \(1\) above the last estimated quantile and so we pay the Wasserstein distance between the empirical distribution and a restricted version of the empirical distribution. Some care needs to be taken while reasoning about expectation versus high probability (for various terms), and in relating population quantities to empirical quantities (which we do using various concentration inequalities). Details can be found in Section E.2.

**Lower Bounds:** We prove that the private and non-private terms are lower bounds separately. Both proofs follow the same framework. The idea is that given knowledge of two distributions \(P\) and \(Q\), we can use a (private) Wasserstein estimation algorithm to construct a hypothesis test distinguishing \(P\) from \(Q\). If the (private) estimate for \(P\) and \(Q\) with \(n\) samples gives error smaller than \((P,Q)\), we can use this to distinguish \(P\) from \(Q\). This would give a contradiction if \(P\) and \(Q\) are (privately) indistinguishable with \(n\) samples. Hence, this would give a lower bound of \((P,Q)\) on the error of the Wasserstein estimation algorithm on \(P\) or \(Q\).

Thus the task reduces to constructing a distribution \(Q\) that satisfies three properties: 1) it is (privately) indistinguishable from \(P\) given \(n\) samples, 2) the Wasserstein distance between \(P\) and \(Q\) is sufficiently large, 3) \(D_{}(P,Q) 2\). The main technical work is in identifying a distribution \(Q\) that satisfies these properties.

For the privacy term, we construct the distribution \(Q\) by taking half the mass from the first \(1/ n\)-quantile of \(P\) (scaling the density function by half) and moving it to the last \(1/ n\)-quantile of \(P\) (scaling the density function by \(3/2\)). The third property is satisfied by definition, so we reason about the other two. Intuitively, since the Wasserstein distance captures how hard it is to'move' \(P\) to \(Q\), this mass needs to move at least the interquantile distance to change \(P\) to \(Q\). This implies that the Wasserstein distance is at least the interquantile distance scaled by \(1/ n\), as described in the rate. Additionally, mass that is further out in the tail needs to move more, which is captured by the Wasserstein distance between the distribution \(P\) and its'restriction'. Hence, the Wasserstein distance between \(P\) and \(Q\) is lower bounded by these two terms of interest. The intuition behind Property 2 is that it is hard for any \(\)-DP algorithm to pinpoint the location of an \(\)-fraction of the points in the dataset. Overall, this shows the privacy lower bound.

The non-private lower bound requires a more careful construction of \(Q\). We divide \(P\) into various scales and carefully adjust them differently to obtain the desired properties. Formally, to construct \(Q\) from \(P\), we consider \(q_{1/2}\) and all quantiles of the form \(q_{1/2^{i}}\) and \(q_{1-1/2^{i}}\) for \(i>1\). For \(1 i< n\), we add mass to \([q_{1/2^{i+1}},q_{1/2^{i}})\), by setting the density \(f_{Q}\) to be \((1+/n})f_{P}\) and balance out the extra mass by setting \(f_{Q}\) to be \((1-/n})f_{P}\) between \([q_{1-1/2^{i}},q_{1-1/2^{i+1}})\). For \(i n\) (i.e. the tail), we add mass to \([q_{1/2^{i+1}},q_{1/2^{i}})\), by setting \(f_{Q}\) to be \((1+)f_{P}\) and balance out the extra mass by setting \(f_{Q}\) to be \((1-)f_{P}\) between \([q_{1-1/2^{i}},q_{1-1/2^{i+1}})\).

The third property is again trivially satisfied. For the first property, observe that to'move' \(P\) to \(Q\) the extra \(n}}\) mass between \([q_{1/2^{i+1}},q_{1/2^{i}})\) has to 'travel' between \(q_{1/2^{i}}\) and \(q_{1-1/2^{i}}\), and so the Wasserstein distance between \(P\) and \(Q\) can be lower bounded by a sum of various scaled interquantile distances. We attempt to upper bound the expected Wasserstein distance between \(P\) and \(_{n}\) by a similar term. It is more intuitive to reason about this using an alternative (equivalent) for mulation of Wasserstein distance as the area between the CDF curves of \(P\) and \(Q\). The intuition is that the expected pointwise CDF difference between \(P\) and \(_{n}\) in the interval \([q_{1/2^{i+1}},\,q_{1/2^{i}})\) would be roughly \(n}}\) (by properties of a Binomial) and hence the contribution of this interval to the area would be roughly \(n}}(q_{1/2^{i}}-q_{1/2^{i+1}})\) and similarly for the corresponding interval \([q_{1-1/2^{i}},\,q_{1/2^{i+1}})\). Hence, the expected Wasserstein distance would be a sum of these scaled quantile interval distances. We formalize this intuition using a result of Bobkov and Ledoux  that characterizes the expected Wasserstein distance between \(P\) and \(_{n}\) as an integral of a function of the CDF of \(P\). We now have a bound in terms of the sum of scaled quantile interval distances, but we want to bound it by a sum of scaled _interquantile_ distances. We can telescope the sum to indeed bound it by a sum of scaled _interquantile_ distances. This establishes that \((P,Q)[(P,_{n}]\). Next, we show that \(P\) is indistinguishable from \(Q\) by analyzing the KL divergence between \(P\) and \(Q\). The main idea is that high density intervals are modified by a small multiplicative factor of roughly \(1+}\), but low density intervals (with mass less than \(1/n\)) are modified by a constant multiplicative factor, so overall the contribution of each interval to the KL divergence is sufficiently small. This establishes indistinguishability with \(n\) samples. For formal details we refer the reader to Section E.1.

#### 1.2.2 Distributions on HSTs

Since the main technical challenge of proving Theorem 1.4 is proving the equivalent result for distributions on HST metric spaces, we focus on that problem in this section. Standard results on low distortion embeddings of metric spaces into HST metric spaces can be used to translate the HST result to \(^{2}\) and to general metric spaces \(X\) with \(|X|\) overhead.

**Definition 1.5** (Hierarchically Separated Tree).: _A hierarchically separated tree (HST) is a rooted weighted tree such that the edges between level \(\) and \(-1\) all have the same weight (denoted \(r_{}\)) and the weights are geometrically decreasing so \(r_{+1}=(1/2)r_{}\). Let \(D_{T}\) be the depth of the tree._

HSTs can be defined with any geometric scaling but we will only need a factor of 2 in this work. HSTs may also have arbitrary degree. A HST defines a metric on its leaf nodes by defining the distance between any two leaf nodes to be the weight of the minimum weight path between them.

HST metric spaces are particularly well-behaved when working with the Wasserstein distance since the Wasserstein distance on a HST has a simple closed form. A distribution \(P\) on the the underlying metric space in a HST induces a function \(_{P}\) on the nodes of the tree where the value of a node \(\) is given by the weight in \(P\) of the leaf nodes in the subtree rooted at \(\). For every level \([D_{T}]\) of the tree, let \(P_{}\) be the distribution induced on the nodes at level \(\) where the probability of node \(\) is \(_{P}()\). Thus \(P_{}\) is a discrete distribution on a domain of size \(N_{}\), where \(N_{}\) is the number of nodes in level \(\) of the tree.

**Lemma 1.6** (Closed form Wasserstein distance formula).: _Given two distributions \(P\) and \(Q\) defined in a HST metric space, the Wasserstein distance between \(P\) and \(Q\) has the closed formula:_

\[(P,Q)=_{}r_{}|_{P}()-_{Q}()|=_{}r_{}(P_{},Q_{}),\]

_where \(r_{}\) is the weight of the edge connecting \(\) to its parent, and the sum is over all nodes in the tree._

We will call a node \(\)_\(\)-active_ under the distribution \(P\) if \(_{P}()\). Let \(_{P}()\) be the set of \(\)-active nodes under \(P\) and \(_{P_{}}()\) be the set of \(\)-active nodes at level \(\). Then there exists an algorithm \(\) such that given a distribution \(P\), \(>0\), and \(n\),

\[_{,n}(P)=(_{}r_{}_{x[N_{ }]}\{P_{}(x)(1-P_{}(x)),(x)(1-P_{ }(x))}{n}}\}+_{x_{P_{}}(2)}P_{}(x)+(| _{P_{}}(2)|-1)),\]

where the max is over all the levels of the tree and \(=()\). Further, this bound matches (up to logarithmic factors) the lower bound \(_{\,^{}}_{P^{}:D_{} (P,P^{}) 2}[(^{}(^{} {}_{n^{}}),P^{})]\) where \(n^{}n}\). The error rate \(_{,n}\) does indeed adapt to easy instances as we expected. The error decomposes into three components. The first component is the non-private sampling error; the error that would occur even if privacy was not required. The second component indicates that we can not privately estimate the value of nodes that have probability less than \( 1/( n)\). The third component is the error due to privacy on the active nodes. If \(P\) is highly concentrated then we expect most nodes to either be \(\)-active or have weight 0, so the first two terms in \(_{,n,}(P)\) are small. There should also be few active nodes, making the last term also small. Conversely, if \(P\) has a large region of low density then we expect a large number of inactive nodes, as well as non-zero inactive nodes that are at higher levels of the tree and hence contribute more to the final term. Thus, in distributions with high dispersion we expect the right hand side to be large.

**Upper Bounds:** As in the one-dimensional setting, we want to restrict to only privately estimating the density at a small number (\( n\)) of points. While we could try to mimic the one-dimensional solution by privately estimating a solution to the \( n\)-median problem, it's not clear how to prove such an approach is instance-optimal. It turns out that a simpler solution more amenable to analysis will suffice. Our algorithm has two stages; first we attempt to find the set of \(\)-active nodes, then we estimate the weight of these active nodes. Since these nodes have weight greater than \(\), we can privately estimate them to within constant multiplicative error. Any nodes that are not detected as active, are initially ascribed a weight of 0. The error due to not estimating the non-active nodes is absorbed into the third error term. The final step is to project the noisy density function into the space of distributions on the underlying metric space. The error of the upper bound algorithm is summed over all levels of the tree, although since the depth of the tree is logarithmic in the size of the metric space, this is within a logarithmic factor of the maximum over the levels.

**Lower Bound:** We first observe that in order to estimate the distribution well in Wasserstein distance, an algorithm must estimate each level of the tree well in TV distance. This is derived from Lemma 1.6. This allows us to reduce to the problem of lower bounding the error of density estimation of discrete distributions in TV distance. The main tool we use is a differentially private version of Assouad's method. Similar to how the technique in the previous section allowed us to relate lower bounding estimation rates to simple hypothesis testing, Assouad's lemma allows us to relate lower bounding estimation rates to multiple hypothesis testing. Note that unlike the technique in the previous section, Assouad's lemma allows us to prove lower bounds on the expected error, rather than lower bounds on high probability error bounds. It involves constructing nets of distributions in \((P)\) that are pairwise far in the relevant metric of interest (which for us in the TV distance) but the multiple hypothesis testing problem between the distributions is sufficiently hard. For proving the third term belongs in the lower bound, the standard statement of DP Assouad's lemma  suffices, where one builds a set of distributions indexed by a hypercube. For the first and second terms, we need to slightly generalise the statement to allow for sets of distributions indexed by a product of hypercubes. We use the approximate DP version of DP Assouad's so while our upper bounds are for pure differential privacy, our lower bounds hold for both pure and approximate differential privacy.

Let us start with the third term. Suppose the number of active nodes is even (a small tweak is made if there is an odd number of active nodes). We pair up the active nodes and index each pair by a coordinate of the hypercube. For each corner of the hypercube, \((u^{0},u^{1},,u^{k})\{ 1\}^{k}\), for each coordinate \(j[k]\), if \(u^{j}=+1\), we move \(()\) mass from one node in the \(j\)th pair to the other node. If \(u^{j}=-1\) then we leave the \(j\)th pair of nodes alone. Since each active node has mass \(>\), it's clear that each resulting distribution belongs in \((P)\). We can also show that these distributions form a sufficiently hard multiple hypothesis testing problem. By DP Assouad's (Lemma D.8), this allows us to lower bound the estimation error by \((k)\), which is within \(\) of the third term when the number of active nodes is \( 2\). We treat the case where there is a single active node separately.

For the second term, we want to pair up the inactive nodes in a similar manner and move half their mass from one node to the other. However, since we want to remain within \((P)\), we can't pair any two inactive nodes together. Thus, we divide the inactive nodes into _scales_, where nodes within a certain scale all have weight within a multiplicative factor of two. We then pair up nodes within each scale and have a different hypercube for each scale. Again, it's clear that these distributions are all in \((P)\) and we can show that these distributions form a sufficiently hard multiple hypothesis testing problem. The proof for the first term follows similarly.