# LogSpecT: Feasible Graph Learning Model from Stationary Signals with Recovery Guarantees

Shangyuan Liu

The Chinese University of Hong Kong

shangyuanliu@link.cuhk.edu.hk &Linglingzhi Zhu

The Chinese University of Hong Kong

llzzhu@se.cuhk.edu.hk &Anthony Man-Cho So

The Chinese University of Hong Kong

manchoso@se.cuhk.edu.hk

###### Abstract

Graph learning from signals is a core task in graph signal processing (GSP). A significant subclass of graph signals called the stationary graph signals that broadens the concept of stationarity of data defined on regular domains to signals on graphs is gaining increasing popularity in the GSP community . The most commonly used model to learn graphs from these stationary signals is _SpecT_, which forms the foundation for nearly all the subsequent, more advanced models. Despite its strengths, the practical formulation of the model, known as _rSpecT_, has been identified to be susceptible to the choice of hyperparameters. More critically, it may suffer from infeasibility as an optimization problem. In this paper, we introduce the first condition that ensures the infeasibility of rSpecT and design a novel model called _LogSpecT_, along with its practical formulation _rLogSpecT_ to overcome this issue. Contrary to rSpecT, our novel practical model rLogSpecT is always feasible. Furthermore, we provide recovery guarantees of rLogSpecT from modern optimization tools related to epi-convergence, which could be of independent interest and significant for various learning problems. To demonstrate the practical advantages of rLogSpecT, a highly efficient algorithm based on the linearized alternating direction method of multipliers (L-ADMM) that allows closed-form solutions for each subproblem is proposed with convergence guarantees. Extensive numerical results on both synthetic and real networks not only corroborate the stability of our proposed methods, but also highlight their comparable and even superior performance than existing models.

## 1 Introduction

Learning with graphs has proved its relevance in many practical areas, such as life science , signal processing , and financial engineering , to just name a few. However, there are many cases that the graphs are not readily prepared and only the data closely related to the graphs can be observed. Hence, a core task in Graph Signal Processing (GSP) is to learn the underlying graph topology based on the interplay between data and graphs .

Among these commonly used data properties, an assumption that is referred to as the graph signal stationarity  is gaining attention . Graph signal stationarity is the extension of the notion of stationarity from the regular space/time domain to the irregular domain (i.e. graphs). The latter is a traditional hypothesis in signal processing to capture a special type of statistical relationship between samples of a temporal signal. It is a cornerstone of many signal processing methods. Although the graph signal stationarity property is proposed more from a theoretic end, thetechniques based on this idealistic assumption pave the way for the development of the following more advanced and applicable models . Furthermore, several works have shown that some real datasets can be approximately viewed as stationary or partially explained by the stationarity assumption. For instance,  revealed that the well-known USPS dataset and the CMUPIE set of cropped faces exhibit near stationarity.  found that certain weather data could be explained by stationary graph signals. Additionally,  highlighted the relevance of the stationarity assumption in learning protein structure.  applied the stationary assumption to US Senate Roll Calls and achieved good performance.

The predominant methods to process stationary graph signals and learn topology under the stationarity assumption are the spectral-template-based models. The start of this line of works is , which proposed a vanilla model called _SpecT_ to learn graphs from stationary graph signals. Many extensions of this fundamental model have been made since then to accommodate the more realistic and complicated settings, e.g. joint inference of multiple graphs , learning with streaming graph signals , learning with hidden nodes , learning from both smooth and stationary graph signals . In practice, SpecT requires the unknown data covariance matrix. Hence, a robust formulation called _rSpecT_ is proposed , which replaces the unknown covariance matrix with its estimate and introduces a hyperparameter to reflect the estimation inaccuracy. Unfortunately, the model is sensitive to this hyperparameter and improper tuning of it may jeopardize the model performance or lead to model infeasibility.

The current approach to selecting an appropriate value is heuristic. It finds the smallest one that allows for a feasible solution. This method has two shortcomings. On one hand, It is computationally costly. The smallest value is the exact solution to a second-order conic programming, which is solved approximately with computational methods. Any approximate solution that is smaller than the exact one will make rSpecT infeasible. On the other hand, such an approach lacks theoretical guarantees as how the smallest value behaves as the number of observations increases is unknown. This issue makes the existing recovery guarantees invalid. To the best of our knowledge, only a few works have focused on how to understand the performance of the robust formulation rSpecT.  empirically showed that the feasible set of the robust formulation approximates the original one as more samples are collected.  provided the first recovery guarantee for rSpecT. Their work exploits the fact that the objective is the \(_{1}\)-norm and the constraints are linear . However, the conditions needed in their work are not only restrictive but also hard to check, as they require the full rankness of a large-scale matrix related to random samples. Another model  tries to circumvent the model infeasibility issue by turning a constraint into a penalty. However, this approach introduces another hyperparameter that is neither easy to tune nor amenable to theoretical analysis. Also, this hyperparameter should go to infinity heuristically when more and more samples are collected, which may exacerbate model instability.

In this paper, we propose an alternative formulation of SpecT that can bypass the aforementioned issues. We first provide a condition that ensures the infeasibility of the fundamental model rSpecT. Then, we propose a novel formulation called _LogSpecT_ and its practical formulation _rLogSpecT_ based on the spectral template and incorporating a log barrier. The inclusion of a log barrier as a regularizer on the degrees is inspired by its extensive application in graph learning from smooth graph signals  and the discussion in  emphasizing that restricting the degrees to preclude isolated nodes is crucial to learning graphs with special structures. It is worth noting that the approach in , which directly imposes constraints, may exacerbate the infeasibility issue in our problem. Although rLogSpecT still requires a parameter to reflect the estimation error of the unknown covariance matrix, it no longer suffers from the infeasibility issue associated with the improper choice of the parameter. Furthermore, we provide the decaying rate of the parameter that can guarantee the recovery of rLogSpecT. This not only aids in the parameter tuning, but also establishes a connection between rLogSpecT and LogSpecT through non-asymptotic recovery analysis. These theoretical results are derived using contemporary optimization tools related to epi-convergence . Unlike the current guarantees for rSpecT that are built on an \(_{1}\) analysis model , our approach based on epi-convergence is not confined to the specific types of optimization problems (e.g. the combination of the \(_{1}\)-norm and log barrier in the objective function) and consequently admits broader applications that can be of independent interest.

In the algorithmic aspect, we design a linearized alternating direction method of multipliers (L-ADMM) to solve rLogSpecT. The subproblems of L-ADMM admit closed-form solutions and can be implemented efficiently due to the linearization technique. Also, we provide the convergence result of the proposed method. Finally, we conduct extensive experiments on both synthetic data and real networks. Notably, the infeasibility issue of rSpecT frequently occurs in experiments on real networks, thereby emphasizing its practical significance beyond theoretical concerns. Furthermore, we observe that even when the classical models are set to be feasible, our novel models (LogSpecT and rLogSpecT) exhibit improved accuracy and stability, particularly in the case of synthetic BA graphs. This empirically illustrates that although our models are initially proposed to circumvent the infeasibility issue, they are superior alternatives to classical methods in the general cases.

### Notation

The notation we use in this paper is standard. We use \([m]\) to denote the set \(\{1,2,,m\}\) for any positive integer \(m\). Let the Euclidean space of all real matrices be equipped with the inner product \(,(^{})\) for any matrices \(,\) and denote the induced Frobenius norm by \(\|\|_{F}\) (or \(\|\|_{2}\) when the argument is a vector). For any \(^{m n}\), we use \((,)\) to denote the closed Euclidean ball centering at \(\) with radius \(\). Let \(\|\|\) be the operator norm, \(\|\|_{1,1}_{i,j}|_{ij}|\), \(\|\|_{,}_{i,j}|_{ij}|\), and let \(()\) be vector formed by the diagonal entries of \(\). For a column vector \(\), let \(()\) be the diagonal matrix whose diagonal elements are given by \(\). Given a closed and convex set \(\), we use \(_{}()_{}\|-\|_{2}\) to denote the projection of the point \(\) onto the set \(\). We use \(\) (resp. \(\)) to denote an all-one vector (resp. all-zero vector) whose dimension will be clear from the context. For a set \(\) and any real number \(\), let \(\{ x x\}\), \(\{\}\), and \(_{}()\) be the indicator function of \(\). For two non-empty and compact sets \(\) and \(\), the distance between them is defined as \((,)_{}_{ }\|-\|_{F}\)1.

## 2 Preliminaries

Let \(=(,)\) be a graph, where \(=[m]\) is the set of nodes and \(\) is the set of edges. Let \(\) be the weight matrix associated with the graph \(\), where \(_{ij}\) represents the weight of the edge between nodes \(i\) and \(j\). In this paper, we consider undirected graphs without self-loops. The set of adjacency matrices for such graphs is \(\{^{m m}=^{},~{}()=,~{}\}\). Suppose that the adjacency matrix \(\) admits the eigen-decomposition \(=^{}\), where \(\) is a diagonal matrix and \(\) is an orthogonal matrix. A graph filter is a linear operator \(h:^{m m}^{m m}\) defined as \(h()=_{i=0}^{p}h_{i}^{i}=(_{i=0}^{p}h_{i}^{i})^{}\), where \(p>0\) is the order of the graph filter and \(\{h_{i}\}_{i=0}^{p}\) are the filter coefficients. According to the convention, we have \(^{0}=\).

A graph signal can be represented by a vector \(^{m}\), where the \(i\)-th element \(x_{i}\) is the signal value associated with node \(i\). A signal \(\) is said to be stationary if it is generated from

\[=h(),\] (1)

where \(\) satisfies \([]=\) and \([^{}]=\). Simple calculations give that the covariance matrix of \(\), which is denoted by \(_{}\), shares the same eigenvectors with \(\). Hence, we have the constraint

\[_{}=_{}.\] (2)

Based on this, the following fundamental model SpecT is proposed to learn graphs from stationary signals without the knowledge of graph filters [31; 34]:

\[_{}~{}\|\|_{1,1},~{}~{}~{}~{}_{}= _{},~{}\{^{m  m}:_{j=1}^{m}_{1j}=1\},\] (SpecT)

where the constraint \(_{j=1}^{m}_{1j}=1\) is used to preclude the trivial optimal solution \(^{*}=\). When \(_{}\) is unknown and only \(n\) i.i.d samples of \(\{_{i}\}\) from (1) are available, the robust formulation, which is based on the estimate \(_{n}\) of \(_{}\) and called _rSpecT_, is used:

\[_{}~{}\|\|_{1,1},~{}~{}~{}~{}\|_{n}-_{n}\|_{F},~{}\{^{m  m}:_{j=1}^{m}_{1j}=1\}.\] (rSpecT)

For this robust formulation, the recovery guarantee is studied empirically in  and theoretically in  under some conditions that are restrictive and hard to check.

## 3 Infeasibility of rSpecT and Novel Models

Even though rSpecT has gained much popularity, there are few works that discuss the choice of the hyperparameter \(\) and how it affects the model feasibility. In this section, we present a condition under which rSpecT is always infeasible and then propose an alternative formulation. To motivate our results, let us consider the following 2-node example.

**Example 3.1**.: _Consider a simple graph containing 2 nodes. Then, the set given by the second constraint of rSpecT is \(\{^{m m}:_{j=1}^{m}_{1j}=1 \}=[0,1;1,0]\), which is a singleton. Suppose that the sample covariance matrix is \(_{n}=[h_{11},h_{12};h_{12},h_{22}]\). Then, the constraint \(\|_{n}-_{n}\|_{F}\) is reduced to \(2(h_{11}-h_{22})^{2}^{2}\). Hence, when \(h_{11} h_{22}\) and \(<|h_{11}-h_{22}|\), rSpecT has no feasible solution._

Before delving into the general case, we introduce a linear operator \(^{m^{2} m(m-1)/2}\) that maps the vector \(_{+}^{(m-1)m/2}\) to the vectorization of an adjacency matrix \(\) of a simple, undirected graph:

\[()_{m(i-1)+j}=x_{i-j+(2m-j)},&\;\;i>j,\\ 0,&\;\;i=j,\\ x_{j-i+(2m-i)},&\;\;i<j,\] (3)

where \(i,j[m]\). We also define \(_{n}_{n}-_{n}^{m^{2} m^{2}}\), so that the first constraint of rSpecT can be rewritten as \(\|_{n}-_{n}\|_{F}\|_{n} {vec}()\|_{2}\), where \(()\) is the vectorization operator. We now give a condition that guarantees the infeasibility of rSpecT.

**Theorem 3.2**.: _Consider the linear system_

\[_{n}=,\;\;,\;\;_{i=1}^{ m-1}y_{i},\] (4)

_where \(^{}\). If (4) has no feasible solution, then there exists a \(_{n}>0\) such that rSpecT is infeasible for all \(_{n}[0,_{n})\)._

**Remark 3.3**.: _From Theorem 3.2 we can infer that for any fixed \(n\), the linear system (4) has no solution when \(_{n}\) has full column rank (e.g., Example 3.1). This leads to the infeasibility of rSpecT with \(_{n}[0,_{n})\). As remarked in , one should tackle the feasibility issue of rSpecT with caution._

The failure of rSpecT (SpecT) lies in the existence of the constraint \(()_{1}=1\), which is used to preclude the trivial solution \(^{*}=\). For this reason, we resort to an alternative approach to bypassing the trivial solution. When the graphs are assumed to have no isolated node, the log barrier is commonly applied . In these cases, the zero solution is naturally precluded. This observation inspires us to propose the following novel formulation, which combines the log barrier with the spectral template (2) to learn graphs without isolated nodes from stationary signals:

\[&_{}\;\|\|_{1,1}-^{ }()\\ &\;\;\;,\;\;_{}= _{},\] (LogSpecT)

where \(\|\|_{1,1}\) is a convex relaxation of \(\|\|_{0}\) promoting graph sparsity, \(^{}()\) is the penalty to guarantee the nonexistence of isolated nodes, and \(\) is the tuning parameter. As can be seen from the following proposition, the hyperparameter \(\) in LogSpecT only affects the scale of edge weights instead of the graph connectivity structure.

**Proposition 3.4**.: _Let \((,)\) be the optimal solution set of LogSpecT with input covariance matrix \(\) and parameter \(>0\). Then, for any \(>0\), it follows that_

\[(,)=(,/ )=(,1).\]

**Remark 3.5**.: _The result of Proposition 3.4 spares us from tuning the hyperparameter \(\) when we are coping with binary graphs. In fact, certain normalization will eliminate the impact of different values of \(\) and preserve the connectivity information. Hence, we may simply set \(=1\) in implementation._

Note that the true covariance matrix \(_{}\) in LogSpecT is usually unknown and an estimate \(_{n}\) from \(n\) i.i.d samples is available. To tackle the estimation inaccuracy, we introduce the following robustformulation:

\[_{}&\|\|_{1,1}-^ {}()\\ &,\ \ \|_{n}-_{ n}\|_{F}^{2}_{n}^{2}.\] (rLogSpecT)

This formulation substitutes \(_{}\) with \(_{n}\) and relaxes the equality constraint to an inequality constraint with a tolerance threshold \(_{n}\). Contrary to rSpecT, we prove that rLogSpecT is always feasible.

**Proposition 3.6**.: _For \(_{n}>0\) and \(_{n}\) with any fixed \(n\), rLogSpecT always has a nontrivial feasible solution._

## 4 Recovery Guarantee of rLogSpecT

In this section, we investigate the non-asymptotic behavior of rLogSpecT when more and more i.i.d samples are collected. For the sake of brevity, we denote \(f\|\|_{1,1}-^{}()\). The theoretical recovery guarantee is as follows:

**Theorem 4.1**.: _If \(_{n} 2\|_{n}-_{}\|\) with given \(\{f^{*}, m, m(-1)\}\), then there exist constants \(c_{1},_{1},_{2}>0\) such that_

\[\ |f_{n}^{*}-f^{*}|_{n}\ (^{n,*},_{2 _{n}}^{*})_{n}\ (^{n,*},_{0}^{*}) _{1}_{n}+_{2}}\]

_where \(_{n} c_{1}(_{n}+2\|_{n}-_{}\|)\), \(^{n,*}\) is the optimal solution set of rLogSpecT, \(f^{*}\) (resp. \(f_{n}^{*}\)) denotes the optimal value of LogSpecT (resp. rLogSpecT), and_

\[_{}^{*}\{_{ }=_{},\ f() f^{*}+\}\]

_is the \(\)-suboptimal solution set of LogSpecT._

**Remark 4.2**.: (i) _Compared with the conclusion_ (ii) _in the Theorem 4.1, the conclusion_ (iii) _links the optimal solution sets of rLogSpecT and LogSpecT instead of the sub-optimal solutions._

(ii) _A byproduct of the proof (see Appendix E.2 for details) shows that the optimal node degree vector set (\(_{0}^{*}\)) is a singleton. However, there is no guarantee of the uniqueness of \(^{*}\) itself. We will discuss the impact of such non-uniqueness on model performance in Section 6.4._

**Remark 4.3**.: _The proof of Theorem 4.1 relies on two important optimization concepts: Epi-convergence and truncated Hausdorff distance. Epi-convergence is closely related to the asymptotic solution behavior of approximate minimization problems, and the truncated Hausdorff distance is used to characterize the epi-convergence non-asymptotically. With the help of the Kemmochi condition, which allows us to explicitly calculate the truncated Hausdorff distance, we are able to study the non-asymptotic behavior of the optimal value and optimal solutions of the models. We refer the reader to Appendix E and also [28, Chapter 7], [29, Chapter 6.J] for more details._

**Corollary 4.4**.: _Under the assumptions in Theorem 4.1, it follows that if \(_{n} 0\), then_

\[_{n}(^{n,*},_{0}^{*})=0.\] (5)

**Remark 4.5**.: _From the strong law of large numbers, if we choose \(_{n}=(\|_{n}-_{}\|)\), then almost surely one has \(_{n}\|_{n}-_{}\|=0\) and consequently \(_{n} 0\). Hence, Corollary 4.4 shows that (5) holds almost surely._

For the remaining part of this section, we study the choice of \(_{n}\) under certain statistical assumptions. A large number of distributions (e.g., Gaussian distributions, exponential distributions and any bounded distributions) can be covered by the sub-Gaussian distribution (cf. [41, Section 2.5]), whose formal definition is as follows.

**Definition 4.6** (Sub-Gaussian Distributions).: _The probability distribution of a random vector \(\) is called sub-Gaussian if there are \(C,v>0\) such that for every \(t>0\), \((\|\|_{2}>t) Ce^{-vt^{2}}\)._

Consider the case that \(\) in the generative model (1) follows a sub-Gaussian distribution. The following result is adapted from [40, Proposition 2.1].

**Lemma 4.7**.: _Suppose that \(\) in the generative model (1) follows a sub-Gaussian distribution. Then, \(\) follows a sub-Gaussian distribution, and with probability larger than \(1-()\), one has \(\|_{n}-_{}\|()\)._Equipped with non-asymptotic results in Theorem 4.1, we choose \(_{n}\) with a specific decaying rate.

**Corollary 4.8**.: _If the input \(\) in (1) follows a sub-Gaussian distribution and \(_{n}=()\), then the assumptions in Theorem 4.1 hold with probability larger than \(1-()\)._

Corollary 4.8 together with Theorem 4.1 illustrates the non-asymptotic convergence of optimal function value/suboptimal solution set/optimal node degree vector of rLogSpect from \(n\) i.i.d samples to the ideal model LogSpect with the convergence rate \((_{n})/(_{n})/( })\), where \(_{n}=(|_{n}-_{}|)( )\) for sub-Gaussian \(\). This matches the convergence rate \(()\) of classic spectral template models, e.g., Proposition 2 in  and Theorem 2 in  for the SpecT model, which shows that LogSpecT is also competitive on recovery rate.

## 5 Linearized ADMM for Solving rLogSpecT

In this section, we design a linearized ADMM algorithm to solve rLogSpecT that admits closed-form solutions for the subproblems. The ADMM-type algorithms have been successfully applied to tackle various graph learning tasks [46; 42].

Firstly, we reformulate rLogSpecT such that it fits the ADMM scheme:

\[_{,,} ,^{}-^{} +_{}()+_{(,_{n})}( {Z})\] (6) s.t. \[_{n}-_{n}=,\ \ =.\]

The augmented Lagrangian function of problem (6) is

\[L(,,,,_{2}) =^{},-^{} +_{}()+_{(,_{n})}( {Z})+,_{n}-_{n}-\] \[+_{2}^{}(-)+ \|_{n}-_{n}-\|_{F}^{2}+ {2}\|-\|_{2}^{2}.\]

Update of primal variables \(\) and \(\)Since \(\) and \(\) are separable given \(\), we update the primal variables in two blocks: \(\) and \((,)\). More specifically, the update of \((,)\) in the \(k\)-th iteration is

\[(^{(k+1)},^{(k+1)})_{(, )}L(^{(k)},,,^{(k)},_{2}^{(k)}),\] (7)

which admits the following closed-form solution:

**Proposition 5.1**.: _The update (7) can be explicitly rewritten as_

\[^{(k+1)}=\{1,}{\|}\|_{F}}\} },^{(k+1)}=}+}^{ 2}+4/}}{2},\]

_where \(}=_{n}^{(k)}-^{(k)}_{n}+^{(k)}}{}\), \(}=^{(k)}-_{2}^{(k)}\)._

Update of primal variables \(\)The ADMM update of \(\) does not admit a closed-form solution. Hence, we solve the linearized version \(_{k}\) of the augmented Lagrangian function \(L\) when all the variables except \(\) are fixed. It is given by

\[_{k}()=^{(k)},+\|-^{(k)}\|_{F}^{2}+_{}(),\]

where \(^{(k)}^{}+_{n}^{(k)}-^{(k)}_{n}-_{2}^{(k)}^{}+(^{(k)} _{n}^{2}+_{n}^{2}^{(k)}-2_{n}^{(k)}_{n}+ ^{(k+1)}_{n}-_{n}^{(k+1)}-^{(k+1)}^{}+ ^{(k)}^{})\) and \(\) is the parameter for linearization. Therefore, the update of \(\) in the \(k\)-th iteration is

\[^{(k+1)}_{}_{k}( )=_{}(^{(k)}-^{(k)}/),\] (8)

where \(_{}()\) is the projection of \(\) onto the closed and convex set \(\) and can be computed by

\[(_{}())_{ij}=\{0,X_{ij}+X_{ji}\},&\ \,i j,\\ 0,&\ \,i=j.\]Update of dual variables and parametersFinally, we update the dual variables \(\) and \(_{2}\) via gradient ascent steps of the augmented Lagrangian function:

\[^{(k+1)} ^{(k)}+(_{n}^{(k+1)}-^{( k+1)}_{n}-^{(k+1)}),\] (9) \[_{2}^{(k+1)} _{2}^{(k)}+(^{(k+1)}-^{(k+1) }).\] (10)

Together with the updating rule for the augmented parameter \(\), we complete the whole algorithm. The details are presented in Appendix F.

Complexity of each iterationTo illustrate the efficiency of L-ADMM, we calculate the complexity of each subproblem. In the update of \(\) and \(\), the most time consuming part is to calculate \(_{n}^{(k)}\). When the number of nodes \(m\) is larger than the sample size \(n\), we rewrite \(_{n}^{(k)}\) as \((1/n)(^{}^{(k)})\), where \(^{m n}\) is the data matrix. In this case, the complexity is in the order of \((nm^{2})\). When \(m\) is smaller than \(n\), we first calculate \(C_{n}\) and then multiply it with \(^{(k)}\). This gives the complexity \((nm^{2})\). Similarly, the complexity needed to update \(\) is \((nm^{2}+mn^{2})\). The update of the dual variables can be decomposed as the summation of components calculated when updating the primal variables. In summary, the total complexity of each iteration is \((nm^{2}+mn^{2})\).

Convergence of L-ADMMFor the convergence analysis of L-ADMM, we treat \((,)\) as one variable, and the two constraints \(_{n}-_{n}=\) and \(=\) can be written into a single one:

\[[_{n}-_{n};^{}] ()=[();].\]

Then we can apply a two-block proximal ADMM (i.e., L-ADMM) to the problem by alternatingly updating \(\) (see details in Appendix F.4 for this part) and \((,)\). Consequently, the convergence result in [45, Theorem 4.2] can be invoked directly to derive the following theorem.

**Theorem 5.2**.: _If \(>m+\|_{n}_{m}-_{m}_{n}\|^{2}\), then \(_{k}f(^{(k)})=f_{n}^{*}\)._

## 6 Experiments

In this section, we evaluate our proposed algorithm and models via numerical experiments. The experiments are conducted on both synthetic and real networks. We apply the standard metrics : F-measure, Precision, and Recall to assess the quality of learned graphs. The source code is available at: https://github.com/StevenSYL/NeurIPS2023-LogSpecT.

### Data Generation

Random Graphs.In the experiments, we consider two types of synthetic graphs, namely, the Erdos-Renyi (ER) graph  and the Barabasi-Albert model graph (BA) . The ER graphs are generated by placing an edge between each pair of nodes independently with probability \(p=0.2\) and the weight on each edge is set to 1. The BA graphs are generated by having two connected nodes initially and then adding new nodes one at a time, where each new node is connected to exactly one previous node that is randomly chosen with a probability proportional to its degree at the time.

Graph Filters and Signals.Three graph filters are used in the experiments. The first one is the low-pass graph filter (lowpass-EXP) \(h()=()\). The second one is the high-pass graph filter (highpass-EXP) \(h()=(-)\). The last one is a quadratic graph filter (QUA) \(h()=^{2}++\). Note that the quadratic graph filter is neither low-pass nor high-pass. We will also use random graph filters developed from the above ones for experiments on real networks. They will be introduced when applied. The stationary graph signals are generated from the generative model (1). The input graph signal \((,_{m})\) is a random vector following the normal distribution.

### How to Infer Binary Graphs

Many practical tasks require binary graphs instead of weighted graphs. However, the results from LogSpect and rLogSpecT are generally weighted. In this section, we tackle the issue of how to convert a weighted graph to a binary one. Firstly, we normalize each edge in the graph \(\) by dividing it over the maximal weight, which yields a collection of values ranging from \(0\) to \(1\). Our task is then to select a threshold \(\) to round these values to \(0\) or \(1\) and construct a binary graph \(^{*}\) accordingly. Mathematically, this procedure is captured by the formula:\[(W^{*})_{ij}=1,&\,W_{ij}/\{W_{i^{}j^{}} \},\\ 0,&\,W_{ij}/\{W_{i^{}j^{}}\}<.\]

To choose the threshold \(\), we either use a training-based strategy or a searching-based strategy. The training-based one searches the best \(\) on \(k\) graphs and applies the value to the newly learned graph. The searching-based one simply searches for \(\) that yields the best performance for a graph.

### Convergence of L-ADMM

We present the convergence performance of L-ADMM in this section. Since there does not literally exist customized algorithm for rLogSpecT, we compare our L-ADMM with CVXPY . The solver used in CVXPY is MOSEK. We conduct the experiments on 100-node BA graphs with 10000 stationary graph signals generated from low-pass EXP graph filter. The parameter \(\) is set as \(10\). For L-ADMM algorithm, the target accuracy is set as \(10^{-6}\) and the initialization is set as zero. The running time for L-ADMM takes around **20** seconds while the solver takes over **110** seconds. The primal residual and dual residual of L-ADMM in each iteration are plot in Figure 1. The results corroborate the efficiency of our proposed algorithm. Furthermore, the locally linear convergence can be observed from the experiment. This implies that L-ADMM converges with fast speed to high accuracy. It remains an open question to prove the property theoretically.

### Experiments on Synthetic Networks

To evaluate the efficacy of LogSpecT and rLogSpecT, we compare them with SpecT and rSpecT on synthetic data. Notice that the prior of no isolated nodes has been incorporated in practice by adding restrictions on the smallest degree. This approach introduces an additional hyperparameter and may exacerbate the infeasibility issue in robust formulation. Also, no recovery guarantees are developed for this trick. For these reasons, we still compare LogSpect \(\&\) rLogSpecT with SpecT \(\&\) rSpecT.

Figure 1: L-ADMM residual on 100-node graph Figure 2: rLogSpecT with \(_{n}=0.2\)

**Performance of LogSpecT**. We first compare the performance of the ideal models: LogSpecT and SpecT on the two types of random graphs with three graph filters. We conduct experiments on graphs with nodes ranging from 20 to 60 and use the searching-based strategy to set threshold \(^{*}\). After 10 repetitions, we report the average results in Figure 3. The left column presents the performance of LogSpecT and the right column presents that of SpecT. Firstly, we observe that on ER graphs, both models achieve good performance. They nearly recover the graphs perfectly. However, the performance on BA graphs differs. In this case, LogSpecT can work efficiently. The learned graphs from LogSpecT enjoy much higher F-measure values than those from SpecT. Secondly, the comparison between different graph filters shows that different graph filters have few impacts on recovery performance for both LogSpecT and SpecT. Finally, we observe that the outputs of LogSpecT on BA graphs tend to possess higher F-measure when the number of nodes increases. This suggests that LogSpecT may behave better on larger networks. However, such phenomena cannot be observed from SpecT results.

**Performance of rLogSpecT**. Since LogSpecT persuasively outperforms SpecT, we only present the performance of rLogSpecT when more samples are collected. As we have mentioned before, the optimal solution to LogSpecT is not necessarily unique. Thus, we do not expect to be able to show how rLogSpecT's optimal solution converges to LogSpecT's optimal solution. Moreover, the non-uniqueness may jeopardize the performance of rLogSpecT intuitively. However, the following experiment on BA graphs shows that the learned graphs from rLogSpecT with the binary-graph-transforming strategy tend to approach the ground truth when enough samples are collected.

In this experiment, the sample size \(n\) is chosen from \(10\) to \(10^{6}\) and \(_{n}\) is set as \(0.2\). We rely on the training-based strategy to obtain the best threshold \(^{*}\) from 10 randomly chosen training graphs. We then calculate the F-measure of the learned binary graph in the testing set. The result is reported in Figure 2. It shows that rLogSpecT works for all three graph filters on BA graphs and tends to possess higher F-measure values when more and more signals are collected. This indicates that similar to LogSpecT, rLogSpecT is efficient for different types of graph filters on BA graphs, including the high-pass ones. For more experiments and discussions, we refer readers to Appendix G.

### Experiments on Real Networks

In this set of experiments, we compare the performance of LogSpecT (resp. rLogSpecT) with SpecT (resp. rSpecT) and other methods from the statistics and GSP communities on _Protein_ database and _Reddit_ database from . The _Protein_ database is a Bioinformatics dataset, where nodes are secondary structure elements (SSEs) and there is an edge between two nodes if they are neighbors in the amino-acid sequence or in 3D space. The _Reddit_ database is a social network dataset. In this dataset, each graph corresponds to an online discussion thread, where nodes correspond to users, and there is an edge between two nodes if at least one of them responds to another's comments. We choose graphs in databases whose numbers of nodes are smaller than 50 and generate stationary graph signals from the generative model (1), with the graph filtering chosen as \(H()=\). Our experiments include in total 871 testing graphs in the _Protein_ database and 309 in _Reddit_ database.

**Infeasibility of rSpecT**. For these real networks, we first check whether the infeasibility issue encountered by rSpecT is significant. To this end, we adopt the random graph filters \(t_{1}^{2}+t_{2}+t_{3}\), where \(t_{i},i=1,2,3\) are random variables following a Gaussian distribution with \(=0\) and \(=2\). Then, we calculate the smallest \(_{n}\) such that rSpecT is feasible. If the smallest \(_{n}>0\), rSpecT is likely to be encountered with the infeasibility issue. The results with different numbers of graph signals observed are shown in Table 1.

The experiment results present a decrease in the mean value of \(_{}\) when more samples are used. However, a high frequency of the infeasibility issue occurring in these real datasets cannot be mitigated by the increasing sample size. These observations highlight the necessity of the careful treatment for choosing \(\) in rSpecT again.

   &  &  \\  sample size & 10 & 100 & 1000 & 10 & 100 & 1000 \\  frequency & 0.980 & 0.999 & 0.994 & 0.984 & 1 & 1 \\  mean of \(_{}\) & 38.514 & 26.012 & 14.813 & 1094.788 & 830.642 & 531.006 \\  

Table 1: Likelihood of infeasibility

**Performance of different graph learning models**. In this section, the stationary graph signals are generated by the low-pass-EXP filter. We choose the famous statistical method called (thresholded) correlation  and the first GSP method that applies the log barrier to graph inference  as the baselines. The optimal threshold for the correlation method is selected from \(0.1\) to \(0.6\) and we search in \(\{0.01,0.1,1,10,100,1000\}\) to obtain the best hyperparameter in Kalofolias' model. The parameter \(_{n}\) in rLogSpecT is set as \(10\) and in rSpecT it is set as the smallest value that allows a feasible solution . We also rely on the searching-based strategy to convert the learned weighted graphs from (r)SpecT & (r)LogSpecT.

The results of the ideal models with the true covariance matrix \(_{}\) applied are collected in Figure 4. We observe that on the real graphs, LogSpecT achieves the best performance on average (median represented by the red line). Also, compared with SpecT, LogSpecT performs more stably. We remark that since the graph signals are not necessarily smooth, Kalofolias' model cannot provide guaranteed performance, especially on the Reddit networks.

Figure 5 compares the performance of four different methods when different numbers of signals are observed.2 When the sample size increases, the models focusing on stationarity property can recover the graphs more accurately while correlation method and Kalofolias' method fail to exploit the signal information. This can also be inferred from the experiment results in Figure 4 since the models fail to achieve good performance from full information, let alone from partial information. The experiment also shows that when a fixed number of samples are observed, the learned graphs from rLogSpecT approximate the ground truth better than rSpecT. This further corroborates the superiority of rLogSpecT on graph learning from stationary signals.

## 7 Conclusion

In this paper, we directly tackle the infeasibility issue encountered in graph learning from stationary graph signals  and propose the first condition that guarantees the model infeasibility. To overcome this, we propose an efficient alternative. The recovery guarantees of its robust formulation are analyzed with advanced optimization tools, which may find broader applications in learning tasks. Compared with current literature , these theoretical results require less stringent conditions. We also design an L-ADMM algorithm that allows for efficient implementation and theoretical convergence. Extensive experiments on both synthetic and real data are conducted in this paper. The results show that our proposed model can significantly outperform the existing ones.

We believe this work represents an important step beyond the fundamental model SpecT. Its general formulation allows for the transfer of the current extensions made on SpecT. Testing our proposed models with these extensions is one future direction. Also, we notice that although the recovery guarantees for robust formulations are clear, the estimation performance analysis for the ideal-case models (i.e. SpecT and LogSpecT) is still incomplete. Investigating the exact recovery conditions is another future direction.

[MISSING_PAGE_FAIL:11]

*  Eric D. Kolaczyk. _Statistical Analysis of Network Data: Methods and Models_. Springer-Verlag, New York, NY, USA, 2009.
*  D. Christopher Manning, Hinrich Schutze, and Prabhakar Raghavan. _Introduction to Information Retrieval_. Cambridge University Press, 2008.
*  Hermina Petric Maretic and Pascal Frossard. Graph laplacian mixture model. _IEEE Transactions on Signal and Information Processing over Networks_, 6:261-270, 2020.
*  Antonio G Marques, Santiago Segarra, Geert Leus, and Alejandro Ribeiro. Stationary graph processes and spectral estimation. _IEEE Transactions on Signal Processing_, 65(22):5911-5926, 2017.
*  Gautier Marti, Frank Nielsen, Mikolaj Binkowski, and Philippe Donnat. A review of two decades of correlations, hierarchies, networks and clustering in financial markets. _Progress in Information Geometry_, pages 245-274, 2021.
*  Gonzalo Mateos, Santiago Segarra, Antonio G Marques, and Alejandro Ribeiro. Connecting the dots: Identifying network structure via graph signal processing. _IEEE Signal Processing Magazine_, 36(3):16-43, 2019.
*  Madeline Navarro, Yuhao Wang, Antonio G Marques, Caroline Uhler, and Santiago Segarra. Joint inference of multiple graphs from matrix polynomials. _Journal of Machine Learning Research_, 23(76):1-35, 2022.
*  Bastien Pasdeloup, Vincent Gripon, Gregoire Mercier, Dominique Pastor, and Michael G Rabbat. Characterization and inference of graph diffusion processes from observations of stationary signals. _IEEE transactions on Signal and Information Processing over Networks_, 4(3):481-496, 2017.
*  Nathanael Perraudin and Pierre Vandergheynst. Stationary signal processing on graphs. _IEEE Transactions on Signal Processing_, 65(13):3462-3477, 2017.
*  Raksha Ramakrishna, Hoi-To Wai, and Anna Scaglione. A user guide to low-pass graph signal processing and its applications: Tools and applications. _IEEE Signal Processing Magazine_, 37(6):74-85, 2020.
*  Samuel Rey, Andrei Buciulea, Madeline Navarro, Santiago Segarra, and Antonio G Marques. Joint inference of multiple graphs with hidden variables from stationary graph signals. In _Proceedings of 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2022)_, pages 5817-5821. IEEE, 2022.
*  R Tyrrell Rockafellar and Roger J-B Wets. _Variational analysis_, volume 317. Springer Science & Business Media, 2009.
*  Johannes O Royset and Roger JB Wets. _An Optimization Primer_. Springer, 2021.
*  Santiago Segarra, Sundeep Prabhakar Chepuri, Antonio G Marques, and Geert Leus. Statistical graph signal processing: Stationarity and spectral estimation. _Cooperative and Graph Signal Processing_, pages 325-347, 2018.
*  Santiago Segarra, Antonio G Marques, Gonzalo Mateos, and Alejandro Ribeiro. Network topology inference from spectral templates. _IEEE Transactions on Signal and Information Processing over Networks_, 3(3):467-483, 2017.
*  Santiago Segarra, Yuhao Wang, Caroline Uhler, and Antonio G Marques. Joint inference of networks from stationary graph signals. In _Proceedings of the 51st Asilomar Conference on Signals, Systems, and Computers_, pages 975-979. IEEE, 2017.
*  Rasoul Shafipour, Abolfazl Hashemi, Gonzalo Mateos, and Haris Vikalo. Online topology inference from streaming stationary graph signals. In _2019 IEEE Data Science Workshop (DSW)_, pages 140-144. IEEE, 2019.
*  Rasoul Shafipour and Gonzalo Mateos. Online topology inference from streaming stationary graph signals with partial connectivity information. _Algorithms_, 13(9):228, 2020.

*  Rasoul Shafipour, Santiago Segarra, Antonio G Marques, and Gonzalo Mateos. Network topology inference from non-stationary graph signals. In _2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 5870-5874. IEEE, 2017.
*  Stephen M Smith, Karla L Miller, Gholamreza Salimi-Khorshidi, Matthew Webster, Christian F Beckmann, Thomas E Nichols, Joseph D Ramsey, and Mark W Woolrich. Network modelling methods for fmri. _Neuroimage_, 54(2):875-891, 2011.
*  Oliver Stegle, Sarah A Teichmann, and John C Marioni. Computational and analytical challenges in single-cell transcriptomics. _Nature Reviews Genetics_, 16(3):133-145, 2015.
*  Yuichi Tanaka and Yonina C Eldar. Generalized sampling on graphs with subspace and smoothness priors. _IEEE Transactions on Signal Processing_, 68:2272-2286, 2020.
*  Yuichi Tanaka, Yonina C Eldar, Antonio Ortega, and Gene Cheung. Sampling signals on graphs: From theory to applications. _IEEE Signal Processing Magazine_, 37(6):14-30, 2020.
*  Roman Vershynin. How close is the sample covariance matrix to the actual covariance matrix? _Journal of Theoretical Probability_, 25(3):655-686, 2012.
*  Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge University Press, 2018.
*  Xiaolu Wang, Chaorui Yao, Haoyu Lei, and Anthony Man-Cho So. An efficient alternating direction method for graph learning from smooth signals. In _Proceedings of 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2021)_, pages 5380-5384. IEEE, 2021.
*  Chenyue Zhang, Yiran He, and Hoi-To Wai. Product graph learning from multi-attribute graph signals with inter-layer coupling. In _Proceedings of 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023)_, pages 1-5. IEEE, 2023.
*  Hui Zhang, Ming Yan, and Wotao Yin. One condition for solution uniqueness and robustness of both l1-synthesis and l1-analysis minimizations. _Advances in Computational Mathematics_, 42(6):1381-1399, 2016.
*  Xiaoqun Zhang, Martin Burger, and Stanley Osher. A unified primal-dual algorithm framework based on bregman iteration. _Journal of Scientific Computing_, 46(1):20-46, 2011.
*  Licheng Zhao, Yiwei Wang, Sandeep Kumar, and Daniel P Palomar. Optimization algorithms for graph laplacian estimation via admm and mm. _IEEE Transactions on Signal Processing_, 67(16):4231-4244, 2019.
*  Shuai Zheng, Zhenfeng Zhu, Zhizhe Liu, Zhenyu Guo, Yang Liu, Yuchen Yang, and Yao Zhao. Multi-modal graph learning for disease prediction. _IEEE Transactions on Medical Imaging_, 41(9):2207-2216, 2022.

Organization of the Appendix

The appendix includes the missing proofs, detailed discussions of some argument in the main body and more numerical experiments. We organize the appendix as follows:

* The proof of infeasibility condition (Theorem 3.2) is provided in Section B.
* Explanations on conditions derived in Theorem 3.2 are included in Section C.
* The proof of properties of the proposed model (r)LogSpecT (Proposition 3.4\(\&\) 3.6) is given in Section D and some additional properties are discussed.
* The truncated Hausdorff distance based proof details of Theorem 4.1 and Corollary 4.4 are given in Section E.
* Details of L-ADMM and its convergence analysis are in Section F.
* Additional experiments and discussions on synthetic and real data are included in Section G.

## Appendix B Proof of Theorem 3.2

Since the linear system (4) has no solution, we know from Farkas' lemma that the following system has solutions:

\[_{m-1}&_{} ^{}_{n}^{}<_{(m-1) 1},\\ _{(m-1)}&_{}^{}_{n}^{}_{ 1}.\] (11)

Let \(^{*}^{m^{2}}\) be a solution to (11). Denote \(_{+}\{^{*},\}\), \(_{-}\{-^{*},\}\). Then, there exists \(c(0,1]\) such that

\[^{}_{n}^{}(_{+}-_{-})+c_{m^{2}}^{}( _{+}+_{-})[_{m-1};_{}].\]

Define \(y-_{m^{2}}^{}(_{+}+_{-})\), \(z c_{m^{2}}^{}(_{+}+_{-})\) and set \(=c\). For all \([0,)\), \((_{+},_{-},y,z)\) is a solution to the following linear system:

\[^{}_{n}^{}(_{+}-_{-})+z[_{ m-1};_{}],\\ _{m^{2}}^{}(_{+}+_{-})+y 0,\\  y+z>0,\\ _{+},_{-},-y.\]

Again, from Farkas' lemma, this implies that the following linear system does not have a solution:

\[ _{n}+t_{m^{2}}& ,\\ _{n}-t_{m^{2}}&,\\ t&,\\ _{m-1}&_{}= 1,\] (12)

where \(^{m(m-1)/2}\) and \(t\). Since (12) is equivalent to:

\[\|_{n}-_{n}\|_{,}, \\ ()_{1}=1,\\ ,\] (13)

the above argument indicates that (13) does not have a solution. Suppose rSpecT has a feasible solution \(^{}\), then

\[\|_{n}^{}-^{}_{n}\|_{,}\| _{n}^{}-^{}_{n}\|_{F}.\]

Hence, \(^{}\) is also a solution to (13). However, (13) does not have a solution. We can conclude that rSpecT is infeasible in this case.

Explanations on Sufficient Conditions in Theorem 3.2

We elaborate more on the infeasibility condition that \(_{n}\) has full column rank. An application of the condition is Example 3.1. Specifically, we know that in this case,

\[=0\\ 1\\ 1\\ 0_{n}=0&h_{12}&-h_{12}&0 \\ h_{12}&h_{22}-h_{11}&0&-h_{12}\\ -h_{12}&0&h_{11}-h_{22}&h_{12}\\ 0&-h_{12}&h_{12}&0.\]

This implies that

\[_{n}=0\\ h_{22}-h_{11}\\ h_{11}-h_{22}\\ 0.\]

Hence, when \(h_{11} h_{22}\), \(_{n}\) has full column rank. This means that when \(\) is small enough (from Example 3.1 we know \(=|h_{11}-h_{22}|\)), the model rSpecT is infeasible.

## Appendix D Proofs of Properties of (r)LogSpecT

### Proof of Proposition 3.4

Since the constraint set \(\) is a cone, it follows that for all \(>0\), \(=\). Then, we know that

\[(,) =*{argmin}_{,= }\|\|_{1,1}-^{}()\] \[=*{argmin}_{, =}\|\|_{1,1}-^{}()\] \[=*{argmin}_{ ,=}\|\|_{1,1}-^{ }()\] \[=*{argmin}_{, =}\|\|_{1,1}-^{}( )\] \[=(,/),\]

where the third equality is from the basic calculus rule of the logarithm function. Set \(=\) and then \((,)=(,1)\), which completes the proof.

### Proof of Proposition 3.6

The proof will be conducted by constructing a feasible solution for rLogSpecT. Recall that \(_{n}=_{n}-_{n}\) and the matrix \(^{m^{2} m(m-1)/2}\) that maps a non-negative vector to the vectorization of a valid adjacency matrix. Let \(=\{_{n}\|_{2}},1\}( )\) with \(^{(m-1)m/2}\) being a non-negative vector, where \(()\) is the matricization operator. Note that

\[(_{n}-_{n})=(_{n}- _{n})\,()=_{n}().\]

Then, we know that

\[\|_{n}-_{n}\|_{F}=\|(_{n}-_{n})\|_{2}=\{_{n}\|_{2}},1 \}\|_{n}\|_{2}.\]

Thus, the given \(\) is a feasible solution for rLogSpecT and it completes the proof.

### Properties of optimal solutions and values of (r)LogSpecT

In this section, we further discuss some properties of the optimal solutions/value of the proposed models, which are useful for deriving the recovery guarantee. More specifically, we obtain an upper bound on the optimal solutions (which may not be unique) independent of the sample size \(n\) and the inaccuracy parameter \(_{n}\). Also, a lower bound of optimal values follows.

**Proposition D.1**.: _The following statements hold:_

* _For an optimal solution_ \(^{*}\) _(resp._ \(^{*}_{n}\)_) to LogSpecT (resp. rLogSpecT with any given sample size_ \(n\)_), it follows that_ \[\|^{*}\|_{1,1}= m\;\;\;\;\|^{*}_{n}\|_{1,1}  m,\;\;_{n}>0.\]
* _If_ \(_{n} 2 m\|_{n}-_{}\|\)_, then_ \[ m(1-) f^{*}_{n} f^{*},\;\; n,\] _where_ \(f^{*}\) _(resp._ \(f^{*}_{n}\)_) denotes the optimal value of LogSpecT (resp. rLogSpecT)._

For the first statement, let us consider the Karush-Kuhn-Tucker (KKT) conditions of LogSpecT and rLogSpecT. Since the LogSpecT is a convex problem and Slater's condition holds, the KKT conditions are necessary and sufficient for the optimality, i.e., there exists \((_{1},_{2})^{m m}_{}(^{*})\) such that

\[_{}(\|^{*}\|_{1,1}-^{}( {S}^{*}))+_{}_{1}-_{1}_{ }+_{2}=,\\ _{}^{*}=^{*}_{},\\ ^{*},\] (14)

where \(_{}(^{*})\{^{m m }:_{}<-^{*},> 0\}\) is the normal cone of \(\) at \(^{*}\), and \(\|^{*}\|_{1,1}\) is well-defined since \(\|\|_{1,1}=,^{}\) at \(^{*} 0\), which is differentiable. Taking further calculation gives that

\[\|^{*}\|_{1,1}=^{},(_{}^{ }(^{*}))_{ij}=^{*})_{i}}.\]

Combining this with (14) by taking inner product of both sides with \(^{*}\), we obtain that

\[_{i,j}(^{*})_{ij}-_{i,j}^{*})_{ij}}{(^ {*})_{i}}+_{1},_{}^{*}-^{*} _{}+_{2},^{*}=0.\] (15)

From the structure of \(\) and the fact that \(_{2}_{}(^{*})\), one has that \(_{2},^{*}=0\). Also, note that \(_{}^{*}=^{*}_{}\). Hence, the equation (15) can be simplified as the desired result:

\[\|^{*}\|_{1,1}=_{i,j}(^{*})_{ij}=_{i,j} ^{*})_{ij}}{(^{*})_{i}}=_{i=1}^{m}_{j=1}^{m}^{*})_{ij}}{(^{*})_{i}}= m.\]

The KKT conditions of rLogSpecT indicate that there exist \(_{1} 0\), \(_{2}_{}(^{*}_{n})\) and \(\|_{n}^{*}_{n}-^{*}_{n}_{n}\|_{F}\) (i.e., the subgradient of the function \(\|_{n}-_{n}\|_{F}\) at \(^{*}_{n}\)) such that

\[_{}(\|^{*}_{n}\|_{1,1}-^{}( ^{*}_{n}))+_{1}+_{2}=,\\ _{1}(\|_{n}^{*}_{n}-^{*}_{n}_{n}\|_{F}-_{ n})=0,\\ ^{*}_{n}.\] (16)

Moreover, from the definition of the convex subdifferential we know that \(0\|_{n}^{*}_{n}-^{*}_{n}_{n}\|_{F}-, ^{*}_{n}\). Thus, after taking inner product of both sides of the equation (16) with \(^{*}_{n}\), it follows that:

\[0 =_{i,j}(^{*}_{n})_{ij}- m+_{1},^{*}_{n}+_{2},^{*}_{n}\] \[_{i,j}(^{*}_{n})_{ij}- m+_{1}\| _{n}^{*}_{n}-^{*}_{n}_{n}\|_{F}+_{2},^ {*}_{n}\] \[=_{i,j}(^{*}_{n})_{ij}- m+_{1}_{n},\]

which implies that \(_{i,j}(^{*}_{n})_{ij} m-_{1}_{n} m\). This completes the proof of the first statement.

For the second statement, we first prove that \(v_{n}^{*}\) and \(v^{*}\) are larger than \( m(1-)\). Define the auxiliary function \(g:\) such that \(g(x) x- x\) for any \(x_{+}\), whose minimum is attained at \(\). Since for any \(\),

\[f()=_{i=1}^{m}g(_{j=1}^{m}S_{ij}),\]

where \(f\) is the objective in LogSpecT, it follows that

\[f()_{i=1}^{m}g()= m(1-).\]

This implies that \(v_{n}^{*}\) and \(v^{*}\) are larger than \( m(1-)\). Next, we will show \(v_{n}^{*} v^{*}\). Consider any optimal solution \(^{*}\) to LogSpecT. We show that it is feasible for rLogSpecT.

\[\|_{n}^{*}-^{*}_{n}\| =\|_{n}^{*}-_{}^{*}+^{*} _{}-^{*}_{n}\|\] \[ 2\|^{*}\|_{1,1}\|_{n}-_{}\| 2  m\|_{n}-_{}\|_{n},\]

where the equality comes from \(_{}^{*}=^{*}_{}\), the first inequality comes from the fact that \(\|\|\|\|_{F}\|\|\|\|_{1,1}\|\|\), the second one comes from the first statement and the last one is due to \(_{n} 2 m\|_{n}-_{}\|\). Hence, \(^{*}\) is feasible for rLogSpecT, which indicates that \(v_{n}^{*} v^{*}\). The proof is completed.

## Appendix E Proof of Theorem 4.1 & Corollary 4.4

### Truncated Hausdorff distance

In this section, we introduce an advanced technique in optimization that is efficient in analyzing the recovery guarantee of robust formulations. Before that, we introduce the concept of truncated Hausdorff distance between two sets.

**Definition E.1** (Truncated Hausdorff Distance [29, 6.J]).: _For any \( 0\), the truncated Hausdorff distance between two sets \(\) and \(\) is defined as:_

\[}_{}(,)=\{((,),),( (,),)\}.\]

It turns out that the distance between the optimum of two minimization problems can be bounded with the truncated Hausdorff distance of the epigraphs under some conditions. The result is captured in the following lemma.

**Lemma E.2** ([29, Theorem 6.56]).: _Let \([0,)\). Suppose that the extended-real-valued lower semicontinuous functions \(f,g:^{n}}\) satisfy_

* \( f, g[-,]\)_,_
* \(*{argmin}f,*{argmin}g(,)\)_._

_Then, it follows that_

\[| f- g|}_{}(*{epi}f,* {epi}g).@note{footnote}{For a function $f:^{n}}$, its epigraph is defined as $*{epi}f:=\{(,y) y f()\}$.}\] (17)

_Suppose further that \( 2\,}_{}(*{epi}f,* {epi}g)\), then one has_

\[(_{g}^{*},*{argmin}f)}_{}(*{epi}f,*{epi}g),\] (18)

_where \(*{argmin}f\) is the \(\)-suboptimal solution set of \(f\) that is defined as \(*{argmin}f\{\,f:f() f+\}\), and \(_{g}^{*}\) is a minimizer of \(g\)._

From the above lemma, we know that if two optimization problems are close enough (in the sense of truncated Hausdorff distance), then the optimum of them should be close to each other. Hence, in order to apply this result, we need to bound the truncated Hausdorff distance in an explicit way, which is solved by the following Kenmochi condition.

**Lemma E.3** (Kenmochi Condition [29, Proposition 6.58]).: _Let \([0,)\). Then, for \(f,g:^{n}\) with nonempty epigraphs, one has that_

\[}_{}(\,f,\,g)=\{>0: _{(,)}g\{f(),-\}+,\;[f](,)\},\]

_where \([f]\{^{n}:f()\}\)._

### Proof of Theorem 4.1

Before presenting the proof, we first introduce the following lemma.

**Lemma E.4** (Hoffman's Error Bound ).: _Consider the set \(\{^{n}:\}\). There exists \(C>0\) such that for any \(^{n}\), one has_

\[(,) C\|(-)_{+}\|_{ 2}.\]

For the sake of brevity, we denote

\[_{n}() \|\|_{1,1}-^{}()+ _{_{-}}(\|_{n}-_{n}\|_{F}-_{n})+ _{}(),\] \[() \|\|_{1,1}-^{}()+ _{\{0\}}(\|_{}-_{}\|_{F})+_{ }().\]

Hence, the optimization problem LogSpecT (resp. rLogSpecT) is equivalent to \(\) (resp. \(_{n}\)).

Now, we aim to use Lemma E.3 to bound \(}_{}(\,,\,_{n})\). Let \((,)\) satisfy

\[()_{}=_{ }.\]

Then, we know that

\[\|_{n}-_{n}\|_{F} 2\|\|_{F}\|_{n}- _{}\| 2\|_{n}-_{}\|_{n},\]

and consequently \(\) is in the domain of \(_{n}\). Then, it follows that for any \(>0\), we have

\[_{(,)}_{n}_{n}()=()\{(),-\},[] (,).\] (19)

Before verifying the reverse side of the Kenmochi condition, we first consider the non-emptiness of \([_{n}](,)\). Since

\[_{n} 2\|_{n}-_{}\| 2 m\|_{n}- _{}\|,\]

it follows from Proposition D.1 that \(\|_{n}^{*}\|_{1,1} m\) and \(f_{n}^{*} f^{*}\), which implies that \([_{n}](,)\) is nonempty. Let \(_{n}[_{n}](,)\). Then, one has that

\[_{n}\ \ \ \ \ \|_{n}_{n}-_{n}_{n }\|_{F}_{n}.\]

Hence, it follows that

\[\|_{}_{n}-_{n}_{}\| 2\|_{n}\|_{F}\| _{}-_{n}\|+\|_{n}_{n}-_{n}_{n}\|_{F}  2\|_{}-_{n}\|+_{n}.\]

Also, note that there exists \(>0\) such that \((_{n})_{i}\) for all \(i[m]\) as \(_{n}\) and \(\|_{n}\|_{1,1}-^{}(_{n})\) when \(_{n}\). Thus, applying Lemma E.4 to the linear system

\[}\{^{m m}:_{ }=_{},\;,\;()_{i} ,\; i[m]\}\]

yields that there exists \(>0\) such that

\[(_{n},})(2\|_ {}-_{n}\|+_{n}).\]

Hence, there exists \(}\) in the domain of \(\) such that

\[\|_{n}-}\|_{F}(2\|_{}-_{n}\|+_{n})\ \ \ \ \ (})_{i},\  i[m].\]Since the function \(\|\|_{1,1}-^{}()\) is locally Lipschitz continuous when \(()_{i}\), there exists \(L>0\) such that

\[(})=\|}\|_{1,1}-^{ }(}) \|_{n}\|_{1,1}-^{}(_{n}) +L\|_{n}-}\|_{F}\] \[=f_{n}(_{n})+L\|_{n}-}\|_{F}\] \[_{n}(_{n})+L(2\|_{} -_{n}\|+_{n}).\]

Setting \(c_{1}\{1,L\}\), one can obtain that for any \(_{n}[_{n}](,)\)

\[_{(_{n},)}(})_{n}(_{n})+c_{1}(2\|_{}-_{n}\|+_{n}) \{_{n}(_{n}),-\}+,\] (20)

where \( c_{1}(2\|_{}-_{n}\|+_{n})\). Combining inequality (19) and (20), we can conclude that

\[}_{}(,_{n} ) c_{1}(2\|_{}-_{n}\|+_{n}).\] (21)

In order to derive the conclusion (i) and (ii), it remains to check the requirements in Lemma E.2. Since \( m\), the first statement of Proposition D.1 shows that the optimal solutions to \(\) and \(_{n}\) lie in \((,)\). Since \( f^{*}\) and \(- m(1-)\), the second statement of the proposition shows that \(,_{n}[-,]\). Hence, applying Lemma E.2 completes the proof of the first two statements.

To prove conclusion (iii), we first make the following two claims:

1. \(_{0}^{*}\) is a singleton, whose element is denoted by \(^{*}\),
2. For any \([0,)\), there exists a \(()>0\) such that for all \(0\) and \(_{}_{}^{*}\), one has that \[\|_{}-^{*}\|_{2}( ).\] (22)

Granting these and with the help of Theorem 4.1 (ii), we can derive that for all \(_{n}^{*}^{n,*}\)

\[(_{n}^{*},_{0}^{*})=\|_{n}^{*}-^{*}\|_{2} \|_{n}^{*}-_{2_{n}}\|_{2}+ \|_{2_{n}}-^{*}\|_{2}\] \[(_{n}^{*},_{2 _{n}}^{*})+\|_{2_{n}}-^{*}\|_{2}\] \[_{1}_{n}+_{2}},\]

where \(_{1}\), \(_{2}\) are positive constants, and \(_{2_{n}}_{2_{n}}^{*}\) satisfies \(\|_{n}^{*}-_{2_{n}}\|_{F}=(_{n }^{*},_{2_{n}}^{*})\) (whose existence is guaranteed since \(_{}^{*}\) is convex and compact). Hence,

\[(^{n,*},_{0}^{*}) _{1}_{n}+_{2}}.\]

To proceed, it remains to prove the claims. Define an auxiliary function \(h:^{m}\) as \(h()=_{i=1}^{m}x_{i}-_{i=1}^{m} x_{i}\) for each \(_{+}^{m}\). Consider the following optimization problem:

\[_{} h()\] (23) s.t. \[\{^{m}\}.\]

For the sake of brevity, denote the \(\)-suboptimal solution set of (23) as \(_{}^{*}\). In the remaining part, we will first show that \(_{}^{*}=_{}^{*}\) and then, by the strict convexity of \(h\), the desired two claims hold.

The first step is to show that the optimal function value of the problem (23) satisfies \(h^{*}=f^{*}\). Since it is obvious that \(}=^{*}\) is feasible for (23), \(h^{*} h(})=f(^{*})=f^{*}\). Suppose to the contrary that \(h^{*}<f^{*}\), from the fact that the objective function is coercive and continuous and the feasible set is closed, there exists \(}\) such that it is feasible for LogSpecT and \(^{*}=}\), where \(^{*}\) is an optimal solution to (23). Since \(h^{*}=h(^{*})=h(})=f(})\), this contradicts the fact that \(f(}) f^{*}\). Hence, \(h^{*}=f^{*}\). Next, we will show that \(_{}^{*}=_{}^{*}\). Consider any \(\)-suboptimal solution \(}_{}^{*}\), i.e.,

\[h()=f() f^{*}+=h^{*}+.\]

Hence, \(_{}^{*}\) and it implies that \(_{}^{*}_{}^{*}\). On the other hand, for any \(\)-suboptimal solution \(_{}^{*}\), there exists \(\) that is feasible for LogSpecT such that \(=\). Thus,

\[f()=h() h^{*}+=f^{*}+.\]This implies that \(_{}^{*}\) and consequently \(_{}^{*}_{}^{*}\). Hence, \(_{}^{*}=_{}^{*}\).

Since \(h\) is strictly convex, its optimal solution set \(_{0}^{*}\) is a singleton. Then, \(_{0}^{*}=_{0}^{*}\) is a singleton, which proves the first claim. For the second claim, we know that for any \(_{}_{}^{*}\) there exists \(_{}_{}^{*}\) such that

\[\|_{}-^{*}\|_{2}=\|_{}- ^{*}\|_{2},\] (24)

where \(^{*}_{0}^{*}\). The coerciveness of \(h\) asserts that \(_{}\) and \(^{*}\) are bounded. This together with the fact that \(h\) is strongly convex on any bounded set, illustrates that there exists \(>0\) such that

\[h(_{}) h(^{*})+ h(^{*}), _{}-^{*}+\|_{}-^ {*}\|_{2}^{2} h(^{*})+\|_{}-^{*} \|_{2}^{2},\] (25)

where the second inequality comes from the global optimality of \(^{*}\). Combining (24) and (25) gives that

\[\|_{}-^{*}\|_{2}=\|_{}- ^{*}\|_{2}_{})-h(^{*}))} .\]

This completes the proof of the claims.

### Proof of Corollary 4.4

Suppose to the contrary that there exists a sequence \(\{_{n}^{*}\}_{n}\), where the \(n\)th element is an optimal solution to rLogSpecT with sample size \(n\), such that

\[(_{n}^{*},_{0}^{*}) 0.\]

From Proposition D.1, we know that \(\{_{n}^{*}\}_{n}\) is bounded, and consequently, has a convergent subsequence. Without loss of generality, we may assume that the sequence itself is convergent and the limiting point is \(^{*}\). Note that

\[\|_{n}_{n}^{*}-_{n}^{*}_{n}\|_{F}_{n},\ \ _{n}_{}\ \ \ \ _{n} 0.\]

Hence, \(_{}^{*}=^{*}_{}\). This indicates that \(^{*}\) is feasible for LogSpecT. Then, from Theorem 4.1, we know that \(f(_{n}^{*})=f_{n}^{*} f^{*}\), which leads to \(f(^{*})=f^{*}\) since \(f()=\|\|_{1,1}-^{}()\) is continuous. Together with the fact that \(^{*}\) is feasible, we conclude that \(^{*}\) is an optimal solution to LogSpecT. This further implies that \((_{n}^{*},_{0}^{*}) 0\), which is a contradiction.

### Proof of Lemma 4.7

Recall the generative model (1). Since \(\) follows a sub-Gaussian distribution, it can be shown that for every \(t>0\),

\[(\|\|_{2}>t)(\|\|_{2}>()\|}) Ce^{-^{}t^{2}},\]

for some positive constant \(v^{}\), which means that \(\) also follows a sub-Gaussian distribution. Thus, due to the sub-Gaussian property, \(\|_{n}-_{}\|\) can be explictly bounded by the following lemma.

**Lemma E.5** ([40, Proposition 2.1]).: _Consider sub-Gaussian, identical, independent random vectors \(_{1},_{2},,_{n}^{m}\) with \(n>m\). Then for all \(>0\), it follows that_

\[(\|_{i=1}^{n}_{i}_{i}^{}- [^{}]\|_{2}) 1-2e^{2m-l ^{2}n},\]

_for some constant \(l>0\)._

Setting \(^{2}=(4/l)(2n)m/n\), Lemma E.5 indicates that with high probability (lower bounded by \(1-n^{-1}\)),

\[\|_{n}-_{}\|(} ).\]

## Appendix F Derivations of L-ADMM and Convergence Analysis

This section includes the details of L-ADMM for rLogSpecT.

### Proof of Proposition 5.1

Note that the minimization problem (7) is separable for \(\) and \(\), and can be split into two subproblems:

\[_{(,_{n})}\|_{n}^{(k) }-^{(k)}_{n}+^{(k)}/-\|_{F}^{2},\] (26) \[_{}\ -^{}+_{2}^{(k) }(-^{(k)})+\|-^{(k)}\|_{ 2}^{2}.\] (27)

For problem (26), the optimal solution is the projection of \(_{n}^{(k)}-^{(k)}_{n}+^{(k)}/\) onto \((,_{n})\), which is given by

\[^{(k+1)}=\{1,}{\|}\|_{F}}\} }\ \ \ \ }=_{n}^{(k)}-^{(k)}_{n}+^{( k)}/.\]

For problem (27), the first-order optimality condition gives

\[- 1/+_{2}^{(k)}+(-^{(k)})=0.\]

This together with the fact that the objective function is convex implies that

\[^{(k+1)}=}+}^{2}+4/}}{2}\ \ \ \ }=(^{(k)}-_{2}^{(k)}).\]

### Calculation of \(_{}()\)

The projection of \(\) to \(\) can be calculated via an optimization problem:

\[_{} \ \|-\|_{F}^{2}\] s.t. \[^{}=,\] \[S_{ii}=0,\ i=1,2,,m,\] \[S_{ij} 0,\  i,j,\]

which is equivalent to

\[ _{i<j}((X_{ij}-S_{ij})^{2}+(X_{ji}-S_{ij})^{2})\] s.t. \[S_{ij} 0,\  i<j,\] \[S_{ii}=0,\  i.\]

Hence

\[(_{}())_{ij}=\{0,X_{ij}+X_{ji }\},& i j,\\ 0,& i=j.\]

### Stopping criterion and updating rule of \(\)

We follow the procedures in  to update \(\) in each iteration. Similarly, we define the primal residual and dual residual as follows:

\[p_{}^{(k+1)} =^{(k+1)}-_{n}^{(k+1)}+^{(k+1)} _{n}\|_{F}^{2}+\|^{(k+1)}-^{(k+1)}\|_{2}^{2}},\] \[d_{}^{(k+1)} =^{(k)}(_{n}(^{(k+1)}-^{(k)})-(^ {(k+1)}-^{(k)})_{n}+^{}(^{(k+1)}-^{(k)})).\]

The aim of updating \(\) is to control the decaying speed of \(p_{}\) and \(d_{}\) such that their difference is not too large. To this end, we update \(\) adaptively following the scheme:

\[^{(k+1)}2^{(k)},&p_{}^{(k+1)}>5d_{ }^{(k+1)},\\ ^{(k)}/2,&d_{}^{(k+1)}>5p_{}^{(k+1)},\\ ^{(k)},&.\]

When \(p_{}\) and \(d_{}\) are both smaller than the threshold \(=10^{-5}\), we stop the algorithm.

### Convergence analysis

Define \((_{m}^{},,_{m}^{})^{m m^{2}}\). Then, \(\) satisfies \(()=\) and \(\|^{}\|=m\). Denote

\[-^{}-_{n}^{}_{n}.\]

Then the linearized ADMM update (8) of \(\) can be written as:

\[_{}\ L()+\|()-( ^{(k)})\|_{},\]

where \(\|\|_{}^{}\). Since \(>m+\|_{n}\|^{2}\), we know that \(\) is positively definite. Consequently, by treating \((,)\) as one variable, we can apply Theorem 4.2 in  and directly obtain the result.

## Appendix G More Experiments and Discussions on Synthetic and Real Data

### Influence of graph filters

To make a fair comparison between rSpecT and rLogSpecT, we test rSpecT on BA graphs with the same graph filters and the results are reported in Figure 6. It is obvious that rSpecT fails in these cases and cannot benefit from the increase in sample size. This is reasonable since SpecT fails on BA graphs as indicated in Figure 3, let alone the approximation formulation rSpecT.

We further test rLogSpecT on ER graphs with different numbers of signals observed. The parameter \(_{n}\) is set as \(20\) and the results are reported in Figure 7. The figure shows that for graph filters that are not high-pass, rLogSpecT can achieve nearly perfect recovery when the sample size is large enough. Also, compared with the performance on BA graphs, rLogSpecT works better on ER graphs. This observation is in accordance with the conclusion from Figure 3 that LogSpecT performs better on ER graphs than BA ones. We further notice that the difference between the low-pass graph filter and the high-pass one is huge. To check the conjecture that rLogSpecT generally performs better on low-pass graph filters, we choose different graph filters \((t)\) with \(t\) ranging from \(-2\) to \(2\) and conduct the experiments on ER graphs. When the graph shifting operator is the adjacency matrix, the positive low-pass parameter \(t\) corresponds to low-pass graph filters and the negative \(t\) corresponds to the high-pass ones . We omit the case when \(t=0\) since this filter does not contain any graph information (note that \((0)=\)).

We then repeat the experiments for 50 times and report the average results in Figure 8. The comparison between the performance of low-pass graph filters and high-pass graph filters indicates that the low-pass graph filters generally outperforms the high-pass ones. A closer look at the results shows that the performance grows faster when the absolute value of \(t\) is smaller. And eventually, the graph filter with smaller absolute value of \(t\) prevails. This observation is interesting since Figure 3 indicates that the choice of graph filters has few impacts on the model performance. One explanation is that both low-pass graph filters and high-pass graph filters attenuate some frequencies of the graph and the larger absolute value of \(t\) leads to the more loss of information carried by finite signals.

Figure 6: Performance of rSpecT on BA graphs. Figure 7: rLogSpecT on ER graphs with \(_{n}=20\).

### Experiments on USPS dataset

USPS dataset is a handwritten digit dataset. As shown in , it is nearly stationary with respect to the nearest neighbour graph. This dataset collects images of handwritten digits. In the experiment, each pixel is viewed as a node and the value on the pixel is view as the graph signal. We follow the approach in  to construct the 20 nearest neighbour graph, on which the data is verified to be nearly stationary. More specifically, we pick the 1296 digit 1 images. The weights between two nodes are decided by the Gaussian radial basis function. It turns out that the stationarity measure \(s:=\|(^{}_{n})\|_{2}/\|^{} _{n}\|_{F}\) equals 0.78 in this dataset. Here \(\) is the eigen-matrix of the constructed graph and \(_{n}\) is the covariance matrix of the observed graph signals. We view the 20 nearest neighbour graph as the groundtruth and compare the learned graph from rLogSpecT with it. We present the subgraphs consisting of the top 10 nodes with the largest degrees in the two graphs respectively in Figure 9. The F-measure of these two subgraphs is 0.96. This result corroborates the efficiency of our proposed rLogSpecT model.

Figure 8: Effect of Low-Pass Parameter: different performance of graph filters \((t)\) with \(t\) ranging from \(-2\) to \(2\).

Figure 9: Graph Learning on USPS dataset. (**Left**: subgraph of the symmetric 20 nearest neighbour graph. **Right**: subgraph of the learned graph from rLogSpecT. The F-measure is 0.96)