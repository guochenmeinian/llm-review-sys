# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

the scene. Additionally, to synthesize new content, a strong prior about the visual world and how it would appear beyond the current field of view is necessary. Finally, the generated content should appear smooth and consistent over time.

Previous methods have tackled the task of perpetual view generation for specific domains, for example, synthesizing landscape flythroughs [e.g., 5, 7, 28, 29], or bedrooms walkthroughs [e.g., 45, 47]. These methods involve large-scale training on videos or images from the target domain, which limits their use. In this work, inspired by the transformative progress in text-to-image generation, we take a different route and propose a new framework for _text-driven perpetual view generation_ - synthesizing long-range videos of scenes solely from free-vocabulary text describing the scene and camera poses. Our method does not require any training data but rather generates the scene in a zero-shot manner by harnessing the generative prior learned by a pre-trained text-to-image diffusion model and the geometric priors learned by a pre-trained depth prediction model.

More specifically, given an input text prompt and a camera trajectory, our framework generates the video in an online fashion, one frame at a time. The text-to-image diffusion model is used to synthesize new content revealed as the camera moves, whereas the depth prediction model allows us to estimate the geometry of the newly generated content. To ensure that the generated scene adheres to a feasible 3D geometry, our framework estimates a unified mesh representation of the scene that is constructed progressively as the camera moves in the scene. Since monocular depth predictions tend to be inconsistent across different frames, we finetune the depth prediction model at test time to match the projected depth from our mesh representation for the known, previously synthesized content. This results in a test-time training approach that enables diverse, 3D-consistent perpetual view generation.

To summarize, our key contributions are the following:

1. The first text-driven perpetual view generation method, which can synthesize long-term videos of diverse domains solely from text and a camera trajectory.
2. The first zero-shot/test-time 3D-consistent perpetual view generation method, which synthesizes diverse scenes without large-scale training on a specific target domain.
3. Achieving 3D-consistent generation by progressively estimating a unified 3D representation of the scene.

We thoroughly evaluate and ablate our method, demonstrating a significant improvement in quality and 3D consistency over existing methods.

## 2 Related Work

**Perpetual view generation.** Exploring an "infinite scene" dates back to the seminal work of Kaneva et al. , which proposed an image retrieval-based method to create a 2D landscape that follows a desired camera path. This task has been modernized by various works. Different methods proposed to synthesize scenes given as input a single image and camera motion [e.g., 21, 26, 28, 29, 45, 46, 56, 58, 63]. For example, synthesizing indoor scenes , or long-range landscapes flythroughs . These methods have demonstrated impressive results and creative use of data and self-supervisory signals [e.g., 5, 7, 11, 12, 28, 45]. However, none of these methods considered the task of _text-driven_ perceptual view generation and rather tailored a model for a specific _scene domain_. Thus, these methods require extensive training on a large-scale dataset to learn proper generative and geometric priors of a specific target scene domain.

**3D-Consistent view synthesis from a single image.** Various methods have considered the task of novel view synthesis from a single input image. A surge of methods has proposed to utilize a NeRF  as a unified 3D representation while training on a large dataset from a specific domain to learn a generic scene prior . Other 3D representations have been used by various methods, including multi-plane images , point clouds which hold high-dimensional features , or textured-mesh representation . In Jain et al. , the task is further regularized using the semantic prior learned by CLIP , encouraging high similarity between the input image and each novel view in CLIP image embedding space. All these methods require training a model on specific domains and cannot produce perpetual exploration of diverse scenes.

#### 3D-Aware image generation.

Related to our task is 3D-aware image synthesize, which typically involves incorporating a 3D representation explicitly into a generative model, e.g., a NeRF representation [8; 9; 16; 36; 50], or neural surface renderer . These methods have shown impressive results on structured domains (e.g., cars or faces), but deploying them for diverse scenes is extremely challenging and often requires additional guidance , or heavy supervision, e.g., large-scale data that contains accurate camera trajectories, which can be obtained reliably mainly for synthetic data . We aim for a much more diverse, flexible, and lightweight generation of arbitrary scenes.

#### Text-to-video generation and editing.

Our work follows a broad line of works in the field of text-to-video synthesis and editing. There has been great progress in developing text-to-video generative models by expanding architectures to the temporal domain and learning priors from large-scale video datasets [e.g. 15; 18; 32; 51]. Nevertheless, these models are still in infancy, and are lagging behind image models in terms of quality (resolution and video length). On the other side of the spectrum, a surge of methods proposes to tune or directly leverage a 2D text-image model for video editing tasks [e.g., 3; 6; 40; 59; 65]. We also follow this approach in conjunction with generating an explicit 3D scene representation.

#### Text-to-3D Generation and Editing.

Recently, language-vision pre-trained models [41; 49], and differentiable renderers [30; 35] have been utilized for text-driven 3D content generation. For example, CLIP-based losses have been used for the text-driven stylization of a given surface mesh  or for synthesizing a NeRF from an input text . Taking this approach further, Poole et al.  distills the generative prior of a pre-trained text-to-image diffusion model by encouraging the optimized NeRF to render images that are in the distribution of the diffusion model . Singer et al.  extends this approach to a 4D dynamic NERF and optimizes using a pre-trained text-to-video model. Nevertheless, it is limited to object-centric scenes while we generate long-term walkthrough-type videos. On the other hand, Chang et al.  and Ma et al.  construct the semantic scene graph from natural language and retrieve the required elements from the 3D objects databases.

Finally, in a very recent concurrent work, Text2Room  proposed a similar approach to ours to generate 3D scenes based on textual prompts. However, this work focuses specifically on creating _room meshes_ and tailors both the camera trajectory, prompts and mesh rendering for this purpose. In contrast, we use the mesh representation only as a tool to achieve _videos_ of diverse scenes.

## 3 Method

The input to our method is a text prompt \(\), describing the target scene, and a camera trajectory denoted by \(\{}\}_{i=1}^{T}\), where \(}^{3 4}\) is the camera pose of the \(i^{th}\) frame. Our goal is to synthesize a long-term, high-quality video depicting the desired scene while adhering to the camera trajectory.

Figure 2: **SceneScape pipeline. We represent the generated scene with a unified mesh \(\), which is constructed in an online fashion. Given a camera at \(}\), at each synthesis step, a new frame is generated by projecting \(_{t}\) into \(_{t+1}\), and synthesizing the newly revealed content by using a pre-trained text-to-image diffusion model. To estimate the geometry of the new synthesized content, we leverage a pre-trained depth prediction model; to ensure the predicted depth is consistent with the existing scene \(_{t}\), we deploy a test-time training, encouraging the predicted depth by the model to match the projected depth from \(_{t}\). We then update our mesh representation to form \(_{t+1}\) which includes the new scene content.**Our framework, illustrated in Fig. 2, generates the video one frame at a time, by leveraging a pre-trained text-to-image diffusion model , and a pre-trained monocular depth prediction model [42; 43]. The role of the diffusion model is to synthesize new content that is revealed as the camera moves, whereas the depth prediction model allows us to estimate the geometry of the newly generated content. We combine these two models through a unified 3D representation of the scene, which ensures geometrically consistent video generation. More specifically, at each time step \(t\), the new synthesized content and its predicted depths are used to update a unified mesh representation \(\), which is constructed progressively as the camera moves. The mesh is then projected to the next view \(}\), and the process is repeated.

Since neither the depth prediction model nor the inpainting model are guaranteed to produce consistent predictions across time, we finetune them both to match the content of our unified scene representation for the known, previously synthesized content. We next describe each of these components in detail.

### Initialization and Scene Representation

We denote the pre-trained text-to-image inpainting model by \(\), which takes as input a text prompt \(\), a mask \(\), and a masked conditioning image \(_{t}^{M}\):

\[_{t}=(,_{t}^{M},)\] (1)

The first frame in our video \(}\) is generated by sampling an image from \(\), using \(\) without masking (all-ones mask). The generated frame is then fed to a pre-trained depth prediction model \(g\), which estimates \(}\) the depth map for \(}\), i.e., \(}=g(})\).

With the estimated depth and camera poses, a naive approach for rendering \(}\) is to warp \(}\) to \(}\), e.g., using 2D splatting . However, even with consistent depths, rendering a 3D-consistent scene is challenging. In typical camera motion, multiple pixels from one frame are mapped into the same pixel in the next frame. Thus, the aggregated color needs to be selected carefully. Furthermore, frame-to-frame warping lacks a unified representation, thus, once the scene content gets occluded, it will be generated from scratch when it gets exposed again

To address these challenges, we take a different approach and represent the scene with a unified triangle mesh \(=(,,)\) which is represented by a tuple of vertices (3D location and color), faces and edges, respectively. We initialize \(_{0}\) by unprojecting \((},})\). After the first frame is generated, each synthesis step consists of the following main stages:

### Project and Inpaint

The current scene content, represented by the unified mesh representation \(_{t}\), is projected to the next camera \(}\):

\[(^{M}},^{M}},})=}(_{t},})\] (2)

producing a mask \(}\) of the visible content in \(}\), a masked frame \(_{t+1}^{M}\), and a masked depth map \(^{M}}\). Given \(}\) and \(^{M}}\), we now turn to the task of synthesizing new content. To do so, we apply \(\) using Eq. 1.

### Test-time Depth Finetuning

We would like to estimate the geometry of the new synthesized content and use it to update our unified scene mesh. A naive approach to do so is to estimate the depth directly from the inpainted frame, i.e., \(}=g(})\). However, monocular depth predictions tend to be inconsistent, even across nearby video frames [27; 31; 64]. That is, there is no guarantee the predicted depth \(}\) would be well aligned with the current scene geometry \(_{t}\). We mitigate this problem by taking a test-time training approach to finetune the depth prediction model to be as consistent as possible with the current scene geometry. This approach is inspired by previous methods [27; 31; 64], which encourage pairwise consistency w.r.t optical flow observations in a given video. In contrast, we encourage consistency between the predicted depth and our unified 3D representation in the visible regions in frame \(t+1\). Formally, our training loss is given by:

\[_{disp}=\|^{M}}-g(})} \|_{1}\] (3)where \(^{M}},}\) are given by projecting the mesh geometry to the current view (Eq. 2). To avoid catastrophic forgetting of the original prior learned by the depth model, we revert \(g\) to its original weights at each generation step.

### Test-time Decoder Finetuning

Our framework uses Latent Diffusion inpainting model , in which the input image and mask are first encoded into a latent space \(z=E_{}(},_{t}^{M})\); the inpainting operation is then carried out in the latent space, producing \(\), and the final output image is given by decoding \(\) to an RGB image: \(D_{}()\). This encoding/decoding operation is lossy, and the encoded image is not reconstructed exactly in the unmasked regions. Following Avrahami et al. , for each frame, we finetune the decoder as follows:

\[_{dec}=\|D_{}()_{t+1}-I_{t+1}^{M} \|_{2}+\|(D_{}()-D_{}^{}( ))(1-_{t+1})\|_{2}\] (4)

The first term encourages the decoder to preserve the known content, and the second term is a prior preservation loss . As in Sec. 3.3, we revert the weights of the decoder to its original weights.

### Online Mesh Update

Given the current mesh \(_{t}\), and the inpainted frame \(}\), we use our depth map and camera pose to update the scene with the newly synthesized content. Note that in order to retain the previously synthesized content, some of which may be occluded in the current view, we consider only the new content, unproject, and mesh it, adding it to \(_{t}\). That is,

\[}_{t+1}=(},},},})\] (5)

Finally, \(}_{t+1}\) is merged into the mesh:

\[_{t+1}=_{t}}_{t+1}=(_{t} }_{t+1},_{t}}_{t+1},_{t}}_{t+1})\] (6)

In practice, we also add to the mesh the triangles used to connect the previous content to the newly synthesized one. See Sec. A.1 of appendix for more details.

### Rendering

Rendering a high-quality and temporally consistent video from our unified mesh requires careful handling. First, stretched triangles may often be formed along depth discontinuities. These triangles encapsulate content that is unseen in the current view, which we would like to inpaint when it becomes visible in other views. To do so, we automatically detect stretched triangles based on their normals and remove them from the updated mesh.

Another issue arises when the content at the border of the current frame is close to the camera relative to the scene geometry. This content is typically mapped towards the interior of the next frame due to parallax. This often results in undesirable "floating artifacts" where the revealed border regions are rendered with the existing distant content (see Fig. 3). We assume that such regions should be inpainted, thus we automatically detect and include them in the inpainting mask. See Sec. A.2 of appendix for more details.

## 4 Results

We evaluate our method both quantitatively and qualitatively on various scenes generated from a diverse set of prompts, including photorealistic as well

Figure 3: **Handling floating artifacts**

as imaginary scenes. We generated 50-frame long videos, with a fast-moving camera motion, which follows a backward motion combined with rotations of the camera (see Sec. A of appendix for more details). Since our method represents the scene via a mesh, we focus mainly on indoor scenes. Synthesizing outdoor scenes would require careful handling in cases of dramatic depth discontinuities (as discussed in Sec. 5).

Figure 1 and Fig. 4 show sample frames from our videos. The results demonstrate the effectiveness of our method in producing high-quality and geometrically-plausible scenes, depicting significant parallax, complex structures, and various complex scene properties such as lighting or diverse materials (e.g., ice). Additional results, videos, and output depth maps are included in the Supplementary materials (SM).

Figure 4: Sample frames from our generated videos. Our method generates high-quality and diverse scenes while following a desired camera trajectory. All videos are included in SM.

### Metrics

In all experiments, we use the following metrics for evaluation:

**Depth consistency.** We evaluate the depth consistency of a video by applying COLMAP, using known camera poses, and measuring the Scale-Invariant Root Mean Square Error (SI-RMSE) :

\[=}_{, I}[( D ^{o}()- D^{o}())-( D^{c}()- D^{c}( ))]^{2}}\] (7)

where \(D^{o}\) and \(D^{c}\) are our output depth maps and COLMAP depths, respectively. We compute the SI-RMSE for each frame and report the average per video. To assess the completeness of the reconstruction, we also measure the percentage of pixels recovered by COLMAP, i.e., passed its geometric consistency test. In addition, we report the average reprojection error of COLMAP reconstruction, which indicates the consistency of the video with COLMAP's point cloud.

**Pose accuracy.** We measure the accuracy of the estimated camera trajectory. Specifically, we provide COLMAP only with intrinsic parameters and align the two camera systems by computing a global rotation matrix . We measure the normalized mean accuracy of camera positions (in percentage, relative to the overall camera trajectory), and camera rotations (in degrees).

**Visual quality.** We use Amazon Mechanical Turk (AMT) platform for evaluating the visual quality of our videos compared to the competitors. We adopt the Two-alternative Forced Choice (2AFC) protocol, where participants are presented with two side-by-side videos, ours and the competitor's (at random left-right ordering) and are asked: "_Choose the video/image that has better visual quality, e.g., sharper, less artifacts such as holes, stretches, or distorted geometry_". Additionally, we compute the CLIP aesthetic score -an image aesthetic linear predictor learned on top of CLIP embeddings.

    & Rot. (deg.) \(\) & Trans. (\%) \(\) & Reproj. (pix)\(\) & SI-RMSE \(\) & Density (\%) \(\) & CLIP-AS \(\) & AMT(\%)\(\) \\  Ours & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & - \\ w/o depth f. & \(2.24\) & \(0.02\) & \(0.37\) & \(0.41\) & \(0.90\) & \(5.75\) & 52.7 \\ w/o decoder f. & \(2.81\) & \(0.02\) & \(0.82\) & \(0.17\) & \(0.87\) & \(5.74\) & 63.0 \\ w/o mesh & \(3.55\) & \(0.04\) & \(0.57\) & \(0.18\) & \(0.78\) & \(5.73\) & 69.1 \\ Warp-inpaint & \(4.89\) & \(0.09\) & \(0.97\) & \(0.67\) & \(0.70\) & \(5.43\) & 85.6 \\   

Table 1: **Ablations.** For each ablation baseline, we report, from left to right: accuracy of estimated camera poses, reprojection error, depth SI-RMSE, and the percentage of reconstructed video pixels. We also report CLIP aesthetics score  and measure visual quality using AMT user study, where we report the percentage of judgments in favor of our full method. See Sec. 4.1 and Sec. 4.2 for more details.

Figure 5: **Sample ablation results** of (a) our full method, (b) w/o depth finetuning (Sec. 3.3), (c) w/o decoder finetuning (Sec. 3.4) and (d) w/o mesh (Sec. 3.1). We present sample frames, output depth maps, and COLMAP’s depth maps. As seen, all components are essential for obtaining high-quality and 3D-consistent videos.

### Ablation

We ablate the key components in our framework: (i) depth finetuning (Sec. 3.3), (ii) decoder finetuning (Sec. 3.4), and (iii) our unified mesh representation (Sec. 3.1). For the first two ablations, we apply our method without each component, and for the third, we replace our mesh with 2D rendering, i.e., warping the content from the current frame to the next, using a depth-aware splatting . In addition, we consider the baseline of simple warp-inpaint, i.e., using splatting with the original estimated depth (w/o finetuning). We ran these ablations on 20 videos, using a set of 10 diverse text prompts. See Sec. B for the list of prompts and SM sample videos.

Table 1 reports the above COLMAP metrics (see Sec. 4.1). Our full method results in notably more accurate camera poses, denser reconstructions, and better-aligned depth maps. Excluding depth finetuning significantly affects the depth consistency (high SI-RMSE). Excluding decoder finetuning leads to high-frequency temporal inconsistency, which leads to higher reprojection error. Without a mesh, the reconstruction is significantly sparser and we observe a drop in all metrics. These results are illustrated in Fig. 5 for a representative video.

To assess visual quality, we follow the AMT protocol described in Sec. 4.1, where we ran each test w.r.t. one of the baselines. We collected 4,000 user judgments from 200 participants over 20 videos. As seen in Table 1, the unified mesh plays a major role in producing high-quality videos, where the judgments were ambiguous w.r.t w/o depth finetuning baseline. We hypothesize this is because this baseline still leverages the mesh representation, thus the visual artifacts are temporally consistent, making them more difficult to perceive.

### Comparison to Baselines

To the best of our knowledge, there is no existing method designed for our task - text-driven perpetual view generation. We thus consider the most relevant methods to our setting:

* **VideoFusion (VF) .** A text-to-video diffusion model2, which takes as input a text prompt and produces a 16-frame long video at \(256 256\) resolution. * **GEN-1 .** A text-driven video-to-video translation model, where the translation is conditioned on depth maps estimated from an input source video. Note that in our setting the scene geometry is unknown and is constructed with the video, whereas GEN-1 takes the scene geometry as input, thus tackling a simpler task.

Due to the inherent differences between the above methods, we follow a different evaluation protocol in comparison with each one of them.

Figure 6: **Baseline comparison. Sample results from our method (top), GEN-1  (middle) and VideoFusion  (bottom) on 3 different prompts. Other methods exhibit saturation and blurriness artifacts in addition to temporally inconsistent frames.**

#### 4.3.1 Evaluating 3D-consistency

To compare to VF, we used the same set of prompts as in Sec. 4.2 as input to their model and generated 1000 videos. We evaluate the 3D consistency of VF's videos and ours, using the same metrics as in Sec. 4.2, where we used COLMAP without any camera information, and downsampled our videos to match their resolution. As seen in Table 2, our method significantly outperforms VF in all metrics. This demonstrates the effectiveness of our mesh representation in producing 3D-consistent videos, in contrast to VF, which may learn geometric priors by training on a large-scale video dataset. Note that since VF does not follow a specific camera path, measuring its camera pose accuracy is not possible.

GEN-1 is conditioned on a source video, while our method takes camera trajectories as input. Thus, we used the RealEstate10K dataset , consisting of curated Internet videos and corresponding camera poses.

We automatically filtered 22 indoor videos that follow a smooth temporal backward camera motion to adapt it to our method's setting. The camera poses are used as input to our method, whereas the RGB frames are used as input video to GEN-1 while supplying both methods with the same prompt. We generate 110 videos3 using five different prompts from our ten prompts set that allow reasonable editing of a house (such as "library" rather than "cave"). See Sec. B of appendix for more details.

We measure the video's 3D consistency metrics, as discussed in Sec. 4.2, and report them in Table 2. Our method outperforms GEN-1 in all metrics, even though the geometric structure of the video (depth maps) was given to GEN-1 as input.

#### 4.3.2 Visual Quality and Scene Semantics

We conducted an extensive human perceptual evaluation as described in Sec. 4.1. We collect 3,000 user judgments over 60 video pairs from 50 participants. The results of this survey w.r.t. each method are reported in Table 2. As seen, when compared to VF, our method was preferred in 96% of the cases and in 69% of the cases in comparison with GEN-1. Sample results of those comparisons can be seen in Fig. 6. Note that since GEN-1, VF, and StableDiffusion do not exhibit the same underlying data distribution, existing visual quality metrics such as FID  cannot be used.

To measure how well our generated videos adhere to the desired scene, we measure the CLIP text-similarity score - mean cosine similarity between the prompt and each of the frames. As seen in Table 2, our method follows the scene text prompt more closely than VF and is on par with GEN-1.

    & Rot. (deg.) \(\) & Trans. (\%) \(\) & Reproj. (px) \(\) & Density (\%) \(\) & CLIP-TS \(\) & AMT (\%) \(\) \\  Ours & - & - & **0.29** & **81.4** & **0.25** & **95.97** \\ VF & - & - & \(0.78\) & \(46.36\) & 0.22 & \\   Ours & **1.71** & **3.94** & **0.60** & **91.72** & **0.26** & **69.34** \\ GEN-1 & \(2.47\) & \(8.09\) & \(1.64\) & \(53.96\) & **0.26** & **69.34** \\   

Table 2: **Comparison to baselines. (top) VideoFusion , and (bottom) GEN-1 . We evaluate 3D consistency using COLMAP metrics and measure visual quality using AMT user study, where we report the percentage of judgments in our favor (see Sec. 4.2). Our method outperforms the baselines on all metrics. Note that the experiential setup for these two baselines is different (see Sec. 4.3).**

Figure 7: **Limitation. Outdoor scenes are poorly represented with our mesh representation (left). Occasionally we may observe quality degradation due to error accumulation (right).**Discussion and Conclusion

We posed the task of text-driven 3D-consistent perpetual view generation and proposed a test-time optimization approach to generate long-term videos of diverse scenes. Our method demonstrates two key principles: (i) how to harness two powerful pre-trained models, allowing us to generate scenes in a zero-shot manner without requiring any training data from a specific domain, and (ii) how to combine these models with a unified 3D scene representation, which by construction ensures feasible geometry, and enables high quality and efficient rendering. As for limitations, the quality of our results depends on the generative and geometric priors and may sometime decrease over time due to error accumulation (Fig. 7 right). In addition, since we represent the scene with triangular meth, it is difficult for our method to represent dramatic depth discontinuities, e.g., sky vs. ground in outdoor scenes (Fig. 7 left). Moreover, since our method works in an online fashion and does not change the previously generated frames, the errors that appeared in previous frames can propagate to the next frames. Devising a new 3D scene representation tailored for the perpetual view generation of arbitrary scenes is an intriguing avenue of future research. We believe that the principles demonstrated in our work hold great promise in developing lightweight, 3D-aware video generation methods.

## 6 Acknowledgments

We thank Shai Bagon for his insightful comments. We thank Narek Tumanyan for his help with the website creation. We thank GEN-1 authors for their help in conducting comparisons. This project received funding from the Israeli Science Foundation (grant 2303/20).