# CausalStock: Deep End-to-end Causal Discovery

for News-driven Stock Movement Prediction

 Shuqi Li\({}^{1}\)1  Yuebo Sun\({}^{1}\)1  Yuxin Lin\({}^{2}\)  Xin Gao\({}^{3}\)  Shuo Shang\({}^{4}\)  Rui Yan\({}^{1}\)2

\({}^{1}\) Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{2}\) Peking University \({}^{3}\) King Abdullah University of Science and Technology

\({}^{4}\) University of Electronic Science and Technology of China

{shuqili, sunyuebo0418, ruiyan}@ruc.edu.cn

linyuxin@stu.pku.edu.cn,xin.gao@kaust.edu.sa,jedi.shang@gmail.com

###### Abstract

There are two issues in news-driven multi-stock movement prediction tasks that are not well solved in the existing works. On the one hand, "relation discovery" is a pivotal part when leveraging the price information of other stocks to achieve accurate stock movement prediction. Given that stock relations are often unidirectional, such as the "supplier-consumer" relationship, causal relations are more appropriate to capture the impact between stocks. On the other hand, there is substantial noise existing in the news data leading to extracting effective information with difficulty. With these two issues in mind, we propose a novel framework called CausalStock for news-driven multi-stock movement prediction, which discovers the temporal causal relations between stocks. We design a lag-dependent temporal causal discovery mechanism to model the temporal causal graph distribution. Then a Functional Causal Model is employed to encapsulate the discovered causal relations and predict the stock movements. Additionally, we propose a Denoised News Encoder by taking advantage of the excellent text evaluation ability of large language models (LLMs) to extract useful information from massive news data. The experiment results show that CausalStock outperforms the strong baselines for both news-driven multi-stock movement prediction and multi-stock movement prediction tasks on six real-world datasets collected from the US, China, Japan, and UK markets. Moreover, getting benefit from the causal relations, CausalStock could offer a clear prediction mechanism with good explainability.

## 1 Introduction

The financial services industry has maintained a leading position in embracing data science methodologies to inform investment determinations. Within this domain, quantitative trading has garnered substantial attention from both academia and industry. Researchers have consistently worked on exploring different approaches to predict the stock movement (rise or fall of stock price) for many years, such as uni-stock movement prediction , multi-stock movement prediction , news-driven stock movement prediction  and so on, which have shown significant success. These methods usually model the stock movement prediction task as a time series classification problem.

In this paper, we focus on the news-driven multi-stock movement prediction task. A prevalent model paradigm for this task often takes the historical price features and the stock-related news of multiple stocks as inputs and then leverages the well-designed neural networks to make stock movement predictions. There are two key modeling points for tackling this task: modeling the stockrelations to enhance the prediction accuracy, and building the text mining module to extract effective information from news data that benefits stock movement prediction. Although previous work has made significant progress, there are still some issues that require further attention. We will elaborate on them in the following.

For stock relation modeling, many existing works are commonly attention-based [15; 19; 23] or graph-based [34; 23]. These methods aim to model the correlation relation between stocks. However, the company relations are often unidirectional, such as the "investing" and "member of," leading to the unidirectional relations of their stocks. Thus, causal relations are more appropriate for depicting the impact between stocks, as they identify the direction of information flow and are more informative than correlations. With the development of causal science, many researchers have started to use deep end-to-end networks for causal relations discovery of panel data or temporal data [9; 14], in which the causal relations are defined as directed acyclic graphs, i.e., causal graphs, and the Functional Causal Models (FCMs) are often utilized to optimize the causal graph by simulating the data generation mechanism. This provides a solid theoretical foundation for causal discovery for stocks.

In recent years, an extrinsic text mining module has emerged as a plausible avenue through the alignment of financial news and social media posts, thereby elucidating intricate market insights that extend well beyond mere considerations of price dynamics, trading volumes, or financial indicators [41; 17; 35; 33]. Conventional text representations obtained by using GRU  or LSTM  exhibit many limitations. Specifically, news text data are often characterized by substantial noise because of the presence of irrelevant or ambiguous information [38; 7; 37]. The effective information for stock movement prediction gets intertwined with this noise, presenting a considerable challenge for these modules to discern meaningful signals accurately. In contrast, Large Language Models (LLMs) have unique advantages in this situation due to their advanced knowledge and reasoning abilities. Besides, LLMs can identify meaningful information within noisy environments [29; 4].

Motivated by these requirements, we propose an innovative news-driven multi-stock movement prediction model named CausalStock. In CausalStock, we design a Denoised News Encoder, which leverages LLMs to score every news text from multiple perspectives. Then the evaluation scores are taken as denoised text representations. To discover the causal relations between stocks, we propose a Lag-dependent temporal causal discovery module, from which we obtain the causal graph distribution. Based on the input market information and learned causal graph distribution, CausalStock employs an FCM  to make predictions. We summarize the contributions of our paper as follows:

* We propose a novel news-driven multi-stock movement prediction method named CausalStock, which could discover the causal relations among stocks and make accurate movement predictions simultaneously.
* Different from the past lag-independent causal discovery method , CausalStock involves a lag-dependent temporal causal discovery module, which intuitively links the temporal causal relations according to the time lag, making it more suitable for temporal stock data.
* To extract useful information from the massive noisy news text data, an LLM-based Denoised News Encoder is proposed by taking advantage of the evaluation ability of LLM, which outputs the denoised news representation for better information utilization.

Experiments on \(6\) public benchmarks show the performance of CausalStock as a news-driven multi-stock movement prediction method. Moreover, we conduct extensive analytical experiments to show the explainability of our key modules.

## 2 Related work

Stock prices predictionIn traditional trading practices, there are two analysis paradigms commonly used to make stock movement predictions: technical analysis and fundamental analysis. With technical analysis, investors and traders tend to forecast stock prices relying on historical price patterns. Fundamental analysis aims to assess the intrinsic value of a stock by considering other factors besides historical prices, such as financial statements, industry trends, and economic conditions.

Since stock movement prediction involves sequential data, RNN-based networks are applied in many works. ALSTM  integrated a dual-stage attention mechanism with LSTM. Adv-ALSTM  further employed adversarial training by adding perturbations to simulate the stochastic and unstable nature of the price variable. In recent years, researchers also exploited attention-based mechanisms to model complex interactions. DTML  is proposed to predict by using a transformer and LSTM to capture the asymmetric and dynamic correlations between stocks. With the development and prosperity of NLP technology, text from social media and online news has become a new popular source of fundamental analysis. HAN  designed two attention networks to recognize both the influential time periods of a sequence and the important news at a given time. Stocknet  proposed a deep generative model with recurrent, continuous latent variables. MSHAN  exploited a multi-stage TCN-LSTM hybrid model. PEN  proposed a Shared Representation Learning module to capture interactions between price data and text data. Additionally, many works modeled the correlation between stocks to enhance stock price prediction. MAN-SF  constructed a graph attention network with price features, social media, and inter-stock relationships based on the interrelationship between price and tweets. CMIN  was proposed to model the asymmetric correlations between stocks by computing transfer entropy. In addition, Co-CPC  modeled the dependence between a certain stock industry and relevant macroeconomic variables. All the aforementioned methods aim to discover the correlation relations among stocks, as elaborated before, the causal relations are more appropriate to depict the information flow of stocks. In this work, we aim to model the causal relations for better stock movement prediction performance.

Causal discoveryThe conventional approach to discovering causal relations typically involves conducting randomized experiments [27; 12]. However, conducting randomized experiments can often be excessively expensive, overly time-consuming, or impossible to execute. Consequently, causal discovery, which aims to infer causal relationships from purely observational data, has attracted considerable attention within the machine learning community over the last decade [5; 12]. Causal discovery can be classified into three groups: constraint-based [10; 30; 31], score-based [2; 46; 26], and functional causal models (FCMs) [12; 26]. FCMs define the causal relations by directed acyclic graphs (DAGs) and identify causal links through nonlinear functions, such as neural networks [14; 9; 47; 18]. Specifically, DECI  is a deep end-to-end framework to discover causal relations based on additive noise FCM. After that, Rhino  was proposed to tackle the temporal causal discovery problem, which incorporates non-linear relations, instantaneous effects, and flexible history-dependent noise. In this work, we focus on utilizing the FCM to discover stock relations.

## 3 Preliminary & problem formulation

### Preliminary

In CausalStock, we integrate the model inputs with causal relations into FCM for prediction. In this section, we introduce the fundamental concepts of FCM and the temporal causal graph.

Temporal causal graphConsider a multivariate time series \(\{X_{t}^{i}\}_{i=1}^{D}\) with \(D\) variables, the temporal causal graph \(\) is commonly defined as a series of directed acyclic graph \(=[G_{1},G_{2},,G_{L}]=\{G_{l}\}_{l=1}^{L}^{L D}\) with maximum time lag \(L\). Each \(G_{l}^{D D}\) specifies the lagged causal relationships between \(X_{t-l}\) and \(X_{t}\), the element \(G_{l,ji}=1\) if there exists a causal link \(X_{t-l}^{j} X_{t}^{i}\) and \(G_{l,ji}=0\) otherwise.

Functional causal model (FCM)FCM represents a set of generative functions that incorporate the input features based on causal knowledge (structured as a causal graph) to produce a final prediction. Optimizing the prediction accuracy concurrently refines the underlying causal graph. The theoretical demonstration presented in [14; 9] indicates that if the prediction is accurate, the causal graph can be considered a reliable approximation of real causal relations. Given the temporal causal graph \(\) defined as before, a temporal FCM is defined as follows:

\[X_{t}^{i}=F_{i}(_{}^{i}(<t),z_{t}^{i}),\] (1)

where \(_{}^{i}(<t)\) indicates the time-lagged parent nodes of variable \(X_{t}^{i}\) following the temporal causal graph \(\) and \(z_{t}^{i}\) represents mutually and serially independent exogenous noise. Here \(F_{i}\) is a function which implies how variable \(X_{t}^{i}\) depends on its parents and the noise \(z_{t}^{i}\). Given the distribution of noises for different variables \(\{z_{t}^{i}\}_{i=1}^{D}\) and causal graph, this FCM induces a joint distribution of the multivariate time series process \(\{X_{t}^{i}\}_{i=1}^{D}\).

### Problem formulation

In this paper, we focus on tackling the news-driven multi-stock movement prediction task. For the target trading day \(T\), we denote the model inputs as the past \(L\) time lag information of \(D\) stocks as \(_{<T}=\{X_{t}^{i}\}_{t=T-L:T-1}^{i=:D}=[_{<T},_{<T}]=\{[C_{i} ^{i},P_{t}^{i}]\}_{t=T-L:T-1}^{i=:D}\), where \(C_{i}^{i}\) and \(P_{t}^{i}\) represent the news corpora representation and the historical price features representation of \(i\)-th stock at time step \(t\) respectively. The objective is to predict the movement of adjusted close prices \(_{T}=\{y_{T}^{i}\}_{i=1}^{D}^{D 1}\) on \(T\)-th trading day of all stocks simultaneously, where \(y_{T}^{i}\{0,1\}\) representing the \(i\)-th stock price will fall or rise at trading day \(T\), i.e., stock movement. In a theoretical way, this task could be trained by maximizing the log-likelihood of conditional probability distribution \(p(_{T}_{<T})\), so that the most likely \(_{T}\) are generated.

## 4 CausalStock

### Model overview

The conditional probability distribution could be further factorized as follows:

\[p(_{T}_{<T})=_{}p(_{T}, _{<T})d=_{}p(_{T}_{<T}, )p(_{<T})d.\] (2)

The overall process is taken as two joint training parts: temporal causal graph discovery \(p(_{<T})\) and the prediction process given the causal relations \(p(_{T}_{<T},)\). The probabilistic graphic representation of this modeling process is shown in Figure 1. In CausalStock, we develop a lag-dependent causal discovery module, according to which we could take another step by modeling \(p(_{<T})\) as a lag-dependent format:

\[p(_{<T})=p(G_{1} X_{T-1})_{t=2 }^{L}p(G_{t} G_{t-1},X_{T-l}).\] (3)

For the prediction part \(p(_{T}_{<T},)\), we design an FCM as shown in Equation 9 to predict the future movement based on the past information \(_{<T}\) and the discovered temporal causal graph \(\).

In a nutshell, CausalStock comprises three primary components as shown in Figure 2:

1. Market Information Encoder (MIE) encodes the news text and price features. In this part, an LLM-based Denoised News Encoder is proposed;
2. Lag-dependent Temporal Causal Discovery (Lag-dependent TCD) module leverages variational inference to mine the causal relationship based on the given market information of stocks, i.e., modeling \(p(_{<T})\);
3. Functional Causal Model (FCM) generates the prediction of future price movements according to the discovered causal graph, i.e., modeling \(p(_{T}_{<T},)\).

### Market information encoder (MIE)

Market Information Encoder (MIE) takes news corpora and numerical stock price features as inputs, and outputs the historical market information representations \(_{<T}=[_{<T},_{<T}]=\{[C_{t}^{i},P_{t}^{i}]\}_{t=T-L:T-1}^{i= 1:D}=\{X_{t}^{i}\}_{t=T-L:T-1}^{i=1:D}\) for \(D\) stocks with time lag \(L\). For \(i\)-th stock, each time step representation \(X_{t}^{i}\) is the combination of the text representation \(C_{t}^{i}\) generated by the news encoder and the historical price features representation \(P_{t}^{i}\) generated by the price encoder.

Price encoderFor \(i\)-th stock, we denote the raw adjusted closing, highest, lowest, open, closing prices and trading volume on trading day \(t\) as \(_{t}^{i}=[_{t}^{i,a},_{t}^{i,h},_{t}^{i,l}, {P}_{t}^{i,o},_{t}^{i,c},V_{t}]\). By feeding \(_{t}^{i}\) into the embedding layer, the historical prices could be represented as \(P_{t}^{i}^{d_{p} 1}\), where \(d_{p}\) is the price embedding size.

Figure 1: Illustration of the process of stock movement \(_{T}\) forecasting. The forecasting process is denoted by solid lines with parameters \(\) and the causal discovery process is denoted by dashed lines with variational approximation parameters \(\), \(q_{}\) is the posterior distribution of the causal graph.

LLM-based denoised news encoder (DNE)News Encoder aims to embed stock-related news text, which evolves from the small sequential module, e.g., GRU , to pre-trained models, e.g., Bert and Roberta [6; 22], offering greater performance and scalability. However, news text data often contains massive noise due to the following factors. Firstly, news comes from a wide range of sources with varying degrees of reliability and editorial standards. This variability contributes to inconsistencies and inaccuracies in the information presented. Secondly, the sheer volume of news content generated daily can lead to information overload, where significant information is buried under less relevant or redundant information. Thirdly, the use of complex or ambiguous language can also add noise, making it difficult to extract precise information relevant to specific needs, such as stock movement prediction. Addressing these challenges requires sophisticated text mining and natural language processing techniques to filter out noise and extract useful, accurate information from news text data. With the development of large language models, current large language models can accurately capture the meaning of text and have a strong capability to evaluate text. Therefore, here we propose an LLM-based Denoised News Encoder to tackle these standing challenges.

LLM-based Denoised News Encoder is an innovative textual representation approach that not only proficiently captures salient information from extensive news texts but also assimilates external knowledge derived from LLMs to enrich the representations. Specifically, we employ an LLM and devise a series of prompts (see Appendix A for the whole designed prompts) to analyze the relationship between a news text and a specific stock from five dimensions: correlation between the news and the stock, sentiment polarity of the news, significance of the news event, potential impact of the news on stock prices, and duration of the news impact. Each dimension is scored, with Correlation, Importance, Impact, and Duration ranging from \(0\) to \(10\), while Sentiment varies from \(-1\) to \(1\). Thus the \(i\)-th text at day \(t\) is represented as a five-dimensional representation \(_{t}^{i}^{l 5}\). After the embedding layer, we obtain the final denoised news embedding \(C_{t}^{i}^{l d_{m}}\). This novel encoding method amalgamates information derived from the primary text, the external knowledge embedded and the evaluation ability within the LLM. Besides, this method effectively reduces the significant noise present in the original text data.

### Lag-dependent temporal causal discovery (Lag-dependent TCD)

In this section, we propose Lag-dependent Temporal Causal Discovery module. Inspired by , our model takes a Bayesian view for modeling the distribution of temporal causal graph, which aims to learn the posterior distribution \(p(_{<T})\). Unfortunately, the exact graph posterior is intractable

Figure 2: The structure of CausalStock. For illustration, we use market information during 07/02 - 07/05 of three stocks (AAPL, GOOG, META) to predict the movements of 07/05.

because of the large combination space of \(\). Here we adopt the variational inference  to get the approximator \(q_{}()\), where \(\) indicates the parameter set of variational inference.

Graph priorThe prior \(p()\) consists of two parts: the graph sparseness prior and the domain-specific knowledge prior. The unnormalised graph prior is as follows,

\[p()(-_{s}\|_{1.L}\|_{F}^{2}- _{d}\|_{1.L}-_{1.L}^{p}\|_{F}^{2}),\] (4)

where \(_{s}\) and \(_{d}\) are scalar weights of graph sparseness and domain-specific knowledge constraint; \(^{p}\) is an optional domain-specific knowledge graph, which allows users to incorporate pre-defined knowledge for guiding CausalStock, turning it into a knowledge and data-driven framework. Suppose a sudden event affects the causal relationships between stocks, such as a company ending a partnership. By incorporating this new pre-defined knowledge into \(^{p}\), the causal graph is dynamically updated to reflect the latest market structure and relationship changes. \(\|\|_{F}\) denotes Frobenius norm. It should be noted that there is no need to give a DAG constraint for the temporal causal graph defined in our paper, it is DAG naturally for the irreversibility of time.

Variational approximating graph posteriorAccording to Equation 3, we factorize the approximator \(q_{}()\) in the same way. For each underlying causal link \(G_{l,ji}\) in \(\), we let the posterior \(q_{}(G_{l,ji} G_{l-1,ji})\) subject to a Bernoulli distribution \(\). So that the probability distribution of \(q_{}()\) could be a product of Bernoulli distributions as follows,

\[q_{}()=q_{}(G_{1})_{l=2}^{L}q_{} (G_{l} G_{l-1})=_{i=1}^{D}_{j=1}^{D}q_{}(G_ {1,ji})_{l=2}^{L}_{i=1}^{D}_{j=1}^{D}q_{}(G_{l, ji} G_{l-1,ji}).\] (5)

The existence and non-existence likelihood tensors of causal links are parameterized as \(=\{U_{l}\}_{l=1}^{L}=\{u_{l,ji}\}_{l=1:L}^{j,i=1:D}^{L D  D}\) and \(=\{V_{l}\}_{l=1}^{L}=\{v_{l,ji}\}_{l=1:L}^{j,i=1:D}^{L  D D}\) separately, where \(u_{l,ji}\) indicates the likelihood for edge existence from \(X_{T-l}^{j}\) to \(y_{T}^{i}\) and \(v_{l,ji}\) is the likelihood for no-edge, which are all learnable parameters. To model the dependency between \(G_{l,ij}\) with \(G_{l-1,ij}\), we propose the following transformation:

\[u_{l,ji}^{}=h_{u}(u_{l,ji},u_{l-1,ji}),v_{l,ji}^{}=h_{v }(v_{l,ji},v_{l-1,ji}),\] (6)

where \(h_{u}\) and \(h_{v}\) are trainable 3-layer MLPs. After normalization, the link existence probability tensor is denoted as \(=\{_{l}\}_{l=1}^{L}=\{_{l,ji}\}_{l=1:L} ^{j,i=1:D}\),

\[_{l,ji}=(u_{l,ji}^{})/((u_{l,ji}^{ })+(v_{l,ji}^{})),\] (7)

where \(_{l,ji}\) represents the link probability from \(X_{T-l}^{j}\) to \(_{T}^{i}\). Thus, we could derive the variational posterior:

\[q_{}(G_{1,ji})(1,_{1,ji}),q_{ }(G_{l,ji}|G_{l-1,ji})(1,_{l,ji}),q_{}()_{l=1}^{L}_{i=1}^{D}_{i=1} ^{D}(1,_{l,ji}).\] (8)

In the training stage, we employ the Gumbel-softmax reparameterization [24; 16] to stochastically estimate the gradients with respect to \(\). Besides, we design another parameterized learnable causal weight graph \(}=\{_{l}\}_{l=1}^{L}^{L D D}\) to measure the causal degree. The separate design of the causal existence graph and the causal weight graph allows for more comprehensive modeling of causality. Once our model is fitted, the time series causal graph \(\) can be sampled by \( q_{}()\) to represent the relation network and information flow of the stock market.

### Functional causal model (FCM)

In this section, we design an FCM to model \(p_{}(_{T}_{<T},)\), where \(\) denotes the parameter set of FCM. We focus on additive noise FCM  to generate \(_{T}=\{_{T}^{i}\}_{i=1}^{D}^{D 1}\):

\[_{T}^{i}=F_{i}(_{}^{i}(<T),z_{T}^{i} )=f_{i}(_{}^{i}(<T))+z_{T}^{i},\] (9)

where \(z_{t}^{i}\) represents mutually and serially independent dynamical noise, and \(f_{i}:^{D L}^{1}\) are general differentiable non-linear function that satisfies the relations specified by the temporal causal graph \(\) strictly, namely, if \(X_{t}^{j}_{G}^{i}(<T)\), then \( f_{i}/ X_{t}^{j}=0\).

We design a novel FCM to aggregate market information including news and prices based on the discovered causal graph \(\) and causal weight graph \(}\):

\[f_{i}(_{}^{i}(<T))=( _{i}(_{l=1}^{L}_{j=1}^{D}G_{l,ji}_{l,ji}[( P_{T-l}^{j}),(C_{T-l}^{j})])),\] (10)

where \(_{i}\), \(\) and \(\) are all neural networks. \(\) and \(\) are shared weights across nodes and lags for efficient computation. \([,]\) denotes the concatenate operation. We apply the logistic Sigmoid function to output the movement probability of \(_{T}\) and use it directly as the output of CausalStock.

For the exogenous noise \(_{T}^{i}\) modeling, we adopt Gaussian distribution, i.e., \(z_{T}^{i}(0,(^{i})^{2})\), where per-variable variances \((^{i})^{2},i[1,D]\) are trainable parameters to represent the uncertainty part. According to Change of variables formula , the conditional distribution \(p_{}(y_{T}^{i}_{}^{i}(<t))\) could be represented as:

\[p_{}(y_{T}^{i}_{}^{i}(<t))=p_ {z_{i}}(z_{T}^{i})|}{ z_{T}^{i}} |^{-1}=p_{z_{i}}(z_{T}^{i}),\] (11)

where \(p_{z_{i}}\) is the aforementioned Gaussian distribution for stock \(i\). \(|}{ z_{T}^{i}}|\) indicates the absolute value of the Jacobian-determinant for \(F_{i}\), \(|}{ z_{T}^{i}}|^{-1}=1\) is derived according to Equation 9. Now the log likelihood \( p_{}(_{T}_{<T},)\) could be further represented as:

\[ p_{}(_{T}_{<T},)=_{i=1}^{D}  p_{}(y_{T}^{i}_{}^{i}(<T) )=_{i=1}^{D} p_{z_{i}}(z_{T}^{i}).\] (12)

### Training objective

We train our model by maximizing the conditional log-likelihood \( p_{}(_{T}_{<T})\). The variational evidence lower bound (_ELBO_) of the model objective is derived as follows:

\[& p_{}(_{T}_{<T} )=_{}()}{q_{}( {G})}p_{}(_{T}_{<T},)p()d\\ &_{}q_{}() p_{} (_{T}_{<T},)p()d+H (q_{}())\\ & E_{q_{}()}[ p_{}( _{T}_{<T},)+ p()]+H (q_{}())\\ & E_{q_{}()}[_{i=1}^{D} p_{ z_{i}}(z_{T}^{i})+ p()]+H(q_{} ()).\] (13)

Here, \(p()\) represents the prior of causal graph, and \(H(q_{}())\) is the entropy of the posterior approximator. \( p_{}(_{T}_{<T},)= p_{z}( _{T})\) is the log-likelihood of the target distribution, in which \(_{T}\) is calculated by Equation 9 at training stage.

Besides, we further adopt the binary cross entropy loss as another objective \((_{T},_{T})\) to improve the learning performance, where \(_{T}\) is the ground truth movement at target trading day \(T\). Overall, the final training loss \(\) is as follows,

\[(_{T},_{T})& =-_{i=1}^{D}(g_{T}^{i}(y_{T}^{i})+(1 \!-\!g_{T}^{i})(1\!-\!y_{T}^{i}))\\ &=(-+ (_{T},_{T}))\] (14)

where \(\) is the scalar weight to balance loss terms. We note that the required assumptions and the theoretical guarantees are summarized in Appendix B.

## 5 Experiments

### Experimental setup

Except for the news-driven multi-stock movement prediction task, our model could also handle the multi-stock movement prediction task without news by removing the Denoised News Encoder. Thus, we do the experiments for both two tasks.

**Dataset** (Appendix C.1): We train and evaluate our model and baselines on six datasets: ACL18 , CMIN-US , CMIN-CN , KDD17 , NI225 , and FTSE100 . The first three of them including both historical prices and text data are used for news-driven multi-stock movement prediction task evaluation, while the last three are for multi-stock movement prediction task evaluation without news data. **Evaluation metrics** (Appendix C.2): We evaluate the prediction performance of models by Accuracy (ACC) and Matthews Correlation Coefficients (MCC). **Baselines** (Appendix C.3): HAN , Stocknet , PEN , CMIN  for news-driven multi-stock movement prediction task. LSTM , ALSTM , Adv-ALSTM , DTML  for multi-stock movement prediction task. **Parameter setup** (Appendix C.4): Our model is implemented with Pytorch on 4 NVIDIA Tesla V100 and optimized by Adam . The parameter sensitivity study can be found in Appendix C.4.

### Results of prediction accuracy

As shown in the top half of Table 1, CausalStock outperforms all baselines on ACC as well as MCC across three datasets demonstrating robustness performance for news-driven multi-stock movement prediction task. For the multi-stock movement prediction task, the results are reported in the bottom half of Table 1. As can be seen, CausalStock exceeds all baselines across three datasets with stable performance. Overall, the results demonstrate that the proposed CausalStock can indeed improve the performance for two stock movement prediction tasks, showing the strong capabilities in handling financial texts and discovering causal relations among stocks.

### Ablation study

For the ablation study, we conduct several model variants on ACL18, CMIN-CN and CMIN-US to explore the contributions of different settings in CausalStock. For the main framework, we have the following five variants. **CausalStock w/o TCD**: removing the causal discovery module from CausalStock; **CausalStock w/o News**: removing the news encoder from CausalStock and just taking prices data as input; **CausalStock w/o link non-existence modeling**: only model the causal link existence likelihood and leverage Sigmoid function to obtain the link existence probability; **CausalStock w/o Lag-dependent TCD**: replacing the Lag-dependent Temporal Causal Discovery module with the Lag-independent Temporal Causal Discovery module; **CausalStock with Variable-dependent TCD**: we add a variable-dependent causal mechanism that explicitly captures the dependencies among different stock edges. Specifically, each edge's probability is conditioned on the states of all other edges at the same time step, and the conditional function is the same as the function in the lag-dependent mechanism (Equation 6). Furthermore, we explore the performance of six different Traditional News Encoders by replacing the denoised news encoder, which outputs the news embeddings as representations. **CausalStock with Glove + Bi-GRU**: leveraging the Glove word embedding

    \\   &  &  &  \\   & **ACC** & **MCC** & **ACC** & **MCC** & **ACC** & **MCC** \\ 
**HAN** & \(57.64 0.0040\) & \(0.0518 0.0050\) & \(53.72 0.0020\) & \(0.0103 0.0015\) & \(53.59 0.0037\) & \(0.0159 0.0026\) \\
**StockNet** & \(58.23 0.0030\) & \(0.0808 0.0071\) & \(52.46 0.0041\) & \(0.0220 0.0025\) & \(54.53 0.0062\) & \(0.0450 0.0043\) \\
**PEN** & \(59.89 0.0090\) & \(0.1556 0.0018\) & \(53.20 0.0051\) & \(0.0267 0.0023\) & \(54.83 0.0086\) & \(0.0857 0.0065\) \\
**CMIN** & \(62.69 0.0029\) & \(0.2090 0.0016\) & \(54.33 0.0085\) & \(0.0460 0.0055\) & \(55.28 0.0094\) & \(0.1110 0.0990\) \\
**CausalStock** & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   \\   &  &  &  \\   & **ACC** & **MCC** & **ACC** & **MCC** & **ACC** & **MCC** \\ 
**LSTM** & \(51.18 0.0066\) & \(0.0187 0.0110\) & \(50.79 0.0079\) & \(0.0148 0.0162\) & \(50.96 0.0065\) & \(0.0187 0.0129\) \\
**ALSTM** & \(51.66 0.0041\) & \(0.0316 0.0119\) & \(50.60 0.0066\) & \(0.0125 0.0139\) & \(51.06 0.0038\) & \(0.0231 0.0077\) \\
**StockNet** & \(51.93 0.0001\) & \(0.0335 0.0050\) & \(501.15 0.0054\) & \(0.0050 0.0118\) & \(50.36 0.0095\) & \(0.0134 0.0135\) \\
**Adv-ALSTM** & \(51.69 0.0058\) & \(0.0333 0.0137\) & \(51.60 0.0103\) & \(0.0340 0.0201\) & \(50.66 0.0067\) & \(0.0155 0.0140\) \\
**DTML** & \(53.53 0.0075\) & \(0.0733 0.0195\) & \(52.76 0.0103\) & \(0.0626 0.0230\) & \(52.08 0.0121\) & \(0.0502 0.0214\) \\
**CausalStock** & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Main results of CausalStock and baselines for two stock movement prediction tasks on multiple datasets. Following the setting of baselines, the standard deviations are calculated across 10 runs for the news-driven task and 5 runs for the task without news.

and the Bi-GRU as news encoder ; **CausalStock with Bert**: leveraging the pre-trained Bert (Bert-base-multilingual-cased ) as news encoder; **CausalStock with Roberta**: leveraging the pre-trained Roberta (Roberta-base ) as news encoder; **CausalStock with FinBert**: leveraging the pre-trained FinBert  as news encoder; **CausalStock with FinGPT**: leveraging the pre-trained FinGPT (FinGPT-v3.3 ) as news encoder; **CausalStock with Llama**: leveraging the pre-trained Llama ( Llama-7b-chat-hf ) as news encoder to output news embeddings. Moreover, we explore the performance of three different LLMs for the denoised news encoder. **CausalStock with FinGPT**: leveraging a financial LLM FinGPT (FinGPT-v3.3 ) as denoised news encoder; **CausalStock with Llama**: leveraging Llama (Llama-7b-chat-hf ) as denoised news encoder. The ablation study results are summarized in Table 2.

We have the following observations: (1) CausalStock with news encoders all perform better than CausalStock without news, suggesting news data is particularly helpful for stock movement prediction. (2) Compared to CausalStock w/o Lag-independent TCD, CausalStock with Lag-dependent TCD has a better performance, demonstrating the value of the lag-dependent mechanism. (3) By comparing the CausalStock and CausalStock with Variable-dependent TCD, the results show that incorporating a variable-dependent causal mechanism has the potential to enhance model performance. However, the improvements are not uniform and vary depending on the dataset, which emphasizes that further validation is needed. While the above results show a promising performance of the variable-dependent causal mechanism, it significantly increases the computational complexity (from \(O(L D^{2})\) to \(O(L D^{4})\)), making it challenging to apply the model to markets with large numbers of stocks. (3) By using FinGPT and Llama as the news encoder and denoised news encoder respectively, we can observe that denoised news encoders have a relatively higher ACC and MCC than their as the traditional news encoders, suggesting the value of denoised news encoders. Overall, the ablation studies show that every component contributes to CausalStock.

### Results of explainability

Here, we present many cases detailing the interpretability of CausalStock from two perspectives: the news representation from the Denoised News Encoder, and the causal graph discovered by the Lag-dependent TCD module.

Firstly, regarding the Denoised News Encoder module, three cases are selected as shown in Figure 3(c). A piece of news about APPL suggests a potential delay in its 5G iPhone launch, with Denoised News Encoder giving it a negative sentiment score of \(-0.7\) and an impact score of \(9\). Similarly, a news about TSLA hints at surpassing a significant delivery milestone, receiving a positive sentiment score of \(0.7\). In contrast, a news piece showing no discernible connection to GOOG is scored with negligible impact. These cases indicate the Denoised News Encoder's efficacy in discerning and quantifying the potential influence of news on respective stock prices.

Secondly, concerning the causal graph discovered by Lag-dependent TCD, we denote the causal strength graph as the dot product of the causal graph \(\) and the causal weight graph \(}\). Every item of causal strength graph indicates not only the causality of two stocks but also the degree of causality. The visualized causal strength matrix for ACL18 is shown in Figure 3(b) with a heatmap. From

    &  &  &  &  \\   & & **ACC** & **MCC** & **ACC** & **MCC** & **ACC** & **MCC** \\   & CausalStock w/o TCD & 51.08 & 0.0102 & 51.48 & 0.0106 & 51.37 & 0.0102 \\  & CausalStock w/o news & 58.10 & 0.1421 & 53.16 & 0.0375 & 54.16 & 0.1264 \\  & CausalStock w/o link non-existence & 58.21 & 0.1652 & 52.32 & 0.0241 & 53.96 & 0.0670 \\  & CausalStock w/o lag-dependent TCD & 59.19 & 0.1757 & 52.93 & 0.0312 & 54.97 & 0.1298 \\  & CausalStock with Variable-dependent TCD & **63.50** & **0.2175** & 54.60 & 0.0479 & **56.25** & **0.1419** \\   & CausalStock with Glove+Bi-GRU & 60.78 & 0.1952 & 53.87 & 0.0467 & 55.13 & 0.1326 \\  & CausalStock with Bert & 61.74 & 0.2067 & 53.92 & 0.0472 & 55.543 & 0.1352 \\  & CausalStock with Roberta & 61.81 & 0.2071 & 54.06 & 0.0477 & 55.58 & 0.1364 \\  & CausalStock with FinBert & 61.72 & 0.2062 & 54.01 & 0.0471 & 55.61 & 0.1362 \\  & CausalStock with FinGPT & 61.69 & 0.2060 & 54.00 & 0.0470 & 55.60 & 0.1360 \\  & CausalStock with Llama & 62.0 & 0.21310 & 54.40 & 0.0480 & 55.85 & 0.1390 \\   & CausalStock with FinGPT & 61.92 & 0.2105 & 54.30 & 0.0475 & 55.67 & 0.1386 \\  & CausalStock with Llama & 62.82 & 0.2164 & 54.52 & **0.0483** & 55.97 & 0.1406 \\   & CausalStock (with GPT-3.5) & 63.42 & 0.2172 & **54.64** & 0.0481 & 56.19 & 0.1417 \\   

Table 2: Ablation study results on different datasets.

various industries, we select companies with the highest and lowest market value. The top half of Figure 3(b) represents stocks corresponding to the nine companies with the largest market value, while the bottom half illustrates stocks from companies with the smallest. The causal strength of stocks is determined based on the average overall lags. In this heatmap, we could observe that distinct patterns emerge according to different market values. Stocks of low-market-value companies appear to have less pronounced causal relationships. We could also observe causal connections between certain high and low-market-value stocks. This is attributable to the dominant roles of large-value companies with their significant impact on those small-value firms and the stock prices.

Based on these observations, we compute the Spearman's rank correlation coefficient  between the aforementioned company's market value and their stock's causal strength on ACL18, CMIN-CN, NI225, and FSTE100 datasets, representing the US, Chinese, Japanese, and UK stock markets respectively. The correlation results are shown in Appendix D and we also visualize some results in Figure 3(a). These results show a strong positive correlation between the market value and causal influence. This aligns with the intuition that not only do large-value companies hold pivotal economic positions, but also play crucial roles in influencing other companies. Our findings demonstrate that CausalStock does well in uncovering the causal relations within the stock market.

### Investment simulation

Following prior works [44; 23], we evaluate CausalStock's applicability to the real world trading scenario. We conduct a portfolio strategy by choosing the top three stocks (based on predicted probabilities) with equal weight on each day of the test set and calculate the Accumulated Portfolio Value (APV) and Sharpe Ratio (SR) for evaluation. See Appendix C.2 for a metrics details. The results on three datasets are shown in Table 4, which indicates that CausalStock achieves higher profits, and the excellent capabilities of CausalStock to balance risk with returns.

## 6 Conclusion

In this paper, we propose a novel news-driven multi-stock movement prediction framework called CausalStock. We design a lag-dependent temporal causal discovery mechanism to uncover the causal relations among the stocks. Then the functional causal model is employed to encapsulate causal relations and predict future movements. The effectiveness of CausalStock is demonstrated by experiments on multiple real-world datasets. Moreover, CausalStock could offer a clear prediction process with explainability.

Figure 4: Investment simulation results.

Figure 3: (a) Correlation visualization between market value and causal strength for the top 20 companies of UK and Chinese markets. (b) Partial causal strength matrix visualization for ACL18, encompassing the companies with the highest and lowest market values across various industries. Each matrix entry indicates causal strength between stocks, with darker shades signifying stronger causality. (c) Examples of denoised news encoder module output.