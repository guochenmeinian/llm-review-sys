# SubjECTive-QA: Measuring Subjectivity in Earnings Call Transcripts' QA Through Six-Dimensional Feature Analysis

SubjECTive-QA: Measuring Subjectivity in Earnings Call Transcripts' QA Through Six-Dimensional Feature Analysis

 Huzaifa Pardawala*, Siddhant Sukhani*, Agam Shah*, Veer Kejriwal, Abhishek Pillai, Rohan Bhasin, Andrew DiBiasio, Tarun Mandapati, Dhruv Adha, Sudheer Chava

Georgia Institute of Technology

Corresponding Authors: {hpardawala3, ssukhani3, ashah482}@gatech.edu

* Indicates equal contribution

###### Abstract

Fact-checking is extensively studied in the context of misinformation and disinformation, addressing objective inaccuracies. However, a softer form of misinformation involves responses that are factually correct but lack certain features such as clarity and relevance. This challenge is prevalent in formal Question-Answer (QA) settings such as press conferences in finance, politics, sports, and other domains, where subjective answers can obscure transparency. Despite this, there is a lack of manually annotated datasets for subjective features across multiple dimensions. To address this gap, we introduce SubjECTive-QA, a human annotated dataset on Earnings Call Transcripts' (ECTs) QA sessions as the answers given by company representatives are often open to subjective interpretations and scrutiny. The dataset includes \(49,446\) annotations for long-form QA pairs across six features: Assertive, Cautions, Optimistic, Specific, Clear, and Relevant. These features are carefully selected to encompass the key attributes that reflect the tone of the answers provided during QA sessions across different domains. Our findings are that the best-performing Pre-trained Language Model (PLM), RoBERTa-base, has similar weighted F1 scores to Llama-3-70b-Chat on features with lower subjectivity, such as Relevant and Clear, with a mean difference of \(2.17\)% in their weighted F1 scores. The models perform significantly better on features with higher subjectivity, such as Specific and Assertive, with a mean difference of \(10.01\)% in their weighted F1 scores. Furthermore, testing SubjECTive-QA's generalizability using QAs from White House Press Briefings and Gaggles yields an average weighted F1 score of \(65.97\)% using our best models for each feature, demonstrating broader applicability beyond the financial domain. SubjECTive-QA is publicly available under the CC BY 4.0 license1.

## 1 Introduction

Earnings Calls (ECs) and their linguistic nuances serve as a vital communication channel between company exectives and investors, offering insights into a company's performance and future outlook . The long-form Question and Answer (QA) sessions of these calls are particularly significant as they provide unscripted interactions that reveal executives' confidence and strategic clarity. Unlike the scripted presentations in the beginning of ECs, the dynamic nature of QA sessions invites real-time scrutiny (Matsumoto et al., 2011) and deeper analysis (Alhamzeh et al., 2022) by analysts and investors as seen in figure 1. Linguistic nuances like tone and sentiment often predict abnormal returns more effectively than the actual earnings surprises disclosed (Price et al., 2012). Traditional approaches to gauging business subjectivity often rely on indices which measures small business sentiment through survey responses that reflect managers perceptions and expectations about economic conditions. (Zorio-Grima and Merello, 2020)

Traditionally analyzed for financial insights, the dynamic nature of these QA interactions have broader applications across various domains including but not limited to presidential debates, journalism, and sports conferences, where the manner of information delivery is as critical as the content itself. Additionally, increasing amount of misinformation is not only about outright falsehoods but also about subtly misleading answers; these answers can be technically true but misleading or irrelevant, a challenge highlighted in a recent study by Li et al. (2021).

The jargon-heavy nature of ECTs, often exceeding \(5,000\) words, poses a complexity for retail investors (Koval et al., 2023). The complexity and forward-looking statements within ECs underscore the need for specialized approaches in Financial Natural Language Processing (FinNLP) to effectively handle and interpret this voluminous and nuanced information. Existing FinNLP datasets derived from ECT data predominantly focus on sentiment classification, stock price prediction (Medya et al., 2022), summarization (Mukherjee et al., 2022), and objective annotation of financial statements, overlooking the subjective nuances embedded within the QA exchanges.

Recognizing these gaps, our paper introduces SubjECTive-QA, a pioneering dataset that enriches the financiala domain. This dataset provides subjectively annotated responses from EC long-form QA sessions, with an average QA length of nearly 186 words, covering 120 ECTs of companies listed on the New York Stock Exchange from 2007 to 2021. Unlike traditional datasets that either quantify sentiment or dissect financial statements into objectively verifiable claims (Maia et al., 2018), SubjECTive-QA delves into the multifaceted nature of answers, offering a novel lens through which financial discourse can be evaluated. The meticulous annotation of these transcripts with a six-label subjective feature rating system aids in capturing the dimensions of clarity, assertiveness, cautiousness, optimism, specificity, and relevance. Our aim is to provide a comprehensive resource that transcends traditional sentiment analysis. The statistical details of SubjECTive-QA are illuminated in table 1.

Furthermore, our dataset and methodology extend beyond the financial domain, addressing the need for robust subjectivity and misinformation detection tools applicable in various domains such as elections, journalism, sports, and public policy. QA sessions are prevalent in these areas, where the quality and clarity of responses significantly impact decision-making and public perception. As shown in Appendix N, we applied our models to White House Press Briefings (The White House, 2024), a setting where transparency and caution are paramount. Our analysis of QA pairs from White House Press Briefings and Gaggles demonstrates the utility of our models in a political con

  
**Metric** & **Value** \\  Dataset size & \(35,711\) \\ Total QA Pairs & \(2,747\) \\ Total Features & \(6\) \\ Total Metadata columns & \(7\) \\ Total Annotations & \(49,446\) \\ Avg. Question Length & \(59.87\) words \\ Avg. Answer Length & \(127.15\) words \\ Unique Questioners & \(756\) \\ Unique Responders & \(305\) \\   

Table 1: Overview of the SubjECTive-QA dataset.

Figure 1: An example of misinformation being present within question answer pairs of ECTs which is taken from the ECT of SWN in 2012 quarter 3.

text. These findings underscore SubjECTive-QA's effectiveness in capturing the nuanced subjective information required in such high-stakes environments. As we delve deeper into the application and evaluation of various Natural Language Processing (NLP) models on the SubjECTive-QA dataset, it is imperative to understand how these models perform in capturing the specific features identified.

In our benchmarking efforts, various NLP models are evaluated on the SubjECTive-QA dataset to measure their effectiveness in capturing these features. While general-purpose models like BERT base (uncased)  and RoBERTa-base  perform well, the results underscore the importance of domain-specific models like FinBERT-tone  which shows higher accuracy in certain features. This work hence not only advances FinNLP but also sets a precedent for broader applications in detecting subjectivity and misinformation in diverse QA contexts.

We aim to contribute significantly to the fields of QA session analysis and NLP by enabling researchers and practitioners to use our dataset as a valuable resource to assess the quality of information across multiple domains.

## 2 Features in SubjECTive-QA

SubjECTive-QA consists of six features for analyzing the quality of speech of the respondent. These features and their definitions are given in table 2. This process was initialised with an LLM-guided approach: passing each QA pair to the PaLM \(2\) API , to obtain the \(10\) most prevalent properties demonstrated by the answer for that particular question. This approach is elaborated in detail in Appendix 1.

The final 6 chosen features were also seen to have a significant impact on almost all QA pairs as per visual inspection over the corpus of our data and the reason for choosing each specific feature can be seen in 2. All the features were shown to be independent after annotation as seen in figure 4. This independence criterion is paramount as it ensures that potential classifiers could be fine-tuned to focus on any one of the 6 features without having to account for the others.

 p{113.8pt} p{113.8pt}}  
**Feature** & **Description** & **Justification of choice** \\  Relevant & The speaker has answered the question with appropriate details. & In a formal environment such as during an EC, relevant answers indicate the speaker addresses concerns directly. Irrelevant answers would lead to poor communication and potential misunderstandings about the company’s strategy or performance. \\ Clear & The speaker is transparent in the answer and about the message to be conveyed. & Clarity is crucial in formal environments. It ensures that the speaker’s message is well understood and transparent, which is often expected in environments like an EC. \\ Optimistic & The speaker answers with a positive outlook regarding future outcomes. & Optimism signals expectations of better future results and performance, indicating the company anticipates favorable tailwinds. \\ Specific & The speaker includes sufficient and technical details in the answer. & Specific answers demonstrate technical and statistical accuracy, which is important in ensuring transparency and reliability in an EC. \\ Cautious & The speaker answers using a more conservative, risk-averse approach. & Cautiousness can indicate defensiveness or a lack of conviction in the company’s future, but it may also reflect a prudent, risk-averse mentality. \\ Assertive & The speaker answers with certainty about the company’s events and outcomes. & Assertiveness shows the competence and reliability of the speaker and the firm. High assertiveness can demonstrate persuasive abilities and trustworthiness. \\   

Table 2: Feature descriptions utilized within SubjECTive-QA, explaining the definitions used for annotation purposes as well as the reason for choosing these features.

[MISSING_PAGE_FAIL:4]

dataset with new industry labels. However, this dataset still contained a significant amount of noise in terms of verbal-filler QA pairs such as salutations, formal introductions and irrelevant text as illustrated with examples in table 8 in Appendix H.1.

Data CleaningIn order to remove verbal filler content within our dataset without losing valuable data, we employed a manual cleaning process. This involved the authors going through each collected QA record to filter the data to remove filler content. Additionally, in case a questioner asked a question and there were multiple respondents who answered the questions subsequently, each of these answers and the respondents were mapped to the same question and questioner, establishing new, individual records. An example of the data cleaning process can be found in table 8 in Appendix H.1. After cleaning up the data, we began the manual annotation phase with our team of annotators.

### Annotations

AnnotatorsThe last step was the manual annotation of each QA pair across the 6 features mentioned in 11. Each ECT was randomly assigned to three annotators. For each ECT, the annotators remained anonymous to one another. The team of annotators comprised nine people whose details are outlined in table 12 in Appendix M.1.

Annotation GuidelineThis paper employed Microsoft Excel for the annotation procedure and figure 3 illustrates the manual annotation process. The annotators were asked to strictly adhere to the following annotation guidelines:

Give the answer a rating of:

* \(2\): If the answer positively demonstrates the chosen feature, with regards to the question.
* \(1\): If there is no evident relation between the question and the answer for the feature.

Figure 3: An example of the annotation process used while generating a rating for the Optimistic feature, indicating the reasons for choosing \(2\) as the rating.

* \(0\): If the answer negatively demonstrates the chosen feature with regards to the question.

At the end, the individual annotations were combined based on majority rating. In case there was no clear majority that particular rating was assigned the value '\(1\)'. A sample annotation is shown in Appendix G to make the annotation procedure clearer. We highlight several QA pairs in table 3 and detailed annotations for each QA pair in table 4 as a sample for our annotation work in Appendix G. The ethical considerations for our annotations are outlined in Appendix F.

Annotator AgreementThe annotator agreement metrics were calculated by obtaining the percentage of times the annotators completely agreed (all \(3\) annotators agree on the same rating), partially agreed (\(2\) of the annotators agree and \(1\) disagrees) and completely disagreed (all \(3\) annotators had different ratings). We obtained an aggregate percentage for the annotator agreement scores across all 6 features. \(48.94\%\) of the times the annotators completely agreed on a rating whereas \(45.18\%\) one of the annotators disagreed with the other two. Lastly, all \(3\) annotators disagreed only \(5.88\%\) of the times. The exact numbers for each feature are elaborated upon in Appendix M.2.

## 4 Dataset Analysis

### Independence Criterion

Upon looking at the correlation matrix in figure 4, a general independence of features can be seen with the range of the correlations being between \(-0.08\) and \(0.39\). As stated before and verified through this correlation matrix, no significant relationship exists between any of the features, indicating that the chosen features uniquely classify the behaviour of the respondent and therefore can be independently modelled in the future. It is important to note that this correlation matrix disregards sector-wise bias between different features.

### Rating Distribution

In order to measure the sector-wise bias and variable distributions of features across sectors, we utilized violin plots as seen in Appendix J as they allow for a compact representation of the kernel density and distribution of the data. Revealing specific asymmetries and skewness in various features across industries, these plots aided in the identification of specific behaviors and distributions of features across sectors.

When considering the spread of the ratings across the features, it can be seen that around \(90\%\) of answers were given a rating of \(2\) for Clear and Relevant, showing that most respondents answer questions in a cohesive manner that is contextually relevant. For answers with a rating of \(0\), the feature that had the highest number of zeroes was Specific with around a fifth of all QA pairs negatively demonstrating this feature, indicating the variance in the quality of answers as shown by its violin plots within Appendix J.

Further exploration into the distribution of the features Clear and Relevant supports the hypothesis that company representatives aim to be confident and on-topic with their violin plots being highly dense to the rating of \(2\); however, there is high variation in the Specific feature across industries,

Figure 4: A correlation matrix depicting the general independence of features utilized within SubjECTive-QA using pearson correlation.

suggesting a lack of technical details possibly to simplify information for a broader audience or to protect the company's reputation.

On the other hand, the violin plots demonstrate the telecommunications sector to be highly Optimistic with overwhelming positive responses and an overall buoyant industry sentiment. However, the respondents within this industry were highly Cautious as seen in its violin plots and this is apparent within all industries, displaying the conservative nature of the answers. Overall, a wide spectrum of densities across different features and industries demonstrate diversity in tones and attitudes, emphasising the multilayered complexity of the proposed dataset.

## 5 Benchmarking

Pre-trained Language Models (PLMs)To establish a performance benchmark, our study encompasses a range of transformer-based Pre-trained Language Models. We employ BERT base (uncased), FinBERT-tone (Huang et al., 2020), and RoBERTa-base. To avoid overfitting on financial text data, we refrain from pre-training any of the models before fine-tuning them. The task performed is sequence classification, minimizing cross-entropy loss. The experiments are conducted using PyTorch (Paszke et al., 2019) on an NVIDIA A40 GPU. Each model is initialized with the pre-trained version from the Transformers library provided by Huggingface (Wolf et al., 2020). We use varying hyperparameters and conduct multiple runs for each model using three seeds (\(5768,78516,944601\)), three batch sizes (\(32,16,8\)), and three learning rates (\(1e-4,1e-5,1e-6\)). Following this, we utilize a grid search strategy to find the best model for each feature. The ethical considerations while using these models are outlined in Appendix F.

Large Language Models (LLMs)Our study also encompasses four popular open-source LLMs: Llama-3-70b-Chat (Dubey et al., 2024), LLama-3-8b-Chat (Dubey et al., 2024), Mixtral-8x22B (141B)(Jiang et al., 2024), and Mixtral-8x7B (46.7B), and one closed-source LLM: GPT-4o-06-08-204 (OpenAI et al., 2024). The hyperparameters for these models were as follows: max_tokens: 512, temperature: 0.0, repetition_penalty: \(1.1\). To access these models and

Figure 5: F1 percentage scores across several LLMs (red) and PLMs (blue) trained on SubjECTive-QA across all features as well as the error bars for the PLMs.

run the fine tuning code, we utilised together.ai API and we are thankful to them for providing us with free credits for the same. The ethical considerations while using these models are outlined in Appendix F.

### Results

As seen in Figure 5, all models had similar performance on the dataset. While Clear and Relevant features were identified correctly a larger proportion of the time, the models' evaluation of Assertive and Specific were not as accurate. For each feature, we observed different models performing better. Due to the independence of our features, we can use each model independently to evaluate a given feature. For Clear, BERT had the highest weighted F1 score of \(80.93\)%. For Optimistic and Assertive, RoBERTa-base had the highest weighted F1 scores of \(62.69\)% and \(49.10\)%, respectively. For Relevant, the LLMs, Llama-3-70b-Chat and Mixtal-8x22B Instruct (141B), outperformed the Pre-trained Language Models (PLMs), with Llama-3-70b-Chat achieving the highest weighted F1 score of \(82.75\)%. For Specific, FinBERT had the highest weighted F1 score. This can be attributed to the fact that the other models are general-purpose models, whereas FinBERT is a domain-specific model for finance. For Cautious, BERT outperformed the other models with a weighted F1 score of \(60.66\)%. Across all six features, RoBERTa-base had the highest average weighted F1 score of \(63.95\)%. Mixtal-8x22B Instruct (141B) had a higher average weighted F1 score than Llama-3-70b-Chat.

The features Clear and Relevant were the easiest for models to identify, with BERT achieving the highest weighted F1 score of 80.93% for Clear and Llama-3-70b-Chat scoring 82.75% for Relevant. These features are more straightforward to detect as they rely on linguistic cues like coherence and topic alignment, making them accessible for general-purpose models. In contrast, detecting Assertive and Specific was more challenging. RoBERTa-base led in detecting Assertive with a score of 49.10%, while FinBERT excelled in identifying Specific, which relies on domain-specific technical details. These lower scores reflect the difficulty models face in capturing nuanced aspects such as tone and technicality.

Analysis of Model PerformanceFinBERTs higher performance for Specific emphasizes the value of domain-specific pre-training, as general-purpose models struggled with specialized financial terminology. The better performance on Clear and Relevant stems from their objective nature, as they rely on straightforward criteriawhether an answer is understandable and relevant to the question. However, detecting Assertive and Specific requires models to interpret subtle cues, making them harder to identify.

The performance discrepancies highlighted in this analysis open up important avenues for further research and development of models capable of handling subjective features more effectively. The challenges faced by current models in identifying nuances like assertiveness, cautiousness, and specificity suggest that standard pre-training on large corpora may not be sufficient for capturing complex human communication in high-stakes environments. Additionally, constructing richer training datasets with more nuanced annotation guidelines could help models learn to distinguish subtle variations in tone, sentiment, and technical specificity. This opens up opportunities to explore new architectures or techniques, such as reinforcement learning or attention mechanisms, that focus on capturing the intent and subjectivity behind language, thereby enhancing the models capacity to perform well in complex, subjective question and answer scenarios. Furthermore, a comparison of model latency, as explored by Shah and Chava (2023), could be an interesting direction for future work to assess the trade-offs between performance and efficiency.

### Transfer Learning Ablations

This study evaluates the transfer learning capabilities of the best-performing model, RoBERTa-base, originally trained and tested on the SubjECTive-QA dataset. Specifically, we investigate its performance when fine-tuned on the SubjECTive-QA dataset, followed by testing on \(65\) question-answer pairs from White House Press Briefings and Gaggles as outlined in Section N with the outcomes of these transfer learning experiments. The model achieved a mean weighted F1 score of \(65.97\)% across all the features, performing the best on Clear and the worst on Cautious. All the individual features' weighted F1 scores are outlined in Appendix N. This shows the broader applicability of the dataset across different significant domains such as Politics where clarity and transparency are of utmost importance.

## 6 Related Works

Subjective DatasetsRecent advancements in sentiment analysis such as Sy et al. (2023) have led to tailored tools and the creation of subjective datasets that have high potential within the financial domain. Many studies emphasize the importance of emotional information (Chen et al., 2023a) and linguistic extremity (Bochkay et al., 2020) on stock returns and investor opinions. The FinArg dataset curated by Alhamzeh et al. (2022) delves into argumentative sentiment while the General Numerical Attachment dataset generated by Shi et al. (2023) enhances numeral interpretation in ECTs, improving volatility forecasting. Similarly, Hiray et al. (2024) introduce CoCoHD, a dataset of U.S. congressional hearings, enabling sentiment and policy analysis on socio-economic issues, which complements our focus on multidimensional subjectivity in financial contexts. However, these datasets remain focused on a singular field, limiting their applicability across financial tasks. To optimize model performance, diverse data is crucial (Liang, 2016; Shah et al., 2022). While the mentioned datasets take a unidimensional approach, our method leverages the multidimensional nature of ECTs to better capture sentiment.

Sentiment Analysis and AnnotationsPrevious studies on the role of language in corporate reporting only take into account the tone of negative or positive words (TETLOCK, 2007; Loughran and McDonald, 2010) whilst our dataset focuses on a multidimensional analysis of \(6\) features. Most prior datasets also annotate single turn QA systems (Zhu et al., 2021; Qu et al., 2019; Li et al., 2022) without taking account the context of the question being asked (Deng et al., 2022). Furthermore general sentiment datasets such as Malo et al. (2014) and Sinha and Khandait (2020) lose accuracy because they annotate over large text (Tang et al., 2023). Our dataset aims to utilize the context of both questions and answers to augment our manual annotation process and incorporate a more nuanced annotation style to not lose accuracy.

Earnings CallsBased on their availability and the vast amount of information prevalent within them, ECTs proved to be a viable data source for our research. ECTs, hosted by publicly traded companies to discuss aspects of their earnings reports (Givoly and Lakonishok, 1980; Keith and Stent, 2019), remain to be a major form of communication that help investors to review their price targets and trade decisions (Frankel et al., 1999; Kimbrough, 2005; Matsumoto et al., 2011). Recently, Shah et al. (2024) proposed a novel framework for fine-tuning LLMs on earnings call transcripts, integrating both sentiment and financial performance features, a method that enhances predictive power for earnings surprises. Secondly, sentiment analysis within ECTs has historically been proven to possess a correlation to earning surprises (Price et al., 2012; Bowen et al., 2002; Doyle et al., 2012), providing quantitative value of analyzing subjectivity in ECTs. Finally, sentiment analysis, fine tuning of LLMs, and deep learning tactics on ECTs offer valuable insight to predict companies future earnings surprises (Koval et al., 2023; Larcker and Zakolyukina, 2012) and emotional reaction (Bochkay et al., 2020; Chava et al., 2022) with reasonable accuracy. Our dataset allows for NLP models to be fine tuned on the subjectivity of ECTs with the goal to be generally used on QA pairs in various fields of research.

Financial Domain QA DatasetsAppendix K provides a brief comparison of SubjECTive-QA with other datasets in the financial domain, focusing on the following attributes: size, number of features, list of labels, and license used. The datasets include TAT-QA (Zhu et al., 2021), a question-answering benchmark based on a hybrid of tabular and textual content in finance; FinQA (Chen et al., 2022), a dataset designed for numerical reasoning over financial data; FinArg (Alhamzeh et al., 2022), which annotates argument structures in earnings calls; TruthfulQA (Lin et al., 2022), which measures how models mimic human falsehoods; Trillion Dollar Words (Shah et al., 2023), which evaluates the meeting minute sentences of the federal reserve of the United States; ConvFinQA (Chen et al., 2022), which explores chains of numerical reasoning in conversational financial question answering; and MathQA (Amini et al., 2019), a dataset focused on interpretable math word problems.

## 7 Limitations and Future Work

Earnings Calls SampledOur dataset of Earnings Calls only encompasses companies listed in the New York Stock Exchange from \(2007\) to \(2021\) balanced across the 6 major industries defined in Appendix H.1. Insights from this dataset may not be applicable for Earnings Calls of companies from other countries or years. We plan to extend our research to other years and countries and test the broader applicability of our models.

Manual AnnotationsThe dataset of manual annotations was curated by the authors provided in the table 12 in Appendix M.1. As these annotations are subjective by definition, the dataset reflects a specific viewpoint and degree of financial knowledge given by the annotators' backgrounds. However, the subjectivity of the annotations presents itself as an interesting area for future work: analyzing perception of the subjective features in communication.

Written TranscriptsOur work uses written transcripts of ECs rather than the original audio. As a result, some aspects such as pitch, intonation, and tone that may be clear in an audio extract will not be reflected in the presented dataset (Sawhney et al., 2020). The feature annotations may not demonstrate the same insights that an investor would discern through listening to an EC audio recording.

## 8 Discussion

SubjECTive-QA offers the first dataset of long form QA pairs annotated across six features. The dataset consists of \(2,747\) QA pairs taken from \(120\) Earnings Call Transcripts annotated on six features: Clear, Assertive, Cautious, Optimistic, Specific, and Relevant. The goal of SubjECTive-QA is to serve as a resource for further research into the intersection between language and financial markets. Rather than solely focusing on the quantitative information within the Earnings Calls, measuring the various features present within QA pairs provides another dimension to analyze the effect of ECs on market dynamics. This paper defines the creation of SubjECTive-QA and examines introductory analysis into the distribution of our manual annotations. We believe that SubjECTive-QA can be a valuable resource for further exploration into the impact of ECs on financial markets and the FinNLP domain at large.

Broader Impact:By capturing the intricate nuances of speech, our subjective dataset also lays the foundation for a new approach to identifying disinformation and misinformation. Conventional detection methods often fail to recognize its subtle linguistic cues, so our findings will prove vital. SubjECTive-QA therefore has applications beyond the field of FinNLP in various fields such as sports, news and politics to identify misinformation and disinformation. Through systematic analysis and refinement of the specifics of this dataset, researchers can develop algorithms capable of discerning various forms of disinformation, thereby advancing the field's ability to combat deceptive narratives effectively.