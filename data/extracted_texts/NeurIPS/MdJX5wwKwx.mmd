# Optimization of Inter-group Criteria for Clustering

with Minimum Size Constraints

 Eduardo S. Laber

Department of Computer Science

PUC-Rio

Rio de Janeiro, RJ - Brazil

laber@inf.puc-rio.br &Lucas Murtinho

Department of Computer Science

PUC-Rio

Rio de Janeiro, RJ - Brazil

lmurtinho@aluno.puc-rio.br

###### Abstract

Internal measures that are used to assess the quality of a clustering usually take into account intra-group and/or inter-group criteria. There are many papers in the literature that propose algorithms with provable approximation guarantees for optimizing the former. However, the optimization of inter-group criteria is much less understood.

Here, we contribute to the state-of-the-art of this literature by devising algorithms with provable guarantees for the maximization of two natural inter-group criteria, namely the minimum spacing and the minimum spanning tree spacing. The former is the minimum distance between points in different groups while the latter captures separability through the cost of the minimum spanning tree that connects all groups. We obtain results for both the unrestricted case, in which no constraint on the clusters is imposed, and for the constrained case where each group is required to have a minimum number of points. Our constraint is motivated by the fact that the popular single-linkage, which optimizes both criteria in the unrestricted case, produces clusterings with many tiny groups.

To complement our work, we present an empirical study with 10 real datasets, providing evidence that our methods work very well in practical settings.

## 1 Introduction

Data clustering is a fundamental tool in machine learning that is commonly used in exploratory analysis and to reduce the computational resources required to handle large datasets. For comprehensive descriptions of different clustering methods and their applications, we refer to Jain et al. (1999); Hennig et al. (2015). In general, clustering is the problem of partitioning a set of items so that similar items are grouped together and dissimilar items are separated. Internal measures that are used to assess the quality of a clustering (e.g. Silhouette coefficient Rousseeuw (1987) and Davies-Bouldin index Davies & Bouldin (1979)) usually take into account intra-group and inter-group criteria. The former considers the cohesion of a group while the latter measures how separated the groups are.

There are many papers in the literature that propose algorithms with provable approximation guarantees for optimizing intra-group criteria. Algorithms for \(k\)-center, \(k\)-medians and \(k\)-means cost functions Gonzalez (1985); Charikar et al. (2002); Ahmadian et al. (2020) are some examples. However, optimizing inter-group criteria is much less understood. Here, we contribute to the state-of-the-art by considering the maximization of two natural inter-group criteria, namely the minimum spacing and the minimum spanning tree spacing.

**Our Results.** The spacing between two groups of points, formalized in Section 2, is the minimum distance between a point in the first and a point in the second group. We consider two criteria forcapturing the inter-group distance of a clustering, the minimum spacing (Min-Sp) and minimum spanning tree spacing (MST-Sp). The former is given by the spacing of the two groups with the smallest spacing in the clustering. The latter, as the name suggests, measures the separability of a clustering according to the cost of the minimum spanning tree (MST) that connects its groups; the largest the cost, the most separated the clustering is.

We first show that single-linkage, a procedure for building hierarchical clustering, produces a clustering that maximizes the MST spacing. This result contributes to a better understanding of this very popular algorithm since the only provable guarantee that we are aware of (in terms of optimizing some natural cost function) is that it maximizes the minimum spacing of a clustering Kleinberg and Tardos (2006)[Chap 4.7]. Our guarantee [Theorem 3.3] is stronger than the existing one in the sense that any clustering that maximizes the MST spacing also maximizes the minimum spacing. Figure 1 shows an example where the minimum spacing criterion does not characterize well the behavior of single-linkage.

Despite its nice properties, single-linkage tends to build clusterings with very small groups, which may be undesirable in practice. This can be seen in our experiments (see Figure 2) and is related to the well-documented fact that single-linkage suffers from the chaining effect Jain et al. (1999)[Chap 3.2].

To mitigate this problem we consider the optimization of the aforementioned criteria under size constraints. Let \(L\) be a given positive integer that determines the minimum size that every group in a clustering should have. A \((k,L)-\)clustering is a clustering with \(k\) groups in which the smallest group has at least \(L\) elements. For the Min-Sp criterion we devise an algorithm that builds clustering with at least \(L(1-)\) points per group while guaranteeing that its minimum spacing is not smaller than that of an optimal \((k,L)\)-clustering. This result is the best possible in the sense that, unless \(P=NP\), it is not possible to obtain an approximation with subpolynomial factor for the case in which the clustering is required to satisfy the hard constraint of \(L\) points per group. For the MST-Sp criterion we also devise an algorithm with provable guarantees. It produces a clustering whose MST-Sp is at most a \( k\) factor from the optimal one and whose groups have each at least \( L(1-)/2\) points, where \(\) is a number in the interval \(\) that depends on the ratio \(n/kL\). We also prove that the maximization of this criterion is APX-Hard for any fixed \(k\).

We complement our investigation with an experimental study where we compare the behaviour of the clustering produced by our proposed algorithms with those produced by \(k\)-means and single-linkage for 10 real datasets. Our algorithms, as expected, present better results than \(k\)-means for the criteria under consideration while avoiding the issue of small groups observed for single-linkage.

**Related Work.** We are only aware of a few works that propose clustering algorithms with provable guarantees for optimizing separability (inter-group) criteria. We explain some of them.

The maximum \(k\)-cut problem is a widely studied problem in the combinatorial optimization community and its solution can be naturally viewed as a clustering that optimizes a separability criterion. Given an edge-weighted graph \(G\) and an integer \(k 2\), the maximum \(k\)-cut problem consists of finding a \(k\)-cut (partition of the vertices of the graph into \(k\) groups) with maximum weight, where the weight of a cut is given by the sum of the weights of the edges that have vertices in different groups of the cut. The weight of the \(k\)-cut can be seen as a separability criterion, where the distance between two groups is given by the sum of the pairwise distances of its points. It is well-known that a random

Figure 1: Partitions with 3 groups (defined by colors) that maximize the minimum spacing. The rightmost one is built by single-linkage, but both of them maximize the minimum spacing – showing that this condition alone is insufficient to properly characterize single-linkage’s behavior.

assignment of the vertices (points) yields an \((1-1/k)\)-approximation algorithm. This bound can be slightly improved using semi-definitive programming Frieze & Jerrum (1997).

The minimum spacing, one of the criteria we studied here, also admits an algorithm with provable guarantees. In fact, as we already mentioned, it can be maximized in polynomial time via the single-linkage algorithm Kleinberg & Tardos (2006)[Chap 4.7]. In what follows, we discuss works that study this algorithm as well as works that study problems and/or methods related to it.

single-linkage has been the subject of a number of researches (Zahn, 1971; Kleinberg, 2002; Carlsson & Memoli, 2010; Hofmeyr, 2020). Kleinberg (2002) presents an axiomatic study of clustering, the main result of which is a proof that it is not possible to define clustering functions that simultaneously satisfy three desirable properties introduced in the paper. However, it was shown that by choosing a proper stopping rule for the single-linkage it satisfies any two of the three properties. Carlsson & Memoli (2010) replaces the property of Kleinberg (2002) that the clustering function must output a partition with the property that it must generate a tree (dendogram). Then, it was established that single-linkage satisfies the new set of properties. In more recent work, Hofmeyr (2020) establishes a connection between minimum spacing and spectral clustering. While the aforementioned works prove that single-linkage has important properties, in practice, it is reported that sometimes it presents poor performance due to the so-called chaining effect (Jain et al. (1999)[Chap 3.2]).

single-linkage belongs to the family of algorithms that are used to build Hierarchical Agglomerative Clustering (HAC). Dasgupta (2016) frames the problem of building a hierarchical clustering as a combinatorial optimization problem, where the goal is to output a hierarchy/tree that minimizes a given cost function. The paper proposes a cost function that has desirable properties and an algorithm to optimize it. This result was improved and extended by a series of papers (Roy & Pokutta, 2016; Charikar & Chatziafratis, 2017; Cohen-Addad et al., 2019). The algorithms discussed in these papers do not seem to be employed in practice. Recently, there has been some effort to analyse HAC algorithms that are popular in practice, such as the Average Link (Moseley & Wang, 2017; Cohen-Addad et al., 2019). Our investigation of single-linkage can be naturally connected with this line of research.

Finally, in a recent work, Ahmadi et al. (2022) studied the notion of individual preference stability (IP-stability) in which a point is IP-stable if it is closer to its group (on average) than to any other group (on average). The clustering produced by single-linkage presents a kind of individual preference stability in the sense that each point is closer to some point in its group than to any point in some other group.

**Potential Applications.** We discuss two cases in which the maximization of inter-group criteria via our algorithms may be relevant: ensuring data diversity when training machine learning algorithms and population diversity in candidate solutions for genetic algorithms.

When training a machine learning model, ensuring data diversity may be crucial for achieving good results (Gong et al., 2019). In situations in which all available data cannot be used for training (e.g. training in the cloud with budget constraints), it is important to have a method for selecting a diverse subset of the data, and our algorithms can be used for this: to select \(n=kL\) elements, one can partition the full data set into \(k\) clusters, all of them containing at least \(L\) members, and then select \(L\) elements from each cluster. Using single-linkage to create \(kL\) groups and then picking up one element per group is also a possibility but it would increase the probability of an over-representation of outliers in the obtained subset, as these outliers would likely be clustered as singletons (see Figure 2 in Section 5).

Note that our algorithms can be used to create not only one but several diverse and disjoint subsets, which might be relevant to generate partitions for cross-validation or for evaluating a model's robustness. For that, each subset is obtained by picking exactly one point per group.

For genetic algorithms, maintaining diversity over the iterations is important to ensure a good exploration of the search space (Gupta & Ghafir, 2012). If all candidate solutions become too similar, the algorithm will become too dependent on mutations for improvement, as the offspring of two solutions will likely be similar to its two parents; and mutation alone may not be enough to fully explore the search space. We can apply our algorithms in a similar manner as mentioned above, by partitioning, at each iteration, the solutions into \(k\) clusters of minimum size \(L\) and selecting the best \(L\) solutions from each cluster, according to the objective function of the underlying optimization problem, to maintain the solution population simultaneously optimized and diverse. Using single-linkage could lead to several poor solutions being selected to remain in the population, in case they are clustered as singletons.

We finally note that an algorithm that optimizes intra-group measures (e.g. \(k\)-means or \(k\)-medians) would not necessarily guarantee diversity for the aforementioned applications, as points from different groups can be close to each other.

## 2 Preliminaries

Let \(\) be a set of \(n\) points and let \(:^{+}\) be a distance function that maps every pair of points in \(\) into a non-negative real.

Given a \(k\)-clustering \(=(C_{1},,C_{k})\), we define the spacing between two distinct groups \(C_{i}\) and \(C_{j}\) of \(\) as

\[(C_{i},C_{j}):=_{x C_{i},y C_{j}}\{(x,y )\}.\]

Then, the minimum spacing of \(\) is given by

\[():=_{,C_{j} C}{i j}}\{(C_{i},C_{j})\}\]

A \(k\)-clustering \(\) induces on \((,)\) an edge-weighted complete graph \(G_{}\) whose vertices are the groups of \(\) and the weight \(w(e_{ij})\) of the edge \(e_{ij}\) between groups \(C_{i}\) and \(C_{j}\) is given by \(w(e_{ij})=(C_{i},C_{j})\). For a set of edges \(S G_{}\) we define \(w(S):=_{e S}w(e)\).

The _minimum spanning tree spacing_ of \(\) (\(()\)) is defined as the sum of the weights of the edges of the minimum spanning tree (\((G_{})\)) for \(G_{}\). In formulae,

\[():=_{e MST(G_{} )}w(e).\]

Here, we will be interested in the problems of finding partitions with maximum Min-Sp and maximum MST-Sp both in the unrestricted case in which no constraint on the groups is imposed and in the constrained case where each group is required to have at least \(L\) points.

**Single-Linkage**. We briefly explain single-linkage. The algorithm starts with \(n\) groups, each of them consisting of a point in \(\). Then, at each iteration, it merges the two groups with minimum spacing into a new group. Thus, by the end of the iteration \(n-k\) it obtains a clustering with \(k\) groups. In Kleinberg & Tardos (2006) it is proved that the single-linkage obtains a \(k\)-clustering with maximum minimum spacing.

**Theorem 2.1** (Kleinberg & Tardos (2006), chap 4.7).: _The single-linkage algorithm obtains the \(k\)-clustering with maximum Min-Sp for instance \((,)\)._

single-linkage and minimum spanning trees are closely related since the former can be seen as the Kruskal's algorithm for building MST's with an early stopping rule. To analyze our algorithms we make use of well-known properties of MST's as the cut property[Kleinberg & Tardos (2006), Property 4.17] and the cycle property[Kleinberg & Tardos (2006), Property 4.20]. Their statements can be found in the appendix.

For ease of presentation, we assume that all values of \(\) are distinct. We note, however, that our results hold if this assumption is dropped.

## 3 Relating Min-Sp and MST-Sp criteria

We show that single-linkage finds the clustering with maximum MST-Sp and the maximization of MST-Sp implies the maximization of Min-Sp. These results are a consequence of Lemma 3.1 that generalizes the result of Theorem 2.1. The proof of this lemma can be found in the appendix.

Fix an instance \(I=(,,k)\). In what follows, \(_{SL}\) is a \(k\)-clustering obtained by single-linkage for instance \(I\) and \(T_{SL}\) is a MST for \(G_{_{SL}}\). Moreover, \(w_{i}^{SL}\) is the weight of the \(i\)-th smallest weight of \(T_{SL}\).

**Lemma 3.1**.: _Let \(\) be a \(k\)-clustering for \(I\) and let \(w_{i}\) be the weight of the \(i\)-th smallest weight in a MST \(T\) for the graph \(G_{}\). Then, \(w_{i}^{SL} w_{i}\)._

**Theorem 3.2**.: _The clustering \(_{SL}\) returned by_ single-linkage _for instance \((,,k)\) maximizes the_ MST-Sp _criterion._

Proof.: Let \(\) be a \(k\)-clustering for \((,,k)\) and let \(w_{i}\) be the weight of the \(i\)-th cheapest edge of the MST for \(G_{}\). Since \(w_{i}^{SL} w_{i}\) for \(i=1,,k-1\), we have that

\[(_{SL})=_{i=1}^{k-1}w_{i}^{SL}_{i=1}^{k -1}w_{i}=()\]

**Theorem 3.3**.: _Let \(^{*}\) be a clustering that maximizes the_ MST-Sp _criterion for instance \((,,k)\). Then, it also maximizes_ Min-Sp _for this same instance._

Proof.: Let us assume that \(^{*}\) maximizes the MST-Sp  criterion but it does not maximize the Min-Sp criterion. Thus, \(w_{1}^{SL}>w_{1}^{*}\), where \(w_{1}^{*}\) is the minimum spacing of \(^{*}\). It follows from the previous lemma that

\[(_{SL})=_{i=1}^{k-1}w_{i}^{SL}>_{i=1}^{k-1}w _{i}^{*}=(^{*}),\]

which contradicts the assumption that \(^{*}\) maximizes the MST-Sp  criterion. 

The next example (in the spirit of Figure 1) shows that a partition that maximizes the Min-Sp criterion may have a poor result in terms of the MST-Sp  criterion.

**Example 3.4**.: _Let \(D\) be a positive number much larger than \(k\). Moreover, let \([t]\) be the set of the \(t\) first positive integers and \(S=\{(D i,j)|i,j[k-1]\}(D,k)\) be a set of \((k-1)^{2}+1\) points in \(^{2}\)._

\(\) _builds a \(k\)-clustering with_ Min-Sp _1 and_ MST-Sp\(1+(k-2)D\) _for \(S\)._

_However, the \(k\)-clustering \((C_{1},,C_{k})\), where \(C_{j}=\{(D i,j)|i=1,,k-1\}\), for \(j<k\) and \(C_{k}=\{(D,k)\}\) has_ Min-Sp _\(1\) and_ MST-Sp\(=(k-1)\)._

## 4 Avoiding small groups

In this section, we optimize our criteria under the constraint that all groups must have at least \(L\) points, where \(L\) is a positive integer not larger than \(n/k\) that is provided by the user. Note that the problem is not feasible if \(L>n/k\).

We say that an algorithm has \((,)\)-approximation for a criterion \(\{,\}\) if for all instances \(I\) it obtains a clustering \(_{I}\) such that \(_{I}\) is a \((k, L)\)-clustering and the value of \(\) for \(_{I}\) is at least \( OPT\), where \(OPT\) is the maximum possible value of \(\) that can be achieved for a \((k,L)\)-clustering.

We first show how to obtain a \((1-,1)\) for the Min-Sp  criterion.

### The Min-Sp  criterion

We start with the polynomial-time approximation scheme for the Min-Sp  criterion. Our method uses, as a subroutine, an algorithm for the max-min scheduling problem with identical machines Csirik et al. (1992); Woeginger (1997). Given \(m\) machines and a set of \(n\) jobs, with processing times \(p_{1},,p_{n}\), the problem consists of finding an assignment of jobs to the machines so that the load of the machine with minimum load is maximized. This problem admits a polynomial-time approximation scheme Woeginger (1997).

Let MaxMinSched\((P,k,)\) be a routine that implements this scheme. It receives as input a parameter \(>0\), an integer \(k\) and a list of numbers \(P\) (corresponding to processing times). Then, it returns a partition of \(P\) into \(k\) lists (corresponding to machines) such that the sum of the numbers of the listwith minimum sum is at least \((1-)OPT\), where \(OPT\) is the minimum load of a machine in an optimal solution of for the max-min scheduling when the list of processing times is \(P\) and the number of machines is \(k\).

Algorithm 1, as proved in the next theorem, obtains a \((k,L(1-))\)-clustering whose Min-Sp is at least the Min-Sp of an optimal \((k,L)\)-clustering. For that, it looks for the largest integer \(t\) for which the clustering \(_{t}\) obtained by executing \(t\) steps of single-linkage and then combining the resulting groups into \(k\) groups (via MaxMinSched) is a \((k,L(1-))\)-clustering. We assume that MaxMinSched, in addition of returning the partition of the sizes, also returns the group associated to each size.

``` \(t n-k\) while\(t 0\)do  Run t merging steps of the single-linkage for input \(\)  Let \(C_{1},,C_{n-t}\) the groups obtained by the end of the \(t\) steps \(P(|C_{1}|,,|C_{n-t}|)\) \(_{t}\)MaxMinSched\((P,k,)\) if the smallest group in \(A_{t}\) has size greater than or equal to \(L(1-)\)then  Return \(A_{t}\) else \(t t-1\) ```

**Algorithm 1**\((\); \(\); \(k\); \(>0\);\(L\))

**Theorem 4.1**.: _Fix \(>0\). The clustering \(_{t}\) returned by the Algorithm 1 is a \((k,(1-)L)\)-clustering that satisfies Min-Sp\((_{t})\) Min-Sp\((^{*})\), where \(^{*}\) is the \((k,L)\)-clustering with maximum Min-Sp._

Proof.: By design, \(_{t}\) has \(k\) groups with at least \(L(1-)\) points in each of them.

For the sake of contradiction, let us assume that Min-Sp\((_{t})<\) Min-Sp\((^{*})\).

Let \(=(C_{1},,C_{n-t})\) be the list of \(n-t\) groups obtained when \(t\) merging steps of single-linkage are performed. We assume w.l.o.g. that \(C_{1}\) and \(C_{2}\) are the two groups with a minimum spacing in this list, so that Min-Sp\(()=\) spacing\((C_{1},C_{2})\). Since \(_{t}\) is a \(k\)-clustering that is obtained by merging groups in \(\) we have Min-Sp\((_{t})\) Min-Sp\(()=\) spacing\((C_{1},C_{2})\).

For \(i=1,,n-t\) we have that \(C_{i} C\) for some \(C^{*}\), otherwise we would have Min-Sp\((_{t})\) Min-Sp\((^{*})\). In addition, we must have \(C_{1} C_{2} C\) for some \(C^{*}\), otherwise, again, we would have Min-Sp\((_{t})\) Min-Sp\((^{*})\).

We can conclude that there is a feasible solution with minimum load not smaller than \(L\) for the max-min scheduling problem with processing times \(P^{}=(|C_{1} C_{2}|,|C_{3}|,,|C_{n-t}|)\) and \(K\) machines. Thus, by running \(t+1\) steps of single-linkage followed by MaxMinSched\((P^{},k,)\), we would get a \(k\)-clustering whose smallest group has at least \(L(1-)\) points. This implies that the algorithm would have stopped after performing \(t+1\) merging steps, which is a contradiction. 

Algorithm 1, as presented, may run single-linkage\(n-k\) times, which may be quite expensive. Fortunately, it admits an implementation that runs single-linkage just once, and performs an inexpensive binary search to find a suitable \(t\).

The next theorem shows that Algorithm 1 has essentially tight guarantees under the hypothesis that \(P NP\). The proof can be found in the appendix

**Theorem 4.2**.: _Unless \(P=NP\), for any \(=poly(n)\), the problem of finding the \((k,L)\)-clustering that maximizes the Min-Sp criterion does not admit a \((1,)\)-approximation._

### The MST-Sp criterion

Now, we turn to the MST-Sp criterion. Let \(:=\{,2\}\). Our main contribution is Algorithm 2, it obtains a \((,})\) approximation for this criterion, where \(H_{k-1}=_{i=1}^{k-1}\) is the \((k-1)\)-th Harmonic number. Note that \(H_{k-1}\) is \(( k)\).

In high level, for each \(=2,,k\), the algorithm calls \(\) (Algorithm 1) to build a clustering \(^{}_{}\) with \(\) groups and then it transforms \(^{}_{}\) (lines 5-13) into a clustering \(_{}\) with \(k\) groups. In the end, it returns the clustering, among the \(k-1\) considered, with maximum MST-Sp.

```
1:for\(=2,,k\)do
2:\(^{}_{}(,\)dist\(,\)\(,\)\(,\)\(L\))
3:\(^{}_{}\)
4:\(_{}\)
5:for each \(A^{}\) in \(^{}_{}\), iterating from the largest group to the smallest do
6:\(\) - \(A^{}\)
7:\(|}{(1-)L}\)
8:if\(|_{}|\)+\(\)+\(<k\)then
9: Split\(A^{}\) into \(\) as balanced as possible groups and add them to \(_{}\)
10:else
11: Split \(A^{}\) into \(k-|_{}|\)-\(|\) as balanced as possible groups; add them to \(_{}\)
12: Add all groups in \(\) to \(_{}\)
13: Break
14:Return the clustering \(A_{}\), among the \(k-1\) obtained, that has the maximum MST-Sp ```

**Algorithm 2**Constrained-\((;\)dist\(;\)\(k\); \(L\); \(\))

We remark that we do not need to scan the groups in \(^{}_{}\) by non-increasing order of their sizes to establish our guarantees presented below. However, this rule tends to avoid groups with sizes smaller than \(L\).

**Lemma 4.3**.: _Fix \(>0\). Thus, for each \(\), every group in \(_{}\) has at least \(\) points._

Proof.: The groups that are added to \(_{}\) in line 12 have at least \((1-)L\) points while the number of points of those that are added at either line 11 or 9 is at least

\[|}{|2|A^{}|/(1-)L|} \]

Moreover, if the **For** is not interrupted by the **Break** command, the total number of groups in \(_{}\) is

\[_{A^{}^{}_{}}|}{(1-)L}_{A^{}^{} _{}}|}{(1-)L}--k-k k\]

Since the **For** is interrupted as soon as \(k\) groups can be obtained then, \(_{}\) has \(k\) groups. 

For the next results, we use \(^{*}\) to denote the \((k,L)\)-clustering with maximum MST-Sp  and \(w^{*}_{i}\) to denote the cost of the \(i\)-th cheapest edge in the MST for \(G_{^{*}}\). Our first lemma can be seen as a generalization of Theorem 4.1. Its proof can be found in the appendix.

**Lemma 4.4**.: _For each \(\), \((^{}_{}) w^{*}_{k-+1}\)._

A simple consequence of the previous lemma is that the MST-Sp  of clustering \(^{}_{}\) is at least \((-1) w^{*}_{k-+1}\). The next lemma shows that this bound also holds for the clustering \(_{}\). The proof consists of showing that each edge of a MST for \(^{}_{}\) is also an edge of a MST for \(_{}\).

**Lemma 4.5**.: _For each \(=2,,k\) we have_ MST-Sp\((_{})(-1) w^{*}_{k-+1}\)._

Proof.: Let \(T_{}\) and \(T^{}_{}\) be, respectively, the MST for \(G_{_{}}\) and \(G_{^{}_{}}\). By the previous lemma, each of the \((-1)\) edges of \(T^{}_{}\) has cost at least \((^{}_{}) w^{*}_{k-+1}\). Thus, to establish the result, it is enough to argue that each edge of \(T^{}_{}\) also belongs to \(T_{}\).

We say that a group \(A_{}\) is is generated from a group \(A^{}^{}_{}\) if \(A=A^{}\) or \(A\) is one of the balanced groups that is generated when \(A^{}\) is split in the internal **For** of Algorithm 2. We say that a vertex \(x\) in \(G_{_{}}\) is generated from a vertex \(x^{}\) in \(G_{^{}_{}}\) if the group corresponding to \(x\) is generated by the corresponding to \(x^{}\).

Let \(e^{}=u^{}v^{}\) be an edge in \(T^{}_{}\) and let \(S^{}\) be a cut in graph \(G_{^{}_{}}\) whose vertices are those from the connected component of \(T^{}_{} e^{}\) that includes \(u^{}\). We define the cut \(S\) of \(G_{_{}}\) as follows \(S=\{x G_{_{}}|x\) is generated from some \(x^{} S^{}\}\).

Let \(u\) and \(v\) be vertices generated from \(u^{}\) and \(v^{}\), respectively, that satisfy \(w(uv)=w(u^{}v^{})\). It is enough to show that \(uv\) is the cheapest edge that crosses \(S\) since by the cut property [Kleinberg & Tardos (2006), Property 4.17.] this implies that \(uv T_{}\). We prove it by contradiction. Let us assume that there is another edge \(f=yz\) that crosses \(S\) and has weight smaller than \(w(uv)\). Let \(y^{}\) and \(z^{}\) be vertices in \(G_{^{}_{}}\) that generate \(y\) and \(z\), respectively, and let \(f^{}=y^{}z^{}\). Thus, \(w(f^{}) w(f)<w(uv)=w(u^{}v^{})=w(e^{})\). However, this contradicts the cycle property of MST's [Kleinberg & Tardos (2006), Property 4.20] because it implies that the edge with the largest weight in the cycle of \(G_{^{}_{}}\) comprised by edge \(f^{}\) and the path in \(T^{}_{}\) the connects \(y^{}\) to \(z^{}\) belongs to the \(T^{}_{}\). 

The next theorem is the main result of this section.

**Theorem 4.6**.: _Fix \(>0\). Algorithm 2 is a \((,})\)-approximation for the problem of finding the \((k,L)\)-clustering that maximizes the MST-Sp criterion._

Proof.: Let \(\) be the clustering returned by Algorithm 2. Lemma 4.3 guarantees that \(\) is a \((k,)\)-clustering.

Thus, we just need to argue about MST-Sp\(()\). We have that

\[(^{*})=_{i=2}^{k}w^{*}_{k-i+1}\]

and, due to Lemma 4.5, \(()\{(-1) w^{*}_{k-+1}|2  k\}\).

Let \(\) be the value \(\) that maximizes \((-1) w^{*}_{k-+1}\). It follows that \(w^{*}_{k-i+1}((-1)/(i-1))w^{*}_{k-+1}\). for \(i=2,,k\). Thus,

\[(^{*})}{()} ^{k}w^{*}_{k-i+1}}{(-1) w^{*}_{k-+ 1}}-1) w^{*}_{k-+1}_{i=2}^{k }}{(-1) w^{*}_{k-+1}}=H_{k-1}\]

We end this section by showing that the optimization of MST-Sp  is APX-HARD (for fixed \(k\)) when a hard constraint on the number of points per group is imposed. The proof can be found in the appendix.

**Theorem 4.7**.: _Unless \(P=NP\), for any \(=poly(n)\), there is no (1,\(+\))-approximation for the problem of finding the \((k,L)\)-clustering that maximizes the MST-Sp criterion._

## 5 Experiments

To evaluate the performance of Algorithms 1 and 2, we ran experiments with 10 different datasets, comparing the results with those of single-linkage  and of the traditional \(k\)-means algorithm from Lloyd (1982) with a ++ initialization (Arthur & Vassilvitskii, 2007). For the implementation of routine MaxMinSched, employed by Algorithm 1, we used the Longest Processing Time rule. This rule has the advantage of being fast while guaranteeing a \(3/4\) approximation for the max-min scheduling problem (Csirik et al., 1992). The code for running the algorithms can be found at https://github.com/lmurtinho/SizeConstrainedSpacing.

Our first experiment investigates the size of the groups produced by single-linkage  for the 10 datasets, whose dimensions can be found in the first two columns of Table 1. Figure 2 shows the proportion of singletons for each dataset with the growth of \(k\). For all datasets but Vowel and Micethe majority of groups are singletons, even for small values of \(k\). This undesirable behavior motivates our constraint on the minimum size of a group.

In our second experiment, we compare the values of Min-Sp and MST-Sp achieved by our algorithms with those of \(k\)-means. While \(k\)-means is not a particularly strong competitor in the sense that it was not designed to optimize our criteria, the motivation to include it is that \(k\)-means is a very popular choice among practitioners. Moreover, for datasets with well-separated groups, the minimization of the squared sum of errors (pursued by \(k\)-means) should also imply the maximization of inter-group criteria.

Table 1 presents the results of this experiment. The values chosen for \(k\) are the numbers of classes (dependent variable) in the datasets, while for the dist function we employed the Euclidean distance. The values associated with the criteria are averages of 10 executions, with each execution corresponding to a different seed provided to \(k\)-means. To set the value of \(L\) for the \(i\)-th execution of our algorithms, we take the size of the smallest group generated by \(k\)-means for this execution and multiply it by \(4/3\). This way, we guarantee that the size of the smallest group produced by our methods, for each execution, is not smaller than that of \(k\)-means, which makes the comparison among the optimization criteria fairer.

With respect to the Min-Sp criterion, Algorithm 1 is at least as good as Algorithm 2 for every dataset (being superior on 6) and both Algorithm 1 and 2 outperform \(k\)-means on all datasets. On the other hand, with respect to the MST-Sp criterion, Algorithm 2 is at least as good as Algorithm 1 for every dataset (being better on 6) and, again, both algorithms outperform \(k\)-means for all datasets. In the appendix, we present additional information regarding this experiment. In particular, we show that

    & Dimensions &  &  \\  & n & k & Algo 1 & Algo 2 & k-means & Algo 1 & Algo 2 & k-means \\  anuran & 7,195 & 10 & **0.19** & 0.09 & 0.05 & 1.71 & **1.87** & 1.01 \\ avila & 20,867 & 12 & **0.07** & 0.04 & 0 & 0.77 & **0.81** & 0.66 \\ collins & 1,000 & 30 & **0.42** & **0.42** & 0.22 & **12.42** & **12.42** & 8.58 \\ digits & 1,797 & 10 & **19.74** & **19.74** & 13.79 & **178.22** & **178.22** & 145.13 \\ letter & 20,000 & 26 & **0.2** & 0.11 & 0.07 & 4.98 & **5.67** & 1.98 \\ mice & 552 & 8 & **0.79** & **0.79** & 0.24 & **5.66** & **5.66** & 2.37 \\ newsgroups & 18,846 & 20 & **1** & 1 & 0.17 & 19 & **19** & 8.4 \\ pendigits & 10,992 & 10 & **23.89** & 9.08 & 8.31 & 215.11 & **217.01** & 119.85 \\ sensorless & 58,509 & 11 & **0.13** & 0.08 & 0.03 & 1.31 & **1.36** & 1.29 \\ vowel & 990 & 11 & **0.49** & **0.49** & 0.11 & **4.94** & **4.94** & 1.84 \\   

Table 1: Min-Sp and MST-Sp for the different methods and datasets.

Figure 2: Proportion of singletons for each dataset with the growth of \(k\)

the MST-Sp achieved by Algorithm 2 is on average 81\(\%\) of the upper bound \(_{=2}^{k}(_{}^{})\) that follows from Lemma 4.4. This is much better than the \(1/H_{k-1}\) ratio given by the theoretical bound.

Finally, Table 2 shows the running time of our algorithms for the datasets that consumed more time. We observe that the overhead introduced by Algorithm 1 w.r.t. single-linkage is negligible while Algorithm 2, as expected, is more costly. In the appendix, we show that a strategy that only considers values of \(\) that can be written as \( k/2^{t}\), for \(t=0,, k\), in the first loop of Algorithm 2 provides a significant gain of running time while incurring a small loss in the MST-Sp. We note that the \( k\) bound of Theorem 4.6 is still valid for this strategy.

## 6 Final Remarks

We have endeavored in this paper to expand the current knowledge on clustering methods for optimizing inter-cluster criteria. We have proved that the well-known single-linkage produces partitions that maximize not only the minimum spacing between any two clusters, but also the MST spacing, a stronger guarantee. We have also studied the task of maximizing these criteria under the constraint that each group of the clustering has at least \(L\) points. We provided complexity results and algorithms with provable approximation guarantees.

One potential limitation of our proposed algorithms is their usage on massive datasets (in particular Algorithm 2) since they execute single-linkage one or many times. If the dist function is explicitly given, then the \((n^{2})\) time spent by single-linkage is unavoidable. However, if the distances can be calculated from the set of points \(\) then faster algorithms might be obtained.

The main theoretical question that remained open in our work is whether there exist constant approximation algorithms for the maximization of MST-Sp. In addition to addressing this question, interesting directions for future research include handling different inter-group measures as well as other constraints on the structure of clustering.