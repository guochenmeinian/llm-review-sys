# DynaMo: In-Domain Dynamics Pretraining

for Visuo-Motor Control

 Zichen Jeff Cui

Hengkai Pan

Aadhithya Iyer

Siddhant Haldar

Lerrel Pinto

New York University

Corresponding author. Email: jeff.cui@nyu.edu

###### Abstract

Imitation learning has proven to be a powerful tool for training complex visuomotor policies. However, current methods often require hundreds to thousands of expert demonstrations to handle high-dimensional visual observations. A key reason for this poor data efficiency is that visual representations are predominantly either pretrained on out-of-domain data or trained directly through a behavior cloning objective. In this work, we present DynaMo, a new in-domain, self-supervised method for learning visual representations. Given a set of expert demonstrations, we jointly learn a latent inverse dynamics model and a forward dynamics model over a sequence of image embeddings, predicting the next frame in latent space, without augmentations, contrastive sampling, or access to ground truth actions. Importantly, DynaMo does not require any out-of-domain data such as Internet datasets or cross-embodied datasets. On a suite of six simulated and real environments, we show that representations learned with DynaMo significantly improve downstream imitation learning performance over prior self-supervised learning objectives, and pretrained representations. Gains from using DynaMo hold across policy classes such as Behavior Transformer, Diffusion Policy, MLP, and nearest neighbors. Finally, we ablate over key components of DynaMo and measure its impact on downstream policy performance. Robot videos are best viewed at https://dynamo-ssl.github.io.

## 1 Introduction

Learning visuo-motor policies from human demonstrations is an exciting approach for training difficult control tasks in the real world . However, a key challenge in such a learning paradigm is to efficiently learn a policy with fewer expert demonstrations. To address this, prior works have focused on learning better visual representations, often by pretraining on large Internet-scale video datasets . However, as shown in Dasari et al. , these out-of-domain representations may not transfer to downstream tasks with very different embodiments and viewpoints from the pretraining dataset.

An alternative to using Internet-pretrained models is to train the visual representations 'in-domain' on the demonstration data collected to solve the task . However, in-domain datasets are much smaller than Internet-scale data. This has resulted in the use of domain-specific augmentations  to induce representational invariances with self-supervision or to collect larger amounts of demonstrations . The reliance of existing methods on large datasets might suggest that in-domain self-supervised pretraining is ineffective for visuo-motor control, and we might be better with simply training end-to-end. In this work, we argue the contrary - in-domain self-supervision can be effective with a better training objective that extracts more information from small datasets.

Prevalent approaches for using self-supervision in downstream control often make a bag-of-frames assumption, using contrastive methods [15; 16] or masked autoencoding [11; 8] on individual frames for self-supervision. Most of these approaches ignore a rich supervision signal: action-based causality. Future observations are dependent on past observations, and unobserved latent actions. Can we obtain a good visual representation for control by simply learning the dynamics? In fact, this idea is well-established in neuroscience: animals are thought to possess internal models of the motor apparatus and the environment that facilitate motor control and planning [17; 18; 19; 20; 21; 22; 23; 24].

In this work, we present **Dynamics** Pretraining for Visuo-**Motor** Control (**DynaMo**), a new self-supervised method for pretraining visual representations for visuomotor control from limited in-domain data. DynaMo jointly learns the encoder with inverse and forward dynamics models, without access to ground truth actions [25; 26].

To demonstrate the effectiveness of DynaMo, we evaluate our representation on four simulation suites - Franka Kitchen , Block Pushing , Push-T , and LIBERO , as well as eight robotic manipulation tasks on two real-world environments. Our main findings are summarized below:

1. DynaMo exhibits an overall \(39\%\) improvement in downstream policy performance over prior state-of-the-art pretrained and self-supervised representations, especially on the harder closed-loop control tasks in Block Pushing and Push-T (Table 1), and on real robot experiments (Table 2).
2. DynaMo is compatible with various policy classes, can be used to fine-tune pretrained weights, and works in the low-data regime with limited demonstrations on a real-world Allegro hand (Tables 4, 5, and 2 respectively).
3. Through an ablation analysis, we study the impact of each component in DynaMo on downstream policy performance (SS4.6).

Figure 1: (a) We present DynaMo, a new self-supervised method for learning visual representations for visuomotor control. DynaMo exploits the causal structure in demonstrations by jointly learning the encoder with inverse and forward dynamics models. DynaMo requires no augmentations, contrastive sampling, or access to ground truth actions. This enables downstream policy learning using limited in-domain data across simulated and real-world robotics tasks. For each environment, we pretrain the visual representation in-domain with DynaMo and learn a policy on the pretrained embeddings. (b) We provide real-world rollouts of policies learned with DynaMo representation on our multi-task xArm Kitchen and Allegro Manipulation environments.

All of our datasets, and training and evaluation code will be made publicly available. Videos of our trained policies can be seen here: https://dynamo-ssl.github.io.

## 2 Background

### Visual imitation learning

Our work follows the general framework for visual imitation learning. Given demonstration data \(=\{(o_{t},a_{t})\}_{t}\), where \(o_{t}\) are raw visual observations and \(a_{t}\) are the corresponding ground-truth actions, we first employ a visual encoder \(f_{}:o_{t} s_{t}\) to map the raw visual inputs to lower-dimensional embeddings \(s_{t}\). We then learn a policy \((a_{t}|s_{t})\) to predict the appropriate actions. For rollouts, we model the environment as a Markov Decision Process (MDP), where each subsequent observation \(o_{t+1}\) depends on the previous observation-action pair \((o_{t},a_{t})\). We assume the action-conditioned transition distribution \(p(o_{t+1}|o_{t},a_{t})\) to be unimodal for our manipulation tasks.

### Visual pretraining for policy learning

Our goal is to pretrain the visual encoder \(f_{}\) using a dataset of sequential raw visual observations \(=\{o_{t}\}_{t}\) to support downstream policy learning. During pretraining, we do not assume access to the ground-truth actions \(\{a_{t}\}_{t}\).

Prior work has shown that pretraining encoders on large out-of-domain datasets can improve downstream policy performance [6; 7; 8; 9; 10; 11]. However, such pretraining may not transfer well to tasks with different robot embodiments .

Alternatively, we can directly pretrain the encoder in-domain using self-supervised methods. One approach is contrastive learning with data augmentation priors, randomly augmenting an image twice and pushing their embeddings closer. Another approach is denoising methods, predicting the original image from a noise-degraded sample (e.g. by masking [11; 8; 30]). A third approach is contrastive learning with temporal proximity as supervision, pushing temporally close frames to have similar embeddings [31; 32].

## 3 DynaMo

Limitations of prior self-supervised techniques:Prior self-supervised techniques can learn to fixate on visually salient features and ignore fine-grained features important for control. We illustrate this limitation using the Block Pushing environment from Florence et al. . In this task, the goal is to push a block into a target square. While the robot arm occupies much of the raw pixel space, the blocks are central to the task despite being smaller in the visual field. Figure 2 visualizes a random frame from the demonstration data and its \(20\) nearest neighbors in the embedding space learned by several self-supervised techniques.

We observe that prior self-supervised methods (details in SS4.2) focus on the visually dominant robot, matching the whole robot arm extremely accurately. However, they fail to capture the block positions, which are important to the task despite being much less salient visually.

Can we learn a visual encoder that extracts task-specific features better? We know that the demonstrations are sequential: each observation is dependent on the previous observation, and an action (unobserved in this setting). Prior self-supervised methods ignore this sequential structure. Contrastive augmentations [16; 33] and autoencoding objectives [30; 8; 11] assume that the demonstration video is a bag of frames, discarding temporal information altogether. Temporal contrast [32; 31] uses temporal proximity but discards the sequential information in the observations: the contrastive objectives are usually symmetric in time, disregarding past/future order.

Instead of a contrastive or denoising objective, we propose a dynamics prediction objective that explicitly exploits the sequential structure of demonstration observations.

Overview of DynaMo:The key insight of our method is that we can learn a good visual representation for control by modeling the dynamics on demonstration observations, without requiring augmentations, contrastive sampling, or access to the ground truth actions. Given a sequence of raw visual observations \((o_{1},,o_{T})\), we jointly train the encoder \(f_{}:o_{t} s_{t}\), a latent inverse dynamics model \(q(z_{t:t+h-1}|s_{t:t+h})\), and a forward dynamics model \(p(_{t+1:t+h}|s_{t:t+h-1},z_{t:t+h-1})\). We model the actions as unobserved latents, and train all models end-to-end with a consistency loss on the forward dynamics prediction. For our experiments, we use a ResNet18  encoder, and causally masked transformer encoders  for the inverse and forward dynamics models. The architecture is illustrated in Figure 3.

### Dynamics as a visual self-supervised learning objective

First, we sample an observation sequence \(o_{t:t+h}\) of length \(h\) and compute its representation \(s_{t:t+h}=f_{}(o_{t:t+h})\). For convenience, we will write \(s_{t:t+h}\) as \(s_{:h}\), and \(s_{t+1:t+h}\) as \(s_{1:h}\) below. At any given step, the distribution of possible actions can be multimodal . Therefore, the forward dynamics transition \(p(s_{1:h}|s_{:h-1})\) can also have multiple modes. To address this, we first model the inverse dynamics \(q(z_{:h-1}|s_{:h})\), where \(z_{t}\) is the latent transition between frames. We assume \(z_{t}\) to be well-determined and unimodal given consecutive frames \(\{s_{t},s_{t+1}\}\). We have \(z^{m},s^{d},m d\) such that the latent cannot trivially memorize the next frame embedding. Finally, we concatenate \((s_{t},z_{t})\) and predict the one-step forward dynamics \(p(_{1:h}|s_{:h-1},z_{:h-1})\).

Figure 3: Architecture of DynaMo. DynaMo jointly learns an image encoder, an inverse dynamics model, and a forward dynamics model with a forward dynamics prediction loss.

Figure 2: Embedding nearest neighbor matches for DynaMo, BYOL, MoCo, and TCN on the Block Pushing environment. **(Top)** The nearest neighbor matches visualized in pixel space. **(Bottom)** Matches visualized in a top-down view. We see that the DynaMo representation captures task-relevant features (end effector, block, and target locations in this case), whereas prior work fixates on the large robot arm.

We compute a dynamics loss \(_{}(,s^{*})\) on the one-step forward predictions \(_{t+1:t+h}\), where \(s^{*}_{t+1:t+h}\) are the target next-frame embeddings; and a covariance regularization loss \(_{}\) from Bardes et al.  on a minibatch of observation embeddings \(S\):

\[_{}(_{t},s^{*}_{t})& =1-_{t},s^{*}_{t}}{\|_{t}\|_{2} \|^{*}_{t}\|_{2}}\\ _{}(S)&=_{i  j}[(S)]^{2}_{i,j}\\ &=_{}+ _{}\] (1)

For environments with multiple views, we compute a loss over each view separately and take the mean. We choose \(=0.04\) following Bardes et al.  for the total loss \(\). We find that covariance regularization slightly improves downstream task performance.

Naively, this objective admits a constant embedding solution. To prevent representation collapse, for \(_{}(,s^{*})\), we follow SimSiam  and set the target embedding \(s^{*}_{t}:=(s_{t})\), where \(\) is the stop gradient operator. Alternatively, our objective is also compatible with a target from a momentum encoder \(f_{}\)[33; 16], \(s^{*}_{t}:=_{t}=f_{}(o_{t})\), where \(\) is an exponential moving average of \(\).

We train all three models end-to-end with the objective in Eq. 1, and use the encoder for downstream control tasks.

## 4 Experiments

We evaluate our dynamics-pretrained visual representation on a suite of simulated and real benchmarks. We compare DynaMo representations with pretrained representations for vision and control, as well as other self-supervised learning methods. Our experiments are designed to answer the following questions: (a) Does DynaMo improve downstream policy performance? (b) Do representations trained with DynaMo work on real robotic tasks? (c) Is DynaMo compatible with different policy classes? (d) Can pretrained weights be fine-tuned in domain with DynaMo? (e) How important is each component in DynaMo?

### Environments and datasets

We evaluate DynaMo on four simulated benchmarks and two real robot environments (depicted in Figure 4). We provide a brief description below with more details included in Appendix A.

1. **Franka Kitchen**: The Franka Kitchen environment consists of seven simulated kitchen appliance manipulation tasks with a \(9\)-dimensional action space Franka arm and gripper. The dataset has \(566\) demonstration trajectories, each completing three or four tasks. The observation space is RGB images of size \((224,224)\) from a fixed viewpoint. We evaluate for 100 rollouts and report the mean number of completed tasks (maximum 4).
2. **Block Pushing**: The simulated Block Pushing environment has two blocks, two target areas, and a robot pusher with \(2\)-dimensional action space (end-effector translation). Both the blocks and targets are colored red and green. The task is to push the blocks into either same- or opposite-colored targets. The dataset has \(1\,000\) demonstration trajectories. The observation is RGB images of size \((224,224)\) from two fixed viewpoints. We evaluate for 100 rollouts and report the mean number of blocks in targets (maximum 2).
3. **Push-T**: The environment consists of a pusher with \(2\)-dimensional action space, a T-shaped rigid block, and a target area in green. The task is to push the block to cover the target area. The dataset has \(206\) demonstration trajectories. The observation space is a top-down view of the environment, rendered as RGB images of size \((224,224)\). We evaluate for 100 rollouts and report the final coverage of the target area (maximum \(1\)).
4. **LIBERO Goal**: The environment consists of 10 manipulation tasks with a \(7\)-dimensional action space simulated Franka arm and gripper. The dataset has \(500\) demonstration trajectories in total, \(50\) per task goal. The observation space is RGB images of size \((224,224)\) from a fixed external camera, and a wrist-mounted camera. We evaluate a goal-conditioned policy for \(100\) rollouts in total, \(10\) per task goal, and report the average success rate (maximum \(1\)).
5. **Allegro Manipulation**: A real-world environment with an Allegro Hand attached to a Franka arm. We evaluate on three tasks: picking up a sponge (\(6\) demonstrations), picking up a teabag (\(7\) demonstrations), and opening a microwave (\(6\) demonstrations). The observation space is RGB images of size (\(224,224\)) from a fixed external camera. The action space is \(23\)-dimensional, consisting of the Franka pose (\(7\)), and Allegro hand joint positions (\(16\)).
6. **xArm Kitchen**: A real-world multi-task kitchen environment with an xArm robot arm and gripper. The environment consists of five manipulation tasks. The dataset includes \(65\) demonstrations across five tasks. The observation space is RGB images of size (\(128,128\)) from three fixed external cameras, and an egocentric camera attached to the gripper. The action space is \(7\)-dimensional with the robot end effector pose and the gripper state.

### Does DynaMo improve downstream policy performance?

We evaluate each representation by training an imitation policy head on the frozen embeddings, and reporting the downstream task performance on the simulated environments. We use Vector-Quantized Behavior Transformer (VQ-BeT)  for the policy head. For xArm Kitchen, we use a goal-conditioned Baku with a VQ-BeT action head. MAE-style baselines (VC-1, MVP, MAE) use a ViT-B backbone. All other baselines and DynaMo use a ResNet18 backbone.

For environments with multiple views, we concatenate the embeddings from all views for the downstream policy. Further training details are in Appendix B. Table 1 provides comparisons of DynaMo pretrained representations with other self-supervised learning methods, and pretrained weights for vision and robotic manipulation:

* **Random, ImageNet, R3M**: ResNet18 with random, ImageNet-1K, and R3M  weights.
* **VC-1**: Pretrained weights from Majumdar et al. .
* **MVP**: Pretrained weights from Xiao et al. .
* **BYOL**: BYOL  pretraining on demonstration data.
* **BYOL-T**: BYOL + temporal contrast . Adjacent frames \(o_{t},o_{t+1}\) are sampled as positive pairs, in addition to augmentations.
* **MoCo-v3**: MoCo  pretraining on demonstration data.
* **RPT**: RPT  trained on observation tokens.
* **TCN**: Time-contrastive network  pretraining on demonstrations. MV: multi-view objective; SV: single view objective.
* **MAE**: Masked autoencoder  pretraining on demonstrations.
* **DynaMo**: DynaMo pretraining on demonstrations.

The best pretrained representation is underlined and the best self-supervised representation is **bolded**. We find that our method matches prior state-of-the-art visual representations on Franka Kitchen, and outperforms all other visual representations on Block Pushing, Push-T, and LIBERO Goal.

Figure 4: We evaluate DynaMo on four simulated benchmarks - Franka Kitchen, Block Pushing, Push-T, and LIBERO Goal, and two real-world environments - Allegro Manipulation, and xArm Kitchen.

### Do representations trained with DynaMo work on real robotic tasks?

We evaluate the representations pre-trained with DynaMo on two real-world robot environments: the Allegro Manipulation environment, and the multi-task xArm Kitchen environment. For the Allegro environment, we use a k-nearest neighbors policy  and initialize with ImageNet-1K features for all pretraining methods, as the dataset is relatively small with around \(1\,000\) frames per task. In the xArm Kitchen environment, we use the Baku architecture for goal-conditioned rollouts across five tasks. For our real-robot evaluations, we compare DynaMo against the strongest performing baselines from our simulated experiments (see Table 1). The results are reported in Table 2. We observe that DynaMo outperforms the best baseline by 43% on the single-task Allegro hand and by 20% on the multi-task xArm Kitchen environment. Additionally, as shown in Table 3, DynaMo exceeds the performance of pretrained representations by 50% on the Allegro hand. These results demonstrate that DynaMo is capable of learning effective robot representations in both single-task and multi-task settings.

### Is DynaMo compatible with different policy classes?

On the Push-T environment , we compare all pretrained representations across four policy classes: VQ-BeT , Diffusion Policy , MLP (with action chunking ), and k-nearest neighbors with locally weighted regression . We present the results in Table 4. We find that DynaMo representa

    &  &  &  & Push-T & LIBERO Goal \\  & & ( \(\)/4 ) & ( \(\)/2 ) & ( \(\)/1 ) & ( \(\)/1 ) \\   & Random & 3.32 & 0.07 & 0.07 & 0.80 \\  & ImageNet & 3.01 & 0.12 & 0.41 & 0.93 \\  & R3M & 2.84 & 0.11 & 0.49 & 0.89 \\  & VC-1 & 2.63 & 0.05 & 0.38 & 0.91 \\  & MVP & 2.31 & 0.00 & 0.20 & 0.88 \\   & BYOL & **3.75** & 0.09 & 0.23 & 0.28 \\  & BYOL-T & 3.33 & 0.16 & 0.34 & 0.28 \\  & MoCo-v3 & 3.28 & 0.03 & 0.57 & 0.70 \\  Self-supervised \\ methods \\  } & RPT & 3.54 & 0.52 & 0.56 & 0.17 \\  & TCN-MV & — & 0.07 & — & 0.69 \\  & TCN-SV & 2.41 & 0.07 & 0.07 & 0.76 \\  & MAE & 2.70 & 0.00 & 0.07 & 0.59 \\  & **DynaMo** & 3.64 & **0.65** & **0.66** & **0.93** \\   

Table 1: Downstream policy performance on frozen visual representation on four simulated benchmarks - Franka Kitchen, Blocking Pushing, Push-T, and LIBERO Goal. We observe that DynaMo matches or significantly outperforms prior work on all simulated tasks.

    & Task & BYOL & BYOL-T & MoCo-v3 & **DynaMo** \\   & Sponge & 2/10 & 4/10 & 5/10 & **7/10** \\  & Tea & 1/10 & 0/10 & 2/10 & **5/10** \\  & Microwave & 2/10 & 3/10 & 1/10 & **9/10** \\   & Put yogurt & 4/5 & 4/5 & 2/5 & **5/5** \\  & Get yogurt & 0/5 & 4/5 & 4/5 & **5/5** \\   & Put ketchup & **5/5** & 3/5 & **5/5** & 4/5 \\   & Get tea & 2/5 & 2/5 & 3/5 & **5/5** \\   & Get water & 0/5 & 0/5 & **3/5** & **3/5** \\   

Table 2: We evaluate DynaMo on eight tasks across two real-world environments: Allegro Manipulation, and xArm Kitchen. Results are presented as (successes/total). We observe that DynaMo significantly outperforms prior representation learning methods on real tasks.

tions improve downstream policy performance across policy classes compared to prior state-of-the-art representations. We also note that our representation works on the robot hand in SS4.3 with a nearest neighbor policy.

### Can pretrained weights be fine-tuned in domain with DynaMo?

We fine-tune an ImageNet-1K-pretrained ResNet18 with DynaMo for each simulated environment, and evaluate with downstream policy performance on the frozen representation as described in SS4.2. The results are shown in Table 5. We find that DynaMo is compatible with ImageNet initialization, and can be used to fine-tune out-of-domain pre-trained weights to further improve in-domain task performance. We also note that our method works in the low-data regime with ImageNet initialization on the real Allegro hand in Table 2.

### How important is each component in DynaMo?

In Table 6, we ablate each component in DynaMo and measure its impact on downstream policy performance on our simulated benchmarks.

**Forward dynamics prediction**: We replace the one-step forward prediction target \(s^{*}_{1:h}\) with the same-step target \(s^{*}_{:h-1}\). To prevent the model from trivially predicting \(s^{*}_{t}\) given \(s_{t}\), we replace the forward dynamics input \((s_{:h-1},z_{:h-1})\) with only \(z_{:h-1}\). The ablated objective is essentially a variant of autoencoding \(s_{t}\). We observe that removing forward dynamics prediction degrades performance across environments.

    & Method & VQ-BeT & Diffusion & MLP (chunking) & kNN \\   & Random & 0.07 & 0.04 & 0.07 & 0.01 \\  & ImageNet & 0.41 & **0.73** & 0.24 & 0.09 \\  & R3M & 0.49 & 0.63 & 0.27 & 0.08 \\  & VC-1 & 0.38 & 0.63 & 0.22 & 0.07 \\  & MVP & 0.20 & 0.49 & 0.11 & 0.08 \\   & BYOL & 0.23 & 0.40 & 0.11 & 0.04 \\  & BYOL-T & 0.34 & 0.50 & 0.16 & 0.04 \\ Self-supervised methods & MoCo v3 & 0.57 & 0.67 & 0.30 & 0.07 \\  & RPT & 0.56 & 0.62 & 0.30 & 0.07 \\  & TCN-SV & 0.07 & 0.14 & 0.07 & 0.01 \\  & MAE & 0.07 & 0.06 & 0.07 & 0.02 \\  & **DynaMo** & **0.66** & **0.73** & **0.35** & **0.12** \\   

Table 4: We evaluate the compatibility of DynaMo with different policy classes for downstream policy learning on the Push-T simulated benchmark. We report the final target coverage achieved (maximum 1) and demonstrate that DynaMo significantly outperforms prior representation learning methods across all policy classes.

    &  Franka Kitchen \\ (\(/4\) ) \\  } &  Block Pushing \\ (\(/2\) ) \\  } & Push-T & LIBERO Goal \\  ImageNet & 3.01 & 0.12 & 0.41 & **0.93** \\
**DynaMo** (random init) & 3.64 & 0.65 & **0.66** & **0.93** \\
**DynaMo** (ImageNet fine-tuned) & **3.82** & **0.67** & 0.50 & 0.90 \\   

Table 5: We evaluate the ability of DynaMo to finetune an ImageNet-pretrained ResNet-18 encoder across 4 benchmarks. We demonstrate that using a pretrained encoder can further improve the performance of DynaMo.

**Inverse dynamics to a transition latent**: As described in SS3.1, the forward dynamics loss assumes that the transition is unimodal and requires an inferred transition latent. We observed that removing the latent from the forward dynamics input results in a significant performance drop.

**Bottleneck on the transition latent dimension**: For the transition latent \(z\) and the observation embedding \(s\), we find that having \( z s\) stabilizes training. Here we set \( z:= s\), and find that our model can still learn a reasonable representation in some environments, but training can destabilize, leading to a high variance in downstream performance.

**Covariance regularization**: We find that covariance regularization from Bardes et al.  improves performance across environments. Training still converges without it, but the downstream performance is slightly worse.

**Stop gradient on target embeddings**: We observe that removing techniques like momentum encoder [33; 16] and stop gradient  leads to representation collapse [41; 16; 36].

**Observation context**: The dynamics objective requires at least \(2\) frames of observation context. For Franka Kitchen, we find that a context of \(2\) frames works best. For the other environments, a longer observation context (\(5\) frames) improves downstream policy performance. Details of hyperparameters used for DynaMo visual pretraining can be found in Appendix B.1.

### Variants with access to ground truth actions

In Table 7, we compare with two variants of DynaMo where we assume access to ground truth action labels during visual encoder training.

**Only inverse dynamics to ground truth actions**: as proposed in Brandfonbrener et al. , we train the visual encoder by learning an inverse dynamics model to ground truth actions, with covariance regularization, and without forward dynamics.

**Full model + inverse dynamics to ground truth actions**: we train the full DynaMo model plus an MLP head to predict the ground truth actions given the transition latents inferred by the inverse dynamics model.

We observe that in both cases, having access to ground truth actions during visual pretraining does not seem to improve downstream policy performance. We hypothesize that this is because the downstream policy already has access to the same actions for imitation learning.

## 5 Related works

This work builds on a large body of research on self-supervised visual representations, learning from human demonstrations, neuroscientific basis for learning dynamics for control, predictive models for decision making, learning from videos for control, and visual pretraining for control.

   Ablations & Kitchen & Block & Push-T & LIBERO \\  No forward & 34\% & 8\% & 44\% & 33\% \\ No inverse & 72\% & 35\% & 97\% & 41\% \\ No bottleneck & 92\% & 22\% & 9\% & 75\% \\ No cov. reg. & 94\% & 62\% & 85\% & 59\% \\ No stop grad. & 1\% & 5\% & 9\% & 0\% \\ Short context & 100\% & 75\% & 88\% & 89\% \\   

Table 6: Ablation analysis of downstream performance relative to the full architecture (100%)

   Variants & Kitchen & Block & Push-T & LIBERO \\  Inverse dynamics only & 100\% & 54\% & 70\% & 11\% \\ DynaMo + action labels & 97\% & 29\% & 94\% & 86\% \\   

Table 7: Variants with ground truth actions, downstream performance relative to the base model (100%)Self-supervised visual representations:Self-supervised visual representations have been widely studied since the inception of deep learning. There are several common approaches to self-supervised visual representation learning. One approach is to recover the ground truth from noise-degraded samples using techniques like denoising autoencoders [42; 43] and masked modeling [44; 45; 30]. Another approach is contrastive learning, which leverages data augmentation priors [31; 36; 41; 33; 46] or temporal proximity [31; 46] to produce contrastive sample pairs. A third self-supervised method is generative modeling [47; 48; 49], which learns to sequentially generate the ground truth data. More recently, self-supervision in the latent space rather than the raw pixel space has proven effective, as seen in methods that predict representations in latent space [50; 51].

Learning from demonstrations:Learning from human demonstrations is a well-established idea in robotics [52; 53; 54; 55]. With the advances in deep learning, recent works such as [3; 2; 5; 4; 1; 56] show that imitation learning from human demonstrations has become a viable approach for training robotic policies in simulated and real-world settings.

Neural basis for learning dynamics:It is widely believed that animals possess internal dynamics models that facilitate motor control. These models learn representations that are predictive of sensory inputs for decision making and motor control [57; 58; 59; 60]. Early works such as [17; 18; 19; 20] propose that there exists an internal model of the motor apparatus in the cerebellum for motor control and planning. [21; 22] propose that the central nervous system uses forward models that predict motor command outcomes and model the environment. Learning forward and inverse dynamics models also helps with generalization to diverse task conditions [23; 24].

Predictive models for decision making:Predictive model learning for decision making is well-established in machine learning. Learning generative models that can predict sequential inputs has achieved success across many domains, such as natural language processing , reinforcement learning [62; 63; 64], and representation learning [65; 46]. Incorporating the prediction of future states as an intrinsic reward has also been shown to improve reinforcement learning performance [66; 67; 68]. Moreover, recent work demonstrates that world models trained to predict environment dynamics can enable planning in complex tasks and environments [69; 70; 71; 72; 73].

Learning from video for control:Videos provide rich spatiotemporal information that can be leveraged for self-supervised representation learning [74; 75; 76; 77; 78; 79]. These methods have been extended to decision-making through effective downstream policy learning [7; 8; 9; 10; 6]. Further, recent work also enables learning robotic policies directly from in-domain human demonstration videos by incorporating some additional priors [80; 81; 82; 83; 84], as well as learning behavioral priors from actionless demonstration data [85; 86; 87].

Visual representation for control:Visual representation learning for control has been an active area of research. Prior work has shown that data augmentation improves the robustness of learned representations and policy performance in reinforcement learning domains [88; 89]. Additionally, pretraining visual representations on large out-of-domain datasets before fine-tuning for control tasks has been shown to outperform training policies from scratch [10; 12; 9; 11; 90; 8; 91]. More recent work has shown that in-domain self-supervised pretraining improves policy performance [92; 93; 94; 95] and enables non-parametric downstream policies .

## 6 Discussion and Limitations

In this work, we have presented DynaMo, a self-supervised algorithm for robot representation learning that leverages the sequential nature of demonstration data. DynaMo incorporates predictive dynamics modeling to learn visual features that capture the sequential structure of demonstration observations. During pretraining, DynaMo jointly optimizes the visual encoder with dynamics models to extract task-specific features. These learned representations can then be used for downstream control tasks, leading to more efficient policy learning compared to prior approaches. We believe that training DynaMo on larger unlabeled datasets could potentially improve generalization. Additionally, while promising for control tasks, more research is needed to evaluate DynaMo's effectiveness on robotic manipulation outside of lab settings.