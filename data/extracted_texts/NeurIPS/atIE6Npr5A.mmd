# Few-Shot Task Learning through

Inverse Generative Modeling

Aiv Netanyahu\({}^{1}\)

Correspondence to Aviv Netanyahu \(<\)avivn@mit.edu\(>\). Project website https://avivne.github.io/fll-igm.

Yilun Du\({}^{1,2}\)

Antonia Bronars\({}^{1}\)

Jyothish Pari\({}^{1}\)

Joshua Tenenbaum\({}^{1}\)

Tianmin Shu\({}^{3}\)

Pulkit Agrawal\({}^{1}\)

###### Abstract

Learning the intents of an agent, defined by its goals or motion style, is often extremely challenging from just a few examples. We refer to this problem as task concept learning and present our approach, Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM), which learns new task concepts by leveraging invertible neural generative models. The core idea is to pretrain a generative model on a set of basic concepts and their demonstrations. Then, given a few demonstrations of a new concept (such as a new goal or a new action), our method learns the underlying concepts through backpropagation without updating the model weights, thanks to the invertibility of the generative model. We evaluate our method in five domains - object rearrangement, goal-oriented navigation, motion caption of human actions, autonomous driving, and real-world table-top manipulation. Our experimental results demonstrate that via the pretrained generative model, we successfully learn novel concepts and generate agent plans or motion corresponding to these concepts in (1) unseen environments and (2) in composition with training concepts.

## 1 Introduction

The ability to learn concepts about a novel task, such as the goal and motion plans, from a few demonstrations is a crucial building block for intelligent agents - it allows an agent to learn to perform new tasks from other agents (including humans) from little data. Humans, even from a young age, can learn various new tasks from little data and generalize what they learned to perform these tasks in new situations .

In machine learning and robotics, this class of problems is referred to as Few-Shot Learning . Despite being a widely studied problem, it remains unclear how we can enable machine learning models to learn concepts of a novel task from only a few demonstrations and generalize the concepts to new situations, just like humans do. Common approaches learn policies either directly, which often suffer from covariate shift , or via rewards [4; 5; 6], which are largely limited to previously seen behavior . In a different vein, other work has relied on pretraining on task families and assumes that task learning corresponds to learning similar tasks to ones already seen in the task family [8; 9].

Inspired by the success of generative modeling in few-shot visual concept learning [10; 11; 12], where concepts are latent representations, in this work, we investigate whether and how few-shot task concept learning can benefit from generative modeling as well. Learning concepts from sequential demonstrations rather than images is by nature more challenging due to sequential data often notsatisfying the i.i.d. assumption in machine learning . In particular, we assume access to a large pretraining dataset of paired behaviors and task representations to learn a conditional generative model that synthesizes trajectories conditioned on task descriptions. We hypothesize that by learning a generative model conditioned on explicit representations of behavior, we can acquire strong priors about the nature of behaviors in these domains, enabling us to more effectively learn new behavior that is not within the pretraining distribution, given a limited number of demonstrations, and further generate the learned behavior in new settings.

To this end, we propose Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM). In our approach, we first pretrain a large conditional generative model which synthesizes different trajectories conditioned on different task descriptions. To learn new tasks from a limited number of demonstrations, we then formulate few-shot task learning as an _inverse generative modeling problem_, where we find the latent task description, which we refer to as a _concept_, which maximizes the likelihood of generating the demonstrations. This approach allows us to leverage the powerful task priors learned by the generative model to learn the shared concepts between demonstrations without finetuning the model (Figure 1). We demonstrate this approach in various domains: object rearrangement, where concepts are relations between objects, goal-oriented navigation, where concepts are target attributes, motion capture, where concepts are human actions, autonomous driving, where concepts are driving scenarios, and real-world table-top manipulation where concepts are manipulation tasks (Figure 2).

New concepts are either (1) compositions of training concepts (_e.g._, multiple desired relations between objects that define a new object rearrangement concept) or (2) new concepts that are not explicit compositions in the natural language symbolic space of training concepts (_e.g._, a new human motion 'jumping jacks' is not an explicit composition of training concepts 'walk', 'golf' etc.) Thanks to generative models' compositional properties that enable compositional concept learning , in addition to being able to learn a single concept from demonstrations directly, FTL-IGM learns compositions of concepts from demonstrations that, when combined, describe the new concept.

We show that our approach generates diverse trajectories encapsulating the learned concept. We achieve this due to two properties of generative models. First, these models have shown strong interpolation abilities [15; 16], which allow generating the new concept on new initial states they were not demonstrated from. Second, these models have compositional properties that enable compositional trajectory generation , which allow composing learned concepts with training concepts to synthesize novel behavior that was not demonstrated (_e.g._, 'jumping jacks' and 'walk'), see Figure 3. We further demonstrate that our approach addresses a unique challenge introduced in learning task concepts: we utilize plans generated by learned concepts in a closed-loop fashion.

Figure 1: **Few-shot concept learning. Given paired task demonstration \(\) (_e.g._, ‘walk’) and concept \(c\) (a latent representation of the task), we train a generative model \(_{}\) to generate behavior from a concept. Then, given demonstrations of a new behavior \(\) (_e.g._, ‘jumping jacks’) without its concept label, we aim to learn its concept representation by optimizing concept \(\) as input to frozen \(_{}\).**

Figure 2: **Experiment Domains. We extensively evaluate our approach for various domains.**

Our main contributions are (1) formulating the problem of task learning from few demonstrations as Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM), (2) adapting a method for efficient concept learning to this problem based on the new formulation, and (3) a systematic evaluation revealing the ability of our method to learn new concepts across a diverse set of domains.

## 2 Related Work

**Learning from few demonstrations.** Our problem setting is closely related to learning from few demonstrations. There has been much work on learning to generate agent behavior given few demonstrations. There are several common approaches to this problem. First, behavior cloning (**BC**) is a supervised learning method to learn a policy from demonstrations that predicts actions from states. Similar to our framework, goal-conditioned BC can predict states from task representations and states . Finetuning these models to learn new behaviors requires labeled demonstrations of the new task. We assume unlabeled demonstrations. BC often suffers from covariate shift  and fails to generate the demonstrated behavior in novel scenarios. This can be mitigated by assuming access to a human in the loop . Second, the inverse reinforcement learning (**IRL**) framework learns a policy that maximizes the return of an explicitly [5; 6; 20] or implicitly  learned reward function. These works learn a reward for a single task or for a set of tasks (_e.g._, goal-conditioned IRL [22; 23] and multi-task IRL ). While IRL is more data efficient than BC, it is computationally costly due to learning a policy every iteration via an inner reinforcement learning (RL) loop. Additionally, it requires access to taking actions in the environment during training and when faced with a new task, we have to retrain the reward again. A third approach is **inverse planning**[25; 26; 27], which can robustly infer concepts such as goals and beliefs even in unseen scenarios. However, it assumes access to a planner, knowledge about environment dynamics, and the task/goal space. Finally, **in-context** learning approaches [28; 8; 29] learn actions in a supervised manner by representing the task with demonstrations. This allows few-shot generalization without further training.

In contrast, we do not learn an action-generating policy directly or via a reward function. For concept learning, we do not assume having access to any given planner, world model, actions, rewards, or prior over the task space. Instead, we learn _concepts_ (task representations) from demonstrations via a pretrained generative model that takes a concept as input and directly produces state sequences. We then input the learned concept into the generative model to produce behavior similar yet diverse to the demonstrated one. We further demonstrate how to use these state sequences with a planner to take actions and achieve the desired behavior. The idea of concept learning via generative models has been explored for computer vision applications [11; 12]. We build on this work and show how to extend it to learn agent task concepts. Our work also differs from prior works on learning trajectory representations [30; 31; 32; 33; 34]. These works focus on learning plans over trajectory embeddings, whereas we learn a task representation from demonstrations on which we condition to generate behavior.

**Generative Models in Decision Making.** There has been work on generative modeling for decision-making, including generative models for single-agent behaviors, such as implicit BC , Diffuser [36; 17], Diffusion Policy , Decision Transformer [38; 39; 40], and for multi-agent motion prediction such as Jiang et al. . The success of diffusion policy in predicting sequences of future actions has led to 3D extensions , and combined with ongoing robotic data collection efforts  and advanced vision and language models, has led to vision-language-action generative models [44; 45; 46]. In this work, we utilize a conditional generative model for the _inverse_ problem, _i.e._, learning concepts from demonstrations.

**Composable representations.** There has been work on obtaining composable data representations. \(\)-VAE  learns unsupervised disentangled representations for images. MONet  and IODINE  decompose visual scenes via segmentation masks and COMET  and  via energy functions. There is also work on composing representations to generate data with composed concepts. Generative models can be composed together to generate visual concepts [51; 52; 53; 14; 54; 15] and robotic skills . The generative process can also be altered to generate compositions of visual [56; 57; 58; 59] and molecular  concepts. We aim to obtain task concepts and generate them in composition with other task concepts.

## 3 Formulation

Inspired by recent success in large generative models, we propose a generative formulation for learning specific behavior given a small set of demonstrations, which we term Few-Shot Task Learning through Inverse Generative Modeling (**FTL-IGM**). In our formulation, we assume access to a large pretraining dataset \(D_{}=\{(_{i},c_{i})\}_{i=1}^{N}\) of _state_-based sequences \(_{i}=\{s_{0},s_{1},...\}\) of states from state space \(S\) annotated with meta-data "concepts" \(c_{i}^{n}\) describing trajectories. This assumption is often not prohibitive in practice. There is typically a vast amount of existing data collected from the internet or prior exploration in an environment, which may only need to be weakly annotated to characterize the trajectory, _e.g._, the goal state. Given \(D_{}\), we learn a conditional generative model \(_{}:\) conditioned on concepts and initial states, which learns to generate future trajectories. We train the parameters of \(_{}\) to maximize likelihood \(_{}_{,c D_{}}[_ {}(|c,s_{0})]\).

Then, given an unlabeled demonstration dataset \(D_{}\), we formulate learning a new concept \(\) that is used to sample trajectories from \(\) as _inverting_ the generative model. In particular, we learn new concept \(\) so that our frozen conditional generative model \(_{}\) maximizes the likelihood of trajectories in \(D_{}\), corresponding to \(_{}_{ D_{}}[_{ }(|,s_{0})]\). We find that this design choice enables us to leverage the priors learned by \(_{}\) from \(D_{}\) to effectively learn concepts from \(D_{}\) given very few demonstrations, even if the demonstrated \(D_{}\) deviates from the concept labels \(c\) seen in \(D_{}\). For evaluation in closed loop, we further assume access to a planner that given two states plans which action to take in the environment, sometimes via access to simulation in the environment. We use this planner sequentially to make decisions in the environment.

The key difference between our approach to few-shot adaptation from demonstrations and prior approaches is the assumption and usage of a large pretraining dataset of paired behaviors and concepts \(D_{}\) combined with an invertible generative model. We learn new concepts solely from demonstrations without finetuning model weights or taking actions in the environment by relying on the pretrained concept space.

## 4 Few-Shot Concept Learning Based on FTL-IGM

We adapt a few-shot concept learning method to task concepts based on the FTL-IGM framework. During training we learn a generative model \(_{}\) from training \(\{(_{i},c_{i})\}_{i}\) pairs. We then freeze \(_{}\), and given demonstrations of a new task \(\{\}_{i}\), optimize a concept \(\) to produce the new behavior via \(_{}\). We then generate a diverse set of behaviors via \(_{}\), either for the learned concept \(\) conditioned on new initial states or for compositions of \(\) with other concepts.

### Training a diffusion model to generate behavior

A **diffusion model** is a generative model that given a forward noise adding process \(q(x_{t}|x_{t-1}):=(x_{t};}x_{t-1},_{t} {I})\) starting from data \(x_{0}\) according to a variance schedule \(_{1},...,_{T}\), learns the reverse process \(p_{}(x_{t-1}|x_{t}):=(x_{t-1};_{}(x_{t},t),_{ }(x_{t},t))\). Ho et al.  simplify the training objective to estimate noise \(_{t\{1,T\},x_{0},(0,)}[||-_{}(x_{t},t)||^{2}]\) where \(x_{t}\) is produced by adding noise \(\) to data \(x_{0}\) by the forward noising process at diffusion step \(t\), \(q(x_{t}|x_{0}):=(x_{t};}x_{0},(1-_{t}) )\) where \(_{t}:=_{s=1}^{t}(1-_{s})\). Dhariwal and Nichol  enable **conditioned generation** by guiding the reverse sampling process with classifier gradients. The noise prediction becomes \(=_{}(x_{t},t)-_{t}_ {x_{t}}}\), log \(p_{}(y|x_{t},t)\) where classifier \(p_{}(y|x_{t},t)\) is trained on noisy images, and \(\) is the guidance scale. Ho and Salimans  introduce **classifier-free guidance** that achieves the same objective without the need for training a separate classifier. This is done by learning a conditional and unconditional model by removing the conditioning information with dropout during training. The noise prediction is then \(=_{}(x_{t},t)+(_{}(x_{t},y,t)- _{}(x_{t},t))\). Ramesh et al.  and Nichol et al.  demonstrate how this idea can be used to generate images conditioned on a class. Diffusion models have recently shown success as generative models for **decision making

Figure 3: **Diverse learned concept generation. We generate versions of the new behavior conditioned on the learned concept and (1) new initial states and (2) composed with other concepts.**

[36; 17]. Specifically, Ajay et al.  used a conditional classifier-free guidance diffusion model  to generate trajectories of future states to reach given an input observation. We adopt this objective and learn a denoising model \(_{}\) conditioned on latent concepts and initial observed states to estimate noise of a future state trajectory:

\[_{(,c) D_{},(0,),t\{1,T\},(p)}[||-_{ }(x_{t}(),(1-)c+ c_{},s_{0},t)||^{2}]\] (1)

where \(p\) is the probability of removing conditioning information which is then replaced by dummy condition \(c_{}\), and \(s_{0}\) is the initial state corresponding to trajectory \(\). \(x_{t}()\) is obtained from \(x_{0}=\) by the forward noising process. We then extend this approach for the inverse problem, namely, learning a concept from demonstrations.

### Few-shot concept learning

Gal et al.  use a frozen generative model to **learn visual concept** representations from few images depicting the concept by optimizing the model's input \(v_{*}=*{arg\,min}_{v}_{x_{0},v,(0,1),t}||-_{}(x_{t},c_{}(v),t)||_{2}^{2} \), where \(c_{}\) and \(_{}\) are fixed. Liu et al.  extend this and **learn visual concept compositions** with a pretrained diffusion model in an unsupervised manner. Namely, from a set of images that depict various concepts, for each image \(x^{i}\) they learn a set of weights \(_{k}^{i}\) and a shared set of visual concepts for all images \(c_{k}\), \(=(x_{t}^{i},t)+_{k=1}^{K}_{k}^{i}((x_ {t}^{i},c_{k},t)-(x_{t}^{i},t))\). We extend these formulations to inferring multiple concepts, whose composition describes a single task concept, from few demonstrations of a task.

Given a trained diffusion model \(_{}\) and demonstrations of a new concept \(\{\}_{i}\) from \(D_{}\), we learn concepts \(\{_{1},...,_{K}\}\) for \(K 1\) and their weights \(\{_{1},...,_{K}\}\) that best describe the demonstrations. Starting from uniformly sampled concept embeddings \(_{k}(^{n})\), we freeze \(_{}\), and optimize \(_{k}\) and \(_{k}\):

\[_{(0,)}[||-(_{ }(x_{t}(),c_{},s_{0},t)+_{k=1}^{K}_{k}( _{}(x_{t}(),_{k},s_{0},t)-_{ }(x_{t}(),c_{},s_{0},t)))||^{2}].\] (2)

We find that this compositional approach enables us to effectively represent and learn new demonstrations, even when demonstrations are substantially different than those seen in training tasks.

### Generating the learned concept

After learning concepts \(_{k}\), whose composition describes the new task \(\), we evaluate the behavior it generates by initializing \(x_{T}()(0,)\), and compute \(x_{t}(_{t-1},_{t-1})\) iteratively as a function of the estimated denoising function \((_{})\), where \(\) and \(\) are the mean and variance that define the reverse process, and \([0,1)\) is a scaling factor that leads to lower temperature samples, until generating \(x_{0}=\) representing the trajectory of the agent. The denoising function is constructed by fixed or learned weights as defined in Eq. 2 and by any number of concepts \( 1\). The applications of the generation procedure can be summarized as:

**Learned concept and demonstrated initial states.** We apply our learned concept to a set of demonstrated initial states. In domains where the initial state and concept jointly determine optimal behavior, the generated trajectory corresponds to optimal actions to execute (_e.g._ goal-oriented navigation). In contrast to other domains where the initial state is irrelevant for a task due to the randomness in sampling \(x_{T}()\) (_e.g._ motion capture), generated trajectories correspond to diverse plausible behaviors exhibiting the learned concept.

**Learned concept and novel initial states.** We further apply the generation procedure from our learned concept on novel initial states, to generate trajectories of new behaviors exhibiting our conditioned concept. Prior methods may suffer from covariate shift in this setting . We empirically show that our method is less prone to this problem.

**Learned concept composed with other concepts.** Finally, we modify the generation procedure of our newly learned concept to generate trajectories that simultaneously exhibit other concepts. To generate a trajectory with an added another concept, we add another term to the sum in Eq. 2 where the learned concept is composed with a training concept \(c_{k}\) and its weight \(_{k}\): \(_{k}(_{}(x_{t}(),c_{k},s_{0},t)-_{}(x_ {t}(),c_{},s_{0},t))\). This modified generation procedure constructs trajectories which exhibits behavior that has a composition of the learned concept and the other specified concepts .

Similarly to Ajay et al. , in environments where an inverse dynamics model is provided, we generate trajectories in a closed loop. We execute actions calculated by the inverse dynamics given the predicted plan by the model and then repeatedly replan given new observations.

## 5 Experiments

We demonstrate results in four domains where concept representations are T5  embeddings of task descriptions in natural language for training, and empty string embeddings for the dummy condition. During few-shot concept learning, we are provided with three to five demonstrations of a composition of training concepts or of a novel concept that is not an explicit composition of training tasks in natural language symbolic space. We ask a model to learn the concept from these demonstrations.

### Task-Concepts

**Learning concepts describing goals that are spatial relations between objects.** Object rearrangement is a common task in robotics  and embodied artificial intelligence (AI) , serving as a foundation for a broader range of tasks such as housekeeping and manufacturing. Here, we use a 2D object rearrangement domain to evaluate the ability of our method to learn task specification concepts. Given a concept representing a relation between objects, we generate a single state describing that relation. The concept in a training example describes the relation (either 'right of' or 'above') between only one pair of objects (out of three objects) in the environment. Then, a model must learn compositions of these pairwise relations and new concepts such as 'diagonal' and 'circle' (see Figure 4). The results in Figures 5 and 6 demonstrate that our method learns unseen compositions of training concepts and new concepts. They further demonstrate how our method composes new concepts with learned concepts. For additional qualitative results, please refer to Appendix A.

While successful in most cases, there are also a few failure examples. The accuracy for the new 'circle' concept is low (\(0.44\)) compared to the mean over task types in Figure 6 Object Rearrangement New Concept (\(0.82 0.09\)). This is most likely due to this concept lying far out of the training distribution. The task'square right of circle \(\) triangle above circle' has low accuracy for 2 concepts (\(0.32\)) compared to the mean in Table 2 Object Rearrangement Training Composition (\(0.75 0.11\)). This may arise from the combined concept-weight optimization process - as there is no explicit regularization on weights, they may converge to 0 or diverge. In Figure 12, we show that concept components may or may not capture new concept relations.

**Learning concepts describing goals based on attributes of target objects.** We test our method in a goal-oriented navigation domain adapted from the AGENT dataset , where an agent navigates to one of two potential targets. Conditioned on a concept representing the attributes of the desired

Figure 4: **Object rearrangement.** Training concepts are single pairwise relations (‘A right of/above B’), and new concepts are either compositions of training concepts (‘A right of/above B’ \(\) ‘B right of/above C’) or new relations (‘A diagonal to B’, ‘A, B, C on circle circumference of radius r’).

Figure 5: **Object rearrangement new concept qualitative evaluation.** Learning the new concept ‘square diagonal to triangle’ and composing it with the training concept ‘circle right of square’.

target object and initial state, we generate a state-based trajectory describing an agent navigating to the target. Each object has a color and a shape out of four possible colors and four shapes. During training, we provide 16 target-distractor combinations that include all colors and shapes (but not all combinations), and a concept is conditioned on one of the target's attributes (_e.g._, color). We introduce new concepts defined by both target attributes, including (1) unseen color-shape target combinations and (2) new target-distractor combinations. Figure 7 shows an example. In training, we see bowl and red object targets. A new concept includes a novel composition as the target - red bowl. The new concept distractor objects (green bowl and red sphere) were introduced during training, but they were not paired with a red bowl as the target. As Figure 13 shows, our method successfully learns concepts where targets are new compositions of target attributes in settings with new target-distractor pairs and generalizes to new initial object states. We further evaluate our model and baselines in closed loop (Figure 6) by making an additional assumption that a planner is provided. The planner produces an action given a current state and a future desired state predicted by a model.

**Learning concepts describing human motion.** Unlike prior work on learning to compose human poses from motion capture (MoCap) data [e.g., 73, 74], here we focus on the inverse problem - learning new actions from MoCap data. In particular, we use the CMU Graphics Lab Motion Capture Database (http://mocap.cs.cmu.edu/). We train on various human actions in the database and few-shot learn three novel concepts (see Appendix B.3 for details). Learning tasks from few demonstrations is especially beneficial in this domain since describing motion concepts in words could be hard. In Table 5.1, we ask five human volunteers to select generated behaviors that depict training and new concepts. We demonstrate quantitatively that our method generates human motion that captures the desired behavior across training and new behaviors. We qualitatively demonstrate how our method generates learned new concepts ('jumping jacks' and 'breaststroke') from new initial states and composes 'jumping jacks' with training concepts 'walk', 'jump', and'march'.

Results are best viewed on our website website. We compare motion generated by our method to various baselines on new initial states for 'jumping jacks' and 'breaststroke'. Our method is noisy yet captures the widest range of motion. While other methods often produce smoother trajectories, they mostly capture local (VAE) or degenerate (BC, In-Context, Language) motion.

**Learning concepts describing driving scenarios.** In an Autonomous Driving domain , an agent acts in a challenging multi-agent environment to complete a driving task. We train on several driving scenarios ('highway', 'exit','merge', and 'intersection') and learn a new driving scenario ('roundabout') from several demonstrations (see Figure 8 and further details in Appendix B.4). We evaluate this scenario in closed loop on new initial states, assuming access to a planner that can simulate taking actions in the environment. Over two evaluation metrics (crash and task completion rate), our method achieves overall best results (Figure 9).

**Learning concepts describing real-world table-top manipulation tasks.** We evaluate our method's capability to learn a novel concept for real-world table-top manipulation with a Franka Research 3 robot. Training concepts include 'pick green circle and place on book', 'pick green circle and place on elevated white surface', 'push green circle to orange triangle' and 'push green circle to orange triangle around purple bowl'. The new scenario includes pushing the green circle to the orange triangle on a book (Figure 10). We evaluate training pushing in closed loop and and achieve success

Figure 6: **Object rearrangement (left) and AGENT closed loop (right) quantitative evaluation on training and few-shot novel concept learning.** Accuracy of FTL-IGM (ours), BC, VAE, and In-Context over concept generation of training concepts, novel compositions, novel concepts, and new initial states. We plot the average and standard error over new task types. Full details of the evaluation metrics appear in Appendix B and for baselines implementation in Appendix C.

[MISSING_PAGE_FAIL:8]

**In-Context learning.** We compare our approach with training a method to in-context learn from demonstrations. Specifically, we compare our approach to Xu et al.  using the pretrained dataset for few-shot behavior generation without further training. We sequentially predict states from demonstrations of a concept and window of current states and show that our method adapts better to new concepts (Figures 6, 9, 13). This emphasizes the need to learn explicit concept representations.

**Conditioning on Language Descriptions of New Concepts.** There has been work on generating actions from language instructions [77; 78; 79]. We demonstrate that in our setup, merely providing new concept language instructions embedded with T5 (as in our pretraining dataset) is insufficient, and generalization is better when learning concepts from few demonstrations. For AGENT, training compositions on new initial states has an average accuracy and standard error of \(0.63 0.07\), lower than ours (\(0.73 0.07\)). For Object Rearrangement, training compositions (\(0.2 0.07\)), new concept (\(0.2 0.05\)), and new and training concept compositions (\(0.13 0.04\)) accuracies are significantly lower than ours (\(0.9 0.04\), \(0.82 0.09\) and \(0.8 0.02\)). For MoCap, we demonstrate qualitatively that instead of capturing new human actions, the agent transitions into walking. Results are best viewed on our website.

### Learning two concepts yields higher accuracy than one concept

When learning weights together with concepts, we check the effect of the number of learned concepts and weights. We report results in Table 2 for Object Rearrangement, AGENT, and Driving, and find that, on average, learning two concepts improves concept learning. We demonstrate qualitatively for MoCap that learning two conditions is preferable. In 'jumping jacks', we observe that the motion lacks raising and lowering both arms and in 'breaststroke', it lacks complete arm and upper torso movement. Results are best viewed on our website.

### How are learned new concepts related to training concepts?

**New concepts that are compositions of training concepts.** We analyze what the learned two concepts in Object Rearrangement and AGENT learn for novel concept compositions (_e.g._,'red bowl'). For each concept (_e.g._,'red' and 'bowl'), we generate two sets of \(50\) samples from the learned components. Table 3 shows accuracy for these sets over the concepts. In some cases (most notably the 'line' concept in Object Rearrangement, 'circle right of triangle and triangle right of square'), each

Figure 10: **Table-top manipulation.** Training concepts: pick-and-place onto elevated surfaces and table-top pushing. New concept: pushing on an elevated surface.

Figure 9: **Driving crash (left) and success (right) rates.** Crash rate (lower is better) and task completion rate (higher is better) averaged over training tasks. We report standard errors over training tasks and accuracy over \(50\) trajectories generated from the learned new concept. VAE has a high completion rate yet a high crash rate. In-context has a low crash rate yet 0% success rate – typically, the controlled vehicle reaches the roundabout’s center but does not complete the crossing. Overall, our method learns to complete the roundabout crossing with competitive crash and success rates.

learned component captures a single composed concept. In other cases, a single learned component captures both concepts (Figure 12).

**New concepts that are not explicit compositions of training concepts in natural language symbolic space.** In Figure 11 we visualize t-SNE  embeddings for T5 training concept representations and learned concept component representations. We note that learned components are relatively close to training concepts, maintaining the model's input distribution, yet capture concepts that are not explicit compositions of training concepts.

## 6 Discussion and Limitations

In this work, we formulate the problem of new task concept learning as Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM). We adapt a method for concept learning based on this new formulation and evaluate task concept learning against baselines in four domains. Our extensive experimental results show that, unlike the baselines, FTL-IGM successfully learns novel concepts from a few examples and generalizes the learned concepts to unfamiliar scenarios. It also composes learned concepts to form unseen behavior thanks to the compositionality of the generative model. These results demonstrate the efficacy, sample efficiency, and generalizability of FTL-IGM.

However, our work has several limitations. First, while our framework is general for any parameterized generative model, our implementation with a diffusion model incurs high inference time. We note that there is still space for improvement in the MoCap generation quality and in the compatibility rate of demonstrations generated by composing learned and training concepts. In addition, we assume that learned concepts lie within the landscape of training concepts to learn them from a few demonstrations without retraining the model. We have approached the question of what new concepts can be represented by compositions of concepts in this landscape empirically, leaving a theoretical analysis as future work. We are hopeful that with the continued progress in the field of generative AI, more powerful pretrained models will become available. Combined with our framework, this will unlock a stronger ability to learn and generalize various task concepts in complex domains.

**Environment** & **Setting** & **1 Concept** & **2 Concepts** \\   Object \\ Rearrangement \\  } & Training Composition & 0.28\(\)0.11 & 0.75\(\)0.11 \\   & New Concept & 0.49\(\)0.11 & 0.77\(\)0.09 \\   & New+Training & 0.38\(\)0.04 & 0.73\(\)0.04 \\   AGENT \\  } & Training Composition & 0.52\(\)0.08 & 0.68\(\)0.13 \\   & New Initial State & 0.54\(\)0.07 & 0.67\(\)0.05 \\   & New Initial State & 0.14 & 0.24 \\  

Table 2: **Ablation on the number of learned concepts.** We test the effect of the number of learned concepts and weights in FTL-IGM on the generation accuracy of new learned concepts. On average, learning two concept components and their weights is preferable to learning one concept component and its weight. We report average accuracy and standard error over task types for Object Rearrangement and AGENT. Driving includes a single new concept and we report accuracy only.

Figure 11: **t-SNE embeddings of new concepts that are not explicit compositions of training concepts.** See interactive version for detailed labels on our website.

Acknowledgments

This research was also partly sponsored by the United States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19- 2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation herein. We acknowledge support from ONR MURI under N00014-22-1-2740 and ARO MURI under W911NF-23-1-0277. Yilun is supported in part by an NSF Graduate Research Fellowship. We would like to thank Abhishek Bhandwaldar for help with the AGENT environment, and Anurag Ajay, Lucy Chai, Andi Peng, Felix Yanwei Wang, Anthony Simeonov and Zhang-Wei Hong for helpful discussions.