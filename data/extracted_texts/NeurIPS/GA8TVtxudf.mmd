# Metric from Human: Zero-shot Monocular Metric

Depth Estimation via Test-time Adaptation

 Yizhou Zhao\({}^{1}\), Hengwei Bian\({}^{1}\), Kaihua Chen\({}^{1}\), Pengliang Ji\({}^{1}\), Liao Qu\({}^{1}\), Shao-yu Lin\({}^{1}\),

**Weichen Yu\({}^{1}\), Haoran Li\({}^{2}\), Hao Chen\({}^{1}\), Jun Shen\({}^{2}\), Bhiksha Raj\({}^{1}\), Min Xu\({}^{1}\)**

\({}^{1}\)Carnegie Mellon University, Pittsburgh \({}^{2}\)University of Wollongong, Wollongong

https://github.com/Skaldak/MfH

Corresponding author.

###### Abstract

Monocular depth estimation (MDE) is fundamental for deriving 3D scene structures from 2D images. While state-of-the-art monocular relative depth estimation (MRDE) excels in estimating relative depths for in-the-wild images, current monocular metric depth estimation (MMDE) approaches still face challenges in handling unseen scenes. Since MMDE can be viewed as the composition of MRDE and metric scale recovery, we attribute this difficulty to scene dependency, where MMDE models rely on scenes observed during supervised training for predicting scene scales during inference. To address this issue, we propose to use humans as landmarks for distilling scene-independent metric scale priors from generative painting models. Our approach, Metric from Human (MfH), bridges from generalizable MRDE to zero-shot MMDE in a generate-and-estimate manner. Specifically, MfH generates humans on the input image with generative painting and estimates human dimensions with an off-the-shelf human mesh recovery (HMR) model. Based on MRDE predictions, it propagates the metric information from painted humans to the contexts, resulting in metric depth estimations for the original input. Through this annotation-free test-time adaptation, MfH achieves superior zero-shot performance in MMDE, demonstrating its strong generalization ability.

Figure 1: **Illustration of our motivation. (a) Fully supervised MMDE cannot generalize well on unseen data as (b) MRDE, with its reliance on training scenes for predicting metric scales during test time. (c) Hence, we develop MfH to distill metric scale priors from generative models in a generate-and-estimate manner, bridging the gap from generalizable MRDE to zero-shot MMDE. We use grayscale to represent normalized depths in MRDE predictions, while a colormap mapping metric depth from meters to RGB values in MMDE results. In \(\), \(()\) denotes rasterized metric depths.**

## 1 Introduction

Monocular depth estimation (MDE) is essential in understanding the 3D structure of scenes from 2D images and has many applications in robotics [3; 4], autonomous driving [5; 6], and virtual reality [7; 8]. It requires recovering depth information from a single image without relying on additional sensors or stereo cameras, thereby being inherently ill-posed.

Recent literature mainly explores MDE in two branches, namely, monocular relative depth estimation (MRDE) [2; 1; 9] and monocular metric depth estimation (MMDE) [10; 11; 12; 13; 14; 15; 16; 17]. MRDE estimates normalized depths or disparities by factoring out the scale. Its scale-invariant nature enables large-scale training on diverse datasets with distinct camera parameters, while at the cost of bringing in scale ambiguity. In contrast, MMDE predicts absolute depths in _meters_. Due to the unbounded output range and the intertwined relationship between depths and focal lengths, early works of this line often cannot perform well on test data with arbitrary scene scales or camera intrinsics. To compensate for this, recent progress resorts to injecting scene information  or camera information [10; 11; 14] into the model. The former attempt learns scene-specific scale priors, modeled with metric heads for indoor or outdoor scenes, and uses either to transform relative depths into metric depths in a heuristic manner. The latter aims to disambiguate scale prediction with extra camera inputs. However, as shown in Fig. 2, both lines of work require notably larger amounts of labeled training data to achieve similar mean absolute relative errors (\(\)) as their MRDE counterparts.

_What causes the data hunger of MMDE_, and _what makes MMDE harder to generalize_? Given that MMDE can be viewed as the composition of MRDE and metric scale recovery, we posit the latter might be the primary factor. MMDE models might face challenges in inferring scene scales without sufficient exposure to similar annotated scenes during training, which is not a problem for scale-invariant MRDE. To validate our assumption, we evaluate a scale-related metric (\(_{1}\)) of an MMDE model, ZoeDepth , on randomly sampled test images. Meanwhile, we calculate the maximum cosine similarity between each test sample and all training samples with metric annotations using DINOv2 . Our findings from Fig. 3 indicate a clear trend: higher similarity to training samples positively correlates with better performance, and vice versa. This reflects a scene dependency of MMDE models, likely arising from their supervised training paradigm. In other words, they tend to learn an implicit mapping between training scenes and metric scales from <image, metric annotation> pairs. As a result, adapting to novel scenes may require extra domain-specific fine-tuning.

Figure 3: **MMDE \(_{1}\) versus the maximum cosine similarity between each test sample and all metric-annotated training samples. “\(/\)”: from indoor/outdoor datasets. We see that the scale-related performance of a test sample positively correlates with its similarity to training samples. Details can be found in Appendix A.1.**

Figure 2: **Comparison of state-of-the-art MRDE and MMDE methods in terms of \(\) and the number of training samples.** Marigold  and Depth Anything  are designed for MRDE, while the rest are for MMDE. We observe MMDE approaches require notably more data to achieve similar \(\) as MRDE.**To address this dependency for better generalization capabilities, we propose to avoid scene-dependent supervised learning, while leveraging a scene-independent metric scale prior. Our insights are two-fold. First, we observe generative painting models can paint objects of proper sizes based on partial contexts, indicating an underlying sense of scales. Additionally, humans can be potentially utilized as relatively universal landmarks, since humans exhibit sizes that are generally more comparable to each other than other common in-the-wild objects, e.g., tables, trees, and cars. To explicitly derive a metric scale prior from generative painting models, we notice state-of-the-art human mesh recovery (HMR) approaches [19; 20; 21] can robustly estimate human dimensions for in-the-wild images. Also, they typically output SMPL [22; 23] representations with shape space defined in _meters_. While the input image does not guarantee to include humans, we speculate an off-the-shelf image painting model can paint proportionate humans in the scene, which provides an opportunity to retrieve metric-scale information for the original input by measuring painted humans. Hence, we introduce a test-time adaptation pipeline, Metric from Human (MfH), as illustrated in Fig. 1. Concretely, it 1) paints humans with partial contexts of the input image, 2) estimates human dimensions from the painted image, and 3) propagates the metric-scale information from humans to the contexts for MMDE. In this way, we can distill the metric scale prior hidden inside the generative painting model, unleashing its power to comprehend diverse scenes. As a result, our MfH mitigates the scene dependency issue in fully supervised MMDE, thereby being potentially more generalizable to unseen scenes.

Our contributions can be summed up as follows:

1. We discuss that the current obstacle for generalizable MMDE lies in scene dependency and propose to use a scene-independent metric scale prior as a solution. Further, we find it possible to establish such prior by distilling from generative painting models.
2. To extract the metric scale prior from generative painting models for zero-shot MMDE, we design a test-time adaptation framework, Metric from Human (MfH). Using humans as landmarks, we bridge from MRDE to MMDE by a generate-and-estimate pipeline.
3. Through qualitative and quantitative experiments, we demonstrate the superiority and generalization ability of our MfH in zero-shot MMDE, needless of any metric depth annotations.

## 2 Related Work

**Monocular Depth Estimation (MDE)** has garnered significant interest in recent years. Early approaches focused on supervised methods that predict either monocular metric depth estimation (MMDE) [16; 24; 25; 26; 27; 15] or monocular relative depth estimation (MRDE) [27; 28; 29; 9]. Despite remarkable progress in network architectures [30; 31; 32; 16; 33; 34; 15], existing MMDE methods often confine their training and testing to specific domains, leading to performance degradation under minor domain shifts and poor generalization to unseen environments. In contrast, relative depth models have demonstrated better generalization by leveraging scale-invariant losses [9; 35; 36] on diverse datasets. However, these models cannot recover metric scales, which are crucial for downstream applications. Recent works explored generalizable MMDE models [12; 11; 14; 10] for diverse domains, leveraging camera awareness through explicit incorporation of intrinsics [37; 11] or normalization based on camera properties [14; 38; 17]. They often require fine-tuning to adjust to specific domains [11; 10]. Several recent studies explore zero-shot MMDE, using language as a prior to ground predictions to metric scale [39; 40; 41]. However, their hand-crafted depth captions to connect the language and metric worlds are often too coarse to capture accurate depths. Our MfH instead distills metric scale priors from generative painting models, enhancing both the generalization capability and pixel-wise precision of zero-shot MMDE models without relying on metric depth annotations.

**Human Mesh Recovery (HMR)** aims to reconstruct 3D human bodies from visual inputs. Optimization-based HMR relies on iterative optimization techniques to fit parametric body models such as SMPL to detect image features. Examples include SMPLify  and its variants [23; 43], which iteratively minimize an objective function to align the model with 2D key points and silhouettes. In contrast, feed-forward methods [44; 45; 46; 47; 48; 49] directly regress the body shape and pose parameters from a single image using deep learning techniques. Among them, HMR 2.0  is a fully transformer-based approach for recovering 3D human meshes from single images. We adopt it as our HMR model for estimating in-the-wild human structures and poses. With HMR, we derive metric scale priors from generative painting models, thereby bridging generalizable MRDE to zero-shot MMDE.

## 3 Method

Taking an RGB image \(^{H W 3}\) with its camera intrinsic \(^{3 3}\) as input, we aim to estimate its pixel-wise metric depths \(^{}^{H W 1}\). Unlike existing MMDE methods [12; 11; 14; 10] that typically train on images with metric annotations and expect the trained model to generalize to unseen inputs, we instead consider a test-time adaptation scenario. That is, to estimate metric depths on a certain image without training on domain-specific metric annotations. To achieve this, we propose a framework to learn a metric head for each input \(\), as depicted in Fig. 4. The metric head is concatenated after an off-the-shelf MRDE model to transform relative depths into metric depths. While the inference pipeline is simplistic, our key insight is to use humans as metric landmarks during test-time training. Since humans are not guaranteed to exist in in-the-wild images, we introduce a generate-and-estimate method in Sec. 3.2 to extract metric scale information from the given image. This extracted information together with the estimated relative depths allows us to approach annotation-free zero-shot metric depth estimation, as outlined in Sec. 3.3.

### Preliminaries

#### 3.1.1 Monocular Depth Estimation (MDE)

Assuming a pinhole camera model, we have the following relation

\[^{}=^{}},\ ^{}=(^{})=^{}-( ^{})}{(^{})},\] (1)

where \(b\) and \(f\) are the camera baseline and focal length, \(^{}\) and \(^{}\) are metric depths and disparities, and \(()\) and \(()\) are scalar functions, denoting a scale and a translation to normalize the input. We use superscript \({}^{}\) and \({}^{}\) to refer to metric and relative values, accordingly. Due to the correlation between camera parameters and the scale of depth, MMDE cannot generalize well if the camera intrinsic is unknown or the scene scale is hard to predict, e.g., when the scene is unseen during training. In contrast, MRDE enjoys better generalization ability for its affine-invariant formulation.

#### 3.1.2 Human Mesh Recovery (HMR)

We adopt a state-of-the-art HMR model, HMR 2.0 , for reconstructing camera-frame human meshes from an image \(}_{n}^{H W 3}\) with \(K\) people. Starting with human segmentation masks \(\{_{nk}^{H W 1}\}_{k=1}^{K}\) from Mask R-CNN , HMR 2.0 predicts SMPL  parameters for each

Figure 4: **The framework of Metric from Human (MH). Our pipeline comprises two phases. The test-time training phase learns a metric head that transforms relative depths into metric depths based on images randomly painted upon the input image and the corresponding pseudo ground truths. After training the metric head, the inference phase estimates metric depths for the original input.**human as \(\{_{nk},_{nk},_{nk},_{nk}\}_{k=1}^{K}\). These parameters represent global orientation \(_{nk}^{3 3}\), body pose \(_{nk}^{22 3 3}\), shape \(_{nk}^{10}\), and root translation \(_{nk}^{3}\). Then the human body meshes with vertices \(_{nk}^{3 6890}\) can be recovered with the SMPL model

\[_{nk}=(_{nk},_{nk},_{ nk})+_{nk},k=1,2,,K.\] (2)

Since the shape space of SMPL is defined in _meters_, the generated vertices \(\{_{nk}\}\) are also in _meters_. As a result, all HMR regressors, such as HMR 2.0 we use, inherit such data prior.

### Generating Humans as Metric Landmarks

To start off, we randomly place people on the input image with generative image painting, with text prompts \(\{_{n}^{}\}_{n=1}^{N}\) sampled from \(\{\}\) and mask prompts \(\{_{n}^{}^{H W 1}\}_{n=1}^{N}\) sampled from rectangles smaller than the whole image. We write \(_{n}=(_{n}^{},}_{n}^{})\) as a shorthand. Once at a time, we randomly generate \(N\) painted images with humans

\[}_{n}=(|_{n}), { where }n=1,2,,N,\] (3)

and \(\{}_{n}^{H W 3}\}_{n=1}^{N}\) are the painted images. We observe the generative image painting model can paint people of proper sizes that are compatible with the unmasked background. This allows us to use the painted people as landmarks to inform our model of the metric scale. To this end, we fed these painted images into HMR 2.0 to predict human instance segmentation masks \(\{_{nk}\}_{n=1,k=1}^{N,K}\) and meshes \(\{_{nk}\}_{n=1,k=1}^{N,K}\), where the subscripts denote the \(k\)-th person in the \(n\)-th image. We obtain the pseudo metric ground truths \(_{n}^{*}\) with rasterization

\[_{n}^{*}=_{k}[(_{nk} _{nk})_{nk}],\] (4)

where \(_{nk},_{nk}=(,_{nk})\) are the rasterized silhouettes and depths, respectively, and \(\) stands for the Hadamard product. We erode the intersection of the instance segmentation mask \(_{nk}\) and the rasterized silhouette \(_{nk}\) to avoid overlapping and take the minimum of \(k\) depth maps so that the depth values from closer humans can occlude the farther ones. Using these pseudo ground truths, we supervise the learning of the metric head with the scale-invariant log (\(_{}\)) loss 

\[_{_{}}(}_{n}^{},_{n}^{*})=_{i}_{i}^{2}-}( _{i}_{i})^{2},\] (5)

where \(=}_{n}^{}-_{n}^{*}\), with \(}_{n}^{}\) being the estimated metric depth map, subscript \({}_{i}\) denotes the index of each pixel, and \(\). Since the rasterized depths \(_{nk}\) are in _meters_ and the first term in \(_{_{}}\) is pixel-wise \(l_{2}\), this loss provides crucial metric scale information to the metric head.

### Transforming Relative Depths into Metric Depths

During training, we estimate a relative depth map \(}_{n}^{}\) for each painted image \(}_{n}\) with a pre-trained MRDE model, and learn a metric head to transform \(}_{n}^{}\) into an estimated metric depth map \(}_{n}^{}\). Similarly, we obtain \(^{}\) and \(^{}\) from the original input \(\) during inference. According to Eq. (1), we can learn a simple linear layer as our metric head, i.e., \(}_{n}^{}=}_{n}^{}+ \). In addition, we need to account for the difference between the relative depths predicted from the original image \(^{}\) and those from the painted images \(\{}_{n}^{}\}\) on the unpainted regions. Considering the affine-invariant nature of MRDE, we further decompose metric depth predictions with

\[}_{n}^{}=s(s_{n}}_{n}^{}+t_{n})+t=s}_{n}^{}+t,}_{n}^{}=s_{n}}_{n}^{}+t_{n},\] (6)

and \(\{s_{n}\},\{t_{n}\},s,t\) are optimizable parameters. Then we align \(^{}\) and \(\{}_{n}^{}\}\) with

\[_{}(}_{n}^{},^{})=\|(1-_{n}^{})(}_{n}^{}-^{})\|_{2}.\] (7)

This objective considers the pixel-wise alignment of unpainted regions with an affine transform specific to each painted image. Finally, we formulate our complete objective function as

\[_{s,t}_{n}_{_{}}(}_{n}^{ },_{n}^{*})+_{n}_{s_{n},t_{n}}_{}( }_{n}^{},^{}).\] (8)Such formulation propagates metric scale information from human pixels to background pixels, thereby enabling the metric head to predict for the non-human context. After optimization, it is then possible to infer metric depths for the original input image. While one can deploy fancier metric head and apply up-to-affine consistency constraints between aligned relative depth predictions \(}_{n}^{}\) and metric depth predictions \(}_{n}^{}\) for better robustness, we observe a simple affine transform is capable of providing good predictions, leaving further parameterization for future works.

## 4 Experiments

### Experimental Setting

**Datasets.** Under our test-time adaptation setting, we do not train on any datasets but only test on each input image directly after image-specific optimizations. Specifically, we evaluate the zero-shot MMDE capability of MfH on NYU-Depth V2 , IBims-1 , ETH-3D  with the split from  and official masks, and KITTI  with the corrected Eigen-split from . Following prior works [12; 15; 16], we apply the Eigen evaluation mask  on NYU-Depth V2 and IBims-1 while the Garg evaluation mask  on KITTI.

**Evaluation Metrics.** We employ several common metrics to assess the performance of all baseline methods and our model. The \(_{1}=_{i=1}^{HW}[(}}{D _{}^{}},}}{D_{}})<1.25]\) metric evaluates the fraction of predicted depth values that are within a threshold factor of their corresponding true values; the Mean Absolute Relative Error, \(=_{i=1}^{HW}}-D_{}| }{D_{}}\), measures the average absolute difference between the predicted and true depth values, normalized by the true depth; the Scale Invariant Logarithmic Error, \(_{}=100( D_{}- D_{ })}\), quantifies the error in a logarithmic scale that is invariant to the absolute scale of the scene; the Root Mean Squared Error, \(=_{i=1}^{HW}(D_{}-D_{})^ {2}}\), focuses on the square root of the mean of squared differences between the predicted and actual depth values, emphasizing larger errors.

**Implementation Details.** We adopt Depth Anything  without fintuning on metric annotations as our MRDE model, Stable Diffusion v2  for generative painting, and HMR 2.0  for human mesh recovery. In \(_{_{}}\), we follow ZoeDepth  to set the \(=0.15\). For optimizing the alignment parameters \(\{s_{n}\},\{t_{n}\}\), we leverage linear regression to obtain a close-formed solution. As for optimizing the metric head parameters, \(s,t\), we use the L-BFGS optimizer with a fixed learning rate of 1 for 50 steps. Unless otherwise specified, we randomly paint 32 images for our comparison experiments and 4 for our ablation studies. All experiments are run on one NVIDIA A100 GPU.

### Comparison Results

We first evaluate the MMDE results on NYU-Depth V2  and KITTI , common benchmarks with indoor and outdoor scenes, respectively. In Tab. 1, we show that our zero-shot MfH consistently outperforms other approaches trained with few/one/zero-shot supervision. This indicates that generative painting models are capable of capturing metric scale information, which is potentially more accurate than that embedded in language models [40; 59]. With our generate-and-estimate pipeline, the metric scale prior hidden inside generative painting models can be leveraged for zero-shot MMDE.

    &  &  &  \\  & & \(_{1}\) & \(\) & \(_{}\) & \(\) & \(_{1}\) & \(\) & \(_{}\) & \(\) \\  ZeroDepth  & many-shot & 90.1 & 10.0 & – & 0.380 & 89.2 & 10.2 & – & 4.38 \\ Metric3D  & many-shot & 92.6 & 9.38 & 9.13 & 0.337 & 97.5 & 5.33 & 7.28 & 2.26 \\ UniDepth-C  & many-shot & 97.2 & 6.26 & 6.41 & 0.232 & 97.9 & 4.69 & 6.71 & 2.00 \\ UniDepth-V  & many-shot & 98.4 & 5.78 & 5.27 & 0.201 & 98.6 & 4.21 & 5.84 & 1.75 \\  LORN  & few-shot & 70.3 & 101 & – & 9.452 & – & – & – & — \\ Hu et al.  & one-shot & 42.8 & 34.7 & – & 1.049 & 31.2 & 38.4 & – & 12.29 \\ DepthCLIP  & zero-shot & 39.4 & 38.8 & – & 1.167 & 28.1 & 47.3 & – & 12.96 \\  MHH (Ours) & zero-shot & **83.2** & **13.7** & **9.78** & **0.487** & **81.2** & **13.3** & **10.5** & **4.21** \\   

Table 1: Performance comparisons of our MfH and state-of-the-art methods on the NYU-Depth V2  and KITTI  datasets. \({}^{}\)LORN uses 200 images and 2,500 partial images for training.

In Tab. 2, we further provide a comparison between Mfh and state-of-the-art many-shot methods, which are typically trained upon large-scale datasets with dense metric depth annotations. Our model achieves performance on par with, and sometimes superior to, these approaches, especially on ETH3D , which contains both indoor and outdoor scenes. It is noteworthy that these prior arts can do well on certain datasets while failing on others. For instance, ZeroDepth  performs well on iBims-1 but struggles to estimate depths on DIODE (Indoor) and ETH3D accurately. Similarly, UniDepth-V  shows promising results on DIODE (Indoor) while underperforming on the other two benchmarks. These findings signify the scene-dependent nature of existing fully supervised methods, which may result in degraded performance on unseen scenes. In contrast, our model demonstrates robust zero-shot generalization capabilities across diverse scenes. We further highlight the comparison among our Mfh and Depth Anything  fine-tuned on NYUv2 or KITTI (\(2^{}\)-\(3^{}\) rows). These methods adopt a common Depth Anything MRDE backbone while deploying different strategies for MMDE. The results demonstrate that our test-time adaptation strategy generally works better than domain-specific fine-tuning, without the need for training on metric depth annotations.

### Ablation Study

**Impact of MRDE models.** In Tab. 3, we investigate the performance of Mfh with different MRDE models and various optimization targets. For ZoeDepth  and Depth Anything , we only adopt their pre-trained MRDE backbone as our MRDE model. Note that we use ground truth depths or disparities as optimization targets to show an approximate upper bound of performances, while only accessing painted depths or disparities as pseudo ground truths during real test-time adaptation. We also ensure consistency by optimizing predictions in the same space as the target, i.e., the depth space for depth targets and the inverted depth space for disparity targets. Overall, we observe using Depth Anything as our MRDE model and painted disparities as our optimization target shows the best performance. Comparing the first two rows and the last two rows for each MRDE model, we see that optimizations in the disparity space yield superior results for ZoeDepth  and Depth Anything , whereas optimizations in the depth space prove more effective for Marigold . A similar scenario can also be found in the last two rows of each MRDE model. This variation in performance may stem from the difference in their output spaces. To be concrete, ZoeDepth and Depth Anything produce inverted depths, while Marigold outputs depths. Optimizing in the original output space can provide better numerical stability, leading to better optimization results.

    & \)} &  &  &  \\  & & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & \(\)th\(\) & RMSE \(\) \\  ZoeDepth-NK  & 38.8 & 33.0 & 13.3 & 1.59 & 61.0 & 18.7 & 8.98 & 0.77 & 33.5 & 47.3 & 1.40 & 2.094 \\ Depth Anything-N  & 29.7 & 32.7 & 12.5 & 1.48 & 17.3 & 15.0 & 7.88 & 0.94 & 25.2 & 38.7 & 10.2 & 2.327 \\ Depth Anything-K  & 11.1 & 23.1 & 15.5 & 5.19 & 2.88 & 21.7 & 17.2 & 5.385 & 16.9 & 136 & 17.1 & 4.02 \\ ZeroDepth  & 43.2 & 30.0 & 13.2 & 1.92 & 74.6 & 16.4 & 10.6 & 6.634 & 31.2 & 32.6 & 1.34 & 1.926 \\ Metric  & - & 26.8 & -- & 1.429 & -- & **14.4** & -- & 0.646 & -- & 34.2 & -- & 2.965 \\ UniDepth-C  & 62.8 & 23.8 & 11.5 & 0.968 & **81.1** & 14.8 & 5.30 & **0.536** & 33.3 & 35.5 & 10.3 & 1.532 \\ UniDepth-V  & **79.8** & **18.1** & **10.4** & **0.76** & 23.4 & 35.7 & **6.87** & 1.063 & 27.2 & 43.1 & 8.93 & 1.950 \\ Mfh (Ours) & 42.2 & 34.5 & 13.2 & 1.363 & 67.7 & 23.3 & 9.73 & 0.738 & **47.1** & **24.0** & **8.16** & **1.36** \\   

Table 2: Performance comparisons of our Mfh and many-shot methods on the DIODE (Indoor) , iBims-1 , and ETH3D  datasets. *-{N, K, NK}: fine-tuned on NYUv2 , KITTI , or the union of them. We re-evaluate all results with a consistent pipeline for metric completeness.

**Impact of optimization parameters.** In Tab. 4, we verify the benefits of aligning relative depth estimations of the original input and painted images with \(\{s_{n}\},\{t_{n}\}\) and parameterizing the metric head with \(s,t\). We view the results from bottom to top. First, optimizing all parameters (7\({}^{}\)-8\({}^{}\) rows) yields the lowest error. When we fix the scaling factor to 1 while keeping the other parameters optimizable (5\({}^{}\)-6\({}^{}\) rows), the model has the highest error and cannot be aligned with the ground truth. Instead, when using optimizable scales (3\({}^{}\)-4\({}^{}\) rows) in the metric head, the model can better capture depths, which indicates an accurate scene scale is crucial in MMDE. Removing the alignment between the input image MRDE and painted image MRDEs (1\({}^{}\)-2\({}^{}\) rows) results in sub-optimal predictions since the same contents on two different images might result in distinct MRDE predictions. The performance difference of using different optimization parameters while the true disparity as the target (3\({}^{}\) vs. 5\({}^{}\) vs. 7\({}^{}\) row) shows it is possible to apply an affine transformation upon MRDE to achieve good MMDE predictions, if with accurate scale and translation recovered.

**Impact of loss functions.** In Tab. 5, we ablate the effect of using various loss functions for test-time training the metric head. Notably, employing an \(l_{1}\) loss (1\({}^{}\) row) yields inferior performance compared to losses incorporating an \(l_{2}\) term (2\({}^{}\)-4\({}^{}\) rows). This is probably because our generate-and-estimate process can introduce noises to a certain degree. Since the \(l_{1}\) loss treats all errors equally, regardless of their magnitude, it can be more sensitive to small perturbations. An \(l_{2}\) term that focuses more on large errors thus provides better robustness. Furthermore, a comparison between the 2\({}^{}\) and the last two rows shows optimizing in the log space brings better performance, which is expected since logarithmic transformation tends to mitigate the impact of outliers. This also accords with the general experience in training depth models using ground truth annotations, suggesting that the depths of generated humans might also be normally distributed in the log space, akin to real-world scenarios. From the last two rows, we see optimizing with the \(_{}\) loss or the \(_{}\) loss is not discrepant by much. We opt for the \(_{}\) loss in our optimization process due to its relatively superior \(\).

**Impact of painting numbers \(N\).** In Fig. 5, we analyze the effect of increasing the number of painted images with humans. Specifically, we paint 4, 8, 16, and 32 images with humans, plotting curves as well as error bars for various metrics against the painting numbers. To draw reliable conclusions, we conduct experiments across five consistent random seeds for each painted image quantity. Our analysis reveals a clear linear association between the per-sample runtime and the number of painted images, as depicted in the time plot. The \(_{1}\) and \(\) plots show an upward trend in MMDE performance with the increment in painting numbers, albeit sublinearly. By further examining the error bars, we see a larger number of painted images results in better robustness of predictions, which is demonstrated by a smaller, gradually converging standard deviation.

Figure 5: **Ablation study for the number of painted images on NYUv2. Horizontal axis: \(N\{4,8,16,32\}\) inpainted images.**

Figure 6: **An alternative perspective on MfH. (a) Using camera intrinsics, the estimated metric depths can be transformed into a metric point cloud. (b) Our generate-and-estimate process can be seen as aligning the estimated point cloud of each painted human to its corresponding human mesh.**

### Qualitative Analysis

We present an alternative view of our Mfh in Fig. 6. With camera intrinsics, the estimated metric depths for both the painted images \(\{}_{n}^{m}\}\) and the original input \(^{}\) correspond to metric point clouds. Our Mfh stretches the point clouds for \(\{}_{n}^{m}\}\) along the z-axis by setting human landmarks in 3D, revealing the 3D structure of the unpainted background in _meters_. With random painting, we progressively capture the 3D structure of the entire original input, thus bridging MRDE to MMDE.

We demonstrate metric depth predictions and pixel-wise \(\) in Fig. 7, highlighting the strong zero-shot generalization ability of our Mfh. Additionally, we show MMDE results for in-the-wild samples captured by DSLR cameras and smartphones in Fig. 8. Besides the robust performance of Mfh, we observe that fully supervised MMDE methods like UniDepth  often provide bounded metric depths, inheriting from the limited range of sensors used in their training ground truths. In contrast, our Mfh can provide more flexible results.

Case studies, user studies, and more qualitative analysis can be found in Appendix D.

## 5 Conclusion

We present Mfh, a method that infers metric depths from in-the-wild images without the need for training on metric depth annotations. Utilizing humans as landmarks to extract metric scale priors from generative painting models, our approach addresses the challenge of scene dependency inherent in MMDE trained with metric depth supervision. Through a test-time adaptation pipeline, Mfh effectively captures metric scale information from images by generating and estimating humans, which is then leveraged for zero-shot MMDE. Our experiments demonstrate that Mfh achieves superior performance and better generalization ability compared to existing methods.

Figure 7: **Zero-shot qualitative results. Each pair of consecutive rows corresponds to one test sample. Each odd row shows an input RGB image alongside the absolute relative error map, while each even row shows the ground truth metric depth and predicted metric depths.**

**Limitation discussion.** Our MfH works based on the assumption that humans can exist in the scene so that it is possible to paint a human upon the input image. While this holds for most usages of MMDE, it might not be ideal for some cases, e.g., close-up scenes. This opens up new challenges, such as incorporating objects other than humans into the generate-and-estimate pipeline as metric landmarks. Another assumption is the MRDE predictions align with true depths up to affine. Despite the training objectives of MRDE being linearly transformed true depths, the MRDE predictions can contain noises, making the linear metric head hard to capture accurate metric depths. Whether other parameterizations of the metric head can tackle this remains an open question.

**Broader impacts.** Our MfH decreases the demand for metric depth annotation which commonly requires depth sensors or stereo systems, making MMDE models more environmentally friendly. However, its usage of human-related models can perpetuate biases present in the training data, leading to unfair or discriminatory outcomes.