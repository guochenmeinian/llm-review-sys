# Spatio-Temporal Interactive Learning for Efficient Image Reconstruction of Spiking Cameras

Bin Fan\({}^{1}\)1  Jiaoyang Yin\({}^{2,3}\)1  Yuchao Dai\({}^{4}\)  Chao Xu\({}^{1}\)  Tiejun Huang\({}^{2,3}\)  Boxin Shi\({}^{2,3}\)2

\({}^{1}\)Nat'l Key Lab of General AI, School of Intelligence Science and Technology, Peking University

\({}^{2}\)State Key Lab of Multimedia Info. Processing, School of Computer Science, Peking University

\({}^{3}\)Nat'l Eng. Research Ctr. of Visual Technology, School of Computer Science, Peking University

\({}^{4}\)School of Electronics and Information, Northwestern Polytechnical University

{binfan,shiboxin,tjhuang}@pku.edu.cn, yinjaioyang@stu.pku.edu.cn, xuchao@cis.pku.edu.cn, daiyuchao@nwpu.edu.cn

Equal contribution.Corresponding author.

###### Abstract

The spiking camera is an emerging neuromorphic vision sensor that records high-speed motion scenes by asynchronously firing continuous binary spike streams. Prevailing image reconstruction methods, generating intermediate frames from these spike streams, often rely on complex step-by-step network architectures that overlook the intrinsic collaboration of spatio-temporal complementary information. In this paper, we propose an efficient spatio-temporal interactive reconstruction network to jointly perform inter-frame feature alignment and intra-frame feature filtering in a coarse-to-fine manner. Specifically, it starts by extracting hierarchical features from a concise hybrid spike representation, then refines the motion fields and target frames scale-by-scale, ultimately obtaining a full-resolution output. Meanwhile, we introduce a symmetric interactive attention block and a multi-motion field estimation block to further enhance the interaction capability of the overall network. Experiments on synthetic and real-captured data show that our approach exhibits excellent performance while maintaining low model complexity. The code is available at https://github.com/GitCVfb/STIR.

## 1 Introduction

High-speed imaging has become a high-profile topic in fields such as autonomous driving, industrial monitoring, and robotics, due to its ability to precisely capture the continuous light intensity behaviour in a scene. Conventional digital cameras often rely on expensive specialized sensors when capturing fast-moving objects, so the trade-off between frame rate and cost has limited the widespread adoption and further development of high-speed cameras. In recent years, neuromorphic cameras, especially event cameras  and spiking cameras , have emerged as innovative vision sensors. They possess characteristics such as high temporal resolution, high dynamic range, and low latency, opening up new possibilities for high-speed imaging of consumer-grade cameras.

The spiking camera achieves integral sampling with 40,000Hz by emulating the central fovea's sampling mechanism in the retina . Each photoreceptive unit continuously and independently captures photons, and asynchronously fires spikes once the accumulated intensity exceeds a given threshold. Unlike event cameras that only record relative changes in light intensity (_i.e._, differential sampling), spiking cameras have the ability to encode the absolute light intensity because the spike firing rate is proportional to the scene brightness. Consequently, the spiking camera can preservemore sufficient scene texture information, making it highly promising for pixel-level tasks, such as image reconstruction [65; 55; 3; 8], depth estimation [53; 46], semantic segmentation [52; 64], and optical flow estimation [23; 59; 49]. However, spiking cameras solely record dense binary time-sequence information, making it difficult to directly apply existing vision algorithms designed for conventional frame-based cameras. To reconstruct dynamic scene content from asynchronous spike streams, traditional methods either exploit the temporal statistical characteristics , _e.g._, texture from playback (TFP) and texture from inter-spike-intervals (TFI), or mimic the human physiological mechanisms, _e.g._, retina-like visual imaging  and short-term plasticity [63; 62]. Nonetheless, noise and motion blur frequently present a tricky trade-off throughout the dynamic scene reconstruction process, which could lead to less than ideal reconstruction results. In contrast, deep learning-based methods [55; 4; 57], with their powerful representation capabilities to mine latent spatio-temporal cues from spike streams through end-to-end learning, offer a more promising way to address the dynamic scene reconstruction problem of spiking cameras.

Deep learning-based methods usually cascade three independent modules: spike embedding representation (SER), temporal motion estimation, and spatial intensity recovery, as illustrated in Fig. 1 (a). The first module [56; 52; 60; 59; 58; 49] typically extracts time-series information from the spike stream, serving as an essential bridge between the binary spikes and the deep model. The temporal motion estimation module either explicitly estimates the motion field [56; 9] or implicitly establishes temporal motion correlations (_e.g._, deformable convolution [55; 60], attention [4; 5]), aiming to align context in the feature space. Following this, an additional spatial intensity recovery module [56; 4; 60] is added to reconstruct the intermediate frame from the aligned feature representations. Although this design paradigm of first estimating motion and then reconstructing images has achieved reasonably good results, it hinders the information interaction and joint optimization in time and space, creating a bottleneck for further improving the image reconstruction quality of spiking cameras. On the one hand, motion estimation and intensity recovery are inherently a "chicken-and-egg" problem: more accurate motion modeling will lead to better intermediate frame reconstruction, and vice versa. On the other hand, this step-by-step combination tends to reduce inference efficiency, which is detrimental to efficient deployment in real-world applications.

In this paper, we point out that temporal motion estimation and spatial intensity recovery can be mutually reinforcing, as shown in Fig. 1 (b). To this end, we design an efficient **S**patio-**T**emporal **I**nteractive **R**econsuction network, termed **STIR**. Specifically, we first deliver a concise hybrid spike embedding representation (HSER) into a hierarchical feature encoder to obtain pyramid features at different granularities. Then, a spatio-temporal interactive decoder is proposed to enable the joint refinement of spatio-temporal complementary information from coarse to fine. In particular, inter-frame feature alignment and intra-frame feature filtering can be performed simultaneously. The former mainly focuses on temporal motion cues to complete warping-based feature registration, while the latter progressively maintains purer image features through synthesis. In addition, we integrate a symmetric interactive attention block at the top-level pyramid and introduce a multi-motionfield estimation block at the bottom-level pyramid, further upgrading the network's spatio-temporal interaction ability. To fully harness the network's potential, a simple yet effective HSER module is also devised, which incorporates the common advantages of explicit spike representation based on internal statistics (with better certainty and explainability) and implicit spike representation based on neural networks (with stronger expressive power). Extensive experimental results on synthetic and real-captured data demonstrate that our approach significantly outperforms state-of-the-art (SOTA) image reconstruction methods, with a 1.35dB improvement in PSNR while also enjoying fast inference speed, as shown in Fig. 2.

The main contributions of this paper can be summarized as follows:

1. We propose STIR, an efficient and flexible framework for image reconstruction of spiking cameras, which facilitates joint learning of complementary motion and intensity information.
2. We design a symmetric interactive attention block that enhances the bilateral correlation between the intermediate frame and temporal contextual features.
3. We develop a simple yet effective hybrid spike embedding representation module with both good interpretability and strong expressive power.

## 2 Related Works

**Neuromorphic Cameras.** Neuromorphic cameras mimic neurobiological structures and functionalities of the retina. Different from conventional frame-based cameras, they operate asynchronously at the pixel level, allowing each pixel to act independently. Two main types of neuromorphic cameras include event cameras, _e.g._, DVS , DAVIS , ATIS , CeleX , and spiking cameras [7; 26]. Event cameras utilize a differential sampling approach, triggering events only when changes in illuminance surpass a specific logarithmic threshold. Conversely, spiking cameras follow an integral sampling method, where photon accumulation leads to spike firing once a given threshold is reached. Therefore, event cameras produce sparser outputs, while spiking cameras provide a more regular input format for reconstructing absolute light intensity.

**Event-to-image Reconstruction.** Deep learning-based methods for reconstructing intensity images from events have demonstrated significant progress. E2VID  is a seminal work for this purpose by using a recurrent fully convolutional network. Following E2VID, numerous studies have augmented it from various angles, including FireNet , E2VID++ , FireNet++ . Also, SPADE layers and Transformer were integrated into E2VID in [2; 48], which enhanced the quality but at a higher cost. HyperE2VID  used hypernetworks to generate per-pixel adaptive filters and adopted a dynamic neural network architecture. However, since event cameras solely record changes in relative light intensity, they struggle to reconstruct the texture details of visual scenes.

**Spike-to-image Reconstruction.** In the task of spike-to-image reconstruction, traditional methods usually leverage the temporal statistical properties of spiking cameras. Zhu _et al_.  explored the spike generation principle and proposed two basic methods, _a.k.a._, TFP and TFI. Zhao _et al_.  hierarchically merged short- and long-term filtering. Another line of work focuses on mimicking human physiological mechanisms [66; 65; 63; 62]. Deep learning techniques have also propelled advancements in this challenging task. Spk2ImgNet  was the first CNN-based architecture and achieved impressive results. The wavelet transform was combined with CNN-based learnable modules in . Recently, an energy-efficient scheme was developed  based on the spiking neural network (SNN). High-dynamic-range and high-frame-rate videos were generated in  by introducing the rolling readout mechanism [13; 11; 16; 14; 15; 17; 18; 12]. Furthermore, several self-supervised CNNs [4; 5] have also been developed to alleviate the dependence on synthetic datasets. However, due to the step-by-step paradigm, the above CNN-based architectures inevitably have higher model complexity, blocking them from mobile and real-time applications. In contrast, our STIR model jointly considers temporal motion estimation and spatial intensity recovery, thus facilitating the intrinsic collaboration of spatio-temporal complementary information.

## 3 Preliminaries

### Working Mechanism of the Spiking Camera

The spiking camera employs an "integrate-and-fire" mechanism. Each pixel independently and continuously receives photons from the scene and converts them into photoelectrons, which are then accumulated via an integrator. When the accumulated photoelectrons exceed the predetermined threshold, the spiking camera asynchronously fires a spike, while clearing the pixel's photoelectrons to start a new accumulation cycle. The working mechanism can be formulated as [56; 26; 61]:

\[_{t}()=_{0}^{t} I_{}() ,\] (1)

where \(_{t}()\) reflects the number of photoelectrons accumulated at pixel \(=(x,y)\) in the integrator. \(I_{}()\) represents the light intensity at pixel \(\) at timestamp \(\). The photoelectric conversion rate is denoted as \(\) and the firing threshold is set to \(\). Assume that \(\) (in microsecond level) is used to quantify a spike accumulation cycle, the spiking camera can output a dense binary spike plane with a spatial resolution of \(H W\) at timestamp \(n,n\). As a result, during an "integrate-and-fire" process with a temporal length of \(N\), the spatio-temporal resolution of the spike stream \(_{t}^{N}\) will reach \(H W N\), where \(t\) denotes the central timestamp of \(_{t}^{N}\).

### Problem Statement

Given a continuous binary spike stream \(_{t_{1}}^{3N}\{0,1\}^{H W 3N}\) with a spatio-temporal resolution of \(H W 3N\), centered at timestamp \(t_{1}\), similar to [4; 5], we divide it evenly into three non-overlapping spike sub-streams \(_{t_{0}}^{N}\), \(_{t_{1}}^{N}\), and \(_{t_{2}}^{N}\) in chronological order, centered at timestamps \(t_{0}\), \(t_{1}\), and \(t_{2}\), respectively. This paper aims to reconstruct an intermediate intensity frame \(I_{t_{1}}\) corresponding to timestamp \(t_{1}\). Notably, under such a problem setting, in order to recover \(I_{t_{1}}\) successfully, \(_{t_{1}}^{N}\) can be utilized to model the intrinsic representation of spatial features corresponding to \(t_{1}\), while \(_{t_{0}}^{N}\) and \(_{t_{2}}^{N}\) can be exploited to complement the contextual information in the temporal domain.

Figure 3: Overview of our STIR framework (a) and details of the key components (b), (c), and (d). In our joint learning architecture, spatio-temporal features are refined progressively from coarse to fine, where warping-based inter-frame feature alignment (Orange line) and synthesis-based intra-frame feature filtering (Purple dashed line) are simultaneously performed in (b). Integrating (c) and (d) at the top and bottom pyramids, respectively, can boost the spatio-temporal interaction of the network.

Methodology

### Overview

The overall network architecture is depicted in Fig. 3. We first propose a hybrid spike embedding representation module in Sec. 4.2, which characterizes the three spike sub-streams \(^{N}_{t_{0}}\), \(^{N}_{t_{1}}\), and \(^{N}_{t_{2}}\) as feature maps \(F_{t_{0}}\), \(F_{t_{1}}\), and \(F_{t_{2}}\) corresponding to timestamps \(t_{0}\), \(t_{1}\), and \(t_{2}\), respectively. Subsequently, to enable spatio-temporal interactions at more granularity, they are adopted to produce multi-scale pyramid features, including intermediate features \(\{F^{l}_{t_{1}}\}_{l=1}^{L}\) and temporal contextual features \(\{F^{l}_{t_{0}}\}_{l=1}^{L},\{F^{l}_{t_{2}}\}_{l=1}^{L}\), through a weight-sharing hierarchical feature encoder in Sec. 4.3. Here, \(L\) indicates the number of pyramid levels. Then, a compact symmetric interactive attention block is leveraged in Sec. 4.4 to initially model the bilateral correlations between the top-level pyramid features \(^{N}_{t_{1}}\) and \(\{^{N}_{t_{0}},^{N}_{t_{2}}\}\). Finally, warping-based inter-frame feature alignment and synthesis-based intra-frame feature filtering are jointly executed across the spatio-temporal interactive decoder in Sec. 4.5. Therefore, progressive motion-intensity collaboration is achieved in a single encoder-decoder. Additionally, we estimate \(G\) groups of motion fields at the bottom-level pyramid, which helps to improve the performance and robustness of the whole model.

### Hybrid Spike Embedding Representation

The spike embedding representation is dedicated to mining time-series information from the input spike stream, serving as a crucial link between the spike stream and the deep network model. To encode the corresponding light intensity features from the spike stream, a simple strategy is to utilize _explicit_ spike representation approaches based on internal statistics, such as TFP  or TFI . This approach builds on the temporal statistical characteristics of spiking cameras, which can physically provide good interpretability and relatively stable intensity frames for spike-to-image reconstruction tasks like video frame interpolation tasks [30; 29; 45]. Nevertheless, this strategy often struggles to balance noise and motion blur, resulting in limited feature expression capabilities. Another more effective way is to _implicitly_ engineer more robust features via CNNs [52; 55; 5; 4; 49; 58; 60]. However, due to the lack of certainty, the spike embedding features obtained in this manner will change when the network parameters are updated during training, limiting the efficient alignment of context in the temporal motion estimation process. In summary, we hope to seek a tractable spike embedding representation method that not only offers good certainty and strong expressive capability but also maintains low computational cost.

To this end, we propose HSER to combine the advantages of explicit and implicit spike representations. Specifically, for the input spike sub-stream \(^{N}_{t_{i}},i=\{0,1,2\}\), we first obtain multiple explicit spike representations using the widely-used TFP method  based on varying temporal windows. This is inspired by [55; 4], because short windows give better details but bring noise, while long windows can suppress noise but easily introduce blur. At the same time, we also feed \(^{N}_{t_{i}}\) into a ResNet  block for implicit modeling. Finally, these resulting features are concatenated along the channel dimension, and then the spike embedding feature \(F_{t_{i}}\) is generated through a 2D convolution.

### Hierarchical Feature Encoder

After obtaining the spike embedding features \(F_{t_{0}}\), \(F_{t_{1}}\), and \(F_{t_{2}}\) corresponding to the three continuous spike sub-streams, we set them as the bottom-level features \(F^{1}_{t_{0}}\), \(F^{1}_{t_{1}}\), and \(F^{1}_{t_{2}}\) of the feature pyramid. On this basis, a hierarchical feature encoder is designed to build an \(L\)-level feature pyramid, such that multi-granularity feature representations \(\{F^{l}_{t_{0}}\}_{l=1}^{L}\), \(\{F^{l}_{t_{1}}\}_{l=1}^{L}\), and \(\{F^{l}_{t_{2}}\}_{l=1}^{L}\) are extracted from the spike streams. Note that the network parameters are shared across the three spike sub-streams. At each level of the pyramid, we use a \(3 3\) 2D convolution with a stride of 2 for feature downsampling, followed by a residual block . Additionally, a PReLU activation  is appended after each 2D convolution. The number of feature channels at the \(l\)-level pyramid is \(C_{l}\). In the following, \(\{F^{l}_{t_{0}}\}_{l=1}^{L}\), \(\{F^{l}_{t_{1}}\}_{l=1}^{L}\), and \(\{F^{l}_{t_{2}}\}_{l=1}^{L}\) will facilitate inter- and intra-frame interactive learning from coarse to fine.

### Symmetric Interactive Attention

Recently, the transformer has demonstrated its capability to model long-range correlations between features in various visual tasks [32; 53; 33; 39; 48]. To inject prior motion-intensity guidance into the subsequent interactive decoder, we present an effective and efficient symmetric interactive attention block. It leverages the multi-head cross-attention mechanism at the top-level pyramid to symmetrically capture the mutual dependencies between \(F_{t_{i}}^{L}\) and \(\{F_{t_{0}}^{L},F_{t_{2}}^{L}\}\). Especially, we utilize the intermediate feature \(F_{t_{i}}^{L}\) as the _query_, while making temporal contextual features \(F_{t_{0}}^{L}\) and \(F_{t_{2}}^{L}\) as _key/value_ separately, to ensure symmetric interaction with the _query_.

As illustrated in Fig. 3 (c), we linearly project each component (_i.e._, _query_, _key_, and _value_) by applying layer normalization. In this way, the intermediate feature \(F_{t_{1}}^{L}\) is projected into two _queries_, \(Q_{t_{0}}\) and \(Q_{t_{2}}\), respectively. At the same time, the temporal contextual features \(F_{t_{0}}^{L}\) and \(F_{t_{2}}^{L}\) are projected into two _keys_, \(K_{t_{0}}\) and \(K_{t_{2}}\), as well as two _values_, \(V_{t_{0}}\) and \(V_{t_{2}}\), respectively. The attention-based bilateral correlations can be symmetrically computed as follows:

\[_{t_{1} t_{i}}=(}^{T}Q _{t_{i}}}{_{i}})V_{t_{i}},\ \ i=0,2,\] (2)

where \(_{i}\) denotes the learnable scaling parameter used to control the magnitude of the dot product. Similar to [51; 6; 50], we perform multi-head _query-key_ feature interaction along the channel rather than spatial dimensions, which can effectively enhance computational efficiency due to linear complexity instead of quadratic. By aggregating local and non-local contexts, the final interaction feature \(^{L}^{H/2^{L-1} W/2^{L-1} 3C_{L}}\) can be yielded as:

\[^{L}=([_{t_{1} t_{0}},F_{t_{1}}^{L}, _{t_{1} t_{2}}]),\] (3)

where \([\ ]\) denotes a channel-wise concatenation operation and Conv is a point-wise convolution layer Note that the intrinsic intermediate feature \(F_{t_{1}}^{L}\) is preserved through skip connections.

### Spatio-Temporal Interactive Decoder

Instead of using a step-by-step network architecture like [56; 4; 5; 60], we propose to jointly and progressively perform temporal motion estimation and spatial intensity recovery (_cf._, Fig. 1 (b)), thereby maximizing their complementary advantages across spatio-temporal contextual features. Specifically, we develop an efficient motion-intensity interactive block to simultaneously accomplish warping-based inter-frame feature alignment and synthesis-based intra-frame feature filtering, which is inspired by well-established event-based video frame interpolation methods [45; 44; 27]. Notably, warping can integrate light intensity information over the time series, while synthesis can mitigate the influence of spike fluctuations. By cascading multiple motion-intensity interactive blocks from coarse to fine granularity, the intermediate frame can be progressively decoded. Moreover, at the bottom-level pyramid, we propose to predict multiple motion fields to aggregate more comprehensive temporal contexts, which is beneficial to further improve image reconstruction quality.

**Motion-Intensity Interactive Block.** The network details are depicted in Fig. 3 (a) and (b). At the top-level pyramid, \(^{L}\) is input to the motion-intensity interactive block, which simultaneously estimates the motion fields \(M_{t_{1} t_{0}}^{L},M_{t_{1} t_{2}}^{L}\) and synthesizes the intermediate intensity frame \(I_{t_{1}}^{L}\) using a dense block . Subsequently, at the \(L-1\) level of the pyramid, \(M_{t_{1} t_{0}}^{L}\) and \(M_{t_{1} t_{2}}^{L}\) are upsampled to backward warp the temporal contextual features \(F_{t_{0}}^{L-1},F_{t_{2}}^{L-1}\), thereby registering the spatio-temporal information around timestamps \(t_{0}\) and \(t_{2}\) to the intermediate timestamp \(t_{1}\). This process is referred to as _inter-frame feature alignment_, where feature warping is expressed as:

\[_{t_{i}}^{L-1}=(F_{t_{i}}^{L-1}, M_{t_{i}  t_{i}}^{L}),\ \ i=0,2,\] (4)

where \(\) indicates the upsampled variables, \(_{t_{i}}^{L-1}\) represents the warped feature candidate at the \(L-1\) level. Meanwhile, we bilinearly upsample \(I_{t_{1}}^{L}\) and then concatenate it with the corresponding intermediate feature \(F_{t_{1}}^{L-1}\), followed by a 2D convolution to reduce the influence of spike fluctuations. This process essentially implements _intra-frame feature filtering_ through the synthesis of the upsampled frame \( I_{t_{1}}^{L}\) and the intermediate feature \(F_{t_{1}}^{L-1}\). Formally,

\[S_{t_{1}}^{L-1}=([F_{t_{1}}^{L-1}, I_{t_{1}}^{L }]).\] (5)

Note that motion-based warping is more effective in handling significant pixel displacements but is less robust to occlusions. Conversely, intensity-based synthesis exhibits better robustness to occlusions and inconsistent brightness but may degrade image quality in short-time spike sub-streams. To this end, we further purchase a dense block to merge the complementary advantages of warping-based and synthesis-based features, _i.e._,

\[I_{t_{1}}^{L-1},M_{t_{1} t_{0}}^{L-1},M_{t_{1} t_{2}}^{L-1}=([S_{t_{1}}^{L-1},_{t_{0}}^{L-1},_{t_{2}}^{L-1},  M_{t_{1} t_{0}}^{L}, M_{t_{1} t_{2}}^{L}] ).\] (6)

Therefore, information sharing and mutual collaboration of spatio-temporal features can be achieved by progressively refining the motion-intensity interactive blocks from \(L\)-level to \(1\)-level pyramids.

**Multi-Motion Field Estimation Block.** Multi-motion field estimation has been proven to be a feasible strategy to improve reconstruction quality in video frame interpolation tasks [30; 24; 29]. Inspired by this, we simply amplify the output channels in the bottom-level pyramid to estimate \(G\) groups of motion fields \(\{M_{t_{1} t_{0}}^{1,g},M_{t_{1} t_{2}}^{1,g} g[1,G]\}\). Hence, \(G\) groups of warped feature candidates can be appended with diversity at full resolution, as shown in Fig. 3 (d). This is beneficial for compensating additional details when local inaccuracies occur in a single group of motion fields, thereby enhancing spatio-temporal interaction capabilities. The analyses are detailed in Sec. 5.3.

### Loss Function

We employ the combination of reconstruction loss \(_{}\), perceptual loss \(_{}\), and multi-scale consistency loss \(_{}\) as the total loss function \(\) to train our network, namely,

\[=_{}+_{}_{}+_{},\] (7)

where we empirically set \(_{}\) to 0.2. The \(_{1}\) distance between the final predicted image and the ground truth image is measured in \(_{}\), _i.e._,

\[_{}=\|_{t_{1}}^{1}-I_{t_{1}}^{1} \|_{1}.\] (8)

We also introduce \(_{}\) to mitigate the blurry effect and preserve more details, that is,

\[_{}=\|_{}(_{t_{1}}^{1})- _{}(I_{t_{1}}^{1})\|_{2},\] (9)

where \(_{}\) is the feature extractor of the pre-trained VGG-Net. Furthermore, we propose \(_{}\) to force the multi-scale intermediate intensity frames \(\{I_{t_{1}}^{l}\}_{l=2}^{L}\), synthesized from the \(2\)-level to \(L\)-level pyramids, to be consistent with the ground truth. The \(_{1}\) distance can be formulated as follows:

\[_{}=_{l=2}^{L}} \|_{t_{1}}^{l}-I_{t_{1}}^{l}\|_{1}.\] (10)

## 5 Experiments

### Implementation Details

**Datasets.** We adopt the recently released SREDS dataset , which is synthesized based on the REDS dataset , for network training. It is divided into 240 training scenes and 30 testing scenes.

   &  &  &  \\  & (M) & (G) & (T) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & NIQE\(\) & BRISQUE\(\) & NIQE\(\) & BRISQUE\(\) \\  TFP  & – & – & – & 25.35 & 0.690 & 0.2547 & 5.970 & 43.074 & 9.342 & 45.202 \\ TFI  & – & – & – & 18.50 & 0.638 & 0.2590 & 4.518 & 44.933 & 10.09 & 58.309 \\ TFSTp  & – & – & – & 20.68 & 0.618 & 0.2761 & 5.348 & 51.697 & 10.92 & 64.566 \\  SSIR  & 0.38 & 10.4 & **0.24** & 32.61 & 0.919 & 0.0500 & 3.467 & 15.664 & 5.750 & 25.341 \\   ET-Net  & 16.7 & 17.7 & 0.52 & 34.57 & 0.938 & 0.0535 & 3.400 & 17.155 & 6.512 & 17.393 \\ HyperE2VID  & 10.7 & **6.87** & 0.43 & 36.37 & 0.947 & 0.0506 & 3.126 & 16.774 & 6.306 & 17.020 \\  Spk2ImgNet  & 3.76 & 14.6 & 9.17 & 36.13 & 0.950 & 0.0294 & 3.084 & 15.348 & 5.662 & 16.518 \\ WGSE  & 3.85 & 19.7 & 3.93 & 37.44 & 0.958 & 0.0241 & 3.032 & 15.555 & 5.620 & 16.154 \\  STIR (Ours) & 5.11 & 9.20 & 0.42 & **38.79** & **0.966** & **0.0183** & **2.915** & **14.835** & **5.394** & **15.854** \\  

Table 1: Quantitative comparisons against SOTA methods on the synthetic SREDS dataset  and real-captured dataset . Best and second-best results are **boldfaced** and underlined, respectively. Thanks to the spatio-temporal interaction, our approach consistently demonstrates optimal reconstruction performance, along with excellent parameter size, GPU memory usage, and FLOPs.

Each scene contains 24 consecutive frames, with a corresponding spike stream of \(N=20\) centered around each frame. The spatial resolution is \(1280 720\). During training, each scene is cropped non-overlappingly to \(96 96\), yielding a total of 21,840 patches. We evaluate our model using real-captured data, including: 1) The publicly available "momVidarReal2021"  and "recVidarReal2019"  datasets (with \(400 250\) resolution, containing high-speed motion of objects and cameras, and also used in ). 2) Real spike data (\(1000 1000\)) collected by ourselves using a spiking camera.

**Training Details.** Our model is trained using the Adam optimizer  for 150 epochs with a batch size of 8. The initial learning rate is 0.0001 and decays by a factor of 0.7 every 50 epochs. The temporal length of the input spike stream is 60, _i.e._, \(N=20\). The number of pyramid levels is set to 5, _i.e._, \(L=5\). Note that the reconstruction loss \(_{}\), perceptual loss \(_{}\), and multi-scale consistency loss \(_{}\) are used together to train our network. In our HSER module, we construct a 5-channel TFP-based explicit representation with a scaling step of 4, as well as an 11-channel ResNet-based implicit representation, for each spike sub-stream. Thus, the number of feature channels is 16, 24, 32, 64, and 96, respectively. Besides, 3 groups of motion fields are estimated at the bottom-level pyramid, _i.e._, \(G=3\). Spikes and ground truth images are randomly flipped vertically as well as rotated \(90^{}\), \(180^{}\), or \(270^{}\) during training. All models are trained and tested on a single NVIDIA RTX 3090 GPU.

**Evaluation Metrics.** We apply standard PSNR and SSIM metrics and learned perceptual metric LPIPS  to measure the visual quality quantitatively. Moreover, two non-reference image quality assessment metrics NIQE  and BRISQUE  are employed. A higher PSNR/SSIM (\(\)) or lower LPIPS/NIQE/BRISQUE (\(\)) score indicates better performance.

**Comparison Methods.** We compare our method with the following four types of baselines. 1) **Traditional methods**: TFP , TFI , and TFSTP . 2) **SNN-based**: SSIR , designed

Figure 4: Visual comparison on synthetic  (top) and real-captured  (bottom) data. Our method reconstructs precise boundaries of fast-moving objects with higher fidelity. Zoom in for more details.

for energy-efficient spike-to-image reconstruction. Note that, except for SSIR, the other comparison methods adopt CNNs. 3) **Event-based**: ET-Net  and HyperE2VID , where our proposed HSER is cascaded with the classical event-to-image reconstruction architectures. 4) **CNN-based**: Spk2ImgNet  and WGS , which are SOTA spike-to-image reconstruction methods.

### Comparison with SOTA Methods

As shown in Table 1, our approach significantly outperforms SOTA methods in terms of reconstruction accuracy on both synthetic and real datasets. Apart from SNN-based SSIR , which has limited performance despite being computationally efficient, our method enjoys the lowest model complexity and competitive model size. CNN-based spike-to-image methods incur high inference costs due to their step-by-step paradigm. Notably, on average, our model is \(11\) faster than Spk2ImgNet  and \(5\) faster than WGSE . Moreover, event-based architectures have limited adaptability. Traditional methods show unsatisfactory reconstruction quality due to restricted modeling power.

The qualitative results are presented in Fig. 4. We can see that our method produces perceptually more pleasing and higher-fidelity images, especially for the edges of fast-moving objects. For instance, in cases involving pedestrians, drones, and small balls, our method achieves sharper edges, less noise, and fewer blurring and aliasing artifacts. Note that our method also ensures fast reconstruction of intensity frames (_cf._, Fig. 2), which further highlights its potential for practical applications.

### Ablation Studies

To verify the effectiveness of the proposed method, we conduct a series of ablation studies from the perspective of network architecture and loss function on the SREDS dataset .

**Ablation on Spike Embedding Representation.** We implement various spike embedding representation methods, including explicit, implicit, and combined. The multi-dilated representation  stacks multiple dilated convolutions for a larger receptive field, while the hierarchical spatial-temporal (HiST) representation  integrates multi-scale 3D convolutions for feature fusion, both of which have been applied for optical flow estimation. As shown in Table 2, despite

   & PSNR & SSIM & LPIPS \\  w/o \(_{}\) & 36.43 & 0.957 & 0.0227 \\ w/o \(_{}\) & 37.30 & 0.954 & 0.0446 \\ w/o \(_{}\) & 38.54 & 0.964 & 0.0194 \\ Total loss & **38.79** & **0.966** & **0.0183** \\  

Table 2: Ablation on spike embedding representation.

   & PSNR & SSIM & \#Paras & TFLOPs \\  Removing & 38.09 & 0.962 & 4.967 & 0.4186 \\ Independent & 38.23 & 0.963 & 5.318 & 0.4218 \\ Interactive & **38.79** & **0.966** & 5.107 & 0.4194 \\  

Table 3: **Ablation studies on the SREDS dataset . Underlining indicates our full model.**

   & PSNR & SSIM & \#Paras & TFLOPs \\  \( 0.5\) & 38.09 & 0.962 & **1.554** & **0.325** \\ \( 1.0\) & 38.79 & 0.966 & 5.107 & 0.419 \\ \( 1.5\) & 38.80 & **0.967** & 10.94 & 0.568 \\ \( 2.0\) & **38.86** & **0.967** & 19.06 & 0.770 \\  

Table 3: **Ablation studies on the SREDS dataset . Underlining indicates our full model.**

   & PSNR & SSIM & \#Paras & TFLOPs \\  \(3\)-level & 37.99 & 0.960 & **0.832** & **0.377** \\ \(4\)-level & 38.02 & 0.962 & 2.119 & 0.403 \\ \(5\)-level & **38.79** & **0.966** & 5.107 & 0.419 \\  

Table 3: **Ablation studies on the SREDS dataset . Underlining indicates our full model.**being straightforward, ResNet  proves to be a relatively more workable spike representation method. Our HSER organically combines TFP and ResNet, which takes full advantage of both explicit and implicit representations, thus achieving the best result. Note that, even using only the simplest TFP , our method demonstrates competitive performance, which also demonstrates the effectiveness of our spatio-temporal interactive learning architecture.

**Ablation on Feature Pyramid Level.** We investigate the influence of varying hierarchical features. As shown in Table (a)a, even with just a 3-level pyramid, our method significantly outperforms existing CNN-based methods in terms of model size and computational efficiency, while still guaranteeing a leading performance. As the pyramid level increases, it will introduce finer-grained spatio-temporal interaction, which is conducive to achieving better image reconstruction quality.

**Ablation on Motion-Intensity Interactive Block.** As reported in Table (b)b, removing the warping-based inter-frame feature alignment results in sub-optimal performance, indicating that temporal contextual information is beneficial for intermediate frame recovery. Notably, the overall performance is severely degraded when the synthesis-based intra-feature filtering is removed, demonstrating the essential role of intermediate features in reconstructing the target frame. When the motion-intensity interaction is performed simultaneously from coarse to fine, a superior performance is obtained.

**Ablation on Symmetric Interactive Attention Block.** We either feed the top-level pyramid features directly into the decoder or use a standard cross-attention beforehand that independently models unilateral feature correlations. Due to the symmetric bilateral feature interaction, which facilitates more accurate context awareness, our method achieves superior performance, as shown in Table (c)c. Also, our interactive attention has small parameters and FLOPs, ensuring lightweight network design.

**Ablation on Multi-Motion Field Estimation Block.** We propose estimating multiple motion fields at the bottom-level pyramid to compensate for more contextual details. As shown in Table (d)d, using multiple groups of motion fields yields higher reconstruction quality, consistent with [30; 24]. As the number of motion fields increases, the model exhibits minor performance fluctuations. Still, it achieves gains over models based on a single group of motion fields.

**Ablation on Model Capacity.** We apply a width multiplier  to the feature channels based on the current configuration. Table (e)e presents that increasing the model capacity has a positive effect, indicating that our architecture is highly flexible. Particularly, the parameter size and computational cost of \( 0.5\) are ahead of SOTA methods, and it also has commendable reconstruction capability.

**Ablation on Loss Function.** We evaluate the impact of different loss terms in Table (f)f. It is evident that our total loss function is effective, as it performs the best when all loss terms are included.

## 6 Conclusion

In this paper, we proposed an efficient spike-to-image reconstruction method based on spatio-temporal interactive learning. In particular, a joint motion-intensity learning architecture was designed to perform inter-frame feature alignment and intra-frame feature filtering progressively. Moreover, we introduced a symmetric interactive attention block and a multi-motion field estimation block for bilateral correlation modeling and context detail compensation. Extensive experiments on synthetic and real data have demonstrated that our approach has excellent performance in reconstruction quality and inference speed, while also enjoying good flexibility and applicability.

**Limitations.** Our proposed HSER, similar to other spike embedding representation methods [56; 52; 60; 59; 58; 49], implicitly assumes that the scene has sufficient illumination, such that the image reconstruction can be achieved based on spike streams with a fixed temporal length. However, in extremely low-light scenarios, the limited accumulated light intensity within a fixed temporal length results in darker reconstructed images and increased noise, adversely affecting the visual experience. Note that this is a common problem for current spike-to-image reconstruction methods [56; 52; 65; 63; 57]. We plan to extend our model to handle these issues in future work.

**Acknowledgments.** This work was supported by National Science and Technology Major Project (2021ZD0109803), Beijing Natural Science Foundation (L233024), Beijing Municipal Science & Technology Commission, Administrative Commission of Zhongguancun Science Park (Z241100003524012), and National Natural Science Foundation of China (62088102, 62136001, 62276007, 62401021). Bin Fan was also supported by China National Postdoctoral Program for Innovative Talents (BX20230013) and China Postdoctoral Science Foundation (2024M750101).