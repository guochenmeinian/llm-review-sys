# Differentiable Blocks World:

Qualitative 3D Decomposition by Rendering Primitives

Tom Monnier\({}^{1}\) Jake Austin\({}^{2}\) Angjoo Kanazawa\({}^{2}\) Alexei A. Efros\({}^{2}\) Mathieu Aubry\({}^{1}\)

\({}^{1}\)LIGM, Ecole des Ponts, Univ Gustave Eiffel \({}^{2}\)UC Berkeley

###### Abstract

Given a set of calibrated images of a scene, we present an approach that produces a simple, compact, and actionable 3D world representation by means of 3D primitives. While many approaches focus on recovering high-fidelity 3D scenes, we focus on parsing a scene into _mid-level_ 3D representations made of a small set of textured primitives. Such representations are interpretable, easy to manipulate and suited for physics-based simulations. Moreover, unlike existing primitive decomposition methods that rely on 3D input data, our approach operates directly on images through differentiable rendering. Specifically, we model primitives as textured superquadric meshes and optimize their parameters from scratch with an image rendering loss. We highlight the importance of modeling transparency for each primitive, which is critical for optimization and also enables handling varying numbers of primitives. We show that the resulting textured primitives faithfully reconstruct the input images and accurately model the visible 3D points, while providing amodal shape completions of unseen object regions. We compare our approach to the state of the art on diverse scenes from DTU, and demonstrate its robustness on real-life captures from BlendedMVS and Nerfstudio. We also showcase how our results can be used to effortlessly edit a scene or perform physical simulations. Code and video results are available at www.tmonnier.com/DBW.

## 1 Introduction

Recent multi-view modeling approaches, building on Neural Radiance Fields , capture scenes with astonishing accuracy by optimizing a dense occupancy and color model. However, they do not incorporate any notion of objects, they are not easily interpretable for a human user or a standard 3D modeling software, and they are not useful for physical understanding of the scene. In fact, even though these approaches can achieve a high-quality 3D reconstruction, the recovered content is nothing but a soup of colorful particles! In contrast, we propose an approach that recovers textured primitives, which are compact, actionable, and interpretable.

More concretely, our method takes as input a collection of calibrated images of a scene, and optimizes a set of primitive meshes parametrized by superquadrics  and their UV textures to minimize a rendering loss. The approach we present is robust enough to work directly from a random initialization. One of its key components is the optimization of a transparency parameter for each primitive, which helps in dealing with occlusions as well as handling varying number of primitives. This notably requires adapting standard differentiable renderers to deal with transparency. We also show the benefits of using a perceptual loss, a total variation regularization on the textures and a parsimony loss favoring the use of a minimal number of primitives.

Our scene representation harks back to the classical Blocks World ideas . An important difference is that the Blocks World-inspired approaches are typically bottom-up, leveraging low-level image features, such as edges , super-pixels , or more recently learned features [72; 32], to infer3D blocks. In contrast, we perform a direct top-down optimization of 3D primitives and texture using a rendering loss, starting from a random initialization in the spirit of analysis-by-synthesis. Unlike related works that fit primitives to 3D point clouds [3; 1; 68; 38; 71; 42; 43] (Figure 1a), our approach, dubbed _Differentiable Blocks World_ (or DBW), does not require any 3D reconstruction _a priori_ but instead operates directly on a set of calibrated input images, leveraging photometric consistency across different views (Figure 1b). This makes our approach more robust since methods based on 3D are very sensitive to noise in the reconstructions and have difficulties dealing with incomplete objects. Our setting is similar to existing NeRF-like approaches, but our model is able to recover a significantly more interpretable and parsimonious representation. In particular, such an interpretable decomposition allows us to easily play with the discovered scene, _e.g_., by performing physics-based simulations (Figure 1c). Code and video results are available on our project webpage: www.tmonnier.com/DBW.

## 2 Related Work

Scene decomposition into 3D primitives.The goal of understanding a scene by decomposing it into a set of geometric primitives can be traced back to the very fist computer vision thesis by Larry Roberts on _Blocks World_ in 1963. In it, Roberts shows a complete scene understanding system for a simple closed world of textureless polyhedral shapes by using a generic library of polyhedral block components. In the 1970s, Binford proposes the use of Generalized Cylinders as general primitives , later refined by Biederman into the recognition-by-components theory . But applying these ideas to real-world image data has proved rather difficult.

A large family of methods does not consider images at all, instead focusing on finding primitives in 3D data. Building upon the classical idea of RANSAC , works like [4; 6; 62; 61; 39; 50; 57] accurately extract various primitive shapes (_e.g_., planes, spheres and cylinders for [62; 61; 39]) from a point cloud. In particular, MonteBoxFinder  is a recent RANSAC-based system that robustly extracts cuboids from noisy point clouds by selecting the best proposals through Monte Carlo Tree Search. To avoid the need for RANSAC hyperparameter tuning while retaining robustness, Liu _et al_.  introduce a probabilistic framework dubbed EMS that recovers superquadrics . Other methods instead leverage neural learning advances to robustly predict primitive decomposition from a collection of shapes (_e.g_., ShapeNet ), in the form of cuboids , superquadrics [55; 53; 71], shapes from a small dictionary [38; 36] or learnable prototypical shapes [10; 54; 43]. However, they are typically limited to shapes of known categories and require perfect 3D data. More generally, the decomposition results of all 3D-based methods highly depend on the quality of the 3D input, which is always noisy and incomplete for real scenes. For a complete survey of 3D decomposition methods, we refer the reader to .

Figure 1: **Differentiable Blocks World.****(a)** Prior works fit primitives to point clouds and typically fail for real data where ground-truth point clouds are extremely noisy and incomplete. **(b)** We propose using calibrated multi-view images instead and simultaneously tackle 3D decomposition and 3D reconstruction by rendering learnable textured primitives in a differentiable manner. **(c)** Such a textured decomposition is highly compact and user-friendly: it enables us to do physics-based simulations, _e.g_., throwing a ball at the discovered primitives.

More recently, there has been a renewed effort to fit 3D primitives to various image representations, such as depth maps, segmentation predictions or low-level image features. Depth-based approaches [27; 12; 40; 18; 32] naturally associate a 3D point cloud to each image which is then used for primitive fitting. However, the resulting point cloud is highly incomplete, ambiguous and sometimes inaccurately predicted, thus limiting the decomposition quality. Building upon the single-image scene layout estimation [23; 24], works like [21; 37] compute cuboids that best match the predicted surface orientations. Finally, Facade , the classic image-based rendering work, leverages user annotations across multiple images with known camera viewpoints to render a scene with textured 3D primitives. In this work, we _do not_ rely on 3D, depth, segmentation, low-level features, or user annotations to compute the 3D decomposition. Instead, taking inspiration from Facade  and recent multi-view modeling advances [69; 51; 45], our approach only requires calibrated views of the scene and directly optimizes textured primitives through photometric consistency in an end-to-end fashion. That is, we solve the 3D decomposition and multi-view stereo problems simultaneously.

Multi-view stereo.Our work can be seen as an end-to-end primitive-based approach to multi-view stereo (MVS), whose goal is to output a 3D reconstruction from multiple images taken from known camera viewpoints. We refer the reader to [22; 14] for an exhaustive review of classical methods. Recent MVS works can be broadly split into two groups.

Modular multi-step approaches typically rely on several processing steps to extract the final geometry from the images. Most methods [82; 16; 73; 74; 79; 20; 65], including the widely used COLMAP , first estimate depth maps for each image (through keypoint matching  or neural network predictions [73; 74; 79; 20; 65]), then apply a depth fusion step to generate a textured point cloud. Finally, a mesh can be obtained with a meshing algorithm [30; 34]. Other multi-step approaches directly rely on point clouds [15; 34] or voxel grids [64; 33; 26; 49]. Note that, although works like [26; 49] leverage end-to-end trainable networks to regress the geometry, we consider them as multi-step methods as they still rely on a training phase requiring 3D supervision before being applied to unknown sets of multi-view images. Extracting geometry through multiple steps involves careful tuning of each stage, thus increasing the pipeline complexity.

End-to-end approaches directly optimize a 3D scene representation using photometric consistency across different views along with other constraints in an optimization framework. Recent methods use neural networks to implicitly represent the 3D scene, in the form of occupancy fields , signed distance functions  or radiance fields, as introduced in NeRF . Several works incorporate surface constraints in neural volumetric rendering to further improve the scene geometry [52; 76; 70; 8; 13], with a quality approaching that of traditional MVS methods. Other methods [17; 80; 19; 48] instead propose to leverage recent advances in mesh-based differentiable rendering [44; 29; 41; 7; 58; 35] to explicitly optimize textured meshes. Compared to implicit 3D representations, meshes are highly interpretable and are straightforward to use in computer graphic pipelines, thus enabling effortless scene editing and simulation . However, all the above approaches represent the scene as a single mesh, making it ill-suited for manipulation and editing. We instead propose to discover the primitives that make up the scene, resulting in an interpretable and actionable representation. A concurrent work PartNeRF  introduces parts in NeRFs. However, only synthetic scenes with a single object are studied and the discovered parts mostly correspond to regions in the 3D space rather than interpretable geometric primitives.

## 3 Differentiable Blocks World

Given a set of \(N\) views \(_{1:N}\) of a scene associated with camera poses \(_{1:N}\), our goal is to decompose the 3D scene into geometric primitives that best explain the images. We explicitly model the scene as a set of transparent superquadric meshes, whose parameters, texture and number are optimized to maximize photoconsistency through differentiable rendering. Note that compared to recent advances in neural volumetric representations [51; 45; 78], we _do not_ use any neural network and directly optimize meshes, which are straightforward to use in computer graphic pipelines.

**Notations.** We use bold lowercase for vectors (_e.g._, \(\)), bold uppercase for images (_e.g._, \(\)), double-struck uppercase for meshes (_e.g._, \(\)) and write \(a_{1:N}\) the ordered set \(\{a_{1},,a_{n}\}\).

### Parametrizing a World of Blocks

We propose to represent the world scene as an explicit set of textured meshes positioned in the 3D space. Figure 2 summarizes our modeling and the parameters updated (top) during the optimization (bottom). Specifically, we model each scene as a union of primitive meshes: (i) an icosphere \(\) modeling a background dome and centered on the scene, (ii) a plane \(\) modeling the ground, and (iii) \(K\) primitive blocks \(_{1:K}\) in the form of superquadric meshes, where \(K\) is fixed and refers to a maximum number of blocks. Unless mentioned otherwise, we arbitrarly use \(K=10\). We write the resulting scene mesh \(_{1}_{K}\).

The goal of the background dome is to model things far from the cameras that can be well approximated with a planar surface at infinity. In practice, we consider an icosphere with a fixed location and a fixed scale that is much greater than the scene scale. On the contrary, the goal of the planar ground and the blocks is to model the scene close to the cameras. We thus introduce rigid transformations modeling locations that will be updated during the optimization. Specifically, we use the 6D rotation parametrization of  and associate to each block \(k\) a pose \(_{k}=\{_{k},_{k}\}^{9}\) such that every point of the block \(^{3}\) is transformed into world space by \(_{}=(_{k})+_{k}\), where \(_{k}^{3}\), \(_{k}^{6}\) and \(\) maps a 6D vector to a rotation matrix . Similarly, we associate a rigid transformation \(_{}=\{_{},_{}\}\) to the ground plane. We next describe how we model variable number of blocks via transparency values and the parametrization of blocks' shape and texture.

**Block existence through transparency.** Modeling a variable number of primitives is a difficult task as it involves optimizing over a discrete random variable. Recent works tackle the problem using reinforcement learning , probabilistic approximations  or greedy algorithms , which often yield complex optimization strategies. In this work, we instead propose to handle variable number of primitive blocks by modeling meshes that are _transparent_. Specifically, we associate to each block \(k\) a learnable transparency value \(_{k}\), parametrized with a sigmoid, that can be pushed towards zero to change the effective number of blocks. Such transparencies are not only used in our rendering process to softly model the blocks existence and occlusions (Section 3.2), but also in regularization terms during our optimization, _e.g_., to encourage parsimony in the number of blocks used (Section 3.3).

**Superquadric block shape.** We model blocks with superquadric meshes. Introduced by Barr in 1981  and revived recently by , superquadrics define a family of parametric surfaces that exhibits a strong expressiveness with a small number of continuous parameters, thus making a good candidate for primitive fitting by gradient descent. More concretely, we derive a superquadric mesh

Figure 2: **Overview. (top) We model the world as an explicit set of learnable textured meshes that are assembled together in the 3D space. (bottom) Starting from a random initialization, we optimize such a representation through differentiable rendering by photometric consistency across the different views.**from a unit icosphere. For each vertex of the icosphere, its spherical coordinates \([-,]\) and \([-,]\) are mapped to the superquadric surface through the parametric equation :

\[(,)=s_{1}^{_{1}}^{_{2} }\\ s_{2}^{_{1}}\\ s_{3}^{_{1}}^{_{2}},\] (1)

where \(=\{s_{1},s_{2},s_{3}\}^{3}\) represents an anisoptropic scaling and \(=\{_{1},_{2}\}^{2}\) defines the shape of the superquadric. Both \(\) and \(\) are updated during the optimization process. Note that by design, each vertex of the icosphere is mapped continuously to a vertex of the superquadric mesh, so the icosphere connectivity - and thus the icosphere faces - is transferred to the superquadric mesh.

**Texturing model.** We use texture mapping to model scene appearance. Concretely, we optimize \(K+2\) texture images \(\{_{},_{},_{}\}\) which are UV-mapped onto each mesh triangle using predefined UV mappings. Textures for the background and the ground are trivially obtained using respectively spherical coordinates of the icosphere and a simple plane projection. For a given block \(k\), each vertex of the superquadric mesh is associated to a vertex of the icosphere. Therefore, we can map the texture image \(_{k}\) onto the superquadric by first mapping it to the icosphere using a fixed UV map computed with spherical coordinates, then mapping the icosphere triangles to the superquadric ones (see supplementary material for details).

### Differentiable Rendering

In order to optimize our scene parameters to best explain the images, we propose to leverage recent mesh-based differentiable renderers [41; 7; 58]. Similar to them, our differentiable rendering corresponds to the soft rasterization of the mesh faces followed by a blending function. In contrast to existing mesh-based differentiable renderers, we introduce the ability to account for transparency. Intuitively, our differentiable rendering can be interpreted as an alpha compositing of the transparent colored faces of the mesh. In the following, we write pixel-wise multiplication with \(\) and the division of image-sized tensors corresponds to pixel-wise division.

**Soft rasterization.** Given a 2D pixel location \(\), we model the influence of the face \(j\) projected onto the image plane with the 2D occupancy function of  that we modify to incorporate the transparency value \(_{k_{j}}\) associated to this face. Specifically, we write the occupancy function as:

\[_{j}^{}()=_{k_{j}} ()}{},\ 0\;,\] (2)

where \(\) is a scalar hyperparameter modeling the extent of the soft mask of the face and \(_{j}()\) is the signed Euclidean distance between pixel \(\) and projected face \(j\), such that \(_{j}()<0\) if pixel \(\) is outside face \(j\) and \(_{j}() 0\) otherwise. We consider the faces belonging to the background and the ground to be opaque, _i.e._, use a transparency of 1 for all their faces in the occupancy function.

**Blending through alpha compositing.** For each pixel, we find all projected faces with an occupancy greater than a small threshold at this pixel location, and sort them by increasing depth. Denoting by \(L\) the maximum number of faces per pixel, we build image-sized tensors for occupancy \(_{}\) and color \(_{}\) by associating to each pixel the \(\)-th intersecting face attributes. The color is obtained through barycentric coordinates, using clipped barycentric coordinates for locations outside the face. Different to most differentiable renderers and as advocated by , we directly interpret these tensors as an ordered set of RGBA image layers and blend them through traditional alpha compositing :

\[(_{1:L},_{1:L})=_{=1}^{L}_ {p<}^{L}(1-_{p})_{}_ {}\;.\] (3)

We found this simple alpha composition to behave better during optimization than the original blending function used in [41; 7; 58]. This is notably in line with recent advances in differentiable rendering like NeRF  which can be interpreted as alpha compositing points along the rays.

### Optimizing a Differentiable Blocks World

We optimize our scene parameters by minimizing a rendering loss across batches of images using gradient descent. Specifically, for each image \(\), we build the scene mesh as described in Section 3.1and use the associated camera pose to render an image \(}\) using the rendering process detailed in Section 3.2. We optimize an objective function defined as:

\[=_{}+_{}_{}+_{}_{}+_{}_{ }\,,\] (4)

where \(_{}\) is a rendering loss between \(\) and \(}\), \(_{},_{},_{}\) are scalar hyperparameters and \(_{}\), \(_{}\), \(_{}\) are regularization terms respectively encouraging parsimony in the use of primitives, favoring smoothness in the texture maps and penalizing the overlap between primitives. Our rendering loss is composed of a pixel-wise MSE loss \(_{}\) and a perceptual LPIPS loss \(_{}\) such that \(_{}=_{}+_{} _{}\). In all experiments, we use \(_{}=0.01,_{}=_{}=0.1\) and \(_{}=1\). Figure 2 (bottom) shows the evolution of our renderings throughout the optimization.

**Encouraging parsimony and texture smoothness.** We found that regularization terms were critical to obtain meaningful results. In particular, the raw model typically uses the maximum number of blocks available to reconstruct the scene, thus over-decomposing the scene. To adapt the number of blocks per scene and encourage parsimony, we use the transparency values as a proxy for the number of blocks used and penalize the loss by \(_{}=_{k}}/\). We also use a total variation (TV) penalization  on the texture maps to encourage uniform textures. Given a texture map \(\) of size \(U V\) and denoting by \([u,v]^{3}\) the RGB values of the pixel at location \((u,v)\), we define:

\[_{}()=_{u,v}(\| [u+1,v]-[u,v]\|_{2}^{2}+\|[u,v+1]- [u,v]\|_{2}^{2})\,,\] (5)

and write \(_{}=_{}(_{})+ _{}(_{})+_{k}_{}(_{k})\) the final penalization.

**Penalizing overlapping blocks.** We introduce a regularization term encouraging primitives to not overlap. Because penalizing volumetric intersections of superquadrics is difficult and computationally expensive, we instead propose to use a Monte Carlo alternative, by sampling 3D points in the scene and penalizing points belonging to more than \(\) blocks, in a fashion similar to . Following , \(\) is set to \(1.95\) so that blocks could slightly overlap around their surface thus avoiding unrealistic floating blocks. More specifically, considering a block \(k\) and a 3D point \(\), we define a soft 3D occupancy function \(_{k}^{}\) as:

\[_{k}^{}()=_{k}( ()}{})\,,\] (6)

where \(\) is a temperature hyperparameter and \(_{k}\) is the superquadric inside-outside function  associated to the block \(k\), such that \(_{k}() 1\) if \(\) lies inside the superquadric and \(_{k}()>1\) otherwise. Given a set of \(M\) 3D points \(\), our final regularization term can be written as:

\[_{}=_{} _{k=1}^{K}_{k}^{}(),\ \,.\] (7)

Note that in practice, for better efficiency and accuracy, we only sample points in the region where blocks are located, which can be identified using the block poses \(_{1:K}\).

**Optimization details.** We found that two elements were key to avoid bad local minima during optimization. First, while transparent meshes enable differentiability w.r.t. the number of primitives, we observed a failure mode where two semi opaque meshes model the same 3D region. To prevent this behavior, we propose to inject gaussian noise before the sigmoid in the transparencies \(_{1:K}\) to create stochasticity when values are not close to the sigmoid saturation, and thus encourage values that are close binary. Second, another failure mode we observed is one where the planar ground is modeling the entire scene. We avoid this by leveraging a two-stage curriculum learning scheme, where texture maps are downscaled by 8 during the first stage. We empirically validate these two contributions in Section 4.3. We provide other implementation details in the supplementary material.

## 4 Experiments

### DTU Benchmark

**Benchmark details.** DTU  is an MVS dataset containing 80 forward-facing scenes captured in a controlled indoor setting, where the 3D ground-truth points are obtained through a structured light scanner. We evaluate on 10 scenes (S24, S31, S40, S45, S55, S59, S63, S75, S83, S105) that have different geometries and a 3D decomposition that is relatively intuitive. We use standard processing practices [77; 76; 8], resize the images to \(400 300\) and run our model with \(K=10\) on all available views for each scene (49 or 64 depending on the scenes). We use the official evaluation presented in , which computes the Chamfer distance between the ground-truth points and points sampled from the 3D reconstruction, filtered out if not in the neighborhood of the ground-truth points. We evaluate two state-of-the-art methods for 3D decomposition, EMS  and MonteboxFinder (MBF) , by applying them to the 3D ground-truth point clouds. We also evaluate them in a setup comparable to ours, where the state-of-the-art MVS method NeuS  is first applied to the multi-view images to extract a mesh, which is then used as input to the 3D decomposition methods. We refer to this input as "NeuS-mesh".

**Results.** We compare our Chamfer distance performances to these state-of-the-art 3D decomposition methods in Table 1. For each method, we report the input used and highlight the average number of discovered primitives #P in green (smaller than 10) or red (larger than 10). Intuitively, overly large numbers of primitives lead to less intuitive and manipulate scene representations. Our performances correspond to a single random run (random) and a run automatically selected among 5 runs using the minimal rendering loss (auto). We augment the best concurrent methods with a filtering step using RANSAC to remove the planar ground from the 3D input. Overall, we obtain results that are much more satisfactory than prior works. On the one hand, EMS outputs a reasonable number of primitives but has a high Chamfer distance reflecting bad 3D reconstructions. On the other hand, MBF yields a lower Chamfer distance (even better than ours with the filtering step) but it outputs a significantly higher number of primitives, thus reflecting over-decompositions.

Our approach is qualitatively compared in Figure 3 to the best EMS and MBF models, which correspond to the ones applied on the 3D ground truth and augmented with the filtering step. Because the point clouds are noisy and incomplete (see 360\({}^{}\) renderings in our supplementary material), EMS and MBF struggle to find reasonable 3D decompositions: EMS misses some important parts, while MBF over-decomposes the 3D into piecewise planar surfaces. On the contrary, our model is able to output meaningful 3D decompositions with varying numbers of primitives and very different shapes. Besides, ours is the only approach that recovers the scene appearance (last column). Also note that it produces a complete 3D scene, despite being only optimized on forward-facing views.

### Real-Life Data and Applications

We present qualitative results on real-life captures in Figure 4. The first row corresponds to the _Campanile_ scene from Nerfstudio repository  and the last four rows correspond to BlendedMVS scenes  that were selected in . We adapt their camera conventions to ours and resize the images to roughly \(400 300\). From left to right, we show a subset of the input views, a rendering overlaid with the primitive edges, the primitives, as well as two novel view synthesis results. For each scene, we run our model 5 times and automatically select the results with the minimal rendering loss. We set the maximum number of primitives to \(K=10\), except the last row where it is increased to

    & &  & Mean & Mean \\  Method & Input & S24 & S31 & S40 & S45 & S55 & S59 & S63 & S75 & S83 & S105 & CD & \#P \\  EMS  & NeuS-mesh & 8.42 & 8.53 & 7.84 & 6.98 & 7.2 & 8.57 & 7.77 & 8.69 & 4.74 & 9.11 & 7.78 & 9.6 \\ EMS  & 3D GT & 6.77 & 5.93 & 3.36 & 6.91 & 6.52 & 3.50 & 4.72 & 7.08 & 7.25 & 6.10 & 5.82 & 7.4 \\ MBF  & NeuS-mesh & 3.97 & 4.28 & 3.56 & 4.76 & 3.33 & 3.92 & 3.63 & 5.58 & 5.3 & 6.07 & 4.44 & 53.5 \\ MBF  & 3D GT & 3.73 & 4.79 & 4.31 & 3.95 & 3.26 & 4.00 & 3.66 & 3.92 & 3.97 & **4.25** & 3.98 & 16.4 \\
**Ours (random)** & Image & 5.41 & **3.13** & 1.57 & 4.93 & 3.08 & 3.66 & **3.40** & 2.78 & 3.94 & 4.85 & 3.67 & **4.6** \\
**Ours (auto)** & Image & **3.25** & **3.13** & **1.16** & **3.02** & **2.98** & **2.32** & **3.40** & **2.78** & **3.43** & 5.21 & **3.07** & 5.0 \\  EMS  + filter & 3D GT & 6.32 & 4.11 & 2.98 & 4.94 & 4.26 & 3.03 & 3.60 & 5.44 & 3.24 & 4.43 & 4.23 & 8.3 \\ MBF  + filter & 3D GT & **3.35** & **2.95** & **2.61** & **2.19** & **2.53** & **2.47** & **1.97** & **2.60** & **2.60** & **3.27** & **2.65** & 29.9 \\   

Table 1: **Quantitative results on DTU . We use the official DTU evaluation to report Chamfer Distance (CD) between 3D reconstruction and ground-truth, best results are highlighted. We also highlight the average number of primitives found (#P) in green (smaller than 10) or red (larger than 10). Our performances correspond to a single random run (random) and a run automatically selected among 5 runs using the minimal rendering loss (auto). We augment the best concurrent methods with a filtering step removing the ground from the 3D input.**Figure 4: **Qualitative results on real-life data.** We run our default model (\(K=10\)) on scenes from Nerfstudio  (first row) and BlendedMVS  (all other rows). The last row corresponds to results where the maximum number of primitives is increased to \(K=50\), yielding 17 effective primitives found.

Figure 3: **Qualitative comparisons on DTU .** We compare our model to state-of-the-art methods (augmented with a preprocessing step to remove the 3D ground) which, unlike ours, find primitives in the ground-truth point cloud that is noisy and incomplete. Additionally, our approach is the only one able to capture the scene appearance (last column).

\(K=50\) due to the scene complexity. These results show that despite its simplicity, our approach is surprisingly robust. Our method is still able to compute 3D decompositions that capture both appearances and meaningful geometry on a variety of scene types. In addition, increasing the maximum number of primitives \(K\) allows us to easily adapt the decomposition granularity (last row).

In Figure 5, we demonstrate other advantages of our approach. First, compared to NeRF-based approaches like Nerfacto  which only reconstruct visible regions, our method performs amodal scene completion (first row). Second, our textured primitive decomposition allows to easily edit the 3D scene (second row). Finally, our optimized primitive meshes can be directly imported into standard computer graphics softwares like Blender to perform physics-based simulations (bottom).

### Analysis

Ablation study.In Table 2, we assess the key components of our model by removing one component at a time and computing the performance averaged over the 10 DTU scenes. We report the final number of primitives, Chamfer distance and rendering metrics. We highlight the varying number of primitives in green (smaller than 5) and red (larger than 5). Results are averaged over five runs,

   Method & \#P \(\) & CD \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\  Complete model & \(4.60 0.23\) & \(\) & \(20.5 0.2\) & \(73.5 0.6\) & \(23.9 0.5\) \\ w/o \(_{}\) & \(8.86 0.27\) & \(3.65 0.22\) & \(\) & \(\) & \(\) \\ w/o \(_{}\) & \(4.38 0.19\) & \(3.80 0.30\) & \(20.4 0.3\) & \(73.2 0.7\) & \(24.1 0.7\) \\ w/o curriculum & \(4.66 0.30\) & \(3.99 0.17\) & \(20.4 0.2\) & \(72.7 0.5\) & \(24.5 0.4\) \\ w/o noise in \(_{1:K}\) & \(3.60 0.21\) & \(4.13 0.28\) & \(20.0 0.2\) & \(72.0 0.6\) & \(25.6 0.6\) \\ w/o \(_{}\) & \(4.04 0.18\) & \(4.58 0.42\) & \(19.7 0.3\) & \(70.8 1.3\) & \(26.5 1.2\) \\ w/o \(_{}\) & \(\) & \(4.80 0.20\) & \(19.7 0.1\) & \(72.7 0.3\) & \(40.0 0.4\) \\   

Table 2: **Ablation study on DTU .** We report metrics averaged over five runs: number of primitives (#P), Chamfer Distance (CD) and rendering metrics (PSNR in dB and SSIM, LPIPS in %). **Best** and **second** best are highlighted, #P variability is emphasized in green (smaller than 5) and red (larger than 5).

Figure 5: **Applications. (top)** Given a set of views constrained to limited viewpoint variations, we compare amodal view synthesis results using Nerfacto  and our approach. **(middle)** After optimization, we can easily modify the rendered scene by editing the different parts. **(bottom)** Our primitive-based representation enables straightforward physics-based simulations, such as throwing a ball at the objects or pouring water on the scene.

we report the means and standard deviations. Overall, each component except \(_{}\) consistently improves the quality of the 3D reconstruction and the renderings. \(_{}\) successfully limits the number of primitives (and thus, primitive duplication and over-decomposition) at a very small quality cost.

Influence of \(K\) and \(_{}\).In Table 3, we evaluate the impact of two key hyperparameters of our approach, namely the maximum number of primitives \(K\) and the weight of the parsimony regularization \(_{}\). Results are averaged over the 10 DTU scenes for 5 random seeds. First, we can observe that increasing \(K\) slightly improves the reconstruction and rendering performances at the cost of a higher effective number of primitives. Second, these results show that \(_{}\) directly influences the effective number of primitives found. When \(_{}=0.1\), this strong regularization limits the reconstruction to roughly one primitive, which dramatically decreases the performances. When \(_{}\) is smaller, the effective number of primitives increases without significant improvements in the reconstruction quality.

Limitations and failure cases.In Figure 6, we show typical failure cases of our approach. First, for a random run, we may observe bad solutions where parts of the geometry are not reconstructed (Figure 5(a)). This is mainly caused by the absence of primitives in this region at initialization and our automatic selection among multiple runs alleviates the issue, yet this solution is computationally costly. Note that we also tried to apply a Gaussian kernel to blur the image and propagate gradients farther, but it had little effect. Second, our reconstructions can yield unnatural decompositions as illustrated in Figure 5(b), where tea boxes are wrongly split or a single primitive is modeling the bear nose and the rock behind. Finally, in Figure 5(c), we show that increasing \(K\) from 10 (left) to 50 (right) allows us to trade-off parsimony for reconstruction fidelity. However, while this provides a form of control over the decomposition granularity, the ideal decomposition in this particular case does not seem to be found: the former seems to slightly under-decompose the scene while the latter seems to over-decompose it.

## 5 Conclusion

We present an end-to-end approach that successfully computes a primitive-based 3D reconstruction given a set of calibrated images. We show its applicability and robustness through various benchmarks, where our approach obtains better performances than methods leveraging 3D data. We believe our work could be an important step towards more interpretable multi-view modeling.

Figure 6: **Failure cases. We show typical failure cases of our approach. All models are optimized with \(K=10\) except the rightmost model which is optimized with \(K=50\). See text for details.**