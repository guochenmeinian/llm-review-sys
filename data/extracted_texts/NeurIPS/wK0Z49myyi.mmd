# CRAYM: Neural Field Optimization

via Camera RAY Matching

 Liqiang Lin\({}^{1}\)  Wenpeng Wu\({}^{1}\)  Chi-Wing Fu\({}^{2}\)  Hao Zhang\({}^{3}\)  Hui Huang\({}^{1}\)

\({}^{1}\)College of Computer Science and Software Engineering, Shenzhen University

\({}^{2}\)Department of Computer Science and Engineering, The Chinese University of Hong Kong

\({}^{3}\)School of Computing Science, Simon Fraser University

linliqiang2020@gmail.com  wenpenggg@gmail.com

 cwfu@cse.cuhk.edu.hk  haoz@sfu.ca  hhzhiyan@gmail.com

Corresponding author.

###### Abstract

We introduce _camera ray matching_ (CRAYM) into the joint optimization of camera poses and neural fields from multi-view images. The optimized field, referred to as a feature volume, can be "probed" by the camera rays for novel view synthesis (NVS) and 3D geometry reconstruction. One key reason for matching camera rays, instead of pixels as in prior works, is that the camera rays can be parameterized by the feature volume to carry both geometric and photometric information. Multi-view consistencies involving the camera rays and scene rendering can be naturally integrated into the joint optimization and network training, to impose physically meaningful constraints to improve the final quality of both the geometric reconstruction and photorealistic rendering. We formulate our per-ray optimization and _matched ray coherence_ by focusing on camera rays passing through _keypoints_ in the input images to elevate both the efficiency and accuracy of scene correspondences. Accumulated ray features along the feature volume provide a means to discount the coherence constraint amid erroneous ray matching. We demonstrate the effectiveness of CRAYM for both NVS and geometry reconstruction, over dense- or sparse-view settings, with qualitative and quantitative comparisons to state-of-the-art alternatives.

## 1 Introduction

Recent advances on multi-view 3D reconstruction have been propelled by the emergence of neural fields , including implicit functions  and radiance fields (NeRF) . A critical component to all image-to-3D reconstruction methods, including traditional approaches such as multi-view stereo (MVS) , is to obtain camera poses for the input images. In practice, the camera information may be available from the acquisition devices, e.g., through the GPS or inertial measurement unit (IMU), while in other cases, it is estimated, e.g., using structure-from-motion (SfM) . In both cases, these camera poses can be noisy, thus hindering the performance of the multi-view 3D reconstruction.

In light of the importance of having accurate camera poses, various methods have been proposed to improve their estimations. One line of approaches, which can be referred to as bundle-adjusting neural fields, jointly optimize  camera poses along with results from rendering and geometry (e.g., depth) estimation, where the camera rays are considered _independently_ for their roles in color and geometry prediction. By now, more works  realize the importance of exploiting correlations between input images, i.e., multi-view consistency, to impose additionalconstraints on the joint optimization. Along these lines, much effort has been invested into matching and optimization with respect to image features, whether convolutional or transformer-based.

Motivated by multi-view _spatial_ analysis, several works have proposed geometric constraints involving camera rays and projections [16; 32; 18]. Most recently, SPARF  defines a _re-projection_ loss as a spatial distance between image pixels to enforce that matched pixels between NeRF training images be back-projected onto the same 3D point. However, the effectiveness of this loss depends critically on how reliable the pixel correspondences are. In their work, these correspondences and their confidence estimates were both obtained by a pre-trained network , which is independent of the joint camera-scene optimization.

In this paper, we introduce _camera ray matching_ into the joint optimization of camera poses and a neural field, referred to as a _feature volume_, which can be "probed" by the camera rays for both rendering, e.g., novel view synthesis as in NeRF , and 3D reconstruction, as in NeuS .

The key reasons for matching camera rays, instead of pixels [32; 18; 6], are two-fold. First, these rays carry 3D spatial information than just 2D pixel values to facilitate formulating _explicit_ geometric losses when optimizing camera poses , as dictated by multi-view analysis. Second and more importantly, the camera rays can be _parameterized_ by the feature volume -- they carry both geometric and photometric information. Any constraint arising from camera ray matching can be passed onto the feature volume. Hence, both the matching itself and the associated matching confidence can be incorporated into the joint optimization and network training, to impose physically meaningful constraints to improve the final quality of both geometry reconstruction and rendering.

Our network, coined CRAYM (for Camera RAY Matching), takes as input an uncalibrated set of images capturing a 3D object, and is trained to predict the feature volume along with all the camera rays subjected to a combination of photometric rendering losses and geometric losses dedicated to ensuring multi-view consistency between camera rays. We consider two types of rays. The first are called _key rays_, which pass through keypoints detected in the input images, typically spanning regions with sharp features and rich textures over the 3D object. The other rays are called _auxiliary rays_, which pass through points around keypoints to offer contextual and local structural information as we reason about the key rays in our optimization framework.

As our main constraint for _matched ray coherence_, we enforce color consistency between renderings along two key rays whose corresponding keypoints from two different views are matched . However, we must account for potential erroneous matches due to occlusion or unreliable local image features used by the matching network. To this end, we aggregate features along each key ray through the feature volume. The matchability between two rays is defined by a cosine similarity between the accumulated ray features and applied as a weight to either accentuate or discount the

Figure 1: Our method, neural field optimization with camera ray matching (CRAYM), incorporates contextual information for per-ray processing and enforces color + geometric consistence between matched rays. Compared to SPARF  which utilizes dense pixel correspondences and the state-of-the-art, bundle-adjusting L2G-NeRF , both aimed at handling noisy camera poses, CRAYM produces superior results especially over fine details; see the zoom-ins on the right. Results are shown the Drums model from NeRF-Synthetic  on dense views.

color consistency constraint, allowing our optimization model to naturally degenerate itself to handle unrelated rays separately. Also, to improve the robustness of feature learning, we enhance the feature along each key ray by integrating features from surrounding auxiliary rays.

We evaluate our method on both the synthetic objects from NeRF-Synthetic  and the real scenes from UrbanScene3D , for novel view synthesis and 3D geometry reconstruction, over dense- and sparse-view settings. Compared to state-of-the-art alternatives, CRAYM produces superior results especially over fine details.

## 2 Related Works

Neural Fields.As a pioneer work, NeRF  synthesizes novel views of static objects/scenes from a set of posed images by optimizing a coordinate-based neural network, which predicts the volume density and color for a sampled point in the 3D space. Since then, numerous methods have emerged to improve the rendering quality [44; 13; 1] and rendering efficiency [10; 27; 19; 7]. To extract high-quality surfaces from the learned implicit representation, NeuS  and VolSDF  propose to learn an implicit signed distance field (SDF) representation for the scenes. These methods can achieve impressive results on both novel view synthesis and 3D reconstruction, however, the requirement of precise camera pose limits their applicability in practice.

Bundle-Adjusting Neural Fields.With the realization that positional encoding is susceptible to suboptimal registration, BARF  applies a smooth mask on the encoding at different frequency bands for a coarse-to-fine training, while  presents an adaptive positional encoding. L2G-NeRF  first learns the pixel-wise transformations for every pixel in a frame and then aligns the frame-wise transformation with the pixel-wise transformations. Common to all the above methods is that their joint optimization of pose and scene representation processes each image and each ray _separately_, without considering their multi-view correlations. As a result, the pose optimization may not be stable, thereby leading to floaters and blurriness in both novel view renderings and 3D reconstruction. Note that our method also involves per-ray processing, by combining information from auxiliary rays with that of a key ray. This is similar to the patch-level feature processing in CR-NeRF , which considers multiple rays indiscriminately across the image, without the notion of key rays.

Neural Fields with Image Matching.Image matching can help establish geometric priors to improve the generalizability of NeRF, to either novel scenes or the sparse-view setting. MVSNeRF  constructs a cost volume by warping the image features extracted with a 3D CNN onto a plane sweep, from which a generalizable radiance field is learned. SparseNeuS  constructs a 3D volume with the variance of all the projected features from multi-view images. DBARF  optimizes camera poses and depth with a cost map constructed by the differences of image features. CorresNeRF  proposes to regularize the NeRF training with a pixel re-projection loss for the associated pixels and a depth loss for the predicted depth. GPNR  aggregates features of the image patches along epipolar lines with several stacked transformers. MatchNeRF  learns a generalizable NeRF with the cosine similarity of image features for each image pair as the shape prior. All these methods integrate image features and utilize the matching within or between different views. With more emphasis placed on multi-view geometry reasoning, SCNeRF  learns a pinhole model for each camera under the supervision of a re-projected ray distance loss, while SPARF  optimizes its network with a re-projection loss, measuring spatial distances between pixels in the same view. In contrast, the matched ray coherence formulation in our optimization accounts for both photometric and geometry information as obtained from the feature volume; the coherence constraint is also explicitly integrated into the network instead of only serving to define a loss.

## 3 Method

We are interested in neural networks that can reconstruct a 3D model, e.g., a radiance field  or an implicit field , from a set of \(M\) images \(\{_{i}\}_{i=1}^{M}\) capturing a 3D object from multiple views. Typically, each image is associated with a known or estimated camera pose \(_{i}=[R_{i}|t_{i}]\), where \(R_{i}(3)\) and \(t_{i}^{3}\). The network is trained by minimizing a photometric error \(L_{p}\) between the input images and the multi-view renderings, \(\{}_{i}\}_{i=1}^{M}\), of the target 3D object from the camera views: \(_{i}_{x}\|_{i}(x)-}_{i}(x)\|_{2}^ {2}\), where \(_{i}(x)\) is the color of image \(_{i}\) at pixel \(x\).

Each pixel is associated with a specific ray in 3D from the object/scene, through the pixel center, towards the camera: \(\{(t)=_{}+t_{}|t 0\}\), where \(_{}\) is the camera center and \(_{}\) is the normalized view direction of ray \(\). The rendered color of ray \(\), i.e., the pixel color \(_{i}(x)\), can be produced using volume rendering by accumulating the color and opacity \(\) along the ray \(\).

Considering that the camera poses can be noisy, the reconstructed radiance field or implicit field may not produce clean and sharp renderings with details. At a high noise level, some methods may even fail to produce results; see examples shown in Sections 4.2 and 4.4. Beyond existing approaches that map \((t)\) to opaque density (or opacity) and color implicitly with a ray-wise network, we propose CRAYM to learn the implicit field by matching rays across different images and formulating geometric priors.

### The CRAYM Pipeline

Figure 2 overview our CRAYM pipeline. From the input images, our goal in the 3D neural field optimization is to construct a 3D feature volume \(\) to faithfully represent the target object. In detail, we represent feature volume \(\) using multi-resolution hash encoding  and end-to-end optimize it for the target object. The feature \(f(p)\) of point \(p\) in the 3D feature volume can be extracted by

\[f(p)=((p)),\] (1)

where \(\) is the progressive feature mask  for filtering out fine-level features during early iterations of the coarse-to-fine training. Very importantly, to account for the noise in the camera poses, we parameterize the transformation matrices of the cameras as variables in the joint optimization of the pose and implicit field with the feature volume \(\).

Figure 2: Overview of our CRAYM pipeline. After extracting keypoints (red dots) from input images and matching them using a pre-trained network, we train our CRAYM network to optimize a 3D feature volume \(\) which encodes both geometric and photometric information about the target 3D object and can be queried by camera rays for both novel view synthesis (via the Texture Network) and 3D reconstruction (via the Geometry Network). The volume optimization is subject to photometric losses through rendering along camera rays passing through the keypoints (i.e., the key rays), which is enhanced (in the KRE) by integrating features from auxiliary rays, i.e., rays passing through nearby auxiliary points (yellow dots) in the images. Matched ray coherence (MRC) is imposed on matched key rays, in terms of color consistency, while potentially mismatched rays can be identified by comparing accumulated features along the key rays through \(\). On top of the standard photometric loss, we introduce two geometric losses, the epipolar loss and point-alignment loss, to explicitly optimize ray-to-ray coherency to maximize the reconstruction quality of the feature volume.

As mentioned in the introduction, we consider two types of rays to probe the feature volume, i.e., _key rays_ and _auxiliary rays_. Both rays are issued from the cameras through the pixel centers. To obtain key rays \(\{_{k}\}\), which typically associate to surface points with rich textures and sharp features, we detect keypoints on each input image using SuperPoint  and perform point-to-point matching between image pairs using SuperGlue . Then, we can obtain a set of _sparse ray-to-ray matchings_ between image pairs. Note that these results may not be accurate for various reasons such as occlusion and unreliable matching, but they provide useful information for our pipeline to start with. As for the auxiliary rays \(\{_{a}\}\), they are sampled around the keypoints to provide contextual or local structural information when we reason about the key rays; see Section 3.2 for details.

Once optimized, the feature volume can be used for novel view synthesis or for multi-view 3D reconstruction. The color prediction for novel view synthesis is accomplished by a texture network \(_{t}\), as in a typical NeRF  setting, and the latter is accomplished by a geometry network \(_{g}\), as in a typical NeuS  setting. Specifically, the geometry network takes a 3D point \(p\) sampled along \(\) and the feature at \(p\) as input to produce an SDF value and then an opaque density \(\) to render the 3D object and extract the 3D reconstructions. Here, we propose the Key Rays Enrichment (KRE) module (Section 3.2) to improve the robustness in the process by enhancing the features along the key ray using the features sampled by the auxiliary rays.

Subsequently, the texture network takes the output features from geometry network, ray directions, and normal at \(p\) as inputs to predict color \(c(p)\) at point \(p\). Further, we design the Matched Rays Coherency (MRC) module (Section 3.3) to enhance the volume rendering quality by considering matchability between rays and learning to maintain coherency between ray matchings. Particularly, the MRC module can effectively reduce the influence of mismatched rays by disambiguating the camera ray matchings.

A pair of the matched key rays, \(}\) and \(}^{}\), are sampled with the corresponding auxiliary rays during each iteration. The geometry network, texture network, and feature volume optimization are jointly trained end-to-end. Besides the photometric loss, we formulate the epipolar loss and point-alignment loss (Section 3.4) to explicitly promote coherency among the ray matchings and boost performance.

### Key Rays Enrichment Module

As the input images are captured through a perspective projection, all rays in 3D through the same image should converge at a common camera point. In previous works, for each iteration, rays are optimized separately, so the pose optimization may not be stable. As different rays may back propagate gradients in different directions, the optimized poses may oscillate during the training. Hence, we introduce the KRE module to stabilize the optimization by learning structural information around each key ray. This is done by sampling auxiliary rays around the key ray to enrich the feature of the key ray with more contextual information:

\[f^{}(p_{k})=_{j}g(f(p_{k}),f(q_{j})),\] (2)

where \(p_{k}\) is a point along key ray \(_{k}\); \(\{q_{j}\}\) are points around \(p_{k}\) sampled along the \(j\)-th auxiliary ray around \(_{k}\); and function \(g\) fuses features \(f(p_{k})\) and \(f(q_{i})\). Then, we employ the geometry network \(_{g}\) to predict the SDF value at \(p_{k}\) and feature vector \(f^{}(p_{k})\), from which we can further obtain the color of point \(p_{k}\) with the texture network. Please refer to the supplemental materials for the details.

### Matched Rays Coherency Module

Next, we propose to learn the coherency of features accumulated in the 3D feature volume between the matched key rays. The purpose is to enhance the camera ray matching and account for imprecise ray matchings, since keypoints matching is performed only on local image features.

Similar to color accumulation in volume rendering, we calculate the aggregated feature along a key ray \(}\) as the feature of \(}\):

\[f(})=_{0}^{}(p_{k})(p_{k})f^{ }(p_{k})dt.\] (3)

The function \((}(t))=(-_{0}^{t}(s)ds)\) denotes the accumulated transmittance along key ray \(}\).

Essentially, the accumulated ray feature \(f(})\) is an integration of density-weighted features along a ray. When the network converges, the actual surface point at which ray \(}\) intersects with the paired matched key ray should have the highest density. Hence, we consider coherency between the matched rays to optimize the learning of the opacity density and point color, such that we can enhance the coherency of features accumulated along the matched key rays. In return, this will help to optimize the parameters and the 3D feature volume, when training the pipeline. Therefore, we fuse the rendered color \((_{k})\) of the matched rays based on the cosine similarity between their accumulated features:

\[(_{k})=w(_{k}^{})+(1-w)(_{k}),\] (4)

where \(w\) is the matchability calculated as the cosine distance between the accumulated features of the matched rays. When two key rays are mismatched, e.g., due to occlusion or ambiguities of weak texture areas and similar structures, our formulation can learn to degenerate itself to a form that separately optimizes individual rays.

### Loss Function

Further, we introduce the following two geometric losses to more explicitly promote the coherency of the ray matchings:

_Epipolar loss._ Given a pair of matched keypoints \(x_{k}\) and \(x_{k}^{}\) on two different input images, which associate with camera centers \(O\) and \(O^{}\), respectively, (see the illustration in Figure 3), we can estimate the depths at \(x_{k}\) and \(x_{k}^{}\) by using a depth accumulation formulation similar to Equation 3, and then project points \(x_{k}\) and \(x_{k}^{}\) into the 3D object space to obtain 3D locations \(p_{k}\) and \(p_{k}^{}\), respectively.

If the camera poses, the matchings, and the depths are precise, the two rays through \(x_{k}\) and \(x_{k}^{}\) should precisely intersect at a common point, say \(P\), on the target object surface, such that \(p_{k}\) and \(p_{k}^{}\) align with \(P\). Also, we denote \(e\) and \(e^{}\) as the epipolar points on the two images; these points are the image-space locations at which the line \(OO^{}\) intersects the two image planes; see Figure 3(a).

During the training, the depth estimation of \(x_{k}\) can vary, so \(p_{k}\) may vary along ray \(r_{k}\). If the camera poses are precise, the projection of \(p_{k}\) onto the image plane of the other camera should lie on the epipolar line \(e^{}O^{}\). In case of noisy camera poses, the projection of \(p_{k}\) may not lie exactly on \(e^{}O^{}\), so we explicitly enforce the epipolarity during the training by minimizing the distance between \(p_{k}\)'s projection and the epipolar line \(e^{}x_{k}^{}\) using

\[L_{e}=}_{i=1}^{N_{k}}(Proj(p_{k}),e^{}x_ {k}^{}).\] (5)

Since the epipolar loss is not affected by depth, we decouple the unreliable depth estimation from the epipolar loss with ray marching to constrain the camera poses.

_Point-alignment loss._ The epipolar loss focuses on enhancing the projection consistency for producing more precise camera poses. To complement it, we introduce the point-alignment loss to facilitate depth convergence for improving the reconstruction of fine details. In detail, we consider the triangle

Figure 3: Illustrating of our geometric losses. The red lines in the left subfigure are epipolar lines. The epipolar loss constrains the relative transformations between cameras, so that the projection of a keypoint \(p_{k}\) onto the image plane of the other camera should lie on the epipolar line \(e^{}x_{k}^{}\). With the camera poses constrained by the epipolar loss, the point-alignment loss further constrains the depth of \(x_{k}\) and \(x_{k}^{}\), aiming to align \(p_{k}\) and \(p_{k}^{}\) with \(P\).

formed by intersection point \(P\), line segment \(p_{k}O\), and line segment \(p_{k}^{}O^{}\) (Figure 3(b)), and aim to minimize the distance between points \(p_{k}\) and \(p_{k}^{}\) and align them:

\[L_{a}=}_{i=1}^{N_{k}}(p_{k},p_{k}^{}).\] (6)

Note that depth estimation may be unreliable and likely unstable early in the training, so we use the point-alignment loss only after a certain number of training iterations.

Overall loss.After constructing the epipolar loss \(L_{e}\) and the point-alignment loss \(L_{a}\), we put them together with the photometric loss \(L_{p}\) and SSIM loss \(L_{s}\) to form the overall loss function

\[L=_{1}*L_{p}+_{2}*L_{s}+_{3}*L_{e}+_{4}*L_{a},\] (7)

where \(L_{p}\) is calculated between the input images and the rendered images and is modeled as an MSE loss.

## 4 Results

We evaluate our method on the NeRF-Synthetic dataset  with eight synthetic objects (Section 4.2), the LLFF dataset , and the real scenes from the UrbanScene3D dataset  (Section 4.3). We compare our method on both novel view synthesis and 3D reconstruction with NeRF , NeuS , BARF , L2G-NeRF , PET-NeuS , SPARF , and BAA-NGP . Since NeRF , NeuS , and PET-NeuS  are designed for neural implicit field with fixed and precise poses, we set the camera transformations as variables to be optimized jointly with the neural field, as in our method.

While other methods optimize the radiance field, in which the target values, radiances, of points are more independent of each other, the optimization of SDFs in NeuS and PET-NeuS poses a challenge to the requirements of non-local geometric constraints to correctly form the shape, making them more vulnerable to unstable pose optimization. As the camera rays are parameterized by our feature volume to carry both geometric and photometric information, our geometric constraints on the camera ray matching can effectively lead to better optimization of the geometry. The joint optimization of camera pose and implicit SDF may also fail to produce results for NeuS and PET-NeuS, when the camera poses are at a high noise level. With the assistance of camera ray matching, CRAYM outperforms other methods on both novel view synthesis and 3D reconstruction at varying noise levels. We report the PSNR, SSIM, and LPIPS  for quantitative comparisons on novel view synthesis and Chamfer distance (CD) for the 3D reconstruction. A test-time photometric pose optimization is performed to evaluate these metrics, following prior works [21; 32; 5]. The quantitative evaluations on the other metrics are provided in the supplementary materials.

### Pose Alignment

To evaluate the registration quality of the optimized training poses, we use Procrustes analysis  to find the 3D similarity transformation that aligns the optimized training poses with the calibrated camera poses, following BARF . As Figure 4 shows, the optimized poses produced by CRAYM align well with the ground-truth poses with lower translation errors.

Figure 4: Visualization of the initial and optimized camera poses for the LEGO scene in the NeRF-Synthetic dataset . (Purple: ground-truth poses; blue: initial or optimized poses; red lines: translation errors.)

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

accuracy reconstructions. With the contextual information learned in the camera ray matching and the explicit utilization of matched rays, CRAYM is able to obtain better results for all the noise settings.

Ablation of major modules and losses.To demonstrate the efficiency of the proposed modules and geometric losses, we conduct an ablation study on the LEGO data sample. The results are reported in Table 6. Similar with BARF, which applies a smooth mask on the encoding at different frequency bands for neural radiance field, we apply a progressive feature mask on the hash encoding with a coarse-to-fine training of the neural implicit field as our baseline, which combines BARF  and NeuS2. The KRE module improves the robustness to noisy poses in the training, whereas the MRC module effectively enhances the quality of the volume renderings with ray matching. In addition to that, the proposed geometric losses further help our framework to obtain better camera pose optimization.

## 5 Conclusion and discussion

Our method, CRAYM, addresses the issue of noise camera poses for multi-view 3D reconstruction and view synthesis. The key idea is to jointly optimize a neural field and camera poses by incorporating contextual information (via KRE) and enforcing geometric and photometric consistency (via MRC and geometric losses) through camera ray matching.

Experiments demonstrate that our method outperforms state-of-the-art alternatives under various settings: dense- vs. sparse-views, and different noise levels. However, the implicit field and optimizable pose transformations may not converge when the poses are randomly initialized or extremely noisy. A stronger pose regularization prior to the field optimization may resolve this problem. Furthermore, the meshes extracted from the constructed SDFs may still contain messy inner structures over invisible areas.

A promising future work is to apply the ray matching to the 3D Gaussian splatting, which will greatly improve the rendering efficiency of CRAYM. However, extracting reconstructions with fine geometric structures from 3D Gaussians is still an open problem.

Finally, CRAYM has been designed to rely on sparse key rays for dense-view reconstruction, while a dense counterpart may bring up extra overhead. In our Matched Ray Coherency formulation, we explicitly account for potentially erroneous (i.e., low-quality) 2D matches by using the matchability between two rays as a weight to either accentuate or discount the color consistency constraint. In terms of sensitivity with respect to the density of the 2D matches, in our experiments, we have observed that even with sparse input views and sparsely distributed matched rays, CRAYM can still notably improve the optimization convergence. An effective approach to utilize ray matching for both sparse and dense inputs may further boost the performance of CRAYM.

   Method & PSNR\(\) & SSIM\(\) & LPIPS\(\) & CD\(\) \\  L2G-NeRF  & 27.71 & 0.91 & 0.06 & 0.12 \\ NeuS2  & 26.83 & 0.86 & 0.17 & 0.08 \\  Baseline & 27.30 & 0.91 & 0.10 & 0.06 \\ + KRE & 28.64 & 0.93 & 0.07 & 0.05 \\ + KRE + MRC & 30.41 & 0.95 & 0.04 & **0.04** \\ + \(L_{e}\) & 29.43 & 0.92 & 0.07 & 0.05 \\ + \(L_{e}\) + \(L_{a}\) & 29.95 & 0.94 & 0.06 & **0.04** \\ Our full pipeline & **31.60** & **0.96** & **0.03** & **0.04** \\   

Table 6: Ablation of major modules and losses.

   Method & PSNR\(\) & SSIM\(\) & LPIPS\(\) & CD\(\) \\   \\  NeRF  & 29.08 & 0.94 & 0.04 & 0.34 \\ NeuS  & 21.18 & 0.82 & 0.09 & 0.04 \\ BARF  & 28.33 & 0.93 & 0.05 & 0.36 \\ SPARF  & 22.73 & 0.80 & 0.25 & 0.09 \\ PET-NeuS  & 21.37 & 0.82 & 0.14 & 0.10 \\ L2G-NeRF  & 27.94 & 0.92 & 0.06 & 0.14 \\ CRAYM (ours) & **32.72** & **0.97** & **0.02** & **0.03** \\   \\  NeRF  & 24.86 & 0.88 & 0.09 & 0.34 \\ NeuS  & 21.76 & 0.83 & 0.14 & 0.05 \\ BARF  & 28.32 & 0.93 & 0.05 & 0.36 \\ SPARF  & 22.55 & 0.80 & 0.25 & 0.09 \\ PET-NeuS  & 21.34 & 0.82 & 0.11 & 0.55 \\ L2G-NeRF  & 27.75 & 0.92 & 0.06 & 0.21 \\ CRAYM (ours) & **32.68** & **0.97** & **0.02** & **0.04** \\   \\  NeRF  & 11.36 & 0.81 & 0.56 & 0.57 \\ NeuS  & N/A & N/A & N/A & N/A \\ BARF  & 14.48 & 0.69 & 0.29 & 0.04 \\ SPARF  & 22.47 & 0.80 & 0.25 & 0.09 \\ PET-NeuS  & N/A & N/A & N/A & N/A \\ L2G-NeRF  & 27.71 & 0.91 & 0.06 & 0.12 \\ CRAYM (ours) & **31.60** & **0.96** & **0.03** & **0.03** \\   

Table 5: Comparing different methods at varying noise levels.