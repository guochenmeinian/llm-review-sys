# K Safeguards

Stability and Generalization of Adversarial Training for Shallow Neural Networks with Smooth Activation

 Kaibo Zhang

Johns Hopkins University

Baltimore, MD 21218

kzhang90@jhu.edu

&Yunjuan Wang

Johns Hopkins University

Baltimore, MD 21218

ywang509@jhu.edu

&Raman Arora

Johns Hopkins University

Baltimore, MD 21218

arora@cs.jhu.edu

###### Abstract

Adversarial training has emerged as a popular approach for training models that are robust to inference-time adversarial attacks. However, our theoretical understanding of why and when it works remains limited. Prior work has offered generalization analysis of adversarial training, but they are either restricted to the Neural Tangent Kernel (NTK) regime or they make restrictive assumptions about data such as (noisy) linear separability or robust realizability. In this work, we study the stability and generalization of adversarial training for two-layer networks **without any data distribution assumptions** and **beyond the NTK regime**. Our findings suggest that for networks with _any given initialization_ and _sufficiently large width_, the generalization bound can be effectively controlled via early stopping. We further improve the generalization bound by leveraging smoothing using Moreau's envelope.

## 1 Introduction

Despite the remarkable performance of over-parameterized deep networks in real-world applications, recent studies have revealed that they are highly vulnerable to adversarial attacks. These attacks use maliciously crafted imperceptible perturbations designed to deceive trained neural networks during inference (Szegedy et al., 2013; Biggio et al., 2013). The lack of adversarial robustness has raised significant concerns for deploying neural network-based models in safety-critical applications. Therefore, it is crucial to design algorithms to learn robust models that can make reliable predictions on test data even in the presence of adversarial perturbations.

One principal approach to robust learning, adversarial training (Madry et al., 2018) (along with its variants (Zhang et al., 2019; Wang et al., 2020)), has proven to be an effective empirical defense mechanism against adversarial attacks. Naturally, this puts an emphasis on also developing a theoretical understanding of robust learning. To study the generalization performance of robust learning, one traditional approach is via uniform convergence (Khim and Loh, 2018; Yin et al., 2019; Awasthi et al., 2020; Mustafa et al., 2022), which provides the worst-case type uniform bounds for a given hypothesis class and are algorithm independent. Another line of work focuses on analyzing the convergence and generalization guarantees of adversarial training, yet they either focus on linear classifiers (Charles et al., 2019; Li et al., 2020; Zou et al., 2021; Chen et al., 2023), or introduce restrictive distribution assumptions such as (noisy) linear separability (Wang et al., 2024) or robust realizability (Mianjy and Arora, 2024). Therefore, it remains unclear whether we can derive theoretical results for adversarial training that extend beyond these simplifying assumptions.

In this work, we leverage a different machinery by analyzing adversarial training algorithm through the lens of uniform stability. Stability is a classical tool in learning theory that has been extensively studied in the literature (Bousquet and Elisseeff, 2002; Hardt et al., 2016). Uniform argument stability measures the difference in output parameters when an algorithm is run on two training sets that differ by only one sample. In the standard (non-robust) setting, Hardt et al. (2016) show a uniform stability bound of \(()\) after \(T\) iterations of gradient descent with step size \(\) on convex and smooth losses using a training dataset of size \(n\). They further provide a uniform stability bound of \((}{n})\) for smooth and non-convex losses with decaying step size \(=()\), where \(q(0,1)\) is a constant. The choice of decaying step size is common in the non-convex setting, as maintaining a constant step size leads to an exponentially increasing bound on uniform stability.

When it comes to the robust setting, the primary challenge lies in the non-smoothness of the robust (adversarial) loss. The robust loss is generally non-smooth even if the standard counterpart is smooth (Xing et al., 2021; Xiao et al., 2022). Previous work by Xing et al. (2021) studied the convex non-smooth adversarial losses and provide an additional term of \(()\) compared to the convex and smooth losses. Later Xiao et al. (2022) studied the general non-smooth adversarial losses by leveraging the approximate co-coercivity of the gradient and provide the bound with an additional term of \(( T)\) that grows linearly in \(T\), where \(\) is the size of adversarial perturbation in \(_{p}\) threat models. These works, while partially addressing the issue, only focus on general convex / non-convex functions. However, neural networks, which are a specific instance of non-convex functions and are widely used in practice, require further investigation.

In this work, we study the stability and generalization guarantees of variants of adversarial training algorithms. We focus on solving the binary classification problem using two-layer over-parameterized neural networks with smooth activation functions and logistic loss. Our key contributions are as follows:

1. We present a bound of \((T++)\) on the uniform argument stability of the gradient descent-based adversarial training of over-parameterized network after \(T\) iterations with step size \(\), where \(\) represents the precision of generating adversarial examples at each iteration.
2. We provide robust generalization guarantees that depend on the Adversarial Regularized Empirical Risk Minimization (ARERM) Oracle. Our results hold for any given initialization and any data distribution. Specifically, if the learner is provided with a good initialization such that there exist robust networks around this initialization, then a small robust test loss is achieved via early stopping. Furthermore, our results can be extended to stochastic gradient descent-based adversarial training.
3. We leverage Moreau's envelope to construct a smooth loss that approximates robust empirical loss. We present bounds on the stability and generalization error of gradient descent with Moreau's smoothing, and demonstrate its superiority compared with gradient descent-based adversarial training algorithm.

### Related Work

Stability Analysis.The notion of stability was initially introduced in Bousquet and Elisseeff (2002) to study the generalization of statistical learning problems. More recently, a fine-grained analysis has been presented by Feldman and Vondrak (2019) and Bousquet et al. (2020). For smooth loss functions, Hardt et al. (2016) explored the stability of SGD in both convex and non-convex settings, which was later extended to convex non-smooth loss functions by Bassily et al. (2020) and the bound incorporated an additional term of \(()\) due to non-smoothness. Lei and Ying (2020) tackled the non-smoothness differently by assuming the gradient of the loss to be Holder continuous. For non-convex and non-smooth loss, Lei (2023) introduced the stability of sub-gradient, as convergence to local minimizers is observed in this setting.

Robust Generalization Guarantee.The standard method of giving a generalization guarantee is through uniform convergence. These theories typically yield an upper bound of \((})\) and require a large number of training samples in order to get a small generalization gap. Techniques in this category include analyzing the Rademacher complexity (Yin et al., 2019; Khim and Loh, 2018; Awasthi et al., 2020), VC dimension (Cullina et al., 2018; Montasser et al., 2020), covering number (Balda et al., 2019; Mustafa et al., 2022; Li and Telgarsky, 2023), PAC Bayesian analysis (Farnia et al., 2018; Viallard et al., 2021; Xiao et al., 2023) and margin-based analysis (Farnia et al., 2018).

Generalization Guarantee of Adversarial Training.Providing generalization guarantees for adversarial training of neural networks is challenging due to its non-convex nature. A series of works (Charles et al., 2019; Li et al., 2020; Zou et al., 2021; Chen et al., 2023) have focused on a simpler problem - adversarial training of linear models with a convex loss wherein generating adversarial examples admits a closed-form solution. Several works bypass this challenge by considering a lazy training regime (Gao et al., 2019; Zhang et al., 2020; Li and Telgarsky, 2023) in which the landscape of the neural network can be studied near certain random initialization, and the generalization guarantee is usually obtained via uniform convergence. Unfortunately, Wang et al. (2022) proved that adversarial robustness is at odds with lazy regime. Recently, Mainjy and Arora (2024), Wang et al. (2024) provide convergence and generalization guarantees for adversarial training of neural networks, yet they make restrictive assumptions on the data distribution such as (noisy) linear separability and robust realizability.

Another line of research investigates the generalization of adversarial training through algorithmic stability analysis. Despite the smoothness of the standard loss, the adversarial loss remains nonsmooth (Liu et al., 2020; Xing et al., 2021; Xiao et al., 2022). To resolve this issue, Farnia and Ozdaglar (2021) make a strong assumption that the loss is concave in input x. Xing et al. (2021) provide adversarial training of convex and non-smooth losses, yielding an additional term of \((T})\) compared to the standard non-robust counterpart. Xiao et al. (2022) and Wang et al. (2024) leverage the idea of approximate smoothness and provide bounds that scale linearly with \( T\) and the perturbation size. Cheng et al. (2024) consider generating adversarial examples via a single step of gradient descent and demonstrate that such variant of adversarial training algorithm achieves better stability. Farnia et al. (2018) also consider specific attack algorithms - these attacks while being more practical and designed with continuity and Lipschitzness property, may differ significantly from the worst-case attack, and do not yield a good bound on the robust generalization gap.

## 2 Problem Setup

Notation.Throughout the paper, we denote scalars, vectors, and matrices with lowercase italics, lowercase bold, and uppercase bold Roman letters, respectively; e.g., \(u\), u, and U. We use \([m]\) to denote the set \(\{1,2,,m\}\) and use both \(\|\|\) and \(\|\|_{2}\) for \(_{2}\)-norm. Given a matrix \(=[_{1},,_{m}]^{d m}\), we use \(\|\|_{F}\) and \(\|\|_{2}\) to represent the Frobenius norm and spectral norm, respectively. We use the standard O-notation (\(\), \(\) and \(\)).

We consider a binary classification problem with a bounded input space \(\) inside a Euclidean ball of radius \(C_{x}\), and label space \(=\{ 1\}\). We assume that data are drawn according to an unknown probability distribution \(\) on \(\). The learner has access to \(n\) training data drawn i.i.d. from \(\); i.e., \(S=\{x_{i}=(_{i},_{i})\}_{i=1}^{n}^{n}\). We do not make any restrictive distributional assumptions such as realizability (Mianjy and Arora, 2024) or (noisy) linearly separability (Wang et al., 2024).

We focus on learning two-layer neural networks, parameterized by a pair of weight matrices \((,)\):

\[f_{}()=f(;):=_{s=1}^{m}a_{s}( _{s},).\]

Here, \(m\) is a positive integer representing the number of hidden units, i.e., the width of the networks. \(:\) is a 1-Lipschitz, \(H\)-smooth activation function. Formally, \( z,z^{},|^{}(z)| 1,|^{}(z)- ^{}(z^{})| H|z-z^{}|\). The smoothness property of activation functions is commonly assumed in algorithmic stability literature and in theory of deep learning and covers a wide range of activation functions such as smoothed ReLU and smoothed leaky ReLU (Frei et al., 2022). The weight matrices at the top and bottom layer are denoted as \(=[a_{1},,a_{m}]^{m}\) and \(=[_{1},,_{m}]^{d m}\), respectively. The top layer weights are initialized such that \(|a_{i}|=}, i[m]\), and are kept fixed throughout the training process. Prior works (Du et al., 2018; Arora et al., 2019; Ji and Telgarsky, 2019) often initialize \(a_{i}\) to be uniformly sampled from \(\{}\}\), which can be seen as a special instance of ours. We do not make any assumption on the initialization of the bottom layer matrix, i.e., \(_{0}\) can be either a standard Gaussian (Du et al., 2018; Ji and Telgarsky, 2019), or a vanishing initialization (Ba et al., 2019; Xing et al., 2021), or a pre-trained model.

Adversarial Attacks.We consider a general threat model where the adversary's perturbation set is defined as \(: 2^{}\). Given an input x, \(()\) represents the set of all possible perturbations of x that an adversary can choose from. This broader definition of attack includes both the standard \(_{p}\) threat models with perturbation size of \(\), i.e., \(()=\{}:\|}-\|_{p}\}\), as well as a discrete set of large-norm transformations. Unlike prior works (Mianjy and Arora, 2024; Wang et al., 2024), we do not make any assumptions on the perturbation size.

In this work, we focus on logistic loss, \((z)=(1+e^{-z})\), which serves as a smooth and convex surrogate loss for the 0-1 loss. With a slight abuse of notation, for a fixed sample \(=(,y)\), we define \((,):=(yf(;))\). The population and empirical loss w.r.t. \(()\) are denoted, respectively, as

\[L():=_{(,y)}(yf(; )),(;S):=_{i=1}^{n}(y_ {i}f(_{i};)).\]

Given \(\), for a fixed sample \(=(,y)\), we define the robust loss as \(_{rob}(,):=_{} ()}(yf(};))\).

The robust population and empirical loss w.r.t. \(()\) are defined as

\[L_{rob}():=_{(,y)}_{}()}(yf(};)) _{rob}(;S):=_{i=1}^{n}_{}_{i}(_{i})}(y_{i}f(}_ {i};)).\]

Adversarial Training.During training, the network bottom layer weight \(\) are updated using gradient descent-based adversarial training (or its stochastic version). We denote the weight matrix at the \(t\)-th iterate of adversarial training as \(_{t}\). For each training example \((_{i},y_{i})\), at iteration \(t\), we generate a \(_{1}\)_-optimal adversarial example_\((}_{i}(_{t}),y_{i})\), which satisfies the following condition:

\[(y_{i}f(}_{i}(_{t});_{t})) _{}(_{i})}(y_{i}f(};_{t}))-_{1}.\] (1)

Setting \(_{1}=0\) recovers the scenario where we have access to the worst-case adversarial attack. As this may not be feasible in practice due to computational reason, the parameter \(_{1}\) allows us to capture the precision of the attack algorithm, which includes common attacks such as projected gradient descent (PGD) (Madry et al., 2018). We should regard \(_{1}\) as a parameter we can choose. Our results in Section 3 suggest that we can achieve better generalization by adding more computation and making \(_{1}\) smaller.

``` Step size \(\). Number of iterations \(T\). Initial weight \(_{0}\). \( 0\). \(>0\). for\(t=0,,T-1\)do \(\): \( i[n]\), compute a \(_{1}\)-optimal adversarial example \(}_{i}(_{t})\) that satisfies Equation (1).  Update \(_{t+1}=_{t}-_{i=1}^{n}_{ }(y_{i}f(}_{i}(_{t});_{t}))\). SGD: Compute a \(_{1}\)-optimal adversarial scheme \(}_{t+1}(_{t})\) that satisfies Equation (1).  Update \(_{t+1}=_{t}-_{}(y_{t+1}f( }_{t+1}(_{t});_{t}))\). Moreau Envelope: Compute a \(_{2}\)-optimal minimizer \(}^{}(_{t};S)\) that satisfies Equation (2).  Update \(_{t+1}=_{t}-(_{t}-}^{}(_{t};S))\). endfor  return: \(\{_{t}\}_{t=0}^{T}\). ```

**Algorithm 1** Variants of Adversarial Training Algorithms

Optimizing the Moreau Envelope.Since the robust loss is non-smooth (Xiao et al., 2022), we utilize Moreau's envelope to construct a smooth function that approximates the empirical robust loss. Such an idea has previously been explored in Xiao et al. (2024). Given training data \(S\) and \(>0\), we redefine the robust surrogate loss as follows:

\[M^{}(;S)=_{}(_{rob}(;S) +\|-\|_{F}^{2}).\]

Selecting \(\) appropriately ensures that \(_{rob}(;S)+\|-\|_{F}^{2}\) is a strongly convex function w.r.t. U. Given W and \(S\), we define \(^{}(;S)=*{argmin}_{^ {d m}}_{rob}(;S)+\|- \|_{F}^{2}\), which can be obtained via subgradient-based method (solve a min-max optimization). The gradient of the Moreau envelope can be simply calculated as \(_{}M^{}(;S)=(-^{ }(;S))\). Given training data \(S\), at each iteration \(t\), we generate a \(_{2}\)_-optimal minimizer_\(}^{}(_{t};S)\) that satisfies

\[_{rob}(}^{}(_{t};S);S)+\|}^{}(_{t};S)-_{t}\|_{F}^{2} _{2}+M^{}(_{t};S).\] (2)

We remark that \(_{2}\)-optimal minimizer defined in Equation (2) and \(_{1}\)-optimal adversarial example defined in Equation (1) are approximating different quantities, which are not comparable. All the algorithms described above are summarized in Algorithm 1.

Uniform Argument Stability.Given a training set \(S=\{_{i}\}_{i=1}^{n}\) drawn i.i.d. from \(\), let \(S^{}\) denote the training set obtained by replacing one example in \(S\) with an independently drawn example \(^{}\). We refer to \(S,S^{}\) as neighboring samples and write \(S S^{}\). Given an algorithm \(:()^{n}\), where the hypothesis class \(\) is parameterized using a parameter matrix \(^{d m}\), we define the uniform argument stability as

\[_{}(S,S^{}):=\|(S)-(S^{ })\|_{F}.\]

For any \(L\)-Lipschitz loss function \(g\), \(|g((S),)-g((S^{}),)| L _{}(S,S^{})\). The standard stability argument [Mohri et al., 2018] relates the expected generalization gap to the uniform argument stability.

\[_{S^{n}}_{gen}((S)):=_{S^{n}}_{}g( (S),)-_{i=1}^{n}g((S),_{i}) L_{S S^{}}_{}(S,S^{}).\] (3)

In this paper, we consider robust generalization using logistic loss, so function \(g(,)=_{rob}(,)\), and \(_{gen}()=_{}[_{ rob}(,)]-_{i=1}^{n}_{rob}(_{i}, )\). We also remark that a high probability bound for stable algorithms can be given based on Feldman and Vondrak . For simplicity, our generalization bounds in this paper are only in expectation.

## 3 Main Result

In this section, we present our main results, providing theoretical guarantees for adversarial training of two-layer neural networks with smooth activation functions. We discuss (stochastic) adversarial training in Section 3.1 and gradient descent-based Moreau's smoothing in Section 3.2. Our generalization bounds rely on a key quantity, the _Adversarial Regularized Empirical Risk Minimization (ARERM) Oracle_ defined as

\[_{S}^{}:=_{^{d m}} (_{rob}(;S)+-_{0}\|_{F }^{2}}{ T}).\]

Given a sample, \(_{S}^{}\) returns the minimal empirical risk in the vicinity of an initialization \(_{0}\).

### Generalization Guarantees for Adversarial Training

We begin by presenting a bound on the uniform argument stability (UAS) of Algorithm 1 with GD.

**Theorem 3.1**.: Assume that the network width satisfies \(m H^{2}C_{x}^{4}^{2}(T+1)^{2}\). Then, after \(T\) iterations of Algorithm 1 with GD, for any neighboring datasets \(S,S^{}\), we have

\[_{S S^{}}_{}(S,S^{})(C_{ x}+C_{x}+ T}).\]

Remarkably, setting \(_{1}=0\) yields a bound of \((+)\) on the UAS of Algorithm 1, thereby recovering the result in prior work of Xing et al. [2021a]. However, note that Xing et al. [2021a] show the result only for convex learning problems, whereas we consider training two-layer neural networks using logistic loss, which is non-convex and non-smooth. Further note that we assume that the networks are sufficiently over-parameterized, i.e., \(m(^{2}T^{2})\), a condition that is commonly assumed in deep learning theory. We can also regard this condition as early stopping, wherein \(T(}{H_{c}^{2}})\). This view is also consistent with several empirical studies [Caruana et al., 2000, Rice et al., 2020, Pang et al., 2021].

Next, we show that stable robust learning rules do not overfit.

**Theorem 3.2**.: Define \(_{1}(,T):=(C_{x}^{2}+C_{x}^{2}+C_{x} T})\). Assume that the width of the networks satisfies \(m H^{2}C_{x}^{4}^{2}(T+1)^{2}\), and \(_{1}(,T)<1\). Then, after \(T\) iterations of Algorithm 1 with GD, we have

\[_{[] t T}_{S^{n}} _{gen}(_{t})(,T)}{1-_{1} (,T)}[_{S^{n}}_{S}^{}+ ^{2}}{2}+_{1}],\]

and

\[_{0 t T}_{S^{n}}L_{rob}(_{t}) (,T)}[_{S^{n}}_ {S}^{}+^{2}}{2}+_{1}].\]The result above bounds the robust generalization gap and the robust loss in terms of the ARERM oracle, a step size-dependent term \(()\), and the precision of the adversarial examples \(_{1}\). Note though that the bound holds for the minimum over the last few iterates (past iterates), rather than for the last iteration. This distinction arises because, unlike standard gradient descent for neural networks, we cannot guarantee a decreasing robust training loss without additional assumptions on the data distributions owing to the non-smooth nature of the robust loss. The step size-dependent term arises for the same reason. A direct corollary gives us a bound on the expected robust loss.

**Corollary 3.3**.: After \(T(\{n^{2},^{2}}\})\) iterations of Algorithm 1 with GD using a step size of \(=(^{2}})\) on a network with width \(m(T)\), for any weight matrix W

\[_{0 t T}_{S^{n}}L_{rob}(_{t}) 1.1L_{rob}()+(^{2}\|- _{0}\|_{F}^{2}}{})+(} ).\]

Since corollary 3.3 holds for any \(_{0}\), it underscores the importance of initialization for robust learning. Given a good initialization, such as a pre-trained model, and assuming that there exists a robust network \(_{*}\) in the vicinity of the initialization (i.e., \(\|_{*}-_{0}\|_{F}=(1)\)) that achieves a small robust loss \(L_{rob}(_{*}) 0\), we have that the minimum expected robust loss over all iterates approaches \((})\). Further, if \(_{1}\) is small enough and \(m n^{2}\), then \(T\) can be of the order \((n^{2})\), leading to a \((1/n)\) upper bound on the robust test loss.

We remark that by a similar analysis, our result can be reduced to the standard (non-robust) setting for gradient descent training of two-layer networks by setting the perturbation set \(()=\{\},\), \(_{1}=0\), and redefining \(_{1}(,T)=(C_{x}^{2})\). In this context, we can show that gradient descent for the binary classification problem can achieve excess risk bound of \((1/)\) by taking \( T=()\) if \(m n\) and assuming \(\|_{*}-_{0}\|_{F}=(1)\), where \(_{*}}{}\,L_{rob}()\).

Next, we extend our result to the stochastic adversarial training.

**Theorem 3.4**.: After \(T\) iterations of Algorithm 1 with SGD on a network of width \(m H^{2}C_{x}^{4}^{2}(T+1)^{2}\) we have that for any weight matrix W,

\[_{0 t T}_{\{x_{1},,x_{t}\}^{t}}L_{ rob}(_{t}) L_{rob}()+-_{0}\|_{F}^{2}} {(T+1)}+^{2}}{2}+_{1}.\]

Similar to the discussion following Corollary 3.3, we assert that if we assume that there exists an over-parameterized robust network with small robust loss, then using a step size of \(=1/\), stochastic adversarial training yields an excess risk bound of \((1/)\).

### Generalization Guarantees for Gradient Descent on Moreau's Envelope

We now present a bound on the uniform argument stability of gradient descent with smoothing based on Moreau's envelope.

**Theorem 3.5**.: After \(T\) iterations of Algorithm 1 with Moreau Envelope with step-size \(\{,}{8HC_{x}^{2}}\}}{2HC_{x}^{ 2}}\), on a network of width \(m H^{2}C_{x}^{4}^{2}T^{2}\), for any neighboring datasets \(S,S^{}\), we have

\[_{S S^{}}_{}(S,S^{}) (C_{x}+ T}{}} ).\]

Setting \(_{2}=0\) yields a bound of \(()\) on the UAS of Algorithm 1, thereby recovering the result in prior work of (Hardt et al., 2016; Xiao et al., 2024) for convex and smooth functions. Note that by using Moreau's envelope, we are able to shave off the \(()\) term that appears in Theorem 3.1.

Although inspired by Xiao et al. (2024), Theorem 3.5 differs from the non-convex setting of Xiao et al. (2024). Our result utilizes the specific structure of over-parameterized neural networks that exhibit weakly convex properties, a special instance of non-convex functions, and allows for a constant step size. In contrast, (Xiao et al., 2024, Theorem 4.7) follows the traditional stability argument for non-convex and smooth functions in Hardt et al. (2016), considering a decaying step size \(_{t}\).

Such a condition might be impractical if \(\) is chosen to be sufficiently small. In fact, our results indicate that it is necessary to select a sufficiently small \(\) so that the robust training loss is well approximated by the Moreau envelope (see Lemma C.1 in the Appendix).

Even though the gradient descent-based algorithm with Moreau's smoothing achieves better stability guarantees compared to gradient descent-based adversarial training when \(_{1}=_{2}=0\), it requires more computational resources. Specifically, for the calculation of the gradient at each step, we need to solve a min-max optimization problem with a strongly convex and non-smooth objective to obtain a \(\)-optimal minimizer. Additionally, for every step of this min-max optimization, we need to generate adversarial examples and apply sub-gradient descent.

**Theorem 3.6**.: Define \(_{2}(,T):=(C_{x}^{2}+C_{x} T}{}})\). Assume \(_{2}(,T)<1\). Then, after \(T 8\) iterations of Algorithm Moreau Envelope with step-size \(\) on a network of width \(m H^{2}C_{x}^{4}^{2}T^{2}\), we have

\[_{[}] t T}_{S^{n} }_{gen}(_{t})(,T)}{1-_{2}( ,T)}[_{S^{n}}_{S}^{}+C _{x}^{2}+2(T+1)}{}],\]

and

\[_{1 t T}_{S^{n}}L_{rob}(_{t}) (,T)}[_{S^{n}} _{S}^{}+C_{x}^{2}+2(T+1)}{} ].\]

Similar to Theorem 3.2, the result above shows that both the robust generalization gap as well as the robust loss can be bounded in terms of the ARERM oracle, parameter \(\) in Moreau's envelope, and a term of \(( T_{2}/)\) dependent on the precision of generating the minimizer of Moreau envelope. While the bound above is on the minimum expected generalization gap (and expected robust test loss) over the last few iterates (past iterates), we can give a bound for the the last iterate for the case when \(_{2}=0\). We conclude the section by presenting the following direct corollary.

**Corollary 3.7**.: After \(T(\{n^{2},^{2/3}}\})\) iterations of Algorithm 1 with Moreau Envelope with step-size \(==(^{2}})\) on a network of width \(m(T)\), we have for any weight matrix \(\),

\[_{1 t T}_{S^{n}}L_{rob}(_{t})  1.1L_{rob}()+(^{2}\|-_{0 }\|_{F}^{2}}{})+(}).\]

## 4 Proof Sketch

We begin by providing a high level intuition behind our analysis technique, and then we highlight the key ideas in the proofs of the main theorems. For simplicity, we assume that the learner can generate optimal attacks during adversarial training, i.e., we consider \(_{1}=0\), \(_{2}=0\) in this section. We refer the reader to the Appendix for proofs of the more general case.

Our analysis relies on a key lemma demonstrating that the objective function (i.e., the robust empirical risk) being minimized in adversarial training of two-layer neural networks with smooth activation functions using the logistic loss function is "almost" convex.

**Definition 4.1**.: Let \(l>0\). A function \(f(x)\) is said to be \(-l\)-weakly convex if \(f(x)+\|x\|_{2}^{2}\) is convex in \(x\).

**Lemma 4.2**.: (Restatement of Lemma A.4) For any weight matrices \(^{1}\) and \(^{2}\),

\[_{rob}(^{2};S)_{rob}(^{1};S)+< _{}_{rob}(^{1};S),^{2}-^{1} >-^{2}}{2}\|^{2}-^{1}\|_{F}^{2}.\]

Equivalently, \(_{rob}(;S)\) is \(-^{2}}{}\)-weakly convex.

We borrow many ideas from Xiao et al. (2024) and Xing et al. (2021) in our proofs. These papers primarily focus on the convex setting, while only giving a general result for non-convex functions. We extend their results to a special case of learning neural networks. We argue that by specializing our analysis to neural networks would lead to sharper results than a general non-convex function class, as we will be able to leverage the "almost" convexity of neural network training [Richardsand Rabbat, 2021; Richards and Kuzborskij, 2021). This allows us to get stability and optimization guarantees that are similar to the convex setting when we consider an over-parameterized network \(m( T)\). An additional challenge we face is that the robust loss is non-smooth even if its standard counterpart (logistic loss) is smooth, making the analysis more complicated than the standard (non-robust) scenario. Nevertheless, we can still leverage the "almost" convex nature of the loss to establish the stability of adversarial training.

The following lemma gives a relationship between stability and generalization which is useful in both standard adversarial training as well as gradient descent with Moreau's envelope. When the robust training loss \(_{rob}(_{T};S)\) is small, Lemma 4.3 provides a tighter bound than directly applying Equation (3). See Proposition A.3 for both results.

**Lemma 4.3**.: (Restatement of Proposition A.3) The robust test loss satisfies the following:

\[_{S^{n}}L_{rob}(_{T})_{S ^{n}}_{S S^{}}_{}(S,S^{})}_{rob}(_{T};S).\]

This result gives a way to bound the expected robust loss. Say you want to bound the expected robust test loss by \((1+)\) times the expected training loss. Then, to ensure \((,T)} 1+\), we need \(_{1}(,T)=O()\).Since \(_{1}(,T)=O(++ T})\), we can set different parameters in more than one way to ensure that \(_{1}(,T)=O()\). We can set \(_{1}=O(^{2})\), \(n=(1/)\), \(T=(1/^{2})\), \(=O()\); or set \(_{1}=O(^{3})\), \(n=(1/^{2})\), \(T=(1/^{4})\), \(=O(})\).

### Generalization Guarantees for Gradient-Based Adversarial Training

The stability guarantee we give in the following Theorem 4.4 is similar to the result in the convex case (Xing et al., 2021). While (Xing et al., 2021) use the monotone subgradient condition of the convex functions, we show that the subgradients of an "almost" convex loss function are "almost" monotone. We do incur an additional term of \((2HC_{x}^{2} T/)\), which is small for over-parameterized neural networks (\(m( T)\)).

**Theorem 4.4**.: (Restatement of Theorem 3.1) Let \(S\) and \(S^{}\) be any two neighboring data sets, i.e., they differ only in one example. Let \(_{T}\) and \(_{T}^{}\) denote the weight matrices returned after \(T\) iterations of Algorithm 1 with GD on \(S\) and \(S^{}\), respectively. Then, we have

\[\|_{T}-_{T}^{}\|_{F}^{2}(1+^{2} T}{})(4C_{x}^{2}^{2}(T+1)+^{2} ^{2}(T+1)^{2}}{n^{2}}).\]

We next provide an intermediate lemma that lead us to Theorem 3.2.

**Lemma 4.5**.: (Restatement of Theorem B.2) Set \(k=(1+^{2}}{})^{-1}\). Then after \(T}{HC_{x}^{2}}-1\) iterations of Algorithm 1 with GD,

\[^{T}k^{t}}_{t=0}^{T}k^{t}_{rob }(_{t};S)_{S}^{}+^{2}}{2}.\]

Richards and Kuzborskij (2021) (see Lemma 2 in their paper) give an optimization guarantee by providing an upper bound on the averaged training loss \(_{t=1}^{T}(_{t};S)\) of all iterates. In Lemma 4.5 we use a more refined analysis by considering the weighted average of the training loss. Specifically, for any weight matrix \(\), we follow the standard technique in the convex case and upper bound the following:

\[\|-_{t+1}\|_{F}^{2}=\|-_{t}\|_{F}^{2} +^{2}\|_{}_{rob}(_{t};S)\|_{F}^{2}+ 2_{}_{rob}(_{t};S), -_{t}.\]

The second term on the right hand side is bounded by the Lipschitzness of the logistic loss. The inner product in the third term is bounded by \(_{rob}(;S)-_{rob}(_{t};S)+^{2}}{2}\|-_{t}\|_{F}^{2}\) using Lemma A.4. We finish the proof by telescoping. The weighted telescoping technique removes all of the \(\|-_{t}\|_{F}^{2}\) terms (\(t>0\)) in the upper bound, thereby giving a simpler result. The term \(C_{x}^{2}/2\) in the upper bound stems from the non-smoothness of the robust loss, and is unavoidable even if the robust loss is convex. Finally, Theorem 3.2 follows from Theorem 4.4 and Lemmas 4.3 and 4.5.

### Generalization Guarantees for Gradient-Descent on Moreau's Envelope

Below we give the key lemmas for bounding the generalization error of GD with Moreau's envelope. The proof technique here is similar to that for standard adversarial training (in the previous section), except that we get to utilize the smoothness of Moreau's envelope. Specifically, Lemma 4.6 leverages the fact that the gradient is "almost" co-coercive to control the uniform argument stability.

**Theorem 4.6**.: (Restatement of Theorem C.4) Let \(S S^{}\) be any two neighboring data sets, i.e., \(S\) and \(S^{}\) differ only in one example. For any \(\{,}{8HC_{x}^{2}}\}}{2HC_{x}^{ 2}}\), let \(_{T}\) and \(_{T}^{}\) be the weight matrices obtained by \(T\) iterations of gradient descent with Moreau's envelopes on datasets \(S\) and \(S^{}\), respectively. Then, we have that

\[\|_{T}-_{T}^{}\|_{F}^{2}(1+^ {2} T}{})^{2}^{2}(T+1)^{2}}{n^{2}}.\]

Lemma 4.7 also leverages smoothness due to Moreau's envelope and yields a bound that does not involve the additional term \(C_{x}^{2}/2\) compared with Lemma 4.5.

**Lemma 4.7**.: (Restatement of Theorem C.6) Set \(k=(1+^{2}}{})^{-1}\). After \(T\) iterations of Algorithm 1 with Moreau Envelope with \(}{2HC_{x}^{2}}\) and \(T}{HC_{x}^{2}}\), we have

\[^{T}k^{t}}_{t=1}^{T}k^{t}M^{}( _{t};S)_{S}^{}.\]

Theorem 3.6 is naturally derived via Theorem 4.6, Lemma 4.3 and 4.7.

## 5 Conclusion

In this work, we establish the generalization guarantees for variants of adversarial training applied to two-layer networks with smooth activation functions. For over-parameterized neural networks, we present robust generalization bound that are controlled by the Adversarial Regularized Empirical Risk Minimization (ARERM) oracle, applicable to any given initialization and any data distributions. One future direction is to extend our analysis to deep neural networks and beyond neural networks with smooth activation functions.