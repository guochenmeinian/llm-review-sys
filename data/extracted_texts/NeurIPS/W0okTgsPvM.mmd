# Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning

Brandon Huang\({}^{1}\)   Chancharik Mitra\({}^{1}\)   Assaf Arbelle\({}^{2}\)   Leonid Karlinsky\({}^{3}\)

&Trevor Darrell\({}^{1}\)   Roei Herzig\({}^{1,\,2}\)

\({}^{1}\) University of California, Berkeley  \({}^{2}\) IBM Research  \({}^{3}\) MIT-IBM Watson AI Lab

Denotes Equal Contribution

###### Abstract

The recent success of interleaved Large Multimodal Models (LMMs) in few-shot learning suggests that in-context learning (ICL) with many examples can be promising for learning new tasks. However, this _many-shot_ multimodal ICL setting has one crucial problem: it is fundamentally limited by the model's context length set at pretraining. The problem is especially prominent in the multimodal domain, which processes both text and images, requiring additional tokens. This motivates the need for a multimodal method to compress many shots into fewer tokens without finetuning. In this work, we enable LMMs to perform multimodal, many-shot in-context learning by leveraging Multimodal Task Vectors (MTV)--compact implicit representations of in-context examples compressed in the model's attention heads. Specifically, we first demonstrate the existence of such MTV in LMMs and then leverage these extracted MTV to enable many-shot in-context learning for various vision-and-language tasks. Our experiments suggest that MTV can scale in performance with the number of compressed shots and generalize to similar out-of-domain tasks without additional context length for inference. Code: https://github.com/Brandon3964/MultiModal-Task-Vector

## 1 Introduction

Large Multimodal Models (LMMs) such as GPT-4V , LLaVA , and the BLIP  family of models demonstrate state-of-the-art performance on a variety of vision and language (VL) tasks due to their strong reasoning capabilities over both text and images. Recent works show that LMMs pre-trained on interleaved text-image data can do multimodal in-context learning . In particular, few-shot, in-context learning (ICL) in text-only LLMs has been scaled with an increasing number of examples in long-context language models--a setting called many-shot learning . A natural question arises on how to perform many-shot learning in the multimodal domain.

The first issue with directly applying a many-shot learning regimen to LMMs is the intrinsic limitation of context length. This is especially true in the multimodal domain, as LMMs must encode both text and images, whose embeddings are token-expensive. Moreover, long-context language models, which LMMs leverage for reasoning, struggle to use their entire context length effectively for ICL . Secondly, perhaps due to the misalignment of pretraining tasks with ICL, many instruction-tuned LMMs underperform on tasks in the ICL setting , suggesting the importance of interleaved LMMs. Finally, there is also the challenge of the increasing memory and run-time required for processing long contexts for every inference call. These challenges motivate a method for compressing multimodal in-context examples into compact, implicit representations. Therefore, in this paper, we proposeMultimodal Task Vectors (MTV)--compact representations of multimodal in-context tasks--within the attention heads of LMMs to enable many-shot ICL. In particular, we show the existence of MTV in interleaved LMMs, and we use them to compress large numbers of multimodal ICL examples.

Recent research in explainability has demonstrated the existence of task vectors in both the language [25; 81] and vision  domains. These task vectors are implicit representations of in-context tasks represented by sets of activations in the model. These activations compactly summarize the information in ICL examples. In our work, we go beyond proving the existence of these task vectors in the multimodal domain by demonstrating their ability to compress examples for many-shot ICL in LMMs without the need for finetuning.

Our method can be described in three steps. First, given a set of many-shot multimodal ICL examples, we calculate the mean activations corresponding to the last token across multiple inference iterations. Second, to avoid the context length constraint, we select a set of attention heads in the model to store the mean activations of the ICL examples. However, since the downstream task may be zero-shot or use a different number of ICL examples, we select a set of examples aligned with its form. We then use these examples to find an optimal set of LMM head locations where the many-shot examples will be encoded. We refer to these mean activations and locations as MTV, which implicitly encodes the many-shot multimodal examples for use in the downstream task. Finally, for downstream inference, we replace the mean activations from Step 1 with the attention head locations found in Step 2. Since we input examples to the LMM across different iterations in Step 1, Multimodal Task Vectors can implicitly encode more examples than are allowable by the context limit. We find that utilizing many examples for extracting MTV surpasses performance on zero-shot and most standard few-shot ICL settings, suggesting the effectiveness of our method. Another key benefit of our method is that it frees up tokens for the model during downstream inference compared to standard few-shot ICL methods. An overview of our method is shown in Figure 1.

We summarize our main contributions as follows: (i) We show the existence of Multimodal Task Vectors, compact implicit representations of in-context functions in LMMs. (ii) MTV can encode more examples than allowed by an LMM's context length, enabling both runtime and memory-efficient multimodal many-shot in-context learning. (iii) MTV surpasses zero-shot and few-shot ICL settings on various VL benchmarks without finetuning. (iv) MTV can scale to larger numbers of examples and can generalize to similar out-of-domain tasks.

## 2 Related Works

**Many-Shot In-Context Learning**. Few-shot in-context learning (ICL) is a significant area of study in text-only LLMs [9; 89]. A natural question arises about the possibility of using a larger number of shots (e.g., hundreds) to further improve performance or learn more complex tasks. Indeed, some early work in text-only _many-shot, in-context learning_ suggests performance on different tasks can scale with a larger number of examples [1; 7; 44; 45].

Figure 1: **Multimodal Task Vectors (MTV) Overview. We overcome an LMM’s context length limitation by encoding many shots of multimodal examples as activations in the LMM’s latent space. We then directly replace this encoding into the LMM’s activation space during downstream inference.**

However, scaling ICL in text-only LLMs is a challenge due to the intrinsic context length. One method to increase context length in these models is to apply positional interpolation methods [10; 63]. However, research on these longer-context models finds that they struggle to use the entire context for ICL [45; 51]. Moreover, as inference on long contexts of inputs is also time and memory-expensive, it is unclear whether simply scaling the context of models is practical for enabling multimodal many-shot ICL in open-source models. There is some early evidence of multimodal many-shot ICL being effective in closed-source models , so the question arises as to how to achieve something similar for open-source models. This has led to work that looks to compress explicit input tokens [11; 20; 34; 58; 73; 77]. But crucially, many of these methods require finetuning and only try to preserve performance. Our work is different in that it is the first to enable _multimodal_ models with many-shot ICL capabilities, while also improving on complex VL tasks without finetuning.

**Task Vectors**. Our work builds off of research in text-only and vision-only domains showing that internal representations of these models called task vectors [25; 27; 81] (or function vectors) can encapsulate tasks outlined by ICL examples. Our is the first demonstration of Multimodal Task Vectors (MTV) in LMMs. Going beyond previous work, however, we show that MTV enable LMMs not only to use many-shot, multimodal ICL examples but also scale with more samples, be used alongside explicit ICL shots, and even generalize to unseen classes or similar tasks.

**Model Domain Adaptation Methods**. As LLM and LMM model architectures have advanced, so have methods to allow these models to generalize beyond their pretraining distributions. Methods like instruction tuning [5; 50; 68; 88] have shown strong zero-shot generalization to some out-of-domain tasks, but forgetting remains an issue. One popular solution to this issue involves Parameter Efficient Fine-tuning (PEFT) : finetuning either a set of soft prompt input tokens [41; 46], low-rank model weights [14; 29; 99], or a separate adapter from the main model [18; 30; 100].

Prompting methods are a well-explored area for adapting models without finetuning. LLM prompting includes zero-shot methods [36; 83; 85], few-shot and ICL methods [9; 15; 53; 56], expert prompting , and Chain-of-Thought (CoT) [90; 101], with extensions like self-consistency , Tree-of-Thought (ToT) , and Graph-of-Thought (GoT) [8; 40; 95] for more complex structures. Similar multimodal prompting methods exist for LMMs as well [57; 84; 87; 102; 104].

**Large Multimodal Models (LMMs)**. The state-of-the-art performance of LMMs [2; 6; 13; 17; 21; 43; 49; 50; 96; 97; 90] on multimodal tasks stems from combining LLMs' reasoning capabilities [3; 12; 26; 66; 71; 78] with the perception abilities of vision models. LMMs' generative reasoning also makes them more applicable to complex tasks than previous contrastive methods [42; 43; 65]. Such tasks include visual question-answering [4; 23; 24; 31; 32; 54; 67] as well as object identification and localization [37; 48; 59; 82]. Visual Programmatic Models (VPMs) are another class of multimodal methods that makes use of in-context APIs code generation [19; 22; 52; 64; 69; 72; 74; 76; 92]. However, context length limits both LMMs' and VPMs' ability to use multimodal prompting methods such as ICL . Another key challenge is that many LMMs are pre-trained on single text-image pair data. Recently, many LMM models now pretrain on interleaved text-image data [2; 6; 16; 33; 39; 75; 103], making effective multimodal ICL possible. In our work, MTV goes beyond simple few-shot multimodal ICL and scales to many-shot multimodal ICL.

## 3 Multimodal Task Vectors

To address the challenge of performing many-shot multimodal in-context learning, we demonstrate the existence of MTV in LMMs and then leverage them for many-shot multimodal ICL. We begin by describing some background on multimodal ICL and task vectors (Section 3.1). We then introduce our three-step approach: (i) We calculate the mean activations of the attention heads from the many-shot multimodal ICL examples (Section 3.2); (ii) We then extract the set of LMM attention heads locations that best align to the downstream task using an adapted version of the REINFORCE  algorithm (Section 3.3); and (iii) We replace the calculated mean activation values into the LMM for a downstream task (Section 3.4). The detailed method visual is shown in Figure 1.

### Preliminaries

In the multimodal in-context learning setting, an LMM learns a new task outlined by a set of multimodal examples. The input to the LMM would be outlined as follows:

\[I_{}=[(x_{1}:y_{1}),(x_{2}:y_{2}),,(x_{n}:y_{n}),Q]\] (1)where the model is prompted to answer a query \(Q\) given a set of input-output examples (each \(x_{i}\) being a multimodal input and each \(y_{i}\) a text output).

We note that in-context examples are commonly passed sequentially to the LMM, necessarily restricting multimodal ICL to being small numbers of shots due to limited context length. Furthermore, the images require more tokens to embed, which means enabling many-shot ICL is even more challenging in the multimodal domain. To solve this, we utilize our method MTV--which are implicit representations in the model's attention heads that encode a many-shot multimodal ICL task.

We start with a background on task vectors for some task \(j\). Given a model \(F\), we denote the set of attention-head locations as \(=\{l l F\}\) where each location \(l\) is indexed as \(l=(h,m)\) for the \(h^{}\) layer and \(m^{}\) attention head. Now, task vectors utilize the intermediate outputs of an LMM, called **activations**. For a given input sequence of written in terms of its tokens \(x=\{x_{1},x_{2},,x_{T}\}\), each attention head \((h,m)\) produces an activation \(z_{l}^{}\) for each token \(x_{i}\), where \(d\) is the model's embedding dimension and \(H\) is the number of heads. These activations are simply the output vectors of each attention head _before_ any linear projection. While each head's activation is typically concatenated with others and projected to form the layer's output, task vectors specifically utilize the

Figure 2: **Multimodal Task Vectors (MTV). In the standard multimodal in-context learning (ICL) paradigm, the number of shots is limited by an LMM’s context length. We solve this issue by first finding the mean activations corresponding to the last token of the examples’ input (Step 1), and then calculating a set of attention head locations (Step 2) that best align with the downstream task. These mean activations are then replaced directly in these attention head locations (Step 3), enabling many-shot multimodal ICL.**

pre-projection activations of the final token \(x_{T}\) from each attention head. We thus define the task vectors as follows: (1) the task vector **values**\(_{j}\) are a subset of mean activations produced by the attention heads of \(F\) given examples of a task, and (2) the task vector **locations**\(_{j}\), which denotes a subset of the attention head indices per task. Thus, the task vector is \((_{j},_{j})\). For inference, \(_{j}\) replaces the activation values of the heads in the locations given by \(_{j}\).

In prior work [25; 27; 81], the calculation of the mean activations \(_{j}\) and the extraction of the attention-head locations \(_{j}\) are used together to extract the task vector. Interestingly, we find that these two steps should be decoupled in order to better align with the downstream task. In our work, we calculate the mean activations \(_{j}\) corresponding to the last token specifically to encode a dataset of many-shot multimodal ICL examples by averaging them across multiple inference calls. However, the downstream task may not always be in the same ICL format as the many-shot examples (e.g., the downstream task uses a different number of shots or is zero-shot). To solve this, we use a separate set of examples that are of the exact format of the downstream task to align the extracted attention-head locations \(_{j}\) with the inference task. This separation of responsibilities, wherein \(_{j}\) captures the essential information from the many-shot examples and \(_{j}\) identifies the specific attention head locations for the downstream task, optimizes the utilization of the encoded information at relevant locations within the model.

Our approach to finding Multimodal Task Vectors (MTV) \((_{j}^{ MTV},_{j}^{ MTV})\) allows LMMs to actually leverage many-shot multimodal ICL examples for complex vision-language tasks without being limited by context length. We proceed by first describing how to calculate the mean activations.

### Step 1: Calculate MTV Mean Activations

The ultimate objective of many-shot multimodal ICL is to use a large number of input-output examples when solving a task \(j\). However, it is not trivial to get the LMM to see more examples during inference time than its context length allows.

To address this issue, we pass a few-shot input \(I_{t}\) for each inference call \(t\) for a total of \(T>1\) inference calls. Each \(I_{t}\) consists of \(N\) shots (where \(N>1\)) of multimodal in-context examples in the form of randomly-selected input-output response pairs \((x_{t}:y_{t})\), and \(Q_{t}\), which is the query to be answered by the LMM in that iteration.

\[I_{t}=[(x_{1}:y_{1}),(x_{2}:y_{2}),,(x_{N}:y_{N}),Q_{t}]\] (2)

Thus, over \(T\) LMM inference calls, we have a many-shot multimodal dataset (of \(N T\) examples):

\[I_{ many}=[I_{1},I_{2},,I_{T}]\] (3)

However, this dataset is still just a disconnected set of few-shot examples. Next, we would like to connect the separate examples into one unified many-shot multimodal ICL representation.

For each inference call, the LMM is given \(N\)-shot ICL examples. We calculate the mean of the activations corresponding to the last token of the input \(z_{l,j}\) for each attention head index \( l\) (Section 3.1) across \(T\) inference calls, yielding:

\[ l:_{l,j}=_{t=1}^{T}[z_{l,j } I_{t}]=_{t=1}^{T}[z_{l,j}(x_{1}:y_{1} ),(x_{2}:y_{2}),,(x_{N}:y_{N}),Q_{t}]\] (4)

In this step, we have found the mean activations \(_{l,j}\), which encode an internal LMM representation of many shots of multimodal ICL examples. In the next subsection, we describe our methodology for selecting the set of attention heads where these mean activations will be used.

### Step 2: Extract MTV Attention Head Locations

After Step 1, we now have mean activations for the attention heads of the last token in a given many-shot multimodal task. Yet, we still need to find which set of attention heads \(_{j}^{ MTV}\) should be chosen to encode our task.

To choose the set of attention heads, we first prepare a separate set of \(S\) examples specifically aligned to the format of the downstream task. For instance, if the downstream setting is a 2-way, one-shotclassification task, then the \(S\) examples should conform to this paradigm. For our explanation, let's consider a downstream task that is zero-shot such that there is a single query \(Q_{s}\) and corresponding response \(R_{s}\) for all \(s[1,2,,S]\).

From these examples, we utilize an adapted version of the REINFORCE  algorithm--an iterative policy optimization method that can be used to find task vector locations . Given an LMM \(F\), we first select a proposed set of attention head locations by sampling a Bernoulli distribution over the locations multiple times. Next, we directly replace the values of the selected attention heads with the corresponding mean activations \(_{l,j}\). Then, after prompting the model with the query \(Q_{s}\), we use the negative cross-entropy loss between the LMM's output logits and the logits of the ground-truth response \(R_{s}\) to optimize the Bernoulli distribution. By optimizing the Bernoulli distribution across \(S\) iterations, we are finding the best attention head locations \(_{j}^{}\) for patching in our mean activations. Finally, we can extract \(_{j}^{}\), the optimized indices of attention heads, by sampling our optimized Bernoulli distribution.

\[_{j}^{}=(F,[Q_{1},Q_{2},,Q_{S}) ],[R_{1},R_{2},,R_{S}])\] (5)

It is important to note that MTV_EXTRACT does not require finetuning of the LMM parameters, but rather only inference calls. We describe further the underlying details of our adapted MTV_EXTRACT algorithm in Section A.2 of the Supplementary. Having found \(_{j}^{}\) and \(_{l,j}\), we describe in what follows, the final procedure to use MTV for inference.

### Step 3: Multimodal Task Vector Application

After we have identified a set of attention heads \(_{j}^{}\), it is straightforward to apply MTV for inference. We denote the set of mean activations \(_{j}^{}\) as follows \(_{j}^{}=\{_{l,j}| l_{j}^{}\}\).

To run downstream inference on a new query \(Q_{}\) with our model \(F\), we directly replace the values of attention heads \(_{j}^{}\) with \(_{j}^{}\) and produce the following response \(R_{}\):

\[R_{}=F(Q_{new}|_{j}^{},_{j}^{})\] (6)

\(R_{}\) is thus a response generated using many shots of multimodal examples as implicit context via MTV. The key insight of our method is the importance of \(N\) (the number of multimodal examples) and many \(T\) (the number of iterations) during the calculation of MTV. This enables an LMM to go beyond its context length to learn more nuanced properties of the task from seeing many examples. Additionally, insertion of MTV directly into the LMM also obviates the need for any context length during downstream inference, actually _freeing_ additional context for other use (e.g., an additional prompt, more ICL examples, etc.). Finally, because we align the attention-head locations with the downstream task, MTV can be effectively applied to zero-shot and different ICL settings.

## 4 Evaluation

In order for LMMs to perform multimodal ICL, it is important for interleaved data to be included in pretraining. We apply our MTV approach to Qwen-VL , Idefics2-8B , and ViLA-1.5-8B  three popular interleaved LMMs. For each model, we compare our method to using few-shot ICL across different vision-and-language tasks like VQA and object identification.

### Implementation Details

We implemented MTV using PyTorch . We used each model's respective official implementation. While the compute and memory requirements differ slightly between models, all our experiments can be run on a single NVIDIA A6000 GPU. For additional information, refer to Supplementary Section B. Our model and weights will be released upon acceptance, and our code is in Supplementary.

### Models

In this work, we apply MTV to the following interleaved LMMs as they are better-suited for multimodal ICL as shown by : (1) **QwenVL** is a LLaMA-based model that has the ability to process high-resolution images, and its two-stage pre-training methodology, which includes multi-task finetuning and interleaved text-image data. (2) **Idefics2-8B** is a Mistral-based model that benefits from its pre-training on the expansive OBELICS dataset, which comprises a web-scale collection of interleaved image-text documents. We utilize the base version of the model. This demonstrates multimodal in-context learning abilities. (3) **LLaMA3-ViLA-1.5-8B** (abbreviated as VILA-1.5-8B). ViLA-1.5-8B  is an architecture that leverages LLaMA-3 as the LLM backbone. As in others, a significant portion of the model's pretraining data is interleaved text-image data. (4) **MANTS-LLaMA3-8B**. MANTS-LLaMA3-8B  is a combination of a SigLIP  visual encoder and LLaMA3  language model finetuned using the MANTS dataset, a specially curated multi-image dataset that emphasizes co-reference, reasoning, comparing, temporal understanding.

We show the number of tokens per image embedding for each model in Table 1 to illustrate the especial importance of MTVs in the image-text domain:

### Datasets

We briefly describe the tasks and datasets we evaluate our method on. More details about the datasets and their setup can be found in Section B.

**VQA Datasets**. We use the following commonly-evaluated datasets which emphasize different aspects of multimodal reasoning, including visual features (VizWiz) and outside knowledge (OK-VQA): (1) **VizWiz** consists of images taken by visually impaired individuals paired with questions they pose about these images, making it crucial for developing AI systems that assist in real-world, accessibility-focused visual understanding tasks. (2) **OK-VQA** dataset  is designed to push the boundaries of Visual Question Answering (VQA) by focusing on knowledge-based questions, where answers require external knowledge beyond the image content. (3)

**Object Classification**. We use the following datasets, which are commonly used for object classification in multimodal ICL: (1) The **Flowers** dataset , commonly known as the Oxford 102 Flowers dataset, is a collection specifically designed for image-based flower species recognition for fine-grained classification of 102 different categories of flowers. (2) **Caltech's CUB Dataset on Birds** is a well-known resource for evaluating algorithms on the task of object identification, specifically focused on bird species. It features 200 bird species with roughly 30 images each, annotated with key attributes and bounding boxes. Both Flowers and Birds are formatted as 2-way,1-shot classification episodes, with model inputs being a positive and negative image for the class to be identified in the query image. The response format is a short text response.

## 5 Results

Our main results are shown in Table 2. For VQA, we show the results of MTV with 4 shots per 100 iterations to calculate the mean activations and 100 examples for task vector locations (500 examples total). The task vector is extracted using examples from the train set of the dataset and evaluated on the validation set. For object classification, we extract MTV based on a 2-way, one-shot regimen per 100 iterations for both mean activations and task vector locations (200 examples total). The task vector is extracted using a train set of 30% of the object classes and evaluated on the remaining 70% of _unseen_ classes. We demonstrate how Multimodal Task Vectors outperforms zero-shot and few-shot ICL settings on three different models on VL tasks, highlighting the effectiveness of our method. Next, we describe the unique capabilities of our method, such as scaling to more samples and showing some generalizations to other tasks. More results can be found in Section A.1 of Supplementary.

  
**Model Name** & **Per Image Token Length** & **Total Context Length** \\  VILA-1.5-8B & 144 & 8192 \\ Idefics2-8B & 64 & 8192 \\ QwenVL & 256 & 8192 \\ MANTIS-LLaMA3-8B & 64 & 8192 \\   

Table 1: Per Image Embedding Token Length and Total Context Length for Models

### MTV scales with more examples

We are interested in evaluating (i) the effect of different numbers of shots used _per iteration_ to extract MTV and (ii) the effect of different numbers of _iterations_ used. We test the impact on accuracy when increasing both of these parameters for QwenVL on the VizWiz validation set. In Figure 3, we show on the left that the optimal number of multimodal ICL shots is 16 shots per iteration. Further, we show on the right side of the figure that 1000 examples yield the best performance. These results illustrate that MTV can effectively scale by utilizing larger numbers of ICL examples per iteration and also in aggregate.

### MTV works with explicit few-shot examples

One of the benefits of MTV over a few-shot ICL is the context length that is saved during inference. This is because the many-shot examples are encoded directly in the activation space rather than in the input token space. Thus, we ask whether the LMM can use the freed context for additional few-shot examples. For object classification, we formulate both Flowers and CUB as a 1-shot comparison between a positive and negative sample to identify the correct class (i.e., 2-way, 1-shot ICL by construction). We report results on 1-shot ICL and MTV with 1-shot classification during inference. MTV+1-shot ICL surpasses 1-shot ICL accuracy on these tasks, showing that MTV can be utilized alongside few-shot examples. Furthermore, it is vital to note that the evaluation classes are completely unseen by MTV. Thus, with just a 1-shot ICL example, MTV is able to generalize to unseen classes.

### MTV heads generalize to other tasks

In this experiment, we further ask whether the MTV heads \(_{j}^{}\) extracted on one task \(j\) can generalize to a separate, but similar task \(k\). To test this, we use the attention heads extracted from

Table 2: **Results**. (Left) MTV evaluated on VQA datasets. (Right) MTV evaluated on object classification datasets. The baselines are shown in gray.

ViLA-1.5-8B on VizWiz for use on OK-VQA. Our results on the left of Table 3 demonstrate that the extracted heads from one task can improve accuracy on another similar task. This generalizability of the heads is significant because it suggests that the heads from MTV may only have to be extracted once to be applied to many other similar tasks. The only calculation necessary then would be the mean activations of the many-shot examples used for the target dataset, making the application of many-shot multimodal ICL even more efficient for similar tasks.

### Finetuning as an upper bound

In Table 2(b), we compare our method to finetuning. To do this, we use finetune on the same number of examples as MTV uses from the train set and evaluate not only on the validation set but also on the validation set of another similar dataset. In particular, for a ViLA-1.5-8B model finetuned on VizWiz, we report accuracy on both VizWiz and OK-VQA validation sets. It can be seen that finetuning is indeed an upper bound on the dataset the model was finetuned on. However, we show that finetuning leads to overfitting on the finetuned dataset and even forgetting the zero-shot capabilities. In contrast, we also show that MTV not only improves zero-shot capabilities but can generalize to similar tasks with only a few inference examples Table 1(b) and Table 2(a).

### Comparison to other methods

We compare our method to two different methods that can find task vectors: Visual Task Vectors (VTV)  and Function Vectors (FV) . Originally, these works could not be applied as-is to support multimodal ICL, but here, we have implemented a version that follows the original exactly with only minor modifications to allow performing our evaluated multimodal tasks. More details about the methods can be found in Section A.2 in the Supplementary. In our experiments Table 2(b), we find that MTV surpasses both methods on VizWiz and OK-VQA. VTV are image-only task vectors that use only one-shot image examples for fixed small \(T\) iterations, and they calculate the

Table 3: **Generalization & Method Comparison** (Left) MTV-VizWiz evaluated on OK-VQA. (Right) MTV compared to VizWiz finetuning, function vectors , and task vectors .

Figure 3: **Scaling of Qwen-MTV on VizWiz:** (Left) We show the effect of varying the number of shots per iteration for a fixed 100 iterations. (Right) We also show the effect of varying numbers of iterations fixing 4 shots per iteration.

mean activations and the locations together without aligning to the downstream task. FV are text-only task vectors that use Causal Mediation Analysis  to extract task vector locations from only the output activations of the last token. The results suggest the importance of finding the task vectors by decoupling the calculation of the mean activations and locations in two separate steps to perform many-shot multimodal ICL more effectively for complex multimodal tasks.

### Compute and runtime efficiency

An important feature of our work is that multimodal ICL examples do not require explicit tokens during inference. Because of this, we are interested in the efficiency gains of our method. Intuitively, the longer MTV extraction time is amortized during downstream inference, where the runtime would be equivalent to the zero-shot case. Similarly, the memory requirements are maximal during the MTV extraction process but require the same memory as the zero-shot case afterward. In contrast, the ICL tasks have a slower runtime and larger memory requirement throughout due to running inference on \(N\) examples _for every iteration_. To demonstrate this, we calculate the maximum memory requirement in gigabytes (GB) for ViLA-1.5-8B on VizWiz using different ICL-shot counts and MTV with 400 examples. As shown in Table 4, MTV requires less runtime than 16-shot, 8-shot, and 4-shot ICL methods and also requires less memory than 16-shot ICL. These results demonstrate that MTV can encode many multimodal ICL examples with greater efficiency than few-shot methods.

## 6 Conclusion

In this work, we present Multimodal Task Vectors a compact, implicit representation that can efficiently encode many-shot multimodal ICL examples for use in complex vision-language tasks. We demonstrate this implicit model representation not only encodes a multimodal ICL task but can also enable many-shot multimodal ICL to surpass zero-shot and few-shot performance on a variety of VL tasks. Our method stands out from previous work in its ability to scale, use additional explicit multimodal ICL examples, and generalize to other similar VL tasks. Our work is a viable way to surpass the limit of context length of an LMM for multimodal ICL and demonstrates clearly that these additional examples aid in multimodal reasoning. Finally, we do not anticipate a specific negative impact, but, as with any Machine Learning method, we recommend exercising caution.

## 7 Limitations

While Multimodal Task Vectors offers substantial benefits for handling complex vision-language tasks compared to finetuning or few-shot ICL, it is important to recognize certain limitations that accompany our approach. MTV requires access to the internal architecture of an LMM, so while it is an effective solution for all open-source models, its application is restricted from proprietary models, such as GPT-4  and Gemini [79; 80]. Furthermore, while many-shot ICL is incredibly attractive for many applications, it may not be practical for low-data scenarios where synthetic data  or the transfer of MTV extracted from another dataset may be required. We feel these challenges represent great opportunities for future work in the many-shot multimodal in-context learning domain.

## 8 Acknowledgements

We would like to thank Deva Ramanan, Grace Luo, and Suzanne Petryk for their insightful feedback and discussions. This project has received funding from Prof. Darrell's group in part by DoD, including PTG and/or LwLL programs, as well as BAIR's industrial alliance programs.

  
**Metric** & **0-shot** & **4-shot** & **8-shot** & **16-shot** & **MTV (400-shot)** \\  Max GPU Memory (GB) & 17.4 & 18.3 & 19.0 & 20.6 & 19.8 \\ Runtime per 100 iterations (min) & 1.1 & 2.7 & 3.1 & 3.3 & 1.9 \\   

Table 4: **Efficiency:** We show that even though MTV encodes 400 multimodal ICL examples in the mean activations, it still requires less runtime and memory than 8-shot and 16-shot multimodal ICL.