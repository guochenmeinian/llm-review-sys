# Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression

Allan Raventos

Equal Contribution. Code released at https://github.com/mansheej/icl-task-diversity

Mansheej Paul1

Feng Chen

Surya Ganguli

Stanford University

{aravento, mansheej, fengc, sganguli}@stanford.edu

###### Abstract

Pretrained transformers exhibit the remarkable ability of in-context learning (ICL): they can learn tasks from just a few examples provided in the prompt without updating any weights. This raises a foundational question: can ICL solve fundamentally _new_ tasks that are very different from those seen during pretraining? To probe this question, we examine ICL's performance on linear regression while varying the diversity of tasks in the pretraining dataset. We empirically demonstrate a _task diversity threshold_ for the emergence of ICL. Below this threshold, the pretrained transformer cannot solve unseen regression tasks, instead behaving like a Bayesian estimator with the _non-diverse pretraining task distribution_ as the prior. Beyond this threshold, the transformer significantly outperforms this estimator; its behavior aligns with that of ridge regression, corresponding to a Gaussian prior over _all tasks_, including those not seen during pretraining. Thus, when pretrained on data with task diversity greater than the threshold, transformers _can_ optimally solve fundamentally new tasks in-context. Importantly, this capability hinges on it deviating from the Bayes optimal estimator with the pretraining distribution as the prior. This study also explores the effect of regularization, model capacity and task structure and underscores, in a concrete example, the critical role of task diversity, alongside data and model scale, in the emergence of ICL.

## 1 Introduction

Pretrained transformers (PTs) can learn new tasks from just a few examples provided in the prompt without taking any gradient steps on those examples . This ability, called _in-context learning (ICL)_, has unlocked the widspread use of language models by making it efficient to adapt general purpose models to bespoke tasks without explicit training. Though remarkable, what makes ICL mysterious, and potentially harmful , is that the learning algorithm implemented by the PT in its forward pass is not built into its architecture or training process; instead it emerges from pretraining on large-scale data with a next token prediction objective. This raises a foundational question: can ICL really solve fundamentally _new_ tasks that are very different from those seen during pretraining? If so, what learning algorithm does ICL implement? To answer these questions, we need to better understand how the different ingredients that go into pretraining influence this ability.

Towards this end, we explore how the diversity of tasks in the pretraining data affects the emergence of ICL. Prior work  has proposed that ICL works by performing Bayesian inference. During pretraining, transformers learn a prior over latent tasks represented in the pretraining data. When prompted with examples at inference time, they "retrieve" relevant pretraining tasks and generate subsequent tokens from the posterior distribution conditioned on the query and inferred tasks. This suggests that ICL performance on a new task is influenced by its similarity to tasks implicitly learned during pretraining. However, the distribution of tasks in our pretraining data, \(_{}\), is usually alimited and unrepresentative subsample of the ideal distribution of tasks, \(_{}\), that we want our model to be capable of learning in-context. For instance, \(_{}\) could be the set of all instructions we want an A.I. assistant to follow. But, large-scale language modeling datasets [4; 5] used to pretrain these models contain very few examples correctly formatted for ICL. Instruction finetuning (IFT) datasets [6; 7; 8; 9; 10; 11] designed to ameliorate this are expensive to collect and thus contain tasks from just a few domains. Under the Bayesian framework, this distribution mismatch would cause the Bayesian estimator with a prior over the limited pretraining tasks, \(_{}\), to perform suboptimally on tasks that are very different from those seen during pretraining. This motivates our question: can a model pretrained on a dataset with low task diversity nevertheless learn _new, unseen_ tasks?

For general purpose language modeling, the size and complexity of \(_{}\) and the vague specification of \(_{}\) make this question challenging to analyze. So, following recent work [12; 13; 14], we study ICL for linear regression. Here, a task is a linear regression problem with a given latent regression vector; the PT must predict the target for a new data point from examples of data-target pairs provided in the prompt. Prior work  has shown that transformers that see an _unlimited_ number of latent regression vectors during pretraining learn to perform ridge regression with the Bayes optimal ridge parameter. We instead consider the case where the pretraining task distribution, \(_{}\), contains a _limited and finite_ set of latent regression vectors (see Section 2 for details). To evaluate its ability to learn new tasks, the PT is tested on the ideal task distribution, \(_{}\), which is a Gaussian distribution over _all_ latent regression vectors. Studying this setting has two advantages: first, we can directly vary the task diversity in \(_{}\) by changing the number of unique latent regression vectors seen during pretraining. Second, we can calculate the optimal estimator that minimizes the pretraining loss--the Bayesian estimator with prior \(_{}\)--as well as the optimal estimator for all tasks--the Bayesian estimator with prior \(_{}\). This allows us to interpret the behavior of the PT by comparing its predictions to those of the optimal estimators under either task distribution. In our work, we vary the pretraining task diversity and probe the PT's ability to learn fundamentally new tasks in-context: does it behave like the optimal estimator for \(_{}\) and perform suboptimally on tasks from \(_{}\), or does it align with the optimal estimator for \(_{}\) which can solve new, unseen tasks?

**Contributions.** Our contributions are as follows:

* We find that a transformer pretrained on data with low task diversity behaves like the Bayesian estimator with prior \(_{}\); it performs optimally on pretraining tasks but cannot learn new tasks in-context. However, as pretraining task diversity increases, the PT deviates from this Bayesian estimator, significantly outperforming it on new tasks, and at a large but still finite number of pretraining tasks, the PT's performance closely matches that of the optimal estimator on \(_{}\).
* We identify a task diversity threshold for the emergence of ICL. Below this threshold, increasing the pretraining dataset size while keeping task diversity constant biases the PT towards the pretraining task distribution. Conversely, beyond this threshold, increasing the dataset size without increasing its task diversity improves the PT's performance on new, unseen tasks. This suggests that the PT's behavior undergoes a sharp algorithmic phase transition in the limit of many examples per task, aligning with the optimal estimators on \(_{}\) before the threshold and on \(_{}\) after it. We also examine this transition from the perspective of learning dynamics.
* We empirically show that increasing the task dimension at fixed SNR increases the task diversity threshold. However, the scaling of the PT's error with dimension is vastly superior to that of the optimal Bayesian estimator for \(_{}\); at a task diversity that is beyond the threshold at all dimensions we consider, the PT remains near-optimal with increasing dimension, whereas the optimal estimator for \(_{}\) grows progressively less similar to the optimal estimator for \(_{}\).
* We show that increasing weight decay significantly decreases the task diversity threshold while increasing number of layers or embedding size increases the task diversity threshold. This elucidates the effect of regularization and model capacity on the emergence of ICL.

Overall these contributions suggest that the emergence of in-context learning in pretrained transformers cannot be fully explained by a theory of Bayesian inference on the pretraining distribution.

## 2 Problem setup

**ICL of linear regression (schematic in Fig. 1).** Each ICL _task_ corresponds to a latent \(D\)-dimensional regression vector, \(^{D}\). At inference time, the transformer takes as input a sequence of \(K\) data-target pairs, \((_{1},y_{1},...,_{K},y_{K})\), which are the in-context examples corresponding to this _single_ task 

[MISSING_PAGE_FAIL:3]

\(_{i=1}^{M}^{(i)}\) and for \(k\{2,...,K\}\), (Appendix A.2)

\[}_{k}^{}=_{i=1}^{M}}_{j=1}^{k-1}(y_{j}-^{(i)}_{j})^{2 })}{_{l=1}^{M}(-}_{j=1}^{k-1}(y_{j} -^{(l)}_{j})^{2})}^{(i)}.\] (2)

Intuitively, \(}_{k}^{}\) is just a weighted sum of the pretraining \(^{(i)}\)s with weight governed by the likelihood of observing targets \(\{y_{1},...,y_{k-1}\}\) conditioned on inputs \(\{_{1},...,_{k-1}\}\) and the task being \(^{(i)}\). A PT that minimizes the pretraining loss \(L^{_{}}\) will behave like this estimator.

For task distribution \(_{}=(,_{D})\), the **Ridge** regression estimator with the ridge parameter set to the noise scale \(^{2}\) is optimal: \(_{k}^{}=(}_{k}^{})^ {}_{k}\), where \(}_{1}^{}=\) and for \(k=\{2,...,K\}\),

\[}_{k}^{}=(^{}+ ^{2}_{D})^{-1}^{},\] (3)

where \(=(_{1}^{},...,_{k-1}^{}) ^{(k-1) D}\) and \(=(y_{1},...,y_{k-1})\) (Appendix A.3). A PT that performs optimally on new tasks will behave like this estimator. We can compare the behavior of the PT to that of the optimal estimators by computing the mean square difference of the predictions under a given task distribution \(\). We write this as

\[_{}^{}= _{1},...,_{K}(,_{D}) \\ _{1},...,_{K}(0,^{2})}{ }[_{k=1}^{K}(f_{}(S_{k})-_{k} ^{})^{2}].\] (4)

## 3 Experiments and results

Unless specified otherwise, we study linear regression in \(D=8\) dimensions with up to \(K=16\) in-context examples and observation noise variance \(^{2}=0.25\). We use either a _base_ transformer

Figure 2: **ICL emerges in PTs beyond a threshold pretraining task diversity.** We show all results on both tasks seen during pretraining (_top row_) and on new tasks (_bottom row_). The _left column_ compares the normalized loss of transformers pretrained with increasing task diversity to that of dMMSE and Ridge. When the pretraining task diversity is small, the PT’s performance matches that of dMMSE; it performs very well on tasks seen during pretraining but poorly on new tasks. As the pretraining task diversity increases, both dMMSE and PT approach Ridge. However, the PT approaches Ridge much faster, significantly outperforming dMMSE on new tasks (_bottom left_). In the _middle and right columns_, we compare the PT’s predictions to those of dMMSE and Ridge respectively (Eq. (4)). We also increase the number of sequences per task at each level of task diversity by increasing the batch size while keeping total training steps fixed. This reveals a task diversity threshold between \(2^{14}\) and \(2^{15}\) pretraining tasks at which there is a phase transition in the behavior of the model. Below the threshold, increasing the dataset size leads to PTs with predictions more aligned with dMMSE on \(_{}\) (_top middle_). However, beyond this threshold (indicated by gray shading), increasing the dataset size leads to PTs more aligned with Ridge on all tasks (_right_).

model with the GPT2 architecture  with 8 layers, 128-dimensional embeddings, and 2 attention heads or a _small_ model with 4 layers, 64-dimensional embeddings, and 2 attention heads. We train with the Adam optimizer  and a one-cycle triangle learning rate schedule  with 50% warmup. The _base_ model is trained with batch size 256 for 500K training steps, though these hyperparameters are varied in our experiments. We always sweep over a range of learning rates and choose the largest learning rate at which training is stable. For further details see Appendix B.

To pretrain a randomly initialized transformer on data with task diversity \(M\), we first construct the pretraining task distribution, \(_{}\), as described in Section 2. We then minimize the objective \(L^{_{}}\) in Eq. (1) using minibatch stochastic gradient descent. For each sequence in a minibatch, we sample a single task \(\) from \(_{}\), as well as _new_ samples of data, \(\{_{i}\}_{i=1}^{K}\), and noise, \(\{_{i}\}_{i=1}^{K}\), from their respective continuous distributions, to form a sequence \((_{1},^{}_{1}+_{1},, _{K},^{}_{K}+_{K})\). If we train for \(N\) steps at batch size \(B\), the transformer will see a total of \(NB\)_unique_ sequences and roughly \(\) unique sequences for each latent task in \(_{}\). By increasing either \(B\) or \(N\) at fixed \(M\), we can increase the total size of the pretraining dataset (or number of sequences per task) while keeping the dataset _diversity_--the number of unique ws in \(_{}\)--fixed.

### Task diversity threshold for the emergence of in-context learning

For Fig. 2, we pretrain our base transformer on datasets with increasing task diversity (on the x-axis) while keeping the total number of sequences seen during pretraining fixed (\(B=256,N=500K\)). We evaluate the PTs and both optimal estimators on tasks seen during pretraining drawn from \(_{}\) (Fig. 2 top left) and on new tasks drawn from \(_{}\) (Fig. 2 bottom left) and plot MSE normalized by task dimension--\(L^{}/D\) from Eq. (1)). Since dMMSE is optimal on tasks from \(_{}\) (as discussed in Section 2), the green dMMSE markers denote the lowest possible loss the PT could achieve in this setting. In fact, the pretraining objective \(L^{_{}}\)_explicitly encourages_ the PT to match dMMSE performance. On the other hand, Ridge is optimal on tasks sampled from \(_{}\) (Fig. 2 bottom left); the blue markers denote the lowest possible MSE the PT could attain on _new_ tasks.

**Low task diversity phase: the PT is Bayesian with respect to the pretraining distribution and cannot solve new tasks.** At low pretraining task diversity--\(M\) up to about \(2^{6}\)--the PT's MSE closely tracks that of dMMSE on tasks sampled from \(_{}\) (Fig. 2 top left); the PT performs optimally on tasks seen during pretraining. But it significantly _underperforms_ on new tasks sampled from \(_{}\), indicated by the gap in MSE between the PT and Ridge (Fig. 2, bottom left). In this regime, it behaves like the Bayesian estimator with prior \(_{}\).

**High task diversity phase: the PT is non-Bayesian with respect to the pretraining task distribution and can solve new tasks.** At higher task diversities--above \(2^{6}\) pretraining tasks--the PT's MSE deviates from dMMSE and approaches Ridge under _both_\(_{}\) and \(_{}\). Crucially, the PT starts to significantly _outperform_ dMMSE on unseen tasks sampled from \(_{}\) (Fig. 2 bottom left) at the expense of not fully minimizing its training objective, \(L^{_{}}\) (gap between PT and dMMSE under \(_{}\), Fig. 2 top left). This suggests that, a PT trained on a finite but large number of pretraining tasks can learn fundamentally new tasks in-context and this ability depends on it deviating from the optimal Bayesian estimator on the pretraining task distribution.

Figure 3: **Increased pretraining steps reveals the same task diversity threshold for the emergence of ICL.**_Columns 1 and 2_ in this figure are similar to the middle column in Fig. 2 and _columns 3 and 4_ correspond to the right column in Fig. 2, except here we increase the number of sequences per task by increasing the number of training steps while keeping batch size = 256. Both methods of increasing dataset size—increasing batch size in Fig. 2 and increasing training steps in this figure—reveal a transition in the behavior of the PT: beyond the task diversity threshold, ICL on new tasks emerges.

**Finite size scaling of training data suggests an algorithmic phase transition as task-diversity increases.** The experiments in Fig. 2 (left column) suggest that, when tested on both task distributions \(_{}\) and \(_{}\), the ICL algorithm implemented by a PT exhibits a smooth crossover in performance from dMMSE to Ridge. We next examine how this transition changes as we increase the number of sequences per task seen over pretraining, at fixed task diversity. One might reasonably expect that, if the transformer sees more sequences per latent task in \(_{}\), both its predictions and performance should become more similar to those of dMMSE, and less similar to those of Ridge, at all values of task diversity. Strikingly, this natural expectation is violated in a manner that facilitates ICL on \(_{}\).

At each number of tasks, we increase the number of sequences per task by increasing batch size from 256 to 512 to 1024, while leaving the number of training steps fixed at 500K. We observe that \(_{}^{_{}}\), which quantifies how different the PT and dMMSE estimator's predictions are when testing on tasks drawn from \(_{}\), does in fact decrease for \(M 2^{10}\) (Fig. 2 top center) as we train on more sequences per task. Moreover, for each \(M\{2^{10},...,2^{14}\}\) the PT's predictions also become less similar to those of Ridge, both on tasks from \(_{}\) (Fig. 2, top right) and \(_{}\) (Fig. 2, bottom right). **Crucially**, this movement in behavior of the PT towards dMMSE and away from Ridge, at least on tasks drawn from \(_{}\), holds _only_ up to a threshold number of tasks between \(2^{14}\) and \(2^{15}\). Beyond this threshold, pretraining on more sequences per task at a fixed task diversity actually makes the PT _more_ like Ridge, in that both \(_{}^{_{}}\) and \(_{}^{_{}}\) decrease (Fig. 2, right top and right bottom respectively). This means that, beyond a task diversity threshold, the PT can not only optimally solve new tasks from \(_{}\) by matching Ridge performance, but also the PT _gets better_ at doing so if trained on more sequences per task, despite the limited set of tasks experienced in pretraining. Thus, in contrast to the natural expectation stated above, more sequences per task does not promote overspecialization of the PT to the \(_{}\) at task diversities beyond the threshold.

Finally, the motion of the ICL algorithm implemented by PT towards (away) from Ridge above (below) a task diversity threshold (Fig. 2, right top and bottom) indicates that as one increases the number of sequences per task at fixed task diversity, the smooth cross over in performance of the PT between dMMSE and Ridge, shown in Fig. 2, left top and bottom, will become sharper and sharper in task diversity, ultimately exhibiting a sharp phase transition in the limit of infinite number of sequences per task. Remarkably, this phase transition in the ICL algorithm implemented by the PT appears at a moderate task diversity threshold below \(2^{15}\) pretraining tasks; even though dMMSE significantly underperforms relative to Ridge on \(_{}\) at this task diversity, the PT nevertheless remains unimpaired by this limited task diversity and can optimally solve new tasks.

**Increased training time at fixed batch size further supports an algorithmic phase transition.** To confirm the above results, we also increase the number of sequences per task, at each task diversity, by increasing the number of training steps \(N\) from 500K to 1M while keeping batch size fixed at 256. We observe that doubling \(N\) (change from pale blue to red in Fig. 3) and doubling \(B\) (change from pale

Figure 4: **Learning dynamics of _small_ PTs shows a transition at the task diversity threshold.** We plot \(_{}^{_{}}\) vs training steps for _small_ PTs. For the same \(M\), learning curves for short (500K steps, **left**) or long (2M steps, **center**) training durations are similar, and for \(M>M^{*} 2^{11.5}\) learning curves are similar to that of a model trained with infinite task diversity. **Right**: For \(M 2^{10}\), \(t^{*}\) (the training step at which \(_{}^{_{}}\) is minimized) is well modeled by a scaling law \(t^{*} M^{}\). A linear fit of \( t^{*}\) vs \( M\) (dashed red line) gives \( 0.47\). But for \(M>2^{10}\), \(_{}^{_{}}\) decreases through training; \(t^{*}=\) 2M, is larger than predicted by the scaling law. This sudden break in the scaling law suggests a fundamental difference in the learning dynamics of models on either side of the threshold.

blue to red in Fig. 2) have very similar effects on \(_{}^{}\) and \(_{}^{}\), for both \(=_{}\) and \(=_{}\). More importantly, the task diversity threshold, which we determined as the cross-over point in \(_{}^{_{}}\) between batch sizes 256, 512, and 1024 at 500K training steps (Fig. 2 bottom right) happens at the same number of tasks as the crossover point between 500K and 1M steps at batch size 256 (Fig. 3, right). Given that our two approaches for training the baseline transformer on more data yield the same task diversity threshold, and that doubling batch size leads to significantly faster training times than doubling number of steps, from here onward we consider the task diversity threshold to be cross-over point in \(_{}^{_{}}\) between batch sizes 256 and 512 when training for 500K steps. See Appendix D for more ablations of batch size and training steps that provide further evidence for how the _number of sequences_ seen by the transformer is the key factor determining the similarity of its predictions to those of dMMSE and Ridge at each number of tasks.

**Learning dynamics and a break in the scaling of early stopping time further supports an algorithmic phase transition.** To probe if the observed transition is merely an effect of under-fitting, we study the learning dynamics of _small_ PTs in the _very large_ number of steps regime. First, in Appendix E, we verify that the _small_ PT also demonstrates an algorithmic phase transition but at lower task diversity threshold between \(2^{11}\) and \(2^{12}\) pretraining tasks. In Fig. 4 left, we visualize the learning curves (\(_{}^{_{}}\) vs training steps) of PTs trained for 500K steps at batch size 512 with pretraining task diversities, \(M\), below and above the task diversity threshold, \(M^{*}\). For \(M<M^{*}\), \(_{}^{_{}}\) decreases early in training until it reaches a minimum at time step \(t^{*}\), and then increases as the PT approaches dMMSE. We define \(t^{*}\) as the early stopping time for Ridge. For \(M>M^{*}\), \(_{}^{_{}}\) decreases throughout training. To evaluate if, in the latter case, models are undertrained and \(t^{*}\) is larger than the total training time, we extend training to 2M steps at batch size 512 (\(4\) the training time, see Appendix B). Fig. 4 center, shows these learning curves along with that of the model trained with infinite task diversity; even in this long training regime, the task diversity threshold does not change. For both short and long training durations, models trained with the same \(M\) have similar qualitative behavior (whether distance to Ridge decreases then increases or monotonically decreases). Additionally, learning curves of the models with \(M>M^{*}\) are very similar to the learning curves for models trained on infinite pretraining task diversities and they achieve similar final accuracy (dahed lines vs markers in Fig. 10), suggesting that these models are approaching the Ridge solution.

In Fig. 4 right, we study how \(t^{*}\), scales with \(M\). For most \(M<M^{*}\), \(t^{*}\) obeys a simple scaling behavior \(t^{*} M^{}\), \( 0.47\). However, for \(M>2^{10}\), the distance to Ridge decreases monotonically through training and \(t^{*}\) = 2M steps. Despite the caveat that our experiments are necessarily in the large but _finite_ training step regime with a decayed learning rate schedule, this stark break in the scaling behavior of \(t^{*}\) near the task diversity threshold suggests that the observed transition is not just caused by under-fitting but an underlying difference in the learning dynamics.

Figure 5: **Transformers pretrained with high, but not low, task diversity can learn _new_ tasks in-context.** We compare the normalized loss of the PT to that of dMMSE and Ridge as we interpolate between tasks in the pretraining dataset. _Left_: At \(2^{5}\) tasks, well below the task diversity threshold, the PT performance matches that of the dMMSE estimator along interpolating paths, but under-performs Ridge on _new_ tasks at the center. _Middle_: At \(2^{10}\) tasks, the PT outperforms dMMSE on _new_ tasks at the center of the interpolation path, but is not yet as good as Ridge on new tasks. _Right_: At \(M=2^{15}\) tasks, just above the task diversity threshold, the PT performs as well as Ridge even on new tasks at the center. This demonstrates that, when pretrained on data with a finite but large number of unique tasks, the PT, unlike the Bayes optimal estimator for \(_{}\), can learn new tasks in-context.

**The transition along interpolating paths.** To obtain an additional description of the algorithmic transition in the PT from dMMSE to Ridge, we compute the ICL performance of the PT, and compare it to both dMMSE and Ridge, on a one parameter family of new tasks \(_{}\) that interpolate between pairs of seen tasks \(_{i}\) and \(_{j}\) in the support of \(_{}\). The interpolation path is given by

\[_{}=(\|_{i}\|_{2}+\|_{j}\|_{2}) _{i}+(1-)_{j}}{\|_{i}+( 1-)_{j}\|_{2}}.\] (5)

Here we fix the norm of the interpolated vector \(_{}\) to the average of the two endpoint norms to avoid \(\|_{}\|\) taking on very small values for \(\). Fig. 5 shows the results of this analysis for \(2^{5}\) (left, low task diversity regime), \(2^{10}\) (center, below task diversity threshold), and \(2^{15}\) (right, just above the task diversity threshold) tasks. At each value of \(\), MSE is averaged over a large number of task pairs \((_{i},_{j})\). Examination of the average performance at the center of many interpolation paths, corresponding to fundamentally _new_ tasks far from tasks seen during pretraining, clearly reveals a transition in PT performance from dMMSE to Ridge, where new tasks can only be optimally learned above, but not below, the task diversity threshold. In contrast, unlike the PT, dMMSE cannot solve new tasks at any task diversity in the range considered.

**The PT outperforms a smoothed dMMSE model.** We have seen that at an intermediate task diversity the PT significantly outperforms dMMSE on new tasks in \(_{}\). It is clear why dMMSE performs poorly on new tasks in \(_{}\) at low task diversity: its prior over tasks concentrates on \(M\) unique tasks in \(_{}\), while the prior over tasks in \(_{}\) is Gaussian. A natural conjecture is that the PT cannot memorize all \(M\) tasks in \(_{}\) for large enough \(M\). Therefore we also compare PT performance to a smoothed dMMSE estimator in which the discrete point prior over \(M\) tasks seen in pretraining is replaced with a mixture of \(M\) isotropic Gaussians with the same centers but with variance chosen to optimize performance on \(_{}\) (see Appendix G for details). This smoothed dMMSE outperforms dMMSE as it has a prior over tasks closer to the Gaussian \(_{}\). But remarkably, the PT still outperforms the smoothed dMMSE even with optimal smoothing (Fig. 12). This indicates the PT, even at moderate task diversity, implements a more sophisticated algorithm than a simple smoothed dMMSE arising from the PT's inability to resolve the \(M\) pretraining tasks to high precision.

### The PT exhibits superior scaling of task diversity threshold with dimension than dMMSE.

We next explore the dependence of the task diversity threshold on the regression problem dimension \(D\). We vary \(D\{8,16,32\}\) while simultaneously scaling up maximal context length as \(K=2D\), and increasing observation noise \(^{2}\) to match the SNR to that of \(D=8\) and \(^{2}=0.25\). We also train a larger model with 12 layers, 256-dimensional embeddings, and 4 attention heads that is sufficiently expressive to match Ridge performance at \(D=32\). Fig. 6, first 3 panels reveal that the task diversity threshold of the PT increases moderately (approximately linearly) with task dimension (i.e. roughly \(2^{14}\), \(2^{15}\), and \(2^{16}\) at \(D=8,16\), and \(32\) respectively). This linear scaling is remarkable considering the volume of all possible tasks scales _exponentially_ with dimension due to the concentration of the Gaussian \(_{}\) to a sphere for large \(D\). Thus we expect dMMSE performance to scale much more poorly with dimension \(D\) since the finite number of tasks in \(_{}\) would need to cover a substantial portion of the sphere for dMMSE to approach Ridge. To test this hypothesis, for \(M=2^{20}\), which is

Figure 6: **The task diversity threshold increases with task dimension, and the PT’s ability to solve new tasks scales significantly better than dMMSE’s.** We vary the dimension of the regression problem \(D\) (_first three panels_) while leaving the signal-to-noise ratio fixed. The task diversity threshold consistently increases with task dimension (gray shading denotes post threshold). At \(2^{20}\) tasks (_right_), the PT’s predictions are similar to those of Ridge at all \(D\) (orange \(_{}^{_{}}\)), whereas dMMSE grows progressively less similar to Ridge (blue \(_{}^{_{}}\)).

the largest task diversity we consider, we explore how the similarity of PT and dMMSE predictions to Ridge on new tasks scales with \(D\) (Fig. 6, right panel). We see that \(_{}^{T_{}}\) grows significantly as we increase \(D\), while remarkably \(_{}^{T_{}}\) is largely dimension independent. Overall this indicates that the scaling of PT error with dimension is vastly superior than that of dMMSE; PT remains near optimal and close to Ridge at all \(D\) for \(M=2^{20}\), while dMMSE departs from Ridge as \(D\) increases.

### Effect of Regularization and model capacity on the task diversity threshold.

We study the dependence of the task diversity threshold on various hyperparameters. First, adding explicit regularization in the form of weight decay (see Appendix B for details), and increasing its value over three orders of magnitude, consistently lowers the threshold task diversity (Fig. 7, left). Note however, the lower task diversity threshold also comes with worse performance (Figure 13, top). This suggests that various forms of _implicit_ regularization could help drive the algorithmic transition in the PT without weight decay. We also explore the effect of model capacity on the task diversity threshold by either increasing the embedding dimension of both _small_ and _base_ PTs or increasing the depth of _small_ PTs. Fig. 7 center shows that increasing embedding dimension over a reasonable range does not affect the task diversity threshold of _base_ PT. However, for _small_ PT, increasing either the embedding dimension (Fig. 7 center) or depth (Fig. 7 right) increases the task diversity threshold. _Base_ PT has a much larger capacity then _small_ PT and also has a larger threshold; we hypothesize that _small_ PT is still in a regime where the threshold is sensitive to capacity while _base_ PT is not. Together, these results suggest that model capacity plays an important role in the emergence of in-context learning: increasing capacity (up to a point) leads to an increase in the task-diversity threshold.

## 4 Related work

The Bayesian framework for ICL introduced by Xie et al. , which motivates our work, hypothesizes that PTs "locate" concepts learned during pretraining to solve ICL tasks. A series of empirical work in language models [18; 19; 20] use this framework to select better in-context examples while Min et al.  use it to study the robustness of latent task inference. Our work builds on this framework in the linear regression setting and validates it at low task diversities. However, we find a regime--large but finite number of pretraining tasks--in which the ability to learn new tasks in-context is an emergent phenomenon that cannot be fully explained by Bayesian inference.

Prior work [12; 13; 14] has also shown that transformers can do linear regression in-context. However, they pretrain with unlimited task diversity, sampling a completely new regression vector for each sequence. In contrast, our work considers pretraining datasets with limited task diversity where ICL on new tasks emerges even though the pretraining loss does not explicitly encode it. Another line of work hypothesizes that ICL performs gradient descent in the activations of the forward pass, providing explicit constructions for the weights of the PT to implement this for linear regression [13; 14] or exploring this hypothesis in language models . However more experiments are required to test the hypothesis that _trained_ transformers actually match proposed constructions. Instead of studying the explicit mechanism by which in-context learning is implemented, our work focuses on the impact of the pretraining task diversity. Similar questions pertaining to the role of task diversification have been explored in the meta-learning literature [23; 24; 25].

Figure 7: **Explicit regularization and model capacity affect the task diversity threshold. Increasing explicit regularization, in the form of weight decay, lowers the task diversity threshold in _base_ PTs (_left_). Increasing embedding dimension, has no effect on the threshold for _base_ PTs, but does increase the threshold for a _small_ PT (_middle_). Increasing depth, while holding other hyperparameters fixed, increases the threshold for a _small_ PT (_right_). See Figure 13 for plots of \(_{}^{T_{}}\) vs \(M\).**

Kirsch et al. (2018) also show the emergence of in-context learning with pretraining task diversity on a toy classification task. By studying this question in the controlled setting of linear regression, we can compare to the optimal estimators on \(_{}\) and \(_{}\). This allows us to establish that ICL at finite task diversity emerges because the PT _departs_ from the optimal estimator on the pretraining task distribution, and is not just a consequence of the pretraining task distribution becoming similar to the ideal task distribution. Among other important perspectives on ICL, Chan et al. (2019) identify, in a toy setting, several properties of the training distribution--burstiness and occurrence of rare classes--that are necessary for the emergence of ICL. Wei et al. (2019) study how ICL in large language models is affected by semantic priors and input-label mappings, focusing on differences across model scale. Olsson et al. (2019) study inductions heads--circuits responsible for completing patterns by copying tokens--as a mechanism for implementing ICL.

## 5 Discussion

Overall, we have extensively explored the impact of pretraining task diversity on the emergence of in-context learning of fundamentally _new_ tasks not seen during pretraining. We found several surprises by working in the controlled setting of linear regression, where we could compare the performance of the PT to Bayesian estimators that are optimal, either for the limited diversity pretraining task distribution \(_{}\) (i.e. dMMSE), or for the diverse ideal task distribution \(_{}\) (i.e. Ridge). These comparisons reveal an algorithmic phase transition in the PT from the former to the latter at an intermediate task diversity threshold; beyond this threshold, the PT solves fundamentally new tasks not seen during pretraining. Strikingly, this task diversity threshold scales moderately with task dimension, over the range of dimensions considered, despite the exponential growth in the volume of all possible tasks with dimension. Indeed this PT scaling vastly outperforms that of dMMSE. Overall, these results indicate that ICL of new tasks by PTs is an emergent phenomenon that cannot be explained by Bayesian inference on limited diversity pretraining task distributions. Moreover, our experiments suggest some form of implicit regularization in PTs allows them to break free of the pretraining task distribution to solve new tasks, given a moderate pretraining task diversity.

Remarkably, beyond the task diversity threshold, PTs learn the _optimal_ estimator for the underlying generative model for pretraining tasks; this is the case for both Gaussian and Laplace priors over tasks (see Figure 14 for experiments with Laplace prior). This is true even though solutions with lower training loss exist; indeed when trained on more data at fixed diversity, PTs behave more like Ridge at the expense of higher training loss. Our experiments in Fig. 4 suggest that this algorithmic transition is due an underlying change in learning dynamics. We explore this hypothesis by probing the _linear mode connectivity_ of the loss landscape (Srivastava et al., 2014; Srivastava et al., 2014). In Fig. 11 we find that PTs trained with large \(M\) inhabit the same loss basin as PTs trained with \(M=\): the training loss barrier between PTs trained with \(M 2^{13}\) and PTs with \(M=\) is similar to two PTs trained with \(M=\). In contrast, there are large loss barriers between PTs trained with \(M<2^{13}\) and \(M=\). Additionally, PTs trained with \(M=\) are closer in weight space to PTs trained with large \(M\) than those trained with small \(M\) (see Appendix F). Overall, these experiments provide further evidence that PTs trained with task diversities beyond the threshold find solutions similar to the optimal model for \(_{}\); we leave further exploration of these loss landscapes to future work.

An intriguing question is how these observations carry over to language. A key mystery about the efficacy of ICL in language tasks lies in how different the tasks learned in-context are from the pretraining distribution of large language corpora. It is also less clear how to categorize the contents of such corpora according to tasks and measure their resulting task diversity. Regardless, our observation in linear regression that a moderate threshold in pretraining task diversity can enable PTs to solve new tasks may imply that many language tasks that are quite different from the statistics of large language corpora can still nevertheless be solved in-context.

Our results also suggest that the scale of data alone does not lead to good ICL performance. In fact, below the task diversity threshold, increasing the size of the pretraining dataset without increasing task diversity hurts ICL performance. It is necessary to increase both the diversity and size of the dataset for ICL to emerge. Thus to improve ICL in language settings, our work motivates future studies into uncovering the relevant notion of tasks in language modeling and approaches to increase task diversity in language corpora. More generally, our empirical analysis of the impact of pretraining task diversity on ICL motivates further theoretical studies. Such studies will be key to understanding the mystery of why simple next token prediction during pretraining can lead to in-context learning of so many apparently different tasks.