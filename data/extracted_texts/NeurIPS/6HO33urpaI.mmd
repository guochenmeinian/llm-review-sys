# Open-Book Neural Algorithmic Reasoning

Hefei Li, Chao Peng, Chenyang Xu, Zhengfeng Yang

Shanghai Key Laboratory of Trustworthy Computing

Software Engineering Institute

East China Normal University, Shanghai, China

51255902127@stu.ecnu.edu.cn,

{cpeng, cyxu, zfyang}@sei.ecnu.edu.cn

Correspondence to Chao Peng and Chenyang Xu. The authors are ordered alphabetically.

###### Abstract

Neural algorithmic reasoning is an emerging area of machine learning that focuses on building neural networks capable of solving complex algorithmic tasks. Recent advancements predominantly follow the standard supervised learning paradigm - feeding an individual problem instance into the network each time and training it to approximate the execution steps of a classical algorithm. We challenge this mode and propose a novel open-book learning framework. In this framework, whether during training or testing, the network can access and utilize all instances in the training dataset when reasoning for a given instance.

Empirical evaluation is conducted on the challenging CLRS Algorithmic Reasoning Benchmark, which consists of 30 diverse algorithmic tasks. Our open-book learning framework exhibits a significant enhancement in neural reasoning capabilities. Further, we notice that there is recent literature suggesting that multi-task training on CLRS can improve the reasoning accuracy of certain tasks, implying intrinsic connections between different algorithmic tasks. We delve into this direction via the open-book framework. When the network reasons for a specific task, we enable it to aggregate information from training instances of other tasks in an attention-based manner. We show that this open-book attention mechanism offers insights into the inherent relationships among various tasks in the benchmark and provides a robust tool for interpretable multi-task training.

## 1 Introduction

Deep neural networks have achieved remarkable advancements in various areas, such as image processing [18; 6] and natural language processing [16; 21]. In recent years, as deep learning continues to evolve, there has been an increasing desire to see deep neural networks take on more complex tasks. Algorithmic reasoning tasks [27; 5; 28] have emerged as a particularly crucial category. In classical domains, deep neural networks have demonstrated their ability to learn predictive patterns from training data. The aspiration now is to extend this capability to the field of algorithmic reasoning, which motivates a burgeoning domain -- _Neural Algorithmic Reasoning_ (NAR).

Neural algorithmic reasoning was initially coined by . The central objective of this domain is to develop and train neural networks with the capability to imitate classical rule-based algorithms, such as sorting algorithms and graph algorithms. Networks built in this manner demonstrate the ability to perform algorithmic computations similar to traditional algorithms in reasoning tasks, while showcasing improved computational efficiency compared to them . Moreover, recent literature [31; 22] shows that owing to the characteristics of deep learning, these networks exhibit flexibility in handling diverse input formats, making them robust even in scenarios where certain input features are missing.

**Challenging Benchmark for NAR.** CLRS Algorithmic Reasoning Benchmark proposed by  is currently the most popular and definitive benchmark for evaluating the algorithmic capabilities of neural networks. This benchmark comprises 30 diverse algorithmic reasoning tasks extracted from the foundational algorithms textbook "Introduction to Algorithms" , including sorting, searching, dynamic programming, graph algorithms, string algorithms, and more. Beyond the task diversity, another notable challenge of this benchmark is the _significant differences_ in scale between problem instances in the training and test sets. The test instances are substantially larger in scale compared to those in the training set.

There have been many recent advances in exploring CLRS [7; 19; 2; 8; 24; 3]. As classical algorithms can often be represented by graph structures, several successful approaches leverage the Graph Neural Network (GNN) framework, including models such as PGN  and MPNN . In addition to directly applying these classical GNNs, the literature has observed that the execution of some classical algorithms often relies on specific data structures. Consequently, there have been proposals to integrate classical GNNs with data structures like priority queues  or stacks  to enhance neural reasoning capabilities.

However, we notice that all prior approaches predict algorithmic executions based solely on their parameters and the features of a single input. Although this mode is commonly used in traditional supervised learning tasks [20; 1], it may not be well-suited for NAR due to the inherent difference between complicated reasoning tasks and traditional tasks like image processing. In practical scenarios, when recognizing images, extensive background knowledge is typically not required; but when faced with complex reasoning tasks, a substantial amount of background knowledge is often necessary to complete various aspects of the reasoning process. In such situations, having real-time illustrative examples or formulas available for reference can significantly reduce our memory burden, thereby enhancing task completion. This naturally raises a question:

_If allowing a neural network to access additional examples for reference during reasoning, will its reasoning capability improve as a result?_

### Our Contributions

We explore the aforementioned question and introduce open-book neural algorithmic reasoning. In this model, the neural architecture is enhanced with an additional memory component that stores representations of instances in the training dataset. Whether during training or testing, whenever the network engages in reasoning for a specific instance, it has the capability to leverage this supplementary memory to aggregate information from other instances within the training set, akin to an open-book exam. The main results of the paper are summarized as follows:

* We present a general framework for open-book NAR. This framework builds upon the foundation of previous NAR architectures by introducing two additional modules for embedding and information aggregation from the training set, and can seamlessly integrate with existing methods. We further provide a detailed implementation of the framework, which is grounded in the cross-attention mechanism. This design not only caters to single-task training but also proves to be highly effective in scenarios involving multi-task training.
* Empirical evaluations are conducted on the challenging CLRS Benchmark . We incorporate the proposed framework with three popular network architectures in the literature. The results demonstrate that each architecture's reasoning capability can be improved significantly when utilizing the training instances through the framework. Across the majority of the reasoning tasks within the benchmark, the framework yields state-of-the-art results.
* Multi-task training is also investigated in the paper. As highlighted in , on certain reasoning tasks, a generalist network trained on all datasets in CLRS outperforms the networks trained in a single-task manner. We provide an interpretation of this observation using the proposed open-book framework. Specifically, when training a neural network to solve a task, we input information from other task datasets into the framework for its use. The results show that our open-book framework can nearly replicate the effects of multi-task training for each algorithmic task, while in some tasks, it even achieves higher accuracies. Additionally, our attention-based implementation enables us to analyze the attention weights of various tasks, facilitating a deeper understanding of the intrinsic relationships amongtasks. A "paired training" experiment is further conducted to verify the effectiveness of the learned attention weights.

### Other Related Work

Our work is closely aligned with the exploration of non-parametric models [23; 25; 13], where models abstain from training specific parameters and, instead, utilize dependencies among training data points for predictions. Our framework can be viewed as a fusion of deep neural networks and non-parametric models. We have noted analogous efforts in recent work within the field of image processing . This work focuses on the CIFAR-10 dataset, employing self-attention mechanisms among different points in the dataset to finish image classification tasks.

## 2 Preliminaries

This section introduces the setting of an NAR dataset formally and outlines the standard paradigm employed in NAR.

**NAR Dataset.** The objective of an NAR task is to train a neural network such that it can imitate each execution step of a classical algorithm on given problem instances. Hence, a NAR dataset is labeled by a specific problem and the algorithm employed to solve it. Each data point includes a problem instance, represented by a graph structure, and the corresponding algorithm execution on that instance, conveyed through a sequence of graph-structured states. Denote by \(\) the problem instance and by \(=\{^{(1)},...,^{(t)},...\}\) the algorithm execution, where \(^{(t)}\) signifies the graph-structured states (e.g., the current nodes in the queue of breadth-first search) at the \(t\)-th step of the algorithm.

**Training Objective.** The training objective of the neural network is to perform sequential reasoning tasks over a given problem instance. At each step \(t\), the network takes as input the pair \((,^{(t-1)})\) and produces the output \(^{(t)}\). This process enables the neural network to learn and predict the evolution of the algorithmic execution on the problem instance in a step-wise fashion.

**Encode-Processor-Decode Paradigm.** To achieve the aforementioned step-wise objective, the literature follows the standard _encode-process-decode_ paradigm , which consists of three modules: Encoder, Processor, and Decoder. At each step \(t\), the input \((,^{(t-1)})\) traverses through these modules sequentially2:

* The encoder module encompasses multiple neural networks that operate on \((,^{(t-1)})\), thereby transforming it into a collection of graph-structured hidden states. Use \(G=(V,E)\) to denote the graph structure. Following this module, we obtain \(h_{v}\) corresponding to each node \(v V\), \(h_{vu}\) associated with each edge \((v,u) E\), and \(h_{g}\) representing the hidden state of the entire graph \(G\).
* The processor module usually consists of a graph neural network. This module maintains the historical hidden states of nodes, edges, and the graph: \(\{h_{v}^{(t-1)}\}_{v V}\), \(\{h_{vu}^{(t-1)}\}_{(v,u) E}\), \(h_{g}^{(t-1)}\), and integrate them with the newly generated states \(\{h_{v}\}\), \(\{h_{v,u}\}\), \(h_{g}\) to yield updated states. We borrow the language of the message-passing architecture  to formalize this process. For brevity, the following focuses only on updating the state of each node \(v\). At each step \(t\), the node computes and aggregates messages \(m_{uv}\) from its incoming edges, updating its own hidden state: \[z_{v}^{(t)}  f_{1}(h_{v},h_{v}^{(t-1)})\;; m_{uv}  f_{2}(z_{v}^{(t)},z_{u}^{(t)},h_{uv},h_{g})( u,v) E\;;\] \[M_{v} _{w:(u,v) E}m_{uv}\;; h_{v}^{(t)}  f_{3}(z_{v}^{(t)},M_{v}).\] Different processors employ different layers \(f_{1}\), \(f_{2}\), \(f_{3}\), and aggregation function \(\).
* The decoder module utilizes the states \(^{(t)}\) as input to forecast the algorithmic execution \(^{(t)}\) at step \(t\). It is noteworthy that recent literature  also incorporates \(\) and \(^{(t-1)}\) within this module.

Open-Book Reasoning

The paradigm above can be denoted by a function \(\) mapping \(\) to \(\) for each data point. Given a NAR dataset, this function implies a standard supervised learning mode: during a training step, a (or a mini-batch of) random datapoint \((,)\) is selected. The loss between \(()\) and \(\) is then computed, and the parameters in \(\) are updated accordingly. In this section, we go beyond the individual \(\) mode in conventional supervised learning, exploring a more general and practical learning paradigm.

### Framework

We introduce an open-book reasoning framework. Within the framework, when the network is tasked with solving problem instance \(\) and deducing \(\), it not only utilizes \(\) as input but is also allowed to leverage information from other data points within the training set during the reasoning process.

The intuition behind the open-book framework is analogous to our real-world scenario of solving algorithmic problems or engaging in other reasoning tasks. In practical situations, we often consult textbooks and refer to example problems to aid in completing tasks. Typically, the structure and solutions of these examples provide substantial assistance in our reasoning process. Denoting the training set as \(\), the framework essentially aims to learn a comprehensive function \(:\).

An illustration of the framework is present in Figure 1. In addition to the original three modules, we introduce two new modules: _Dataset Encoder_ and _Open-Book Processor_:

* The dataset encoder module employs an encoding function \(f_{E}\) to compute the latent feature of each data point \(_{i}=(_{i},_{i})\) in the training set: \(_{i} f_{E}(_{i},_{i}).\) It is worth noting that this encoder module is essentially different from the original one. It maps an entire data point \(_{i}=(_{i},_{i})\), encompassing the ground truth of each node (and edge) at each step, into a single representation \(_{i}\).
* The open-book processor module is incorporated between the original processor and decoder modules. The output \(^{(t)}\) from the processor no longer directly feeds into the decoder; instead, it passes through the open-book processor, where it undergoes information aggregation with the training data representation \(=\{_{1},...,_{i},...\}\) (generated by the dataset encoder). Subsequently, the open-book processor produces the latent features \(}^{(t)}\) required by the decoder. Formally, for each node \(v V\), \(_{v}^{(t)} f_{P}(h_{v}^{(t)},).\)

The central component of the framework is the open-book processor module. Within this module, the current problem instance to be solved is integrated with examples from the training set. It is crucial to acknowledge that this integration has both advantages and disadvantages. While it often enhances the architecture's reasoning capabilities, there are instances when it may lead to counterproductive effects, particularly during multi-task training. We will elaborate on this in the experiments.

Figure 1: An illustration of the open-book framework. At each reasoning step \(t\), we simultaneously input \((,^{(t-1)})\) and instances from the training set \(\), yielding \(^{(t)}\).

### Attention-Based Implementation

Diverse implementations within the framework can be achieved by employing different functions for \(f_{E}\) and \(f_{P}\). For the ease of investigating multitask training, we adopt an attention mechanism-based implementation. A description of the network implementation and training is given in Algorithm 1.

```
0: Training set \(\).
1:while current epochs \(\) maximum training epochs do
2: Initialize the hidden state \(^{(0)}\) and \(}^{(0)}\).
3: Randomly pick a _target_ data point \(=(,)\) and several _auxiliary_ data points \(_{1},_{2},..._{}\). \(\)Dataset Encoder
4:for each auxiliary data point \(_{i}=(_{i},_{i})\)do
5: Let \(G=(V,E)\) represent the underlying graph of \(_{i}\). This data point encompasses a state sequence \(<_{i}=_{i}^{(0)},_{i}^{(1)},...,_{i}^ {(t)},...>\) on the graph.
6: Randomly select two adjacent states \(_{i}^{(p)},_{i}^{(p+1)}\) from the sequence.
7:for each node \(v V\)do
8: Let \(y_{v}\) and \(y_{v}^{}\) be node \(v\)'s states in \(_{i}^{(p)}\) and \(_{i}^{(p+1)}\) respectively.
9: Employ a linear layer \(z_{v}((y_{v}+y_{v}^{}))\).
10:endfor
11:\(_{i}_{v V}z_{v}\).
12:endfor
13: Define \(:=[_{1},...,_{}]\).
14:for each algorithm execution step \(t\) of the target data point do
15: Feed \(\) and the outcome from the previous step into the encoder and processor sequentially to obtain the hidden states \(^{(t)}\).
16: Employ a linear layer: \(^{(t)}(^{(t)})\). \(\)Open-Book Processor
17: Set a QKV attention function with linear layers \((),(),()\).
18: Compute a cross-attention between \(^{(t)}\) and \(^{(t)}\): \[}^{(t)}( (^{(t)})(^{(t)})} {}})\ \ (^{(t)}),\] where \(d_{k}\) is the dimension of the key vectors.
19: Add a gate function: \(}^{(t)}(^{(t)}, }^{(t)}).\)
20: Feed \(}^{(t)}\) into the decoder module, yielding the predictions of this step.
21:endfor
22: Compute the prediction loss and update the network parameters.
23:endwhile ```

**Algorithm 1** Attention-Based Implementation of Open-Book Reasoning

From the description, a _target_ data point and several _auxiliary_ data points are randomly selected in each training iteration. The target data point serves as the focal point for neural optimization in this iteration: the network predicts its ground truths, computes the loss, and consequently updates the network parameters. The auxiliary data points assist the network in reasoning for the target data point. Their latent features are obtained through the dataset encoder, and they subsequently influence predictions through the open-book processor.

At each algorithmic step \(t\), the hidden states \(^{(t)}\) are computed conventionally. However, these states are not directly input into the decoder. Instead, they need to undergo cross-attention with the representations of auxiliary data points within the open-book processor module. This design allows the network to incorporate hints provided by the auxiliary data points during the reasoning process.

The construction of the dataset encoder is a bit subtle. We observe a crucial aspect that all these pieces of information ultimately serve the decoder module. In a single algorithmic step, the decoder's role is to facilitate the transition between two adjacent states throughout the entire reasoning process. Therefore, to better provide effective hints to the final decoder, for each auxiliary data point, werandomly sample a pair of adjacent states from its corresponding state sequence. Subsequently, we employ a linear layer to yield the latent representations of these data points.

**Remark.** The testing process is essentially similar to the training. It is worth noting that during the testing phase, the target data points are sourced from the testing set, while the auxiliary data points must still originate from the training set.

## 4 Experiments

This section evaluates the open-book implementation empirically on the CLRS benchmark. We aim to investigate the following three questions during the experiments:

* For various processor architectures present in the literature, can the open-book framework consistently enhance their empirical performances across the majority of the algorithmic tasks within the CLRS benchmark?
* There is a recent literature  proposing a multi-task training approach for CLRS. They train a common network for various tasks in the benchmark and find that some tasks benefit from the multi-task approach, achieving higher accuracy than when trained individually. In the context of the open-book setting, does this phenomenon imply that incorporating training sets from various tasks into the open-book framework may enhance the network's performance on certain tasks?
* Can the attention-based implementation serve as a robust tool for interpretable multi-task training? When integrating training sets from various tasks into the open-book framework for a specific task, the network eventually learns attention weights in the open-book processor, signifying the task's relevance to other tasks. Does this imply that if a task performs better in multi-task training than in single-task training, retaining only those tasks with prominent attention for multi-task training can still outperform single-task training?

To tackle these questions, we conduct three types of experiments3: single-task augmenting, multi-task augmenting, and multi-task interpretation. Note that our "multi-task augmenting" experiment differs essentially from traditional multi-task training; here, we still train the network for a specific task, but with the inclusion of datasets from other tasks in the dataset encoder. Additional ablation experiments are also conducted. Due to space limitations, we defer them to the full version of this paper. We initially outline the experimental setup and subsequently delve into each experiment.

### Setup

**Baselines.** We incorporate the open-book framework into three existing processor architectures: PGN , MPNN  and Triplet-GMPNN . Given that the feature dimension of hidden states is set to 128 in the literature, we adjust the parameters of the dataset encoder and open-book processor to ensure seamless integration. The results (F1 scores) achieved by open-book reasoning are compared with them. Moreover, we also compare the performance with other recent architectures like Memnet  and NPQ .

**Computational Details.** The experiments are conducted on a machine equipped with an i7-13700K CPU, an RTX 4090 GPU, and an RTX A6000 GPU. The results are averaged over 4 runs. To ensure fair comparisons, we follow the widely-used experimental hyperparameter settings in , where the batch size is 32 and the network is trained for 10,000 steps by Adam optimizer with a learning rate of 0.001. During each training and testing iteration, we allow Algorithm1 to sample \(240\) auxiliary data points and use only one attention head. The average training time for each reasoning task is approximately 0.5 GPU hours.

### Single-Task Augmenting

This subsection considers a single-task environment: for each reasoning task in CLRS, both target and auxiliary data points in Algorithm1 are sourced from its own dataset. We create comparison charts for results on three existing architectures. Due to space, only one chart Figure2 is presentedin the main body, while the other two are deferred to the full version of this paper.The figure uses bar charts to illustrate average scores for each task, with standard deviations denoted by black lines. Additionally, we arrange the tasks in descending order of improvement magnitude to better illustrate trends.

We also provide tables to comprehensively compare the accuracies that the open-book framework yields with existing results. In CLRS, the 30 tasks are partitioned into 8 categories: Divide and Conquer, Dynamic Programming, Geometry, Graphs, Greedy, Search, Sorting, and Strings. So we present two tables: one showcasing the performance on the 30 individual tasks and another displaying the average performance for each of the 8 task categories. Due to space constraints, the latter is included in the main body (Table 1), while the former is deferred to the full version of this paper.

From the figures and tables, we observe that our approach outperforms the original architectures in the majority of tasks. The improvements provided by the open-book framework are particularly significant for certain tasks, such as the Naive String Matcher task (see Figure 2). However, we also notice a relatively large standard deviation in performance for some tasks. We attribute this variability to the fact that during testing, we sample data from the training set and input it into the dataset encoder each time. The quality of the sampled data influences the final inference results, leading to performance fluctuations.

  Task Category & Prior Best & Triplet-GMPNN & Ours \\  Graphs & 64.98\%\(\)2.59 & 81.41\%\(\)1.53 & **85.37\%\(\)1.73** \\ Geometry & 92.48\%\(\)1.35 & 94.09\%\(\)0.77 & **96.55\%\(\)0.50** \\ Strings & 4.08\%\(\)0.57 & 49.09\%\(\)4.78 & **72.41\%\(\)2.66** \\ Dynamic Programming & 76\%\(\)2.47 & 81.99\%\(\)1.30 & **82.14\%\(\)1.45** \\ Divide and Conquer & 65.23\%\(\)2.56 & **76.36\%\(\)0.43** & 74.52\%\(\)1.88 \\ Greedy & 84.13\%\(\)2.59 & 91.22\%\(\)0.40 & **93.40\%\(\)2.12** \\ Search & 56.11\%\(\)0.36 & 58.61\%\(\)1.05 & **63.15\%\(\)0.90** \\ Sorting & 71.53\%\(\)0.97 & 60.38\%\(\)5.25 & **83.65\%\(\)3.06** \\  

Table 1: The summary of our results on each task category in CLRS. The best-performing results in each row are highlighted in bold. To save space, we use the column “Prior Best” to denote the best results among four existing approaches: Memnet , PGN , MPNN , and NPQ , and the column “Ours” to denote the best results achieved by applying the open-book framework to the three existing architectures.

Figure 2: Comparison of the MPNN architecture’s performance before and after augmentation with the open-book framework. The 30 tasks are arranged in descending order of improvement magnitude.

### Multi-Task Augmenting

This subsection considers a "multi-task" environment: for each task in CLRS, Algorithm 1 selects target points from its own dataset, while the sampled auxiliary points are drawn from all datasets in CLRS. Since CLRS comprises 30 datasets, in each iteration, we randomly sample 8 instances from each dataset, ensuring that the total number of auxiliary points remains the same as in the single-task experiment, i.e., 240. Given that Triplet-GMPNN is the only architecture used for multi-task training in the literature, both this subsection and the following one "multi-task interpretation" focus exclusively on the results obtained by integrating the open-book framework with Triplet-GMPNN.

The results are present in Figure 3. We find that incorporating data from different tasks into the open-book processor indeed replicates multi-task training. Our multi-task augmented method closely matches the previous multi-task training results, and even outperforms them on the vast majority of tasks. It is worth noting that multi-task training requires simultaneous training on all 30 algorithmic tasks, which is extremely time-consuming. If the goal is simply to enhance performance on a specific task using multi-task training, the cost is substantial. However, with the open-book framework, we can nearly achieve the effects of multi-task training on a target task in approximately the same amount of time it takes to train a single algorithm.

### Multi-Task Interpretation

This subsection delves into interpreting multi-task training. In our multi-task augmenting experiments, the acquired attention weights in the open-book processor reveal the significance of each task in relation to others. Specifically, for each task, we aggregate the attention weights of each node at every algorithmic step on each test instance. The resulting 30-dimensional vector is then normalized, serving as the total attention vector for that task relative to other tasks in the benchmark. Table 2 shows the task with the highest attention weight for each task. Moreover, we present a heatmap regarding the attention weights among CLRS tasks in the full version of this paper.

Surprisingly, the table indicates that the majority of tasks exhibit a preference for attention toward tasks outside their own categories, contrary to our initial expectations. Only four bolded pairs show high attention to tasks within the same category, with most of these being graph algorithms. An intuitive explanation for this phenomenon is that tasks within the same category might not contribute additional information compared to the dataset used for training the task itself. Instead, tasks from other categories seem to play a crucial role in improving training accuracy.

Figure 3: Comparisons between our multi-task augmented approach and Triplet-GMPNN. The 30 tasks are arranged in descending order of improvement magnitude.

We proceed to a more in-depth examination of the relationships among tasks learned by the framework. We select a partner for each task according to Table 2 - namely, the task it pays the most attention to. We conduct training and testing in a multi-task manner for each task paired with its chosen partner, and refer to this type of training as paired-task training. In this experiment, we only focus on tasks that either demonstrate accuracy improvements or slight declines in multi-task training compared to single-task training, and train them in the paired manner. The results are given in Table 3. The table validates our hypothesis. On these tasks, paired-task training achieves improvements compared to single-task training, with most tasks even surpassing the performance of multi-task training.

### Experimental Summary

The experiments address the three questions posed at the beginning of the section.

* The open-book framework can significantly enhance the reasoning capabilities of various existing architectures, yielding state-of-the-art results across the majority of tasks in CLRS.
* By feeding data from various tasks into the dataset encoder, the framework can successfully replicate the effects of multi-task training, and in most datasets, even outperform it.

  Target & Auxiliary & Target & Auxiliary \\  Activity Selector & Topological Sort & Jarvis’ March & MST-Kruskal \\ Articulation Points & Knuth-Morris-Pratt & Knuth-Morris-Pratt & Quicksort \\ Bellman-Ford & Bridges & LCS Length & Dijkstra \\ BFS & Task Scheduling & Matrix Chain Order & Jarvis’ March \\
**Binary Search** & **Quicksselect** & Minimum & Quicksort \\ Bridges & Optimal BST & MST-Kruskal & Heaepsort \\ Bubble Sort & Task Scheduling & **MST-Prim** & **Bridges** \\ DAG Shortest Paths & Naive String Matcher & LCS Length \\ DFS & Binary Search & Optimal BST & Find Max. Subarray \\
**Dijkstra** & **Bellman-Ford** & Quickselect & Dijkstra \\ Find Max. Subarray & Jarvis’ March & Quicksort & BFS \\ Floyd-Warshall & Heapsort & Segments Intersect & Topological Sort \\ Graham Scan & Quicksort & SCC & Task Scheduling \\ Heapsort & Activity Selector & Task Scheduling & Heapsort \\ Insertion Sort & Minimum & **Topological Sort** & **DAG Shortest Paths** \\  

Table 2: For each target (task), we show the task with the highest attention weight among other tasks in column “Auxiliary”. We use bold text to indicate when the paired tasks belong to the same algorithmic category.

  Task & Single-Task & Multi-Task & Paired-Task \\  Heapsort & 31.04\%\(\)5.82 & **55.62\%\(\)**15.91 & 46.63\%\(\)10.43 \\ Knuth-Morris-Pratt & 19.51\%\(\)4.57 & 51.61\%\(\)8.63 & **65.67\%\(\)12.36** \\ Insertion Sort & 78.14\%\(\)4.64 & 87.00\%\(\)4.16 & **95.78\%\(\)0.80** \\ LCS Length & 80.51\%\(\)1.84 & 83.43\%\(\)1.19 & **85.86\%\(\)1.47** \\ Quicksort & 64.64\%\(\)5.12 & 75.10\%\(\)9.52 & **88.43\%\(\)6.25** \\ SCC & 43.43\%\(\)3.15 & 48.48\%\(\)9.96 & **73.39\%\(\)3.00** \\ Jarvis’March & 91.01\%\(\)1.30 & 74.51\%\(\)10.71 & **94.44\%\(\)0.63** \\ MST-Kruskal & 89.80\%\(\)0.77 & 89.08\%\(\)1.64 & **90.55\%\(\)1.12** \\ MST-Prim & 86.39\%\(\)1.33 & 86.26\%\(\)2.08 & **92.56\%\(\)0.99** \\ Topological Sort & 87.27\%\(\)2.67 & 81.65\%\(\)2.53 & **87.30\%\(\)4.62** \\ Dijkstra & 96.05\%\(\)0.60 & 94.29\%\(\)1.04 & **97.44\%\(\)0.50** \\ Binary Search & 77.58\%\(\)2.35 & 69.30\%\(\)5.65 & **79.17\%\(\)2.79** \\ Bubble Sort & 67.68\%\(\)5.50 & 52.94\%\(\)9.96 & **70.30\%\(\)6.77** \\ Graham Scan & 93.62\%\(\)0.91 & 87.74\%\(\)3.87 & **94.58\%\(\)0.87** \\ Minimum & 97.78\%\(\)0.55 & 92.50\%\(\)2.53 & **98.32\%\(\)0.14** \\  

Table 3: Comparisons among three training manners under Triplet-GMPNN.

* The attention-based implementation provides a valuable tool for the interpretability of multi-task training. By examining the learned attention weights, we can gain insights into the influences and intrinsic relationships among tasks during multi-task training.

## 5 Conclusion

This paper considers open-book neural algorithmic reasoning, introducing a novel open-book framework accompanied by an attention-based implementation. Through empirical evaluations, we demonstrate that this implementation not only enhances the reasoning capabilities of the existing architecture but also functions as an effective tool for interpretable learning.

Several interesting direction for future research exist, such as exploring more effective implementations within the open-book framework. Note that although our current implementation demonstrates performance improvements for the majority of tasks in CLRS, there are instances where the open-book approach may yield counterproductive results. Refining the current architecture to ensure performance enhancements across all tasks remains a significant challenge.