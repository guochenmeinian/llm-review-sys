# We summarize the contributions of this paper in the following:

Unveiling The Matthew Effect Across Channels: Assessing Layer Width Sufficiency via Weight Norm Variance

 Yiting Chen, Jiazi Bu, Junchi Yan

Dept. of CSE & School of AI & MoE Key Lab of AI, Shanghai Jiao Tong University

{sjtucyt, bujiazi001, yanjunchi}@sjtu.edu.cn

https://github.com/Ytchen981/Channel_Matthew_Effect

Corresponding author. This work was in part supported by NSFC (92370201, 62222607) and Shanghai Municipal Science and Technology Major Project under Grant 2021SHZDZX0102.

###### Abstract

The trade-off between cost and performance has been a longstanding and critical issue for deep neural networks. One key factor affecting the computational cost is the width of each layer. However, in practice, the width of layers in a neural network is mostly empirically determined. In this paper, we show that a pattern regarding the variance of weight norm corresponding to different channels can indicate whether the layer is sufficiently wide and may help us better allocate computational resources across the layers. Starting from a simple intuition that channels with larger weights would have larger gradients and the difference in weight norm enlarges between channels with similar weight, we empirically validate that wide and narrow layers show two different patterns with experiments across different data modalities and network architectures. Based on the two different patterns, we identify three stages during training and explain each stage with corresponding evidence. We further propose to adjust the width based on the identified pattern and show that conventional layer width settings for CNNs could be adjusted to reduce the number of parameters while boosting the performance.

## 1 Introduction

The cost-accuracy trade-off has been a longstanding and critical issue for deep neural networks. As one key factor that affects the computational cost, the width of each layer is mostly empirically determined or extensively searched over an architecture space in neural network architecture search literature . Despite the research on over-parameterization  and empirical evidence  showing that the wider network leads to better performance, there are few works about how we allocate the computational resources across the layers. On understanding the layer width requirement in a deep neural network,  theoretically proves the lower bound of layer width for the model to approximate any Lebesgue-integrable function while  improves the width lower bound with dynamic systems. However, a practical indicator of sufficient layer width is still lacking.

In this paper, we identify a simple pattern difference between wide and narrow layers regarding the weight norm variance between different channels during training. A simple observation is that larger-weight channels would have a larger gradient. As the weight norm increases during training , we show the disparity of weight norm between similar channels also increases during training, which we call the Matthew effect between channels. It motivates us to investigate the weight norm variance across the channels in each layer during training. With experiments on image, graph, and text datasets (including CIFAR-10 , cora , and enwiki8 ) with various model structures including(GCN , GRU , ViT ), we show that wide and narrow layers show two different patterns during training. As we change the layer width, we show that narrow layers show a decrease until the saturate (DS) pattern, such that the weight norm variance first increases and then decreases. For wider layers, the weight norm variance keeps increasing until saturate (IS). We conjecture that, for sufficiently wide (or over-parameterized) layers, channels with a small weight norm would always be surpassed by similar channels with larger weights, and therefore, the variance keeps increasing until convergence. On the other hand, for narrow layers with few channels, the channels with small weights could be orthogonal to channels with large weights, and the variance decreases.

We further show that the training from random initialization to convergence can be divided into three stages for neural networks showing the identified IS or DS pattern. In the first stage, typically a few epochs after random initialization, since the gradient is almost orthogonal to the weight vector, the variance between weight norms corresponding to different neurons does not increase or even decrease. In the second stage, both the performance of the network and the variance between neurons drastically increase for a dozen epochs. It indicates a fast learning process with the weight norm of a few neurons increasing drastically. In the third stage, the wide layers show the IS pattern, such that the weight norm variance keeps increasing and staying high, while the narrow layers show the DS pattern, such that the weight norm variance starts to decrease. We provide our explanations for each stage of training with corresponding empirical evidence on VGGs  trained on CIFAR10.

Furthermore, we show that the conventional layer width setting for CNNs such as VGG or ResNet might not be optimal. Generally, with the conventional layer width setting, the layers in the middle show the DS pattern while the former and latter layers show the IS pattern. It indicates that the layers in the middle could use more width, which also cohere with the intrinsic dimension firstly increasing then decreasing and reaching the high point at the middle . We propose adjusting the width of each layer of widely used networks to boost the performance and reduce the number of parameters.* To our best knowledge, we are the first to identify patterns regarding the variance of weight norm between different channels during training that can indicate whether a layer is wide enough. We have observed the pattern with different model structures on different data modalities (including CNNs and ViTs for image classification [29; 14; 9], GCN for graph node classification , and GRU for language modeling ).
* Based on the identified pattern, we further show that there are, in general, three training stages from random initialization to convergence. We provide our explanations with empirical evidence on VGG models trained for image classification that supports our explanation.
* We verify that with the identified pattern, the conventional width setting for CNNs could be adjusted for fewer parameters and better performance on CIFAR10 and TinyImageNet.

## 2 Related Works

**Researches on the Width of Neural Networks.** By simply widening the network, early works [37; 15] empirically show that the performance can be improved. On the other hand, structured pruning methods [33; 36; 16] propose to reduce the width for efficiency. Compared to the static pruning methods, another branch of research focusing on dynamically changing the width of the network conditioned on the input by skipping neurons in linear layer , branches in Mixture of Experts (MoE)  and channels in CNN [12; 19] at inference time. Readers interested in dynamic networks could refer to  for more details. Notably, a recent work  proposes to merge neurons to reduce the computational cost where layers at the middle have fewer neurons that could be merged. Similar results are also reported regarding investigating the intrinsic dimension  across the layers. Unlike these previous works focusing on the trained model, we show that one could identify the

   Phenomenon & Description & Measured by \\  The critical period in training  & Providing converged data in the early training phase & Testing accreacy after convergence. \\  Frequency- & Neural networks first learn principle , low-frequency information & Gradient norm on different frequency \\
34] & and the high-frequency information. & Training and testing accreacy. \\  Griding  & Neural networks show a repeated of near-perfect training performance and nearly & Testing accreacy. \\  Double- & The testing accuracy firstly improves, then networks, and they are the model capacity increases or the training goes on. & Testing accreacy. \\  Ours & See details in Fig. 1(b), variance in a layer & Weight norm variance in a layer \\   

Table 1: We list some different training dynamics phenomena.

layers with insufficient width during training. We further propose a simple width-wise streamlining technique to boost the network performance and reduce the number of parameters. Note that the objective of our width-wise streamlining technique differs from the pruning method and we make the FLOPs of the adjusted model close to the original network.

**Dynamics of Network Training** Investigating the training process of neural networks has always been an active research topic. The early phase of neural network training is investigated in  and a timeline for the early phase of training (the first several epochs) is provided while other works [24; 8; 25] have focused on the later training phase investigating the effect of over-parameterization on the generalization and the training heuristics that lead to simpler solutions. From the frequency spectrum perspective, F-principle [27; 34; 32] shows that low-frequency components play a more important role in the generalization and that the training of CNN could be divided into two phases where the CNN firstly learn low-frequency components then learn high-frequency components. The frequency principle theoretically proves that the gradient on low-frequency components will be larger . In this paper, we analyze the training dynamics from the weight norm variance perspective and show that the training of neural networks could be summarized into three stages in general. To our knowledge, this is the first work to investigate the variance between weight norms corresponding to different neurons. We list some well-known phenomena in Table 1. While a recent work  correlates grokking  with double-descent , we hope the relationship between the three stages of training and the other phenomena could be further explored in future works. For example, the difference between the second stage and the third stage could be the result of network learning different frequency components, easy or hard data samples _etc._

## 3 Different Patterns between Wide and Narrow Layers

In this section, we first provide a simple analysis showing the motivation for us to investigate the weight norm variance across the channels of a layer. We then provide empirical results from experiments across different data modalities and model structures to show that there are two different patterns between wide and narrow layers.

### Simple Analysis Regarding Channels in a Layer During Training

Let us consider an MLP with ReLU activation for simplicity. Suppose we have \(d\) channels in the \(l\)-th layer. Let \(^{l-1}^{d_{in}}\) and \(^{l+1}^{d_{out}}\) denote the features at the \(l-1\)-th layer and the \(l+1\)-th layer. The weight for \(l\)-th layer and \(l+1\)-th layer is \(W^{l}^{d_{in} d}\) and \(W^{l+1}^{d d_{out}}\). Then we have:

\[^{l+1}=W^{l+1}(W^{l}^{l-1})\] (1)

Figure 1: Illustration of our motivation and the identified weight norm variance pattern. **Fig. (a)**: For two channels with weight vectors of similar direction, the gradient of the former layer is proportional to the weight norm of the latter layer and vice versa. As empirical evidence indicated, the weight norm increases during training, and the weight norm disparity between channels increases. **Fig. (b):** The two different patterns we identified of how weight norm variance would change during training. For wide layers, the weight norm variance keeps increasing, which we call the increase to saturate pattern (IS). For narrow layers, the weight norm starts to decrease after reaching a high point, which we call the decrease to saturate pattern (DS).

where \(()\) is the ReLU activation function. Let \(_{i}^{l}^{d_{in}}\) denotes the \(i\)-th row vector of \(W^{l}\) and \(_{i}^{l+1}^{d_{out}}\) denotes the \(i\)-th column vector of \(W^{l+1}\), the output corresponding to the \(i\)-th channel is \((_{i}^{l}^{l-1})_{i}^{l+1}\) and the output \(z^{l+1}\) is the combination of the outputs of all the \(d\) channels as

\[^{l+1}=_{i=1}^{d}(_{i}^{l}^{l-1}) _{i}^{l+1}\] (2)

Suppose the loss function is \(\), and the gradient for \(^{l+1}\) is \(_{^{l+1}}\), then we have

\[_{_{i}^{l+1}}=\{_ {i}^{l}^{l-1}_{^{l+1}},&_{i}^{l}^{l-1} 0\\ ,&_{i}^{l}^{l-1}<0.\] (3)

\[_{_{i}^{l}}=\{(_ {i}^{l+1}_{^{l+1}})^{l-1},& _{i}^{l}^{l-1} 0\\ ,&_{i}^{l}^{l-1}<0.\] (4)

For two channels \(m,n[1,d]\), suppose \(_{m+1}^{l+1}}{\|_{m}^{l+1}\|}_{m}^{l+1}}{\|_{n}^{l+1}\|}\) and \(_{m}^{l}}{\|_{m}^{l}\|}_{m}^ {l}}{\|_{n}^{l}\|}\) then \(_{m}^{l}^{l-1}\) and \(_{n}^{l}^{l-1}\) would have the same sign and we have

\[_{_{m}^{l+1}}_{m}^{l}}{\| _{n}^{l}\|}_{_{n}^{l+1}}.\] (5)

\[_{_{m}^{l}}_{m}^{l+1}\|}{ \|_{n}^{l+1}\|}_{_{n}^{l}}.\] (6)

It means that for channels with weights of similar direction, the larger \(\|^{l}\|\) leads to larger \(_{^{l+1}}\) and the larger \(\|^{l+1}\|\) leads to larger \(_{^{l}}\). While empirical evidence in previous works [30; 11] has shown that the weight norm increases during training. It means that, for two channels with similar weight direction, the weight norm disparity between them would generally keep increasing during training, which we call **the Matthew effect between similar channels**. For the channel with a larger weight, the larger weight norm at the \(l+1\) layer leads to a larger gradient of the corresponding weight at the \(l\) layer, which increases the weight norm at the \(l\) layer. In turn, a larger weight norm at the \(l\) layer further speeds up the increase of the weight norm at the \(l+1\) layer. According to , sufficiently wide layers may learn many similar channels after training, therefore, we expect the variance of weight norm across different channels would be higher in the wide layer than in the narrow layer. It motivates us to investigate further the weight norm variance between different channels in a layer.

### Empirical Evidence of Two Different Patterns for Wide and Narrow Layers.

In this subsection, we investigate the variance of weight norms corresponding to different channels. For the \(i\)-th channel in Eq. 2, with ReLU as activation functions we have

\[(_{i}^{l}^{l-1})_{i}^{l+1}= \{_{i}^{l+1}(_{i}^{l}^{l-1}),&_{i}^{l}^{l-1} 0\\ ,&_{i}^{l}^{l-1}<0.\] (7)

Therefore, we define the weight norm corresponding to the \(i\)-th channel as \(\|_{i}^{l}\|\|_{i}^{l+1}\|\)1. We conduct our experiments on several datasets across different data modalities with different model architectures and record the variance of corresponding weight norms during training.

We report the weight norm variance results for Graph Convolutional Neural Network (GCN)  trained on graph dataset cora , Gated Recurrent Units (GRU)  trained on text dataset enwiki  and Vision Transformers (ViT)  trained on image dataset CIFAR10 . As we change the width of a certain layer (_e.g._ the MLP module in the transformer), we show that the wide layer and the narrow layer show two different patterns. For the wide layer, the weight norm variance firstly increases and stays at a high level until convergence, which we call the increase to saturate (IS) pattern. For the narrow layer, the weight norm variance decreases after the initial increase, which we call the decrease to saturate (DS) pattern.

As shown in Fig. 2, we present the results of GCN  on cora  and GRU  on enwiki8 . In Fig. 2(a), the results show the weight norm variance change of the second layer with different widths. As the width increases from \(16\) to \(64\), the weight norm variance changes from DS pattern to IS pattern. Notably, increasing the width when the DS pattern is identified (from \(16\) to \(64\)) leads to an accuracy boost, while increasing the width when the IS pattern is identified (from \(64\) to \(128\)) does not. In Fig. 2(b), we report the results of the weight norm variance change of the input-hidden gate of the second layer of the GRU. Though the recurrent neural networks (RNN) have different architectures compared to the multi-layer ReLU network we discussed before, the weight norm variance of the narrow and wide layers also shows DS and IS patterns, respectively.

Fig. 3 shows the results of two ViTs trained on CIFAR10. We use a ViT with \(6\) layers where each layer has \(8\) heads and the hidden size for each attention layer is \(512\). The width of the MLP at the 5th layer is set at \(32\) and \(512\) respectively. For the layer of width \(512\), the weight norm variance always increases and stays at a high level following the IS pattern. As we reduce the width of the MLP to \(32\), the weight norm variance starts to decrease after reaching a high point following the DS pattern. For more details on the GCN, GRU, and ViT training, please refer to Appendix B.

**Our Conjecture About the Different Patterns Regarding Layer Width.** As shown in Eq. 2, each layer is like a "mixture of experts" where each channel has the same weight. The only difference between channels is the random initialization. For wide layers with a large number of channels, the chance for two channels learning weight of similar direction is much higher than that for narrow layers with limited channels. Therefore, the weight norm variance among channels of a wide layer does not decrease while the weight norm variance among channels of a narrow layer decreases after reaching a high point. In the next section, we provide a more detailed analysis with empirical evidence about the three training stages we identified.

Figure 3: Weight norm variance between different channels of the MLP at the 5th layer of a tiny ViT trained on CIFAR-10. The left figure corresponds to the result where the MLP size is set as \(512\), and the right figure corresponds to the result where the MLP size is set as \(32\).

Figure 2: Weight norm variance change during training of GCN  on cora  and GRU  on enwiki8 . We change the width of the second layer in GCN and the input-hidden weight of the second layer in GRU. As the width increases, the weight norm variance during training shows two different patterns. For the wide layer, the weight norm variance increases and stays at a high level until convergence, which we call an increase to saturate (IS) pattern. For the narrow layer, the weight norm variance decreases after the initial increase, which we call the decrease to saturate (DS) pattern.

## 4 Three training Stages from Random Initialization to Convergence

Based on the two patterns we identified regarding the weight norm variance, we further provide our explanation for this phenomenon. We show that, generally, we can divide the training of the neural networks showing the identified pattern into three stages. We provide our explanation for each stage with corresponding empirical evidence on VGG-16  and ResNet-18  trained on CIFAR10 . As shown in Fig. 4, we report the weight norm variance of some of the layers in VGG-16 and ResNet-18. In Fig. 5, we also present the density plots of the weight norm for different layers of a ResNet-18 trained on CIFAR10 at different epochs. Generally, different layers show different patterns. The layers in the middle typically follow the DS pattern and the former and latter layers follow the IS pattern. We will discuss this phenomenon in the next section Sec. 5.

### Stage 1: Weight Variance Stay Low Shortly after Initialization

Starting from a random initialization, the first stage of neural network training lasts for a few epochs. In this stage, the variance of the weight norm between neurons does not increase. At certain layers, the variance even decreases between neurons. Starting from random initialization, due to the high-dimensional weight vectors of each neuron, the gradient is more likely to be orthogonal to the weight vectors corresponding to different neurons. When measuring the cosine similarity between the gradient and the weight vector across all the channels across all the layers in a VGG16_bn trained on CIFAR-10, the largest absolute value is \(0.0003\) at initialization, which provides an explanation for the weight variance not increasing.

In some scenarios, we also find weight norm variance decreases at the first stage. We conjectured that the weight variance is \(0.0003\) at initialization, which provides an explanation for the weight variance not increasing.

Figure 4: The variance of weight norm \(\|_{i}^{l}\|\|_{i}^{l+1}\|\) at different layers of VGG-16 and ResNet18 trained on CIFAR-10. The y-axis corresponds to the variance while the x-axis corresponds to the epochs trained after initialization. From left to right is the result for the first layer, the layer in the middle, and the last layer. For more results please refer to Appendix A

Figure 5: Density plots of the weight norm of different layers of a ResNet-18 trained on CIFAR10. The x-axis corresponds to the weight norm, and the y-axis corresponds to the density. For clarity, we omit the label for the y-axis. From top to bottom, each density plot corresponds to different epochs, we demonstrate how the weight norm distribution of different layers changes during training.

Figure 6: Ratio of weight elements where the sign is changed compared to the initialization during training different models on CIFAR-10. We show the result for the first \(10\) epochs. After the first \(5\) epochs, the sign of at least \(10\%\) weights changes, indicating a dramatic change in the weight space.

ture that it results from the fact that the direction of weight vectors is drastically changed in the first stage. In fact, the first stage of neural network training is an extremely chaotic stage where the direction of each weight vector is dramatically changed. Fig. 6 shows that, for different networks, the sign of at least \(10\%\) weight elements changes after the first \(5\) epochs. As a result of the dramatic change in weight space, the direction of many weight vectors is reversed and the weight norm firstly decreases and then increases.

### Stage 2: Both Performance and Variance Drastically Increase

In the second stage, the variance of weight norm among channels starts to increase drastically. This phenomenon indicates that for some of the neurons, whose corresponding weight vectors are close to the gradient direction, the corresponding weight norm increases much faster than other neurons. At the same time, the performance also increases rapidly as shown in Fig. 7(a). The test performance after this stage is very close to the final performance, _e.g._ the testing accuracy for the VGG16 on CIFAR-10 at \(20\) epoch in Fig. 7(a) is at \(84.65\%\), which is relatively close to the final testing accuracy \(92.94\%\). We conjecture that to reach a fair performance, the network only needs to learn a few significant features, channels whose weight direction is close to the direction of the gradient would have advantages.

The second stage generally determines the relative weight norm after convergence. Compared to the chaotic first stage, the second stage is much more stable, where the weight norm of those neurons corresponding to the "significant features" discussed above constantly increases faster than other neurons. Notably, at the beginning of the second stage, the tendency of how the weight norm would change provides more information than the weight norm itself. As shown in Fig. 7(b), we demonstrate the weight norm at the beginning of the second stage and the weight norm after convergence. The result is from channels at the first layer of a VGG16 trained on CIFAR-10. The x-axis corresponds to the weight norm after convergence. For the upper subplot, the y-axis corresponds to the difference between the weight norm at the \(4\)-th epoch and the weight norm at the \(6\)-th epoch. For the lower subplot, the y-axis corresponds to the weight norm at the \(6\)-th epoch. The Pearson correlation between the weight norm difference and the final weight norm is \(0.8891\) while the Pearson correlation between the weight norm at \(6\)-th epoch and the final weight norm is only \(0.4254\). We conjecture that the channels whose weight norm increases rapidly in this stage correspond to those easy features such as the low-frequency information in Frequency-principle [27; 34; 32].

Figure 7: **(a)**: The loss, accuracy, and weight norm variance between neurons at \(5\)-th layer of a VGG16 trained on CIFAR-10. The model is trained for \(150\) epochs, after the second stage of training mentioned in Sec. 4.2 the model has achieved a high performance at around \(20\) epoch. For more details please refer to Appendix B. **(b)**: We demonstrate the correlation between the weight norm at the \(6\)-th epoch and the weight norm after the last epoch. The x-axis corresponds to the weight norm after the last epoch (\(150\)-th epoch). For the upper plot, the y-axis corresponds to the change of weight norm from \(6\)-th epoch to \(4\)-th epoch. For the lower plot, the y-axis corresponds to the weight norm at the \(6\)-th plot. Each point represents a neuron at the first layer of VGG16.

### Stage 3: Variance Decreases or Increases to Saturation at Different Layers

The third stage is the longest stage where the performance slowly increases. In the third stage, layers show two distinct patterns which indicates whether this layer learns similar neurons. For the variance of weight norm between neurons, there are two different patterns across the layers:

* **Increase to Saturate (IS):** The variance continues to increase slowly and stay high.
* **Decrease to Saturate (DS):** The variance decreases after reaching a high value.

Interestingly, as shown in Table 2, we find that for layers where neurons (or channels) could be merged using the algorithm proposed in , the variance between neurons follows the IS pattern that increases and stays at a high level. On the other hand, for layers where no neurons could be merged, the variance between neurons follows the DS pattern and decreases. Fig. 8 shows the cosine similarity of weight vectors between different neurons. For layers following the DS pattern, the cosine similarity of weight vectors between most different neurons is almost zero while the cosine similarity between neurons for layers following the IS pattern is much higher. It indicates that the weight vectors of channels in layers showing the DS pattern are nearly orthogonal while layers following the IS pattern contain many similar channels. Note that the neurons are sorted by the weight norm where the \(0\)-th neuron is the neuron with the largest weight norm. Fig. 8(a) and Fig. 8(c) show that neurons with different weight norms could be in a similar direction. Further results of the weight norm distribution are presented in Appendix A.

## 5 Width Modification on CNNs for Fewer Parameters and Better Performance

As discussed in Sec. 4.3, the layers in VGG-16 show different patterns. In this section, we investigate the weight norm variance change during the training of CNNs. We empirically show that for widely used CNNs such as VGG and ResNet, the middle layers show the DS pattern and other layers

    &  &  \\    & origin & & after neuron merging \\  Layer 1 & 64 & 42 & IS pattern \\ Layer 2 & 64 & 63 & DS pattern \\ Layer 3 & 128 & 128 & DS pattern \\ Layer 4 & 128 & 128 & DS pattern \\ Layer 5 & 256 & 256 & DS pattern \\ Layer 6 & 256 & 256 & DS pattern \\ Layer 7 & 256 & 256 & DS pattern \\ Layer 8 & 512 & 504 & IS pattern \\ Layer 9 & 512 & 500 & IS pattern \\ Layer 10 & 512 & 509 & IS pattern \\ Layer 11 & 512 & 506 & IS pattern \\ Layer 12 & 512 & 420 & IS pattern \\ Layer 13 & 512 & 250 & IS pattern \\   

Table 2: We employ IFM  to merge the neurons (channels) across the layers of a VGG16 trained on CIFAR-10. We list the original width, the width after merging, and the weight norm variance pattern in the third stage.

Figure 8: The cosine similarity between weight vectors at the same layer of a VGG16 trained on CIFAR-10. The value at \(i\)-th row and \(j\)-th column correspond to the cosine similarity between the weight vectors of the \(i\)-th neuron and the \(j\)-th neuron. Note that we sort the neurons by their weight norm in descending order which means the \(0\)-th neuron is the neuron with the largest weight norm. The results show that weights for neurons at the \(5\)-th layer of VGG16 are almost orthogonal.

show the IS pattern. We then adjust the layer width for VGG and ResNet, reducing the number of parameters while boosting the performance compared to the conventional layer width setting.

### Investigating Weight Norm Variance Across the Layers of CNNs

As shown in Fig. 4, we report the variance of weight norm \(\|_{i}^{l}\|\|_{i}^{l+1}\|\) at different layers of VGG-16 and ResNet18 trained on CIFAR-10. Generally, we find that the first several layers follow the IS pattern while the layers in the middle follow the DS pattern, and the layers at last follow the IS pattern again. It reflects the intrinsic nature of the neural network training such that layers in the middle learn orthogonal neurons. One may naturally associate this phenomenon with the results that intrinsic dimension firstly increases then decreases and reaches a high point in the middle . Intrinsic dimension describes the minimum dimension needed to solve a certain problem to a certain precision level. We conjecture that intrinsic dimension is a possible explanation for the two different kinds of layers where high intrinsic dimension leads to orthogonal weights. For more results, please refer to Appendix A. It further indicates that the conventional layer width setting for CNNs might not be optimal. In the conventional setting (such as in VGGs and ResNets) the layer width increases through the first several layers and then stays large through the following layers. Since the layers in the middle could use more width and the layers in the front and back could use less width, a better setting might be that the width first increase and then decrease across the layers with the widest layer in the middle just like a streamline.

### Streamlining the Width of Popular CNNs

To verify that setting the middle layer to be the widest is better than setting the last layer to be the widest, we adjust the width of each layer for widely used VGG and ResNet networks. For both VGG and ResNet, the width starts at \(64\) and ends at \(512\). We increase the width of the layers showing the DS pattern and decrease the width of the last several layers showing the IS pattern. For a fair comparison, we control the number of FLOPs of the width-adjusted model to be similar to the original model. As a result of the larger feature map in the middle compared to the last several layers, the number of parameters is reduced by nearly half. We train these models on CIFAR-10, CIFAR-100, and TinyImageNet. Each model was trained for \(120\) epochs with SGD. The learning rate is \(0.1\) at the beginning and is reduced by \(0.1\) every \(40\) epochs. The weight decay is set at \(1e-4\), and momentum is \(0.9\). For more details about the training recipe and the width-adjusted architecture please refer to Appendix B.

As shown in Table 3 and Table 4, we report the experimental results of our adjusted version of VGG and ResNet on CIFAR-10, CIFAR-100, and Tiny-ImageNet. Each result is averaged over \(10\) runs.

    & Validation accuracy \\   & origin & \(49.78 0.92\) \\  & streamline width & \(54.40 0.38\) \\   & origin & \(54.46 0.41\) \\  & streamline width & \(56.56 0.27\) \\   

Table 4: The result of VGG16 and ResNet18 trained on Tiny-ImageNet. Each result is averaged over \(10\) runs. Each model is trained for \(90\) epochs. More details are in Appendix B.

    &  &  &  &  &  \\   & & & & & CIFAR 10 & CIFAR-100 \\   & origin & \(14.73\)M & - & \(314.31\)M & - & \(91.02 0.35\) & \(66.17 0.71\) \\  & streamline width & \(9.06\)M & \(61.55\%\) & \(325.55\)M & \(103.89\%\) & \(93.41 0.15\) & \(68.63 0.31\) \\   & origin & \(20.04\)M & - & \(399.35\)M & - & \(91.84 0.46\) & \(65.53 0.66\) \\  & streamline width & \(11.50\)M & \(57.39\%\) & \(404.95\)M & \(101.04\%\) & \(92.38 0.09\) & \(66.99 0.52\) \\   & origin & \(11.17\)M & - & \(557.89\)M & - & \(92.99 0.23\) & \(72.84 0.27\) \\  & streamline width & \(6.26\)M & \(55.99\%\) & \(604.22\)M & \(108.30\%\) & \(93.60 0.13\) & \(73.38 0.30\) \\   & origin & \(23.52\)M & - & \(1311.59\)M & - & \(92.47 0.43\) & \(73.23 0.45\) \\  & streamline width & \(14.91\)M & \(63.40\%\) & \(1404.87\)M & \(107.11\%\) & \(93.12 0.33\) & \(73.53 0.50\) \\   

Table 3: In this table, we report the number of parameters, FLOPs, and the testing accuracy of the original VGG and ResNet models and our width-adjusted models. Each result is averaged over \(10\) runs with a different random seed. For a fair comparison, we make the FLOPs of the width-adjusted models close to the original model with nearly \(40\%\) parameters reduced.

The results show that with similar FLOPs, by adjusting the width across the layer, we could boost the performance and largely reduce the parameters.

## 6 Conclusion

In this paper, we propose to investigate the weight norm variance among channels in the same layer and identify two different patterns between narrow and wide layers with much empirical evidence (Sec. 3). We further show that training neural networks with the identified pattern could be divided into three stages from random initialization to convergence. We provide our explanations of the three stages with corresponding empirical evidence (Sec. 4). Using the identified pattern as an indicator, in Sec. 5, we propose to adjust the conventional layer width setting of VGG and ResNet to reduce the number of parameters and boost the performance. The main limitation of the width adjustment is that we manually adjust the layer width to control the FLOPs of the adjusted model. Another limitation is that we can not determine which pattern (IS or DS) the layer follows until convergence. A more automatic and efficient width adjustment method could be proposed regarding the new weight norm variance perspective, which we leave for future works. In assessing the potential broader impact, this work provides a new perspective to evaluate the layer width setting of deep neural networks, which have the potential to advance the neural network architecture design. This work has no significant negative potential impact.