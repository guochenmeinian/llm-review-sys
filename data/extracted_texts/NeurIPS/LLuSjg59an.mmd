# Where does In-context Learning

Happen in Large Language Models?

 Suzanna Sia

Johns Hopkins University

ssial@jh.edu

&David Mueller

Johns Hopkins University

dam@cs.jhu.edu

&Kevin Duh

Johns Hopkins University

kevinduh@cs.jhu.edu

Corresponding Author, suzyahyah@gmail.com and Code Repository https://github.com/suzyahyah/where_does_in-context-learning_happen_in_LLMs

###### Abstract

Self-supervised large language models have demonstrated the ability to perform various tasks via in-context learning, but little is known about where the model locates the task with respect to prompt instructions and demonstration examples. In this work, we attempt to characterize the region where large language models transition from recognizing the task to performing the task. Through a series of layer-wise context-masking experiments on GPTNeo2.7B, Bloom3B, and Starcoder2-7B, Llama3.1-8B, Llama3.1-8B-Instruct, on Machine Translation and Code generation, we demonstrate evidence of a "task recognition" point where the task is encoded into the input representations and attention to context is no longer necessary. Taking advantage of this redundancy results in 45% computational savings when prompting with 5 examples, and task recognition achieved at layer 14 / 32 using an example with Machine Translation. Our findings also have implication for resource and parameter efficient fine-tuning; we observe a correspondence between fine-tuning performance of individual LoRA layers and the task recognition layers.

## 1 Introduction

_In-context learning_ (ICL) refers to the phenomenon in which large generative pretrained transformers (GPTs) perform tasks with no gradient updates when shown task examples or descriptions in their context . Recent work on in-context learning has focused on _prompt-engineering_, treating GPT models as black boxes and focusing on which examples to provide in-context . However, many of these works apply surface level interventions leaving the internal mechanism of task recognition in GPT models largely not understood.

In this work, we ask **where does in-context Learning occur** in GPT models? Our view of In-context Learning is that of "task recognition" not "task learning" . While in-context learning in GPT models appears to be generally applicable to any natural language task, to study task location, we focus on two tasks, Machine Translation (MT) and Code generation, as there is little to no ambiguity in evaluating whether the model has recognized the task. For MT, the model must generate tokens in a different language. For Code generation, the model must produce a working program in the correct programming language. These two tasks are unlikely to be "learnt" from following patterns, and are more complex than a lookup in associative memory for simple Question-Answer tasks.

We focus on multi-head attention layers as a unit of study, as the self-attention mechanism is designed to allow the model to attend to it's context during generation of the target sentence . Using causal masking over different parts of the context we demonstrate that there exists a "task-recognition" point after which attention to the context is no longer necessary (Section 4). Concurrent and previouswork on Task and Function Vectors [32; 58] have also characterised a similar phenomena where the activations induced by in-context examples can be used to control tasks in the model.

We further characterise this phenomena, by studying the effect of on various ablations of masking self-attention over the instructions, examples, and even the query sentence itself. We report that not only is it _unnecessary to compute self-attention across the instructions and examples, in later layers self-attention over the query itself may also be redundant_.

This work informs the design of efficient inference and training for LLMs with the following contributions

1. We discover large computational savings when the context is several times longer than the test source sentence, a typical phenomena in prompt engineering (Section 5).
2. Parameter efficient fine-tuning corresponding to the phenomena of in-context learning. We observe that very lightweight fine-tuning of LoRA parameters  are most effective at earlier layers of the model compared to the later ones (Section 6). The effectiveness of the LoRA training corresponds directly to the layers that occur before the 'task recognition' point.

We further investigate the extent of MT _task redundancy_ using differentiable \(L_{0}\) regularisation to train discrete attention head gates (Section 5.1) and find that only around 10% of the attention heads can be fully masked. This indicates that the attention-heads themselves are not redundant, it is attention over all of the context that can be redundant. This fundamentally differs from the literature in supervised learning where more than half of the attention heads can be pruned, and Transformers are highly specialised for particular tasks [61; 44; 7].

## 2 Background

In-Context Learningwas first demonstrated by  who showed that GPT-3 could be used to perform a huge variety of tasks without any task-specific parameters or training, by conditioning the model's generation on a _prompt_ which included a few labeled examples of the task of interest. Since then, interest in using GPT models for ICL has grown significantly [45; 3; 66], with several recent works introducing methods such as instruction-tuning [55; 63] or chain-of-thought prompting  to improve downstream ICL accuracy. One key characteristic of In-context Learning is its reliance on prompt examples demonstrating the task that the model should carry out .

In-context Learning as Task Recognition.Ostensibly, ICL can work for nearly any task that can be defined or described in natural language, and therefore has potential for incredibly broad impact. However, ICL can often still underperform supervised fine-tuning , prompting research in analyzing the mechanisms underlying ICL. One line of work studies in-context learning with _linear_ functions, typically linear regression, characterizing the learnability of these functions with ICL [39; 28] and even the learning algorithm a transformer uses [2; 18; 62]. A second body of work suggests that in-context learning locates _existing_ latent concepts (tasks) which have been _already learnt_ during pretraining [69; 65]. Notably,  describe function vectors which are robust to changes in context.  try to characterise the extent of task recognition from the pre-training data. Although there have been many studies on task recognition, our work presents a complementary perspective for task recognition, by demonstrating that there exists a point in the model's _layers_ where the task has been located and causal self-attention onto the context is no longer needed for the model to perform the task.2

Transformer Layers and Self-attention as the Unit of Study.Many works study layers of the model as a natural unit of analysis for interpretability [33; 20; 48; 24; 8; 54]. We highlight some of the work which is more closely related to task performance.  study the layer-wise adaptability by a hidden-state variability ratio while  study evolution of representations in MT-supervised transformer models.  studies when model layers can be skipped by feeding intermediate representations into the final output layer of a pre-trained supervised model. Our work adds to this body of work by considering the perspective of when and where layers are responsible for task location in in-context learning models.

The self-attention mechanism specifically has been highlighted as a source of redundancy by many previous and concurrent works [10; 46; 31]. This is due to it's causal structure over the input symbols under the specific context of the input sequence within it's context window . In this paper, we study a major source of causal redundancy in the input, the "prompt examples" that are provided as input-output demonstrations to the model for "in-context learning".

Transformer overparameterization and redundancy has been an active area of research  with multiple works suggesting to adapt transformer inference depth [36; 15; 25]. While we draw inspiration from these, our main objective is not to compress models for inference, but to highlight the redundancy in computing over long context token sequences.

## 3 Data and Settings

ModelsWe use GPTNeo2.7B , Bloom3B , Llama3.1-8B and Llama3.1-8B-Instruct in all of our experiments with Machine Translation. For code generation, we used Llama3.1-8B-Instruct and starcoder2-7B. GPTNeo2.7B has 32 layers and 20 heads, Bloom3B has 30 layers and 32 heads, llama2-7B and llama3.1-8B has 32 layers and 32 heads and Starcoder2 has 30 layers and 24 heads. The checkpoints we use are from Meta AI (for Llama) and the transformers library . Starcoder2 and llama models utilises grouped-query attention , while the rest of the models use "regular" multi-head self-attention.

GPTNeo was trained on The PILE , an 825GB text dataset which consists of roughly 98% English data. Despite being mostly monolingual, The PILE contains Europarl which GPTNeo was trained on at a document level (rather than a sentence level). Conversely, Bloom was trained on the ROATS corpus , a composite collection of 498 datasets that were explicitly selected to be multilingual, representing 46 natural languages and 13 programming languages. Llama training data consists primarily of common crawl, C4, wikipedia, stackexchange as major sources. Starcoder2 was trained on Github as well as Arxiv and Wikipedia. To our knowledge, there has not been any reports of sentence level parallel corpora in the training datasets of these models.

DataWe test our models using two datasets, Flores for Translation and HumanEval for Code generation. For Flores, we experiment with en\(\)fr (main paper) and en\(\)pt (appendix). Prompt examples are drawn from the development set. We evaluate the generations using BLEU scores, following the implementation from . For HumanEval, we evaluate on the execution accuracy of the generated code using the Pass@1 metric. As HumanEval does not have an explicit train set, the prompt set is drawn from the Mostly Basic Python Program (MBPP) dataset . To account for example selection and ordering effects,3 all inference runs were repeated with 5 randomly sampled prompt example sets.

Prompt FormatOur prompts may consist of instructions, examples, both, or none. Importantly, we adopt _neutral_ delimiters, "Q:" and "A:" to separate the prompt and the start of machine generated text. This ensures that the models do not have any information from the delimiters on what the task is and must recognise the task from examples. 4

For the translation task, when no natural language instructions are used the model input will be Q: {source_sentence} A: Instructions are given in natural language and take the form: Translate from {L1} to {L2}: Q: {source_sentence} A:, where L1 = English and L2 = French if the source and target languages are English and French respectively. Examples are given after instructions, and similarly delimited by Q: and A:. See Appendix: Table 1 for an example.

For the code generation task, when no natural language instructions are used, the model input will be Q: {program_description}, where the program_description is Instructions are given in natural language and take the form: "Write a program for the following task:".

## 4 Where Does In-Context MT happen?

### Analysis Methodology: Layer-from Masking

In-context learning differs from task-specific supervised learning in that, during test time, the desired task must be identified, or learned, from the context first and then applied to the input. At what stage in the feed-forward computation does a causal Large Language Model transition from an in-context learner to a translation or code-generation model? To explore this question, we introduce _layer-from context-masking_ which masks out all attention weights to the context (instructions, examples, or queries) from a certain layer _onwards_ (see Figure 1 for a graphical description).

For Causal Decoder-only Transformer Language Models, given each position \(i\), the Attention weight \(_{ij}\) over context positions \(j,j<i\) can be computed by a \(_{ij}=(}{}})_{ij}\). Each element in \((QK^{T})\) is the dot product between a query vector and key vector \(q_{i} k_{j}\), where \(q_{i}=W_{q}x_{i},k_{j}=W_{k}x_{j}\) for trained weight matrices \(W_{k}\) and \(W_{q}\).5 We apply the attention mask over the input so that the attention score is \((q_{i} k_{j})+m(j,)\). Here, \(\) are the tokens that we wish to mask, and

\[m(j,)=0&x_{j}=\\ -&x_{j}().\]

is implemented in practice as the smallest floating point value for that datatype. All masks operate from the \(j\)-th layer (\(_{j}\)) _onwards_, i.e. masking from \(_{20}\) means zeroing out attention to all positions

Figure 1: **(Top):** Graphical explanation of Masking the Attention over Instructions and Examples. The leftmost image has instructions and masks examples (\(,}^{Mask}\)), while the right image has both instructions and examples masked (\(,^{Mask}\)). **(Bottom):** We demonstrate which components of the input prompt are masked for each setting that we experiment with. The overline of the setting name describes which portion of the input is highlighted (and thus masked). \(N/Y\) refer to absence / presence of either Instruction (Instr) or Examples (Ex). Although we are primarily concerned with the effects of masking out task-identifying context (i.e. instructions and examples), in some experiments we additionally consider masking out the input query as well.

in \(\) from \(_{20:n_{}}\), where \(n_{}\) is the total number of layers. To construct Fig 2, we increment \(_{j}\) from \(1\) to \(n_{}\) and apply the set of masks \(\{m(j,)\}^{_{j}:n_{}}\) in each experiment and observe the performance of the model.

When masking input tokens from layer \(\), the model must rely on only the information in the hidden state representations of the remaining, unmasked tokens from layer \(+1\), since representations of the masked tokens can no longer be incorporated moving forwards; if the unmasked representations do not already encode enough information to complete the task (e.g., Machine translation) then the model will fail to generate the correct output. Our intuition is the following: if, at layer \(\), the model can perform the target-task without attending to the task _context_--task-identifying tokens such as instructions and examples--then information about the task has already been incorporated into the query representations and the model has identified, or "recognized", the target-task by layer \(\).

### Experiments on _Layer-from context-masking_

In Figure 1 (Table) we show the various masking treatments that we apply to the input in our experiments. We ablate over 3 different task-context masking settings to test the impact of various parts of the context: providing only task examples and masking them from a given layer \((}^{Mask},)\); including the instruction and but only masking out the examples \((,}^{Mask},)\); and including instructions but masking them with the examples \((,^{Mask},)\). As a control in our experiments, we also experiment with masking the entire input \((},,^{Mask})\) to study whether the model needs to attend to _any_ input beyond a certain layer, and with masking only the query \((,,}^{Mask})\) to study whether masking the context vs the query have similar effects. For each masking setting, we apply the mask from all layers in the model (\(j=1,,n_{}\)) and observe how task performance is affected at each layer. When examples are provided in-context, we use 5 examples per prompt and we re-sample these examples to control for variance in example selection.

### Results

Models do not need to maintain attention over the task context past a certain layer to perform the task.In all models, we observe that when applying masking from \(\{m(j,)\}^{:n_{}}\) over the task context, models obtain their maximum performance well before the final layer, i.e., when \(<n_{}\). The results of our experiment for \(\) and \(\) are shown in Figure 2, and additional experiments for GPTNeo and Bloom on \(\) and \(\) are shown in Section A.4. Different models reach this plateau point at different layers: in GPTNeo this point occurs around layer 25, in Bloom this point occurs around layer 15-20, and in Llama models this occurs around layer 13-15. As English is the dominant language in most model's training, models can successfully perform translation into English upon earlier layers of masking, than translation out of English. Once this plateau is reached, the models benefits only marginally, if at all, from retaining attention to the context, suggesting most of the task "location" has already occurred.

We observed that with Llama-3.1 models, when masking only task context, there is a jump in performance from nearly negligible to nearly optimal in the course of a few layers. Conversely, when the query is masked we see both performance begin to rise much later in the model and the approach to optimal performance occur much more slowly, often plateauing only a few layers before the end of the model.

In some models, there may be a point where forward computation is independent of even the query.There _is_ also a point in the model where it no longer needs access to any input query tokens. We find this effect to be much less pronounced in GPTNeo2.7B, Bloom3b and Starcoder on the code generation task, and thus maybe a characteristic of Llama models training.

There exists critical layers for task location.Prior to the task recognition point, around the middle layers of the models, moving the context mask up a layer results in a significant increase to performance. We consider these critical layers, as instead of a gradual increase in performance, we observe very steep jumps, accounting for more than 80% of the model's ceiling performance for that task. We conjecture that the model is locating the correct task during processing in these middle layers, after which the context is no longer necessary to perform the task.

Overall, our findings suggest a 3-phase process to in-context learning: in the first phase, moving the mask up makes little difference in performance, which is close to 0. This suggests that the context has not influenced task location at all. In the second phase, shifting the mask upwards makes a large difference in performance, suggesting that the model has started to locate the task but can improve significantly with more processing of the context. Finally, in the third phase, shifting the mask upwards again has little-to-no effect on the performance, suggesting that the model has fully recognized the task as translation and no longer requires the context to interpret the task.

Figure 3: _Layer-from context-masking experiments_ for Starcoder2-3B, Starcoder2-7B, Llama7b, Llama7b-chat on a text to code generation task. The graphs show translation performance when masking contexts from the \(j^{}\) layer onwards. Different lines indicate different treatments of the instruction, as described in Figure 1. The dashed black line is the performance when shown both examples and instructions without masking.

Figure 2: _Layer-from context-masking experiments_ for Llama3.1-8B, Llama3.1-8B-Instruct \(\) (main figure), and GPTNe02.7B, BLOOM3B, \(\), Llama3.1-8B, Llama3.1-8B-Instruct on \(\). The graphs show translation performance when masking contexts from the \(j^{}\) layer onwards. Different lines indicate different masking treatments, as described in Figure 1. The dashed black line is the performance when no masking of the input occurs.

### Instruction-tuned vs Non-instruction Tuned Models

When comparing non-instruction tuned Llama3.1-8B vs instruction-tuned models Llama3.1-8B-Instruction, we do not observe any noticeable difference in where performance plateaus, i.e., where the model no longer requires attention over the context. This occurs around layers \(16\) for both Llama models in \(\) and around layer \(13\) for \(\). The main difference is that instruction-tuned model is able to achieve better performance in the earlier layers for the setting where instructions are present and examples are masked (\(,}^{Mask}\)). This is to be expected as these models are tuned towards following instructions.

Overall we find that the observation of task recognition layers and a task recognition point is present across both non-instruction tuned and instruction tuned models, and that this presents itself similarly in both types of models.

Do models have a distinct task recognition region regardless of the type of task? (Experiments on Code Generation)

For tasks that the model does not perform fluently, we do not observe a sharp increase at any particular layer. For instance, for code generation (HumanEval) where the llama2 model performs poorly, we can observe only a very gradual effect of masking the self-attention layers, and not a distinct increase as compared to the llama2's performance on Translation.

However when we consider Starcoder2 while masking instructions or no instructions, i.e., the \(,}^{Mask}\) and \(}^{Mask}\), we again see the same pattern demonstrating the task recognition phenomena on layer 19 of the 3B model, and layer 20-23 of the 7B model.

To understand Starcoder2's strong performance on the (\(,}^{Mask}\)) condition, investigations found that the instructions and the test prompt alone contain sufficient information for the model to recognise that the task is to generate a Python program, even though the model is not instruction tuned. This happens as the model is very specialised towards code generation and has a strong prior to generate python code given its prevalence in it's training data.

### The Role of Instructions vs Examples

In separate experiments, we found that when shown only instructions and no examples, GPTNeo and Bloom models are unable to translate, and their performance is nearly at 0 BLEU Score. For GPTNeo and Bloom we see that the behavior of the model is similar when no instructions are present (\(}^{Mask}\)) and when instructions are masked (\(},}^{Mask}\)). However, if the model is given complete access to instructions (\(}^{Mask}\)), it can use the intermediate processing of examples to reach baseline performance earlier.

Figure 4: _Layer-from context-masking experiments_ for GPTNeo and BLOOM on \(\) investigating number of examples in the \(}^{Mask}\) mask setting. The dashed black line refers to no instructions and no examples.

### Does the Number of Prompts Affect Task Recognition?

In Section 4 we study context-masking with a fixed number of prompts. However, it is not clear if the number of prompts affects how fast, layer-wise, the model is able to recognize the task. We plot these results for en\(\)fr in Figure 4, for both GPTNeo and BLOOM. In general, we find that the number of prompt examples has little effect on which layer the task is recognized at. While there is some variation in performance when the context is masked around the middle layers of the model, the final performance plateau occurs at the same layer regardless of the number of prompts.

## 5 Inference Efficiency

Speeding up transformer inference is of great interest to the community . We highlight the potential of speeding up inference time as a direct consequence of identifying where task recognition occurs in the model and redundancy of self-attention processing. In Figure 5, we illustrate self-attention from the query and generated sequence tokens over itself (Task Processing) and Self-attention over the prompt Instructions and Examples (Masked). If ceiling performance is achieved, then self-attention over the previous context becomes redundant (Redundant Self-attention Computation).

Our results indicate that we can achieve significant speedups in inference by removing the processing of context-tokens all-together after a certain point in the model, with little to no impact on downstream performance. Let \(_{r}\) be the \(r^{}\) layer where we can mask out the attention of the context across subsequent layers and match the "ceiling" performance. Let \(k\) be the number of prompt examples, where each example consists of a pair of parallel sentences. Then, for a model with \(n_{}\) layers, the amount of processing in terms of speed and memory saved is approximately \((n_{}-r)/n_{}(k/k+1)\).

Using the example of Llama3.1-8B (\(32\) layers) on en\(\)fr, we see from Figure 3 that the model is very close to it's ceiling score after processing the examples at layer 14 (\(=14\)). If we no longer need to process examples after \(=14\), **under a prompt size of \(5\) the savings are approximately 50%.**

For instruction-tuned models which are typically deployed in production, even if we assume that no examples are provided, savings can be non-trivial as very long-form instructions are typically provided to the model in an attempt to control it's behavior (prompt engineering).

Figure 5: **(Left) Illustration of redundancy in self-attention computation based on our masking setup (\(,^{Mask},\)). (Right) Visualisation of attention head masks for GPTNeo and BLOOM, learned with \(L_{0}(=0.01)\) regularisation under a 0-prompt train scheme in en\(\)fr. A value of \(0\) (in black) indicates that the attention head is effectively masked out by the trained attention gate. Around 10% of attention heads are masked out i.e., redundant, with a majority of them occuring at the later layers for GPTNeo and distributed across layers for BLOOM. fr\(\)en is availble in Section A.7.1**Although we had demonstrated the redundancy of self-attention over the input context, the significance of this computational savings extends to _all_ components of the transformer during forward inference. Since all subsequent layers of forward inference no longer rely on computations on previous token positions, all processing related to those redundant token positions (from the task recognition layer onwards) can be effectively removed.

Overall, observing redundancy over the context is not surprising. To explain why models can have such redundancy, we refer to  who identify a phenomena where attention heads attend almost exclusively to delimiter and separator tokens such as [SEP], periods and commas. This is thought to act as a "no-op" as the value of such tokens in changing the current hidden representation is very small. Note that it is then possible to mask entire Transformer layers and still achieve a sensible output due to residual connections in the Transformer architecture at every layer.

### Are There Specialised Attention Heads?

A well established finding for supervised encoder-decoder MT models, is that up to 90% of the attention heads can be pruned while minimising fall in translation performance [61; 6; 44]. We note that asking about the extent of pruning is a slightly ill-formed research question, as it depends on the type of pruning technique used. However broad trends of highly prunable models have been observed in the supervised MT paradigm. For instance,  studied attention-head importance for a broader set of ICL tasks, finding that the most important heads for ICL occur in the middle layers of the model. We train discrete attention head gates with \(L_{0}\) regularisation for GPTNeeo and Bloom on en\(\) fr (see Section A.7.1). Overall, we report that there are no "few" specialised heads, which directly contrasts with the literature on compression in supervised MT models [61; 44]. Potential reasons for this difference might be due to cross-entropy loss associated with task tuning for MT vs non-specific training on large corpora. We leave this as an open question for future work.

## 6 The Adaptability of Task Layers

The layers prior to "task recognition" should contain information about locating the MT task. To test this, we further explore the adaptability of these layers by lightweight fine-tuning experiments on Llama3.1-8B and Llama3.1-8B-Instruct on en\(\) fr.

We trained a single Low-rank Adaptation matrix (LoRA; ) for each layer of the output projection while keeping the rest of the network frozen.6 This means there were \(n_{}\) individual layers trained for \(n_{}\) experiments in Figure 6, where \(n_{}\) is the total number of layers of the model.

The model was shown parallel sentences as input, and layers were trained with no explicit translation instructions. We split the dev set of FLORES into \(400\) and \(800\) training examples and \(200\) dev examples, we repeated the experiments with 2 random seeds initialisations. Note that this setup is designed to tune the layers for task location. It is highly unlikely that the model can learn translation

Figure 6: Performance of models ((Llama3.1-8B and Llama3.1-8B-Instruct for en\(\)fr) trained with single LoRA layer, where each point on the x-axis reflects a single trained LoRA layer. The LoRA layer was trained without instructions, and with causal LM cross-entropy loss over next token prediction of parallel translation sentences. \(400\) and \(800\) refer to the size of the training set. The layers which are most amenable to lightweight fine-tuning occur in the earlier layers before the ”task recognition” point.

knowledge from this small amount of supervision. The LoRA layers were trained for up to \(50\) epochs with batch size\(=32\), learning rate\(=1e-4\), early stopping patience\(=5\) and threshold\(=0.01\), with \(=32,r=8\) and dropout\(=0.05\). These values are default and there was no hyper-parameter optimisation over the training parameters. The cross-entropy loss was computed across the entire sequence, and we used the best checkpoint on the 200 held out dev examples for evaluation.

Without any fine-tuning, performance is close to 0 because the model will generate sequences continuing from the source language instead of doing translation. **While each layer can be trained to perform better than no fine-tuning at all, tuning different layers have vastly different impacts on performance** (see Figure 6). In particular, we find that high performing layers occur at the earlier to middle parts of the network, with the peak occuring strictly before the "task-locating" layers from Section 4.

### Task Locating Layers are critical for resource efficient fine-tuning

With half the number of training examples (400 instead of 800), the range of trainable layers drop very greatly. For the more challenging direction of generating in French, \(\), reducing the number of training examples result in none of the layers being successfully fine-tuned for translation task location. For \(\), the range of trainable layers is much more concentrated around layers \(10\) to \(15\), which occurs just before the 'task recognition' layers as shown in Figure 2.

We demonstrate that in contrast to common fine-tuning wisdom, additional tuning on later layers in the transformer network has a much smaller impact on final performance, and this is strong correlated with where the 'task locating' layers are in the model. The major reason for this discrepancy from conventional fine-tuning wisdom, is that we are performing extremely lightweight parameter-efficient fine-tuning for task location, and not full fine-tuning on large datasets. Our results should thus be interpreted under the lens of highly resource efficient fine-tuning of layers for task location, and is fundamentally different from the wisdom of "true" task fine-tuning.

## 7 Conclusion

We demonstrate evidence that In-context Causal Decoder models locate their task at a specific layers during forward inference. To study this, we introduced causal masking of self-attention over the context from layer \(\) onwards (Section 4). The findings generalise across 4 models of different sizes and in both non instruction-tuned and instruction-tuned models. We further identify certain layers as task critical, and show that this corresponds to the task recognition point of the model (Section A.9) and is not influenced by increasing number of examples (Section 4.7) shown to the models.

Our central finding that models do not need to maintain attention over all of the context across every layer has direct implications for inference efficiency of transformers, with estimated up to 45% cost-savings for llama model with 5 examples (Section 5).

Contrary to common fine-tuning wisdom, we show that it is sometimes beneficial to target middle layers for fine-tuning the model which could be associated with task recognition ( Section 6). Finally, we trained attention head gates using differentiable \(L_{0}\) regularisation (Section 5.1), and found that around 10% of attention heads can be masked. These are mostly distributed across the later layers of the model, providing some support for the idea that later layers are redundant. Although we have characterised this phenomena using Machine Translation and Code Generation, we believe that the broad findings are likely to generalise to other tasks.

### Limitations (and Future Work)

* There is limited exploration of why different models exhibit varying behaviors in terms of their "task recognition point" and critical layers. Unfortunately, the differences are not due to easily observable hyperparameters like model size or architecture. To put in another way, why do large models exhibit different characteristics?
* This paper reports on empirical analysis and observations, and currently lacks a more theoretical framework that could explain why this phenomena is being observed.

#### Acknowledgments

We would like to thank all the anonymous reviewers for their invaluable comments and suggestions, as well as Daniel Kashabi and Marc Marone for feedback on earlier drafts.