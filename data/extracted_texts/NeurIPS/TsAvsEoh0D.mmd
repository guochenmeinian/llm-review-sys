# WILT: A Multi-turn, Memorization-Robust Inductive Logic Benchmark for LLMs

Eryk Banatt

Riot Games

Los Angeles, CA 90064

ebanatt@riotgames.com

&Jonathan Cheng

Riot Games

Los Angeles, CA 90064

joncheng@riotgames.com

&Tiffany Hwu

Riot Games

Los Angeles, CA 90064

thwu@riotgames.com

###### Abstract

While large language models (LLMs) have shown impressive capabilities across a wide range of domains, they still encounter significant challenges in reasoning tasks that require gathering evidence over multiple turns and drawing logical conclusions from this evidence. Despite the multi-turn nature of many real-world LLM use cases, most existing benchmarks rely on carefully curated single-turn tests, which often blur the line between memorization and genuine reasoning. To address this, we introduce the **Wason Inductive Logic Test (WILT)**, a simple yet challenging multi-turn reasoning benchmark designed to resist memorization. WILT is inspired by the Wason 2-4-6 task , where participants must infer a basic boolean function involving three variables (e.g., \(x<y<z\)) by proposing test cases (such as \((2,4,6)\)). In WILT, each test starts from a clean slate, with only the initial instructions provided, preventing models from relying on pre-learned responses. Our findings reveal that LLMs struggle with this task, with the best-performing model achieving only 28% accuracy, highlighting a significant gap in LLM performance on complex multi-turn reasoning tasks.

## 1 Introduction

Large Language Models (LLMs) powered by the transformer architecture  have enabled a new computing paradigm powered by natural language. LLMs have seen an increased ubiquitousness in day-to-day life beyond the machine learning research space, where they help many people across the world with common tasks. These models interact with users through _multi-turn conversations_, a capability added to next-token-prediction models via instruction-tuning  and alignment post-training phases .

Measuring the performance of LLMs has been challenging for the research community. Publicly available benchmarks are subject to Goodhart's law  or implicit overfitting , and difficult benchmarks often keep a held-out, publicly unavailable test set to accurately evaluate models . In addition, the vast majority of benchmarks are suites of single-turn tests, which differ substantially from the common day-to-day use of LLMs .

The reasoning capability of LLMs, particularly in multi-turn scenarios, is of substantial interest. A commonly reported failure pattern for LLMs is the "doom loop", where the model repeatedly responds with a near-identical message to one of its earlier messages, providing minimal utility. While some benchmarks have emerged to attempt to measure this multi-turn propensity of repetition , none so far have done so for multi-step inductive reasoning.

In this work, we make the following contributions:1. We introduce the **Wason Inductive Logic Test (WILT)**, a multi-turn inductive logic benchmark that is robust to memorization. We show that frontier LLMs struggle significantly on this task.
2. We further evaluate the reasoning capabilities of the LLMs by framing the task as an exploration of hypothesis space. The LLMs show marked differences in the rate of hypothesis space reduction, novelty of responses, and complexity.

## 2 Wilt

The Wason Inductive Logic Test (WILT) is a benchmark for LLM reasoning inspired by the Wason 2-4-6 task . Models begin with the instruction that they must uncover the hidden rule, and may pose up to 30 test cases of that rule. For example, they can pose the tuple \((2,4,6)\) and the test will respond with "True, 29 Attempts Remaining."

All hidden rules take three numbers and return a boolean. These rules are simple and non-stochastic, so there is no additional value to posing the same test multiple times. Valid inputs include any float or integer that can be typed in three or fewer characters, excluding signs and the decimal point (e.g. -999, 1.23, 5). The hidden rules are written as Python lambda functions. After a maximum of thirty tries (or any turn before then), the model may make one attempt to guess the function, after which the test will terminate. The model must return a Python lambda function that is the same as or equivalent1 to the hidden rule in order to receive full points.

WILT is conceptually simple, but very challenging. Humans can identify simple rules fairly easily despite the infinitely large hypothesis space, the unbounded difficulty of a hidden function, and the impossibility of verifying the correctness of your response. For example, consider the canonical Wason task rule of \(x<y<z\). This rule has very high overlap with the much more arbitrary rule \((x<y<z)(x 12)\). The WILT benchmark therefore tests a few high-value behaviors of interest:

1. **Multi-Turn Capability**: Participants that fall into "doom loops" are punished by virtue of having less useful information with which to infer the hidden rule.
2. **Hypothesis Space Reduction**: Participants are rewarded for proposing test cases that effectively narrow down the possible rules, despite that hypothesis space being infinitely large.
3. **Susceptibility to Confirmation Bias**: Participants who are more prone to "confirming their hypothesis" rather than seeking falsification will perform poorly upon this task.
4. **Inductive Mathematical Reasoning**: Proposing good test cases is a useful test of inductive reasoning and the ability to generalize from a number of specific examples.
5. **Deductive Mathematical Reasoning**: Proposing sensible functions after observing many test cases is a useful test of deductive reasoning, with successful performance rewarding identifying a specific function that fits a suite of examples.
6. **Occam's Razor**: Participants are rewarded for finding the simplest explanation fitting the examples.

We release two test suites: a _lite split_, with 10 very easy tests and a canonical _full split_ with 50 moderately difficult tests. Future work will extend this to include a procedurally generated split for robustness to overfitting. We find that the lite split quickly produces a roughly similar ordering to the full split, but we report results upon the full split for the remainder of this work. Please see the appendix for further details. The test and associated code can be found at github.com/riotgames/wilt.

## 3 Related Work

Compared to other reasoning benchmarks, WILT stands out as both highly multi-turn focused and unusually robust to memorization. In contrast to other benchmarks, WILT requires models to _interact with an environment_ by proposing their own test cases to uncover a hidden function without relying on pre-provided examples. This setup reduces the risk of overfitting, as each test begins with the same initial instructions, and the model must generate and interpret its own data.

### Reasoning Benchmarks

There are a wide variety of reasoning benchmarks used to evaluate large language models. Some very notable among these are MATH , GSM8K , CommonsenseQA , StrategyQA , BIG-BENCH , SciBench , SVAMP , ARC-AGI , MMLU , GPQA , and HumanEval . These benchmarks are the standard for measuring LLM reasoning capabilities, but are overwhelmingly carefully chosen single-turn problems which aim to meaningfully separate the performance of different models on reasoning-like outputs such as math, code, or logic puzzles. However, these benchmarks are subject to train-on-test leakage, even if efforts are made to decontaminate the dataset , and the majority are explicitly single-turn tests. Our benchmark directly measures the model's ability to navigate multi-turn scenarios and does not require careful hiding of a test set to prevent misleading results.

With respect to reasoning about simple functions, a benchmark that stands out as similar to ours is CRUXEval , which assembles a list of 800 simple Python functions and input-output pairs, evaluating language models on their ability to predict input from output and output from input. Our work could be seen as a multi-turn, more difficult extension of this work - one where the function is replaced with a black box, where helpful and informative input-output pairs are not provided but instead need to be searched for by the language model, and where the objective is to infer the hidden function rather than the input or output.

### Multi-Turn Benchmarks

There are a handful of multi-turn benchmarks used to evaluate LLMs. PlanBench  is one prominent benchmark that attempts to measure the ability of LLMs to navigate planning problems. This is a class of problems that is solved easily by classical planning algorithms such as STRIPS , and like our benchmark poses a significant challenge to LLMs. PlanBench is a primarily multi-step, single-turn benchmark with a multi-turn component (i.e. replanning based on unexpected events), which contrasts with our benchmark's more direct multi-turn focus. This can be observed in the o1 models performing comparatively well on PlanBench , since scaling inference time compute within a single turn would be expected to improve performance substantially. Similarly, BIG-BENCH  includes a "20 Questions" task, which pairs two language models together to guess a word by posing questions about it. Like ours, this involves an iterated reduction of hypothesis space over multiple turns, but unlike ours relies on potentially subjective or unreliable LLM responses to queries about the concept.

Closest to ours are MINT  and Aidan-bench , which have more direct multi-turn focus. MINT repurposes existing single-turn benchmarks by allowing models to use tools before answering. While the range of tasks in MINT is therefore quite large, strong models can still solve these tasks in few (or one) turns, and the unmodified prompts remain subject to test set leakage. Aidan-bench measures the cosine similarity between multi-turn responses. This represents a more pure measurement of the doom loop phenomenon. In our benchmark, rather than directly measuring the doom loops, we are instead measuring how often those doom loops lead to failures of reasoning. We see similar performances in our benchmark compared to Aidan-bench (e.g. Mistral Large), but with an ordering more tied to capabilities (e.g. Sonnet's strong results, see Table 1).

### Hypothesis Space Reduction

Hypothesis space representation is a commonly used framing in inductive logic tasks for LLMs. In , the authors show a technique called _hypothesis search_ where the model will propose hypotheses in natural language and then implement these hypotheses as Python programs. This technique was shown to improve performance on ARC-AGI , but a similar approach could be used along with chain-of-thought  for WILT as well.

## 4 Results

Our results for this test can be found in Table 1. We show that despite the test's relative simplicity, most models struggle substantially both to propose good tests and to infer a rule based on available evidence. As in the original Wason 2-4-6 task, we find a common failure mode to be confirmationbias - a participant will identify a plausible hypothesis and continue to propose tests that attempt to _confirm_ it. Stronger models will more explicitly attempt to _falsify_ these hypotheses instead.

In Table 1, we include a column _approximately correct_, measuring the number of rules in which the model was able to correctly identify some critical behavior of the rule, but returned a rule with failing edge cases. For example, guessing \((x<y<z)\) instead of \((x y z)\) is approximately correct. We include this column to highlight models that are more willing to guess immediately instead of uncovering edge cases by default (e.g. Llama 3.1 405B). In these cases, we could see potentially improved performance through more targeted prompting techniques.

We find that LLMs (particularly smaller ones) will frequently repeat tests they have already used, sometimes dozens of times. We therefore also provide a column _repeats_ which counts the total proposed tests which were already tested for that rule. Further discussion on test novelty can be found in Appendix A.4. Additionally, further experiments analyzing what parts of the task models are strong or weak at can be found in Appendix A.2. We find that some models are produce very useful test cases (e.g. chatgpt-4o-latest) whereas other models are strong at guessing the rule (e.g. o1-mini).

### Hypothesis Space Reduction

To compare the LLMs' ability to efficiently reason about the task, we estimate how effectively each model reduces the hypothesis space . At best, an LLM should always propose a triplet that eliminates as many untested hypotheses as possible. At worst, a model repeatedly proposes a triplet confirming previously covered hypotheses. For example, an LLM that has already guessed \((2,4,6)\) retreads much of the same hypothesis space by guessing \((4,6,8)\) rather than \((0.01,-1,100)\).

To represent that hypothesis space, we randomly generate a large number of lambdas that encompass a wide range of potential hypotheses. For example, we randomly generate lambdas having to do with: ordering \((x<y<z)\), equality \((x=y z)\), arithmetic relations \((x+y=z)\), parity \((x 10,y 5,z 100)\), etc. When an LLM proposes a triplet, we cross off any lambdas that do not match the observed behavior of the hidden rule. Figure 1 illustrates the rate at which different models reduce the hypothesis space over successive turns. Models with worse reasoning spend more attempts to clear less of the hypothesis space.

  
**Model** & **Accuracy** & **Approx. Correct** & **Avg. Guesses** & **Repeats** \\  Claude 3.5 Sonnet 20240620  & **14/50** & **10/50** & 16.38 & 27 \\ o1-mini 2024-09-12  & 13/50 & 8/50 & 12.1 & **3** \\ o1-preview 2024-09-12  & 12/50 & 6/50 & **8.12\({}^{2}\)** & **3** \\ chatgpt-4o-latest  & 11/50 & 7/50 & 14.22 & 38 \\ Mistral Large 2  & 11/50 & 5/50 & 26.56 & 142 \\ GPT-4o 2024-08-06  & 9/50 & 6/50 & 15.26 & 26 \\ Llama 3.1 405B  & 8/50 & 9/50 & 12.21 & 30 \\ Gemini 1.5 Flash 0827  & 7/50 & 4/50 & 14.04 & 108 \\ Llama 3.1 70B  & 7/50 & 2/50 & 15.18 & 74 \\ Deepseek-v2.5-chat  & 6/50 & 5/50 & 27.22 & 489 \\ GPT-4o-mini  & 6/50 & 2/50 & 20.36 & 54 \\ Gemini 1.5 Pro  & 5/50 & 6/50 & 16.78 & 41 \\ Gemini 1.5 Flash  & 5/50 & 6/50 & 16.5 & 123 \\ Deepseek-v2-coder  & 5/50 & 5/50 & 21.82 & 335 \\ Deepseek-v2-chat  & 3/50 & 3/50 & 25.32 & 334 \\ Llama 3.1 8b  & 3/50 & 0/50 & 26.18 & 223 \\ Open Mistral Nemo  & 2/50 & 3/50 & 27.34 & 400 \\ Claude 3 Haiku 20240307  & 1/50 & 1/50 & 6.76 & 11 \\ Gemini 1.5 Flash 8b 0827  & 0/50 & 2/50 & 26.76 & 386 \\ Gemma 2 9B  & 0/50 & 2/50 & 8.82 & 70 \\   

Table 1: Model Accuracy Comparison

### Response Complexity

To determine how well the models employ Occam's Razor, we explore different metrics to gauge whether the models find the simplest rule that covers the examples. From existing Bayesian models of cognition [32; 33], the _size principle_ uses hypothesis size as a measure of simplicity. In these Bayesian models, hypothesis size is calculated as the number of values that match the hypothesis. Calculating hypothesis size in this manner is only possible when the test values are within a limited countable range. In our case, the possible test values are infinite, requiring some alternative metrics to gauge hypothesis size. We use three metrics:

1. **Number of Operators**: We count the number of operators used in the rule expression.
2. **Response Length**: We calculate the string length of the rule expression. The longer the length, the more complex it is likely to be. As longer outputs tend to be arbitrarily preferred by automatic evaluators , it is particularly important to measure the brevity of the response for cases in which simplicity is desired.
3. **Set Inclusion**: We generate a grid of integer-float tuples and apply them to guessed and actual rules to generate sets of tuples returning "True". If the set of the guessed rule is a subset or superset of the actual rule, we then calculate their set size ratio. A ratio of \(1\) is ideal, \(>1\) suggests a less complex guess, and \(<1\) a more complex one.

Appendix A.3 shows the complexity metrics of the LLMs. Most LLMs with high accuracy have long response lengths and many operators, with some exceptions.

## 5 Discussion

In our experiments, we show that LLMs struggle substantially with this task. Specifically, their propensity to repeat test cases, propose useless test cases, and guess very unlikely rules harms their performance on this task substantially. The varying performance in a multi-turn setting represents a previously underappreciated dimension of measuring reasoning capability in LLMs. There is much work in language modeling for code-based agents  and LLM-driven unit testing , and the difficulty of LLMs to explore edge cases effectively has substantial implications on those applications.

With this work, we aim to provide a benchmark that measures a model's capacity for _exploring an environment_ and reasoning based on its own decisions across multiple turns. We believe that this paradigm offers a more direct measurement of reasoning capability compared to other benchmarks. By focusing specifically on how LLMs handle multi-turn reasoning, we better understand their most common real-world applications.

Figure 1: Models can succeed upon this task by reducing the hypothesis space quickly or providing useful tests for many turns. We show that models with strong reasoning capabilities can narrow the space quickly, but weaker multi-turn capability harms their ability to get value out of later tests.