# Mirror and Preconditioned Gradient Descent in Wasserstein Space

Clement Bonet

CREST, ENSAE, IP Paris

clement.bonet@ensae.fr

&Theo Uscidda

CREST, ENSAE, IP Paris

theo.uscidda@ensae.fr

&Adam David

Institute of Mathematics

Technische Universitat Berlin

david@math.tu-berlin.de

Pierre-Cyril Aubin-Frankowski

TU Wien

pierre-cyril.aubin@tuwien.ac.at

&Anna Korba

CREST, ENSAE, IP Paris

anna.korba@ensae.fr

###### Abstract

As the problem of minimizing functionals on the Wasserstein space encompasses many applications in machine learning, different optimization algorithms on \(^{d}\) have received their counterpart analog on the Wasserstein space. We focus here on lifting two explicit algorithms: mirror descent and preconditioned gradient descent. These algorithms have been introduced to better capture the geometry of the function to minimize and are provably convergent under appropriate (namely relative) smoothness and convexity conditions. Adapting these notions to the Wasserstein space, we prove guarantees of convergence of some Wasserstein-gradient-based discrete-time schemes for new pairings of objective functionals and regularizers. The difficulty here is to carefully select along which curves the functionals should be smooth and convex. We illustrate the advantages of adapting the geometry induced by the regularizer on ill-conditioned optimization tasks, and showcase the improvement of choosing different discrepancies and geometries in a computational biology task of aligning single-cells.

## 1 Introduction

Minimizing functionals on the space of probability distributions has become ubiquitous in machine learning for _e.g._ sampling [13; 129], generative modeling [53; 86], learning neural networks [36; 90; 107], dataset transformation [4; 63], or modeling population dynamics [23; 120]. It is a challenging task as it is an infinite-dimensional problem. Wasserstein gradient flows  provide an elegant way to solve such problems on the Wasserstein space, _i.e._, the space of probability distributions with bounded second moment, equipped with the Wasserstein-2 distance from optimal transport (OT). These flows provide continuous paths of distributions decreasing the objective functional and can be seen as analog to Euclidean gradient flows . Their implicit time discretization, referred to as the JKO scheme , has been studied in depth [1; 26; 95; 111]. In contrast, explicit schemes, despite being easier to implement, have been less investigated. Most previous works focus on the optimization of a specific objective functional with a time-discrication of its gradient flow with the Wasserstein-2 metrics. For instance, the forward Euler discretization leads to the Wasserstein gradient descent. The latter takes the form of gradient descent (GD) on the position of particles for functionals with a closed-form over discrete measures, _e.g._ Maximum Mean Discrepancy (MMD), which can be of interest to train neural networks [7; 30]. For objectives involving absolutely continuous measures, such as the Kullback-Leibler (KL) divergence for sampling, other discretizations can be easily computed such as the Unadjusted Langevin Algorithm (ULA) . This leaves the question open of assessingthe theoretical and empirical performance of other optimization algorithms relying on alternative geometries and time-discretizations.

In the optimization community, a recent line of works has focused on extending the methods and convergence theory beyond the Euclidean setting by using more general costs for the gradient descent scheme . For instance, mirror descent (MD), originally introduced by Nemirovskij and Yudin  to solve constrained convex problems, uses a cost that is a divergence defined by a Bregman potential . Mirror descent benefits from convergence guarantees for objective functions that are relatively smooth in the geometry induced by the (Bregman) divergence , even if they do not have a Lipschitz gradient, _i.e._, are not smooth in the Euclidean sense. More recently, a closely related scheme, namely preconditioned gradient descent, was introduced in . It can be seen as a dual version of the mirror descent algorithm, where the role of the objective function and Bregman potential are exchanged. In particular, its convergence guarantees can be obtained under relative smoothness and convexity of the Fenchel transform of the potential, with respect to the objective. This algorithm appears more efficient to minimize the gradient magnitude than mirror descent . The flexible choice of the Bregman divergence used by these two schemes enables to design or discover geometries that are potentially more efficient.

Mirror descent has already attracted attention in the sampling community, and some popular algorithms have been extended in this direction. For instance, ULA was adapted into the Mirror Langevin algorithm . Other sampling algorithms have received their counterpart mirror versions such as the Metropolis Adjusted Langevin Algorithm , diffusion models , Stein Variational Gradient Descent (SVGD) , or even Wasserstein gradient descent . Preconditioned Wasserstein gradient descent has been also recently proposed for specific geometries in  to minimize the KL in a more efficient way, but without an analysis in discrete time. All the previous references focus on optimizing the KL as an objective, while Wasserstein gradient flows have been studied in machine learning for different functionals such as more general \(f\)-divergences , interaction energies , MMDs  or Sliced-Wasserstein (SW) distances . In this work, we propose to bridge this gap by providing a general convergence theory of both mirror and preconditioned gradient descent schemes for general target functionals, and investigate as well empirical benefits of alternative transport geometries for optimizing functionals on the Wasserstein space. We emphasize that the latter is different from , wherein mirror descent is defined in the Radon space of probability distributions, using the flat geometry defined by TV or \(L^{2}\) norms on measures, see Appendix A for more details.

Contributions.We are interested in minimizing a functional \(:_{2}(^{d})\{+\}\) over probability distributions, through schemes of the form, for \(>0\), \(k 0\),

\[_{k+1}=*{argmin}_{ L^{2}(_{k})}\ _{_{2}}(_{k}),- _{L^{2}(_{k})}+(,), _{k+1}=(_{k+1})_{\#}_{k},\] (1)

with different costs \(:L^{2}(_{k}) L^{2}(_{k})_{+}\), and in providing convergence conditions. While we can recover a map \(}=_{k}_{k-1}_{1}\) such that \(_{k}=_{\#}_{0}\), the scheme (1) proceeds by successive regularized linearizations retaining the Wasserstein structure, since the tangent space to \(_{2}(^{d})\) at \(\) is a subset of \(L^{2}()\). This paper is organized as follows. In Section 2, we provide some background on Bregman divergences, as well as on differentiability and convexity over the Wasserstein space. In Section 3, we consider Bregman divergences on \(L^{2}()\) for the cost in (1), generalizing the mirror descent scheme to the Wasserstein space. We study this new scheme by discussing its implementation, and proving its convergence under relative smoothness and convexity assumptions. In Section 4, we consider alternative costs in (1), that are analogous to OT distances with translation-invariant cost, extending the dual space preconditioning scheme to the latter space. Finally, in Section 5, we apply the two schemes to different objective functionals, including standard free energy functionals such as interaction energies and KL divergence, but also to Sinkhorn divergences  or SW  with polynomial preconditioners on single-cell datasets.

Notations.Consider the set \(_{2}(^{d})\) of probability measures \(\) on \(^{d}\) with finite second moment and \(_{2,}(^{d})_{2}(^{d})\) its subset of absolutely continuous probability measures with respect to the Lebesgue measure. For any \(_{2}(^{d})\), we denote by \(L^{2}()\) the Hilbert space of functions \(f:^{d}^{d}\) such that \(\|f\|^{2}<\) equipped with the norm \(\|\|_{L^{2}()}\) and inner product \(,_{L^{2}()}\). For a Hilbert space \(X\), the Fenchel transform of \(f:X\) is \(f^{*}(y)=_{x X}\  x,y-f(x)\). Given a measurable map \(:^{d}^{d}\) and \(_{2}(^{d})\), \(_{\#}\) is the pushforward measure of \(\) by T; and \(=(-x)(x)\). For \(,_{2}(^{d})\), the Wasserstein-2 distance is \(_{2}^{2}(,)=_{(,)}\|x-y\|^{2}\; (x,y)\), where \((,)=\{(^{d}^{d}),\; _{\#}^{}=,\;_{\#}^{}=\}\) with \(^{i}(x_{1},x_{2})=x_{i}\), is the set of couplings between \(\) and \(\), and we denote by \(_{}(,)\) the set of optimal couplings. When the optimal coupling is of the form \(=(,_{}^{})_{\#}\) with \(:x x\) and \(_{}^{} L^{2}()\) satisfying \((_{}^{})_{\#}=\), we call \(_{}^{}\) the OT map. We refer to the metric space \((_{2}(^{d}),_{2})\) as the Wasserstein space. We note \(S_{d}^{++}()\) the space of symmetric positive definite matrices, and for \(x^{d}\), \( S_{d}^{++}()\), \(\|x\|_{}^{2}=x^{T} x\).

## 2 Background

In this section, we fix \(_{2}(^{d})\) and introduce first the Bregman divergence on \(L^{2}()\) along with the notions of relative convexity and smoothness that will be crucial in the analysis of the optimization schemes. Then, we introduce the differential structure and computation rules for differentiating a functional \(:_{2}(^{d})\) along curves and discuss notions of convexity on \(_{2}(^{d})\). We refer the reader to Appendix B and Appendix C for more details on \(L^{2}()\) and the Wasserstein space respectively. Finally, we introduce the mirror descent and preconditioned gradient descent on \(^{d}\).

Bregman divergence on \(L^{2}()\).Frigyik et al. (2016, Definition 2.1) defined the Bregman divergence of Frechet differentiable functionals. In our case, we only need Gateaux differentiability. In this paper, \(\) refers to the Gateaux differential, which coincides with the Frechet derivative if the latter exists.

**Definition 1**.: _Let \(_{}:L^{2}()\) be convex and continuously Gateaux differentiable. The Bregman divergence is defined for all \(, L^{2}()\) as \(_{_{}}(,)=_{}()-_{ }()-_{}(),- _{L^{2}()}\)._

We use the same definition on \(^{d}\). The map \(_{}\) (respectively \(_{}\)) in the definition of \(_{_{}}\) above is referred to as the Bregman potential (respectively mirror map). If \(_{}\) is strictly convex, then \(_{_{}}\) is a valid Bregman divergence, _i.e._ it is positive and separates maps \(\)-almost everywhere (a.e.). In particular, for \(_{}()=\|\|_{L^{2}()}^{2}\), we recover the \(L^{2}\) norm as a divergence \(_{_{}}(,)=\|- \|_{L^{2}()}^{2}\). Bregman divergences have received a lot of attention as they allow to define provably convergent schemes for functions which are not smooth in the standard (_e.g._ Euclidean) sense [11; 88], and thus for which gradient descent is not appropriate. These guarantees rely on the notion of relative smoothness and relative convexity [88; 89], which we introduce now on \(L^{2}()\).

**Definition 2** (Relative smoothness and convexity).: _Let \(_{},_{}:L^{2}()\) be convex and continuously Gateaux differentiable. We say that \(_{}\) is \(\)-smooth (respectively \(\)-convex) relative to \(_{}\) if and only if for all \(, L^{2}(),\;_{_{}}(, )_{_{}}(,)\) (respectively \(_{_{}}(,)_{_{}} (,)\))._

Similarly to the Euclidean case , relative smoothness and convexity are equivalent to respectively \(_{}-_{}\) and \(_{}-_{}\) being convex (see Appendix B.2). Yet, proving the convergence of (1) requires only that these properties hold at specific functions (directions), a fact we will soon exploit.

In some situations, we need the \(L^{2}\) Fenchel transform \(_{}^{*}\) of \(_{}\) to be differentiable, _e.g._ to compute its Bregman divergence \(_{_{}^{*}}\). We show in Lemma 18 that a sufficient condition to satisfy this property is for \(_{}\) to be strictly convex, lower semicontinuous and superlinear, _i.e._\(_{\|\|}_{}()/\|\|_{L^{2}()}=+\). Moreover, in this case, \((_{})^{-1}=_{}^{*}\). When needed, we will suppose that \(_{}\) satisfies this assumption.

Differentiability on \((_{2}(^{d}),_{2})\).Let \(:_{2}(^{d})\{+\}\), and denote \(D()=\{_{2}(^{d}),\;()<+\}\) the domain of \(\) and \(D(}_{})=\{ L^{2}(),\;_{\#}  D()\}\) the domain of \(}_{}\) defined as \(}_{}():=(_{\#})\) for all \( L^{2}()\). In the following, we use the differential structure of \((_{2}(^{d}),_{2})\) introduced in [17; Definition 2.8], and we say that \(_{_{2}}()\) is a Wasserstein gradient of \(\) at \( D()\) if for any \(_{2}(^{d})\) and any optimal coupling \(_{o}(,)\),

\[()=()+_{_{2}}( )(x),y-x\;(x,y)+o_{2}(,) .\] (2)

If such a gradient exists, then we say that \(\) is Wasserstein differentiable at \(\)[17; 74]. Moreover there is a unique gradient belonging to the tangent space of \(_{2}(^{d})\) verifying (2) [74, Proposition2.5], and we will always restrict ourselves without loss of generality to this particular gradient, see Appendix C.1. The differentiability of \(\) and \(}_{}\) are very related, as described in the following proposition.

**Proposition 1**.: _Let \(:_{2}(^{d})\{+\}\) be a Wasserstein differentiable functional on \(D()\). Let \(_{2}(^{d})\) and \(}_{}()=(_{\#})\) for all \( D(}_{})\). Then, \(}_{}\) is Frechet differentiable, and for all \( D(}_{})\), \(}_{}()=_{_{2}}(_{\#})\)._

The Wasserstein differentiable functionals include \(c\)-Wasserstein costs on \(_{2,}(^{d})\)[74, Proposition 2.10 and 2.11], potential energies \(()= V\) or interaction energies \(()= W(x-y)\ (x)(y)\) for \(V:^{d}\) and \(W:^{d}\) differentiable and with bounded Hessian [74, Section 2.4]. In particular, their Wasserstein gradients read as \(_{_{2}}()= V\) and \(_{_{2}}()= W\). However, entropy functionals, _e.g._ the negative entropy defined as \(()=(x)(x)\) for distributions \(\) admitting a density \(\)_w.r.t._ the Lebesgue measure, are not Wasserstein differentiable. In this case, we can consider subgradients \(_{_{2}}()\) at \(\) for which (2) becomes an inequality. To guarantee that the Wasserstein subgradient is not empty, we need \(\) to satisfy some Sobolev regularity, see _e.g._[5, Theorem 10.4.13] or . Then, if \( L^{2}()\), the only subgradient of \(\) in the tangent space is \(_{_{2}}()=\), see [5, Theorem 10.4.17] and [47, Proposition 4.3]. Then, free energies are functionals that write as sums of potential, interaction and entropy terms [110, Chapter 7]. It is notably the case for the KL to a fixed target distribution, that is the sum of a potential and entropy term , or the MMD as a sum of a potential and interaction term .

Examples of functionals.The definitions of Bregman divergences on \(L^{2}()\) and of Wasserstein differentiability enable us to consider alternative Bregman potentials than the \(L^{2}()\)-norm mentioned above. For instance, for \(V\) convex, differentiable and \(L\)-smooth, we can use potential energies \(_{}^{V}():=(_{\#})\), for which \(_{_{}^{V}}(,)=_{V} (x),(x)(x)\) where \(_{V}\) is the Bregman divergence of \(V\) on \(^{d}\). Notice that \(_{}()=\|\|_{L^{2}()}^{2}\) is a specific example of a potential energy where \(V=\|\|^{2}\). Moreover, we will consider interaction energies \(_{}^{W}():=(_{\#})\) with \(W\) convex, differentiable, \(L\)-smooth, and satisfying \(W(-x)=W(x)\); for which \(_{_{}^{W}}(,)=_{W}(x)-(x^{}),(x)-(x^{ })(x)(x^{})\) (see Appendix I.3). We will also use \(_{}^{}()=(_{\#})\) with \(\) the negative entropy. Note that Bregman divergences on the Wasserstein space using these functionals were proposed by Li , but only for \(=\) and optimal transport maps \(\).

Convexity and smoothness in \((_{2}(^{d}),_{2})\).In order to study the convergence of gradient flows and their discrete-time counterparts, it is important to have suitable notions of convexity and smoothness. On \((_{2}(^{d}),_{2})\), different such notions have been proposed based on specific choices of curves. The most popular one is to require the functional \(\) to be \(\)-convex along geodesics (see Definition 10), which are of the form \(_{t}=(1-t)+t_{_{0}}^{_{1}})_{\#}_{0}\) if \(_{0}_{2,}(^{d})\) and \(_{1}_{2}(^{d})\), with \(_{_{0}}^{_{1}}\) the OT map between them. In that setting,

\[_{2}^{2}(_{0},_{1})=\|_{_{0}}^{_{1}}-\|_{L^{2}(_{0})}^{2}(_{1})- (_{0})-_{_{2}}(_{0}),_{_{0}}^{_{1}}-_{L^{2}(_{0})}.\] (3)

For instance, free energies such as potential or interaction energies with convex \(V\) or \(W\), or the negative entropy, are convex along geodesics [110, Section 7.3]. However, some popular functionals, such as the Wasserstein-2 distance \(_{2}^{2}(,)\) itself, for a given \(_{2}(^{d})\), are not convex along geodesics. Instead Ambrosio et al. [5, Theorem 4.0.4] showed that it was sufficient for the convergence of the gradient flow to be convex along other curves, _e.g._ along particular generalized geodesics for the Wasserstein-2 distance [5, Lemma 9.2.7], which, for \(,_{2}(^{d})\), are of the form \(_{t}=(1-t)_{}^{}+t_{}^{}_{ \#}\) for \(_{}^{}\), \(T_{}^{}\) OT maps from \(\) to \(\) and \(\). Observing that for \(_{}()=\|\|_{L^{2}()}^{2}\), we can rewrite (3) as \(_{_{_{0}}}(_{_{0}}^{_{1}},) _{}_{_{0}}}(_{_{0}}^{_{1}}, )\) and see that being convex along geodesics boils down to being convex in the \(L^{2}\) sense for \(=\) and \(\) chosen as an OT map. This observation motivates us to consider a more refined notion of convexity along curves.

**Definition 3**.: _Let \(_{2}(^{d})\), \(, L^{2}()\) and for all \(t\), \(_{t}=(_{t})_{\#}\) with \(_{t}=(1-t)+t\). We say that \(:_{2}(^{d})\) is \(\)-convex (resp. \(\)-smooth) relative to \(:_{2}(^{d})\) along \(t_{t}\) if for all \(s,t\), \(_{}_{}}(_{s},_{t}) _{}_{}}(_{s},_{t})\) (resp. \(_{}_{}}(_{s},_{t}) _{}_{}}(_{s},_{t})\))._Notice that in contrast with Definition 2, Definition 3 is stated for a fixed distribution \(\) and directions (\(,\)), and involves comparisons between Bregman divergences depending on \(\) and curves \((_{s})_{s}\) depending on \(,\). The larger family of \(\) and \(\) for which Definition 3 holds, the more restricted is the notion of convexity of \(-\) (resp. of \(-\)) on \(_{2}(^{d})\). For instance, Wasserstein-2 generalized geodesics with anchor \(_{2}(^{d})\) correspond to considering \(,\) as all the OT maps originating from \(\), among which geodesics are particular cases when taking \(=\) (hence \(=\)). If we furthermore ask for \(\)-convexity to hold for all \(_{2}(^{d})\) and \(, L^{2}()\) (_i.e._, not only OT maps), then we recover the convexity along acceleration free-curves as introduced in [28; 98; 117]. Our motivation behind introducing Definition 3 is that the convergence proofs of MD and preconditioned GD require relative smoothness and convexity properties to hold only along specific curves.

Mirror descent and preconditioned gradient descent on \(^{d}\).These schemes read respectively as \((x_{k+1})-(x_{k})=- f(x_{k})\) and \(y_{k+1}-y_{k}=- h^{*} g(y_{k})\), where the objectives \(f,g\) and the regularizers \(h,\) are convex \(C^{1}\) functions from \(^{d}\) to \(\). The algorithms are closely related since, using the Fenchel transform and setting \(g=^{*}\) and \(h^{*}=f\), we see that, for \(y=(x)\), the two schemes are equivalent when permuting the roles of the objective and of the regularizer. For MD, convergence of \(f\) is ensured if \(f\) is both \(}{{}}\)-smooth and \(\)-convex relative to \(\)[88, Theorem 3.1]. Concerning preconditioned GD, assuming that \(h,g\) are Legendre, \(g(y_{k})_{k}\) converges to the minimum of \(g\) if \(h^{*}\) is both \(}{{}}\)-smooth and \(\)-convex relative to \(g^{*}\) with \(>0\)[89, Theorem 3.9].

## 3 Mirror descent

For every \(_{2}(^{d})\), let \(_{}:L^{2}()\) be strictly convex, proper and differentiable and assume that the (sub)gradient \(_{_{2}}() L^{2}()\) exists. In this section, we are interested in analyzing the scheme (1) where the cost \(\) is chosen as a Bregman divergence, _i.e._\(_{_{}}\) as defined in Definition 1. This corresponds to a mirror descent scheme in \(_{2}(^{d})\). For \(>0\) and \(k 0\), it writes:

\[_{k+1}=*{argmin}_{ L^{2}(_{k})}_{_{_{k}}}(,)+_{_{2}} (_{k}),-_{L^{2}(_{k})}, _{k+1}=(_{k+1})_{\#}_{k}.\] (4)

Iterates of mirror descent.In all that follows, we assume that the iterates (4) exist, which is true _e.g._ for a superlinear \(_{_{k}}\), since the objective is a sum of linear functions and of the continuous \(_{_{k}}\). In the previous section, we have seen that the second term in the proximal scheme (4) can be interpreted as a linearization of the functional \(\) at \(_{k}\) for Wasserstein (sub)differentiable functionals. Now define for all \( L^{2}(_{k})\), \(()=_{_{_{k}}}(,)+ _{_{2}}(_{k}),- _{L^{2}(_{k})}\). Then, deriving the first order conditions of (4) as \((_{k+1})=0\), we obtain \(_{k}\)-a.e.,

\[_{_{k}}(_{k+1})=_{_{k}}()- _{_{2}}(_{k})_{k +1}=_{_{k}}^{*}_{_{k}}()- _{_{2}}(_{k}).\] (5)

Note that for \(_{}()=\|\|_{L^{2}()}^{2}\), the update (5) translates as \(_{k+1}=-_{_{2}}(_{k})\), and our scheme recovers Wasserstein gradient descent [35; 91]. This is analogous to mirror descent recovering GD when the Bregman potential is chosen as the Euclidean squared norm in \(^{d}\). We discuss in Appendix D.2 the continuous formulation of (4), showing it coincides with the gradient flow of the mirror Langevin [3; 130], the limit of the JKO scheme with Bregman groundcosts , Information Newton's flows , or Sinkhorn's flow  for specific choices of \(\) and \(\).

Our proof of convergence of the mirror descent algorithm will require the Bregman divergence to satisfy the following property, which is reminiscent of conditions of optimality for couplings in OT.

**Assumption 1**.: _For \(,_{2,}(^{d})\) and \(_{2}(^{d})\), setting \(_{_{}}^{,}=*{argmin}_{_{\#}= }\ _{_{}}(,)\), \(_{_{}}^{,}=*{argmin}_{_{\#}= }\ _{_{}}(,)\), the functional \(_{}\) is such that, for any \( L^{2}()\) satisfying \(_{\#}=\), we have \(_{_{}}(_{_{}}^{,},)_{_{}}(_{_{}}^{,},)\)._

The inequality in Assumption 1 can be interpreted as follows: the "distance" between \(\) and \(\) is greater when observed from an anchor \(\) that differs from \(\) and \(\). We demonstrate that Bregman divergences satisfy this assumption under the following conditions on the Bregman potential \(\).

**Proposition 2**.: _Let \(,_{2,}(^{d})\) and \(_{2}(^{d})\). Let \(_{}\) be a pushforward compatible functional, i.e. there exists \(:_{2}(^{d})\) such that for all \( L^{2}()\), \(_{}()=(_{\#})\). Assume furthermore \(_{_{2}}()\) and \(_{_{2}}()\) invertible (on \(^{d}\)). Then, \(_{}\) satisfies Assumption 1._All the maps \(_{}^{V}\), \(_{}^{W}\) and \(_{}^{}\) defined in Section 2 satisfy the assumptions of Proposition 2 under mild requirements, see Appendix D.1. The proof of Proposition 2 is given in Appendix H.2. It relies on the definition of an appropriate optimal transport problem

\[_{}(,)=_{(,)}\ ()-()- _{_{2}}()(y),x-y\ (x,y),\] (6)

and on the proof of existence of OT maps for absolutely continuous measures (see Proposition 15), which implies \(_{}(,)=_{_{}}(^{,}_{_{ }},)\) with \(^{,}_{_{}}\) defined as in Assumption 1. From there, we can conclude that \(_{}\) satisfies Assumption 1. We notice that the corresponding transport problem recovers previously considered objects such as OT problems with Bregman divergence costs , but is strictly more general (as our results pertain to the existence of OT maps), as detailed in Appendix D.1.

We now analyze the convergence of the MD scheme. Under a relative smoothness condition along curves generated by \(=\) and \(=_{k+1}\) solutions of (4) for all \(k 0\), we derive the following descent lemma, which ensures that \((_{k})_{k}\) is non-increasing. Its proof can be found in Appendix H.3 and relies on the three-point inequality , which we extended to \(L^{2}()\) in Lemma 29.

**Proposition 3**.: _Let \(>0\), \(\). Assume for all \(k 0\), \(\) is \(\)-smooth relative to \(\) along \(t(1-t)+t_{k+1}_{\#}_{k}\), which implies \(_{_{_{k}}}(_{k+1},)_{ _{_{k}}}(_{k+1},)\). Then, for all \(k 0\),_

\[(_{k+1})(_{k})-_{_ {_{k}}}(,_{k+1}).\] (7)

Assuming additionally the convexity of \(\) along the curves \(_{t}=(1-t)+t^{,}_{_{}}_{ \#}\), \(t\) and that \(\) satisfies Assumption 1, we can obtain global convergence.

**Proposition 4**.: _Let \(_{2}(^{d})\), \( 0\). Suppose Assumption 1 and the conditions of Proposition 3 hold, and that \(\) is \(\)-convex relative to \(\) along the curves \(t(1-t)+t^{_{k},}_{_{_{k}}} _{\#}_{k}\). Then, for all \(k 1\),_

\[(_{k})-()-1} _{}(,_{0})_{} (,_{0}).\] (8)

_Moreover, if \(>0\), taking \(=^{*}\) the minimizer of \(\), we obtain a linear rate: for all \(k 0\), \(_{}(^{*},_{k})(1-)^{k}\,_{}( ^{*},_{0})\)._

The proof of Proposition 4 can be found in Appendix H.4, and requires Assumption 1 to hold so that consecutive distances between iterates and the global minimizer telescope. This is not as direct as in the proofs of  over \(^{d}\), because the minimization problem of each iteration (4) happens in a different space \(L^{2}(_{k})\). We discuss in Section 5 how to verify the relative smoothness and convexity on some examples. In particular, when both \(\) and \(\) are potential energies, it is inherited from the relative smoothness and convexity on \(^{d}\), and the conditions are similar with those for MD on \(^{d}\). We also note that relative smoothness assumptions _along descent directions_ as stated in Proposition 3 and relative strong convexity _along optimal curves between the iterates and a minimizer_ as stated in Proposition 4 have been used already in the literature of optimization over measures in very specific cases, _e.g._ for descent results for the KL along SVGD  or for Sinkhorn convergence in . We further analyze in Appendix F the convergence of Bregman proximal gradient scheme  for objectives of the form \(()=()+()\) with \(\) non smooth; which includes the KL divergence decomposed as a potential energy plus the negative entropy.

Implementation.We now discuss the practical implementation of MD on \((_{2}(^{d}),_{2})\) as written in (5). If \(_{}\) is pushforward compatible, we have \(_{_{k}}(_{k+1})=_{_{2}}( _{k+1})_{\#}_{k}_{k+1}\); but if \(_{_{k}}^{*}\) is unknown, the scheme is implicit in \(_{k+1}\). A possible solution is to rely on a root finding algorithm such as Newton's method to find the zero of \(\) at each step, which we use in Section 5 for \(_{}^{W}\) as Bregman potential. However, this procedure may be computationally costly and scale badly _w.r.t._ the dimension and the number of samples, see Appendix G.1. Nonetheless, in the special case \(_{}^{V}()= V\ \) with \(V\) differentiable, strongly convex and \(L\)-smooth, since \(_{_{2}}()= V\) and \(( V)^{-1}= V^{*}\), the scheme reads as

\[ k 0,\ _{k+1}= V^{*} V- _{_{2}}(_{k}),\] (9)and can be implemented on particles, _i.e._ for \(_{k}=_{i=1}^{n}_{x_{i}^{k}}\), \(x_{i}^{k+1}= V^{*} V(x_{i}^{k})-_{_{2}} (_{k})(x_{i}^{k})\) for all \(k 0,\ i\{1,,n\}\). This scheme is analogous to MD in \(^{d}\) and has been introduced as the mirror Wasserstein gradient descent . Moreover, for \(V=\|\|_{2}^{2}\), as observed earlier, we recover the usual Wasserstein gradient descent, _i.e._\(_{k+1}=-_{_{2}}(_{k})\). The scheme can also be implemented for Bregman potentials that are not pushforward compatible. For specific \(\), it recovers notably SVGD and its variants [83; 84; 114; 131] or the Kalman-Wasserstein gradient descent . We refer to Appendix D.4 for more details.

## 4 Preconditioned gradient descent

As seen in Section 2, preconditioned gradient descent on \(^{d}\) has dual convergence conditions compared to mirror descent. Our goal is to extend these to (1) and \(_{2}(^{d})\). Let \(>0\), \(_{2}(^{d})\) and \(h:^{d}\) proper and strictly convex on \(^{d}\). We consider in this section \(_{}^{h}()= h\,\) and \((,)=_{_{k}}^{h}(- )/= h(x-(x))/\, _{k}(x)\). This type of discrepancy is analogous to OT costs with translation-invariant ground cost \(c(x,y)=h(x-y)\), which have been popular as they induce an OT map [110; Box 1.12]. Such costs have been introduced _e.g._ in [39; 70] to promote sparse transport maps. More generally, for \(_{}\) strictly convex, proper, differentiable and superlinear, we have \((_{})^{-1}=_{}^{}\) and the following theory is still valid. For simplicity, we leave studying more general \(\) for future works. Here, the scheme (1) results in:

\[_{k+1}= L^{2}(_{k})}{}\,  h((x)}{})\,_{k}(x)+ _{_{2}}(_{k}),- _{L^{2}(_{k})},\ \ _{k+1}=(_{k+1})_{\#}_{k}.\] (10)

Deriving the first order conditions similarly to (5) in Section 3, we obtain the following update:

\[ k 0,\ _{k+1}=-(_{_{k}}^{h})^ {-1}_{_{2}}(_{k})=-  h^{}_{_{2}}(_{k}).\] (11)

Notice that for \(h=\|\|_{2}^{2}\) the squared Euclidean norm, \(_{}^{h}\) and \(_{}^{h^{}}\) recover the squared \(L^{2}()\) norm, and schemes (4) and (10) coincide. The scheme (10) is analogous to preconditioned gradient descent [68; 75; 76; 89; 119], which provides a dual alternative to mirror descent. For the latter, the goal is to find a suitable preconditioner \(h^{}\) allowing to have convergence guarantees, or to speed-up the convergence for ill-conditioned problems. It was recently considered on the Wasserstein space by Cheng et al.  and Dong et al.  with a focus on the KL divergence as objective \(\) and for \(h=\|\|_{p}^{p}\) with \(p>1\) or \(h\) quadratic . Moreover, their theoretical analysis  was mostly done using the continuous formulation. Instead we focus on deriving conditions for the convergence of the discrete-time scheme (11) for more general functionals objectives.

Convergence guarantees.Inspired by , we now provide a descent lemma on \(_{_{k}}^{h^{}}(_{_{2}}(_{k})) _{k}\) under a technical inequality between the Bregman divergences of \(_{_{k}}^{h^{}}\) and \(}_{_{k}}\) for all \(k 0\). Additionally, we also suppose that \(\) is convex along the curves generated by \(=_{k+1}\) and \(=\). This last hypothesis ensures that \(_{}_{_{k}}}(_{k+1},) 0\), and thus that \(_{_{k}}^{h^{}}(_{_{2}}(_{k})) _{k}\) is non-increasing. Analogously to the Euclidean case, \(_{}^{h^{}}\) quantifies the magnitude of the gradient, and provides a second quantifier of convergence leading to possibly different efficient methods compared to mirror descent . The proof relies mainly on the three-point identity (see _e.g._[54; Appendix B.7] or Lemma 28) and algebra with the definition of Bregman divergences.

**Proposition 5**.: _Let \(>0\). Assume \(\), and for all \(k 0\), \(\) convex along \(t(1-t)_{k+1}+t_{\#}_{k}\) and \(_{_{k}^{}}^{h^{}}_{_{2}}(_{k +1})_{k+1},_{_{2}}(_{k}) _{}_{_{k}}}(,_{k+1})\). Then, for all \(k 0\),_

\[_{_{k+1}}^{h^{}}_{_{2}}(_{k+1}) _{_{k}}^{h^{}}_{_{2}}( _{k})-_{}_{_{k}}}( _{k+1},).\] (12)

Under an additional assumption of a reverse inequality between the Bregman divergences of \(_{_{k}}^{h^{}}\) and \(}_{_{k}}\), and assuming that \(_{}^{h^{}}\) attains its minimum in 0, we can show the convergence of the gradient quantified by \(_{}^{h^{}}\) (see Lemma 21), and the convergence of \((_{k})_{k}\) towards the minimum of \(\).

**Proposition 6**.: _Let \( 0\) and \(^{}_{2}(^{d})\) be the minimizer of \(\). Assume the conditions of Proposition 5 hold, and that for \(}=_{,_{\#}_{k}=^{}}\, _{}_{_{k}}}(,),\ \ _{}_{_{k}}}(,}) _{}_{_{k}}}(,})\)._\[_{}_{}}(,)_ {^{*}_{}}_{_{2}}(_{\#}) ,_{_{2}}().\] (15)

In particular, for \(\) a potential energy, the conditions coincide with those of  in \(^{d}\). We refer to Appendix E.1 for more details.

## 5 Applications and Experiments

In this section, we first discuss how to verify the relative convexity and smoothness between functionals in practice. Then, we provide some examples of mirror descent and preconditioned gradient descent on different objectives. We refer to Appendix G for more details on the experiments1.

Relative convexity of functionals.To assess relative convexity or smoothness as stated in Definition 3, we need to compare the Bregman divergences along the right curves. When both functionals \(\) and \(\) are of the same type, for example potential (respectively interaction) energies, this property is lifted from the convexity and smoothness on \(^{d}\) of the underlying potential functions (respectively interaction kernels) to \(_{2}(^{d})\), see Appendix E.2 for more details. When both \(\) and \(\) are potential energies, the schemes (4) and (10) are equivalent to parallel MD and preconditioned GD since there are no interactions between the particles. The conditions of convergence then coincide with the ones obtained for MD and preconditioned GD on \(^{d}\)[88; 89]. In other cases, (4) and (10) provide schemes that are novel to the best of our knowledge.

For functionals which are not of the same type, it is less straightforward. Using equivalent notions of convexity (see Proposition 13), we may instead compare their Hessians along the right curves,see Appendix E.2 for an example between an interaction and a potential energy. We note also that for the particular case of a functional obtained as a sum \(=+\) with \(}_{}\) and \(}_{}\) convex, since \(_{}_{}}=_{}_{}}+ _{}_{}}\), \(_{}_{}}\{_{}_{}},_{}_{}}\}\), and thus \(\) is 1-convex relative to \(\) and \(\). This includes _e.g._ the KL divergence which is convex relative to the potential and the negative entropy.

MD on interaction energies.We first focus on minimizing interaction energies \(()= W(x-y)\ (x)(y)\) with kernel \(W(z)=\|z\|_{^{-1}}^{4}-\|z\|_{^{-1}}^{2}\), \( S_{d}^{++}()\), whose minimizer is an ellipsoid . Since the Hessian norm of \(W\) can be bounded by a polynomial of degree 2, following [88, Section 2], \(W\) is \(\)-smooth relative to \(K_{4}(z)=\|z\|_{2}^{4}+\|z\|_{2}^{2}\) with \(=4\), and \(\) is \(\)-smooth relative to \(_{}()= K_{4}(x)-( y)\ (x)(y)\). Supposing additionally that the distributions are compactly supported, we can show that \(\) is smooth relative to the interaction energy with \(K_{2}(z)=\|z\|_{2}^{2}\). For ill-conditioned \(\), _i.e._ for which the ratio between the largest and smallest eigenvalues is large, the convergence can be slow. Thus, we also propose to use \(K_{2}^{}(z)=\|z\|_{^{-1}}^{2}\) and \(K_{4}^{}(z)=\|z\|_{^{-1}}^{4}+\|z\|_{^ {-1}}^{2}\). We illustrate these mirror descent schemes on Figure 1 and observe the convergence we expect for the ones taking into account \(\). In practice, since \(_{}()=( K_{\#})\), the scheme (5) needs to be approximated using Newton's algorithm which can be computationally heavy. Using \(_{}^{V}()= V\ \) with \(V=K_{2}^{}\), we obtain a more computationally friendly scheme with the same convergence, see Appendix G.2, but for which the smoothness is trickier to show.

MD on KL.We now focus on minimizing \(()= V+()\) for \(V(x)=x^{T}^{-1}x\) with \(\) possibly ill-conditioned, whose minimizer is the Gaussian \(=(0,)\), and for which Wasserstein gradient descent is slow to converge. We study the MD scheme in (4) with negative entropy \(\) as the Bregman potential (NEM), and compare it on Figure 2 with the Forward-Backward (FB) scheme studied in  and the ideally preconditioned Forward-Backward scheme (PEB) with Bregman potential \(_{}^{}\) (see (116) in Appendix F). For computational purpose, we restrain the minimization in (4) over affine maps, which can be seen as taking the gradient over the submanifold of Gaussians . Starting from \((0,_{0})\), the distributions stay Gaussian over the flow, and their closed-form is reported in (62) (Appendix D.3). We note that this might not be the case for the scheme (4), and thus that this scheme does not enter into the framework developed in the previous sections. Nonetheless, it demonstrates the benefits of using different Bregman potentials. We generate 20 Gaussian targets \(\) on \(^{10}\) with \(=UDU^{T}\), \(D\) diagonal and scaled in log space between 1 and 100, and \(U\) a uniformly sampled orthogonal matrix, and we report the averaged KL over time. Surprisingly, NEM, which does not require an ideal (and not available in general) preconditioner, is almost as fast to converge as the ideal PFB, and much faster than the FB scheme.

Preconditioned GD for single-cells.Predicting the response of cells to a perturbation is a central question in biology. In this context, as the measuring process is destructive, feature descriptions of control and treated cells must be dealt with as (unpaired) source \(\) and target distributions \(\). Following , OT theory to recover a mapping \(\) between these two populations has been used in . Inspired by the recent success of iterative refinement in generative modeling, through diffusion  or flow-based models , our scheme (1) follows the idea of transporting \(\) to \(\) via successive and dynamic displacements instead of, directly, with a static map \(}\). We model the transition from unperturbed to perturbed states through the (preconditioned)gradient flow of a functional \(()=D(,)\) initialized at \(_{0}=\), where \(D\) is a distributional metric, and predict the perturbed population via \(=_{}()\). We focus on the datasets used in , consisting of cell lines analyzed using (i) 4i , and (ii) scRNA sequencing . For each profiling technology, the response to respectively (i) 34 and (ii) 9 treatments are provided. As in , training is performed in data space for the 4i data and in a latent space learned by the scGen autoencoder  for the scRNA data. We use three metrics: the Sliced-Wasserstein distance \(_{2}^{2}\), the Sinkhorn divergence \(_{z,2}^{2}\) and the energy distance \(\)[59; 60; 105], and we compare the performances when minimizing this functional via preconditioned GD vs. (vanilla) GD. We measure the convergence speed when using a fixed relative tolerance \(=10^{-3}\), as well as the attained optimal value \(()\). Note that we follow  and additionally consider 40% of unseen (test) target cells for evaluation, _i.e._, for computing \(()=D(,)\). As preconditioner, we use the one induced by \(h^{*}(x)=(\|x\|_{2}^{2}+1)^{1/a}-1\) with \(a>0\), which is well suited to minimize functionals which grow in \(\|x-x^{*}\|^{a/(a-1)}\) near their minimum . We set the step size \(=1\) for all the experiments. Then, we tune the parameter \(a\) very simply: for a given metric \(D\) and a profiling technology, we pick a random treatment and select \(a\{1.25,1.5,1.75\}\) by grid search, and we generalize the selected \(a\) for _all the other treatments_. Results are described in Figure 3: Preconditioned GD significantly outperforms GD over the 43 datasets, in terms of convergence speed and optimal value \(()\). For instance, for \(D=_{2,}^{2}\), we converge in 10 times less iterations while providing, on average, a better estimate of the treated population. We also compare our iterative (non parametric) approach with the use of a static (non parametric) map in Appendix G.4.

## 6 Conclusion

In this work, we extended two non-Euclidean optimization methods on \(^{d}\) to the Wasserstein space, generalizing \(_{2}\)-gradient descent to alternative geometries. We investigated the practical benefits of these schemes, and provided rates of convergences for pairs of objectives and Bregman potentials satisfying assumptions of relative smoothness and convexity along specific curves. While these assumptions can be easily checked is some cases (_e.g._ potential or interaction energies) by comparing the Bregman divergences or Hessian operators in the Wasserstein geometry, they may be hard to verify in general. Different objectives such as the Sliced-Wasserstein distance or the Sinkhorn divergence, or alternative geometries to the Wasserstein-2 as studied in this work, require to derive specific computations on a case-by-case basis. We leave this investigation for future work.

Figure 3: Preconditioned GD vs. (vanilla) GD to predict the responses of cell populations to cancer treatment on 4i (**Upper row**) and scRNAeq (**Lower row**) datasets. For each treatment, starting from the untreated cells \(_{i}\), we minimize \(()=D(,_{i})\) with \(_{i}\) the treated cells. The plot is organized as pairs of columns, each corresponding to optimizing a specific metric, with two scatter plots displaying points \(z_{i}=(x_{i},y_{i})\) where **(First column)**\(y_{i}\) is the attained minima \(()=D(,_{i})\) with preconditioning and \(x_{i}\) that without preconditioning, and **(Second column)**\(y_{i}\) is the number of iterations to reach convergence with preconditioning and \(x_{i}\) that without preconditioning. A point below the diagonal \(y=x\) then refers to an experiment in which preconditioning provides **(First column)** a better minima or **(Second column)** faster convergence. We assign a color to each treatment and plot three runs, obtained with three different initializations, along with their mean (brighter point).