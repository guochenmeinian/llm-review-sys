# On the Impacts of the Random Initialization in the Neural Tangent Kernel Theory

Guhan Chen

Department of Statistics and Data Science

Tsinghua University

Beijing, China

chen-gh23@mails.tsinghua.edu.cn

&Yicheng Li

Department of Statistics and Data Science

Tsinghua University

Beijing, China

liyc22@mails.tsinghua.edu.cn

&Qian Lin

Department of Statistics and Data Science

Tsinghua University

Beijing, China

qianlin@tsinghua.edu.cn

Corresponding author

###### Abstract

This paper aims to discuss the impact of random initialization of neural networks in the neural tangent kernel (NTK) theory, which is ignored by most recent works in the NTK theory. It is well known that as the network's width tends to infinity, the neural network with random initialization converges to a Gaussian process \(f^{}\), which takes values in \(L^{2}()\), where \(\) is the domain of the data. In contrast, to adopt the traditional theory of kernel regression, most recent works introduced a special mirrored architecture and a mirrored (random) initialization to ensure the network's output is identically zero at initialization. Therefore, it remains a question whether the conventional setting and mirrored initialization would make wide neural networks exhibit different generalization capabilities. In this paper, we first show that the training dynamics of the gradient flow of neural networks with random initialization converge uniformly to that of the corresponding NTK regression with random initialization \(f^{}\). We then show that \((f^{}[^{}]^{s})=1\) for any \(s<\) and \((f^{}[^{}]^{s})=0\) for any \(s\), where \([^{}]^{s}\) is the real interpolation space of the RKHS \(^{}\) associated with the NTK. Consequently, the generalization error of the wide neural network trained by gradient descent is \((n^{-})\), and it still suffers from the curse of dimensionality. On one hand, the result highlights the benefits of mirror initialization. On the other hand, it implies that NTK theory may not fully explain the superior performance of neural networks.

## 1 Introduction

In recent years, the advancement of neural networks has revolutionized various domains, including computer vision, generative modeling, and others. Notably, large language models like the renowned GPT series [8; 51] have shown exceptional proficiency in language-related tasks. Similarly, neural networks have achieved significant successes in image classification, as evidenced by works such as [27; 34; 37]. This proliferation of neural networks spans a wide range of fields. Despite theseimpressive achievements, a comprehensive theoretical understanding of why neural networks perform so well remains elusive in the academic community.

Several studies have delved into the theoretical properties of neural networks. Initially, researchers were keen on exploring the expressive capacity of networks, as demonstrated in seminal works like [17; 28]. These studies established the Universal Approximation Theorem, asserting that sufficiently wide networks can approximate any continuous function. More recent research, such as [15; 26; 43] extended this exploration to the effects of deeper and wider network architectures. However, a significant challenge remains in these studies: they often do not fully explain the generalization power of neural networks, which is crucial for evaluating the performance of a statistical model.

Recently, some researchers have examined the generalization properties of networks. Bauer and Kohler , Schmidt-Hieber  showed the minimax optimality of networks with various activation functions for specific subclasses of Holder functions, within the nonparametric regression framework. In contrasts to the static ERM approach, some studies made more attention to the dynamics of neural networks, particularly those trained using gradient descent (GD) and stochastic gradient descent (SGD)[2; 13; 20].

With similar insights, Jacot et al.  explicitly introduced the Neural Tangent Kernel (NTK) concept, demonstrating that there exists a time-varying neural network kernel (NNK) which converges to a fixed deterministic kernel and remains almost invariant during training as network width approaches infinity. And thus NTK theory proposes that network training can be approximated by a kernel regression problem [4; 29; 39; 50]. As a general case, fully-connected networks directly trained by GD, Lai et al. , Li et al.  showed the generalization ability of two-layer and multi-layer networks, respectively.

This paper mainly follows [4; 35; 41], and explores the impact of initialization in the NTK theory. Prior research [35; 41] which verified the minimax optimality of network utilized the so-called _mirrored initialization_ setting. It refers to a combination of mirrored structure and mirrored initial value of parameters, which results in a zero initial output function. However, the assumption divides from the commonly used initialization strategy in real-world applications, whose initial output is actually non-zero. To bridging the gap, in this study we explore the generalization ability of standard non-zero initialized network, within the NTK theory framework. Our findings reveal that the vanilla non-zero initialization will theoretically results in poor generalization ability of network, especially when the data has relatively large dimension. If that is true, it suggests a divergence between theoretical models and real-world applications, highlighting a potential limitation in the current understanding of the NTK theory. Therefore, we arrive at a critical problem central to this study:

_Does initialization significantly impact the generalization ability of networks within the kernel regime?_

### Our contribution

\(\)_Network converges to a NTK predictor uniformly_. We show that under standard initialization, the network function converges to the NTK predictor uniformly over the entire training process and over all possible input in the domain. The convergence is essential in the study of the generalization ability of network in NTK theory. However, in previous work, the initial values of network has long been overlooked. Under mirrored initialization which leads to zero initial output function, Arora et al. , Lai et al. , Li et al.  demonstrated the point-wise convergence and the uniform convergence of network, respectively. More recently, Xu and Zhu  studied the uniform convergence of NTK under standard initialization, but did not study the convergence of the network function. Why the initial output of the network is ignored is not that it is insignificant, but rather because it is a stochastic function, making it challenging to analyze in convergence. Our findings make it valid to approximate the network's generalization ability based on the corresponding NTK predictor's performance.

\(\)_The generalization ability of standardly non-zero initialized fully-connected network_. Our research explores the impact of standard non-zero initialization in NTK theory. At this issue, Zhang et al.  proposes the existence of implicit bias induced by non-zero initialization, when the neural network is completely overfitted. We delve deeper into this argument, studies the exact formula of the bias at any stage of training, within the framework of NTK theory. Additionally, we established that the (optimally tuned) learning rate of network is \(n^{-}\), even when the regression function is sufficiently smooth. This insightful discovery implies a notable limitation in the generalization ability of networks with non-zero initialization, if NTK theory can precisely approximate the performance of real network. Consequently, we need to reconsider the weakness of NTK in the study of network theory. Also, the results show that mirrored initialization is superior to standard initialization in practical applications.

### Related works

Our research is conducted within the framework of NTK theory. This type of research, in general, can be categorized into two main steps: the approximation of the network trained by GD through a kernel regression problem, and the evaluation on the corresponding kernel regression predictor. Several studies [2; 4; 19; 31] which focused on the former step, illustrated the point-wise convergence of NTK for multi-layer ReLU networks. Additionally,  demonstrated the point-wise convergence of the kernel regression predictor to the network. Furthermore, Lai et al. , Li et al. , Xu and Zhu  demonstrated the uniform convergence result with respect to all input and all time on two-layer and multi-layer networks. As to the latter step, a few researchers have analyzed the spectral properties of the NTK [6; 7] as well as kernel regression [40; 55]. Building upon these findings, Lai et al.  and Li et al.  demonstrated that early-stopping GD induces minimax optimality of the network. It is worth noting that the setting in these works assumes mirrored initialization, which may not be well-aligned with real-world scenarios. When it comes to initialization, Zhang et al.  provided insights into the impact of initialization under kernel interpolation, which is a special case of our results at \(t=\).

## 2 Preliminaries

### Model and notations

Suppose that \(\{(x_{i},y_{i})\}_{i=1}^{n}\) are i.i.d. drawn from an unknown distribution \(\) which is given by

\[y=f^{*}(x)+,\] (1)

where \(f^{*}(x)\) is the _regression function_ and \(\) is a centered random noise. Suppose that the marginal distribution \((x)\) of the radon variable \(x\) is supported in a non-empty bounded subset \(\) of \(^{d}\) with \(C^{}\) smooth boundary. The generalization error of an estimator \(\) of \(f^{*}\) is given by excess risk

\[(;f^{*})=\|-f^{*}\|_{L_{2}(, )}^{2}.\] (2)

We introduce the following standard assumption on the noise(e.g., [21; 42]). It is clear that sub-Gaussian noise satisfying this assumption.

**Assumption 1** (Noise).: The noise term \(\) satisfies the following condition for some positive constant \(,L\), and \(m 2\):

\[(||^{m}|x)m!^{2}L^{m-2}, a.e.\,x .\] (3)

NotationsGiven a set of samples pairs \(\{(x_{i},y_{i})\}_{i=1}^{n}\), we denote \(X\) and \(Y\) to be vector \((x_{1},,x_{n})^{T}\) and \((y_{1},,y_{n})^{T}\), respectively. In a similar manner, \((f(x_{1}),,f(x_{n}))^{T}\) and \((f(y_{1}),,f(y_{n}))^{T}\) are represented as \(f(X)\) and \(f(Y)\), where \(f():^{d}\) is an arbitrary given function. Regarding a kernel function \(k(,):^{d}^{d}\), we use \(k(x,X)\) to denote the vector \((k(x,x_{1}),k(x,x_{2}),,k(x,x_{n}))\) and \(k(X,X)\) to denote the matrix \([k(x_{i},x_{j})]_{n n}\). For real number sequences such as \(\{a_{n}\}\) and \(\{b_{n}\}\), we write \(a_{n}=O(b_{n})\) (or \(a_{n}=o(b_{n})\)), if there exists absolute positive constant \(C\) such that \(|a_{n}| C|b_{n}|\) holds for any sufficiently large \(n\) (or \(|a_{n}|/|b_{n}|\) approaches zero). We also denote \(a_{n} b_{n}\) if there exists absolute positive constant \(c\) abd \(C\) such that \(c|b_{n}||a_{n}| C|b_{n}|\) holds for any sufficiently large \(n\).

### Reproducing kernel Hilbert space

Suppose that \(k\) is kernel function defined on the domain \(\) satisfying that \(\|k\|_{}^{2}\). Let \(_{k}\) be the reproducing Hilbert space associated with \(k\) which is the closure of linear span of \(\{k(x,),x\}\)under the inner product induced by \( k(x,),k(y,)=k(x,y)\). Given a distribution \((x)\) on \(\), we can introduce an integral operator \(T_{k}:L^{2}(,) L^{2}(,)\):

\[T_{k}f(x)=_{}k(x,y)f(y)\,(y).\] (4)

The celebrated Mercer's decomposition  asserts that

\[T_{k}f=_{i}_{i} f,e_{i}_{L^{2}}e_{i},  k(x,y)=_{i}_{i}e_{i}(x)e_{i}(y),\] (5)

where \(\{e_{i}\}_{i}\) and \(\{_{i}^{}e_{i}\}_{i}\) are the orthonormal basis of \(L^{2}(,)\) and \(_{k}\) respectively. It is well known that \(_{k}\) can be canonically embedded into \(L^{2}(,)\).

If the eigenvalues \(_{i}\) of \(k\) are polynomially decaying at rate \(\) ( i.e, \(_{i} i^{-}\)), we can further introduce a concept of the relative smoothness of a function \(f L^{2}(,)\). More precisely, let us recall the concept of _real interpolation space_ ( Please see more detailed information in the Appendix).

Real interpolation spaceThe real interpolation space \([_{k}]^{s}\) is given by

\[[_{k}]^{s}\{_{i}a_{i}_{i}^{ }e_{i}(x)_{i}a_{i}^{2}<\},\] (6)

with the inner product \(_{i}a_{i}_{i}^{}e_{i}(x),_{i }b_{j}_{j}^{}e_{j}(x)_{[_{k}]^ {s}}=_{i}a_{i}b_{i}\) for \(s 0\).

It is clear that \([_{k}]^{s}\) is a separable Hilbert space and is isometric to the \(l_{2}\) space. With the definition above, we can see that \([_{k}]^{0}=L^{2}(,)\) and \([_{k}]^{1}=_{k}\). Also, for any \(s_{2} s_{1} 0\), we know \([_{k}]^{s_{1}}[_{k}]^{s_{2}}\) with compact embedding. Let

\[_{0}=_{s}\{s[_{k}]^{s} C^{0}()\}\]

which is often referred to the embedding index of an RKHS \(_{k}\). It is well known that \(_{0}\) and the equality holds for a large class of usual RKHSs if the eigenvalue decay rate is \(\). We further define the relative smoothness of a given function \(f\):

**Definition 2.1** (Relative smoothness).: Given a kernel \(k\) on \(\) with respect to measure \(\), the smoothness of a function \(f\) is defined as

\[(f,k)=\{>0_{i}_{i}^{- }c_{i}^{2}<\},\] (7)

where \(c_{i}= f,e_{i}_{L^{2}(,)}\).

### Kernel gradient flow

For a positive definite reproducing kernel \(k\), the dynamic of kernel gradient flow (KGF)  is

\[}{t}f_{t}^{}(x)=-k(x,X)( f_{t}^{}(X)-Y),\] (8)

where \(f_{t}^{}\) is the KGF predictor. In kernel gradient flow, the performance of kernel predictor depends on the relative smoothness of regression function. People often consider the case that \((f,k) 1\). When the smoothness satisfies \((f,k)<1\), the regression function is said to be poorly smooth and belongs to the so-called misspecified spectral algorithm problem. We collect the related result in Zhang et al.  and apply it to our case, to derive the following proposition:

**Proposition 2.2**.: Suppose the eigenvalue decay rate of \(k\) is \(\) and the embedding index is \(\) with respect to \(\). Suppose the noise term \(\) satisfies Assumption 1. Let the dynamic (8) starts from \(f_{0}^{}=0\). Also, suppose the regression function satisfies \(f^{*}[_{k}]^{s}\) and \(\|f^{*}\|_{[_{k}]^{s}} R\), for some \(s>0\). Let \( s\) and \(0 1\). By choosing \(t n^{}\), for any fixed \((0,1)\), when \(n\) is sufficient large, with probability at least \(1-\), we have

\[f_{t}^{}-f^{*}_{|[_{k}]^{}}^{2} ()^{2}R^{2}Cn^{-},\]

where \(C\) is a positive constant.

## 3 Network and Neural Tangent Kernel

### Network settings

We consider the fully-connected network with \(L\) hidden layers. As is commonly-used in deep learning, we consider the ReLU activation  defined by \((x)(x,0)\). Denote \(f(;):^{d}\) as the network output function, where \(\) representing the column vector that all parameters flattened into. We can write the recursive structure of network as following:

\[^{(1)}(x) =}}(W^{(0)}x+b^{(0)});\] (9) \[^{(l)}(x) =}}W^{(l-1)}(x)(^{(l-1)}(x)),  l=2,3,,L;\] \[f(x;) =W^{(L)}(^{(L)}(x)),\]

The parameter matrix for the \(l\)-th layer is denoted as \(W^{(l)}\). Their dimensions are of \(m_{l+1} m_{l}\), where \(m_{l}\) is the number of units in layer \(l\) and \(m_{l+1}\) is that of layer \(l+1\). Also, the bias term of the first layer is denoted as \(b^{(0)}^{m_{1} 1}\). The setting of bias term is to make sure the positive definiteness of NTK . We further assume that the number of units in each layer is at the same order while the width comes to infinity, as \(cm(m_{1},,m_{L+1})(m_{1},,m_{L+1}) Cm\) where \(c,C\) are some absolute positive constants.

Standard initializationAt initialization, the parameters are randomly set as i.i.d. standard normal variables:

\[W^{(l)}_{ij},b^{(0)}_{k}}}{{}} (0,1), l=0,1,,L; k=1,,m_{1}.\] (10)

**Remark 3.1** (Mirrored initialization).: As to the _mirrored initialization_ considered in , part of the network \(f^{(1)}(;^{(1)}_{0})\) undergoes standard initialization, while the other complicated corresponding part \(f^{(2)}(;^{(2)}_{0})\) holds the same structure as \(f^{(1)}(;^{(1)}_{0})\), with parameters initialized to the same values as \(^{(2)}_{0}=^{(1)}_{0}\). Lastly, the neural network output function is defined as \(f(;_{0})=}{2}(f^{(1)}(;^{(1)}_{0})-f ^{(2)}(;^{(2)}_{0}))\). This setup ensures that \(f(;_{0})\) is constantly zero.

The network is trained under the mean square loss function. If we suppose \(\{(x_{i},y_{i})\}_{i=1}^{n}\) be the training data, then the loss function is specified as

\[()=_{i=1}^{n}(f(x_{i};)-y_{i})^{2}.\] (11)

For notational simplicity, we denote by \(f^{}_{t}(x)=f(x;_{t})\). The training process for the network is performed by gradient flow, where the parameters are updated through the differential equation:

\[}{t}_{t}=-_{}( )=-[_{}f^{}_{t}(X)]^{T}(f^{}_{ t}(X)-Y),\] (12)

where \(_{}f^{}_{t}(X)\) is a matrix with dimensions \(n M\), with \(M\) being the length of the parameter vector \(\). This matrix represents the gradient of the network output \(f^{}_{t}(X)\) with respect to the parameters \(\) at time \(t\). Incorporating the chain rule, we can formulate the gradient flow equation for the network function as follows:

\[}{t}f^{}_{t}(x)=-_{ }f^{}_{t}(x)[_{}f^{}_{t}(X)]^{T}(f^ {}_{t}(X)-Y).\] (13)

### Network at initialization

In order to state the properties of wide network with standard initialization, we need to introduce the concept of Gaussian process.

Gaussian processGaussian process is a stochastic process for which every finite collection of random variables follows a multivariate Gaussian distribution. Let \(X\) be a Gaussian process with index \(t T\). If the mean and covariance are given by the mean function \(m\) and the positive definite kernel \(k\) such that \([X(t)]=m(t)\) and \([X(t)X(t^{})]=k(t,t^{})\), which holds for any \(t,t^{} T\), then we say \(X(m,k)\).

In standard initialization (10), the parameters of the neural network are i.i.d. samples from a standard normal distribution. If the network contains only one hidden-layer (that is, if \(L=2\)), it is direct to prove that \(f_{0}^{}(x)\) converges to a centered Gaussian distribution by CLT, for any fixed point \(x\). As to the multi-layer network, prior research  also proved that such initialized network converges to a Gaussian process, as following:

**Lemma 3.2** (Limit distribution of initialization).: _As the network width \(m\) tend to infinity, the sequence of network stochastic process \(\{f_{0}^{}\}_{m=1}^{}\) converges weakly in \(C(,)\) to a centered Gaussian process \(f^{}\). The covariance function is the so-called random feature kernel (RFK), which is denoted by \(K^{}(x,x^{})\) as defined in (42) in Appendix C.1._

### The kernel regime

As the gradient descent of neural network involves high non-linearity and non-convexity, it is difficult to study the training process. However, Jacot et al.  introduced the Neural Tangent Kernel (NTK) theory which provides a connection between network training and a class of kernel regression problems, when the network width comes to infinity. To demonstrate this, we first define a Neural Network Kernel (NNK):

\[K_{t}^{m}(x,x^{})=[_{}f_{t}(x)]^{T}[_{}f_{t} (x^{})].\] (14)

Using this notation, we reformulate (13) in a kernel regression format:

\[}{t}f_{t}^{}(x)=-K_{t}^{m}(x, X)(f_{t}^{}(X)-Y).\] (15)

NTK theory shows, if the network width \(m\) tends to infinity, then the random kernel \(K_{t}^{m}(,)\) will converge to a time-invariant kernel \(K^{}(,):^{d}^{d}\), which is referred to as the NTK of network. The phenomenon is the so-called NTK regime . The fixed kernel \(K^{}\) only depends on the structure of the neural network and the way of initialization. To get more knowledge of NTK, we present the explicit expression of NTK in Appendix C.1. In NTK theory, the dynamic of network (15) can be approximated by a kernel gradient flow equation:

\[}{t}f_{t}^{}(x)=-K^{}(x,X)(f_{t}^{}(X)-Y),\] (16)

which starts from Gaussian process \(f_{0}^{}=f^{}\). In this way, if we aims to derive the generalization property of sufficiently wide network, we can achieve by considering the corresponding kernel gradient flow predictor. Such approximation is strictly ensured by uniform convergence of \(f_{t}^{}\) and \(f_{t}^{}\) over all \(x\) and all \(t 0\) as \(m\), since we use \(L^{2}\) excess risk to evaluate the generalization ability. Actually, we have the following theorem, whose proof is given in Appendix B.

**Proposition 3.3** (Uniform convergence).: Given training sample pairs \(\{(x_{i},y_{i})\}_{i=1}^{n}\). For any \((0,1)\) and \(>0\), when network width \(m\) is large enough, we have

\[_{x,x^{}}_{t 0} f_{t}^{}(x)-f _{t}^{}(x)\]

holds with probability at least \(1-\).

In this theorem, we show the uniform convergence of network under standard initialization. Previous related studies  always utilized delicately designed mirrored initialization (as shown in Remark 3.1) to avoid the analysis on the initial output function of network, since it will lead to the challenging problem that \(f_{t}^{}\) and \(f_{t}^{}\) are both random, unlike that \(f_{t}^{}\) is a fixed function in the case of mirrored initialization. However, as shown in Section 3.2, the initial output function is near a Gaussian process that can not be overlooked. To under the performance of neural networks commonly used in real world, it is necessary to analyzing the network initialization. To the best of our knowledge, we are the first to consider the initial output function of network in uniform convergence. This comprehensive result allows us to study the generalization error of network more precisely.

Impact of Initialization

### Impact of standard initialization on the generalization error

The standard kernel gradient flow is always considered to start from zero, as in Proposition 2.2. Therefore, we need to do a transformation since the initial value of predictor \(f_{t}^{}\) is actually \(f^{}\) instead of zero. Firstly, we can yield a solution of (8) in matrix form:

\[f_{t}^{}(x)=f_{0}^{}(x)+k(x,X)(I-e^{-k(X,X)} )[k(X,X)]^{-1}(f_{0}^{}(X)-f^{*}(X)-_{X}),\] (17)

where \(_{X}\) is employed to represent the \(n 1\) column noise term vector \(Y-f^{*}(X)\). We denote by \(f_{t}^{}\) the kernel gradient flow predictor under initial function \(f_{0}\) and denote by \(_{t}^{}\) the KGF predictor under initialization \(_{0}^{} 0\). If we plug them into (17) and excess risk (2), respectively, we directly have the following theorem:

**Proposition 4.1** (Impact of initialization in kernel gradient flow).: Denote \(^{*}=f^{*}-f_{0}\) as the biased regression function. For the KGF predictor \(f_{t}^{}\) and \(_{t}^{}\) defined above, we have

\[(f_{t}^{};f^{*})=(_{t}^{ };^{*}).\] (18)

The theorem establishes the equivalence of the generalization properties between the KGF predictor with initial value \(f_{0}\), regression function \(f^{*}\) and the KGF predictor with initial value zero, regression function \(f^{*}-f_{0}\). Back to the network case, combining uniform convergence result in Proposition 3.3, it suggests that, compared to mirrored initialization, the impact of standard initialization which has non-zero initial output function is equivalent to introducing a same-valued implicit bias to the regression function. This is a generalization of the main result in Zhang et al. , which only focused on case at \(t=\). To summarize, Proposition 4.1 provides a convenient approach to quantify the impact of standard initialization in early-stopping neural networks.

### Smoothness of Gaussian process

Building upon the analysis above, our focus now turns to illustrating the smoothness of the Gaussian process \(f^{}\), as it is the limit distribution of \(f_{0}^{}\). Actually, we can derive the following theorem:

**Theorem 4.2** (Smoothness of Gaussian Process).: _Suppose that \(f^{}\) is a Gaussian process with mean function 0 and covariance function \(K^{}\). The following statements hold:_

\[(f^{}[^{ }]^{s})&=1, s<;\\ (f^{}[^{}]^{s} )&=0, s.\] (19)

We furnish a comprehensive proof for Theorem 4.2 in the Appendix C.

Let us now turn our attention to the implications established by this theorem. Recall that Proposition 4.1 has shown that, in KGF, the existence of initialization function \(f^{}\) is equivalent to adding a same-valued bias term to the regression function \(f^{*}\). Consequently, the poor smoothness of initialization function causes the high smoothness assumption on the regression function meaningless. Regardless of how smooth we assume the regression function to be (e.g., \((f^{*},K^{}) 2\)), the value of (relative) smoothness \((f^{*}-f^{},K^{})\) will always be at most \(\). Namely, the biased regression function \(f^{*}-f^{}\) is always poorly smooth. In this specific case, we could hardly expect the KGF predictor to have fine performance.

### Upper bound

Now we are ready to provide the upper bound of generalization error of network. With the help of Proposition 2.2, Proposition 3.3, Proposition 4.1 and Theorem 4.2, we derive the following theorem:

**Theorem 4.3** (Generalization error upper bound).: _Assume that the regression function \(f^{*}[^{}]^{s}\) for some \(s>0\), and \(\|f^{*}\|_{[^{}]^{s}} R\) where \(R\) is a positive constant. Assume the marginal probability measure \(\) with density \(p(x)\) satisfies \(c p(x) C\) for some positive constant \(c\) and \(C\)._* _For the case of_ \(s\)_, for any_ \((0,1)\) _and_ \((0,)\)_, by choosing certain_ \(t=t(n)\) _(as shown in Appendix), when_ \(n\) _is sufficiently large and_ \(m\) _is sufficiently large, with probability_ \(1-\) _we have_ \[\|f_{t}^{}-f^{*}\|_{L^{2}}^{2}( )^{2}(R+C_{})^{2}Cn^{-+ },\] (20) _where_ \(C_{}\) _is a positive constant related to_ \(\)_._
* _For the case of_ \(0<s<,\) _for any_ \((0,1)\)_, by choosing_ \(t n^{}\)_, when_ \(n\) _is sufficiently large and_ \(m\) _is sufficiently large, with probability_ \(1-\) _we have_ \[\|f_{t}^{}-f^{*}\|_{L^{2}}^{2}( )^{2}(R+C_{s})^{2}Cn^{-},\] (21) _where_ \(C_{s}\) _is a positive constant related to_ \(s\)_._

The proof is provided in Appendix D. This result shows the generalization error upper bound for network with standard initialization and demonstrate its negative effect. Even if the goal function \(f^{*}\) is quite smooth, the generalization error upper bound \(n^{-}\) remains to be a quite low rate, particularly considering that the dimension \(d\) of data is usually large in real world. It suggests that the network no longer generalizes well, even if we adopt the once useful early stopping strategy in Li et al. .

### Lower bound

From the analysis above, we can see the poor generalization ability of network under standard initialization. Furthermore, in this section, we take spherical data as example and provide the lower bound of generalization error. Namely, we presume the input vectors \(x\) are distributed on the sphere \(^{d}\) with probability measure \(\), which is a common assumption in NTK theory . We also slightly change the network structure. Compared to the network (9), we eliminate the bias term of the initial layer, as shown in (40) in Appendix. In this case, the NTK of new network is denoted by \(K_{0}^{}\), and the RKHS \(_{0}^{}(^{d})\) is abbreviated as \(_{0}^{}\), whose detailed properties is also given in Appendix C.1. Additionally, we make more assumption on the noise of data. We assume the noise term \(\) in (1) to have a constant second moment, as \([||^{2}|x]=^{2}\) for \(x^{d},a.e..\). Under these conditions, with the help of method in Li et al. , we derive the theorem:

**Theorem 4.4** (Generalization error lower bound).: _We assume that the regression function \(f^{*}[_{0}^{}]^{s}\) for some \(s>\), and denote by \(\|f^{*}\|_{[_{0}^{}]^{s}} R\) where \(R\) is a positive constant. Assume that \(\) is the uniform measure. For any \((0,1)\), when \(n\) is large enough and \(m\) is large enough, for any choice of \(t=t(n)\), with probability at least \(1-\) we have_

\[[\|f_{t}^{}-f^{*}\|_{L^{2}}^{2}|X ]=(n^{-}).\] (22)

The proof is given in Appendix E. Through Theorem 4.4, we derive \(n^{-}\) as the generalization lower bound of standardly random-initialized network in NTK theory, even if the regression function is quite smooth. The rate \(n^{-}\) means model suffers notably from data that has large dimension: If \(d\) is relatively large, then this rate of convergence can be extremely slow. This is a manifestation of the curse of dimensionality. In fact, it contrasts with the fact that neural networks excel at high-dimensional problems. This contradiction underscores the limitation of NTK theory for interpreting network performance.

## 5 Experiments

Our numerical experiments are conducted in two aspects to fully understand the impact of standard initialization. First, we show the performance of standard initialized network is indeed worse than the mirrored initialized case, on the aspect of learning rate. The phenomenon is in line with our theoretical analysis. Second, the smoothness of regression function of real data is significantly larger than \(\), which suggest the bad effect of non-zero intial output function of standard initialization will indeed destroy the performance of network if NTK theory holds. It demonstrates the drawback of NTK theory through contradiction.

### Artificial data

In the first experiment, we employ artificial data to show the negative effect of standard initialization on the generalization error of network. The detailed settings are shown in Appendix F.

Learning rate of network under different initializationThe experiments are conducted for both \(d=5\) and \(d=10\), contrasting network performance subject to mirrored and standard initialization strategies. We choose a relatively smooth goal function to emphasize the impact of initialization. Specifically, we use \(m=20n\), epoch \(=10n\), and the gradient learning rate \(lr=0.6\). The networks are made sufficiently wide to ensure the overparametrization assumption is met. Additionally, we implement the early-stopping strategy as mentioned in Theorem 4.3, that is, selecting the minimum loss across all epochs as the generalization error. Finally, we test the network's generalization error on different levels of sample size \(n\), and plot the log value of the generalization error corresponding to \((n)\) as shown in Figure 1. As we expected, the points in Figure 1 fits a linear trend. Moreover, the figure highlights the difference in learning rate under different initialization methods. This aligns with our theoretical results.

### Real data

In this subsection, we focus on datasets from the real world and estimate the smoothness of function. Although we could not know the goal function that the real data is generated from, there exists a way to estimate its smoothness . We show the technical details in Appendix G.

Smoothness of goal function in real datasetsWe employed the MNIST, CIFAR-10 and Fashion-MNIST datasets. In the experiments, we evaluate the smoothness of goal function of the datasets, with respect to the one-hidden layer NTK. The results are presented in Table 1. With the input dimension \(d=784,3072,784\), we can compute that the smoothness of initialization function is equal to \( 0\). However, the smoothness of goal function is far better than \(\), which implies that standard initialization will indeed destroy the generalization performance, under NTK theory. The contradiction between NTK theory and the real situation shows its limitation and once again confirms our conclusion.

    \\  Name & Dimension & Smoothness \\  MNIST & \(28 28 1\) & \(0.40\) \\ CIFAR-10 & \(32 32 3\) & \(0.09\) \\ Fashion-MNIST & \(28 28 1\) & \(0.22\) \\   

Table 1: Smoothness of goal function

Figure 1: Generalization error decay curve of network. The scatter points show the averaged log error over \(20\) trials. The dashed lines are computed through least-squares. The scale of \(n\) is not broad because a larger \(n\) requires a larger \(m\), which would induce higher computational costs.

Discussion

To summarize, this research focuses on the impact of standard random initialization on generalization property of fully-connected network in the NTK theory, which makes up the gap in this field. Many previous work [35; 41] verified the statistical optimality of neural network under delicately designed mirrored initialization, whose initial output function of network is zero. However, through our study, we pinpoint that if we consider the commonly-used standard initialization, the learning rate of network is notably slow when the dimension of data is slightly large, which fails to explain network's favorable performance in overcoming the curse of dimensionality. A direct implication of our work is the superiority of mirror initialization over standard initialization, which suggests a direction for future improvements. On a deeper level, although NTK theory can describe many properties of network, at least for the fully connected networks with Gaussian initialization discussed in this paper, we can explore better theoretical frameworks to characterize their generalization ability in the future.