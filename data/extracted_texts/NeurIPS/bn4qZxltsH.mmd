# Hierarchical Open-vocabulary Universal Image Segmentation

Xudong Wang\({}^{1*}\)  Shufan Li\({}^{1*}\)  Konstantinos Kallidromitis\({}^{2*}\)

Yusuke Kato\({}^{2}\)  Kazuki Kozuka\({}^{2}\)  Trevor Darrell\({}^{1}\)

\({}^{1}\)Berkeley AI Research, UC Berkeley \({}^{2}\)Panasonic AI Research

project page: [http://people.eecs.berkeley.edu/](http://people.eecs.berkeley.edu/)\(\)xdwang/projects/HIPIE

###### Abstract

Open-vocabulary image segmentation aims to partition an image into semantic regions according to arbitrary text descriptions. However, complex visual scenes can be naturally decomposed into simpler parts and abstracted at multiple levels of granularity, introducing inherent segmentation ambiguity. Unlike existing methods that typically sidestep this ambiguity and treat it as an external factor, our approach actively incorporates a hierarchical representation encompassing different semantic-levels into the learning process. We also propose a decoupled text-image fusion mechanism and representation learning modules for both "things" and "stuff".1 Additionally, we systematically examine the differences that exist in the textual and visual features between these types of categories. Our resulting model, named **HIPIE**, tackles **H**ierarchical, **o**Pen-vocabulary, and un**I**v**E**rsal segmentation tasks within a unified framework. Benchmarked on over 40 datasets, _e.g._, ADE20K, COCO, Pascal-VOC Part, RefCOCO/RefCOCOg, ODinW and SeginW, HIPIE achieves the state-of-the-art results at various levels of image comprehension, including semantic-level (_e.g._, semantic segmentation), instance-level (_e.g._, panoptic/referring segmentation and object detection), as well as part-level (_e.g._, part/subpart segmentation) tasks.

## 1 Introduction

Image segmentation is a fundamental task in computer vision, enabling a wide range of applications such as object recognition, scene understanding, and image manipulation . Recent advancements in large language models pave the way for open-vocabulary image segmentation, where models can handle a wide variety of object classes using text prompts. However, there is no single "correct" way to segment an image. The inherent ambiguity in segmentation stems from the fact that the interpretations of boundaries and regions within an image depend on the specific tasks.

Existing methods for open-vocabulary image segmentation typically address the ambiguity in image segmentation by considering it as an external factor beyond the modeling process. In contrast, we adopt a different approach by embracing this ambiguity and present **HIPIE**, as illustrated in Fig. 1, a novel **H**lerarchical, **o**Pen-vocabulary and un**I**v**E**rsal image segmentation and detection model. This includes semantic-level segmentation, which focuses on segmenting objects based on their semantic meaning, as well as instance-level segmentation, which involves segmenting individual instances of objects or groups of objects (_e.g._, instance and referring segmentation).

Additionally, our model captures finer details by incorporating part-level segmentation, which involves segmenting object parts/subparts. By encompassing different granularity, HIPIE allows for a more comprehensive and nuanced analysis of images, enabling a richer understanding of their contents.

To design HIPIE, we begin by investigating the design choices for open-vocabulary image segmentation (OIS). Existing methods on OIS typically adopt a text-image fusion mechanism, and employ a shared representation learning module for both stuff and thing classes [4; 62; 58; 10; 56]. Fig. 2 shows the similarity matrices of visual and textual features between stuff and thing classes. On this basis, we can derive several conclusions:

* Noticeable discrepancies exist in the between-class similarities of textual and visual features between stuff and thing classes.
* Stuff classes exhibit significantly higher levels of similarity in text features than things.

Figure 1: HIPIE is a unified framework which, given an image and a set of arbitrary text descriptions, provides hierarchical semantic, instance, part, and subpart-level image segmentations. This includes open-vocabulary semantic (_e.g._, crowds and sky), instance/panoptic (_e.g._, person and cat), part (_e.g._, head and torso), subpart (_e.g._, ear and nose) and referring expression (_e.g._, umbrella with a white pole) masks. HIPIE outperforms previous methods and established new SOTAs on these tasks regardless of their granularity or task specificity. Bottom images: our method can seamlessly integrate with SAM to enable class-aware image segmentation on SA-1B.

Figure 2: Noticeable discrepancies exist in the between-class similarities of visual and textual features between stuff and thing classes. We propose a decoupled representation learning approach that effectively generates more discriminative visual and textual features. We extract similarity matrices for the visual features, obtained through a pretrained MAE  or our fine-tuned one, and for the text features, produced using a pretrained BERT  or fine-tuned one. We report results on COCO-Panoptic  and measure the mean similarity (\(\)).

This observation suggests that integrating textual features may yield more significant benefits in generating discriminative features for thing classes compared to stuff classes. Consequently, for thing classes, we adopt an early image-text fusion approach to fully leverage the benefits of discriminative textual features. Conversely, for stuff classes, we utilize a late image-text fusion strategy to mitigate the potential negative effects introduced by non-discriminative textual features. Furthermore, the presence of discrepancies in the visual and textual features between stuff and thing classes, along with the inherent differences in their characteristics (stuff classes requiring better capture of texture and materials, while thing classes often having well-defined geometry and requiring better capture of shape information), indicates the need for decoupling the representation learning modules for producing masks for stuffs and things.

In addition to instance/semantic-level segmentation, our model is capable of open-vocabulary hierarchical segmentation. Instead of treating part classes, like 'dog leg', as standard multi-word labels, we concatenate class names from different granularity as prompts. During training, we supervise the classification head using both part labels, such as 'tail', and instance labels, such as 'dog', and we explicitly contrast a mask embedding with both instance-level and part-level labels. In the inference stage, we perform two separate forward passes using the same image but different prompts to generate instance and part segmentation. This design choice empowers _open-vocabulary_ hierarchical segmentation, allowing us to perform part segmentation on novel part classes by randomly combining classes from various granularity, such as 'giraffe' and 'leg', even if they have never been seen during training. By eliminating the constraints of predefined object classes and granularity, HIPIE offers a more flexible and adaptable solution for image segmentation.

We extensively benchmark HIPIE on various popular datasets to validate its effectiveness, including MSCOCO, ADE20K, Pascal Panoptic Part, and RefCOCO/RefCOCOg. HIPIE achieves state-of-the-art performance across all these datasets that cover a variety of tasks and granularity.

To the best of our knowledge, HIPIE is the first hierarchical, open-vocabulary and universal image segmentation and detection model (see Table 1). By decoupling representation learning and text-image fusion mechanisms for things vs. stuff classes, HIPIE overcomes the limitations of existing approaches and achieves state-of-the-art performance on various benchmarks.

## 2 Related Works

**Open-Vocabulary Semantic Segmentation**[2; 53; 26; 16; 44; 32; 54; 55] aims to segment an image into semantic regions indicated by text descriptions that may not have been seen during training. ZS3Net  combines a deep visual segmentation model with an approach to generate

  & Open & Instance & Semantic & Panoptic & Referring & Cls-agnostic & Cls-aware & Object \\  & Vocab. & Segment. & Segment. & Segment. & Segment. & Part Seg. & Part Seg. & Detection \\  SAM  & ✗ & ✓ & ✗ & ✗ & ✗ & ✓ & ✗ & * \\ SEEM  & ✓ & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ & * \\ ODISE  & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ & ✗ & * \\ UNINEXT  & † & ✓ & ✗ & ✗ & ✓ & ✗ & ✗ & ✓ \\ X-Decoder  & ✓ & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ & * \\ G-DINO  & ✓ & ✓ & ✗ & ✗ & ✓ & ✗ & ✗ & ✓ \\ PPS  & ✗ & ✗ & ✗ & ✗ & ✓ & ✓ & ✗ \\  HIPIE & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\ _vs. prev. SOTA_ & - & +**5.1** & +**2.0** & +**1.3** & +**0.5** & - & +5.2 & +3.2 \\ 

Table 1: Our HIPIE is capable of performing all the listed segmentation and detection tasks and achieves the state-of-the-art performance using a unified framework. We present performance comparisons with SOTA methods on a range of benchmark datasets: AP\({}^{}\) for instance segmentation on MSCOCO , AP\({}^{}\) for object detection on MSCOCO, oIoU for referring segmentation on RefCOCO+ , mIoU for semantic segmentation on Pascal Context, and mIoU\({}_{}\) for part segmentation on Pascal-Panoptic-Parts . The second best performing method for each task is underlined. *: object detection can be conducted via generating bounding boxes using instance segmentation masks. ‘Seg.’ denotes segmentation. †: In principle, UNINEXT can take arbitrary texts as labels, however, the original work focused on close-set performance and did not explore open-vocabulary inference.

visual representations from semantic word embeddings to learn pixel-wise classifiers for novel categories. LSeg  uses CLIP's text encoder  to generate the corresponding semantic class's text embedding, which it then aligns with the pixel embeddings. OpenSeg  adopts a grouping strategy for pixels prior to learning visual-semantic alignments. By aligning each word in a caption to one or a few predicted masks, it can scale-up the dataset and vocabulary sizes. GroupViT  is trained on a large-scale image-text dataset using contrastive losses. With text supervision alone, the model learns to group semantic regions together. OVSgemtort  uses learnable group tokens to cluster image pixels, aligning them with the corresponding caption embeddings.

**Open-Vocabulary Panoptic Segmentation** (OPS) unifies semantic and instance segmentation, and aims to perform these two tasks for arbitrary categories of text-based descriptions during inference time [10; 56; 66; 67; 58]. MaskCLIP  first predicts class-agnostic masks using a mask proposal network. Then, it refines the mask features through Relative Mask Attention interactions with the CLIP visual model and integrates the CLIP text embeddings for open-vocabulary classification. ODISE  unifies Stable Diffusion , a pre-trained text-image diffusion model, with text-image discriminative models, _e.g._ CLIP , to perform open-vocabulary panoptic segmentation. X-Decoder  takes two types of queries as input: generic non-semantic queries that aim to decode segmentation masks for universal segmentation, and textual queries to make the decoder language-aware for various open-vocabulary vision tasks. UNINEXT  unifies diverse instance perception tasks into an object discovery and retrieval paradigm, enabling flexible perception of open-vocabulary objects by changing the input prompts.

**Referring Segmentation** learns valid multimodal features between visual and linguistic modalities to segment the target object described by a given natural language expression [19; 60; 20; 22; 13; 59; 52; 35; 63]. It can be divided into two main categories: 1) _Decoder-fusion_ based method [8; 51; 63; 35] first extracts vision features and language features, respectively, and then fuses them with a multimodal design. 2) _Encoder-fusion_ based method [13; 59; 30] fuses the language features into the vision features early in the vision encoder.

**Parts Segmentation** learns to segment instances into more fine-grained masks. PPP  established a baseline of hierarchical understanding of images by combining a scene-level panoptic segmentation model and part-level segmentation model. JPPF  improved this baseline by introducing joint Panoptic-Part Fusion module that achieves comparable performance with significantly smaller models.

**Promptable Segmentation.** The Segment Anything Model (SAM)  is an approach for building a fully automatic promptable image segmentation model that can incorporate various types of human interventions, such as texts, masks, and points. SEEM  proposes a unified prompting scheme that encodes user intents into prompts in a joint visual-semantic space. This approach enables SEEM to generalize to unseen prompts for segmentation, achieving open-vocabulary and zero-shot capabilities. Referring segmentation can also be considered as promptable segmentation with text prompts.

**Comparison with Previous Work.** Table 1 compares our HIPIE method with previous work in terms of key properties. Notably, HIPIE is the only method that supports open-vocabulary universal image segmentation and detection, enabling the object detection, instance-, semantic-, panoptic-, hierarchical-(whole instance, part, subpart), and referring-segmentation tasks, all within a single unified framework.

## 3 Method

We consider all relevant tasks under the unified framework of language-guided segmentation, which performs open-vocabulary segmentation and detection tasks for arbitrary text-based descriptions.

### Overall Framework

The proposed HIPIE model comprises three main components, as illustrated in Fig. 3:

_1) Text-image feature extraction and information fusion (detailed in Secs. 3.2 to 3.4):_ We first generate a text prompt \(T\) from labels or referring expressions. Then, we extract image (\(I\)) and text (\(T\)) features \(F_{v}=_{v}(I),F_{t}=_{t}(T)\) using image encoder \(_{v}\) and text encoder \(_{t}\), respectively. We then perform feature fusion and obtain fused features \(F^{}_{v},F^{}_{t}=(F_{v},F_{t})\).

2) Foreground (referred to as things) and background (referred to as stuffs) mask generation (detailed in Sec. 3.5):_ Each of the decoders takes in a set of image features and text features and returns sets of masks, bounding boxes, and object embeddings \((M,B,E)\). We compute the foreground and background proposals and concatenate them to obtain the final proposals and masks as follows:

\[&(M_{2},B_{2},E_{2})&=& (F_{v},F_{t})\\ &(M_{1},B_{1},E_{1})&=&((F_{v},F_{t}))\\ &(M,B,E)&=&(M_{1} M_{2},B_{1} B_{2},E_{1} E_ {2}) \]

where \(\) denotes the concatenation operation.

3) Proposal and mask retrieval using text prompts (detailed in Sec. 3.6):To assign class labels to these proposals, we compute the cosine similarity between object embedding \(E\) and the corresponding embedding \(E^{}_{i}\) of class \(i\{1,2...,c\}\). For a set of category names, the expression is a concatenated string containing all categories. We obtain \(E^{}_{i}\) by pooling tokens corresponding to each label from the encoded sequence \(F_{t}\). For referring expressions, we taken the [CLS] token from BERT output as \(E^{}_{i}\).

### Text Prompts

Text prompting is a common approach used in open-vocabulary segmentation models [19; 60; 57; 58].

For open-vocabulary instance segmentation, panoptic segmentation, and semantic segmentation, the set of all labels \(C\) is concatenated into a single text prompt \(T_{i}\) using a "." delimiter. Given an image \(I\) and a set of text prompts \(T\), the model aims to classify \(N\) masks in the label space \(C\{\)"\(other\)"\(\}\), where \(N\) is the maximum number of mask proposals generated by the model.

For referring expressions, the text prompt is simply the sentence itself. The goal is to locate one mask in the image corresponding to the language expression.

### Image and Text Feature Extraction

We employ a pretrained BERT model  to extract features for text prompts. Because the BERT-base model can only process input sequences up to 512 tokens, we divide longer sequences into segments of 512 tokens and encode each segment individually. The resulting features are then concatenated to obtain features of the original sequence length.

We utilize ResNet-50  and Vision Transformer (ViT)  as base architectures for image encoding. In the case of ResNet-50, we extract multiscale features from the last three blocks and denote them as \(F_{v}\). For ViT, we use the output features from blocks 8, 16, and 32 as the multiscale features \(F_{v}\).

Figure 3: Diagram of HIPIE for hierarchical, universal and open-vocabulary image segmentation and detection. The image and text prompts are first passed to the image and text decoder to obtain visual features \(F_{v}\) and text features \(F_{t}\). Early fusion is then applied to merge image and text features to get \(F^{}_{v}\), \(F^{}_{t}\). Two independent decoders are used for things (foreground) classes and stuff (background) classes.

### Text-Image Feature Fusion

We explored several design choices for text-image feature fusion and mask generation modules as shown in Fig. 4 and Table 5, and discovered that Fig. 4c) can give us the optimal performance. We adopt bi-directional cross-attention (\(Xattn}\)) to extract text-guided visual features \(F_{t2v}\) and image-guided text features \(F_{v2t}\). These attentive features are then integrated with the vanilla text features \(F_{t}\) and image features \(F_{v}\) through residual connections, as shown below:

\[F_{t2v},\ F_{v2t}&=&Xattn}(F_{v},F_{t})\\ (F^{}_{v},\ F^{}_{t})&=&(F_{v}+F_{t2v},\ F_{t}+F_{v2t}) \]

where \(F_{v}\) and \(F_{t}\) represent the visual and text-prompt features, respectively.

### Thing and Stuff Mask Generation

We then generate masks and proposals for the thing and stuff classes by utilizing \(F^{}_{v}\) and \(F^{}_{t}\) that we obtained in Sec. 3.4.

**Model Architecture.** While architectures such as Mask2Former and MaskDINO [4; 28] can perform instance, semantic and panoptic segmentation simultaneously, models trained jointly show inferior performance compared with the same model trained for a specific task (_e.g._ instance segmentation only). We hypothesize that this may result from the different distribution of spatial location and geometry of foreground instance masks and background semantic masks. For example, instance masks are more likely to be connected, convex shapes constrained by a bounding box, whereas semantic masks may be disjoint, irregular shapes spanning across the whole image.

To address this issue, in a stark contrast to previous approaches [58; 36; 57] that use a unified decoder all both stuffs and things, we decouple the stuff and thing mask prediction using two separate decoders. For the thing decoder, we adopt Deformable DETR  with a mask head following the UNINEXT  architecture and incorporate denoising procedures proposed by DINO . For the stuff decoder, we use the architecture of MaskDINO .

**Proposal and Ground-Truth Matching Mechanisms.** We make the following distinctions between the two heads. For thing decoder, we adopt simOTA  to perform many-to-one matching between box proposals and ground truth when calculating the loss. We also use box-iou-based NMS to remove duplicate predictions. For the stuff decoder, we adopt one-to-one Hungarian matching . Additionally, we disable the box loss for stuff masks. We set the number of queries to 900 for the things and 300 for the stuffs.

**Loss Functions.** For both decoders, we calculate the class logits as the normalized dot product between mask embeddings (\(M\)) and text embeddings (\(F^{}_{t}\)). We adopt Focal Loss  for classification outputs, L1 loss, and GIoU loss  for box predictions, pixel-wise binary classification loss and DICE loss  for mask predictions. Given predictions \((M_{1},B_{1},E_{1}),(M_{2},B_{2},E_{2})\), groundtruth labels \((M^{},B^{},C)\) and its foreground and background subset \((M^{}_{f},B^{}_{f},C_{f})\) and \((M^{}_{b},B^{}_{b},C_{b})\), The final Loss is computed as

\[_{}&=&_{}_{}(E_{1},C^{}_{f})+_{}_{}(M_{1},M^{}_{f})+_{}_{}(B_{1},B^{ }_{f})\\ _{}&=&_{}_{}(E_{2},C^{})+_{}_{}(M_{2},M^{}) +_{}_{}(B_{2},B^{}_{b})\\ &=&_{}+_{} \]

where \(_{}=_{L1}_{L1}+_{} _{}\), \(_{}=_{}_{}+_{ }_{}\), and \(_{}=_{}\). Note that while we do not use the stuff decoder for thing prediction, we still match its predictions with things and

Figure 4: Various design choices for generating thing and stuff masks with arbitrary text descriptions. In version a), We use a single decoder for all masks. Early fusion is applied. In version b), two independent decoders are used for things and stuff classes. Early fusion is adopted for both decoders. Version c) is identical to version b) with the only difference being that the stuff decoder do not make use of early fusion.

compute the class and box losses in the training. We find such auxiliary loss setup make the stuff decoder aware of the thing distribution and imporves the final performance.

### Open-Vocabulary Universal Segmentation

In closed set setting, we simply merge the output of two decoders and perform the standard postprocessing of UNINEXT  and MaskDINO  to obtain the final output.

In zero-shot open vocabulary setting, we follow ODISE  and combining our classification logits with a text-image discriminative model, _e.g._, CLIP . Specially, given the a mask \(M\) on image \(I\), its features \(E\) and test classes \(C_{}\), we first compute the probability \(p_{1}(E,C_{})=(C_{}|E)\) in the standard way as mentioned before. We additionally compute mask-pooled features of \(M\) from the vision encoder \(\) of CLIP as \(E_{}=(M,(I))\). Then we compute the CLIP logits \(p_{2}(E,C_{})=(C_{}|E_{})\) as the similarity between the CLIP text features and the \(E_{}\). Finally we combine the final prediction as

\[p_{}(E,C_{}) p_{1}(E,C_{})^{ }p_{2}(E,C_{})^{1-} \]

Where \(\) is a balancing factor. Emprically, we found such setting leads to better performance than naively relying completely on CLIP features only or close-set logits.

### Hierarchical segmentation

In addition to the instance-level segmentation, we can also perform part-aware hierarchical segmentation. We concatenate the instance class names and part class names as labels. Some examples are "human ear", and "cat head". In the training process, we supervise the classification head with both part labels and instance labels. Specifically, we replace \(L_{cls}\) with \(L_{clsPart}+L_{clsThing}\) in Eq. (3). We combine parts segmentation and instance segmentation of the same image to get part-aware instance segmentation. Additionally, layers of hierarchy is obtained by grouping the parts. For example, the "head" consists of ears, hair, eyes, nose, etc. Fig. 5 illustrates this process. Fig. A1 highlights the difference of our approach with other methods.

### Class-aware part segmentation with SAM

We can also perform the class-aware hierarchical segmentation by combining our semantic output with class-agnostic masks produced by SAM . Specifically, given semantic masks \(M\), their class probability \(P_{M}\), and SAM-generated part masks \(S\), we compute the class probability of mask \(S_{i} S\) with respect to class \(j\) as

\[P_{S}(S_{i},j)_{M_{k} M}P_{M}(M_{k},j)|M_{k} S_{i}| \]

Where \(|M_{k} S_{i}|\) is the area of intersection between mask \(M_{k}\) and \(S_{i}\). We combine our semantic output with SAM because our pretraining datasets only contains object-centric masks, whereas the SA-1B dataset used by SAM contains many local segments and object parts.

Figure 5: Hierarchical segmentation pipeline. We concatenate the instance class names and part class names as labels. During the training process, we supervise the classification head using both part labels and instance labels. During inference, we perform two separate forward passes using the same image but different prompts to generate instance and part segmentations. By combining the part segmentation and instance segmentation of the same image, we obtain hierarchical segmentation results on the right side.

## 4 Experiments

We comprehensively evaluate HIPIE through quantitative and qualitative analyses to demonstrate its effectiveness in performing various types of open-vocabulary segmentation and detection tasks. The implementation details of HIPIE are explained in Sec. 4.1. Sec. 4.2 presents the evaluation results of HIPIE. Additionally, we conduct an ablation study of various design choices in Sec. 4.3.

### Implementation Details

**Model Learning Settings** can be found in our appendix materials.

**Evaluation Metrics.**_Semantic Segmentation_ performance is evaluated using the mean Intersection-Over-Union (mIoU) metric. For _Part segmentation_, we report mIoU\({}_{}\), which is the mean IoU for part segmentation on grouped part classes . _Object Detection and Instance Segmentation_ results _are_ measured using the COCO-style evaluation metric - mean average precision (AP) . _Panoptic Segmentation_ is evaluated using the Panoptic Quality (PQ) metric . _Referring Image Segmentation_ (RIS) [19; 60] is evaluated with overall IoU (oloU).

### Results

**Panoptic Segmentation.** We examine Panoptic Quality (PQ) performance across MSCOCO  for closed-set and ADE20K  for open-set zero shot transfer learning. Based on Table 3 our model is able to outperform the previous close-set state-of-the-art using a ViT-H backbone by +1.8. In addition, we match the best open-set PQ results, while being able to run on more tasks and having a simpler backbone than ODISE . **Semantic Segmentation.** The evaluation of our model's performance on various open-vocabulary semantic segmentation datasets is presented in Table 4. These datasets include: 1) A-150: This dataset comprises 150 common classes from ADE20K . 2) A-847: This dataset includes all 847 classes from ADE20K . 3) PC-59: It consists of 59 common classes from Pascal Context . 4) PC-459: This dataset encompasses the full 459 classes of Pascal Context . 5) PAS-21: The vanilla Pascal VOC dataset , containing 20 foreground classes and 1 background class. These diverse datasets enable a comprehensive evaluation of our model's performance across different settings, such as varying class sizes and dataset complexities. Table 4 provides insights into how our model performs in handling open-vocabulary semantic segmentation tasks, demonstrating

  &  &  &  &  \\   & & PQ & AP\({}^{}\) & AP\({}^{}\) & mIoU & PQ & AP\({}^{}\) & AP\({}^{}\) & mIoU & mIoU\({}_{}\) \\  MaskCLIP  & ViT16 & - & - & - & - & 15.1 & 6.0 & - & 23.7 & - \\ X-Decoder  & FocalT & 52.6 & 41.3 & - & 62.4 & 18.8 & 9.8 & - & 25.0 & - \\ X-Decoder & DaViT-B & 56.2 & 45.8 & - & 66.0 & 21.1 & 11.7 & - & 27.2 & - \\ SEEM  & FocalT & 50.6 & 39.5 & - & 61.2 & - & - & - & - & - & - \\ SEEM & DaViT-B & 56.2 & 46.8 & - & 65.3 & - & - & - & - & - & - \\ ODISE  & ViT-H+SD & 55.4 & 46.0 & 46.1 & 65.2 & **22.6** & 14.4 & 15.8 & **29.9** & - \\ JPPF  & EffNet-b5 & - & - & - & - & - & - & - & 54.4 \\ PPS  & RNST269 & - & - & - & - & - & - & - & - & 58.6 \\ HIPIE & RN50 & 52.7 & 45.9 & 53.9 & 59.5 & 18.4 & 13.0 & 16.2 & 26.8 & 57.2 \\ HIPIE & ViT-H & **58.0** & **51.9** & **61.3** & **66.8** & 20.6 & **15.0** & **18.7** & 29.0 & **63.8** \\ 

Table 2: Open-vocabulary panoptic segmentation (PQ), instance segmentation (AP\({}^{}\)), semantic segmentation (mIoU), part segmentation (mIoU\({}_{}\)), and object detection (AP\({}^{}\)). N/A: not applicable. -: not reported.

Figure 6: **Qualitative Analysis of Open Vocabulary Hierarchical Segmentation**. Because of our hierarchal design, our model produces better-quality masks. In particular, our model can generalize to novel hierarchies that do not exist in part segmentation datasets.

its effectiveness and versatility in detecting and segmenting a wide range of object categories in real-world scenarios.

**Part Segmentation.** We evaluate our models performance on Pascal-Panoptic-Parts dataset  and report mIoUparts in Table 3. We followed the standard grouping from . Our model outperforms state-of-the-art by +5.2 in this metric. We also provide qualitative comparisons with Grounding DINO + SAM in Fig. 7. Our findings reveal that the results of Grounding SAM are heavily constrained by the detection performance of Grounding DINO. As a result, they are unable to fully leverage the benefits of SAM in producing accurate and fine-grained part segmentation masks.

 Method & Data & A-150 & PC-59 & PAS-21 & COCO & & & & \\   ZS3Net  & - & 19.4 & 38.3 & - & & & & \\ LSeg+  & 18.0 & 46.5 & - & 55.1 & & & & \\ HIPIE & 26.8 & 53.6 & 75.7 & 59.5 & & & & \\ _vs. prev. SOTA_ & **+7.1** & **+10.7** & **+28.3** & **+4.4** & & & & \\  GroupViT  & 10.6 & 25.9 & 50.7 & 21.1 & & & & \\ OpenSeg  & 21.1 & 42.1 & - & 36.1 & & & & \\ MaskCLIP  & 23.7 & 45.9 & - & - & & & \\ ODISE  & 29.9 & 57.3 & 84.6 & 65.2 & & & & \\ HIPIE & 29.0 & 59.3 & 83.3 & 66.8 & & & & \\ _vs. prev. SOTA_ & -0.9 & **+2.0** & -1.3 & **+1.6** & & & & \\ 

Table 4: Comparison on open-vocabulary semantic segmentation. Baseline results are copied from .

 Method & Data & A-150 & PC-59 & PAS-21 & COCO & & & & \\   OpenSeed & O365,COCO & 19.7 & 15.0 & 17.7 & 23.4 & - & - & 36.1 \\ X-Decoder & coco,ccm,sibu,c,g,coco,Cognition,Fluence & 21.8 & 13.1 & - & 29.6 & 9.2 & 16.1 & 32.2 \\ UNINEXT & O365,COCO,RefCOCO & 8.9 & 14.9 & 11.9 & 6.4 & 1.8 & 5.8 & 42.1 \\ HIPIE w/o CLIP & O365,COCO,RefCOCO,PACO & 18.1 & 16.7 & 20.2 & 19.8 & 4.8 & 12.2 & 41.0 \\ HIPIE w/ CLIP & + (CLIP) & 22.9 & 19.0 & 22.9 & 29.0 & 9.7 & 14.4 & 41.6 \\ 

Table 3: Open-Vocabulary Universal Segmentation. We compare against other universal multi-task segmentation models. (*) denotes pretraining dataset of representations.

Figure 7: Results of merging HIPIE with SAM for class-aware image segmentation on SA-1B dataset. Grounded-SAM (Grounding DINO + SAM)  cannot fully leverage the benefits of SAM in producing accurate and fine-grained part segmentation masks. Our method demonstrates fewer misclassifications and overlooked masks across the SA-1B dataset compared to the Grounded-SAM approach.

**Object Detection and Instance Segmentation.** We evaluate our model's object detection and instance segmentation capabilities following previous works [28; 67; 56]. On MSCOCO  and ADE20K  datasets, HIPIE achieves an increase of +5.1 and +0.6 AP\({}^{}\) respectively. Detailed comparisons are provided in Sec. 4.2 which demonstrate state-of-the-art results on ResNet and ViT architectures consistently across all Average Precision metrics.

**Referring Segmentation.** Referring image segmentation (RIS) tasks are examined using the RefCOCO, RefCOCO+, and RefCOCOg datasets. Our model outperforms all the other alternatives by an average of +0.5 in overall IoU (oloU).

### Ablation Study

To demonstrate the effectiveness of our design choices for text-image fusion mechanisms and representation learning modules for stuff and thing classes, we conduct an ablation study (depicted in Fig. 4) and present the results in Table 5. From this study, we draw several important conclusions: _1)_ Text-image fusion plays a critical role in achieving accurate referring segmentation results. _2)_ The early text-image fusion approach for stuff classes negatively impacts the model's performance on panoptic segmentation. This finding validates our analysis in the introduction section, where we highlighted the challenges introduced by the high levels of confusion in stuff's textual features, which can adversely affect the quality of representation learning. _3)_ Our design choices significantly improve the performance of panoptic segmentation, instance segmentation, and referring segmentation tasks. These conclusions underscore the importance of our proposed design choices in achieving improved results across multiple segmentation tasks.

## 5 Conclusions

This paper presents HIPIE, an open-vocabulary, universal, and hierarchical image segmentation model that is capable of performing various detection and segmentation tasks using a unified framework, including object detection, instance-, semantic-, panoptic-, hierarchical-(whole instance, part, subpart), and referring-segmentation tasks. Our key insight is that we should decouple the representation learning modules and text-image fusion mechanisms for background (_i.e._, referred to as stuff) and foreground (_i.e._, referred to as things) classes. Extensive experiments demonstrate that HIPIE achieves state-of-the-art performance on diverse datasets, spanning across a wide range of tasks and segmentation granularity.