# Fine-grained Expressivity of Graph Neural Networks

Jan Boker

RWTH Aachen University

&Ron Levie

Technion - Israel Institute of Technology

 Ningyuan Huang

Johns Hopkins University

&Soledad Villar

Johns Hopkins University

&Christopher Morris

RWTH Aachen University

###### Abstract

Numerous recent works have analyzed the expressive power of message-passing graph neural networks (MPNNs), primarily utilizing combinatorial techniques such as the \(1\)-dimensional Weisfeiler-Leman test (\(1\)-\(\)) for the graph isomorphism problem. However, the graph isomorphism objective is inherently binary, not giving insights into the degree of similarity between two given graphs. This work resolves this issue by considering continuous extensions of both \(1\)-\(\) and MPNNs to graphons. Concretely, we show that the continuous variant of \(1\)-\(\) delivers an accurate topological characterization of the expressive power of MPNNs on graphons, revealing which graphs these networks can distinguish and the level of difficulty in separating them. We identify the finest topology where MPNNs separate points and prove a universal approximation theorem. Consequently, we provide a theoretical framework for graph and graphon similarity combining various topological variants of classical characterizations of the \(1\)-\(\). In particular, we characterize the expressive power of MPNNs in terms of the tree distance, which is a graph distance based on the concept of fractional isomorphisms, and substructure counts via tree homomorphisms, showing that these concepts have the same expressive power as the \(1\)-\(\) and MPNNs on graphons. Empirically, we validate our theoretical findings by showing that randomly initialized MPNNs, without training, exhibit competitive performance compared to their trained counterparts. Moreover, we evaluate different MPNN architectures based on their ability to preserve graph distances, highlighting the significance of our continuous \(1\)-\(\) test in understanding MPNNs' expressivity.

## 1 Introduction

Graph-structured data is widespread across several application domains, including chemo- and bioinformatics , image analysis , and social-network analysis , explaining the recent growth in developing and analyzing machine learning methods tailored to graphs. In recent years, _message-passing graph neural networks_ (MPNNs)  emerged as the dominant paradigm, and alongside the growing prominence of MPNNs, numerous works  analyzed MPNNs' expressivity. The analysis, typically based on combinatorial techniques such as the \(1\)-_dimensional Weisfeiler-Leman test_ (\(1\)-\(\)) for the graph isomorphism problem , provides explanations of MPNNs' limitations (see  for a thorough survey). However, since the graph isomorphism problem only concerns whether the graphs are exactly the same, it only gives insights into MPNNs' ability to distinguish graphs. Hence, such approaches cannot quantify the graphs' degree of similarity. Nonetheless, understanding the similarities induced by MPNNs is crucial for precisely quantifying their generalization abilities , stability , or robustness properties .

**Present work.** To address these shortcomings, we show how to integrate MPNNs into the theory of _iterated degree measures_ first developed by Grebik and Rocha , which generalizes the \(1\)-\(\) and its characterizations to graphons . Integrating MPNNs into this theory allows us to identify the finesttopology in which MPNNs separate points, allowing us to prove a universal approximation theorem for graphons. Inspired by the Weisfeiler-Leman distance , we show that metrics on measures also integrate beautifully into the theory of iterated degree measures. Concretely, we define metrics \(_{}\) via the _Prokhorov metric_ and \(_{}\) via an _unbalanced Wasserstein metric_ that metrize the compact topology of iterated degree measures. _By leveraging this theory, we show that two graphons are close in these metrics if, and only if, the output of all possible MPNNs, up to a specific Lipschitz constant and number of layers, is close as well._ This refines the result of Chen et al. , which shows only one direction of this equivalence, i.e., graphs similar in their metric produce similar MPNN outputs.

We focus on graphons without node feature information to focus purely on MPNNs' ability to distinguish their structure. Our main result offers a topological generalization of classical characterizations of the \(1\)-WL, showing that the above metrics represent the optimal approach for defining a metric variant of the \(1\)-WL. Informally, the main result states the equivalence of our metrics \(_{}\) and \(_{}\), the tree distance \(_{}^{T}\) of Boker , the Euclidean distance of MPNNs' output, and tree homomorphism densities. These metrics arise from the topological variants of the \(1\)-WL test, fractional isomorphisms, MPNNs, and tree homomorphism counts.

**Theorem 1** (informal).: The following are equivalent for all graphons \(U\) and \(W\):

1. \(U\) and \(W\) are close in \(_{}\) (or alternatively \(_{}\)).
2. \(U\) and \(W\) are close in \(_{}^{T}\).
3. MPNN outputs on \(U\) and \(W\) are close for all MPNNs with Lipschitz constant \(C\) and \(L\) layers.
4. Homomorphism densities in \(U\) and \(W\) are close for all trees up to order \(k\).

Up to now, except for the connection between the tree distance and tree homomorphism densities by Boker , these equivalences were only known to hold on a discrete level where graphs are either exactly isomorphic or not. The "closeness" statements in the above theorem are epsilon-delta statements, i.e., for every \(>0\), there is a \(>0\) such that, if graphons are \(\)-close in one distance measure, they are \(\)-close in the other distance measures, where the constants are independent of the actual graphons. In particular, for graphs, these constants are independent of their number of vertices. Theorem 1 is formally stated and proved in Appendix C.4. A key point in the proof is to consider compact operators (graphons) as limits of graphs. Empirically, we verify our findings by demonstrating that untrained MPNNs yield competitive predictive performance on established graph-level prediction tasks. Further, we evaluate the usefulness of our derived metrics for studying different MPNN architectures. Our theoretical and empirical results also provide an efficient lower bound of the graph distances in Boker , Chen et al.  by using the Euclidean distance of MPNN outputs.

In summary, we quantify which distance MPNNs induce, leading to a more fine-grained understanding of their expressivity and separation capabilities. Our results provide a deeper understanding of MPNNs' capacity to capture graph structure, precisely determining when they can and when they cannot assign similar and dissimilar vectorial representations to graphs. _Our work establishes the first rigorous connection between the similarity of graphs and their learned vectorial presentations, paving the way for a more detailed understanding of MPNNs' expressivity and their connection to graph structure._

### Related work and motivation

In the following, we discuss relevant related work and provide additional background and motivation.

**MPNNs.** Following Gilmer et al. , Scarselli et al. , MPNNs learn a vectorial representation, i.e., a \(d\)-dimensional real-valued vector, representing each vertex in a graph by iteratively aggregating information from neighboring vertices. Subsequently, MPNNs compute a single vectorial representation of a given graph by aggregating these vectorial vertex representations. Notable instances of this architecture include, e.g., Duvenaud et al. , Hamilton et al. , and Velickovic et al. , which can be subsumed under the message-passing framework introduced in Gilmer et al. . In parallel, approaches based on spectral information were introduced in, e.g., Bruna et al. , Defferrard et al. , Gama et al. , Kipf and Welling , Levie et al. , and Monti et al. --all of which descend from early work in Baskin et al. , Goller and Kuchler , Kireev , Merkwirth and Lengauer , Micheli , Micheli and Sestito , Scarselli et al. , and Sperduti and Starita .

**Expressivity and limitations of MPNNs.** The _expressivity_ of an MPNN is the architecture's ability to express or approximate different functions over a domain, e.g., graphs. High expressivity means the neural network can represent many functions over this domain. In the literature, the expressivity of MPNNs is modeled mathematically based on two main approaches, algorithmic alignment with graph isomorphism test  and universal approximation theorems . Works following the first approach study if an MPNN, by choosing appropriate weights, can distinguish the same pairs of non-isomorphic graphs as the \(1\)-\(\) or its more powerful generalization the \(k\)-\(\). Here, an MPNN distinguishes two non-isomorphic graphs if it can compute different vectorial representations for the two graphs. Specifically, Morris et al.  and Xu et al.  showed that the \(1\)-\(\) limits the expressive power of any possible MPNN architecture in distinguishing non-isomorphic graphs. In turn, these results have been generalized to the \(k\)-\(\), see, e.g., Azizian and Lelarge , Geerts , Maron et al. , Morris et al. . Works following the second approach study, which functions over the domain of graphs, can be approximated arbitrarily close by an MPNN ; see also next paragraph. Further, see Appendix B for an extended discussion of related works about MPNNs' expressivity.

**Limitations of current universal approximation theorems for MPNNs.**_Universal approximation theorems_ assume that the domain of the network is a compact metric space and show that a neural network can approximate any continuous function over this space. Current approaches studying MPNNs' universal approximation capabilities employ the (graph) edit distance to define the metric on the space of graphs, e.g., see . However, the edit distance is not a natural notion of similarity for practical machine learning on graphs. That is, any two graphs on a different number of vertices are far apart, not fully reflecting the similarity of real-world graphs. More generally, the same holds for any pair of non-isomorphic graphs. Hence, any rewiring of a graph leads to a far-away graph. However, we would like to interpret the rewiring of a small number of edges in a large graph as a small perturbation close to the original graph. Additionally, since the edit metric is not compact, the Stone-Weierstrass theorem, the primary tool in universal approximation analysis, cannot be applied directly to the whole space of graphs. For example,  uses other non-compact metrics to circumvent this, artificially choosing a compact subset of graphs from the whole space by uniformly limiting the size of the graphs and the edge weights. Alternatively, Chen et al.  resorted to the graph signal viewpoint, allowing real-valued edge features, and showed that the algorithmic alignment of GNNs with graph isomorphism algorithms can be utilized to prove universal approximation theorems for MPNNs. In contrast, here we suggest using graph similarity measures from graphon analysis, for which simple graphs of arbitrarily different sizes can be close to each other and by which the space of all graphs is dense in the compact metric space of graphons, allowing us to use the Stone-Weierstrass theorem directly; see also Appendix B.2 for an extended discussion on graph metrics beyond the edit distance.

Figure 1: Illustration of the procedure to compute the distance \(_{}\) between graphs \(G\) and \(H\). Columns A and C show the colors obtained by \(1\)-\(\) iterations on graphs \(G\) and \(H\), respectively. Columns B and D show the iterated degree measures (IDMs) \(_{G,h}\) and \(_{H,h}\) for iterations \(h=1,2\) (see Eq. (1)), and the output distributions of iterated degree measures (DIMs) \(_{G}\) and \(_{H}\) (see Eq. (2)). Column E depicts the recursive construction to compute the distance \(_{}\) between the IDMs from columns B and D (outlined in Section 3, detailed in Appendix C.2.2).

**Graphon theory.** The book of Lovasz  provides a thorough treatment of _graphons_, which emerged as limit objects for sequences of graphs in the theory of dense graph limits developed by Borgs et al. [20; 21], Lovasz and Szegedy . These limit objects allow the completion of the space of all graphs to a compact metric space. When endowed with the _cut distance_, first introduced by Frieze and Kannan , the space of graphons is compact [78, Theorem \(9.23\)]. We can interpret graphons in two ways. First, as a weighted graph on the continuous vertex set \(\). Secondly, we can think of every point from \(\) as an infinite set of vertices and two points in \(\) as being connected by a random bipartite graph with edge density given by the graphon. This second point of view naturally leads to using graphons as generative models of graphs and the theory of graph limits. Our work follows the first point of view, and we emphasize that we _do not_ use graphons as a generative model. This also means that we do not need large graphs for the asymptotics to work since every graph--and, in particular, every small graph--is also a graphon. Grebik and Rocha  generalized the \(1\)-WL test and various of its characterizations to graphons, while Boker  did this for the \(k\)-WL test.

**Graphon theory in graph machine learning.** Keriven et al. , Maskey et al. , Ruiz et al.  use graphons to analyze graph signal processing and MPNNs. These papers assume a single fixed graphon generating the data, i.e., any graph from the dataset is randomly sampled from this graphon, and showed that spectral MPNNs on graphs converge to spectral MPNNs on graphons as the size of the sampled graphs increases. Maskey et al.  developed a generalization analysis of MPNNs, assuming a pre-defined finite set of graphons. Further, Keriven et al.  compared the expressivity of two types of spectral MPNNs on spaces of graphons, assuming graphons are Lipschitz continuous kernels. To that, the metric on the graphon space is taken as the \(L_{}\) distance between graphons as functions. However, the paper does not directly characterize the separation power of the studied classes of MPNNs, and it requires the choice of an arbitrary compact subset to perform the analysis. In contrast, in the current paper, we use graphon analysis to endow the domain of definition of MPNNs, the set of all graphs, with a "well-behaved" structure describing a notion of natural graph similarity and allowing us to analyze properties of MPNNs regardless of any model of the data distribution.

## 2 Background

Here, we provide the necessary background and define notation.

**Analysis.** We denote the Lebesgue measure on \(\) by \(\) and consider measurability w.r.t. the Borel \(\)-algebra on \(\). Let \((X,)\) be a standard Borel space, where we sometimes just write \(X\) when \(\) is understood and then use \((X)\) to explicitly denote \(\). For a measure \(\) on \(X\), we let \(\|\|(X)\) denote its _total mass_, and for a standard Borel space \((Y,)\) and a measurable map \(f X Y\), let the _push-forward_\(f_{*}\) of \(\)_via_\(f\) be defined by \(f_{*}(A)(f^{-1}(A))\) for every \(A\). Let \((X)\) and \(_{ 1}(X)\) denote the spaces of all probability measures on \(X\) and all measures of total mass at most one on \(X\), respectively. Let \(C_{b}(X)\) denote the set of all bounded continuous real-valued functions on \(X\). We endow \((X)\) and \(_{ 1}(X)\) with the topology generated by the maps \(_{}fd\) for \(f C_{b}(X)\), the _weak topology_ (_weak* topology_ in functional analysis); see [63, Section \(17.\)E] or [17, Chapter 8]. Then, \((X)\) and \(_{ 1}(X)\) are again standard Borel spaces, and if \(K\) is a compact metric space, then \((K)\) and \(_{ 1}(K)\) are compact metrizable; see [63, Theorem \(17.22\)]. For a sequence \((_{i})_{i}\) of measures and a measure \(\), we have \(_{i}\) if and only if \(_{X}fd_{i}_{X}fd\) for every \(f C_{b}(X)\), and for measures \(\), \(\), we have \(=\) if and only if \(_{X}fd=_{X}fd\) for every \(f C_{b}(X)\). In both statements, we may replace \(C_{b}(X)\) by a dense (w.r.t. the sup norm) subset. See Appendix A.3 for basics on topology. We denote a function \(f A B\) also by \(f(-)\) or \(f_{-}\) in case the evaluation of \(f\) on a point \(a A\) is denoted by \(f(a)\) or \(f_{a}\), respectively.

**Graphs and graphons.** A _graph_\(G\) is a pair \((V(G),E(G))\) with _finite_ sets of _vertices_ or _nodes_\(V(G)\) and _edges_\(E(G)\{u,v\} V(G) u v\}\). If not otherwise stated, we set \(n|V(G)|\), and the graph is of _order_\(n\). We also call the graph \(G\) an \(n\)-order graph. For ease of notation, we denote the edge \(\{u,v\}\) in \(E(G)\) by \(uv\) or \(vu\). The _neighborhood_ of \(v\) in \(V(G)\) is denoted by \(N(v)\{u V(G) vu E(G)\}\) and the _degree_ of a vertex \(v\) is \(|N(v)|\). Two graphs \(G\) and \(H\) are _isomorphic_ and we write \(G H\) if there exists a bijection \( V(G) V(H)\) preserving the adjacency relation, i.e., \(uv\) is in \(E(G)\) if and only if \((u)(v)\) is in \(E(H)\). Then \(\) is an _isomorphism_ between \(G\) and \(H\).

A _kernel_ is a measurable function \(U^{2}\), and a symmetric measurable function \(W^{2}\) is called a _graphon_. The set of all graphons is denoted by \(\). Graphons generalize graphs in the following way. Every graph \(G\) can be viewed as a graphon \(W_{G}\) by partitioning \(\) into \(n\) intervals\((I_{v})_{v V(G)}\), each of mass \(1/n\), and letting \(W_{G}(x,y)\) for \(x I_{u},y I_{v}\) be one or zero depending on whether \(uv\) is an edge in \(G\) or not. The _homomorphism density_\(t(F,W)\) of a graph \(F\) in a graphon \(W\) is \(t(F,W)_{^{V(F)}}_{j E(F)}W(x_{i},x_{j})\,d()\), where \(\) is the tuple of variables \(x_{v}\) for \(v V(G)\).

**The tree distance.** A graphon \(W\) defines an operator \(T_{W} L^{2}() L^{2}()\) on the space \(L^{2}()\) of square-integrable functions modulo equality almost everywhere by setting \((T_{W}f)(x)_{}W(x,y)f(y)\,d(y)\) for every \(x X\) and every \(f L^{2}()\). Boker  defined the _tree distance_ of two graphons \(U\) and \(W\) by \(_{}^{}(U,W)_{S}_{f,g}| f,(T_{U } S-S T_{W})g|\), where the supremum is taken over all measurable functions \(f,g\) and the infimum is taken over all _Markov operators_\(S\), i.e., operators \(S L^{2}() L^{2}()\) such that \(S(f) 0\) for every \(f 0\), \(S(_{})=_{}\), and \(S^{*}(_{})=_{}\) also for the _Hilbert adjoint_\(S^{*}\). Markov operators are the infinite-dimensional analog to doubly stochastic matrices, see  for a thorough treatment of Markov operators. One can verify that the tree distance is a lower bound for the _cut distance_.

**Iterated measures.** Here, we define _iterated degree measures (IDMs)_, which is basically a sequence of measures, by adapting the definition of Grebik and Rocha . Let \(_{0}\{1\}\) and inductively define \(_{h+1}_{ 1}(_{h})\) for every \(h 0\). Then, the spaces \(_{0},_{1},\) are all compact metrizable. For \(0 h<\), inductively define the _projection_\(p_{h+1,h}_{h+1}_{h}\) by letting \(p_{1,0}\) be the trivial map and, for \(h>0\), letting \(p_{h+1,h}()(p_{h,h-1})_{*}\) for every \(_{h+1}=_{ 1}(_{h})\). This extends to \(p_{h,}_{h}_{}\) for \(0 h<\) by composition in the obvious way. Let

\[_{}(_{h})_{h} _{h}_{h} p_{h+1,h}(_{h+1})=_{h} h}\]

be the _inverse limit_ of \(_{0},_{1},\); see the Kolmogorov Consistency Theorem [63, Theorem \(17.20\)]). Then, \(\) is compact metrizable [52, Claim \(6.2\)]. For \(0 h<\), let \(p_{,h}_{h}\) denote the projection to the \(h\)th component. We remark that we slightly simplified the definition of Grebik and Rocha  by not including previous IDMs from \(_{h^{}}\) for \(h^{} h\) in \(_{h+1}\) and directly defining \(\) as the inverse limit; corresponding to the definition of the space \(\) in . These changes yield equivalent definitions that simplify the exposition.

**The \(1\)-WL for graphons.** See Appendix A.1 for the standard definition of \(1\)-WL on graphs. Grebik and Rocha  generalized \(1\)-WL to graphons by defining a map \(_{W}\), mapping every point of the graphon \(W\) to an iterated degree measure as follows. First, inductively define the map \(_{W,h}_{h}\) by setting \(_{W,0}(x) 1\) for every \(x\) and

\[_{W,h+1}(x) A_{_{W,h}^{-1}(A)}W(x,y)d (y),\] (1)

for all \(x,A(_{h})\), and \(h 0\). Intuitively, \(_{W,h}(x)\) is the color assigned to point \(x\) after \(h\) iterations of \(1\)-WL. Observe that \(_{W,1}(x)\) encodes the degree of point \(x\), and \(_{W,h}(x)\) for \(h>1\) represents the iterated degree sequence information. Then, we define \(_{W}_{W,}\) by \(_{W}(x)_{W,}(x)_{h} _{W,h}(x)\) and let

\[_{W}_{W,}(_{W})_{*}()\] (2)

be the _distribution of iterated degree measures (DIM) of \(W\)_. In other words, \(_{W}(A)\) is the volume that the colors in \(A\) occupy in the graphon domain \(\). Then, the \(1\)-_WL test on graphons_ is the mapping that takes a graphon \(W\) and returns \(_{W}\). In addition to \(_{W}\), we also define \(_{W,h}(_{W,h})_{*}(_{h})\) for \(0 h<\), corresponding to running \(1\)-WL for \(h\) rounds.

While every DIDM of a graphon is a measure from the compact space \(()\), not every measure in \(()\) is the DIDM of a graphon. Grebik and Rocha  address this by giving a definition of a DIDM that is independent of a specific graphon. For us, it suffices to remark that the set \(_{h}\{_{W,h} W\}( _{h})\) is compact as it is the image of the compact space of graphons [78, Theorem \(9.23\)] under a continuous function . For us, this means that \(_{h}\) and \((_{h})\) can be used interchangeably in our arguments, and we do not have to be overly careful with distinguishing them. For simplicity, we simply stick to \((_{h})\) and refer to all elements of \((_{h})\) as _DIMs_.

**Message-passing graph neural networks.** MPNNs learn a \(d\)-dimensional real-valued vector for each vertex in a graph by aggregating information from neighboring vertices; see Appendix A.2 for more details. Here, we consider MPNNs where both the update functions and the readout functions are Lipschitz continuous and use sum aggregation normalized by the order of the graph. Formally, we first let \(=(_{i})_{i=0}^{L}\) denote a tuple of continuous functions \(_{0}^{0}^{d_{0}}\) and \(_{t}^{d_{t-1}}^{d_{t}}\) for \(t[L]\), where we simply view \(_{0}\) as an element of \(^{d_{0}}\). Furthermore, let \(\) denote a continuous function \(^{d_{L}}^{d}\). For a graph \(G\), an MPNN initializes a feature \(_{v}^{(0)}_{0}^{d_{0}}\). Then, for \(t[L]\), we compute \(_{-}^{(t)} V(G)^{d_{t}}\) and the single graph-level feature \(_{G}^{d}\) after \(L\) layers by

\[_{v}^{(t)}_{t}_{u N (v)}_{u}^{(t-1)}_{G} _{v V(G)}_{v}^{(L)} .\]

For a graphon \(W\), an MPNN initializes a feature \(_{x}^{(0)}_{0}^{d_{0}}\) for \(x\). Then, for \(t[L]\), we compute \(_{-}^{(t)}^{d_{t}}\) and the single graphon-level feature \(_{W}^{d}\) after \(L\) layers by

\[_{x}^{(t)}_{t}_{}W(x,y)_ {y}^{(t-1)}\,d(y)_{W} _{}_{x}^{(L)}\,d(x).\]

This generalizes the previous definition, i.e., for a graph \(G\) and its (induced) graphon \(W_{G}\), we have \(_{G}=_{W_{G}}\) and \(_{v}^{(t)}=_{x}^{(t)}\) for all \(t[L]\), \(v V(G)\), and \(x I_{v}\); see Appendix C.1.

We now extend the definition of MPNNs to IDMs. While the above definition of \(_{-}^{(t)}\) depends on a specific graphon \(W\), an IDM already carries the aggregated information of its neighborhood. Hence, the initial feature \(_{}^{(0)}_{0}^{d_{0}}\) for \(_{0}\) and \(_{-}^{(t)}_{t}^{d_{t}}\)_are defined for all IDMs at once_. The intuition is that MPNNs cannot assign at layer \(t\) different feature values to nodes that have the same color at step \(t\) of the graphon \(1\)-WL algorithm. Hence, it is enough to only consider an assignment between colors and feature values, and not consider the graph/graphon structure directly. Then, for a DIDM \((_{L})\), we define the single DIDM-level feature \(_{}^{d}\). Formally, we let

\[_{}^{(t)}_{t}_{_{t-1}} _{-}^{(t-1)}\,d _{}_{_{L}}_{-}^{(L) }\,d.\]

That is, messages are aggregated via the IDM itself. In addition to \(_{-}^{(t)}_{t}^{d_{t}}\), and \(_{-}(_{L})^{d}\), we define \(_{-}^{(t)}^{d_{t}}\) and \(_{-}()^{d}\) by setting \(_{}^{(t)}_{p_{,t}()}^{(t)}\) for every \(\) and \(_{}_{(p_{,L})_{+}}\) for every \(()\); it will always be clear from the context which of these functions we mean. These definitions of MPNNs on IDMs extend the previous definitions of MPNNs on graphons via the following identities. For a graphon \(W\), we have \(_{W}=_{_{W,L}}=_{_{W}}\) and \(_{x}^{(t)}=_{w_{t}(x)}^{(t)}=_{w(x)}^{(t)}\) for almost every \(x\); see Appendix C.1. That is, the feature values of an MPNN on a graphon \(W\) are equal to the feature values of that MPNN on the (D)IDM computed by the \(1\)-WL on \(W\). We call a tuple \(\) as defined above an _(L-layer) MPNN model_ if \(_{t}\) is Lipschitz continuous on \(\{_{_{t-1}}_{-}^{(t-1)}\,d _{t}\}\) for every \(t[L]\), and we call \(\) as defined above _Lipschitz_ if it is Lipschitz continuous on \(\{_{_{L}}_{-}^{(L)}\,d(_{L})\}\). We use \(\|-\|_{L}\) to denote the Lipschitz constants on these sets. In this paper, \(\) and \(\) always denote an MPNN model and a Lipschitz function, respectively. We use the term \(\)-layer MPNN model to refer to an \(L\)-layer MPNN model for an arbitrary \(L\).

## 3 Metrics on iterated degree measures

Chen et al.  recently introduced the _Weisfeiler-Leman distance_, a polynomial-time computable pseudometric on graphs combining the \(1\)-WL test with the well-known _Wasserstein metric_ from optimal transport , where their approach resembles that of iterated degree measures as introduced by Grebik and Rocha . To use metrics from optimal transport, Chen et al.  resorted to mean aggregation instead of sum aggregation to obtain probability measures instead of finite measures with total mass at most one. Using mean aggregation, however, is different from the \(1\)-WL test, which relies on sum aggregation. That is, sum aggregation allowed the algorithm to start with a constant coloring, something impossible with mean aggregation, potentially leading to a constant coloring. Chen et al.  circumvented this problem by encoding vertex degrees and the total number of vertices in the initial coloring.

Here, we show that the _Prokhorov metric_ and an unbalanced variant of the Wasserstein metric can be beautifully integrated into the theory of iterated degree measures, eliminating the need to work around the limits of mean aggregation. Both metrics metrize the weak topology, which is precisely the topology of the space \(_{h}\) of IDMs is endowed with; see Section 2. In modern-day literature, the Prokhorov metric is usually only defined for probability measures [35, Section \(11.3\)], yet the original definition by Prokhorov  already was for finite measures. That is, let \((S,d)\) be a complete separable metric space with Borel \(\)-algebra \(\). For a subset \(A S\) and \( 0\), let \(A^{}\{y S d(x,y)<x A\}\), and define the _Prokhorov metric_\(\) on \(_{ 1}(S)\) by

\[(,)\{>0(A)(A^{ })+(A)(A^{})+A \}.\]

As the name suggests, \(\) is a metric on \(_{ 1}(S)\)[99, Section \(1.4\)], and moreover, convergence in \(\) is equivalent to convergence in the weak topology [99, Theorem \(1.11\)]. For the _unbalanced Wasserstein metric_ let \(,_{ 1}(S)\), where we assume \(\|\|\|\|\) without loss of generality, and define

\[(,)\|\|-\|\|+_{(, )}_{S S}d(x,y)\,d(x,y),\]

where \((,)\) is the set of all measures \(_{ 1}(S S)\) such that \((p_{1})_{*}\) and \((p_{2})_{*}=\). Here, \(p_{1}\) and \(p_{2}\) are the projections from \(S S\) upon \(S\) to the first and the second component, respectively, i.e., \((p_{1})_{*}(A)=(A S)\) and \((p_{2})_{*}(A)=(S A)\). We prove that \(\) is a well-defined metric on \(_{ 1}(S,)\) that coincides with the Wasserstein distance [35, Section 11.8] on probability measures. Furthermore, it satisfies \((,) 2(,) 4(,)}\) for all \(,_{ 1}(S)\), which implies that it metrizes the weak topology; see Appendix C.2.

The metric \(\) is used to define the metric \(d_{,h}\) on \(_{h}\) for \(0 h\) as follows. Let \(d_{,0}\) be the trivial metric on the one-point space \(_{0}\) and, for \(h 0\), inductively let \(d_{,h+1}\) be the Prokhorov metric on \((_{h+1},\,d_{,h})\). Then, define \(d_{,}\) on \(_{}\) by setting \(d_{}(,) d_{,}(,) _{h} d_{,h}(_{h}, _{h})\) for \(,_{}\). The factor of \(1/h\) is included in this definition on purpose to ensure that \(d_{,}\) metrizes the product topology and not the uniform topology. The metric \(d_{,h}\) on \(_{h}\) is defined completely analogously via \(\) instead of \(\).

The metrics \(d_{,h}\) and \(d_{,h}\) on \(_{h}\) allow us, for example, to compare the IDM of a point in a graphon to the IDM of a point in another graphon. To compare two graphons' distributions on iterated degree measures, we let \(_{,h}\) be the Prokhorov metric on \(((_{h}),d_{,h})\) for \(0 h\) and again define \(_{,h}\) analogously via the distance \(\). We note that these metrics directly apply to graphons \(U,W\) by simply comparing their DIDMs \(_{U,h}\) and \(_{W,h}\).

**Theorem 2**.: Let \(0 h\). The metrics \(d_{,h}\) and \(d_{,h}\) are well-defined and metrize the topology of \(_{h}\). The metrics \(_{,h}\) and \(_{,h}\) are well-defined and metrize the topology of \((_{h})\). Moreover, these metrics are computable on graphs in time polynomial in the size of the input graphs and \(h\), up to an additive error of \(\) in the case of \(d_{,}\) and \(_{,}\).

While these metrics are polynomial-time computable, in Appendix C.2, we derive the same impractical upper bound of \((h n^{5} n)\) for \(_{,h}\) as Chen et al.  get for their Weisfeiler-Leman distance and the even worse bound of \((h n^{7})\) for \(_{,h}\). This means that these metrics are not suitable as a computational tool in practice. Theorem 1 hints that we can instead use easy-to-compute MPNNs to lower-bound these metrics, which leads to our experiments in Section 6.

MPNNs are Lipschitz in the metrics we defined, where the Lipschitz constant only depends on basic properties of the MPNN model. That is, if two graphons are close in our metrics, then MPNNs outputs for _all_ MPNN models up to a specific Lipschitz constant are close. Formally, let \(=(_{i})_{i=0}^{L}\) be an MPNN model with \(L\) layers, and for \(t\{0,,L\}\), let \(_{t}(_{i})_{i=0}^{t}\). Then, we inductively define the _Lipschitz constant_\(C_{} 0\) of \(\) by \(C_{_{0}} 0\) for \(t=0\) and \(C_{_{t}}\|_{t}\|_{L}(\|_{-}^{( t-1)}\|_{}+C_{_{t-1}})\) for \(t>0\). This essentially depends on the product of the Lipschitz constants of the functions in \(\), and the bounds for the MPNN output values, which are finite since a continuous function on a compact set attains its maximum. Including these bounds in the constant is necessary since we consider sum aggregation. That is, a constant function mapping all inputs to some \(c\) has Lipschitz constant zero, but when integrated with measures of total mass zero and one, for example, the difference of the outputs is \(c\). We define \(C_{(,)}\) for Lipschitz \(\) analogously by essentially viewing \((,)\) as an MPNN model.

**Lemma 3**.: Let \(\) be an \(L\)-layer MPNN model for \(L\) and \(\) be Lipschitz. Then,

\[\|_{}^{(L)}-_{}^{(L)}\|_{2} C_{} d_{,L}(,)\|_{ }-_{}\|_{2} C_{(,)}_{ ,L}(,)\]

for all \(,_{L}\) and all \(,(_{L})\), respectively. These inequalities also hold for \(d_{,}\) and \(_{,}\) with an additional factor of \(L\) in the Lipschitz constant.

Universality of message-passing graph neural networks

In this section, we prove a universal approximation theorem for MPNNs on IDMs and DIDMs, deriving our main result from it. For \(0 L\), let \(_{L}^{n} C(_{L},^{n})\) denote the set of all functions \(_{-}^{(L)}_{L}^{n}\) for an \(L\)-layer MPNN model \(\) with \(d_{L}=n\). Similarly, let

\[_{L}^{n}\{_{-}\ L^{d_{L}}^{n}\}  C((_{L}),^{n})\]

be the set of all functions computed by an MPNN after a global readout. Our universal approximation theorem, Theorem 4, shows that all continuous functions on IDMs and DIDMs, i.e., functions on graphons that are invariant w.r.t. the (colors of) the \(1\)-WL test, can be approximated by MPNNs. Hence, our result extends the universal approximation result of Chen et al.  for _measure Markov chains_ in two ways. First, measure Markov chains are restricted to finite spaces by definition, which is not the case for graphons and our universal approximation theorem. Secondly, the spaces \(_{L}\) and \((_{L})\) are compact, which means we obtain a universal approximation theorem for the whole space of graphons, including all graphs, not restricted to an artificially chosen compact subset.

**Theorem 4**.: Let \(0 L\). Then, \(_{L}^{1}\) is dense in \(C(_{L},)\) and \(_{L}^{1}\) is dense in \(C((_{L}),)\).

The proof of Theorem 4 is elegant and does not rely on encoding the \(1\)-WL test as an MPNN. That is, it follows by inductive applications of the Stone-Weierstrass theorem [35, Theorem \(2.4.11\)] combined with the definition of IDMs. It is strikingly similar to the proof of Grebik and Rocha  for a similar result concerning tree homomorphism densities; see Appendix D.

While the second statement of Theorem 4, i.e., the graphon-level approximation, is interesting in its own right, the crux of Theorem 4 lies in its first statement, namely, that \(_{L}^{1}\) is dense in \(C(_{L},)\), immediately implying that the topology induced by MPNNs on \((_{L})\) is the weak topology, i.e., the topology we endowed this space within Section 2.

**Corollary 5**.: Let \(0 L\) and \(n>0\). Let \((_{L})\) and \((_{i})_{i}\) be a sequence with \(_{i}(_{L})\). Then, \(_{i}\) if and only if \(_{_{i}}_{}\) for all \(L\)-layer MPNN models \(\) and Lipschitz \(^{d_{L}}^{n}\).

By combining standard compactness arguments with Theorem 2 and Corollary 5, we can now prove that two graphons are close in our metrics if and only if the output of all possible MPNNs, up to a specific constant and number of layers, is close. Formally, the forward direction of this equivalence is just Lemma 3, while the backward direction reads as follows.

**Theorem 6**.: Let \(n>0\) be fixed. For every \(>0\), there are \(L,C>0\), and \(>0\) such that, for all graphons \(U\) and \(W\), if \(\|_{U}-_{W}\|_{2}\) for every \(L^{}\)-layer MPNN model \(\) and Lipschitz \(^{d_{L^{}}}^{n}\) with \(L^{} L\) and \(C_{(,)} C\), then \(_{}(U,W)\).

We stress that the constants \(L\), \(C\), and \(\) in the theorem statement are independent of the graphons \(U\) and \(W\). The proof of Theorem 6 is simple: one assumes that the statement does not hold to obtain two sequences of counterexamples, which have to have convergent subsequences by compactness, and the limit graphons allow us to derive a contradiction. This establishes the equivalence between our metrics and MPNNs stated in Theorem 1. Analogous reasoning together with the universality of tree homomorphism densities , cf. Appendix D, yields the equivalence between our metrics and tree homomorphism densities. Then, the missing equivalence to the tree distance follows from the result of Boker , which connects the tree distance to tree homomorphism densities. See Appendix C.4 for the formal statements of all these equivalences as epsilon-delta-statements and their proofs.

## 5 Extension to graphons with signals

Our focus in this work lies on MPNNs' ability to distinguish the structure of graphons. However, the definitions of IDMs, MPNNs, and our metrics can be adapted to graphons with signals, i.e., graphons \(W\) equipped with a measurable _signal function_\( K\), where \((K,d)\) is some fixed compact metric space. In the following, we briefly sketch how to do this. First, replace the one-point space \(_{0}\) by \((K,d)\) and modify the \(1\)-WL for graphons by setting \(i_{(W,),0}\), i.e., use the signal function as the initial coloring. For \(h>0\), adapt the definition of the IDM spaces \(_{h}\) and of the refinement rounds \(i_{(W,),h}\) of the \(1\)-WL for graphons to include the previous IDM of a point in its new IDM, like in the original definition of Grebik and Rocha . Omitting the previous IDM, as we have done before in our definition, is only reasonable if the initial coloring is constant. Then, since \(_{0}=(K,d)\) is a compact metric space,the spaces \(_{h}\) are compact metrizable as before. Modify the definition of an MPNN model \(\) such that \(_{0}\) is a Lipschitz function \(K^{d_{0}}\). Finally, adapt the definition of the metrics \(d_{,h}\) and \(d_{,h}\) by letting \(d_{,0} d_{,0} d\) be the metric of the compact metric space \((K,d)\) and then adapting \(d_{,h}\) and \(d_{,h}\) for \(h>0\) to the modified definition of \(_{h}\) by using the product metric. Then, the proofs of our results can be adapted to this more general setting. In particular, one can prove a universal approximation theorem since Lipschitz functions on \(K\) are dense in \(C(K)\).

## 6 Experimental evaluation

In the following, we investigate the applicability of our theory on real-world prediction tasks. Specifically, we answer the following questions.

1. To what extent do our graph metrics \(_{}\) and \(_{}\) act as a proxy for distances between MPNNs' vectorial representations?
2. Our theoretical results imply that untrained MPNNs can be as effective as their trained counterparts when using _enough_ of them. Can untrained MPNNs remain competitive when only using a finite number of them (measured by the hidden dimensionality)?

The source code of all methods and evaluation protocols are available at https://github.com/nhuang37/finegrain_expressivity_GNN. We conducted all experiments on a server with 256 GB RAM and four NVIDIA RTX A5000 GPU cards.

**Fine-grained expressivity comparisons of MPNNs.** To answer **Q1**, we construct a graph sequence that converges in our graph metrics. Given an MPNN, we compute the sequence of its embeddings on such graphs and the corresponding embedding distance using the \(_{2}\)-norm. Hence, comparing different MPNNs amounts to comparing the convergence rate of their Euclidean embedding distances concerning the graph distances. Concretely, we simulate a sequence of \(50\) random graphs \(\{G_{i}\}\) for \(i\) with \(30\) vertices using the stochastic block model, where \(G_{i}(p,q_{i})\), with \(p=0.5\) and \(q_{i}[0.1,0.5]\) increases equidistantly. Let \(G\) denote the last graph in the sequence, and observe that \(G\) is sampled from an Erdos-Renyi model, i.e., \(G(p)\). For \(i\), we compute the Wasserstein distance \(_{,h}(G_{i},G)\) and the Euclidean distance \(\|_{G_{i}}-_{G}\|_{2}\).1 For demonstration purposes, we compare two common MPNN layers, GIN  and GraphConv , using sum aggregation normalized by the graph's order, varying the hidden dimensions and the number of layers.

Figure 2 visualizes their normalized embedding distance and normalized graph distance, with an increasing number of hidden dimensions, from left to right. GIN, top-row, and GraphConv, bottom-row, produce more discriminative embeddings as the number of hidden dimensions increases, supporting Theorem 4. Each point corresponds to a different (untrained) MPNN. Note that we interpret the hidden dimension \(w\) (i.e., width) as concatenating \(w\) random MPNNs. We observe similar behavior when increasing the number of layers; see Figure 3 in the appendix. Untrained GraphConv embeddings are more robust than untrained GIN embeddings regarding the choice of hidden dimensions and number of layers. Figure 4 in the appendix shows the same experiments on the real-world dataset Mutag, part of the TUDataset . We observe that increasing the number of hidden dimensions improves performance. Nonetheless, increasing the number of layers seems to first improve and then degrade performance. This observation coincides with the downstream graph classification performance, as discussed in the next section.

**The surprising effectiveness of untrained MPNNs.** To answer **Q2**, we compare popular MPNN architectures, i.e., GIN and GraphConv, with their untrained counterparts. For untrained MPNNs, we freeze their input and hidden layer weights that are randomly initialized and only optimize for the output layer(s) used for the final prediction. We benchmark on a subset of the established TUDataset . For each dataset, we run _paired_ experiments of trained and untrained MPNNs on the same ten random splits (train/test) and 10-fold cross-validation splits, using the evaluation protocol outlined in Morris et al. . We report the test accuracy with 10-run standard deviation in Table 1 and the mean training time per epoch with standard deviation in Table 2 in the appendix. Table 1 and Table 2 show that untrained MPNNs with sufficient hidden dimensionality perform competitively as trained MPNNs while being significantly faster, with 20%-46% time savings. As shown in Figure 6 in the appendix, increasing the hidden dimension (i.e., the number of MPNN models) improves the performance of untrained MPNNs. Our theory states that it is necessary to use all MPNNs to preserve graph distance, which is impossible to compute in practice. Nonetheless, our experiment shows that using enough of them suffices for graph classification tasks.

We also investigate the effect of the number of layers, i.e., the \(1\)-\(\) iteration in IDM. As shown in Figure 7 in the appendix, increasing the number of layers first improves and then degrades untrained MPNNs' performance, which is likely due to the changes in MPNNs' ability to preserve graph distance observed as in Figure 5 in the appendix.

## 7 Conclusion

This work devised a deeper understanding of MPNNs' capacity to capture graph structure, precisely determining when they learn similar vectorial representations. To that, we developed a comprehensive theory of graph metrics on graphons, demonstrating that two graphons are close in our metrics if, and only if, the outputs of all possible MPNNs are close, offering a more nuanced understanding of their ability to capture graph structure similarity. In addition, we established a connection between the continuous extensions of \(1\)-\(\) and MPNNs to graphons, tree distance, and tree homomorphism densities. Our experimental study confirmed the validity of our theory in real-world prediction tasks. _In summary, our work establishes the first rigorous connection between the similarity of graphs and their learned vectorial presentations, paving the way for a more nuanced understanding of MPNNs' expressivity and robustness abilities and their connection to graph structure._

Looking forward, future research could focus on extending all characterizations of Theorem 1 to graphons with signals. While we briefly sketched in Section 5 how to do this for MPNNs and the metrics we defined, this still presents a challenge as tree distance and tree homomorphism densities do not readily generalize to graphons with signals. A different direction could be an extension of our theory to different aggregation functions like max- or mean-aggregation. Since the \(1\)-\(\) paradigm of summing over neighbors is crucial in our proofs, it is not clear how one would approach this. Additionally, further quantitative versions of equivalences in Theorem 1, not resorting to epsilon-variants statements, and generalizing our results to the \(k\)-\(\) are interesting avenues for future exploration.

  
**Accuracy \(\)** & \(_{0}\) & \(\) & \(\) & \(\) & Protens & Redet-Binary \\  GNN-m (trained) & 79.01 \(\) 2.34 & 69.96 \(\) 1.40 & 46.29 \(\) 0.76 & **78.61 \(\) 34** & **73.51 \(\) 0.6** & **89.73 \(\) 4.7** \\ GNN-m (trained) & **25.86 \(\) 3.32** & **79.70 \(\) 1.40** & **47.57 \(\) 0.85** & 73.72 \(\) 0.75 & 73.45 \(\) 0.8 & **82.32 \(\) 0.4** \\  GraphConv-m (trained) & **81.68 \(\) 2.18** & 99.14 \(\) 1.39 & 38.15 \(\) 1.62 & **63.88 \(\) 0.66** & **71.99 \(\) 0.67** & **82.44 \(\) 0.78** \\ GraphConv-m (trained) & 78.03 \(\) 1.57 & **65.77 \(\) 1.33** & **43.29 \(\) 0.86** & 62.36 \(\) 0.45 & **71.83 \(\) 0.4** & 77.15 \(\) 0.29 \\   

Table 1: Untrained MPNNs show competitive performance as trained MPNNs given sufficiently large hidden dimensionality (\(3\)-layer, \(512\)-hidden-dimension). To be consistent with our theory, we use standard architectures with sum aggregation, layer-wise \(1/V(G)\) normalization, and mean pooling, denoted by “MPNN-m.” We report the mean accuracy \(\) std over ten data splits.

Figure 2: MPNNs preserve graph distance better when increasing the number of hidden dimensions. Comparatively, untrained GIN embeddings are more sensitive than untrained GraphConv to changes in the number of hidden dimensions.