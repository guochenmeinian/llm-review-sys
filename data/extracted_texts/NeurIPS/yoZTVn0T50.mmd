# CaMP: Causal Multi-policy Planning for Interactive Navigation in Multi-room Scenes

Xiaohan Wang

Institute of Artificial Intelligence and Robotics

Xi'an Jiaotong University

wuhanvzh2016@stu.xjtu.edu.cn

&Yuehu Liu

Institute of Artificial Intelligence and Robotics

Xi'an Jiaotong University

liuyh@mail.xjtu.edu.cn

&Xinhang Song

Institute of Computing Technology

Chinese Academy of Sciences

University of Chinese Academy of Science

xinhang.song@vipl.ict.ac.cn

&Beibei Wang

Institute of Artificial Intelligence and Robotics

Xi'an Jiaotong University

wangbb@stu.xjtu.edu.cn

&Shuqiang Jiang

Institute of Computing Technology

Chinese Academy of Sciences

University of Chinese Academy of Science

sqjiang@ict.ac.cn

###### Abstract

Visual navigation has been widely studied under the assumption that there may be several clear routes to reach the goal. However, in more practical scenarios such as a house with several messy rooms, there may not. Interactive Navigation (InterNav) considers agents navigating to their goals more effectively with object interactions, posing new challenges of learning interaction dynamics and extra action space. Previous works learn single vision-to-action policy with the guidance of designed representations. However, the causality between actions and outcomes is prone to be confounded when the attributes of obstacles are diverse and hard to measure. Learning policy for long-term action planning in complex scenes also leads to extensive inefficient exploration. In this paper, we introduce a causal diagram of InterNav clarifying the confounding bias caused by obstacles. To address the problem, we propose a multi-policy model that enables the exploration of counterfactual interactions as well as reduces unnecessary exploration. We develop a large-scale dataset containing 600k task episodes in 12k multi-room scenes based on the ProcTHOR simulator and showcase the effectiveness of our method with the evaluations on our dataset.

## 1 Introduction

Embodied AI, incorporating internet intelligence such as vision, language, and reasoning into an embodiment, aims to deploy autonomous agents in real-world environments . In Visual Navigation, a cornerstone task, agents navigate to the goal in static environments where the movement of scene objects is avoided. In a practical scenario an indoor robot may need to navigate in a house with multiple rooms and get blocked by obstacles(see Figure 1). Interactive Navigation (InterNav) considers navigating more efficiently with object interaction rather than merely moving forward and turning around.

Years of visual navigation exploration witnessed the growing interest in long-horizon navigation in complex environments . Zeng et al. build a dataset benchmarking interactive navigation task on the AI2-THOR simulator, where agents receive visual observation and output the sequence of discrete actions. New challenges are raised compared to visual navigation: First, the agent needs to understand the outcome of object interaction. Second, the need for interaction with obstacles leads to a larger action space for reinforcement learning (RL). However, the dataset is limited to a simple scenario where a few objects are lined up between the starting and target point in a single room. Navigation in multi-room scenes requires agents to plan longer action sequences and understand more diverse states of the environment.

Since the dynamics of object interaction follow the physical rules that are structurally invariant in the environment, there naturally exists a cause-effect relation between the action and its outcome. From the perspective of causality, an RL agent performs interventions to the environment (by taking actions) and estimates the effect (rewards) in order to learn a policy that maximizes the long-term reward. However, learning causality through RL training is challenging due to the existence of unobserved confounders (UCs), which affect both the decision-making of actions and the generation of rewards . In InterNav, the agent may decide to navigate or interact based on whether it is blocked by an object (referred to as an obstacle) whose distribution is unobserved, namely unmeasurable. The existence or type of obstacles is also decisive to the outcome (see Figure 1). For instance, when blocked by a desk the agent may choose to push it aside since empirically it leads to a high reward, while the interaction will turn out otherwise if the desk gets stuck. Without knowing the true causality in interaction, the agent is prone to learn a sub-optimal policy. Prior work  on InterNav attempts to predict the change of object pose as a representation to understand the interaction outcomes. The model performance relies on effective obstacle detection and being familiar with obstacle distribution, which may not work in complex scenes with various obstacles. Long-term planning in complex environments also leads to inefficient exploration. Moving forward at blocked positions or trying to interact with no object around makes no difference to the environment and the agent can learn little about the causality from those experiences. Common solutions to visual navigation  and InterNav  train single policy with end-to-end RL, which requires extensive training to learn a universal policy tackling various scenarios. However, an agent may come up with multiple intents but can only conduct one behavior at once due to its embodiment. In a short-term view, the agent can either interact with objects or navigate somewhere. Hence it is more efficient to learn separate policies for interaction and navigation.

Figure 1: Illustration of InterNav in a 4-room scene. Trying to reach the goal as a blue circle, the agent is blocked by obstacles in yellow boxes spawned in three rooms. The agent needs to make decisions at three blocked positions marked in blue boxes: to bypass or to interact with obstacles? The outcomes depend on the interactive attributes of obstacles (whether they can be pushed away or picked up, e.g. heavy, small, etc.). The green arrow is a path the agent may take to reach the target and the nodes denote interaction with obstacles. Markings in colors are invisible to the agent.

In this paper, we introduce a structural causal model, analyzing InterNav through causal lenses and clarifying the key challenge as the confounding bias caused by obstacles. We show that learning a policy conditioned on its primary decision (referred to as "intent") can enable the agent to learn from counterfactual experiences and make better decisions. As a solution to InterNav, we propose a Causal Multi-Policy model (CaMP), which reduces unnecessary exploration by breaking down the task into simple sub-tasks and enables counterfactual exploration by trailing conditioned on the agent's intent. In particular, three action policies (i.e. navigation, pushing interaction, and picking interaction) are developed by training three actor-critic networks in designed auxiliary tasks. We then train a global policy that receives both observation and action intent and calls action policies in sequence. An intent policy is developed to duplicate the decision of the global policy so that action policies can be called to produce their action intents. To study the InterNav in more practical scenarios, we collect a dataset containing 600k task episodes with various obstacle arrangements, based on the ProcTHOR simulator  of large-scale multi-room scenes. We also introduce a new metric measuring the time efficiency of task completion due to the need for interaction in InterNav. Evaluations on our dataset showcase the effectiveness of the proposed model CaMP.

## 2 Related Works

**Interactive Navigation** comes up as the downstream task of visual navigation, which has been extensively studied in Embodied AI literature in the past decade. Most works address PointNav [5; 27; 35] and ObjNav [4; 6; 44; 42; 43], while others develop Visual-language Navigation [1; 29; 33; 21], AudioNav [8; 7], etc that bring in new intelligence components. Environments of sim-to-real  or large-scale  indoor scenes are also proposed for more complexity. Zeng et al.  focus on vision-based agents interacting with directional pushes on iTHOR environment . However, they consider elementary scenarios where obstacles are spawned in a single room for agents to push out of the way. In this work we construct a new dataset based on ProcTHOR environment  that provides 12k multi-room scenes with diverse floorplans and various assets, supporting large-scale training.

**Causal Reinforcement Learning.** Causal inference [24; 25; 26] has been a longstanding methodology adopted to pursue the causal effect. Recent years have witnessed the progress of applying causal theory in computer vision tasks including visual question answering [23; 37; 9], scene graph generation [32; 10], visual recognition [22; 31; 37; 38], etc. Causal reinforcement learning (CRL) [18; 17; 40; 3; 41] is motivated by the challenges of RL including data inefficiency and lack of interpretability, and has been applied in traditional RL tasks such as multi-armed bandit problem , Markov decision process . Naturally, the causal bias also exists in Embodied AI tasks, but there exists little work that helps train RL agents to complete tasks applying the theory of causal reasoning. We make the first attempt to model the causalities among the interactive navigation and address the confounding bias of obstacles by implementing a counterfactual policy.

**Hierarchical Models for Embodied AI.** To address complex and long-horizon tasks, one of the most popular solutions is the hierarchical approach inspired by Hierarchical Reinforcement learning (HRL) [30; 15]. The main idea is to divide the whole task into solvable sub-tasks through temporal abstraction which allows taking actions at different time scales . Previous attempts applied on embodied question answering [11; 12], and interactive question answering  mainly follow the flavor of finding sub-goals for the low-level module to achieve. However, learning policy associated with sub-goals can be data expensive and requires much handcraft design for vision-based tasks. Our model is different from those works following the idea of the option framework  that develops multiple policies for different task situations without the need for any sub-goals.

## 3 A Causal View of InterNav

We start by grounding the problem of InterNav through the causal lens and clarify the challenges brought by obstacles. Then in the face of the confounding bias of obstacles, we let the agent learn counterfactual policy conditioned on its original intent so that it can gain experiences of counterfactual situations and make better decisions.

### Structural Modelling

We first formulate the underlying problem as a time-discrete Markov Decision Process (MDP) defined by a tuple \( S,A,T,R,\), where \(S\) is the state space, \(A\) is the action space, \(T:S A S\) is a transition function, \(R\) is a reward function, and \(\) is the discount factor. At each step \(t\), the agent observes state \(s_{t}=(m_{t},p_{t})\) representing an egocentric visual observation and a goal coordinate, executes an action \(a_{t} A\), and receives a reward \(r_{t}\). Here, the action space \(A=\{\)_MoveAhead, RotateLeft, RotateRight, LookUp, LookDown, PushAhead, PushBack, PushLeft, PushRight, PickUp, Drop, Done_\(\}\).

We then analyze the above MDP with the form of Structural Causal Model (SCM, see Figure 2). Formally, the SCM \(M\) is defined by a tuple \( O,V,F,P(u)\), where \(V=S A R\) is a set of observable variables, \(O\) denotes the obstacles that hinder the navigation of agents, \(F=\{f_{s},f_{a},f_{r}\}\) is the set of structural functions relative to \(V\), and \(P(o)\) is the probability distribution over obstacles which is unobserved in InterNav. In particular, objects that currently serve as obstacles are decided by the environmental state such as the layout of surroundings (S\(\)O). At step \(t\), action \(a_{t}=f_{a}(s_{t},o_{t})\) is decided by the agent based on state observation and its recognition of obstacles (S\(\)A\(\)O). Practically, a stochastic policy \((a_{t}|s_{t}):S A\) is learned to cover the process. Note that although \(S\) denotes the state encompassing all information of the environment, it causally influences agent's actions in a partially observable way through visual observations. Reward \(r_{t}=f_{r}(s_{t},a_{t},o_{t})\) is returned by the environment based on state, action and obstacles to which the action is applied (S,A,O\(\)R). The current state \(s_{t}=f_{s}(s_{t-1},a_{t-1})\) is decided by the state and action at last time step. The goal of the agent is to learn a policy \(^{*}=argmaxV_{}(s_{t})\) that maximizes the expected cumulative reward:

\[V_{}(s_{t})=[_{k=0}^{}^{k}r_{t+k+1}|s_{t}]\] (1)

which is formulated in the form of state-value function \(V_{}\) and \(\) is the discount factor.

Through interaction with the environment, the agent performs intervention \(do(A=a)\), that actively sets \(A\) as \(a\) rather than passively observes when \(A\) appears to be \(a\), and learns from the effect which can be written probabilistically as \(P(R=r|do(A=a),S=s)\). Since obstacles influence both the decision of actions and the generation of rewards (A\(\)O\(\)R, see dashed arrows in Figure 2(a)), the causal relation between A and R is confounded such that \(P(r|do(a)) P(r|a)\), to which the _confounding bias_ refers . During RL training, the model estimates spurious value of actions (in the form of reward) due to the confounding bias which may result in sub-optimal policy. For example, frequent encounters with heavy obstacles (e.g. chairs, tables) make the agent predict a higher value of pushing them than picking them up. The agent can get familiar with limited distribution \(P(o)\) with extensive training. However, without considering the causalities concerning obstacles (A\(\)O\(\)R), it's hard to generalize to unseen environments or large-scale datasets.

### Counterfactual Decision-making

The underlying confounding bias can be tackled with the ability of counterfactual decision-making. A counterfactual effect \(P(R_{A=a}|A=a^{})\) of taking action can be read as "The reward the agent

Figure 2: Causal diagram of proposed SCM. The part in gold of (b) denotes the causal entity of the counterfactual policy. The action policy \(\) shares the same architecture with the intent policy so that they can receive the state observation in the same form.

would obtain had it taken action \(A=a\) (contrary to the fact), given \(A=a^{ n}\), which is imaginary thus usually not estimable from data. However, Bareinboim et al.  show that the above counterfactual quantity can be estimated if the action is interrupted before its decision is executed (ETT, Effect of Treatment on the Treated). We call the decision before execution as agent's intent \(I=i_{t}=f_{i}(s_{t},o_{t})\) which shares the same causal parents of action \(A\). Our goal is to learn a counterfactual policy \(^{*}_{clf}=argmaxV_{_{clf}}(s^{}_{t})\) (see Figure 2(b)) that maximizes the long-term value conditioned on the intent-specific state \(s^{}_{t}=(s_{t},i_{t})\):

\[V_{_{clf}}(s^{}_{t})=[_{k=0}^{}^{k}r_{t+k+1 }|s_{t},i_{t}]\] (2)

The action distribution of the counterfactual policy \(P(a_{t}|i_{t})=_{cft}(s_{t},i_{t})\) is posterior to the intent distribution \(P(i_{t})=(s_{t})\). Thus the agent can obtain both experimental experiences (ones the original policy "would have collected" when \(a_{t}=i_{t}\)) and counterfactual experiences (when \(a_{t} i_{t}\)) that explore external strategies. The intent also provides context about the obstacles due to their causal relation. Thus the counterfactual policy obtains more value than standard policy :

\[V_{^{*}_{clf}}(s_{t},i_{t}) V_{^{*}}(s_{t})\] (3)

where equality does not hold when unobserved confounders (obstacles) exist. Intuitively, an agent that explores different counterfactual situations (e.g. "what if I push/pick up the box instead of bypassing it?") has a better understanding of the expected value of the current state, compared to the agent taking actions out of intuition. We illustrate the idea of how we implement a counterfactual policy in Figure 2(c).

## 4 The Proposed Solution

In this section, we first introduce a hierarchical framework for multiple policies to cooperate. Then we implement a global policy with the ability of counterfactual decision-making which plans three action policies based on the integrated intent of the model. The model architecture is shown in Figure 3.

### Hierarchical Policy Framework

Essentially, we reduce the task complexity following a prior understanding that the InterNav task is temporally separable. 3 action policies are defined in Table 1 to split the action space. Policy

Figure 3: Model overview. Our model CaMP is composed of a global policy, three action policies, and an intent policy. Given an observation, the intent policy predicts a policy \(^{}_{t}\) and low-level policies predict an action \(a^{}_{t}\), which is then weighted by the policy distribution as the integrated intent \(i_{t}\) (equation (7)). Finally the global policy decides the policy \(_{t}\) based on the state representation and the intent, and the chosen policy executes the action \(a_{t}\).

_Navigate_ focuses on the ability of point navigation without considering the environment dynamics. While _Push_ and _Pick_ policies aim for the ability of object manipulation according to the goal and agent position. Formally, assume action policy \(_{}\) is called at step \(t\) mapping the transition from state observation \(s_{t}\) to the distribution of action \(a_{t}\):

\[P(a_{t})=_{}(s_{t},h_{t-1}),a_{t}_{}\] (4)

where \(h_{t-1},_{}\) denotes the recurrent hidden state and the action space of policy \(_{}\). In our model, \(s_{t}\) is the concatenation of goal embedding and visual features extracted with a convolutional neural network. \(_{}\) is implemented with a GRU and two linear layers for both policy (actor) and value (critic). We pretrain each action policy with an auxiliary task designed according to their objectives.

With the short-term abilities of action policies, a global policy \(_{}\) aims to plan a sequence of policies \(_{t}=\{\)_Navigate, Push, Pick, Done_\(\}\) for the whole task completion. We let the agent follow the _Navigate_ policy by default and the global policy decides whether to interact during the long horizon navigation. Once an interaction policy is called, it does not return the control until the sub-task termination (output _Done_). \(_{}\) shares the same model architecture with \(_{}\).

### Intent-aware policy planning

Since the global policy plans when to interact with obstacles, it's crucial to learn action-outcome causality (e.g. whether calling _push_ policy results in a clear path). Moreover, unlike low-level policies, the global policy does not receive direct rewards for its decisions during training, which makes their causal relation harder to learn. Thus we introduce a counterfactual global policy which makes plans based on the theory in section 3.2, in order to enable more effective policy learning. The intent of the agent is combined with its observation as the intent-specific state that is fed to the global policy. Instead of planning policies based on its own intent of policy, \(_{}\) is provided with the integrated intent of basic actions \(i_{t}\):

\[P(_{t})=_{}(s_{t},P(i_{t}),h_{t-1})\] (5) \[P(^{}_{t})=^{}_{}(s_{t},P(}),h_{t-1})\] (6) \[P(i_{t})=_{^{}}_{^{}}(s_ {t},h_{t-1}) P(^{}_{t}=^{j})\] (7)

where \(^{}_{}\) is the _intent policy_ that duplicates the decision-making of global policy and \(}\) is a plane vector denoting an intent without any inclination. Each action intent (zero-padded to the same size) produced by policy \(_{^{j}}\) is weighted by the probability that the decision of \(^{}_{}\) is \(^{j}\). Note that \(P(i_{t})\) is encoded with a linear function and concatenated to the observation \(v_{t}\) as the input of GRU. By providing information of the intents of low-level policies, the new context bridges the relation between actions and rewards for the global policy.

We train the model with Proximal Policy Optimization (PPO)  algorithm which updates the model parameters for \(k\) iterations with a rollout of data. To ensure that the intent represents the decision of the agent, we synchronize the parameters of two networks (\(^{}_{}_{}\)) during training. Since the global policy does trials conditioned on the distribution of model intent, similar distributions of actions and intents are more likely to be sampled identically (experimental case, \(a_{t}=i_{t}\)). On the contrary, the distinctive distributions of actions and intents lead to more counterfactual cases (\(a_{t} i_{t}\)), which brings entropy increase for the system. In other words, the agent tends to exploit its internal knowledge about influencing UCs (obstacles) receiving its own intent, while exploring the external space of policy when setting off from the old intent. Since the global policy is becoming distinct from the intent policy after iterations, we set the synchronization frequency as \(K\) rollouts to balance the exploration and exploitation of policy learning.

  
**Policy** & **Action space** & **Objective** \\   & _MoveAhead, RotateLeft, RotateRight, LookUp, LookDown, Done_ & Navigating to the goal point within a distance of 0.2m. \\   & _PushAhead, PushBack, PushLeft, PushRight, Done_ & Moving obstacles until there is a reachable path towards the goal or the existing path gets shorter. \\   & _PickUp, Drop, RotateLeft, RotateRight, Done_ & Picking and dropping small obstacles until there is a reachable path towards the goal or the existing path gets shorter. \\   

Table 1: Descriptions of action policies

## 5 Experiments

### Experiment Setup

**Data collection.** We perform experiments with the ProcTHOR simulator  on our dataset, which contains 600k task episodes in 12k scenes (50 per scene). For each episode, we first randomly set the starting and target points in two different rooms. Second, we calculate all room paths (e.g. room1\(\)room4\(\)room7) and all node paths (e.g. coordinates of start\(\)door3\(\)target). At last we randomly pick obstacle assets (655 in total) at random positions between adjacent nodes until there is no clear path or lengthen the shortest path (50% probability each). The dataset is split by scenes so the models are evaluated in unseen scenes: 11.8k training scenes, 100 validation scenes (5k episodes), and 100 testing scenes (500 episodes). In Table 2 the statistics of our dataset show that the agent is challenged by multiple obstacles, long-distance navigation, and multi-door passing. Figure 4 shows several top-view examples of our dataset.

**Environment settings.** Following the standard settings, we let _MoveAhead_ move the agent ahead by 0.25 meters, _RotateRight_ and _RotateLeft_ change the agent's azimuth angle by \(\)90 degrees, _LookUp_ and _LookDown_ rotate the agent's camera elevation angle by \(\)30 degrees. The _DirectionalPushs_ let the agent push (along \(\)z and \(\)x axis) the closest visible object with a constant force. The agent takes the _END_ to indicate that it has completed an episode.

**Evaluation metrics.** We adopt Success Rate (SR), Final Distance to Target (FDT), and Success weighted by Path Length (SPL). SR is the ratio of successful episodes in total episodes, FDT is the average geodesic distance between the agent and the goal when the episode is finished, and SPL is calculated as \(_{n=1}^{N}Suc_{n}}{max(P_{n},L_{n})}\), where \(N\) is the total episodes number, \(Suc_{n}\) is the successful indicator of \(n\)-th episode, \(L_{n}\) is the shortest path length, and \(P_{n}\) is the length of the real path.

   Dataset splits & ratio & obstacle & distance(m) & cross-room \\ (room num) & ratio & num & num \\ 
1\(\)2 & 39.0\% & 2.05 & 5.45 & 1.44 \\
3\(\)5 & 42.3\% & 3.37 & 9.92 & 2.50 \\
6\(\)10 & 18.7\% & 4.58 & 14.67 & 3.29 \\   

Table 2: Statistics of proposed dataset split by the number of rooms. The ratio of each split, number of spawned obstacles, initial distance between start and target, and number of times the agent needs to across the room are counted.

Figure 4: Top view examples of our dataset. Blue circles denote the starting and target point. Yellow boxes denote the obstacles we spawn on the path.

In non-interactive navigation, the metric SPL that measures path efficiency is also a usable indicator of time efficiency, since the length of a path is usually proportional to the time it takes to execute the corresponding navigation actions (e.g. move ahead, rotate right). However, a shorter path does not guarantee less time consumption for task completion in InterNav due to the object interaction. For instance, the agent may take the shortest path to the target through extensive interactions which cost a large amount of time and end up less effective than taking a detour path. Thus we evaluate an additional metric STS (Success rate weighted by Time Steps) to measure the time cost of task completion: \(STS=_{n=1}^{N}Suc_{n}/grid}{TS_{n}}\), where \(L_{n}\) is the shortest path length, \(TS_{n}\) is the timesteps the agent takes to complete the task, and \(grid=0.25m\) is the unit distance of a step. Thus \(L_{n}/grid\) represents the number of timesteps it takes to navigate to the target by merely moving forward. STS can be regarded as a time-measurement variant of SPL and the score is higher when the agent accomplishes the task with less time. The ideal situation is that the target is directly ahead and there is no need for interaction where STS is 1.

### Implementation Details

Our method is implemented and evaluated using the AllenAct  framework. The egocentric observation is set as 300*300 RGB and depth images. We first train three action policies then the whole model, utilizing Adam with an initial learning rate of \(3 10^{-4}\) for policy training and \(1 10^{-4}\) for joint model training. The hidden size of CNN, GRU is set as 512. The intent embedding size is set as 12 (the same as the intent size and the length of action space). The frequency of synchronization \(K\) and epoch number \(k\) are set as 3 and 1. All models are trained for 10 million steps with 2 million warm-up steps in single-room scenes (1m for action policies and 1m for joint training) and 8 million in all scenes1.

**Reward shaping.** We set the reward for our task as three parts: \(r=r_{success}+_{dis}-r_{sp}\), where \(r_{success}=10\) is provided if the agent takes action _Done_ meanwhile achieving the goal. \(_{dis}\) represents the difference of geodesic distance between the agent and the goal position, and \(r_{sp}=0.01\) denotes the step penalty.

Three auxiliary tasks are designed as: (1) The _Navigate_ policy is pretrained with the PointNav task where the agent moves to the goal point in an obstacle-free environment. The task shares the same reward \(r_{nav}=r\) as above. (2) To pretrain the policy of _Push_ and _Pick_, the agent is placed aside an obstacle opposite to the goal. It achieves success by taking _Done_ when the obstacle is cleared. The reward \(r_{inter}=r_{nav}+r_{as}-r_{af}\), where \(r_{as},r_{af}\) denote the successful reward and failed penalty of chosen action (e.g. fail to conduct _Drop_ before picking up something). Since effective pushes or picks change the length of the shortest path, \(_{dis}\) also applies to interaction reward.

### Baselines

**Random**: This baseline randomly takes actions with the same probability until the agent reaches the target or reaches the maximum number of steps.

**PPO**: We set a common baseline trained with PPO algorithm according to . Similar to our method, it extracts visual features with an encoder and reads the goal position with an embedding layer, then a single policy network predicts actions based on fused features.

**NIE**: We implement the method NIE  which predicts the state change of observed objects conditioned on actions with a module in addition to the PPO baseline. The representation of object change is fused with visual features and goal embedding.

**HRL**: To study the effect of our multi-policy framework, we implement a common hierarchical policy supported by AllenAct  that shares the same three sub-spaces of action. The difference is that the low-level policies are called at each step, not able to plan actions in a period.

**PPO+intent**: We develop a counterfactual policy based on the PPO baseline according to Figure 2(c) to study the effect of intent-aware decision-making.

### Results Analysis

**Comparison with baselines.** We pick model parameters on the validation set and report their performance on the whole testing set, hard scenes with more rooms (\(N 4\)), and the splits of episodes that exist clear paths (non-interactive). As quantitatively shown in Table 3, CaMP outperforms all baselines and obtains +13.6/+7.5, +18.1/+9.2, and +4.3/+3.5 improvement in SR, SPL, and STS (all/\(N 4\), %) respectively compared to the model PPO. Compared with PPO+intent, CaMP obtains more improvement in hard scenes (+3.4 and +3.3 in SR, SPL) which shows the robustness of multi-policy facing complex environment. Besides, the relatively poorer performances of HRL and Random model reflect the challenge of our dataset. The limited performance in hard scenes shows that effective methods for navigation in plenty of rooms are still in need. The contrary trends across STS and SPL when comparing PPO and HRL on the hard split and the non-interactive split indicate that both path and time efficiency should be measured considering InterNav scenarios.

**Ablation study.** Our ablation study aims to explore the effect of two model designs. First, we change the synchronization frequency of model PPO+intent as 1 epoch, 1 rollout, 3 rollouts, and 5 rollouts. As shown in Table 4, the variant with \(K=3\) gains the relatively best performance, which reflects how the balanced exploration-exploitation benefits policy learning.

Second, we study how the model intent influences performance: We implement (1) CaMP without the intent network as _wo/intent_, (2) CaMP that inferences without the intent network as _test wo/intent_, (3) CaMP provided with the intent recursively generated based on the prior intent as _recursive intent_, and (4) CaMP provided with the intent of the global policy as _policy intent_. Here our model CaMP is referred to as _integrated intent_. As shown in Table 4, the integrated intent effectively improves the decision-making ability of the global policy compared with _wo/intent_ (+5.5/+3.3 and +10.4/+8.0 in SR, SPL.). Without the information of the intent of action policies, _policy intent_ shows less gain effect compared with _integrated intent_ (-3.8/-2.3 and -5.6/-4.0 in SR, SPL). The limited performance of _test wo/intent_ suggests that the inference process of our model also relies on the intent. The results of _recursive intent_ suggest that although the secondary intent still provides incremental information about the system, the counterfactual situation based on the "counterfact" may become less meaningful for learning causality.

    &  &  &  \\  & **SR(\%)\(\)** & **FDT(m)\(\)** & **SPL\(\)** & **STS** & **SR** & **FDT** & **SPL** & **STS** & **SR** & **FDT** & **SPL** & **STS** \\ 
**Sync. variants:** & & & & & & & & & & & \\ sync. /epoch & 53.0 & 4.16 & **0.316** & 0.160 & 36.7 & 6.95 & **0.196** & 0.107 & 63.4 & 2.53 & 0.375 & 0.219 \\ sync. /rollout & 50.7 & 4.22 & 0.277 & 0.145 & 36.0 & 6.82 & 0.175 & 0.105 & 57.9 & 3.10 & 0.345 & 0.191 \\ sync. /3*rollout & **54.7** & **3.84** & 0.305 & **0.163** & **37.3** & **6.54** & 0.195 & **0.109** & **70.4** & **2.38** & **0.390** & **0.222** \\ sync. /5*rollout & 50.3 & 4.25 & 0.279 & 0.150 & 30.7 & 6.85 & 0.147 & 0.098 & 55.9 & 3.21 & 0.337 & 0.185 \\ 
**intent variants:** & & & & & & & & & & & \\ wo/intent & 50.8 & 4.03 & 0.294 & 0.151 & 31.0 & 6.79 & 0.151 & 0.095 & 59.2 & 2.98 & 0.347 & 0.202 \\ test wo/intent & 41.7 & 4.95 & 0.202 & 0.111 & 24.7 & 7.72 & 0.110 & 0.085 & 50.5 & 3.56 & 0.296 & 0.168 \\ recursive intent & 51.4 & 4.06 & 0.292 & 0.156 & 32.7 & 7.04 & 0.159 & 0.096 & 62.8 & 2.61 & 0.353 & 0.215 \\ policy intent & 52.5 & 4.32 & 0.304 & 0.155 & 35.8 & 7.09 & 0.191 & 0.100 & 62.6 & 2.91 & 0.357 & 0.216 \\ integrated intent & **56.3** & **3.67** & **0.327** & **0.177** & **41.4** & **5.76** & **0.231** & **0.121** & **72.3** & **2.05** & **0.407** & **0.236** \\   

Table 4: Ablation results.

    &  &  &  \\  & **SR(\%)\(\)** & **FDT(m)\(\)** & **SPL\(\)** & **STS\(\)** & **SR** & **FDT** & **SPL** & **STS** & **SR** & **FDT** & **SPL** & **STS** \\  Random & 1.32 & 7.68 & 0.002 & 0.001 & 0 & 12.29 & 0 & 0 & 2.42 & 7.35 & 0.003 & 0.001 \\ PPO  & 42.7 & 4.84 & 0.252 & 0.134 & 23.3 & 7.79 & 0.139 & 0.086 & 51.5 & 3.44 & 0.306 & 0.181 \\ NIE  & 52.0 & 3.94 & 0.287 & 0.155 & 37.3 & 6.54 & 0.195 & 0.102 & 58.8 & 3.01 & 0.345 & 0.188 \\ HRL & 43.4 & 4.80 & 0.253 & 0.135 & 24.3 & 7.01 & 0.169 & 0.084 & 51.9 & 3.40 & 0.316 & 0.176 \\ PPO+intent & 54.7 & 3.84 & 0.305 & 0.163 & 38.0 & 6.29 & 0.198 & 0.109 & 70.4 & 2.38 & 0.390 & 0.222 \\ CaMP & **56.3** & **3.67** & **0.327** & **0.177** & **41.4** & **5.76** & **0.231** & **0.121** & **72.3** & **2.05** & **0.407** & **0.236** \\   

Table 3: Quantitative results.

**Case study.** We qualitatively compare our model with a baseline PPO on a typical instance in a 4-room scene in Figure 5. The blue trajectory of CaMP shows that the agent successfully removes obstacles of _tomato_ and _pillow_, and attempts to bypass an immovable _sofa_ to reach the target. While the red trajectory shows that PPO intends to interact with a _table_ and a _bed_ which are too heavy to be pushed aside and ends up getting stuck in a corner. The comparison suggests that our model can make better decisions on whether and how to interact with obstacles.

**Study of obstacle interaction.** To study how intent-aware policy learns the obstacle interaction and its outcome, we make a direct evaluation on obstacle interaction of several models in Table 5: (1) Interaction Reward (IR): the reduction of geodesic distance \(_{dis}\) caused by interactions. (2) Interaction Success Rate (ISR): the ratio of taking interactions successfully (a more detailed measurement compared to the overall metric SR measuring task completion). A failed case of interaction is when the agent tries to pick up a heavy table or attempts to push a chair out of sight. We also report the success rate of Push and Pick interactions separately as PuSR (Push Success Rate) and PiSR (Pick Success Rate). The results (comparison between PPO and PPO+intent, CaMP and CaMP wo/intent) show the outcome of our causal method in pursuing the interaction efficacy of reducing the distance to the target (IR) and avoiding invalid actions (ISR). The results on PuSR and PiSR also suggest that successfully pushing an obstacle is more challenging since the obstacle may get stuck and is not movable in all directions.

## 6 Conclusions

In this paper, we tackle the task of Interactive Navigation (InterNav) which requires mastery of long-horizon navigation and object interaction in complex environments. We take a causal view of InterNav and clarify the challenge caused by the confounding bias of obstacles. A Causal Multi-policy Planning (CaMP) model is proposed following the main idea of reducing inefficient exploration with multi-policy and exploring counterfactual interaction by considering the primary intent of model's decision-making. We construct a large-scale dataset of interactive navigation with various obstacles spawned across multiple rooms. Experiments on our dataset illustrate the effectiveness of our method and support our causal modeling of InterNav.

Figure 5: Qualitative cases. The starting and target points are marked by blue circles and obstacles are marked by yellow boxes. The blue arrows and red arrows denote the trajectories from CaMP and PPO, respectively. The nodes and crosses denote successful object interactions and failed ones. Markings in colors are invisible to agents.

  
**Methods** & **IR(m)** & **ISR(\%)** & **PuSR(\%)** & **PiSR(\%)** \\  PPO  & 0.271 & 23.2 & 23.8 & 21.9 \\ PPO+intent & 0.371 & 27.0 & 25.1 & 28.6 \\ CaMP wo/intent & 0.287 & 20.4 & 19.9 & 23.5 \\ CaMP & 0.402 & 27.6 & 27.1 & 29.8 \\   

Table 5: Obstacle interaction results.