# Implicitly Guided Design with PropEn:

Match your Data to Follow the Gradient

 Natasa Tagasovska

Prescient/MLDD, Genentech Research and Early Development

Vladimir Gligorijevic

Department of Computer Science, Center for Data Science, New York University

Kyunghyun Cho

Presentent/MLDD, Genentech Research and Early Development

Andreas Loukas

Department of Computer Science, Center for Data Science, New York University

###### Abstract

Across scientific domains, generating new models or optimizing existing ones while meeting specific criteria is crucial. Traditional machine learning frameworks for guided design use a generative model and a surrogate model (discriminator), requiring large datasets. However, real-world scientific applications often have limited data and complex landscapes, making data-hungry models inefficient or impractical. We propose a new framework, _PropEn_, inspired by "matching", which enables implicit guidance without training a discriminator. By matching each sample with a similar one that has a better property value, we create a larger training dataset that inherently indicates the direction of improvement. Matching, combined with an encoder-decoder architecture, forms a domain-agnostic generative framework for property enhancement. We show that training with a matched dataset approximates the gradient of the property of interest while remaining within the data distribution, allowing efficient design optimization. Extensive evaluations in toy problems and scientific applications, such as therapeutic protein design and airfoil optimization, demonstrate PropEn's advantages over common baselines. Notably, the protein design results are validated with wet lab experiments, confirming the competitiveness and effectiveness of our approach. Our code is available at https://github.com/prescient-design/propen.

## 1 Introduction

Navigating the complex world of design is a challenge in many fields, from engineering  to material science  and life sciences . In life sciences, the goal may be to refine molecular structures for drug discovery , focusing on properties like binding affinity or stability. In engineering, optimizing the shapes of aircraft wings or windmill blades to achieve desired aerodynamic traits like lift and drag forces is crucial . The common thread in these fields is the _design cycle_: experts start with an initial design and aim to improve a specific property. Guided by intuition and expertise, they make adjustments and evaluate the changes. If the property improves, they continue this iterative optimization process. This cycle is repeated multiple times, making it time-consuming and resource-intensive. ML holds promise to reduce these costs, speed up design cycles, and create better-performing designs .

Yet, progress in ML methods for design is hindered by practical challenges. The first challenge is _limited data availability_. Since gathering label measurements is resource intensive , designers are more often than not constrained to very small-scale datasets. In addition, there are often _non-smooth functional dependencies between the features and outcome_, complicating approximation, even for deep neural networks . Traditional methods use two part frameworks, requiring discriminators to guide the property enhancement for examples produced by a generative model. Such discriminators should be able to reliably predict the property of interest given some training data or its latent representations. Because of the dependency on training a discriminator for guidance,we denote these methods as _explicit guidance_. While Genetic Algorithms were once prevalent , contemporary models like auto-encoders , GANs , and diffusion models now dominate both research and practice  the role of generative models. Despite their flexibility, such models face challenges typical to deep learning: they are "data-hungry" and unreliable when encountering out-of-distribution examples [29; 21; 7; 37].

Motivated by these challenges, we propose a new approach inspired by the concept of "matching". Matching techniques in econometrics are used to address the challenge of selection bias and confounding when estimating causal effects in observational studies [2; 38; 39; 43]. These techniques aim to create comparable groups of units by matching treated and control observations based on observable characteristics. The basic idea behind matching is to identify untreated units that are similar to treated units in terms of observed covariates, effectively creating a counterfactual comparison group for each treated unit. Matching techniques in ML as in econometrics have only been used to provide more robust, reliable causal-effect estimation [24; 48; 9].

This work argues that, in lack of large datasets, matching allows for implicit guidance, completely sidestepping the need for training a discriminator (differentiable surrogate model). We match each sample with a similar one that has a superior value for the property of interest. By doing so, we obtain a much larger training dataset, inherently embedding the direction of property enhancement. We name our method _PropEn_ and we illustrate it in Figure 1. By leveraging this expanded dataset within a standard encoder-decoder framework, we circumvent the need for a separate discriminator model. We show that PropEn is domain agnostic, can be applied to any data modality continuous or discrete. Additionally, PropEn alleviates some common problems with explicit guidance such as falling off the data manifold or requiring complex engineering.

Overall, our contributions are as follows:

* We propose "matching" (inspired by causal effect estimation) to expand small training datasets (subsection 2.1);
* We provide a theoretical analysis on how training on a matched dataset implicitly learns an approximation of the gradient for a property of interest (subsection 2.2);

Figure 1: Conceptual summary of implicit and explicit guidance. The task is increasing the size of the objects. Top - in implicit guidance, first we match the training dataset, by pairing each sample with the closest one w.r.t. its shape, which has a better property (size). Then, we train a encoder-decoder framework which due to the construction of the dataset learns a lower dimensional manifold where the embeddings are ordered by the property value. Bottom - in explicit guidance, we train two separate models: a generator and a discriminator that guides the optimization in latent space.

* We provide guarantees that the proposed designs are as likely as the training distribution, avoiding common pitfalls where unreliable discriminators lead to unrealistic, pathological designs (subsection 2.3);
* We demonstrate the effectiveness and advantages of implicit guidance through extensive experiments in both toy and real-world scientific problems, using both numerical and wet lab validation (section 3).

## 2 Property Enhancer (PropEn)

Given a set of initial examples, our objective is to propose a new design which is similar to the initial set, but, exceeds it in some property value of interest.

**Problem setup.** Concretely, we start with a dataset \(=\{x_{i},y_{i}\}_{i=1}^{n}\) consisting of \(n\) observed examples \(x_{i}^{m}\) drawn from a distribution \(p\) together with their corresponding properties \(y_{i}=g(x_{i})\). Our objective is to determine ways to improve the property of a test example. Concretely, at test time, we are given a point \(x_{0}\) and aim to identify some new point \(x^{} p\) close to \(x_{0}\) such that \(g(x^{})>g(x_{0})\). In effect, our problem combines constrained optimization (maximize \(g(x^{})\) while staying close to \(x_{0}\)) with sampling from a distribution (point \(x^{}\) should be likely according to \(p\)).

Hereafter, we will refer to the initial example we wish to optimize as _seed_ design and the model's proposal as _candidate_ design. Our method, _PropEn_, entails three steps: (i) matching a dataset, (ii) approximating the gradient by training a model with a matched reconstruction objective and (iii) sampling with implicit guidance.

### Match the dataset

We view the group of samples with superior property values as the treated group and their lower-value counter part as the control group. This motivates us to construct a "matched dataset" for every \((x,y)\) within \(\):

\[=\{(x,x^{})|x,x^{} \\ \|x^{}-x\|^{2}_{x},\;g(x^{})-g(x)(0,_{y}] .\},\] (1)

where \(_{x}\) and \(_{y}\) are predefined positive thresholds.

The matched dataset gives us a new and extended collection \(\) whose size \(N=O(n^{2}) n\) can significantly exceed that of the training set, depending on the choice of matching thresholds.

### Approximate the gradient

After matching the dataset, we train a deep encoder-decoder network \(f_{}\) over \(\) by minimizing the _matched reconstruction_ objective:

\[(f_{};)=|}_{(x,x^{}) }(f_{}(x),x^{}),\] (matched reconstruction objective)

where \(\) is an appropriate loss for the data in question, such as an mean-squared error (MSE) or cross-entropy loss.

Before illustrating the properties of our method empirically, we perform a theoretical analysis. We show that minimizing the matched reconstruction objective yields a model that approximates the direction of the gradient of \(g()\), even if no property predictor has been explicitly trained:

**Theorem 1**.: Let \(f^{*}\) be the optimal solution of the matched reconstruction objective with a sufficiently small \(_{x}\). For any point \(x\) in the matched dataset for which \(p\) is uniform within a ball of radius \(_{x}\), we have \(f^{*}(x) c g(x)\) for some positive constant \(c\).

The detailed proof is provided in subsection A.1.

**Remark 1**.: The proof of 1 is founded on the assumption that distribution is uniformly distributed within a ball of radius \(_{x}\) around point \(x\). This assumption is made to maintain the generality of the theorem without specific information about the sampling distribution, assuming uniformity avoidsintroducing any biases that could arise from other distributional assumptions, such as symmetry, finite variance etc.

**Remark 2**.: It can be shown that with with a matched reconstruction objective we learn a direction that is \(a\)-colinear with the gradient of \(g\), avoiding the isotropy assumption. This leads to additional analysis on understanding the implications of the choices while matching, all included in Appendix A.

### Optimize designs with implicit guidance

Training on a matched dataset allows for auto-regressive sampling. Starting with a design seed \(x_{0}\), for \(t=1,2,\), we can generate \(x_{t}=f_{}(x_{t-1})\) until convergence, \(f_{}(x_{t})=x_{t}\), s.t. \(g(x_{t})>g(x_{t-1})\). At test time, we feed a seed design \(x_{0}\) to PropEn, and read out an optimized design \(x_{1}\) from its output. We then proceed to iteratively re-feed the current design to PropEn until \(f_{}(x_{t})=x_{t}\), which is analogous to arriving at a stationary point with \( g(x_{t})=0\) and we have exhausted the direction of property enhancement given the training data. Exploiting the implicit guidance from matching results in a trajectory of multiple optimized candidate designs.

We next show that optimized samples are almost as likely as our training set according to the data distribution \(p\). This serves as a guarantee that the generated designs lie within distribution, as desired:

**Theorem 2**.: Consider a model \(f^{*}\) trained to minimize the matched reconstruction objective. The probability of \(f^{*}(x)\) is at least

\[p(f^{*}(x))_{x^{}_{x}}[p(x^{})]-(f(x))\|_{2}\,^{2}(_{x})}{2},\]

where \(_{x}\) is the empirical measure on the dataset, \(H_{p}(x)\) is the Hessian of \(p\) at \(x\) and \(^{2}(_{x})=_{x^{}_{x}}[\|x^{ }-_{x^{}_{x}}[x^{}]\|_{2} ^{2}]\) is the variance induced by the matching process.

The detailed proof is provided in subsection A.3.

We use a synthetic example to illustrate optimizing designs with PropEn. We choose a 2d pinwheel dataset. As a property to optimize, we choose the log-likelihood of the data as estimated by a KDE with Gaussian kernel with \(=0.01\). Figure 2 depicts in gray the training points, with the color intensity representing the value of the property--hence a higher/darker value is better in this example. After training PropEn, we take held out points (pink squares) and use them as seed designs. With orange x-markers, we illustrate PropEn candidates, with the color intensity increasing at each step \(t\). We notice that PropEn moves towards the regions of the training data with highest property value, consistently improving at each step (right-most panel). Additionally, we also use out-of-distribution seeds, and we demonstrate in the middle panel that PropEn chooses to optimize them by proposing designs from the closest regions in the training data.

Figure 2: Illustration of PropEn on the pinwheel toy example with only 72 training examples. The training data are circles in grey, colored by the value of the property. With pink we mark the initial hold out test points and in orange ‘\(\)’ the PropEn trajectories. The color of the candidates intensifies with each iteration step. On the right-hand-side, we depict the sum of negative log likelihoods of the seeds and optimized designs across optimization steps.

## 3 Experimental Results

We empirically evaluate PropEn on synthetic and real-world experiments to answer the following main questions: (i) Can PropEn be applied across various domains and datasets? (ii) Does PropEn provide reliable guidance, especially in situations with limited data and when dealing with out-of-distribution examples? (iii) How effective is PropEn in recommending optimal designs? Can it suggest candidates with property values exceeding those in the training set? (iv) How does PropEn's performance vary with different data characteristics (e.g., dimensionality, sample size, heterogeneity) and hyperparameters (such as \(_{x}\), \(_{y}\), and regularization terms)? Our code is available at https://github.com/prescient-design/propenhttps://github.com/prescient-design/propen.

**Datasets.** We consider three different data types: synthetic 2d toy datasets and their higher dimension transformations, NACA airfoil samples, and therapeutic antibody proteins. An overview of the data is given in Table 1. We present our results in two settings, _in silico_ where we rely on experimental validation using computer simulations and solvers, and _in vitro_ experiments where candidate designs were tested in a wet lab. Each of the experiments is evaluated under the baselines and metrics suitable for the domain.

**PropEn variants.** We investigate the utilization of matching and reconstruction within the PropEn framework. Two key considerations emerge: first, whether to reconstruct solely the input features (x2x) or both the input features and the property (xy2xy); second, the proximity to the initial sample, regulated by incorporating a straightforward reconstruction regularizer into the training loss \((f_{}(x),x)\). This regularized variant will be referred to as _mixup/mix_.

### _In silico_ experiments

#### 3.1.1 Toy data

We choose two well-known multi-modal densities: pinwheel and 8-Gaussians. These are 2d datasets, but, in order to make the task more challenging, we expand the dimensionality to \(d\{10,50,100\}\) by randomly isometrically embedding the data within a higher dimensional space. Our findings are summarized in Figure 3 and we include the tabular results in item B.7. We empirically validate the four variants of PropEn and we compare against explicit guidance method: for consistency, we chose an auto-encoder of the same architecture as PropEn augmented with a discriminator for guidance in the latent space. We denote this baseline _Explicit_. We compare the methods by ratio of improvement, the proportion of holdout samples for which PropEn or baselines demonstrate enhanced property values. To assess the quality of the generated samples we report _uniqueness_ and _novelty_ in tables. We use a likelihood model derived from a Kernel Density Estimation (KDE) fit on the training data. The negative log-likelihood scores under this model serve as an indicator of in-distribution performance. Higher values for all metrics indicate better performance.

**Results.** Several insights can be gleaned from these experiments. When analyzing the results based on the number of samples, a clear trend emerges: as the number of training samples increases, PropEn consistently outperforms explicit guidance across all metrics, except for average improvement, where all methods exhibit similar behavior. The choice of the preferred metric may vary depending on the specific application; however, it is noteworthy that while explicit AE guidance improves approximately 50% of the designs for all datasets, PropEn demonstrates the potential to enhance up to 85% of the designs. Importantly, this improvement trend remains consistent regardless of the

  
**Dataset** & **Domain** & **Size \(n\)** & **Type** & **Metric** & **Property** & **Preview** \\ 
**Toy** & \(^{10},^{50},^{100}\) & \(50,100\) & cont. & L2 & log-likelihood & \\
**Airfoil** & \(^{400}\) & \(200,500\) & cont. & L2 & lift-to-drag & \\  & & & & ratio & & \\
**Antibodies** & \(20^{297}\) & \(200-400\) & discrete & Levenshtein & binding affinity & \\   

Table 1: Overview of the datasets in experiments.

dimensionality of the data. Furthermore, an intriguing observation pertains to the performance of different variations of PropEn. It is noted that as the sample size increases, PropEn xy2xy does not exhibit an advantage over PropEn x2x. Moreover, the impact of iterative sampling with PropEn is notable. With each step, the property improves until it reaches a plateau after multiple iterations, albeit with a simultaneous drop in the uniqueness of the solution to around 80%. Nevertheless, iterative optimization can be continued until convergence, with all designs saved along the trajectory for later filtering according to user needs.

#### 3.1.2 Engineering: airfoil optimization

In an engineering context, shape optimization entails altering the shape of an object to enhance its efficiency. NACA airfoils (National Advisory Committee for Aeronautics) , rooted in aerodynamics and parameterized by numerical values, serve as a well-documented benchmark due to their versatility. These airfoils span diverse aerodynamic characteristics, from high lift to low drag, making them ideal for exploring different optimization objectives. Given their integral role in aircraft wings, optimizing airfoil shapes can significantly impact aerodynamic performance by improving lift, drag, and other properties essential for aerospace engineering.

**Data & experimental design.** We generate NACA 4-digit series airfoil coordinates by choosing these parameters: \(M\) (maximum camber percentage), \(P\) (location of maximum camber percentage), and \(T\) (maximum thickness percentage). Each airfoil is represented by 200 coordinates, resulting in a 400 vector representation when flattened. Our objective is to optimize the lift-to-drag ratio (\(C_{l}/C_{d}\) ratio) for each shape. We calculate lift and drag using NeuralFoil , a precise deep-learning emulator of XFoil .

Note that the lift-to-drag ratio is pivotal in aircraft design, reflecting the wing's lift generation efficiency relative to drag production. A high value signifies superior lift production with minimal drag, translating to enhanced fuel efficiency, extended flight ranges, and overall improved performance. This ratio is paramount in aerodynamic design and optimization, facilitating aircraft to travel farther and more efficiently through the air. Traditionally, engineers have relied on genetic algorithms guided by Gaussian Process models (kriging) , however, in recent years the community has moved towards ML-based methods which consist of a generative model that can be GAN-based  or a variation of a VAE . Similarly, for the surrogate, guidance model, GPs and numerical solvers have been replaced by deep models . For our experiments we follow this standard setup: we choose a VAE-like baseline as it is the most similar architectural choice to PropEn, and for guidance we use a MLP. All networks (encoder, decoder and surrogate) are fully connected 3 layer MLPs with 50 units, ReLU activations per layer and a latent space of dimension 50. We randomly select 0.1% as holdout dataset for seeds, and use the rest for training.

**Results.** Our numerical findings are summarized in 1. Similar to the toy dataset, the designs produced by PropEn variants demonstrate enhanced properties compared to those guided explicitly. Delving into further analysis with ablation studies, depicted in Figure 4(b) and (c), we observe that increasing the matching thresholds in PropEn correlates with higher rates of improvement. Remarkably, all designs within a PropEn trajectory are deemed plausible, as depicted in the accompanying figure. Moreover, a consistent enhancement in the lift-to-drag ratio \(C_{l}/C_{d}\) is noted along the optimization trajectory until convergence. This consistent trend underscores the effectiveness of PropEn in progressively refining airfoil designs to bolster their aerodynamic performance.

Interestingly, we find that in larger training datasets the threshold for property improvement (\(_{y}\)) may not be necessary for optimization, as the PropEn x2x demonstrates satisfactory performance.

Figure 3: PropEn in toy examples in \(d\{50,100\}\), left side: 8-Gaussians, right side: pinwheel. Distribution of evaluation metrics from 10 repetitions of each experiment.

We also notice that the mix variant of PropEn may require longer training. This observation is consistent with the notion that mix, by introducing a regularization term in the training loss, may improve at a slower rate compared to other PropEn variants. This slower improvement can be attributed to the regularization term's tendency to pull generated designs closer to the initial seed, thereby limiting the extent of exploration in the design space.

### _In vitro_ experiment: therapeutic protein optimization

The design of antibodies good functional and developability properties is essential for developing effective treatments for diseases ranging from cancer to autoimmune disorders. We here focus on the task of optimizing the binding affinity of a starting antibody (the seed) while staying close to it in terms of edit distance. The task is referred to as affinity maturation in the drug design literature and constitutes an essential and challenging step in any antibody design campaign.

Antibody binding affinity refers to the strength of the interaction between an antibody molecule and its target antigen. High binding affinity is crucial in antibody-based therapeutics as it determines the antibody's ability to recognize and bind to its target with high specificity and efficiency. We follow the standard practice of quantifying binding affinity by the negative log ratio of the association and dissociation constants (pKD), which represents the concentration of antigen required to dissociate half of the bound antibody molecules. Higher pKD indicates a tighter and more stable interaction, leading to improved therapeutic outcomes such as enhanced neutralization of pathogens or targeted delivery of drugs to specific cells.

**Data & experimental design.** The data collection process involved conducting low-throughput Surface Plasmon Resonance (SPR) experiments aimed at measuring with high accuracy the binding affinity of antibodies targeting three different target antigens: the human epidermal growth factor receptor 2 (HER2) and two additional targets that we denote as T1 and T2. For each of those targets, one or more seed designs were selected by domain experts. In the case of HER2, we used the cancer drug Herceptin as seed. We ensured the correctness of the SPR measurements by validating the fit of the SPR kinetic curves according to standard practices.

As the targets differ in the properties of their binding sites, we trained a PropEn model per each target (but for all seeds for that target jointly). For this application, we opted for the PropEn x2x mix variant. The reconstruction of the original sequence (mix) complies with antibody engineering wisdom that a candidate design should not deviate from a seed by more than small number of mutations. Similar to , we used a one-hot encoded representation of antibodies aligned according to the AHo numbering scheme  determined by ANARCI . The encoder-decoder architecture is based on a ResNet  with 3 blocks each. We compare PropEn with four strong baselines: two state-of-the-art methods for guided and unpaired antibody design namely walk-jump sampler  and lambo ; as well as two variants of a diffusion model trained on AHo antibody sequences differing on their use of guidance. The first one (labeled as **diffusion**) is based on a variational diffusion model  trained on a latent space obtained by projecting AHo 1-hot representation using an encoder-decoder type of architecture similar to _PropEn_'s architecture; encoder-decoder model is trained simultaneously with the diffusion model. The second one (labeled as **diffusion(guided)**) is a variant of the first one with

Figure 4: Ablation studies for PropEn on Airfoils. (a) PropEn improves Cl/Cd ratio along its trajectory and produces realistic/valid airfoil shapes. (b and c) the impact of choice of threshold \(_{x}\) and \(_{y}\) in the matching phase. When varying \(_{x}\), \(_{y}\) is set to 1, and vice versa.

added guidance based on the _iterative latent variable refinement_ idea described in the paper by Choi _et al._, which ensures generating samples that are close to the initial seed.

We evaluate the set of designs in terms of their binding rate (fraction of designs tested that were binders), the percentage of designs than improve the seed, and their binding affinity improvement (pKD design - pKD of seed).

**Results.** As seen in Tables 2 and 3, PropEn excelled in generating functional antibodies with consistently high binding rates (94.6%) and 34.5% of the tested designs showed improve binding than the seed, outpacing other models in overall performance. To account for the trade-off between binding rate and affinity improvement (larger affinity improvement requires making risky mutations that might end-up killing binding), we visualize the Pareto front in Figure 5(a). In the plot, we mark the performance of each method for a specific seed design by placing a marker based on the achieved binding rate (x-axis) and maximum affinity improvement (y-axis). Compared to baseline methods, PropEn struck a beneficial trade-off, on average achieving a larger affinity improvement than methods with a high binding rate.

Figure 5(b) takes a closer look at the affinity improvement on the subset of designs that bound. As observed, all models produced some binders that were better than the seed, speaking for the strength of all considered models. Interestingly, none the top three models in terms of binding affinity improvement relied on explicit guidance, which aligns with our argument about the brittleness of explicit guidance in low-data regimes. Out of the those three models, PropEn generated two designs that improved the seed by at least one pKD unit (10 times better binder) followed by the walk-jump and the unguided diffusion model, that generated one such design each.

    & Heroerin & T1S1 & T1S2 & T1S3 & T2S1 & T2S2 & T2S3 & T2S4 & overall \\ 
**PropEn** & 90.9\% (11) & 100.0\% (4) & 100.0\% (6) & 100.0\% (24) & 20.0\% (5) & 100.0\% (23) & 100.0\% (16) & 100.0\% (4) & **94.6\% (93)** \\
**walk-jump** & - & 25.0\% (4) & 80.0\% (15) & 100.0\% (18) & 26.7\% (30) & 41.7\% (12) & 100.0\% (15) & 63.6\% (11) & 62.9\% (105) \\
**lamble (guided)** & 50.0\% (10) & 0.0\% (4) & 100.0\% (5) & 0.0\% (9) & - & 100.0\% (17) & 57.1\% (14) & 42.4\% (33) \\
**diffusion** & - & 62.5\% (8) & 14.3\% (14) & - & - & - & 88.2\% (17) & 66.7\% (66) & 86.7\% (45) \\
**diffusion (guided)** & - & 51.9\% (27) & 15.6\% (32) & - & - & - & 93.3\% (15) & 100.0\% (10) & 92.9\% (84) \\   

Table 2: Binding rate (and number of designs submitted). Higher is better.

Figure 5: Therapeutic protein optimization results: (a) The left figure contrasts the binding rate with the 90-th percentile of the binding affinity improvement for each method and seed. Points on the top-right are on the Pareto front. (b) The right figure focuses on binders and reports the histograms of binding affinity improvement across all designs and seeds.

    & Heroerin & T1S1 & T1S2 & T1S3 & T2S1 & T2S2 & T2S3 & T2S4 & overall \\ 
**PropEn** & 0.0\% (11) & 100.0\% (4) & 33.3\% (6) & 41.7\% (24) & 0.0\% (5) & 69.6\% (23) & 0.0\% (16) & 0.0\% (4) & **34.4\% (93)** \\
**walk-jump** & - & 25.0\% (4) & 6.7\% (15) & 5.6\% (18) & 3.3\% (30) & 8.3\% (12) & 0.0\% (15) & 0.0\% (11) & 4.8\% (105) \\
**lamble (guided)** & 10.0\% (10) & 0.0\% (4) & 0.0\% (5) & 0.0\% (9) & - & 0.0\% (17) & 35.7\% (14) & 14.0\% (43) \\
**diffusion** & - & 62.5\% (8) & 14.3\% (14) & - & - & - & 0.0\% (17) & 0.0\% (6) & 15.6\% (45) \\
**diffusion (guided)** & - & 51.9\% (27) & 15.6\% (32) & - & - & - & 0.0\% (15) & 0.0\% (10) & 22.6\% (84) \\   

Table 3: Fraction of designs improving the seed and total designs tested. Higher is better.

Related work

As design optimization has been ubiquitous across science domains, naturally our approach relates to a variety of methods and applications. In the molecular design domain, data are bound to discrete representation which can be challenging for ML methods. A natural way to circumvent that is by optimization in a latent continuous space. Bombarelli et al.  presented such an approach in their seminal work on optimizing molecules for chemical properties, and it has since spawn across different domains . Recently, one of the common way for obtaining embeddings for explicit guidance relies on language models . One of the challenges of using a latent space is the issue for blindly guiding the optimization into ambiguous regions of the manifold where no training data was available . Follow up works attempt to address this problem  by incorporating uncertainty estimates into black-box optimization.

Another line of work perhaps more close to our approach is the notion of neural translation, where the goal is to go from one language to another by training on aligned datasets.  have build on this idea to improve properties for small molecules translating one graph or sequence to another one with better properties. These works propose tailored approaches for domain specific applications. With PropEn we take a step further and propose a domain-agnostic framework that is empirically validated across different domains (uniquely including wetlab experiments). We also derive novel theoretical guarantees that illustrate the relation of the generated samples with the property gradient, as well as provide guarantees that our designs fall within distribution.

Previous works have also considered learning an optimizer for some function based on observed samples . This is usually achieved by either (i) rendering the optimizer differentiable and training some of its hyperparameters; or (ii) by unfolding the iterative optimizer and treating each iteration as a trainable layer. Our approach is different, thanks to the matched reconstruction objective that lets us implicitly approximate the gradient of a function of interest.

## 5 Conclusion

**Strengths.** We introduced PropEn, a new method for implicit guidance in design optimization that approximates the gradient for a property of interest. We achieve this by leveraging matched datasets, which increase the size of the training data and inherently include the direction of property enhancement. Our findings highlight the versatility and effectiveness of PropEn in optimizing designs in engineering and drug discovery domains. We include wet lab in-vitro results for comparison with state-of-the-art baselines in therapeutic protein design. By utilizing thresholds for shape dissimilarity and property improvement, PropEn efficiently navigates the design space, generating diverse and high-performance configurations. We believe our method offers a simple yet effective recipe for design optimization that can be applied across various scientific domains.

**Limitations.** The matching step adds some computation overhead with complexity depending on the choice of distance metric. Since PropEn is targeting low data-regime applications, scalability is out of scope for the current work. However, we are considering on-the-fly distance evaluation or parallelisation across multiple nodes. The choice of distance metric for matching to a certain extent, can be considered a limitation because it requires understanding of the context for the application. However, this choice is also what allows for incorporating domain knowledge and constraints, which can be meaningful and necessary in the domain of interest (edit distance for antibodies, deviations only in the camber of the airfoil etc).

**Future work.** Immediate extensions of PropEn are applications to other molecular modalities, such as small molecules, material discovery, and optimization of adeno-associated virus vectors for gene-therapy. Additionally, we are keen to explore how different similarity metrics incorporate various inductive biases that can be leveraged for property optimization. Ongoing worthwhile efforts include developing a multi-property PropEn framework to address the optimization of properties simultaneously, offering a more comprehensive approach to the design process.