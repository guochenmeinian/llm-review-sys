# Pseudo-Siamese Blind-Spot Transformers for

Self-Supervised Real-World Denoising

 Yuhui Quan\({}^{*}\),   Tianxiang Zheng\({}^{*}\)

School of Computer Science and Engineering

South China University of Technology

&Hui Ji\({}^{*}\)

Department of Mathematics

National University of Singapore

email: csyhquan@scut.edu.cn (Y. Quan); cszhengtx@mail.scut.edu.cn (T. Zheng); matjh@nus.edu.sg (H. Ji).

###### Abstract

Real-world image denoising remains a challenge task. This paper studies self-supervised image denoising, requiring only noisy images captured in a single shot. We revamping the blind-spot technique by leveraging the transformer's capability for long-range pixel interactions, which is crucial for effectively removing noise dependence in relating pixel-a requirement for achieving great performance for the blind-spot technique. The proposed method integrates these elements with two key innovations: a directional self-attention (DSA) module using a half-plane grid for self-attention, creating a sophisticated blind-spot structure, and a Siamese architecture with mutual learning to mitigate the performance impacts from the restricted attention grid in DSA. Experiments on benchmark datasets demonstrate that our method outperforms existing self-supervised and clean-image-free methods. This combination of blind-spot and transformer techniques provides a natural synergy for tackling real-world image denoising challenges.

## 1 Introduction

Images taken with digital cameras inevitably acquire noise from various sources. Image denoising aims to recover a clean (noise-free) image from its noisy counterpart, serving as an important technique in many low-level vision tasks. Recently, deep learning has prominently driven advancements in image denoising; see _e.g._, [1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11]. Early works typically take a supervised learning approach to train a neural network (NN) on a dataset with paired noisy and clean images, where noisy images are synthesized by corrupting clean images with Additive White Gaussian Noise (AWGN). As observed in [6; 12], a denoising NN trained on AWGN fails to generalize well on real-world noisy images due to the statistical distribution gap between AWGN and real-world noise.

To address this issue in supervised learning, several studies have concentrated on constructing datasets with paired real-world noisy and clean images, such as the Smartphone Image Denoising Dataset (SIDD)  and the Darmstadt Noise Dataset (DND) . However, creating these datasets is labor-intensive, involving multiple acquisitions of images of the same scene, requiring rigorous image alignment, and is inapplicable to dynamic scenes. Additionally, the statistical properties of real noise vary for different camera systems and settings. These limitations restrict the wider application of supervised-learning-based solutions in real-world image denoising.

To address the difficulties of creating paired noisy-clean image datasets that capture the noise characteristics of the targeted testing data, some approaches train denoising NNs using unpaired noisy and clean images [15; 16; 17; 8]. Other studies [18; 19; 20] use multiple noisy images of the same scene as training data. While these methods relax data collection requirements, limitations remain. The former still requires clean images, which is challenging to collect, especially in scientific/medical imaging. The latter involves rigorous image alignment, making it unsuitable for dynamic scenes.

Self-supervised denoising methods:In recent years, there has been increasing interest in self-supervised denoising methods, where network training requires only a set of _noisy images captured in a single shot_. Among these methods, Blind-Spot Networks (BSNs) and their variations are particularly popular; see, _e.g._[21; 22; 23; 24; 25; 26; 27; 28; 29; 30]. The concept of BSNs has played a significant role in self-supervised learning for various denoising-related tasks. In principle, through specific architectural design, a BSN estimates each output pixel from the surrounding noisy pixels, excluding the corresponding one. This design prevents the network from converging to an identity mapping when trained to minimize the distance between the output of the NN and the input noisy image, as no clean image is available. Note that pixel-wise noise independence is critical for BSN to work effectively.

Recently, transformers, NNs utilizing self-attention (SA) for sequence modeling, have shown great performance in many applications, including image denoising; see, for example, [31; 32; 33; 34; 35]. Compared to Convolutional Neural Networks (CNNs), which use multiple convolution layers to extend the receptive field and connect distant pixels, transformers directly model interactions between distant pixels via attention, capturing long-range dependencies more effectively.

The effectiveness of transformers or SA in connecting distant pixels is particularly attractive for self-supervised denoising, especially for Blind-Spot Networks (BSNs). The ability of transformers to exploit long-range dependencies aligns well with the need for pixel-wise noise independence in BSNs to achieve optimal performance, as noise on distant pixels is more likely to be independent than on neighboring pixels. While the integration of transformers with BSNs is very promising, it has not been well studied in self-supervised denoising.

Currently, there are few studies [28; 29; 36] integrate SA and BSN for self-supervised denoising, LG-BSN  used channel SA  with dilated convolutions for blind-spot learning but did not utilize spatial SA. SS-BSN  integrated a lightweight spatial SA module into a BSN with dilated convolutions but lacked rigorous justification of blind-spot constraints. SwinIA  applied masking to the Swin Transformer  attention matrix, targeting only AWGN. As a result, these transformer-based methods do not outperform recent CNN-based approaches  in denoising real-world images. It remains an open question how to fully leverage the effectiveness of long-range dependence in transformers for better performance in self-supervised real-world denoising.

Main idea of our approach:The direct combination of SA and BSD, as done in , may compromise the blind-spot property . Another approach is to mask out input pixels, as in [21; 22]. However, pixel masking can reduce the integrity and the accuracy of long-range feature interactions within transformer blocks. It also limits the number of available pixels for loss calculation, thereby leads to sub-optimal performance. We address these issues with SelfFormer, a self-supervised transformer model featuring a built-in blind-spot structure that avoids input pixel masking.

Our SelfFormer is built on a directional self-attention (DSA) mechanism, inspired by directional convolution . Unlike plain self-attention, where the attention window spreads out in all directions (see Figure1(a)), DSA's attention window is constrained to a half-plane excluding the token itself (see Figure 1(b)). This unidirectional flow ensures that information broadcasted by a token does not return to it, even after multiple DSA applications, thus creating a blind-spot mechanism.

SelfFormer has four branches, each using DSA in one direction: left, right, up, and down. Combining outputs from all branches allows attention to extend arbitrarily far in every direction without including the center pixel. To improve computational efficiency, we use a gridding scheme on the directional attention window. In addition, we introduce a channel attention (CA) mechanism with blind-spot properties to capture channel interdependence, complementing the spatial similarity captured by SA.

Figure 1: Illustration of basic idea of our approach.

To further enhance the effectiveness of DSA caused by its restricted attention locations, we introduce a pseudo-Siamese architecture for SelfFormer. One sub-NN, _SelfFormer-D_, employs DSA with four branches, while the other, _SelfFormer-F_, uses full-grid SA with only one branch. SelfFormer-D's four branches share weights and process four rotated inputs, allowing identical learnable weights for DSA and full-grid SA in the pseudo-Siamese learning. Despite different structures, this setup ensures both sub-networks have consistent weights. SelfFormer-D and SelfFormer-F, with their different inductive biases, provide mutual regularization through joint learning. Due to its better capability of exploiting long-range dependencies with unconstrained attention windows, SelfFormer-F is used for inference.

**Contributions:** The main contributions of this paper are summarized as follows:

* SelfFormer, an efficient self-supervised transformer, is introduced for real-world image denoising, integrating the blind-spot mechanism with transformers.
* DSA, a specific type of SA, is developed to embed the blind-spot mechanism in transformers.
* A pseudo-Siamese architecture is introduced to address the potential negative performance impact of the restricted attention locations caused by the blind-spot mechanism.

The proposed SelfFormer for denoising real-world images is evaluated on two popular datasets, SIDD and DND, and compared with many existing image denoisers. The results showed that SelfFormer outperforms existing self-supervised denoisers and other methods that not using clean images.

## 2 Related Work

**Supervised denoising:** The study of supervised denoisers focus on designed network architecture. Most existing methods are based on CNNs; see _e.g._[1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 37; 11]. Recently, transformers have become a primary choice due to their performance advantage [31; 32; 33; 34; 35]. Training data is another concern. Instead of using AWGN for data synthesis, CBDNet  synthesized heteroscedastic Gaussian noise and processed it through a Image Signal Processor. Zhou _et al._ trained a noise estimator and a denoiser with mixed AWGN and random-valued impulse noise and then utilized pixel-shuffle down-sampling to adapt the trained model to real noises. For better generalization and evaluation, two widely-used real-world noisy datasets were constructed in [13; 14], respectively. However, large-scale paired data collection remains a challenge.

**Weakly-supervised denoising:** The related approaches can be categorized into learning on unpaired image data, and learning on paired noisy image data. The former leverages unpaired noisy and clean images, using either generative adversarial networks [15; 16] or flow-based methods [17; 20]. The latter use multiple noisy images of the same scene to train a denoiser [18; 19; 20]. Note that such multi-capture of noisy images still requires image alignment and is not applicable to dynamic scenes.

**Self-supervised denoising:** Self-supervised denoisers are trained using only a set of noisy images captured in a single shot. BSN is one popular self-supervised denoiser. Masking-based BSNs (e.g., Noise2Void  and Noise2Self ) address overfitting caused by the absence of clean images in the loss function by masking a portion of input pixels and predicting them using the remaining pixels. Noise2Same  introduces an additional self-reconstruction loss to utilize center pixels' information. Blind2Unblind  trains a masker to better preserve valuable pixels in the masked input. SASL  gives separate treatment to flat and textured regions in masking-based self-supervision.

A large percentage of input pixels are not used in the loss function when they are masked out, leading to sub-optimal performance . To address this, some works design specific network architectures to ensure the receptive-field center is not seen by the corresponding pixel during prediction. Laine19 _et al._ occludes half of the receptive fields of a CNN in four different directions. D-BSN  applies a center-masked convolution, followed by a series of dilated convolutions with specific step sizes. MM-BSN  uses multiple convolutional kernels masked in different shapes.

The less correlated the noise of related pixels is, the better the denoiser performs. However, in real images, neighboring pixels' noise is highly correlated. One solution is to relate distant pixels. AP-BSN  uses pixel-shuffle downsampling with high strides in training, and low strides in testing. PUCA  leverages patch-unshuffle/shuffle to expand receptive fields for relating distant pixels. LG-BPN  uses channel SA with dilated convolution for relating distant pixels and densely-sampled patch-masked convolution to recover local structures. SS-BSN  combines grid SA with a simplified D-BSN . SwinIA  masks the diagonal of the attention matrix in a Swin Transformer .

Besides BSNs, there are also other approaches which are architecture-independent. SURE-based methods (_e.g._), introduce Stein's unbiased estimator  to regularize the training. Recorruption-based methods, _e.g._, Noisier2Noise , R2R , IDR , and Zheng _et al._, define the loss using pairs derived from the input noisy image to simulate a supervised loss. Sampling-based methods (_e.g._ Neighbor2Neighbor ) form training pairs using a random neighbor sampler on the noisy image. Score-based methods, _e.g._, Noise2Score , NDASSID , and Xie _et al._, perform self-supervised training via score matching . Disentanglement-based methods, _e.g._, CVF-SID  use a cyclic loss function to decompose the noisy image into clean and noisy components.

**Zero-shot denoising:** This approach performs per-sample self-supervised training. The sparse-coding-based denoisers learn a dictionary [54; 55; 56; 57] from the noisy image, and the denoising output is defined as a sparse approximation to the input based on the learned dictionary. The NN-based denoiser, such as DIP , Self2Self, NoisyAsClean, Noise2Fast  and ScoreDVI , also follow this paradigm. However, this approach is computationally costly when applied to a large number of images.

## 3 Methodology

In this section, we give a detailed description of our proposed SelfFormer, which effectively integrates the blind-spot mechanism with transformers for self-supervised real-world image denoising.

### Grid Self-Attention and Directional Self-Attention

For a set of tokens stored as \(=[_{1};;_{L}]^{L D}\), SA seeks to derive new token representations by assessing interdependence among every pair of input tokens. Consider SA in a multi-head setting . For the \(h\)-th of \(H\) attention head, all tokens undergo a linear transform resulting in \(_{h},_{h},_{h}^{L D}\), which represent queries, keys and values respectively:

\[(_{h},_{h},_{h})=(_{h}^{},_{ h}^{},_{h}^{}),\] (1)

where \(_{h}^{},_{h}^{},_{h}^{}\) are learnable matrices. Subsequently, attention weights are derived, determining the extent to which each token interacts with its counterparts. This is achieved through the calculation of similarity scores between queries and keys, leading to:

\[_{h}=(_{h}_{h}^{T}/)_{h}.\] (2)

To consolidate results from all attention heads, the output is given by

\[()=([_{1},_{2},, {Head}_{H}])^{O},\] (3)

where \(^{O}\) is a learnable matrix dedicated to fusing the results of the different attention heads.

SA involves pairwise comparison of tokens, which is costly when directly working on all spatial tokens from a feature tensor. As an acceleration scheme, grid SA  grids the tensor of shape \((H,W,C)\) into the shape (\(G^{2}\), \(}\), C) using a fixed \(G G\) uniform grid, resulting in windows with adaptive size \(\). Then SA is performed on the decomposed grid axis (_i.e._, \(G G\)), corresponding to dilated, global spatial mixing of tokens. See Figure 2 (a) for an illustration. Note that grid SA does not satisfy the blind-spot property.

Our DSA also uses the gridding trick for acceleration; see Figure 2(b) for the illustration. Instead of using a half-plane attention window (Figure1(b)), DSA employs a half-plane grid to reduce the number of compared tokens for \(\). Only a subset of equally spaced grids on the half-plane is involved, which lowers computational cost. To enhance diversity, adjacent tokens have different attention grids. For example, tokens of different colors correspond to different grids (Figure2 (b)). Each attention head shifts the grid differently, ensuring each token visits all grids in one pass.

Our DSA can effectively simulate different attention grids by rotating and shifting the input. Despite different definition of attention grid,, DSA and grid SA in our approach share the same parameters \(_{h}^{},_{h}^{},_{h}^{},_{h}{}_ {h}\), allowing the introduction of the Siamese structure into our SelfFormer.

### Architecture of SelfFormer

As illustrated in Figure 3, the proposed SelfFormer has a pseudo Siamese architecture consisting of two sub-NNs: SelfFormer-D (D for Directional) and SelfFormer-F (F for Full), both aiming at mapping an input noisy image to its clean counterpart. The SelfFormer-D consists of four branches, each performing DSA with one specific direction. The SelfFormer-F has only one branch which has the same structure as the branches in SelfFormer-D, except that its performs grid SA instead of DSA.

Each branch consists of six attention blocks (ABs) for performing DSA or grid SA, with two \(1 1\) convolutions at the beginning and end. The ABs are arranged in a U shape, with downsampling in the first half and upsampling in the second half. All branches in SelfFormer-D and SelfFormer-F share weights, except for the last \(1 1\) convolution. The last \(1 1\) convolution in SelfFormer-D integrates outputs from its four branches, resulting in a different input channel dimension from SelfFormer-F. Consequently, SelfFormer-D and SelfFormer-F have different structures but share identical learnable weights, except for the last layer, forming a pseudo-Siamese pair.

The pseudo-Siamese architecture benefits both training and testing. While SelfFormer-D's blind-spot mechanism aids self-supervised training to avoid overfitting, its restricted attention grid may weaken the utilization effectiveness of long-range dependency. In contrast, SelfFormer-F uses a full attention grid, avoiding this issue though losing the function of a BSN. Mutual learning between these sub-networks mitigates SelfFormer-D's structural bias, enhancing effectiveness. Additionally, SelfFormer-F utilizes a single path, as opposed to the four-path structure of SelfFormer-D, resulting in significantly faster performance. Due to these advantages, SelfFormer-F is employed for inference.

**Downsampling and upsampling:** The pairs of downsampling and upsampling used in SelfFormer can extend the receptive field in all directions. To maintain a blind-spot structure, we implement the same idea in  which attaches offsets to the downsampling layers. For a \(2 2\) average downsampling layer, we restrict the receptive field to extend upwards only by padding the input tensor with one row of zeros at top and cropping out the bottom row before operating downsampling.

**Attention blocks:** Each AB sequentially comprises a CA, a Feed-Forward Network (FFN), a DSA module, and another FFN. ABs involve multiple \(1 1\) convolutions and summation operations, which do not need specific treatment for blind-spot mechanism, as they neither change the receptive field nor spread information spatially. The CA is implemented as the NAFBlock  in SelfFormer-F, and we construct its blind-spot counterpart (called BSCA) for SelfFormer-D.

**Channel attention w/ and w/o blind-spot:** Channel attention re-calibrates global feature responses for primitive inputs by explicitly modeling inter-dependencies of channels, complementing the function of SA. We use NAFBlock  for channel attention, consisting of SimpleGate and Simplified Channel Attention (SCA). See Figure 3 for details. Given \(2\) feature tensors, \(_{1}\) and \(_{2}\), generated by two \(3 3\) convolutions on an input feature, SimpleGate outputs \(=_{1}_{2}\), where \(\) denotes element-wise multiplication, maintaining the receptive field. The \(3 3\) convolutions in SelfFormer-D are directional convolution  to own the blind-spot property. Then, SCA re-calibrates channels by multiplying each channel of \(\) with a weight scalar, calculated using a channel-wise Global Average Pooling (GAP) and a \(1 1\) convolution. As GAP squeezes all spatial values into a scalar and scale-multiplication do not spread spatial information, the SCA preserves the blind-spot property.

Figure 2: Illustration of grid SA and grid-based DSA. In grid SA, attention grids are scattered in all directions, with attention among tokens on same-colored grids. In DSA, attention grids are constrained to a half-plane, varying by token and attention head.

### Loss Function

The overall loss function consists of a self-reconstruction loss \(_{}\) and a mutual learning loss \(_{}\):

\[_{}=_{}+_{},\] (4)

where \(\) is set to \(1\). Let \(\) denote a noisy image. Let \(_{},_{}\) denote the models of SelfFormer-D and SelfFormer-F, respectively. The self-reconstruction loss is then defined by

\[_{}=_{}||_{}()- ||_{1}.\] (5)

Due to the blind-sport structure of SelfFormer-D, \(_{}\) trained with this loss will not converge to the trivial identity mapping, but rather an effective denoiser. The mutual loss is defined by

\[_{}=_{}||_{}()-_{}()||_{1}.\] (6)

This loss enables the regularization effect of the two sub-networks to each other, leading to better generalization performance.

## 4 Experiments

### Experimental setting

**Datasets:** Three widely-used real-world datasets are used for evaluation: SIDD , DND , and NIND . SIDD is created by photographing a scene many times and using its mean as the GT clean image. The images of SIDD are captured by five different smartphone, and they are divided into non-overlapping subsets for training, validation, and test, respectively. As most works do, these samples are cropped into 24542 pairs of patches. the SIDD-Medium subset is chosen as training data, consisting of 320 noisy/clean image pairs. The validation subset, denoted by SIDD-Validation, consists of 1280 paired samples for hyper-parameter tuning and ablation study. The subset for test is denoted by SIDD-Benchmark, consisting of 1280 noisy samples. DND consists of 50 noisy-clean pairs, formed by shooting the same scene twice with different ISO values. The high-ISO images serve as noisy inputs and the corresponding low-ISO images serve as GT images. DND is used only for testing for performance evaluation. NIND is a real-world dataset consisting of clean-noisy image pairs captured at ISO levels of 3200, 4000, 5000, and 6400, with 22, 14, 13, and 79 pairs, respectively. For evaluation, we select ISO 3200 and ISO 5000, following the provided training/test split. Note

Figure 3: Architecture of the proposed SelfFormer.

that as the GTs of SIDD-Benchmark and DND are not accessible to users. All denoising results are uploaded to the official websites of these two datasets for calculating the peak signal-to-noise ratio (PSNR) and the structural similarity (SSIM) index,

**Implementation details:** Our work is implemented on PyTorch1.10 and CUDA 11.8, which will be released upon paper acceptance. All experiments are conducted on an NVIDIA A6000 GPU. The grid size of SelfFormer-D is set to image size divided by \(8\), and it doubles for SelfFormer-F. To better address noise correlation, we mask out the \(4 4\) half-plane neighboring locations around the center pixel within the attention window of the DSA of SelfFormer-D during training. This ensures that the pixels used in the attention window are at a distance from the central pixel, thereby reducing the noise correlation as the distance between the pixels increases. SelfFormer-D is optimized using Adam with a learning rate of \(0.0001\), and that of SelfFormer-F is doubled. Other parameters of Adam are set to default. The entire model is trained for 30 epochs for full convergence.

### Performance Evaluation on Real-world Denoising

We include an extensive list of image denoisers for comparison: _(a) Two representative non-learning-based methods_: BM3D  and WNNM ; _(b) Three classic supervised denoisers trained on synthetic noisy data_: DnCNN , CBDNet  and Zhou _et al._; _(c) Five supervised denoisers trained on real-world noisy images of SIDD-Medium_: DnCNN , VDN , AINDNet(R) , DANet  and NAFNet ; _(d) Three unpaired learning methods_ GCBD , C2N+DIDN  and D-BSN+MWCNN ; _(e) Four zero-shot denoisers_: Self2Self , NoisyAsClean , ScoreDVI  and MASH ; and _(f) Twelve Self-supervised denoisers_: Noise2Void , Noise2Self , Laine _et al._, Recrorupted2Recrorupted (R2R) , CVF-SID , AP-BSN , LG-BPN , MM-BSN , SASL , SS-BSN , C-BSN , and PUCA .

Table 1 compares the quantitative results of different methods, where we mark in bold the best results among all methods that do not call any clean images for training, including the non-learning-based, zero-shot and self-supervised methods. It can be seen from Table 1 that our SelfFormer achieved the best results on all the benchmark datasets, in terms of both PSNR and SSIM.

R2R and input-masking-based BSNs, including Noise2Void, Noise2Self, and Laine _et al._'s, rely heavily on the spatial independence of noise, resulting in poor performance on real-world images with locally-correlated noise. In contrast, SelfFormer leverages interactions of distant pixels via its transformer architecture during training, leading to significant performance gains. Compared to AP-BSN, MM-BSN, SASL, and PUCA, SelfFormer's superior performance comes from utilizing the transformer's long-range perception capability for relating distant pixels.

Regarding SA-based BSNs, including LG-BSN and SS-BSN, our SelfFormer performs noticeably better than LG-BSN. Although SS-BSN matches SelfFormer in SSIM on SIDD, its PSNR performance on DND is noticeably worse. These results confirm the higher effectiveness of our SSA module compared to the dilated SA in LG-BSN and the plain grid SA in SS-BSN.

**Visual comparison:** Refer to Figure 4 for a visualization of denoising results from top BSN-based methods. We selecte the images with richly textured regions from SIDD-benchmark and DND for comparison. Laine _et al._failed to break spatial noise correlation, resulting in undesirable denoising. While achieving adequate global denoising, AP-BSN and LG-BPN generate artifacts, and PUCA smooths out some details. We successfully recovers the detailed texture of the clean image by proposed DSA. Unlike SIDD-Benchmark and DND, the SIDD-Validation provides GT images and we choose some images of it for evaluation. See Figure 5 for a visual comparison of these images relies on global information. Both AP-BSN and LG-BPN fail to separate spatial noise from image details, leading to deficient denoising. In contrast, SelfFormer-F effectively eliminate spatial correlation noise while preventing the creation of erroneous flat regions.

**Computational complexity:** Table 2 compares the computational complexity of transformer-based self-supervised denoisers and top CNN-based denoiser, in terms of model size (number of parameters) and inference time for a \(256 256\) image. SelfFormer has a smaller size than the second-best performer, PUCA, and is much faster than transformer-based methods: LG-BPN and SS-BSN.

    &  &  \\  & AP-BSN & MM-BSN & PUCA & LG-BPN & SS-BSN & SelfFormer-F \\  PSNR(dB) & 36.87 & 37.38 & 37.51 & 37.28 & 37.32 & 37.63 \\ \#Param(M) & 3.7 & 5.7 & 12.8 & 4.8 & 6.4 & 10.8 \\ Time(ms) & 382 & 539 & 529 & 5208 & 3976 & 1812 \\   

Table 2: Computational complexity comparison in terms of model size and inference time.

Figure 4: Visual inspection of the results of some samples from SIDD-Benchmark and DND.

Figure 5: Visual comparison on samples from SIDD-Validation.

### Ablation Studies

Following existing works, we conduct the ablation study on SIDD-Validation. We consider four baselines, summarized in Table 3. (a) DSA\(\)Grid SA: The effectiveness of DSA is tested by replacing it with grid SA in SelfFormer-D and reducing the branch number to one, resulting in a true Siamese pair structure similar to SelfFormer-S. Despite some performance gain from mutual learning, this baseline performs significantly worse than the original model. This is due to the arbitrary feature flow, which loses the blind-spot property. (b) w/o CA: We replace all BSCA (CA) modules with DSA (Grid SA) modules. (c) Only SelfFormer-D: We train SelfFormer-D without \(_{}\) and test with it. Without the Siamese structure and mutual learning, the performance noticeably decreases due to the limitations of the constrained attention grids in SelfFormer and the loss of regularization from mutual learning. (d) Only SelfFormer-F: We train SelfFormer-F only using \(_{}\) and test with it. Lacking any blind-spot mechanism and regularization from mutual learning, this network performs very poorly, indicating overfitting to noisy image. (e) w/o DSA: All DSA (Grid SA) modules are replaced with the BSCA (CA) modules. Since the blind spot property is preserved, the performance remains better than "Only SelfFormer-F", which lacks this property. (f) w/o WeightShare: In this case, the SelfFormer-D and SelfFormer-F do not share weight, leading to a performance loss. This suggests that the weight sharing not only reduces the the number of parameters but also provides a regularization effect.

Table 4 shows the experimental results on SIDD-Validation, with varying grid sizes and loss function weight \(\). The results indicate that increasing the grid size can improve performance, as more pixels are utilized in DSA and grid SA. However, setting the loss weight \(\) too small or too large results in a noticeable decrease in performance.

## 5 Conclusion

We presented a transformer-based self-supervised framework for real-world image denoising, optimizing its performance by exploiting distant pixel interactions in transformer to reduce noise correlation. Our key innovation is a DSA module using a half-plane grid for SA, creating a blind-spot structure. A Siamese architecture with mutual learning addressed the performance impact caused by the restricted attention grid in DSA. Experiments on benchmark datasets show our method outperforms existing self-supervised and clean-image-free methods.

While SelfFormer outperforms all related self-supervised denoisers, transformer-based models generally incur higher computational costs compared to CNN-based models. Additionally, there remains a significant performance gap between SelfFormer and supervised denoisers. In the future, we aim to improve both the performance and efficiency of transformer-based denoiser.

    &  \\  Grid size & PSNR(dB) & SSIM & Loss weight & PSNR(dB) & SSIM \\ 
8 & 37.43 & 0.874 & 0.01 & 37.35 & 0.879 \\
12 & 37.51 & 0.879 & 0.1 & 37.57 & 0.881 \\
16 & 37.63 & 0.882 & 1 & 37.63 & 0.882 \\
20 & 37.65 & 0.882 & 10 & 37.51 & 0.880 \\   

Table 4: Results with varying grid sizes and loss function weights on SIDD-Validation

   DSA\(\)Grid SA & w/o CA & Only SelfFormer-D Only SelfFormer-F & w/o DSA & w/o WeightShare & Original \\ 
23.96/0.336 & 37.54/0.881 & 37.29/0.879 & 23.66/0.328 & 37.22/0.880 & 37.45/0.880 & **37.63/0.882** \\   

Table 3: PSNR(dB)/SSIM results of ablation studies on SIDD-Validation.