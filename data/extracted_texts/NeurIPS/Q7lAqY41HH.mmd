# CRAG - Comprehensive RAG Benchmark

Xiao Yang*

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

 Kai Sun*

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Hao Xin*

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Yushi Sun*

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Nikita Bhalla

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Xiangsen Chen

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Sajal Choudhary

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Rongze Daniel Gui

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Ziran Will Jiang

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Ziyu Jiang

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Lingkun Kong

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Brian Moran

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Jiaqi Wang

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Yifan Ethan Xu

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

An Yan

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Chenyu Yang

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Eting Yuan

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Hanwen Zha

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Nan Tang

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Lei Chen

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yue Liu

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yushi Sun*

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yushi Sun*

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Yushi Sun*

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)

Nicolas Scheffer

Meta Reality Labs, 2 FAIR, 3 HKUST, 4 HKUST (GZ)
its potential, RAG still faces many challenges, such as selecting the most relevant information, reducing question answering latency, and synthesizing information to answer complex questions.

A comprehensive benchmark is currently missing to advance continued research efforts in this field. Our goal is to build a benchmark that can provide a holistic view of the important capabilities and fast but reliable evaluation for RAG to propel the area forward. What is a good benchmark for QA over LLMs? We consider five critical features.

1. **Realism:** First and foremost, a good benchmark shall best reflect real use cases. In other words, a solution that achieves high metrics in the benchmark shall also perform very well in real scenarios. For example, the questions in a RAG benchmark shall be similar to questions people ask in real-world QA scenarios.
2. **Richness:** The benchmark shall contain a diverse set of instance types, covering both common use cases and some complex and advanced use cases, to represent real-world challenges and reveal possible limitations of existing solutions.
3. **Insightfulness:** The benchmark shall allow for an easy understanding of performance on different slices of the data, reflecting the capability of the solution in addressing different types of challenges.
4. **Reliability:** The benchmark shall allow reliable assessment of metrics: the ground truths shall be accurate; the metrics shall well capture the performance of the model; the evaluation shall be easy and reliable, and the computed metrics shall hold statistical significance.
5. **Longevity:** Finally, to enable research and experimental comparison in a long term, the scenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed and improved over time.

We strive to create a benchmark that have all of the aforementioned features, and we call it _CRAG - Comprehensive benchmark for RAG_. Our work makes three contributions.

Our first contribution is the dataset itself (Section 3). CRAG contains a _rich_ set of 4,409 QA pairs from five domains: _Finance, Sports, Music, Movie_, and _Open domain_. In addition to simple-fact questions (asking for an attribute of an entity), CRAG contains seven types of complex questions to cover real user queries: questions with _Conditions_, _Comparison_ questions, _Aggregation_ questions, _Multi-hop_ questions, _Set queries_, _Post-processing-heavy_ questions, and _False-premise_ questions. CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from seconds to years, allowing easy deep dives for _insights_. As we generated the questions, we referred to smart assistant use cases to make sure the questions are _realistic_, paraphrased the questions to increase the _diversity_ of expressions, and manually verified ground truths to ensure _reliability_.

In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of available information. This includes up to 50 full HTML pages for each question returned from a real-world search engine--the Brave Search API , and mock KGs with 2.6 million entities. For the mock KGs, we deliberately make sure that the retrieval candidates reflect noises in a _realistic_ setting.

Our second contribution is the evaluation mechanism to allow for _reliable_ comparisons. We designed 3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph and web retrieval, and end-to-end retrieval-augmented generation (Section 2). Instead of computing the percentage of correctly answered questions, our score system distinguishes hallucinated answers and missing answers, and gives the former a higher penalty as it can be more harmful to ruin user

Figure 1: QA using LLMs (a) without RAG vs. (b) with RAG.

trust. We also design an effective automatic evaluation mechanism to allow for fast evaluations and iterations (Section 4).

Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry state-of-the-art solutions on RAG (Section 5). Whereas most advanced LLMs achieve \( 34\%\) accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions answer only \(63\%\) questions without any hallucination, still having much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity. These evaluations serve two roles: first, they demonstrate that CRAG has appropriate level of difficulty and allows insights drawn from different dimensions of diversities the benchmark has incorporated; second, they highlight the gaps and research directions to a fully trustworthy QA system.

The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands of participants and submissions within the first 50 days of the competition. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions.

**Comparison with existing benchmarks.** Table 1 compares CRAG with existing benchmarks for factual question answering. Traditional QA benchmarks such as Natural Questions (NQ) , TriviaQA , MS MARCO , and QALD-10  have advanced QA in the past decade but consider only web retrieved _or_ KG retrieved contents, and do _not_ adequately represent the diverse and dynamic challenges that RAG is facing. New benchmarks for LLM or RAG usually target certain capabilities of the QA system. Researchers created benchmarks to evaluate how well the systems can answer simple knowledge questions  and handle more advanced scenarios. These include answering questions with changing answers , integrating information from multiple documents , addressing multi-hop questions , and answering questions with long texts . Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE  or F1 to evaluate the quality of the responses . These metrics, although working well for extractive methods, are known to not perform very effectively for LLMs that generate free-form responses .

Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages: comprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact popularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. These features make CRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing a shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information retrieval and synthesis challenges for reliable LLM-based question answering.

## 2 Problem Description

A RAG QA system takes a question \(Q\) as input and outputs an answer \(A\); the answer is generated by LLMs according to information retrieved from external sources or directly from the knowledge internalized in the model. The answer should provide useful information to answer the question without adding any hallucination.

   Benchmark & Web & KG & Mock & Dynamic & Torso and & Beyond & Question \\  & retrieval & search & API & question & tail facts & Wikipedia & size \\  QALD-10  & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ & 0.8K \\ MS MARCO  & ✓ & ✗ & ✗ & not explicitly & not explicitly & ✓ & 100K \\ NQ  & ✓ & ✗ & ✗ & not explicitly & not explicitly & ✗ & 323K \\ RGB  & ✓ & ✗ & ✗ & ✗ & ✓ & 1K \\ FreshLLM  & ✗ & ✗ & ✗ & ✓ & ✗ & ✓ & 0.6K \\ CRAG & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & 4.4K \\   

Table 1: Comparing CRAG to existing benchmarks for factual question answering.

We designed three tasks. They share the same set of (question, answer) pairs but differ in the external data accessible for retrieval to augment QA. Here, we provide the content that can be leveraged in QA to ensure fair comparisons. We describe how we generated the data in Section 3.

**Task 1: Retrieval Summarization.** In Task 1, we provide up to five web pages for each question. These web pages are likely, but not guaranteed, to be relevant to the question. This task aims to test the answer generation capability of a RAG system.

**Task 2: KG and Web Retrieval Augmentation.** In Task 2, we in addition provide _mock APIs_ to access information from underlying _mock KGs_. The mock KGs store structured data relevant to the questions; answers to the questions may or may not exist in the mock KGs. The mock APIs take input parameters, oftentimes parsed from the question, and provide structured data from the mocked KGs to support answer generation. This task tests how well a RAG system 1) queries structured data sources and 2) synthesizes information from different sources.

**Task 3: End-to-end RAG.** Similar to Task 2, Task 3 also provides both web search results and mock APIs as candidates for retrieval but provides \(50\) web pages, instead of \(5\), as candidates. The larger set of web pages are more likely to provide necessary information to answer the question, but meanwhile are more likely to contain noises. As such, Task 3 in addition tests how a RAG system ranks a larger number of retrieval results.

The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG systems. The only component of a RAG system not covered by these tasks is search retrieval. One may easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully end-to-end testing.

## 3 Dataset Description

CRAG contains two parts of data: the QA pairs and the contents for retrieval. We now describe each part of the data. Data generation details can be found in Appendix A.1.1-A.1.6.

 p{284.5pt}}  
**Question type** & **Definition** \\  Simple & Questions asking for simple facts that are unlikely to change overtime, such as the birth date of a person and the authors of a book. \\  Simple w. Condition & Questions asking for simple facts with some given conditions, such as stock prices on a certain date and a director’s recent movies in a certain genre. \\  Set & Questions that expect a set of entities or objects as the answer (e.g., “_what are the continents in the southern hemisphere?_”). \\  Comparison & Questions that compare two entities (e.g., “_who started performing earlier, Adele or Ed Sheeran?_”). \\  Aggregation & Questions that require aggregation of retrieval results to answer (e.g., “_how many Oscar awards did Meryl Strep win?_”). \\  Multi-hop & Questions that require chaining multiple pieces of information to compose the answer (e.g., “_who acted in Ang Lee’s latest movie?_”). \\  Post-processing-heavy & Questions that need reasoning or processing of the retrieved information to obtain the answer (e.g., “_how many days did Murgood Marshall serve as a Supreme Court justice?_”). \\  False Premise & Questions that have a false preposition or assumption (e.g., “_What’s the name of Taylor Swift’s rap album before she transitioned to pop?_” (Taylor Swift has not yet released any rap album)). \\   

Table 2: Definition of CRAG question types.

[MISSING_PAGE_FAIL:5]

Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall of 85% when using all 50 pages. It reflects multiple advantages of the benchmark by design. First, the recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is about 69%. This is comparable to what we observe in practice when developing a RAG system. The non-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit "I don't know" when the retrieval results do not contain the necessary information. Second, compared to the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the importance of extracting and understanding HTML contents. Moreover, the estimated web search recall (50 web pages) is \(93\%\) for _Web Questions_ and \(74\%\) for _KG Questions_, indicating significantly lower recall for KG questions than web questions. This aligns with our observations that web search recall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in Task 2 and 3.

**Mock KGs.** We created mock KGs that contain publicly available KG data used to generate the questions, randomly selected entities of the same type, and also "hard negative" entities with similar names (e.g., _"phantom"_ for _"phantom of the opera"_).

**Mock APIs.** We created mock APIs with pre-defined parameters to support structured search in the mock KGs. For example, for queries asking for stock prices, an example mock API is in the form of get_price_history(ticker).

We collected snapshots of the KG and web search data concurrently while posing real-time and fast-changing questions. This approach ensures that we capture the "snapshot" of the information world at the time of question answering. A RAG solution that performs well on the benchmark should also be capable of reasoning over time and generalizing to evolving questions.

In total, the resulting data contains 220K webpages, a KG of 2.6M entities, and 38 mock APIs. See Table 9 in the Appendix for a complete list of the mock APIs.

## 4 Metrics and Evaluation

In this section, we present the metrics for evaluating the RAG systems and briefly describe the 2024 Meta KDD Cup challenge in Appendix A.2.3.

### Metrics

We use a scoring method to assess the performance of RAG systems. For each question in the evaluation set, we first label the answer with **perfect, acceptable, missing,** or **incorrect**, according to the following criteria.

**Perfect.** The response correctly answers the user's question and contains no hallucinated content.

Figure 2: For \(85\%\) of CRAG questions, the web search results are estimated to contain the ground truth facts. The curve shows that the retrieval recall grows sharply at the beginning and flattens out later on.

**Acceptable.** The response provides a useful answer to the user's question but may contain minor errors that do not harm the usefulness of the answer.

**Missing.** The response is "I don't know", "I'm sorry I can't find...", a system error such as an empty response, or a request from the system to clarify the original question.

**Incorrect.** The response provides wrong or irrelevant information to answer the user's question.

We use a scoring method with score \(1\), \(0.5\), \(0\), and \(-1\) for each _perfect, acceptable, missing_, and _incorrect_ answer, respectively, where we penalize hallucinated answers and prefer _missing_ answers to _incorrect_ ones. We then define **truthfulness** as the average score from all examples in the evaluation set for a given RAG system.

### Evaluation

Similar to previous work , we employ both human evaluation **(human-eval)** and model-based automatic evaluation **(auto-eval)**. In the former, we use manual grading to judge _perfect, acceptable, missing_, and _incorrect_ for each answer. In the latter, we merge _perfect_ and _acceptable_, call it **accurate**, and use a three-way scoring system with \(1,-1,0\) for _accurate_, _incorrect_, and _missing_ answers.

We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly, it is considered _accurate_; otherwise, we use LLMs to determine whether the response is _accurate, incorrect_, or _missing_. To avoid the _self-preference_ problem , we use two LLM evaluators: ChatGPT (gpt-3.5-turbo-0125)  and Llama 3 (llama-3-70B-instruct)  and report the average _accurate, hallucination, missing_ rates, and _truthfulness_ scores from the two models for each RAG system. Our offline experiment shows that this two-step method yields an average F1 score of \(94.7\%\) for ChatGPT and \(98.9\%\) for Llama 3 compared to human-eval. See Appendix A.2.2 for more details.

**Test data split.** We split the data randomly into _validation_ (30%), _public test_ (30%), and _private_ (40%), and released the validation and public test sets (Appendix A.2.3). Participants of the KDD Cup challenge can use the validation and public test sets to develop and test their models, and the submitted solutions were evaluated on the private test set. Future offline users of CRAG can use the validation set for development, fine-tuning, and validation, and the public test set for testing and result reporting.

## 5 Benchmarking

In this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating that CRAG has a reasonable level of difficulty and can help draw insights and show directions in developing RAG techniques.

### Straightforward RAG solutions

**Experiment setup:** We started with running LLM-only solutions on the CRAG public test set with \(1,335\) questions, using simple prompts that encourage brief answers and _"I don't know"_ answers when the confidence is low (Appendix A.3.1). We employed Llama 2 Chat (llama-2-7b-chat and llama-2-70b-chat) , Llama 3 Instruct (llama-3-8B-instruct and llama-3-70B-instruct) , Mixtral (Mixtral-8x7B-Instruct-v0.1) , Falcon (40B) , FLAN-T5 (FLAN-T5-XXL) , and GPT-4 Turbo (gpt-4-turbo-2024-04-09) . The web-only RAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon and FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated webpage snippets using the original order from the data as the reference text, until filling up the window (similar to ). Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length KG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama 3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant query entities using llama-3-8B-instruct with in-context learning (similar to ) detailed in Appendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the extracted entities), until filling up the window. We provide an extensive comparison of all LLMs in Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4 Turbo) in this section.

Table 5 shows the average evaluation results from the two auto-evalators (ChatGPT and Llama 3) and illustrates that the CRAG benchmark is _non-trivial_. First, the best LLM-only solutions (GPT-4 Turbo) obtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information _does_ help answer more questions reliably. Interestingly, none of the RAG solutions obtain truthfulness higher than 20%; this is because all RAG solutions introduce more hallucinations generated from irrelevant retrieval results, showing a big challenge in RAG--_How to judiciously use retrieval results without being distracted by retrieval noises?_ Third, we found that Task 2 truthfulness scores are higher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even lower hallucination rate, because the KG knowledge is typically brief but precise. Unfortunately, the improvement is mediocre, showing a second challenge in RAG--_How to best leverage the power of KG data?_ Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search ranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results) and better search recall. In particular, we found that the ground truths of over 30% of questions are available in the web retrieval results but are not included in the prompt due to the context window limitation. This shows _the importance of search ranking_ in RAG.

Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type dimension. The results reveal a lot of interesting observations and show that the CRAG benchmark allows more _insightful_ conclusions. First, it shows _which slices of the benchmark are harder_. For example, we found much lower RAG truthfulness on the _Finance_ and _Sports_ domains, for _real

   & **Model** & **Accuracy** & **Hallucination** & **Missing** & **Truthfulness\({}_{a}\)** \\ 
**LLM only** & Llama 3 70B Instruct & 32.3 & 28.9 & 38.8 & 3.4 \\  & GPT-4 Turbo & **33.5** & **13.5** & 53.0 & **20.0** \\ 
**Task 1** & Llama 3 70B Instruct & 35.6 & 31.1 & 33.3 & 4.5 \\  & GPT-4 Turbo & **35.9** & **28.2** & 35.9 & **7.7** \\ 
**Task 2** & Llama 3 70B Instruct & 37.5 & 29.2 & 33.3 & 8.3 \\  & GPT-4 Turbo & **41.3** & **25.1** & 33.6 & **16.2** \\ 
**Task 3** & Llama 3 70B Instruct & 40.6 & 31.6 & 27.8 & 9.1 \\  & GPT-4 Turbo & **43.6** & **30.1** & 26.3 & **13.4** \\  

Table 5: Performance of straightforward RAG solutions. All numbers are in percentage. LLM-only solutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. The subscript in “truthfulness\({}_{a}\)” denotes the result is reported by auto-eval.

Figure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain, dynamism, popularity, and question type.

time_ and _fast-changing_ facts, for _tail_ entities, and for complex questions requiring _set answers, post-processing,_ and with _false premises_. Second, it shows _where it is harder to leverage retrieval results_. Take the popularity slices as an example, we observed that GPT-4 Turbo's truthfulness dropped from head (21%) to torso (11%) to tail (8%), consistent with past observations ; however, the straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso (+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). Finally, although our goal is _not_ to compare different LLMs, the different dimensions allow us to understand the strengths and weaknesses of each method. For example, although the RAG system based on Llama 3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a similar or slightly higher truthfulness in answering _simple_ and _comparison_ questions, whereas much lower truthfulness in answering _set_ and _post-processing_ questions, suggesting investigations on the reasoning capabilities.

### State-of-the-art industry solutions

Next, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. We selected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG questions, collected the responses, and applied manual grading (details in Appendix A.4).

In addition, we applied traffic weights to the questions to understand the solutions in real-world use cases. The traffic weights reflect the frequency of each question type, as defined in Table 2, in real QA traffic. We gave the same weights to all domains and reported the macro average across domains. This is because we have observed quite different domain-level distributions in different use cases, but have been observing similar distributions at the query-type level.

Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance across different dimensions. The evaluation results confirm our belief that the CRAG benchmark _reveals interesting insights and shows room for improvement for existing RAG solutions._ First, the results from SOTA solutions achieve much better truthfulness (highest \(51\%\)) compared to the straightforward solutions. However, the hallucination rate ranges from 16% to 25%, so the answers are still _not_ trustworthy. Note that the truthfulness scores between the SOTA solutions and the straightforward solutions are not completely comparable, as they have different accesses to retrieval contents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval; however, the trend is valid. Second, we observed very different latency, ranging from \(3.4\)s to \(11.6\)s, reflecting the different design options in trading off latency and quality; for example, Copilot Pro has the highest truthfulness, but meanwhile highest latency, whereas Meta SG  has mid-tier truthfulness but lowest latency. (See Appendix A.4.2 for additional results and how we measured latency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for SOTA solutions: _real-time_ and _fast-changing_ queries, and questions regarding _torso_ and _tail_ entities, showing the improvement needed for handling retrieval noises when the system relies on retrieval results to answer the question; as another example, we see lower truthfulness for queries requiring _aggregation, multi-hop reasoning_ or _post-processing_, showing the improvement space for reasoning

    & **System** & **Perfect** & **Acc.** & **Hall.** & **Miss.** & **Truth\({}_{h}\)** & **Latency (ms)** \\ 
**Equal weighted** & Copilot Pro & **62.6** & 11.7 & 17.9 & 7.8 & **50.6** & 11,596 \\  & Gemini Advanced & 60.8 & 10.1 & 16.6 & 12.5 & 49.3 & 5,246 \\  & ChatGPT Plus & 59.8 & **13.3** & 25.0 & 1.9 & 41.5 & 6,195 \\  & Meta SG & 52.5 & 9.7 & **16.0** & 21.8 & 41.4 & **3,431** \\  & Perplexity.ai & 55.8 & 8.8 & 25.3 & 10.1 & 34.9 & 4,634 \\ 
**Traffic weighted** & Copilot Pro & **70.0** & 9.5 & 14.3 & 6.1 & **60.5** & - \\  & Gemini Advanced & 67.1 & 10.0 & **12.7** & 10.2 & 59.3 & - \\  & ChatGPT Plus & 61.8 & **11.4** & 25.7 & 1.3 & 41.8 & - \\  & Meta SG & 61.0 & 7.1 & 14.1 & 17.8 & 50.5 & - \\  & Perplexity.ai & 63.7 & 6.3 & 20.9 & 9.1 & 45.9 & - \\   

Table 6: Benchmarking CRAG questions with industry SOTA RAG systems. Perfect, acceptable (Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulness\({}_{h}\) reported by human-eval (Truth\({}_{h}\)) are in percentages. The best system achieves truthfulness of 51% and provides perfect answers for up to 63% of questions.

in question answering. Last, truthfulness on _set_ and _false premise_ questions improved significantly in the SOTA solutions compared to the straightforward solutions, showing advancement in RAG systems in providing accurate and complete set answers and detecting _false premises_.

## 6 Conclusion

This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in retrieval-augmented generation (RAG). With detailed empirical studies, CRAG reviewed gaps in existing RAG solutions and provided valuable insights for future improvement. We plan to continue improving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-turn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to emerging challenges, and evolves for new research needs.