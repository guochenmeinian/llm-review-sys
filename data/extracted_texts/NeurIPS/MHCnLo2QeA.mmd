# Estimating Ego-Body Pose from Doubly Sparse Egocentric Video Data

Seunggeun Chi\({}^{}\)

Purdue University

chi65@purdue.edu

&Pin-Hao Huang\({}^{*}\), Enna Sachdeva\({}^{*}\)

Honda Research Institute USA

{pin-hao_huang, enna_sachdeva}@honda-ri.com

&Hengbo Ma\({}^{}\)

Honda Research Institute USA

hengbo.academia@gmail.com

&Karthik Ramani

Purdue University

ramani@purdue.edu

&Kwonjoon Lee

Honda Research Institute USA

kwonjoon_lee@honda-ri.com

co-second authorswork done at Honda Research Institute USA.

###### Abstract

We study the problem of estimating the body movements of a camera wearer from egocentric videos. Current methods for ego-body pose estimation rely on _temporally dense_ sensor data, such as IMU measurements from _spatially sparse_ body parts like the head and hands. However, we propose that even _temporally sparse_ observations, such as hand poses captured intermittently from egocentric videos during natural or periodic hand movements, can effectively constrain overall body motion. Naively applying diffusion models to generate full-body pose from head pose and sparse hand pose leads to suboptimal results. To overcome this, we develop a two-stage approach that decomposes the problem into temporal completion and spatial completion. First, our method employs masked autoencoders to impute hand trajectories by leveraging the spatiotemporal correlations between the head pose sequence and intermittent hand poses, providing uncertainty estimates. Subsequently, we employ conditional diffusion models to generate plausible full-body motions based on these temporally dense trajectories of the head and hands, guided by the uncertainty estimates from the imputation. The effectiveness of our method was rigorously tested and validated through comprehensive experiments conducted on various HMD setup with AMASS and Ego-Exo4D datasets. Project page: https://sgchi.github.io/dsposer/

## 1 Introduction

The evolution of augmented reality (AR) devices such as the Apple Vision Pro, Meta Quest 3, Microsoft HoloLens 2, and etc. has dramatically reshaped interactive technologies. These head-mounted displays (HMDs) feature inertial measurement units (IMUs) and video capture capabilities, offering a unique egocentric perspective. However, their limited visibility of the user's body parts poses a significant challenge for accurate egocentric body pose estimation--a key element for immersive AR experiences.

Previous approaches have tackled this problem by spatially reconstructing the entire body from spatially sparse data. For instance, EgoEgo  first estimates head poses using SLAM on the egocentric video, then generates body poses from these estimated head positions. Other methods, such as AvatarPoser  and BoDiffusion , primarily depend on temporally dense tracking signal from spatially sparse body parts, notably the head and hands. This dependency on specific hardwaresuch as head-mounted displays and hand controllers constrains their versatility and diminishes their applicability in broader AR/VR scenarios where hand controllers might not be used, like sports training or analysis applications where the user needs to move freely without holding any devices, or augmented reality experiences in outdoor environments where carrying controllers is impractical.

We observe that even _temporally sparse_ observations, such as hand poses captured intermittently from egocentric videos during natural or periodic hand movements, can effectively constrain overall body motion. While it is possible to utilize other visible body parts such as feet or elbows, we opted to rely on hand poses. This decision is based on the availability of hand pose detectors [24; 14] and the fact that hands are visible in approximately 20\(\%\) of video frames, as demonstrated in Table 8. Unlike previous work that concentrated only on spatial completion, our method incorporates temporal completion by leveraging the intermittent appearance of hands in egocentric videos. This dual completion approach not only enhances the robustness of body pose estimation under varying conditions but also reduces reliance on specific sensor hardware, making it more adaptable to various AR environments. In our setup, we use temporally sparse 3D hand poses from detections in egocentric videos combined with dense head tracking signals to reconstruct the full body. Initially, we temporally complete sparse hand information using a Masked Autoencoder (MAE) , which estimates hand pose trajectories by capturing the spatiotemporal correlations between intermittent hand poses and head tracking signals. We develop a probabilistic extension of the MAE to provide uncertainty estimates of the predicted hand pose sequence. Subsequently, using a conditional diffusion model, we spatially reconstruct the full body based on the head tracking signal data and imputed hand trajectories along with their predictive uncertainties. We call our approach **DSPoser** (Doubly Sparse Poser) because it can effectively utilize data that is _doubly sparse_ (sparse both temporally and spatially), as shown in Figure 1.

This flexible framework is designed to seamlessly adapt to diverse AR/VR setups and devices, ranging from spatially sparse scenarios (e.g., using only head tracking signal or combining it with hand controllers) to doubly sparse scenarios (utilizing head signal data alongside hand detection from egocentric video). The key advantage lies in the assumption that the HMD's tracking signal is consistently available, enabling our approach to function across a wide range of environments and hardware configurations. Extensive experiments have proved our model's versatility and accurate pose estimation capabilities in various settings. Furthermore, our ablation studies highlight the significance of incorporating uncertainty estimates, as this crucial information enhances the overall quality of pose estimation, resulting in more reliable outputs. By addressing both temporal and spatial completion through our double completion approach, we have developed a robust and adaptable solution that reduces dependency on specific sensor hardware, making it well-suited for immersive AR experiences in diverse scenarios, such as sports training, outdoor environments, and beyond.

Figure 1: Overview of DSPoser. Our goal is to estimate ego-body pose without dependency on hand controllers in an HMD environment. (a) Given the egocentric video and head tracking signals as input, (b) our approach first predicts the hand pose in the frames where hands are visible (dark blue). It then estimates the hand poses in frames with invisible hands (light blue) using imputation, and (c) estimates uncertainty associated with the hand poses where the hands are invisible, (d) The predicted and imputed hand pose is then used with head pose to predict the 3D full body pose.

In summary, our research presents three key contributions:

* A robust and versatile framework for egocentric body pose estimation tailored for HMDs. The framework adapts to various AR/VR settings and can leverage tracking signals available in most modern HMD devices without controllers.
* We decomposed the problem into temporal completion and spatial completion. Our approach captures the uncertainty from hand trajectory imputation to guide the diffusion model for accurate full-body motion generation.
* Extensive evaluations demonstrating the effectiveness of our framework on diverse datasets, outperforming existing methods and underscoring its potential for enhancing user interaction and immersion in AR experiences.

## 2 Problem Formulation

In our work, we aim to estimate the 3D human pose of a HMD user from sequences of RGB video and head tracking signal. We note that head tracking signal data is commonly accessible from IMU in most HMDs, such as Meta Quest and Apple Vision Pro. Suppose we are given an egocentric video \(_{}=\{_{1},,_{T_{w}}\}\) where \(_{}\) is an RGB image and \(T_{w}\) denotes the sequence length, and a corresponding head tracking signal sequence \(_{}=\{_{1},,_{T_{w}}\}\) where \(_{}^{D_{head}}\) and \(D_{head}\) is the dimension of head tracking signal including 3D pose. Our goal is to estimate the full body pose \(=\{_{1},,_{T_{w}}\}\), where pose state \(_{}^{J D}\) at time \(\), \(J\) is the number of body joints and \(D\) is the dimensionality of pose state. We solve the problem of estimating \(p(|_{ego},_{})\) by decomposing it into 2 the stages of imputation and generation, assuming that we have _temporally sparse_ hand data \(}\) from hand detection module \(f()\): \(}=f(_{ego})\). We first _temporally complete_ hand trajectory \(}\) based on \(}\) and \(_{}\), which can be written as \(p(}|},_{})\). Then, we _spatially complete_ full body pose \(\) from the imputed hands \(}\) and \(_{}\), which can be written as \(p(|},_{})\). Since \(}\) is a probabilistic variable, we need to marginalize over \(}\) as follows:

\[p(|_{ego},_{})=_{}}p(|},_{})p( }|f(_{ego}),_{}).\] (1)

## 3 Methods

### Detection: Hand Pose Estimation from Egocentric Video

In this work, we estimate the 3D position of the hand from an egocentric camera using a two-step process. First, we use FrankMocap  to predict hand poses as SMPL-X parameters , from which we extract local 3D hand joint positions relative to the root of the hand model's kinematic tree, denoted as \(_{h}^{3D}^{21 3}\). Simultaneously, we use RTM-Pose  to estimate 2D hand joint positions in the image, \(_{I}^{2D}^{21 2}\). Finally, we determine the 3D hand joint positions in the camera coordinate system, \(_{I}^{3D}=_{h}^{3D}+\) by solving for \(^{3}\) that minimizes the reprojection

Figure 2: Overall pipeline of our proposed work DSPoser, composed of _Temporal Completion_ stage and _Spatial Completion_ stage to tackle pose estimation problem from _doubly sparse_ data.

error \(\|_{I}^{2D}-(_{h}^{3D}+)\|_{2}\). Here, \(\) is the intrinsic matrix, obtained by transforming the original camera parameters into a pinhole model through undistortion.

To better constrain the hand trajectories, we attempted to obtain rotation information from the 3D hand detection. However, due to the inconsistent quality of hand detection, the rotational information derived from the hand pose was noisy. Therefore, we decided not to incorporate this rotational information into our hand tracking approach on the Ego-Exo4D dataset. We utilized only the 3D wrist location from the Ego-Exo4D dataset, represented by \(D_{hand}=3\). In contrast, for the AMASS dataset, we leveraged both rotational information and 3D location, as this data is readily available, resulting in \(D_{hand}=9\).

### Temporal Completion: Hand Trajectory Imputation from Sparse Hand Pose

Masked Auto-Encoder (MAE)In our work, we employed a Masked Autoencoder (MAE)  to impute missing hand trajectories using head tracking signal \(_{head}\) and detected hand pose \(}\). Inspired by Vision Transformer (ViT), we treated each \(_{}\) and \(}_{}\) at time \(\) as a token similar to an image patch in ViT. To accommodate this, we implemented two embedding layers, one for head tracking signal \(_{}^{D_{head}}\) and the other for hand \(}_{}^{D_{hand}}\), both projecting into the common token dimension \(D_{M}\). For the AMASS dataset, we follow the head tracking signal representation \(D_{}=18\) as in . For the Ego-Exo4D dataset, \(D_{}=15\), which includes head position and left/right IMU signals. Consequently, the total number of token amounts to \(3 T_{w}\), where 3 accounts for the head and both hands, and \(T_{w}\) is the sequence length. Sinusoidal positional encoding (PE) is used for both the encoder and decoder patches after tests showed it suffices for learning different modalities, compared to learnable PE. In an HMD environment, we assume that the head tracking signal \(_{head}\) is always available, but hand visibility depends on the egocentric video. Thus, masking is applied only to the hand tokens based on their visibilities within egocentric view.

In contrast to the MAE  training approach, which maintains a consistent number of masked patches due to a fixed masking ratio, the count of frames with invisible hand varies across instances in our setup. To address this variability, our encoder selectively applies attention masking to these inputs, ensuring that queries do not attend to tokens where hand is invisible. This attention masking technique adapts dynamically to the fluctuating numbers of missing frames across the instances, enhancing the model's ability to handle data sparsity effectively. For decoder, we adopted MAE decoder design except the last projection layer to guide the uncertainty. To capture the uncertainty, we split the final projection layer into two heads for mean and variance of a Gaussian distribution.

Uncertainty-aware MAEFollowing the [26; 30], to make the MAE aware of the predictive uncertainty of imputed hand pose sequence, we employ the \(\)-NLL loss  function to manage uncertainty by using a set of mean heads \(_{i}()\) and variance heads \(_{i}^{2}()\), which are derived from \(M\) models initialized differently, where \(=[};]\) is an input to the MAE and \(i[1,M]\). The mean heads \(_{i}()\) and variance heads \(_{i}^{2}()\) are trained using the Gaussian negative log-likelihood loss, which applies to each sample indexed by \(n\) with input \(_{n}\) and ground truth hand pose sequence \(_{n}\).

\[L_{-}(_{n},_{n}) =(_{i}^{2})L_{}(_{n}, _{n}),\] (2) \[L_{}(_{n},_{n}) =^{2}(_{n})}{2}+( _{n})-_{n})^{2}}{2_{i}^{2}(_{n})}.\] (3)

The \(L_{}\) loss function causes the predicted variance to act as a weighting factor for each data point, emphasizing those with higher variances. The parameter \(\) adjusts the intensity of this weighting. The sg\(()\) function is used to apply the stop-gradient operation, thus preventing gradients from propagating through this part of the computation.

After training, we measure the aleatoric (data) uncertainty \(_{ale}()\) by averaging the variances across models, and epistemic (model) uncertainty \(_{epi}()\) by calculating variance of model means, and total uncertainty by adding both uncertainties:

\[_{ale}() =_{i}[_{i}^{2}()] M^{-1}_{i} _{i}^{2}()\] (4) \[_{epi}() =_{i}[_{i}()]\] (5) \[_{tot}() =_{ale}()+_{epi}()\] (6)Note that \(_{ale}()\) and \(_{epi}()\) provide uncertainties for each frame and each pose state dimension. The captured uncertainty is visualized in Figure 3, demonstrating that MAE effectively captures uncertainty.

Spatial Completion: Uncertainty-guided Body Pose Generation from Imputed Hand Trajectories and Head Tracking Signal

We employed the VQ-Diffusion  to generate full body poses from imputed hand trajectories and head tracking signal. The exposition of VQ-Diffusion can be found in Section D of the Appendix. As illustrated in Figure 2, our motion generation module is designed to generate human motion sequences from the temporally dense hand and head trajectories with uncertainty obtained from the MAE model.

Vq-VaeWe first train the VQ-VAE to represent human motion with a discrete codebook representation as described in Appendix D.1. We mostly followed the architectural design and training methods of . After the codebook representation is learned by the VQ-VAE, we utilize this latent codebook representation to train a denoising diffusion model.

Denoising TransformerMotivated by the work of VQ-Diffusion, we design a denoising transformer that estimates the distribution \(p(_{0}|_{t},)\). An overview of our proposed model is depicted in Figure 2. We closely follow the implementation of . To incorporate the diffusion step \(t\) into the network, we employ the adaptive layer normalization (AdaLN) . We concatenated the estimated hand and head trajectory with codebook after a embedding layer, to match the dimension with codebook representation. Finally, we use the decoder to decode \(_{0}\) to obtain a full body pose sequence.

Uncertainty GuidanceWe introduce several strategies to guide the denoising process using uncertainty estimates of imputed hand trajectories: sampling, dropout, and distribution embedding.

For _sampling_, we sample a hand sequence from the distribution \(}(^{*}(),^ {*}()})\) and regard it as the conditioning vector \(\), where \(^{*}()=_{i}[_{i}()] M^{-1}_{i} _{i}()\) and \(^{*}()\) is measured by one of Eq. (9), (10), and (11). While it would be ideal to sample multiple times to better approximate the marginalization in Equation 1, we find just using one sample provides a competitive performance.

For _dropout_, we set each dimension of \(()\) to zero with a certain probability, which is determined by the corresponding dimension of \(^{*}()\), and denote the result as \(y\). The probability of the \(d\)-th dimension of \(()\) being zero is \(p_{d}=1-(^{*}_{d}()-^{*}_{d^{min}}())/(^{*}_{d^{max}}()-^{*}_{d^{min}}())\) where \(^{*}_{d}()\) is the \(d\)-th dimension of \(^{*}()\), and \(^{*}_{d^{min}}()\), \(^{*}_{d^{max}}()\) are the minimum and maximum values over the sequence length, respectively.

For _distribution embedding_, we embed the Gaussian distribution \((^{*}(),^{*}()})\) to a vector by concatenating the \(^{*}()\) and \(^{*}()\) in the feature dimension. The resulting embedding will be further concatenated with the head pose sequence to form a conditioning vector \(\).

## 4 Experiments

### Datasets \(\&\) Evaluation Metrics

Ego-Exo4D datasetEgo-Exo4D  contains simultaneous captures of egocentric (first-person) and exocentric (third-person) video perspectives of participants performing complex activities like sports, dance, and mechanical tasks. The dataset comprises 1,422 hours of video ranging from 1 to 42 minutes per video. In addition to video, it provides camera poses, IMU data, and human pose annotations. Specifically for the egopose task, it includes separate training and validation video sets containing 334 and 83 videos respectively. Our problem formulation of ego body pose estimation differs from the ego body pose prediction task from , which aims to predict a single future frame given a specific time window.

AMASS datasetThe AMASS dataset  is a large human motion database that unifies different existing optical marker-based MoCap datasets by converting them into realistic 3D human meshesrepresented by SMPL  model parameters. Following the AvatarPoser  evaluation, we used the CMU , BMLrub , and HDM05  subsets from the AMASS dataset and their preprocessing of tracking signal information. Since AMASS does not include RGB images, we set \(D_{hand}=9\) assuming that 3D hand position and 6D rotation is available when the hand is "visible". To determine visibility, we compute the angle between the z-axis vector of the head rotation and the vector from the head position to the hand. We define the hand as "visible" if this angle is within a 45\({}^{}\) range, corresponding to a 90\({}^{}\) field of view (FoV) of HMD devices.

Evaluation metricWe evaluate our results using the following metrics: Mean Per Joint Position Error (MPJPE), Mean Per Joint Velocity Error (MPJVE), and Mean Per Joint Rotation Error (MPJRE), following the evaluation of [13; 3]. Since Ego-Exo4D dataset doesn't have the annotations for 6D rotation, MPJRE is reported only for AMASS. We report all values with the confidence interval of 95%. We also provide details on MPJPE across hands, upper body above the pelvis, and lower body below the pelvis, denoted as Hand PE, Upper PE, and Lower PE, respectively.

### Full Body Pose Estimation from Doubly Sparse data

To demonstrate the effectiveness of our framework on doubly sparse egocentric video data, we investigated the results of our framework, DSPoser, on the AMASS dataset and Ego-Exo4D, as shown in Table 1 and Table 2, respectively. Since the task of body pose estimation from doubly sparse data is newly introduced in our paper, we compare our results to other baselines, EgoEgo , Bodiffusion , AvatarPoser , and AvatarJLM . Those baselines are designed to estimate human body poses from spatially sparse data. EgoEgo estimates body poses from head poses, and the others estimate body poses from head and hand tracking signals. We report the experimental results using the sampling strategy with aleatoric uncertainty unless otherwise stated. To train the baslines on temporally sparse data, we extend the algorithm as follows: (1) _Interpolation_: we imputed hand poses with linear interpolation; (2) _MAE_: we use our trained MAE to impute the hand trajectory. In \(T_{s}=1\) setup, we report our result after averaging 16 samples while the result in \(T_{s}=20\) setup is from a single sample.

As shown in Table 1, DSPoser consistently outperforms baseline methods on AMASS across all metrics, underscoring the effectiveness of our two-stage approach for ego-body pose estimation. DSPoser achieves notable improvements in MPJPE for both sliding window sizes, \(T_{s}=20\) and \(T_{s}=1\). For \(T_{s}=20\), DSPoser reduces MPJPE from 7.35 cm to 5.51 cm, significantly outperforming the Bodiffusion extension, which uses MAE to impute invisible hands. For \(T_{s}=1\), DSPoser

  Methods & \(T_{s}\) & **x** & Imputation & **y** & MPJPE & MPJVE & MPJRE \\   VQ-VAE (Recons) & 20 & Full body & - & Full body & 1.26 & 11.37 & 1.81 \\  EgoEgo\({}^{+}\) & 20 & \(\) & - & \(\) & 19.17 & 46.17 & 7.30 \\ Bodiffusion\({}^{+}\) & 20 & \(\) & - & \(\) & 19.27 & 60.29 & 8.51 \\
**DPSoser (Ours)** & 20 & \(\) & - & \(\) & \(12.08^{ 0.04}\) & \(75.07^{ 0.26}\) & \(7.04^{ 0.02}\) \\
**DPSoser (Ours)** & 20 & \(\) & MAE & \(\) \& \(\) & \(^{ 0.02}\) & \(^{ 0.05}\) & \(^{ 0.01}\) \\  Bodiffusion  & 20 & \(\) \& \(\) & Interpolation & \(\) \& \(\) & 46.45 & 75.33 & 17.99 \\ Bodiffusion  & 20 & \(\) \& \(\) & MAE & \(\) \& \(\) & 7.35 & 31.33 & 5.47 \\
**DPSoser (Ours)** & 20 & \(\) \& \(\) & MAE & \(\) \& \(^{ 0.02}\) & \(^{ 0.10}\) & \(^{ 0.02}\) \\  AvatarPoser  & 1 & \(\) \& \(\) & Interpolation & \(\) \& \(\) & 40.42 & 64.07 & 16.37 \\ AvatarJLM  & 1 & \(\) \& \(\) & Interpolation & \(\) \& \(\) & 25.02 & 68.42 & 14.14 \\ AvatarPoser  & 1 & \(\) \& \(\) & MAE & \(\) \& \(\) & 9.88 & 62.31 & 5.98 \\ AvatarJLM  & 1 & \(\) \& \(\) & MAE & \(\) \& \(\) & 7.12 & **37.60** & 5.24 \\
**DPSoser (Ours)** & 1 & \(\) \& \(\) & MAE & \(\) \& \(\) & \(^{ 0.13}\) & \(49.12^{ 0.24}\) & \(^{ 0.10}\) \\  

Table 1: Performance comparisons across baseline models for doubly sparse video data on the **AMASS** test set. We report MPJRE [\({}^{}\)], MPJPE [cm], and MPJVE [cm/s], with the best results highlighted in **boldface**. Models trained by us are marked with \({}^{*}\). The notation data denotes temporally sparse data, data indicates imputed data, and all other cases involve dense data. \(T_{s}\) indicates the sliding window, **x** indicates the input of our whole pipeline, and **y** indicates the input of denoising Transformer.

achieves superior MPJPE compared to AvatarJLM, though it showswylimitations in MPJVE due to the stochasticity of the diffusion model. In the experimental results presented in Table 2, our DSPoser model demonstrates superior performance on the Ego-Exo4D validation set. The model outperforms existing baselines, achieving a lower MPJPE of 16.84 cm, which represents an improvement over the next best model by 5.49 cm. Additionally, DSPoser achieves an MPJVE of 39.86 cm/s, improving upon the basline of naive extension of Bodiffusion by 7.64 cm/s.

It is evident that by incorporating temporally sparse hand pose data, our DSPoser framework significantly enhances pose estimation accuracy. For instance, on the AMASS dataset, MPJPE improved dramatically from 12.08 cm to 5.51 cm, while on the Ego-Exo4D dataset, it improves from 19.12 cm to 16.84 cm in \(T_{s}=20\) setup. This indicates that even sparse hand trajectory data, when effectively utilized, can provide crucial information for refining the accuracy of ego body pose estimation. Our method's ability to harness sparsely available data underscores its potential in applications where capturing dense sequence is challenging.

### Full Body Pose Estimation from Spatially Sparse data

To demonstrate the versatility of our framework, we conduct experiments on spatially sparse video data. In the temporally dense data setup, where there is no uncertainty regarding hand poses, the dense data directly works as a condition \(\) for spatial completion on the right side of Figure 2. Table 3 presents the results, demonstrating that DSPoser performs comparably to baseline models designed specifically for dense data setups on MPJPE and MPJRE metrics, underscoring the versatility of our dual approach in handling dense data scenarios. As discussed in Section 4.2, the higher MPJVE error results from the inherent stochasticity of the diffusion model.

  Methods & \(T_{s}\) & \(\) & Imputation & \(\) & MPJPE & MPJVE \\   VQ-VAE (Recons) & 20 & Full body & - & Full body & 6.77 & 33.29 \\  EgoEgo\({}^{*}\) & 20 & - & \(\) & 29.49 & 47.50 \\ Bodiffusion\({}^{*}\) & 20 & - & - & \(\) & 28.56 & 109.71 \\
**DSPoser (Ours)** & 20 & - & - & \(\) & \(19.12^{ 0.06}\) & \(48.54^{ 0.11}\) \\
**DSPoser (Ours)** & 20 & - & MAE & \(\) \& **18.46\({}^{ 0.06}\)** & **40.67\({}^{ 0.11}\)** \\  Bodiffusion\({}^{*}\) & 20 & \(\) \& \(\) & Interpolation & \(\) \& 59.81 & 120.12 \\ Bodiffusion\({}^{*}\) & 20 & \(\) \& \(\) & MAE & \(\) \& 52 & 22.12 & 53.30 \\
**DSPoser (Ours)** & 20 & \(\) \& \(\) & MAE & \(\) \& **16.84\({}^{ 0.04}\)** & **39.86\({}^{ 0.05}\)** \\  AvatarPoser\({}^{*}\) & 1 & \(\) \& \(\) & Interpolation & \(\) \& 47.28 & 89.34 \\ AvatarJLM\({}^{*}\) & 1 & \(\) \& \(\) & Interpolation & \(\) \& 43.01 & 61.98 \\ AvatarPoser\({}^{*}\) & 1 & \(\) \& \(\) & MAE & \(\) \& 24.54 & 62.34 \\ AvatarJLM\({}^{*}\) & 1 & \(\) \& \(\) & MAE & \(\) \& 21.08 & **45.77** \\
**DSPoser (Ours)** & 1 & \(\) \& \(\) & MAE & \(\) \& **19.09\({}^{ 0.21}\)** & \(55.82^{ 0.27}\) \\  

Table 2: Performance comparisons across baseline models for doubly sparse video data on the **Ego-Exo4D** validation set. We report MPJPE [cm] and MPJVE [cm/s], with the best results highlighted in **boldface**. Models trained by us are marked with \({}^{*}\). The notation Data denotes temporally sparse data, data indicates imputed data, and all other cases involve dense data.

  Methods & \(\) & MPJPE & MPJVE & MPJRE & Hand PE & Upper PE & Lower PE \\   FinalIK  & \(\) \& \(\) & 18.09 & 59.24 & 16.77 & - & - & - \\ LoBSTR  & \(\) \& \(\) & 9.02 & 44.97 & 10.69 & - & - & - \\ VAE-HMD  & \(\) \& \(\) & 6.83 & 37.99 & 4.11 & - & - & - \\ CollMoves  & \(\) \& \(\) & 5.55 & 65.28 & 4.58 & - & - & - \\ AvatarPoser  & \(\) \& \(\) & 4.20 & 28.23 & 3.08 & 2.34 & 1.88 & 8.06 \\ AvatarJLM  & \(\) \& \(\) & **3.35** & **20.79** & **2.90** & **1.24** & **1.72** & **6.20** \\
**DSPoser (Ours)\({}^{}\)** & \(\) \& \(\) & \(3.73^{ 0.08}\) & \(43.43^{ 0.14}\) & \(2.94^{ 0.09}\) & 3.26 & 1.92 & 6.53 \\  

Table 3: Performance comparisons across baseline models on the **AMASS** test set. We report MPJRE [\({}^{}\)], MPJPE [cm], and MPJVE [cm/s], with the best results highlighted in **boldface**. Note that \({}^{}\) is trained only with dense data without uncertainty.

### Ablation Studies

Based on the ablation study results shown in Tables 4 and 5, we can analyze the impact of different uncertainty guidance strategies and types of uncertainty on the performance of the model for body pose estimation. The ablation study is conducted with AMASS dataset with the sliding window \(T_{s}=20\) to better analyze the effect of the uncertainty guidance. Table 4 investigates the effects of various uncertainty guidance strategies, including no uncertainty guidance, sample, distribution embedding, and dropout. The results suggest that incorporating uncertainty guidance through these strategies can improve the model's performance across different metrics. The sampling strategy achieves the best performance, with the lowest MPJPE of 5.51, MPJVE of 24.19, and MPJRE of 4.09, indicating its effectiveness in capturing uncertainty and improving pose estimation accuracy.

Table 5 examines the contributions of different types of uncertainty, including epistemic uncertainty, aleatoric uncertainty, and total uncertainty. The results show that accounting for aleatoric uncertainty leads to the best overall performance. This suggests that considering data uncertainty can provide complementary information and improve the robustness of the pose estimation model. Overall, the ablation study highlights the importance of incorporating uncertainty guidance and considering different types of uncertainty in the model design for accurate and reliable body pose estimation.

In Table 6, we analyzed the effect of different \(\) values on the AMASS dataset during the uncertainty capturing process of the Masked Auto-Encoder (MAE). The results, shown in the table, indicate that \(=0.5\) provides the best temporal completion for head and hand 3D positions from the doubly sparse input. Therefore, we set \(\) to 0.5 for training the MAE.

### Hand Detection Accuracy and Hand Visibility Statistics

We investigate the error of the hand detector applied to the Ego-Exo4D dataset in terms of MPJPE, as shown in Table 7. The detection results indicate an average error of less than 10 cm. We also analyze the visibility statistics for the AMASS and Ego-Exo4D datasets in Table 8. In the AMASS dataset, at least one hand is visible in 18% of all frames with a 90\({}^{}\) field of view (FoV), whereas in the Ego-Exo4D dataset, at least one hand is visible in 27% of all frames.

### Qualitative Results

We visualized the aleatoric uncertainty in Figure 3, captured by a model trained using MAE on the AMASS dataset. In cases of partial visibility, as shown in Figure 3 (a-1) and (a-2), the uncertainty range is notably small. Conversely, in frames where the subject is completely obscured, the uncertainty range increases significantly. Even in fully invisible scenarios, the model captures a range of uncertainty, likely influenced by head movements. Most of the estimated frames fall within the \( 2\) range.

We also visualized the qualitative results on the Ego-Exo4D dataset and AMASS dataset in Figure 4. The qualitative results for AMASS show that our method improves the estimation results when sparse

  \(\) & MPJPE (cm) \\  
1.00 & 11.57 \\
0.50 & 10.85 \\
hand information is available, compared to the Head Only results. Additionally, in the Ego-Exo4D results, the hands are more aligned compared to the lower body when hands are available.

## 5 Related Works

Human Pose Estimation from Sparse InputA common capture setting in mixed reality involves using a head-mounted device and hand controllers. Estimating full-body motion from the sparse input of head and hand movements is challenging. Recently, several methods have been proposed to tackle this: AvatarPoser  is the first learning-based method to predict full-body poses in world coordinates using only head and hand motion inputs. It uses a Transformer encoder to extract deep features and decouples global motion from local joint orientations, refining arm positions with inverse kinematics for accurate full-body motion. BoDiffusion  employs a generative diffusion model for motion synthesis, addressing the under-constrained reconstruction problem. It uses a time and space conditioning scheme to leverage sparse tracking inputs, generating smooth and realistic full-body motion sequences. AvatarJLM  uses a two-stage framework where sparse signals are embedded into high-dimensional features and processed by an MLP to generate joint-level features. These features are then converted into tokens and fed into a transformer-based network to capture spatial and temporal dependencies, with an SMPL regressor transforming them into 3D full-body pose sequences. HMD-poser  combines a lightweight temporal-spatial feature learning network with regression layers and uses forward kinematics to achieve real-time human motion tracking. AGRoL  utilized conditional diffusion model to generate full body pose from sparse upper-body tracking signals. It is worth noting a concurrent work, EgoPoser , which also addresses ego body pose estimation

Figure 4: (a) Ego-Exo4D video frames, (b) the corresponding skeleton ground truth and our prediction results, and (c) qualitative results on AMASS data under different input conditions. green indicates the ground truth, blue indicates the predicted result, and red indicates the visible hands. Head only estimates body pose from head trajectories, whereas Ours estimates body pose from imputed hand and head trajectories.

Figure 3: Uncertainty visualization of the right hand pose captured by the MAE. Gray areas represent frames where the hand is invisible, and white areas denote visible frames. We depict aleatoric uncertainty within ranges of \( 1\) and \( 2\) from the estimated \(\).

from doubly sparse observations. Their focus lies in preparing training data through field-of-view (FoV) modeling rather than introducing new algorithms. Our work is orthogonal to theirs, providing algorithmic contributions through a multi-stage pipeline including an uncertainty-aware masked auto-encoder (MAE).

Human Body Pose Estimation from Egocentric VideosEstimating full 3D human body pose from egocentric videos is an ill-posed problem due to the partial visibility of wearer's body parts from the camera mounted on wearer's head. Recently, several approaches have been proposed to address this challenge. EgoEgo  integrates SLAM and a learned transformer to estimate head motion, then leverages estimated head pose to generate plausible full-body motions using diffusion models.  designs a kinematic policy to generate per-frame target motion from egocentric inputs, and leverages a pre-learned dynamics model to distill human dynamics information into the kinematic model. GIMO  integrates motion, 3D eye gaze, and 3D scene features to generate gaze informed long term intention-aware human motion prediction.  leverages external camera to generate pseudo labels to estimate full 3D body pose from single head mounted fish eye camera using weak supervision.  estimates geometry of surrounding objects and extracts 2D body pose features using EgoPW  to regress 3D body pose with a voxel-to-voxel network .

## 6 Conclusion

In this paper, we have addressed the problem of egocentric body pose estimation using temporally sparse observations from head-mounted displays (HMDs). By leveraging both temporal and spatial completion, our approach effectively utilizes intermittent hand pose detections from egocentric videos, alongside consistently available head pose data, to reconstruct full-body motions. Through comprehensive experiments on datasets such as AMASS and Ego-Exo4D, we have demonstrated the effectiveness of our framework. Our results indicate significant improvements over existing methods, particularly in scenarios where dense sensor data may not be available or practical. This advancement opens up new possibilities for beneficial augmented reality experiences in various applications, including sports training by providing feedback on body mechanics, and other scenarios where users need to move freely without additional sensors such as hand controllers. However, our method has not been explicitly tested for fairness across different demographic groups. Potential biases in the datasets used could result in uneven performance across various user populations. Careful curation of training datasets is necessary to prevent unfair failures for underrepresented groups.

## 7 Limitations

While our proposed method for estimating the body movements of a camera wearer from sparse tracking signals shows promising results, several limitations should be acknowledged. Firstly, our method has been tested with only one type of sparse body part tracking signal, specifically the hand. Incorporating the detection of other body parts, such as feet and elbows, may improve overall body pose estimation. Additionally, variations in lighting, occlusions, and the quality of the egocentric video can impact the accuracy of hand pose detection, subsequently affecting the overall body pose estimation.

The effectiveness of our method was validated using the AMASS and Ego-Exo4D datasets. Although these datasets are comprehensive, they may not encompass the full spectrum of possible real-world variations. Our study focused on pose estimation within a window size of less than a few seconds, following standard settings from the literature. It remains unclear how our method will perform with larger window sizes. Furthermore, the scalability of our method with larger datasets has not been thoroughly evaluated. The use of diffusion models for pose estimation may limit its utility for real-time applications due to their inference speed. Additionally, using multiple models to compute epistemic uncertainty can be computationally intensive.

AcknowledgementWe acknowledge Feddersen Chair Funds and the US National Science Foundation (FW-HTF 1839971, PFI-TT 2329804) for Professor Karthik Ramani. Any opinions, findings, and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the funding agency. We sincerely thank the reviewers for their constructive suggestions.