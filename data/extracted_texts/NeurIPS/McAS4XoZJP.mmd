# Alexa Arena: A User-Centric Interactive Platform for Embodied AI

Qiaozi Gao Govind Thattai Suhaila Shakiah Xiaofeng Gao Shreyas Pansare Vasu Sharma Gaurav S. Sukhatme Hangjie Shi Bofei Yang Desheng Zhang Lucy Hu Karthika Arumugam Shui Hu Matthew Wen Dinakar Guthy Cadence Chung Rohan Khanna Osman Ipek Leslie Ball Kate Bland Heather Rocker Michael Johnston Reza Ghanadan Dilek Hakkani-Tur Prem Natarajan Amazon Alexa AI

Equal contribution. Correspondence to arena-admins@amazon.comWork done while at Amazon Alexa AI

###### Abstract

We introduce Alexa Arena, a user-centric simulation platform to facilitate research in building assistive conversational embodied agents. Alexa Arena features multi-room layouts and an abundance of interactable objects. With user-friendly graphics and control mechanisms, the platform supports the development of gamified robotic tasks readily accessible to general human users, allowing high-efficiency data collection and EAI system evaluation. Along with the platform, we introduce a dialog-enabled task completion benchmark with online human evaluations.

## 1 Introduction

A longstanding goal of AI is to develop autonomous robotic agents that can assist humans in day-to-day activities. Experiments with robots are often conducted in controlled environments, limiting their variety and scale of operation. To mitigate this problem, several EAI simulation platforms have been proposed, which support virtual scenes that can be either manually designed, synthetically generated, or captured from real scenes. An embodied agent can freely navigate and interact with objects in these scenes to complete tasks. However, the current EAI platforms suffer from a set of limitations that curtail the ability to build generalizable assistive AI agents.

**Facilitating HRI data collection by gamification.** A persistent challenge in Human-Robot Interaction (HRI) is collecting human interaction data comprising of natural language instructions along with visual content and actions. Currently available EAI platforms, however, are not designed for humans to effectively interact with the agent, making the data collection process expensive and time consuming . Games have been historically utilized to encourage wide-spread user participation and engagement with services . To ease the data collection effort, we introduce gamification to EAI, which is achieved by presenting tasks or user interactions in the form of games,and by introducing scoring mechanisms, achievements and streaks to stimulate and engage users, promoting user participation.

**Reasoning based on visual observations.** Tasks in current EAI platforms have limited in-class variability: each task type requires the agent to make use of the same objects repeatedly. As a result, agents may resort to over-engineering (over-fitting to specific and simplistic tasks) and memorization to converge to trivial and non-generalizable solutions, or use predefined task decomposition mappings . One way to mitigate this is to design missions that can be completed in multiple ways by interacting with varied objects in the environment with compositional and causally interconnected state changes, forcing the agent to then use common-sense reasoning to understand its current state from visual observations, past actions and associated state changes.

**Reconciling offline and online execution scenarios.** Real-world operation of assistive agents is characterized by object interactions and collaborative engagement with other agents or humans in the form of visual cues or dialog. In the offline setting, the agent passively follows human instructions. Most currently proposed dialog guided interactive benchmarks and platforms operate in this fashion [11; 2; 26]. Online execution features an agent interacting actively with users that have control, interfacing through natural language in a continuously evolving environment. For example, if the agent veers off its path in the online phase, the human user can regain control of the situation and guide the robot to success, sometimes accomplishing some of the the tasks themselves, contributing a learning opportunity for the agent . Such a corrective mechanism that incorporates the elements of real-time user interaction can be implemented limitedly in an offline setting, to the extent of augmenting datasets with interactive elements like dialogs. With our platform, we aim to bridge the fundamental operational gap between the offline and online scenarios by demonstrating the use of offline interaction data to bootstrap an EAI agent, following which runtime data and interactions could be used to quantitatively and qualitatively improve user experience and task execution success rates, leading to a sucessful demonstration of a human-in-the loop EAI system that more closely resembles real-world deployment scenarios.

To this end, we propose Alexa Arena, a user-centric EAI platform. For better user experience, the platform design has commonalities with games, with features like task-guiding UI elements and engaging visual effects. The platform boasts many variations of objects and state transitions, on top of which a variety of missions are designed (Figure 1). We show an example use case of Arena on dialog-guided task completion, where the embodied agent can communicate with the user through natural language to finish an indoor mission. To assist in developing models for this task, we release a dataset in which each mission is annotated with an expert demonstration and corresponding human-annotated language instructions and dialogues. To evaluate the dialog-guided agent in real-time, we also release a web-based user interface (UI), where a user can communicate with the agent in text and receive visual observations of the agent from Arena. This paper makes the following contributions:

1. Alexa Arena as a new user-centric EAI platform which focuses on building assistive conversational agents that can assist humans through reasoning and procedural learning.
2. A dataset of over 9K dialog sessions and 46K human annotated instructions. Each data session simulates a human user communicating with an embodied agent to complete a household activity.
3. In addition to offline evaluation of the dialogue-guided embodied agents, we also provide an interactive evaluation protocol. Using the web-based interface, the user can provide instructions to the agent and observe the progress in real-time. We also present results from baseline models for both evaluations.

## 2 Related Work

**Embodied AI platforms.** In recent years, many platforms have been proposed for embodied agents to perform household activities in indoor virtual environments [28; 10; 47]. Most simulators are designed with performance and realism as the top priorities . Although some simulators have built-in infrastructure for human data collection [12; 46], the user interactions are generally not the focus of their designs. On the contrary, Alexa Arena is a user-centric embodied AI platform with features designed specifically to enhance user experience. There are also several EAI platforms that are inspired by video games [19; 4; 39; 7; 13; 48], but the agent observations are often simplified.

**Language-guided navigation and task completion.** Most existing work for learning language-guided embodied agents focuses on navigation tasks [3; 36; 24; 9; 31]. For increased task complexity,[23; 33] enable the agent to follow natural language instructions and complete household activities, where both navigation and object manipulation actions are required. Most recently, [26; 11] propose new datasets and benchmarks for training and evaluating task-oriented embodied agents that can engage in dialogue. However, both works use offline settings where the dialogues are pre-collected from humans or generated with templates. In comparison, the Arena platform enables embodied agents to communicate with human users in a real-time interactive fashion.

**Task planning with large language models.** Recently, there is a growing trend of using large language models (LLMs) for assisting robot task planning in learning novel activities or completing complex tasks [1; 44]. LLMs have been shown to be good at providing high-level semantic knowledge about the physical world and common human activities [16; 34]. When combined with the sensory perception of the embodied agent, such knowledge can often substantially improve the agent's capability of solving complex tasks in unseen scenarios [18; 17]. The Arena platform provides a good testbed for this line of work. With numerous object types, properties and state changes, along with various environmental causal events, Arena supports the creation of robot tasks that require adequate reasoning capability on common world knowledge.

## 3 The Arena Platform

In this section, we describe the Alexa Arena Platform. We briefly describe the attributes of the simulator and tools that we are releasing as a part of the platform.

### Objects Properties and States

There are 336 unique objects in Arena. Each object has a set of properties, called affordances, which specify if a certain type of robot-object interaction is possible. For example, the agent can toggle the _3-D printer_ since it has an object property _toggleable_. At the same time, the agent cannot pick it up since it does not have the _pickupable_ property. In total, there are 14 object properties, including _pickupable_, _openable_, _breakable_, _recpatacle_, _toggleable_, _powerable_, _dirtyable_, _heatable_, _eatable_, _chillable_, _fillable_, _cookable_, _decor_ and _infectable_. Each object property has a corresponding action and a subsequent object state upon taking that action. For example, _break_ is the corresponding action _for breakable_, and _broken_ is the corresponding state after the action has been performed. The states of objects will change as a consequence of the action, given that the pre-conditions are met. For example, if powered on (Fig. 1 (2)), the _3-D printer_ can be used to make toys (Fig. 1 (4)).

Figure 1: The Alexa Arena EAI platform has a variety of object categories, like a set of fantastical objects as engagement enhancers, such as the freeze ray (3), the time machine (6) and the color changer (7). The agent (1) can make use of different tools to change the state of the object. For example, the agent can use either the freeze ray (3) or the fridge (5) to cool an object. Arena also has more interaction actions and state changes compared to existing platforms. For instance, the agent can use the time machine (6) to repair broken objects, or use the color changer (7) to change the color of objects.

### Robot Action Space

In general, Arena supports two kinds of actions: 1) user interaction actions for communicating with the user, and 2) robot physical actions to interact with the simulation environment. There are two types of robot physical actions - navigation actions and object interaction actions. For better user experience, all the navigation and interaction actions are animated in a continuous fashion with associated appropriate environmental sounds also being played during the animation.

**User interaction.** To communicate with the user, the robot can initiate a dialog, the contents of which are displayed on the user interface (Figure 2). The robot can also _highlight_ objects for real-time visual feedback and use it for confirmation or user instruction disambiguation.

**Navigation.** The goal of Arena is to aid in compositional task learning and reasoning, instead of indoor navigation. To this end, we simplify the navigation in Arena by enabling the robot to directly navigate to a viewpoint in a room by specifying the viewpoint name or the room name, or to an object by specifying the object mask. Meanwhile, if preferred, the robot can also perform step-by-step navigation by a combination of parametrized local primitive actions like _MoveFoward_ and _Rotate_.

**Object interaction.** Arena supports 11 actions for object interaction, including _Examine, Pickup, Place, Open, Close, Break, Pour, Toggle, Fill, Scan_ and _Clean_. Each action is associated with a set of objects in a specific state in which the objects can afford that action to be performed upon them. E.g., _toggling_ can be performed on a _Time Machine_ when it is in a _closed_ state. Interaction actions are accompanied by a change in the associated object's state, which gets updated in the metadata.

### Enhanced User Experience

We incorporate into Arena elements that enhance application usability and improve engagement quality: 1) the platform includes engage-enhancers, such as fantastical objects and a scoring system for each mission; 2) for better visual effects, actions in Arena are animated in a continuous played-out fashion for both manipulation and navigation actions, makes the scenes more natural, user-friendly and engaging; 3) the platform features a unique user interface with several elements to provide users with better task guidance, including minimap and sub-task hints, all of which work in an integrated fashion to improve user-experience.

## 4 Dialog-guided Task Completion

We now present the Dialog-guided Task Completion benchmark. The benchmark is designed to evaluate dialog-guided agents for indoor object interaction tasks. To support model development, we release a hybrid dataset where ground-truth robot action trajectories are paired with human annotated language. We also set up two evaluation protocol: an offline evaluation on the validation split of the dataset and an online evaluation with human users.

Figure 2: The web-based user interface. The user can use the chat box to communicate with the agent in Arena for task completion. Meanwhile, video from Arena is streamed in real time to show the progress. The minimap is displayed on the top right corner of the game UI, showing the room layout and the robot location and orientation. The mission goal and subgoals are displayed to the users in text on the top left corner of the UI.

### Task settings

Each task in the benchmark is specified by the initial states and the goal states of the scene, and a sequence of human instructions. The agent is required to understand the language instructions and interact with the environment through a series of object manipulation and navigation actions to achieve the goal states.

### Arena Interactive Dialogue (AID) Dataset

To enable training and evaluation of the dialog-guided embodied agents, we collect the Arena Interactive Dialogue (AID) dataset via crowd-sourcing. The dataset contains expert action trajectories, human language instructions, and associated questions and answers for missions to simulate dialog. We split the dataset into training and validation folds. There are 2661 tasks in training and 383 in validation, in which each task is annotated by 3 annotators. Each human annotation is considered one session, giving total of 7983 training sessions and 1149 validation sessions. There are in total 12 unique tasks types in the dataset. Examples data points are shown in Figure 3.

**Expert demonstrations.** The game mission and expert demonstration generation follows a two-step process. In step one, game missions are programmatically generated via sampling from initial environment states and mission goals. The second step generates expert demonstrations using a planner. Along with the game definitions, we also generate PDDL (Planning Domain Definition Language) planning problem definitions for each game mission. A symbolic planner is used to solve the planning problem and the output action sequence is collected as the expert demonstration. One thing to note is that the planner has access to the game metadata, which is not available to the agents during inference time. For tasks that can be completed in different ways (e.g., by using different tools), we pick one unique way for each task to generate expert demonstration. For example, for freeze and deliver, in some missions the fridge is used to cool the object, while in other cases the freeze ray is used.

**Human Language Annotation.** To collect natural language dialogue for the missions, we design a two-stage data annotation process on Amazon Mechanical Turk (AMT). In both stages, the annotator watches a video containing an expert demonstration for the mission and provides the annotations in free-form text data or answers to multiple choice questions. In the first stage, annotators write instructions to tell a "smart robot" how to accomplish a task. During the process, an annotator first watches the video of the ground-truth robot actions, then writes instructions for each highlighted video segment. After all the instructions are collected, we start the second stage of annotation, where the annotators are asked to raise questions to better complete the task, as if they are controlling the robot to follow the instructions. They also need to subsequently answer their own raised questions. Similar to , the question choices in the second stage of annotation are generated using predefined templates. For more details on the human language annotation process, see Appendix.

Figure 3: Examples in the AID dataset. Each data session corresponds to one language annotation on a mission that needs to be completed in a specific scene. Each language annotation includes a sequence of instructions guiding the robot to complete the task, as well as questions and answers for clarification. Expert demonstrations are provided for each data session.

### Offline evaluation

We expect that a good embodied agent should be able to finish the missions efficiently. To achieve this goal, the agent should understand the human instructions and generate a corresponding sequence of actions. Thus, we evaluate the agent using the following metrics:

* **Mission success rate (MSR).** For each mission, there is a mission success variable \(m\) indicating whether the goal conditions have been met for the mission. \(m=1\) if all the goal conditions are met. Mission completion rate is calculated by averaging \(m\) across all the missions.
* **Average number of robot actions (NRA).** To measure the efficiency of task completion, we also record the number of actions taken by the agent to complete each mission. Average number of robot actions is calculated by averaging the number of actions across all the missions.

### Online Human evaluation

To demonstrate the potential of Alexa Arena for real-time user interactions, we design a human evaluation protocol for dialog-guided task completion. Different from the offline evaluation where the agent only passively follows the user instructions, in this online setting, the agent can also engage in multi-turn dialog with the user to communicate its internal states and clarify the mission goals.

**Missions.** On top of the 12 task types presented in the AID dataset, we design and curate 14 more complex binge-horizon missions, each requiring the agent to complete several sub-goals by following the user instructions, aiming to test compositional learning capabilities of the agents. The missions are designed to be gamified to engage the users and the environment also contains hints to guide the users to complete the tasks. The setting encourages dialog, where the human and the robot share the goal to complete the mission. An example of such a task is provided in the Appendix. To test the agent's capability to generalize across tasks, we keep 5 missions as the unseen test set.

**Metrics.** For online evaluation, we use the MSR as the task performance metric. As a subjective evaluation metric, we collect users' overall satisfaction toward the game on a 5-point likert scale after each interaction.

## 5 Experiments

### Offline evaluation

We use the AID dataset to train two baseline models. The inputs to the models are the agent egocentric view and natural language conversations. The outputs are a series of executable action sequences and associated interactable masks (where needed) to complete the task at hand. For both models, we experiment using only the human instructions, as well as the instructions appended with the questions and answers (QA) as input to simulate human-agent dialog during the mission in offline evaluation.

**Neural-Symbolic Approach.** Episodic Transformer (ET) is a neural-symbolic model for visual-language task completion . ET uses a multi-modal transformer to predict actions and object pairs based on the visual and language information in the entire episode, with a separate object detector to generate object masks.

Figure 4: Model architecture of the vision language model for end-to-end action prediction and visual grounding.

**End-to-end Vision Language Model.** We experiment with an end-to-end vision language (VL) model shown in Figure 4. A transformer model encodes the natural language instruction, and the egocentric view is encoded using a ResNet , both of which are initialized with pre-trained CLIP weights . The encodings are passed through a joint VL encoder to get a combined vision-language embedding. This is then channelled through two heads: 1) The task planning head is an LSTM module encoding the context of previous predicted outputs, followed by two linear classifiers to predict the _(action, object)_ pairs. 2) The mask prediction head uses the current first party view, the encoded language and the action-object prediction hidden state from the task planning head to predict a single mask referring to the interactable object using the referential image segmentation architecture described in . The above procedure is repeated for a natural language instruction and a series of first party views until an End-of-Sequence (EOS) flag is emitted by a separate binary classifier.

#### 5.1.1 Results and analysis

The baseline models are trained on the training split, and evaluated on the validation split of the AID dataset. The overall mission completion results are displayed in Table 1. Both models are evaluated with a cap of 50 maximum allowed steps and 10 failed steps per mission, beyond which if the mission goal is not completed, the agent stops execution leaving the goal incomplete. Below, we analyze the results for both models and provide insights on the model performances.

**Neural-Symbolic Model.** Overall, adding QA leads to a marginal improvement in the performance for the Neural-Symbolic model. The model performs well in MSR for missions with short horizons (e.g. toggleDevice). As an ablation study, we also evaluate the multi-modal transformer on the validation set by providing the ground-truth visual observations in the dataset at each time step. As a result, the model can correctly predict all the actions and objects for 71.3% of the missions without QA, and 76.9% with QA. Since it is common that there are multiple instances of the same class in a room, the model can choose an incorrect interactable mask even if the transformer model correctly predicts the object class and the vision model correctly predicts all the object masks of that class in the image. This can also explain why the improvement with QA is limited, since the additional information does not participate in the grounding once the action and object pairs are predicted.

**Vision Language Model.** The VL model shows a 6.88% absolute improvement in MSR when trained and evaluated with QA compared to training and evaluating without QA. The agent is also able to complete the missions more efficiently with QA, demonstrating that asking the right questions

 
**Mission** & **NS** & **NS** & **VL** & **VL** \\
**Type** & w/o QA & w/o QA & w/o QA & w/o QA \\   & 0.00\(\)0.00 & 0.00\(\)0.00 & 28.88\(\)3.62 & 37.41\(\)3.66 \\  & (10.93\(\)0.22) & (9.90\(\)0.05) & (8.90\(\)0.42) & (9.07\(\)0.56) \\   & 14.18\(\)1.43 & 13.79\(\)0.00 & 11.49\(\)2.48 & 16.85\(\)3.01 \\  & (21.66\(\)0.06) & (15.14\(\)0.09) & (16.12\(\)0.86) & (13.63\(\)0.93) \\   & 0.00\(\)0.00 & 0.00\(\)0.00 & 2.77\(\)3.92 \\  & (14.44\(\)1.43) & (13.22\(\)0.17) & (16.69\(\)0.45) & (18.05\(\)0.67) \\   & 14.58\(\)0.00 & 20.14\(\)0.98 & 11.80\(\)4.28 & 23.61\(\)5.47 \\  & (20.38\(\)0.27) & (17.12\(\)0.08) & (15.43\(\)1.27) & (13.79\(\)1.86) \\   & 31.94\(\)1.96 & 19.44\(\)1.96 & 13.88\(\)1.96 & 5.55\(\)5.19 \\  & (17.17\(\)0.07) & (17.42\(\)0.32) & (21.48\(\)1.65) & (19.71\(\)2.79) \\   & 5.13\(\)0.00 & 5.13\(\)0.00 & 23.07\(\)5.53 & 28.20\(\)2.09 \\  & (14.79\(\)0.36) & (18.22\(\)0.45) & (19.24\(\)1.97) & (16.91\(\)0.57 \\   & 14.69\(\)0.00 & 14.88\(\)0.53 & 15.63\(\)2.62 & 22.03\(\)1.22 \\  & (11.53\(\)0.15) & (11.45\(\)0.04) & (7.54\(\)0.46) & (1.22\(\)0.43) \\   & 9.47\(\)0.00 & 11.93\(\)0.29 & 21.28\(\)0.92 & 26.54\(\)1.67 \\  & (11.15\(\)0.10) & (12.66\(\)0.08) & (8.03\(\)0.36) & (7.89\(\)0.40) \\   & 15.10\(\)0.40 & 14.81\(\)1.07 & 25.92\(\)5.04 & 31.05\(\)1.06 \\  & (12.19\(\)0.04) & (12.71\(\)0.12) & (11.09\(\)0.90) & (10.07\(\)0.29) \\   & 11.11\(\)0.00 & 12.96\(\)0.00 & 11.72\(\)0.87 & 18.51\(\)3.02 \\  & (17.19\(\)0.28) & (15.56\(\)0.35) & (21.69\(\)1.14) & (18.39\(\)1.23) \\   & 41.44\(\)0.00 & 41.44\(\)0.00 & 41.44\(\)3.20 & 57.65\(\)4.59 \\  & (4.90\(\)0.00) & (4.84\(\)0.00) & (3.72\(\)0.19) & (3.96\(\)0.36) \\   & 57.14\(\)0.00 & 56.19\(\)0.00 & 84.12\(\)3.23 & 87.62\(\)4.85 \\  & (5.09\(\)0.00) & (5.08\(\)0.00) & (4.13\(\)0.22) & (4.49\(\)0.68) \\   & 18.42\(\)0.00 & 18.97\(\)0.00 & 27.00\(\)0.39 & 33.88\(\)0.71 \\  & (11.89\(\)0.01) & (11.62\(\)0.03) & (9.88\(\)0.29) & (9.20\(\)0.28) \\  

Table 1: Experimental results on the validation dataset for the Neural-Symbolic (NS) and Vision-Language (VL) models. For each task, we show mission success rate and average number of robot actions (in the parenthesis). For each metric, we report the mean and standard deviation from three runs.

improves task execution efficiency and accuracy. For tasks like _toggling_ which few objects can afford, the instructions tend to already be descriptive enough in natural language (_"Turn on the red computer", "Press the blue button"_), which explains why adding QA shows a mere 3% improvement in MSR. For all other task types which involve numerous objects, the QAs provide crucial additional information that is not naturally provided with the instruction in the first turn, thus leading to significant performance improvement both in MSR and NRA. From analysis, the _color & deliver_ and _freeze & deliver_ missions are prone to failure (as evidenced by low MSRs) because of relatively longer contexts than other missions and difficulty in disambiguating the receptacle of _delivery_.

### Online Human evaluation

Alexa Arena is used as the interactive EAI platform in the Alexa Prize Simbot Challenge, in which human users can perceive the environment through the agent's egocentric view, and communicate with the embodied agent via natural language commands. To understand how additional user interaction data can help model development, the challenge schedule follows a continual learning setting in a four-week duration: in the first three weeks, the teams can freely update their model using interaction data on 9 missions, and in the final week the models are tested on 5 unseen missions for generalizability.

#### 5.2.1 Modeling Methodologies

In total, 10 teams participated in the challenge and implemented their own models for Arena. The models output physical actions for the agent and engages in dialog with users to successfully complete missions. We briefly describe the design of some representative models. SlugJARVIS  use a hybrid approach for action prediction, with a rule-based parser to process most requests and an end-to-end Pythia  model to handle corner cases. They also design a progressive task experience retrieval module which helps make use of the existing successful user interaction data on seen tasks to enhance performance. ScottyBot  design a modularized system containing a neural NLU module for action prediction, named entity recognition, and action/object names standardization. The proposed design also incorporates semantic maps for efficient navigation and object localization. SEAGULL  use a rule-based system to detect user intent, and a PDDL planner to generate robot actions. They also design a hierarchical vision model to detect environment states, including fine-grained object categories, object states and objects spatial relations. EMMA  propose an encoder-decoder architecture to encode the multi-modal input and generate outputs as textual and visual tokens. The model is first pretrained on multiple vision-language tasks including masked language modeling and image captioning, before finetuning on embodied planning tasks. GauchoAI  also use a modular framework for their agent, with a hybrid rule-based and neural action prediction module, an agent state tracking module, a preprocessing module to reject non-plausible actions, and a MaskFormer  based visual grounding model. We refer readers to the technical reports for additional details 3.

#### 5.2.2 Results and analysis

In Table 2, we show human evaluation results for each team on a weekly-basis. For the first three weeks, we can see a clear upward trend for user satisfaction within each week across all the models. For the unseen missions, most teams are able to achieve similar performance as seen ones, showing the potential to generalize to novel tasks. Compared to the offline performance, we observe that the teams are able to achieve a much higher mission success rate with human evaluation despite the tasks being more complicated. This shows the effects of having interactive dialog during task execution, which helps the agent understand the task goal and the user to understand the model's limitations. This is validated qualitatively by playing with the virtually deployed bots, where we observe critical

 
**Team** &  &  &  &  &  \\  SlugJARVIS & 3.98\(\)0.18 (52.82) & 4.10\(\)0.19 (67.35) & 4.19\(\)0.18 (49.56) & 3.90\(\)0.20 (55.56) & (3.66) \\ ScottyBot & 3.48\(\)0.21 (46.46) & 3.67\(\)0.21 (40.40) & 3.97\(\)0.15 (49.04) & 3.63\(\)0.21 (57.95) & (3.36) \\ EMMA & 3.31\(\)0.20 (47.66) & 3.90\(\)0.18 (54.44) & 3.88\(\)0.18 (57.61) & 4.28\(\)0.14 (56.12) & (30.55) \\ SEAGULL & 3.57\(\)0.19 (33.78) & 3.77\(\)0.18 (50.00) & 4.31\(\)0.16 (51.72) & 3.63\(\)0.21 (37.86) & (30.98) \\ GauchoAI & 4.05\(\)0.17 (62.36) & 4.31\(\)0.16 (68.38) & 4.48\(\)0.15 (64.35) & 4.33\(\)0.17 (66.67) & (36.47) \\  

Table 2: For online evaluation results, we show the weekly user satisfaction on a 5-point scale and missions success rate in percentage (in the parenthesis). We report the mean and standard deviation for the user satisfaction. For each team, we also show the MSR achieved in the offline evaluation for comparison.

contributors to a higher user satisfaction and MSR. While there is a positive correlation between MSR and user ratings across different teams, users tend to give a higher rating to bots that are transparent and collaborative, able to give clear feedback, let their limitations be known, and are good at anticipating user actions in a balanced manner, as opposed to bots taking actions without appropriate feedback. For example, from Week 2 to Week 3 for certain models such as EMMA and GauchoAI, the user ratings increased while the overall MSR decreased. The increase in user satisfaction is attributed to a variety of factors, including that these models became more user-centric, were more interactive and feedback friendly to the users. The teams were able to update their models with real-time human feedback signals to align their bots with the users that interact with them.

## 6 Conclusion and future work

We introduce Alexa Arena, a user-centric EAI platform, featuring user-friendly graphics, animations and control mechanisms to facilitate user-centric EAI research. We provide baseline results for a dialogue-guided task completion use-case, and also release the AID dataset. While the Arena platform is designed to improve user-experience for EAI systems, the proposed online models cannot be evaluated in a straightforward fashion without a human-in-the-loop to supply interactive dialog and feedback. To overcome this limitation, we aim to incorporate a model-based user-simulator in future work, to which an oracle provides relevant answers during task execution. Another direction to explore is enabling the human and the robot to control different embodied agents and create more scenarios for HRI.

**Ethical Considerations.** Via introducing Arena as a public embodied AI platform built for human-robot communications, we mainly target English language and do not explicitly look at the issues innate to natural language understanding such as comprehending under-represented languages. The missions, tasks and game designs can be controlled by the platform users as needed, and is backed by a framework that has been validated quantitatively and qualitatively, to the best of our abilities, by carefully constructed and robust user studies as presented in the paper. Since Arena is to be open-sourced, it is possible for users to modify the platform for unintended use cases, which is a common risk for all open-sourced embodied AI platforms. But we believe the risk for Arena is low comparing to other platforms since Arena uses a gamified setting and the released recources are not directly applicable for real-world malicious applications. As for real world transferability of the trained agents, robust testing and continual safety monitoring mechanisms should be put in place to minimize risks of unpredictable robot behaviours in unexpected scenarios.