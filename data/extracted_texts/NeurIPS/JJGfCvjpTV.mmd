# Hamiltonian Score Matching and Generative Flows

Peter Holderrieth

MIT CSAIL

phold@mit.edu &Yilun Xu

NVIDIA

yilunx@nvidia.com &Tommi Jaakkola

MIT CSAIL

tommi@csail.mit.edu

###### Abstract

Classical Hamiltonian mechanics has been widely used in machine learning in the form of Hamiltonian Monte Carlo for applications with _predetermined_ force fields. In this work, we explore the potential of deliberately designing force fields for Hamiltonian ODEs, introducing Hamiltonian velocity predictors (HVPs) as a tool for score matching and generative models. We present two innovations constructed with HVPs: _Hamiltonian Score Matching (HSM)_, which estimates score functions by augmenting data via Hamiltonian trajectories, and _Hamiltonian Generative Flows (HGFs)_, a novel generative model that encompasses diffusion models and flow matching as HGFs with zero force fields. We showcase the extended design space of force fields by introducing Oscillation HGFs, a generative model inspired by harmonic oscillators. Our experiments validate our theoretical insights about HSM as a novel score matching metric and demonstrate that HGFs rival leading generative modeling techniques.

## 1 Introduction

Hamiltonian mechanics is a cornerstone of classical physics, providing a powerful framework for analyzing the dynamics of physical systems . The Hamiltonian formalism has been widely applied in machine learning and Bayesian statistics via Hamiltonian Monte Carlo (HMC) . In this setting, the goal is to sample from a probability distribution \(\) whose density \((x)\) is known up to a normalization factor. In HMC, one interprets \((x)\) as a force function and plugs it into a Hamiltonian ODE to construct a fast-mixing Markov chain exploring the data space quickly. This makes HMC one of the most powerful sampling algorithms to date .

In generative modeling, the density \((x)\) is unknown, only data samples \(x_{1},,x_{n}\) are given, and the goal is to learn to generate novel samples from \(\). Current state-of-the-art models are based on diffusion  and enjoy widespread success in image generation , molecular generation , and robotics . Diffusion models _learn_ the score function \(_{}(x)\) for a range of noise scales \(\) via denoising score matching (DSM) . This enables one to subsequently generate high-quality samples by following a stochastic differential equation .

In light of the success of HMC, it is natural to ask whether the Hamiltonian formalism can also improve generative models or provide novel insights into their construction. Previous works have exploited the connection between Hamiltonian physics and generative modeling for specific force fields . However, these works usually consider particular (fixed) force fields and stay within the diffusion framework.

More recently, flow-based generative models such as flow matching have enabled scalable training of continuous normalizing flows (CNFs) . These ODE-based models allow to craft first-order ODEs transforming arbitrary distributions from one to another. Inspired by these successes, we consider the Hamiltonian ODE as a Neural ODE . We show that we can marginalize out velocitydistributions and then follow backward ODEs that faithfully recover data distributions. Importantly, we provide an associated theorem that holds for any force field. With the growing success of generative models in physical sciences [11; 49; 1], it is striking that most approaches do not use existing known force fields - sometimes leading to physically implausible results . A framework that allows to reason natively about force fields in the context of generative models would be very promising for such applications [2; 12]. This work aims to build towards such a deeper integration.

We explore the intricate relationship between Hamiltonian dynamics, force fields, and generative models. Specifically, we make the following contributions:

1. **Hamiltonian velocity predictor:** We introduce the concept of a Hamiltonian Velocity Predictor (HVP) and show the utility of HVPs for score matching and generative modeling (Section 3).
2. **Hamiltonian score discrepancy (HSD):** We introduce and validate Hamiltonian score discrepancy (HSD), a novel score matching metric based on HVPs and a corresponding score matching method (Section 4).
3. **Hamiltonian generative flows (HGFs):** We show that the location marginal of a Hamiltonian ODE is generated via the Hamiltonian Velocity Predictor (Section 5). This leads to a novel generative model generalizing diffusion models and flow matching (Section 6).
4. **Oscillation HGFs:** As special HGFs, we study Oscillation HGFs, a simple generative model rivaling the performance of diffusion models due to in-built scale-invariance (Section 7).

## 2 Background

### Hamiltonian Dynamics

The inspiration of using Hamiltonian dynamics in machine learning comes from considering a data point \(x^{d}\) as coordinates of an object in \(^{d}\). Such an object also has a velocity \(v^{d}\). Let \(\) be a probability distribution with density function \(:^{d}_{ 0}\). Assuming unit mass, the energy of such an object is given by its Hamiltonian [33; 4], defined by:

\[H(x,v)=U(x)+\|v\|^{2},\] (1)

where \(U:^{d}\) is a potential function. The key idea behind using Hamiltonian dynamics in the context of probabilistic modeling and sampling is to set the potential function as negative log-likelihood, i.e., \(U(x)=-(x)\). One defines the _Boltzmann-Gibbs distribution_\(_{BG}\) then as:

\[_{BG}=(0,_{d}),_{BG}(x,v)=(-H(x, v))/Z=(x)(v;0,_{d}),\]

i.e. the product distribution of the data distribution and normal distribution. In particular, one can easily draw a sample \(z\) from \(z_{BG}\) by sampling \(x,v(0,_{d})\) and setting \(z=(x,v)\).

Hamiltonian dynamics describe how an object described by \(z=(x_{0},v_{0})\) evolves over time. It is defined by the ODE 

\[(x(t),v(t)) =(v(t),- U(x(t)))=(v(t),(x(t))),\] (2) \[(x(0),v(0)) =z\] (3)

i.e. the change of location is the velocity and the change of velocity is the force (here, equals acceleration as we assume unit mass). Let \(:^{2d}^{2d},(z,t) _{t}(z)\) be the corresponding flow, i.e. the function \(t_{t}(z)\) is a solution to the above ODE with starting point \(z\).

As one would expect from physics, Hamiltonian dynamics \(_{t}\) preserve the energy of a system, i.e. \(H(_{t}(x,v))=H(x,v)\) for all \(t,x,v\) (see proof in Appendix B.1). This physical intuition translates into the fact that Hamiltonian dynamics preserve the Boltzmann-Gibbs distribution, i.e.

\[Z_{BG}_{t}(Z)_{BG}t 0\] (4)

We include a derivation of this well-known statement in Appendix C.1 as it is so central to this work.

### Score Matching

The goal of score matching is to learn the _score function_\(\) from data samples \(x_{1},,x_{n}\). As the score function naturally appears in the Hamiltonian ODE (see Equation (2)), we interpret it as a force function and denote a parameterized score model by \(F_{}:^{d}^{d}\). A natural approach to fitting \(F_{}\) is to minimize the mean squared error between \(F_{}\) and the true score weighted by their likelihood under \(\). This leads to the _explicit score matching loss_ given by

\[L_{}(;)=_{x}[\| (x)-F_{}(x)\|^{2}].\] (5)

This loss cannot be minimized directly as one does not have access to \(\) and various score-matching methods differ in how they circumvent not having access to \(\) (see Appendix A for a detailed overview).

A different approach to score matching is to slightly modify the objective by adding Gaussian noise to the data distribution \(\) to get the noisy distribution \(_{}(x)=(x;x_{0};^{2}_{d})(x_{0})dx _{0}\). The objective can then be expressed as _denoising score matching_:

\[L_{}(;_{})=_{x, (0,_{d})}[\|F_{}(x+)+\|^{2}]=L_{}(;_{})+C_{}\] (6)

for a constant \(C_{}\). Noiced data distributions naturally appear in diffusion models, and the denoising score-matching objective, therefore, became the state-of-the-art method to train diffusion models. However, denoising score matching suffers from high variance leading to long training times as well as computationally expensive sampling . In addition, it would be an unreasonable choice for an application where it is important to learn the original data distribution.

## 3 Hamiltonian Velocity Predictors

The Hamiltonian ODE has been widely used in Bayesian statistics in the form Hamiltonian Monte Carlo . However, in such settings, it is assumed that one has access to the score \(\) (=force), and the goal is to sample from \(\) (by sampling from \(_{BG}\)). In machine learning tasks, the inverse is true. For such tasks, one has access to data samples \(x_{1},,x_{n}\) and the goal is to (1) learn \(\) (score matching) or (2) create new samples \(x\) (generative modeling) - or both.

Parameterized Hamiltonian ODEs (PH-ODEs).This inspires the definition of a _parameterized Hamiltonian ODE_ (PH-ODE). A PH-ODEs consists of two components:

1. **Initial distribution**\(\): The starting condition \(z=(x,v)\) is distributed according to a joint _location-velocity distribution_\(\) such that its marginal over \(x\) is \(\): \[(x,v)dv=(x)\] (7) In the default cause, \(\) equals the Boltzmann-Gibbs distribution \(_{BG}\), i.e. \(=(0,_{d})\).
2. **Force field:** The evolution is governed by a parameterized force field \(F_{}:^{d}^{d}\) via: \[(x(t),v(t)) =(v(t),F_{}(x(t),t))\] (8) \[(x(0),v(0)) =(x_{0},v_{0})=z\] (9) As we consider \(\) as part of the definition of a PH-ODE, we write with a slight abuse of notation \[_{t}^{}(z)=(x_{t}^{}(z),v_{t}^{}(z))=(x_{t}^{},v_{t}^{})\] (10) for the solution of the ODE assuming \(z=(x_{0},v_{0})\). We note that while \(F_{}\) might refer to a \(\)-parameterized neural network, we can also set it to a fixed known vector field. Both cases will be important.

Velocity prediction is all you need.The crucial idea of this work is that one can use PH-ODEs for both score matching and generative modeling by **predicting velocities**. For this, we use an auxiliary family of functions \(V_{}:^{d}^{d}\), here usually a neural network. Let us consider the following velocity prediction loss:

\[L_{}(|,t)= _{(x,v)}[\|V_{}(x_{t}^{},t)-v_{t}^{ }\|^{2}]\] (11)

By minimizing the above loss over \(\), we train \(V_{}\) to predict the velocity given the location after running the Hamiltonian ODE with starting conditions defined by \(\). For a sufficiently rich class of functions \(V_{}\), the minimizer \(V_{^{*}}\) of the above loss is the expected velocity conditioned on the location (see Appendix D):

\[V_{^{*}}(x,t)=[v_{t}^{}|x_{t}^{}=x]\] (12)

As we will see, this quantity is "all you need" for generative modeling and score matching. We call \(V_{^{*}}\) the **Hamiltonian velocity predictor (HVP)**. We note that many of flow-based works also implicitly learn to predict velocities , although they do not consider them as separate states.

## 4 Hamiltonian Score Matching

In this section, we provide a novel score-matching method using PH-ODEs and velocity predictors. We will first connect the score function to a preservation property of Hamiltonian systems (Section 4.1), and then introduce a new score-matching objective derived from this property (Sections 4.2 and Section 4.3).

### Characterizing Scores with Hamiltonian Dynamics

The conservation of the Boltzmann-Gibbs distribution \(_{BG}\) (see Equation (4)) is the crucial property that enables Hamiltonian Monte Carlo as it allows for proposals of distant states enabling fast mixing of a Markov chain. The inspiration of this work was to take the inverse perspective: rather than considering the preservation of \(_{BG}\) a _useful_ property of the Hamiltonian ODE, we ask whether it is the _defining_ property of the score function. In other words, **is any vector field that preserves the Boltzmann-Gibbs distribution under a PH-ODE automatically the score?** And if yes, **could we train for this property to learn a score?** As we will show, we can characterize the score with an even easier-to-train preservation property solely depending on the velocity predictor.

**Theorem 1**.: _Let \(T>0\) and \(F_{}(x)\) a force field. Let \(=_{BG}=(0,_{d})\). The following statements are equivalent:_

1. _Score vector field:_ _The force field_ \(F_{}\) _equals the score, i.e._ \(F_{}(x)=_{x}(x)\) _for_ \(\)_-almost every_ \(x^{d}\)_._
2. _Preservation of Boltzmann-Gibbs:_ _The PH-ODE with_ \(F_{}\) _preserves the Boltzmann-Gibbs distribution_ \(_{BG}\)_._
3. _Conditional velocity is zero:_ _The velocity given the location after running the PH-ODE with_ \(F_{}\) _is zero if starting conditions_ \(z=(x_{0},v_{0})\) _are sampled from_ \(_{BG}\)_:_ \[z_{BG}[v_{t}^{}(z)|x_{t}^{} (z)]=00 t<T\] (13)

Figure 1: Results of training HSM on a Gaussian mixture. The score vector field faithfully recovers gradients of the density. The optimal velocity predictor is zero everywhere.

A proof can be found in Appendix C and is based on the fact that test functions linear in \(v\) have vanishing expectation if Equation (13) holds. We note that the equivalence of conditions (1) and (2) is well-known in statistical physics.

### Hamiltonian Score Discrepancy

Condition (3) in Theorem 1 naturally motivates a new way of training \(F_{}\) to approximate a score. Specifically, our goal is **train the force field \(F_{}\) such that its optimal velocity predictor is zero.** By Theorem 1, it necessarily holds \(F_{}=\) in this case. Unfortunately, such a bilevel optimization is not tractable with stochastic gradient descent in general, as it contains two different objectives.

However, a simple trick allows us to convert the above into a single objective. For this, we define the _Hamiltonian Score Matching_ loss:

\[L_{}(|,t)= _{z_{B}G}[\|V_{}(x_{t}^{},t)\|^{2 }-2V_{}(x_{t}^{},t)^{T}v_{t}^{}]=L_{}(| ,t)-C(,t)\] (14)

where \(C(,t)=[\|v_{t}^{}\|^{2}]\). As the value of \(C(,t)\) is a constant in \(\), it holds that **the optimal velocity predictor is also the unique minimizer of the Hamiltonian Score Matching loss \(L_{}(|,t)\)**. However, while the argmin is the same, the actual obtained minimum value is drastically different as the next proposition shows.

**Proposition 1**.: _For a sufficiently rich class of functions \((V_{})_{ I}\), it holds that_

\[_{}(|t,):=-_{ I}L_{}(| ,t)=_{z_{B}}[\|[v_{t}^{}|x_{t}^{ }]\|^{2}]\] (15)

The proof relies on plugging the identity of \(V_{^{*}}\) (see Equation (12)) into Equation (14)) and can be found in Appendix E. By condition (3) in Theorem 1 (see Equation (13)), we want to minimize \(D_{}(|t,)\) in order to learn scores. For this, let's define a distribution \(\) with full support over \([0,T)\) for \(T_{>0}\{\}\)). With this, we define the **Hamiltonian score discrepancy (HSD)** as

\[_{}(|)=_{t}[_ {}(|t,)]\] (16)

Note that the discrepancy is defined for an arbitrary (regular) vector field \(F_{}\) - not restricted to scores of probability distributions. By Theorem 1, the discrepancy fulfills all properties that we would expect from a discrepancy to hold: \(D(|) 0\) for all \(\) and \(D(|)=0\) if and only if \(F_{}=\). We summarize the findings in the below theorem.

**Theorem 2**.: _Minimization of the Hamiltonian score discrepancy results in learning the score \(\):_

\[^{*}=*{arg\,min}_{}_{}(| ) s_{^{*}}=\] (17)

The full proof is stated in Appendix F. The Hamiltonian score discrepancy gives a natural measure of how far a vector field is from the desired score vector field. However, at first, it seems rather abstract. The following proposition shows that minimizing this measure has a very intuitive interpretation. In fact, it is closely connected to the explicit score matching loss \(L_{}\) (see Equation (5)).

**Proposition 2** (Taylor approximation of HSM loss).: _There exists an error term \((t)\) such that_

\[_{}(|t,)=2t^{2}L_{}(;)+ (t)\] (18)

_and \(_{t 0}}|(t)| 0\)._

A proof can be found in Appendix G. Intuitively, minimizing the Hamiltonian score discrepancy, therefore, consists of pushing the parabola in Equation (18) down onto the x-axis. The above theorem also indicates the optimal choice of \(T\): one should choose \(T\) high enough to have a loss value high enough to give signal but low enough to avoid errors due to numerical integration of the ODE.

### Hamiltonian Score Matching

Beyond its theoretical value, we can explicitly minimize the HSD, a method we coin _Hamiltonian Score Matching_ (HSM). To minimize the HSD, two networks \(V_{}\) and \(F_{}\) can jointly optimize Equation (14). There are two difficulties coming along with this: (1) One has to simulate trajectories. This can be done via Neural ODEs  with constant memory. (2) One has to run a min-max optimization. Here, a big toolbox developed for GANs for training stabilization can be used [35; 19]. On the other hand, we hypothesize that HSM has two advantages: (1) every trajectory of HSM gives several points of supervision effectively augmenting our data and (2) we can learn the original ("unnoised") data distribution \(\). However, please note that we do _not_ propose Hamiltonian Score Matching as a replacement for denoising score matching in diffusion models. Rather, it is a scalable alternative to score matching methods that learn the original ("unnoised") data distribution \(\).

Hamiltonian Generative Flows

Next, we show that training a general velocity predictor of a Hamiltonian ODE is useful even if \(F_{}\). This leads to a generative model that we coin **Hamiltonian Generative Flows (HGFs)**. As \(F_{}=F\) is fixed and not trained here, we write \((x_{t},v_{t})=(x_{t}^{},v_{t}^{})\) for the solution of the PH-ODE. Let us denote \((x,v,t)\) as the distribution of \((x_{t},v_{t})\) at time \(t\) and the **location marginal**

\[(x,v,t)dv=(x,t)\] (19)

The location marginal describes a probability flow starting from our data distribution \(=(,0)\). It turns out that the optimal velocity predictor is exactly the vector field that generates \((x,t)\).

**Proposition 3**.: _Let \(_{T}\) be the distribution such that \(x_{T}^{}_{T}\). Let \(V_{}^{*}\) be the Hamiltonian Velocity Predictor (see Equation (12)). Then by sampling \(x_{T}_{T}\) and running the **velocity predictor ODE**_

\[x(t)=V_{^{*}}(x,t) x(0)\] (20)

_backwards in time, we will have \(x(0)\), i.e. we can sample from the data distribution \(\). More specifically, the optimal velocity predictor \(V_{^{*}}\) generates the probability path \((,t)\)._

The proof uses the fact that the vector field \(G(x,v)=(v,F_{}(x,t))\) is divergence-free to show that \(V_{^{*}}\) fulfills the deterministic Fokker-Planck equation (see Appendix H). The above proposition allows us to build a generative model by training an HVP. We coin this model _Hamiltonian Generative Flows (HGFs)_. To make this framework tractable, we need two criteria to be fulfilled:

1. **(C1) Forward ODE efficiently computed:** For efficient training, we need to be able to compute \(x_{t}^{},v_{t}^{}\) efficiently - either via an analytical expression or ODE solvers.
2. **(C2) Initial distribution should be approximately known:** In order to be able to sample the initial point of the ODE faithfully, we need to (approximately) know \((x,T)\).

## 6 Diffusion Models and Flow Matching as HGFs with zero force field

### Diffusion Models as HGFs

We can recover diffusion models with a variance-preserving (VP-SDE) noising process  as a special case of HGFs. If we simply set \(=_{BG}\) and \(F_{}(x)=0\) - no force applied. In this case, we get that \(x_{t}=x+tv\) and \(v_{t}=v\) leading to the training objective:

\[_{x,v(0,_{d})}[\|V_{ }(x_{t},t)-v_{t}\|^{2}]= _{x,(0,_{d})}[ \|V_{}(x+t,t)-\|^{2}]\]

which equals denoising score matching (Equation (6)) with score model \(-tV_{}(x,t)=_{(t)}\) with \((t)=t\). In this case, Hamiltonian Generative Flows correspond to training a diffusion model

Figure 2: Evolution of various HGFs in joint coordinate-velocity space from \(t=0\) (blue) to \(t=T\) (red) with trajectories (black). Data distribution \((x)=0.4*(-2,1)+0.6*(2,1)\). Diffusion models and flow matching have zero force fields, i.e. the velocity does not change. Diffusion models do not converge in finite time (here, \(T=3\)). The coupled distribution in FM allow for a convergence for \(T=1\). Both distort the joint distribution. Oscillation HGFs only rotate the distribution.

and the velocity predictor corresponds to a _denoising network_ (often denoted as \(_{}\) in DDPMs ). It then holds \(x_{T}_{T}(0,^{2}(t)_{d})\) and the velocity predictor ODE then reduces to the well-known probability flow ODE formulation of diffusion models with noise schedule \((t)=t\):

\[x_{T}_{T}(0,^{2}(t)_{d}),x(t)=-(t)(t)_{x}_{(t)}(x)=V(x(t),t)\]

In fact, the above is a universal way of modeling diffusion models . In other words, **diffusion models are a special case of HGFs for the zero-force field**. In this perspective, different diffusion models correspond to different time rescaling and preconditioning of the network. The location marginals \((x,t)\) only fully converge to a Gaussian in the limit of \(t\) (see Figure 2).

### Flow Matching as HGFs

Flow matching with the CondOT probability path , a current state-of-the-art generative model, can be easily considered an HGF model. As in diffusion, let us consider the zero force field \(F_{}\) and let's consider a coupled initial distribution \(\):

\[x, v=-x,(0,_{d})\] (21)

Similarly, the velocity prediction loss corresponds to the OT-flow matching loss:

\[_{x(x),(0,_{d})}[\|V_{ }((1-t)x+t,t)-(-x)\|^{2}]\] (22)

and flow model corresponds to the velocity predictor ODE. Therefore, **diffusion models and OT-flow matching are both HGFs with the zero force field** - the difference lies in a coupled construction of the initial distribution (see Figure 2). The coupled construction allows OT-flow matching to have straighter probability paths, leading to improved generation quality for the same number of steps .

## 7 Oscillation HGFs

So far, we studied optimal velocity predictors \(V_{}\) for two extreme cases: either \(F_{}=0\) or \(F_{}=\). Finally, we want to investigate a different choice of \(F_{}\) to construct HGFs. Here, we study _Oscillation HGFs_ that correspond to a natural extension. In Appendix I, we give another example that we coin _Reflection HGFs_.

A simple design of a force field is to use the physical model of a harmonic oscillator, i.e. to set \(F_{}(x)=-^{2}x\) with \(=_{BG}\) and \(>0\). The flow of the ODE then becomes:

\[(x_{t},v_{t})=(( t)x+( t)v,- ( t)x+( t)v)\] (23)

I.e. condition (C1) is fulfilled as we can simply compute the ODE analytically. Setting \(T=/(2)\), it holds that \((x_{t},v_{t})=(v,- x)\). In particular, \(_{T}=(0,_{d}/^{2})\) - condition (C1) is easily fulfilled. Therefore, the above choice gives us a natural generative model based on harmonic oscillators that we coin _Oscillation HGFs_. To summarize, they have the following simple training objective:

\[_{x,v(0,_{d})}[\|V_{}((  t)x+v,t)-[-( t)x+(  t)v]\|^{2}]\] (24)

A natural choice for \(\) is to set \(=_{x}[\|x\|^{2}]}\). With this, the scale of the \(n\)-th derivative (including \(n=0\)) of the inputs and outputs in the training objective is constant in time (see Figure 2), i.e. for all \(t=0\):

\[_{x,v(0,_{d})}[\|}{d ^{n}t}x_{t}\|^{2}]=^{n-2}d,_{x,v (0,_{d})}[\|}{d^{n}t}v_{t}\|^{2}]= ^{n}d\] (25)

In the context of critically-damped Langevin diffusion , it was already observed that a constant scale in velocity space leads to improved training and better performance. Here, we extend this idea of a constant scale from the velocity to the \(n\)-th derivative.

## 8 Related Work

Assessing and training energy-based models.Stein's discrepancy  is a well-known measure to assess the quality of energy-based models based on Stein's identity . Based on this metric, developed a method that is similar to ours where a critic is optimized to assess the quality of an energy-based model via Stein's discrepancy and jointly trained with the energy model via min-max optimization.  introduced score matching as a method by showing that the explicit score matching loss (see Equation (5)) can be implicitly trained if one computes the trace of Hessian of the energy function - an expensive step. To expedite this,  introduced curvature propagation for an unbiased Hessian estimate, while  used Hutchinson's Trick to estimate the trace. In practice, both methods suffer from high variance due to their underlying Monte Carlo estimators.

Flow matching and Stochastic interpolants.As already seen for a special case in Section 6, HGFs are strongly connected to Flow Matching  and Stochastic Interpolants . They construct probability paths that fulfill the continuity equation by predicting derivatives of flows (i.e. velocities) in the same way how in this work, we predict velocities as marginals of an extended state space. The differences of these 3 works lie in the design perspective: HGFs consider 2nd-order ODEs in an extended state space \(^{d}^{d}\) with a simple _initial velocity distribution_ (here, \((0,_{d})\)), while FM considers 1st-order ODE paths in \(^{d}\) converging to a simple _final location distribution_. Flow matching conditions on final states (usually at \(t=1\)), while our framework conditions on the velocity of the current state (arbitrary \(t\)) and is centered around forces. This work arrives at the ideas of conditional velocity predictors via the search of properties that are conserved under Hamiltonian dynamics (see Theorem 1). We elucidate the mathematical connection in more detail in Appendix J.

Generative models and Hamiltonian physics.V-Flows also consider augmenting the state space with velocities deriving an ELBO objective for CNFs .  extend diffusion models to joint state-velocity samples that converge to a joint normal distribution. One difference is that we only need to run the backward equation in state space \(^{d}\) as opposed to extended state-velocity space \(^{d}^{d}\). Though rather unmotivated, Oscillation HGFs could, in principle, also be derived as an EDM model with preconditioning . Finally, several works have, like this, explored generative models based on specific physical processes, e.g. Poisson flow generative models . A few works also combined Hamiltonian physics with deep learning. For example,  use conservation of energy as an implicit bias to learn networks for physical data. Conversely, deep learning was also used to accelerate HMC sampling , e.g. by training custom MCMC kernels  or correct for complex geometries via flows . Very recently, score matching approaches were also designed to leverage existing force fields as part of a diffusion model that samples from an energy landscape .

Acceleration Generative Model (AGM).The AGM model  also uses constructions in phase space (joint position and velocity space) and 2nd order ODEs. While AGM focuses on learning the force field, our approach primarily focuses on learning the optimal velocity predictor. While we also consider optimizing the force field by minimizing the norm of the optimal velocity predictor, this happens in the "outer loop" of the maximization - the inner loop optimizes the optimal velocity predictor. Further, ATM focuses on bridging two desired distributions by posing a stochastic bridge problem in phase space. We do not consider the problem of bridging distributions. In contrast, our framework centers around energy preservation and divergence from that preservation (for optimal velocity predictors that are not zero). Specifically, we establish a connection to Hamiltonian physics and a property of the preservation of energy. This allows us to introduce a further bi-level optimization and the possibility of joint training for score matching.

## 9 Experiments

### Hamiltonian Score Discrepancy

As we introduced Hamiltonian score discrepancy as a novel score-matching metric, we first empirically investigate our theoretical insights on Gaussian mixtures (see Figure 1). As one can see in Figure 2(a), the Hamiltonian score discrepancy is highly correlated with the explicit score matching loss. Further, we can validate empirically that the Taylor approximation derived in Proposition 2 is pretty accurate for large \(t\) (see Appendix I). Overall, these results indicate that the Hamiltonian score discrepancy is a natural metric to assess score approximations.

Further, we investigate whether explicitly minimizing the Hamiltonian score discrepancy leads to accurate score approximations. We jointly train velocity predictors and score networks as described in Section 4. As one can see visually in Figure 1, this approach can faithfully learn score vector fields. In addition, we investigate the signal-to-noise ratio for gradient estimation. As shown in Figure 2(c), the gradient estimates of HSM have significantly lower variance compared to denoisingscore matching at lower noise levels \(\). The reason for that is that we allow for supervision across a full trajectory at locations for the same data points - effectively acting as data augmentation. Of course, this comes at the expense of simulating the Hamiltonian trajectories for \(~{}5\) steps.

### HGF experiments - Image Generation

In the form of diffusion models and flow matching, HGFs have already been extensively optimized and achieved state-of-the-art results. Instead, we investigate whether also other non-zero force fields, specifically Oscillation HGFs, can lead to generative models of high quality.

Specifically, we train a Oscillation HGF on CIFAR-10 unconditional and conditional. As two central benchmarks, we use the original SDE formulation of diffusion models  as well as the EDM framework , a highly-tuned optimization of diffusion models. Our hypothesis is that Oscillation HGFs should work well out-of-the-box, as the scale of their inputs and outputs stay around constant (_c.f._ Eq. 25). Therefore, we remove any preconditioning optimized for diffusion models (scaling of inputs and outputs and skip connections)  and train on the default DDPM architecture  (see details in Appendix L). Our results are encouraging: without hyperparameter tuning, Oscillations HGFs can sample high-quality images and surpass most previous methods (see Table 1) measured by Frechet Inception Distance (FID) . While they still lack behind the EDM model, this difference might well be explained by the fact that architectures and hyperparameters have been optimized for diffusion models over several works that are hard to replicate.

  METHOD & FID \(\) & NFE \(\) \\  _CIFAR-10 (unconditional)-32x32_ & & \\  StyleGAN2:ADA  & \(2.92\) & \(1\) \\ DDPM  & \(3.17\) & \(1000\) \\ LSGM  & \(2.10\) & \(147\) \\ PFGM  & \(2.48\) & \(104\) \\ VE-SDE  & \(3.77\) & \(35\) \\ VD-SDE  & \(3.01\) & \(35\) \\ EDM  & \(1.98\) & \(35\) \\ FM-OT (BNS)  & \(2.73\) & \(8\) \\ Oscillation HGF (ours) & \(2.12\) & \(35\) \\ _CIFAR-10 (class conditional)-32x32_ & & \\  VE-SDE  & \(3.11\) & \(35\) \\ VP-SDE  & \(2.48\) & \(35\) \\ EDM  & \(1.79\) & \(35\) \\ Oscillation HGF (ours) & \(1.97\) & \(35\) \\ _FFHQ (unconditional)-64x64_ & & \\  VE-SDE  & \(25.95\) & \(79\) \\ VP-SDE  & \(3.39\) & \(79\) \\ EDM  & \(2.39\) & \(79\) \\ Oscillation HGF (ours) & \(2.86\) & \(79\) \\  

Table 1: Sample quality (FID) and number of function evaluation (NFE).

Figure 4: Image generation examples based on Oscillation HGFs for FFHQ.

Figure 3: Empirical investigation of Hamiltonian score discrepancy (HSD). (a) The Taylor approximation is a good approximation. (b) Hamiltonian score discrepancy is strongly correlated with explicit score matching loss. (c) Signal-to-noise ratio is significantly better for HSM vs DSM for low \(\).

To investigate whether similar results can be achieved similar performance at higher resolutions, we perform another benchmark on the FFHQ dataset at 64x64 resolution. Here, our results are similar: Oscillation HGFs improve upon original diffusion models with a small performance margin to the EDM model. They can generate high-quality faces that appear realistic (see Figure 4).

## 10 Conclusion

Our work systematically elucidates the synergy between Hamiltonian dynamics, force fields, and generative models - extending and giving a new perspective on many known generative models. We believe that this opens up new avenues for applications of machine learning in physical sciences and dynamical systems. However, several limitations remain. Minimizing the Hamiltonian Score Discrepancy (HSD) via a default min-max algorithm is scalable but requires adversarial optimization. Future work can focus on adapting the HSD framework, e.g. to develop _denoising_ Hamiltonian score matching that could allow for guaranteed convergence. For HGFs, the extended design space presented has only been explored for data without known force fields (image generation, here). Future work can focus on specific applications that require domain-specific force fields, e.g. for molecular data. Further adaptions might be required in such settings as such data often lie on manifolds. A further challenge is that HGFs not necessarily converge to a known distribution for more complex force fields. Therefore, we anticipate that future work will focus on adapting HGFs and related models to this challenge to design domain-specific models.