# AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis

Swapnil Bhosale

University of Surrey, UK

Haosen Yang

University of Surrey, UK

Diptesh Kanojia

University of Surrey, UK

Jiankang Deng

Imperial College London, UK

Xiatian Zhu

University of Surrey, UK

###### Abstract

Novel view acoustic synthesis (NVAS) aims to render binaural audio at any target viewpoint, given a mono audio emitted by a sound source at a 3D scene. Existing methods have proposed NeRF-based implicit models to exploit visual cues as a condition for synthesizing binaural audio. However, in addition to low efficiency originating from heavy NeRF rendering, these methods all have a limited ability of characterizing the entire scene environment such as room geometry, material properties, and the spatial relation between the listener and sound source. To address these issues, we propose a novel _Audio-Visual Gaussian Splatting_ (AV-GS) model. To obtain an implicit material-aware and geometry-aware condition for audio synthesis, we learn an explicit point-based scene representation with an audio-guidance parameter on locally initialized Gaussian points, taking into account the space relation from the listener and sound source. To make the visual scene model audio adaptive, we propose a point densification and pruning strategy to optimally distribute the Gaussian points, with the per-point contribution in sound propagation (e.g., more points needed for texture-less wall surfaces as they affect sound path diversion). Extensive experiments validate the superiority of our AV-GS over existing alternatives on the real-world RWAS and simulation-based SoundSpaces datasets. Project page: https://surrey-uplab.github.io/research/avgs/

## 1 Introduction

Novel view synthesis  allows to generate images for any target viewpoints, which has been extensively studied and advanced. For real-world applications in augmented reality (AR) and virtual reality (VR), solely visual rendering of 3D scenes without spatial audio (i.e., deaf) fails to fully immerse users in the virtual environment. This thus inspires a recent surge of investigating novel view acoustic synthesis (NVAS) . By synthesizing binaural audio (two channels corresponding to the left and right ear) taking into account the factors like directionality, distance and relative elevation, this can create a spatial audio experience akin to real-life perception . However, realistic binaural audio synthesis is challenging, since the wavelengths of sound waves are much longer, necessitating the modeling of wave diffraction and scattering. To illustrate, while blocking the sun with your thumb is easy, blocking thunder sounds with your thumb is difficult because sound waves wrap around obstacles. A sound wave propagating through a 3D space undergoes various sophisticated acoustic phenomena, like direct sound, early reflections, and late reverberations.

Aiming to render binaural audio from the mono audio for target poses, Neural Acoustic Field (NAF)  learns room acoustics in a synthetic environment with a 2D grid of implicit representations as a condition. However, deriving a 2D representation grid for real-view scenes is extremely challenging in the presence of unconstrained objects, materials, and occlusion in 3D scenes. Alternatively, AV-NeRFTo overcome these limitations, in this work, we present a novel _Audio-Visual Gaussian Splatting_ (AV-GS) model, characterized by efficiently learning an explicit, holistic 3D scene condition with rich material and geometry information, as well as more comprehensive contextual knowledge beyond the listener's field of view for enhanced synthesis conditioning. Due to the intrinsic discrepancy between vision and audio as discussed earlier, extending existing 3D Gaussian splitting-based (3DGS)  from visual scenes to 3D audio (_i.e._, spatial audio) is non-trivial. Optimizing towards scene visual reconstruction, points tend to over-populate along object edges and under-populate in texture-less regions such as walls, doors etc. But, such point distribution is rhetoric when learning sound propagation since, major changes in sound paths (absorption and diversion) primarily happen around those texture-less regions. The key challenge lies in jointly modeling 3D geometry and implicit material characteristics of the visual scene objects to instigate direction and distance awareness for realistic binaural audios. To that end, we decouple the physical geometric prior from the 3DGS representation to learn an acoustic field network by introducing audio-guidance parameters. These audio-guidance parameters combined with their relative distance and direction from the listener and sound source, are projected on-the-fly to derive holistic scene-aware and implicit material-aware conditions for synthesizing binaural audios (see Figure 1). We hypothesize that converging AV-GS on binaural audio reconstruction loss, requires location and density adjustment of the Gaussian points relative to the "audio-guidance" provided by the individual point. Towards this direction, we design a Gaussian point densification and pruning strategy based on the individual per-point contribution in providing this "guidance" for sound propagation, ultimately improving the overall binaural audio synthesis.

To summarize, we make the following _contributions_: (1) The first novel view acoustic synthesis work with conditions on holistic scene geometry and implicit material information; (2) A novel AV-GS model that learns a holistic geometry-aware material-aware scene representation in a tailored 3DGS pipeline; (3) Extensive evaluations validating the advantages of our method over prior art alternatives on both synthetic and real-world datasets.

## 2 Related Work

**Audio binauralization** converts monaural (single-channel) audio signals into binaural (two-channel) audio signals. It can enhance immersion by simulating sound directionality, distance, and spatial cues. Generating realistic binaural audio from mono audio is challenging [31; 30], prompting the exploration of various conditioning techniques in the literature.

_Geometry and material conditioning_ Luo et al.  introduced Neural Acoustic Field (NAF) for room acoustics modeling using implicit neural representations of geometric features, capturing spatial information of speakers and receivers. Su et al.  also learned implicit neural representations for audio, focusing on interactive acoustic radiance transfer, relying on scene geometry as input. Anton and Dinesh  proposed a material-aware binaural sound propagation model, incorporating

Figure 1: Sound propagation point patterns between a listener (blue sphere) and emitter (yellow sphere) captured by our AV-GS. Notice the points outside the propagation path (points behind the speaker, points behind rigid walls). Please note we slice the scene into half along the y-axis (omitting the points from the ceiling) in order to facilitate better visibility.

material and topology data, necessitating an acoustic material database. In contrast, our approach learns explicit representation from 3D scenes and disentangles learning of physical and (implicit) material properties, eliminating reliance on scene geometry and material characteristics as inputs.

_Visual cue conditioning_ Recent works leverage visual cues to generate spatial audio from mono audio, capturing complementary scene characteristics [9; 38; 27; 41; 18; 43; 19; 28]. Li et al.  proposed a multi-task approach optimizing binaural audio generation and flipped audio classification, sharing visual cues from corresponding video frames. Chen et al.  synthesized sound by analyzing input audio-visual cues, incorporating active speaker features, target pose, and encoded visual features in the acoustic synthesis pipeline. Alternatively, Liang et al.  encoded the listener's position and orientation, conditioned by the listener's view. It additionally provides local visual depth information dependent on the listener's position. However, focusing solely on the listener's view overlooks the broader 3D scene geometry's contribution, which is crucial for sound propagation. We thus propose to learn a holistic 3D scene representation, enhancing binauralization guidance with additional audio parameters.

**3D Scene Representation Learning** Point-based rendering techniques, initiated by , utilize point-based representation where each point affects a single pixel. Zwicker et al.  advanced this with ellipsoid-based rendering (splatting), allowing mutual overlap to fill image holes. At the absence of given geometry, Mildenhall et al.  explored neural implicit representation, NeRF, predicting view-dependent radiance via implicit density fields. NeRF requires combining colors of densely sampled points along the camera rays for high-quality rendering. 3D Gaussian splatting (3D-GS) , a novel-view synthesis method, employs explicit point-based representation, contrasting with NeRF's volumetric rendering. Since its real-time high-quality rendering capabilities that 3DGS has been applied to various domains, including simultaneous localization [13; 24], content generation , and 4D dynamic scenes [16; 39; 42], among others.

However, no works take the 3DGS advantages to improve the NVAS task. Hence, in this work, we for the first time exploit the 3D-GS for NVAS, to the best of our knowledge, including capturing the scene geometry and material-related information for audio signal processing purposes. Further, we adapt the point management mechanism to account for the characteristics of sound propagation. In a nutshell, our approach aims to improve binaural audio reconstruction loss in AV-GS by adjusting Gaussian point location and density based on their contribution to sound propagation guidance.

## 3 Method

**Problem definition** Deployed at a location \(X_{S}\) in a 3D scene, a sound source \(S\) emits a mono audio \(a_{mono}\). A listener entity \(L\) moves around in this 3D scene, capturing multiple observations using a camera mounted with a binaural microphone. Each observation \(O_{p}=(V_{C},V_{A})\), constitutes a pair of a camera view \(V_{C}\) and an auditory perspective \(V_{A}\), w.r.t a specific listener pose \(p=(X_{L},d)\), where \(X_{L}\) is the 3D position of the listener and \(d\) is the viewing direction corresponding to listener's head. A camera view \(V_{C}\) defines that at the pose \(p\), the listener would observe a RGB image, \(I\) (as their view). Similarly, an auditory perspective \(V_{A}\) defines that at the pose \(p\), the listener hears a binaural audio, \(a_{bi}=(a_{l},a_{r})\) where \(l\) and \(r\) represent the left and right ear respectively. Given \(N\) observations from the listener, \(O=\{O_{1},O_{2},...,O_{N}\}\), the task is to predict the binaural audio \(a_{bi}{}^{*}\) for a novel observation \(O_{p}\)- having an arbitrary unseen pose \(p^{*}\).

### Audio-Visual Gaussian Splatting (AV-GS)

Our model is comprised of a 3D Gaussian Splatting model \(G\), an acoustic field network \(\), and an audio binocularizer \(\). In order to model sound propagation in space and time, having a holistic scene prior as contextual guidance is essential. Crucially, this guidance must incorporate both geometric and material-related characteristics. For facilitating the learning of visual and auditory modalities with inherently distinct characteristics, we decouple the physical geometry and the acoustic field by introducing in-between an _acoustic field network_\(\) that learns geometry-aware and implicit material-aware scene contexts.

In the following, we first, provide a brief regarding learning the scene geometry. Later, we explain how to construct an audio-focused representation \(G_{a}\). We further describe the process of decoding the parameters of \(G_{a}\) on the fly using view-dependent information, followed by point densification and pruning tailored for sound propagation.

**3D Gaussian Splitting** (3D-GS)  is a state-of-the-art novel-view synthesis method that learns an explicit point-based representation of the 3D scene. It is utilized here to capture the scene geometry. Specifically, each point in the explicit representation \(G\) is represented as a 3D Gaussian ellipsoid to which physical attributes like position \(\), quaternion \(\), scale \(\), opacity \(\) and Spherical Harmonic coefficients (\(\)) representing view-dependent color are attached. For an arbitrary camera view \({V_{C}}^{*}\) with a pose \(p^{*}\), we project/splat 3D Gaussians onto the 2D image plane to obtain the listener's view as an RGB image \(I^{*}\). The projection of 3D Gaussian ellipsoids can be formulated as:

\[^{}=JW W^{T}J^{T}\] (1)

where \(^{}\) and \(=RSS^{T}R^{T}\) are the covariance matrices for 3D Gaussian ellipsoids and projected Gaussian ellipsoids on 2D image from a viewpoint with viewing transformation matrix \(W\). \(J\) is the Jacobian matrix for the projective transformation. Please refer to  for more details on splatting.

#### 3.1.1 Acoustic Field Network

As a part of the decoupling approach, we further introduce an audio-focused point-based representation \(G_{a}\). Concretely, every point in \(G_{a}\) is parameterized using the location \(X\), alongside a learnable audio-guidance parameter \(\) to encapsulate implicit material-specific characteristics of the scene. \(X\) is initialized using the location of Gaussian points in \(G\), in order to impose the scene geometry knowledge. To initialize \(\) we concatenate the view-dependent color priors from \(G\), particularly spherical harmonics \(\) and quaternion feature \(\). An intuition is to choose the parameters that provide information regarding the color and density characteristics, which are often correlated with material properties  (see ablation on the choice of different parameters from \(G\) for forming \(\) in Section 4.4).

In order to drive the mono to binaural audio transformation, a pose-specific holistic scene context has to be derived from \(G_{a}\). To achieve this, we derive a position-guidance feature \(\) w.r.t both, the listener \(L\) and the sound source \(S\). Position-guidance \(\) concatenated with the audio-guidance \(\) is used as an input to the acoustic field network \(\) to obtain a joint context for each point w.r.t to the listener and the sound source separately.

\[=_{S}_{L}\] (2)

\[_{i}=(,_{i});_{i}=}{\|X-X_{i}\|_{2}},i\{S,L\}\] (3)

Figure 2: Overview of our proposed AV-GS. Our model is comprised of a 3D Gaussian Splatting model \(G\), an acoustic field network \(\) and an audio binauralizer \(\). We first train \(G\) to capture the scene geometry information. Next, we construct an audio-focused point representation \(G_{a}\), with the location \(X\) and audio-guidance parameter \(\) initialized by the pre-trained \(G\). Then the acoustic field network \(\) is used to process the \(\) parameters for all the Gaussian points in the vicinity of the listener and the sound source (in the 3D space). The output from \(\) is finally used to condition the audio binauralizer \(\), which transforms the mono audio to binaural audio w.r.t the listener and sound source location.

Concatenating (represented using \(\)) the listener-context and the speaker-context yields the overall pose-specific holistic scene context. We obtain the condition for binauralization by averaging the context across all points in \(G_{a}\), post dropping points outside the vicinity of the listener and sound source. The vicinity for the listener and sound source is defined by the nearest \(k_{L}\) and \(k_{S}\) points based on the Euclidean distance between \(X\) and \(X_{L}\) and that between \(X\) and \(X_{S}\), respectively. We provide an ablation for selecting the size of vicinity points in Section 4.4.

#### 3.1.2 Audio Binauralizer

An audio binauralization module \(\) transforms the mono audio \(a_{mono}\) emergent from the sound source \(S\) into binaural audio \(a_{bi}=\{a_{l},a_{r}\}\), representing the left and right audio channels. The position and orientation of the listener (relative to the sound source) along with the learned holistic scene context \(\) is used to condition this transformation.

\[a_{bi}=(a_{mono}|,X_{L})\] (4)

where \(X_{L}\) denotes the listener's position and orientation. We adopt a similar architecture for the binauralization as , wherein \(X_{L}\) and \(\) are fused to generate acoustics masks \(m_{m}\) and \(m_{d}\) representing the mixture and difference of the sound fields respectively. Specifically, an MLP combines \(X_{L}\) and \(G_{a}\) with a frequency query \(f[0,F]\) to produce a feature vector which is further projected using a linear projection to obtain a mixture mask \(m_{m}\) for frequency \(f\). This feature vector is concatenated with the transformed direction \(\) and passed to a second MLP to generate a difference mask \(m_{d}\). All frequencies within \([0,F]\) are queried to generate complete masks for both the mixture and difference. The magnitude spectrogram \(s_{mono}\), computed using short-time Fourier transform (STFT) over \(a_{mono}\), is multiplied with \(m_{m}\) and \(m_{d}\) to obtain the mixture magnitude \(s_{m}\) and the difference magnitude \(s_{d}\), respectively. Finally, an inverse STFT is operated on \(s_{m}\) and \(s_{d}\) to obtain the binaural audios for the left and right channels, \(a_{l}\) and \(a_{r}\). The architecture of the binauralizer is discussed in our appendix A.2.

### Model training

Due to our decoupling design, we adopt a dual-stage optimization, starting with an initial warm-up stage that learns the physical properties of the Gaussian points using gradients obtained while reconstructing camera views (RGB images). The objective of this initial stage is to infer an explicit point-based representation \(G\) to capture the scene geometry. Optimization of \(G\) is adapted from the original 3D-GS  using the loss function,

\[_{G}=(1-)_{1}+_{SSIM}\] (5)

In the second stage, we initialize the audio-guidance parameters of \(G_{a}\) using physical parameters of the warmed-up 3D Gaussians. Subsequently, we learn the implicit material properties for the Gaussian points using gradients obtained in the binauralization task for every training auditory perspective. Optimization of \(G_{a}\) is guided by a combination of the binaural audio reconstruction loss, \(_{m}\) and a volume regularization loss, \(_{v}\).

\[_{G_{a}}=(1-_{a})_{m}+_{a}_{v}\] (6)

\[_{m}=_{2}(s_{m})+_{2}(s_{l})+_{2} (s_{r})\] (7)

where, \(_{m}\) is the summation of the \(_{2}\) loss calculated individually between the predicted magnitudes for \(s_{m},s_{l},s_{r}\) (i.e., mixture, left and right audio channel, respectively) and their respective ground-truth magnitudes. \(_{v}=_{i=1}^{N_{a}}{{{prod(_{i})}}}\), where \({{{prod(.)}}}\) is the product of the values of the audio-guidance parameter \(\) of each Gaussian point in the auditory perspective, encouraging the audio-guidance parameters to be small, and non-overlapping.

The physical properties and material-related properties of the new Gaussians are decoded from the learned physical parameters and audio-guidance parameters respectively, in a view-dependent manner on-the-fly.

**Audio-aware point management** The location of points in \(G_{a}\) are initialized from \(G\), however since \(G\) is optimized using an image reconstruction loss, the initial placement of the points in \(G_{a}\) is not necessarily contributive to an optimal audio guidance. Especially, it has been observed in recent works that 3D-GS is inadequate in densifying texture-less and less observed areas [20; 6; 2].

To address this problem, we propose an error-based point growing policy that populates new points in \(G_{a}\), wherever deemed significant. Specifically, the densification step is interleaved across multiple binauralization forward passes. During each densification step, we compute averaged gradients (across all steps up to the previous consecutive densification step) for each point in \(G_{a}\) that lies in the vicinity, denoted as \(_{g}\). Points with \(_{g}>_{g}\) are deemed as significant, where \(_{g}\) is a pre-defined threshold. Using the original 3D position as a probability distribution function (PDF), we sample new points. The audio-guidance parameters for these new points are obtained by random initialization. Additionally, we regularly employ a random elimination of points to avoid rapid expansion of new points (e.g., every 3000 iterations).

## 4 Experiments

### Datasets

We evaluate both a real-world dataset-RWAVS and a synthetic dataset-SoundSpaces.

**Real-World Audio-Visual Scene (RWAVS)** The RWAVS dataset offers realistic multi-modal training samples constituting camera poses, high-quality binaural audios, and images. The dataset consists of 11 indoor and 2 outdoor scenes. Randomly sampling the sound source and listener positions, the authors collected data ranging from 10 to 25 minutes for every scene. For every scene, a data sample includes a set of camera poses (sound source and listener), extracted RGB key frame, followed by one-second binaural audio (as received at the listener position) and one-second mono source audio (as emitted by the sound source). We follow an 80:20 train-validation split for every scene in the dataset.

**SoundSpaces synthetic dataset** The Soundspaces dataset consists of 6 indoor scenes with varying degrees of complexity (2 scenes having a single room with rectangular walls; 2 scenes having a single room with a non-rectangular walls; 2 with multi-room layout). Since the dataset includes room impulse responses (RIR) recorded at receiver/listener positions, we replace the acoustic mask generation block with an RIR prediction block. The listener consists of a stereo listener, with four discrete head orientations among 0, 90, 180, and 270. We follow the same 80:20 train-validation split, as  for every scene in the dataset.

### Implementation Details

Our complete implementation is based in PyTorch using a single Nvidia A550 GPU. The learning rate for all physical parameters in \(G\) is adopted from the original 3D-GS implementation . For the acoustic field network \(\), we use MLP network with 64 nodes. Hyper-parameters for the binauralizer \(\), as well as the metrics are adopted from AV-NeRF . For the audio-aware point management, we set \(_{g}\) to 0.0004. Additionally, after every 3k iterations we randomly eliminate points using an outlier removal process (from Open3D ) that removes points that have less than 8 neighbors in a sphere less than radius of 0.1.

As a part of the proposed decoupling, we train the 3D-GS model \(G\) for 3k iterations, wherein each iteration involves randomly sampling a camera view \(V_{C}\) and optimizing the parameters for \(G\). Post this we initialize \(\) with the physical parameters from \(G\), and train for 40k iterations, wherein now each iteration involves randomly sampling an auditory perspective \(V_{A}\) and optimizing the parameters for \(G_{a}\) and \(\).

### Results

**Binaural audio synthesis - RWAVS dataset - Table 1** In order to have a fair comparison, we adopt the same metrics - magnitude distance [MAG]  (computed in time-frequency domain) and envelope distance [ENV]  (computed in time domain), as . Particularly, (1) Mono-Mono duplicates the source audio to create fake binaural audio without modifications; (2) Mono-Energy scales the input audio's energy to match the target, generating stereo audio by duplicating the scaled audio; (3) Stereo-Energy separately scales the input audio's energy for each channel to match the target, then combines them for stereo audio; In addition to the codec baselines above, we also compare with NAF , INRAS , and AV-NeRF . AV-GS significantly surpasses all prior arts across all scenes, by a significant margin. Particularly compared against AV-NeRF, which utilizesaudio-visual cues, AV-GS achieves a 5.7% and 3.4% relative improvement in terms of MAG and ENV respectively.

We highlight the qualitative improvement of AV-GS over AV-NeRF in Fig. 3 (and in Section A.1). AV-NeRF extracts visual cues from the listener's view using a 256x256 RGB and depth image (rendered by a pre-trained NeRF) and processed by a frozen ResNet18  model that is trained on ImageNet . When the listener's view includes multiple objects or complex geometries (Fig. 3(a)), or meaningless information (Fig. 3(b)), it leads to sub-optimal visual cue extraction and poor binauralization. In contrast, our AV-GS uses learned representations (combining \(\) of points in \(G_{a}\) w.r.t to the listener and speaker) to extract context at a more holistic level, remaining unaffected by these scenarios.

**RIR generation - Soundspaces dataset - Table 2(a)** Following  and , we compare AV-GS against classical high-performance audio coding methods: Advanced Audio Coding (AAC)  and Xiph Opus , applying both linear and nearest neighbor interpolation techniques to the coded acoustic fields. Additionally, we also compare with existing neural methods: NAF , INRAS , and AV-NeRF . It can be observed that AV-GS outperforms all the prior arts by a significant margin.

    &  &  &  &  &  &  \\  & & Audio & Visual & MAG & ENV & MAG & ENV & MAG & ENV & MAG & ENV \\  Mono-Mono & ✓ & ✗ & 9.269 & 0.411 & 11.889 & 0.424 & 15.120 & 0.474 & 13.957 & 0.470 & 12.559 & 0.445 \\ Mono-Energy & ✓ & ✗ & 1.536 & 0.142 & 4.307 & 0.180 & 3.911 & 0.192 & 1.634 & 0.127 & 2.847 & 0.160 \\ Stereo-Energy & ✓ & ✗ & 1.511 & 0.139 & 4.301 & 0.180 & 3.895 & 0.191 & 1.612 & 0.124 & 2.830 & 0.159 \\  INRAS  & ✓ & ✗ & 1.405 & 0.141 & 3.511 & 0.182 & 3.421 & 0.201 & 1.502 & 0.130 & 2.460 & 0.164 \\ NAF  & ✓ & ✗ & 1.244 & 0.137 & 3.259 & 0.178 & 3.345 & 0.193 & 1.284 & 0.121 & 2.283 & 0.157 \\  AV-NeRF  & ✓ & ✓ & 0.930 & 0.129 & 2.009 & 0.155 & 2.230 & 0.184 & 0.845 & 0.111 & 1.504 & 0.145 \\
**AV-GS (ours)** & ✓ & ✓ & **0.861** & **0.124** & **1.970** & **0.152** & **2.031** & **0.177** & **0.791** & **0.107** & **1.417** & **0.140** \\   

Table 1: Comparison with state-of-the-art methods on RWAVS dataset.

Figure 3: In the presence of (a) complex geometry, and (b) meaningless views, AV-NeRF, when compared to our AV-GS makes errors in binaural synthesis. For both scenarios we showcase the corresponding listener view, used by AV-NeRF, as well as the learned holistic scene representation that is used by AV-GS, and hence unaffected by both scenarios.

**Improved efficiency** Decoding \(\) on-the-fly helps AV-GS retain the efficiency advantage of having an explicit point based representation over learning an implicit representation of the visual 3D scene. In contrast, AV-NeRF trains an additional NeRF model, whose outputs are used to render the listener's view which is the condition for their binauralizer module. In Table 3, we compare the inference time for a single scene (House scene 1) from the RWAVS dataset. Although the original implementation of AV-NeRF proposed a single view in the direction of listener's viewing direction (used as the listener's context), we modify their implementation in which the V-NeRF renders 2 and 4 perspective views with a field of view of 90\({}^{}\)for each receiver's position. It can be observed, although using the additional views, helps provide additional local context around the listener (hence improving the binauralization performance), but only at the cost of increased inference time. AV-GS on the other hand requires least rendering time, while providing the best binauralization performance. It is important to note that, although we train the 3D-GS for only an initial warm-up stage of 3k iterations, we can still jointly train both \(G\) and \(G_{a}\) on the corresponding camera view \(V_{C}\) and the auditory perspective \(V_{A}\) respectively. By doing so, AV-GS would be able to synthesize both binaural audio as well the visual view at the unseen target (novel) viewpoint, without any increase in the training time than that of as shown in Table 3 (last row). In this work, we instead focus on only the binaural audio synthesis and hence train only \(G_{a}\) in the second stage.

Although we train the 3D-GS for just an initial 3k iterations, we can still train both \(G\) and \(G_{a}\) together for the corresponding camera view \(V_{C}\) and auditory perspective \(V_{A}\), such that the trained AV-GS is able to synthesize both the binaural audio and the visual view from a novel viewpoint, thereby making it a fair comparison with AV-NeRF . However, in this work, we focus solely on binaural audio synthesis and only train \(G_{a}\) in the second stage.

### Ablation study

We use the binaural audio synthesis task for performing our ablation study and report average performance across all scenes in the RWAVS dataset.

**Initialization of \(\).** 3D-GS learns a point-based representation (\(G\)) of the 3D scene such that each point in the explicit representation is represented as a 3D Gaussian ellipsoid to which physical attributes like position \(\), quaternion \(\), scale \(\), opacity \(\) and Spherical Harmonic coefficients (\(\)) representing view-dependent color are attached. As discussed above, we initialize the audio-guidance parameter \(\) using the learned physical parameters from \(G\). Table 2(b) shows an in-depth study of the effect on the binauralization performance when choosing different parameters from \(G\) for the initialization. It is evident that \(\) and \(\), when considered individually as well as when combined, are crucial parameters that help provide an overall better binauralization performance.

**Size of the vicinity** The learned Gaussian points \(G\) representation normally consists of millions of points, and using all the points for computing the condition for the binauralization task will result in a huge computational overload. Moreover, intuitionally, points near the listener and speaker location in the 3D space (_i.e._vicinity) are more contributive to determining the local geometry and material-related characteristics, and greatly influence the sound field. Empirically in Table 4a, we find that using 15 percentile of the points in the vicinity (listener and speaker considered separately) yields the best performance. Using an extremely high percentile for capturing the vicinity generally helps, but only to a certain extent, since it increases the risk of adding unnecessary information. Moreover, it is important to note that in scenes with smaller room sizes or when the listener and speaker are

    & Time (\(\)) &  \\  & Inference & MAG & ENV \\  AV-NeRF - 1 view & 1.4s & 1.504 & 0.145 \\ AV-NeRF - 2 views & 2.9s & 1.495 & 0.145 \\ AV-NeRF - 4 views & 7.6s & 1.482 & 0.144 \\  AV-GS (ours) & **0.08s** & **1.417** & **0.140** \\   

Table 3: Comparing for efficiency in terms of the inference time with the existing AV-NeRF using multiple perspective views. Please note the inference time includes the time for rendering the view from V-NeRF, extracting features through AV-Mapper () and then rendering the binaural audio from A-NeRF.

placed nearby, a larger vicinity capture might lead to overlapping points thus adding to redundant information (see Fig. 4 (d)).

**Effect of audio-aware point management.** AV-GS infers local geometry from an explicit point-based prior and learns additional audio-specific parameters for every point. The local geometry can be inferred either directly from the sparse points produced during camera calibration using Structure-from-motion (SfM)  or warming up a 3D-GS model on the SfM points. From Table 3(b) it is evident that audio-aware point management helps improve the binarization performance irrespective of the adopted point initialization approach. Texture-less regions such as walls, doors, etc are the regions with maximum sound absorption and diversion and hence having optimal point density promotes better modeling of sound propagation.

## 5 Conclusion

In this work, we present AV-GS, a holistic scene representation learning approach for conditioning novel view acoustic synthesis. The core of AV-GS lies in the proposed decoupled modeling of 3D scene geometry and material-related characteristics of scene objects for sound propagation, enabled by the introduction of additional audio-guidance parameters per point within an acoustic field network. We show that our approach leverages audio-aware Gaussian point positioning to improve the binauralization performance, on real world as well as synthetic 3D scenes, significantly in comparison to prior art alternatives.

**Limitations** Although AV-GS achieves a new state-of-the-art on the NVAS task by learning a holistic scene representation with geometry-aware and implicit material-aware characteristics, it faces some challenges. Like AV-Nerf, AV-GS currently learns a separate representation for each scene, posing a significant challenge for generalizability and transferability across multiple scenes, which is crucial for improving efficiency and reducing computational demands. Also, as observed in Table 1, the performance drops when the scene size increases (e.g., from Office to House), due to the difficulty in learning representations for larger, complex geometries with multiple rooms. An interesting research direction would be to model individual rooms using separate 3D-GS representations and then learn a transfer function across rooms.

Table 4: (a)Ablation on the size of vicinity w.r.t the listener and sound source position. (b) Effect of audio-aware point management.

Figure 4: Ablation on the size of vicinity w.r.t the listener and sound source position. Percentile-k denotes the top \(k\) % points nearest to the listener and sound source.