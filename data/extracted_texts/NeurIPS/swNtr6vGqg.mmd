# The noise level in linear regression

with dependent data

 Ingvar Ziemann

University of Pennsylvania

&Stephen Tu

Google Research

George J. Pappas

University of Pennsylvania

&Nikolai Matni

University of Pennsylvania

###### Abstract

We derive upper bounds for random design linear regression with dependent (\(\)-mixing) data absent any realizability assumptions. In contrast to the strictly realizable martingale noise regime, no sharp _instance-optimal_ non-asymptotics are available in the literature. Up to constant factors, our analysis correctly recovers the variance term predicted by the Central Limit Theorem--the noise level of the problem--and thus exhibits graceful degradation as we introduce misspecification. Past a burn-in, our result is sharp in the moderate deviations regime, and in particular does not inflate the leading order term by mixing time factors.

## 1 Introduction

Ordinary least squares (OLS) regression from a finite sample is one of the most ubiquitous and widely used technique in machine learning. When faced with independent data, there are now sharp tools available to analyze its success optimally under relatively general assumptions. Indeed, a non-asymptotic theory matching the classical asymptotically optimal understanding from statistics (van der Vaart, 2000) has been developed over the last decade (Hsu et al., 2012; Oliveira, 2016; Mourtada, 2022). However, once we relax the independence assumption and move toward data that exhibits correlations, the situation is much less well-understood--even for a problem as seemingly simple as linear regression. While sharp asymptotics are available through various limit theorems, there are no general results matching these in the finite sample regime.

In this paper, we study the instance-specific performance of ordinary least squares in a setting with dependent data--and in contrast to much contemporary work on the theme--without imposing realizability.1 If in addition to a realizability assumption the noise forms a martingale difference sequence, it is now well-known that martingale methods can be used to demonstrate that dependent linear regression is no harder than its independent counterpart (Simchowitz et al., 2018). Furthermore, as long as one maintains such an assumption on the noise, a similar observation even holds true for generalized linear and bilinear models (Kowshik et al., 2021; Sattar et al., 2022), and regression with square loss more generally (Ziemann and Tu, 2022).

However, barring any such strong realizability assumption, martingale methods are no longer directly available, and neither are there any sharp non-asymptotics in the learning theory literature. Absent martingale techniques, a natural approach is to use the blocking technique (Bernstein, 1927; Yu, 1994) to port concentration inequalities valid for independent data to the dependent setting. However, since blocking effectively reduces the sample size by a factor of the degree of dependence of the data, a judicious application is necessary in order to recover the correct noise level of the problem--the level predicted by the Central Limit Theorem (CLT).

### Contributions

This paper serves to explain how the combination of two simple yet powerful observations sidestep the aforementioned issues with blocking. To better appreciate these observations, we recall that the analysis of random design linear regression decomposes into: (1) controlling the _lower tail_ of the empirical covariance matrix; and (2) controlling the interaction between the noise and the covariates.

First, as noted by Mendelson (2014), the dominant contribution to the error rate is due to the interaction of the noise with the covariates via the hypothesis class. In linear regression this interaction term takes the form of a random walk (see (2.6) and (2.8) below). While one must also analyze the lower tail of the empirical covariance matrix, its contribution to the final error tends to be lower order. This is exactly the point: the empirical covariance matrix tends to dominate its population counterpart under very mild assumptions (see e.g. Koltchinskii and Mendelson, 2015; Oliveira, 2016; Simchowitz et al., 2018). Hence deflating the sample size for this purpose by using dependency is of relatively minor consequence and only amounts to an additional burn-in.

Second, turning to the random walk--the noise-class interaction term--the above issue with blocking can be remedied if one restricts its use to control only the largest scale of deviation. This observation can be traced to the moderate deviations literature, but does not seem to have made its way into the learning theory literature (see e.g. Merlevede et al., 2011). To explain this idea, let us recall Bernstein's inequality: for \(b>0\), \((0,1)\), and a sequence of \(n\) iid mean zero \(b\)-bounded scalar random variables \(V_{1:n}\),

\[(_{i=1}^{n}V_{i} 2V_{1} ^{2}(1/)}{n}}+).\] (1.1)

In the moderate deviations bandwidth (\((-nV_{1}^{2}/b^{2})\)), the leading term of (1.1) is exactly of the expected order, seen from a central limit heuristic: \(V_{1}^{2}(1/)}{n}}\). Assume now for sake of argument that \(k\) divides \(n\) and set \(m=n/k\). Applying (1.1) instead to the \(bk\)-bounded variables \(_{i:m},_{i}_{j=ik-k-1}^{ik}V_{j}\) we find instead:

\[(_{i=1}^{n}V_{i} 2( _{1})^{2}(1/)}{n}}+).\] (1.2)

The (normalized) variance of iid random variables tensorizes nicely (\(k^{-1}(_{1})^{2}=V_{1}^{2}\)), and so the only difference between (1.1) and (1.2) is that the large deviations term has been inflated by a factor \(\). More generally, however, (1.2) remains valid as long as every \(k\) samples are blockwise independent. The leading term of (1.2) already captures the correct variance term in the blockwise independent, one-dimensional and bounded setting.

The above two paragraphs illustrate the core of our argument: by combining the above two observations we can entirely relegate any dependence on mixing to additive burn-in factors. In the sequel, we produce a more general version of this argument. To allow for arbitrary dimensions and handle unbounded processes, we first replace Bernstein's inequality with a corollary to Talagrand's inequality due to Einmahl and Li (2008). To allow for \(\)-mixing processes, we replace the blockwise independence assumption with the blocking strategy of Yu (1994). By combining with control of the lower tail, which as noted above holds under mild assumptions, this leads to our main result Theorem 3.1, captured informally below.

**Informal version of Theorem 3.1.** _Past a mild burn-in, polynomial in relevant problem quantities including the \(\)-mixing coefficients of the data, and for a fixed failure probability \((0,1)\), OLS with one-dimensional targets and \(d_{}\)-dimensional covariates enjoys the following excess risk guarantee_:

\[\,\,() (d_{}+(1/))}{n}.\] (1.3)

_Moreover, the term \(^{2}\) in (1.3) accurately captures the noise level of the problem solely via the relevant second order statistics; it is not inflated by any mixing times._

The crux of this result is that past a burn-in, the OLS excess risk does not directly depend on mixing times, but only on the relevant second order statistics. Put differently, the effect of slow mixing hasbeen relegated to a small _additive_ term with higher order dependence on \(1/n\). This stands in stark contrast to the usual invocation of the blocking technique where the effect of mixing typically enters _multiplicatively_, thereby degrading the rate of convergence uniformly across all sample-sizes and past any burn-in times (see e.g. Steinwart and Christmann, 2009; Kuznetsov and Mohri, 2017; Wong et al., 2020; Roy et al., 2021).

Applicability.Before we proceed with the main development, we remark that the class of \(\)-mixing is quite broad; a few examples where Theorem 3.1 can be instantiated are as follows:

* all \(\)-mixing processes are \(\)-mixing (Doukhan, 2012),
* stationary uniformly ergodic Markov chains are \(\)-mixing,
* stationary Gaussian vector autoregressive moving average (ARMA) processes are \(\)-mixing (Mokkadem, 1988),
* many other sub-classes of GARCH models, often studied in the economics and finance literature, are \(\)-mixing (Carrasco and Chen, 2002).

The list is far from exhaustive and further examples can for instance be found in Doukhan (2012). The stationarity assumptions above can also typically be dropped. We also point out that it is precisely because we can handle misspecification that our result is of interest for many of these examples.

Outline.The rest of this article is structured as follows. Section 2 fixes our notation and yields a more formal problem formulation. We provide our main result, Theorem 3.1, in Section 3. After stating our main theorem, we highlight its features and then proceed to compare it to related work in Section 3.1. We outline the proof of Theorem 3.1 and provide supporting results in Section 4, including separate analyses of the noise-interaction and the lower tail of the empirical covariance matrix. Section 5 concludes and technical details are relegated to Appendix A.

## 2 Preliminaries

Notation.Expectation (resp. probability) with respect to all the randomness of the underlying probability space is denoted by \(\) (resp. \(\)). For two probability measures \(\) and \(\) defined on the same probability space, their total variation is denoted \(\|-\|_{}\). Maxima (resp. minima) of two numbers \(a,b\) are denoted by \(a b=(a,b)\) (resp. \(a b=(a,b)\)). For an integer \(n\), we also define the shorthand \([n]\{1,,n\}\).

The Euclidean norm on \(^{d}\) is denoted \(\|\|_{2}\), and the unit sphere in \(^{d}\) is denoted \(^{d-1}\). The standard inner product on \(^{d}\) is denoted \(,\). We embed matrices \(M^{d_{1} d_{2}}\) in Euclidean space by vectorization: \(\,M^{d_{1}d_{2}}\), where \(\) is the operator that vertically stacks the columns of \(M\) (from left to right and from top to bottom). For a matrix \(M\), the Euclidean norm is the Frobenius norm, i.e., \(\|M\|_{F}\|\,M\|_{2}\). We similarly define the inner product of two matrices \(M,N\) by \( M,N\,M,\,N\). The transpose of a matrix \(M\) is denoted by \(M^{}\)--and if \(M\) is square--\(M\) denotes its trace. We also write \(\|M\|_{}\) for the induced \((^{d},\|\|_{2})(^{d},\|\|_{2})\) norm. For two symmetric matrices \(M,N\), we write \(M N\) (\(M N\)) if \(M-N\) is positive (semi-)definite. If \(M\) is positive semidefinite we write \( M\{x^{d} x^{}Mx=1\}\) for the boundary of the ellipsoid induced by \(M\) (note that \(^{d-1}= I_{d}\)).

Problem Formulation.We are given \(n\) input-output tuples: \(X_{1:n}^{X}_{1:n}\) (taking values in \(^{d_{}}\)) and \(Y_{1:n}^{Y}_{1:n}\) (taking values in \(^{d_{}}\)). Using these samples, the goal of the learner is to estimate the best linear hypothesis:

\[M_{}*{argmin}_{M^{d_{}} }\{\|Y-MX\|_{2}^{2}\}\] (2.1)

where the distributions of \(X\) and \(Y\) in (2.1) are specified via:

\[X_{i=1}^{n}^{X}_{i} Y_{i=1}^{n}^{Y}_{i}.\] (2.2)Note that (2.2) is equivalent to sampling from the uniform mixture over \((X_{1:n},Y_{1:n})\) with the index \(i[n]\) sampled uniformly. The operator \(_{X}[XX^{}]\) is the averaged covariance operator (with \(X\) as in (2.2)). The excess risk of a linear hypothesis \(M\) can then be written as:

\[\,(M)\|Y-MX\|_{2 }^{2}-\|Y-M_{}X\|_{2}^{2}=\|(M-M_{})}\|_{F }^{2}.\] (2.3)

We now define the _noise variable_\(W_{i} Y_{i}-M_{}X_{i}\) but, as mentioned above, do not impose any (conditional) mean zero assumptions on the noise. To simplify the exposition, we will henceforth assume that \(_{X} 0\), but our results easily extend to the case \(_{X} 0\) by restricting attention to the span of \(_{X}\). With these preliminaries in place, on the event that the design is nondegenerate, the OLS and its error equation can be specified as follows:

\[(_{i=1}^{n}Y_{i}X_{i}^{} )(_{i=1}^{n}X_{i}X_{i}^{})^{-1} -M_{}=(_{i=1}^{n}W_{i}X_{i}^{} )(_{i=1}^{n}X_{i}X_{i}^{})^{-1}.\] (2.4)

Our task in the sequel is to establish that the choice \(\) renders the excess risk (2.3) small. We note in passing that \(\) is an empirical risk minimizer:

\[*{argmin}_{M^{d_{} d _{}}}\{_{i=1}^{n}\|Y_{i}-MX_{i}\|_{2}^{2}\}.\]

The Noise Term.Let us also define the following prefiltered noise-class interaction variables:

\[V_{i} W_{i}X_{i}^{}_{X}^{-1/2} i [n].\] (2.5)

The square of the following (weighted and possibly biased) random walk effectively characterizes the noise level in our problem:

\[S_{n}_{i=1}^{n}V_{i}.\] (2.6)

We remark that by construction \(S_{n}=0\) using the optimality of \(M_{}\) in (2.1). To see this, simply invoke the optimality equation for \(M_{}\) and note that \(^{d_{} d_{}}\) induces a convex class in the corresponding \(L^{2}\)-space over the mixtures (2.2). Note however that the increments of (2.6) are not necessarily mean zero unless \(X_{1:n}\) and \(Y_{1:n}\) are stationary. However, since \(S_{n}=0\), we also have with \(_{i} V_{i}-V_{i}\):

\[S_{n}=_{i=1}^{n}_{i}.\] (2.7)

In light of (2.4) and (2.6), we have that the empirical excess risk depends on the norm of:

\[(-M_{})}=S_{n}( _{i=1}^{n}_{X}^{-1/2}X_{i}X_{i}^{}_{X}^{-1/2}) ^{-1}.\] (2.8)

Hence, we need to control the random walk in (2.7) and the lower tail of

\[_{n}_{i=1}^{n}_{X}^{-1/2}X_{i}X_{i }^{}_{X}^{-1/2},\] (2.9)

the prefiltered empirical covariance matrix. As mentioned previously, lower uniform laws for (2.9) are valid under mild assumptions (Koltchinskii and Mendelson, 2015; Oliveira, 2016), and blocking such results does not incur more than a worsening of the burn-in. Hence, the noise level of the problem is very much dictated by the random walk (2.6).

\(\)-mixing and the Blocking Technique.In the sequel we demonstrate that the standard blocking device combined with a (functional) version of Bernstein's inequality allows us to pass the distributional (or coarse) measure of dependency to a higher order additive term, yielding non-asymptotic rates consistent with the CLT as described in Section 1. We will also use blocking to derive our lower uniform law, controlling the lower tail of (2.9). To make these ideas rigorous we require the following standard measure of dependence (take \(Z_{1:n}=(X,Y)_{1:n}\) below).

**Definition 2.1**.: _Let \(Z_{1:n}\) be a stochastic process. The \(\)-mixing coefficients of \(Z\), \(_{Z}(i)\), are:_

\[_{Z}(i)=_{t[n]:t+i n}\|_{Z_{i+t}}( Z _{1:t})-_{Z_{i+t}}\|_{}, i[n].\] (2.10)

Intuitively, the coefficients \(_{Z}(i)\) in (2.10) measure the dependence at range \(i\) of the process \(Z_{1:n}\). This measurement is done in total variation distance by comparing the distribution of \(Z_{t+i}\) with the conditional distribution \(Z_{t+i} Z_{1:t}\). More concretely, the notion of \(\)-mixing allows us to use the blocking technique Yu (1994). This technique splits the process \(Z_{1:n}\) into blocks, such that every other block is approximately independent, leaving us with two separate processes, consisting of odd and even blocks, that are almost independent (see (4.1) below). One then proceeds to use \(\)-mixing to construct a "parallel" probability space, approximating the original one, but in which the odd and even blocks _are independent_. The price we pay for this is measured in terms of the coefficients (2.10). The essence of this idea--to use that data points sufficiently separated in time are often roughly independent--can be traced back to Bernstein (1927).

## 3 Main Result

To give our main result for general target dimension we require one last preliminary notion. Given a \(d\)-dimensional square, symmetric positive semidefinite matrix \(M^{d d}\), we say that its _effective_ dimension is \((M)\,M/\|M\|_{}\). Our main result is the following theorem.

**Theorem 3.1**.: _Fix \((0,1)\) and \(n,m\) with \(2m n\). Let \(a_{1:2m}\) be a monotone partition of \([n]\) such that if \(k a_{i}\), \(l a_{j}\), and \(i>j\), then \(k>l\) holds. Set \(a_{}_{i[2m]}|a_{i}|\). Fix also a \(\)-mixing sequence \((X,Y)_{1:n}\) of which each element admits at least \(s[4,)\) moments. Assume that there exists a positive number \(h\) such that for every \(v_{X}\) and \(i[n]\) we have that \( v,X_{i}^{4}^{2} v,[X_ {i}X_{i}^{}]v\). Define also the noise interaction terms:_

\[_{i=1}^{2m}_{i},_{i} [(_{j a_{i}}} )(_{j a_{i}}})^{} ],^{2}\|\|_{}.\] (3.1)

_There exist universal positive constants \(c_{1},c_{2},c_{3},c_{4},c_{5},c_{6}\) such that with probability at least \(1-\):_

\[\|(-M_{})}\|_{F}^{2}^{2} (()+(1/))}{n}\] (3.2)

_as long as the following burn-in conditions hold:_

\[|} c_{2}(d_{}+^{2}(1/)); (|})^{1-2/s} c_{3}s^{2}_{i=1}^{2m}\||}}_{j  a_{i}}}\|_{F}^{s})^{2/s}}{() ^{2}^{2/s}};\] (3.3)

\[c_{4}^{-1}<}|a_{i}|}{_{i[2m](2 -1)}|a_{i}|}<c_{4};_{i[2m](2-1)}c_{5}^{-1} _{i}_{i[2m] 2}_{i}_{i[2m](2 -1)}c_{5}_{i};\] (3.4)

\[_{i=2}^{2m-1}_{X,Y}(|a_{i}|) c_{6};\] (3.5)

_and where \(_{X,Y}\) are the \(\)-mixing coefficients of \((X,Y)_{1:n}\)._

To interpret Theorem 3.1 we proceed with a sequence of remarks, discussing its features. These remarks also serve to parse the terminology above and lead us to a simplified statement for stationary data and \(1\)-dimensional targets. We present this simplified version as Corollary 3.1 below.

* The dimensional scaling in (3.2) is captured by the effective dimension term \(()\), which always lies in the interval \([1,d_{}d_{}]\). For one-dimensional targets and benign noise interaction, and since the \(X_{i}\) in the noise term \(V_{i}\) have been whitened (see (2.5)), we expect \(\) to be roughly isotropic. Indeed, whenever \(d_{}=1\), the trivial bound \(() d_{}\) produces the familiar behavior: \(\|(-M_{})}\|_{2}^{2}^{2}d_{ }/n\) with high probability.

* The scaling with \(d_{},d_{},^{2}\) and \(n\) thus scales as expected in the iid regime, but also degrades gracefully with dependence. We reiterate that (3.2) does not depend directly on mixing in the leading order term. For comparison, under suitable regularity conditions, the noise term predicted by the CLT is: \[_{}_{n}n^{-1}[ (_{j=1}^{n}})(_{j=1} ^{n}})^{}],\] (3.6) and one can achieve \(=_{}+o(1)\) in most situations of practical interest by tuning the block-length; therefore, our analysis of OLS essentially matches the optimal asymptotics.
* Moreover, the constant \(c_{1}\) appearing in (3.2) is quite benign and can be made arbitrarily close to \(2\) by suitably inflating our burn-in constants \(c_{2},c_{3},c_{4},c_{5},c_{6}\). We have however not been able to approach the optimal leading constant \(1\) in front of \(()\). This can be traced to an application of the triangular inequality in Corollary 4.1.
* The moment bound \( v,X_{i}^{4}^{2} v,[X_ {i}X_{i}^{}]v\)\((v_{X})\) is easily satisfied for e.g., Gaussian or bounded processes but is of course much milder than either assumption. The assumption that \(s 4\) can be relaxed to \(s>2\) by replacing our result controlling the lower tail, Theorem 4.3. We have chosen to present our result for \(s 4\) to strike a balance between expositional clarity and generality.
* The first burn-in condition of (3.3) is standard for control of the lower tail--beside the deflation factor \(|a_{}|\), it is necessary even for iid data to guarantee that the "denominator" (2.9) is nonsingular. The second condition of (3.3)--in which a ratio of \(s\):th and \(2\)nd moment of the noise variable appears--is the price we pay in the moderate deviations bandwidth for only having \(s\) moments: it controls the rate at which the random walk (2.6) approaches asymptotic normality and reduces to the condition of Oliveira (2016) in the iid regime. This latter condition can in principle be removed with slightly modified constants if sufficiently many moments of the data-generating process satisfy a sub-Gaussian type moment equivalence condition (in which case the above-mentioned ratio is constant). Without such an assumption, we note that some polynomial dependence on \(1/\) is necessary under our tail assumptions and is not an artifact of our analysis; OLS is not deviation-optimal in the entire range of \((0,1)\)--due to the presence of the random walk (2.6) in the numerator--unless the noise variables have Gaussian-like tails (cf. Mendelson, 2018, Section 6.4).
* The conditions in (3.4) and (3.5) relate to dependence. The last condition (3.5) simply asks that our process mixes sufficiently fast. If the mixing coefficients \(_{X,Y}(|a_{i}|)\) are exponential, this amounts to a logarithmic burn-in in \(1/\). However, we can still handle slow, polynomial mixing rates, at the cost of a polynomial burn-in in \(1/\). The conditions in (3.4) asks that the odd and even blocks are balanced in terms of their length and second order statistics. It is trivially satisfied for (weakly) stationary processes analyzed using a uniform blocking length (length of the \(a_{i}\)).

In light of the above remarks, we are now in position to simplify Theorem 3.1. If we impose stationarity, quite a few terms in the burn-in conditions (3.3),(3.4) and (3.5) either simplify or vanish. Further restricting to the case where targets are \(1\)-dimensional and letting the sample-size be divisible by the block-length yields the corollary below.

**Corollary 3.1**.: _Fix \((0,1)\) and \(n,\) and let \(2\) divide \(n\). Fix also a joint distribution of \(1\)-dimensional targets and \(d_{}\)-dimensional covariates \(^{X,Y}\) with at least \(s[4,)\) moments. Let \((X,Y)_{1:n}\) be a stationary \(\)-mixing sequence with marginals equal to \(^{X,Y}\). Assume further that there exists a positive number \(\) such that for every \(v_{X}\) we have that \( v,X^{4}^{2}\) where \(X^{X}\). Define also the noise interaction term: \(^{2}_{v^{d_{}-1}} [(_{i=1}^{}},v^{2})]\). There exist universal positive constants \(c_{1},c_{2},c_{3},c_{4}\) such that with probability at least \(1-\):_

\[\|(-M_{})}\|_{2}^{2}^{2} (d_{}+(1/))}{n}\]

_as long as the following burn-in conditions hold:_

\[ c_{2}(d_{}+^{2}(1/)); ()^{1-2/s} c_{3}s^{2}\| d_{}}_{i=1}^{}}\|_{2}^{ s})^{2/s}}{^{2}^{2/s}};_{X,Y}()  c_{4}.\] (3.7)Corollary 3.1 takes a very similar form to--by now--standard results in the iid regime (Hsu et al., 2012, Oliveira, 2016). Indeed, if the data is stationary the price we pay for dependence is that:

* variance and moment terms need to be computed in blocks;
* the burn-in is deflated by a factor of the block-length (the first two parts of (3.7)); and
* we incur an additional burn-in penalizing slow mixing--the last part of (3.7) asks that the block-length is not "too small".

### Comparison to Related Work

Having established our main result, Theorem 3.1, we now provide a more detailed comparison to the relevant literature. Most closely related to our results is Nagaraj et al. (2020), who study bounded linear regression models in which the data comes from an exponentially ergodic Markov chain. They find that strictly realizable linear regression is no harder than its iid counterpart in this setting, and show that a parallelized gradient algorithm achieves the optimal rate. More interestingly, in the absence of realizability, they also establish a lower bound demonstrating that the worst-case (global minimax) excess risk across all Markov chains with a given mixing time is deflated by said mixing time, thereby establishing a gap between realizable and non-realizable learning from dependent data.

Of course, their lower bound is no longer valid if one drops the requirement that the predictor performs uniformly well across all distributions with a prescribed mixing time. It is exactly herein that our analyses differ. While Nagaraj et al. (2020) characterize the worst-case (or global) complexity of linear regression, we focus on the instance-specific (or local) complexity. In other words, they compete against the worst distribution at a given level of mixing, whereas we compete against a fixed distribution. To appreciate this distinction, let us momentarily assume that \(d_{}=d_{}=1\). The noise term \(^{2}\) in Theorem 3.1 can be upper-bounded as:

\[^{2}=_{i=1}^{2m}[(_{j a_{i}} _{j})^{2}]_{i[2m]}|a_{i}|_{i[n]} _{i}^{2}\] (3.8)

by the Cauchy-Schwarz inequality. The right hand side of (3.8) is precisely inflated by the (maximal) block-length \(_{i[2m]}|a_{i}|\).

Seen in this light, our results being sharper in terms of the measure of dependency reduces to stating that our results are sharper by an application of the Cauchy-Schwarz Inequality. Moreover, the statement that the global complexity is worse than its iid counterpart by a factor of the mixing time amounts in our setting to stating that there exists a distribution achieving equality in (3.8). We remark that such a distribution is easily constructed by taking \(X_{1:n}\) and \(Y_{1:n}\) to be constant within each block and stationary across the blocks; this is precisely when the application of the Cauchy-Schwarz inequality in (3.8) turns to equality. To further appreciate the distinction between our results, note that our result measures dependence through correlation. By contrast, a result scaling with the mixing time measures dependence in a stronger variational sense. That is, the former measures dependence at the level of orthogonality of the random variables themselves, whereas the latter measures it at the level of orthogonality of all measurable functions of these random variables.

Further Related Work.Another closely related line of work studies parameter identification in auto-regressive models (for a recent survey, see Tsiamis et al., 2022). When the noise model is strictly realizable--the variables \(W_{1:n}\) form a martingale difference sequence with respect to the filtration generated by \(X_{1:n}\)--identification is possible at the iid rate even in the absence of mixing (Simchowitz et al., 2018, Faradonbeh et al., 2018, Sarkar and Rakhlin, 2019). Naturally, our results do not cover the mixing-free regime as we consider: (1) the agnostic setting in which self-normalized martingale arguments (Pena et al., 2009, Abbasi-Yadkori et al., 2011) are not available; and (2) excess risk bounds instead of parameter identification--it seems unlikely that (3.2) holds without some notion of stochastic stability due to the presence of \(_{X}\) on the left hand side.

More generally--moving beyond linear time-series models--several authors have considered learning under various weak dependency notions. Kuznetsov and Mohri (2017) give generalization bounds in a more general setting using the same blocking technique--due to Yu (1994)--used here. Statements similar in spirit can also be found in e.g., Steinwart and Christmann (2009), Duchi et al. (2012) and most recently Roy et al. (2021). However, they all suffer the dependency deflation discussed above and in our introduction (Section 1). We also note that Ziemann and Tu (2022) obtain rates for strictly realizable square loss that--similar to ours here--relegate mixing times into additive burn-in factors. While they treat more general hypothesis classes, they do not go beyond strict realizability, and their analysis rests on the assumption that the noise interaction term is a martingale difference sequence.

## 4 Proof Overview

Theorem 3.1 is the direct consequence of two separate results: Theorem 4.2, which controls the centered noise (2.7) term, and Theorem 4.3, which bounds the lower tail of the normalized empirical covariance matrix (2.9). We prove Theorem 4.2 by blocking the Fuk-Nagaev inequality of Einmahl and Li (2008), and Theorem 4.3 by a truncation argument combined with blocking. These results are found in Section 4.2 and Section 4.3. Our proof idea is heavily inspired by that of Oliveira (2016) and the idea is very much to adjust his approach in such a way that blocking does not affect the leading term in the rate.2 As either result relies on blocking, it is now pertinent to describe this technique in a little more detail.

### Blocking

Recall that we partition \([n]\) into \(2m\) consecutive intervals, denoted \(a_{j}\) for \(j[2m]\), so that \(_{j=1}^{2m}|a_{j}|=n\). Denote further by \(O\) (resp. by \(E\)) the union of the oddly (resp. evenly) indexed subsets of \([n]\). We further abuse notation by writing \(_{Z}(a_{i})=_{Z}(|a_{i}|)\) in the sequel.

We split the process \(Z_{1:n}\) as:

\[Z_{1:|O|}^{o}(Z_{a_{1}},,Z_{a_{2m-1}}), Z_{1:|E|}^{e} (Z_{a_{2}},,Z_{a_{2m}}).\] (4.1)

Let \(_{1:|O|}^{o}\) and \(_{1:|E|}^{e}\) be blockwise decoupled versions of (4.1). That is we posit that \(_{1:|O|}^{o}_{_{1:|O|}^{o}}\) and \(_{1:|E|}^{e}_{_{1:|E|}^{e}}\), where:

\[_{_{1:|O|}^{o}}_{Z_{a_{1}}} _{Z_{a_{3}}}_{Z_{a_{2m-1}}}_{_{1:|E|}^{e}}_{Z_{a_{2}}} _{Z_{a_{4}}}_{Z_{a_{2m}}}.\] (4.2)

The process \(_{1:n}\) with the same marginals as \(_{1:|O|}^{o}\) and \(_{1:|E|}^{e}\) is said to be the decoupled version of \(Z_{1:n}\). To be clear: \(_{_{1:n}}_{Z_{a_{1}}}_ {Z_{a_{2}}}_{Z_{a_{2m}}}\), so that \(_{1:|O|}^{o}\) and \(_{1:|E|}^{e}\) are alternatingly embedded in \(_{1:n}\). The following result is key--by skipping every other block, \(_{1:n}\) may be used in place of \(Z_{1:n}\) for evaluating scalar functions at the cost of an additive mixing-related term.

**Proposition 4.1** (Lemma 2.6 in Yu (1994); Proposition 1 in Kuznetsov and Mohri (2017)).: _Fix a \(\)-mixing process \(Z_{1:n}\) and let \(_{1:n}\) be its decoupled version. For any measurable function \(f\) of \(Z_{1:|O|}^{o}\) (resp. \(g\) of \(Z_{1:|E|}^{e}\)) with joint range \(\) we have that:_

\[|(f(Z_{1:|O|}^{o}))-(f(_{1:|O|}^{o}))| _{i E\{2m\}}_{Z}(a_{i}),\] (4.3) \[|(g(Z_{1:|E|}^{e}))-(g(_{1:|E|}^{e}))| _{i O\{1\}}_{Z}(a_{i}).\]

The following corollary to Proposition 4.1 is convenient for controlling norms of random walks.

**Corollary 4.1** (Lemma 3 in Kuznetsov and Mohri (2017)).: _Let \(Z_{1:n}\) be a \(\)-mixing process taking values in a normed space \((,\|\|)\), and let \(_{1:n}\) be its decoupled version. For any \[\|_{i O}_{i} \|\|_{i E}_{i}\|\]

_we have that:_

\[(\|_{i=1}^{n}Z_{i}\|> )_{i=1}^{2m}_{Z}(a_{i})\\ +(\|_{i O}_{i} \|>\|_{i O}_{i}\|+ _{o})+(\|_{i E} {Z}_{i}\|>\|_{i E}_{i}\| +_{e}),\] (4.4)

_where \(_{o}=-\|_{i O} {Z}_{i}\|\) and \(_{e}=-\|_{i E} {Z}_{i}\|\)._

In short, up to a mild failure additional failure probability term, we only need to control the tensor product processes (4.2).

### Dependent Random Walks

Once equipped with Corollary 4.1, we still require control of the independent blocks. The following Fuk-Nagaev inequality due to Einmahl and Li (2008) provides such control.

**Theorem 4.1** (Theorem 4 in Einmahl and Li (2008)).: _Fix \(s>2\), a separable normed space \((,\|\|)\) and a \(\)-valued sequence \(U_{1:n}\) of independent random variables. Assume that \(\|U_{i}\|^{s}<\) for \(i[n]\). Then for any \((0,)\), \((0,1]\), and \(t 0\), we have that:_

\[(_{k[n]}\|_{i=1}^{k}U_{i}\| (1+)\|_{i=1}^{n}U_{i}\|+(1+9)t )\\ (-}{(2+)})+C_{ ,,s}_{i=1}^{n}\|U_{i}\|^{s}}{t^{s}},\] (4.5)

_where \(_{v^{*}}_{i=1}^{n}v^{2}(U_ {i})\) and where \(^{*}\) is unit disk in the dual space of \((,\|\|)\). Moreover, we may take \(C_{,,s}=(1+(2s/e)^{2s}(2(1+2/)(3+4/))^{2}+ ^{-s})\).3_

If our data were drawn independently, Theorem 4.1 would give us the required control of the random walk (2.7). The right hand side of (4.5) consists of a _mixed tail_: (1) a sub-Gaussian term with a CLT-like weak variance term \(\); and (2) a polynomial term accounting for the fact that we only imposed the existence of \(s\) moments. With the preliminary results Corollary 4.1 and Theorem 4.1 in place, we are now in position to control dependent random walks of the form (2.7).

**Theorem 4.2**.: _Fix \(s>2\), a separable normed space \((,\|\|)\), constants \(,>0\), and set \(C_{,,s}=(1+(2s/e)^{2s}(2(1+2/)(3+4/))^{2}+ ^{-s})\). Fix also a consecutive partition \(a_{1:2m}\) of \([n]\) and let \(Z_{1:n}\) be a mean zero, \(\)-mixing process taking values in \(\) with block decoupled version \(_{1:n}\). Let \(O\) be the union of the odd \(a_{i}\) and \(E\) be the union of the even \(a_{i}\). Assume that \(\|Z_{i}\|^{s}<\) for \(i[n]\). For every \(,>0\) and \((0,1)\), we have that:_

\[(\|_{i=1}^{n}Z_{i}\| _{\{O,E\}}}}{||}}((1+2)+(1+9)))\\  2+_{i=2}^{2m-1}_{Z}(a_{i})+(1+9)^{s}}{r^{s/2}^{s}}_{ \{O,E\}}_{i[2m]:a_{i}}\| _{j a_{i}}Z_{j}\|^{s}}{||^{s/2}_{}^{s/2}},\] (4.6)

_where \(_{}_{v^{*}}|}_{a_{i}}v^{2}(_{j a_ {i}}Z_{j})\) for \(\{O,E\}\), \(^{*}\) is unit disk in the dual space of \((,\|\|)\), and \(_{\{O,E\}}\||}}_{i}_{i}\|}{}}}\)._In Theorem 4.2 we have combined the blocking technique with the Fuk-Nagaev inequality (4.5). The right hand side of (4.6) is exactly as in (4.5) but instantiated to our setting and with extra additive mixing-related term.

### The Lower Tail of the Empirical Covariance Matrix

We now proceed to analyze the lower tail of the empirical covariance matrix (2.9).

**Theorem 4.3**.: _Fix \(>0\) and a consecutive partition \(a_{1:2m}\) of \([n]\). Let \(X_{1:n}\) be a sequence of \(\)-mixing random variables taking values in \(^{d_{}}\) with finite fourth moment. Assume that there exists a positive number \(\) such that for every \(v_{X}\) and \(i[n]\), we have \( v,X_{i}^{4}^{2} v,[X_ {i}X_{i}^{}]v\). There exists a positive universal constant \(C\) such that as long as_

\[n C_{j[2m]}|a_{j}|(d_{}+^{2}(1/)) _{i=2}^{2m-1}_{X}(a_{i}),\] (4.7)

_then_

\[( v^{d_{}}\;:\;_ {i=1}^{n} v,X_{i}^{2}_{i=1}^{n}  v,X_{i}^{2}) 1-.\] (4.8)

It is by now a well-established fact that lower uniform laws of the form (4.8) hold under mild assumptions for various function classes (linear functions on \(^{d_{}}\) in this case). Since these assumptions are quite mild and only affect burn-in conditions, deflating the sample-size via blocking does not deflate the final convergence rate. The particular approach we have chosen here to establish Theorem 4.3 is to combine blocking with the approach found in (Wainwright, 2019, Theorem 14.12). We remark that similar statements hold if one instead blocks the arguments of say Oliveira (2016) or Koltchinskii and Mendelson (2015).

## 5 Summary

The leading order term of our main result, Theorem 3.1, does not directly depend on any mixing-time type quantities. It mimics the asymptotic rate and scales solely in terms of the second order statistics of the process at hand. To arrive at this result, we rely on two facts:

* The lower tail of the empirical covariance matrix (2.9) is well-behaved under mild assumptions. In an excess risk bound, the contribution of the lower uniform law to the overall error is not of leading order. Hence, incurring a sample size deflation for this purpose is not critical.
* By combining blocking with a version of Bernstein's inequality, we are able to push the effect of blocking to only affect the large deviations regime. In the moderate and small deviations regimes, control of the leading order of the random walk in (2.7) is not directly impacted by slow mixing.