# Continuous-Time Functional Diffusion Processes

Giulio Franzese

EURECOM, France

&Giulio Corallo

EURECOM, France

&Simone Rossi

Stellantis, France

&Markus Heinonen

Aalto University, Finland

&Maurizio Filippone

EURECOM, France

&Pietro Michiardi

EURECOM, France

This work was done while working at EURECOM

###### Abstract

We introduce Functional Diffusion Processes (FDPs), which generalize score-based diffusion models to infinite-dimensional function spaces. FDPs require a new mathematical framework to describe the forward and backward dynamics, and several extensions to derive practical training objectives. These include infinite-dimensional versions of Girsanov theorem, in order to be able to compute an ELBO, and of the sampling theorem, in order to guarantee that functional evaluations in a countable set of points are equivalent to infinite-dimensional functions. We use FDPs to build a new breed of generative models in function spaces, which do not require specialized network architectures, and that can work with any kind of continuous data. Our results on real data show that FDPs achieve high-quality image generation, using a simple MLP architecture with orders of magnitude fewer parameters than existing diffusion models. Code available here.

## 1 Introduction

Diffusion models have recently gained a lot of attention both from academia and industry. The seminal work on denoising diffusion (Sohl-Dickstein et al., 2015) has spurred interest in the understanding of such models from several perspectives, ranging from denoising autoencoders (Vincent, 2011) with multiple noise levels (Ho et al., 2020), variational interpretations (Kingma et al., 2021), annealed (Song and Ermon, 2019) and continuous-time score matching (Song and Ermon, 2020; Song et al., 2021). Several recent extensions of the theory underpinning diffusion models tackle alternatives to Gaussian noise (Bansal et al., 2022; Rissanen et al., 2022), second order dynamics (Dockhorn et al., 2022), and improved training and sampling (Xiao et al., 2022; Kim et al., 2022; Franzese et al., 2022).

Diffusion models have rapidly become the go-to approach for generative modeling, surpassing gans(Dhariwal and Nichol, 2021) for image generation, and have recently been applied to various modalities such as audio (Kong et al., 2021; Liu et al., 2022), video (Ho et al., 2022; He et al., 2022), molecular structures and general 3D shapes (Trippe et al., 2022; Hoogeboom et al., 2022; Luo and Hu, 2021; Zeng et al., 2022). Recently, the generation of diverse and realistic data modalities (images, videos, sound) from open-ended text prompts (Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022) has projected practitioners into a whole new paradigm for content creation.

A common trait of diffusion models is the need to understand their design space (Karras et al., 2022), and tailor the inner working parts to the chosen application and data domain. Diffusion models require specialization, ranging from architectural choices of neural networks used to approximate the score (Dhariwal and Nichol, 2021; Karras et al., 2022), to fine-grained details such as an appropriate definition of a noise schedule (Dhariwal and Nichol, 2021; Salimans and Ho, 2022), and mechanisms to deal with resolution and scale (Ho et al., 2021). Clearly, the data domain impacts profoundly such design choices. As a consequence, a growing body of work has focused on the projection of datamodalities into a latent space (Rombach et al., 2022), either by using auxiliary models such as a Vaes(Vahdat et al., 2021), or by using a functional data representation (Dupont et al., 2022). These approaches lead to increased efficiency, because they operate on smaller dimensional spaces, and constitute a step toward broadening the applicability of diffusion models to general data.

The idea of modelling data with continuous functions has several advantages (Dupont et al., 2022): it allows working with data at arbitrary resolutions, it enjoys improved memory-efficiency, and it allows simple architectures to represent a variety of data modalities. However, a theoretically grounded understanding of how diffusion models can operate directly on continuous functions has been elusive so far. Preliminary studies apply established diffusion algorithms on a discretization of functional data by conditioning on point-wise values (Dutordoir et al., 2022; Zhuang et al., 2023). A line of work that is closely related to ours include approaches such as Kerrigan et al. (2022), who consider a Gaussian noise corruption process in Hilbert space and derive a loss function formulated on infinite-dimensional measures to approximate the conditional mean of the reverse process. Within this line of works, Mittal et al. (2022) consider diffusion of Gaussian processes. We are aware of other concurrent works that study diffusion process in Hilbert spaces (Lim et al., 2023; Pidstrigach et al., 2023; Hagemann et al., 2023). However, differently from us, these works do not formally prove that the score matching optimization is a proper evidence lower bound (ELBO), but simply propose it as an heuristic. None of these prior works discuss the limits of discretization, resulting in the failure of identifying which subset of functions can be reconstructed through sampling. Finally, the parametrization we present in our work merges how functions and score are approximated using a single, simple model.

The main goal of our work is to deepen our understanding of diffusion models in function space. We present a new mathematical framework to lift diffusion models from finite-dimensional inputs to function spaces, contributing to a general method for data represented by continuous functions.

In SS 2, we present Functional Diffusion Processs (Fdps), which generalize diffusion processes to infinite-dimensional functional spaces. We define forward (SS 2.1) and backward (SS 2.2) FDPs, and consider _generic_ functional perturbations, including noising and Laplacian blurring. Using an extension of Girsanov theorem, we derive in SS 2.3 an ELBO, which allows defining a parametric model to approximate the score of the functional density of Fdps. Given a Fdp and the associated ELBO, we are one-step closer to the definition of a loss function to learn the parametric score. However, our formulation still resides in an abstract, infinite-dimensional Hilbert space.

Then, for practical reasons, in SS 3, we specify for which subclass of functions we can perfectly reconstruct the original function given only its evaluation in a countable set of points. This is an extension of the sampling theorem, which we use to move from the infinite-dimensional domain of functions to a finite-dimensional domain of discrete mappings.

In SS 4, we discuss various options to implement such discrete mappings. In this work, we explore in particular the usage of implicit neural representations (INs) (Sitzmann et al., 2020) and Transformers Vaswani et al. (2017) to jointly model both the sampled version of infinite-dimensional functions, and the score network, which is central to the training of FDPs, and is required to simulate the backward process. Our training procedure, discussed in SS 5, involves approximate, finite-dimensional Stochastic Differential Equations (SDEs) for the forward and backward processes, as well as for the ELBO.

We complement our theory with a series of experiments to illustrate the viability of FDPs, in SS 6. In our experiments, the score network is a simple multilayer perceptron (MLP), with several orders of magnitude fewer parameters than any existing score-based diffusion model. To the best of our knowledge, we are the first to show that a functional-space diffusion model can generate realistic image data, beyond simple data-sets and toy models.

## 2 Functional Diffusion Processes (FDPs)

We begin by defining diffusion processes in Hilbert Spaces, which we call functional diffusion processes (FDPs). While the study of diffusion processes in Hilbert spaces is not new (Follmer and Wakolbinger, 1986; Millet et al., 1989; Da Prato and Zabczyk, 2014), our objectives depart from prior work, and call for an appropriate treatment of the intricacies of FDPs, when used in the context of generative modeling. In SS 2.1 we introduce a generic class of diffusion processes in Hilbert spaces. The key object is Equation (1), together with its associated path measure \(\) and the time varyingmeasure \(_{t}\), where \(_{0}\) represents the starting (data) measure. In SS 2.2 we derive the reverse FDP with the associated path-reversed measure \(}\), and in SS 2.3 we use an extension of Girsanov theorem for infinite-dimensional SDEs to compute the ELBO. The ELBO is a training objective involving a generalization of the score function (Song et al., 2021).

### The forward diffusion process

We consider \(H\) to be a real, separable Hilbert space with inner product \(,\), norm \(_{H}\), and countable orthonormal basis \(\{e^{k}\}_{k=1}^{}\). Let \(L(H)\) be the set of bounded linear operators on \(H\), \(B(H)\) be its Borel \(-\)algebra, \(B_{b}(H)\) be the set of bounded \(B(H)-\)measurable functions \(H\), and \(P(H)\) be the set of probability measures on \((H,B(H))\). Consider the following \(H\)-valued SDE:

\[X_{t}=(X_{t}+f(X_{t},t))\,t+ W_{t},\\ X_{0}_{0} P(H),\] (1)

where \(t[0,T]\), \(W_{t}\) is a \(R-\)Wiener process on \(H\) defined on the quadruplet \((,,(_{t})_{t 0},)\), and \(,\) are the sample space and canonical filtration, respectively. The domain of \(f\) is \((f) B([0,T] H)\), where \(f:D(f)[0,T] H H\) is a measurable map. The operator \(:D() H H\) is the infinitesimal generator of a \(C_{0}-\) semigroup \((t)\) in \(H\)\((t 0)\), and \(_{0}\) is a probability measure in \(H\). We consider \(\) to be \(C^{1}([0,T])\), that is the space of all continuous mappings \([0,T] H\), and \(X_{t}()=(t),\) to be the _canonical_ process. The requirements on the terms \(,f\) that ensure existence of solutions to Equation (1) depend on the type of noise -- _trace-class_ (\(\{R\}<\)) or _cylindrical_ (\(R=I\)) -- used in the FDP (Da Prato & Zabczyk (2014), _Hypothesis 7.1_ or _Hypothesis 7.2_ for trace-class and cylindrical noise, respectively).

The measure associated with Equation (1) is indicated with \(\). The _law_ induced at time \([0,T]\) by the canonical process on the measure \(\) is indicated with \(_{} P(H)\), where \(_{}(S)=(\{:X_{}() S\})\), and \(S\) is any element of \(\). Notice, that in infinite dimensional spaces there is not an equivalent of the Lebesgue measure to get densities from measures. In our case we consider however, when it exists, the single dimensional density \(_{}^{(d)}(x^{i}|x^{j i})\), defined implicitly through \(_{}(x^{i}|x^{j i})=_{}^{(d)}(x^{i}|x^{j i}) x^{i}\), being \(x^{i}\) the Lebesgue measure. To avoid cluttering the notation, in this work we simply shorten \(_{}^{(d)}(x^{i}|x^{j i})\) with \(_{}^{(d)}(x)\) whenever unambiguous. In Appendix B we provide additional details on the time-varying measure \(_{}(x)t\). Before proceeding, it is useful to notice that Equation (1) can also be expressed as an (infinite) system of stochastic differential equations in terms of \(X_{t}^{k}= X_{t},e^{k}\) as:

\[X_{t}^{k}=b^{k}(X_{t},t)t+W_{t}^{k}, k=1, ,,\] (2)

where we introduced the projection \(b^{k}(X_{t},t)=X_{t}+f(X_{t},t),e^{k}\). Moreover, \(W_{t}^{k}= dW_{t},e^{k}\) with covariance given by \([W_{t}^{k}W_{s}^{j}]=(k-j)r^{k}(s,t)\), \(\) in Kroenecker sense, and \(r^{k}\) is the projection on the base of \(R\).

### The reverse diffusion process

We now derive the reverse time dynamics for FDPs of the form defined in Equation (1). We require that the time reversal of the canonical process, \(_{t}=X_{T-t}\), is again a diffusion process, with distribution given by the **path-reversed** measure \(}()\), along with the reversed filtration \(}\). Note that the time reversal of an infinite dimensional process is more involved than for the finite dimensional case (Anderson, 1982; Follmer, 1985). There are two major approaches to guarantee the existence of the reverse diffusion process. The first approach (Follmer & Wakolbinger, 1986) is applicable only when \(R=I\) (the case of cylindrical Wiener processes) and it relies on a finite local entropy condition. The second approach, which is valid in the case of trace class noise \(\{R\}<\), is based on stochastic calculus of variations (Millet et al., 1989). The full technical analysis of the necessary assumptions for the two approaches is involved, and we postpone formal details to Appendix A.

**Theorem 1**.: _Consider Equation (1). If \(R=I\), suppose Assumption 1 in Appendix A.1 holds; else, (\(R I\)) suppose Assumption 5 amd Assumption 6 in Appendix A.2 hold. Then \(_{t}\), corresponding to the path measure \(}()\), has the following SDE representation:_\[_{t}=(-_{t}-f(_{t},T-t)+ RD_{x}_{T-t}^{(d)}(_{t}))t+_{t},\\ _{0}_{T},\] (3)

_where \(\) is a \(}\)\(R-\)Wiener process, and the notation \(D_{x}_{t}^{(d)}(x)\) stands for the mapping \(H H\) that, when projected, satisfies \( D_{x}_{t}^{(d)}(x),e^{k}=}_{t}^{(d)}(x^{k}\,|\,x^{i k})\)._

_By projecting onto the eigenbasis, we have an infinite system of SDEs:_

\[_{t}^{k}=(-b^{k}(_{t},T-t)+r^{k}}_{T-t}^{(d)}(_{t})) t+_{t}^{k}, k=1,,.\] (4)

The methodology proposed in this work requires to operate on proper Wiener processes, with \(\{R\}<\), which implies, intuitively, that the considered noise has finite variance. We now discuss a Corollary, in which Assumption 5 is replaced by stricter conditions, that we use to check the validity of the practical implementation of FDPs.

**Corollary 1**.: _Suppose Assumption 6 from Appendix A.2 holds. Assume that i) \(\{R\}=_{i}r^{i}<\), ii) \(b^{i}(x,t)=b^{i}x^{i}, i\), i.e. the drift term is linear and only depends on \(x\) through its projection onto the corresponding basis and iii) the drift is bounded, such that \( K>0:-K<b^{i}<0, i\). Then, the reverse process evolves according to Equation (4)._

Theorem 1 stipulates that, given some conditions, the reverse time dynamics for FDPs of the form defined in Equation (1) exist. Our analysis provides theoretical grounding to the observations in concurrent work (Lim et al., 2023) where, empirically, it is observed that the cylindrical class of noise is not suitable. We argue that, when \(R=I\), the difficulty stems from designing the coefficients \(b^{i}\) of the SDEs such that the forward (see requirement (5.3) in Da Prato & Zabczyk (2014)) as well as the backward processes Assumption 1 exist. The work by Bond-Taylor & Willcocks (2023) uses cylindrical (white) noise, but we are not aware of any theoretical justification, since the model architecture is only partially suited for the functional domain.

As an addendum, we note that the advantages of projecting the forward and backward processes on the eigenbasis of the Hilbert space \(H\), as in Equation (2) and Equation (4), become evident when discussing about the implementation of FDPs, specifically when we derive practical expressions for training and the simulation of the backward process, as discussed in SS 5, and in a fully expanded toy example in Appendix D.

### A Girsanov formula for the ELBO

Direct simulation of the backward fdp described by Equation (3) is not possible. Indeed, we have no access to the **true score** of the density \(_{r}^{(d)}\) induced at time \([0,T]\). To solve the problem, we introduce a **parametric score function**\(s_{}:H[0,T]^{m} H\). We consider the dynamics:

\[_{t}=(-_{t}-f(_{t },T-t)+Rs_{}(_{t},T-t))t+ _{t},\\ _{0}_{T} P(H),\] (5)

with path measure \(}^{_{T}}\), and \(_{t}\) being a \(}^{_{T}}\)\(R-\)Wiener process. To emphasize the connection between Equation (3) and Equation (5), we define initial conditions with the subscript \(T\), instead of 0. In principle, we should have \(_{T}=_{T}\), as it will be evident from the ELBO in Equation (8). However, \(_{T}\) has a simple and easy-to-sample-from distribution only for \(T\), which is not compatible with a realistic implementation. The analysis of the discrepancy when \(T\) is finite is outside of the scope of this work, and the interested reader can refer to Franzese et al. (2022) for an analysis on standard diffusion models. The final measure of the new process at time \(T\) is indicated by \(_{0}\), i.e. \(_{0}(S)=}^{_{T}}(\{:_{T}()  S\})\).

Next, we quantify the discrepancy between \(_{0}\) and the true data measure \(_{0}\) through an ELBO. Thanks to an extension of Girsanov theorem to infinite dimensional SDEs(Da Prato & Zabczyk, 2014), it is possible to relate the path measures (\(}\) and \(}^{_{T}}\), respectively) of the process \(_{t}\) induced by different drift terms in Equation (3) and different initial conditions.

Starting from the score function \(s_{}\), we define:

\[_{}(x,t)=R(s_{}(x,T-t)-D_{x}_{T-t}^{(d)} (x)).\] (6)

Under loose regularity assumptions (see Condition 2 in Appendix A.4) \(_{t}=_{t}-_{0}^{t}_{}(X_{s},s)ds\) is a \(}^{_{T}}\)\(R-\)Wiener process (Theorem 10.14 in Da Prato & Zabczyk (2014)), where Girsanov Theorem also tells us that the measure \(}^{_{T}}\) satisfies the Radon-Nikodym derivative:

\[}^{_{T}}}{}}= (_{0}^{T}_{}(_{t},t), _{t}_{R^{}H}-_{0}^{T}\| _{}(_{t},t)\|_{R^{}H}^{2}t ).\] (7)

By virtue of the disintegration theorem, \(}=}_{0}_{T}\) and similarly \(}^{_{T}}=}_{0}_{T}\), being \(}_{0},}_{0}\) the measures of the processes when considering a particular initial value. Then, \(}^{_{T}}\) satisfies \(}^{_{T}}=}^{ _{T}}\), for any measure \(_{T} P(H)\). Consequently, the canonical process \(_{t}\) has an SDE representation according to Equation (5), under the new path measure \(}^{_{T}}\). Then (see Appendix A.5 for the derivation) we obtain the elbo:

\[[_{0}_{0}]_{ }[_{0}^{T}\|_{}(X_{t},t)\| _{R^{}H}^{2}t]+[_{T} _{T}].\] (8)

Provided that the required assumptions in Theorem 1 are met, the validity of Equation (8) is general. Our goal, however, is to set the stage for a practical implementation of fdps, which calls for design choices that easily enable satisfying the required assumptions for the theory to hold. Then, for the remainder of the paper, we consider the particular case where \(f=0\) in Equation (1). This simplifies the dynamics as follows:

\[X_{t} =X_{t}t+W_{t}, X_{0}_ {0} P(H)\] (9) \[_{t} =(-_{t}+Rs_{}(_{t},T-t) )t+_{t},_{0}_{T} P(H)\] (10)

Since the only drift component in Equation (9) is the linear term \(\), the projection \(b^{j}\) will be linear as well. Such a design choice, although not necessary from a theoretical point of view, carries several advantages. The design of a drift term satisfying the conditions of Corollary 1 becomes straightforward, where such conditions naturally aligns with the requirements of the existence of the forward process (Chapter 5 of Da Prato & Zabczyk (2014)). Moreover, the forward process conditioned on given initial conditions admits known solutions, which means that simulation of SDE paths is cheap and straightforward, without the need for performing full numerical integration. Finally, it is possible to claim existence of the **true score function** and even provide its analytic expression (full derivation in Appendix A.7) as:

\[D_{x}_{t}^{(d)}(x)=-(t)^{-1}(x-(t) [X_{0}\,|\,X_{t}=x]),\] (11)

where \((t)=(_{s=0}^{t}((t-s))R((t-s) ^{})s)\). This last aspect is particularly useful when considering the conditional version of Equation (6), through \( D_{x}_{t}^{(d)}(x\,|\,x_{0}),e^{k}=}_{t}^{(d)}(x^{k}\,|\,x^{i k},x_{0})\), as:

\[_{}(x,x_{0},t)=R(s_{}(x,T-t)-D_{x} _{T-t}^{(d)}(x\,|\,x_{0})),\] (12)

where, similarly to the unconditional case, we have \(D_{x}_{t}^{(d)}(x\,|\,x_{0})=-(t)^{-1}(x-(t)x_{0})\). Then, Equation (12) can be used to rewrite Equation (8):

\[_{}[_{0}^{T}\|_{}(X_{ t},t)\|_{R^{}H}^{2}t]=_{} [_{0}^{T}\|_{}(X_{t},X_{0},t) \|_{R^{}H}^{2}t]+I,\] (13)

where \(I\) is a quantity independent of \(\). Knowledge of the conditional true score \(D_{x}_{t}^{(d)}(x\,|\,x_{0})\) and cheap simulation of the forward dynamics, allows for easier numerical optimization than the more general case of \(f 0\).

Sampling theorem for FDPs

The theory of FDPs developed so far is valid for real, separable Hilbert spaces. Our goal now is to specify for which subclass of functions it is possible to perfectly reconstruct the original function given only its evaluation in a countable set of points. We present a generalization of the sampling theorem (Shannon, 1949), which allows us to move from generic Hilbert spaces to a domain which is amenable to a practical implementation of FDPs, and their application to common functional representation of data such as images, data on manifolds, and more. We model these functions as objects belonging to the set of square integrable functions over \(C^{}\)_homogeneous_ manifolds \(M\) (such as \(^{N},^{N}\), etc...), i.e., the Hilbert space \(H=L_{2}(M)\). Then, exact reconstruction implies that all the relevant information about the considered functions is contained in the set of sampled points.

First, we define functions that are _band-limited_:

**Definition 1**.: _A function \(x\) in \(H=L_{2}(M)\) is a spectral entire function of exponential type \(\) (SE-\(\)) if \(|^{}x|^{k}|x|,k\). Informally, the "Fourier Transform" of \(x\) is contained in the interval \([0,]\)(Pesenson, 2000)._

Second, we define grids that cover the manifold with balls, without too much overlap. Those grids will be used to collect the function samples. Their formal definition is as follows:

**Definition 2**.: \(Y(r,)\) _denotes the set of all sets of points \(Z=\{p_{i}\}\) such that: i) \(_{j i}(p_{j},p_{i})>0\) and ii) balls \(B(p_{i},)\) form a cover of \(M\) with multiplicity \(<r\)._

Combining the two definitions, we can state the key result of this Section. As long as the sampled function is band-limited, if the samples grid is sufficiently fine, exact reconstruction is possible:

**Theorem 2**.: _For any set \(Z Y(r,)\), any SE-\(\) function \(x\) is uniquely determined by its set of values in \(Z\) (i.e. \(\{x[p_{i}]\}\)) as long as \(<d\), that is_

\[x=_{p_{i} Z}x[p_{i}]m_{p_{i}},\] (14)

_where \(m_{p_{i}}:M H\) are known polynomials2, and the notation \(x[p]\) indicates that the function \(x\) is evaluated at point \(p\)._

A precise definition of the value of the constant \(d\) and its interpretation is outside the goal of this work, and we refer the interested reader to Pesenson (2000) for additional details. For our purposes, it is sufficient to interpret the condition in Theorem 2 as a generalization of the classical Shannon-Nyquist sampling theorem (Shannon, 1949). Under this light, Theorem 2 has practical relevance, because it gives the conditions for which the sampled version of functions contains all the information of the original functions. Indeed, given the set of points \(p_{i}\) on which function \(x\) is evaluated, it is possible to reconstruct exactly \(x[p]\) for arbitrary \(p\).

**The uncertainty principle.** It is not always possible to define Hilbert spaces of square integrable functions that are simultaneously homogeneous and separable, for all the manifolds \(M\) of interest. In other words, it is difficult in practice to satisfy both the requirements for FDPs to exist, and for the sampling theorem to be valid (see an example in Appendix C). Nevertheless, it is possible to quantify the reconstruction error, and realize that practical applications of FDPs are viable. Indeed, given a compactly supported function \(x\), and a set of points \(Z\) with _finite_ cardinality, we can upper-bound the reconstruction error \(\|_{p_{i} Z}x[p_{i}]m_{p_{i}}-x\|_{H}\) with:

\[ Z}(x[p_{i}]-x^{}[p_{i}])m_{p_ {i}}\|_{H}}_{_{1}}+ Z}x^{}[p _{i}]m_{p_{i}}-x^{}\|_{H}}_{_{2}}+- x\|_{H}}_{_{3}}=,\] (15)

where \(x^{}\) is the SE-\(\) bandlimited version of \(x\), obtained by filtering out - in the frequency domain - any component larger than \(\). The error \(_{1}\) is due to \(x x^{}\). The term \(_{2}\) is the reconstruction error due to finiteness of \(|Z|\): the sampling theorem applies to \(x^{}\), but the corresponding sampling grid has infinite cardinality. Finally, the term \(_{3}\) quantifies the energy omitted by filtering out the frequency components of \(x^{}\) larger than \(\). This (loose) upper bound allows us to understand quantitatively the degree to which the sampling theorem does not apply for the cases of interest. Although deriving tighter bounds is possible, this is outside the scope of this work. What suffices is that in many practical cases, when functions are obtained from natural sources, it has been observed that functions are nearly time and bandwidth limited (Slepian, 1983). Consequently, as long as the sampling grid is sufficiently fine, the reconstruction error \(\) is negligible.

We now hold all the ingredients to formulate generative functional diffusion models using the Hilbert space formalism and _implement_ them using a finite grid of points, which is what we do next.

## 4 Score Network Architectural Implementations

We are now equipped with the ELBO (Equation (8)) and a score function \(s_{}\) that implements the mapping \(H[0,T]^{m} H\). We could then train the score by optimizing the ELBO and produce samples arbitrary close to the true data measure \(_{0}\). However, since the domain of the score function is the infinite-dimensional Hilbert space, such a mapping cannot be implemented in practice. Indeed, having access to samples of functions on finite grid of points is, in general, not sufficient. However, when the conditions for Theorem 2 hold, we can substitute - with no information loss - \(x H\) with its collection of samples \(\{x[p_{i}],p_{i}\}\). This allows considering score network architectures that receive as input a collection of points, and not _abstract_ functions. Such architectures should be flexible enough to work with an arbitrary number of input samples at arbitrary grid points, and produce as outputs functions in \(H\).

### Implicit Neural Representation

The first approach we consider in this work is based on the idea of Implicit Neural Representations (INRs) (Sitzmann et al., 2020). These architectures can receive as inputs functions sampled at arbitrary, possibly irregular points, and produce output functions evaluated at any desired point. Unfortunately, the encoding of the inputs is not as straightforward as in the Neural Fourier Operator (NFO) case, and some form of autoencoding is necessary. Note, however, that in traditional score-based diffusion models (Song et al., 2021), the parametric score function can be thought of as a denoising autoencoder. This is a valid interpretation also in our case, as it is evident by observing the term \([X_{0}\,|\,X_{t}=x]\) of the true score function in Equation (11). Since INRs are powerful denoisers (Kim et al., 2022), combined with their simple design and small number of parameters, in this Section we discuss how to implement the score network of FDPs using INRs.

We define a _valid_ INR as a parametric family (\(,t,\)) of functions in \(H\), i.e., mappings \(^{m}[0,T]^{m} H\). A valid INR is the central building block for the implementation of the parametric score function, and it relies on two sets of parameters: \(\), which are the parameters of the score function that we optimize according to Equation (8), and \(\), which serve the purpose of building a mapping from \(H\) into a finite dimensional space. More formally:

**Definition 3**.: _Given a manifold \(M\), a valid Implicit Neural Representation (INr) is an element of \(H\) defined by a family of parametric mappings \(n(,t,)\), with \(t[0,T],,^{m}\). That is, for \(p M\), we have \(n(,t,)[p]\). Moreover, we require \(n(,t,) L_{2}(M)\)._

A valid INR as defined in Definition 3 is not sufficiently flexible to implement the parametric score function \(s_{}\), as it cannot accept input elements from the infinite-dimensional Hilbert space \(H\): indeed, the score function is defined as a mapping over \(H[0,T]^{m} H\), whereas the valid INR is a mapping defined over \(^{m}[0,T]^{m} H\). Then, we use the second set of parameters \(\) to condensate all the information of a generic \(x H\) into a finite-dimensional vector. When the conditions for Theorem 2 hold, we can substitute -- with no information loss -- \(x H\) with its collection of samples \(\{x[p_{i}],p_{i}\}\). Then, we can construct an implicitly defined mapping \(g:H[0,T]^{m}^{m}\) as:

\[g(\{x[p_{i}],p_{i}\},t,)=_{}_ {p_{i}}(n(,t,)[p_{i}]-x[p_{i}])^ {2}.\] (16)

In this work, we consider the _modulation_ approach to INRs. The set of parameters \(\) are obtained by minimizing Equation (16) using few steps of gradient descent on the objective \(_{p_{i}}(n(,t,)[p_{i}]-x[p_{i}])^{2}\), starting from the zero initialization of \(\). This approach, also explored by Dupont et al. (2022b), is based on the concept of meta-learning (Finn et al., 2017). In summary, our method constructs mappings \(H[0,T]^{m} H\), where the same INR is used first to encode \(x\) into \(\), and subsequently to output the value functions for any desired input point \(p\), thus implementing the following score network:

\[s_{}(x,t)=-((t))^{-1}(x-(t)n(g(\{x[p _{i}],p_{i}\},t,),t,)).\] (17)

### Transformers

As an alternative approach, we consider implementing the score function \(s_{}\) using transformer architectures Vaswani et al. (2017), by interpreting them as mappings between Hilbert spaces (Cao, 2021). We briefly summarize here such a perspective, focusing on a single attention layer for simplicity, and adapt the notation used throughout the paper accordingly.

Consider the space \(L_{2}(M)\), with the usual collection of samples \(\{x[p_{i}],p_{i}\}\). As a first step, both the "_features_" \(\{x[p_{i}]\}\) and positions \(\{p_{i}\}\) are embedded into some higher dimensional space and summed together, to obtain a sequence of vectors \(\{y_{i}\}\). Then, three different (learnable) matrices \(^{(Q)},^{(K)},^{(V)}\) are used to construct the linear transformations of the vector sequence \(\{y_{i}\}\) as \(^{(Q)}=\{_{i}{}^{(Q)}=^{(Q)}y_{i}\},^{(K)}=\{_{i}{}^{(K)}=^{(K)}y_{i}\},^{(V)}=\{_{i}{}^{(V)}= ^{(V)}y_{i}\}\). Finally, the three matrices \(^{(Q,K,V)}\) are multiplied together, according to any variant of the attention mechanism. Indeed, different choices for the order of multiplication and normalization schemes in the products and in the matrices correspond to different attention layers Vaswani et al. (2017). In practical implementations, these operations can be repeated multiple times (multiple attention layers) and can be done in parallel according to multiple projection matrices (multiple heads).

The perspective explored in (Cao, 2021) is that it is possible to interpret the sequences \(}{}^{(Q,K,V)}\) as **learnable** basis functions in some underlying _latent_ Hilbert space, evaluated at the set of coordinates \(\{p_{i}\}\). Furthermore, depending on the type of attention mechanism selected, the operation can be interpreted as a different mapping between Hilbert spaces, such as Fredholm equations of the second-kind or Petrov-Galerkin-type projections (Cao (2021) Eqs. 9 and 14).

While a complete treatment of such an interpretation is outside the scope of this work, what suffices is that it is possible to claim that transformer architectures are a viable candidate for the implementation of the desired mapping \(H[0,T]^{m} H\), a possibility that we explore experimentally in this work. It is worth noticing that, compared to the approach based on INRs, resolution invariance is only _learned_, and not guaranteed, and that the number of parameters is generally higher compared to an INR. Nevertheless, learning the parameters of transformer architectures does not require meta-learning, which is a practical pain-point of INRs used in our context. Additional details for the transformer-based implementation of the score network are available in Appendix E.

Finally, for completeness, it is worth mentioning that a related class of architectures, the Neural Operators and NFos(Kovachki et al., 2021; Li et al., 2020), are also valid alternatives. However, such architectures require the input grid to be regularly spaced (Li et al., 2020), and their output function is available only at the same points \(p_{i}\) of the input, which would reduce the flexibility of FDPs.

## 5 Training and sampling of FDPs

Given the parametric score function \(s_{}\) from Equation (17), by simulating the reverse FDP, we generate samples whose statistical measure \(_{0}\) is close in KL sense to \(_{0}\). Next, we explain how to numerically compute of the quantities in Equation (13), which is part of the ELBO in Equation (8), and how to generate new samples from the trained FDP (simulation of Equation (10)).

**ELBO Computation.** Equation (8) involves Equation (13), which requires the computation of the Hilbert space norm. The grid of points \(x[p_{i}]\) is interpolated in \(H\) as \(_{i}x[p_{i}]^{i}\). Then, the norm of interest can be computed as:

\[\|_{i}x[p_{i}]^{i}\|_{R^{}H}^{2}= R^{- {1}{2}}_{i}x[p_{i}]^{i},R^{-}_{i}x[p_{i}]^{i}_ {H}=_{k=1}^{}(r^{k})^{-1}(_{i=1}^{N}x[p_{i}] ^{i},e^{k})^{2}.\] (18)Depending on the choice of \(^{i},e^{i}\), the sum w.r.t the index \(k\) is either naturally truncated or it needs to be further approximated by selecting a cutoff index value. Finally, training can then be performed by minimizing:

\[_{}[_{0}^{T}\|_{}(X_ {t},t)\|_{R^{}H}^{2}t]_{(20)} [_{0}^{T}_{k=1}^{}(r^{k})^{-1}( _{i=1}^{N}(_{}(_{i}X_{t}[p_{i}]^{i},t)[p_{ i}])^{i},e^{k})^{2}t].\] (19)

**Numerical integration.** Simulation of infinite dimensional SDEs is a well studied domain (Debussche, 2011), including finite difference schemes (Gyongy, 1998, 1999; Yoo, 2000), finite element methods and/or Galerkin schemes (Hausenblas, 2003a,b; Shardlow, 1999). In this work, we adopt a finite element approximate scheme, and introduce the _interpolation_ operator, from \(^{|Z|}\) to \(H\), i.e. \(_{i}x[p_{i}]^{i}\)(Hausenblas, 2003b). Notice that, in general, the functions \(^{i}\) differ from the basis \(e^{i}\). In addition, the _projection_ operator maps functions from \(H\) into \(^{L}\), as \( x,^{j},^{j} H\). Usually, \(L=|Z|\). When \(^{i}=^{i}\) the scheme is referred to as the Galerkin scheme. We consider instead a point matching scheme (Hausenblas, 2003b), in which \(^{i}=[p-p_{i}]\) with \(\) in Dirac sense, and consequently \( x,^{i}=x[p_{i}]\). Then, the infinite dimensional SDE of the forward process from Equation (9) is approximated by the finite (\(|Z|\)) dimensional SDE:

\[X_{t}[p_{k}]=(_{i}X_{t}[p_{i}]^ {i},^{k})t+W_{t}[p_{k}], k=1, ,|Z|.\] (20)

Similarly, the reverse process described by Equation (10) corresponds to the following SDE:

\[_{t}[p_{k}]=(- _{i}_{t}[p_{i}]^{i},^{k}+ Rs_{}(_{i}_{t}[p_{i}]^{i},T-t),^{k} )t+_{t}[p_{k}],\] (21) \[k=1,,|Z|.\]

Equation (21) is a finite dimensional SDE, and consequently we can use any known numerical integrator to simulate its paths. In Appendix D we provide a complete toy example to illustrate our approach in a simple scenario, where we emphasize practical choices.

## 6 Experiments

Despite a rather involved theoretical treatment, the implementation of fdps is simple. We implemented our approach in JaX (Bradbury et al., 2018), and use WanDB(Biewald, 2020) for our experimental protocol. Additional details on implementation, and experimental setup, as well as more experiments are available in Appendix E.

We evaluate our approach on image data, using the CELEBA\(64 64\)(Liu et al., 2015) dataset. Our comparative analysis with the state-of-the-art includes generative quality, using the FID score (Heusel et al., 2017), and parameter count for the score network. We also discuss (informally) the complexity of the network architecture, as a measure of the engineering effort in exploring the design space of the score network. We compare against vanilla Score Based Diffusion (SBD) (Song et al., 2021), From Data To Functa (FD2F) (Dupont et al., 2022) which diffuses latent variables obtained from an INR, Infinite Diffusion (\(\)-DIFF) (Bond-Taylor and Willcocks, 2023), which is a recent approach that is only partially suited for the functional domain, as it relies on the combination of Fourier Neural Operators and a classical convolutional U-net backbone. Our FDP method is implemented using either MLP or Transformers. In the first case, we consider a score network implemented as a simple MLP with 15 layers and 256 neurons in each layer. The activation function is a Gabor wavelet activation function (Saragadam et al., 2023). In the latter case, our approach is built upon the UViT backbone as detailed by Bao et al. (2022). The architecture comprises 7 layers, with each layer composed of a self-attention mechanism with 8 attention heads and a feedforward layer.

We present quantitative results in Table 1, showing that our method **FDP(MLP)** achieves an impressively low FID score, given the extremely low parameter count, and the simplicity of the architecture. fd2F obtains a worse (larger) FID score, while having many more parameters, due to the complex parametrization of their score network. As a reference we report the results of SBD, where the price to be pay to achieve an extremely low FID is to have many more parameters and a much more intricate architecture. Finally, the very recent \(\)-DIFF method, has low FID-CLIP score (Kynkaanniemi et al., 2022), but requires a very complex architecture and more than 2 orders of magnitude more parameters than our approach. Showcasing the flexibility of the proposed methodology, we consider a more complex architecture based on Vision Transformers (**FDP(UViT)**). These corresponding results indicate improvements in terms of image quality (FID score=11) and do not require meta-learning steps, but require more parameters (O(20M)) than the 1NR variant. To the best of our knowledge, none of related work in the purely functional domain (Lim et al., 2023; Hagemann et al., 2023; Dutordoir et al., 2022; Kerrigan et al., 2022) provides results going beyond simple data-sets. Finally, we present some qualitative results in Figures 2 and 2 clearly showing that the proposed methodology is capable of producing diverse and detailed images.

## 7 Conclusion, Limitations and Broader Impact

We presented a theoretical framework to define functional diffusion processes for generative modeling. FDPs generalize traditional score-based diffusion models to infinite-dimensional function spaces, and in this context we were the first to provide a full characterization of forward and backward dynamics, together with a formal derivation of an ELBO that allowed the estimation of the parametric score function driving the reverse dynamics.

To use FDPs in practice, we carefully studied for which subset of functions it was possible to operate on a countable set of samples without losing information. We then proceeded to introduce a series of methods to jointly model - using only a simple INR or a Transformer - an approximate functional representation of data on discrete grids, and an approximate score function. Additionally, we detailed practical training procedures of FDPs, and integration schemes to generate new samples.

The implementation of FDPs for generative modeling was simple. We validated the viability of FDPs through a series of experiments on real images, where we show, while only using a simple MLP for learning the score network, extremely promising results in terms of generation quality.

Like other works in the literature, the proposed method can have both positive (e.g., synthesizing new data automatically) and negative (e.g., deep fakes) impacts on society depending on the application.

## 8 Acknowledgments

GF gratefully acknowledges support from Huawei Paris and the European Commission (ADROIT6G Grant agreement ID: 101095363). MF gratefully acknowledges support from the AXA Research Fund and the Agence Nationale de la Recherche (grant ANR-18-CE46-0002 and ANR-19-P3IA-0002).

 
**Methods** & **FID (\(\))** & **FID-CLIP (\(\))** & **Params** \\  
**FDP(MLP)** & 35.00 & 12.44 & \(O\)(1 M) \\ 
**FDP(UViT)** & 11.00 & 6.55 & \(O\)(20 M) \\  FD2F & 40.40 & - & \(O\)(10 M) \\  SBD & 3.30 & - & \(O\)(100 M) \\  \(\)-DIFF & - & 4.57 & \(O\)(100 M) \\  

Table 1: Quantitative results, CELEBA data-set. (FID-CLIP (Kynkaanniemi et al., 2022))