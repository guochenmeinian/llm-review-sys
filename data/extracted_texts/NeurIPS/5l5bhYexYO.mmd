# Reinforcement Learning Gradients as Vitamin for Online Finetuning Decision Transformers

Kai Yan  Alexander G. Schwing  Yu-Xiong Wang

University of Illinois Urbana-Champaign

{kaiyan3, aschwing, yxw}@illinois.edu

https://github.com/KaiYan289/RL_as_Vitamin_for_Online_Decision_Transformers

###### Abstract

Decision Transformers have recently emerged as a new and compelling paradigm for offline Reinforcement Learning (RL), completing a trajectory in an autoregressive way. While improvements have been made to overcome initial shortcomings, online finetuning of decision transformers has been surprisingly under-explored. The widely adopted state-of-the-art Online Decision Transformer (ODT) still struggles when pretrained with low-reward offline data. In this paper, we theoretically analyze the online-finetuning of the decision transformer, showing that the commonly used Return-To-Go (RTG) that's far from the expected return hampers the online fine-tuning process. This problem, however, is well-addressed by the value function and advantage of standard RL algorithms. As suggested by our analysis, in our experiments, we hence find that simply adding TD3 gradients to the fine-tuning process of ODT effectively improves the online finetuning performance of ODT, especially if ODT is pretrained with low-reward offline data. These findings provide new directions to further improve decision transformers.

## 1 Introduction

While Reinforcement Learning (RL) has achieved great success in recent years , it is known to struggle with several shortcomings, including training instability when propagating a Temporal Difference (TD) error along long trajectories , low data efficiency when training from scratch , and limited benefits from more modern neural network architectures . The latter point differs significantly from other parts of the machine learning community such as Computer Vision  and Natural Language Processing .

To address these issues, Decision Transformers (DTs)  have been proposed as an emerging paradigm for RL, introducing more modern transformer architectures into the literature rather than the still widely used Multi-Layer Perceptrons (MLPs). Instead of evaluating state and state-action pairs, a DT considers the whole trajectory as a sequence to complete, and trains on offline data in a supervised, auto-regressive way. Upon inception, DTs have been improved in various ways, mostly dealing with architecture changes , the token to predict other than return-to-go , addressing the problem of being overly optimistic , and the inability to stitch together trajectories . Significant and encouraging improvements have been reported on those aspects.

However, one fundamental issue has been largely overlooked by the community: _offline-to-online RL using decision transformers_, i.e., finetuning of decision transformers with online interactions. Offline-to-online RL  is a widely studied sub-field of RL, which combines offline RL learning from given, fixed trajectory data and online RL data from interactions with the environment. By first training on offline data and then finetuning, the agent can learn a policy with much greater data efficiency, while calibrating the out-of-distribution error from the offline dataset. Unsurprisingly, this sub-field has become popular in recent years.

While there are numerous works in the offline-to-online RL sub-field [35; 28; 62], surprisingly few works have discussed the offline-to-online finetuning ability of decision transformers. While there is work that discusses finetuning of decision transformers predicting encoded future trajectory information , and work that finetunes pretrained decision transformers with PPO in multi-agent RL , the current widely adopted state-of-the-art is the Online Decision Transformer (ODT) : the decision transformer training is continued on online data following the same supervised-learning paradigm as in offline RL. However, this method struggles with low-reward data, as well as with reaching expert-level performance due to suboptimal trajectories  (also see Sec. 4).

To address this issue and enhance online finetuning of decision transformers, we theoretically analyze the decision transformer based on recent results , showing that the commonly used conditioning on a high Return-To-Go (RTG) that's far from the expected return hampers results. To fix, we explore the possibility of using _tried-and-true RL gradients_. Testing on multiple environments, we find that simply combining TD3  gradients with the original auto-regressive ODT training paradigm is surprisingly effective: it improves results of ODT, especially if ODT is pretrained with low-reward offline data.

Our contributions are summarized as follows:

1) We propose a simple yet effective method to boost the performance of online finetuning of decision transformers, especially if offline data is of medium-to-low quality;

2) We theoretically analyze the online decision transformer, explain its "policy update" mechanism when using the commonly applied high target RTG, and point out its struggle to work well with online finetuning;

3) We conduct experiments on multiple environments, and find that ODT aided by TD3 gradients (and sometimes even the TD3 gradient alone) are surprisingly effective for online finetuning of decision transformers.

## 2 Preliminaries

**Markov Decision Process.** A Markov Decision Process (MDP) is the basic framework of sequential decision-making. An MDP is characterized by five components: the state space \(S\), the action space \(A\), the transition function \(p\), the reward \(r\), and either the discount factor \(\) or horizon \(H\). MDPs involve an _agent_ making decisions in discrete steps \(t\{0,1,2,\}\). On step \(t\), the agent receives the current state \(s_{t} S\), and samples an action \(a_{t} A\) according to its _stochastic_ policy \((a_{t}|s_{t})(A)\), where \((A)\) is the probability simplex over \(A\), or its _deterministic_ policy \((s_{t}) A\). Executing the action yields a reward \(r(s_{t},a_{t})\), and leads to the evolution of the MDP to a new state \(s_{t+1}\), governed by the MDP's transition function \(p(s_{t+1}|s_{t},a_{t})\). The goal of the agent is to maximize the total reward \(_{t}^{t}r(s_{t},a_{t})\), discounted by the discount factor \(\) for infinite steps, or \(_{t=1}^{H}r(s_{t},a_{t})\) for finite steps. When the agent ends a complete run, it finishes an _episode_, and the state(-action) data collected during the run is referred to as a _trajectory_\(\).

**Offline and Online RL.** Based on the source of learning data, RL can be roughly categorized into offline and online RL. The former learns from a given finite dataset of state-action-reward trajectories, while the latter learns from trajectories collected online from the environment. The effort of combining the two is called _offline-to-online_ RL, which first pre-trains a policy using offline data, and then continues to finetune the policy using online data with higher efficiency. Our work falls into the category of offline-to-online RL. We focus on improving the decision transformers, instead of Q-learning-based methods which are commonly used in offline-to-online RL.

**Decision Transformer (DT).** The decision transformer represents a new paradigm of offline RL, going beyond a TD-error framework. It views a trajectory \(\) as a sequence to be auto-regressively completed. The sequence interleaves three types of tokens: **returns-to-go (RTG, the target total return)**, states, and actions. At step \(t\), the past sequence of context length \(K\) is given as the input, i.e., the input is \((_{t-K},s_{t-k},a_{t-k},,_{t},s_{t})\), and an action is predicted by the auto-regressive model, which is usually implemented with a GPT-like architecture . The model is trained via supervised learning, considering the past \(K\) steps of the trajectory along with the current state and the current return-to-go as the feature, and the sequence of all actions \(a\) in a segment as the labels. At evaluation time, a **desired return RTGeval** is specified, since the **ground truth future return RTGreal** isn't known in advance.

**Online Decision Transformer (ODT).**ODT has two stages: offline pre-training which is identical to classic DT training, and online finetuning where trajectories are iteratively collected and the policy is updated via supervised learning. Specifically, the action \(a_{t}\) at step \(t\) during rollouts is computed by the deterministic policy \(^{}(s_{t-T:t},a_{t-T:t-1},_{t-T:t},T=T_{}, =_{})\),1 or sampled from the stochastic policy \(^{}(a_{t}|s_{t-T:t},a_{t-T:t-1},_{t-T:t},T=T_{},=_{})\). Here, \(T\) is the context length (which is \(T_{}\) in evaluation), and \(_{}\) is the target return-to-go. The data buffer, initialized with offline data, is gradually replaced by online data during finetuning.

When updating the policy, the following loss (we use the deterministic policy as an example, and thus omit the entropy regularizer) is minimized:

\[_{t=1}^{T_{}}\|^{}(s_{0:t},a_{0:t-1}, _{0:t},=_{},T=t)-a_{t}\| _{2}^{2}.\] (1)

Note, \(T_{}\) is the training context length and \(_{}\) is the real return-to-go. For better readability, we denote \(\{s_{x+1},s_{x+2},,s_{y}\}\), \(x,y\) as \(s_{x:y}\) (i.e., left _exclusive_ and right _inclusive_), and similarly \(\{a_{x+1},a_{x+2},,a_{y}\}\) as \(a_{x:y}\) and \(\{_{x+1},,_{y}\}\) as \(_{x:y}\). Specially, index \(x=y\) represents an empty sequence. For example, when \(t=1\), \(a_{0:0}\) is an empty action sequence as the decision transformer is not conditioned on any past action.

One important observation: the decision transformer is inherently _off-policy_ (the exact policy distribution varies with the sampled starting point, context length and return-to-go), which effectively guides our choice of RL gradients to off-policy algorithms (see Appendix C for more details).

**TD3.** Twin Delayed Deep Deterministic Policy Gradient (TD3)  is a state-of-the-art online off-policy RL algorithm that learns a _deterministic_ policy \(a=^{}(s)\). It is an improved version of an actor-critic (DDPG ) with three adjustments to improve its stability: 1) _Clipped double Q-learning_, which maintains two critics (estimators for expected return) \(Q_{_{1}},Q_{_{2}}:|S||A|\) and uses the smaller of the two values (i.e., \((Q_{_{1}},Q_{_{2}})\)) to form the target for TD-error minimization. Such design prevents overestimation of the \(Q\)-value; 2) _Policy smoothing_, which adds noise when calculating the \(Q\)-value for the next action to effectively prevent overfitting; and 3) _Delayed update_, which updates \(^{}\) less frequently than \(Q_{_{1}},Q_{_{2}}\) to benefit from a better \(Q\)-value landscape when updating the actor. TD3 also maintains a set of _target networks_ storing old parameters of the actor and critics that are soft-updated with slow exponential moving average updates from the current, active network. In this paper, we adapt this algorithm to fit the decision transformer architecture so that it can be used as an auxiliary objective in an online finetuning process.

## 3 Method

This section is organized as follows: we will first provide intuition why RL gradients aid online finetuning of decision transformers (Sec. 3.1), and present our method of adding TD3 gradients (Sec. 3.2). To further justify our intuition, we provide a theoretical analysis on how ODT fails to improve during online finetuning when pre-trained with low-reward data (Sec. 3.3).

### Why RL Gradients?

In order to understand why RL gradients aid online finetuning of decision transformers, let us consider an MDP which only has a single state \(s_{0}\), one step, a one dimensional action \(a[-1,1]\) (i.e., a bandit with continuous action space) and a simple reward function \(r(a)=(a+1)^{2}\) if \(a 0\) and \(r(a)=1-2a\) otherwise, as illustrated in Fig. 1. In this case, a trajectory can be represented effectively by a scalar, which is the action. If the offline dataset for pretraining is of low quality, i.e., all actions in the dataset are either close to \(-1\) or \(1\), then the decision transformer will obviously not generate trajectories with high RTG after offline training. As a consequence, during online finetuning, the new rollout trajectory is very likely to be uninformative about how to reach \(_{}\), since it is too far from \(_{}\). Worse still, it cannot improve locally either, which requires \(}{ a}\). However, the decision transformer _yields exactly the inverse_, i.e., \(}\). Since the transformer is not invertible (and even if the transformer is invertible, often the ground truth \((a)\) itself is not), we cannoteasily estimate the former from the latter. Thus, the hope for policy improvement relies heavily on the generalization of RTG, i.e., policy yielded by high RTG\({}_{}\) indeed leads to better policy without any data as evidence, which is not the case with our constructed MDP and dataset.

In contrast, applying traditional RL for continuous action spaces to this setting, we either learn a value function \(Q(s_{0},a):\), which effectively gives us a direction of action improvement \(,a)}{ a_{i}}\) (e.g., SAC , DDPG , TD3 ), or an advantage \(A(s_{0},a)\) that highlights whether focusing on action \(a\) improves or worsens the policy (e.g., AWR , AWAC , IQL ). Either way provides a direction which suggests how to change the action locally in order to improve the (estimated) return. In our experiment illustrated in Fig. 2 (see Appendix F for details), we found that RL algorithms like DDPG  can easily solve the aforementioned MDP while ODT fails.

Thus, adding RL gradients aids the decision transformer to improve from given low RTG trajectories. While one may argue that the self-supervised training paradigm of ODT  can do the same by "prompting" the decision transformer to generate a high RTG trajectory, such paradigm is still unable to effectively improve the policy of the decision transformer pretrained on data with low RTGs. We provide a theoretical analysis for this in Sec. 3.3. In addition, we also explore the possibility of fixing this problem using other existing algorithms, such as JSRL  and slowly growing RTG (i.e., curriculum learning). However, we found that those algorithms cannot address this problem well. See Appendix G.8 for ablations.

Figure 1: An overview of our work, illustrating why ODT fails to improve with low-return offline data and RL gradients such as TD3 could help. The decision transformer yields gradient \(}\), but local policy improvement requires the opposite, i.e., \(\). Therefore, the agent cannot recover if the current policy conditioning on high target RTG does not actually lead to high real RTG, which is very likely when the target RTG is too far from the pretrained policy and out-of-distribution. By adding a small coefficient for RL gradients, the agents can improve locally, which leads to better performance.

Figure 2: An illustration of a simple MDP, showing how RL can infer the direction for improvement, while online DT fails. Panels (a) and (b) show, DDPG and ODT+DDPG manage to maximize reward and find the correct optimal action quickly, while ODT fails to do so. Panel (c) shows how a DDPG/ODT+DDPG critic (from light blue/orange to dark blue/red) manages to fit ground truth reward (green curve). Panel (d) shows that the ODT policy (changing from light gray to dark) fails to discover the hidden reward peak near \(0\) between two low-reward areas (near \(-1\) and \(1\) respectively) contained in the offline data. Meanwhile, ODT+DDPG succeeds in finding the reward peak.

[MISSING_PAGE_EMPTY:5]

in Sec. 3.1, in this section, we will analyze more formally why such a paradigm is unable to improve the policy given offline data filled with low-RTG trajectories.

Our analysis is based on the performance bound proved by Brandfonbrener et al. . Given a dataset drawn from an underlying policy \(\) and given its RTG distribution \(P_{}\) (either continuous or discrete), under assumptions (see Appendix E), we have the following _tight_ performance bound for a decision transformer with policy \(^{}(a|s,_{})\) conditioned on \(_{}\):

\[_{}-_{=(s_{1},a_{1},,s_{H},_{ H})^{}(a|s,_{})}[_{}] (}+2)H^{2}.\] (4)

Here, \(_{f}=_{s_{1}}P_{}(_{}=_{}|s_{1})\) for every initial state \(s_{1}\), \(>0\) is a constant, \(H\) is the horizon of the MDP.2 Based on this tight performance bound, we will show that **with high probability, \(}\) grows _superlinearly_ with respect to \(_{}\)**. If true, then the \(_{}\) term (i.e., the actual return from online rollouts) must decrease to fit into the tight bound, as \(_{}\) grows.

To show this, we take a two-step approach: First, we prove that the probability mass of the RTG distribution is concentrated around low RTGs, i.e., _event probability_\(_{}(-_{}(|s) c|s)\) for \(c>0\) decreases superlinearly with respect to \(c\). For this, we apply the Chebyshev inequality, which yields a bound of \(O(})\). However, without knowledge on \(P_{}(|s)\), the variance can be made arbitrarily large by high RTG outliers, hence making the bound meaningless.

Fortunately, we have knowledge about the RTG distribution \(P_{}(|s)\) from the collected data. If we refer to the maximum RTG in the dataset via \(_{}\) and if we assume all rewards are non-negative, then all trajectory samples have an RTG in \([0,_{}]\). Thus, with adequate prior distribution, we can state that with high probability \(1-\), the probability mass is concentrated in the low RTG area. Based on this, we can prove the following lemma:

**Lemma 1**.: _(Informal) Assume rewards \(r(s,a)\) are bounded in \([0,R_{}]\),3 and \(_{}_{}\). Then with probability at least \(1-\), we have the probability of event \(_{}\) bounded as follows:_

\[_{}(_{}-V^{}(s) c|s) O (}^{2}T^{2}}{c^{2}}),\] (5)

_where \(\) depends on the number of trajectories in the dataset and prior distribution (see Appendix E for a concrete example and a more accurate bound). \(V^{}(s)\) is the value function of the underlying policy \((a|s)\) that generates the dataset, for which we have \(V^{}(s)=_{}(RTG|s)\)._

The second step uses the bound of probability mass \(_{}( c|s)\) to derive the bound for \(_{f}\). For the discrete case where the possibly obtained RTGs are finite or countably infinite (note, state and action space can still be continuous), this is simple, as we have

\[P_{}(=V^{}(s)+c|s)=_{}( =V^{}(s)+c|s)_{}( V^{}(s)+c|s ).\] (6)

Thus \(_{f}=_{s_{1}}P_{}(|s_{1})\) can be conveniently bounded by Lemma 1. For the continuous case, the proof is more involved as probability density \(P_{}(|s)\) can be very high on an extremely short interval of RTG, making the total probability mass arbitrarily small. However, assuming that \(P_{}(|s)\) is Lipschitz when \(_{}\) (i.e., RTG area not covered by dataset), combined with the discrete distribution case, we can still get the following (see Appendix E for proof):

**Corollary 1**.: _(Informal) If the RTG distribution is discrete (i.e., number of possible different RTGs are at most countably infinite), then with probability at least- \(1-\), \(}\) grows on the order of \((_{}^{2})\) with respect to \(_{}\). For continuous RTG distributions satisfying a Lipschitz continuous RTG density \(p_{}\), \(}\) grows on the order of \((_{}^{1.5})\)._

Here, \(()\) refers to the big-Omega notation (asymptotic lower bound).

## 4 Experiments

In this section, we aim to address the following questions: **a)** Does our proposed solution for decision transformers indeed improve its ability to cope with low-reward pretraining data. **b)** Is improving what to predict, while still using supervised learning, the correct way to improve the finetuning ability of decision transformers? **c)** Does the transformer architecture, combined with RL gradients, work better than TD3+BC? **d)** Is it better to combine the use of RL and supervised learning, or better to simply abandon the supervised loss in online finetuning? **e)** How does online decision transformer with TD3 gradient perform compared to other offline RL algorithms? **f)** How much does TD3 improve over DDPG which was used in Fig. 2?

**Baselines.** In this section, we mainly compare to six baselines: the widely recognized state-of-the-art DT for online finetuning, Online Decision Transformer (**ODT**) ; **PDT**, a baseline improving over ODT by predicting future trajectory information instead of return-to-go; **TD3+BC**, a MLP offline RL baseline; **TD3**, an ablated version of our proposed solution where we use TD3 gradients only for decision transformer finetuning (but only use supervised learning of the actor for offline pretraining); **IQL**, one of the most popular offline RL algorithms that can be used for online finetuning; **DDPG**+**ODT**, which is the same as our approach but with DDPG instead of TD3 gradients (for ablations using SAC , IQL , PPO , AWAC  and AWR , see Appendix C). Each of the baselines corresponds to one of the questions a), b), c), d), e) and f) above.

**Metrics.** We use the normalized average reward (same as D4RL's standard ) as the metric, where higher reward indicates better performance. If the final performance is similar, the algorithm with fewer online examples collected to reach that level of performance is better. We report the reward curve, which shows the change of the normalized reward's mean and standard deviation with \(5\) different seeds, with respect to the number of online examples collected. The maximum number of steps collected is capped at 500K (for mujoco) or 1M (for other environments). We also report evaluation results using the rliable  library in Fig. 7 of Appendix B.

**Experimental Setup.** We use the same architecture and hyperparameters such as learning rate (see Appendix F.2 for details) as ODT . The architecture is a transformer with \(4\) layers and \(4\) heads in each layer. This translates to around \(13\)M parameters in total. For the critic, we use Multi-Layer Perceptrons (MLPs) with width \(256\) and two hidden layers and ReLU  activation function. Specially, for the random dataset, we collect trajectories until the total number of steps exceeds \(1000\) in every epoch, which differs from ODT, where only \(1\) trajectory per epoch is collected. This is because many random environments, such as hopper, have very short episodes when the agent does not perform well, which could lead to overfitting if only a single trajectory is collected per epoch. For fairness, we use this modified rollout for ODT in our experiments as well. Not doing so does not affect ODT results since it does generally not work well on random datasets, but will significantly increase the time to reach a certain number of online transitions. After rollout, we train the actor for \(300\) gradient steps and the critic for \(600\) steps following TD3's delayed update trick.

### Adroit Environments

**Environment and Dataset Setup.** We test on four difficult robotic manipulation tasks , which are the Pen, Hammer, Door and Relocate environment. For each environment, we test three different datasets: expert, cloned and human, which are generated by a finetuned RL policy, an imitation learning policy and human demonstration respectively. See Appendix F.1 for details.

**Results.** Fig. 3 shows the performance of each method on Adroit before and after online finetuning. TD3+BC fails on almost all tasks and often diverges with extremely large \(Q\)-value during online finetuning. ODT and PDT perform better but still fall short of the proposed method, TD3+ODT. Note, IQL, TD3 and TD3+ODT all perform decently well (with similar average reward as shown in Tab. 2 in Appendix B). However, we found that TD3 often fails during online finetuning, probably because the environments are complicated and TD3 struggles to recover from a poor policy generated during online exploration (i.e., it has a _catastrophic forgetting_ issue). To see whether there is a simple fix, in Appendix G.7, we ablate whether an action regularizer pushing towards a pretrain policy similar to TD3+BC helps, but find it to hinder performance increase in other environments. IQL is overall much more stable than TD3, but improves much less during online finetuning than TD3+ODT. ODT can achieve good performance when pretrained on expert data, but struggles with datasets of lower quality, which validates our motivation. DDPG+ODT starts out well in the online finetuning stage but fails quickly, probably because DDPG is less stable compared to TD3.

### Antmaze Environments

**Environment and Dataset Setup.** We further test on a harder version of the Maze2D environment in D4RL  where the pointmass is substituted by a robotic ant. We study six different variants, which are umaze, umaze-diverse, medium-play, medium-diverse, large-play and large-diverse.

**Results.** Fig. 4 lists the results of each method on umaze and medium maze before and after online finetuning (see Appendix C for reward curves and Appendix B for results summary on large antmaze). TD3+ODT works the best on umaze and medium maze, and significantly outperforms TD3. This shows that RL gradients alone are not enough for offline-to-online RL of the decision transformer. Though TD3+ODT does not work on large maze, we found that IQL+ODT works decently well. However, we choose TD3+ODT in this work because IQL+ODT does not work well on the random datasets. This is probably because IQL aims to address the Out-Of-Distribution (OOD) estimation problem , which makes it better at utilizing offline data but worse at online exploration. See Appendix C for a detailed discussion and results. DDPG+ODT works worse than TD3+ODT but much better than baselines except IQL.

### MuJoCo Environments

**Environment and Dataset Setup.** We further test on four widely recognized standard environments , which are the Hopper, Halfcheetah, Walker2d and Ant environment. For each environment, we study three different datasets: medium, medium-replay, and random. The first and second one contain trajectories of decent quality, while the last one is generated with a random agent.

Figure 4: Reward curves for each method in Antmaze environments. IQL works best on the large maze, while our proposed method works the best on the medium maze and umaze. DDPG+ODT works worse than our method and IQL but much better than the rest of the baselines, which again validates our motivation that adding RL gradients to ODT is helpful.

Figure 3: Results on Adroit  environments. The proposed method, TD3+ODT, improves upon baselines. Note that TD3, IQL, and TD3+ODT all perform decently at the beginning of online finetuning, but TD3 fails while TD3+ODT improves much more than IQL during online finetuning.

**Results.** Fig. 6 shows the results of each method on MuJoCo before and after online finetuning. We observe that autoregressive-based algorithms, such as ODT and PDT, fail to improve the policy on MuJoCo environments, especially from low-reward pretraining with random datasets. With RL gradients, TD3+BC and IQL can improve the policy during online finetuning, but less than a decision transformer (TD3 and TD3+ODT). In particular, we found IQL to struggle on most random datasets, which are well-solved by decision transformers with TD3 gradients. TD3+ODT still outperforms TD3 with an average final reward of \(88.51\) vs. \(84.23\). See Fig. 6 in Appendix B for reward curves.

**Ablations on \(\).** Fig. 5 (a) shows the result of using different \(\) (i.e., RL coefficients) on different environments. We observe an increase of \(\) to improve the online finetuning process. However, if \(\) is too large, the algorithm may get unstable.

**Ablations on evaluation context length \(T_{}\).** Fig. 5 (b) shows the result of using different \(T_{}\) on halfcheetah-medium-replay-v2 and hammer-cloned-v1. The result shows that \(T_{}\) needs to be balanced between more information for decision-making and potential training instability due to a longer context length. As shown in the halfcheetah-medium-replay-v2 result, \(T_{}\) too long or too short can both lead to performance drops. More ablations are available in Appendix G.

## 5 Related Work

**Online Finetuning of Decision Transformers.** While there are many works on generalizing decision transformers (e.g., predicting waypoints , goal, or encoded future information instead of return-to-go [22; 5; 57; 36]), improving the architecture [37; 16; 53; 65] or addressing the overly-optimistic  or trajectory stitching issue ), there is surprisingly little work beyond online decision transformers that deals with online finetuning of decision transformers. There is some loosely related literature: MADT  proposes to finetune pretrained decision transformers with PPO. PDT  also studies online finetuning with the same training paradigm as ODT . QDT  uses an offline RL

   & TD3+BC & IQL & ODT & PDT & TD3 & DDPG-ODT & TD3+ODT (5urs) \\  HoM-v2 & 60.24(+4.4) & 47.42(-2.13) & **97.48(+48.90)** & 74.43(+22.21) & 89.89(+29.25) & 41.7(-13.18) & 89.07(+25.97) \\ HoM-v2 & **99.07(+33.33)** & 62.76(-7.63) & 83.29(+65.17) & 84.53(+82.23) & 93.72(+55.66) & 32.36(-9.9) & 95.65(+65.89) \\ HoR-v2 & 8.36(-0.35) & 20.42(+12.36) & 29.08(+26.92) & 35.95(+34.67) & 75.68(+73.69) & 25.12(+23.14) & **76.13(+74.15)** \\ HaM-v2 & 51.59(+27.3) & 37.12(-10.35) & 42.27(+19.23) & 93.35(+35.95) & 70.94(-25.95) & 55.69(+14.71) & **76.91(+35.35)** \\ HaM-v2 & 56.53(+13.07) & 49.97(+6.84) & 41.45(+26.76) & 31.47(+31.38) & 69.87(+45.09) & 53.71(+24.91) & **73.27(+43.98)** \\ HaR-v2 & 44.78(+31.12) & 47.85(+40.3) & 2.15(-0.09) & 0.74(+0.9) & **68.55(+66.3)** & 34.56(+32.31) & 59.35(+57.1) \\ WaM-v2 & 85.34(+34.9) & 65.55(-15.12) & 75.57(+18.47) & 63.37(+63.63) & 90.49(+24.74) & 2.01(+69.54) & **97.86(+27.08)** \\ WaM-v2 & 83.28(+0.00) & 95.90(+28.78) & 77.27(+14.56) & 54.04(+51.18) & **100.88(+85.24)** & 104.06(-5.99) & 10.06(+42.54) \\ WaR-v2 & 6.99(+5.86) & 10.67(+4.96) & 14.12(+9.82) & 15.47(+15.32) & **69.91(+66.31)** & 2.91(+4.27) & 57.86(+53.27) \\ An-M-v2 & 129.11(+7.11) & 10.36(+14.26) & 86.18(-0.51) & 52.08(+48.47) & 125.67(+37.55) & 10.81(+75.52) & **123.08(+41.22)** \\ An-M-v2 & 129.33(+41.03) & 113.16(+24.24) & 85.64(+4.49) & 36.92(+32.41) & **133.58(+51.77)** & 4.05(+87.7) & 10.32(+52.08) \\ An-R-v2 & 67.89(+33.47) & 12.28(+0.97) & 24.96(-6.44) & 14.88(+10.13) & 63.47(+32.20) & 4.93(-26.55) & **71.69(+40.31)** \\  Average & 68.52(+14.6) & 55.9(+7.8) & 55.14(+15.85) & 41.97(+0.44) & 87.64(+44.95) & 22.87(-19.22) & **88.38(+46.59)** \\  

Table 1: Average reward for each method in MuJoCo environments before and after online finetuning. The best performance for each environment is highlighted in bold font, and any result \(>90\%\) of the best performance is underlined. To save space, the name of the environments and datasets are abbreviated as follows: for the environments Ho=Hopper, Ha=HalfCheetah, Wa=Walker2d, An=Ant; for the datasets M=Medium, MR=Medium-Replay, R=Random. The format is “final(+increase after finetuning)”. The proposed solution performs well.

Figure 5: Panel (a) shows ablations on RL coefficient \(\). While higher \(\) aids exploration as shown in the halfcheetah-medium-replay-v2 case, it may sometimes introduce instability, which is shown in the hammer-human-v1 case. Panel (b) shows ablations on \(T_{}\). \(T_{}\) balances training stability and more information for decision-making.

algorithm to re-label returns-to-go for offline datasets. AFDT  and STG  use decision transformers offline to generate an auxiliary reward and aid the training of online RL algorithms. A few works study in-context learning [33; 34] and meta-learning [60; 30] of decision transformers, where improvements with evaluations on new tasks are made possible. However, none of the papers above focuses on addressing the general online finetuning issue of the decision transformer.

**Transformers as Backbone for RL.** Having witnessed the impressive success of transformers in Computer Vision (CV)  and Natural Language Processing (NLP) , numerous works also studied the impact of transformers in RL either as a model for the agent [45; 38] or as a world model [39; 50]. However, a large portion of state-of-the-art work in RL is still based on simple Multi-Layer Perceptrons (MLPs) [35; 28]. This is largely because transformers are significantly harder to train and require extra effort , making their ability to better memorize long trajectories  harder to realize compared to MLPs. Further, there are works on using transformers as feature extractors for a trajectory [37; 45] and works that leverage the common sense of transformer-based Large Language Model's for RL priors [10; 9; 70]. In contrast, our work focuses on improving the new "RL via Supervised learning" (RvS) [7; 18] paradigm, aiming to merge this paradigm with the benefits of classic RL training.

**Offline-to-Online RL.** Offline-to-online RL bridges the gap between offline RL, which heavily depends on the quality of existing data while struggling with out-of-distribution policies, and online RL, which requires many interactions and is of low data efficiency. Mainstream offline-to-online RL methods include teacher-student [51; 6; 59; 72] and out-of-distribution handling (regularization [21; 29; 62], avoidance [28; 23], ensembles [2; 15; 24]). There are also works on pessimistic Q-value initialization , confidence bounds , and a mixture of offline and online training [56; 73]. However, all the aforementioned works are based on Q-learning and don't consider decision transformers.

## 6 Conclusion

In this paper, we point out an under-explored problem in the Decision Transformer (DT) community, i.e., online finetuning. To address online finetuning with a decision transformer, we examine the current state-of-the-art, online decision transformer, and point out an issue with low-reward, sub-optimal pretraining. To address the issue, we propose to mix TD3 gradients with decision transformer training. This combination permits to achieve better results in multiple testbeds. Our work is a complement to the current DT literature, and calls out a new aspect of improving decision transformers.

**Limitations and Future Works.** While our work theoretically analyzes an ODT issue, the conclusion relies on several assumptions which we expect to remove in future work. Empirically, in this work we propose a simple solution orthogonal to existing efforts like architecture improvements and predicting future information rather than return-to-go. To explore other ideas that could further improve online finetuning of decision transformers, next steps include the study of other environments and other ways to incorporate RL gradients into decision transformers. Other possible avenues for future research include testing our solution on image-based environments, and decreasing the additional computational cost compared to ODT (an analysis for the current time cost is provided in Appendix H).