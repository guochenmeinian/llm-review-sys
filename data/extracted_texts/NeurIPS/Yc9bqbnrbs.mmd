# Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms

Qining Zhang

University of Michigan, Ann Arbor

qiningz@umich.edu

&Lei Ying

University of Michigan, Ann Arbor

leiying@umich.edu

###### Abstract

This paper considers a stochastic Multi-Armed Bandit (MAB) problem with dual objectives: (i) quick identification and commitment to the optimal arm, and (ii) reward maximization throughout a sequence of \(T\) consecutive rounds. Though each objective has been individually well-studied, i.e., best arm identification for (i) and regret minimization for (ii), the simultaneous realization of both objectives remains an open problem, despite its practical importance. This paper introduces _Regret Optimal Best Arm Identification_ (ROBAI) which aims to achieve these dual objectives. To solve ROBAI with both pre-determined stopping time and adaptive stopping time requirements, we present an algorithm called EOCP and its variants respectively, which not only achieve asymptotic optimal regret in both Gaussian and general bandits, but also commit to the optimal arm in \(( T)\) rounds with predetermined stopping time and \((^{2}T)\) rounds with adaptive stopping time. We further characterize lower bounds on the commitment time (equivalent to the sample complexity) of ROBAI, showing that EOCP and its variants are sample optimal with pre-determined stopping time, and almost sample optimal with adaptive stopping time. Numerical results confirm our theoretical analysis and reveal an interesting "over-exploration" phenomenon carried by classic UCB algorithms, such that EOCP has smaller regret even though it stops exploration much earlier than UCB, i.e., \(( T)\) versus \((T)\), which suggests over-exploration is unnecessary and potentially harmful to system performance.

## 1 Introduction

The stochastic Multi-Armed Bandit (MAB) problem , which models a wide range of applications including online recommendations [2; 3; 4], job assignments [5; 6; 7], clinical trials [8; 9], and etc, is a sequential decision-making process between an agent and an environment which consists of a number of arms (actions). In this paper, we will use "arm" and "action" interchangeably. Most existing studies, say [10; 11; 12; 13; 14; 15], formulate a regret minimization problem to study the bandit model, where the agent aims to maximize the cumulative reward through interacting with the environment for a number of consecutive rounds. The well-known UCB algorithm  and its variants  are among the most popular bandit algorithms for regret minimization, which exhibit outstanding performances both theoretically and empirically in bandit models. Moreover, state-of-the-art reinforcement learning algorithms, e.g., [16; 17], are also motivated by the essence of UCB which sets up confidence intervals to encourage exploration. However, the UCB algorithms do not commit to a single arm. Instead, they continue to switch among all arms based on reward signals received. Since one of the ultimate goals of the MAB model is to learn the optimal arm, it would be ideal if the algorithm would commit to an arm quickly without sacrificing the regret. In fact and in practice, a number of applications such as occupational decisions , medicine release and pandemic control [19; 20], and long-term investments , require or prefer quick commitment to an action instead of continuous exploration. This motivated us to consider an MAB problem with dualobjectives: (i) quick identification and commitment to the optimal arm, and (ii) minimization of the cumulative regret throughout a sequence of rounds.

**Regret Optimal Best Arm Identification:** The lack of commitment in traditional regret minimization formulation motivates us to propose a new viewpoint towards online decision-making called _Regret Optimal Best Arm Identification_ (ROBAI), which intends to manage two goals at the same time: minimizing regret while committing to an arm quickly. Specifically, it is ideal for the agent to quickly commit to the optimal arm which has the highest expected reward while minimizing the exploration regret. To solve ROBAI, we need to answer three fundamental questions: (1) how should the learner explore arms while maintaining a low-regret performance (**exploration strategy**)? (2) when should the learner stop exploration and commit to an arm (**stopping criterion**)? and (3) which arm to commit to when the exploration ends (**action selection strategy**)? All three components need to be designed together to make the algorithm most efficient. The fundamental question this paper addresses is: Can we design an efficient algorithm that is both regret optimal and identifies the optimal arm quickly, and what are the fundamental limits of such algorithms?

**Connection to Best Arm Identification:** One approach people may take to solve ROBAI is the Best Arm Identification (BAI) algorithm, which studies how to learn the optimal arm with the minimum number of samples (rounds) and then commit to the selected arm. However, since BAI focuses purely on sample complexity (or commitment time), the algorithms for BAI explore sub-optimal arms too aggressively and too often, leading to large regret. As shown in , the regret is asymptotically at least twice as large as the regret under the classic UCB algorithm. Modifications shown in  may lead to better regret performance, but they also make the algorithm too complicated to find the optimal arm quickly. Moreover, these algorithms adapted from BAI often exhibit poor empirical regret performances as shown in our numerical experiments of Fig. 1.

**Our Contributions:** We propose an algorithm called EOCP, which stands for _Explore Optimistically then Commit Pessimistically_, to solve the ROBAI problem. It first uses an optimistic modified version of the classic UCB algorithm to explore arms with a slightly larger exploration function, and then it commits to the optimal arm candidate according to a pessimistic LCB algorithm when the exploration ends, by selecting the arm that has the largest lower confidence bound. The exploration and action identification strategies are respectively motivated by the inflated bonus trick  and the principle of pessimism from the literature of offline bandits  and offline reinforcement learning . Our greatest contributions include designing new stopping rules with both pre-determined stopping time (vanilla EOCP) and adaptive stopping time (the EOCP-UG variant), which probably balance the trade-off between regret minimization and action identification. We theoretically

   Algorithm & Regret & Setting & Optimality & Commitment & Confidence \\  UCB  & \( T\) & Gaussian & Yes & \(T\) & N/A \\ KL-UCB  & \((_{2},_{1})} T\) & General & Yes & \(T\) & N/A \\ TS  & \((_{2},_{1})} T\) & General & Yes & \(T\) & N/A \\ BAI-ETC  & \( T\) & Gaussian & No & \(( T)\) & \(}(T^{-1})\) \\ DETC  & \( T\) & Gaussian & Yes & \((^{2}T)\) & \((T^{-1})\) \\ UCB\({}_{}\) & \(+o(1)}{} T\) & Gaussian & No & \(( T)\) & \(}(T^{-1})\) \\ EOCP (**Ours**) & \( T\) & Gaussian & Yes & \(( T)\) & \((T^{-1})\) \\ EOCP-UG (**Ours**) & \( T\) & Gaussian & Yes & \((^{2}T)\) & \((T^{-1})\) \\ KL-EOCP (**Ours**) & \((_{2},_{1})} T\) & General & Yes & \(( T)\) & \((T^{-1})\) \\ LB(pre-determined) & \( T\) & Gaussian & \((^{c}T)\) & \(( T)\) & \((T^{-1})\) \\ LB(adaptive) & \( T\) & Gaussian & \((^{c}T)\) & \((^{2-c}T)\) & \((T^{-1})\) \\   

Table 1: Caparison under \(2\)-armed bandits where \(=|_{1}-_{2}|\) is the expected reward difference, \((_{2},_{1})\) is the Kullback-Leibler divergence between reward distributions, and \(>1\). EOCP and KL-EOCP use a pre-determined stopping time and require the knowledge of \(\), while EOCP-UG uses an adaptive stopping time. Both LBs represent the commitment time lower bound under Gaussian bandits for regret optimal algorithms with \((^{c}T)\) finite-time regret violation and \((T^{-1})\) confidence.

show that both algorithms are asymptotically regret optimal in Gaussian bandit models. Moreover, EOCP and EOCP-UG algorithms commit to the optimal arm in \(( T)\) and \((^{2}T)\) number of rounds respectively, both with \((T^{-1})\) confidence. We further characterize the fundamental commitment time (sample complexity until commitment) limits of action identification for regret optimal algorithms, which shows that \(( T)\) number of samples is always required with pre-determined stopping time, and \((^{2-c}T)\) number of samples is required with adaptive stopping time if the finite-time regret of the algorithm does not exceed its asymptotic regret rate by \((^{c}T)\). This shows that EOCP is sample optimal and EOCP-UG is nearly sample optimal. We also propose an improved algorithm called KL-EOCP to achieve regret optimality in general bandits beyond Gaussian rewards. To the best of our knowledge, KL-EOCP is the first algorithm that not only achieves asymptotic regret optimality in general bandit models but also commits to the optimal arm in \(( T)\) rounds, which also matches the commitment time lower bound. The more detailed comparison between existing algorithms and our proposed algorithms with lower bounds is summarized in Tab. 1. Numerical experiments confirm the superiority of our proposed algorithm and show an interesting "over-exploration" phenomenon carried by UCB algorithms. As shown in Fig. 1, our EOCP algorithm reduces more than \(20\%\) of regret compared to the vanilla UCB algorithm by finding and committing to the optimal arm early.

## 2 Preliminaries

**Stochastic Multi-armed Bandits:** A stochastic multi-armed bandit problem is an online decision-making process between an agent and an environment for a number of \(T\) consecutive rounds. At each round \(t\{1,2,,T\}\), the agent can choose an action \(A_{t}\) among a set of actions (arms) with cardinality \(A\), denoted by \(=\{1,2,,A\}\), to interact with the environment. Each action \(a\) is associated with a probability distribution \(_{a}\) and we denote the set of distributions as \(=\{_{1},,_{A}\}\) with respective expectations \(=\{_{1},,_{A}\}\) which is unknown to the agent a priori. The expectations are assumed to be bounded so without loss of generality, we have \(_{a}\) for any action \(a\). After the agent chooses an action, say action \(A_{t}\) at round \(t\), it will observe an independent reward \(r_{t}\) which is sampled from the distribution \(_{A_{t}}\) associated with the action \(A_{t}\) that it chooses. We define the optimal action \(a^{*}\) to be the action which has the highest expected reward, i.e., \(_{a}*=_{a}}_{a}\), and for simplicity, we assume it is unique. Let \(_{a}=|_{a}*-_{a}|(0,1]\) to be the expected reward gap between the optimal action and a sub-optimal action \(a\), and we use \(_{}=_{a:_{a}>0}|_{a}*-_{a}|\) to denote the minimum reward gap among all sub-optimal actions.

**Regret:** The goal of the agent is to maximize the expected cumulative reward from the total \(T\) rounds of interactions with the environment, i.e., to maximize \([r_{1}+r_{2}++r_{T}]\), where the expectation is taken over all randomness. The performance of any bandit algorithm Alg chosen by the agent is usually measured by the cumulative _Regret_ up to round \(T\) defined as follows:

\[_{}^{}(T)=T_{a*}-_{ }[_{t=1}^{T}r_{t}],\]

where the subscript \(\) denotes the bandit instance represented by the reward expectations. Maximizing reward is equivalent to minimizing the cumulative regret. For most of the algorithms to achieve this goal, the agent will make action-choosing decisions based on two statistics maintained and updated at each round for every action: the empirical mean \(_{t}(a)\) and the number of pulls \(N_{t}(a)\) in previous rounds. They are defined as:

\[N_{t}(a)=_{k=1}^{t}_{A_{t}=a},_{t}(a)=(a)}_{k=1}^{t}r_{t}_{A_{t}=a}.\]

The theoretical regret limit of any algorithm Alg is studied and characterized in , which shows:

\[_{T}_{}^{}(T)}{ T}_{a:_{a}>0}}{(_{a}, _{a}*)},\] (1)

where \((,)\) denotes the Kullback-Leibler divergence between two distributions. We call an algorithm Alg regret optimal (asymptotically) if the asymptotic regret performance of Alg achieves this lower bound. Therefore, whether an algorithm is asymptotically regret optimal will depend on the distributions \(\) of the rewards. Specifically, for Gaussian bandits, the \(\) divergence between distributions \((_{a},1)\) and \((_{a^{}},1)\) is simply \((_{a}-_{a^{}})^{2}/2\). So the asymptotic regret rate lower bound in the RHS for Gaussian bandits would be \(2_{a:_{a}>0}_{a}^{-1}\).

**Commitment:** In ROBAI, commitment to a single action \(\) (ideally, the optimal action) is required. After a stopping time \(T_{}\), the agent will not be allowed to switch actions and will commit to the same action until the end. We consider two categories of commitment: the pre-determined stopping-time setting and the adaptive stopping time setting. The pre-determined stopping time requires \(T_{}\) to be pre-specified before the first round of interaction, while the adaptive stopping criterion requires \(T_{}\) to be a stopping time measurable to the natural filtration. How quickly the agent commits is measured by the _Sample Complexity until Commitment_ (also called commitment time) which is the expected number of exploration rounds, i.e., \(^{}_{}(T)=_{}[T_{}]\). The accuracy of identifying the optimal action is measured by the confidence, which is the probability that the agent commits to a sub-optimal action, i.e., \(_{}( a^{*})\). An ideal algorithm should minimize the commitment time while maintaining confidence lower than a pre-specified threshold.

**ROBAI Problem Formulation:** We use \(_{}\) to denote the class of regret optimal algorithms which commit to the optimal action with a confidence lower than \((T^{-1})\), i.e.,

\[_{}=\{|_{T}^{}_{}(T)}{ T}_{a:_{a}>0} }{(_{a},_{a}*)}_{}(  a^{*})=(T^{-1})\}.\]

ROBAI aims to design a regret optimal algorithm \(_{}\) to minimize the commitment time:

\[^{}_{}(T)=_{}[T_{ }],,_{}.\]

## 3 Low-Complexity Algorithms

In this section, we propose a low-complexity algorithm called EOCP with a pre-determined stopping time to solve ROBAI. Then, we propose a variant called EOCP-UG with adaptive stopping time.

### The Pre-determined Stopping Time Setting

The pre-determined stopping time setting is motivated by real-world applications such as A/B tests in medical experiments with operational or budget limits , where the number of testers is usually pre-designed and determined before the experiment starts. Therefore, we require the agent to pre-specify the stopping time \(T_{}\) before the first round. We also assume the algorithm knows a strictly positive lower bound \(_{}\) on the minimum reward gap \(_{}\) between the optimal action and sub-optimal actions. We propose our EOCP algorithm in Algorithm. 1.

```
0: Exploration function \(l\); Lower bound \(_{}\) on minimum reward gap.
1: Let \(T_{}=}{_{}^{2}}+A\) be the pre-determined stopping time.
2: Initialize by pulling each arm \(a\) once.
3:for\(t=A+1:T_{}\)do
4: Set uncertainty bonus \(b_{t-1}(a)=(a)}}\), and \(_{t-1}(a)=_{t-1}(a)+b_{t-1}(a)\).
5: Take action \(A_{t}=_{a}_{t-1}(a)\). // \(\) Exploration
6:endfor
7: Set bonus \(b_{T_{}}(a)=}}(a)}}\), and \(_{T_{}}(a)=_{T_{}}(a)-b_{T_{} }(a)\).
8: For \(t[T_{}+1,T]\), commit to action \(=_{a}_{T_{}}(a)\). // \(\) Commitment ```

**Algorithm 1** EOCP with Pre-determined Stopping Time

In EOCP, the agent will spend the first \(A\) rounds exploring each arm once as a start. Our choice of exploration strategy is a modified version of the classic UCB algorithm where the agent will choose the action with the largest upper confidence bound in terms of empirical reward. The exploration function \(l\) controls the intensity of exploration to achieve the optimal trade-off between reducing uncertainty for action identification and minimizing regret. After the pre-determined stopping time \(T_{}\), the agent will commit to an action that has the largest lower confidence bound of empirical reward. This LCB commitment strategy is inspired by the principle of pessimism from the literature of offline learning [25; 26; 27; 28; 29; 30], where the empirical reward of each action is penalized by the amount of uncertainty to combat the imbalanced data coverage of actions in the offline dataset. It is also shown that the pessimistic principle works well when the data coverage is concentrated on the optimal action, i.e., the optimal action has the largest number of pulls. This trait of the LCB algorithm matches the trait of UCB exploration, in which the optimal action will be chosen much more often than sub-optimal actions in exploration. So by designing such a proper \(T_{}\), we will be able to achieve the best of both worlds: a low-regret exploration of the UCB algorithm, and a fast best arm identification through the choice of LCB algorithm.

### The Adaptive Stopping Time Setting

In this setting, we do not assume the algorithm is provided a priori with additional information on the lower bound \(_{}\) on the minimum reward gap, so there is no hope of designing a pre-determined stopping time. Instead, we design our stopping criterion based on the samples collected from the explorations, which leads to the fact that \(T_{}\) is a stopping time measurable to the natural filtration. We propose our EOCP-UG algorithm in Algorithm 2 corresponding to an unknown gap.

```
0: Exploration function \(l\).
1: Initialize by pulling each arm \(a\) once.
2:while\(_{a}_{a^{}}N_{t-1}(a)-lN_{t-1}(a^{}) 1\)do
3: Set uncertainty bonus \(b_{t-1}(a)=(a)}}\), \(_{t-1}(a)=_{t-1}(a)+b_{t-1}(a)\).
4: Take action \(A_{t}=_{a}_{t-1}(a)\). // \(\) Exploration
5:endwhile
6: Let \(T_{} t-1\), bonus \(b_{T_{}}(a)=}}(a)}}\), and \(_{T_{}}(a)=_{T_{}}(a)-b_{T_{}}(a)\).
7: For \(t[T_{}+1,T]\), commit to the action \(=_{a}_{T_{}}(a)\). // LCB Commitment ```

**Algorithm 2** EOCP-UG with Adaptive Stopping Time

In EOCP-UG, we use the same UCB exploration and LCB best action identification strategies as in the pre-determined stopping time setting. The only difference compared to EOCP comes from the new stopping rule based on the number of pulls \(N_{t}(a)\) for each action. Specifically, the exploration ends if there is an imbalanced fraction of \(N_{t}(a)\) among all actions, that is, one action has \(l\) times more pulls than all other actions in previous rounds. Here, \(l\) is the exploration function and we will select \(l\) to be slightly larger than \( T\) in later sections. The intuition of such a stopping criterion comes from the characteristics of UCB exploration, i.e., as round \(t\) increases, the algorithm will slowly adapt to choosing the optimal action more often. When the fraction between the number of pulls for the optimal action and any sub-optimal action is large enough, the optimal action will be identifiable. Note that in action identification, we can simply choose the action that has the largest number of pulls \(N_{T_{}}(a)\) when we stop, and obtain exactly the same performance guarantees. However, to keep it consistent with the pre-determined stopping time setting, we use the LCB commitment.

## 4 Main Results

In this section, we assume the distributions \(\{_{1},,_{A}\}\) come from a Gaussian family, where \(_{a}\) associated with action \(a\) follows a Gaussian distribution with mean \(_{a}\) and unit variance, i.e., \(_{a}(_{a},1)\). The results in this section can be easily generalized to sub-Gaussian distributions. The key idea towards the optimal trade-off between controlling regret and identifying the optimal action is inspired by the inflated bonus trick from , where we will choose the exploration function \(l\) to be slightly larger than the \( T\) used in vanilla UCB algorithms to encourage more exploration of sub-optimal arms, i.e., we will select \(l= T+()\).

### Regret Optimality for Gaussian Bandits with Pre-Determined Stopping Time

In Theorem. 1, we present the theoretical regret performance guarantee of the EOCP algorithm:

**Theorem 1**: _Let \(l=(T)+()\) and when \(T\) is large enough, the expected regret of the EOCP algorithm in Algorithm. 1 with pre-determined stopping time can be upper-bounded by:_

\[_{}^{}(T)_{a:_{a}>0} }+(}{_{}} ).\]

A direct asymptotic bound can be obtained from Theorem. 1, i.e.,

\[_{T}_{}^{}(T)}{  T}_{a:_{a}>0}}.\]

Comparing it to Eq. (1), it is clear that EOCP is asymptotically regret optimal in Gaussian bandits. The commitment time and confidence guarantees can be extracted from the setup of Algorithm. 1 itself and the proof of Theorem. 1. Recall that in Algorithm. 1, we pre-determined the length of exploration \(T_{}\) to be \((_{}^{-2}l)\). The following corollary characterizes these parts of theoretical performance:

**Corollary 1**: _Let \(l=(T)+()\), the expected commitment time for EOCP in Algorithm. 1 is given by_

\[_{}^{}(T)=(_{ }^{-2} T),\]

_and the confidence level is \((T^{-1})\)._

The complete proofs of Theorem. 1 and Corollary. 1 are provided in the supplementary material. In order to upper bound the cumulative regret of \(T\) rounds, we divide the total regret into the regret accumulated in exploration and the regret accumulated in commitment.

**Bounding Regret from Exploration:** To bound the regret accumulated in exploration and since we use a variant of UCB exploration, we follow the standard procedure of proofs for UCB algorithms, e.g., proof of Theorem 8 from. . Then, this procedure results in an order \((l)\) dominating regret term, which is \(( T)\) by the choice of our exploration function. Through carefully applying any-time concentration inequalities, we are able to show that the constant in front of this dominating regret term is exactly the constant we obtained in Theorem. 1.

**Bounding Regret from Committing to the Wrong Action:** As for the regret accumulated from commitment, the key is to prove the \((T^{-1})\) confidence level upper bound presented in Corollary. 1. We follow a procedure similar to the proof of Theorem 1 in  to utilize the adaptivity of UCB exploration and the pessimistic LCB commitment. We first show that with high probability, the number of pulls \(N_{t}(a)\) for any sub-optimal actions in the exploration phase is upper-bounded. This is because after a certain number of pulls, the uncertainty bonus \(b_{t-1}(a)\) for any sub-optimal action will be so small that the upper confidence bound \(_{t-1}(a)\) cannot be larger than \(_{a}*\), thus less than the upper confidence bound of the optimal action. Therefore, sub-optimal actions will not be chosen in future rounds. After our carefully designed \(T_{}\), we make sure that the optimal action has the largest number of pulls \(N_{T_{}}(a)\) among all actions, thus its bonus is so small so that its lower confidence bound \(_{T_{}}(a*)\) is larger than the expectations \(_{a}\) of any sub-optimal action \(a\), and thus larger than the lower confidence bound of other actions. So with high probability, we will commit to the optimal action. Then, the \((T^{-1})\) confidence level will provide us with a constant regret in commitment, and the overall dominating regret comes from exploration.

Combining both bounds, we are able to show the regret performance upper bound in Theorem. 1.

### Regret Optimality for Gaussian Bandits with Adaptive Stopping Time

We present the regret performance of EOCP-UG in Theorem. 2. Compared to Theorem. 1, EOCP-UG has exactly the same regret performance guarantees as EOCP even without the knowledge of a lower bound \(_{}\) on the minimum reward gap. This implies that EOCP-UG adapts to the reward gap.

**Theorem 2**: _Let \(l=(T)+()\) and when \(T\) is large enough, the expected regret of the EOCP-UG algorithm in Algorithm. 2 with adaptive stopping time is upper-bounded by:_

\[_{}^{}}(T)_{a: _{a}>0}}+(}{ _{}}).\]We can also directly derive an asymptotic regret bound which shows that EOCP-UG is regret optimal in Gaussian bandits:

\[_{T}_{}^{}(T)}{ T}_{a:_{a}>0}}.\]

With adaptive stopping, the sample complexity until the commitment is not a pre-determined value. However, we can still extract similar guarantees along with the confidence level from the proof of Theorem. 2. We present these results in the following corollary:

**Corollary 2**: _Let \(l=(T)+()\), the sample complexity until commitment for \(\) algorithm in Algorithm. 2 is upper-bounded by:_

\[_{}^{}(T)_{a:_{a}>0 }T}{_{a}^{2}}+(}T} {_{}^{2}}),\]

_and the confidence level is upper bounded by \((T^{-1})\)._

**Proof Roadmap:** The complete proofs of Theorem. 2 and Corollary 2 are provided in the supplementary material. Compared to the proof of Theorem. 1, the major difference lies in bounding the regret of commitment. Similarly, we are required to bound the probability of committing to a sub-optimal action. We first show that when the agent stops exploration according to Line 2 of Algorithm. 2, the action that has the maximum number of pulls is the optimal action with high probability. Then, we show that under this event we will commit to the optimal action if we use LCB commitment.

**Magic Choice of Exploration Function:** Bounding the regret in both exploration and commitment requires a delicate analysis with any time concentration inequalities. Our choice of exploration function \(l\) plays an important role which manages the trade-off between low-regret exploration and high-probability optimal action commitment. If the exploration function is too large, i.e., if \(l=2 T\), the regret in exploration will not be optimal. If the exploration function is too small, i.e., if \(l= T\), the probability of committing to the wrong action can not be bounded by \((T^{-1})\). Our magic choice of exploration function \(l\) achieves the best of both worlds.

**Loss of \(\) from Unknown Gap:** Even though we have the same regret performance, the guarantee for commitment time is \((^{2}T)\) which is worse than \(( T)\) in the pre-determined setting. So, is this order fundamental with adaptive stopping time? In the next section, we provide the answer by investigating the theoretical limits of commitment time for asymptotic regret optimal algorithms.

### Fundamental Limits of Sample Complexity until Commitment

In order to answer the question regarding the fundamental sample complexity until commitment for regret optimal algorithms with both pre-determined and adaptive stopping times, we consider a simplified Gaussian bandit model where there are only \(2\) arms. The reward gap between the two actions is \(\), so \(_{}=\). From Eq. (1), it is clear that the optimal regret of any algorithm is asymptotically \(2^{-1}(T)\), so the set of all regret optimal algorithms is characterized by:

\[_{}=\{|_{T}_{}^{}(T)}{ T}_{}( a^{*})= (T^{-1}).\}.\]

Furthermore, for each regret optimal algorithm, we say it has \(c\)-logarithm regret violation if there exists a constant \(c(0,1)\) (choose the minimum \(c\) if there exists multiple) such that the following inequality is satisfied when \(T\) is large enough:

\[|_{}^{}(T)-|=(^{c}T).\]

If an algorithm has \(c\)-logarithm regret violation, the largest additional lower order term in its finite-time regret bound should be \((^{c}T)\). On the other hand, it also characterizes the convergence rate of the regret to its asymptote. For example, the vanilla UCB algorithm has at most \(1/2\)-logarithm regret violation, and our EOCP and EOCP-UG algorithms both have at most \(1/2\)-logarithm regret violation (22, Theorem. 8). We then provide the following theorem to characterize the fundamental commitment time limits for algorithms in \(_{}\):

**Theorem 3** (Information-Theoretic Limits of \(\)): _Consider a \(2\)-armed Gaussian bandits, for any asymptotically regret optimal algorithm \(\) which has \(c\)-logarithm regret violation, in order to guarantee \((T^{-1})\) confidence level, the sample complexity until commitment with pre-determined stopping time is lower bounded by_

\[T_{}=(^{-2}(T)),\]

_and with adaptive stopping time, the sample complexity until commitment is lower-bounded by:_

\[_{}[T_{}]=(^{-2}^{2-c}(T)).\]

**Proof Roadmap:** The proof is provided in the supplementary material. The proof idea relies on the well-known "transportation" lemma [32, Lemma. 1] originally derived to prove the theoretical limits of best arm identification algorithms. This lemma characterizes the expected number of pulls for each action by hypothesis testing between the original bandit problem and another bandit instance with a different optimal action. Then by finding a proper bandit instance and combining the lemma with regret optimal algorithms, we will be able to prove Theorem. 3 for both settings.

**Sample Optimality:** It is shown by Corollary. 1 and Theorem. 3 together that our proposed EOCP algorithm achieves the optimal commitment time with \(( T)\) with pre-determined stopping time if the reward gap \(\) is known a priori. However, with adaptive stopping time, our EOCP-UG algorithm has \(1/2\)-logarithm regret violation and \((^{2}T)\) commitment time which is larger than the \((^{1.5}T)\) lower bound indicated by Theorem. 3. Even though EOCP-UG is not shown to be exactly sample optimal, we conjure that the performance gap comes from our analysis techniques which makes one of the bounds (maybe both) not tight. Whether EOCP-UG is indeed sample optimal, and if not, how to design regret optimal algorithms with \((^{1.5}T)\) commitment time remains open.

## 5 Regret Optimality for General Bandits

Even though EOCP is applicable in sub-Gaussian bandits, it is not regret optimal any more beyond Gaussian bandits. This can be seen by comparing Theorem. 1 and the fundamental regret limit (1) with Pinsker's inequality, and it is clear that the lower bound is smaller than our upper bound asymptotically beyond Gaussian bandits. To close this gap, we propose an improved algorithm called KL-EOCP which is provably regret optimal in general bandits.

**Natural Exponential Family:** We assume the reward distributions of each action belong to a natural exponential family, i.e., \(=\{(_{})_{}:d_{}/d=( x -b())h(x)\}\), where \(\) is the set of all parameters \(\) such that the expectation \(\) is positive and bounded, i.e., \(\). \(\) is some reference measure on \(\) and \(b:\) is a convex twice differentiable function. This distribution \(_{}\) can also be parameterized by its expectation \(=b^{}()\), the derivative of \(b()\), and for every \(\) we denote by \(^{}\) the unique distribution in \(\) with expectation \(\) and by \(^{}\) its corresponding parameter. Gaussian distribution with unit variance is an example of this family. Moreover, the Kullback-Leibler divergence from \(_{_{1}}\) to \(_{_{2}}\) (with a little abuse of notation) can be expressed as :

\[(_{1},_{2})=(_{_{1}},_{_{2}})=b( _{2})-b(_{1})-b^{}(_{1})(_{2}-_{1}).\]

The set of exponential family bandit models \(=(_{_{1}},,_{_{A}})\) can be characterized by the expectations of the actions \(=(_{1},,_{A})\). We assume for all \(\), and \(\) the moment generating function \(M_{_{}}()=_{_{}}[( W)]\) for the distribution \(_{}\) is well-defined and is finite.

**Algorithm:** Analog to \(_{}\) in Algorithm. 1, the KL-EOCP algorithm requires the knowledge of a strictly positive lower bound \(_{}\) on the "minimum KL divergence", denoted as \(_{}\), which captures the minimum reward distribution gap (distance) between the optimal action and any sub-optimal action. Considering the asymmetricity of the KL divergence, we define \(_{}\) as follows:

\[_{}=_{a a*}\{(_{a},_{a*}),4 (^{}_{a},_{a})\},\]

where \(^{}_{a}(_{a},_{a*})\) such that \(4(^{}_{a},_{a*})=(_{a},_{a*})\). The term \(4(^{}_{a},_{a})\) reflects the skew of \(\) divergence when the two distributions are switched. A simple lower bound to \(_{}\) can be easily computed given a lower bound \(_{}\) on the minimum reward gap \(_{}\) with the expression of the exponential family, while our KL-EOCP algorithm can operate with any lower bound \(_{}\).

The KL-EOCP algorithm is summarized in Algorithm. 3. It designs the UCB and LCB bonuses based on the KL divergence of the reward distributions for all actions. In general, these designs would lead to smaller confidence intervals with the same \((T^{-1})\) concentration guarantees as first adopted in the KL-UCB algorithm proposed in . If the lower bound information \(_{}\) is not known a priori, we can combine Algorithm. 3 with the adaptive stopping time of EOCP-UG to deal with this setting.

```
0: Exploration function \(l\); Lower bound \(_{}\) on the minimum KL divergence.
1: Let \(T_{}=_{}^{-1}}+A\) be the length of the exploration phase.
2: Initialize by pulling each arm \(a\) once.
3:for\(t=A+1:T_{}\)do
4: Set upper confidence bound: \[_{t-1}(a)=*{arg\,max}_{ r_{t-1}(a)} \{N_{t-1}(a)(_{t-1}(a),) l\}.\]
5: Take action \(A_{t}=*{arg\,max}_{a}_{t-1}(a)\). // UCB Exploration
6:endfor
7: Set lower confidence bound: \[_{T_{}}(a)=*{arg\,min}_{_{T_{}}(a)}\{N_{T_{}}(a)(_{T_{ }}(a),) l\}.\]
8: For \(t[T_{}+1,T]\), commit to action \(=*{arg\,max}_{a}_{T_{}}(a)\). // LCB Commitment ```

**Algorithm 3** KL-EOCP with Pre-Determined Stopping Time

**Regret Optimality in General Bandits:** The theoretical regret performance of the KL-EOCP is summarized in the following Theorem. To the best of our knowledge, it is the first result achieving asymptotic regret optimality in general bandit problems with commitment.

**Theorem 4**: _Let \(l=(T)+()\), when \(T\) is large enough and the reward distributions \(\) of each action belong to the same natural exponential family, the expected regret of KL-EOCP in Algorithm. 3 is upper-bounded by:_

\[_{}(T)_{a:_{a}>0} T}{(_{a},_{1})}+(}T}{_{}}).\]

Therefore, it is clear that we can derive an asymptotic upper bound as follows:

\[_{T}_{}(T)}{ T} _{a:_{a}>0}}{(_{a},_{1})},\]

which exactly matches the regret limit in Eq. (1). The complete proof of Theorem. 4 is provided in the supplementary materials. Even though the proof roadmap is similar to the proof of Theorem. 1, the major difference comes from the use of a tighter concentration lemma modified from [12, Theorem. 11] which captures the low probability event when the KL divergence of the empirical mean is far away from its expectation. The sample complexity until commitment and confidence level guarantees can also be extracted from the proof. Specifically, \(_{}^{}(T)=(_ {}^{-1} T)\) and the confidence level is upper bounded by \((T^{-1})\).

## 6 Numerical Experiments

In this section, we study the empirical performance of our proposed EOCP algorithm with variants compared to existing algorithms in the literature, including BAI-ETC , UCB , KL-UCB , and DETC  in both Gaussian and Bernoulli bandit settings. In this section, we only test the algorithms on a two-armed bandit problem because some baselines are only applicable in two-armed settings. The performance of our algorithms in bandit models with multiple arms is demonstrated in Appendix. E. In the Gaussian setting, we test all the algorithms with distributions \((_{i},1)\) for arm \(i=1,2\) with a total of \(10^{6}\) rounds, and in the Bernoulli bandit setting, we test the algorithms with distribution \((_{i})\) for arm \(i=1,2\) with a total of \(10^{5}\) rounds. We set \(_{1}=0.7\) and \(_{2}=0.2\), so the gap between the arms is \(=0.5\). The results are averaged over \(10^{5}\) iterations and in Fig. 1.

In the Gaussian bandit setting, it shows that both BAI-ETC and DETC algorithms exhibit unsatisfactorily high regret. On the contrary, our proposed algorithms EOCP and EOCP-UG have lower final regret even compared to the popular UCB algorithm, surprisingly. Even though our algorithms use a larger exploration function than the vanilla UCB algorithm, and accumulate regret more quickly in exploration, i.e., approximately the first \(1000\) rounds, it allows us to commit to the optimal action. Our algorithms almost find the optimal action in all simulated traces which gives rise to the very slowly-increasing behavior of regret in the commitment phase. This phenomenon coincides with our theoretical analysis which shows that the regret in commitment is \((1)\). However, the UCB algorithm continues to explore sub-optimal actions, so its regret continues to grow when the EOCP algorithm has already committed to the optimal action. Numerically, the EOCP algorithm reduces \(20\%\) of the final regret compared to UCB through early commitment, and we conjure that preventing the "over-exploration" phenomenon of the UCB algorithm is behind the reason for such empirical regret reduction. Comparing the commitment time of EOCP-UG and EOCP, we can see that both algorithms stop exploration at approximately \(1000\) rounds. This means that not knowing the gap information won't harm the empirical sample complexity until commitment too much, which in turn may imply that our theoretical analysis of commitment time upper bound in Corollary. 2 is not tight enough. The same trend can be witnessed in the results of the Bernoulli bandits. However, in Bernoulli bandits, KL-UCB and KL-EOCP algorithms have a much better regret performance than other algorithms, which shows that knowledge of the reward distribution family improves the performance significantly.

## 7 Conclusion

We studied ROBAI which intends to both minimize regret and commit to the optimal action. We proposed EOCP with variants, which combine UCB exploration and LCB commitment with novel stopping criteria in both pre-determined and adaptive settings. We showed that both EOCP and EOCP-UG are regret asymptotic optimal in Gaussian bandits with \(( T)\) and \((^{2}T)\) commitment time respectively, almost matching the theoretical limits we derived. For general bandits, we proposed KL-EOCP which is provably regret optimal. Numerical experiments confirmed the superiority of our algorithms and revealed the "over-exploration" phenomenon of UCB algorithms.