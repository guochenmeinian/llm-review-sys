# Double and Single Descent in Causal Inference with an Application to High-Dimensional Synthetic Control

Double and Single Descent in Causal Inference with an Application to High-Dimensional Synthetic Control

Jann Spiess Guido Imbens Amar Venugopal

Stanford University

Stanford, CA, USA

jspiess,imbens,amarvenu@stanford.edu

###### Abstract

Motivated by a recent literature on the double-descent phenomenon in machine learning, we consider highly over-parameterized models in causal inference, including synthetic control with many control units. In such models, there may be so many free parameters that the model fits the training data perfectly. We first investigate high-dimensional linear regression for imputing wage data and estimating average treatment effects, where we find that models with many more covariates than sample size can outperform simple ones. We then document the performance of high-dimensional synthetic control estimators with many control units. We find that adding control units can help improve imputation performance even beyond the point where the pre-treatment fit is perfect. We provide a unified theoretical perspective on the performance of these high-dimensional models. Specifically, we show that more complex models can be interpreted as model-averaging estimators over simpler ones, which we link to an improvement in average performance. This perspective yields concrete insights into the use of synthetic control when control units are many relative to the number of pre-treatment periods.

+
Footnote â€ : Replication code is available at github.com/amarvenu/causal-descent.

## 1 Introduction

Motivated by a recent literature on the double-decent phenomenon in machine learning, we investigate the properties of common econometric estimators when we increase their complexity. For high-dimensional linear regression and for synthetic control with many control units, we document empirical applications where extremely over-parameterized models impute missing out-of-sample and out-of-time outcomes well. We then provide a common explanation for the returns to complexity in high-dimensional linear regression and synthetic control in terms of a simple model-averaging property, which we link to an improvement in imputation performance.

We often conceptualize the effect of complexity on econometric models in terms of a bias-variance trade-off: Adding complexity makes models more expressive and reduces bias, while increased overfitting leads to additional variance. In this view, an overly complex model fits overly well in the training sample, but fails to recover true parameters or generalize to new data. For example, linear regression with too many variables or synthetic control with many control units may perform poorly because of overfitting and excess variance. Consistency results for high-dimensional econometric models therefore usually assume that the expressiveness of models is limited relative to the available sample size, and practical advice often highlights choosing simple models or regularizing complex ones, in a way that balances bias and variance optimally to achieve good estimation and prediction performance.

A recent literature in statistics and machine learning has highlighted the surprisingly good prediction performance of extremely high-dimensional models that fit the data perfectly. That literature has documented a so-called double-descent phenomenon for deep neural networks and other high-dimensional regression models: increasing complexity beyond the interpolation threshold at which the training sample error is zero can lead to a gradual reduction in variance and improvement in out-of-sample performance. In such cases, there are often two complexity regimes: below the interpolation threshold, the usual bias-variance trade-off leads to a decrease (first descent) and then an increase in out-of-sample loss, while beyond the interpolation threshold, out-of-sample loss decreases again (second descent).

In order to investigate the properties of highly over-parameterized models in causal inference, we first demonstrate a double-descent curve for linear regression in imputing wage outcomes and estimating average-treatment effects. In the LaLonde (1986) sample of the Current Population Survey (CPS) drawn from Dehejia and Wahba (1999, 2002), we generate over 8,000 variables from binning and interacting the original eight demographic and employment-related variables. We then fit a linear regression model on 3,000 training units and an increasing, randomly chosen subset of these variables, choosing the norm-minimizing solution once the model fits perfectly. Evaluated on the LaLonde (1986) control sample from the National Supported Work Demonstration (NSW) experiment, we observe the usual bias-variance trade-off for a low and moderate number of included covariates, where performance first increases slightly, before deteriorating substantially when approaching the interpolation threshold. Beyond the interpolation threshold, however, out-of-sample performance increases again. Ultimately, an extremely over-parameterized model on over 8,000 variables even outperforms less complex regressions with a randomly chosen set of covariates and achieves performance comparable to a linear regression on the original, unmanipulated set of covariates.

Having demonstrated the returns to complexity in high-dimensional linear regression, we document the performance of synthetic control estimators with many control units. In the California smoking data (Abadie et al., 2010), we impute missing smoking rates based on a small number of pre-treatment periods and an increasingly large number of control states. As in the case of linear regression, we see returns to increasing model complexity, even beyond the point where some of the synthetic-control models fit the training data perfectly. However, for synthetic control we do not observe an initial trade-off between bias and variance: in our empirical example, the performance on future periods is always better for the more complex models, with no intermittent increase in errors. Unlike the double-descent relationship for linear regression, the performance of synthetic control in our application yields a single-descent curve that only improves with complexity, no matter whether there are a few or many control states.

We then connect the returns to complexity in high-dimensional linear regression and in synthetic control in terms of a simple model-averaging property. While both estimators are constructed differently and represent different regressions, they both share a common feature: More complex models can be represented as convex averages over simpler models. We show that this model-averaging property applies to linear regression in the interpolation regime, as well as to synthetic control in general. The property holds purely mechanically and does not depend on the training or target distributions. It relates to other model-agnostic properties of linear regression, for which we also show a reduction in (conditional) variance for minimal-norm least-squares estimators beyond the interpolation threshold.

Having established a model-averaging property for interpolation linear regression and for synthetic control, we provide high-level assumptions under which this property translates to better imputation performance. A direct consequence of model averaging is that the (convex) prediction loss of a more complex model cannot be worse than the corresponding average prediction loss of simpler models, when the same weights are used to average. When the complex model on average also outperforms comparable models of the same complexity, then we show that the model-averaging property translates into a reduction in average out-of-sample error relative to a randomly selected simpler model. These results only put high-level, largely model-agnostic assumptions on the data-generating process, and are driven by mechanical properties of the underlying estimators.

Our results have practical implications for the use of synthetic control with many control units. Conventional wisdom may indicate that synthetic control with a very large number of donor units relative to the number of pre-treatment periods is problematic, and that selecting among many, ex-anteexchangeable units in this case may represent a challenge. However, our results imply that this is not the case: as long as ties are broken by a suitable regularization procedure (such as the minimum-norm solution in our case), making an ex-ante choice among many control units is not necessary. That said, if ex-ante information is available about which units are particularly suitable as controls, then using this information can still be helpful and improve imputation.

We build upon results on over-parameterized regression in machine learning and statistics, where double-descent curves for kernel and linear regression and the performance of norm-minimizing interpolating solutions have been studied extensively (including Liang and Rakhlin, 2020; Liang et al., 2020; Liang and Recht, 2023; Bartlett et al., 2020; Hastie et al., 2022) as part of a broader literature on double descent and interpolation in deep learning (Zhang et al., 2016; Belkin et al., 2018; Belkin, 2021; Mei and Montanari, 2022). Kelly et al. (2022) documents the benefits of over-parameterized models in asset return prediction both in theory and empirically. Kato and Imaizumi (2022) provides results on the estimation of conditional average causal effects using over-parametrized linear regression. Relative to this work on interpolating regression, we show connections to synthetic control based on simple mechanical properties that are largely agnostic about the true data-generating process. Thereby, we also relate to work on high-dimensional and regularized synthetic control (Doudchenko and Imbens, 2016; Abadie and L'Hour, 2021; Ben-Michael et al., 2021) and the connections between synthetic control and linear regression (Athey et al., 2021; Agarwal et al., 2021; Shen et al., 2022; Bruns-Smith et al., 2023).1 Finally, we connect to a literature on model averaging in econometrics and statistics (e.g. Hansen, 2007; Claeskens and Hjort, 2008). More specifically, Wilson and Izmailov (2020) consider Bayesian model averaging in deep learning, and discuss relationships to double descent. While our linear-regression solutions are closely related to available results on norm-minimizing regression, we are not aware that the results on synthetic control were noted previously.

The remaining note is structured as follows. Section 2 provides an empirical example of double descent for linear regression and discusses some properties of the norm-minimizing linear-regression estimator in the interpolating case. Section 3 discusses the relationship of complexity and imputation performance of synthetic control in an empirical example, and makes connections to the properties of interpolating linear regression. Section 4 discusses high-level consequences of the model-averaging property of interpolating linear regression and synthetic control. Section 5 concludes by discussing some limitations and open questions.

## 2 Double Descent for Linear Regression

In this section, we consider imputation by high-dimensional linear regression as an illustration of the performance of highly over-parameterized models. We start with an empirical illustration of wage imputation in CPS data with a varying number of randomly ordered covariates, which we evaluate in terms of its ability to estimate an average treatment effect. We then discuss theoretical properties of the interpolating linear-regression estimator, followed by a graphical illustration. These results are closely related to prior work on interpolating linear and kernel regression (including Bartlett et al., 2020; Liang et al., 2020; Hastie et al., 2022; Kato and Imaizumi, 2022).

### Setup and Estimator

We consider a linear-regression estimator in data \((y_{i},x_{i})_{i=1}^{n}^{k}\) from \(n\) observations of a scalar outcome and \(k\) scalar covariates. We write \(X=(x_{i}^{})_{i=1}^{n}^{n k}\) and \(Y=(y_{i})_{i=1}^{n}^{n}\). For a subset \(J\{1,,k\}\) of the covariates, we denote by \(^{J}=*{arg\,min}_{^{k};_{j}=0  j J}_{i=1}^{n}(y_{i}-x_{i}^{})^{2}\) the set of all least-squares linear-regression estimates on \(J\). Among these, we choose the norm-minimizing estimate \(^{J}=*{arg\,min}_{^{J}}\|\|\). Throughout, we assume that \(X_{J}=(x_{ij})_{i\{1,,n\},j J}^{n|J|}\) is of full row rank. Hence, there are multiple least-squares solutions, \(|^{J}|>1\), if and only if \(|J|>n\). In that case, \(Y=X^{J}\) and \(^{J}\) is the minimal-norm interpolating solution (cf. Liang et al., 2020).

### Empirical Motivation

We impute wages in the non-experimental LaLonde (1986) sample of the Current Population Survey (CPS) using linear regression on a large number of covariates, based on data drawn from Dehejia and Wahba (1999, 2002). From the original eight covariates, we obtain \(k=8408\) explanatory variables by binning and interacting the available features. We train minimal-norm least-squares regression models on a training sample of \(n=3000\) randomly chosen observations, and evaluate their mean-squared error in imputing average wages for subsets (of various sizes) of the 260 LaLonde (1986) National Supported Work Demonstration (NSW) experimental controls as provided through Dehejia and Wahba (1999, 2002). To these covariates we add a small amount of iid Gaussian noise to avoid rank deficiency when estimating linear regression. When fitting wage imputation models, we vary the set of covariates included in the regression. Specifically, we order all features randomly. For a complexity \(\{1,,k\}\), we then choose the first \(\) covariates, and obtain the estimate \(^{J}\) for \(J=\{1,,\}\). Our presented results reflect an average over five such random orderings of covariates.

In order to assess the viability of this approach for applications to average treatment effect (ATE) estimation, we assess each model by its ability to accurately predict the average outcome on a new dataset. In particular, we consider various subsets of size \(m\) of the NSW experimental control set. For each \(m\), we draw 1000 samples of size \(m\) without replacement from the NSW experimental control dataset. For each sample, we average the \(m\) observation-level outcome predictions and compare the result to the true mean outcome of the given sample. We then take the RMSE across the 1000 draws of size \(m\) to obtain our evaluation metric.2

In Figure 0(a), we consider CPS data and plot the average root-mean-squared error (RMSE) for pointwise prediction of the wages, reporting the in-sample performance as well. The vertical dashed line denotes the interpolation threshold where \(=n\). Figure 0(b) shows the ATE RMSE metric, once again averaged over five random orderings of the covariates, for various subset sizes. The horizontal dashed lines show the RMSE of a simple linear regression on the original, unmanipulated (low-dimensional) features, colored according to corresponding sample size \(m\). A zoomed-in version of Figure 0(b) focusing on the highly overparametrized regime can be found in Figure 7.

The out-of-sample losses in these illustrations show a descent in loss right of the interpolation threshold (denoted by the vertical dashed line), at which point in-sample loss is zero. For NSW experimental controls, out-of-sample error initially decreases (first descent), while for CPS non-experimental controls it remains initially flat. For both out-of-sample datasets, loss then goes on to increase and peaks sharply at \(=n\), at which point the linear models start to fit perfectly. As complexity increases further, loss decreases again (second descent). In both cases, loss continues to decrease throughout. For NSW experimental controls, loss ultimately reaches a minimum that is below the lowest error achieved left of the interpolation threshold, while the loss achieved in the CPS case is similar between the right tails and the minimum on the left. Furthermore, as the sample size \(m\) increases in Figure 0(b), the highly complex interpolating models outperform a simple model based

Figure 1: Average RMSE for linear regression for a varying number of covariates.

on the original set of provided features; the \(m=50\) and \(m=100\) curves have minima below their corresponding dashed lines.

In this setting, highly over-parameterized linear models constructed via discretizing and interacting available features exhibit performance improvements over a linear model fitted on the original features. This result appears remarkable, since the heavily over-parameterized models do not include the original non-binary covariates, but only indicators obtained from quantile binning (see Section B.1 for details). Further improvement could be achieved by always including the original covariates in the model.

We note that our illustration is extreme in that it artificially creates a large number of low-signal covariates, for which the best model only slightly outperforms a small, hand-curated model based on the original covariates. Nevertheless, this simple empirical exercise demonstrates the non-monotonicity in the relationship of complexity to variance and loss that has been studied by the literature on interpolating regression and double descent, and extends it to causal target parameters like the average treatment effect.

### Theoretical Properties and Geometric Illustration

Motivated by the empirical illustration, we note some theoretical properties that are direct consequences of the structure of the norm-minimizing linear least-squares estimator, and will later serve as a comparison point for synthetic control. We note that formal results on the bias-variance properties in terms of more primitive properties of the data-generating process are available, including in Bartlett et al. (2020); Liang et al. (2020); Hastie et al. (2022). Here, we focus on illustrating properties that follow mechanically from the construction of the estimator. Our focus is on comparing more complex models, with covariates \(J\), to slightly simpler (more sparse) ones, with covariates \(J\{j\}\), where the \(j\)-th covariate is dropped. Throughout, we assume that the covariate matrices are of full row rank for the more and less complex models

**Assumption 1** (Full rank).: _The covariate matrix \(X_{J}^{n|J|}\) with columns \(J\) as well as the covariate matrices \(X_{J\{j\}}^{n(|J|-1)}\) for all \(j J\) are of full row rank._

We first consider the geometry of interpolating solutions, for which we note that more complex model can be expressed by an average over simpler models.

**Proposition 1** (Model averaging for interpolating linear least-squares regression).: _For every \(X\) and \(J\) with \(|J|>n\) such that Assumption 1 holds there exists_

\[^{J},_{j J}_{j}=1\] _such that_ \[^{J}=_{j J}_{j}^{J\{j\}}.\]

We can choose the weights in Proposition 1 as a function of the covariate matrix \(X_{J}\) only. Specifically, weights can be chosen as \(_{j}=^{}(X_{J}X_{J}^{})^{-1}X_{j}}{|J|-n}\), where \(X_{j}\) is the \(j\)-th column of \(X\) and \(X_{j}^{}(X_{J}X_{J}^{})^{-1}X_{j}\) can be seen as the "leverage" of feature \(j J\), analogously to the leverage of an observation in the usual (low-dimensional) linear regression case.3

Figure 2: Minimal-norm least-squares solutions for linear regression with \(J=\{1,2\}\), where \(n\) varies.

The model averaging property of interpolating linear regression is illustrated in the right panel of Figure 2 for the simple case of two covariates (\(J=\{1,2\}\)) and a single data point (\(n=1\)). Here, the models \(^{J}\) that for the model perfectly lie on the line through \(^{\{1\}}\) and \(^{\{2\}}\), with the norm-minimizing solution \(^{\{1,2\}}\) lies between the two. In contrast, if \(^{\{1\}}\) and \(^{\{2\}}\) are not interpolating (such as in the case of the left panel of Figure 2, where \(n=2\)), then the more complex model \(^{\{1,2\}}\) does not generally lie in the convex hull of the simpler ones.

As an immediate consequence, any interpolating model can in this case be expressed as a convex average of simpler interpolating models, \(^{J}=_{L J;|L|=l}_{j}^{L}\) for all \(\{n,,|J|\}\) (provided all \(X_{L}\) are of full row rank). A particularly interesting special case is \(=n\), for which we express \(^{J}\) as a model average of just-interpolating models \(^{L}\) with \(^{L}_{L}=X_{L}^{-1}Y\).

In addition to the model-averaging property, the geometry of interpolating solutions also implies that the variance of the norm-minimizing linear least-squares estimator generically decreases in the interpolation regime \(|J|>n\), for which the complex estimator \(^{J}\) along with the simpler estimator \(^{J\{j\}}\) both fit the training data perfectly.

**Proposition 2** (Variance reduction for linear least-squares regression).: _Suppose Assumption 1 and that \(Y\) has a second moment. If \(|J|>n\) then a.s. \((^{J}|X)_{j J} (^{J\{j\}}|X)\)._

The reduction in variance is a direct consequence of a more general property of the least-squares solution. Specifically, the next proposition shows that if we redraw new outcome data in the interpolation regime, then the distance between more complex solutions is smaller than the distance between less complex models.

**Proposition 3** (Variation hierarchy for linear least-squares regression).: _For fixed \(X\) and \(J\) for which Assumption 1 holds consider two draws \(Y_{A}\) and \(Y_{B}\) yielding minimal-norm least-squares estimates \(^{J}_{A},^{J\{j\}}_{A}\) and \(^{J}_{B},^{J\{j\}}_{B}\), respectively._

1. _If_ \(|J| n\)_, then_ \(\|^{J}_{A}-^{J}_{B}\|_{X^{}X}_{j J}\| ^{J\{j\}}_{A}-^{J\{j\}}_{B}\|_{X^{ }X}\)_._
2. _If_ \(|J|>n\)_, then_ \(\|^{J}_{A}-^{J}_{B}\|_{j J}\|^{J \{j\}}_{A}-^{J\{j\}}_{B}\|\)_._

_Here, we write \(\|\|_{M}=M}\) for some positive semi-definite symmetric matrix \(M^{k k}\)._

In words, the variation of models across draws of the outcome data increases with complexity on the left of the interpolation threshold, while it decreases on the right. Here, the choice of norm is essential for these results to hold uniformly across simpler models.4

Figure 3 depicts this graphically. In Figure 2(a), for the non-interpolating case we observe that the norm of the difference between the model coefficient vectors making use of both covariates (\(^{\{1,2\}},^{\{1,2\}}\)) is larger than the norm of the differences between the coefficient vectors taking into consideration just a single covariate at a time. In Figure 2(b) for the interpolating case, we observe that the reverse is true; in this case, the norm of the difference between the complex models is smaller than the norms of the differences between the simpler models.

In practice, we may care about model properties beyond variance, and consider imputation loss beyond the above norms in the parameters. In Section 4, we will leverage the model-averaging properties from Proposition 1 to establish such bounds on more general imputation errors.

We believe that these variance and geometric properties of linear regression are well understood in the literature and likely not new, although we are not aware of an explicit statement of the model-averaging connection between more and less complex interpolating linear-regression models.

## 3 Single Descent for Synthetic Control

We next consider imputation using synthetic control with many control units. As in the case of linear regression, we start with an empirical illustration. In the Abadie et al. (2010) California smoking dataset, we impute smoking rates for a target state for a varying number of control states. We then discuss theoretical properties of the synthetic control estimator, which we connect to its imputation quality in the following Section 4.

### Setup and Estimator

We consider a panel of \(N+1\) units observed over \(T\) time periods, \(Y=(y_{it})_{i\{0,,N\},t\{1,,T\}}^{(N+1) T}\), where \(i=0\) denotes the target unit. Our goal is to impute \(y_{0t}\) for \(t\{T+1,,T+S\}\) given \(y_{it}\) for \(i\{1,,N\},t\{T+1,,T+S\}\) by the synthetic-control estimator \(_{0t}=_{i=1}^{N}_{i}y_{it}\) with convex weights \(=\{w^{N};_{i=1}^{n}w_{i}=1\}\). Specifically, for a subset \(J\{1,,N\}\) of control units, we consider the synthetic control weights

\[^{J} =*{arg\,min}_{w}^{J}}\|w\| }^{J} =*{arg\,min}_{w;w_{j}=0 j  J}_{t=1}^{T}(y_{0t}-_{i=1}^{N}w_{i}y_{it})^{2}.\] (1)

Here, we choose the (unique) norm-minimizing synthetic control weights whenever there is more than one empirical risk minimizer. We can also interpret this solution as the limit \(^{J}\) of a ridge penalized synthetic control estimator \(^{J}_{}\),

\[^{J} =_{ 0}^{J}_{} ^{J}_{} ^{J}_{} =*{arg\,min}_{w;w_{j}=0 j  J}_{t=1}^{T}(y_{0t}-_{i=1}^{N}w_{i}y_{it})^{2}+\| w\|^{2},\] (2)

where \(^{J}_{}\) puts a penalty on the Euclidean norm \(\|w\|^{2}\) of the weights, multiplied by a factor \(>0\). This form of the penalized synthetic-control estimator is also considered by Shen et al. (2022).

We note that, unlike in the linear-regression case, we can now end up with non-interpolating solutions even in the case of high model complexity (many control units). The reason is that the convexity restriction \(\) allows for interpolation only if the target outcomes are in the convex hull of the control outcomes.

### Empirical Motivation

To illustrate some properties of the synthetic control estimator, we impute California smoking rates in the Abadie et al. (2010) dataset. In that dataset, California experiences the introduction of smoking legislation in 1989, for which Abadie et al. (2010) provides a causal effect estimate by imputing counterfactual smoking rates for those years when the legislation is in effect. We instead consider only the time before the legislation is introduced, giving us access to observed control outcomes in all years, even for California. Specifically, we fit synthetic control models for California on \(T=3\) years of data (1984-1986), and evaluate their imputation performance on the following \(S=2\) years (1987-1988) in terms of mean-squared error.

When fitting synthetic control models, we vary how many of the \(N=20\) control states we include in the estimation process. Specifically, for a given complexity \(\{1,,N\}\), we average out-of-time

Figure 3: Minimal-norm least-squares solutions for draws \(Y_{A}\) (black) and \(Y_{B}\) (orange) for linear regression with \(J=\{1,2\}\), where \(n\) varies

root-mean squared error (RMSE) across all \(\) combinations of control states. We report results in Figure 4.

Unlike the linear-regression case, the loss in the synthetic-control case changes monotonically: as we increase the number of control units, average RMSE decreases. We therefore observe a single-descent curve in the relationship of complexity and loss, with no notable change in regimes when the number of control states surpasses the number \(T=3\) of training periods.

As before, our illustration is extreme: by using only three training periods and a random selection of control states, we can provide a stark illustration of the difference in behavior between the synthetic control and linear-regression estimators.

### Theoretical Properties and Graphical Illustration

While linear regression and synthetic control behave differently in terms of their double- vs single-descent behavior, we note that both exhibit continuing returns to increasing complexity, with no limit. In our empirical illustration, that return to complexity kicks in in the interpolation regime for linear regression and throughout for synthetic control. We now connect this commonality in returns to complexity to a corresponding theoretical connection.

**Proposition 4** (Model averaging for synthetic control).: _For all \(J\) with \(|J|>1\) and data \(Y\) there exists_

\[^{J},_{j J}_{j}=1\] _such that_ \[^{J}=_{j J}_{j}^{J\{j\}}.\]

Hence, synthetic control has the same model-averaging property as interpolating linear regression (Proposition 1). Now, however, the model-averaging property also holds without interpolation, and is instead driven by the convexity of synthetic control weights. Furthermore, the weights can depend on outcome data, although only for pre-treatment outcomes. We further note that the result extends to penalized synthetic control with some fixed penalty parameter \(\).

We illustrate the model-averaging property for synthetic control in Figure 5, where we show that synthetic California with \(|J|=2\) (left) and \(|J|=3\) (right) control states is a convex combination of synthetic California with fewer control states. Here, we focus on the fit in the training data, and do not explicitly show the underlying synthetic-control weights. We note that Figure 5 covers both cases where the synthetic estimator for California has perfect training fit (right) and cases where it does not (left).

Unlike in the case of linear regression (Proposition 3) we do not, however, obtain an immediate bound on the variation of synthetic control models. Indeed, as the case of \(|J|=2\) controls in Figure 5 clarifies, the complex model can have more variance in weights than the simple ones (which here do not vary at all), despite the model-averaging property. In the next section, we will therefore connect model averaging directly to improvements of imputation quality, without relying on explicit variation results.

Figure 4: Average out-of-time (blue) and training (orange) RMSE for synthetic control for a varying number of control units.

## 4 Model-Averaging Based Risk Bounds

Above, we argued that more complex models are model averages over simpler models in two cases that are relevant to causal inference: interpolating linear regression and synthetic control. In this section, we discuss conditions under which model averaging leads to better imputation quality.

To unify the above cases, we now consider generic estimated functions \(^{*}:\) that can be related to simpler models \(^{j}:\) for an index set \(j J\) by the model-averaging property

\[^{*}=_{j J}_{j}^{j}\] for some \[^{J},_{j J}_{j}=1.\] (MA)

We see this model-averaging property as a purely mechanical property of estimators, which we applies to interpolating linear regression (Proposition 1) and to synthetic control (Proposition 4) by our results above

Model averaging provides some insurance against excess loss. Intuitively, being able to represent a more complex model \(^{*}\) in terms of a model average over simpler models diversifies the risk of a bad imputation fit. When considering convex loss functions, we can make this intuition precise via a simple application of Jensen's inequality, which yields for the case of squared error that

\[(y-^{*}(x))^{2}_{j J}_{j}(y-^{j}(x))^{2}.\] (3)

for any target point \((y,x)\). Hence, imputation loss using the complex model is at most a weighted average over the loss of simpler models. The relationship also extends directly to estimating averages of outcomes \(y\) by averages of predictions \((x)\), as in the case of average treatment effects in Section 2.

In principle, the simple portfolio bound in (3) leaves open the possibility that the more complex model \(^{*}\) performs as poorly as the worst of the simpler models \(^{j}\). However, for this to occur, the weights would have to be positively correlated with _bad_ performance. A condition on imputation quality we can therefore consider imposing is that the selection of weights is not, on average, working _against_ imputation quality. The following result formalizes this idea on a (very) high level.

**Proposition 5** (Model-agnostic risk bound).: _Assume that (MA) holds and that for some distribution over training and target data we have that for all permutations \(:J J\)_

\[[(y-^{*}(x))^{2}][(y-^{*}_{ }(x))^{2}]\] _where_ \[^{*}_{}=_{j J}_{(j)}^{j}.\] (P)

_Then we obtain the bound \([(y-^{*}(x))^{2}]_{j J} [(y-^{j}(x))^{2}]\)._

In words, if the model chosen by the data on average over some distribution is not worse than a model where we mix up weights, then the imputation performance of the complex model is not worse than the average of the imputation performances of simple models, leading to observations like those in Figures 0(b) and 4 where increased model complexity leads to improved imputation quality for randomly-ordered models.

Figure 5: Synthetic-control examples for \(T=2\), where the set \(J\) of included units varies.

We make two comments on the formal conditions of the proposition. First, it is enough to assume that (P) holds on average over permutations chosen uniformly at random. Second, the distribution behind the expectation E can incorporate prior distributions over underlying parameters, in which case the resulting bound holds on average over the same distribution.

In the examples of linear regression and synthetic control, Figure 6 illustrates the respective permuted models. In those cases, the permutation assumption (P) assumes that, on average, the model chosen by the data is not worse than a model where we mix up the weights (in gray).

While more primitive conditions may be helpful to judge when a condition like (P) holds, we note two attractive properties. First, the assumption complements the model-averaging property (MA) in an important way: While model averaging relates more complex to less complex models, the permutation property only compares models of comparable complexity (assuming that there are no systematic ex-ante differences between the \(^{j}\)). To violate this property would thus amount to assuming that selection among models with _comparable_ complexity is disadvantageous, which may be unreasonable to expect on average. Second, we can formulate this condition on the level of estimators, without explicit reference to the underlying data-generating process.

## 5 Conclusion

We study the imputation performance of interpolating linear regression and synthetic control, and provide a unified perspective on returns to complexity in both cases: More complex models can be expressed as model averages over simpler ones. While we provide some high-level assumptions on when this model-averaging property improves average imputation risk, more work is needed to establish primitive sufficient conditions. This includes, in particular, studying how the bias changes as models become more complex. In addition, we limit our analysis to comparing more complex to simpler models when features or control units are randomly ordered, but in practice, we may have knowledge about which simple models are more plausible than others. However, our results show that highly over-parameterized models that achieve perfect in-sample fit can yield measurable performance improvements over non-random simple models in causal settings. Future research could explore conditions under which this phenomenon holds, namely where complex models can beat non-random simple ones.

Figure 6: Illustration of permutation bound based on the examples from Figures 2 and 5.