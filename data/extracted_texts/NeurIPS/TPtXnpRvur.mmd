# One-Step Effective Diffusion Network for

Real-World Image Super-Resolution

Rongyuan Wu\({}^{1,2,@paragraphsign}\), Lingchen Sun\({}^{1,2,@paragraphsign}\), Zhiyuan Ma\({}^{1,@paragraphsign}\), Lei Zhang\({}^{1,2,@paragraphsign}\)

\({}^{1}\)The Hong Kong Polytechnic University \({}^{2}\)OPPO Research Institute

{rong-yuan.wu, ling-chen.sun, zm2354.ma}@connect.polyu.hk, cslzhang@comp.polyu.edu.hk

\({}^{}\)Equal contribution \({}^{}\)Corresponding author

###### Abstract

The pre-trained text-to-image diffusion models have been increasingly employed to tackle the real-world image super-resolution (Real-ISR) problem due to their powerful generative image priors. Most of the existing methods start from random noise to reconstruct the high-quality (HQ) image under the guidance of the given low-quality (LQ) image. While promising results have been achieved, such Real-ISR methods require multiple diffusion steps to reproduce the HQ image, increasing the computational cost. Meanwhile, the random noise introduces uncertainty in the output, which is unfriendly to image restoration tasks. To address these issues, we propose a one-step effective diffusion network, namely OSEDiff, for the Real-ISR problem. We argue that the LQ image contains rich information to restore its HQ counterpart, and hence the given LQ image can be directly taken as the starting point for diffusion, eliminating the uncertainty introduced by random noise sampling. We finetune the pre-trained diffusion network with trainable layers to adapt it to complex image degradations. To ensure that the one-step diffusion model could yield HQ Real-ISR output, we apply variational score distillation in the latent space to conduct KL-divergence regularization. As a result, our OSEDiff model can efficiently and effectively generate HQ images in just one diffusion step. Our experiments demonstrate that OSEDiff achieves comparable or even better Real-ISR results, in terms of both objective metrics and subjective evaluations, than previous diffusion model-based Real-ISR methods that require dozens or hundreds of steps. The source codes are released at https://github.com/cswry/OSEDiff.

+
Footnote â€ : This work is supported by the PolyU-OPPO Joint Innovation Lab.

## 1 Introduction

Image super-resolution (ISR)  is a classical yet still active research problem, which aims to restore a high-quality (HQ) image from its low-quality (LQ) observation suffering from degradations of noise, blur and low-resolution, _etc_. While one line of ISR research  simplifies the degradation process from HQ to LQ images as bicubic downsampling (or downsampling after Gaussian blur) and focus on the study on network architecture design, the trained models can hardly be generalized to real-world LQ images, whose degradations are often unknown and much more complex. Therefore, another increasingly popular line of ISR research is the so-called real-world ISR (Real-ISR)  problem, which aims to reproduce perceptually realistic HQ images from the LQ images captured in real-world applications.

There are two major issues in training a Real-ISR model. One is how to build the LQ-HQ training image pairs, and another is how to ensure the naturalness of restored images, _i.e._, how to ensure that the restored images follow the distribution of HQ natural images. For the first issue, some researchershave proposed to collect real-world LQ-HQ image pairs using long-short camera focal lenses [3; 51]. However, this is very costly and can only cover certain types of real-world image degradations. Another more economical way is to simulate the real-world LQ-HQ image pairs by using complex image degradation pipelines. The representative works include BSRGAN  and Real-ESRGAN , where a random shuffling of basic degradation operators and a high-order degradation model are respectively used to generate LQ-HQ image pairs.

With the given training data, how to train a robust Real-ISR model to output perceptually natural images with high quality becomes a key issue. Simply learning a mapping network between LQ-HQ paired data with pixel-wise losses can lead to over-smoothed results [24; 46]. It is crucial to integrate natural image priors into the learning process to reproduce HQ images. A few methods have been proposed to this end. The perceptual loss  explores the texture, color, and structural priors in a pre-trained model such as VGG-16  and AlexNet . The generative adversarial networks (GANs)  alternatively train a generator and a discriminator, and they have been adopted for Real-ISR tasks [24; 46; 45; 61; 27; 53]. The generator network aims to synthesize HQ images, while the discriminator network aims to distinguish whether the synthesized image is realistic or not. While great successes have been achieved, especially in the restoration of specific classes of images such as face images [56; 44], GAN-based Real-ISR tends to generate unpleasant details due to the unstable adversarial training and the difficulties in discriminating the image space of diverse natural scenes.

The recently developed generative diffusion models (DM) [39; 16], especially the large-scale pre-trained text-to-image (T2I) models [37; 36], have demonstrated remarkable performance in various downstream tasks. Having been trained on billions of image-text pairs, the pre-trained T2I models possess powerful natural image priors, which can be well exploited to improve the naturalness and perceptual quality of Real-ISR outputs. Some methods [42; 57; 31; 52; 40; 59] have been developed to employ the pre-trained T2I model for solving the Real-ISR problem. While having shown impressive results in generating richer and more realistic image details than GAN-based methods, the existing SD-based methods have several problems to be further addressed. First, these methods typically take random Gaussian noise as the start point of the diffusion process. Though the LQ images are used as the control signal with a ControlNet module , these methods introduce unwanted randomness in the output HQ images . Second, the restored HQ images are usually obtained by tens or even hundreds of diffusion steps, making the Real-ISR process computationally expensive. Though some one-step diffusion based Real-ISR methods  have been recently proposed, they fail in achieving high-quality details compared to multi-step methods.

To address the aforementioned issues, we propose a **One**-**Step Effective **Diffusion** network, **OSEDiff** in short, for the Real-ISR problem. The UNet backbone in pre-trained SD models has strong capability to transfer the input data into another domain, while the given LQ image actually has rich information to restore its HQ counterpart. Therefore, we propose to directly feed the LQ images into the pre-trained SD model without introducing any random noise. Meanwhile, we integrate trainable

Figure 1: Performance and efficiency comparison among SD-based Real-ISR methods. (a). Performance comparison on the DrealSR benchmark . Metrics like LPIPS and NIQE, where smaller scores indicate better image quality, are inverted and normalized for display. OSEDiff achieves leading scores on most metrics with only one diffusion step. (b). Model efficiency comparison. The inference time is tested on an A100 GPU with \(512 512\) input image size. OSEDiff has the fewest trainable parameters and is over 100 times faster than StableSR .

LoRA layers  into the pre-trained UNet, and finetune the SD model to adapt it to the Real-ISR task. On the other hand, to ensure that the one-step model can still produce HQ natural images as the multi-step models, we utilize variational score distillation (VSD) [49; 58; 10] for KL-divergence regularization. This operation effectively leverages the powerful natural image priors of pre-trained SD models and aligns the distribution of generated images with natural image priors. As illustrated in Fig. 1, our extensive experiments demonstrate that OSEDiff achieves comparable or superior performance measures to state-of-the-art SD-based Real-ISR models, while it significantly reduces the number of inference steps from \(N\) to 1 and has the fewest trainable parameters, leading to more than \( 100\) speedup in inference time over previous methods such as StableSR .

## 2 Related Work

Starting from SRCNN , deep learning-based methods have become prevalent for ISR. A variety of methods have been proposed [30; 66; 67; 9; 5; 29; 65; 6; 7] to improve the accuracy of ISR reconstruction. However, most of these methods assume simple and known degradations such as bicubic downsampling, limiting their applications to real-world images with complex and unknown degradations. In recent years, researches have been exploring the potentials of generative models, including GAN  and diffusion networks , for solving the Real-ISR problem.

**GAN-based Real-ISR.** The use of GAN for photo-realistic ISR can be traced back to SRGAN , where the image degradation is assumed to be bicubic downsampling. Later on, researchers found that GAN has the potential to perform real-world image restoration with more complex degradations [61; 45]. Specifically, by using randomly shuffled degradation and high-order degradation to generate more realistic LQ-HQ training pairs, BSRGAN  and Real-ESRGAN  demonstrate promising Real-ISR results, which trigger many following works [4; 27; 28; 53]. DASR  designs a tiny network to predict the degradation parameters to handle degradations of various levels. SwinIR  switches the generator from CNNs to stronger transformers, further enhancing the performance of Real-ISR. However, the adversarial training process in GAN is unstable and its discriminator is limited in telling the quality of diverse natural image contents. Therefore, GAN-based Real-ISR methods often suffer from unnatural visual artifacts. Some works such as LDL  and DeSRA  can suppress much the artifacts, yet they are difficult to generate more natural details.

**Diffusion-based Real-ISR.** Some early attempts [21; 20; 47] employ the denoising diffusion probabilistic models (DDPMs) [16; 39; 11] to address the ISR problem by assuming simple and known degradations (_e.g._, bicubic downsampling). These methods are training-free by modifying the reverse transition of pre-trained DDPMs using gradient descent, but they cannot be applied to complex unknown degradations. Recent researches [42; 57; 31; 40; 59] have leveraged stronger pre-trained T2I models, such as Stable Diffusion (SD) , to tackle the Real-ISR problem. In general, they introduce an adapter  to fine-tune the SD model to reconstruct the HQ image with the LQ image as the control signal. StableSR  finetunes a time-aware encoder and employs feature warping to balance fidelity and perceptual quality. PASD  extracts both low-level and high-level features from the LQ image and inputs them to the pre-trained SD model with a pixel-aware cross attention module. To further enhance the semantic-aware ability of the Real-ISR model, SeeSR  introduces degradation-robust tag-style text prompts and utilizes soft prompts to guide the diffusion process. To mitigate the potential risks of diffusion uncertainty, CCSR  leverages a truncated diffusion process to recover structures and finetunes the VAE decoder by adversarial training to enhance details. SUPIR  leverages the powerful generation capability of SDXL model and the strong captioning capability of LLaVA  to synthesize rich image details.

The above mentioned methods, however, require tens or even hundreds of steps to complete the diffusion process, resulting in unfriendly latency. SinSR shortens ResShift  to a single-step inference by consistency preserving distillation. Nevertheless, the non-distrbution-based distillation loss tends to obtain smooth results, and the model capacity of SinSR and ResShift are much smaller than the SD models to address Real-ISR problems.

## 3 Methodology

### Problem Modeling

Real-ISR is to estimate an HQ image \(}_{H}\) from the given LQ image \(_{L}\). This task can be conventionally modeled as an optimization problem: \(}_{H}=*{argmin}_{_{H}}(_{data}( (_{H}),_{L})+_{reg}(_{H} ))\), where \(\) is the degradation function, \(_{data}\) is the data term to measure the fidelity of the optimization output, \(_{reg}\) is the regularization term to exploit the prior information of natural images, and scalar \(\) is the balance parameter. Many conventional ISR methods  restore the desired HQ image by assuming simple and known degradation models and employing hand-crafted natural image prior models (_i.e._, image sparsity based prior ).

However, the performance of such optimization-based methods is largely hindered by two factors. First, the degradation function \(\) is often unknown and hard to model in real-world scenarios. Second, the hand-crafted regularization terms \(_{}\) are hard to effectively model the complex natural image priors. With the development of deep-learning techniques, it has become prevalent to learn a neural network \(G_{}\), which is parameterized by \(\), from a training dataset \(S\) of \((_{L},_{H})\) pairs to map the LQ image to an HQ image. The network training can be described as the following learning problem:

\[^{*}=_{}_{(_{L},_{H}) S} [_{}(G_{}(_{L}),_{H} )+_{}(G_{}(_{L})) ],\] (1)

where \(_{}\) and \(_{}\) are the loss functions. \(_{}\) enforces that the network output \(}_{H}=G_{}(_{L})\) can approach to the ground-truth \(_{H}\) as much as possible, which can be quantified by metrics such as \(L_{1}\) norm, \(L_{2}\) norm and LPIPS . Using only the \(_{}\) loss to train the network \(G_{}\) from scratch may over-fit the training dataset. In this work, we propose to finetune a pre-trained generative network, more specifically the SD  network, to improve the generalization capability of \(G_{}\). In addition, the regularization loss \(_{}\) is critical to improve the generalization capability of \(G_{}\), as well as enhance the naturalness of output HQ images \(}_{H}\). Suppose that we have the distribution of real-world HQ images, denoted by \(p(_{H})\), the KL divergence  is an ideal choice to serve as the loss function of \(_{}\); that is, the distribution of restored HQ images, denoted by \(q_{}(}_{H})\), should be identical to \(p(_{H})\) as much as possible. The regularization loss can be defined as:

\[_{}=_{}(q_{}(}_{H})\|p(_{H})).\] (2)

Existing works  mostly instantiate the above objective via adversarial training , which involves learning a discriminator to differentiate between the generated HQ image \(}_{H}\) and the real HQ image \(_{H}\), and updating the generator \(G_{}\) to make \(}_{H}\) and \(_{H}\) indistinguishable. However, the discriminators are often trained from scratch alongside the generator. They may not be able to acquire the full distribution of HQ images and lack enough discriminative power, resulting in sub-optimal Real-ISR performance.

Figure 2: The training framework of OSEDiff. The LQ image is passed through a trainable encoder \(E_{}\), a LoRA finetuned diffusion network \(_{}\) and a frozen decoder \(D_{}\) to obtain the desired HQ image. In addition, text prompts are extracted from the LQ image and input to the diffusion network to stimulate its generation capacity. Meanwhile, the output of the diffusion network \(_{}\) will be sent to two regularizer networks (a frozen pre-trained one and a fine-tuned one), where variational score distillation is performed in latent space to ensure that the output of \(_{}\) follows HQ natural image distribution. The regularization loss will be back-propagated to update \(E_{}\) and \(_{}\). Once training is finished, only \(E_{}\), \(_{}\) and \(D_{}\) will be used in inference.

The recently developed T2I diffusion models such as SD  offer new options for us to formulate the loss \(_{}\). These models, trained on billions of image-text pairs, can effectively depict the natural image distribution in latent space. Some score distillation methods have been reported to employ SD to optimize images by using the KL-divergence as the objective [49; 25; 43]. In particular, variational score distillation (VSD) [49; 58; 10] induces such a KL-divergence based objective from particle-based variational optimization to align the distributions represented by two diffusion models. Based on the above discussions, we propose to instantiate the learning objective in Eq. (1) by designing an efficient and effective one-step diffusion network. In specific, we finetune the pre-trained SD with LoRA  as our Real-ISR backbone network and employ VSD as our regularizer to align the distribution of network outputs with natural HQ images. The details are provided in the next section.

### One-Step Effective Diffusion Network

**Framework Overview.** As discussed in Sec. 1, the existing SD-based Real-ISR methods [42; 57; 31; 52; 40] perform multiple timesteps to estimate the HQ image with random noise as the starting point and the LQ image as control signal. These approaches are resource-intensive and will inherently introduce randomness. Based on our formulation in Sec. 3.1, we propose a one-step effective diffusion (OSEDiff) network for Real-ISR, whose training framework is shown in Fig. 2. Our generator \(G_{}\) to be trained is composed of a trainable encoder \(E_{}\), a finetuned diffusion network \(_{}\) and a frozen decoder \(D_{}\). To ensure the generalization capability of \(G_{}\), the output of the diffusion network \(_{}\) will be sent to two regularizer networks, where VSD loss is performed in latent space. The regularization loss are back-propagated to update \(E_{}\) and \(_{}\). Once training is finished, only the generator \(G_{}\) will be used in inference. In the following, we will delve into the detailed architecture design of OSEDiff, as well as its associated training losses.

**Network Architecture Design.** Let's denote by \(E_{}\), \(_{}\) and \(D_{}\) the VAE encoder, latent diffusion network and VAE decoder of a pretrained SD model, where \(\) denotes the model parameters. Inspired by the recent success of LoRA  in finetuning SD to downstream tasks [34; 35], we adopt LoRA to fine-tune the pre-trained SD in the Real-ISR task to obtain the desired generator \(G_{}\).

As shown in the left part of Fig. 2, to maintain SD's original generation capacity, we introduce trainable LoRA  layers to the encoder \(E_{}\) and the diffusion network \(_{}\), finetuning them into \(E_{}\) and \(_{}\) with our training data. For the decoder, we fix its parameters and directly set \(D_{}=D_{}\). This is to ensure that the output space of the diffusion network remains consistent with the regularizers.

Recall that the diffusion model diffuses the input latent feature \(\) through \(_{t}=_{t}+_{t}\), where \(_{t},_{t}\) are scalars that are dependent to diffusion timestep \(t\{1,,T\}\). With a neural network that can predict the noise in \(_{t}\), denoted as \(}\), the denoised latent can be obtained as \(}_{0}=_{t}-_{t}}}{_{t}}\), which is expected to be cleaner and more photo-realistic than \(_{t}\). Moreover, SD is a text-conditioned generation model. By extracting the text embeddings , denoted by \(c_{y}\), from the given text description \(y\), the noise prediction can be performed as \(}=_{}(_{t};t,c_{y})\).

We adapt the above text-to-image denoising process to the Real-ISR task, and formulate the LQ-to-HQ latent transformation \(F_{}\) as a text-conditioned image-to-image denoising process as:

\[}_{H}=F_{}(_{L};c_{y})_{L}- _{T}_{}(_{L};T,c_{y})}{_{T}},\] (3)

where we conduct only one-step denoising on the LQ latent \(_{L}\), without introducing any noise, at the \(T\)-th diffusion timestep. The denoising output \(}_{H}\) is expected be more photo-realistic than \(_{L}\). As for the text embeddings, we apply some text prompt extractor, such as the DAPE , to LQ input \(_{L}\), and obtain \(c_{y}=Y(_{L})\). Finally, the whole LQ-to-HQ image synthesis can be written as:

\[}_{H}=G_{}(_{L}) D_{}(F_{}(E_{ }(_{L});Y(_{L}))).\] (4)

As mentioned in Sec. 3.1, to improve the performance for a Real-ISR model, it is required to supervise the generator training with both the data term \(_{}\) and regularization term \(_{}\). As shown in the right part of Fig. 2, we propose to adapt VSD  as the regularization term. Apart from utilizing the SD model as the pre-trained regularizer \(_{}\), VSD also introduces a finetuned regularizer, _i.e._, a latent diffusion module finetuned on the distribution \(q_{}(}_{H})\) of generated images with LoRA. We denote this finetuned diffusion module as \(_{^{}}\).

**Training Loss**. We train the generator \(_{}\) with the data loss \(_{}\) and regularization loss \(_{}\). We set \(_{}\) as the weighted sum of MSE loss and LPIPS loss:

\[_{}(_{}(_{L}),_{H} )=_{}(_{}(_{L}), _{H})+_{1}_{}(_{}( _{L}),_{H}),\] (5)

where \(_{1}\) is a weighting scalar. As for \(_{}\), we adopt the VSD loss via:

\[_{}(_{}(_{L}))= _{}(_{}(_{L}), )=_{}(_{}(_{L}), (_{L})).\] (6)

Given any trainable image-shape feature \(\), its latent code \(=_{}()\) and encoded text prompt condition \(c_{y}\), VSD optimizes \(\) to make it consistent with the text prompt \(y\) via:

\[_{}_{}(,c_{y})= _{t,}[(t)(_{}(_{t};t,c_{y})- {}_{^{}}(_{t},t;c_{y}))}{ }],\] (7)

where the expectation of the gradient is conducted over all diffusion timesteps \(t\{1,,T\}\) and \((0,)\). Therefore, the overall training objective for the generator \(_{}\) is:

\[(_{}(_{L}),_{H})=_{}(_{}(_{L}),_{H})+ _{2}_{}(_{}(_{L}) ),\] (8)

where \(_{2}\) is a weighting scalar. Besides, as required by VSD, the finetuned regularizer \(_{^{}}\) should also be trainable, and its training objective is:

\[_{}=_{t,,c_{y}=(_{L} ),}_{H}=_{}(_{}(_{L}); (_{L}))}_{}(_{ ^{}}(_{t}}_{H}+_{t};t,c_{y}),).\] (9)

Note that the above \(_{}\) loss is only applied to update \(_{^{}}\). The whole algorithm to illustrate the training pipeline can be found in the **Appendix**.

**VSD in Latent Space**. The original VSD computes the gradients in the image space. When it is used to train an SD-based generator network, there will be repetitive latent decoding/encoding in computing \(_{}\). This is costly and makes the regularization less effective. Considering the fact that a well-trained VAE should satisfy \(_{}()=_{}(_{}()) \), we can approximately let \(_{}(}_{H})=}_{H}\). In this case, we can eliminate the redundant latent encoding/decoding in computing the regularization loss, as we follow DMD  to optimize the distribution loss in the latent state space rather than in the noise space. The gradient of the regularization loss w.r.t. the network parameter \(\) in the latent space is:

\[_{}_{}(_{}( _{L}),c_{y}) =_{}_{H}}_{}(}_{H},c_ {y})}_{H}}{}\] (10) \[=_{t,,}_{t}=_{t}_{ }(}_{H})+_{t}}[(t)(_{ }(}_{t};t,c_{y})-_{^{}}(}_{t };t,c_{y}))}_{H}}{}_{H}} }_{H}}{}]\] \[=_{t,,}_{t}=_{t}}_{H }+_{t}}[(t)(_{}(}_{t };t,c_{y})-_{^{}}(}_{t};t,c_{y})) }_{H}}{}].\]

## 4 Experiments

### Experimental Settings

**Training and Testing Datasets.** Prior works [42; 57; 31; 52] employed different training datasets, making unified training standards for Real-ISR difficult to establish. For simplicity, we adopt SeeSR's setup  and train OSEDiff using the LSDIR  dataset and the first 10K face images from FFHQ . The degradation pipeline of Real-ESRGAN  is used to synthesize LQ-HQ training pairs. We evaluate OSEDiff and compare it with competing methods using the test set provided by StableSR , including both synthetic and real-world data. The synthetic data includes 3000 images of size \(512 512\), whose GT are randomly cropped from DIV2K-Val  and degraded using the Real-ESRGAN pipeline . The real-world data include LQ-HQ pairs from RealSR  and DRealSR , with sizes of \(128 128\) and \(512 512\), respectively.

**Compared Methods.** We compare OSEDiff with state-of-the-art DM-based Real-ISR methods, including StableSR , ResShift , PASD , DiffBIR , SeeSR  and SinSR . Among them, StableSR, PASD, DiffBIR, and SeeSR are all based on the pre-trained SD model. ResShift trains a DM from scratch in the pixel domain, while SinSR is a one-step model distilledfrom ResShift. Note that we do not compare with the recent method SUPIR  because it tends to generate rich yet excessive details, which are however unfaithful to the input image.

For those GAN-based Real-ISR methods, including BSRGAN , Real-ESRGAN , LDL , and FeMaSR , we put their results into the **Appendix**.

**Evaluation Metrics.** To provide a comprehensive and holistic assessment on the performance of different methods, we employ a range of full-reference and no-reference metrics. PSNR and SSIM  (calculated on the Y channel in YCbCr space) are reference-based fidelity measures, while LPIPS , DISTS  are reference-based perceptual quality measures. FID  evaluates the distance of distributions between GT and restored images. NIQE , MANIQA-pipal , MUSIQ , and CLIPIQA  are no-reference image quality measures. We also conduct a user study, which is presented in the **Appendix**.

**Implementation Details.** We train OSEDiff with the AdamW optimizer  at a learning rate of 5e-5. The entire training process took approximately 1 day on 4 NVIDIA A100 GPUs with a batch size of 16. The rank of LoRA in the VAE Encoder, diffusion network, and finetuned regularizer is set to 4. For the text prompt extractor, although advanced multimodal language models  can provide detailed text descriptions, they come at a high inference cost. We adopt the degradation-aware prompt extraction (DAPE) module in SeeSR  to extract text prompts. The SD 2.1-base is used as the pre-trained T2I model. The weighting scalars \(_{1}\) and \(_{2}\) are set to 2 and 1, respectively.

### Comparison with State-of-the-Arts

**Quantitative Comparisons.** The quantitative comparisons among the competing methods on the three datasets are presented in Table 1. We can have the following observations. (1) First, OSEDiff exhibits clear advantages over competing methods in full-reference perceptual quality metrics LPIPS and DISTS, distribution alignment metric FID, and semantic quality metric CLIPIQA, especially

   Datasets & Methods & PSNR\(\) & SSIM\(\) & LPIPS\(\) & DISTS\(\) & FID\(\) & NIQE\(\) & MUSIQ\(\) & MANIQA\(\) & CLIPIQA\(\) \\   & StableSR+x200 & 23.26 & 0.5726 & **0.3113** & 0.2048 & **24.44** & 4.7581 & 65.92 & 0.6192 & 0.6771 \\  & DiffBR+s50 & 23.64 & 0.5647 & 0.3524 & 0.2128 & 30.72 & **4.7042** & 65.81 & 0.6210 & 0.6704 \\  & SeeSR+s50 & 23.68 & 0.6043 & 0.3194 & **0.1968** & **25.90** & 4.810 & **68.67** & **0.6240** & **0.6936** \\  & PASSD & 23.14 & 0.5505 & 0.3571 & 0.2207 & 29.20 & **4.3617** & **68.95** & **0.6483** & **0.6788** \\  & ResShift+s1 & **24.65** & **0.6181** & 0.3349 & 0.2213 & 36.11 & 6.8212 & 61.09 & 0.5454 & 0.6071 \\  & SinSR+s1 & **24.41** & **0.6018** & 0.3240 & 0.2066 & 35.57 & 6.0159 & 62.82 & 0.5386 & 0.6471 \\  & OSEDiff-s1 & 23.72 & **0.6108** & **0.2941** & **0.1976** & 26.32 & 4.7097 & 67.97 & 0.6148 & 0.6683 \\   & StableSR+x200 & 28.03 & 0.7536 & 0.3284 & **0.2269** & 1489.6 & 6.5239 & 58.51 & 0.5601 & 0.6356 \\  & DiffBR+s50 & 26.71 & 0.6571 & 0.4557 & 0.2748 & 166.79 & **6.3124** & 61.07 & 0.5930 & 0.6395 \\  & SeeSR+s50 & 28.17 & **0.7691** & **0.3189** & 0.2315 & **147.39** & 6.3967 & **64.93** & **0.6042** & 0.6804 \\  & PASSD & 27.36 & 0.7073 & 0.3760 & 0.2531 & 156.13 & **5.5474** & **64.87** & **0.6169** & **0.6808** \\  & ResShift+s1 & **28.46** & 0.7673 & 0.4006 & 0.2656 & 172.26 & 8.1249 & 50.60 & 0.4586 & 0.5342 \\  & SinSR+s1 & **28.36** & 0.7515 & 0.3665 & 0.2485 & 170.57 & 6.9907 & 55.33 & 0.4884 & 0.6383 \\  & OSEDiff-s1 & 27.92 & **0.7835** & **0.2968** & **0.2165** & **135.30** & 6.4902 & 64.65 & 0.5899 & **0.6963** \\   & StableSR+x200 & 24.70 & 0.7085 & 0.3018 & 0.2288 & 128.51 & 5.9122 & 65.78 & 0.6221 & 0.6178 \\  & DiffBR+s50 & 24.75 & 0.6567 & 0.3636 & 0.2312 & 128.99 & 5.5346 & 64.98 & 0.6246 & 0.6463 \\  & SeeSR+s50 & 25.18 & 0.7216 & **0.3099** & **0.2223** & 125.55 & **5.4081** & **69.77** & **0.6442** & 0.6612 \\  & PASSD & 25.21 & 0.6798 & 0.3380 & 0.2260 & **124.29** & **5.4137** & 68.75 & **0.6487** & **0.6620** \\  & ResShift+s15 & **26.31** & **0.7421** & 0.3460 & 0.2498 & 141.71 & 7.2635 & 58.43 & 0.5285 & 0.5444 \\  & SusSR+s1 & **26.28** & **0.7347** & 0.3188 & 0.2353 & 135.93 & 6.2872 & 60.80 & 0.5385 & 0.6122 \\  & OSEDiff-s1 & 25.15 & 0.7341 & **0.2921** & **0.2128** & **123.49** & 5.6476 & **69.09** & 0.6326 & **0.6693** \\   

Table 1: Quantitative comparison with state-of-the-art methods on both synthetic and real-world benchmarks. â€˜sâ€™ denotes the number of diffusion reverse steps in the method. The best and second best results of each metric are highlighted in **red** and **blue**, respectively.

    & StableSR & DiffBIR & SeeSR & PASD & ResShift & SinSR & OSEDiff \\  Inference Step & 200 & 50 & 50 & 20 & 15 & 1 & 1 \\ Inference Time (s) & 11.50 & 2.72 & 4.30 & 2.80 & 0.71 & 0.13 & 0.11 \\ MACs (G) & 79940 & 24234 & 65857 & 29125 & 5491 & 2649 & 2265 \\ \# Total Param (M) & 1410 & 1717 & 2524 & 1900 & 119 & 119 & 1775 \\ \# Trainable Param (M) & 150.0 & 380.0 & 749.9 & 625.0 & 118.6 & 118.6 & 8.5 \\   

Table 2: Complexity comparison among different methods. All methods are tested with an input image of size \(512 512\), and the inference time is measured on an A100 GPU.

on the two real-world datasets DrealSR and RealSR. (2) Second, SeeSR and PASD show better no-reference metrics like NIQE, MUSIQ and MANIQA. This is because these multi-step methods can produce rich image details in the diffusion process, which are preferred by no-reference metrics. (3) Third, ResShift and its distilled version SinSR show better full-reference fidelity metrics such as PSNR. This is mainly because they train a DM from scratch specifically for the restoration purpose, instead of exploring the pre-trained T2I model such as SD. However, ResShift and SinSR show poor perceptual quality metrics than other methods.

**Qualitative Comparisons.** Fig. 3 presents visual comparisons of different Real-ISR methods. As illustrated in the first example, ResShift and SinSR severely blur the facial details due to the lack of pre-trained image priors. StableSR, DiffBIR and SeeSR reconstruct more facial details by exploring the image prior in pre-trained SD model. PASD generates excessive yet unnatural details. Though OSEDiff performs only one step forward propagation, it reproduces realistic and superior facial details to other methods. Similar conclusion can be drawn from the second example. StableSR and DiffBIR are limited in generating rich textures due to the lack of text prompts. PASD suffers from incorrect semantic generation because its prompt extraction module is not robust to degradation. While SeeSR utilizes degradation-aware semantic cues to stimulate image generation priors, the generated leaf veins are unnatural, which may be influenced by its random noise sampling. In contrast, OSEDiff can generate detailed and natural leaf veins. More visualization comparisons and the results of subjective user study can be found in the **Appendix**.

**Complexity Comparisons.** We further compare the complexity of competing DM-based Real-ISR models in Table 2, including the number of inference steps, inference time, and trainable parameters. All methods are tested on an A100 GPU with an input image of size \(512 512\). OSEDiff has the fewest trainable parameters, and the trained LoRA layers can be merged into the original SD to further reduce the computational cost. By using only one forward pass, OSEDiff has significant advantage in inference time over multi-step methods. Specifically, OSEDiff demonstrates a substantial speed advantage, being approximately 105 times faster than StableSR, 39 times faster than SeeSR, and 6 times faster than ResShift. When compared to the single-step method SinSR, OSEDiff not only achieves faster inference but also delivers significantly higher output quality. In terms of complexity, OSEDiff requires the lowest MACs at just 2265G, as it operates with only a single diffusion step. In contrast, methods like StableSR, which require 200 steps, incur substantially higher MACs (_e.g._, 79940G). Regarding trainable parameters, OSEDiff is highly parameter-efficient, requiring only 8.5M parameters (LoRA layers), compared to models such as SeeSR, which necessitates 749.9M parameters. This highlights the efficiency of OSEDiff during the training process.

Figure 3: Qualitative comparisons of different Real-ISR methods. Please zoom in for a better view.

[MISSING_PAGE_FAIL:9]

**Setting of LoRA Rank.** When finetuning the VAE encoder and the UNet, we need to set the rank of LoRA layers. Here we evaluate the effect of different LoRA ranks on the Real-ISR performance by using the RealSR benchmark . The results are shown in Tables 5 and 6, respectively. As shown in Table 5, if a too small LoRA rank, such as 2, is set for the VAE encoder, the training will be unstable and cannot converge. On the other hand, if a higher LoRA rank, such as 8, is used for the VAE encoder, it may overfit in estimating image degradation, losing some image details in the output, as evidenced by the PSNR, DISTS, MUSIQ and NIQE indices. We find that setting the rank to 4 can achieve a balanced result for the VAE encoder. Similar conclusions can be made for the setting of LoRA rank on UNet. As can be seen from Table 6, a rank of 4 strikes a good balance. Therefore, we set the rank as 4 for both the VAE encoder and UNet LoRA layers.

**The Finetuning on the VAE Encoder and Decoder.** We conducted ablation studies to examine the impact of finetuning the VAE encoder and decoder, as shown Table 7. In the first row, where neither the VAE encoder nor decoder is finetuned, the results show poor perception performance. Comparing with OSEDiff, where only the VAE encoder is finetuned, we observe significant improvements in perceptual quality (_e.g._, MUSIQ improves from 58.99 to 69.09). This demonstrates that finetuning the VAE encoder is important for removing degradation and enhancing overall performance. When comparing the third row, where both the VAE encoder and decoder are finetuned, with OSEDiff, where only the encoder is trained and the decoder is fixed, we note that OSEDiff also achieves better perceptual quality (CLIPIQ improves from 0.5778 to 0.6693). This indicates that fixing the VAE decoder is important to ensure that the UNet output remains in the original VAE latent space, which helps minimizing the VSD loss more effectively. Thus, finetuning the VAE encoder is important to remove degradation, while fixing the VAE decoder helps maintaining stability in the latent space, leading to better perceptual quality.

## 5 Conclusion and Limitation

We proposed OSEDiff, a one-step effective diffusion network for Real-ISR, by utilizing the pre-trained text-to-image model as both the generator and the regularizer in training. Unlike traditional multi-step diffusion models, OSEDiff directly took the given LQ image as the starting point for diffusion, eliminating the uncertainty associated with random noise. By fine-tuning the pre-trained diffusion network with trainable LoRA layers, OSEDiff can well adapt to the complex real-world image degradations. Meanwhile, we performed the variational score distillation in the latent space to ensure that the model's predicted scores align with those of multi-step pre-trained models, enabling OSEDiff to efficiently produce HQ images in one diffusion step. Our experiments showed that OSEDiff achieved comparable or superior Real-ISR outcomes to previous multi-step diffusion-based methods in both objective metrics and subjective assessments. We believe our exploration can facilitate the practical application of pre-trained T2I models to Real-ISR tasks.

There are some limitations of OSEDiff. First, the details generation capability of OSEDiff can be further improved. Second, like other SD-based methods, OSEDiff is limited in reconstructing fine-scale structures such as small scene texts. We will investigate these problems in further work.

Figure 4: The impact of different prompt extraction methods. Please zoom in for a better view.