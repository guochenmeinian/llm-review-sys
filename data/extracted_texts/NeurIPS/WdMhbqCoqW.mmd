# Learning to be Smooth:

An End-to-End Differentiable Particle Smoother

Ali Younis and Erik B. Sudderth

Department of Computer Science, University of California, Irvine, CA 92617

ayounis@uci.edu, sudderth@uci.edu

###### Abstract

For challenging state estimation problems arising in domains like vision and robotics, particle-based representations attractively enable temporal reasoning about multiple posterior modes. Particle smoothers offer the potential for more accurate offline data analysis by propagating information both forward and backward in time, but have classically required human-engineered dynamics and observation models. Extending recent advances in discriminative training of particle filters, we develop a framework for low-variance propagation of gradients across long time sequences when training particle smoothers. Our "two-filter" smoother integrates particle streams that are propagated forward and backward in time, while incorporating stratification and importance weights in the resampling step to provide low-variance gradient estimates for neural network dynamics and observation models. The resulting mixture density particle smoother is substantially more accurate than state-of-the-art particle filters, as well as search-based baselines, for city-scale global vehicle localization from real-world videos and maps.

## 1 Introduction

Global localization of the state of a moving vehicle using a city-scale map is challenging due to the large area, as well as the inherent ambiguity in urban landscapes, where many street intersections and buildings appear similar. Recent work on global localization  has typically localized each time point independently during training, sometimes followed by temporal post-processing, often with demanding requirements like near-exact external estimation of relative vehicle poses .

For a broader range of state estimation problems in fields like vision and robotics, a number of methods for end-to-end _particle filter_ (PF) training have been proposed . Learnable PFs are suitable for global localization because they can represent multi-modal posterior densities, propagate uncertainty over time, and learn models of real vehicle dynamics and complex sensors directly from data. However, most learnable PF methods have only been applied to simulated environments , with only a few preliminary applications to real-world data .

Particle filters  only use past observations to estimate the current state. For offline inference from complete time series, more powerful particle smoothing (PS) algorithms  may in principle perform better by integrating past and future data. But to our knowledge, recent advances in end-to-end differentiable training of PFs have not been generalized to the more complex PS scenario, requiring error-prone human engineering of PS dynamics and observation models. Classical work on generative parameter estimation via PS  is limited to parametric models with few parameters. In contrast, we develop differentiable PS that scale to complex models defined by deep neural networks.

After introducing differentiable particle filters (Sec. 2) and classical particle smoothers (Sec. 3), we develop our differentiable, discriminative _Mixture Density Particle Smoother_ (MDPS, see Fig. 1) in Sec. 4. Thorough experiments in Sec. 5 then highlight the advantages of our MDPS over differential PFs on a synthetic bearings-only tracking task, and also show substantial advantages over search-based and retrieval-based baselines for challenging real-world, city-scale global localization problems.

## 2 Differentiable Particle Filters

Particle filters iteratively estimate the posterior distribution \(p(x_{t}|y_{1:t},a_{1:t})\) over the state \(x_{t}\) at discrete time \(t\) given a sequence of observations \(y_{t}\) and, optionally, actions \(a_{t}\). PFs use a collection of \(N\) samples, or _particles_, \(x_{t}^{(:)}=\{x_{t}^{(1)},...,x_{t}^{(N)}\}\) with weights \(w_{t}^{(:)}=\{w_{t}^{(1)},...,w_{t}^{(N)}\}\) to flexibly capture multiple posterior modes nonparametrically. Classic PFs are derived from a Markov generative model, leading to an intuitive recursive algorithm that alternates between proposing new particle locations and updating particle weights. End-to-end training requires gradients for each PF step.

**Particle Proposals.** At each iteration, new particles \(x_{t}^{(:)}\) are proposed by applying a model of the state transition dynamics individually to each particle \(x_{t}^{(i)} p(x_{t}^{(i)}|x_{t-1}^{(i)},a_{t})\), conditioned on actions \(a_{t}\) if available. Of note, only simulation of the dynamics model is required; explicit density evaluation is unnecessary. Using reparameterization [29; 30; 31], the dynamics model can be defined as a feed-forward neural network \(f()\) that transforms random (Gaussian) noise to produce new particles:

\[x_{t}^{(i)}=f(_{t}^{(i)};x_{t-1}^{(i)},a_{t}),_{t}^{(i)}  N(0,I).\] (1)

**Measurement Updates and Discriminative Training.** Proposed particles are importance-weighted by the likelihood function, \(w_{t}^{(i)} p(y_{t}|x_{t}^{(i)})w_{t-1}^{(i)}\), to incorporate the latest observation \(y_{t}\) into the posterior. The updated weights are then normalized such that \(_{i=1}^{N}w_{t}^{(i)}=1\). However, for complex observations like images or LiDAR, learning accurate generative models \(p(y_{t}|x_{t})\) is extremely challenging. Recent work [8; 12; 13; 14; 15] has instead learned _discriminative_ PFs parameterized by differentiable (typically, deep neural network) _measurement models_:

\[w_{t}^{(i)} l(x_{t}^{(i)};y_{t})w_{t-1}^{(i)}.\] (2)

Here, \(l(x_{t};y_{t})\) scores particles to minimize a loss, such as negative-log-likelihood or squared-error, in the prediction of true target states \(x_{t}\) that are observed during training.

### Particle Resampling

The stochastic nature of PF dynamics causes some particles to drift towards states with low posterior probability. These low-weight particles do not usefully track the true system state, wasting computational resources and reducing the expressiveness of the overall approximate posterior.

Particle resampling offers a remedy by drawing a new uniformly weighted particle set \(_{t}^{(:)}\) from \(x_{t}^{(:)}\), with each particle duplicated (or not) proportional to its current weight \(w_{t}^{(i)}\). The simplest

Figure 1: _Left:_ Our MDPS method showing the forward and backward particle filters, which are integrated (via learned neural networks, indicated by trapezoids) to produce a smoothed mixture posterior. _Right:_ Feature encoders and measurement model used for global localization. First-person camera views are encoded into a Birds-Eye-View (BEV) feature map by extracting features before applying a geometric projection . Map features are extracted via a feed-forward encoder, and un-normalized particle weights are computed as an inner product between BEV features and features of a local map extracted from the global map at the particle location.

_multinomial_ resampling strategy  chooses particles independently with replacement:

\[_{t}^{(i)}=x_{t}^{(j)}, j(w_{t}^{(1)},,w_{t }^{(N)}).\] (3)

To maintain an unbiased posterior, resampled particles have weight \(_{t}^{(i)}=\). Multinomial resampling may be implemented  by drawing a continuous \((0,1)\) variable for each particle, and transforming these draws by the inverse _cumulative distribution function_ (CDF) of particle weights.

_Stratified_ resampling  reduces the variance of conventional multinomial resampling, by first partitioning the interval \((0,1]\) into \(N\) sub-intervals \((0,]...(1-,1]\). One uniform variable is then sampled within each sub-interval, before transforming these draws by the inverse CDF of particle weights. Our differentiable PS incorporate stratified resampling to reduce variance with negligible computational overhead, making training more robustly avoid local optima (see Fig. 2).

While other methods like _residual_ resampling  have been proposed in the PF literature, this partially-deterministic approach is less robust than stratified resampling in our experiments (see Fig. 2), and also much slower because residual resampling cannot be parallelized across particles.

For our mixture density PS, particles are resampled from a continuous Gaussian mixture, in which all components share a common standard deviation \(\). This resampling can equivalently be expressed as \(_{t}^{(i)}=_{t}^{(i)}+_{t}^{(i)}\), where \(_{t}^{(i)} N(0,I)\) and \(_{t}^{(i)}\) is generated via discrete sampling of the mixture component means. We incorporate stratified resampling in this step to boost performance.

### Differentiable Approximations of Discrete Resampling

For discriminative PFs to effectively learn to propagate state estimates over time, gradients are needed for all steps of the PF. While differentiable dynamics and measurement models are easily constructed via standard neural-network architectures, discrete particle resampling is _not_ differentiable.

**Truncated-Gradient Particle Filters (TG-PF)**, the first so-called "differentiable" particle filter, actually treated the resampling step as non-differentiable and simply truncated gradients to zero at resampling, preventing _back-propagation through time_ (BPTT) . Due to this weakness, dynamics models were assumed known rather than learned, and measurement models were learned from biased gradients that fail to propagate information over time, reducing accuracy .

**Soft Resampling Particle Filters (SR-PF)** utilize a differentiable resampling procedure that sets particle resampling weights to be a mixture of the true weights and a discrete uniform distribution:

\[_{t}^{(i)}=x_{t}^{(j)}, j(v_{t}^{(1)},,v_{t}^ {(N)}), v_{t}^{(i)}=(1-)w_{t}^{(i)}+.\] (4)

Gradients are then propagated via the resampled particle weights defined as:

\[_{t}^{(i)}=^{(i)}}{(1-)w_{t}^{(i)}+/N}, _{}_{t}^{(i)}=_{}^{ (i)}}{(1-)w_{t}^{(i)}+/N}.\] (5)

This simple approach resamples low-weight particles more frequently, degrading performance. The gradients of Eq. (5) also have substantial bias, because they incorrectly assume model perturbations only influence the particle weights in (5), and not the discrete particle resampling in (4).

**Relaxations of Discrete Resampling.** While discrete particle resampling could potentially be replaced by continuous particle interpolation with samples from a Gumbel-Softmax or Concrete distribution , no work has successfully applied such relaxations to PFs, and experiments in Younis and Sudderth  show very poor performance for this baseline. Alternatively, _entropy-regularized optimal transport particle filters_ (OT-PF)  replace discrete resampling with an entropy-regularized optimal transport problem, that minimizes a Wasserstein metric to determine a probabilistic mapping between the weighted pre-resampling particles and uniformly weighted post-resampling particle. OT-PF performance is sensitive to a non-learned entropy regularization hyperparameter, and the biased gradients induced by this regularization may substantially reduce performance . Furthermore, "fast" Sinkhorn algorithms  for entropy-regularized OT still scale quadratically with the number of particles, and in practice are orders of magnitude slower than competing resampling strategies. This makes OT-PF training prohibitively slow on the challenging city-scale localization tasks considered in this paper, so we do not compare to it.

### Mixture Density Particle Filters

_Mixture Density Particle Filters_ (MDPF)  are a differentiable variant of regularized PFs [40; 41]. MDPF estimates a continuous _kernel state density_ by convolving particles with a continuous, and differentiable, kernel function \(K\) (such as a Gaussian) with bandwidth hyperparameter \(\):

\[m(x_{t} x_{t}^{(i)},w_{t}^{(i)},)=_{i=1}^{N}w_{t}^{(i)}K(x_{t}-x_{ t}^{(i)};).\] (6)

Particles are then resampled \(_{t}^{(i)} m(x_{t} x_{t}^{(i)},w_{t}^{(i)},)\) from this continuous mixture instead of via discrete resampling. Unbiased and low-variance _Importance Weighted Sample Gradient_ (IWSG)  estimates may then be constructed by viewing the particle proposal \(q(z)=m(z_{0})\) to be fixed to the mixture model parameters \(_{0}=\{x_{t}^{(i)},w_{t}^{(i)},\}\) at the current training iteration. Gradients then account for parameter perturbations _not_ by perturbing particle locations as in standard reparameterization [29; 30; 31], but by perturbing particle importance weights away from uniform:

\[^{(i)}=)_{=_{0}}}{m(z^{(i)} _{0})}=1,_{}^{(i)}=m(z^{( i)})_{=_{0}}}{m(z^{(i)}_{0})}.\] (7)

With this approach, the bandwidth parameter \(\) may also be tuned for end-to-end prediction of state distributions, avoiding the need for classic bandwidth-selection heuristics [42; 43; 44]. An "adaptive" variant of MDPF  incorporates two bandwidths, one for particle resampling (to propagate information over time) and a second for estimation of state posteriors (to compute the loss). Our MDPS also incorporates separate bandwidths for resampling and state estimation, as detailed below.

## 3 From Filtering to Smoothing

Particle smoothers extend PFs to estimate the state posteriors \(p(x_{t}|y_{1:T})\) given a full \(T\)-step sequence of observations. (To simplify equations, we do not explicitly condition on actions \(a_{1:T}\) in the following two sections.) Particle smoothers continue to approximate posteriors via a collection of particles \(_{t}^{(1:N)}\) with associated weights \(_{t}^{(1:N)}\), where we use bi-directional overhead arrows to denote smoothed posteriors. Classical particle smoothing algorithms, which are non-differentiable and typically assume human-engineered dynamics and likelihoods, fall into two broad categories.

**Forward-Filtering, Backward Smoothing (FFBS)** algorithms [24; 25] compute \(p(x_{t}|y_{1:T})\) by factoring into forward filtering and backward smoothing components:

\[p(x_{t}|y_{1:T})= p(x_{t},x_{t+1}|y_{1:T})dx_{t+1}=| y_{1:t})}_{}|y_{1:T})p(x_{t+1}|x_{t})}{ p(x_{t+1}|x_{t})p(x_{t}| y_{1:t})}dx_{t+1}}_{}.\] (8)

A natural algorithm emerges from Eq. (8), where a conventional PF first approximates \(p(x_{t}|y_{1:t})\) for all times via particles \(_{t}^{(1:N)}\) with weights \(_{t}^{(1:N)}\). A backward smoother then recursively reweights

Figure 2: Box plots showing median (red line), quartiles (blue box), and range (whiskers) over 11 training runs for Bearings-Only tracking (Sec. 5.1). We boost the robustness of the top-performing MDPF , which previously used multinomial resampling, by incorporating variance-reduced stratified resampling; residual resampling is both slower and less effective. Stratified resampling provides larger advantages for the less-sophisticated TG-PF  and SR-PF  gradient estimators, but these baselines remain inferior to MDPF. Our MDPS substantially improves on all PFs by incorporating both past and future observations when computing posteriors. Classic FFBS particle smoothers [24; 25] have poor performance, even when provided the true likelihoods (rather than a learned approximation), showing the effectiveness of our end-to-end learning of particle proposals and weights. Forward PFs are initialized with noisy samples of the true state, while MDPF-Backward (the backwards-time PF component of MDPS) is initialized by sampling uniformly from the state space.

the "forward" particles to account for future data, but does _not_ change particle locations:

\[_{t}^{(i)}_{t}^{(i)}_{j=1 }^{N}_{t+1}^{(j)}_{t+1}^{(j)}| _{t}^{(i)})}{_{k=1}^{N}_{t}^{(k)}p( _{t+1}^{(j)}|_{t}^{(k)})}.\] (9)

Because FFBS sets \(_{t}^{(i)}=_{t}^{(i)}\), it is only effective when filtered state posteriors \(p(x_{t}|y_{1:t})\) substantially overlap with smoothed posteriors \(p(x_{t}|y_{1:T})\); performance deteriorates when future data is highly informative. FFBS also requires explicit evaluation, not just simulation, of the state transition dynamics \(p(x_{t+1}|x_{t})\), which is not tractable for dynamics parameterized as in Eq. (1).

**Two Filter Smoothing (TFS)** algorithms [22; 24; 25] instead express the smoothed posterior as a normalized product of distinct forward-time and backward-time filters:

\[p(x_{t}|y_{1:T})=|y_{1:t})p(y_{t+1:T}|x_{t})}{p(y_{t+1:T}|y_{1:t}) } p(x_{t}|y_{1:t})p(y_{t+1:T}|x_{t}).\] (10)

Here \(p(x_{t}|y_{1:t})\) may be approximated by a standard PF, and \(p(y_{t+1:T}|x_{t})\) is the so-called _backward information filter_[25; 24] defined as

\[p(y_{t:T}|x_{t})= p(y_{t+1:T}|x_{t+1})p(x_{t+1}|x_{t})p(y_{t}|x_{t})dx_{t+ 1}.\] (11)

Because \(p(y_{t:T}|x_{t})\) is a likelihood function rather than a probability density in \(x_{t}\), and it is possible that \( p(y_{t:T}|x_{t})dx_{t}=\). This is not an issue when \(p(y_{t:T}|x_{t})\) is computed analytically as in Kalman smoothers for Gaussian models , but particle-based methods can only hope to approximate finite measures. Bresler  addresses this issue via an _auxiliary_ probability measure \(_{t}(x_{t})\):

\[p(y_{t:T}|x_{t})(x_{t}|y_{t:T})}{_{t}(x_{t})}, (x_{t:T}|y_{t:T})_{t}(x_{t})p(y_{t}|x_{t})_{ s=t+1}^{T}p(x_{s}|x_{s-1})p(y_{s}|x_{s}).\] (12)

From Eqs. (10,12), the smoothed posterior is a reweighted product of forward and backward filters:

\[p(x_{t}|y_{1:T})|y_{1:t})}^{}}{_{t}(x_{t})}.\] (13)

This suggest an algorithm where two PFs are run on the sequence independently, one forward and one backward in time, to compute forward particles \(\{_{t}^{(1:N)},_{t}^{(1:N)}\}\) and backward particles \(\{_{t}^{(1:N)},_{t}^{(1:N)}\}\). Because continuously sampled forward and backward particle sets will not exactly align, classic TFS integrate these two filters by rewriting Eq. (13) as follows:

\[p(x_{t}|y_{1:T})|x_{t})(x_{t}|y_{t+1:T}) p(x_ {t}|x_{t-1})p(x_{t-1}|y_{1:t-1})dx_{t-1}}{_{t}(x_{t})}.\] (14)

This yields a particle re-weighting approach where backward filter particles \(_{t}^{(1:N)}\) are re-weighted using the forward filter particle set, to produce the final smoothed particle weights \(_{t}^{(1:N)}\):

\[_{t}^{(i)}_{t}^{(i)}_{j=1}^{N} _{t-1}^{(j)}_{t}^{(i)}| _{t-1}^{(j)})}{_{t}(_{t}^{(i)})}, _{i=1}^{N}_{t}^{(i)}=1.\] (15)

Conventional TFS set \(_{t}^{(1:N)}=_{t}^{(1:N)}\), which similar to FFBS, makes performance heavily dependant on significant overlap in support between \(p(x_{t}|y_{t+1:T})\) and \(p(x_{t}|y_{1:T})\). Like FFBS, TFS also restrictively requires evaluation (not just simulation) of the state transition dynamics.

## 4 Mixture Density Particle Smoothers

Our novel _Mixture Density Particle Smoother_ (MDPS, Fig. 1) can be seen as a differentiable TFS, where the forward and backward filters of Eq. (13) are defined as MDPFs (Sec. 2.3). Using discriminative differentiable particle filters (MDPFs) within the TFS frameworks, and replacing Eq. (14) with an importance-weighted integration of forward and backward particles, enables an effective and end-to-end differentiable particle smoother. We begin by rewriting Eq. (13) as

\[p(x_{t}|y_{1:T})|x_{t})p(x_{t}|y_{1:t-1})(x_{t}|y_{t +1:T})}{_{t}(x_{t})},\] (16)where the forward and backward filters no longer condition on the current observation. This allows for functionally identical MDPs to be used for both directions, simplifying implementation. MDPFs parameterize state posteriors as continuous kernel density mixtures:

\[p(x_{t}|y_{1:t-1})=_{i=1}^{N}_{t}^{(i)}K(x_{t}- _{t}^{(i)};), p(x_{t}|y_{t+1:T} )=_{i=1}^{N}_{t}^{(i)}K(x_{t}-_{t}^{( i)};).\] (17)

Unlike discrete probability measures, these continuous mixture distributions can be combined via direct multiplication to give a smoothed posterior mixture containing \(N^{2}\) components, one for each pair of forward and backward particles. For this product integration of forward and backward filters, the normalizing constants for all pairs of kernels must be explicitly computed to correctly account for the degree to which hypotheses represented by forward and backward particles are consistent. These normalization constants are tractable for some simple kernels including Gaussians , but more complex for the other cases such as von Mises kernels of orientation angles [47; 48].

Direct mixture multiplication eliminates the need to evaluate the dynamics model, as in classic TFS, but introduces significant overhead due to the quadratic scaling of the number of mixture components. To address this issue, our MDPS uses importance sampling where the smoothed posterior is defined by \(M N^{2}\) particles drawn from a mixture of the filter posteriors:

\[_{t}^{(i)} q(x_{t})=p(x_{t}|y_{1:t-1})+ {1}{2}p(x_{t}|y_{t+1:T}), i=1,,M.\] (18)

By construction, this proposal will include regions of the state space that lie within the support of _either_\(p(x_{t}|y_{1:t-1})\) or \(p(x_{t}|y_{t+1:T})\), improving robustness. Our experiments set \(M=2N\), drawing \(N\) particles from each of the filtered and smoothed posteriors. Given true dynamics and likelihood models, importance sampling may correct for the fact that smoothed particles are drawn from a mixture rather than a product of filtered densities, as well as incorporate the local observation:

\[_{t}^{(i)}|_{t}^{(i)})p (_{t}^{(i)}|y_{1:t-1})(_{t}^{(i )}|y_{t+1:T})}{_{t}(_{t}^{(i)})q(_{t} ^{(i)})},_{i=1}^{M}_{t}^{(i)}=1.\] (19)

To more easily train a discriminative PS, rather than estimating each term in Eq. (19) separately, we directly parameterize their product via a feed-forward neural network \(l()\):

\[_{t}^{(i)}_{t}^{(i)};y_{t},p (_{t}^{(i)}|y_{1:t-1}),(_{t}^{( i)}|y_{t+1:T}))}{q(_{t}^{(i)})},_{i=1}^{M} _{t}^{(i)}=1.\] (20)

The posterior weight network \(l()\) scores particles based on agreement with \(y_{t}\), as well as consistency with the forward and backward filters, and implicitly accounts for the auxiliary distribution \(_{t}()\). To allow state prediction and compute the training loss, the smoothed posterior is approximated as:

\[p(x_{t}|y_{1:T}) m(x_{t}|_{t}^{(:)},_ {t}^{(:)},^{(:)})=_{i=1}^{N}_{t}^{(i )}K(x_{t}-_{t}^{(i)};^{(:)}),\] (21)

where \(^{(:)}\) is a learned, dimension-specific bandwidth parameter.

Figure 3: Position and error recall using the MGL  dataset. Recall is computed with the top posterior mode as well as with the best of the top-3 posterior modes, extracted via non-maximal suppression. As expected, Retrieval  methods do poorly due to their lack of discrimination power between neighboring map patches. Dense search  does better by using fine map details during localization, but it requires a ground truth hint (“Cheating” with GT, which artificially improves performance) to work well at city-scale environments. Retrieval (PF)  uses unlearned state dynamics, which proves useful, but still suffers from the poor discriminative ability of retrieval. In contrast, MDPF  uses end-to-end learned dynamics and measurement models, allowing for good performance but suffering from only using past information when estimating posterior densities. Our MDPS is able to learn similar strong dynamics and measurement models as MDPF, and also incorporates future as well as past information to achieve a more accurate posterior density and thus higher recall.

**Training Loss and Gradient Computation**. We discriminatively train our MDPS by minimizing the negative log-likelihood of the true state sequence:

\[=_{t T}-(m(x_{t}|_{t}^{( :)},_{t}^{(:)},^{(:)})).\] (22)

During training, the IWSG estimator of Eq. (7) provides unbiased estimates of the gradients of the forward and backward resampling steps. We may similarly estimate gradients of the mixture resampling (18) that produces smoothed particles, enabling the first end-to-end differentiable PS:

\[_{}_{t}^{(i)}l( _{t}^{(i)};y_{t},p(_{t}^{(i)}|y _{1:t-1}),(_{t}^{(i)}|y_{t+1:T}))}{q( _{t}^{(i)})}.\] (23)

**Training Details.** Because the smoother weights of Eq. (20) cannot be effectively learned when filter parameters are random, we train MDPS via a three-stage procedure. In stage 1, the forward and backward PFs are trained separately (sharing only parameters for the encoders, see Fig. 1) to individually predict the state. In stage 2, the PFs are frozen and the particle smoother measurement model \(l()\) of Eq. (20) is trained. In stage 3, all models are unfrozen and trained jointly to minimize the loss in the MDPS output state posterior predictions of the true states. The forward MDPF, backward MDPF, and MDPS posterior each have separate kernel bandwidths (\(\), \(\), \(\)) that are jointly learned with the dynamics and measurement models. We randomly resample a stochastic subset of the training sequences for each step, and adapt learning rates via the Adam  optimizer.

**Computational Requirements.** At training time, to allow unbiased gradient propagation, MDPS computes importance weights for each particle during resampling. For \(N\) particles and \(T\) time-steps, this requires \((TN^{2})\) operations. At inference time, importance weighting is not needed as the particle weights can simply be set as uniform, and resampling only requires \((TN)\) operations. All phases of our MDPS scale linearly with \(N\) at test time, in contrast with other differentiable relaxations such as OT-PF , which requires \((TN^{2})\) operations for both training and inference.

## 5 Experiments

We evaluate our MDPS on a synthetic bearings-only tracking task , as well as on real-world city-scale global localization. For all tasks, we estimate the MDPF/MDPS posterior distributions of a 3D (translation and angle) state \(x_{t}=(x,y,)\), using Gaussian kernels for the position dimensions, and von Mises kernels for the angular dimensions of the state posterior mixtures.

Figure 4: Example trajectories from the MGL dataset with observations shown in the top row. We show the current true state and state history (black arrow and black line), the estimated posterior density of the current state (red cloud, with darker being higher probability) and the top 3 extracted modes (blue arrows) for the MDPS as well as its forward and backward MDPFs. Due to ambiguity at early time-steps, MDPF  is unable to resolve the correct intersection, and instead places probability mass at multiple intersections. By fusing both forward and backward filters, our MDPS resolves this ambiguity with probability mass focused on the correct intersection. Furthermore, MDPS provides a tighter posterior density than either MDPF-Forward or MDPF-Backward.

### Bearings Only Tracking Task

To allow comparison to prior discriminative PFs, we use the same bearings-only tracking task as , where the 3D state of a variable-velocity synthetic vehicle is tracked via noisy bearings from a fixed-position radar. 85% of observations are the true bearing plus von Mises noise, while 15% are uniform outliers. Train and evaluation sequences have length \(T=50\). Unlike Younis and Sudderth , we find truncated BPTT  is not necessary if bandwidths are initialized appropriately. Filtering particles are initialized as the true state with added Gaussian noise, while MDPF-Backward (and the MDPS backwards filter) are initialized with uniformly sampled particles to mimic datasets where often only the starting state is known. More details can be found in the Appendix.

We compare our MDPS methods to several existing differentiable particle filter baselines, but no differentiable particle smoother baseline exists. Instead, we implement the classic FFBS [24; 25] algorithm (Sec. 3), which assumes known dynamics and measurement models. Since FFBS is not differentiable, we learn the dynamics model using the dataset true state pairs \(\{x_{t-1},x_{t}\}\) outside of the FFBS algorithm. In order to simulate from and evaluate the state transition dynamics, as needed by the FFBS, we parameterize the dynamics model to output a mean and use a fixed bandwidth parameter (tuned on validation data) to propose new particles. We also use the true observation likelihood as the measurement model, instead of a learned approximation; this boosts FFBS performance.

**Results.** In Fig. 2 we show statistics of performance over 11 training and evaluation runs for each method. We compare to TG-PF , SR-PF , the classical FFBS [24; 25], and MDPF . Interestingly, MDPF outperforms SR-PF and TG-PF even when the initial particle set is drawn uniformly from the state space as in MDPF-Backward.

By incorporating more temporal data, MDPS substantially outperforms MDPF. Even when unfairly provided the true observation likelihood, FFBS performs poorly since particles are simply re-weighted (not moved) by the backward smoother. This inflexibility, and lack of end-to-end learning, makes FFBS less robust to inaccuracies in the forward particle filter.

We are the first to compare resampling variants in the context of modern discriminative PFs. Stratified resampling substantially improves TG-PF and SR-PF performance, but only modestly improves the worst-performing MDPF runs. This may be because even with basic multinomial resampling, the lower-variance MDPF gradients dramatically outperform all TG-PF and SR-PF variants. Residual resampling performs worse than stratified resampling, and is also much slower since it cannot be easily parallelized on GPUs, so we do not consider it for other datasets.

### City Scale Global Localization Task

Our global localization task is adopted from Sarlin et al. , where we wish to estimate the 3D state (position and heading) of a subject (person/bike/vehicle) as it moves through real-world city-scale environments. Observations are gravity-aligned first-person images, actions are noisy odometry, and a 2D planimetric map is provided to help localize globally. We use the Mapillary Geo-Localization  and KITTI  datasets to compare our MDPS method to MDPF  as well as other methods specifically designed for the global localization task, which are not sequential Monte Carlo methods.

Our global localization task is distinct from local localization systems, which aim to track subject positions relative to their starting position, instead of in relation to the global map origin. Visual SLAM systems  almost exclusively solve the local localization task, using the starting position as the origin of their estimated map. If a map is provided, then just the localization part of Visual SLAM can be run, but detailed visual or 3D maps of the environment are needed. These have prohibitive memory requirements at city-scales, and need constant updating as the visual appearance of the environment changes (e.g., with the weather/seasons) . Hybrid place recognition with localization

Figure 5: Learned dynamics from the forward filter of MDPS trained on the MGL dataset. Density cloud illustrates density of particles after applying dynamics while marginalizing actions. MDPS clearly learns informative, non-linear dynamics models which aid in state posterior estimation.

[MISSING_PAGE_FAIL:9]

**Computational Requirements.** While MDPS is more accurate than other methods, it is also more efficient than dense search. Because dense search must try many options to find the best alignment of the BEV and extracted map features, it has complexity \((KW^{2}H^{2})\) for \(K\) search rotations, and search locations defined on a grid of width \(W\) and height \(H\). (For notational simplicity, we assume the BEV features and the map features have the same width and height.) This complexity can be reduced to \((KWH(WH))\) by using the Fast Fourier Transform. In contrast, MDPS has complexity \((NWH)\), where \(N K(WH)\), as it only compares the BEV and map features at the particle locations. This allows MDPS to better scale to large-scale environments.

### KITTI Dataset

We also evaluate our MDPS method for the global localization task using the KITTI  dataset, where observations are forward-facing camera images from a moving vehicle. We augment this datatset with noisy odometry computed from the ground truth states and use the default _Train_, _Test1_, and _Test2_ splits for training, validation, and evaluation respectively. Like MGL, a planimetric map of the environment is provided via the OpenStreetMap platform  at 0.5 meter/pixel resolution. Due to the small size of the KITTI dataset, we pre-train all methods using MGL before refining on KITTI, using the same network architectures as was used for the MGL dataset. See Appendix for details.

**Results.** Due to the forward-facing camera, the observation images lack visual features for useful localizing along the roadway, therefore we decouple the position error into lateral and longitudinal errors when reporting recalls in Fig. 6. Understandably, all methods have larger longitudinal error than lateral error. Interestingly, MDPF and MDPS offer similar Top 3 mode performance for small lateral errors (under 2 meters) while significantly outperforming all other methods. When the lateral error is greater than 2 meters, MDPS sees a performance gain as it maintains a more diverse set of posterior modes, whereas MDPF prematurely collapses the posterior density to incorrect modes.

### Limitations

Like all particle-based methods, our MDPS suffers from the _curse of dimensionality_ where particle sparsity increases as the dimension of the state space increases, reducing expressiveness of state posteriors. More effective use of particles via smarter dynamics and measurement models, as enabled by end-to-end MDPS training, can reduce but not eliminate these challenges.

## 6 Discussion

We have developed a fully-differentiable, two-filter particle smoother (MDPS) that robustly learns more accurate models than classic particle smoothers, whose components are often determined via heuristics. MDPS successfully incorporates temporal information from the past as well as the future to produce more accurate state posteriors than state-of-the-art differentiable particle filters. When applied to city-scale global localization from real imagery, our learned MDPS dramatically improves on search and retrieval-based baselines that were specifically engineered for the localization task.

Figure 6: _Left:_ Lateral and longitudinal errors are in the vehicle frame of reference. _Right:_ Position recall versus error for the KITTI  dataset. Recall is computed with the top posterior mode as well as with the best of the top-3 posterior modes. Longitudinal localization performance is poor for all methods due to lack of visual features. MDPF  offers dramatic improvements for lateral error over Retrieval , Retrieval (PF)  and Dense Search  baselines, even when these baselines are constrained to operate around the ground truth state (“Cheating” with GT). For longitudinal recall, methods using “Cheating” with GT have good performance because they are _artificially_ constrained to be near the true state, and thus have significantly less position ambiguity along the roadway. MDPS offers further improvements over MDPF as it maintains a more diverse set of posterior modes, instead of prematurely collapsing to incorrect modes.