# Embedding-Aligned Language Models

Guy Tennenholtz\({}^{}\), Yinlam Chow\({}^{}\), Chih-Wei Hsu\({}^{}\), Lior Shani\({}^{}\), Ethan Liang\({}^{}\), Craig Boutilier\({}^{}\)

\({}\) Google Research, \(\) Google Deepmind

Correspondence to: guytenn@gmail.com

###### Abstract

We propose a novel approach for training large language models (LLMs) to adhere to objectives defined within a latent embedding space. Our method leverages reinforcement learning (RL), treating a pre-trained LLM as an environment. Our _embedding-aligned guided language_ (EAGLE) agent is trained to iteratively steer the LLM's generation towards optimal regions of the latent embedding space, w.r.t. some predefined criterion. We demonstrate the effectiveness of the EAGLE agent using the MovieLens 25M and Amazon Review datasets to surface content gaps that satisfy latent user demand. We also demonstrate the benefit of using an optimal design of a state-dependent action set to improve EAGLE's efficiency. Our work paves the way for controlled and grounded text generation using LLMs, ensuring consistency with domain-specific knowledge and data representations.

## 1 Introduction

Large language models (LLMs) such as Gemini (Team et al., 2023) and GPT (Achiam et al., 2023) have revolutionized the field of natural language processing, achieving remarkable success in text generation, translation, comprehension, as well as expert-level performance on challenging tasks (e.g., exams, coding). However, effectively applying (or fine-tuning) LLMs to _domain-specific_ tasks often requires considerable domain knowledge and labeled human data (Jeong et al., 2023; Ouyang et al., 2022; Ziegler et al., 2019). Fortunately, in many cases, domain knowledge is already captured and encoded within _latent embedding spaces_.

Latent embeddings are continuous vectors, ubiquitous in fields such as recommender systems (Hansen et al., 2020), reinforcement learning (RL) (Nabati et al., 2023; Pertsch et al., 2021), and image classification (Girdhar et al., 2023; Radford et al., 2021). They offer a powerful means to represent entities, concepts, and relationships within a specific domain. For example, in recommender systems, embeddings of items and users encapsulate information about preferences and behavior (Zhao et al., 2023), embeddings of images can capture their content and style (Radford et al., 2021), while embeddings of scientific articles can represent their research area and findings (Taher Harikandeh et al., 2023). The utility of a latent embedding lies in its underlying compact representation of entities, concepts or relationships, and the associated metrics, allowing one to construct simpler, more efficient models or induce control over various processes (Arvanitidis et al., 2018, 2021; Radford et al., 2021; Tennenholtz and Mannor, 2022). Importantly, latent embeddings are often pre-computed, and can thus serve as a readily available rich source of domain knowledge.

This leads to the key question: **can we leverage latent embeddings to better control and guide LLM generation?** In this paper, we present a novel framework which accomplishes this by exploiting latent embedding spaces to define an objective function for an LLM in an iterative RL-driven process.

As an example, consider the challenge of assisting content creators in generating valuable content within a recommender ecosystem (e.g., YouTube, Reddit, Spotify) (Boutilier et al., 2024). Animportant aspect of this problem includes identifying and surfacing of _content gaps_, i.e., identifying hypothetical content which could potentially drive value for users, and subsequently, describing it to creators. Latent embeddings offer an effective way of defining a content gap. Informally, a content gap is a hypothetical (i.e., non-existing) content item, corresponding to some point in a latent embedding space for which: (1) no content currently exists, implying an unexplored area within the existing content landscape, and (2) adding this content would improve overall system welfare (i.e., drive net positive value for both users and creators). While the latent embedding representation facilitates identification of content gaps, describing or _surfacing_ this hypothetical continuous vector to its potential creator requires that we move beyond the latent representation. The content gap embedding must be translated into a description that communicates its essence to a creator in a clear and actionable manner, requiring some way of interpreting the latent embedding.

In this paper, we address the general problem of _aligning LLMs with latent embedding spaces_. We address this by working directly with both the embedding space and the generated outputs of an LLM. We formulate an RL-driven framework which iteratively steers an LLM towards regions of an embedding space deemed optimal according to some predefined criterion (e.g., a content gap). To do so, we use a language-based agent to guide the LLM by modifying a textual representation of an entity. We then embed the resulting description into the latent embedding space to assess its quality w.r.t. our predefined criterion. Finally, we use this feedback to guide the LLM's next modification, steering it closer to optimal regions of the latent space.

Our contributions are as follows: (1) We propose an Embedding-Aligned Guided LanguageE (EAGLE) agent, which aligns an LLM with a latent embedding space; (2) We provide a comprehensive framework for novel content creation using LLMs, formulating a new technique for designing exploratory and high quality action sets by leveraging the power of LLMs, latent embeddings, and _G-optimal design_(Zhu et al., 2022); (3) We validate the effectiveness of our approach on the MovieLens 25M dataset (Harper and Konstan, 2015), aligning creation to behavioral movie and user embeddings. Our results are evaluated by human raters demonstrating EAGLE's ability to guide an LLM to generate text that is both creative and consistent with the domain's data representations.2

## 2 Preliminaries and Related Work

**Large Language Models (LLMs).** An LLM \(:_{}\) maps sequence of tokens to probabilities over sequences of tokens. Transformers (Vaswani et al., 2017) are often used to train such LLMs over vast amounts of data, predicting the next token \(x_{t}\) in a sequence given the preceding tokens (\(x_{1},x_{2},,x_{t-1}\)), by minimizing the cross-entropy loss. Pre-trained LLMs often vary in size, with larger models having greater capabilities (Achiam et al., 2023; Team et al., 2023; Touvron et al., 2023); e.g., Llama 2 (Touvron et al., 2023) models have 7B, \(13\)B, and \(70\)B parameter variants.

**Embedding Language Models (ELMs).** An _embedding language model (ELM)_(Tennenholtz et al., 2024) combines textual prompts with vector representations of domain knowledge. It uses adapter layers to connect embedding inputs to a text-only pre-trained LLM. By training the model on these combined inputs, ELM generates language that reflects the information contained in the domain embedding vectors, even for hypothetical entities (Tennenholtz et al., 2024).

**Reinforcement Learning (RL).** A _Markov decision process (MDP)_ is a tuple \((,,P,r,H,)\), where \(\) is the state space, \(\) is the action space, \(P:_{}\) is the transition function, \(r:\) is the reward function, \(H\) is the horizon, and \(\) is the discount factor. An _agent_\(:_{}\) is a stationary stochastic policy mapping states to a distribution over actions.3 Its value at state \(s\), \(V^{}(s)=_{P,}_{t=0}^{H-1}^{t}(s_{t},a_{t})\, \,s_{0}=s\,\), is its expected discounted return starting at \(s\). The objective in RL is to compute an _optimal policy_, which maximizes value over some initial state distribution \(_{0}\), i.e., \(^{*}_{}_{s_{0}_{0}}[V^{}(s_{0})]\).

_Policy gradient (PG)_ methods are a class of RL algorithms that directly optimize the policy to maximize expected return (Schulman et al., 2017; Sutton et al., 1999). In LLM tasks, PG often uses a reference policy \(_{}\) as an anchor to regularize the RL task (e.g., stay close to \(_{}\)). This is the prevalent approach in _reinforcement learning from human feedback_(RLHF, Ouyang et al. (2022)). Inthis work we achieve this using a KL-divergence loss over policy outputs; i.e.,

\[()=()- D_{KL}(||_{}).\] (1)

**Related Work.** Generative models are powerful tools for creative generation of text, images, and more (Achiam et al., 2023; Ho et al., 2020, 2022; Team et al., 2023). Embedding representations (Arvanitidis et al., 2018, 2021; Oord et al., 2018; Yang et al., 2017) have been applied in many fields, including recommender systems (Liang et al., 2018), healthcare (Rampasek et al., 2019), neuroscience (Keshtkaran and Pandarinath, 2019), genetics (Frazer et al., 2021), astronomy (Ravanbakhsh et al., 2017), and more (Gomez-Bombarelli et al., 2018; Kingma et al., 2014; Thadani et al., 2023).

Aligning LLM generation with embedding spaces holds great potential. Our work constrains the generation process to work with a predefined utility over an embedding space. Other methods have been proposed to align language models to knowledge graphs (Bao et al., 2016), safety constraints (Ji et al., 2024; Yang et al., 2021), and human preferences (Bakker et al., 2022; Ouyang et al., 2022; Wang et al., 2024). Our approach can be generalized to such tasks with a suitable latent embedding space and utility specification.

The use of RL with human and AI feedback for LLM fine-tuning has been shown to induce significant improvement in LLM capabilities (Lee et al., 2023; Ouyang et al., 2022). Embedding-driven guidance is an additional form of feedback that can further improve creative LLM generation, and potentially reduce hallucination (Dhuliawala et al., 2023; Gunjal et al., 2024; Li et al., 2023).

## 3 Problem Setup

We assume an _ambient entity space_\(\), which represents the space of all possible entities (e.g., all movie plots, or all animal photos). Since our setup is agnostic to the modality of the space \(\), we do not emphasize language entities in this section. We assume \(\) is equipped with a metric \(d:_{+}\), which may not be known. We also assume access to a _latent embedding_\(E_{D}:\), which maps entities in \(\) to a latent embedding space \(^{n}\). If \(E_{D}\) is injective, it induces a metric \(d_{}(z,w)=d(E_{D}^{-1}(z),E_{D}^{-1}(w))\) over \(\). Otherwise, we define the metric \(d_{}(z,w)=_{x E_{D}^{-1}(z),y E_{D}^{-1}(w)}d(x,y)\) for \(z w\), and \(d_{}(z,z)=0\).4 Let \((,d_{})\) be the embedding manifold. Though \(d_{}\) is not known, various methods exist to estimate the underlying metric (e.g., see Appendix B on the use of Riemannian manifolds and pullback metrics of generative models to estimate \(d\)(Arvanitidis et al., 2018, 2021)).

Given a fixed set of entities \(=\{x_{i}\}_{i=1}^{N}\) of size \(N\) (i.e., a dataset), we define a utility function \(U:\) over embeddings on the manifold \((,d_{})\). Below we illustrate an example of a utility function for quantifying the quality of a _content gap_.

**Example: Content Gaps.** Consider a utility function \(U\) measuring if a hypothetical point \(z\) is a content gap in a recommender ecosystem (Boutilier et al., 2024). We might define \(U\) as:

\[U(z;)=}U_{}(z; )}_{}+}U_{}(z; )}_{}+}U_{d}(d_{ }(E_{D}(x),z))}_{}.\] (2)

Here \(\) is the set of _existing_ content items, and \(U_{},U_{},U_{d}\) are the utility functions for users, creators, and distance, defined over the latent embedding space. That is, a content gap \(z\) should bring high utility to users and creators, while also meeting some latent demand (i.e., non-existing content). In Sec. 5 we explicitly formulate this utility for a behavioral latent embedding space trained over movie and user data.

   \(\) & Ambient space (e.g., descriptions of movies) & \(d:_{+}\) & Ambient metric \\ \(\) & Latent embedding space & \(d_{}:_{+}\) & Latent metric \\ \(\) & Action space (e.g., changes to a movie plot) & \(U:_{+}\) & Utility function \\ \(n\) & Latent dimension & \(E_{D}:\) & Latent encoder \\ \(\) & Dataset of entities & \(_{}:_{}\) & Reference policy \\ \(H\) & Horizon & \(q:_{}\) & reference policy distribution \\   

Table 1: GlossaryOptimality Criterion.Given the dataset of entities \(=\{x_{i}\}_{i=1}^{N}\), embedding manifold \((,d_{})\), a corresponding embedding function \(E_{D}:\), and a utility function \(U:\), our goal is to find a novel entity \(x^{*}\) such that

\[x^{*}_{x,x}U(E_{D}(x);).\] (3)

We emphasize that, while optimality is defined in the _latent embedding_ space, our goal is to identify a novel entity \(x^{*}\) in the _ambient_ space \(\) (e.g., an image or description of the hypothetical). As such, it is not enough to search for an optimal point in \(\). Moreover, the solution is not necessarily unique.

## 4 Surfacing Optimal Entities Using LLMs

We now turn to generation of novel entities in language space; that is, we assume \(\) is a subset of the space of all language sequences (e.g., plots of movies, or descriptions of clothes). Nevertheless, our methods are applicable more broadly to other modalities including images, videos, audio, or even RL policies. In what follows we describe two methods of finding an optimal entity \(x^{*}\) in Eq. (3), using either ELMs or RL. A high-level illustration of these methods is shown in Fig. 1.

### Embedding Language Models (ELMs) as Decoders

ELMs (Sec. 2) offer an explicit method of finding an optimal entity \(x^{*}\). Indeed, if one can train a decoder \(f:\) which satisfies \(E_{D}(f(z))=z\), then \(x^{*}\) can be determined by maximizing \(z^{*}_{z}U(z;)\) (i.e., identifying an optimal latent embedding w.r.t. \(U\)) and returning \(x^{*}=f(z^{*})\) (i.e., some entity corresponding to \(z^{*}\)). Such a decoder \(f\) can be trained using a fixed dataset and an ELM architecture (Tennenholtz et al., 2024).

However, this approach has major limitations. First, learning an ELM decoder \(f\) requires training an LLM to interpret embeddings, a costly process which may negatively impact the expressive power of the LLM. Second, the manifold \((,d_{})\) can be highly complex and non-linear. Moreover, the latent space metric \(d_{}\) may not be known. As such, a latent embedding \(z^{*}\) deemed optimal in latent

Figure 1: An illustration comparing the creation of descriptions of novel entities using ELM (Tennenholtz et al., 2024) vs. EAGLE (ours). The latent embedding space \(\) is illustrated as a complex surface on the right. Red points on the surface are illustrated as latent embeddings of existing entities. Black points are used to illustrate hypothetical (i.e., non-existing) entities. In ELM, a utility is maximized _in latent embedding space_ to identify an optimal point. This hypothetical embedding is then decoded back to ambient space \(\) to form a description of the hypothetical entity. Conversely, the EAGLE agent utilizes a highly capable pre-trained LLM as an environment to search for novel entities in ambient space \(\). EAGLE does not use a decoder, but rather only requires an encoder \(E_{D}:\). More specifically, the EAGLE agent uses action prompts to change an existing entity using the environment LLM. Every new changed entity is then mapped back to the latent embedding space using the encoder \(E_{D}\). The utility function is optimized by the EAGLE agent through a reward signal to stir the changes to areas of high utility in the latent embedding space. A final description is then returned after \(H\) steps.

space, may be "off-manifold", and therefore not mapped to a valid entity \(x\). To mitigate this, one can choose to stay close to points in the data, using Euclidean distance to approximate \(d\)(Chen et al., 2019), or employing a pullback metric defined by a Riemannian manifold (Arvanitidis et al., 2018, 2021) (see Appendix B for further discussion). Finally, the embedding space \(\) may not contain enough semantic information to ensure that \(f\) generalizes well (e.g., embeddings trained over behavioral interactions with items). Training a new embedding, e.g., using Variational Auto-Encoders (VAEs) (Yang et al., 2017) can mitigate this problem; yet, this would require training a new latent embedding, a challenging problem in its own right (Vahdat and Kautz, 2020). Furthermore, training a new latent representation may not be feasible (e.g., due to limited data). As such, using ELMs to train a decoder LLM over the embedding space may be impractical in many domains.

### A Reinforcement Learning Perspective

As an alternative to ELMs, we formulate the optimization directly in the ambient space \(\) (i.e., language space) using RL. Specifically, we leverage a highly capable pre-trained LLM (e.g., Gemini-Ultra (Team et al., 2023), or GPT-4 (Achiam et al., 2023)) to act as an _environment_ for our RL formulation. The environment LLM remains fixed, and is only used to generate new entities in \(\).

We formulate the problem using an MDP \((,,P,r,H)\), where \(\) is the state space comprising of all possible entities (in language), \(\) is an action space, defined by language prompts for a pre-trained environment LLM.5 The action prompts are used to modify or _steer_ an entity \(x\). We elaborate on these actions later in this section. The transition function, \(P:_{}\) is induced by the environment LLM. Finally, the reward function is defined using the utility \(U\) such that \(r_{t}(x)=0\) for all \(t<H-1\), and \(r_{H-1}(x)=U(E_{D}(x);)\).6 While we do not define it explicitly, the MDP can also be endowed with contextual information such as information about nearby entities, or a user for whom we wish to personalize creation.

At time \(t=0\) the process is initialized at an existing entity \(x_{i}\). For every \(t<H\), the agent sees state \(x_{t}\) and selects an action \(a_{t}\). The action is provided to the environment LLM as a language prompt, the environment LLM generates the next state \(x_{t+1}\), and the agent receives a reward \(r(x_{t})\). We assume the environment LLM generates states in \(\) (i.e., feasible states).

### Designing a State-Dependent Action Space

Designing an action space for an RL problem can be challenging (Guss et al., 2019; Kanervisto et al., 2020; Tennenholtz and Mannor, 2019). In our setting, actions are prompts used to change entities in \(\). As such, a good set of actions should be diverse and induce high utility states (entities).

**LLMs for Action Creation.** We exploit the generative power of LLMs to construct an exploratory set of actions for each state. Specifically, for each \(x\), an LLM is used to construct a set \((x)\) of \(K\) actions. For example, we might prompt an LLM to generate \(100\) ways to change a plot of a movie, or the specification of an designed product. This ensures each existing \(x\) is associated with a diverse set of actions that can modify it. To ensure coverage, \(K\) must be large enough to adequately search the state space (see Sec. 7 and appendix C further discussion).

**Reference Policy and G-Optimal Design.** We use the (possibly large) set of actions to learn a reference policy for a PG algorithm (see Sec. 2), trained via supervised learning. Specifically, given the action space, we define a _reference_ policy \(_{}(a|x)=q_{a}\) where \(q((x))\).

We consider three choices for distribution \(q\): (1) uniform, i.e., \(q_{a}=\); (2) myopic best next-step action, i.e., \(q_{a}=a*{arg\,max}_{x^{}  P(|x,a)}[U(x^{})]}\); and (3) myopic _G-optimal design_, which is a distribution over actions that minimizes a form of worse-case variance (Atwood, 1969; Kiefer and Wolfowitz, 1960). We define it formally as follows.

**Definition 1** (G-optimal design).: _Let \((x)\) be given. Denote \(z_{a}(x)=_{x^{} P(|x,a)}[E_{D}(x^{})]\). A distribution \(q(x)((x))\) is a myopic G-optimal design with approximation factor \(C 1\) if_

\[_{a(x)}\|z_{a}(x)\|_{(q(x))^{-1}}^{2} Cn,\] (4)

_where \((q(x))=_{a q(x)}z_{a}(x)z_{a}^{T}(x)\), and \(n\) is the dimension of the latent embedding space \(\)._G-optimal designs have proven useful for contextual bandits with large action spaces, allowing one to achieve efficient regret guarantees with minimal assumptions (Zhu et al., 2022). Notably, an optimal design as a uniform \(\)-exploration strategy has been shown to produce an efficient algorithm. We implement such exploration using a G-optimal design-based reference policy. A G-optimal design always exists whenever \(\{z_{a}(x)\}_{a A(x)}\) is compact, and \(C=1\)(Kiefer and Wolfowitz, 1960; Zhu et al., 2022). While it is generally NP-hard to compute (Grotschel et al., 2012), an approximate optimal design can be computed efficiently (Zhu et al., 2022) (see Appendix C for specific implementation details in our setup using uniform sampling).

### Embedding-Aligned Guided LanguageE (EAGLE) Agent

We are now ready to describe our approach, the Embedding-Aligned Guided LanguageE (EAGLE) agent, which is detailed in Algorithm 1. An illustration comparing EAGLE to ELM is depicted in Fig. 1. The EAGLE agent is trained using the pre-trained environment LLM. We first generate \(K\) actions using the dataset of existing entities \(\) and the pre-trained LLM. Then, using the encoder \(E_{D}:\), we create an approximate myopic G-optimal design \(q\) over these actions. We use the G-optimal design to train an initial reference policy \(_{}\) to match the action distribution \(q\). Finally, we use \(_{}\) as a reference policy for a PG algorithm, regularizing the loss as in Eq. (1). The PG algorithm is trained using the environment LLM and the encoder \(E_{D}\) to optimize a utility \(U\).

```
1:Input: Environment LLM, dataset \(\), reward function \(r\), encoder \(E_{D}\), PG algorithm PG-ALG
2: Generate \(K\) candidate actions using pre-trained environment LLM for every \(x\).
3: Compute an approximate myopic G-optimal design \(q(x)\) for every \((x),x\) (Definition 1).
4: Train reference policy \(_{}(|x)=q(x)\) over candidate action data and G-optimal design.
5: Train PG-ALG with reference policy \(_{}\), pre-trained environment LLM, encoder \(E_{D}\), and reward \(r\). ```

**Algorithm 1** Embedding-Aligned Guided LanguageE (EAGLE) Agent

## 5 Experiment Design

We detail our experiment design; specifically, the data generation process, the training process, and our evaluation methods. Our work uses the MovieLens 25M dataset (Harper and Konstan, 2015), which contains 25 million ratings (1 to 5) of 62,423 movies and 162,541 users. We also conduct experiments on the Amazon review dataset (Ni et al., 2019), whose results are provided in Appendix I.

### Data Generation

**Latent Embeddings.** We create embeddings for both users and movies in the MovieLens dataset. We consider _behavioral embeddings_, trained solely using user ratings (i.e., no explicit semantic information) via _matrix factorization_ using _weighted alternating least squares_(Hu et al., 2008). A user \(u\)'s predicted rating for movie \(m\) is given by the dot product \( z_{u},z_{m}\) of their embeddings.7

**State Space.** We generate textual descriptions of movies and users in the MovieLens dataset using Gemini Ultra (Team et al., 2023). For movies, we prompt Gemini to generate a plot for the movie, list five reasons someone might like the movie, and list five reasons they might dislike it (we refer to these as the _description_ of a movie). To describe a user \(u\), we provide Gemini with the list of movies \(u\) rated with their ratings, and prompt Gemini to create a textual _user profile_ describing the user's preferences using a structured template with the following categories: general, genre, director, and aesthetic preferences. Examples of generated movie descriptions, user profiles, and prompts are provided in Appendices D, F and H. To test the consistency of the generated profile for a user \(u\), we train an encoder to map it to \(u\)'s behavioral embedding (see Appendix D).

**Action Space.** We generate 100 candidate actions for each movie in the dataset. Specifically, we generate 50 generic actions and 50 personalized actions for every movie. We do this by prompting Gemini Ultra with the plot of the movie, and the list of reasons someone might like or dislike it,and ask it to generate 50 actionable ways in which to change the movie. Similarly, to generate personalized actions, we also provide Gemini with a textual user profile, and ask it to generate 50 changes that are personalized to that user. To increase action set diversity, we prompt the model to output actions in five categories: plot changes, character enhancements, visual and storytelling improvements, thematic exploration, and audience appeal adjustments. See Appendices C, F and H for the precise prompts and sample outputs.

Notice that we rely on an LLM's ability (in our case, Gemini Ultra) to generate diverse actions that suffice to explore the ambient space of movies. Other domains may require further considerations to ensure action space diversity (see further discussion of this limitation in Appendix B).

### Training

**Training Task.** We consider a specialized content gap task described in Sec. 3. We simplify somewhat by not accounting for creator utility, and only considering "local" content gaps around anchor items and specified users. We compute user utility for items in behavioral embedding space using dot products of user and movie embeddings, \( z_{u},z_{m}\). We ensure the generated movie is, in fact, a content gap by also accounting for distance to existing movies; specifically, \(_{2}\) distance (in embedding space) of the generated movie to its three nearest neighbors. Formally, given user \(u\) with embedding \(z_{u}\), and a target movie \(m\) with embedding \(z_{m}=E_{D}(m)\), our utility is defined by

\[U(z_{m};) = z_{u},z_{m}+_{m^{}( m)}\|z_{m}-z_{m^{}}\|,\] (Content Gap Utility)

where NN\((m)\) is the set of three nearest neighbors to \(m\) in \(\) (w.r.t. \(_{2}\) distance in \(\)), and \(>0\).

**Encoder.** We fine-tune a Gemini Nano-2 LLM (3.25B parameters) [Team et al., 2023] to serve as a movie encoder \(E_{D}\). We train it using \(_{2}\) regression loss to map movie descriptions (plot, reasons to like, and reasons to dislike) to their behavioral embeddings. We use \(E_{D}\) to compute the utility \(U\).

**Reference Policy.** We use Gemini Nano-2 to initialize a reference policy \(_{}\). We fine-tune the model using next-token prediction (cross-entropy loss) of actions as targets. We create three reference policies: _(a) Uniform:_ We use the full set of 100 generated actions for each movie as targets. _(b) Best next-step action:_ We first create transitions for each action in the dataset. Specifically, we use Gemini Ultra to act as an environment, and prompt it with each action to generate a new movie (plot and reasons to like or dislike). The new movie is then encoded using \(E_{D}\), and its utility computed. Finally, we create a dataset of the "best" actions based on the computed scores. _(c) Approximate G-optimal design:_ We calculate the next step embeddings for each action, as before. We then select ten actions which are a solution to Eq. (4). The new dataset consists of ten "exploratory" actions per movie, which we use to train a reference policy via supervised learning, as before. We refer to Appendix C for details on computing the optimal design.

**RL Training.** We use Gemini Ultra as our environment LLM. It is prompted to generate a transition given the current movie description and the agent action (see Appendix F for a review of all prompts). We inject randomness by increasing the sampling temperature of the agent model and the environment LLM. Specific hyperparameters are given in Appendix E.

### Evaluation Methods

We employ two evaluation methods; namely, raw utility scores and human feedback, comparing EAGLE-generated outputs with their initial anchors. We use a pool of 124 human raters to provide feedback on our generated results.8 Raters are shown a textual user profile together with descriptions of two movies: a ground truth (real) movie, and a variant generated using \(H=5\) steps of agent changes. Importantly, the two descriptions are ordered randomly, and raters do not know which is the original or generated variant.

We quantify each rater's personal preferences by having them rate 10-20 movies from a set of 600 popular movies, and ask them to write a textual profile of their own preferences. To increase confidence in our human evaluation, raters evaluate each algorithm twice, once w.r.t. a textual user profile of a user _from the MovieLens dataset_, and again w.r.t. _their own_ preference profile. Each rater is asked a series of questions to quantify: (1) _User Utility_, the quality of the generated movie w.r.t. a user profile; (2) _Rater Utility_, its quality w.r.t. their personal preferences; and (3) _Distance Score_, the distance of the generated movie from the anchor (i.e., the rater scores how different the two movies are). We express our results as the percentage of raters who preferred the generated result to the original (values in \(\)). See Appendix G for a complete overview of rater questions.

## 6 Experiments

Table 2 compares results of ELM, supervised training (i.e., the reference policy), and EAGLE (ours) using the three reference action distributions (see Secs. 4.3 and 5.2). Specific hyperparameters are detailed in Appendix E. We train ELM to map behavioral embeddings of existing movies to their descriptions. Given an input movie embedding, we search for a nearby embedding point with optimal utility, and use ELM as a decoder to produce its description. We see that EAGLE outperforms ELM by a large margin w.r.t. both user and rater utility. This suggests a limitation in the generalization capability of ELM, and a fundamental problem of moving "out of manifold" when searching for an optimal point in embedding space. We discuss this further in Appendix B. Despite this, raters judge ELM to generate movie descriptions that are further from (more different than) the anchor.

The reference policy performs best with optimistic actions (i.e., max next-step utility). Interestingly, even the randomized policies--uniform and G-optimal design--perform well. We believe this is due to the choice of action space, which includes many personalized actions, providing a strong initialization for the randomized policies. Moreover, EAGLE performs better with a G-optimal design baseline than with the uniform or optimistic action baselines. This suggests that G-optimal design helps search the state space more efficiently. This is also evident in the distance score, which increases with the use of G-optimal design.

**Environment Transfer.** Table 3 compares results for different training/inference setups to test transfer effectiveness. Specifically, we compare training with Gemini-Pro or Gemini-Ultra models, and testing (i.e., inference) with Gemini-Ultra or GPT-4 models. All results use EAGLE with a G-optimal design baseline. Our results suggest that a Gemini-Pro training environment suffices to obtain high-quality inference results, as training with Gemini-Ultra did not improve performance.

    &  &  \\   & \(U(z)\) &  User \\ Utility \\  &  Rater \\ Utility \\  &  Distance \\ Score \\  \\  ELM  & 0.57 & 0.31 & 0.37 & **0.52** \\   &  Uniform \\ Optimistic Action \\ G-Optimal Design \\  & 0.49 \(\) 0.02 & 0.59 & 0.62 & 0.44 \\   &  Optimistic Action \\ G-Optimal Design \\  & 0.58 \(\) 0.02 & 0.62 & 0.6 & 0.34 \\   &  Optimistic Action \\ G-Optimal Design \\  & 0.51 \(\) 0.02 & 0.51 & 0.67 & 0.45 \\   &  Uniform \\ Optimistic Action \\ G-Optimal Design \\  & 0.65 \(\) 0.03 & 0.69 & 0.64 & 0.43 \\   &  Optimistic Action \\ G-Optimal Design \\  & 0.69 \(\) 0.04 & 0.71 & 0.69 & 0.25 \\   & 
 Optimistic Action \\ G-Optimal Design \\  & 0.74 \(\) 0.03 & **0.76** & **0.74** & 0.47 \\   

Table 2: Comparison of ELM, supervised training, and EAGLE. We test three distribution of reference policies: uniform (i.e., random), optimistic (i.e., next-step best action), and G-optimal design (Definition 1).

    &  &  \\   & \(U(z)\) &  User \\ Utility \\  &  Rater \\ Utility \\  & 
 Distance \\ Score \\  \\  Gemini Pro \(\) Ultra & 0.75 \(\) 0.03 & 0.76 & 0.74 & **0.47** \\ Gemini Ultra \(\) Ultra & **0.8 \(\) 0.02** & 0.74 & 0.66 & **0.47** \\ Gemini Pro \(\) GPT-4 & 0.65 \(\) 0.02 & **0.86** & **0.78** & **0.45** \\   

Table 3: For \(\{\}\) and \(\{,\}\), the experiment \(\)\(\)EnvTest shows results for training EAGLE on \(\) and testing it on \(\). All experiments use an EAGLE based on Gemini-Nano and a G-optimal design baseline.

Interestingly, a trained EAGLE agent tested on a fixed GPT-4 environment maintains (and sometimes increases) its level of performance, suggesting that EAGLE is robust to the LLM environment on which it acts. More broadly, it seems that one can train an EAGLE agent using any suitably capable LLM, and then apply it successfully in environments defined by different LLMs.

**Action Space.** We test the effect of changing the resolution and personalization of the action space on an EAGLE agent with a G-optimal design baseline in Table 4. We first test the effect of making personalized actions unavailable. As expected, we see significant degradation in performance, suggesting personalized actions are critical to performance.

Second, we construct a _macro-action space_ comprising tuples \((a_{1},a_{2},a_{3},a_{4},a_{5})\) of five consecutive actions. Thus, instead of making a single change to a movie at each step, the environment LLM makes five changes at once (i.e., \(H=1\)). We evaluate the macro action space by executing the five actions chosen by the trained EAGLE agent on the default action space, at once. We also construct a _combined action space_, that _adds_ macro-actions to the original action space (rather than replacing it). We see that macro-actions degrade performance w.r.t. the original finer-grained changes, suggesting the LLM performs better when making smaller changes. We also find the combined action space significantly outperforms other baselines, suggesting value to equipping the agent with both low and high resolution actions in a sequential setup.

**EAGLE on Highly Rated Movies.** Finally, we assess the conditional performance of the EAGLE agent over movies that are originally rated high. We find that, while EAGLE improves overall scores of anchor movies that are originally rated poorly (i.e., scores \(1,2,3\)), it does not perform as well on movies that are rated highly (e.g., scores \(4,5\)), sometimes even decreasing user utility. Specifically, conditioned on poorly rated movies, EAGLE achieves an average utility of \(0.7 0.04\) (an approximate \(30\%\) increase), whereas for highly rated movies, it achieves an average utility of \(0.83 0.03\) (an approximate \(5\%\) decrease). This result is expected as MovieLens rating data is limited to ratings between \(1\) and \(5\), where a rating of \(5\) is (perhaps, incorrectly) assumed to be optimal for the user. To address this, we might only suggest or create a new entity if it has a higher utility than its anchor.

## 7 Creative Generation: Discussion and Limitations

ELM vs. EAGLE.Our work reveals two distinct methodologies for generating novel entities that adhere to an underlying embedding space: using ELMs as decoders and our proposed EAGLE agent. While ELM offers efficient optimization within the embedding space and, in theory, can reach an unconstrained optimal point, it faces challenges related to the computational cost of fine-tuning the decoder, unknown generalization errors, and difficulties in constraining optimization due to the unknown manifold metric. Conversely, EAGLE leverages the existing textual capabilities of LLMs, enabling more interpretable optimization and potentially requiring a smaller agent model. However, EAGLE's coverage is inherently limited by the defined action space, and training can be computationally expensive due to its reliance on LLM environment queries. We refer the reader to Appendix B for further discussion on these tradeoffs.

Action Space Design.A principal building block of EAGLE is defining and / or designing an action space to change entities in \(\). Designing a good action space is critical to ensure reachability

    & Utility &  \\   & \(U(z)\) & User Utility & Rater Utility & Distance Score \\  Default Actions & **0.75 \(\) 0.03** & 0.76 & 0.74 & **0.47** \\ Without Personalized & 0.56 \(\) 0.02 & 0.54 & 0.59 & **0.45** \\ Macro Actions & 0.71 \(\) 0.02 & 0.67 & 0.7 & 0.32 \\ Combined Actions & **0.74 \(\) 0.04** & **0.88** & **0.92** & 0.4 \\   

Table 4: The effect of changing the action space on EAGLE, showing (1) the default action space as described in Sec. 5, (2) the default action space with personalized actions removed, (3) only macro actions (i.e., taking three actions at once), and (4) a combined action space, with both the default actions as well as macro actions. All experiments use a G-optimal design strategy as reference policy.

of an optimal novel entity. In this work we define a methodology for designing such an action space using LLMs and G-optimal design (Definition 1).

Fig. 2 illustrates three examples of potential action spaces and their inherent bias in embedding space. Informally, an action space may allow for "better" coverage if it can induce changes in all direction on the embedding manifold \(\). More generally, an action space should allow one to reach any point on the embedding manifold (up to some ball of radius \(\)) in at most \(H\) iterations, starting from any point \(z=E_{D}(x)\) such that \(x\). Indeed, as we illustrate in Fig. 2, our action space may be constrained to certain directions, thereby limiting our ability to cover the full span of possible entities.

To increase the overall coverage of an action space one can take one of several measures, including: (1) generating a large amount of candidate actions, (2) using experts to enhance the action space with additional missing actions, or (3) changing the methods used to generate those actions (e.g., using different prompting strategies). While all of these methods could potentially increase the coverage of the action space, none of them would guarantee coverage of the embedding space. In fact, it may be that _no action exists_ that would allow one to move in any direction of the embedding space. That is, the environment LLM is incapable of moving in a specific embedding direction (i.e., an embedding direction cannot be mapped into an actionable text prompt). This is a particular disadvantage of EAGLE, and an advantage of ELM, as ELM is not constrained by movements in embedding space (see Appendix B).

Apart from the coverage challenge of designing an action space, we also consider the bias of such an action space. If an action space is biased, then a randomized policy executing those actions uniformly would be biased as well. To mitigate this bias effect we use a G-optimal design. The G-optimal design allows us to identify a set of \(k\) actions that would be best for exploring the action space in terms of overall utility, thereby mitigating the inherent bias for exploration.

## 8 Conclusion

This paper introduces EAGLE, a novel framework that leverages RL to align LLM generation with domain-specific objectives encoded in latent embedding spaces. Treating a pre-trained LLM as an environment, EAGLE iteratively steers its output towards optimal regions of the embedding space based on a predefined criterion. Our experiments on the MovieLens 25M dataset demonstrate the effectiveness of EAGLE in surfacing content gaps, hypothetical content items that satisfy latent user demand. The results highlight the benefit of incorporating a state-dependent action set based on G-optimal design for enhanced efficiency. EAGLE's capacity to leverage the power of large pre-trained LLMs without requiring explicit decoders opens up new possibilities for controlled and grounded text generation, ensuring consistency with domain knowledge and data representations.

Further research directions include: exploring different action space design methodologies beyond G-optimal design, generalizing EAGLE to diverse modalities like images, audio, and video, and applying EAGLE to other applications that require aligning LLM generation with specific domain knowledge or constraints. EAGLE's flexible framework holds promise for various tasks, including personalized content creation, targeted advertising, and controlled dialogue generation.

Figure 2: An illustration of different forms of coverage and bias of an action space. As actions are textual prompts, their corresponding embedding directions may either provide good coverage, or partial coverage of the underlying embedding manifold (e.g., there may be directions that are not covered by the generated actions). Moreover, actions may be uniformly biased toward specific directions. We accommodate for this using a G-optimal design which reduces this bias through a set of exploratory actions in embedding space.