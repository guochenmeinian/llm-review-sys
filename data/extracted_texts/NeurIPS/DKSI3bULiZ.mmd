# Multiple Physics Pretraining for Spatiotemporal

Surrogate Models

 Michael McCabe\({}^{*,1,2}\), Bruno Regaldo-Saint Blancard \({}^{1}\), Liam Parker \({}^{1}\), Ruben Ohana \({}^{1}\),

**Miles Cranmer**\({}^{3}\), **Alberto Bietti**\({}^{1}\), **Michael Eickenberg**\({}^{1}\),**Siavash Golkar \({}^{1}\),

**Geraud Krawezik**\({}^{1}\), **Francois Lanusse**\({}^{1,4}\), **Mariel Pettee**\({}^{1,5}\),

**Tiberiu Tesileanu**\({}^{1}\), **Kyunghyun Cho**\({}^{6,7,8}\), **Shirley Ho**\({}^{1,6,9}\)

Contact: mmccabe@flatironinstitute.org

###### Abstract

We introduce multiple physics pretraining (MPP), an autoregressive task-agnostic pretraining approach for physical surrogate modeling of spatiotemporal systems with transformers. In MPP, rather than training one model on a specific physical system, we train a backbone model to predict the dynamics of multiple heterogeneous physical systems simultaneously in order to learn features that are broadly useful across systems and facilitate transfer. In order to learn effectively in this setting, we introduce a shared embedding and normalization strategy that projects the fields of multiple systems into a shared embedding space. We validate the efficacy of our approach on both pretraining and downstream tasks over a broad fluid mechanics-oriented benchmark. We show that a single MPP-pretrained transformer is able to match or outperform task-specific baselines on all pretraining sub-tasks without the need for finetuning. For downstream tasks, we demonstrate that finetuning MPP-trained models results in more accurate predictions across multiple time-steps on systems with previously unseen physical components or higher dimensional systems compared to training from scratch or finetuning pre-trained video foundation models. We open-source our code and model weights trained at multiple scales for reproducibility.

## 1 Introduction

In recent years, the fields of natural language processing and computer vision have been revolutionized by the success of large models pretrained with task-agnostic objectives on massive, diverse datasets (Chen et al., 2020; Devlin et al., 2018; He et al., 2021). This has, in part, been driven by the development of self-supervised pretraining methods which allow models to utilize far more training data than would be accessible with supervised training (Balestriero et al., 2023). These so-called "foundation models" have enabled transfer learning on entirely new scales. Despite their task-agnostic pretraining, the features they extract have been leveraged as a basis for task-specific finetuning, outperforming supervised training alone across numerous problems especially for transfer to settings that are insufficiently data-rich to train large models from scratch (Bommasani et al., 2021).

Deep learning for computational science has begun to see first steps in this direction. Large domain-specific pretrained models have emerged in diverse fields such as chemistry (Bran et al., 2023; Chithrananda et al., 2020), medicine (Jiang et al., 2023; Tu et al., 2023; Tu et al., 2023), astrophysics (Leung and Bovy, 2023; Nguyen et al., 2023b), and climate (Nguyen et al., 2023a) and the trend only seems to be growing as more and more models are developed for new fields both as refined versions of existing large language models and as new models trained entirely on field-specific data.

In this work, we demonstrate that similar approaches can be extended to the surrogate modeling of spatiotemporal physical systems. Spatiotemporal prediction tasks, like those found in fluids, solids, or general continuum mechanics, have attracted significant attention from the deep learning community. From direct prediction methods (Dang et al., 2022; Li et al., 2020; Lusch et al., 2018; Pfaff et al., 2021; Stachenfeld et al., 2022) to neural PDE solvers (Bruna et al., 2022; Raissi et al., 2019), researchers have sought to develop fast, accurate models for physics either as faster surrogates for the partial differential equation (PDE) solvers that dominate the field or to simulate systems that cannot be exactly described or resolved by current mechanistic models and available hardware. While directly outperforming PDE solvers is difficult (Grossmann et al., 2023), deep learning has already begun to impact fields like atmospheric science (Ben-Bouallegue et al., 2023; Bi et al., 2023; Lam et al., 2023; Pathak et al., 2022) and cosmology (Cranmer et al., 2021; He et al., 2019; Jamieson et al., 2023), where the systems are too large or too imprecisely described to be simulated exactly.

Unfortunately, outside of a few observation-rich outliers, settings where numerical simulation is expensive or unreliable also tend to be settings where the difficulty of acquiring training data makes it impractical to train surrogates conventionally. Most deep learning-based surrogates thus far have focused on specific physical systems or families of parameterized PDEs where data can easily be acquired. However, for the low-data settings often found in simulation-driven exploration and design, it would be valuable to have large, task-agnostic models with a broad understanding of common physical behavior to act as a foundation for finetuning.

**Contributions.** To address this need, we introduce _Multiple Physics Pretraining_ (MPP), a new approach for task-agnostic pretraining of physical surrogate models. Our method enables large-scale pretraining for transfer across diverse physics which we study using fluid-oriented benchmarks. Our specific contributions are:

* We develop MPP, a pretraining approach in which we embed multiple hetereogeneous physical systems into a shared embedding space and learn to autoregressively predict the dynamics of all systems simultaneously.
* We show that single transformer models pretrained with MPP are able to match or surpass modern task-specific baselines without applying task-specific finetuning to the MPP models.
* We demonstrate the transfer capabilities of models trained with MPP to systems with limited training examples (referred to as _low-data systems_ thereafter) displaying new physics in the form of previously unseen parameter regimes generating notably different qualitative behavior and inflated to higher dimensions.
* We open-source our code and provide our pretrained models at a variety of sizes for the community to experiment with on their own tasks.

## 2 Background

**Notation.** Let \(S\) be an arbitrary physics-driven spatiotemporal dynamical system, either described by a parameterized family of PDEs with fixed parameters, or where snapshots are gathered from observation of a unique physical phenomenon. To simplify notation, we discuss systems with a single state variable in one spatial dimension. A continuous state variable for system \(S\) is represented as \(u^{S}(x,t):[0,L_{S}][0,)\). We discretize the system uniformly in space and time at resolutions \(N_{S}\), \(T_{S}\) respectively. A snapshot \(^{S}_{t}^{N_{S}}\) represents the value of state variable \(u^{S}\) at all \(N_{S}\) spatial discretization points at time \(t\). Our pretraining task is then to learn a single model \(\) that can take a uniformly spaced sequence of \(T_{S}\) snapshots \(^{S}_{t}=[^{S}_{t-T_{s} t_{S}},,^{S}_{t}]\) from system \(S\) sampled from some distribution over systems and predict \((^{S}_{t})\) such that \((^{S}_{t})^{S}_{t+ t_{S}}\).

**Autoregressive Pretraining.** In vision and language, the dominant pretraining strategies include autoregressive prediction (Radford et al., 2018), masked reconstruction (Devlin et al., 2018; Heet al., 2021), and contrastive learning (Chen et al., 2020). In language, autoregressive generation emerged as a convenient self-supervised task. In surrogate modeling of dynamical systems, next-step prediction is often a primary goal. This makes autoregressive pretraining a natural choice of objective for training time-dependent surrogate models.

We note that it is common to use the simulation parameters to condition the predictions of models operating on PDE-generated data (Gupta and Brandstetter, 2022; Subramanian et al., 2023; Takamoto et al., 2023). Our goal is not to develop a new PDE solver, but rather to design an approach that is broadly applicable to both observed and simulated dynamics. Thus, we do not assume a known functional form in MPP and the model must instead implicitly infer the impact of these parameters on the dynamics from the history provided in \(_{t}^{S}\).

**Surrogate Modeling for Spatiotemporal Physical Systems.** We are primarily concerned with modeling dynamical systems varying in both time and space, where the time evolution of the system is intrinsically tied to spatial relationships amongst the state variables according to physical laws. PDEs are one of the primary modeling tools for this setting. They are often derived from fundamental conservation laws of properties such as mass, momentum, and energy (Farlow, 1993). Many PDEs describe variations of the same physical laws, which is why concepts like diffusion, advection, reactivity, and connections between time and spatial gradients appear in many different PDEs. These shared underlying principles suggest we can extract features relevant to multiple physical systems.

## 3 Related Work

**Foundation models.** Massive pretrained models dubbed "foundation models" (Bommasani et al., 2021), particularly large transformer-based architectures (Vaswani et al., 2017), have recently attracted significant attention. The most prevalent foundation models are pretrained language models like GPT (Brown et al., 2020; Radford et al., 2018, 2019) and BERT (Devlin et al., 2018). Emergent abilities (Wei et al., 2022) demonstrated by large language models highlight the importance of scale in manifesting higher-order capabilities absent at smaller scales. Vision has seen similar developments with the growth of masked (He et al., 2021; Tong et al., 2022) and contrastive (Chen et al., 2020) pretraining. The data in this work is insufficiently diverse to call the resulting models "foundational". However, we provide the first large-scale implementation of successful multiple nonlinear physics pretraining for spatiotemporal systems.

**Scientific machine learning.** While a wide range of architectures have been employed for physical surrogate modeling (Bar and Sochen, 2019; Han et al., 2018; Sirignano and Spiliopoulos, 2018; Yu et al., 2018; Zang et al., 2020), we position our work with respect to three three major classes. One prominent class is the neural-network-as-PDE-solution approach (Bruna et al., 2022; Raissi et al., 2019) which requires the PDE to be known and solves a single system on a single domain. Other methods do not learn the solution directly, but instead augment a PDE-solver as learned corrections (Dresdner et al., 2023; Rackauckas et al., 2021; Um et al., 2021), learned closures (Duraisamy et al., 2019; Sirignano and MacArt, 2023), or learned algorithmic components (Bar and Sochen, 2019; Kochkov et al., 2021). A broader, but less physically constrained approach, is learning a solution operator from the data without knowledge of the governing equations (Cao, 2021; Kovachki et al., 2023; Li et al., 2020, 2021; Lu et al., 2019). While these methods are often evaluated using PDE-generated benchmarks, these are designed to learn directly from data rather than learning to solve a PDE. Neural operators typically do not reach the accuracy of numerical PDE solvers, but they are applicable for domains without explicitly provided equations. This last family is the most similar to our approach, especially Cao (2021) as we use a transformer-based architecture. However, our pretraining procedure is developed for training across multiple operators.

The high cost of training scientific models from scratch has led to significant exploration of transfer learning. Prior work has explored transfer learning in operator networks in such scenarios as conditional shift (Goswami et al., 2022) or new domains, boundary conditions, or distributions over parameters (Li et al., 2021; Subel et al., 2023; Wang et al., 2022; Xu et al., 2023). However, these too need to be retrained from scratch for new differential operators in the PDE. More recently, efforts have been made to explore transfer across operators and benefits from training on multiple physical systems simultaneously. Subramanian et al. (2023) explores how transfer scales in this setting. However, their study is limited to steady-state linear systems with periodic boundary conditions.

Other works have explored similarly restricted classes or low dimensional, low resolution systems (Desai et al., 2022; Yang et al., 2023).

## 4 Scalable Multiple Physics Pretraining

### Compositionality and Pretraining

Many specialized PDEs demonstrate a form of compositionality, as a range of physical phenomena can be described by core components like nonlinear advection or diffusion, but then are augmented or restricted by specialized terms representing concepts like buoyancy or system constraints. To motivate a useful pretraining procedure from this compositionality, we examine two hypotheses:

1. Single models can learn the dynamics for multiple classes of physical behavior.
2. Learning partially overlapping physics is beneficial for transfer learning.

Since many real-world systems share core components, under these hypotheses, training single models on many distinct systems is a natural approach for developing foundation models for physical dynamics. We therefore start by eliminating the complexity related to hypothesis (1) in order to isolate hypothesis (2). We do this by choosing a simple problem setting with one shared scalar field along with consistent scales and geometry: constant-coefficient linear advection-diffusion on a periodic 1D domain. Let \((x,t)\) be a scalar-valued function defined on a periodic spatial domain, \(v\) a constant one-dimensional velocity coefficient and \(\) a constant diffusion coefficient, then:

**Advection:** \[+(v)=0\]
**Diffusion:** \[+(-)=0\]
**Advection-Diffusion:** \[+(v-)=0.\]

If hypothesis (2) holds, we would expect pretraining on advection and diffusion systems individually could be beneficial for transfer to advection-diffusion systems.

We find that this is indeed the case. We pretrain a spatiotemporal transformer model on a large amount of trajectories (100,000 each) with uniformly sampled coefficients (\(v[-3,-.1][.1,3],\ [10^{-3},1.]\) generated from the advection and diffusion equations while finetuning on restricted samples from advection-diffusion simulations. The pretrained model is able to achieve much lower error with far fewer samples (Figure 1) without observing advection and diffusion together in the same trajectory during pretraining.

However, to validate hypothesis (1), we must handle much larger spatial resolutions, varying scales, and heterogeneous relationships between fields. Over the rest of this section, we develop an approach for handling these challenges.

### Architecture

**Axial Attention.** Given the success of large transformer models in other domains, we employ a scalable axial attention (Dong et al., 2022; Ho et al., 2019; Huang et al., 2019) transformer backbone. For a (2+1)-dimensional system with \(T H W\) tokens, conventional dense attention attends over all tokens simultaneously and has cost \(O((HWT)^{2})\). Axial attention instead performs a series of attention operations over each axis in turn, limiting the cost to \(O(H^{2}+W^{2}+T^{2})\). In Figure 2, it can be seen that while we perform attention on each axis independently, the spatial \(K,\ Q,\ V\) projections are shared between the height (y) and width (x) axes.

Figure 1: Finetuning a model pretrained on large amounts of advection and diffusion data outperforms models trained from scratch on advection-diffusion data across a wide range of data availability (16-100K examples).

Axial attention has been used in video transformers (Arnab et al., 2021; Bertasius et al., 2021) due to the improved scalability in higher dimensions. While the tools used in our transformer backbone were introduced in prior work, our choice of using fully axial attention differs from ViViT which opted to only separate space and time attention. We favor scalability over maximizing accuracy and so chose fully axial attention. In the following, we refer to this architecture as an Axial ViT (AViT).

**Field Embedding and Normalization.** Embedding multiple physical systems into a single shared representation is complicated by the fact that fields from different systems may operate on entirely different scales in terms of both magnitude and resolution. This is one of the primary challenges that must be addressed for multiple-physics pretraining.

To unify magnitudes, we use Reversible Instance Normalization (Kim et al., 2022, RevIN). We compute the mean and standard deviation of each channel over space-time dimensions and use them to normalize input fields. These statistics are saved and used to denormalize model outputs. While this approach was initially developed for time-series forecasting, in practice the effect is similar to that of the norm scaling utilized in Subramanian et al. (2023).

After rescaling, the data is projected into a shared embedding space. This is the only component with unique weights for each source system. Given a system \(S\) with state variables \(u(x,t),\ v(x,t),\ p(x,t)\), we project each point or "pixel" into a space of dimension \(D^{}\):

\[(x,t)=u(x,t)_{u}+v(x,t)_{v}+p(x,t)_{p}\] (1)

where \(\) are embedding vectors in \(^{D^{}}\). This can be seen as a convolution with \(1 1\) filters where the input channels of the filter are sub-selected to correspond to the fields present within a given dataset. On the right side of Figure 2, the filter is assembled by sub-selected columns of the larger filter corresponding to the provided fields. It is important to note that this initial projection setup is amenable to fine-tuning to unseen field types. This can be achieved by adding new channels to the initial embeddings, and training them from random initialization. In our models, the shared full resolution space is converted into patched tokens by a sequence of strided convolutions separated by pointwise nonlinearities as in Touvron et al. (2022).

Figure 2: (Left) MPP works by individually normalizing each example using Reversible Instance Normalization (RevIN) then embedding each field individually into a shared, normalized space. A single transformer backbone can then predict the next step for multiple sets of physics. We use an AViT backbone which attends over space and time axis sequentially. Spatial attention is further split by axis, though these share linear projection weights. (Right) The embedding and reconstruction matrices are formed by subsampling a larger \(1 1\) convolutional filter based on input fields.

The predictions are reconstructed from the output tokens by reversing this process. The tokens are decoded by a sequence of transposed convolution blocks and projected onto the output fields by taking coordinate-wise inner products with reconstruction vectors \(\):

\[u(x,t+ t)=(x,t+ t),_{u}.\] (2)

This can similarly be implemented as a \(1 1\) convolution with the _output_ channels of the convolution filter sub-selected. The mean and standard deviation computed from the inputs are then applied to these normalized outputs to produce the final de-normalized predictions as in Kim et al. (2022).

**Position Biases and Boundaries.** While in most cases, we would like the model to infer boundary conditions from the provided history, we make an exception to this policy for periodic boundaries as they change the continuity of the domain. Transformers are inherently permutation equivariant, and it is essential to include position biases so that the model can learn locality.

With a slight modification, we can use our position biases to capture the change in locality imposed by periodic boundaries. T5-style (Raffel et al., 2020) relative position encodings (RPE) utilize a lookup table to access learned embeddings corresponding to ranges of "relative distance". For periodic boundary conditions, we modify the relative distance computation to account for neighbors across the periodic boundary. In Appendix D.1, we examine simple systems that differ only in boundary conditions and find that this minor change improves generalization in the case where we must learn both periodic and non-periodic conditions.

### Balancing Objectives During Training

**Task Sampling.** Our pretraining procedure operates on multiple levels of sampling. The task distribution varies in system \(S\), spatial resolution \(N_{S}\), and time resolution \(T_{S}\) and we want diverse batches that accurately capture the signal this provides. However, sampling a full batch from multiple systems at different resolutions simultaneously would be inefficient on modern hardware as it would require batch processing of differently shaped tensors. Multi-GPU training adds an additional complication as the variance in execution time due to unbalanced workloads can lead to inefficient hardware usage.

We mitigate both of these concerns with a simple randomization scheme involving gradient accumulation. Gradient accumulation utilizes multiple backward passes per synchronization step. We therefore sample a single system \(S\) uniformly from \(\) for each _micro-batch_. With \(m\) micro-batches per synchronization step, we reduce the work-per-GPU variance \(_{}^{2}\) to \(_{}^{2}\), significantly reducing the average lost cycles due to work discrepancies. This could likely be further reduced by an approximate packing problem solution (Cormen et al., 2022), but we found the random approach was sufficient for our needs. As we employ gradient accumulation in order to increase our batch sizes, this sampling procedure incurs no additional cost.

**Scaled Training Objective.** The simplest approach to obtaining updates from the different tasks is to add their gradients. However, as the magnitudes of the state variables can vary significantly between systems, unweighted losses will result in the gradients from the problems with the largest scales drowning out losses on smaller scales (Yu et al., 2020). To partially control this behavior, we train using the normalized MSE (NMSE) defined as:

\[_{}=|}_{S} (_{t}^{S})-_{t+1}^{S}\|_{2}^{2}}{\|_{t+1} ^{S}\|_{2}^{2}+}\] (3)

where \(\) denotes the micro-batch and \(\) is a small number added for numerical stability. This does not account for the full variation in difficulty. Even if sub-task losses have similar magnitudes at the start of training, it is possible for some systems to converge quickly while other losses remain high. Nonetheless, we found that this allows our training process to produce strong results on multiple systems simultaneously.

## 5 Experiments

We design our experiments to probe three vital questions about the utility of MPP:

1. Can large transformer models learn the dynamics of multiple physical systems simultaneously?2. Does pretraining lead to improved accuracy on previously unseen physics?
3. Does MPP provide a finetuning advantage over existing spatiotemporal foundation models?

**Data.** We use the full collection of two-dimensional time-dependent simulations from PDEBench (Takamoto et al., 2022) as our primary source for diverse pretraining data. This includes systems governed by four unique nonlinear PDEs defined over a variety of state variables, resolutions, initial conditions, boundary conditions, and simulation parameters. The specific PDEs are the compressible and incompressible Navier-Stokes equations (CNS/INS), the shallow-water equations (SWE), and a 2D Diffusion-Reaction equation (DiffRe2D). Full details on the data used can be found in Appendix B.1.

**Training settings.**\(T_{S}\) is fixed at 16 for all experiments as our VideoMAE comparison in Section 5.2 was unable to scale to larger sizes without gradient checkpointing. Autoregressive training is performed only one step ahead--no longer rollouts, noise corruption, or post-processing are included for stability. Training from scratch and MPP pretraining are always performed on the AViT architecture described in section 4.2. Full training details including data splits, optimization details, and hardware are documented in Appendix C.

### Pretraining Representations

First, we evaluate whether pretraining on multiple task actually leads to effective representations by comparing MPP-pretrained models to dedicated baselines from prior work across all available systems. The models are pretrained at a variety of sizes so we can begin to explore to benefits of scaling our approach. Precise model sizes can be found in Appendix C.1. Unlike the baselines which are trained on only one system and so must only learn one parameter regime, our models (denoted by MPP-AViT-*) must handle all systems and regimes without finetuning. The effect of physical parameters, forcing, and simulation parameters must be inferred from context \(_{t}^{S}\). The UNet (Ronneberger et al., 2015) and FNO (Li et al., 2020) results are sourced from Takamoto et al. (2022) while the results from Shen et al. (2023) with a finetuned SWIN (Liu et al., 2021) are used for ORCA. As the lightweight FNO proved to be the most competitive comparison, we train an additional FNO beyond the PDEBench results that has been scaled to 115M parameters (labeled "FNO-B") for fairness. Results are reported in terms of Normalized RMSE (NRMSE, the square root of Equation 3) averaged over fields and examples, as in Takamoto et al. (2023). Our Compressible Navier-Stokes results are aggregated based on the mach number here due to space limitations. Fully granular results can be found in Appendix D.4. Ablation results demonstrating the importance of balanced losses and normalization are shown in Appendix D.2.

Our pretrained models are able achieve high-end performance on all datasets (Table 1) despite the difficulty of multi-task training (Yu et al., 2020) while showing improved performance with scale. Our smallest pretrained model, the MPP-AViT-Ti outperforms the PDEBench baselines on all problems except for SWE. However, though both models improve in absolute performance with scale, the

   Model & \(\#\)Param & SWE & DiffRe2D & CNS M1.0 & CNS M0.1 \\  MPP-AViT-Ti & 7.6M & 0.0066 & **0.0168** & **0.0442** & **0.0312** \\ UNet & 7.7M & 0.083- & 0.84– & 0.4725 & 1.6650 \\ FNO & 927K & **0.0044** & 0.12– & 0.1685 & 0.2425 \\  FNO-B & 115M & 0.00246 & 0.0599 & 0.1451 & 0.1978 \\ ORCA-SWIN-B & 88M & 0.00600 & 0.82– & — & — \\ AViT-B & & & & & \\ Task-Specific & 116M & 0.00047 & 0.0110 & 0.0316 & 0.0261 \\ MPP & 116M & 0.00240 & 0.0106 & 0.0281 & 0.0172 \\ MPP + Finetuned & 116M & **0.00043** & **0.0087** & **0.0187** & **0.0079** \\  MPP-AViT-S & 29M & 0.0039 & 0.0112 & 0.0319 & 0.0213 \\ MPP-AViT-L & 409M & 0.0022 & 0.0098 & 0.0208 & 0.0147 \\   

Table 1: NRMSE comparison between MPP-pretrained models and dedicated baselines on shallow water equations (SWE), 2D Diffusion-Reaction (DiffRe2D), and compressible Navier-Stokes (CNS) at Mach numbers \(M=.1\) and \(M=1\). Complex parameters counted as two real. Top performing within size range and overall are bolded. Dashes indicate precision not provided by source.

pretrained AViT-B catches up to the FNO-B. It is important to clarify that we are not claiming the pretrained models are optimal--with a series of comparisons on the AViT-B models, we show that at times, the multi-task training does hurt performance on individual tasks and that we can improve upon the pretrained model performance by finetuning our own models on specific tasks. What this experiment answers affirmatively is that large transformers can learn multiple sets of dynamics simultaneously. Trajectories from pretrained models are displayed in Appendix D.6.

### Transfer to Low-data Domains

Harkening back to Section 4.1, since we have now shown that we can learn multiple sets of dynamics with a single model, we return to the question of whether these multiple physics models are well suited to transfer learning. For a more realistic exploration of transfer, we construct a setting where the model must learn new physical behavior by removing all compressible fluid data from the pretraining corpus and pretraining on the three remaining spatiotemporal systems. We then evaluate transfer to two specific compressible Navier-Stokes datasets:

* _"Near":_\(M=0.1\), viscosity\(=10^{-2}\), Random Periodic Initial Conditions
* _"Far":_\(M=1.0\), viscosity\(=10^{-8}\), Turbulent Initial Conditions

Snapshots of the kinetic energy for the finetuning systems and incompressible pretraining data are shown in Figure 4. While quantitatively evaluating the physics gap is an unsolved problem, the names reflect both prior physical knowledge and qualitative evaluation. "Near" features a low Mach number, the dimensionless quantity that correlates with compressible behavior, and viscosity similar to that of the incompressible simulation. "Far" has wildly different turbulent behavior that induces small scale structure never seen during training. However, despite the similarity in physical behavior, the simulations are still quite different: the compressible and incompressible simulations in PDEBench differ in spatial and temporal resolution, initial condition distribution, boundary conditions, viscosity, and velocity range in addition to the difference in compressibility. We use these sets to compare the finetuning performance of MPP, training from scratch, and an existing pretrained spatiotemporal transformer, VideoMAE (Tong et al., 2022) pretrained on both K400 (Kay et al., 2017) and SSV2 (Goyal et al., 2017) datasets. Details on the finetuning procedure followed can be found in Appendix C.3.1.

Figure 4: Kinetic energy for incompressible pretraining and compressible finetuning examples. The “near” compressible snapshot resembles the pretraining snapshot while “far” displays new turbulent small scales.

Figure 3: NRMSE for transfer learning tasks. Solid lines are one-step error. Dashed lines are averaged error over five step rollouts. The MPP model shows clear performance benefits in both cases. The more turbulent behavior of “far” seems to be difficult to learn from scratch or from video data, but pretraining on physical data leads to much stronger results.

Figure 3 shows that the MPP models outperform VideoMAE and training from scratch by a large margin in the low-data regime. Numerical results are listed in Appendix C. VideoMAE displays surprisingly strong finetuning performance given that the pretraining data is conventional video, but it is unable to match the much lower memory (VideoMAE at 79.3 GB vs. AViT-B at 24.7 GB peak VRAM for batch size 1) MPP-AviT-B in either setting. Predictably, both pretraining approaches are less accurate in the long-run on the turbulent "far" dataset. However, in the short-term the physical pretraining seems to provide an even larger advantage in this regime compared to the far smoother "near" data. Rollout visualizations are included in Appendix D.7.

One possible explanation for the strong performance of finetuning for "far" is that this experiment can be viewed as a more realistic example of the compositionality exploration in Section 4.1 from the perspective of classification of second-order PDEs. The solution to Navier-Stokes in the vanishing viscosity limit represents one possible weak solution to the Euler equations which are classically hyperbolic (LeVeque & Leveque, 1992). While the viscous incompressible flow from pretraining is governed by the same transport equations as "far", those solutions are dominated by smoothing locally parabolic behavior. However, the inviscid shallow water simulations in the training set are hyperbolic. The model has therefore seen two major components of "far", but has never seen them within one system until finetuning.

### Inflation to 3D

While 2D problems offer a compelling middle ground between complexity and cost for experimentation, most physical phenomena of real-world interest are fundamentally three-dimensional. We therefore examine the usefulness of our pretrained models when "inflated" to 3D. Inflation techniques were first demonstrated in Carreira & Zisserman (2017) and have seen use for extending 2D visual (image) classifiers to 3D spatiotemporal (video) settings (Nguyen et al., 2022; Xie et al., 2018). Here we employ the technique to add an additional spatial dimension.

The factored architecture of the AViT is well-suited to inflation. We initialize the projection weights discussed in Section 4.2 to those from the 2D compressible Navier-Stokes data seen during training with the new velocity direction initialized as the average of the previous two velocity projections. Since the transformer backbone acts on each spatial axis independently, the only dimension-dependent operations are the learned downsampling or "patching" convolutions. These convolutional layers are modified by following the inflation procedure of Carreira & Zisserman (2017): a 2D kernel of size \(P P\) is inflated into a 3D kernel by repeating the 2D kernel \(P\) times along the new axis and rescaling by \(\). This gives us a 3D convolutional operation that is equivalent to applying previous 2D filters then average pooling in the new direction. In practice, we find performance can be slightly improved by adding low magnitude Gaussian noise to the resulting filter. Due to the low resolution of the "Turbulent" dataset, we additionally reduced the stride of the first convolution in the hMLP by a factor of 2 for a total downsampling factor of 8 rather than the 16 used elsewhere. Standard training details are found in Appendix C.4

We compare these inflated 2D to 3D models to both training an identical architecture from scratch and PDEBench baselines. Unlike in Section 5.2 where we held out the 2D Compressible Navier-Stokes data, we use the full pretrained models from section 5.1 here. Nonetheless, this remains a significant physical gap as 2D and 3D turbulence are well-understood to have major differences in behavior (Ouellette, 2012). Due to the difficulty of scaling 3D training, results in this section are reported at the "Ti" scale.

Table 2 demonstrates significant improvements on both the turbulently and randomly initialized Compressible Navier-Stokes datasets from PDEBench with a \(11.7\%\) improvement for the smaller dataset of randomly initialized simulations and even a \(4.1\%\) improvement for the larger turbulently initialized dataset where both the dimensionality and sampling are adjusted. Training on high resolution 3D data is an enormously expensive procedure. Our results suggest that pretraining on 2D data and inflating to 3D is a promising strategy for developing models that can be used in this space.

    & Turbulent & Random \\ Size (B, T, N) & \((600,21,64^{3})\) & \((100,21,128^{3})\) \\  FNO & 0.240 & 0.370 \\ UNet & 0.230 & 1.000 \\ AViT-Scratch & 0.098 & 0.299 \\ AViT-MPP & **0.094** & **0.264** \\   

Table 2: NRMSE for 2D to 3D inflation. Subheadings are initial condition type.

Conclusion

We introduced an autoregressive pretraining strategy, Multiple Physics Pretraining, for the development of multi-use physical surrogates. Through per-sample normalization, field embeddings, appropriately scaled losses, and efficient task sampling, we are able to train scalable transformer models capable of predicting multiple sets of independent dynamics simultaneously. We evaluated several sizes of model and observed that the approach benefits from scale. MPP models were able to match dedicated modern baselines on benchmarks containing fluid and reaction simulations derived from multiple equations, simulation parameters, and boundary conditions from pretraining alone with even stronger performance after undergoing task-specific finetuning. Our pretrained models also showed positive transfer by outperforming both training from scratch and existing video models on previously unseen physics. Furthermore, through kernel inflation approaches, we were able to demonstrate improved results on 3D simulation compared to training from scratch.

**Limitations and Future Work.** The focus of our work is on transfer and learning from multiple data sources, however, many interesting questions remain before we can develop true foundation models for spatiotemporal physics. For example, our choice of architecture enabled us to scale to some of the higher resolution benchmarks available today, but it also limits our models to uniform grids. Extensions to the non-uniform geometries frequently encountered in physical simulation will require further work balancing efficiency and flexibility. Extending to 3D, for example, is an enormously expensive task for many families of models. Our inflation approach is promising and future work can help identify better approaches for performing this inflation. In terms of more physically motivated concerns, we must also explore the role of constraints and conservation laws in models developed for multiple physical systems. Hard constraints that are necessary for a closed system may ensure error in an open system, thus adaptable approaches may need to be developed.

Similarly, many physically interesting tasks require handling incomplete or noisy data. While certain datasets in PDEBench do not provide all relevant physical fields (the Shallow Water equations, for instance, only contain \(h\) rather than full velocity fields needed to simulate the data) all the data that is available is observed at every grid point without noise. It remains to be seen if these approaches generalize to settings without this type of clean data.

Finally, as these are early days in the development of foundation models for physics, the limits of transfer in these spaces are poorly understood. PDEBench, which we used here, is constructed from largely fluid-like data. While we demonstrated strong transfer benefits from pretraining, it remains to be seen how far away from the training distribution or training tasks these benefits persist. It is even an open question how to define distance in this space. Future work will need to expand data diversity to push these questions and others forward. MPP opens up many new research directions and paves the way for this development in the future.