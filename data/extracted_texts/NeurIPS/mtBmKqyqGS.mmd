# Phased Consistency Models

Fu-Yun Wang\({}^{1}\) Zhaoyang Huang\({}^{2}\) Alexander William Bergman\({}^{3,6}\) Dazhong Shen\({}^{4}\)

Peng Gao\({}^{4}\) Michael Lingelbach\({}^{3,6}\) Keqiang Sun\({}^{1}\) Weikang Bian\({}^{1}\)

Guanglu Song\({}^{5}\) Yu Liu\({}^{4}\) Xiaogang Wang\({}^{1}\) Hongsheng Li\({}^{1,4,7}\)

\({}^{1}\)MMLab, CUHK \({}^{2}\)Avolution AI \({}^{3}\)Hedra

\({}^{4}\)Shanghai AI Lab \({}^{5}\)Sensetime Research \({}^{6}\)Stanford University \({}^{7}\)CPII under InnoHK

{fywang@link, xgwang@ee, hsli@ee}.cuhk.edu.hk

###### Abstract

Consistency Models (CMs) have made significant progress in accelerating the generation of diffusion models. However, their application to high-resolution, text-conditioned image generation in the latent space remains unsatisfactory. In this paper, we identify three key flaws in the current design of Latent Consistency Models (LCMs). We investigate the reasons behind these limitations and propose **P**hased **C**onsistency **M**odels (PCMs), which generalize the design space and address the identified limitations. Our evaluations demonstrate that PCMs outperform LCMs across 1-16 step generation settings. While PCMs are specifically designed for multi-step refinement, they achieve comparable 1-step generation results to previously state-of-the-art specifically designed 1-step methods. Furthermore, we show the methodology of PCMs is versatile and applicable to video generation, enabling us to train the state-of-the-art few-step text-to-video generator. Our code is available at https://github.com/G-U-N/Phased-Consistency-Model.

Figure 1: _PCMs: Towards stable and fast image and video generation._

## 1 Introduction

Diffusion models [20; 15; 57; 68] have emerged as the dominant methodology for image synthesis [45; 41; 7] and video synthesis [22; 53; 52; 63]. These models have shown the ability to generate high-quality and diverse samples conditioned on varying signals. At their core, diffusion models rely on an iterative evaluation to generate new samples. This iterative evaluation trajectory models the probability flow ODE (PF-ODE) [56; 57] that transforms an initial normal distribution to a target real data distribution. However, the iterative nature of diffusion models makes the generation of new samples time-intensive and resource-consuming. To address this challenge, consistency models [56; 11; 63; 55] have emerged to reduce the number of iterative steps required to generate samples. These models work by training a model that enforces the self-consistency  property: _any point along the same PF-ODE trajectory shall be mapped to the same solution point._ These models have been extended to high-resolution text-to-image synthesis with latent consistency models (LCMs) . Despite the improvements in efficiency and the ability to generate samples in a few steps, the sample quality of such models is still limited.

We show that the current design of LCMs is flawed, causing inevitable drawbacks in controllability, consistency, and efficiency during image sampling. Fig. 2 illustrates our observations of LCMs. The limitations are listed as follows:

1. **Consistency.** Due to the specific consistency property, CMs can only use the purely stochastic multi-step sampling algorithm, which assumes that the accumulated noise variable in each generative step is independent and causes varying degrees of stochasticity for different inference-step settings. As a result, we can find inconsistency among the samples generated with the same seeds in different inference steps.
2. **Controllability.** Even though diffusion models can adopt classifier-free guidance (CFG)  in a wide range of values (i.e. 2-15), equipped with weights of LCMs, they can only accept values of CFG within range of 1-2. Larger values of CFG would cause the exposure problem. This brings difficulty for the hyper-parameter selection. Additionally, we find that LCMs are insensitive to the negative prompt. As shown in the figure, LCMs still generate black dogs even when the negative prompt is set to "black dog". Both phenomenon reduce the controllability on generation. We show the reason behind this is the CFG-augmented ODE solver adopted in the consistency distillation stage.
3. **Efficiency.** LCMs tend to generate much inferior samples at the few-step settings, especially in less than 4 inference steps, which limits the sampling efficiency. We argue that the reason lies in the traditional L2 loss or the Huber loss used in the LCMs procedure, which is insufficient for the fine-grained supervision in few-step settings.

To this end, we propose Phased Consistency Models (PCMs), which can tackle the discussed limitations of LCMs and are easy to train. Specifically, instead of mapping all points along the ODE trajectory to the same solution, PCMs phase the ODE trajectory into several sub-trajectories and only enforce the self-consistency property on each sub-trajectory. Therefore, PCMs can sample

Figure 2: **Summative motivation. We observe and summarize three crucial limitations for (latent) consistency models, and generalize the design space, well tackling all these limitations.**samples along the solution points of different sub-trajectories in a deterministic manner without error accumulation. As an example shown in Fig. 3, we train PCMs with two sub-trajectories, with three edge points including \(_{T}\), \(_{0}\), and \(_{ T/2}\) selected. Thereby, we can achieve 2-step deterministic sampling (i.e., \(_{T}_{ T/2}_ {0}\) ) for generation. Moreover, the phased nature of PCMs leads to an additional advantage. For distillation, we can choose to use a normal ODE solver without the CFG alternatively in the consistency distillation stage, which is not viable in LCMs. As a result, PCMs can optionally use larger values of CFG for inference and be more responsive to the negative prompt. Additionally, to improve the sample quality for few-step settings, we propose an adversarial loss in the latent space for more fine-grained supervision.

To conclude, we dive into the design of (latent) consistency models, analyze the reasons for their unsatisfactory generation characteristics, and propose effective strategies for tackling these limitations. We validate the effectiveness of PCMs on widely recognized image generation benchmarks with stable diffusion v1-5 (0.9 B)  and stable diffusion XL (3B)  and video generation benchmarks following AnimateLCM . Vast experimental results show the effectiveness of PCMs.

## 2 Preliminaries

**Diffusion models** define a forward conditional probability path, with a general representation of \(_{t}_{0}+_{t}\) for intermediate distribution \(_{t}(|_{0})\) conditioned on \(_{0}_{0}\), which is equivalent to the stochastic differential equation \(_{t}=f_{t}_{t}t+g_{t} _{t}\) with \(_{t}\) denoting the standard Winer process, \(f_{t}=_{t}}{_{t}}\) and \(g_{t}^{2}=_{t}^{2}}{t}-2 _{t}}{t}_{t}^{2}\). There exists a deterministic flow for reversing the transition, which is represented by the probability flow ODE (PF-ODE) \(_{t}=[f_{t}_{t}-g_{t}^{2}/2_{_{t}}_{t}(_{t})]t\). In standard diffusion training, a neural network is typically learned to estimate the score of marginal distribution \(_{t}(_{t})\), which is equivalent to \(_{}(,t)_{}_{t}( )=_{_{0}(_{0}|)}[_{}_{t}(|_{0})]\). Substitute the \(_{}_{t}(x)\) with \(_{}(x,t)\), and we get the empirical PF-ODE. Famous solvers including DDIM , DPM-solver , Euler and Heun  can be generally perceived as the approximation of the PF-ODE with specific orders and forms of diffusion for sampling in finite discrete inference steps. However, when the number of discrete steps is too small, they face inevitable discretization errors.

**Consistency models**, instead of estimating the score of marginal distributions, learn to directly predict the solution point of ODE trajectory by enforcing the self-consistency property: all points at the same ODE trajectory map to the same solution point. To be specific, given a ODE trajectory \(\{_{t}\}_{t[,T]}\), the consistency models \(_{}(,t)\) learns to achieve \(_{}(_{t},t)=_{}\) by enforcing \(_{}(_{t},t)=_{}(_{t},t^{ })\) for all \(t,t^{}[,T]\). A boundary condition \(_{}(_{},)=_{}\) is set to guarantee the successful convergence of consistency training. During the sampling procedure, we first sample from the initial distribution \(_{T}(,)\). Then, the final sample can be generated/refined by alternating denoising and noise injection steps, i.e,

\[}_{}^{_{h}}=_{}(}_{ _{k}},_{k}),\ \ }_{_{k-1}}=}_{}^{_{h}}+, \ \ (,)\] (1)

where \(\{_{k}\}_{k=1}^{K}\) are selected time points in the ODE trajectory and \(_{K}=T\). Actually, because only one solution point is used to conduct the consistency model, it is inevitable to introduce stochastic error, i.e., \(\), for multiple sampling procedures.

Figure 3: (Left) Illustrative comparison of diffusion models , consistency models , consistency trajectory models , and our phased consistency model. (Right) Simplified visualization of the forward SDE and reverse-time PF-ODE trajectories.

Recently, Consistency trajectory models (CTMs)  propose a more flexible framework. specifically, CTMs \(}(_{t},t,s)\) learns to achieve \(}(_{t},t,s)=(x_{t},t,s;)\) by enforcing \(}(_{t},t,s)=}(_{t^{}},t ^{},s)\) with \(s(t,t^{})\) and \(t,t^{}[,T]\). However, the learning objectives of CTMs are redundant, including many trajectories that will never be applied for inference. More specifically, if we split the continuous time trajectory into \(N\) discrete points, diffusion models learn \((N)\) objectives (i.e., each point learns to move to its adjacent point), consistency models learn \((N)\) objectives (i.e., each point learns to move to solution point), and consistency trajectory models learn \((N^{2})\) objectives (i.e., each point learns to move to all the other points in the trajectory). Hence, except for the current timestep embedding, CTMs should additionally learn a target timestep embedding, which is not comprised of the design space of diffusion models.

Different from the above approaches, PCMs can be optimized efficiently and support deterministic sampling without additional stochastic error. Overall, Fig. 3 illustrates the difference in training and inference processes among diffusion models, consistency models, consistency trajectory models, and phased consistency models.

## 3 Method

In this section, we introduce the technical details of PCMs, which overcome the limitations of LCMs in terms of consistency, controllability, and efficiency. **Consistency**: Specifically, we first introduce the main framework of PCMs, consisting of definition, parameterization, the distillation objective, and the sampling procedure in Sec. 3.1. In particular, by enforcing the self-consistency property in multiple ODE sub-trajectories respectively, PCMs can support deterministic sampling to preserve image consistency with varying inference steps. **Controllability**: Secondly, in Sec. 3.2, to improve the controllability of text guidance, we revisit the potential drawback of the guided distillation adopted in LCMs, and propose to optionally remove the CFG for consistency distillation. **Efficiency**: Thirdly, in Sec. 3.3, to further improve inference efficiency, we introduce an adversarial consistency loss to enforce the modeling of data distribution, which facilitates 1-step generation.

### Main Framework

**Definition.** For a solution trajectory of a diffusion model \(\{_{t}\}_{t[,T]}\) following the PF-ODE, we split the trajectory into multiple sub-trajectories with hyper-defined edge timesteps \(s_{0},s_{1},,s_{M}\), where \(s_{0}=\) and \(s_{M}=T\). The \(M\) sub-trajectories can be represented as \(\{_{t}\}_{t[s_{m},s_{m+1}]}\) with \(m=0,1,,M-1\). We treat each sub-trajectory as an independent CM and define the consistency function as \(^{m}:(_{t},t)_{s_{m}}, t[s_{m},s _{m+1}]\). We learn \(_{}^{m}\) to estimate \(\) by enforcing the self-consistency property on each sub-trajectory that its outputs are consistent for arbitrary pairs on the same sub-trajectory. Namely, \(^{m}(_{t},t)=^{m}(_{t^{}},t^{})\) for all \(t,t^{}[s_{m},s_{m+1}]\). Note that the consistency function is only defined on the sub-trajectory. However, for sampling, it is necessary to define a transition from timestep \(T\) (i.e., \(s_{M}\)) to \(\) (i.e., \(s_{0}\)). Thereby, we defined \(^{m,m^{}}=^{m^{}}(^{m-2}( ^{m-1}(^{m}(_{t},t)\,,s_{m}),s_{m-1}),s _{m^{}})\) that transforms any point \(_{t}\) on \(m\)-th sub-trajectory to the solution point of \(m^{}\)-th trajectory.

**Parameterization.** Following the definition, the corresponding consistency function of each sub-trajectory should satisfy boundary condition \(^{m}(_{s_{m}},s_{m})=_{s_{m}}\), which is crucial for guaranteeing

Figure 4: **Training paradigm of PCMs. ‘?’ means optional usage.**

the successful training of consistency models. To satisfy the boundary condition, we typically need to explicitly parameterize \(_{}^{m}(,t)\) as \(_{}^{m}(_{t},t)=c_{}^{m}(t)_{r}+c _{}^{m}(t)F_{}(_{t},t,s_{m})\), where \(c_{}^{m}(t)\) gradually increases to \(1\) and \(c_{}^{m}(t)\) gradually decays to \(0\) from timestep \(s_{m+1}\) to \(s_{m}\).

Another important thing is how to parameterize \(F_{}(_{t},t,s)\), basically it should be able to indicate the target prediction at timestep \(s\) given the input \(\) at timestep \(t\). Since we build upon the epsilon prediction models, we hope to maintain the epsilon-prediction learning target.

For the above-discussed PF-ODE, there exists an exact solution  from timestep \(t\) to \(s\)

\[_{s}=}{_{t}}_{t}+_{s}_{ _{t}}^{_{s}}e^{-}_{t_{}()} _{t_{}()}(_{t_{}()})\] (2)

where \(_{t}=}{_{}}\) and \(t_{}\) is a inverse function with \(_{t}\). The equation shows that the solution of the ODE from \(t\) to \(s\) is the scaling of \(_{t}\) and the weighted sum of scores. Given a epsilon prediction diffusion network \(_{}(,t)\), we can estimate the solution as \(_{s}=}{_{t}}_{t}-_{s}_{ _{t}}^{_{s}}e^{-}_{}(_{t _{}()},t_{}())\). However, note that the solution requires knowing the epsilon prediction at each timestep between \(t\) and \(s\), but consistency modes need to predict the solution with only \(_{t}\) available with single network evaluation. Thereby, we parameterize the \(F_{}(,t,s)\) as following,

\[_{s}=}{_{t}}_{t}-_{s}}_{}(_{t},t)_{_{t}}^{_{s}}e^{- }\,.\] (3)

One can show that the parameterization has the same format with DDIM \(_{s}=_{s}(_{t}-_{t}_{}(_{t},t)}{_{t}})+_{s}_{} (_{t},t)\) (see Theorem 3). But here we clarify that the parameterization has an intrinsic difference from DDIM. DDIM is the first-order approximation of solution ODE, which works because we assume the linearity of the score in small intervals. This causes the DDIM to degrade dramatically in few-step settings since the linearity is no longer satisfied. Instead, our parameterization is not approximation but exact solution learning. The learning target of \(}_{}(,t)\) is no more the scaled score \(-_{t}_{}_{t}()\) (which epsilon-prediction diffusion models learn to estimate) but. Actually, we can define the parameterization in other formats, but we find this format is simple and has a small gap between the original diffusion models. The parameterization of \(F_{}\) also allows for a better property that we can drop the introduced \(c_{}^{m}\) and \(c_{}^{m}\) in consistency models to ease the complexity of the framework. Note that following Eq. 3, we can get \(F_{}(_{s_{m}},s_{m},s_{m})=}}{_{s _{m}}}_{s_{m}}-0=_{s_{m}}\). Therefore, the boundary condition is already satisfied. Hence, we can simply define \(_{}^{m}(,t)=F_{}(,t,s_{m})\). This parameterization also aligns with several previous diffusion distillation techniques  utilizing DDIM format, building a deep connection with previous distillation methods. The difference is that we provide a more fundamental explanation of the meaning and learning objective of parameterizations.

**Phased consistency distillation objective.** Denote the pre-trained diffusion models as \(_{}(,t)=-_{}(, t)}{_{t}}\), which induces an empirical PF-ODE. We firstly discretize the whole trajectory into \(N\) sub-intervals with \(N+1\) discrete timesteps from \([,T]\), which we denote as \(t_{0}=<t_{1}<t_{2}<<t_{N}=T\). Typically \(N\) should be sufficiently large to make sure the ODE solver approximates the ODE trajectory correctly. Then we sample \(M+1\) timesteps as edge timesteps \(s_{0}=t_{0}<s_{1}<s_{2}<<s_{M}=t_{N}\{t_{i}\}_{i=0}^{N}\) to split the ODE trajectory into \(M\) sub-trajectories. Each sub-trajectory \([s_{i},s_{i+1}]\) consists of the set of sub-intervals \(\{[t_{j},t_{j+1}]\}_{t_{j} s_{i},t_{j+1} s_{i+1}}\).

Here we define \((_{t_{n+k}},t_{n+k},t_{n};)\) as the \(k\)-step ODE solver that approximate \(_{t_{n}}^{}\) from \(_{t_{n+k}}\) on the same sub-trajectory following Equation 2, namely,

\[}_{t_{n}}^{}=(_{t_{n+k}},t_{n+k},t_{n}; ).\] (4)

Following CMs , we set \(k=1\) to minimize the ODE solver. The training loss is defined as

\[_{}(,^{-};)=_{ (m),(n|m),(_{t_{n+1}}|n,m)}[ (t_{n})d(_{}^{m}(_{t_{n+1}},t_{n+1}), {f}_{^{-}}^{m}(}_{t_{n}}^{},t_{n}))]\] (5)

where \((m):=(\{0,1,,M-1\})\), \((n|m):=(\{n+1|t_{n+1} s_{m+1},t_{n} s_{m}\})\), \((_{t_{n+1}}|n,m)=_{t_{n+1}}()\), and \(^{-}=^{-}+(1-)\).

We show that when \(_{}(,^{-};)=0\) and local errors of ODE solvers uniformly bounded by \((( t)^{p+1})\), the solution estimation error within the arbitrary sub-trajectory \(_{}^{m}(_{t_{n}},t_{n})\) is bounded by \((( t)^{p})\) in Theorem 1. Additionally, the solution estimation error across any sets of sub-trajectories (i.e., the error between \(_{}^{m,m^{}}(_{t_{n}},t_{n})\) and \(^{m,m^{}}(_{t_{n+1},t_{n+1}};)\)) is also bounded by \((( t)^{p})\) in Theorem 2.

**Sampling.** For a given initial sample at timestep \(t\) which belongs to the sub-trajectory \([s_{m},s_{m+1}]\), we can support deterministic sampling following the definition of \(^{m,0}\). Previous work [21; 67] reveals that introducing a certain degree of stochasticity might lead to better generation quality. We show that our sampling method can also introduce randomness through a simple modification, which we discuss at Sec. IV.1

### Guided Distillation

For convenience, we take the epsilon-prediction format with text conditions \(\) for the following discussion. The consistency model is denoted as \(_{}(_{t},t,)\), and the diffusion model is denoted as \(_{}(_{t},t,)\). A commonly applied strategy for text-conditioned diffusion models is classifier-free guidance (CFG) [16; 51; 1]. At training, \(\) is randomly substituted with null text embedding \(\). At each inference step, the model computes \(_{}(_{t},t,)\) and \(_{}(_{t},t,_{})\) simultaneously, and the actual prediction is the linear combination of them. Namely,

\[_{}(_{t},t,,_{};w)=_{}(_{t},t,_{})+w(_{ }(_{t},t,)-_{}(_{t}, t,_{})),\] (6)

where \(w\) controlling the strength and \(_{}\) can be set to \(\) or text embeddings of unwanted characteristics. A noticeable phenomenon is that diffusion models with this strategy can not generate content with good quality without using CFG. That is, the empirical ODE trajectory induced with pure \(_{}(_{t},t,)\) deviates away from the ODE trajectory to real data distribution. Thereby, it is necessary to apply CFG-augmented prediction \(_{}(_{t},t,,_{};w)\) for ODE solvers. Recall that we have shown that the consistency learning target of the consistency model \(_{}(_{t},t,)\) is the weighted sum of epsilon prediction on the trajectory. Thereby, we have \(_{}(_{t},t,)_{ }(_{t^{}},t^{},,;w)\), for all \(t^{} t\). On this basis, if we additionally apply the CFG for the consistency models, we can prove that

\[_{}(_{t},t,,_{};w^{ }) ww^{}(_{}(_{t^{}}, t^{},)-_{}^{}))+_{}(_{t^{}},t^{},_{})\,,\] (7)

where \(_{}^{}=(1-)_{}( _{t^{}},t^{},_{})+_{ }(_{t^{}},t^{},)\) and \(=}\) (See Theorem 4). This equation indicates that applying CFG \(w^{}\) to the consistency models trained with CFG-augmented ODE solver confined with \(w\), is equivalent to scaling the prediction of original diffusion models by \(w^{}w\), which explains the the exposure problem. We can also observe that the epsilon prediction with negative prompts is _diluted_ by the prediction with null text embedding, which reveals that the impact of negative prompts is reduced.

Figure 5: **Qualitative Comparison.** Our method achieves top-tier performance.

We ask the question: _Is it possible to conduct consistency distillation with diffusion models trained for CFG usage without applying CFG-augmented ODE solver?_ Our finding is that it is not applicable for original CMs but works well for PCMs especially when the number of sub-trajectory \(M\) is large. We empirically find that \(M=4\) is sufficient for successful training. As we discussed, the text-conditioned diffusion models trained for CFG usage fail at achieving good generation quality when removing CFG for inference. That is, target data distribution induced by PF-ODE with \(}(_{t},t,)\) has a large distribution distance to real data distribution. Therefore, fitting the spoiled ODE trajectory is only to make the generation quality bad. In contrast, when phasing the whole ODE trajectory into several sub-trajectories, the negative influence is greatly alleviated. On one hand, the starting point \(_{s_{m+1}}\) of sub-trajectories is replaced by adding noise to the real data. On the other hand, the distribution distance between the distribution of solution points \(_{s_{m}}\) of sub-trajectories and real data distribution \(_{s_{m}}^{}\) at the same timestep is much smaller proportional to the noise level introduced. To put it straightforwardly, even though the distribution gap between the real data and the samples generated from \(}(_{t},t,)\) is large, adding noise to them reduce the gap. Thereby, we can optionally train a consistency model whether supporting larger values of CFG or not.

### Adversarial Consistency Loss

We introduce an adversarial loss to enforce the distribution consistency, which greatly improves the generation quality in few-step settings. For convenience, we introduce an additional symbol \(_{t s}\) which represents a flow from \(_{t}\) to \(_{s}\). Let \(_{t s}^{}\), \(_{t s}^{}\) be the transition mapping following the ODE trajectory of pre-trained diffusion and our consistency models. Additionally, let \(_{s t}^{-}\) be the distribution transition following the forward process SDE (adding noise). The loss function is defined as the following

\[_{}^{adv}(,^{-};,m)=D (_{s_{m} s}^{-}_{t_{n+k} s_{m}}^{} \#_{t_{n+k}}_{s_{m} s}^{-}_{t_{n}  s_{m}}^{^{-}}_{t_{n+1} t_{n}}^{} \#_{t_{n+1}})\,\] (8)

where \(\#\) is the pushforward operator, and \(D\) is the distribution distance metric. To penalize the distribution distance, we apply the GAN-style training paradigm. To be specific, as shown in Fig. 4, for the sampled \(_{t_{n+k}}\) and the \(_{t_{n}}^{}\) solved through the pre-trained diffusion model \(\), we first compute their predicted solution \(}_{s_{m}}=_{}^{m}(_{t_{n+k}},t_{n+ k})\) and \(}_{s_{m}}=_{^{-}}(_{t_{n}}^{ },t_{n})\). Then we randomly add noise to \(}_{s_{m}}\) and \(}_{s_{m}}\) to obtain \(}_{s}\) and \(}_{s}\) with randomly sampled \(s[s_{m},s_{m+1}]\). We optimize the adversarial loss between \(}_{s}\) and \(}_{s}\). Specifically, the \(_{}^{adv}\) can be re-written as

\[_{}^{adv}(,^{-};,m)= {ReLU}(1+(}_{s},s,))+(1-(}_{s},s,)),\] (9)

where \((x)=x\) if \(x>0\) else \((x)=0\), \(\) is the discriminator, \(\) is the image conditions (e.g., text prompts), and the loss is updated in a min-max manner . Therefore the eventual optimization objective is \(_{}+_{}^{adv}\) with \(\) as a hyper-parameter controlling the trade-off of distribution consistency and instance consistency. We adopt \(=0.1\) for all training settings. However, we hope to clarify that the adversarial loss has an intrinsic difference from the GAN. The GAN training aims to align the training data distribution and the generation distribution of the model. That is, \(D(_{t_{n+k} s_{m}}^{}\#_{t_{n+k}}\| _{s_{m}})\) In Theorem 5, we show that consistency property is enforced, our introduced adversarial loss will also coverage to zero. Yet in Theorem 6, we show that combining standard GAN with consistency distillation is a flawed design when considering the pre-trained data distribution and distillation data distribution mismatch. Its loss will be non-zero when the self-consistency property is achieved, thus corrupting the consistency distillation learning. Our experimental results also verify our statement (Fig. 7).

    &  &  \\   &  &  &  \\   & 1 & 2 & 4 & 8 & 16 & 1 & 2 & 4 & 8 & 16 & 1 & 2 & 4 & 8 & 16 \\  InstaFlow  & 11.51 & 69.79 & 102.15 & 122.20 & 139.29 & 11.90 & 62.48 & 88.64 & 105.34 & 113.56 & **9.56** & 29.38 & 38.24 & 43.60 & 47.32 \\ SD-Turbo  & 10.62 & 12.22 & 16.66 & 24.30 & 30.32 & 10.35 & 12.03 & 15.15 & 19.91 & 23.34 & 12.08 & 15.07 & 15.12 & 14.90 & 15.09 \\ ICM  & 5.34 & 11.03 & 6.66 & 6.62 & 7.56 & 42.67 & 10.51 & 5.60 & 5.99 & 9.02 & 21.04 & 10.18 & 10.64 & 12.15 & 13.85 \\ CTM  & 63.55 & 9.93 & 9.30 & 15.62 & 21.75 & 28.47 & 8.98 & 8.22 & 13.27 & 17.43 & 30.39 & 10.32 & 10.31 & 11.27 & 12.75 \\  Ours & **8.27** & **9.79** & **5.81** & **5.00** & **4.70** & **7.91** & **8.93** & **4.93** & **3.88** & **3.88** & 11.66 & 9.17 & 9.07 & 9.49 & 10.13 \\ Ours* & - & - & 7.46 & 6.49 & 5.78 & - & - & 4.09 & 5.01 & 5.13 & - & - & **8.85** & **8.33** & **8.09** \\   

Table 1: Comparison of FID-SD and FID-CLIP with Stable Diffusion v1-5 based methods under different steps.

## 4 Experiments

### Experimental Setup

**Dataset. Training dataset:** For image generation, we train all models on the CC3M  dataset. For video generation, we train the model on WebVid-2M . **Evaluation dataset:** For image generation, we evaluate the performance on the COCO-2014  following the 30K split of karpathy. We also evaluate the performance on the CC12M with our randomly chosen 30K split. For video generation, we evaluate with the captions of UCF-101 .

**Backbones.** We verify the text-to-image generation based on Stable Diffusion v1-5  and Stable Diffusion XL . We verify the text-to-video generation following the design of AnimateLCM  with decoupled consistency distillation.

**Evaluation metrics. Image:** We report the FID  and CLIP score  of the generated images and the validation 30K-sample splits. Following [8; 47], we also compute the FID with CLIP features (FID-CLIP). Note that, all baselines and our method focus on distilling the knowledge from the pre-trained diffusion models for acceleration. Therefore, we also compute the FID of all baselines and the generated images of original pre-trained diffusion models including Stable Diffusion v1-5 and Stable Diffusion XL (FID-SD). **Video:** For video generation, we evaluate the performance from three perspectives: the CLIP Score to measure the text-video alignment, the CLIP Consistency to measure the inter-frame consistency of the generated videos, the Flow Magnitude to measure the motion magnitude of the generated videos with Raft-Large .

### Comparison

**Comparison methods.** We compare PCM with Stable Diffusion v1-5 to methods including Stable Diffusion v1-5 , InstaFlow , LCM , CTM , TCD  and SD-Turbo . We compare PCM with Stable Diffusion XL to methods including Stable Diffusion XL , CTM , SDXL-Lightning , SDXL-Turbo , and LCM . We apply the 'Ours' and 'Ours*' to denote our methods trained with CFG-augmented ODE solver or not. We only report the performance of 'Ours*' with more than 4 steps which aligns with our claim that it is only possible when phasing the ODE trajectory into multiple sub-trajectories. For video generation, we compare with DDIM , DPM , and AnimateLCM .

**Qualitative comparison.** We evaluate our model and comparison methods with a diverse set of prompts in different inference steps. The results are listed in Fig. 5. Our method shows clearly the top performance in both image visual quality and text-image alignment across 1-16 steps.

**Quantitative comparison. One-step generation:** We show the one-step generation results comparison of methods based on Stable Diffusion v1-5 and Stable Diffusion XL in Table 2 and Table 5, respectively. Notably, PCM consistently surpasses the consistency model-based methods including LCM and CTM by a large margin. Additionally, it achieves comparable or even superior to the state-of-the-art GAN-based (SD-Turbo, SDXL-Turbo, SDXL-Lightning) or Rectified-Flow-based (InstaFlow) one-step generation methods. Note that InstaFlow applies the LIPIPS  loss for training and SDXL-Turbo can only generate \(512 512\) resolution images, therefore it is easy for them to obtain higher scores. **Multi-step generation:** We report the FID changes of different methods on COCO-30K and CC12M-30K in Table 1 and Table 3. 'Ours' and 'Ours*' achieve the best or second-best performance in most cases. It is notably the gap of performance between our methods and other baselines becoming large as the timestep increases, which indicates the phased nature of our methods supports more powerful multi-step sampling ability. **Video generation:** We show the quantitative comparison of video generation in Table 4, our model achieves consistent superior

    &  &  &  \\  & FID & FID-CLIP & FID-SD & CLIP & SCORE \(\) & FID & FID-CLIP & FID-SD & CLIP SCORE \(\) \\  InstaFlow  & **13.99** & **9.56** & 11.51 & 29.37 & 15.06 & 6.16 & 11.90 & 24.52 & 0.61 \\ SD-Turbo  & 16.56 & 12.08 & 10.62 & **31.21** & 17.17 & 6.18 & 12.48 & 26.30 & 0.71 \\ CTM  & 67.55 & 30.39 & 63.55 & 23.98 & 56.39 & 28.47 & 56.34 & 18.81 & 0.65 \\ LCM  & 53.81 & 21.04 & 53.43 & 25.23 & 44.35 & 18.58 & 42.67 & 20.38 & 0.62 \\ TCD  & 71.69 & 31.69 & 68.04 & 23.60 & 57.97 & 30.03 & 57.21 & 18.57 & - \\  Ours & 17.91 & 11.66 & **8.27** & 29.26 & **14.79** & **5.38** & **7.91** & **26.33** & **0.81** \\   

Table 2: One-step generation comparison on Stable Diffusion v1-5.

performance. The 1-step and 2-step generation results of DDIM and DPM are very noisy, therefore it is meaningless to evaluate their Flow Magnitude and CLIP Consistency.

**Human evaluation metrics.** To more comprehensively reflect the performance of phased consistency models, we conduct a thorough evaluation using human aesthetic preference metrics, encompassing 1-16 steps. This assessment employs well-regarded metrics, including HPSv2 (HPS) , PickScore (PICKSCORE) , and Laion Aesthetic Score (AES) , to benchmark our method against all comparative baselines. As shown in Table 6 and Table 7, across all evaluated settings, our method consistently achieves either superior or comparable results, with a marked performance advantage over the consistency model baseline LCM, demonstrating its robustness and appeal across diverse human-centric evaluation criteria. We conduct a human preference ablation study on the proposed adversarial consistency loss, with the results presented in Table 8. The inclusion of adversarial consistency loss consistently enhances human evaluation metrics across different inference steps.

### Ablation Study

**Sensitivity to negative prompt.** To show the comparison of sensitivity to negative prompt between our model tuned without CFG-augmented ODE solver and LCM. We provide an example of a prompt

   Steps & Methods & HPS & AES & PICKSCORE \\   & InstaFlow & 0.267 (1) & 5.010 & 0.207 \\  & SD-Turbo & 0.276 (1) & 5.445 (1) & 0.223 (1) \\
1 & CTM & 0.240 & 5.155 & 0.195 \\  & LCM & 0.251 & 5.178 & 0.201 \\  & Ours & 0.276 (1) & 5.389 (2) & 0.213 (2) \\   & InstaFlow & 0.249 & 5.050 & 0.196 \\  & SD-Turbo & 0.278 (1) & 5.570 (1) & 0.226 (1) \\
2 & CTM & 0.267 & 5.117 & 0.208 \\  & LCM & 0.266 & 5.135 & 0.210 \\  & Ours & 0.275 (2) & 5.370 (2) & 0.217 (2) \\   & InstaFlow & 0.243 & 4.765 & 0.192 \\  & SD-Turbo & 0.278 (2) & 5.537 (1) & 0.224 (1) \\
4 & CTM & 0.274 & 5.189 & 0.213 \\  & LCM & 0.273 & 5.264 & 0.215 \\  & Ours & 0.279 (1) & 5.412 (2) & 0.217 (2) \\   & InstaFlow & 0.267 & 4.548 & 0.189 \\  & SD-Turbo & 0.276 (2) & 5.390 (2) & 0.221 (1) \\
8 & CTM & 0.271 & 5.026 & 0.210 \\  & LCM & 0.274 & 5.366 & 0.216 \\  & Ours & 0.278 (1) & 5.398 (1) & 0.218 (2) \\   & InstaFlow & 0.237 & 4.347 & 0.187 \\  & SD-Turbo & 0.277 (1) & 5.275 & 0.219 (1) \\
16 & CTM & 0.270 & 4.870 & 0.209 \\  & LCM & 0.274 & 5.352 (2) & 0.216 \\  & Ours & 0.277 (1) & 5.442 (1) & 0.217 (2) \\   

Table 6: Aesthetic evaluation on SDXL.

   Methods &  &  &  &  &  \\   & HPS & AS & PICKSCORE & HPS & AS & PICKSCORE & HPS & AS & PICKSCORE & HPS & AS & PICKSCORE & HPS & AS & PICKSCORE \\  PCM v/adv & 0.250 & 5.530 & 0.225 & 0.252 & 5.608 & 0.225 & 0.284 & 5.645 & 0.228 & 0.284 & 5.608 & 0.228 & 0.284 & 5.646 & 0.228 \\ PCM v/adv & 0.251 & 4.994 & 0.206 & 0.275 & 5.901 & 0.220 & 0.281 & 5.756 & 0.228 & 0.283 & 5.607 & 0.227 & 0.281 & 5.632 & 0.227 \\   

Table 4: Quantitative comparison for video generation.

   Methods &  &  &  &  &  \\   & HPS & AS & PICKSCORE & HPS & AS & PICKSCORE & HPS & AS & PICKSCORE & HPS & AS & PICKSCORE & HPS & AS & PICKSCORE \\  PCM v/adv & 0.250 & 5.530 & 0.225 & 0.252 & 5.608 & 0.225 & 0.284 & 5.645 & 0.228 & 0.284 & 5.608 & 0.228 & 0.284 & 5.608 & 0.228 \\ PCM v/adv & 0.251 & 4.994 & 0.206 & 0.275 & 5.901 & 0.220 & 0.281 & 5.756 & 0.225 & 0.283 & 5.607 & 0.227 & 0.281 & 5.632 & 0.227 \\  

Table 8: Aesthetic ablation study on the adversarial consistency loss.

   Methods &  &  &  \\   & FID & FD-ClIP & FD-SD & FD-SD & CLIP & SCORE \(\) & FID & FD-ClIP & FD-SD & CLIP & SD-CD & CLIP & SCORE \(\) \\  SDXL-Turbo (512 \(\) 512)  & 19.84 & 13.56 & 9.40 & **32.31** & **15.36** & **5.26** & **6.53** & **27.91** & 0.74 \\ SDXL-Lightning  & **19.73** & 13.33 & **9.11** & 30.81 & 17.99 & 7.39 & 7.29 & 26.31 & 0.76 \\ SDXL-LCM  & **74.65** & 31.63 & 74.46 & 27.29 & 25.88 & 10.36 & 16.64 & 25.84 & 0.66 \\ SDXL-CTM  & 82.14 & 37.43 & 88.20 & 26.48 & 32.05 & 12.50 & 24.06 & 24.79 & 0.66 \\  Ours & 21.23 & 13.66 & 9.23 & 31.55 & 12.87 & 5.62 & 7.03 & 22.10 & **0.83** \\   

Table 7: Aesthetic evaluation on SDXL.

   Methods &  &  &  \\   & FID & FD-ClIP & FD-SD & CLIP & SCORE \(\) & FID & FD-ClIP & FD-SD & CLIP & SD-CD & CLIP \\  SDXL-Turbo (512 \(\) 512)  & 19.84 & 13.56 & 9.40 & **32.31** & **15.36** & **5.26** & **6.53** & **27.91** & 0.74 \\ SDXL-Lightning  & **19.73** & 13.33 & **9.11** & 30.81 & 17.99 & 7.39 & 7.29 & 26.31 & 0.76 \\ SDXL-LCM  & **74.65** & 31.63 & 74.46 & 27.29 & 25.88 & 10.36 & 16.64 & 25.84 & 0.66 \\ SDXL-CTM  & 82.14 & 37.43 & 88.20 & 26.48 & 32.05 & 12.50 & 24.06 & 24.79 & 0.66 \\  Ours & 21.2and negative prompt to GPT-4o and ask it to generate 100 pairs of prompts and their corresponding negative prompts. For each prompt, we generate 10 images. We first generate images without using the negative prompt to show the positive prompt alignment comparison. Then we generate images with positive and negative prompts. We compute the CLIP score of generated images and the prompts and negative prompts. Fig. 6 shows that we not only achieve better prompt alignment but are much more sensitive to negative prompts.

**Consistent generation ability.** Consistent generation ability under different inference steps is valuable in practice for multistep refinement. We compute the average CLIP similarity between the 1-step generation and the 16-step generation for each method. As shown in the rightmost column of Table 2 and Table 5, our method achieves significantly better consistent generation ability.

**Adversarial consistency design and its effectiveness.** We show the ablation study on the adversarial consistency loss design and its effectiveness. **From the architecture level of discriminator**, we compare the latent discriminator shared from the teacher diffusion model and the pixel discriminator from pre-trained DINO . Note that DINO is trained with 224 resolutions, therefore we should resize the generation results and feed them into DINO. We find this could make the generation results fail at details as shown in the left of Fig. 9. **From the adversarial loss**, we compare our adversarial loss to the normal GAN loss. We find normal GAN loss causes the training to be unstable and corrupts the generation results, which aligns with our previous analysis. **For its effectiveness**, we compare the FID-CLIP and FID scores with the adversarial loss or without the adversarial loss under different inference steps. Fig. 7 shows that it greatly improves the FID scores in the low-step regime and gradually coverage to similar performance of our model without using the adversarial loss as the step increases.

**Randomness for sampling.** Fig. 8 illustrates the influence of the randomness introduced in sampling as Eq. 44. The figure shows that introducing a certain of randomness in sampling may help to alleviate unrealistic objects or shapes.

## 5 Limitations and Conclusions

Despite being able to generate high-quality images and videos in a few steps, we find that when the number of steps is very low, especially with only one step, the generation quality is unstable. The model may produce structural errors or blurry images. Fortunately, we discover that this phenomenon can be mitigated through multi-step refinement. In conclusion, in this paper, we observe the defects in latent consistency models. We summarize these defects on three levels, analyze their causes, and generalize the design framework to address these defects.

Acknowledgements

This project is funded in part by National Key R&D Program of China Project 2022ZD0161100, by the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)'s InnoHK, by General Research Fund of Hong Kong RGC Project 14204021. Hongsheng Li is a PI of CPII under the InnoHK.