# CAPro: Webly Supervised Learning with Cross-Modality Aligned Prototypes

Yulei Qin\({}^{1}\) Xingyu Chen\({}^{2}\) Yunhang Shen\({}^{1}\) Chaoyou Fu\({}^{1}\)

Yun Gu\({}^{3}\) Ke Li\({}^{1}\) Xing Sun\({}^{1}\) Rongrong Ji\({}^{4}\)

\({}^{1}\)Tencent YouTu Lab \({}^{2}\)ByteDance

\({}^{3}\)Shanghai Jiao Tong University \({}^{4}\)Xiamen University

yuleiqin@tencent.com

###### Abstract

Webly supervised learning has attracted increasing attention for its effectiveness in exploring publicly accessible data at scale without manual annotation. However, most existing methods of learning with web datasets are faced with challenges from label noise, and they have limited assumptions on clean samples under various noise. For instance, web images retrieved with queries of "_tiger cat_" (a cat species) and "_drumstick_" (a musical instrument) are almost dominated by images of tigers and chickens, which exacerbates the challenge of fine-grained visual concept learning. In this case, exploiting both web images and their associated texts is a requisite solution to combat real-world noise. In this paper, we propose Cross-modality Aligned Prototypes (CAPro), a unified prototypical contrastive learning framework to learn visual representations with correct semantics. For one thing, we leverage textual prototypes, which stem from the distinct concept definition of classes, to select clean images by text matching and thus disambiguate the formation of visual prototypes. For another, to handle missing and mismatched noisy texts, we resort to the visual feature space to complete and enhance individual texts and thereafter improve text matching. Such semantically aligned visual prototypes are further polished up with high-quality samples, and engaged in both cluster regularization and noise removal. Besides, we propose collective bootstrapping to encourage smoother and wiser label reference from appearance-similar instances in a manner of dictionary look-up. Extensive experiments on WebVision1k and NUS-WIDE (Web) demonstrate that CAPro well handles realistic noise under both single-label and multi-label scenarios. CAPro achieves new state-of-the-art performance and exhibits robustness to open-set recognition. Codes are available at https://github.com/yuleiqin/capro.

## 1 Introduction

Large-scale annotated datasets (_e.g._, ImageNet ) were the driving force behind the revolution of computer vision in the past decade, but the expensive and time-consuming collection process now becomes the bottleneck of model scaling. Consequently, researchers seek to crawl web images, where search queries and user tags are directly used as labels. However, the large proportion of noise in web datasets (_e.g._, 20% in JMT-300M , 34% in WebVision1k , and 32% in WebFG496 ) impedes learning visual concepts. Many studies on Webly Supervised Learning (WSL) are conducted to reduce the negative impact of noise and effectively explore web data .

Early WSL methods claim that simply scaling up datasets with standard supervised learning suffices to overcome web noise [3; 11; 2; 12]. Such a solution comes at the cost of huge computation resources, and the supervision source from noisy labels is proved suboptimal [13; 10; 14]. Therefore, various techniques are designed to reduce noise , such as neighbor density , guidance of clean samples [17; 18; 19], confidence bootstrapping [20; 21; 22], and side information [23; 24].

Despite the promising improvement, the above-mentioned methods still face challenges. First, most of them address certain types of noise such as label-flipping noise and out-of-distribution (OOD), neglecting the critical-yet-under-explored **semantic noise**. To clarify, semantic noise is caused by the misalignment between image contents and the associated texts when the search query (_e.g._, class) has multiple and ambiguous interpretations and the retrieved images do not correspond to the correct semantics. Without proper contextual information, it is rather difficult to pinpoint clean examples in polysemy classes. Second, the dominant idea of bootstrapping labels and discarding incorrect data is prone to noise overfitting . The model predictions on individual images vary sharply over training epochs and such inconsistency also makes WSL inefficient. Some methods [4; 25; 26] also maintain peer models and require alternative steps to improve the reliability of bootstrapping. However, their complicated training procedures restrict scalability in practice.

To this end, we propose CAPro: **C**ross-modality **A**ligned **P**rottypes for robust representation learning from web data. Compared with previous prototypical methods [19; 27; 28], CAPro is able to handle label-flipping noise, OOD, and especially the semantic noise that remains unexplored (see Fig. 1).

First, CAPro exploits web data across modalities to formulate _semantically-correct textual and visual prototypes_. Since visual prototypes simply formed with images suffer from semantic ambiguity, we propose **text matching** to leverage textual prototypes to establish their noise-robust estimation. Motivated by successful language models [29; 30; 31], we extract textual knowledge to imply the extent to which one instance is aligned with its textual prototype. Specifically, we prepare descriptive texts (_e.g._, definitions) of each category and project them into the embedding space as textual prototypes via the pre-trained language model. For each sample, its affiliated texts of the website title, caption, and tags are also encoded into the same embedding space. The similarity between a sample and its prototype indicates "cleanness". Since incomplete and mismatched image-text pairs introduce additional noise , we bring in **text enhancement** with text guidance from visually-similar neighbors to mitigate the effect of noisy texts on clean sample selection. We consecutively construct image and text graphs and rerank neighbor candidates for text matching and enhancement. Samples that exactly match the target semantics are chosen as anchors for the initialization of visual prototypes. These visual prototypes are continuously polished up by high-quality web images to improve generalizability and discriminability. During representation learning, the intra-class distance between prototypes and instances is minimized by contrastive learning for regularization. By means of _class-representative visual prototypes_, various kinds of noise can be filtered out.

Second, we propose **collective bootstrapping** (CB) to provide smoother label reference by _extending bootstrapping with collective knowledge_. For each sample, instead of bootstrapping its target independently [33; 34], CAPro keeps bootstrapping the entire dynamic dictionary and provides label reference in the mode of dictionary look-up. The dictionary keys are the learned representations from the sampled data while the query is the current encoded sample. We aggregate model predictions on all keys and use their weighted combination as the pseudo target, where weights are determined by the matching scores of query-key pairs. By penalizing deviation from such targets, The proposed

Figure 1: (a) We explore cross-modality alignment to select clean examples and generate visual prototypes with correct semantics. (b) Collective bootstrapping provides consistent label references and regularization from visual dictionary. (c) Compared to \(765\) unambiguous classes, our advantage is much more highlighted on \(235\) classes where semantic noise prevails due to polysemy concepts.

CB achieves two advantages: 1) It encourages consistent performance among the query and visually similar keys. 2) Unstructured noise is suppressed by referring to the dictionary for label regularization. Our CB can also be viewed as encoding the neighborhood structure of data in the low-dimensional space, where the closely matched keys are neighbors of the query. Inductive propagation of self-labels is implicitly realized through such a structure, which draws on the assumption of manifold regularization  that close samples probably belong to the same class.

In summary, our contributions are summarized as:

* We propose CAPro, a prototypical learning framework to efficiently handle various noises including label-flipping noise, OOD, and semantic noise for webly supervised learning.
* We integrate class prototypes across text and image modalities to align with unambiguous semantics. We verify that text enhancement by visual guidance is the key to handling noisy texts for clean sample selection, which in turn improves visual prototypes.
* We investigate collective bootstrapping as label regularization by matching one query to all keys in a dynamic dictionary for reference. We show that scaling up the nearest neighbors from the mini-batch to the entire dictionary better leverages the visual data structure.

Experiments on WebVision1k and NUS-WIDE (Web) confirm the competitiveness of CAPro with prior state-of-the-art methods. CAPro performs robustly against real-world noise under single-label and multi-label scenarios and demonstrates superiority in open-set recognition.

## 2 Related Work

Webly Supervised Learning (WSL)WSL aims at utilizing abundant but weakly labeled web data. It serves a range of tasks, such as recognition [36; 37; 38; 39; 40; 41], detection [42; 43], and segmentation [44; 45]. In this study, we focus on visual representation learning [46; 11; 2; 3; 47; 12]. To combat noise, previous studies combine web labels with the pseudo labels generated by the model. With respect to the pseudo labels, Hinton _et al_.  adopts soft targets in a fashion of distillation.

Figure 2: Overview of CAPro. Images \(_{i}\) and texts \(_{i}\) are respectively fed into the image and text encoders for features \(_{i}\) and \(_{i}\). Then, \(_{i}\) is projected into the embedding space as \(_{i}\), followed by the reconstruction from \(_{i}\) to \(}_{i}\). Visual prototypes \(^{c}\) are initialized with anchor instances that are selected by matching enhanced texts \(}_{i}\) to textual prototypes \(^{c}\) for semantic alignment. They are constantly polished up by clean images and engage in contrastive learning to constrain cluster distribution. Collective bootstrapping exploits visual dictionary for regularization on the auxiliary classifier output \(_{i}\), where each key embedding is matched to the query for the reference \(_{i}\). Web labels \(y_{i}\) are simultaneously refined as \(_{i}\) for “denoised” supervision on the classifier output \(_{i}\).

Tanaka _et al_.  considers both soft and hard targets and proposes an alternative optimization framework. Yang _et al_.  estimates the correctness of hard targets on a case-by-case basis and dynamically balances the two supervision sources with label confidence. Recently, the idea of prototypical learning  has been applied for WSL. Han _et al_.  predicts self-labels by prototype voting and uses a constant ratio to combine web and pseudo labels. Momentum Prototype (MoPro)  improves prototypes with the momentum update policy  for smooth label adjustment.

The differences between CAPro and the closely related MoPro exactly highlight our contributions. First, we take advantage of both textual and visual prototypes to handle semantic misalignment noise. MoPro neglects such noise and its prototypes could be overwhelmed by irrelevant samples, which impairs the subsequent sample correction. Second, MoPro does not refer to appearance-similar samples for self-labels, which is prone to real-world noise that causes unreasonable label updates. In contrast, CAPro adopts bootstrapping wisely by performing dictionary look-up: the model prediction of the current query sample refers to its matching keys in the dictionary, where the matching score by visual similarity determines the contribution of each key. Third, MoPro only tackles single-label representation learning while CAPro extends prototypical contrastive learning for the multi-label scenario, which is non-trivial due to the intricate nature of noisy multi-labeled data. In that case, CAPro maintains prototypes in subspaces of the shared embedding space, which not only resolves the inter-class contrastive conflicts but also fosters implicit exploitation of label dependency.

Noise-Robust Learning from NeighborsSeveral approaches attempt to correct labels with neighborhood consensus. Both Wang _et al_.  and Guo _et al_.  measure data complexity using local neighbor density for sample selection. Huang _et al_.  progressively discovers neighbors to form decision boundaries in an unsupervised manner. Bahri _et al_.  filters out training data whose label collides with the predictions of its \(k\)-NN. Wu _et al_.  employs topology to only keep the largest connected component of a \(k\)-NN graph. Neighborhood collective estimation  evaluates model confidence on the "cleanness" of each candidate by its neighbors. Ortego _et al_.  identifies correct examples by comparing original labels with the soft labels from their neighbors. Neighbors are also involved in consistency regularization  to encourage similar outputs on samples within the \(k\)-neighborhood. Such inductive label propagation allows correct supervision to transfer directly to mislabeled data via the neighborhood structure.

Unfortunately, the aforementioned methods are not scalable due to the huge complexity of updating a global \(k\)-NN graph frequently. Both Li _et al_.  and Iscen _et al_.  only consider neighbors within a mini-batch for on-the-fly graph construction. However, web data tends to be sparsely distributed and the graph built within mini-batch samples hardly provides reliable neighbors. To deal with such a trade-off, CAPro pinpoints all potential neighbors in the dictionary by matching representations without explicit graph building. We maintain the dictionary as a queue whose length is much larger than the batch size, enabling collective bootstrapping from appropriate neighbors.

Nearest neighbors play a vital role throughout our CAPro, from text enhancement to matching and collective bootstrapping. Compared with previous methods, our mechanism differs in that: 1) We acquire guidance from cross-modality neighbors, where noisy texts are enhanced by image neighbors to alleviate the mismatch problem. In contrast, existing studies investigate neighbors of one modality. 2) We exploit reciprocal structures to filter nearest neighbors for pertinent text matching, while most works neglect those top-ranked false positive neighbors. 3) We resort to neighbors for on-line collective bootstrapping in a manner of dictionary look-up instead of explicit graph construction.

Learning with Visual-Semantic AlignmentVarious tasks seek to learn visual representations for semantic concepts, including retrieval , caption , matching , visual question answer , and zero-shot learning . Recently, learning unified embeddings has been studied for foundation models by language-image pre-training .

In WSL, textual metadata such as titles and hashtags are too scarce to carry out language-image pre-training. Few studies harness both images and texts to learn semantically-correct representations. Zhou _et al_.  designs a co-training scheme to extract semantic embeddings to transfer knowledge from head to tail classes. Cheng _et al_.  builds visual and textual relation graphs to choose prototypes by graph-matching. Yang _et al_.  builds a visual-semantic graph and uses a graph neural network for label refinement. Nevertheless, these methods do not take noisy texts into serious consideration and underestimate their negative effect on seed selection. We also find that image outliers are wrongly kept even with textual concept matching. For example, images of a rugby team are ranked top for the class "tiger-cat" just because the team name "tiger-cat" is frequently mentioned. On the contrary, CAPro introduces text enhancement by smoothing and reranking to improve its robustness to noise. Furthermore, the prototypes in [24; 78] are fixed during training, while CAPro keeps polishing up prototypes with clean examples for better domain generalizability.

Noisy Correspondence RectificationOne paradigm similar to WSL is noisy correspondence rectification or calibration [60; 65; 62; 66; 61; 63; 68; 79]. It tackles the mismatched image and text pairs and aims to simultaneously learn aligned visual and textual embeddings for improved cross-modal retrieval. Huang _et al_.  utilizes the memorization effect of neural networks to partition clean and noisy data and then learns to rectify correspondence. Hu _et al_.  derives a unified framework with contrastive learning to reform cross-modal retrieval as an N-way retrieval. Han _et al_.  proposes a meta-similarity correction network to view the binary classification of correct/noisy correspondence as the meta-process, which facilitates data purification. Our CAPro differs in two aspects: 1) We focus on the label noise where images are wrongly-labeled by keywords or hashtags. Noisy correspondence emphasizes the instance-level mismatch between an image and its associated text. 2) We aim to learn visual representations with categorical labels while most methods on noisy correspondence align image and text embeddings to improve cross-modal retrieval.

## 3 Method

### Problem Definition and Framework Architecture

Given an interested class \(y_{i}\{1,...,C\}\), web data are collected as \(D=\{(_{i},_{i},y_{i})\}_{i=1}^{N}\), where \(_{i}\) and \(_{i}\) respectively denote the image and textual metadata. Due to the noise issues, \(y_{i}\) might not equal to the ground-truth \(y_{i}^{*}\). We aim to optimize a deep model \((_{e};_{c})\) with parameters of an encoder \(_{e}\) and a classifier \(_{c}\). Existing WSL studies often neglect \(_{i}\) and seldom consider the intervention between images and texts. Comparatively, our CAPro unearths \(_{i}\) for aligning visual representations with semantic concepts, which facilitates correction of various kinds of noise.

CAPro consists of the following components (see Fig. 2). **Siamese image encoders** extract features \(_{i},_{i}^{}^{d_{v}}\) from inputs \(_{i}\) and their augmented counterparts \(_{i}^{}\). Following MoCo , parameters of the first query encoder \(_{e}^{1}\) are updated by back-propagation and those of the second key encoder \(_{e}^{2}\) are updated by the momentum method. **A text encoder** generates embeddings \(_{i},^{c}^{d_{t}}\) respectively from the instance \(_{i}\) and the category \(^{c}\). Any off-the-shelf language model can be used with its pre-trained encoder frozen. **A classifier**, via a fully-connected (FC) layer, maps \(_{i}\) to predictions \(_{i}^{C}\) over \(C\) classes. **A projector** distills discriminative low-dimensional embeddings \(_{i}^{d_{p}}\) from \(_{i}\). It has two FC layers, followed by \(_{2}\)-normalization for unit-sphere constraint on \(_{i}\). **A reconstructor**, symmetric to the projector, recovers \(}_{i}\) from \(_{i}\) to be close to \(_{i}\). **An auxiliary classifier**, of the same structure as the classifier, outputs predictions \(_{i}^{C}\) on \(_{i}\). **A dictionary**, implemented as a queue of size \(Q d_{p}\), records keys for both contrastive learning and collective bootstrapping. The latest embeddings \(_{i}^{}\) are enqueued while the oldest are dequeued.

Image encoders and classifiers are trained with a cross-entropy loss. Since features \(_{i}\) contain redundant description that is vulnerable to image corruption and domain gap, we emphasize class-indicative contents by learning a low-dimensional embedding space. Inspired by denoising autoencoders [80; 27], a projector and a reconstructor are designed to optimize the projection from \(_{i}\) to \(_{i}\). An auxiliary classifier helps retain the representation capacity of \(_{i}\).

\[_{i}^{}=-(_{i(y_{i})}),\ _{i}^{ }=\|}_{i}-_{i}\|_{2}^{2}-( _{i(y_{i})}).\] (1)

### Cross-Modality Alignment

Text EncodingFor raw texts in metadata, we remove all html tags, file format extensions, punctuations, digits, and stop words. Then, tokenization is performed in accordance with the language model in use. After that, we obtain the pre-processed metadata \(_{i}\) and use the text encoder to extract \(_{i}\).

Text EnhancementTo handle missing and mismatched texts, we assume that similar images should share similar texts, and consider text enhancement with guidance from visual data structure. One simple way of encoding visual structure is to build a global \(k\)-NN graph on \(_{i}\). However, our preliminary experiments show that the top-ranked neighbors may not be pertinent due to the noise and domain gap. To detect _truly matched neighbors_, we construct a \(k\)-reciprocal-NN graph \(=\{,\}\) and use the re-ranking technique to evaluate neighbor relationship. Each node in \(\) denotes an image and the edge connectivity from \(\) is represented as the adjacency matrix \(A\).

\[A_{ij}=1-d(_{i},_{j})&,_{i} (_{j},k)_{j}(_{i},k),\\ 0&,,\] (2)

where \((_{i},k)\) and \((_{i},k)=\{_{j}|_{j}( _{i},k)_{i}(_{j},k)\}\) respectively denote \(k\)-NN and \(k\)-reciprocal-NN of \(_{i}\). The cosine distance is used here: \(d(_{i},_{j})=1-_{i}_{j}}{ \|_{i}\|\|_{j}\|}.\) Neighbor re-ranking is achieved by re-calculating the pairwise distance. The vanilla cosine distance only weighs relative priority by measuring features, overlooking the context information of overlapped reciprocal neighbors. Hence, Jaccard Distance  is introduced to measure the intersection between reciprocal neighbors. The refined distance \(d^{*}(_{i},_{j})=(d(_{i},_ {j})+d_{J}(_{i},_{j}))\):

\[d_{J}(_{i},\!_{j})\!=\!1\!-\!^{N}\!(V_{_{i}_{k}},V_{_{j}_{k}})}{_{k=1}^{N} \!(V_{_{i}_{k}},V_{_{j}_{k} })},\,V_{_{i}_{j}}\!=\!\{\!(\! -\!d(_{i},_{j}))&,_{j}( _{i},k)\\ 0&,..\] (3)

Given \(\), smoothing is performed on \(=(_{1},_{2},...,_{N})^{N d_{t}}\) via graph convolution : \(}=^{-}^{-} ,=A+I_{N},_{ii}=_{j}_{ij},\) where \(\) refers to the adjacency matrix with self-connection and \(\) is the diagonal degree matrix.

Textual PrototypesTo establish textual prototypes, we do not estimate \(^{c}\) from instances in one class (_e.g._, average) considering the insufficient, noisy nature of metadata. Instead, we refer to WordNet  for the vocabulary hierarchy . For the \(c\)-th class, we extract its definition in WordNet and expand context with its siblings (synonyms), children (hyponyms), and parents (hypernyms). Then, we get \(^{c}\) and encode it for \(^{c}\). Such prototypes \(^{c}\) have two advantages: 1) It enriches semantic representations of classes. For instance, the comprehensive text of the class "tiger cat" is _a cat having a striped coat; domestic_cat, house_cat, felis_domesticus, felis_catus: any domesticated member of the genus Felis._ It provides disambiguation to web instances of "tiger-cat" (_medium-sized wildcat in Central South America_) and "tiger, cat" (_large feline of forests in most of Asia having a funny coat with black stripes; endangered_). 2) It reveals the underlying inter-class relationship by language encoding. The structural information of class hierarchy is hard to infer from metadata instances but can be directly indexed in WordNet.

Text MatchingWith textual prototypes as queries, web instances with correct semantics can be retrieved by matching queries to their embeddings as keys. To improve precision, the same distance measurement in Eq. (3) for \(k\)-reciprocal-NN encoding is adopted to rerank the matched candidates. We sort samples by distance in an ascending order, and select the top-\(K\) as clean set \(D_{K}\).

\[D_{K}=D_{K}^{1} D_{K}^{2}... D_{K}^{C},D_{K}^{c}=\{(_{i}, _{i},y_{i})|(y_{i}=c)(d^{*}(}_{i},^{ c})_{K}^{c})\},\] (4)

where \(_{K}^{c}\) denotes the \(K\)-th smallest distance in the \(c\)-th class.

Visual Prototypes\(D_{K}\) plays an anchoring role in shaping visual prototypes. We initialize the \(c\)-th prototype \(^{c}\) by averaging instances in \(D_{K}^{c}\): \(}^{c}=_{_{i} D_{K}^{c}}_{ i},^{c}=}^{c}}{\|}^{c}\|_{2}}.\) Given such a good starting point, visual prototypes are consistently polished up by trustworthy web examples with a momentum coefficient \(m_{p}\): \(}^{c}=m_{p}^{c}+(1-m_{p})_{i},\ ^{c}=}^{c}}{\|}^{c}\|_{2}}.\) We perform instance-prototype contrastive learning to pull instances around their prototypes and push apart different class clusters. Instance-level discrimination is also encouraged to improve separation across classes.

\[_{i}^{}=-_{i}^{y_{i }}/)}{_{c=1}^{C}(_{i}^{c}/)},\ _{i}^{}=-_{i}^{ }_{i}/)}{_{j=1}^{Q}(_{i}^{}_{j} /)},\] (5)

where \(\) is a temperature coefficient.

Noise RemovalNoisy instances can be filtered out by self-prediction and instance-prototype similarity. We refer to MoPro  for rules of label adjustment by \(_{i}^{C}\):

\[_{i}=_{i}+(1-)_{i},\;_{i(k) }=_{i}^{k}/)}{_{c=1}^{C}( _{i}^{c}/)},\] (6)

where \(\) balances two terms. Given a threshold \(0 1\), the pseudo-label \(_{i}\) is estimated by:

\[_{i}=\{y_{i}&_{i} D_{K}, \\ _{c}_{i(c)}&_{c}_{i(c)}>,\\ y_{i}&_{i(y_{i})}>1/C,\\ (OOD)&..\] (7)

The above control flow guarantees continuous guidance from \(D_{K}\) on cluster separation. If the highest score of \(_{i}\) is above \(\), the label will be changed accordingly. To prevent aggressive elimination of hard examples, we keep an instance till the next epoch so long as its confidence is above average. Otherwise, it is removed as OOD. The refined label \(_{i}\) successively affects Eqs. (1) and (5).

### Collective Bootstrapping

Due to memorization , as the training epoch increases, deep models will be prone to overfit noise even with the carefully designed logic of noise removal. We assume that overfitting occurs less dramatically when a majority can be consulted for the model to avoid over-confident decision on one single instance. With regard to the consultancy basis, the low-dimensional embedding is a good choice because its distilled description about visual contents is robust enough. Therefore, we propose to exploit the large dictionary, which is originally set up for instance-wise contrastive learning, to realize collective bootstrapping by dictionary look-up. The matching scores of the current query \(_{i}\) to all keys \(^{}_{j}\) in the dictionary act as the weights for the bootstrapped representations \(_{i}^{C}\).

\[_{i}\!=\!_{j=1}^{Q}w_{ij}(^{}_{j}+(1- )^{}_{j})\;w_{ij}\!=\!_{i} ^{}_{j}/)}{_{j=1}^{Q}(_{i}^{}_{j}/)},\;^{}_{j(k)}\!=\!^{ }_{j}^{k}/)}{_{c=1}^{C}(^{}_ {j}^{c}/)}.\] (8)

We minimize the difference between predictions and bootstrapping targets via a KL-divergence loss.

\[^{}_{i}=D_{KL}(_{i}_{i}) =_{c=1}^{C}_{i(c)}_{i(c)}}{_{i(c) }}.\] (9)

It not only allows collaborative contribution to individual soft label estimation, but also encourages consistent performance on visually similar examples. Note that such regularization is imposed on the auxiliary classifier \(_{i}\). Compared with \(_{i}\), constraints on \(_{i}\) coincide with our contrastive learning setting without potential conflicts with the hard label assignment in Eq. (7). The total objective is: \(=_{i=1}^{N}(1-^{})^{}_ {i}+^{}^{}_{i}+^{} ^{}_{i}+^{}^{ }_{i}+^{}^{}_{i}.\)

## 4 Experiments

### Experimental Setup

We evaluate CAPro on WebVision1k  (Google500 ) and NUS-WIDE (Web)  for single-label and multi-label representation learning, respectively. They contain image-text pairs which are in line with our WSL setting. All datasets under investigation are described in Sec. A. We perform ablation studies on Google500 and NUS-WIDE for low cost without losing generalization [22; 78]. The R50 /MiniLM (L6)  are used as image/text encoders by default. Exhaustive details about hyper-parameters, implementation, and training are elaborated in Secs. B C and Algo. 1.

### Comparison with the SOTA

Table 1 reports the top1/top5 accuracy of WebVision1k and Google500. Results of the SOTA methods trained and evaluated on the same datasets are quoted here. Due to different choices of image encoders and training strategies, prior methods may not be directly comparable. For example, VSGraph adopts

the same R50, but is trained with a batch-size of 1024. The benefits of a larger batch size have been validated in NCR, where the same method achieves 75.7% and 73.9% in top1 accuracy respectively for the batch size of 1024 and 256. We believe batch size is the reason that a vanilla baseline  surpasses most SOTAs. Due to the limited budget, training with a batch size of 1024 is currently not affordable, but we will experiment in the future. In this case, methods within each row group of Table 1 are fairly comparable with each other, including the vanilla trained by a cross-entropy loss.

CAPro achieves quite competitive performance on WebVision1k, with an improvement of 1.6% (top1 accuracy) over our vanilla. Although SCC and VSGraph respectively opt for stronger backbones (_e.g._, R50D) and longer training epochs (_e.g._, 150) with a larger batch size (_e.g._, 1024), CAPro still excels in terms of the top5 accuracy. Furthermore, our gain of 1% (top1 accuracy) on ImageNet1k demonstrates that CAPro is robust to the domain gap between web and real-world datasets. Web data include advertisements, artworks, and renderings that differ from realistic photographs. On Google500 and ImageNet500, CAPro outperforms existing methods despite our disadvantages.

Table 2 reports per-Class F1 (C-F1), Overall F1 (O-F1), and mean Average Precision (mAP) on NUS-WIDE. Most prior multi-label methods are developed for ground-truth labels, while we are concerned with noisy WSL settings. Under this circumstance, CAPro is compared with methods that are trained on NUS-WIDE (Web) and evaluated on clean testing set. Following [96; 78], the top three categories of an image by prediction confidence are chosen for metric calculation. CAPro reaches the SOTA with a significant increase of 1.5% (C-F1), 3.0% (O-F1), and 9.7% (mAP) over our vanilla.

### Discussion on Open-Set Recognition

To verify if CAPro can identify outliers of unknown categories, we conduct experiments on open-set recognition. Specifically, we train CAPro on Google500 and validate on the testing sets of WebVision1k and ImageNet1k. Images from the remaining 500 classes all belong to one open-set

   & Back- &  & ImageNet1k & Google500 & ImageNet500 \\  & bone & Top1 & Top5 & Top1 & Top5 & Top1 & Top5 & Top1 & Top5 \\  MentorNet  & IRV2  & 72.6 & 88.9 & 64.2 & 84.8 & – & – & – & – \\ Curriculum  & IV2  & 72.1 & 89.1 & 64.8 & 84.9 & – & – & – & – \\ Multimodal  & IV3  & 73.2 & 89.7 & – & – & – & – & – & – \\  Vanilla  & R50D  & 75.0 & 89.2 & 67.2 & 84.0 & 75.4 & 88.6 & 68.8 & 84.6 \\ SCC  & R50D & 75.3 & 89.3 & 67.9 & 84.7 & **76.4** & 89.6 & 69.7 & 85.3 \\  Vanilla\({}^{}\) & R50 & 74.2 & 89.8 & 68.2 & 86.2 & 66.9 & 82.6 & 61.5 & 78.8 \\ CoTeach [20; 78] & R50 & – & — & — & — & 67.6 & 84.0 & 62.1 & 80.9 \\ VSGraph\({}^{}\) & R50 & **75.4** & 90.1 & **69.4** & 87.2 & 68.1 & 84.4 & 63.1 & 81.4 \\  Vanilla  & R50 & 72.4 & 89.0 & 65.7 & 85.1 & – & – & – & – \\ SOMNet  & R50 & 72.2 & 89.5 & 65.0 & 85.1 & – & – & – & – \\ Curriculum  & R50 & 70.7 & 88.6 & 62.7 & 83.4 & – & – & – & – \\ CleanNet  & R50 & 70.3 & 87.7 & 63.4 & 84.5 & – & – & – & – \\ SINet  & R50 & 73.8 & **90.6** & 66.8 & 85.9 & – & – & – & – \\ NCR  & R50 & 73.9 & – & – & – & – & – & – \\ NCR\({}^{}\) & R50 & 75.7 & – & – & – & – & – & – \\ MILe  & R50 & 75.2 & 90.3 & 67.1 & 85.6 & – & – & – & – \\ MoPro  & R50 & 73.9 & 90.0 & 67.8 & 87.0 & – & – & – \\  Vanilla (ours) & R50 & 72.6 & 89.7 & 67.0 & 86.8 & 69.9 & 86.5 & 64.5 & 83.1 \\ CAPro (ours) & R50 & 74.2 & 90.5 & 68.0 & **87.2** & 76.0 & **91.3** & **72.0** & **89.2** \\   \({}^{}\) Results on WebVision1k are under optimized training settings with batch size of 1024.

Table 1: Results on WebVision1k and Google500. Best/2nd best are marked bold/underlined.

   & Back- &  & WebVision & ImageNet \\  & bone & C-F1 & O-F1 & mAP \\  Vanilla  & R50 & 37.5 & 39.6 & 43.9 \\ VSGraph  & R50 & 38.6 & 40.2 & 44.8 \\ MCPL  & R101 & 22.5 & 17.2 & 47.4 \\  Vanilla (ours) & R50 & 37.8 & 42.4 & 38.3 \\ CAPro (ours) & R50 & **39.3** & **45.4** & **48.0** \\   \({}^{}\) Results on WebVision1k are under optimized training settings with batch size of 1024.

Table 2: Results on NUS-WIDE (Web).

   & WebVision & ImageNet \\  & C-F1 & C-F1 \\  Vanilla  & R50 & 37.5 & 39.6 & 43.9 \\ VSGraph  & R50 & 38.6 & 40.2 & 44.8 \\ MCPL  & R101 & 22.5 & 17.2 & 47.4 \\  Vanilla (ours) & R50 & 37.8 & 42.4 & 38.3 \\ CAPro (ours) & R50 & **39.3** & **45.4** & **48.0** \\   \({}^{}\) Results on WebVision1k are under optimized training settings with batch size of 1024.

Table 3: Results on open-set recognition.

category. We follow [97; 98; 78] to classify an image as open-set if its highest prediction confidence is below a threshold. The average C-F1 is adopted to reflect whether a model can discriminate between base and novel classes (501 in total). Table 3 confirms our superiority over existing methods, showing that CAPro is capable of detecting semantic novelty. More analysis can be found in Sec. E.

### Ablation Study

Text Encoding and EnhancementTable 4 reveals the benefit from cross-modality alignment. For the method without text encoding and enhancement, we sample \(K\) examples randomly from each category. These instances barely provide reliable prototypes with semantic correctness. With respect to the text encoder, we additionally validate XLNet (base)  and GPT-Neo (1.3B) . MiniLM surpasses XLNet by a minor margin, but both exhibit similar performance with our enhancement. GPT-Neo displays its power even with the plain \(k\)-NN-based smoothing in VSGraph , implying that advanced a Large Language Model (LLM) would boost CAPro. Note that encoders in CLIP  are not applicable here to avoid visual data leakage. All methods with text encoding and enhancement outperforms the vanilla one, validating the guidance from textual knowledge. Besides, we notice an increase up to 3.8%/4.7% on Google500/ImageNet500 by our text enhancement, showing that proper noise suppression is indispensable. Fig. 3 presents a comparison on the top-\(K\) matched instances. Enhancement in VSGraph can filter out OOD to a certain degree, but can do nothing with lexical confusion (_e.g._, food in _Drumstick_ and players in _Tiger cat_). More qualitative results are in Sec. D.1.

Reference ProviderOur collective bootstrapping is compared against commonly used regularization methods which provide reference on targets. Surprisingly, most existing methods bring about no or even negative impact. In one respect, bootstrapping and label smoothing are already implicitly inherited in our framework as Eqs. (6) and (7). Therefore, no further gains can be seized. As for SCC, its estimated confidence may not comply with our Eq. (6), which leads to incompatible optimization. NCR has two drawbacks: 1) The chance of similar instances appear in one mini-batch is fairly small for large web datasets. 2) It only counts on self-prediction as reference source which is fragile to noise. In contrast, CAPro is enlightened by MoCo to maintain the dictionary as a queue, which enlarges the number of reference objects beyond instances in a mini-batch. Our enriched sources from both self-prediction and instance-prototype similarity expedite a steady learning progress. Moreover, mix-up improves CAPro in top1 but lowers top5. It adopts convex combinations for both inputs and targets, enforcing a stronger regularization than our CB where we only recombine targets. For WebVision1k, examples with noisy labels still resemble prototypes and therefore neighbor knowledge brings useful reference. Mix-up does not consider appearance similarity and causes over-regularization.

\(^{}\) and top-\(K\)Fig. 4 shows that an increasing \(^{}\) triggers off worse results on Google500 and ImageNet500. This suggests that collective knowledge should not overwhelm individual instance decision. With regard to top-\(K\) on prototype initialization, there exists a trade-off between domain-variety and class-purity. The rise of \(K\) increases diversity but also the risk of noise.

  Text & Text Enhan- & Reference &  &  &  \\ Encoding & cement & Provider & Top1 & Top5 & Top1 & Top5 & C-F1 & O-F1 & mAP \\  \(\) & \(\) & \(\) & 71.5 & 87.8 & 66.5 & 84.6 & 37.2 & 42.4 & 46.2 \\ MiniLM & VSGraph  & \(\) & 72.0 & 88.0 & 66.9 & 85.4 & 39.2 & 44.4 & 46.8 \\ MiniLM & ✓ (ours) & \(\) & 75.5 & 91.0 & 71.5 & 88.8 & 39.3 & 44.9 & 47.4 \\ XLNet & VSGraph  & \(\) & 71.6 & 87.8 & 66.8 & 84.8 & 38.6 & 43.4 & 47.6 \\ XLNet & ✓ (ours) & \(\) & 75.4 & 91.0 & 71.5 & 88.8 & 39.3 & 45.1 & 47.5 \\ GPT-Neo & VSGraph  & \(\) & 72.0 & 88.0 & 67.2 & 85.3 & 39.2 & 45.0 & 47.4 \\ GPT-Neo & ✓ (ours) & \(\) & 75.7 & 91.1 & 71.6 & 88.8 & 39.2 & 45.1 & 47.6 \\  MiniLM & ✓ (ours) & Mix-up (MU)  & 75.7 & 90.9 & 71.4 & 88.6 & 38.7 & 45.3 & 47.2 \\ MiniLM & ✓ (ours) & Bootstrap  & 75.5 & 90.8 & 71.3 & 88.4 & 38.1 & 43.2 & 46.0 \\ MiniLM & ✓ (ours) & Label smooth  & 75.4 & 90.8 & 71.2 & 88.4 & 36.9 & 42.1 & 46.8 \\ MiniLM & ✓ (ours) & SCC  & 73.8 & 89.9 & 70.2 & 88.0 & 35.6 & 41.3 & 45.0 \\ MiniLM & ✓ (ours) & NCR  & 75.5 & 91.1 & 71.5 & 88.8 & 37.6 & 43.4 & 46.8 \\ MiniLM & ✓ (ours) & ✓ CB (ours) & 76.0 & 91.3 & 72.0 & 89.2 & 39.3 & 45.4 & 48.0 \\ MiniLM & ✓ (ours) & ✓ CB (ours) + MU & 76.5 & 91.1 & 71.9 & 88.8 & 40.4 & **46.7** & 49.9 \\ GPT-Neo & ✓ (ours) & ✓ CB (ours) & 76.1 & **91.4** & **72.1** & **89.4** & 39.3 & 44.9 & 47.7 \\ GPT-Neo & ✓ (ours) & ✓ CB (ours) + MU & **76.5** & 91.2 & 72.0 & 88.8 & **40.7** & 45.2 & **50.0** \\  

Table 4: Ablation study on text encoding, enhancement, and reference provider.

More ablation studies on the **threshold \(\)**, the **update frequency** of visual prototypes, and the **noise removal policy** can be found in Secs. D.2, D.3, and D.4, respectively. Empirical guidelines on **hyper-parameter tuning** are concluded in Sec. F. We also provide analysis of **failure cases** in Sec. G. Our computation cost with respect to performance gains can be found in Sec. H.

## 5 Conclusion

CAPro utilizes web datasets to learn visual representations that are aligned with correct semantics. Cross-modality alignment and collective bootstrapping are corroborated as two keys to improve WSL. The benefits of building prototypes are two-fold: 1) Noisy web data whose visual and textual descriptions can be efficiently removed by simply measuring the distance between an instance and its prototype in the embedding space. 2) The inter-class relationship can be statistically studied by comparing each instance to all class prototypes, which may shed light on visual similarity for species. Three potential drawbacks should be considered: 1) the limited intra-class diversity with less tolerance to the minority in one category. Images crawled from websites follow the long-tailed distribution, which means that the more common or typical one instance is, the greater the likelihood that it gets exposed online. Over-emphasis on the purity of class prototypes leads to false negatives on the recognition of atypical samples. One possible solution is to introduce randomness into the initialization and update of prototypes to improve generalization. 2) the noteworthy domain gap and bias of web data. Even image contents are correct, their styles (_e.g._, advertising photos, artworks, and rendering) are different from the realistic datasets. When it comes to modalities such as infrared or medical tomography images, there exist very few images online. Therefore, it is encouraged to prepare realistic images for guided-training and evaluation, where early-stopping and normalization techniques can be used to avoid overfitting. 3) the accuracy of prior knowledge about class hierarchy. Since definitions of class prototypes rely on the systematic understanding of concepts, improper, coarse-grained, or even wrong descriptions would devalue the semantic alignment. A thorough analysis on class concepts is a requisite to developing prototypes. Future work includes extension to other modalities (_e.g._, audio and video clips) and to other tasks (_e.g._, semi-supervised learning).

Broader ImpactCAPro manoeuvres language models for visual concept learning, where LLMs (_e.g._, GPT-Neo) can deliver promising results for future development. Regardless of data sources, we showcase a cost-efficient and practical way to utilize cross-modality data. It also promotes rethinking the key-value matching mechanism for creative usages. For example, the visual dictionary originally built for instance-wise contrastive learning is re-discovered for our collective bootstrapping.

Figure 4: Impact of hyper-parameters \(^{}\) and top-\(K\) on CAPro.

Figure 3: Top-matched WebVision1k instances are chosen: (a) without text enhancement, (b) with text enhancement in VSGraph , and (c) with our text enhancement.