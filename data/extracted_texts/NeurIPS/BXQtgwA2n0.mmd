# Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization

Xiangsen Wang\({}^{1}\)   Haoran Xu\({}^{2}\)   Yinan Zheng\({}^{3}\)   Xianyuan Zhan\({}^{3,4}\)

\({}^{1}\) Beijing Jiaotong University \({}^{2}\) UT Austin \({}^{3}\) Tsinghua University

\({}^{4}\) Shanghai Artificial Intelligence Laboratory

wangxiangsen@bjtu.edu.cn, haoran.xu@utexas.edu, zhengyn23@mails.tsinghua.edu.cn, zhanxianyuan@air.tsinghua.edu.cn

Work done during the internships with Institute for AI Industry Research (AIR), Tsinghua University.Corresponding Author.

###### Abstract

Offline reinforcement learning (RL) has received considerable attention in recent years due to its attractive capability of learning policies from offline datasets without environmental interactions. Despite some success in the single-agent setting, offline multi-agent RL (MARL) remains to be a challenge. The large joint state-action space and the coupled multi-agent behaviors pose extra complexities for offline policy optimization. Most existing offline MARL studies simply apply offline data-related regularizations on individual agents, without fully considering the multi-agent system at the global level. In this work, we present OMIGA, a new offline multi-agent RL algorithm with implicit global-to-local value regularization. OMIGA provides a principled framework to convert global-level value regularization into equivalent implicit local value regularizations and simultaneously enables in-sample learning, thus elegantly bridging multi-agent value decomposition and policy learning with offline regularizations. Based on comprehensive experiments on the offline multi-agent MuJoCo and StarCraft II micro-management tasks, we show that OMIGA achieves superior performance over the state-of-the-art offline MARL methods in almost all tasks. Our code is available at https://github.com/ZhengYinan-AIR/OMIGA.

## 1 Introduction

Multi-agent reinforcement learning (MARL) is an active research area to tackle many real-world problems that involve multi-agent systems, such as autonomous vehicle coordination, network traffic routing , and multi-player strategy games . Although MARL has made some impressive progress in recent years, most of its successes are restricted to simulation environments. In most real-world applications, building high-fidelity simulators can be rather costly or even infeasible, and online interaction with the real system during policy learning is also expensive or risky. Inspired by the recently emerged offline RL methods [4; 5; 6; 7; 8; 9; 10], equipping MARL with offline learning capability has become an attractive direction to tackle real-world multi-agent tasks.

Offline RL focuses on learning optimal policies with pre-collected offline datasets with no further environmental interaction. Under the offline setting, evaluating value function on out-of-distribution (OOD) samples can cause extrapolation error accumulation on Q-values, leading to erroneously overestimated values and misguiding policy learning [4; 5]. This issue, which is also called distributional shift , is one of the core challenges in offline RL. Current offline RL methods tackle this issue primarily by incorporating various forms of data-related regularizations to restrict policy learningfrom deviating too much from the behavioral data. However, extending the same offline regularization technique to the multi-agent setting poses some unique challenges. The joint state-action space grows exponentially with the number of agents, making the global-level regularization hard to compute and may also result in very sparse constraints under the huge joint state-action space, especially when the size and coverage of the offline dataset are limited.

To tackle the challenges, some recent works [11; 12; 13] attempt to design offline MARL algorithms heuristically by enforcing local-level regularizations. By using the Centralized Training with Decentralized Execution (CTDE) framework  to decompose the global value function into a combination of local value functions, these works turn the original offline multi-agent RL problem into the offline single-agent RL problem and apply policy or value regularizations at the local level. While straightforward and easy to implement, simply enforcing local-level regularization cannot guarantee the induced regularization at the global-level still remains valid, which can potentially lead to over-conservative policy learning. Existing approaches offer no guarantee whether the optimized local policies are jointly optimal under a given value decomposition scheme. Furthermore, because the offline regularization of most existing methods is completely imposed from the local-level without considering the global information, these methods fail to capture coordinated agent behaviors and credit assignment in the multi-agent system.

In this paper, we aim to give a principled approach to the offline MARL problem. We start by imposing global-level regularizations to form a _behavior-regularized_ MDP. We then find that using some particular regularization choice in this MDP will have closed-form solutions, which naturally turn the global-level regularizations into local-level ones, under some mild assumptions. Finally, we show that the local-level regularization can be derived in an implicit way by using only samples from the offline datasets, without knowing the behavior policy or approximating the regularization term. We dub this new algorithm as offline multi-agent RL algorithm with implicit global-to-local value regularization (OMIGA). In OMIGA, the local-level value regularization is rigorously derived from the global regularization under value decomposition, thus also capturing global information and the impact of multi-agent credit assignment. Under this design, we can also guarantee that the learned local policies are jointly optimal at the global level. Also, OMIGA enables complete in-sample learning without querying OOD action samples, which enjoys better stability during policy learning.

We evaluate our method using various types of offline datasets on both multi-agent MuJoCo  and StarCraft Multi-Agent Challenge (SMAC) tasks . Under all settings, OMIGA achieves better performance and enjoys faster convergence compared with other strong baselines.

## 2 Related Work

**Offline reinforcement learning.** Due to the absence of online data collection, offline policy learning suffers from severe distributional shift and exploitation error accumulation issues when evaluating the value function on OOD actions [4; 5]. As a result, existing offline RL methods adopt various approaches to learn policies pessimistically and regularize policy learning close to data distribution. Policy constraint methods [4; 5; 9; 17; 18; 19] add explicit or implicit policy constraints to regularize the policy from deviating too much from behavioral data. Value regularization methods [20; 21; 22; 23] learn conservatively estimated value functions on OOD data. Uncertainty-based methods add uncertainty penalization terms on value functions [24; 25] or rewards [26; 27; 28] to enable pessimistic policy learning. Recently, in-sample learning methods [7; 8; 10; 29] provide a new direction to avoid the distributional shift by learning value functions and policies completely within data. These methods eliminate the involvement of policy-generated OOD samples during policy evaluation, thus enjoying superior learning stability. Our method embeds multi-agent value decomposition within the in-sample learning paradigm, which fully exploits the multi-agent problem structure for offline policy optimization.

**Multi-agent reinforcement learning.** The complexity of multi-agent RL problems typically arises from the huge joint action space . In recent years, the CTDE framework [31; 32] has been proposed to decouple agents' learning and execution phases to handle the exploding action space issue. Under the CTDE framework, agents are trained in a centralized manner with global information and make decisions with learned local policies during execution. Some representative works are the value decomposition methods [33; 34; 35; 36], which decompose the global Q-value function into a set of local Q-value functions for scalable multi-agent policy optimization.

There have been some recent attempts to design MARL algorithms for the offline setting, which can be broadly categorized as RL-based [11; 12; 13; 37] and goal-conditioned supervised learning (GCSL) based methods [38; 39]. Existing RL-based methods are typically built upon the CTDE framework and perform offline policy regularization at the local level. For example, ICQ  uses importance sampling to implicitly constrain local policy learning on OOD samples. OMAR  adopts CQL  to learn local Q-value functions and adds zeroth-order optimization to avoid bad local optima. In these methods, the multi-agent modeling and offline RL ingredients are fragmented, which cannot guarantee that the local-level regularizations still remain valid at the global level, and also ignores the cooperative behavior among agents. Moreover, there is no consideration of the value decomposition's influence on the optimal local policies learned with offline regularizations. On the other hand, GCSL-based methods [38; 39] leverage the sequential data modeling capability of transformer architecture to solve offline MARL tasks. However, recent studies also show that GCSL-based methods can be over-conservative  and cannot guarantee optimality under stochastic environments [40; 41].

In this work, our proposed OMIGA is an RL-based method, which uses an implicit global-to-local value regularization scheme to tackle the aforementioned limitations, and offers an elegant solution to marry multi-agent modeling and offline learning.

## 3 Preliminaries

### Notations

We focus on the fully cooperative multi-agent tasks, which can be modeled as a multi-agent Partially Observable Markov Decision Process (POMDP) , defined by a tuple \(G=,,,r,,,n,\). \(s\) denotes the true state of the environment. \(\) is the action set for each of the \(n\) agents. At each step, an agent \(i\{1,2,...n\}\) chooses an action \(a_{i}\), forming a joint action \(=(a_{1},a_{2},...a_{n})^{n}\). \(P(^{}|,):^{n}\) is the transition dynamics to the next state \(^{}\). \([0,1)\) is a discount factor. In the partial observable environment, each agent receives an observation \(o_{i}\) at each step based on an observation function \((,i): N\), and we denote \(=(o_{1},o_{2},...o_{n})\). All agents share the same global reward function \(r(,):^{n}\). In cooperative MARL, all agents aim to learn a set of policies \(_{tot}=\{_{1},,_{n}\}\) that jointly maximize the expected discounted returns \(_{_{tot},}[_{t=0}^{ }^{t}r(_{t},_{t})]\). Under the offline setting, a pre-collected dataset \(\) is obtained by sampling with the behavior policy \(_{tot}=\{_{1},,_{n}\}\) and the policy learning is conducted entirely with the data samples in \(\) without any environment interactions.

### CTDE Framework and Value Decomposition

In MARL, the joint action space increases exponentially with the increase in the number of agents. Therefore, it is difficult to directly query an optimal joint action from the global Q-value function \(Q_{tot}(,)\), and the global Q-value function could be negatively affected by the suboptimality of individual agents. To address these problems, the Centralized Training with Decentralized Execution (CTDE) framework [14; 31; 32] is proposed. In the training phase, agents can access the full environment information and share each other's experiences. In the execution phase, each agent chooses actions only according to its individual observation \(o_{i}\). Through the CTDE framework, optimization at the individual level results in the optimization of the joint action space, which avoids the aforementioned problems. Value decomposition methods [33; 34; 35; 36] are popular solutions under the CTDE framework to achieve the decomposition of the joint action space. Value decomposition is also used in recent offline MARL methods [11; 12], which decomposes the global Q-value function \(Q_{tot}(,)\) into a linear combination of local Q-value functions \(Q_{i}(o_{i},a_{i})\):

\[Q_{tot}(,)=_{i}w_{i}()Q_{i}(o_{i},a_{i})+b (),\] \[w_{i} 0,\  i=1,n\] (1)

where \(w_{i}()\) and \(b()\) capture the weights and offset on local Q-value functions.

### Behavior-Regularized MDP in Offline RL

To avoid the distributional shift in offline RL, existing approaches typically incorporate various data-related regularizations on rewards, value functions, or the policy optimization objective. In our work, we are specifically interested in the treatment that adds a behavior regularizer to the rewards \(r(s,a)\) to penalize OOD samples . This framework leads to a special behavior-regularized MDP, which optimizes a policy with the following objective:

\[_{}[_{t=0}^{}^{t}(r(s_{t},a_{t })- f((a_{t}|s_{t}),(a_{t}|s_{t}) ))],\] (2)

where \(\) is a scale parameter, and \(f(,)\) is the function that captures the divergence between \(\) and \(\), which is similar to the entropy regularization in SAC . In the above objective, we regularize the deviation between the learned policy \(\) and behavior policy \(\), so as to avoid the distributional shift issue. This behavior-regularized MDP corresponds to the following modified policy evaluation operator \(_{f}\):

\[(_{f}^{})Q(s,a):=r(s,a)+_{s^{ }|s,a}[V(s^{})]\]

\[(_{f}^{})V(s):=_{a}[r(s,a)+ _{s^{}|s,a}[V(s^{})]],\]

where

\[V(s)=_{a}[Q(s,a)- f((a_{t}|s_{t} ),(a_{t}|s_{t}))].\]

In the next section, we will derive our proposed method OMIGA based on the behavior-regularized framework, and demonstrate the benefits and desired properties of this framework for the multi-agent setting.

## 4 Method

In this section, we formally present our implicit global-to-local value regularization approach OMIGA for offline MARL and explain how it can be integrated into effective offline learning. We begin with the multi-agent POMDP with a reverse KL global value regularization added to the rewards. We then show how we can make an equivalent reformulation that naturally converts the global-level regularization into local-level value regularizations. Lastly, we derive a practical algorithm that implicitly enables local-level value regularizations via in-sample learning and provide a thorough discussion of it.

### Multi-Agent POMDP with Global Value Regularization

When extending Eq.(2) to multi-agent POMDP, we can get the following learning objective:

\[_{_{tot}}[_{t=0}^{}^{t}(r( _{t},_{t})- f(_{tot}( _{t}|_{t}),_{tot}(_{t }|_{t})))]\]

However, unlike the single-agent setting, it is difficult to directly compute the regularization term between the global policy \(_{tot}\) and the global behavior policy \(_{tot}\) due to the huge state-action space in multi-agent RL. Can we choose a function \(f\) to allow certain forms of policy decomposition so as to simplify the calculations?

In this work, we find that choosing the regularization function \(f\) to be the reverse KL divergence, which means \(f(_{tot},_{tot})=(_{tot}/_{tot})\), leads to natural decomposition of the global regularization into the summation of a set of local regularizations, by only assuming the factorization structure of the behavior policy (i.e., \(_{tot}(|)=_{i=1}^{n}_{i}(a_{i}|o_{ i})\)). Note that using reverse KL divergence as the behavior constraint has been widely studied in both online and offline single-agent RL literature [43; 17], and has been shown to produce the nice mode-seeking behavior to avoid OOD actions in the offline setting . Plug in the specific form of \(f\), and we obtain the following global policy evaluation operator:

\[(_{f}^{_{tot}})Q_{tot}(,):=r(,)+_{^{} |,}[V_{tot}(^{})]\] (3)

\[(_{f}^{_{tot}})V_{tot}():=_{ _{tot}}[r(,)+ _{^{}|,}[V_{ tot}(^{})]],\] (4)where

\[V_{tot}()=_{_{tot}}[Q_{tot}(,)- ((|)}{_{tot}(|)})]\] (5)

**Theorem 4.1**.: _Define \(^{*}_{f}\) the case where the global policy in \(^{_{tot}}_{f}\) is the optimal policy \(^{*}_{tot}\), then \(^{*}_{f}\) is a \(\)-contraction._

The proof is in Appendix A. This theorem indicates global Q-value will converge to the Q-value under the optimal policy \(^{*}_{tot}\) when applying the fixed-point iteration with the policy evaluation operator.

We now aim to analyze and derive the closed-form solution of the optimal value functions \(Q^{*}_{tot}\) and \(V^{*}_{tot}\). According to the Karush-Kuhn-Tucker (KKT) conditions where the derivative of a Lagrangian objective function with respect to the global policy is zero at the optimal solution, we have the following proposition:

**Proposition 4.2**.: _For a behavior-regularized multi-agent POMDP with \(f(_{tot},_{tot})=(_{tot}/_{tot})\), the optimal global policy \(^{*}_{tot}\) and its optimal value functions \(Q^{*}_{tot}\) and \(V^{*}_{tot}\) satisfy the following optimality condition:_

\[& Q^{*}_{tot}(,)=r(,)+ _{^{}|,}[V^{*}_{tot}(^ {})]\\ & V^{*}_{tot}()=u^{*}()+\\ &^{*}_{tot}(|)=_{tot}(|) (_{tot}(,)-u^{*}()}{}-1)\] (6)

_where \(u()\) is a normalization term and has a optimal value \(u^{*}\) that makes the corresponding optimal policy \(^{*}_{tot}\) satisfy \(_{^{n}}^{*}_{tot}(|)=1\)._

The proof is in Appendix A. Note that Proposition 4.2 can be further simplified. Since \(V^{*}_{tot}()=u^{*}()+\), \(u^{*}\) and \(V^{*}_{tot}\) can be converted to each other without any approximation. For the global policy, replacing \(u^{*}()\) with \(V^{*}_{tot}()-\), we get the following formulation:

\[^{*}_{tot}(|)=_{tot}(|)(_{tot}(,)-V^{*}_{tot}()}{})\] (7)

Now we have the relationship among the optimal global policy \(^{*}_{tot}\), behavior policy \(_{tot}\), Q-value function \(Q^{*}_{tot}\) and state-value function \(V^{*}_{tot}\). Under the multi-agent setting, due to the exponential growth of the joint state-action space with the number of agents, these global values are hard to be evaluated. We now show that we can resort to the value decomposition strategy to further derive a tractable relationship between local policies and value functions.

### Global-to-Local Value and Policy Decomposition

In this work, we introduce the following value decomposition scheme for both the global Q-value function and the state-value function:

\[ Q_{tot}(,)&=_{i}w_{i }()Q_{i}(o_{i},a_{i})+b()\\ V_{tot}()&=_{i}w_{i}()V_{i}(o_{i}) +b()\\ w_{i}& 0,\  i=1,n\] (8)

Compared with existing offline MARL methods [11; 12; 13] that only decompose the global Q-value function \(Q_{tot}\), we additionally decompose the global state-value function \(V_{tot}\). The decomposition of \(Q_{tot}\) and \(V_{tot}\) share a common weight function \(w_{i}()\), since the credit assignment on \(Q\) and \(V\) should be related. It should also be noted that \(V_{tot}\) is free of the joint action space and thus not affected by OOD actions under offline learning.

If we incorporate the value decomposition scheme Eq. (8) into the optimal global policy \(^{*}_{tot}\) in Eq. (7) and utilize the property of the exponential function, we can naturally decompose the optimal global policy \(^{*}_{tot}\) into a combination of optimal local policies \(^{*}_{i}\).

**Proposition 4.3**.: _Under the value decomposition scheme specified in Eq. (8) and assume the global behavior policy is decomposable (i.e., \(_{tot}(|)=_{i=1}^{n}_{i}(a_{i}|o_{i})\)), we have \(_{tot}^{*}(|)=_{i=1}^{n}_{i}^{*}(a_{i}|o_{i})\), where \(_{i}^{*}\) is defined as:_

\[_{i}^{*}(a_{i}|o_{i})=_{i}(a_{i}|o_{i})(()}{}(Q_{i}^{*}(o_{i},a_{i})-V_{i}^{*}(o_{i})))\] (9)

The result immediately follows by observing that:

\[_{tot}^{*}(|) =_{tot}(|)(w_{i}()(Q_{i}^{*}(o_{i},a_{i})-V_{i}^{*}(o_{i}))}{})\] \[=_{i=1}^{n}_{i}(a_{i}|o_{i})(( )}{}(Q_{i}^{*}(o_{i},a_{i})-V_{i}^{*}(o_{i})))= _{i=1}^{n}_{i}^{*}(a_{i}|o_{i})\]

\(_{i}^{*}\) can be perceived as the optimal local policy under the behavior regularization, since the RHS of Eq. (9) only depends on the optimal local value functions \(Q_{i}^{*}\), \(V_{i}^{*}\), and the local behavior policy \(_{i}\) of each agent \(i\). This policy decomposition structure has a number of attractive characteristics. First, this design naturally bridges global value regularization and value decomposition. Second, \(w_{i}()\) explicitly appears in the optimal local policy \(_{i}^{*}\), which is calculated from global observation \(\). This makes the local policy optimizable with global information and consistent with the credit assignment in the multi-agent system.

### Equivalent Implicit Local Value Regularizations

We have converted the relationship between global policies and values to the relationship between local policies and values. However, it is still unclear how to calculate the optimal local value functions in Eq. (9). Note that each local policy needs to satisfy \(_{a_{i}}_{i}^{*}(a_{i}|o_{i})=1\) in Eq. (9) in order to ensure it is well-defined, it is also worth noting that when each local policy \(_{i}^{*}\) satisfies this self-normalization constraint, the optimal global policy \(_{tot}^{*}=_{i}_{i}^{*}\) will also satisfy \(_{^{*}}_{tot}^{*}(|)=1\). Thus, we only need to impose self-normalization constraints on local policies. Integrating the RHS of Eq. (9) over the local action space, and have:

\[_{a_{i}_{i}}[(w_{i}() (Q_{i}^{*}(o_{i},a_{i})-V_{i}^{*}(o_{i})))]=1\] (10)

We have now established the global-to-local relationships among the optimal values and policies, however, one annoying issue is that the global and local behavior policies \(_{tot}\) and \(_{i}\) are typically unknown, and it is hard to estimate them accurately, especially when they are multi-modal. However, perhaps surprisingly, we show that it is possible to learn optimal local value functions in an implicit way without knowing either \(_{tot}\) or \(_{i}\).

**Proposition 4.4**.: \(V_{i}^{*}()\) _can be obtained by solving the following convex optimization problem:_

\[_{V_{i}}\ _{a_{i}_{i}}(()}{ }(Q_{i}^{*}(o_{i},a_{i})-V_{i}(o_{i})))+( )V_{i}(o_{i})}{}\] (11)

The proof follows that the first-order optimality condition of the above optimization objective (i.e., derivative with respect to \(V_{i}\) equals \(0\)) is exactly the condition Eq. (10).

The above optimization problem provides a new learning objective for the local state-value function \(V_{i}\). It is also worth noting that this objective actually corresponds to adding an equivalent implicit local value regularization on \(V_{i}\). To see this, note that in addition to performing expected regression to let \(V_{i}\) fit \(Q_{i}\) by the first term, this objective also minimizes the second term \(w_{i}()V_{i}(o_{i})/\) with respect to \(V_{i}\) on behavioral data \(_{i}\). This is similar to performing conservative optimal value estimation in existing offline RL literature [20; 27] that learns an underestimated value function to avoid the distributional shift.

### Algorithm Summary

Based on previous analysis, we are now ready to present our final algorithm, OMIGA. OMIGA consists of three supervised learning steps: learning local state-value function \(V_{i}\), learning global andlocal Q-value functions, and learning local policies \(_{i}\). We can use Eq. (11) to learn the optimal local state-value function \(V_{i}^{*}\) as:

\[_{V_{i}}_{(o_{i},a_{i})}(()}{}(Q_{i}(o_{i},a_{i})-V_{i}(o_{i})))+()V_{i}(o_{i})}{}\] (12)

With the learned local \(V_{i}^{*}\), we can jointly optimize the local Q-value function \(Q_{i}\), the weight and offset function \(w_{i}\) and \(b\) (\(i=1,,n\)) with the following objective, where \(Q_{tot}\) and \(V_{tot}\) are computed based on the value decomposition in Eq. (8).

\[_{Q_{i}w_{i},b\\ i=1,,n}_{(o,,^{})}[(r(,)+ V_{tot}(^{})-Q_{ tot}(,))^{2}]\] (13)

After obtaining the optimal local value functions, we can further learn the local policies \(_{i}\) by minimizing the KL divergence between \(_{i}\) and \(_{i}^{*}\) in Eq. (9) , which yields the following learning objective:

\[_{_{i}}_{(o_{i},a_{i})}( ()}{}(Q_{i}(o_{i},a_{i})-V_{i}(o_{i})) )_{i}(a_{i}|o_{i})\] (14)

Through the above three learning steps, we convert the initial intractable global behavior regularization in multi-agent POMDP to tractable implicit value regularizations at the local level. Moreover, all three steps are learned in a completely in-sample manner, which performs supervised learning only on dataset samples without the involvement of potentially OOD policy-generated actions, thus greatly improving the training stability. We summarize the psuedocode of OMIGA in Algorithm 1.

```
0: Offline dataset \(\). hyperparameter \(\).
1: Initialize local state-value network \(V_{i}\), local action-value network \(Q_{i}\) and its target network \(_{i}\), and policy network \(_{i}\) for agent \(i\)=1, 2,... \(n\).
2: Initialize the weight function network \(w\) and \(b\).
3:for\(t=1,,\) max-value-iterationdo
4: Sample batch transitions \((,,r,^{})\) from \(\)
5: Update local state-value function \(V_{i}(o_{i})\) for each agent \(i\) via Eq. (12).
6: Compute \(V_{tot}(^{})\), \(Q_{tot}(,)\) via Eq. (8).
7: Update local action-value network \(Q_{i}(o_{i},a_{i})\), weight function network \(w()\) and \(b()\) with objective Eq. (13).
8: Update local policy network \(_{i}\) for each agent \(i\) via Eq. (14).
9: Soft update target network \(_{i}(o_{i},a_{i})\) by \(_{i}(o_{i},a_{i})\) for each agent \(i\).
10:endfor ```

**Algorithm 1** Pseudocode of OMIGA

**Discussion with prior works** OMIGA draws connections with several prior works such as ICQ , DMAC  and IVR . ICQ bears some similarities with OMIGA, however, ICQ needs to estimate the normalizing partition function \(Z(s)=_{}(a|s)(Q(s,a)/)\) to compute the weight in learning both \(J_{}\) and \(J_{Q}\). \(Z\) is estimated by learning an auxiliary behavior model or approximating with softmax operation over a mini-batch, which is hard to compute accurately, especially in the continuous action space. OMIGA uses Eq (11) to provide a new learning objective, it only needs to solve \(V\) in a simple and elegant way by imposing self-normalization constraints. The difference between OMIGA and DMAC is that DMAC considers the KL divergence between current policy and previous policy, so that it can compute the KL divergence explicitly. While OMIGA considers the KL divergence between current policy and behavior policy, the behavior policy is typically unknown and hard to estimate, hence OMIGA proposes a principled way to implicitly compute the KL divergence. Compared with IVR which proposes a general implicit regularized RL framework in the offline single-agent setting, OMIGA starts from a similar regularized framework but is designed for the offline multi-agent setting. Also, OMIGA transforms the regularization from global to local in the multi-agent setting, this provides valuable insights to the community, justifies why previous offline multi-agent methods that apply local behavior regularization can work, and also reveals their limitations.

[MISSING_PAGE_FAIL:8]

our method reflects the global regularization in the local value function learning implicitly, which leads to better performance than other baseline algorithms. Moreover, the state-value function and Q-value function in OMIGA are completely learned in an in-sample manner without the involvement of the agent policy, which also leads to better training stability and performance. Besides, compared with baseline algorithms, the local policy learning in OMIGA contains global information, and thus enjoys obvious advantages in tasks with continuous action space like multi-agent MuJoCo.

We also conduct experiments on more diverse datasets which are obtained by mixing datasets of various quality. The results are shown in Appendix D. On these mixed datasets, the behavior policies can be suboptimal and more heterogeneous. Therefore, it is difficult for algorithms such as BCQ-MA and OMAR to learn an accurate behavior policy, making implicit value learning with the regularization framework of OMIGA more appealing.

### Analyses on Policy Learning with Global Information

As Eq. (12) and Eq. (14) show, the learning of both local state-value function \(V_{i}\) and local policy \(_{i}\) in OMIGA contains global information, which is reflected in \(w_{i}()\). Therefore, OMIGA can make the credit assignment during value and policy learning and leverage the global information to improve local policy. To verify the effect of this design, we transform OMIGA into two versions without global information, and then compare their performance on offline multi-agent MuJoCo and SMAC datasets. In OMIGA-w/o-w, local policy learning uses formula \(_{_{i}}_{(o_{i},a_{i})}[((Q_{i}(o_{i},a_{i})-V_{i}(o_{i})))_{i}(a_{i}|o_{i})]\) and the weight function is not available during the policy learning. In OMIGA-local-w, the weight function is \(w_{i}(o_{i})\) that only contains local information, and local policy learning uses formula \(_{_{i}}_{(o_{i},a_{i})}[((o_{ i})}{}(Q_{i}(o_{i},a_{i})-V_{i}(o_{i})))_{i}(a_{i}|o_{i})]\).

Figure 1(a) shows the experimental results on offline HalfCheeta datasets and 6h_vs_8z datasets. For these difficult multi-agent tasks, the cooperative relationships among agents are extremely complex, so global information is important to guide local policy learning. OMIGA-w/o-w and OMIGA-local-w lack global information, and policies are only learned at the local level, which causes worse performance and stability.

### Analyses on the Regularization Hyperparameter

In OMIGA, hyperparameter \(\) is used to control the degree of regularization. The higher \(\) encourages the policy to stay closer to the behavioral distribution, and the lower \(\) makes the policy more aggressive and optimistic. Figure 1(b) shows the experiments about \(\) on the offline MuJoCo HalfCheeta task and SMAC 6h_vs_8z task. In the HalfCheetah task, too high and too low \(\) will lead to performance degradation. An appropriate \(\) can ensure that the policy is restricted near the dataset distribution and also does not lose performance due to being too conservative. In the 6h_vs_8z task, the dataset contains enough good transitions. Therefore, the degree of regularization should be higher to make the algorithm more conservative. Experimental results indicate that a higher \(\) brings better performance on such datasets. When \(\) is very small, the performance of the algorithm becomes very poor due to the lack of value regularization.

Figure 1: Analyses and ablations.

Conclusion

In this paper, we study the key challenge of the offline MARL problem. We start from the usage of global behavior constraints and smartly derive a global-to-local value regularization scheme under value decomposition. By doing so, we naturally get a new offline MARL algorithm, OMIGA, that in principle applies global-level regularization but actually imposes equivalent implicit local-level regularization. Our work reveals why the local-level regularization used by existing algorithms works and gives theoretical insights into their weaknesses and how to develop better algorithms. The global-to-local regularization design of OMIGA can capture global information and the impact of multi-agent credit assignment under the offline setting, which guarantees that the learned local policies are jointly optimal at the global level. One future work is to develop other offline MARL algorithms by using other choices of \(f\) functions.