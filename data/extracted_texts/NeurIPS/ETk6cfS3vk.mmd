# SlotDiffusion: Object-Centric

Generative Modeling with Diffusion Models

Ziyi Wu\({}^{1,2}\) Jingyu Hu\({}^{1,*}\) Wuyue Lu\({}^{1,*}\) Igor Gilitschenski\({}^{1,2}\) Animesh Garg\({}^{1,2}\)

\({}^{1}\)University of Toronto \({}^{2}\)Vector Institute

* Equal contribution37th Conference on Neural Information Processing Systems (NeurIPS 2023).

###### Abstract

Object-centric learning aims to represent visual data with a set of object entities (a.k.a. slots), providing structured representations that enable systematic generalization. Leveraging advanced architectures like Transformers, recent approaches have made significant progress in unsupervised object discovery. In addition, slot-based representations hold great potential for generative modeling, such as controllable image generation and object manipulation in image editing. However, current slot-based methods often produce blurry images and distorted objects, exhibiting poor generative modeling capabilities. In this paper, we focus on improving slot-to-image decoding, a crucial aspect for high-quality visual generation. We introduce SlotDiffusion - an object-centric Latent Diffusion Model (LDM) designed for both image and video data. Thanks to the powerful modeling capacity of LDMs, SlotDiffusion surpasses previous slot models in unsupervised object segmentation and visual generation across six datasets. Furthermore, our learned object features can be utilized by existing object-centric dynamics models, improving video prediction quality and downstream temporal reasoning tasks. Finally, we demonstrate the scalability of SlotDiffusion to unconstrained real-world datasets such as PASCAL VOC and COCO, when integrated with self-supervised pre-trained image encoders. Additional results and details are available at our website.

## 1 Introduction

+
Footnote â€ : * Equal contribution

Humans perceive the world by identifying discrete concepts such as objects and events , which serve as intermediate representations to support high-level reasoning and systematic generalization of intelligence . In contrast, modern deep learning models typically represent visual data with patch-based features , disregarding the compositional structure of scenes. Inspired by the human perception system, object-centric learning aims to discover the modular and causal structure of visual inputs. This approach thus holds the potential to improve the generalizability and interpretability of AI algorithms . For example, explicitly decomposing scenes into conceptual entities facilitates visual reasoning  and causal inference . Also, capturing the compositional structure of the world proves beneficial for both image generation  and video prediction .

However, unsupervised object-centric learning from raw perceptual input is challenging, as it requires capturing the appearance, position, and motion of each object. Earlier attempts at this problem usually bake in strong scene or object priors in their frameworks , limiting their applicability to synthetic datasets . Later works extend the Scaled Dot-Product Attention mechanism  and introduce a general Slot Attention operation , which eliminates domain-specific priors. With a simple input reconstruction objective, these models learn to segment objects from raw images  and videos . To date, state-of-the-art unsupervised slot models  have shown promising scene decomposition results on naturalistic data under specialized scenarios such as traffic videos.

Despite tremendous progress in object segmentation, the generative capability of slot-based methods has been underexplored. For example, while the state-of-the-art object-centric model STEVE is able to decompose complex data into slots, it reconstructs videos with severe object distortion and temporal inconsistency from slots . Moreover, our own experiments reveal that all popular slot models [46; 60; 83] suffer from low slot-to-image decoding quality, preventing their adoption in visual generation such as image editing. Therefore, the goal of this work is to improve the generative capacity of object-centric models on complex data while preserving their segmentation performance.

We propose SlotDiffusion, an unsupervised object-centric model with a Latent Diffusion Model (LDM)  based slot decoder. We consider slots extracted from images as basic visual concepts (i.e., objects with different attributes), akin to text embeddings of phrases from sentences. The LDM decoder learns to denoise image feature maps guided by object slots, providing training signals for object discovery. Thanks to the expressive power of diffusion models, SlotDiffusion achieves a better trade-off between scene decomposition and visual generation. We evaluate our method on six synthetic and three real-world datasets consisting of complex images and videos. SlotDiffusion improves object segmentation results and achieves significant advancements in compositional generation. Crucially, we demonstrate that SlotDiffusion seamlessly integrates with recent development in object-centric learning, such as dynamics modeling  and the integration of pre-trained image encoders .

**Our main contributions** are as follows: **(i)** SlotDiffusion: a diffusion model based object-centric learning method. **(ii)** We apply SlotDiffusion to unsupervised object discovery and visual generation, where it achieves state-of-the-art results across both image and video datasets. **(iii)** We demonstrate that our learned slots can be directly utilized by the state-of-the-art object-centric dynamics model, improving future prediction and temporal reasoning performance. **(iv)** We further scale up SlotDiffusion to real-world datasets by integrating it with a self-supervised pre-trained image encoder.

## 2 Related Work

In this section, we provide a brief overview of related works on unsupervised object-centric learning and diffusion models, which is further expanded in Appendix B.

**Unsupervised object-centric learning** aims to discover the underlying structure of visual data and represent it with a set of feature vectors (a.k.a. slots). To achieve unsupervised scene decomposition, most methods use input reconstruction as the training signal. Earlier works often perform iterative inference to extract object slots, followed by a CNN-based decoder for reconstruction [6; 17; 23; 40; 55; 56]. AIR  and SQAIR  use a patch-based decoder to decode each object in a canonical pose, and then transform them back to the original position. Slot Attention  and SAVi  adopt the spatial broadcast decoder  to predict the RGB images and segmentation masks from each slot, which are combined via alpha masking. However, the per-slot CNN decoder limits the modeling capacity of these methods to synthetic datasets [5; 42]. Recently, SLATE  and STEVE  proposed a Transformer-based slot decoder. They first pre-train a dVAE to tokenize the input, and then train the slot-conditioned Transformer decoder to reconstruct patch tokens in an autoregressive manner. The powerful Transformer architecture and the feature-level reconstruction target enable them to segment naturalistic images and videos [79; 81; 85]. However, as observed in [83; 100], the slot-to-image reconstruction quality of Transformer decoders is low on complex data, sometimes even underperform CNN decoders. In this work, we leverage diffusion models [34; 73] as the slot decoder, which runs iterative denoising conditioned on slots. Thanks to their expressive power, our method not only improves scene decomposition, but also generates results with higher visual quality.

**Diffusion models** (DMs) [34; 87] have recently achieved tremendous progress in image generation [11; 64; 71; 77], showing their great capability in sample quality and input conditioning. The generative process of DMs is formulated as an iterative denoising procedure from Gaussian noise to clean data, where the denoiser is typically implemented as a U-Net . However, the memory and computation requirements of DMs scale quadratically with the input resolution due to the self-attention layers in the U-Net. To reduce the training cost, LDM  proposes to first down-sample images to feature maps with a pre-trained VAE encoder, and then run the diffusion process in this low-resolution latent space. LDM also introduces cross-attention as a flexible mechanism for conditional generation. For example, text-guided LDMs  perform cross-attention between text embeddings and U-Net's feature maps at multiple resolutions to guide the denoising process. In this work, we adopt LDM as the slot decoder due to its strong generation capacity, where the conditioning is achieved by cross-attention between the denoising feature maps and the object slots.

While conventional DMs excel in controllable generation such as image editing [31; 44] and compositional generation [57; 75], they often require specific supervision such as text to guide the generation process. In contrast, SlotDiffusion discovers meaningful visual concepts from data in an unsupervised manner, which can be used for controllable generation as will be shown in the experiments.

**Concurrent work.** Recently, Jiang et al.  also employ LDM in object-centric learning. However, they utilize an image tokenizer pre-trained on external datasets, while all components of our model are trained purely on target datasets. Moreover, their work only evaluates on images, while we test SlotDiffusion on both images and videos. In addition, we combine our model with recent development in object-centric learning, showing our potential in dynamics modeling and handling real-world data.

## 3 SlotDiffusion: Object-Oriented Diffusion Model

In this section, we describe SlotDiffusion by first reviewing previous unsupervised object-centric learning methods (Section 3.1). Then, we introduce our slot-conditioned diffusion model decoder (Section 3.2). Finally, we show how to perform compositional generation with the visual concepts learned by SlotDiffusion (Section 3.3). The overall model architecture is illustrated in Figure 1.

### Background: Slot-based Object-Centric Learning

Unsupervised object-centric learning aims to represent a scene with a set of object _slots_ without object-level supervision. Here, we review the SAVi family [46; 84] as they achieve state-of-the-art object discovery on videos. SAVi builds upon Slot Attention  and runs on videos in a recurrent encoder-decoder manner. Given \(T\) input frames \(\{_{t}\}_{t=1}^{T}\), SAVi first leverages an encoder to extract per-frame features, adds positional encodings, and flattens them into a set of vectors \(_{t}=f_{}(_{t})^{M D_{}}\). Then, the model initializes \(N\) slots \(}_{t}^{N D_{}}\) from a set of learnable vectors (\(t=1\)), and updates them with Slot Attention as \(_{t}=f_{}(}_{t},_{t})\). \(f_{}\) performs soft feature clustering, where slots compete with each other to capture certain areas of the input via iterative attention . To achieve temporally aligned slots, SAVi leverages a Transformer-based predictor to initialize \(}_{t}\) (\(t 2\)) as \(}_{t}=f_{}(_{t-1})\). Finally, the model utilizes a decoder \(f_{}\) to reconstruct input \(_{t}\) from slots \(_{t}\) and uses the reconstruction loss as training signal. Below we detail the slot decoder \(f_{dec}\) studied in previous works as we focus on improving it in this paper, and refer the readers to  for more information about other model components. See Appendix A for figures of different slot decoders.

**Mixture-based decoder.** Vanilla SAVi  adopts the Spatial Broadcast decoder  consisting of a stack of up-sampling deconvolution layers. It first broadcasts each slot \(_{t}^{i}\) to a 2D grid to form a feature map. Each feature map is then decoded to an RGB image \(_{t}^{i}\) and an alpha mask \(_{t}^{i}\), which are combined into the final reconstructed image \(}_{t}\). SAVi is trained with an MSE reconstruction loss:

\[(_{t}^{i},_{t}^{i})=f_{}^{}(_{t}^{ i}),\ \ }_{t}=_{i=1}^{N}_{t}^{i}_{t}^{ i},\ \ _{}=_{t=1}^{T}||_{t}-}_{t}||^{2}. \]

**Transformer-based decoder.** The above mixture-based decoder has limited modeling capacity as it decodes each slot separately without interactions. Also, pixel-level reconstruction biases the model to low-level color statistics, which only proves effective on objects with uniform colors, and cannot

Figure 1: SlotDiffusion architecture overview. Given an initial frame of a video \(x_{0}\), we initialize slots from a set of learnable vectors \(}_{0}\), and perform Slot Attention with image features to update object slots \(_{0}\). During training, a U-Net denoiser predicts the noise \(_{t}\) added to the image tokens \(z_{0}\) conditioned on slots via cross-attention. The entire model is applied recurrently to all video frames with a Predictor initializing future slots from current step.

scale to complex data with textured objects. Current state-of-the-art model STEVE  thus proposes to reconstruct intermediate features produced by another network . Given frame \(_{t}\), STEVE leverages a pre-trained dVAE encoder to convert it into a sequence of patch tokens \(_{t}=\{_{t}^{1}\}_{i=1}^{L}\), which serve as the reconstruction targets for the autoregressive Transformer decoder:

\[_{t}=f_{}^{}(_{t}),\;\;}_{t}^{l}=f_ {}^{}(_{t};_{t}^{1},...,_{t}^{l-1 }),\;\;_{}=_{t=1}^{T}_{l=1}^{L}( _{t}^{l},}_{t}^{l}). \]

Thanks to the cross-attention mechanism in the Transformer decoder and the feature-level reconstruction objective, STEVE works on naturalistic videos with textured objects and backgrounds. However, the generation quality of the Transformer slot decoder is still low on complex data [83; 100].

### Slot-conditioned Diffusion Model

Object-centric generative models [90; 110] often decompose the generation process to first predicting object slots in the latent space, and then decoding slots back to the pixel space. Here, the image-to-slot abstraction and the slot-to-image decoding are often performed by a pre-trained object-centric model. Therefore, the generation quality is greatly affected by the scene decomposition result and the slot decoder capacity. As observed in [83; 100], methods with mixture-based decoders usually generate images with blurry objects, as they fail to capture objects from complex data. While models with Transformer-based decoders are better at scene decomposition, they still produce results of low visual quality. We attribute this to two issues: **(i)** treating images as sequences of tokens ignores their inherent spatial structure; **(ii)** autoregressive token prediction causes severe error accumulation. In this work, we overcome these drawbacks by introducing diffusion models as the slot decoder, which preserves the spatial dimension of images, and iteratively refines the generation results. From now on, we omit \(t\) as the video timestamp, and will only use \(t\) to denote the timestep in the diffusion process.

**Diffusion model.** Diffusion models (DMs) [34; 87] are probabilistic models that learn a data distribution \(p_{}(_{0})\) by gradually denoising a standard Gaussian distribution, in the form of \(p_{}(_{0})= p_{}(_{0:T})\;d_{1:T}\), where \(_{1:T}\) are intermediate denoising results. The forward process of DMs is a Markov Chain that iteratively adds Gaussian noise to the clean data \(_{0}\), which is controlled by a pre-defined variance schedule \(\{_{t}\}_{t=1}^{T}\). Let \(_{t}=1-_{t}\) and \(_{t}=_{s=1}^{t}_{s}\), we have:

\[q(_{t}|_{t-1})=(_{t}|_{t}}_{t-1},_{t})\;\;q(_{t}|_{0})=( _{t}|_{t}}_{0},(1-_{t})). \]

During training, a denoising model \(_{}(_{t},t)\) is trained to predict the noise applied to a noisy sample:

\[_{t}=_{t}}_{0}+_{t}} _{t},\;\;_{}=||_{t}-_{}(_{t},t )||^{2},\;\;\;_{t}(,). \]

At inference time, we can start from a random Gaussian noise, and apply the trained denoiser to iteratively refine the sample. See Appendix C for a detailed formulation of diffusion models.

**SlotDiffusion.** Our model consists of the same model components as SAVi and STEVE, while only replacing the slot decoder with the DM-based one. Inspired by the success of feature-level reconstruction in STEVE, we adopt the Latent Diffusion Model (LDM)  as the decoder, which denoises features in the latent space. This improves the segmentation results with a higher-level reconstruction target, and greatly reduces the training cost as it runs at a lower resolution. Specifically, we pre-train a VQ-VAE  to extract feature maps \(^{h w D_{}}\) from \(\) before training SlotDiffusion:

\[=f_{}^{}(),\;\;}=f_{}^{ }(),\;\;_{}=||-}||^{2}+ (,}), \]

where \((,)\) is the VGG perceptual loss . In preliminary experiments, we also tried KL-VAE  and VQ-GAN  as the image tokenizer, but did not observe clear improvements.

To condition the LDM decoder on slots \(\), we notice that slots are a set of \(N\) feature vectors, which are similar to text embeddings output by language models [69; 70]. Therefore, we follow recent text-guided LDMs  to guide the denoising process via cross-attention:

\[=(Q(}),K(),V()). \]

Here, \(Q\), \(K\), \(V\) are learnable linear projections, \(}\) is an intermediate feature map from the denoising U-Net \(_{}\), and \(\) is the feature map updated with slot information. An important property of cross-attention is that the result in Eq. (6) is order-invariant to the conditional input \(\), thus preserving the permutation-equivariance of slots, which is a key property of object-centric models. In practice, we perform conditioning after several layers in the U-Net at multiple resolutions. Overall, our model is trained with a slot-conditioned denoising loss over VQ-VAE feature maps:

\[_{t}=_{t}}+(1-_{t})_{t},\;\; _{}=||_{t}-_{}(_{t},t, )||^{2},\;\;\;_{t}(,). \]

### Compositional Generation with Visual Concepts

Previous conditional DMs  rely on labels such as text to control the generation process. In contrast, SlotDiffusion is conditioned on slots output by Slot Attention, which is trained jointly with the DM. We consider each object slot extracted from an image as an instance of a basic visual concept (e.g., red cube). Therefore, we can discover visual concepts from unlabeled data, and build a library of slots for each of them. Then, we can compose concepts via slots to generate novel samples.

**Visual concept library.** We follow SLATE  to build visual concept libraries by clustering slots. Take image datasets for example, we first collect slots extracted from all training images, and then apply K-Means clustering to discover \(K\) clusters, each serves as a concept. All slots assigned to a cluster form a library for that concept. On video datasets, we discover concepts by clustering slots extracted from the first frame. As shown in our experiments, SlotDiffusion discovers semantically meaningful concepts, such as objects with different shapes, and different components of human faces.

To generate new samples, we first select \(N\) concepts from \(K\) candidates, pick one slot from each concept's library, and then decode them with our LDM-based slot decoder. To avoid severe occlusions, we reject slots with object segmentation masks overlapping greater than an mIoU threshold.

## 4 Experiments

SlotDiffusion is a generic unsupervised object-centric learning framework that is applicable to both image and video data. We conduct extensive experiments to answer the following questions: **(i)** Can we learn object-oriented scene decomposition supervised by the denoising loss of DMs? (Section 4.2) **(ii)** Will our LDM-based decoder improve the visual generation quality of slot models? (Section 4.3) **(iii)** Is the object-centric representation learned by SlotDiffusion useful for downstream dynamics modeling tasks? (Section 4.4) **(iv)** Can we extend our method to handle real-world data? (Section 4.5) **(v)** Can SlotDiffusion benefit from other recent improvements in object-centric learning? (Section 4.6) **(vi)** What is the impact of each design choice on SlotDiffusion? (Section 4.7)

### Experimental Setup

**Datasets.** We evaluate our method in unsupervised object discovery and slot-based visual generation on six datasets, namely, the two most complex image datasets _CLEVRTex_ and _CelebA_ from SLATE , and four video datasets _MOVi-D/E/Solid/Tex_ from STEVE . Then, we show SlotDiffusion's capability for downstream video prediction and reasoning tasks on _Physion_. Finally, we scale our method to unconstrained real-world images on _PASCAL VOC 2012_ and _MS COCO 2017_. We briefly introduce each dataset below and provide more details in Appendix D.1.

_CLEVRTex_ augments the CLEVR  dataset with more diverse object shapes, materials, and textures. The backgrounds also present complex textures compared to the plain gray one in CLEVR. We train our model on the training set with 40k images, and test on the test set with 5k samples.

_CelebA_ contains over 200k real-world celebrity images. All images are mainly occupied by human faces, with roughly front-view head poses. This dataset is more challenging as real-world images typically have background clutter and complicated lighting conditions. We train our model on the training set with around 160k images, and test on the test split with around 20k images.

_MOVi-D/E/Solid/Tex_ consist of videos generated using the Kubric simulator . Their videos feature photo-realistic backgrounds and diverse real-world objects, where one or several objects are thrown to the ground to collide with other objects. We follow the official train-test split to evaluate our model.

_Physion_ is a VQA dataset containing realistic simulations of eight physical scenarios, such as dropping and soft-body deformation. The goal of Physion is to predict whether two target objects will contact as the scene evolves. Following the official protocol, we first train models to learn scene dynamics and evaluate the video prediction results, and then perform VQA on the model's future rollout.

_PASCAL VOC 2012/MS COCO 2017_ are real-world datasets commonly used in object detection and segmentation. They are more challenging than CelebA as the images capture unconstrained natural scenes with multiple objects of varying sizes. We will denote them as _VOC_ and _COCO_ for short.

**Baselines.** We compare SlotDiffusion with state-of-the-art fully unsupervised object-centric models. On image datasets, we adopt _Slot Attention_ (SA for short)  and _SLATE_. On video datasets, we adopt _SAVi_ and _STEVE_. They are representative models which use the mixture-baseddecoder and the Transformer-based decoder. We will introduce other baselines in each task below. See Appendix D.3 for implementation details of all baselines adopted in this paper.

**Our Implementation Details.** We use the same image encoder, Slot Attention module, and transition predictor as baselines, while only replacing the slot decoder with the conditional LDM. We first pre-train VQ-VAE on each dataset, and then freeze it and train the object-centric model with Eq. (7). Please refer to Appendix D.4 for complete training details, and Appendix E.1 for a comparison of computation requirements of SlotDiffusion and baseline methods.

### Evaluation on Object Segmentation

**Evaluation Metrics.** Following previous works, we use foreground Adjusted Rand Index (_FG-ARI_) to measure how well the predicted object masks match the ground-truth segmentation. As pointed out by , FG-ARI is sub-optimal since it only considers foreground pixels and ignores false segmentation of backgrounds. Therefore, we also adopt the mean Intersection over Union (_mIoU_) and mean Best Overlap (_mBO_) computed over all pixels. See Appendix D.2 for metric implementations.

**Results.** Figure 2 presents the results on unsupervised object segmentation. We do not evaluate on CelebA as it does not provide object mask annotations. SlotDiffusion outperforms baselines in three metrics across all datasets, proving that the denoising loss in LDMs is a good training signal for scene decomposition. Compared to CLEVRTex, our method achieves larger improvements on video datasets since SlotDiffusion also improves the tracking consistency of objects. Figure 3 shows a qualitative result of video segmentation. As observed in previous work , MOVi-E's moving camera provides motion cues compared to static cameras in MOVi-D, making object discovery easier. Indeed, SAVi degenerates to stripe patterns on MOVi-D, but is able to produce meaningful masks on MOVi-E. Compared to STEVE, our method has fewer object-sharing issues (one object being captured by multiple slots), especially on large objects. See Appendix E.2 for more qualitative results.

### Evaluation on Generation Capacity

**Evaluation Metrics.** To generate high-quality results, object-centric generative models need to first represent objects with slots faithfully, and then compose them in novel ways. Hence, we first evaluate slot-to-image reconstruction, which inspects the expressive power of object slots. We adopt _MSE_ and _LPIPS_ following previous works. As discussed in , LPIPS better aligns with human perception, while MSE favors blurry images. We thus focus on comparing LPIPS, and report MSE

Figure 3: Visualization of video segmentation results on MOVi-D (left) and MOVi-E (right).

Figure 2: Unsupervised object segmentation measured by FG-ARI (left), mIoU (middle), and mBO (right). Mixture, Transformer, and Diffusion stand for SA/SAVi, SLATE/STEVE, and SlotDiffusion, respectively.

for completeness. In addition to reconstruction, we conduct compositional generation with the visual concepts learned in Section 3.3. _FID_ and _FVD_ are used to measure the generation quality.

**Results on reconstruction.** Figure 4 presents the reconstruction results. SlotDiffusion outperforms both baselines with a sizeable margin in LPIPS on all datasets, and achieves the second best MSE. As shown in the qualitative results in Figure 5, SA with a mixture-based decoder produces blurry images despite a lower MSE. Also, the Transformer-based decoder reconstructs distorted objects, and loses all the textures on object surfaces and backgrounds. In contrast, our LDM-based decoder is able to iteratively refine the results, leading to accurate textures and details such as human hairs.

**Results on compositional generation.** As shown in Figure 6 (a), we discover visual concepts by clustering slots extracted from images, and randomly composing slots from each concept's library to generate novel samples. Figure 6 (b) summarizes the quantitative results. SlotDiffusion significantly improves the FID and FVD scores, since it synthesizes images and videos with coherent contents and much better details compared to baselines. This is verified by the qualitative results in Figure 7. See Appendix E.3 for more visualizations of compositional generation results.

Figure 4: Slot-based image and video reconstruction measured by MSE (left) and LPIPS (right).

Figure 5: Reconstruction results on CLEVRTex (left) and CelebA (right). Our method better preserves the textures on objects and backgrounds in CLEVRTex, and the human hairs and facial features in CelebA.

Figure 6: The mechanism of slot-based compositional generation (a) and the quantitative results (b). Our method first discovers visual concepts from images in the form of slots, such as objects and background maps in CLEVRTex. Then, we can synthesize novel samples by composing concept slots from different images.

Figure 7: Compositional generation samples on CLEVRTex (left), CelebA (middle), and MOVi-E (right). We randomly select slots from the visual concept libraries built from training data, and decode them into images or videos. SlotDiffusion synthesizes higher-fidelity results with more details compared to baselines.

**Slot-based image editing.** In Figure 8, we apply SlotDiffusion to image editing by manipulating slots. On CLEVRTex, we can remove an object by segmenting it out, and insert it to another scene. We can also extract background maps from an image and swap it with another image. On CelebA, SlotDiffusion decomposes images to faces, hairs, clothes, and backgrounds. By swapping the face slot, our method generates realistic images with new faces, while keeping other components unchanged.

### Downstream Tasks: Video Prediction and VQA

In this subsection, we evaluate the object slots learned by SlotDiffusion in downstream tasks. We select Physion dataset as it requires capturing the motion and physical properties of objects for accurate dynamics simulation. We train the state-of-the-art object-centric dynamics model SlotFormer  over slots extracted by our model, and unroll it to perform video prediction and question answering.

**Video prediction baselines.** We adopt three baselines that build upon different image feature representations: _PredRNN_ that uses global image feature maps, _VQFormer_ which utilizes image patch tokens encoded by VQ-VAE , and _STEVE + SlotFormer_ that runs the same SlotFormer dynamics model over object-centric slot representations extracted by STEVE.

**Results on video prediction.** Table 1 presents the results on the visual quality of the generated videos. SlotDiffusion with SlotFormer outperforms baselines clearly in LPIPS and FVD, and achieves a competitive result on MSE. As discussed in Section 4.3 and , MSE is a poor metric in generation. Figure 9 visualizes the model rollouts. Compared to STEVE with SlotFormer, our method generates videos with higher fidelity such as the consistent boxes and the red object.

  
**Method** &  & LPIPS & FVD \\  PredRNN & **286.3** & 0.38 & 756.7 \\ VQFormer & 619.0 & 0.33 & 694.5 \\ STEVE + SlotFormer & 832.0 & 0.43 & 930.6 \\ 
**Ours** + SlotFormer & 477.7 & **0.26** & **582.2** \\   

Table 1: Video prediction results on Physion.

  
**Method** &  &  & \(\) (\%) \\  RPIN & 62.8 & 63.8 & +1.0 \\ pDETI-lstm & 59.2 & 60.0 & +0.8 \\ STEVE + SlotFormer & 65.2 & 67.1 & +1.9 \\ 
**Ours** + SlotFormer & **67.5** & **69.7** & **+2.2** \\ 
25 & 30 & 35 & 40 & 45 \\   

Table 2: Physion VQA accuracy. We report observation (Obs.), observation plus rollout (Dyn.), and the improvement (\(\)).

Figure 8: Slot-based image editing. _Left_: we can manipulate objects, retrieve and swap background maps in a scene. _Right_: we can replace the human faces in the first column with the faces from the first row.

Figure 9: Qualitative results of video prediction on Physion. The first row shows ground-truth frames, while the other rows show the model rollout results. See Appendix E.4 for visualizations of all baselines.

**VQA baselines.** We select two baselines following . _RPIN_ is an object-centric dynamics model with ground-truth object bounding box information. _pDEIT-lstm_ builds LSTM over frame features extracted by an ImageNet  pre-trained DeiT . We also compare with _STEVE + SlotFormer_.

**Results on VQA.** Table 2 summarizes the VQA accuracy on observation only (Obs.) and observation plus rollout frames (Dyn.). SlotDiffusion with SlotFormer achieves the best result in both cases, with the highest rollout gain. This proves that our method learns informative slots which capture correct features for learning object dynamics, such as the motion of objects falling down in Figure 9.

### Scaling to Real-World Data

To show the scalability of our method to real-world data, we follow  to leverage pre-trained models. We replace the image encoder in SlotDiffusion with a self-supervised DINO  pre-trained ViT encoder. The model is still trained with the LDM denoising loss over VQ-VAE image tokens.

**Baselines.** To directly compare the effect of the slot decoders, we adopt SA and SLATE with the same DINO pre-trained ViT image encoder. We also compare with DINOSAUR  with the same image encoder, which reconstructs the ViT feature maps instead of input images as training signal.

**Results.** Table 3 presents the segmentation results on VOC and COCO. We follow  to report mBO\({}^{i}\) and mBO\({}^{c}\), which are mBOs between predicted masks and ground-truth instance and semantic segmentation masks. SlotDiffusion consistently outperforms SA and SLATE over all metrics, proving the effectiveness of our LDM-based slot decoder. Our model is also competitive with the state-of-the-art method DINOSAUR. The performance variations may be because COCO has more instances per image than VOC. As shown in the qualitative results in Figure 10, SlotDiffusion is able to segment the major objects in an image, and discover unannotated but semantically meaningful regions.

### Combining SlotDiffusion with Recent Improvements in Slot-based Models

As an active research topic, there have been several recent improvements on slot-based models [39; 67; 85]. Since these ideas are orthogonal to our modifications to the slot decoder, we can incorporate them into our framework to further boost the performance.

**Slot-TTA** proposes a test-time adaptation method by optimizing the reconstruction loss of slot models on testing images. Similarly, we optimize the denoising loss of SlotDiffusion at test time. As shown in Table 4 left, Slot-TTA consistently improves the segmentation results of our method.

**BO-QSA** applies bi-level optimization in the iterative slot update process. We directly combine it with SlotDiffusion (dubbed _BO-SlotDiffusion_). As shown in Table 4 right, BO-SlotDiffusion greatly

  
**Method** & FG-ARI & mIoU & mBO \\  SlotDiffusion & 68.39 & 57.56 & 61.94 \\ SlotDiffusion + Slot-TTA & **69.66** & **58.67** & **62.82** \\   

Table 4: Unsupervised object segmentation results of SlotDiffusion with other improvements on CLEVRTex.

Figure 10: Segmentation results on real-world images: COCO (left) and VOC (right). SlotDiffusion is able to discover unannotated but meaningful regions highlighted by the red arrows, such as trees and keyboards.

  
**PASCAL VOC** & FG-ARI & mBO\({}^{c}\) & mBO\({}^{c}\) \\  SA + DINO ViT & 12.3 & 24.6 & 24.9 \\ SLATE + DINO ViT & 15.6 & 35.9 & 41.5 \\ DINOSAUR & **23.2** & 43.6 & 50.8 \\ 
**Ours + DINO ViT** & 17.8 & **50.4** & **55.3** \\   

Table 3: Unsupervised object segmentation results on real-world datasets: VOC (left) and COCO (right).

improves the segmentation results compared to vanilla SlotDiffusion, and achieves comparable performance with BO-QSA. Note that the generation quality of BO-QSA is close to vanilla Slot Attention, and thus both our methods are significantly better than it.

### Ablation Study

We study the effect of the number of diffusion process steps \(T\) in our LDM-based decoder. We plot both the reconstruction and segmentation results on MOVi-D and MOVi-E in Figure 11. As expected, using more denoising steps leads to better generation quality. Interestingly, smaller \(T\) results in better segmentation performance. This indicates that there is a trade-off between the reconstruction and segmentation quality of our model, and we select \(T=1000\) to strike a good balance. Literature on self-supervised representation learning [29; 38] suggests that a (reasonably) more difficult pretext task usually leads to better representations. Analogously, we hypothesize that a smaller \(T\) makes the denoising pretext task harder, resulting in better object-centric representations.

Overall, SlotDiffusion outperforms baselines consistently across different \(T\) in LPIPS, mIoU, and FG-ARI, proving that our method is robust against this hyper-parameter.

## 5 Conclusion

In this paper, we propose SlotDiffusion by introducing diffusion models to object-centric learning. Conditioned on object slots, our LDM-based decoder performs iterative denoising over latent image features, providing strong learning signals for unsupervised scene decomposition. Experimental results on both image and video datasets demonstrate our advanced segmentation and visual generation performance. Moreover, SlotDiffusion can be combined with recent progress in object-centric models to achieve state-of-the-art video prediction and VQA results, and scale up to real-world data. We briefly discuss the limitations and future directions here, which are further extended in Appendix F.

**Limitations.** With the help of pre-trained encoders such as DINO ViT , SlotDiffusion is able to segment objects from naturalistic images [21; 54]. However, we are still unable to decode natural images faithfully from slots. In addition, objects in real-world data are not well-defined. For example, the part-whole hierarchy causes ambiguity in the segmentation of articulated objects, and the best way of decomposition depends on the downstream task. Finally, our learned slot representations are still entangled, while many tasks require disentanglement between object positions and semantics.

**Future Works.** From the view of generative modeling, we can explore better DM designs to improve the modeling capacity of local details. To generate high-quality natural images, one solution is to incorporate pre-trained generative models such as the text-guided Stable Diffusion . From the view of object-centric learning, we want to apply SlotDiffusion to more downstream tasks such as reinforcement learning [95; 105] and 3D modeling [86; 104]. It is interesting to see if task-specific supervision signals (e.g., rewards in RL) can lead to better scene decomposition.

## Broader Impacts

Unsupervised object-centric learning is an important task in machine learning and computer vision. Equipping machines with a structured latent space enables more interpretable and robust AI algorithms, and can benefit downstream tasks such as scene understanding and robotics. We believe this work benefits both the research community and the society.

**Potential negative societal impacts.** We do not see significant risks of human rights violations or security threats in our work. However, since our method can be applied to generation tasks, it might generate unpleasing content. Future research should also avoid malicious uses such as social surveillance, and be careful about the training cost of large diffusion models.

Figure 11: Ablation study on the number of diffusion steps \(T\) of SlotDiffusion. We show the reconstruction performance (MSE, LPIPS) and segmentation results (FG-ARI, mIoU) on MOVi-D and MOVi-E. We also plot the _best baseline results_ on each dataset as dashed lines. We do not plot MSE as it favors blurry generations.