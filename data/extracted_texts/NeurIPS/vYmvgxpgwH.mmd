# An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth. We study _compute-optimal inference_: designing models and inference strategies that optimally trade off additional inference-time compute for improved performance. As a first step towards understanding and designing compute-optimal inference methods, we assessed the effectiveness and computational efficiency of multiple inference strategies such as Greedy Search, Majority Voting, Best-of-N, Weighted Voting, and their variants on two different Tree Search algorithms, involving different model sizes (e.g., 7B and 34B) and computational budgets. We found that a smaller language model with a novel tree search algorithm typically achieves a Pareto-optimal trade-off. These results highlight the potential benefits of deploying smaller models equipped with more sophisticated decoding algorithms in end-devices to enhance problem-solving accuracy. For instance, we show that the Lemma-7B model can achieve competitive accuracy to a Lemma-34B model on MATH500 while using 2\(\) less FLOPs. Our findings could potentially apply to any generation task with a well-defined measure of success.

## 1 Introduction

Scaling laws of neural networks (Hestness et al., 2017; Rosenfeld et al., 2019) have been established across a range of domains, including language modeling (Kaplan et al., 2020; Hoffmann et al., 2022; OpenAI, 2023), image modeling (Henighan et al., 2020; Yu et al., 2022; Peebles and Xie, 2023), video modeling (Brooks et al., 2024), reward modeling (Gao et al., 2023), and board games (Jones, 2021). These studies have demonstrated how model performance is influenced by both the size of the model and the amount of training computation. However, there is limited knowledge on how varying the compute during inference affects model performance after the model has been trained.

To improve the task performance of large language models (LLMs), inference techniques typically involve additional computation in a _performance maximization_ step at inference time (Nye et al., 2021; Wei et al., 2022; Wang et al., 2022b; Yao et al., 2023; Chen et al., 2024b). This cost must be taken into account for _compute-optimal inference._ For example, a Monte Carlo Tree Search (MCTS) method (Jones, 2021) may improve task performance, but potentially cost much more than simply sampling solutions multiple times. Generally speaking, we need a comprehensive understanding of how various inference-time methods (e.g., Best-of-N, majority voting) trade off between performance and cost. To improve our understanding, this paper presents a thorough empirical evaluation with careful analysis over various configurations of representative LLMs and inference algorithms.

Specifically, we explore how to select an optimal model size (e.g., 7B or 34B) for the policy model and an effective inference strategy (e.g., Greedy Search, Majority Voting, Best-of-N, Weighted Voting,and their Tree Search variants) to maximize performance (i.e., accuracy) within a given compute budget. We manipulate the inference computation (FLOPs) of a fixed model by generating additional tokens through the policy model, sampling further candidate solutions, and ranking them with a reward model. We analyze the performance of a family of math-specialized LLMs (i.e., Llemma-7B and Llemma-34B (Azerbayev et al., 2023)) fine-tuned on the MetaMath dataset (Yu et al., 2023) and measure the error rate on the GSM8K test set (Cobbe et al., 2021) and MATH500 test set (Hendrycks et al., 2021, Lightman et al., 2023).

Our analysis shows that voting-based methods generally outperform the strategy which selects the best solution (i.e., Best-of-N), and weighted voting has the most favorable results (Section 4.3, Figure 5 & 6). However, neither method shows a desirable behavior at high levels of compute. For instance, weighted voting saturates when sampling more than 128 solutions (Figure 1). We have also found that the commonly used MCTS method does not perform well with weighted voting, as it often yields many unfinished solutions, hence having less votes. To address this issue, we propose a novel tree search algorithm, _REward BAlanced SEarch_ (REBASE), which pairs well with weighted voting and improves the Pareto-optimal trade-off between accuracy and inference compute. The key idea of REBASE is to use a node-quality based reward to control the exploitation and pruning properties of tree search, while ensuring enough candidate solutions for voting or selection.

In our experiments, REBASE consistently outperforms sampling and MCTS methods across all settings, models, and tasks. Importantly, we find that REBASE with a _smaller_ language model typically achieves a Pareto-optimal trade-off. For instance, we show that the Llemma-7B model can achieve competitive accuracy to a Llemma-34B model while using \(2\) less FLOPs when evaluating on MATH500 (Figure 1) or GSM8K (Figure 4). These findings underscore the advantages of using smaller models with advanced inference-time algorithms on end-devices to improve problem-solving.

## 2 Related Works

Mathematical Reasoning with LLMs.Large language models have made significant progress in recent years, and have exhibited strong reasoning abilities (Brown et al., 2020, Hoffmann et al., 2022, Chowdhery et al., 2022, Lewkowycz et al., 2022). Mathematical problem solving is a key task for measuring LLM reasoning abilities (Cobbe et al., 2021, Hendrycks et al., 2021). (Ling et al., 2017) first developed the method of producing step by step solutions that lead to the final answer. Later, (Cobbe et al., 2021) extended the work by finetuning the pre-trained language model on a large dataset to solve math word problems, a verifier is trained for evaluating solutions and ranking solutions. Nye et al. (2021) train models to use a scratchpad and improve their performance on algorithmic tasks. Wei et al. (2022) demonstrate that the reasoning ability of a language model can be elicited through the prompting. Subsequent research (Kojima et al., 2022, Lewkowycz et al., 2022, Zhou et al., 2022) in reasoning tasks has also highlighted the efficacy of rationale augmentation. We choose problem solving in mathematics as the task to study the compute-optimal strategy since it allows us to accurately evaluate the problem solving ability of LLMs.

Figure 1: **The inference computation scaling laws exhibited in error rate on the **MATH500** test set based on weighted majority voting, where the left figure shows sampling vs. MCTS, and the right figure shows our proposed REBASE. Clearly, the error rate decreases steadily when the computation increases, and REBASE exhibits a Pareto-optimal tradeoff during inference.

Inference Strategies of LLM Problem Solving.A variety of inference (also called decoding) strategies have been developed to generate sequences with a trained model. Deterministic methods such as greedy decoding and beam search (Teller, 2000; Graves, 2012) find highly probable sequences, often yielding high quality results but without diversity. Sampling algorithms (e.g., temperature sampling (Ackley et al., 1985)) can produce a diverse set of results which are then aggregated to achieve higher accuracy (e.g., via majority voting (Wang et al., 2022)). Recent methods combine search algorithms with modern LLMs, including breadth-first or depth-first search (Yao et al., 2023), Monte-Carlo Tree Search (MCTS) (Zhang et al., 2023; Zhou et al., 2023; Liu et al., 2024; Choi et al., 2023), and Self-evaluation Guided Beam Search (Xie et al., 2023). All of these methods show that using search at inference time can lead to performance gains in various tasks. However, the trade-off for the improved performance is the use of compute to perform the search. Analyzing the trade-off between compute budget and LLM inference performance remains understudied. In this paper, we systematically analyze the trade-off between compute budget and problem-solving performance, and propose a tree search method that is empirically Pareto-optimal.

Process Reward Models.Process reward models (PRMs) have emerged as a technique to improve the reasoning and problem-solving capabilities of LLMs. These models assign rewards to the intermediate steps of the LLM generated sequences. PRMs have been shown effective in selecting reasoning traces with a low error rate, and for providing rewards in reinforcement learning-style algorithms (Usato et al., 2022; Polu and Sutskever, 2020; Gudibande et al., 2023). Ma et al. (2023) applies the PRM to give rewards on the intermediate steps and guide the multi-step reasoning process. The PRM can be either trained on human labeled data (Lightman et al., 2023) or model-labeled synthetic data (Wang et al., 2023). In our work, we use the PRM as the reward model for selecting generated solutions, and for selecting which partial solutions to explore in tree search.

## 3 An Empirical Analysis of Compute-Optimal Inference for Problem-Solving

We explore the following question: _Given a fixed FLOPs budget, how should one select an optimal model size for the policy model, and an effective inference strategy to maximize performance (i.e., accuracy)?_ To address this, we represent the problem-solving error rate \(E(N,T)\) as a function of the number of model parameters \(N\) and the number of generated tokens \(T\). The computational budget \(C\) is a deterministic function FLOPs\((N,T)\), based on \(N\) and \(T\). Our goal is to minimize \(E\) under the test-time compute constraint FLOPs\((N,T)=C\):

\[N_{opt}(C),T_{opt}(C)=*{arg\,min}_{N,T(N,T)=C}E(N,T)\] (1)

where \(N_{opt}(C)\) and \(T_{opt}(C)\) denote the optimal allocation of a computational budget \(C\).

Figure 2: **Illustration of compute-optimal scaling laws in training and inference. The Chinchilla scaling law shows how to choose a model size and number of training tokens under a training-compute budget, while ours shows how to choose a model size and an inference strategy under a inference-compute budget.**

Here, the inference computation (FLOPs) for a fixed model can be modulated by generating more tokens with the policy model, e.g., by sampling additional candidate solutions and subsequently ranking them using a reward model. We primarily consider sampling and tree-search approaches with reranking or majority voting as the means to consume more tokens, including Greedy Search, Majority Voting, Best-of-N, Weighted Voting, and their variants on tree search methods.

### Inference Strategies

#### 3.1.1 Sampling

**Greedy Search.** This strategy generates tokens one at a time by selecting the highest probability token at each step, without considering future steps. It is computationally efficient but often suboptimal in terms of diversity.

**Best-of-n.** This strategy, also known as rejection sampling, samples multiple solutions and chooses the one with the highest score given by the reward model.

**Majority Voting.** In this strategy, multiple model outputs are generated, and the final answer to the problem is determined by the most frequently occurring answer in all the outputs.

**Weighted Majority Voting.** This strategy is a variant of majority voting in which the votes are weighted based on the score given by the reward model.

#### 3.1.2 Monte Carlo Tree Search (MCTS)

Monte Carlo Tree Search (MCTS) has proven effective in domains such as board games where strategic decision-making is required (Silver et al., 2016, 2017; Jones, 2021). Recent work has shown that adapting MCTS to the context of LLMs can enhance the text generation process (Zhang et al., 2023; Zhou et al., 2023; Liu et al., 2024; Choi et al., 2023; Chen et al., 2024; Tian et al., 2024; Chen et al., 2024a). In this context, MCTS is often paired with a value model to score and guide the exploration steps. For additional background, we provide a review of MCTS in Appendix B.

Recent work in MCTS or its variants (e.g., Tree of Thoughts (Yao et al., 2023)) mainly focus on improving the performance (e.g., accuracy) on the studied tasks. However, generic comparisons of MCTS with conventional methods like Best-of-n and Majority Voting in terms of computational budget, measured in generated tokens or processing time, are either scarce or indicating inference-time issues. For example, MCTS consumes substantially more resources, often requiring dozens of times more generated tokens than simpler methods. Specifically, a significant portion of the paths in the search tree are used to estimate and select nodes, and these paths do not necessarily become a part of the final candidate solution, although MCTS ensures that the sampled solutions comprise high-quality intermediate steps. In contrast, sampling methods generate multiple solutions in parallel and independently, and all the generated sequences are included in the candidate solutions. However, the intermediate steps in these sequences are not guaranteed to be of high quality, as there is no mechanism for pruning poor steps or exploiting promising ones.

This highlights the need for developing a new tree search method that can achieve a comparable (or better) performance as MCTS, and that is computationally less costly, just like weighted majority voting and best-of-n. This need leads to the development of our new method named Reward Balanced SEarch (REBASE), as introduced next.

#### 3.1.3 Reward Balanced Search (REBASE)

The REBASE tree search method inherits the exploitation and pruning properties of tree search, while using the reward model alone to estimate the nodes' qualities without additional computation for estimating values by sampling children. The efficiency is achieved by constraining the total expansion width of the tree at a certain depth. REBASE balances the expansion width among the nodes at the same depth based on the rewards given by the Process Reward Model (PRM). The details are provided below:

**Notations.** We consider the fine-tuned LLM as a policy \(_{}\). Given a question \(q\) and the first \(k\) steps of a solution \(x_{1},,x_{k}\), the \((k+1)\)-th step is produced by \(_{}(x_{k+1}|q,x_{1} x_{k})\). When generating solutions using tree search, the root of the tree corresponds to the question \(q\). The node corresponding to \(x_{k+1}\) is the child of the node corresponding to \(x_{k}\) if it is sampled from \(_{}(|q,x_{1},x_{k})\). The reward of a node \(n(x_{k})\) is determined by the PRM as \(R(n(x_{k}))=R(q,x_{1},,x_{k})\).

**Initialization.** Given the question \(q\), balance temperature \(T_{b}\), and sampling number of solutions N, we sample N instances of the first step for the question, yielding all the nodes of depth 1 in the search tree. We set the sampling budget of depth 0 \(B_{0}=N\) as initialization.

**Reward modeling and update.** In the \(i\)-th iteration, the PRM assigns the rewards to all the nodes at depth \(i\). After that, the algorithm examines whether the solutions up to depth \(i\) are complete. Supposing there are \(C_{i}\) completed solutions, we update the sampling budget using \(B_{i} B_{i-1}-C_{i}\). If \(B_{i}=0\), the process ends, and we obtain \(N\) solutions.

**Exploration balancing and expansion.** For all of the nodes \(n_{j}\) with reward \(R(n_{j})\) in the depth \(i\) of the tree, we calculate the expansion width of the \(n_{j}\) as:

\[W_{j}=(B_{i})/T_{b})}{_{k} (R(n_{k})/T_{b})}).\] (2)

Then we sample \(W_{j}\) children for \(n_{j}\) for all the nodes in depth \(i\), and start the next iteration.

#### 3.1.4 Theoretical Analysis

Before empirically studying the scaling effects of increasing the inference-time compute budget, we present two theorems which will help us understand the experimental results later. These two theorems give an upper bound on the performance of sampling when fixing the LLM generator.

We assume the vocabulary is limited and the sequence length is constrained, thus the number of possible solutions and answers are finite. The proofs are provided in the Appendix A.

**Theorem 1**.: _Given a test dataset \(\) and a LLM \(\). \(||\) is the finite set of all possible answers given by LLM, the ground truth function \(g\) maps test data \(d\) to the true answer. Denote the accuracy of the LLM on this dataset with majority over N samples as \(ACC_{MV}(,,N)\). The accuracy of majority voting on the LLM will eventually saturate, i.e._

\[_{N}ACC_{MV}(,,N)=}((g(d)=*{arg\,max}_{a} (a|d)).}{||}.\] (3)

Figure 3: Illustration of one iteration of REward BAlanced SEarch (REBASE).

_where \((x|d)\) denotes the probability that the LLM answers \(x\) given input \(d\) and \(\) is the indicator function._

**Theorem 2**.: _Assume the reward model assigns an expected reward of \(R(a)\) to \(a\) among the different solutions generated by LLM that yields \(a\). Given a test dataset \(\) and a LLM \(\). \(||\) is the finite set of all possible answers given by LLM, the ground truth function \(g\) maps test data \(d\) to the true answer. Denote the accuracy of the LLM on this dataset with weighted majority over N samples as \(ACC_{W}(,,N,R)\). The accuracy of weighted majority voting on the LLM will eventually saturate, i.e._

\[_{N}ACC_{WV}(,,N,R)=}((g(d)=*{arg\,max}_{a} R(a)(a|d))}{||}.\] (4)

_where \((x|d)\) denotes the probability that the LLM answers \(x\) given input \(d\) and \(\) denotes the indicator function._

Theorem 2 shows that as long as the reward model assigns higher rewards than the policy for correct answers versus other answers in expectation, the upper bound of Weighted Majority Voting will be higher than Majority Voting since \(((g(d)=*{arg\,max}_{a}R(a)(a|d) )>((g(d)=*{arg\,max}_{a}(a |d))\). We put the figures comparing BoN and Weighted Majority Voting in the main paper and leave the Majority Voting figures in Appendix D since Majority Voting is dominated by Weighted Majority Voting.

## 4 Experiments

### Setup

Datasets.We conduct experiments on two mathematical problem-solving datasets to investigate the scaling effects of computation and our REBASE method for both challenging and simpler problems. Specifically, MATH (Hendrycks et al., 2021) and GSM8K(Cobbe et al., 2021) are datasets containing high school mathematics competition-level problems and grade-school level mathematical reasoning problems, respectively. Following (Lightman et al., 2023; Wang et al., 2024; Sun et al., 2024), we use the MATH500 subset as our test set.

Generators.We use Lemma-7B and Llemma-34B (Azerbayev et al., 2024) as our base models and finetune them on the MetaMath dataset (Yu et al., 2024) using full parameter supervised fine-tuning (Full-SFT), The detailed finetuning configuration is given in the Appendix. Additionally, we test the Mistral-7B model to expand our findings across different models.

Reward Model.All of the experiments use the same Lemma-34B reward model, which we finetuned on the synthetic process reward modeling dataset, Math-Shepherd (Wang et al., 2024). We added a reward head to make the model, enabling it to output a scalar reward at the end of each step.

Figure 4: **The inference computation scaling comparisons across different model sizes**. The left/right panel shows the GSM8K problem-solving error rate on GSM8K based on Weighted Majority/Best-of-N.

[MISSING_PAGE_FAIL:7]

fixing the model and the evaluation task. Table 1 shows that REBASE can achieve competitive accuracy with even a lower compute budget than the sampling method. This finding is novel, and differs from previous tree search works which typically improve the performance at the cost of higher computational expense compared to sampling (Chen et al., 2024; Xie et al., 2023). Table 2 shows that given the same compute budget (sampling 32 solutions for the 7B model and 8 solutions for 34B model), using REBASE yields higher accuray than sampling.

Weaker models gain more from Tree Search.From Fig. 2, we saw that compared with sampling, Mistral-7B, Llemma-7B, Llemma-34B increase \(5.3\%\), \(3.3\%\), \(2.6\%\) in MATH and \(0.7\%\), \(1.9\%\), \(0.9\%\) in GSM8K. The order of accuracy increase is inversely related to the model's corresponding greedy search on those datasets. This suggests that weaker models, as indicated by their lower greedy search accuracy, benefit more from tree search methods like REBASE.

REBASE saturates later than sampling with higher accuray.From Figure 5 and Figure 6, we observe that both sampling and REBASE saturate early in GSM8K and relatively late in MATH, which we attribute to the difference of the difficulty level. This can be explained through the LLM may assign high probability to the true answer in easy problems than those of harder problem, as suggested by Theorem 1 and 2 with their proofs A. On MATH (Figure 5), we see that REBASE finally saturates with a higher accuracy than sampling. We hypothesize the reason is that REBASE samples the truth answer with higher probability than sampling. And as demonstrated by Theorem 1 and 2, the upper bound becomes higher.

    & \# Samples & FLOPs & MATH500 \\   \\  Sampling & 256 & \(8.7 10^{14}\) & 42.8 \\ REBASE & 32 & \(}\) & **45.0** \\   \\  Sampling & 256 & \(10.0 10^{14}\) & 45.5 \\ REBASE & 32 & \(}\) & **46.8** \\   \\  Sampling & 64 & \(12.1 10^{14}\) & 46.7 \\ REBASE & 32 & \(}\) & **49.2** \\   

Table 1: REBASE with lower compute budget has competitive accuracy against Sampling with higher compute budget. We use weighted voting to aggregate the candidate solutions in both Sampling and REBASE.

Figure 6: **The inference computation scaling laws** of different models for the problem-solving error rate on **GSM8K** test set. The tested models are Llemma-7B (left), Llemma-34B (middle), & Mistral-7B (right). In the legend, W.M. and BoN refer to Weighted Majority and Best-of-N, respectively.

## 5 Conclusion & Limitations

In this work, we have conducted a comprehensive empirical analysis of compute-optimal inference for problem-solving with language models. Our study has revealed several key findings. First, with an optimal inference configuration, a small language model can achieve competitive accuracy to a \(4\) larger model while using approximately \(2\) less total FLOPs under the same inference method (Sampling, MCTS, REBASE) and task (MATH, GSM8K), suggesting that training and inference with smaller models could be more favorable in terms of compute budget when combined with multiple sampling or search strategies. Second, our new REBASE tree-search method consistently outperforms sampling (and MCTS) across all settings, models, and tasks, achieving competitive accuracy with even lower compute budget compared to sampling. Our findings highlight the potential of deploying smaller models equipped with more sophisticated inference strategies like REBASE to enhance problem-solving accuracy while maintaining computational efficiency.

LimitationsFirst, our experiments focused specifically on mathematical problem-solving tasks, and the generalizability of our findings to other domains remains to be explored. Second, we only investigated a limited range of model scales, primarily focusing on 7B and 34B models. Future research could extend our analysis to a wider range of model sizes to gain a more comprehensive understanding of the scaling laws for compute-optimal inference. Finally, our experiments mainly utilized the MetaMath dataset for training the models. It would be valuable to explore the impact of different training datasets on the performance and efficiency of compute-optimal inference strategies for mathematical problem-solving.

    & \# Samples & MATH FLOPs & GSM8K FLOPs & MATH500 & GSM8K \\   \\  Greedy & 1 & \(3.4 10^{12}\) & \(2.3 10^{12}\) & 28.6 & 77.9 \\ Sampling + MV & 32 & \(109.2 10^{12}\) & \(72.6 10^{12}\) & 36.1 & 85.7 \\ Sampling + BoN & 32 & \(109.2 10^{12}\) & \(72.6 10^{12}\) & 40.3 & 89.4 \\ Sampling + WV & 32 & \(109.2 10^{12}\) & \(72.6 10^{12}\) & 39.7 & 89.1 \\ REBASE + MV & 32 & \(136.2 10^{12}\) & \(78.9 10^{12}\) & 44.1 & 88.8 \\ REBASE + BoN & 32 & \(136.2 10^{12}\) & \(78.9 10^{12}\) & **45.4** & 89.4 \\ REBASE + WV & 32 & \(136.2 10^{12}\) & \(78.9 10^{12}\) & 45.0 & **89.8** \\   \\  Greedy & 1 & \(3.92 10^{12}\) & \(2.3 10^{12}\) & 30.0 & 68.5 \\ Sampling + MV & 32 & \(125.4 10^{12}\) & \(73.9 10^{12}\) & 41.0 & 80.0 \\ Sampling + BoN & 32 & \(125.4 10^{12}\) & \(73.9 10^{12}\) & 41.7 & 85.6 \\ Sampling + WV & 32 & \(125.4 10^{12}\) & \(73.9 10^{12}\) & 43.5 & 85.4 \\ REBASE + MV & 32 & \(148.0 10^{12}\) & \(82.6 10^{12}\) & 46.1 & 86.1 \\ REBASE + BoN & 32 & \(148.0 10^{12}\) & \(82.6 10^{12}\) & 44.1 & 86.9 \\ REBASE + WV & 32 & \(148.0 10^{12}\) & \(82.6 10^{12}\) & **46.8** & **87.3** \\   \\  Greedy & 1 & \(19.0 10^{12}\) & \(11.2 10^{12}\) & 33.0 & 78.4 \\ Sampling + MV & 8 & \(152.3 10^{12}\) & \(89.7 10^{12}\) & 39.9 & 84.3 \\ Sampling + BoN & 8 & \(152.3 10^{12}\) & \(89.7 10^{12}\) & 40.4 & 86.7 \\ Sampling + WV & 8 & \(152.3 10^{12}\) & \(89.7 10^{12}\) & 41.0 & 86.0 \\ REBASE + MV & 8 & \(176.8 10^{12}\) & \(98.7 10^{12}\) & **43.9** & 86.1 \\ REBASE + BoN & 8 & \(176.8 10^{12}\) & \(98.7 10^{12}\) & 43.6 & **86.9** \\ REBASE + WV & 8 & \(176.8 10^{12}\) & \(98.7 10^{12}\) & 42.9 & **86.9** \\   

Table 2: Accuracy of different inference configurations under a specific compute budget. MV, BoN and WV denote Majority Voting, Best-of-N and Weighted Voting, respectively.