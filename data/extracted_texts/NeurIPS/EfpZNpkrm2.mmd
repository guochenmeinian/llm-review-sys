# QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation

Zhuo Chen\({}^{12}\) &Rumen Dangovski\({}^{13}\) &Charlotte Loh\({}^{13}\)

&Owen Dugan\({}^{12}\) &Di Luo\({}^{124}\)\({}^{*}\) &Marin Soljacic\({}^{12}\)

\({}^{1}\)NSF AI Institute for Artificial Intelligence and Fundamental Interactions

\({}^{2}\)Department of Physics, Massachusetts Institute of Technology

\({}^{3}\)Department of EECS, Massachusetts Institute of Technology

\({}^{4}\)Department of Physics, Harvard University

{chenzhuo,rumend,cloh,odugan,diluo,soljacic}@mit.edu

###### Abstract

We propose **Quan**tum-informed **T**ensor **A**daptation (**QuanTA**), a novel, easy-to-implement, fine-tuning method with no inference overhead for large-scale pretrained language models. By leveraging quantum-inspired methods derived from quantum circuit structures, QuanTA enables efficient _high-rank_ fine-tuning, surpassing the limitations of Low-Rank Adaptation (LoRA)--low-rank approximation may fail for complicated downstream tasks. Our approach is theoretically supported by the universality theorem and the rank representation theorem to achieve efficient high-rank adaptations. Experiments demonstrate that QuanTA significantly enhances commonsense reasoning, arithmetic reasoning, and scalability compared to traditional methods. Furthermore, QuanTA shows superior performance with fewer trainable parameters compared to other approaches and can be designed to integrate with existing fine-tuning algorithms for further improvement, providing a scalable and efficient solution for fine-tuning large language models and advancing state-of-the-art in natural language processing.

## 1 Introduction

Pre-trainied large language models (LLMs) have revolutionized natural language processing (NLP) by achieving state-of-the-art performance across various tasks [1; 2]. Traditionally, these models are adapted to specific downstream applications via full fine-tuning, where all model parameters are retrained. However, as model sizes increase, the computational cost and memory requirements for full fine-tuning become prohibitive, especially with models like GPT-3  with 175 billion parameters, Mixtral  with \(8 22\) billion parameters, and more recently the LLaMA series [5; 6; 7], containing soon up to 400 billion parameters . These constraints have spurred the development of parameter-efficient fine-tuning (PEFT) methods, which aim to adapt LLMs by updating only a small subset of parameters, thereby reducing resource demands [9; 10].

Among PEFT methods, Low-Rank Adaptation (LoRA)  has gained prominence due to its simplicity and effectiveness. LoRA fine-tunes LLMs by introducing low-rank matrices into the pre-trained model's weight updates, pragmatically reducing the number of trainable parameters while maintaining performance close to full fine-tuning in many tasks. However, LoRA's reliance on low-rank approximations can sometimes lead to a performance gap compared to full fine-tuning, particularly for complex tasks, as it may not capture all necessary task-specific adaptations .

Recently, there have been many attempts to generalize LoRA using tensor-based methods [12; 13]. However, these approaches primarily focus on reducing the number of trainable parameters within the low-rank framework yet they continue to face the same limitations of restricted representation. In Quantum mechanics, quantum circuit provides a natural realization of unitary matrix which is full rank, motivating us to develop new schemes for high-rank fine-tuning.

Inspired by these advancements, we propose **Qua**antum-informed **T**ensor **A**daptation (**Q**uan**TA**) * a novel, easy-to-implement, fine-tuning method with no inference overhead inspired by quantum circuits (Fig. 1). QuaTA enables efficient high-rank adaptations by utilizing tensor operations analogous to those in quantum circuits, addressing the limitations inherent in low-rank methods like LoRA.

Footnote *: https://github.com/quanta-fine-tuning/quanta

In summary, our contributions are as follows:

1. We introduce QuaTA, a novel, easy-to-implement, PEFT method with no inference overhead inspired by quantum circuits, enabling efficient high-rank fine-tuning without additional inference latency and offering the potential for integration with other existing PEFT methods for further enhancement.
2. We present the universality theorem and the rank representation theorem, theoretically proving that QuaTA can efficiently parameterize high-rank matrices, overcoming the limitations of low-rank methods.
3. We validate QuaTA's performance through extensive experiments, demonstrating significant improvements in various reasoning tasks and efficiency compared to traditional methods.

## 2 Related Works

**Parameter-Efficient Fine-Tuning (PEFT)** methods aim to address the computational burdens associated with fine-tuning large-scale models by adjusting a relatively small fraction of the total parameters to fit a specific downstream task. Roughly speaking, there are three existing categories of PEFT methods:

1. **Adapter-based methods.** These methods introduce additional trainable modules into the structure of a pre-trained, otherwise frozen, model. These modules can be integrated in various ways: series adapters are interposed between existing layers like attention or MLP components [9; 14; 15; 16], while parallel adapters coexist alongside these components . In general, these methods tend to increase the inference load due to the extra components that are not readily integrated into the original model weights.

Figure 1: Conceptual comparison of QuaTA and LoRA methods. LoRA parameterizes the weight matrix update as a outer product of two low-rank matrices, limiting its capacity. QuaTA, inspired by quantum circuits, uses tensors that operate on specific axes of the (reshaped) input, enabling high-rank parameterization. Supported by the universality theorem and rank representation theorem, QuaTA can represent arbitrary matrices effectively, allowing it to achieve performance comparable to or sometimes even better than full fine-tuning, with only a fraction of the parameters. Note: the performance graph is a conceptual illustration.

2. **Prompt/Prefix-based methods.** These methods employ additional prompts or soft tokens at the beginning of the input sequence, focusing fine-tuning efforts on these newly introduced vector embeddings while maintaining the original model weights static [18; 19]. However, this approach can suffer from suboptimal performance and increased inference times. In addition, the soft tokens take up space of real tokens and therefore reduce the effective context size available for the model.
3. **Reparameterization-based methods.** These methods modify the existing weights with some parameter-efficient parameterization during the fine-tuning phase. Among these methods, Low-Rank Adaptation (LoRA)  and its variants, such as DoRA  and VeRA , are particularly noteworthy for their widespread adoption and robust performance across various tasks. In addition to LoRA, many other PEFT methods also belong to this category, including more sophisticated approaches such as Hadamard , Kronecker product  reparameterizations as well as many other methods [24; 25; 26; 27]. Crucially, methods in this category do not impose additional inference burdens after fine-tuning as the modified weights can be merged into the pre-trained model weights prior to deployment.

Besides these three categories, there are additional PEFT methods such as LoTA , where tensor decompositions are performed across multiple weights, LoRETTA , which uses tensor train decomposition for each weight matrix and has both adapter-based and reparameterization-based variants, MPO-based fine-tuning , and very recently LISA , ReFT  and MoRA .

**Physics-inspired machine learning** In parallel, there have been various attempts to integrate physics-based priors into machine learning for many years. Symmetries and physics structure have been incorporated into the neural networks architecture and training in various applications to achieve notable performance [32; 33; 34; 35; 36; 37; 38; 39]. Various classical and quantum physics processes have been utilized to design new neural networks [40; 41] and generative models [42; 43; 44; 45; 46; 47; 48].

## 3 Motivation: Low Rank is not Always Sufficient

LoRA operates under the hypothesis that parameter updates during fine-tuning exhibit a low "intrinsic rank." For a pretrained weight matrix \(W_{0}^{d k}\), LoRA parameterizes the weight update as \(W^{}=W_{0}+ W=W_{0}+BA\), where \(A^{r k}\) and \(B^{d r}\) are low-rank matrices. In this configuration, only \(A\) and \(B\) are trainable, while \(W_{0}\) remains fixed. Consequently, the rank of the weight update \( W\) is limited to \(r\).

Although the original LoRA paper shows empirical evidence to support the low-rank hypothesis, recently it has been found that this hypothesis may still fail for more complex tasks, especially for those that significantly differ from the pre-training dataset, leading to suboptimal performance [11; 31]. To assess the general applicability of the low-rank hypothesis, we examine two datasets of varying difficulties: the RTE dataset , a classification task where the model is tasked to verify the correctness of statements, and the DROP dataset , a generation task where the model performs discrete reasoning over paragraphs. We posit that the RTE dataset is simpler, thus more likely to conform to the low-rank hypothesis, whereas the DROP dataset presents a greater challenge.

As shown in Table 1, the LLaMA2-7B model  in general can achieve a better score on the RTE dataset than the DROP dataset. In addition, as we increase the rank from 64 to 128, LoRA's performance on the RTE dataset remains the same, consistent with

    & \)Score (\(\))**} \\   & **RTE** & **DROP** \\  LLaMA2\({}_{79}\) Base & 61.0 & 19.8 \\  LLaMA2\({}_{79}\) LoRA\({}_{r-61}\) & 86.0 & 55.2 \\  LLaMA2\({}_{79}\) LoRA\({}_{r-128}\) & 85.8 & 56.2 \\   

Table 1: Performance of base and LoRA fine-tuned LLaMA2-7B on RTE  and DROP  datasets. We use accuracy and \(F_{1}\)-score as the metrics for them respectively.

Figure 2: Subspace similarities between two LoRA experiments of different ranks (64 and 128) for two datasets. Each point \((i,j)\) represents the subspace similarity between the first \(i\) right singular vectors of the \(r=64\) experiment, and the first \(j\) right singular vectors of the \(r=128\) experiment. Only points for \(i j\) are plotted. DROP dataset has a significantly high “intrinsic rank” than RTE dataset.

the low-rank hypothesis, while the performance on the DROP dataset improves, suggesting the DROP dataset may require a higher "intrinsic rank."

To further measure the "intrinsic rank" of weight updates for these datasets, we follow the methodology outlined in  and compare the subspace spanned by the right singular vectors of the resulting weight updates between the \(r=64\) and \(r=128\) experiments. Figure 2 shows the subspace similarities between the query weight updates of the two ranks at layer 16 for both datasets. In the figure, each point \((i,j)\) represents the subspace similarity between the first \(i\) singular vectors of the \(r=64\) experiment and the first \(j\) singular vectors of the \(r=128\) experiment. A subspace similarity close to 1 indicates significant overlap, suggesting that the subspace is crucial for fine-tuning, while a similarity close to 0 suggests orthogonality, implying that the vectors represent noise. For the RTE dataset, subspace similarity is large only for very small \(i\) values, and quickly decays to 0 for larger \(i\), indicating that fine-tuning on the RTE dataset has a low "intrinsic rank." Conversely, for the DROP dataset, subspace similarity remains large across all 64 singular vectors, demonstrating a high "intrinsic rank." Additional details of subspace similarity and addition data are provide in Appendix A

These findings demonstrate the necessity of high-rank fine-tuning in complex tasks, challenging the effectiveness of LoRA. This naturally prompts the following question: _How can we design efficient methods to facilitate high-rank updates during fine-tuning?_

## 4 Preliminary: Quantum Circuit

The behavior of quantum mechanical systems, especially those involving particles with discrete degrees of freedom, is well described by matrix theory. Quantum circuits naturally realize unitary matrices whose sizes grow exponentially with the number of particles, providing a potent framework for high-rank representation. Here, we review some fundamental concepts of quantum states and quantum circuits to motivate our approach.

**Quantum state and vector representation.** An \(N\)-qubit quantum state \(=_{i}_{i}^{2^{N}}\) is a \(2^{N}\)-dimensional complex-valued vector in Hilbert space, with \(_{i}\) the components and \(\) the basis vectors (similar to \(_{i}\) in vector notation). Since quantum states typically consist of qubits with local dimensions of 2, it is instructive to view the quantum state as a multi-dimensional tensor with different indices labeling different qubits: \(=_{i_{1},i_{2},,i_{N}},i_{2},,i_{N}}\), where \(i_{1},i_{2},,i_{N}\) is the binary representation of \(i\). This can be equivalently viewed as reshaping the quantum state from a vector in \(^{2^{N}}\) to a tensor in \(^{2 2 2}\).

**Quantum circuit and matrix representation.** A quantum circuit is a unitary matrix \((2^{N})^{2^{N} 2^{N}}\) that transforms one quantum state into another: \(=\). These circuits are constructed from smaller unitary matrices known as quantum "gates," which operate on one or two qubits. A one-qubit gate is a unitary matrix \(U^{(1)}(2^{1})\), while a two-qubit gate is a unitary matrix \(U^{(2)}(2^{2})\)+. These gates are applied to specific qubits as follows:

Footnote †: Typically, quantum circuits and quantum gates are considered within the group \((2^{N})\). However, the groups \((2^{N})\) and \((2^{N})\) differ only by a \((1)\) factor, which does not affect the results presented in this paper.

\[U^{(1)}=_{j_{n}}U^{(1)}_{i_{n};j_{n}}_{i_{1},i_{2},,j_ {n},,i_{N}},i_{2},,i_{N}}\] (1)

for a one-qubit gate applied to qubit \(n\), and

\[U^{(2)}=_{j_{m},j_{n}}U^{(2)}_{i_{m},i_{n};j_{m},j_{n}}_{i_ {1},i_{2},,j_{m},,j_{n},,i_{N}},i_{2},,i_{N}}\] (2)

for a two-qubit gate applied to qubits \(m\) and \(n\). (Note that \(m\) and \(n\) do not need to be consecutive qubits.)

A quantum circuit comprises a series of these one- and two-qubit gates \(\{U^{()}\}\) applied sequentially to the quantum state:

\[=_{}U^{()}.\] (3)

Figure 3: Any unitary matrix can be decomposed into a quantum circuit using one- and two-qubit gates.

Since quantum circuits are unitary, they inherently represent full-rank matrices in finite-dimensional systems.

**Universality of quantum circuit.** Similar to the universal approximation theorem for neural networks, it has been established that any quantum circuit on \(N\) qubits can be decomposed into a quantum circuit using only one- and two-qubit gates , as shown in Figure 3. This is particularly relevant for reparameterization-based fine-tuning methods, where we aim to parameterize a matrix matching the shape of the base model's weight matrix using a small number of parameters.

## 5 Quantum-informed Tensor Adaptation

Since quantum circuits offer an elegant parameterization for large unitary matrices of shape \(2^{N} 2^{N}\), by relaxing the unitarity constraint and allowing for arbitrary local dimensions, we can develop an effective tool for high-rank, parameter-efficient fine-tuning. Inspired by this, we propose **QuanTA**: **Quantum-informed T**ensor **A**daptation, which parameterizes the parameter updates in a way analogous to a quantum circuit.

**Construction.** To illustrate the construction of QuanTA, we focus on the case of square weight matrices \(W^{d d}\) in the main paper and defer the general case to Appendix B. In addition, we assume the hidden dimension \(d\) can be decomposed as \(d=d_{1} d_{2} d_{N}\)2. This condition is often satisfied for large language models. By reshaping \(x^{d}\) to \(x^{d_{1} d_{2} d_{N}}\), the hidden vector can be interpreted as a quantum state with \(N\) "qudits," with the \(n\)th axis corresponding to a qudit with local dimension \(d_{n}\).

Similar to a quantum circuit, QuanTA consists of "gates" (or tensors) that apply to only specific axes. Since single-axis gates are subsets of two-axis gates, it suffices to consider parameterizations using only two-axis gates. Let \(T^{()}\) be a tensor of shape \(T^{()}^{d_{m^{()}}d_{n^{()}} d_{m^{( )}}d_{n^{()}}}\) that operates on the \(m^{()}\)th and \(n^{()}\)th axes with corresponding dimensions \(d_{m^{()}}\) and \(d_{n^{()}}\). Analogous to applying a two-qubit gate to a quantum state, applying this tensor to the hidden vector is defined as

\[(T^{()}x)_{i_{1},,i_{m},,i_{n},,i_{N}}:=_{j_{m},j_ {n}}T^{()}_{i_{m},i_{n};j_{m},j_{n}}x_{i_{1},,j_{m},,j_{n}, ,i_{N}},\] (4)

where the \(\) labels are dropped for simplicity, but it should be noted that different \(T^{()}\)'s can be defined on different axes. Equivalently, this operation can be viewed as a matrix-vector multiplication with all but the \(m^{()}\)th and \(n^{()}\)th axes created as batch dimensions.

QuanTA is then constructed by sequentially applying a collection of such tensors \(\{T^{()}\}\) in the same manner as a quantum circuit:

\[x:=_{}T^{()}x.\] (5)

Although it is difficult to write the full Eq. (5) in index notation for an arbitrary set of tensors, we demonstrate in Appendix G that the einsum expression for this operation can be systematically generated.

As a concrete example of translating Eq. (5) to index notations and einsum, consider the case of \(N=3\); \(\{T^{()}\}\) consists of three tensors, each applied to two axes (as depicted in Fig. 1). In this case, it is easy to express in index notation the application of the QuanTA operator to the hidden vector;

\[(x)_{i_{1},i_{2},i_{3}}=_{k_{1},k_{2}}T^{(1)}_{i_{1},i_{2};k_{1 },k_{2}}_{j_{1},k_{3}}T^{(2)}_{k_{1},i_{3};j_{1},k_{3}}_{j_{2},j_{3}}T^ {(3)}_{k_{2},k_{3};j_{2},j_{3}}x_{j_{1},j_{2},j_{3}}\] (6)

as well as the calculation of the full QuanTA matrix;

\[_{i;j}=_{i_{1},i_{2},i_{3};j_{1},j_{2},j_{3}}=_{k_{ 1},k_{2}}T^{(1)}_{i_{1},i_{2};k_{1},k_{2}}_{k_{3}}T^{(2)}_{k_{1},i_{3};j_{1 },k_{3}}T^{(3)}_{k_{2},k_{3};j_{2},j_{3}}.\] (7)

Although Eq. 6 and 7 may look complex in their formulation, in practice they can be easily implemented respectively using einsum as 

**Initialization method.** At initialization, the adapted model should be the same as the base model and all the weight updates should be 0. However, enforcing \(x=0\) requires setting one or more \(T^{()}=0\), impeding gradient propagation through the tensors and negatively impacting training performance.

To address this issue, we use another set of tensors \(\{S^{()}\}\) (with the corresponding QuanTA operator \(\)) that are initialized to the same value as \(\{T^{()}\}\) but remain frozen throughout fine-tuning. We then define the adapted layer as

\[y=W_{}x:=W_{0}x+_{}x-x,\] (8)

where we use the subscript \(\) to denote tranable paraemters. At initialization, the terms \(_{}x\) and \(-x\) exactly cancel out, ensuring the adapted layer reduces to the base model.

It is important to note that this initialization method does not introduce additional costs. After initialization, the full \(\) matrix can be explicitly constructed, allowing us to redefine \(W_{0}^{}=W_{0}+\) and simplify the adapted layer to

\[y=W_{}x=W_{0}^{}x+_{}x.\] (9)

## 6 Theoretical Results

Here, we list a few important theorem and provide the proofs in Appendix C

**Theorem 6.1** (Universality of QuanTA).: _Let \(W\) be an arbitrary matrix of shape \(2^{M} 2^{M}\). For any collection of local dimensions \(\{d_{n}\}\) such that each \(d_{n}\) is a power of 2 and \(_{n}d_{n}=2^{M}\), it is always possible to decompose \(W\) into a finite sequence of tensors \(\{T^{()}\}\), where each tensor applies on two axes with local dimensions \(d_{m^{()}}\) and \(d_{n^{()}}\)._

We note that the fine-tuning method KronA  can be incorporated into our framework and considered as a special case of QuanTA.

**Theorem 6.2** (Rank representation).: _Let \(R=r()\) be the rank of the full QuanTA operator, \(R^{()}=r(T^{()})\) be the rank of individual tensors, \(d\) be the total dimension of \(\), \(d^{()}=d_{m^{()}}d_{n^{()}}\) be the total dimension of the individual tensor \(T^{()}\), and \(N_{T}\) be the total number of tensors. The following inequality always holds_

\[_{}}{d^{()}}-d(N_{T}-1) R_{ }}{d^{()}}.\] (10)

In the special case when all the tensors are full rank (\(R^{}=d^{()}\) for all \(\)), the full QuanTA operator is also full rank (\(R=d\)).

**Theorem 6.3** (Composition openness).: _There exists a set \(=\{_{k}\}\) of matrices generated from a fixed QuanTA structure and two matrices \(_{1},_{2}\) such that \(_{1}_{2}\)._

We note that the composition openness condition is not satisfied by low-rank matrix decomposition because, for any two low-rank matrices of the same rank, their composition remains of the same rank. While LoRA may mitigate this limitation by introducing nonlinearity, its expressivity is still constrained by closure under composition. In contrast, QuanTA satisfies the composition openness condition even in the absence of nonlinearity, which suggests that its expressivity can continue to grow as the depth of the neural network increases, even if the network is nearly linear.

**No inference overhead**. As reparameterization-based methods, QuanTA does not impose any inference latency, since the trained \(\) operator can be explicitly constructed as a matrix and merged into the base model weight matrix.

**Memory and computational complexity during fine-tuning.** In the forward pass, only a hidden vector of size \(d\) is kept in the memory as we sequentially apply the tensors to it. Each tensor operation can be viewed as a batched matrix-vector multiplication and has a computational complexity of \(d d_{m}d_{n}\) for tensor applying on the \(m\)th and \(n\)th axes, so the total computational complexity for a QuanTA layer is \(d_{}d_{m^{()}}d_{n^{()}}\). In addition, each tensor contains \((d_{m}d_{n})^{2}\) elements. Therefore, each QuanTA layer contains \(_{}(d_{m^{()}}d_{n^{()}})^{2}\) trainable parameters that need to be stored in the optimizer. As an illustrative example, suppose \(d_{m}=d^{1/N}\) for all \(m\) and there is one tensor for every two axes, the computational complexity can be simplified to \(N(N-1)/2 d^{1+2/N}\), and the parameter count becomes \(N(N-1)/2 d^{4/N}\). When \(N=2\), QuanTA reduces to full fine-tuning.

## 7 Experiments

To benchmark QuanTA against other fine-tuning methods, we performed experiments on a wide range of datasets (see Appendix D for details). For all experiments, we avoid optimizing the hyperparameters on the test set. Instead, we create a validation set from the train set and optimize the hyperparameters on the validation set. All the results reported in this section are averaged over multiple experiments with varying random seeds, and the term "parameters" and "# params" in this section always refer to the trainable parameters. Details on the experiments and hyperparameters are shown in Appendix E.

**DROP Dataset**. We begin our benchmark with the DROP dataset , chosen as a representative example that requires high-rank fine-tuning. In Table 2, we compare our QuanTA method with LoRA of different ranks, as well as series and parallel adapters, by fine-tuning LLaMA2  with up to 70 billion parameters.

As shown in Table 2, LoRA consistently underperforms compared to other fine-tuning methods. While increasing the rank improves performance, LoRA still falls short, suggesting the necessity of high-rank fine-tuning for this task. In addition, QuanTA achieves performance on par with, or better than, full fine-tuning using only a a small fraction of the parameters, demonstrating the effectiveness of QuanTA's high-rank fine-tuning capability.

To investigate how these methods scale with the number of trainable parameters, we conducted experiments varying the number of trainable parameters on LLaMA2-7B model. The results are

  
**Model** & **PEFT Method** & **\# Params (\%)** & \(F_{1}\) **Score (\(\))** \\  _{7B}\)} & FT & 100\% & 59.4 \\  & Series & 0.747\% & 58.8 \\  & Parallel & 0.747\% & 59.0 \\  & LoRA\({}_{r=8}\) & 0.062\% & 54.0 \\  & LoRA\({}_{r=32}\) & 0.249\% & 54.8 \\  & LoRA\({}_{r=128}\) & 0.996\% & 56.2 \\  & **QuanTA\({}_{16.5+4}\) (Ours)** & 0.041\% & 59.5 \\  & **QuanTA\({}_{16.16+16}\) (Ours)** & 0.261\% & **59.6** \\  LLaMA2\({}_{18}\) & LoRA\({}_{r=8}\) & 0.050\% & 61.0 \\  & **QuanTA\({}_{16.58+5}\) (Ours)** & 0.029\% & **69.0** \\  LLaMA2\({}_{70B}\) & LoRA\({}_{r=8}\) & 0.024\% & 74.3 \\  & **QuanTA\({}_{16.58+8}\) (Ours)** & 0.014\% & **79.4** \\   

Table 2: Benchmark of various fine-tuning methods on the DROP dataset using LLaMA2 7-70 billion parameter models as the base model. In each case, we report the average of \(F_{1}\) score over 2-4 experiments with different random seeds.

Figure 4: Benchmark of different fine-tuning methods on the DROP dataset as a function of training parameters using LLaMA2 7 billion parameter model as the base model.

shown in Fig. 4. Each point in the figure represents an average of four experiments with different random seeds, and the standard deviation across these experiments is shown as error bars 8.

As illustrated in the figure, QuanTA achieves performance comparable to or better than full fine-tuning using a small fraction of trainable parameters. Conversely, LoRA only achieves subpar performance with a small number of trainable parameters, though its performance improves with an increase in parameters. Other PEFT methods, such as series and parallel adapters, achieve results close to full fine-tuning but use significantly more parameters than QuanTA.

**Commonsense Reasoning.** We continue to evaluate our method on a collection of commonsense reasoning datasets. Following the methodology in , we first fine-tune the model on the Commonsense170K dataset , a comprehensive collection of commonsense reasoning questions, and subsequently evaluate it on eight different downstream tasks.

In Table 3, we benchmark our QuanTA method against other fine-tuning techniques using 7- and 13-billion-parameter LLaMA and LLaMA2 models, as well as the 8-billion-parameter LLaMA3 model. Alongside prefix tuning, adapter methods, and LoRA, we also compare our approach to the recently proposed LoRA variant, the DoRA method . The results clearly indicate that our QuanTA method outperforms LoRA in all cases and surpasses the DoRA method in most benchmarks, using less than one-tenth of the parameters.

**Arithmetic Reasoning**. We further test our method on arithmetic reasoning tasks by fine-tuning the model on Math10K dataset  and assessing its performance on four tasks. We note that while  includes additional downstream tasks in the arithmetic reasoning benchmark, some test data was later found to have leaked into the training dataset. In this study, we only benchmark the four downstream

    &  &  &  \\   & & & **Bool** & **PIQA** & **SIQA** & **HellaS** & **WinoG** & **ARC-e** & **ARC-c** & **OBQA** & **Avg.** \\  GPT-3\({}_{1758}\)* & – & – & 60.5 & 81.0 & – & 78.9 & 70.2 & 68.8 & 51.4 & 57.6 & – \\ PaLM\({}_{3406}\)* & – & – & 88.0 & 82.3 & – & 83.4 & 81.1 & 76.6 & 53.0 & 53.4 & – \\ ChaGPT* & – & – & 73.1 & 85.4 & 68.5 & 78.5 & 66.1 & 89.8 & 79.9 & 74.8 & 77.0 \\    } & FT & 100\% & 71.3 & 82.1 & 78.6 & 90.2 & 79.0 & 82.9 & 67.2 & 76.8 & 78.5 \\  & Prefix* & 0.11\% & 64.3 & 76.8 & 73.9 & 42.1 & 72.1 & 72.9 & 54.0 & 60.6 & 64.6 \\  & Series* & 0.99\% & 63.0 & 79.2 & 76.3 & 67.9 & 75.7 & 74.5 & 57.1 & 72.4 & 70.8 \\  & Parallel* & 3.54\% & 67.9 & 76.4 & 78.8 & 69.8 & 78.9 & 73.7 & 57.3 & 75.2 & 72.3 \\  & LoRA* & 0.83\% & 68.9 & 80.7 & 77.4 & 78.1 & 78.8 & 77.8 & 61.3 & 74.8 & 74.7 \\  & DoRA\({}^{}\) & 0.43\% & 70.0 & 82.6 & **79.7** & 83.2 & 80.6 & 80.6 & 65.4 & 77.6 & 77.5 \\  & DoRA\({}^{}\) & 0.84\% & 69.7 & **83.4** & 78.6 & 87.2 & 81.0 & 81.9 & 66.2 & 79.2 & 78.4 \\  & **QuanTA (Ours)** & 0.041\% & **71.6** & 83.0 & **79.7** & **91.8** & **81.8** & **84.0** & **68.3** & **82.1** & **80.3** \\    } & Prefix* & 0.03\% & 65.3 & 75.4 & 72.1 & 55.2 & 68.6 & 79.5 & 62.9 & 68.0 & 68.4 \\  & Series* & 0.80\% & 71.8 & 83.0 & 79.2 & 88.1 & 82.4 & 82.5 & 67.3 & 81.8 & 79.5 \\  & Parallel* & 2.89\% & 72.5 & 84.8 & 79.8 & 92.1 & 84.7 & 84.2 & 71.2 & 82.4 & 81.5 \\  & LoRA\({}^{}\) & 0.67\% & 72.1 & 83.5 & 80.5 & 90.5 & 83.7 & 82.8 & 68.3 & 83.2 & 84.0 \\  & DoRA\({}^{}\) & 0.35\% & 72.5 & 85.3 & 79.9 & 90.1 & 82.9 & 82.7 & 69.7 & 83.6 & 80.8 \\  & DoRA\({}^{}\) & 0.68\% & 72.4 & 84.9 & 81.5 & 92.4 & 84.2 & 84.2 & 69.6 & 82.8 & 81.5 \\  & **QuanTA (Ours)** & 0.029\% & **73.2** & **85.4** & **82.1** & **93.4** & **85.1** & **87.8** & **73.3** & **84.4** & **83.1** \\    } & FT & 100\% & **72.9** & 83.0 & 79.8 & 92.4 & 83.0 & **86.6** & 72.0 & 80.1 & 81.2 \\  & LoRA\({}^{}\) & 0.83\% & 69.8 & 79.9 & 79.5 & 83.6 & 82.6 & 79.8 & 64.7 & 81.0 & 77.6 \\   & DoRA\({}^{}\) & 0.43\% & 72.0 & 83.1 & **79.9** & 89.1 & 83.0 & 84.5 & 71.0 & 81.2 & 80.5 \\   & DoRA\({}^{}\) & 0.84\% & 71.8 & 83.7 & 76.0 & 89.1 & 82.6 & 83.7 & 68.2 & 82.4 & 79.7 \\   & **QuanTA (Ours)** & 0.01\% & 72.4 & **83.8** & 79.7 & **92.5** & **83.9** & 85.3 & **72.5** & **82.6** & **81.6** \\    } & LoRA & 0.67\% & 73.3 & 85.6 & 80.8 & 91.6 & 85.5 & 84.2 & 73.7 & 83.3 & 82.3 \\  & **QuanTA (Ours)** & 0.029\% & **75.8** & **86.9** & **81.2** & **94.4** & **87.0** & **89.6** & **77.9** & **85.2** & **84.8** \\    } & LoRA\({}^{}\) & 0.70\% & 70.8 & 85.2 & 79.9 & 91.7 & 84.3 & 84.2 & 71.2 & 79.0 & 80.8 \\  & DoRA\({}^{}\) & 0.35\% & 74.5 & 88.8 & 80.3 & **95.5** & 84.7 & 90.1 & 79.1 & **87.2** & 85.0 \\   & DoRA\({}^{}\) & 0.71\% & **74.6** & **89.3** & 79.9 & **95.5** & 85.6 & 90.5 & 80.4 & 85.8 & 85.2 \\   & **QuanTA (Ours)** & 0.035\% & 74.3 & 88.1 & **81.8** & 95.1 & **87.3** & **91.1** & **81.7** & **87.2** & **85.8** \\   

Table 3: Benchmark on various commonsense reasoning tasks. All results of models and PEFT methods labeled with “*” are from , and results with “*” are from .

tasks unaffected by this data leakage. Additionally, our evaluation procedure differs slightly from that in  (see Appendix E for details).

Table 4 presents the evaluation results on the four downstream tasks. Notably, all questions in the AQuA dataset are multiple-choice with mostly five options, and all models except GPT-3.5 failed to achieve accuracy higher than 20%. Therefore, we conclude that all models perform equally poorly on this task and exclude it from the average accuracy computation. This phenomenon is also consistent with previous findings [54; 20]. The results show that QuanTA significantly outperforms LoRA and even surpasses full fine-tuning with a small number of parameters. It is surprising that QuanTA exceeds full fine-tuning in these tasks, which may be due to overfitting or the challenges of optimizing hyperparameters for full fine-tuning.

In Appendix F, we include benchmarks with additional fine-tuning methods and on additional datasets.

**Limitations.** QuanTA currently requires applying the tensors sequentially to the hidden vectors, which may result in underutilizing the GPU when the tensors are too small. It will be helpful to develop a more efficient implementation to fully utilize GPU resources. The hyperparameters in QuanTA, such as the number of tensors applying on the same axes, have not been optimized. Choosing an optimal set of tensors could further enhance the performance of QuanTA. In the current experiments, we only consider LLaMA model series and a thorough study on different models will be beneficial if more computational resources are available.

## 8 Conclusion

In this paper, we introduced QuanTA, a novel, easy-to-implement, PEFT method with no inference overhead for large language models. QuanTA leverages quantum-inspired techniques to achieve high-rank adaptations, addressing the limitations of existing low-rank methods. QuanTA introduces high-rank fine-tuning through the universality theorem and rank representation theorem. Our extensive experiments demonstrate the efficacy of QuanTA across various tasks, including commonsense reasoning, arithmetic reasoning, and scalability. QuanTA consistently outperforms traditional fine-tuning methods and other PEFT approaches, achieving superior performance with a significantly smaller number of trainable parameters. This highlights the potential of quantum-informed techniques in enhancing the adaptability and efficiency of large language models.

QuanTA offers a scalable and efficient solution for fine-tuning large language models, advancing the state-of-the-art in natural language processing. There are several promising directions for future research and development of QuanTA. Expanding its application to a wider range of tasks and specialized domains could demonstrate its versatility and robustness. Combining QuanTA with other PEFT methods or incorporating it into ensemble models might further enhance performance, particularly for complex tasks. The parameter efficiency of QuanTA may also imply a lower chance of overfitting. Additionally, exploring advanced optimization techniques tailored specifically for QuanTA could improve convergence rates and overall efficiency. Further design based on principles from quantum computing, such as entanglement and superposition, may lead to even more efficient fine-tuning methods. Our work paves the way for further exploration of quantum-informed methods or even future quantum technologies for machine learning, making it a valuable approach for both research and practical applications with broader impacts.

    &  &  &  \\   & & & **AQuA** & **GSSK** & **MAWPS** & **SVAMP** & **Avg. W/O AQuA** \\  GPT-3.5\({}_{178}\)* & – & – & 38.9 & 56.4 & 87.4 & 69.6 & 71.1 \\  LLaMA2\({}_{78}\) & FT & 100\% & _19.3_ & 65.2 & 92.0 & 80.7 & 79.3 \\  & LoRA & 0.83\% & _17.5_ & 65.7 & 91.2 & **80.8** & 79.6 \\  & **QuanTA (Ours)** & 0.19\% & _16.7_ & **67.0** & **94.3** & 80.3 & **80.5** \\  LLaMA2\({}_{198}\) & LoRA & 0.67\% & _16.7_ & 72.3 & 90.8 & 84.3 & 82.5 \\  & **QuanTA (Ours)** & 0.13\% & _18.9_ & **72.4** & **94.5** & **84.8** & **83.9** \\   

Table 4: Benchmark on various arithmetic reasoning tasks. GPT-3.5 (labeled with “*”) results are taken from .

## Broader Impacts

The development of QuanTA represents an important advancement in the fine-tuning of LLMs, with profound societal implications. By leveraging quantum-informed methods, QuanTA reduces computational and memory demands, making advanced NLP capabilities more accessible and cost-effective. This democratization of AI technology can facilitate its adoption in resource-constrained environments, bridging technological disparities. Additionally, the integration of quantum techniques could spark interdisciplinary innovations, enhancing healthcare diagnostics, financial risk assessment, and personalized education. Furthermore, QuanTA's efficiency aligns with global sustainability efforts by reducing the energy consumption associated with AI training, contributing to the reduction of AI's carbon footprint. Thus, QuanTA not only advances NLP but also promotes inclusive, sustainable, and impactful AI technologies across various sectors. However, the deployment of such powerful AI models raises concerns about data privacy, security, and the potential misuse of AI technologies. Addressing these ethical and societal challenges is crucial to ensure that the benefits of QuanTA are realized responsibly and equitably.