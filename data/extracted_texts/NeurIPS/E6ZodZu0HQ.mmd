# PuLID: Pure and Lightning ID Customization via Contrastive Alignment

Zinan Guo1  Yanze Wu1  Zhuowei Chen  Lang Chen  Peng Zhang  Qian He

ByteDance Inc.

guozinan.1@bytedance.com  Wuyanze123@gmail.com

Equal contributions. \({}^{}\) Corresponding author

###### Abstract

We propose Pure and Lightning ID customization (PuLID), a novel tuning-free ID customization method for text-to-image generation. By incorporating a Lightning T2I branch with a standard diffusion one, PuLID introduces both contrastive alignment loss and accurate ID loss, minimizing disruption to the original model and ensuring high ID fidelity. Experiments show that PuLID achieves superior performance in both ID fidelity and editability. Another attractive property of PuLID is that the image elements (_e.g._, background, lighting, composition, and style) before and after the ID insertion are kept as consistent as possible. Codes and models are available at [https://github.com/ToTheBeginning/PuLID](https://github.com/ToTheBeginning/PuLID).

Figure 1: We introduce PuLID, a tuning-free ID customization approach. PuLID maintains high ID fidelity while effectively reducing interference with the original model’s behavior.

Introduction

As a special category of customized text-to-image (T2I) generation [7; 36; 14; 19; 46; 50], identity (ID) customization allow users to adapt pre-trained T2I diffusion models to align with their personalized ID. One line of work [7; 36; 14; 19] fine-tunes certain parameters on several images with the same ID provided by the user, thereby embedding the ID into the generative model. These methods have spawned many popular AI portrait applications, such as PhotoAI and EPIK.

While tuning-based solutions have achieved commendable results, customizing for each ID requires tens of minutes of fine-tuning, thus making the personalization process economically expensive. Another line of work [48; 50; 3; 42; 22; 21; 44] forgoes the necessity of fine-tuning for each ID, instead resorting to pre-training an ID adapter [13; 28] on an expansive portrait dataset. These methods typically utilize an encoder (_e.g._, CLIP image encoder ) to extract the ID feature. The extracted feature is then integrated into the base diffusion model in a specific way (_e.g._, embedded into cross-attention layer). Although highly efficient, these tuning-free methods face two significant challenges.

\(\)**Insertion of ID disrupts the original model's behavior**. A pure ID information embedding should feature two characteristics. Firstly, an ideal ID insertion should alter only ID-related aspects, such as face, hairstyle, and skin color, while image elements not directly associated with the specific identity, such as background, lighting, composition, and style, should be consistent with the behavior of the original model. To our knowledge, this point has not been focused by previous works. While some research [50; 44; 22] has shown the ability for stylized ID generation, notable style degradation occurs when compared with images before ID insertion (as depicted in Fig. 1). Methods with higher ID fidelity tend to induce more severe style degradation.

Secondly, after the ID insertion, it should still retain the ability of the original T2I model to follow prompts. In the context of ID customization, this generally implies the capacity to alter ID attributes (_e.g._, age, gender, expression, and hair), orientation, and accessories (_e.g._, glasses) via prompts. To achieve these features, current solutions generally fall into two categories. The first category involves enhancing the encoder. IPAdapter [50; 1] shifted from early-version CLIP extraction of grid features to utilizing face recognition backbone  to extract more abstract and relevant ID information. Despite the improved editability, the ID fidelity is not high enough. InstantID  builds on this by including an additional ID&Landmark ControlNet  for more effective modulation. Even though the ID similarity improves significantly, it compromises some degree of editability and flexibility. The second category of methods  supports non-reconstructive training to enhance editability by constructing datasets grouped by ID; each ID includes several images. However, creating such datasets demands significant effort. Also, most IDs correspond to a limited number of celebrities, which might limit their effectiveness on non-celebrities.

\(\)**Lack of ID fidelity**. Given our human sensitivity to faces, maintaining a high degree of ID fidelity is crucial in ID customization tasks. Inspired by the successful experience of face generation [35; 45] tasks during the GAN era , a straightforward idea for improving ID fidelity is to introduce ID loss within diffusion training. However, due to the iterative denoising nature of diffusion models , achieving an accurate \(_{0}\) needs multiple steps. The resource consumption for training in this manner can be prohibitively high. Consequently, some methods  predict \(_{0}\) directly from the current timestep and then calculate the ID loss. However, when the current timestep is large, the predicted \(_{0}\) is often noisy and flawed. Calculating ID loss under such conditions is obviously inaccurate, as the face recognition backbone  is trained on photo-realistic images. Although some workarounds have been proposed, such as calculating ID loss only at less noisy timesteps  or predicting \(_{0}\) with an additional inference step , there still remains room for improvement.

In this work, to maintain high ID fidelity while reducing the influence on the original model's behavior, we propose **PuLID**, a pure and lighting ID customization method via contrastive alignment. Specifically, we introduce a **Lightning T2I branch** alongside the standard diffusion-denoising training branch. Leveraging recent fast sampling methods [26; 38; 23], the lighting T2I branch can generate high-quality images from pure noise with a limited and manageable number of steps. With this additional branch, we can simultaneously address the two challenges mentioned above. Firstly, to minimize the influence on the original model's behavior, we construct a contrastive pair with the same prompt and initial latent, with and without ID insertion. During the Lightning T2I process, we align the UNet features between the contrastive pair semantically, instructing the ID adapter how to insert ID information without affecting the behavior of the original model. Secondly, as we now have the precise and high-quality generated \(_{0}\) after ID insertion, we can naturally extract its face embedding and calculate an accurate ID loss with the ground truth face embedding. It is worth mentioning that such \(_{0}\) generation process aligns with the actual test setting. Our experiments demonstrate that optimizing the ID loss in this context can significantly increase ID similarity.

The contributions are summarized as follows. **(1)** We propose a tuning-free method, namely, PuLID, which preserves high ID similarity while mitigating the impact on the original model's behavior. **(2)** We introduce a Lightning T2I branch alongside the regular diffusion branch. Within this branch, we incorporate a contrastive alignment loss and ID loss to minimize the contamination of ID information on the original model while ensuring fidelity. Compared to the current mainstream approaches that improve the ID encoder or datasets, **we offer a new perspective and training paradigm. (3)** Experiments show that our method achieves SOTA performance in terms of both ID fidelity and editability. Moreover, compared to existing methods, our ID information is less invasive to the model, making our method more flexible for practical applications.

## 2 Related Work

**Tuning-free Text-to-image ID Customization.** ID Customization for text-to-image models aims to empower pre-trained models to generate images of specific identities while following the text descriptions. To ease the resource demand necessitated by tuning-based methods [7; 36; 14; 19; 10; 41], a series of tuning-free methods [42; 44; 29; 50; 22; 48; 51; 4] have emerged, which directly encode ID information into the generation process. The major challenge these methods encounter is minimizing disruption to the original behavior of T2I models while still maintaining high ID fidelity.

In terms of minimizing the disruption, one plausible approach is to utilize a face recognition model  to extract more abstract and relevant facial domain-specific representations, as done by IP-ApdaterFaceID  and InstantID . A dataset comprising multiple images from the same ID can facilitate the learning of a common representation . Despite the progress made by these approaches, they have yet to fundamentally solve the disruption issue. Notably, models with higher ID fidelity often cause more significant disruptions to the behavior of the original model. In this study, we propose a new perspective and training method to tackle this issue. Interestingly, the suggested method does not require laborious dataset collection grouped by ID, nor is it confined to a specific ID encoder.

To improve ID fidelity, ID loss is employed in previous works [18; 3], motivated by its effectiveness in prior GAN-based works [35; 45]. However, in these methods, \(x_{0}\) is typically directly predicted from the current timestep using a single step, often resulting in noisy and flawed images. Such images are not ideal for the face recognition models , as they are trained on real-world images. PortraitBooth  alleviates this issue by only applying ID loss at less noisy stages, which ignores such loss in the early steps, thereby limiting its overall effectiveness. Diffswap  obtains a better predicted \(x_{0}\) by employing two steps instead of just one, even though this estimation still contains noisy artifacts. In our work, with the introduced Lightning T2I training branch, we can calculate ID loss in a more accurate setting.

We notice a concurrent work, LCM-Lookahead , which also uses fast sampling technology (_i.e._, LCM ) to achieve a more precise prediction of \(x_{0}\). However, there are several differences between this work and ours. Firstly, LCM-Lookahead makes a precise prediction of \(x_{0}\) during the conventional diffusion-denoising process, whereas we start from pure noise and iteratively denoise to \(x_{0}\). Our approach, which aligns better with actual testing settings, makes the optimization of ID loss more direct. Secondly, to enhance prompt editing capability, LCM-Lookahead capitalized on the mode collapse phenomenon of SDXL-Turbo  to synthesis an ID-consistent dataset. However, the synthetic dataset might face diversity and consistency challenges, and the authors found that training with this dataset may lean towards stylized results more frequently than other methods. In contrast, our method does not need an ID-grouped dataset. Instead, we enhance prompt follow ability through a more fundamental and intuitive approach, namely, contrastive alignment.

**Fast Sampling of Diffusion Models.** In practice, diffusion models are typically trained under \(1000\) steps. During inference, such a lengthy process can be shortened to a few dozen steps with the help of advanced sampling methods [39; 25; 17]. Recent distill-based works [23; 26; 38] further accelerate this generation process within \(10\) steps. The core motivation is to guide the student network to align with points further from the base teacher model. In this study, the Lightning T2I training branch we introduce leverages the SDXL-Lightning  acceleration technology, thus enabling us to generate high-quality images from pure noise in just \(4\) steps.

## 3 Methods

### Preliminary

**Diffusion models** are a class of generative models capable of synthesizing desired data samples through iterative denoising. A conventional diffusion training encapsulates two procedures, the forward diffusion process and reverse denoising process. During the diffusion process, noise \(\) is sampled and added to the data sample \(x_{0}\) based on a predefined noise schedule. This process yields a noisy sample \(x_{t}\) at timestep \(t\). Conversely, during the denoising process, a denoising model \(e_{}\) takes \(x_{t}\), \(t\), and optional additional conditions \(C\) as inputs to predict the added noise, the optimization process can be articulated as:

\[_{}=_{x_{0},,t}(\|-_{ }(x_{t},t,C)\|). \]

The denoising model \(_{}\) in modern T2I diffusion models [37; 33; 30] is predominantly a UNET composed of residual blocks , self-attention layers, and cross-attention  layers. The prompt, as a condition, is embedded into the cross-attention layers adhering to the attention mechanism, illustrated as follows:

\[\{(Q,K,V)=(}{})V\\ K=_{K}_{txt}(C_{txt});\;V=_{V}_{txt}(C_{txt}),. \]

where \(Q\) is projected from the UNET image features, \(_{txt}\) denotes a pre-trained language model that converts prompt \(C_{txt}\) to textual features, \(_{K}\) and \(_{V}\) are the learned linear layers.

**ID Customization** in T2I diffusion introduces ID images \(C_{id}\) as an additional condition, working together with the prompt to control image generation. Tuning-free customization [16; 48; 50] methods

Figure 2: **Overview of PuLID framework. The upper half of the framework illustrates the conventional diffusion training process. The face extracted from the same image is employed as the ID condition \(C_{id}\). The lower half of the framework demonstrates the Lightning T2I training branch introduced in this study. It leverages the recent fast sampling methods to iteratively denoise from pure noise to high-quality images in a few steps (\(4\) in this paper). In this branch, we construct contrastive paths with and without ID injection and introduce an alignment loss to instruct the model on how to insert ID condition without disrupting the original model’s behavior. As this branch can produce photo-realistic images, it implies that we can achieve a more accurate ID loss for optimization.**

typically employ an encoder to extract ID features from \(C_{id}\). The encoder often includes a frozen backbone, such as CLIP image encoder  or face recognition backbone , along with a learnable head. A simple yet effective technique to embed the ID features to the pre-trained T2I model is to add parallel cross-attention layers to the original ones. In these parallel layers, learnable linear layers are introduced to project the ID features into \(K_{id}\) and \(V_{id}\) for calculating attention with \(Q\). This technique, proposed by IP-Adapter , has been widely used, we also adopt it for embedding ID features in this study.

### Basic Settings

We build our model based on the pre-trained SDXL , which is a SOTA T2I latent diffusion model. Our ID encoder employs two commonly used backbones within the ID customization domain: the face recognition model  and the CLIP image encoder , to extract ID features. Specifically, we concatenate the feature vectors from the last layer of both backbones (for the CLIP image encoder, we use the CLS token feature), and employ a Multilayer Perceptron (MLP) to map them into \(5\) tokens as the global ID features. Additionally, following ELITE's approach , we use MLPs to map the multi-layer features of CLIP to another \(5\) tokens, serving as the local ID features. It is worth noting that our method is not restricted to a specific encoder.

### Discussion on Common Diffusion Training in ID Customization

Currently, tuning-free ID customization methods generally face a challenge: the embedding of the ID disrupts the behavior of the original model. The disruption manifests in two ways: firstly, the ID-irrelevant elements in the generated image (e.g., background, lighting, composition, and style) have changed extensively compared to before the ID insertion; secondly, there is a loss of prompt adherence, implying we can hardly edit the ID attributes, orientations, and accessories with the prompt. Typically, models with higher ID fidelity suffer more severe disruptions. Before we present our solutions, we first analyze why conventional diffusion training would cause this issue.

In conventional ID Customization diffusion training process, as formulated in Eq. 1, the ID condition \(C_{id}\) is usually cropped from the target image \(x_{0}\)[50; 44]. In this scenario, the ID condition aligns completely with the prompt and UNET features, implying the ID condition does not constitute contamination to the T2I diffusion model during the training process. This essentially forms a reconstruction training task. So, to better reconstruct \(x_{0}\) (or predict noise \(\)), the model will make the utmost effort to use all the information from ID features (which may likely contain ID-irrelevant information), as well as bias the training parameters towards the dataset distribution, typically in the realistic portrait domain. Consequently, during testing, when we provide a prompt that is in conflict or misaligned with the ID condition, such as altering ID attributes or changing styles, these methods tend to fail. This is because there exists a disparity between the testing and training settings.

### Uncontaminated ID Insertion via Contrastive Alignment

While it is difficult to ascertain whether the insertion of ID disrupts the original model's behavior during the conventional diffusion training, it is rather easy to recognize under the test settings. For instance, we can easily observe whether the elements of the image change after the ID is embedded, and whether it still possesses prompt follow ability. Thus, our solution is intuitive. We introduce a Lightning T2I training branch beyond the conventional diffusion-denoising training branch. Just like in the test setting, the Lighting T2I branch starts from pure noise and goes through the full iterative denoising steps until reaching \(x_{0}\). Leveraging recent fast sampling methods [26; 38; 23], the Lighting T2I branch can generate high-quality images from pure noise with a limited and manageable number of steps. Concretely, we employ SDXL-Lightning  with \(4\) denoising steps. We prepare a list of challenging prompts that can easily reveal contamination, as shown in Table 8. During each training iteration, a random prompt from this list is chosen as the textual condition for the Lightning T2I branch. Then, we construct contrastive paths that start from the same prompt and initial latent. One path is conditioned only by the prompt, while the other path employs both the ID and the prompt as conditions. By semantically aligning the UNET features on these two paths, the model will learn how to embed ID without impacting the behavior of the original model. The overview of our method is shown in Fig. 2.

We chose to align the contrastive paths in their corresponding UNET's cross-attention layers. Specifically, we denote the UNET features in the path without ID embedding as \(Q_{t}\), whereas the corresponding UNET features in the contrastive path with ID embedding as \(Q_{tid}\). For simplicity, we omit the specific layers and denoising steps here. In actuality, alignment is conducted across all layers and time steps.

Our alignment loss consists of two components: the semantic alignment loss and the layout alignment loss. An illustration is presented in Fig. 3 (a). We use textual features \(K\) to query the UNET features \(Q\). For each token in \(K\), it will calculate the correlation with \(Q\), and further aggregate \(Q\) based on the correlation matrix. Analogous to Eq. 2, the attention mechanism here can be expressed as \((K,Q,Q)\), which can be interpreted as the response of the UNET features to the prompt. The insight behind our semantic alignment loss is simple: if the embedding of ID does not affect the original model's behavior, then the response of the UNET features to the prompt should be similar in both paths. Therefore, our semantic alignment loss \(_{}\) can be formulated as follows:

\[_{}=\|(^{T}}{ })Q_{tid}-(^{T}}{})Q_{t}\|_{2}. \]

As illustrated in Fig. 3 (b), the introduction of \(_{}\) significantly mitigates the issue of ID information contaminating the model's behavior. However, it cannot guarantee layout consistency, so we add a layout alignment loss \(_{}\), which is defined as:

\[_{}=\|Q_{tid}-Q_{t}\|_{2}. \]

The full alignment loss is formulated as

\[_{}=_{}_{}+_{}_{}, \]

where \(_{}\) and \(_{}\) serve as hyperparameters that determine the relative importance of each loss item. In practice, we set \(_{}\) to a relatively small value, as we found that a larger value compromises the ID fidelity.

### Optimizing ID Loss in a More Accurate Setting

In ID Customization tasks, ensuring a high degree of ID fidelity is essential, given our innate human sensitivity towards discerning facial features. To improve the ID fidelity, aside from enhancements on the ID encoder , another universal and parallel improvement is the introducing of an ID loss  during the training. However, these methods directly predict \(x_{0}\) at the \(t\)-th timestep in the diffusion training process, only using a single step. This will produce a noisy and flawed predicted \(x_{0}\), subsequently leading to inaccurate calculation of ID loss. To ease this issue, recent work  proposes to only applying the ID loss on less noisy stages. However, since the ID loss only affects a portion of timesteps, which may potentially limit the full effectiveness of it. In this study, thanks to the introduced Lightning T2I branch, the above issue can be fundamentally resolved. Firstly, we can swiftly generate an accurate \(x_{0}\) conditioned on the ID from pure noise within \(4\) steps. Consequently, calculating the ID loss on this \(x_{0}\), which is very close to the real-world data distribution, is evidently

Figure 3: Illustration and Effect of the alignment loss.

more precise. Secondly, optimizing ID loss in a setting that aligns with the testing phase, is more direct and effective. Formally, the ID loss \(_{id}\) is defined as:

\[_{}=1-CosSim((C_{id}),((x_{T},C_{id},C_{txt}))), \]

where \(x_{T}\) denotes the pure noise, L-T2I represents the Lightning T2I branch, and \(\) denotes the face recognition backbone . To generate photo-realistic faces, we fix the prompt \(C_{txt}\) to "portrait, color, cinematic".

### Full Objective

The full learning objective is defined as:

\[=_{}+_{}+_{ }_{}. \]

During training, only the newly introduced MLPs and the learnable linear layers \(K_{id}\) and \(V_{id}\) in cross-attention layers are optimized with this objective, with the rest remaining frozen.

## 4 Experiments

### Implementation Details

We build our PuLID model based on SDXL  and the \(4\)-step SDXL-Lightning . For the ID encoder, we use antelopev2  as the face recognition model and EVA-CLIP  as the CLIP Image encoder. Our training dataset comprises \(1.5\) million high-quality human images collected from the Internet, with captions automatically generated by BLIP-2 . Our training process consists of three stages. In the first stage, we use the conventional diffusion loss \(_{}\) to train the model. In the second stage, we resume from the first stage model and train with the ID loss \(_{}\) (we use arcface-50  to calculate ID loss) and diffusion loss \(_{}\). This model strives for the maximum ID fidelity without considering the contamination to the original model. In the third stage, we add the alignment loss \(_{}\) and use the full objective as shown in Eq. 7 to fine-tune the model. We set the \(_{}\) to \(0.6\), \(_{}\) to \(0.1\), and \(_{}\) to \(1.0\). In the Lightning T2I training branch, we set the resolution of the generated image to \(768 768\) to conserve memory. Training is performed with PyTorch and diffusers on \(8\) NVIDIA A100 GPUs in an internal cluster.

### Test Settings

For consistency in comparison, unless otherwise specified, all the results in this paper are generated with the SDXL-Lightning  base model over \(4\) steps using the DPM++ 2M sampler . The CFG-scale is set to \(1.2\), as recommended by . Moreover, for each comparison sample, all methods utilize the same seed. We observe that the comparison methods, namely InstantID  and IPAdapter (more specifically, IPAdapter-FaceID ) are highly compatible with the SDXL-Lightning model. As shown in appendix subsection 7.5, when compared to using SDXL-base  as the base model, employing SDXL-Lightning results in InstantID generating more natural and aesthetically pleasing images, and enables IPAdapter to achieve higher ID fidelity. Furthermore, we provide a quantitative comparison with these methods on SDXL-base, with the conclusions remaining consistent with those on SDXL-Lightning.

To more effectively evaluate these methods, we collected a diverse portrait test set from the internet. This set covers a variety of skin tones, ages, and genders, totaling 120 images, which we refer to as DivID-120. As a supplementary resource, we also used a recent open-source test set, Unsplash-50 , which comprises 50 portrait images uploaded to the Unsplash website between February and March 2024.

### Qualitative Comparison

As shown in Fig. 4, when compared to SOTA methods such as IPAdapter and InstantID, our PuLID tends to achieve higher ID fidelity while creating less disruption to the original model. From columns 1, 2, 5, 6, and 7, it is clear that our method can attain high ID similarity in realistic portrait scenes and delivers better aesthetics. Conversely, other methods either fall short in ID fidelity or show diminished aesthetics compared to the base model. Another distinct advantage of our approach is that as the disruption to the model decreases, the results produced by PuLID accurately replicate the lighting (1st column), style (4th column), and even layout (5th column) of the original model. In contrast, although comparative methods can also perform stylization, notable style degradation can be noticed when compared to the original model. Finally, our model also possesses respectable prompt-editing capabilities, such as changing orientation (2nd column), altering attributes (6th column), and modifying accessories (7th column). **More qualitative results can be found in subsection 7.1 and subsection 7.2 of the appendix**.

### Quantitative Comparison

The quantitative results are reported in Table 1. **Face Sim.** represents the ID cosine similarity, with ID embeddings extracted using CurricularFace . CurricularFace is different from the face recognition models we use in the ID encoder and for calculating ID loss. **CLIP-T** measures the ability to follow prompt. We also use **CLIP-I** to quantify the CLIP image similarity between two images before and after the ID insertion. A higher CLIP-I metric indicates a smaller modification in image elements (such as the background, composition, style) after ID insertion, suggesting a lower degree

    &  &  \\  & Face Sim.\(\) & CLIP-T\(\) & CLIP-I\(\) & Face Sim.\(\) & CLIP-T\(\) & CLIP-I\(\) \\  PhotoMaker* & 0.271 & 26.06 & 0.649 & 1 & 0.193 & 27.38 & 0.692 \\ IPAdapter & 0.619 & 28.36 & 0.703 & 1 & 0.615 & 28.71 & 0.701 \\ InstantID & 0.725 & 28.72 & 0.680 & 1 & 0.614 & 30.55 & 0.736 \\  PuLID (ours) & 1 & **0.733** & **31.31** & **0.812** & 1 & **0.659** & **32.16** & **0.840** \\   

Table 1: **Quantitative comparisons.** *We observed that PhotoMaker shows limited compatibility with SDXL-Lightning, hence, we compare its performance on SDXL-base in this table.

Figure 4: **Qualitative comparisons.** T2I w/o ID represents the output generated by the original T2I model without inserting ID, which reflects the behavior of the original model. Our PuLID achieves higher ID fidelity while causing less disruption to the original model. As the disruption to the model is reduced, results generated by PuLID accurately reproduce the lighting (1st row), style (4th row), and even layout (5th row) of the original model. This unique advantage broadens the scope for a more flexible application of PuLID.

of disruption to the original model's behavior. We provide the evaluation prompts in the appendix. As observed from Table 1, our method, PuLID, surpasses comparison methods across all three metrics, achieves SOTA performance in terms of both ID fidelity and editability. Furthermore, our method significantly outperforms others with respect to the CLIP-I metric, implying that our method incurs much less intrusion on model behavior compared to other methods.

### Ablation

**Alignment loss ablation**. Fig. 5 displays a qualitative comparison between models trained with and without the alignment loss \(_{}\). As observed, without \(_{}\), the embedding of ID severely disrupts the behavior of the original model. This disruption manifests as an inability for the prompt to precisely modify style (the left two cases of Fig. 5) and orientation (the lower right case of Fig. 5). Also, the layout would collapse to the extent that the face occupies the majority of the image area, resulting in a diminished diversification of the layout. However, with the introduction of our alignment loss, this disruption can be significantly reduced. From Table 2, we could also observe a large improvement in CLIP-T and CLIP-I when equipped with \(_{}\) (from Stage2 to Stage3).

**ID loss ablation**. Table 2 illustrates the improvement in ID fidelity using the naive ID loss (directly predicting \(x_{0}\) from current timestep) and the more accurate ID loss \(_{}\) introduced in this paper, in comparison to the baseline. As observed, \(_{}\) can accomplish a greater improvement compared to the naive ID loss. We attribute this to the more precise \(x_{0}\) provided by the Lightning-T2I branch, which also better aligns with the testing setting, thereby making the optimization of ID loss more direct and effective. Another worth mentioning point is that the baseline, naively trained on the internal dataset, underperforms in ID fidelity and editability. Conversely, the introduction of the PuLID training paradigms delivers significant enhancement. Therefore, this substantiates that the improvement mainly comes from the method, rather than the dataset.

## 5 Limitation

While our PuLID achieves superior ID fidelity and editability with minimal disruption to the base model's behavior, it still has limitations. Due to the incorporation of the Lightning T2I branch, the training speed per iteration is slower than that of conventional diffusion training, and it also demands more CUDA memory. However, this issue can be significantly mitigated when future fast sampling

    & &  &  \\  &, & Face Sim.\(\) & CLIP-T\(\) & CLIP-I\(\) & Face Sim.\(\) & CLIP-T\(\) & CLIP-I\(\) \\  Baseline (Stage1) &   } & 0.561 & 29.06 & 0.736 &   } & 0.514 & 30.16 & 0.769 \\   w/ ID Loss naive & & 0.652 & 27.05 & 0.683 & & 0.601 & 28.00 & 0.707 \\ w/ ID Loss (Stage2) &   } & **0.761** & 24.91 & 0.624 &   } & **0.708** & 25.83 & 0.646 \\   PuLID (Stage3) & & 0.733 & **31.31** & **0.812** & & 0.659 & **32.16** & **0.840** \\   

Table 2: **Quantitative comparisons for ablation studies on ID loss and alignment loss.**

Figure 5: **Qualitative comparison for ablation study on alignment loss.**

methods can generate satisfying results in a single step (currently, we use 4 steps). Another limitation is that although the accurate ID loss introduced in the Lightning T2I branch markedly enhances ID fidelity, it also impacts image quality to some extent, such as causing blurriness in faces. This can be discerned in Fig. 3 (b) and Fig. 5. Nonetheless, this issue is not uniquely associated to our method, as similar phenomena has been observed in reward tuning methods [49; 31; 5; 47], with  attributing it to the reward-hacking problem. Although this issue can be largely alleviated by pairing with our proposed contrastive alignment loss, future work could explore a more effective ID loss that does not negatively affect image quality.

## 6 Conclusion

This paper presents PuLID, a novel tuning-free approach to ID customization in text-to-image generation. By incorporating a Lightning T2I branch along with a contrastive alignment strategy, PuLID achieves superior ID fidelity and editability with minimal disruption to the base model's behavior. Experimental results demonstrate that PuLID surpasses current methods, showcasing its potential for flexible and efficient personalized image generation. Future work could explore the application of this proposed training paradigm to other image customization tasks, like IP and style customization.