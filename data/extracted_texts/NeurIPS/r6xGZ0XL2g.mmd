# Meta-Learning Adversarial Bandit Algorithms

Mikhail Khodak

Carnegie Mellon University

khodak@cmu.edu

&Ilya Osadchiy

Technion - Israel Institute of Technology

osadchiy.ilya@gmail.com

&Keegan Harris

CMU

&Maria-Florina Balcan

CMU

&Kfir Y. Levy

Technion

&Ron Meir

Technion

&Zhiwei Steven Wu

CMU

###### Abstract

We study online meta-learning with bandit feedback, with the goal of improving performance across multiple tasks if they are similar according to some natural similarity measure. As the first to target the adversarial online-within-online partial-information setting, we design meta-algorithms that combine outer learners to simultaneously tune the initialization and other hyperparameters of an inner learner for two important cases: multi-armed bandits (MAB) and bandit linear optimization (BLO). For MAB, the meta-learners initialize and set hyperparameters of the Tsallis-entropy generalization of Exp3, with the task-averaged regret improving if the entropy of the optima-in-hindsight is small. For BLO, we learn to initialize and tune online mirror descent (OMD) with self-concordant barrier regularizers, showing that task-averaged regret varies directly with an action space-dependent measure they induce. Our guarantees rely on proving that unregularized follow-the-leader combined with two levels of low-dimensional hyperparameter tuning is enough to learn a sequence of affine functions of non-Lipschitz and sometimes non-convex Bregman divergences bounding the regret of OMD.

## 1 Introduction

Learning-to-learn  is an important area of research that studies how to improve the performance of a learning algorithm by _meta-learning_ its parameters--e.g. initializations, step-sizes, and/or representations--across many similar tasks. The goal is to encode information from previous tasks in order to achieve better performance on future ones. Meta-learning has seen a great deal of experimental work , practical impact , and theoretical effort . One important setting is online-within-online meta-learning , where the learner performs a sequence of tasks, each of which has a sequence of rounds. Past work has studied the _full-information_ setting, where the loss for every arm is revealed after each round. This assumption is not realistic in many applications, e.g. recommender systems and experimental design, where often partial or _bandit_ feedback--only the loss of the action taken--is revealed. Such feedback can be _stochastic_, e.g. the losses are i.i.d. from some distribution, or _adversarial_, i.e. chosen by an adversary. We establish the first formal guarantees for online-within-online meta-learning with adversarial bandit feedback.

As with past full-information meta-learning results, our goal when faced with a sequence of bandit tasks will be to achieve low regret _on average_ across them. Specifically, our task-averaged regret should (a) be no worse than that of algorithms for the single-task setting, e.g. if the tasks are not very similar, and should (b) be much better on tasks that are closely related, e.g. if the same small set of arms do well on all of them. We show that a natural way to achieve both is to initialize and tune online mirror descent (OMD), an algorithm associted with a strictly convex regularizer whose hyperparameters have a significant impact on performance. Our approach works because it can learn the best hyperparameters in hindsight across tasks, which will recover OMD's worst-case optimal performance if the tasks are dissimilar but will take advantage of more optimistic settings if they are related. As generalized distances, the regularizers also induce interpretable measures of similarity between tasks.

### Main contributions

We design a meta-algorithm (Algorithm 1) for learning variants of OMD--specifically those with entropic or self-concordant regularizers--that are used for adversarial bandits. This meta-algorithm combines three _full-information_ algorithms--follow-the-leader (FTL), exponentially weighted online optimization (EWOO), and multiplicative weights (MW)--to set the initialization, step-size, and regularizer-specific parameters, respectively. It works by optimizing a sequence of functions that each _upper-bound_ the regret of OMD on a single task (Theorem 2.1), resulting in (a) interesting notions of task-similarity because these functions depend on generalized notions of distances (Bregman divergences) and (b) adaptivity, i.e not needing to know how similar the tasks are beforehand.

Our first application is to OMD with the Tsallis regularizer , a relative of Exp3  that is optimal for adversarial MAB. We bound the task-averaged regret by the Tsallis entropy of the _estimated_ optima-in-hindsight (Corollary 3.1), which we further extend to that of the _true_ optima by assuming a gap between the best and second-best arms (Corollary 3.2). Both results are the first known consequences of the online learnability of Bregman divergences that are _non-convex_ in their second arguments , while the latter is obtained by showing that the loss estimators of a modified algorithm identify the optimal arm w.h.p. As an example, our average \(m\)-round regret across \(T\) tasks under the gap assumption is

\[o_{T}((m))+2_{(0,1]}d^{}m/}+o ()\] (1)

where \(d\) is the number of actions and \(H_{}\) is the Tsallis entropy  of the distribution of the optimal actions (\(=1\) recovers the Shannon entropy).1 This entropy is low if all tasks are usually solved by the same few arms, making it a natural task-similarity notion. For example, if only \(s d\) of the arms are ever optimal then \(H_{}=(s)\), so using \(=1/ d\) in (1) yields an asymptotic task-averaged regret of \(()\), dropping fast terms. For \(s=_{d}(1)\) this beats the minimax optimal rate of \(()\). On the other hand, since \(H_{1/2}=()\), the same bound recovers this rate in the worst-case of dissimilar tasks.

Lastly, we adapt our meta-algorithm to the adversarial BLO problem by setting the regularizer to be a self-concordant barrier function, as in Abernethy et al. . Our bounds yield notions of task-similarity that depend on the constraints of the action space, e.g. over the sphere the measure is the closeness of the average of the estimated optima to the sphere's surface (Corollary 4.1). We also instantiate BLO on the bandit shortest-path problem (Corollary 4.2) .

### Related work

While we are the first to consider meta-learning under adversarial bandit feedback, many have studied meta-learning in various _stochastic_ bandit settings . The latter three study stochastic bandits under various task-generation assumptions, e.g. Azizi et al.  is in a batch-within-online setting where the optimal arms are adversarial. In contrast, we make no distributional assumptions either within or without. Apart from this difference, the results of Azizi et al.  are the ones our MAB results are most easily compared to, which we do in detail in Section 3. Notably, they assume that only \(s d\) of the \(d\) arms are ever optimal across \(T\) tasks and show (roughly speaking) \(}()\) asymptotic regret; we instead focus on an entropic notion of task-similarity that achieves the same asymptotic regret when specialized to their \(s d\). However, avoiding their explicit assumption has certain advantages, e.g. robustness in the presence of \(o(T)\) outlier tasks (c.f. Section 3.3).

A setting that bears some similarity to online-within-online bandits is that of switching bandits , and more generally online learning with dynamic comparators . In such problems, instead of using a static best arm as the comparator we use a piecewise constant sequence of arms, with a limited number of arm switches. The key difference between such work and ours is our assumption that task-boundaries are known; this makes the other setting more general. However, while e.g. Exp3.S  can indeed be applied to online meta-learning, guarantees derived from switching costs _cannot_ improve upon just running Tsallis-INF on each task [39, Table 1]. Furthermore, these approaches usually quantify difficulty by the number of switches, whereas we focus on task-similarity. While there exists stochastic-setting work that measures difficulty using a notion of average change in distribution across rounds , it does not lead to improved performance if this average change is \((T)\), as is the case in e.g. the \(s\)-sparse setting discussed above.

There has been a variety of work on full-information online-within-online meta-learning [32; 12], including tuning OMD [31; 19]. Doing so for bandit algorithms has many additional challenges, including (1) their inherent and high-variance stochasticity, (2) the use of non-Lipschitz and even unbounded regularizers, and (3) the lack of access to task-optima in order to adapt to deterministic, algorithm-independent task-similarity measures. Theoretically our analysis draws on the average regret-upper-bound analysis (ARUBA) framework , which observes that OMD can be tuned by targeting its upper bounds, which are affine functions of Bregman divergences, and provide online learning tools for doing so. Our core structural result shows that the distance generating functions \(_{}\) of these Bregman divergences can be tuned without interfering with meta-learning the initialization and step-size; tuning \(\) is critical for adapting to settings such as that of a small set of optimal arms in MAB. Doing so depends on several refinements of the original approach, including bounding the task-averaged-regret via the spectral norm of \(^{2}_{}\) and expressing the loss of the meta-comparator using only \(_{}\), rather than via its Bregman divergence as in prior work. Finally, applying our structural result requires setting-specific analysis, e.g. to show regularity w.r.t. \(\) or to obtain MAB guarantees in terms of the entropy of the true optimal arms. The latter is especially difficult, as Khodak et al.  define task-similarity via full information upper bounds, and involves applying tools from the best-arm-identification literature  to show that a constrained variant of Exp3 finds the optimal arm w.h.p.

## 2 Learning the regularizers of bandit algorithms

We consider the problem of meta-learning over bandit tasks \(t=1,,T\) over some fixed set \(^{d}\), a (possibly improper) subset of which is the action space \(\). On each round \(i=1,,m\) of task \(t\) we play action \(_{t,i}\) and receive feedback \(_{t,i}(_{t,i})\) for some function \(_{t,i}:A[-1,1]\). Note that all functions we consider will be linear and so we will also write \(_{t,i}()=_{t,i},\). Additionally, we assume the adversary is _oblivious within-task_, i.e. it chooses losses \(_{t,1},,_{t,m}\) at time \(t\). We will also denote \((a)\) to be the \(a\)-th element of the vector \(^{d},^{}\) to be the interior of \(\), \(\) its boundary, and \(\) to be the simplex on \(d\) elements. Finally, note that all proofs can be found in the Appendix.

In online learning, the goal on a single task \(t\) is to play actions \(_{t,1},_{t,m}\) that minimize the regret \(_{i=1}^{m}_{t,i}(_{t,i})-_{t,i}(}_{t})\), where \(}_{t}_{}_{i=1}^{m}_ {t,i}()\). Lifting this to the meta-learning setting, our goal as in past work [31; 19] will be to minimize the **task-averaged regret**: \(_{t=1}^{T}_{i=1}^{m}_{t,i}(_{t,i})-_{t,i} (}_{t})\). In particular, we want to use multi-task data to improve average performance as the number of tasks \(T\). For example, we wish to attain a task-averaged regret bound of the form \(o_{T}((m))+}(V)+o()\), where \(V_{ 0}\) is a measure of task-similarity that is small if the tasks are similar but still yields the worst-case single-task performance--\(()\) for MAB and \((d)\) for BLO--if they are not.

### Online mirror descent as a base-learner

In meta-learning we are commonly interested in learning a within-task algorithm or **base-learner**, a parameterized method that we run on each task \(t\). A popular approach is to learn the initialization and other parameters of a gradient-based method such as gradient descent [24; 44; 36]. If the task optima are close, the best initialization should perform well after only a few steps on a new task. We take a similar approach applied to online mirror descent, a generalization of gradient descent to non-Euclidean geometries . Given a strictly convex **regularizer**\(:^{}\), step-size \(>0\), and initialization \(_{t,1}^{}\), OMD has the iteration

\[_{t,i+1}=*{arg\,min}_{^{}}B (\|_{t,1})+_{j i}_{t,j}( _{t,j}),\] (2)

where \(B(\|)=()-()-( ),-\) is the **Bregman divergence** of \(\). OMD recovers online gradient descent when \(()=\|\|_{2}^{2}\), in which case \(B(\|)=\|-\|_{2}^{2}\); another example is exponentiated gradient, for which \(()=,\) is the negative Shannon entropy on probability vectors \(\) and \(B\) is the KL-divergence . An important property of \(B\) is that the sum over functions \(B(_{t}||)\) is minimized at the mean \(}\) of the points \(_{1},,_{T}\).

OMD on **loss estimators**\(_{t,i}\) constructed via partial feedback forms an important class of bandit methods [6; 2; 3]. Their regularizers \(\) are often non-Lipschitz, e.g. the negative entropy, or even unbounded, e.g. the log-barrier. Thus full-information results for tuning OMD, e.g. by Khodak et al.  and Denevi et al. , do not suffice. We do adapt the former's approach of online learning a sequence \(U_{t}(,,)\) of affine functions of Bregman divergences from initializations \(\) to known points in \(\). We are interested in them because the regret of OMD w.r.t. a comparator \(\) is bounded by \(B(||)/+( m)\)[46; 25]. In our case the operator is based on the estimated optimum \(}_{t}*{arg\,min}_{} _{t},\), where \(_{t}=_{i=1}^{m}_{t,i}\), resulting from running OMD on task \(t\) using initialization \(\) and hyperparameters \(\) and \(\), which we denote \(_{,}()\). Unlike full-information meta-learning, we use a parameter \(>0\) to constrain this optimum to lie in a subset \(_{}^{}\). Formally, we fix a point \(_{1}^{}\) to be the "center"--e.g. \(_{1}=_{d}/d\) when \(\) is the \(d\)-simplex \(\)--and define the projection \(_{}()=_{1}+- _{1}}{1+}\) mapping from \(\) to \(_{}\). For example, \(_{}()=(1-) +_{d}/d\) on the simplex. This projection allows us to handle regularizers \(\) that diverge near the boundary, but also introduces \(\)-dependent error terms. In the BLO case it also forces us to tune \(\) itself, as initializing too close to the boundary leads to unbounded regret while initializing too far away does not take advantage of task-similarity. Thus the general upper bounds of interest are the following functions of the initialization \(\), the step-size \(>0\), and a third parameter \(\) that is either \(\) or \(\), depending on the setting (MAB or BLO):

\[U_{t}(,,)=(_{}(}_{t})\|)}{}+ g()m+f()m\] (3)

Here \(B_{}\) is the Bregman divergence of \(_{}\) while \(g() 1\) and \(f() 0\) are tunable constants. We overload \(\) to be either \(\) or \(\) for notational simplicity, as we will not tune them simultaneously; if \(=\) (for MAB) then \(c_{}()=_{1}+-_{1}}{1+}\) for fixed \(\), while if \(=\) (for BLO) then \(B_{}\) is the Bregman divergence of a fixed \(\). The reason to optimize this sequence of upper bounds \(U_{t}\) is because they directly bound the task-averaged regret while being no worse than the worst-case single-task regret. Furthermore, an average over Bregman divergences is minimized at the average \(}}}=_{t=1}^{T}}_{t}\), where it attains the value \(_{}^{2}=_{t=1}^{T}_{}(_{ }(}_{t}))-_{}(_{}(}}}))\) (c.f. Claim A.1). We will show that this quantity leads to intuitive and interpretable notions of task-similarity in all the applications we study.

### A meta-algorithm for tuning bandit algorithms

To learn these functions \(U_{t}(,,)\)--and thus to meta-learn \(_{,}()\)--our meta-algorithm sets \(\) to be the projection \(_{}\) of the mean of the estimated optima--i.e. follow-the-leader (FTL) over the Bregman divergences in (3)--while simultaneously setting \(\) via EWOO and \(\) via discrete multiplicative weights (MW). We choose FTL, EWOO, and MW because each is well-suited to the way \(U_{t}\) depends on \(\), \(\), and \(\), respectively. First, the only effect of \(\) on \(U_{t}\) is via the Bregmandivergence \(B_{}(_{}(}_{t})||)\), over which FTL attains logarithmic regret . For \(\), \(U_{t}\) is exp-concave on \(>0\) so long as the first term is nonzero, but it is also non-Lipschitz; the EWOO algorithm is one of the few methods with logarithmic regret on exp-concave losses without a dependence on the Lipschitz constant , and we ensure the first term is nonzero by _regularizing_ the upper bounds as follows for some \(>0\) and \(D_{}^{2}=_{,_{}}B_{}( ||)\):

\[U_{t}^{()}(,,)=(_{}( }_{t})||)+^{2}D_{}^{2}}{}+ g( )m+f()m\] (4)

Note that this function is fully defined after obtaining \(}_{t}\) by running OMD on task \(t\), which allows us to use full-information MW to tune \(\) across the grid \(_{k}\). Showing low regret w.r.t. any \(_{k}\) then just requires sufficiently large \(k\) and Lipschitzness of \(U_{t}\) w.r.t. \(\). Combining all three algorithms together thus yields the guarantee in Theorem 2.1, which is our main structural result. It implies a generic approach for obtaining meta-learning algorithms by (1) bounding the task-averaged regret by an average of functions of the form \(U_{t}\), (2) applying the theorem to obtain a new bound \(o_{T}(1)+_{,}_{}^{2}}{}+ g()m+ f()m\), and (3) bounding the estimated task-similarity \(_{}^{2}\) by an interpretable quantity. Crucially, since we can choose any \(>0\), the asymptotic regret is always as good as the worst-case guarantee for running the base-learner separately on each task.

**Theorem 2.1** (c.f. Thm. A.1).: _Suppose \(_{1}=_{}_{}()\ \ \) and let \(D\), \(M\), \(F\), and \(S\) be maxima over \(\) of \(D_{}\), \(D_{}\), \(f()\), and \(\|^{2}_{}\|_{2}\), respectively. For each \((0,1)\) we can set \(\), \(\), \(\), and \(\) s.t. the expected average of the losses \(U_{t}(_{_{t}}(_{t}),_{t}(_{t}),_{t})\) of Algorithm 1 is at most_

\[_{,>0}_{}^{2}}{}+ g ()m+f()m+}(+Fm}{}+}{k}+T}+\{D^{2}}{ }, M\}+)\] (5)

_Here \(_{}^{2}=_{t=1}^{T}_{}(_{ }(}_{t}))-_{}(_{}(}))\) and \(L_{}\) bounds the Lipschitz constant w.r.t. \(\) at \(_{}^{2}/+ g()m+f()m\). The same bound plus \((M/+Fm)}\) holds w.p. \( 1-\)._

We keep details of the dependence on \(S\) and other constants as they are important in applying this result, but in most cases setting \(=}\) yields \(}(T^{})\) regret. While a slow rate, the losses \(U_{t}\) are non-Lipschitz and non-convex in-general, and learning them allows us to tune \(\) over user-specified intervals and \(\) over all positive numbers, which will be crucial later. At the same time, this tuning is what leads to the slow rate, as without tuning (\(k=1\), \(L_{}=0\)) the same \(\) yields \(}()\) regret. Lastly, while we focus on learning guarantees, we note that Algorithm 1 is reasonably efficient, requiring a \(2k\) single-dimensional integrals per task; this is discussed in more detail in Section A.3.

## 3 Multi-armed bandits

We now turn to our first application: the multi-armed bandit problem, where at each round \(i\) of task \(t\) we take action \(a_{t,i}[d]\) and observe loss \(_{t,i}(a_{t,i})\). As we are sampling actions from distributions \(=\) on the \(k\)-simplex, the inner product \(_{t,i},_{t,i}\) is the expected loss and the optimal arm \(_{t}\) on task \(t\) can be encoded as a vector \(}_{t}\) s.t. \(}_{t}(a)=1_{a=_{t}}\).

We use as a base-learner a generalization of Exp3 that uses the negative Tsallis entropy \(_{}()=^{d}^{}(a)}{1-}\) for some \((0,1]\) as the regularizer; this improves regret from Exp3's \(()\) to the optimal \(()\). Note that \(-_{}\) is the Shannon entropy in the limit \( 1\) and its Bregman divergence \(B_{}(||)\) is non-convex in the second argument. As the Tsallis entropy is non-Lipschitz at the simplex boundary, which is where the estimated and true optima \(}_{t}\) and \(}_{t}\) lie, we will project them using \(_{}()=(1-) +_{d}/d\) to the set \(_{}=\{: _{a}(a)/d\}\). We denote the resulting vectors using the superscript \(()\), e.g. \(}_{t}^{()}=_{}(}_{t})\), and also use \(^{()}=_{}\) to denote the constrained simplex. For MAB we also study two base-learners: (1) **implicit exploration** and (2) **guaranteed exploration**. The former uses low-variance loss _under_-estimators \(_{t,i}(a)=(a)1_{_{t,i}=a}}{_{t,i}(a)+ }\) for \(>0\), where \(_{t,i}(a)\) is the probability of sampling \(a\) on task \(t\) round \(i\), to enable high probability bounds . On the other hand, **guaranteed exploration** uses unbiased loss estimators (i.e. \(=0\)) but constrains the action space to \(^{()}\), which we will use to adapt to a task-similarity determined by the _true_ optima-in-hindsight.

### Adapting to low estimated entropy with high probability using implicit exploration

In our first setting, the base-learner runs \(_{_{t},_{t}}(_{t,1})\) on \(\)-regularized estimators with Tsallis regularizer \(_{_{t}}\), step-size \(_{t}\), and initialization \(_{t,1}^{()}\). Standard OMD analysis combined with implicit exploration analysis  shows (44) that the task-averaged regret is bounded w.h.p. by

\[(+ d)m+}(}{ T} )+_{t=1}^{T}}(}_ {t}^{()}||_{t,1})}{_{t}}+d^{_{t}}m }{_{t}}\] (6)

The summands have the desired form of \(U_{t}(_{t,1},_{t},_{t})\), so by Theorem 2.1 we can bound their average by

\[_{[,],>0}_{ }^{2}}{}+m}{}+}( }{k}+)^{2-}}{ T}+(+}+T})d )\] (7)

where \(_{}^{2}=_{t=1}^{T}_{}(}_{t }^{()})-_{}(}^{()})\) is the average difference in Tsallis entropies between the (\(\)-constrained) estimated optima \(}_{t}\) and their empirical distribution \(}=_{t=1}^{T}}_{t}\), while \(L_{}\) is the Lipschitz constant of \(_{}^{2}}{}+m}{}\) w.r.t. \([,]\). The specific instantiation of Algorithm 1 that (7) holds for is to do the following at each time \(t\):

\[ 1.&_{t}(_{t})_{k}[,]\\ 2.&_{_{t},_{t}} _{t,1}=_{s<t}}_{t}^{( )}=_{d}+ _{s<t}}_{t}\\ 3.&_{k}(}_{t}^{()}||_{t,1})+^{2}D _{}^{2}}{}+m}{},D_{}^{2}=-1}{1-}\\ 4.&_{t+1}(}_{t}^{()}||_{t,1})}{}+ m}{}\] (8)

The final guarantee for this procedure, given in full in Theorem B.1, follows by two properties of the Tsallis entropy \(-_{}\): (1) its Lipschitzness w.r.t. \(\) (c.f. Lem B.1) and (2) the fact that \(_{}^{2}\) is bounded by the entropy \(_{}=-_{}(})\) of the empirical distribution of estimated optima (c.f. Lem B.2), which yields our first notion of task-similarity: _multi-armed bandit tasks are similar if the empirical distribution of their (estimated) optimal arms has low entropy_.

We exemplify the implications of Theorem B.1 in Corollary 3.1, where we consider three regimes of the lower bound \(\) on the entropy parameter: \(=1\), i.e. always using Exp3; \(=1/2\), which corresponds to the optimal worst-case setting ; and \(=1/ d\), below which the OMD regret-upper-bound always worsens (and so it does not make sense to try \(<1/ d\)).

**Corollary 3.1** (c.f. Cors. B.1, B.2, and B.3).: _Suppose \(=1\) and we set the initialization, step-size, and entropy parameter of Tsallis OMD with implicit exploration via Algorithm 1 as in Theorem B.1._

1. _If_ \(=1\) _and_ \(T}{m}\) _we can ensure_ \(_{t=1}^{T}_{i=1}^{m}_{t,i}(_{t,i })-_{t,i}(}_{t}) 2_{1}dm}+} (_{3}m^{2}}{})\) _w.h.p._
2. _If_ \(=\) _and_ \(T}{m}\) _we can set_ \(k=\) _and ensure w.h.p. that task-averaged regret is_ \[_{[,1]}2_{}d^{}m/}+}(m^{5/7}}{T^{2/7}}+}{})\] (9)
3. _If_ \(=\) _and_ \(T}{m}\) _we can set_ \(k=\) _and ensure w.h.p. that task-averaged regret is_ \[_{(0,1]}2_{}d^{}m/}+} (m^{3/4}+d}{})\] (10)

In all three settings, as \(T\) the regret scales directly with the entropy of the estimated optima-in-hindsight, which is small if most tasks are estimated to be solved by one of a few arms and large if all arms are used roughly equally. Corollary 3.1 demonstrates the importance of tuning \(\): even if tasks are dissimilar, we asymptotically recover the worst-case optimal guarantee \(()\) in cases two and three because the entropy is at most \(}{1-}\). On the other hand, if a constant \(s d\) actions are always minimizers, i.e. the empirical distribution \(}\) is \(s\)-sparse, then the last bound (10)implies that Algorithm 1 can achieve task-averaged regret \(o_{T}(md)+()\). At the same time, this tuning is costly, with the last two results having an extra \(}(}{}T})\) term because of it. Furthermore, the bound of \(=\) has a slightly better dependence on \(d\), \(m\), and \(T\) compared to that of \(=\) due to the \(()^{-}\) term in the bound (7) returned for MAB by our structural result.

We can compare the \(s\)-sparse result to Azizi et al. , who achieve task-averaged regret \(}(m/+)\) for _stochastic_ MAB. Despite our adversarial setting and no stipulations on how tasks are related, our bounds are asymptotically comparable if the estimated and true optima are roughly equivalent (ignoring their \(()\)-factor), as we also have \(}()\) average regret as \(T\). Their rate in the number of tasks is better, but at a cost of runtime exponential in \(s\). Apart from generality, we believe a great strength of our results is their adaptiveness; unlike Azizi et al. , we do not need to know how many optimal arms there are to adapt to there being few of them.

### Adapting to the entropy of the true optima-in-hindsight using guaranteed exploration

While the entropy of estimated optima-in-hindsight may be useful in some cases where we wish to actually _compute_ the task-similarity, it is otherwise generally more desirable to adapt to an intrinsic and algorithm-independent measure, e.g. the entropy of the _true_ optima-in-hindsight. However, doing so is difficult without further assumptions, as the optima are both hard to identify and the measure itself may not be fully defined in case of ties. Thus in this section we focus on the setting where we have a nonzero performance gap \(>0\) between the best and second-best arms:

**Assumption 3.1**.: _For some \(>0\) and all tasks \(t[T]\), \(_{i=1}^{m}_{t,i}(a)-_{t,i}(_{t})\; \;a_{t}\)._

This assumption is common in the best-arm identification literature [28; 1], which we adapt to show that the estimated optimal arms match the true optima, and thus so do their entropies. To do so, we switch to _unbiased_ loss estimators, i.e. \(=0\), and control their variance by lower-bounding the probability of selecting an arm to be at least \(\); this can alternatively be expressed as running OMD using the regularizer \(_{}+I_{^{()}}\), where for any \(^{d}\) the function \(I_{}()=0\) if \(\) and \(\) otherwise. Guaranteed exploration allows us extend the analysis of Abbasi-Yadkori et al.  to show that the estimated arm is optimal w.h.p.:

**Lemma 3.1** (c.f. Lem C.1).: _Suppose for \(>0\) and any \((0,1]\) we run OMD on task \(t[T]\) with regularizer \(_{}+I_{^{()}}\). If \(m=(})\) then \(}_{t}=}_{t}\) w.p. \( 1-d(-(^{2}m/d))\)._

However, the constraint that the probabilities are at least \(\) does lead to \( m\) additional error on each task, with the upper bound on the task-averaged expected regret becoming

\[_{t=1}^{T}_{i=1}^{m}_{t,i}(a_{t,i})-_{t,i }(_{t}) m+_{t=1}^{T}B_{ _{t}}(}_{t}^{()}||_{t,1})}{_{t}} +d^{_{t}}m}{_{t}}\] (11)

Moreover, we will no longer set \(=o_{T}(1)\), as this would require \(m\) to be _increasing_ in \(T\) for the best-arm identification result of Lemma C.1 to hold. Thus, unlike in the previous section, our results will contain "fast" terms--terms in the task-averaged regret that are \(o()\) but not decreasing in \(T\) nor affected by the task-similarity. They will still improve upon the \(()\) MAB lower bound if tasks are similar, but the task-averaged regret will not converge to zero as \(T\) if the tasks are identical.

Nevertheless, the tuning-dependent component of the upper bounds in (11) has the appropriate form for our structural result--in fact we can use the same meta-algorithm (8) as for implicit exploration--and so we can again apply Theorem 2.1 to get a bound on the task-averaged regret in terms of the average difference \(_{}^{2}=_{t=1}^{T}_{}(}_{t} ^{()})-_{}(}^{()})\) of the entropies of the \(\)-constrained estimated task-optima \(}_{t}^{()}\) and their mean \(}^{()}\). The easiest way to apply Lemma C.1 to bound \(_{}^{2}\) in terms of \(H_{}=_{t=1}^{T}_{}(}_{t})-_ {}(})\) is via union bound on all \(T\) tasks to show that \(}_{t}=}_{t}\;\;t\; 1-dT(- (^{2}m/d))\); however, setting a constant failure probability leads to \(m\) growing, albeit only logarithmically, in \(T\). Instead, by analyzing the worst-case best-arm identification probabilities, we show in Lemma C.2 that the expectation of \(_{}^{2}\) is bounded by \(H_{}+3-1}{1-}(-m}{28d})\) without resorting to \(m=_{T}(1)\). Assuming \(m}}\) is enough (69) to bound the second term by \(\). Then the final result (c.f. Thm. C.1) bounds the expected task-averaged regret as follows (ignoring terms that become \(o_{T}(1)\) after setting \(\) and \(k\)):\[ m+_{[,],>0}()}{}+m}{} h_{} ()=H_{}+& m}}\\ -1}{1-}&\] (12)

If the gap \(\) is known and sufficiently large, then we can set \(=(m})\) to obtain an asymptotic task-averaged regret that scales only with the entropy \(H_{}\) and a fast term that is logarithmic in \(m\):

**Corollary 3.2** (c.f. Cor. C.3).: _Suppose we set the initialization, step-size, and entropy parameter of Tsallis OMD with guaranteed exploration via Algorithm 1 as specified in Theorem C.1. If \([,]=[,1]\) and \(m}}\), then setting \(=(m})\), \(=}\), and \(k=^{2}mT\) ensures that the expected task-averaged regret is at most_

\[_{(0,1]}2d^{}m/}+} (}+}m^{}}{} +}m^{}}{T^{}}+m^{3} }{T})\] (13)

Knowing the gap \(\) is a strong assumption, as ideally we could set \(\) without it. Note that if \(=(})\) for some \(p(0,1)\) then the condition \(m}}\) only fails if \(m()\), i.e. for gap decreasing in \(m\). We can use this together with the fact that minimizing over \(\) and \(\) in our bound allows us to replace them with any value, even a gap-dependent one, to derive a gap-_independent_ setting of \(\) that ensures a task-similarity-adaptive bound when \(\) is not too small and falls back to the worst-case optimal guarantee otherwise. Specifically, for indicator \(_{}=1_{m}}}\), setting \(=(()}{d^{2}m/}})\) in (12) and using \(=\) if the condition \(_{}\) fails yields asymptotic regret at most

\[ m+_{(0,1]}\!\!(\!_{} {d^{}m}{}}+(1-_{})\!\!\!)\!  m+}\!(\!\{_{(0,1]} d^{}m}{}}+}, \!)\!\!)\] (14)

Thus setting \(=(/m^{})\) yields the desired dependence on the entropy \(H_{}\) and a fast term in \(m\):

**Corollary 3.3** (c.f. Cor. C.4).: _In the setting of Corollary 3.2 but with \(m=(d^{})\) and unknown \(\), using \(=(/m^{})\) ensures expected task-averaged regret at most_

\[\{_{(0,1]}2d^{}m/}+}(}m^{}}{}),8 {dm}\}+}(}m^{}}{ }+}m^{}}{T^{}}+m^{}}{T})\] (15)

While not logarithmic, the gap-dependent term is still \(o()\), and moreover the asymptotic regret is no worse than the worst-case optimal \(()\). Note that the latter is only needed if \(=o(1/)\).

The main improvement in this section is in using the entropy of the true optima, which can be much smaller than that of the estimated optima if there are a few good arms but large noise. Our use of the gap assumption for this seems difficult to avoid for this notion of task-similarity. We can also compare to Corollary 3.1 (10), which did not require \(>0\) and had no fast terms but had a worse rate in \(T\); in contrast, the \((})\) rates above match that of the closest stochastic bandit result . As before, for \(s d\) "good" arms we obtain \(()\) asymptotic regret, assuming the gap is not too small. Finally, we can also compare to the classic shifting regret bound for Exp3.S , which translated to task-averaged regret is \(()\). This is worse than even running OMD separately on each task, albeit under weaker assumptions (not knowing task boundaries). It also cannot take advantage of repeated optimal arms, e.g. the case of \(s d\) good arms.

### Adapting to entropic task similarity implies robustness to outliers

While we considered mainly the \(s\)-sparse setting as a way of exemplifying our results and comparing to other work such as Azizi et al. , the fact that our approach can adapt to the Tsallis entropy \(_{}H_{}\) of the optimal arms implies meaningful guarantees for any low-entropy distribution over the optimal arms, not just sparsely-supported ones. One way to illustrate the importance of this is through an analysis of robustness to outlier tasks. Specifically, suppose that the \(s\)-sparsity assumption--that optima \(_{t}\) lie in a subset of \([T]\) of size \(s d\)--only holds for all but \((T^{p})\) of the tasks \(t[T]\), where \(p[0,1)\). Then the best we can do using an asymptotic bound of \(}()\)--e.g. that of Azizi et al.

 in the stochastic case or from naively applying \(_{(0,1]}H_{}d^{3}m/ esm d\) to any of our previous results--is to substitute \(s+T^{p}\) instead of \(s\), which will only improve over the single-task bound if \(d=(T^{p})\), i.e. in the regime where the number of arms increases with the number of tasks.

However, our notion of task-similarity allows us to do much better, as we can show (c.f. Prop. D.1) that in the same setting \(H_{}=(s+}{T^{(1-p)}})\) for any \([,]\). Substituting this result into e.g. Corollary 3.3 yields the same asymptotic result of \(()\), although the rate in \(T\) is a very slow \((/T^{})\). This demonstrates how our entropic notion of task-similarity simultaneously yields strong results in the \(s\)-sparse setting and is meaningful in more general settings.

## 4 Bandit linear optimization

Our last application is bandit linear optimization, in which at task \(t\) round \(i\) we play \(_{t,i}\) in some convex \(^{d}\) and observe loss \(_{t,i},_{t,i}[-1,1]\). We will again use a variant of mirror descent, using a **self-concordant barrier** for \(\) and the specialized loss estimators of Abernethy et al. [2, Alg. 1]. More information on such regularizers can be found in the literature on interior point methods . We pick this class of algorithms because of their optimal dependence on the number of rounds and their applicability to any convex domain \(\) via specific barriers \(\), which will yield interesting notions of task-similarity. Our ability to handle non-smooth regularizers via the structural result (Thm. 2.1) is even more important here, as barriers are infinite at the boundaries. Indeed, we will _not_ learn a \(\) parameterizing the regularizer and instead focus on tuning a boundary offset \(>0\). Here we make use of notation from Section 2, where \(_{}\) maps points in \(\) to a subset \(_{}\) defined by the Minkowski function (c.f. Def. E.1) centered at \(_{1}=_{}()\).

From Abernethy et al.  we have an upper bound on the expected task-averaged regret of their algorithm run from initializations \(_{t,1}^{}\) with step-sizes \(_{t}>0\) and offsets \(_{t}>0\):

\[_{t=1}^{T}_{i=1}^{m}_{t,i}, _{t,i}-}_{t}_{t=1}^{T}B(_{_{t}}(}_{t})\|_{t,1})}{ _{t}}+(32_{t}d^{2}+_{t})m\] (16)

We can show (88) that \(D_{}^{2}=_{,_{}}B (||)K}}{}\), where \(\) is the self-concordance constant of \(\) and \(S_{1}=||^{2}(_{1})||_{2}\) is the spectral norm of its Hessian at the center \(_{1}\) of \(\). Restricting to tuning \([,1]\)--which is enough to obtain constant task-averaged regret above if the estimated optima \(}_{t}\) are identical--we can now apply Algorithm 1 via the following instantiation:

* sample \(_{t}\) via the MW distribution \((_{t})\) over the discretization \(_{k}\) of \([,1]\)
* run \(_{_{t},_{t}}\) using the initialization \(_{t,1}=_{s<t}_{_{t}}(}_{t})=_{1}+}_{t}-_{1}}{(1+_{t})(t-1)}\) (FTL)
* update EWOO at each \(_{k}\) with loss \(_{}(}_{t})||_{t,1})+ ^{2}D_{}^{2}}{}+32 d^{2}\) for \(D_{}^{2}=K}}{}\)
* update \(_{t+1}\) using multiplicative weights with expert losses \(_{}(}_{t})||_{t,1})}{ }+ m\)

Note the similarity to the MAB case (8), with the difference being the upper bound passed to EWOO and MW. Our structural result bounds the expected task-averaged regret as follows (c.f. Thm. E.1):

\[_{[,1],>0}_{ }^{2}}{}+(32 d^{2}+)m+}( }{T}+}{}++m\{}{},d\}+}+T})\] (18)

For \(=_{T}(1)\) and \(k=_{T}(1)\) this becomes \(o_{T}((m))+_{[,1],>0} _{}^{2}}{}+32 d^{2}m+ m\), where \(_{}^{2}=_{t=1}^{T}(_{ }(}_{t})-(_{}(}_{t})\). Then by tuning \(\) we get an asymptotic (\(T\)) regret of \(4d_{}+ m\) for any \([,1]\). Our analysis removes the explicit dependence on \(\) that appears in the single-task regret ; as an example, \(\) equals the number of inequalities defining a polytope \(\), as in the bandit shortest-path application below.

The remaining challenge is to interpret \(_{}^{2}\), which as we did for MAB we do via specific examples, in this case concrete action domains \(\). Our first example is for BLO over the unit sphere \(=\{^{d}:\|\|_{2} 1\}\) using the appropriate log-barrier regularizer \(()=-(1-\|\|_{2}^{2})\):

**Corollary 4.1** (c.f. Cor. E.1).: _For BLO on the sphere, Algorithm 1 has expected task-averaged regret_

\[}(}}{T^{}}+})+_{[,1]}4d\|}\|_{2}^{2}}{2+^{2}} )}+ m\] (19)

The bound above is decreasing in \(\|}\|_{2}^{2}\), the expected squared norm of the average of the estimated optima \(}_{t}\). We thus say that _bandit linear optimization tasks over the sphere are similar if the norm of the empirical mean of their (estimated) optima is large_. This makes intuitive sense: if the tasks' optima are uniformly distributed, we should expect \(\|}\|_{2}^{2}\) to be small, even decreasing in \(d\). On the other hand, in the degenerate case where the estimated optima \(}_{t}\) are the same across all tasks \(t[T]\), we have \(\|}\|_{2}^{2}=1\), so the asymptotic task-averaged regret is 1 because we can use \(=\). Perhaps slightly more realistically, if it is \(}\)-away from 1 for some power \(p\) then setting \(=}\) can remove the logarithmic dependence on \(m\). These two regimes illustrate the importance of tuning \(\).

As a last application, we apply our meta-BLO result to the shortest-path problem in online optimization [50; 30]. In its bandit variant [8; 17], at each step \(i=1,,m\) the player must choose a path \(p_{i}\) from a fixed source \(u V\) to a fixed sink \(v V\) in a directed graph \(G(V,E)\). At the same time the adversary chooses edge-weights \(_{i}^{|E|}\) and the player suffers the sum \(_{ p_{i}}_{i}(e)\) of the weights in their chosen path \(p_{t}\). This can be relaxed as BLO over vectors \(\) in a set \(^{|E|}\) defined by a set \(\) of \((|E|)\) linear constraints \((,b)\)\(, b\) enforcing flows from \(u\) to \(v\); \(u\) to \(v\) paths can be sampled from any \(\) in an unbiased manner [2, Proposition 1]. On a single-instance, applying the BLO method of Abernethy et al.  ensures \((|E|^{})\) regret on this problem.

In the multi-instance setting, comprising a sequence \(t=1,,T\) of shortest path instances with \(m\) adversarial edge-weight vectors \(_{t,i}\) each, we can attempt to achieve better performance by tuning the same method across instances. Notably, we can view this as the problem of learning predictions  in the algorithms with predictions paradigm from beyond-worst-case analysis , with the OMD initialization on each instance being effectively a prediction of its optimal path. Our meta-learner then has the following average performance across bandit shortest-path instances:

**Corollary 4.2** (c.f. Cor. E.2).: _For multi-task bandit online shortest path, Algorithm 1 with regularizer \(()=-_{,b}(b-, )\) attains the following expected average regret across instances_

\[}(m^{}}{T^{}}+}m^{}}{})+_{[ ,1]}4|E|,b} (_{t=1}^{T}b-,_{ }(}_{t})}{^{T}b- ,_{}(}_{t})}})}+  m\] (20)

Here the asymptotic regret scales with the sum across all constraints \(,b\) of the log of the ratio between the arithmetic and geometric means across tasks of the distances \(b-,_{}(}_{t})\) from the estimated optimum flow \(_{}(}_{t})\) to the constraint boundary. As it is difficult to separate the effect of the offset \(\), we do not state an explicit task-similarity measure like in our previous settings. Nevertheless, since the arithmetic and geometric means are equal exactly when all entries are equal--and otherwise the former is larger--the bound does show that regret is small when the estimated optimal flows \(}_{t}\) for each task are at similar distances from the constraints, i.e. the boundaries of the polytope. Indeed, just as on the sphere, if the estimated optima are all the same then setting \(=\) again yields constant averaged regret.

## 5 Conclusion and limitations

We develop and apply a meta-algorithm for learning to initialize and tune bandit algorithms, obtaining task-averaged regret guarantees for both multi-armed and linear bandits that depend on natural, setting-specific notions of task similarity. For MAB, we meta-learn the initialization, step-size, and entropy parameter of Tsallis-entropic OMD and show good performance if the entropy of the optimal arms is small. For BLO, we use OMD with self-concordant regularizers and meta-learn the initialization, step-size, and boundary-offset, yielding interesting domain-specific task-similarity measures. Some natural directions for future work involve overcoming some limitations of our results: can we adapt to a notion of task-similarity that depends on the true optima without assuming a gap for MAB, or at all for BLO? Alternatively, can we design meta-learning algorithms that adapt to both stochastic and adversarial bandits, i.e. a "best-of-both-worlds" guarantee? Beyond this, one could explore other partial information settings, such as contextual bandits or bandit convex optimization.