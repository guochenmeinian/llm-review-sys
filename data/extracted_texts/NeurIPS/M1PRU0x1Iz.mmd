# FedAvP: Augment Local Data via Shared Policy in Federated Learning

Minui Hong Junhyeog Yun Insu Jeon Gunhee Kim

Seoul National University, Seoul, South Korea

{alsdml123,junhyeog,gunhee}@snu.ac.kr

{insuj3on}@gmail.com

###### Abstract

Federated Learning (FL) allows multiple clients to collaboratively train models without directly sharing their private data. While various data augmentation techniques have been actively studied in the FL environment, most of these methods share input-level or feature-level data information over communication, posing potential privacy leakage. In response to this challenge, we introduce a federated data augmentation algorithm named FedAvP that shares only the augmentation policies, not the data-related information. For data security and efficient policy search, we interpret the policy loss as a meta update loss in standard FL algorithms and utilize the first-order gradient information to further enhance privacy and reduce communication costs. Moreover, we propose a meta-learning method to search for adaptive personalized policies tailored to heterogeneous clients. Our approach outperforms existing best performing augmentation policy search methods and federated data augmentation methods, in the benchmarks for heterogeneous FL.

## 1 Introduction

Federated Learning (FL) is a collaborative learning approach that allows multiple clients to learn without sharing their private information . A central server coordinates the training process across multiple devices and aggregates the locally trained models into a global one; thus reducing the communication cost of exchanging raw data and mitigating the risk of privacy leakage associated with data sharing .

However, the limited accessibility of data in FL still poses many challenges, such as insufficient training data and local data bias. To address these challenges, there has been a growing interest in federated data augmentation techniques . They aim to increase the diversity and volume of data available at each client, thereby improving the overall robustness and performance of the federated models. For example, FedMix  improves performance and privacy by averaging multiple images to facilitate data mixup among clients. Similarly, FedFA  utilizes feature statistics to mitigate local data biases, to improve model generalization. Despite their benefits, these methods often apply the sharing of input-level  or feature-level  information. Such information sharing poses additional privacy concerns since malicious attackers could potentially reconstruct original data by applying gradient matching loss  on the additional input and feature information.

We propose a novel federated data augmentation algorithm named FedAvP (**A**ugmet Local Data via Shared **P**olicy in Federated Learning), which shares only the _augmentation policies_ during training. Thus, each client does not need to share its own data or data-related information directly but obtains collective knowledge on how to augment the dataset at local learning. Previously, AutoAugment  utilizes reinforcement learning (RL) to automatically find the optimal data augmentation policy for a target dataset. While AutoAugment requires extensive GPU resources, more efficient and faster policy search has been studied . However, these methods are not designed for FL environments butthey perform policy search for public batch datasets. The data scarcity and heterogeneity in FL could fail these standard policy search frameworks.

To improve the robustness and generalization of model training in the heterogeneous FL setting, we first introduce a Federated Meta-Policy Loss (FMPL) specifically designed to compute a gradient of augmentation policy that updates a shared data augmentation policy for each client's unique environment. Our approach guides the policy gradient to account for the effects of data augmentation on the unseen local data. The policy gradient utilizes higher-order information; the impact of the data augmentation is estimated by its effect on the validation loss observed after a few gradient descent steps with the initial augmented data. However, computing the direct meta-policy gradient in FL requires an additional communication step between the server and clients. To bypass this, we also develop an alternative meta-policy search method that utilizes a first-order approximation. We further demonstrate that the adaptive policy search technique can adapt to heterogeneous data distributions among clients in the FL environment.

In the experiments, our FedAvP demonstrates superior performance on CIFAR-10/100 , SVHN , and FEMNIST  datasets within an FL context, compared to existing federated learning algorithms, including FedAvg , FedProx , FedDyn , FedExP , and federated data augmentation algorithms, including FedGen , FedMix , and FedFA . Moreover, to further leverage the potential performance of these algorithms, we also conducted experiments applying data augmentation techniques such as RandAugment  and TrivialAugment  to these algorithms for comparison. We also evaluate our algorithm in environments where data is non-i.i.d. with heterogeneous clients . We further compare the effectiveness of the utility of sharing this policy across clients for searching, in contrast to conducting a local policy search.

Our primary contributions are as follows.

1. We propose FedAvP(Augment Local Data via Shared Policy in Federated Learning) as the first algorithm in federated learning that facilitates shared augmentation policies among clients for federated policy search, to the best of our knowledge.
2. We introduce the federated meta-policy loss for effective policy search, and further propose a first-order approximation to this loss to enhance privacy and reduce communication costs.
3. Enabling meta-learning, our algorithm allows for rapid adaptation of a personalized policy by each client, addressing the challenge of highly heterogeneous data distributions among clients in the FL environment.

## 2 Related Work

**Automated Data Augmentation**. AutoAugment  utilizes RL to find an optimal data augmentation policy for a target dataset automatically. FastAA  proposes a more efficient search strategy by training small NNs in parallel without iterative training, using the density matching method. RandAugment  suggests a simplified search method composed of two hyper-parameters, which find the augmentation policy without a separate search process. TrivialAugment  further simplifies the algorithm and applies a single augmentation to each image as a parameter-free method. MetaAugment  proposes a sample-aware augmentation policy network to capture the variability of training samples more accurately than previous dataset-based search methods. Deep AutoAugment  proposes a fully automated search method that builds a multi-layer data augmentation pipeline from scratch by stacking augmentation layers. All of these recent data augmentation methods were developed under the assumption that all training data is accessible on a server. This assumption is invalid in FL since data privacy is a significant concern and it is challenging to tune these algorithms for each of the numerous heterogeneous local datasets. Therefore, our study aims to develop a new data augmentation policy search algorithm that takes into account the distributed FL process while preserving data security. RandAugment  and TrivialAugment  can be applied simultaneously to many clients in a federated learning environment, so we compared these methods in our experiments.

**Federated Data Augmentation**. The standard Federated Learning (FL) framework, such as FedAvg , typically performs iterative local model updates at each client and a global update at a server. Since the clients and server only communicate through the model parameters instead of raw data, it enables secured and decentralized learning . Despite its benefits, FL still has challengessuch as convergence degradation and model overfitting due to heterogeneity and sparsity among the data caused by the differences in client's actions and preferences . To address these challenges, federated data augmentation (FDA) employs a data augmentation approach instead of a model-centric approach. FedMix  applies Mixup  to FL, augmenting data by linear interpolation of two random training examples and their labels. For privacy reasons, FedMix transfers mixup data to the server by averaging multiple images from the local device. In FedGen , the server learns a lightweight generator to ensemble user information, which is then broadcasted to users to regulate local training using the learned knowledge. FedFA  assumes that the data distribution of the clients can be summarized by the statistics of the latent features (_i.e._, mean and standard deviation). This allows learning local models by regularizing the gradients of the latent representations, weighted by the variances of the feature statistics estimated from the entire client federation. StatMix  sends the mean and standard deviation information of local client images to the server and makes it available for learning for each client. These methods use input-level data (image) averaging or feature-level statistics to prevent direct data transfer. Unlike previous methods, our methodology focuses on transmitting only the policy information optimized for local datasets from each client. ATSPrivacy  has demonstrated that searching for transformation policies can also protect against reconstruction attacks in Federated Learning (FL), while preserving performance. We compare our algorithm with this approach in terms of both vulnerability to reconstruction attacks and performance in the experimental section

## 3 Approach: FedAvP

We introduce FedAvP (Augment Local Data via Shared Policy in Federated Learning), which performs data augmentation search by sharing policies among clients in a federated learning (FL) environment. Starting from the problem formulation (SS3.1), we address the challenges of heterogeneous clients with a proposal for adaptive policy search (SS3.2). Finally, we extend its applicability through integration with the FedAvg algorithm  and joint learning (SS3.3).

### Problem Formulation

Our approach is based on the idea of centralized FL ; it is not possible to share personal data neither between the server and clients nor among clients. This is different from traditional automated policy search algorithms, which find the optimal policy in a single batch dataset [15; 16; 20; 19; 18].

**Objective**. We begin with the standard FL algorithm, FedAvg , aiming to find augmentation policies that minimize a given objective function as follows:

\[_{w}_{k=1}^{K}_{k}(w;t_{p_{}}(D_{k} ^{})),_{p_{}}_{k=1}^{K}_{k}(w;D_{k}^{} ).\] (1)

\(K\) is the number of clients used to train the global model \(w\), and \(D_{k}\) is the local data of client \(k\). The transformation policy \(p_{}\) is used to augment the data denoted by \(t_{p_{}}(D_{k}^{})\) with coefficients \(_{k}\) satisfying \(_{k}=1\) and \(_{k} 0\). Assuming client \(k\) has \(n_{k}\) data samples, \(_{k}\) is defined as \(_{k}=}{n}\) where \(n=_{k}n_{k}\). Under a global policy assumption, all clients share the same \(p_{}\). Alternatively, if we assume that each client has its own transformation policy \(p_{^{}}^{}\), we represent \(p_{}^{}=\{p_{^{1}}^{},p_{^{2}}^{ },,p_{^{K}}^{}\}\).

**Search Space**. We use the augmentation space having a sequence of two operations, following the search space from previous studies on automated policy search [17; 19]. We examine a set of 17 operations in total, including {Identity, ShearX/Y, TranslateX/Y, Rotate, AutoContrast, Equalize, Solarize, Posterize, Contrast, Color, Brightness, Sharpness, RandFlip, RandCutout, RandCrop}. Each operation includes a random magnitude, rescaled and uniformly sampled from the normalized interval \(\). Assuming each of the \(K\) clients has individual policy, the entire search space consists of \(K\) joint distributions, each having a size of \(17 17\). For a global policy, we learn a single \(17 17\) dimensional joint distribution corresponding to the two operations. Specifically, \(p_{}\) has a value in \(\) using the sigmoid function on \(\). For augmentation sampling, we normalize the policy parameter vector \(p_{}\) whose sum to be one, and sample from a joint distribution with a regularization term \(\). Operation pairs \((op1,op2)\) are drawn from a mixed distribution:

\[(op1,op2)(1-)}{(p_{})}+ }\] (2)This sampling strategy utilizes a uniform probability across all operation pairs for a balanced exploration and exploitation, as adopted in previous work .

### Policy Optimization

To search the policy in a differentiable manner, we adopt the concept from meta-learning , where a model is trained on training data \(n\) times by SGD algorithm (\(n\) inner steps), followed by validation of one outer step. In our context, training is performed on augmented data, followed by evaluation on validation data [29; 19]. However, directly applying this concept of inner and outer steps to FL is quite challenging due to the added complexity of FL, which involves training local models and aggregating them to update the global model.

Since our goal is to optimize the global augmentation policy, we redefine the inner and outer steps from the FL perspective. In a standard FL setting, the server sends a global model to the clients for each round, which is trained with their local training data. Afterward, the aggregation process occurs on the server to update the global model. We can regard one round of local training and aggregation as _one inner step_. After \(r\) rounds, where the global model is updated \(r\) times, validating the final updated model on each client can be considered _one outer step_. We set \(r=1\), meaning validation occurs after each round.

**Federated Meta-Policy Loss**. In a single round of client updates, we first perform local training on each client and aggregate the local models to update the global weights, which are then redistributed to the clients for computing the validation loss. Figure 1 (a) illustrates this process. In each round, the initial weight for a client \(k\), denoted \(w_{_{0}^{k}}^{k}\), is set to the global weight \(w_{g_{r}}\). The local training consists of \(N\) iterations, with a batch size \(B\) of the training data \(D_{k,n}^{}\) at each iteration \(n\). A transformation according to the policy \(p_{_{n}^{k}}\) is applied to each data sample. The local loss at iteration \(n\) for client \(k\) is calculated as

\[=_{i=1}^{B}P_{_{n}^{k}}^{i}(w_{n}^{k };t_{p_{_{n}^{k}}}^{i}(D_{k,n}^{(i)})),\] (3)

where \(P_{_{n}^{k}}^{i}\) is the unnormalized probability \(p_{_{n}^{k}}(op1,op2)\) for the \(i\)-th transformation, when the sampled transformation \(t_{p_{_{k}^{k}}}^{i}\) is the operations \((op1,op2)\). This reweighting strategy is inspired by recent sample reweighting [29; 19]. Following the local updates with the augmentation policy \(p_{^{k}}\), we aggregate the results to obtain the new global weights \(w_{g_{r+1}}\), which are redistributed to the clients for validation loss assessment. At the start of round \(r\), after distributing the global weights \(w_{g_{r}}\) to

Figure 1: Overview of FedAvP. (a) The server sends global model parameters \(w_{g_{r}}\) and policy parameters \(_{g_{r}}\) to clients. Clients train local models with augmented data, and the server aggregates them to compute \(w_{g_{r+1}}\). Clients update policies on \(w_{g_{r+1}}\) using validation data, and the server aggregates these policies. (b) Clients update the model and policy parameters via first-order approximation. The server aggregates client updates to form the updated global model \(w_{g_{r+1}}\) and policy parameters \(_{g_{r+1}}\).

each participating client, the procedure for the federated meta-policy loss can be summarized as follows:

1. Each client \(k\) performs local updates by optimizing their local weights \(w_{n}^{k}\) using the augmented data with the local loss in Eq. (3). The updated local weights are then aggregated according to \(w_{g_{r+1}}=_{k}_{k}w_{N}^{k}\).
2. The aggregated global weights \(w_{g_{r+1}}\) are then sent back to the same clients.
3. The Federated Meta-Policy Loss (FMPL) is computed on each client's validation set. \[=_{D_{k}^{}}(w_{g_{r+1}}).\] (4)

For this loss, the gradient with respect to the augmentation policy \(p_{}\) is computed using backpropagation. Nonetheless, this approach presents two significant challenges. Firstly, it necessitates access to validation gradient information of other clients, which poses privacy concerns. Secondly, it requires revisiting the same clients for additional updates, thereby doubling the communication overhead.

**First-order Approximation**. To ensure security by preventing access to other clients' gradients and to reduce communication costs, we derive an approximation for the policy gradient with respect to \(_{n-1}^{k}\) at the local step \(n\) of the client \(k\) by a Taylor expansion, as in the following proposition. See Appendix B for more details.

**Proposition 1**.: _Consider the federated meta-policy loss derived from the updated weight \(w_{n}^{k}\) for client \(k\) at step \(n\) using a first-order Taylor expansion:_

\[_{D_{k}^{}}(w_{g_{r+1}})_{D_{k}^{}}(w_{n}^{k })+_{D_{k}^{}}(w_{n}^{k})^{T}(w_{g_{r}}-w_{n}^{k}).\] (5)

_When computing the policy gradient of the loss with respect to \(_{n-1}^{k}\), the first-order gradient approximation is_

\[^{}}(w_{n} ^{k})^{T}_{t_{p_{n-1}^{k}}(D_{k,n-1}^{})}(w_{n-1}^{k}))}{ _{n-1}^{k}},\] (6)

_where \(w_{n}^{k}=w_{g_{r}}-lr g_{w_{0}^{k}}^{}--lr g_{w_{n- 1}^{k}}^{}\) and \(_{k}\) is a coefficient proportional to the client's data size._

In Proposition 1, \(g_{w_{n}^{k}}^{}=_{t_{p_{_{n}^{k}}}(D_{k,n}^{ {min}})}(w_{n}^{k})\) is the gradient obtained from the local loss in Eq. (3) at step \(n\) for client \(k\). We calculate the gradient within the same client to prevent gradient leakage across clients. We optimize a policy that maximizes the inner product of the gradient obtained from sampling the validation data and the gradient obtained through augmentation using \(p_{_{n-1}^{k}}\). The validation data are not used separately; instead, they are replaced by sampling the next batch, inspired by Reptile . Proposition 1 also indicates that policy gradients can be computed concurrently with local updates. We utilize this approach to joint training (SS3.3), which enables simultaneous model and policy training.

**Gradient Clipping**. From the policy gradient of Eq. (6), it becomes evident that policy search aims to maximize the dot product of the gradients derived from both augmentation and validation data. This approach, however, can introduce a bias toward augmentations with larger gradient magnitudes due to the nature of the dot product. Inspired by , we mitigate the influence of gradient magnitude by gradient clipping . We apply gradient clipping to the gradients from both validation and augmentation data in Eq. (6) using a regularizer hyperparameter \(c\).

**Adaptive Policy Search**. Our first-order approximation computes policy gradients at each local update step \(n\) without aggregation. These gradients are then averaged across all steps for each client to update the policy after a single communication round. However, this method can slow the policy search, since it only permits one gradient descent update per communication round. Drawing inspiration from meta-learning , which quickly adapts to various tasks using neural network training, we propose an adaptive policy search. The augmentation policy is represented as a vector parameter \(p_{}=([_{1},_{2},,_{289}])\), where \([_{1},_{2},,_{289}]\) denotes an \(17 17\) joint distribution. We use a neural network comprising the following dense layers: one dummy embedding layer, two 100-dim hidden layers, and an output layer shaped to the \(17 17\) distribution size. We update our policy as done in Reptile [30; 35]. That is, the local policy updates on each client correspond to the inner steps in the Reptile, while the global policy updates on the server are analogous to the outer steps, as detailed in Algorithm 1. We train the policy neural network by increasing the dot-product between policy gradients on each client as follows:

\[_{g_{r+1}}_{g_{r}}-^{k}}_{j=0}^{n}L_{k,j}-_ {j=0}^{n}_{s=0}^{j-1} L_{k,j} L_{k,s},\] (7)

where \(L_{k,j}=_{D_{k,j}^{}}^{}(_{0}^{k})\) is the federated meta-policy loss in Eq. (4) computed on the client \(k\)'s \(j\)-th validation data batch using the global policy parameters \(_{0}^{k}\). \( L_{k,j} L_{k,s}\) is the dot-product between policy gradients on the client \(k\). See Appendix C for more details. This process enables the policy neural network to learn in a direction that enhances the dot-product between the policy gradients of each client, thereby facilitating efficient policy search and enabling personalized policy search. We incorporate this strategy in the joint training in SS3.3. An experiment result regarding this will be conducted in SS4.

``` Input: # of communication round \(R\), # of client \(N\), server policy learning rate \(\), client model learning rate \(\), client policy learning rate \(\), local steps \(E\), gradient clipping threshold \(c\), regularization term \(\). Initialize the global model parameter \(w_{g_{r}}\) and the global policy parameter \(_{g_{r}}\) for\(r=1,...,R\)do  Sample K clients from \(1,...,N\) clients for\(k=1,...,K\)do  Set \(w_{0}^{k}=w_{g_{r}}\) and \(_{0}^{k}=_{g_{r}}\) \(w_{k}^{k},_{e}^{k}(w_{0}^{k},_{0}^{k})\) \(w_{g_{r+1}}_{k}_{k}w_{k}^{k}\) \(_{k}_{k}(_{e}^{k}-_{g_{r}})\) \(_{g_{r+1}}_{g_{r}}+\) ```

**Algorithm 1** FedAvP: Joint Training

### Joint Training

As done in AutoAugment, one could employ a pretrained network to perform policy search [16; 18]. However, such methods require a separate training phase and present complications in FL environments, since training a pretrained network beforehand is cumbersome, and the separate phase is disadvantageous for parallel training. Note that we previously adopt an adaptive policy search that can simultaneously train the model and policy search. At each local update step \(n\), the weights \(w_{n+1}^{k}\) and gradients concurrently update the model and policy by comparing the gradient on validation data at \(w_{n+1}^{k}\).

Algorithm 1 illustrates this joint training of our FedAvP. It begins by initializing the global model parameter \(w_{g_{r}}\) and the policy parameter \(_{g_{r}}\), which are then sent to the clients from the server. Each client generates augmented data \(t_{p_{_{n}}}(D_{k,n}^{})\) using the policy parameter \(p_{_{n}}\) at every step \(n\) and updates the model accordingly. Subsequently, the gradient \(_{D_{k,n}^{}}(w_{n+1}^{k})\) is computed using the newly updated \(w_{n+1}^{k}\). Following Proposition 1, the policy parameter \(_{n}^{k}\) is updated to maximize the gradient on both validation and augmentation data \(_{t_{p_{_{n}}}(D_{k,n}^{})}(w_{n}^{k})\). This process of model and policy updates is repeated in every local update. Afterwards, the model and policy parameters from each client are aggregated at the server using \(_{k}\).

**Fast Update** One limitation of joint learning is that it requires backpropagation for the policy gradient at every step. To reduce the computation load on local clients, the policy can be updated periodically instead of at every local step \(n\), specifically when \(n==0\). In all our experiments, we set \(=5\). See the variant of the algorithm in Appendix A.3. We also reduced the hidden size of the policy neural network to two 25-dimensional hidden layers. In our experiments in SS4, we will compare the performance of the Fast Update.

## 4 Experiments

### Experimental setup

**Environments**. Previous FL studies have demonstrated that the standard algorithms are effective and converged when the data is i.i.d. [36; 37; 38]. To evaluate robust performance in non-i.i.d. data, we set up our experimental environment by distributing the CIFAR-10/100  and SVHN  datasets with different levels of data heterogeneity among clients. We assign the data to 130 clients based on a Dirichlet distribution with different hyperparameters of \(=[5.0,0.1]\), as done in pFL-Bench . The smaller \(\) is, the higher the degree of heterogeneity is. Among these clients, only 100 randomly selected clients participate in the training, while the remaining 30 are nominated as out-of-distribution (OOD) clients. In each communication round, only 10 clients are sampled. We employed a standard CNN model, consistent with those in previous studies [39; 40; 41], for the global model. Experiments involving the larger model and OOD clients are provided in Appendix A.2. For the FEMNIST dataset , we introduced variability in data size by distributing data based on the writers [8; 11]. Further environments details are provided in Appendix A. Our code is available at https://github.com/alsdml/FedAvP.

**Baselines**. For a comprehensive evaluation, we compared our method with state-of-the-art federated learning algorithms such as FedAvg , FedProx , FedDyn , FedExP , and federated data augmentation algorithms including FedGen , FedMix , and FedFA . To further explore the potential performance of these algorithms, we conducted experiments applying data augmentation techniques such as RandAugment  and TrivialAugment . We also compared the results with those using default augmentations (random crops and horizontal flipping). Additionally, for comparison with our proposed model, we included FedAvP (W/ Local Policy), which trains each

  
**Dataset** &  &  & SVHN & FEMNIST \\  & \(=5.0\) & \(=0.1\) & \(=5.0\) & \(=0.1\) & \\
**Method** & Test (\%) & Test (\%) & Test (\%) & Test (\%) & Test (\%) \\  FedAvg + Default & \(40.05\) & \(37.34\) & \(79.76\) & \(85.58\) & \(80.65\) \\  + RandAugment & \(47.29\) & \(43.60\) & \(82.82\) & \(84.84\) & \(79.40\) \\  + TrivialAugment & \(46.61\) & \(42.16\) & \(82.00\) & \(83.36\) & \(79.01\) \\  FedProx + Default & \(40.57\) & \(37.71\) & \(80.64\) & \(86.79\) & \(81.45\) \\  + RandAugment & \(45.97\) & \(41.39\) & \(82.56\) & \(85.52\) & \(77.11\) \\  + TrivialAugment & \(46.61\) & \(41.81\) & \(81.83\) & \(84.11\) & \(79.67\) \\  FedDyn + Default & \(42.09\) & \(38.52\) & \(80.36\) & \(87.60\) & \(80.47\) \\  + RandAugment & \(45.70\) & \(42.24\) & \(82.51\) & \(81.47\) & \(77.64\) \\  + TrivialAugment & \(46.83\) & \(41.10\) & \(82.03\) & \(83.41\) & \(79.31\) \\  FedExP + Default & \(42.76\) & \(38.28\) & \(80.64\) & \(86.66\) & \(81.45\) \\  + RandAugment & \(46.13\) & \(42.23\) & \(82.86\) & \(84.63\) & \(79.69\) \\  + TrivialAugment & \(48.55\) & \(42.09\) & \(82.51\) & \(83.72\) & \(80.20\) \\  FedGen + Default & \(42.14\) & \(38.27\) & \(80.23\) & \(86.79\) & \(81.86\) \\  + RandAugment & \(47.11\) & \(43.10\) & \(81.90\) & \(84.39\) & \(79.34\) \\  + TrivialAugment & \(47.71\) & \(40.76\) & \(82.58\) & \(83.23\) & \(77.35\) \\  FedMix + Default & \(40.26\) & \(38.69\) & \(80.99\) & \(86.02\) & \(81.63\) \\  + RandAugment & \(46.69\) & \(43.00\) & \(83.08\) & \(83.44\) & \(79.46\) \\  + TrivialAugment & \(46.64\) & \(42.63\) & \(81.83\) & \(82.34\) & \(77.84\) \\  FedFA & + Default & \(43.70\) & \(41.21\) & \(82.61\) & \(87.33\) & \(81.13\) \\  + RandAugment & \(48.86\) & \(43.44\) & \(82.44\) & \(81.32\) & \(78.71\) \\  + TrivialAugment & \(47.86\) & \(43.45\) & \(80.12\) & \(78.62\) & \(78.96\) \\   FedAvP (W/ Local Policy) & \(49.04\) & \(43.86\) & \(83.64\) & \(87.05\) & \(83.94\) \\ FedAvP (Fast Update) & \(49.97\) & \(( 0.04)\) & \(45.08\) & \(( 0.01)\) & \(83.55\) & \(( 0.06)\) & \([rgb]{0,0,0}[named]{pgfstrokecolor}{rgb}{0,0,0} @color@rgb@stroke{0}{0}@color@rgb@fill{0}{0}{0}}\) (\( 1.53\)) & \([rgb]{0,0,0}[named]{pgfstrokecolor}{rgb}{0,0,0} @color@rgb@stroke{0}{0}@color@rgb@fill{0}{0}{0}}\) (\( 0.006\)) \\ FedAvP & \(50.47\) & \(( 0.03)\) & \([rgb]{0,0,0}[named]{pgfstrokecolor}{rgb}{0,0,0} @color@rgb@stroke{0}{0}@color@rgb@fill{0}{0}{0}}\) (\( 0.01\)) & \([rgb]{0,0,0}[named]{pgfstrokecolor}{rgb}{0,0,0} @color@rgb@stroke{0}{0}@color@rgb@fill{0}{0}{0}}\) (\( 0.004\)) & \([rgb]{0,0,0}[named]{pgfstrokecolor}{rgb}{0,0,0} @color@rgb@stroke{0}{0}@color@rgb@fill{0}{0}{0}}\) (\( 1.55\)) & \([rgb]{0,0,0}[named]{pgfstrokecolor}{rgb}{0,0,0} @color@rgb@stroke{0}{0}@color@rgb@fill{0}{0}{0}}\) (\( 0.07\)) \\   

Table 1: Classification accuracies with different heterogeneity degrees (\(=5.0\) and \(=0.1\)) across CIFAR-100/10, SVHN, and FEMNIST datasets. We report results averaged over 3 random seeds with variances for FedAvP (Fast Update) and FedAvP.

local client without policy aggregation, and FedAvP (Fast Update) which reduces computation load on local clients (Algorithm 2). Further details of baselines are provided in Appendix A.

### Performance on Non-i.i.d. Settings

**Settings.** Table 1 reports the _test accuracy_, which measures the accuracy on the test dataset of the 100 participating clients. The test accuracy is calculated as the weighted average of each client's accuracy by the number of data points they have. We compared each baseline with three data augmentations: +Default (random crops and horizontal flipping), +RandAugment, and +TrivialAugment. For FedAvP (W/ Local Policy), each client has its own transformation policy and policy aggregation is removed from our algorithm (Algorithm 1). For FedAvP (Fast Update), we used periodic local updates (Algorithm 2). For FedAvP, we used full local updates (Algorithm 1).

**Results.** The application of automated data augmentation algorithms such as RandAugment and TrivialAugment within a federated learning framework does not consistently enhance performance across all cases. In contrast, our algorithm learned distinct augmentations for each dataset, as depicted in Figure 2. When compared to FedAvP (W/ Local Policy) and FedAvP, notable performance improvements were observed in highly non-i.i.d. scenarios such as \(=0.1\) in CIFAR-100 and SVHN. For instance, the standard deviation of local dataset sizes in CIFAR-100 with \(=5.0\) was relatively small at 19.43, while it was significantly higher at 118.00 for CIFAR-100 and 533.60 for SVHN with \(=0.1\), indicating that imbalanced datasets in non-i.i.d. settings pose challenges for training local policies with smaller local datasets. Although FedAvP (Fast Update) experienced some performance declines compared to FedAvP, it generally achieved higher performance than baseline methods across most datasets.

### Policy Adaptation on Clients

In our FedAvP, at the beginning of each round, the participating clients receive a global policy from the server, which is then optimized into a local policy using each client's local data. That is, this optimization adapts the global policy into a personalized policy on the local data of each client. The clients use the personalized policy to train their local model to achieve high performance and to aggregate well with other local models in the server. Figure 3 shows the statistics on the Euclidean distances between the personalized policies of clients participating in each round and the global policy for that round. With \(=5.0\) in CIFAR-100, the data is sufficiently i.i.d., and thus the personalized policies of clients tend not to deviate from the global policy. On the other hand, with \(=0.1\), the deviation from the global policy is initially high but decreases as training progresses, particularly after about 100 rounds. The variance of the Euclidean distances also follows this pattern.

Figure 3: Statistics of personalized policies between different clients on CIFAR-100.

Figure 2: Visualization of global policies learned in CIFAR-100, SVHN and FEMNIST.

### Reconstruction Attack

**Settings.** In collaborative learning systems, it has been reported that gradient leakage attacks can occur [14; 42; 43], leveraging gradients to reconstruct the original training data. We conducted these reconstruction attack  experiments to evaluate whether our algorithm, which shares policies in a federated learning setting, provides enhanced privacy compared to FedGen, FedMix, and FedFA. Specifically, FedGen shares information at the generator and label distribution levels, FedMix shares at the input level, and FedFA shares at the feature level.

We included a defense algorithm in the performance comparison, ATSPrivacy . Our experiments were conducted on CIFAR-100 with \(=0.1\), involving two clients: Client(L), which had the most training data (895 data points), and Client(S), with the least (156 data points). For ATSPrivacy, we used policies from  that showed the highest accuracy performance. Further experimental details are provided in Appendix A.1.

**Results.** The experimental results for the Reconstruction Attack are summarized in Table 2. Lower PSNR values indicate that the reconstructed images are less similar to the original data, thereby reflecting better privacy preservation. FedGen+label+generator utilizes the label distribution of local data and the generative model. FedMix+input and FedFA+feature represent the results of attacks utilizing input level and feature level information, respectively. FedAvP+policy gradients denotes the results of attacks using the policy gradient of our algorithm. ATSPrivacy recorded the lowest PSNR values, indicating that it makes reconstruction difficult. However, this was coupled with a decline in accuracy performance. Despite increases in PSNR for FedGen, FedMix and FedFA, our algorithm's use of policy gradients did not elevate PSNR values.

### Computation and Communication Cost

For computational comparison, we measured the time taken to reach a target accuracy of 35% on the CIFAR-100 dataset with \(=0.1\). Regarding communication comparison in FedMix, the method involves sending an average image to the server prior to training, which is detailed in the "Before" section of the table. For FedAvP (Fast Update) using small neural networks, although it is higher than these baselines, the increase of 0.38MB from the cost of FedAvg is about 2.48\(\%\) compared to the gradient transmission cost of the model. See A.4 for additional results.

## 5 Conclusion

We proposed a novel federated data augmentation method named FedAvP (Augment Local Data via Shared Policy in Federated Learning). It shares the augmentation policies during training rather than preprocessed or encoded data such as the average of data or statistics of features. Direct exposure of personal information was constrained, yet clients still benefited from the policies learned and shared

    &  \\ Method &  \\  FedAvg & \(0.00\) & \(15.35\) \\ FedMix & \(1.27\) & \(15.35\) \\ FedFA & \(0.00\) & \(15.38\) \\  FedAvP (Fast Update) & \(0.00\) & \(15.73\) \\ FedAvP & \(0.00\) & \(17.47\) \\   

Table 4: Communication costs in CIFAR-100

    & Metric &  & Accuracy \\  & Method & Client(S) & Client(L) & Test(\%) \\  FedAvg & \(10.88\) & \(11.36\) & \(37.34\) \\ FedGen & \(8.86\) & \(9.27\) & \(38.27\) \\ FedMix & \(10.27\) & \(10.48\) & \(38.69\) \\ FedFA & \(10.86\) & \(11.82\) & \(41.21\) \\
**FedAvP** & \(8.72\) & \(9.25\) & **45.96** \\  FedGen + label + generator & \(9.21\) & \(9.81\) & \(38.27\) \\ FedMix + input & \(11.89\) & \(12.40\) & \(38.69\) \\ FedFA + feature & \(12.11\) & \(12.87\) & \(41.21\) \\
**FedAvP + policy gradients** & \(8.77\) & \(9.20\) & **45.96** \\  ATSPrivacy (7-4-15) & \(8.45\) & \(8.89\) & \(38.61\) \\ ATSPrivacy (21-13-3,7-4-15) & **6.70** & **6.69** & \(36.42\) \\   

Table 2: Reconstruction Attack Results

    &  \\ Method &  \\  FedAvg + Default & \(300\) & \(1.05\) hours \\ FedAvg + RandAugment & \(300\) & \(1.62\) hours \\ FedAvg + TrivialAugment & \(450\) & \(2.17\) hours \\  FedAvP (Fast Update) & \(200\) & \(1.18\) hours \\ FedAvP & \(200\) & \(4.01\) hours \\   

Table 3: Computation Time in CIFAR-100across the clients to augment their local data. We also proposed Federated Meta-Policy Loss (FMPL) and used the first-order gradient information to enhance privacy with reduced communication costs. A potential limitation of our algorithm is the introduction of Joint Training. This approach requires consideration when applying our federated data augmentation method to existing federated learning algorithms. Also, our algorithm assumed the use of FedAvg, which does not account for model personalization  in the computation of FMPL. Investigating a policy loss that aligns with model personalization algorithms would be interesting.