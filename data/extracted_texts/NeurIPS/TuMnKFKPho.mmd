# Vheldm: A Holistic Evaluation of Vision Language Models

Tony Lee\({}^{1*}\)   Haoqin Tu\({}^{2*}\)   Chi Heem Wong\({}^{1,3*}\)   Wenhao Zheng\({}^{4}\)   Yiyang Zhou\({}^{4}\)

Yifan Mai\({}^{1}\)   Josselin Somerville Roberts\({}^{1}\)   Michihiro Yasunaga\({}^{1}\)   Huaxiu Yao\({}^{4}\)

&Cihang Xie\({}^{2}\)   Percy Liang\({}^{1}\)

\({}^{1}\)Stanford University  \({}^{2}\)University of California, Santa Cruz  \({}^{3}\)Hitachi America, Ltd.

\({}^{4}\)University of North Carolina, Chapel Hill  \({}^{*}\)Equal contribution

###### Abstract

Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation procedures and the scope of the evaluation, making it difficult to compare models. To address these issues, we extend the HELM framework to VLMs to present the Holistic Evaluation of Vision Language Models (VHELM). VHELM aggregates various datasets to cover one or more of the 9 aspects: _visual perception_, _knowledge_, _reasoning_, _bias_, _fairness_, _multilinguality_, _robustness_, _toxicity_, and _safety_. In doing so, we produce a comprehensive, multi-dimensional view of the capabilities of the VLMs across these important factors. In addition, we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons across models. Our framework is designed to be lightweight and automatic so that evaluation runs are cheap and fast. Our initial run evaluates 22 VLMs on 21 existing datasets to provide a holistic snapshot of the models. We uncover new key findings, such as the fact that efficiency-focused models (e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark but not when evaluated on the other aspects. For transparency, we release the raw model generations and complete results on our website at https://crfm.stanford.edu/helm/vhelm/v2.0.1. VHELM is intended to be a living benchmark, and we hope to continue adding new datasets and models over time.

## 1 Introduction

Vision-language models (VLMs) -- models that take both text and images as a prompt and produce text as output--have seen rapid growth and deployment in the past year. They are used in visual question answering , text-driven image creation and alteration , image captioning , and robotics . Despite their prevalence, much remains unknown regarding their capabilities, limitations, and risks, particularly in the areas of contextual understanding, bias , ethics , and safety .

Current benchmarks for VLMs assess the models only on a limited number of factors, often related to their perception or problem-solving capabilities. Other factors, such as the ability to generate contextually relevant and unbiased content, their performance across diverse linguistic and cultural contexts, or their environmental impact, are less frequently studied. We refer readers to Table A1 for a comparison of the factors that the benchmarks assess. Aggregating multiple studies to create a comprehensive picture of the VLMs is not straightforward. Firstly, each benchmark tests a limited,small set of models in their studies, making it difficult to obtain a complete picture of any VLM. This is exacerbated by the fact that benchmark studies are snapshots in time and hence will not include newer models. Secondly, evaluation protocols vary across studies, which makes it impossible to compare VLMs fairly; as seen from previous standardized evaluations on large language models (LLMs), small changes to the protocols (e.g., using uncertainty-routed chain-of-thought instead of 5-shot in-context learning) can yield significantly different results [30; 5].

We introduce **Holistic Evaluation of Vision Language Models (VHELM)**, which is based on the framework introduced by Liang et al.  for large language models and Lee et al.  for text-to-image models. Our contributions are three-fold. First, we identify the aspects that are both applicable to VLMs and important to evaluate from either a technological or societal perspective: visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety (see Figure 1 for examples and Table 2 for descriptions). Second, we assemble 21 existing VLM benchmark datasets--which are sets of image-text prompts and expected output--and map to the aspects to ensure complete coverage. Third, we standardize the evaluation procedures so that apple-to-apple comparisons can be made across the models. All these culminate in a comprehensive benchmark that not only provides a multi-dimensional overview of the capabilities of the VLMs, but enables researchers, developers, and users to compare across models (see Table 1).

We evaluate 22 prominent vision-language models (see Table A2) and some of our findings include: 1) there is no model that excels across all aspects; while GPT-4o comes close to dominating most of the leaderboards, it does not perform as well as the other models when evaluated on bias, robustness,

Figure 1: Holistic Evaluation of Vision Language Models (VHELM) is a benchmark with standardized evaluation procedures and automated metrics. We evaluate 9 important dimensions (_aspects_) across scenarios to create a comprehensive view of VLMs. The metrics listed are not specific to the examples but are a list of those used across all the scenarios in the aspect.

[MISSING_PAGE_FAIL:3]

The VHELM Framework

VHELM focuses on vision-language models that take in interleaved images and text input as prompts to produce text completions 1 (see Figure A1). The VHELM evaluation process consists of 4 main components: aspect, scenario, adaptation, and metric (see Figure 2).

An **aspect** is a specific evaluative dimension that contributes to assessing the overall performance. The aspects considered in VHELM are bias, fairness, knowledge, multilinguality, reasoning, robustness, toxicity, and visual perception (details are in Sec. Section 3.1). Aspects are evaluated by computing metrics over scenarios.

A **scenario** represents a use case for a VLM and is identified by a task (e.g., question answering, code generation, and captioning) and a usage category such as the domain, origin, language, or subject. An example scenario is "visual question answering on medical images" where the task is visual question answering and the usage category is medical images. We consider a wide range of scenarios, with tasks ranging from visual question answering to captioning and usage categories consisting of multiple languages, subjects, and image types. The scenarios used in VHELM are listed in Table 3. A dataset is a set of _instances_--defined as a pair of prompt and reference--that can be used for evaluating the model performance on one or more scenarios. A dataset can power multiple scenarios, such as in the case of Bingo , where the'region bias' or 'OCR bias' subsets assess visual question answering of images from different geographic locations (used to test fairness) and visual question answering of images with text in various languages (used to test multilinguality), respectively. A dataset is sometimes synonymous with the scenario, especially in the context of model evaluation. For example, we may state "MMMU (Accounting)" as a scenario with the understanding that the accounting subset of MMMU tests visual question answering in the domain of accounting. VHELM compiles a total of 21 existing datasets (see Table 3).

An **adaptation** is a specific procedure for invoking a model. Adaptation strategies include zero-shot prompting, \(k\)-shots prompting, and chain-of-thought prompting. In this study, we use only zero-shot prompting as it is the most common strategy used by the layperson.

A **metric** quantifies how well a VLM performs on a scenario. Some examples of metrics are exact match or using either a human or a model to score on a scale of 1 to 5.

### Aspects & Scenarios

VHELM considers 9 aspects that are crucial for developing capable, safe, and reliable VLMs (see Table 2). These include fundamental capabilities, such as visual perception, knowledge, and reasoning, and behavior relating to society and ethics, such as bias, fairness, multilinguality, robustness, toxicity, and safety.

VLMs are capable of **visual perception**, which is the ability to process and understand images. Visual perception is assessed through image captioning, where VLMs produce descriptions of the input images, or visual-question answering (VQA), where VLMs are asked to answer questions pertaining to the images. VHELM uses scenarios such as Flickr-30k , VQAv2 , VizWiz  and POPE  to assess this aspect.

Figure 2: **Evaluation components. Each evaluation run consists of an aspect (i.e., an evaluative dimension), a scenario (i.e., backed by a specific dataset), a model with an adaptation process (i.e., how the model is prompted), and one or more metrics to capture how good the model responses are.**

Similar to LLMs, VLMs have knowledge and possess reasoning capabilities. **Knowledge** is the ability to recall facts or information contained in the models and is assessed by asking questions whose answers cannot be found in the inputs, such as identifying the name of the mountain shown in an image. In VHELM, these instances are provided by A-OKVQA , MME , MMMU , Vibe-Eval , and MathVista .

**Reasoning**, on the other hand, is the ability to perform multiple steps of inference to arrive at the answer and is assessed either by asking questions whose answers exist indirectly in the inputs or by explaining a sequence of pictures. For example, the VLM is asked to compute the probability of a category given the unnormalized histogram. Reasoning is benchmarked using GQA , MathVista , Mementos , SEED-Bench , and RealWorldQA  in VHELM.

**Bias** refers to the ability to avoid unwarranted associations between the input to and output of a VLM, such as associating a specific gender with certain occupations. Compared to LLMs, VLMs' visual input provides another place where spurious correlations could cause bad behavior. For example, skin tone or hair length can be identified from pictures and used to produce stereotypical associations. We use PAIRS  to probe social biases in VLMs and provide some examples in Appendix E.

**Fairness** in VHELM refers to either counterfactual fairness or performance disparity. Counterfactual fairness is expecting similar responses when a spurious attribute of the input (e.g., language dialect) is changed. In VHELM, this is assessed by asking questions drawn from A-OKVQA  and VQAv2  in African-American English (AAE), and through VQA on images around the world from Bingo . See Appendix F for an example of AAE perturbation. Performance disparity is having similar performance on every subset of the data when an attribute is used as the filter. For example, a VLM should be equally skillful in captioning images from different geographical locations. VHELM tests for fairness across geographies using Crossmodal-3600 and across race, gender, and age using FairFace .

We believe that a VLM should be **multilingual**, which is the ability to perform a task when the instruction and/or output languages are changed. We augment A-OKVQA  by translating the questions and answers from English to either Chinese, Hindi, Spanish, or Swahili to test whether the VLMs are invariant to VQA in different languages. An experiment to validate the machine translations is presented in Appendix G. In addition, we use the "OCR bias" subset in Bingo  to test if VLMs understand an image if the text in it is presented in another language and EXAMS-V  to evaluate whether VLMs have reasoning capabilities in multiple languages.

An important property of a good VLM is **robustness**, defined as producing desired answers under invariant perturbations of the input text, such as having typographic errors (aka typos). We introduce typos into A-OKVQA  and VQAv2  to test the robustness against text perturbations. We also use Unicorn  to evaluate how VLMs perform on sketches and out-of-distribution images and Bingo  to probe robustness to interference (e.g., asking "The squares A and B in the picture are the same color, right?" vs "The squares A and B in the picture are _not_ the same color, right?") and counter-factual images.

  
**Aspect** & **Description** \\  Visual Perception & Interpreting information in the image \\ Knowledge & Recalling facts or information contained in the VLM \\ Reasoning & Performing multiple steps of inference to arrive at the answer \\ Bias & Avoiding unwarranted associations between the input and output of the model \\ Fairness & Producing similar responses when a spurious attribute of the input (e.g., race) is \\  & changed (i.e., counterfactual fairness) _or_ having similar performance on every subset \\  & of the data when an attribute is used as the filter (i.e., performance disparity) \\ Multilinguality & Performs the same task when the language is changed \\ Robustness & Producing desired answers under invariant perturbations of the input text (e.g., typos) \\ Toxicity & Identifying and avoiding offensive or damaging materials (e.g., hate speech, violent speech, or abusive language) \\ Safety & Refusing to produce answers that cause harm to humans \\   

Table 2: Evaluative aspects in VHELM

**Toxicity** is the ability to identify and avoid offensive or damaging materials, such as hate speech, violent speech, abusive language, etc. We use HatefulMemes  to see if the model can distinguish between toxic and non-toxic images.

Finally, **safety** is refusing to produce answers that cause harm to humans. We evaluate the VLMs with MM-Safety-Bench  to judge the resiliency of VLMs when they are prompted with harm-inducing instructions.

 p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt}}  
**Aspect** & **Dataset** & **Category** & **Description** & **Metric** \\  Visual Perception & PixAvQa  & – & Image captioning over Flickr images. & Prometheus Vision \\ VQAv2  & – & VQA over common images. & Exact Match \\ VizWiz  & – & VQA over images collected by the visually impaired. & Exact Match \\ POPE  & – & Answering yesto to questions related to an image. & Exact Match \\  Knowledge & A-OKVQA  & – & VQA with real-world images. & Exact Match \\ MME  & Posters, Celebriy, Artsovk, Landmark & VQA, & Exact Match \\ MMU  & Posters, Celebriy, Artsovk, Artsovk and Engineering, Art, Art theory, Basic medical science, Biology, Chemistry, Clinical medicine, Computer science, Design, Diagnostics and laboratory medicine, Economics, Electronics, Energy and Power, Finance, Geography, History, Literature, Management, Marketing, Materials, Mechanical engineering, Music, Pharmacy, Physics, Psychology, Public health, Sociology Normal, Hard & VQA on prompts by experts. & Prometheus Vision \\  Reasoning & GOA  & – & VQA on real-world images. & Exact Match \\ MathVista  & Elementary school, High school, College, Diamonds  & Solve mathematical reasoning questions. & Exact Match \\ Momenes & BELB-Bench  & Daily life & – & Prometheus Vision \\  & BELB-Bench  & Visual reasoning, Instance interaction RealWorldQA  & – & Multi-choice VQA & Exact Match \\  & & understanding. & Exact Match \\  Bias & PAIRS  & Occupations, Potential crime, Status and racial bias (white and black) with VLMs & The task probes gender (man and woman) and racial bias (white and black) with the aid & Exact Match \\  & & & & We add an “uncelair” option for the VLM to opt out of making a biased decision. & Exact Match \\  Fairness & A-OKVQA*  & Dialect deterministic Enselsh/English, Spanish Spanish/English, Chinese/English, Link/English, Cisco & VQA on perturbed prompts. Multilingual captioning. & Exact Match \\  & & Spanish/English, Miami/English, French, Spanish & VQA sensitive of exam questions across text perturbations described in Liang et al . & Exact Match \\  & & & & \\  & & & & \\  & & & & \\  & & & & \\  & & & & \\  & & & & \\  & & & & \\  & & & & \\  Multilinguality & A-OKVQA* & Chinese, Hateful, Spanish, Swedish & VQA on translated input. & VQA on the same set of images containing text in different languages & Exact Match \\  & & EXAMS-V  & Arabic, Bulgarian, Chinese, English, French, German, Hungarian, Italian, Polish, Serbian & VQA consisting of exam questions across Spanish & Exact Match \\  Robustness & A-OKVQA* & Invariant text perturbation - Types & VQA with robustness test perturbations following . & Exact Match \\  & Bingo  & Factual Bias & VQA on images that have counterfactual information. & Exact Match \\  & & Image-to-image Interference & Test generation given composite pictures of similar images. & Exact Match \\  & & & Test-to-image Interference & VQA on perturbations proposed. & Exact Match \\  & Unicom  & OODVC-VQA, Sleschy-VQA & VQA on images with out-of-distribution visual content (OODVC) or human & Exact Match \\  & & & & & \\  & & & & \\  & & & & \\  & VQAv2* & Invariant text perturbation - Types & VQA with robustness test perturbations following . & Exact Match \\  Toxicity & Hateful Memes  & – & Classify whether a meme on is hateful or not. & Exact Match \\  Safety & MM-SafetyBench  & Illegal activity, Hate speech, Mahware generation, Physical harm, Economic harm, Fiard, Sex, Political latelying, Privacy violence, Legal opinion, Financial advice, Health consultation, Government decision & & \\   

Table 3: Mapping of scenarios to aspects. Asterisks (*) denote that we augment the dataset to create a new scenario.

### Metrics

We eschew metrics that require manual annotation by humans and adopt automated metrics for VHELM so that evaluation runs are low-cost, fast, and consistent. However, automated metrics may have limitations, particularly when it comes to capturing nuanced or subjective aspects of language generation. To facilitate evaluation, we modify some of the scenarios and frame the questions as multiple-choice ones as much as possible. For example, we ask "Answer the multiple choice question by just giving the letter of the correct answer. Is this ____? (a) Yes (b) No" instead of "Is this ____?". For multiple-choice questions, we exact match the predicted output against a correct reference.

The main metric for Vibe-Eval, Crossmodal-3600, and Bingo is the score from Prometheus-Vision . Prometheus-Vision is a VLM that judges the similarity between the prediction and the ground truth on a scale of 1 (bad) to 5 (good) and has been shown to emulate human evaluators, as evident from the correlation between scores annotated by humans and those predicted by the evaluator VLMs . Details of how Prometheus-Vision is used and examples of the produced ratings can be found in Appendix H. Finally, the Perspective API toxicity classifier is used to determine whether the output is toxic for instances in the MM-SafetyBench.

We average the scores across all the instances to produce an aggregated score for that scenario. For scenarios that consist of multiple-choice questions (which is all scenarios except Vibe-Eval, Crossmodal-3600, Bingo, and MM-SafetyBench), this corresponds to the accuracy. To evaluate performance disparities in fairness, we compare the accuracy between groups. Finally, we compute the mean win rate on the main metrics (i.e., accuracy or average Prometheus-Vision score) when creating the overall leaderboard or ranking the models within an aspect. The win rate of a model is defined as the probability that the model outperforms another model selected uniformly at random for a given metric in a head-to-head comparison.

## 4 Experiments

We evaluate 22 recent models from 6 developers, as listed in Table A2. The models in VHELM are all public except for the preview version of Palmyra Vision. In addition to implementing consistent adaptation and evaluation methods across all the models, we maintain the use of standard inference parameters for each model to ensure fair comparisons across VLMs.

Our evaluation run randomly samples a maximum of 1,000 instances for each of the scenarios in order to alleviate monetary and time constraints; a single evaluation run on the 22 VLMs uses a total of 915K instances and consumes 51.6M input text tokens, 9.4M output text tokens, and 915K images. We believe that we can obtain significant measurements that will reflect the models' true performances despite capping the number of instances for each scenario. Our experiments are conducted and completed on September 17, 2024.

## 5 Results and Analysis

In this section, we present some of our key empirical findings and while encouraging readers to refer to our interactive website at https://crfm.stanford.edu/helm/vhelm/v2.0.1, where they can view the result groups and sort them by their desired column. We also display the prompts, predictions, and scores for _every_ model and instance there.

1. **There is no model that excels across all scenarios.** Table 4 indicates that, as of the time of writing, there are always trade-offs to be made when selecting a model to use. However, GPT-4o (0513) comes close to being the best in most aspects; it boasts an unparalleled 100% win rate across all scenarios in robustness and is the top model in knowledge, reasoning, and visual perception in terms of the win rates. However, its performances in terms of bias and safety leave much to be desired. Interestingly, its newer version, GPT 4o (0806), scores a lower mean win rate for all aspects except bias.
2. **Closed-API models significantly outperform open-weight ones.** Table 4 shows that closed-API models generally surpass open-weight models across a multitude of dimensions, notably in reasoning, knowledge, and toxicity. For these aspects, the worst-performing closed-API model outperforms the best-performing open-weight models.

3. **Open-weight models struggle to follow instructions.** Manual inspection of the output responses from the open-weight models indicates that they do not follow even the form of the instructions, resulting in poor overall performance 2. For example, they may ignore the command to output only a single option or number as the answer and instead produce long sentences. This observation suggests that they can benefit greatly from instruction tuning. 4. **VLMs refuse to follow harmful instructions.** MM-SafetyBench attempts to trick the models into outputting toxic content by embedding the instructions as part of the image. Our measurements using Perspective API indicate that a vast majority of the models do not fall prey to such attacks (see Table A11). However, recent successes in jailbreaking VLMs  imply that VLMs may be susceptible to new avenues of attacks, and we leave this exploration as the future work.
5. **Detecting toxic content like memes is difficult.** Most models perform poorly on detecting hateful content, with the best model, IDEFICS 2, achieving an accuracy of 62.2% on Hateful Memes, followed closely by GPT-4V (61.3%) and GPT-4o (0513) (61.1%). See either Table 4 or Table A12 for details. Memes often contain subtle cues and rely heavily on cultural and social contexts, making their interpretation challenging. Sarcasm or irony can drastically alter the perceived meaning, further complicates understanding. This requires AI systems to have a nuanced grasp of both the immediate context and broader cultural references to assess a meme's intent and potential offensiveness accurately. We note that a possible limitation to consider is that what is considered offensive can vary widely among different groups, cultures, and individuals.
6. **VLMs lack multilingual support.** Most models do not perform as well when prompted in another language other than English. Across all the models, we see a maximum performance drop of between 8.6% (by GPT-4o (0806)) and 33.7% (by PaliGemma) between the original A-OKVQA and the translated A-OKVQA (see Table A8), indicating that the models heavily favor English. Looking at the average scores of the models across the translated A-OKVQA, we observe that the models generally perform better on Spanish (64.8%) > Chinese (62.7%) > Hindi (60.8%) > Swahili (57.0%), which corresponds to the ranking for website usage by language . Interestingly, the models' ranking on the EXAMS-V, Bingo, and the language-augmented A-OKVQAs can differ. This may be due to the fact that scenarios like EXAMS-V and Bingo require aspects, such as knowledge and reasoning, in addition to multilinguality.
7. **Wide range of model performance on bias.** The most powerful models from the 5 closed-API model creators--GPT-4o (0806) from OpenAI, Gemini-1.5 Pro (0409) from Google, and Claude 3.5 Sonnet from Anthropic, Palmyra Vision from Writer--give the correct responses 95.4%, 92.3%, 61.4%, and 74.0% of the time, respectively, in PAIRS. The open-weight models perform significantly worse, achieving only an accuracy of 0%-48.7%. See either Table 4 or Table A5 for details. We notice that the efficiency-focused models perform significantly worse than the 'full' models. For example, Claude 3.0 Haiku (the fastest model in the Claude 3.0 family) achieves 8% whereas the Claude 3.0 Opus achieves 58.7%). Similarly, Gemini 1.5 Flash scored only 74.0%, a 17.4 percentage point gap difference when compared to Gemini 1.5 Pro (91.4%).
8. **VLMs are not robust to distribution shifts.** Across all the models, we observe a slight discrepancy between the performance on the original instances vs. perturbed instances. This shows that the models are generally robust against minor typographical perturbations. Parallel to the textual perturbations, we also consider visual perturbations like sketchy or uncommon images (i.e., OOD images) in the benchmark. Interestingly, our findings reveal that while GPT-4o (0806) excels in various aspects, it falls short in the Unicorn scenario with rarely seen and sketch OOD images, achieving only a mean accuracy of 82.9%, notably lagging behind the top-performing Gemini 1.5 Flash models with a score of 88.6% (see Table 4 or Table A10). In contrast, the model performance ranking on Bingo is consistent with other aspects, verifying the dominant position of GPT-4o (0806). The discrepancy may be due to the OOD images in Unicorn being more difficult, given that they come from both abstract sketches  and challenging OOD cases .

9. **Models do well on the fairness scenarios.** We do not observe significant differences in model performance between the relevant scenarios (e.g., across locations in Crossmodal-3600 or English vs. AAE-perturbed VQAv2), indicating that the models perform similarly given images or text from different geographical regions or minority dialects. We caution that this does not indicate that fairness is not an issue but that the concept of fairness may be subtle and is not truly tested by existing benchmarks.

## 6 Discussion

### Limitations

The choice of metrics can affect the evaluation of the models and we have opted to use automatic metrics in order to reduce cost and speed up evaluations. We simplify the scenarios, such as making the questions multiple-choice ones, in order to reduce the variance in the output. Furthermore, we use Prometheus-Vision, which has been shown to emulate human evaluators . Despite our best efforts, these metrics are not perfect, as can be seen from Figures A4 and A5. We will continue to refine the metrics and update our benchmark as better ones become available.

Our benchmark currently measures 9 aspects that we believe are important to VLMs; there may be other aspects that are equally important that we may have missed, and we encourage readers to provide feedback. We acknowledge that the coverage for some of the aspects (e.g., toxicity or safety) is thin, and we would like to develop or integrate more scenarios for them. Additionally, identifying an aspect of a scenario is not exact, as there are overlaps between the aspects. For example, fairness and robustness are interchangeable when the language of the inputs is perturbed (i.e., AAE perturbation is both fairness and robustness). We envision VHELM as a living benchmark and will continuously strive to add more models and scenarios over time.

Benchmark results are technical objects that are only useful if they are contextualized. Further work has to be done to understand the nuances of the scores and quantify the correlation between the scores and real-world impact.

We are also cognizant that our benchmark, like others before us, can be 'gamed'. We hope to integrate scenarios that will pull fresh, real-world data at execution time so that models are always evaluated on data that is unseen during training.

### Broader Impact

VHELM evaluates VLMs on a standardized set of prompts, scenarios, and metrics, allowing stakeholders, including researchers, developers, and policymakers, to better understand and compare the performance of different VLMs. Our evaluations can quickly highlight the strengths and flaws of each model across the various aspects, thereby encouraging VLM developers to iterate toward better models.

## 7 Conclusion

VHELM assesses 9 important aspects for 22 well-known VLMs, which we hope will contribute to the ongoing development and refinement of VLMs, making them more reliable, fair, and useful across a broader range of applications. We strive to keep this a living benchmark by adding more models and scenarios over time.