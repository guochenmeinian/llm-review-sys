# Online Bayesian Persuasion Without a Clue

Francesco Bacchiocchi

Politecnico di Milano

francesco.bacchiocchi@polimi.it

Matteo Bollini

Politecnico di Milano

matteo.bollini@polimi.it

Matteo Castiglioni

Politecnico di Milano

matteo.castiglioni@polimi.it

Alberto Marchesi

Politecnico di Milano

alberto.marchesi@polimi.it

Nicola Gatti

Politecnico di Milano

nicola.gatti@polimi.it

###### Abstract

We study _online Bayesian persuasion_ problems in which an informed sender repeatedly faces a receiver with the goal of influencing their behavior through the provision of payoff-relevant information. Previous works assume that the sender has _knowledge about_ either the prior distribution over states of nature or receiver's utilities, or both. We relax such unrealistic assumptions by considering settings in which the sender does _not know anything_ about the prior and the receiver. We design an algorithm that achieves sublinear--in the number of rounds--regret with respect to an optimal signaling scheme, and we also provide a collection of lower bounds showing that the guarantees of such an algorithm are tight. Our algorithm works by searching a suitable space of signaling schemes in order to learn receiver's best responses. To do this, we leverage a non-standard representation of signaling schemes that allows to cleverly overcome the challenge of _not_ knowing anything about the prior over states of nature and receiver's utilities. Finally, our results also allow to derive lower/upper bounds on the _sample complexity_ of learning signaling schemes in a related Bayesian persuasion PAC-learning problem.

## 1 Introduction

_Bayesian persuasion_ has been introduced by Kamenica and Gentzkow (2011) to model how strategically disclosing information to decision makers influences their behavior. Over the last years, it has received a terrific attention in several fields of science, since it is particularly useful for understanding strategic interactions involving individuals with different levels of information, which are ubiquitous in the real world. As a consequence, Bayesian persuasion has been applied in several settings, such as online advertising (Emek et al., 2014; Badanidiyuru et al., 2018; Bacchiocchi et al., 2022; Agrawal et al., 2023), voting (Alonso and Camara, 2016; Castiglioni et al., 2020; Castiglioni and Gatti, 2021), traffic routing (Vasserman et al., 2015; Bhaskar et al., 2016; Castiglioni et al., 2021), recommendation systems (Cohen and Mansour, 2019; Mansour et al., 2022), security (Rabinovich et al., 2015; Xu et al., 2016), e-commerce (Bro Miltersen and Sheffet, 2012; Castiglioni et al., 2022) medical research (Kolotilin, 2015), and financial regulation (Goldstein and Leitner, 2018).

In its simplest form, Bayesian persuasion involves a _sender_ observing some information about the world, called _state of nature_, and a _receiver_ who has to take an action. Agents' utilities are misaligned, but they both depend on the state of nature and receiver's action. Thus, sender's goal is to devise amechanism to (partially) disclose information to the receiver, so as to induce them to take a favorable action. This is accomplished by committing upfront to a _signaling scheme_, encoding a randomized policy that defines how to send informative signals to the receiver based on the observed state.

Classical Bayesian persuasion models (see, _e.g._, (Dughmi and Xu, 2016, 2017, Xu, 2020)) rely on rather stringent assumptions that considerably limit their applicability in practice. Specifically, they assume that the sender perfectly knows the surrounding environment, including receiver's utilities and the probability distribution from which the state of nature is drawn, called _prior_. This has motivated a recent shift of attention towards Bayesian persuasion models that incorporate concepts and ideas from _online learning_, with the goal of relaxing some of such limiting assumptions. However, existing works only partially fulfill this goal, as they still assume some knowledge of either the prior (see, _e.g._, (Castiglioni et al., 2020, 2021, 2023, Babichenko et al., 2022, Bernasconi et al., 2023)) or receiver's utilities (see, _e.g._, (Zu et al., 2021, Bernasconi et al., 2022, Wu et al., 2022, Lin and Li, 2025)).

### Original contributions

We address--for the first time to the best of our knowledge--Bayesian persuasion settings where the sender _has no clue_ about the surrounding environment. In particular, we study the online learning problem faced by a sender who repeatedly interacts with a receiver over multiple rounds, _without_ knowing anything about both the prior distribution over states of nature and receiver's utilities. At each round, the sender commits to a signaling scheme, and, then, they observe a state realization and send a signal to the receiver based on that. After each round, the sender gets _partial feedback_, namely, they only observe the best-response action played by the receiver in that round. In such a setting, the goal of the sender is to minimize their _regret_, which measures how much utility they lose with respect to committing to an optimal (_i.e._, utility-maximizing) signaling scheme in every round.

We provide a learning algorithm that achieves regret of the order of \(}()\), where \(T\) is the number of rounds. We also provide lower bounds showing that the regret guarantees attained by our algorithm are tight in \(T\) and in the parameters characterizing the Bayesian persuasion instance, _i.e._, the number of states of nature \(d\) and that of receiver's actions \(n\). Our algorithm implements a sophisticated _explore-then-commit_ scheme, with exploration being performed in a suitable space of signaling schemes so as to learn receiver's best responses _exactly_. This is crucial to attain tight regret guarantees, and it is made possible by employing a non-standard representation of signaling schemes, which allows to cleverly overcome the challenging lack of knowledge about both the prior and receiver's utilities.

Our results also allow us to derive lower/upper bounds on the _sample complexity_ of learning signaling schemes in a related Bayesian persuasion PAC-learning problem, where the goal is to find, with high probability, an approximately-optimal signaling scheme in the minimum possible number of rounds.

### Related works

Castiglioni et al. (2020b) were the first to introduce _online learning_ problems in Bayesian persuasion scenarios, with the goal of relaxing sender's knowledge about receiver's utilities (see also follow-up works (Castiglioni et al., 2021, 2023, Bernasconi et al., 2023)). In their setting, sender's uncertainty is modeled by means of an _adversary_ selecting a receiver's _type_ at each round, with types encoding information about receiver's utilities. However, in such a setting, the sender still needs knowledge about the finite set of possible receiver's types and their associated utilities, as well as about the prior.

A parallel research line has focused on relaxing sender's knowledge about the prior. Zu et al. (2021) study online learning in a repeated version of Bayesian persuasion. Differently from this paper, they consider the sender's learning problem of issuing _persuasive_ action recommendations (corresponding to signals in their case), where persuasiveness is about correctly incentivizing the receiver to actually follow such recommendations. They provide an algorithm that attains sublinear regret while being persuasive at every round with high probability, despite having _no_ knowledge of the prior. Wu et al. (2022), Gan et al. (2023), Bechiocchi et al. (2024c) achieve similar results for Bayesian persuasion in episodic Markov decision processes, while Bernasconi et al. (2022) in non-Markovian environments. All these works crucially differ from ours, since they strongly rely on the assumption that receiver's utilities are known to the sender, which is needed in order to meet persuasiveness requirements. As a result, the techniques employed in such works are fundamentally different from ours as well.

Finally, learning receiver's best responses exactly (a fundamental component of our algorithm) is related to learning in Stackelberg games (Letchford et al., 2009; Peng et al., 2019; Bacchiocchi et al., 2024a). For more details on these works and other related works, we refer the reader to Appendix A.

## 2 Preliminaries

In Section 2.1, we introduce all the needed ingredients of the classical _Bayesian persuasion_ model by Kamenica and Gentzkow (2011), while, in the following Section 2.2, we formally define the Bayesian persuasion setting faced in the rest of the paper and its related _online learning_ problem.

### Bayesian persuasion

A Bayesian persuasion instance is characterized by a finite set \(\{_{i}\}_{i=1}^{d}\) of \(d\) states of nature and a finite set \(\{a_{i}\}_{i=1}^{n}\) of \(n\) receiver's actions. Agents' payoffs are encoded by utility functions \(u,u^{s}:\), with \(u_{}(a) u(,a)\), respectively \(u^{s}_{}(a) u^{s}(,a)\), denoting the payoff of the receiver, respectively the sender, when action \(a\) is played in state \(\). The sender observes a state of nature drawn from a commonly-known _prior_ probability distribution \((_{})\),1 with \(_{}(0,1]\) denoting the probability of \(\). To disclose information about the realized state, the sender can publicly commit upfront to a _signaling scheme_\(:_{}\), which defines a randomized mapping from states of nature to signals being sent to the receiver, for a finite set \(\) of signals. For ease of notation, we let \(_{}()\) be the probability distribution over signals prescribed by \(\) when the the sate of nature is \(\), with \(_{}(s)\) denoting the probability of sending signal \(s\).

The sender-receiver interaction goes as follows: (1) the sender commits to a signaling scheme \(\); (2) the sender observes a state of nature \(\) and sends a signal \(s_{}\) to the receiver; (3) the receiver updates their belief over states of nature according to _Bayes rule_; and (4) the receiver plays a best-response action \(a\), with sender and receiver getting payoffs \(u_{}(a)\) and \(u^{s}_{}(a)\), respectively. Specifically, an action is a _best response_ for the receiver if it maximizes their expected utility given the belief computed in step (3) of the interaction. Formally, given a signaling scheme \(:_{}\) and a signal \(s\), we let \(^{}(s)\) be the set of receivers' best-response actions, where:

\[^{}(s)\{a_{i}_{ }_{}_{}(s)u_{}(a_{i})_{ }_{}_{}(s)u_{}(a_{j}) a_{j}\}.\] (1)

As customary in the literature on Bayesian persuasion (see, _e.g._, (Dughmi and Xu, 2016)), we assume that, when the receiver has multiple best responses available, they break ties in favor of the sender. In particular, we let \(a^{}(s)\) be the best response that is actually played by the receiver when observing signal \(s\) under signaling scheme \(\), with \(a^{}(s)_{a^{}(s)}_{}_{ }_{}(s)u^{s}_{}(a)\).

The goal of the sender is to commit to an _optimal_ signaling scheme, namely, a \(:_{}\) that maximizes sender's expected utility, defined as \(u^{s}()_{s}_{}_{} _{}(s)u^{s}_{}(a^{}(s))\). In the following, we let \(_{}u^{s}()\) be the optimal value of sender's expected utility. Moreover, given an additive error \((0,1)\), we say that a signaling scheme \(\) is \(\)-optimal if \(u^{s}()-\).

### Learning in Bayesian persuasion

We study settings in which the sender _repeatedly_ interacts with the receiver over multiple rounds, with each round involving a one-shot Bayesian persuasion interaction (as described in Section 2.1). We assume that the sender has _no_ knowledge about both the prior \(\) and receiver's utility \(u\), and that the only feedback they get after each round is the best-response action played by the receiver.

At each round \(t[T]\),2 the sender commits to a signaling scheme \(_{t}:_{}\) and observes a state of nature \(^{t}\). Then, they draw a signal \(s^{t}_{t,^{t}}\) and send it to the receiver, who plays a best-response action \(a^{t} a^{_{t}}(s^{t})\). Finally, the sender gets payoff \(u^{s}_{t} u^{s}_{^{t}}(a^{t})\) and observes a _feedback_ consisting in the action \(a^{t}\) played by the receiver. The goal of the sender is to learn how to maximize their expected utility while repeatedly interacting with the receiver. When the sender commits to a sequence \(\{_{t}\}_{t[T]}\) of signaling schemes, their performance over the \(T\) rounds is measured by means of the following notion of _cumulative (Stackelberg) regret_:

\[R_{T}(\{_{t}\}_{t[T]}) T-[_{t =1}^{T}u^{s}(_{t})],\]

where the expectation is with respect to the randomness of the algorithm. In the following, for ease of notation, we omit the dependency on \(\{_{t}\}_{t[T]}\) from the cumulative regret, by simply writing \(R_{T}\). Then, our goal is to design _no-regret_ learning algorithms for the sender, which prescribe a sequence of signaling schemes \(_{t}\) that results in the regret \(R_{T}\) growing sublinearly in \(T\), namely \(R_{T}=o(T)\).

## 3 Warm-up: A single signal is all you need

In order to design our learning algorithm in Section 4, we exploit a non-standard representation of signaling schemes, which we introduce in this section. Adopting such a representation is fundamental to be able to learn receiver's best responses _without_ any knowledge of both the prior \(\) and receiver's utility function \(u\). The crucial observation that motivates its adoption is that receiver's best responses \(a^{}(s)\) only depend on the components of \(\) associated with \(s\), namely \(_{}(s)\) for \(\). Thus, in order to learn them, it is sufficient to learn how \(a^{}(s)\) varies as a function of such components.

The signaling scheme representation introduced in this section revolves around the concept of _slice_.

**Definition 1** (Slice).: _Given a signaling scheme \(:_{}\), the slice of \(\) with respect to signal \(s\) is the \(d\)-dimensional vector \(x^{d}\) with components \(x_{}_{}(s)\) for \(\)._

In the following, we denote by \(^{}^{d}\) the set of _all_ the possible slices of signaling schemes. Moreover, we let \(^{}\) be the set of _normalized_ slices, which is simply obtained by restricting slices to lie in the \((d-1)\)-dimensional simplex. Thus, it holds that \(^{}\{x^{d}_{ }x_{}=1\}\).

Next, we show that receiver's actions induce particular coverings of the sets \(^{}\) and \(^{}\), which also depend on both the prior \(\) and receiver's utility \(u\). First, we introduce \(_{ij}^{d}\) to denote the halfspace of slices under which action \(a_{i} A\) is (weakly) better than action \(a_{j} A\) for the receiver.

\[_{ij}\{x^{d}_{} x_{}_{}u_{}(a_{i})-u_{}(a_{j}) 0\}.\]

Moreover, we denote by \(H_{ij}_{ij}\) the hyperplane constituting the boundary of the halfspace \(_{ij}\), which we call the _separating hyperplane_ between actions \(a_{i}\) and \(a_{j}\).3 Then, for every \(a_{i}\), we introduce the polytopes \(^{}(a_{i})^{}\) and \(^{}(a_{i})^{}\):

\[^{}(a_{i})^{}_ {a_{j}:a_{i} a_{j}}_{ij} ^{}(a_{i})^{} _{a_{j}:a_{i} a_{j}}_{ij}.\]

Clearly, the sets \(^{}(a_{i})\), respectively \(^{}(a_{i})\), define a cover of \(^{}\), respectively \(^{}\).4 Intuitively, the set \(^{}(a_{i})\) encompasses all the slices under which action \(a_{i}\) is a best response for the receiver. The set \(^{}(a_{i})\) has the same interpretation, but for normalized slices. Specifically, if \(x^{}(a_{i})\) is a slice of \(\) with respect to \(s\), then \(a_{i}^{}(s)\). Notice that a slice \(x^{}\) may belong to more than one polytope \(^{}(a_{i})\), when it is the case that \(|^{}(s)|>1\) for signaling schemes \(\) having \(x\)as slice with respect to \(s\).5 In order to denote the best-response action actually played by the receiver under a slice \(x^{}\), we introduce the symbol \(a(x)\), where \(a(x) a^{}(s)\) for any \(\) having \(x\) as slice with respect to \(s\). Figure 1 depicts an example of the polytopes \(^{}(a_{i})\) and \(^{}(a_{i})\) in order to help the reader to grasp more intuition about them.

A crucial fact exploited by the learning algorithm developed in Section 4 is that knowing the separating hyperplanes \(H_{ij}\) defining the polytopes \(^{}(a_{i})\) of normalized slices is _sufficient_ to determine an optimal signaling scheme. Indeed, the polytopes \(^{}(a_{i})\) of unnormalized slices can be easily reconstructed by simply removing the normalization constraint \(_{}x_{}=1\) from \(^{}(a_{i})\). Furthermore, as we show in Section 4 (see the proof of Lemma 4 in particular), there alway exits an optimal signaling scheme using at most one slice \(x^{a}^{}(a)\) for each receiver's action \(a\).

We conclude the section with some remarks that help to better clarify why we need to work with signaling scheme slices in order to design our learning algorithm in Section 4.

Why we need slices for learningThe coefficients of separating hyperplanes \(H_{ij}\) are products between prior probabilities \(_{}\) and receiver's utility differences \(u_{}(a_{i})-u_{}(a_{j})\). In order to design a no-regret learning algorithm, it is fundamental that such coefficients are learned exactly, since even an arbitrarily small approximation error may result in "missing" some receiver's best responses, and this may potentially lead to a large loss in sender's expected utility (and, in its turn, to large regret). As a result, any naive approach that learns the prior and receiver's payoffs separately is deemed to fail, as it would inevitably result in approximate separating hyperplanes being learned. Operating in the space of signaling scheme slices crucially allows us to learn separating hyperplanes exactly. As we show in Section 4, it makes it possible to directly learn the coefficients of separating hyperplanes without splitting them into products of prior probabilities and receiver's utility differences.

Why we need normalized slicesOne may wonder why we cannot work with (unnormalized) slices in \(^{}\), rather than with normalized ones in \(^{}\). Indeed, as we show in Section 4, our procedure to learn separating hyperplanes crucially relies on the fact that we can restrict the search space to a suitable subset of the \((d-1)\)-dimensional simplex. This makes it possible to avoid always employing a number of rounds exponential in \(d\), which would lead to non-tight regret guarantees.

Why _not_ working with posteriorsIn Bayesian persuasion, it is oftentimes useful to work in the space of posterior distributions induced by sender's signals (see, _e.g._, ). These are the beliefs computed by the receiver according to Bayes rule at step (3) of the interaction. Notice that posteriors do _not_ only depend on the signaling scheme \(\) and the sent signal \(s\), but also on the prior distribution \(\). Indeed, the same signaling scheme may induce different posteriors for different prior distributions. Thus, since in our setting the sender has no knowledge of \(\), we cannot employ posteriors. Looking at signaling scheme slices crucially allows us to overcome the lack of knowledge of the prior. Indeed, one way of thinking of them is as "prior-free" posterior distributions.

## 4 Learning to persuade without a clue

In this section, we design our no-regret algorithm (Algorithm 1). We adopt a sophisticated _explore-then-commit_ approach that exploits the signaling scheme representation based on slices introduced in Section 3. Specifically, our algorithm works by first exploring the space \(^{}\) of normalized slices in order to learn satisfactory "approximations" of the polytopes \((a_{i})^{}(a_{i})\).6 Then, it exploits them in order to compute suitable approximately-optimal signaling schemes to be employed in the remaining rounds. Effectively implementing this approach raises considerable challenges.

The **first** challenge is that the algorithm cannot directly "query" a slice \(x\) to know action \(a(x)\), as it can only commit to fully-specified signaling schemes. Indeed, even if the algorithm commits to a signaling scheme including the selected slice \(x\), the probability that the signal associated with \(x\) is sent depends on the (unknown) prior, as it is equal to \(_{}_{}x_{}\). This probability can be arbitrarily small. Thus, in order to observe \(a(x)\), the algorithm may need to commit to the signaling scheme for an unreasonably large number of rounds. To circumvent this issue, we show that it is possible to focus on a subset \(_{}\) of normalized slices "inducible" with at least a suitably-defined probability \((0,1)\). Such a set \(_{}\) is built by the algorithm in its first phase.

The **second** challenge that we face is learning the polytopes \(_{}(a_{i})(a_{i})_{}\). This is done by means of a technically-involved procedure that learns the separating hyperplanes \(H_{ij}\) needed to identify them. This procedure is an adaptation to Bayesian persuasion settings of an algorithm recently introduced for a similar problem in Stackelberg games [Bacchiocchi et al., 2024a].

Finally, the **third** challenge is how to use the polytopes \(_{}(a_{i})\) to compute suitable approximately-optimal signaling schemes to commit to after exploration. We show that this can be done by solving an LP, which, provided that the set \(_{}\) is carefully constructed, gives signaling schemes with sender's expected utility sufficiently close to that of an optimal signaling scheme.

The pseudocode of our no-regret learning algorithm is provided in Algorithm 1. In the pseudocode, we assume that all the sub-procedures have access to the current round counter \(t\), all the observed states of nature \(^{t}\), and all the feedbacks \(a^{t}\), \(u_{t}^{s}\) received by the sender.7 Moreover, in Algorithm 1 and its sub-procedures, we use \(_{t}_{}\) to denote the _prior estimate_ at any round \(t>1\), which is a vector with components defined as \(_{t,} N_{t,}/(t-1)\) for \(\), where \(N_{t,}\) denotes the number of times that state of nature \(\) is observed up to round \(t\) (excluded). Algorithm 1 can be conceptually divided into three phases. In _phase 1_, the algorithm employs the Build-Search-Space procedure (Algorithm 2) to build a suitable subset \(_{}\) of "inducible" normalized slices. Then, in _phase 2_, the algorithm employs the Find-Polytopes procedure (see Algorithm 6 in Appendix E) to find a collection of polytopes \(\{_{}(a)\}_{a}\), where each \(_{}(a)\) is either \(_{}(a)\) or a suitable subset of \(_{}(a)\) that is sufficient for achieving the desired goals (see Section 4.2). Finally, _phase 3_ uses the remaining rounds to exploit the knowledge acquired in the preceding two phases. Specifically, at each \(t\), this phase employs the Compute-Signaling procedure (Algorithm 3) to compute an approximately-optimal signaling scheme, by using \(_{}\), the set \(_{}\), and the current prior estimate \(_{t}\).

In the rest of this section, we describe in detail the three phases of Algorithm 1, bounding the regret attained by each of them. This allows us to prove the following main result about Algorithm 1.8

We observe that the regret bound in Theorem 1 has an exponential dependence on the number of states of nature \(d\) and the number of receiver's actions \(n\), due to the binomial coefficient. Indeed, when \(d\), respectively \(n\), is constant, the regret bound is of the order of \(}(n^{d})\), respectively \(}(d^{n})\). Such a dependence is tight, as shown by the lower bounds that we provide in Section 5.

### Phase 1: Build-Search-Space

Given an \((0,}{{6d}})\) and a number of rounds \(T_{1}\), the Build-Search-Space procedure (Algorithm 2) computes a subset \(_{}\) of normalized slices satisfying two crucial properties needed by the learning algorithm to attain the desired guarantees. Specifically, the first property is that any slice \(x_{}\) can be "induced" with sufficiently high probability by a signaling scheme, while the second one is that, if \(x_{}\), then signaling schemes "induce" such a slice with sufficiently small probability. Intuitively, the first property ensures that it is possible to associate any \(x_{}\) with the action \(a(x)\) in a number of rounds of the order of \(}{{}}\), while the second property is needed to bound the loss in sender's expected utility due to only considering signaling schemes with slices in \(_{}\).

Algorithm 2 works by simply observing realized states of nature \(^{t}\) for \(T_{1}\) rounds, while committing to any signaling scheme meanwhile. This allows the algorithm to build a sufficiently accurate estimate \(\) of the true prior \(\). Then, the algorithm uses such an estimate to build the set \(_{}\). Specifically, it constructs \(_{}\) as the set containing all the normalized slices that are "inducible" with probability at least \(2\) under the estimated prior \(\), after filtering out all the states of nature whose estimated probability is _not_ above the \(2\) threshold (see Lines 8-10 in Algorithm 2).

The following lemma formally establishes the two crucial properties that are guaranteed by Algorithm 2, as informally described above.

**Lemma 1**.: _Given \(T_{1}:=(}{{}} )\) and \((0,}{{6d}})\), Algorithm 2 employs \(T_{1}\) rounds and terminates with a set \(_{}\) such that, with probability at least \(1-\): (i) \(_{}_{}x_{}\) for every slice \(x_{}\) and (ii) \(_{}_{}x_{} 10\) for every slice \(x_{}\)._

To prove Lemma 1, we employ the multiplicative version of Chernoff bound, so as to show that it is possible to distinguish whether prior probabilities \(_{}\) are above or below suitable thresholds in a number of rounds of the order of \(}{{}}\). Specifically, we show that, after \(T_{1}\) rounds and with probability at least \(1-\), the set \(\) in the definition of \(_{}\) does _not_ contain states \(\) with \(_{}\), while it contains all the states with a sufficiently large \(_{}\). This immediately proves properties (i) and (ii) in Lemma 1. Notice that using a multiplicative Chernoff bound is a necessary technicality, since standard concentration inequalities would result in a number of needed rounds of the order of \(}{{}}^{2}\), leading to a suboptimal regret bound in the number of rounds \(T\).

For ease of presentation, we introduce the following _clean event_ for phase 1 of Algorithm 1. This encompasses all the situations in which Algorithm 2 outputs a set \(_{}\) with the desired properties.

**Definition 2** (Phase 1 clean event).: \(_{1}\) _is the event in which \(_{}\) meets properties (i)-(ii) in Lemma 1._

### Phase 2: Find-Polytopes

Given a set \(_{}\) computed by the Build-Search-Space procedure and \((0,1)\) as inputs, the Find-Polytopes procedure (Algorithm 6 in Appendix E) computes a collection \(_{}:=\{_{}(a)\}_{a}\) of polytopes enjoying suitable properties sufficient to achieve the desired goals.

Ideally, we would like \(_{}(a)=_{}(a)\) for every \(a\). However, it is _not_ possible to completely identify the polytopes \(_{}(a)\) with \((_{}(a))=0\). Indeed, if \((_{}(a_{i}))=0\), then \(_{}(a_{i})_{}(a_{j})\) for some other polytope \(_{}(a_{j})\) with positive volume. Thus, due to receiver's tie breaking, it could be impossible to identify the whole polytope \(_{}(a_{i})\). As a result, the Find-Polytopes procedure can output polytopes \(_{}(a)=_{}(a)\) only if \((_{}(a))>0\). However, we show that, if \((_{}(a))=0\), it is sufficient to guarantee that the polytope \(_{}(a)\) contains a suitable subset \(_{}(a)\) of the vertices of \(_{}(a)\); specifically, those in which the best response actually played by the receiver is \(a\). For every \(a\), such a set is formally defined as \(_{}(a)\{x V(_{}(a)) a(x )=a\}\). Thus, we design Find-Polytopes so that it returns a collection \(_{}\{_{}(a)\}_{a}\) of polytopes such that:

1. if it holds \((_{}(a))>0\), then \(_{}(a)=_{}(a)\), while
2. if \((_{}(a))=0\), then \(_{}(a)\) is a (possible improper) face of \(_{}(a)\) with \(_{}(a)_{}(a)\).

As a result each polytope \(_{}(a)\) can be either \(_{}(a)\) or a face of \(_{}(a)\), or it can be empty, depending on receiver's tie breaking. In all these cases, it is always guaranteed that \(_{}(a)_{}(a)\).

To achieve its goal, the Find-Polytopes procedure works by searching over the space of normalized slices \(_{}\), so as to learn _exactly_ all the separating hyperplanes \(H_{ij}\) characterizing the needed vertices.

The algorithm does so by using and extending tools that have been developed for a related learning problem in Stackelberg games (see [Bacchiocchi et al., 2024a]). Notice that our Bayesian persuasion setting has some distinguishing features that do _not_ allow us to use such tools off the shelf. We refer the reader to Appendix E for a complete description of the Find-Polytopes procedure.

A crucial component of Find-Polytopes is a tool to "query" a normalized slice \(x_{}\) in order to obtain the action \(a(x)\). This is done by using a sub-procedure that we call Action-Oracle (see Algorithm 5 in Appendix E), which works by committing to a signaling scheme including slice \(x\) until the signal corresponding to such a slice is actually sent. Under the clean event \(_{1}\), the set \(_{}\) is built in such a way that Action-Oracle returns the desired action \(a(x)\) in a number of rounds of the order of \(}{{}}\), with high probability. This is made formal by the following lemma.

**Lemma 2**.: _Under event \(^{1}\), given any \((0,1)\) and a normalized slice \(x_{}\), if the sender commits to a signaling scheme \(:\{s_{1},s_{2}\}\) such that \(_{}(s_{1})=x_{}\) for all \(\) during \(q:=(}{{}})\) rounds, then, with probability at least \(1-\), signal \(s_{1}\) is sent at least once._

Notice that the signaling scheme used to "query" an \(x_{}\) only employs _two_ signals: one is associated with slice \(x\), while the other crafted so as to make the signaling scheme well defined.

The following lemma provides the guarantees of Algorithm 6 in Appendix E.

**Lemma 3**.: _Given inputs \(_{}\) and \((0,1)\) for Algorithm 6, let \(L B+B_{}+B_{}\), where \(B\), \(B_{}\), and \(B_{}\) denote the bit-complexity of numbers \(_{}u_{}(a_{i})\), \(\), and \(\), respectively. Then, under event \(_{1}\) and with at probability at least \(1-\), Algorithm 6 outputs a collection \(_{}\{_{}(a)\}_{a}\), where \(_{}(a)\) is a (possibly improper) face of \(_{}(a)\) such that \(_{}(a)_{}(a)\), in a number of rounds \(T_{2}\):_

\[T_{2}}(}{}^{2}( )(d^{7}L+)).\]

For ease of presentation, we introduce the _clean event_ for phase 2 of Algorithm 1, defined as follows:

**Definition 3** (Phase 2 clean event).: \(_{2}\) _is the event in which \(_{}(a)_{}(a)\) for every \(a\)._

### Phase 3: Compute-Signaling

Given the collection of polytopes \(_{}\{_{}(a)\}_{a}\) returned by the Find-Polytopes procedure and an estimated prior \(_{t}_{}\), the Compute-Signaling procedure (Algorithm 3) outputs an approximately-optimal signaling scheme by solving an LP (Program (2) in Algorithm 3).

Program (2) maximizes an approximated version of sender's expected utility over a suitable space of (partially-specified) signaling schemes. These are defined by tuples of slices \((x^{a})_{a}\) containing an (unnormalized) slice \(x^{a}_{}^{}(a)\) for every receiver's action \(a\). The objective function being maximized by Program (2) accounts for the sender's approximate utility under each of the slices \(x^{a}\), where the approximation comes from the estimated prior \(_{t}\). The intuitive idea exploited by the LP formulation is that, under slice \(x^{a}\), the receiver always plays the same action \(a\) as best response, since \(a(x^{a})=a\) holds by the way in which \(_{}(a)\) is constructed by Find-Polytopes. In particular, each polytope \(_{}^{}(a)\) is built so as to include all the unnormalized slices corresponding to the normalized slices in the set \(_{}(a)\). Formally, for every receiver's action \(a\), it holds:

\[_{}^{}(a)\{x^{}  x= x^{} x^{}_{}(a) \}\{\},\]

where \(\) denotes the vector of all zeros in \(^{d}\). We observe that, since the polytopes \(_{}(a)\) are constructed as the intersection of some halfspaces \(_{ij}\) and \(_{}\), it is possible to easily build polytopes \(_{}^{}(a_{i})\) by simply removing the normalization constraint \(_{}x_{}=1\). Notice that, if \(_{}(a)=\), then \(_{}^{}(a)=\{\}\), which implies that action \(a\) is never induced as a best response, since \(x^{a}=\).

After solving Program (2) for an optimal solution \(x^{}(x^{,a})_{a}\), Algorithm 3 employs such a solution to build a signaling scheme \(\). This employs a signal \(s^{a}\) for every action \(a A\), plus an additional signal \(s^{}\), namely \(\{s^{}\}\{s^{a} a\}\). Specifically, the slice of \(\) with respect to \(s^{a}\) is set to be equal to \(x^{a}\), while its slice with respect to \(s^{}\) is set so as to render \(\) a valid signaling scheme (_i.e._, probabilities over signal sum to one for every \(\)). Notice that this is always possible thanks to the additional constraints \(_{a}x_{}^{a} 1\) in Program (2). Moreover, such a slice may belong to \(^{}_{}^{}\). Indeed, in instances where there are some \(^{}(a)\) falling completely outside \(_{}^{}\), this is fundamental to build a valid signaling scheme. Intuitively, one may think of \(s^{}\) as incorporating all the "missing" signals in \(\), namely those corresponding to actions \(a\) with \(^{}(a)\) outside \(_{}^{}\).

The following lemma formally states the theoretical guarantees provided by Algorithm 3.

**Lemma 4**.: _Given inputs \(_{}\{_{}(a)\}_{a}\), \(_{}\), and \(_{t}_{}\) for Algorithm 3, under events \(_{1}\) and \(_{2}\), the signaling scheme \(\) output by the algorithm is \((end+)\)-optimal for \(_{}_{t,}-_{} \)._

In order to provide some intuition on how Lemma 4 is proved, let us assume that each polytope \(_{}(a)\) is either empty or has volume larger than zero, implying that \(_{}(a)=_{}(a)\). In Appendix D, we provide the complete formal proof of Lemma 4, working even with zero-measure non-empty polytopes. The first observation the we need is that sender's expected utility under a signaling scheme \(\) can be decomposed across its slices, with each slice \(x\) providing a utility of \(_{}_{}x_{}u_{}^{}(a(x))\). The second crucial observation is that there always exists an optimal signaling scheme \(^{}\) that is _direct_ and _persuasive_, which means that \(^{}\) employs only one slice \(x^{a}\) for each action \(a\), with \(a\) being a best response for the receiver under \(x^{a}\). It is possible to show that the slices \(x^{a}\) that also belong to \(_{}\) can be used to construct a feasible solution to Program 2, since \(x^{a}_{}^{}(a)\) by definition. Thus, restricted to those slices, the signaling scheme \(\) computed by Algorithm 3 achieves an approximate sender's expected utility that is greater than or equal to the one achieved by \(^{}\). Moreover, the loss due to dropping the slices that are _not_ in \(_{}\) can be bounded thanks to point (ii) in Lemma 1. Finally, it remains to account for the approximation due to using \(_{t}\) instead of the true prior in the objective of Program 2. All the observations above allow to bound sender's expected utility loss as in Lemma 4.

## 5 Lower bounds for online Bayesian persuasion

In this section, we present two lower bounds on the regret attainable in the setting faced by Algorithm 1. The first lower bound shows that an exponential dependence in the number of states of nature \(d\) and the number of receiver's actions \(n\) is unavoidable. This shows that one cannot get rid of the binomial coefficient in the regret bound of Algorithm 1 provided in Theorem 1. Formally:

**Theorem 2**.: _For any sender's algorithm, there exists a Bayesian persuasion instance in which \(n=d+2\) and the regret \(R_{T}\) suffered by the algorithm is at least \(2^{(d)}\), or, equivalently, \(2^{(n)}\)._

Theorem 2 is proved by constructing a collection of Bayesian persuasion instances in which an optimal signaling scheme has to induce the receiver to take an action that is a best response only for a unique posterior belief (among those computable by the receiver at step (3) of the interaction). This posterior belief belongs to a set of possible candidates having size exponential in the number of states of nature \(d\). As a result, in order to learn such a posterior belief, any algorithm has to commit to a number of signaling schemes that is exponential in \(d\) (and, given how the instances are built, in \(n\)).

The second lower bound shows that the regret bound attained by Algorithm 1 is tight in \(T\).

**Theorem 3**.: _For any sender's algorithm, there exists a Bayesian persuasion instance in which the regret \(R_{T}\) suffered by the algorithm is at least \(()\)._

To prove Theorem 3, we construct two Bayesian persuasion instances with \(=\{_{1},_{2}\}\) such that, in the first instance, \(_{_{1}}\) is slightly greater than \(_{_{2}}\), while the opposite holds in the second instance. Furthermore, the two instances are built so that the sender does _not_ gain any information that helps to distinguish between them by committing to signaling schemes. As a consequence, to make a distinction, the sender can only leverage the information gained by observing the states of nature realized at each round, and this clearly results in the regret being at least \(()\).

The sample complexity of Bayesian persuasion

In this section, we show how the no-regret learning algorithm developed in Section 4 can be easily adapted to solve a related _Bayesian persuasion PAC-learning problem_. Specifically, given an (additive) approximation error \((0,1)\) and a probability \((0,1)\), the goal of such a problem is to learn a \(\)-optimal signaling scheme with probability at least \(1-\), by using the minimum possible number of rounds. This can be also referred to as the _sample complexity_ of learning signaling schemes.

As in the regret-minimization problem addressed in Section 4, we assume that the sender does _not_ know anything about both the prior distribution \(\) and receiver's utility function \(u\).

We tackle the Bayesian persuasion PAC-learning problem with a suitable adaptation of Algorithm 1, provided in Algorithm 4. The first two phases of the algorithm follow the line of Algorithm 1, with the Build-Search-Space and Find-Polytopes procedures being called for suitably-defined parameters \(,\), \(T_{1}\), and \(\) (taking different values with respect to their counterparts in Algorithm 1). In particular, the value of \(\) depends on \(\) and is carefully computed so as to control the bit-complexity of numbers used in the Find-Polytopes procedure (see Lemma 3), as detailed in Appendix G. Finally, in its third phase, the algorithm calls Compute-Signaling to compute a signaling scheme \(\) that can be proved to \(\)-optimal with probability at least \(1-\).

The most relevant difference between Algorithm 4 and Algorithm 1 is the number of rounds used to build the prior estimate defining \(_{}\). Specifically, while the latter has to employ \(T_{1}\) of the order of \(}{{}}\) and rely on a multiplicative Chernoff bound to get tight regret guarantees, the former has to use \(T_{1}\) of the order of \(}{{^{2}}}\) and standard concentration inequalities to get an \(()\)-optimal solution. Formally:

**Lemma 5**.: _Given \(T_{1}}(}{{ }})\) and \((0,1)\), Algorithm 2 employs \(T_{1}\) rounds and outputs \(_{}\) such that, with probability at least \(1-\): (i) \(_{}_{}x_{}\) for every slice \(x_{}\), (ii) \(_{}_{}x_{} 6\) for every slice \(x_{}\), and (iii) \(|_{}-_{}|\) for every \(\)._

By Lemma 5, it is possible to show that the event \(^{1}\) holds. Hence, the probability that a signaling scheme including a slice \(x_{}\) actually "induces" such a slice is at least \(\), and, thus, the results concerning the second phase of Algorithm 1 are valid also in this setting. Finally, whenever the events \(_{1}\) and \(_{2}\) hold, we can provide an upper bound on the number of rounds required by Algorithm 4 to compute a \(\)-optimal signaling scheme as desired. Formally:

**Theorem 4**.: _Given \((0,1)\) and \((0,1)\), with probability at least \(1-\), Algorithm 4 outputs a \(\)-optimal signaling scheme in a number of rounds \(T\) such that:_

\[T}(}{^{2}}^{2}( {1}{})(d^{}B+d)).\]

We conclude by providing two negative results showing that the result above is tight.

**Theorem 5**.: _There exist two absolute constants \(,>0\) such that no algorithm is guaranteed to return a \(\)-optimal signaling scheme with probability of at least \(1-\) by employing less than \(2^{(n)}\) and \(2^{(d)}\) rounds, even when the prior distribution \(\) is known to the sender._

**Theorem 6**.: _Given \((0,}{{8}})\) and \((0,1)\), no algorithm is guaranteed to return a \(\)-optimal signaling scheme with probability at least \(1-\) by employing less than \(}(}{{}})\) rounds._

In Appendix H, we also study the case in which the prior \(\) is known to the sender. In such a case, we show that the sample complexity can be improved by a factor \(}{{}}\), which is tight.