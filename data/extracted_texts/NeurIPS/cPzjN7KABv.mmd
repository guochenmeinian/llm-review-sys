# Private Geometric Median

Mahdi Haghifam

Khoury College of Computer Sciences, Northeastern University. Supported by a Khoury College of Computer Sciences Distinguished Postdoctoral Fellowship. m.haghifam@northeastern.edu

Thomas Steinke

Google DeepMind.

Khoury College of Computer Sciences, Northeastern University. Supported by NSF awards CNS-2232692 and CNS-2247484.

Jonathan Ullman

Hookeyan College of Computer Sciences, Northeastern University. Supported by NSF awards CNS-2232692 and CNS-2247484.

one individual, and we are interested in approximately solving the following optimization problem:

\[^{}(^{(n)})_{^{d}}F(;^{(n)}), F(;^{(n)}) _{i[n]}-x_{i}_{2}.\] (1)

The geometric median generalizes the standard one-dimensional median. The geometric median is a useful tool for robust estimation and aggregation, because it is less sensitive to outliers than the mean of the data, i.e., it is a nontrivial estimator even when \( 49\%\) of the input data is arbitrarily corrupted. These properties make GM a popular tool for designing robust versions of distributed optimization methods [13; 14; 15; 16], boosting the confidence of weakly concentrated estimators , clustering , etc.

**Baseline for Private GM.** Since the geometric median is the minimizer of a Lipschitz convex loss function, we can privately approximate it using the standard approach of DP-(S)GD. In particular, if we know a priori that all the data points lie in a known ball of radius \(R\) (without loss of generality this ball is centered at the origin, i.e., \( x_{i}_{2} R\) for every \(i[n]\)), then DP-(S)GD guarantees \((,)\)-DP with the following excess error :

\[F(_{n}(^{(n)});^{(n)})-F(^{}; ^{(n)})=O}{}.\] (2)

As discussed in the beginning of this section, this guarantee has a significant drawback: the excess error of the algorithm depends _linearly_ on the radius \(R\) of the a priori bound on the data. This bound could be very loose; it does not scale with the data. Can we do better? What quantity should the excess error guarantee scale with?

It is known that the GM is inside the convex hull of the datapoints. However, this convex hull can have a very large diameter due to a small number of _outliers_, while _most_ of the datapoints live in a ball with a small diameter. A key property of GM is robustness to outliers, so we want our accuracy guarantee to also be robust to some outliers. Specifically, if \( 51\%\) of the points lie in a ball of diameter \( R\) then the geometric median is \(O()\) far from that ball (see Lemma C.6 for a more precise statement). Thus, we aim to design a DP algorithm whose error is proportional to the actual scale of the majority of the data, rather than the a priori worst-case bound. However, the algorithm designer does not have a priori knowledge of the location or diameter of a ball that contains most of the data; the algorithm must discover this information from the data. This prompts the following question: _Can we design an efficient and private algorithm with an excess error guarantee that scales with the radius that contains majority of the datapoints?_ Our results provide a positive answer.

### Contributions

Our main contribution is a pair of _polynomial-time_ DP algorithms for approximating the geometric median with an excess error guarantee that scales with the effective diameter of the datapoints. Also, the sample complexity and the runtime of our algorithms depend logarithmically on the a priori bound \(R\). Both of our algorithms achieve the same excess error bounds up to logarithmic factors, but have incomparable running times. We also give a simple numerical experiment on synthetic data as a proof of concept that our algorithm improves over DP-(S)GD, as predicted by the theory. In

 
**Algorithm** & **Privacy** & **Utility [\(F(_{n}(^{(n)});^{(n)})\)]** & **Run-time** & **Samples** \\   LocDPSGD \\ (Section 3.1) \\  & Approx & \((1+}{n})\)OPT & \((R/)}\) & \(+n^{2}d\) & \(}{}\) \\   LocDPCuttingPlane \\ (Section 3.2) \\  & Approx & \((1+}{n})\)OPT & \((R/)}\) & \(+nd^{2}+d^{2+}\) & \(}{}\) \\  
 SInvS \\ (Section 4) \\  & Pure & \((1+)\)OPT & Exponential & \(\) \\  Baseline: DP-(S)GD & Approx & \(+}{}\) & \(n^{2}d\) & N/A \\  

Table 1: Summary of our results. Here \(=_{^{d}}F(;^{(n)})\) denotes the optimal loss and \(\) is the matrix-multiplication exponent. The highlighted part is the runtime of the warm-up phase which is the same for LocDPSGD and LocDPCuttingPlane. We also assume that \(_{i[n]}|^{(n)}_{d}(x_{i},r)|<3n/4\). (See Section 3.1, Section 3.2, and Section 4 for the general results without this restriction.) For readability, we omit logarithmic factors that depend on \(n\) and \(d\).

terms of optimality, we show the our proposed algorithms is optimal in terms of sample complexity. Furthermore, we propose an algorithm based on the _inverse smooth sensitivity_ mechanism for the private geometric median problem that satisfies the more restrictive notion of _pure_ DP. Below, we give an overview of these algorithms and the techniques involved.

**Polynomial-Time Algorithms.** Both of our algorithms for the private geometric median problem are two-phase algorithms: in the first phase, which we refer to as _warm-up_, the algorithm shrinks the feasible set to a ball whose diameter is proportional to what we call the _quantile radius_ in time that depends logarithmically on \(R\). The second phase, which we call _fine-tuning_, uses the output of the warm-up algorithm to further improve the error.

First, we formalize the notion of the quantile radius as the radius of the smallest ball containing sufficiently many points.

**Definition 1.1** (Quantile Radius).: Fix a dataset \(^{(n)}=(x_{1},,x_{n})(^{d})^{n}\) and \(^{d}\). For every \(\), define \(_{ n}()\{:|i[n]:\|x_{i}-\| | n\}\).

To motivate the idea behind our algorithms, assume the algorithm designer _knew_ a ball, with center \(_{0}\) and radius \(\) such that \(\|_{0}-^{}\| O()\) and \(=(_{4n/5}(^{}))\). Then, running DP-(S)GD over this ball would give excess error \(O(_{4n/5}(^{})/)\). This guarantee is particularly interesting as the excess error scales with the quantile radius and not the largest possible norm of any point. Also, by definition of the quantile radius and the geometric median loss function, we have that \(F(^{};^{(n)})(1-)n_{ n}(^{ })\). This inequality shows that an algorithm whose excess error depends on \(_{ n}(^{})\) has a _multiplicative guarantee_ rather than the standard additive guarantee for DP-(S)GD. This type of guarantee is particularly desirable for the geometric median since an algorithm with a multiplicative guarantee will be scale free and be adaptive to the niceness of the dataset. However, since we do not know such a pair \(_{0}\) and \(\) a priori, the objective of the warm-up algorithm is to privately find these quantities.

The warm-up algorithm is based on the following structural result: given a point \(\) that satisfies \(\|-^{}\|_{3n/4}(^{})\), we have \(F(;^{(n)})-F(^{};^{(n)})\|- ^{}\|\). (See Lemma 2.6 for a formal statement.) This result implies that, even though \(F(;^{(n)})\) is not a strongly convex function, we have a _growth condition_ such that the excess error increases with the distance to the global minimizer, at least when the excess error is large enough. (In contrast, strong convexity would imply quadratic growth \(F(;^{(n)})-F(^{};^{(n)})\|- ^{}\|^{2}\), rather than linear growth.) Intuitively, this growth condition allows us to take larger step sizes and make progress faster, consuming less of the privacy budget. However, since this growth condition only holds for \(\) that is more than \(_{3n/4}(^{})\) away from the minimizer, which is a data-dependent property, we first need to develop a private algorithm to estimate \(_{3n/4}(^{})\) in order to make use of this property. In Section 2.1, we develop an efficient algorithm, RadiusFinder, for this task, which is inspired by . Our procedure assumes that we know some potentially very small lower bound \(r_{3n/4}(^{})\), which is necessary by the impossibility results in . Since the sample complexity of this procedure depends only on \((1/r)\), we can choose this parameter to be very small. In Section 2.1, we show how to eliminate this assumption at the cost of a small additive error. With high probability, RadiusFinder (see Theorem 2.4) outputs \(\) such that \(_{3n/4}(^{}) O(_{4n/5}(^{ }))\). Having obtained \(\), the second step of the warm-up algorithm is finding a good initialization point. In Section 2.2, we propose Localization, based on DP-GD with _geometrically decaying step sizes_, to perform this task. Due to the growth condition we show that DP-GD makes a fast progress towards some point that is within \(O(_{4n/5}(^{}))\) from the optimizer: in \((R)\) iterations, with high probability, it outputs \(_{0}\) such that \(^{}\) is in the ball of radius \(O()=O(_{4n/5}(^{}))\) centered at \(_{0}\).

**DP Cutting Plane Method for Private GM.** The main drawback of using DP-SGD for the fine-tuning stage is that its run-time can be large when \(n d\). To address this, we design the second fine-tuning algorithm, LocDPCuttingPlane, based on private variant of the cutting plane method that has faster running time when \(n\) is large. There are two challenges in the analysis: by using the noisy gradients, we cannot argue that the optimal point always lives in the intersection of the cutting planes, which is a crucial part of the standard analysis. The second challenge is that the cutting plane method is not a _descent_ method in the sense that the loss function is not decreasing with the iteration, and we need to privately select an iterate with small loss. The challenge for developing the private variant here is that the loss \(F(;^{(n)})\) has sensitivity proportional to \(R\), so running the exponential mechanism in the natural way incurs loss proportional to \(R\). We address both of these challenges and develop an algorithm whose excess error is proportional to \(_{4n/5}(^{})\).

**Pure DP algorithm for Private Geometric Median Problem.** In Section 4, we propose a pure \((,0)\)-DP algorithm for the geometric median problem, albeit a computationally inefficient one. Our algorithm is based on the _inverse smooth sensitivity_ mechanism of . At a high level, the algorithm outputs \(^{d}\) with a probability proportional to \((-(^{(n)},)/2)\) where \((^{(n)},)\) is the minimum number of data points from \(^{(n)}\) that needs to be modified to obtain a dataset \(}^{(n)}\) such that the geometric median of \(}^{(n)}\) be equal \(\). Our analysis shows that the proposed mechanism outputs \(=(}^{(n)})\) such that \(}^{(n)}\) and \(^{(n)}\) differ in at most \(k^{}=O(d(R)/)\) with a high probability. Then, by a careful sensitivity analysis, we show \(\|-^{}\|\) can be upper bounded by the \(F(^{};^{(n)})\). Using this result we provide an algorithm with a multiplicative guarantee. Moreover, we show \(\|-^{}\|\) is upper bounded \(O(_{ n}(^{}))\) for some \((1/2,1]\).

**Lower bound on the Sample Complexity.** We show every \((,)\)-DP algorithm requires \((/)\) samples so that it satisfies \(_{_{n}(^{(n)})}[F(;^{(n)})](1+)_{^{d}}F(,^{(n)})\) for a constant \(\). This result shows that the sample complexity of our polynomial-time algorithms is nearly optimal.

A summary of the results is provided in Table 1, comparing the proposed algorithms in terms of privacy, utility, runtime, and sample complexity. As discussed earlier, algorithms with error adaptive to the quantile radius can achieve a nearly multiplicative guarantee. The utility column in Table 1 compares the algorithms based on the achievable \(_{}\) and \(_{}\) in order to \(F(_{n}(^{(n)});^{(n)})(1+_{})F(^{};^{(n)})+_{}\) with a high probability.

### Related Work

DP convex optimization is a well-studied problem . There has been significant interest in developing new algorithms that offer improved guarantees compared to DP-(S)GD for specific problem classes or by leveraging additional information. For instance,  demonstrate that for linear models the dependency of the excess error on the dimension can be improved,  study the impact of the second-order information on the convergence,  explore the impact of public data, etc. The current paper addresses a drawback of DP-(S)GD, namely, the linear dependence of the excess error on the distance from the initializer to the optimal point in non-strongly convex settings.

Another related line of work to our warm-up strategy is private averaging of . The advantage of the algorithm proposed in this work is its simplicity while being optimal in terms of sample complexity: we exploit a structural property of the geometric median and show that running DPGD with the geometrically decaying stepsizes can yield a suitable initialization point without the need for preprocessing steps such as filtering , coordinate-wise discretization , hashing , etc. The proposed quantile radius can be seen as a robust notion of radius proposed in .

In one dimension (i.e., \(d=1\)), private versions of the median are well studied . In particular, these works improve the dependence on the a priori bound \(R\) to \(^{*}R\), rather than \( R\) in our results.

### Notation

Let \(d\). For a vector \(x^{d}\), \(\|x\|\) denotes the \(_{2}\) norm of \(x\). We use the following notation for the ball of radius \(R\): \(_{d}(a,R)=\{x^{d}:\|x-a\| R\}\). Also, \(_{d}^{}(a,R)\) denotes \(\{x^{d}:\|x-a\|_{} R\}\). We refer to \(_{d}(0,R)=_{d}(R)\), similarly, it holds for \(_{d}^{}(0,R)=_{d}^{}(R)\). Let \(,\) denote the standard inner product in \(^{d}\). For a convex and closed subset \(^{d}\), let \(_{}:^{d}\) be the Euclidean projection operator, given by \(_{}(x)=*{arg\,min}_{y}\|y-x\|_{2}\). For a (measurable) space \(\), \(_{1}()\) denotes the set of all probability measures on \(\). Let \(\) be the data space and let \(^{d}\) be the parameter space. Let \(f:\) be a loss function. We say \(f\) is _L-Lipschitz_ iff there exists \(L\) such that \( z\), \( w,v:|f(w,z)-f(v,z)| L\|w-v\|\).

### Notions of DP

**Definition 1.2**.: Let \(>0\) and \([0,1)\). A randomized mechanism \(_{n}:^{n}_{1}()\) is \((,)\)-DP, iff, for every neighbouring dataset (i.e., replacement) \(^{n}\) and \(^{}^{n}\), and for every measurable subset \(M\), it holds \(_{_{n}()}( M) e^{ }_{_{n}(^{})}(  M)+\).

For some of our privacy analysis, we use concentrated differential privacy , as it provides a simpler composition theorem - the privacy parameter \(\) adds up when we compose.

**Definition 1.3** ([17, Def. 1.1]).: A randomized mechanism \(:^{n}_{1}()\) is \(\)-zCDP, iff, for every neighbouring dataset (i.e., replacement) \(^{n}\) and \(^{}^{n}\), and for every \((1,)\), it holds \(_{}(_{n}()\|_{n}(^{ })),\) where \(_{}(_{n}()\|_{n}(^{ }))\) is the \(\)-Renyi divergence between \(_{n}()\) and \(_{n}(^{})\).

We should think of \(^{2}\): to attain \((,)\)-DP, it suffices to set \(=}{4(1/)+4}\)[17, Lem. 3.5].

**Lemma 1.4** ([17, Prop. 1.3]).: _Assume we have a randomized mechanism \(:_{1}()\) that satisfies \(\)-zCDP, then for every \(>0\), \(\) is \((+2,)\)-DP._

## 2 Private Localization

In this section, we present the proposed algorithm for the warm-up stage; it has two steps: _Private Estimation of Quantile Radius_ and _Private Localization_.

### Step 1: Private Estimation of Quantile Radius

Algorithm 1 describes our private algorithm RadiusFinder for quantile radius estimation.

```
1:Inputs: data set \(^{(n)}(_{d}(R))^{n}\), fraction \((1/2,1]\), privacy budget \(\)-zCDP, failure probability \(\), discretization error \(0<r<R\).
2:\(m= n\).
3:For every \( 0\) and \(i[n]\), let \[N_{i}() |^{(n)}_{d}(x_{i},)|.\] \[$}\]
4:For every \( 0\), define \[N()_{\{i_{1},,i_{m}\}[n ]}\{N_{i_{1}}()++N_{i_{m}}()\}.\]
5:\(=\{r,2r,4r,2^{()}r\}.\)
6:\(=\{N(v):v\}\)
7:\[=,,m+}\]
8:Output \(=2^{}r\) if \(\); else Output Fail. ```

**Algorithm 1**RadiusFinder\({}_{n}\)

_Remark 2.1_.: The runtime of RadiusFinder is \(((n^{2}+n(n))( R/r))\): First, we need to compute the pairwise distances which take \(n^{2}\) time. Then, for a fixed \(\), we can compute \(N()\) using the pairwise distances in time \((n^{2})\). To compute \(N()\), we need to sort \(\{N_{i}()\}_{i[n]}\), in \((n(n))\) time, and pick top \(m\). Finally, we need to repeat this for each \([r,,2^{()}r]\). \(\)

Notice that Algorithm 1 uses the datapoints as centers for computing the number of the datapoints in a given distance. The privacy proof of Algorithm 1 is based on the following lemma.

**Lemma 2.2**.: _Fix \(n\). For every dataset \(^{(n)}\), for every \(1/2 1\) and for every fixed \(\), the query \(N()_{\{i_{1},,i_{m}\}[n]}\{N_{i_{1}}( )++N_{i_{m}}()\},\) has a sensitivity upper-bounded by \(3\) where \(m= n\) and \(N_{i}()|^{(n)}_{d}(x_{i},)|\). Here \(_{d}(x,):=\{y^{d}:\|y-x\|\}\)._The objective of Algorithm 1 is to privately approximate \(_{ n}(^{})\). Nonetheless, Algorithm 1 relies on computing the pairwise distances between datapoints. The following lemma elucidates why computing these pairwise distances serves as an effective proxy for computing \(_{ n}(^{})\).

**Lemma 2.3**.: _Fix \(n\), \(1 m n\), \(_{1},_{2}(1/2,1]\) such that \(_{2}_{1}\), and dataset \(^{(n)}\). For every \( 0\), define \(N()_{\{i_{1},,i_{m}\}[n]}\{N_{i_{1} }()++N_{i_{m}}()\},\) where \(N_{i}()|^{(n)}_{d}(x_{i},)|\). Let \(^{}=(^{(n)})\). For every \(\) such that \(N()_{1}n\) and \(N(/2)<_{2}n\), we have_

\[_{_{1}n}(^{})-n}{4_{1}-1}  4_{_{2}n}(^{}).\]

Using these two lemmas, in the next theorem, we present the privacy and utility guarantees of Algorithm 1. As we are interested in finding the smallest radius, we use the standard AboveThreshold from  as a subroutine in Algorithm 1. The algorithmic description of AboveThreshold is provided in Appendix B for completeness.

**Theorem 2.4**.: _Let \(_{n}\) denote Algorithm 1. Fix \(d\), \(R>0\), \(r>0\), \((0,1]\), and \(>0\). Then, for every \(n\) and every dataset \(^{(n)}(_{d}(R))^{n}\) the output of \(_{n}\) satisfies \(\)-zCDP. Also, the output of \(_{n}\) satisfies the following utility guarantees:_

1. _Given_ \(n>}(4/)\)_, then_ \(_{ n}(^{})  1-\)_._
2. _Assume that the data points satisfies_ \(N(r)<m\)_. Let_ \(\{+} 2+1/ ,1\}\)_, then, given_ \(n>}(4/)\)_, we have_ \[_{ n}(^{}) 4_{ n}(^{}) 1- .\]
3. _Let_ \(\{+} 2+1/ ,1\}\)_. Given_ \(n>}(4/)\)_, we have_ \[_{ n}(^{}) 4_{n}(^{})=r} 1-2.\]

_Remark 2.5_.: A sufficient condition for \(N(r)<m\) in Item 2 is that \(_{i[n]}|^{(n)}_{d}(x_{i},r)|<m= n\). Intuitively, this means that no data point should have a significant portion of other data points within a ball of radius \(r\) centered on it. \(\)

### Step 2: Fast Localization

In the second step of the warm-up phase, we develop a fast algorithm for finding a good initialization point using the private estimate of the quantile radius. The main structural result that we use for the algorithm design is stated in the next lemma.

**Lemma 2.6**.: _Fix \(n\), \(^{(n)}(^{d})^{n}\) and \(_{1},_{0}^{d}\). For every \(\), define \(_{ n}(_{0})\{r 0:|i[n]:\|x_{i}-_{0} \| r\| n\}\). Assume there exists \( 0\) such that \(F(_{1};^{(n)})-F(_{0};^{(n)}) n\). Then, for every \((1/2,1]\), we have_

\[(2-1)\|_{1}-_{0}\|-2_{ n}(_{0})\]

To gain some intuition behind Lemma 2.6, let us instantiate \(_{0}=^{}\). This result implies that for a \(^{d}\) such that \(\|-^{}\|_{ n}(^{})\), the loss function of the geometric median satisfies \(F(;^{(n)})-F(^{};^{(n)})\|- ^{}\|\). Using this result, we propose Algorithm 2 for finding a good initialization. The next theorem states the privacy and utility guarantees of Algorithm 2.

**Theorem 2.7**.: _Let \(_{n}\) denote Algorithm 2. Fix \(d\), \(R>0\), \(r>0\), \(>0\), and \((0,1)\). Then for every dataset \(^{(n)}(_{d}(R))^{n}\) the outputs of \(_{n}\) satisfies \(\)-zCDP. Moreover, let \((,)=_{n}(^{(n)},,r,)\) and define random set \(_{}=\{_{d}(R):\|- \| 25\}.\) Then, given_

\[n}{}},}} ,\]we have \(^{}_{}_{0.75n}(^{}) 4  4_{0.8n}(^{})=r} 1-2\). Also, assuming that the datapoints satisfies \(_{i[n]}^{(n)}_{d}(x_{i},r)<3n/4\), we have_

\[^{}_{}_{0.75n}(^{}) 4  4_{0.8n}(^{}) 1-2.\]

## 3 Private Fine-tuning

In Section 2, we developed an algorithm for the warm-up stage. The output of the warm-up stage is \(_{0}\) and radius \(\) such that \(\|_{0}-^{}\| O()\) and \(=(_{4n/5}(^{}))\) as formalized in Theorem 2.7. In this section, we build upon the output of the warm-up algorithm to develop two polynomial-time algorithms for the fine-tuning stage.

### Fine-tuning Using DPGD

Our first algorithm is based on DP-GD . The main ideas behind Algorithm 3 is as follows: 1) from the utility guarantee of the warm-up phase in Theorem 2.7, the distance of the initialization and \(^{}\) only depends on \(\), i.e., it does not depend on \(R\), 2) By definition of the quantile radius in Definition 1.1 and Equation (1), we have that \(F(^{};^{(n)})(1-)n_{ n}(^{ })\), 3) in the case that the data satisfies some regularity conditions, we have \( 4_{0.8n}(^{})\) from Theorem 2.7. The next theorem summarizes the utility and privacy guarantees of this algorithm.

```
1:Inputs: dataset \(^{(n)}(_{d}(R))^{n}\), privacy parameters \(\)-zCDP, discretization error \(r\), failure probability \(\).
2:\(_{0},=_{n}^{(n)}, ,r,\)\(\) Algorithm 2
3:\(_{0}=\{_{d}(R):\|-_{0}\| 25\}\)
4:\(_{h}=50}}\) and \(T_{h}=}{256d}\)
5:\(=_{0},^{(n)},,_{0},_{h},T_{h}\)\(\) Algorithm 6
6:Output \(\) ```

**Algorithm 3**LocDPGD\({}_{n}\)

**Theorem 3.1**.: _Let \(_{n}\) denote Algorithm 3. For every \(d\), \(R>0\), \(r>0\), \(>0\), and \((0,1]\), \(=\{_{n}\}_{n 1}\) satisfies the following: for every \(n\) and every dataset \(^{(n)}(_{d}(R))^{n}\) the output of \(_{n}\) satisfies \(\)-\(z\)-\(\). Also, given_

\[n}{})},}\!()},\]

_we have_

\[F\!(;^{(n)})1+O }{n}F\! (^{};^{(n)})+O}r 1-2.\]

_Moreover, given that the datapoints satisfies \(_{i[n]}^{(n)}_{d}(x_{i},r)<3n/4\), we have_

\[F\!(;^{(n)})1+O }{n}F\! (^{};^{(n)}) 1-2,\]

_where \(\) is the output of Algorithm 3._

### Fine-tuning Using Noisy Cutting Plane Method

In this section, we present the second fine-tuning algorithm: \(\) of Algorithm 4. This algorithm is based on the well-known cutting plane method .

```
1:Inputs: dataset \(^{(n)}(_{d}(R))^{n}\), privacy parameters \((,)\)-DP, discretization error \(r\), failure probability \(\)
2:\(=}{16(2/3)+8}\)
3:\(_{0},=_{n}\!(^{(n)}, ,r,\{,\})\)\(\) Algorithm 2
4:\(_{0}=\{_{d}(R):-_{0} 25 \}\)
5:\(k_{}=\!( }{}+)\)\(\) See Assumption 1 for definition of \(\)
6:for\(t\{0,,k_{}-1\}\)do\(_{t}=(_{t})\)\(\) See Assumption 1
7:\(_{,t}0,}}{2}_{d} \)\(\) See Assumption 1
8:\(_{t+1}=_{t} F(_{t}; ^{(n)})+_{,t},-_{t}<0}\)
10: Define Probability Measure: \((t)\!(-}F_{ t};^{(n)})\) for \(t\{0,,k_{}-1\}\)
11:Output \(_{i}\) where \(\) ```

**Algorithm 4**\(_{n}\)

Similar to non-private cutting plane method, \(\) is not a descent algorithm. As a result, we need to devise a mechanism for selecting an iterate with minimal loss. In the next lemma, we provide a bespoke analysis of the exponential mechanism with the score function \(F(;^{(n)})\) defined in Equation (1). Note that the sensitivity of \(F(;^{(n)})\) is \(R\). However, the next result demonstrates that through a novel analysis of the sensitivity of \(F(;^{(n)})\), the noise scale due to privacy can be significantly reduced. Proof can be found in Appendix E.

**Lemma 3.2**.: _Let \(\), \(k\), and \(d\) be constants. Let \(^{d}\) be a set with a bounded diameter of \(\). Let \(^{(n)}(^{d})^{n}\) be a dataset and \(^{}(^{(n)})\). Let \(\{_{1},,_{k}\}\) be \(k\) fixed vectors. Also, assume that \(^{}\). Let \(\) be such that \(3_{3n/4}(^{})+2\). Consider the following probability measure over \(\{1,,k\}\):_

\[(i;^{(n)})=F( _{i};^{(n)}))}{_{j[k]}\!(-F(_{j};^{(n)}))}, i[k].\]

1. _Let_ \((;^{(n)})\) _and OPT_ \(_{i[k]}\{F(_{i};^{(n)})-F(^{}; ^{(n)})\}\)_. Then, for every_ \((0,1]\)_, we have_ \[F(_{i};^{(n)})-F(^{};^{ (n)})+(k/) 1-.\]2. _Let_ \(}^{(n)}\) _be a dataset of size_ \(n\) _that differs in one sample from_ \(^{(n)}\)_. Then, for every_ \(i[k]\)_, we have_ \[(-)(i;}^{(n)})(i;^{(n)}) ()(i;}^{(n)}).\]

The next theorem provides the privacy guarantee of Algorithm 4. The privacy analysis differs from the rest of the algorithms in the paper. This deviation arises from the fact that for analyzing the privacy guarantee of Line 10 of Algorithm 4, we use Lemma 3.2. Notice that the guarantee in Lemma 3.2 holds provided that \(_{0}\), defined in Line 4 of Algorithm 4, satisfies \(^{}_{0}\). Ergo, the privacy guarantee of Algorithm 4 only satisfies _approximate-DP_.

**Theorem 3.3**.: _Let \(_{n}\) denote Algorithm 4. Fix \(d\), \(R>0\), \(r>0\), \(>0\), \((0,1]\), and \((0,1]\). Then, for every \(n\) and every dataset \(^{(n)}(_{d}(R))^{n}\) the output of \(_{n}\) satisfies \((,)\)-DP._

We also make the following assumption about the performance of Centre subroutine in Algorithm 4.

**Assumption 1**.: _There exists some \((0,1]\) such that for all \(t\{0,,k_{}}-1\}\), the subroutine of_ Centre _in Algorithm 4 satisfies \((_{t+1})(1-)(_{t})\). Furthermore, the time for calling the routine_ Centre _is \(T_{c}\)._

Using the John Ellipsoid  as the Centre makes \(\) a dimension independent constant and \(T_{c}=(d^{1+})\) (by ). Now we are ready to state the utility guarantee of Algorithm 4.

**Theorem 3.4**.: _Let \(_{n}\) denote Algorithm 4. For every \(d\), \(R>0\), \(r>0\), \(>0\), \((0,1]\), and \((0,1]\), \(=\{_{n}\}_{n 1}\) satisfies the following: for every \(n\) and every dataset \(^{(n)}(_{d}(R))^{n}\), given_

\[n}{}},}} ,\]

_where \(=}{16(2/)+8}\), we have the following: Let \(}{}+\) and \(=O}\). Then,_

\[F;^{(n)}1+ F(^{};^{(n)})+r 1-3,\]

_Moreover, assuming that the datapoints satisfies \(_{i[n]}^{(n)}_{d}(x_{i},r)|<3n/4\), we have_

\[F;^{(n)}1+ F(^{};^{(n)}) 1-3,\]

_where \(\) is the output of Algorithm 4._

## 4 Pure-DP Algorithm for Geometric Median

In this section, we propose an algorithm based on the assumption that we have an access to an oracle that outputs an _exact_\(^{(n)}\). Before presenting the algorithm, we need a definition: For two sequences of \(=(a_{1},,a_{n})(^{d})^{n}\) and \(=(b_{1},,b_{n})(^{d})^{n}\), we define the hamming distance as \(_{}(,)=_{i=1}^{n}[a_{i} b_{i}]\). The proposed algorithm is shown in Algorithm 5, and its utility and privacy guarantees are presented in the following theorem.

**Theorem 4.1**.: _Let \(_{n}\) denote the algorithm in Algorithm 5. Fix \(d\), \(R>0\), \(r>0\), and \(>0\). Then, for every \(n\) and every dataset \(^{(n)}(_{d}(R))^{n}\) the output of \(_{n}\) satisfies \(\)-DP. Also, for every \((0,1)\) and for every \(n>2k^{} 2((1/)+d( R/r))\), with probability at least \(1-\), we have:_

1. _The value of the cost function satisfies_ \[F(;^{(n)})1+}{n-2k^{}} F(^{};^{(n)})+nr.\]
```
1:Input: dataset \(^{(n)}(_{d}(R))^{n}\), privacy parameter \(\)-DP, discretization error \(r\).
2:For every \(y_{d}(R)\)

\[_{r}(,y)_{}(^{ d})^{n}}\{_{}(^{(n)},}^{(n)}) z_{d}(y,r)(}^{(n)})=z\}\]
3:Define density: \(d(y)=-_{r}( ,y)}{_{y_{d}(R)}-_{r}(,y)\,dy}[y _{d}(R)]\)
4:Output \(\) ```

**Algorithm 5**\(_{n}\)

#### 4.2.2 In terms of distance,

\[\|-^{}\| r+_{(1/2,1]: >}{n}+}(^{ })}{/n)-(-k^{}/n)^{2}}}.\]

The proof of Theorem 4.1 is provided in Appendix F. The proof is based on showing that the output \(=}^{(n)}\) is such that \(}^{(n)}\) and \(^{(n)}\) differ in at most \(k^{}=O(d(R)/)\) datapoints with a high probability. Then, we use the properties of the geometric median to show that the sensitivity of GM to changing \(k<n/2\) points can be bounded by the value of the optimal loss at \(^{}=(^{(n)})\).

**Lemma 4.2**.: _For every \(n\) and for every \(k<\), and for every \((x_{1},,x_{n},y_{1},,y_{k})(^{d})^{n+k}\), define \(_{0}=((x_{1},,x_{n}))\) and \(_{k}=((x_{1},,x_{n-k},y_{1},,y_{k}))\). Then, \(\|_{k}-_{0}\|F(_{0};(x_{1}, ,x_{n})).\)_

## 5 Lower Bound on the Sample Complexity

In this section we prove a lower bound on the sample complexity of any \((,)\)-DP algorithm for the task of private geometric median with a multiplicative error.

**Theorem 5.1**.: _Let \(_{0},_{0},d_{0}\) be universal constants. Then, for every \(_{0}\), \(_{0}\), and \(d d_{0}\) and every \((,)\)-DP algorithm \(_{n}:(^{d})^{n}_{1}(^{d})\) (with \(=(/n)\)) such that for every dataset \(^{(n)}(^{d})^{n}\) its output satisfies \(_{_{n}(^{(n)})}F ;^{(n)}(1+)_{ _{d}^{}(1)}F(;^{(n)})\), we require \(n=}{}\)._

This result, whose proof can be found in Appendix G, shows that the sample complexity of the proposed polynomial time algorithms is tight in terms of the dependence on \(\) and \(d\).

## 6 Numerical Example

In this section, we numerically compare \(_{n}\) (Algorithm 3) and DPGD on a synthetic dataset. The dataset consists of two subsets: one tightly clustered at a random location on \(_{d}(R)\), and the other uniformly distributed over \(_{d}(R)\). We plot \(F(;^{(n)})/F(^{};^{(n)})\) for both algorithms as \(R\) varies. The results show that \(_{n}\)'s performance degrades more gracefully than DP-GD with increasing \(R\). See Appendix H for experimental details and more results.

## 7 Conclusion and Limitations

In this paper, we presented three private algorithms for the geometric median task, ensuring an excess error guarantee that scales with the effective data scale. Our results open up many directions: we believe our warm-up algorithm has broader applications, and finding other problems where it can be used as a subroutine is interesting. Another direction is to characterize the optimal run-time: is it possible to develop a linear time algorithm, i.e. \((nd)\), with an optimal excess error?