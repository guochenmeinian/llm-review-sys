# Smoothed Embeddings for Robust Language Models

Ryo Hase

Midusubishi Electric Corporation

Kamakura, Japan

Hase.Ryo@dc.MitsubishiElectric.co.jp

&Md Rafi Ur Rashid

Pennsylvania State University

University Park, PA 16802

mur5028@psu.edu

&Ashley Lewis

The Ohio State University

Columbus, OH 43210

lewis.2799@osu.edu

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Md Rafi Ur Rashid

Pennsylvania State University

University Park, PA 16802

mur5028@psu.edu

&Ashley Lewis

The Ohio State University

Columbus, OH 43210

lewis.2799@osu.edu

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139

{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139
{jiliu, koike, parsons, jewang}@merl.com

&Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang

Mitsubishi Electric Research Laboratories

Cambridge, MA 02139
previously working attacks). The beam search-based adversarial attack (BEAST) (Sadasivan et al., 2024), incorporates attack suffix optimization into beam search decoding with the target model to achieve fast generation of attacks with low perplexity.

Our proposed Randomized Embedding Smoothing and Token Aggregation (RESTA) defense is inspired by the randomized smoothing defense (Lecuyer et al., 2019; Li et al., 2019; Cohen et al., 2019; Salman et al., 2019), which is typically applied to classification tasks with continuous input features. The common, high-level idea is the aggregation of model decisions produced from multiple noisy samples of the input, which has the effect of disrupting adversarial perturbations. While other LLM defenses are similarly inspired by randomized smoothing, our method introduces several novel concepts and offers some advantages:

1. We propose adding noise to the embedding vectors with the aim of better preserving semantic information.
2. We investigate how directional embedding noise impacts semantic information preservation.
3. We introduce a token-level aggregation approach integrated with auto-regressive generation.
4. Our method is applied during only when generating the prefix, which reduces compute costs.
5. Our defense does not use any auxiliary LLMs, which avoids additional complexity and concern of also protecting the behavior of secondary model(s).

Our experiments evaluate the effectiveness of our defense, applied to the Vicuna-13B model (Zheng et al., 2024) and the Llama-2-7B model (Touvron et al., 2023), against the GCG, PAIR, and RS attack prompt artifacts provided by the JailbreakBench dataset (Chao et al., 2024). We also evaluate the utility preservation of our defense with the AlpacaEval (Dubois et al., 2024) and Instruction-Following Evaluation (IFEval) (Zhou et al., 2023) benchmark datasets. We demonstrate that our method achieves a superior tradeoff between robustness and utility in comparison to the SmoothLLM defense (Robey et al., 2023), which represents its class of inference-time defenses.

In comparison with broader classes of defenses, and considering the design of practical safety systems, we emphasize that various defense concepts may ultimately be used as complementary parts, combined in a multi-layered security system. In addition to using RESTA to directly defend a

Figure 1: Randomized Embedding Smoothing with Token Aggregation (RESTA)

model, one might also deploy a secondary, supervisory model that detects attacks and/or intercepts harmful outputs, such as Llama-Guard (Inan et al., 2023), which is an LLM specifically tuned to detect jailbreaking attacks. Such a guard model should also be robust to attacks itself, or the possible vulnerability of a simultaneous attack against the target and guard remains. For example, the Greedy Coordinate Query (GCQ) attack of (Hayase et al., 2024) demonstrated the effectiveness of their query-based attack for both jailbreaking closed-weight models and undermining the OpenAI content moderation system that guard their models. Even with the ready availability of guard models that are both effective and robust, it is of course still vital to develop methods that directly defend (by making models inherently robust), since the guard models must themselves eventually be robust.

## 2 Preliminaries

### LLM Notation and Conventions

At the high level of abstraction, we denote the generation of a response \(:=(y_{1},y_{2},)^{*}\) from a prompt \(:=(x_{1},x_{2},)^{*}\) with an LLM as a (potentially probabilistic) mapping \(F:^{*}^{*}\), where \(\) denotes a finite set of tokens (i.e., the vocabulary), \(^{*}\) denotes the set of token sequences of arbitrary length, and the input and output token sequences are related by \(=F()\). While the sequence lengths may vary, in practice, there is a maximum length imposed on both, due to computational limitations.

The simple notation \(=F()\) is convenient to denote the generation process while omitting the details. However, in order to explain our methodology, we use additional notation to detail the autoregressive generation procedure. The initial step is to apply the token embedding mapping \(E:^{d}\), where \(d\) is the embedding dimensionality, to the input tokens \(:=(x_{1},,x_{n})\) to produce a sequence of embedding vectors \(:=(e_{1},,e_{n})=(E(x_{1}),,E(x_{n}))\). We denote the rest of the LLM, with the mapping \(f:^{d*}^{||}\), which, in typical transformer-based architectures, consists of positional embedding and a series of multi-headed attention, normalization, and feed-forward modules. The mapping \(f\) takes the sequence of embedding vectors \(\) as input, and outputs a distribution (expressed as a logit vector) over the token set \(\), indicating the likelihoods of the next token that should follow the input. Figure 7 in the Appendix illustrates the standard autoregressive generation procedure.

To clarify our notation and conventions, we note that the typical autoregressive generation (without any defense) is obtained as a special case of our method, described in Algorithm 1, by setting the prefix smoothing length \(l=0\), and skipping all lines involving the perturbation function \(H_{}\) or sample parameter \(k\). In this work, for simplicity, we restrict our investigation to greedy token selection.

### Related Work

There have been a variety of defenses proposed in the literature, which have been recently surveyed by (Jain et al., 2023). The following LLM defense methods are similarly inspired by randomized smoothing: Randomized Smoothing with Masked Inference (RSMI) (Moon et al., 2023), SelfDenoise (Zhang et al., 2023), Erase-and-Check (Kumar et al., 2023), SmoothLLM (Robey et al., 2023), Semantic Smoothing (Ji et al., 2024), and RigorLLM (Yuan et al., 2024). RSMI and SelfDenoise are specifically applied to language classification. The others are defenses against jailbreaking in text generation, however (except for SmoothLLM) they require an auxiliary LLM to perform either response judging or prompt paraphrasing, which adds significant computational complexity and raises concerns about also defending this secondary model.

Another class of defense strategies aims to detect and filter out attack prompts and/or harmful content generated in the responses, such as via Llama-Guard, as mentioned earlier. Perplexity filtering (Jain et al., 2023; Alon and Kamfonas, 2023) can readily isolate some attacks (such as GCG) that produce high-perplexity prompts, but are less effective against other attacks. The PARDEN defense (Zhang et al., 2024) is a form of self-filtering, where the target model is instructed to repeat its own output and the presence of an attack can be inferred from a drop in a self-consistency.

**SmoothLLM** applies random character perturbations to the input sequence \(\) to produce \(k\) noisy samples of the input sequence, \(}^{1},,}^{k}\). Their exemplary perturbation method is to randomly select characters to perturb with probability \(q\) (i.e., the perturbation rate parameter) and swap the selected characters with a uniform random sample from the given alphabet. Each of the perturbed sequences are input to the LLM to generate responses, \(}^{i}=F(}^{i})\), for \(i\{1,,k\}\). SmoothLLM also assumes access to a judge function \(J:^{*}\{0,1\}\) that outputs one if and only if the input is the LLM response of a successful jailbreaking attack. Each response \(}^{i}\) is judged with \(J\), and the final defended output is a response randomly selected from the majority set.

## 3 Randomized Embedding Smoothing

We propose _Randomized Embedding Smoothing and Token Aggregation_ (RESTA), which applies random noise to the embedding vectors to realize a defense analogous to randomized smoothing. By operating in the embedding domain, our approach aims to retain the semantic information of the original prompt, while disrupting the presence of adversarial input perturbations. In contrast to other methods, our efficient approach does not require a separate, auxiliary LLM to perform perturbation or judgement tasks. Our approach is specified in Algorithm 1, and Figure 1 depicts the high-level concept. The following subsections describe the novel features of our approach.

``` Input: token sequence \(:=(x_{1},,x_{n})^{*}\), LLM embedding mapping \(E:^{d}\), rest of LLM model \(f:^{d*}^{||}\), perturbation function \(H_{}:^{d*}^{d*}\), smoothing samples \(k\), prefix smoothing length \(l\), maximum output length \(m\). Initialize empty output sequence: \(()\). Embed input sequence: \((E(x_{1}),,E(x_{n}))\). Perturb embeddings: for \(i\{1,,k\}\), \(}^{i} H_{}()\). repeat if\(()<l\)then for\(i\{1,,k\}\)do  Calculate next token logits: \(f(}^{i})\).  Select next token: \(^{i}*{arg\,max}_{j}f(}^{i})[j]\). endfor  Majority vote: \(y(^{1},,^{k})\). else  Calculate next token logits: \(f()\).  Select next token: \(y*{arg\,max}_{j}f()[j]\). endif  Append token to output: \((,y)\).  Embed token and append: \((,E(y))\).  Append: for \(i\{1,,k\}\), \(}^{i}(}^{i},E(y))\). until\(y=[\)End of Sequence token\(]\)or\(()=m\). return Output sequence: \(\). ```

**Algorithm 1** Generation with Randomized Embedding Smoothing and Token Aggregation (RESTA)

### Embedding Perturbation Applied to User Content

At the core of Algorithm 1 is adding noise to the embedded input sequence \(\) to produce a set of \(k\) perturbed embedding sequences \(\{}^{},,}^{k}\}\), where each \(}^{i}\) is an independent sample produced by the randomized perturbation function \(H_{}:^{d*}^{d*}\), where \(\) denotes the hyperparameter(s) associated with the noising process. While we consider several different methods to generate noise (as described later), they all generally share the following common structure: (1) Perturbation is only applied to the embeddings corresponding to the user content portion of the input, since the remaining tokens are fixed (and inaccessible to the attacker) as part of the system prompt and conversation template. Figure 8 in the Appendix illustrates how only user content is perturbed. (2) A statistically independent and identical noising procedure is applied to each perturbed embedding vector. Thus, we will simply define the noising procedure, applied independently to each embedding vector of the user content, as a randomized mapping \(h_{}:^{d}^{d}\).

In order to explore the impact of embedding vector direction on preserving semantic meaning, we consider several options for the embedding perturbation function \(h_{}\):Isotropic (Normal) Gaussian noiseAs a simple baseline approach, we define \(h^{}_{}(e):=e+z\), where \(z(0,^{2}I)\) is multivariate (\(d\)-dimensional) isotropic Gaussian noise with standard deviation \(\). A potential drawback of this approach is that isotropic noise at larger values of \(\) may disrupt the direction of the embedding vector, which may encode vital semantic information.

Hard directional noiseAs an approach that aims to preserve the semantic information that may be encoded in the direction of the embedding vector, we define \(h^{}_{}(e):=e+z_{1}(e)\), where \(z_{1}\) is scalar Gaussian noise, i.e., the first element of \(z\), and it is applied to scale the direction vector \((e):=e/\|e\|_{2}\).

Soft directional noiseAs another noising approach that emphasizes the direction of the embedding vector \(e\), but does not enforce a hard directional constraint, we define \(h^{}_{}(e):=e+z(e)\), where \(\) denotes the Hadamard (element-wise) product.

Orthogonal noiseTo investigate the relative effectiveness of noise orthogonal to the embedding direction, we define \(h^{}_{}(e):=e+(I-(e)(e)^{})z\), where \(z\) is projected to the subspace orthogonal to the embedding \(e\).

### Generation with Token Aggregation

Our method performs autoregressive generation in parallel, producing a tentative next token \(^{i}\) corresponding to each perturbed embedding sequence \(}^{i}\), for \(i\{1,,k\}\). These tentative next tokens are aggregated by majority voting to select the next output token \(y\), which is then embedded as \(E(y)\) and appended to each perturbed embedding sequence. This process repeats until either the "End of Sequence" token is selected or the maximum output length \(m\) is reached. Note that the embeddings of the newly generated output tokens are not perturbed.

### Response Prefix Smoothing

To improve the efficiency of randomized embedding smoothing, which requires running the bulk of the LLM in \(k\) parallel token generation instances, we propose _response prefix smoothing_ that applies this defense to only the first \(l\) output tokens. The rest of the output token generation is conducted with a single instance of autoregressive token generation, using the original unperturbed embedding sequence \(\) appended with the embeddings of the initial \(l\) output tokens produced when the defense was active. With this approach, our defense only occurs additional computation cost in the generation of the first \(l\) output tokens. Note that if \(m l\), then the entire sequence will be generated with embedding smoothing applied, and the special case of \(l=0\) is essentially standard autoregressive generation with no defense applied. Figures 9 and 10 in the Appendix illustrate this concept.

Response prefix smoothing is motivated the observation that autoregressive generation generally continues along the same theme established by preceding tokens. For example, when faced with a harmful prompt, if the LLM begins the response with phrasing that indicates acceptance, such as "Sure, this is..." or "Here is...", then it typically continues with generation of harmful content. However, if the response begins with phrasing that indicates refusal, such as "Sorry, but I cannot...", then it usually continues with possible elaboration of the reasons for rejection. This phenomenon has been observed in the design of the GCG attack (Zou et al., 2023), where the objective for crafting adversarial inputs is to target a response beginning with acceptance of the harmful prompt, and then rely on the language model to continue along that established sentiment.

## 4 Experimental Results

Our experiments used Vicuna-13B-v1.5 and Llama-2-7B-chat-hf as the victim LLMs. For evaluating our RESTA defense, we used \(k=10\) smoothing samples and a prefix smoothing length of \(l=20\) tokens. We evaluated all four embedding perturbation schemes presented in the earlier Embedding Perturbation section. The Appendix provides links to all of the public code, models, and datasets used for our experiments, illustrates the evaluation pipelines in Figures 11 and 12. We describe our evaluation methodology in the following.

### Jailbreaking Attacks

Against the Vicuna-13B model, we used the \(100\) GCG attack prompts, \(82\) PAIR attack prompts, and \(100\) RS attack prompts available in the JailbreakBench (Chao et al., 2024) dataset. Against the Llama-2-7B model, we used the \(100\) RS attack prompts available from JailbreakBench. Note that we omit consideration of the GCG and PAIR attacks against Llama-2-7B, since the artifacts provided for those attacks have very low reported success rates (\(3\%\) and \(0\%\), respectively). JailbreakBench sources the original harmful behavioral goals from AdvBench (Zou et al., 2023), Trojan Detection Challenge (TDC) (Mazeika et al., 2023), and HarmBench datasets (Mazeika et al., 2024). When generating responses to these attack prompts, we used a maximum output length of \(m=150\) tokens.

**Attack Success Rate (ASR)**, the fraction of attack prompts that successfully induced a jailbreak, was automatically evaluated with the Llama-3-70B-Instruct model (AI@Meta, 2024), following a procedure similar to the automatic evaluation methodology of JailbreakBench. Further details, including the judge prompt template, are provided in the Appendix.

### Utility Evaluation

Evaluation of model utility is essential, since defensive measures may also disrupt the nominal LLM performance for benign inputs. We used the AlpacaEval (Dubois et al., 2024) and Instruction-Following Evaluation (IFEval) (Zhou et al., 2023) datasets to evaluate utility preservation.

**AlpacaEval** is an automatic evaluation framework of instruction following performance for LLMs. AlpacaEval "Win Rate" scores are determined by an LLM annotator that evaluates the responses for 805 prompts from a target LLM, against the responses from a reference LLM. We used the precomputed Vicuna-13B responses as the reference responses because Vicuna-13B is one of our target LLMs. We used AlpacaEval 2 with GPT-4o provided by the Azure OpenAI API as the annotator. We selected LC (Length-Controlled) Win Rate as a utility measure, which is a improved version of Win Rate which formerly assigned high scores for longer responses. For AlpacaEval experiments, we used a maximum output length of \(m=2048\), since some instructions require longer output.

**IFEval** provides 541 prompts which contains instructions with systematic evaluation criteria that allows for deterministic evaluation of whether the responses generated by the LLM follow the instructions of the prompts. We specifically use the prompt-level loose accuracy as the utility metric, as it suggested by IFEval to alleviate the issues of false negatives. We used a maximum output length of \(m=1024\) for the IFEval experiments.

### Character Perturbation Ablation

In order to study the importance of embedding perturbation, we use an ablation of RESTA, where character-level perturbation (in a manner similar to SmoothLLM) is performed instead. Essentially, this is a hybrid of the two methods, combining character perturbation with the token aggregation and prefix smoothing techniques of RESTA. For this ablation defense, we used the same \(k=10\) and \(l=20\) smoothing parameters as RESTA, and varied the choice of random character swapping, insertion, or patch swapping, with noise levels ranging from \(2\%\) to \(12\%\).

### Results

Our experimental results for defense performance comparison are summarized in Table 1. In general, we observed that RESTA provided favorable tradeoffs in reducing ASR, while incurring less impact on model utility, across the variety of attack and target model combinations. The SmoothLLM baseline defense was evaluated with its default parameters of \(10\) samples and random character swapping at a rate of \(10\%\). For our RESTA defense, against both the GCG and PAIR attacks, we noted performance at two noise levels \(\) (for hard embedding perturbation) in the summary table, in order to briefly note the tradeoff between robustness and utility. For the character-perturbation ablation, despite aiming to pick a fairly competitive operating point among its hyper-parameter choices, we see that it generally achieves a relatively poor tradeoff, which suggests that the embedding smoothing technique is an essential part of our defense. In the following, we present some highlights of our experiments, while further details are given in the Appendix, due to space constraints.

GCG attack against VicunaIn Figure 3 we show the impact on ASR of GCG on Vicuna as a function of the noise level \(\) across the choice of embedding perturbation. This choice has a clear impact on the effect of the defense, and the scale of the noise required. For isotropic (normal) and orthogonal embedding noise, \(\) ranging from \(0.01\) to \(0.04\) corresponds to ASR from close to the undefended ASR of \(94\%\) to \(0\%\). However, for hard (or soft) directional noise, \(\) must range from \(0.2\) to \(1.2\) (or \(0.5\) to \(2.5\)) to have a similar effect on ASR. Similarly, in Figure 3, we see similar difference on the scale of \(\) needed in the impact on the AlpacaEval utility measure. However, note that utility declines slower than ASR as noise level is increased, which allows for an effective tradeoff between utility and robustness, as illustrated in Figure 4. In comparison, the most competitive character perturbation defense used random patch swapping at \(8\%\) noise.

PAIR attack against VicunaIn this case, while RESTA still dominated in the comparison, overall the ASR was not driven as close to zero, without more substantially compromising utility. This tradeoff (with respect to utility measured by AlpacaEval) is illustrated in Figure 5, which shows a larger advantage for hard directional embedding perturbation over other methods. The listed character perturbation defense used random insertions at \(6\%\) noise.

RS attack against VicunaThe tradeoff for this attack against AlpacaEval utility is shown in Figure 6, which exhibited a larger advantage for soft directional embedding perturbation, and the RESTA operating point listed in Table 1 corresponds to soft embedding noise with \(=1.5\). The listed character perturbation defense used random insertions at \(8\%\) noise.

RS attack against LlamaThe undefended Llama model seems to be inherently more resilient to jailbreaking attacks, due to the very low ASRs reported for the GCG and PAIR attacks. While the RS attack did achieve an ASR of \(69\%\) against the undefended Llama model, it was possible to easily disrupt it with very little perturbation added by the smoothing defenses, with all of them achieving \(0\%\) ASR and small impact to the utility metrics. For this case, RESTA used orthogonal embedding noise with \(=0.05\), and the character perturbation defense used random swapping at \(2\%\) noise.

## 5 Conclusion and Future Work

We proposed the Randomized Embedding Smoothing and Token Aggregation (RESTA) defense, which adds noise to the embedding vectors and aggregates during token generation. Our experimental results provide an initial proof-of-concept that demonstrated the effectiveness of RESTA to reduce the ASR of jailbreaking attacks, while maintaining model utility, and explored the effect of embedding perturbation direction. As many jailbreaking attacks and defense methods have been recently emerging, we will conduct further experiments to compare with other methods in our future work.