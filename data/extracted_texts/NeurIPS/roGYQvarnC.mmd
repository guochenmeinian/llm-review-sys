# ConRad: Image Constrained Radiance Fields for 3D Generation from a Single Image

Senthil Purushwalkam

Salesforce AI Research

spurushwalkam@salesforce.com &Nikhil Naik

Salesforce AI Research

nnaik@salesforce.com

Project webpage: https://www.senthilpurushwalkam.com/publication/conrad/

###### Abstract

We present a novel method for reconstructing 3D objects from a single RGB image. Our method leverages the latest image generation models to infer the hidden 3D structure while remaining faithful to the input image. While existing methods[1; 2] obtain impressive results in generating 3D models from text prompts, they do not provide an easy approach for conditioning on input RGB data. Naive extensions of these methods often lead to improper alignment in appearance between the input image and the 3D reconstructions. We address these challenges by introducing _Image Constrained Radiance Fields (ConRad)_, a novel variant of neural radiance fields. ConRad is an efficient 3D representation that explicitly captures the appearance of an input image in one viewpoint. We propose a training algorithm that leverages the single RGB image in conjunction with pretrained Diffusion Models to optimize the parameters of a ConRad representation. Extensive experiments show that ConRad representations can simplify preservation of image details while producing a realistic 3D reconstruction. Compared to existing state-of-the-art baselines, we show that our 3D reconstructions remain more faithful to the input and produce more consistent 3D models while demonstrating significantly improved quantitative performance on a ShapeNet object benchmark.

## 1 Introduction

Humans posses the ability to accurately infer the full 3D structure of an object even after observing just one viewpoint. Since the RGB and depth observed for one viewpoint does not provide sufficient information, we have to rely on our past experiences to make intelligent inferences about a realistic reconstruction of the full 3D structure. This ability helps us navigate and interact with the 3D world around us seamlessly. Capturing a similar capability of generating the full 3D structure from a single image has been a long-standing problem in Computer Vision. Such systems could facilitate advances in robotic manipulation and navigation systems, and has applications in video games, augmented reality and e-commerce. Despite the importance of this capability, success on solving this problem has been limited.

This lack of success can partly be attributed to the lack of useful large scale data. While we have made progress on collecting large scale image datasets, the scale of 3D object or multi-view image datasets has been severely limited. The availability of billions of RGB images and captions  has facilitated significant advances in semantic understanding of images [4; 5; 6]. Over the last few years in particular, approaches for generative modeling of the image manifold have demonstrated capabilities including controllable generation of highly realistic images [7; 8], inpainting of occluded images  and realistic image editing [10; 11; 12]. These success stories demonstrate the ability of the 2D generative models to capture prior knowledge about the visual world. In light of this evidence,we ask the question--can this 2D prior knowledge be leveraged to infer the hidden 3D structure of objects?

DreamFusion  generates 3D models based on an input text prompt, using the distribution captured by a pretrained diffusion model to update the parameters of a neural radiance field (NERF) . While DreamFusion demonstrates impressive capabilities, generating arbitrary instances of objects from text prompts severely limits its applications. In recent concurrent work, RealFusion  and NeuralLift360  extend DreamFusion to accommodate image input. These methods propose to optimize additional objectives to reconstruct the given input image in one reference viewpoint of the NERF while still relying on a diffusion model to generate a realistic reconstruction in novel viewpoints.

In this work, we propose a novel method for generating a 3D object from a single input image. Instead of designing additional objectives to reconstruct the input image, we propose to rethink the underlying 3D representation. We introduce Image Constrained Radiance Fields (ConRad), a novel variant of NERFs that can explicitly capture an input image without any training. Given an input image and a chosen arbitrary reference camera pose, ConRad utilizes multiple constraints to incorporate the appearance of the object and the estimated foreground mask into the color and density fields respectively. These constraints ensure that the rendering of the radiance field from the chosen viewpoint exactly matches the input image while allowing the remaining viewpoints to be optimized.

The proposed ConRad representation significantly simplifies the process of generating a consistent 3D model for the input image using any pretrained diffusion model. Since the representation accounts for the reference view, the training simply has to focus on distilling the diffusion model prior for the other viewpoints. Therefore, we show that we can leverage a training algorithm that has minimal deviations from DreamFusion. We propose a few key improvements to the training algorithm which leads to more robust training and higher quality 3D reconstructions.

We demonstrate results using images gathered from multiple datasets including CO3D , ShapeNet , and images generated from pretrained generative models. ConRad produces 3D reconstructions that are consistently faithful to the input image while producing high quality full 3D reconstructions. In contrast to RealFusion and NeuralLift360, ConRad has fewer hyperparameters and does not rely on balancing the trade off between image reconstruction vs satisfying the diffusion prior, leading to a more robust training pipeline. We conduct a quantitative comparison on objects from the ShapeNet dataset and observe that our approach demonstrates significant gains in reconstruction quality compared to state-of-the-art methods.

## 2 Related Work

3D RepresentationsOver the last decade, research in Computer Vision and Graphics has led to development of efficient parametric representations of 3D scenes . Traditional voxel

Figure 1: **ConRad representation: We propose a novel radiance field,ConRad, which allows conditioning on a single image. In contrast to regular radiance fields, our approach can accurately model the input image in one reference viewpoint of the radiance field without any training. Utilizing ConRad representations simplifies the optimization of a radiance field for generating a 3D model from a single image.**

grid representations [20; 21; 22] maintain a dense grid of 3D scene parameters but consume a lot of memory. Neural Radiance Fields (NERFs)  offer a continuous parametric representation of the scene modeled as a neural network while requiring significantly lower memory while being computationally inefficient. Several approaches [23; 18] have been proposed to take advantage of both forms of representation. Instant-NGP  is one such approach that models a 3D scene using a multi-resolution hash-encoding followed by a shallow neural network. Our proposed ConRad representation builds on Instant-NGP but can be used with any radiance field.

Learning to Estimate 3D StructureTo recover the full 3D structure from a single image, many works propose to learn category-specific 3D representations  then later fit to an input image. These approaches primarily relied on coarse representations like point clouds  and compositions of primitive shapes . Furthermore, these methods focused on the geometry and could not infer the hidden appearance of the objects. NERFs demonstrated the capability to accurately infer novel viewpoints of a scene using densely sampled images. This has inspired numerous approaches to train similar radiance fields using sparse samples of a scene or object using semantic and geometric priors. [26; 27; 28; 29; 30] train models on large scale multi-view data to estimate radiance fields given a single or few images. The limited diversity of available 3D data restricts the generalizability of these approaches.

Generating 3D from Pretrained 2D modelsLarge scale 2D image data is much more readily available compared to 3D or multi-view data. This has led to significantly larger advances in modeling semantics in images [4; 5; 6] and generative modeling of images [7; 8]. In light of this, several works have attempted to leverage the knowledge from these 2D models for the task of inferring the radiance fields from one or few images. Our proposed method belongs to this category of approaches since we rely on the distribution modeled by a pretrained image diffusion model.

DreamFields  generates 3D models from text prompts by optimizing the CLIP  distance between the NERF renderings and the textual CLIP embedding. DreamFusion  proposes an approach called Score Distillation Sampling (SDS) for distilling knowledge from a pretrained text-conditioned Diffusion Model  into a NERF representation (see Section 3 for more details). Magic3D  proposes a coarse-to-fine strategy that also leverages SDS to first train a NERF and then finetune a mesh representation. Similar to DreamFields, DietNERF  proposed to train a NERF on a few input images while enforcing consistency of CLIP  features in other unknown views. However, this approach fails to generate 3D objects using a single image input. Pretrained image generative models have also been used to infer radiance fields in existing work  but only focuses on estimating geometry of the regions visible in the input image.

RealFusion  and NeuralLift360  are two recent methods that are very relevant to our proposed approach. Inspired by DreamFusion, these approaches leverage a pretrained Stable Diffusion model to infer the appearance and geometry of unknown viewpoints and optimize several additional objectives to enforce consistency to the input image in one reference viewpoint. In concurrent work, Nerdi also follows a similar approach of using a reconsruction loss and depth loss for the known viewpoint, and uses diffusion model priors for unknown viewpoints. Unless properly tuned, these approaches lead to poor alignment between the input image and reconstructed 3D model (more details in Section 5). In contrast, ConRad focuses on modifying the underlying 3D representation to easily incorporate the input image and generates a more consistent 3D reconstruction.

## 3 Preliminaries

We first provide a concise summary of the prerequisite concepts from generative modeling of images and 3D objects that we build upon for ConRad.

Diffusion ModelsA diffusion model is a recently developed generative model that synthesizes images by iteratively denoising a sample from a Gaussian distribution. For training a diffusion model, noise is added to a real image \(I\) iteratively for \(T\) timesteps to synthesize training data \(\{I_{0},I_{1},...,I_{T}\}\). Specifically, the noised image for timestep \(t\) can be computed as \(I_{t}=}I+}\) where \((,)\) and \(_{t}\) is predefined noising schedule. This data is used to a train a denoising model \(_{}(I_{t},y,t)\) which estimates the noise \(\) using the noisy image \(I_{t}\), the timestep \(t\) and an optional embedding \(y\) of the caption associated with the image. A pretrained diffusion model can be used to synthesize images by following the inverse process. First, a noise sample \(I_{T}\) is drawn from \((0,)\). Then \(_{}\) is iteratively used to estimate the sequences of images \(\{I_{T-1},I_{T-2},...,I_{0}\}\) where \(I_{0}\) is the finally synthesized image.

**Neural Radiance Fields** A Neural Radiance Field (NERF)  is a parametric representation of a 3D scene as a continuous volume of emitted color \(c\) and density \(\). Formally, it can be written as a mapping \(_{}:(x)(c(x),(x))\) from the 3D coordinates of a point \(x\), to it's associated color and density. These radiance fields allow rendering of 2D images by accumulating color over points sampled on camera rays. For a camera ray \((t)=+t\), the accumulated color is expressed as:

\[C()=_{t}T(t)(r(t))c(r(t))dt\] (1)

where \(T(t)=exp(-_{0}^{t}(r(s))ds)\) is the accumulated transmittance of the volume along the ray. Learning an accurate representation \(_{}\) of a 3D scene generally requires supervision in the form of several ground truth samples of \(C()\)_i.e._ several images of a scene taken from different viewpoints. Representing \(_{}\) as a neural network has been shown to demonstrate several desirable properties like the ability to synthesize novel views and realistic high-resolution renderings of complex scenes.

**Image Diffusion to Radiance Fields** DreamFusion proposed an approach to leverage a pretrained image diffusion model and a text prompt to optimize a 3D radiance field. The key idea is to optimize parameters \(\) such that the rendering of the radiance field from any viewpoint looks like a sample from the diffusion model. This is accomplished by randomly sampling camera poses \(p\) during training, rendering images from the radiance field for these viewpoints \(I_{}^{p}\) and using a _Score Distillation Sampling_ objective to optimize the radiance field. The gradient of the Score Distillation Sampling (SDS) objective \(_{}\) is defined as:

\[_{}_{}(,I_{}^{p},y)=_{ t,e}w(t)(_{}(}I_{}^{p}+},y,t)-)_{}I_{}^{p}\] (2)

where \(w(t)\) is a timestep dependent weighting function. We refer the readers to  for the derivation of this objective. In practice, each update is computed using a randomly sampled timestep \(t\) and noise sample \(\). Intuitively, this is equivalent to first perturbing the rendered image using \(,t\) and then updating the radiance field using the difference between the diffusion model estimated noise and \(\).

## 4 Method

### Problem Setup

The goal of this work is to optimize a 3D radiance field \(_{}\) to capture the visible appearance and geometry of an object depicted in an input image, while inferring a realistic reconstruction of the unknown/hidden parts. Let the input image be represented by \(\) and \(\) be a reference camera pose (which can be arbitrarily chosen) associated with the image. Let \(I_{}^{p}\) be the image obtained as a differentiable rendering of \(_{}\) viewed from camera pose \(p\). The optimal desired representation \(F_{}\) should satisfy two criteria: (_i_) \(I_{}^{p}=\), and (_ii_) For all viewpoints \(p\), \(I_{}^{p}\) should be semantically and geometrically consistent.

A simple approach for satisfying requirement _(i)_ could be to optimizing an \(L_{2}\) distance objective \(I_{}^{}\) and \(\). Furthermore, Score Distillation Sampling (see Section 3) can be used to satisfy requirement _(ii)_ by optimizing randomly rendered viewpoints using diffusion model priors. This approach forms the basis of concurrent works RealFusion and NeuralLift360. We observe in experiments (Sec 5) that this approach often leads to misaligned final appearance of novel viewpoints.

### ConRad: Image Constrained Radiance Fields

We propose a novel variant of neural radiance fields called Image Constrained Radiance Fields (ConRad) that allows us to effectively satisfy the two objectives. Intuitively, we wish to formulate a radiance field that accurately depicts the input image in one viewpoint while allowing us to optimize and infer the other viewpoints. First, we note that the input image depicts a "visible" part of an object's surface. Under some assumptions on the object properties2, we can make inferences about the 3D space around this object. Any point that lies on/in front of the visible surface is known to us and does not need to be re-inferred in our radiance field. Specifically, points between the reference camera and the surface should have zero density, and points on the surface should have high densityand color equal to the corresponding pixel in the input image. Unfortunately, since we do not have access to the depth map corresponding to the reference view, it is not possible to infer which 3D points lie in front or behind the surface. As a workaround, we leverage the radiance field to obtain an estimate of the depth. Using this estimated depth, we propose to incorporate these observations into the color and density of the radiance field to leverage the input image as an explicit constraint.

Concretely, let the ray corresponding to pixel \((i,j)\) in the reference view be \(_{}^{(i,j)}(t)=_{}+t_{}^{(i,j)}\). We define the _visibility depth_ of this pixel \(V_{}[i,j]\) as the value of \(t\) such that

\[1-^{t}T(t)(r_{p}^{(i,j)}(t))}{_{s=0}^{}T(s) (r_{p}^{(i,j)}(s))}=\] (3)

where \(\) is a small value set to 0.1 in our experiments. Intuitively, the visibility depth for each pixel is a point on the ray beyond which the contribution of color is minimal (less than 10%).

Let \(Q_{p}:^{3}[-1,1][-1,1]\) be the camera projection matrix corresponding to projection from world coordinates to the normalized reference image coordinates. We can reformulate the color \(c(x)\) of the radiance field \(_{}\) as follows:

\[v_{x}=(\ \|x-_{}\|<V_{}[Q_{ }(x)]\ )\] (4) \[c^{}(x)=v_{x}*I^{}(Q_{}(x))+(1-v_{x})*c(x)\] (5)

where we use bilinear interpolation to compute pixel values \(I^{}(Q_{}(x))\). This constraint enforces the appearance of the reference viewpoint to explicitly match the image. Since we only estimate the depth of each pixel based on a potentially incorrect density field, we enforce that all points in between the camera and the estimated surface along the ray should have color equal to the corresponding pixel. As shown in Figure 1, this still constrains the reference view to match the input image but allows the density to be optimized through our training process.

Additionally, the foreground mask of the input image informs the density of the radiance field. We estimate the binary foreground mask \(\) using a pretrained foreground estimation method. We know that any point on the reference rays corresponding to the background pixels should have zero density. We reformulate the density \((x)\) as:

\[m_{x} =[Q_{}(x)]\] (6) \[^{}(x) =m_{x}*(x)\] (7)

In summary, ConRad is the radiance field defined by the constrained color \(c^{}(x)\) and density \(^{}(x)\).

### Optimizing Image Constrained Radiance Fields

In this section, we present our approach for training a ConRad representation using a single input image \(\). First, we preprocess the input image to extract several supervisory signals. Since we wish to leverage a text-conditioned diffusion model, we need to generate text embeddings for a caption of the input image. We use the caption "a photo of a <token>" where the embedding of the special token is inferred using Textual Inversion. In order to learn an embedding that accurately captures the input image, we pass in multiple synthetically generated augmentations of the input image by randomly cropping, resizing and blurring it. Due to space constraints, we presented more details in the supplementary material. We also estimate a depth map for the reference view \(\) using MiDaS.

Diffusion PriorOne of the key advantages of ConRad is the simplicity of the training pipeline. Since ConRad already incorporates the appearance of the reference view, the training algorithm has minimal deviations from DreamFusion. We simply compute SDS updates on random viewpoints using the pretrained Diffusion Model conditioned on the previously obtained text embeddings and update the radiance field to infer the unknown density and color regions.

Depth LossSince the depth of the reference viewpoint is unknown, we find that providing additional supervision through the estimated reference depth map \(\) sometimes improves the results of our training. The preprocessed depth estimate \(\) only provides relative depth values. Therefore, the ground truth depth should be close to \(\) up to a scalar scaling and translation factor. We accommodate Figure 2: **ConRad Image-to-3D Reconstruction:** Here we visualize the 3D structures generated by our proposed approach using images taken from different sources. We observe that the proposed ConRad representation leads to high quality realistic reconstructions of the objects while completely preserving input image details. Please see Appendix for comparison to RealFusion and NeuralLift360 on these input images.

this in the depth loss \(_{}}\) by formulating it as the Pearson Correlation coefficient between \(D\) and \(\). NeuralLift360 adopts a similar idea and uses a ranking loss for providing depth supervision.

Warm StartThe visibility and mask values in Eq 4 & 6 are non-differentiable and also cause a sharp boundary in the radiance field. For some input objects, we observe that this disrupts training. This can easily be resolved by performing a "_warm start_". We multiply the visibility and mask scores in Eq 4 & 6 with a scalar \(\) that is linearly annealed from 0 to 1 over the first 50% of the updates and then kept constant at 1.

## 5 Experiments and Results

### Implementation Details

The simplicity of Image Constrained Radiance Fields representations facilitates a straightforward implementation of the training process. We use Instant-NGP as the base representation comprising of a shared multi-resolution hash encoding and two separate small MLPs for the unconstrained color \(c(x)\) and density \((x)\) respectively. For all experiments in this paper, we choose the reference view camera to lie at a distance of 3.2 from origin, azimuth angle 0 and elevation 0 except where specified. We precompute and store rays for the camera corresponding to this reference view. Before each update, we estimate the Visibility Depth (Eq 4) by evaluating on points along these rays and choosing the closest solution. For each update, a random camera pose \(p\) is obtained by uniformly sampling elevation angle in \([-15^{},45^{}]\), azimuth angle in \([0^{},360^{}]\) and distance to origin in \([3,3.5]\). We then render the image \(I_{p}\) for this viewpoint using the constrained color \(c^{}(x)\) and density \(^{}(x)\). We also compute the estimated reference view depth \(D_{}\) using the precomputed rays and radiance field density \(^{}(x)\). We compute the SDS sampling update (Eq 2) using \(I_{p}\) and the gradient of the Depth Loss \(D_{}\) to update all the parameters. Additionally, the regularizations proposed in  are adopted to enforce smoothness of surface normals and encourage outward facing surface normals in the radiance field. We keep all the hyperparameters unchanged for all experiments except when explicitly indicated (Sec 5.4). Please refer to the supplementary material for details of regularizations and their associated weights, hyperparameters of Instant-NGP, optimizer hyperparameters and pseudo-code of the training algorithm.

Note that we do not need to render the image of the reference view during training. This eliminates the need for tuning viewpoint sampling strategies and additional reference loss hyperparameters, leading to a simpler and more robust training pipeline. Computation of visibility depth also does not significantly increase GPU memory consumption since we do not compute its gradients.

### Visualizing 3D Reconstructions

We first visualize the final 3D reconstructions of ConRad representations trained on single images taken from different sources. In Figure 2, we present the RGB and surface normal reconstructions of the reference view and three novel viewpoints. Observe that the final representation always accurately reconstructs the input viewpoint due to the constraints incorporated in ConRad. Furthermore, the novel viewpoints generally depict a realistic reconstruction of the input object.

We also observe that the reconstructions are faithful to the specific instance of the object, presenting a smooth transition from reference viewpoint to unobserved viewpoints. This can be attributed to two aspects of the model: (i) The textual inversion embedding passed to the diffusion model captures the details of the object and (ii) Since ConRad depicts the input image accurately from beginning of training, it provides a strong prior for the appearance for the diffusion model, especially around the reference viewpoint. In Figure 2, "Novel View 1" presents a viewpoint close to the reference view.

Our proposed approach can also be used to generate 3D model from text prompts. To accomplish this, we can leverage pretrained Stable Diffusion to first generate a 2D image from the text prompt and then use ConRad to learn a 3D reconstruction of this object. Figure 2 Rows 3 & 4 demonstrate this capability on two prompts.

### Comparison to existing work

**RealFusion** relies on alternating between rendering the reference viewpoint and a random viewpoint. For the reference viewpoint, additional mean squared error objectives are used to distill the reference RGB and foreground mask values into the 3D representation. For other viewpoints, Score Distillation Sampling is used to update the representation. Similar to our approach, textual inversion is used to compute the conditioning text embedding for Stable Diffusion.

**NeuralLift360** proposes a similar approach to RealFusion. In addition to reference RGB and foreground mask, NeuralLift360 also utilizes an estimated depth map and the CLIP features of the input image as supervision. A ranking loss is used to account for the scale and translation ambiguity in the estimated depth map. This is similar in spirit to our proposed depth objective in Section 4.3. NeuralLift360 also encourages the CLIP features of renderings from all viewpoints to be consistent with the input viewpoint.

**Qualitative Comparisons** In Figure 3, we present qualitative comparisons to ConRad. While both RealFusion and NeuralLift360 released official implementations, we observed that they require tuning of hyperparameters or fail to generate meaningful reconstructions for several objects. For fair comparison, we evaluate our method on images taken from the respective papers. We observe

Figure 3: **Qualitative comparisons:** We perform qualitative comparisons of our approach to state-of-the-art image to 3D generation methods. For fair comparison and to demonstrate robustness, we evaluate on images taken from the respective papers. We observe that ConRad is able to generate higher quality 3D models while more accurately preserving input image details.

ConRad can better capture the details of the input image consistently. In contrast, RealFusion and NeuralLift360 often generate a similar instance of the same category with a different appearance.

Evaluation MetricsThe task of image to 3D generation is difficult to quantitatively evaluate due to the inherent ambiguity in the expected output _i.e._ there are several possible realistic reconstructions of the same object. NeuralLift360 evaluates the ability to capture semantic content by measuring the CLIP feature distance between all viewpoints of the generated object and the single input reference image. We build upon this idea and propose metrics to evaluate the ability to generate different viewpoints of an object.

First, we render 20 objects from 10 categories of the ShapeNet dataset viewed from 68 different camera poses to create a ground truth (GT) set. We choose a front-facing view from each object as input to an Image-to-3D approach. We then render the generated object from the same 68 camera poses. Due to ambiguity of depth, corresponding camera poses between GT and rendered images could depict very different object poses. Using these two sets of images, we compute three metrics - \(d_{ref}\) is the mean CLIP feature distance between the reference image and all the rendered viewpoints (same as ). \(d_{all}\) is the mean CLIP feature distance between all pairs of GT and rendered images. \(d_{oracle}\) is the solution to a linear sum assignment problem where the cost of assigning a GT view to a rendered image is the CLIP feature distance between them. This evaluates the ability of the representation to generate images as diverse as the ground truth while preserving semantic content.

We evaluate these three metrics for the two sets of 68 images ("All Views"). We also evaluate on a subset of camera poses that lie within a \(15^{}\) elevation change and \(45^{}\) azimuth change ("Near Reference") giving us 15 images each for ground truth and rendered images. This measures the semantic consistency in viewpoints where parts of the input image are visible.

    &  &  \\  & \(d_{ref}\) & \(d_{all}\) & \(d_{oracle}\) & \(d_{ref}\) & \(d_{all}\) & \(d_{oracle}\) \\   PointE & 0.496 & 0.482 & 0.453 & 0.496 & 0.499 & 0.399 \\ RealFusion & 0.495 & 0.486 & 0.460 & 0.483 & 0.482 & 0.464 \\ NeuralLift-360 & 0.528 & 0.516 & 0.498 & 0.543 & 0.544 & 0.534 \\ ConRad (ours) & **0.332** & **0.316** & **0.273** & **0.286** & **0.257** & **0.230** \\   

Table 1: **Quantitative Comparisons:** We perform quantitative comparisons using 3D object data of 20 objects depicting instances of 10 categories taken from the ShapeNet dataset. We evaluate the CLIP semantic similarity of rendered object viewpoints to ground truth viewpoint samples. We demonstrate that generating 3D models with ConRad leads to significant improvements over state-of-the-art across all metrics.

Figure 4: **Ablation Study:** We present a qualitative investigation of the importance of various components proposed in our approach on two prototypical examples. The components are arranged left to right in decreasing order of importance for final reconstruction quality.

### Analysis of ConRad Components

Quantitative EvaluationIn Table 1, we compare ConRad to RealFusion, NeuralLift360, and PointE. PointE is a point cloud diffusion model was trained on several million 3D models. It can directly generate point clouds using CLIP features of the input image. Since there is no corresponding reference view in the output, we synthesize 150 random views for PointE instead of the 68 chosen views. For all methods, we use the implementations and hyperparameters provided by the authors. We observe that ConRad outperforms these methods across all the metrics by a significant margin. We present a per-object breakdown of these metrics in the supplementary material.

We now investigate the various components of ConRad to understand their significance. In Figure 4, we present a visualization of the effect of removing each component on two typical examples. In most experiments, we observe that performing a _warm start_ is not necessary but leads to crisper final 3D structure. We also observe that removing the Depth Loss significantly reduces computational cost, but this leads to incorrect 3D structure for some objects (see row 2 of Figure 4). Textual Inversion is crucial to maintain consistent appearance of the object from all viewpoints. Moreover, removing the color constraint (Eq 4) generally still leads to a realistic 3D model but depicts an arbitrary object. Finally, removing both color and density constraints (still using the depth loss) leads to very unrealistic objects.

### Failure Cases and Limitations

While ConRad significantly improves the simplicity and robustness of learning the 3D structure, it occasionally demonstrates issues that are common to Image-to-3D methods. In Figure 5, we present two typical examples of failure cases. Row 1 demonstrates the Janus effect, where the final 3D model has two faces. We also observe that performing score distillation sampling using Stable Diffusion leads to saturated colors in the 3D model. Leveraging textual inversion embeddings mitigates this issue to some extent, but still occasionally leads to incorrect appearance in novel viewpoints, as demonstrated in Row 2. We also observe that for a few objects, parts of the generated 3D object is semi-transparent (see videos in supplementary material).

## 6 Conclusion

In this work, we present a novel radiance field variant, ConRad, that significantly simplifies the training pipeline for generating 3D models from a single image. By explicitly incorporating the input image into the color and density fields, we show that we can eliminate tedious tuning of additional hyperparameters. We demonstrate that ConRad representations lead to more accurate depiction of the input image, produce higher quality and more realistic 3D models compared to existing work.

Figure 5: **Failure Cases:** On some objects, our proposed approach suffers from the Janus effect and is affected by the bias of Stable Diffusion to produce saturated colors.