# Enhancing Large Vision Language Models with Self-Training on Image Comprehension

Yihe Deng\({}^{*1}\), Pan Lu\({}^{*1,3}\), Fan Yin\({}^{1}\), Ziniu Hu\({}^{1}\), Sheng Shen\({}^{2}\)

Quanquan Gu\({}^{1}\), James Zou\({}^{3}\), Kai-Wei Chang\({}^{1}\), Wei Wang\({}^{1}\)

Equal contribution.\({}^{1}\)University of California, Los Angeles

\({}^{2}\)University of California, Berkeley \({}^{3}\)Stanford University

[https://stic-lvm.github.io/](https://stic-lvm.github.io/)

###### Abstract

Large vision language models (LVLMs) integrate large language models (LLMs) with pre-trained vision encoders, thereby activating the model's perception capability to understand image inputs and conduct subsequent reasoning for different queries. Improving this capability requires high-quality vision-language data, which is costly and labor-intensive to acquire. Self-training approaches have been effective in single-modal settings to alleviate the need for labeled data by leveraging model's own generation. However, effective self-training remains a challenge regarding the unique visual perception and reasoning capability of LVLMs. To address this, we introduce **S**elf-**T**raining on **I**mage **C**omprehension (**STIC**), which emphasizes a self-training approach specifically for image comprehension. First, the model self-constructs a preference dataset for image descriptions using unlabeled images. Preferred responses are generated through a step-by-step prompt, while dis-preferred responses are generated from either corrupted images or misleading prompts. To further self-improve reasoning on the extracted visual information, we let the model reuse a small portion of existing instruction-tuning data and append its self-generated image descriptions to the prompts. We validate the effectiveness of STIC across seven different benchmarks, demonstrating substantial performance gains of \(4.0\%\) on average while using \(70\%\) less supervised fine-tuning data than the current method. Further studies investigate various components of STIC and highlight its potential to leverage vast quantities of unlabeled images for self-training. Code and data are made publicly available on GitHub.

## 1 Introduction

In recent years, we have witnessed remarkable advancements in large language models (LLMs), such as GPT-4 (OpenAI, 2023) and the LLaMA family (Touvron et al., 2023, 2023). The increasing importance of processing multimodal inputs, including images and text, has significantly driven progress in vision language models (Radford et al., 2021; Jia et al., 2021, 2022). Leveraging the powerful language understanding and generation capabilities of LLMs, researchers have advanced vision language models into large vision language models (LVLMs). This enhancement is achieved by integrating LLMs with image encoders (Radford et al., 2021; Li et al., 2023), which were pre-trained on large-scale image-text pairs to ensure alignment between the two domains. For instance, LLaVA (Liu et al., 2023) integrates a vision encoder from CLIP (Radford et al., 2021) with the LLM Vicuna (Chiang et al., 2023), which is further fine-tuned on carefully constructed vision-language instructional datasets to activate the model's perception capability of capturing the vision information according to different queries. This recent development has substantially expanded the requirement for large-scale instruction fine-tuning data for LVLMs (Gao et al., 2023; Bai et al., 2023; Chen et al., 2023; Gao et al., 2024; Anthropic, 2024; McKinzie et al., 2024).

While LVLMs have shown promising results, a key challenge lies in the acquisition of high-quality fine-tuning data. Obtaining human-curated content at scale is often prohibitively expensive, especially for multi-modal data. Many recent studies resort to GPT-4V (OpenAI, 2023b) for generating or labeling high-quality vision-language fine-tuning data. However, this approach does not significantly reduce the cost (Liu et al., 2023b; Wu et al., 2024). For instance, using GPT-4V to generate \(6k\) image descriptions with \(1k\) tokens per output would cost approximately $200. There remains a pressing need for cost-effective methods to gather fine-tuning data to further enhance LVLMs.

To tackle the data acquisition bottleneck in multi-modality, we propose **S**elf-**T**raining on **I**mage **C**omprehension (**STIC**). Our method is inspired by the recent success of self-training (Chen et al., 2024; Yuan et al., 2024; Franken et al., 2024; Rosset et al., 2024) in LLMs, which leverages self-generated data to improve their downstream performance. However, different from the text-only domain, the unique vision modality of LVLMs introduces new challenges, as LVLMs must understand the input image content before reasoning and responding to any related textual queries about the image. Therefore, the proposed STIC approach is a novel two-stage self-training method that targets both _image perception_ and _reasoning over images and texts_.

The overall framework is summarized in Figure 2. STIC specifically emphasizes the **image comprehension self-training** of LVLMs where the model generates its own preference dataset focused on image description. The self-generated _dispreferred response_ is obtained by gathering model responses from either (1) prompts likely to elicit inaccurate responses or (2) corrupted images. The _preferred responses_ are collected via a detailed prompt that guides the model through a step-by-step image description process. Figure 3 shows examples of such generated responses. During fine-tuning, we consider a DPO loss (Rafailov et al., 2023) with an additional regularized term explicitly emphasizing the preferred response. Lastly, we allow the model to self-improve its reasoning ability based on its own extracted image information by reusing a small amount of existing instruction fine-tuning data and appending its self-generated image descriptions to the prompts. We refer to this second stage as **description-infused fine-tuning**. Notably, our STIC approach _does not require pre-labeled information of the images_, which contrasts to the recent works that rely on such information for constructing vision-language preference data (Zhou et al., 2024).

To demonstrate the effectiveness of STIC, we conduct extensive experiments on seven vision-language benchmarks, including ScienceQA (Lu et al., 2022), TextVQA (Singh et al., 2019), ChartQA (Masry et al., 2022), LLaVA-Bench (Liu et al., 2023a), MMBench (Liu et al., 2023c), MM-Vet (Yu et al., 2023), and MathVista (Lu et al., 2024). These benchmarks encompass scientific reasoning, math reasoning, optical character recognition (OCR), and conversation capabilities based on vision inputs, spanning various image sources such as natural, chart, and text-rich images. We employ LLaVA-v1.6 (Liu et al., 2024) as the primary base LVLM for our experiments and unitize \(6k\) images from MSCOCO (Lin et al., 2014) to construct the image description preference data. As depicted in Figure 1, STIC achieves consistent and significant performance improvements across these benchmarks, with an average accuracy gain of **4.0%** over the base LVLM and a notable gain of **6.4%** on ScienceQA. We also provide an example of the different responses from the original LVLM and STIC in Figure 1, where STIC successfully identifies the key visual information and accurately reason with it. These results demonstrate the remarkable effectiveness of our image comprehension self-training approach in enhancing the visual perception capabilities of LVLMs.

Figure 1: **Left**: Accuracy improvement of our method, STIC, compared to the original LLaVA-v1.6 (Liu et al., 2024) on seven benchmarks. **Right**: Response examples from the original LLaVA-v1.6 and STIC (LLaVA-v1.6), which enhances image comprehension and subsequent reasoning capabilities.

In addition, we explore the benefits of the various components of STIC. First, based on the description-infused fine-tuning stage that enhances the model's reasoning ability with self-generated description, we show that further letting the model describe the image before responding to a query provides further improved reasoning capability. This results in a notable improvement of \(2.8\%\) on ScienceQA and \(1.1\%\) on average as compared to direct responses to queries (Table 2). Moreover, we examine the impact of self-generated dispreferred responses, from either bad prompting or image corruption. By excluding these dispreferred responses and conducting SFT solely with preferred responses, we observed a performance decrease of \(2.5\%\) on average across three benchmarks as compared to STIC with the preference data (Table 3). This highlights the importance of the negative samples in the self-constructed preference data by STIC. We also assess the scalability of our self-training scheme. By increasing the amount of generated preference data from \(6k\) to \(12k\), we show an even further improvement of STIC from \(1.9\%\) to \(3.1\%\) on LLaVA-Bench (Figure 6). This result suggests that STIC holds considerable potential for leveraging vast quantities of unlabeled images for self-training, given the immense availability of unlabeled image data. Lastly, our t-SNE visualization analysis shows that the closer the distribution between MSCOCO images, which we use for preference data construction, to images in downstream tasks, the more likely STIC results in higher performance gains (Figure 7).

The main contributions of this work are summarized as follows:

* We propose STIC, a novel two-stage self-training approach for LVLMs that focuses on enhancing their image comprehension capabilities by generating a preference dataset for image description without relying on pre-labeled image information.
* Through extensive experiments on seven diverse benchmarks, STIC demonstrates significant performance gains over the base LVLM, achieving an average accuracy gain of \(4.0\%\).
* We explore the benefits of various components of STIC, highlighting its potential to leverage vast quantities of unlabeled images for self-training.

## 2 Related Work

**Vision language models (VLMs).** VLMs (Tan and Bansal, 2019; Li et al., 2019, 2020; Kim et al., 2021; Wang et al., 2022b; Bao et al., 2022; Wang et al., 2022a; Alayrac et al., 2022; Li et al., 2023b; Chen et al., 2022; Jia et al., 2021; Shen et al., 2022; Singh et al., 2021), processing both images and text, are pivotal in a wide range of multimodal understanding and reasoning tasks, capable of generating text or encoding multimodal representations. These models have shown increasing proficiency in visual perception and textual reasoning, and are also capable of following complex instructions (OpenAI, 2023b; Team et al., 2023). Recent advancements in the field have been propelled by the availability of open-source large language models (LLMs) (Touvron et al., 2023a, b; Jiang et al., 2023) and innovative image encoders (Radford et al., 2021; Li et al., 2022). For instance, LLaVA (Liu et al., 2023b) combines a vision encoder from CLIP (Radford et al., 2021) with the Vicuna LLM (Chiang et al., 2023b), and has been further fine-tuned on vision-language instruction

Figure 2: Framework overview of STIC, a two-stage self-training algorithm focusing on the image comprehension capability of the LVLMs. In Stage 1, the base LVLM self-constructs its preference dataset for image description using well-designed prompts, poorly-designed prompts, and distorted images. In Stage 2, a small portion of the previously used SFT data is recycled and infused with model-generated image descriptions to further fine-tune the base LVLM.

following datasets. The recent development of LVLMs has significantly expanded the scale and diversity of VL instruction-following data, including models such as LLaMA-Adapter-V2 (Gao et al., 2023b), Qwen-VL (Bai et al., 2023), InternVL (Chen et al., 2023b), InstructBLIP (Dai et al., 2024), SPHINX-X (Gao et al., 2024), Claude-3 (Anthropic, 2024), MM1 (McKinzie et al., 2024), and Grok-1.5V (xAI, 2024). In this work, we focus on enhancing the visual perception and mathmatical reasoning capabilities of LVLMs by efficiently aligning them with purely unsupervised data.

**Alignment fine-tuning.** Subsequent to supervised fine-tuning (SFT), alignment fine-tuning has emerged as a prominent approach to further enhance the performance of LLMs by aligning them with human preferences (Ouyang et al., 2022; Casper et al., 2023). Early efforts utilized on-policy reinforcement learning (RL) methods, such as proximal policy optimization (PPO) (Schulman et al., 2017), to train a reward model based on preference data (Bai et al., 2022; Touvron et al., 2023a). With the notable introduction of direct policy optimization (DPO) (Rafailov et al., 2023), a new line of research emphasizes direct learning from human preferences without relying on an explicit reward model (Zhao et al., 2023; Azar et al., 2024; Ethayarajh et al., 2024; Zheng et al., 2024). Another prominent direction is iterative preference fine-tuning, which has proven effective in enhancing model performance by repeatedly optimizing on newly generated preference pairs in each iteration (Adolphs et al., 2023; Xu et al., 2023; Xiong et al., 2023; Pang et al., 2024). While substantial research has focused on alignment fine-tuning for LLMs, efforts to adapt these techniques for LVLMs have been significantly limited. Initial attempts involve constructing preference datasets using human-labeled data (Sun et al., 2023) or GPT-4 generations for fine-tuning with a DPO loss (Zhou et al., 2024). Concurrent works (Pi et al., 2024; Zhou et al., 2024) begin to focus on generating preference dataset of LVLMs, while our method distinguishes itself with the unique preference prompt set.

**Self-training.** Traditional self-supervised training schemes (He et al., 2019; Xie et al., 2020; Wei et al., 2020; Zoph et al., 2020; Sohn et al., 2020; Ghiasi et al., 2021; Kang et al., 2023) leverage trained models to generate labels for unlabeled data and incorporate these self-labeled examples into training as a form of data augmentation. These frameworks primarily focus on self-supervised representation learning of vision models. While both classical self-training schemes and our approach share the fundamental goal of effectively utilizing unlabeled data to enhance model performance, our method differs in its focus on vision LLMs, maintaining an LLM as the backbone architecture. Rather than optimizing image representations, our approach aims to generate synthetic data that enables the LLM to produce higher-quality responses to image queries.

## 3 Problem Setting and Preliminaries

**Notation.** We use lower case letters and lower case bold face letters to denote scalars and vectors. We use the symbol \(p\) to represent the probability of an LLM's response. And we denote the sequence of tokens generated from the LLM before the \(t\)-th token as \(_{<t}=[y_{1},,y_{t-1}]\) for \(t>1\).

**Generative vision language models.** LVLM typically consists of three components: a vision encoder \(f()\), a projection network \(g()\), and an LLM \(p_{}\) parameterized by \(\). The model processes an image input \(\) along with a text sequence \(=[x_{1},,x_{n}]\) as the prompt to generate a corresponding

Figure 3: Examples of the self-constructed preference data in STIC.

response \(=[y_{1},,y_{m}]\), where \(x_{i}\) and \(y_{j}\) represent individual tokens from the vocabulary of the LLM. The image is therefore converted into visual tokens within the language token space by the vision encoder and the projection network, producing \(=[v_{1},,v_{k}]=f g()\). The response \(\) is then considered as a sample from the conditional probability distribution \(p_{}(|,)\). As a Markov process, the conditional probability distribution \(p_{}(|,)\) can be decomposed as

\[p_{}(|,)=_{j=1}^{m}p_{}(y_{j}|,,_{<j}). \]

**Alignment fine-tuning.** To improve LLM alignment with human preferences, RL fine-tuning (Bai et al., 2022; Gao et al., 2023) is typically employed after supervised fine-tuning (SFT). This process involves a reward function \(r(,)\) for a given sequence pair \((,)\). The more preferred response \(\) is expected to result in a higher reward \(r(,)\), where the corresponding objective is to maximize the following:

\[L()=_{, p_{}(|)}[r(,)]-_{ }p_{}(|) ||p_{}(|), \]

where \(\) is sampled from a given distribution \(\) and the KL regularization term prevents the new model \(p_{}\) from deviating too much from the reference model \(p_{}\), with \(>0\) as the regularization parameter. Training the reward function is challenging in practice, but direct preference optimization (DPO) (Rafailov et al., 2023) simplifies this process using a predefined preference dataset \(S_{}=^{(i)},_{w}^{(i)},_{l}^{(i)}}_{i[N]}\), where \(_{w}^{(i)}\) denotes the preferred response and \(_{l}^{(i)}\) denotes the dispreferred response given the same prompt \(^{(i)}\). The objective function is then formulated as

\[L_{}(,_{})=_{(,_{w},_{l}) S_{}} }(_{w}|)}{p_{_{ }}(_{w}|)}-}( _{l}|)}{p_{_{}}(_{l}| )}, \]

where \((t)=(1+(-t))\) is the logistic loss function and \(_{}\) is the reference model.

## 4 Our Method: STIC

In this section, we introduce STIC, a two-stage self-training algorithm designed to enhance image comprehension capabilities. The first stage constructs its own preference dataset and the second stage infuses the used SFT data with _self-generated_ image descriptions for fine-tuning. Figure 2 presents the general framework of STIC. Notably, unlike recent work on fine-tuning algorithms (Sun et al., 2023; Zhou et al., 2024), STIC enables a base LVLM, such as LLaVA-v1.6 (Liu et al., 2024), to evolve from _self-generated_ image captions, thus eliminating the need for additional supervised and preference data from human annotators or advanced teacher models. This approach _fundamentally_ enhances image comprehension abilities and can be seamlessly applied to a wide range of vision-language reasoning tasks. We summarize STIC in Algorithms 1 and 2, and detail the process below.

**Stage 1: Image comprehension self-training.** The process begins with a self-constructed preference dataset from the base LVLM, which we aim to improve through fine-tuning. The dataset contains paired preference data for image descriptions:

* _Preferred_ response: Model-generated image descriptions derived from well-crafted prompts with explicit reasoning steps.
* _Dispreferred_ response: Model-generated descriptions resulting from either (1) corrupted image with low resolution or distorted color, or (2) "bad" prompts that cause the base model to hallucinate and describe elements that may not logically exist in the image.

The self-constructed preference dataset is used for the first-stage self-training using DPO (Rafailov et al., 2023) with an additional regularization term to further emphasize the preferred response, controlled by the hyperparameter \(\). The regularized loss function is as follows:

\[L(,_{})=_{(,_{w },_{l}) S}}( _{w}|)}{p_{_{}}(_{w}| )}-}(_{l}|)}{p_ {_{}}(_{l}|)}- p_{ }_{w}|. \]

The use of an explicit loss term for positive examples can be similarly found in previous studies on contrastive learning (Chen et al., 2021; Chen and He, 2021; Chen et al., 2023) and more recently in preference fine-tuning (Pang et al., 2024). Specifically, Chen et al. (2023) demonstrated in the context of contrastive learning that a regularization term applied to positive samples provably enhances the model's ability to differentiate between positive and negative samples. As demonstrated in our experiments in Section 6, the LVLM after Stage 1 has shown notable improvement in downstream vision-language reasoning tasks, confirming that the enhanced visual comprehension ability directly benefits the model performance and its multimodal reasoning ability.

**Prompt design.** Our prompt design for the well-crafted prompt focuses on quality and diversity. We use GPT-4 to generate and sample multiple initial prompts, which are then refined through human filtering. To ensure effectiveness, we test these prompts on MSCOCO samples, verifying their ability to produce well-structured and relevant responses from the model. The bad prompts are sampled from GPT-4 and, in contrast, designed to elicit inaccurate descriptions by setting up a slightly different task (describe objects that would logically exist in the image) for the model. We thus work under the assumption that responses generated from prompts that have differences in human preference lead to responses of the same preference with high probability. The key is that the discrepancy between good and bad prompts should result in pairs of responses that share the same implicit preference with high probability, which is sufficient for effective DPO training.

**Stage 2: Description-infused fine-tuning.** In the second stage, we further fine-tune the self-trained LVLM to leverage self-generated high-quality image descriptions for instruction-following tasks, and thus help ground its reasoning ability on self-generated descriptions. To achieve this, we randomly select a small subset of data from the model's instruction fine-tuning dataset already used during SFT. We then infuse the instructions in this subset with model-generated image descriptions as follows:

Image description: {model description}  <original instruction>

The original ground-truth completions remain unchanged. We then fine-tune the LVLM for one epoch on this small description-infused subset. This fine-tuning step ensures that the model effectively integrates visual information into its responses, thereby enhancing its ability to handle a variety of vision-language reasoning tasks.

**Describe and Respond.** During inference, optionally, we can let the model self-augment its prompt for downstream vision-language reasoning tasks by describing the image before answering. Rather than generating an immediate response, we first elicit an image description, which is then concatenated with the original question to produce a more informed answer.

## 5 Experiments

In this section, we present the experiment results of STIC across seven visual question answering (VQA) benchmarks. We demonstrate that STIC effectively and substantially improves LVLM's performance across different VQA tasks using a self-constructed preference dataset without labels.

### Experiment Setup

**Model and datasets.** In experiments, we consider llava-v1.6-mistral-7b (Liu et al., 2023a) as our base model for self-training with model generated preference data. We additionally consider llava-v1.5-7b (Liu et al., 2023a) based on Vicuna-7B (Chiang et al., 2023b) to directly compare with one concurrent baseline POVID (Zhou et al., 2024). A detailed discussion with POVID can be found in Appendix C.3. We follow the optimization process described in Section 4 for self-training on image description in Algorithm 1 and description-infused fine-tuning in Algorithm 2 to achieve improved downstream performances. For the self-constructed preference dataset, we gather **6\(k\) unlabeled image data** randomly sampled from the MSCOCO dataset (Lin et al., 2014) and specifically the train2014 split for its high-quality images popularly used for pre-training and fine-tuning. In the second stage, we randomly subsample **5\(k\) used instruction fine-tuning data** from LLaVA's SFT data to construct the description-infused fine-tuning data with model-generated image descriptions. Lastly, we use low-rank adaptation (LoRA) fine-tuning (Hu et al., 2021) instead of full fine-tuning for efficient computation. We defer the detailed prompts and corruptions to Appendix B.

**Evaluation.** We consider the widely used benchmarks for LVLM evaluation across different domains including: ScienceQA (Lu et al., 2022), TextVQA (Singh et al., 2019), ChartQA (Masry et al., 2022), LLaVA-Bench (Liu et al., 2023a), MMBench (Liu et al., 2023c), MM-Vet (Yu et al., 2023), and MathVista (Lu et al., 2024). Specifically, ScienceQA focuses on scientific question answering and MathVista focuses on math reasoning with visual information. TextVQA consists of images with text-rich contents and ChartQA with visual charts. Lastly, LLaVA-Bench, MMBench, and MM-Vet are three recent benchmarks to comprehensively evaluate a model's capabilities in a wide range of tasks and evaluation criteria. We use the evaluation scripts provided by LLaVA (Liu et al., 2023a) to obtain the results for both our base model and after using STIC to ensure a fair comparison.

### Main Results

We present our main results in Table 1 and detail the benchmark performances of STIC (LLaVA-v1.6-7B) on MMBench and MM-Vet in Figure 4. In Appendix B, we present detailed results for MMBench in Table 7 and MM-Vet in Table 8. Our results show a consistent and significant improvement of STIC over the original models (LLaVA-v1.5 and LLaVA-v1.6) across all seven datasets. This improvement is achieved using only self-constructed preference data and a small portion of the model's SFT dataset, which had already been used for fine-tuning the original model.

   Model & ScienceQA & TextVQA & ChartQA & LLaVA-Bench & MMBench & MM-Vet & MathVista \\   InstructBLIP (7B) & 60.5 & 50.1 & – & 60.9 & 36.0 & 26.2 & 25.3 \\ mPLUG-OWL2 (7B) & 64.5 & 54.3 & – & 59.9 & 64.5 & 36.2 & 22.2 \\  LLaVA-v1.5 (7B) & 66.8 & 58.2 & 6.3 & 65.4 & 64.3 & 31.1 & 25.1 \\ w/ POVID & 68.8 & – & – & 68.7 & 64.9 & 31.8 & – \\ w/ STIC & **69.5** & **61.4** & **6.6** & **68.9** & **65.3** & **32.6** & **27.2** \\  LLaVA-v1.6 (7B) & 68.9 & 60.3 & 36.4 & 77.3 & 63.7 & 42.2 & 34.6 \\  w/ STIC & **75.3** & **65.2** & **41.5** & **79.2** & **67.8** & **45.0** & **37.0** \\   

Table 1: Performance of STIC compared with the original LVLM model across vision-language reasoning tasks. For LLaVA-v1.5 (Vicuna 7B), we directly report the values in the paper of POVID, and “–” indicates an unreported value.

On average, STIC improves LLaVA-v1.5 by \(1.7\%\), increasing from \(45.3\%\) to \(47.0\%\), and LLaVA-v1.6 by a notable score of \(4.0\%\), increasing from \(54.7\%\) to \(58.7\%\). The improvement is comprehensive, as detailed in Tables 7 and 8, where STIC consistently enhances performance across all evaluation tasks and targets. Moreover, while STIC improves both LLaVA-v1.5 and LLaVA-v1.6, a more significant improvement is observed in the more advanced model, LLaVA-v1.6. This trend suggests that the extent of self-improvement could be correlated with the model's inherent capabilities.

## 6 Ablation Studies and Discussions

In this section, we conduct ablation studies on the key components of STIC to demonstrate their importance and effectiveness. Additionally, we examine the image distribution of our self-training data (MSCOCO) alongside the image distributions of benchmark datasets, revealing a positive correlation between performance gains and similarity in image distributions.

**Effectiveness of describe-and-respond (DaR) prompting.** We assess the significance of the fine-tuning process in STIC by comparing it to the approach of directly allowing the base LVLM to describe an image and then respond to a query with a self-augmented prompt, which we refer to as the describe-and-respond (DaR) prompting method. As indicated in Table 2, applying DaR to the base LVLM yields mixed results, with performance improvements on some datasets and degradation on others, resulting in an overall average drop of \(2.3\%\). In contrast, when DaR is combined with the fine-tuning process of STIC, it leads to a further average enhancement of \(1.1\%\) and a notable \(2.8\%\) on ScienceQA. This demonstrates the synergistic effect of DaR and the fine-tuning process in STIC. Additionally, it is worth noting that STIC achieves a substantial average improvement of \(2.8\%\) even without the DaR prompting method, compared to the base LVLM.

**Progression of stages.** In Figure 5, we illustrate the sequential improvement in performance of STIC on ScienceQA. While stage 1 focuses exclusively on enhancing the perception capabilities of the LVLM, it still notably improves performance on downstream VQA tasks. Building on the improved image comprehension achieved in stage 1, stage 2 introduces an enhanced reasoning process that utilizes the model's self-generated image descriptions and results in an even more significant gain. This enhancement further enables the model to self-augment its prompts with Describe and Respond (DaR), resulting in total the substantial performance gains of \(6.4\%\) observed.

  Method & DaR & ScienceQA & TextVQA & ChartQA & LLaVA-Bench & MMBench & MM-Vet & MathVista & Average \\   & \(\) & 68.9 & 60.3 & 36.4 & 77.3 & 63.7 & 42.2 & 34.6 & 54.8 \\  & \(\) & 69.9 & 56.6 & 34.6 & 78.5 & 50.7 & 42.3 & 34.7 & 52.5 \\   & \(\) & 72.5 & 63.4 & 39.3 & 78.4 & **68.7** & **45.7** & 35.2 & 57.6 \\   & \(\) & **75.3** & **65.2** & **41.5** & **79.2** & 67.8 & 45.2 & **37.0** & **58.7** \\  

Table 2: Test performance of STIC based on llava-v1.6-mistral-7b. We investigate the benefit of DaR as a prompting method toward the base LVLM model as compared to the benefit on STIC.

Figure 4: Accuracy improvement of STIC compared to the base LLaVA-v1.6 model across different tasks in **Left**: MMBench, where the original performances are re-scaled to 60 in plotting and STIC accordingly with the same coefficient for each task. **Middle**: MM-Vet, where the performances of the original model are re-scaled to 40 and STIC accordingly. **Right**: LLaVA-Bench, where we report the error bars over three independent runs due to the randomness of GPT-4 evaluation.

Figure 5: Progression of stages in STIC.

**The role of dispreferred samples in STIC.** To understand the importance of dispreferred samples in STIC, we conduct an ablation study using llava-v1.6-mistral-7b as the base LVLM. We remove the negative examples from the preference data and only use the positive samples for supervised fine-tuning (SFT), effectively creating an SFT version of STIC. Table 3 shows that omitting the dispreferred samples even leads to a performance drop of \(0.6\%\) on LLaVA-Bench, while failing to provide equally significant improvement as STIC with preference data. This highlights the crucial role of negative examples in aligning preferences and enabling the model to distinguish between high-quality and low-quality responses. By leveraging both positive and negative examples, STIC effectively improves the model's performance and generates more preferred outputs.

**Scaling law of STIC.** We explore the scaling law of STIC by expanding the preference data in Stage 1. Using the LLaVA-Bench benchmark as a case study, we scale up the preference data from \(6k\) to \(12k\) MSCOCO images. As depicted in Figure 6, there is an obvious gain on the LLaVA-Bench from \(1.9\%\) to \(3.1\%\). This finding demonstrates that STIC can effectively leverage larger amounts of unlabeled image data and presents a cost-effective solution to the challenge of acquiring high-quality vision-language data.

**Correlation between image distribution and performance gains.** To gain further insight into the effectiveness of STIC across different benchmarks, we conducted a t-SNE visualization analysis comparing the image distributions of MSCOCO, which we used for preference data construction, with those of four benchmarks: ScienceQA, TextVQA, MathVista, and ChartQA (Figure 7). Our analysis revealed a general trend: the greater the overlap between the MSCOCO image distribution and that of a benchmark, the higher the performance gain achieved by STIC on that benchmark. This observation held true for ScienceQA and TextVQA, which exhibited substantial distributional overlap with MSCOCO and yielded the highest performance gains of \(6.4\%\) and \(4.9\%\), respectively. Conversely, MathVista, with its diverse image types and limited overlap with MSCOCO, saw a more modest gain of \(2.4\%\). Interestingly, ChartQA was an outlier, achieving a high gain of \(5.1\%\) despite minimal overlap with MSCOCO, suggesting that the improved image comprehension from STIC played a fundamental role in understanding and reasoning about the charts. Detailed per-benchmark visualizations and discussions are provided in Appendix C.2.

**Diversity in image distribution.** Based on the observation on the effect of image distribution in the final performance, we further utilize the Vision Flan dataset (VFLAN+) for stage 1 image comprehension self-training. This dataset includes images from 191 diverse vision tasks, providing a broader spectrum of image types. We ensured a fair comparison by maintaining the same sample

   Model & ScienceQA & TextVQA & LLaVA-Bench \\  Original & 68.9 & 60.3 & 77.3 \\ w/ STIC (positive) & 71.8 & 63.7 & 76.7 \\ w/ STIC & **75.3** & **65.2** & **79.2** \\   

Table 3: Test performance of STIC if we remove negative examples and use positive ones to perform SFT in Stage 1.

Figure 6: Scaling law in STIC.

Figure 7: t-SNE visualization of images from MSCOCO and four benchmarks, each sampling \(1k\).

size (randomly sampled \(6k\) images) and present the experimental results in Table 4. As shown, our approach improves consistently across different datasets, demonstrating its robustness and adaptability. Notably, the increased diversity of VFLAN led to further improvements in STIC, suggesting the potential for even greater enhancement with better sets of unlabeled images.

**Scalability.** To explore STIC's applicability to models with higher representation capacity, we conducted supplementary experiments using LLaVA-v1.6 (Vicuna-13B). Table 5 shows the detailed experiment results. We used the same images for STIC fine-tuning as in our experiments for LLaVA-v1.6 (Mistral-7B) to ensure fairness and the same set of hyperparameters. The improvements observed with LLaVA-v1.6 (Vicuna-13B) demonstrate that STIC is not only effective with smaller models but also scales well with larger or more capable LVLMs.

**Qualitative example.** In Figure 8, we show an example output of STIC. Despite the task being focused on mathematical reasoning, STIC enhanced the model's response by improving its image comprehension capabilities. While the original model merely identified one of the recognized numbers in the image as the final answer, the STIC fine-tuned model was able to interpret the meaning of each number, describe them accurately, and perform reasoning based on this understanding.

## 7 Conclusion

We introduce Self-Training on Image Comprehension (STIC), a novel self-training approach designed to enhance the image comprehension capabilities of large vision language models (LVLMs). Our method leverages a two-stage self-training process, creating a preference dataset for image descriptions from unlabeled images and refining reasoning abilities through description-infused fine-tuning. Extensive experiments across seven vision-language benchmarks demonstrated significant performance improvements, with an average accuracy gain of \(4.0\%\), while reducing the need for supervised fine-tuning data by \(70\%\). Our findings underscore the potential of STIC to harness vast quantities of unlabeled images, offering a cost-effective solution for advancing LVLMs.

The promising results demonstrated by STIC in enhancing the capabilities of 7B LVLMs suggest its potential applicability to larger models, such as those with 40B, and 100B parameters, if computational resources permit.

    &  &  &  &  \\   & & Complex & Conv & Detail & **All** & Rec & Ocr & Know & Gen & Spat & Math & **Total** & **All** \\   LLaVA-v1.6 (7B) & - & 87.4 & 61.3 & 77.8 & 77.3 & 43.1 & 40.6 & 29.6 & 32.5 & 44.7 & 15.4 & 42.2 & 63.7 \\  w/ STIC & COCO & 89.1 & 63.7 & 79.5 & 79.2 & 45.7 & 42.5 & 30.4 & 34.9 & 45.1 & 22.7 & 45.0 & 67.8 \\  w/ STIC & VFLAN & 92.8 & 68.4 & 77.9 & **81.9** & 45.7 & 43.0 & 31.0 & 36.2 & 45.1 & 26.5 & **45.1** & **68.3** \\   

Table 4: Performance of STIC on different stage-1 training images compared with the original LVLM model LLaVA-v1.6 (Mistral 7B) across vision-language reasoning benchmarks.

Figure 8: Response examples from original LLaVA-v1.6 and STIC (LLaVA-v1.6) in MM-Vet.

    &  &  &  \\   & Complex & Conv & Detail & **All** & Rec & Ocr & Know & Gen & Spat & Math & **Total** & **All** \\   LLaVA-v1.6 (7B) & 87.4 & 61.3 & 77.8 & 77.3 & 43.1 & 40.6 & 29.6 & 32.5 & 44.7 & 15.4 & 42.2 & 63.7 \\  LLaVA-v1.6 (13B) & 94.0 & 73.8 & 78.7 & 84.5 & 52.2 & 47.1 & 38.8 & 45.2 & 42.7 & 26.9 & 48.9 & 70.6 \\  w/ STIC & 93.5 & **78.1**(+-43) & **79.4** & **85.6**(+-11) & **54.5** & **48.0** & **42.3**(+-35) & **49.4**(+-42) & 42.0 & 23.1 & **50.5**(+-16) & **72.3**(+-17) \\   

Table 5: Performance of STIC compared with the original LVLM model LLaVA-v1.6 (Vicuna 13B) across vision-language reasoning tasks. Image data used for 13B model remain the same as what we used for the 7B model.

### Contribution Statement

The project began in January 2024 with Deng as part of Gu's group, collaborating with Lu from Chang's group. This project was in the line of research on self-training using synthetic data developed by Gu and Deng. Deng proposed the initial idea of this project, which was further developed with Lu. In February 2024, Yin joined the project. Both Gu and Chang offered early feedback to help shape the project. In March, Chang met with Deng, Lu, and Yin to discuss the initial experiment results and finalize the research plan. Chang continued supervising the project through one-on-one meetings with Lu. In April, Deng changed advisors and moved to Wang's lab, inviting Hu, Shen, Wang, and Zou to join the project. Hu and Shen contributed algorithmic improvements, while Wang and Zou provided detailed feedback on the experimental design and ablation studies. Deng and Lu primarily conducted the experiments and drafted the paper.