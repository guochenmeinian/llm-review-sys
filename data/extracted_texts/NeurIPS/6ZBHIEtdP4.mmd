# PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models

Fanxu Meng\({}^{1,2}\), Zhaohui Wang\({}^{1}\), Muhan Zhang\({}^{1,2}\)

\({}^{1}\)Institute for Artificial Intelligence, Peking University

\({}^{2}\)State Key Laboratory of General Artificial Intelligence, Peking University

https://github.com/GraphPKU/PiSSA

Correspondence to: Muhan Zhang <muhan@pku.edu.cn>

###### Abstract

To parameter-efficiently fine-tune (PEFT) large language models (LLMs), the low-rank adaptation (LoRA) method approximates the model changes \( W^{m n}\) through the product of two matrices \(A^{m r}\) and \(B^{r n}\), where \(r(m,n)\), \(A\) is initialized with Gaussian noise, and \(B\) with zeros. LoRA **freezes the original model \(W\)** and **updates the "Noise & Zero" adapter**, which may lead to slow convergence. To overcome this limitation, we introduce **P**rincipal **S**ingular values and **S**ingular vectors **A**daptation (PiSSA). PiSSA shares the same architecture as LoRA, but initializes the adaptor matrices \(A\) and \(B\) with the principal components of the original matrix \(W\), and put the remaining components into a residual matrix \(W^{res}^{m n}\) which is frozen during fine-tuning. Compared to LoRA, PiSSA **updates the principal components** while **freezing the "residual" parts**, allowing faster convergence and enhanced performance. Comparative experiments of PiSSA and LoRA across 11 different models, ranging from 184M to 70B, encompassing 5 NLG and 8 NLU tasks, reveal that PiSSA consistently outperforms LoRA under identical experimental setups. On the GSM8K benchmark, Gemma-7B fine-tuned with PiSSA achieves an accuracy of 77.7%, surpassing LoRA's 74.53% by 3.25%. Due to the same architecture, PiSSA is also compatible with quantization to further reduce the memory requirement of fine-tuning. Compared to QLoRA, QPiSSA (PiSSA with 4-bit quantization) exhibits smaller quantization errors in the initial stages. Fine-tuning LLaMA-3-70B on GSM8K, QPiSSA attains an accuracy of 86.05%, exceeding the performance of QLoRA at 81.73%. Leveraging a fast SVD technique, PiSSA can be initialized in only a few seconds, presenting a negligible cost for transitioning from LoRA to PiSSA.

## 1 Introduction

Fine-tuning large language models (LLMs) is a highly effective technique for boosting their capabilities in various tasks , ensuring models to follow instructions , and instilling models with desirable behaviors while eliminating undesirable ones . However, the fine-tuning process for very large models is accompanied by prohibitive costs. For example, regular 16-bit fine-tuning of a LLaMA 65B parameter model requires over 780 GB of GPU memory , and the VRAM consumption for training GPT-3 175B reaches 1.2TB . Consequently, various parameter-efficient fine-tuning (PEFT)  methods have been proposed to reduce the number of parameters and memory usage required for fine-tuning. Due to the ability to maintain the performance of full fine-tuning without adding additional inference latency, Low-Rank Adaptation (LoRA)  has emerged as a popular PEFT method.

LoRA  hypothesizes that the modifications to parameter matrices during fine-tuning exhibit low-rank properties. As depicted in Figure 0(b), for a pre-trained weight matrix \(W^{m n}\), LoRA substitutes the updates with a low-rank decomposition \( W=AB\), where \(A^{m r}\) and \(B^{r n}\), and the rank \(r(m,n)\). For \(Y=XW\), the modified forward pass is as follows:

\[Y=X(W+ W)=X(W+AB),\] (1)

A random Gaussian initialization is used for \(A\) and zero for \(B\), making \(AB=0\) at the beginning of training, thereby the injection of adapters does not affect the model's output initially. LoRA avoids the need to compute gradients or maintain the optimizer states for the original matrix \(W\), instead optimizing the injected, significantly smaller low-rank matrices \(A,B\). Thus, it could reduce the number of trainable parameters by 10,000\(\) and the GPU memory requirement by 3\(\). LoRA is capable of achieving comparable performance to full parameter fine-tuning. By integrating the quantization of pre-trained matrices \(W\), LoRA also enables reducing the average memory requirements by 16\(\). Meanwhile, the adapters can still utilize higher precision weights, thus, the quantization usually does not significantly degrade the performance of LoRA.

According to Equation 1, the gradients of A and B are \(=X^{}( )B^{}\) and \(=A^{}X^{}()\). Compared to full fine-tuning, using LoRA initially does not change the output \(Y\) for the same input \(X\), so the magnitude and direction of gradient are primarily determined by the values of \(A\) and \(B\). Since \(A\) and \(B\) are initialized with Gaussian noise and zeros in LoRA, the gradients could be small

    &  \\  Forward & \(Y=X(+)=X(+)\) & \(Y=X(}+})=X(}+)\) \\   & \((0,^{2})^{m r}\) & \(=U_{[,]}\)\(S^{1/2}_{[,]}^{m r}\) \\  & \(=0^{r n}\) & \(=S^{1/2}_{[,]}\)\(V^{}_{[]}^{r n}\) \\  & \(}=U_{[,]}\)\(S_{[,]}\)\(V^{}_{[,]}\)\(^{m n}\) \\   & \(=X^{}( )B^{}\) & \(=X^{}( )^{}\) \\  & \(=A^{}X^{}()\) & \(=^{}X^{}()\) \\   & Fine-tunes noise while freezing \(\). & Fine-tunes **principal** parts freezing \(}\). \\  & Slow convergence and underperformance. & **Fast** convergence and **better** performance. \\  & QLoRA cannot reduce quantization error. & QPiSSA **can** reduce quantization error. \\   

Table 1: Comparison of similarities and differences between PiSSA and LoRA. In this table, **bold** highlights the model’s primary component, while underline denotes the residual component.

Figure 1: The comparison among Full Fine-tuning, training with LoRA, and PiSSA. In this visualization, blue modules represent parts of the model whose parameters are frozen during training, while orange modules indicate components that require updates. QLoRA quantizes the pretrained matrix in LoRA to 4-bit, whereas QPiSSA quantizes the residual matrix in PiSSA.

and uninformative for a long time, leading to slow convergence in the fine-tuning process. We also observe this phenomenon empirically, as LoRA often wastes much time around the initial point.

Our **P**rincipal **S**ingular values and **S**ingular vectors **A**dapter (PiSSA) diverges from LoRA and its successors by focusing not on approximating \( W\), but \(W\). We apply singular value decomposition (SVD) to matrix \(W\). Based on the magnitude of the singular values, we partition \(W\) into two parts: the principal low-rank matrix \(W^{pri}\), comprising a few largest singular values, and the residual matrix \(W^{res}\), which possesses the remaining smaller singular values (with a larger quantity, representing a possible long-tail distribution). The principal matrix \(W^{pri}\) can be represented by the product of \(A^{m r}\) and \(B^{r n}\), where \(r(m,n)\). As depicted in Figure 0(c), \(A\) and \(B\) are initialized based on the principal singular values and singular vectors and are trainable. Conversely, \(W^{res}\) is initialized with the product of the residual singular values and singular vectors and remains frozen during fine-tuning. Since the principal singular vectors represent the directions in which the matrix \(W\) has the most significant stretching or impact, by directly tuning these principal components, PiSSA is able to **fit the training data faster and better** (as demonstrated in Figure 1(a)). Moreover, the loss and gradient norm curves of PiSSA often demonstrate a similar trend to those of full parameter fine-tuning in our experiments (Figure 4), indicating that fine-tuning the principal components matches the behavior of fine-tuning the full matrix to some degree.

Because the principal components \(W^{pri}\) are preserved in the adapter at full precision, an additional benefit of PiSSA is that when applying quantization to the frozen part \(W^{res}\), we can significantly **reduce the quantization error** compared to QLoRA (which quantizes the entire \(W\)), as illustrated in Figure 1(b). Therefore, PiSSA is even more compatible with quantization than LoRA, making it a superior plug-and-play substitution for LoRA.

Our paper makes several significant contributions:

* We analyze the initial gradient magnitude and direction in LoRA, demonstrating that \(A\) initially has a zero gradient and \(B\) has a random gradient, which slows down convergence and may lead to convergence at suboptimal local minima.
* We propose PiSSA initialization, a novel method that approximates the optimization direction of full-parameter fine-tuning by adapting a model's principal components. To our knowledge, PiSSA is the first to apply SVD to the original model, using principal singular values and vectors to initialize the adapter for fine-tuning, while keeping the residual components frozen. Experiments show that PiSSA converges faster and outperforms LoRA.
* We combine PiSSA with NF4 quantization to propose QPiSSA, which reduces quantization error by about 20% compared to QLoRA, while maintaining the fast convergence and high performance of PiSSA.

Figure 2: We illustrate the two key advantages of PiSSA: converging faster and better, and reducing quantization error. In the left figure, we use a toy example to show PiSSA’s faster convergence, where we first train a two-layer MLP classifying odd numbers of MNIST, and then fine-tune the model on even numbers. PiSSA finds the right direction more quickly and achieves a lower loss with the same number of steps. In the right figure, PiSSA reduces quantization error more effectively than LoftQ , with an optional 5-iteration SVD for further error reduction, as detailed in Appendix E.

Related Works

The vast complexity and computational needs of large language models (LLMs) with billions of parameters present significant hurdles in adapting them for specific downstream tasks. Parameter Efficient Fine-Tuning (PEFT) [12; 13] emerges as a compelling solution by minimizing the fine-tuning parameters and memory requirements while achieving comparable performance to full fine-tuning. PEFT encompasses strategies like partial fine-tuning [15; 16; 17; 18; 19; 20; 21; 22], soft prompt fine-tuning [23; 24; 25; 26; 27; 28; 29], non-linear adapter fine-tuning [30; 31; 32; 33; 34; 35; 36; 37; 38; 39], and low rank adapter based fine-tuning [40; 41; 11; 42].

LoRA  injects trainable adapters to the linear layers. After fine-tuning, these adaptations can be re-parameterized into the standard model structure, thus gaining widespread adoption due to their ability to maintain the model's original architecture while enabling efficient fine-tuning. Following LoRA, AdaLoRA  dynamically learns the rank size needed for LoRA in each layer of the model. DeltaLoRA  updates the original weights of the model using parameters from adapter layers, enhancing LoRA's representational capacity. LoSparse  incorporates LoRA to prevent pruning from eliminating too many expressive neurons. DoRA  introduces a magnitude component to learn the scale of \( W\) while utilizing the original AB as a direction component of \( W\). Unlike LoRA and its successors, which focus on learning low-rank approximations of weight updates, our PiSSA directly tunes the essential low-rank parts of the model while keeping the noisier, high-rank, and nonessential parts frozen. Although our approach differs in philosophy from LoRA, it shares most of LoRA's structural benefits and can be extended by these methods to enhance its performance.

QLoRA  integrates LoRA with 4-bit NormalFloat (NF4) quantization, along with Double Quantization and Paged Optimizers, enabling the fine-tuning of a 65B parameter model on a single 48GB GPU while preserving the performance of full 16-bit fine-tuning tasks. QA-LoRA  introduces group-wise operators to increase the degree of freedom in low-bit quantization. LoftQ  reduces quantization error by decomposing the quantization error matrix of QLoRA and retaining the principal components with an adapter. PiSSA can also be combined with quantization techniques, and we have found that PiSSA significantly reduces quantization error compared to QLoRA and LoftQ.

## 3 PiSSA: Principal Singular Values and Singular Vectors Adaptation

This section formally presents our **P**nincipal **S**ingular values and Singular vectors **A**daptation method. PiSSA computes the singular value decomposition (SVD) of matrices \(W\) within the self-attention and multilayer perceptron (MLP) layers. The (economy size) SVD of a matrix \(W^{m n}\) is given by \(W=USV^{}\), where \(U^{m(m,n)},V^{n(m,n)}\) are the singular vectors with orthonormal columns, and \(V^{}\) is the transpose of \(V\). \(S=()^{(m,n)(m,n)}\), where the operation \(()\) transforms \(\) to a diagonal matrix \(S\), and \(^{(m,n)}_{ 0}\) represents the singular values arranged in descending order. When the top \(r\) singular values \(_{[:r]}\) are significantly larger than the remaining singular values \(_{[:r]}\), we denote the intrinsic rank of \(W\) as \(r\). Consequently, \(S\), along with \(U\) and \(V\), can be divided into two groups: the principal singular values and vectors--\(\{U_{[:,r]},S_{[:,r:]},V_{[:,:r]}\}\), and the residual singular values and vectors--\(\{U_{[:,r]},S_{[r:,r]},V_{[:,r:]}\}\), where the matrix slicing notations are the same as those in PyTorch and \([:r]\) denotes the first \(r\) dimensions. The principal singular values and vectors are utilized to initialize the injected adapter consisting of \(A^{m r}\) and \(B^{r n}\):

\[A =U_{[:,r]}\,S_{[r:,r]}^{1/2}^{m r},\] (2) \[B =S_{[r:,r]}^{1/2}\,V_{[:,r]}^{}^{r n}.\] (3)

The residual singular values and vectors are used to build the residual matrix which is frozen during fine-tuning:

\[W^{res}=U_{[:,r]}\,S_{[r:,r]}\,V_{[:,r:]}^{}^{m n}.\] (4)

As indicated by Equation 5, the integration of \(AB\) with the residual matrix also preserves the full capability of the pre-trained model in the beginning of fine-tuning:

\[Y=XW=X(W^{res}+W^{pri})=X(W^{res}+AB).\] (5)Similar to LoRA, the gradients of \(A\) and \(B\) are also given by \(=X^{}()B ^{}\) and \(=A^{}X^{}()\). Since elements of \(_{[:r]}\) elements of \(_{[:r]}\), the trainable adapter \(W^{pri}=AB\) contains the most essential directions of \(W\). In the ideal case, training \(AB\) mirrors the process of fine-tuning the entire model despite using fewer parameters. The ability to directly fine-tune the most essential part of a model enables PiSSA to converge faster and better. In contrast, LoRA initializes the adapters \(A\) and \(B\) with Gaussian noise and zeros while keeping \(W\) frozen. Consequently, the gradients are small or in random directions during the early stages of fine-tuning, possibly introducing much waste of gradient descent steps. Moreover, an inferior initialization might lead to suboptimal local minimum, resulting in worse generalization performance.

Since PiSSA shares the identical architecture with LoRA, it inherits most of LoRA's benefits. These include but are not limited to the capability of fine-tuning a model with a reduced number of trainable parameters, quantizing the residual model to decrease memory consumption during forward propagation in training, and easy deployment. The adapter's straightforward linear structure facilitates the integration of trainable matrices with the pre-trained weights upon deployment, thereby maintaining the original inference speed of a fully fine-tuned model. Employing the Fast SVD technique  allowed PiSSA to finish initialization in several seconds (Appendix B), which is a negligible cost.

For storage efficiency, we can choose not to store the dense parameter matrix \( W\), but to store the low-rank matrices, \( A\) and \( B\) instead. As shown in Appendix C, leveraging solely the \( A\) and \( B\) facilitates their seamless integration with the original pre-trained models. Finally, one pre-trained model can accommodate multiple \( A, B\), fine-tuned by diverse PiSSA or LoRA procedures, which enables fast adaptation of the pre-trained model to different downstream applications.

## 4 QPiSSA: An Extension Method with Lower Quantization Error

Quantization divides the value range of a matrix into several continuous regions, and maps all values falling inside a region into the same "quantized" value. It is an effective technique to reduce the memory consumption of forward propagation. At the same time, LoRA greatly reduces the backward memory requirement, making it highly suitable to use LoRA and quantization together, where the base model is quantized for memory-efficient forward propagation, and the LoRA adaptors are kept in full precision for accurate backward parameter updates. One representative previous work, QLoRA, quantizes the base model to Normal Float 4-bit (NF4) and initializes the full-precision \(A\) and \(B\) with Gaussian-Zero initialization. Therefore, the overall error is given by:

\[=||W-(nf4(W)+AB)||_{*}=||W-nf4(W)||_{*},\] (6)

where \(||M||_{*}\) denotes the nuclear norm (also known as the trace norm ), defined as:

\[\|M\|_{*}=(M})=_{i=1}^{\{m,n\}} _{i}(M),\] (7)

where \(_{i}(M)\) is the \(i^{}\) singular value of \(M\). As we can see, the quantization error of QLoRA is the same as that of directly quantizing the base model. Our QPiSSA, however, **does not quantize the base model but the residual model**. Therefore, its error is given by:

\[=||W-(nf4(W^{res})+AB)||_{*}=||W^{res}- nf4(W^{res})||_{*}.\] (8)

Since the residual model has removed the large-singular-value components, \(W^{res}\) has a **narrower distribution** than that of \(W\), as can be seen in Figures 2(a) and 2(b) (comparing the singular value distributions of \(W\) and \(W^{res}\)), as well as Figures 2(c) and 2(f) (comparing the value distributions of \(W\) and \(W^{res}\)), which is **beneficial for reducing the quantization error**. Moreover, given that NF4 is optimized for data with a normal distribution, as discussed by Dettmers et al. , we fit the values of \(W\) and \(W^{res}\) to a Gaussian distribution respectively. As illustrated in Figures 2(c) and 2(f), \(W^{res}\) aligns more closely with a Gaussian distribution and exhibits a smaller standard deviation, making it more suitable for applying NF4 than \(W\). Both the above lead QPiSSA to achieve a significantly lower quantization error than QLoRA, shown in Figures 2(d) and 2(e).

Besides the advantage of reducing quantization error, QPiSSA's gradient direction is similar to that of PiSSA, resulting in significantly better fine-tuning performance compared to QLoRA.

## 5 Experiments

The experiments were conducted on the NVIDIA A800-SXM4(80G) GPU. In our experiments, we adopt the Alpaca  implementation strategy, using the AdamW optimizer with a batch size of 128, a learning rate of 2e-5, cosine annealing schedules, and a warmup ratio of 0.03, without any weight decay. As discussed in Section B.3 of QLoRA , we compute the loss using only the responses from the instruction-following datasets. We ensure lora_alpha is always equal to lora_r, set lora_dropout to 0, and incorporate the adapters into all linear layers of the base model. We utilize the Float32 computation type for both the base model and the adapter in LoRA and PiSSA. For QLoRA, LoftQ, and QPiSSA, we use 4-bit NormalFloat  for the base model and Float32 for the adapter. BFloat16  is used for full parameter fine-tuning to save the resources (see Appendix D).

### Evaluating the Performance of PiSSA on both NLG and NLU Tasks

We begin by comparing PiSSA, LoRA, and full-parameter fine-tuning on natural language generation (NLG) tasks. We fine-tuned LLaMA 2-7B , Mistral-7B-v0.1 , and Gemma-7B  on the MetaMathQA dataset  to assess their mathematical problem-solving capabilities on the GSM8K  and MATH  validation sets. Additionally, the models were fine-tuned on the Code-Feedback dataset  and evaluated for coding proficiency using the HumanEval  and MBPP  datasets. Furthermore, the models were trained on the WizardLM-Evol-Instruct dataset  and tested for conversational abilities on the MT-Bench dataset . All experiments were conducted using subsets containing 100K data points and were trained for only one epoch to reduce training overhead.

As shown in Table 2, across all models and tasks, fine-tuning with PiSSA consistently surpasses the performance of fine-tuning with LoRA. Further experiments demonstrated that this improvement is robust across various amounts of training data and epochs (Section 5.2), including both 4-bit and full precision (Section 5.3), different model sizes and types (Section 5.4), and varying proportions of trainable parameters (Section 5.5).

We also evaluate PiSSA's natural language understanding (NLU) capability on the GLUE benchmark  with DeBERTa-v3-base . Table 3 presents the results across 8 tasks. PiSSA outperforms LoRA in 7 out of 8 NLU tasks, achieving an overall average improvement of 1.21%. Upon reviewing the training loss on the exceptional dataset, MNLI, we observed that PiSSA's average loss of \(0.17\) was lower than LoRA's \(0.24\) in the final epoch. This indicates that the fitting ability of PiSSA remains stronger than that of LoRA.

Figure 3: Visualizations of LLaMA 2-7B’s “layers.self_attn_q_proj” matrix, with distributions for the full model shown in Appendix G. Figures (a), (b), (d), and (e) display the singular values of \(W\), \(W^{res}\), \(W-nf4(W)\), and \(W^{res}-nf4(W^{res})\), respectively. Figures (c) and (f) show the data distributions of \(W\) and \(W^{res}\).

### Experiments using Full Data and More Epochs

In this section, we finetune LLaMA 2-7B model on the complete MetaMathQA-395K dataset for 3 epochs to ensure thorough saturation. The training loss and gradient norms is visualized to demonstrate quicker convergence and evaluated on the GSM8K dataset every 1000 steps to demonstrate superior performance of PiSSA compared to LoRA. The results are depicted in Figure 4. Additionally, similar comparisons on Mistral-7B and Gemma-7B are detailed in Appendix J.

According to Figure 3(a), the loss of PiSSA reduces rapidly during the first 100 steps, and the grad norm (shown in Figure 3(b)) of PiSSA is significantly higher than that of LoRA, with a trend similar to full fine-tuning. Throughout the process, the loss of PiSSA remains lower than that of LoRA, indicating

  
**Model** & **Strategy** & **GSM8K** & **MATH** & **HumanEval** & **MBPP** & **MT-Bench** \\   & Full FT & 49.13\(\)0.21 & 7.29\(\)0.22 & 21.20\(\)0.30 & 35.59\(\)0.25 & **4.91\(\)0.01** \\  & LoRA(gaussian) & 42.85\(\)0.12 & 5.50\(\)0.33 & 18.35\(\)0.31 & 35.50\(\)0.14 & 4.59\(\)0.07 \\  & LoRA(kaiming) & 43.23\(\)0.64 & 5.90\(\)0.16 & 18.21\(\)0.45 & 35.47\(\)0.37 & 4.56\(\)0.04 \\  & PiSSA & **53.22\(\)0.55** & **7.47\(\)0.34** & **21.92\(\)0.38** & **37.24\(\)0.63** & 4.88\(\)0.03 \\   & Full FT & 69.91\(\)0.25 & 18.64\(\)0.35 & 45.31\(\)0.14 & 51.46\(\)0.13 & 4.95\(\)0.05 \\  & LoRA(gaussian) & 69.50\(\)0.42 & 20.08\(\)0.20 & 43.78\(\)0.34 & 58.46\(\)0.37 & 4.90\(\)0.05 \\  & LoRA(kaiming) & 69.40\(\)0.25 & 19.99\(\)0.44 & 43.74\(\)0.14 & 58.39\(\)0.02 & 4.93\(\)0.05 \\  & PiSSA & **73.31\(\)0.23** & **23.12\(\)0.52** & **46.88\(\)0.25** & **62.55\(\)0.58** & **5.34\(\)0.04** \\   & Full FT & 72.09\(\)0.32 & 22.71\(\)0.34 & 47.02\(\)0.27 & 55.67\(\)0.50 & 5.40\(\)0.12 \\  & LoRA(gaussian) & 75.11\(\)0.64 & 30.41\(\)0.48 & 53.70\(\)0.23 & 65.58\(\)0.29 & 4.98\(\)0.02 \\  & LoRA(kaiming) & 74.53\(\)0.47 & 29.90\(\)0.16 & 53.57\(\)0.27 & 65.25\(\)0.29 & 4.97\(\)0.09 \\  & PiSSA & **77.78\(\)0.32** & **31.33\(\)0.33** & **54.31\(\)0.28** & **66.17\(\)0.43** & **5.64\(\)0.10** \\   

Table 2: Comparison of PiSSA and LoRA on NLG tasks, with results averaged over three runs and reported with standard deviations.

Figure 4: The loss, grad norm, and evaluation accuracy over the training steps of LoRA (indicated in blue), PiSSA (in orange), and full parameter fine-tuning (in red).

  
**Method** & **Params** & **MNLI** & **SST2** & **MRPC** & **CoLA** & **QNLI** & **QQP** & **RTE** & **STSB** & **ALL** \\  Full FT & 184M & 89.90 & 95.63 & 89.46 & 69.19 & 94.03 & **92.40** & 83.75 & 91.60 & 88.25 \\ BitFit & 0.1M & 89.37 & 94.84 & 87.75 & 66.96 & 92.24 & 88.41 & 78.70 & 91.35 & 86.20 \\ HAdapter & 1.22M & 90.13 & 95.53 & 89.95 & 68.64 & 94.11 & 91.91 & 84.48 & 91.48 & 88.28 \\ PAdapter & 1.18M & 90.33 & 95.61 & 89.46 & 68.77 & 94.29 & 92.04 & 85.20 & 91.54 & 88.41 \\ LoRA\({}^{G}\) & 1.33M & 90.65 & 94.95 & 89.95 & 69.82 & 93.87 & 91.99 & 85.20 & 91.60 & 88.50 \\ LoRA\({}^{K}\) & 1.33M & 89.96 & 95.64 & 90.28 & 70.69 & 93.84 & 92.03 & 84.84 & 91.68 & 88.62 \\ DoRA & 1.27M & 90.29 & 95.79 & 90.93 & 70.85 & 94.10 & 92.07 & 86.04 & 91.79 & 88.98 \\ AdaLoRA & 1.27M & **90.76** & 96.10 & 90.69 & 71.45 & **94.55** & 92.23 & 88.09 & 91.84 & 89.46 \\ PiSSA & 1.33M & 90.37 & **96.22** & **91.50** & **73.12** & 94.43 & 92.33 & **88.69** & **92.00** & **89.83** \\   

Table 3: Comparison of PiSSA and LoRA on NLU tasks. LoRA\({}^{G}\) and LoRA\({}^{K}\) denote LoRA with Gaussian and Kaiming initialization for \(B\), respectively. Results for full fine-tuning, BitFit , HAdapter , PAdapter , LoRA\({}^{G}\) and AdaLoRA are from AdaLoRA , averaged over five runs. Remaining methods are averaged over three runs, with details in Appendix L.

that PiSSA converges to a better local optimum. As shown in Figure 3(c), PiSSA consistently achieves higher accuracy compared to LoRA, and in most cases also surpasses full parameters fine-tuning. We hypothesize that this is because PiSSA is a denoised version of full fine-tuning. Comparing the grad norm and loss curves of PiSSA and full fine-tuning, we can see that the larger grad norm of full fine-tuning does not bring lower loss, indicating that a portion of the grad norm is spent on noisy directions not beneficial for loss reduction. This phenomenon is consistent with Figure 1(a).

### Conducting 4-bit Quantization Experiments

In this section, we first compare the initial quantization error reduction ratio of PiSSA, QLoRA, and LoftQ. This ratio is defined as \((1-)+AB)||_{*}}{||W-nI4(W^{})||_{*}}) 100\%\), measuring the relative error decrease achieved by each mehod compared to directly quantizing the base model. The partial results are presented in Table 4, and the complete results can be found in Table 8 in Appendix E.

In Table 4, PiSSA reduces the quantization error by about 20% compared to directly quantizing the base model. The reduction is more significant for lower-rank matrices. For instance, in the LLaMA-3-70B , all "Key" projection layers see a reduction of 49%. The results in Table 4 validate that QLoRA, discussed in Section 4, does not reduce quantization error. In contrast, PiSSA significantly outperforms LoftQ in reducing quantization error, as further discussed in Appendix H.

    & **Method** & **Rank** & **Q** & **K** & **V** & **O** & **Gate** & **Up** & **Down** & **AVG** \\   & QLoRA & – & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  & loftQ & 128 & 16.5 & 16.5 & 15.9 & 16.0 & 12.4 & 12.4 & 12.3 & 14.6 \\  & **PiSSA** & **128** & **27.9** & **27.2** & **18.7** & **18.6** & **15.8** & **13.6** & **13.6** & **19.4** \\   & QLoRA & – & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  & loftQ & 128 & 16.4 & 29.8 & 28.8 & 16.1 & 11.9 & 11.7 & 11.7 & 18.1 \\  & **PiSSA** & **128** & **26.3** & **41.7** & **32.3** & **20.1** & **14.4** & **12.5** & **12.9** & **22.9** \\   & QLoRA & – & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  & LoftQ & 64 & 6.1 & 17.8 & 17.0 & 6.0 & 4.3 & 4.4 & 4.2 & 8.5 \\   & **PiSSA** & **64** & **15.7** & **34.2** & **18.9** & **7.5** & **6.7** & **5.7** & **4.7** & **13.4** \\   & **PiSSA** & **128** & **23.2** & **49.0** & **30.5** & **12.5** & **10.1** & **8.8** & **8.2** & **20.3** \\   

Table 4: The quantization error reduction ratio of QLoRA, LoftQ, and PiSSA across different layers.

Figure 5: The loss, grad norm, and evaluation accuracy over the training steps of (Q)LoRA, (Q)PiSSA, LoftQ and full parameter fine-tuning.

The difference between QPiSSA and PiSSA is the quantization of the residual model to 4 bits. As introduced in Section 4, the residual model has less influence on the optimal direction compared with the principal adapter, which is the same in both QPiSSA and PiSSA. Therefore, besides reducing the quantization error, we expect QPiSSA to also converge faster than QLoRA and LoftQ. We train LLaMA 3-8B using LoRA/QLoRA, PiSSA/QPiSSA, LoftQ, and full fine-tuning on MetaMathQA-395K for 3 epochs, recording the loss, gradient norm, and accuracy on GSM8K, as shown in Figure 5.

According to Figure 5, QPiSSA's loss reduction speed in the first 100 steps is even faster than PiSSA and full fine-tuning. Although LoftQ can reduce the quantization error, its loss convergence speed is not faster than LoRA and QLLoRA, indicating that QPiSSA's ability to reduce the quantization error and its fast convergence might also be orthogonal capabilities. After sufficient training, QPiSSA's loss is also much lower than that of LoRA/QLoRA and LoftQ. The grad norm is significantly larger than those of LoRA/QLoRA and LoftQ. In terms of fine-tuning performance, QPiSSA's accuracy is higher than that of QLoRA and LoftQ and even better than that of full-precision LoRA.

### Experiments Across Various Sizes and Types of Models

In this section, we compare (Q)PiSSA and (Q)LoRA across 9 models, ranging from 7-70B parameters, including LLaMA 2-7/13B , LLaMA-3-8/70B , Mistral-7B , Gemma-7B , and Qwen1.5-7B , Yi-1.5-34B  and MoE models: DeepSee-MoE-16B  and Mistral-8x7B . These models were fine-tuned on the MetaMathQA-100K and CodeFeedback-100K dataset and evaluated on the GSM8K and HumanEval. DeepSee-MoE-16B, Mistral-8x7B, Yi-1.5-34B, and LLaMA-3-70B were fine-tuned with QPiSSA and QLoRA, while the other models were using PiSSA and LoRA. From Figure 6, (Q)PiSSA, compared to (Q)LoRA, shows improved accuracy across various sizes and types of models, demonstrating its consistent advantage over (Q)LoRA.

### Experiments on Various Ranks

This section explores the impact of incrementally increasing the rank of PiSSA/QPiSSA and LoRA/QLoRA from 1 to 128, aiming to determine whether PiSSA/QPiSSA consistently outperforms LoRA/QLoRA under different ranks. The training is conducted using the MetaMathQA-100K dataset for 1 epoch, while the validation is performed on the GSM8K and MATH datasets. The outcomes of these experiments are depicted in Figure 7, with additional results presented in Appendix K.

Figure 6(a) illustrates the quantization error reduction ratio across various ranks. In this figure, QLoRA shows no reduction in quantization error, while QPiSSA consistently outperforms LoftQ in reducing quantization error across all ranks, with a particularly notable advantage at lower ranks. In Figure 6(b), the final loss on the training set is shown for models trained with ranks ranging from 1 to 128. The results indicate that PiSSA and QPiSSA achieve a better fit to the training data compared to LoRA, QLoRA, and LoftQ. In Figures 6(c) and Figures 6(d), we compare the accuracy of the fine-tuned models on the GSM8K and MATH validation sets under various ranks, finding that PiSSA consistently outperforms LoRA with the same amount of trainable parameters. Furthermore, as the rank increases, PiSSA will reach and surpass the performance of full-parameter fine-tuning.

Figure 6: Comparison of (Q)QiSSA and (Q)LoRA across models from 7B to 70B.

## 6 Conclusion

This paper presents a PEFT technique that applies singular value decomposition (SVD) to the weight matrix of pre-trained models. The principal components obtained from the SVD are used to initialize a low-rank adapter named PiSSA, while the residual components are kept frozen, to achieve effective fine-tuning and parameter efficiency simultaneously. Through extensive experiments, we found that PiSSA and its 4-bit quantization version QPiSSA significantly outperform LoRA and QLoRA in both NLG and NLU tasks, across different training steps, various model sizes and types, and under various amount of trainable parameters. PiSSA provides a novel direction for research in PEFT by identifying and fine-tuning the principal components within the model, analogous to _slicing and re-baking the richest slice of a pizza_. As PiSSA shares the same architecture as LoRA, it can be seamlessly used in existing LoRA pipelines as an efficient alternative initialization method.

## 7 Limitation

There are still some questions with PiSSA not addressed in this paper: 1) Besides language models, can PiSSA also be adapted to convolutional layers and enhance the performance of vision tasks? 2) Can PiSSA also benefit from some improvements to LoRA, such as AdaLoRA  and DyLoRA  which adaptively adjust the rank? 3) Can we provide more theoretical explanations for the advantages of PiSSA over LoRA? We are actively exploring these questions. Nevertheless, we are excited to see the huge potential of PiSSA already demonstrated in existing experiments and look forward to more tests and suggestions from the community.

## 8 Acknowledgements:

This work is supported by the National Key R&D Program of China (2022ZD016030).

Figure 7: The comparison among (Q)LoRA, (Q)PiSSA, LoftQ, and full fine-tuning across ranks.