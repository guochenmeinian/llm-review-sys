# Learning threshold neurons via the "edge of stability"

Kwangjun Ahn

MIT EECS

Cambridge, MA

kjahn@mit.edu

&Sebastien Bubeck

Microsoft Research

Redmond, WA

sebbeck@microsoft.com

&Sinho Chewi

Institute for Advanced Study

Princeton, NJ

schewi@ias.edu Yin Tat Lee

Microsoft Research

Redmond, WA

yintat@uw.edu

&Felipe Suarez

Carnegie Mellon University

Pittsburgh, PA

felipesc@mit.edu

&Yi Zhang

Microsoft Research

Redmond, WA

zhayi@microsoft.com

###### Abstract

Existing analyses of neural network training often operate under the unrealistic assumption of an extremely small learning rate. This lies in stark contrast to practical wisdom and empirical studies, such as the work of J. Cohen et al. (ICLR, 2021), which exhibit startling new phenomena (the "edge of stability" or "unstable convergence") and potential benefits for generalization in the large learning rate regime. Despite a flurry of recent works on this topic, however, the latter effect is still poorly understood. In this paper, we take a step towards understanding genuinely non-convex training dynamics with large learning rates by performing a detailed analysis of gradient descent for simplified models of two-layer neural networks. For these models, we provably establish the edge of stability phenomenon and discover a sharp phase transition for the step size below which the neural network fails to learn "threshold-like" neurons (i.e., neurons with a non-zero first-layer bias). This elucidates one possible mechanism by which the edge of stability can in fact lead to better generalization, as threshold neurons are basic building blocks with useful inductive bias for many tasks.

## 1 Introduction

How much do we understand about the training dynamics of neural networks? We begin with a simple and canonical learning task which indicates that the answer is still "far too little".

Figure 1: _Large step sizes are necessary to learn the “threshold neuron” of a ReLU network_ (2) _for a simple binary classification task_ (1). We choose \(d=200\), \(n=300\), \(=3\), and run gradient descent with the logistic loss. The weights are initialized as \(a^{-},a^{+}(0,1/(2d))\) and \(b=0\). For each learning rate \(\), we set the iteration number such that the total time elapsed (iteration \(\)\(\)) is \(10\). The vertical dashed lines indicate our theoretical prediction of the phase transition phenomenon (precise threshold at \(=8/d^{2}\)).

_Motivating example:_ Consider a binary classification task of labeled pairs \((^{(i)},y^{(i)})^{d}\{ 1\}\) where each covariate \(^{(i)}\) consists of a \(1\)-sparse vector (in an unknown basis) corrupted by additive Gaussian noise, and the label \(y^{(i)}\) is the sign of the non-zero coordinate of the \(1\)-sparse vector. Due to rotational symmetry, we can take the unknown basis to be the standard one and write

\[^{(i)}= y^{(i)}_{j(i)}+^{(i)}^{d}\,, \]

where \(y^{(i)}\{ 1\}\) is a random label, \(j(i)[d]\) is a random index, \(^{(i)}\) is Gaussian noise, and \(>1\) is the unknown signal strength. In fact, (1) is a special case of the well-studied sparse coding model (Olshausen and Field, 1997; Vinje and Gallant, 2000; Olshausen and Field, 2004; Yang et al., 2009; Koehler and Risteski, 2018; Allen-Zhu and Li, 2022). We ask the following fundamental question:

_How do neural networks learn to solve the sparse coding problem (1)?_

In spite of the simplicity of the setting, a full resolution to this question requires a thorough understanding of surprisingly rich dynamics which _lies out of reach of existing theory_. To illustrate this point, consider an extreme simplification in which the basis \(_{1},,_{d}\) is known in advance, for which it is natural to parametrize a two-layer ReLU network as

\[f(;a^{-},a^{+},b)=a^{-}_{i=1}^{d}-[i]+b +a^{+}_{i=1}^{d}+[i]+b\,. \]

The parametrization (2) respects the latent data structure (1) well: a good network has a negative bias \(b\) to threshold out the noise, and has \(a^{-}<0\) and \(a^{+}>0\) to output correct labels. We are particularly interested in understanding the mechanism by which the bias \(b\) becomes negative, thereby allowing the non-linear ReLU activation to act as a threshold function; we refer to this as the problem of learning "threshold neurons". More broadly, such threshold neurons are of interest as they constitute basic building blocks for producing neural networks with useful inductive bias.

We train the parameters \(a^{-}\), \(a^{+}\), \(b\) using gradient descent with step size \(>0\) on the logistic loss \(_{i=1}^{n}_{}(y^{(i)}\ f(^{(i)};a^{-},a^{+},b))\), where \(_{}(z)(1+(-z))\), and we report the results in Figures 1 and 2. The experiments reveal a compelling picture of the optimization dynamics.

* _Large learning rates are necessary, both for generalization and for learning threshold neurons._ Figure 1 shows that the bias decreases and the test accuracy increases as we increase \(\); note that we plot the results after a fixed _time_ (iteration \(\)), so the observed results are not simply because larger learning rates track the continuous-time gradient flow for a longer time.
* _Large learning rates lead to unexpected phenomena: non-monotonic loss and wild oscillations of \(a^{-}+a^{+}\)._ Figure 2 shows that large learning rates also induce stark phenomena, such as non-monotonic loss and large weight fluctuations, which lie firmly outside the explanatory power of existing analytic techniques based on principles from convex optimization.
* _There is a phase transition between small and large learning rates._ In Figure 1, we zoom in on learning rates around \( 0.0006\) and observe _sharp_ phase transition phenomena.

Figure 2: _Large learning rates lead to unexpected phenomena: non-monotonic loss and wild oscillations of weights._ We choose the same setting as Figure 1. With a small learning rate (\(=2.5 10^{-5}\)), the bias does not decrease noticeably, and the same is true even when we increase the learning rate by ten times (\(=2.5 10^{-4}\)). When we increase the learning rate by another ten times (\(=2.5 10^{-3}\)), we finally see a noticeable decrease in the bias, but with this we observe unexpected behavior: the loss decreases non-monotonically and the sum of second-layer weights \(d(a^{-}+a^{+})\) oscillates wildly._

We have presented these observations in the context of the simple ReLU network (2), but we emphasize that _these findings are indicative of behaviors observed in practical neural network training settings_. In Figure 3, we display results for a two-layer ReLU network trained on the full sparse coding model (1) with unknown basis, as well as a deep neural network trained on CIFAR-10. In each case, we again observe non-monotonic loss coupled with steadily decreasing bias parameters. For these richer models, the transition from small to large learning rates is oddly reminiscent of well-known separations between the "lazy training" or "NTK" regime Jacot et al. (2018) and the more expressive "feature learning" regime. For further experimental results, see Appendix B.

We currently do not have right tools to understand these phenomena. First of all, a drastic change in behavior between the small and the large learning rates cannot be captured through well-studied regimes, such as the "neural tangent kernel" (NTK) regime (Jacot et al., 2018; Allen-Zhu et al., 2019; Arora et al., 2019; Chizat et al., 2019; Du et al., 2019; Oymak and Soltanolkotabi, 2020) or the mean-field regime Chizat and Bach (2018); Mei et al. (2019); Chizat (2022); Nitanda et al. (2022); Rotskoff and Vanden-Eijnden (2022). In addition, understanding why a large learning rate is required to learn the bias is beyond the scope of prior theoretical works on the sparse coding model (Arora et al., 2015; Karp et al., 2021). Our inability to explain these findings points to a serious gap in our grasp of neural network training dynamics and calls for a detailed theoretical study.

### Main scope of this work

In this work, we do not aim to understand the sparse coding problem (1) in its full generality. Instead, we pursue the more modest goal of shedding light on the following question.

**Q.** What is the role of a large step size in learning the bias for the ReLU network (2)?

As discussed above, the dynamics of the simple ReLU network (2) is a microcosm of emergent phenomena beyond the convex optimization regime. In fact, there is a recent growing body of work (Cohen et al., 2021; Arora et al., 2022; Ahn et al., 2022; Lyu et al., 2022; Ma et al., 2022; Wang et al., 2022b; Chen and Bruna, 2023; Damian et al., 2023; Zhu et al., 2023) on training with large learning rates, which largely aims at explaining a striking empirical observation called the "_edge of stability (EoS)_" phenomenon.

Figure 3: (Top) Results for training an over-parametrized two-layer neural network \(f(;,,b)=_{i=1}^{m}a_{i}(_{i}^{ }+b)\) with \(m d\) for the _full sparse coding model_ (1); in this setting, the basis vectors are unknown, and the neural network learn them through additional parameters \(=(_{i})_{i=1}^{m}\). Also, we use \(m\) different weights \(=(a_{i})_{i=1}^{m}\) for the second layer. (Bottom) Full-batch gradient descent dynamics of _ResNet-18 on (binary) CIFAR-10 with various learning rates_. Details are deferred to Appendix B.

The edge of stability (EoS) phenomenon is a set of distinctive behaviors observed recently by Cohen et al. (2021) when training neural networks with gradient descent (GD). Here we briefly summarize the salient features of the EoS and defer a discussion of prior work to Subsection 1.3. Recall that if we use GD to optimize an \(L\)-smooth loss function with step size \(\), then the well-known descent lemma from convex optimization ensures monotonic decrease in the loss so long as \(L<2/\). In contrast, when \(L>2/\), it is easy to see on simple convex quadratic examples that GD can be unstable (or divergent). The main observation of Cohen et al. (2021) is that when training neural networks1 with constant step size \(>0\), the largest eigenvalue of the Hessian at the current iterate (dubbed the "sharpness") initially increases during training ("progressive sharpening") and saturates near or above \(2/\) ("EoS").

A surprising message of the present work is that _the answer to our main question is intimately related to the EoS_. Indeed, Figure 4 shows that the GD iterates of our motivating example exhibit the EoS during the initial phase of training when the bias decreases rapidly.

Consequently, we first set out to thoroughly understand the workings of the EoS phenomena through a simple example. Specifically, we consider a single-neuron linear neural network in dimension \(1\), corresponding to the loss

\[^{2}(x,y)(xy)\,,\,. \]

Although toy models have appeared in works on the EoS (see Subsection 1.3), our example is simpler than all prior models, and we provably establish the EoS for (3) with transparent proofs.

We then use the newfound insights gleaned from the analysis of (3) to answer our main question. To the best of our knowledge, we provide the first explanation of the mechanism by which a large learning rate can be _necessary_ for learning threshold neurons.

### Our contributions

Explaining the EoS with a single-neuron example.Although the EoS has been studied in various settings (see Subsection 1.3 for a discussion), these works either do not rigorously establish the EoS phenomenon, or they operate under complex settings with opaque assumptions. Here, we study a simple two-dimensional loss function, \((x,y)(xy)\), where \(\) is convex, even, and Lipschitz. Some examples include2\((s)=(1+(-s))+(1+(+s))\) and \((s)=}\). Surprisingly, GD on this loss already exhibits rich behavior (Figure 5).

En route to this result, we rigorously establish the quasi-static dynamics formulated in Ma et al. (2022).

Figure 4: **Understanding our main question is surprisingly related to the EoS. Under the same setting as Figure 1, we report the largest eigenvalue of the Hessian (“sharpness”), and observe that GD iterates lie in the EoS during the initial phase of training when there is a fast drop in the bias.**

Figure 5: Illustration of two different regimes (the “gradient flow” regime and the “EoS” regime) of the GD dynamics.

The elementary nature of our example leads to transparent arguments, and consequently our analysis isolates generalizable principles for "bouncing" dynamics. To demonstrate this, we use our insights to study our main question of learning threshold neurons.

_Learning threshold neurons with the mean model._ The connection between the single-neuron example and the ReLU network (2) can already be anticipated via a comparison of the dynamics: _(i)_ for the single neuron example, \(x\) oscillates wildly while \(y\) decreases (Figure 5); _(ii)_ for the ReLU network (2), the sum of weights \((a^{-}+a^{+})\) oscillates while \(b\) decreases (Figure 2).

We study this example in Section 2 and delineate a transition from the "gradient flow" regime to the "EoS regime", depending on the step size \(\) and the initialization. Moreover, in the EoS regime, we rigorously establish asymptotics for the limiting sharpness which depend on the higher-order behavior of \(\). In particular, for the two losses mentioned above, the limiting sharpness is \(2/+O()\), whereas for losses \(\) which are exactly quadratic near the origin the limiting sharpness is \(2/+O(1)\).

In fact, this connection can be made formal by considering an approximation for the GD dynamics for the ReLU network (2). It turns out (see Subsection 3.1 for details) that during the initial phase of training, the dynamics of \(A_{t} d(a_{t}^{-}+a_{t}^{+})\) and \(b_{t}\) due to the ReLU network are well-approximated by the "rescaled" GD dynamics on the loss \((A,b)_{}(A g(b))\), where the step size for the \(A\)-dynamics is multiplied by \(2d^{2}\), \(g(b)_{z(0,1)}(z+b)\) is the "smoothed" ReLU, and \(_{}\) is the symmetrized logistic loss; see Subsection 3.1 and Figure 8. We refer to these dynamics as the _mean model_. The mean model bears a great resemblance to the single-neuron example \((x,y)(xy)\), and hence we can leverage the techniques developed for the latter in order to study the former.

Our main result for the mean model precisely explains the phase transition in Figure 1. For any \(>0\),

* if \((8-)/d^{2}\), then _the mean model fails to learn threshold neurons_: the limiting bias satisfies \(|b_{}|=O_{}(1/d^{2})\).
* if \((8+)/d^{2}\), then _the mean model enters the EoS and learns threshold neurons_: the limiting bias satisfies \(b_{}-_{}(1)\).

### Related work

_Edge of stability._ Our work is motivated by the extensive empirical study of Cohen et al. (2021), which identified the EoS phenomenon. Subsequently, there has been a flurry of works aiming at developing a theoretical understanding of the EoS, which we briefly summarize here.

_Properties of the loss landscape._ The works (Ahn et al., 2022; Ma et al., 2022) study the properties of the loss landscape that lead to the EoS. Namely, Ahn et al. (2022) argue that the existence of forward-invariant subsets near the minimizers allows GD to convergence even in the unstable regime. They also explore various characteristics of EoS in terms of loss and iterates. Also, Ma et al. (2022) empirically show that the loss landscape of neural networks exhibits subquadratic growth locally around the minimizers. They prove that for a one-dimensional loss, subquadratic growth implies that GD finds a \(2\)-periodic trajectory.

_Limiting dynamics._ Other works characterize the limiting dynamics of the EoS in various regimes. (Arora et al., 2022; Lyu et al., 2022) show that (normalized) GD tracks a "sharpness reduction flow" near the manifold of minimizers. The recent work of Damian et al. (2023) obtains a different predicted dynamics based on self-stabilization of the GD trajectory. Also, Ma et al. (2022) describes a quasi-static heuristic for the overall trajectory of GD when one component of the iterate is oscillating.

_Simple models and beyond._ Closely related to our own approach, there are prior works which carefully study simple models. Chen and Bruna (2023) prove global convergence of GD for the two-dimensional function \((x,y)(xy-1)^{2}\) and a single-neuron student-teacher setting; note that unlike our results, they do not study the limiting sharpness. Wang et al. (2022) study progressive sharpening for a neural network model. Also, the recent and concurrent work of Zhu et al. (2023) studies the two-dimensional loss \((x,y)(x^{2}y^{2}-1)^{2}\); to our knowledge, their work is the first to asymptotically and rigorously show that the limiting sharpness of GD is \(2/\) in a simple setting,

Figure 6: Illustration of GD dynamics on the ReLU network (2). The sum of weights \((a^{-}+a^{+})\) oscillates while \(b\) decreases.

at least when initialized locally. In comparison, in Section 2, we perform a global analysis of the limiting sharpness of GD for \((x,y)(xy)\) for a class of convex, even, and Lipschitz losses \(\), and in doing so we clearly delineate the "gradient flow regime" from the "EoS regime".

_Effect of learning rate on learning._ Recently, several works have sought to understand how the choice of learning rate affects the learning process, in terms of the properties of the resulting minima (Jastrzebski et al., 2018; Wu et al., 2018; Mulayoff et al., 2021; Nacson et al., 2022) and the behavior of optimization dynamics (Xing et al., 2018; Jastrzebski et al., 2019, 2020; Lewkowycz et al., 2020; Jastrzebski et al., 2021).

Li et al. (2019) demonstrate for a synthethtic data distribution and a two-layer ReLU network model that choosing a larger step size for SGD helps with generalization. Subsequent works have shown similar phenomena for regression (Nakkiran, 2020; Wu et al., 2021; Ba et al., 2022), kernel ridge regression Beugnot et al. (2022), and linear diagonal networks Nacson et al. (2022). However, the large step sizes considered in these work still fall under the scope of descent lemma, and most prior works do not theoretically investigate the effect of large step size in the EoS regime. A notable exception is the work of Wang et al. (2022), which studies the impact of learning rates greater than \(2/\)smoothness for a matrix factorization problem. Also, the recent work of Andriushchenko et al. (2023) seeks to explain the generalization benefit of SGD in the large step size regime by relying on a heuristic SDE model for the case of linear diagonal networks. Despite this similarity, their main scope is quite different from ours, as we _(i)_ focus on GD instead of SGD and _(ii)_ establish a direct and detailed analysis of the GD dynamics for a model of the motivating sparse coding example.

## 2 Single-neuron linear network

In this section, we analyze the single-neuron linear network model \((x,y) f(x,y)(x y)\).

### Basic properties and assumptions

_Basic properties._ If \(\) is minimized at \(0\), then the _global minimizers_ of \(f\) are the \(x\)- and \(y\)-axes. The GD _iterates_\(x_{t},y_{t}\), for step size \(>0\) and iteration \(t 0\) can be written as

\[x_{t+1}=x_{t}-\,^{}(x_{t}y_{t})\,y_{t}\,, y_{t+1}=y_{t}- \,^{}(x_{t}y_{t})\,x_{t}\,.\]

_Assumptions._ From here onward, we assume \(<1\) and the following conditions on \(:\).

1. \(\) is convex, even, \(1\)-Lipschitz, and of class \(^{2}\) near the origin with \(^{}(0)=1\).
2. There exist constants \(>1\) and \(c>0\) with the following property: for all \(s 0\), \[^{}(s)/s 1-c\,|s|^{}\,}\{|s|  c\}\,.\] We allow \(=+\), in which case we simply require that \((s)}{s} 1\) for all \(s 0\).

Assumption (A2) imposes decay of \(s^{}(s)/s\) locally away from the origin in order to obtain more fine-grained results on the limiting sharpness in Theorem 2. As we show in Lemma 5 below, when \(\) is smooth and has a strictly negative fourth derivative at the origin, then Assumption (A2) holds with \(=2\). See Example 1 for some simple examples of losses satisfying our assumptions.

### Two different regimes for GD depending on the step size

Before stating rigorous results, in this section we begin by giving an intuitive understanding of the GD dynamics. It turns out that for a given initialization \((x_{0},y_{0})\), there are two different regimes for the GD dynamics depending on the step size \(\). Namely, there exists a threshold on the step size such that _(i)_ below the threshold, GD remains close to the gradient flow for all time, and _(ii)_ above the threshold, GD enters the edge of stability and diverges away from the gradient flow. See Figure 9.

First, recall that the GD dynamics are symmetric in \(x,y\) and that the lines \(y= x\) are invariant. Hence, we may assume without loss of generality that

\[y_{0}>x_{0}>0\,, y_{t}>|x_{t}|\,t 1\,,(0,y_{})y_{}>0\,.\]

From the expression (8) for the Hessian of \(f\) and our normalization \(^{}(0)=1\), it follows that the sharpness (the largest eigenvalue of loss Hessian) reached by GD in this example is precisely \(y_{}^{2}\).

Initially, in both regimes, the GD dynamics tracks the continuous-time gradient flow. Our first observation is that the gradient flow admits a conserved quantity, thereby allowing us to predict the dynamics in this initial phase.

**Lemma 1** (conserved quantity).: _Along the gradient flow for \(f\), the quantity \(y^{2}-x^{2}\) is conserved._

Proof.: Differentiating \(y_{t}^{2}-x_{t}^{2}\) with respect to \(t\) gives \(2y_{t}(-^{}(x_{t}y_{t})\,x_{t})-2x_{t}(-(x_{t}y _{t})\,y_{t})=0\). 

Lemma 1 implies that the gradient flow converges to \((0,y_{}^{})=(0,^{2}-x_{0}^{2}})\). For GD with step size \(>0\), the quantity \(y^{2}-x^{2}\) is no longer conserved, but we show in Lemma 6 that it is _approximately_ conserved until the GD iterate lies close to the \(y\)-axis. Hence, GD initialized at \((x_{0},y_{0})\) also reaches the \(y\)-axis approximately at the point \((x_{t_{0}},y_{t_{0}})(0,^{2}-x_{0}^{2}})\).

At this point, GD either approximately converges to the gradient flow solution \((0,^{2}-x_{0}^{2}})\) or diverges away from it, depending on whether or not \(y_{t_{0}}^{2}>2/\). To see this, for \(|x_{t_{0}}y_{t_{0}}| 1\), we can Taylor expand \(^{}\) near zero to obtain the approximate dynamics for \(x\) (recalling \(^{}(0)=1\)),

\[x_{t_{0}+1} x_{t_{0}}- x_{t_{0}}y_{t_{0}}^{2}=(1- y_{t_{0}}^{2} )\,x_{t_{0}}\,. \]

From (4), we deduce the following conclusions.

1. If \(y_{t_{0}}^{2}<2/\), then \(|1- y_{t_{0}}^{2}|<1\). Since \(y_{t}\) is decreasing, it implies that \(|1- y_{t}^{2}|<1\) for all \(t t_{0}\), and so \(|x_{t}|\) converges to zero exponentially fast.
2. On the other hand, if \(y_{t_{0}}^{2}>2/\), then \(|1- y_{t_{0}}^{2}|>1\), i.e., the magnitude of \(x_{t_{0}}\) increases in the next iteration, and hence GD cannot stabilize. In fact, in the approximate dynamics, \(x_{t_{0}+1}\) has the opposite sign as \(x_{t_{0}}\), i.e., \(x_{t_{0}}\) jumps across the \(y\)-axis. One can show that the "bouncing" of the \(x\) variable continues until \(y_{t}^{2}\) has decreased past \(2/\), at which point we are in the previous case and GD approximately converges to \((0,2/)\).

This reasoning, combined with the expression for the Hessian of \(f\), shows that

\[(0,y_{}):=_{}^{2 }f(0,y_{}) y_{0}^{2}-x_{0}^{2},\,2/}\] \[=\{\}\,.\]

Accordingly, we refer to the case \(y_{0}^{2}-x_{0}^{2}<2/\) as the _gradient flow regime_, and the case \(y_{0}^{2}-x_{0}^{2}>2/\) as the _EoS regime_.

See Figure 5 and Figure 9 for illustrations of these two regimes; see also Figure 10 for detailed illustrations of the EoS regime. In the subsequent sections, we aim to make the above reasoning rigorous. For example, instead of the approximate dynamics (4), we consider the original GD dynamics and justify the Taylor approximation. Also, in the EoS regime, rather than loosely asserting that \(|x_{t}| 0\) exponentially fast and hence the dynamics stabilizes "quickly" once \(y_{t}^{2}<2/\), we track precisely how long this convergence takes so that we can bound the gap between the limiting sharpness and the prediction \(2/\).

### Results

_Gradient flow regime._ Our first rigorous result is that when \(y_{0}^{2}-x_{0}^{2}=}{{}}\) for some constant \((0,2)\), then the limiting sharpness of GD with step size \(\) is \(y_{0}^{2}-x_{0}^{2}+O(1)=}{{}}+O(1)\), which is precisely the sharpness attained by the gradient flow up to a controlled error term.

In fact, our theorem is slightly more general, as it covers initializations in which \(\) can scale mildly with \(\). The precise statement is as follows.

**Theorem 1** (gradient flow regime; see Subsection C.2).: _Suppose we run GD with step size \(>0\) on the objective \(f\), where \(f(x,y):=(xy)\), and \(\) satisfies Assumptions (A1) and (A2). Let \((,)^{2}\) satisfy \(>>0\) with \(^{2}-^{2}=1\). Suppose we initialize GD at \((x_{0},y_{0}):=()^{1/2}\,(,)\), where \((0,2)\) and \(^{1/2}(2-)\). Then, GD converges to \((0,y_{})\) satisfying_

\[-O(2-)-O} _{}^{2}f(0,y_{})+O\,,\]

_where the implied constants depend on \(\), \(\), and \(\), but not on \(\), \(\)._The proof of Theorem 1 is based on a two-stage analysis. In the first stage, we use Lemma 6 on the approximate conservation of \(y^{2}-x^{2}\) along GD in order to show that GD lands near the \(y\)-axis with \(y_{t_{0}}^{2}}{{}}\). In the second stage, we use the assumptions on \(\) in order to control the rate of convergence of \(|x_{t}|\) to 0, which is subsequently used to control the final deviation of \(y_{}^{2}\) from \(}{{}}\).

_EoS regime._ Our next result states that when \(y_{0}^{2}-x_{0}^{2}>2/\), then the limiting sharpness of GD is close to the EoS prediction of \(2/\), up to an error term which depends on the exponent \(\) in (A2).

**Theorem 2** (EoS; see Subsection C.4).: _Suppose we run GD on \(f\) with step size \(>0\), where \(f(x,y)(xy)\), and \(\) satisfies (A1) and (A2). Let \((,)^{2}\) satisfy \(>>0\) with \(^{2}-^{2}=1\). Suppose we initialize GD at \((x_{0},y_{0})}}{{}}(, )\), where \(>0\) is a constant. Also, assume that for all \(t 1\) such that \(y_{t}^{2}>2/\), we have \(x_{t} 0\). Then, GD converges to \((0,y_{})\) satisfying_

\[2/-O(^{1/(-1)})_{}^{2}f(0,y_{})  2/\,,\]

_where the implied constants depend on \(\), \(\), \( 1\), and \(\), but not on \(\)._

_Remarks on the assumptions._ The initialization in our results is such that both \(y_{0}\) and \(y_{0}-x_{0}\) are on the same scale, i.e., \(y_{0},y_{0}-x_{0}=(1/)\). This rules out extreme initializations such as \(y_{0} x_{0}\), which are problematic because they lie too close to the invariant line \(y=x\). Since our aim in this work is not to explore every edge case, we focus on this setting for simplicity. Moreover, we imposed the assumption that the iterates of GD do not exactly hit the \(y\)-axis before crossing \(y^{2}=2/\). This is necessary because if \(x_{t}=0\) for some iteration \(t\), then \((x_{t^{}},y_{t^{}})=(x_{t},y_{t})\) for all \(t^{}>t\), and hence the limiting sharpness may not be close to \(2/\). This assumption holds generically, e.g., if we perturb each iterate of GD with a vanishing amount of noise from a continuous distribution, and we conjecture that for any \(>0\), the assumption holds for all but a measure zero set of initializations.

When \(=+\), which is the case for the Huber loss in Example 1, the limiting sharpness is \(2/+O(1)\). When \(=2\), which is the case for the logistic and square root losses in Example 1, the limiting sharpness is \(2/+O()\). Numerical experiments show that _our error bound of \(O(^{1/(-1)})\) is sharp_; see Figure 11 below.

We make a few remark about the proof. As we outline the proof in Subsection C.3, in turns out in order to bound the gap \(2/-y_{}^{2}\), the proof requires a control of the size \(|x_{}y_{}|\), where \(\) is the first iteration such that \(y_{}^{2}\) crosses \(2/\). However, controlling the size of \(|x_{}y_{}|\) is surprisingly delicate as it requires a fine-grained understanding of the bouncing phase. The insight that guides the proof is the observation that during the bouncing phase, the GD iterates lie close to a certain envelope (Figure 9).

As a by-product of our analysis, we obtain a rigorous version of the quasi-static principle from which can more accurately track the sharpness gap and convergence rate (see Subsection C.5). The results of Theorem 1, Theorem 2, and Theorem 5 are displayed pictorially as Figure 9.

## 3 Understanding the bias evolution of the ReLU network

In this section, we use the insights from Section 2 to answer our main question, namely understanding the role of a large step size in learning threshold neurons for the ReLU network (2). Based on the observed dynamics (Figure 2), we can make our question more concrete as follows.

**Q.** What is the role of a large step size during the "_initial phase_" of training in which _(i)_ the bias \(b\) rapidly decreases and _(ii)_ the sum of weights \(a^{-}+a^{+}\) oscillates?

### Approximating the initial phase of GD with the "mean model"

Deferring details to Appendix D, the GD dynamics for the ReLU network (2) in the initial phase are well-approximated by

\[(a^{-},a^{+},b)_{}(d(a^{-}+a^{+} )g(b))\,,\]

where \(_{}(s)((1+(-s))+(1+(+s)))\) and \(g(b)_{z(0,1)}(z+b)\) is the'smoothed' ReLU. The GD dynamics can be compactly written in terms of the parameter \(A_{t} d(a_{t}^{-}+a_{t}^{+})\).

\[A_{t+1}=A_{t}-2d^{2}\,_{}^{}(A_{t}g(b_{t}))\,g(b_{t })\,, b_{t+1}=b_{t}-\,_{}^{}(A_{t}g(b_{t}))\,A _{t}g^{}(b_{t})\,. \]We call these dynamics _the mean model_. Figure 8 shows that the mean model closely captures the GD dynamics for the ReLU network (2), and we henceforth focus on analyzing the mean model.

The main advantage of the representation (5) is that it makes apparent the connection to the single-neuron example that we studied in Section 2. More specifically, (5) can be interpreted as the "rescaled" GD dynamics on the objective \((A,b)_{}(Ag(b))\), where the step size for the \(A\)-dynamics is multiplied by \(2d^{2}\). Due to this resemblance, we can apply the techniques from Section 2.

### Two different regimes for the mean model

Throughout the section, we use the shorthand \(_{}\), and focus on initializing wiht \(a_{0}^{}=(1/d)\), \(a^{-}+a^{+} 0\), and \(b_{0}=0\). This implies \(A_{0}=(1)\). We also note the following fact for later use.

**Lemma 2** (formula for the smoothed ReLU; see Subsection E.1).: _The smoothed ReLU function \(g\) can be expressed in terms of the PDF \(\) and the CDF \(\) of the standard Gaussian distribution as \(g(b)=(b)+b\,(b)\). In particular, \(g^{}=\)._

Note also that \(b_{t}\) is monotonically decreasing. This is because \(^{}(A_{t}g(b_{t}))\,A_{t}g^{}(b_{t}) 0\) since \(^{}\) is an odd function and \(g(b),g^{}(b)>0\) for any \(b\).

Following Subsection 2.2, we begin with the continuous-time dynamics of the mean model:

\[=-2d^{2}\,^{}(Ag(b))\,g(b)\,,=-^{}(Ag( b))\,Ag^{}(b)\,. \]

**Lemma 3** (conserved quantity; see Subsection E.1).: _Let \(:\) be defined as \((b)_{0}^{b}g/g^{}\). Along the gradient flow (6), the quantity \(A^{2}-2d^{2}(b)\) is conserved._

Based on Lemma 3, if we initialize the continuous-time dynamics (6) at \((A_{0},0)\) and if \(A_{t} 0\), then the limiting value of the bias \(b_{}^{}\) satisfies \((b_{}^{})=-}\,A_{0}^{2}\), which implies that \(b_{}^{}=-(})\); indeed, this holds since \(^{}(0)=g(0)/g^{}(0)>0\), so there exist constants \(c_{0},c_{1}>0\) such that \(c_{0}b(b) c_{1}b\) for all \(-1 b 0\). Since the mean model (5) tracks the continuous-time dynamics (6) until it reaches the \(b\)-axis, the mean model initialized at \((A_{0},0)\) also approximately reaches \((A_{t_{0}},b_{t_{0}})(0,-(}))(0,0)\) in high dimension \(d 1\). In other words, _the continuous-time dynamics_ (6) _fails to learn threshold neurons_.

Once the mean model reaches the \(b\)-axis, we again identify two different regimes depending on the step size. A Taylor expansion of \(^{}\) around the origin yields the following approximate dynamics (here \(^{}(0)=1/4\)): \(A_{t_{0}+1} A_{t_{0}}-}{2}\,A_{t_{0}}\,g(b_{t_{0}})^{2 }=A_{t_{0}}\,(1-}{2}\,g(b_{t_{0}})^{2})\). We conclude that the condition which now dictates whether we have bouncing or convergence is \(\,d^{2}g(b_{t_{0}})^{2}>2/\).

Figure 8: Under the same setting as Figure 1, we compare the mean model with the GD dynamics of the ReLU network. The mean model is plotted with black dashed line. Note that _the mean model tracks the GD dynamics quite well during the initial phase of training._

Figure 7: The ‘smoothed’ ReLU \(g(b)\)

1. _Gradient flow regime:_ If \(2/>d^{2}g(0)^{2}/2=d^{2}/(4)\) (since \(g(0)^{2}=1/(2)\)), i.e., the step size \(\) is _below_ the threshold \(8/d^{2}\), then the final bias of the mean model \(b_{}^{}\) satisfies \(b_{}^{} b_{}^{} 0\). In other words, _the mean model fails to learn threshold neurons_.
2. _EoS regime:_ If \(2/<d^{2}/(4)\), i.e., the step size \(\) is _above_ the threshold \(8/d^{2}\), then \(\,d^{2}g^{2}(b_{}^{})<2/\), i.e., \(b_{}^{}<g^{-1}(2/})\). For instance, if \(=}\), then \(b_{}^{}<-0.087\). In other words, _the mean model successfully learns threshold neurons_.

### Results for the mean model

**Theorem 3** (mean model, gradient flow regime; see Appendix E).: _Consider the mean model (5) initialized at \((A_{0},0)\), with step size \(=}\) for some \(>0\). Let \(\{,\,8-,\,|}\}\). Then, as long as \(/|A_{0}|\), the limiting bias \(b_{}^{}\) satisfies_

\[0 b_{}^{}-(/)\,|A_{0}|=-O_{A_{0},}(1/ d^{2})\,.\]

_In other words, the mean model fails to learn threshold neurons._

**Theorem 4** (mean model, EoS regime; see Appendix E).: _Consider the mean model initialized at \((A_{0},0)\), with step size \(=}\) for some \(>0\). Furthermore, assume that for all \(t 1\) such that \(\,d^{2}g(b_{t})^{2}>2/\), we have \(A_{t} 0\). Then, the limiting bias \(b_{}^{}\) satisfies_

\[b_{}^{} g^{-1}2/- _{}(1)\,.\]

_For instance, if \(=}\), then \(b_{}^{}<-0.087\). In other words, the mean model successfully learns threshold neurons._

## 4 Conclusion

In this paper, we present the first explanation for the emergence of threshold neuron (i.e., ReLU neurons with negative bias) in models such as the sparse coding model (1) through a novel connection with the "edge of stability" (EoS) phenomenon. Along the way, we obtain a detailed and rigorous understanding of the dynamics of GD in the EoS regime for a simple class of loss functions, thereby shedding light on the impact of large learning rates in non-convex optimization.

Our approach is largely inspired by the recent paradigm of "_physics-style_" approaches to understanding deep learning based on simplified models and controlled experiments (c.f. (Zhang et al., 2022; von Oswald et al., 2023; Abernethy et al., 2023; Allen-Zhu and Li, 2023; Li et al., 2023; Ahn et al., 2023a, b)). We found such physics-style approach quite effective to understand deep learning, especially given the complexity of modern deep neural networks. We hope that our work inspires further research on understanding the working mechanisms of deep learning.

Many interesting questions remain, and we conclude with some directions for future research.

* _Extending the analysis of EoS to richer models._ Although the analysis we present in this work is restricted to simple models, the underlying principles can potentially be applied to more general settings. In this direction, it would be interesting to study models which capture the impact of the depth of the neural network on the EoS phenomenon. Notably, a follow-up work by Song and Yun (2023) uses bifurcation theory to extend our results to more complex models.
* _The interplay between the EoS and the choice of optimization algorithm._ As discussed in Subsection 2.3, the bouncing phase of the EoS substantially slows down the convergence of GD (see Figure 11). Investigating how different optimization algorithm (e.g., SGD, or GD with momentum) interact with the EoS phenomenon could potentially lead to practical speed-ups or improved generalization. Notably, a follow up work by Dai et al. (2023) studies the working mechanisms of a popular modern optimization technique called _sharpness-aware minimization_(Foret et al., 2021) based on our sparse coding problem.
* _An end-to-end analysis of the sparse coding model._ Finally, we have left open the motivating question of analyzing how two-layer ReLU networks learn to solve the sparse coding model (1). Despite the apparent simplicity of the problem, its analysis has thus far remained out of reach, and we believe that a resolution to this question would constitute compelling and substantial progress towards understanding neural network learning. We are hopeful that the insights in this paper provide the first step towards this goal.