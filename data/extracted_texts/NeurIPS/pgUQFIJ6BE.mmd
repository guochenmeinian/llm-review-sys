# Near-Optimal Distributed Minimax Optimization

under the Second-Order Similarity

Qihao Zhou

School of Data Science, Fudan University

zhouqh20@fudan.edu.cn

&Haishan Ye

School of Management, Xi'an Jiaotong University

SGIT AI Lab, State Grid Corporation of China

yehaishan@xjtu.edu.cn

&Luo Luo

School of Data Science, Fudan University

Shanghai Key Laboratory for Contemporary Applied Mathematics

luoluo@fudan.edu.cn

The corresponding author

###### Abstract

This paper considers the distributed convex-concave minimax optimization under the second-order similarity. We propose stochastic variance-reduced optimistic gradient sliding (SVOGS) method, which takes the advantage of the finite-sum structure in the objective by involving mini-batch client sampling and variance reduction. We prove SVOGS can achieve the \(\)-duality gap within communication rounds of \(}( D^{2}/)\), communication complexity of \(}(n+ D^{2}/)\), and local gradient calls of \(}(n+(+L)D^{2}/(1/))\), where \(n\) is the number of nodes, \(\) is the degree of the second-order similarity, \(L\) is the smoothness parameter, and \(D\) is the diameter of the constraint set. We can verify that all of above complexity (nearly) matches the corresponding lower bounds. For the specific \(\)-strongly-convex-\(\)-strongly-convex case, our algorithm has the upper bounds on communication rounds, communication complexity, and local gradient calls of \((/(1/))\), \(((n+/)(1/))\), and \(}(n+(+L)/)(1/))\) respectively, which are also nearly tight. Furthermore, we conduct the numerical experiments to show the empirical advantages of the proposed method.

## 1 Introduction

We study the distributed minimax optimization problem

\[_{x}_{y}f(x,y):=_{i=1}^{n}f _{i}(x,y),\] (1)

where \(f_{i}\) is the differentiable local function associated with the \(i\)-th node, and \(^{d_{x}}\) and \(^{d_{y}}\) are the constraint sets. We are interested in the centralized setting, where there are one server node and \(n-1\) client nodes that collaboratively solve the minimax problem. Without loss of generality, we assume the function \(f_{1}\) is located on the server node and the functions \(f_{2},,f_{n}\) are located on the client nodes. This formulation is a cornerstone in the study of game theory, aiming to achieve the Nash equilibrium . It covers a lot of applications such as signal processing , optimal control , adversarial learning , robust regression  and portfolio management .

We focus on the first-order optimization methods for solving convex-concave minimax problem. The classical full-batch approaches including extra-gradient (EG) method , forward-backward-forward (FBF) , optimistic gradient descent ascent (OGDA) , dual extrapolation  and so forth [33; 34; 38] achieve the optimal first-order oracle complexity under the assumption of Lipschitz continuous gradient [17; 42; 55]. For the objective with finite-sum structure, the stochastic variance reduced methods [1; 10; 14; 30; 53] can reduce the cost of per iteration by using the inexact gradient and lead to the better overall computational cost than full-batch methods. It is natural to design the parallel iteration schemes by directly using above ideas to reduce the computational time in distributed setting.

The communication complexity is a primary bottleneck in distributed optimization. The local functions in machine learning applications typically exhibit homogeneity [3; 15; 18], which is helpful to improve the communication efficiency. One common measure used to describe relationships among local functions is the second-order similarity, e.g., the Hessian of each local function differs by a finite quantity from the Hessian of global objective. Based on such characterization, several communication efficient distributed optimization methods have been established [4; 5; 8; 19; 20; 25; 29; 46; 48; 50; 56]. The highlight of these methods is their communication complexity bounds mainly depend on the degree of second-order similarity, which is potentially much tighter than the results depend on the smoothness parameter [6; 7; 11; 13; 16; 21; 22; 25; 26; 28; 32; 36; 37; 47].

Recently, Khaled and Jin , Lin et al.  showed iterations with partial participation can further reduce the communication complexity, improving the dependence on the number of nodes. They proposed stochastic variance reduced proximal point methods for convex optimization, which allow only one of clients to participate into the communication in the most of rounds. Additionally, Beznosikov et al.  combined partial participation with forward-backward-forward based method, reducing volume of communication complexity for minimax optimization. However, these methods [8; 20; 29] increase the communication rounds, which result in more expensive time cost in communication than the full participation strategies [5; 25]. In other words, the partial participation methods [8; 20; 29] only reduce the overall volume of information exchanged among the nodes, while the advantage of parallel communication enjoyed in full participation methods is damaged.

In this paper, we propose a novel distributed minimax optimization method, called stochastic variance-reduced optimistic gradient sliding (SVOGS), which uses the mini-batch client sampling to balance communication rounds, communication complexity, and computational complexity. We prove SVOGS simultaneously achieves the (near) optimal communication complexity, communication rounds, and local gradient calls for convex-concave minimax problem under the assumption of second-order similarity. We also conduct numerical experiments to show the superiority of SVOGS.

## 2 Preliminaries

We focus on the distributed optimization in client-sever framework for solving minimax problem (1). The notation \(f_{i}\) presents the local function on the \(i\)-th node. We assume the function \(f_{1}\) is located on the server and the other individuals are located on clients. We stack variables \(x^{d_{x}}\) and \(y^{d_{y}}\) as the vector \(z=[x;y]^{d}\), where \(d=d_{x}+d_{y}\). We let \(:=^{d}\) and define the projection operator \(_{}(v):=_{z}\|z-v\|\) for given \(v^{d}\). We also denote the vector functions \(F_{i}:^{d}^{d}\) and \(F:^{d}^{d}\) as

\[F_{i}(z):=_{x}f_{i}(x,y)\\ -_{y}f_{i}(x,y) F(z):= _{i=1}^{n}F_{i}(z).\]

We consider the following common assumptions for our minimax problem.

**Assumption 1**.: _We suppose the constraint set \(^{d}\) is a non-empty, closed, and convex._

**Assumption 2**.: _We suppose the constraint set \(^{d}\) is bounded by diameter \(D>0\), i.e., we have \(\|z_{1}-z_{2}\| D\) for all \(z_{1},z_{2}\)._

**Assumption 3**.: _We suppose each local function \(f_{i}:^{d_{x}}^{d_{y}}\) is smooth, i.e., there exists \(L>0\) such that \(\|F_{i}(z_{1})-F_{i}(z_{2})\| L\|z_{1}-z_{2}\|\) for all \(i[n]\) and \(z_{1},z_{2}^{d}\)._

**Assumption 4**.: _We suppose each differentiable local function \(f_{i}:^{d_{x}}^{d_{y}}\) is convex-concave, i.e., we have \(f_{i}(x,y) f_{i}(x^{},y)+_{x}f_{i}(x^{},y),x-x^ {}\) and \(f_{i}(y) f_{i}(y^{})+_{y}f_{i}(y^{}),y-y^{}\) for all \(i[n]\), \(x,x^{}^{d_{x}}\) and \(y,y^{}^{d_{y}}\)._

**Assumption 5**.: _We suppose the global objective \(f:^{d_{x}}^{d_{y}}\) is strongly-convex-strongly-concave, i.e., there exists \(>0\) such that the function \(f(x,y)-\|x\|^{2}+\|y\|^{2}\) is convex-concave._

Besides, we introduce the assumption of second-order similarity to measure the homogeneity in local functions .

**Assumption 6**.: _The local functions \(f_{1},,f_{n}:^{d_{x}}^{d_{y}}\) are twice differentiable and hold the \(\)-second-order similarity, i.e., there exists \(>0\) such that_

\[\|^{2}f_{i}(x,y)-^{2}f(x,y)\|\]

_for all \(i[n]\), \(x^{d_{x}}\) and \(y^{d_{y}}\)._

We measure the sub-optimality of the approximate solution \(z=(x,y)\) by duality gap, that is

\[(x,y):=_{y^{}}f(x,y^{})-_{x^{ }}f(x^{},y).\]

We also consider the criterion of the gradient mapping for given \(z=(x,y)\), that is,

\[_{}(z)=_{}(z- F(z))}{},\]

where \(>0\). The gradient mapping \(_{}(z)\) is a natural extension of gradient operator \(F(z)\). Noticing that we have \(_{}(z)=F(z)\) if the problem is unconstrained (i.e., \(=^{d}\)), and the condition \(_{}(z)=0\) is equivalent to the point \(z\) is a solution of the problem. Compared with the duality gap, the norm of gradient mapping is a more popular measure in empirical studied since it is easy to achieve in practice.

For the specific strongly-convex-strongly-concave case, we can also measure the sub-optimality by the square of Euclidean distance to the unique solution \(z^{*}=(x,y)\), that is

\[\|z-z^{*}\|^{2}=\|x-x^{*}\|^{2}+\|y-y^{*}\|^ {2}.\]

Moreover, we use notations \(()\), \(()\) and \(()\) to hide constants which do not depend on parameters of the problem, and notations \(}()\), \(()\), and \(()\) to additionally hide the logarithmic factors of \(n\), \(L\), \(\) and \(\).

## 3 Related Work

For convex-concave minimax optimization, the full batch first-order methods  can achieve \(\)-duality gap within at most \((LD^{2}/)\) iterations. Applying these idea to distributed setting naturally leads to the communication rounds of \((LD^{2}/)\) and each round requires all of the \(n\) nodes to compute and communicate their local gradient.

In a seminar work, Beznosikov et al.  proposed Star Min-Max Data Similarity (SMMDS) algorithm, which additionally consider the second-orders similarity (Assumption 6) by involving gradient sliding technique . The SMMDS requires communication rounds of \(( D^{2}/)\), which benefits from the homogeneity in local functions. Each round of this method needs to communicate/compute the local gradient of all \(n\) nodes, and perform the local updates on the server within \(}(L/(1/))\) local iterations, which results in the overall communication complexity of \((n D^{2}/)\) and local gradient complexity of \(}((n+L)D^{2}/(1/))\). Later, Kovalev et al.  introduced extra-gradient sliding (EGS), which further improves the local gradient complexity to \(((n+L)D^{2}/)\). It is worth pointing out that the communication rounds of \(( D^{2}/)\) achieved by SMMDS and EGS matches the lower complexity bound under the second-order similarity assumption . However, these methods enforce all nodes to participate into communication in every round, which does not sufficiently take the advantage of finite-sum structure in the objective.

Recently, Beznosikov et al.  proposed Three Pillars Algorithm with Partial Participation (TPAPP), which uses the variance-reduced forward-backward-forward method  to encourage only one of clients participate into the communication in most of the rounds. The TPAPP can achieve point \(z^{d}\) such that \([\|F(z)\|^{2}]\) for unconstrained case within the communication rounds of \((n^{2}D^{2}/)\)

[MISSING_PAGE_FAIL:4]

optimistic gradient descent ascent (OGDA) method [34; 43] iterates with

\[z^{k+1}=_{}z^{k}-()+F(z^{k}) -F(z^{k-1})}_{}),\] (4)

where \(>0\) is the step size. It is well-known that OGDA achieves optimal convergence rate under the first-order smoothness assumption [42; 55], which motivated us construct the quadratic approximation of \(g(x,y)\) by using the optimistic gradient of \(g\) at \((x^{k},y^{k})\) in the linear terms, that is

\[& g(x,y)(x,y)\\ =& g(x^{k},y^{k})+g(x^{k },y^{k})+_{x}g(x^{k},y^{k})-_{x}g(x^{k-1},y^{k-1})}_{ {optimistic gradient with respect to $x$}},x-x^{k}+\|x-x^{k}\|^{2}\\ &+g(x^{k},y^{k})+_{y}g(x^{k},y^{k})-_{y}g(x^{k-1},y^{k-1})}_{},y-y^{k}-\|y-y^{k}\|^{2}.\] (5)

Applying approximation (5) to formulation (3), we obtain the optimistic gradient sliding (OGS), which iteratively solve the sub-problem

\[(x^{k+1},y^{k+1})_{}_{ }\ (,)+f_{1}(,).\] (6)We can verify function \(g(x,y)\) is \(\)-smooth under Assumption 6, which indicates taking \(=(1/)\) and solving the sub-problem sufficiently accurate can find an \(\)-suboptimal solution within the iteration numbers of \(( D^{2}/)\) and \((/(1/))\) for the convex-concave case and the strongly-convex-strongly-concave case respectively (see Section 5). The dependence on \(\) implies OGS benefits from the second-order similarity in local functions, while each of its iteration requires the communication and the computation of the exact gradient of \(f(x,y)\) within the complexity of \((n)\).

The key idea to improve the cost in each iteration is involving the mini-batch client sampling and variance reduction with momentum . Specifically, we estimate the optimistic gradient in formulation (5) as follows

\[G(z^{k})+G(z^{k})-G(z^{k-1})^{k}|}\!\!_{j ^{k}}\!G(w^{k-1})+G_{j}(z^{k})-G_{j}(w^{k-1})+(z^{k})-G_{j}(z^{k-1}))}_{},\] (7)

where \(G(z):=F(z)-F_{1}(z)\), \(^{k}[n]\) is the random index set, \(w^{k}\) is the snapshot point which is updated infrequently in iterations, and \((0,1)\) is the momentum parameter. Applying the optimistic gradient estimation (7) to formulations (5)-(6), we achieve our stochastic variance-reduced optimistic gradient sliding (SVOGS) method (Algorithm 1).

The proposed SVOGS enjoys the mini-batch partial participation in the steps communication and computation in most of rounds, which is the main difference between SVOGS and existing methods . Concretely, taking the mini-batch size \(|^{k}|=(\,)\) for SVOGS can simultaneously balance communication rounds, communication complexity and local gradient complexity. The SVOGS keeps both the benefit of parallel communication like full participation methods (i.e., SMMDS  and EGS ) and the low communication cost like existing participation methods (i.e., TPAPP ). Additionally, the communication advantage of SVOGS also makes the algorithm achieves better local gradient complexity than state-of-the-arts .

## 5 The Complexity Analysis

In this section, we provide the complexity analysis of proposed SVOGS (Algorithm 1) to show its superiority. In particular, we let \(=0\) for the convex-concave case to the ease of presentation.

We analyze the convergence of SVOGS (Algorithm 1) by establishing the Lyapunov function

\[^{k}:=&( +)\|z^{k}-z^{*}\|^{2}+2 F(z^{k-1})-F_{1}(z^{k-1})-F( z^{k})+F_{1}(z^{k}),z^{k}-z^{*}\\ &+\|z^{k}-z^{k-1}\|^{2}+\|w^ {k-1}-z^{k}\|^{2}+\|w^{k}-z^{*}\|^{2},\] (8)

where we take weight \( 1/8\) and the step size \( 1/(32)\) which always guarantees \(^{k} 0\) by using Young's inequality and the similarity assumption (see detailed proof in Appendix B).

We show that the decrease of Lyapunov function in expectation as follows.

**Lemma 1**.: _Suppose Assumptions 1, 3, 4, and 6 hold with \(0 L\), running SVOGS (Algorithm 1) with \(\{1/,1/(32)\}\), \(=\{1-/(6(1-)),1-pp/(2+)\}\), \( 1/8\), \(256^{2}^{2}^{2}(b+1)/b\), \(4^{2}/b/(4)\), and \(_{k} c^{-1}\{\|^{k}-z^{k}\|,\|^{k}-z^{k} \|^{2}\}\) for some \(c=(,)\), then we have_

\[[^{k+1}] \{1-,1-\}[^{k}]-[ \|z^{k}-^{k}\|^{2}]-[\|w^{k} -^{k}\|^{2}].\] (9)

### The Convex-Concave Case

For the convex-concave case, we use Jensen's inequality and the convexity (concavity) to bound the duality gap at \(u^{K}_{}=_{k=0}^{K-1}u^{k}\) as follows

\[(u^{K}_{})_{(x^{},y^{}) }_{k=0}^{K-1}(f(u^{k}_{x},y^{})-f(x^{ },u^{k}_{y}))_{z}_{k=0}^{K-1 } F(u^{k}),u^{k}-z.\] (10)Applying Lemma 1 by summing over inequality (9), we can bound the right-hand side of (10) via the terms of \(_{k=0}^{K-1}[\|z^{k}-^{k}\|^{2}]\) and \(_{k=0}^{K-1}[\|w^{k}-^{k}\|^{2}]\), and achieve the following theorem.

**Theorem 1**.: _Suppose Assumptions 1, 2, 3, 4 and 6 hold with \(0< L\) and \(D>0\), we run Algorithm 1 with \(b=\), \(=p=1/(+8)\), \(=\{/(4),1/(32)\}\), \(=1\), and \(_{k}=\{,^{-1}\{\|^{k}-z^{k}\|, \|^{k}-z^{k}\|^{2}\}\}\) for some \(=(L,,n,D,)\) and \(=()\). Then we have_

\[[_{z}_{k=0}^{K-1}  F(u^{k}),u^{k}-z]}{ K}+,\;\;u_{}^{K}=_{k= 0}^{K-1}u^{k}.\]

Theorem 1 shows we can run SVOGS with step size \(=(1/)\) and communication rounds of \(K=( D/)\) to achieve the \(\)-sub-optimality in expectation. Additionally, each communication round contains the expected communication complexity of \(b(1-p)+np=(\,)\), leading to the overall communication complexity of \((n+ D^{2}/)\).

The sub-problem (2) in SVOGS (line 8 of Algorithm 1) is a minimax problem with \((L+1/)\)-smooth and \((1/)\)-strongly-convex-\((1/)\)-strongly-concave objective. Therefore, the setting of \(_{k}\) and \(\) in the theorem indicates the condition \(\|u^{k}-^{k}\|^{2}_{k}\) can be achieved by the local iterations number of \(((L+)/(_{k}))=}(L/ (1/))\) on the server (e.g., use EG ). Additionally, each round of SVOGS contains the expected local gradient complexity of \(b(1-p)+np=(\,)\) to achieve the (mini-batch) optimistic gradient \(^{k}\). Hence, the overall local gradient complexity of SVOGS is \(}(K(+L/(1/)))=}(n+(+L)D^{2}/(1/))\). We formally present the upper complexity bounds of SVOGS for the convex-concave case as follows.

**Corollary 1**.: _Following the setting of Theorem 1, we can achieve \([(u_{}^{K})]\) within communication rounds of \(( D^{2}/)\), communication complexity of \((n+ D^{2}/)\), and local gradient complexity of \(}(n+(+L)D^{2}/(1/))\), where \(u_{}^{K}=_{k=0}^{K-1}u^{k}\)._

### The Strongly-Convex-Strongly-Concave Case

By appropriate settings of SVOGS, Lemma 1 leads to the following linear convergence of our Lyapunov function in the strongly-convex-strongly-concave case.

**Theorem 2**.: _Suppose Assumptions 1, 3, 4, 5 and 6 hold with \(0< L\), we run Algorithm 1 with \(b=\{,/\}\), \(=p=1/(\{,/\}+8)\), \(=\{/(4),1/(32)\}\), \(=\{1-/(6(1-)),1-p/(2+)\}\), and \(_{k}=c^{-1}\{\|^{k}-z^{k}\|,\|^{k}-z^{k}\|^ {2}\}\) for some \(c=(,)\). Then we have_

\[[^{K}]\{1-,1-\}^{K}^{0}.\]

We then apply Theorem 2 with \(K=(/(1/))\) and analyze the complexity like the discussion after Theorem 1, which results in the upper complexity bounds as follows.

**Corollary 2**.: _Following the setting of Theorem 2, we can achieve \([\|z^{K}-z^{*}\|^{2}]\) within communication rounds of \((/(1/))\), communication complexity of \(((n+/)(1/))\), and local gradient complexity of \(}((n+(+L)/)(1/))\)._

### Making the Gradient Mapping Small

For the convex-concave case (under the assumptions of Theorem 1), we can achieve the points with small gradient mapping by solving the regularized problem

\[_{x}_{y}(x,y):=f(x,y)+ {2}\|x-x^{0}\|^{2}-\|y-y^{0}\|^{2}\] (11)

for some \(>0\). Noticing that the function \((x,y)\) is \((L+)\)-smooth, \(\)-strongly-convex-\(\)-strongly-concave and \(\)-similarity. Then Corollary 2 implies running SVOGS (Algorithm 1) by iterations number \(K=( D/(L/))\) to solve problem (11) with \(=(/D)\) can achieve \([\|_{}(z^{K})\|^{2}]\), which results in the complexity shown in Table 3. For the strongly-convex-strongly-concave case, the complexity of achieving \([\|_{}(z^{K})\|^{2}\) nearly matches the complexity of achieving \(\|z^{K}-z^{*}\|^{2}\). We defer the detailed derivation for these results of making the gradient mapping small to Appendix G.

## 6 The Optimality of SVOGS

In this section, we provide the lower complexity bounds for solving our minimax problems by using distributed first-order oracle (DFO) methods. The class of algorithms considered in our analysis follows the definition of Beznosikov et al. , which is formally described in Appendix D. Compared with existing lower bound analysis for second-order similarity only focusing on communication [5; 8], we additionally study the computation complexity by considering the local gradient calls. The results in this section imply the complexity of proposed SVOGS (nearly) matches the lower bounds on the communication rounds, the communication complexity and the local gradient calls simultaneously.

### The Lower Bounds for Convex-Concave Case

We first provide the following lower bounds for convex-concave case.

**Theorem 3**.: _For any \(0< L\), \(n 3\), \(D>0\) and \( D^{2}/(12\,)\), there exist \(L\)-smooth and convex-concave functions \(f_{1},,f_{n}:^{d_{x}}^{d_{y}}\) with \(\)-second-order similarity, and closed convex set \(=\) with diameter \(D\). In order to find an approximate solution \(z=(x,y)\) of problem (1) such that \([(z)]\), any DFO algorithm needs at least \(( D^{2}/)\) communication rounds._

**Theorem 4**.: _For any \(0< L\), \(n 2\), \(D>0\) and \( D^{2}/(16\,)\), there exist \(L\)-smooth and convex-concave functions \(f_{1},,f_{n}:^{d_{x}}^{d_{y}}\) with \(\)-second-order similarity, and closed convex set \(=\) with diameter \(D\). In order to find an approximate solution \(z=(x,y)\) of problem (1) such that \([(z)]\), any DFO algorithm needs at least \((n+ D^{2}/)\) communication complexity and \((n+ D^{2}/)\) local gradient calls._

The lower bounds on communication round and communication complexity shown in Theorem 3 and 4 match the corresponding upper bounds of SVOGS shown in Corollary 1. However, the lower bound on local gradient complexity shown in Theorem 4 only nearly matches the result of Corollary 1 in the case of \((L)\). Therefore, we also provide the following lower bound on local gradient complexity to show the tightness of dependence on the smoothness parameter \(L\).

**Lemma 2**.: _For any \(L>0\), \(n\), \(D>0\) and \( D^{2}/(4)\), there exist \(L\)-smooth and convex-concave functions \(f_{1},,f_{n}:^{d_{x}}^{d_{y}}\) with \(\)-second-order similarity, and closed convex set \(=\) with diameter \(D\). In order to find an approximate solution \(z=(x,y)\) of problem (1) such that \([(z)]\), any DFO algorithm needs at least \((n+LD^{2}/)\) local gradient calls._

Combining the results of Theorem 4 and Lemma 2, we achieve the following lower bound on local gradient complexity, which nearly matches the corresponding upper bound shown in Corollary 1.

**Theorem 5**.: _For any \(0< L\), \(n 2\), \(D>0\) and \( D^{2}/(16)\), there exist \(L\)-smooth and convex-concave functions \(f_{1},,f_{n}:^{d_{x}}^{d_{y}}\) with \(\)-second-order similarity, and closed convex set \(=\) with diameter \(D\). In order to find an approximate solution \(z=(x,y)\) of problem (1) such that \([(z)]\), any DFO algorithm needs at least \((n+(+L)D^{2}/)\) local gradient calls._

The constructions in our lower bound analysis is based on the modifications on the bilinear functions provided by Han et al. , which are originally used to analyze the minimax optimization in non-distributed setting. We provide detailed proofs in Appendix E. In related work, Beznosikov et al.  also provide the lower bound of \(( D^{2}/)\) (matching the result of Theorem 3) for communication rounds by using the regularized function, which is different from our construction in the proof of Theorem 3. In addition, our lower bounds on the communication complexity and the local gradient complexity shown in Theorem 4 and 5 are new.

### The Lower Bounds for Strongly-Convex-Strongly-Concave Case

The tight lower bound on communication rounds in strongly-convex-strongly-concave case has been provided by Beznosikov et al. (2015, Theorem 1). We present the result as follows.

**Theorem 6** ().: _For any \(,,L>0\) with \(L\{,\}\) and \(n 3\), there exist \(L\)-smooth and convex-concave functions \(f_{1},,f_{n}:^{d_{x}}^{d_{y}}\) with \(\)-second-order similarity such that the function \(f(x,y)=_{i=1}^{n}f_{i}(x,y)\) is \(\)-strongly-convex-\(\)-strongly-concave. In order to find a solution of problem (1) such that \([\|z-z^{*}\|^{2}]\), any DFO algorithm needs at least \((/(1/))\) communication rounds._

The tight lower bound on communication complexity has been provided by Beznosikov et al. (2015). We follow their construction to establish the lower bound on local gradient complexity, nearly matching the corresponding upper bound of our SVOGS. We formally present these lower bounds as follows.

**Theorem 7**.: _For any \(,,L>0\) with \(L\{,\}\) and \(n 2\), there exist \(L\)-smooth and convex-concave functions \(f_{1},,f_{n}:^{d_{x}}^{d_{y}}\) with \(\)-second-order similarity such that the function \(f(x,y)=_{i=1}^{n}f_{i}(x,y)\) is \(\)-strongly-convex-\(\)-strongly-concave. In order to find a solution of problem (1) such that \([\|z-z^{*}\|^{2}]\), any DFO algorithm needs at least \(((n+/)(1/))\) communication complexity and \(((n+(+L)/)(1/))\) local gradient calls._

## 7 Experiments

We conduct the experiment on robust linear regression (2015, 2016, 2017). Concretely, we consider the constrained convex-concave minimax problem

\[_{\|x\|_{1} R_{x}}_{\|y\| R_{y}}_{i=1}^{N} (x^{}(a_{i}+y)-b_{i})^{2},\] (12)

and the unconstrained strongly-convex-strongly-concave minimax problem

\[_{x^{d^{}}}_{y^{d^{}}}_{i=1}^{N}(x^{}(a_{i}+y)-b_{i})^{2}+\| x\|^{2}-\|y\|^{2},\] (13)

where \(x\) contains the weights of the model, \(y\) describes the noise, and \(\{(a_{i},b_{i})\}_{i=1}^{N}\) is the training set.

Figure 1: Results for convex-concave minimax problem (12) on a9a.

Figure 2: Results for strongly-convex-strongly-concave minimax problem (13) on a9a.

We compare the proposed SVOGS (Algorithm 1) with baselines Extra-Gradient method (EG) , Star Min-Max Data Similarity algorithm (SMMDS) , Extra-Gradient Sliding (EGS) ), and Three Pillars Algorithm with Partial Participation (TPAPP) . We test the algorithms on real-world datasets "a9a" (\(N=32,561\), \(d^{}=123\)), "w8a" (\(N=49,749\), \(d^{}=300\)) and "covtype" (\(N=581,012\), \(d^{}=54\)) from LIBSVM repository  and set the nodes number be \(n=500\). For problem (12), we set \(R_{x}=2\) and \(R_{y}=0.05\), respectively.

We implement all of the methods by Python 3.9 with NumPy and run on a machine with AMD Ryzen(TM) 7 4800H 8 core with Radeon Graphics 2.90 GHz CPU with 16GB RAM. We solve the sub-problem in SVOGS (Algorithm 1), SMMDS , EGS , and TPAPP  by Extra-Gradient method of Korpelovich . We tune the step-size \(\) of SVOGS from \(\{0.01,0.1,1\}\). The probability \(p\) is tuned from \(\{p_{0},5p_{0},10p_{0}\}\), where \(p_{0}=1/\{+/\}\). The batch size \(b\) is determined from \(\{ b_{0}/10, b_{0}/5, b_{0}\}\), with \(b_{0}=1/p_{0}\). We set the other parameters by following our theoretical analysis. We set the average weight as \(=1-p\). For the momentum parameter, we set \(=1\) for convex-concave case and \(=\{1-/(6(1-)),1-p/(2+)\}\) for strongly-convex-strongly-concave case, where we estimate \(\) by \(\{,\}\) for problem (13). For the sub-problem solver, we set its step-size according to the smoothness parameter of sub-problem, i.e., \(1/(L+1/)\). In addition, we estimate the smooth parameter \(L\) and the similarity parameter \(\) by following the strategy in Appendix C of Beznosikov et al. .

We present the experimental results in Figure 1 to 4 for datasets "a9a" and "w8a". The results for dataset "covtype" is displayed in Appendix H due to the space limitation. We can observe that our SVOGS outperforms all baselines in terms of the local gradient complexity. Additionally, the SVOGS requires less communication rounds than classical EG and existing partial participation method TPAPP, and it requires significantly less communication complexity than full participation methods EG, SMMDS and EGS. All of these empirical results support our theoretical analysis.

## 8 Conclusion

This paper presents a novel distributed optimization method named SVOGS, which use the second-order similarity in local functions and the finite-sum structure in objective to solve the convex-concave minimax problem within the near-optimal complexity. Our theoretical results are also validated by the numerical experiments. In future work, it is interesting to use our ideas to improve the efficiency of distributed nonconvex minimax optimization under the second-order similarity.

Figure 4: Results for strongly-convex-strongly-concave minimax problem (13) on w8a.

Figure 3: Results for convex-concave minimax problem (12) on w8a.