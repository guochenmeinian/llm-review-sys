# Is Programming by Example Solved by LLMs?

Wen-Ding Li

Cornell University

wl678@cornell.edu &Kevin Ellis

Cornell University

kellis@cornell.edu

###### Abstract

Programming-by-Examples (PBE) aims to generate an algorithm from input-output examples. Such systems are practically and theoretically important: from an end-user perspective, they are deployed to millions of people, and from an AI perspective, PBE corresponds to a very general form of few-shot inductive inference. Given the success of Large Language Models (LLMs) in code-generation tasks, we investigate here the extent to which LLMs can be said to have'solved' PBE. We experiment on classic domains such as lists and strings, and an uncommon graphics programming domain not well represented in typical pretraining data. We find that pretrained models are not effective at PBE, but that they can be fine-tuned for much higher performance, provided the test problems are in-distribution. We analyze empirically what causes these models to succeed and fail, and take steps toward understanding how to achieve better out-of-distribution generalization. Collectively these results suggest that LLMs make strong progress toward solving the typical suite of PBE tasks, potentially increasing the flexibility and applicability of PBE systems, while also identifying ways in which LLMs still fall short.

## 1 Introduction

Programming-by-Example (PBE) systems solve a challenging task: Given input-output examples of a hidden algorithm, they seek to construct the source code of the underlying function [1; 2]. PBE is deployed to millions of users [3; 4; 5; 6], lies near the heart of core AI challenges [7; 8; 9; 10], and is a qualitatively different problem from the bulk of recent work on LLM code generation, because rather than generate source code from natural language , PBE is instead fundamentally about few-shot inductive inference: Given a handful of examples, inferring the program that will generalize to new inputs, or which captures the true latent regularity, _without_ relying on natural-language guidance.

We investigate here the extent to which large language models pretrained on source code can solve PBE. If they can, this unlocks the ability to do PBE in general-purpose Turing complete languages like Python, unlike the restricted domain-specific languages which have so far dominated PBE [4; 12; 13; 14, i.a.], thereby increasing the scope and power of this paradigm. If LLMs cannot perform PBE, then this highlights a deficit of inductive reasoning and problem solving, and suggests LLMs lean too heavily on natural language cues to generate code.

We find that pretrained and instruction-tuned models serve as poor PBE systems, a finding also supported by recent work [15; 16; 12; 17]. But our investigation further finds that LLMs can be fine-tuned for significantly higher performance, provided they are not asked to generalize far beyond the fine-tuning data. To address this failure of generalization we give an algorithm for taking a small unlabeled dataset of problems and adapting the LLM to it, which we find narrows this domain gap.

The resulting recipe allows PBE over Turing-complete languages across three qualitatively different domains (Fig. 1): algorithms on vectors of numbers, string manipulation macros, and graphics programs in LOGO/Turtle. In every case, our final model is at least as effective as custom symbolic search algorithms operating over domain-specific languages, and surpasses powerful closed-sourcemodels such as GPT4 . We also find that the resulting system can cover a broader scope of problems than classic symbolic methods, owing to the use of a Turing-complete language, which, at least theoretically, allows learning any computable function.

## 2 Background

**Programming by Example** considers synthesizing a program \(\) given a vector of inputs \(X\) and corresponding outputs \(Y\). Typically the program is expected to exactly fit the provided examples, \((X_{i})=Y_{i}, i\), where \(i\) indexes examples. The program \(\) is drawn from a (potentially infinite) language \(\). Typically \(\) is a domain-specific language designed for a specific PBE system, not a general-purpose programming language. For example, the PBE system FlashFill synthesizes string manipulation macros designed to automate common spreadsheet edits . FlashFill's domain-specific language \(\) includes commonly occurring regular expressions, together with string slicing and concatenation, and restricted forms of loops. The language \(\) is also designed to allow polynomial-time construction of programs consistent with input-output examples. FlashFill's goal, like most PBE systems, is to generalize to hold-out test cases: inputs \(X^{}\) with (hidden) target outputs \(Y^{}\).

\[\{\;:\;(X_{i})= (Y_{i}), i\}.(X^{}_{j})=(Y^{}_{j}), j\] (1)

In its simplest forms, PBE can be accomplished by guess-and-check enumeration until a program is found that is consistent with the examples. Although there exist more sophisticated search algorithms, including those accelerated by neural guidance [19; 20; 21; 22], a key enabler of practical PBE systems is the design of a carefully restricted domain-specific language \(\). The domain-specific language effectively hardcodes symbolic knowledge, focusing the system on what programs the human engineer thinks are most promising, but at the expense of the wider set of computable functions expressible in general-purpose languages.

The PBE setup covers other cases as well, such as sequence extrapolation (the inputs are indices into the sequence), as well as data compression (the input is null, and the data is compressed by synthesizing a program that reproduces the output data). Therefore, a truly general solution to PBE--one which could express its solutions in general purpose programming languages, and cover most

Figure 1: Domains, including standard ones that resemble programs found in pretraining data, as well as a less common graphics domain, which is likely less represented in LLM pretraining data.

practically relevant problems--would be broadly applicable to many inductive inference problems, a point that has been long appreciated .

**LLMs for solving programming problems** have been recently very successful [11; 23; 24; 25; 26]. These systems typically input a prompt describing a problem in natural language, then sample candidate programs, and optionally filter those samples by checking them against input-output test cases, with the goal of passing holdout tests:

Draw \(_{k} p_{}(|)\). Pick a \(\{_{k}\ :\ _{k}(X_{i})=_{k}(Y_{i}), i\}\). Success: \((X_{j}^{})=(Y_{j}^{}), j\)

Unlike PBE, the primary driver of program generation is a natural language prompt, although input-outputs may also be in the prompt [27; 28]. Recent work using LLMs to synthesize programs solely from examples has either obtained negative results [16; 12], or focused on simple and/or nonstandard problems [29; 30; 31], leaving the extent to which PBE is'solved' by LLMs an open question.

## 3 Methods

**Basic prompting** is the most straightforward way of performing PBE with a pre-trained model: Given input-output examples \((X,Y)\) a prompt is constructed and \(K\) programs are generated. Programs are filtered by the I/O examples, and a random satisfying program is returned:

Sample

\[_{k} p_{}(|(X,Y)),k1..K\] (2) Pick a \[\{_{k}\ :\ _{k}(X_{i})=_{k}(Y_{i}), i\}\] (3)

**Fine-tuning** improves the above approach in a conceptually straightforward way. Given a dataset comprising tuples of programs and I/O examples, \(\{(,X,Y)\}\), we fine-tune the LM to predict a program from its input-outputs. But this dataset is hard to come by: Although there are web-scale corpora of naturally occurring source code, there is no analogous dataset of runnable code snippets paired with representative input-output examples, and this data deficit is especially true for new or unusual applications of PBE, such as the graphics programs we consider.

To assemble a large dataset of \((,X,Y)\) triples we start with a small manually-constructed seed dataset, \(_{}\), and then randomly generate new programs \(\) and inputs \(X\) by prompting an LLM with members of \(_{}\). The output \(Y\) comes from running \(\) on \(X\). The seed dataset effectively defines a prior over \((,X)\), notated \(\) in Fig. 2. We sample from \(\) to collect many program-input pairs, but use program execution to predict \(Y\), not an LLM. The resulting dataset, which we call \(_{}\), is used to train an LLM to generate programs when prompted with input-outputs. As this fine-tuned LLM effectively learns to do probabilistic inference in the graphical model shown in Fig. 2 (right), we write this fine-tuned LLM as \(q_{}(|X,Y)\). This inference network is trained to maximize

\[_{} q_{}(|X,Y)(,X)( _{})Y=(X)\] (4)

This method is closely related to self-instruct  and wake-sleep . Like self-instruct, we use prompting to bootstrap a large dataset from a small manually-constructed one. Our method differs by using the LLM to generate a hidden latent variable (the program) while a different generative process produces an observed variable (the program outputs). Like wake-sleep, we use samples from a generative model to train an inference network, but we do not further train the generative model itself. Next, we will see that bringing the method much closer to wake-sleep by updating the generative model plays an important role when deploying the system on out-of-distribution problems.

Figure 2: Left: Data generation pipeline. Right: The fine-tuned network \(q_{}\) learns to do inference in a graphical model where the prior over programs, \(\), is defined by prompting an LLM with example code in \(_{}\), while the likelihood \(p(Y|,X)\) is defined by program execution.

Adaptation.One of the most powerful features of source code as a representation is its ability to efficiently express a wide range of computations. Therefore it is of interest to study the ability of fine-tuned LLMs to extrapolate to PBE problems outside the distribution of the fine-tuning data.

We consider a basic approach to adapting to a different distribution of problems, assuming access to problems drawn from the testing distribution, but without labeled program solutions. This mimics the deployment of PBE systems to end-users who may have their own idiosyncratic distribution of problems they care about, and who do not provide ground-truth programs, but who can provide feedback on if a generated program has correct behavior. This means we have an unlabeled dataset \(_{}\) comprising input-outputs \((X,Y)\), as well as a labeled seed dataset \(_{}\) comprising triples \((,X,Y)\). Adaptation proceeds by iterating between pretraining with \((_{})\), testing on \(_{}\), and adding back into \(_{}\) any program solutions found on the adaptation problems, which then become seeds for the next iteration. This produces a sequence of fine-tuned models, indexed below by \(i\):

\[ ^{i}=*{arg\,max}_{} q_{}( |X,Y)(,X)(_{}^{i})Y=(X)\] \[ _{k}^{X,Y} q_{^{i}}(|X,Y)(X,Y) _{}k1..K\] \[ ^{i+1}=^{i}\{(_{k}^{X,Y},X,Y) \ :\ (X,Y)_{},\ k[K]_{k}^{X,Y}(X)=Y\}\] (5)

The equations can be seen as a wake-sleep algorithm where "dreaming" corresponds to training \(q\) on fantasy data (first equation) while "waking" corresponds to running inference and updating the generative model \(\) (by updating the seed, second pair of equations). Ideally, each cycle of this wake-sleep adaptation solves more out-of-distribution problems, which tugs the generative model \(\) toward the target distribution, unlocking solutions to more out-of-distribution problems, etc. This hinges on each iteration actually solving new problems from the unlabeled dataset. Theoretically this is guaranteed given enough inference-time compute (large \(K\) above). We explore in Sec. 4.3 the extent to which this holds in practice.

## 4 Experiments

We study different LLM-approaches to programming-by-examples across three domains (Fig. 1):

1. **List functions** is a PBE domain meant to model a "programmer's assistant". It concerns discovering algorithms that transform lists of numbers, given input-output examples. This problem statement has a long history within program synthesis [13; 34], and was popularized within machine learning by DeepCoder . We consider two modern list function datasets created by Rule et al. 2024  and Shi et al. 2023 , which both involve higher-order functions and nontrivial procedures such as map, filter, and sort. Rule et al. was recently added to BigBench .
2. **Text editing** is a domain where a program synthesizer assists an end-user edit their spreadsheets or other documents. From string-to-string examples, the system generates edit macros for tasks such as reformatting dates, extracting fields from semistructured text, etc. [2; 37; 38; 4]. Text editing is the most prominent commercial success of PBE: The FlashFill PBE system ships in Microsoft Excel and is used by many millions of people . We consider two text editing datasets: SyGuS problems --which are easier--and PROSE  problems, which constitute the most challenging dataset of its kind .
3. **LOGO/Turtle graphics** is a domain whose goal is to synthesize a program that generates a target image.1 Systems of this kind can be used both for high-level visual reasoning and for helping artists make structured edits to images [40; 41]. We use a dataset of geometric designs expressed as LOGO/Turtle  programs--where the programs move a simulated pen over a canvas--taken from Wong et al. . To allow the LLM to visually perceive the input image, we convert the image to ASCII-art style strings; see Fig. 5 and Appendix. A.1.3.

### How well does the fine-tuned model perform?

We prepare seed datasets for each domain, synthetically generate a large training set, and then fine-tune a DeepSeekCoder LLM  that was pretrained on source code.2 For list functions we seed with 50 problems from Rule et al. 2024; For text editing, we consider seeding with either SyGuS or a 40-problem subset of PROSE; for LOGO we seed with 200 training-set problems in Wong et al. .

The resulting fine-tuned models are surprisingly effective within their respective PBE domains. On list functions our finetuned model surpasses the best symbolic search baselines reported in Rule et al. (Fig. 2(a)), surpasses the best neurosymbolic search method from Shi et al. (Appendix Fig. 10), and surpasses GPT4. It also solves 100% of the list to list benchmark problems from \(^{2}\) (a well-known symbolic synthesizer), shown in Appendix Tbl. 4: although plausibly, many \(^{2}\) problems are in the pretraining data. On text editing, it surpasses the performance of FlashFill and approaches the level of FlashFill++ (Tbl. 1, Fig. 2(b)). On LOGO, it solves 90% of the test set (Fig. 2(c)), surpassing systems such as DreamCoder , which introduced the first version of these LOGO problems. It also solves more problems than LILO and Regal , which are LOGO program synthesizers that input natural language describing how the image should be drawn. In contrast, our model does not use any language clues, generating purely from the image. In addition to quantitatively solving more problems, we note that there are qualitative improvements to the breadth of problems that can be solved in the first place because the LLM can generate Turing-complete code spanning a much broader space of computations (Fig. 4).

There are caveats to the above results. First, the fine-tuned model essentially never produces a correct program on the first try: It requires tens or hundreds of samples, each of which is compared against the ground-truth input-outputs, and discarded if it contradicts the examples. On a GPU like the one we use (an Nvidia A6000) this rejection sampling takes on the order of a few minutes to solve a given problem. However, compared to classic enumerative program synthesizers , or even compared to those with neural guidance , proposing a few thousand programs is relatively little, and could not plausibly cover a significant fraction of the exponentially large search space.

  & gen. & oracle \\  & accuracy & accuracy \\  FlashFill & 33\% & — \\ FlashFill++ & — & \(\)100\% \\ ours, 33B & 82\% & 88\% \\ 

Table 1: Generalization accuracy: % problems where the program makes correct predictions on every holdout test. Oracle accuracy: % problems where a correct program was generated (even if incorrect programs were also generated that also passed the training input-outputs). FlashFill++  only reports oracle accuracy. 3

Figure 3: Test set performance. A problem is solved if the predicted program generates correct outputs on the holdout inputs. Metagol , RobustFill , and Fleet  results taken from The second caveat is that the model degrades when tested out-of-distribution. An example of this degradation is illustrated in Fig. 6, which tests the LOGO graphics model on hand drawings (after training on clean computer graphics). On the out-of-distribution hand drawing the model mostly samples programs that do not fit the data, but its accuracy does not fall to zero, meaning that with enough compute budget, it does actually generate reasonable programs. This foreshadows the results in Sec. 4.3, which more systematically studies out-of-distribution behavior.

Figure 4: PBE with LLMs allows using general-purpose programming languages which can mix string and numerical operations in ways not allowed by domain-specific languages  (top), and allows world knowledge to inform code generation (bottom). I/Os and code partly elided for space.

Figure 5: ASCII representation of LOGO graphics. Average pixel intensity indicated by numbers 0-9

Figure 6: Example out-of-distribution LOGO test: inferring a graphics program from a hand drawing. See also Appendix Fig. 12

### What causes the fine-tuned model to succeed or fail?

Classic symbolic approaches to PBE, when they are based on enumeration, tend to succeed whenever the target program is syntactically small. Approaches based on clever dynamic programming, such as the FlashFill family , succeed when the program is representable in the domain-specific language. What predicts success for these LLM approaches?

To answer this question we investigate several hypotheses. First, potentially the success is determined by program size, and degrades as programs grow longer. Second, as a more refined notion of size, we instead measure the description length _under the prior_, which for a program \(\), is \(- p_{}(|(_{}))\). Description length under the prior would be a good predictor of success if the fine-tuned model engages in blind guess-and-check: simply learning the distribution \((_{})\), and sampling from this prior while ignoring the input-outputs. Third, one possibility is that success is predicted by description length _under the approximate posterior_ (\(- q_{}(|X,Y)\)), which would be the case if the fine-tuned model attends closely to the input-outputs and reshapes its distribution accordingly, instead of defaulting to the prior. To test these hypotheses we calculate the average compute budget needed to solve each problem, and compare it with these different variables. Fig. 7 shows that posterior description length is more predictive than program size and prior description length: unlike classical methods, metrics of program length correlate poorly with problem difficulty, and there is no evidence that the fine-tuned model's behavior can be characterized as blind guess-and-check. (See also Fig. 5).

### Out-of-distribution generalization

One advantage of classic symbolic PBE methods is that they do not make statistical assumptions about their test problems. Indeed, some classic methods can, within their domains, synthesize programs perfectly (i.e. always find a program that fits the training input-outputs). In contrast, neural networks can struggle to generalize beyond the training distribution.

We therefore consider train/test splits that force the model to generalize beyond the distribution of its training data (beyond \(_{}\)). On text editing, we seed with SyGuS problems, and perform out-of-distribution testing on PROSE problems (PROSE is much harder than SyGuS). On list functions, we seed with problems from Rule et al. 2024 and test on Shi et al. 2023 (the Shi dataset contains unusual combinators, such as Scan). On LOGO, we seed with short programs (\( 12\) lines of code), and test on long programs (\(>12\) lines of code). Using these splits we also measure the ability of the adaptation method in Sec. 3 to improve out-of-distribution generalization.4

Fig. 8 shows that there is nontrivial degradation when testing out of distribution. For example, a 7B model seeded with PROSE problems and tested on a different subset of PROSE has an accuracy of 76% (Fig. 3b), but this degrades to 59% when seeded with SyGuS problems, which follow a different distribution and are generally simpler and easier than PROSE (Fig. 8b).

Figure 7: Compute budget needed to solve a problem is best predicted by description length under the approximate posterior, _not_ program size or prior description length, suggesting that the fine-tuned model is not engaging in blind guess-and-check.

We further perform the adaptation method described in Sec. 3 in order to measure the extent to which it can narrow these domain gaps. In every case it allows solving more out-of-distribution problems, increasing absolute performance by around 10% or more in all domains, which is a relative increase of about 16% for text/list and a relative increase of about 190% for LOGO (approximately tripling the number of solved LOGO problems).

To better understand the dynamics of adaptation, we visualize the specific problems solved before and after adaptation on LOGO graphics (Fig. 9). Before adaptation, only a handful of out-of-distribution problems are solvable, and only with a significant search budget. Adaptation allows the system to quickly solve similar out-of-distribution problems in the future, but does not allow the system to generalize to problems very unlike those originally solvable by the fine-tuned model. In principle, expanding the inference-time compute budget should allow successful adaptation (large \(K\) in Eq. 5). Another more compute-efficient approach would be to increase the amount of adaptation data by introducing'steppingstone' problems in the adaptation set that give a gentler transition from the original training distribution.

Figure 8: Out-of-distribution generalization and adaptation to new test distribution.

Figure 9: Out-of-distribution LOGO problems (requiring long programs with \(>12\) lines of code). We show example problems that are solved by the original model fine-tuned on short programs, which then become training data for the next round of adaptation. Adaptation allows consistent solving of problems similar to those that the original fine-tuned model could sometimes solve, but is not a panacea: Problems dissimilar to those solved by the initial model are not ever correctly generated, despite the fact that they are solvable by a model fine-tuned in-distribution.

## 5 Related Work

Automatic data generation with LLMs, such as self-instruct , WizardCoder , and many others [51, 52, 53, i.a.], works by prompting an LLM to produce outputs which are then used for later learning stages such as fine-tuning. These approaches are applied recursively to their own output: Previously generated data is incorporated into future prompts. We similarly generate a dataset \(_{}\) by prompting an LLM with \(_{}\), but (1) do not recursively prompt the LLM with its own outputs and (2) combine the LLM generations with program execution to make program outputs. This gives a different mathematical interpretation to our data generator. First, the programs are samples from a prior, \(\), defined by \(_{}\), which would not be a valid interpretation if the LLM was repeatedly fed its own outputs. Second, there is an observation model or likelihood function, \(p(Y|,X)\), which is defined not by the LLM, but by a Python interpreter. In this way, our data generator constructs training examples for fine-tuning that teach the network how to invert the execution process of the Python interpreter.

Machine learning applied to PBE has often sought to accelerate search: to find _any program at all_ consistent with the I/O examples , which is nontrivial due to the combinatorial nature of the search, even after confining to a domain-specific programming language. A complementary line of research explores inductive biases that favor programs likely to generalize to new inputs, such as learning a prior or ranking function . Our work should be seen within the tradition of learning to search for programs. We show that finetuned models serve as an effective yet simple foundation for accelerating search in PBE, allowing search to be tractable over much richer and more expressive languages such as Python.

Classic PBE.Traditional approaches to programming-by-examples operate by symbolically searching or solving for programs consistent with the input-output examples . They use domain-specific programming languages that are designed to either enable efficient search and/or bias the system toward functions that are likely to generalize new inputs. Search for programs can even be polynomial time when this domain-specific language has a special structure (roughly, when every function can be 'inverted'), a key enabler of FlashFill, the first commercial success of PBE .

LLMs as inductive reasoners.Using an LLM to perform inductive reasoning--to generate abstract hypotheses from concrete specific examples--has been explored by several recent works , all of which has found significant value in translating these hypotheses into programs, and all of which have worked by prompting pretrained GPT-style models. Our work can be seen as helping answer a natural question posed by these previous works: Given that LLMs can generate hypotheses from examples, can they produce programs of the nature and complexity demanded by PBE? We find this is largely the case after fine-tuning, both for classic PBE domains and unusual ones.

Self-Debugging, Refinement, and Self-repair.One way of improving the code generation abilities of an LLM is to have it attempt to debug its own code whenever the initially generated code does not pass the provided test cases . We did not explore this strategy, however, because a more basic approach that simply regenerated a new program from scratch already surpassed the prior state of the art (both symbolic and neural baselines), provided we finetune. However, further pushing the boundary of PBE may benefit from self-debugging strategies.

Ranking LLM-generated code.Past work considers a variety of ways to select an output from a collection of LLM-sampled programs , many of which are more sophisticated than simply filtering by the examples, which is what we do here. Like with self-debugging, integrating these techniques should be synergistic with our approach.

## 6 Limitations

Our work has important limitations. From an engineering perspective, using a 7B-33B neural network to perform PBE is not practical for most end-users, who may be doing PBE on their laptop or desktops in order to accomplish small one-off tasks. For this reason, true deployment to end-users may requireinvestigating the effectiveness of much smaller neural networks (not an LLM), and it may also be valuable to study the effect of network compression and distillation upon our finetuned models.

From the perspective of understanding where and why our system succeeds and fails, we have shown that neither program size nor likelihood under the prior suffice to predict success, finding the posterior likelihood is a better predictor, albeit an imperfect one. Although this allows us to discard the hypothesis that the system is merely sampling from the prior, it also just pushes the question back one stage further: What exactly about specific problems causes the neural network's approximate posterior to put more or less probability mass on correct solutions? While in classic PBE one can obtain sharp answers as to why a certain problem was solved or not, this is a much harder question with neural networks, whose workings are more opaque.

## 7 Discussion

PBE with fine-tuned LLMs is surprisingly effective, surpassing many of the best neural and symbolic baselines we know of, even for uncommon domains such as LOGO graphics. Why is that? Fundamentally, the neural network only needs to act as a heuristic proposer of solutions, because we can check against the input-outputs. Therefore, one possible explanation is that the tendency of language models to over-generate, hallucinate, and cover the long tail of possibilities is actually an asset, instead of a liability. And although there is a degree of degradation on out-of-sample problems, the degradation is not so severe that out-of-distribution problems become utterly unsolvable: Instead, they merely become harder to solve, a phenomenon that allows adaptation to work in the first place.

Simultaneously one should be hesitant about claiming that PBE is'solved.' Optimistically, current PBE benchmarks exist to test the frontier of what is possible, and so doing well on those benchmarks might just mean that the frontier has moved. More realistically, determining if an AI system truly works in the wild requires more than just pushing benchmark numbers, which can be misleading when those benchmarks do not capture the long tail of naturally-occurring tests. Furthermore, all AI systems present tradeoffs, and a neural system's unpredictability, high computational cost, and out-of-distribution fragility should be weighed against whatever high benchmark numbers they may achieve. Despite these caveats, we are optimistic about the promise of tuning LLMs for PBE, and believe that it has the potential to dramatically expand the scope of solvable problems and even solvable domains.

Acknowledgements.We are grateful for assistance from Joshua Rule in the processing of the list functions data, and for feedback from Yewen Pu on the manuscript. This work was supported by an NSF CAREER grant as well as gifts from Google and Cisco.