# Formalizing locality for normative synaptic plasticity models

Colin Bredenberg

Mila- Quebec AI Institute

Montreal, Quebec H2S 3H1

&Ezekiel Williams

Mila- Quebec AI Institute

Montreal, Quebec H2S 3H1

Cristina Savin

New York University

New York, NY 10003

&Blake Richards

McGill University

Mila- Quebec AI Institute

Montreal, Quebec H2S 3H1

&Guillaume Lajoie

Universite de Montreal

Mila- Quebec AI Institute

Montreal, Quebec H2S 3H1

###### Abstract

In recent years, many researchers have proposed new models for synaptic plasticity in the brain based on principles of machine learning. The central motivation has been the development of learning algorithms that are able to learn difficult tasks while qualifying as "biologically plausible". However, the concept of a biologically plausible learning algorithm is only heuristically defined as an algorithm that is potentially implementable by biological neural networks. Further, claims that neural circuits could implement any given algorithm typically rest on an amorphous concept of "locality" (both in space and time). As a result, it is unclear what many proposed local learning algorithms actually predict biologically, and which of these are consequently good candidates for experimental investigation. Here, we address this lack of clarity by proposing formal and operational definitions of locality. Specifically, we define different classes of locality, each of which makes clear what quantities cannot be included in a learning rule if an algorithm is to qualify as local with respect to a given (biological) constraint. We subsequently use this framework to distill testable predictions from various classes of biologically plausible synaptic plasticity models that are robust to arbitrary choices about neural network architecture. Therefore, our framework can be used to guide claims of biological plausibility and to identify potential means of experimentally falsifying a proposed learning algorithm for the brain.

## 1 Introduction

Over the last several decades, computational neuroscience researchers have proposed a variety of "biologically plausible" models of synaptic plasticity that seek to provide normative accounts of a variety of learning processes in the brain--these models aim to explain how modifications of cellular properties such as excitability and synaptic strengths can improve performance on a variety of important tasks for an organism, using only the information "locally" available to a real neuron in the brain. However, there is no consensus on what "biologically plausible" really means. This is due to a lack of any precise definition of locality, with individual studies adopting different, architecture-specific heuristics that are often not explicitly articulated; further, at an experimental level, the precise quantities available to neurons for computing synaptic updates are still up for debate , and likely vary across cell types  and brain regions .

As an example, some studies postulate that global neuromodulatory signals containing reward information could be available at individual synapses, in addition to the more traditional pre- and post-synaptic information typically required for Hebbian plasticity [11; 12; 1]. Within this broad class, individual studies differ in their assumptions about this reward information (e.g. is it completely global  or potentially distinct for different layers or regions ) and in the precise details about the pre- and post-synaptic signals available (e.g. is it spike-timing ) that is available or spike-rate ?). Furthermore, there are many more signals that could theoretically be available at synapses: for example, other studies propose that individual pyramidal neurons could receive detailed error (or target) feedback information at their apical dendrites [17; 18; 19; 2] which can be used for learning. Previous work has taxonomized different normative plasticity models according to these feedback signals . Within this space of normative plasticity models, it can be difficult to identify which features of a given model constitute necessary, testable predictions that can be used for experimental verification--i.e., if this information were not available at a synapse, learning with the given algorithm would be impossible--and which features are due to arbitrary choices that may vary with more or less realistic neural network architectures.

As a consequence of these heterogeneous conceptions of locality and given the advancements in the field, it is arguably necessary to develop a clear and precise way to identify what variables synapses should have access to in a learning algorithm, whenever claims of locality are made. In this paper, we focus on formalizing this process. Our central contributions are as follows:

1. We develop an architecture-independent formal framework for locality that we term \(p\)-locality, which requires one to precisely specify the set of variables that synapses are assumed to have access to within a model: different choices of allowed variables for the set \(\) produce different classes of locality. Importantly, this definition can be used for any learning algorithm applied to any stochastic neural network.
2. We use our framework to group existing plasticity models into different locality classes. Intriguingly, we find that many different algorithms derive their locality from similar principles, even though they belong to different locality classes.
3. We show that our different locality classes make different experimental predictions, which makes it possible to identify which algorithms can be cleanly distinguished experimentally and which cannot.

## 2 Formalizing notions of locality

Our ultimate goal is a formal definition of locality that makes it easy to identify which variables are locally available to synapses in a learning algorithm within any specific model architecture. However, this can be challenging. For example, when we consider a single-compartment neuron model it seems reasonable to assume that the postsynaptic voltage is locally available to synapses. In contrast, in a multi-compartment model, voltages can be different in each compartment, and synapses likely only have access to voltages within nearby compartments. Ideally, we would avoid a definition that simply requires us to list all of the variables that must be made available to the synapse, as that would carry little conceptual weight. However, we will begin by taking this approach, as it will help us to build the groundwork for a more conceptually useful definition in what follows.

**Notation.** We have \(\) as a random vector in \(^{N}\) denoting network variables (e.g. voltage, spike-rate, inputs, etc.), and \(\) as a random vector in \(^{N}\) denoting network parameters (e.g. synaptic weights, spike threshold, etc.). We combine these as \(=[,]\)--the concatenation of the two vectors. We will use \( p()\) to denote a random vector with support \(^{N+N}\), but will also overload this notation to indicate a vector subject to either universal or existential quantification (e.g. \(\ \ f()}{}=0\)) or a dummy variable in an expectation (e.g. \(_{p()}[]=\)). We will use the subscript "\( i\)" to denote removal of the \(i^{th}\) element of a vector; for example \(_{ i}=[_{1},,_{i-1},_{i+1}, ,_{N+N}]^{}\), and will occasionally use the more general notation \(_{}\), where \(\) is a set of indices, to denote the vector with the associated set of elements removed.

### S-locality: graph-specific locality based on a predefined set of available variables

We begin with a definition of locality that formalizes the concept of a set of variables, \(_{k}\), to which the \(k^{th}\) synapse can have access in order to compute synaptic updates (e.g. some neural voltages,reward signals, etc.). We then consider the set of these sets across all synapses, \(=\{_{1},,_{N_{}}\}\). This inspires the following definition:

**Definition 2.1**.: _Given parameters \(^{N_{}}\), random variables \( p(|)^{N_{}}\), and function \(f():^{N_{}+N_{}} ^{N_{}}\), the update function \(f()\) is \(\)-local with respect to some set \(=\{_{k}:_{k}\}\) if and only if \( k,i\):_

\[()}{ _{i}} 0_{i}_{k}.\] (1)

In short, \(\)-locality requires that if parameter update \(f_{k}()\) has a _direct_ dependence on \(_{i}\), then \(_{i}\) must be included in the set \(_{k}\) of "allowed" variables for that parameter. For example, in the classic approach to locality in a neural network, for any synapse \(_{ij}\), we might set \(_{ij}=\{_{i},_{j}\}\), allowing any \(\)-local parameter update under this definition to use pre- (\(_{j}\)) and post-synaptic (\(_{i}\)) firing rate information, and nothing else. If we wanted to allow a third factor such as a global neuromodulatory signal, \(R\), to project to diffusely to every synapse in the network, as in reward-modulated synaptic plasticity rules, we might take \(_{ij}=\{_{i},_{j},R\}\). In this manner, for any model architecture, we can manually construct a set \(_{ij}\) that defines our notion of locality for each synapse.

While our definition is intuitive and flexible, it is cumbersome: for every parameter we have to explicitly define which variables are local. This means that \(\)-locality is architecture-specific, requiring an evaluation of which \(_{k}\) is an acceptable set of variables for each synapse's update rule. For example, if we switched from a single- to a multi-compartment model, we would now have to modify \(_{ij}\) to accept only the voltage of the compartment where the synapse is located. Ideally, our definition would make it obvious whether a learning rule is local in both models without having to redefine the list of which variables are local. To do that, we will use the structure of the distribution \(p(,)\) to construct a definition of locality that captures intuitions across network architectures.

### \(p\)-locality

Despite its ability to concretely define locality for any model, \(\)-locality is architecture-specific and cumbersome: in this section, we will develop an alternative _operationalized_ notion of locality, called \(p\)-locality (where \(p\) stands for "probability"). Our idea is to formulate locality in terms of adjacency in the computational graph underlying network dynamics. To do this we will leverage conditional dependencies in the joint distribution \(p(,)\) to generate an architecture-general concept of locality3. This is considerably more useful as both a conceptual tool and organizing principle for normative synaptic plasticity models. However, we will still call on \(\)-locality later to formulate our final locality definition: \(p\)-locality.

Further, it is important to note that for this definition, as with \(\)-locality, we do not restrict the functional form of parameter updates \(f_{k}()\)--we only restrict which variables they are allowed to depend on. This is in contrast to previous, complementary work which has focused on defining allowable local computations and memory complexity constraints for 'biologically plausible' learning algorithms . Our decision not to restrict \(f_{k}()\) enables us to abstract away the details of 'allowed' subcellular computations, and will consequently enable us to make much more general statements about the locality properties of normative plasticity algorithms as a whole, without having to focus on the specifics of particular models of neural dynamics.

**Definition 2.2**.: _p-locality: Consider a probability distribution \(p()\) over network activity variables and parameters, and an update function \(f:^{N_{}+N_{}}^{N_{ }}\), mapping variables and parameters to parameter updates. Assume that \(_{i}_{j})}{_{i}}\) and \((_{i})}{_{i}}\) exist for all \(\) and \(i,k\) indices. The update function \(f()\) is local with respect to \(p\) (\(p\)-local) if and only if \( k,i\):_

\[\;\;\;()} {_{i}} 0_{p}[(_{i}|_{ i})}{_ {k}})^{2}] 0.\] (2)

To unpack this definition and render it more intuitive, as with \(\)-locality, \(()}{_{i}}\) indicates a test to see whether the \(k\)th index of \(f\) has a direct dependence on a particular variable \(_{i}\)--we assume that \(f_{k}()\) is differentiable primarily for simplicity, and differentiable approximations can always be substituted in practice if \(f_{k}()\) happens to not be differentiable. If there is a dependency, then we want that associated random variable \(_{i}\) to be considered local to \(_{k}\). To measure this, we have selected the Fisher Information of \(p(_{i}|_{ i})\) with respect to \(_{k}\). This measurement quantifies direct influence. For example, if \(_{i}\) is a neuron downstream of neuron \(_{j}\), and \(_{k}\) is a synapse onto \(_{j}\), then by conditioning on \(_{j}\), \(_{k}\) is no longer able to have any effect on the conditional probability distribution of \(_{i}\) through its influence on \(_{j}\). In other words, the statistical influence of \(_{k}\) on \(_{i}\) is mediated _through_ the influence of \(_{j}\) on \(_{i}\), and thus it is indirect. By conditioning on \(_{j}\), we seal off one pathway of indirect influence. \(_{y}\) conditioning on all \(_{ i}\), we seal off _all_ pathways of indirect influence, which makes \(_{i}|_{ i})}{_{k}}=0\) for all \(\).

It is worth stressing that \(p()=p(|)p()\) is a joint distribution over both \(\) and \(\), even though most algorithms operate only on the conditional distribution \(p(|)\). As a consequence, the marginal probability that we place on \(p()\) can be a free choice, which is useful for clarifying the assumptions in the model. For example, we will typically assume that synapses do not have any marginalized dependence on one another, i.e. \(p()=_{k}p(_{k})\).

### Properties of \(p\)-locality

Having defined \(p\)-locality, we can now talk about its interesting properties, which will give intuitions for how it functions and which will figure prominently in our proofs of algorithms' locality. While any \(p\)-local update could also technically be reformulated as a \(\)-local rule, by taking the set \(\) to be the set allowed under \(p\)-locality, the conciseness of \(p\)-locality and its convenient mathematical properties will prove to be its principal benefits over \(\)-locality. For proofs of these properties, see Appendices A-C. To see the power and generality of \(p\)-locality as a definition, we first provide two properties that show how one can immediately make statements about the locality of a learning algorithm by simply inspecting graphs associated with the model. Assume all quantities are defined as in Definition 2.2 and that, for certain properties, the joint density satisfies mild regularity constraints (see Appendix C.1). For the first two properties we further assume that \(p()\) is strictly positive (see Appendix B). Then we have the following:

**Property 2.1**.: _Assume \(_{}\) is a Directed Acyclic Graph (DAG) for \(p\). If \(()}{_{i}} 0\) for \(_{i}\) that is not a parent, co-parent, or child of \(_{k}\) in \(_{}\), then \(f\) is not \(p\)-local._

**Property 2.2**.: _Assume that \(\) defines an Undirected Graph (UG) for \(p\). If \(()}{_{i}} 0\) for \(_{i}\) that is not a neighbour of \(_{k}\) in \(\), then \(f\) is not \(p\)-local._

By these properties, as long as the conditional dependencies of \(p()\) can be summarized by an UG or DAG (two classes which subsume many modern neural network architectures, see Figure 1 for

Figure 1: Common neural network graphical models and the permissible parameter updates for a synapse \(_{ij}\) under \(p\)-locality. **a.** Feedforward neural networks can be described as DAGs, so variables that are not children or coparents of \(_{ij}\) are excluded from parameter updates under \(p\)-locality (by Property 2.1). **b.** For fixed weights in a dynamic neural network like a recurrent neural network (RNN), \(p\)-locality excludes variables that are not children and coparents of \(_{ij}\) for all time points (also by Property 2.1). **c.** For an undirected graphical model like a Boltzmann machine, variables that are not neighbors of \(_{ij}\) are excluded under \(p\)-locality (by Property 2.2).

examples), we can identify many of the variables disallowed by \(p\)-locality. Therefore, for such a network, we can decide whether \(p\)-locality conforms to intuitions about biological plausibility as easily as we can for a hand-crafted set of allowed variables under \(\)-locality; for most practical neural network architectures, we will see below that \(p\)-locality does behave intuitively. Thus, with \(p\)-locality we have a definition of locality that does not require an exhaustive list of the variables available to a synapse, but rather, which relies on the implicit computational structure of the model (i.e. \(p()\)).

**Property 2.3**.: _For any function \(b:^{N_{}+N_{}}^{N_{ }}\) defined such that \(b_{k}()=h_{k}(f_{k}(),g_{k}())\), where \(f()\) and \(g()\) are \(p\)-local and \(h_{k}\) is differentiable, \(b()\) is also \(p\)-local._

This demonstrates that \(p\)-local functions can be arbitrarily combined without the combination losing the \(p\)-local property. It also shows that \(p\)-locality places no restrictions on the functional form of parameter updates, so long as they are exclusively functions of variables allowed under \(p\)-locality.

**Property 2.4**.: _For any function \(f():^{N_{}+N_{}} ^{N_{}}\), there exists a probability distribution \(p()\) such that the random variable \(f()\) with \( p()\) is \(p\)-local._

This is intended as a cautionary note: unless \(p\) defines a neural network-like probability distribution, \(p\)-locality does not necessarily conform to intuitions about biological locality (see Appendix F for examples). That \(f()\) is \(p\)-local for _some_ probability distribution says nothing about a function \(f()\); \(p\)-locality only becomes informative when we narrow our focus to probability distributions with biological relevance.

The following properties will turn out to be extremely important in Section 3: all algorithms that we survey below with provable \(p\)-locality properties will involve the score function or the derivative of the unnormalized energy function.

**Property 2.5**.: _The derivative of the log joint distribution \(,)}{}\) is \(p\)-local._

**Property 2.6**.: _For a probability distribution given by \(p()=(-E())\), where \(\) is a normalizing constant, the expression \(}E()\) is \(p\)-local._

**Property 2.7**.: _If the parameter marginal distribution factorizes as \(p()=_{k}p(_{k})\), i.e. the parameters are independent from one another, then the score function \(|)}{}\) is \(p\)-local._

**Property 2.8**.: _For a mixture distribution \(p_{12}(,)=p_{1}()^{}p_{2}()^{1- }p()\) for some binary variable \(\{0,1\}\) with nonzero probabilities, if \(f()\) is \(p_{1}\)-local (or equivalently \(p_{2}\)-local), then \(f()\) is \(p_{12}\)-local._

Intuitively, this feature holds because mixture distributions introduce _more_ dependencies between parameters \(\) and random variables \(\). Therefore, \(p\)-locality under a mixture of probability distributions is more permissive than \(p\)-locality under any single one of its constituent probability distributions.

### A simple motivating example

To make our definition of \(p\)-locality more concrete, we will now explore how it functions for a common network architecture. Consider a simplified linear-nonlinear feedforward neuron model with \(L\) layers of neurons \(^{(l)}\) (\(=[^{(1)},...,^{(L)}]\)) connected by synaptic weights \(^{(l)}\) (\(=[^{1},...,^{(L)}]\)), with additive Gaussian noise at each layer. Conditioned on our feedforward weight matrices and stimuli \(\), the probability distribution of neural firing rates is given by:

\[p(|,) =_{l=1}^{L}_{i=1}^{N_{l}}p(_{i}^{(l)}| ^{(l-1)},^{(l)})\] (3) \[p(_{i}^{(l)}|^{(l-1)},^{(l)}) (h(_{i:}^{(l)}^{(l-1)}), ^{2}),\] (4)

where for notational simplicity we have taken \(^{(0)}=\), \(h()\) is a pointwise nonlinearity, and \(_{i:}^{(l)}\) corresponds to the \(i\)th row of \(^{(l)}\). As discussed above, most algorithms operate on the conditional distribution, \(p(|)\), so we only define a prior over parameters for the purposes of assessing \(p\)-locality. To ensure that our joint distribution is a directed graphical model, and to ensure that we have no dependencies between parameters, we will assume that the parameters for the network are all independently distributed, i.e. \(p()=_{i,j,l}p(_{ij}^{(l)})\), where \(_{ij}^{(l)}\) is the synapse between postsynaptic neuron \(i\) and presynaptic neuron \(j\) in layer \(l\). To make our choice of conditional probability distribution more concrete (see Figure 1a), we can see that neural firing rates for this model can be sampled by:

\[^{(l)}=h(^{(l)}^{(l-1)})+^{(l)},\] (5)

where \(^{(l)}(0,1)\). This corresponds to an ordinary multilayer perceptron neural network with noise added at every layer. We can use this model to get intuition for how this choice of probability distribution \(p\) constrains the variables allowed for an update for a particular parameter \(^{(l)}_{ij}\).

Now, our probability distribution corresponds to a directed graphical model with dependencies given by Eq. 3, so by Property 2.1, we know that variables that are not parents, coparents, or children of \(^{(l)}_{ij}\) violate \(p\)-locality if included in the parameter update for \(^{(l)}_{ij}\). Figure 1 summarizes the variables allowed under \(p\)-locality for such a DAG. \(^{(l)}_{ij}\) has no parents under our graphical model, leaving only children and coparents. \(^{(l)}_{ij}\) has only one child, \(^{(l)}_{i}\), whose coparents are given by \(^{(l)}_{ik}\) s.t. \(k j\) (all other synapses onto \(^{(l)}_{i}\)), and \(^{(l-1)}\) (all presynaptic neurons).

This shows that updates for synapses in our simple feedforward network cannot depend on neurons that are in layers \(>l\) or \(<l-1\) while maintaining \(p\)-locality. We also found that dependencies on neurons within the postsynaptic layer \(l\) for indices \(k i\) were also not permissible. As a concrete example, by Property 2.7, we found that the score function of \(p(|)\) is \(p\)-local if the parameters are assumed to be independently distributed. Our example satisfies this criterion. Its score function is given by: \(|,)}{^{(l )}_{ij}}=^{(l)}_{i}-h(^{(l)}_{i}))}{ ^{2}}h^{}(^{(l)}_{i})^{(l-1)}_{j},\) where \(^{(l)}_{i}=^{(l)}_{i:}^{(l-1)}\). By inspection, this function only requires information about the post- and presynaptic neurons, as well as the summed input to the postsynaptic neuron, \(_{i}\).

This simple example clearly outlines why \(p\)-locality aligns with expected notions of locality. More importantly, the use of \(p\)-locality also has the benefit of generalizing cleanly to alternative network architectures. For example, if we were to instead inspect a multilayer recurrent neural network (Figure 1b), we would see that for \(^{(l)}_{ij}\) the firing rates \(^{(l)}_{i}(t)\) and all presynaptic neural firing rates would be permissible _for all time steps_, because the activities at each timestep are children or coparents of \(^{(l)}_{ij}\). For another classical example of a biologically plausible probability distribution corresponding to an UG (as in Figure 1c.), see Appendix D for a discussion of a linear continuous Boltzmann machine.

Figure 2: Different versions of \(p\)-locality. **a.**\(Rp\)-locality functions as \(p\)-locality, but also allows synapses to use rewards \(R\) for parameter updates, corresponding to an additional diffuse neuromodulatory signal. **b.**\( p_{md}\)-locality allows synapses to use \(p_{m}\)-local variables _and_\(p_{d}\)-local variables, as well as the global gating variable \(\). For the Wake-Sleep algorithm, a \( p_{md}\)-local probability distribution could correspond to a network of pyramidal neurons receiving \(p_{m}\)-related synapses in their apical compartments (orange) and \(p_{d}\)-related synapses in their basal compartments. The variable \(\) controls whether \(p_{d}\) or \(p_{m}\) synapses affect cellular dynamics. **c.** Backpropagation and its approximations are \(_{i}p\)-local, which functions like \(p\)-locality, except it additionally permits a neuron-specific error signal to be used for parameter updates. Typically, \(\) is constructed by sequentially propagating error signals backwards through a feedforward network.

### S\(p\)-locality

In almost all situations, a given class of plasticity algorithms will produce updates that are _almost_\(p\)-local, in that synapses are required to have access to some small number of global state variables that a biological system could in principle project diffusely throughout a network, but which would not be allowed under generic \(p\)-locality (e.g. reward, which is usually produced far downstream of any given synapse in a neural network). For this eventuality, we introduce a notion called \(p\)-locality, which is best considered a hybrid of \(\)-locality and \(p\)-locality.

**Definition 2.3**.: _Given parameters \(^{N_{}}\) and random variables \(^{N_{}}\), concatenated as \(=[,]^{}\), and a function \(f():^{N_{}+N_{}} ^{N_{}}\), the update function \(f()\) is \(p\)-local with respect to probability distribution \(p()\) and some set \(=\{_{k}:_{k}\}\) if and only if \( k\)\(f_{k}=h_{k}(_{k},g_{k}())\), where \(h_{k}\) is an arbitrary function and \(g_{k}()\) is \(p\)-local._

The value of this definition is that it negotiates a compromise between the architecture-generality of \(p\)-locality, and the flexibility of \(\)-locality. Obviously if we take \(_{k}=\  k\), all functions are \(\)-local, and likewise, if we define \(p\) sufficiently generally, all functions are \(p\)-local (Property 2.4). However, if some class of optimization algorithms operating on a predefined probability distribution \(p\) are provably guaranteed to produce parameter updates that are \(p\)-local for some small set \(\), then we will have obtained a very concise description of the types of information required by that algorithm.

## 3 Applying S\(p\)-locality to normative plasticity models

Given a probability distribution over a neural network states and parameters, \(p(,)\), which could be experimentally motivated or even observed, it is natural to ask which learning algorithms respect locality principles in the way parameter updates are made during learning. Alternatively, if we denote an algorithm which operates on a probability distribution \(p(,)\) to output an update \(f()\) by \((p(,))=f()\), where \(f():^{N_{}+N_{}} ^{N_{}}\), we can ask which probability distributions \(p(,)\) and variable collections \(\) leave \(f()\)\(p\)-local. This is one way S\(p\)-locality could potentially facilitate experimentally testing biologically plausible learning models.

To demonstrate the utility of our approach, in this section we characterize the \(p\)-locality properties of a wide variety of algorithms that have historically been used to produce normative models of synaptic plasticity, as summarized in Table 1. Though our framework covers many algorithms, here we expand on three prototypical algorithms in detail, focusing on features that are relevant for our discussion of the algorithms' locality properties. More algorithmic details as well as theorem proofs are provided in Appendix E. Schematics depicting biological interpretations for the types of \(p\)-locality discussed below are provided in Figure 2.

### Reinforce

REINFORCE , also known as policy gradient learning, produces reward-modulated Hebbian parameter updates for neural networks  similar to the one discussed in Section 2.4. Here we will show that REINFORCE is \(Rp\)-local, meaning that it is \(p\)-local where \(=\{_{k}=R\  k\}\), which assumes each synapse has access to a global scalar reward signal \(R\) that needs to be delivered diffusely to all synapses in a network.

**Theorem 3.1**.: _If \(p()=_{k}p(_{k})\), the REINFORCE estimator given by \(_{R}(p(R,|))\) is \(Rp\)-local._

As an example, consider the network defined by Eq. 3. The REINFORCE update for this network, for a single sample of the network state \(\) and reward \(R\), is given by:

\[_{ij}^{(l)}=R|, )}{_{ij}^{(l)}}=R_{i}^{(l)}- h(_{i}^{(l)}))}{^{2}}h^{}(_{i}^{(l)}) _{j}^{(l-1)},\] (6)

where again \(_{i}^{(l)}=_{i:}^{(l)}^{(l-1)}\). This is just the scalar multiplication of the reward signal \(R\) with the score function; because the score function is \(p\)-local, the full update is \(Rp\)-local (see Appendix E for more detail).

### Wake-Sleep (WS)

Whereas REINFORCE has been used to model reinforcement learning in neural networks, the Wake-Sleep algorithm has been used to model unsupervised learning . It assumes that a neural network has two modes of operation, controlled by a scalar variable \(\) that determines whether the network is in inference mode ('wake', \(=1\)) or in generative mode ('sleep,' \(=0\)). In all, neural activity samples are drawn from the mixture distribution given by \(p_{md}(,|,)=p_{d}(|)^{ }p_{m}(|)^{(1-)}p()\), where \(p()\) defines the probability of sampling from the wake phase (\(p_{d}\); \(d\) corresponds to 'data') or the sleep phase (\(p_{m}\); \(m\) corresponds to'model'), and \(\) corresponds to the feedforward weights while \(\) correspond to feedback weights. For conciseness, in all subsequent sections we will denote the mixture distribution \(p_{1}()^{}p_{2}()^{1-}p()\) by \(Mix(p_{1}(),p_{2}())\). As an example, building on the feedforward network defined by Eq. 3 by adding a generative feedback pathway, we have neural dynamics given by:

\[^{(l)}= h_{w}(^{(l)}^{(l-1)})+(1-)h _{s}(^{(l)}^{(l+1)})+^{(l)},\] (7)

where \(h_{w}()\) corresponds to the 'wake' nonlinearity, and \(h_{s}()\) corresponds to the'sleep' nonlinearity. Under this formulation, \(\) gates, for _all neurons_, whether activity is driven by the feedforward or feedback pathways; there are several hypotheses for how this could be implemented at a neuronal level, including global neuromodulatory or inhibitory gating of the apical and basal dendrites of pyramidal neurons in the cortex  (Fig 2b).

The Wake-Sleep algorithm updates \(\) to fit \(p_{m}\) as a generative model of the network's input data, and updates \(\) to fit \(p_{d}\) to perform approximate inference with respect to that generative model, in a manner similar to variational autoencoders (VAE) [24; 25]; unlike VAE training with backpropagation, however, the Wake-Sleep algorithm is \( p_{md}\)-local, meaning \(=\{S_{k}=\;\; k\}\) (see Appendix E for the proof).

**Theorem 3.2**.: _If \(p(,^{(d)})=_{k}p(_{ k})_{k}p(_{k}^{(d)})\), the Wake-Sleep estimator given by \(_{WS}(p_{m}(|),p_{d}(|^{(d)}))\) is \( p_{md}\)-local, where \(p_{md}=Mix(p_{m}(|),p_{d}(|^{(d)}))p(,^{(d)})\)._

As a concrete example, the Wake-Sleep parameter update for \(_{ij}^{(l)}\) for a single sample of the state \(\) from the network above is given by:

\[_{ij}^{(l)}=(1-)(| ,)}{_{ij}^{(l)}}=(1-)_{i}^{(l)}-h_{w}(_{i}^{(l)}))}{^{2}}h_{w}^{ }(_{i}^{(l)})_{j}^{(l-1)},\] (8)

where \(_{i}^{(l)}=_{i:}^{(l)}^{(l-1)}\); a similar update holds for \(\). Similar to REINFORCE, this parameter update is only the combination of a scalar variable \(\) and the score function, and is consequently \( p_{md}\)-local (see Appendix E for the proof). Interestingly, Wake-Sleep is not the only algorithm to obey this form of locality: several other normative plasticity models, including Boltzmann machine learning , equilibrium propagation , and impression learning  have essentially the same \(p\)-locality properties. However, some of these have additional requirements on the probability distribution that make them less biologically plausible. For example, Boltzmann machine learning is \( p_{md}\)-local only for distributions that can be captured with energy-based models (which typically require weight symmetry).

### Backpropagation (BP) and its approximations

Here we provide a characterization of the locality of the backpropagation algorithm , which as we will show using our definitions, is only a local algorithm with biologically implausible assumptions . This demonstration is primarily important because the \(p\)-locality properties of several biologically plausible backpropagation approximations, including feedback alignment , weight mirror , and Burstprop  satisfy a similar notion of \(p\)-locality, but using more biologically realistic assumptions about \(\) (see Table 1 and Appendix E). Derivations for the backpropagation algorithm and its approximations require more stringent network assumptions (here we assume a feedforward multilayer perceptron network as in Section 2.4), because the algorithm itself is not well-defined for arbitrary probabilistic network architectures.

**Theorem 3.3**.: _If \(p()=_{k}p(_{k})\) and \(p(|)\) is defined by Eq. 3, the BP update for \(_{ij}^{(l)}\) with a loss \(()\), given by \(_{BP}(p(|),())\) is \(_{i}^{(l)}\)-\(p\)-local, where \(_{i}^{(l)}=L}{_{i}}\). Similarly, the updates for feedback alignment, weight mirror, and Burstprop are \(}_{i}^{(l)}\)-\(p\)-local, where \(}_{i}^{(l)}\) is given by their respective gradient approximations._

This notion of \(}_{i}^{(l)}\)\(p\)-locality, where \(=\{_{ij}^{(l)}=}_{i}^{(l)}\;\; i,j,l\}\), includes more restrictive biological assumptions than the one provided by \(Rp\)-locality and \( p_{md}\)-locality, because it relies heavily on the neuron-specific error \(}_{i}^{(l)}\), which must be then accounted for using physiological mechanisms. For our network architecture given by Eq. 3, the functional form of all backpropagation approximations listed in Table 1 is given by:

\[_{ij}^{(l)}=-}_{i}h^{}(_{i}^{( l)})_{j}^{(l-1)},\] (9)

where \(}_{i}\) is an algorithm-specific approximation of \(L}{_{i}}\). Similar to \( p_{md}\)-local algorithms, recent work has suggested that this error signal could be propagated through networks by a top-down error signal relayed to the apical dendrites of pyramidal neurons  (see Fig. 2c). Thus, these algorithms make specific biological predictions, driven by the need for \(}_{i}^{(l)}\) to be accessible to each synapse.

## 4 Discussion

We have constructed a formal definition of locality, \(p\)-locality, that combines set-based and probabilistic graphical model-based formalisms (\(\)-locality and \(p\)-locality, respectively). Our goal was standardizing and operationalizing notions of locality, and we also demonstrated the usefulness of these definitions in characterizing normative plasticity models. Notably, this approach enabled the identification of distinct classes of \(p\)-locality (Table 1) which subsume learning algorithms spanning

  Optimization Alg. & Locality supported & Architectural restrictions \\  REINFORCE & \(Rp\)-local & DAG \\ Maximum Likelihood Estimation & \(p_{m}\)-local & DAG \\ Generalized EM & \(p_{m}\)-local & DAG \\ Predictive Coding & \(p_{m}\)-local & MAP (weight symmetry) \\ Wake-Sleep & \( p_{md}\)-local & DAG \\ Impression learning & \( p_{md}\)-local & DAG \\ Contrastive Divergence & \( p_{md}\)-local & K-Step EBM (weight symmetry) \\ Equilibrium Propagation & \( p_{md}\)-local & EBM (weight symmetry) \\ WTA-STDP & \(p_{m}\)-local & WTA \\ Backpropagation & \(_{i}^{(l)}\)\(p\)-local & MLP (weight transport) \\ Feedback alignment & \(}_{i}^{(l)}\)\(p\)-local & MLP \\ Weight mirror & \(}_{i}^{(l)}\)\(p\)-local & MLP \\ Burstprop & \(}_{i}^{(l)}\)\(p\)-local & MLP \\ RTRL & \(\)\(\)\(\)\(\)\(\) & RNN \\ e-prop & \(_{i}\)\(p\)-local & RNN \\ RFLO & \(}_{i}\)\(p\)-local & RNN \\ FOLLOW & \(p\)-local & error-RNN \\  

Table 1: **Summarizing \(p\)-locality for classical learning algorithms.** First column: the optimization algorithm of interest. Second column: the variant of \(p\)-locality that we have proven in Appendix E. Third column: the network architectures required for both the algorithm and our proofs to work. DAG = Directed Acyclic graphical model, MAP = Maximum _a posteriori_ gradient descent dynamics, WTA = winner-take-all circuit, EBM = Energy-based model, MLP = Multilayer perceptron, RNN = recurrent neural network, error-RNN = error-driven recurrent neural network. ‘K-Step’ refers to the small, finite number of sampling steps required, as opposed to sampling from a steady-state distribution or calculating an equilibrium, which are more time-intensive. Weight symmetry indicates that symmetric recurrent connectivity is required for the original proposed algorithm. Weight transport indicates that symmetric weights are required exclusively for error propagation.

more than thirty years of research, from Boltzmann machines  to modern incarnations like equilibrium propagation . Importantly, our framework can be applied to any neural network architecture, incorporating networks that are recurrent , spiking , and multilayer  into a single framework. This is thanks, in large part, to the architecture-generality of \(p\)-locality, which abstracts away specific details of network models, defining locality instead in terms of statistical dependencies--this makes our framework much more powerful than simpler linguistic descriptions of locality. As we discuss in Appendix F, this abstraction comes at the cost of requiring researchers to specify a distribution, \(p\), that is grounded in biological constraints. We note that for non-biological probability distributions the variables allowed to be "local" under \(p\)-locality can violate standard intuitions; however, for standard neural networks like those discussed in Section 2.4 and Appendix D, \(p\)-locality behaves as one would expect, with "local" learning rules being those that use pre- and postsynaptic activity.

Table 1 organizes existing normative plasticity models into distinct classes. The first locality class, \(Rp\)-locality, requires individual synapses to have access to reward information; this class encompasses many reward-modulated Hebbian plasticity models . The second class, \(p_{(m)}\)-locality, encompasses predictive coding and its variations , and aligns with more traditionally Hebbian plasticity updates (often at the cost of unrealistic network architectures; see Table 1). The third class, \( p_{md}\)-locality, requires neural networks to have two distinct modes of operation (e.g. the 'wake' and'sleep' modes in the Wake-Sleep algorithm) gated by a scalar global variable \(\). Thus, \( p_{md}\)-locality requires synapses to have access to the value of \(\) to compute updates, and subsumes many algorithms that require generative feedback pathways for learning . The final class of algorithms are \(}_{i}^{(l)}p\)-local for feedforward neural networks, where \(}_{i}^{(l)}\) is a neuron-specific approximate error signal, and includes feedback alignment , weight mirror , and Burstprop , as well as temporal variants of these algorithms . These four categories--\(Rp\)-locality, \(p_{(m)}\)-locality, \( p_{md}\)-locality, and \(}_{i}^{(l)}\)-locality--subsume, to our knowledge, almost all existing normative plasticity models.

Because these plasticity models are sometimes only constructed for specific network architectures (which may violate known facts about the brain), we also specify the broadest class of network architectures for which we have successfully derived a model's \(p\)-locality properties (proven in Appendix E). As noted in Appendix F, 'biological plausibility' requires both network architectures and parameter updates to be in line with existing experimental evidence, while \(p\)-locality provides a formal framework for assessing only the latter of these two factors. We therefore stress that \(p\)-locality properties are only relevant for neuroscience if they can be determined for biologically plausible network architectures. As a consequence, classes of network architectures that encompass a broad range of biologically plausible network models (such as DAGs) are desirable, whereas algorithms that are only viable for implausible architectures (e.g. those that imply weight symmetry or weight transport) are undesirable.

Interestingly, the distinct classes of locality also delineate distinct sets of experimentally testable predictions for plasticity. Critically, \(p\)-locality abstracts away details that are not important for testing predictions, and helps identify important features of learning algorithms. For example, the REINFORCE (Eq. 6) and Wake-Sleep (Eq. 8) algorithms are equivalent with respect to the '\(p\)' in \(p\)-locality, which tells us that the set of allowed local variables under \(p\)-locality itself (e.g. pre- and post-synaptic information) are not the best targets for experimental testing between these algorithms, and instead the focus must be on the variables included in '\(\)' (a reward signal \(R\) versus a clamping variable \(\) that switches the network between different modes of synaptic plasticity and neural activity). Thus, by identifying whether plasticity is modulated by reward, neuron-specific error teaching signals, transitions in network dynamics, or none of the above, one may narrow the space of possible candidate plasticity models down to \(Rp\)-, \(}_{i}^{(l)}\)-, \( p_{md}\)-, or \(p_{(m)}\)-locality, respectively. These distinct phenomena are _necessary_ features of the learning algorithms in question for any neural network architecture used and are consequently good targets for experimental testing. In contrast, other features of parameter updates are more about specific architectural choices, e.g. switching from a rate-based network  to spiking , from feedforward  to recurrent , or single-to multi-compartment . Thus, beyond clarifying and categorizing the locality properties of normative plasticity models, our hope is that our framework will also support experimental efforts to differentiate between such models, helping the field to focus in on the most important predictions to test experimentally.

## 5 Acknowledgments

BR was supported by NSERC (Discovery Grant: RGPIN-2020-05105; Discovery Accelerator Supplement: RGPAS-2020-00031; Arthur B. McDonald Fellowship: 566355-2022), CIFAR (Canada AI Chair; Learning in Machine and Brains Fellowship), the Canada Research Chair in Neural Computations and Interfacing. GL was supported by NSERC (Discovery Grant: RGPIN-2018-04821), CIFAR (Canada AI Chair), and a Canada Research Chair in Neural Computations and Interfacing. CB and GL acknowledge support from a Simon's Collaboration on the Global Brain Pilot Award. EW was supported by an NSERC CGS-D scholarship. CS was supported by the National Science Foundation under NSF Award No. 1922658.