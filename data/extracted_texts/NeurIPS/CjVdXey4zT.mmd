# When Do Neural Nets Outperform Boosted Trees on Tabular Data?

Duncan McElfresh\({}^{*}\)\({}^{1,2}\), Sujay Khandagale\({}^{3}\), Jonathan Valverde\({}^{4}\), Vishak Prasad C\({}^{5}\),

**Ganesh Ramakrishnan\({}^{5}\), Micah Goldblum\({}^{6}\), Colin White\({}^{1,7}\)**

\({}^{1}\) Abacus.AI, \({}^{2}\) Stanford, \({}^{3}\) Pinterest, \({}^{4}\) University of Maryland,

\({}^{5}\) IIT Bombay, \({}^{6}\) New York University, \({}^{7}\) Caltech

Correspondence to: {duncan, colin}@abacus.ai. Work done while SK and JV were at Abacus.AI.

###### Abstract

Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and question the importance of this debate. To this end, we conduct the largest tabular data analysis to date, comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than choosing between NNs and GBDTs. Next, we analyze dozens of metafeatures to determine what _properties_ of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better than NNs at handling skewed or heavy-tailed feature distributions and other forms of dataset irregularities. Our insights act as a guide for practitioners to determine which techniques may work best on their dataset. Finally, with the goal of accelerating tabular data research, we release the TabZilla Benchmark Suite: a collection of the 36 'hardest' of the datasets we study. Our benchmark suite, codebase, and all raw results are available at https://github.com/naszilla/tabzilla.

## 1 Introduction

Tabular datasets are data organized into rows and columns, consisting of distinct features that are typically continuous, categorical, or ordinal. They are the oldest and among the most ubiquitous dataset types in machine learning in practice , due to their numerous applications across medicine , finance , online advertising , and many other areas .

Despite recent advances in neural network (NN) architectures for tabular data , there is still an active debate over whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with multiple works arguing either for  or against  NNs. This is in stark contrast to other areas such as computer vision and natural language understanding, in which NNs have far outpaced competing methods .

Nearly all prior studies of tabular data use fewer than 50 datasets or do not properly tune baselines , putting the generalizability of these findings into question. Furthermore, the bottom line of many prior works is to answer the question, 'which performs better, NNs or GBDTs, in terms of the average rank across datasets' without searching for more fine-grained insights.

In this work, we take a completely different approach by focusing on the following points. First, we question the importance of the 'NN vs. GBDT' debate, by investigating the significance of algorithm selection. Second, we analyze what _properties_ of a dataset make NNs or GBDTs better-suited to perform well. We take a data-driven approach to answering these questions, conducting the largest tabular data analysis to date, by comparing **19 algorithms each with up to 30 hyperparameter settings, across 176 datasets**, including datasets from the OpenML-CC18 suite  and the OpenML Benchmarking Suite . To assess performance differences across datasets, we consider dozens of metafeatures. We use 10 folds for each dataset to further reduce the uncertainty of our results.

We find that for a surprisingly high fraction of datasets, either a simple baseline (such as a decision tree or KNN) performs on par with the top algorithms; furthermore, for roughly one-third of all datasets, light hyperparameter tuning on CatBoost or ResNet increases performance more than choosing among GBDTs and NNs. These results show that for many tabular datasets, it is not necessary to try out many different NNs and GBDTs: **in many cases, a strong baseline or a well-tuned GBDT will suffice**. While NNs are the best approach for a non-negligible fraction of the datasets in this study, we do find that GBDTs outperform NNs on average over all datasets.

Next, we run analyses to discover what _properties_ of datasets explain which methods, or families of methods, do or do not succeed. We compute the correlations of various metafeatures with algorithm performance, and we demonstrate that these correlations are predictive. Our main findings are as follows (also see Figure 5): **dataset _regularity_ is predictive of NNs outperforming GBDTs** (for example, feature distributions that are less skewed and less heavy-tailed). Furthermore, GBDTs tend to perform better on larger datasets.

Finally, with the goal of accelerating tabular data research, we release the **TabZilla Benchmark Suite**: a collection of the 'hardest' of the 176 datasets we studied. We select datasets on which a simple baseline does not win, as well as datasets such that most algorithms do not reach top performance.

Our work provides a large set of tools for researchers and practitioners working on tabular data. We provide the largest open-source codebase of algorithms and datasets in one interface, together with a set of the 'hardest' datasets, and raw results (over 500K trained models) at https://github.com/naszilla/tabzilla for researchers and practitioners to more easily compare methods. Finally, our metafeature insights can be used by researchers, to uncover the failure modes of tabular algorithms, and by practitioners, to help determine which algorithms will perform well on a new dataset.

**Our contributions.** We summarize our main contributions below:

* We conduct the largest analysis of tabular data to date, comparing 19 methods on 176 datasets, with more than half a million models trained. We show that for a surprisingly high fraction of datasets, either a simple baseline performs the best, or light hyperparameter tuning of a GBDT is more important than choosing among NNs and GBDTs, suggesting that the 'NN vs. GBDT' debate is overemphasized.

Figure 1: Overview of our work. We start by conducting the largest study on tabular data to date (left); we analyze the importance of algorithm selection (‘NNs vs. GBDTs’) as well as metafeatures (middle); and based on our study, we release TabZilla, a collection of the hardest tabular datasets.

* After analyzing dozens of metafeatures, we present a number of insights into the properties that make a dataset better-suited for GBDTs or NNs.
* We release the TabZilla Suite: a collection of 36 'hard' datasets, with the goal of accelerating tabular data research. We open-source our benchmark suite, codebase, and raw results.

Related work.Tabular datasets are the oldest and among the most common dataset types in machine learning in practice [7; 55], due to their wide variety of applications [5; 10; 13; 36; 50]. GBDTs  iteratively build an ensemble of decision trees, with each new tree fitting the residual of the loss from the previous trees, using gradient descent to minimize the losses. XGBoost , LightGBM , and Catboost  are three widely-used, high-performing variants. Borisov et al. described three types of tabular data approaches for neural networks : data transformation methods [29; 64], architecture-based methods [4; 11; 28; 47] (including transformers [26; 34; 56]), and regularization-based methods [35; 37; 54]. Several recent works compare GBDTs to NNs on tabular data, often finding that _either_ NNs [4; 26; 37; 47]_or_ GBDTs [7; 26; 55] perform best.

Perhaps the most related work to ours is by Grinsztajn et al. , who investigate why tree-based methods outperform neural nets on tabular data. There are a few differences between their work and ours. First, they consider seven algorithms and 45 datasets, compared to our 19 algorithms and 176 datasets. Second, their dataset sizes range from 3 000 to 10 000, or seven that are exactly 50 000, in contrast to our dataset sizes which range from 32 to 1 025 009 (see Table 6). Additionally, they further control their study, for example by upper bounding the ratio of size to features, by removing high-cardinality categorical feature, and by removing low-cardinality numerical features. While this has the benefit of being a more controlled study, their analysis misses out on some of our observations, such as GBDTs performing better than NNs on 'irregular' datasets. Finally, while Grinsztajn et al. focused in depth on a few metafeatures such as dataset smoothness and number of uninformative features, our work considers orders of magnitude more metafeatures. Again, while each approach has its own strengths, our work is able to discover more potential insights, correlations, and takeaways for practitioners. To the best of our knowledge, the only related work has considered more than 50 datasets is TabPFN , which considered 179 datasets which are size 2 000 or smaller. See Appendix C for a longer discussion of related work.

## 2 Analysis of Algorithms for Tabular Data

In this section, we present a large-scale study of techniques for tabular data across a wide variety of datasets. Our analysis seeks to answer the following two questions.

1. How do algorithms (and algorithm families) compare across a wide variety of datasets?
2. What properties of a dataset are associated with algorithms (and families) outperforming others?

Algorithms and datasets implemented.We present results for 19 algorithms, including popular recent techniques and baselines. The methods include three GBDTs: CatBoost , LightGBM , and XGBoost ; 11 neural networks: DANet , FT-Transformer , two MLPs , NODE , ResNet , SAINT , STG , TabNet , TabPFN , and VIME ; and five baselines: Decision Tree , KNN , Logistic Regression , Random Forest , and SVM . We choose these algorithms because of their popularity, diversity, and strong performance.

We run the algorithms on 176 classification datasets from OpenML . Our aim is to include most classification datasets from popular recent papers that study tabular data [7; 26; 37; 55], including datasets from the OpenML-CC18 suite , the OpenML Benchmarking Suite , and additional OpenML datasets. Due to the scale of our experiments (538 650 total models trained), we limit the run-time for each experiment (described below), which precluded the use of datasets of size larger than 1.1M. Table 6 shows summary statistics for all datasets. CC-18 and OpenML Benchmarking Suite are both seen as the go-to standards for conducting a fair, diverse evaluation across algorithms due to their rigorous selection criteria and wide diversity of datasets [6; 25]. To the best of our knowledge, our 19 algorithms and 176 datasets are the largest number of _either_ algorithms _or_ datasets (with the exception of TabPFN ) considered by recent tabular dataset literature, and the largest number available in a single open-source repository.

Metafeatures.We extract metafeatures using the Python library PyMFE , which contains 965 metafeatures. The categories of metafeatures include: 'general' (such as number of datapoints, classes, or numeric/categorical features),'statistical' (such as the min, mean, or max skewness, or kurtosis, of all feature distributions), 'information theoretic' (such as the Shannon entropy of the target), 'landmarking' (the performance of a baseline such as 1-Nearest Neighbor on a subsample of the dataset), and'model-based' (summary statistics for some model fit on the data, such as number of leaf nodes in a decision tree model). Since some of these features have long-tailed distributions, we also include the log of each strictly-positive metafeature in our analysis.

Experimental design.For each dataset, we use the ten train/test folds provided by OpenML, which allows our results on the test folds to be compared with other works that used the same OpenML datasets. Since we also need validation splits in order to run hyperparameter tuning, we divide each training fold into a training and validation set. For each algorithm, and for each dataset split, we run the algorithm for up to 10 hours. During this time, we train and evaluate the algorithm with at most 30 hyperparameter sets (one default set and 29 random sets, using Optuna ). Each parameterized algorithm is given at most two hours on a 32GiB V100 to complete a single train/evaluation cycle. In line with prior work, our main metric of interest is _accuracy_, and we report the test performance of the hyperparameter setting that had the maximum performance on the validation set. We also consider log loss, which is highly correlated with accuracy but contains significantly fewer ties. We also include results for F1-score and ROC AUC in Appendix D. Similar to prior work , whenever we average across datasets, we use the average distance to the minimum (ADTM) metric, which consists of 0-1 scaling (after selecting the best hyperparameters, which helps protect against outliers ). Finally, in order to see the variance of each method on different folds of the same dataset, we report the average (scaled) standard deviation of each method across all 10 folds.

### Relative Algorithm Performance

In this section, we answer the question, "How do individual algorithms, and families of algorithms, perform across a wide variety of datasets?" We especially consider whether the difference between GBDTs and NNs is significant.

No individual algorithm dominates.We start by comparing the average rank of all algorithms across all datasets, while excluding datasets which ran into memory or timeout issues on a nontrivial number of algorithms using the experimental setup described above. Therefore, we consider a set

    &  &  &  &  \\ Algorithm & Class & min & max & mean & med. & mean & med. & mean & med. & mean & med. \\  CatBoost & GBDT & 1 & 17 & 5.06 & 4 & 0.89 & 0.94 & 0.31 & 0.22 & 30.27 & 2.22 \\ XGBoost & GBDT & 1 & 16 & 6.38 & 6 & 0.83 & 0.91 & 0.34 & 0.22 & 0.94 & 0.42 \\ ResNet & NN & 1 & 18 & 6.87 & 7 & 0.77 & 0.84 & 0.31 & 0.21 & 16.07 & 10.04 \\ SAINT & NN & 1 & 18 & 7.15 & 6 & 0.75 & 0.87 & 0.32 & 0.23 & 168.14 & 144.84 \\ NODE & NN & 1 & 18 & 7.35 & 7 & 0.75 & 0.85 & 0.27 & 0.19 & 146.89 & 118.94 \\ FT-Transformer & NN & 1 & 17 & 7.60 & 7 & 0.76 & 0.82 & 0.32 & 0.21 & 29.47 & 18.63 \\ RandomForest & base & 1 & 18 & 7.66 & 7 & 0.78 & 0.84 & 0.33 & 0.22 & 0.36 & 0.25 \\ LightGBM & GBDT & 1 & 18 & 7.80 & 8 & 0.78 & 0.85 & 0.37 & 0.22 & 1.06 & 0.37 \\ SVM & base & 1 & 17 & 8.34 & 9 & 0.70 & 0.80 & 0.27 & 0.20 & 29.22 & 1.37 \\ DANet & NN & 1 & 17 & 8.87 & 9 & 0.75 & 0.80 & 0.33 & 0.22 & 69.29 & 60.15 \\ MLP-rtdl & NN & 1 & 18 & 9.59 & 10 & 0.65 & 0.74 & 0.29 & 0.15 & 14.33 & 7.62 \\ STG & NN & 1 & 18 & 10.88 & 11 & 0.58 & 0.66 & 0.30 & 0.17 & 18.47 & 16.00 \\ DecisionTree & base & 1 & 18 & 10.91 & 12 & 0.61 & 0.68 & 0.36 & 0.25 & 0.03 & 0.01 \\ LinearModel & base & 1 & 18 & 11.31 & 13 & 0.53 & 0.58 & 0.32 & 0.24 & 0.04 & 0.03 \\ MLP & NN & 1 & 18 & 11.35 & 12 & 0.57 & 0.57 & 0.30 & 0.19 & 18.61 & 11.92 \\ TabNet & NN & 1 & 18 & 11.87 & 13 & 0.56 & 0.62 & 0.40 & 0.23 & 35.09 & 30.83 \\ KNN & base & 1 & 18 & 12.96 & 14 & 0.46 & 0.52 & 0.29 & 0.23 & 0.01 & 0.00 \\ VIME & NN & 2 & 18 & 14.27 & 16 & 0.36 & 0.32 & 0.27 & 0.17 & 17.10 & 14.97 \\   

Table 1: Performance of algorithms across 104 datasets (see Table 11 for extended results). Columns show the algorithm family (GBDT, NN, or baseline), rank over all datasets, the average normalized accuracy (Mean Acc.), the std. dev. of normalized accuracy across folds (Std. Acc.), and the train time in seconds per 1000 instances. Min/max/mean/median of these quantities are taken over all datasets.

of 104 datasets (and we include results on all 176 datasets in the next section and in Appendix D.2). As mentioned in the previous section, for each algorithm and dataset split, we report the test set performance after tuning on the validation set; see Table 1.

Surprisingly, _nearly every algorithm ranks first on at least one dataset and last on at least one other dataset_. As expected, baseline methods tend to perform poorly while neural nets and GBDTs tend to perform better on average. The fact that the best out of all algorithms, CatBoost, only achieved an average rank of 5.06, shows that there is not a single approach that dominates across most datasets. In Table 2, we compute the same table for the 57 datasets with size at most 1250 (training set at most 1000), so that we can include TabPFN  in our rankings. **We find that TabPFN achieves the best average performance of all algorithms, while also having the fastest training time.** However, with an average rank of 4.88, it still does not dominate all other approaches across different datasets. Furthermore, the _inference_ time for TabPFN is higher than other algorithms.

Performance vs. runtime.In Figure 2, we plot the accuracy vs. runtime for all algorithms, averaged across all datasets. Overall, neural nets require the longest runtime, and often outperform baseline methods. On the other hand, GBDTs simultaneously require little runtime while also achieving strong performance: they consistently outperform baseline methods, and consistently require less runtime than neural nets. There is one caveat: our experiments train each neural net for a pre-determined number of epochs (100 in most cases), with early stopping if there is no improvement for 20 epochs. It is possible that these neural nets can achieve strong performance with less runtime, e.g., with a more aggressive early-stopping criterion.

    &  &  &  &  \\ Algorithm & min & max & mean & med. & mean & med. & mean & med. & mean & med. \\  TabPFN & 1 & 18 & 4.88 & 3 & 0.84 & 0.93 & 0.35 & 0.26 & 0.00 & 0.00 \\ CatBoost & 1 & 18 & 5.37 & 4 & 0.85 & 0.91 & 0.39 & 0.30 & 26.22 & 2.75 \\ ResNet & 1 & 19 & 6.75 & 6 & 0.77 & 0.79 & 0.42 & 0.30 & 23.67 & 13.87 \\ RandomForest & 1 & 18 & 7.65 & 7 & 0.76 & 0.82 & 0.40 & 0.29 & 0.47 & 0.32 \\ SAINT & 1 & 19 & 7.67 & 6 & 0.74 & 0.87 & 0.42 & 0.31 & 197.41 & 181.62 \\ FTTransformer & 1 & 18 & 7.93 & 7 & 0.75 & 0.78 & 0.42 & 0.32 & 32.93 & 26.39 \\ XGBoost & 1 & 17 & 8.30 & 8 & 0.74 & 0.80 & 0.42 & 0.30 & 0.95 & 0.61 \\ NODE & 1 & 19 & 8.35 & 8 & 0.73 & 0.75 & 0.36 & 0.28 & 173.55 & 144.45 \\ SVM & 1 & 18 & 9.54 & 11 & 0.68 & 0.72 & 0.35 & 0.28 & 23.90 & 0.42 \\ MLP-rtdl & 1 & 19 & 9.77 & 10 & 0.64 & 0.69 & 0.39 & 0.31 & 21.48 & 12.21 \\ LightGBM & 1 & 19 & 10.00 & 10 & 0.68 & 0.71 & 0.45 & 0.38 & 0.64 & 0.23 \\ LinearModel & 1 & 19 & 10.21 & 11 & 0.61 & 0.71 & 0.38 & 0.29 & 0.06 & 0.05 \\ DANet & 1 & 18 & 10.74 & 10 & 0.68 & 0.69 & 0.41 & 0.34 & 83.57 & 71.19 \\ DecisionTree & 1 & 19 & 11.44 & 13 & 0.60 & 0.67 & 0.45 & 0.32 & 0.02 & 0.01 \\ MLP & 1 & 19 & 11.49 & 13 & 0.57 & 0.54 & 0.38 & 0.30 & 27.88 & 16.81 \\ STG & 1 & 19 & 11.49 & 12 & 0.57 & 0.64 & 0.40 & 0.34 & 21.22 & 18.24 \\ KNN & 1 & 19 & 13.12 & 15 & 0.46 & 0.51 & 0.38 & 0.32 & 0.00 & 0.00 \\ TabNet & 3 & 19 & 14.54 & 16 & 0.42 & 0.40 & 0.52 & 0.49 & 41.83 & 34.35 \\ VIME & 2 & 19 & 14.88 & 17 & 0.33 & 0.27 & 0.36 & 0.29 & 18.95 & 16.43 \\   

Table 2: Performance of algorithms across 57 datasets of size less than or equal to 1250. Columns show the rank over all datasets, the average normalized accuracy (Mean Acc.), the standard deviation of normalized accuracy across folds (Std. Acc), and the train time per 1000 instances. Min/max/mean/median are taken over all datasets.

Figure 2: Median runtime vs. median normalized accuracy for each algorithm, over 104 datasets. The bars span the 20th to 80th percentile over all datasets.

Statistically significant performance differences.Table 1 shows that many algorithms have similar performance. Next, we determine _statistically significant_ (\(p<0.05\)) performance differences between algorithms across the 104 datasets described above. First, we use a Friedman test to determine whether performance differences between each algorithm are significant ; we can reject the null hypothesis (p\(<0.05\)) for this test; the p-value is less than \(10^{-20}\). We then use a Wilcoxon signed-rank test to determine which pairs of algorithms have significant performance differences (p\(<\)\(0.05\)) . With the Wilcoxon tests we use a Holm-Bonferroni correction to account for multiple comparisons . Due to the presence of many ties in the test accuracy metric, we use the test log loss metric. See Figure 3 (and see Figure 7 for the F1 score). In these figures, the average rank of each algorithm is shown on the horizontal axis; if differences between algorithms are _not significant_ (p\( 0.05\)), then algorithms are shown connected by a horizontal bar. We find that CatBoost outperforms all other algorithms across 104 datasets.

GBDTs vs. NNs.Although Table 11 tells us which _individual_ methods perform best on average, now we consider the age-old question, 'are GBDTs better than NNs for tabular data?' We split the 19 algorithms into three _families_: GBDTs (CatBoost, XGBoost, LightGBM), NNs (the 11 listed above), and baselines (Decision Tree, KNN, LinearModel, RandomForest, SVM). We say that an

Figure 4: Left: Venn diagram of the number datasets where each algorithm is ‘high-performing’ for each algorithm class, over all 176 datasets. An algorithm is high-performing if its test accuracy after 0-1 scaling is at least 0.99 (we show 0.9999 in Appendix D.2). Right: the performance improvement of hyperparameter tuning on CatBoost, compared to the absolute performance difference between the best neural net and the best GBDT using default hyperparameters. Each point indicates the normalized log loss of one dataset, Points on or below the dotted line indicate that the performance improvement due to tuning is as high as the difference between NN-GBDT algorithm selection.

Figure 3: Critical difference plot comparing all algorithms according to their mean log loss rank over 104 datasets. Each algorithm’s average rank is shown as a horizontal line on the axis. Sets of algorithms which are _not significantly different_ are connected by a horizontal black bar. Algorithm family is indicated by a marker next to the algorithm name: red “X” indicates a neural net, blue circle indicates a baseline algorithm, and green triangles indicate GBDTs.

algorithm is 'high-performing' if it achieves a 0-1 scaled test accuracy of at least \(0.99\), and then we determine which algorithm families (GBDTs, NNs, baselines) have a high-performing algorithm; see Figure 4. Surprisingly, the three-way Venn diagram is relatively balanced among GBDTs, NNs, and baselines, although GBDTs overall have the edge. In Appendix D.2, we run the same analysis, using a threshold of \(0.9999\). In this case, GBDTs are the sole high-performing algorithm family for most datasets. Since these wins are by less than 0.01%, they may not be significant to practitioners.

Algorithm selection vs. tuning.Next, we determine whether it is more important to select the best possible algorithm family, or to simply run light hyperparameter tuning on an algorithm that performs well in general, such as CatBoost or ResNet. We consider a scenario in which a practitioner can decide to _(a)_ test several algorithms using their default hyperparameters, or _(b)_ optimize the hyperparameters of a single model, such as CatBoost or ResNet. We compute whether _(a)_ or _(b)_ leads to better performance. Specifically, we measure the performance difference between the best-performing GBDT and NN using their default hyperparameters, as well as the performance difference between CatBoost with the default hyperparameters vs. CatBoost tuned via 30 iterations of random search on a validation set; see Figure 4 (right), and see Appendix D.2 for the same analysis with ResNet. Surprisingly, light hyperparameter tuning yields a greater performance improvement than GBDT-vs-NN selection for about one-third of all datasets. Once again, this suggests that for a large fraction of datasets, it is not necessary to determine whether GBDTs or NNs are better: light tuning on an algorithm such as CatBoost or ResNet can give just as much performance gain. In the next section, we explore _why_ a dataset might be more amenable to a neural net or a GBDT.

### Metafeature Analysis

In this section, we answer the question, "What properties of a dataset are associated with certain techniques, or families of techniques, outperforming others?" We answer this question by computing the correlation of metafeatures with three different quantities related to the performance difference between algorithm families, the performance difference between pairs of algorithms, and the relative performance of individual algorithms.

In order to assess the difference in performance between NNs and GBDTs, we calculate the difference in normalized log loss between the best NN and the best GBDT, which we refer to as \(_{}\). We compute the correlation of \(_{}\) to various metafeatures across all datasets; see Figure 12 and Table 14 in Appendix D.3. Next, in order to determine the individual strengths and weaknesses of each algorithm, we compute the correlation of various metafeatures to the performance of an individual algorithm relative to all other algorithms; see Table 3 and Figure 5. Finally, we compute the correlation of metafeatures to the difference in performance between pairs of the top-performing algorithms from Section 2.1: CatBoost, XGBoost, SAINT, and ResNet. See Figure 5 and Table 19.

In order to show that these metafeatures are _predictive_, we train and evaluate a meta-learning model using a leave-one-out approach: one dataset is held out for testing, while the remaining 175 datasets are used for training, averaged across all 176 possible test sets; see Appendix D.3. In the rest of this section, we state and discuss the main findings of our metafeature analysis.

Neural nets perform comparatively worse on larger datasets.Throughout our metafeature analyses, we find that GBDTs perform comparatively better than NNs and baselines with larger datasets. Figure 5 shows that XGBoost achieves top performance compared to all 19 algorithms on the seven largest datasets, and GBDTs overall perform well. In Table 3, somewhat surprisingly, dataset size is the most negatively-correlated metafeature with the relative performance of both LightGBM and XGBoost. Finally, Table 14 shows that the GBDT family's performance is also positively correlated with the _ratio_ of size to the number of features. Notably, all of these analyses are relative to the performance of all algorithms, which includes the newly released TabPFN , a neural net that performs remarkably well on datasets of small size, due to its carefully-designed prior. On the other hand, GBDTs excel when the ratio of dataset size to number of features is high, because all split in the decision trees are computed using more datapoints. Some of our above findings are backed up by prior work, for example, Grinsztajn et al.  showed that increasing the ratio of (uninformative) features to dataset size, hurts the performance of ResNet, and our results indicate the same trend (Table 20). On the other hand, NNs as a whole see the opposite trend (Table 14), which at least is shown for FTTransformer in Grinsztajn et al. . We provide additional analyses in Appendix D.2.6. Note that, while the general trend shows GBDTs outperforming NNs on largerdatasets, this does not imply that all GBDTs are better than all NNs on larger datasets. For example, TabPFN and TabNet are both neural nets, yet TabPFN performs particularly well on smaller datasets, and TabNet performs particularly well on larger datasets. **It is important to note that when choosing an algorithm for a new use-case, practitioners should focus on algorithm-specific analyses (such as in Table 1, Table 2, and Appendix D.2.6) rather than general 'GBDT vs. NN' trends.**

GBDTs favor irregular datasets.Another trend present throughout all three metafeatures analyses is that GBDTs consistently favor 'irregular' datasets. When comparing pairs of the top GBDTs and NNs, we find that both CatBoost and XGBoost outperform ResNet and SAINT on datasets whose feature distributions are heavy-tailed, skewed, or have high variance (see Table 19 for the full details). That is, some datasets' feature distributions all have a similar amount of skewness, while other datasets' feature distributions are more irregular, with a high range of skewness. It is the latter type of datasets on which GBDTs outperform NNs. We also find that GBDTs perform better when datasets are more class imbalanced (on SAINT in particular). In Figure 5, GBDTs perform best on the most irregular datasets, computed via a linear combination of five metafeatures each measuring the skewness or kurtosis of the feature distributions.

The bottom line.Overall, we answer the title question of our paper: GBDTs outperform NNs on datasets that are more 'irregular', as well as large datasets, and datasets with a high ratio of size to number of features. When a practitioner is faced with a new dataset, based on all the analysis in Section 2, we give the following recommendation: first try simple baselines, and then conduct light hyperparameter tuning on CatBoost. Surprisingly, this often will already result in strong performance. As a next step, the practitioner can try NNs and other GBDTs that are most-correlated with strong performance based on the dataset's metafeatures, using analyses such as Table 1 and Appendix D.2.6.

   Alg. & Metafeature Description & Corr. \\  CatBoost & Noisiness of the features: \(\{_{i}S_{i}-_{i}MI(i,y)\}/_{i}MI(i,y)\), where \(S_{i}\) is the entropy & [0.25, 0.34] \\  & of feature \(i\), and \(MI(i,y)\) is the mutual information between feature \(i\) and the target \(y\). & \\  XGBoost & Log. number of instances. & [-0.27, -0.18] \\  LightGBM & Log. number of instances. & [-0.36, -0.28] \\  ResNet & Mean canonical correlation between any numeric feature and the target. & [-0.28, -0.19] \\  FTTransformer & Number of target classes. & [0.23, 0.32] \\  SAINT & Noisiness of the features. & [0.19, 0.29] \\   

Table 3: Metafeatures that are most correlated with the performance of each top-performing algorithm, calculated as the Pearson correlation between the metafeature and normalized log loss over 176 datasets. Metafeatures with the largest absolute Pearson correlation with algorithm performance are shown, and correlations are presented as 95% confidence intervals, calculated using the Fisher transformation.

Figure 5: Left: scatterplot of the best algorithm on all 176 datasets across metafeatures. The vertical axis indicates the dataset size, and the horizontal axis combines five dataset metafeatures related to irregularity. Right: scatterplot of the difference in normalized log loss between XGBoost and ResNet, by dataset size (middle subplot) and irregularity (right subplot). The irregularity feature is a linear combination of five standardized dataset attributes: the minimum eigenvalue of the feature covariance matrix (-0.33), the skewness of the standard deviation of all features (0.23), the skewness of the range of all features (0.22), the interquartile range of the harmonic mean of all features (0.21), and the standard deviation of the kurtosis of all features (0.21).

## 3 TabZilla Benchmark Suite

In order to accelerate tabular data research, we release the TabZilla Benchmark Suite: a collection of the 36 'hardest' of the 176 datasets we studied in Section 2. We use the following three criteria.

Hard for baseline algorithms.As discussed in Section 2.1, simple baselines perform very well on a surprisingly large fraction of the datasets in our experiments. Therefore, to select our suite of hard datasets, we remove any dataset such that a baseline (as defined in Section 2) achieved a normalized log loss within 20% of the top-performing algorithm. This criterion is not perfect; for example, if a dataset is so hard that all 19 algorithms fail to reach non-trivial performance, it would not satisfy the criterion. However, the criterion is a good proxy for dataset hardness given the available information.

Hard for most algorithms.This criterion is designed to include datasets on which most algorithms were not able to reach top performance. In particular, a dataset satisfies the criterion if the fourth-best log loss out of 19 algorithms is at least 7% worse than the top log loss. In other words, this criterion will include datasets on which one, two, or three algorithms were able to stand out in terms of performance. For example, if ten algorithms were all able to achieve a performance within 7% of the top-performing algorithm, we can reasonably assume that the dataset might not be 'hard'. Interestingly, this criterion subsumes the majority of the datasets from the previous criterion.

Hard for GBDTs.The first two criteria result in datasets such that GBDTs primarily are the top-performing methods. This is not surprising in light of the overall performance of GBDTs that we showed in Section 2. However, for the field of tabular data to progress, focusing on datasets for which GBDTs already perform well would leave a blindspot on datasets for which GBDTs perform poorly. Therefore, we add all datasets for which GBDTs perform 10% worse than the top-performing algorithm, in order to achieve a greater diversity of datasets.

    &  &  &  \\  Dataset & base &  & GBDT & \(N\) \# &  & Std. Kurtosis &  &  &  \\  credit-g & **0.26** & **0.13** & **0.12** & 1 0 00 & 21 & 1.92 & ResNet & FTTransformer & CatBoost \\ jungle-chess & **0.30** & **0.18** & **0.17** & 44 81 & 7 & 0.08 & SAINT & TaNet & LightGBM \\ MiniBoONE & **0.20** & **0.09** & 0.00 & 130 064 & 51 & 1216.5 & LightGBM & XGBoost & ResNet \\ albert & **0.42** & **0.28** & 0.00 & 425 240 & 79 & 1686.90 & CatBoost & XGBoost & ResNet \\ electricity & **0.46** & **0.38** & 0.00 & 45 312 & 9 & 2696.51 & LightGBM & XGBoost & FTTransformer \\ elevators & **0.36** & **0.08** & 0.05 & 16 599 & 12986.50 & TaNet & XGBoost & CatBoost \\ guillermo & **0.35** & **0.60** & 0.00 & 20 000 & 4 & 297 & NaN & XGBoost & RandomForest & TaNet \\ higgs & **0.41** & **0.10** & 0.07 & 98 050 & 29 & 15.53 & ResNet & XGBoost & LightGBM \\ normo & **0.22** & **0.18** & 0.00 & 34 465 & 119 & 1100.34 & LightGBM & XGBoost & CatBoost \\
100-plants-texture & **0.20** & **0.11** & 0.00 & 1 599 & 65 & 17.66 & CatBoost & XGBoost & ResNet \\ poker-hand & **0.58** & **0.98** & 0.00 & 102 05 09 & 11 & 0.08 & XGBoost & CatBoost & KNN \\ probub & **0.39** & **0.38** & 0.00 & 672 & 10 & 0.95 & CatBoost & DeepFM & MLP-rdl \\ socomb & **0.24** & **0.10** & 0.00 & 1 156 & 6 & NaN & XGBoost & CatBoost & ResNet \\ mediology & **0.43** & 0.03 & 0.00 & 226 & 70 & NaN & STG & XGBoost & ResNet \\ splice & **0.30** & 0.03 & 0.00 & 3 190 & 61 & NaN & LightGBM & XGBoost & CatBoost \\ vehicle & 0.05 & **0.10** & **0.10** & 846 & 19 & 15.16 & TaPhFN & SVM & DANet \\ Australian & 0.15 & **0.08** & 0.00 & 690 & 15 & 2.00 & CatBoost & XGBoost & TaPhFN \\ Birospone & 0.07 & **0.07** & 0.00 & 3 751 & 1 777 & 328.77 & TaPhGBM & XGBoost & CatBoost \\ GesturePhase & 0.08 & **0.08** & 0.00 & 9 872 & 33 & 52.18 & LightGBM & XGBoost & CatBoost \\ SpeedDating & 0.18 & **0.14** & 0.00 & 8 378 & 121 & 36.43 & XGBoost & CatBoost & LightGBM \\ ada-agnostic & 0.12 & **0.11** & 0.00 & 4 562 & 49 & NaN & XGBoost & CatBoost & LightGBM \\ airlines & **0.20** & **0.18** & 0.00 & 539 382 & 8 & 2.01 & LightGBM & XGBoost & CatBoost \\ artificial-characters & 0.13 & **0.11** & 0.00 & 10 218 & 8 & 0.63 & XGBoost & LightGBM & CatBoost \\ colic & 0.13 & **0.11** & 0.00 & 368 & 27 & 4.00 & CatBoost & XGBoost & FTTransformer \\ credit-approval & 0.12 & **0.08** & 0.00 & 690 & 16 & 74.77 & CatBoost & TaPhFN & XGBoost \\ heart-h & 0.10 & **0.07** & 0.08 & 294 & 14 & NaN & DeepFM & TaTransformer & NAH \\ jaamine & 0.13 & **0.13** & 0.00 & 2 984 & 145 & 47.60 & CatBoost & XGBoost & LightGBM \\ kcl & 0.14 & **0.07** & 0.00 & 2 109 & 22 & 28.34 & CatBoost & XGBoost & FFTransformer \\ lymph & 0.14 & **0.08** & 0.00 & 148 & 19 & 17.04 & XGBoost & DANet & SAINT & STG \\ mfeat-fourier & 0.00 & **0.07** & 0.07 & 2 000 & 77 & 0.64 & SVM & SAINT & STG \\ phoneme & 0.10 & **0.15** & 0.00 & 5 404 & 6 & 1.23 & XGBoost & LightGBM & RandomForest \\ qasar-bideg & 0.08 & **0.08** & 0.05 & 1 055 & 42 & 93.24 & TaPhFN & CaBoost & SAINT \\ balance-scale & 0.07 & 0.05 & **0.16** & 625 & 5 & 0.02 & TaPhFN & SAINT & MLP \\ cane-9 & 0.11 & 0.04 & **0.10** & 10 080 & 857 & NaN & TaPhTransformer & STG & MLP-rdl \\ mfeat-zernike & 0.00 & 0.04 & **0.10** & 2 000 & 48 & 1.42 & SVM & DANet & ResNet \\ monoks-problems-2 & 0.04 & 0.00 & **0.17** & 601 & 7 & NaN & SAINT & ResNet & MLP-rdl \\   

Table 4: The TabZilla Benchmark Suite. Columns show the hardness metrics used as selection criteria, dataset attributes, and the top-performing algorithms. ‘Std. Kurtosis’ indicates the std. dev. of the kurtosis of all features. Hardness metrics that meet our selection criteria are shown in bold.

TabZilla Characteristics.Table 4 shows all datasets, their statistics, and their top three algorithms. Based on the criteria alone, the dataset characteristics are diverse, with sizes ranging from 148 to over 1 million, as well as a large range of the variance of kurtosis of the features (one of our measures of irregularity). In Table 5, we compare the performance of all algorithms on the benchmark suite. The top five algorithms are XGBoost, CatBoost, LightGBM, ResNet, and SAINT, with mean ranks of 3.27, 3.86, 6.06, 6.14, and 6.37, respectively. In order to accelerate research in tabular data, we release TabZilla as a collection in OpenML, and we open-source all of our computed metafeatures and results. In Appendix B, we give the full dataset documentation, including a datasheet .

## 4 Conclusions and Future Work

In this work, we conducted the largest tabular data analysis to date, by comparing 19 approaches across 176 datasets. We found that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either a simple baseline method performs on par with all other methods, or light hyperparameter tuning on a GBDT increases performance more than choosing the best algorithm. On the other hand, on average, GBDTs do outperform NNs. We also analyzed what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, GBDTs are better than NNs at handling various types of data irregularity. Finally, based on our analysis, we released TabZilla, a collection of the 36 'hardest' out of the 176 datasets we studied: hard for baselines, most algorithms, and GBDTs. The goal in releasing TabZilla is to accelerate tabular data research by focusing on improving the current blind spots in the literature.

Our work provides a large set of tools to accelerate research on tabular data. For example, researchers developing a new neural net for tabular data can use our open-source repository to immediately compare their method to 19 algorithms across 176 datasets. Researchers can also use our metafeature analysis to improve the weaknesses of current or future algorithms; for example, making neural nets more robust to data irregularities is a natural next step. Furthermore, researchers studying ensemble methods can weight the models differently for each dataset based on its metafeatures. Finally, our open-source collection of extensive datasets and metafeatures can make it easier for researchers to design new meta-learned  or pre-trained models for tabular data [31; 41; 65]. There are several interesting ideas for extensions such as regression datasets, time-series forecasting datasets, studying uncertainty quantification, studying the effect of the percentage of categorical features on NNs, and studying more comprehensive hyperparameter optimization including regularization methods.

    &  &  &  &  \\ Algorithm & min & max & mean & med. & mean & med. & mean & med. & mean & med. \\  XGBoost & 1 & 14 & 3.27 & 2.0 & 0.06 & 0.03 & 0.11 & 0.06 & 2.25 & 0.28 \\ CatBoost & 1 & 12 & 3.86 & 3.0 & 0.09 & 0.06 & 0.11 & 0.07 & 26.46 & 1.15 \\ LightGBM & 1 & 14 & 6.06 & 5.5 & 0.13 & 0.09 & 0.20 & 0.09 & 1.23 & 0.36 \\ ResNet & 1 & 13 & 6.14 & 5.5 & 0.19 & 0.17 & 0.12 & 0.07 & 8.24 & 5.30 \\ SAINT & 1 & 17 & 6.37 & 5.0 & 0.20 & 0.15 & 0.12 & 0.08 & 130.18 & 92.42 \\ DANet & 2 & 15 & 7.26 & 7.0 & 0.18 & 0.16 & 0.15 & 0.11 & 58.70 & 52.74 \\ FTTransformer & 2 & 14 & 7.66 & 7.0 & 0.25 & 0.20 & 0.13 & 0.12 & 17.41 & 12.64 \\ RandomForest & 2 & 16 & 8.67 & 8.0 & 0.29 & 0.28 & 0.22 & 0.08 & 0.42 & 0.25 \\ MLP-rtdl & 2 & 18 & 8.76 & 7.0 & 0.38 & 0.26 & 0.17 & 0.13 & 6.30 & 4.40 \\ SVM & 1 & 17 & 9.17 & 9.0 & 0.29 & 0.22 & 0.14 & 0.08 & 19.73 & 2.81 \\ STG & 1 & 17 & 9.28 & 10.0 & 0.32 & 0.24 & 0.11 & 0.06 & 15.97 & 15.33 \\ TabNet & 1 & 18 & 10.12 & 9.5 & 0.37 & 0.28 & 0.27 & 0.12 & 26.39 & 27.00 \\ LinearModel & 3 & 18 & 10.57 & 11.0 & 0.47 & 0.38 & 0.11 & 0.06 & 0.04 & 0.02 \\ MLP & 2 & 17 & 10.62 & 10.0 & 0.46 & 0.40 & 0.15 & 0.13 & 8.73 & 4.28 \\ NODE & 5 & 16 & 10.63 & 10.0 & 0.38 & 0.35 & 0.08 & 0.07 & 153.72 & 124.27 \\ VIME & 4 & 17 & 13.00 & 14.0 & 0.51 & 0.48 & 0.09 & 0.08 & 20.79 & 15.18 \\ DecisionTree & 5 & 18 & 13.19 & 14.5 & 0.61 & 0.61 & 0.38 & 0.21 & 0.20 & 0.01 \\ KNN & 3 & 18 & 14.55 & 16.0 & 0.70 & 0.74 & 0.36 & 0.21 & 0.03 & 0.00 \\   

Table 5: Performance of algorithms across the Tabular Benchmark Suite of 36 datasets. Columns show the rank over all datasets, the average normalized accuracy (Mean Acc.), the standard deviation of normalized accuracy across folds (Std. Acc.), and the train time per 1000 instances. Min/max/mean/median of these quantities are taken over all datasets.