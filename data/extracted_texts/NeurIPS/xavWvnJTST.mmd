# Feedback control guides credit assignment in recurrent neural networks

Klara Kaleb

Department of Bioengineering

Imperial College London

London, UK

klara.kaleb18@imperial.ac.uk

&Barbara Feulner

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Claudia Clopath

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Claudia Clopath

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering
Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering
Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London
London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering
Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London London

London, UK

&Juan A. Gallego

Department of Bioengineering

Imperial College London

London, UK

&Juan A. Gallego

Department of Bioengineering
Imperial College London
and exploding gradient problem (Pascanu et al., 2013; Hochreiter and Schmidhuber, 1997). This challenge is exacerbated under biological constraints, which require learning rules with significant approximations of the full temporal memory (Marschall et al., 2020; Bellec et al., 2020; Murray, 2019; Liu et al., 2021), and often lead to an undesirable generalization gap (Liu et al., 2022).

In addition to intra-layer recurrence, the brain also features extensive feedback connections from top-down sources (Peters and Payne, 1993; Peters et al., 1994) that can modulate network activity in lower layers (Przybyszewski, 1998; Girard et al., 2001; De Pasquale and Sherman, 2011; Briggs and Usrey, 2011; Jordan and Keller, 2020; Keller et al., 2020) and improve bottom-up information processing (Gilbert and Li, 2013; Manita et al., 2015; Fyall et al., 2017). This multiplicity of simultaneously active pathways contrasts vanilla RNNs, where feedback connections are used exclusively for gradient propagation.

It is precisely this simultaneous integration of several streams of information that is thought to mediate ongoing movement correction in both humans and non-human primates (Thoroughman and Shadmehr, 2000; Perich et al., 2018; Krakauer et al., 2000). For example, when subjects receive distorted feedback during a learned reaching movement, such as an unexpected force acting on the limb, they are able to make movement corrections already at the first trial (Thoroughman and Shadmehr, 2000). Moreover, they rapidly adapt to the mismatch between the observed and expected movement trajectory upon further perturbed trials, requiring no more online corrections (Krakauer et al., 2000). Recent data-driven modelling work (Feulner et al., 2022) proposed an elegant solution to this problem by integrating feedback control into the recurrent network dynamics, and pre-training the whole network on a stereotypical motor task. The resulting networks showed rapid adaptation to task perturbation during ongoing movement and further improvement in the task performance upon persistent perturbation using a _feedback-driven, local learning rule_, and thus reproduce key behavioural and neurophysiological findings. However, the relationship between this local feedback-driven learning and gradient descent, or any of its approximations, remains unclear.

In this work, we investigate the mechanistic properties of such recurrent networks pre-trained with feedback control on a stereotypical motor task. Our key findings are listed as follows:

1. Feedback control allows for approximate learning in the activity space 3.2.
2. Feedback control enables increased accuracy of approximate, local learning rules in the recurrent layer due to the "decoupling" of the network from its past activity 3.4.
3. Feedback control enables more efficient weight updates during task adaptation due to the implicit incorporation of adaptive, second-order gradient into the network dynamics 3.5.

## Related work

In most modelling studies, the role of feedback connections is restricted to gradient signal propagation only (Bellec et al., 2020; Murray, 2019; Payeur et al., 2021; Liu et al., 2021). The need for separation seems clear: in the studies where this restriction is lifted, the plasticity mechanisms involved often require tight coordination (Scellier and Bengio, 2017; Whittington and Bogacz, 2017; Sacramento et al., 2018; Payeur et al., 2021; Podlaski and Machens, 2020) that may be hard to implement and maintain in the brain. Furthermore, the relative influence of feedback may still be kept small to avoid interference with the forward inference.

Nevertheless, several other computational studies have shown that a stronger influence of feedback on the network activities may actually aid network training (Gilra and Gerstner, 2017; Deneve et al., 2017; Alemi et al., 2018; Bourdoukan and Deneve, 2015; Meulemans et al., 2021, 2022, 2020). For example, Meulemans et al. (2020) formally characterize the link between previously proposed Target Propagation (Bengio, 2014; Lee et al., 2015) to more efficient learning with second-order optimization methods (Gauss, 187). We also highlight Meulemans et al. (2022), where the authors formally derive the relationship between gradient-based learning and optimal control through feedback activity influence for equilibrium systems. However, despite the success on a number of difficult problems, it remains unknown how well these findings extend to out-of-equilibrium dynamics commonly found in the brain. In contrast, Feulner et al. (2022) takes a more agnostic approach by pre-training recurrent networks with feedback control of their output out-of-equilibrium and under non-stationary conditions. However, while the resulting networks and the local learning algorithm reproduce key behaviouraland neurophysiological findings from both humans and non-human primates, their relationship with gradient descent remains unclear.

## 2 Methods

### Training stages

In this work, we used a modelling setup built to reproduce motor adaption experiments in monkeys (Perich et al., 2018) and similar to that used in Feulner et al. (2022) (see Appendix A.6 for key differences). We construct a model that mimics both the movement repertoire and the flexibility of the motor cortex by applying a two-stage training strategy: pre-training followed by fine-tuning.

Pre-training.To obtain stereotypical motor movement dynamics, we first train the recurrent networks with feedback control on a synthetic _instructed delay centre-out reaching_ task using BPTT (Werbos, 1990). Here, the task is to produce a broad set of two-dimensional reaching velocity trajectories of varying lengths, following an instructed delay phase (see Section A.1 for more details). A visualization of a sub-task with 8 equidistant targets can be seen in Figure 0(a)-0.

Fine-tuning.To probe the recurrent network's ability to adapt to task perturbations following initial pre-training, we replicate a classic visuomotor (VR) perturbation paradigm (Perich et al., 2018; Krakauer et al., 2000) in which the model output is rotated around the centre by a certain fixed angle. Motivated by progressive adaptations to such persistent perturbations in the experimental studies (Perich et al., 2018), in our work perturbation fine-tuning occurs through a local, biologically plausible learning rule.

### Network architecture

We train the recurrent neural networks with a single recurrent hidden layer whose activity is described by the dynamics in Equation 1. Here, a hidden layer unit \(h_{j}\) receives the stimulus signal \(s^{t}\), the previous network activity post-nonlinearity \((h_{i})\) and the positional error \(_{i}\) from the previous time step i.e. (\(t-1\)).

\[}=(-h_{j}+_{i}W_{ji}^{in}s_{i}+_{i}W_{ji}^{h}(h _{i})+_{k}W_{ji}^{fb}_{i}+b_{j}^{h})\] (1)

In Equation 1 \(\) represents the ReLU function, and \(^{fb}\) are the backward projecting feedback weights. The 2D output velocity \(\) of the recurrent network is obtained through a linear readout of its hidden layer activity post-nonlinearity, where \(a_{j}=(h_{j})\):

\[v_{k}=W_{kj}^{o}a_{j}+b_{k}^{o}\] (2)

### Pre-training procedure

We pre-train \(^{in},^{h},^{fb},^{o},^{h}\) and \(^{o}\) using BPTT (Werbos, 1990) with mean squared error (MSE) over the integrated position space using Adam (Kingma & Ba, 2014). To encourage the networks to learn to use the feedback layer to correct their output online, we use randomly distributed velocity perturbations (see Section A.3 for more details). The 2D error feedback signal \(_{k}\) projected back to the network with feedback weights \(^{fb}\) is the derivative of the MSE of the integrated position \(p_{k}^{t}=p_{k}^{t-1}+dt v_{k}^{t}\), i.e. \(_{k}^{t}=p_{k}^{t*}-p_{k}^{t}\), where \(p_{k}^{t*}\) is the task target.

### Adaptation and feedback-driven plasticity rule

To assess how the feedback integration may enable biologically plausible learning in the hidden recurrent layer during persistent task perturbation, we implement a previously proposed, temporally and spatially local learning rule, Random Feedback Local Online (RFLO) learning (Murray, 2019).

\[} =(-B_{ji}+^{}(h_{j})a_{i}^{prev})\] (3) \[W_{ji}^{h} =_{2}W_{jk}^{fb}_{k}B_{ji}\] (4)

where \(_{2}\) is the adaptation learning rate and \(a_{i}^{prev}\) is hidden activity from the previous timestep i.e. \((t-1)\). Thus, the change in weights is proportional to the eligibility trace \(B_{ji}\) and the feedback signal received \(W_{jk}^{fb}_{k}\). Note that when the same weight matrix \(}\) is used for both control and learning, we denote this by adding \(+c\) to the respective learning rule. For further biological realism, the learning batch size is fixed to 1 and the weight updates are applied online i.e. after every presented example.

## 3 Results

Recurrent neural networks with feedback control adapt well to task perturbation using feedback control and local learning rules

First, we pre-train the networks with feedback control on the _instructed delay center-out-reaching_ task, where the networks need to produce 2D sigmoidal velocity profiles based on the target and the _go_ signal input given (see Section 2 for details). Once trained, we test their ability to adapt to task perturbations using the visuo-motor rotation (VR) paradigm. Here, we rotate the target positions by 30\({}^{}\) around the centre axis, creating a mismatch between the learned relationship between the input and the target. As previously shown in Feulner et al. (2022), the networks with feedback control show online adaptation to such a perturbation during ongoing movement in the first trial (Figure 0(a)). With persistent perturbation, the networks with feedback control show further improvement in the task performance, where the adapted network readout is independent of feedback control (Figure 0(b)).

Feedback control approximates the true gradient w.r.t. networks activations during task perturbation

What happens during task perturbation in the networks with feedback control that allows for such rapid adaptation? In Figure 1(a), we show in networks with feedback control, the feedback contribution to the overall network output increases during perturbation (see A.7 for full analysis details). Thus, during the task perturbation, the network activity is increasingly driven by the feedback signal. The average magnitude of this drive is proportional to the movement perturbation magnitude (Figure 1(b)), as the feedback drive is used to correct the recurrent network activity during perturbed trials. To further understand the mechanism behind this rapid movement correction, we investigate the alignment of the online feedback signal \(W^{fb}^{t-1}\) with the true first-order gradient w.r.t. the network

Figure 1: **Recurrent neural networks with feedback control feature rapid adaptation to _acute_ task perturbation and can learn the _persistent_ task perturbation using a local learning rule. A recurrent neural network with feedback control is first pre-trained on produce a 2D sigmoidal velocity profile based on a target and _go_ cue input input, and then tested on a 30\({}^{}\) target rotation. (a) The pre-trained network adapts to the perturbation during the first trial with feedback control (full-line). The dashed line denotes the same network without feedback control. (b) The network further improves its performance during persistent perturbation with feedback-driven local learning (Feulner et al., 2022)**

activations. We show that during predicted movement (between \(t=50\) and \(t=100\)), the online feedback signal injected into the ongoing network dynamics approximates well the optimal, global trial gradient (Figure 1(c)). This adaptive behaviour is not typically observed in vanilla recurrent neural networks, where any performance feedback is _"locked"_ i.e. not available to the network activity until the end of the whole trial, and even then only indirectly so through an update in the network weights. Thus, with feedback control, the feedback is _"unlocked"_ and corrects the network activity in real-time.

Feedback control _and_ local, feedback-driven learning enable rapid network adaptation during persistent task perturbation

We next focus on network adaptation during persistent perturbation with feedback-driven plasticity _only_ in the recurrent layer weights. Here, plasticity is governed by the temporally and spatially local learning rule that uses the learned feedback weights to project errors from the readout layer back to the recurrent layer. In addition to guiding credit assignment for the recurrent network weights, recurrent network activations are also concurrently influenced by feedback control. Thus, we refer to this as RFLO with control, i.e. RFLO+c (for more details, see Section 2.4). Such plasticity further improves the network performance during persistent adaptation (Figure 2(a), blue line, Feulner et al. (2022)). Moreover, the network becomes independent of feedback control after a limited number of trials (Figure 2(a), grey line). In Figure 2(b), we show the final test performance without feedback control (i.e. control OFF) for the different learning algorithms at varying perturbation degree magnitudes. Using this simple learning rule with online feedback control during learning (RFLO+c), networks show performance similar to those trained with online BPTT on minor perturbations (Figure 2(b)). However, with larger perturbations, networks trained using local learning rules show an increasing performance gap with online BPTT (Figure 2(b)).

Figure 2: **Feedback control modulates rapid adaptation during task perturbation through approximate learning in the activity space.** (a) Increasing task perturbations degree (\(0 30 60\)) increases the relative feedback contribution to the network readout during a single task trial, which included a complete reaching movement. **start** denotes the start of the movement, and **peak** denotes the velocity peak. The shaded region denotes the standard error of the mean over 20 network seeds. (b) The average magnitude of the feedback drive is proportional to the movement perturbation magnitude. (c) The cosine similarity (reported in degrees) of the online feedback signal injected into the ongoing network dynamics and the optimal, global trial gradient w.r.t. activations during a single task trial. The shaded region denotes the standard error of the mean over 20 network seeds.

### Feedback control enables accurate recurrent weight adaptation during task perturbation

What makes online recurrent network adaptation so hard in the first place, and how might feedback control help to guide learning? To answer this question, we outline the computational requirements of the true online gradient w.r.t. the network activations and the feedback-driven local learning rule (Equation 4) using the notation from Marschall et al. (2020). Using online BPTT (Werbos, 1990), i.e. Real-Time Recurrent Learning (RTRL) (Williams and Zipser, 1989), one can decompose the total loss \(_{t}\) w.r.t. parameters \(\) using the chain rule as follows:

\[_{t}_{t}}{}=_{t}_{t}}{_{t}}_{t}}{}\] (5)

where \(_{t}}{_{t}}=}_{t}\) is the credit assignment vector and \(_{t}}{}=_{t}\) is the influence matrix (Marschall et al., 2020). Using RTRL, the credit assignment vector \(}\) is _immediate_ as it is simply calculated by backpropagating the immediate error \(^{t}\) through the derivative of the output function \(F_{}^{}\)1. However, the influence matrix \(\) requires the recursive computation of the past network states using the network Jacobian \(_{t}=_{t}/_{t-1}\) (for details, see Marschall et al. (2020)).

In this work, similar to previous work (Feulner et al., 2022; Bellec et al., 2020; Murray, 2019), \(\) is approximated with an _eligibility trace_\(\) (see Equation 4). However, as shown in Marschall et al. (2020), such a severe approximation of the Jacobian can fail to capture the true temporal dependencies, instead biasing the learning trajectory towards capturing only short-term dependencies present in the neural activity at the point of learning. Depending on the task and the recurrent matrix structure, this can lead to the capturing of spurious correlations and a significant generalization gap (Liu et al., 2022). However, in networks with feedback control during task perturbation, the network activities are increasingly driven by the _present_ feedback signal (Figure 1(a)), and thus less so by their _past_, recurrent state. We can quantify this further by calculating the norm of the Jacobian of the current network hidden activity w.r.t. past hidden activity with and without feedback control during task perturbation, which gives us an indication of current's networks sensitivity to its _past_ activity vs the _present_, immediate feedback error. This is shown in Figure 3(a), where we see that there is a drop in sensitivity to the past activity in the networks with feedback control during increasing task perturbation. Thus, as the network relies less on the past and more on the present, and is therefore "uncoupled", we expect the Jacobian approximation in the eligibility trace to the more accurate.

Figure 3: **Feedback control aids local learning in the recurrent network layer.** (a) Train loss as a function of trial number during persistent perturbation with feedback-driven local learning. Grey line denotes the performance of the same network tested without feedback control, showing that the network relies less on feedback control with learning. The shaded regions denote the standard error of the mean over 10 random network seeds. (b) The final performance (after 1000 trials, without feedback control) of recurrent networks trained with RFLO with (+c, blue) or without (yellow) feedback control during learning with varying degrees of persistent perturbation. The performance of same networks trained with online BPTT with (+c, orange) or without (red) feedback. The error bars denote the standard error of the mean over 10 random network seeds.

We empirically validate this hypothesis by comparing the gradients of the local learning rule with that of online BPTT during task adaptation, with (RFLO+c) and without (RFLO) feedback control. Note that we calculate both the local, i.e. at each learning step, and global, i.e. after all the learning steps, gradient alignments. We show that while there is only a minor difference in the local gradient alignements, this compounds at the global level, where the local learning rule in the networks with feedback control aligns better with the true, global gradient than without feedback control (Figure 3(b)-3(c)). This may explain the better performance of the networks with feedback control during task adaptation (Figure 2(b)), as the local learning rule is more accurate.

### Feedback control enables efficient recurrent weight adaptation during task perturbation

To further understand the impact of feedback control on weight-based learning, we next focus on the relative magnitude of weight updates during task adaptation. Specifically, for each learning episode consisting of a limited number of trials, we can calculate the magnitude of final, global weight update \(\|_{global}\|\), as well as the total combined magnitude of the online, local weight updates \(\|_{local}\|\). The ratio of the two quantities allows us to quantify the efficiency of the online weight updates during task adaptation. For instance, the less erratic the online weights updates are, the more efficient the learning trajectory, and our calculated ration will be closer to 1. Despite the relatively minor difference in local gradient accuracy (Figure 3(b)), we show that our pre-defined "efficiency ratio" is significantly different between the network with and without feedback control during learning, at all tested perturbation magnitudes (Figure 4(a)). Note that we don't observe this difference with more accurate gradients i.e. with BPTT (Figure 9(b)), but we do also observe it on an additional toy task (see Figure 11). Thus, with online feedback control, local weight updates are more efficient and less sensitive to the noise inherent to online learning using pseudo-gradients.

What could be the mechanism behind this marked increase in efficiency of the online weights updates with feedback control? In our networks, the feedback-controlled network activities are more constrained during task perturbation, where the network is initially more driven by the feedback signal that approximately aligns with their first-order gradient (see Figure 1(c)), irrespective of any weight based learning. This can be seen as a form of local adaptive inference in the activity space, or "prospective configuration" (Song et al., 2022), that can _guide_ subsequent update steps in the weight space. Furthermore, the efficiency of learning is often linked to the network's sensitivity to its higher-order gradients, which equips it with better navigation of pathological curvatures in the objective function (Martens et al., 2010). To assess this in our networks, we calculate the alignment of the online feedback control signal injected into the network dynamics with the exact gradient used in the canonical \(2^{nd}\) order optimization scheme, Newton's method. Here, the gradient is defined as \((-)^{-1}\), where \(\) is the Hessian w.r.t. the network activations, \(\) is some small constant that re-conditions it and \(\) is the \(1^{st}\) order gradient. We show that during peaks of movement error, the feedback control signal injected into the ongoing network dynamics is also sensitive to their \(2^{nd}\)

Figure 4: **Feedback increases the accuracy of local learning. (a) The norm of the Jacobians of the network activities at time \(t\) w.r.t. to activities at a previous timestep during a single trial. The shaded regions denote the standard deviation over 10 random network seeds. (b) The mean local alignment of the feedback-driven local learning rule with the true local gradient during task adaptation. The error bars denote the standard deviation over 10 seeds. The error bars denote the standard deviation over 10 random network seeds. (c) The mean global alignment of the feedback-driven local learning rule with the true global gradient during task adaptation. The error bars denote the standard deviation over 10 random network seeds.**

order gradient (Figure5b, see also Figure 10a). Thus, feedback control also _implicitly_ injects adaptive, second-order gradient information into the network dynamics, which may explain the magnitude of the increased efficiency observed in online weight updates during task adaptation.

## 4 Discussion

Summary.Although feedback connections are known to influence forward network activity in biological networks, the interaction of such feedback-driven activity for control with weight-based learning is less understood. In this work, we investigated the mechanistic properties of recurrent neural networks pre-trained with online feedback control using BPTT and tested on a task perturbation, which can also be seen as task modification, originally proposed in Feulner et al. (2022). We show that the online adaptation of such networks during task perturbations is due to (1) the increased feedback drive of the network and (2) the approximate alignment of the feedback signal with the true first and second-order gradient w.r.t. the network activations. Furthermore, we show that the consequence of such control is that the temporally and spatially local weight updates during task adaptation are both more accurate, robust and efficient.

Feedback control and learning.Our results are in line with recent theoretical work showing that injecting control signals into network activity may aid local, biologically plausible learning in systems at equilibrium (Meulemans et al., 2022). Here, we build on this work by showing that such strict equilibrium conditions can be relaxed in the presence of learned online feedback control during an ongoing network trial. Moreover, we show that the feedback control integration in the activity space also _implicitly_ injects adaptive, second-order gradient information into the network dynamics, which may explain the magnitude of the increased efficiency observed in online weight updates during task adaptation. This is also in line with recent theoretical (Innocenti et al., 2023; Alonso et al., 2022) work linking network inference in predictive coding networks with the second-order, trust region optimization methods (Conn et al., 2000; Dauphin et al., 2014; Yuan, 2015). Here, unlike in standard line-search methods, a "safe" region in the loss landscape is first determined, before the actual weight updates within this region are calculated and applied. As in Innocenti et al. (2023), we show that in our networks this "safe" region could be implicitly calculated through adaptive, feedback-driven inference in the activity space. Importantly, in our networks this is done without an additional relaxation phase nor any explicit, second-order gradient calculation, and thus without the additional, often prohibitive, computational cost associated with such methods compared to a standard forward and backward network pass in BPTT.

Figure 5: **Feedback increases the efficiency of local learning. (a) The mean norm or efficiency ratio of the total, global weight update with that of the online, local weight update during task adaptation at various perturbation degrees. The error bars denote the standard deviation over 10 random network seeds. (b) The cosine similarity between the online feedback control signal injected into the ongoing network dynamics and the second-order gradient (calculated using the Newton method) w.r.t. activations (blue) during a single trial. The grey line denotes the first-order gradient as in Figure 2c. The shaded region denotes the standard error of the mean over 10 network seeds.**

Limitations & future work.In this work, we demonstrate the benefits of feedback control on a simple, biologically relevant task in single layer RNN. Further work is needed to investigate the generality of our findings to a wider range of tasks and network architectures. Moreover, we remain agnostic to the exact biological implementation of the feedback signal (probably via cerebellum, see Pemberton et al. (2022)) and the local learning rule (but see Aceituno et al. (2023)). Finally, the feedback form used in this work is a simple, linear projection of the signed error signal back to the network, and we anticipate that this may not hold in many brain regions. We believe it would be interesting both for computational neuroscience _and_ machine learning to investigate the impact of less explicit feedback mechanism. Thus, extending our results to learning conditions with increased generality is an important direction for future work.

## 5 Impact statement

This paper presents work whose goal is to advance our understanding of fundamental principles of biological motor control. We recognize that this could have far-reaching implications in the future, particularly in the fields of robotics and prosthetics. Beyond these applications, this research may also enhance our understanding of motor disorders, potentially leading to improved rehabilitation strategies. Careful ethical considerations throughout our research are vital to proactively navigate the benefits and potential challenges these advancements may introduce to society.

## 6 Acknowledgements

This work was supported by the Wellcome Trust [grants 200790/Z/16/Z and 219995/Z/19/Z], the Simons Foundation [grant 564408], and EPSRC [grant EP/R035806/1]. We gratefully acknowledge the computational resources and support provided by the Imperial College Research Computing Service (http://doi.org/10.14469/hpc/2232). We also thank Dr. Tudor Berariu for the insightful discussions throughout this project and the reviewers for their constructive feedback, both of which greatly improved the quality of this paper.