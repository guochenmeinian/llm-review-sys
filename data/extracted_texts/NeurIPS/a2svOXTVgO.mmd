# On the Convergence of CART under Sufficient

Impurity Decrease Condition

 Rahul Mazumder

Sloan School of Management

Massachusetts Institute of Technology

rahulmaz@mit.edu

&Haoyue Wang

Operations Research Center

Massachusetts Institute of Technology

haoyuew@mit.edu

This work is supported by Office of Naval Research (N00014-21-1-2841).

###### Abstract

The decision tree is a flexible machine learning model that finds its success in numerous applications. It is usually fitted in a recursively greedy manner using CART. In this paper, we investigate the convergence rate of CART under a regression setting. First, we establish an upper bound on the prediction error of CART under a sufficient impurity decrease (SID) condition  - our result improves upon the known result by  under a similar assumption. Furthermore, we provide examples that demonstrate the error bound cannot be further improved by more than a constant or a logarithmic factor. Second, we introduce a set of easily verifiable sufficient conditions for the SID condition. Specifically, we demonstrate that the SID condition can be satisfied in the case of an additive model, provided that the component functions adhere to a "locally reverse Poincare inequality". We discuss several well-known function classes in non-parametric estimation to illustrate the practical utility of this concept.

## 1 Introduction

The decision tree  is one of the most fundamental and popular methods in the machine learning toolbox. It utilizes a flowchart-like structure to recursively partition the data and allows users to derive interpretable decisions. It is usually constructed in a data-dependent greedy manner, with an algorithm called CART . Thanks to its computational efficiency and ability to capture nonlinear structures, decision trees have served as the foundation for various influential algorithms for ensemble learning, including bagging , random forest  and gradient boosting . These algorithms are among the best-known nonparametric models for supervised learning with tabular data .

Despite its remarkable empirical success in various applications, our theoretical understanding of decision trees remains somewhat limited. The data-dependent splits employed in CART pose challenges for rigorous theoretical analysis. While some easy-to-analyze variants of CART have been widely studied [18; 4; 3], rigorous theoretical analysis of the original version of CART has been absent until recently. In particular, a recent line of work [19; 24; 10; 16] has established the consistency and prediction error bounds for CART and random forest. See Section 1.1 for a comprehensive review of related literature.

In this paper, we study the decision tree under a nonparametric regression model:

\[y_{i}=f^{*}(x_{i})+_{i} i[n]\] (1)

with \(x_{i}=(x_{i}^{(1)},...,x_{i}^{(p)})^{p}\), \(y_{i}\); \(_{i}\) is the noise with zero-mean, and \(f^{*}(x)\) is the signal function. We consider a random design where \(\{x_{i}\}_{i=1}^{n}\) are i.i.d. from some underlying distribution \(\) supported on \(^{p}\), and \(\{_{i}\}_{i=1}^{n}\) are i.i.d. independent of \(\{x_{i}\}_{i=1}^{n}\). For an estimator \(\) of \(f^{*}\), we usethe \(L^{2}\) prediction error \(\|-f^{*}\|_{L^{2}()}^{2}\) as a measure of the estimator \(\). Consider a regression tree algorithm that produces a heuristic (not necessarily exact) solution \(\) of

\[_{f_{d}}\ _{i=1}^{n}(f(x_{i})-y_{i})^{2}\] (2)

where \(_{d}\) is the space of binary (i.e. each splitting node has two children) axis-aligned regression trees with maximum depth \(d\). The statistical performance of the estimator \(\), therefore, is affected by the following three factors: (i) Approximation error: how accurately can \(f^{*}\) be approximated by a regression tree in \(_{d}\). (ii) Estimation error: the variance of the estimator, which is affected by the complexity of the function space \(_{d}\). A larger space \(_{d}\) (i.e. larger \(d\)) can reduce the approximation error, but it may also increase the estimation error and lead to overfitting. (iii) Optimization error: the discrepancy between \(\) and the exact optimal solution of the optimization problem (2). Indeed, if \(\) is the exact optimal solution of (2) (i.e. optimal regression tree), it can approximate a broad class of \(f^{*}\), and the prediction error can be analyzed via the classical theory of least-squares estimators [22; 13]. However, since solving (2) to optimality is computationally challenging for large-scale data, heuristics like CART are typically employed in practice. To understand the statistical performance of CART, the major difficulty lies in the analysis of the optimization error of CART.

As a greedy algorithm, CART examines only one split in each iteration (see Section 1.3 for details). Although decision trees are intended to capture multivariate structures across different features, the greedy nature of CART poses challenges in approximating functions \(f^{*}\) that have complex multivariate structures. For example, consider the two-dimensional "XOR gate" function \(f^{*}(u):=1_{\{u[0,1/2)^{2}\}}+1_{\{u[1/2,1)^{2}\}}\), and assume that \(x_{i}\) has a uniform distribution on \(^{2}\). Note that \(f^{*}\) itself is a depth-2 decision tree, with (e.g.) the root node split with the first coordinate at \(1/2\), and the left and right children split with the second coordinate at \(1/2\). However, due to the symmetry structure of \(f^{*}\), when the sample size \(n\) is large, the root split of CART is completely fitting to noises and is likely to generate a split near the boundary [5; 8]. Consequently, the resulting estimator \(\) fails to capture the true tree structure encoded in \(f^{*}\). The major difficulty in fitting the XOR gate function with CART lies in the complicated interaction between the two coordinates.

The discussions above naturally lead to the following two questions about CART:

* What conditions on \(f^{*}\) are needed such that CART can approximate \(f^{*}\) and yield a consistent estimator?
* If \(f^{*}\) satisfies such conditions, what is the convergence rate of the prediction error of CART?

In this paper, we aim to address these fundamental questions by providing sufficient conditions on \(f^{*}\) such that one can establish convergence rates of CART. These sufficient conditions are quite flexible to include a broad class of nonparametric models. In the following, we first discuss some related literature and the basics of CART. In Section 2, we prove a general error bound for CART under a _sufficient impurity decrease (SID)_ condition  of \(f^{*}\). This error bound is an improvement over the existing results under similar conditions . In Section 3, we introduce a sufficient condition for the SID condition on \(f^{*}\), and use it to show that the SID condition can be satisfied by a broad class of \(f^{*}\).

### Related literature

The inception and analysis of decision trees and CART algorithm can be traced back to the seminal work of Breiman . Breiman's study marked the first significant step towards establishing the consistency of CART, albeit under stringent assumptions concerning tree construction. The original formulation of CART, characterized by data-dependent splits, poses challenges when it comes to rigorous mathematical analysis. To address this issue, subsequent research efforts [18; 4; 3] have introduced alternative variants of CART that lend themselves to more tractable theoretical examinations. For instance, certain studies [12; 4; 3; 2] assume random selection of splitting features in the tree. Other approaches incorporate random selections of splitting thresholds [12; 2], while some restrict splits to occur exclusively at the midpoint of the corresponding cell in the chosen coordinate . These simplifications ensure the independence between the splitting rules of the tree and the training data - the tree depends on the data only via the estimation in each cell. Such independence simplifies the analysis of the resultant trees. In a similar vein, alternative strategies have been proposed, such as the employment of a two-sample approach  to compute splitting rules and cell estimations separately, giving rise to what is known as an "honest tree". This methodology also ensures independence between various aspects of the tree construction and estimation process.

Since the first consistency result in , the strict theoretical analysis of the original CART algorithm has remained elusive until recent advancements. A significant breakthrough came with the seminal work of Scornet et al. , which demonstrated the consistency of CART under the assumption of an additive model  with continuous component functions. It makes a technical contribution on the decrease of optimization error of CART. This technical argument strongly relies on the additive model. Subsequent works  further extended the theoretical analysis by providing non-asymptotic error bounds for CART under a similar additive model. However, the rates derived in  is a slow rate of \(O(((n))^{-1})\), which seems not easy to be improved under the same assumption . Although the additive model is a well-studied model in statistical learning, it is not immediately evident why such a modeling assumption is specifically required for CART to function effectively.

Another line of work  adopts more intuitive assumptions on the signal function \(f^{*}\) to ensure the progress of CART. In particular, Chi et al.  introduced a sufficient impurity decrease (SID) condition. In essence, the SID condition posits that for every rectangular region \(B\) within the feature space, there exists an axis-aligned split capable of reducing the population variance of \(f^{*}\) (conditional on \(B\)) by a fixed ratio. Note that such a split cannot be directly identified by CART, as it necessitates knowledge of the population distribution of \(x_{i}\). Instead, CART employs empirical variance of \(y_{i=1}^{n}\) to guide its cell splitting decisions. Nonetheless, the SID condition is a strong assumption on the approximation power of tree splits, which can ensure the consistency of CART via an empirical process argument . A similar condition has also appeared implicitly in . Under the SID condition, Chi et al.  studied the consistency and convergence rates of CART, demonstrating a polynomial convergence \(O(n^{-})\) of the prediction error. A possible limitation of the SID condition is the difficulty of verification: it necessitates the decreasing condition to hold true for all rectangles. Although a few examples are provided in , the general procedure for verifying whether a specific \(f^{*}\) satisfies this condition remains unclear. Notably, its relationship with the additive model assumption in  remains to be fully understood.

Lastly, it is worth mentioning that several recent studies  have investigated the performance lower bounds of CART, shedding light on its limitations. Specifically, Tan et al.  demonstrated that even when assuming an additive regression function \(f^{*}\), any single-tree estimator still suffers from the curse of dimensionality. Consequently, these estimators can only achieve convergence rates considerably slower than the oracle rates established for additive models . Another study by Cattaneo et al.  delved into the splitting behavior of CART and provided theoretical support for empirical observations  indicating that CART tends to generate splits near boundaries in cases where the signal is weak. These lower bounds support the importance of using trees in ensemble models to reduce the prediction error. Although we focus on analyzing CART for a single tree in this paper, the result can be adapted to the analysis of ensembles of trees for the fitting of more complex function classes.

### Contributions

The first contribution of our paper is a refined analysis of CART under the SID condition. Although the convergence of CART has been studied in  under SID condition, the analysis of  seems not tight when the noises have light-tails (see discussions in the Appendix C). We establish an improved analysis of CART under this setting, and show by examples that our error bound is tight up to log factors.

The second contribution of this paper is the decoding of the mystery of the SID condition. We discuss a sufficient condition under which the SID condition holds true. In particular, we introduce a class of univariate functions that satisfy an _locally reverse Poincare inequality_, and show that additive functions with each univariate component being in this class satisfy the SID condition. This builds a connection between the two types of assumptions in the literature: additive model and SID condition. We discuss a few examples that how the locally reverse Poincare inequality can be verified.

### Basics of CART

We review the methodology of CART to build the tree. The primary objective of decision trees is to find optimal partitions of feature space that produce a minimal variance of response variables. CART constructs the tree and minimizes the variance using a top-down greedy approach. It starts with a root node that represents the whole feature space \(^{p}\) - this can be viewed as an initial tree with depth \(0\). At each iteration, CART splits all the leave nodes in the current tree into a left child and a right child; the depth of the tree increases by \(1\), and the number of leave nodes in the tree doubles. At each leave node, it takes an _axis-aligned splits_ with maximum variance (impurity) decrease. Let \(\) be the set of all intervals (which can be open, closed, or open on one side and closed on another side). Define the set of rectangles in \(^{p}\) as:

\[:=_{j=1}^{p}E_{j}\;\;E_{j}\;\; \;j[p]}\] (3)

For each \(A\), define \(_{A}:=\{i[n]\;|\;x_{i} A\}\). For a leave node that represents a rectangle \(A\), the impurity decrease of a split in feature \(j[p]\) with threshold \(b\) is given by:

\[(A,j,b):=_{i_{A}}(y_{i}-_ {_{A}})^{2}-_{i_{A_{L}}}(y_{i}-_{_{A_{L}}})^{2}-_{i_{A_{R}}}(y_{i}- _{_{A_{R}}})^{2}\] (4)

where \(y_{}:=|}_{i}y_{i}\) for any \([n]\), and

\[ A_{L}&=A_{L}(j,b):=A\{v^{ p}\;|\;v_{j} b\},\\ A_{R}&=A_{R}(j,b):=A\{v^{p}\;|\;v_{j}>b\}.\] (5)

To split on \(A\), CART takes \(\) and \(\) that maximize the impurity decrease, i.e.,

\[,*{argmax}_{j[p],b} (A,j,b)\] (6)

This splitting procedure is repeated until the tree has grown to a given maximum depth \(d\).

### Notations

For \(a,b>0\), we write \(a=O(b)\) if there exists a universal constant \(C>0\) such that \(a Cb\); we write \(b=(a)\) if \(a=O(b)\), and write \(a=(b)\) if both \(a=O(b)\) and \(a=(b)\) are true. For values \(a_{n,p},b_{n,p}\) that may depend on \(n\) and \(p\), we write \(a_{n,p}=(b_{n,p})\) if \(a_{n,p}=O(b_{n,p}^{}(np))\) for some fixed constant \( 0\).

## 2 Error bound of CART under SID property

We study the prediction error bound of CART for the regression problem (1). We focus on a random design as in the following assumption.

**Assumption 2.1**: _(i) (Random design) Suppose \(\{x_{i}\}_{i=1}^{n}\) are i.i.d. random variables with a distribution \(\) supported on \(^{p}\). Suppose \(\) has a density \(p_{X}\) (w.r.t. Lebesgue measure) on \(^{p}\) satisfying \(0< p_{X}(u)<\) for all \(u^{p}\) for some constants \( 1\) and \( 1\)._

_(ii) (Error distribution) Suppose \(\{_{i}\}_{i=1}^{n}\) are i.i.d. zero-mean bounded random variables with \(|_{i}| m<\) for some \(m>0\). Suppose \(\{_{i}\}_{i=1}^{n}\) are independent to \(\{x_{i}\}_{i=1}^{n}\)._

_(iii) (Bounded signal function) Suppose \(_{u^{p}}|f^{*}(u)| M<\) for some constant \(M\)._

In Assumption 2.1 (i), it is assumed that the features \(x_{i}\) are supported on \(^{p}\) for convenience. The same results (different by at most a constant) can be proved if we replace \(^{p}\) with an arbitrary bounded rectangle. The second assumption (Assumption 2.1 (ii)) requires the error terms \(\{_{i}\}_{i=1}^{n}\) to be i.i.d. bounded - this is the key difference of our paper and the assumptions made in . In particular,  made a milder assumption on the error terms that allows them to have heavy tails. Their result, however, when specifying to the case of bounded noises, is not tight (seediscussions in Appendix C for a comparison). Note that for convenience we have assumed that \(_{i}\) is bounded. If instead, we assume the noises are sub-Gaussian, similar conclusions can also be proved via a truncation argument (the bound may be different by a log factor). The third assumption (Assumption 2.1 (iii)) is a standard assumption for nonparametric regression models .

Under the random design setting in Assumption 2.1, we are ready to introduce the sufficient impurity decrease condition on the underlying function \(f^{*}\). Let \(X\) be a random variable in \(^{p}\) that has the same distribution as \(x_{1}\) and is independent to \(\{x_{1},...,x_{n}\}\). For any \(A\), \(j[p]\) and \(b\), define:

\[(A,j,b):=(X A)(f^{*}(X)|X A)- (X A_{L})(f^{*}(X)|X A_{L})\] (7) \[-(X A_{R})(f^{*}(X)|X A _{R})\]

where \(A_{L}\) and \(A_{R}\) are defined in (5), and \((f^{*}(X)|X A)\) means the conditional variance defined as

\[(f^{*}(X)|X A):=f^{*}(X)-(f^ {*}(X)|X A)^{2}X A\] (8)

It is not hard to see the similarity between \((A,j,b)\) and \((A,j,b)\) (defined in (4)), where the former can be viewed as a population variant of the latter. Intuitively, for a split with feature \(j\) and threshold \(b\), \((A,j,b)\) measures variance decrease of \(f^{*}\) after this split. The SID condition below assumes that there is a split with a large variance decrease.

**Assumption 2.2**: _(Sufficient Impurity Decrease) There exists a constant \((0,1]\) such that for all \(A\),_

\[_{j[p],b}(A,j,b)\;\;(X A )(f^{*}(X)|X A)\] (9)

Note that Assumption 2.2 is a condition depending on both the function \(f^{*}\) and the underlying distribution \(X\). Briefly speaking, the SID condition requires that the best split must decrease the population variance by a constant factor (note that \(\) is required to be bounded away from \(0\)). Intuitively, the condition (9) can be satisfied if \(f^{*}\) has a significant "trend" along some axis in \(A\), but may be hard to be satisfied if \(f^{*}\) is relatively "flat" in \(A\). Since Assumption 2.2 requires (9) being satisfied for all cells \(A\), it essentially requires that \(f^{*}\) is not "too flat" on any local rectangle \(A\). In Section 3, we develop a technical argument to translate this intuition into a rigorous statement, and use it to check Assumption 2.2 for a wide class of functions.

With Assumptions 2.1 and 2.2 at hand, we are ready to present the main result in this section. For any function \(g:^{p}\), let \(\|g\|_{L^{2}()}\) denote the \(L^{2}\)-norm of \(g\) with respect to the measure \(\).

**Theorem 2.3**: _Suppose Assumptions 2.1 and 2.2 hold true. Let \(^{(d)}()\) be the tree estimated by CART with depth \(d\). Suppose \(n\) is large enough such that \(e^{2}d}{n}(n+1)^{2d}/)}{n^{ }()}<3/4\). Then there exists a universal constant \(C>0\) such that with probability at least \(1-\), it holds that for any \(>0\),_

\[\|^{(d)}-f^{*}\|_{L^{2}(X)}^{2} 2(f^{*}(X)) 1-}^{d}+C(d(np)+ (1/))}{ n}U^{2}\] (10)

_where \(U:=M+m\). In particular, taking \(=1/d\) and \(d=_{2}(n)/(1-_{2}(1-))\), it holds_

\[\|^{(d)}-f^{*}\|_{L^{2}(X)}^{2} C_{,U}(n) (np)+(n)(1/)}{n^{()}}\] (11)

_where \(():=(1-)}{1-_{2}(1-)}\), and \(C_{,U}\) is a constant that only depends on \(\) and \(U\)._

The error bound in (10) consists of two terms. The first term corresponds to the bias of the approximating \(f^{*}\) using CART. It decreases geometrically as \(d\) increases, which is suggested by the SID condition. The second term is \(O(2^{d}d(np)/n)\), which corresponds to the estimation variance of CART. Here the term \(2^{d}\) represents the model complexity - a fully-grown depth \(d\) tree has \(2^{d}\) leaves. The proof of Theorem 2.3 is presented in Appendix A. It is based on a careful technical argument that controls the difference between the population impurity decrease \((A,j,b)\) and the empirical impurity decrease \((A,j,b)\). In particular, our analysis is different from that of . Note that  makes a different assumption on the error distribution, and their technical argument is not sufficient for the proof of Theorem 2.3. Particularly, the error bound in (11) is an improvement of the results proved in Theorem 1 of --see Section C for a detailed comparison.

Note that the convergence rate in (11) has a crucial dependence on the coefficient \(\). For a large value of \(\), the exponent \(()\) is also larger, leading to a faster convergence rate in (11). On the other end, if \(\) is small, as \( 0\), we have \(() 0\), and the convergence rate in (11) can be arbitrarily slow. Nonetheless, as long as the SID condition is satisfied with \(\) bounded away from \(0\), (11) shows a polynomial rate, which is better than the logarithmic rate in  without the SID condition.

To explore the tightness of the error bounds in Theorem 2.3, we consider a simple example with \(f^{*}\) being a univariate linear function.

**Example 2.1**: _(Univariate linear function) Suppose \(p=1\), and \(x_{i}\) are i.i.d. from the uniform distribution on \(\). Suppose \(f^{*}\) is a univariate linear function: \(f^{*}(t)=t\). Then the SID condition (Assumption 2.2) is satisfied with coefficient \(=3/4\). As a result, it holds \(()=2/3\), and the error bound in the RHS of (11) is \((n^{-2/3})\)._

_Proof._ For any interval \(A=[,r]\), we have

\[(X A)(f^{*}(X)|X A)=_{}^{r}(t-)^{2}t=}{12}\] (12)

Take \(b:=(+r)/2\), then we have

\[(A,1,b)&=_{}^{r}(t -)^{2}t-_{}^{b}(t-)^{2}t-_{b}^{r}(t-)^{2}t\\ &=(r-)^{3}-(b-)^{3}-( r-b)^{3}=(r-)^{3}\] (13)

Combining (12) and (13) we know that Assumption 2.2 is satisfied with \(=3/4\). \(\)

As shown by Example 2.1, when \(f^{*}\) is a univariate linear function and \(\{x_{i}\}_{i=1}^{n}\) are from a uniform distribution, the error bound in (11) reduces to \((n^{-2/3})\). This rate matches the lower bound for any partition-based estimator (see e.g.  Chapter 4), 2 hence showing that the upper bound in (11) cannot be improved (by more than a log factor) via any refined analysis. To verify the rate \((n^{-2/3})\), we conduct a simulation shown by Figure 1. In particular, Figure 1 presents the log-log plot of the prediction error of CART versus different sample sizes, where the depth \(d\) is taken as stated above (11). A reference line of slope \(-2/3\) is also shown. It can be seen that the slope of the line for CART is also roughly \(-2/3\), which is consistent with the rate \((n^{-2/3})\) derived in Example 2.1.

Figure 1: Prediction error of CART versus sample size \(n\) for a univariate linear model.

Function classes satisfying SID condition

In this section, we introduce a class of sufficient conditions that facilitate the easy verification of the SID condition for a broad range of functions. Specifically, we focus on the case when \(f^{*}\) has an additive structure:

\[f^{*}(u)=f^{*}_{1}(u_{1})+f^{*}_{2}(u_{2})++f^{*}_{p}(u_{p})\] (14)

where \(f^{*}_{j}\)'s are univariate functions. Note that additive model has also been assumed in prior works such as  for the study of CART. If we only assume that each component function \(f^{*}_{j}\) has bounded total variation, then only a slow rate \(O(^{-1}(n))\) is known . In the following, we discuss a few sufficient conditions concerning the component functions \(f^{*}_{j}\) under which \(f^{*}\) satisfies the SID condition, consequently guaranteeing a faster convergence rate as indicated by Theorem 2.3.

First, we introduce a key class of univariate functions that satisfy a special integral inequality.

**Definition 3.1**: _(Locally Reverse Poincare Class) For a univariate differentiable function \(g\) on an interval \(Q\), we say it belongs to the Locally Reverse Poincare (LRP) class on \(Q\) with parameter \(\) if for any subinterval \([a,b] Q\),_

\[_{a}^{b}|g^{}(t)|\;t^{2}} {b-a}_{w}_{a}^{b}|g(t)-w|^{2}\;t\] (15)

_We use the notation \(LRP(Q,)\) to denote the class of all such functions._

It is worth noting that for any given univariate differentiable function \(g\) and a fixed interval \([a,b]\), there exists a constant \(\) such that the reverse of inequality (15) holds true, as established by the Poincare inequality. The condition (15) requires a uniform constant \(>0\) such that the reverse of the Poincare inequality holds true. Note that the LHS of (15) is equivalent to the square of the total variation of \(g\) on \([a,b]\), and the RHS is a constant multiple of the conditional variance of \(g\) on \([a,b]\) (under the uniform distribution). Intuitively, this condition can be satisfied when the function \(g\) exhibits a clear "trend" within any interval \([a,b]\) rather than being fuzzy fluctuations. We give a few examples below.

**Example 3.1**: _(Strongly increasing function) Let \(g\) be a strictly increasing function on \(\) with \(c_{2} g^{}(t) c_{1}>0\) for all \(t\). Then \(g LRP(,2c_{2}/c_{1})\)._

Example 3.1 provides a direct generalization of the simple linear function showcased in Example 2.1. Note that this example was also discussed in . In Example 3.1, since \(g^{}(t)\) is consistently bounded away from zero, verifying the inequality 15 becomes straightforward. Below we present a more nuanced example where \(g^{}(t)\) may equal zero at certain point \(t\).

**Example 3.2**: _(Smooth and strongly convex function) Let \(g\) be a \(L\)-smooth and \(\)-strongly-convex function on \(\) for some \(L>>0\), i.e.,_

\[(t-s)^{2} g(t)-g(s)-g^{}(s)(t-s)(t-s )^{2}\] (16)

_for all \(t,s\). Then \(g LRP(,110(L/))\)._

Note that for a smooth and strongly convex function \(g\), the gradient \(g^{}(t)\) can be zero at some \(t\). But the strong convexity condition guarantees that there is at most one point \(t\) where \(g^{}(t)=0\), and the inequality (15) can still be satisfied. Additional details can be found in Appendix B.

**Example 3.3**: _(Polynomial with degree \(r\)) There exists a constant \(C_{r}\) that only depends on \(r\) such that for any univariate polynomial \(g\) of degree at most \(r\), \(g LRP(,C_{r})\)._

Although the set of polynomials (with a degree at most \(r\)) appears to be complicated, it is a finite-dimensional linear space. Therefore the differentiation is a bounded linear operator between finite dimensional normed linear spaces. As a result, the conclusion of Example 3.3 can be proved via a variable transformation argument. See Appendix B for details.

Examples 3.1 - 3.3 demonstrate that the LRP condition can be readily verified for various well-known function classes, but the relationship between the LRP class and the SID condition remains unclear. Below we present the first main result in this section, which establishes a connection between the LRP class and the SID condition under the additive model.

**Proposition 3.1**: _Suppose Assumption 2.1 holds true, and \(f^{*}\) has an additive structure as in (14). If \(f^{*}_{k} LRP(,)\) for all \(k[p]\), then Assumption 2.2 is satisfied with \(=4/(p^{2})\)._

As shown by Proposition 3.1, for the additive model (14), when the basic conditions in Assumption 2.1 are satisfied, the SID condition holds true for \(f^{*}\) as long as each component function is in an LRP class. Particularly, the SID condition is satisfied for component functions from Examples 3.1 - 3.3. Note that the coefficient \(\) in Proposition 3.1 is proportional to \(1/p\), which implies that it can be very small when \(p\) is large. In particular, with this \(=(1/p)\) (with \(,\) and \(\) being constants), by Theorem 2.3, the prediction error bound in the RHS of (11) is \(O(n^{-c/p})\) for some constant \(c>0\)3. Since the exponent is dependent on \(p\), the curse of dimensionality becomes evident. This seems to indicate that the analysis of Proposition 3.1 could potentially be refined. However, given the negative results demonstrated by [20; 8], it is likely that for tree-based estimators, even with the additive model, the curse of dimensionality cannot be circumvented.

Proposition 3.1 requires each component of the additive function to lie in an LRP class. Actually, this condition can be further relaxed as in the following proposition.

**Proposition 3.2**: _Suppose Assumption 2.1 holds true, and \(f^{*}\) has an additive structure as in (14). Given integer \(r 1\) and constants \(>0, 1\). Suppose for each \(k[p]\) there exist \(0=t_{0}^{(k)}<t_{1}^{(k)}<<t_{r-1}^{(k)}<t_{r}^{(k)}=1\) with \(t_{j}^{(k)}-t_{j-1}^{(k)}/r\) and \(f_{k}^{*} LRP([t_{j-1},t_{j}],)\) for all \(j[r]\). Then Assumption 2.2 is satisfied with parameter \(=/(p^{2})\) where \(^{2}=\{}{},}{2}\} \{9^{2},32+^{2}\}\)._

By Proposition 3.2, an additive function \(f^{*}\) satisfies the SID condition if each component function is a piecewise function, with each piece belonging to an LRP class. Notably, continuity at the joining points of consecutive pieces is not necessary. However, for technical reasons, it is required that each piece has a length that is not excessively small, as determined by the parameter \(\). See Appendix B for the proofs of Proposition 3.1 and 3.2.

## 4 Proof sketch of Theorem 2.3

We provide a sketch of the proof of Theorem 2.3. For \(p 1\) and \(d 1\), define

\[_{p,d}:=_{j=1}^{p}[_{j},u_{j}]\, \,\#\{j[p][_{j},u_{j}]\} d}\] (17)

That is, each rectangle in \(_{p,d}\) has at most \(d\) dimensions that are not the full interval \(\). When \(p d\), \(_{p,d}\) contains all the rectangles in \(^{p}\), i.e. \(_{p,d}=\). However, when \(d\) is much smaller than \(p\) (in a high-dimensional setting), \(_{p,d}\) contains much fewer rectangles than \(\). Note that for a decision tree with depth \(d\), each leaf node represents a rectangle in \(_{p,d}\).

The proof of Theorem 2.3 builds upon a few technical lemmas which provide uniform bounds between the empirical and populational quantities. These results may hold their own significance in the study of other partition-based algorithms. For \((0,1)\), define values

\[_{1}() =_{1}(,n,d):=\ (2p^{d}(n+1)^{2d}/)\] (18) \[_{2}() =_{2}(,n,d):=\ e^{2}d}{n}(n+1)^{2d}/)}{n}\] \[() =(,n,d):=\ _{1}(,n,d)_{2}( ,n,d)\]

where \(\) is the constant in Assumption 2.1\((i)\). Note that we have \(() O()\). We have the following uniform bounds on empirical and populational mean on every rectangle in \(_{p,d}\).

**Lemma 4.1**: _Suppose Assumption 2.1 holds true. Suppose \(_{2}(/12)<3/4\). Then with probability at least \(1-\), it holds_

\[_{A_{p,d}}(X A)}(f^{*}(X )|X A)-_{_{A}}\ \ 20U(/12)}\] (19)For a tree-based estimator in practice, the prediction at each leaf node is usually given by the sample average of \(y_{i}\) for all the samples \(i\) routed to that leaf, i.e., the value \(_{A}\) for a leaf with rectangle \(A\). Therefore, Lemma 4.1 provides a uniform bound on the error if we replace the prediction at each leaf by the computational conditional mean \((f^{*}(X)|X A)\). It is worth noting that the gap between \(_{A}\) and \((f^{*}(X)|X A)\) is rescaled by the quantity \((X A)}\). Intuitively, when the rectangle \(A\) is small and hence \((X A)\) is small, few points are routed to the rectangle \(A\). As a result, the corresponding gap between \(_{A}\) and \((f^{*}(X)|X A)\) can be large - which leads to a large discrepancy of the estimate and the signal locally in this cell \(A\). However, in another perspective, since the cell \(A\) is small, a large discrepancy in \(A\) only makes a small contribution to the overall error \(\|^{(d)}-f^{*}\|^{2}_{L^{2}(X)}\). Therefore, the factor \((X A)}\) in the LHS of (19) is a proper balance of these two aspects. This is the key technical argument that differs from the analysis in . It helps establish the tighter bounds in Theorem 2.3.

**Lemma 4.2**: _Suppose Assumption 2.1 holds true. Given a constant \(>0\). Given any \((0,1)\), suppose \(_{2}(/36)<3/4\). Then with probability at least \(1-\), it holds_

\[(A,j,b) (1+)(A,j,b)+(1+1/) C^{ }U^{2}(/36)\;A_{p,d-1},\;j[p],\;b \] \[(A,j,b) (1+)(A,j,b)+(1+1/) C^{}U^{2} {t}(/36)\;A_{p,d-1},\;j[p],\;b\]

_for some universal constant \(C^{}>0\)._

Lemma 4.2 establishes upper bounds on the discrepancy between empirical and populational impurity decrease. It builds an avenue to translate the SID condition into an empirical counterpart, which is further used to derive the decrease of objective value.

With Lemmas 4.1 and 4.2 at hand, we are ready to wrap up the proof of Theorem 2.3. First, we have

\[\|^{(k)}-f^{*}\|^{2}_{L^{2}(X)}\;\;2\|f^{*}-^{(k) }\|^{2}_{L^{2}(X)}+2\|^{(k)}-^{(k)}\|^{2}_{L^{2}(X)} \;:=\;2J_{1}(k)+2J_{2}(k)\] (20)

where \(^{(k)}\) is a tree with the same splitting rule as \(^{(k)}\) but leaf predictions replaced by the populational means (in that leaf). The term \(J_{2}(k)\) can be bounded as

\[J_{2}(k)\;=\;_{t^{(k)}}(X A_{t})(f^{*}(X)|X A_{t},_{1}^{n})-_{_{A_{t}}} ^{2} CU^{2}2^{k}\]

where \(^{(k)}\) is the set of leaves of \(^{(k)}\) at depth \(k\); \(A_{t}\) is the corresponding rectangle for a leaf \(t\); and \(C\) is a universal constant. The last inequality in (21) is by Lemma 4.1. The term \(J_{1}(k)\) can be written as

\[J_{1}(k)\;=\;_{t^{(k)}}(X A_{t}|_{1} ^{n})(f^{*}(X)|X A_{t},_{1}^{n})\]

Making use of Lemma 4.2 and by some algebra (see Appendix A for details), a recursive inequality can be established:

\[J_{1}(k+1)1-}J_{1}(k)+2^{k} (k(np)+(1/))}{n}\]

The proof of Theorem 2.3 is complete by telescoping the inequality above, and combining the upper bounds for \(J_{1}(k)\) and \(J_{2}(k)\) with (20).

## 5 Conclusion and discussions

We have proved an error bound on the prediction error of CART for a regression problem when \(f^{*}\) satisfies the SID condition. This error bound cannot be improved by more than a log factor. We have also discussed a few sufficient conditions under which we can show that an additive model satisfies the SID condition.

One possible limitation of this work is that: it seems that the SID coefficients \(\) derived in Section 3 are not tight. Since \(\) appeared in the exponent of the rate in Theorem 2.3, a tighter estimate of \(\) leads to an improved convergence rate in (11). We leave it as a future work for a more precise analysis of the SID condition.