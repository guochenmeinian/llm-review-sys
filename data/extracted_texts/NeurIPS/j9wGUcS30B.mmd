# On Masked Pre-training and the Marginal Likelihood

Pablo Moreno-Munoz

Section for Cognitive Systems

Technical University of Denmark (dtu)

pabmo@dtu.dk

&Pol G. Recasens

cromai, Barcelona Supercomputing Center

Universitat Politecnica de Catalunya (upc)

pol.garcia@bsc.es

&Soren Hauberg

Section for Cognitive Systems

Technical University of Denmark (dtu)

sohau@dtu.dk

Work done during an _Erasmus_ exchange in dtu (Denmark).

###### Abstract

Masked pre-training removes random input dimensions and learns a model that can predict the missing values. Empirical results indicate that this intuitive form of self-supervised learning yields models that generalize very well to new domains. A theoretical understanding is, however, lacking. This paper shows that masked pre-training with a suitable cumulative scoring function corresponds to maximizing the model's marginal likelihood, which is _de facto_ the Bayesian model selection measure of generalization. Beyond shedding light on the success of masked pre-training, this insight also suggests that Bayesian models can be trained with appropriately designed self-supervision. Empirically, we confirm the developed theory and explore the main learning principles of masked pre-training in large language models.

## 1 Introduction

Masked pre-training (mpt) is a family of self-supervised learning methods (Dosovitskiy et al., 2020; Devlin et al., 2018; Caron et al., 2021), that empirically has been demonstrated to result in models that generalize very well to new settings. In essence, masked pre-training removes random features of the data and learns a model to recover these from the remaining input. While empirical results are impressive, a deeper understanding of _why_ pre-trained models generalize so well is lacking. Is it due to the use of transformer architectures (Vaswani et al., 2017), the vast over-parametrization (Neyshabur et al., 2019), or something entirely different?

The marginal likelihood or _evidence_ is commonly used as the measure of generalization ability in Bayesian models (Tenenbaum and Griffiths, 2001; MacKay, 2003). While computationally expensive, the _blessing_ of the marginal likelihood comes from the probabilistic integration of hypotheses. Whenever we are considering a latent variable model in the Bayesian framework, such integration can be thought of as the average over all the possible latent variable mappings, weighted by our prior beliefs. Since masked pre-training drives generalization so well, the lingering question in the Bayesian modeling community is then: _Is masked pre-training somehow related to the maximization of the marginal likelihood?_

**In this paper,** we provide a positive answer. We show that masked pre-training optimizes according to a stochastic gradient of the log-marginal likelihood (lml). Importantly, the log-marginal likelihood is equivalent to the cumulative sum of masked pre-training losses shaped with different sizes for the random mask. Even if its practical use avoids this cumulative sum, we demonstrate that choosing a_fixed_ masking rate, e.g. \(15\%\) as in bert(Devlin et al., 2018), leads to a stochastic _biased_ estimation which still maximizes the log-marginal likelihood.

**Our proof** relies on a previous observation from Fong and Holmes (2020), who show that the log-marginal likelihood equals the average of exhaustive _leave-\(M\)-out_ cross-validation (cv) given posterior predictive scores. Intuitively, our formal results can be seen as the _transposed_ version of Fong and Holmes's results: where cv removes _random observations_ to measure generalization, masked pre-training removes _random features_. While the seminal link between cv and the marginal likelihood was purely a formal result that pointed out the underlying presence of Bayesian principles in a well-known class of learning, our work extends the theory behind the marginal likelihood to comprehend the impressive behavior of the latest generative models.

## 2 Masked pre-training

Masked pre-training (mpt) is a variant of self-supervised learning (Dosovitskiy et al., 2020; Devlin et al., 2018) that removes random input dimensions (also known as _masking_) in the observed data and learns a model that accurately predicts the missing values. This family of methods, well-known due to their success in natural language understanding, typically adopts a transformer architecture (Vaswani et al., 2017) as the feature extractor, that together with positional encodings and random masked dimensions allows capturing the bidirectional context in the data.

In bert(Devlin et al., 2018), each sentence is usually considered as a \(D\) dimensional observation vector, \(=(x_{1},x_{2},,x_{D})^{}\), where dimensions \(x_{t}\) are named _tokens_. Given a _random mask_\(\) of size \(M{<}D\), as a set of indices drawn uniformly from \(\{1,,D\}\), each token whose index belongs to \(\) is considered to be in the subset \(_{}=\{x_{(1)},x_{(2)},,x_{ (M)}\}\). We refer to these as the _masked tokens_. The rest of indices \(=\{1,2,,D\}\) induce the complementary subset \(_{}\), such that \(=_{}_{}\). Under this notation, mpt learns the parameters \(\) of a model \(p_{}()\) by maximising an average of the following objective

\[ p_{}(_{}|_{})_{t=1}^ {M} p_{}(x_{(t)}|_{})\] (1)

for every observation in the dataset \(\). The stochastic choice of \(_{}\) makes predictive conditionals \(p_{}(_{}|_{})\) to be different for every observation and training step. Once the pre-training of \(\) has converged, this naturally allows the model to capture the underlying structure between dimensions of the data. One additional remark is the number of random masks needed to cover all combinations between _masked_ and observed tokens, which can be obtained as \(_{M}=\). In the particular example of bert, where the masking rate is \(15\%\) with \(D=512\) and \(M=76\), the total number of random masks needed to cover all combinations of tokens is \(_{M} 1.21{}10^{92}\). This shows the inner combinatorial problem behind mpt. We provide empirical results on why this is not a limitation for learning with mpt in Sec. 3.1.

## 3 A probabilistic perspective, theory, and analysis

Our key objective is to demonstrate that the good generalization of mpt can be explained as an equivalence with the model's high marginal likelihood. Indeed, we will prove that mpt _implicitly_ maximizes marginal likelihood according to some latent variable model of the form \(p_{}(|)\).

Marginal likelihood.For our theory, we consider some dataset \(\) consisting of \(n\) i.i.d. observations \(_{1:n}\), where each sample \(_{i}\) could be either continuous or discrete and is of dimensionality \(D\). We also assume that there exists a latent space \(^{K}\) where we can find unobserved variables \(_{1:n}\) which are part of the generative process of the data. This assumption is inspired in the common use of latent encodings in recent models fitted with mpt. In this direction, we also consider the observations to be samples of a likelihood function \(p_{}(|)\), where the mapping between the latent and observed variable is controlled by some parameters \(\), which might also include likelihood or prior _hyperparameters_.

Importantly, we consider the parameters \(\) to be _deterministic_, while we are interested in integrating out the latent variables that we cannot observe. Automatically, this leads us to the log-marginallikelihood (lml) of the model, which may factorize as a sum of marginals and can be also written as \( p_{}(_{1:n})=_{i=1}^{n} p_{}(_{i})\), where the \(i^{}\) probability density comes from the integral \(p_{}(_{i})= p_{}(_{i}|_{i})p(_{i}) _{i}\). This definition coincides with the target lml used in the lower bound of variational autoencoders (vae) (Kingma and Welling, 2013; Rezende et al., 2014) and it is widely used in probabilistic generative models.

Masking and conditional probabilities.From the properties of probability distributions, we can decompose the individual lml functions \( p_{}(_{i})\) as a sum of log-conditionals between _tokens_. Omitting the \(i^{}\) observation subscript in \(\) to keep the notation uncluttered, the sum takes the form

\[ p_{}()=_{t=1}^{D} p_{}(x_{t}|_{t+1: D}).\] (2)

However, the previous sum imposes a particular order on the selection of variables for conditioning, e.g. \(\{x_{1}|x_{2},x_{3},\},\{x_{2}|x_{3},x_{4},\}\), etc. Moreover, the order of tokens in the observation vector remains predetermined, as dimensions are not _exchangeable_. Thus, we can consider a different combination of conditional probabilities in the sum -- for instance, \(\{x_{4}|x_{1},x_{2},\},\{x_{3}|x_{1},x_{2},\}\), etc. Here, the key insight is that the rules of probability applied to the log-marginal likelihood make it _invariant_ to the combination of different conditional factors, as we are observing different views of the same graphical model.

This combinatorial process between tokens in \(\) can be understood as the selection problem of indices. For that reason, we can assume a mask \(\) of the largest size \(||=D\), such that \(\{1,2,,D\}\). Using similar properties of combinatorics, we can also obtain \(D!\) different choices for \(\). While _all_ the indices are always in the set, the order of indices differs between combinations. This principled order in \(\) indicates how we sum the conditional probabilities in Eq. 2.

Since the lml is invariant to random choices of \(\), we can re-write the sum in Eq. 2 as an expectation with a _countable set_ of possible outcomes. Each outcome corresponds to one of the \(D!\) choices for \(\), such that

\[ p_{}()=_{=1}^{D!}_{t=1}^{D} p_{ }(x_{(t)}^{()}|_{(t+1:D)}^{()} )=_{t=1}^{D}_{}[ p_{}(x_{(t) }^{()}|_{(t+1:D)}^{()})],\] (3)

where the superscript \(()\) denotes which mask \(\) are we using for indexing the tokens. We also _swapped_ the order of the sums to obtain the desired expectation in the r.h.s. of the formula.

The role of random masking.If we now take a particular index \((t)\) and we look at the \(^{}\) summand in the previous expression, we can see that the lml is still _invariant_ to the order of the conditioning tokens \(_{(t+1:D)}\) in the log-probabilities: \( p_{}(x_{(t)}|_{(t+1:D)})\) in the sum. Intuitively, we can use both -- \(\{x_{1}|x_{2}\},\{x_{2}\}\) or \(\{x_{2}|x_{1}\},\{x_{1}\}\); independently of the conditional factors previously considered. In practice, this indicates that we can insert a _second set_ of indices to the r.h.s. variables, which is the key point to link negative mpt loss and lml.

Now, assume that \(\) indexes less than \(100\%\) of tokens, while the rest is indexed by \(\) as defined in Sec. 2. If we match both complementary masks to be aligned with the _conditional_ and _conditioning_ variables in the log-probabilities, this allows us to rewrite the \(t^{}\) summands in Eq. 2 as

\[_{=1}^{D!} p_{}(x_{(t)}^{()}| _{(t+1:D)}^{()})=_{=1}^{D!} p_ {}(x_{(t)}^{()}|_{(1:D-t)}^{()} ).\]

Here, we can easily see that there are \(\) choices for the _unmasked_ tokens in the r.h.s. of the conditional distribution, where we have previously fixed the index \(t\). If we set the _binomial_ coefficient \(_{t}\) as the maximum number of choices, we can obtain the following equality

\[_{=1}^{D!} p_{}(x_{(t)}^{()}|_{ (1:D-t)}^{()})=_{=1}^{_{t}}_{j=1}^{D -t+1} p_{}(x_{(j)}^{()}|_{(1:D-t )}^{()}),\] (4)

since \(D!>_{t}\;\; t\{1,2,,D\}\). Notice that once we have chosen a specific order \(()\) in the masking pattern of \(\) and \(\) in Eq. 4, there are still \((D-t+1)\) choices for the _masked_ tokensunder evaluation in the probability distribution. Alternatively, we can think of this method as taking advantage of the properties of probability to split the \(D!\) choices in the order of log-conditionals into the two sums in Eq. 4. The driving idea is then that the two sums in the previous expression still remain _invariant_ given any \(t\{1,2,,D\}\).

Using the previous notion in Eq. 2, we obtained our main result, which holds under the assumption of i.i.d. observations with correlated tokens and the previous definition of the lml as the integral over the stochastic latent variables in the model.

**Proposition 1**: _The cumulative expected loss of masked pre-training along the sizes of the mask of tokens \(M\{1,2,,D\}\) is equivalent to the log-marginal likelihood of the model when using self-predictive conditionals probabilities, such that_

\[ p_{}()=_{M=1}^{D}_{}(;M),\] (5)

_where the score function \(_{}(;M)\) corresponds to_

\[_{}(;M):=_{M}}_{=1}^{ _{M}}_{j=1}^{M} p_{}(x_{(j)}^{ ()}|_{(1:D-j)}^{()})=_{ }[_{j=1}^{M} p_{}(x_{(j)}|_{}) ].\]

_Proof: In the supplementary material._

It is remarkably important to link the sum of log-conditionals \( p_{}(x_{(j)}|_{})\) in our proposition with the main objective used in mpt in Eq. 1. The main message of our result is that the _score function_\(_{}(;t)\) acts as an average over the different random masks. These shape the structure of conditioning in probabilities. The cumulative sum of the score function \(_{}(;t)\) over different sizes of the mpt's mask formally leads to the true value of the model's lml. This result is exact whenever we consider the closed-form self-predictive probabilities of the model and _all_ the possible choices for the masking pattern \(\). Since this is usually not affordable, due to the combinatorial cost and the lack of tractability, we usually have a _biased_ estimator. However, it is still sufficient to prove that mpt maximizes lml during training as we will show later. This point will be discussed in the following empirical studies. Further details on the derivations are included in the supplementary material.

### Formal results in tractable models

To verify that masked pre-training effectively maximizes lml, we need a tractable probabilistic model based on latent variables as the _proof-of-concept_. Probabilistic pca (ppca) (Tipping and Bishop, 1999) is perhaps the best option here, as it has been previously used to understand other empirical observations in generative methods, e.g. posterior _collapse_ in vaes(Lucas et al., 2019), or even considered as the starting point of gplvms(Lawrence, 2005). In particular, the pca model assumes that Gaussian observations map linearly to sets of real-valued latent variables \(_{1:n}\), such that \(=++\), where \((0,_{0}^{2})\). Importantly, the prior is conventionally defined as isotropic, where \(p()=(0,)\). We are therefore interested in the closed form expression of the ppca's lml, which also factorizes across samples as follows

\[ p_{}(_{1:n})=_{i=1}^{n} p_{}(_{i}), \ \ \ p_{}(_{i})=(_{i}|,),\] (6)

and we obtain the covariance matrix using \(=^{}+_{0}^{2}\). For our analysis, the Gaussian nature of \(p_{}(_{i})\) is of fundamental importance. Given the random mask \(\), the self-predictive conditionals used in mpt naturally emerge from the formulation using properties of Gaussian marginals, such that \(p_{}(_{}|_{})=(_{ |},_{|})\) is parameterized according to

\[_{|}=_{}^{}_ {}^{-1}_{},_{| }=_{}+_{}^ {}_{}^{-1}_{},\] (7)

where we split the lml covariance matrix \(\) into the blocks corresponding to the indices included in \(\) and \(\). We use these _mean_ and _variance_ parameters of the self-predictive density to recursivelyevaluate the log-probabilities in Prop. 1. In practice, two elements become critical for the computation, one is the size \(M\) of the _mask_ and another is the number of random masks \(P<_{M}\) considered. These induce a _trade-off_ between accuracy and computational cost. Moreover, their role in approximating lml using a biased estimate is carefully analyzed in the following empirical studies. We also included additional details on the previous derivations in the supplementary material.

Fast asymptotic convergence.Our theory indicates that we should evaluate _all_\(_{M}\) random masks of the tokens to achieve the exact value of the lml. However, even if the combinatorial nature of the sum in the r.h.s. of the last equation in Prop. 1 becomes very large when the dimensionality of data augments, we suspect that it might converge relatively fast to the true value of the lml. This hypothesis would explain why large models that are fitted with standard mpt generalize well using just one random mask per training epoch.

Here, we empirically study if the cumulative mpt loss converges to the _true_ value of the lml under the definition of the ppca model. In particular, to the lml obtained with the original parameters that generated the data. The results in Fig. 1 and Tab. 1 indicate that as long as we average over more random masking patterns, the cumulative mpt loss approximates the lml of the model very well. Thus, having defined a ppca model with a latent space of \(K=2\) dimensions, we observe in the _left_ and _middle_ plots that the asymptotic convergence happens for both small (\(D=5\)) and large (\(D=50\)) number of tokens per observation. Additionally, we observe that the estimation of lml is clearly _unbiased_ if we use the cumulative mpt loss according to Eq. 1, which is an important insight. Notice that \(P=1\) is usually set up in mpt in practice.

Additionally, we tested the tractable model using a dimensionality similar to the input data used in bert(Devlin et al., 2018), where the number of tokens is typically \(D=512\) per observation and the mask rate is fixed to \(15\%\). The fact of fixing the rate of masking in mpt produces that the sum in Eq. 5 is incomplete. Thus, we have a _biased_ estimation of the lml. However, this bias is _known_ and constant during the training of parameters \(\), which does not prevent the general maximization of lml. This point is carefully analyzed in the next empirical study with learning curves. One additional finding here is that as \(P_{M}\), the cumulative mpt loss also converges asymptotically to the _biased_ estimator of the lml as shown in the right plot in Fig. 1.

LML maximization and biased estimation.We next seek to extend the previous study to understand the behavior of the cumulative mpt loss in training curves. So far, we have observed how the number of random mask patterns affects the precision around the _unbiased_ estimation of the

   True lml (\(\)) & \(P=1\) & \(P=10\) & \(P=100\) \\  \((-60.34)\) & \(-60.44 0.47\) & \(-60.22 0.12\) & \(=\)60.34 \(\) 0.03 \\   

Table 1: Evolution of negative mpt loss w.r.t. max. number of random masks \(P\).

Figure 1: Asymptotic convergence of the cumulative mpt loss to lml as the number of random masks \(P\) augments. Curves indicate the relative difference, where \(0.0\) means that mpt equals lml. **(Left).** Each observation consists of 5 tokens. **(Center)** Each observation consists of 50 tokens. **(Right).** Observations have \(512\) tokens and the rate of masking is fixed to \(15\%\) (76 tokens). As the theory indicates, when the size of \(\) is fixed, the cumulative mpt loss becomes a _biased_ estimator of the lml. The curves converge asymptotically to the bias.

lml. Theory and previous empirical results indicate that we are targeting lml or at least a decent _biased_ estimate of lml when averaging self-predictive conditionals as in mpt. However, we still want to examine if this maximizes lml in _all cases_ and under stochastic gradient optimization. This principal hypothesis is confirmed in Fig. 2, where different training curves are shown for different initializations and setups of the same ppca model. The key insight showed by this experiment is that the exact lml is iteratively maximized at each epoch, in parallel with the maximization of the negative mpt loss. On the other side, we also have that mpt is an unbiased stochastic approximation of lml, as in Fig. 1, whenever we consider different rates of random masking \(\). We can also observe that as soon as we fix the size of the mask to index the \(20\%\) of tokens, the mpt loss becomes a _biased_ estimate. Intuitively, this is equivalent to fixing \(M\) in the sum in Eq. 5. Again, it converges to the same value from different initializations of parameters \(\). Additionally, we highlight that the lml is still maximized in this case, which is of high similarity to practical uses in larger models. Overall, this result first confirms the main insight of the work on the link between generalization when using mpt and the maximization of the model's lml.

Beyond tractable models and implicit integration.One remaining question in our analysis is how the probabilistic theory around mpt adapts to intractable or non-linear models. In practice, self-predictive probabilities imply integrating out the latent variables, often given the posterior distribution. In most cases, performing this integration is extremely difficult or not possible in training time. Therefore, we are interested in finding if alternative approximations \(q_{}\) to the _true_ self-conditional probabilities still produce accurate estimation and maximization of the lml. This point is confirmed in Fig. 3. Inspired by the experiments of Lucas et al. (2019) with _linear_ vaes, we set up a Bernoulli likelihood on top of the latent variable model. The tractable formulation in the Gaussian example coincides with ppca. Since predictive conditionals are no longer tractable for us, we use numerical integration to obtain the probabilities of masked tokens. In Fig. 3, we test the training with the cumulative mpt loss as well as compare with standard variational inference using the model's evidence lower bound (elbo). For the _mini_-dataset with mnist samples, we observe that both models converge to a similar value of the lml. Thus, the fundamental insight here is that mpt maximizes lml even under training with approximate self-predictive conditional probabilities. For the lml curves, we also used numerical integration.

Beyond linear models, our theory is useful when applied to non-linear models. Moreover, in Fig. 3 we also include the results for _deep_ vaes based on nns. While the estimation of lml was obtained via Monte Carlo (mc) samples, we used iterative _encoding-decoding_ to produce the self-conditional probabilities for masked tokens -- see Sec. F in Rezende et al. (2014). In this scenario, we also observe the maximization of the lml according to the evolution of the mpt loss.

Another key insight showed by this study is the ability of mpt to perform _implicit_ integration. The cumulative sum over the different rates of random masking is another way to see a discrete integral

Figure 2: Training curves of the negative cumulative mpt loss in ppca vs. the ground truth (gt) lml. The number of samples is \(N=2000\) and the number of tokens is \(D=10\). All plots used \(P=1\) random masks per epoch and five different initializations. **(Left).** The rate of masking is _unfixed_ and it varies from \(1\%\) until \(100\%\). The negative mpt loss converges to the gt-lml (dashed line). Darker curves are the exact lml per epoch. **(Center).** Convergence with _fixed_ mask to \(20\%\) of tokens. The negative mpt loss is no longer centered around the lml and it converges to a _biased_ estimate. **(Right).**_Zoomed_ curves of convergence. The _bias_ is constant and all mpt losses converge to the same point. The lml per epoch is also maximized and converges to gt-lml.

under the curve described by the score function \(_{}(;M)\) in Eq. 5. In Fig. 4, we show the areas under the curve and the effect of reducing the number of random masks \(P\). The blue plots correspond to a trained ppca model and the area corresponds to the lml estimate. The long tail in the right part of the curves, when the rate of masking is larger than \(90\%\), indicates that the model is no longer able to produce good estimates of the tokens with only \(10\%\) of the input dimensions observed. This explains, why the probabilities have an approximately exponential decay. However, this effect is not constant, and it might depend on the latent structure of the model. In the r.h.s. plot we observe that the decay of conditional probabilities happens earlier at approximate \(50\%\) random masking or larger. The role of the masking rate is perhaps the missing part in the picture (Wettig et al., 2022), as it is the one that

Figure 4: Area under the curve described by \(_{}(;M)\). The area is approximately equal to the modelâ€™s lml according to the theory. Larger probability values are obtained with smaller rates of masking. **(Left).** Area described with \(P=1\) random masking per epoch. The curve is more noisy and the area slightly loses precision w.r.t. lml. **(Center). Area under the mpt curve for \(P=100\). **(Right).** Latent space is augmented to be of \(K=50\). Decay of predictive probabilities begins at around \(50\%\) masking rate.

Figure 3: Training curves for linear VAE and deep VAE models with variational inference (vi) and mpt. Data consist of subsets of mnist and fmnist. **(Upper Row).** A linear VAE model with Bernoulli likelihood function in \(N=2000\) samples of mnist and fmnist. Shaded curves correspond to the target losses used in the optimizer (elbo and mpt). Darker lines indicate the evolution of the lml, which are approximated via numerical integration in a latent space \(\) of dimensionality \(K=2\). **(Lower Row).** Vanilla VAE with Gaussian likelihood for mnist. The lml curves are approximated via Monte Carlo (mc) samples. Self-predictive conditional probabilities are obtained via _recursive_ encoding-decoding. The size of the random masking is fixed and set to \(33\%\).

determines the approximation to the lml. With the purpose of providing an intuition on how rates of \(15\%\) or \(85\%\) affect to the area under the curve, we indicate with two black lines the approximate area that approximates the lml. A longer discussion is provided in the supplementary material.

### Applied theory on large language models

In this section, we aim to understand how the area under the mpt curve evolves and behaves for large language models (llms). While the direct computation of the lml is not feasible for non-linear transformer models, we are interested in checking how the rate of masking affects the curve compared with the tractable ppca model. The results provided in Fig. 5 and Tab. 2 give us insights into this behavior. First, we observe that the mpt curve is approximately _flat_ for every rate of masking in the ppca when parameters are randomly initialized. Intuitively, this indicates that the model is not able to correctly predict any token given some context. In some way, it produces noise independently of the number of conditional tokens, which explains the low log-probabilities. Second, we can also notice that the curve changes its shape as more training epochs are considered. The curve after \(600\) epochs produces high probability values for different rates of masking, while the long tail of low probabilities appears when masking more than \(85\%\) of tokens. Moreover, the area under these curves is the estimation of the lml, which accurately converges to the _ground truth_ value of the lml with the original generative parameters.

For the study of the curves in llms, we used four datasets from the General Language Understanding Evaluation (glue) (Wang et al., 2019). Additionally, we consider a \(110\)M parameters bert model using pre-trained checkpoints2 and random initializations. To draw the mpt curves, we computed the mean cross-entropy per each rate of masking between \(1\%\) and \(99\%\). In Fig. 5, we observe that random initializations of bert parameters lead to _flat_ curves of low self-predictive probabilities. On the other hand, the pre-trained curves show similar behavior as in the tractable model, where the area

    &  &  &  & \\  Area / Random init. (\(\)) & \(-5245.31\) & \(-5283.52\) & \(-5343.98\) & \(-5362.21\) \\ Area / Pre-trained (\(\)) & \(-1715.75\) & \(-1657.68\) & \(-1770.28\) & \(-1773.45\) \\   

Table 2: Area under the mpt curve for bert model and four glue datasets.

Figure 5: Evolution of the area under the mpt curve. Comparison between one tractable model (ppca) and bert. The area under the curves is _approximately_ the lml. Random initialization of the parameters produces mpt curves with similar low probabilities for all \(\%\) of masking. As the number of epochs increases, the curve brings higher values of log-probability for lower ratios of masking. The area also converges to the true value of lml. **(Left)**. ppca model trained for \(600\) epochs. Each curve represents \(\{0,100,200,300,400,500,600\}\) epochs of training with mpt. **(Right)**. Random initialization and end-of-pretraining curves for the mpt loss w.r.t. the \(\%\) of masked tokens. Curves are similar but not identical for the \(4\) different datasets given the pre-trained bert model.

is reduced and a long tail of low probabilities happens when the rate of masking becomes larger. This result supports our hypothesis that mpt in llms might be performing implicit integration of the latent space and maximizing the marginal likelihood of the model.

Results on vision models.In addition to the results shown in Fig. 5 with the bert model, we are also interested in the _behavior_ of the theory on large models oriented to _vision_. For this study we provide the curves of the area under the mpt loss for vit-mae (Dosovitskiy et al., 2020; He et al., 2022) with different _masking_ rates. In a similar way as in Sec. 3.2, we use an (already) pre-trained vit-mae model loaded from a public repository. To draw the curves shown in Fig. 6, we computed the losses per _each_ rate of masking between \(5\%\) and \(95\%\) for samples from three different test image datasets (fashion-mnist, cifar-100 and tiny-imagenet). We can observe that the curves described by the pre-trained vit-mae model show a similar behaviour to the one we obtained with bert and they are also _aligned_ with the analysis done in this work with tractable models. We highlight that one must be aware that masking on vision models is often performed via _patches_. In practice, this could affect the expectation in Proposition 1, so this point should be taken into consideration if theory is applied to this case.

**Reproducibility.** All the empirical studies and results are _reproducible_. We provide the code and details for every figure in the public repository at https://github.com/pmorenoz/MPT-LML/.

## 4 Related work

Masked pre-training and large scalability are key elements of the current success of transformer models (Vaswani et al., 2017) on natural language processing (nlp) tasks. Vision transformers (vit) (Dosovitskiy et al., 2020) bridged the architectural gap between nlp and computer vision, making masked language modeling (mlm) suitable for images. In this regard, beit(Bao et al., 2022) adopted vit and proposed to mask and predict discrete visual tokens. Most recently, masked autoencoders (He et al., 2022) also adopted masked pre-training by predicting pixel values for each masked patch, and beit3(Wang et al., 2022) performs mlm on texts, images, and image-text pairs, obtaining state-of-the-art performance on all-vision and vision-language tasks. Additionally, masked pre-training has been successfully adapted to video (Tong et al., 2022), where random temporal cubes are iteratively masked and reconstructed.

The surprising ability of recent generative models to generalize and do impressive in-context learning has inspired earlier works to study this phenomenon from the Bayesian lens. The notion that llms might be performing _implicit_ Bayesian inference was first described in Xie et al. (2021) where in-context learning is described as a mixture of hmms. However, the equivalence between the log-marginal likelihood and _exhaustive_ cross-validation was first provided in Fong and Holmes (2020). Earlier works (Vehtari and Lampinen, 2002; Gelman et al., 2014) also provided a Bayesian perspective of cv. Additionally, Moreno-Munoz et al. (2022) leveraged this link for training Gaussian process models according to a stochastic approximation to the marginal likelihood. Similarly to current masked pre-training, the size of the conditioning variable (masking rate) was held constant. This was reported to improve notably upon traditional variational lower bounds.

Figure 6: Evolution of the area under the mpt curve. Comparison between three different datasets with ViT-mae. The area under the curves is approximately the lml. Random initialization of the parameters produces mpt curves with similar low probabilities for all % of masking. As the number of epochs increases, the curve brings higher values of log-probability for lower ratios of masking. The area also converges to the true value of lml.

Discussion and outlook

In this paper, we have shown that masked pre-training implicitly performs stochastic maximization of the model's marginal likelihood. The latter is generally acknowledged as being an excellent measure of a model's ability to generalize (Fong and Holmes, 2020), and our results help to explain the strong empirical performance associated with masked pre-training. We have further seen that the developed theory matches the empirical training behavior well. Moreover, we illustrated the role that the rates and the number of random samples of masking play in the estimation of the lml. We have also provided insights and a new perspective to study masked pre-training in tractable models while also finding strong similarities with llms.

Limitations.We have developed a formal probabilistic theory that links masked pre-training with the Bayesian principles. While we provide evidence that the impressive performance in recent large models is related to the maximization of the marginal likelihood, these methods usually introduce new elements of improvement but may not entirely fit the propositions provided in this work. In practice, this is not a limitation but a remark that there is still room for understanding the abilities of recent generative modeling. In this regard, one example might be autoregressive modeling between the masked tokens. While these are not currently analyzed in our work, we hypothesize that they could also be linked in further development to our formal propositions.

Relevance for large models using masked pre-training.We have shown empirical results of the connection between mpt and lml. This link sheds light on the understanding of generalization, particularly in recent pre-trained models. One positive outcome of our studies is the notion of having _biased_ Bayesian estimators whenever a practitioner fixes the masking rate, e.g. to \(15\%\). Currently, there is a significant interest in the role of masking rates in llms(Wettig et al., 2022). These studies could benefit from the insights provided in this paper. We also argue that the theory offers _hints_ that may be beneficial, for instance, for uniformly sampling the mask size, instead of the current fixed-rate practice. This practice is empirically shown in the supplementary material, and it leads to _unbiased_ estimation which may result in better performance for certain scenarios (Tay et al., 2023).

Relevance for Bayesian models.Current Bayesian modeling is dominated by approximate methods. Variational inference foregoes the ambition of training according to the marginal likelihood and instead resorts to bounds thereof. This inherently yields suboptimal models. Our theory suggests that if we can design Bayesian models in which conditioning is _cheap_, then we can stochastically optimize w.r.t. the true marginal likelihood easily. Beyond shedding light on the success of masked pre-training, the theory also suggests that large-scale Bayesian models could be successfully trained in the future with appropriately designed self-supervision.