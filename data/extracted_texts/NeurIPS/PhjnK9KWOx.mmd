# PSL: Rethinking and Improving Softmax Loss from Pairwise Perspective for Recommendation

Weiqin Yang

Zhejiang University

tinysnow@zju.edu.cn

&Jiawei Chen

Corresponding author.

&Xin Xin

Zhejiang University

sleepyhunt@zju.edu.cn

&Xin Xin

Shandong University

xinxin@sdu.edu.cn

&Sheng Zhou

Zhejiang University

zhousheng_zju@zju.edu.cn

&Binbin Hu

Ant Group

bin.hbb@antfin.com

&Yan Feng

Zhejiang University

fengyan@zju.edu.cn

&Chun Chen

Zhejiang University

chenc@zju.edu.cn

&Zhejiang University

wcan@zju.edu.cn

Corresponding author.State Key Laboratory of Blockchain and Data Security, Zhejiang University. College of Computer Science and Technology, Zhejiang University. Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security.

###### Abstract

Softmax Loss (SL) is widely applied in recommender systems (RS) and has demonstrated effectiveness. This work analyzes SL from a pairwise perspective, revealing two significant limitations: 1) the relationship between SL and conventional ranking metrics like DCG is not sufficiently tight; 2) SL is highly sensitive to false negative instances. Our analysis indicates that these limitations are primarily due to the use of the exponential function. To address these issues, this work extends SL to a new family of loss functions, termed Pairwise Softmax Loss (PSL), which replaces the exponential function in SL with other appropriate activation functions. While the revision is minimal, we highlight three merits of PSL: 1) it serves as a tighter surrogate for DCG with suitable activation functions; 2) it better balances data contributions; and 3) it acts as a specific BPR loss enhanced by Distributionally Robust Optimization (DRO). We further validate the effectiveness and robustness of PSL through empirical experiments. The code is available at https://github.com/Tiny-Snow/IR-Benchmark.

## 1 Introduction

Nowadays, recommender systems (RS) have permeated various personalized services . What sets recommendation apart from other machine learning tasks is its distinctive emphasis on ranking . Specifically, RS aims to retrieve positive items in higher ranking positions (i.e., giving larger prediction scores) over others and adopts specific ranking metrics (e.g., DCG  and MRR ) to evaluate its performance.

The emphasis on ranking inspires a surge of research on loss functions in RS. Initial studies treated recommendation primarily as a classification problem, utilizing pointwise loss functions (e.g., BCE , MSE ) to optimize models. Recognizing the inherent ranking nature of RS, pairwise lossfunctions (e.g., BPR ) were introduced to learn a partial ordering among items. More recently, Softmax Loss (SL)  has integrated contrastive learning paradigms [12; 13], augmenting positive items as compared with negative ones, achieving state-of-the-art (SOTA) performance.

While SL has proven effective, it still suffers from **two limitations**: 1) SL can be used to approximate ranking metrics, e.g., DCG and MRR [11; 14], but their relationships are not sufficiently tight. Specifically, SL uses the exponential function \(()\) as the _surrogate activation_ to approximate the Heaviside step function in DCG, resulting in a notable gap, especially when the surrogate activation takes larger values. 2) SL is sensitive to noise (e.g., false negatives ). Gradient analysis reveals that SL assigns higher weights to negative instances with large prediction scores, while the weights are rather skewed and governed by the exponential function. This characteristic renders the model highly sensitive to false negative noise. Specifically, false negative instances are common in RS, as a user's lack of interaction with an item might stem from unawareness rather than disinterest [16; 17; 18]. These instances would receive disproportionate emphasis, potentially dominating the training direction, leading to performance degradation and training instability.

To address these challenges, we propose a new family of loss functions, termed **Pairwise Softmax Loss (PSL)**. PSL first reformulates SL in a pairwise manner, where the loss is applied to the score gap between positive-negative pairs. Such pairwise perspective is more fundamental to recommendation as the ranking metrics are also pairwise dependent. Recognizing that the primary weakness of SL lies in its use of the exponential function, PSL replaces this with other surrogate activations. While this extension is straightforward, it brings significant theoretical merits:

* **Tighter surrogate for ranking metrics.** We establish theoretical connections between PSL and conventional ranking metrics, e.g., DCG. By choosing appropriate surrogate activations, such as ReLU or Tanh, we demonstrate that PSL achieves a tighter DCG surrogate loss than SL.
* **Control over the weight distribution.** PSL provides flexibility in choosing surrogate activations that control the weight distribution of training instances. By substituting the exponential function with an appropriate surrogate activation, e.g., ReLU or Tanh, PSL can mitigate the excessive impact of false negatives, thus enhancing robustness to noise.
* **Theoretical connections with BPR loss.** Our analyses reveal that optimizing PSL is equivalent to performing Distributionally Robust Optimization (DRO)  over the conventional pairwise loss BPR . DRO is a theoretically sound framework where the optimization is not only on a fixed empirical distribution but also across a set of distributions with adversarial perturbations. This DRO characteristic endows PSL with stronger generalization and robustness against out-of-distribution (OOD), especially given that such distribution shifts are common in RS, e.g., shifts in user preference and item popularity [16; 20; 21].

Our analyses underscore the theoretical effectiveness and robustness of PSL. To empirically validate these advantages, we implement PSL with typical surrogate activations (Tanh, Atan, ReLU) and conduct extensive experiments on four real-world datasets across three experimental settings: 1) IID setting  where training and test distributions are identically distributed ; 2) OOD setting  with distribution shifts in item popularity; 3) Noise setting  with a certain ratio of false negatives. Experimental results demonstrate the superiority of PSL over existing losses in terms of recommendation accuracy, OOD robustness, and noise resistance.

## 2 Preliminaries

**Task formulation.** We will conduct our discussion in the scope of collaborative filtering (CF) , a widely-used recommendation scenario. Given the user set \(\) and item set \(\), CF dataset \(\) is a collection of observed interactions, where each instance \((u,i)\) means that user \(u\) has interacted with item \(i\) (e.g., clicks, reviews, etc). For each user \(u\), we denote \(_{u}=\{i:(u,i)\}\) as the set of positive items of \(u\), while \(_{u}\) represents the negative items.

The goal of recommendation is to learn a recommendation model, or essentially a scoring function \(f(u,i):\) that quantifies the preference of user \(u\) on item \(i\) accurately. Modern RS often adopts an embedding-based paradigm . Specifically, the model maps user \(u\) and item \(i\) into \(d\)-dim embeddings \(,^{d}\), and predicts their preference score \(f(u,i)\) based on embedding similarity. The cosine similarity is commonly utilized in RS and has demonstrated particular effectiveness . Here we set \(f(u,i)=}{\|\|\|\|} \), where the scaling factor \(\) is introduced for facilitating analyses and can be absorbed into the temperature hyperparameter (\(\)). The scores \(f(u,i)\) are subsequently utilized to rank items for generating recommendations.

**Ranking metrics.** The Discounted Cumulative Gain (DCG)  is a prominent ranking metric for evaluating the recommendation quality. Formally, for each user \(u\), DCG is calculated as follows:

\[(u)=_{i_{u}}(1+_{u}(i))}\] (2.1)

where \(_{u}(i)\) is the ranking position of item \(i\) in the ranking list sorted by the scores \(f(u,i)\). DCG quantifies the cumulative gain of positive items, discounted by their ranking positions. Similarly, the Mean Reciprocal Rank (MRR) [7; 28] is another popular ranking metric using the reciprocal of the ranking position as the gain, i.e., \((u)=_{i_{u}}1/_{u}(i)\). Additionally, other metrics such as Recall , Precision , and AUC  are also utilized in RS . Compared to these metrics, DCG and MRR focus more on the top-ranked recommendations, thus attracting increasing attention in RS [11; 31]. In this work, we aim to explore the surrogate loss for DCG and MRR.

**Recommendation losses.** To train recommendation models effectively, a series of recommendation losses has been developed. Recent work on loss functions can mainly be classified into three types:

* **Pointwise loss** (e.g., BCE , MSE , etc.) formulates recommendation as a specific classification or regression task, and the loss is applied to each positive and negative instance separately. Specifically, for each user \(u\), the pointwise loss is defined as \[_{}(u)=-_{i_{u}}(^{+} (f(u,i)))-_{j_{u}}(^{-}(f(u,j)))\] (2.2) where \(^{+}()\) and \(^{-}()\) are the activation functions adapted for different loss choices.
* **Pairwise loss** (e.g., BPR , etc.) optimizes partial ordering among items, which is applied to the score gap between negative-positive pairs. BPR  is a representative pairwise loss, which is defined as \[_{}(u)=_{i_{u}}_{j _{u}}(f(u,j)-f(u,i))\] (2.3) where \(\) denotes the activation function that approximates the Heaviside step function. The basic intuition behind BPR loss is to let the positive instances have higher scores than negative instances. In practice, there are various choices of the activation function. For instance, Rendle et al.  originally uses the sigmoid function, and the resultant BPR loss can approximate AUC metric.
* **Softmax Loss** (i.e., SL ) normalizes the predicted scores into a multinomial distribution  and optimizes the probability of positive instances over negative ones , which is defined as \[_{}(u)=-_{i_{u}}(}(f(u,j)/)})\] (2.4) where \(\) is the temperature hyperparameter. SL can also be understood as a specific contrastive loss, which draws positive instances \((u,i)\) closer and pushes negative instances \((u,j)\) away .

## 3 Analyses on Softmax Loss from Pairwise Perspective

In this section, we aim to first represent the Softmax Loss (SL) in a pairwise form, followed by an analysis of its relationship with the DCG metric, where two limitations of SL are exposed.

**Pairwise form of SL.** To facilitate the analysis of SL and to build its relationship with the DCG metric, we rewrite SL (cf. Equation (2.4)) in the following pairwise form:

\[_{}(u)=_{i_{u}}(_{j }(d_{uij}/)),d_{uij}=f(u,j)-f(u,i)\] (3.1)

Equation (3.1) indicates that SL is penalized based on the score gap between negative-positive pairs, i.e., \(d_{uij}=f(u,j)-f(u,i)\). This concise expression is fundamental for ranking, as it optimizes the relative order of instances rather than their absolute values.

**Connections between SL and DCG.** We now analyze the connections between SL and the DCG metric (cf. Equations (2.1) and (3.1)), which could enhance our understanding of the advantages and disadvantages of SL. Our analysis follows previous work [11; 14], which begins by relaxing the negative logarithm of DCG with

\[-(u)+|_{u}|-( _{u}|}_{i_{u}}(i)})_{u}|}_{i_{u}}_{u}(i)\] (3.2)

where the first inequality holds due to \(_{2}(1+_{u}(i))_{u}(i)\), and the second inequality holds due to Jensen's inequality . Note that the ranking position \(_{u}(i)\) of item \(i\) can be expressed as

\[_{u}(i)=_{j}(f(u,j) f(u,i))=_{j }(d_{uij})\] (3.3)

where \(()\) denotes the Heaviside step function, with \((x)=1\) for \(x 0\) and \((x)=0\) for \(x<0\). Since \((d_{uij})(d_{uij}/)\) holds for all \(>0\), we deduce that SL is a smooth upper bound of Equation (3.2), and thus serves as a reasonable surrogate loss for DCG and MRR metrics1.

However, our analysis also reveals **two limitations of SL**:

* **Limitation 1: SL is not tight enough as a DCG surrogate loss.** There remains a significant gap between the Heaviside step function \(()\) and the exponential function \(()\), especially when \(d_{uij}\) reaches a relatively large value, where \(()\) becomes substantially larger than \(()\). This gap is further exacerbated by the temperature \(\). Practically, we find that the optimal \(\) is usually chosen to be less than 0.2 (cf. Appendix B.5.2). Given the explosive nature of \(()\), the gap becomes extremely large, potentially leading to suboptimal performance of SL in optimizing DCG.
* **Limitation 2: SL is highly sensitive to noise (e.g., false negative instances).** False negative instances  are common in the typical RS. This is often due to the exposure bias , where a user's lack of interaction with an item might stem from unawareness rather than disinterest. Unfortunately, SL is highly sensitive to these false negative instances. On one hand, these instances \((u,j)\), which may exhibit patterns similar to true positive ones, are difficult for the model to differentiate and often receive larger predicted scores, thus bringing potentially larger \(d_{uij}\) for positive items \(i\). As analyzed in Limitation 1, these instances can significantly enlarge the gap between SL and DCG due to the exponential function, causing the optimization to deviate from the DCG metric.

**Gradient analysis of SL.** Another perspective to support the view of Limitation 2 comes from the gradient analysis. Specifically, the gradient of SL w.r.t. \(d_{uij}\) is

\[_{}(u)}{ d_{uij}}=/)/}{||_{j^{}}[(d_{uij }/)]}(d_{uij}/)/\] (3.4)

As can be seen, SL implicitly assigns a weight to the gradient of each negative-positive pair, where the weight is proportional to \((d_{uij}/)\). This suggests that instances with larger \(d_{uij}\) will receive larger weights. While this property may be desirable for hard mining , which can accelerate convergence, it also means that false negative instances, which typically have larger \(d_{uij}\), will obtain disproportionately large weights, as shown in the weight distribution of SL in Figure 0(b). Therefore, the optimization of SL can be easily dominated by false negative instances, leading to performance drops and training instability.

**Discussions on DRO robustness and noise sensitivity.** Recent work  claims that SL exhibits robustness to noisy data through Distributionally Robust Optimization (DRO) . However, we argue that this is not the case. DRO indeed can enhance model robustness to distribution shifts, but it also increases the risk of noise sensitivity, as demonstrated by many studies on DRO [35; 36]. Intuitively, DRO emphasizes hard instances with larger losses, making noisy data contribute more rather than less to the optimization. This is also demonstrated from the experiments with false negative instances (cf. Figure 8 in ), where the improvements of SL over other baselines in Noise setting do not increase significantly but sometimes decay.

## 4 Methodology

### Pairwise Softmax Loss

Recognizing the limitations of SL, particularly its reliance on the unsatisfactory exponential function, we propose to extend SL with a more general family of losses, termed **Pairwise Softmax Loss (PSL)**. In PSL, the exponential function \(()\) is replaced by other _surrogate activations_\(()\) approximating the Heaviside step function \(()\). For each user \(u\), the PSL is defined as

\[_{}(u)=_{i_{u}}(_{j }(d_{uij})^{1/})\] (4.1)

One might wonder why we apply the temperature outside the activation function (i.e., extending \((d_{uij})^{1/}\) to \((d_{uij})^{1/}\) )2 rather than within it (i.e., extending \((d_{uij}/)\) to \((d_{uij}/)\)). This subtlety will be elucidated later as we demonstrate that the form in Equation (4.1) offers superior properties over the alternative.

Our PSL provides a flexible framework for selecting better activation functions, allowing the loss to exhibit improved properties compared to SL. We advocate for three activations, including **PSL-tanh**: \(_{}=(d_{uij})+1\), **PSL-atan**: \(_{}=(d_{uij})+1\), and **PSL-relu**: \(_{}=(d_{uij}+1)\). In the following, we will discuss the advantages of PSL and provide evidence for the selection of these surrogate activations.

**Advantage 1: PSL is a better surrogate for ranking metrics.** To highlight the advantages of replacing \(()\) with alternative surrogate activations, we present the following lemma:

**Lemma 4.1**.: _If the condition_

\[(d_{uij})(d_{uij})(d_{uij})\] (4.2)

_is satisfied for any \(d_{uij}[-1,1]\), then PSL serves as a tighter DCG surrogate loss compared to SL._

The proof is presented in Appendix A.1. This lemma reveals that PSL could be a tighter surrogate loss for DCG compared to SL. Additionally, it provides guidance on the selection of a proper surrogate activation -- we may choose the activation that lies between \(()\) and \(()\). As demonstrated in Figure 0(a), our chosen surrogate activations \(_{}\), \(_{}\), and \(_{}\) adhere to this principle.

**Advantage 2: PSL controls the weight distribution.** The gradient of PSL w.r.t. \(d_{uij}\) is

\[_{}(u)}{ d_{uij}}=(d_{uij})(d_{uij})^{1/-1}/}{|| _{j^{}}[(d_{uij^{}})^{1/}]} ^{}(d_{uij})(d_{uij})^{1/-1}/\] (4.3)

Figure 1: (a) Illustration of different surrogate activations. (b) The weight distribution of SL as compared with PSL using three different surrogate activations. Here we set \(=0.2\), which typically achieves optimal results in practice.

This implies that the shape of the weight distribution is determined by the choice of surrogate activation. By selecting appropriate activations, PSL can better balance the contributions of instances during training. For example, the three activations advocated before can explicitly mitigate the explosive issue on larger \(d_{uij}\) (cf. Figure 0(b)), bringing better robustness to false negative instances.

One might argue that adjusting \(\) in SL could improve noise resistance. However, such adjustments do not alter the fundamental shape of the weight distribution, which remains exponential. Furthermore, as we discuss subsequently, \(\) plays a crucial role in controlling robustness against distribution shifts. Thus, indiscriminate adjustments to \(\) may compromise out-of-distribution (OOD) robustness.

**Advantage 3: PSL is a DRO-empowered BPR loss.** We establish a connection between PSL and BPR  based on Distributionally Robust Optimization (DRO) . Specifically, optimizing PSL is equivalent to applying a KL divergence DRO on negative item distribution over BPR loss (cf. Equation (2.3)), as demonstrated in the following theorem3:

**Theorem 4.2**.: _For each user \(u\) and its positive item \(i\), let \(P=P(j|u,i)\) be the uniform distribution over \(\). Given a robustness radius \(>0\), consider the uncertainty set \(\) consisting of all perturbed distributions \(Q=Q(j|u,i)\) satisfying: (i) \(Q\) is absolutely continuous w.r.t. \(P\), i.e., \(Q P\); (ii) the KL divergence between \(Q\) and \(P\) is constrained by \(\), i.e., \(D_{}(Q\|P)\). Then, optimizing PSL is equivalent to performing DRO over BPR loss, i.e.,_

\[_{i_{u}}[_ {j}[e^{((d_{uij}))/}]]\} \}_{_{}(u)}_{i_{u}}[_{Q}_{j Q (j|u,i)}[(d_{uij})]]\}}_{_{}(u)}\] (4.4)

_where \(=()\) is a temperature parameter controlled by \(\)._

The proof is presented in Appendix A.2. Theorem 4.2 demonstrates how PSL, based on the DRO framework, is inherently robust to distribution shifts. This robustness is particularly valuable in RS, where user preference and item popularity may shift significantly. Therefore, PSL can be regarded as a robust generalization of BPR loss, offering better performance in OOD scenarios.

In addition, Theorem 4.2 also gives insights into the **rationality of PSL** that differs from serving as a DCG surrogate loss, but rather as a DRO-empowered BPR loss:

* **Rationality of surrogate activations:** The activation function in BPR is originally chosen as an approximation to the Heaviside step function . Since PSL is a generalization of BPR as stated in Theorem 4.2, it is reasonable to select the activations in PSL that aligns with the ones in BPR. Interestingly, this principle coincides with our analysis from the perspective of DCG surrogate loss.
* **Rationality of the position of temperature:** Theorem 4.2 also rationalizes the extension form that places the temperature on the outside rather than inside. For the outside form (i.e., \((d_{uij})^{1/}\)), Theorem 4.2 holds, and the temperature \(\) can be interpreted as a Lagrange multiplier in DRO optimization, which controls the extent of distribution perturbation. However, for the inside form (i.e., \((d_{uij}/)\)), Theorem 4.2 no longer holds, and it would be challenging to establish the relationship between PSL and BPR.
* **Rationality of pairwise perspective:** Recent work such as BSL  also reveals the DRO property of SL (cf. Lemma 1 in ). However, we wish to highlight the distinctions between Theorem 4.2 and Wu et al. 's analyses: 1) Wu et al.  views SL from a pointwise perspective and associates it with a specific, less commonly used pointwise loss. In contrast, our analyses adopt a pairwise perspective and establish a relationship between PSL and the widely used BPR loss. 2) We construct a link between two families of losses with flexible activation selections, and Wu et al. 's analyses can be regarded as a special case within our broader framework.

The above analyses underscore the advantages of PSL and provide the principles to select surrogate activations. Remarkably, PSL is easily implemented and can be integrated into various recommendation scenarios. This can be achieved by merely replacing the exponential function \(()\) in SL with another activation \(()\) surrogating the Heaviside step function, requiring minimal code modifications.

### Discussions

**Comparisons of two extension forms.** In previous discussions, we highlight the advantages of the form that positions the temperature outside (i.e., \((d_{uij})^{1/}\)) over the inside (i.e., \((d_{uij}/)\)). As discussed in the analyses of Theorem 4.2, the outside form can be regarded as a DRO-empowered BPR, while the inside form cannot, which ensures the robustness of PSL against distribution shifts.

Here we provide an additional perspective on the advantages of the outside form. In fact, the outside form facilitates the selection of surrogate activations. For instance, to ensure that PSL serves as a tighter DCG surrogate loss compared to SL (i.e., ensure Lemma 4.1 holds), the outside form only need to consider the condition (4.2) on the range of \(d_{uij}[-1,1]\). However, for the inside form, this condition should be satisfied on the entire domain of the activation \(()\), which complicates the selection of activation functions. Therefore, the outside form is more flexible and easier to implement. We further provide empirical evidence in Appendix C.3, demonstrating that the inside form will lose the advantages of achieving tighter DCG surrogate loss, leading to compromised performance.

**Connections with other losses.** We further discuss the connections between PSL and other losses:

* **Connection with AdvInfoNCE :** According to Theorem 3.1 in Zhang et al. , AdvInfoNCE can indeed be considered as a special case of PSL with \(()=(())\). We argue that this activation is not a good choice as it would enlarge the gap between the loss and DCG. In fact, we have \(-_{}_{} _{}\) (cf. Appendix A.3 for proof). While AdvInfoNCE may achieve good performance in some specific OOD scenarios as tested in Zhang et al. , we argue that AdvInfoNCE is a looser DCG surrogate loss and would be highly sensitive to noise (cf. Table 1 and Figure 2 in Section 5.2 for empirical validation).
* **Connection with BPR :** Besides the DRO relation stated in Theorem 4.2, we also derive the bound relation between BPR and PSL with the same activation, i.e., \(-_{}_{}\) (cf. Appendix A.3 for proof). This relation clearly demonstrates the effectiveness of PSL over BPR -- performing DRO over BPR results robustness to distribution shifts, while also achieving a tighter surrogate of DCG, which is interesting (cf. Tables 1 and 2 in Section 5.2 for empirical validation). An intuitive explanation is that DCG focuses more on the higher-ranked items. Given that DRO would give more weight to the hard negative instances with larger prediction scores and higher positions, it would naturally narrow the gap between BPR and DCG.

## 5 Experiments

### Experimental Setup

**Testing scenarios.** We adopt three representative testing scenarios to comprehensively evaluate model accuracy and robustness, including: 1) **IID setting:** the conventional testing scenario where training and test data are randomly split and identically distributed; 2) **OOD setting:** to assess the model's robustness on the out-of-distribution (OOD) data, we adopt a debiasing testing paradigm where the item popularity distribution shifts. We closely refer to Zhang et al. , Wang et al. , and Wei et al. , sampling a test set where items are uniformly distributed while maintaining the long-tail nature of the training dataset; 3) **Noise setting:** to evaluate the model's sensitivity to noise, following Wu et al. , we manually impute a certain proportion of false negative items in the training data. The details of the above testing scenarios are provided in Appendix B.1.

**Datasets.** Four widely-used datasets including Amazon-Book, Amazon-Electronic, Amazon-Movie [40; 41], and Gowalla  are used in our experiments. Considering the item popularity is not heavily skewed in the Amazon-Book and Amazon-Movie datasets, we turn to other conventional datasets, Amazon-CD [40; 41] and Yelp2018 , as replacements for OOD testing. All datasets are split into 80% training set and 20% test set, with 10% of the training set further treated as the validation set. The details of the above datasets are summarized in Appendix B.1.

**Metrics.** We closely refer to Wu et al.  and Zhang et al. , adopting Top-\(K\) metrics including \(@K\) and \(@K\) for performance evaluation, where NDCG is the normalized DCG, i.e., dividing DCG by the ideal value. Here we simply set \(K=20\) as in recent work [15; 38] while observing similar results with other choices. For more details, please refer to Appendix B.2.

[MISSING_PAGE_FAIL:8]

metrics, thus achieving better NDCG performance (cf. Lemma 4.1). This is also empirically evident from the larger improvements in NDCG compared to Recall. In contrast, as discussed in Section 4.2, other baselines like AdvInfoNCE and BSL either widen the gap or fail to connect with the ranking metrics, resulting in slight improvements or even performance drops.

**Results under OOD setting.** Table 2 presents the results in OOD scenarios with popularity shift. Given the consistent behavior across the three backbones, here we only report the results on MF.

* **PSL is robust to distribution shifts.** Experimental results indicate that PSL has a strong robustness against distribution shifts, which is consistent with PSL's Advantage 3 in Section 4. As can be seen, PSL not only outperforms all baselines (2%-5%), but also achieves more pronounced improvements than in IID setting, like on Amazon-Electronic (2.31% \(\) 5.02%) and Gowalla (1.42% \(\) 2.02%). This demonstrates the superior robustness of PSL to distribution shifts, as shown in Theorem 4.2.
* **PSL is a DRO-enhancement of more reasonable loss.** Although both PSL and SL can be considered as DRO-enhanced losses (cf. Theorem 4.2), the original loss of our three PSLs before DRO-enhancement is more reasonable than that of SL, which degenerates from BPR loss to a linear triplet loss . Therefore, we observe significant improvements of PSL over SL.

**Results under Noise setting.** Figure 2 and Appendix C.1 presents the results with a certain ratio of imputed false negative noise. Specifically, we regard 10% of the positive items in the training set as false negative noise and allow the negative sampling procedure to have a certain probability \(p\) of sampling those items. We test the model performance with varying noise ratios \(p\{0.05,0.1,0.2,0.3,0.5\}\).

* **PSL has strong noise resistance.** Experimental results demonstrate that as the noise ratio \(p\) increases, both the performance of SL and PSL decline. The performance decline rate of PSL is significantly smaller than that of other baselines, resulting in higher performance enhancement(\(>10\%\) when \(p=0.5\)). These results indicate that PSL possesses stronger noise resistance than SL, which stems from our rational activation design, as discussed in PSL's Advantage 2 in Section 4.

    &  &  &  &  \\   & **Recall** & **NDCG** & **Recall** & **NDCG** & **Recall** & **NDCG** & **Recall** & **NDCG** \\  BPR  & 0.0518 & 0.0318 & 0.0132 & 0.0069 & 0.0382 & 0.0273 & 0.0118 & 0.0072 \\ LLPAUC  & 0.1103 & 0.0764 & 0.0225 & 0.0134 & 0.0729 & 0.0522 & 0.0324 & 0.0210 \\ SL- & 0.1184 & 0.0815 & 0.0230 & 0.0142 & 0.1006 & 0.0737 & 0.0349 & 0.0224 \\ AdvInfoNCE  & 0.1189 & 0.0818 & 0.0228 & 0.0139 & 0.0927 & 0.0676 & 0.0348 & 0.0223 \\ BSL  & 0.1184 & 0.0815 & 0.0231 & 0.0142 & 0.1006 & 0.0738 & 0.051 & 0.0225 \\ PSL-tanh & 0.1202 & 0.0834 & 0.0239 & 0.0146 & 0.1013 & 0.0748 & 0.0357 & 0.0228 \\ PSL-atan & 0.1202 & **0.0835** & 0.0239 & 0.0146 & 0.1013 & 0.0748 & **0.0358** & 0.0228 \\ PSL-relu & **0.203** & **0.0839** & **0.0241** & **0.0149** & **0.014** & **0.0752** & **0.0358** & **0.0229** \\ 
**Imp.\%** &  &  &  &  \\   

Table 2: Performance comparison in terms of Recall@20 and NDCG@20 under the OOD setting with popularity shift (on MF backbone). The best result is bolded, and the blue-colored zone indicates that PSL is better than SL. Imp.% denotes the NDCG@20 improvement of PSL over SL. The marker ”*” indicates that the improvement is statistically significant (\(p\)-value \(<0.05\)).

Figure 2: Performance comparison of SL and PSL in terms of NDCG@20 with different false negative noise ratio (on MF backbone). We also present the relative improvements (i.e., Imp.%) achieved by PSL over SL. The complete results of other baselines are provided in Appendix C.1.

However, for DRO-enhanced losses such as AdvInfoNCE, the performance declines similarly to or even more quickly than SL (cf. Appendix C.1), which coincides with our theoretical analyses.

## 6 Related Work

**Model-related recommendation research.** Recent years have witnessed flourishing publications on collaborative filtering (CF) models. The earliest works are mainly extensions of Matrix Factorization , building more complex interactions between embeddings , such as MF , LRML , SVD [49; 50], SVD++ , NCF , etc. In recent years, given the effectiveness of Graph Neural Networks (GNNs) [52; 53; 54; 55; 56; 57; 58] in capturing high-order relations, which align well with CF assumptions, GNN-based models have emerged and achieved great success, such as LightGCN , NGCF , LCF , APDA , etc. Building upon LightGCN, some works attempt to introduce contrastive learning [12; 61] for graph data augmentation, such as SGL  and XSimGCL , achieving SOTA performance in recommendation.

**Loss-related recommendation research.** Existing recommendation losses can be primarily categorized into pointwise loss [8; 9], pairwise loss , and Softmax Loss (SL) , as discussed in Section 2. Given the effectiveness of SL, recently some researchers have proposed to enhance SL from different perspectives. For instance, BSL  aims to enhance the positive distribution robustness by leveraging Distributionally Robust Optimization (DRO); AdvInfoNCE  employs adversarial learning to enhance SL's robustness; Zhang et al.  suggests incorporating bias-aware margins in SL to tackle popularity bias. Beyond these three types of losses, other approaches have also been explored in recent years. For example, Zhao et al.  introduces auto-loss, which utilizes automated machine learning techniques to search the optimal loss; Shi et al.  proposes LLPAUC to approximate Recall@\(K\) metric. The main concerns with these losses are their lack of theoretical connections to ranking metrics like DCG, which may result in them not consistently outperforming the basic SL. Moreover, both auto-loss and LLPAUC require iterative learning, leading to additional computational time and increased instability.

## 7 Conclusion and Limitations

In this work, we introduce a new family of loss functions, termed Pairwise Softmax Loss (PSL). PSL theoretically offers three advantages: 1) it serves as a better surrogate for ranking metrics with appropriate surrogate activations; 2) it allows flexible control over the distribution of the data contribution; 3) it can be interpreted as a specific BPR loss enhanced by Distributionally Robust Optimization (DRO). These properties demonstrate that PSL has greater effectiveness and robustness compared to Softmax Loss. Our extensive experiments across three testing scenarios validate the superiority of PSL over existing methods.

One limitation of both PSL and SL is inefficiency, as they require sampling a relatively large number of negative instances per iteration. How to address this issue and improve the efficiency of these losses is an interesting direction for future research.