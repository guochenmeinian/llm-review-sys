# Toward Efficient Inference for Mixture of Experts

Haiyang Huang\({}^{1}\) Newsha Ardalani\({}^{2}\) Anna Sun\({}^{2}\) Liu Ke\({}^{3*}\)

Hsien-Hsin S. Lee\({}^{4*}\) Shruti Bhosale\({}^{2}\)  Carole-Jean Wu\({}^{2}\) Benjamin Lee\({}^{5*}\)

\({}^{1}\)Duke University \({}^{2}\)FAIR at Meta \({}^{3}\)Washington University in St. Louis

\({}^{4}\)Intel Corporation \({}^{5}\) University of Pennsylvania

hyhuang@cs.duke.edu

{new, annaysun, shru, carolejeanwu}@meta.com

ke.l@wustl.edu

lee.sean@gmail.com

leebcc@seas.upenn.edu

Work done while interning/working/visiting FAIR at Meta.

###### Abstract

Mixture-of-Experts (MoE) models have recently gained steam in achieving the state-of-the-art performance in a wide range of tasks in computer vision and natural language processing. They effectively expand the model capacity while incurring a minimal increase in computation cost during training. However, deploying such models for inference is difficult due to their large model size and complex communication pattern. In this work, we provide a characterization of two MoE workloads, namely Language Modeling (LM) and Machine Translation (MT) and identify their sources of inefficiencies at deployment. We propose three optimization techniques to mitigate sources of inefficiencies, namely (1) Dynamic gating, (2) Expert Buffering, and (3) Expert load balancing. We show that dynamic gating improves maximum throughput by 6.21-11.55\(\) for LM, 5.75-10.98\(\) for MT Encoder and 2.58-5.71\(\) for MT Decoder. It also reduces memory usage by up to 1.36\(\) for LM and up to 1.1\(\) for MT. We further propose Expert Buffering, a new caching mechanism that only keeps hot, active experts in GPU memory while buffering the rest in CPU memory. This reduces static memory allocation by 1.47\(\). Finally, we propose a load balancing methodology that provides additional robustness to the workload. Our code is available at https://github.com/hyhuang00/moe_inference.

## 1 Introduction

A machine learning model's predictive ability increases with the number of parameters. Model capacity has grown at an exponential rate of 10\(\) per year , which in turn has driven demand for computation. Mixture of Experts (MoEs) decouple model capacity from computational demands by using conditionally, sparsely activated neural networks. They can reduce training costs yet improve accuracy [2; 3; 4] for language modeling [5; 6; 7; 8], machine translation , and image recognition [10; 11].

But training is only half the story. MoE inference is important yet challenging as large language models are deployed for production services. Our experiments show MoE inference is relatively inefficient, requiring much more time to perform the same number of calculations. In Section 3, we show that MoEs are 15\(\) slower for language models and 3\(\) slower for machine translation compared to their FLOP-equivalent dense counterparts. Distillation could shrink models and reduce latency but harm model quality [6; 8; 2]. Optimizations could increase parallelism and GPU utilization, but they narrowly target specific kernels for communication collectives and GPU computation [12; 13]. Theylack a comprehensive analysis of inference costs and neglect inefficiencies in the MoE algorithms themselves.

_We explore optimizations for MoE inference to improve three important dimensions--token throughput, memory use, load balance--without degrading model quality._ We begin by identifying sources of inefficiency in MoE inference, breaking down latency and memory use across components of the model architecture. We find the gating function, which assigns tokens to experts, is a major contributor to MoE's high latency and large memory footprint.

We address MoE's inefficiency with **Dynamic Gating**, a new gating function that better matches each expert's computational capacity to its token assignments, thereby reducing communication and computation for placeholders. Dynamic gating reduces latency and memory use while enabling inference with larger batch sizes and fewer GPUs. We implement this new gating function on an open-source MoE Transformer . Dynamic gating could be integrated with other optimizations for distillation, communication collectives, and GPU kernels for even greater benefit.

Furthermore, we develop optimizations that allow experts to better use GPU memory and cores. **Expert Buffering** exploits high temporal locality across experts. It allocates a fixed, but limited, amount of GPU memory for hot, active experts and relies on CPU memory to buffer all other experts. Less frequently accessed experts are brought into GPU memory as needed, significantly reducing demand for GPU memory. Expert buffering is orthogonal to existing memory management techniques such as offloading. **Load Balancing** mitigates severe load imbalance across experts. Although MoEs are trained with a loss function encouraging load balance, the token distribution during training often differs from that during inference. Our load balancing technique tracks and estimates expert loads (and their hosting GPUs) based on runtime expert activation data. It then redistributes tokens to balance the load, improving system robustness and reducing risks of out-of-memory errors and oversubscribed GPUs.

We implement and evaluate these optimizations for language modeling (LM) and machine translation (MT) tasks. These optimizations significantly improve inference throughput, memory use, and load balance. Moreover, they outperform the state-of-the-art and previously proposed optimizations.

## 2 Background

In this section, we aims to provide a basic background for readers unfamiliar with MoE transformers. A more detailed description on the MoEs and related works can be found in App. A.

Figure 1: Illustration of MoE Models. **(a)** MoE module uses a gating function to assign inputs to experts ; **(b)** Dense Transformer Decoder Layer, which consists of Multi-head Attention (MHA) followed by a Feed-Forward Network (FFN); **(c)** MoE Transformer Layer in which an FFN block is replaced by a set of expert FFNs, which operate in parallel; **(d)** MoE Transformer with expert parallelism. Each device hosts a subset of experts. Tokens assigned to remote experts are dispatched via all-to-all communication.

Mixture-of-Experts (MoEs) use different models for different inputs to improve versatility and robustness . An MoE consists of multiple, independent models (_i.e._, experts) and a gating function that assigns inputs to experts. Each input activates only its assigned expert, allowing the model capacity to expand "outrageously" with more tractable increases in computational cost.

**MoE Transformer.** The Transformer model architecture has defined the state-of-the-art for computer vision and natural language processing [16; 17]. Illustrated in Figure 1, sparse MoE layers replace the FFN block in the Transformer architecture with an MoE block that consists of multiple expert FFNs. These layers use a gating function to decide which experts are most suitable for each token, and then routes tokens to their corresponding experts. Typically, a token is routed to one or two experts in a top-1 or top-2 gating policy. Compared to traditional Transformers, where the FLOP count per batch scales linearly with the number of parameters, MoE networks require much less computation and allow large models to be trained efficiently. MoE Transformers have reduced training costs for large models [2; 6; 7; 8] and achieved high accuracy in vision, text, speech and multi-task learning [18; 5; 19; 20; 11].

**Expert Parallelism.** MoE models present an interesting trade-off, requiring less computation but more memory usage than traditional Transformers of the same capacity. Expert layers deploy many additional FFNs, which increase model size and demands for GPU memory. GShard  addresses these challenges with expert parallelism, distributing workload across multiple GPUs. Each GPU holds a subset of expert FFNs and copy of all other parameters. All-to-all communication is required when distributing tokens to experts and collecting results from experts.

## 3 Mixture-of-Experts Characterization

We characterize MoE Transformers for Language Modeling (LM) and Machine Translation (MT) against FLOP-equivalent dense models. We study recent models on high-performance testbeds, and a detailed description can be found in Table 1 and 2 in App. B. The mini batch size is set to 8 for Language Modeling and 48 for Machine Translation, the largest feasible values under baseline.

### Expert Activation

Under the baseline MoE design, an expert always processes a capacity of tokens regardless of the number of tokens actually assigned. Experts configured with excess capacity will suffer longer latencies and use more memory. But how much of this waste is incurred in practice? We answer this question by analyzing expert activations on several tasks.

**Language Modeling (LM).** We use three domains--Wikipedia, PubMed, Github, from the PILE dataset  as input following . Fig. 5(a) in App. C.1 shows a highly imbalanced load across experts. Multiple hot experts consistently receive a large share of tokens, while other experts consistently receive a small share or no tokens at all. While all inputs show sparse expert activations, the set of hot experts and their hotness levels vary across domains.

**Machine Translation (MT).** We evaluate expert activation by performing translation from English to French, Japanese, and Austrian using validation data from NLLB-200 . Fig. 5(b) in App. C.1 shows that MT also exhibits load imbalance and a small fraction of experts are hotter than others. Noticeably, decoder activation is extremely sparse, and differs from the encoder in one key aspect: it exhibit strong temporal locality. An expert may be active for several consecutive batch, then go inactive again. This temporal locality for hot experts is key motivation for our expert buffering optimization in Section 5.

### Latency

Fig. 7 in App. C.2 examines inference latency. The MoE is 15\(\) slower for language models (LM) and 3\(\) slower for machine translation (MT). While MoE model will be more accurate than the FLOP-equivalent dense model, the MoE will exhibit much higher latencies than the dense. The finding that MoEs perform the same number of FLOPs but require much more time illustrates the need to mitigate the intrinsic inefficiency of MoE inference.

Fig. 9 in App. C.2 shows contributors to inference latency for different models and node counts. Prior studies attribute MoE's longer latency to frequent all-to-all communication , which is true for MoE training where computation is distributed across many nodes and all-to-all communication is the main bottleneck. However, our characterization of MoE inference reveals that, while all-to-all communication occurs in multi-node deployments, it is less significant to latency than the computation for the gating function and experts. This finding informs our optimization in Sec. 4.

### Memory Usage

Fig. 8 in App. C reveals contributors to MoE's significant demands for memory capacity. For LM, the dense model only requires 2.2GB on each GPU whereas the MoE requires up to 18.9GB, an increase of 8.6\(\). For MT, the dense and MoE models use 7.0GB and 21.2GB, respectively, an increase of 3.0\(\). Beyond the static model parameter usage, MoE models also use much more dynamic memory than dense models.

Fig. 10 in App. C illustrates dynamic memory use for the baseline MoE on the _Pear_ cluster. A significant amount of memory is allocated during the gating and reordering computation, and then freed nearly instantaneously. Our close examination of the memory trace indicates that primary cause of this memory use is batch matrix multiplication within the static gating function. We address this challenge with dynamic gating in Section 4.

### Load Imbalance

The root cause of MoE's high latency and memory use lies in its static gating policy. Recent implementations assume experts' computational loads are balanced [23; 12; 5; 8]. Under this assumption, token distribution can be simplified and implemented with all-to-all collectives, which we detail later in Fig. 1(a)(1). Inefficiencies arise when assumptions about load balance fail. If the gating function assigns fewer tokens than an expert's capacity, the remaining capacity is filled with placeholders (_i.e._, zeros). If more tokens are assigned than the expert's capacity, excess tokens are dropped, with their information retained only by residual connections. Dropping tokens harms accuracy, so capacity \(C\) usually set to large values to prevent information loss and accuracy fluctuations. However, large capacities increase latency and memory use. We find that it can leads to a waste as large as \(12.8\) for LM and \(64\) for MT (see App. C.3 for detail).

We study whether waste during inference is avoidable. If the workload is well balanced and token allocations across experts are comparable, we could reduce waste by simply scaling down expert capacity. On the other hand, if expert activation is sparse, scaling down capacity risks dropping tokens and harming model accuracy.

## 4 Dynamic Gating Optimization

Our characterization reveals a gap between assumptions and practice regarding load balance across experts, which gap leads to poor performance and resource inefficiency during inference. Although prior studies also notice load imbalance across experts [13; 12], they retain a gating policy that increases expert capacity in response to severe imbalance. Such a policy seeks model accuracy by ensuring the most heavily loaded experts do not drop tokens. But such a policy also exacerbates the high latency and inefficient memory use we have observed.

We propose dynamic gating that tunes efficient, variable capacities for experts. This improves upon static gating, which inefficiently sets large, fixed capacities. Changing the gating policy to permit dynamism is non-trivial. Major MoE implementations rely on statically set expert capacities to ensure all messages sent with all-to-all collectives are equally sized [13; 8; 10]. The NCCL all-to-all primitive requires recipients to pre-allocate memory for messages between GPUs. With equally sized messages, each GPU knows the required memory size beforehand. However, when message sizes vary due to dynamic gating and differing token assignments, a lightweight message is needed to notify each GPU recipient of its incoming tensor size.

**Static Baseline** Figure 1(a)(1) illustrates static gating, which constitutes our baseline. Here, \(S\) represents the sequence length, \(C\) represents the Capacity Factor, \(E\) represents the number of total experts, and \(D\) represents the dimension of each token. The gating function generates expert assignments and translates them into \(E\) dispatch masks, each with dimension \((S,\ S C)\). Entries in the mask are generated as follows. If token \(i\) is assigned to expert \(e\) and the \(e\)-th mask still hascapacity, the \(i\)-th column of the first empty row is marked 1 and other entries are marked 0. This process produces a highly sparse mask, which is a tensor with dimension \((E,\ S,\ S C)\) that contains at most \(S\) 1's. Input tokens are multiplied with the mask to reorder inputs into \(E\) sets of inputs, each with \(S C\) tokens, indicating the assignment of tokens to experts.

**Dynamic Capacity for Gating** Figure 1(a)(2) illustrates our new dynamic gating procedure, which transfers a variable number of tokens to experts and devices. The procedure simplifies token distribution by transforming a vector of gating decisions. First, the procedure performs an argsort to generate indices that sort the decision vector by expert ID. Second, it uses the indices to produce a sorted decision vector. Finally, it counts the number of occurrences of each expert in the decision vector, thereby determining the number of tokens assigned to each expert.

Because the number of tokens assigned to each expert varies, dispatch requires two rounds of communication. First, experts are notified about the number of incoming tokens (_i.e._, size) using an all-to-all collective. This notification happens as soon as sizes are known, allowing its latency to be hidden behind other computation. In parallel with this first round of communication, input tokens are re-ordered with optimized indices and then split based on sizes. Second, the gating function transfers the actual tokens with another all-to-all.

After all experts process their assigned tokens, tokens are collected with another all-to-all and sent to their original devices. Tokens are restored to their original order. This re-ordering is typically implemented using batch matrix multiplication but, as in the dispatch stage, the multiplication could be replaced with a more efficient indexing operation.

**Costs and Benefits** Dynamic gating complexity is \(O(SD+S S)\) where \(S\) is sequence length and \(D\) is token dimension. The dispatch requires a sort of \(O(S S)\), a bin-count of \(O(S)\), and an indexing operation of \(O(SD)\). The complexity of dynamic gating is much smaller than that of static gating, which requires batch matrix multiplication of \(O(S^{2}EDC)\) to reorder tokens such that those assigned to the same expert are contiguous. Dynamic gating eliminates the dispatch mask and avoids the multiplication. Instead, it uses an indexing operation that directly places tokens in the desired order.

Beyond this complexity analysis, dynamic gating incurs only modest additional communication costs. An additional all-to-all notifies experts about the number of incoming tokens, a single integer that is communicated at very low cost, which only 20 \(\)s on average in our experiments.

Figure 2: (a) Comparison between the static gating in  and our implementation of dynamic gating. We assume E=3, S=6, C=0.5 and top-1 gating in this example. (See Sec. 4.)(b) Illustration of the Expert Buffering mechanism. We move the expert parameters to CPU memory to reduce burden on GPU memory. (See Sec. 5.)

In exchange for these modest costs, dynamic gating offers several significant benefits. It enhances model robustness by ensuring tokens are not dropped. When an expert receives more tokens, it adjusts its capacity accordingly, preventing load from exceeding capacity and thus avoiding token loss.

Additionally, dynamic gating improves computational efficiency by eliminating empty placeholders. When fewer tokens are sent, the expert is notified of the small load, avoiding unnecessary memory allocation and communication bandwidth associated with placeholders.

## 5 Expert Buffering Optimization

Our analysis of expert activation patterns indicates that, although some experts are often inactive, all experts are activated at least a few times across time and batches. This observation motivates our buffering mechanism, which judiciously offloads expert parameters to CPU memory, freeing GPU memory to hold frequently activated experts and enable larger batch sizes.

Figure 1(b) illustrates the buffering mechanism. During inference, each GPU hosts a number of experts and receives their corresponding tokens. An active expert is one that receives tokens for the current batch. If an active expert is not already present in GPU memory, a memory copy transfers expert parameters from CPU memory into GPU memory. The transfer of parameters proceeds in parallel with the transfer of tokens, allowing the buffering mechanism to overlap data movement and hide latency.

Scarce GPU memory is managed with an eviction policy that accounts for expert activation patterns. First, the policy evicts the experts that are not active in the current batch as these experts are less likely to be used in the near future (_i.e._, concept of temporal locality). Second, the policy evicts experts with the Last In, First Out (LIFO) policy.

The choice of the LIFO policy is rooted in how recent MoEs have been implemented. When a single GPU hosts multiple experts, the MoE executes experts sequentially by increasing order of their IDs. Suppose the GPU hosts \(E=4\) experts and caches parameters for \(2\) experts. Further suppose that the batch activates experts 1, 2 and 3. The MoE Transformer first fetches and executes experts 1 and 2. It then evicts expert 2 and fetches expert 3. By evicting expert 2 instead of expert 1, the cache retains the expert with the shortest re-use distance.

No prior work exploits unique MoE characteristics to optimize memory use during inference. As a caching strategy tailored for MoEs, expert buffering is orthogonal to prior memory efficiency schemes and can be seamlessly integrated with other techniques, such as offloading [24; 3], for even greater memory savings.

## 6 Load Balancing Optimization

Our analysis of token distribution indicates severe computational load imbalance across experts. GPUs that host hot experts can become oversubscribed and vulnerable to out-of-memory errors. GPUs that host cold experts can idle while waiting for others to complete their computation. These observations motivate our load balancing technique, which colocates heavily loaded experts with lightly loaded ones using run-time activation data.

Let \(P_{mn}\) denote expert placement where \(m{}\{1,,E\}\) is expert ID and \(n{}\{1,,D\}\) is device ID. When \(P_{mn}=1\), the \(m\)-th expert is placed on the \(n\)-th device. Let \(A_{mb}\) denote expert activation where \(m\) is expert ID and \(b{}\{1,,B\}\) is batch ID. Each value in \(A_{mb}\) is the fraction of tokens from batch \(b\) assigned to expert \(m\).

The placement problem can be reduced to the multi-way, number partitioning problem , which is NP-hard. Moreover, this optimization should be constrained such that each GPU hosts the same number of experts. This constraint balances memory use across GPUs and simplifies communication processes. The problem can be formulated as follows.

\[_{m,b}|_{n}P_{mn}A_{mb}-|_{m}P_{mn}=\; n\]

**Greedy Balancing.** We implement a greedy algorithm to optimize the assignment of experts to GPUs. The algorithm sorts experts by their average historical load \(_{m}\) and sequentially assigns experts to GPUs in descending order. Each step in the sequence assigns an expert to the GPU with the smallest expected load \(_{m}P_{mn}_{m}\). A GPU is excluded from consideration once it reaches designated load.

**Anti-Correlation Balancing.** Although the Greedy algorithm is effective when expert activations are independent (LM, MT-Encoder), it is less effective when activations are correlated (MT-Decoder). With correlated experts, the number of activations that are estimated from historical data \(A_{mb}\) is a poor indicator for load. We address this challenge with Anti-Correlation Balancing. Let \(S_{ab}\) denote the Pearson correlation between experts \(a\) and \(b\) that is observed in historical data. We revise the estimate of a GPU's expected load to \(_{m}P_{mn}(_{m}+0.5*S_{am})\). This revision reduces load and tends to improve expert buffering performance for MT-decoder.

## 7 Evaluation

There are few direct baselines for efficient MoE serving. We obtain robust baselines by re-implementing and adapting previously proposed optimizations for inference. We preserve output quality and deliberately exclude token dropping, which harms quality .

**Methods.**_Fairseq_ is our baseline MoE implementation . _Tutel_ improves latency and memory use with custom kernels . It replaces the gating function's sparse matrix multiplication with a hash table lookup to distribute tokens, and creates a custom cumulative summation kernel to reorder inputs. _FasterMoE_ organizes experts into fine-grained groups, combining token communication and expert execution within each group . It overlaps the communication for one group with execution for another group. FasterMoE also places hot experts on each GPU, reducing communication. _Megablock_ uses a custom kernel to execute experts and accelerate operations on a block-sparse matrix, which organizes token inputs and outputs . Nevertheless, the custom kernel depends on features in advanced GPUs, and may compromise backward compatibility. For Megablock, we adapt the experts' MLPs to omit the bias term aligning with Megablock's structural constraints.

**Clusters.** Table 2 details our experimental clusters. We use _Apple_ to characterize MoE workloads (Table 1) and study the impact of our proposed optimizations. Due to limited machine availability on the _Apple_ cluster, we perform additional experiments on _Pear_ with NVIDIA's Ampere. Although our Ampere GPUs provide advanced features for Megablock's custom kernels, they offer limited memory capacity and restrict our experiments to a single node and smaller LM workloads. We report an average of multiple throughput experiments.

### Impact of Dynamic Gating

**Single-Node.** Fig. 3-4 indicate dynamic gating significantly increases throughput for varied batch sizes, tasks, and clusters. LM throughput increases by \(6.21\) and \(3.32\) when compared against Fairseq and Tutel, respectively. Similarly, MT-encoder's throughput increases by \(5.75\) and \(5.33\) while MT-decoder's increases by \(2.58\) and \(1.88\). Beyond throughput increases, dynamic gating permits larger batch sizes by replacing the large dispatch mask with assignment indices and sorted decision vectors.

Compared to other methods, our dynamic gating technique outperforms FasterMoE by up to \(2.49\), given the same batch size, by avoiding GPU kernel launch overheads. Moreover, our dynamic gating

Figure 3: Throughput Comparison (_Pear_). Dynamic gating outperforms static, Tutel, and FastMoE consistently. It outperforms Megablock as batchsize scales. Missing bars represent infeasible combinations of policy and batch size.

technique scales to larger batch sizes (and thus higher throughputs) better than Megablocks. Although dynamic gating underperforms for small batch sizes (4-16), it outperforms by increasingly large margins as batch size scales (32-80). When batch size is 80, dynamic gating outperforms Megablock by \(1.46\), demonstrating better scalability. We argue that performance under large batch sizes is more important, as small batch sizes are uncommon in real-world inference workloads, particularly in online serving scenarios. Inference with small batch sizes limits throughput due to the reduced computational intensity. With the help of online batching , larger batch sizes can be easily achieved and become more frequent than the small batch size case. We also provide a detailed analysis on the underlying reason for the behavior in App. D.1.

**Multi-Node.** Fig. 4 shows performance when the system scales to two and four nodes. Dynamic gating tunes expert capacity to token load, thereby eliminating wasted communication for placeholders. More efficient communication translates into throughput gains when MoE models are deployed across multiple nodes. Dynamic gating improves throughput by up to 11.55\(\), 10.98\(\), 5.71\(\) for LM, MT-Encoder, and MT-decoder, respectively, when compared against Fairseq's static gating.

**Memory Use.** Fig. 11 assesses the impact on gating policy on memory use. Dynamic gating reduces memory use by eliminating the mask for token dispatch and by avoiding computation for placeholders. On the _Apple_ cluster, memory use for activations falls by 79.6%, from 6.29GB to 1.28GB, when performing LM inference with a batch size of 8. Similarly, memory use falls by 44.2%, from 1.89GB to 1.05GB, when performing MT inference with a batch size of 8.

When activations use memory more efficiently, the MoE system can support larger batch sizes and achieve higher performance. On the _Apple_ cluster, dynamic gating permits LM and MT to use batch sizes of 64 and 96, respectively. These batch sizes are 8\(\) and 2\(\) than those permitted by Fairseq's static baseline.

### Impact of Expert Buffering

**Cache Miss Rate.** Each GPU deploys a cache to hold a subset of the experts assigned to it. When an expert is not found in GPU memory, the system incurs additional latencies to transfer the desired expert from CPU memory to GPU memory. For _Apple_'s MT tasks, we vary cache size from 16

Figure 4: Throughput Comparison (_Apple_). Dynamic gating reduces memory use and communication, enabling larger batches and faster processing compared to static gating. Expert buffering (EB) trades latency for reduced memory use while maintaining high throughput. Load balancing (LB) can further improve latency. Note LB is relevant only for dynamic gating where each expert receives a different number of tokens. Missing bars indicate infeasible combinations of policy and batch size.

experts--which accommodates all experts, fully occupies GPU memory, and offers zero reduction for static memory use--to 1 expert, which offers a 32% reduction in static memory use.

For each cache size, we report the global worst-case cache miss rate. The miss rate is global because it counts misses based on accesses across all GPUs and their memories. The miss rate is worst-case because it reports the highest miss rate across all GPUs and batches. This metric conservatively estimates the least efficient scenario in cache utilization, offering valuable insights for predicting throughput reduction.

Fig. 12(a) indicates cache miss rates for our LIFO policy approximates those from Belady's MIN, the theoretically optimal policy. The figure also indicates that cache miss rates improve most when cache capacity is greater than 5 experts per GPU or 40 experts across 8 GPUs. This observation corresponds with our prior finding that, on average, over 90 experts are not assigned any tokens by the MT-decoder.

**Throughput.** Fig. 4 shows the impact of expert buffering for MT decoding. The cache accommodates 10 experts per GPU and 80 experts across 8 GPUs. This cache size is the point at which Fig. 12(a) indicates diminishing benefits for cache misses.

For single-node inference, buffering negates some of the throughput gains from dynamic gating as cache misses impact performance. Nonetheless, dynamic gating and expert buffering together are still competitive with our baselines. For multi-node inference, dynamic gating and expert buffering together report throughput gains of 2.21\(\) and 4.30\(\) over baselines for two and four nodes, respectively.

**Memory Use.** Latency rises as memory use falls. Fig. 11 indicates buffering experts on CPU memory reduces GPU static memory use, for expert parameters, by 2.25GB. But Fig. 13 reports that as cache sizes shrink and memory use falls, latency rises as more experts are transferred between CPU and GPU memories. As expert transfers consume the limited bandwidth between the CPU and GPU, data rates peak at 12 GB/s and increase latency. New technologies that enhance CPU-GPU bandwidth (e.g., NVIDIA's Grace Hopper) can mitigate these latency issues when GPU memory capacity is constrained and caching only a subset of experts is necessary.

### Impact of Load Balancing

Load balancing (LB) can further improve latency. This optimization is particularly beneficial when applied to multi-node settings or combined with expert buffering because balance improves cache performance. Note LB is relevant only for dynamic gating where each expert receives a different number of tokens.

We analyze load with and without our balancing optimizations using activation data from Section 3.1. We separate this data into two halves: the first half of the activation data is used to generate a device assignment for each expert, and the second half to estimate load under generated assignments. We record Max load, which is the maximum share of the tokens received by a GPU across all batches. Max load is a worst-case scenario and assesses risks from out-of-memory errors. We also record Avg-Max load, which is the maximum share of the tokens received by a GPU averaged over all batches. Avg-Max estimates typical load and assesses performance risks from oversubscribed GPUs.

Fig. 14 indicates that Greedy balancing successfully equalizes expert load for LM, reducing loads per GPU from upwards of 0.6 to below 0.4. Fig. 4 shows how balanced load translates into performance. First, Greedy balancing increases throughput by up to 1.11\(\) and 1.19\(\) when compared against pure dynamic gating. Second, it permits larger batch sizes of 64 and 128 when LM is deployed on multiple nodes. Greedy balancing is similarly effective for MT-encoder.

Anti-Correlation balancing is robust to MT-decoder's correlated expert activations. It successfully reduces Max and Avg-Max load in most cases, but the balanced load produces only modest throughput gains of \(1.02\) when compared against pure dynamic gating.

## 8 Conclusion

We analyze the behavior of standard MoE Transformer workloads, pinpointing their inefficiencies in inference latency and memory usage. We introduce a Dynamic Gating policy that significantly enhances efficiency in terms of latency and memory demands during inference. Building on this, we propose an Expert Buffering mechanism, demonstrating its effectiveness in substantially reducing memory requirements for MoE inference deployment with a nominal increase in latency. Additionally, we implement load balancing, leveraging historical activation data and heuristic methods to bolster deployment robustness.

Recent years witness the boom of MoE LLM models in commercial applications, including API services and personal assistants , and as a result, much more research is needed for efficient inference of MoE models. Techniques such as heterogeneous experts and token-dropping  could offer trade-offs between performance and quality, which in turn could be potentially supported by heterogeneous allocations of GPU resources. Heterogeneous experts would motivate even intelligent and dynamic gating functions that assign tokens based on task difficulty. More efficient interconnect (e.g., optical) could benefit expert movement as well as all-to-all communication, which currently remain a challenge.