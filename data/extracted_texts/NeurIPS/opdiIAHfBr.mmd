# AlignedCut: Visual Concepts Discovery on Brain-Guided Universal Feature Space

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We study the intriguing connection between visual data, deep networks, and the brain. Our method creates a universal channel alignment by using brain voxel fMRI response prediction as the training objective. We discover that deep networks, trained with different objectives, share common feature channels across various models. These channels can be clustered into recurring sets, corresponding to distinct brain regions, indicating the formation of visual concepts. Tracing the clusters of channel responses onto the images, we see semantically meaningful object segments emerge, even without any supervised decoder. Furthermore, the universal feature alignment and the clustering of channels produce a picture and quantification of how visual information is processed through the different network layers, which produces precise comparisons between the networks.

## 1 Introduction

Introducing a novel approach, Yang et al. (2024) has successfully established a method of computing a mapping between the brain and deep-nets, effectively linking two black boxes. The brain fMRI prediction task allows for visualizing information flow from layer to layer, using the brain as an analysis tool.

If a picture is worth a thousand words, the main idea is that the brain's thousands of voxels can be thought of as alphabets for these words that describe an image. Just as alphabets must be combined to form words and phrases with meanings, we need to find the grouping of brain voxels and their network channel counterparts to understand their meaning (Figure 1).

Our main discovery is that while the network layer structure differs, channel feature correspondence exists across networks with a shared encoding of reoccurring visual concepts. This paper builds upon the idea of 'Rosetta stone' neurons (Dravid et al., 2023), which find channels across networks that share similar image responses in binary segmentation. If channels are alphabets, 'Rosetta stone' provides an alphabet-level translation between networks.

Individual channel-level analysis could miss feature correspondence across networks at finer and coarser levels. On a finer level, because the channels are invariant up to a linear transformation, we might miss a reconstituted feature constructed from a composition of existing channels. On a coarse level, the channels can be combined and clustered to form a bigger 'Rosetta' concept.

Figure 1: Transform the hidden channel activation of deep-nets into visual brain voxels’ response.

To address fine-level channel analysis, we use brain voxel response as a reference signal and linearly transform channels for each network into a shared space sufficient for brain fMRI prediction. This process produces a universal feature space that aligns channel features across the layers and models.

To find bigger visual concepts, one can start with Neuroscience knowledge of brain regions (ROIs) with specific brain functionality, i.e., V1, V4, and EBA. While tracing the mapping of the ROIs to channels can produce visual concepts (Figure 2), brain regions don't function in isolation.

Instead of searching through all possible channel grouping combinations, our first insight is that we can create a channel grouping hypothesis by examining channels from each pixel's perspective. Think of the pixels and channels forming a bipartite graph; each channel produces a per-pixel response (image activation map), defining the graph edge between the pixels and channels. Taking the perspective of pixels, one can collect graph edges incident on each pixel into a vector, which can be thresholded to produce a hypothesis grouping over channels.

Our second insight is that if a channel grouping hypothesis repeats across images, layers, and models, it is highly unlikely to be accidental and, therefore, signals meaningful visual concepts.

We formulate this clustering problem as a graph partition task. The graph nodes are the product space of pixels and layers. We apply spectral clustering to produce k-top eigenvectors. We take advantage of two properties of spectral clustering: it makes 1) soft-cluster embedding space in the form of eigenvectors and 2) hierarchical clustering by varying the number of eigenvectors.

We made the following discoveries. First, shared channel sets, reoccurring across layers and models, predict response in distinct brain regions. By tracing the channel activation to the known brain ROI properties, we observe that the channel cluster encodes visual concepts at various levels of visual abstraction.

Second, meaningful object segments can emerge by tracing the channel cluster responses onto each image. We observed that some channel clusters produce figure/ground separation while others produce fine-grained category classification. Our image segmentation requires no additional segmentation decoder and uses only a simple distance measure over the eigenvectors.

Finally, the universal feature alignment and the spectral clustering of channels produce a picture and quantification of how visual information is processed through the different network layers.

While these discoveries are promising, there are two main technical hurdles to overcome to verify them on a large scale. Our method rests upon a crucial assumption: the channels across the different layers and models can be mapped into a shared space. While brain prediction over thousands of voxels can provide strong guidance for this alignment, an additional constraint would be needed when the shared space has a large dimension (suitable for expressiveness). We use clustering as a constraint, ensuring alignment linear transformation preserves spectral clustering eigenvectors. Furthermore, the graph size is enormous as it is a product space over pixels, layers, images, and models; therefore, computing eigenvectors over their pairwise affinity matrix can be computationally infeasible. We developed a Nystrom-like approximation to ensure efficient computation.

Figure 2: From the 768D feature on CLIP layer-6, we extract different levels of segmentation by restricting the use of a subset of channels. _Left:_ Channel activation on example image patches. The ordering of channels is sorted from the early brain to the late brain by their weights for brain voxels. _Right:_ Spectral clustering on each subset of channels filtered by each brain ROI (V1, V4, EBA), image pixels colored by 3D spectral-tSNE of top 10 eigenvectors.

In summary, our key contributions are:

1. We constructed a universal channel-aligned space using brain encoding as supervision and spectral clustering eigenvector constraints to ensure minimal channel signal loss. Brain encoding associates the aligned channel space to brain regions and gives them meanings.
2. Models trained with different objectives learned similar visual concepts: corresponding channel patterns exist across different models. The resulting visual concepts can be validated by unsupervised segmentation benchmarks on ImageNet-segmentation and PASCAL VOC.
3. Models show divergent computation paths over the visual concept space formed by the top-k spectral eigenvectors. Different models differ in trajectories and pace of movement layer-to-layer.

## 2 Methods: AlignedCut

Just as human languages might consist of distinct alphabets, features across different models appear superficially in embedding spaces as almost mutually orthogonal (Figure 3). However, the underlying information that they represent can be similar. To jointly analyze features across models and layers, we proposed the **channel align transform** that linearly projects features to a universal space.

The learning signal for the channel align transform is provided by **brain response prediction**. Learning from brain prediction offers two advantages. First, brain response covers rich representations from all levels of semantics; the channel alignment removes irrelevant information while preserving the necessary and sufficient visual image features. Second, knowledge of brain regions provides an interpretable understanding of their corresponding channels derived from the alignment.

Our visual concept discovery is formulated as a graph partitioning task using **spectral clustering**. We term our approach for this channel align and graph partitioning as **AlignedCut**. Furthermore, a major challenge in applying spectral clustering to large graphs is the complexity scaling issue. To address this, we developed a **Nystrom-like approximation** to reduce the computational complexity.

### Brain-Guided Universal Channel Align

Brain DatasetWe used the Algonauts competition (Gifford et al., 2023) release of Nature Scenes Dataset (NSD) (Allen et al., 2022). Briefly, NSD provides an fMRI brain scan when watching COCO images. Each subject viewed 10,000 images over 40 hours of scanning. We used the first subject's publicly shared pre-processed and denoised (Prince et al., 2022) data.

Channel AlignLet \(=\{_{1},_{2},,_{n}|_{i}^ {P D^{}}\}\) be the set of image features, extracted from each layer of pre-trained ViT models, where \(P=(H W+1)\) is image patches and class token, \(D_{i}\) is the hidden dimension. In particular, we used the attention layer output for each \(}\) without adding residual connections from previous layers. Let \(^{}\) be the channel-aligned features; the goal of channel alignment is to learn a set of linear transform \(=\{_{1},_{2},,_{n}|_{i}^ {D_{i} D^{}}\}\). In the new \(D^{}\) dimensional space, channels are aligned.

\[^{}==\{_{1}_{1},_{2}_{2},,_{n}_{n}|_{i}_{i}^ {P D^{}}\}\] (1)

Brain PredictionTo produce a learning signal for channel align \(\), features from \(^{}\) are summed (_not concatenated_) to do brain prediction. Let \(^{1 N}\) be the brain prediction target, where \(N\) is the number of flattened 3D brain voxels, and \(1\) indicates that each voxel's response is a scalar value. Let \(F_{}:^{P D^{}}^{1 N}\) be the learned brain encoding model; without loss of generalizability, we set \(F_{}\) as global average pooling then linear weight \(_{}^{D^{} N}\) and bias \(_{}^{1 N}\):

\[[*{}_{p P}(_{i=1}^{n}( {V}_{i}_{i}))_{}+_{}] \] (2)Channel in the Brain's SpaceLet \(=\{_{1},_{2},,_{n}|_{i}^{P  N}\}\) be the set of channel activations in the brain's space. By defining \(_{i}:=_{i}_{i}_{}\), we have the brain response prediction \(=_{p P}(_{i=1}^{n}_{i})+_{}\) (Eq. (2)). Intuitively, we linearly transformed the activation to the brain's space, such that the activation from all slots sum up to the brain response prediction.

### Graph Spectral Clustering

Spectral ClusteringWe use spectral clustering for visual concepts discovery and image-channel analysis; it provides 1) soft-cluster embedding space and 2) unsupervised hierarchical image segmentation. Normalized Cut (Shi and Malik, 2000) partitions the graph into sub-graphs with minimal cost of breaking edges. It embeds the graph into a lower dimensional eigenvector representation, where each eigenvector is a hierarchical sub-graph assignment.

Let \(^{M M}\) be the symmetric affinity matrix, where \(M\) denotes the total number of image patches. Given channel aligned features \(^{}^{M D^{}}\), we define \(_{ij}:=((^{}_{i},^{}_{j})-1)\) such that \(_{ij}>0\) measures the similarity between data \(i\) and \(j\). The spectral clustering embedding \(^{M C}\) is solved by the top \(C\) eigenvectors of the following generalized eigenproblem:

\[(^{-1/2}^{-1/2})=\] (3)

where \(\) is the diagonal degree matrix \(_{ii}=_{j}_{ij}\), \(\) is diagonal eigenvalue matrix.

Nystrom-like ApproximationComputing eigenvectors for \(^{M M}\) is prohibitively expensive for enormous \(M\) with a time complexity of \(O(M^{3})\). The original Nystrom approximation method (Fowlkes et al., 2004) reduced the time complexity to \(O(m^{3}+m^{2}M)\) by solving eigenvectors on sub-sampled graph \(^{}^{m m}\), where \(m M\). In particular, the orthogonalization step of eigenvectors introduced the time complexity of \(O(m^{2}M)\). Because our Nystrom-like approximation trades the \(O(m^{2}M)\) orthogonalization term with the K-nearest neighbor, our Nystrom-like approximation reduced the time complexity to \(O(m^{3}+mM)\).

Our Nystrom-like Approximation first solves the eigenvector \(^{}^{m C}\) on a sub-sampled graph \(^{}^{m m}\) using Equation (3), then propagates the eigenvector from the sub-graph \(m\) nodes to the full-graph \(M\) nodes. Let \(}^{M C}\) be the approximation \(}\). The eigenvector approximation \(}_{i}\) of full-graph node \(i M\) is assigned by averaging the top K-nearest neighbors' eigenvector \(^{}_{k}\) from the sub-graph nodes \(k m\):

\[_{i}&=KNN(_{*i};m,K) =*{arg\,max}_{k m}_{k=1}^{K}_{ki}\\ }_{i}&= _{i}}_{ki}}_{k_{i}}_{ki}^{}_{k}\] (4)

where \(KNN(_{*i};m,K)\) denotes KNN from full-graph node \(i M\) to sub-graph nodes \(k m\).

### Affinity Eigen-constraints as Regularization for Channel Align

While brain prediction can provide strong supervision for the learned channel align operation, we observed that the quality of unsupervised segmentation dropped after the channel alignment. To address this issue, a regularization term is added:

\[_{eigen}=\|_{b}_{b}^{T}-_{a}_{a}^{T}\|\] (5)

where \(_{b}\) and \(_{a}^{ c}\) are affinity matrix eigenvectors before and after channel alignment, respectively; \(=100\) are randomly sampled nodes in a mini-batch and \(c=6\) are the top eigenvectors. The eigen-constraint preserves spectral clustering eigenvectors in dot-product space, invariant to random rotations in eigenvectors. We found adding eigen-constraints improved both the performance of segmentation (Figure 5) and the brain prediction score (Table 1).

    & \) (\(\) 0.001)} \\ \(_{eigen}\) & V1 & V4 & EBA & all \\ 
1.0 & **0.170** & **0.181** & 0.295 & **0.196** \\
0.1 & 0.167 & 0.179 & 0.294 & 0.193 \\
0 & 0.155 & 0.166 & 0.296 & 0.188 \\   

Table 1: Affinity eigen-constraints improved brain score (\(R^{2}\): variance explained).

Results

Our spectral clustering analysis aims to discover visual concepts that share the same pattern of channel activation across different models and layers. However, implementing spectral clustering analysis comes with two main challenges. First, the models sit in different feature spaces, so direct clustering will not reveal their overlap and similarities. Second, when scaling up to a large graph, spectral clustering is computationally expensive.

To address the first challenge, we developed our channel align transform to align features into a universal space. We extracted features from all 12 layers of the CLIP (ViT-B, OpenAI) (Radford et al., 2021), DINov2 (ViT-B with registers) (Darcet et al., 2024), and MAE (ViT-B) (He et al., 2022) and then transformed features from each layer into the universal feature space.

To address the second challenge, we developed our Nystrom-like approximation to reduce the computational complexity. We extracted features from 1000 ImageNet (Deng et al., 2009) images, with each image consisting of 197 patches per layer. The entire product space of all images and features totaled \(M=76\) nodes, from which we applied our Nystrom-like approximation with subsampled \(m=54\) nodes and KNN \(K=100\), computing the top 20 eigenvectors.

To visualize the affinity eigenvectors, the top 20 eigenvectors were reduced to a 3-dimensional space by t-SNE, and a color value was assigned to each node by the RGB cube. We call this approach AlignedCut color.

In Figure 4, we displayed the analysis, AlignedCut color, and made the following observations:

1. In CLIP layer-5, DINO layer-6, and MAE layer-8, there is class-agnostic figure-ground separation, with foreground objects from different categories grouped into the same AlignedCut color.
2. In CLIP layer-9, there is a class-specific separation of foreground objects, with foreground objects grouped into AlignedCut colors with associated semantic categories.
3. Before layer-3, CLIP and DINO produce the same AlignedCut color regardless of the image input. From layer-4 onwards, the AlignedCut color smoothly changes over layers.

Figure 4: Spectral clustering in the universal channel aligned feature space. The image pixels are colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of the top 20 eigenvectors. The coloring is consistent across all images, layers, and models.

### Figure-ground representation emerge before categories

In this section, we benchmark each layer in CLIP with unsupervised segmentation. The key findings from this benchmarking are: **1)** The figure-ground representation emerges at CLIP layer-4 and is preserved in subsequent layers; **2)** Categories emerge over layers, peaking at layer-9 and layer-10.

_From which layers did the figure-ground and category representations emerge?_ We conducted experiments that compared the unsupervised segmentation scores across layers, tracing how well each representation is encoded at each layer. We used two datasets: a) ImageNet-segmentation (Guillaumin et al., 2014) with binary figure-ground labels, and b) PASCAL VOC (Everingham et al., 2010) with 20 category labels. The results are presented in Figure 5. On the ImageNet-segmentation benchmark, the score peaks at layer-4 (mIoU=0.6) and plateaus in subsequent layers, suggesting that the figure-ground representation is encoded and preserved from layer-4 onwards. On the PASCAL VOC benchmark, the score peaks at layer-9 and layer-10 (mIoU=0.5) even though it is low at layer-4 (mIoU=0.2), indicating that category information is encoded at layer-9 and layer-10. Overall, we conclude that the figure-ground representation emerges before the category representation.

### Visual concepts: class-agnostic figure-ground

In this section, we use brain activation heatmaps and image similarity heatmaps to describe figure-ground visual concepts. The key findings from these heatmaps are: **1)** The figure vs. ground pixels activate different channels; **2)** The figure-ground visual concept is class-agnostic; **3)** The figure-ground visual concept is consistent across models.

_How can the channel activation patterns of the figure-ground visual concept be described?_ We averaged the channel activations from foreground and background pixels, using the ground-truth labels from the ImageNet-segmentation dataset. The averaged channel activations were transformed into the brain's space. In Figure 6, foreground pixels exhibit positive activation in early visual brain ROIs (V1 to V4) and the face-selective ROI (FFA), while negatively activating place-selective ROIs (OPA and PPA). Interestingly, background pixels activate the reverse pattern compared to foreground pixels. Overall, the figure and ground pixels activate distinct brain ROIs.

_Is the figure-ground visual concept class-agnostic?_ We manually selected _one_ pixel and computed the cosine similarity to all of the other image pixels. In Figure 6, the results demonstrate that one pixel (on the human) could segment out foreground objects from all other classes (shark, dog, cat, rabbit). The same result holds true for one background pixel. We conclude that the figure-ground visual concept is class-agnostic.

Figure 5: Unsupervised segmentation scores from spectral clustering on each CLIP layer. ImageNet-segmentation dataset is used with binary figure-ground labels, and the mIoU score peaks plateau from layer-4 to layer-10. In PASCAL VOC with 20 class labels, the mIoU score peaks at layer-9.

Figure 6: The figure-ground visual concepts in CLIP layer-5. _Left:_ Mean activation of foreground or background pixels, linearly transformed to the brain’s space. _Right:_ Cosine similarity from _one_ reference pixel marked. The figure-ground visual concepts are agnostic to image categories.

_Is the figure-ground visual concept consistent across models?_ We performed the channel analysis for CLIP, DINO, and MAE. In Figure 7, the foreground or background pixels activates similar brain ROIs across the three models. Additionally, spectral clustering grouped the representations of foreground objects into similar colors for CLIP and DINO (light blue), the grouping for MAE is less similar (dark blue). Overall, the figure-ground visual concept is consistent across models.

### Visual concepts: categories

In this section we use AlignedCut to discover category visual concepts. The key findings from the category visual concepts are: **1)** Class-specific visual concepts activate diverse brain regions; **2)** Visual concepts with higher channel activation values are more consistent.

_How does each class-specific concept activate the channels?_ To answer this question, we sampled class-specific concepts from CLIP layer-9. First, we used farthest point sampling to identify candidate centers in the 3D spectral-tSNE space. Then, each candidate center was grouped with its neighboring pixels within an Euclidean sphere in the spectral-tSNE space. Finally, the channel activations of the grouped pixels were averaged to produce the mean channel activation for each visual concept. In Figure 8, Concept 1 (duck, goose) negatively activates late brain regions; Concept 2

Figure 8: Category visual concepts in CLIP layer-9. _Left:_ Mean activation of all pixels within an Euclidean sphere centered at the visual concept in the 3D spectral-tSNE space; the concepts activate different brain regions. _Middle:_ The standard deviation negatively correlates with absolute mean activations. _Right:_ AlignedCut, pixels colored by 3D spectral-tSNE of the top 20 eigenvectors.

Figure 7: The same figure-ground visual concepts are found in CLIP, DINO and MAE. _Left:_ Mean activation of all foreground (top) and background (bottom) pixels; the three models exhibit similar activation patterns. _Right:_ AlignedCut, pixels colored by 3D spectral-tSNE of the top 20 eigenvectors; the three models show similar grouping colors for foreground pixels.

(snake, turtle) positively activates early brain regions and also FFA; Concept 3 (dog) negatively activates early brain regions. Overall, category-specific visual concepts activate diverse brain regions.

_How do we quantify the consistency of each visual concept?_ Qualitatively, Concept 1 exhibits more consistent coloring (Figure 8, pink) than Concept 3 (purple). To further quantify this observation, we computed the mean and standard deviation of channel activations for each Euclidean sphere centered on a concept. In Figure 8, there is a reverse U-shape relation between magnitude and standard deviation. The reverse U-shape implies that larger absolute mean channel activation corresponds to lower standard deviation. Overall, higher channel activation magnitudes suggest more consistent visual concepts.

### Transition of visual concepts over layers

In this section, instead of using 3D spectral-tSNE, we use 2D spectral-tSNE to trace the layer-to-layer feature computation. The key findings of spectral-tSNE in 2D are: **1)** The figure vs. ground pixels are encoded in separate spaces in late layers; **2)** The representations for foreground and background bifurcate at CLIP layer-4 and DINO layer-5.

_How does the network encode figure and ground pixels in each layer?_ We performed spectral clustering and 2D t-SNE on the top 20 eigenvectors to project all layers into a 2D spectral-tSNE space. In Figure 9, we found that all foreground and background pixels are grouped together in each early layer. Each early layer (dark dots) forms an isolated cluster separate from other layers, while late layers (bright dots) are grouped in the center. In the late layers, there is a separation where foreground pixels occupy the upper part of 2D spectral-tSNE space, while background pixels occupy the middle part. Overall, foreground and background pixels are encoded in separate spaces in late layers.

_How does the network process each pixel from layer to layer?_ In the 2D spectral-tSNE plot, we traced the trajectory for each pixel from layer-3 to the last layer. In Figure 9, we found that the trajectories for foreground and background pixels bifurcate: foreground pixels (person, horse, car) traverse to the upper side and remain within the upper side; background pixels (grass, road, sky) jump between the middle right and left sides. The same bifurcation is consistently observed for CLIP from layer-3 to layer-4 and DINO from layer-4 to layer-5. Furthermore, to quantify the bifurcation for foreground and background pixels, we first sampled 5 visual concepts from CLIP layer-3 and layer-4. Then, we measured the transition probability between visual concepts, defined as the proportion of pixels that transited from an Euclidean circle around concept A to a circle around concept B. In Figure 10, the transition probability of foreground pixels to the upper side (A1 to B0) is higher than that of background pixels (0.44 vs. 0.16), while the transition probability of background pixels to the right side (A4 to B4) is higher than that of foreground pixels (0.36 vs. 0.06). Overall, this suggests a bifurcation of figure and ground pixel representations at the middle layers of both CLIP and DINO.

Figure 9: Trajectory of feature progression in layers for six example pixels. _Left:_ 2D spectral-tSNE plot of the top 20 eigenvectors, jointly clustered across all models; the foreground and background pixels bifurcate at CLIP layer-4 and DINO layer-5. _Right:_ Pixels colored by unsupervised segmentation.

## 4 Related Work

**Mechanistic Interpretability** is a field of study that intends to understand and explain the inner working mechanisms of deep networks. One approach is to interpret individual neurons (Bau et al., 2017; Dravid et al., 2023) and circuit connections between neurons (Olah et al., 2020). Another approach is to interpret transformer attention heads (Gandelsman et al., 2024) and circuit connections between attention heads (Wang et al., 2023). Other approaches also looked into the role of patch tokens (Sun et al., 2024). These approaches made the assumption that channels are aligned within the same model; we compare across models by actively aligning the channels to a universal space.

**Spectral Clustering** is a graphical method to analyze data grouping in the eigenvector space. Spectral methods have been widely used for unsupervised image segmentation (Shi and Malik, 2000; von Luxburg, 2007; Wu et al., 2018; Wang et al., 2023). One major challenge for applying spectral clustering to large graphs is the complexity scaling issue. To solve the scaling issue, the Nystrom approximation (Fowlkes et al., 2004) approaches solve eigenvectors on sub-sampled graphs and then propagate to the full graph. Another approach is the gradient-based eigenvector solver (Zhang et al., 2023), which solves the eigenvectors in mini-batches. Our proposed Nystrom-like approximation achieves a computational speedup over the original Nystrom approximation, albeit at the expense of weakened orthogonality of the eigenvectors.

**Brain Encoding Model** is widely used by the computational neuroscience community (Kriegeskorte and Douglas, 2018). They have been using deep nets to explain the brain's function. One approach is to use the gradient of the brain encoding model to find the most salient image features (Sarch et al., 2023). Another approach generate text caption for brain activation (Luo et al., 2024). Other approaches compare brain prediction performance for different models (Schrimpf et al., 2020). The field focused on using deep nets as a tool to explain the brain's function; we go in the opposite direction by using the brain to explain deep nets.

## 5 Conclusion and Limitations

We present a novel approach to interpreting deep neural networks by leveraging brain data. Our fundamental innovation is twofold: First, we use brain prediction as guidance to align channels from different models into a universal feature space; Second, we developed a Nystrom-like approximation to scale up the spectral clustering analysis. Our key discovery is that recurring visual concepts exist across networks and layers; such concepts correspond to different levels of objects, ranging from figure-ground to categories. Additionally, we quantified the information flow from layer to layer, where we found a bifurcation of figure-ground visual concepts.

**Limitations.** While the learned channel align transformation projects all features onto a universal feature space, the nature of learned transformation does not preserve all the information. There is a small drop in unsupervised segmentation performance after channel alignment, which is not fully addressed by our proposed eigen-constraint regularization. Secondly, as a trade-off for faster computation, our Nystrom-like approximation does not produce strictly orthogonal eigenvectors. To produce expressive eigenvectors, our approximation relies on using larger sub-sample sizes than the original Nystrom method.

Figure 10: Transition probability of visual concepts from CLIP layer-3 to layer-4. _Left:_ Five visual concepts sampled from CLIP layer-3 and layer-4. _Right:_ Transition probability measured separately for foreground and background pixels; a bifurcation occurs where foreground pixels have more traffic to concept B0, while background pixels have more traffic to concepts B3 and B4.