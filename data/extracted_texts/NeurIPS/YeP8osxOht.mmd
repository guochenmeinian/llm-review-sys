# Bandit Social Learning under Myopic Behavior

Kiarash Banihashem

University of Maryland, College Park

kiarash@umd.edu &MohammadTaghi Hajiaghayi

University of Maryland, College Park

hajiagha@umd.edu &Suho Shin

University of Maryland, College Park

suhoshin@umd.edu &Aleksandrs Slivkins

Microsoft Research NYC

slivkins@microsoft.com

###### Abstract

We study social learning dynamics motivated by reviews on online platforms. The agents collectively follow a simple multi-armed bandit protocol, but each agent acts myopically, without regards to exploration. We allow a wide range of myopic behaviors that are consistent with (parameterized) confidence intervals for the arms' expected rewards. We derive stark exploration failures for any such behavior, and provide matching positive results. As a special case, we obtain the first general results on failure of the greedy algorithm in bandits, thus providing a theoretical foundation for why bandit algorithms should explore.1

## 1 Introduction

Reviews and ratings are pervasive in many online platforms. A customer consults reviews/ratings, then chooses a product and then (often) leaves feedback, which is aggregated by the platform and served to future customers. Collectively, customers face a tradeoff between _exploration_ and _exploitation_, _i.e.,_ between acquiring new information while making potentially suboptimal decisions and making optimal decisions using information currently available. However, individual customers tend to act myopically and favor exploitation, without regards to exploration for the sake of the others. On a high level, we ask **whether/how the myopic behavior interferes with efficient exploration**. We are particularly interested in _learning failures_ when only a few agents choose an optimal action.

We distill this issue down to its purest form. We posit that the customers make one decision each and do not observe any personalized payoff-relevant information prior to their decision, whether public or private. In particular, the customers believe they are similar to one another. They have only two alternative products/experiences to choose from, a.k.a., _arms_, and no way to infer anything about one arm from the other. The platform provides each customer with full history on the previous agents.2

Concretely, we posit _Bandit Social Learning_ (BSL): a variant of social learning in which the customers (henceforth, _agents_) arrive sequentially and follow a simple multi-armed bandit protocol. Each agent observes full history, chooses an arm, and receives a reward: a Bernoulli random draw whose mean is arm-specific and unknown. Initial knowledge (a dataset with some samples of each arm) may be available to all agents. When all agents are governed by a centralized algorithm, this setting is known as _stochastic bandits_, a standard and well-understood variant of multi-armed bandits.

We allow a wide range of myopic behaviors that are consistent with observations. Given an arm, consider the confidence interval for its mean reward, parameterized by \( 0\): the sample average plus/minus the "confidence term", \(}\). 3 An \(\)_-confident_ agent evaluates each arm to an _index_: some number within this arm's confidence interval (and otherwise arbitrary), and chooses an arm with a largest index. (Computational implementation of this process is irrelevant to our model.) Crucially, the \(\) and the agents' behavior are given and cannot be influenced by the platform.

This model subsumes the "unbiased" behavior, when the index equals the sample average, as well as various "behavioral biases" (see "related work" for citations). Most notably: "optimism" and "pessimism", when the index is, resp., larger or smaller than the sample average. (These can also be interpreted as, resp., risk seeking and risk aversion.) The model also allows for probabilistic decisions (via randomized indices), correlated behaviors (when samples from one arm affect the behavioral bias on another), and recency bias (when one favors more recent observations). Further, an agent may treat each arm differently, and different agents may exhibit different biases.

We target the regime when parameter \(\) is constant w.r.t. the number of agents \(T\). _I.e.,_ the agents' population is characterized by a constant \(\). We are interested in the asymptotic behavior when \(T\) increases. An extreme version of our model, with \((T)\), is only considered for intuition and sanity checks. Interestingly, this version subsumes two well-known bandit algorithms: UCB1  and Thompson Sampling , which achieve optimal regret bounds. These algorithms can be seen as behaviors: resp., extreme optimism and _probability matching_, a well-known randomized behavior. More "moderate" versions of these behaviors are consistent with \(\)-confidence as defined above, and are subject to the learning failures described below.

**Our results.** We are interested in learning failures when all but a few agents choose the bad arm, and how the failure probability scales with the \(\) parameter. Our main result is that with \(\)-confident agents, the failure probability is at least \(e^{-O()}\) (see Section 3). Consequently, regret is at least \((T e^{-O()})\) for any given problem instance, in contrast with the \(O( T)\) regret rate obtained by optimal bandit algorithms. Further, the \(e^{-O()}\) scaling is the best possible: indeed, regret for optimistic agents is at most \(O(\,T e^{-()}+\,)\) for a given problem instance (Theorem 4.1). Note that the negative result deteriorates as \(\) increases, and becomes vacuous when \( T\); the upper bound then essentially matches the optimal \(O( T)\) regret of the UCB algorithm ).

We refine these results in several directions. First, if all agents are "unbiased", the failure probability scales as the difference in expected reward between the two arms (Corollary 3.6). Second, if all agents are pessimistic, then any level of pessimism, whether small or large or different across agents, leads to the similar failure probability as in the unbiased case (Theorem 3.10). Third, a small fraction of optimists goes a long way! That is, if all agents are \(\)-confident and even a \(q\)-fraction of them are \(\)-optimistic, then we obtain regret \(O(\,T e^{-()}+/q\,)\) regardless of the other agents.4

Our results extend to Bayesian agents who have independent priors across the arms and act according to their posteriors. Such agents are consistent with our main model of \(\)-confident agents, and therefore are subject to the same negative results (Section 5). Further, we focus on Bayesian-unbiased agents and allow arbitrary _correlated_ Bayesian beliefs (when the agents can make inferences about one arm from the observations on the other). We derive a general result on learning failures, assuming that the mean rewards are actually drawn according to the beliefs (Section 6).

 Mean rewards & Beliefs & Behavior & Result \\  fixed & “frequentist” & \(\)-confident & Thm. 3.1 (main), \\  & confidence intervals & & Thm. 3.9 (small \(N_{0}\)). \\  & & unbiased/Greedy & Cor. 3.6 \\  & & \(_{t}\)-pessimistic & Thm. 3.10 \\  & Bayesian (independent) & Bayesian-unbiased & Thm. 5.1(a) \\  & & \(\)-Bayesian-confident & Thm. 5.1(b) \\ Bayesian (correlated) & Bayesian (and correct) & Bayesian-unbiased & Thm. 6.1 \\ 

Table 1: Our negative results: learning failures.

Our results are summarized in Tables 1 and 2.

**Implications for multi-armed bandits.** The negative results for unbiased agents can be seen as general results on the failure of the _greedy algorithm_: a bandit algorithm that always exploits. This is a theoretical foundation for why bandit algorithms should explore - and indeed why one should design them. We are not aware of any general results of this nature, whether published or known previously as "folklore", which is quite surprising given the enormous literature on multi-armed bandits. Therefore, we believe our results fill an important gap in the literature.

How surprising are these results? It has been folklore knowledge for several decades that the greedy algorithm is inefficient in some simple special cases, and folklore _belief_ that this should hold much more generally. However, recent results reveal a more complex picture: the greedy algorithm fails under some strong assumptions, but works well under some other strong assumptions (see Related Work). Thus, it has arguably became less clear which assumptions would be needed for negative results and what would be the "shape" and probability of learning failures.

Further, our results on \(\)-confident agents explain why UCB1 algorithm requires extreme optimism, why any algorithm based on narrow (constant-\(\)) confidence intervals is doomed to fail, and also why "pessimism under uncertainty" is not a productive approach for exploration.

**Novelty and significance.**BSL was not well-understood previously even with unbiased agents, as discussed above, let alone for more permissive behavioral models. It was very unclear a priori how to analyze learning failures and how strong would be the guarantees, in terms of the generality of agents' behaviors, the failure events/probabilities, and the technical assumptions.

On a technical level, our proofs have very little to do with standard lower-bound analyses in bandits stemming from . These analysis apply any algorithm and prove "sublinear" lower bounds on regret, such as \(( T)\) for a given problem instance and \(()\) in the worst case. Their main technical tool is KL-divergence analysis showing that no algorithm can distinguish between a given tuple of "similar" problem instances. In contrast, we prove _linear_ lower bounds on regret, our results apply to a particular family of behaviors/algorithms, and we never consider a tuple of similar problem instances. Instead, we use anti-concentration and martingale tools to argue that the best arm is never played (or played only a few times), with some probability. The result on correlated beliefs in Section 6 has a rather short but "conceptual" proof which we believe is well-suited for a textbook.

While our positive results in Section 4 are restricted to "optimistic" agents, we do not assert that such agents are necessarily typical. The primary point here is that our results on learning failures are essentially tight. That said, "optimism" is a well-documented behavioral bias (_e.g.,_ see  and references therein). So, a small fraction of optimists (leveraged in Theorem 4.5) is not unrealistic.

Our proofs are more involved compared to the standard analysis of the UCB1 algorithm. This is because we cannot make the \(\) parameter as large as needed to ensure that the complements of certain "clean events" can be ignored. Instead, we need to define and analyze these "clean events" in a more careful way. These difficulties are compounded in Theorem 4.5, our most general result. As far as the statements are concerned, the basic result in Theorem 4.1 is perhaps what one would expect to hold, whereas the extensions in Theorem 4.4 and Theorem 4.5 are more surprising.

**Framing.** We target the scenario in social learning when both actions and rewards are observable in the future, and the agents do not receive any other payoff-relevant signals. As in much of algorithmic game theory, we discuss the influence of self-interested behavior on the overall welfare of the system. We consider how such behaviour can cause "learning failures", which is a typical framing in the literature on social learning. From the perspective of multi-armed bandits, we investigate the failures of the greedy algorithm, and more generally any algorithm that operates on narrow confidence intervals. We do not attempt to design new algorithms, as a version of UCB1 is proved optimal.

 Mean rewards & Beliefs & Behavior & Result \\  fixed & “frequentist” & \(\)-optimistic & Thm. 4.1 \\  & confidence & \(_{t}\)-optimistic, \(_{t}[,_{}]\) & Thm. 4.4 \\  & intervals & small fraction of optimists & Thm. 4.5 \\ 

Table 2: Our positive results: upper bounds on regret.

**Map of the paper.** Section 2 defines our model. Section 3 derives the learning failures. Section 4 provides positive results for optimistic agents. Section 5 and Section 6 handle agents with Bayesian beliefs. Most proofs are moved to the supplement.

**Related work.** A vast literature on social learning studies agents that learn over time in a shared environment. A prominent topic is learning failures such as ours. Models vary across several dimensions: _e.g.,_ the information acquired/transmitted, the communication network, agents' life-span and decision rules, etc. All models from prior work are very different from ours. In the supplement, we separate our model from several most relevant ones: "sequential social learning" , "strategic experimentation" , networked myopic learners [8; 46], and misspecified beliefs [31; 15; 25; 44].

Positive results for the greedy bandit algorithm [37; 11; 51] focus on _contextual bandits_, an extension of stochastic bandits where a payoff-relevant signal (_context_) is available before each round. Equivalently, each agent in BSL receives such signal along with the history (incl. all previous signals). Very strong assumptions are needed: linearity of rewards and diversity of contexts. A similar result holds for BSL with _private_ signals, under different (and also very strong) assumptions on structure and diversity . In all this work, agents' diversity substitutes for exploration, and structural assumptions allow aggregation across agents. Moreover, the greedy algorithm obtains \(o(T)\) regret in various scenarios with a very large number of near-optimal arms [13; 35], _e.g.,_ in Bayesian bandits with \(K\) arms and independent uniform priors. We focus on a more basic model, with only two arms and no contexts, where all these channels are ruled out.

Learning failures for the greedy algorithm are derived for bandit problems with 1-dimensional action spaces under (strong) structural assumptions: _e.g.,_ dynamic pricing with linear demands [30; 22] and dynamic control in a (generalized) linear model [42; 40]. In all these results, the failure probability is only proved positive, but not otherwise characterized.

_Incentivized exploration_ takes a mechanism design perspective on BSL, whereby the platform strives to incentivize individual agents to explore for the sake of the common good. In most of this work, starting from [41; 19], the platform controls the information flow, _e.g.,_ can withhold history and instead issue recommendations, and uses this information asymmetry to create incentives; see , [54, Ch. 11] for surveys.5 In particular, [48; 34; 53] target stochastic bandits as the underlying learning problem, same as we do. In , the platform constructs a (very) particular communication network for the agents, and then the agents engage in BSL on this network.

Non-Bayesian models of behavior are prominent in social learning, starting from DeGroot : agents use variants of statistical inference and/or naive rules-of-thumb to infer the state of the world. In particular, our model of \(\)-confident agents is essentially a special case of "case-based decision theory" . Well-documented behavioral biases allowed by our model include: optimism and pessimism (_e.g.,_ and [18; 12], resp., and references therein), risk aversion/risk seeking [36; 10], recency bias (_e.g.,_ and references therein), randomized decisions (with theory tracing back to Luce ), and _probability matching_ more specifically [49; 59].

Our perspective of multi-armed bandits is very standard in machine learning theory: we consider asymptotic regret rates without time-discounting. The vast literature on regret-minimizing bandits is summarized in books [17; 54; 45]. Stochastic bandits is a standard, basic version with i.i.d. rewards and no auxiliary structure. Most relevant are the UCB1 algorithm , Thompson Sampling [57; 52] (particularly the "frequentist" analyses thereof [2; 4; 38]), and the lower-bound results [43; 7].

## 2 Our model and preliminaries

Our model, called **Bandit Social Learning**, is defined as follows. There are \(T\) rounds, where \(T\) is the time horizon, and two _arms_ (_i.e.,_ alternative actions). We use \([T]\) and \(\) to denote the set of rounds and arms, respectively.6 In each round \(t[T]\), a new agent arrives, observes history \(_{t}\) (defined below), chooses an arm \(a_{t}\), receives reward \(r_{t}\) for this arm, and leaves forever. When a given arm \(a\) is chosen, its reward is drawn independently from Bernoulli distributionwith mean \(_{a}\). 7 The mean reward is fixed over time, but not known to the agents. Some initial data is available to all agents, namely \(N_{0} 1\) samples of each arm \(a\). We denote them \(r^{0}_{a,i}\), \(i[N_{0}]\). The history in round \(t\) consists of both the initial data and the data generated by the previous agents. Formally, it is a tuple of arm-reward pairs,

\[_{t}:=\,(a,r^{0}_{a,i}):\ a,i[N_{0}];\ (a_{s},r_{s}):\ s [t-1]\,\,.\]

We summarize the protocol for Bandit Social Learning as Protocol 1.

``` Problem instance: two arms \(a\) with (fixed, but unknown) mean rewards \(_{1},_{2}\) ; Initialization: \(\{\,N_{0}\) samples of each arm \(\}\); for each round \(t=1,2,,T\)do  agent \(t\) arrives, observes \(\) and chooses an arm \(a_{t}\) ;  reward \(r_{t}\) is drawn from Bernoulli distribution with mean \(_{a_{t}}\);  new datapoint \((a_{t},r_{t})\) is added to \(\) ```

**Protocol 1**Bandit Social Learning

_Remark 2.1_.: The initial data-points represent reports created outside our model, _e.g.,_ by ghost shoppers or influencers, and available before the products enter the market. One could interpret them as a simple "frequentist" representation for the initial beliefs of the agents, with \(N_{0}\) as the beliefs' "strength". We posit \(N_{0} 1\) to ensure that the arms' average rewards are always well-defined.

If the agents were controlled by an algorithm, this protocol would correspond to _stochastic bandits_ with two arms, the most basic version of multi-armed bandits. A standard performance measure in multi-armed bandits (and online machine learning more generally) is _regret_, defined as

\[(T):=^{*} T-\,\,_{t[T]}\ _{a_{t}}\,,\] (2.1)

where \(^{*}=(_{1},_{2})\) is the maximal expected reward of an arm.

Each agent \(t\) chooses its arm \(a_{t}\) myopically. Each agent is endowed with some (possibly randomized) mapping from histories to arms, and chooses an arm accordingly. This mapping, called _behavioral type_, encapsulates how the agent resolves uncertainty on the rewards. More concretely, each agent maps the observed history \(_{t}\) to an _index_\(_{a,t}\) for each arm \(a\), and chooses an arm with a largest index. The ties are broken independently and uniformly at random.

We allow for a range of myopic behaviors, whereby each index can take an arbitrary value in the (parameterized) confidence interval for the corresponding arm. Formally, fix arm \(a\) and round \(t[T]\). Let \(n_{a,t}\) denote the number of times this arm has been chosen in the history \(_{t}\) (including the initial data), and let \(_{a,t}\) denote the corresponding average reward. Given these samples, standard (frequentist, truncated) upper and lower confidence bounds for the arm's mean reward \(_{a}\) (UCB and LCB, for short) are defined as follows:

\[^{}_{a,t}:=\,1,_{a,t}+}\,}^{}_{a,t}:=\,0, _{a,t}-}\,}\,,\] (2.2)

where \( 0\) is a parameter. The interval \(\,^{}_{a,t},^{}_{a,t}\,\) will be referred to as \(\)-_confidence interval_. Standard concentration inequalities imply that \(_{a}\) is contained in this interval with probability at least \(1-2\,e^{-2}\) (where the probability is over the random rewards, for any fixed value of \(_{a}\)). We allow the index to take an arbitrary value in this interval:

\[_{a,t}\,^{}_{a,t},^{}_{ a,t}\,\,,.\] (2.3)

We refer to such agents as \(\)_-confident_; \(>0\) will be a crucial parameter throughout.

**On special cases.** Our model accommodates a number of behavioural biases. Most notably: _unbiased agents_, who set \(_{a,t}=_{a,t}\), \(\)_-optimistic agents_, who set \(_{a,t}=^{}_{a,t}\), and \(\)_- pessimistic agents_ who set \(_{a,t}=^{}_{a,t}\). Unbiased agents formally correspond to the _greedy algorithm_, whereas _extreme_ optimism, i.e., \(\)-optimism with \((T)\), corresponds to UCB1 algorithm .

Our model also allows a version of Thompson Sampling in which the posterior samples are truncated to the \(\)-confidence interval. 8 More generally, we allow _Bayesian agents_ that preprocess observations to a Bayesian posterior, and use the latter to define their indices. See the supplement for more details.

**Preliminaries.** When \(_{1},_{2}\) are fixed (not drawn from a prior), we posit \(_{1}>_{2}\), _i.e.,_ arm \(1\) is the _good arm_, and arm \(2\) is the _bad arm_. Our guarantees depend on quantity \(:=_{1}-_{2}\), called the _gap_ (between the two arms). It is a very standard quantity for regret bounds in multi-armed bandits.

We use the big-O notation to hide constant factors. Specifically, \(O(X)\) and \((X)\) mean, resp., "at most \(c_{0} X\)" and "at least \(c_{0} X\)" for some absolute constant \(c_{0}>0\) that is not specified in the paper. When and if \(c_{0}\) depends on some other absolute constant \(c\) that we specify explicitly, we point this out in words and/or by writing, resp., \(O_{c}(X)\) and \(_{c}(X)\). As usual, \((X)\) is a shorthand for "both \(O(X)\) and \((X)\)", and writing \(_{c}(X)\) emphasizes the dependence on \(c\).

Algorithms UCB1 and Thompson Sampling achieve regret

\[(T) O(\;(\;1/,\;) T\;).\] (2.4)

This regret rate is essentially optimal among all bandit algorithms: it is optimal up to constant factors for fixed \(>0\), and up to \(O( T)\) factors for fixed \(T\) (see "related work" for citations).

A key property of a reasonable bandit algorithm is that \((T)/T 0\); this property is also called _no-regret_. Conversely, algorithms with \((T)(T)\) are considered very inefficient.

## 3 Learning failures

We are interested in learning failures when all but a few agents choose the bad arm. More precisely, we define the \(n\)-_sampling failure_ as an event that all but at most \(n\) agents choose the bad arm.

We make two technical assumptions:

mean rewards satisfy

\[c<_{2}<_{1}<1-c\]

 for some absolute constant

\[c(0,}{{2}}),\] (3.1) the number of initial samples satisfies

\[N_{0} 64\,/c^{2}+1/c\] (3.2)

The meaning of (3.1) is that it rules out degenerate behaviors when mean rewards are close to the known upper/lower bounds. The big-O notation hides the dependence on the absolute constant \(c\), when and if explicitly stated so. Assumption (3.2) ensures that the \(\)-confidence interval is a proper subset of \(\) for all agents; we sidestep this assumption later in Theorem 3.9.

Our main result allows arbitrary \(\)-confident agents and asserts that \(0\)-sampling failures happen with probability at least \(p_{} e^{-O()}\). This is a stark failure when \(\) is a constant relative to \(T\).

**Theorem 3.1** (\(\)-confident agents).: _Suppose all agents are \(\)-confident, for some fixed \( 0\). Make assumptions (3.1) and (3.2). Then the \(0\)-sampling failure occurs with probability at least 9_

\[p_{}=_{c}(\;+}\;) e^{-O_{c} (\;\;+\;N_{0}^{2}\;)},=_{1 }-_{2}.\] (3.3)

_Consequently, \((T) p_{} T\)._

We emphasize generality: the agents can exhibit any behaviors consistent with \(\)-confidence, possibly different for different agents and different arms. From multi-armed bandit perspective, the theorem implies that bandit algorithms consistent with \(\)-confidence cannot have regret sublinear in \(T\).

The guarantee in Theorem 3.1 deteriorates as the parameter \(\) increases, and becomes vacuous when \((T)\). This makes sense, as this regime of \(\) is used in UCB1 algorithm.

_Discussion 3.2_.: Assumption (3.2) is innocuous from the social learning perspective: essentially, the agents hold initial beliefs grounded in data and these beliefs are not completely uninformed. From the bandit perspective, this assumption is more substantive, as an algorithm can always choose to discard data. In any case, we remove this assumption in Theorem 3.9 below.

_Remark 3.3_.: A weaker version of (3.2), namely \(N_{0}\), is necessary to guarantee an \(n\)-sampling failure for any \(\)-confident agents. Indeed, suppose all agents are \(\)-optimistic for arm \(1\) (the good arm), and \(\)-pessimistic for arm \(2\) (the bad arm). If \(N_{0}<\), then the index for arm \(2\) is \(0\) after the initial samples, whereas the index of arm \(1\) is always positive. Then all agents choose arm \(1\).

Next, we spell out two corollaries which help elucidate the main result.

**Corollary 3.4**.: _If the gap is sufficiently small, \(<O(\,1/}\,)\), then Theorem 3.1 holds with_

\[p_{}=_{c}(\,+}\,) e^{-O_{c}( )}.\] (3.4)

_Remark 3.5_.: The assumption in Corollary 3.4 is quite mild in light of the fact that when \(>(\,}\,)\), the initial samples suffice to determine the best arm with high probability.

**Corollary 3.6**.: _If all agents are unbiased, then Theorem 3.1 holds with \(=0\) and_

\[p_{} =_{c}(\,\,) e^{-O_{c}(\,N_{0}\, ^{2}\,)}\] (3.5) \[=_{c}(\,\,)<O(\,1/}\,).\]

_Remark 3.7_.: A trivial failure result for unbiased agents relies on the event \(\) that all initial samples of arm \(1\) (_i.e.,_ the good arm) are realized as \(0\). This would indeed imply a \(0\)-sampling failure (as long as at least one initial sample of arm \(1\) is realized to \(1\)), but the event \(\) happens with probability exponential in \(N_{0}\), the number of initial samples. In contrast, in our result \(p_{}\) only depends on \(N_{0}\) through the assumption that \(<O(\,1/}\,)\).

_Discussion 3.8_.: Corollary 3.6 can be seen as a general result on the failure of the greedy algorithm. This is the first such result with a non-trivial dependence on \(N_{0}\), to the best of our knowledge.

We can remove assumption (3.2) and allow a small \(N_{0}\) if the behavioral type for each agent \(t\) also satisfies natural (and very mild) properties of symmetry and monotonicity:

1. _(symmetry)_ if all rewards in \(_{t}\) are \(0\), the two arms are treated symmetrically;10 2. _(monotonicity)_ Fix any arm \(a\), any \(t\)-round history \(H\) in which all rewards are \(0\) for both arms, and any other \(t\)-round history \(H^{}\) that contains the same number of samples of arm \(a\) such that all these samples have reward \(1\). Then \[[\,a_{t}=a_{t}=H^{}\,][\,a_ {t}=a_{t}=H\,].\] (3.6)

Note that both properties would still be natural and mild even without the "all rewards are zero" clause. The resulting guarantee on the failure probability is somewhat cleaner.

**Theorem 3.9** (small \(N_{0}\)).: _Fix \( 0\), assume Eq. (3.1), and let \(N_{0}[1,N^{*}]\), where \(N^{*}:= 64/c^{2}+1/c\). Suppose each agent \(t\) is \(\)-confident and satisfies properties (P1) and (P2). Then an \(n\)-sampling failure, \(n=N^{*}-N_{0}\), occurs with probability at least_

\[p_{}=_{c}(\,c^{2N^{*}}\,)=_{c}(\,e^{-O_{c}()}\,).\] (3.7)

_Consequently, \((T) p_{}(T-n)\)._

If all agents are pessimistic, we find that _any levels of pessimism_, whether small or large or different across agents, lead to a \(0\)-sampling failure with probability \(_{c}()\), matching Corollary 3.6 for the unbiased behavior. This happens in the (very reasonable) regime when

\[_{c}()<N_{0}<O(1/^{2}).\] (3.8)

**Theorem 3.10** (pessimistic agents).: _Suppose each agent \(t[T]\) is \(_{t}\)-pessimistic, for some \(_{t} 0\). Suppose assumptions (3.1) and (3.2) hold for \(=_{t[T]}_{t}\). Then the \(0\)-sampling failure occurs with probability lower-bounded by Eq. (3.5). Consequently, \((T)_{c}(^{2}) e^{-O_{c}(\,N_{0}\, ^{2}\,)}\)._

We allow extremely pessimistic agents (\(_{t} T\)), and the pessimism levels \(_{t}\) can vary across agents \(t\). While the relevant parameter is \(=_{t[T]}_{t}\), the failure probability in (3.5) does not contain the \(e^{-}\) term. In particular, \(p_{}=()\) when \(N_{0}<O(1/^{2})\). However, the dependence on \(\) "creeps in" through assumption (3.2), _i.e.,_ that \(N_{0}>_{c}()\).

**Proof Overview.** We first show that the average reward of arm 1 (the good arm), is upper bounded by some threshold \(q_{1}\). This is only guaranteed with some probability and only when this arm is sampled exactly \(N\) times, for a particular \(N N_{0}\). Next, we lower bound the average reward of arm 2 (the bad arm): we show that with some probability it is _always_ above some threshold \(q_{2}(q_{1},_{2})\). Focuson the round \(t^{*}\) when the good arm is sampled for the \(N\)-th time (if this ever happens). If both of these events hold, from round \(t^{*}\) onwards the bad arm will have a larger average reward by a constant margin \(q_{2}-q_{1}\). Consequently, as we prove, the bad arm has a larger index, and therefore gets chosen.

The details of this argument differ from one theorem to another. For Theorem 3.1, it suffices to set the thresholds \(q_{2},q_{1}\) such that \(q_{2}-q_{1}=(})\). For Theorem 3.10, we use a more involved argument: since the LCB of an arm increases when it is played, playing this arm only strengthens the preference of pessimistic agents for this arm. We are therefore less constrained in the choice of \(q_{1},q_{2}\) and we can prove that a learning failure occurs whenever \(q_{1}<q_{2}\).11 In both proofs, we also require \(q_{1}\) and \(q_{2}\) to be close to \(_{1}\) and \(_{2}\), resp., so as to lower-bound the probability of the two desirable events. For Theorem 3.9, the case of small \(N_{0}\), our analysis becomes more subtle. We can (in some sense) simplify the two events defined above, but we need to introduce a _third_ event: if arm 1 is chosen by at least \(n\) agents (for a suitably defined \(n\)), then arm 2 is chosen by \(n\) agents before arm 1 is. The crux is a "deterministic" argument which derives a failure when all three events hold jointly.

To formalize, we represent realized rewards of each arm \(a\) as written out in advance on a "tape", where each entry is an independent Bernoulli draw with mean \(_{a}\).12 The \(i\)-th entry is returned as reward when and if arm \(a\) is chosen for the \(i\)-th time. (We start counting from the initial samples, which comprise entries \(i[N_{0}]\).) We analyze each arm separately (and then invoke independence).

We use some tools from probability: a sharp anti-concentration inequality for arm 1 and a martingale argument for arm 2. Let \((X_{i})_{i}\) be a sequence of i.i.d. Bernoulli random variables with mean \(p[c,1-c]\), for some absolute constant \(c(0,}{{2}})\). The anti-concentration is as follows:

\[(\, n 1/c,\ q(c/8,\,p)\,)\,_{i=1} ^{n}X_{i} q\,(\,e^{-O(n(p-q)^{2})} \,),\] (3.9)

The martingale argument leads to this:

\[ q[0,p)\, n 1:_{i=1} ^{n}X_{i} q\,_{c}(p-q).\] (3.10)

We each tool to the tape for the respective arm, and lower bound the probability of the desirable event.

While the novelty is mainly in how we _use_ these tools, the tools themselves are not very standard. Eq. (C.1) follows from the anti-concentration inequality in  and a reverse Pinsker inequality in . More standard anti-concentration results via Stirling's approximation lead to an additional factor of \(1/\) on the right-hand side of (C.1). For Eq. (C.2), we introduce an exponential martingale and relate the event in Eq. (C.2) to a deviation of this martingale. We then use Ville's inequality (a version of Doob's martingale inequality) to bound the probability that this deviation occurs.

## 4 Upper bounds for optimistic agents

We upper-bound regret for optimistic agents: we match the exponential-in-\(\) scaling from Corollary 3.4 and then extend this result to different behavioral types. On a technical level, we prove three regret bounds of the same shape (4.1), but with a different \(\) term. (The unified presentation emphasizes this similarity.) Throughout, \(=_{1}-_{2}\) denotes the gap between the two arms.

**Theorem 4.1**.: _Suppose all agents are \(\)-optimistic, for some fixed \(>0\). Then, letting \(=\),_

\[(T) O\,\,T e^{-()}(1+ }{{}})\ +\ /\,\,.\] (4.1)

_Discussion 4.2_.: The main take-away is that the exponential-in-\(\) scaling from Corollary 3.4 is tight for \(\)-optimistic agents, and therefore the best possible lower bound for \(\)-confident agents. This result holds for any given \(N_{0}\), the number of initial samples.13 Our guarantee remains optimal in the "extreme optimism" regime when \((T)\), matching the optimal regret rate, \(O((T)/\,)\).

What if different agents can hold different behavioral types? First, let us allow agents to have varying amounts of optimism, possibly different across arms and possibly randomized.

**Definition 4.3**.: Fix \(_{}>0\). An agent \(t[T]\) is called _\([\,,_{}\,]\)-optimistic_ if its index \(_{a,t}\) lies in the interval \([\,_{a,t}^{},_{a,t}^{_{}}\,]\), for each arm \(a\).

We show that the guarantee in Theorem 4.1 is robust to varying the optimism level "upwards".

**Theorem 4.4** (robustness).: _Fix \(_{}>0\). Suppose all agents are \([\,,_{}\,]\)-optimistic. Then regret bound (4.1) holds with \(=_{}\)._

Note that the upper bound \(_{}\) has only a mild influence on the regret bound in Theorem 4.4.

Our most general result only requires a small fraction of agents to be optimistic, whereas all agents are only required to be \(_{}\)-confident (allowing all behaviors consistent with that).

**Theorem 4.5** (recurring optimism).: _Fix \(_{}>0\). Suppose all agents are \(_{}\)-confident. Further, suppose each agent's behavioral type is chosen independently at random so that the agent is \([\,,_{}\,]\)-optimistic with probability at least \(q>0\). Then regret bound (4.1) holds with \(=_{}/q\)._

Thus, with even a small fraction of optimists, \(q>}\), the behavioral type of less optimistic agents does not have much impact on regret. In particular, it does not hurt much if they become very pessimistic. A **small fraction of optimists goes a long way!** Further, a small-but-constant fraction of _extreme_ optimists, _i.e., \(,_{}(T)\)_ in Theorem 4.5, yields optimal regret rate, \((T)/\).

## 5 Learning failures for Bayesian agents

In this section, we posit that agents are endowed with Bayesian beliefs. The basic version is that all agents believe that the mean reward of each arm is initially drawn from a uniform distribution on \(\). (We emphasize that the mean rewards are fixed and _not_ actually drawn according to these beliefs.) Each agent \(t\) computes a posterior \(_{a,t}\) for \(_{a}\) given the history \(_{t}\), for each arm \(a[a]\), and maps this posterior to the index \(_{a,t}\) for this arm.14

The basic behavior is that \(_{a,t}\) is the posterior mean reward, \([\,_{a,t}\,]\). We call such agents _Bayes-unbiased_. Further, we consider a Bayesian version of \(\)-confident agents, defined by

\[_{a,t}[\,Q_{a,t}(),\,\,Q_{a,t}(1-)\,] ,\] (5.1)

where \(Q_{a,t}()\) denotes the quantile function of the posterior \(_{a,t}\) and \((0,}{{2}})\) is a fixed parameter (analogous to \(\) elsewhere). The interval in Eq. (5.1) is a Bayesian version of \(\)-confidence intervals. Agents \(t\) that satisfy Eq. (5.1) are called \(\)_-Bayes-confident_.

We allow more general beliefs given by independent Beta distributions. For each arm \(a\), all agents believe that the mean reward \(_{a}\) is initially drawn as an independent sample from Beta distribution with parameters \(_{a},_{a}\). Our results are driven by parameter \(M=_{a}_{a}+_{a}\). We refer to such beliefs as _Beta-beliefs with strength_\(M\). The intuition is that the prior on each arm \(a\) can be interpreted as being "based on" \(_{a}+_{a}-2\) samples from this arm.15

Our technical contribution here is that Bayes-unbiased (resp., \(\)-Bayes-confident) agents are \(\)-confident for a suitably large \(\), and therefore subject to the learning failure in Theorem 3.1.

**Theorem 5.1**.: _Consider a Bayesian agent that holds Beta-beliefs with strength \(M 1\)._

1. _If the agent is Bayes-unbiased, then it is_ \(\)_-confident for some_ \(=O(M/})\)_._
2. _If the agent is_ \(\)_-Bayes-confident, then it is_ \(\)_-confident for_ \(=O(\,M/}+(1/)\,)\)_._

_Discussion 5.2_.: Beta-beliefs may be completely unrelated to the actual mean rewards. If \(\) and \(M\) are constants relative to \(T\), the resulting \(\) is constant, too. Our guarantee is stronger if the beliefs are weak (_i.e.,_\(M\) is small) or are "dominated" by the initial samples, in the sense that \(N_{0}>(M^{2})\).

_Discussion 5.3_.: \(\)-Bayes-confident agents subsume Bayesian version of optimism and pessimism, where the index \(_{a,t}\) is defined as, resp., \(Q_{a,t}(1-)\) and \(Q_{a,t}()\), as well as all the Bayesian versions of all other behaviorial biases discussed previously as special cases of \(\)-confidence.

## 6 Bayesian model with arbitrary priors

We consider Bayesian-unbiased agents in a "fully Bayesian" model such that the mean rewards are actually drawn from a prior. We are interested in _Bayesian probability_ and _Bayesian regret_, _i.e.,_ resp.,probability and regret in expectation over the prior. We focus on learning failures when the agents never choose an arm with the largest prior mean reward (as opposed to an arm with the largest _realized_ mean reward, which is not necessarily the same arm). We do not explicitly allow initial samples (_i.e.,_ we posit \(N_{0}=0\) here), because they are implicitly included in the prior.

Compared to Section 5, the benefit is that we allow arbitrary priors, possibly correlated across the two arms. Further, our guarantee does not depend on the prior, other than through the _prior gap_\([_{1}-_{2}]\), and does not contain any hidden constants. On the other hand, the guarantees here are only in expectation over the prior, whereas the ones in Section 5 hold for fixed \(_{1},_{2}\). Also, our result here is restricted to Bayesian-unbiased agents.

**Theorem 6.1**.: _Suppose the pair \((_{1},_{2})\) is initially drawn from some Bayesian prior \(\) such that \([_{1}]>[_{2}]\). Assume that all agents are Bayesian-unbiased, with beliefs given by \(\). Then with Bayesian probability at least \([_{1}-_{2}]\), the agents never choose arm \(2\)._

Proof.: W.l.o.g., assume that agents break ties in favor of arm \(2\). In each round \(t\), the key quantity is \(Z_{t}=[_{1}-_{2}_{t}]\). Indeed, arm \(2\) is chosen if and only if \(Z_{t} 0\). Let \(\) be the first round when arm \(2\) is chosen, or \(T+1\) if this never happens. We use martingale techniques to prove that

\[[Z_{}]=[_{1}-_{2}].\] (6.1)

We use the optional stopping theorem (OST). We observe that \(\) is a stopping time relative to \(=(_{t}:\,t[T+1]\,)\), and \((\,Z_{t}:t[T+1]\,)\) is a martingale relative to \(\). 16 OST asserts that \([Z_{}]=[Z_{1}]\) for any martingale \(Z_{t}\) and any bounded stopping time \(\). Eq. (6.1) follows because \([Z_{1}]=[_{1}-_{2}]\). On the other hand, by Bayes' theorem it holds that

\[[Z_{}]=[Z_{}][\,>T\,]= [\,2\,].\]

As a corollary, we derive a 0-sampling failure, leading to \((T)\) Bayesian regret. Specifically, the agents start out playing arm \(1\) (because it has a higher prior mean reward), and never try arm \(2\)_when it is in fact the best arm_. This happens whenever the prior is independent across arms and has a positive density on the entire \(\) interval (see the supplement for the exact statement). Note that it is a (much) more general family of priors compared to independent Beta-priors allowed in Section 5.

## 7 Conclusions and open questions

We examine the dynamics of social learning in a multi-armed bandit scenario, where agents sequentially choose arms and receive rewards, and observe the full history of previous agents. For a range of agents' myopic behavior, we investigate how they impact exploration, and provide tight upper and lower bounds on the learning failure probabilities and regret rates. In particular, we obtain the first general results on the failure of the greedy algorithm in bandits.

With our results as a "departure point", one could study BSL in more complex bandit models with many arms and/or some known structure of rewards that the agents' myopic behaviour would account for.17 The greedy algorithm fails for some structures (_e.g.,_ our current model) and works well for some others (_e.g.,_ for linear contextual bandits with smoothed contexts , or when all arms have the same rewards). The whole world is in between these two extremes. It is not at all clear which structures would cause learning failures and which would enable learning, and which structures would be amenable to analysis, one way or another.

## 8 Acknowledgements

This work is partially supported by DARPA QuICC NSF and AF:Small #2218678, #2114269.