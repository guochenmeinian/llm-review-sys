# Attention Bias as an Inductive Bias: How to Teach Transformers Simple Arithmetic

Shaoxiong Duan, Yining Shi

ICC, RDFZ

shaoxiongduan@gmail.com &Wei Xu

Institute for Interdisciplinary Information Sciences

Tsinghua University

###### Abstract

In this paper, we study the Transformer model's capability in learning arithmetic from an inductive learning perspective and draw attention to the importance of inductive biases. We first introduce a definition of length generalization, requiring the model to maintain near perfect accuracy on samples with length at least 10 times the training length, as an indicator of successful learning. Through experiments and attention analysis, we show that the failure of the vanilla Transformer on learning arithmetic is due to inadequate inductive biasing. We then present Attention Bias Scaffolding (ABS) which uses attention masking to enforce the necessary inductive bias, making it the _first_ Transformer-based architecture to achieve _complete_ generalization on several arithmetic tasks such as addition and parity. Additionally, we introduce Attention Bias Calibration (ABC), a calibration stage that allows the model to learn the proper attention biases, and obtain complete length generalization _automatically_ on tasks that could interpolate. 1

## 1 Introduction

Transformers have been the fundamental building blocks of many SOTA solutions across a wide range of machine learning tasks, yet they struggle to model many simple formal languages, such as addition and parity. In this paper we study the problem from an inductive learning perspective, since the tasks are, by nature, inductive learning: the process of inferring general rules from finite number of observations. Successful learning of the desired generation rules allows the model to produce correct results regardless of the input length, as long as resources permit. Thus we use length generalization, defined as the model's ability to maintain at least 99% accuracy when tested on samples with lengths at least 10 times the training length, as an indicator to differentiate successful learning from memorization of surface statistics.

Arithmetic has been known to be hard for Transformers. There are some works that achieve a certain level of generalization on some of the tasks we study but they all use specially constructed architectures [6; 9]. The extensive empirical studies conducted by Deletang et al.  and Ruoss et al. , which consider five major position encodings, obtain only slightly better results than random guessing on parity, and 54.3% and 64.5% on binary addition, respectively. Neither of the studies show signs of generalization.

In fact, some works even imply a certain theoretical impossibility. Bhattamishra et al.  provides evidence that Transformers are relatively more biased towards functions of low sensitivity, which does not include parity. The RASP-Generalization Conjecture in Zhou et al.  indicates that Transformers tend to learn a length-generalizing solution if there exists a short RASP-L program that works for all input lengths. Again this condition excludes both addition and parity.

Parity is the simplest non-counter-free regular language, the lowest layer of the Chomsky hierarchy. This limitation may imply an impossibility for Transformers to solve any regular language that is not counter-free . Attempts to overcome the limitations include scratchpads, index hints, reversing the output order, as well as allowing model weights to include \(\) etc. They only achieve mild generalization (e.g., from 30 digits to 50 , or 2.5\(\) extrapolation ).

**Our Contributions**. It is known that inductive learning requires inductive biases , additional assumptions that are independent of the data. This is because any finite number of training samples has infinite possible continuations corresponding to different generation rules. Failures of previous works indicate that, while the model architecture is an important source of inductive bias, it may not be adequate to enable true learning.

We make the following contribution in addressing this limitation: (1) We show that attention biasing is an effective way to enforce inductive bias for Transformers; (2) We propose _attention biasing scaffolding_ (ABS) which biases the attention directly to introduce proper inductive bias, making it the _first_ Transformer-based architecture to obtain _complete_ generalization on a number of arithmetic tasks; (3) We extend ABS to _attention bias calibration_ (ABC), a process that collects attention patterns learned from training data and extends them to long lengths, enabling Transformers to learn the algorithms automatically. We also show ABC's relation to RPE  and LoRA , which indicates the potential for its applications to more complicated tasks.

Figure 1 summarizes the generalization that our ABC scheme achieves on two of the tasks we study, with comparisons against popular alternatives such as ALiBi , RoPE , etc. Ours is the only solution achieving perfect generalization. We obtain similar results on other tasks.

## 2 Setup

**Tasks**. Let \(\) be the set of natural numbers. We consider the following 4 arithmetic tasks: (1) \(\): \(S(n)=n+1\) for \(n\); (2) \(\): \(y=x_{1}+x_{2}\) for \(x_{1},x_{2}\); (3) \(\): \(y=_{i=1}^{n}x[i]\) where \(x[i]\) denotes the \(i\)-th bit of \(x\) in a binary representation and \(\) is bitwise xor; and (4) \(N 1\): \(y=x_{1} x_{2}\) for \(x_{1}\) and \(x_{2}\{0,1,,9\}\)., i.e., a restricted form of multiplication where one of the operands is restricted to single-digit.

These tasks are well-known examples in the theory of computation. The seemingly trivial \(\) is the basic component of Peano axioms, which formalize the structure of the natural numbers. \(\), \(\) and \(N 1\) all belong to the Type-1 context-sensitive (CS) category of the Chomsky hierarchy, while \(\) is in Type-3 Regular (R) category . \(N 1\) is a task that, unlike \(\), involves more complex carry, which can be any digit among \(\{0,1,,9\}\).

**Problem Representation**. We tokenize the sequences into digits, which can represent infinite number of integers using a finite set of tokens, enabling unbounded extrapolation testing. With this tokenization, all tasks are naturally sequence-to-sequence except for \(\) which is classification. We turn \(\) into a sequence-to-sequence task using the scratch pad approach similar to . We also reverse the ordering of the output sequence to match the natural generation process.

Figure 1: Generalization results for models trained on 6-digit samples.

**Model Configuration**. We train a small encoder-decoder Transformer from scratch using cross-entropy loss and Adam optimizer. The training length is restricted to 6 digits and we test the model using lengths up to 60 digits. We use greedy decoding for all inferences. Exact match is used as the criteria for accuracy. The detailed model configuration and training setup is provided in appendix B.

## 3 (The Right) Attention is All You Need

We first train vanilla Transformers with some commonly used positional encodings. The results on Successor and Addition have been shown in figure 1. All models achieve some levels of interpolation but none could extrapolate. RoPE and the vanilla Transformer perform almost identically, dropping precipitously to almost 0 accuracy once the length goes beyond 6. We observe similar patterns with other tasks.

To figure out the causes of failure, we extract and analyze the model's attention weights. Figure 2 shows the attention heat maps of one specific head in the last decoder layer when decoding Successor, and two heads for Addition. Detailed analysis is presented in appendix C.3 but the patterns are very clear: the vanilla Transformer correctly learns the right attention patterns up to the training length and fails beyond that. This correlates perfectly with the extrapolation performance shown in figure 1.

### Attention Bias Scaffolding

We then introduce several methods that guide the model to attend to the right places. Assisting model learning is a common practice. Relevant techniques include inputs combination , "arithmetic prompting" , representation transformation , scratch pad , etc. Indeed, most of our methods are drawn from these toolboxes as well. However, we use them to target directly at the attention, thus we call our approach Attention Bias Scaffolding (ABS).

We briefly summarize two of the main components in Attention Bias Scaffolding, leaving detailed treatment to appendix C.

Figure 3: Attention bias matrices for unary and binary operations.

Figure 2: Attention heat maps for Successor (Left) and Addition (Right).

**Windowed Attention Biasing**. The idea was developed by Longformer . The intuition is that the most important local dependency is typically restricted by a limited range, which can be captured by a sliding window of width \(w\). Specifically, let \(_{0}=^{T}}{}\) be the original attention weight matrix before softmax, we bias the weights by \(=_{0}+_{w}_{w}\) is constructed via the sliding window mechanism and is detailed in appendix C. Figure 3 visualizes this process for unary and binary operations.

**Cyclic Position Indexing**. Position indexing refers to how we identify each individual position. The simplest way is just to index them \(0,1,\). As our tasks have very restricted dependency contexts which are localized by the windowed attention biases, the model only needs a way to differentiate positions within the window. Long position indexing is not necessary and even harmful sometimes, as our empirical study shows. Therefore we propose Cyclic Position Indexing: Let \(i\) be the position index of a token and \(T\) a period parameter, the token position is converted to \(i T\) before entering into the model.

### Results of ABS

We conduct extensive experiments on each of the arithmetic tasks with various configurations, with results shown in table 1. More detailed discussions are presented in appendix C.5. We summarize the main findings here:

* None of the previous works achieves extrapolation on any of the tasks.

    &  \\
**Task** & **Model** & **6** & **10** & **15** & **20** & **60** \\   & Vanilla & **100.0** & 0.0 & 0.0 & 0.0 & 0.0 \\  & + \(w=1\) & **100.0** & **100.0** & **100.0** & **100.0** & **100.0** \\  & + \(T=3\) & **100.0** & **100.0** & **100.0** & **100.0** & **100.0** \\  & NoPE + \(w=1\) & **100.0** & **100.0** & **100.0** & **100.0** & **100.0** \\  & ALBi & \(1.3\) & \(0.0\) & \(0.0\) & \(0.0\) & \(0.0\) \\  & RoPE & **100.0** & 0.0 & 0.0 & 0.0 & 0.0 \\   & Vanilla & **100.0** & 0.0 & 0.0 & 0.0 & 0.0 \\  & + \(w=1\) & **100.0** & 0.0 & 0.0 & 0.0 & 0.0 \\  & + \(T=3\) & **100.0** & **100.0** & **100.0** & **100.0** & **100.0** \\  & NoPE + \(w=1\) & 99.95 & 99.81 & 99.84 & 99.76 & 99.35 \\  & ALBi & \(0.0\) & \(0.0\) & \(0.0\) & \(0.0\) & \(0.0\) \\  & RoPE & **100.0** & 0 & 0 & 0 & 0 \\  & RPE\({}^{*}\) & **100.0** & 99.9 & 97.2 & \(21.3\) & N/A \\   & Transformer\({}^{}\) & /52.60^{}\)} \\  & + \(scratchpad\) & 29.23 & 0.29 & 0.0 & 0.0 & 0.0 \\   & + \(w=1\) & **100.0** & **100.0** & **100.0** & **100.0** & **100.0** \\   & + \(T=3\) & **100.0** & **100.0** & **100.0** & **100.0** & **100.0** \\   & NoPE + \(w=1\) & **100.0** & **100.0** & **100.0** & **100.0** & **100.0** \\   & Vanilla & **100.0** & 0.0 & 0.0 & 0.0 & 0.0 \\  & + \(w=1\) & **100.0** & 6.0 & 0.19 & 0.0 & 0.0 \\   & + \(T=3\) & **100.0** & **100.0** & **100.0** & **100.0** & **100.0** \\   & NoPE + \(w=1\) & 99.89 & 99.63 & 99.49 & 99.39 & 98.31 \\   & RoPE & **100.0** & 0 & 0 & 0 & 0 \\   

* Data taken from Jelassi et al.  which is an encoder-only architecture with shared layers.

\(\) Data taken from Deletang et al.  which evaluates five encodings (none, sin/cos, RoPE, ALBi, and the relative positional encoding from Transformer-XL) and reports the best-performing variant.

\(\) Data taken from Ruoss et al.  which uses randomized positional encodings to boost length generalization.

Table 1: Extrapolation results measured as percent accuracy (%). Numbers in bold show the best accuracies achieved for the corresponding input length limit.

* Attention bias scaffolding achieve complete length generalization on all tasks, maintaining 100% accuracy up to 60 digits.
* Unary tasks (Successor and Parity) appear to be not relying on any positional embedding at all once the windowed bias is in place. The cyclic positional indexing is not necessary either.
* For binary tasks (Addition and \(N 1\)), on the other hand, a windowed attention bias alone does not guarantee success. It must be combined with cyclic position indexing to achieve complete generalization. Interestingly, the model obtains slightly imperfect generalization (99+% accuracies up to 60 digits) with windowed biases and no positional embedding at all. Sinusoidal positional encoding does not work with a windowed attention bias, achieving only interpolation but not extrapolation. Cyclic position indexing is necessary to enforce stronger localization for complete generalization.

The findings suggest that the right attention is the key to achieving good generalization (thus the title of this section). The different reliance on positional encoding between unary and binary tasks is interesting and we believe it is caused by different attention pattens (i.e., inductive bias) the two types of tasks require. See figures 7 and 8.

## 4 Attention Bias Calibration (ABC)

We now introduce Attention Bias Calibration (ABC), an automatic process that extends the working attention patterns of a model that achieves successful interpolation to arbitrary lengths while preserving its near-perfect performance. The idea is that a model trained to full interpolation must be able to produce the right attention pattern on interpolation data (see section C.3), which captures the local dependencies for recurrent arithmetic algorithms. ABC extracts and aggregates the attention weights and uses them as attention bias, like Press et al. , to fine-tune the model for long inputs. Similar to the scaffolding in section 3.1, ABC is also a kind of inductive bias, but it is fully automatic.

Detailed development, the actual algorithm, and evaluation results are presented in appendix D. We summarize the main findings here.

* ABC could solve all the tasks we study except Parity. The failure with Parity is due to the fact that the vanilla model does not interpolate, even with a scratchpad. This is one of the limitations of ABC.
* The attention patterns show that diagonal and vertical extensions are most useful. The former echos the finds of numerous previous works on positional encoding that favor recency, while the latter appears to be connected to the "attention sink" phenomenon .
* ABC has connections to other effective schemes of position and attention manipulation. We elaborate on two specific examples, RPE and LoRA, in section E.

## 5 Conclusion

This work aims to show the importance of inductive biases by approaching arithmetic tasks through the perspective of inductive learning. Our solution solves a few long-standing difficult or even "impossible" tasks (e.g., Parity). In its current form, we do not expect ABS or ABC to work directly on more complex tasks. Our works show that LLM's embarrassing failures in solving simple tasks such as multi-digit addition may not be solvable by current methods. As these tasks are, by nature, inductive learning, and current length generalization methods may not provide proper inductive biases. How to achieve this for LLMs to solve arithmetic tasks while maintaining their general performances in multi-task settings is an important line of future work.