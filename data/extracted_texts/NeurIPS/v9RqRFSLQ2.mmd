# Learning from Uncertain Data: From Possible Worlds to Possible Models

Jiongli Zhu\({}^{1}\) Su Feng\({}^{2}\) Boris Glavic\({}^{3}\) Babak Salimi\({}^{1}\)

\({}^{1}\)University of California, San Diego \({}^{2}\)Nanjing Tech University \({}^{3}\)University of Illinois, Chicago

###### Abstract

We introduce an efficient method for learning linear models from uncertain data, where uncertainty is represented as a set of possible variations in the data, leading to predictive multiplicity. Our approach leverages abstract interpretation and zonotopes, a type of convex polytope, to compactly represent these dataset variations, enabling the symbolic execution of gradient descent on all possible worlds simultaneously. We develop techniques to ensure that this process converges to a fixed point and derive closed-form solutions for this fixed point. Our method provides sound over-approximations of all possible optimal models and viable prediction ranges. We demonstrate the effectiveness of our approach through theoretical and empirical analysis, highlighting its potential to reason about model and prediction uncertainty due to data quality issues in training data.

## 1 Introduction

This paper addresses the challenges of learning from uncertain datasets by employing the framework of _possible world semantics_, a well-established concept in AI and database theory . In this approach, uncertainty in a dataset \(\) is conceptualized through a collection of possible datasets \(\{_{1},_{2},\}\), each representing a potential state of the real world, reflecting variations due to missing entries, errors, inconsistencies, and biases. Given this framework and a learning algorithm, our objective is to construct a set of models \(\{f_{1},f_{2},\}\), where each model \(f_{i}\) is trained on a corresponding potential dataset \(_{i}\). This method, that we implement in a system called Zorro (_ZO_notope-based _R_obustness Analysis for _R_egression with _O_ver-approximations), allows for a thorough evaluation of how data uncertainties affect the robustness, reliability, and fairness of models in predictive modeling and statistical inference, particularly in scenarios where the ground truth is unidentifiable, necessitating consideration of all possible dataset variations.

While the framework of possible world semantics is essential for modeling dataset uncertainties, it poses significant challenges due to the potentially infinite number of scenarios each dataset might represent. Exploring every possibility and training a model for each is impractical. The concept of _model multiplicity_, which highlights situations where models with similar accuracy differ in individual predictions, has gained traction , yet it primarily focuses on competing models without addressing the full range of dataset variations. Similarly, _dataset multiplicity_ introduced in  recognizes data variations due to uncertainty but  only proposed a solution for linear models with label errors. Our approach expands these ideas by using possible world semantics to systematically manage uncertainty across all features and labels, thus creating a comprehensive framework for evaluating model robustness amid data uncertainties.

To address the challenges of learning from uncertain datasets, we employ the method of _abstract interpretation_. Utilizing zonotopes--a type of convex polytope well-suited for compactly representing high-dimensional data spaces --we over-approximate the set of all possible dataset variations. This framework allows for the simultaneous symbolic execution of gradient descentacross all possible datasets, compactly over-approximating all possible optimal model parameters as a zonotope. We demonstrate that for linear regression with \(_{2}\) regularization (Ridge), our method admits a non-trivial closed-form solution. The zonotope representation of model parameters enables efficient inference, facilitating reasoning about the range of possible predictions or of specific parameters.

**Contributions.** The key contributions of this research are:

1. We introduce an abstract gradient descent algorithm for learning linear regression models from uncertain data. This method over-approximates data variations using zonotopes and symbolically executes gradient descent on all possible datasets concurrently. We define and prove the existence of a fixed point that soundly over-approximates all potential models.
2. Symbolic execution generates intractable polynomial zonotopes for gradients due to non-linear terms and monomial growth that is exponential in the number of iterations. We use linearization and order reduction to compactly over-approximate these polynomial zonotopes using linear zonotopes at each step, introducing an efficient version of abstract gradient descent.
3. The efficient version, however, does not guarantee a fixed point for arbitrary order reduction techniques. To address this, we develop advanced order reduction techniques that ensure fixed points and provide a non-trivial closed-form solution for these fixed points in ridge regression.
4. We implement our approach in a system called Zorro and use it to evaluate the impact of data uncertainty on linear regression models. Our empirical results and analytical solutions validate the effectiveness of our approach, demonstrating its efficacy in computing prediction ranges and verifying robustness.

Related Work.Predictive multiplicity has shown that a single dataset can produce multiple optimally fitted models due to variations in training processes  or modifications to training parameters such as random seeds, data ordering, and hyperparameters. For predictive multiplicity due to missing data, Khosravi et al. address the issue using a probabilistic method that computes expected predictions for all possible imputations. Our approach can be seen as an extreme case of multiple imputation , where we consider all possible data variations rather than just a few plausible scenarios. Meyer et al.  recently introduced the concept of dataset multiplicity, using possible world semantics to model how uncertain, biased, or noisy training data can lead to predictive multiplicity. However, their focus is on uncertainty in training labels, and they use interval arithmetic for over-approximation of prediction intervals for linear regression. In contrast, our approach handles arbitrary uncertainty in features and labels during both training and testing using zonotope-based learning for over-approximation of prediction ranges and model parameters. We show that interval arithmetic fails to provide tight prediction ranges even for uncertainty in labels.

Our work is broadly related to robust model learning, which ensures robustness against data quality issues such as attacks . Distributional robustness  studies model reliability against varying data distributions, while robust statistics  examines model performance under outliers or data errors. Our approach provides exact provable robustness guarantees by exploring the entire range of models under extreme dataset variations, which is crucial for individual-level predictions and reasoning about the robustness of specific parameters.

Our work is also related to robustness certification, which certifies ML models' robustness against data perturbations and uncertainties . These efforts mainly focus on test-time robustness, validating predictions for inputs in the vicinity of a test sample. In contrast, we address training-time robustness, considering the effects of possible datasets on training models. Closest to our approach is the work by Meyer et al.  for decision trees and Karlas et al.  for nearest neighbor classifiers. We use zonotopes to over-approximate prediction ranges for linear regression, generating robustness certificates. While zonotopes have been used for test-time robustness , our work is the first to apply zonotopes for training-time robustness for an iterative learning algorithm.

Approaches for uncertainty quantification (UQ) aim to understand the range of outcomes a model may produce using Bayesian methods, ensembling, conformal prediction, and bootstrapping . UQ focuses on epistemic and aleatoric uncertainty, stemming from insufficient data, noisy data, or uncertainty about the model parameters, and does not account for uncertainty due to systematic data quality issues, such as non-random data errors or missing values , which induce a multiplicity of possible datasets. In this case, UQ methods might underestimate the uncertainty as they rely on critical assumptions. Bayesian methods, for instance, require correctly specified priors to accurately model uncertainty, often failing under conditions with unknown or erroneous priors [73; 67; 72], while conformal prediction (CP) assumes data exchangeability--a condition that breaks down when data errors are systematic [21; 75]. In contrast, our approach addresses this distinct challenge by computing sound over-approximations that guarantee complete coverage of potential predictions across all variations of the dataset. This sound coverage is essential in high-stakes settings such as evaluating the robustness of predictive models for medical use.

## 2 Notation, Problem Formulation and Background

Denote a training dataset \(=(,)\) with \(=[_{1}_{n}]^{T}^{n  d}\) as the matrix of features, and \(=[y_{1} y_{n}]^{T}^{n}\) as the corresponding ground truth labels. Let \(f(;)\) be a model parameterized by \(^{p}\) that maps an input data point to a label. A learning algorithm \(\) maps a training dataset \(\) to the parameters of the trained model, \(^{*}=()\). Given a test dataset \(_{}^{n d}\), for any test sample \(\) from \(_{}\), the function \(f\) computes a prediction \(=f(;^{*})\).

### Learning Possible Models from Possible Worlds

We use possible world semantics to represent the uncertainty in a dataset \(\).

**Definition 2.1** (Possible Datasets).: _Given an uncertain dataset \(\), the uncertainty in \(\) can be represented by a set of possible datasets \(^{}\): \(^{}=\{_{1},_{2},\}\)._

Each dataset \(_{i}^{}\) is a "possible world", i.e., a hypothetical variation of the dataset \(\) that could potentially exist in the real world based on our knowledge about the uncertainty in \(\).

**Example 2.2**.: _Consider an e-commerce dataset \(\) where some product price is missing, meaning the exact price is unknown. Using possible world semantics, we represent this uncertainty with a set of possible datasets \(^{}\), each containing a possible clean price, which could be obtained from prices of the same items on the market._

In App. D, we discuss construction methods for common data quality issues. Our goal is to efficiently construct the set of all possible models from uncertain data and understand their behavior in making predictions.

**Definition 2.3** (Possible Models and Prediction Range).: _Given a set of possible datasets \(^{}\) associated with an uncertain dataset \(\), the possible models, denoted \(f^{}\), are obtained by applying the learning algorithm \(\) to each training dataset \(_{i}\) within \(^{}\) to obtain the set of all possible optimal model parameters \(^{*}\), i.e.,_

\[^{*}=\{^{*}_{i}^{*}_{i}=(_{i}), _{i}^{}\}\]

_For a test data point \(\), the viable prediction range \(V()\) is defined as the interval between the least upper bound and the greatest lower bound of the outputs produced by all models in \(f^{}\), i.e.,_

\[V()=[_{^{*}^{*}}f(,^{*}),_ {^{*}^{*}}f(,^{*})]\]

This prediction range quantifies the minimum and maximum predictions that can be expected for \(\), highlighting the variability in model outputs due to differences in the training data. Our framework supports uncertainty in training and test data. We discuss test data uncertainty in App. F.3.

### Sound Approximation of Possible Models with Abstract Interpretation

The set of all possible datasets associated with uncertain data can be intractable. We use _abstract_ interpretation  to over-approximate sets of elements of a concrete domain \(\) (the training data and model weights) with elements from an abstract domain \(^{}\). Specifically, we use the abstract domain of zonotopes, a type of convex polytope, to over-approximate the possible datasets \(^{}\) using a zonotope \(^{}\) that has a compact symbolic representation. Instead of applying the learning algorithm \(\) to each possible dataset to compute all possible optimal model parameters \(^{*}\), we develop an abstract learning algorithm \(^{}\) that operates directly on the abstract domain of zonotopes. Given \(^{}\), \(^{}\) generates a zonotope \(^{}=^{}(^{})\) that over-approximates \(^{*}\) (demonstrated in the graph on the right). Intuitively, this represents the symbolic execution of the learning algorithm across all possible datasets simultaneously.

**Definition 2.4** (Abstract Domain).: _Let \(\) be a concrete domain. An abstract domain for \(\) is a set \(^{}\) paired with two functions:_

\[\ :()^{} \ :^{}()\]

_which satisfy the following condition for any subset \(S\): \(((S)) S\). Two abstract elements \(d_{1}\) and \(d_{2}\) are equivalent, written as \(d_{1}_{}d_{2}\), if \((d_{1})=(d_{2})\)._

Def. 2.4 ensures that the abstract element \((S)\) associated with a set \(S\) through application of the abstraction function \(\) encodes an over-approximation of \(S\). We will use an abstract element \(^{}=(^{})\) to over-approximate the possible worlds of an uncertain training dataset \(^{}\). We discuss abstraction functions \(\) for specific types of training data uncertainty in App. D.

**Definition 2.5** (Abstract Transformer).: _Consider a function \(F:_{1}_{2}\) on concrete domains \(_{1}\) and \(_{2}\). An abstract transformer \(F^{}:_{1}{}^{}_{2}{}^{}\) over-approximates \(F\) in the abstract domain:_

\[ S():(F^{}((S)))  F(S)\]

_An abstract transformer is exact (does not loose precision) if \( d^{}:(F^{}(d))=F( (d))\)._

Importantly, (exact) abstract transformers compose (see App. B.2, Prop. B.1) and, thus, we can construct an abstract transformer for complex functions from simpler parts.

To over-approximate the set of possible model parameters \(^{*}\), we will develop an abstract transformer \(^{}\) for the learning algorithm \(\) to get \((^{}(^{}))^{ *}\).

Symbolic Abstract Domains and Zonotopes.We consider a symbolic abstract domain \(\) of vectors and matrices (marked with \(^{}\)) with elements that are polynomials \(\) over variables \(=\{_{i}\}\). The concretization of a polynomial \(\) is the result of evaluating \(\) on all assignments \(e:[-1,1]\), encoded as vectors \([-1,1]^{|\,\,|}\): \(()=\{(e) e[-1,1]^{|\,\,|}\}\). We lift concretization to vectors and matrices through point-wise application. Such an object \(^{}\) is typically referred to as a _polynomial zonotope_ or _zonotope_ if all symbolic expressions are linear (see App. C.2). The concretization of \(^{}\) is: \((^{})=\{^{}(e) e[-1,1]^{| \,\,|}\}\).

## 3 Exact Abstract Transformers for Learning Linear Models

Given an uncertain training dataset \(^{}\), we aim to over-approximate the set of possible optimal linear models \(^{*}=\{_{1}^{*},_{2}^{*},\}\), where \(_{i}^{*}^{p}\) represents the optimal parameters of a linear model trained on \(_{i}^{}\). These optimal parameters are the fixed point of the sequence \(\{_{i}^{k}\}_{k=0}^{}\) generated by: \(_{i}^{k+1}=(_{i}^{k})\) where the operator \(:^{p}^{p}\) captures one step of gradient descent, i.e., \(()=- L()\), for a learning rate \(\) and a loss function \(L()\).

In the abstract domain, we use the zonotope representations \(^{}\) and \(^{}\) to abstract the possible datasets \(^{}\) and the set of possible model weights \(^{*}\). While it is theoretically possible to compute symbolic expressions for the standard closed form solution for linear regression, this can result in large expressions that contain fractions with polynomial numerators and denominators and, thus, computing prediction intervals based on such expressions is computationally infeasible (see App. L). Instead, we over-approximate the optimal parameters using an abstract operator \(_{exact}^{}:^{}^{}\) that generates a sequence of abstract elements \(\{^{}\,k\}_{k=0}^{}\):

\[^{\,k+1}=_{exact}^{}(^{\,k}),\]

where the abstract operator \(_{exact}^{}\) given by \(_{exact}^{}(^{})=^{}- L(^{ })\) captures one gradient descent step in the abstract domain. Specifically, for any loss function \(L\) whose gradient \( L\) consists of linear or polynomial expressions, such as the mean squared error (MSE) loss, \(_{exact}^{}\) is an exact abstract transformer. This follows from the existence of exact abstract transformers for addition and multiplication over polynomial zonotopes  and the fact that abstract transformers compose (see App. E, Prop. B.1 and Prop. E.1).

**Proposition 3.1**.: _The abstract gradient descent operator \(_{exact}^{}\) is an exact abstract transformer for the concrete gradient descent operator \(\). Formally, for any abstract \(^{}\),_

\[(_{exact}^{}(^{}))=((^{})),\]

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

the _robustness ratio_ which is the fraction of the test data receiving robust predictions as a metric in all robustness verification experiments. The robustness threshold is set to 5% of the label range for the _MPG_ data, and 0.8% of the label range for the _Insurance_ data. Additionally, we assess the _worst-case test loss_ using certain test data and uncertain model weights trained from uncertain training data.

### Robustness Verification

Prediction Robustness (Uncertain Labels).Fig. 1(a)(b) compares Zorro with the baseline Meyer on a setting where only training labels suffer from uncertainty. We vary the uncertainty radius and uncertain data percentage. As both systems provide sound over-approximations of prediction ranges, they may underestimate a model's robustness. As shown, Zorro consistently certifies significantly higher robustness ratios than Meyer. This is due to the fact that Meyer uses interval-arithmetic which ignores the correlation between model weights in different dimensions and between the training labels and the weights, leading to overly conservative prediction ranges. Specifically, for higher uncertainty radius values, Meyer fails to certify robustness for most of the data while Zorro still can certify robustness for 100% of the instances.

Prediction Robustness (Uncertain Features).We also evaluate the impact of training feature uncertainty (not supported by Meyer). Specifically, we introduce uncertainty into the vehicle weight column for the MPG dataset. As shown in Fig. 1(c), uncertainty in the features results in relatively less robust predictions compared to uncertain labels for a similar uncertainty radius (Fig. 1(a)). This is primarily because uncertain features result in more high-order terms in the closed form solution than uncertain labels which in turn leads to larger over-approximation errors during linearization.

In all experiments the standard deviation of the robustness ratio, calculated by repeating experiments with different random seeds, is large when the average robustness ratio is close to 0.5. This is because when the uncertain data percentage is low, the model will be robust no matter which training instances are selected to be uncertain. Likewise, when the uncertain data percentage is high, then most predictions will be uncertain no matter which training data points are uncertain.

Parameter Robustness.Next, we apply Zorro for robustness certification of parameters in linear regression models, crucial for statistical estimation and causal analysis. We compare the ground truth coefficients for a treatment variable with the results obtained by Zorro and through KNN imputation on a dataset with injected missing data. Fig. 2 shows the treatment variable coefficient and

Figure 1: Robustness verification on using intervals (Meyer) and zonotopes (Zorro).

the regression model's intercept. The intercept captures the baseline level of the outcome variable when all predictors are zero, highlighting how baseline values can shift under uncertainty. While the model trained after KNN imputation sometimes correctly identifies the directionality of the treatment effect, this is not always the case as shown in Fig. 2(b). This highlights the needs for techniques like Zorro which guarantee that the true treatment effect is within certain bounds.

### Solution Quality with Varying Uncertainty and Hyperparameters

We evaluate Zorro's effectiveness by testing the tightness of the over-approximation and the accuracy of possible models. Specifically, we examine how the over-approximation quality and worst-case loss are influenced by the level of data uncertainty and the regularization coefficient.

Varying Data Uncertainty.To evaluate how specific characteristics of the data affect the effectiveness of Zorro, we injected errors into real datasets, varying uncertain data percentage and uncertainty range. We compare Zorro with the ground truth range of the loss computed by enumerating all possible worlds (GT). The results shown in Fig. 3(a) demonstrate that Zorro tightly over-approximates the ground truth loss range, especially for smaller uncertainty radius values. As uncertainty increases, the over-approximation gap widens due to the increased coefficient of higher-order terms in the gradient, which are linearized, leading to higher linearization errors. As shown in Fig. 3(b), the tightness of Zorro's over-approximation is not affected by the dimension of the data.

Effect of Regularization.We investigate the impact of the regularization coefficient on the robustness of predictions and the worst-case loss of possible models. Following a similar approach to Sec. 5.1, we introduce uncertainty in both features and labels in the MPG dataset and use zonotopes with varying levels of uncertainty to over-approximate the training data uncertainty. The results, shown in Fig. 4, indicate that a higher regularization coefficient leads to more robust predictions, as regularization tends to "compress" all possible model weights towards the origin. Interestingly, the worst-case loss shows that \(=0\) is not optimal across all scenarios, especially when the fraction uncertain instances is high. Instead, a small, positive \(\) (e.g., 0.02 or 0.025) generally yields the best worst-case losses. Combining these results, the optimal regularization coefficient should enhance robustness (i.e., a higher robustness ratio) while maintaining an acceptable worst-case loss. Therefore, the regularization coefficient should be tuned based on a validation dataset to achieve a small range of accurate possible models.

## 6 Conclusions, limitations and broader impacts

We introduce an approach for propagating uncertainty through model training and inference for linear models. Given an abstract uncertain training dataset that over-approximates the possible worlds of a training dataset, we develop abstract interpretation techniques to over-approximate the set of possible models and inference results for this set of models using zonotopes. This is challenging, as we need to compute fixed points of gradient descent in the abstract domain. Our main technical

Figure 3: Range of the loss, through enumeration of all possible worlds (GT) and Zorro.

Figure 2: Applying Zorro to causal inference. The intercept (y-axis) is the modelâ€™s bias term, the treatment effect (x-axis) is the coefficient for the treatment variable.

contribution is the development of closed-form solutions for such fixed points that can be solved efficiently. Our techniques efficiently over-approximate models and inference for several use cases, including robustness verification, uncertainty management in causal reasoning, and improving the interpretability and reliability of predictions and inferences. This framework can be particularly valuable in critical applications where data quality and robustness are paramount. While we propose an effective method for abstract learning of linear models, extending our approach to more complex models is challenging. Non-linear models, such as neural networks, would require more advanced linearization and order reduction techniques, as well as parallelization, to manage the increased complexity of the involved symbolic operations. In addition, adapting our method to a broader classes of models through efficiently approximating the fixed points remains a challenging and promising future direction.

## 7 Acknowledgment

This research was supported by NSF awards IIS-2340124, IIS-2420691, and IIS-2420577, as well as by NIH grant U54HG012510. The views, opinions, and findings presented are those of the authors and do not necessarily represent those of the NSF or NIH.