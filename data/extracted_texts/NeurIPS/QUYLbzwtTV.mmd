# Bias in Motion: Theoretical Insights

into the Dynamics of Bias in SGD Training

 Anchit Jain

University of Cambridge

aj6250@cantab.ac.uk

&Rozhin Nobahari

MILA - Quebec AI institute

Universite de Montreal

&Aristide Baratin

Samsung - SAIT AI Lab Montreal

&Stefano Sarao Mannelli

Gatsby & SWC - University College London

s.saraomannelli@ucl.ac.uk

Work done as an intern at the Gatsby Computational Neuroscience Unit, University College London

###### Abstract

Machine learning systems often acquire biases by leveraging undesired features in the data, impacting accuracy variably across different sub-populations. Current understanding of bias formation mostly focuses on the initial and final stages of learning, leaving a gap in knowledge regarding the transient dynamics. To address this gap, this paper explores the evolution of bias in a teacher-student setup modeling different data sub-populations with a Gaussian-mixture model. We provide an analytical description of the stochastic gradient descent dynamics of a linear classifier in this setting, which we prove to be exact in high dimension. Notably, our analysis reveals how different properties of sub-populations influence bias at different timescales, showing a shifting preference of the classifier during training. Applying our findings to fairness and robustness, we delineate how and when heterogeneous data and spurious features can generate and amplify bias. We empirically validate our results in more complex scenarios by training deeper networks on synthetic and real datasets, including CIFAR10, MNIST, and CelebA.

## 1 Introduction

Over the past decade, the problem of assessing the fairness of classifiers has garnered significant attention, revealing that machine learning (ML) systems not only reproduce existing biases in the data but also tend to amplify them . Given the complexity of the ML pipeline, isolating and characterising the key drivers of this amplification is challenging. Recent studies have begun to disentangle the contributions from architectural design choices, including overparameterisation , model complexity, activation functions , learning protocols , post-processing practices such as pruning , and intrinsic aspects of the data like its geometrical properties .

Theoretical results in this area (e.g., ) are mostly based on asymptotic analysis, leaving the transient learning regime poorly understood. Due to limitations on computational resources, a trained ML system may operate far from the asymptotic regime and hence existing results may not always apply. Insights from class imbalance literature  indicate that classifiers converge faster for classes with more data, but how this applies to fairness, where datasets might be balanced by label but imbalanced by demographics, remains unclear.

Our analysis addresses this gap by providing a precise characterisation of the transient dynamics of online stochastic gradient descent (SGD) in a high dimensional prototypical model of linearclassification. We use the teacher-mixture (TM) framework , where different data sub-populations are modeled with a mixture of Gaussians, each having its own linear rule (teacher) for determining the labels. Adjusting the parameters of the data distribution in our framework connects models of fairness and spurious correlations, providing a unifying framework and a general set of results applicable to both domains. Remarkably, our study reveals a rich behaviour divided into three learning phases, where different features of data bias the classifier and causing significant deviations from asymptotic predictions. We reproduce our theoretical findings through numerical experiments in more complex settings, demonstrating validity beyond the simplicity of our model.

Our key contributions are:

* **High-dimensional analysis**: We demonstrate that in the high-dimensional limit, relevant properties of the classifier, such as the generalisation error, can be expressed using a few sufficient statistics. We prove that their evolution converges to a set of ordinary differential equations (ODEs) that can be solved explicitly in our setting.
* **Bias evolution characterisation**: Using our solution, we characterise the evolution of bias throughout training, showing a three-phase learning process where bias exhibits non-monotonic behaviour. Specifically: 1. **Initial phase**: The classifier is initially influenced by sub-populations with strong class imbalance. 2. **Intermediate phase**: The dynamics shifts towards the saliency, or norm, of the samples in a sub-population. 3. **Final phase**: Sub-population imbalance, or relative representation, becomes the dominant factor.
* **Empirical validation**: We validate and extend our theoretical results through numerical experiments in both synthetic and real datasets, including CIFAR10, MNIST, and CelebA.

Altogether, our study reveals a complex time-dependence of learning with structured data that previous theoretical studies have failed to capture. This characterisation is crucial for developing effective bias mitigation strategies, especially under limited computational resources.

### Further related works

**Class imbalance and fairness.** A key element in our study is the presence of heterogeneous data distributions within the dataset. In the context of fairness, these distributions model different groups in a population. Sampling unbalance is particularly critical, as minority groups are often misclassified [9; 20]. However, theoretical studies on group imbalance have been limited to asymptotic analyses , which may not apply in practical settings. Related questions have been explored in the label imbalance literature , where it has long been known [1; 17] that underrepresented classes have slower convergence rate and may even experience increased errors early in training. Our work shows that pre-asymptotic analysis can reveal complex transient dynamics, which is practically relevant when learning slows down or training to convergence is not possible. Similar to our analysis,  has shown that supposedly neutral choices, like activation functions or pooling operations, can generate strong biases. In contrast to prior work, our focus on data properties identifies several timescales associated to different data features relevant to bias generation.

**Simplicity bias.** Several studies [31; 16; 41; 10; 32] have highlighted a bias of deep neural networks (DNNs) towards _simple_ solutions, suggesting this bias is a key to their generalisation performance. Simplicity bias also influences learning dynamics: [4; 32; 28; 30; 33] have showed that DNNs learn progressively more complex functions during training, with a notion of complexity often defined implicitly by other DNNs or observations like the time to memorisation. Our results connect with simplicity bias by identifying interpretable properties of the data that make samples appear "simple" to a shallow network. Interestingly, our findings reveal that different phases of learning experience simplicity in different ways, leading to forgetting of previously learned features.

**Spurious correlations.** Simplicity bias can also lead to shortcomings  by excessively relying of spurious features in the data, possibly hurting generalisation, especially in out-of-distribution contexts . Theoretical works [29; 37; 18] have identified statistical properties that cause a classifier to favour spurious features over potentially more complex but more predictive features.

Various methods have been proposed to address this problem using explicit partitioning of the data [2; 36]; some approaches implicitly infer subgroups with various degrees of correlation as spurious features. Notably, [26; 42] rely on early stages of learning to detect bias and adjust sample importance accordingly. Our study provides a unifying view of learning in fairness and spurious correlation problems, highlighting the presence of ephemeral biases characterised by multiple timescales during training. This adds complexity to the understanding of learning dynamics and points out potential confounding effects in existing mitigation methods.

## 2 Problem setup

**Data distribution.** We consider a standard supervised learning setup where the training data consists of pairs of a feature vector \(^{d}\) and a binary label \(y= 1\). To model subgroups within the data , we assume that the feature vectors are structured as clusters \(c_{1},,c_{m}\), respectively centered on some fixed attribute vectors \(_{1},,_{m}^{d}\). Specifically, \(\) is sampled from a mixture of \(m\) isotropic Gaussians:

\[_{j=1}^{m}_{j}\,(_{j}/,_{j} _{d d}),\] (1)

with mixing probabilities \(_{1},,_{m}\) and scalar variances \(_{1},,_{m}\). Assuming the entries of \(_{j}\) are of order \(1\) as \(d\) gets large, the scaling factor \(1/\) ensures that the Euclidean norm of the renormalised vector is of order \(1\). This prevents the problem from becoming either trivial or overly challenging in the high-dimensional limit [25; 24]. We adopt a teacher-mixture (TM) scenario  where each cluster has its own teacher rule:

\[ c_{j} y=(}_{ j}^{}/).\] (2)

This rule is characterised by the teacher vectors \(}_{j}^{d}\), ensuring linear separability within each cluster. Fig. 1b-d illustrate the data distribution for two clusters with opposite mean vectors \(\), which will be the primary case study for our analysis.

**Model.** In this study we analyse a linear model applied to the above data distribution. We aim to learn a vector parameter \(\), referred to as the'student', such that predictions are given by

\[()=^{}/.\] (3)

The training process involves applying online SGD on the squared loss \(=(y-)^{2}\). At the \(k\)-th iteration, a feature vector \(^{k}\) is sampled from (1), the ground truth label \(y^{k}\) and current model

Figure 1: **Teacher-Mixture in fairness and robustness.**_Panel (a)_ shows the generalisation errorsâ€”for the subpopulations \(+\) (blue) and \(-\) (red)â€”obtained through simulation (crosses) and predicted by the theory (solid lines) for a network with linear activation. The inset shows the same comparison for the _order parameters_: \(R_{+}\) (blue), \(R_{-}\) (red), \(M\) (green), and \(Q\) (orange). _Panels (b-d)_ exemplify the different scenarios achievable in the TM model investigated in Sec. 4. _Panel (b)_ represent a model for robustness where a spurious featureâ€”given by the shift vectorâ€”can mislead the classifier, see Sec. 4.1. _Panels (c,d)_ are instead discussed in Sec. 4.2 and represent two models of fairness. First, _Panel (b)_ has no shift, \(v=0\), allowing us to remove the confounding effects. Finally, _Panel (d)_ shows the general fairness problem.

prediction \(^{k}\) are respectively given by (2) and (3), and the parameter is updated as:

\[^{k}:=^{k+1}-^{k}=-^{k }(^{k})=}(y^{k}-^{k})\,^{k}\] (4)

where \(/2>0\) denotes the learning rate. It is important to note that in this online setting the number of time steps is equivalent to the number of training examples. In our analysis, the model is evaluated by its generalisation error, or population loss, \(:=[]\).

## 3 SGD analysis

We study the evolution of the generalisation error during training with SGD with constant learning rate in the high dimensional setting (i.e. large \(d\)). Following a classical approach [34; 8], we streamline the problem by focusing on a small set of summary statistics, referred to as 'order parameters', which fully characterises the dynamics. As the dimension increases, it can be shown by concentration arguments that the evolution of these order parameters converges to the deterministic solution of a system of ODEs [15; 6; 3]. Notably, in our setting, we achieve an analytical solution of this ODE system. We sketch our main results below, referring to the Appendix for derivations and proofs.

### Order parameters

In the setup described in Section 2, consider the following \(2m+1\) variables:

\[R_{j}=^{}}_{j}, M_{j}= ^{}_{j}, Q=\|\|^{2},\] (5)

for \(1 j m\). These variables correspond to key statistics of the student, namely its alignment to the cluster teachers, its alignment to the cluster centers, and its magnitude, respectively.

**Lemma 3.1**.: _The generalisation error can be written as an average \(=_{j=1}^{m}_{j}_{j}\) over the clusters, where \(_{j}\) is a degree 2 polynomial in \(R_{j},M_{j}\) and \(Q\) taking the form_

\[_{j}=1-2_{j}M_{j}+M_{j}^{2}-_{j}R_{j}+Q_{j}\] (6)

_where \(_{j},_{j}\) are constants independent of the parameter \(\)._

We present the derivation of this result and the explicit form of the constants \(_{j},_{j}\) in Appendix B.1.

Our problem thus reduces to characterising the evolution of order parameters (5). Using the gradient update of the parameter in Eq. 4 and the notation \(^{k}:=y^{k}-^{k}\), we can write update equations for the order parameters as follows:

\[ M_{j}^{k}=^{k}_{j}^{}^{k}}{ }, R_{j}^{k}=^{k}}_{j}^{}^{k}}{}, Q^{k}=^{k} _{j}^{}^{k}}{}+}{d^{2}}(^{k })^{2}\|^{k}\|^{2}.\] (7)

### High dimensional dynamics

We build upon classic results [34; 8], recently put on rigorous grounds [15; 6; 3], leveraging the _self-averaging_ property of the order parameters in the high dimensional limit \(d\). As a result, as the dimension gets large, the discrete, stochastic evolution (7) of the order parameters can be effectively described in terms of the deterministic solution of the average continuous-time dynamics.

Let \(:=(S_{i})_{1 i 2m+1}\) denote the collection of order parameters. The following lemma shows that the average of the updates (7) over the sample \(^{k}\) can be expressed solely in terms of \(^{k}\).

**Lemma 3.2**.: \([ S_{i}^{k}]=f_{i}(^{k})\) _for some functions \((f_{i}())_{1 i 2m+1}\) in O(1) as \(d\)._

The theorem below states that as \(d\) gets large, the stochastic evolution \(^{k}\) of the order parameter gets uniformly close, with high probability, to the average continuous-time dynamics described by the ODE system:

\[}_{i}(t)}{dt}=f_{i}(}(t)), 1 i  2m+1,\] (8)

where the continuous _time_ is given by the example number divided by the input dimension, \(t=k/d\). Formally,

**Theorem 3.3**.: _Fix a time horizon \(T>0\). For \(1 i 2m+1\),_

\[_{0 k dT}|_{i}^{k}-_{i}}(k/d)| 0d.\] (9)

where \(\) denotes convergence in probability. A proof is provided in Appendix B. We provide the explicit expression of the functions \(f_{i}\) in the ODEs (8) in Appendix C, focusing on \(m=2\) clusters for clarity.

### Solving the ODEs

Here we present the explicit solution of the ODEs (8) in the case of two clusters (\(m=2\)) with opposite mean vectors \(\), as in . Henceforth, we refer to \(\) as the shift vector and to the two clusters as the 'positive' and 'negative' sub-populations, with mixing probabilities \(\) and \((1-)\), variances \(_{}\) and teacher vectors \(}_{}\), respectively. The order parameters introduced in Eq. 5 are specifically denoted as \(M=^{}/d,R_{+}=^{}}_{+}/d\), and \(R_{-}=^{}}_{-}/d\) in this setting.

**Theorem 3.4**.: _In the above setting, solutions to the order parameter evolution take the form_

\[M(t) =M_{0}e^{-(v+^{mix})t}+M^{}(1-e^{-(v+ ^{mix})t}),\] (10) \[R_{}(t) =R_{}^{0}e^{-^{mix}t}+R_{}^{}(1-e^{- ^{mix}t})+k_{1}^{}(e^{-^{mix}t}-e^{-(v+^{mix})t }),\] (11) \[Q(t) =Q_{0}e^{-(2^{mix}-^{2mix})t}+Q^{}(1-e^ {-(2^{mix}-^{2mix})t})\] \[+k_{2}(e^{-t(2^{mix}-^{2mix})}-e^{-t^{ mix}})+k_{3}(e^{-t(2^{mix}-^{2mix})}-e^{-t(v+^{ mix})})\] \[+k_{4}(e^{-t(2^{mix}-^{2mix})}-e^{-t(2v+2 ^{mix})}),\] (12)

_with \(^{mix}=_{+}+(1-)_{-}\), \(^{2mix}=_{+}^{2}+(1-)_{-}^{2}\) and \(v=||||^{2}/d\)._

The remaining constants are less significant and are reported in Appendix D.1 and discussed further in Appendix E. This solution allows us to describe important observables such as the generalisation error (via Lemma 3.1) at any timestep. Fig. 0(a) plots the theoretical closed-form solutions along with values obtained through simulation when we set \(d=1000\). Note the remarkable agreement between the analytical ODE solution and simulations of the online SGD dynamics in this high dimensional data limit.

## 4 Insights

In this section, we delve deeper into the solution derived in Theorem 3.4. By examining the exponents in Eqs. 10-12, we can identify the relevant training timescales. Notably, \(M\) follows a straightforward behaviour dominated by a single timescale, whereas \(R_{}\) and \(Q\) exhibit multiple timescales, leading to significant implications for the emergence and evolution of bias during training.

In the following sections, we analyse increasingly complex scenarios to understand how bias develops and evolves. Parameters specifying these different scenarios are the shift norm \(v=||||^{2}/d\) and relative representation \(\), the subpopulation variances \(_{}\), and the teacher overlap \(T_{}=}_{+}^{}}_{-}/d\). For simplicity we fix the teacher norm \(\|_{}\|_{2}=\), so that \(T_{}\) is the cosine similarity between the two teachers.

### Spurious correlations

The emergence of spurious correlations during training exemplifies a type of bias where a classifier favours a spurious feature over a core one. To isolate the impact of spurious correlation in our model while

Figure 2: **Spurious correlations transient alignment. Time-evolution of loss (purple), student-teacher (red) and student-shift (green) cosine similarities. The initial phase (green background) of learning aligns classifier and shift vector before aligning with the teacher (red background), Sec. 4.1. Parameters: \(v=16,=0.5,_{-}=_{+}=0.1,T_{}=1,=0.5\). For these parameters, spurious features allow the correct classification of 90% of the samples.**avoiding confounding effects, we consider perfectly overlapping teachers (\(}_{+}=}_{-}\)) and sub-populations with equal variance and representation (\(=0.5,_{+}=_{-}\)). With non-perfectly overlapping clusters \(v 0\), we introduce a spurious correlation by adding a small cosine similarity between the shift vector and the teacher, creating a label imbalance--an imbalance between the proportion of positive and negative labels--within each sub-population. The setting is illustrated in Fig. 0(b).

From Eqs. 10-12, two relevant timescales for the problem are observed:

\[_{M} =)}, _{R} =1/^{mix}.\] (14)

The shortest timescale, \(_{M}\), associated with \(M\), indicates that the student first aligns with the spurious feature. By aligning with the shift vector, the student can predict most examples correctly, but not all. The effect of spurious correlations is transient; at \(t_{R}\), the student starts disaligning from the spurious feature and aligns with the teacher vector, eventually achieving nearly perfect alignment. This is illustrated in Fig. 2, where the student initially picks up on the spurious correlation (green) and achieves almost perfect alignment with the shift vector during intermediate times before aligning nearly perfectly with the teacher (red).

### Fairness

In this section, we identify the properties of sub-populations that determine the bias during learning and show how bias evolves in three phases. To quantify bias, we use the _overall accuracy equality_ metric , which measures the discrepancy in accuracy across groups. Intuitively, we aim for equal loss on both groups, considering any deviation from this condition as bias.

#### 4.2.1 Zero shift

We first consider a simplified case where we assume that both clusters are centered at the origin \(v=0\) as shown in Fig. 0(c). We will later reintroduce the shift and analyse the transient dynamics it introduces as per the discussion in section 4.1. The zero shift case represents an extreme situation where not only is the classification not solvable if \(}_{+}}_{-}\), but it is also difficult to identify which cluster generated a given data point since the shift provides no information. This setting is particularly suited to analysing the effects of 'group level' features, such as group variance and relative representation, on the preference of the classifier.

Figure 3: **The crossing phenomenon.**_Panel (a) (left side)_ shows the loss curves of sub-population \(-\) (in red) and sub-population \(+\) in blue along with the overall loss (in purple). We observe a crossing cause by a higher variance but lower representation in sub-population \(-\). The background colours represent the different phases of bias that are characterised by the evolution of the order parameters shown in _Panel (a) (right side). Panel (b)_ shows the presence of the crossing phenomenon in a large portion of the parameter space using a phase diagram. Blue indicates an asymptotic preference for sub-population \(+\) and red the opposite. Dark colours indicates regions where bias is consistent across training, while regions in light colours undergo a crossing phenomenon. White indicates that learning rate was too high and training diverged. Parameters: \(v=0,_{+}=1,T_{}=0.9,=0.1\).

In this simplified setting, \(M(t)\) is always zero and the constants \(k_{1}^{},k_{3},k_{4}\) presented in equations 11 and 12 are zero. Thus, the dynamics only involve two relevant timescales given by \(_{R}\) in Eq. 14 and

\[_{Q}=1/((2^{mix}-^{mix})).\] (15)

Fig. (a)a illustrates the changing preference of the classifier. Specifically, we observe that the variance of the sub-population is particularly relevant initially and the sub-population with higher variance (red) is _learnt_ faster, i.e. its generalisation error drops faster. However, asymptotically we observe that the relative representation becomes more important wherein the student aligns itself with the teacher that has a higher product of representation and standard deviation (blue), i.e.

\[}(1-)} R_{+}^{}  R_{-}^{}.\] (16)

Thus, the network can advantage the cluster with higher variance initially but asymptotically advantage the other cluster if its representation is high enough. This leads to the interesting behaviour shown in Fig. 3 wherein we observe a 'crossing' of the losses on the two sub-populations. A more detailed analysis of the 'crossing' is presented in E.2.

_Initial dynamics._ Starting from small initialisation, the initial rate of change of the generalisation error for sub-population \(+\) is

\[.}{dt}|_{t=0}=-^{2}^{mix}_{+} (}}^{}}{}-1)\] (17)

and analogously for \(-\). The learning rate \(\) must be chosen to be small enough such that the generalisation errors decrease and hence the first term in the brackets must dominate over the 1. Since \(R_{+}^{}/R_{-}^{}[T_{};1/T_{}]\) (for \(T_{}>0\)), the ratio between generalisation error rates is bounded by

\[T_{}}{_{-}}}/dt}{d _{g-}/dt}_{t=0}}}{ _{-}}}.\] (18)

When the teachers are only slightly misaligned--\(T_{} 1\)--the bound is tight and we can see that it is the ratio of the square roots of the variances that determines which cluster is learnt faster initially. As precisely detailed below, the initial bias can substantially differ from the asymptotic bias of the classifier. Indeed, Fig. (b)b shows in a phase diagram the existence of 'bias crossing' across a wide range of variances and representations. The transition between the phases that represent a initial preference for the positive sub-population (light red and dark blue) and the phases that represent an initial preference for negative sub-population (dark red and light blue) is approximately given by the line \(_{-}=_{+}=1\), independent of the representation as predicted by Eq. 18. The portion of the dark blue phase just above the white divergent phase marks a 'quasi-divergent' region wherein the generalisation error on the negative sub-population rises even at \(t=0\) because the learning rate is too large for such high variances. It hence marks a region of impractical behaviour that is only observed with poorly optimised learning rates.

_Asymptotic preference._ In the limit of small learning rates \( 0\), the student will asymptotically exhibit lower loss on whichever sub-population's teacher it has better alignment with. Thus, Eq. 16 provides a simple characterisation of asymptotic preference from representations and standard deviations in the small learning rate limit. However, the situation is more complex in the case of finite learning rate, which may disrupt learning in one or both clusters. Without indulging further into the discussion of asymptotic performance, which is not the main goal of the paper, we refer to Appendix E.3 for more in depth and analysis and additional phase diagrams.

#### 4.2.2 General case

We now consider the general case shown in Fig. (d)d, where the shift is non zero and all three timescales identified so far play a role.

As observed in Sec. 4.1, when the shift norm \(v\) is large, the effect of spurious correlations becomes significant and the timescale associated with the spurious correlations is the fastest. In general, when \(v 0\) we observe an additional phase due to the effect of spurious correlation. In this new first phase, the student advantages the cluster with higher representation and lower variance since the salient information received from this cluster is more coherent and easier to access.

More precisely, in high dimensions the shift and the teachers are likely to exhibit a small cosine similarity leading to a class imbalance in the clusters and creating spurious correlation. The amount of label imbalance within a cluster is characterised by the value of \(\), as detailed in Appendix A. For smaller variances, \(\) takes more extreme values leading to stronger spurious correlation of that cluster with the shift. If a cluster has more positive examples, we would observe a reduction in loss for that cluster if the student aligns with the mean of that cluster (and opposite to the mean if the cluster has mostly negative examples). When both clusters have different majority classes, the direction of spurious correlation for the two are same. However, when the majority classes are the same, we have competing directions for spurious correlation. The expression for \(M_{}\) in Appendix D.1 Eq. D.42 shows that in this case the relative representation comes into play and the mean of the cluster with greater representation and class imbalance will be chosen by the teacher to align with. Fig. 4 shows such a scenario with three phase bias evolution:

* The green phase is driven by spurious correlation where the positive cluster is advantaged since it has greater representation and class imbalance.
* Then, the red phase is driven by greater variance where the negative cluster is learnt faster as discussed through Eq. 18.
* Finally, we observe the orange phase wherein the student starts aligning with the positive cluster as per the asymptotic rule in Eq. 16.

In summary, the student first advantages the sub-population with higher representation and lower variance. Next, it advantages the sub-population with higher variance. Asymptotically, it advantages the sub-population with higher representation and variance. Our analysis thus shows that bias is a dynamical quantity that can vary non-monotonically during training and cannot be characterised by simply the initial and asymptotic values.

## 5 Ablations using numerical simulations

### Rotated MNIST

We train a 2-layer neural network with 200 hidden units, ReLU activation, and sigmoidal readout activation, using online SGD on MSE loss in a MNIST classification task. Data are centered and the variance is set to 1 following standard pre-processing practices. We consider a variation of the MNIST dataset that mimics the TM model, allowing us to verify our theory when network and the data structure are mismatched. Digits 0 to 4 and 5 to 9 are grouped to form the two subpopulations. With probability \(p_{+}\) and \(p_{-}\), digits of both subpopulations are rotated with a subpopulation-specific angle--i.e. Fig. 5a uses angles of rotation \(_{-}=45^{o}\) and \(_{-}=-90^{o}\). The goal of the classifier is to detect rotations.

The experimental framework gives a correspondence between parameters of the generative model and properties of a real dataset. We can control relative representation by subsampling, teacher similarity by playing with angle difference, label imbalance by changing the probability of rotation, and saliency by increasing and decreasing the norm of the subpopulation using multiplicative factors \(_{}\). The only parameter that we cannot control is the shift \(\) which is a property of the data.

Figure 4: **Double crossing phenomenon.**_(Left panel)_ shows the loss for the two sub-populations (blue and red lines) and the global one (in purple). _(Right panel)_ shows the value of the order parameters across time. The behaviour of the order parameters across time provides a precise characterisation and understanding of the different phases. Parameters: \(v=100,=0.75,_{+}=0.1,_{-}=0.5,=0.03,T_{}=0.9,_{ +}=0.343,_{-}=0.12\).

Therefore, in order to reproduce the zero-shift case of Sec. 4.2, we remove the label imbalance by setting the probability of rotation \(p_{+}=p_{-}=0.5\). By properly calibrating the saliency \(\) and the relative representation \(\), it is possible to bias the classifier towards one subpopulation at the beginning of training and the other in the end. This is shown in Fig. 4(a) where \(=0.1\) and \(_{+}>_{-}\). The saliency difference favours subpopulation \(+\) initially while setting \(\) small enough advantages subpopulation \(-\) later in training. This is precisely what we observe in the plot.

In Fig. 4(b), we extend the MNIST experiment by varying the average image brightness of subpopulation \(-\), which reflects the group variance in our theory. The results show that greater brightness leads to faster learning in the second phase and an increasing asymptotic preference, consistent with our predictions.

Finally, we consider the general fairness case. By creating label imbalance, i.e. setting \(p_{+}=0.3\) and \(p_{-}=0.7\), we observe an additional phase of bias evolution, wherein the classifier prefers dense regions with consistent labels. This advantages subpopulation \(-\) and indeed it is what we see in Fig. 4(c). The result of the simulations matches the theory displaying a double crossing phenomenon.

### Cifar10

We consider the same architecture and pre-processing described for MNIST on a CIFAR10 classification task. We select 8 classes and assign 4 of them to the positive group and 4 to the negative group. Inside each group, 2 classes are labelled as negative and 2 as positive. This simulation framework is similar to the one considered by  where the authors used sub-populations with only 2 classes each.

The average brightness of the samples in each cluster plays the same role as the parameter \(\) in the synthetic model. Our theory predicts that the classifier will advantage the group with highest average brightness, see Eq. 16. In order to achieve the same generalisation error on both subpopulations, the less bright group needs more samples (larger \(\)). This is shown in Fig. 5(a), where the three panels correspond to different assignment of the classes: in the top panel classes are randomly assigned to

Figure 5: **Numerical simulations on MNIST. The figure shows the average (solid lines) and standard deviation (shaded area) of 100 simulations run in this framework. In particular the upper plots show the test loss and lower plots the test accuracy for subpopulation \(+\) (blue) and \(-\) (red). _Panel (a)_ an example of crossing phenomenon obtained by imposing \(_{+}=1\), \(_{-}=0.2\), and \(=0.1\). _Panel (b)_ shows the double crossing, obtained by introducing an additional timescale to the previous case by tuning label imbalance. _Panel (c)_ explore the effect of changing \(_{-}\) while keeping a constant \(_{+}=1\).

the two groups; in the middle panel classes are randomly partitioned in two groups and the brighter one is assigned to group \(-\); finally the last panel assigns the brightest classes to group \(-\) and least bright to group \(+\). As predicted, we need increasingly high relative representation \(\) to achieve a balance in losses at the end of training.

When labels are balanced, our theory predicts that the classifier is initially attracted by the larger \(\) and eventually--if the relative representation of the group with smaller \(\) is large enough--it switches and favours the other group. This effect is indeed verified in the CIFAR10 experiments. Starting from the partitioning in Fig. 5(a) (bottom) with \(=0.8\), the dynamics is initially attracted by group - before advantaging the other group, giving rise to a crossing that can be observed-among other things-in Fig. 5(b).

Fig.5(b) highlights another prediction of our theory concerning the timescale when \(\) becomes relevant. Our theory predicts that \(\) should become relevant only in the final stages of the dynamics and indeed the curves are almost perfectly overlapping until for most of training. As already noticed, \(\) sufficiently large gives rise to a crossing behaviour as indicated demonstrated in the synthetic model.

### Additional numerical experiments

In Appendix F, we provide additional experiments within the TM model and the CelebA dataset, exploring different architectures and losses. Even under these new conditions, we observe that bias presents different timescales and shows crossing behaviours.

## 6 Conclusion

This paper examined the dynamics of bias in a high dimensional synthetic framework, showing that it can be explicitly characterised to reveal transient behaviour. Our findings reveal that classifiers exhibit biases toward different data features during training, possibly alternating sub-population preference. Although our analysis is based on certain assumptions, numerical experiments that violate these assumptions still display the behaviour predicated by our theory.

While this paper centered on bias propagation in a controlled synthetic setting, the study of bias in ML systems is a significant issue with profound societal implications. We believe this line of research will have practical impacts in the medium term, aiding the design of mitigation strategies that account for transient dynamics. Future research will further explore this connection, proposing theory-based dynamical protocols for bias mitigation.

Figure 6: **Numerical simulations on CIFAR10. The figure shows experiments of a 2L neural network on CIFAR10 where classes were grouped together to form the subpopulations. The plots show the average performanceâ€”measure by loss or accuracyâ€”achieved over 100 simulations (for _Panel (a)_) and 10 simulations (for _Panel (b)_, respectively) using the shaded area to quantify the standard deviation. _Panel (a)_ shows the result at the end of training changing relative representation \(\), while _Panel (b)_ shows the training trajectories as \(\) changes as indicated in the colour-bar, see text for more details.**