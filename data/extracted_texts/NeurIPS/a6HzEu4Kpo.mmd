# Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series OOD Generalization

Chengtao Jian

Tongji University, Shanghai, China

jct@tongji.edu.cn

&Kai Yang

Tongji University, Shanghai, China

kaiyang@tongji.edu.cn

Corresponding author.

Yang Jiao

Tongji University, Shanghai, China

yangjiao@tongji.edu.cn

Corresponding author.

###### Abstract

Out-of-Distribution (OOD) generalization in machine learning is a burgeoning area of study. Its primary goal is to enhance the adaptability and resilience of machine learning models when faced with new, unseen, and potentially adversarial data that significantly diverges from their original training datasets. In this paper, we investigate time series OOD generalization via pre-trained Large Language Models (LLMs). We first propose a novel **T**ri-level learning framework for **T**ime **S**eries OOD generalization, termed TTSO, which considers both sample-level and group-level uncertainties. This formula offers a fresh theoretic perspective for formulating and analyzing OOD generalization problem. In addition, we provide a theoretical analysis to justify this method is well motivated. We then develop a stratified localization algorithm tailored for this tri-level optimization problem, theoretically demonstrating the guaranteed convergence of the proposed algorithm. Our analysis also reveals that the iteration complexity to obtain an \(\)-stationary point is bounded by \((})\). Extensive experiments on real-world datasets have been conducted to elucidate the effectiveness of the proposed method.

## 1 Introduction

In machine learning, a common challenge arises when the distributions of training and test sets differ significantly . This mismatch demands that models, trained on specific distribution data, should generalize well on unseen distribution data, known as OOD generalization . Despite a vast amount of research on the OOD generalization , the field of OOD generalization in time series is relatively limited and presents more significant challenges. This is primarily due to the inherent temporal dependencies and dynamic changes characteristic of time series data . Therefore, a critical aspect of improving time series OOD generalization is to learn robust representations that remain stable despite shifts in distributions.

Recently, the field of machine learning has witnessed remarkable advancements in pre-trained foundation models, with notable examples including Large Language Models (LLMs) such as GPT , LLaMA  and CLIP . These models have been instrumental in capturing and leveraging complex patterns across various domains. In addition, using foundation models, especially LLMs, in processing non-linguistic data, e.g., time series is increasingly drawing attention. By fine-tuning only a few handful of parameters, thesemodels show remarkable versatility in diverse data formats ranging from audio (Ghosal et al., 2023), image (Lu et al., 2021) and time series (Chang et al., 2023; Jin et al., 2023). Studies indicate that LLMs, as part of the broader foundation model spectrum, demonstrate sophisticated reasoning and strong pattern recognition capabilities (Wang et al., 2023; Chu et al., 2023), fundamentally acting as pattern machines (Mirchandani et al., 2023). Moreover, LLMs have been shown to be effective in transfer learning across various modalities, due to their data-independent self-attention mechanism (Zhou et al., 2023).

Additionally, recent advancements in vision-language foundation models have shown promising developments in OOD generalization (Zheng et al., 2022), yet the exploration in time series remains underdeveloped. The potential of using foundational models is highlighted by the study in Liu et al. (2023), Hendrycks et al. (2020), which suggests that pre-trained transformers can improve OOD robustness. _Despite existing efforts, the limited exploration of foundational model applications in time series OOD generalization suggests an emerging field._

In this paper, we propose a **T**ri-level learning framework for **T**ime **S**eries **O**OD generalization, named TTSO. Unlike conventional OOD generalization methods that focus solely on group-level (Jiao et al., 2022; Huang et al., 2020) or sample-level uncertainties (Zhang et al., 2017; Zhou et al., 2021; Han et al., 2024), our framework uniquely addresses both by combing a minimization problem for optimal model parameter learning, a maximization problem for dynamically data re-grouping, and another maximization problem for data augmentation under a tri-level framework. To tackle this tri-level problem, we propose a stratified localization algorithm via cutting planes. Leveraging the advanced representation learning capabilities of LLMs, we adapt this tri-level learning framework for fine-tuning LLMs.

Our contributions can be summarized as follows:

* **Tri-level Learning Framework.** In contrast to most existing works in OOD generalization, which primarily focus on either group-level or sample-level uncertainties, TTSO uniquely integrates both aspects under a tri-level learning framework. Specially, this comprehensive framework emphasizes the interdependent relationship between problems of each level, advancing beyond the typical single or bi-level methodologies in OOD generalization. Moreover, a theoretical framework based on Vapnik-Chervonenkis dimension has been developed to rigorously analyze and elucidate the generalization properties of TTSO. We then leverage this tri-level framework to fine-tune LLMs, achieving an maximum 4.9% improvement in performance on time series classification in OOD scenarios.
* **Stratified Localization Algorithm.** To tackle the aforementioned tri-level optimization problem, we develop a stratified localization method using cutting planes. Unlike traditional gradient-based methods, TTSO removes the necessity of computing the hypergradient for the outer optimization problem. This computation is typically very challenging and computationally intensive due to the nested structure of the tri-level optimization problem. Furthermore, the decomposable nature of cutting planes offers a promising avenue for enabling distributed implementations of TTSO, thereby potentially enhancing scalability and computational efficiency.
* **Iteration Complexity Analysis.** To validate the effectiveness of our method, we conducted a thorough theoretical analysis of the algorithm. We theoretically derive that the iteration complexity of the proposed algorithm for achieving an \(\)-stationary point is bounded by \((})\).

## 2 Related Work

In this section, we provide an overview of the foundational concepts and methodologies related to our research, including OOD Generalization and the LLM in time series.

**OOD Generalization.** OOD Generalization research focuses on improving the model's ability to generalize when there is a difference in distribution between the training and test data, and has been widely studied in the fields of Computer Vision (CV) (Recht et al., 2019; Salman et al., 2021) and Natural Language Processing (NLP) (Tu et al., 2020; Schneider et al., 2020). Existing works for out-of-distribution (OOD) generalization are diverse and can generally be categorized into approaches that consider sample-level (Zhang et al., 2017; Zhou et al., 2021) or group-level (Sagawa et al., 2019,Huang et al., 2020) uncertainty. However, the exploration of OOD generalization specially for time series remains relatively underdeveloped. A recent study (Lu et al., 2023) introduced 'Diversify', an innovative approach that models time series data from the perspective of distribution and obtain superior performance. In our work, we consider both sample-level and group-level uncertainties and formulate them as a tri-level optimization problem.

**LLM in Time Series**. The integration of LLMs in time series analysis is a rapidly evolving field, drawing significant interest due to their superior pattern recognition and reasoning abilities (Wang et al., 2023; Chu et al., 2023). A recent example is Time-LLM (Jin et al., 2023), which introduces an innovative method by reprogramming time series and incorporating linguistic prompts, effectively activating the extensive capabilities of LLM. In addition, the OFA framework (Zhou et al., 2023), utilizing the frozen pretrained transformer framework, validates the versatility and effectiveness of pre-trained models in time series analysis. Another innovative approach is PromptCast (Xue and Salim, 2023), which employs a prompt-based learning method, transforming numerical input and output data into prompts for effective forecasting in zero-shot settings. The TEMPO (Cao et al., 2023) adapts to changes in time series distribution by decomposing time series and adding different prompts for each component and obtain competitive performance in time series forecasting. In specialized domains like traffic (Xue et al., 2022), finance (Zhang et al., 2023) and healthcare (Liu et al., 2023), LLMs have also shown unique advantages. In this work, we aim to enhance OOD robustness for time series by fine-tuning LLMs with TTSO.

## 3 Problem Formulation and Algorithm

**Notations.**\(\) and \(\) represent the input and target spaces of samples, respectively. The predictor \(f_{}=h_{} r_{}\) consists of the representation function \(r_{}()\) with parameter \(\) and the classifier \(h_{}\) with parameter \(\). The function \(f_{}:\) maps time series \(\) to \(Y\), where \(^{T F}\) and \(Y_{+}\), with \(T\) as the time series length and \(F\) as the feature dimensions. The multivariate time series \(\), composed of \(F\) univariate time series each with \(T\) observations, is sampled i.i.d. from distribution \(\) and represented as \(=[_{1},_{2},,_{F}]\), where \(_{i}=[x_{v_{1}},x_{v_{2}},,x_{T}]\) for \(i=1,,F\). Assume source domain distributions are \(_{S_{i}}\) for \(i\{1,2,,K\}\) and the target domain distribution is \(_{}\). The source domain data \(_{S_{i}}\) is sampled i.i.d. from \(_{S_{i}}\), and the target domain data \(_{T}\) is sampled i.i.d. from \(_{}\).

### Preliminary

Given a training dataset \(_{}=\{(_{i},Y_{i})\}_{i=1}^{N}\) sampled from the distribution \(_{}(,Y)\). In supervised learning, the goal is to learn an optimal predictor \(f_{^{*}}\) on \(_{}\) such that \(f_{^{*}}\) generalizes well on a test dataset \(_{}\) sampled from the distribution \(_{}(,Y)\). In self-supervised contrastive learning, for a given time series \(\), we generate two augmented views, \(_{a_{1}}\) and \(_{a_{2}}\), using augmentation methods \(a_{1},a_{2}\). These augmentations produce the time series representations \(_{1}=r_{}(a_{1}())^{T M}\) and \(_{2}=r_{}(a_{2}())^{T M}\), through the representation function \(r_{}\). The objective of contrastive learning is to minimize the distance between positive pairs \((_{1},_{2})\) while maximizing the distance between positive and negative pairs. The general formula for contrastive loss, as detailed in Zhao et al. (2022), is formulated as follows

\[_{}=_{}(r_{};,)+ _{}(r_{};,). \]

The first term aims to minimize the distance between positive pairs in the latent space, while the second term served as a regularizer prevents representation collapse. To evaluate the performance of the model, a classifier \(h_{}\) is trained using the representation function\(r_{^{*}}\)

\[h_{^{*}}=_{h_{}}\ _{(,Y)_{ }}_{}_{}(h_{} r_{ ^{*}}(),Y), \]

where the representation function \(r_{^{*}}()\) is optimized via the supervised loss in Eq. (2). The classification is performed using \(f_{^{*}}=h_{^{*}} r_{^{*}}\). However, the discrepancy between the training distribution \(_{}(,Y)\) and the test distribution \(_{}(,Y)\) poses a challenge for generalizing \(f_{^{*}}\) to test data. Directly optimizing \(_{}(f_{}(),Y)\) may lead to overfitting, compromising performance on unseen data. To mitigate this issue, invariant representation learning (Arjovsky et al., 2019) is employed to handle distribution shifts by learning robust invariant representations across diverse distributions. To achieve this, we begin with the following assumption.

**Assumption 1** (Invariant Assumption (Zhao et al., 2022)).: _Considering K different environments (domains) \(\), there exists a random variable \(()\) such that for any \(e,e^{}()\), it holds that \((Y(_{e}))=(Y (_{e^{}}))\)._

This assumption implies that for time series \(\) observed in different environments, invariant rationales exist and their relationship with the corresponding labels remains stable. This stability ensures that predictions remain consistent across various environments, relying on these rationales. Assuming \(()\) represents the representation function \(r_{}()\) parameterized by \(\), then it follows that

\[r_{^{*}}(_{e})=r_{^{*}}(_{e^{}})=(). \]

In contrastive representation learning, where labels are not available, the theoretical analysis of the downstream performance is challenging. To address this, research (Zhao et al., 2022) bridges this gap by connects contrastive loss to downstream risks,

\[(h_{} r_{};_{})\!\!c\|h_{} \|(\!_{}(r_{};_{})\!)^{}\!+\!\|h_{}\|(,\!)\!+\!_{k}\!_{} (_{k})\|e_{k}\!-\!h_{}_{k}(r_{};_{ })\| \]

where \(c\) is a positive constant, \((,)\) refers to a set of constants determined by the \((,)\)-augmentation, and \(C_{k}\) corresponds to the sample set for class \(k\). The first term is optimized during contrastive pre-training. The second term depends on data augmentations \((,)\). The third term, related to the linear layer \(h_{}\), is optimized in downstream tasks. As shown by Eq. (4), contrastive learning on distribution \(\) with augmentation function \(\) essentially optimizes the upper bound of the supervised risk.

Each environment \(\) corresponds to a domain distribution \(_{S_{i}}\). To learn an invariant representation over the domain set \(\), we first provide a mathematical definition of invariant risk minimization.

**Definition 1** (Invariant Risk Minimization (Arjovsky et al., 2019)).: _If there exists a classifier \(h_{0}\) that is optimal for all domains in \(\), i.e., \(h_{0}*{argmin}_{h}(h r_{};_{S_{i}}),_{S_{i}}\), then the representation function \(r_{}\) elicits an invariant predictor \(h_{0} r_{}\) across the domain set \(\)._

This definition is equivalent to learning features that have a stable association with the target variable, which has been theoretically and empirically proven to improve the transferability of supervised learning across different distributions (Arjovsky et al., 2019; Zhao et al., 2022).

### A Tri-level Learning Framework

To address OOD challenges, GroupDRO (Sagawa et al., 2019) propose a minax formulation to minimizes the maximum domain supervised loss to enhance robustness against unseen data. According to Eq. (4), contrastive learning optimizes the upper bound of supervised risk. Thus, we extend GroupDRO by replacing the supervised loss with a self-supervised contrastive loss, aiming to learn invariant representations. We further impose constraints on the group distribution \(\) to mitigate the risk of overfitting to specific domains. This results in a bi-level optimization problem

\[_{,}&_{i=1}^{K}q_{i}_{ }(r_{};_{S_{i}},)\\ &=*{arg\,max}_{^{}^ {K}}_{i=1}^{K}q_{i}^{}_{}(r_{};_{S_{ i}},)\\ &\ d(,^{}), \]

where \(d(,)\) denotes a distribution distance metric, such as KL divergence, Wasserstein distance, or Euclidean distance, \(^{K}\) is a probability simplex, and \(\) is a constant. Following previous work (Qian et al., 2019), we adopt the Euclidean distance due to its strong convexity, which reults in faster convergence (Rakhlin et al., 2012). The outer optimization seeks the best parameters across all domains to optimize overall performance, while the inner optimization, representing the group-level uncertainty, optimizes the worst-case distribution to enhance representation robustness.

**Definition 2** (Augmentation Robust Alignment Loss (Zhao et al., 2022)).: _For any two augmentation methods \(a,a^{}\), the robust alignment loss is defined as follows_

\[_{}(r_{};):=_{} _{(a,a^{})}\|r_{}(a())-r_{}(a^{ }())\|^{2}. \]

**Theorem 1** (Upper Bound of Risk Gap Between Augmented Domains (Shen et al., 2021)).: _For any two augmentation methods \(a,a^{}\), representation function \(r_{}\) and classifier \(h_{}\), we have_

\[_{a,a^{}}\|(h_{} r_{}; _{a})-(h_{} r_{};_{a ^{}})\| c\|h_{}\|_{}(r_{};_{}). \]_Fix \(r_{}\), let \(h_{a}=_{h_{}}(h_{} r;_{})\), we have_

\[\|(h_{a} r_{};_{a^{}})- (h_{a^{}} r_{};_{a^{}}) \| 2c(\|h_{a}\|+\|h_{a^{}}\|)_{ar}(r_{}; _{}). \]

Theorem 1 states that minimizing \(_{}(r_{};_{})\) makes the optimal predictor more consistent across different augmentation domains, i.e., minimize \(_{}(r_{};_{})\) can enhance the invariance of the learned representation. Nonetheless, evaluating \(_{}(r;_{})\) involves a supremum operator, and the large set \(\) makes accurate computation infeasible. Therefore, we propose an approximation for \(_{}(r_{};_{})\). We start with the following reasonable assumption.

**Assumption 2**.: _For any pair of augmentation methods \(a,a^{}\), they can be viewed as introducing specific perturbations \(\) to the sample \(\), i.e., \(a()=+_{a},a^{}( {X})=+_{a^{}}\)._

Suppose \(_{a}\) and \(_{a^{}}\) are sampled from \(_{}\), representing the distribution of perturbations induced by augmentation techniques. We adopt a Gaussian Mixture Model (GMM)  to accurately characterize the uncertain perturbation distribution. Thus, the distribution of \(\) is given by

\[p(;,,)= _{m=1}^{M}_{m}(;_{m},_{m}^ {2}), \]

where \(_{m}\) represents the weight of the \(m^{th}\) component in the mixture, and \(_{m=1}^{M}_{m}=1\). The expression for \(_{}(r_{};_{})\) can be written as \(_{ p(;, ,)}_{i=1}^{K}q_{i}_{}( ,;_{S_{i}})\). Consequently, we can further extend problem (5) to the following tri-level optimization problem.

\[_{,,}&_{i=1}^{K}q_{i}_{}(, ;_{S_{i}})\\ &=^{}^{K}}{ }_{i=1}^{K}q_{i}^{}_{}(,;_{S_{i}})\\ &&d(,^{})\\ &=^{} p(^{};,,)}{ }_{i=1}^{K}q_{i}^{}_{}(, ^{};_{S_{i}})\\ &&\|\| C_{1},\|\| C_{2},_{m=1}^{M}_{m}=1,_{m} 0, \]

where \(C_{1}\) and \(C_{2}\) are constants. The third-level optimization addresses sample-level uncertainties by maximizing the alignment loss under the worst-case perturbation distribution. Figure 1 illustrates these concepts, showing how group-level and sample-level optimizations interact within the tri-level framework. To theoretically justify our approach in Eq. (10), we present the following theorem.

**Theorem 2** (Upper Bound on Target Error).: _Given the previous setup, let \(\) be a hypothesis space of Vapnik-Chervonenkis (VC) Dimension \(d\) and \(h_{T}^{*}=_{h}_{T}(h)\). Let \(_{}=\{_{}_{}=_{i} _{i}_{S_{i}},\;_{i}_{i}=1,\;_{i} 0\,\}\). If \(\) is the empirical minimizer on \(_{}\), then for any \(\) and \(_{C}_{}\), with probability at least \(1-\),_

\[_{T}() 3_{T}(h_{T}^{*})++d_{}(_{C},_{T})+_{i,j}d_{}(_{S_{i}},_{S_{j}})+C(,m,d), \]

_where \(=2_{i=1}^{K}_{i}_{S_{i}}(h^{*})\) and \(C(,m,d)\) is a statistical term. \(d_{}(,)\) is a metric function which measures differences in distribution . \(_{S_{i}}(h)\) and \(_{T}(h)\) is the source error and the target error._

**Discussion**: Theorem 2 provides a theoretical framework for estimating performance on a new target distribution. The TTSO framework in Eq. (10) focuses on minimizing the terms \(d_{}(_{},_{})\) and \(_{i,j}d_{}(_{S_{i}},_{S_{j}})\), thereby giving a tighter bound of target error to improve generalization ability. Proof of Theorem 2 and further discussion of our motivation are in Appendix A.2.

Figure 1: The depiction of sample-level, group-level, and combined uncertainties.

```
0: Training datasets \(\{_{S_{i}}\}\), learning rates \(_{},_{q},_{}\), number of iterations \(T\).
0: Optimized parameters \(^{*}\)
1: Initialize parameters \(,,\) and initial set of cutting planes \(^{0}\).
2:for\(t=0\) to \(T-1\)do
3: Update variable \(^{(t+1)}\), \(^{(t+1)}\) and \(^{(t+1)}\) according to Eq. (19), (20) and (21)
4:if t mod k == 0 then
5:if\(h(^{(t+1)},^{(t+1)},^{(t+1)})>\)then
6: Add new cutting planes to set \(^{t+1}\) according to Eq. (22)
7:endif
8:endif
9:endfor
10:return\(^{(T)}\)
```

**Algorithm 1** SLA: Stratified Localization Algorithm

However, solving the constrained tri-level optimization is extremely challenging. In the next subsection, we introduce a stratified localization algorithm to address this problem effectively.

### Stratified Localization Algorithm

Due to the hierarchical structure of the tri-level problem, we develop a stratified version of the localization method (Boyd and Vandenberghe, 2007; Jiao et al., 2023) to tackle the problem presented in Eq. (10). First, we use exterior penalty method to reformulate the third level problem, the resulting problem is

\[_{,,}&_{i=1}^{K}q _{i}_{}(,;_{S_{i}}) \\ &=*{arg\,max}_{^{}^{K }}_{i=1}^{K}q_{i}^{}_{}(,; _{S_{i}})\\ &&d(,^{})\\ &=*{arg\,max}_{^{} p(^{};,,)}_{i=1}^{K}q_{i}^{} _{}(,^{};_{S_{i}} )-P_{3}, \]

where \(P_{3}\) is a penalty term defined as \(P_{3}=_{1}((0,\|\|-C_{1}))^{2}+_{2}((0,\|\| -C_{2}))^{2}+_{3}(_{m=1}^{M}_{m}-1)^{2}+_{4}((0,-_{m})) ^{2}\), and \(_{i}\) are penalty coefficients.

Given that the third-level optimization is a constraint for the second-level optimization, we employ \(T_{3}\) steps of gradient ascent to approximate the third-level problem. This technique is commonly used in previous bi-level optimization studies (Ji et al., 2021). By defining \(f_{3}(,^{},^{})=_{i=1}^{K}q_{i}^{ }(_{}(,^{};_{ S_{i}})-P_{3}\) and using the exterior penalty method, the resulting optimization problem can be expressed as

\[_{,,}&_{i=1}^{K}q _{i}_{}(,;_{S_{i}}) \\ &=*{arg\,max}_{^{}^{K }}_{i=1}^{K}q_{i}^{}_{}(,; _{S_{i}})-P_{2}, \]

where \(P_{2}=_{1}(_{i=1}^{K}q_{i}-1)^{2}+_{i=1}^{K}_{2}(0,-q _{i})+_{3}\|-^{(0)}-_{i=0}^{T_{3}-1}_{ }_{^{}}f_{3}(,^{},^{})\|^{2}\). Likewise, we perform \(T_{2}\) steps of gradient ascent to replace the second level optimization problem. With the definition of \(f_{2}(,^{},)=_{i=1}^{K}q_{i}^{} _{}(,;_{S_{i}}^{tr}) -P_{2},(,)=*{arg\,max}_{^{ }}f_{2}(,^{},)\) and \(h(,,)=\|-(,)\|\), Eq. (13) can be reformulated as follows

\[_{,,}&_{i=1}^{K}q _{i}_{}(,;_{S_{i}}) \\ &h(,,)=0. \]

Let \(f_{1}(,,)=_{i=1}^{K}q_{i}_{}(,;_{S_{i}})\). Considering the approximations of \(\) and \(\), the above problem can be relaxed as

\[_{,,}&f_{1}(,,)\\ &h(,,), \]

where \(>0\) is a constant. Inspired by the polyhedral approximation method (Burger et al., 2013), we utilize cutting planes to approximate the feasible region with respect to \(h(,,)\). In the \((t+1)^{th}\) iteration, the set of cutting planes, denoted as \(^{t}\), is defined as follows

\[^{t}=\{_{i}^{}+_{i}^{}+_{i} ^{}+d_{i} 0,i=1,,|^{t}|\}, \]

where \(_{i}^{N},_{i}^{M},_{i}^{H},d_{i}^{1}\), and \(|^{t}|\) represents the number of cutting planes in \(^{t}\). Then Eq. (15) can be expressed as the following approximation problem

\[_{,,} f_{1}(,,)\] (17) s.t. \[_{i}^{}+_{i}^{}+_{i}^{ }+d_{i} 0, i=1,,|^{t}|.\]

The penalty function with respect to Eq. (17) can be described as

\[F(,,)=f_{1}(,, )+_{i}_{i}(0,_{i}^{}+_{i}^{}+_{i}^{}+d_{i})^{2}. \]

In \((t+1)^{th}\) iteration, the variables are updated as follows

\[^{t+1} =^{t}-_{}_{}F(^{t}, ^{t},^{t}), \] \[^{t+1} =^{t}-_{}_{}F(^{t},^ {t},^{t}),\] (20) \[^{t+1} =^{t}-_{}_{}F(^{t}, ^{t},^{t}). \]

Throughout the iteration process, the set of cutting planes \(^{t}\) is updated every \(k\) iterations for a tighter and more accurate polyhedral approximation. Before adding new cutting planes, we first check whether \((^{t+1},^{t+1},^{t+1})\) is a solution for Eq. (15). If it is not a feasible solution to Eq. (15), i.e., \(h(,,)>\), new cutting planes are added to \(^{t}\) based on Theorem 3 and Proposition 22. Algorithm 1 provides details of the proposed method.

**Theorem 3**.: _Let \(T_{2}=1\). If a first-order Taylor expansion is applied to the function \(f_{2}(,,)\) at the point \((},})\), it follows that the function \(h(,,)\) is convex with respect to \((,,)\). The detailed proof can be found in Appendix A.3._

**Proposition 1**.: _Given the convexity of the function \(h(,,)\), a new cutting plane is generated when the condition \(h(,,)>\) is not met. This cutting plane is formally expressed as_

\[h(^{t+1},^{t+1},^{t+1})+[ _{}h(^{t+1},^{t+1},^{t+1})\\ _{}h(^{t+1},^{t+1},^{t+1})\\ _{}h(^{t+1},^{t+1},^{t+1}) ]^{}[-^{t+1}\\ -^{t+1}\\ -^{t+1}]. \]

For the detailed derivation and proof of Proposition 22, please see Appendix A.1.

### TTSO for Fine-tuning LLMs

LLMs have garnered considerable attention in time series applications (Jin et al., 2023; Zhou et al., 2023). The emergent abilities of LLMs, especially in OOD scenarios, largely depends on the robustness of their representations(Wang et al., 2023; Chu et al., 2023). This section connects the established theoretical foundation with the practical application of fine-tuning LLMs for time series OOD generalization. We adapt TTSO framework for fine-tuning LLMs to enhance the performance in time series OOD generalization. Our proposed method involves a dual-stage fine-tuning method tailored for time series. The main process of fine-tuning are described below.

**Time Series Pre-processing.** Preprocessing starts with an input projection layer to bridge the gap in dimensions between raw time series data and the LLM's native embedding dimension. This step is crucial for the LLM's effective integration of time series. Following this, positional encoding is applied to preserve the sequential integrity of the time series.

**Dual-stage Fine-tuning Method.** In the first stage, we employ TTSO framework to fine-tune LLMs, in line with the previously mentioned tri-level optimization framework as illustrate in Eq. (10). We adopt the contrastive loss function designed for time series from Yue et al. (2022). In the second stage, the learned weights of the LLM, including the projection layer, are transferred to the downstream fine-tuning stage for time series classification. To retain the knowledge learned by the LLM from the corpus, we follow Chang et al. (2023); Zhou et al. (2023) by fixing the weights of the fully connected and attention layers, using Layer Normalization Tuning (Lu et al., 2022) to adjust only the layer normalization parameters, making the affine transformation trainable.

**Constrained Optimization for Fine-Tuning.** Research (Wortsman et al., 2022) indicates that adopting radical strategies for fine-tuning models, such as larger learning rates, can reduce out-ofdistribution robustness. Unconstrained optimization of model parameters during fine-tuning can lead to knowledge forgetting issues and decrease the model's generalization ability, as mentioned in Xuhong et al. (2018). Therefore, during fine-tuning for downstream tasks, we impose constraints on the parameters, following Xuhong et al. (2018), resulting in the following optimization problem

\[_{}&_{cls}(r_{ } h_{};)\\ &\|-_{0}\| , \]

where \(_{0}\) and \(\) respectively denote the weights from the first and second fine-tuning phases of the LLMs. More details of fine-tuning LLMs can be found in appendix D.

## 4 Convergence Analysis

**Assumption 3** (**Lipschitz Continuity of Gradient)**.: _Assume that the gradient of the function \(F\) is \(L\)-Lipschitz continuous gradient, i.e., for any \(,\), there exists \(L>0\) such that:_

\[\| F()- F()\| L\|-\|. \]

**Assumption 4** (**Unbiasedness and Variance Bound of Stochastic Gradients)**.: _Assume for the stochastic gradients \(g_{},g_{q},g_{}\), the following conditions are satisfied_

\[&_{_{j}^{t}}[g_{}( ^{t},^{t},^{t};_{j}^{t})-_{}F(^{ t},^{t},^{t})]=0,\\ &_{_{j}^{t}}[g_{}(^{t},^{t},^{t};_{j}^{t})-_{q}F(^{t},^{t},^{t})]=0,\\ &_{_{j}^{t}}[g_{}(^{t},^{t},^{t};_{j}^{t})-_{}F(^{t},^{t}, ^{t})]=0,\\ &_{_{j}^{t}}[\|g_{}(^{t},^{ t},^{t};_{j}^{t})-_{}F(^{t},^{t}, ^{t})\|^{2}]_{1}^{2},\\ &_{_{j}^{t}}[\|g_{}(^{t},^{ t},^{t};_{j}^{t})-_{q}F(^{t},^{t},^{t})\|^{2}]_{2}^{2},\\ &_{_{j}^{t}}[\|g_{}(^{t},^{ t},^{t};_{j}^{t})-_{}F(^{t},^{t}, ^{t})\|^{2}]_{3}^{2}, \]

_where \(_{_{j}^{t}}[]\) denotes the expectation over the \(_{j}^{t}\)._

**Assumption 5** (**Bounded Gradient)**.: _Assume that the gradient of the function \(F\) is bounded, i.e., \( t,\|_{}F(^{t},^{t},^{t})\|^{2} _{1}^{2},\|_{q}F(^{t},^{t},^{t})\| ^{2}_{2}^{2},\|_{}F(^{t},^{t},^{t})\|^{2}_{3}^{2}\)._

**Definition 3** (\(\)-**Stationary Point)**.: _Following Xu et al. (2023), Jiao et al. (2024), a point \((,,)\) is considered an \(\)-stationary point (where \(>0\)) of a differentiable function \(F\) if the sum of squares of its gradients on these variables satisfies \(\| G^{t}\|\). Let \(T()\) be the index of the first iteration that satisfies \(\| G^{t}\|\), i.e., \(T()=\{t\ \| G^{t}\|,t>t_{1}\}\)._

**Theorem 4** (**Convergence Guarantee)**.: _With the continuous addition of cutting planes, the optimal objective value of the approximated problem, delineated in Eq. (17), is guaranteed to converge monotonically. For further details, see the proof of Theorem 4 in appendix A.4._

**Theorem 5** (**Convergence Rate)**.: _Under the assumptions 3, 4, and 5, by setting the step-sizes as \(_{}=_{q}=_{}=-t_{1}}}\) and the batch size as \(B\), for a given \(\), it follows that_

\[T()(t_{1}+(m(_{1}^{2}+_{2}^ {2}+_{3}^{2})+_{1}^{2}+_{2}^{2}+_{3}^{2})^{2}}{4m^{2}( -F(^{T_{1}},^{T_{1}},^{T_{1}})+F^{*})^{2 }}), \]

_where \(F^{*}\) represents the lower bound of \(F\). The proof of Theorem 5 is detailed in appendix A.5._

## 5 Experiment

To evaluate the proposed TTSO framework, we conduct experiments on 6 real-world time series datasets using the leave-one-domain-out setting, including HHAR (Blunck et al., 2015), PAMAP (Reiss, 2012), WESAD (Philip Schmidt et al., 2018), SWELL (Koldijk et al., 2014), USC-HAD(Zhang and Sawchuk, 2012) and DSADS (Barshan and Altun, 2013). We compare with baseline method ERM (Vapnik, 1991) and 8 general OOD generalization methods: IRM (Arjovsky et al., 2019), GroupDRO (Sagawa et al., 2019), ANDMask (Parascandolo et al., 2020), RSC (Huang et al., 2020), Mixup (Zhang et al., 2017), VERx (Krueger et al., 2021), DIFEX(Lu et al., 2022b). And we further compare with 2 recent strong approach in time series: AdaRNN(Du et al., 2021) and GILE (Qian et al., 2021). We also include DIVERSIFY(Lu et al., 2023), DFDG(Zhang et al., 2021), and CCDG(Ragab et al., 2022), three methods specifically designed for time series OOD

[MISSING_PAGE_FAIL:9]

This demonstrates that even in the absence of a pre-trained model, TTSO fine-tuning can effectively enhance the model's OOD generalization capabilities, though not as significantly as that with a pre-trained GPT2.

## 6 Conclusion

Existing OOD generalization methods mainly focus on sample-level uncertainties or group-level uncertainties, often overlooking the interplay between these two aspects. In light of this, we propose the TTSO framework to integrate both sample-level and group-level uncertainties within a unified tri-level learning approach, thereby enhancing the model's robustness and adaptability in facing diverse and unforeseen distribution shifts. In addition, this innovative framework introduces a fresh perspective for the development and analysis of the Out-of-Distribution (OOD) generalization problem. Based on this formulation, we develop a stratified localization algorithm for the tri-level optimization problem and provide theoretical analysis regarding the iteration complexity of the proposed algorithm. Comprehensive studies have been carried out to assess the performance of the proposed algorithm and substantiate the theoretic claims. It is seen that TTSO with LLM can considerably improves the performance of time series OOD generalization.

## 7 Acknowledgements

This work was supported in part by the National Natural Science Foundation of China under Grant 12371519 and 61771013; in part by Asiainfo Technologies; in part by the Fundamental Research Funds for the Central Universities of China; and in part by the Fundamental Research Funds of Shanghai Jiading District.