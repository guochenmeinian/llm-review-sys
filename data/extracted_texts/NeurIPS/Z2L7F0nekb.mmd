# Meta-Learning with Neural Bandit Scheduler

Yunzhe Qi

University of Illinois at Urbana-Champaign

Champaign, IL

yunzheq2@illinois.edu

&Yikun Ban

University of Illinois at Urbana-Champaign

Champaign, IL

yikunb2@illinois.edu

&Tianxin Wei

University of Illinois at Urbana-Champaign

Champaign, IL

twei10@illinois.edu

&Jiaru Zou

University of Illinois at Urbana-Champaign

Champaign, IL

jiaruz2@illinois.edu

&Huaxiu Yao

University of North Carolina at Chapel Hill

Chapel Hill, NC

huaxiu@cs.unc.edu

&Jingrui He

University of Illinois at Urbana-Champaign

Champaign, IL

jingrui@illinois.edu

Equal Contribution.

###### Abstract

Meta-learning has been proven an effective learning paradigm for training machine learning models with good generalization ability. Apart from the common practice of uniformly sampling the meta-training tasks, existing methods working on task scheduling strategies are mainly based on pre-defined sampling protocols or the assumed task-model correlations, and greedily make scheduling decisions, which can lead to sub-optimal performance bottlenecks of the meta-model. In this paper, we propose a novel task scheduling framework under Contextual Bandits settings, named BASS, which directly optimizes the task scheduling strategy based on the status of the meta-model. By balancing the exploitation and exploration in meta-learning task scheduling, BASS can help tackle the challenge of limited knowledge about the task distribution during the early stage of meta-training, while simultaneously exploring potential benefits for forthcoming meta-training iterations through an adaptive exploration strategy. Theoretical analysis and extensive experiments are presented to show the effectiveness of our proposed framework.

## 1 Introduction

Meta-learning algorithms  have been receiving increased attention due to their strong generalization performance across a wide range of tasks . Most existing meta-learning methods often assume a uniform distribution when drawing meta-training tasks, treating each task as equally important . However, this assumption can possibly fail in real-world scenarios. For example, during data collection, candidate training tasks can be subject to noise perturbation, leading to performance bottlenecks in the meta-model if noisy and clean tasks are treated on an equal footing . In addition, some tasks can pose greater challenges for the meta-model to adapt to, necessitating a more flexible allocation of computational resources during the meta-training process. Furthermore, the task distribution may be skewed, with "tail" tasks receiving inadequate attention under uniform sampling. As a result, the task scheduling methods  have been proposed for a refined meta-training strategy.

Existing scheduling approaches mainly aim to improve meta-training strategies based on various pre-defined criteria and assumptions, including both non-adaptive and adaptive methods. Among them, non-adaptive methods working on the gradient that updates trainable parameters [31; 13] have proven their superiority over meta-training methods with uniform sampling. But they are unable to adjust the task scheduling strategy adaptively based on the status of the meta-model. On the other hand, adaptive methods aim to schedule the tasks based on Curriculum Learning [14; 52; 50] or adaptively sample the tasks based on the task adaptation difficulty (loss) . However, the existing approaches are all greedy algorithms, which means that they tend to make locally optimal decisions based on the current knowledge (i.e., exploitation only). As the learner only has limited knowledge regarding the task and data distribution at the early stage of meta-training, the greedy strategy can lead to the sub-optimal meta-model due to multiple reasons, such as being misled by the noisy tasks or affected by the skewness of the task distribution. Here, we use Figure 2 to illustrate an example where the greedy approach (only exploitation) may be trapped in sub-optimal solutions.

One intuitive solution for the aforementioned problems is tackling the exploitation-exploration dilemma  in meta-training process: balancing the exploitation of current knowledge of selected tasks, and the exploration of under-explored tasks for potential long-term benefits. Therefore, unlike existing approaches with greedy strategies, we propose a novel task scheduling framework under the Contextual Bandits settings [15; 28; 4; 1; 54; 53; 2], named **BA**ndit **TaSk **S**cheduler (BASS). In each meta-training iteration, we formulate each candidate meta-training task as an arm under the contextual bandit settings, and the corresponding arm reward will naturally be the meta-model generalization ability score after including this candidate task (arm) into the meta-training process. To achieve this objective, BASS directly learns the mapping from the meta-parameters to the meta-model generalization ability score, instead of depending on the hand-crafted criteria or assumptions. This design also enables us to update meta-parameters and BASS simultaneously within one round of meta-optimization. In particular, instead of greedily scheduling the meta-training tasks solely based on the current knowledge (i.e., exploitation), BASS leverages an additional adaptive exploration module with two different exploration objectives to explore for unrevealed benefits. Our main contributions can be summarized as follows:

* **[Problem and Proposed Framework]** We are the first to formulate the meta-learning task scheduling problem under the Contextual Bandits settings, where we optimize the meta-model w.r.t. chosen task batches in each meta-training iteration. To tackle this problem, we propose a novel bandit-based meta-learning task scheduling framework named BASS, which is model-agnostic and can be applied to various meta-learning frameworks. Different from existing works, BASS directly learns the relationship between the meta-model parameters and the meta-model generalization ability. In addition, instead of greedily exploiting the current knowledge as in existing works, BASS utilizes a novel exploration module to adaptively plan for the future meta-training iterations.
* **[Theoretical Analysis]** Under the general meta-learning settings as well as standard assumptions of over-parameterized neural networks and neural bandits, we derive the theoretical performance guarantee for the proposed BASS framework in terms of regret bound.
* **[Experiments]** To demonstrate the effectiveness of BASS, we compare our method against seven strong baselines on three real data sets, with different specifications. In addition, complementary experiments as well as a case study on Ensemble Inference are also provided to better understand the property and behavior of BASS.

## 2 Related Works

In this section, we briefly review the related works of our proposed BASS framework from the aspects of task scheduling in meta-learning and contextual Bandits.

Figure 1: Exploitation and exploration in meta-training iteration \(k[K]\). E.g., tasks 1, 2, 3 are “freequent” tasks in a skewed task distribution, and tasks 4, 5, 6 are from task distribution “tail”. Exploring the “tail” tasks can help improve the meta-model generalization performance (Subsec. 6.2).

Task Scheduling in Meta-Learning.By considering each meta-training task to be equally important, many existing works sample the training tasks uniformly from the given task distribution [19; 40]. Then, with fixed sampling strategies, [24; 35] propose to assign the task sampling probability based on the quantity of task information, and  utilizes a probabilistic sampling method based on class-pairs. Meanwhile, there are also works try to improve task-specific gradients for the randomly sampled tasks [13; 34]. On the other hand, Curriculum-based approaches [14; 52; 50] schedule the tasks based on the difficulty of task adaptation, and  propose to adaptively schedule the tasks based on the assumed correlation between the difficulty of meta-training tasks and the meta-model generalization ability. However, the existing works are all greedy approaches that solely focus on the instant benefits, which can lead to sub-optimal meta-models due to the potential performance bottleneck. Compared with existing works, our proposed BASS directly learns the mapping from the meta-parameters to the generalization loss with no pre-defined criteria. With the adaptive exploration strategy, our proposed BASS helps tackle the insufficient knowledge regarding the task distribution by balancing the exploitation and exploration, as well as focusing on the long-term effects.

Contextual Bandits.Contextual bandits algorithms aim to solve the sequential decision making problem under the online learning settings, such as online recommendation [28; 49; 6]. Assuming that the mapping from arm contexts to rewards is linear, linear upper confidence bound (UCB) algorithms [15; 28; 4] are proposed to solve the exploitation-exploration dilemma. After kernel-based methods [46; 17] being applied to deal with the non-linear setting where the reward mapping is assumed to be a function in Reproducing Kernel Hilbert Space (RKHS), neural bandit algorithms [54; 53; 7; 5; 39; 8] are proposed to utilize neural networks for the reward and confidence bound estimation. In particular, neural bandit algorithms have demonstrated their superior performance over linear and kernel-based algorithms , thanks to the representation power of neural networks. Moreover, instead of using non-negative UCB, EE-Net [9; 10] achieves adaptive exploration by adopting an additional neural network to estimate the potential gain of reward estimations. In this paper, we model the task scheduling problem under the Contextual Bandit settings to balance the exploitation and exploration dilemma regarding the meta-task scheduling.

## 3 Problem Definition and Learning Objective

Under the settings of few-shot supervised meta-learning , our goal is to train a meta-model \((;)\) with parameters \(^{p}\) that can generalize well to meta-testing tasks, where \(p\) is the number of trainable meta-parameters. The meta-model parameters are initialized as \(^{(0)}\). Note that in this work, the trainable parameters are all represented by column vectors, for the simplicity of notation. Following similar settings in [19; 51], in each training iteration \(k[K]\), we will receive a pool of candidate training tasks \(^{(k)}_{}=\{_{k,i}\}_{i}\) where its cardinality \(|^{(k)}_{}|=N_{}\) and the index set \(=\{1,,N_{}\}\). Given a task distribution \(()\), we draw each candidate task \(_{k,i}\) from \(()\) to form the candidate pool, i.e., \(_{k,i}()\). Each task \(_{k,i}\) is also associated with a support data set \(D^{s}_{k,i}\) and a query data set \(D^{}_{k,i}\), where the samples (including their labels) of \(D^{s}_{k,i},D^{}_{k,i}\) are drawn from the corresponding task data distribution \(_{_{k,i}}\) of task \(_{k,i}\).

Meta-training Process.Then, we need to choose a batch of \(B\) tasks \(_{k}=\{_{k,}\}_{}_{k}}^{(k)}_{}\) for each meta-training iteration \(k[K]\), where \(}_{k}\) refers to the indices of chosen tasks. With \(^{(k-1)}\) being the meta-model parameters after completing the \((k-1)\)-th meta-training iteration, for each **chosen** meta-training task \(_{k,},}_{k}\), we run Gradient Descent (GD) for \(J\) steps on its support set \(D^{s}_{k,}\) to obtain the task-specific parameters \(^{(J)}_{k,}\). This refers to the _inner-loop optimization_. In this work, we consider a loss function \((;)\) that maps the meta-model parameters and the input sample (or sample set) to the loss value, with the range \(\) (e.g., the MSE loss on normalized user ratings ). Denoting \(^{(0)}_{k,}=^{(k-1)}\) in meta-training iteration \(k\), each GD step is represented as

\[^{(j)}_{k,}=^{(j-1)}_{k,} -_{1}_{}(D^{s}_{k,}; ^{(j-1)}_{k,}), j[J]\] (1)

where \(_{1}^{+}\) is the learning rate of GD. For the simplicity of notation, we formulate the above \(J\)-iteration _inner-loop optimization_ as an operator \((,):\), such that

\[^{(J)}_{k,}=(_{k,} \,,\,^{(k-1)}).\] (2)Then, we optimize the meta-parameters through the _outer-loop optimization_ with query sets

\[^{(k)}=\ ^{(k-1)}[_{k}]=\ ^{(k-1)}-_{2} _{}|}_{_{k,} _{k}}(D_{k,}^{q};_{k,}^{(J)})\] (3)

where \(_{2}^{+}\) is the learning rate. Here, the trained meta-model parameters \(^{(K)}\) is expected to minimize the generalization loss \(_{(),_{ }};(,^{(K)}),\) where we let \(_{}\) being the associated data distribution of task \(\).

**Evaluating Task Batch Benefits.** Recall that in meta-training iteration \(k[K]\), the meta-model will be updated based on the chosen batch of \(B\) tasks (**Eq. 3**), and we will need to evaluate the benefit in terms of the whole task batch. By the task scheduling problem definition, the learner will _select one task batch_\(_{k}_{}^{(k)}\) based on its strategy, and the chosen task batch \(_{k}\) will be used for meta-training in this iteration \(k\). Here, we define the reward of the corresponding task batch \(_{k}\) as

\[h^{(k-1)}[_{k}]=1-_{ (),_{}};(,^{(k-1)}[_{k}]) \] (4)

where \(^{(k-1)}[_{k}]\) refer to the meta-parameters after adapting \(^{(k-1)}\) to task batch \(_{k}\) based on **Eq. 3**. For simplicity, we let \(h:^{p}\) be the mapping function conditioned on distribution \(()\), which maps the trained meta-model parameters \(^{(k-1)}[_{k}]\) to task batch rewards.

**Learning Objective.** Up to meta-training iteration \(k\), we have \(^{*}(K)=\{_{1}^{*},,_{K}^{*}\}\) being a series of unknown _optimal_ task batches that minimizes the generalization loss. Each optimal batch is \(_{k}^{*}=\{_{k,i^{*}}\}_{i^{*}_{k}^{*}} _{}^{(k)},k[K]\) including a total of \(|_{k}^{*}|=B\) tasks, with the indices \(_{k}^{*}\). The corresponding optimal meta-parameters are defined as \(^{(K),*}=_{}_{ (),_{}} ;(,) ,\) where \(^{p}\) refers to the reachable parameter space, which contains the possible trained meta-parameter values, after observing the available candidate tasks \(\{_{}^{(k)}\},k[K]\) and the randomly initialized meta-parameters \(^{(0)}\). Meanwhile, denoting our selection of the meta-training task batches as \((K)=\{_{1},,_{K}\}\), we will have the corresponding trained meta-parameters \(^{(K)}\). Here, we can define the \(K\)-iteration regret as

\[R(K)=_{(), _{}};(,^{(K)})-;(,^{(K),*})\] (5)

which measures the difference of the generalization ability between the trained meta-parameters \(^{(K)}\) and the optimal meta-parameters \(^{(K),*}\), after \(K\) meta-training iterations.

## 4 Proposed Framework: BASS

In this section, we introduce our proposed BASS framework, which simultaneously optimizes the meta-model and the task scheduling strategy on the fly. The pseudo-code is presented in **Algo.** 1, and the illustration for each meta-training iteration \(k[K]\) is shown in **Figure** 2.

Figure 2: In meta-training iteration \(k[K]\), the BASS framework overview. We only need one round of the optimization process (LHS of the figure) to update the meta-model and BASS.

**Remark 1** (Task Scheduling with Contextual Bandits).: _By the problem definition, there are \(N_{}=}}{B}\) candidate task batches in each meta-training iteration, which can be a large number. In this case, enumerating all possible task batches and estimating their rewards will be time consuming. Therefore, under the Contextual Bandits settings, BASS alternatively considers **each candidate task**\(_{k,i}_{}^{(k)}\)**as an arm**, and directly chooses \(B\) arms as the meta-training tasks \(_{k}_{}^{(k)}\). As a result, BASS can (1) reduce the arm space size from \((N_{})\) to \((N_{})\), while (2) enjoy the performance guarantee (**Section**5) in terms of regret bound (**Eq. 5**)._

Section Outline.Here, we will first present our definition of the **arm contexts** (**Subsec. 4.1**), whose formulation is challenging, because our settings are different from conventional Contextual Bandits where arm contexts are readily available from the environment. Then, applying two neural networks \(f_{1},f_{2}\) for exploitation and exploration respectively, we formulate the **arm benefit score** (**Subsec. 4.2**), which measures the benefit if we include the corresponding arm (task) into the current meta-training process. Next, we define the **arm rewards** and **exploration scores** as the labels for training \(f_{1},f_{2}\) respectively. In particular, to deal with the challenge of achieving exploration in task scheduling, we incorporate the information and the dynamics w.r.t. meta-optimizations, and formulate two separate exploration objectives for a refined exploration strategy (**Subsec. 4.3**). Finally, we update BASS with GD (**Subsec. 4.4**), and train the meta-model with chosen tasks.

### Formulating Arm Contexts

To encode the information from both the task side and the meta-model side, for each candidate task (i.e., arm) \(_{k,i}_{}^{(k)}\) in meta-training iteration \(k[K]\), we formulate its arm contexts as the meta-parameters after task adaptations, denoted by

\[_{k,i}^{s}:=_{k,i}^{(J)}=(_{k,i}, {}^{(k-1)});_{k,i}^{q}:=^{(k-1)}[_{k, i}]=^{(k-1)}-_{2}_{}(D_{k,i}^{q}; _{k,i}^{(J)})\] (6)

where \(_{k,i}^{(J)}\) are the task-specific parameters after adapting meta-parameters \(^{(k-1)}\) to task \(_{k,i}\) with inner-loop optimization (**Eqs. 1**-2); while \(^{(k-1)}[_{k,i}]\) refer to the meta-parameters after adapting the current \(^{(k-1)}\) to task \(_{k,i}\), with both inner-loop and outer-loop optimization (as in **Eq. 3**). In particular, we assign each arm \(_{k,i}\) with two different arm contexts to model the dynamics of meta-parameters w.r.t. inner-loop optimization and outer-loop optimization respectively. For example, the variance of the corresponding data distribution \(_{_{k,i}}\) can be high. In this case, the support set \(D_{k,i}^{s}\) and the query set \(D_{k,i}^{q}\) will be considerably different, which tends to make the correspondingarm contexts \(^{s}_{k,i}\), \(^{q}_{k,i}\) divergent. As a result, the gradient vectors \(_{}f_{1}(^{s}_{k,i})\), \(_{}f_{1}(^{q}_{k,i})\) will likely be distinct from each other. Alternatively, if the support set \(D^{s}_{k,i}\) and the query set \(D^{q}_{k,i}\) are not significantly distinct (the distance between \(^{s}_{k,i}\) and \(^{q}_{k,i}\) is also likely to be relatively small), these two gradient vectors tend to change dramatically when adapting to \(_{k,i}\). The reason is possibly that the exploitation model \(f_{1}\) is not well adapted to this task \(_{k,i}\). For both scenarios above, it can be beneficial to include more exploration for the task \(_{k,i}\), and the target is helping \(f_{1}\) better learn the reward for this task by actively acquiring the knowledge of it. And the two formulated arm contexts can provide important reference for our exploration module.

**Remark 2** (Recycling Arm Contexts).: _In order to derive the arm contexts (**Eq. 6**), the gradients for the outer-loop optimization \(_{}(D^{q}_{k,},^{(J)}_{k, }),}_{k}\) of the chosen arms \(_{k,}_{k}\) are calculated. As a result, these gradients can be **recycled** to update the meta-model parameters based on **Eq. 3** (line 15, **Algo. 1**), which helps reduce the computational cost when updating the meta-model._

### Estimating Benefit Scores for Tasks

To determine which arms (tasks) should be included to the meta-training iteration \(k\), we formulate the **arm benefit score** estimation for each candidate arm \(_{k,i}^{(k)}_{}\). The estimated benefit score consists of two parts: (1) the estimated _arm reward_ of choosing this task based on existing knowledge (i.e., exploitation); (2) and the _exploration score_ for the future potential benefit (i.e., exploration). Inspired by recent advances in neural bandits , we introduce two separate neural networks, \(f_{1}(;_{1})\) and \(f_{2}(;_{2})\), to estimate the _arm reward_ and _exploration score_ respectively. The exploitation network \(f_{1}(;_{1})\) aims to learn the mapping \(h()\) from arm contexts (i.e., meta-parameters) to rewards, while the exploration network \(f_{2}(;_{2})\) aims to learn the uncertainty of reward estimations as the exploration criterion. Different from conventional bandit models, e.g. , that works on static arm contexts given by the environment, our design alternatively leverages the evolving information from both the task (arm) side and meta-parameters side, across meta-training iterations. In addition, we consider the dynamics of meta-optimizations for a more comprehensive modeling of the exploration aspect, and the details will be introduced later. Here, given a candidate arm \(_{k,i}^{(k)}_{}\), its estimated benefit score \(_{k,i}\) is formulated as

\[_{k,i}=_{k,i}+_{k,i}= f _{1}(^{q}_{k,i};^{(k-1)}_{1})+f_{2}[_{}f_{1}(^{s}_{k,i});\;_{}f_{1}(^{q}_{ k,i})];^{(k-1)}_{2}\] (7)

where \((0,1]\) is the exploration coefficient to balance exploitation and exploration. Notice that \(f_{2}(;_{2})\) will take the _concatenated gradient_ of \(f_{1}(;_{1})\) w.r.t. both arm contexts \(^{s}_{k,i},^{q}_{k,i}\) as the input, represented by \([_{}f_{1}(^{s}_{k,i});_{}f_{1}( {}^{q}_{k,i})]\). And the output will be the exploration score estimation \(_{k,i}\). To obtain \(_{}f_{1}(^{q}_{k,i})\), we also calculate \(f_{1}(^{q}_{k,i};_{1})\) and run the back-propagation. Afterwards, we choose the top-\(B\) arms with the highest estimated benefit scores \(_{k,i},i\), as the chosen task batch \(_{k}^{(k)}_{}\) (line 10, **Algo. 1**).

**Design Intuition.** First, recent advances of neural Contextual Bandits [54; 9; 38] have shown that the uncertainty of reward estimations is directly related to the gradients of the estimation model. Therefore, we leverage an exploration module \(f_{2}(;_{2})\) to directly learn this unknown relationship. Second, since \(D^{s}_{k,i},D^{q}_{k,i}\) are from the same data distribution \(_{_{k,i}},\) if these two gradients \(_{}f_{1}(^{s}_{k,i}),_{}f_{1}( {}^{q}_{k,i})\) are distinct, the reason can be: (1) the variance of the data distribution \(_{_{k,i}}\) is high (due to the potentially noisy or difficult task); or (2) the gradients of \(f_{1}(;_{1})\) tend to change significantly when adapting to task \(_{k,i}\). In both cases, it can be harder for \(f_{1}(;_{1})\) to accurately predict the arm reward \(r_{k,i}\), and the meta-model can fail to properly adapt to the task \(_{k,i}\). In this case, we apply the concatenated gradients w.r.t. both arm contexts as the input of \(f_{2}(;_{2})\), in order to provide the information for \(f_{2}(;_{2})\) to evaluate exploration scores.

**Network Architecture and Parameter Initialization.** Here, we consider \(f_{1}(;_{1}),f_{2}(;_{2})\) to be two \(L\)-layer fully-connected (FC) networks with network width \(m\), while \(=\{_{1},_{2}\}\) refer to their trainable parameters. For their randomly initialized parameters \(^{(0)}=\{_{1}^{(0)},_{2}^{(0)}\}\), the weight matrix entries for the first \(L-1\) layers are drawn from the Gaussian distribution \(N(0,2/m)\), while the entries of the last layer (\(L\)-th layer) are sampled from \(N(0,1/m)\).

**Remark 3** (Reducing Input Complexity).: _The input of \(f_{1}(;_{1})\) is the arm context \(^{q}_{k,i}\), whose dimensionality is the number of meta-parameters \(p\). A similar situation also exists for the exploration network \(f_{2}(;_{2})\). Inspired by the idea of learning dense low-dimensional representations with Convolutional Neural Networks (CNNs) (e.g., ), we apply the **average pooling** approach to approximate original inputs for reducing the running time and space complexity in practice. To show its effectiveness, we will apply this approach on BASS for all the experiments in Section 6._

### Formulating Arm Rewards and Exploration Scores

Different from the conventional neural bandit algorithms [53; 54; 9] where the reward is provided by the environment oracle, we need to carefully design the arm rewards to reflect the arm benefit in terms of the meta-model's generalization ability. Analogous to task batch rewards (**Eq. 4**), we formulate the single **arm reward**\(r_{k,i}=h(^{(k-1)}[_{k,i}])=1-_{ (),_{}} ;(,^{(k-1)}[ _{k,i}])\) for arm \(_{k,i}\). Since it is impractical to calculate the arm reward by enumerating over \(()\), we sample a batch of validation tasks \(_{k}^{}\) to derive the unbiased reward approximation, denoted by

\[_{k,i}=1-^{}|}_{ ^{}_{k}^{}}D_{ ^{}}^{q};(^{},^{(k-1 )}[_{k,i}]).\] (8)

Here, we adopt the single-step inner-loop optimization [19; 40] to derive \((_{k,};^{(k-1)}[_{k,i}])\) in **Eq. 8**, in order to save the computational cost in practice. Under the few-shot settings, the computation of arm rewards is efficient, since the support set is generally small for inner-loop optimization. The approximation error here can be bounded by the concentration inequality, as the validation tasks \(_{k}^{}\) are sampled from \(()\).

On the other hand, to formulate the **exploration score**\(e_{k,i}\) (i.e., the label for \(f_{2}(;_{2})\)), we consider two separate exploration objectives: (1) the prediction uncertainty for the exploitation module \(f_{1}(;_{1})\), which is \(r_{k,i}-f_{1}(_{k,i}^{q};_{1})\); (2) the validation loss of the meta-model, which represents the difficulty of adapting to \(_{k,i}\), inspired by the "task difficulty measure" in Curriculum Learning [52; 50]. As a result, with \(_{k,i}=(D_{k,i}^{q};_{k,i}^{(J)})\) being the validation loss of arm \(_{k,i}\), we formulate the exploration score as \(e_{k,i}=r_{k,i}-f_{1}(_{k,i}^{q};_{1}^{( k-1)})+(1-)_{k,i}\). Analogously, with the approximated reward \(_{k,i}\) (**Eq. 8**), we calculate the exploration score approximation by

\[_{k,i}=_{k,i}-f_{1}(_{k, i}^{q};_{1}^{(k-1)})+(1-)_{k,i}.\] (9)

Here, the exploration coefficient \((0,1]\) (in **Eq. 7**) is also used to balance our two exploration objectives, which are (1) prediction uncertainty \(r_{k,i}-f_{1}(;_{1})\): if the exploitation model \(f_{1}(;_{1})\) is under-estimating the arm reward, leading to the positive residual \(r_{k,i}-f_{1}(;_{1})\), we will have a high exploration score to enhance the exploration for this arm; otherwise, when \(r_{k,i}-f_{1}(;_{1})\) is negative, it indicates an excessively high estimation, which will alternatively lower the exploration score to compensate for the over-estimation. With a higher \(\) value, our exploration strategy will focus more on the behavior of the exploitation model \(f_{1}()\); (2) the difficulty of task adaptation (i.e., validation loss) \(_{k,i}\): if the current meta-model does not generalize well to arm \(_{k,i}\), the validation loss \(_{k,i}\) will be high, which will also lead to a high exploration score. In this way, our formulation considers two different exploration objectives as well as the dynamics of meta-optimizations (base on concatenated network gradients), for a refined exploration strategy.

### Updating Bandit Scheduler Parameters

After updating the meta-parameters (Line 14, **Algo. 1**, **Remark 2**) with tasks \(_{k}=\{_{k,}\}_{}_{k}}\), we proceed to update the parameters of BASS (Line 15, **Algo. 1**). Recall that \(f_{1}(;_{1})\) tries to learn the reward mapping function \(h()\), and \(f_{2}(;_{2})\) aims to learn the exploration score. Given the selected arms \(_{k}\), with \(_{1}^{},_{2}^{}^{+}\) being the learning rates, we apply the GD and quadratic loss to update the parameters of BASS, denoted by \(_{1}^{(k)}=_{1}^{(k-1)}-_{1}^{} _{_{1}}_{_{k,} _{k}}f_{1}(_{k,}^{q};_{1}^{(k-1 )})-_{k,}^{}^{2},\)\(_{2}^{(k)}=_{2}^{(k-1)}-_{2}^{}_{_{2}}_{_{k,}_{k}} f_{2}[_{}f_{1}(_{k,}^{s}); _{}f_{1}(_{k,}^{q}]);_{2}^{(k- 1)}-_{k,}^{}^{2}\). We refer to **Eqs. 8-9** for calculating the approximated arm reward \(_{k,}\) and exploration score \(_{k,}\).

## 5 Theoretical Analysis

Recall that in each iteration \(k[K]\), we receive candidate arms (tasks) \(_{}^{(k)}=\{_{k,i}\}_{i}\), and each arm \(_{k,i}\) is associated with two context vectors \(_{k,i}^{s},_{k,i}^{q}\). For the sake of analysis, we normalize these two contexts such that \(\|_{k,i}^{k}\|_{2}=\|_{k,i}^{q}\|_{2}=1\), and set the exploration coefficient \(=1\). Following the existing work [47; 48], we let the meta-model \((;)\) be a \(L_{}\)-layer FC network with Gaussian Initialization, with the network width \(m_{}\). Note that our results can also be generalized to other network architectures, such as CNN and ResNet , based on the analysis of over-parameterized neural networks [11; 48; 3]. For the theoretical analysis, we adopt Sigmoid activation for \(f_{1}\) and ReLU for \(f_{2}\), in order to make \(f_{1}\) Lipschitz smooth under over-parameterization settings. Then, we draw trained parameters of BASS with \(\{_{1}^{(k)},_{2}^{(k)}\}\{}_ {1}^{()},}_{2}^{()}\}_{[k]}\). Here, starting from the randomly initialized parameters \(\{_{1}^{(0)},_{2}^{(0)}\}\), each parameter pair \(\{}_{1}^{()},}_{2}^{()}\},[k]\) is separately trained on past arm rewards \(\{_{r^{},}\}_{^{}[]; _{r^{}}}^{}\) and exploration scores \(\{e_{r^{},}\}_{^{}[]; _{r^{}}}^{}\) with \(J_{}\)-iteration GD. Next, similar to existing neural bandit works (e.g., [54; 9; 53]) and the works on meta-model convergence analysis (e.g., [47; 48]), we have the following separateness assumption.

**Assumption 5.1** (\(\)-Separateness).: _After \(K\) meta-training iterations, for every pair of arm contexts \(_{k,i}^{q},_{k^{},i^{}}^{q}\) with \(k,k^{}[K]\) such that the corresponding arms \(_{k,i}_{k}_{k^{},i^{}} _{k^{}}\), if \((k,i)(k^{},i^{})\), we have \(\|_{k,i}^{q}-_{k^{},i^{}}^{q}\|_{2}\) where \(0<()\)._

The assumption above is mild because of two main reasons: (1) since \(L\) is manually chosen (e.g., \(L=2\)), we can easily satisfy the condition \(0<()\) as long as no two arm contexts are identical; (2) since the meta-parameters \(^{(k)},k[K]\) are constantly changing, the corresponding arm contexts will also be distinct across different meta-training iterations. Additional discussions on this assumption are in Appendix B. With standard settings of over-parameterized neural networks [47; 3; 11] and the definition of regret \(R(K)\) in Eq. 5, we have the following **Theorem**5.2.

**Theorem 5.2**.: _Define \((0,1)\), \(0<_{1},_{2}(1/K)\), \(_{f}=\{_{1},_{2}\}\), \(0<(1/L)\), \(c_{}>0\), \(_{L}=(c_{})^{L}\). Suppose the network width \(m(K,L,N_{},^{-1})(1/) ;m_{}(K,L_{},N_{ })(1/)\). Then, let the learning rates be \(_{1},_{2}=}^{-1}}{(K,N_{ },L_{})};\)\(_{}^{1},_{}^{2}=}{(K,N_{},L)} J_{}= (K,N_{},L)}{^{2}}( })\). Following **Algo.** 1, with probability at least \(1-\), the \(K\)-round \(R(K)\) of BASS could be bounded by_

\[R(K)}(}+}+(1+2_{1}))+(^ {2}KJB}}}{}}})+_{m}\] (10)

_where \(_{m}=(}}),c_{}>1\), and \(_{1}=(1)\) with sufficient network width \(m\) of BASS._

The proof is presented in Appendix C. Here, the first term on the RHS is scaled by the \(1/\) term, which means the regret bound will shrink along with more iterations \(K\). The second term on the RHS is scaled by \(1/}}\), which makes it a diminutive term under the over-parameterization settings. Since the network depth \(L\) of BASS is a small integer (we apply \(L=2\) for experiments in Section 6), \(_{L}\) will also be a relatively small constant. Meanwhile, \(_{m}\) will also decrease significantly with increasingly large network width \(m\) of BASS. In contrast, with a convex loss function (e.g., \(L_{2}\) loss or cross-entropy loss) and the same over-parameterization settings, the regret upper bound of the _uniform sampling_ strategy [19; 40] can possibly scale up to \(1\) for the worst-case scenario, and the upper bound will not decrease with more iterations \(K\) (Appendix C.13). Alternatively, BASS works under the bandit settings, by directly measuring the meta-model performance difference w.r.t. the chosen task batch and the optimal one. With more iterations \(K\), BASS tends to make more accurate scheduling decisions, which makes our regret bound possible. For the existing works,  prove that they can improve the optimization landscape with the assumed correlation of task difficulties and meta-model generalization ability.  show that their self-paced strategy can improve the model robustness when facing noisy training tasks. Different from previous works, we provide the performance guarantee for the proposed BASS under the neural bandit framework.

## 6 Experiments

In this section, we compare BASS against seven strong baselines, including: (1) Uniform Sampling; non-adaptive self-paced methods and task schedulers (2) SPL , (3) Focal-Loss (FOCAL) , (4) DAML , (5) GCP , (6) PAML ; and the adaptive task scheduler (7) ATS . Since GCP is not originally compatible with our problem settings and can only work with classification problems, we properly adapt it by choosing the tasks with the highest probabilities, and apply it on the classification data sets. ANIL  is adopted as the backbone meta-learning framework. Due to page limit, we include the complementary experiments (e.g., parameter study for \(\), effects of different levels of task skewness), and the configurations to Appendix **Section** A.

### Real Data Sets with Noisy Meta-Training Tasks

We adopt Drug , Mini-ImageNet (M-ImageNet)  and CIFAR-100 (CIFAR)  data sets under the few-shot learning scenario. Similar to , we apply classification accuracy as the evaluation criterion for the Mini-ImageNet and CIFAR-100 data sets, and consider the squared Pearson coefficient for the Drug data set. For each meta-learning iteration \(k[K]\), the learner is given a candidate pool of 10 tasks (i.e., \(|_{}^{(k)}|=N_{}=10\)), and it will need to choose a batch of \(B=2\) tasks as the training tasks \(_{k}\) for this iteration. For the Mini-ImageNet and CIFAR-100 data sets, we consider half of the meta-training tasks are perturbed by the label flipping noise , where the chance of a label being flipped is \(\). As the Drug data set stands for a regression problem, we draw the label noise from the Gaussian distribution \(N(0,^{2})\). The experimental settings are under 1-shot or 5-shot, 5-Way (for classification data sets) learning scenario with the noise level \(=0.5\). The experiment results are shown in **Table 1** and **Figure 3**. For the average ranking column, we exclude results from the M-ImageNet (1) setting, since BASS and the baselines tend to train a meta-model performing "random guessing". More discussions are in the next paragraph.

Here, BASS can generally outperform the baselines by directly learning the mapping from the meta-parameters to the arm rewards, as well as balancing the exploitation and exploration. ATS also achieves good performance as it adaptively learns the correlation between the task adaptation difficulty and task scheduling, which proves that it is necessary to apply the adaptive scheduling strategy instead of staying with a fixed protocol. Meanwhile, BASS can generally train the meta-model more efficiently (**Figure 3**), leading to good performances at the early stage of meta-training. In particular, for the Mini-ImageNet data set under the 1-shot, 5-way settings, all the algorithms fail to train an effective meta-model. Here, under the 5-way classification scenario, all the methods will likely generate a meta-model performing "random guessing" (around \(20\%\) accuracy). In this case, utilizing Ensemble Inference techniques [12; 16] can help alleviate this problem, and we include further discussion in the case study (**Subsec.**6.3).

### Effects of the Skewed Task Distribution

The skewed task distribution \(()\) commonly exists in real-word cases. For instance, consider an animal image classification data set where each class (i.e., task) corresponds to one kind of animals. In this case, field classes can be considered as "frequent" tasks in the task distribution due to their large quantity and strong mutual correlations, compared with "tail" tasks like kangaroo classes. In this case, paying insufficient attention to the "tail" classes can impair the

   Algo. \(\) Data & Drug (1) & Drug (5) & M-ImageNet (1) & M-ImageNet (5) & CIFAR (1) & CIFAR (5) & **Avg. Rank** \\  Uniform & 0.210\(\)0.013 & 0.220\(\)0.001 & _0.201\(\)0.002_ & 0.301\(\)0.025 & 0.234\(\)0.029 & 0.526\(\)0.011 & 4.4 \\ SPL & **0.244\(\)0.008** & 0.236\(\)0.004 & _0.203\(\)0.002_ & 0.240\(\)0.018 & 0.200\(\)0.002 & 0.367\(\)0.039 & 5.0 \\ FOCAL & 0.222\(\)0.024 & 0.223\(\)0.003 & _0.200\(\)0.000_ & 0.316\(\)0.029 & 0.231\(\)0.024 & 0.485\(\)0.006 & 4.4 \\ DAML & 0.146\(\)0.009 & 0.177\(\)0.003 & _0.201\(\)0.001_ & 0.310\(\)0.016 & 0.247\(\)0.003 & 0.414\(\)0.025 & 5.4 \\ GCP & N/A & N/A & _0.201\(\)0.001_ & 0.282\(\)0.016 & 0.243\(\)0.007 & 0.508\(\)0.009 & 6.0 \\ PAML & 0.192\(\)0.020 & 0.205\(\)0.009 & _0.199\(\)0.001_ & 0.218\(\)0.013 & 0.199\(\)0.004 & 0.316\(\)0.022 & 7.2 \\ ATS & 0.230\(\)0.002 & 0.237\(\)0.014 & _0.201\(\)0.001_ & 0.334\(\)0.053 & 0.257\(\)0.048 & 0.515\(\)0.015 & 2.4 \\ 
**BASS** & 0.242\(\)0.012 & **0.245\(\)0.006** & _0.198\(\)0.004_ & **0.351\(\)0.012** & **0.272\(\)0.025** & **0.553\(\)0.008** & **1.2** \\   

Table 1: Results on real data sets [data set (shot); results \(\) standard deviation]. _For 1-shot M-ImageNet, all the methods end up with an invalid meta-model performing "random guessing"._

Figure 3: Accuracy results (5-shot, \(=0.5\)). BASS can achieve a good performance at early meta-training stage.

generalization performance of the trained meta-model. Thus, to investigate the effects of when the task distribution \(()\) is skewed, we randomly choose some tasks from \(()\), and assign them with higher sampling probabilities (weights). This corresponds to the situation when \(()\) is skewed, so that sampling from \(()\) will likely lead to similar tasks. Here, with the CIFAR-100 data set, we sample 10 tasks and assign them with higher sampling probabilities (weights) (\(5\) tasks with \(10\%\), 5 tasks with \(5\%\)), while the rest of the tasks equally share the remaining \(25\%\) probability.

### Case Study: BASS-aided Ensemble Inference

From **Figure**3, we notice that BASS can train a meta-model that achieves good generalization performance at the early stage of meta-training. One application of this property is using BASS to assist meta-learning models under the Ensemble Inference settings, where separate models are combined to enhance the generalization ability. One renowned ensemble approach is the model-parameter ensemble . With a collection of \(N_{E}\) individual models \(\{(;_{i})\}_{i[N_{E}]}\) of the same architecture, the ensemble model will be \(_{E}(;_{E})\), and its parameters \(_{E}=}_{i[N_{E}]}_{i}\) are the averaged parameters across individual models. Then, the ensemble model \(_{E}(;_{E})\) will be applied as the inference model for downstream problems. Here, one natural way of obtaining the individual models \((;_{i}),i[N_{E}]\) is deeming the models from different training iterations as the individual models for ensemble . Here, we conduct experiments using the ensemble techniques with individual models \((;_{i})\) trained by baselines and BASS. We choose the top \(N_{E}=10\) models with the smallest validation loss across different meta-training iterations as the individual models \(\{(;_{i})\}_{i[N_{E}]}\) for ensemble. The results are shown in **Table**3. We label the ensemble version of BASS as "BASS-E", and the non-ensemble version as "BASS-S".

Compared with the non-ensemble settings (**Table**1), we see that the BASS-aided ensemble model can generally perform better. In particular, the ensemble model can improve the meta-model inference performance in significantly difficult cases, such as the Mini-ImageNet under the 1-shot setting (**Subsec.**6.1). As a result, BASS can help generate high-quality ensemble model with the insufficient knowledge problem at the early stage of meta-training, as well as plan for the future meta-training iterations with the adaptive exploration strategy. We include both theoretical analyses and a comprehensive set of experiments to demonstrate the effectiveness of our proposed framework as well as its key properties.

Figure 4: Average weights of chosen meta-training tasks. The testing accuracy vs. iterations needed. BASS can actively exploring for “tail” tasks, and requires much fewer iterations for the same performance (as few as \( 1/3\) of baselines’ iterations).

   Align \(\) Data & M-ImageNet (1) & M-ImageNet (5) & CIFAR (1) & CIFAR (5) \\  Uniform & 0.231\(\)0.014 & 0.313\(\)0.027 & 0.270\(\)0.014 & 0.534\(\)0.012 \\ SPL & 0.218\(\)0.006 & 0.298\(\)0.004 & 0.219\(\)0.005 & 0.363\(\)0.038 \\ FOCAL & 0.204\(\)0.005 & 0.347\(\)0.030 & 0.235\(\)0.015 & 0.499\(\)0.003 \\ DAML & 0.222\(\)0.001 & 0.326\(\)0.031 & 0.261\(\)0.008 & 0.432\(\)0.019 \\ GCP & 0.226\(\)0.006 & 0.297\(\)0.011 & 0.268\(\)0.019 & 0.512\(\)0.015 \\ PAML & 0.213\(\)0.024 & 0.232\(\)0.009 & 0.232\(\)0.009 & 0.336\(\)0.029 \\ ATS & 0.202\(\)0.002 & 0.334\(\)0.052 & 0.313\(\)0.081 & 0.517\(\)0.017 \\ 
**BASS-S** & 0.198\(\)0.004 & 0.351\(\)0.012 & 0.272\(\)0.025 & **0.553\(\)0.008** \\
**BASS-E** & **0.242\(\)0.004** & **0.366\(\)0.003** & **0.327\(\)0.010** & 0.551\(\)0.004 \\   

Table 3: Ensemble case study [dataset (shot); results \(\) standard deviation].