# RealCompo: Balancing Realism and Compositionality Improves Text-to-Image Diffusion Models

Xinchen Zhang\({}^{1}\) Ling Yang\({}^{2}\)1 Yaqi Cai\({}^{3}\) Zhaochen Yu\({}^{2}\) Kai-Ni Wang\({}^{4}\)

Jiake Xie\({}^{5}\) Ye Tian\({}^{2}\) Minkai Xu\({}^{6}\) Yong Tang\({}^{5}\) Yujiu Yang\({}^{1}\) Bin Cui\({}^{2}\)

\({}^{1}\)Tsinghua University \({}^{2}\) Peking University \({}^{3}\) University of Science and Technology of China

\({}^{4}\) Southeast University \({}^{5}\) LibAI Lab \({}^{6}\) Stanford University

https://github.com/YangLing0818/RealCompo

###### Abstract

Diffusion models have achieved remarkable advancements in text-to-image generation. However, existing models still have many difficulties when faced with multiple-object compositional generation. In this paper, we propose _RealCompo_, a new _training-free_ and _transferred-friendly_ text-to-image generation framework, which aims to leverage the respective advantages of text-to-image models and spatial-aware image diffusion models (e.g., layout, keypoints and segmentation maps) to enhance both realism and compositionality of the generated images. An intuitive and novel _balancer_ is proposed to dynamically balance the strengths of the two models in denoising process, allowing plug-and-play use of any model without extra training. Extensive experiments show that our RealCompo consistently outperforms state-of-the-art text-to-image models and spatial-aware image diffusion models in multiple-object compositional generation while keeping satisfactory realism and compositionality of the generated images. Notably, our RealCompo can be seamlessly extended with a wide range of spatial-aware image diffusion models and stylized diffusion models.

## 1 Introduction

The field of diffusion models has witnessed exciting developments and significant advancements recently. Among various generative tasks, text-to-image (T2I) generation  has gained considerable interest within the community. T2I diffusion models such as Stable Diffusion , Imagen  and DALL-E 2/3  have exhibited powerful capabilities in generating images with high aesthetic quality and realism . However, they often struggle to align accurately with the compositional prompt when it involves multiple objects or complex relationships , which requires the model to have strong spatial-aware ability.

One potential solution to optimize the compositionality of generated images is providing a spatial-aware condition to control diffusion models , such as layout/boxes , keypoint/pose  and segmentation map . These spatial-aware conditions are fundamentally similar in functioning, thus we mainly focus our analysis on layout-to-image (L2I) models for simplicity. With the control of layout, L2I models  improve compositionality by generating objects at specified locations. For instance, GLIGEN  designs trainable gated self-attention layers to incorporate layout input and controls the strength of its incorporation by changing parameter \(\). Although L2I models improve the weaknesses of compositional text-to-image generation, their generated images exhibit a significant decline in realism compared to T2I models .

We conducted experiments to analyze why a significant decrease in image realism exists. We analyze the layout injection mechanism in GLIGEN  by controlling the density of layout through parameter \(\). As shown in Fig. 1 (a) and (c), our experiments indicate that the density of layout directly influences the realism of generated images. As the control of layout gradually increases, the generated images become less aesthetic and more unstable. This demonstrates that layout and text, as different control conditions, guide the model towards different generation directions, with the former emphasizing compositionality and the latter emphasizing realism. To alleviate this issue, some models  leverage the early-stage localization capability of diffusion models  and incorporate layouts only during the initial denoising phase. In the later denoising stage, only use text to balance image realism. However, we found this approach yielded minimal effectiveness. We assumed \(=1\) in the first \(t\) denoising steps and \(=0\) in the subsequent denoising steps. As shown in Fig. 1 (b), the object's position is already determined around \(20\) steps. However, it is common that the generated images exhibit almost no difference between \(t=20\) and \(t=50\). This suggests that even when the injection of layout is stopped in the later denoising stages, the control of text alone still fails to alleviate the poor realism issue. The trade-off between realism and compositionality in T2I and L2I models is challenging yet necessary.

To this end, we introduce a general _training-free_ and _transferred-friendly_ text-to-image generation framework _RealCompo_, which utilizes a novel _balancer_ to achieve dynamic equilibrium between realism and compositionality in generated images. We first utilize LLMs to generate scene layouts from text prompt through in-context learning . Then we propose an innovative _balancer_ to dynamically compose pre-trained fidelity-aware (T2I, stylized T2I) and spatial-aware (e.g., layout, keypoint, segmentation map) image diffusion models. This balanced automatically adjusts the coefficient of the predicted noise for each model by analyzing their cross-attention maps during the denoising stage. By combining the respective strengths of the two models, it achieves a trade-off between realism and compositionality. Finally, we extend RealCompo to various spatial-aware conditions through a general compositional denoising process. Moreover, by changing the T2I model to a stylized T2I model, Realcompo can seamlessly achieve compositional generation specified with a particular style. These dramatically demonstrate the great generalization ability of RealCompo. Although there exist methods  for composing multiple diffusion models, their application lacks flexibility because they require additional training and cannot be generalized to other conditionss and models. Our method effectively composes two models in a training-free manner, allowing for a seamless transition between various models.

To the best of our knowledge, RealCompo effectively achieves a trade-off between realism and compositionality in text-to-image generation. Choosing one (stylized) T2I model and one spatial-aware (e.g., layout, keypoint, segmentation map) image diffusion model, RealCompo automatically balances their fidelity and spatial-awareness to realize a collaborative generation. We expands the family of model ensembling/checkpoint merging techniques, which are extensively used in the diffusion community. We believe RealCompo opens up a new research perspective in controllable and compositional image generation.

Our main contributions are summarized as the following:

* We introduce a new _training-free_ and _transferred-friendly_ text-to-image generation framework RealCompo, which enhances compositional text-to-image generation by balancing the realism and compositionality of generated images.

Figure 1: **Motivations of RealCompo**. **(a)** and **(c)** The realism and aesthetic quality of generated images become poor as more layout is incorporated. **(b)** Even if layout is incorporated only in the early denoising stages, the control of text alone still fails to alleviate the poor realism issue. More results are shown in Appendix B.

* We design a novel _balancer_ to dynamically combine the predict noise from T2I model and spatial-aware (e.g., layout, keypoint, segmentation map) image diffusion model.
* RealCompo has strong flexibility, can be generalized to balance various (stylized) T2I models and spatial-aware image diffusion models and can achieve high-quality compositional stylized generation. It provides a fresh perspective for compositional image generation.
* Extensive qualitative and quantitative comparisons with previous outstanding methods demonstrate that RealCompo has significantly improved the performance in generating multiple objects and complex relationships.

## 2 Related Work

Text-to-Image GenerationIn recent years, the field of text-to-image generation has made remarkable progress [47; 60; 36; 18; 11; 74; 63], largely attributed to breakthroughs in diffusion models. By training on large-scale image-text paired datasets, T2I models such as Stable Diffusion (SD) , DALL-E 2/3 [39; 4], MDM , and Pixart-\(\), have demonstrated remarkable generative capabilities. However, there is still significant room for improvement in compositional generation when text prompts include multiple objects and complex relationships . Many studies have attempted to address this issue through controllable generation  by providing additional conditions such as segmentation map , scene graph , layout , etc., to constrain the model's generative direction to ensure the accuracy of the number and position of objects in the generated images. However, due to the constraints of the additional conditions, image realism may decrease . Furthermore, several works [37; 9; 68; 65; 30] have attempted to bridge the language understanding gap in models by pre-processing prompts with Large Language Models (LLMs) [1; 48]. It is challenging for T2I models to achieve trade-off between realism and compositionality  of generated images.

Compositional Text-to-Image GenerationRecently, numerous methods have been introduced to improve compositional text-to-image generation [53; 78; 69; 55; 25; 29]. These methods enhance diffusion models in attribute binding, object relationship, numeracy, and complex prompts. Recent studies can generally be divided into two types : one primarily uses cross-attention maps for compositional generation [31; 24; 76], while the other provides more conditions (e.g., layout, keypoint, segmentation map) to achieve controllable generation [16; 78]. The first methods delve into a detailed analysis of cross-attention maps, particularly emphasizing their correspondence with the text prompt. Attend-and-Excite  dynamically intervenes in the generation process to improve the model's generation results in terms of attribute binding (such as color). Most of the second methods offer layout as a constraint, enabling the model to generate images that meet this condition. This approach directly defines the area where objects are located, making it more straightforward and observable compared to the first type of methods . LMD  provides an additional layout as input with LLMs. Afterward, a controller is designed to predict the masked latent for each object's bounding box and combine them in the denoising process. However, these algorithms are unsatisfactory in the realism of generated images. A recent powerful framework RPG  utilizes Multimodal LLMs to decompose complex generation tasks into simpler subtasks to obtain satisfactory realism and compositionality of generated images. Orthogonal to this work, we achieve dynamic equilibrium between realism and compositionality by combining T2I and spatial-aware image diffusion models.

## 3 Method

In this section, we introduce our method, RealCompo, which designs a novel balancer to achieve dynamic equilibrium between realism and compositionality of generated images. We initially focus on the layout-to-image models. In Section 3.1, we analyze the necessity of incorporating influence for the predictive noise of each model and provide a method for calculating coefficients. In Section 3.2, we provide a detailed explanation of the update rules employed by the balancer, which utilizes a training-free approach to update coefficients dynamically. In Section 3.3, we provide a universal formula and denoising procedure that enable the balance of T2I models with any spatial-aware image diffusion model, such as keypoint or segmentation-to-image models based on ControlNet . We also extend RealCompo to stylized compositional generation by stylized T2I models.

### Combination of Fidelity and Spatial-Awareness

LLM-based Layout Generation.Since spatial-aware conditions are similar essentially, we first choose layout as the representative of spatial-aware condition for introduction. As shown in Fig. 2, we leverage the powerful in-context learning [57; 79] capability of Large Language Models (LLMs) to analyze the input text prompt and generate an accurate layout to achieve "pre-binding" between objects and attributes. The layout is then used as input for the L2I model. In this paper, we choose GPT-4 for layout generation. Please refer to Appendix C.1 for detailed explanation.

Combination of Two Types of Noise.In diffusion models, the model's predicted noise \(_{t}\) directly affects the direction of the generated images. In T2I models, \(_{t}^{}\) exhibits more directive toward realism , whereas in L2I models, \(_{t}^{}\) demonstrates more directive toward compositionality . To achieve the trade-off between realism and compositionality, a feasible but untapped solution is to compose the predicted noise of two models. However, the predicted noise from different models has its own generative direction, contributing differently to the generated results at different timesteps and positions. Based on this, we design a novel balancer that achieves dynamic equilibrium between the two models' strengths at every position \(i\) in the noise for timestep \(t\). This is achieved by analyzing the influence of each model's predicted noise. Specifically, we first set the same coefficient for the predicted noise of each model to represent their influence before the first denoising step:

\[_{T}^{}=_{T}^{}\] (1)

In order to regularize the influence of each model, we perform a softmax operation on the coefficients to get the final coefficients:

\[_{t}^{c}=_{t}^{c})}{(_{t}^{ })+(_{t}^{})}\] (2)

where \(c\{,\}\).

The balanced noise can be derived according to the coefficient of each model:

\[_{t}=_{t}^{}_{t}^{}+_{t}^{}_{t}^{}\] (3)

where \(\) denotes pixel-wise multiplication.

Once the predicted noise \(_{t}^{c}\) and the coefficient \(_{t}^{c}\) of each model are provided, the balanced noise can be derived from Eq. 2 and Eq. 3. At timestep \(t\), the balancer dynamically updates coefficients as described in Section 3.2.

Figure 2: An overview of RealCompo framework for text-to-image generation. We first use LLMs or transfer function to obtain the corresponding layout. Next, the balancer dynamically updates the influence of two models, which enhances realism by focusing on contours and colors in the fidelity branch, and improves compositionality by manipulating object positions in the spatial-aware branch.

### Influence Estimation with Dynamic Balancer

The alignment between the generated images and the input prompts is largely influenced by model's cross-attention maps, which encapsulate a wealth of matching information between visual and textual elements, such as location and shape. Specifically, given the intermediate feature \((_{t})\) and the text embeddings \(_{}(y)\), cross-attention maps can be derived in the following manner:

\[^{c}=((K^{c})^{T}}{^{c}} }),c\{,\}\] (4)

\[Q=W_{Q}(_{t}),\ K=W_{K}_{}(y)\] (5)

where \(Q\) and \(K\) are respectively the dot product results of the intermediate feature \((_{t})\), text embeddings \(_{}(y)\), and two learnable matrices \(W_{Q}\) and \(W_{K}\). \(_{ij}\) defines the weight of the value of the \(j\)-th token on the \(i\)-th pixel. Here, \(j\{1,2,,N(_{}(y))\}\), and \(N(_{}(y))\) denotes the number of tokens in \(_{}(y)\). The dimension of \(K\) is represented by \(d_{k}\).

Update Rule of Dynamic Balancer.We designed a novel balancer that dynamically balances two models according to their cross-attention maps at timestep \(t\). Specifically, we represent layout as \(=\{b_{1},b_{2},,b_{v}\}\), which is composed of \(v\) bounding boxes \(b\). Each bounding box \(b\) corresponds to a binary mask \(_{b}\), where the value inside the box is \(1\) and the value outside the box is \(0\). Given the predicted noise \(_{t}^{c}\) and the coefficient \(_{t}^{c}\) of each model, the balanced noise \(_{t}\) and denoised latent \(_{t-1}\) can be derived from Eq. 3 and Eq. 12. By feeding \(_{t-1}\) into two models, we obtain the cross-attention maps \(_{t-1}^{c}\) output by the two models at timestep \(t-1\), which indicates the denoising quality feedback after the noise \(_{t}^{c}\) of the model at time \(t\) is weighted by \(_{t}^{c}\). Based on \(_{t-1}^{c}\), we define the loss function as follows:

\[(_{t-1}^{},_{t-1}^{}) =_{c}_{b}(1-_{(ij_{k},t-1)}^{c} _{b}}{_{i}_{(ij_{k},t-1)}^{c}})\] (6)

where \(c\{,\}\), \(j_{b}\) denotes the token corresponding to the object in bounding box \(b\). Since two models are controlled by different conditions, averaging the predicted noise equally will lead to instability in the generated images. This is because the T2I model breaks the layout constraints of the L2I model, reducing the compositionality of the generated images, as we have demonstrated in experimmrts in Fig. 9. Therefore, we designed this loss function to measure the alignment between the cross-attention maps and layout for each model. A smaller loss indicates better compositionality. The following rule is used to update \(_{t}^{c}\):

\[_{t}^{c}=_{t}^{c}-_{t}_{_{t}^{c}}(_{t-1}^{},_{t-1}^{})\] (7)

where \(_{t}\) is the updating rate. This update rule continuously strengthens the constraints on both models by assessing the positional alignment of the layout within the cross-attention maps, ensuring the maintenance of the localization capability of L2I model while injecting fidelity information of T2I model. It is worth noting that previous methods [6; 59; 28] for parameter updates based on function gradients were primarily using energy functions to update latent \(_{t}\). We are the first to update the influence of predicted noise based on the gradient of the loss function, which is a novel and stable method well-suited to our task. The complete denoising process is detailed in Appendix C.3.

### Extend RealCompo to any Spatial-Aware Conditions in a General Form

Other spatial-aware text-to-image diffusion models are essentially similar to L2I models. Keypoint-to-image (K2I) models generate specified actions or poses within each group of keypoints region, and segmentation-to-image (S2I) models fill indicated objects within each segmented region. The concept of "region" is always present, which transforms T2I generation from a macro perspective to utilizing region-based control for T2I generation from a micro perspective. This concept is also the core of enhancing image compositionality. Compared with layout-based T2I generation, the only difference is that keypoints and segmentation maps have stronger control over the model based on regions, requiring that the pose is maintained and the object is correct and unique.

General Form for Extension to Other Spatial-Aware ConditionsWe rethink Eq. 6, which is RealCompo's core approach in combining T2I and L2I models, where the only layout-related variable is the binary masks \(\). Considering that spatial-aware controllable T2I generation inherently focus on the concept of "region control", we introduce a transfer function:

\[=f()\] (8)

where \(\) represents other spatial-aware conditions such as keypoint and segmentation map. \(f()\) represents the calculation of the minimum and maximum values of the horizontal and vertical coordinates occupied by each set of keypoints or a segmentation block within the entire image coordinate system, which can be transformed into a layout and a binary mask \(\). Therefore, for any T2I models with spatial-aware control, the general loss function of RealCompo is:

\[(_{t-1}^{},_{t-1}^{})=_{c}_{b}(1-_{(ij_{b},t-1)}^{c} f _{b}()}{_{i}_{(ij_{b},t-1)}^{c}})\] (9)

where \(c\{,\}\). Similarly, \(_{t}^{c}\) is dynamically updated using Eq. 7. ControlNet  enables controllable T2I generation based on various spatial-aware conditions. In this work, the spatial-aware branches besides layout are all based on ControlNet, which is illustrated in Fig. 4. The generated images of keypoint- and segmentation-based RealCompo are shown in Fig. 3.

Extend RealCompo to Stylized Image GenerationAs an essential indicator of fidelity, image style [50; 67] guides us to expand the application potential of RealCompo. Since RealCompo mainly leverages T2I models to enhance and guide the realism and aesthetic quality of generated images. By replacing the T2I model with various stylized T2I models and combining it with a spatial-aware image diffusion model, we can achieve outstanding compositional generation under this style. The experiments are shown in Fig 8.

## 4 Experiments

### Experimental Setup

Implementation DetailsOur RealCompo is a generic, scalable framework that can achieve the complementary advantages of the model with any chosen (stylized) T2I models and spatial-aware image diffusion models. We selected GPT-4  as the layout generator in our experiments, the detailed rules are described in Appendix C.1. For layout-based RealCompo, we chose SD v1.5  and GLIGEN  as the backbone. For keypoint-based RealCompo, we chose SDXL  and

Figure 4: RealCompo constructed on ControlNet.

Figure 3: Extend RealCompo to keypoint- and segmentation-based image generation.

ControlNet  as the backbone. For segmentation-based RealCompo, we chose SD v2.1  and ControlNet  as the backbone. For style-based RealCompo, we chose two stylized T2I models: Coloring Page Diffusion and CuteYukiMix as the backbone, and chose GLIGEN  as the backbone of L2I model. All of our experiments are conducted under 1 NVIDIA 80G-A100 GPU.

Baselines and BenchmarkTo evaluate compositionality, we compare our RealCompo with the outstanding T2I and L2I models on T2I-CompBench . This benchmark test models across aspects of attribute binding, object relationship, numeracy and complexity. To evaluate realism, we randomly select 3K text prompts from the COCO validation set, we utilize ViT-B-32  to calculate the CLIP score and LAION aesthetic predictor to calculate aesthetic score, reflecting the degree of match between generated images and prompts as well as the aesthetic quality, respectively. In addition to objective evaluations, we conducted a user study to evaluate RealCompo and stylized RealCompo in terms of realism, compositionality, and comprehensive evaluation.

### Main Results

Results of Compositionality: T2I-CompBenchWe conducted tests on T2I-CompBench  to evaluate the compositionality of RealCompo compared to the outstanding T2I and L2I models. As demonstrated in Table 1, RealCompo achieved state-of-the-art performance on all seven evaluation tasks. It is clear that RealCompo and L2I models GLIGEN  and LMD+  show significant improvements in spatial-aware tasks such as spatial and numeracy. These improvements are largely

    &  &  &  &  \\    & **Color\(\)** & & & & & & \\  Stable Diffusion v1.4  & 0.3765 & 0.3576 & 0.4156 & 0.1246 & 0.3079 & 0.4461 & 0.3080 \\ Stable Diffusion v2  & 0.5065 & 0.4221 & 0.4922 & 0.1342 & 0.3096 & 0.4579 & 0.3386 \\ Structured Diffusion  & 0.4990 & 0.4218 & 0.4900 & 0.1386 & 0.3111 & 0.4550 & 0.3355 \\ Attn-Extv2  & 0.6400 & 0.4517 & 0.5963 & 0.1455 & 0.3109 & 0.4767 & 0.3401 \\ DALL-E 2  & 0.5750 & 0.5464 & 0.6374 & 0.1283 & 0.3043 & 0.4873 & 0.3696 \\ Stable Diffusion XL  & 0.6369 & 0.5408 & 0.5637 & 0.2032 & 0.3110 & 0.4988 & 0.4091 \\ PixArt-\(\) & 0.6886 & 0.5582 & 0.7044 & 0.2082 & 0.3179 & 0.5058 & 0.4117 \\  GLIGEN & 0.4288 & 0.3998 & 0.3904 & 0.2632 & 0.3036 & 0.4970 & 0.3420 \\ LMD+ & 0.4814 & 0.4865 & 0.5699 & 0.2537 & 0.2828 & 0.5762 & 0.3323 \\ 
**RealCompo (Ours)** & 0.7741 & 0.6032 & 0.7427 & 0.3173 & 0.3294 & 0.6592 & 0.4657 \\   

Table 1: Evaluation results about compositionality on T2I-CompBench . RealCompo consistently demonstrates the best performance regarding attribute binding, object relationships, numeracy and complex compositions. We denote the best score in blue, and the second-best score in green. The baseline data is quoted from PixArt-\(\).

Figure 5: Qualitative comparison between our RealCompo and the outstanding text-to-image model Stable Diffusion v1.5 , as well as the layout-to-image models, GLIGEN  and LMD+ . Colored text denotes the advantages of RealCompo in generated images.

attributed to the guidance provided by the additional conditions, which greatly enhances the model's compositional performance. RealCompo employs a balancer for better control over positioning, boosting its advantages in these aspects. However, the L2I models exhibit a noticeable decline in performance on tasks like texture and non-spatial. This decline is due to the injection of layout embeddings, which dilute the density of text embeddings, leading to suboptimal semantic understanding by the model. By composing additional T2I models, RealCompo provides sufficient textual information during the denoising process and achieves outstanding results in tasks that reflect realism, such as texture, non-spatial and complex tasks. As shown in Fig. 5, compared with the current outstanding L2I models GLIGEN and LMD+, RealCompo achieves a high level of realism while keeping the attributes of the objects matched and the number of positions generated correctly.

Results of Realism: Quantitative ComparisonAs shown in Table 2, our model significantly outperforms existing outstanding T2I and L2I models in both CLIP score and aesthetic score. We attribute this to the dynamic balancer, which enhances image realism and aesthetic quality while maintaining high compositionality.

User StudyIn addition to objective evaluations, we designed a user study to subjectively assess the practical performance of various methods. We randomly selected 15 prompts, including 5 for stylization experiments. Comparative tests were conducted using T2I models, spatial-aware image diffusion models, and RealCompo. We invited 39 users from diverse backgrounds to vote on image realism, image compositionality, and comprehensive evaluation, resulting in a total of 1755 votes. As illustrated in Fig. 6, RealCompo received widespread user approval in terms of realism and compositionality.

Reasonable Composition Improves RealismWe provide examples from the user study in Fig. 7, which demonstrates the advantages of RealCompo over the T2I model in realism. As shown in Fig. 7(a), T2I model generates a teapot that is visibly suspended in the air, which doesn't conform to the physical laws of real-world scenes. In contrast, RealCompo generates objects within reasonable

  
**Model** & **CLIP Score\(\)** & **Aesthetic Score\(\)** \\  Stable Diffusion v1.4  & 0.307 & 5.326 \\ TokenCompose v2.1  & 0.323 & 5.067 \\ Stable Diffusion v2.1  & 0.321 & 5.458 \\ Stable Diffusion XL  & 0.322 & 5.531 \\  Layout Guidance & 0.294 & 4.947 \\ GLIGEN & 0.301 & 4.892 \\ LMD+ & 0.298 & 4.964 \\ 
**RealCompo (Ours)** & 0.334 & 5.742 \\   

Table 2: Evaluation results on image realism.

Figure 6: Results of user study.

Figure 7: Text-to-image models often generate unrealistic images due to unreasonable object positions. Our method improves image authenticity through conditional control while maintaining detail and aesthetic quality.

bounds through layout constraints, ensuring both the aesthetic quality and positional reasonableness. In Fig. 7(b), the red chair generated by the T2I model is unnaturally placed on top of the table, and in Fig. 7(c), two people generated by the T2I model are too close to each other. These examples illustrate that although T2I model outperforms in detail and visual refinement, its positional reasonableness needs improvement. Our method utilizes LLM to generate conditions that comply with physical laws, guiding the model to generate images with both high positional reasonableness and aesthetic quality. Therefore, under similar detail and aesthetic quality, RealCompo's more reasonable composition gives it an advantage over the T2I models in terms of realism.

Results of Extend Applications: More Spatial-Aware ConditionsWe extend RealCompo to more spatial-aware controlled image generation. As shown in Fig. 3, keypoint- and segmentation-based RealCompo achieves outstanding performance in both realism and compositionality. This promising result reveals that as spatial-aware conditions, layout, keypoint, and segmentation map are fundamentally similar, RealCompo focuses on these similarities and achieves a general generative paradigm for compositional generation.

Results of Extend Applications: Stylized GenerationImage style is an essential indicator of fidelity. We experiment with generalizing RealCompo to various pre-trained stylized T2I models. We selected the Coloring Page Diffusion and Cutyukimix as the foundational stylized models, focusing on the coloring page style and adorable style, respectively. As shown in Fig. 8, RealCompo perfectly inherits the style of the T2I models and, with the help of L2I model, achieves powerful compositional generation under these styles, which is currently difficult for stylized diffusion models to accomplish. We found it difficult for LMD to strictly maintain the style by simply replacing the backbone with a stylized model, often leading to text leakage . For example, terms like "crayon" frequently appear in the coloring page style, indicating that the layout control disrupts the style or text control, making it challenging for L2I models to achieve stylized compositional generation. In contrast, by maintaining image realism and style, RealCompo demonstrates strong compositionality while better preserving the style compared to currently outstanding stylized models like InstantStyle .

### Ablation Study

Importance of Dynamic BalancerAs shown in Fig. 9, we conducted experiments on the importance of the dynamic balancer. It is clear that without the use of the dynamic balancer, the generated images do not align with the layout. This is because the predicted noise in T2I model is not constrained by the layout, leading to the model generating the object at any position, and the quantity is uncontrollable. Although the image realism is high, the predicted noise of T2I model disrupts the object distribution of the predicted noise of L2I model, leading to poor compositionality of the generated images and uncontrollable in the generation process.

Generalizing to Different BackbonesTo explore the generalizability of RealCompo for various models, we choose two T2I models, SD v1.5  and TokenCompose , and two L2I models, GLIGEN  and LayGuide (Layout Guidance) . We combine them two by two, yielding four

Figure 8: Extend RealCompo to stylized compositional generation.

versions of RealCompo v1-v4. The experimental results are shown in Fig. 9. The four versions of RealCompo all have a high degree of realism in generating images and achieving desirable results regarding instance composition. This is attributed to the dynamic balancer combining the strengths of T2I and L2I models, and it can seamlessly switch between models because it is simple and requires no training. We also found that RealCompo, when using GLIGEN as the L2I model, performs better than when using LayGuide in generating objects that match the layout. For instance, in the images generated by RealCompo v4 in the first and third rows, "popcms" and "sunflowers" do not fill up the bounding box, which can be attributed to the superior performance of the base model GLIGEN compared to LayGuide. Therefore, when combined with more powerful T2I and L2I models, RealCompo is expected to yield more satisfactory results.

## 5 Conclusion

In this paper, to solve the challenge of complex or compositional text-to-image generation, we propose the SOTA training-free and transferred-friendly framework RealCompo. In RealCompo, we propose a novel balancer that dynamically combines the advantages of various (stylized) T2I and spatial-aware (e.g., layout, keypoint, segmentation map) image diffusion models to achieve the trade-off between realism and compositionality in generated images. In future work, we will continue to improve this framework by using a more powerful backbone and extend it to more realistic applications.