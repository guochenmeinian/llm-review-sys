# Fused Gromov-Wasserstein Graph Mixup for

Graph-level Classifications

 Xinyu Ma\({}^{1}\)   Xu Chu\({}^{2}\)1   Yasha Wang\({}^{1,3}\)   Yang Lin\({}^{1}\)   Junfeng Zhao\({}^{1}\)

**Liantao Ma\({}^{1,3}\)   Wenwu Zhu\({}^{2}\)**

\({}^{1}\)School of Computer Science, Peking University

\({}^{2}\)Department of Computer Science and Technology, Tsinghua University

\({}^{3}\)National Research and Engineering Center of Software Engineering, Peking University

maxinyu@pku.edu.cn, chu_xu@mail.tsinghua.edu.cn

Corresponding Author

###### Abstract

Graph data augmentation has shown superiority in enhancing generalizability and robustness of GNNs in graph-level classifications. However, existing methods primarily focus on the augmentation in the graph signal space and the graph structure space independently, neglecting the joint interaction between them. In this paper, we address this limitation by formulating the problem as an optimal transport problem that aims to find an optimal inter-graph node matching strategy considering the interactions between graph structures and signals. To solve this problem, we propose a novel graph mixup algorithm called FGWMixup, which seeks a "midpoint" of source graphs in the Fused Gromov-Wasserstein (FGW) metric space. To enhance the scalability of our method, we introduce a relaxed FGW solver that accelerates FGWMixup by improving the convergence rate from \((t^{-1})\) to \((t^{-2})\). Extensive experiments conducted on five datasets using both classic (MPNNs) and advanced (Graphormers) GNN backbones demonstrate that FGWMixup effectively improves the generalizability and robustness of GNNs. Codes are available at https://github.com/ArthurLeoM/FGWMixup.

## 1 Introduction

In recent years, Graph Neural Networks (GNNs) [1; 2] have demonstrated promising capabilities in graph-level classifications, including molecular property prediction [3; 4], social network classification , healthcare prediction [6; 7], etc. Nevertheless, similar to other successfully deployed deep neural networks, GNNs also suffer from data insufficiency and perturbation, requiring the application of regularization techniques to improve generalizability and robustness . Data augmentation is widely adopted for regularization in deep learning. It involves creating new training data by applying various semantic-invariant transformations to the original data, such as cropping or rotating images in computer vision , randomly inserting and rephrasing words in natural language [10; 11], etc. The augmented data fortify deep neural networks (DNNs) against potential noise and outliers underlying insufficient samples, enabling DNNs to learn more robust and representative features.

Data augmentation for GNNs requires a unique design due to the distinctive properties of attributed graphs , such as irregular sizes, misaligned nodes, and diverse topologies, which are not encountered when dealing with data in the Euclidean spaces such as images and tabular data. Generally, the augmentation for GNNs necessitates the consideration of two intertwined yet complementary input spaces, namely the graph signal space \(\) and the graph structure space \(\), which are mapped to an aligned latent space \(\) with GNNs. The graph signal space \(\) consists of node features. CurrentGNNs rely on parameterized nonlinear transformations to process graph signals, which can be efficiently encoded and serve as crucial inputs for downstream predictions. Therefore, the augmentation of graph signals is significant for regularizing the GNN parameter space. On the other hand, the graph structure space \(\) consists of information about graph topology. Traditional Message Passing Neural Networks (MPNNs), such as GCN and GIN, perform feature aggregation based on edge connectivity. Great efforts [13; 14; 15; 16] have been further exerted on enhancing the expressive power of GNNs , which carry out new GNN architectures with stronger sensitivity to graph topology compared with MPNNs. Hence, a good augmentation method should also consider the graph structure space.

Recently, huge efforts have been made to design graph data augmentation methods based on the two spaces. Mainstream research [18; 19; 20; 21; 22] considers the augmentation in the graph signal space and the graph structure space **independently**. For instance, ifMixup  conducts Euclidean mixup in the graph signal space, yet fails to preserve key topologies of the original graphs. \(\)-Mixup  realizes graph structure mixup based on the estimated graphons, yet fails to assign semantically meaningful graph signals. In fact, the graph signal and structure spaces are not isolated from each other, and there are strong entangling relations between them . Therefore, **a joint modeling of the interaction between the two spaces is essential for conducting graph data augmentation** (joint modeling problem for short).

Aiming at graph data augmentation and addressing the joint modeling problem, we design a novel graph mixup  method considering the interaction of the two spaces during the mixup procedure. The key idea is to solve a proper inter-graph node matching strategy in a metric space that measures the distance with respect to both graph signals and structures. We propose to compute the distance metric by solving an optimal transport (OT) problem . The OT problem solves the optimal coupling between nodes across graphs in the fused Gromov-Wasserstein metric space , wherein the distance between points takes the interaction between the graph signals and structures into account. Specifically, following , graphs can be modeled as probability distributions embedded in the product metric space \(\). Our objective is to solve an augmented graph that minimizes the weighted sum of transportation distances between the distributions of the source graphs and the objective graph in this metric space. Developing from the Gromov-Wasserstein metric , Fused Gromov-Wasserstein (FGW) distance  has been designed to calculate the transportation distance between two unregistered probability distributions defined on different product metric spaces comprising two components, such as graph signals and structures, which defines a proper distance metric for attributed graphs. In short words, the solved graph is the augmented mixup graph in a space considering the interaction between the graph signals and structures.

However, trivially adopting FGW distance solvers [28; 26] is not scalable to large graph datasets due to a heavy computation burden. The computational bottleneck of FGW-based solvers is the nested triple-loop optimization , mainly due to the polytope constraint for the coupling matrix in the FGW solver. Inspired by , we disentangle the polytope constraint into two simplex constraints for rows and columns respectively, thence executing mirror descent with projections on the two simplexes in an alternating manner to approximate the original constraint. We prove that with a bounded gap with the ideal optimum, we may optimize the entire algorithm into a double-loop structure at convergence rate \((t^{-2})\), improving the \((t^{-1})\) rate of traditional FGW solvers.

In summary, we highlight the contributions of this paper: We address the challenge of enhancing the generalizability and robustness of GNNs by proposing a novel graph data augmentation method that models the interaction between the graph signal and structure spaces. We formulate our objective as an OT problem and propose a novel graph mixup algorithm dubbed FGWMixup that seeks a "midpoint" of two graphs defined in the graph structure-signal product metric space. We employ FGW as the distance metric and speed up FGWMixup by relaxing the polytope constraint into disentangled simplex constraints, reducing the complexity from nested triple loops to double loops and meanwhile improve the convergence rate of FGWMixup. Extensive experiments are conducted on five datasets and four classic (MPNNs) and advanced (Graphormers) backbones. The results demonstrate that our method substantially improves the performance of GNNs in terms of their generalizability and robustness.

## 2 Methodology

In this section, we formally introduce the proposed graph data augmentation method dubbed FGWMixup. We first introduce Fused Gromov-Wasserstein (FGW) distance that presents a distance metric between graphs considering the interaction of graph signal and structure spaces.

Then we propose our optimization objective of graph mixup based on the FGW metric and the algorithmic solutions. Finally, we present our acceleration strategy. In the following we denote \(_{n}:=\{_{+}^{n}|_{i}_{i}=1\}\) as the probability simplex with \(n\)-bins, and \(_{n}()\) as the set of symmetric matrices of size \(n\) taking values in \(\).

### Fused Gromov-Wasserstein Distance

In OT problems, an undirected attributed graph \(G\) with \(n\) nodes is defined as a tuple \((,,)\). \(_{n}\) denotes the probability measure of nodes within the graph, which can be modeled as the relative importance weights of graph nodes. Common choices of \(\) are uniform distributions  (\(=_{n}/n\)) and degree distributions  (\(=[(v_{i})]_{i}/(_{i}(v_{i}))\) ). \(=(^{(1)},,^{(n)})^{}^{n d}\) denotes the node feature matrix with \(d\)-dimensional feature on each node. \(_{n}()\) denotes a matrix that encodes structural relationships between nodes, which can be selected from adjacency matrix, shortest path distance matrix or other distance metrics based on the graph topologies. Given two graphs \(G_{1}=(_{1},_{1},_{1})\) and \(G_{2}=(_{2},_{2},_{2})\) of sizes \(n_{1}\) and \(n_{2}\) respectively, Fused Gromov-Wasserstein distance can be defined as follows:

\[_{q}(G_{1},G_{2})=_{(_{1},_{2}) }_{i,j,k,l}((1-)d(_{1}^{(i)},_{2}^{(j)}) ^{q}+|}(i,k)-}(j,l)|^{q})_{i,j} _{k,l},\] (1)

where \((_{1},_{2}):=\{_{+}^{n_{1} n_{2 }}|_{n_{2}}=_{1},^{}_{n_{1}}=_ {2}\}\) is the set of all valid couplings between node distributions \(_{1}\) and \(_{2}\), \(d(,)\) is the distance metric in the feature space, and \(\) is the weight that trades off between the Gromov-Wasserstein cost on graph structure and Wasserstein cost on graph signals. The FGW distance is formulated as an optimization problem that aims to determine the optimal coupling between nodes in a fused metric space considering the interaction of graph structure and node features. The optimal coupling matrix \(^{*}\) serves as a soft node matching strategy that tends to match two pairs of nodes from different graphs that have both similar graph structural properties (such as \(k\)-hop connectivity, defined by \(\)) and similar node features (such as Euclidean similarity, defined by \(d(,)\)). In fact, FGW also defines a strict distance metric on the graph space \((,,)\) when \(q=1\) and \(\)s are distance matrices, and defines a semi-metric whose triangle inequality is relaxed by \(2^{q-1}\) for \(q>1\). In practice, we usually choose \(q=2\) and Euclidean distance for \(d(,)\) to calculate FGW distance.

Solving FGW DistanceSolving FGW distance is a non-convex optimization problem, whose non-convexity comes from the quadratic term of the GW distance. There has been a line of research contributing to solving this problem.  proposes to apply the conditional gradient (CG) algorithm to solve this problem, and  presents an entropic approximation of the problem and solves the optimization through Mirror Descent (MD) algorithm according to KL-divergence.

### Solving Graph Mixup in the FGW Metric Space

Building upon the FGW distance and its properties, we propose a novel graph mixup method dubbed \(\). Formally, our objective is to solve a synthetic graph \(=(},},})\) of size \(\) that minimizes the weighted sum of FGW distances between \(\) and two source graphs \(G_{1}=(_{1},_{1},_{1})\) and \(G_{2}=(_{2},_{2},_{2})\) respectively. The optimization objective is as follows:

\[_{(_{},^{ d}, }_{}())}(,G_{ 1})+(1-)(,G_{2}),\] (2)

where \(\) is a scalar mixing ratio, usually sampled from a Beta(\(k,k\)) distribution with hyperparameter \(k\). This optimization problem formulates the graph mixup problem as an OT problem that aims to find the optimal graph \(\) at the "midpoint" of \(G_{1}\) and \(G_{2}\) in terms of both graph signals and structures. When the optimum \(^{*}\) is reached, the solutions of FGW distances between \(^{*}\) and the source graphs \(G_{1},G_{2}\) provide the node matching strategies that minimize the costs of aligning graph structures and graph signals. The label of \(\) is then assigned as \(y_{}= y_{G_{1}}+(1-)y_{G_{2}}\).

In practice, we usually fix the node probability distribution of \(\) with a uniform distribution (i.e., \(}=_{}/\)) . Then, Eq.2 can be regarded as a nested bi-level optimization problem, which composes the upper-level optimization _w.r.t._ the node feature matrix \(}\) and the graph structure \(}\)and the lower-level optimization _w.r.t._ the couplings between current \(\) and \(G_{1},G_{2}\) denotes as \(_{1},_{2}\). Inspired by [28; 26; 33], we propose to solve Eq.2 using a Block Coordinate Descent algorithm, which iteratively minimizes the lower-level and the upper-level with a nested loop framework. The algorithm is presented in Alg.1. The inner loop solves the lower-level problem (i.e., FGW distance and the optimal couplings \(_{1}^{(k)},_{2}^{(k)}\)) based on \(}^{(k)},}^{(k)}\) at the \(k\)-th outer loop iteration, which is non-convex and requires optimization algorithms introduced in Section 2.1. The outer loop solves the upper-level problem (i.e., minimizing the weighted sum of FGW distances), which is a convex quadratic optimization problem w.r.t. \(}^{(k)}\) and \(}^{(k)}\) with exact analytical solution (i.e., Line 7,8).

```
1:Input:\(}\), \(G_{1}=(_{1},_{1},_{1})\), \(G_{2}=(_{2},_{2},_{2})\)
2:Optimizing:\(}^{ d},}_{ }(),_{1}(},_{1}),_{2}(},_{2})\).
3:for\(k\) in outer iterations and not converged do:
4:\(^{(k)}:=(},}^{(k)},}^{(k)})\)
5: Solve \(_{_{1}^{(k)}}(^{(k)},G_{1})\) with MD or CG (inner iterations)
6: Solve \(_{_{2}^{(k)}}(^{(k)},G_{2})\) with MD or CG (inner iterations)
7: Update \(}^{(k+1)}}}^{ }}(_{1}^{(k)}_{1}_{1}^{(k)}{}^{}+(1- )_{2}^{(k)}_{2}_{2}^{(k)}{}^{})\)
8: Update \(}^{(k+1)}(1/})_{1}^{(k)}_{1}+(1-)(1/})_ {2}^{(k)}_{2}\)
9:endfor
10:return\(^{(k)},y_{}= y_{G_{1}}+(1-)y_{G_{2}}\) ```

**Algorithm 1**FGWMixup: Solving Eq.2 with BCD Algorithm

### Accelerating FGWMixup

Algorithm 1 provides a practical solution to optimize Eq.2, whereas the computation complexity is relatively high. Specifically, Alg.1 adopts a nested double-loop framework, where the inner loop is the FGW solver that optimizes the couplings \(\), and the outer loop updates the optimal feature matrix \(}\) and graph structure \(}\) accordingly. However, the most common FGW solvers, such as MD and CG, require another nested double-loop algorithm. This algorithm invokes (Bregman) projected gradient descent type methods to address the non-convex optimization of FGW, which involves taking a gradient step in the outer loop and projecting to the polytope-constrained feasible set _w.r.t._ the couplings in the inner loop (e.g., using Sinkhorn  or Dykstra  iterations). Consequently, this makes the entire mixup algorithm a triple-loop framework, resulting in a heavy computation burden.

Therefore, we attempt to design a method that efficiently accelerates the algorithm. There lie two efficiency bottlenecks of Alg.1: 1) the polytope constraints of couplings makes the FGW solver a nested double-loop algorithm, 2) the strict projection of couplings to the feasible sets probably modifies the gradient step to another direction that deviates from the original navigation of the gradient, possibly leading to a slower convergence rate. In order to alleviate the problems, we are motivated to slightly relax the feasibility constraints of couplings to speed up the algorithm. Inspired by [29; 30], we do not strictly project \(\) to fit the polytope constraint \((_{i},_{j}):=\{_{+}^{n_{1} n_{2}}| _{n_{2}}=_{1},^{}_{n_{1}}=_{2}\}\) after taking each gradient step. Instead, we relax the constraint into two simplex constraints of rows and columns respectively (i.e., \(_{1}:=\{_{+}^{n_{1} n_{2}}|_{n_{2}}= _{1}\}\), \(_{2}:=\{_{+}^{n_{1} n_{2}}|^{}_ {n_{1}}=_{2}\}\)), and project \(\) to the relaxed constraints \(_{1}\) and \(_{2}\) in an alternating fashion. The accelerated algorithm is presented in Alg.2, dubbed FGWMixup\({}_{}\).

Alg.2 mainly substitutes Line 5 and Line 6 of Alg.1 with a single-loop FGW distance solver (Line 7-12 in Alg.2) that relaxes the joint polytope constraints of the couplings. Specifically, we remove the constant term in FGW distance, and denote the equivalent metric as \(}(G_{1},G_{2})\). The optimization objective of \(}(G_{1},G_{2})\) can be regarded as a function of \(\), and we name it the FGW function \(f()\) (See Appendix A.1 for details). Then we employ entropic regularization on \(f()\) and select Mirror Descent as the core algorithm for the FGW solver. With the negative entropy \((x)=_{i}x_{i} x_{i}\) as the Bregman projection, the MD update takes the form of:

\[(-_{}f()), _{_{i}}():= _{^{}_{i}}\|^{}-\|,\] (3)

where \(\) is the step size. The subgradient of FGW _w.r.t._ the coupling \(\) can be calculated as:

\[_{}f()=(1-)-4_{1}_{2},\] (4)where \(=(d(_{1}[i],_{2}[j]))_{n_{j} n_{2}}\) is the distance matrix of node features between two graphs. The detailed derivation can be found in Appendix A.2.

Our relaxation is conducted in Line 9 and 11, where the couplings are projected to the row and column simplexes alternately instead of directly to the strict polytope. Although this relaxation may sacrifice some feasibility due to the relaxed projection, the efficiency of the algorithm has been greatly promoted. On the one hand, noted that \(_{1}\) and \(_{2}\) are both simplexes, the Bregman projection _w.r.t._ the negative entropy of a simplex can be extremely efficiently conducted without invoking extra iterative optimizations (i.e., Line 9, 11), which simplifies the FGW solver from a nested double-loop framework to a single-loop one. On the other hand, the relaxed constraint may also increase the convergence efficiency due to a closer optimization path to the unconstrained gradient descent.

We also provide some theoretical results to justify our algorithm. Proposition 1 presents a convergence rate analysis on our algorithm. Taking \(1/\) as the entropic regularization coefficient, our FGW solver can be formulated as Sinkhorn iterations, whose convergence rate can be optimized from \((t^{-1})\) to \((t^{-2})\) by conducting marginal constraint relaxation. Proposition 2 presents a controlled gap between the optima given by the relaxed single-loop FGW solver and the strict FGW solver.

**Proposition 1**.: _Let \((,),(,)\) be Polish probability spaces, and \((,)\) the probability measure on \(\) with marginals \(,\). Let \(_{t}\) be the Sinkhorn iterations \(_{2t}=_{(*,)}H(|_{2t-1}),_{2t+1}=_{(,*)}H(|_{2t})\), where \(H(p|q)=-_{i}p_{i}}{p_{i}}\) is the Kullback-Leibler divergence, and \((*,)\) is the set of measures with second marginal \(\) and arbitrary first marginal (\((,*)\) is defined analogously). Let \(^{*}\) be the unique optimal solution. We have the convergence rate as follows:_

\[H(_{t}|^{*})+H(^{*}|_{t})=(t^{-1}),\] (5) \[H(_{t}|)+H(|_{t})+H(_{t}|)+H(|_{t})= (t^{-2}),\] (6)

Proposition 1 implies the faster convergence of marginal constraints than the strict joint constraint. This entails that with \(t\) Sinkhorn iterations of solving FGW, the solution of the relaxed FGW solver moves further than the strict one. This will benefit the convergence rate of the whole algorithm with a larger step size of \(_{i}^{(k)}\) in each outer iteration.

**Proposition 2**.: _Let \(C_{1},C_{2}\) be two convex sets, and \(f()\) denote the FGW function w.r.t. \(\). We denote \(\) as the critical point set of strict FGW that solves \(_{}f()+_{ C_{1}}+_{ C_{2}}\), defined by: \(=\{ C_{1} C_{2}:0 f()+_{C_{1}}( )+_{C_{2}}()\}\), and \(_{C}()\) is the normal cone to at \(\). The fix-point set \(_{rel}\) of the relaxed FGW solving \(_{,}f()+f()+h()+h()\) is defined by: \(_{rel}=\{ C_{1}, C_{2}:0 f()+( h ()- h())+_{C_{1}}(),\ 0 f( )+( h()- h())+_{C_{2}}()\}\) where \(h()\) is the Bregman projection function. Then, the gap between \(_{rel}\) and \(\) satisfies:_

\[,\ (^{*},^{*})_{rel},\ (+^{*}}{2},):=_{x }\|+^{*}}{2}-x\|/.\] (7)

This bounded gap ensures the correctness of FGWMixup\({}_{*}\) that as long as we select a step size \(=1/\) that is small enough, the scale of the upper bound \(/\) will be sufficiently small to ensure the convergence to the ideal optimum of our algorithm. The detailed proofs of Propositions 1 and 2 are presented in Appendix B.

## 3 Experiments

### Experimental Settings

DatasetsWe evaluate our methods with five widely-used graph classification tasks from the graph benchmark dataset collection TUDataset : NCI1 and NCI109 [36; 37] for small molecule classification, PROTEINS  for protein categorization, and IMDB-B and IMDB-M  for social networks classification. Noted that there are no node features in IMDB-B and IMDB-M datasets, we augment the two datasets with node degree features as in [2; 19; 20]. Detailed statistics on these datasets are reported in Appendix D.1.

BackbonesMost existing works select traditional MPNNs such as GCN and GIN as the backbone. However, traditional MPNNs exhibit limited expressive power and sensitivity to graph structures (upper bounded by 1-Weisfeiler-Lehman (1-WL) test [17; 37]), while there exist various advanced GNN architectures [13; 14; 16] with stronger expressive power (upper bound promoted to 3-WL test). Moreover, the use of a global pooling layer (e.g., mean pooling) in graph classification models may further deteriorate their perception of intricate graph topologies. These challenges may undermine the reliability of the conclusions regarding the effectiveness of graph structure augmentation. Therefore, we attempt to alleviate the problems from two aspects. 1) We modify the READOUT approach of GIN and GCN to save as much structural information as we can. Following , we apply a virtual node that connects with all other nodes and use the final latent representation of the virtual node to conduct READOUT. The two modified backbones are dubbed vGIN and vGCN. 2) We select two Transformer-based GNNs with stronger expressive power as the backbones, namely Graphormer  and Graphormer-GD . They are proven to be more sensitive to graph structures, and Graphormer-GD is even capable of perceiving cut edges and cut vertices in graphs. Detailed information about the selected backbones is introduced in Appendix D.2.

Comparison BaselinesWe select the following data augmentation methods as the comparison baselines. DropEdge  randomly removes a certain ratio of edges from the input graphs. DropNode  randomly removes a certain portion of nodes as well as the edge connections. M-Mixup  conducts Euclidean mixup in the latent spaces, which interpolates the graph representations after the READOUT function. ifMixup  applies an arbitrary node matching strategy to conduct mixup on graph node signals, without preserving key topologies of original graphs. \(\)-Mixup  conducts Euclidean addition on the estimated graphons of different classes of graphs to conduct class-level graph mixup. We also present the performances of the vanilla backbones, as well as our proposed method w/ and w/o acceleration, denoted as FGWMixup\({}_{*}\) and FGWMixup respectively.

Experimental SetupsFor a fair comparison, we employ the same set of hyperparameter configurations for all data augmentation methods in each backbone architecture. For all datasets, We randomly hold out a test set comprising 10% of the entire dataset and employ 10-fold cross-validation on the remaining data. We report the average and standard deviation of the accuracy on the test set over the best models selected from the 10 folds. This setting is more realistic than reporting results from validation sets in a simple 10-fold CV and allows a better understanding of the generalizability . We implement our backbones and mixup algorithms based on Deep Graph Library (DGL)  and Python Optimal Transport (POT)  open-source libraries. More experimental and implementation details are introduced in Appendix D.3 and D.4.

### Experimental Results

Main ResultsWe compare the performance of various GNN backbones on five benchmark datasets equipped with different graph data augmentation methods and summarize the results in Table 1. As is shown in the table, FGWMixup and FGWMixup\({}_{*}\) consistently outperform all other SOTA baseline methods, which obtain 13 and 7 best performances out of all the 20 settings respectively. The superiority is mainly attributed to the adoption of the FGW metric. Existing works hardly consider the node matching problem to align two unregistered graphs embedded in the signal-structure fused metric spaces, whereas the FGW metric searches the optimum from all possible couplings and conducts semantic-invariant augmentation guided by the optimal node matching strategy. Remarkably, despite introducing some infeasibility for acceleration, FGWMixup\({}_{*}\) maintains consistent performance due to the theoretically controlled gap with the ideal optimum, as demonstrated in Prop. 1. Moreover, FGWMixup and FGWMixup\({}_{*}\) both effectively improve the performance and generalizability of various GNNs. Specifically, our methods achieve an average relative improvement of 1.79% on MPNN backbones and 2.67% on Graphormer-based backbones when predicting held-out unseen samples. Interestingly, most SOTA graph data augmentation methods fail to bring notable improvements and even degrade the performance of Graphormers, which exhibit stronger expressive power and conduct more comprehensive interactions between graph signal and structure spaces. For instance, none of the baseline methods improve GraphormerGD on NCI109 and NCI1 datasets. However, our methods show even larger improvements on Graphormers compared to MPNNs, as we explicitly consider the interactions between the graph signal and structure spaces during the mixup process. In particular, the relative improvements of FGWMixup reach up to 7.94% on Graphormer and 2.95% on GraphormerGD. All the results above validate the effectiveness of our methods. Furthermore, we also conduct experiments on large-scale OGB  datasets, and the results are provided in Appendix E.5.

Robustness against Label CorruptionsIn this subsection, we evaluate the robustness of our methods against noisy labels. Practically, we introduce random label corruptions (i.e., switching to another random label) with ratios of 20%, 40%, and 60% on the IMDB-B and NCI1 datasets. We employ vGCN as the backbone model and summarize the results in Table 2. The results evidently demonstrate that the mixup-series methods consistently outperform the in-sample augmentation method (DropEdge) by a significant margin. This improvement can be attributed to the soft-labeling strategy employed by mixup, which reduces the model's sensitivity to individual mislabeled instances and encourages the model to conduct more informed predictions based on the overall distribution of the blended samples. Notably, among all the mixup methods, FGWMixup and FGWMixup\({}_{*}\) exhibit stable and consistently superior performance under noisy label conditions. In conclusion, our methods effectively enhance the robustness of GNNs against label corruptions.

Analysis on Mixup EfficiencyWe run FGWMixup and FGWMixup\({}_{*}\) individually with identical experimental settings (including stopping criteria and hyper-parameters such as mixup ratio, etc.) on the same computing device and investigate the run-time computational efficiencies. Table 3 illustrates

    &  &  &  &  &  &  \\  &  &  &  &  &  &  &  &  &  &  \\  vanilla & 74.93(0.3) & 74.75(26.0) & 76.98(18.7) & 76.91(18.0) & 75.70(18.5) & 75.89(13.5) & 710.34(96.9) & 72.30(34.3) & 49.00(26.4) & 49.47(3.76) \\  & 73.95(20.9) & 74.48(14.6) & 76.42(85.0) & 76.62(42.0) & 75.62(83.0) & 75.77(53.3) & 73.30(32.9) & 73.94(72.6) & 69.49(3.52) \\  & 29.70(28.4) & 74.88(16.7) & 76.42(85.5) & 76.62(42.7) & 75.90(28.5) & 77.57(18.9) & 75.03(18.5) & 73.30(32.9) & 73.95(58.0) & 50.00(3.41) \\  & 29.70(28.4) & 74.88(29.5) & 74.68(19.1) & 77.26(27.1) & 79.30(28.5) & 75.41(19.9) & 75.20(28.5) & 72.04(45.3) & 49.13(12.5) & 49.47(25.6) \\  & 29.70(28.4) & 74.60(28.4) & 77.16(18.7) & 77.26(26.3) & 75.39(18.7) & 76.74(18.5) & 76.34(18.5) & 52.03(38.9) & 72.40(44.5) & 49.73(16.49) \\  & 29.70(28.4) & 74.52(28.8) & 74.61(24.1) & 77.97(18.8) & 75.55(22.5) & 76.38(11.3) & 76.34(18.9) & 72.04(48.5) & 49.73(16.49) & 49.73(16.47) \\  & 29.70(28.4) & 75.01(28.8) & 76.04(21.9) & 77.97(18.8) & 75.55(22.5) & 76.38(11.3) & 77.44(18.9) & 72.04(48.5) & 49.73(16.49) & 49.63(19.5) \\  & 29.70(28.4) & 75.01(28.8) & 76.04(21.9) & 77.97(18.8) & 75.55(22.5) & 76.38(11.3) & 77.03(30.69) & 77.40(28.45) & 49.73(13.49) & 49.60(3.90) \\  & 29.70(28.4) & 75.02(38.0) & 76.03(19.9) & **73.23(26.5)** & **73.27(26.4)** & 76.40(26.1) & **76.79(21.5)** & **73.03(0.69)** & 73.05(12.5) & **49.80(28.5)** & **50.80(40.46)** \\  & 29.70(28.3) & **75.23(29.3)** & 73.27(27.1) & **73.07(24.7)** & **76.40(24.6)** & **76.40(26.0)** & **76.52(1.59)** & **73.50(45.44)** & **74.00(29.09)** & **49.20(38.38)** & **50.47(45.44)** \\    &  \\  &  &  &  &  &  &  &  &  &  &  &  \\   & 75.47(16.1) & 76.02(2) & 61.56(30.7) & 77.92(21.6) & 76.92(21.5) & 65.34(30.4) & 74.93(12.2) & 72.10(56.5) & 71.50(24.0) & 48.87(14.0) & 47.72(29.9) \\  & 72.00(28.5) & 72.15(23.2) & 76.23(29.1) & 76.24(26.4) & 76.52(30.3) & 74.23(72.7) & 71.60(55.8) & 72.30(18.9) & 73.05(18.4) & 48.47(40.8) & 47.67(28.5) \\  & 29.70(28.4) & 75.20(28.3) & 76.28(39.4) & 60.46(28.1) & 78.62(10.45) & 65.37(30.34) & 74.78(20.7) & 71.60(5.8) & 71.30(38.1) & 48.47(40.8) & 47.67(28.38) \\  & 29.70(28.4) & 75.11(38.9) & 74.93(8.3) & 62.31(48.8) & 74.51(45.4) & 65.42(79.7) & 74.61(43.68) & 71.61(10.43) & 71.10(40.3) & 70.54(24.0) & 49.61(42.45) & 48.00(30.35) \\  &the average time spent on the mixup procedures of FGWMixup and FGWMixup\({}_{*}\) per fold. We can observe that FGWMixup\({}_{*}\) decreases the mixup time cost by a distinct margin, providing at least 2.03\(\) and up to 3.46\(\) of efficiency promotion. The results are consistent with our theoretical analysis of the convergence rate improvements, as shown in Proposition 1. More detailed efficiency statistics and discussions are introduced in Appendix E.3.

Infeasibility Analysis on the Single-loop FGW Solver in FGWMixup.We conduct an experiment to analyze the infeasibility of our single-loop FGW solver compared with the strict CG solver. Practically, we randomly select 1,000 pairs of graphs from PROTEINS dataset and apply the two solvers to calculate the FGW distance between each pair of graphs. The distances of the \(i\)-th pair of graphs calculated by the strict solver and the relaxed solver are denoted as \(d_{i}\) and \(d_{i}^{*}\), respectively. We report the following metrics for comparison:

* **MAE**: Mean absolute error of FGW distance, i.e., \(|d_{i}-d_{i}^{*}|\).
* **MAPE**: Mean absolute percentage error of FGW distance, i.e., \(-d_{i}^{*}|}{d_{i}}\).
* **mean-FGW**: Mean FGW distance given by the strict CG solver, i.e., \( d_{i}\).
* **mean-FGW***: Mean FGW distance given by the single-loop solver, i.e., \( d_{i}^{*}\).
* **T-diff**: L2-norm of the difference between two transportation plan matrices (divided by the size of the matrix for normalization).

The results are shown in Table 4. We can observe that the MAPE is only 0.0748, which means the FGW distance estimated by the single-loop relaxed solver is only 7.48% different from the strict CG solver. Moreover, the absolute error is around 0.01, which is quite small compared with the absolute value of FGW distances (~0.21). We can also find that the L2-norm of the difference between two transportation plan matrices is only 0.0006, which means two solvers give quite similar transportation plans. All the results imply that the single-loop solver will not produce huge infeasibility or make the estimation of FGW distance inaccurate.

### Further Analyses

Effects of the Trade-off Coefficient \(\)We provide sensitivity analysis w.r.t. the trade-off coefficient \(\) of our proposed mixup methods valued from {0.05, 0.5, 0.95, 1.0} on two backbones and two

   MAE & MAPE & mean-FGW & mean-FGW* & T-diff \\ 
0.0126(0.0170) & 0.0748(0.1022) & 0.2198 & 0.2143 & 0.0006(0.0010) \\   

Table 4: Infeasibility analysis on the single-loop FGW solver in FGWMixup\({}_{*}\).

    &  &  \\  & 20\% & 40\% & 60\% & 20\% & 40\% & 60\% \\  vanilla & 70.00(5.16) & 59.70(5.06) & 47.90(4.30) & 70.58(1.29) & 61.95(2.19) & 48.25(4.87) \\ DropEdge & 68.30(5.85) & 59.40(5.00) & 50.10(1.92) & 69.51(2.27) & 60.32(2.60) & 49.61(1.28) \\ M-Mixup & 70.70(5.90) & 59.70(5.87) & 50.90(1.81) & 71.53(2.75) & 63.24(2.59) & 48.66(3.02) \\ \(\)-Mixup & 67.50(4.52) & 59.10(4.74) & 49.40(2.87) & 72.46(1.95) & 63.26(4.39) & 50.01(1.26) \\ FGWMixup\({}_{*}\) & 70.10(4.39) & **61.90(6.17)** & 50.80(3.19) & **72.92(1.56)** & 62.99(1.35) & **50.12(3.51)** \\ FGWMixup\({}_{*}\) & **70.80(3.97)** & 61.80(5.69) & **51.00(1.54)** & 72.75(2.29) & **63.55(2.60)** & 50.02(3.38) \\   

Table 2: Experimental results of robustness against label corruption with different ratios.

    &  \\  & PROTEINS & NCI1 & NCI109 & IMDB-B & IMDB-M \\  FGWMixup & 802.24 & 1711.45 & 1747.24 & 296.62 & 212.53 \\ FGWMixup\({}_{*}\) & **394.57** & **637.41** & **608.61** & **85.69** & **74.53** \\ Speedup & 2.03\(\) & 2.67\(\) & 2.74\(\) & 3.46\(\) & 2.85\(\) \\   

Table 3: Comparisons of algorithm execution efficiency between FGWMixup and FGWMixup\({}_{*}\).

datasets. Note that \(\) controls the weights of the node feature alignment and graph structure alignment costs, and \(=1.0\) falls back to the case of GW metric where node features are not incorporated. From the results shown in Table 5, we can observe that: 1) when FGW falls back to GW (\(\) =1), where node features are no longer taken into account, the performance will significantly decay (generally the worst among all investigated \(\) values). This demonstrates the importance of solving the joint modeling problem in graph mixup tasks. 2) \(\)=0.95 is the best setting in most cases. This empirically implies that it is better to conduct more structural alignment in graph mixup. In practice, we set \(\) to 0.95 for all of our reported results.

Effects of Mixup Graph SizesWe investigate the performance of FGWMixup and FGWMixup\({}_{*}\) with various mixup graph sizes (node numbers), including adaptive graph sizes (i.e., weighted average size of the mixup source graphs, \(= n_{1}+(1-)n_{2}\), which is selected as our method) and fixed graph sizes (i.e., the median size of all training graphs, 0.5 \(\) and 2 \(\) the median). The ablation studies are composed on NCI1 and PROTEINS datasets using vGCN backbone as motivating examples, and the results are illustrated in Fig.1. We can observe that the best performance is steadily obtained when selecting graph sizes with an adaptive strategy, whereas the fixed graph sizes lead to unstable results. Specifically, selecting the median or larger size may occasionally yield comparable performances, yet selecting the smaller size can result in an overall performance decay of over 1%. This phenomenon is associated with the graph size generalization problem [47; 48] that describes the performance degradation of GNNs caused by the graph size distributional shift between training and testing data. The fixed strategy may aggravate this problem, particularly for small graphs that struggle to generalize to larger ones. In contrast, the adaptive strategy can potentially combat this distributional shift by increasing the data diversity and reach better test time performance.

Effects of GNN DepthsTo validate the improvement of our methods across various model depths, we evaluate the performance of FGWMixup and FGWMixup\({}_{*}\) using vGCNs equipped with different numbers (3-8) of layers. We experiment on NCI1 and PROTEINS datasets, and the results are illustrated in Fig.2. We can observe that our methods consistently improve the performance on NCI1 under all GCN depth settings by a significant margin. The same conclusion is also true for most cases on PROTEINS except for the 7-layer vGCN. These results indicate a universal improvement of our methods on GNNs with various depths.

Other DiscussionsMore further analyses are introduced in the Appendix, including qualitative analyses of our mixup results (see Appendix E.1), further discussions on \(\)-Mixup (see Appendix E.2), and sensitivity analysis of the hyperparameter \(k\) in Beta distribution where mixup weights are sampled from (see Appendix E.4).

Figure 1: Test performance of our methods with different graph size settings on NCI1 and PROTEINS datasets using vGCNs as the backbone.

    &  &  \\  & \(\)=0.95 & \(\)=0.05 & \(\)=1.0 & \(\)=0.95 & \(\)=0.5 & \(\)=0.05 & \(\)=1.0 \\  vGIN-FGMMixup & 75.02(3.86) & **75.30(2.58)** & 74.86(2.40) & 74.57(2.62) & **78.32(2.65)** & 77.42(1.93) & 77.62(2.37) & 75.91(2.93) \\ vGCN-FGMMixup & **76.01(3.19)** & 75.47(3.56) & 74.93(2.74) & 74.40(3.57) & **78.37(2.40)** & 77.93(1.68) & 78.00(1.00) & 77.27(0.92) \\ vGIN-FGMMixup, & **75.20(3.30)** & 74.57(3.30) & 74.39(3.01) & 73.94(3.86) & **77.27(2.71)** & 77.23(2.47) & 76.59(2.14) & 77.20(1.69) \\ vGCN-FGMMixup, & **75.20(3.03)** & 74.57(3.52) & 74.84(3.16) & 74.66(2.91) & 78.47(1.74) & 77.66(1.48) & **78.93(1.91)** & 77.71(1.97) \\   

Table 5: Experimental results of different \(\) on PROTEINS and NCI1 datasets.

## 4 Related Works

Graph Data AugmentationThere are currently two mainstream perspectives of graph data augmentation for graph-level classifications. A line of research concentrates on graph signal augmentation . Node Attribute Masking  assigns random node features for a certain ratio of graph nodes. IfMixup  conducts Euclidean mixup on node features from different graphs with arbitrary node alignment strategy, whereas it damages the critical topologies (e.g., rings, bipartiblity) of the source graphs. Another line of work focuses on graph structure augmentation . Approaches like Subgraph, DropEdge and GAug conduct removals or additions of graph nodes or edges to generate new graph structures. Graph Transplant and Submix realize CutMix-like augmentations on graphs, which cut off a subgraph from the original graph and replace it with another. \(\)-Mixup  estimates a graph structure generator (i.e., graphon) for each class of graphs and conducts Euclidean addition on graphons to realize structural mixup. However, as GNNs can capture the complex interaction between the two entangled yet complementary input spaces, it is essential to model this interaction for a comprehensive graph data augmentation. Regrettably, current works have devoted little attention to this interaction.

Optimal Transport on GraphsBuilding upon traditional Optimal Transport (OT)  methods (e.g., Wasserstein distance), graph OT allows defining a very general distance metric between the structured/relational data points embedded in different metric spaces, where the data points are modeled as probability distributions. It proceeds by solving a coupling between the distributions that minimizes a specific cost. The solution of graph OT hugely relies on the (Fused) Gromov-Wasserstein (GW)  distance metric. Further works employ the GW couplings for solving tasks such as graph node matching and partitioning , and utilize GW distance as a common metric in graph metric learning frameworks . Due to the high complexity of the GW-series algorithm, another line of research  concentrates on boosting the computational efficiency. In our work, we formulate graph mixup as an FGW-based OT problem that solves the optimal node matching strategy minimizing the alignment costs of graph structures and signals. Meanwhile, we attempt to accelerate the mixup procedure with a faster convergence rate.

## 5 Conclusion and Limitation

In this work, we introduce a novel graph data augmentation method for graph-level classifications dubbed FGWMixup. FGWMixup formulates the mixup of two graphs as an optimal transport problem aiming to seek a "midpoint" of two graphs embedded in the graph signal-structure fused metric space. We employ the FGW distance metric to solve the problem and further propose an accelerated algorithm that improves the convergence rate from \((t^{-1})\) to \((t^{-2})\) for better scalability of our method. Comprehensive experiments demonstrate the effectiveness of FGWMixup in terms of enhancing the performance, generalizability, and robustness of GNNs, and also validate the efficiency and correctness of our acceleration strategy.

Despite the promising results obtained in our work, it is important to acknowledge its limitations. Our focus has primarily been on graph data augmentation for graph-level classifications. However, it still remains a challenging question to better exploit the interactions between the graph signal and structure spaces for data augmentation in other graph prediction tasks, such as link prediction and node classification. Moreover, we experiment with four classic (MPNNs) and advanced (Graphormers) GNNs, while there remain other frameworks that could be taken into account. We expect to carry out a graph classification benchmark with more comprehensive GNN frameworks in our future work.

Figure 2: Test performance of our methods using vGCNs with varying numbers of layers on NCI1 and PROTEINS datasets.