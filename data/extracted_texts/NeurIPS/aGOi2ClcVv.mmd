# Token Highlighter: Inspecting and Mitigating

Jailbreak Prompts for Large Language Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Large Language Models (LLMs) are increasingly being integrated into services such as ChatGPT to provide responses to user queries. To mitigate potential harm and prevent misuse, there have been concerted efforts to align the LLMs with human values and legal compliance by incorporating various techniques, such as Reinforcement Learning from Human Feedback (RLHF), into the training of the LLMs. However, recent research has exposed that even aligned LLMs are susceptible to adversarial manipulations known as Jailbreak Attacks. To address this challenge, this paper proposes a method called **Token Highlighter** to inspect and mitigate the potential jailbreak threats in the user query. Token Highlighter introduced a concept called AffirmationLoss to measure the LLM's willingness to answer the user query. It then uses the gradient of AffirmationLoss for each token in the user query to locate the jailbreak-critical tokens. Further, Token Highlighter exploits our proposed _Soft Removal_ technique to mitigate the jailbreak effects of critical tokens via shrinking their token embeddings. Experimental results on two aligned LLMs (LLaMA-2 and Vicuna-V1.5) demonstrate that the proposed method can effectively defend against a variety of Jailbreak Attacks while maintaining competent performance on benign questions of the AlpacaEval benchmark. In addition, **Token Highlighter** is a cost-effective and interpretable defense because it only needs to query the protected LLM once to compute the AffirmationLoss and can highlight the critical tokens upon refusal.

## 1 Introduction

Large Language Models (LLMs) like GPT-4 , LLaMA-2 , and Vicuna  have demonstrated impressive capabilities in achieving state-of-the-art results in a wide range of natural language processing and generation tasks. With the surging interest and integration into services such as ChatGPT, ensuring the safety and trustworthiness of their output becomes crucial. Techniques such as Reinforcement Learning from Human Feedback (RLHF) have been proven to be effective in aligning LLMs with human values .

Despite advancements in alignment techniques, aligned LLMs have been found to be susceptible to jailbreak attacks, which involve rewriting the malicious query at token-level or prompt-level to bypass and circumvent the safety guardrails of aligned LLMs. A notable example is that a jailbroken LLM would be tricked into giving tutorials on how to cause harm to others, as demonstrated in Figure 1. Different jailbreak attack algorithms  have been proposed recently to automatically construct the jailbreak attacks. Take GCG  as an example, GCG can successfully trick several LLMs to output objectionable responses by simply inserting a universal adversarial suffix.

Since the exposure of jailbreak risks for LLMs, various methods of defending against jailbreak attacks have been explored  and are indeed empirically successful in defendingagainst certain types of jailbreak attacks. However, existing defenses are challenged by three main considerations: **(1)** Some defenses like perplexity filtering (PPL ) showed little effect on interpretable and fluent jailbreak prompts . **(2)** Some detector-based defenses have a high False Positive Rate  and thus would significantly compromise the LLM's performance on benign user queries. **(3)** Some defenses that rely on querying an LLM multiple times , may incur unacceptable inference costs.

Recent works  exposed an observation that successful jailbreaks often succeed in tricking the LLMs to first generate an affirmative response like "Sure, here's...". This motivates us to find the tokens in the jailbreak prompt that are most critical to generating these affirmations, and then mitigate the potential jailbreak threat by reducing the influence of those tokens in the response generation process. Motivated by this thought, we propose **Token Highlighter** to alleviate the threats of jailbreak attacks and avoid the aforementioned limitations of existing defenses. An overview of how Token Highlighter works can be found on the bottom left of Figure 1. Firstly, we define the Affirmation Loss using the loss function of the LLM generating a pre-defined affirmation (we use "Sure, I'd like to help you with this." throughout this paper) to measure the LLM's willingness to respond to the user query. Next, we use the gradient of Affirmation Loss to locate the jailbreak-critical tokens in the user query. Finally, we diminish the influence of these tokens in the response generation process by multiplying the original embeddings of these tokens by a value \(\) between \(0\) and \(1\). We call the operation of multiplying a small value _Soft Removal_, as opposed to directly removing these tokens from the user query, which can be understood as _Hard Removal_ (equivalently, setting \(=0\)). We use _Highlight_ to vividly describe the process of identifying an influential token and then shrinking its embedding. The bottom right of Figure 1 shows that the LLM equipped with **Token Highlighter** can correctly reject the malicious user query owing to soft removals on self-discovered jailbreak-critical prompts.

Empirical results show that **Token Highlighter** can significantly mitigate jailbreak attacks while maintaining the performance of LLMs on benign user queries (see Figure 2). Our comprehensive analysis in Section 4 also underscores Token Highlighter's running efficiency and robustness against adaptive attacks.

Figure 1: Overview of **Token Highlighter**. (a) The top panel illustrates the concept of LLM jailbreaks by presenting examples of two types of jailbreak prompts (token-level jailbreak by GCG  and sentence-level jailbreak by TAP . (b) The bottom left panel explains how Token Highlighter finds the jailbreak-critical tokens and mitigates the potential jailbreak effects. We define a loss function called Affirmation Loss to measure the modelâ€™s willingness to generate affirmative responses to the user query. In step 1, our method selects a set of tokens in the user query that have a large influence on generating the affirmation. In step 2, our method applies _Soft Removal_ on these tokens by shrinking the embeddings of these tokens. We call the user query modified by _Soft Removal_ the _Highlighted User Query_. The bottom right panel demonstrates that Token Highlighter can inspect suspicious tokens and help the LLM to correctly refuse malicious user queries.

We summarize our **main contributions** as follows:

* We propose a jailbreak defense method called Token Highlighter, which uses our proposed AffirmationLoss and _Soft removal_ techniques to reduce potential jailbreak risks by finding and mitigating jailbreak-critical tokens in the user query when generating responses.
* Experiments on 2 aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5), 6 jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Mansyhot, and AIM) [28; 13; 5; 14; 2; 1] and a common LLM performance evaluation benchmark (AlpacaEval  ) demonstrate that Token Highlighter can achieve outstanding performance in defending against various jailbreak prompts while maintaining good utility on benign user queries.
* Token Highlighter is a cost-efficient and interpretable defense. Compared to standard LLM inference, Token Highligter only needs one extra query for the computation of the AffirmationLoss. The highlighted tokens can be used to provide explanations of refusal responses.

## 2 Related Work

**Jailbreak Attacks.** Jailbreak attack methods can be divided into token-level jailbreaks and prompt-level jailbreaks. The seminal work in token-level jailbreaks is GCG , which computes the target LLM's generative loss for an affirmation and then uses the loss's gradients with respect to the one-hot token indicators to find better token choices at each position. Prompt-level jailbreaks try to find a prompt to lure the LLM to respond to the malicious instruction. The prompt can be manually designed or automatically generated. Manually designed prompts, like AIM  and Mansyhot , often involve encapsulating the malicious user instruction into a pre-defined template with a placeholder. Automated prompt-level jailbreak methods often utilize the LLM's feedback to iteratively refine the prompt until the target LLM is successfully jailbroken. AutoDAN  employs the target LLM's generative loss of the target response to design the fitness score of the candidate jailbreak prompt to guide further optimization. PAIR  and TAP  use another two LLMs as the attacker and evaluator respectively. At each iteration, the attacker-generated jailbreak prompt would be rated and commented on by the evaluator model according to the target LLM's response to the attack. Next, the attacker would generate new jailbreak prompts based on the evaluator's comments and ratings, and repeat the above cycle until the jailbreak prompt can get full marks from the evaluator.

**Jailbreak Defenses.** Existing jailbreak defense methods can be divided into detector-based defense, smoothing-based defense, and prompt-engineering-based defense. Detector-based Defense [8; 7] utilizes a detector to distinguish whether the user query is malicious and only the query that could pass the checking of the detector would be sent to query the target LLM. Typical ones of this type of method is PPL , which uses an LLM to compute the perplexity of the input query and rejects those with high perplexity. Smoothing-based Defense, which is motivated by randomized smoothing , transforms the original input query to obtain multiple copies and then aggregates the corresponding responses of the target LLM to give the final response to the original query. The earliest one of this line of work is SmoothLLM , which uses character-based perturbation. Semantic Smoothing  tries to preserve the semantic information when perturbing the user query by using semantic transformations such as summarize, paraphrase, and spell-check. Prompt-engineering-based methods are different from these. In these works [24; 25; 23; 21], prompt engineering techniques are used to defend against jailbreak attacks by either altering the system prompt or embedding the user input into a pre-defined template. Self Reminder  is a representative of this line of work, which alters the system prompt of the LLM to instruct the model to remind itself to engage and reply to the user while maintaining the perspective of being an aligned LLM.

## 3 Methodology and Algorithms

Following the overview in Figure 1, in this section we will introduce how **Token Highlighter** works to inspect and mitigate jailbreak prompts for LLMs. Especially, in Section 3.1, we will introduce the concept of the AffirmationLoss and explain how to utilize this loss to locate the tokens with a high influence on tricking the LLM into the affirmative mode. In Section 3.2, we will introduce what **Token Highlighter** does with _Soft Removal_ to mitigate the potential jailbreak risks in user queries.

### Affirmation Loss Function and Critical Token Set Construction

Recent research [22; 26] found that many successful jailbreak attempts share a common property that they all trick the LLM into generating affirmations like starting with "Sure, here is" at the beginning of their responses. Drawing upon this inspiration, our proposed defense aims to find the tokens that are most critical in forcing the LLM to generate such affirmative responses, decrease their importance in the generation, and thereby resolve the potential jailbreak risks brought by these tokens. To identify these tokens, we propose a new concept called the \(\). Given the target LLM \(T_{}\) parameterized with \(\) and a user query \(q_{1:n}\) (where \(n\) is the number of tokens in this query), we define \(x_{1:n}\) as the embedding matrix of \(q_{1:n}\):

\[x_{1:n}=_{}(q_{1:n}) \]

where \(_{}()\) indicates the embedding layer in \(T_{}\), and \(x_{i}=_{}(q_{1:n})_{i}=_{}(q_{i})\) is the embedding of the \(i^{th}\) token \(q_{i}\) in \(q_{1:n}\).

The \(T_{}\)'s \((x_{1:n},)\) with respect to \(x_{1:n}\) is defined as:

\[(x_{1:n},)=- P_{}(y|x_{1:n}), \]

where \(y=\)"Sure, I'd like to help you with this.", which is our default sentence to represent the \(T_{}\)'s affirmation to answer the question. We then further define the \(\) of each token embedding \(x_{i}\) in \(x_{1:n}\) when generating \(y\) as follows:

\[(x_{i})=\|_{x_{i}} P_{}(y|x_{1:n})\|_{2}, \]

where \(_{x_{i}}\) denotes the gradient operation with respect to \(x_{i}\). Finally, we sort the \(\) metric and select the top-\(n\) tokens to construct the \(\)\(\) of tokens:

\[=n(\{(x_{i}), x_{ i} x_{1:n}\})=\{q_{i}, x_{i}\}. \]

, where \(\) is the highlight percentage and \(n\) means the total number of the tokens we selected.

### Mitigating Jailbreak Effect by _Soft Removal_

With the identified top-influence tokens, one naive idea to mitigate the jailbreak threats brought by the tokens \(\{q_{i}\}\) in \(\) is to directly erase some of them from \(q_{1:n}\), which shares a similar idea with Erase Check . However, prior works [9; 7] found that although directly removing them can effectively reduce the attack success rate of jailbreak prompts, this "hard removal" leads to a considerable drop in the model's performance on processing with benign user queries. To better trade-off the model's performance on benign user queries and the defense effectiveness against jailbreak attacks, we propose _Soft Removal_, which shrinks the embeddings of the candidate tokens in \(\) to decrease \(q_{1:n}\)'s influence on manipulating \(T_{}\) to generate affirmation responses. We call the query processed by _Soft Removal a highlighted user query_. Given a user query \(q_{1:n}\) and its corresponding _highlighted user query_\(q^{}_{1:n}\), we denote the embedding matrix for \(q^{}_{1:n}\) as \(x^{}_{1:n}\). Mathematically, \(x^{}_{1:n}\) is computed as:

\[x^{}_{i}=(q_{i}),q_{i} \\ (q_{i}), \]

with \(\) acting as the soft removal level. For a given input user query \(q_{1:n}\), we define the LLM \(T_{}\)'s native response to it (i.e., when there is no defense) as \(r_{}(q_{1:n}) P_{}(|x_{1:n})\). After deploying our Token Highlighter for \(T_{}\), the response to \(q_{1:n}\) would be replaced as \(r_{}(q_{1:n}) P_{}(|x^{}_{1:n})\).

### Token Highlighter: Inspect and Mitigate Jailbreak Prompts

Based on the technical details of \(\) and _Soft Removal_ in Section 3.1 and Section 3.2, we now formally introduce the **Token Highlighter** framework. At a high level, the proposed method aims to locate the parts of the user query that show signs of jailbreaking, and then mitigate the possible jailbreak threats by suppressing the influence of these suspicious tokens before generating the response. Token Highlighter can be summarized in two steps:

* **Step #1: Critical Token Set Construction.** In this step, we compute the \(\) metric defined by Equation 2 and Equation 3 for each token \(q_{i}\) in the user query \(q_{1:n}\) and construct the Critical Set \(\) using the tokens with the top-\(n\)\(\).

* **Step #2: Token Soft Removal.** In this step, we multiply a value \(\) to the token embedding of each token in the Critical Set \(\), get the embeddings of the highlighted user query \(q^{}_{1:n}\) following Equation 5, and use the \(T_{}\)'s response to \(x^{}_{1:n}\) as the final response to \(q_{1:n}\).

The algorithmic description for our method can be found in Algorithm 1. It can be clearly seen that our defense is quite cost-efficient, as there is only one forward and backward pass of the LLM in Step #1.

```
1:Input: User input query \(q_{1:n}\), Target LLM \(T_{}\) and its token embedding layer embed\({}_{}()\), Highlight Percentage \(\), and the Soft Removal Level \(\ \)
2:
3:Step #1: Critical Token Set Construction.
4:Compute the embedding matrix \(x_{1:n}\) for \(q_{1:n}\) based on Equation 1.
5:Compute the Affirmation Loss\((x_{1:n},)\) for \(x_{1:n}\) based on Equation 2.
6:Compute the Influence\((x_{i})\) for all the \(x_{i}\) in \(x_{1:n}\) based on Equation 3
7:Construct \(\) based on Equation 4
8:
9:Step #2: Token Soft Removal.
10:Get initial embedding for the highlighted user query \(q^{}_{1:n}:x^{}_{1:n}=_{}(q_{1:n})\)
11:for\(q_{i}\); do
12:\(x^{}_{i}= x^{}_{i}\)
13:endfor
14:
15:Output: The LLM's response to \(q_{1:n}\): \(r(q_{1:n}) P_{}(|x^{}_{1:n})\)
```

**Algorithm 1** Token Highlighter

## 4 Performance Evaluation

### Experiment Setup

**Malicious User Queries**. We sampled 100 harmful behavior instructions from AdvBench1 in  as jailbreak prototypes, each of which elicits the target LLM to generate answer for a specified question with harmful contents. We then use various existing jailbreak attack methods to generate jailbreak prompts for them. Specifically, for each harmful behavior instruction, we use GCG  to generate a universal adversarial suffix, use AutoDAN , PAIR , and TAP  to automatically generate a new semantic-preserving instruction, use AIM  to encapsulate it to a manually designed template, and use Mansyshot  to insert multiple faux dialogues between a human user and an AI assistant as the prefix of the original user query, where the user asks malicious queries and the AI assistant responds with affirmations. See Appendix A.3 for more details on generating these jailbreak prompts.

**Utility Evaluation Benchmark**. We tested our method as well as all the defense baselines on AlpacaEval2 to evaluate how these defense methods would affect the target LLM's utility (performance on benign user queries). AlpacaEval is a benchmark to measure how well the responses of a given LLM align with human preferences. In this paper, we select the text-davinci-003's responses to the AlpacaEval questions as a reference and use GPT-4 as a judge to compare the outputs of the target LLM with the reference.

**Aligned LLMs.** We conduct the jailbreak experiments on 2 aligned LLMs: LLaMA-2-7B-Chat  and Vicuna-7B-V1.5 . LLaMA-2-7B-Chat is the aligned version of LLAMA-2-7B. Vicuna-7B-V1.5 is also based on LLAMA2-7B and has been further supervised fine-tuned on 70k user-assistant conversations collected from ShareGPT 3. We use **protected LLM** to represent these two models in the experiments.

**Defense Baselines.** We compare our method with three types of jailbreak defense methods, including (I) detector-based methods: PPL , Erase Check , and Gradient Cuff ; (II) smoothing based methods: SmoothLLM  and Semantic Smoothing ; and (III) prompt-engineering-based methods: Self Reminder . To implement PPL, we use the protected LLM itself to compute the perplexity for the input user query and directly reject the one with a perplexity higher than a threshold in our experiment. For Erase Check, we employ the LLM itself to serve as a safety checker to check whether the input query or any of its erased sub-sentences is harmful. Gradient Cuff, which is a two-stage detection framework, proposed a loss function called Refusal Loss. Gradient Cuff detects jailbreaks by checking the value and gradient norm of Refusal Loss. SmoothLLM and Semantic Smoothing perturb the original input query to obtain multiple copies and then aggregate the protected LLM's responses to generate the final response. Self Reminder converts the protected LLM into a self-remind mode by modifying the system prompt. For Token Highlighter, to demonstrate the effectiveness of the construction of the Critical Set, we also include a new baseline called Random Soft Removal, which does soft removal on randomly selected tokens. For more details on the implementation of these baselines, please refer to Appendix A.5.

**Metrics.** We report the Attack Success Rate (**ASR**) measured by LLaMA-Guard-2  to evaluate each defense against various jailbreak attacks. We also report the **Win Rate** measured on Alpaca Eval to show how the protected LLM's utility is affected. In general, a higher Win Rate and lower ASR indicate a better defense. Details about computing the metrics are given in Appendix A.4.

**Implementation of Token Highlighter.** We use \(=0.25\) in all our experiments for both the two protected LLMs. In terms of \(\), we use \(0.3\) for Vicuna-7B-V1.5 and \(0.5\) for LLaMA2-7B-Chat to keep a balanced trade-off between the Win Rate and the ASR. For the text generation setting, we use temperature \(=0.6\) and top-p parameter \(=0.9\) for both LLaMA2-7B-Chat and Vicuna-7B-V1.5, and adopt Nucleus Sampling. As for the system prompt, we use the default setting provided in the fastchat repository . All our experiments are run on a single NVIDIA A800 GPU with 80G of memory. We run all the experiments with the random seed set to \(100\) to ensure reproducibility.

### Comparison with Existing Methods

We begin by comparing our methods and all the defense baselines, jointly considering the AlpacaEval Win Rate and the Average ASR which is averaged across all six jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Mansshot, and AIM). From Figure 2, we can conclude that our method outperforms all other baselines by showing strong defense against jailbreak attacks and good utility on benign user queries. Though smoothing-based methods like Semantic Smoothing can also achieve comparable or even lower ASR than Token Highlighter, these methods would cause a large drop in the utility of the protected LLM, due to the fact that the perturbations they applied to the original user query may deteriorate the semantic information. For example, SmoothLLM uses meaningless characters to replace some words in the original query. Though Semantic Smoothing tries to preserve the semantic information by using summarization to transform the query, the summarization technique would also affect the semantics of the original query. Detector-based methods like Gradient Cuff and PPL can attain good utility because these methods can limit the False Positive Rate (FPR) to a small value (e.g., 5%) by adjusting the threshold. Erase Check, another detector-based method in which there is no

Figure 2: Performance evaluation on Vicuna-7B-V1.5 (a) and LLaMA2-7B-Chat (b). The horizon axis represents the Attack Success Rate (ASR) averaged over 6 jailbreak attacks, and the vertical axis shows the Win Rate on Alpaca Eval of the protected LLM when the corresponding defense is deployed. Complete results can be found in Appendix A.6.

threshold to be adjusted, cannot attain good utility as it has a large and uncontrollable FPR, as also mentioned in prior works . Self Reminder can maintain a high Win Rate on Vicuna-7B-V1.5 but also show utility degradation on LLaMA-2-7B-Chat.

Our method stands out by having the lowest ASR among all the methods that can keep a high Win Rate. In particular, Token Highlighter decreases the ASR from 0.730 to 0.142 on Vicuna-7B-V1.5 while the best baseline Gradient Cuff can only decrease the ASR to 0.243. Token Highlighter outperforms Gradient Cuff by 20.7% (0.588 vs 0.487) in terms of the ASR reduction. On LLaMA-2-7B-chat, all baselines can make the ASR close to zero, because LLaMA-2 is more difficult to jailbreak. The comparison between Token Highlighter and Random Soft Removal reveals the effectiveness of the construction of the Critical Set using the gradient of the Affirmation Loss. Another notable fact is that Random Soft Removal can also keep the utility almost unchanged compared with when there is no defense. This finding suggests that in terms of maintaining utility, exploring the effect on the values of \(\) and \(\) in soft removal may be more crucial than which tokens are softly removed. More studies on the trade-off between ASR and Win Rate by adjusting \(\) and \(\) are presented in Section 4.3.

The results in Figure 1(a) show that Self Reminder is not effective on Vicuna-7B-V1.5. Since prompt-engineering-based methods can be easily combined with Token Highlighter, we choose to combine our method with Self Reminder by simply replacing the system prompt used in our method with that used in Self Reminder. We call the combined version Self Reminder (TH) and run experiments under varying values of \(\) to see whether Token Highlighter can improve Self Reminder. The results in Table 1 show that Self Reminder (TH) can have a much better performance than the plain Self Reminder in terms of the trade-off between ASR and Win Rate. Specifically, Token Highlighter further decreases the ASR of Self Reminder by \(15.2\%\) (0.362 vs 0.427) while maintaining the \(95.5\%\) win rate of the vanilla Self Reminder (0.653 vs 0.684). Reducing the \(\) from \(0.5\) to a smaller number like \(0.3\) can continually reduce the ASR at the cost of decreased win rate. When \(\) is set to 0.3, the ASR is nearly zero while the win rate can still maintain almost 80% of the vanilla Self Reminder.

### Trade-off Analysis between ASR and Win Rate

Recall that we have two parameters for the Token Highlighter algorithm: the highlight percentage \(\) and the soft removal level \(\). In Figure 3, we report the average **ASR** and the **Win Rate** for various \(\) and \(\). From Figure 3, we can find that the ASR has the same trend as the Win Rate with the changing of \(\) and \(\). Specifically, when \(\) is fixed, a larger value of \(\) would make both the Win Rate and the ASR increase. When \(\) is fixed, larger \(\) would both reduce the ASR and the Win Rate.

This phenomenon can be interpreted as follows. Taking a larger \(\), Token Highlighter would highlight more tokens in the jailbreak prompt, thus improving the chance to mitigate the jailbreak effects. However, in another prospective, highlighting more tokens would decrease the model's utility because more tokens in benign queries would also be highlighted. Taking a smaller \(\) would further suppress the importance of the highlighter tokens in generating responses, thus better at mitigating the jailbreak effects. However, heavier soft removals are more likely to destroy the semantic context of the token embeddings. An extreme case is that the soft removal becomes "hard removal" when \(\) is set to zero.

### Interpretability Analysis of Highlighted Queries

We show Token Highlighter can be used as a post-hoc explanation tool upon LLM refusal, by identifying jailbreak-critical tokens in 3 separate examples from GCG and TAP. Table A1 shows that our Token Highlighter indeed underscores some jailbreak-critical components of the prompt for interpreting jailbreak attempts. For GCG, which is a token-level jailbreak attack that appends an optimized adversarial suffix to the original malicious query, the highlighted tokens are all within the

 
**Defense Method** & \(\) & **ASR \(\)** & **Win Rate \(\)** \\  Self Reminder & NA & 0.427 & 0.684 \\   & 0.5 & **0.362** & **0.653** \\  & 0.4 & 0.248 & 0.599 \\   & 0.3 & 0.023 & 0.536 \\   & 0.2 & 0.015 & 0.328 \\  

Table 1: Performance evaluation of combining Self Reminder and Token Highlighter. \(\) means that larger value is better while \(\) means the opposite.

inserted suffix parts. For TAP, which is a prompt-level jailbreak method that uses an attacker LLM to refine the malicious instruction, Token Highlighter can facilitate the explanation on the strategies the attacker used to rewrite the prompt. For instance, raising the LLM's mismatched generalization by role-playing (e.g., "You are a private investigator..."), story-telling (e.g., content dilution), introducing illusive objectives (e.g., write dialogue, report, etc), and instructing the LLM to begin the response with an affirmation (e.g., "begin with 'Sure, here is guidance on...").

In addition, we also explored some highlighted cases for benign user queries and vanilla malicious queries (w/o jailbreak prompts). We found that the highlighted tokens in these cases are just some words or simply some punctuation marks to represent the interrogative/imperative moods (e.g., "How", "What", "Please", "?" and "."). In summary, for interpretability analysis, we advocate using Token Highlighter to inspect which tokens are more crucial to cause refusal responses by the protected LLM (e.g., "I am sorry, but I cannot..." as shown in Figure 1, bottom right panel), to facilitate the explanation to end users and model developers.

## 5 Conclusion

This paper presents a novel jailbreak defense method called **Token Highlighter**. Token Highlighter can effectively capture the jailbreak-critical components designed by the attacker in the malicious user query and then mitigate their jailbreak effects by applying _Soft Removal_ on these critical tokens. Our extensive experiments on 2 aligned LLMs (LLaMA-2-7b-Chat and Vicuna-7B-V1.5) and 6 jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Mansyhot, and AIM) validate the effectiveness of Token Highlighter over existing defenses by achieving state-of-the-art performance in alleviating jailbreak attacks while maintaining good utility on benign user prompts and low running time cost.