# Scaling Sign Language Translation

Biao Zhang Garrett Tanzer Orhan Firat

Google DeepMind

{biaojiaxing,gtanzer,orhanf}@google.com

###### Abstract

Sign language translation (SLT) addresses the problem of translating information from a sign language in video to a spoken language in text. Existing studies, while showing progress, are often limited to narrow domains and/or few sign languages and struggle with open-domain tasks. In this paper, we push forward the frontier of SLT by scaling pretraining data, model size, and number of translation directions. We perform large-scale SLT pretraining on different data including 1) noisy multilingual YouTube SLT data, 2) parallel text corpora, and 3) SLT data augmented by translating video captions to other languages with off-the-shelf machine translation models. We unify different pretraining tasks with task-specific prompts under the encoder-decoder architecture, and initialize the SLT model with pretrained (m/By)T5 models across model sizes. SLT pretraining results on How2Sign and FLEURS-ASL#0 (ASL to 42 spoken languages) demonstrate the significance of data/model scaling and cross-lingual cross-modal transfer, as well as the feasibility of zero-shot SLT. We finetune the pretrained SLT models on 5 downstream _open-domain_ SLT benchmarks covering 5 sign languages. Experiments show substantial quality improvements over the vanilla baselines, surpassing the previous state-of-the-art (SOTA) by wide margins.

## 1 Introduction

Scalable neural networks trained on large amount of unlabeled and/or weakly-labeled data from multiple modalities and multiple tasks have resulted in performance significantly exceeding that of single-task models trained on particular domains . Sign language translation (SLT),

Figure 1: BLEU scores on different benchmarks: our model sets new SOTA results across benchmarks and sign languages. Note we didn’t show BLEURT because not all previous studies report BLEURT.

as a video-to-text translation task1, features significant cross-modality challenges in video understanding and text generation. While extra forms of supervision such as glosses have been helpful in bridging the modality gap , they are nonstandardized/incomplete systems available only for small datasets . Researchers have instead turned to more scalable approaches such as adapting pretrained vision and text models [7; 37; 50] and jointly modeling with machine translation [MT, 58]. Despite encouraging progress, these studies were performed at small scale with success on narrowed domains and on few sign languages. In open-domain SLT settings, unfortunately, they have shown limited effectiveness .

In this paper, we aim to improve _open-domain_ SLT for multiple sign languages by means of large-scale SLT pretraining with more data, larger models and more languages. Inspired by the finding that jointly training SLT models with MT data enables positive knowledge transfer to SLT , we explore the following pretraining tasks and data: _web-crawled multilingual SLT_, _multilingual MT_, and _augmented SLT_. Although high-quality SLT training data are scarce, weakly-labeled SLT data covering diverse topics and signers are readily available from platforms like YouTube. Prior studies have demonstrated the feasibility of collecting massive YouTube SLT data and its effectiveness on improving SLT [46; 44; 43], and we follow this effort in a multilingual setup. Different from SLT, text-based MT datasets are massive and resource-rich across hundreds of spoken languages [3; 10]. We explore a subset of MADLAD-400  including up to 41 spoken languages for the pretraining. In addition, we construct synthetic multiway SLT data by translating video captions with an off-the-shelf MT model, which allows us to strengthen direct SLT across more translation directions. We investigate different ways of mixing these data to exploit weakly supervised SLT knowledge as well as cross-task cross-modality transfer at scale.

Figure 2: Illustration of model architecture and pretraining task for SLT. We perform large-scale pretraining and adopt multi-task learning at clip level (multiple captions) to better leverage the supervised knowledge.

As in Figure 2, we extend the unified encoder-decoder SLT framework from [46; 42; 44; 43] with extra tasks and modalities similar to , across different pretrained model families (T5, mT5 and ByT5) and different model sizes. We distinguish different tasks by carefully designed input prompts that contain task-specific control tokens. This affords us high flexibility in choosing what tasks and languages to incorporate into the pretraining, easing ablations and the scaling. We then finetune the pretrained SLT models on downstream SLT benchmarks to refine the learned SLT knowledge.

We evaluate the effect of scaling on 6 open-domain SLT benchmarks across 5 sign languages. FLEURS-ASL#0 , built on FLORES-200 , gives us a testbed to analyze multiway American Sign Language (ASL)-to-X SLT (we examine English and 41 other target languages), while the other benchmarks are for a single language pair. While pretraining results show the acquired general SLT capability, we also report finetuning results following . Our main findings are below:

* Adding more pretraining data, either machine translation or sign language translation data, is a promising way to improve SLT, yielding quality gains of varying degrees.
* Zero-shot ASL-to-X translation for language pairs not seen during pretraining is achievable by jointly training on ASL-to-En SLT data and En-to-X MT data.
* Augmenting SLT data by translating target captions to other languages with off-the-shelf MT models substantially improves the translation.
* Using larger models is not always helpful: ByT5 Base (582M) often outperforms XL (3.74B), but model scaling does benefit SLT when modeling capacity becomes a bottleneck (e.g., when more languages and data are used).
* Learned metrics (e.g., BLEURT) show higher correlation between pretrained and finetuned SLT scores than classical metrics (e.g., BLEU or ChrF).

Putting everything together, our model achieves new state-of-the-art results across the benchmarks as shown in Figure 1, demonstrating the significance of scaling SLT.

## 2 Sign Language Translation

### Modeling

We build on a line of work using T5 model families for SLT [46; 42; 44; 43], which build upon earlier SLT work [4; 61; 58] using the encoder-decoder architecture [41; 47]. Figure 1(a) shows the overall structure. The encoder takes as input the concatenation of a prompt instructing the task and a sequence of sign language video frames; the decoder predicts the text output in a target spoken language one token at a time. We adopt the family of pretrained (m/ByT5 models [35; 53] as the backbone and adapt them to SLT via large-scale SLT pretraining followed by downstream finetuning, i.e. **(m/ByT5 initialization \(\) SLT pretraining \(\) SLT finetuning**.

We rely on web-crawled YouTube SLT data for SLT pretraining, which provide high coverage on domains and signers albeit at lower quality. Although recent debates value data quality over data quantity in pretraining [21; 24; 29], we argue that they were established on the availability of massive high-quality training data, which doesn't hold for SLT yet. We expect that the pretraining could capture the (weakly) supervised SLT knowledge from the crawled data as in previous studies .

As shown in Figure 1(b), we adopt the clip-level training following  that randomly samples a clip of \(N\) seconds from the sign video and then predicts various types of in-clip information (such as caption texts and their start and end timestamps) based on the frames of the entire clip. Detailed tasks are listed in Figure 1(a), which are all formulated as sequence-to-sequence tasks. They are distinguished by prompts with different control tokens and are trained with the standard maximum likelihood objective. For the **baseline**, we consider the following two tasks: _SLT_ and _alignment_, and train it by mixing these two tasks with a pre-specified mix ratio.

**SLT**: This is the core task that directly models the translation from clip frames to the clip text in a target language. It is indispensable for the model to acquire the translation capability.
**Alignment**: It is an auxiliary task for SLT, learning to align the input clip with its captions. We train the model to infer the start and end time stamp for each in-clip caption. Apart from regularization, this task could improve the model's understanding of sign language .

### Scaling Model Size, Number of Languages and Pretraining Data Size

**Model Scaling** Scaling model size increases modeling capacity, which has been widely proven effective in improving the task performance [30; 18; 19]. We study whether and how increasing model size affects the SLT performance and compare (By/m)T5 models for SLT at different scales.

**Language Scaling** While most SLT works focus on a few sign and spoken languages, we expand our study to massive languages, covering up to 80 sign/spoken languages during pretraining, and 5 sign language and 42 spoken languages at evaluation. We are interested in whether a single SLT model could support multiple sign/spoken languages with non-trivial performance, and whether knowledge transfer could improve SLT on low-resource languages [57; 44].

**Data Scaling** Data scarcity is the main bottleneck hindering the development of SLT. To address this issue, we investigate the following three types of data for the pretraining:

**SLT**: We crawl multilingual YouTube SLT data following the recipe  except that we didn't perform human annotation and filtering. This allows us to significantly scale up the SLT data by 3\(\)6 times, reaching \(\)6,600 hours in total, albeit at much lower quality.
**Machine Translation**: Unlike SLT, MT is a text-to-text translation task with rich parallel resources, particularly for high-resource languages . We explore adding multilingual MT data into the pretraining and mark this task with control token "\(\)>" .
**Augmented SLT**: SLT data are often one-to-one translation data, where each sign language only has translation in one spoken language. This makes the translation of a sign language to other spoken languages difficult. We thus augment SLT data to one-to-many by translating the target text to other spoken languages via off-the-shelf MT models. As in Figure 1(a), we use "\(<\)aug\(>\)" to separate genuine SLT data from the augmented one .

## 3 Setup

**MT Pretraining Data** We use the parallel sentence-level portion of MADLAD-400  as the MT pretraining data. We extract a subset of MADLAD-400 for experiments, including 41 languages (apart from English (En)) covering diverse language families and scripts, and explore the impact of En\(\)Xx and Xx\(\)En MT data on SLT in experiments. We create two settings for the pretraining:

* **MT-Small**: A high/medium-resource subset including 11 languages es, de, fr, it, cs, pl, ru, zh, ar, ja, hi.
* **MT-Large**: This set includes all 41 languages. Apart from MT-Small, it has nl, pt, sv, hu, da, fi, el, sk, no, bg, lt, lv, sl, et, ko, hr, is, sr, tr, vi, id, he, th, ms, uk, ca, ta, fil, ne, cy.

Table 7 shows the statistics for each language. Unless otherwise specified, we balance the MT data distribution over languages during training by temperature sampling with a rate of 5 .

**SLT Pretraining Data** We experiment with noisy captioned sign language videos from YouTube. This is the full set of videos pre-manual filtering in . Estimated statistics for each sign language are summarized in Table 7. We also have two settings for this data:

* **YT-ASL**: \(\)2,800 hours of noisy captioned ASL videos; a superset of YouTube-ASL  (modulo video churn) and the same dataset used by .
* **YT-Full**: \(\)6,600 hours of noisy captioned multilingual sign language videos; a superset of .

During training, we mix the SLT data for all languages in proportion to their duration. We further augment these data with other spoken languages via MADLAD-MT-3B . For ASL SLT data, we translate the English captions to 41 spoken languages listed in MT-Large, which makesYT-ASL 43-way multilingual SLT, namely **Aug-YT-ASL**; for other SLT data, we translate the target text into English, resulting in 3-way multilingual SLT.2 We refer to the augmented SLT data for all signlanguages as **Aug-YT-Full**. Similar to MT-Small and MT-Large, we reorganize the augmented data to **Aug-YT-ASL-Small/Aug-YT-Full-Small** and **Aug-YT-ASL-Large/Aug-YT-Full-Large**.

**SLT Pretraining Mixture** We ablate across several SLT pretraining mixtures.

* **Baseline**: Caption alignment and SLT tasks. We use the task weights from , including \(4\%\) for alignment.
* **Baseline + MT**: We mix MT data into Baseline with a sampling probability of \(p_{MT}\).
* **Baseline + Augmented SLT**: We replace the Baseline SLT data with the augmented SLT data and uniformly sample the target language for each example at each step.
* **Baseline + MT + Augmented SLT**: _Baseline + MT_ but with augmented target languages, as above.

**Downstream Benchmarks, Evaluation and Model Setting** We thoroughly evaluate the translation performance on a range of _open-domain_ SLT benchmarks, including How2Sign , Elementary23 3, WMT23  and FLEURS-ASL#0 (signer id #0) . Detailed information for each benchmark is given in Table 1. Overall, the evaluation covers 5 source sign languages and 42 target spoken languages.4

We report translation results for **Pretraining** and **Finetuning**. During inference, we use beam search with a beam size of 5. We evaluate translation with detokenized BLEU  and ChrF , as well as neural metric, BLEURT . We use BLEURT as the main metric . We initialize our SLT model with three T5 model families: T5 , mT5  and ByT5 , at three different sizes: Base, Large and XL. We optimize models with Adafactor , and set the maximum text input, landmark input, and text output length to 512. More setup details are given in Appendix A.1.

## 4 Experiments

### SLT Pretraining Results

**Model scaling doesn't improve SLT consistently: Base often outperforms Large/XL.** Table 2 also shows that scaling up model size rarely results in consistent quality improvements. Different from findings on text-only tasks [35; 53], Base surpasses Large and XL in most cases, where Large often converges the slowest and performs the worst. Model scaling alone doesn't significantly reduce the video-text modality gap, although better optimization and checkpoint selection could help. XL performs relatively comparable to Base. When MT data is mixed in and modeling capacity becomes the bottleneck, the value of model scaling by XL emerges as shown in Figure 8 and Table 5.

   Task & Sign Lang & Target Lang & \#Train & \#Dev & \#Test \\  How2Sign & ASL & En & 183,097 & 10,277 & 13,890 \\ Elementary23 & GSS & El & 35,970 & 512 & 512 \\  & LIS-CH & It & 1,901 & 100 & 250 \\ WMT23 & LSF-CH & Fr & 5,560 & 100 & 250 \\  & DSGS & De & 310,840 & 420 & 250/246 \\  & & & (WMT22) & (SS/SRF split) \\  FLEURS-ASL\#0 & ASL & 200 Flores Langs & - & - & 353 \\   

Table 1: Summary of downstream SLT benchmarks. “#Train/#Dev/#Test”: the number of examples in the train, dev and test split. Note the sign language video and the target text in these benchmarks are often pre-segmented and aligned at sentence level. “DGS/ASL/GSS”: German/American/Greek Sign Language; “En/De/Fr/It”: English/German/French/Italian; “LIS-CH”: Italian Sign Language of Switzerland; “LSF-CH”: French Sign Language of Switzerland; “DSGS”: Swiss German Sign Language.

**Backbone affects SLT substantially; ByT5 generally performs the best.** While several previous studies selected T5  or mT5  as the SLT backbone, we observe in Table 2 that ByT5-based SLT outperforms its T5 counterpart in most settings, confirming the results of  at scale. Given that larger models do not consistently perform better, it seems less likely that ByT5's superiority comes from its encoder-heavy parameter allocation, and more likely that it is due to its spelling capabilities and reduced input length gap between byte text sequences and video frame sequences. Unless otherwise stated, we use ByT5 Base for the following experiments.

**Scaling SLT data generally improves quality significantly.** Adding more SLT data, i.e. from YT-ASL to YT-Full, largely improves the translation quality in most settings. For ByT5-based SLT particularly, the gain reaches \(\)7 BLEURT on How2Sign and \(\)11 BLEURT on FLEURS-ASL#0 (En) for Base and XL, respectively. We conjecture that adding more (multilingual) SLT data helps reduce the modality gap (especially with skeletons, which lack pretrained representations) and enable cross-lingual knowledge transfer .

**Mixing MT and SLT data yields positive knowledge transfer to SLT.** We next explore whether and how the addition of MT data benefits SLT, starting with YT-ASL and bilingual MT data with \(p_{mt}=0.5\). Figures 2(a) and 5 show that adding bilingual translation data improves SLT performance generally, confirming the findings of SLTUNet --that jointly training with MT enables positive knowledge transfer--at scale. The quality gains vary greatly across languages, which show little correlation with language family or training data scale. For example, adding a large amount of Fr\(\)En data (\(\)243M sentence pairs) helps little (or even hurts) on FLEURS-ASL#0 (En), while adding a small amount of Ja\(\)En data (\(\)5M sentence pairs) gives a gain of at least \(3\) BLEURT on How2Sign and FLEURS-ASL#0 (En).

**Translation direction of MT data affects transfer to SLT.** There are three ways to leverage MT data for SLT: 1) X\(\)En, 2) En\(\)X, and 3) both. We compare 1) and 3) in Figures 2(a) and 5 for ASL-to-En SLT. The translation direction of MT data influences SLT performance greatly and varies

    &  &  &  \\   & & Base & Large & XL & Base & Large & XL \\   & T5 & **29.54** & 27.95 & 22.96 & **32.8** & 4.18 & 32.41 \\  & mT5 & **34.94** & 8.46 & 23.7 & 35.59 & **43.53** & 23.09 \\  & ByT5 & **30.36** & 23.51 & 29.2 & **44.84** & 28.47 & 41.65 \\   & T5 & **31.64** & 25.45 & 8.57 & **42.86** & 37.55 & 30.02 \\  & mT5 & **31.46** & 19.37 & 24.46 & **38.03** & 24.56 & 33.16 \\   & ByT5 & **37.13** & 22.61 & 29.59 & 52.48 & 43.01 & **52.71** \\   

Table 2: Pretraining performance (BLEURT \(\)) for different sized (By/m)T5 models when pretrained on YT-ASL and YT-Full. Results are reported on the test set of How2Sign and FLEURS-ASL#0 (\(\)En, i.e. English as the target). Best results for each model family are highlighted in bold.

Figure 3: Pretraining performance for _Baseline + MT_ when varying MT languages. We show BLEURT\(\) results on FLEURS-ASL#0, and set \(p_{mt}=0.5\). Note MT languages are added separately instead of jointly. Results are for ByT5 Base. “X\(\)En”: MT data for translation into English; “X++En”: MT data for both translation directions; “Avg”: average performance over languages. MT languages are arranged in descending order from left to right based on their training data quantity.

across languages. On average, X\(\)En benefits ASL-to-En SLT more than X\(\)En: +0.08 and +0.9 BLEURT on How2Sign and FLEURS-ASL#0 (EN), respectively. We speculate that including translation into X uses model capacity, which, while enabling zero-shot ASL-to-X SLT as discussed below, results in slightly worse ASL-to-En performance. This suggests that MT data with the same target language as SLT is most effective for transfer. Table 3 shows further support where En\(\)X surpasses X\(\)En on multilingual SLT.

**We can achieve zero-shot bilingual ASL-to-X SLT via ASL-to-En SLT + En\(\)X MT, albeit at poor quality.** If knowledge can be transferred from MT to SLT, one straightforward question is whether we can achieve zero-shot SLT by jointly training with MT. We do so by training on ASL-to-En SLT + En\(\)X MT data and examining zero-shot ASL-to-X SLT on FLEURS-ASL#0 (X). Figure 3b shows that this works effectively. On Pl and It, we observe quality gains over 12 BLEURT; on average, adding MT data improves zero-shot SLT by \(\)6 BLEURT. Nevertheless, the overall zero-shot SLT performance is ind inddling, and the gains are unstable across languages, e.g. performance degrades for ASL-to-Hi SLT with joint MT training. Similar findings were also observed in multilingual MT and speech translation . Deeper analysis in Appendix A.2 reveals that zero-shot SLT also suffers from the off-target translation problem , i.e. translating into a wrong target language; adding MT data can alleviate it, mostly for high-resource languages.

**Using a higher sampling ratio for the MT data, i.e. larger \(p_{mt}\), often improves SLT.** We start with \(p_{mt}=0.5\), i.e., sampling equal amount of SLT and MT data, in the above experiments following intuition. However, the proportion of different types of data often has non-negligible influence in multilingual modeling . We next explore its impact on SLT and use MT En-De for illustration. Figure 4 and 6 shows that \(p_{mt}=0.5\) is sub-optimal and sampling more MT data improves SLT in most settings, regardless of using ByT5 Base or XL, YT-ASL or YT-Full, MT De\(\)En or De\(\)En, and How2Sign or FLEURS-ASL#0 (En/De). In addition, increasing the proportion of MT data also improves zero-shot ASL-to-De SLT. Note another benefit of using more MT data is to accelerate training, as loading SLT data is much slower than loading text-only MT data. We use \(p_{mt}=0.9\) by default in the following experiments.

**Multilingual MT improves multilingual (zero-shot) SLT.** The above experiments mainly analyze SLT with bilingual MT. We next investigate how multilingual MT affects multilingual (zero-shot) SLT, particularly the use of MT-Small and MT-Large. We report results for ASL-to-_Small_ and ASL-to-_Large_ SLT on FLEURS-ASL#0 where Small and Large denote the target languages covered by

Figure 4: Pretraining performance for _Baseline + MT_ when changing the mixing ratio of MT data \(p_{mt}\) on FLEURS-ASL#0 (En and De) test set. We show BLEURT\(\) results as we vary \(p_{mt}\) from 0.3 to 0.9.

    &  &  \\   & & Small & Large \\  Baseline + YT-ASL & 15.85 & 17.21 \\ Baseline + YT-Full & 24.36 & 23.16 \\  _Baseline + MT-Small_ & & \\  & En\(\)X & 23.51 & 21.25 \\ YT-ASL & X\(\)En & 17.44 & 19.72 \\  & En\(\)X & 23.84 & 19.19 \\   & En\(\)X & 27.29 & 23.15 \\ YT-Full & X\(\)En & 22.48 & 22.47 \\  & En\(\)X & 26.33 & 21.48 \\  _Baseline + MT-Large_ & & \\ YT-ASL & En\(\)X & 24.69 & 26.60 \\ YT-Full & En\(\)X & **29.52** & **30.69** \\   

Table 3: Pretraining performance for _Baseline + MT_ with \(p_{mt}=0.9\) when scaling up languages and data. We show averaged BLEURT\(\) results on FLEURS-ASL#0. Results are for ByT5 Base. “Dir”: translation direction of MT data; “Small-Large”: average results over the target languages included in MT-Small/MT-Large on FLEURS-ASL#0.

MT-Small and MT-Large, respectively. Note all SLT directions are zero-shot except the translation to English.

Table 3 summarizes the average performance. Using multilingual X\(\)En MT data results in unstable ASL-to-X SLT performance, which even hurts SLT on YT-Full. In contrast, multilingual En\(\)X and En\(\)X MT data are both very helpful to SLT, where the former often outperforms the latter. By default, we still use En\(\)X MT data in the following experiments so as to fully leverage the knowledge in MT data during pretraining.

Figures 6(a) and 6(b) further show the language breakdown results. Adding multilingual MT significantly improves ASL-to-En SLT when using YT-ASL alone, while the gain almost disappears when using larger-scale SLT data, YT-Full. Again, we note that the overall zero-shot translation quality is poor - the best average BLEURT on Small and Large is 29.52 and 30.69, respectively. Achieving significant ASL-to-X SLT requires techniques beyond naive SLT and MT data mixing.

**Data augmentation and large-capacity modeling are promising methods for multilingual SLT.** In MT, a common solution to improve zero-shot quality is to construct pseudo translation data for zero-shot directions [2; 15; 56; 16]. We examine this practice for SLT. We adopt publicly pretrained MT models to generate data for more target languages for the YouTube SLT data (i.e., Augmented SLT). Results in Table 4 demonstrate the effectiveness of Augmented SLT, which significantly improves the best performance for ByT5 Base-based SLT to 36.01 and 39.85 average BLEURT on Small and Large with a gain of 6.49 (29.52\(\)36.01) and 9.16 (30.69\(\)39.85), respectively. Note there are 42 languages in Large. ByT5 Base may be insufficient in accommodating translation for such amount of languages. Increasing the modeling capacity to XL yields another gain of 9.11 (36.01\(\)45.12) and 8.2 (39.85\(\)48.05) average BLEURT on Small and Large, respectively. On YT-ASL, Augmented SLT and ByT5 XL also lead to substantial quality improvements by 13.84 (24.69\(\)38.53)/15.96 (26.60\(\)42.56) average BLEURT on Small/Large. The final performance even surpasses the cascading baseline, i.e. ASL-to-En SLT chained with En-to-X MT, under both YT-ASL and YT-Full. Figures 7(a) and 7(b) also show the quality improvements across languages resulted from data augmentation and ByT5 XL.

### SLT Finetuning Results

We report the finetuning performance measured by BLEURT in Table 5. We also include the BLEU and ChrF results as well as the corresponding pretraining results in Appendix (Tables 8 and 9).

**Finetuning on downstream benchmarks substantially improves SLT performance.** Table 5 shows that finetuning the pretrained SLT models yields substantial quality gains across benchmarks and settings. This is because the potential of pretrained models is not fully elicited by direct evaluation due to video recording, domain and (clip-based) pretraining vs. (segment-based) inference mismatches, and finetuning largely mitigates these gaps. For example, pretraining with external augmented SLT and MT data results in even worse pretraining performance ((6)\(\)(7)) in Table 9. After finetuning, nevertheless, model (7) significantly surpasses model (6) by 5.12 BLEURT on average.

Adding multilingual SLT data (YT-Full) into the pretraining greatly improves the performance from 14.26 (model (5)) to 32.48 BLEURT (model (8)) in Table 9. However, the quality gain after finetuning for YT-ASL based models is often higher than their YT-Full counterparts, where the largest gain reaches \(\)28 BLEURT for model (5). We argue that pretraining on YT-ASL mainly teaches

    &  \\   & Small & Large \\  Baseline + YT-ASL & 15.85 & 17.21 \\ + Aug-YT-ASL-Small & 31.14 & 19.74 \\ + MT-Small & 30.51 & 19.70 \\ + Aug-YT-ASL\&MMT-Large & 25.83 & 33.71 \\ + ByT5 XL & **38.53** & **42.56** \\ + MT-3B Cascading & 34.82 & 37.82 \\  Baseline + YT-Full & 24.36 & 23.16 \\ + Aug-YT-Full-Small & 38.53 & 29.49 \\ + MT-Small & 36.84 & 25.67 \\ + Aug-YT-Full\&MT-Large & 36.01 & 39.85 \\ + ByT5 XL & **45.12** & **48.05** \\ + MY-3B Cascading & 43.54 & 46.32 \\ + ByT5 XL & 44.82 & 47.52 \\   

Table 4: Pretraining performance (averaged BLEURT) for _Baseline + Augmented SLT + MT_ with \(p_{mt}=0.9\) on FLEURS-ASL#0 test set. MT data are multilingual in both directions. Baseline is for ByT5 Base; “MT-3B”: MADLAD-MT-3B, the model used for SLT augmentation; “Cascading”: translating FLEURS-ASL#0 to English and then performing MT to other target languages.

understanding of ASL, so pretrained performance on other sign languages is poor, but finetuning can quickly adapt the learned representations to other sign languages.

Note we also finetuned the vanilla ByT5 model without SLT pretraining for reference, which achieves 7.68 and 3.10 BLEU on Elementary23 and WMT23 DSGS SS, respectively. Despite their inferiority, these results already surpass the previous SOTA, further showing the potential of ByT5.

**A model's pretraining performance may be misleading when estimating its downstream finetuning performance, depending on the evaluation metric.** Intuitively, a model with better pretrained results should result in better finetuned results. The Spearman's correlation results in Table 6 confirm this intuition, where the correlation scores are positive across metrics. However, BLEU and ChrF have a correlation score of 0.347 and 0.186, respectively, which are very moderate. The correlation for ChrF is even not significant, which may be caused by the use of BLEU as the model selection metric. In contrast, the correlation of BLEURT reaches 0.578 and is significant at \(p<0.01\).

**Model, data and language scaling together leads to new state-of-the-art results.** Diving deeper into Table 5, we see clear improvements brought by scaling model size, data, and/or languages for SLT. Adding YT-ASL SLT data into the pretraining yields \(\)10 average BLEURT improvement ((1)\(\)(2)). Jointly training SLT with MT data produces another gain of \(\)6 BLEURT ((2)\(\)(3)). Data augmentation adds an improvement of \(\)3 BLEURT ((3)\(\)(4)), which matches the quality achieved by adding large amount of extra multilingual SLT data to the baseline, i.e. (4) 40.91 vs. (6) 40.28. By further increasing the amount of MT and augmented SLT data as well as the ByT5 model size, we reach an average BLEU, ChrF and BLEURT of 16.90, 39.49, and 49.60, respectively (model (8)). These results also outperform previous best results, establishing the new SOTA.

**Multilingual finetuning improves multilingual SLT with encouraging performance, although it still underperforms bilingual finetuning on average.** We next study multilingual finetuning on the direct mix of different SLT benchmarks. Table 5 ((8)\(\)(9)) shows that multilingual SLT outperforms previous SOTA on almost all benchmarks, but underperforms its bilingual counterpart by 1.22 BLEURT on average. How to balance modeling capacity among different languages in a joint model and avoid cross-lingual/modality interference is a well known issue in multilingual modeling , and multilingual SLT also suffers , which we leave to future. Still, multilingual SLT facilitates transfer to LIS-CH, leading to a substantial gain of 2.6 BLEURT ((8)\(\)(9)).

## 5 Related Work

The main bottleneck of SLT is data scarcity. Early studies address this issue by developing more data efficient neural architectures and/or training algorithms. Camgoz et al.  pioneered the study with

    & BLEU & ChrF & BLEURT \\ Spearman’s \(\) & 0.347\({}^{}\) & 0.186 & **0.578\({}^{}\)** \\   

Table 6: Spearman correlation between direct (i.e. pretraining) and finetuning SLT results under different metrics based on Tables 5 and 9. \({}^{}\)\({}^{}\): significant at \(p<0.05/0.01\).

    &  &  &  &  &  \\  & & & & LIS-CH & LSF-CH & SRF & SS & \\ 
0 & Prevous SOTA & 50.80 & - & 25.20 & 18.80 & 24.60 & 37.70 & - \\ 
1 & ByT5 Base & 34.00 & 22.14 & 22.77 & 7.74 & 15.41 & 26.88 & 21.49 \\
2 & 1 + Baseline + YT-ASL & 51.74 & 37.79 & 24.24 & 15.43 & 21.82 & 35.59 & 31.10 \\
3 & 2 + MT-Small (\(_{mt}=0.9\)) & 52.62 & 45.98 & 33.10 & 24.85 & 23.33 & 45.45 & 37.51 \\
4 & 3 + Aug-YT-ASL-Small & 53.36 & 49.34 & 38.61 & 28.70 & 25.87 & 49.61 & 40.91 \\
5 & 4 + Aug-YT-ASL\&MT-Large + ByT5 XL & 54.28 & 54.16 & 38.93 & 27.29 & 28.42 & 51.73 & 42.47 \\ 
6 & 2 + YT-Full & 53.51 & 49.48 & 42.11 & 31.16 & 21.15 & 44.28 & 40.28 \\
7 & 6 + Aug-YT-ASL\&MT-Small & 53.70 & 53.13 & 45.09 & 37.69 & 30.31 & 52.45 & 45.40 \\
8 & 7 + Aug-YT-ASL\&MT-Large + ByT5 XL & **55.69** & **56.94** & 51.94 & **41.14** & **33.94** & 57.96 & **49.60** \\ 
9 & 8 + Multilingual SLT Tuning & 53.47 & 55.57 & **54.54** & 39.26 & 29.33 & **58.08** & 48.38 \\   

Table 5: Finetuning performance (BLEURT\(\)) on downstream SLT benchmarks. “H2S/E23”: How2Sign/Elementary23. “SRF/SS”: WMT23 DSGS SRF/SS test split. “Avg”: averaged performance over all benchmarks. MT data are added in both translation directions. Previous SOTA: How2Sign , Elementary23  and WMT23 SRF , WMT23 LIS-CH, LSF-CH, SS . _All models are finetuned on each SLT benchmark separately except (9)._encoder-decoder based recurrent models for SLT, which was quickly replaced by Transformer and multi-task learning with CTC regularization . Zhou et al.  developed spatial-temporal architecture to model the collaboration of different visual cues. Another way is to transfer the knowledge from pretrained models, augmentations, and other tasks. Chen et al. [7; 8] proposed to leverage pretrained visual encoders and MT models to improve SLT, while Zhang et al.  explored transferring translation knowledge from MT data directly. Zhou et al.  employed back-translation to generate pseudo SLT training data. Ye et al.  augmented the training data by the mix-up algorithm. Yet another way to address data scarcity is to make data less scarce. Shi et al. , Uthus et al. , and Tanzer and Zhang  collected large-scale SLT data from YouTube and improved data quality via manual filtering; Albanie et al.  developed a British Sign Language translation corpus based on BBC broadcasts instead. Tanzer  scaled up ASL data by eschewing manual filtering and tolerating misaligned or irrelvant data. We follow and scale to noisy multilingual sign language data, MT data, and augmented paralel data.

Despite the aforementioned advancements, many studies still heavily depend on _sign glosses_. As a bridge between sign video and target text, sign glosses ease learning, but are expensive to annotate, not always available, nonstandardized, and cannot cope with sign language grammar in generality . Recent research therefore turns to gloss-free SLT, which often underperforms gloss-based counterparts [59; 25; 50] and performs poorly in open-domain settings [38; 51; 28]. We substantially improve gloss-free SLT performance across benchmarks through scaling. In this regard, our work is closely related to SSVP-SLT  but with different focuses. SSVP-SLT improves SLT by pretraining a neural sign encoder through large-scale self-supervised learning. By contrast, we adopt static landmarks to represent sign frames and improve the translation by transferring knowledge from other languages and tasks. The methods used in our study are orthogonal to SSVP-SLT. In addition, our work also falls into the category of improving multilingual SLT [55; 20]. We didn't evaluate our models on these multilingual benchmarks though as they are either unavailable at the time of paper writing or unusable due to licensing issues.

## 6 Conclusion, Limitations, and Future Work

We presented a systematic study of data, model and language scaling for SLT via large-scale SLT pretraining. In general, scaling substantially improves SLT. We observe positive knowledge transfer from other sign language data and from machine translation data. By joint SLT and MT training, we show the feasibility of achieving zero-shot SLT. Data augmentation expanding SLT data to more spoken languages via off-the-shelf MT models significantly improves multilingual SLT. Putting everything together, finetuning our pretrained SLT models leads to new state-of-the-art SLT results across 5 benchmarks covering 5 sign languages (but still far from usable quality).

Although our models have nominally been pretrained on a massive number of sign languages (up to 80), we lack comprehensive and reliable multilingual benchmarks to fully understand their abilities and limitations. In addition, our models are limited to encoder-decoder based (m/By)T5 models, and SLT pretraining requires many computational resources, increasing the difficulty of reproduction.

In the future, we expect that continuing to scale sign language data, number of sign languages, vision pretraining/multimodality, etc. will reap further gains. As suggested by , it will be important to evaluate these growing capabilities on multilingual, standardized, open-domain SLT benchmarks.

## Ethics Statement

We preprocess all sign videos with simplified landmarks as a form of anonymization and privacy protection. While the pretraining SLT data is larger scale than prior work, it may still suffer from demographic biases. Even if demographics were represented in proportion to the real world, and even with simplified landmarks, the resulting SLT models may not perform equally across groups and should be evaluated for fairness before real-world deployment. Our study mainly aims to understand the impact of scaling on SLT, and while we significantly improve translation quality, it is still far from usable for real-world applications. For many such applications, the other half of sign language translation--sign language generation--is also essential, whereas we focus only on sign language understanding in this work. Advancing both of these is critical to ensure that Deaf/Hard of Hearing signers get equal access to technology and the information that comes through it.