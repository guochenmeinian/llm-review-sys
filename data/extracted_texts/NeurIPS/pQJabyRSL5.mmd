# The Importance of Being Bayesian in Online Conformal Prediction

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Based on the framework of _Conformal Prediction_ (CP), we study the online construction of valid confidence sets given a black-box machine learning model. Converting the targeted confidence levels to quantile levels, the problem can be reduced to predicting the quantiles (in hindsight) of a sequentially revealed data sequence, where existing results can be divided into two types.

* Assuming the data sequence is iid, one could maintain the empirical distribution of the observed data as an algorithmic belief, and directly predict its quantiles.
* As the iid assumption is often violated in practice, a recent trend is to apply first-order online optimization on moving quantile losses . This indirect approach requires knowing the targeted quantile level beforehand, and suffers from certain validity issues on the obtained confidence sets, due to the associated loss linearization.

This paper presents a Bayesian approach that combines their strengths. Without any statistical assumption, it is able to both

* answer multiple arbitrary confidence level queries online, with provably low regret; and
* overcome the validity issues suffered by first-order optimization baselines, due to being "data-centric" rather than "iterate-centric".

From a technical perspective, our key idea is to take the above iid-based procedure and regularize its algorithmic belief by a Bayesian prior, which "robustifies" it by simulating a non-linearized _Follow the Regularized Leader_ (FTRL) algorithm on the output. For statisticians, this can be regarded as an online adversarial view of Bayesian nonparametric distribution estimation. Importantly, the proposed belief update backbone is shared by "prediction heads" targeting different confidence levels, bringing practical benefits similar to U-calibration .

## 1 Introduction

Modern machine learning (ML) models are better at point prediction compared to probabilistic prediction. For example, when given an image classification task, they are better at responding "_this image is most likely a white cat_", rather than "_I'm 90% sure this image is an animal, 60% sure it's a cat, and 30% sure it's a white cat_". For downstream users, the more nuanced probabilistic predictions are often important for risk assessment. The challenge, however, lies in aligning the model's own uncertainty evaluation with its actual performance in the real world.

_Conformal Prediction_ (CP)  has recently emerged as a premier framework to address this challenge, as it blends the empirical strength of modern ML with the theoretical soundness oftraditional statistical methods. In a nutshell, CP algorithms make _confidence set predictions_ (rather than point predictions) on the label space, by sequentially interacting with three other parties: the _nature_ (i.e., the data stream), a _black-box ML model_, and _downstream users_. Writing the covariate-label space as \(\) and the time horizon as \(T\), we consider the following sequential interaction protocol. In each (the \(t\)-th) round,

1. We, as the CP algorithm, observe a _target covariate_\(x_{t}\) from the nature, and a _score function_\(s_{t}:[0,R]\) generated by a black-box ML model Base.1 2. The downstream users select a finite set of _confidence level queries_, \(A_{t}\). Given each \( A_{t}\), we predict a _score threshold_\(r_{t}()\),2 which leads to a _confidence set_ 3. We observe the _ground truth label_\(y_{t}\) from the nature, and send the \((x_{t},y_{t})\) pair to Base (which it optionally uses to update the score function \(s_{t+1}\)). Define the _true score_\(r^{*}_{t}:=s_{t}(x_{t},y_{t})\).

Limitation of prior workThe essential objective of CP is to have the prediction \(r_{t}()\) close to the (\(1-\))-_quantile_ of the true score sequence \(r^{*}_{1:T}\), while only knowing \(r^{*}_{1:t-1}\). For the readers' reference, the \((1-)\)-quantile of a real random variable \(X\) is defined as \(q_{1-}(X):=\{x;(X x) 1-\}\). Guided by this general principle, the community has focused on two very different approaches under distinct assumptions.

* Assuming the sequence \(r^{*}_{1:T}\) is iid, it suffices to maintain the empirical distribution of \(r^{*}_{1:t-1}\), denoted as \(P_{t}=(r^{*}_{1:t-1})\), as an _algorithmic belief_. Then, when queried with the confidence level \(\), the CP algorithm directly "post-processes" the belief by setting \(r_{t}()=q_{1-}(P_{t})\), or in situations with only _exchangeability_, \(q_{1--o(1)}(P_{t})\). This is essentially _Empirical Risk Minimization_ (ERM) with the quantile loss \(l_{}(r,r^{*}):=(-[r<r^{*}])(r-r^{*})\), i.e., \[r_{t}()=q_{1-}(P_{t})*{arg\,min}_{r[0,R]} _{i=1}^{t-1}l_{}(r,r^{*}_{i}).\] (2)
* Since the iid assumption is often violated in practice, a recent trend  is to indirectly view CP as an instance of _adversarial online learning_, and apply first-order optimization algorithms from there. Taking gradient descent for example, such an approach amounts to picking \(r_{1}()[0,R]\) and following with the projected incremental update \[r_{t+1}()=_{[0,R]}[r_{t}()-_{t} l_{}(r_ {t}(),r^{*}_{t})],\] where \(_{t}>0\) is the _learning rate_, and \( l_{}(r,r^{*})\) can be any subgradient of the quantile loss \(l_{}\) with respect to the first argument. Strictly speaking the two approaches are incomparable due to targeting different performance metrics, but nonetheless, let us compare the _algorithms_ side by side. Although first-order optimization seems more robust due to the nonnecessity of statistical assumptions, it requires being "iterate-centric" rather than "data-centric": one needs to fix a single confidence level \(\) beforehand, and the prediction \(r_{t}()\) depends on how previous predictions \(r_{1:t-1}()\) compare to the "data" \(r^{*}_{1:t-1}\), rather than just the "data" itself. This leads to some paradoxical observations regarding the obtained confidence sets. For example,
* The confidence set \(_{t}\) is not invariant to permutations of \(r^{*}_{1:t-1}\).
* Suppose one runs two first-order optimization algorithms targeting different \(\) (say, \(_{1}<_{2}\)), then even if the initialization \(r_{1}(_{1})=r_{1}(_{2})\), it is still possible that \(_{t}(x_{t},_{1})\) is strictly contained in \(_{t}(x_{t},_{2})\). That is, the confidence sets violate the monotonicity of probability measures.

In contrast, the ERM approach does not suffer from such issues, therefore is more "valid / plausible" in some sense. The problem is that ERM, also known as _Follow the Leader_ (FTL) in online learning, is not robust to adversarial environments with quantile losses. Can we enjoy the best of both worlds?ContributionThis paper presents a Bayesian approach to CP, which (\(i\)) does not require any statistical assumption; (\(ii\)) does not suffer from the aforementioned validity issues; and (\(iii\)) efficiently handles multiple, arbitrary confidence levels revealed online, with provably low regret. Our main workhorse, in short, is an online adversarial view of Bayesian nonparametric estimation.

## 2 Main result

OverviewOur proposed algorithm (Algorithm 1) is perhaps the simplest one could think of. Defining the _Bayesian prior_ as an arbitrary distribution \(P_{0}\) on the domain \([0,R]\) (with strictly positive density \(p_{0}:[0,R]_{>0}\)), we update the algorithmic belief \(P_{t}\) by mixing \(P_{0}\) with the empirical distribution of the previous true scores, \((r_{1:t-1}^{*})\). This can be seen as regularizing the frequentist belief update \(P_{t}=(r_{1:t-1}^{*})\). Then, given each queried confidence level \(\), our algorithm picks \(r_{t}()=q_{1-}(P_{t})\) just like the iid-based approach. It is clear that \(r_{t}()\) is invariant to permutations of \(r_{1:t-1}^{*}\), and for any \(_{1}<_{2}\) we always have \(r_{t}(_{1}) r_{t}(_{2})\).

```
0: Step sizes \(\{_{t}\}_{t_{+}}\), where each \(_{t}\) and \(_{1}=1\). Bayesian prior \(P_{0}\).
1:for\(t=1,2,\)do
2: Compute the empirical distribution \((r_{1:t-1}^{*})\), and set the algorithmic belief \(P_{t}\) to \[P_{t}=_{t}P_{0}+(1-_{t})(r_{1:t-1}^{*}).\] (3)
3:for\( A_{t}\)do
4: Output the score threshold \(r_{t}()=q_{1-}(P_{t})\).
5:endfor
6: Observe the true score \(r_{t}^{*}\).
7:endfor ```

**Algorithm 1** Online conformal prediction with regularized belief.

Our central observation, however, is quite profound in our opinion:

The Bayesian regularization on the algorithmic belief \(P_{t}\) induces _downstream regularizations_ on the predicted threshold \(r_{t}()\).

In particular, Theorem 1 shows that despite not knowing \(\) beforehand, Algorithm 1 generates the same output \(r_{t}()\) as a non-linearized _Follow the Regularized Leader_ (FTRL) algorithm with the quantile loss \(l_{}\). To provide more context, FTRL is a standard improvement of ERM / FTL with better stability in adversarial environments, and our framework involves its non-linearized version which retains the full structure of quantile losses. It is also important to note that the _downstream simulation_ of FTRL deviates from the common scope of online learning (which requires specifying a single loss function in each round ), and instead has a similar flavor as the recently proposed _U-calibration_: forecasting for an _unknown_ downstream agent.

From a more technical perspective: prior works on U-calibration considered the setting of "finite-class distributional prediction" with generic _proper_ losses , while our paper focuses on the continuous domain \([0,R]\) (i.e., "infinitely many classes") with the more specific quantile losses. The extra problem structure allows our algorithm to be deterministic (rather than _Follow the Perturbed Leader_; FTPL), thus establishing a closer connection to deterministic _online convex optimization_.

Appendix A further discusses the interpretation of the belief update Eq.(3) as _Bayesian nonparametric distribution estimation_. The nontrivial insight here is that this statistical procedure induces downstream adversarial regret bounds, without statistical assumptions at all.

AnalysisFormally, we first present the FTRL-equivalence of Algorithm 1, which can be compared to the FTL-equivalence of the iid-based approach, i.e., Eq.(2).

**Theorem 1**.: _With a base regularizer defined as \((r):=_{r^{*} P_{0}}[l_{}(r,r^{*})]\), the output \(r_{t}()\) of Algorithm 1 satisfies_

\[r_{t}()*{arg\,min}_{r[0,R]}[(t -1)}{1-_{t}}(r)+_{i=1}^{t-1}l_{}(r,r_{i}^{*}) ],,t 2.\] (4)_Specifically, (i) \(\) is strongly convex with coefficient \(_{r[0,R]}p_{0}(r)\); and (ii) if \(P_{0}\) is the uniform distribution on \([0,R]\), then \(\) is the quadratic function,_

\[(r)=r^{2}-(1-)r+(1-)R.\]

Next, using Theorem 1, we obtain the following _regret bound_ for our CP algorithm. Here we only consider the uniform prior, and defer the case of generic priors to longer versions of this paper (the benefit of good priors can be shown using the _local norm_ analysis of FTRL [11, Section 7.4]).

**Theorem 2**.: _Let \(P_{0}\) be the uniform distribution on \([0,R]\). With the step size \(_{t}=1/\), the output of Algorithm 1 against any \(r_{1:T}^{*}\) sequence satisfies_

\[_{t=1}^{T}l_{}(r_{t}(),r_{t}^{*})-_{t=1}^{T}l_{}(q_ {1-}(r_{1:T}^{*}),r_{t}^{*})=O(R),,\]

_where \(q_{1-}(r_{1:T}^{*})\) denotes the \((1-)\)-quantile of the hindsight empirical distribution \((r_{1:T}^{*})\), and \(O()\) subsumes absolute constants._

Let us interpret this bound. Suppose \((r_{1:T}^{*})\) is known beforehand (but the exact \(r_{1:T}^{*}\) sequence is unknown), then for all \(\), a very reasonable strategy is to predict \(r_{t}()=q_{1-}(r_{1:T}^{*})\). Theorem 2 shows that without statistical assumptions, Algorithm 1 asymptotically performs as well as this oracle in terms of the total quantile loss. Existing first-order optimization baselines are equipped with regret bounds of a similar type , but the key difference is that they require knowing \(\) beforehand, whereas Algorithm 1 achieves low regret simultaneously for all \(\).

## 3 Discussion

Any-\(\) baselineAlthough not studied in existing works, it is actually possible to construct a nonstochastic CP algorithm from first-order optimization algorithms, without specifying a fixed \(\) beforehand. The idea is simple: (\(i\)) evenly discretize the \(\) interval using a grid \(\) of size \(\); (\(ii\)) for each \(\), run a "base" CP algorithm targeting \(\); and (\(iii\)) at test time, given a queried \(\), follow the base algorithm corresponding to its nearest neighbor in \(\). It also satisfies the regret bound in Theorem 2, since the nearest-neighbor approximation only adds an additive \(O(R)\) factor due to the Lipschitzness of the quantile loss function \(l_{}(r,r^{*})\) with respect to \(\).

However, such a baseline also suffers from the previously mentioned validity issues. Even more, the update (based on \(r_{t}^{*}\)) and the queries (based on \(A_{t}\)) are coupled: if \(A_{t}\) is empty for a certain \(t\) (all the users abstain), the baseline still needs \(O()\) time in that round to process the observation \(r_{t}^{*}\). In comparison, Algorithm 1 needs one UpdateTime to process \(r_{t}^{*}\) and \(|A_{t}|\) QueryTime to answer the \(\)-queries, where their exact values depend on the data structure used to maintain the belief \(P_{t}\).

Coverage boundA common objective in online CP, initiated by , is to show that given a confidence level \(\), the post-hoc empirical _coverage frequency_ of an algorithm approaches \(\), i.e.,

\[|-T^{-1}_{t=1}^{T}[r_{t}^{*} r_{t}^{*}()] |=o(1).\]

Since this can be achieved by switching between \(r_{t}^{*}()=0\) and \(r_{t}^{*}()=R\) independently of data , one needs an extra objective, such as the regret (Theorem 2), to justify the validity of an online CP method. Existing first-order optimization baselines satisfy both desirable bounds.

Here we argue that the regret could be a better-posed objective than the coverage. To support this argument, notice that just like the previous pathological example, first-order optimization baselines achieve the coverage bound due to the "overshooting" provided by the loss linearization, and the latter also causes the validity issues discussed earlier. Besides, achieving the coverage bound requires adjusting the prediction based on the _coverage history_: if an algorithm keeps mis-covering, then it has to predict a very small \(r_{t}()\) to "almost ensure" coverage. These are different from regret minimization, where loss linearization is not necessary, and the algorithm is incentivized to best-respond to its belief (on the empirical distribution of the environment in hindsight).