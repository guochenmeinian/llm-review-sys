# Convergence Analysis of ODE Models for Accelerated First-Order Methods via Positive Semidefinite Kernels

Convergence Analysis of ODE Models for Accelerated First-Order Methods via Positive Semidefinite Kernels

Jungbin Kim

Seoul National University

kjb2952@snu.ac.kr

&Insoon Yang

Seoul National University

insoonyang@snu.ac.kr

Corresponding author.

###### Abstract

We propose a novel methodology that systematically analyzes ordinary differential equation (ODE) models for first-order optimization methods by converting the task of proving convergence rates into verifying the positive semidefiniteness of specific Hilbert-Schmidt integral operators. Our approach is based on the performance estimation problems (PEP) introduced by Drori and Teboulle . Unlike previous works on PEP, which rely on finite-dimensional linear algebra, we use tools from functional analysis. Using the proposed method, we establish convergence rates of various accelerated gradient flow models, some of which are new. As an immediate consequence of our framework, we show a correspondence between minimizing function values and minimizing gradient norms.

## 1 Introduction

We consider the following convex optimization problem:

\[_{x^{d}}\ f(x),\] (1)

where \(f:^{d}\) is a continuously differentiable (\(\)-strongly) convex function. We assume that a minimizer \(x^{*}\) exists. First-order methods, for example, gradient descent and Nesterov's accelerated gradient method, are popular in solving this problem due to their low cost per iteration and dimension-free oracle complexities. These methods can be analyzed by examining their limiting ODEs. For instance, the gradient descent \(x_{k+1}=x_{k}-s f(x_{k})\) corresponds to the _gradient flow_\((t)=- f(X(t))\). Building upon this idea, Su _et al._ derived the limiting ODE for Nesterov's accelerated gradient method (AGM)  and analyzed its convergence rate, offering valuable insights into momentum-based algorithms. A common approach to establishing convergence rates of continuous-time ODE models involves using Lyapunov functions .

In this paper, we propose a generic framework that builds upon the performance estimation problem (PEP) presented in  for analyzing the convergence rates of ODE models for various accelerated first-order methods, including Nesterov's AGM. The proposed method is designed from the Lagrangian dual of a relaxed version of continuous-time PEP. Consequently, our framework transforms the task of proving convergence rate into verifying the positive semidefiniteness of a specific integral kernel. Moreover, our framework can also ensure the tightness of the resulting guarantee, meaning that the obtained convergence guarantee is optimal among all possible convergence rates that can be derived from weighted integrals of the inequalities employed in convergence proofs. Using the proposed framework, we confirm the convergence rates of existing ODE models and uncover those of new accelerated ODE models. In traditional convergence analysis of ODE models, it can be challenging to design appropriate Lyapunov functions.2 However, in our framework, we only need to verify the positive semidefiniteness of a specific integral kernel. This approach circumvents the need for Lyapunov function design, making our framework more straightforward for analyzing convergence rates.

In the discrete-time setting, the PEP framework has been extensively studied for its ability to systematically obtain tight convergence guarantees  and facilitate the design of new optimization methods [17; 18; 40]. However, analyzing PEP is typically regarded as challenging to comprehend due to the involvement of large matrices with complex expressions. In contrast, our framework utilizes integral kernels, which serve as a continuous-time counterpart to matrices. The computational process within our approach yields simpler outcomes. Consequently, our continuous-time PEP framework has the potential to offer valuable insights into the analysis of the discrete-time PEP, similar to how ODE models have helped designing and analyzing discrete-time methods in the literature [20; 47]. By bridging the gap between the continuous and discrete settings, our methodology enhances the understanding of the PEP framework.

### Related work

Continuous-time models for first-order methods.The investigation into the continuous-time limit of accelerated first-order methods began with the study of AGM ODE [36; 2; 3]. Since then, subsequent researches have explored various aspects of the ODE models. These include generalizations within the mirror descent setup , a broader family of dynamics derived using Lagrangian mechanics [47; 48; 19], high-resolution ODE models [34; 33], and continuized methods . Systematic methodologies for finding Lyapunov functions were developed, including deriving them from Hamilton's equations  or dilated coordinate systems . For obtaining accelerated discrete-time algorithms, several studies have applied discretization schemes, such as symplectic  and Runge-Kutta  schemes, to discretize accelerated ODE models.  showed that applying multi-step integration schemes to the gradient flow also yields accelerated algorithms. A particularly relevant study is , as they present the dynamics in the form of (4) using the _H-kernel_, which plays a crucial role in our analysis.

Performance estimation problems.The idea of using performance estimation problems to analyze the convergence rate of optimization methods was first introduced by . This concept was further refined by employing a convex interpolation argument in  and was applied to a wide range of settings in [5; 43; 44; 12; 11; 7; 16]. The idea of performance estimation has been used to construct Lyapunov functions in [41; 39; 25; 45]. In particular,  analyzes continuous-time ODE models. However, their methodology differs from ours, as they employed semidefinite programs of finite dimension. Another closely related approach is based on _integral quadratic constraints_ (IQC) from control theory , which were used to analyze the convergence rate of first-order methods in . The IQC framework has been further studied in [14; 15; 10; 22; 31]. One practical application of PEP and IQC is the design of novel algorithms by optimizing the convergence guarantees. Some notable examples include OGM , TMM , ITEM , and OGM-G .

## 2 Preliminaries and notations

In this section, we review some basic notions from functional analysis that will be used throughout the paper. For a more detailed treatment, we refer the reader to the textbooks [29; 30; 28].

Function spaces.We denote the set of continuous functions from \([0,T]\) to \(^{d}\) by \(C([0,T];^{d})\) and the set of continuously differentiable functions from \([0,T]\) to \(^{d}\) by \(C^{1}([0,T];^{d})\). We define the space \(L^{2}([0,T];^{d})\) as the set of all measurable functions \(f:[0,T]^{d}\) that satisfy \(_{0}^{T}\|f(x)\|_{^{d}}^{2}\,dx<\). Then, \(L^{2}([0,T];^{d})\) is a Hilbert space, equipped with an inner product and a norm defined by \( f,g_{L^{2}([0,T];^{d})}=_{0}^{T} f(t),g(t) _{^{d}}\,dt\) and \(\|f\|_{L^{2}([0,T];^{d})}=([0,T]; ^{d})}}\).

Integral operators.An integral operator is a linear operator that maps a function \(f\) to another function \(Kf\) given by

\[(Kf)(t)=_{0}^{T}k(t,)f()\,d,\] (2)where \(k:[0,T]^{2}\) is the associated integral kernel. Intuitively, an integral kernel can be seen as a continuous-time version of a matrix. A Hilbert-Schmidt kernel is an integral kernel \(k\) that is square integrable, i.e., \(k L^{2}([0,T]^{2};)\). When \(k\) is a Hilbert-Schmidt kernel, the associated integral operator \(K\) is a well-defined operator on \(L^{2}([0,T];^{d})\), called a Hilbert-Schmidt integral operator. If a Hilbert-Schmidt kernel \(k\) is symmetric, i.e., \(k(t,)=k(,t)\) for all \(t,[0,T]\), then the associated operator is also symmetric in the sense that \( Kf,g= f,Kg\) for all \(f,g L^{2}([0,T];^{d})\). Throughout this paper, we will use the term 'kernel' to refer to a Hilbert-Schmidt kernel.

Positive semidefinite kernels.A symmetric operator \(K\) on a Hilbert space is said to be positive semidefinite and denoted by \(K 0\) if \( Kf,f 0\) for all \(f\). When a symmetric kernel \(k\) is associated with a positive semidefinite operator \(K\), i.e., \(_{0}^{T}_{0}^{T}k(t,)f(t)f()\,dtd 0\) for all \(f L^{2}([0,T];)\), we say that the kernel \(k\) is (integrally) positive semidefinite and denote it by \(k 0\). For continuous kernels, positive semidentiveness of \(k\) is equivalent to the following condition: \(_{i=1}^{n}_{j=1}^{n}c_{i}c_{j}k(t_{i},t_{j}) 0\) for any \(t_{1},,t_{n}[0,T]\) and \(c_{1},,c_{n}\), given \(n\).

**Proposition 1**.: _We summarize some basic properties of continuous positive semidefinite kernels:_

1. _For_ \( C([0,T];)\)_, the kernel_ \(k(t,)=(t)()\) _is positive semidefinite._
2. _For_ \(k_{1},k_{2} 0\)_, their product_ \(k(t,)=k_{1}(t,)k_{2}(t,)\) _is positive semidefinite._
3. _For_ \(k 0\)_, its anti-transpose_ \((t,) k(T-,T-t)\) _is also positive semidefinite._
4. _If_ \( C^{1}([0,T];_{ 0})\) _is an increasing function on_ \([0,T]\)_, then the symmetric kernel_ \(k\) _defined as_ \(k(t,)=()\) _for_ \(t\) _and_ \(k(t,)=(t)\) _for_ \(t\) _is positive semidefinite._3__ 5. _For_ \(k 0\)_, we have_ \(k(t,t) 0\) _for all_ \(t[0,T]\)_._

## 3 Continuous PEP for minimizing objective function value

In this section, drawing inspiration from its discrete-time counterpart , we propose a novel framework for analyzing the convergence rate of ODE models for first-order methods, called the _continuous-time performance estimation problem (Continuous PEP)_. To illustrate this framework, we use the accelerated gradient flow as an example. Detailed steps can be found in Appendix C. Su _et al._ derived the limiting ODE of Nesterov's AGM  as follows:

\[++ f(X)=0,\] (AGM ODE)

with initial conditions \(X(0)=x_{0}\) and \((0)=0\). Suppose we want to establish a convergence guarantee of AGM ODE in the form of

\[f(X(T))-f(x^{*})\|x_{0}-x^{*}\|^{2}.\] (3)

Here, we observe that the constant \(\) can be seen as an upper bound of the performance of AGM ODE for the criterion \((f(X(T))-f(x^{*}))/\|x_{0}-x^{*}\|^{2}\). To formalize this idea, we introduce the following optimization problem, which seeks to find the worst-case performance of the given ODE model:

\[_{f_{0}(^{d}; )\\ X C^{1}([0,T];^{d})} )}{\|x_{0}-x^{*}\|^{2}}\] (Exact PEP) subject to \[X\] is a solution to AGM ODE with \[X(0)=x_{0},\ (0)=0\] \[x^{*}\] is a minimizer of \[f,\]

where \(_{}(^{d};)\) denotes the set of continuously differentiable \(\)-strongly convex functions on \(^{d}\). This problem is useful to analyze the convergence properties of ODE models because the optimal value \(()\) of Exact PEP directly provides the guarantee (3) with \(=()\) regardless of any particular choice of \(f\).

### Relaxation of PEP

Exact PEP is challenging to solve due to the presence of an unknown function \(f\) as an optimization variable. To address this difficulty, we relax the constraint \(f_{0}(^{d};)\) with a set of inequalities that are satisfied by \(f_{0}(^{d};)\). Before that, we first note that AGM ODE can be expressed as the following continuous-time dynamical system (see ):

\[(t)=-_{0}^{t}H(t,) f(X())\,d\] (4)

by setting \(H(t,)=^{3}/t^{3}\). Here, \(H(t,)\) is called the _H-kernel_. We introduce two functions, \(:[0,T]\) and \(:[0,T]^{d}\), defined as follows:

\[(t)=-x^{*}\|^{2}}(f(X(t))-f(x^{*} )),(t)=-x^{*}\|} f(X (t)).\]

Using the chain rule and the convexity of \(f\), we can derive the following equality and inequality:

\[ 0&=(t)+(t), _{0}^{t}H(t,)()\,d,\\ 0&(t)+(t),v+_{0}^{ t}_{}^{t}H(s,)()\,ds\,d,\] (5)

where \(v=(x^{*}-x_{0})/\|x_{0}-x^{*}\|\). We can now relax Exact PEP by replacing its constraints with the equality and inequality above, resulting in the following problem:

\[_{,,v}& (T)\\ &\] (Relaxed PEP)

Since any feasible solution to Exact PEP can be transformed into a feasible solution to Relaxed PEP, we have \(()()\). Therefore, the convergence guarantee (3) holds with \(=()\) when using the proposed relaxation.

### Lagrangian dual of relaxed PEP

To obtain an upper bound on \(()\), we use Lagrangian duality. We introduce two _Lagrange multipliers_\(_{1} C^{1}([0,T];)\) and \(_{2} C([0,T];_{ 0})\), where we imposed certain regularity conditions, such as continuity and differentiability, to ensure that the dual problem is well-defined. We then define the Lagrangian function as

\[(,,v;_{1},_{2 })&=(T)-_{0}^{T}_{1}(t)((t)+ (t),_{0}^{t}H(t,)()\,d )\,dt\\ &-_{0}^{T}_{2}(t)((t)+ (t),v+_{0}^{t}_{}^{t}H(s,)()\,ds\,d )\,dt.\]

When expressed in terms of the inner products in function spaces, we have

\[(,,v;_{1},_{2 })&=(T)-_{1},_ {L^{2}([0,T];)}-_{2},_{L^{2}( [0,T];)}\\ &- K,_{L^{2} ([0,T];^{d})}-_{2}(t)v,(t)_{L^ {2}([0,T];^{d})},\] (6)

where \(K\) is the Hilbert-Schmidt integral operator with the symmetric kernel \(k\) defined by

\[k(t,)=_{1}(t)H(t,)+_{2}(t)_{}^{t}H(s,)\,ds, t.\]

The dual function is defined as \((_{1},_{2})=_{,,v}(,,v;_{1},_{2})\). By weak duality, we have \(()(_{1}, _{2})\) for any feasible dual solution \((_{1},_{2})\). After performing some computations, we obtain the following expression for the dual objective function (see Appendix C):

\[(_{1},_{2})= _{(0,)}\{:S_{_{1},_{2},} 0\}&_{1}(0)=0,\;_{1}(T)=1\;_{1}(t)=_{2}(t)\\ &,\] (7)where \(S_{_{1},_{2},}\) is a symmetric kernel on \([0,T]^{2}\) given by

\[S_{_{1},_{2},}(t,)=(_{1}(t)H(t,)+_ {2}(t)_{}^{t}H(s,)\,ds)-_{2}(t)_{2} (), t.\] (8)

We refer to \(S_{_{1},_{2},}\) as the _PEP kernel_. In Appendix H.2, we show that (8) can be viewed as the continuous-time limit of the discrete-time PEP kernel presented in .

To describe our framework, given \(_{}(0,)\), suppose that the PEP kernel \(S_{_{1},_{2},_{}}\) is positive semidefinite with appropriate multiplier functions \(_{1}\) and \(_{2}\). Then, \(_{}\) is a feasible solution of the minimization problem in (7), and thus \((_{1},_{2})_{}\). On the other hand, by weak duality, \(()( _{1},_{2})\). Therefore, we conclude that \(()() (_{1},_{2})_{}\), which implies that the convergence guarantee (3) automatically holds with \(=_{}\).

Using this approach, we can recover the known convergence guarantee for AGM ODE in .

**Proposition 2**.: _AGM ODE achieves the convergence rate (3) with \(=2/T^{2}\)._

Proof.: By choosing the multiplier functions \(_{1}(t)=t^{2}/T^{2}\) and \(_{2}(t)=2t/T^{2}\), we can compute the PEP kernel (8) as \(S_{_{1},_{2},}(t,)=(-})}\). Since the kernel \((t,) t\) is nonzero and positive semidefinite, we have \(S_{_{1},_{2},} 0\) if and only if \( 2/T^{2}\). Thus, we obtain \((_{1},_{2})=2/T^{2}\), which establishes the convergence guarantee (3) with \(=2/T^{2}\). 

**Remark 1**.: _Furthermore, this convergence guarantee is optimal among all possible guarantees obtained through the weighted integral of (5). The optimality of this rate follows from the fact that \((_{1},_{2})\) is the optimal solution to the dual problem \(_{_{1},_{2}}(_{1},_{2})\). See Appendix H.1 for details._

### Applying continuous PEP to various accelerated gradient flows

Note that the proposed method is not dependent on the choice of the H-kernel \(H(t,)\). Thus, it can be applied to arbitrary dynamics represented in the form of (4). Furthermore, while we have focused on the non-strongly convex case (\(=0\)) so far, the following paragraph demonstrates that our method can handle strongly convex objective functions by using a reparametrization technique.4

Reparametrization from \(_{}(^{d};)\) to \(_{0}(^{d};)\).Consider a \(\)-strongly convex objective function \(f\). Since the proposed method is tailored for non-strongly convex objective functions, we choose to work with the convex function \((x):=f(x)-\|x-x_{0}\|^{2}\) rather than working directly with \(f\). Accordingly, we consider the following alternative formulation for the dynamical system (4), which involves \(\) instead of \( f\):5

\[(t)=-_{0}^{t}H^{F}(t,)(X())\,d.\] (9)

The following theorem offers a general result that can be used to establish convergence guarantees for dynamical systems of the form (9).

**Theorem 1**.: _Let \(>0\) and \(^{F} C^{1}([0,T];_{ 0})\) such that \(0^{F}(0)<1\), \(^{F}(T)=1\), and \(^{F}(t) 0\) for all \(t(0,T)\). Then, any solution to the integro-differential equation (9) satisfies_

\[(X(T))-(x^{*})^{F}(0)((x_{0})- {f}(x^{*}))+\|x_{0}-x^{*}\|^{2},\]

_where \((x):=f(x)-\|x-x^{*}\|^{2}\), if the following PEP kernel is positive semidefinite:_

\[S^{F}(t,)=(^{F}(t)H^{F}(t,)+^{F}(t)_{ }^{t}H^{F}(s,)\,ds)-2^{F}(t)^{F}(), t,\] (10)

_where \(^{F}(t)=\{^{F}(t)(1-_{0}^{t}_{ 0}^{s}H^{F}(s,)\,d ds)\}\)._

The proof of Theorem 1 is done by finding a dual feasible point to the PEP and can be found in Appendix C. Below, we establish convergence rates for various ODE models using Theorem 1.

AGM-SC ODE.We consider the following dynamical system modeling Nesterov's AGM for strongly convex case [26, Equation 2.2.22] (see [48, Equation 7]):

\[+2+ f(X)=0.\] (AGM-SC ODE)

This ODE model can be written as (9) with \(H^{F}(t,)=(1+-t)e^{(-t)}\) (see Appendix F.1). To use Theorem 1, we choose the multiplier function as \(^{F}(t)=e^{(t-T)}\).6 The PEP kernel (10) can be computed as (see Appendix G.1)

\[S^{F}(t,)= e^{(-T)}-e^{-2T}, t .\] (11)

When \(=e^{-T}\), this kernel is written as \(S^{F}(t,)=e^{-2T}(e^{}-1)\), and is visualized in Figure 1. It is positive semidefinite since the function \( e^{}-1\) is a nonnegative increasing function (see Proposition 1 (d)). It follows from Theorem 1 that AGM-SC ODE achieves the following convergence guarantee:

\[(X(T))-(x^{*}) e^{-T}((x_{0})- (x^{*})+\|x_{0}-x^{*}\|^{2}),\] (12)

which is consistent with the well-known \(O(e^{-T})\) convergence rate of AGM-SC ODE.

Unified AGM ODE.Using a unified Bregman Lagrangian framework,  obtained the following ODE that unifies AGM ODE and AGM-SC ODE:7

\[+}{2}(_{t}+3_{t})+ f (X)=0,\] (Unified AGM ODE)

where \(_{t}\) and \(_{t}\) denote the corresponding hyperbolic functions with the argument \(}{2}t\). This ODE model can be written as (9) with \(H^{F}(t,)=(1+_{t}^{2}((_{t}^{2})-( _{}^{2})))_{t}_{t}\) (see Appendix F.2). We select the multiplier function as \(^{F}(t)=_{t}^{2}/_{T}^{2}\). With this choice, the PEP kernel (10) can be expressed as follows (see Appendix G.2):

\[S^{F}(t,)=(-_{T}^{2}) {_{t}_{}}{_{T}^{2}}+_{}_{ }^{2}}{_{T}^{2}}, t.\] (13)

We show that this kernel is positive semidefinite for \(=_{T}^{2}\). Proposition 1 (a) shows that the kernel \((t,)_{t}_{}\) is positive semidefinite. Proposition 1 (d) shows that the kernel \((t,)_{}^{2}\) is positive semidefinite because the function \(_{}^{2}\) is a nonnegative increasing function. Since the PEP kernel (13) with \(=_{T}^{2}\) can be expressed as a product of two positive semidefinite kernels, it is positive semidefinite by Proposition 1 (b). Consequently, Theorem 1 implies that Unified AGM ODE achieves the following convergence guarantee:

\[(X(T))-(x^{*})_{T}^{2} \|x_{0}-x^{*}\|^{2}.\] (14)

This guarantee aligns with the \(O(_{T}^{2})\) convergence rate reported in .

Figure 1: Visualization of the PEP kernel (11) for AGM-SC ODE.

TMM ODE.We consider the following novel limiting ODE for the _triple momentum method_ (TMM)  (see Appendix E.1 for the derivation and a comparison with the one in ):

\[+3+2 f(X)=0.\] (TMM ODE)

This ODE model can be written as (9) with \(H^{F}(t,)=-2e^{(-t)}+4e^{2(-t)}\) (see Appendix F.3). By setting the multiplier function as \(^{F}(t)=e^{2(t-T)}\), the PEP kernel (10) can be computed as (see Appendix G.3)

\[S^{F}(t,)=2(- e^{-2T})e^{(t+-2T)},\] (15)

which is positive semidefinite for \(= e^{-2T}\). Consequently, Theorem 1 implies that TMM ODE achieves the following convergence guarantee:

\[(X(T))-(x^{*}) e^{-2T}((x_{0})- (x^{*})+\|x_{0}-x^{*}\|^{2}),\] (16)

which is new to the literature. In Appendix H.3, we show that this convergence rate match aligns with the known convergence guarantee for TMM in the discrete-time case.

ITEM ODE.We consider the following new limiting ODE of the _information-theoretic exact method_ (ITEM)  (see Appendix E.2 for the derivation of ITEM ODE):

\[+3_{t}+2 f(X)=0,\] (ITEM ODE)

where \(_{t}\) denotes the corresponding hyperbolic function with the argument \(t\). This ODE model can be written as (9) with \(H^{F}(t,)=4_{}_{}_{t}_{t}^{2}+2_{} _{t}(1-2_{t}^{2})\) (see Appendix F.4). By choosing the multiplier function as \(^{F}(t)=^{2}(t)/^{2}(T)\), the PEP kernel (10) can be computed as (see Appendix G.4)

\[S^{F}(t,)=2_{T}^{2}(-_{T}^{2})_{t}_{}.\] (17)

For \(=_{T}^{2}\), this kernel is positive semidefinite. It follows from Theorem 1 that ITEM ODE achieves the following convergence guarantee:

\[(X(T))-(x^{*})_{T}^{2}\|x_{0}-x^{*}\| ^{2},\] (18)

which is a novel result. In Appendix H.4, we show that this guarantee matches the known convergence rate for ITEM in the discrete-time case.

## 4 Continuous PEP for minimizing velocity and gradient norm

In this section, we present a result analogous to Theorem 1 to address convergence rates on the squared velocity norm \(\|(T)\|^{2}\) or the squared gradient norm \(\| f(X(T))\|^{2}\). For continuous-time ODE models, the analysis of convergence rates on the squared gradient norm \(\| f(X(T))\|^{2}\) was first presented in . However, their argument relies on the use of L'Hopital's rule, which might give the impression that their approach is based on a clever trick or appears somewhat mysterious.

Translating convergence rates on \(\|(t)\|^{2}\) into convergence rates on \(\| f(X(T))\|^{2}\)

In this subsection, we present a novel approach for establishing the convergence guarantee of ODE models on the squared gradient norm \(\| f(X(T))\|^{2}\). The crucial insight lies in expressing \( f(X(T))\) as \(_{0}^{T} f(X())_{T}()\,d\), where \(_{T}\) denotes the Dirac delta function centered at \(=T\). Suppose we have a guarantee of the following form:

\[\|_{0}^{T}_{t}() f(X())\,d\|^{2} (f(x_{0})-f(x^{*})),\] (19)

where \(X C^{1}([0,T];^{d})\) and \(\{_{t}\}\) is a family of functions parametrized by \(t(0,T)\). In particular, we note that a convergence guarantee on \(\|C(t)(t)\|^{2}\) of the dynamics (4) can be written as (19) with \(_{t}()=C(t)H(t,)\). A well-known argument for constructing the Dirac delta function (see[35, Section 3.2]) shows that the weighted integral \(_{0}^{T}_{t}() f(X())\,d\) converges to \( f(X(T))\) as \(t T\), if the following conditions hold: \((i)\ _{t}() 0\), \((ii)_{0}^{T}_{t}()\,d 1\) as \(t T\), and \((iii)\) for every \((0,T)\), we have \(_{0}^{}_{t}()\,d 0\) as \(t T\). When \(_{t}\) satisfies these properties, we say that the function \(_{t}\) converges to the Dirac delta function \(_{T}\). Consequently, taking the limit \(t T\) in (19) yields the following guarantee on \(\| f(X(T))\|^{2}\):

\[\| f(X(T))\|^{2}(f(x_{0})-f(x^{*})).\]

### Convergence analysis via positive semidefinite kernels

In this subsection, we introduce a variant of continuous PEP that establishes the convergence rate on \(\|(T)\|^{2}\) through checking the positive semidefiniteness of the PEP kernel. Notably, this methodology can also prove convergence rates on \(\| f(X(T))\|^{2}\) because the convergence rates on \(\|(t)\|^{2}\) can be translated into those on \(\| f(X(T))\|^{2}\), as discussed in the previous subsection.

Reparametrization to time-varying functions.In Section 3.3, we employed a reparametrization technique to deal with strongly convex objective functions. In this section, we first apply the same technique again, leading to the following expression:8

\[(t)=-_{0}^{t}H^{G}(t,)(X())\,d,\] (20)

where \((x):=f(x)-\|x-x_{0}\|^{2}\). However, we do not proceed directly with this form. Instead, we introduce an additional reparametrization step. Given a solution \(X\) to (20), and a function \(^{G} C^{1}([0,T);_{ 0})\), we define a family of functions \(\{_{t}\}_{t[0,T)}\) as \(_{t}(x):=^{G}(t)(x)-_{0}^{t}^{G }()(X())\,d,x\). Then, we can show that (20) can be equivalently written in the following form (see Appendix D.1):

\[(t)=-_{0}^{t}^{G}(t,)_{}(X())\,d,\] (21)

for some kernel \(^{G}\). The following theorem is analogous to Theorem 1 for our current purpose.

**Theorem 2**.: _Let \(>0\), \(t_{ end}(0,T]\), \(^{G} C([0,t_{ end}],)\), and \(^{G} C^{1}([0,t_{ end}];_{ 0})\) such that \(^{G}(0)=1\) and \(^{G}(t) 0\) for all \(t\). Then, any solution to (21) satisfies_

\[\|_{0}^{t_{ end}}^{G}()_{}(X())\, d\|^{2}_{x^{d}}\{(x_{0})-( x)\},\] (22)

_if the following PEP kernel defined on \([0,t_{ end}]^{2}\) is positive semidefinite:_

\[S^{G}(t,)=^{G}(t,)-2^{G}(t)^{G}(), t .\] (23)

_In particular, the choice \(^{G}(t)=C(t_{ end})^{G}(t_{ end},t)\) gives a guarantee on \(\|C(t_{ end})(t_{ end})\|^{2}\)._

The proof of Theorem 2 can be found in Appendix D. We now use this theorem to establish convergence rates of the anti-transposed dynamics9 of the ODE models studied in Section 3.3.

Ogm-G ODE.By taking the limit of the stepsize in OGM-G , Suh _et al._ obtained the following ODE model for the non-strongly convex case (\(=0\)):10

\[++ f(X)=0.\] (OGM-G ODE)

This ODE model is the anti-transposed dynamics of AGM ODE, as it can be expressed as (20) with \(H^{G}(t,)=(T-t)^{3}/(T-)^{3}\) (see Appendix F.5). To use Theorem 2, we choose \(^{G}(t)=T^{2}/(T-)^{3}/(T-)^{3}\)\(t\))2.11 We set the terminal time \(t_{ end}\) before \(T\) and apply a limiting argument to prove the convergence rate on \(\| f(X(T))\|^{2}\). By setting \(^{G}(t)=C(t_{ end})^{G}(t_{ end},t)\) with \(C(t_{ end})=1/(T-t_{ end})\), we can compute the PEP kernel (23) as (see Appendix G.5)

\[S^{G}(t,)=(-})}.\] (24)

Since this kernel is the anti-transpose of the PEP kernel for AGM ODE in the proof of Proposition 2, we have \(S^{G}(t,) 0\) when \(=2/T^{2}\) by Proposition 1 (c). By Theorem 2, we obtain the following inequality:

\[\|_{0}^{t_{ end}})^{2}}{(T-)^{3}} f (X())\,d\|^{2}=\|(t_{ end})}{T-t_{ end} }\|^{2}}_{x^{d}}\{f(x_{0})-f(x)\}.\] (25)

Observing that the function \()^{2}}{(T-)^{3}}_{[0,t_{ end }]}()\) converges to \(_{T}\) as \(t_{ end} T\), we have \(_{0}^{t_{ end}})^{2}}{(T-)^{3}} f(X( )) f(X(T))\) as \(t_{ end} T\).12 Substituting this result into (25), we deduce the following convergence guarantee:

\[\| f(X(T))\|^{2}}_{x^{d}} \{f(x_{0})-f(x)\},\] (26)

which recovers the known convergence rate of OGM-G ODE in .

AGM-SC ODE.We now analyze the convergence of AGM-SC ODE in terms of the squared velocity norm, using Theorem 2. This ODE model is the anti-transposed dynamics of itself, as it can be expressed as (20) with \(H^{G}(t,)=(1+-t)e^{(-t)}\). We choose \(^{G}(t)=e^{t}\) and \(t_{ end}=T\). By setting \(^{G}(t)=C(T)^{G}(T,t)\) with \(C(T)=/2\), the PEP kernel (23) is expressed as (see Appendix G.6)

\[S^{G}(t,)= e^{-t}-e^{-2T}, t.\] (27)

Since this kernel is the anti-transpose of (11), it is positive semidefinite when \(=e^{-T}\) by Proposition 1 (c). Therefore, we conclude that AGM-SC ODE achieves the following convergence guarantee:

\[\|}{2}(T)\|^{2}e^{-T}_{x^{d}}\{(x_{0})-(x)\},\] (28)

which is new to the literature. A numerical experiment for this guarantee can be found in Appendix I.1.

Unified AGM-G ODE.In , the following unified AGM-G ODE is proposed:

\[+}{2}(_{T-t}+3_{T-t})+  f(X)=0,\]

where \(_{T-t}\) and \(_{T-t}\) denote the corresponding hyperbolic functions with the argument \(}{2}(T-t)\). This ODE model is the anti-transposed dynamics of Unified AGM ODE, as it can be expressed as (20) with \(H^{G}(t,)=(1+_{T-}^{2}((_{T-}^{2})- (_{T-t}^{2})))_{T-t}}{_{T- }_{T-}}\) (see Appendix F.6). To identify its convergence rate in terms of the squared gradient norm, we choose \(^{G}(t)=_{T-t}^{2}/_{T}^{2}\). By setting \(^{G}(t)=C(t_{ end})^{G}(t_{ end},t)\) with \(C(t_{ end})=}{2}_{T-t_{ end}} _{T-t_{ end}}\), the PEP kernel (23) is expressed as (see Appendix G.7)

\[S^{G}(t,)=(-_{T}^{2})_{T-}}{_{T}^{2}}+_{T-} _{T-t}^{2}}{_{T}^{2}}, t.\] (29)

Since this kernel is the anti-transpose of (13), it is positive semidefinite when \(=_{T}^{2}\) by Proposition 1 (c). Therefore, Unified AGM-G ODE achieves the following convergence guarantee:

[MISSING_PAGE_FAIL:10]