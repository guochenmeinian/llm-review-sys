# Vision Mamba Mender

Jiacong Hu\({}^{1,3}\), Anda Cao\({}^{1}\), Zunlei Feng\({}^{2,3,4}\),

**Shengxuming Zhang\({}^{2}\), Yi Wang\({}^{1}\), Lingxiang Jia\({}^{1}\), Mingli Song\({}^{1,3,4}\)**

\({}^{1}\)College of Computer Science and Technology, Zhejiang University,

\({}^{2}\)School of Software Technology, Zhejiang University,

\({}^{3}\)State Key Laboratory of Blockchain and Data Security, Zhejiang University,

\({}^{4}\)Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security

{jiaconghu,caoanda,zunleifeng}@zju.edu.cn,

{zsxm1998,y_w,lingxiangjia,brooksong}@zju.edu.cn

Corresponding author.

###### Abstract

Mamba, a state-space model with selective mechanisms and hardware-aware architecture, has demonstrated outstanding performance in long sequence modeling tasks, particularly garnering widespread exploration and application in the field of computer vision. While existing works have mixed opinions of its application in visual tasks, the exploration of its internal workings and the optimization of its performance remain urgent and worthy research questions given its status as a novel model. Existing optimizations of the Mamba model, especially when applied in the visual domain, have primarily relied on predefined methods such as improving scanning mechanisms or integrating other architectures, often requiring strong priors and extensive trial and error. In contrast to these approaches, this paper proposes the Vision Mamba Mender, a systematic approach for understanding the workings of Mamba, identifying flaws within, and subsequently optimizing model performance. Specifically, we present methods for predictive correlation analysis of Mamba's hidden states from both internal and external perspectives, along with corresponding definitions of correlation scores, aimed at understanding the workings of Mamba in visual recognition tasks and identifying flaws therein. Additionally, tailored repair methods are proposed for identified external and internal state flaws to eliminate them and optimize model performance. Extensive experiments validate the efficacy of the proposed methods on prevalent Mamba architectures, significantly enhancing Mamba's performance. For more information, please visit [https://vision-mamba-mender.github.io/](https://vision-mamba-mender.github.io/).

## 1 Introduction

Deep learning has demonstrated outstanding performance in various fields of artificial intelligence, with deep neural networks such as convolutional neural networks (CNNs)  and Vision Transformer  dominating the field of computer vision. However, the limited receptive field  of CNNs and the quadratic computational complexity  of Transformers constrain their further development. To overcome these limitations, an increasing number of studies have attempted to propose more advanced models . Recently, Mamba , based on the state space model , has become the focus of these research efforts. Mamba can maintain nearly linear computational complexity while achieving a global receptive field, leading to outstanding performance. Consequently, it has been widely adopted in the field of computer vision .

To enhance the performance of Mamba in visual tasks, existing works mainly focus on improving the architecture of Mamba [17; 18; 19; 20; 21]. For instance, Zhu et al. proposed Vision Mamba, which adds branches to the original Mamba to simultaneously process image sequences in both forward and backward directions. Nearly simultaneously, Liu et al. introduced V Mamba, which adopts a four-way scanning strategy on top of the original Mamba to achieve a more comprehensive global receptive field. Other improvements include PlainMamba , Mamba-ND , SiMBA , and MambaMixer , all of which are variants of the original Mamba framework.

However, the aforementioned methods that optimize the Mamba model by improving its architecture are predefined and require strong prior knowledge and extensive trial and error. Additionally, Yu et al.  recently pointed out in their latest research that the current improvements made to Mamba-based visual models are unnecessary for visual tasks, especially for visual recognition tasks. This opposing view underscores the necessity and urgency of further optimizing Mamba models in the field of computer vision. Hence, unlike the aforementioned pre-optimization methods, this paper attempts to analyze the working mechanism of Mamba from a post-perspective, identify the reasons of flaws leading to incorrect prediction results, and automatically rectify them to further enhance the performance of Mamba models. Moreover, this approach is applicable to all Mamba-like models.

Based on this idea, in this paper, we propose Vision Mamba Mender, a systematic approach to understanding the working mechanism of Mamba from a post-perspective, identifying flaws within it, and rectifying them to ultimately improve model performance. In understanding the operational mechanism of the Mamba model, we categorize the computational process of Mamba into external state interaction and internal state interaction.

Along these two perspectives of external and internal states, we introduce a state correlation analysis method tailored for Mamba to establish the correlation between hidden states and predicted results. Additionally, we define external state correlation scores and internal state correlation scores to quantitatively analyze differences in state correlations, revealing flaws existing respectively in the external and internal states. Specifically, external state flaws refer to instances where correct model predictions in certain states are predominantly associated with foreground regions, while incorrect predictions are primarily linked to background regions. Internal state flaws, on the other hand, pertain to cases where correct predictions within a class are correlated with the same regions within the state and exhibit low overall complexity, whereas incorrect predictions within the class focus on different internal regions of the state and demonstrate higher overall complexity.

Furthermore, we propose repair methods tailored for addressing both external state flaws and internal state flaws. Specifically, in the repair of external state flaws, we impose constraints on the external state correlations within certain modules of Mamba to increase their correlation with foreground information during prediction. On the other hand, in the repair of internal state flaws, we impose constraints on the internal state correlations within certain modules of Mamba to enhance their correlation with genuine class-specific internal information during prediction. Through extensive experimentation, we demonstrate that the proposed Vision Mamba Mender is applicable to state-of-the-art Vision Mamba architectures.

The contributions of this paper are summarized as follows:

* We propose a novel post-hoc optimization method named Vision Mamba Mender. This method is applicable to existing state-of-the-art Vision Mamba architectures, identifying and repairing flaws in the Mamba model's mechanisms for visual recognition tasks from a post-hoc perspective, ultimately enhancing the model's performance.
* We introduce a state correlation analysis method and correlation score definitions for Mamba from both external and internal hidden state perspectives. These methods are used to identify flaws. Additionally, we introduce a state correlation constraint method to rectify these flaws.
* Extensive experiments demonstrate that the proposed Vision Mamba Mender can effectively identify and repair flaws in the Mamba model without introducing additional parameters, significantly improving the performance of the Vision Mamba.

## 2 Preliminaries

Mamba  is a novel and efficient sequence modeling model composed of multiple uniformly stacked Mamba blocks. Each Mamba block is constructed based on a selective state space model (SSM). Unlike the previous time-invariant SSM , it allows the parameters of the SSM to depend on the input, enhancing the model's expressive capacity. Furthermore, inspired by H3  and Gated MLP , each Mamba block also incorporates modules such as causal convolution and gating mechanisms.

As illustrated in Figure 1, given the hidden state \(h_{n}^{()-1}\) of the \(i\)-th token in the input of the \(\)-th Mamba block, the computation of the hidden state \(h_{n}^{()}\) of the \(i\)-th token in the output of the \(\)-th Mamba block is as follows:

\[x_{n}^{()} =(h_{n}^{()-1} W_{x}^{()}), \] \[c_{1}^{()},c_{2}^{()},,c_{n}^{()} =(x_{1}^{()},x_{2}^{()},,x_{n} ^{()}),\] (2) \[s_{n}^{()} =(c_{1}^{()},c_{2}^{()},,c_{n}^{( )}),\] (3) \[z_{n}^{()} =(h_{n}^{()-1} W_{n}^{()}),\] (4) \[y_{n}^{()} =(s_{n}^{()} z_{n}^{()}) W_{y}^{()}, \]

where SiLU\((.)\), causal-Conv1D\((.)\), and selective-SSM\((.)\) denote the activation function, the casual 1D convolution, and the selective state model, respectively. \(W_{x}^{()}\), \(W_{z}^{()}\), and \(W_{y}^{()}\) are the projection matrices for the linear operations in the Mamba block, while \(x\), \(z\), \(c\), \(s\), and \(y\) represent the intermediate states within the Mamba block.

Finally, by applying a residual connection, the hidden state \(h_{n}^{()}\) is obtained as follows:

\[h_{n}^{()}=h_{n}^{()-1}+y_{n}^{()}. \]

In Eqn. (5), \(\) denotes the Hadamard product. The term \(z_{n}^{()}\) in Eqn. (4) is computed through a separate pathway, serving as a gating mechanism to regulate the information flow in the main pathway.

However, the original Mamba block is designed for one-dimensional sequences and is not suitable for handling multidimensional visual data, particularly for vision tasks requiring spatial awareness. Existing vision Mamba architectures enhance the basic Mamba block to accommodate these requirements, such as ViM , V Mamba , PMamba , Mamba-ND , and SiMBA . Unlike the predefined optimizations, our approach analyzes the working mechanism of the Mamba model post-hoc, identifying flaws and making repairs to further enhance the model's performance.

## 3 Where Do Flaws Occur?

Identifying flaws in Mamba first requires an understanding of how Mamba operates. Some studies have provided empirical evidence to elucidate the mechanisms of Mamba models in the NLP domain, such as their contextual learning ability , factual recall capability , and interpretability . However, elucidating the operational mechanisms of Mamba models in the visual domain remains a significant challenge. Ali et al.  established a connection between the selective SSM within the Mamba block and the self-attention mechanism in Transformers, allowing the selective SSM to represent the interaction process between any two states by constructing a self-attention matrix, which is utilized for image feature attribution. However, focusing solely on the selective SSM within the Mamba block is far from sufficient, as causal convolution, as shown in Eqn. (2), also participates in the interaction between states. Moreover, other computational modules within the Mamba block used for state interaction must also be considered.

In this section, we first investigate the working mechanisms of Mamba. We summarize the computational processes within Mamba as state interactions1. These interactions are categorized into _external state interactions_ (where a state interacts with other states to form a new state, as shown in Eqn. (2) and (3) and _internal state interactions_ (where a state interacts only with its internal information to form a new state, as shown in Eqn. (1), (4), and (5). We explore the operational mechanisms of Mamba from both the external and internal state interaction perspectives to identify flaws.

Figure 1: The computational process of a Mamba block.

### External State Correlation Analysis

To understand the impact of external states on predictions within Mamba's working mechanisms, we propose the Grad-ESC method, inspired by Grad-CAM , which uses gradient and activation information to assess the importance of each neuron in decision-making. Grad-ESC calculates the correlation between external states and the model's prediction outcomes, termed external state correlation. Unlike the attention mechanism derived by Ali et al. , which applies only to the selective SSM module within the Mamba block, Grad-ESC allows for correlation analysis of the outputs from any module within the Mamba block.

Specifically, given the predictive distribution \(p\) output by the Mamba model and the true class \(k\) of the input sample, the calculation process for external state correlation \(^{(,s)}^{H W}\) (where \(H\) and \(W\) represent the height and width of the input image) of the states \(\{s_{n}^{()}\}_{n=1}^{N}\) output by the selective SSM module (where \(s_{n}^{()}^{D}\), \(N\) and \(D\) are the number and dimensionality of the states) is as follows:

\[^{(,s)}=(_{1}^{()},_{2}^{ ()},,_{N}^{()}),\,_{n}^{()}=_{D}(g^{(,s)} s_{n}^{()}),\,g^{(,s)}=_{n=1}^{N} }{ s_{n}^{()}}, \]

where \(g^{(,s)}^{D}\) represents the weights used to weight the state \(s_{n}^{()}\) along its dimensions based on gradient information, \(_{D}()\) denotes the average across all dimensions, \(_{n}^{()}\) signifies the degree of correlation between the \(n\)-th state and the prediction after integrating gradient information, and \(\) denotes the operation of reshaping and scaling to the original image size after retaining only the states corresponding to the image patches.

As illustrated in Figure 2, taking the state \(s_{n}^{()}\) as an example, we visualize the external state correlations of \(s_{n}^{()}\) in different blocks of ViM . It can be observed that the external state correlations of \(s_{n}^{()}\) vary across different blocks; some blocks exhibit correlations with foreground regions (such as the 1st layer), while others correlate with background regions (such as the 2nd layer). We consider correlations with the foreground to be interpretable, whereas correlations with the background are deemed uninterpretable. To quantify the interpretability of external state correlations, we propose the following definition of an external state correlation score based on commonly used interpretable evaluation methods such as perturbation test [29; 31] and segmentation test [32; 30; 33]:

**Definition 1** (External Correlation Score).: _Given a pre-trained Mamba model \(()\), an input image \(i\), foreground annotations \(m\) of the input image, and external state correlation \(^{(,s)}\) computed through the proposed method, the external correlation score is defined as follows:_

\[(^{(,s)})=((^{(,s)}+ i))}{((^{( ,s)-} i))}}_{}( ^{(,s)+},m)}_{}, \]

where \(^{(,s)+}=(^{(,s)})\) denotes the retention of important regions (those greater than the threshold \(\)) in \(^{(,s)}\), setting them to 1, and the removal of unimportant regions (those less than the threshold \(\)), setting them to 0, for use in negative perturbation tests. Conversely, \(^{(,s)-}=(^{(,s)}<)\) denotes the removal of important regions and the retention of unimportant regions, for use in positive perturbation tests. Additionally, \((^{(,s)})[0,+)\), where a higher value indicates a higher correlation score.

Figure 2: Visualization of the external state correlation \(^{(,s)}\) of the output \(s_{n}^{()}\) from the selective-SSM module in different ViM  blocks. The depth of the ViM blocks increases from left to right.

### Internal State Correlation Analysis

To comprehend the influence of internal states on prediction outcomes within the operational mechanism of Mamba, we introduce the Grad-ISC method. This method is utilized for computing the degree of correlation between the internal states and model predictions, termed as the state-internal correlation.

Specifically, given the predicted distribution \(p\) outputted by the Mamba model and the true class \(k\) of the input sample, let's take the example of the \(n\)-th state \(x_{n}^{()}^{D}\), which is the output of the linear mapping matrix \(W_{x}^{()}\). The computation process for the corresponding internal state corelation \(_{n}^{(,x)}^{D}\) is as follows:

\[_{n}^{(,x)}=g_{n}^{(,x)} x_{n}^{()},g_{n}^{(,x) }=}{ x_{n}^{()}}, \]

where \(g_{n}^{(,x)}^{D}\) denotes the weights applied to the dimensions of state \(x_{n}^{()}\) using gradient information. Similarly, the Grad-ISC method can perform internal state correlation analysis on the output of any module within the Mamba block.

Continuing with the example of state \(x_{n}^{()}\), we visualize the internal state correlations \(_{n}^{(,x)}\) for samples belonging to the same class in ViM , as shown in Figure 3. It can be observed that for the same class, the regions of internal state correlation are relatively consistent. We posit that the more consistent and simpler the internal state correlation regions are for samples of the same class, the more interpretable they are. Conversely, the more inconsistent and complex the internal state correlation regions are for samples of the same class, the more difficult they are to interpret. To quantify the interpretability of internal state correlations, we propose a novel definition for the Internal Correlation Score:

**Definition 2** (**Internal Correlation Score**).: _Given \(J\) samples belonging to the same class and the internal state correlation \(_{n}^{(,x)}^{D}\) computed using the proposed method for a particular sample, the Internal Correlation Score is defined as follows:_

\[(_{n}^{(,x)})=_{D}( _{j=1}^{J}_{n,j}^{(,x)+})}_{} _{D}(_{j=1}^{J}_{n,j}^{( ,x)+}}{_{n,1}^{(,x)+}_{n,2}^{(,x)+} _{n,J}^{(,x)+}})}_{}, \]

where \(_{n,j}^{(,x)}\) represents the internal state correlation for the \(j\)-th sample, \(_{n,j}^{(,x)+}=(_{n,j}^{(,x)})\) denotes the binarized internal correlation (set to 1 if greater than \(\), otherwise set to 0), \(\) denotes the XOR operation, and \(_{D}()\) denotes the average over all dimensions. Furthermore, \((^{(,x)})[0,+)\), where a higher value indicates a higher correlation score.

### Identifying Flaw through Correlation Analysis

To analyze flaws within the Mamba model, we examined the external and internal state correlation scores under different conditions using the ViM model and the ImageNet-10 dataset. This analysis allowed us to observe variations in correlation relationships across different states.

Figure 3: Visualization of the internal state correlations \(_{n}^{(,x)}\) of the output states \(s_{n}^{()}\) from the linear mapping module \(W_{s}^{()}\) in ViM  for samples of the same class. The horizontal axis represents the state dimensions, and the vertical axis represents the samples.

Flaws in External State Correlation.To uncover flaws in the external correlations of states, we compare the scores of external state correlations in the Mamba model between simple and difficult samples, as illustrated in Fig. 4(a) and (b). For simple samples, it can be observed that across different blocks, the external correlation scores of states \(x_{n}^{()}\), \(c_{n}^{()}\), \(s_{n}^{()}\), and \(z_{n}^{()}\) are all better than those of state \(y_{n}^{()}\), especially in deeper blocks. Furthermore, by comparing simple samples (a) with difficult samples (b), it can be noted that the external correlation scores of all states in all blocks have decreased. This indicates that for difficult samples, the Mamba model tends to associate certain incomprehensible regions in the external states.

Flaws in Internal State Correlation.To unveil patterns in internal state correlations and detect flaws within them, we also conducted a comparative analysis of the internal correlation scores within the Mamba model, as depicted in Figure 4(c) and (d). For both simple and difficult samples, the internal correlation score of state \(x_{n}^{()}\) is generally better than that of other states. Furthermore, simultaneous comparison of simple and difficult samples reveals a decrease in the internal correlation scores of all states, including state \(x_{n}^{()}\). This suggests that for difficult samples, the Mamba model tends to correlate with some incomprehensible regions within the internal states.

## 4 How to Repair Flaws?

In principle, repairing flaws within the model's internals can enhance Mamba's decision-making process and improve its performance. However, there has been limited research specifically addressing such issues in Mamba models, particularly when applied in the domain of visual processing. Therefore, in this section, we investigate post-hoc flaw repair in the Mamba model from two perspectives: external state correlation and internal state correlation.

### External State Correlation Repair

To repair flaws related to external state correlations, it is essential to identify the key components within the Mamba block that need fixing. In flaw identification regarding external state correlations, it is observed that the states \(x_{n}^{()}\), \(c_{n}^{()}\), \(s_{n}^{()}\), and \(z_{n}^{()}\) exhibit flaws when predicting difficult samples. For these states, the external correlation scores for simple samples are higher than those for difficult samples. This implies that the model primarily correlates with foreground regions in correct predictions and with background regions in incorrect predictions. Furthermore, considering that external state interactions occur only within Conv and SSM modules (as indicated by Eqn.(2) and Eqn.(3)), we empirically suggest that the Conv and SSM components are crucial for influencing the model's anomalous decisions. Theoretically, it is also feasible to apply external flaw repair to other states.

Figure 4: Comparison of external and internal state correlation scores across different blocks in the Mamba model between simple and difficult samples. **(a)** and **(b)** show the external state correlation scores for simple and difficult samples, respectively. **(c)** and **(d)** present the internal state correlation scores for simple and difficult samples, respectively.

We focus on repairing the external correlation flaws of the states \(c_{n}^{()}\) and \(s_{n}^{()}\) output by the Conv and SSM modules in the deeper blocks. Specifically, we first identify difficult samples from the training set and then constrain the external correlations of the hidden states \(c_{n}^{()}\) and \(s_{n}^{()}\) using foreground annotations \(m\):

\[_{}=_{HW}(^{(,c)} m)+_{HW}(^{(,s)} m) \]

where \(_{HW}\) denotes the averaging operation over the two-dimensional matrix. It is important to note that during backpropagation, each term in the computational graph of \(_{}\) is differentiable, which involves second-order gradients as specified in Eqn. (7). The complete loss function, combining with the original task loss, is formulated as follows:

\[=_{}+_{}  \]

where \(_{}\) denotes the cross-entropy loss for the image recognition task, and \(\) serves to balance the magnitudes of the respective loss components.

### Internal State Correlation Repair

Similarly, when identifying flaws related to internal state correlations, it is observed that states in different blocks exhibit flaws when predicting difficult samples. For instance, the internal correlation scores of state \(x_{n}^{()}\) for simple samples are higher than those for difficult samples. This suggests that in predictions biased toward correctness within a class, the model aligns with internally consistent regions of the state, characterized by lower overall complexity. Conversely, in predictions biased toward incorrectness within a class, the model tends to focus on internally inconsistent regions of the state with higher overall complexity. In our experiments, we consider the linear mapping \(W_{s}^{()}\) within deeper blocks as a critical component influencing the model's predictions. However, theoretically, it is also feasible to apply internal flaw repair to other states.

We focus on repairing the internal correlation flaws of the states \(x_{n}^{()}\) output by the linear mapping \(W_{x}^{()}\) within deeper blocks. Specifically, we first select \(J\) simple samples for each class from the training set and create corresponding internal correlation templates \(}_{n}^{(,x)+}=1-(_{j=1}^{J}_{n,j} ^{(,x)+})\) for each class. We then utilize these templates to constrain the internal correlations of \(x_{n}^{()}\):

\[_{}=_{D}(_{n}^{(,x)}_{n}^{(,s)+}) \]

Similarly, during backpropagation, each term in the computational graph of \(_{}\) is also differentiable, which involves second-order gradients as specified in Eqn. (9). Combined with the loss for the original task, the complete loss is as follows:

\[=_{}+_{}  \]

where \(\) is a balancing factor to adjust the scale between loss functions.

It is important to note that external and internal state corrections represent two distinct perspectives, allowing these two methods to be used orthogonally. To maximize flaw repair, we recommend sequentially repairing external flaws (Eqn. (11)) followed by internal flaws (Eqn. (13)).

### Results of Flaw Repair

We evaluated the proposed flaw repair method on five state-of-the-art Mamba models for visual tasks, including ViM-T , V Mamba-T , SiMBA-S , EfficientVMamba-T (EMamba-T) , and LocalVim-T . To ensure smooth operation of the Mamba models within limited computational resources, we modified certain model parameters, such as the number of blocks and the dimensionality of hidden states. Additionally, we conducted experiments on three scales of the ImageNet  dataset: ImageNet-50, ImageNet-300, and ImageNet-1K. To obtain the image foreground annotations \(m\) as defined in **Definition 1** from the ImageNet dataset, we utilized annotations from the ImageNet-S dataset . These annotations correspond to those used during training, covering all 50 classes in ImageNet-50, 300 classes in ImageNet-300, and 919 classes in ImageNet-1K, with an average of 10 foreground-labeled samples per class. Further details on experimental settings are provided in Appendix B.

Comparing State Correlation Scores Before and After Repair.To validate the effectiveness of the flaw repair methods, we compared the internal and external state correlation scores of difficult samples before and after flaw repair, as shown in Figure 5. It can be observed that the proposed flaw repair methods effectively enhance the correlation scores of the Mamba model, thereby improving the predicted correlation regions within both internal and external states. Specifically, for the ViM model, after external state flaw repair, the external correlation scores of the states \(c_{n}^{()}\) are significantly higher than before repair. Similarly, after internal state flaw repair, the internal correlation scores of the states \(x_{n}^{()}\) are also notably higher than before repair.

Repairing External and Internal Flaws in Different States.As shown in Table 1 (second row), we conducted experiments on external flaw repair for different states within the same block. The results indicate that simultaneously performing external flaw repair on states \(c_{n}^{()}\) and \(s_{n}^{()}\) leads to an improvement in model accuracy. This aligns with our findings in the main text regarding flaw detection, where states \(x_{n}^{()}\), \(c_{n}^{()}\), \(s_{n}^{()}\), and \(z_{n}^{()}\) exhibit flaws when predicting challenging samples. Furthermore, since the Conv and SSM modules in each block facilitate external interactions, applying external flaw repair to states \(c_{n}^{()}\) and \(s_{n}^{()}\) is effective.

Similarly, as illustrated in Table 1 (third row), we performed experiments on internal flaw repair for different states within the same block. The results demonstrate that individually addressing internal flaws in state \(x_{n}^{()}\) also results in improved model accuracy. This supports our earlier findings regarding flaw detection, where state \(x_{n}^{()}\) shows flaws when predicting challenging samples, thus validating the effectiveness of internal flaw repair for state \(x_{n}^{()}\).

Repairing SOTA Vision Mamba Models.As shown in the Table 2, we validated the proposed flaw repair methods on various mainstream Vision Mamba models. It can be observed that regardless of whether external or internal state flaw repair is performed, the accuracy of the repaired models exceeds

  
**Base** & \(x_{n}^{()}\) & \(c_{n}^{()}\) & \(s_{n}^{()}\) & \(z_{n}^{()}\) & \(y_{n}^{()}\) & \(x_{n}^{()}\)+\(c_{n}^{()}\) & \(x_{n}^{()}\)+\(s_{n}^{()}\) & \(x_{n}^{()}\)+\(z_{n}^{()}\) & \(c_{n}^{()}\)+\(s_{n}^{()}\) \\ 
76.44 & 78.40 & 78.44 & 78.28 & 77.96 & 78.28 & 78.56 & 78.04 & 78.04 & 78.48 \\
76.44 & 78.20 & 79.16 & 78.88 & 78.32 & 78.32 & 77.72 & 78.76 & 78.44 & 78.42 \\   

Table 1: Comparison of model accuracy after external state flaw repair for different states. Taking \({}^{*}\!x_{n}^{()}\)* as an example, it represents the external flaw repair of state \(x_{n}^{()}\). The results in the second row of the table correspond to external flaw repair, while the third row presents the outcomes of internal flaw repair. The experiment was conducted within the last block of the ViM model, using the ImageNet-50 dataset.

Figure 5: Comparison of external and internal state correlation scores across different blocks in the Mamba model before and after flaw repair. **(a)** and **(b)** show the external and internal state correlation scores before flaw repair, respectively. **(c)** and **(d)** present the external and internal state correlation scores after flaw repair, respectively.

that of the original models. Specifically, for external state flaw repair, on ImageNet-50, the accuracy of V Mamba increased by 1.12% after flaw repair, and the accuracy of SiMBA-S increased by 3.16%. For internal state flaw repair, on ImageNet-1K, the accuracy of ViM increased by 1.15% after flaw repair, and the accuracy of LocalViM increased by 2.20%. It is worth noting that external state flaw repair and internal state flaw repair can be orthogonal. For example, on ImageNet-50, simultaneously performing external state flaw repair and internal state flaw repair on ViM increased the accuracy by 2.04% and 1.76%, respectively, compared to performing each repair method individually. This resulted in an overall improvement of 3.24% compared to the original model.

## 5 Related Works

Model Optimization.Optimizing model performance through various prior designs or theoretical derivations has always been a pursuit in the field of artificial intelligence. Existing methods for optimizing the Mamba model, especially when applied to visual tasks, mainly involve adjusting the network architecture [17; 18; 19; 20; 21; 22]. For instance, improvements have been made through the bidirectional scanning mechanism , cross-scanning mechanism , continuous 2D scanning mechanism , and incorporating new channel modeling techniques . However, these enhancements are pre-designed and require extensive trial and error. For a more detailed understanding of Mamba's applications in computer vision, we recommend readers refer to recent surveys by Zhang et al.  and Xu et al. . So far, there has been no research focusing on post-hoc optimization to correct internal flaws within the Mamba model to improve its performance. Optimizing models beyond Mamba, such as Convolutional Neural Networks [3; 38] and Transformers [3; 39], is also primarily achieved through architectural improvements [2; 40; 41; 42; 4], feature enhancement [43; 44; 45; 46], or some post-hoc debugging methods [47; 48; 49; 50]. However, these approaches either cannot be directly applied to a brand-new Mamba model or do not involve post-hoc optimization. In summary, unlike the aforementioned studies, the proposed Vision Mamba Mender is the first framework dedicated to post-hoc analysis and optimization of the Mamba model. Its aim is to rectify flaws within Mamba models in the domain of vision, thereby further enhancing the performance of Mamba models.

    & **ViM-T** & **V Mamba-T** & **SiMBA-S** & **EMamba-T** & **LocalVim-T** \\    \\ 
**Base** & 76.44 & 79.96 & 81.32 & 81.44 & 75.08 \\
**+Ext** & 78.48+2.04 & 81.08+1.12 & 84.48+3.16 & 82.68+1.24 & 78.68+3.60 \\
**+Int** & 78.20+1.76 & 82.36+2.40 & 84.64+3.32 & 83.24+1.80 & 78.20+3.12 \\
**+All** & 79.68+3.24 & 82.28+2.32 & 86.52+5.20 & 83.32+1.88 & 80.56+5.48 \\    \\ 
**Base** & 75.11 & 75.04 & 68.08 & 74.67 & 70.39 \\
**+Ext** & 77.87+2.76 & 76.01+0.97 & 69.73+1.65 & 75.03+0.36 & 73.09+2.70 \\
**+Int** & 77.76+2.65 & 76.31+1.27 & 69.93+1.85 & 75.55+0.88 & 72.71+2.32 \\
**+All** & 79.53+4.42 & 76.75+1.71 & 70.58+2.50 & 75.66+0.99 & 74.84+4.45 \\    \\ 
**Base** & 71.64 & 67.54 & 51.06 & 67.35 & 57.70 \\
**+Ext** & 73.02+1.38 & 68.34+0.80 & 52.12+1.06 & 67.62+0.27 & 59.30+1.60 \\
**+Int** & 72.79+1.15 & 68.67+1.13 & 52.23+1.17 & 67.89+0.54 & 59.90+2.20 \\
**+All** & 73.30+1.66 & 68.68+1.14 & 52.24+1.18 & 67.84+0.49 & 60.83+3.13 \\   

Table 2: Comparison of the accuracy of the SOTA Vision Mamba model after flaw repair. Base’ denotes the original model, +Ext’, +Int’, and +All’ represent models after external flaw repair, internal flaw repair, and simultaneous external and internal flaw repair, respectively.2Model Explanation.Enhancing model transparency and trustworthiness through research on model explainability has been a major focus in the field of artificial intelligence. In the context of Mamba model explainability, most studies have primarily concentrated on the model's context learning capabilities , factual recall abilities , and comparisons of the explainability between Mamba and previous models . These studies do not address the identification of flaws in the Mamba model's mechanisms and are focused on natural language processing (NLP), leaving a gap in their application to visual tasks. In the realm of computer vision, Ali et al. established a connection between the selective SSM in the Mamba block and the self-attention mechanism in Transformers. They developed a feature attribution method for Mamba based on Attention-Rollout and Transformer-Attribution , which can be used for image feature attribution. However, this method is limited to the selective SSM module and does not consider or apply to other modules within the Mamba model. In research on model explainability beyond the Mamba model, a plethora of methods have emerged, including activation-based methods [52; 53; 54; 55; 56], gradient-based methods [30; 57; 33], LRP-based methods [58; 59; 31], and perturbation-based methods [60; 61; 62]. However, most of these explainability methods are designed for specific network architectures and may not be directly applicable to Mamba. Inspired by Grad-CAM , this paper introduces Grad-ESC and Grad-ISC methods tailored for the Mamba's external and internal states, respectively. These methods effectively establish the correlation between the model's states and predictions, providing simple yet powerful analytical tools for identifying flaws in the Mamba model's mechanisms.

## 6 Discussion

As one of the most prominent models today, Mamba has found widespread application in the field of computer vision. While attitudes towards its utilization in visual tasks may vary, it does not impede the exploration of its novel internal operational mechanisms. On the contrary, it urges further research into optimizing the performance of Mamba models applied in visual tasks. Moreover, at this juncture, considering the potential changes in future architectures, devising a set of architecture-agnostic post-optimization methods becomes crucial.

However, in this work, we only analyzed the overall operational mechanisms of each module within Mamba, without delving into the internal details of each module. For instance, we hypothesize that there might be flaws within the SSM module itself during prediction, and further optimization of its internal details may potentially enhance the model's performance. Additionally, similar post-optimization paradigms should be further explored in non-visual tasks within Mamba or even applied in non-Mamba architectures.

## 7 Conclusion

Mamba incorporates intricate computational modules, making it non-trivial to delve into its operational mechanisms. In this paper, we have proposed a post-analysis and post-optimization approach to understand the workings of Mamba in visual recognition tasks and enhance its performance. Specifically, departing from existing pre-defined optimization methods, we have introduced predictive correlation analysis and correlation scoring definitions from both internal and external perspectives of Mamba's states. These methodologies aim to identify flaws within Mamba's operational mechanisms. Simultaneously, we have presented corresponding approaches for repairing internal and external flaws to optimize model performance. Extensive and comprehensive experiments have demonstrated the effectiveness of Vision Mamba Mender on mainstream Mamba architectures.