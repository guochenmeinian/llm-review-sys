# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

Introduction

Neural 3D scene reconstruction has taken a giant step beyond the limitations of traditional photogrammetry tools. Neural radiance fields (NeRFs) , which encode scenes implicitly in MLP or explicitly in voxels, exhibit superior resilience to non-Lambertian effects, appearance changes, dynamic scenes, _etc_. However, most NeRF-based methods are inefficient in rendering scenes due to the need to query massive points for volume rendering to infer scene geometry and color. Recently, 3D Gaussian Splitting (3DGS)  has shown promising results on real-time applications and inspires many follow-up works. 3DGS encodes scenes into a set of 3D anisotropic Gaussians, where each 3D Gaussian is represented by a covariance matrix, a center position, opacity, and the latent features to encode color information. Pixel colors by projecting 3D Gaussians into 2D image space can be efficiently computed via rasterization, which is highly optimized on modern graphics processors. However, 3DGS often requires larger memory during training compared to NeRF methods. This is because 3DGS needs millions of 3D Gaussians to represent a scene to recover high-fidelity scene details. Consequently, the memory footprint increases drastically for training 3DGS on larger scenes, _e.g._ city-scale scenes. Moreover, the training of a huge number of 3D Gaussians on larger scenes leads to longer training time. Unfortunately, in comparison to NeRF where the rays can be conveniently distributed into different compute nodes, dispatching 3D Gaussians into different compute nodes is much more difficult due to the highly customized rasterization procedure of 3DGS. In summary, the _two challenges_ for reconstructing large-scale scenes with 3DGS are: 1) High GPU memory to hold the large 3D model during training; 2) Long training time due to the large areas of scenes.

Previous large-scale NeRF methods  solve the above-mentioned two issues by embracing a divide-and-conquer approach, where scenes are split into individual blocks with smaller models fitted into each block. However, these methods require querying multiple sub-models during inference, which slows down the rendering efficiency. This leads us to the following question:

"_Can we apply a similar methodology to 3DGS during training while querying only a global consistent model during inference?_"

In this work, we propose DOGS to answer the aforementioned question. Following previous large-scale NeRF methods, our DOGS splits scene structures into blocks for distributed training. Inspired by previously distributed bundle adjustment methods , we apply the Alternating Direction Method of Multipliers (ADMM)  to ensure the consistency of the shared 3D Gaussians between different blocks. Specifically, we first estimate a tight bounding box for each scene. Subsequently, we split training views and point clouds into blocks. To guarantee each block has a similar size, we split scenes into two blocks each time along the longer axis of the bounding box. Scenes are split recursively in the same way until we obtain the desired number of blocks. Finally, we re-estimate a bounding box for each block and expand the bounding box to construct shared 3D Gaussians between blocks. During training, we maintain a global 3D Gaussian model on the master node and dispatch local 3D Gaussians into other slave nodes. We further guarantee the consistency of the shared 3D Gaussians and the convergence of the training through 3D Gaussian consensus. Specifically, the local 3D Gaussians are collected onto the master node and averaged to update the current global 3D Gaussian model at the end of each training iteration, and then the updated global 3D Gaussian model is shared to all slave nodes to regularize the training of local 3D Gaussians. In this way, our method guarantees that the local 3D Gaussians converge to the global 3D Gaussian model during training. By training 3DGS in a distributed way, our DOGS can reduce the training time by 6+ times compared to the original 3DGS. Furthermore, our DOGS guarantees training convergence and therefore achieves better rendering quality than its counterparts with the 3D Gaussians consensus in the distributed training. After training, we can drop all local 3D Gaussians and maintain only the global 3D Gaussians. During inference, we only need to query the global model while maintaining the rendering performance of 3DGS.

Our contributions are summarized as follows:

* We propose a recursive approach to split scenes into blocks with balanced sizes.
* We introduce a distributed approach to train 3D Gaussians for large-scale scenes. The training time is reduced by \(6+\) times compared to the original 3DGS.
* We conduct exhaustive experiments on standard large-scale datasets to validate the effectiveness of our method.

Related Work

Neural Radiance Fields.Neural radiance fields  enable rendering from novel viewpoints with encoded frequency features . To improve training and rendering efficiency, follow-up works  either encodes scenes into sparse voxels, or a multi-resolution hash table  where the hash collision is implicitly handled during optimization. TensoRF  uses CP-decomposition or VM-decomposition to encode scenes into three orthogonal axes and planes. While the previously mentioned work focuses on per-scene reconstruction, other methods also focus on the generalizability of NeRF , bundle-adjusting camera poses and NeRF , and leveraging sparse or dense depth to supervise the training of NeRF , _etc_. To address the aliasing issue in vanilla NeRF, Mip-NeRF  proposed to use Gaussian to approximate the cone sampling, the integrated positional encodings are therefore scale-aware and can be used to address the aliasing issue of NeRF. Mip-NeRF360  further uses space contraction to model unbounded scenes. ZipNeRF  adopted a hexagonal sampling strategy to handle the aliasing issue for Instant-NGP . NeRF2NeRF  and DReg-NeRF  assumes images are only available during training in each block, and they propose methods to register NeRF blocks together.

Gaussian Splatting.Gaussian Splatting  initializes 3D Gaussians from a sparse point cloud. The 3D Gaussians are used as explicit scene representation and dynamically merged and split during training. Real-time novel view synthesis is enabled by the fast splatting operation through rasterization. Scaffold-GS  initializes a sparse voxel grid from the initial point cloud and encodes the features of 3D Gaussians into corresponding feature vectors. The introduction of the sparse voxel reduced the Gaussian densities by avoiding unnecessary densification on the scene surface. Octree-GS  introduces the level-of-details to dynamically select the appropriate level from the set of multi-resolution anchor points, which ensures consistent rendering performance with adaptive LOD adjustment and maintains high-fidelity rendering results. To reduce the model size, methods  also remove redundant 3D Gaussians through exhaustive quantization. Other methods also focus on alleviating the aliasing issue of 3D Gaussians , or leveraging the efficient rasterizer of point rendering for real-time indoor reconstruction .

Large-Scale 3D Reconstruction.The classical photogrammetry methods utilized Structure-from-Motion (SfM)  and keypoint extraction with SIFT  to reconstruct sparse scene structures. One of such foundational software is Phototourism . To handle city-scale scenes, the 'divide-and-conquer' strategy is widely adopted for the extensibility and scalability of the 3D reconstruction system. SfM methods  splitting scenes based on the view graph, where images with strong connections are divided into the same block. By estimating similarity transformations and merging tracks, all local SfM points and camera poses are fused into a global reference system. Existing NeRF methods also follow a similar strategy for reconstructing city-scale scenes. When camera poses are known, the scene can be split into grid cells. Block-NeRF  focus on the day-night or even cross-season street views. It utilizes the appearance encoding in NeRF-W  to fix the appearance inconsistency issue between different blocks, while Mega-NeRF  aims at encouraging the sparsity of the network under aerial scenes. Urban-NeRF  leverages lidar points to supervise the depth of NeRF in outdoor scenes. SUDS  further extended Mega-NeRF into dynamic scenes. Different from previous large-scale NeRF methods, Switch-NeRF  uses a switch transformer that learns to assign rays to different blocks during training. Grid-NeRF  designed a two-branch network architecture, where the NeRF branch can encourage the feature plane  branch recover more scene details under large-scale scenes. However, the two-branch training scheme is trivial and needs a long time to train. Concurrent works to our method are VastGaussian  and Hierarchy-GS , which also utilize 3D Gaussians for large-scale scene reconstruction. VastGaussian and Hierarchy-GS split scenes into independent chunks and train independent chunks simultaneously. However, VastGaussian relies on exhaustive searching of the training views and initial points to guarantee the convergence of training, and each block is trained without data sharing. Hierarchy-GS consolidates independent chunks into intermediate nodes for further rendering. However, the hierarchical approach needs to preserve redundant models and it is specially designed for street view scenes. Our method, on the other hand, focuses on the distributed training of 3DGS and built upon the consensus of shared 3D Gaussians between different blocks has a guaranteed convergence rate that achieves better performance.

## 3 Our Method

### Preliminary

Gaussian Splatting.3D Gaussian Splatting represents a scene with a set of anisotropic 3D Gaussians \(=\{_{i}\ |\ i N\}\). Each 3D Gaussian primitive \(_{i}\) has a center \(^{3}\) and a covariance \(^{3 3}\) and can be described by:

\[_{i}()=\{-(-_{i})^{ }_{i}^{-1}(-_{i})\}.\] (1)

During training, the covariance is decomposed into a rotation matrix \(^{3 3}\) and a diagonal scaling matrix \(^{3 3}\), _i.e._\(_{i}=^{}^{}\) to ensure the covariance matrix is positive semi-definite. To render the color for a pixel \(\), the 3D Gaussians are projected into the image space for alpha blending:

\[=_{i}_{i}_{i}_{j=1}^{i-1}(1-_{j}),\] (2)

where \(_{i}\) is the rendering opacity and is computed by \(=o^{}()\).

When training 3D Gaussian Splatting, we minimize the loss function below:

\[()=_{}+_{},\] (3)

where \(_{i}=\{_{i},_{i},_{i},_{i}, o_{i}\}\), \(\) are quaternions corresponds to the rotation matrix \(\), \(\) are vectors corresponds to the three diagonal elements of \(\), and \(\) are coefficients of the spherical harmonics.

### Distributed 3D Gaussian Consensus

The 'divide-and-conquer' method is a common paradigm for large-scale 3D reconstruction, which we also adopt in our framework. Different from previous methods such as Block-NeRF  and VastGaussian  which are pipeline parallelized, our method is optimization parallelized with guaranteed convergence. The pipeline of our algorithm is shown in Fig. 2. Firstly, we split a scene (training views and point clouds) into \(K\) intersected blocks. Secondly, we assign training views and points into different blocks. By introducing the ADMM into the training process, we also maintain a globally consistent 3DGS model on a master node. Thirdly, during training, we collect and average the local 3D Gaussians to update the global 3DGS model in each consensus step. The global 3D Gaussians are also shared with each block before we distributedly train the local 3D Gaussians in each block. Finally, we drop all local 3D Gaussians while only using the global 3D Gaussians to render novel views.

In this section, we first introduce the ADMM algorithm. Subsequently, we derive the distributed 3DGS training algorithm. We also present a scene splitting algorithm, which recursively and evenly splits the scene into two blocks each time.

Figure 2: **The pipeline of our distributed 3D Gaussian Splatting method**. 1) We first split the scene into \(K\) blocks with similar sizes. Each block is extended to a larger size to construct overlapping parts. 2) Subsequently, we assign views and points into different blocks. The shared local 3D Gaussians (connected by solid lines in the figure) are a copy of the global 3D Gaussians. 3) The local 3D Gaussians are then collected and averaged to the global 3D Gaussians in each consensus step, and the global 3D Gaussians are shared with each block before training all blocks. 4) Finally, we use the final global 3D Gaussians to synthesize novel views.

ADMM.A general form for consensus ADMM is given by:

\[\ _{i=1}^{N}f_{i}(_{i}),\ _{i}-=0,\ i[1,N].\] (4)

By definition, the constraints are applied such that all the local variables \(_{i}\) agree with the global variable \(\). By applying the augmented Lagrangian method, we have:

\[_{}(,,)=_{i=1}^{N}f_{i }(_{i})+_{i}^{}(_{i}-)+ {2}\|_{i}-\|_{2}^{2},\] (5)

where \(_{i}\) is the dual variable, \(\) is the penalty parameter. During optimization, ADMM alternatively updates the local variables \(_{i}\), global variable \(\) and the dual variables \(_{i}\) at the \(t+1\) iteration by:

\[_{i}^{t+1} :=f_{i}(_{i})+_{i}^{}( _{i}-^{t})+\|_{i}-^{t} \|_{2}^{2},\] (6a) \[^{t+1} :=_{i=1}^{N}_{i}^{t+1}+_{i}^{t},\] (6b) \[_{i}^{t+1} :=_{i}^{t}+(_{i}^{t+1}-^{t+1}).\] (6c)

Distributed Training.We apply the ADMM method to distributedly train a large-scale 3D Gaussian Splatting model. In our problem, \(f_{i}()\) in Eq. (4) corresponds to the loss function \(()\) in Eq. (3). To simplify the implementation for Eq. (6a), we adopt a scaled form of ADMM by defining \(_{i}=_{i}\). We can then rewrite Eq. (5) into (see supplementary for the derivation):

\[_{}(,,)=_{i=1}^{N}f_{ i}(_{i})+\|_{i}-+_{i} \|_{2}^{2}-\|_{i}\|_{2}^{2}.\] (7)

Compared to Eq. (5), Eq. (7) can be made easier to implement by expressing all terms in the squared difference errors. Suppose the variables are decomposed into \(K\) blocks, we then denote the \(i\)th 3D Gaussians in the \(k\)th block as \(_{i}^{k}\). Accordingly, we revise the ADMM updating rule by:

\[(_{i}^{k})^{t+1} :=f(_{i}^{k})+\|(_{i}^{k})^{t}-^{t}+(_{i}^{k})^{t}\|_{2}^{2},\] (8a) \[^{t+1} :=_{k=1}^{K}(_{i}^{k})^{t+1},\] (8b) \[(_{i}^{k})^{t+1} :=(_{i}^{k})^{t}+(_{i}^{k})^{t+1}-^ {t+1}.\] (8c)

Eq. (8a) is the original loss function in Gaussian Splatting with an additional regularizer term \(\|(_{i}^{k})^{t}-^{t}+(_{i}^{k})^{ t}\|_{2}^{2}\). Note that Eq. (8b) should be \(^{t+1}:=_{k=1}(_{i}^{k})^{t+1}+( _{i}^{k})^{t}\). However, the dual variables have an average value of zero after the first iteration. Consequently, Eq. (8b) can be simplified as the global 3D Gaussians are formed by the average of the local 3D Gaussians from all blocks. Moreover, Eq. (8b) is called a 'consensus' step and it requires collecting the local 3D Gaussians from all blocks. After updating the global model \(\), we update the dual variables \(_{i}\) in Eq. (8c) and share the global 3D Gaussians \(\) to each block for optimizing the local 3D Gaussians in Eq. (8a). Note that each 3D Gaussian \(_{i}\) has different properties \(\{_{i},\ _{i},_{i},_{i},o_{i}\}\). As a result, the penalty terms and dual variables should be represented separately according to these properties. The detailed form of Eq. (8) is given in the supplementary.

Scene Splitting.We decompose the scene into \(K\) blocks before applying the updating rule in Eq. (8). Unlike VastGaussian , which focuses mostly on splitting large-scale scenes and needs exhaustive search on the training views and point clouds to ensure the consistency of 3D Gaussians in different blocks, our method relies on the consensus step to ensure the consistency of the 3DGS. However, scene splitting is still important to the convergence of our method. We propose two constraints for the scene-splitting method to best balance the training efficiency and rendering quality:

1. Individual blocks should have a similar size.

## 2 Adjacent blocks should have enough overlaps to boost the convergence of training.

The first constraint is proposed to ensure that: 1) Each block can be fed into GPUs with the same capacity. This is important since a larger block can cause an out-of-memory of the GPU during training due to the imbalanced splitting results. 2) Each block has a similar training time at each iteration. After every \(t\) iteration, we collect all local 3D Gaussians from each block. Intuitively, larger blocks require a longer time to train. Consequently, the master node and all other slave nodes have to wait for the nodes with larger blocks to finish, which increases the training time unnecessarily.

The second constraint is used to boost the convergence of ADMM. From Eq. (8b), the local 3D Gaussians would converge to the global 3D Gaussians by averaging the shared local 3D Gaussians during training. Sufficient shared 3D Gaussians encourage reconstruction consistency between different blocks. Too many shared local 3D Gaussians can bring more communication overhead, which inevitably slows down the training while a lack of shared 3D Gaussians leads to divergence of the algorithm. Although there is no theoretical analysis to show the optimal value of overlapping parts, we empirically use a constant value which we will introduce later in our experiments.

We assume one of the axes of the scene is aligned to physical ground, which can usually be done under the Manhattan world assumption. VastGaussian  adopts a grid-splitting method that first splits the scene into \(m\) cells along the x-axis, and then splits each of the \(m\) cells into \(n\) sub-cells along the y-axis. As we show in Fig. 2(a), this strategy can result in imbalanced blocks. Our splitting method is inherited from VastGaussian while adopting a recursive spitting method to resolve the imbalanced issue. Specifically, we first estimate a tight bounding box for the initial scene. We then split the scene into two parts along the longer axis of the scene. Splitting the scene along the longer axis can prevent the blocks from becoming too shallow on one axis. We re-estimate a tighter bounding box for each of the two cells and split them into smaller blocks. This step is repeated until the number of blocks reaches our requirement. We present the result of this recursive method in Fig. 2(b). Compared to Fig. 2(a), we produce more balanced blocks. To construct overlapping areas, we expand the bounding box of each block by a scale factor \(s\), any points/views that are covered by the same bounding box are grouped into the same block. The training views and point clouds are split in the same way as is shown in Fig. 2(c).

### Improving Convergence Rate

ADMM is known to be sensitive to the initialization of penalty parameters. Since improper initial penalty parameters can slow down the training, we introduce the _adaptive penalty parameters_ and _over-relaxation_ to improve the convergence rate.

Primal Residual and Dual Residual.We define the primal residual \(^{t}\) and the dual residual \(^{t}\) as

\[^{t}=_{i}^{t}-^{t},^{t}=( ^{t}-^{t-1}).\] (9)

Figure 3: **Scene splitting results of our method v.s. VastGaussian **. (a) VastGaussian can result in imbalanced blocks. (b) Our recursive bipartite strategy solves the imbalanced splitting issue. (c) Points and views with the same grid coordinate are assigned to the same block.

In ADMM, the primal residual and dual residual are used as the stopping criteria which terminate the optimization. In our method, we use a hard threshold of training iteration to terminate the algorithm. The primal residual and dual residual are used to adaptively adjust the penalty parameters.

Adaptive Penalty Parameters.We adopt a similar scheme from  to adaptively adjust the penalty parameters:

\[^{t+1}=^{}^{t},\|^{t}\|_{2}> \|^{t}\|_{2},\\ }{^{}},\|^{t}\|_{2}>\| ^{k}\|_{2},\\ ^{t},\|^{t}\|_{2}=\|^{k}\|_{2},\] (10)

where \(>1\), \(^{}\), \(^{}\) are hyper-parameters. The existing convergence proof of the ADMM algorithm is based on the fixed penalty parameters . To guarantee the convergence of our algorithm, we stop adjusting the penalty parameters after \(2000\) iterations in all of our experiments.

Over Relaxation.Similar to , we replace \(^{t+1}\) with \(^{t}^{t+1}-(1-^{t})(-^{t})\) in Eq. (8b) and Eq. (8c), where \(^{t}(0,2)\) is the relaxation parameter and experiments show that the over-relaxation with \(^{t}[1.5,1.8]\) can improve convergence.

## 4 Experiments

Datasets.We evaluate our method on the two large-scale urban datasets, the Mill19 1 dataset, the UrbanScene3D 2 dataset, and the MatrixCity  dataset. Both datasets are captured by drones and each scene contains thousands of high-resolution images. During training and evaluation, we adopt the original image splitting in Mega-NeRF , and downsample the images by 6+ times from the original resolution.

Implementation Details.For Mill19 and UrbanScene3D, we use the camera poses provided by the official site of Mega-NeRF . The y-axis of the scene is aligned to the horizontal plane by COLMAP  under the Manhattan world assumption. We use the CPU version of SIFT (SIFT-CPU) in COLMAP  to extract keypoints and match each image to its nearest 100 images using vocabulary trees. With known camera poses and keypoint matches, we further triangulate 3D points and bundle adjust them. The SIFT-CPU can extract more points than the SIFT-GPU, which can benefit the initialization of 3D Gaussians. For the original 3D Gaussian Splatting (denoted as '3DGS'), we train it in 500,000 iterations and densify it for every 200 iterations until it reaches 30,000 steps. We train both VastGaussian and our method in 80,000 iterations. The densification intervals and termination steps are the same as the 3DGS. We reimplement VastGaussian  since its code was not released during this work. Note that we did not implement the decoupled appearance embedding in VastGaussian, which can be used to remove floaters caused by inconsistent image appearance. We argue that we still provided a fair comparison since this module can be applied to all 3DGS-based methods. For our method, consensus and sharing are enabled every 100 iterations. We leverage the remote procedure call (RPC) framework of PyTorch  to implement our distributed algorithm and transmit data across different compute nodes.

Results.We employ PSNR, SSIM  and LPIPS  as metrics for novel view synthesis. We compare our method against the state-of-the-art large-scale NeRF-based methods: Mega-NeRF , Switch-NeRF 3, and 3DGS-based methods: 3DGS 4, Fed3DGS , VastGaussian , Hierarchy-GS . For Mega-NeRF and Switch-NeRF, we use the officially provided checkpoints on 8 blocks for evaluation. The results of Hierarchy-GS are cited from the original paper since its code has not been released during this work.

We present the quantitative visual quality results in Table 1. The training time and rendering efficiency are also provided in Table 2. As shown in the tables, methods based on 3D Gaussians achieved better results than NeRF-based methods. Although NeRF-based methods are comparable to 3DGS methods in PSNR, the rendered images lack details in such large-scale scenes. This is also validated from 

[MISSING_PAGE_FAIL:8]

the over-relaxation is denoted as _w.o. OR_. As shown in the table, the performance drastically drops without the ADMM consensus step. Furthermore, the results without applying the self-adaptation of penalty parameters is about 1.5 dB lower than the full model in PSNR. The model without applying over-relaxation is comparable to the full model in SSIM and LPIPS but has lower PSNR. We thus employ over-relaxation in our method. We also present the qualitative differences in Fig. 7, and it clearly shows the full model has better quality in the rendered images and geometries. We include more qualitative results in Fig. 8 to show the importance of the consensus step. We can observe that the distributed training presents noisy results without the consensus step. From the two bottom-right figures, we can observe obvious artifacts along the block boundary without the consensus step.

Moreover, we ablate the scale factor in constructing the overlapping areas in Table 5. We can find that the performance of our method is improved with a larger scale factor. Our method has similar performance when the scale factor is \(1.4\) and \(1.5\). However, we select \(1.4\) in our experiments since a larger scale factor brings a longer time and more memory requirement.

## 5 Conclusion

In this paper, we proposed DOGS, a scalable method for training 3DGS distributedly under large-scale scenes. Our method first splits scenes into multiple intersected blocks recursively and then trains each block distributedly on different workers. Our method additionally maintains a global 3DGS model on the master node. The global 3DGS model is shared with each block to encourage each block to converge to the global model. The local 3DGS of all blocks is collected to update the global 3DGS model. When evaluated on large-scale datasets, our method accelerates the 3DGS training time by \(6 8\) while achieving the best rendering quality in novel view synthesis.

    & **PSNR \(\)** & **SSIM \(\)** & **LPIPS \(\)** \\  w.o. CS & 22.80 & 0.677 & 0.326 \\ w.o. SD & 24.30 & 0.729 & 0.285 \\ w.o. OR & 24.45 & 0.766 & 0.259 \\  full model & 25.78 & 0.765 & 0.257 \\    
    & **PSNR \(\)** & **SSIM \(\)** & **LPIPS \(\)** & **Times \(\)** & **Points** & **FPS \(\)** \\ 
1.2 & 24.25 & 0.739 & 0.276 & 02:27 & 4.95 & 129.87 \\
1.3 & 24.86 & 0.750 & 0.270 & 02:27 & 5.02 & 128.73 \\
1.4 & 25.78 & 0.765 & 0.257 & 02:25 & 4.74 & 147.06 \\
1.5 & 25.97 & 0.767 & 0.257 & 02:39 & 4.84 & 130.28 \\   

Table 4: **Ablation study** of our method.

Figure 5: **Qualitative comparisons of our method and others on the UrbanScene3D dataset. From top to bottom are respectively the results of scenes ‘campus’, ‘residence’, and ‘sci-art’.**

    &  &  \\   & PSNR \(\) & SSIM\(\) & LPIPS \(\) & Time \(\) & Points & Mem \(\) & FPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) & Time \(\) & Points & Mem \(\) & FPS \(\) \\ 
3DGS & 27.36 & 0.818 & 0.237 & 47.40 & 11.9 & 6.31 & **45.57** & 20.03 & 0.643 & 0.650 & 14.24 & 1.85 & 2.33 & 19.32 \\ VastGaussian\({}^{}\) & 28.33 & 0.853 & 0.220 & 00.853 & 12.5 & 6.99 & 40.04 & - & - & - & - & - & - \\ DOGS & 28.58 & 0.847 & 0.219 & 06.34 & 10.3 & 5.82 & 48.34 & 21.61 & 0.652 & 0.659 & 02.03 & 2.37 & 2.89 & 180.51 \\   

Table 3: **Quantitative results of novel view synthesis on the MatrixCity  dataset**. \(\): higher is better, \(\): lower is better. The red, orange and yellow colors respectively denote the best, the second best, and the third best results.

Figure 8: Importance of the consensus step.

Figure 6: Qualitative results on the MatrixCity  dataset.

Figure 7: **Ablation study** of our method. Top: rendered images; Bottom: rendered depths.

**Acknowledgement.** This research / project is supported by the National Research Foundation (NRF) Singapore, under its NRF-Investigatorship Programme (Award ID. NRF-NRFI09-0008). Yu Chen is also partially supported by a Google PhD Fellowship.