# Policy Gradient for Rectangular Robust Markov Decision Processes

Navdeep Kumar

Technion

&Esther Derman

MILA, Universite de Montreal

Matthieu Geist

Goodle Deepmind

&Kfir Levy

Technion

&Shie Mannor

Technion, NVIDIA Research

###### Abstract

Policy gradient methods have become a standard for training reinforcement learning agents in a scalable and efficient manner. However, they do not account for transition uncertainty, whereas learning robust policies can be computationally expensive. In this paper, we introduce robust policy gradient (RPG), a policy-based method that efficiently solves rectangular robust Markov decision processes (MDPs). We provide a closed-form expression for the worst occupation measure. Incidentally, we find that the worst kernel is a rank-one perturbation of the nominal. Combining the worst occupation measure with a robust Q-value estimation yields an explicit form of the robust gradient. Our resulting RPG can be estimated from data with the same time complexity as its non-robust equivalent. Hence, it relieves the computational burden of convex optimization problems required for training robust policies by current policy gradient approaches.

## 1 Introduction

Markov decision processes (MDPs) provide an analytical framework to solve sequential decision-making problems and seek the best performance in a fixed environment. Since the resulting policy can be highly sensitive to parameter values , the robust MDP setting alternatively maximizes return under the worst scenario, thus yielding robustness to uncertain environments . In practice, the robust MDP paradigm quantifies the level of uncertainty through a set \(\) determining the possible range of model perturbations. Then, a policy is said to be robust-optimal if it reaches maximal performance under the most adversarial model within the uncertainty set. Developing efficient solvers for robust MDPs is of great interest, as it can lead to behavior policies with generalization guarantees .

If not computationally expensive, robust MDPs can be strongly NP-hard . Thus, to preserve tractability, we commonly assume that \(\) is convex and \(s\)-rectangular, i.e., \(=_{s}_{s}\). Well established in the robust reinforcement learning (RL) literature , the latter assumption means that the overall uncertainty should be designed independently for each state. Further simplification may consider \((s,a)\)-rectangular uncertainty sets of the form \(=_{(s,a)}_{(s,a)}\), albeit this naturally leads to more conservative strategies. In any case, planning in robust MDPs can be computationally costly, as it involves successive max-min problems . To address this issue, the works  have established an equivalence between robustness and regularization in RL in order to derive efficient robust planning methods for \(s\) and \((s,a)\)-rectangular robust MDPs. Indeed, it appears that resorting to proper regularization instead of solving a minimization problem can yield robust behavior without requiring the polynomial time complexity of convex optimization problems .

Alternatively to planning, policy gradient algorithms (PG) directly learn an optimal policy by applying gradient steps towards better performance . Due to its scalability, ease of implementation, and adaptability to many different settings such as model-free and continuous state-action spaces [13; 24], PG has become the workhorse of RL. Although regularization techniques such as max-entropy  or Tsallis  have shown robust behavior without impairing computational cost, they only account for adversarial reward [2; 6; 3]. Differently, robust PG formulations (RPG) formulations aim to address uncertainty to reward _and_ transition functions.

Despite their ability to propel robust behavior, RPG methods that target robust optimal policies are still rare in the RL literature. The global convergence of RPG established in [16; 30] already ensures global convergence of our proposed algorithm, but motivates a practical method for estimating the gradients. Indeed, [16; 30] occult the estimation part, as they assume full access to the policy gradient. Alternatively, the inner loop solution proposed in [Sec. 4.1] requires solving convex optimization problems to find the worst model, which represents a larger time complexity of \(O(S^{4}A^{-1})\) for \((s,a)\)-rectangular, or \(O(S^{4}A^{3}^{-1})\) for \(s\)-rectangular uncertainty sets. These worst kernel and reward models are needed to compute RPG using the policy gradient theorem . Other approaches that elicit an expression for RPG rely on a specific type of uncertainty set such as reward uncertainty with known kernel [3; 5], \(r\)-contaminated kernel with known reward , or \((s,a)\)-rectangular uncertainty , whereas we aim to tackle more general robust MDPs.

In this work, we introduce an RPG method for both \(s\) and \((s,a)\)-rectangular ball-constrained uncertainty sets, with similar complexity as non-robust PG. Our approach provides a closed-form expression of RPG without relying on an oracle while applying to the most common robust MDPs. To this end, we derive the worst reward and transition functions, thus revealing the adversarial nature of the corresponding uncertainty set. Surprisingly, we also find that the worst kernel is a rank-one perturbation of the nominal kernel. Leveraging this rank-one perturbation enables us to derive a robust occupation measure. We concurrently propose an alternative definition of the robust Q-value together with an efficient way to estimate it. Combining these results enables us to obtain RPG in closed form. Our resulting RPG update requires \(O(S^{2}A^{-1})\) computations, thus showing similar time complexity as non-robust PG.

To summarize our contributions: _(i)_ We establish the worst reward and transition models in closed-form; _(ii)_ We show that the worst-case transition function is a rank-one perturbation of the nominal; _(iii)_ We introduce alternative robust Q-values that can be evaluated through efficient Bellman recursion while retrieving the robust value function; _(iv)_ We establish an expression of RPG that can be estimated with similar time complexity as non-robust PG. Experiments show that our RPG speeds up state-of-the-art robust PG updates by 2 orders of magnitude.

## 2 Related work

Although some previous works use gradient methods to learn robust policies, they seek empirical robustness to adversarial behavior rather than robust MDP solutions [21; 29; 4]. In that sense, our study differs from adversarial RL as we explicitly optimize the max-min objective to find a robust optimal policy. Accordingly, the risk-averse approach focuses on the _internal uncertainty_ due to the stochasticity of the system, whereas robust RL addresses the _external uncertainty_ of the system's dynamics. As a result, common risk-averse objectives can be reformulated as robust problems with specific uncertainty sets .

Previous studies that did aim to derive robust policy-based methods are [3; 32; 30]. These are summarized in Table 1, which also displays the complexity of existing approaches.  established RPG for \(s\)-rectangular reward-robust MDPs, i.e., robust MDPs with uncertain reward but given kernel. Although it applies to general norms, their result does not account for transition perturbation. Differently, in , the authors introduced RPG for \(r\)-contaminated MDPs, i.e., robust MDPs with uncertainty set \(:=\{R_{0}\}[(1-r)P_{0}+r_{S}^{}]\). Although it has similar complexity as non-robust PG, by construction, their setting is limited to \((s,a)\)-rectangularity with known reward and mixed transition. As such, the proof techniques in  are tailor-made to the \(r\)-contamination framework and do not apply to more general robust MDPs. In fact, we remark that the \(r\)-contamination setting is equivalent to the action robustness approach introduced in , which emphasizes its limitation to action perturbation. Differently, our RPG holds whenever the worst kernel is a rank-one perturbation of the nominal transition function (see Lemma 4.4).

The work  provides a convergence proof of robust policy mirror-descent in the \((s,a)\)-rectangular case, whereas we study robust policy optimization for \(s\)-rectangular uncertainty sets. In fact, its restriction to the \((s,a)\)-case prevents us from transposing the analysis to our setting. This is due to the fact that the standard robust Bellman operator on \(Q\)-functions can no longer be applied on \(s\)-rectangular sets. To address generic robust MDPs,  recently introduced RPG for general uncertainty sets. Their gradient update has a complexity of \(O(S^{6}A^{4}^{-4})\), which is more expensive than non-robust PG by a factor of \(S^{4}A^{3}^{-4}\). Both works [16; 30] additionally assume access to an oracle gradient of the robust return with respect to the transition model. Avoiding this oracle assumption naturally leads to even higher time complexity in  which is not scalable. At the same time, the two works [16; 30] guarantee global convergence of projected robust gradient iterates, thus establishing the potential promise of RPG. In fact, equipped with RPG convergence, the remaining challenge in making it practical is to efficiently estimate the gradient. This represents the main focus of our study: We aim to explicit an RPG method that generalizes existing results on specific uncertainty sets [3; 32] while holding for \(s\)-rectangular robust MDPs.

## 3 Preliminaries

Notation:We denote the cardinal of an arbitrary finite set \(\) by \(||\). Given two real functions \(,:\), their inner product is \(,_{}:=_{z} (z)\,(z)\), which induces the \(_{2}\)-norm \(\|\|_{2}:=,_{}}\). More generally, the \(_{p}\)-norm of \(\) is denoted by \(\|\|_{p}\) whose conjugate norm is \(\|\|_{q}:=_{\|\|_{q} 1}, _{}\) with \(q^{-1}=1-p^{-1}\), by Holder's inequality. The vector of all zeros (resp. all ones) with appropriate dimensions is denoted by \(\) (resp. \(\)); the probability simplex over \(\) by \(_{}:=\{:_{+}| ,_{}=1\}\), and \(I_{}\) designates the identity matrix in \(^{}\). Given \(v^{}\), we finally define the variance function as \(_{q}(v)=_{w}\|v-w\|_{q}\) and the mean function as \(_{q}(v)_{w}\|v-w\|_{q}\) (see Tab. 2 for their closed-form expression when \(q\{1,2,\}\)).

### Markov Decision Processes

A Markov decision process (MDP) is a tuple \((,,,,P,R)\) such that \(\) and \(\) are finite state and action spaces of cardinal \(S\) and \(A\) respectively, \([0,1)\) is a discount factor and \(_{}\) the initial state distribution. Denoting \(:=\), the couple \((P,R)\) corresponds to the MDP model with \(P:_{}\) being a transition kernel and \(R:\) a reward function. A policy \(:_{}\) maps each state to a probability distribution over \(\), and we denote by \(\) the set of such functions. For any policy \(\), \(R^{}^{}\) is the expected immediate reward defined as \(R^{}(s):=_{s},R(s,)_{}, s \), where \(_{s}\) is a shorthand for \((|s)\). We similarly define the stochastic matrix induced by \(\) as \(P^{}(s^{}|s):=_{s},P(s^{}|s,)_{},  s,s^{}\), and extend the occupation measure to an arbitrary initial vector \(^{}\) by defining

\[d_{P,}^{}:=^{}(I_{}- P^{})^{-1}.\]

   Uncertainty set \(\) & Time Complexity & Reference \\  \(\{R_{0}\}\{P_{0}\}\) & \(S^{2}A^{-1}\) &  \\  \(\{R_{0}\}[(1-r)P_{0}+r_{}^{}]\) & \(S^{2}A^{-1}\) &  \\ \((s,a)\)-rectangular ball \(_{p}^{}\) & \(S^{2}A^{-1}\) & **This work** \\ \((s,a)\)-rectangular, convex \(^{}\) & \(S^{4}A^{-1}\) & Convex optimization \\  \(s\)-rectangular ball \(_{p}^{}\) & \(S^{2}A^{-1}\) & **This work** \\ \(s\)-rectangular ball \((R_{0}+_{p}^{})\{P_{0}\}\) & \(S^{2}A^{-1}\) &  \\ \(s\)-rectangular, convex \(^{}\) & \(S^{4}A^{3}^{-1}\) & Convex optimization \\ \(s\)-rectangular, convex \(^{}\) & \(S^{6}A^{4}^{-4}\) &  \\ \(s\)-rectangular, non-convex \(^{}\) & NP-hard &  \\  Non-rectangular, convex \(\) & NP-hard &  \\   

Table 1: Time complexity of RPG update according to the type of uncertainty set. For conciseness, the displayed complexity hides logarithmic factors in \(A\) and \(S\). Our RPG method has the same complexity as non-robust PG while it generalizes other RPG methods with similar efficiency.

Note that the initial vector here is not necessarily a probability measure: it can be the initial state distribution, but also the balanced value function introduced in Sec. 4[Eq. (2)]. The performance measure we aim to maximize is the value function \(v^{}_{(P,R)}:=(I_{}- P^{})^{-1}R^{}\), or alternatively, the return \(^{}_{(P,R)}:=,v^{}_{(P,R)}_{}\). We denote the optimal value function (resp. optimal return) by \(v^{*}_{(P,R)}=_{}v^{}_{(P,R)}\) (resp. \(^{*}_{(P,R)}=,v^{*}_{(P,R)}\)). It can be obtained using Bellman operators, which are defined as \(T^{}_{{}_{(P,R)}}v:=R^{}+ P^{}v\) and \(T^{*}_{{}_{(P,R)}}v:=_{}T^{}_{{}_{(P,R)}}v, v ^{}\), respectively . For any vector \(v^{}\), we associate its Q-function \(Q^{}\) such that

\[Q(s,a)=R(s,a)+ P(|s,a),v_{},( s,a)\,.\]

With a slight abuse of notation, we can similarly define a Bellman operator over Q-values as

\[T^{}_{{}_{(P,R)}}Q(s,a):=R(s,a)+_{(s^{},a^{ })}P(s^{}|s,a)_{s^{}}(a^{})Q(s^{ },a^{}),(s,a)\,.\]

### Robust Markov Decision Processes

In a robust MDP setting, we assume that \((P,R)\) and aim to maximize return under the worst model from the set. We denote the robust performance of a policy \(\) by \(^{}_{}:=_{(P,R)}^{}_{(P,R)}\). It is maximal when it reaches \(^{*}_{}:=_{}^{}_{}\) at an optimal robust policy \(^{*}_{}_{}^{}_{}\). When considering the robust value function \(v^{}_{}:=_{(P,R)}v^{}_{(P,R)}\), we further need to assume that \(\) is convex and rectangular so that an optimal robust policy realizing \(v^{*}_{}:=_{}v^{}_{}\) can be computed in polynomial time . We thus assume \(\) to be convex and rectangular in the remainder of this work. Specifically, we denote an \((s,a)\)-rectangular uncertainty set by \(^{}:=_{(s,a)}(_{(s,a)}, _{(s,a)})\). It represents a particular case of \(s\)-rectangular uncertainty which we similarly denote by \(^{}:=_{s}(_{s},_{s})\). In both cases, there exists an optimal robust policy that is stationary, although all optimal ones may be stochastic .

Similarly to non-robust MDPs, robust MDPs can be solved through Bellman recursion. Indeed, the robust value function \(v^{}_{}\) (resp., optimal robust value function \(v^{*}_{}\)) is known to be the unique fixed point of the robust Bellman operator \(T^{}_{}v:=_{(P,R)}T^{}_{(P,R)}v\) (resp., the optimal robust Bellman operator \(T^{*}_{}v:=_{}T^{}_{}\)), \( v^{}\), both being \(\)-contractions for the sup-norm. Although this ensures linear convergence of robust value iteration, the evaluation of each Bellman operator can still be prohibitive for practical use.

#### 3.2.1 Ball Constrained Uncertainty set

To facilitate the computation of robust Bellman updates, we consider uncertainty sets that are centered around a nominal model \((P_{0},R_{0})\), i.e., of the form \(=(P_{0},R_{0})+(,)\), and constrained according

    & \(_{q}(v)\) & \(_{q}(v)\) & \(_{v}_{q}(v)\) \\  \(q\) & \(_{w} v-w_{q}\) & \(_{} v-_{q}\) & \((v)}{ v(s_{i})}\) \\ \(\) & \()+v(s_{S})}{2}\) & \()-v(s_{S})}{2}\) & \(&i=1\\ -&i=S\\ 0&\) \\ \(2\) & \(^{S}v(s_{i})}{S}\) & \(^{S}(v(s_{i})-_{2}(v))^{2}}\) & \()-_{2}(v)}{_{2}(v)}\) \\ \(1\) & \(})+v(s_{n_{u}})}{2}\) & \(_{i=1}^{n_{l}}(v(s_{i})-v(s_{S-i}))\) & \(1&i<n_{l}\\ -1&i>n_{u}\\ 0&\) \\   

Table 2: Expressions of the \(q\)-mean, the \(q\)-variance, and its gradient. We assume that the vector \(v\) is sorted, i.e., \(v(s_{i}) v(s_{i+1}), i\{1,2,,S\}\), and denote \(n_{l}:=(S+1)/2,n_{u}:=(S+1)/2\).

to \(_{p}\)-norm balls . In the \((s,a)\)-rectangular case, the corresponding uncertainty set is denoted by \(_{p}^{}:=_{p}^{}_{p} ^{}=_{(s,a)}(_{(s,a)},_{(s,a)})\) where for any \((s,a)\),

\[_{(s,a)}=\{r|r|_{s,a}\}, _{(s,a)}=\{p^{}  p,_{}=0,\|p\|_{p}_{s,a}\}.\]

Similarly, an \(s\)-rectangular norm-constrained uncertainty is denoted by \(_{p}^{}:=_{s}(_{s}, _{s})\) where for any \(s\),

\[_{s}=\{r^{}\|r\|_{p}_{s}\}, _{s}=\{p^{} p (,a),_{}=0 a,\|p \|_{p}_{s}\}.\]

In both cases, the kernel uncertainty set conceals linear constraints ensuring all entries in \(P_{0}+\) are non-negative. Indeed, we generally ignore \(P_{0}\) to satisfy these constraints in practice . Although it may include absurd models and unnecessarily lead to conservative policies, this proxy region is appropriate for model-free robust learning. Moreover, the norm-ball structure on uncertainty sets above enables us to compute robust Bellman updates with similar time complexity as non-robust ones using regularization . We formalize this below.

**Proposition 3.1**.: _([14, Thm. 2-3].) For any policy \(\) and any rectangular \(_{p}\)-ball-constraint uncertainty set, the robust Bellman operator is equivalent to its regularized form:_

\[(T_{}^{}v)(s)=T_{ p_{0},R_{0}}^{}v(s)+_{q }(,,v),\]

_where \(_{q}(,,v):=-_{s},_{s,}+_{q} (v)_{s,}^{P}_{}\) for \((s,a)\)-rectangular uncertainty \(_{p}^{}\), and \(_{q}(,,v):=-(_{s}+_{s}_{q}(v))\|_{ s}\|_{q}\) for \(s\)-rectangular uncertainty \(_{p}^{}\)._

In the following, we leverage the regularized formulation of robust value functions to explicitly derive RPG for rectangular \(_{p}\)-ball uncertainty sets.

#### 3.2.2 Robust Gradient Method

Since the robust return can be non-differentiable, we need to follow the projected sub-gradient ascent rule in order to optimize the robust return, namely, update \(_{k+1}:=_{}(_{k}+_{}_{}^{ _{k}})\) where

\[_{}_{}^{}:=_{}_{(P,R)}^{}\ _{(P,R)=(P_{}^{},R_{}^{})},\] (1)

\(\) is the learning rate, \(_{}\) denotes the orthogonal projection on \(\), and \((P_{}^{},R_{}^{})\) is the worst model associated with \(\) and \(\), i.e., \((P_{}^{},R_{}^{})_{(P,R)} _{(P,R)}^{}\).

Given oracle access to sub-gradient \(_{}^{}\), projected gradient ascent converges to an \(\)-optimal policy \(_{}^{*}\). Moreover, under similar conditions as in the non-robust setting, projected gradient ascent holds an iteration complexity of \(O(S^{4}A^{2}^{-4})\). Yet, the sub-gradient in (1) is generally intractable, particularly because general convex uncertainty sets may yield NP-hard complexity. Instead, we propose to focus on ball-constrained uncertainty sets in order to efficiently compute RPG updates.

## 4 Towards RPG: Expressing the worst quantities

In this section, we provide all the ingredients needed for deriving RPG. Before diving into the gradient expression, we first settle on the general framework of policy gradient. Secondly, in Sec. 4.1, we focus on expressing the worst model according to the nominal explicitly. Surprisingly, we find that the worst transition kernel is a rank-one perturbation of the nominal. This finding enables us to derive the robust occupancy measure, i.e., the visitation frequency of the worst kernel in Sec. 4.2. As a last piece, in Sec. 4.3, we propose an alternative definition of robust Q-value and show that it can be estimated from a specific Bellman recursion.

Consider again the projected gradient ascent rule:

\[_{k+1}:= _{}(_{k}+_{}_{}^{ _{k}}).\]

By definition of the sub-gradient in (1) and applying the standard PG theorem , it holds that:

\[_{}_{}^{}=_{(s,a)}d_{ }^{}(s)Q_{}^{}(s,a)_{s}(a),\]where \(Q^{}_{}:=Q^{}_{(P^{}_{},R^{}_{})}\) is the Q-value associated with the worst-case model, and \(d^{}_{}:=d^{}_{^{}_{}}\) the occupation measure of the worst transition kernel. In fact, for the uncertainty sets we focus on in this work, the worst Q-value \(Q^{}_{}\) retrieves the common definition of robust Q-value  (see the appendix for a detailed discussion). Therefore, for conciseness and with a slight abuse, we shall designate \(Q^{}_{}\) by the robust Q-value, and \(d^{}_{}\) by the robust visitation frequency. The remaining question is how to compute these quantities and in particular, can we efficiently find the worst parameters \((P^{}_{},R^{}_{})\)? The following part of our study aims to address these questions.

Given an uncertainty set \(\), let first define the normalized and balanced robust value function as:

\[u^{}_{}(s):=(v^{}_{}(s)-_{ q}(v^{}_{}))\|v^{}_{}(s)-_{q}(v^{}_{ })\|^{q-1}}{_{q}(v^{}_{})^{q-1}}.\] (2)

By construction, it has zero mean and unit norm, i.e., \( u^{}_{},_{}=0\) and \(\|u^{}_{}\|_{p}=1\). In fact, as stated in the result below, \(u^{}_{}\) is the gradient of the \(q\)-variance function, and correlates with the (unnormalized, unbalanced) robust value function according to the same \(q\)-variance.

**Proposition 4.1**.: _For any policy \(\) and \(_{p}\)-ball rectangular uncertainty set, the following holds:_

\[u^{}_{} =_{v}k_{q}(v)\;_{v=v^{}_{}},\] \[ u^{}_{},v^{}_{} =_{q}(v^{}_{}).\]

### Worst Kernel and Reward

In the following results, we explicit the relationship between the nominal and the worst-case model for \((s,a)\) and \(s\)-rectangular \(_{p}\)-balls. We will then leverage this relationship to compute the robust Q-values and the robust occupation measure, both necessary for RPG.

**Theorem 4.2** (\((s,a)\)-rectangular case).: _Given uncertainty set \(=^{ a}_{p}\) and any policy \(\), the worst model is related to the nominal one through:_

\[R^{}_{}(s,a)=R_{0}(s,a)-_{s,a} P^{ }_{}(|s,a)=P_{0}(|s,a)-_{s,a}u^{}_{}.\]

Based on Thm. 4.2, it follows that in the \((s,a)\)-rectangular case, the worst reward function is independent of the employed policy. As we establish in Thm. 4.3 below, this no longer applies under \(s\)-rectangularity. In either case, the worst kernel is policy-dependent, discouraging the system to move toward high-rewarding states and directing it to low-rewarding ones instead. Surprisingly, the vector penalty \(u^{}_{}^{}\) additionally illustrates that the worst kernel is a rank-one perturbation of the nominal. Indeed, considering the stochastic matrix induced by any policy \(\), we have

\[[P^{}_{}-P^{}_{0}](s^{}|s)=-(_{a}_{s,a}_{s}(a))u^{}_{}(s^{}), s ,\]

so that the perturbation matrix \(P^{}_{}-P^{}_{0}\) is of rank one. In the sequel, we will leverage this finding to compute the robust visitation frequency.

**Theorem 4.3** (\(s\)-rectangular case).: _Given uncertainty set \(=^{s}_{p}\) and any policy \(\), the worst model is related to the nominal one through:_

\[R^{}_{}(s,a)=R_{0}(s,a)-_{s}((a)}{\|_ {s}\|_{q}})^{q-1} P^{}_{}(|s,a)=P _{0}(|s,a)-_{s}u^{}_{}((a)}{\|_{ s}\|_{q}})^{q-1}.\]

Similarly to the \((s,a)\)-case, the adversarial kernel is a rank-one perturbation of the nominal. Yet, an extra dependence on the policy through the coefficient \(((a)}{\|_{s}\|_{q}})^{q-1}\) appears in the \(s\)-case, affecting both the worst reward and the worst kernel. Intuitively, it means that the worst model cannot be chosen independently for each action, but must instead depend on the agent's policy. This further explains why optimal policies can all be stochastic in \(s\)-rectangular robust MDPs .

Thms. 4.2 and 4.3 enable us to derive the worst MDP model in closed form with time complexity \(O(S^{2}A^{-1})\), up to logarithmic factors (please see the appendix for a detailed discussion). It thus holds the same complexity as non-robust value iteration, since we additionally need to compute the value function to derive its corresponding regularizer . On the other hand, if we employ convex optimization using value methods instead, obtaining the worst model requires a time complexity of \(O(S^{4}A^{-1})\) in the \((s,a)\)-rectangular case, and \(O(S^{4}A^{3}^{-1})\) in the \(s\)-rectangular case [Sec. 4.1].

### Robust Occupation Measure

We finally derive the robust occupation measure using nominal values, which will lead to an explicit RPG. Although intractable in general, we show that focusing on ball-constrained uncertainty enables deriving the robust occupation matrix efficiently from the (nominal) occupation measure. We first establish the lemma below, which leverages the fact that the worst transition function is a rank-one perturbation of the nominal and represents our core contribution.

**Lemma 4.4**.: _Let \(b,k^{}\) and \(P_{0},P_{1}(_{})^{}\) two transition matrices. If \(P_{1}=P_{0}-bk^{}\), i.e., \(P_{1}\) is a rank-one perturbation of \(P_{0}\), then their occupation matrices \(D_{i}:=(I- P_{i})^{-1},i=0,1\) are related through:_

\[D_{1}=D_{0}-bk^{}D_{0}}{(1+ k^{}D_{0}b)}.\]

Combining Thms. 4.2 and 4.3 with the above lemma, we obtain the robust occupation in terms of the nominal, as stated in Thm. 4.6 below. Prior to this, we introduce the notion of _expected transition uncertainty_ below.

**Definition 4.5**.: _Let \(\) a rectangular \(_{p}\)-ball-constrained uncertainty set of transition radius \(\). For any policy \(\), the expected transition uncertainty at any state \(s\) is given by \(_{s}^{}:=_{a}_{s}(a)_{s,a}\) if \(=_{p}^{s}\), and \(_{s}^{}:=_{s}\|_{s}\|_{q}\) if \(=_{p}^{s}\)._

**Theorem 4.6**.: _For any rectangular \(_{p}\)-ball-constrained uncertainty and \(\), it holds that:_

\[d^{}_{,}=d^{}_{P_{0},}-_{P_{ 0},},^{}_{}}{1+ d^{}_{P_{0},u^{ }_{}},^{}_{}}d^{}_{P_{0},u^{}_{ }}.\] (3)

Thm. 4.6 explicitly highlights the relationship between the robust visitation frequency and the nominal one. Thus, according to Eq. (3), the standard non-robust occupation measure in the first term needs to be penalized by another one, \(d^{}_{P_{0},u^{}_{}}=(u^{}_{})^{}(I_{ }- P_{0}^{})^{-1}\), to obtain the robust occupation measure. Recall that \(u^{}_{}\) is the balanced-scaled value function determined by \(\) and uncertainty set \(\). Thus, the penalty term \(d^{}_{P_{0},u^{}_{}}\) tends to zero if all coordinates of the robust value function vector converge to the same value.

Nonetheless, our expression (3) does present some challenges. First, the visitation frequency appearing in the correction term indicates that instead of taking a fixed initial state distribution, we should start from a _varying_ and _signed_ measure represented by the balanced value function. Although it suggests putting more weight on worst-performing states, obtaining a non-biased estimator for this occupancy measure remains unclear in model-free learning. One may use importance sampling, but as any off-policy approach, both variance and bias would need to be controlled then. Such statistical analysis goes beyond the scope of this work.

### Robust Q-values

In this section, we focus on the last element needed for RPG and aim to estimate the robust Q-value denoted previously by \(Q^{}_{}:=Q^{}_{(P^{}_{},R^{}_{})}\). Define its associated value function as \(v^{}_{}(s)=_{s},Q^{}_{}(s,),  s,\). Based on standard Bellman recursion, it thus holds that:

\[Q^{}_{}(s,a)=R^{}_{}(s,a)+ P^{}_{ }(|s,a),v^{}_{},s,(s,a) ,,\]

while \(Q^{}_{}\) is the unique fixed point of the \(\)-contracting operator

\[(^{}_{})Q(s,a):=T^{}_{(P^{}_{},R^{ }_{})}Q(s,a), Q^{}\,.\] (4)

The relations above hold for general uncertainty sets, provided that we have access to the worst model. The \(s\)-rectangularity assumption additionally enables us to retrieve the robust value function using the Bellman operator above . Concretely, we have: \(v^{}_{}=_{(P,R)}v^{}_{(P,R)}=v^{}_{(P^{ }_{},R^{}_{})}\).

The following result derives a regularized operator equivalent to \(^{}_{}\), which results in an efficient iteration method to compute the robust Q-value.

**Proposition 4.7**.: _The Bellman operator \(^{}_{}\) defined in Eq. (4) is equivalent to:_

\[(^{}_{})Q(s,a)=T^{}_{(P_{0},R_{0})}Q(s,a)+^{ }_{q}(_{s,a},_{s,a},v),\]

_where \(v(s):=_{s},Q(s,)_{}\), \(^{}_{q}(,,v):=-(_{s,a}+_{s,a}_{q }(v))\) for \((s,a)\)-rectangular uncertainty \(^{s}_{p}\), and \(^{}_{q}(,,v):=-((a)}{\|_{s}\|_{q }})^{q-1}(_{s}+_{s}_{q}(v))\) for \(s\)-rectangular \(^{s}_{p}\)._Robust Policy Gradient

We are now able to derive an RPG by combining our previous results. Notably, unlike previous works that need to sample next-state transitions based on all models from the uncertainty set [21; 17; 4], here, we only need the nominal kernel to get the occupation measures.

**Theorem 5.1** (RPG).: _For any rectangular \(_{p}\)-ball-constrained uncertainty, the robust policy gradient is given by:_

\[_{}_{}^{}=_{(s,a)}(d_{ _{0},}^{}(s)-c^{}(s))Q_{}^{}(s,a) _{s}(a),\] (5)

_where_

\[c^{}(s):=_{0},}^{},^{}  s}{1+ d_{_{0},u_{}^{}}^{}, ^{} S}d_{_{0},u_{}^{}}^{}(s),  s\,.\]

The implementation of RPG directly follows and can be found in Alg. 1. Thm. 5.1 is a straightforward application of non-robust PG, as its proof simply consists in plugging Eq. (3) into the standard PG expression \(_{}_{}^{}=_{(s,a)}d_{,}^{}(s)Q_{}^{}(s,a)_{s}(a)\). We obtain a regular PG in the first term, with the robust Q-value instead of the non-robust one, plus a correction term \(c^{}\) resulting from taking the visitation frequency of the worst kernel instead of the nominal. Unlike previous work that uses policy regularization to achieve empirical robustness in PG methods [2; 11], Thm. 5.1 establishes an RPG that accounts for transition uncertainty and targets a robust optimal policy. It crucially relies on the rank-one-perturbation structure of the worst transition kernel (see Lem. 4.4). As established in Thm. 4.6, \(_{p}\)-ball uncertainty implies such property, but the converse as to whether any convex set leads to the worst transition kernel being a rank-one perturbation of the nominal remains an open question. For example, it would be interesting to investigate the structural properties needed on the uncertainty set for the rank-one perturbation to hold.

```
0:\(,\) Initialize:\(v_{k},_{k}\)
1:for\(k=1,2,\)do
2:\(_{}_{k}=_{(s,a)}(d_{_{0}, }^{}(s)-c^{}(s))Q_{}^{}(s,a)_{s}(a)\)\(\) Compute policy gradient
3:\(_{k}_{}(_{k}+_{}_{k})\)\(\) Update policy
4:endfor ```

**Algorithm 1** RPG

### Complexity Analysis

A major concern in solving robust MDPs is time complexity . Similarly, it is of major importance to assess the additional time required for computing an RPG update, compared to its non-robust variant. Although previous work has analyzed the convergence rate of RPG to a global optimum , it assumes access to an oracle gradient, thus occulting the computational concerns raised from gradient estimation. In fact, the NP-hardness of non-rectangular and/or non-convex robust MDPs  already indicates that their resulting RPG can be intractable.

To compute RPG in Thm. 5.1, we first need to evaluate the robust Q-value. Based on Lemma 4.7 and the Bellman operators introduced there, our evaluation method involves an additional estimation of the variance function \(_{p}\). According to , this takes logarithmic time at most, using binary search. As to the compensation term \(c^{}\) in Eq. (11), it requires computing occupancy measures with respect to two different initial vectors, namely the balanced value function and the initial distribution. Thus, the computational cost for estimating \(c^{}\) is the same as estimating a non-robust occupancy measure. Tab. 1 summarizes the complexity of different approaches while a detailed discussion can be found in the appendix. We refer to [Sec. 4.1] for the complexity of RPG based on convex optimization.

Generalization to arbitrary norms.Until now, we have focused on \(_{p}\)-norm for concreteness. However, the above results apply to any norm \(\|\|\), at least if the uncertainty set is \((s,a)\)-rectangular, in which case the variance function changes to \((v):=_{\|c\| 1,^{}=0} c,v\) and the balanced value to \(_{\|c\| 1,^{}=0} c,v\). The rank-one perturbation structure of the worst kernel is preserved, so the robust occupation measure can be obtained similarly using Lemma 4.4. The \(s\)-rectangular is more involved. We defer its discussion to the appendix and leave its complete derivation for future work.

Experiments

In order to test the effectiveness of our RPG update, we evaluate its increased time complexity relative to non-robust PG. In the following experiments, we randomly generate nominal models for arbitrary state-action space sizes. Each experiment was averaged over 100 runs. We refer the reader to the appendix for more details on the radius levels and other implementation choices.

We first focus on \(_{1}\)-robust MDPs to compare our RPG with a convex optimization approach. Specifically, we consider a robust PG with an optimization solver, which we designate by LP-RPG. Indeed, recall that \(_{1}\)-ball-constraints induce a linear program (LP) rather than a more general convex optimization problem. Therefore, to compute the robust value function for a given policy, we iteratively evaluate the robust Bellman operator using LP [30, Section 4.1]. Using this approximated value function, we can compute the worst value parameters to apply PG theorem by  and deduce an LP-based robust PG update. Differently, our RPG method relies on the regularized formulation of robust value iteration proposed in [3; 14], from which we deduce the normalized-balanced value function as in Eq. (2). We finally apply Thm. 4.6 to compute the robust occupation measure, and Prop. 4.7 to obtain the robust Q-value.

Tab. 3 displays the results obtained for the two alternative methods described above. In all experiments, the standard deviation was typically 2-10% so we omitted it for brevity. As can be seen in Tab. 3, LP-RPG does not scale well compared to RPG, whereas RPG has similar time complexity as PG. Notably, the running time of \(s\)-rectangular LP-RPG scales much better with the space size than its \((s,a)\)-rectangular equivalent, which confirms the theoretical complexities from Tab. 1. Yet, since these methods were time-consuming, we repeated these for a few runs only. In fact, LP-RPG is more expensive than RPG by 1-3 orders of magnitude, which illustrates its inefficiency. We emphasize that here, we only focused on \(_{1}\)-robust MDPs to leverage LP solvers in robust policy evaluation. We expect the computational cost of LP-RPG to scale even more poorly for other \(_{p}\)-robust MDPs that involve polynomial time-consuming convex programs.

We further compare our RPG to non-robust PG on different \(_{p}\)-balls. Tab. 4 confirms the comparable time complexity of RPG to non-robust PG, thus demonstrating the effectiveness of our method. We note that for \(p\{1,2,\}\), the corresponding regularization quantities can be computed in closed form, whereas they involve a binary search for other values . We thus get a slight running-time increase for \(p\{5,10\}\).

   S & A & \(\{(P_{0},R_{0})\}\) & \(_{2}^{}\) & \(_{2}^{}\) & \(_{5}^{}\) & \(_{5}^{}\) & \(_{10}^{}\) & \(_{10}^{}\) & \(_{}^{}\) & \(_{}^{}\) \\ 
10 & 10 & 1 & 1.5 & 1.5 & 4.9 & 4.7 & 4.7 & 4.9 & 1.5 & 1.6 \\
30 & 10 & 1 & 1.4 & 1.5 & 4.2 & 4.3 & 4.2 & 4.0 & 1.4 & 1.4 \\
50 & 10 & 1 & 1.5 & 1.4 & 4.5 & 4.1 & 4.0 & 4.0 & 1.4 & 1.4 \\
100 & 20 & 1 & 1.4 & 1.3 & 2.6 & 2.5 & 2.5 & 2.4 & 1.3 & 1.2 \\
500 & 50 & 1 & 1.2 & 1.2 & 1.7 & 1.7 & 1.7 & 1.2 & 1.3 \\   

Table 4: Relative running time for computing RPG under different types of uncertainty sets.

    & & \(\{(P_{0},R_{0})\}\) & _{1}^{}\)} & _{1}^{}\)} \\  S & A & PG & RPG & LP-RPG & RPG & LP-RPG \\ 
10 & 10 & 1 & 1.4 & 326 & 1.4 & 77 \\
30 & 10 & 1 & 1.4 & 351 & 1.4 & 109 \\
50 & 10 & 1 & 1.4 & 408 & 1.4 & 159 \\
100 & 20 & 1 & 1.5 & 469 & 1.3 & 268 \\
500 & 50 & 1 & 1.3 & 925 & 1.3 & 5343 \\   

Table 3: Comparison of the relative running time between RPG and the convex optimization approach (here, LP). Our method is faster than LP-based updates by 1 to 3 orders of magnitude.

Discussion

This paper introduced an explicit expression of RPG for rectangular robust MDPs. Our approach involved auxiliary results such as deriving the worst model in closed form and showing that it is a rank-one perturbation of the nominal kernel. The resulting RPG extends vanilla PG with additional correction terms that can be derived in closed form as well. Thus, the computational time of RPG is similar to its non-robust variant.

A key assumption that would be interesting to relax is the normed-ball structure of the uncertainty sets considered in this study. Indeed, since the proofs of our technical results rely on norm properties, it is still unclear if and how RPG can generalize to metric-based or \(f\)-divergence uncertainty sets. The latter type of uncertainty can be particularly useful for data-driven settings, as the radius can be chosen according to cross-validation or statistical bounds . Another compelling direction would be to explore other variants of RPG using mirror descent or natural policy gradient and examine their compatibility with deep architectures, which would further demonstrate the practical efficiency of our RPG method.