# ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise Optimization

Luca Eyring\({}^{1,2,3,}\)1 Shyamgopal Karthik\({}^{1,2,3,4}\)1 Karsten Roth\({}^{2,3,4}\)

Alexey Dosovitskiy\({}^{5}\) Zeynep Akata\({}^{1,2,3}\)

###### Abstract

Text-to-Image (T2I) models have made significant advancements in recent years, but they still struggle to accurately capture intricate details specified in complex compositional prompts. While fine-tuning T2I models with reward objectives has shown promise, it suffers from "reward hacking" and may not generalize well to unseen prompt distributions. In this work, we propose **R**eward-based **N**oise **O**ptimization (**ReNO**), a novel approach that enhances T2I models at inference by optimizing the initial noise based on the signal from one or multiple human preference reward models. Remarkably, solving this optimization problem with gradient ascent for 50 iterations yields impressive results on four different one-step models across two competitive benchmarks, T2I-CompBench and GenEval. Within a computational budget of 20-50 seconds, ReNO-enhanced one-step models consistently surpass the performance of all current open-source Text-to-Image models. Extensive user studies demonstrate that our model is preferred nearly twice as often compared to the popular SDXL model and is on par with the proprietary Stable Diffusion 3 with 8B parameters. Moreover, given the same computational resources, a ReNO-optimized one-step model outperforms widely-used open-source models such as SDXL and PixArt-\(\), highlighting the efficiency and effectiveness of ReNO in enhancing T2I model performance at inference time. Code is available at https://github.com/ExplainableML/ReNO.

## 1 Introduction

Advancements in Text-to-Image (T2I) models have been achieved in recent years, largely due to the availability of massive image-text datasets  and the development of denoising diffusion models . Despite these improvements, T2I models often struggle to accurately capture the intricate details specified in complex compositional prompts . Common challenges include incorrect text rendering, difficulties with attribute binding, generation of unlikely object combinations, and color leakage. While recent models have begun to address these issues by employing enhanced language encoders, larger diffusion models, and better data curation , these approaches typically involve training larger models from scratch, making them inapplicable to existing models.

As a more efficient alternative, fine-tuning T2I models has gained significant attention. This approach can be tailored either toward specific preferences  or general human preferences. Inspired by the success of Reinforcement Learning from Human Feedback (RLHF)  in Large LanguageModels (LLMs), several works  propose aligning T2I models by fine-tuning them on human-preferred prompt-image sets using RLHF-inspired techniques. Additionally, human preference reward models, such as PickScore , HPSv2 , and ImageReward , have gained popularity. These models are trained to output a score reflecting human preference for an image given a specific prompt, typically by measuring human preferences for various images generated from the same prompt. The scores predicted by these models have been utilized as evaluation metrics for the quality of generated images. Furthermore, Clark et al. , Li et al. , Prabhudesai et al.  directly fine-tune T2I models on these differentiable reward models to maximize the predicted reward of generated images. This approach is efficient due to the directly differentiable objective.

Fine-tuning T2I models with reward objectives has a major drawback of "reward hacking", which occurs when a reward model gives a high score to an undesirable image. Reward hacking points to deficiencies in existing reward models, highlighting gaps between the desired behavior and the behavior implicitly captured by the reward model, which is especially prone to appear when explicitly fine-tuning for a reward . Additionally, these models are often fine-tuned on a small scale (e.g., <50 prompts in some cases ) and thus may not generalize well to unseen prompt distributions with complex compositional structures. In this work, our aim is to enhance T2I models at _inference_ for each unique generation, similar to the paradigm of test-time training for classification models . Fine-tuning diffusion models for every single prompt would both be computationally expensive (Dreambooth  takes 5 minutes on 1 A100), and susceptible to "reward-hacking".

We sidestep the challenge of fine-tuning the model's parameters and instead explore optimizing the initial random noise during _inference_ without adapting any of the model's parameters. To obtain more optimal noise and a higher-quality generated image, we introduce Reward-based Noise Optimization (ReNO), where the initial noise is updated based on the signal from a reward model evaluated on the generated image. The main challenges in this approach are twofold. First, backpropagating the gradient through the denoising steps can lead to exploding/vanishing gradients, rendering the optimization process unstable. Our insight is that by employing a distilled _one-step_ T2I model , we circumvent the issue of exploding/vanishing gradients since backpropagation is performed through a single step. Second, naively optimizing the initial latent for an arbitrary objective can lead to collapse due to reward hacking. To mitigate this, we propose the use of a combination of reward objectives to not overfit to any single reward. Moreover, given a well-calibrated one-step T2I model with frozen parameters, the generated images should not exhibit reward hacking if the initial noise remains in the proximity of the initial noise distribution. Therefore, we propose an optimization scheme with limited steps, regularization of the noise to stay in-distribution, and gradient clipping.

Figure 1: Qualitative results of four different one-step Text-to-Image models with and without ReNO over different prompts. The same initial random noise is used for the one-step generation and the initialization of ReNO. ReNO significantly improves upon the initially generated image with respect to both prompt faithfulness as well as aesthetic quality for all four models. Best viewed zoomed in.

In essence, ReNO involves optimizing the initial latent noise given an one-step T2I model (e.g., SD/SDXL-Turbo) and a reward model (e.g., ImageReward) for a limited number of iterations (10-50 steps). On the popular evaluation benchmarks T2I-Compbench and GenEval, our noise optimization strategy (ReNO) significantly improves performance, increasing scores by over 20% in some cases. This enhancement allows SD2.1-Turbo models to approach the performance of closed-source proprietary models such as DALL-E 3  and SD3 . We demonstrate that ReNO substantially improves the performance of four different one-step T2I models (e.g. Figure 1), both in terms of quantitative evaluation and extensive user studies, while only requiring 20-50 seconds to generate an image. Moreover, given the same computational budget, ReNO surpasses the performance of competing multi-step models, offering an attractive trade-off between performance and inference speed. ReNO not only motivates the development of more robust reward models but also provides a compelling benchmark for their evaluation. Finally, our results highlight the importance of the noise distribution in T2I models and encourage further research into understanding and adapting it.

## 2 Reward-based Noise Optimization (ReNO)

Despite the remarkable progress in Text-to-Image (T2I) generation, current state-of-the-art models still struggle to consistently produce visually satisfactory images that fully adhere to the input prompt. Recent studies have highlighted the significant impact of the initial noise vector \(\) on the quality of the generated image [19; 85]. In fact, selecting and re-ranking images generated from a set of initial noises based on reward models has been shown to substantially improve performance [41; 46]. This observation naturally leads to the question of whether it is possible to identify an _optimal_ noise vector that maximizes a given goodness measure for the generated image. In this section, we first provide an overview of one-step diffusion models, which serve as the foundation for our work. We then introduce our simple yet principled approach that enables practical noise optimization to enhance the performance of one-step T2I models based on human-preference reward models, addressing the challenge of generating high-quality images that align with the input prompt.

### Background: One-Step Diffusion Models

T2I models aim to generate images \(_{0}\) conditioned on a given textual prompt p. A generative model \(G_{}\) parameterized by \(\) takes as input a noise vector \((0,)\) and a prompt p, and outputs an image \(G_{}(,)=_{0}\). The objective is to learn the parameters \(\), such that the generated image \(_{0}\) aligns with the semantics of the prompt p. This is typically achieved by training the model on a large dataset of paired text and images. Recent models are based on a time-dependent formulation between a standard Gaussian distribution \((0,)\), and data \(_{0} p_{0}()\). These models define a probability path between the initial noise distribution and the target data distribution, such that

\[_{t}=_{t}_{0}+_{t},\] (1)

where \(_{t}\) is a decreasing and \(_{t}\) is an increasing function of \(t\). Score-based diffusion [40; 45; 86] and flow matching [1; 54; 57] models share the observation that the process \(_{t}\) can be sampled dynamically using a stochastic or ordinary differential equation (SDE or ODE). Consider the forward SDE that transforms data into noise as \(t\) increases \(_{t}=(_{t},t)\,t+g(t)\, _{t}\), where \(_{t}(x_{t},t)\) denotes the drift, \(_{t}\) is a Wiener process and \(g(t)\) represents the diffusion schedule. Then, the marginal probability distribution \(p_{t}()\) of \(_{t}\) in (1) coincides with the distribution of the probability flow ODE [45; 86], as well as the reverse-time SDE 

\[_{t}=[(_{t},t)-g(t)^{2}( _{t},t)]\,t+g(t)\,}_{t},\] (2)

where \((,t)= p_{t}()\) is the score function. By solving either the ODE or SDE backward in time from \(_{T}=(0,)\), we can generate samples from \(p_{0}()\). This relies on a good estimate of the parameterized score \(_{}(_{t},t)\). The choice of functions \(_{t}\) and \(_{t}\) are defined implicitly based on the forward SDE [40; 45; 87; 85]. Furthermore, the process \(_{t}\) is considered on an interval \([0,T]\) with \(T\) sufficiently large such that \(_{T}\) approximates the initial noise distribution \((0,)\). Then, it has been shown that the score can be approximated efficiently based on, e.g. the denoising loss 

\[_{}()=_{_{0} p(_{0} ),(0,),t(0,T)}[\| _{t}_{}(_{t},t)+\|^{2}].\] (3)

During inference, these models simulate an ODE/SDE through discretization for a number of steps. This can be computationally expensive as the trained model must be evaluated sequentially.

Distillation.As a means to reduce inference time, distillation techniques have recently gained traction with the intent to learn a student model that approximates the solution of the simulated differential equation with a trained teacher model given fewer inference steps, e.g. score distillation  penalizes the estimated score to the real data distribution. Furthermore, several methods have been proposed to distill models into _one-step_ generators, which learn to approximate the full ODE or SDE in _one step_. Our work builds upon the following one-step T2I models which we refer to as \(_{}\). Adversarial Diffusion Distillation (ADD)  combines score distillation with an adversarial loss and is employed to train SD-Turbo based on SD 2.1  as a teacher and SDXL-Turbo  based on SDXL . Diffusion Matching Distillation (DMD)  additionally leverages a distributional loss based on an approximated KL divergence and is applied for PixArt-\(\) DMD [12; 13]. Lastly, Trajectory Segmented Consistency Distillation (TSCD)  introduces a progressive segment-wise consistency distillation [87; 44] loss to train HyperSDXL  with reward fine-tuning. All these models are trained in latent space such that during inference, an image is generated by first generating a sample in latent space and then decoding it \(G_{}(,)=(_{}(,))\) with a pre-trained decoder \(\).

### Initial Noise Optimization

Given a Text-to-Image generative model \(G_{}(,)\) that generates images based on a noise \(\) and a prompt \(\), we defined the following optimization problem following previous work [5; 43; 80; 91] with the objective of optimizing the noise \(\) based on a criterion function \(:^{H W c}\) evaluated on the generated image

\[^{}=*{arg\,max}_{}(G_{}(,)).\] (4)

Then, given a differentiable \(\), (4) can be solved through iterative optimization via standard gradient ascent techniques. However, backpropagating through \((G_{}(,))\) is non-trivial as current Text-to-Image models are based on the simulation of ODEs or SDEs (Section 2.1). Several methods have been proposed to enable backpropagation through time-dependent generative models [14; 17; 60; 91], based on e.g., the adjoint method . Our method, in contrast, leverages the crucial observation that selecting a one-step model as \(G_{}\) enables efficient backpropagation through (4). Although this realization may initially appear trivial, it proves to be a fundamental step in facilitating practical noise optimization in Text-to-Image models. Current methods require between 10  and 40  minutes to optimize noise and thus, to generate a single image. Our approach achieves image generation, including noise optimization, in 20-50 seconds, making it suitable for practical applications.

Figure 2: Overview of our proposed ReNO framework. Given reward models based on human preferences, we optimize the initial latent noise to maximize the reward scores (consisting HPSv2 , PickScore , ImageReward , and CLIP ) for the images generated by the one-step T2I model. Over 50 iterations, the quality of the images and the prompt faithfulness are improved.

Noise regularization.One important consideration, is that it is desirable for \(\) to stay within the proximity of the initial noise distribution \((0,)\) as otherwise \(G_{}\) might provide unwanted generations. This can be realized by including a regularization term inside of \(\). Samuel et al.  propose instead of directly optimizing the likelihood of \(p_{T}()\), to instead consider the likelihood of the norm of the noise \(r=||||\), which is distributed according to a \(^{d}\) distribution \(p(r)\). Thus, following Ben-Hamu et al. , Samuel et al.  we maximize the log-likelihood of the norm of a noise sample \(K()=(d-1)(||||)-||||^{2}/2\). In our framework, this corresponds to employing a regularized criterion function given by \((_{0},)=}(_{0})+ K()\), which can be plugged into (4).

In Figure 3, we provide an illustrative example where we chose the criterion to maximize a selected color channel \(c\) of the generated image while minimizing the other two \(_{1},_{2}\)

\[}(_{0})=_{i,j}_{0}^{i,j,c }-_{0}^{i,j,_{1}}-_{0}^{i,j,_{2}},\] (5)

where \(_{0}^{i,j,c}\) denotes the channel \(c\) of the pixel at \((i,j)\). Note that due to the calibration of the trained model and the noise staying in-distribution, the noise does not collapse to the optimal \(^{*}\), which would result in the generation of a fully blue or red image. Also, the optimization first adapts the color of the car and then starts changing the background. Here, 10 optimization steps provide satisfactory results illustrating the efficacy of the proposed one-step noise optimization framework.

### Human Preference Reward Models and Our Reward Criterion

Inspired by the success of Reinforcement Learning From Human Feedback [16; 29] in aligning LLMs with human preferences, similar methods have been explored for T2I generation. The underlying idea is to train a model \(_{}\) that takes in an input along with the generated output (in this case a prompt and the corresponding image) and provides a score for the "goodness" of the generated output. Notable open-source human preference reward models for T2I include ImageNet  based on BLIP  and human preferences collected for the DiffusionDB dataset, PickScore , and HPSv2  both based on a CLIP  ViT-H/14 backbone. These reward models provide a quantitative measure of the image's quality and relevance to the prompt through a prediction by a differentiable neural network. Thus, they have not only been employed for the evaluation of T2I models but also to fine-tune them [17; 18; 100] as a means of achieving higher reward scores. Lastly, CLIPScore  has also been leveraged to measure the prompt alignment of a generated image.

To _generally_ enhance the performance of Text-to-Image models _without_ any fine-tuning, we propose to leverage a **Re**ward-based criterion function \(\) for Noise **O**ptimization (**ReNO**). Specifically, we propose to use a weighted combination of a number \(n\) of pre-trained reward models \(_{}^{0},\)\(_{}^{n}\) as the criterion function

\[}(_{0},)=_{i}^{n}_ {i}_{}^{i}(_{0},),\] (6)

where \(_{i}\) denotes the weighting for reward model \(_{}^{i}\). Employing a combination of reward models can help prevent "reward-hacking" and allow capturing various aspects of image quality and prompt adherence, as different reward models are trained on different prompt and preference sets. This not only effectively combines the strengths of multiple reward models, but also helps mitigate their weaknesses. **ReNO** then boils down to iteratively solving (4) with gradient ascent

\[^{t+1}=^{t}+_{^{t}}[K( ^{t})+_{i}^{n}_{i}_{}^{i}(G_{}( ^{t},),)],\] (7)

where \(\) is the learning rate. Similar to the color example in Figure 3, it is actually not desirable to find the optimal \(^{*}\) as we want to prevent adversarial samples that exploit the reward models. We find that already a few optimization steps (<50) of **ReNO** lead to significant improvements in both prompt following and visual aesthetics, striking a good balance between reward optimization and the prevention of reward hacking. Due to the efficacy of the proposed framework, generating one image, including noise optimization, takes between 20-50 seconds, depending on the model and image size, enabling its practical use. We provide a sketch of **ReNO** in Figure 2 and full details in Algorithm 1.

Figure 3: Initial noise optimization for one-step \(G_{}\) HyperSDXL with two color channel criterions (5).

## 3 Related Work

**Initial Noise Optimization.** The initial noise optimization framework was first introduced in DOODL  for improved guidance in Text-to-Image models. Subsequently, it was leveraged by Karunaratanakul et al.  for 3D universal motion priors, for rare-concept generation [61; 79; 80] and enhancing image quality [31; 89] in text-to-image models, music generation [62; 63], and by D-Flow  for solving inverse problems in various settings. While these methods mainly focus on controlling the generated sample for specific applications, our proposed method is designed to _generally_ improve Text-to-Image models without the need for additional techniques to mitigate exploding or vanishing gradients on the optimization process. Most related to our work is DOODL , which also proposes to improve the textual alignment of text-to-image models using a CLIP-score-based criterion function, which we similarly employ in our method. These existing methods, however, take 10 (DOODL) to 40 (D-Flow) minutes to generate a single image due to their application on time-dependent generative models with a large number of denoising steps. To mitigate this, Samuel et al.  propose a bootstrap-based method to increase the efficiency of generating a batch of images. However, this method is limited to settings where the goal is to generate samples including a concept jointly represented by a set of input images.

**Reward Optimization for Text-to-Image Models.** Reward models [46; 47; 97; 98; 100] were first introduced to mimic human preferences given an input prompt and generated images. There have been several attempts at incorporating these signals to enhance text-to-image generation. One notable direction is the idea of using reinforcement learning based algorithms to fine-tune text-to-image models to better align with these rewards either with an explicit reward model [8; 11; 18; 23; 30; 109] or by bypassing it entirely with Direct Preference Optimization [50; 74; 92; 101]. However, this can be expensive, requiring thousands of queries to generalize, and therefore a lot of work has explored directly fine-tuning diffusion models [17; 51; 72] using differentiable rewards [39; 46; 47; 98; 100]. Additionally, there has also been works exploring the concept of using reward models to perform classifier-guidance [4; 34] as well as using rewards to distill diffusion models into fewer steps [48; 75]. Differently from these works, we focus on adapting a diffusion model during inference by purely optimizing the initial latent noise using a differentiable objective.

## 4 Experiments

**Experimental Setup.** We evaluate the effectiveness of our proposed method, ReNO, using four open-source one-step image generation models: SD-Turbo, SDXL-Turbo, PixArt-\(\) DMD, FLUX-schnell and HyperSDXL. HyperSDXL generates images of size \(1024 1024\) while the others generate \(512 512\). To assess the performance across diverse scenarios, we consider three challenging tasks. First, we evaluate on T2I-CompBench , which comprises 6000 compositional prompts spanning six categories, using a VQA, object detection, and image-text matching scores. Second, we employ GenEval , consisting of 552 object-focused prompts, measuring the quality of the generated images using a pre-trained object detector. Finally, we utilize Parti-Prompts , a collection of more than 1600 complex prompts, and assess the generated images using both reward-based metrics and extensive human evaluation. Throughout all experiments, we optimize Equation (7) for 50 steps using gradient ascent with Nesterov momentum and gradient norm clipping for stability. Lastly, we select the image with the highest reward score from the optimization trajectory for evaluation.

### Effect of Reward Models

We analyze the effect of various reward models in Table 1. We see that optimizing ImageNet/or CLIPscore alone improves the text-image faithfulness (i.e., attribute binding from T2I-Compbench). However, this comes at the cost of decreased aesthetic score. PickScore and HPSv2 improve the image quality, however the gains in faithfulness are modest. Combining all the rewards leads to having strong improvements in faithfulness, while

    &  &  \\    & **Color \(\)** & **Shape \(\)** & **Texture \(\)** \\  SD-Turbo & 0.5513 & 0.4448 & 0.5690 & 5.647 \\ + CLIPScore & 0.6625 & 0.5501 & 0.6621 & 5.475 \\ + HPSv2 & 0.6443 & 0.5451 & 0.6859 & 5.752 \\ + ImageNet & 0.7720 & 0.6104 & 0.7334 & 5.611 \\ + PickScore & 0.6341 & 0.5059 & 0.6242 & 5.711 \\  + All & 0.7830 & 0.6244 & 0.7466 & 5.704 \\   

Table 1: SD-Turbo evaluated on the attribute binding categories of T2I-CompBench and the LAION aesthetic score predictor  for different reward models.

also increasing the image quality. Thus, we employ ReNO with all four reward models. We report further details in Appendix C, including the performance of all combinations of reward models.

### Quantitative Results

Table 2 presents the quantitative results of ReNO on T2I-Compbench. Most notably, we observe that for both Pixart-\(\) DMD and SD-Turbo, there are improvements of over 20% in the Color, Shape, and Texture Categories. For instance, on Color SD-Turbo improves from 55% to 78%, which is only slightly below DALL-E \(3\). Similar improvements can also be seen for SDXL-Turbo and HyperSDXL models where performance increases by over 10 percentage points in these categories. Even outside this, there are significant boosts in the Spatial, Non-Spatial, and Complex categories, highlighting

    &  &  &  \\    & **Color \(\)** & **Shape\(\)** & **Texture\(\)** & **Spatial\(\)** & **Non-Spatial\(\)** & \\  SD v1.4 & 0.38 & 0.36 & 0.42 & 0.12 & 0.31 & 0.31 \\ SD v2.1 & 0.51 & 0.42 & 0.49 & 0.13 & 0.31 & 0.34 \\ SDXL & 0.64 & 0.54 & 0.56 & 0.20 & 0.31 & 0.41 \\ PixArt-\(\) & 0.69 & 0.56 & 0.70 & 0.21 & **0.32** & 0.41 \\ DALL-E 2 & 0.57 & 0.55 & 0.64 & 0.13 & 0.30 & 0.37 \\ DALL-E 3 & **0.81** & **0.68** & **0.81** & - & - & - \\  (1) PixArt-\(\) DMD & 0.38 & 0.34 & 0.47 & 0.19 & 0.30 & 0.36 \\
**(1) + ReNO (Ours)** & 0.64 & 0.57 & 0.72 & 0.25 & 0.31 & 0.46 \\  (2) SD-Turbo & 0.55 & 0.44 & 0.57 & 0.17 & 0.31 & 0.41 \\
**(2) + ReNO (Ours)** & 0.78 & 0.62 & 0.75 & 0.22 & **0.32** & **0.48** \\  (3) SDXL-Turbo & 0.61 & 0.44 & 0.60 & 0.24 & 0.31 & 0.43 \\
**(3) + ReNO (Ours)** & 0.78 & 0.60 & 0.74 & **0.26** & 0.31 & 0.47 \\  (4) HyperSDXL & 0.65 & 0.50 & 0.65 & 0.25 & 0.31 & 0.46 \\
**(4) + ReNO (Ours)** & 0.79 & 0.63 & 0.77 & **0.26** & 0.31 & **0.48** \\   

Table 2: **Quantitative Results on T2I-CompBench**. ReNO combined with (1) PixArt-\(\) DMD , (2) SD-Turbo , (3) SDXL-Turbo , (4) HyperSD  demonstrates superior compositional generation ability in both attribute binding, object relationships, and complex compositions. The best value is bolded, and the second-best value is underlined. Multi-step results taken from .

  
**Model** & **Mean \(\)** & **Single\(\)** & **Two\(\)** & **Counting\(\)** & **Colors\(\)** & **Position\(\)** & **Color Attribution\(\)** \\  SD v2.1 & 0.50 & 0.98 & 0.51 & 0.44 & 0.85 & 0.07 & 0.17 \\ SDXL & 0.55 & 0.98 & 0.74 & 0.39 & 0.85 & 0.15 & 0.23 \\ IF-XL & 0.61 & 0.97 & 0.74 & 0.66 & 0.81 & 0.13 & 0.35 \\ PixArt-\(\) & 0.48 & 0.98 & 0.50 & 0.44 & 0.80 & 0.08 & 0.07 \\ DALL-E 2 & 0.52 & 0.94 & 0.66 & 0.49 & 0.77 & 0.10 & 0.19 \\ DALL-E 3 & 0.67 & 0.96 & 0.87 & 0.47 & 0.83 & **0.43** & 0.45 \\ SD3 (8B) & 0.68 & 0.98 & 0.84 & 0.66 & 0.74 & 0.40 & 0.43 \\  (1) PixArt-\(\) DMD & 0.45 & 0.95 & 0.38 & 0.46 & 0.76 & 0.05 & 0.09 \\ (**1) + ReNO (Ours)** & 0.59 & 0.98 & 0.72 & 0.58 & 0.85 & 0.15 & 0.27 \\  (2) SD-Turbo & 0.49 & 0.99 & 0.51 & 0.38 & 0.85 & 0.07 & 0.14 \\ (**2) + ReNO (Ours)** & 0.62 & **1.00** & 0.82 & 0.60 & 0.88 & 0.12 & 0.33 \\  (3) SDXL-Turbo & 0.54 & **1.00** & 0.66 & 0.45 & 0.84 & 0.09 & 0.20 \\ (**3) + ReNO (Ours)** & 0.65 & **1.00** & 0.84 & 0.68 & 0.90 & 0.13 & 0.35 \\  (4) HyperSDXL & 0.56 & **1.00** & 0.76 & 0.43 & 0.87 & 0.10 & 0.21 \\ (**4) + ReNO (Ours)** & 0.65 & **1.00** & **0.90** & 0.56 & **0.91** & 0.17 & 0.33 \\   FLUX-schnell & 0.64 & 0.98 & 0.80 & 0.64 & 0.78 & 0.18 & 0.43 \\ FLUX-schnell + ReNO (Ours) & **0.72** & 0.99 & **0.90** & **0.79** & 0.87 & 0.21 & **0.56** \\ FLUX-dev & 0.68 & 0.99 & 0.85 & 0.74 & 0.79 & 0.21 & 0.48 \\   

Table 3: **Quantitative Results on GenEval**. ReNO combined with (1) PixArt-\(\) DMD , (2) SD-Turbo , (3) SDXL-Turbo , (4) HyperSDXL  improves results across all categories. The best value is bolded, and the second-best value is underlined. Multi-step results taken from .

both the efficacy of the noise optimization framework, as well as the utility of human preference models for improving T2I generation at inference. Similar trends can also be noticed for GenEval in Table 3, where applying our noise optimization framework helps improve the performance of various one-step diffusion models. For instance, SD-Turbo improves its mean score from 0.49 to 0.62. Notably, our strongest model, HyperSDXL + ReNO, comes very close to the proprietary DALL-E 3 and SD3, i.e., beating DALL-E 3 on 4/6 categories in GenEval. In the case of FLUX-schnell, ReNO improves the performance (0.72) to even surpass that of the base FLUX-dev model (0.68). Most notably, this is the strongest open-source results reported on the GenEval benchmark. In both of these benchmarks, our noise optimization framework improves results for all the models in all the categories. It is also important to note that both T2I-Compbench and GenEval use a variety of methods unrelated to human preference rewards, such as VQA models and object detectors, to detect different objects in the generated images. We report further quantitative results including comparisons to other test-time-based methods in Appendix B. Additionally, these quantitative results are supported by the qualitative results reported in Figure 1 and Appendix A. Lastly, we report full details for the conducted FLUX-schnell experiments in Appendix E.8.

### User Study Results

To further validate ReNO we perform a user study on the commonly used Parti-Prompts  with Amazon Mechanical Turk (AMT). Parti-Prompts generally includes longer complex prompts that test artistic generation capabilities as opposed to T2I-Compbench and GenEval, which purely focus on faithfulness. We conducted user studies with ReNO applied to SD-Turbo for \(512 512\) and HyperSDXL for \(1024 1024\) generation. We compare SD-Turbo + ReNO against SD-Turbo, SDXL-Turbo, SD2.1 (50 Steps), and SDXL-Base (50 Steps). The results in Figure 4 confirm our findings in the quantitative evaluation. SD-Turbo + ReNO has an above 60% win rate against all benchmarked models reaching up to 77% against the SD-Turbo base. To contextualize these results, SD3  conducts a similar user study on Parti-Prompts and reports a 70% win rate against SDXL (50 steps). Our strongest base model, HyperSDXL, already beats SDXL (50 steps)  without ReNO. Thus, we compare it with and without ReNO as well as against the proprietary SD3 (8B) . Again, HyperSDXL + ReNO achieves an above 60% win rate, and notably, it also narrowly beats SD3 with 54%. This confirms our finding in ReNO, which substantially improves overall generative quality, pushing results at least close to the ones of even current state-of-the-art proprietary models. Lastly, we note that user studies on AMT can potentially be noisy and, therefore, view the results holistically along with quantitative evaluation. We provide a detailed breakdown of the preference for image quality and faithfulness, as well as full details of the user study in Appendix D.

### Computational Cost of ReNO

The primary concern of our proposed method is the increased inference cost since existing methods (e.g. DOODL, D-Flow) are impractical for regular T2I generation usage. However, we circumvent this issue through our restriction to one-step models and 50 optimization steps, which makes ReNO run in 20-50 seconds. To analyze the performance of ReNO with respect to the number of optimization steps we evaluate its performance over a set of reference points. We report results on the attribute binding part of T2I-CompBench for SD-Turbo + ReNO in Figure 5 and visually corroborate these results with Figure 6. Note that even when restricted to the same compute budget as SDXL (50 steps, ~7sec), SD-Turbo + ReNO significantly outperforms it while in this comparison PixArt-\(\) (20 steps, ~7sec) lies shortly below the Pareto-frontier of ReNO.

Figure 4: User Study Results for ReNO

Figure 5: Attribute binding results on T2I-CompBench with varying number of iterations.

### Effect of ReNO on the Diversity of Generated Images

To investigate the effect of noise optimization on output diversity, we evaluate images generated across 50 different random seeds for 110 prompts from Parti-Prompts. Specifically, we generate a batch of images and use LPIPS  and DINO [9; 64] scores to compute the average similarity of the generated batch, where a lower similarity score corresponds to higher diversity. As shown in Table 4, one-step models (SD-Turbo, SDXL-Turbo) exhibit lower diversity compared to their multi-step counterparts (SD2.1, SDXL), likely due to adversarial training. However, applying ReNO not only maintains but actually _increases_ diversity. For both SD-Turbo and SDXL-Turbo, ReNO achieves diversity levels approaching their respective multi-step base models highlighting an unexpected benefit of noise optimization increasing diversity. Figure 10 illustrates these improvements qualitatively. We hypothesize that the reason for this increased diversity is that ReNO adds structure to the noise, thus optimizes it away from the zero mean of the noise distribution and creating more diverse noises compared to sampling from the standard Gaussian.

### Comparison to Multi-Step Noise Optimization

We benchmark ReNO against DOODL , which performs noise optimization using the 50-step SD2.1 model. Due to DOODL's computational demands, we evaluate on the first 50 prompts from T2I-CompBench. Despite using the same CLIPScore objective, ReNO achieves four times larger improvements in the optimized criterion while requiring 75% less GPU memory and running 100x faster, highlighting the effectiveness of our one-step approach. Moreover, ReNO's multi-reward objective leads to substantially larger gains in attribute binding accuracy (21.0-28.9%) compared to solely using CLIPScore, reiterating the efficacy of ReNO's optimization objective.

    & LPIPS \(\) & DINO \(\) \\  SD-Turbo & 0.382 \( 0.043\) & 0.770 \( 0.101\) \\
**SD-Turbo + ReNO** & 0.246 \( 0.046\) & 0.712 \( 0.132\) \\ SD2.1 (50-step) & **0.243**\( 0.049\) & **0.623**\( 0.150\) \\  SDXL-Turbo & 0.391 \( 0.044\) & 0.835 \( 0.073\) \\
**SDXL-Turbo + ReNO** & **0.291**\( 0.041\) & 0.763 \( 0.116\) \\ SDXL (50-step) & 0.351 \( 0.042\) & **0.700**\( 0.128\) \\   

Table 4: We measure the average LPIPS and DINO similarity scores over images generated for 50 different seeds for 100 prompts from Parti-Prompts.

Figure 6: The initial images are generated with four different one-step models \(G_{}\) given the prompt p ”A yellow reindeer and a blue elephant” and randomly initialized noise \(^{0}\). Each column shows the result of optimizing the noise latent \(^{t}\) for \(t\) steps with respect to our reward-based criterion.

### Limitations

An interesting observation in our experiments is that despite using different image generation models of varying architectures and sizes, they broadly converge to similar performance on both T2I-Compbench and GenEval. In addition to the limitations of the generative models, we hypothesize that this could be due to the limitations of the reward models themselves, given their limited compositional reasoning abilities . Stronger reward models  and preference data  would be crucial in enhancing results further.

Secondly, not only the runtime but also the amount of needed GPU VRAM is significantly higher when using ReNO. We reduce it by leveraging fp16 quantization and the pytorch  memory reduction technique introduced in Bhatia and Dangel , which for ReNO lowers the VRAM by another -15%. Then, all of the models can be optimized on a single A100 GPU in 20-50 seconds, and e.g., SD-Turbo requires only 15GB VRAM for the entire optimization process. Note, however, that the amount of VRAM also scales with the size of the generated image. Thus, HyperS-DXL needs 39GB of VRAM. We provide a summary of the computational cost of ReNO in Table 6, which lays out ReNO's main limitation. Finally, current T2I models struggle with generating humans, rendering text, and also modeling complex compositional relations. While our work attempts to alleviate these issues and provides a flexible framework for further improvements, future work is required to resolve these issues.

## 5 Conclusion

We introduce ReNO, a test-time optimization strategy for enhancing text-to-image generation without any fine-tuning. Not only do we achieve the strongest results among all open-source models on T2I-Compbench and GenEval, but images from ReNO on a single-step SD-Turbo have over a 60% win rate against a 50-step SDXL model and is competitive with the 8B parameter SD3 model on user studies. We also demonstrate that ReNO outperforms SDXL even when restricted to the same computational budget, highlighting the benefits of ReNO for practical use cases. The performance gains from ReNO underscore the importance of developing even better and more robust reward models and, moreover, establish a valuable benchmark for assessing their effectiveness. Furthermore, the substantial impact of optimizing the initial noise distribution motivates further research into understanding, manipulating, and controlling this crucial aspect of generative models.