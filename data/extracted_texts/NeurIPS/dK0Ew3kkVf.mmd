# Keypoint-Augmented Self-Supervised Learning for Medical Image Segmentation with Limited Annotation

Zhangshao Yang

Arizona State University

zshyang1106@gmail.com

Equal contribution.

&Mengwei Ren

New York University

mengwei.ren@nyu.edu

Kaize Ding

Northwestern University

kaize.ding@northwestern.edu

&Guido Gerig

New York University

gerig@nyu.edu

&Yalin Wang

Arizona State University

ylwang@asu.edu

###### Abstract

Pretraining CNN models (i.e., UNet) through self-supervision has become a powerful approach to facilitate medical image segmentation under low annotation regimes. Recent contrastive learning methods encourage similar global representations when the _same_ image undergoes different transformations, or enforce invariance across _different_ image/patch features that are intrinsically correlated. However, CNN-extracted global and local features are limited in capturing long-range spatial dependencies that are essential in biological anatomy. To this end, we present a keypoint-augmented fusion layer that extracts representations preserving both short- and long-range self-attention. In particular, we augment the CNN feature map at multiple scales by incorporating an additional input that learns long-range spatial self-attention among localized keypoint features. Further, we introduce both global and local self-supervised pretraining for the framework. At the global scale, we obtain global representations from both the bottleneck of the UNet, and by aggregating multiscale keypoint features. These global features are subsequently regularized through image-level contrastive objectives. At the local scale, we define a distance-based criterion to first establish correspondences among keypoints and encourage similarity between their features. Through extensive experiments on both MRI and CT segmentation tasks, we demonstrate the architectural advantages of our proposed method in comparison to both CNN and Transformer-based UNets, when all architectures are trained with randomly initialized weights. With our proposed pretraining strategy, our method further outperforms existing SSL methods by producing more robust self-attention and achieving state-of-the-art segmentation results. The code is available at [https://github.com/zshyang/kaf.git](https://github.com/zshyang/kaf.git).

## 1 Introduction

Large-scale and diverse source of training data has significantly empowered the generalization ability of supervised segmentation models . However, in biomedical image analysis, manual delineation of images/volumes requires extensive domain knowledge and can be extremely time-consuming and error-prone. Recently, self-supervised learning (SSL)  has emerged in medical vision to pretrain a segmentation backbone (e.g. UNet) leveraging large scale unlabelled data, to serve as a better initialization before finetuning the model with limited annotation.

As one of the most successful learning paradigms in SSL, contrastive learning pulls closer the representations of positive pairs (i.e. the same image under different augmentations), while pushing apart the negative pairs (i.e. different images) [9; 28; 10; 25; 46; 11; 12]. However, in medical images, strong anatomical similarities often exist across different images, resulting in an increased number of false negative samples which is detrimental to the representation learning process. Therefore, heuristics about relationship across different images are utilized to constitute positive pairs, such as encouraging the global representations of 2D slices at similar volumetric positions [7; 72] to be close. To further benefit pixel-level tasks such as segmentation, local representation learning methods [7; 51] learn distinctive local representations by attracting patch features at the same position from two augmented views , or across registered intra-subject volumes . Nevertheless, the potential dependencies among patch features that are spatially distant are not properly taken into account. Such region-awareness can be enhanced by attracting spatial features points with the same semantics [35; 27; 38; 75; 1]. However, their training requires segmentation ground truth, which limits their applicability in medical vision when adequate annotation is unavailable.

In this paper, we incorporate the long-range spatial dependencies into the UNet backbone with a plug-and-play keypoint-augmented fusion layer (KAF layer), accompanied by keypoint-enhanced global and local objectives for pretraining the network with self-supervision. To do so, we first identify a set of sparse keypoint locations from the input image. On the output feature map of each UNet encoder block, the keypoint features are sampled from the convolutional feature map, and the attention among them are modeled through a Vision Transformer. The output is then scattered back to the feature grid, and gets fused to the original CNN feature, to explicitly provide clues on the spatial long-range dependencies. Further, we propose the global and local self-supervised learning objectives to pretrain the keypoint-augmented fusion layer -enhanced network. The global contrastive loss is applied on both the UNet bottleneck feature, as well as keypoint-augmented global feature obtained by aggregating the multiscale keypoint features. To further benefit pixel-level down-stream tasks, we identify the correspondence among keypoints across different image slices and maximize their feature similarities with a local self-supervised objective.

Our contributions are threefold. (1) We develop a plug-and-play keypoint-augmented fusion layer that augments the convolutional feature map with long-range dependencies among keypoint features. When trained with only limited annotation, our proposed KAF layer achieves noticeably better results than the CNN-only and/or Transformer-based backbone. (2) We further propose the keypoint-aware global and local self-supervised learning objectives to pretrain our model before finetuning it with annotations. (3) We conduct extensive experiments on three MRI and CT datasets, and achieve state-of-the-art few-shot segmentation results, verifying both architecture advantage of the proposed layer and our pretraining strategies.

## 2 Related Work

**Biomedical Image Segmentation** is the process of dividing images into various segments or regions and isolating certain structures of interest in modalities such as magnetic resonance imaging (MRI) and computed tomography (CT) [50; 37]. UNet  is the pioneering method for addressing biomedical image segmentation tasks using deep learning. Subsequent research aims to enhance or apply the UNet model in various ways, such as extending to 3D volumes [44; 64], auto-configuration , altering skip connections [77; 66], and replacing and/or combining the convolutional backbone layers with Transformers [23; 5; 22; 30]. However, a common limitation shared by all end-to-end methods lies in the requirement of large-scale and high-quality biomedical annotations which are both time-consuming and requires expert knowledge . This limitation underscores the need for novel methods that can learn effectively with fewer or without annotations.

**Self-Supervised Learning (SSL)** learns meaningful representations from unlabeled datasets with self-supervised objectives, and the pretrained network constitutes a better initialization than random initialization when it is finetuned on downstream tasks with limited label supervision, e.g., classification or segmentation. Existing SSL algorithms either define pretext tasks with heuristics, or employ self-supervised objectives to learn invariance and/or equivariance of image features. Early pre-text tasks based pretraining designs objective that does not require additional labels, such as rotation prediction , context restoration , reconstruction , among others [24; 42; 15]. Recently, contrastive learning has become a prevailing paradigm in SSL and has shown great success in both natural image [6; 59; 78; 63; 76] and medical vision [7; 72; 51; 58; 71; 70; 56; 49; 8; 62; 16; 47; 73; 68].

It enforces similar representations between positive pairs and distinct representations between negative pairs. Typically, similarity is determined in an unsupervised way, i.e., the same image under different transformations are recognized as positive pairs, whose global representations should remain invariant, and features from different images are pulled apart. In medical images, heuristics about relationships among different images are also utilized to define positives, e.g., 2D slices at similar volumetric positions , or local patches at the same location from intra-subject volumes .

**Long-Range Dependency** generally cannot be well captured in CNN due to its locality nature, thus neglecting the potential relationship across spatially distant local regions that possess high semantic consistency. To tackle the limitation, existing works focus on either (1) architectural design that injects long-range attention, or (2) SSL objectives that explicitly enforce regional similarity. For the first line of research, a series of non-local networks have been proposed. For instance, Vision Transformer (ViT)  enables self-attention on uniformly sampled image patches and has been incorporated into CNN backbones for segmentation tasks ;  adopt graph networks to model dependencies among context, keypoints or boundaries. The other line of research captures regional correlation with customized contrastive formulation objective, typically in a label-supervised manner , where local features within the same semantic label form the positive pairs. In our setting, we eliminate the need of segmentation labels via establishing correspondence among detected keypoints and apply local self-supervision on the keypoint features.

**Keypoint Descriptors** have been successfully exploited in various domains to improve the performance on tasks such as pose estimation , image generation  and object detection . Specifically, in biomedical imaging, keypoint descriptors have found applications in a variety of areas, including anatomy object classification , medical image retrieval , segmentation tasks , motion estimation of organs , and medical image registrations . They aim to incorporate sparse and localized key points into the training process, in order to attend the network to the most important features within the image. This is typically done via first localizing a sparse set of keypoint locations in the image, where keypoint features are obtained and their interactions are modeled. Finally, the feature is aggregated and used for downstream task prediction. In this work, we propose to inject learnable keypoint descriptors into the UNet building block so that the network is able to model complex long-range attention without a significant increase in computational cost.

## 3 Methodology

Fig. 1 illustrates an overview of the proposed method. Built upon a segmentation backbone (i.e., UNet), we attach our proposed keypoint-augmented fusion layer (Sec. 3.1) after each convolutional block, to inject long-range self-attention into the original convolutional feature map. Further, we propose both global and local self-supervision (Sec. 3.2) to pretrain the network before finetuning with limited annotation.

### Keypoint-Augmented Fusion (KAF) Layer

The proposed KAF layer is diagrammed in the red dashed box in Fig. 1. Given an input image \(^{W H C}\), we start with detecting keypoints \(^{N 2}\) using a keypoint detector \(_{k}:^{W H C}^{N 2}\). In particular, we employ the Scale-Invariant Feature Transform (SIFT) . We note that alternative keypoint detection methods can also be applied, and we have included the ablation in the appendix. At \(l\)-th layer of the 2D UNet, we acquire \(N\) CNN features \(_{k}^{l}^{N C^{l}}\) from the dense convolutional feature map \(^{l}^{W H^{l} C^{l}}\), based on the keypoint coordinates \(^{l}\). Note that at each scale, \(^{l}\) is acquired by re-scaling \(\) with the resolution ratio between the input image and the feature map. Then, we map the keypoint features to the embedding space with dimension \(E\) with a projector \(:^{N C^{l}}^{N E}\). We use a single-layer MLP in our implementation as the projector. The embedded features are then fed into a Transformer \(:^{N E}^{N E}\) to learn the self-attention among keypoints. and we define the transformed feature as \(U^{l}^{N E}\) (which is further used as the keypoint feature at layer \(l\) for calculating the SSL losses). Additionally, to properly propagate the keypoint features learned at the current scale \(l\) to \(l+1\) within the convolutional UNet, \(U^{l}\) is scattered back into the feature grid indexed by the rescaled keypoint position \(^{l}\). The output is sparse feature map \(_{s}^{l}\) which retains information exclusively at the keypoint positions, and the rest of the feature map are assigned with zeros.

We further attach a two-layer CNN \(_{l}\) to diffuse the sparse keypoint feature map into a dense image feature. Lastly, we concatenate the diffused keypoint feature map with the input feature map \(F^{l}\), constituting the final output of the proposed layer, represented as \(^{i}_{o}=(F^{l},_{l}(^{l}_{s}))\).

### Keypoint-Augmented Self-Supervised Learning

Furthermore, we design the global and local self-supervised learning algorithms to enhance the keypoint-augmented feature learning, thus the learned feature representations can be well generalized to different downstream tasks through finetuning.

**Global SSL Loss.** In order to apply a global contrastive loss (e.g., [9; 72]) that learns image wise feature similarity, we first aggregate multiscale keypoint features and extract a keypoint-enhanced global representation for each input image. Specifically, keypoint features from four encoder blocks are first concatenated as \(Concat(U^{1},U^{2},U^{3},U^{4})\), which is then fed into a Multilayer Perceptron (MLP) to get a global feature \(g_{i}\), where \(i\) is the index of the input image from the dataset. In our training, we adopt the global contrastive loss from PCL  on the keypoint-enhanced global feature, which assumes 2D slices within a certain positional distance (within the 3D volume) threshold to be anatomically similar and constitute similar pairs. Formally, the global self-supervised learning loss function can be formulated as:

\[L^{i}_{global}=-_{i}|}_{j^{+}_{i}},g_{j})}{}}}{_{k=1}^{2N}1_{1 k} e^{ { im(g_{i},g_{k})}{}}}, \]

where \(sim(,)\) is the cosine similarity between two vectors in the embedding space, \(\) indicates the temperature term. \(^{+}\) is the set of positive images to the input \(x_{i}\).

We additionally keep the global contrastive loss applied on the features from the last layer of the UNet encoder, as commonly done in existing SSL literature [72; 7; 9]. To do so, a global pooling layer transforms the feature \(^{last}\) from \(^{W^{last} H^{last} C^{last}}\) to \(^{1 C^{last}}\). Afterward, an MLP projector is applied to obtain the global embedding for contrastive objectives, denoted as \(c_{i}\). Similar to Eq. 1, the loss can be formulated by substituting \(g_{i}\) with \(c_{i}\) and \(g_{j}\) with \(c_{j}\) in Eq. 1. As we adopt PCL loss  to the global CNN feature, we denote this loss as \(L^{i}_{PCL}\) in the following context.

**Local SSL Loss.** While  only considers image-wise similarity based on the slice positions, we claim that pixel-wise similarity across slices should also be exploited to supervise the keypoint feature learning, enhancing fine-grained and localized control of the learned representation. We

Figure 1: **Top**: The overview of the proposed SSL framework that incorporates both local and global self-supervision with the keypoint guidance. The UNet decoder is omitted for better readability. **Bottom**: The proposed Keypoint-Augmented Fusion Layer (KAF layer), which learns long-range spatial dependencies among localized keypoint features. We insert the KAF layer after each encoder block of the UNet backbone to augment the original convolutional features.

leverage the intrinsic structural correlation in 3D medical imaging, where nearby slices display strong local similarities. For instance, in cardiac imaging, anatomical structures such as blood vessels and ventricles span across multiple 2D slices within the 3D volume. Therefore, keypoints within a certain spatial distance from adjacent slices are likely to be semantically correlated and form positive pairs.

In order to locate the correspondence for each keypoint, we define two major criteria based on heuristics: (1) the spatial distance in the 2D plane between two correspondences should be within a threshold; (2) the distance between their SIFT features should be similar. Correspondingly, we first sample two positive slices along with their keypoints \((_{i},_{i})\) and \((_{j},_{j})\), where the positional distance between \(_{i}\) and \(_{j}\) fall within the positive threshold defined in . For each keypoint \(a_{i}\), we aim to find its correspondence from \(_{j}\). We do so by first filtering out a set of keypoints whose Manhattan distances with \(a\) is less than a predefined threshold. Then, we find the keypoint \(b\) as the correspondence of \(a\) based on the closest L2 distance of the SIFT features. Once matched, \((a,b)\) is added into the groundtruth match set \(\). However, if no points in slice \(_{j}\) meet the criteria, the candidate point \(a\) is treated as a negative sample and added to the unmatched set \(\). Similarly, for slice \(_{j}\), the candidate point without match is added to the unmatched set \(\). \(,,\) are then used as the groundtruth for the keypoint feature matching detailed below.

We repurpose SuperGlue  as the backbone method to learn the correspondence between keypoint features on slice \(_{i}\) and keypoints on slice \(_{j}\), supervised by the correspondence extracted above. We first compute the feature associated with keypoints \(U_{i}=(U_{i}^{1},U_{i}^{2},U_{i}^{3},U_{i}^{4})\), and \(U_{j}=(U_{j}^{1},U_{j}^{2},U_{j}^{3},U_{j}^{4})\), along with their groundtruth matches \(\) and unmatched sets \(\) and \(\). Then, SuperGlue takes \(U_{i}\), \(U_{j}\), \(_{i}\), and \(_{j}\) as inputs and computes an output \(^{(N_{i}+1)(N_{j}+1)}\) through self- and cross- graph message passing. Here, \(_{a,b}\) represents the probability that keypoint \(a_{i}\) matches with \(b_{j}\). Given \(\), \(\), and \(\), we minimize the negative log-likelihood of the assignment \(\), formulated as follows:

\[L_{local}^{i}=-_{(a,b) M}a,b-_{a I}_{a,N_{ i}+1}-_{b J}_{N_{j}+1,b}. \]

By establishing such localized correlation among keypoint features, we assume the network is able to better recognize local correspondence even when the input image undergoes various transformations. Such localized correlation ensures that the network can maintain a consistent understanding of the relationships between keypoint features, regardless of changes in the image appearance or orientation. Our assumption is empirically verified from the computed equivariant self-attention map shown in Fig. 3.

**Total SSL Loss.** With the proposed keypoint-augmented fusion layer integrated into existing CNN backbone, the model can be pretrained with the above global and local losses, optionally with existing global contrastive loss (in our case, PCL ). The final objective is:

\[L_{total}=w_{1} L_{PCL}+w_{2} L_{global}+w_{3} L_{local}, \]

where the coefficients \(w_{1}\), \(w_{2}\), and \(w_{3}\) are employed to balance the contributions of each term. In our experiments, we use a grid search method to optimize these coefficients.

## 4 Experiments

**Datasets.** We conduct experiments on two publicly accessible cardiac MRI datasets for the task of segmentation under limited annotation. For fair benchmarking, we follow the same preprocessing steps as . For each cross-validation fold, \(N\)images are held-out for validation, and varying \(M\) images from the remaining images are used for the few-shot training. We additionally include results on a non-cardiac CT dataset Synapse1 in the appendix to illustrate the generalization of our method.

CHD (Congenital Heart Disease)  is a CT dataset consisting of 68 3D cardiac images, covering patent ages ranging from 1 month to 21 year. It includes 14 types of congenital heart disease and the segmentation labels consists of seven distinct substructures: left ventricle (LV), right ventricle (RV), left atrium (LA), right atrium (RA), myocardium (Myo), aorta (Ao), and pulmonary artery (PA). For each cross-validation fold, \(N\)=18 images are used for validation, and \(M\) images are selected from the remaining 50 images for few-shot training.

ACDC (Automatic Cardiac Diagnosis Challenge)  is a MRI dataset consists of cardiac images from 100 patients under several different pathologies. Manual expert segmentation of the RV, LV cavities, and Myo are conducted for the volumes from end-diastolic and end-systolic phase. For each cross-validation fold, \(N=20\) images are used as the validation set, and \(M\) images from the remaining 80 images are used for supervised finetuning.

**Baselines and Evaluation Settings.** We evaluate segmentation performance trained with limited annotation under both (1) random initialized network backbones (UNet v.s. Ours) to verify the architectural advantages of the proposed keypoint-augmented fusion layer. (2) pretrained weights from our self-supervision compared with various self-supervised pretraining methods including pre-text tasks  and contrastive learning . We quantify the segmentation performance via the Dice coefficient with five-fold cross validataion following the set up in . Additionally, we conducted ablation studies to isolate each component in the pipeline to assess individual effects.

**Implementation Details.** For network configuration, we build our framework on 2D UNet, and set the starting number of channels of the network as 32 for CHD, and 48 for ACDC. Five convolutional blocks are included in the encoder and four blocks are used within the decoder. Each block contains two convolution layers, followed by batch normalization, and ReLU activation. Each encoder block downsample the feature map by a factor of 2, while simultaneously doubling the number of channels. In between the first four encoder blocks, we further append the proposed keypoint-augmented fusion layer, and concatenate the output to the original convolutional feature map (Fig. 1). Each Transformer inside KAF layer consists of six self-attention layers, and the comparison among different number of self-attention layers are presented in ablation study below. For pretraining, we assign loss weights \(w_{1},w_{2},w_{3}\) to 1.0, 1.0, and 0.01, respectively. We employ the SGD optimizer with a learning rate of 0.002 and batch sizes of 32 for CHD and ACDC. A cosine learning rate scheduler is utilized, with the minimum learning rate set to 0. We pretrain the model for 50 epochs. For finetuning, we use the standard cross-entropy loss with the Adam  optimizer, with learning rates of to \(5 10^{-5}\) for CHD, \(5 10^{-4}\) for ACDC. The batch size is set to 10, and we finetune on CHD for 100 epochs and on ACDC for 200 epochs. Additional implementation details are provided in the appendix.

    & &  \\  Init. & Method & \(M\)=2 & \(M\)=6 & \(M\)=10 & \(M\)=15 & \(M\)=20 & \(M\)=30 & \(M\)=51 \\   & UNet  & 0.184(0.06) & 0.508(0.06) & 0.584(0.05) & 0.627(0.05) & 0.658(0.04) & 0.693(0.04) & 0.754(0.02) \\  & Swin-Unet  & 0.291(0.07) & 0.543(0.07) & 0.624(0.04) & 0.675(0.05) & **0.717(0.04)** & **0.732(0.05)** & 0.784(0.03) \\  & SwinUNETR  & 0.345(0.07) & 0.565(0.06) & 0.638(0.05) & 0.682(0.06) & 0.711(0.05) & 0.725(0.06) & **0.785(0.03)** \\  & Ours & **0.344(0.05)** & **0.576(0.07)** & **0.646(0.03)** & **0.686(0.03)** & 0.706(0.03) & 0.728(0.04) & 0.778(0.03) \\    } & Rotation  & 0.171(0.06) & 0.488(0.07) & 0.575(0.04) & 0.625(0.04) & 0.651(0.04) & 0.691(0.04) & 0.749(0.03) \\  & PIRL  & 0.196(0.07) & 0.504(0.08) & 0.617(0.05) & 0.658(0.03) & 0.674(0.04) & 0.714(0.04) & 0.761(0.03) \\  & SimCLR  & 0.192(0.05) & 0.515(0.06) & 0.599(0.06) & 0.631(0.05) & 0.666(0.05) & 0.699(0.05) & 0.756(0.03) \\  & GLCL-_global_ & 0.255(1.0) & 0.564(0.04) & 0.646(0.03) & 0.669(0.04) & 0.697(0.04) & 0.725(0.04) & 0.766(0.03) \\  & GLCL-_[full_7] & 0.286(0.06) & 0.555(0.07) & 0.614(0.06) & 0.666(0.04) & 0.694(0.03) & 0.773(0.04) & 0.772(0.03) \\  & CALD  & 0.266(0.08) & 0.581(0.06) & 0.647(0.04) & 0.684(0.04) & 0.700(0.04) & 0.737(0.04) & 0.771(0.02) \\  & PCL  & 0.356(0.08) & 0.600(0.06) & 0.661(0.05) & 0.686(0.05) & 0.716(0.04) & 0.735(0.05) & 0.774(0.03) \\  & Ours & **0.392(0.06)** & **0.636(0.06)** & **0.693(0.03)** & **0.712(0.03)** & **0.728(0.04)** & **0.754(0.04)** & **0.78

**Segmentation Results without Pretraining.** To verify the benefits of the proposed layer, we start with analyzing the performance gain without any pretraining, to isolate and evaluate the impact solely attributed to the architectural change on top of the UNet backbone. Specifically, we train the randomly initialized UNet and our backbone with varying limited number of training data, and the results are shown in Table 1 under Init.Random. Compared with the UNet backbone with an identical number of convolutional blocks and network configurations, introducing KAF layer consistently and significantly improves the segmentation results on both datasets across various training sample sizes. The empirical findings strongly support our assumption that incorporating long-range dependencies into the segmentation backbone is crucial and helps better modeling and utilizing regional image information when only limited annotations are available. Notably, our network demonstrates superior performance over the UNet baselines when the training size is extremely small. For instance, when the sample size is reduced to two (\(M=2\)) on the CHD dataset, the inclusion of KAF layer leads to 87% performance gain in dice score over UNet. We also include Transformer-based UNets , which also consider long-range spatial dependencies. Our observation is consistent with the conclusion in , where Transformers may be more severely affected by the initialization and require large \(M\) in order to perform well, while method obtain much better results when only limited data is available.

Fig. 2 Random Initialization shows a visual comparison of the segmentation between UNet and Ours. While both methods suffer from sub-optimal generalization ability on the input image from the unseen validation set, our method largely reduces false positive prediction and presents higher similarity with the reference image on the right. Such improvement can be attributed to the utilization of keypoints, which enhances the feature learning process by directing attention to a more constrained set of regions rather than the entire image space. we deduce that this focused awareness contributes to better localization of important features thus leading to better semantic segmentation.

**Segmentation Results with Pretraining and Finetuning.** To further enhance the segmentation performance and promote better generalization of our model, we pretrain the network with the proposed SS objectives and leverage the pretrained weights as the initialization for fine-tuning on the same amount of labeled dataset. Quantitative results are shown in Table 1 under the tab SSL pretrain initialization, where benchmarking is conducted across existing state-of-the-art SSL pretraining methods. Notably, our pretraining technique yields significant improvements in few-shot

Figure 2: **Few-shot segmentation results.** Colomn (a)-(c) show a comparison among the UNet backbone, SwinUNETR , and our proposed backbone trained on limited dataset with random weight initialization. Our architecture allows for reduced false positive and improved segmentation accuracy. Column (c)-(e) compares our pretraining strategy with the existing state-of-the-art SSL methods  and . When finetuned with small number of labeled dataset, our method presents higher coherence with the reference image (column (f)). The number beneath each prediction represents the dice value for the displayed slice. For better readability, we scale the value by 100.

segmentation tasks compared to the random initialization. Moreover, in comparison to alternative pretraining strategies that do not incorporate long-range dependencies, our method consistently achieves state-of-the-art dice scores across different numbers of training subjects for both datasets, demonstrating the benefits of the proposed keypoint-augmented SSL objectives. In Fig. 2 SSL initialization, we present visual comparisons between our method and the best-performing SSL methods GLCL  and PCL . Visually, we observe that GLCL tends to produce false positives or incorrectly labels the anatomy, as evidenced by the CHD result. Conversely, PCL tends to generate false negatives, as observed in the ACDC example. In contrast, our proposed method achieves the highest level of similarity with the reference image, accurately capturing the desired anatomical structures. These visual comparisons further reinforce the effectiveness and superior performance of our approach in comparison to existing SSL methods.

**Ablation Study.** As our proposed framework consists of both architectural change on the UNet building blocks and pretraining strategy, we conduct ablation analysis over different architectural configurations and pretraining objectives, with results presented in Table 2. Additional ablations on keypoint detection method and threshold for identifying correspondence are provided in the appendix.

Architecture (Exp A-H). We start with architectural analysis by training different randomly initialized backbone networks under different configurations over the number of self-attention (\(\#\)T) within the KAF layer, and the injection of KAF layer at different encoder scales \(l_{1},,l_{4}\).

Exp A represents a UNet backbone without the use of KAF layer. With the same \(\#\)T, Exp F inserts four KAF layer after each of the encoder layer. A significant performance gain was obtained on CHD by 0.06, and on ACDC by 0.04. To analyze layerwise effects, we remove one KAF layer each time from Exp B-E, and observe a degradation in performance, indicating the importance of utilizing a multiscale setting. We observe that the last layer plays the most crucial role in achieving superior segmentation results. In addition, we investigate the effect of the number of self-attention within the Transformer used in our architecture in Exp F-H. We observe that the performance reaches a plateau with \(\#T=9\), and when it reduces to 3, the performance further degrades. Nevertheless, Exp H still overperforms Exp A, indicating the benefits of introducing long-range self-attention.

Pretraining Strategy (Exp I-L). On the architectures of Exp F, we further add the proposed pretraining strategies, and isolate the different loss weights during the training. \(w_{1},w_{2}\) indicate the global PCL loss applied on the convolutional feature and keypoint features respectively, \(w_{3}\) indicates the local loss weight (See Eq. 3). Compared with single global loss on either the global CNN feature (Exp I) or the keypoint feature (Exp J), a combination of the two terms (Exp K) shows improved results on CHD. The best performance is achieved with a combination of both global and local loss (Exp L). Additional weight tuning details are provided in appendix.

**Qualitative Analysis of the Learned Self-Attention.** In Fig. 3, we qualitatively compare the learned self-similarity from our pretraining, with the established PCL  (global), and GLCL  (global and local) pretraining. Given an input image \(x\), we perform two random transformations \(T_{1}\) and \(T_{2}\) to obtain a simulated positive pairs in contrastive set up. A query point is randomly sampled from

    &  &  &  &  \\   & & \#\(\) & \(l_{1}\) & \(l_{2}\) & \(l_{3}\) & \(l_{4}\) & \(w_{1}\) & \(w_{2}\) & \(w_{3}\) & CHD (\(M\)=15) & ACDC (\(M\)=6) \\   & A & 9 & 0 & 0 & 0 & 0 & - & - & - & 0.627(.05) & 0.782(.03) \\  & B & 9 & 1 & 1 & 1 & 0 & - & - & - & 0.658(.04) & 0.806(.04) \\  & C & 9 & 1 & 1 & 0 & 1 & - & - & - & 0.677(.04) & 0.817(.04) \\  & D & 9 & 0the \(x\) and gets transformed to \(T_{1}(x)\) and \(T_{2}(x)\) respectively (stars in Fig. 3). The image is fed into the pretrained network, and the feature map from the \(i\)-th layer is obtained. The self-similairty \(S\) is computed as the dot product between the query point feature and other feature points within the same map. Fig. 3 visualizes the self-attention from the third and fourth layer of the pretrained UNet (\(l_{3},l_{4}\)).

We observe two major differences between our method and PCL and GLCL: (1) our keypoint-augmented fusion layer helps the network to concentrate its attention on more constrained sub-regions. For example, both PCL and GLCL tend to learn high response values between the query point and background regions (e.g., Fig. 3 (b), (e)), whereas our method exhibits a more refined attention mechanism, directing the focus of the query point towards anatomically relevant regions, which helps the model to capture and leverage important features within the desired regions, leading to improved segmentation performance. (2) our proposed self-supervision helps maintain better local equivariance of the self-attention, i.e., with the same query point location, its interaction with other points remains identical no matter how the image is transformed. This is verified by a comparison between Fig. 3 (a) v.s. (b), and (d) v.s. (e), where our method achieves the best consistency of the local similarity when features are extracted from the image with different transforms.

Additionally, we calculate the self-attention based on the input image \(x\) and transform the resulting similarity map using \(T_{2}\). The transformed similarity map is expected to exhibit a high level of coherence with the similarity map obtained from the transformed input. Compared with PCL and GLCL, our method demonstrates stronger consistency between (b) and (c), as well as (e) and (f), which validates the advantage of introducing the keypoint-augmented self-supervised learning.

## 5 Discussion

In this work, we present a keypoint-augmented fusion layer to incorporate long-range dependencies into the UNet-based segmentation framework, accompanied by global & local SSL objectives for pretraining. Open questions exist and will be included in future work. Currently, our backbone is built upon 2D UNet following [72; 7], while 3D UNet naturally serves as a better baseline in many biomedical segmentation tasks . Although our method is generic and can be scaled up to 3D data, it may require different architectural configurations to accommodate the increasing number of keypoints within the 3D volume. Besides, our assumption for identifying correspondence may fail in 3D volume with the sparse acquisition, when the neighboring slices no longer maintain semantically

Figure 3: Learned multiscale self-similarity between the query point feature (star) and all other feature points within the same feature map from the UNet encoder. The comparison between (a) and (b), (d) and (e) indicates that our method maintains better invariance of the local feature self-similarity under different transformations. Additionally, the coherence between (b) and (c), (e) and (f) verify that our method learns better equivariance. Details are elaborated in Sec. 4.

similar structures. Therefore, more robust and complicated criteria for obtaining the correspondence is required.