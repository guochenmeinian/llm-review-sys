# Iba: Towards Irreversible Backdoor Attacks in Federated Learning

Dung Thuy Nguyen\({}^{1,2}\)1, Tuan Nguyen\({}^{2,3}\), Tuan Anh Tran\({}^{4}\), Khoa D Doan\({}^{2,3}\), Kok-Seng Wong\({}^{2,3}\)

\({}^{1}\) Department of Computer Science, Vanderbilt University, Nashville, TN 37212, USA

\({}^{2}\)VinUni-Illinois Smart Health Center, VinUniversity, Hanoi, Vietnam

\({}^{3}\)College of Engineering & Computer Science, VinUniversity, Hanoi, Vietnam

\({}^{4}\)VinAI Research, Hanoi, Vietnam

dung.t.nguyen@Vanderbilt.Edu, tuan.nm@vinuni.edu.vn,

v.anhtt152@vinai.io, khoa.dd@vinuni.edu.vn, wong.ks@vinuni.edu.vn

###### Abstract

Federated learning (FL) is a distributed learning approach that enables machine learning models to be trained on decentralized data without compromising end devices' personal, potentially sensitive data. However, the distributed nature and uninvestigated data intuitively introduce new security vulnerabilities, including backdoor attacks. In this scenario, an adversary implants backdoor functionality into the global model during training, which can be activated to cause the desired misbehaviors for any input with a specific adversarial pattern. Despite having remarkable success in triggering and distorting model behavior, prior backdoor attacks in FL often hold impractical assumptions, limited imperceptibility, and durability. Specifically, the adversary needs to control a sufficiently large fraction of clients or know the data distribution of other honest clients. In many cases, the trigger inserted is often visually apparent, and the backdoor effect is quickly diluted if the adversary is removed from the training process. To address these limitations, we propose a novel backdoor attack framework in FL, the **Irreversible Backdoor Attack** (IBA), that jointly learns the optimal and visually stealthy trigger and then gradually implants the backdoor into a global model. This approach allows the adversary to execute a backdoor attack that can evade both human and machine inspections. Additionally, we enhance the efficiency and durability of the proposed attack by selectively poisoning the model's parameters that are least likely updated by the main task's learning process and constraining the poisoned model update to the vicinity of the global model. Finally, we evaluate the proposed attack framework on several benchmark datasets, including MNIST, CIFAR-10, and Tiny ImageNet, and achieved high success rates while simultaneously bypassing existing backdoor defenses and achieving a more durable backdoor effect compared to other backdoor attacks. Overall, IBA2 offers a more effective, stealthy, and durable approach to backdoor attacks in FL.

## 1 Introduction

Federated Learning (FL)  is a novel distributed learning paradigm that enables multiple parties to train machine learning models collaboratively without centralized data sharing. Specifically, each participant trains a local model on their private data, and only the model updates are exchanged with a central server for aggregation. However, FL also introduces new security and privacy challengesthat need to be addressed to ensure the confidentiality and integrity of the learning process [33; 24], such as model poisoning, membership inference, data leakage, and Sybil attacks [9; 20].

Backdoor attacks are a specific form of model poisoning in which an attacker inserts a backdoor trigger during the training process that can later be exploited to manipulate the model's behavior [4; 12; 17; 6]. FL is particularly vulnerable to backdoor attacks since each participant trains a local model on their private data, and the central server aggregates the model updates without validating their integrity. An attacker can inject poisoned data or backdoor triggers into the local models of a subset of participants. When the models are aggregated, the backdoor trigger can be activated to compromise the model's behavior [2; 28; 32].

Artificial backdoor attacks are a standard method for backdoors in FL. In an artificial backdoor attack, an apparent trigger, such as a specific pixel value, is injected into the local training data [25; 32]. Although meaningless in the data context, the apparent trigger causes the trained model to associate a signal for the model to optimize during training. When trained on such data, the model will induce misclassifications toward a target class when making predictions on inputs with the presence of the trigger. Existing artificial backdoor attacks require the involvement of a portion of compromised clients and need assistance from scaling-based methods to achieve high attack success rates [32; 24]. This makes these attacks vulnerable to mainstream backdoor defenses [30; 31; 15]. Furthermore, most existing attacks are not durable (i.e., when the adversary is removed from the FL process, the backdoor effect is gradually diluted ), especially when the size of compromised clients is small, and they do not regularly participate in the FL training.

In this paper, we exploit the principle of _adversarial attacks_ to craft a novel instance-specific backdoor attack, called **Irreversible Backdoor Attack** (IBA), that only requires a small number of compromised clients but achieves more robust attack performance and durability. IBA is a two-phase attack scheme, including _Trigger Generating_ and _Backdoor Injection_. During training rounds, the former manipulates the attack model, which is a generative model, so the triggers generated by this attack model can deceive the local model into misbehaving. After that, we formulate the _Backdoor Injection_ phase as a constrained optimization problem. Then, we propose a simple but effective recall to keep the attack model updated regularly. This algorithm allows the adversary to achieve high attack success rates even when a small number (i.e., 1) of clients are compromised. In addition, we enhance the durability and effectiveness of the proposed attack by selectively poisoning the model's parameters that are least likely updated by the main task's learning process and constraining the poisoned model update to the vicinity of the global model, respectively. This strategy helps IBA bypass mainstream defenses in FL and achieve extended backdoor durability.

Our **contributions** are summarized below:

* Proposal of a two-phase backdoor injection mechanism, IBA, in FL: The paper introduces IBA, which combines the manipulation of the trigger generator and the poisoning of the local model into a unified two-phase approach. IBA constrains the poisoned model update to be within the vicinity of the global model to improve the practicality and effectiveness of the attack.
* Proposal of selective parameter poisoning in the constrained optimization algorithm: The paper enhances the durability of IBA by selectively poisoning the model's parameters less likely to be updated during the main task's learning process.
* State-of-the-art attack performance and stealthiness: The paper demonstrates that IBA achieves state-of-the-art attack performance and durability and exhibits high stealthiness against existing defense mechanisms. This highlights the robustness and effectiveness of IBA as a practical backdoor attack method.

## 2 Related Work

**Backdoor Attacks in FL.** Backdoor attacks in FL have been extensively studied, revealing vulnerabilities and proposing various attack strategies . Backdoor attackers generally aim to incorporate concealed backdoors into Deep Neural Networks (DNNs) during the training phase. This allows the compromised DNNs to exhibit normal behavior when presented with benign samples. However, their predictions will be deliberately and consistently altered when triggered by specific patterns defined by the attacker. One of the most commonly employed methods for incorporating backdoor functionality during training is poisoning the training samples [2; 32]. Bagdasaryan et al.  also proposed model replacement to implant backdoors into the joint model and demonstrated that even a single-round attack with a small fraction of clients could achieve a 100% Attack Success Rate (ASR). Xie et al. investigated Distributed Backdoor Attacks (DBA), which decompose the global trigger into non-overlapping local triggers. DBA exhibits higher ASR and increased robustness against FL aggregation methods. Additionally, backdoor triggers in machine learning can be designed to be imperceptible in the input space [10; 12; 17; 6] and in the latent space , or to possess sophisticated, multi-target payloads ; the ground-truth labels of the poisoned samples can also align with the intended target label [26; 22; 35]. These characteristics contribute to the stealthiness of backdoor attacks. Recently, Neurotoxin  proposed to enhance backdoor durability by targeting specific coordinates less likely to be updated by benign agents, partially poisoning the model by projecting gradients onto a coordinate-wise constraint. This extends the backdoor's impact and prevents catastrophic forgetting, strengthening the effectiveness of the attack. The recent work that is most aligned with ours is PerDoor , which uses the Basic Iterative Method (BIM) to generate the trigger, which relies on gradients of the model's loss function concerning the input data to alter the original inputs into backdoored inputs. However, this method fails to learn underlying patterns and distributions from the training dataset and lacks a sharing and collaborating mechanism among multiple malicious clients, which is very beneficial for the adversary in collaborative attacks.

**Defense Mechanisms against Backdoor Attacks in FL.** The mainstream backdoor-attack defenses in FL aim to reduce the effect on the global model, as it has been shown that detecting backdoors is challenging without breaking the privacy of participants . For instance, Krum  aggregates local models by choosing a single local model as the aggregated model with the smallest Euclidean distance to a certain fraction of other models. Norm difference clipping (NDC) examines the norm-difference between the global model sent to and model updates shipped back from selected clients, using a pre-specified threshold to clip model updates that exceed the norm difference. RFA aggregates local models by computing a weighted geometric median using the smoothed Weiszfeld's algorithm. Ozdayi et al.  proposed a defense mechanism RLR, that adjusts the learning rate of the aggregation server in FL based on the sign information of agents' updates by carefully considering it per dimension and round. These defense mechanisms offer valuable insights and strategies for countering backdoor attacks in FL settings. Another approach is identifying suspicious clients and removing them from the aggregation process. FoolsGold  assumes that benign datasets from different clients differ from each other and assigns low weights to models that are similar to many other models, but it can be vulnerable to sophisticated adversaries submitting poisoned updates without raising suspicion. FLAME  uses the HDBSCAN algorithm to detect malicious updates, combines model filtering with poison elimination to detect and remove malicious updates, and is robust against inference attacks.

## 3 Irreversible Backdoor Attack (Iba) in FL

**FL Framework.** The training process in the standard FL framework involves the following steps:

_Step 0 (FL Initialization):_ The central server \(\) initializes the weight of the global model \(w\) and sets hyperparameters such as the number of FL rounds and local epochs.

_Step 1 (Local Model Training and Update):_ Selected clients \(_{1},_{2},...,_{K}\) receive the current global weight \(w_{0}\) from \(\). Each client \(_{i}\) updates its local model parameters \(w_{i}\) using its local dataset \(_{i}\). After local training, the clients send their local weights to \(\) for model aggregation.

_Step 2 (Global Model Aggregation and Update):_\(\) aggregates the received local weights and computes the aggregation result. The aggregated result is sent back to the clients for the next round of training. \(\) aggregates the received local model weights using the FedAvg  formula:

\[w=_{k}|}_{k=1}^{K}|_{k}| w_{k}\]

where \(|_{k}|\) represents the number of data samples held by client \(_{k}\).

**Attacker Threat.** To address the weakness of existing works, i.e., the adversary \(\) controls a number of compromised clients, we focus on the scenario that \(\) controls only one client \(k\) and it will participate in the training for each \(f\) rounds, i.e., fixed-frequency attack .

_Adversary Capability:_ We follow the same assumption used in the existing works on backdoor attacks in FL [2; 15], where adversary \(\) has total control over the malicious participant(s). However, \(\) has no control or access to other benign participants' data or local updates. Also, we assume \(\) thoroughly understands the central server and any potentially deployed defenses but cannot alter the server's configuration parameters and algorithms.

_Adversary Objectives:_ The goal of \(\) is learning a trigger generator \(_{}\) such that its outputted triggers can activate the backdoor function to mislead the global model \((w)\) while ensuring the behaviors on clean samples are unaffected. This trigger is expected to bypass human and machine inspections. Moreover, \(\) can bypass configured defenses in FL and prolong the backdoor effect even when \(\) is removed from the training process. The overall mechanism of our proposed backdoor attack IBA is illustrated in Figure 1.

### The Trigger Generating Function

In FL, when clients actively engage in a training round, they have the opportunity to observe and influence the changes in the global model \(w\). In particular, a client \(k\) receives the global model \(w_{t-1}\) and updates it using their local data to produce a new local model, denoted as \(w_{k}\). After local training, the learned surrogate model is aggregated to partially reflect the subsequent global model. In light of this process, we propose an initial phase called the _Trigger Generating_. In this phase, the trigger generation model \(_{}\) is updated to generate adversarial noise. This noise is combined with the original image \(x\) to create a backdoor sample. This sample remains imperceptibly different from the original, but it effectively misleads the model \(_{}\) into making incorrect classifications for the target class. The rationale behind this approach is that manipulating \(_{}\) based on this local model allows the adversary \(\) to optimally adapt the generative function, even in the face of round-by-round variations in the global model. The transformation function is inspired by adversarial examples, in which we model it as a perturbation on the input, as follows:

\[T_{}(x)=x+_{}(x),||_{}(x)||_{} , x\] (1)

Figure 1: **Irreversible Backdoor Attack Scheme.** IBA includes two training phases: (1) Trigger Generating Function and (2) Backdoor Injection. In the first phase, the trigger generation model \(_{}\) is updated to generate adversarial noise. This noise is combined with the original image \(x\) to create a backdoor sample to fool the local model \(_{i}^{}\). In the second phase, the local model \(f_{w_{k}}\) is updated to perform indifferently on clean sample \(x\) but distorts its prediction on the backdoor image \(T_{}(x)\) to the target class \(y_{T}\), then this poisoned model is sent to the aggregator. Model poisoning technique is leveraged to enhance the stealthiness and durability of the attack.

The function \(_{}\) takes an input \(x\) and generates adversarial noise on the same input space, guaranteeing the backdoor attack's stealthiness.

To achieve this goal, we utilize the local model as the surrogate model to update the attack model \(_{}\) using the following objective function:

\[-_{}_{x}_{}(f_{w_{ k}}(T_{}(x)),y_{T}),\] (2)

in which \(f_{w}\) is the trained local model and \(y_{T}\) is the targeted label. It's worth noting that the local model \(f_{w}\) submitted to the server will be totally vanilla during this phase.

While considering various norms for bounding the generator noise in Equation 1, such as the \(L_{2}\) norm, it's generally inadvisable for backdoor attacks. This infinity norm guarantees a widespread distribution of the generated trigger across the input image (encompassing all pixels in the trigger); in contrast, the \(L_{2}\) norm can result in localized artifacts within the image (with only select pixels forming the trigger). Employing the \(L_{2}\) norm can make the backdoor attack more susceptible to detection by trigger-synthesis defenses like Neural Cleanse . Thus, the \(\|.\|_{}\) norm is a preferred choice.. We can design such generator function as an auto-encoder or the more complex U-Net architecture . However, we observe no significant performance difference between a simple auto-encoder and U-Net by training the generator function and the classifier with the proposed training algorithm.

In the proposed transformation function, parameter \(\) controls the visual stealthiness of the triggers. For instance, even on the gray-scale MNIST dataset, if \(\) is less than 0.01, there is usually no discernible difference between the clean and disturbed images. A greater value for \(\), i.e., more obvious triggers, simplifies generative model learning. On the other hand, a too-small \(\) may make learning a generative model challenging since it is heavily dependent on the variance of the surrogate model, which varies across FL training rounds. Therefore, during this phase, we keep the \(\) sufficiently large, i.e., \(_{0}\), until we reach the expected attack success rate.

### The Backdoor Injection Process

At the second phase aims to gradually insert the backdoor function into the global model by manipulating the poisoned local model. Our objective is to learn classifier (i.e., local model) \(f_{w_{k}}\) that concurrently performs indifferently on \(x\) in comparison to the classifier's clean version but distorts its prediction on the backdoor image \(T_{}(x)\) to the target class \(y_{T}\). The above task can be formulated as the following objective function:

\[_{w}_{x}_{clean}(f_{w_{k}}(x),y_{x})+ _{poison}(f_{w_{k}}(T_{}(x)),y_{T})\] (3)

The classification model \(f_{w}\) in the above problem is trained based on local data and the optimal function \(_{}\) learned in the previous phase. When training the classifier, the parameters \(\) and \(\) regulate a mixture of the strengths of the loss items from clean and poisoned data, i.e., \(_{clean}\) and \(_{poison}\), respectively. If \(\) is more significant than \(\), the classifier's performance on clean data rapidly converges to the optimal performance of the vanilla classifier, according to our experiments. Otherwise, the classifier's performance on the backdoor data quickly reaches the optimal value. To compensate for the performance of the local model on both the main task and the backdoor task, we assume \(\) = 0.5 and \(\) = 0.5 in the remaining part of the paper. As previously stated, the value of \(\) must be assigned cautiously to overcome the unpredictability of the global model and assist the local model in learning the generated triggers more effectively. To achieve this goal, we propose a self-adjusting value for \(\) from the starting round of this phase, i.e., \(t_{I}\):

\[_{t}=(,_{0}*(1.0-_{})^{t-t_{I}}),\] (4)

where the value of \(\) is gradually decayed until it reaches objective \(\).

_Attack Model Retraining._ During this phase, we propose frequently updating the attack model \(_{}\) to keep it in sync with the state of the global model. The retraining frequency should be chosen (20, 50).

**Partial Model Poisoning.** To enhance the longevity and effectiveness of the attack, we propose to partially poison the model based on selected poisoning space and poisoning dimension. In which space-based poisoning helps the attack bypass the backdoor defenses because it constrains the poisoned model such that it does not go far from the vicinity of the benign model, while dimension-based poisoning helps to reduce dilution effect from benign clients by shrinking the poisoned neurons to infrequently updated neurons.

_Poisoning-space based._ Under this attack, the submitted local model is partially contaminated by being constrained by the magnitude of its parameters. Specifically, the adversary applies projected gradient descent on the losses for \(^{}\), where \(,^{}\) are benign and poisoned datasets, respectively. In other words, it periodically projects the model parameter on the ball centered around the global model of the previous iteration. This enables IBA to circumvent defenses based on norm and distance investigation by preventing the resulting poisoned model from deviating substantially from the original. Mathematically, this method guarantees that \(||w_{k}-w||\). Note that, this attack strategy can be combined with model replacement , where the model parameter is scaled before being sent to the server to cancel the contributions from the other honest clients . The scaled model is calculated by the following:

\[_{k} w_{k}+_{i k}}{n_{C  k}}(w_{k}-w^{*}).\] (5)

_Poisoning-dimension based._ Because the attacker cannot participate in every round of training, directly submitting the poisoned model to the server is not effective . To address it, we propose leveraging historical gradient variation to select infrequently updated neurons, which are considered poisoning dimensions for IBA. Then the adversary computes bottom\(-k\%\) coordinates of the gradient vector \(g\), obtained by learning the main task on the clean data \(\). In particular,

\[_{t}*+*_{t} s.t., _{t}=bottom_{k}(g_{t}),\] (6)

in which \(p\) refers to the number of rounds up to round \(t\) that adversary \(\) has been selected to participating in FL training. After that, when finishing calculating gradient \(\) of local model \(f_{w}\) using Equation. 3, \(\) projects gradient onto coordinate-wise constraint on the top\(-k\%\) coordinates of \(\). As a result, only bottom\(-k\%\) coordinates are poisoned, which can prolong the backdoor effect since other benign clients rarely update these coordinates. Utilizing historical information aids in determining round-by-round bottom\(-k\%\) coordinates, and it becomes more valuable as rounds advance and local client models converge on the global model.

## 4 Experiments

Our empirical study aims to highlight the effectiveness of the IBA attack against the state-of-the-art (SOTA) FL defenses. We conduct our experiments on real-world datasets and a simulated FL environment. We design three experiment sets: (1) IBA under different attack scenarios, (2) IBA under different FL defenses, and (3) IBA's imperceptibility and durability evaluation. Our results demonstrate that the IBA can achieve significant efficiency, stealthiness, and durability. In particular, IBA attains persistent performance with/without SOTA defenses and maintains higher visual imperceptibility and longer durability compared with existing artificial backdoor attacks [32; 2]. Our implementation is publicly available to reproduce experimental results.

### Experimental Setup

**Dataset and Backdoor Tasks.** The IBA method is evaluated on three classification datasets: MNIST, CIFAR-10, and Tiny ImageNet. We simulate heterogeneous data partitioning by sampling \(p_{k} Dir_{K}(0.5)/Dir_{K}(0.01)\) for MNIST, CIFAR-10/Tiny ImageNet and allocating a proportion of each class to participating clients. We follow the established protocol outlined in previous works [28; 32], employing stochastic gradient descent (SGD) optimization with \(E\) local epochs, a local learning rate of \(l_{r}\), and a batch size of 32. The number of clients selected in each round is \(10/200\). The details of the datasets and other parameter configurations are presented in the supplementary material3.

**Trigger Generation and Backdoored Model Training.** The generation of triggers is accomplished through the utilization of the U-Net architecture , incorporating specific parameter configurations.

The constraint values are set to \(=0.3,=0.05\); while \(=0.5\) and \(=0.5\) define additional parameter values. During trigger training, the threshold for backdoor accuracy (BA) is established as \(local_{BA}=0.85\). To facilitate the generation of updates, the learning rate for the update generation model is determined as \(_{}=0.0001\), and for the update classifier model, the learning rate is set to \(=0.01\). Additionally, the performance of IBA is evaluated in conjunction with prominent FL strategies, namely FedAvg , FedProx , and FedNova . This comprehensive evaluation aims to assess the effectiveness of IBA in the context of different FL methodologies.

**Participating patterns of attackers.** In addition to the _fixed-frequency_ case described in the threat model, we further evaluate the efficiency of IBA under a more favorable condition known as _fixed-pool_ case (or random sampling), where there is a fixed pool of clients are controlled by \(\). The result for the fixed-pool case is left in the appendix.

**FL Backdoor Defenses.** We examine six SOTA defense techniques designed to mitigate backdoor attacks in FL: (i) NDC , (ii) KRUM and (iii) Multi-KRUM , (iv) RFA , (v) RLR , (vi) FoolsGold . By evaluating the performance of these defense techniques, we aim to provide insights into their effectiveness in countering backdoor attacks in the FL setting.

### Experimental Results

**(1)** IBA under different attack scenarios.** Firstly, we study the performance of IBA under fixed-frequency attacks with three datasets: MNIST, CIFAR-10, and Tiny ImageNet. The evaluation of main accuracy (MA) and backdoor accuracy (BA) from Table 1 reveals interesting insights. In the absence of the attack, the MAs of the baseline are 99.74%, 84.71%, and 65.15% for MNIST, CIFAR-10, and Tiny ImageNet, respectively, indicating accurate classification on clean samples, while the BA remains low at 0.56% to 9%, indicating no backdoor effect on global model. Under IBA attack with FedAvg aggregator, the corresponding MAs are negligibly affected, while the BAs are notably high, i.e., 99.07% for MNIST dataset. This indicates that IBA can stealthily insert a backdoor function without negatively impacting the model's behavior with benign samples.

In FL, many existing backdoor attacks target vulnerabilities within the FedAvg aggregator since the contribution of each client is considered equally. Therefore, we examined IBA's resilience across different aggregation methods, specifically FedAvg, FedProx, and FedNova, as presented in Table 1. FedProx, with its proximal term, enhances model accuracy and is beneficial in scenarios with non-IID data distributions. Besides, FedNova, designed to address objective inconsistency in FL, ensures consistent and accurate model updates even with varying update frequencies from clients, making it suitable for scenarios with heterogeneous data. The results demonstrate the consistent efficacy of IBA with three datasets and three FL aggregators. In particular, IBA persistently obtained significant BAs, with the lowest BA being 87.13% and the highest being 99.77%, demonstrating its efficacy and potential challenges in diverse FL settings.

**Fixed-pool attacks.** We study the performance of IBA under fixed-pool attack setting (percentage of attackers in the overall clients varies from 5% to 20%). The results are shown in Figure 2, demonstrating that a higher compromising ratio leads to a better backdoor accuracy or more substantial backdoor effect.

**(2)** IBA under different FL defenses.** We study the effectiveness of stand-alone IBA and hybrid attacks (i.e., IBA and partial model poisoning) against the aforementioned defense techniques. As demonstrated in

    &  &  &  &  \\   & MA & BA & MA & BA & MA & BA & MA & BA \\  CIFAR-10 & 84.71\% & 9.52\% & 84.49\% & 87.13\% & 84.62\% & 86.59\% & 83.35\% & 94.32\% \\ MNIST & 99.74\% & 9.25\% & 98.11\% & 99.07\% & 97.98\% & 99.97\% & 98.32\% & 92.74\% \\ Tiny ImageNet & 65.15\% & 0.56\% & 64.36\% & 94.19\% & 65.02\% & 92.52\% & 64.17\% & 93.11\% \\   

Table 1: Effectiveness of IBA under different strategies in fixed-frequency attacks setting (\(f=10\))

Figure 2: IBA under fixed-pool attacks with different compromising ratios.

Figure 3, the stand-alone IBA can circumvent NCA, Multi-Krum, and FoolsGold and attain a high backdoor accuracy (i.e., 95.11%, 79.5% and 76.46% for CIFAR-10 dataset, respectively). Krum is the most challenging defense since it replaces the global model with only the most representative local model (i.e., the one with the shortest distance to its neighbors) in each round. As a result, the global model experiences substantial fluctuation over rounds and does not fully capture the knowledge of all participants, i.e., a significant decline in primary task accuracy in Figure 3. This mechanism hampers the performance of the model with respect to the global model.

    &  &  &  &  \\   & & MA & BA & MA & BA & MA & BA \\   & RFA & 81.97\% & 98.03\% & 80.71\% & 96.7\% & 81.25\% & 98.17\% \\  & NCA & 84.64\% & 95.11\% & 84.77\% & 98.73\% & 84.21\% & **99.12\%** \\  & Krum & 81.65\% & 27.07\% & 81.59\% & 50.33\% & 81.44\% & 65.98\% \\  & Multi-krum & 82.40\% & 79.5\% & 83.78\% & **97.16\%** & 83.25\% & 95.02\% \\  & RLR & 83.91\% & 63.94\% & 82.59\% & 65.14\% & 82.85\% & 47.07\% \\  & FoolsGold & **84.90\%** & 76.46\% & **85.02\%** & 76.78\% & 82.90\% & 84.36\% \\  & No-defense & 84.28\% & 85.63\% & 84.35\% & 83.80\% & **83.81\%** & 97.79\% \\   & RFA & 98.04\% & **100.0\%** & 98.89\% & 99.64\% & 98.04\% & **100.0\%** \\  & NCA & 98.66\% & **100.0\%** & 98.97\% & **99.69\%** & 98.62\% & 100.0\% \\   & Krum & 97.60\% & 14.13\% & 97.82\% & 87.48\% & 97.44\% & 17.29\% \\   & Multi-krum & 97.98\% & 81.27\% & 98.97\% & 98.82\% & 99.01\% & 80.18\% \\   & RLR & 90.91\% & 12.60\% & 87.48\% & 18.74\% & 90.03\% & 29.97\% \\   & FoolsGold & 98.87\% & 80.08\% & **99.10\%** & 96.43\% & **99.05\%** & 99.83\% \\   & No-defense & **99.98\%** & 98.03\% & 98.81\% & 99.73\% & 98.84\% & 99.97\% \\   

Table 2: Robustness of IBA and the combination of IBA and partial model poisoning under mainstream backdoor defenses in FL

Figure 4: The improved stealthiness of IBA when combined with partial model poisoning attacks under various defenses for the CIFAR-10 dataset. The attack is conducted from round 0 with fixed-frequency attacks of 10.

Figure 3: Effectiveness of stand-alone IBA against mainstream defenses for FL on CIFAR-10 dataset.

[MISSING_PAGE_FAIL:9]

backdoored images have negligible deviations from the Grad-CAM behaviors on the clean images for both black-and-white and colored images. On the other hand, backdoored images using patched triggers generate a considerable difference in visualization heat maps (DBA ). Because IBA is based on input perturbation, the image is formed by adding very little perturbation to the clean image; consequently, the difference in the latent space where the heat maps are generated is likewise minimal. Aside from that, the patch backdoor (such as the pattern in the upper-left corner) is apparent, resulting in a significant distinction between the backdoor and clean versions. We also evaluate the backdoor generated by IBA and another attack scheme in FL, i.e., DBA with **Neural Cleanse**, a widely-used backdoor model mitigation method based on the pattern optimization approach. Neural Cleanse assumes that the backdoor trigger is patch-based, which makes it suitable for evaluating the proposed method. For each image label, Neural Cleanse identifies if there is a patch pattern that produces a misclassification result for that target label. If any class label yields a significantly smaller pattern, Neural Cleanse considers it a sign of a potential backdoor. Neural Cleanse quantifies such deviations from the optimal patch of each class label by using the Anomaly Index metric. If the Anomaly Index is less than a threshold of 2 for a class, Neural Cleanse considers that there is a backdoor with this class as the target label. Since IBA does not generate the patch-based trigger pattern, it can efficiently bypass this defense by having _Anomaly Index_ much lower than DBA (Figure 7), which indicates the superior stealthiness of backdoor attacks with adversarial triggers compared to the patch-based.

As previously stated, strong attacks should be successful even if the attacker only participates a few times. As a result, we construct a scenario to assess the endurance of IBA compared to a chosen baseline: DBA. We employ the same configuration as the original work DBA but limit each of the four malicious clients' participation frequency to 10 without utilizing any scaling strategy. Regarding IBA, we apply the dimension-based model poisoning mentioned above and use one malicious client with the same frequency. As observed in Figure. 5(a), after \(\) is removed from the training (i.e., at round 200), the BA of DBA reduces gradually while the accuracy drop of IBA is almost negligible. As demonstrated in Figure 5(b) and Table 3, IBA delivers superior backdoor efficiency and durability within the same attacking duration. After round 400, the BA of DBA drops quickly compared to IBA. We can see the most significant difference towards the end of the comparison duration, i.e., at round 700, the BAs of IBA and DBA are \( 85\%\) and \( 45\%\), respectively. After 250 rounds of elimination, the BA of DBA retains 38.76% of its peak value, but the comparable value of IBA is 99.14%. Furthermore, while the BA can preserve more than 60% of its peak value with the CIFAR-10 dataset, the highest accuracy it can attain is significantly lower than IBA, i.e., less than 20%, which is consistent with the prior conclusion of the original when the scaling factor is 1. In conclusion, IBA can achieve better durability and imperceptibility than SOTA attacks.

## 5 Conclusion

In conclusion, we have presented the IBA, a novel backdoor attack framework for FL that overcomes the limitations of existing approaches. Our framework combines a learnable trigger-generating function with a gradual implantation process, resulting in an attack that can evade detection and achieve high success rates. The selective poisoning of model parameters and the constraint on poisoned updates enhance the attack's efficiency and durability. Our work demonstrates promising results and suggests several directions for future research. Robust defense mechanisms against FL backdoor attacks must be developed, considering the unique challenges of the distributed and decentralized learning process. Generalizing our approach to diverse FL settings, such as heterogeneous or non-i.i.d. data distributions, would provide a comprehensive understanding of the attack's effectiveness. Exploring alternative triggers and their impact on stealthiness and success rates could yield valuable insights. Evaluating the framework on larger-scale FL systems and real-world scenarios will assess its scalability and practicality. Advancing adversarial defense strategies, including proactive detection and mitigation techniques, will contribute to more secure and trustworthy FL systems. These future research directions will contribute to strengthening the security of FL against backdoor attacks.