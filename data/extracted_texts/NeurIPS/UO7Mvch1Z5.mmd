# Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image

**Kailu Wu**

Tsinghua University

**Fangfu Liu**

Tsinghua University

**Zhihan Cai**

Tsinghua University

**Runjie Yan**

Tsinghua University

**Hanyang Wang**

Tsinghua University

**Yating Hu**

AVAR Inc.

**Yueqi Duan\({}^{}\)**

Tsinghua University

**Kaisheng Ma\({}^{}\)**

Tsinghua University

**Kaisheng Ma\({}^{}\)**

Tsinghua University

**Gallery of Unique3D. High-fidelity and diverse textured mesh generated by Unique3D from single-view wild images within 30 seconds.**

**Abstract**

In this work, we introduce **Unique3D**, a novel image-to-3D framework for efficiently generating high-quality 3D meshes from single-view images, featuring state-of-the-art generation fidelity and strong generalizability. Previous methods based on Score Distillation Sampling (SDS) can produce diversified 3D results by distilling 3D knowledge from large 2D diffusion model, but they usually suffer from long per-case optimization time with inconsistent issues. Recent works address the problem and generate better 3D results either by finetuning a multi-view diffusion model or training a fast feed-forward model. However, they still lack intricate textures and complex geometries due to inconsis

Figure 1: **Gallery of Unique3D. High-fidelity and diverse textured mesh generated by Unique3D from single-view wild images within 30 seconds.**ated resolution. To simultaneously achieve high fidelity, consistency, and efficiency in single image-to-3D, we propose a novel framework Unique3D that includes a multi-view diffusion model with a corresponding normal diffusion model to generate multi-view images with their normal maps, a multi-level upscale process to progressively improve the resolution of generated orthographic multi-views, as well as an instant and consistent mesh reconstruction algorithm called _ISOMER_, which fully integrates the color and geometric priors into mesh results. Extensive experiments demonstrate that our Unique3D significantly outperforms other image-to-3D baselines in terms of geometric and textural details. Project page: https://wukailu.github.io/Unique3D/.

## 1 Introduction

Automatically generating diverse and high-quality 3D content from single-view images is a fundamental task in 3D Computer Vision [15; 13; 29; 60; 63], which can facilitate a wide range of versatile applications [25; 49], including gaming, architecture, art, and animation. However, this task is challenging and ill-posed due to the underlying ambiguity of 3D geometry in a single view.

Recently, the rapid development of diffusion models [12; 52; 39] has opened up new perspectives for 3D content creation. Powered by the strong prior of 2D image diffusion models, DreamFusion  proposes Score Distillation Sampling (SDS) to address the limitation of 3D data by distilling 3D knowledge from 2D diffusions , inspiring the progress of SDS-based 2D lifting methods [20; 37; 59; 24; 5]. Despite their diversified compelling results, they usually suffer from long per-case optimization time for hours, poor geometry, and inconsistent issues (_e.g._,, Janus problem ), thus not practical for real-world applications. To overcome the problems, a series of works leverage larger-scale open-world 3D datasets [7; 4; 6] either to fine-tune a multi-view diffusion model [29; 28; 57] and recover the 3D shapes from the generated multi-view images or train a large reconstruction model (LRM) [13; 64; 60; 63] by directly mapping image tokens into 3D representations (_e.g._,, triplane or 3D Gaussian ). However, due to local inconsistency in mesh optimization [29; 65] and limited resolution of the generative process with expensive computational overhead [13; 63], they struggle to produce intricate textures and complex geometric details with high resolution.

In this paper, we present a novel image-to-3D framework for efficient 3D mesh generation, coined **Unique3D**, to address the above challenges and simultaneously achieve high-fidelity, consistency, and generalizability. Given an input image, Unique3D first generates orthographic multi-view images from a multi-view diffusion model. Then we introduce a multi-level upscale strategy to progressively improve the resolution of generated multi-view images with their corresponding normal maps from a normal diffusion model. Finally, we propose an instant and consistent mesh reconstruction (_ISOMER_) algorithm to reconstruct high-quality 3D meshes from the multiple RGB images and normal maps, which fully integrates the color and geometric priors into mesh results. Both diffusion models are trained on a filtered version of the Obojavce dataset  with \( 50k\) 3D data. To enhance the quality and robustness, we design a series of strategies into our framework, including the noise offset channel in the multi-view diffusion training process to correct the discrepancy between training and inference , a stricter dataset filtering policy, and an expansion regularization to avoid normal collapse in mesh reconstruction. Overall, our method can generate high-fidelity, diverse, and multi-view consistent meshes from single-view wild images within 30 seconds, as shown in Figure 1.

We conduct extensive experiments on various wild 2D images with different styles. The experiments verify the efficacy of our framework and show that our Unique3D outperforms existing methods for high fidelity, geometric details, high resolution, and strong generalizability.

In summary, our contributions are:

* We propose a novel image-to-3D framework called Unique3D that holistically archives a leading level of high-fidelity, efficiency, and generalizability among current methods.
* We introduce a multi-level upscale strategy to progressively generate higher-resolution RGB images with the corresponding normal maps.
* We design a novel instant and consistent mesh reconstruction algorithm (_ISOMER_) to reconstruct 3D meshes with intricate geometric details and texture from RGB images and normal maps.

* Extensive experiments on image-to-3D tasks demonstrate the efficacy and generation fidelity of our method, unlocking new possibilities for real-world deployment in the field of 3D generative AI.

## 2 Related Work

**Mesh Reconstruction**. Despite the significant advancements in various 3D representations (_e.g.,_, SDF [61; 34], NeRF [31; 32], 3D Gaussian ), meshes remain the most widely used 3D format in popular 3D engines (_e.g.,_, Blender, Maya) with a mature rendering pipeline. Reconstructing high-quality 3D meshes efficiently from multi-view or single-view images is a daunting task in graphics and 3D computer vision. Early approaches usually adopt a laborious and complex photogrammetry pipeline with multiple stages, with techniques like Structure from motion (SfM) [1; 42; 51], Multi-View Stereo (MVS) [9; 43], and mesh surface extraction [35; 30]. Powered by deep learning and powerful GPUs, recent works [44; 45; 14; 67; 13; 60; 64] have been proposed to pursue higher efficiency and quality with gradient-based mesh optimization or even training a large feed-forward reconstruction network. However, their pipeline still suffers from heavy computational costs and struggles to adapt to complex geometry. To balance efficiency and quality, we propose a novel instant and high-quality mesh reconstruction algorithm in this paper that can reconstruct complex 3D meshes with intricate geometric details from sparse views.

**Score Distillation for 3D Generation**. Recently, data-driven large-scale 2D diffusion models have achieved notable success in image and video generation [39; 41; 66; 50]. However, transferring it to 3D generation is non-trivial due to curating large-scale 3D datasets. Pioneering works DreamFusion  proposes Score Distillation Sampling (SDS) (also known as Score Jacobian Chaining ) to distill 3D geometry and appearance from pretrained 2D diffusion models when rendered from different viewpoints. The following works continue to enhance various aspects such as fidelity, prompt alignment, consistency, and further applications [20; 37; 59; 5; 19; 54; 69]. However, such optimization-based 2D lifting methods are limited by long per-case optimization time and multi-face problem  due to lack of explicit 3D prior. As Zero123  proves that Stable Diffusion  can be finetuned to generate novel views by conditioning on relative camera poses, one-2-3-45  directly produce plausible 3D shapes from generated images in Zero123. Though it achieves high efficiency, the generated results show poor quality with a lack of texture details and 3D consistency.

**Multi-view Diffusion Models for 3D Generation**. To achieve efficient and 3D consistent results, some works [29; 28; 47; 57; 48] fine-tune the 2D diffusion models with large-scale 3D data  to generate multi-view consistent images and then create 3D contents using sparse view reconstruction. For example, SyncDreamer  leverages attention layers to produce consistent multi-view color images and then use NeuS  for reconstruction. Wonder3D  explicitly encodes the geometric information into 3D results and improves quality by cross-domain diffusion. Although these methods generate reasonable results, they are still limited by local inconsistency from multi-views generated by out-domain input images and limited generated resolution from the architecture design, producing coarse results without high-resolution textures and geometries. In contrast, our method can generate higher-quality textured 3D meshes with more complex geometric details within just 30 seconds.

## 3 Method

In this section, we introduce our framework, _i.e.,_ Unique3D, for high-fidelity, efficient, and generalizable 3D mesh generation from a single in-the-wild image. Given an input image, we first generate four orthographic multi-view images with their corresponding normal maps from a multi-view diffusion model and a normal diffusion model. Then, we lift them to high-resolution space progressively, (Sec 3.1). Given high-resolution multi-view RGB images and normal maps, we finally reconstruct high-quality 3D meshes with our instant and consistent mesh reconstruction algorithm _ISOMER_, (Sec 3.2). ISOMER directly handles the case where the global normal of the same vertex is inconsistent across viewpoints to enhance the consistency. An overview of our framework is depicted in Figure 2.

### High-resolution Multi-view Generation

We first explain the design of our high-resolution multi-view generation model that generates four orthographic view images from a single input image. Instead of directly training a high-resolution (2K)multi-view diffusion that would consume excessive computational resources, we adopt a multi-level generation strategy to upscale the generated resolution progressively.

**High-resolution Multi-view Image Generation**. Instead of training from scratch, we start with the initialization of the pre-trained 2D diffusion model using the checkpoint of Stable Diffusion  and encode multi-view dependencies to fine-tune it to obtain a multi-view diffusion model that is able to generate four orthographic view images (256 resolution) from a single in-the-wild image. It is worth noting that the images generated in this step have relatively low resolution and suffer from multi-view inconsistency in out-of-the-domain data. This significantly limits the quality of recent works . In contrast, we address the multi-view consistency issue during the reconstruction phase (Sec 3.2). Given the generated four orthographic view images, we then finetune a multi-view aware ControlNet  to improve the resolution of images. This model leverages the four collocated RGB images as control information to generate corresponding clearer and more precise multi-view results. It enhances the details and ameliorates unclear regions, leading the resolution of images from 256 to 512. Finally, we employ a single-view super-resolution model  to further upscale the image by a factor of four, achieving a resolution of 2048 that offers sharper edges and details without disrupting the multi-view consistency.

**High-resolution Normal Map Prediction**. Using pure RGB images makes it extremely hard to reconstruct correct geometry. To effectively capture the rich surface details of the target 3D shape, we finetune a normal diffusion model to predict normal maps corresponding to multi-view color images. Similar to the above high-resolution image generation stage, we also employ the super-resolution model  to quadruple the normal resolution, which enables our method to recover high-fidelity geometric details, especially the accuracy of the edges.

To enhance the capability of the image generation model and the standard normal prediction model in producing high-quality images with uniform backgrounds, we adopt a channel-wise noise offset strategy . This can alleviate the problem caused by the discrepancy between the initial Gaussian noise during sampling and the noisiest training sample.

### ISOMER: An Efficient Method for Direct Mesh Reconstruction

Despite impressive results generated by recent popular image-to-3D methods  that follow the field-based reconstruction , they have limited potential for higher-resolution

Figure 2: **Pipeline of our Unique3D**. Given a single in-the-wild image as input, we first generate four orthographic multi-view images from a multi-view diffusion model. Then, we progressively improve the resolution of generated multi-views through a multi-level upscale process. Given generated color images, we train a normal diffusion model to generate normal maps corresponding to multi-view images and utilize a similar strategy to lift it to high-resolution space. Finally, we reconstruct high-quality 3D meshes from high-resolution color images and normal maps with our instant and consistent mesh reconstruction algorithm _ISOMER_.

applications as their computational load is proportional to the cube of the spatial resolution. _In contrast, we design a novel reconstruction algorithm directly based on mesh, where the computational load scales with only the square of the spatial resolution and relates to the number of faces, thus achieving a fundamental improvement._ This enables our model to efficiently reconstruct meshes with tens of millions of faces within seconds.

We now move to introduce our instant and consistent mesh reconstruction algorithm (_ISOMER_), which is a robust, accurate, and efficient approach for direct mesh reconstruction from high-resolution multi-view images. Specifically, the _ISOMER_ consists of three main steps: (a) estimating the rough topological structure of the 3D object and generating an initial mesh directly; (b) employing a coarse-to-fine strategy to further approximate the target shape; (c) explicitly addressing inconsistency across multiple views to reconstruct high-fidelity and intricate details. Notably, the entire mesh reconstruction process takes no more than 10 seconds.

**Initial Mesh Estimation**. Unlike popular reconstruction methods based on signed distance fields  or occupancy fields , mesh-based reconstruction methods  struggle with changing topological connectivity during optimization, which requires correct topological construction during initialization. Although initial mesh estimation can be obtained by existing methods like DMTet , they cannot accurately reconstruct precise details (_e.g.,_, small holes or gaps). To address the problem, we utilize front and back views to directly estimate the initial mesh, which is fast for accurate recovery of all topologically connected components visible from the front. Specifically, we integrate the normal map from the frontal view to obtain a depth map by

\[d(i,j)=_{t}=0^{i}n_{x}(t,j)\] (1)

where \(n_{x}(t,j)\) is the normal vector of the \(t\)-th pixel in the \(j\)-th row. Although the diffusion process generates pseudo normal maps, these maps do not yield a real normal field which is irrotational. To address this, we introduce a random rotation to the normal map before integration. The process is repeated several times, and the mean value of these integrations is then utilized to calculate the depth, providing a reliable estimation. Subsequently, we map each pixel to its respective spatial location using the estimated depth, creating mesh models from both the front and back views of the object. The two models are seamlessly joined through Poisson reconstruction, which guarantees a smooth connection between them. Finally, we simplify them into \(2000\) fewer faces for our mesh initialization.

**Coarse-to-Fine Mesh Optimization**. Building upon the research in inverse rendering , we iteratively optimize the mesh model to minimize a loss function. During each optimization step, the mesh undergoes differentiable rendering to compute the loss and gradients, followed by vertex movement according to the gradients. Finally, the mesh is corrected after iteration through edge collapse, edge split, and edge flip to maintain a uniform face distribution and reasonable edge lengths. After several hundred coarse-to-fine iterations, the model converges to a rough approximation of the target object's shape. The loss function for this part includes a mask-based loss

\[_{mask}=_{i}\|_{i}-M_{i}^{pred}\|_{2}^{2},\] (2)

where \(_{i}\) is the rendered mask under view \(i\) and \(M_{i}^{pred}\) is the predicted mask from previous subsection under view \(i\). The mask-based loss regulates the mesh contour. Additionally, it includes a normal-based loss

\[_{normal}=_{i}M_{i}^{pred}\|_{i}-N_{i}^{pred }\|_{2}^{2},\] (3)

concerning the rendered normal map \(_{i}\) of the object and the predicted normal map \(N_{i}^{pred}\), optimizing the normal direction in the visible areas, where \(\) denotes element-wise production. We compute the final loss function as:

\[_{recon}=_{mask}+_{normal}.\] (4)

To address potential surface collapse issues under limited-view normal supervision as shown in Figure 5-(b), we employ a regularization method called Expansion. At each step, vertices are moved a small distance in the direction of their normals, akin to weight decay.

**ExplicitTarget Optimization for Multi-view Inconsistency and Geometric Refinement**. Due to inherent inconsistencies in generated multi-view images from out-of-distribution (OOD) in-the-wildinput, no solution can perfectly align with every viewpoint. After the above steps, we can only reconstruct a model that roughly matches the shape but lacks detail, falling short of our pursuit of high-quality mesh. Therefore, we cannot use the common method that minimizes differences in all views, which would lead to significant wave-pattern flaws, as shown in Figure 5-(a). To overcome this challenge, finding a more suitable optimization target becomes crucial. Under single-view supervision, although a complete model cannot be reconstructed, the mesh shape within the visible area of that view can meet the supervision requirements with highly detailed structures. Based on this, we propose a novel method that assigns a unique optimization target for each vertex to guide the optimization direction. In contrast to the conventional implicit use of multi-view images as optimization targets, we **explicitly** define the optimization target with better robustness. We call this explicit optimization target as _ExplicitTarget_ and devise it as follows:

(_ExplicitTarget_). Let \(Avg(V,W)=V_{i}W_{i}}{W_{i}}\) represent the weighted average function, and \(V_{M}(v,i):(^{+},^{+})\{0,1\}\) represent the visibility of vertex \(v\) in mesh \(M\) under view \(i\). \(Col_{M}(v,i)\) Indicate the color of vertex v in viewpoint i. We compute the ExplicitTarget \(ET\) of each vertex in mesh \(M\) as

\[ET_{M}(v)=Avg(Col_{M}(v,i),V_{M}(v,i)W_{M}(v,i)^{2})&_{i}V_{M}(v,)>0\\ &\] (5)

where\(W_{M}(v,i)=-(N_{v}^{(M)},N_{i}^{(view)})\) is a weighting factor that\(N_{v}^{(M)}\) is the vertex normal of \(v\) in mesh \(M\), and \(N_{i}^{(view)}\) is the view direction of view \(i\).

In the function \(ET_{M}(,_{m})\), the predicted color of vertex \(v\) is computed as the weighted sum of supervised views, with weights determined by the square of cosine angles. This is because the projected area is directly proportional to the cosine value, and the prediction accuracy is also positively correlated with the cosine value. The object loss function for ExplicitTarget is defined as

\[_{ET}=_{i}M_{i}^{pred}\|_{i}-N_{i}^{ET} \|_{2}^{2},\] (6)

where \(N_{i}^{ET}\) is the rendering result of mesh \(M\) with \(\{ET_{M}(,N^{pred},v)|v M\}\) under the \(i\)-th viewpoint. The final optimization loss function is

\[_{refine}=_{mask}+_{ET}.\] (7)

Towards this end, we finish the introduction of the _ISOMER_ reconstruction process, which includes three stages: Initialization, Reconstruction, and Refinement.

Upon generating precise geometric structures, it is necessary to colorize them based on multi-view images. Given the inconsistencies across multi-view images, the colorizing process adopts the same method used in the refinement stage. Specifically, the colors of mesh \(M\) is \(\{ET_{M}(,_{rgb}^{pred},v)|v M\}\). Moreover, certain regions of the model may remain unobservable from the multi-view perspective, necessitating the coloring of these invisible areas. To address this, we utilize an efficient smoothing coloring algorithm to complete the task. More detailed and specific algorithmic procedures can be found in the Appendix.

## 4 Experiments

### Experimental Setting

**Dataset:** Utilizing a subset of the Objaverse dataset as delineated by LGM , we apply a rigorous filtration process to exclude scenes containing multiple objects, low-resolution imagery, and unidirectional faces, leading to a refined dataset of approximately \(50k\) objects. To address surfaces without thickness, we render eight orthographic projections around each object horizontally. By examining the epipolar lines corresponding to each horizontal ray, we identify \(13k\) instances of illegitimate data. For rendering, we employ random environment maps and lighting to augment the dataset, thereby enhancing the model's robustness. To ensure high-quality generation, all images are rendered at a resolution of \(2048 2048\) pixels.

**Network Architecture:** The initial level of image generation is initialized with the weight of the Stable Diffusion Image Variations Model , while the subsequent level employs an upscaled

**Reconstruction Details:** The preliminary mesh structure is inferred from a normal map with a resolution of \(256 256\), which is then simplified to a mesh comprising \(2,000\) faces. The reconstruction process involves \(300\) iterations using the SGD optimizer , with a learning rate of \(0.3\). The weight of expansion regularization is set to \(0.1\). Subsequent refinement takes \(100\) iterations, maintaining the same optimization parameters.

**Training Details:** The entire training takes around 4 days on 8 NVIDIA RTX4090 GPUs. The primary level of multiview image generation uses \(30k\) training iterations with a batch size of \(1,024\). The training of multi-view image upscaling involves \(10k\) iterations with a batch size of \(128\). Normal

Figure 3: **Qualitative Comparison**. Our approach provides superior geometry and texture.

map prediction is trained for \(10k\) iterations at a batch size of \(128\). Additional training specifics are accessible in the Appendix.

### Comparisons

**Qualitative Comparison:** To highlight the advantages of our methodology, we perform a comprehensive comparison with existing works, including CRM , one-2-3-45 , Wonder3D , OpenLRM , and InstantMesh . For a fair quality comparison, we choose to present samples previously selected in the referenced papers, originating from Wonder3D , SyncDreamer , CRM , and InstantMesh . The results are shown in Figure 3. Our results clearly surpass the existing works in both geometric and material quality, thereby emphasizing the benefits of our approach in achieving high resolution and intricate details in both geometry and material. In addition to the above overall quality comparison, we further show the comparison of the details in Figure 4, highlighting the advantage of our method in high resolution. The reconstruction process of ISOMER is completed in under \(10\) seconds, while the entire procedure from the input image to high-precision mesh is accomplished in less than \(30\) seconds on an RTX4090.

  
**Method** & **PSNR\(\)** & **SSIM\(\)** & **LPIPS\(\)** & **Clip-Sim\(\)** & **CD\(\)** & **Vol. IoU\(\)** & **F-Score\(\)** \\  One-2-3-45 & 16.1058 & 0.8874 & 0.1812 & 0.7782 & 0.0313 & 0.4142 & 0.5518 \\ OpenLRM & 18.0433 & 0.8957 & 0.1560 & 0.8416 & 0.0336 & 0.3947 & 0.5354 \\ Wonder3D & 18.0932 & 0.8995 & 0.1536 & 0.8535 & 0.0261 & 0.4663 & 0.6016 \\ InstantMesh & 18.8262 & 0.9111 & 0.1283 & **0.8795** & 0.0161 & 0.5083 & 0.6491 \\ CRM & 18.4407 & 0.9088 & 0.1366 & 0.8639 & **0.0141** & 0.5218 & 0.6574 \\  Unique3D & **20.0611** & **0.9222** & **0.1070** & 0.8787 & 0.0143 & **0.5416** & **0.6696** \\  Unique3D w/o ET & 20.0383 & 0.9199 & 0.1129 & 0.8675 & 0.0158 & 0.5320 & 0.6594 \\  Wonder3D+ISOMER & 18.6131 & 0.9026 & 0.1470 & 0.8621 & 0.0244 & 0.4743 & 0.6088 \\   

Table 1: Quantitative comparison results for mesh visual and geometry quality. We report the metrics of PSNR, SSIM, LPIPS and Clip-Similarity , ChamferDistance (CD), Volume IoU and F-score on GSO  dataset.

Figure 4: **Detailed Comparison**. We compare our model with InstantMesh , CRM  and OpenLRM . Our models generates accurate geometry and detailed texture.

[MISSING_PAGE_FAIL:9]

Additionally, we added a new test with randomly rotated objects sampled from \(azimuth U[-180,180],elevation U[-30,30]\) in Table 2 to test robustness in non-front-facing views. The test results show that Unique3D still performs well in this case, and even the geometry prediction is more accurate.

We expand our study to include a qualitative comparison across various resolutions in order to demonstrate the differences between different resolutions in Figure 7. The results demonstrate the necessity of high resolution maps in generating high resolution meshes.

**Challenging Examples:** The majority of existing common samples are overly simplistic to effectively demonstrate the advantages of our study. Consequently, we select two complex samples: an object featuring detailed text and a photograph of a human, as shown in Figure 8. It is observable that our method exhibits exceptional mesh materials and geometry, even capable of sculpting geometric structures with textual detail. In the context of photographs, our reconstruction results are nearly on par with specialized image-to-character mesh generation methods.

## 5 Conclusion

In this paper, we introduce Unique3D, a pioneering image-to-3D framework that efficiently generates high-quality 3D meshes from single-view images with unprecedented fidelity and consistency. By integrating advanced diffusion models and the powerful reconstruction method ISOMER, Unique3D generates detailed and textured meshes within \(30\) seconds, significantly advancing the state-of-the-art in 3D content creation from single images.

**Limitation and Future Works.** Our method, while capable of generating high-fidelity textured meshes rapidly, faces challenges. The multi-view prediction model may produce less satisfactory predictions for skewed or non-perspective inputs. Furthermore, the geometric coloring algorithm currently does not support texture maps. In the future, we aim to enhance the robustness of the multi-view prediction model by training on a more extensive and diverse dataset.

  
**Method** & **PSNR\(\)** & **SSIM\(\)** & **LPIPS\(\)** & **Clip-Sim\(\)** & **CD\(\)** & **Vol. IoU\(\)** & **F-Score\(\)** \\  Unique3D & 19,6744 & 0.9217 & 0.1101 & 0.8864 & 0.0118 & 0.5463 & 0.6833 \\   

Table 2: Quantitative comparison results for ablation on 100 random samples with random rotation on GSO dataset.

Figure 8: **Challenging examples.**

Figure 7: **Ablation on Resolution**. The visualization of the generated multi-views images at different stages is shown. Multi-level super-resolution does not change the general structure, but only improves the detail resolution, allowing the model to remain well-detailed.