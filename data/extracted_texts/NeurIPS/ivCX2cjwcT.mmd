# Identifiable Shared Component Analysis of Unpaired Multimodal Mixtures

Subash Timilsina

School of EECS

Oregon State University

Corvallis, OR 97331

timilsis@oregonstate.edu

&Sagar Shrestha

School of EECS

Oregon State University

Corvallis, OR 97331

shressag@oregonstate.edu

&Xiao Fu

School of EECS

Oregon State University

Corvallis, OR 97331

xiao.fu@oregonstate.edu

###### Abstract

A core task in multi-modal learning is to integrate information from multiple feature spaces (e.g., text and audio), offering modality-invariant essential representations of data. Recent research showed that, classical tools such as _canonical correlation analysis_ (CCA) provably identify the shared components up to minor ambiguities, when samples in each modality are generated from a linear mixture of shared and private components. Such identifiability results were obtained under the condition that the cross-modality samples are aligned/paired according to their shared information. This work takes a step further, investigating shared component identifiability from multi-modal linear mixtures where cross-modality samples are unaligned. A distribution divergence minimization-based loss is proposed, under which a suite of sufficient conditions ensuring identifiability of the shared components are derived. Our conditions are based on cross-modality distribution discrepancy characterization and density-preserving transform removal, which are much milder than existing studies relying on independent component analysis. More relaxed conditions are also provided via adding reasonable structural constraints, motivated by available side information in various applications. The identifiability claims are thoroughly validated using synthetic and real-world data.

## 1 Introduction

The same data entities can often be represented in different feature spaces (e.g., audio, text and image), due to the variety of sensing modalities or domains. Learning common latent components of data from multiple modalities is well-motivated in representation learning. The shared components are considered modality-invariant essential representations of data, which can often enhance performance of downstream tasks by shedding modality-specific noise  and avoiding over-fitting .

A prominent theoretical aspect of shared component learning lies in _identifiability_ of the components of interest. The literature posed an intriguing theoretical question : If every modality of data is represented by a linear mixture of shared and private components with an unknown mixing system, are the shared components identifiable (up to acceptable ambiguities)? Such component identification problems are often nontrivial due to the ill-posed nature of any linear mixture model (see, e.g., ). Interestingly, the work  showed that using the classical _canonical correlationanalysis_ (CCA) provably find the shared components up to rotation and scaling. In fact, shared component identification from multimodal/multiview linear mixtures were considered in various contexts (see, e.g., [15; 16; 17; 18]), although some of these works did not model private components. The identifiability results in [1; 2] were generalized to nonlinear mixture models as well [4; 19]. The shared component identification perspective was also related to the success of representation learning in self-supervised learning (SSL) [5; 6; 7].

Nonetheless, the treatment in [1; 2] and the related works [15; 16; 17] all assumed that the cross-modality data are aligned (i.e., paired) according to their shared components. In many applications, such as cross-language information retrieval [20; 21; 22], domain adaptation [23; 24; 25], and biological data translation [26; 27], aligned cross-modality data are hard to acquire, if not outright unavailable. A natural question is: _When the multimodal linear mixtures are unaligned, can the shared latent components still be provably identified under reasonably mild conditions?_

**Existing Studies.** Theoretical characteristics of unaligned multimodal learning were studied under various settings. The work  considered a case where one modality is a linear transform of another modality, and showed that the linear transformation is potentially identifiable. The recent work  extended this model to a nonlinear transform setting. However, these works did not consider _latent_ component models--yet the latter are more versatile in many ways, e.g., facilitating one-to-many cross-domain translations [30; 31]. The work  considered unaligned mixtures of shared and private components, but the assumptions (e.g., the availability of a large amount of modalities) to ensure identifiability may not be easy to satisfy. The most related work is perhaps . But their approach also relied on somewhat stringent assumptions, e.g., that all the latent components are element-wise statistically independent with at most one component being Gaussian. This is because their procedure had to invoke the classical _independent component analysis_ (ICA) .

**Contributions.** In this work, we provide a suite of sufficient conditions under which the shared components can be provably identified from unaligned multimodal linear mixtures up to reasonable ambiguities. The model and identification problem are referred to as _unaligned shared component analysis_ (unaligned SCA) in the sequel.

_(i) An Identifiable Learning Loss for Unaligned SCA._ We propose to tackle the unaligned SCA problem by matching the probability distributions of linearly embedded multi-modal data. We show that under reasonable conditions, the linear transformations identifies the shared components up to the same ambiguities as those in the aligned case [1; 2]. The conditions are considerably milder compared to the existing unaligned SCA work .

_(ii) Enhanced Identifiability via Structural Constraints._ We come up with two types of structural constraints, motivated by available side information in applications, to further relax the identifiability conditions. Specifically, we look into cases where the multi-modal data have similar linear mixing systems and cases where a few cross-domain aligned samples available. We show that by adding constraints accordingly, unaligned SCA are identifiable under much milder conditions.

Our contributions primarily lie in identifiability analysis. Nonetheless, we also show the usefulness of our results in real-world applications, namely, _cross-lingual word retrieval_, _genetic information alignment_ and _image data domain adaptation_. Particularly, it shows that our succinct multimodal linear mixture model can effectively post-process outputs of pre-trained encoders, e.g., those in [34; 35], to improve data representations and enhance downstream task performance.

**Notation.** Notation definitions can be found in Appendix A.

## 2 Background

**Generative Model of Interest.** Following the classical settings in [1; 2; 15; 16; 18], we consider modeling the multi-modal data as linear mixtures. More specifically, we adopt the model in [1; 2]that splits the latent representation of data into shared components and private components:

\[^{(q)}=^{(q)}^{(q)},^{(q)}=[^{},(^{(q)})^{}]^{},\ q=1,2,\] (1)

where \(^{(q)}^{d^{(q)}}\) represents the data from the \(q\)th modality, \(^{(q)}^{d_{}+d_{}^{(q)}}\) represents the corresponding latent code, \(^{d_{}}\) and \(^{(q)}^{d_{}^{(q)}}\) stand for the shared components and the private components, respectively. The data \(^{(q)}\)'s are assumed to be zero-mean, which can be enforced by centering. Note that the positions of \(\) and \(_{q}\) are not necessarily arranged as \([^{},(^{(q)})^{}]^{}\) (more generally, \(^{(q)}=^{(q)}[^{},(^{(q)})^{}]^{}\) with an unknown permutation matrix \(^{(q)}\)). However, the representation in (1) is without loss of generality as one can define \(^{(q)}:=^{(q)}(^{(q)})^{}\) to reach the representation in (1). For all the domains, we have

\[_{},^{(q)}_{^{(q)}},\] (2)

where \(_{}\) and \(_{^{(q)}}\) represent the distributions of the shared components and the domain-private components, respectively. Under (1), the two different range spaces \((^{(q)})\) for \(q=1,2\) represent two feature spaces. Then latent \(^{(q)}\) further distinguishes the modalities and often has interesting physical interpretation. For example, some vision literature use \(\) to model "content" and \(^{(q)}\) "style" of the images . In cross-lingual word embedding retrieval , \(\) represents the semantic meaning of the words, while \(^{(q)}\) represents the language-specific components. The goal of SCA boils down to finding linear operators to recover \(\) to a reasonable extent.

**Aligned SCA: Identifiability of CCA and Extensions.** Learning \(\) without knowing \(^{(q)}\) is a typical component analysis problem. Learning latent components from _linear mixture models_ (LMMs) like \(=\) lacks identifiability in general, due to the bilinear nature of the models. This is because one can find an infinite number of invertible matrices \(\) such that \(=^{-1}\). Then, both \((,)\) and \((,^{-1})\) can fit to the data \(\), making the problem ill-posed in tens of solution uniqueness; see, e.g.,  and more discussions in Sec. 5. Nonetheless, the works  studied the identifiability of \(\) under the model (1), using the assumption that the cross-modality samples share the same \(\) are aligned. In particular,  formulated the \(\)-identification problem as a CCA problem:

\[^{(q)}\}_{q=1}^{2}}{} [\|^{(1)}^{(1)}-^{(2)}^{(2)}\|_{2}^{2}]\] (3a) \[ ^{(q)}[^{(q)}(^{(q)})^{} ](^{(q)})^{}= q=1,2,\] (3b)

where \(^{(q)}^{d_{C} d^{(q)}}\). The expectation in (3a) is taken from the joint distribution of the _aligned pairs_\(_{^{(1)},^{(2)}}\), where every pair \((^{(1)},^{(2)})\) shares the same \(\). The formulation aims to find \(^{(q)}\) such that the transformed representations of the aligned pairs \(^{(1)}^{(1)}\) and \(^{(2)}^{(2)}\) are equal. In , it was shown that

\[}^{(q)}^{(q)}=\] (4)

under mild conditions (see Appendix E.1 for details), where \((}^{(1)},}^{(2)})\) is an optimal solution of the CCA formulation and \(\) is a certain non-singular matrix. Eq. (4) means that \(}^{(q)}\) finds the range space where \(\) lives in, i.e., \((^{(q)}_{1:d_{C}})\) under our notation.

**Unaligned SCA: Existing Result and Theoretical Gap.** The work in  studied the identifiability of \(\) under (1) when \(^{(1)}\) and \(^{(2)}\) are _unaligned_. Their approach works under the condition _that the elements of \(^{(q)}=[^{},(^{(q)})^{}]^{}\) are mutually statistically independent_. There, \(}^{(q)}=^{(q)}^{(q)}^{(q)}\) is assumed to have been estimated by ICA, where \(^{(q)}\) and \(^{(q)}\) represent the scaling and permutation ambiguities, respectively, which cannot be removed by ICA. The work  assumed \(^{(q)}=\) by imposing a unit-variance assumption on all the \(z^{(q)}_{i}\)'s. Then, a cross-domain matching algorithm is used to match the shared elements in \(}^{(1)}\) and \(}^{(2)}\). The formulation can be summarized as finding \(d_{C}\) pairs of non-repetitive \((i,j)\) such that \(_{i}^{}}^{(1)}\) and \(_{j}^{}}^{(2)}\) have identical distributions, where \(_{i}\) is the \(i\)th unit vector. Denote \(_{m}^{(1)}=_{i_{m}}^{}}^{(1)}\) and \(_{m}^{(2)}=_{j_{m}}^{}}^{(2)}\) for \(m[d_{C}]\). It can be shown that

\[_{m}^{(q)}=kc_{(m)}^{(q)},\ m[d_{C}],\] (5)

where \(k\{+1,-1\}\) and \(\) is a permutation of \(\{1,,d_{C}\}\) (see details in Appendix E.2 summarized from ). This method effectively applies ICA to each modality, and thus the ICA identifiability conditions  have to met by \(^{(1)}\) and \(^{(2)}\) individually. However, if one only aims to extract \(\) as in CCA, these assumptions appear to be overly stringent.

## 3 Proposed Approach

**Unaligned SCA: Problem Formulation** We assume that \(^{(q)}\)'s are zero-mean. We use the notation from CCA in (3a). However, since no aligned samples are available, we replace the sample-level matching objective with a distribution matching (DM) module, as DM can be carried out without sample level alignment:

\[ ^{(q)}^{d_{C} d^{(q)}},\;q=1,2,\] (6a) subject to \[^{(1)}^{(1)})}}{ {=}}^{(2)}^{(2)},\] (6b) \[^{(q)}[^{(q)}(^{(q)})^{} ](^{(q)})^{}= q=1,2.\] (6c)

where "\()}}{{=}}\)" means the distributions of \(\) and \(\) are the same.

The formulation in (6) can be realized using various distribution matching tools, e.g., _maximum mean discrepancy_ (MMD)  and _Wasserstein distance_. We use the adversarial loss:

\[_{^{(1)},^{(2)}}_{f}\,_{^{(1)}}( f(^{(1)}^{(1)}))+_{^{(2)}}(1-f( ^{(2)}^{(2)}))+_{q=1}^{2}(^{(q)} ),\] (7)

The first and second terms comprise the adversarial loss from GAN . It finds \(^{(q)}\) to confuse the best-possible discriminator \(f:^{d_{C}}\), where \(f\) is represented by a neural network in practice. It is well known that the minimax optimal point of the first two terms is attained when (6b) is met . We use \((^{(q)})=\|^{(q)}[^{(q)}(^{(q)}) ^{}](^{(q)})^{}-\|_{}^{2}\) to "lift" the constraints. This way, the learning criterion in (7) can be readily handled by any off-the-shelf adversarial learning tools.

**Identifability of Unaligned SCA** As we saw in Theorem 4, CCA identifies \(}^{(q)}^{(q)}=\) where \(^{d_{C} d_{C}}\) under the settings of aligned SCA. Establishing a similar result for unaligned SCA is much more challenging. First, it is unclear if (6b) could disentangle \(\) from \(^{(q)}\). In general, \(^{(q)}^{(q)}\) could still be a mixture of \(\) and \(^{(q)}\) yet (6b) still holds (e.g., when both \(\) and \(^{(q)}\) are Gaussian.)

Second, even when the disentanglement is attained via enforcing (6b) and we have \(^{(q)}^{(q)}=^{(q)}\), in general it does not hold that \(^{(1)}=^{(2)}\). This is because \(^{(1)})}}{{=}}^ {(2)}\) where \(^{(1)}^{(2)}\) can still be perfectly met (e.g., when \(_{^{(q)}}\) is symmetric Gaussian in Fig. 2 ). However, \(^{(1)}^{(2)}\) means that the extracted representations from the two modalities are not matched. This creates challenges for applications like cross-domain information retrieval, language translation, or domain adaptation.

Our intuition is as follows: If the two distributions \(_{,^{(1)}}\) and \(_{,^{(2)}}\) are very different, then \(^{(1)}^{(1)})}}{{=}}^{(2 )}^{(2)}\) cannot hold unless \(^{(q)}^{(q)}=[^{(q)},]\). We use the following to characterize such difference between the joint distributions:

**Assumption 1** (Modality Variability).: _For any two linear subspaces \(^{(q)}^{d_{C}+d_{}^{(q)}},\;q=1,2,\) with \((^{(q)})=d_{}^{(q)}\), \(^{(q)}^{d_{}^{(q)}}\) and linearly independent vectors \(\{_{i}^{(q)}^{c_{}+d_{}^{(q)} }\}_{i=1}^{d_{}},\;q=1,2\), the sets \(^{(q)}=\{,_{1}^{(q)},, _{d_{C}}^{(q)}\}+^{(q)},\;q=1,2,\) are such that if \(_{,^{(q)}}[^{(q)}]>0\) for \(q=1\) or \(q=2\), then there exists a \(k\) such that the joint distributions \(_{,^{(1)}}[k^{(1)}]_{,^{(2)}}[k^{(2)}]\), where \(k^{(q)}=\{ka\;|\;a^{(q)}\}\)._

The condition in Assumption 1 is a geometric way to characterize the difference between \(_{,^{(1)}}\) and \(_{,^{(2)}}\)--if the joint distributions have different measures for all possible "stripes", each being a direct sum of a subspace and a convex hull (see Fig. 2), then \(_{,^{(1)}}\) and \(_{,^{(2)}}\) must be very different. Note that the difference is contributed by the modality-specific term \(^{(q)}\), and thus we call this condition "modality variability". Modality variability is similar to the "domain variability" used in --both characterize the discrepancy of the joint probabilities \(_{,^{(1)}}\) and \(_{,^{(2)}}\). However, there are key differences: The domain variability was defined in a unified latent domain over _arbitrary_ sets \(\), which could be stringent. Instead, we use the fact that (6) relies on linear operations to construct \(^{(q)}\), which makes the condition defined over a much smaller class of sets--thereby largely relaxing the requirements. Restricting \(^{(q)}\) to be stripes also makes the modality variability condition much more relaxed compared to the domain variability condition.

We show the following:

**Theorem 1**.: _Under Assumption 1 and the generative model in (1), denote any solution of (6) as \(}^{(q)}\)\(q=1,2\). Then, if the mixing matrices \(^{(q)}\) are full column ranks and \([^{}])\) is full rank, we have \(}^{(q)}^{(q)}=^{(q)} \). In addition, assume that either of the following is satisfied:_

* _The individual elements of the content components are statistically independent and non-Gaussian. In addition,_ \(c_{i})}}{{}}kc_{j}, i j,  k\) _and_ \(c_{i})}}{{}}-c_{i}, i\)_, i.e., the marginal distributions of the content elements cannot be matched with each other by mere scaling._
* _The support of_ \(_{}\)_, denoted by_ \(\)_, is a hyper-rectangle, i.e.,_ \(=[-a_{1},a_{1}][-a_{d_{C}},a_{d_{C}}]\)_. Further, suppose that_ \(c_{i})}}{{}}kc_{j}, i j,  k\) _and_ \(c_{i})}}{{}}-c_{i}, i\)_._

_Then, we have \(}^{(q)}^{(q)}= \), i.e., \(^{(q)}=\) for all \(q=1,2\), where \(^{(q)}\)._

In Theorem 1, Assumption 1 is used to guarantee \(}^{(q)}^{(q)}=^{(q)} \) and either of conditions (a) or (b) is used to make sure \(^{(1)}=^{(2)}\). Note that both (a) and (b) are milder than those in  (cf. Theorem 5), where the element-wise statistical independence of \(^{(q)}\) was relied on to find shared representation of \(^{(1)}\) and \(^{(2)}\). The proof is in Appendix B.

**Numerical Validation.** In Fig. 3, the top and bottom rows validate Theorem 1 under the assumptions in (a) and (b), respectively. In the top row, we set \(^{2}\), where \(c_{1}\) is sampled from Gaussian mixtures with three components and \(c_{2}\) is sampled from a Gamma distribution (and \(c_{1}\!\!\! c_{2}\)). We set \(p^{(1)}\) and \(p^{(2)}\) as one-dimensional Laplacian and uniform distributions. In the bottom row, the dimensions of \(\) and \(^{(q)}\) for \(q=1,2\) are unchanged, but their distributions are replaced in order to satisfy conditions in (b) (see details in Appendix F). One can see that clearly \(}^{(q)}=\); i.e., the learned \(}^{(q)}\) for \(q=1,2\) are identically rotated and scaled versions of \(\).

A remark is that our framework still allows to identify individual \(c_{i}\)'s as in .

**Corollary 1**.: _Under the conditions in Theorem 1 (a), Assume that at most one \(c_{i}\) for \(i[d_{}]\) is Gaussian. Then, the components of \(\) are identifiable up to permutation and scaling ambiguities by applying ICA to \(}^{(q)}=}^{(q)}^{ (q)}\) for either \(q=1\) or \(q=2\)._

The corollary means that to identify individual \(c_{i}\), using our formulation still enjoys much milder conditions relative to . Specifically, our condition only specifies the independence among elements of \(\), but the condition in  needs that all the elements in \(^{(q)}=[^{},(^{(q)})^{}]^{}\) are independent.

## 4 Enhanced Identifiability via Structural Constraints

Theorem 1 was well-supported by the synthetic data experiments. However, our experiments found that the learning criterion (6) often struggles to produce sensible results in some applications. Our conjecture is that the Assumptions in Theorem 1 (a) and (b) might not have been satisfied by the real data under our tests. Although they are not necessary conditions for identifiability, these conditions do indicate that the requirements to guarantee identifiability of unaligned SCA using (6) are nontrivial to meet. In this section, we explore a couple of structural constraints arising from side information in applications to remove the need for the relatively stringent assumptions on \(\).

**Homogeneous Domains.** The first structural constraint that we consider is \(^{(q)}=\) for \(q=1,2\). This model is motivated by the fact that advanced representation learning tools, e.g., self-supervised learning tools (e.g., SimCLR ) and foundation models (e.g., CLIP ), are already capable of mapping the data clusters to a shared linearly separable space--which indicates that the representations share a subspace, i.e., \(^{(q)}^{(q)}\). Under such circumstances, the proposed model and method can be used to further process the data by discarding the private components in the latent representation.

Here, we consider the special case of generative process in (1) where,

\[^{(q)}=[^{},(^{(q)})^{}]^{}.\] (8)

Under this model, we look for the shared components by solving (6) with a single \(=^{(1)}=^{(2)}\). We use the following version of the modality variability condition:

**Assumption 2**.: _For any linear subspace \(^{d_{}+d_{}},\;d_{}= d_{}^{(1)}=d_{}^{(2)}\), with \(()=d_{}\), \(^{d_{}}\) and linearly independent vectors \(\{_{i}^{d_{}+d_{}}\}_{i=1}^{d_{}},\;q=1,2\), the sets \(=\{,_{1},,_{d_{}}\} +,\;q=1,2.\) are such that if \(_{,^{(q)}}[]>0\) for \(q=1\) or \(q=2\), then the joint distributions \(_{,^{(1)}}[k]_{,^{( 2)}}[k]\) for some \(k\)._

**Theorem 2**.: _Consider the mixture model in (8). Assume that \(()=d_{}+d_{}\) and \(([^{}])=d_{}\), and that Assumption 2 holds. Denote \(}\) as any solution of (6) by constraining \(=^{(1)}=^{(2)}\). Then, we have \(}^{(q)}=\)._

One can see that the conditions (a) and (b) in Theorem 1 are completely removed, if the structure \(^{(1)}=^{(2)}\) is imposed. In fact, the result in Theorem 2 is expected and readily seen from the proof of Theorem 1, as the cause for \(^{(1)}^{(2)}\) is the use of two different \(^{(q)}\)'s. Nonetheless, this simple variation will prove useful in a series of real-data experiments.

**The Weakly Supervised Case.** Another way to add structural constraints is to use available auxiliary information. For example, some datasets have weak annotations and selected pairs; see, e.g., [43; 44].

**Assumption 3** (Weak Supervision).: _There exist a set of available aligned samples \((_{}^{(1)},_{}^{(2)})\) for \(\) such that \(_{}^{(q)}=^{(q)}_{}^{(q)},\;_{}^{(q)}=[ _{}^{},(_{}^{(q)})^{}]^{};\) i.e., \((_{}^{(1)},_{}^{(2)})\) share the same \(_{}\)._

Figure 3: Validation of Theorem 1. Top row: results under assumption (a). Bottom row: results under assumption (b).

The condition can be added into our formulation in (6) as a constraint, i.e.,

\[^{(1)}_{}^{(1)}=^{(2)}_{}^{(2)},\  .\] (9)

In the next theorem, we show that the incorporation of aligned samples helps relax conditions (a) and (b) in Theorem 1:

**Theorem 3**.: _Assume that Assumption 1 is satisfied, that \(|| d_{}\) paired samples \((_{}^{(1)},_{}^{(2)})\) are available, that \(^{(q)}\) for \(q=1,2\) have full column rank, and that \(_{}\) is absolutely continuous. Denote \((}^{(1)},}^{(2)})\) as any optimal solution of (6) under the constraint (9). Then, we have \(}^{(q)}^{(q)}=\)._

The proof and synthetic data validation can be found in Appendices D and F, respectively. Note that to realize (9), one only needs to add a regularization term \(_{}\|^{(1)}_{}^{(1)}-^{(2)} _{}^{(2)}\|_{2}^{2}\) to the loss in (7), where \( 0\) is a tunable parameter. The overall loss is still differentiable and thus can be easily handled by gradient based approaches.

A remark is that our weakly supervised formulation can use as few as \(d_{}\) pairs of \((_{}^{(1)},_{}^{(2)})\) to establish identifiability of shared component. In contrast, CCA requires at least \(d_{}+d_{}^{(1)}+d_{}^{(2)}\) pairs to attain the same identifiability (cf. Appendix. E.1).

**Private Component Identifiability.** Although our focus is shared component identification, we show that private components are also identifiable with additional assumptions; see Appendix H.

## 5 Related Works

_Identifiability of Component Analysis under Linear Mixture Models._ Various component analysis models were studied in the past several decades, e.g., principal component analysis , independent component analysis , sparse component analysis [10; 12], bounded component analysis , simplex component analysis [46; 47], and polytopic component analysis --motivated by their applications in dimensionality reduction, representation learning, and latent variable identification (see, e.g., topic mining [48; 49], hyperspectral unmixing [46; 47], audio/speech separation  and community detection ). The classical component analysis tools mostly study a single modality. The identifiability results under these models are well developed and documented.

_Identifiability of Shared Components from Aligned Modalities._ Modeling multimodal data as two or more linear/nonlinear mixtures of latent components was considered in CCA-related works [1; 2; 15; 19], _independent vector analysis_ (IVA) works [17; 18], multiview ICA works [16; 51], and SSL works [52; 5; 6; 7]. Partitioning the latent components into shared and private blocks was considered in [1; 2; 4; 5; 7; 52]. Shared component identifiability was established at the block level (see, e.g., [1; 2; 5]) and the individual component level (e.g., ) in these works. Nonetheless, they all rely on completely paired/aligned cross-modality samples, which we do not use in this work.

_Distribution Matching and Unaligned Multimodal Analysis._ Using distribution matching in unaligned multimodal data analytics for different purpose also has a long history; see applications in image-to-image translation , domain adaptation , cross-platform image super-resolution , and cross-domain information retrieval . The recent works  and  pointed out the identifiability challenge and the existence of density-preserving transforms. The works in [28; 29] started studying the uniqueness issues in distribution matching. However, the latent mixture models were not studied in this line of work.

_Identifiability of Unaligned SCA._ The works in [32; 41] investigated the shared component identifiability when the multimodal data are nonlinear mixtures of content and style (which are shared and private components, respectively) under the same mixing system. Hence, our identical linear mixing case in Theorem 2 can be understood as a special case of theirs. But their analysis relies on the assumption that all the latent components are statistically independent, which is much stronger than our conditions in Theorem 2. Their results also require that there are a large amount of modalities available. But our proof works for just two modalities. The most related work is , which uses the model in (1) in the context of multi-view causal graph learning. As discussed before, their assumptions on the latent components are much stronger than ours (see Corollary 1 and Appendix E.2).

## 6 Numerical Validation

**More Synthetic-Data Validation.** We first validate our proposed method on synthetic data that follows our model; see Appendix F for details.

**Application (i) - Domain Adaptation.** We first test the proposed methods over a number of domain adaptation (DA) tasks. In DA, we have the source domain data \(\{^{(1)}\}\) and the target domain \(\{^{(2)}\}\), respectively. Only the source domain data have labels and the two domains are unaligned. We hope to use our method to find shared representations of source and target, and thus the classifier trained using source data can also work well on the target data.

_Dataset_: We use two standard benchmarks of DA, i.e., _Office-31_ and _Office-Home_. The _Office-31_ dataset has 4652 images and 31 categories from three domains, namely, Amazon images (**A**), Webcam images (**W**) and DSLR images (**D**). The _Office-Home_ dataset contains 15,500 images with 65 object classes from four domains, i.e., Artistic images (**Ar**), Clip art images (**Cl**), Product images (**Pr**), and Real-world images (**Rw**).

_Setup_: We first test the homogeneous domain model in Sec. 4. The images are pre-processed using a ResNet50-based image encoder pre-trained over ImageNet1k . As mentioned, it was observed that self-supervised representation encoders find embeddings that are linearly separable , which justifies the use of the model \(^{(q)}^{(q)}\) in the embedding domain. After pre-processing, each image is represented by \(d^{(q)}=2048\) features for \(q=1,2\). We set \(d_{}=256\) for _Office-31_ and \(d_{}=512\) for _Office-Home_. More detailed settings are in Appendix G.

_Baselines and Training Setup_: The baselines are representative DA methods, namely, DANN, MDD, MCC, SDAT, and ELS. All the baselines use the same encoder-produced embeddings as inputs; see Appendix G.1 for their configurations. We also use ResNet encoder's outputs as an extra baseline as it learns informative and transferable features from the ImageNet-1K dataset. We follow the training strategies adopted by the baselines [25; 60; 62] to learn a classifier jointly with the shared latent components. This strategy arguably regularizes towards more classification-friendly geometry of the shared features. Therefore we append a cross-entropy (CE) based classifier training module to our loss in (7) that learns our feature extractor \(\). More details are in Appendix G.1.

_Metric_: The evaluation metric is the classification accuracy in the target domain \(\{^{(2)}\}\). The classifier is trained with the projected source domain \(}^{(1)}\) and the associated labels.

_Result_: Table 1 and Table 2 show the classification accuracy (mean\(\)std) on _Office-31_ and _Office-Home_, respectively. The results are averaged over 5 runs. One can observe that the proposed method offers the best and second best performance in most of the cases. In some tasks (e.g.,"**A\(\)W**", "**Ar\(\)CI**", "**Ar\(\)Pr**" and "**Rw\(\)CI**"), the proposed method outperforms the best-performing baselines by at least 2% in accuracy.

More results on the DA task can be found in Appendix G.1.

**Application (ii) - Single Cell Sequence Analysis.** In biomedical research, it is desired to fuse measurements from multiple sensorial modalities of the same cells, in order to have better characterizations of the cells. However, obtaining multimodal data of the same cells simultaneously is almost impossible, due to the sensing limitations. Therefore, many methods are proposed in the literature for aligning unpaired multi-modal single cell data [27; 64; 65]. We focus on the following two modalities of single-cell data : (1) the RNA sequences \(\{^{(1)}\}\) and (2) the ATAC sequences \(\{^{(2)}\}\).

 
**source \(\) target** & ResNet & DANN & MDD & MCC & SDAT & ELS & Proposed \\   \(\) & 85.2 \(\) 0.2 & 86.3 \(\) 0.3 & 86.4 \(\) 0.4 & 88.3 \(\) 0.3 & 88.6 \(\) 0.4 & 87.2 \(\) 0.3 & **94.4**\(\) 0.4 \\ \(\) & 97.5 \(\) 0.1 & 97.4 \(\) 0.3 & 97.7 \(\) 0.1 & 96.9 \(\) 0.1 & 97.6 \(\) 0.1 & 97.7 \(\) 0.1 & **97.8**\(\) 0.2 \\ \(\) & 99.5 \(\) 0.3 & 98.7 \(\) 0.2 & **99.7**\(\) 0.1 & 97.4 \(\) 0.2 & 99.1 \(\) 0.2 & 99.3 \(\) 0.2 & 99.5 \(\) 0.3 \\ \(\) & 89.4 \(\) 0.2 & 84.3 \(\) 0.4 & 89.9 \(\) 0.2 & 87.4 \(\) 0.5 & 86.3 \(\) 0.4 & 87.1 \(\) 0.2 & **90.1**\(\) 0.3 \\ \(\) & 71.4 \(\) 0.3 & 71.7 \(\) 0.4 & 70.6 \(\) 0.3 & **74.9**\(\) 0.4 & 72.3 \(\) 0.4 & 71.6 \(\) 0.3 & 71.9 \(\) 0.1 \\ \(\) & 73.1 \(\) 0.2 & 73.5 \(\) 0.2 & 72.3 \(\) 0.4 & 73.0 \(\) 0.4 & 73.6 \(\) 0.3 & 73.7 \(\) 0.3 & **74.6**\(\) 0.1 \\ 
**Average** & 86.0 \(\) 0.2 & 85.3 \(\) 0.3 & 86.1 \(\) 0.2 & 86.3 \(\) 0.3 & 86.2 \(\) 0.3 & 86.1 \(\) 0.2 & **87.3**\(\) 0.2 \\  

Table 1: Classification accuracy on the target domain of _office-31_ dataset (ResNet50 embedding).

_Dataset_: We use human lung adenocarcinoma A549 cells data from . The dataset contains 1,874 samples of RNA sequences \(\{^{(1)}\}\) and ATAC sequences \(\{^{(2)}\}\). Each data set is split into 1534 training samples and 340 testing samples as in . The data have labeled associations between the two domains--part of which will be used to test our weakly supervised formulation. For this experiment, features of RNA sequence and the ATAC sequence have dimensions of \(d^{(1)}=815\) and \(d^{(2)}=2613\), respectively. We set \(d_{}=256\). We use our weakly supervised formulation as shown in (9). We uniformly sampled a set of indices from the training set to serve as \(\).

_Baseline and Metric_: We use weakly supervised algorithm, namely, cross-modal autoencoder (CM-AE) work in , as a baseline, which also learns the shared representation between unaligned RNA and ATAC sequences. We use the \(K\)-nearest neighbor (\(k\)-NN) accuracy to evaluate the performance as suggested in .

_Result_: The plot in Fig. 4 shows the \(k\)-NN accuracy of the methods on the test set. Results show the mean and standard deviation over 10 runs, each having a different random initialization. For the proposed method, we vary the number of available paired samples from \(0\) (cf. Theorem 1) to \(d_{}=256\) (cf. Theorem 3). Note that the baseline uses more (i.e., \(256\) and \(770\)) paired samples. It also needs additional class labels, i.e., \(y_{i}^{(q)}\) for the \(i\)th sample \(_{i}^{(q)}\). Here, \(y_{i}^{(q)}\) represents the number of hours (\(0\), \(1\) or \(3\)) of cell treatment [27; 66]. The proposed method without any supervision (i.e., \(0\) paired samples) already exhibits around 3 times greater \(k\)-NN accuracy compared to the baseline for all \(k\). Moreover, including just one paired sample boosts the \(k\)-NN accuracy of the proposed method to around 5 times higher than the baseline for all \(k\). Finally, one can observe a steadily increasing \(k\)-NN accuracy with respect to the number of available paired samples. This corroborates with our Theorem 3.

 
**source \(\) target** & **ResNet** & **DANN** & **MDD** & **MCC** & **SDAT** & **ELS** & **Proposed** \\  \(}}\) & 42.0 \(\) 0.2 & 46.7 \(\) 0.2 & 47.4 \(\) 0.3 & 44.4 \(\) 0.3 & 47.3 \(\) 0.4 & 48.5 \(\) 0.2 & **51.0 \(\) 0.3** \\ \(}}\) & 69.2 \(\) 0.1 & 70.2 \(\) 0.4 & 72.8 \(\) 0.4 & 72.4 \(\) 0.2 & 71.1 \(\) 0.3 & 71.0 \(\) 0.3 & **75.8 \(\) 0.1** \\ \(}}\) & 80.2 \(\) 0.3 & 81.2 \(\) 0.4 & 81.2 \(\) 0.1 & 80.3 \(\) 0.3 & 80.5 \(\) 0.1 & 80.8 \(\) 0.4 & **82.5 \(\) 0.2** \\ \(}}\) & 60.7 \(\) 0.4 & 60.8 \(\) 0.3 & 62.4 \(\) 0.1 & 59.2 \(\) 0.4 & 57.6 \(\) 0.2 & 59.8 \(\) 0.1 & **62.7 \(\) 0.4** \\ \(}}\) & 71.0 \(\) 0.1 & 69.8 \(\) 0.3 & 70.0 \(\) 0.4 & **71.1 \(\) 0.4** & 66.5 \(\) 0.1 & 68.5 \(\) 0.2 & 72.5 \(\) 0.3 \\ \(}}\) & 74.8 \(\) 0.2 & 73.3 \(\) 0.1 & 74.1 \(\) 0.1 & **76.2 \(\) 0.2** & 70.7 \(\) 0.1 & 71.7 \(\) 0.1 & 75.8 \(\) 0.1 \\ \(}}\) & 60.6 \(\) 0.2 & 62.2 \(\) 0.1 & 64.3 \(\) 0.1 & 59.2 \(\) 0.1 & 62.5 \(\) 0.4 & 60.9 \(\) 0.2 & **64.4 \(\) 0.3** \\ \(}}\) & 44.8 \(\) 0.1 & 48.8 \(\) 0.1 & 48.0 \(\) 0.3 & 46.2 \(\) 0.2 & 49.0 \(\) 0.3 & 49.6 \(\) 0.3 & **50.4 \(\) 0.1** \\ \(}}\) & 79.6 \(\) 0.1 & 80.3 \(\) 0.4 & 79.6 \(\) 0.3 & 80.3 \(\) 0.2 & 80.0 \(\) 0.1 & 79.2 \(\) 0.1 & **81.7 \(\) 0.2** \\ \(}}\) & 70.1 \(\) 0.2 & 71.5 \(\) 0.1 & 71.4 \(\) 0.3 & 67.8 \(\) 0.2 & 71.6 \(\) 0.4 & 71.3 \(\) 0.4 & **72.6 \(\) 0.1** \\ \(}}\) & 45.8 \(\) 0.2 & 50.9 \(\) 0.2 & 50.3 \(\) 0.1 & 50.0 \(\) 0.2 & 51.4 \(\) 0.1 & 50.7 \(\) 0.1 & **53.2 \(\) 0.1** \\ \(}}\) & 80.7 \(\) 0.1 & 80.6 \(\) 0.4 & 81.1 \(\) 0.1 & 81.2 \(\) 0.1 & 80.7 \(\) 0.1 & 79.8 \(\) 0.3 & **82.9 \(\) 0.3** \\ 
**Average** & 64.9 \(\) 0.1 & 66.3 \(\) 0.2 & 66.8 \(\) 0.2 & 65.6 \(\) 0.2 & 65.7 \(\) 0.2 & 65.9 \(\) 0.2 & **68.7 \(\) 0.2** \\  

Table 2: Classification accuracy on the target domain of _office-Home_ dataset (ResNet50 embedding).

Figure 4: \(k\)-NN accuracy for single-cell sequence alignment.

**Application (iii) - Multi-lingual Information Retrieval.** We also evaluated our method on a word embedding association problem from the natural language processing literature [20; 21]. This task aims to associate high-dimensional word embeddings across different languages according to their semantic meaning. The word embeddings in two languages are represented using two sets of vectors, i.e., \(\{_{i}^{(1)}\}_{i=1}^{I}\) and \(\{_{i}^{(2)}\}_{j=1}^{J}\). The postulate is that if \(_{i}^{(1)}\) and \(_{j}^{(2)}\) have the same meaning (e.g., both representing "cat") in two languages (e.g., English and German), they should share a latent components \(\).

_Dataset_: We use the word embeddings from the MUSE dataset (https://github.com/facebookresearch/MUSE) . These monolingual word embedding are generated using fastText  and has dimensions of \(d^{(q)}=300\) for \(q=1,2\). The training dataset include 200,000 word embeddings in each language. In our experiment we set \(d_{C}=256\). We follow the generative model under (8) and run the formulation in (7) to learn the linear transformation \(\).

_Baseline_: We use Adv as the baseline which also uses distribution matching between two language domains. Unlike our method, Adv does not use linear mixture models.

_Metric_: We follow  to use the average precision score calculated based on _nearest neighbor_ (NN) and _cross domain similarity local scaling_ (CSLS). Precision at \(k\) ("\(k\) precision") is computed by the number of times that one of the correct translations of source word is retrieved at top-\(k\) results (\(k\) ={1, 5, 10}). The final score is normalized to be in the range of 0 to 100, with 100 being the highest score indicating the best performance. To evaluate the performance, we use the same test data as in . For each source and target language pair, this dataset includes 1,500 source word embeddings. The source embeddings are used to retrieve corresponding embeddings from a pool of 200,000 target word embeddings.

_Result_: Table 3 reports the P@1 scores over the test data calculated for each source and target language pair. The languages are denoted as as **en** - English, **es** - Spanish, **it** - Italian, **fr** - French, **de** - Germany, **ru** - Russian, **ar** - Arabic and **vi** - Vietnamese. One can observe that the proposed method exhibits a better precision performance than that of Adv in most of the translation tasks. In particular, the proposed method significantly outperforms the baseline on the tasks **en\(\)ar**, **ar\(\)en**, **en\(\)vi** and **vi\(\)en**, showing at least \(10\%\) precision gains. Similarly, our method shows at least \(5\%\) improvements in both NN and CSLS based precision metrics in **en\(\)es** and **es\(\)en** tasks.

More details and additional experiments can be found in Appendix G.3.

## 7 Conclusion

In this work, we considered the problem of identifying shared components from unaligned multi-domain mixtures. We proposed a learning loss that matches the distributions of linearly transformed data. Based on this loss, we came up with a suite of sufficient conditions to ensure the identifiability of shared components. Furthermore, we proposed modified models and losses that enjoy more relaxed conditions for shared component identifiability. This was achieved via introducing structural constraints, namely, the homogeneity of the mixing systems and the existence of weak supervision. Our theoretical claims were validated with both synthetic and real-world data, demonstrating soundness of the theorems and usefulness of the models/algorithms.

**Limitations.** First, our conditions for shared component identification are sufficient. The necessary conditions are not underpinned, but necessary conditions assist understanding the limitations of the models and algorithms. Second, our methods were developed under the linear mixture model, which has limited expressiveness, and thus often requires pre-processing to approximately meet the model specification. We expect that results with similar flavors to be derived for nonlinear models in the future. Third, the results were derived under an unlimited data assumption. It would be interesting have a finite sample analysis. Finally, optimizing GAN-based losses is sensitive to hyperparameter settings. Back-propagation based minimax optimization occasionally fails to converge. More optimization-friendly losses and more stable algorithms are desirable in the context of distribution matching.