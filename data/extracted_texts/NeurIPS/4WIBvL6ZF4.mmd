# Dynamic Subgroup Identification in Covariate-adjusted Response-adaptive Randomization Experiments

Yanping Li

School of Statistics and Data Science

Nankai University

yanpingli@mail.nankai.edu.cn&Jingshen Wang

Division of Biostatistics

University of California, Berkeley

jingshenwang@berkeley.edu

&Waverly Wei

Department of Data Sciences and Operations

University of Southern California

waverly@marshall.usc.edu

Corresponding author

###### Abstract

Identifying subgroups with differential responses to treatment is pivotal in randomized clinical trials, as tailoring treatments to specific subgroups can advance personalized medicine. Upon trial completion, identifying best-performing subgroups-those with the most beneficial treatment effects-is crucial for optimizing resource allocation or mitigating adverse treatment effects. However, traditional clinical trials are not customized for the goal of identifying best-performing subgroups because they typically pre-define subgroups at the beginning of the trial and adhere to a fixed subgroup treatment allocation rule, leading to inefficient use of experimental efforts. While some adaptive experimental strategies exist for the identification of the single best subgroup, they commonly do not enable the identification of the best set of subgroups. To address these challenges, we propose a dynamic subgroup identification covariate-adjusted response-adaptive randomization (CARA) design strategy with the following key features: (i) Our approach is an adaptive experimental strategy that allows the dynamic identification of the best subgroups and the revision of treatment allocation towards the goal of correctly identifying the best subgroups based on collected experimental data. (ii) Our design handles ties between subgroups effectively, merging those with similar treatment effects to maximize experimental efficiency. In the theoretical investigations, we demonstrate that our design has a higher probability of correctly identifying the best set of subgroups compared to conventional designs. Additionally, we prove the statistical validity of our estimator for the best subgroup treatment effect, demonstrating its asymptotic normality and semiparametric efficiency. Finally, we validate our design using synthetic data from a clinical trial on cirrhosis.

## 1 Introduction

Most clinical trial designs adopt "one-size-fits-all" rules for treatment assignment and evaluation based on models that ignore patient heterogeneity. This approach is disconnected from medical practice in recent years, where physicians use each patient's diagnosis and prognostic variables tomake personalized, precision medicine treatment decisions. As such, identifying patient subgroups with differential responses to a treatment plays a pivotal role in designing randomized clinical trials . Adaptive clinical trials-that allow randomization probabilities to be adaptively optimized during the trial based on sequentially accrude data-have received much attention due to their potential advantages in promoting precision health. Nevertheless, these trials often involve pre-specifying the patient subgroups to be analyzed . This approach not only leads to inefficient use of experimental efforts but may also reduce the statistical power to detect non-pre-specified subgroups that exhibit high effect sizes. Consequently, there is a pressing need for novel and statistical clinical trial designs for dynamic subgroup identification to address these issues.

In this paper, we propose a novel statistical design that dynamically performs subgroup identification in CARA experiments. Our contributions are summarized as follows:

1. From the design perspective, there are three highlights of our design: (i) Our design facilitates dynamic identification and sequential refinement of the best subgroups within the framework of covariate-adjusted response-adaptive (CARA) experiments (Section 4). This adaptive setting enhances the ability to identify and adjust to the best-performing subgroups over time. (ii) Unlike traditional designs that focus on identifying a single best-performing subgroup, our design is tailored to identify the best set of subgroups with competitive performance (Section 3). This broader objective makes our approach suitable for more general application settings. (iii) Our design demonstrates a higher probability of correctly identifying the best subgroups compared to conventional designs. This efficiency ensures that experimental efforts are utilized more effectively.
2. From a theoretical perspective, our proposed design strategy accommodates tied treatment effects among candidate subgroups, whereas many existing methods demand that these effects be distinctly separated. Additionally, our algorithm involves resampling in the presence of dependent data structures caused by adaptive treatment allocation, presenting technical challenges in proving statistical validity. We overcome these challenges and show that the best subgroup identified by our design asymptotically converges to the true set of best subgroups (Theorem 1). We also demonstrate that our proposed design strategy converges to the oracle design, which is the optimal design under the setting that the underlying data-generating distribution is known (Theorem 2). Furthermore, we establish valid statistical inference for the treatment effect of the identified best subgroup and demonstrate that our constructed estimator is semiparametrically efficient (Theorem 3).

In comparison to the existing literature, our design strategy is closely related to CARA designs. Originating from response-adaptive randomization (RAR) designs . Heuristically, CARA incorporates covariate information along with treatment assignment probabilities based on observed outcomes . Building upon RAR designs, CARA designs utilize both outcome and covariate information to optimize for design objectives.  introduces a family of CARA designs that balance efficiency and ethics objectives. Further generalizations to incorporate semiparametric estimates have been explored by . Related developments in CARA designs include . However, conventional CARA designs often consider subgroups to be pre-specified, which is different from our goal of data-adaptively identifying the best subgroups.

Our proposed method also connects with the literature on subgroup identification. In post-hoc analyses using previously collected data, the first line of work uses data from prior randomized controlled trials.  employs clustering techniques based on randomized controlled trial data. A comprehensive review can be found in . The second line of work uses data from prior observational studies.  identifies subgroups from existing observational data using machine learning.  introduces a causal inference tree approach for subgroup identification, which requires specifying the conditional distribution of the outcome given covariates. Similarly,  develops causal inference tree types of algorithm that include double robust estimators for constructing subgroup-specific splitting criteria. Other tree-based approaches for subgroup identification include . Besides the tree-based approaches,  develops a subgroup identification method within the value function framework. While post-hoc subgroup analyses do not require new data collection, they often rely on untestable causal assumptions, limiting the credibility of causal conclusions. For instance, the unconfoundedness assumption necessary for causal inference in observational studies assumes random treatment assignment based on observed confounders, but unmeasured confounders can compromise these conclusions. Conversely, in randomized experiments, valid causal conclusions do not depend on such assumptions.

In the adaptive experiment literature, relatively few methods have been developed for identifying the best subgroups. While  is in the adaptive experiment setting and proposes a Bayesian adaptive design that sequentially revises treatment allocation to identify the effective subgroup-treatment pairs, their approach is carried out under the Bayesian framework, relying on the specification of prior distributions and does not provide theoretical justification for the identified subgroups. In contrast, our design is aligned with the frequentist framework and is model-free, avoiding any parametric modeling assumption on the joint distribution of potential outcomes and covariates, and providing theoretical investigations from three aspects.

As our method aims to identify the best subgroups, it shares some similarities with the multi-armed bandit (MAB) literature. Notable MAB algorithms, such as the Thompson sampling method , the \(\)-greedy algorithm, and the upper confidence bound algorithm , focus on identifying the best arm. Similar to the contextual bandit literature [21; 6; 1], our design also incorporates covariate information. However, our design objective diverges significantly from those in traditional MAB approaches as we seek to identify the best subgroups rather than the best arm. Moreover, recognizing that randomized experiments can be time-consuming, costly, and may result in adverse outcomes for patients if treatments are ineffective, our approach seeks to efficiently allocate experimental resources within a constrained budget to identify the most beneficial subgroups. Specifically, we focus on scenarios in which the cost per experimental unit (e.g., per patient) is significant. For instance, in clinical settings, randomized experiments are often expensive due to the substantial costs of treatment medications. As a result, the primary resource constraint in our framework is the limited number of treatments that can be administered, underscoring the need for a resource-efficient experimental design.

## 2 Formulation of CARA

In this section, we shall introduce the formulation of our covariate-adjusted response-adaptive randomization (CARA) experiment framework.

We enroll participants sequentially across \(T\) stages, where \(T<\). Denote the total number of enrolled participants as \(N=_{t=1}^{T}n_{t}\), where \(n_{t}\) is the number of participants in Stage \(t\), for \(t=1,,T\). The cumulative sample size up to Stage \(t\) is denoted as \(N_{t}=_{s=1}^{t}n_{s}\). In Stage \(t\), we denote the treatment assignment status of participant \(i\) as \(D_{it}\{0,1\}\), \(i=1,,n_{t}\), where \(D_{it}=1\) denotes the treatment arm, and \(D_{it}=0\) denotes the control arm. The observed outcome is denoted as \(Y_{it}\). We follow the Neyman-Rubin causal model [27; 34] to define \(Y_{it}(d)\) as the potential outcome we would have observed if participant \(i\) receives treatment \(d\) at Stage \(t\), for \(d\{0,1\}\). The observed outcome can then be represented as

\[Y_{it}=D_{it}Y_{it}(1)+(1-D_{it})Y_{it}(0), i=1,,n_{t}, t=1, ,T.\]

We assume that the outcomes are observed without delay, and their underlying distributions do not shift over time .

In CARA experiments, covariate information is also available to practitioners. We denote the covariate information for participant \(i\) as \(X_{it}\) and assume the covariate space \(\) can be partitioned into \(m\) regions, denoted as \(\{_{j}\}_{j=1}^{m}\). In clinical settings, each partition of the sample space is commonly referred to as a subgroup [3; 19; 44]. We denote the number of subjects enrolled in subgroup \(j\) at Stage \(t\) as \(n_{tj}=_{i=1}^{n_{t}}_{(X_{it}_{j})}\) and the cumulative sample size for group \(j\) up to Stage \(t\) is \(N_{tj}=_{s=1}^{t}n_{sj}\). Denote the total number of subjects enrolled in subgroup \(j\) as \(N_{j}=N_{Tj}\).

As we are interested in assessing the effectiveness of the treatment in each subgroup, we define the subgroup average treatment effect as

\[_{j}=[Y_{it}(1)-Y_{it}(0)|X_{it}_{j}], j=1, ,m.\]

Due to the adaptive nature of CARA experiments, practitioners can sequentially revise treatment allocation based on outcome and covariate information accumulated during the experiment. Formally, we define the treatment assignment probability for participants in subgroup \(j\) as

\[e_{tj}=(D_{it}=1|X_{it}_{j},}_{t-1}),  t=1,,T, j=1,,m,\]where \(}_{t-1}=\{(Y_{is},D_{is},X_{is})_{i=1}^{n}\}_{s=1}^{t-1}\) denotes the historical information up to Stage \(t-1\). In CARA experiments, we aim to dynamically revise \(e_{tj}\) to reach desired design goals, which shall be introduced in the following section.

## 3 Design objective for best subgroups identification

In real-world applications, suppose we start with \(m\) subgroups, and practitioners may only aim to find subgroups with the largest treatment effects. It is possible that the best-performing subgroup is not unique. For example, the FORTE trial is a clinical trial aiming to investigate the treatment effect of carfilizomib-based induction-intensification-consolidation regimens on a patient's progression-free survival rate . Instead of reporting the single best subgroup with the largest survival rate improvement, the trial reports that both the risk myeloma patient subgroup and high-risk patient subgroup show similar survival rate improvement.

Without loss of generality, suppose the population subgroup treatment effects follow the order \(_{1}_{2}_{k}>>_{m}\). In this case, there are \(k\) subgroups exhibiting the largest treatment effects. Thus, it is natural to identify all of the \(k\) best subgroups instead of the single best subgroup.

To describe the set of best-performing subgroups, we introduce the concept of a tie set. We denote the tie set of \(_{1}\) as \(_{1}=\{k:|_{1}-_{k}|=o(N^{-1/2}),\ k=1,,m\}\) which contains the indices of the tied subgroups. This tie set is also known as the "near tie set" as it captures the subgroups of which the treatment effects lie in the \(\)-local neighborhood of \(_{1}\). We then denote the subgroups that belong to the best set as \(_{_{1}}\). Note that when \(_{1}=\{1\}\), \(_{_{1}}\) is equivalent to \(_{1}\). The population treatment effect under the best subgroups is defined as \(_{_{1}}=[Y_{it}(1)-Y_{it}(0)|X_{it}_{j _{1}}_{j}]\). We further assume that \(_{_{1}}>_{j}\), for \(j_{1}\).

Our design objective is to correctly identify all the best subgroups. Mathematically, we aim to maximize the correct identification probability:

\[_{}\ _{_{1}}_{j _{1}}_{j},\ \ _{1}=\{1,,k\}.\]

Leveraging the large deviation theory [8; 12], we can formulate our design objective as

\[_{}_{j_{1}}G(_{ _{1}},_{j};e_{1},e_{j}):=-_{_{1}})^{2}}{2_{_{1}}(e_{1})+_{j}(e_{j}) }}, \ \] \[\  e_{j} 1-, \ \]

where \(_{1}=\{1,,k\}\), \(1 k<m\), \((0,1/2)\), and \(_{_{1}}\) denotes the variance of the best subgroups. Detailed derivations of the equivalence between the correct identification probability and the optimization objective see Appendix (Section C).

However, in practice, solving this optimization problem is challenging. On the one hand, as we do not have knowledge regarding the membership of subgroups that have the largest treatment effects, we do not have any information regarding \(_{_{1}}\). On the other hand, because experimenters have no prior information about the joint distribution of the subgroup treatment effects, \(_{j}\)'s and \(_{j}\)'s are also unknown. To address these two practical challenges, we propose a dynamic subgroup identification algorithm that can adaptively identify and merge the set of best subgroups. Additionally, the dynamic subgroup identification method operates seamlessly under a CARA experimental strategy, which allows experimenters to sequentially learn the unknown parameters and adjust the subgroup treatment allocation to attain our design objective.

## 4 Proposed design: Dynamic subgroup identification with CARA

In this section, we shall illustrate our proposed dynamic subgroup identification strategy with CARA design in Algorithm 1: the design strategy in Section 4.1, encompassing two sub-algorithms (Algorithm 2 and 3), followed by the statistical inference procedure in Section 4.2. We defer several variations of our algorithm to Appendix (Section J). To clarify our design strategy, we also provide a notation table in the Appendix (Section A, Table 2).

### Proposed design strategy

In Stage 1 (line 1-4), we obtain initial estimates of the group-level treatment effect \(_{1j}\) and the associated variances \(}_{1j}\). Then, in Stage \(t=2,,T-1\) (Algorithm 1 line 6-11), we perform three tasks: (1) adaptively update treatment allocation and treatment effect estimates, (2) dynamically identify best subgroups, and (3) select hyperparameters that help with dynamic subgroup identification.

For the first task, we obtain the optimal treatment allocation \(}_{t}^{*}=(_{t1}^{*},\ ,\ _{tm_{t}^{*}}^{*})\) by solving the following optimization problem based on sequentially collected data:

\[_{}_{2 j m_{t}^{*}}_{t-1,(j)}- _{t-1,(1)})^{2}}{2}_{t-1,(1)}(e_{1})+ }_{t-1,(j)}(e_{j})},\ _{l=1}^{m_{t}^{*}}_{il}e_{l} c_{1},\ c_{2} e_{l} 1 -c_{2},\ l=1,,m_{t}^{*},\]

where \(c_{1}(0,1)\) and \(c_{2}(0,1/2)\), \(_{il}=^{t}_{l=1}^{n_{s}}_{(X_{is} _{(l)})}}{_{l=1}^{t}n_{s}}\) is the estimated subgroup proportion. The total number of subgroups after merging the identified tie set in Stage \(t-1\) is denoted as \(m_{t}^{*}=m-|}_{t-1,1}|+1\) and the subscript \((j)\) indexes the subgroup with the \(j\)-th largest estimated treatment effect. The procedure of finding \(}_{t-1,1}\) shall be illustrated in Algorithm 2. In the set of constraints, the first one is the resource constraint, and the second one is the feasibility constraint. Because of the nonlinear objective function of the optimization problem above, we instead work with its equivalent epigraph representation:

\[}_{t}^{*}=_{}z:\ _{l=1}^{m_{t}^{*}} _{il}e_{l} c_{1},\ c_{2} e_{l} 1-c_{2},_{2 j  m_{t}^{*}}_{t-1,(j)}-_{t-1,(1)})^{2}} {2}_{t-1,(1)}(e_{1})+}_{t-1,(j)} (e_{j})}-z 0}.\] (1)

To calibrate for the complete randomized treatment allocation in Stage 1, we require an additional calibration step in Stage \(t\):

\[_{t,(j)}=_{t,(j)}^{*}N_{t,(j)}-N_ {t-1,(j)}(1)}{n_{t,(j)}},\ j=1,,m_{t}^{*},\] (2)

where \(n_{t,(j)}=_{i=1}^{n_{t}}_{(X_{is}_{(j)})}\), \(N_{t-1,(j)}(1)=_{s=1}^{t-1}_{i=1}^{n_{s}}_{(X_{is} _{(j)})}D_{is}\), and \(N_{t,(j)}=_{s=1}^{t}n_{s,(j)}\). We then allocate treatments with calibrated probability \(_{t,(j)}\) and update the subgroup treatment effects as in Eq (8).

**Dynamic identification of the best subgroups (Algorithm 2).** The dynamic subgroup identification algorithm for identifying \(}_{t,1}\) at Stage \(t\) involves a resampling step that generates bootstrap samples \(}_{t}^{}\) from a Gaussian distribution centering around \(}_{t}\) at Stage 1 and a resampling step that generates bootstrap samples with accrued data \(\{_{s}\}_{s=1}^{t}\) at later stages. In line 9, we identify the best subgroups at Stage \(t\) as

\[}_{t,1}=\{k:w_{k,(1)}^{}=1,k=1,,m_{t }^{*}\},\] (3) \[w_{k,(1)}^{}=\{-c_{}^{t}  N_{t}^{-}}_{t,(1)}^{}(_{tk}^{}-_{t,(1)}^{}) c_{}^{t} N _{t}^{-}}_{t,(1)}^{}\},\]

where the distance between the upper and lower bounds of the interval for \(w_{k,(1)}^{}\) is of order \(n^{}\) with \(=0.25\) to guarantee the statistical validity of our proposed procedure and balance the trade-off between bias and variance. Note that the dynamic subgroup identification procedure relies on a pair of hyperparameters \((c_{}^{t},c_{}^{t})\), which are selected data-adaptively. In what follows, we shall illustrate the algorithm for selecting these hyperparameters.

**Hyperparameter selection (Algorithm 3).** In line 7 of Algorithm 3, we adopt a bootstrap method and propose several alternative bootstrap methods in Algorithm 5 (line 3) in Appendix (Section 1). Algorithm 3 involves a resampling step that generates bootstrap samples \(}_{t}^{*}\) from a Gaussian distribution centering around \(_{t}^{*}\) at Stage 1. In line 2, we compute \(_{t}^{*}=(_{t1}^{*},,_{t,m_{t}^{*}}^{*})^{}\) as

\[_{tj}^{*}=^{m_{t}^{*}}_{tj}}{m_{t}^{ *}}+(1-)_{tj},\ j=1,,m_{t}^{*},\] (4)where \(=\{0.99,^{m_{t}^{*}}_{tj}}{N_{t}_{j=1 }^{m_{t}^{*}}(_{tj}-_{t})^{2}} N_{t}^{}\}\) and \((0,0.2)\). We impose a lower bound on \(\) to ensure that \(\) does not equal to 1. We choose \(=0.05\) in our simulation studies, and our procedure is shown to be not sensitive to the choice of \(<1\). In line 7, we compute \(_{t}^{*}=(_{t1}^{*},,_{t,m_{t} ^{*}}^{*})^{}\) at Stage \(t\) for \(t>1\) as

\[_{tj}^{*}=^{m_{t}^{*}} _{tj}^{}}{m_{t}^{*}}+(1-)_{tj}^{ },\] (5)

where \(_{tj}^{}\) is computed with the bootstrap samples as in Eq (8). In line 10, we compute

\[_{t,(1)}^{*}=_{k=1}^{m_{t}^{*}}w_{k,(1)}^{*} _{tk}^{*}_{k=1}^{m_{t}^{*}}w_{k,(1)}^{*}, _{tb}(c_{},c_{})=(_{t,( 1)}^{*,b}_{t,(1)}^{*}),\] (6)

where \(w_{k,(1)}^{*}=\{-c_{}^{t} N_{t}^{-} }_{t,(1)}^{}(_{tk}^{*}-_{t,(1)}^{*}) c_{}^{t} N_{t}^{-}}_{t,(1)}^{}\}\) and the subscript \((1)\) indexes the subgroup with the largest estimated treatment effect. We also propose a double bootstrap-based alternative hyperparameter selection procedure in Algorithm 4 in Appendix (Section J). Finding the optimal hyperparameters involves minimizing a loss function \(L_{t}(c_{},c_{})\) at each Stage \(t\), which is defined as

\[L_{t}(c_{},c_{})=(L_{ t0}(c_{},c_{})+L_{t1}(c_{},c_{ })),\] (7)

where for \(l=0,1\), \(L_{tl}(c_{},c_{})=_{b=1}^{B}(( _{tb}(c_{},c_{})=l)-^{B} (_{tb}(c_{},c_{})=l)}{B})^{2}\). Given a desirable pair of hyperparameters \((c_{},c_{})\) and \(l\) value, the indicator function \((_{tb}(c_{},c_{})=l)\) is binary, which roughly follows a Bernoulli distribution with probability \(_{k=1}^{B}(_{tb}(c_{},c_{})=l)\). Intuitively, the loss function defined in Eq (7) measures the average of squared differences between \((_{tb}(c_{},c_{})=l)\) and the expected value of Bernoulli\((^{B}(_{tb}(c_{},c_{})=l)}{B})\) random variables. We would expect that the optimal pair of hyperparameters \((c_{}^{t},c_{}^{t})\) at Stage \(t\) minimizes such a loss.

``` Stage 1 (Initialization):
1:Enroll \(n_{1}\) participants, and assign treatments in group \(j\) with \(e_{1j}=\);
2:Compute \(_{1j}\) and \(}_{1j}\);
3:Choosing hyperparameters \((c_{}^{1},c_{}^{1})\) using single bootstrap method (see Algorithm 3);
4:Identify tie set and merge tied subgroups with the best subgroup (see Algorithm 2). Stage \(t\) (Adaptive treatment allocation revision):
5:for\(t 2\) to \(T\)do
6: With \(_{t-1,(j)}\) and \(}_{t-1,(j)}\) estimated using Eq 8, solve the optimization problem in Eq (1) to find \(_{t,(j)}^{*}\);
7: Enroll \(n_{t}\) participants and assign treatment with calibrated probability \(_{t,(j)}^{*}\) as in Eq (2);
8: Update \(_{t,(j)}\) and \(}_{t,(j)}\) as in Eq (8);
9: Choosing hyperparameters \((c_{}^{t},c_{}^{t})\) using single bootstrap (see Algorithm 3);
10: Identify tie set and merge tied subgroups with the best subgroup (see Algorithm 2).
11: Calculate the merged subgroup ATE estimator \(_{t,}_{t1}}\) as in Eq (9), and its variance estimator \(}_{t,}_{t1}}\) as in Eq (10).
12:endfor Stage \(T\) (Inference):
13:Identify the best tie set \(}_{1}\), and construct two-sided confidence intervals for \(_{}_{1}}\) as in Eq (11). ```

**Algorithm 1** Dynamic subgroup identification CARA design

### Statistical inference

Our CARA design also enables making valid statistical inference on the estimated best subgroup treatment effect. We highlight two parts of the statistical inference procedure: (1) estimating unknown parameters based on accrued experimental data while dynamically identifying best subgroups, and (2) constructing valid confidence intervals to confirm the estimated best subgroup treatment effect.

First, based on accumulated experimental data, the subgroup treatment effects and associated variances can be updated as

\[_{t-1,(j)} =^{t-1}_{i=1}^{n_{s}}_{(X_{is} _{(j)})}D_{is}Y_{is}}{N_{t-1,(j)}(1)}-^{t-1}_{i= 1}^{n_{s}}_{(X_{is}_{(j)})}(1-D_{is})Y_{is}}{N_{t-1,(j )}(0)},\] (8) \[}_{t-1,(j)}(e_{j}) =^{t-1}_{i=1}^{n_{s}}_{(X_{is} _{(j)})}D_{is}Y_{is}-_{t-1,(j)}(1)^{2}}{N_{t-1,(j)}(1)} N_{t-1,(j)}}{N_{t-1}}^{-1}\] \[+^{t-1}_{i=1}^{n_{s}}_{(X_{is} _{(j)})}(1-D_{is})Y_{is}-_{t-1,(j)}(0)^{2}} {N_{t-1,(j)}(0)}) N_{t-1,(j)}}{N_{t-1}}^{-1},\]

where \(N_{t-1,(j)}(1)=_{s=1}^{t-1}_{i=1}^{n_{s}}_{(X_{is} _{(j)})}D_{is}\), \(N_{t-1,(j)}(0)=_{s=1}^{t-1}_{i=1}^{n_{s}}_{(X_{is} _{(j)})}(1-D_{is})\), \(N_{t-1,(j)}=_{s=1}^{t-1}_{i=1}^{n_{s}}_{(X_{is}_{(j)})}\), \(_{t-1,(j)}(0)=_{s=1}^{t-1}_{i=1}^{n_{s}}_{(X_{is} _{(j)})}(1-D_{is})Y_{is}/N_{t-1,(j)}(0)\), and \(_{t-1,(j)}(1)=_{s=1}^{t-1}_{i=1}^{n_{s}}_{(X_{is} _{(j)})}D_{is}Y_{is}/N_{t-1,(j)}(1)\).

Additionally, denote the subgroup proportions as \(p_{1},,p_{m}\). Following dynamic subgroup identification at each stage, we merge all subgroups in the \(}_{t1}\) and estimate the merged best subgroup treatment effect as

\[_{t,}_{t1}}=_{j }_{t1}}p_{j}_{tj}_{j}_{t1}}p_ {j},\] (9)

and estimate the variance of the merged best subgroup treatment effect estimator as

\[}_{t,}_{t1}}=_{j}_{t1}}p_{j}^{2}}_{tj}_{j }_{t1}}p_{j}^{2}.\] (10)

In Stage \(T\) (line 13), we let \(}_{1}:=}_{T1}\), \(_{}_{1}}:=_{_{t} }_{1}}\) and \(}_{}_{1}}:=}_{T, }_{1}}\). Lastly, to confirm the estimated best subgroup treatment effect, we construct a two-sided level-\(\) confidence interval as

\[_{}_{1}}^{-1}(1-/2) }_{}_{1}}/N}.\] (11)

**Step 1 (Input)**:

```
1:Input \(\{_{s}\}_{s=1}^{t}\), \(_{tj}\), \(}_{tj}\), and \((c_{}^{t},c_{}^{t})\) computed from Algorithm 3.
2:Step \(b\) (Bootstrap)**:
3:for\(b 1\) to \(B\)do
4:if\(t=1\)then
5: Generate \(_{1}^{}\) from \((_{1},_{n}/n_{1})\), where \(_{n}=(}_{11},, }_{1m})\);
6:elseif\(t>1\)then
7: Generate \(n_{s}\) resamples randomly with replacement sequentially from each \(_{s}\);
8: Compute \(_{tj}^{}\) as in Eq (8) with the bootstrap samples;
9:endif
10: Identify the best subgroups \(}_{t1}\) with the bootstrap samples as in Eq (3).
11:endfor Step \(B\) (Output):
12:Choose \(}_{t1}\) with the highest frequency of occurrence and merge subgroups that belong to \(}_{t1}\). ```

**Algorithm 2** Dynamic subgroup identification in Stage \(t\)

## 5 Theoretical investigation

**Assumption 1** (Regularity conditions).: \((Y_{it}(0),Y_{it}(1),X_{it})\) _are independently identically distributed for \(i=1,,n_{t}\), \(t=1,,T\). In addition, \([|Y_{it}(d)|]^{4}<\), \(d\{0,1\}\). Lastly, there exists some \(>0\), such that \([Y_{it}(d)|X_{it}_{j}]\) for \(d\{0,1\}\), \(j=1,,m\)._

**Assumption 2** (Positivity).: _The subgroup proportions \( p_{1},,p_{m} 1-\), \((0,1/2)\)._

Assumption 1 says that the potential outcomes have bounded moments and have variability in each subgroup. Assumption 2 says that the subgroup proportions are non-zero in the population.

**Theorem 1** (Dynamic best subgroup identification consistency).: _Under Assumptions 1 and 2, for \(j=1,,m\) and for \(>0\), we have_

\[_{N}(|(j}_{1})-(j_{1})|> )=0.\]

Theorem 1 suggests that our dynamic subgroup identification algorithm correctly identifies the best set of subgroups as the sample size tends to infinity.

**Theorem 2** (Design strategy consistency).: _Under Assumptions 1 and 2, for \(>0\), as \(n_{t}\), \(t=1,,T\), for the actual treatment allocation, we have_

\[(||}_{t}-^{*}||) 1,\]

where \(^{*}=(e_{1}^{*},\ ,\ e_{m}^{*})\) is the optimal treatment allocation.

Theorem 2 says that the actual treatment allocation under our proposed design strategy converges to the optimal treatment allocation asymptotically.

**Theorem 3** (Asymptotic normality).: _Under Assumptions 1 and 2, as \(N\),_

\[(_{}_{1}}-_{_{1}}) 0,_{_{1}}(e_{1}^{*}),\]

_and_

\[}_{}_{1}}-_{_{1}} (e_{1}^{*})=O_{p}(}).\]

Theorem 3 says that the estimated treatment effect of the identified best subgroups converge to a Gaussian distribution asymptotically, and our variance estimator consistently estimates the asymptotic variance. Theorem 3 also verifies the validity of our constructed confidence interval.

## 6 Synthetic real data study

In this section, we investigate the performance of our proposed design strategy for identifying the tie set of best-performing subgroups in a synthetic case study using clinical trial data.

We design our synthetic case study using the dataset from the Mayo Clinic's trial on primary biliary cirrhosis (PBC), containing clinical biomarkers, treatments, and patient outcomes. PBC is a progressive autoimmune liver disease marked by inflammation and damage to the intrahepatic bile ducts. The Mayo Clinic conducted an extensive trial from 1974 to 1984 to assess the effectiveness of D-penicillamine in treating PBC. This dataset includes 424 patients, encompassing both those who were actively enrolled in the trial and additional cases who consented to provide basic measurements .

In this case study, we work with a subset (\(n=312\)) of patients who participate in the randomized controlled trial. These patients are randomly assigned to one of the two arms: the treatment arm (\(n=158\)), who receive D-penicillamine \(D=1\), and the control arm (\(n=154\)), who receive placebos \(D=0\). The outcome of interest is the square root of the survival time, defined as the number of days from registration to the earlier death, transplantation, or the time of study analysis. This dataset includes 17 covariates, and we use median imputation to handle missingness in these covariates. We aim to investigate the effectiveness of D-penicillamine in improving liver function and symptoms in five subgroups defined by age (in days): (1) patients with age in \(\), (2) age in \((15,695,17,082]\), (3) age in \((17,082,20,440]\), (4) age in \((20,440,21,900]\), (5) age in \((21,900,28,650]\). We generate synthetic experimental data based on the original dataset, which shall be illustrated in the next section.

We generate synthetic data that mimic the original PBC dataset. Denote the subgroup membership for each participant \(i\) as \(}=(_{(X_{i}_{1})},,_{(X_{i} _{s})})^{}\). We generate the potential outcome from \(Y_{i}(d)|X_{i}_{j}(_{dj},_{dj}^{2}),j=1, ,5\), where \(_{1}=(42.57,50.44,44.37,44.30,37.71)^{},_{0}=(45.34,3 9.91,45.58,33.42,39.17)^{},_{1}=(10.85,12.29,12.64,14.28,1.64)^{},_{0}=(11.50,15.18,14.57,13.09,15.06)^{}\). The subgroup proportions are \(=(0.28,0.13,0.30,0.11,0.19)^{}\). We denote the true subgroup treatment effects as \(=(-2.77,10.53,-1.21,10.89,-1.46)^{}\). Therefore, Subgroup 2 and Subgroup 4 are the set of best subgroups with a merged average treatment effect of 10.70. The treatment assignment \(D_{i}\) is decided based on different experiment strategies, which shall be discussed later in the section. To generate synthetic data, We mimic CARA experiments where participants are enrolled sequentially across \(T\) experimental stages. Here, we set \(T=15\) and \(n_{t}=400\), for \(t=1,,T\). All experiments are conducted with an Intel Core i7-11800H CPU and 16 GB of RAM.

We compare our proposed design strategy with the complete randomization design and two multi-armed bandit (MAB) algorithms. (1) The complete randomization design refers to a design that fixes \(e_{tj}=\) across all experimental stages, \(t=1,,T,j=1,,m\). (2) To customize the MAB algorithms to our setting, for each subgroup, we consider two candidate arms: treatment and control. We set the rewards as the negative asymptotic variance of the treatment effect estimator, i.e., \(-_{j}(e_{j})\). We consider two MAB algorithms: (a) The \(\)-greedy algorithm, which aims to balance the exploration and the exploitation efforts . Here, we set \(=0.1\). (b) The upper confidence bound 1 algorithm balances exploration and exploitation using confidence intervals and chooses the arm that maximizes the upper confidence bound on the estimated reward . When customizing MAB algorithms to our setting, we omit the step of identifying and merging tie sets using conventional methods, such as K-means or agglomerative clustering, due to several challenges: the requirement to predefine the number of clusters, the limited applicability of clustering for a small number of subgroups, and the inconsistency of these methods in effectively merging the best subgroups.

Figure 1: Comparison of the correct selection probability among three conventional methods and our proposed design strategy.“Single and separate bootstrap” refers to our proposed design.

To evaluate the performance of different design strategies, we assess the effectiveness of each adaptive experiment strategy from two aspects. First, we compare the correct selection probability of identifying the best subgroups. The correction selection probability can be written as \((_{_{1}}_{j_{1}} _{j})\). Second, we compare the \(95\%\) confidence interval, \(\)-scaled bias, and standard deviation of the estimated best subgroup treatment effect. In our resampling procedure, we set \(B=2,000\). The synthetic case study results are summarized in Figure 1 and Table 1.

First, from Figure 1, our proposed design strategy shows a higher correct selection probability than the complete randomization design and the MAB algorithms. Additionally, the correct selection probability under our proposed design strategy increases with the number of experimental stages. Specifically, our proposed design has a correct selection probability tending to \(1\) after 15 experimental stages. We also adopt the normalized mutual information as an additional metric to compare our proposed design and three competing methods in the Appendix (Section K.1), which further confirms that our proposed design strategy outperforms the conventional methods.

Second, from Table 1, we observe that our proposed design strategy has a smaller standard deviation and smaller \(\)-scaled bias, implying that our method is more efficient and less biased. In sum, our proposed adaptive design demonstrates efficient use of experimental data to correctly identify the best-performing subgroups of the three competing methods. We defer additional simulation studies to the Appendix (Section K.1) and an additional synthetic real data study to the Appendix (Section K.3). We also extend our proposed dynamic subgroup identification with CARA to the augmented inverse propensity score weighting (AIPW) estimator. Then we compare our proposed design with AIPW estimator with the three contextual MAB algorithms in the Appendix (Section K.2).

## 7 Discussion

We propose a dynamic subgroup identification method within the CARA design framework that could significantly advance precision medicine. However, we acknowledge some limitations in our approach. We aim to explore and address these challenges in our future research.

First, our method assumes that outcomes in CARA experiments are observed immediately at the end of each stage without delay. This assumption, prevalent in adaptive experiments such as  and , simplifies the modeling process and facilitates quick adjustments based on the latest data. However, in some practical scenarios, outcomes may be observed with delays, complicating the process of adjusting treatment allocations, as highlighted by  and . In future work, we plan to revise our design framework and update our estimators to account for the impact of delayed responses on both treatment effects and variance estimators.

Second, our work addresses scenarios where assigning a treatment is costly, and there is an overall constraint on how many treatments can be deployed. A key challenge arises from the potential misalignment between efficiently estimating the best causal effect and determining the optimal causal decision rule-particularly when the best decision rule is to treat all patients when there is any positive effect. For instance,  focuses on ensuring that all individuals who would benefit are accurately assigned to the treatment arm, assuming negligible treatment costs. To maximize the welfare for participating subjects, in future work, we plan to incorporate an "early stopping" step which would not only identify the most effective subgroups but also halt enrollment for subgroups exhibiting significantly adverse treatment effects.

   Method & Est (\(95\%\) CI) & \(\)Bias & SD \\  CR & 11.33 (9.30,13.36) & 34.55 & 80.29 \\ \(\)-greedy & 11.26 (9.20,13.31) & 28.84 & 81.19 \\ UCB 1 & 11.34 (9.29,13.38) & 34.82 & 80.85 \\ Proposed & 10.31 (9.28,11.35) & 29.98 & 40.94 \\   

Table 1: Comparison among three conventional methods and our proposed design strategy based on estimated best tie set or subgroup treatment effect (Est), \(95\%\) confidence interval (95% CI), \(\)-scaled bias, and standard deviation (SD).