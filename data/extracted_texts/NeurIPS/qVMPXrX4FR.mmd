# LambdaBeam: Neural Program Search with Higher-Order Functions and Lambdas

Kensen Shi

Google DeepMind

kshi@google.com

&Hanjun Dai

Google DeepMind

hadai@google.com

&Wen-Ding Li

Cornell University

wl678@cornell.edu

&Kevin Ellis

Cornell University

kellis@cornell.edu

&Charles Sutton

Google DeepMind

charlessutton@google.com

###### Abstract

Search is an important technique in program synthesis that allows for adaptive strategies such as focusing on particular search directions based on execution results. Several prior works have demonstrated that neural models are effective at guiding program synthesis searches. However, a common drawback of those approaches is the inability to handle iterative loops, higher-order functions, or lambda functions, thus limiting prior neural searches from synthesizing longer and more general programs. We address this gap by designing a search algorithm called LambdaBeam that can construct arbitrary lambda functions that compose operations within a given DSL. We create semantic vector representations of the execution behavior of the lambda functions and train a neural policy network to choose which lambdas to construct during search, and pass them as arguments to higher-order functions to perform looping computations. Our experiments show that LambdaBeam outperforms neural, symbolic, and LLM-based techniques in an integer list manipulation domain.

## 1 Introduction

Program synthesis involves finding a program meeting a given specification of what the program should do . When the specification is in the form of input/output examples, known as programming by example (PBE), combinatorial search has been an especially popular technique . Learning can also play a key role in PBE, because well-designed search algorithms can learn to adapt to information collected during the ongoing search, such as execution results or other analyses of candidate programs considered so far. This information can be used to prune redundant parts of the search space or focus on parts deemed more promising. For example, DeepCoder  and TF-Coder  use neural models to define a search space that is explored by traditional non-neural search, while Bustle, CrossBeam, and Execution-Guided Synthesis  use neural models to guide the search process itself. However, those prior works are unable to generate programs with arbitrary looping computations, whether implemented via loop control structures or through the use of higher-order functions with arbitrary lambda functions.1 Even though large language models (and other sequence models) can output programs with loops and are very effective at synthesizing programs from natural language , PBE demands a more systematic search strategy that adapts to valuable information like execution results during the search.

The fundamental question explored in this paper is whether a neural program synthesis search policy can learn to reason about lambdas and higher-order functions, which would enable the synthesis of arbitrary looping computations that were not previously possible with neural synthesis search techniques that rely on intermediate expression evaluation. Previous work [27; 33] has shown that neural models can effectively guide search when every candidate program can be evaluated to produce a concrete value for the model to inspect, allowing it to make decisions based on comparisons between explored values and the desired output. Lambda functions however are extremely different: they represent plans of functionality to be performed later, without specifying the context in which this functionality will be used. As a result, reasoning about lambdas requires a more abstract form of planning. Without knowing how the lambda might be used later, the search policy must understand the different behaviors of lambdas, predict whether a lambda will be useful for a given task to prioritize search directions, and recognize when and how to use lambdas within higher-order functions to actually perform useful computations.

In order to design a neural search algorithm that handles lambdas and higher-order functions, we address some key difficulties. One challenge is in the algebraic representation and subsequent manipulation of lambdas. We want to represent "practically equivalent" lambdas like \( x.\)\(x+1\), \( y.\)\(y+1\), and \( x,y.\)\(x+1\) in a canonical way to prune the search space. If \( x.\)\(x+1\) is their canonical representation, then how can we reuse that lambda to create a new lambda such as \( x,y.\)\((x+1)(y+1)\) where the "\(+1\)" functionality is used in different ways? We address this challenge by defining a new Merge operation that combines lambda expressions into larger ones while allowing for variable renaming and adhering to other representational constraints, therefore enabling a bottom-up search algorithm to systematically build larger lambdas from existing ones.

A second difficulty is in the encoding of lambdas when used as inputs to neural models. A naive representation would be to encode the code tokens, but slight changes in the code could lead to drastic differences in the lambda's behavior. Instead, we introduce a method of encoding lambdas that more directly reflects their execution semantics. We do this using _property signatures_ in a way agnostic to how the lambda is used later (i.e., what inputs are given to the lambda by a higher-order function), but still analyzing the lambda's behavior in the context of the current PBE task. One conclusion of our work is that this encoding does enable neural models to reason about lambdas effectively.

We present a new neural synthesis search method called LambdaBeam, which combines our solutions to these challenges within the search framework of the recent work CrossBeam. CrossBeam performs a bottom-up search applying DSL operations to previously-explored values, using a neural search policy to choose the operation's arguments with a pointer network. Thus, in LambdaBeam, the neural policy is able to reason about lambdas by choosing which ones to construct and when to use them, such that the search direction is tailored to the synthesis task.

We demonstrate the effectiveness of LambdaBeam in the DeepCoder  domain of integer list manipulation. We extend the DeepCoder DSL by adding many first-order operations, keeping its higher-order functions, and replacing its limited set of hardcoded lambda functions with arbitrary lambdas using compositions of other DSL operations. Using a benchmark suite containing 100 natural hand-crafted evaluation tasks and 100 synthetically-generated tasks, we experimentally show that LambdaBeam outperforms prior approaches including state-of-the-art symbolic search, neural sequence models trained from scratch, and a 62 billion parameter large language model (LLM). We release our LambdaBeam code and trained model checkpoints at https://github.com/ellisk42/LambdaBeam.

## 2 Background

Programming By Example_Programming by Example_(PBE) is the task of synthesizing programs that satisfy a given set of input/output (I/O) examples. In this task, we have a domain-specific language (DSL) \(\) describing a space of programs, and a set of example inputs \(=\{I_{1},,I_{N}\}\) and corresponding outputs \(=\{O_{1},,O_{N}\}\). The goal is to find a program \(P\) such that \(P(I_{i})=O_{i}\) for all \(i\{1,,N\}\). The DSL \(\) describes atomic values (constants and input variables) and operations that can be applied to arguments to produce new values. Programs in \(\) are arbitrarily-nested compositions of operations applied to atomic values or other such compositions.

\(\)-CalculusThe lambda calculus [10; 28] is a formalism for universal computation. A lambda calculus _term_ is either a variable, function application, or a lambda abstraction. Lambda abstractionsdefine new functions by introducing new lexically-scoped variables, as well as a function body (which is itself a term). We consider lambda abstractions that are allowed to introduce multiple variables at once (we do not "Curry" our lambdas). Terms in the lambda calculus are constructed recursively from smaller terms, resulting in tree-like structures like in Figure 3(a). We extend \(\)-calculus with domain-specific primitives (\(\), \(\), \(\), filter, etc.) as well as a basic type system known as simply-typed lambda calculus; see  for details.

Property SignaturesDuring a neural program synthesis search, many different expressions are encountered, possibly including lambda functions with arbitrary input and output types. How might a neural model learn to reason about these expressions? One option is to represent expressions as source code and apply standard text encoders like recurrent neural networks or Transformers. However, two expressions with similar source code might have very different execution behavior, or two syntactically distinct expressions might have identical semantics. An alternative approach that is more semantically aware is based on _property signatures_, inspired by property-based testing in the programming languages literature . Unary properties describe a single value by mapping it to a boolean, such as whether a list is sorted, or whether a string is empty. Binary properties can describe the relationship between the input and output of a PBE task, such as whether the input and output lists have the same length. In either case, given a list of \(k\) property functions, we can evaluate all property functions to form a vector of length \(k\) called a _property signature_. Each element of the signature is either True, False, or N/A.2 This vector may be used as input to a deep neural network. Furthermore, given multiple executions of an expression (e.g., over different examples), we can identify how often each property holds, leading to a more granular representation.

CrossBeamOur work LambdaBeam builds upon the prior work CrossBeam. Both systems have a similar overall structure, illustrated in Figure 1 with differences shown in red. The core

Figure 1: Overview of LambdaBeam which builds upon the prior work CrossBeam (new elements in LambdaBeam shown in red).

Figure 2: Diagram of the LambdaBeam model architecture, closely mirroring that of CrossBeam.

idea in CrossBeam is to use a neural policy network to guide a bottom-up search over programs, where execution results of explored expressions provide a powerful signal for the policy to guide the search by exploring new expressions whose values are closer to the intended output.

In CrossBeam, a table stores the expressions explored so far in search, along with their execution values when run on the I/O examples. As diagrammed in Figure 2, the Value Module encodes each explored value into a vector that is stored in a matrix \(E_{S}\). Meanwhile, the I/O Module encodes the I/O examples into an analogous vector \(_{IO}\). Then, the Search Context Summary Module combines the value encodings \(E_{S}\) and the I/O encoding \(_{IO}\) to produce a latent "summary" \(_{c}\) of the search context so far. From this, the Argument Selector Module, which is a recurrent pointer network , predicts an argument list for a DSL operation via a sequence of pointers into the table of explored values, thus generating a new expression to explore. By structuring the search in this way, the neural policy is able to take a "hands on" role during search, using information in the search context to choose which programs should be explored next.

The network is trained on-policy using imitation learning. Each item in the training set consists of a set of I/O examples and a target program. During training, we run the search algorithm on the I/O examples where at each step the policy network proposes values to explore by predicting argument lists for DSL operations. We then apply a softmax loss that encourages the policy network to make predictions that would lead to progress according to the target program, instead of other argument lists that the policy network actually proposed.

During evaluation, getting argument lists via beam search as in training can lead to the search stalling if all new values are semantically equivalent to values already seen, since beam search is deterministic if the value set is unchanged. To address this, during evaluation CrossBeam randomly samples different argument lists using UniqueRandomizer  to avoid duplicate samples.

## 3 The LambdaBeam Approach

### Building \(\)-Terms

LambdaBeam constructs terms in the lambda calculus during its search. A natural design choice is to construct terms in a way that avoids considering semantically equivalent expressions. For example, the terms \(( x,y.\ x-y)\), \(( a,b.\ a-b)\), and \(( y,x.\ y-x)\) are all capable of expressing exactly the same computations, so there should be a single canonical way of building this family of terms.

An important aspect of our canonicalization of semantically equivalent expressions is to enforce that _every term constructed during search has no free variables_ (but terms may include variables referring to the inputs given in the I/O examples). Enforcing this property means that we can treat every term we build during search as an ordinary program, and run it on different inputs to probe its input-output behavior. However, the most straightforward way of building lambda terms bottom-up violates this important property. Consider the term \(_{5}=( v_{1}.\ ( u.\ v_{1}+u^{2}, (x)))\) whose tree structure is illustrated in Figure 3(a), and where \(x\) refers to an input variable. This has subterms such as \( u.\ v_{1}+u^{2}\), where the variable \(v_{1}\) occurs free. As \(v_{1}\) is free, it is unclear what the semantics of this expression should be. Constructing the expression with \(v_{1}\) free would also introduce a spurious redundancy with \( v_{1},u.\ v_{1}+u^{2}\). During search, we would like to keep only a canonical version of

Figure 3: Different ways of representing the lambda expression \( v_{1}.\ ( u.\ v_{1}+u^{2},(x))\), where \(x\) is an input variable. The model predicts \(}}\) term pointers and variables, the search tries every \(}}\) operation, and unshaded code tokens in (b) are known from the other shaded tokens.

those two terms to better prune the search space, which in this case would be \(_{3}= v_{1},v_{2}\). \(v_{1}+v_{2}^{2}\), which as shown in Figure 3(b) can be used to construct the larger term \(_{5}\).

To build terms bottom-up while maintaining desired constraints such as that every intermediate program has no free variables, we define an operator for algebraically combining smaller terms to build larger programs. This operator, which we call Merge, takes as input a primitive DSL function \(f\) and a list of terms \(a_{1} a_{K}\) constructed earlier during search, and it returns a new term that applies \(f\) to \(a_{1} a_{K}\). Merge does extra bookkeeping to ensure that every function (both \(f\) and any \(a_{k}\) that are lambda terms) is called with the correct number of arguments, and that the final output of Merge has no free variables. The function \(f\) can be a higher-order function, such as \(\), or a normal first-order function, such as addition and subtraction.

Additional arguments to Merge specify how to unify lambda arguments (if any) that appear in \(a_{1} a_{K}\). To evaluate Merge on \(f\) and \(a_{1} a_{K}\), we first apply each lambda argument \(a_{k}\) to a tuple of variable names, denoted \(i_{k}\). This gives a name to each argument of each \(a_{k}\). By reusing the same name across different \(a_{k}\)'s, the same variable can be shared across arguments. For instance, if the function \(f\) is multiplication (\(\)), and we are merging with the arguments \(a_{1}=( v.\ v+1)\) and \(a_{2}=( v.\ v-1)\), then we can share the variable by setting \(i_{1}=i_{2}=[v_{1}]\), giving \(( v.\ v+1)(v_{1})( v.\ v-1)(v_{1})=(v_{1}+1)(v_{1}-1)\). Alternatively, if we set \(i_{1}=[v_{1}]\) and \(i_{2}=[v_{2}]\), then merging gives a different program, \((v_{1}+1)(v_{2}-1)\). These tuples of variable names, \(\{i_{k}\}_{k=1}^{K}\), are also input to Merge, because they determine how variable names are unified across arguments. Finally Merge runs \(f\) on the arguments \(\{a_{k}(i_{k})\}_{k=1}^{K}\), pads the expression with lambdas at the beginning to bind any new free variables, and lastly canonicalizes variable naming and variable ordering.

Special care is needed for the arguments of higher-order functions like \(\), which themselves need to be functions. So far, each argument \(a_{k}(i_{k})\) evaluates to a concrete value, not a function. To handle higher-order functions, Merge automatically adds extra lambdas to each function-valued argument. These extra lambdas have variables denoted \(u_{1},u_{2},\). All other variables used to unify variables across arguments are denoted \(v_{1},v_{2},\). Although this may seem ad-hoc at first, this convention actually corresponds to a well-known canonicalization of typed lambda forms known as \(\)-long normal form . Putting everything together, Merge is defined as

\[(f,a_{1},i_{1},a_{2},i_{2},)=  v_{1}v_{2} f( u_{1}u_{2}...u_{_{1}}.\ a_{1}(i_{1}),  u_{1}u_{2}...u_{_{2}}.\ a_{2}(i_{2}),)\] \[\{v_{1},v_{2},\}= _{k}i_{k}-\{u_{j}\}_{j=1}^{\{_{1}, _{2},\}}\] (1)

where \(_{k}\) is the arity of the \(k^{}\) argument to \(f\). For example, the higher-order function \(\) first takes a function expecting one argument, so \(_{1}=1\), followed by a list (not a function) expecting no arguments, so \(_{2}=0\). We also enforce that \(|i_{k}|=(a_{k})\). The Merge operation is complete, in the sense that we can use it to generate any PBE solution within the DSL (see Appendix A).

Fundamentally, our neural model predicts the inputs to Merge. It scans through its different operators (functions \(f\)), and for each operator, generates arguments \(a_{k}\) by pointing into the set of explored values, and variables \(i_{k}\) by emitting special tokens corresponding to \(v_{1}\), \(u_{1}\), \(v_{2}\), \(u_{2}\), etc. Figure 3(b) and (c) illustrate how a nontrivial lambda expression is built step by step, with the tokens emitted by the neural model highlighted in blue. In this figure, each call to Merge in the third column returns the term that appears in the middle column, e.g., Merge\((},}_{1},[\,])\) returns \( v_{1}\). \(}(}_{1})\) and so on. Critically, Merge makes sure that (1) every intermediate term that the model builds along the way can be evaluated so that the neural model can inspect its progress, and also (2) intermediate terms are generated with _every_ function application, giving fine-grained feedback to the model.

We define the _weight_ of an expression to be the number of nodes in its tree representation using the Merge operator. More specifically, atomic terms like variable names and literal constants have weight \(1\), and a term constructed with Merge has weight \(1\) (for the operation) plus the sum of the weights of all terms and variables in the Merge call. For example, in Figure 3(b), terms \(_{1}\) through \(_{5}\) have weights \(1\), \(2\), \(5\), \(2\), and \(10\) respectively.

### Learning over \(\)-Expressions

One core technical challenge is how to encode lambda expressions to allow neural models to reason about them. In LambdaBeam we solve this by constructing a new generalization of property signatures which is designed to represent lambda expressions and non-lambda expressions using similar sets of properties.

Non-lambda expressions can be evaluated to produce a single result per I/O example. However, we cannot evaluate lambda expressions in the same way, because we do not know how the lambda expression will be used in the eventual solution, so we do not know what its arguments will be. For instance, if \(x\) is an input list, the expressions \(( u_{1},u_{2}.\ u_{1}+u_{2},x,(x))\) and \(( u_{1},u_{2}.\ u_{1}+u_{2},x)\) use the same lambda expression \( u_{1},u_{2}.\ u_{1}+u_{2}\) but for different sets of \((u_{1},u_{2})\) arguments. Thus, in order to describe the lambda expression's execution behavior, we run the lambda on a fixed set of _canonical argument tuples_ based on the number of arguments and their types. These argument tuples are held constant across training and evaluation so that the model can learn from consistent signals.

In our experiments, we hardcoded the canonical argument tuples without changing them afterward, trying to cover common values and a variety of scenarios. For instance, our integer arguments include those between \(-3\) and \(5\) inclusive, and other integers with varying combinations of magnitude, sign, even/odd parity, primality, and so on. There is also a tradeoff in the number of canonical argument tuples, where having more leads to finer-grained execution information but more time spent running lambdas during search. In our experiments, we use 16 canonical argument tuples for each combination of tuple length and argument types in our DSL. Note that the lambda expression can refer to inputs from the I/O examples. Instead of running the lambda on each argument tuple for each example, for efficiency, we run on each argument tuple once, under the context of one I/O example which is changed per argument tuple in a round-robin fashion.

To represent a lambda \(f\), we evaluate it on each canonical argument tuple \(t_{i}\) with I/O example \((I,O)\). First we evaluate \(f\) on \(t_{i}\), yielding a result \(r_{i}=f(t_{i},I)\). Then we use a signature of unary properties to describe \(r_{i}\). Second, we use a signature of binary properties to compare \(r_{i}\) and \(O\); intuitively, this helps the model learn about what work remains. Similarly, we use the binary properties to compare \(r_{i}\) and \(t_{ij}\), for each argument \(t_{ij} t_{i}\). By concatenating these, we obtain a single property vector for each tuple \(t_{i}\). Finally, we then reduce the property vectors across the runs of the lambda, i.e., computing for each property the fraction of runs where it is applicable and the fraction of applicable runs where it is True. Encoding non-lambda expressions is similar, except that we use I/O examples \((I_{i},O_{i})\) in place of the canonical tuples. Note that the reduced property signatures for lambda and non-lambda expressions have different formats and different lengths, and hence they are embedded by separate parts of the neural model (Section3.3).

The properties used in our property signatures come from combinatorially combining hand-designed features as "building blocks" to create a rich space of properties that describe individual objects as well as comparisons between two objects. AppendixB contains more details.

### LambdaBeam Model and Search

To guide the bottom-up search over lambda and non-lambda expressions, we generally follow the design of the neural policy network in CrossBeam, with the following major changes:

Value ModuleWe maintain a set of explored values \(\) which contains variable tokens for constructing lambdas, lambda expressions, and non-lambda expressions including constants and input variables. The Value Module embeds each element of \(\) forming a matrix of embeddings \(E_{}^{|| d}\). Elements of \(\) are embedded as follows. A variable token is embedded as a vector of trainable parameters. Note that the set of such variable tokens is fixed and determined by the DSL.3 A lambda expression is embedded by \(+\), where \(\) is the property signature of this lambda function encoded by an MLP, and \(\) is an embedding of the weight of this value. Non-lambda expressions are embedded like lambda expressions except using a different MLP to encode their property signatures.

Argument Selector ModuleGiven an operator \(op\), we use an operator-specific \(_{op}\) to select the arguments using a pointer mechanism  from the encoded value matrix \(E_{}\), in an autoregressive way. In addition to selecting \((op)\) arguments, for an argument \(a_{k}\) that is a lambda expression, we also need to predict the variables \(i_{k}\) as required for Merge, where \(i_{k}\) is a tuple of \((a_{k})\) variable tokens. All of the \(a_{k}\) and \(i_{k}\) predictions are done as a single autoregressive sequence. Furthermore,for convenience we predict all of the \(a_{k}\) arguments first, followed by the \(i_{k}\) variable tuples which are constrained (via masking and padding) to include exactly \((a_{k})\) variable tokens.

Search with RestartsWe also change the inference time search procedure. Recall that CrossBeam uses random sampling during evaluation, making the search nondeterministic. In LambdaBeam, instead of performing one synthesis search until timeout, we restart the search from scratch whenever the search has run for a certain amount of time without finding a solution. Even though this discards work done in previous searches, in practice this helps LambdaBeam solve more tasks because it may be otherwise difficult to recover from exploring the wrong search direction.

## 4 Experiments

In this section, we experimentally evaluate the effectiveness of LambdaBeam, comparing to prior neural and symbolic approaches in an integer list manipulation domain.

### Integer List Manipulation DSL

To measure a synthesizer's ability to create and use lambda expressions, we create a domain-specific language (DSL) that emphasizes lambda expressions and higher-order functions. Specifically, the DSL from DeepCoder  includes higher-order functions and has been used in subsequent work [37; 34]. However, DeepCoder's DSL only contains a hardcoded set of lambda functions and is not expressive enough to fully exercise LambdaBeam's ability to create arbitrary lambda expressions. Therefore, we extend DeepCoder's DSL by allowing lambda expressions to include arbitrary compositions of DSL operations, and replacing the hardcoded lambda functions with DSL operations and literal constants that enable a superset of the original functionality. For example, DeepCoder's original DSL includes hardcoded lambdas such as \(( x.\ x-1)\), \(( x,y.\ x-y)\), and \(( x,y.\ \{x,y\})\). By introducing first-order functions including subtract and max, and constant literals including \(0\) and \(1\), we can create the hardcoded lambdas as well as lambdas like \(( x.\{x,0-x\})\) that were not possible in the original DeepCoder DSL. Additionally, we add a new if-then-else operation, which further enriches our space of possible programs. The full DSL contains \(23\) first-order operations, \(5\) higher-order operations, and \(6\) integer literals, described fully in Appendix C. In our DSL, all integers are in the range \([-256,255]\) as in DeepCoder, and lists have lengths in the range \(\).

### Experimental Setup

Training DataSimilar to previous works including CrossBeam, we create synthetic training data by generating random tasks within our DSL. This is done by performing exhaustive bottom-up searches starting from random inputs and enumerating programs in order of increasing weight, and then sampling a subset of the resulting programs to serve as training tasks. Each task has between 2 and 5 I/O examples and between 1 and 3 input variables, and we sample tasks such that approximately 80% of them use lambdas in the ground-truth program. We used a time limit of 1 hour per data generation search (reaching programs of weight at most 12), sampling up to 1600 tasks per search, and performing enough searches parallelized across cloud CPU workers such that training uses less than 1 epoch over the dataset. We furthermore removed from the training dataset all programs that would solve any of our 200 evaluation tasks, described below.

Evaluation TasksFor evaluation, we use 100 handwritten evaluation tasks plus 100 synthetically generated tasks, with a time limit of 10 minutes per task. The handwritten tasks include all 9 "example programs" from Appendix A of the DeepCoder paper , plus other tasks that we created from scratch by brainstorming many natural but varied tasks that we could solve using our DSL (near the end of this process, it became quite difficult to come up with new tasks that were not merely slight variations of existing ones). Handwritten tasks include between 1 and 3 input variables, 3 I/O examples if the output is a list or 5 I/O examples if the output is an integer, and a handwritten ground-truth solution that has minimal weight to our knowledge. When creating I/O examples, we aimed to make the examples informative but reasonably succinct with lists of length at most 10. Every DSL operation is used in the solution for at least 4 handwritten tasks, and every higher-order operation is used in at least 10. For the 100 synthetic tasks, we sampled distinct random programs using the same procedure as for generating training data, except also enforcing that we have exactly 10 tasks of each weight between 3 and 12 inclusive. Appendix D contains example handwritten and synthetic tasks, along with LambdaBeam's solutions for them.

ApproachesOur experiments compare several approaches:

1. LambdaBeam with random restarts: We trained the LambdaBeam model using on-policy training as in CrossBeam. The model has about 13 million trainable parameters and was trained on about 6.5 million tasks, which took about a week of training using 8 V100 GPUs. See Appendix E for more details on the model architecture and training. During evaluation, we use only 1 V100 GPU and perform random restarts every 6 seconds on the handwritten tasks, or every 30 seconds on the synthetic tasks, both chosen from a coarse search over restart frequencies. We run this approach for 5 trials using different randomness for the UniqueRandomizer sampling method carried over from CrossBeam.
2. LambdaBeam without random restarts: We use the LambdaBeam approach without random restarts as an ablation, also for 5 trials with different randomness for UniqueRandomizer sampling.
3. Enumeration: We run the same exhaustive bottom-up enumerative synthesis algorithm that was used to create the training data. We use 5 trials with different random orderings of DSL operations, which changes the enumeration order of programs with the same weight.
4. RobustFill : This approach treats the synthesis problem as a sequence to sequence prediction task from the I/O examples to the program tokens. Specifically, we train a plain 3-layer LSTM-based encoder-decoder model on our synthetic training dataset, using approximately the same number of trainable parameters and training tasks as for the LambdaBeam model. We get model predictions via a single beam search of size 65536 which nearly exhausts the GPU memory and evaluate all resulting programs on the I/O examples. Since the beam search is deterministic, we perform 5 trials by re-training the model with different initializations.
5. \(^{2}\): This is a state-of-the-art symbolic program synthesizer that handles lambda functions and higher-order functions. We implemented our DSL within the \(^{2}\) framework (using the more extensible version provided by the \(^{2}\) authors). \(^{2}\) is deterministic so we only use 1 trial.
6. Python-Finetuned LLM: We try asking a pretrained large language model (LLM) to solve our evaluation tasks using Python code. Specifically, we use PaLM 62B that was trained for longer as described in Appendix F of Chowdhery et al. , with further fine-tuning on general Python code. The prompt contains natural language instructions and 2 examples of an I/O specification followed by Python code that solves the task (for 2 new tasks), and then the I/O specification of the evaluation task we wish to solve.4 We repeatedly draw batches of 16 independent samples with temperature sampling and run those programs on the I/O examples, until a solution is found or timeout is reached. We ran the LLM using 16 accelerators so this approach uses significantly more compute than the others over the same time limit. We repeat for 3 trials with different randomness for temperature sampling. 
### Results

Figure 4 plots the synthesis performance of the various methods over time. Notably, LambdaBeam with restarts is the best approach for both handwritten and synthetic tasks. The gap is wider on the handwritten tasks where LambdaBeam with restarts solves \(67.2\) out of \(100\) tasks on average, which is \(24\%\) more tasks than the next best method \(^{2}\). Figure 5 plots the various approaches' success rates for different task weights, which can be used as a measure of task difficulty. As expected, we observe that all methods perform worse on harder tasks with larger weight, but that LambdaBeam with restarts generally achieves higher success rates on the difficult tasks compared to other methods. This means that our approach scales better to larger programs compared to the other methods, except the LLM which has a more constant but lower success rate overall.5

Running LambdaBeam with random restarts helps overall but more so for the handwritten tasks. We believe this is because the synthetic evaluation tasks have the same distribution as the training tasks while the handwritten tasks are different. So, for the handwritten tasks, exploring the wrong part of the search space early on might cause further mistakes that lead the search astray, while the model may be better trained to stay on track for the synthetic tasks. This would also explain why more frequent restarts are effective for the handwritten tasks. We note that random restarts would not be possible for \(^{2}\), enumeration, or RobustFill's beam search, and would not help the LLM where each sample is already independent.

We also identify _false positives_ by running solutions on 2 held-out test cases per task, generated mostly synthetically with some manual editing. The results are in Figure 6, showing that LambdaBeam with restarts has the highest number of true positive solutions on handwritten tasks by a margin of nearly 8 tasks, while barely losing to Enumeration on synthetic tasks.6 While symbolic approaches (Enumeration and \(^{2}\)) have fewer false positives due to focusing on small solutions, we observe that LambdaBeam has the fewest false positives among the neural approaches. The LLM produces many false positives on the synthetic tasks where the ground-truth solutions are less similar to programs seen during its training, and in fact many of its false positive solutions are if-elif-else chains that hardcode the examples in some way (which is feasible to implement in Python but not in our DSL). Finally, we note that some false positive solutions could be transformed into true positives with a postprocessing step, e.g., one that attempts to simplify or minimize subtrees of the solution. In this sense, false positive solutions may still be useful for synthesis, and LambdaBeam with restarts achieves the highest number of total positive solutions by a wide margin.

Appendix F contains analysis showing some of the differences in distributions between the handwritten and synthetic evaluation tasks, which helps to contextualize the experimental results. For example, lambda expressions are used in 85% of the handwritten tasks but only 53% of the synthetic tasks. The median task weight is 9 for handwritten tasks and only 7.5 for synthetic tasks. These comparisons suggest that the handwritten tasks are harder than the synthetic tasks on average, which

Figure 4: Synthesis results over time for various methods on the handwritten and synthetic evaluation tasks. Shaded areas represent the minimum and maximum across trials for that method.

Figure 5: Success rates of various methods broken down by the task weight (i.e., the smallest known weight of a solution). Task weights are bucketed such that each group contains at least \(15\) tasks.

is also reflected in the overall performance in Figure 4. We observe that LambdaBeam achieves a greater performance gap over the other approaches on the handwritten tasks versus on the synthetic tasks, which is a promising trend because the handwritten tasks are both harder and more natural.

Although LambdaBeam resolves CrossBeam's limitations of not handling lambdas or looping computations, some other limitations are carried over. On-policy training is slow due to performing search during training, but this could be addressed with an initial training phase of off-policy teacher forcing. At evaluation time, even with UniqueRandomizer sampling to avoid duplicate argument lists within one sampling phase, our approach still encounters many duplicate values across sampling phases and across restarts. Finally, our DSL is small compared to general programming languages.

## 5 Related Work

Machine learning for program synthesis has been an active area [17; 18; 1]. Within programming by example, deep learning architectures for sequences, like LSTMs and Transformers, have been particularly effective . Our work builds upon CrossBeam, which itself combines three lines of research in program synthesis. The first are learned search strategies for program synthesis, that is, using a learned policy or value function to guide search [36; 20; 13], or multi-level strategies that combine the results of search over different spaces [25; 22; 19; 34]. The second are _execution-guided neural synthesis_ methods, which guide the search over partial programs by evaluating them [37; 13; 7; 27; 8]. Finally, CrossBeam's use of imitation learning to train the policy is inspired by work in learning to search [11; 29; 5] and beam-aware training [23; 24].

In contrast, we are unaware of previous work that synthesizes helper functions, such as lambda functions, during neural program synthesis. The original DeepCoder DSL contains only a small set of predefined lambda functions. Even within symbolic program synthesis, \(^{2}\) is one of the few examples of work that synthesizes lambda expressions . To control the size of the search space, \(^{2}\) employs type-directed synthesis, but we handle more general domains where the type system is not informative enough to reduce the search space sufficiently. DreamCoder  can also infer ad-hoc helper functions like \(^{2}\), but its neural network provides no fine-grained guidance on how to compose those lambdas. Because DreamCoder is an algorithm for enriching an impoverished DSL to improve a neurally-guided program search, one could combine DreamCoder's DSL enrichment process with LambdaBeam's search strategy. Other work reuses fragments of code from partially-correct solutions [30; 34], but these are executable portions of straightline code, not lambda functions.

Our integer manipulation domain is inspired by DeepCoder  and subsequent work [37; 34].

## 6 Conclusion

We introduced the first neural search method for programming by example that is able to synthesize intermediate helper functions (lambdas) by resolving two key difficulties. First, we algebraically represent lambda expressions in a canonical way and construct new lambdas with the Merge operator that enforces desirable representational constraints. Second, we encode arbitrary lambda functions as inputs to a neural network by using property signatures to analyze the lambda's execution semantics. With these innovations, LambdaBeam learns a neural policy to drive a bottom-up search over programs. We experimentally show that LambdaBeam outperforms symbolic search, a sequence model, and a pretrained code LLM with 62 billion parameters.

Figure 6: True positive versus false positive solutions measured using held-out test cases.

#### Acknowledgments

The authors would like to thank Henryk Michalewski for his thoughtful ideas, and Christian Walder, Rif Saurous, and the anonymous reviewers for their helpful comments.