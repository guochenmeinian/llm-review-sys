# Stable Minima Cannot Overfit in Univariate ReLU Networks: Generalization by Large Step Sizes

Dan Qiao

CSE, UC San Diego

d2qiao@ucsd.edu

&Kaiqi Zhang

CS, UC Santa Barbara

kzhang70@ucsb.edu

&Esha Singh

CSE, UC San Diego

e3singh@ucsd.edu

&Daniel Soudry

Technion - Israel Institute of Technology

daniel.soudry@gmail.com

&Yu-Xiang Wang

Halcioglu Data Science Institute, UC San Diego

yuxiangw@ucsd.edu

###### Abstract

We study the generalization of two-layer ReLU neural networks in a univariate nonparametric regression problem with noisy labels. This is a problem where kernels (_e.g._ NTK) are provably sub-optimal and benign overfitting does not happen, thus disqualifying existing theory for interpolating (0-loss, global optimal) solutions. We present a new theory of generalization for local minima that gradient descent with a constant learning rate can _stably_ converge to. We show that gradient descent with a fixed learning rate \(\) can only find local minima that represent smooth functions with a certain weighted _first order total variation_ bounded by \(1/-1/2+(+})\) where \(\) is the label noise level, \(\) is short for mean squared error against the ground truth, and \(()\) hides a logarithmic factor. Under mild assumptions, we also prove a nearly-optimal MSE bound of \((n^{-4/5})\) within the strict interior of the support of the \(n\) data points. Our theoretical results are validated by extensive simulation that demonstrates large learning rate training induces sparse linear spline fits. To the best of our knowledge, we are the first to obtain generalization bound via minima stability in the non-interpolation case and the first to show ReLU NNs without regularization can achieve near-optimal rates in nonparametric regression.

## 1 Introduction

How do gradient descent-trained neural networks work? It is an intriguing question that depends on model architecture, data distribution, and optimization algorithms used for training (Zhang et al., 2021). Specifically, in the overparameterized regime with specific random initialization of the weights, it was shown that gradient descent finds global optimal (0-loss or interpolating) solutions despite the non-convex objective function (Jacot et al., 2018; Du et al., 2018; Liu et al., 2022). It was also shown that among the (many) global optimal solutions, the particular solutions that are selected by gradient descent often do not overfit despite having 0 training error (Chizat et al., 2019; Arora et al., 2019; Mei et al., 2019), sometimes even if the data is noisy -- a phenomenon known as "benign overfitting" (e.g., Belkin et al., 2019; Bartlett et al., 2020; Frei et al., 2022).

Figure 1: We show that “Large step size selects simple functions that generalize.”What is less well-known is that interpolating solutions do overfit for ReLU neural networks (ranging from tempered to catastrophic) (Mallinar et al., 2022; Joshi et al., 2023; Haas et al., 2023) and the generalization bounds in the kernel regime are provably suboptimal for certain univariate nonparametric regression problems (Suzuki, 2018; Zhang and Wang, 2022). These "exceptions" significantly limit the abilities of the kernel theory or "benign overfitting" theory in predicting the performance of an overparameterized neural network in practice.

In many learning problems with noisy data, the best solutions are simply not among those that interpolate the data. For example, no interpolating solutions can be _consistent_ in a _fixed-design_ nonparametric regression problem. Even if interpolating solutions that satisfy benign overfitting can be found, they could be undesirable due to their "spikiness" (Haas et al., 2023) and lack of robustness (Hao and Zhang, 2024). In addition, it was reported that when the label is noisy, it takes much longer for gradient descent to overfit (Zhang et al., 2021). Most practical NN training would have entered the _Edge-of-Stability_ regime (Cohen et al., 2020) or stopped before the interpolation regime kicks in.

These observations motivate us to come up with an alternative theory for gradient-descent training of overparameterized neural networks that do not require interpolation.

### Summary of Contributions

In this paper, we present a new theory of generalization for solutions that gradient descent (GD) with a fixed learning rate can _stably_ converge to. Specifically:

1. We show that for 1D nonparametric regression (\(n\) data points with noisy labels), the solutions that GD can stably converge to _must be_ regular functions with small (weighted) first-order _total variation_ (Theorem 4.1 and Corollary 4.2), thus promoting sparsity in the number of linear pieces. This generalizes the result of Mulayoff et al. (2021) by removing the "interpolation" assumption. We also show that in the noisy case, there is no "flat" interpolating solution and gradient descent cannot converge to them unless the learning rate is \(O(1/n^{2})\) no matter how overparameterized the two-layer ReLU network is (Theorem 3.1).
2. We show that such solutions (stable local minima that GD converges to) _cannot overfit_, in the sense that the _generalization gap_ vanishes as \(n\) inside the strict interior of the data support (Theorem 4.3). Moreover, under a mild additional assumption on gradient descent finding solutions with training loss smaller than \(^{2}\), we prove that these solutions achieve _near-optimal_ rate for estimating (the strict interior of) _first-order bounded variation functions_ (Theorem 4.4) -- provably faster than any kernel ridge regression estimators, including neural networks in the "kernel" regime.
3. We conduct extensive numerical experiments to demonstrate our theoretical predictions, validate our technical assumptions, and illustrate the functional form of the ReLU NNs as well as the

Figure 2: Empirical evidence of our claim. Constant step size gradient descent-trained two-layer ReLU neural networks generalize because of minima stability. The **left panel** shows that with increasing step size, gradient descent finds smoother solutions (linear splines) with a smaller number of knots. The **middle panel** illustrates our theoretical result with a _numerically accurate_ upper bound using \(1/+O(1)\) of the curvature and TV1-complexity of the smooth solution. The **right panel** shows that tuning \(\) gives the classical U-shape bias-variance tradeoff for overparameterized NN.

learned basis functions that gradient descent finds with different step sizes. These results reveal new insights into how gradient descent training aggressively learns representation and induces implicit sparsity.

To the best of our knowledge, these results are new to this paper. We emphasize that (1) the training objective function is not explicitly regularized; (2) we do not early-stop training in favor of algorithmic-stability; and (3) the solutions that gradient descent converges to are not global optimal (interpolating) solutions unless the label noise is \(0\). Our approach is new in that we directly analyze the complexity of a superset of solutions that gradient descent can stably converge to, which enables us to prove end-to-end generalization bounds that are near-optimal in nonparametric regression tasks.

Our analysis for gradient descent-training is categorically different from those in the "kernel" (a.k.a. "lazy") regime about interpolating solutions. Instead, we rigorously prove (and empirically demonstrate) that large step-size gradient descent do not behave this way and it does not converge to interpolating solutions. Our results fall into the non-kernel regime of neural network learning known as the "rich" (a.k.a. the "feature learning" or "sparse") regime (Chizat et al., 2019; Woodworth et al., 2020), in which the weights and biases can move arbitrarily far away from their initialization.

**Technical novelty.** The main technical innovation in our analysis is in handling the \(_{i}(y_{i}-f_{}(x_{i}))_{}^{2}f_{}(x_{ i})\) term that arises in the minima stability analysis when it was previously handled by Mulayoff et al. (2021) using interpolation, i.e., \(y_{i}=f(x_{i})\ \ i[n]\). It turns out in the noisy-label case, we can decompose the term into a certain Gaussian complexity measure and a self-bounding style MSE of that \(f_{}\). A non-trivial step is to bound the largest eigenvalue of \(^{2}f_{}()\) by a constant which results in a uniform bound of both the Gaussian complexity and the MSE. These bounds themselves are vacuous in terms of the implied generalization error, but plugging them into the function-space constraint imposed by the noisy minima stability bound restricts the learned ReLU NN to be inside a weighted TV1 function class (Details in Theorem 4.1). This, in turn, allows us to _amplify_ a vacuous MSE bound into a new MSE bound that is nearly optimal in the strict interior of the data support. The main technique in the last step involves bounding the metric entropy of the weighted TV1 class and then carefully working out a self-bounding (square loss) version of the Dudley's chaining argument (Wainwright, 2019).

**Disclaimers and limitations.** It is important to note that while we analyze gradient descent training of overparameterized neural networks, the computational claim is very different from those in the kernel regime. The analysis in the kernel regime ensures that gradient descent finds interpolating solutions efficiently. We do not have a comparable efficiency claim. While computational guarantees on stationary point (and local minima) convergence in non-convex optimization problems are well-understood (Ghadimi and Lan, 2013; Jin et al., 2017), we do not have guarantees on whether the solution that gradient descent finds satisfies our assumption on the training loss being "optimized" (smaller than label noise \(^{2}\)). Instead, our results provide a generalization gap bound for any stable solutions (Theorem 4.3) and a near-optimal excess risk (MSE vs ground truth) bound (Theorem 4.4) when the solution that GD finds happens to satisfy the assumption (empirically it does!). This is a meaningful middle ground between classical learning theory which does not concern optimization at all and modern theory that is fully optimization-dependent.

While we focus on (full batch) gradient descent for a clean presentation, the minima stability for stochastic gradient descent is immediate under the stochastic definition of minima stability (Mulayoff et al., 2021). The same reason applies to us focusing on univariate nonparametric regression. Our technique can be used to generalize the multivariate function space interpretation of minima stability from Nacson et al. (2022) to the noisy case, but it will take substantial effort to formalize the corresponding generalization bounds in the multivariate case, which we leave as a future work.

### Related Work and Implications of Our Results

**Generalization in Overparameterized NNs, Interpolation, and Benign Overfitting.** Most existing theoretical work on understanding the generalization of overparameterized neural networks focuses on the interpolation regime (Cao and Gu, 2019; Frei et al., 2022; Kou et al., 2023; Buzaglo et al., 2024). Handling label noise either requires explicit regularization (Hu et al., 2020; Zhang and Wang, 2022), algorithmic stability through early stopping (Hardt et al., 2016; Richards and Kuzborskij, 2021), or carefully crafted data distribution that leads to a phenomenon known as benign overfitting (Bartlett et al., 2020; Frei et al., 2022; Kou et al., 2023). Benign overfitting could happen for nonparametricregression tasks (Belkin et al., 2019), but there is well-documented empirical and theoretical evidence that benign overfitting does not occur for regression tasks with ReLU activation (Mallinar et al., 2022; Haas et al., 2023; Joshi et al., 2023) and that the excess risk is required to be proportional to the standard deviation of the label noise (Kornowski et al., 2024). We are the first to go beyond the interpolation regime and show that gradient descent-trained neural networks generalize in noisy regression tasks without explicit regularization.

**Implicit bias of gradient descent.** The implicit bias of gradient descent training of overparameterized NN is well-studied. It was shown that among the many globally optimal (interpolating) solutions, gradient descent finds the ones with the smallest norm in certain Hilbert spaces (Arora et al., 2019; Mei et al., 2019), classifiers with largest-margin (Chizat and Bach, 2020), or the smoothest cubic spline interpolation (Jin and Montifar, 2023). None of these results, however, imply generalization bounds when the labels are noisy. Interestingly, our results show that gradient descent with a large step size induces an implicit bias that resembles sparse L1-regularization rather than the dense L2 regularization from gradient flow (Jin and Montifar, 2023).

**Implicit bias of minima stability.** The closest to our work is the line of work on the implicit bias of minima stability (Ma and Ying, 2021; Mulayoff et al., 2021; Wu and Su, 2023). We build directly on top of the function-space interpretation of minima stability established by Mulayoff et al. (2021). However, these works critically rely on the minima interpolating the data, which makes their results inapplicable to settings with label noise. Mulayoff et al. (2021) also did not establish formal generalization bounds. Ma and Ying (2021); Wu and Su (2023) do have generalization bounds, but (again) their results require interpolation and thus do not apply to our settings.

**Flat/Sharp Minima and generalization.** Our work is also connected to the body of work on the hypothesis that "flat local minima generalize better". Despite compelling empirical evidence (Hochreiter and Schmidhuber, 1997; Keskar et al., 2017), rigorous theoretical understanding of this hypothesis is still lacking (see, e.g., Wu and Su, 2023, and the references therein). Our work contributes to this literature by formally proving that the hypothesis is real for two-layer ReLU NNs in a noisy regression task.

**Edge-of-Stability and Catapults.** Empirical observations on how large learning rate training of NN finds solutions with Hessian's largest eigenvalue dancing around \(2/\), i.e., "edge of stability" regime (Cohen et al., 2020); and that the loss may go up first before going down to a good solution ("catapult") (Lewkowycz et al., 2020). Existing theoretical understanding of these curious behaviors of GD training is still limited to toy-scale settings (e.g., Arora et al. (2022); Ahn et al. (2023); Kreisler et al. (2023)). Our work is complementary in that we provide generalization bound to the final solution GD stabilizes on no matter how GD gets there. Outside the context of GD and neural networks, "edge of stability" and the implicit bias of large step-size were observed for forward stagewise regression (see, e.g. Tibshirani, 2015, 2014, Section 4.4 and Page 42) albeit only empirically. Our results may provide a theoretical handle in formally analyzing these observations.

**Optimal rates of NNs in nonparametric regression.** Finally, it was previously known that neural networks can achieve optimal rates for estimating TV1 functions (Suzuki, 2018; Liu et al., 2021; Parhi and Nowak, 2021; Zhang and Wang, 2022). Specifically, Savarese et al. (2019); Ongie et al. (2020); Parhi and Nowak (2021); Zhang and Wang (2022) show that weight decay in ReLU networks is connected to total variation regularization. However, these works assume one can solve an appropriately constrained or regularized empirical risk minimization problem. Our work is the first to show that the optimal rate is achievable with gradient descent without weight decay. In fact, it was a pleasant surprise to us that both weight decay and large learning rate induce total variation-like implicit regularization in the function space.

## 2 Notations and Problem Setup

Let us set up the problem formally. Throughout the paper, we use \(O(),()\) to absorb constants while \(()\) suppresses logarithmic factors. Meanwhile, \([n]=\{1,2,,n\}\).

**Two-layer neural network.** We consider two-layer (_i.e._ one-hidden-layer) univariate ReLU networks,

\[=\{f:\ \ f(x)=_{i=1}^{k}w_{i}^{(2)} (w_{i}^{(1)}x+b_{i}^{(1)})+b^{(2)}\},\] (1)where the network consists of \(k\) hidden neurons and \(()\) denotes the ReLU activation function.

**Training data and loss function.** The training dataset is denoted by \(=\{(x_{i},y_{i}),i[n]\}\). \(\{x_{i}\}_{i=1}^{n}\) is assumed to be supported by \([-x_{},x_{}]\) for some constant \(x_{}>0\). We focus on regression problems with square loss \((f,(x,y))=(f(x)-y)^{2}\). The training loss is defined as \((f)=_{i=1}^{n}(f(x_{i})-y_{i})^{2}\). Notice that \(f\) is parameterized by \(:=[w_{1:k}^{(1)},b_{1:k}^{(1)},w_{1:k}^{(2)},b^{(2)}]^{3k+1}\). As a short hand, we define \(_{i}():=(f_{},(x_{i},y_{i}))\) and \(():=_{i[n]}_{i}()\).

**Gradient descent.** We focus on the Gradient descent (GD) learner, which iteratively updates \(\):

\[_{t+1}=_{t}-(_{t}),\ t 0,\] (2)

where \(>0\) is the step size (a.k.a. learning rate) and \(_{0}\) is the initial parameter. Detailed calculation of gradient for two-layer ReLU networks is deferred to Appendix E. Below we define stability for local minima and discuss the conditions for a minimum to be stable.

**Twice differentiable stable local minima.** Similar to Mulayoff et al. (2021), we consider twice differentiable minima. According to Taylor's expansion around a twice differentiable minimum \(^{}\),

\[()(^{})+(-^{} )^{T}(^{})+(-^{})^{T} ^{2}(^{})(-^{}),\] (3)

where \(^{2}\) denotes the Hessian matrix and \((^{})=0\). Therefore, as \(_{t}\) gets close to \(^{}\), the update rule for GD (2) can be approximated as \(_{t+1}_{t}-((^{})+ ^{2}(^{})(_{t}-^{}))\). Such approximation motivates the definition of linear stability, which is first stated in Wu et al. (2018).

**Definition 2.1** (Linear stability).: With the update rule \(_{t+1}=_{t}-((^{})+^{ 2}(^{})(_{t}-^{}))\), a twice differentiable local minimum \(^{}\) of \(\) is said to be \(\) linearly stable if for any \(_{0}\) in the \(\)-ball \(_{}(^{})\), it holds that \(_{t}\|_{t}-^{}\|\).

Note that different from previous works (Wu et al., 2018; Mulayoff et al., 2021), we remove the expectation before \(\|_{t}-^{}\|\) since under GD everything is deterministic. Intuitively speaking, linear stability requires that once we have arrived at a distance of \(\) from \(^{}\), we end up staying in the \(\)-ball \(_{}(^{})\). It is known that linear stability is connected to the flatness of the local minima.

**Lemma 2.2**.: _Consider the update rule in Definition 2.1, for any \(>0\), a local minimum \(^{}\) is an \(\) linearly stable minimum of \(\) if and only if \(_{}(^{2}(^{}))\)._

The implication is that the set of stable minima is equivalent to the set of flat local minima whose largest eigenvalue of Hessian is smaller than \(2/\). The proof is adapted from Mulayoff et al. (2021) and we state the proof in Appendix C for completeness. When the result does not depend on \(\) (as above), we simply say "linearly stable". Throughout the paper, we overload the notation by calling a function \(f=f_{}\) linearly stable function if \(\) is linearly stable.

**"Edge of Stability" regime.** Extensive empirical and theoretical evidence (Cohen et al. (2020), Damian et al. (2024), and see Section 1.2) have shown that the threshold of linear stability (from Lemma 2.2) is quite significant in GD dynamics: GD iterations initially tend to exhibit "progressive sharpening", where \(_{}(^{2}(_{t}))\) is increasing, until finally GD reaches the "Edge of Stability", where \(_{}(^{2}(_{t})) 2/\). We capture this phenomenon with the following definition.

**Definition 2.3** (Below Edge of Stability).: We say that a sequence of parameters \(\{_{t}\}_{t=1,2,}\) generated by gradient descent with step-size \(\) is \(\)-approximately Below-Edge-of-Stability (BEoS) for \(>0\) if there exists \(t^{*}>0\) such that \(_{}(^{2}(_{t}))}{}\) for all \(t t^{*}\). Any \(_{t}\) with \(t t^{*}\) is referred to as an \(\)-BEoS solution.

The BEoS regime provides a strong justification for the connection between the step size \(\) and the largest eigenvalue of the Hessian matrix. It holds for all twice-differentiable solutions GD finds along the way -- even if the GD does not converge to a (local or global) minimum. Empirically, BEoS is valid for both the "progressive sharpening" phase and the oscillating EoS phase for a small constant \(\).

Our goal in this paper is to understand generalization for both (twice differentiable) stable local minima (Definition 2.1) and any other solutions satisfying \(\)-BEoS (Definition 2.3), which are both subsets of

\[(,,):=\{f_{}\ |\ _{}(^{2}())}{} \}.\] (4)To simplify the presentation, we focus on the case with \(=0\) w.l.o.g.1 and unless otherwise specified, a _"stable solution"_ refers to an element of \((,0,)\) in the remainder of the paper.

For the data generation process, we will consider two settings of interest: (1) the fixed design nonparametric regression setting (with noisy labels) (2) the agnostic statistical learning setting. They have different data assumptions and performance metrics to capture "generalization".

**Nonparametric Regression with Noisy labels.** In this setting, we assume fixed input \(x_{1},,x_{n}\) and \(y_{i}=f_{0}(x_{i})+_{i}\) for \(i[n]\), where \(f_{0}:\) is the ground-truth (target) function and \(\{_{i}\}_{i=1}^{n}\) are independent Gaussian noises \((0,^{2})\). Our goal is find a ReLU NN \(f\) using the dataset to minimize the mean squared error (MSE):

\[(f)=_{i=1}^{n}(f(x_{i})-f_{0}(x_{i}))^{2}.\] (5)

It is nonparametric because we do not require \(f_{0}\) to be described by a smaller number of parameters, but rather satisfy certain regularity conditions. Specifically, we focus on estimating target functions inside the first order bounded variation class

\[f_{0}^{(1)}(B,C_{n}):=\{f:[-x_{},x_{}] \ _{x}|f(x)| B,_{-x_{}}^{x_{}}|f^{}(x)| dx C_{n}\},\]

where \(f^{}\) denotes the second-order weak derivative of \(f\) and we define a short hand \(^{(1)}(f):=_{-x_{}}^{x_{}}|f^{}(x)|dx\), which we refer to as the \(^{(1)}\) (semi)norm of \(f\) throughout the paper. We refer readers to a recent paper (Hu et al., 2022, Section 1.2) for the historical importance and the challenges in estimating the BV functions. The complexity of such function class is discussed in Appendix D.

**Agnostic statistical learning and generalization gap.** In this setting, we assume the \(n\) data points \(\{(x_{i},y_{i})\}_{i=1}^{n}\) are drawn i.i.d. from an unknown distribution \(\) defined on \([-x_{},x_{}][-D,D]\). The expected performance on new data points is called "Risk", \(R(f)=_{(x,y)}[(f,(x,y))]\). We define the absolute difference between training loss and the risk:

\[(f):=(f)=|R(f)-(f)|.\]

We say that \(f\) generalizes if \((f) 0\) as \(n\) with high probability.

## 3 Stable Solutions Cannot Interpolate Noisy Labels

A large portion of previous works studying minima stability assume the learned function interpolates the data. However, for various optimization problems, it is unclear whether there exists such an interpolating solution that is stable, especially when the number of samples \(n\) becomes large.

For the nonparametric regression problem with noisy labels, we design an example where any interpolating function can not be stable. Before presenting the example, we first define the \(g\) function, which will be the weight function of the weighted \(^{(1)}\) norm throughout the paper: for \(x[-x_{},x_{}]\), \(g(x)=\{g^{-}(x),g^{+}(x)\}\) with

\[ g^{-}(x)&=^{2}(X<x)[x-X|X<x][X|X<x])^{2}},\\ g^{+}(x)&=^{2}(X>x)[X-x|X>x] [X|X>x])^{2}},\] (6)

where \(X\) is drawn from the empirical distribution of the data (a sample chosen uniformly from \(\{x_{j}\}\)).

For various distributions of training data (_e.g._ Gaussian distribution, uniform distribution), most of \(g\)'s mass is located at the center while \(g\) decays towards the extreme data points. The same \(g(x)\) is also applied as the weight function in Mulayoff et al. (2021), where they derived an upper bound \(_{-x_{}}^{x_{}}|f^{}(x)|g(x)dx- {1}{2}\) assuming \(f\) is linearly stable (Definition 2.1) and interpolating. We generalize the same upper bound to all stable solutions in \((,0,)\) as in Theorem C.2. Below we construct a counter-example, where we can prove a contradicting lower bound of \(_{-x_{}}^{x_{}}|f^{}(x)|g(x)dx\) for any interpolating \(f\), thus disproving the assumption of interpolation.

**Counter-example.** We fix \(x_{i}=}{n-1}-}{n-1}\) for \(i[n]\) and \(f_{0}(x)=0\) for any \(x\), which implies that \(y_{i}\)'s are independent random variables from \((0,^{2})\).

**Theorem 3.1**.: _For the counter-example, with probability \(1-\), for any interpolating function \(f\),_

\[_{-x_{}}^{x_{}}|f^{}(x)|g(x)dx=(  n[n-24()]),\] (7)

_where the randomness comes from the noises \(\{_{i}\}\). Under this high-probability event, when \(n(}())\), any stable solution \(f\) for GD with step size \(\) will not interpolate the data, i.e._

\[(,0,)\{f f(x_{i})=y_{i},\ \ i[n]\}=.\] (8)

The proof of Theorem 3.1 is deferred to Appendix F due to space limit. This result, together with Mulayoff et al. (2021, Theorem 1), implies that gradient descent cannot converge to interpolating solutions unless \(=O(1/n^{2})\). It also implies (when combined with Theorem 4.1) an intriguing geometric insight that all twice-differentiable interpolating solutions must be very sharp, _i.e._, its largest eigenvalue is larger than \((n^{2})\) (see details in Appendix J ). Moreover, we highlight that the conclusion of Theorem 3.1 is consistent with our observation in Figure 2(a), where the learned function tends to be smoother and would not interpolate the data as \(\) becomes larger. Therefore, in the following discussion, we consider the case without assuming interpolation.

## 4 Main Results

In this section, we present the main results about stable solutions for GD (functions in \((,0,)\)) from three aspects. Section 4.1 describes the implicit bias of stable solutions of gradient descent with large learning rate in the function space. Section 4.2 and 4.3 derive concrete generalization bounds that leverage the implicit biases in the _distribution-free statistical learning_ setting and the _non-parametric regression_ setting respectively. An outline of the proof of our main theorems is given in Appendix B. The full proof details are deferred to the appendix.

### Implicit Bias of Stable Solutions in the Function Space

We begin with characterizing the stable solutions for GD with step size \(\) without the assumption of interpolation (there can be \(i[n]\) such that \(f(x_{i}) y_{i}\)). Similar to the interpolating case, the learned stable function \(f\) enjoys a (weighted) \(^{(1)}\) bound as below.

**Theorem 4.1**.: _For a function \(f=f_{}\) where the training (square) loss \(\) is twice differentiable at \(\),2_

\[_{-x_{}}^{x_{}}|f^{}(x)|g(x)dx(_{}^{2}())}{2}-+x_{ }()},\] (9)

_where \(g(x)\) is defined as (6). Moreover, if we assume \(y_{i}=f_{0}(x_{i})+_{i}\) for independent noise \(_{i}(0,^{2})\), then with probability \(1-\) where the randomness is over the noises \(\{_{i}\}\),_

\[_{-x_{}}^{x_{}}|f^{}(x)|g(x)dx(_{}^{2}())}{2}-+ ( x_{}\{1,}\} )+x_{}(f)}.\] (10)

_In addition, if \(f=f_{}\) is a stable solution of GD with step size \(\) on dataset \(\), i.e., \(f_{}(,0,)\) as in (4), then we can replace \((_{}^{2}())}{2}\) with \(\) in (9) and (10)._

Theorem 4.1, which we prove in Appendix G, associates the local curvature of the loss landscape at \(\) with the smoothness of the function \(f_{}\) it represents as measured in a weighted \(^{(1)}\) norm. In short, it says that _flat solutions are simple_. The result is a strict generalization of Theorem 1 in Mulayoff et al. (2021) which requires interpolation, i.e., \(()=0\). Observe that the number of neurons \(k\) does not appear in (9) and have no effect in (10) when \(k>n\), thus the result applies to arbitrarily overparameterized two-layer NNs. Under the standard nonparametric regression assumption, (10) is a stronger bound that asymptotically matches the bound under interpolation (Mulayoff et al., 2021, Theorem 1) when \(k=o(n)\) and \((f_{})=o(1)\) as the number of data points \(n\).

Another interesting observation when combining (10) with Theorem 3.1 is that for all interpolating solutions (observe that \((f_{})=(^{2})\) w.h.p.)

\[_{}(_{}^{2}())(n^{2})- ().\]To say it differently, _all interpolating solutions are very sharp minima when the labels are noisy_. This provides theoretical explanation of the empirical observation that noisy labels are harder to overfit using gradient training (Zhang et al., 2021).

Note that we leave the term \(()\) (or \((f)\)) in the \(^{(1)}\) bound. Therefore, we can plug any upper bound for these terms into Theorem 4.1 for a concrete result, and below we instantiate the \(^{(1)}\) bound with a crude MSE bound under the assumption that \(f\) is "optimized".

**Corollary 4.2**.: _In the nonparametric regression problem with ground-truth function \(f_{0}\), for a stable solution \(f=f_{}\) of GD with step size \(\) where \(\) is twice differentiable at \(\), assume that \(f\) is optimized, i.e, the empirical loss of \(f\) is smaller than \(f_{0}\): \(_{i=1}^{n}(f(x_{i})-y_{i})^{2} _{i=1}^{n}(f_{0}(x_{i})-y_{i})^{2}\), then with probability \(1-\), the function \(f\) satisfies_

\[_{-x_{}}^{x_{}}|f^{}(x)|g(x)dx- {1}{2}+( x_{}),\] (11)

_where the randomness is over the noises \(\{_{i}\}\) and \(\) suppresses logarithmic terms of \(n,1/\)._

The assumption of optimized \(f\) is rather mild since, in practice, gradient-based optimizers are commonly quite effective in loss minimization (see also our experiments). With such assumption, we can derive an MSE upper bound (with high probability) of order \((^{2})\) (details in Lemma G.5), and thus the \(^{(1)}\) bound above.

In some cases, we can decrease the MSE upper bound we assumed in Corollary 4.2, and use this to improve the resulting \(^{(1)}\) bound (11). For instance, if the neural network is under-parameterized (i.e. \(k\) is smaller than \(n\)), in Appendix G.2 we derive a (high-probability) bound for MSE of order \((k/n)\), which implies that the last term in (11) becomes \(( x_{})\). The term vanishes if \(n/k\) is large enough, where the \(^{(1)}\) bound reduces to the noiseless and interpolating case.

### GD on ReLU NN Does Not Overfit

Why would anyone care about the function space implications of stable solutions? The next theorem shows that these solutions cannot overfit (in the strict interior of the data support) without making strong assumptions on the shape of the data distribution.

**Theorem 4.3**.: _Let \(\) be a joint distribution of \((x,y)\) supported on \([-x_{},x_{}][-D,D]\). Assume the dataset \(^{n}\) i.i.d. For any fixed interval \([-x_{},x_{}]\) and a universal constant \(c>0\) such that with probability \(1-/2\), \(g(x) c\) for all \(x\), if the function \(f=f_{}\) is a stable solution of GD with step size \(\) such that \(\) is twice differentiable at \(\) and \(\|f\|_{} D\), with probability \(1-\) (randomness over the dataset), the generalization gap restricted to \(\) satisfies_

\[_{}(f):=|_{}[(f(x)-y)^{ 2}]-}}_{x_{i}}(f(x_{i})-y _{i})^{2}|(D^{}[(-+2x_{}D)}{n_{}^{2} }]^{}),\] (12)

_where \(_{}\) means that \((x,y)\) is a new sample from the data distribution conditioned on \(x\) and \(n_{}\) is the number of data points in \(\) such that \(x_{i}\)._

The proof of Theorem 4.3 is deferred to Appendix H. Briefly speaking, we show that in the strict interior of the data support, the generalization gap will vanish as the number of data points \(n\) increases. This vanishing generalization gap further implies that the expected performance on new data points is close to the (observable) training loss, i.e., the output stable solutions do not overfit.

Regarding our assumptions, in addition to standard boundedness assumptions, we focus on the strict interior of the domain where \(g\) can be lower bounded (i.e. interval \(\)). This is because for extreme data points, \(g(x)\) decays and thus imposes little constraint on the output function \(f\). In Appendix H.1, we show that if the marginal distribution of \(x\) is the uniform distribution on \([-x_{},x_{}]\), when \(n\) is sufficiently large, \(\) can be chosen as \([-}{3},}{3}]\). In this case, with high probability, \(\) incorporates a large portion of the data points and \(n_{}=(n)\). More illustrations about the choice of \(\) under various data distributions are deferred to Appendix H.2.

Meanwhile, the generalization gap bound has dependence \(\) on the learning rate \(\). Therefore, as we increase the learning rate (in a reasonable range), the learned stable solution tends to be smoother, which further implies better generalization performances.

### GD on ReLU NN Achieves Optimal Rate for Estimating BV(1) Functions

Finally, we zoom into the nonparametric regression task that we described in Section 2 where \(x_{1},,x_{n}\) are fixed and the noisy labels \(y_{i}=f_{0}(x_{i})+(0,^{2})\) independently for \(i[n]\) for a ground truth function \(f_{0}\) in the first order bounded variation class (see Section 2 for details). Similar to Theorem 4.3, we focus on the strict interior \(\) of \([-x_{},x_{}]\), but instead of a generalization gap bound, we prove an MSE bound against \(f_{0}\) on \(\) that nearly matches the theoretical limit.

**Theorem 4.4**.: _Under the same conditions in Corollary 4.2, for any interval \([-x_{},x_{}]\) and a universal constant \(c>0\) such that \(g(x) c\) for all \(x\) and \(f\) is optimized over \(\), i.e. \(_{x_{i}}(f(x_{i})-y_{i})^{2}_{x_{i}}( f_{0}(x_{i})-y_{i})^{2}\), if the output stable solution \(\) satisfies \(\|\|_{}\) (for some constant \(>0\)) and the ground truth \(f_{0}^{(1)}(k^{2},(+  x_{}))\), then with probability \(1-\) (over the random noises \(\{_{i}\}\)), the function \(f=f_{}\) satisfies_

\[_{}(f)=}}_{x_{i} }(f(x_{i})-f_{0}(x_{i}))^{2}( (}{n_{}})^{}( }{}+ x_{}^{2})^{}),\] (13)

_where \(n_{}\) is the number of data points in \(\) such that \(x_{i}\)._

The proof of Theorem 4.4 is deferred to Appendix I. Below we discuss some interesting aspects of the result. First of all, Theorem 4.4 focuses on an interval \(\) where \(g(x)\) can be lower bounded. In this way, we ignore the extreme data points and derive an MSE upper bound (restricted to \(\)) of order \((n_{}^{-4/5})\), which matches the minimax optimal rate for estimating \(^{(1)}\) functions (see, e.g., Donoho and Johnstone, 1998, Theorem 1). In contrast, it is well-known that neural networks in the kernel regime (and any other "linear smoothers") must incur a strictly suboptimal worst-case \(=(n_{}^{-2/3})\)(Donoho et al., 1990, Suzuki, 2018). According to the discussions in Appendix H.1, if the data follows a uniform distribution, the interval \(\) can incorporate most of the data points where \(n_{}=(n)\).

Meanwhile, the MSE bound has dependence \(^{-2/5}\) on the learning rate \(\). Such dependence is because with a larger learning rate, the learned function \(f\) will have smaller \(^{(1)}\) bound, and therefore the set of possible output functions will contain fewer non-smooth functions, which implies a tighter MSE bound. However, this does not mean that a larger learning rate is always better. When \(\) is too large, GD may diverge, and even if it does not, the set of stable solutions cannot approximate the ground truth \(f_{0}\) well if \(_{-x_{}}^{x_{}}|f_{0}^{}(x)|dx+ }{c}\), thus failing to satisfy the "optimized" assumption. In our experiments, we verify the "optimized" assumption numerically for a wide range of \(\) and demonstrate that by tuning learning rate \(\), we are adapting to the unknown \(^{(1)}(f_{0})\).

Lastly, we remark that Theorem 4.4 holds for arbitrary \(k\), even if the neural network is heavily over-parameterized (\(k n\)). The dependence \(+ x_{}\) on \(\) results from the \(^{(1)}\) bound in Corollary 4.2, where the term \(\) will dominate if \(}\), which is the case if the step size is not large. Furthermore, if \(\) is sufficiently large, we can improve the \(^{(1)}\) bound in Corollary 4.2 as in Appendix G.2, and improve the mean squared error \(_{}(f)\) to \(((}{n_{}})^{}(}{})^{})\) accordingly. The assumptions and detailed statements for the improved MSE are deferred to Appendix I.1.

## 5 Experiments

In this section, we empirically validate our claims by training a two-layer fully connected neural network with ReLU activation using gradient descent (GD) with varying step sizes. We focus on fitting a mildly overparameterized ReLU network to a simple nonparametric regression problem. The input dataset comprises of 30 equally spaced fixed design points \(\{x_{i}\}_{i=1}^{n}\), where each \(x_{i}[-0.5,0.5]\) (\(n=30\)). Label \(y_{i}=f_{0}(x_{i})+(0,^{2})\) with \(=0.5\) and \(f_{0}(x)=(2x+1)(x 0)+(-2x+1)(x>0)\)The two-layer ReLU network is parameterized by \(\) (see Section 2) with \(k=100\) neurons per layer. The network uses standard parameterization (scale factor of 1) and parameters are initialized randomly (see Figure 7 for the initial basis functions).

Figure 2 (in the introduction) illustrates how changing the learning rate affects the learned ReLU NN that GD-training stabilizes on. The main take-aways are (a) large learning rate learns flatter minima which represent more regular functions (in \(^{(1)}\)); (b) Our bound from Theorem 4.1 is a very accurate description of the curvature of the Hessian as well as a valid upper bound of the \(^{(1)}\)-(pseudo) norm; (c) When we tune learning rate \(\), it is implicitly regularizing the complexity, which provides a satisfying variance tradeoff explanation to how GD-training works.

Figure 3 provides further details on the learning curves and representation learning. We note that the learned representation is very different from the initialization, thus our experiments are clearly describing phenomena not covered by the "kernel" regime. In addition, it seems that all solutions that GD finds after a small number of iterations satisfy the "optimized" assumption as required in Theorem 4.4. In the appendix (Figure 8), we provide empirical justification for the other assumption we make about the twice-differentiability of the solutions. More experiments can be found in the appendix with more learning rate choices, as well as a discussion on the _catastrophic_ and _tempered_ overfitting of interpolating solutions when we adjust \(k\).

## 6 Conclusion

In this paper, we took a new look into how gradient descent-trained two-layer ReLU neural networks generalize from a lens of minima stability (and the closely related Edge-of-Stability phenomena). We focused on univariate inputs with noisy labels and showed GD with typical choice of learning rate cannot interpolate the data. We also established that local smoothness of the training loss functions implies a first order total variation constraint on the function the neural network represents, hence proving that all such solutions have a vanishing generalization gap inside the strict interior of the data support. In addition, under a mild assumption, we prove that these stable solutions achieve near-optimal rate for estimating first-order bounded variation functions. Future work includes generalization beyond 1D input, two-hidden layers, and understanding the choice of optimization algorithms.

Figure 3: Highlights of our numerical simulation for large step size (\(=0.4\), **first row**) and small step size (\(=0.01\), **second row**) gradient descent training of a univariate ReLU NN with \(n=30\) noisy observations and \(k=100\) hidden neurons. From left to right, the three columns illustrate (a) Trained NN function (b) Learning curves (c) Learned basis functions (each of the 100 neurons).