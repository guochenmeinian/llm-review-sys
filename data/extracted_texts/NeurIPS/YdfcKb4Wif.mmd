# Learning Trajectories are Generalization Indicators

Jingwen Fu\({}^{1}\), Zhizheng Zhang\({}^{2}\), Dacheng Yin\({}^{3}\), Yan Lu\({}^{2}\), Nanning Zheng\({}^{1}\)

ful1371252069@stu.xjtu.edu.cn

{zhizhang,yanlu}@microsoft.com

ydc@mail.ustc.edu.cn

nnzheng@mail.xjtu.edu.cn

\({}^{1}\)National Key Laboratory of Human-Machine Hybrid Augmented Intelligence,

National Engineering Research Center for Visual Information and Applications,

and Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University,

\({}^{2}\)Microsoft Research Asia, \({}^{3}\)University of Science and Technology of China

Work done during internships at Microsoft Research Asia.Corresponding Authors

###### Abstract

This paper explores the connection between learning trajectories of Deep Neural Networks (DNNs) and their generalization capabilities when optimized using (stochastic) gradient descent algorithms. Instead of concentrating solely on the generalization error of the DNN post-training, we present a novel perspective for analyzing generalization error by investigating the contribution of each update step to the change in generalization error. This perspective enable a more direct comprehension of how the learning trajectory influences generalization error. Building upon this analysis, we propose a new generalization bound that incorporates more extensive trajectory information. Our proposed generalization bound depends on the complexity of learning trajectory and the ratio between the bias and diversity of training set. Experimental observations reveal that our method effectively captures the generalization error throughout the training process. Furthermore, our approach can also track changes in generalization error when adjustments are made to learning rates and label noise levels. These results demonstrate that learning trajectory information is a valuable indicator of a model's generalization capabilities.

## 1 Introduction

The generalizability of a Deep Neural Network (DNN) is a crucial research topic in the field of machine learning. Deep neural networks are commonly trained with a limited number of training samples while being tested on unseen samples. Depite the commonly used independent and identically distributed (i.i.d.) assumption between the training and testing sets, there often exists a varying degree of discrepancy between them in real-world applications. Generalization theories study the generalization of DNNs by modeling the gap between the empirical risk  and the popular risk . Classical uniform convergence based methods  adopt the complexity of the function space to analyze this generalization error. These theories discover that more complex function space results in a larger generalization error . However, they are not well applicable for DNNs . In deep learning, the double descent phenomenon  exists, which tells that larger complexity of function space may lead to smaller generalization error. This violates the aforementioned property in uniform convergence methods and imposes demands in studying the generalization of DNNs.

Although the function space of DNNs is vast, not all functions within that space can be discovered by learning algorithms. Therefore, some representative works bound the generalization of DNNs basedon the properties of the learning algorithm, _e.g._, stability of algorithm , information-theoretic analysis . These works rely on the relation between the input (_i.e._, training data) and output (weights of the model after training) of the learning algorithm to infer the generalization ability of the learned model. Here, the relation refers to how the change of one sample in the training data impacts the final weights of model in the stability of algorithms while referring to the mutual information between the weights and the training data in the information-theoretic analysis. Although some works [24; 11] leverage some information from training process to understand the properties of learning algorithm, there is limited trajectory information conveyed.

The purpose of this article is to enhance our theoretical comprehension of the relation between learning trajectory and generalization. While some recent experiments [9; 13; 29] have shown a strong correlation between the information contained in learning trajectory and generalization, the theoretical understanding behind this is still underexplored. By investigating the contribution of each update step to the change in generalization error, we give a new generalization bound with rich trajectory related information. Our work can serve as a starting point to understand those experimental discoveries.

### Our Contribution

Our contributions can be summarized below:

* We demonstrate that learning trajectory information serves as a valuable indicator of generalization abilities. With this motivation, we present a novel perspective for analyzing generalization error by investigating the contribution of each update step to the change in generalization error.
* Utilizing the aforementioned modeling technique, we introduce a novel generalization bound for deep neural networks (DNNs). Our proposed bound provides a greater depth of trajectory-related insights than existing methods.
* Our method effectively captures the generalization error throughout the training process. And the assumption corresponding to this method is also confirmed by experiments. Furthermore, our approach can also track changes in generalization error when adjustments are made to learning rates and label noise levels.

## 2 Related Work

Generalization TheoriesExisting works on studying the generalization of DNNs can be divided into three categories: the methods based on the complexity of function space, the methods based on learning algorithms, and the methods based on PAC Bayes. The first category considers the generalization of DNNs from the perspective of the complexity of the function space. Many methods for measuring the complexity of the function space have been proposed, _e.g._, VC dimension , Rademacher Complexity  and covering number . These works fail in being applied to DNN models since the complexity of the function space of a DNN model is too large to deliver a trivial result . This thus motivates recent works to rethink the generalization of DNNs based on the accessible information in different learning algorithms such as stability of algorithm , information-theoretic analysis . Among them, the stability of algorithm  measures how one sample change of training data impacts the model weights finally learned, and the information theory [30; 31; 40] based generalization bounds rely on the mutual information of the input (training data) and output (weights after training) of the learning algorithm. Another line is PAC Bayes  based method, which bounds the expectation of the error rates of a classifier chosen from a posterior distribution in terms of the KL divergence from a given prior distribution. Our research modifies the conventional Rademacher Complexity to calculate the complexity of the space explored by a learning algorithm, which in turn helps derive the generalization bound. Our approach resembles the first category, as we also rely on the complexity of the function space. However, our method differs as we focus on the function space explored by the learning trajectory, rather than the entire function space. The novelty of our technique lies in addressing the issue of dependence between training data and the function space explored by the learning trajectory, a dependence that is not permitted by the original Rademacher Complexity Theory.

Generalization Analysis for SGDThe optimization plays an nonnegligible role in the success of DNN. Therefore, there are many prior works studying the generalization of DNNs by exploring property of SGD, which could be summarized into two categories: stability of SGD and information-theoretic analysis. The most popular way of the former category is to analyze the stability of the weights updating. Hardt et al.  is the first work to analyze the stability of SGD with the requirements of smooth and Lipschitz assumptions. Its follow-up works try to discard the smooth , or Lipschitz  assumptions towards getting a more general bound. Information-theoretic methods leverage the chain rule of KL-divergence to calculate the mutual information between the learned model weights and the data. This kind of works is mainly applied for Stochastic Gradient Langevin Dynamics(SGLD), _i.e_., SGD with noise injected in each step of parameters updating . Negrea et al. , Haghifam et al.  improve the results using data-dependent priors. Neu et al.  construct an auxiliary iterative noisy process to adapt this method to the SGD scenario. In contrast to these studies, our approach utilizes more information related to learning trajectories. A more detailed comparison can be found in Table 2 and Appendix B.

## 3 Generalization Bound

Let us consider a supervised learning problem with a instance space \(\) and a parameter space \(\). The loss function can be defined as \(f:_{+}\). We denote the distribution of the instance space \(\) as \(\). The \(n\) i.i.d samples draw from \(\) are denoted as \(S=\{z_{1},...,z_{n}\}^{n}\). Given parameters \(\), the empirical risk and popular risk are denoted as \(F_{S}()_{i}^{n}f(,z_{i})\), and \(F_{}()_{z}[f(,z)]\) respectively. Our work studies the generalization error of the learned model, _i.e_. \(F_{}()-F_{S}()\). For an optimizaiton process, the learning trajectory is represented as a function \(:\). We use \(}\) to denote the weights of model after \(t\) times updating, where \(}=(t)\). The learning algorithm is defined as \(:^{n}\), where the second input \(\) denotes all randomness in the algorithm \(\), including the randomness in initialization, batch sampling _et al_.. We simply use \((S)\) to represent a random choice for the second input term. Given two functions \(U,V\), \(_{t}U(t)V(t)_{t}U(t)(V(t+1)-V(t))\) and we use \(\|\|\) to denote \(L2\) norm. If \(S\) is a set, then \(|S|\) denotes the number of elements in \(S\). \(_{t}\) denotes taking the expectation conditioned on \(\{}|i t\}\).

Let mini-batch \(B\) be a random subset sampled from dataset \(S\), and we have \(|B|=b\). The averaged function value of mini-batch \(B\) is denoted as \(F_{B}()_{z B}f(,z)\). The parameters updated with gradient descent can be formulated as:

\[}=}-_{t} F_{S}(}).\] (1)

where \(_{t}\) is the learning rate for the \(t\)-th update. The parameter updating with stochastic gradient descent is:

\[}=}-_{t} F_{B}(}).\] (2)

Let \(() F_{S}()- F_{B}()\) be the gradient noise in mini-batch updating, where \(\) is the weights of a DNN. Then we can transform Equation (2) into:

\[}=}-_{t} F_{S}(})+_{t} (}).\] (3)

The covariance of the gradients over the entire dataset \(S\) can be calculated as:

\[()_{i=1}^{n} f(,z_{i }) f(,z_{i})^{}- F_{S}() F_{S} ()^{}.\] (4)

Therefore, the covariance of the gradient noise \(()\) is:

\[C()().\] (5)

Since for any \(w\) we have \((())=0\), we can represent \(()\) as \(C()^{}^{}\), where \(^{}\) is a random distribution whose mean is zero and covariance matrix is an identity matrix. Here, \(^{}\)**can be any distributions**, including Guassian distribution  and \(\) distribution .

The primary objective of our work is to suggest a new generalization bound that incorporates more comprehensive trajectory-related information. The **key aspects** of this information are: 1) It should be adaptive and change according to different learning trajectories. 2) It should not rely on the extra information from data distribution \(\) except from the training data \(S\).

### Investigating generalization alone learning trajectory

As annotated before, the learning trajectory is represented by a function \(:\), which defines the relationship between the model weights and the training timesteps \(t\). \(_{t}\) denotes the model weights after \(t\) times updating. Note that \(\) depends on \(S\), because it comes from the equation \(=(S)\). We simply use \(f(}):_{+}\) to represent the function after \(t\)-times update. Our goal is to analyze the generalization error, i.e., \(F_{}(})-F_{S}(})\), where \(T\) represents the total training steps.

We reformulate the function corresponding to the finally obtained model as:

\[f(})=f(})+_{t=1}^{T}(f(})-f( })).\] (6)

Therefore, the generalization error can be rewritten as:

\[F_{}(})-F_{S}(})=(})-F_{S}(})}_{(i)}+_{t=1}^{T}(})-F_{}(}))-(F_{S}(})-F_{S}(}))]}_{ (ii)_{t}}.\] (7)

In this form, we divide the generalization error into two parts. \((i)\) is the generalization error before the training. \((ii)_{t}\) is the generalization error caused by \(t\)-step update.

Typically, there is independence between \(}\) and the data \(S\). Therefore, we have \((i)=0\). Combining with this, we have:

\[[F_{}(})-F_{S}(})]=_{t=1} ^{T}(ii)_{t}.\] (8)

Analyzing the generalization error after training can be transformed into analyzing the increase of generalization error for each update. This is a straighforward and quite different way to extract the information from learning trajectory compared with previous work. Here, we list two techniques that most used by previous works to extract the information from learning trajectory.

* (T1). This method leverages the chaining rule of mutual informaton to calculate a upper bound of the mutual information between \(}\) and the training data \(S\), _i.e._\(I(S;}) I(S;})_{t=0}^{T}I(S; }|})\). \(I(S;})\) is the value of concerning for their theory.
* (T2). This method assumes we have another data \(S^{}\), which is obtained by replacing one sample in data \(S\) with another sample drawing from distribution \(\). \(}\) is the learning trajectory trained from data \(S^{}\) with same randomness value as \(\). Denote \(_{k}\|}-_{k}}\|\) and assume \(_{0}=0\). Then, the value of concerning is \(_{T}\). The upper bound of \(_{T}\) is calculate by iterately apply the formular \(_{k} c_{k-1}_{k-1}+e_{k-1}\).

(T1) is commonly utilized in analyzing Stochastic Gradient Langevin Dynamics(SGLD) [18; 2; 28], while (T2) is frequently employed in stability-based works for analyzing SGD [11; 15; 5]. Our method offers several benefits, including: **1) We directly focus on the change in generalization error**, rather than intermediate values such as \(_{k}\) and \(I(S;}|})\), **2) The generalization error is equivalent to the sum of \((ii)_{t}\)**, while (T1) and (T2) takes the upper bound value of \(I(S;})\) and \(_{T}\), and **3) From this perspective, we can extract more in-depth trajectory-related information**. For (T1), the computation of \(I(S;}|})\) primarily involves the information of \( F_{}(})\), which is inaccessible to us (Detail in Appendix D and Neu et al. ). (T2) faces the challenge that only the upper bounds of \(c_{k}\) and \(c_{k}\) can be calculated. The upper bounds remain unchanged across various learning trajectories. Consequently, both (T1) and (T2) have difficulty conveying meaningful trajectory information.

### A New Generalization Bound

In this section, we introduce the generalization bound based on our aforementioned modeling. Let us start with the definition of commonly used assumptions.

**Definition 3.1**.: The function \(f\) is \(L\)-Lipschitz, if for all \(},}\) and for all \(z\), wherein we have \(\|f(},z)-f(},z)\| L\|}-}\|\).

**Definition 3.2**.: The function \(f\) is \(\)-smooth, if for all \(_{1},_{2}\) and for all \(z\), wherein we have \(\| f(_{1},z)- f(_{2},z)\|\| _{1}-_{2}\|\).

**Definition 3.3**.: The function \(f\) is convex, if for all \(_{1},_{2}\) and for all \(z\), wherein we have \(f(_{1},z) f(_{2},z)+(_{1}-_{2})^{ } f(_{2},z)\).

Here, \(L\)-lipschitz assumption implies that the \(\| f(,z)\| L\) holds. \(\)-smooth assumption indicates the largest eigenvalue of \(^{2}f(,z)\) is smaller than \(\). The convexity indicates the smallest eigenvalue of \(^{2}f(,z)\) are positive. These assumptions tell us the constraints of gradients and Hessian matrices of the training data and the unseen samples in the test set. Since the values of gradients and Hessian matrices in the training set are accessible, the key role of these assumptions is to deliver knowledge about the unseen samples in the test set.

In the following, we introduce a new generalization bound. We give the assumption required by our new generalization bound in the following.

**Assumption 3.4**.: There is a value \(\), so that for all \(\{_{}|t\}\), we have \(\| F_{}()\|\| F_{S}()\|\).

_Remark 3.5_.: Assumption 3.4 gives a restriction with the norm of popular gradient \( F_{}()\). This assumption is easily satisfied when \(n\) is a large number, because we have \(_{n}\| F_{S}()\|=\| F_{}()\|\). When the \(n\) is not large enough, the assumption will hold before SGD enter the neighbourhood of convergent point. Under the case that SGD enters the neighbourhood of convergent point, we give a relaxed assumption and its corresponding generalization bound in Appendix B. According to paper , this case will ununsually happen in real situation. Section 4 gives experiments to explore the assumption.

**Theorem 3.6**.: _Under Assumption 3.4, given \(S^{n}\), let \(=(S)\), where \(\) denoted the SGD or GD algorithm training with \(T\) steps, we have:_

\[[F_{}(_{})-F_{S}(_{ })]-2^{}_{m}_{t} (_{})}{}((_{}))}{\| F_{S}(_{})\|_{2}^{2}}}+ (_{m})\] (9)

_where \(()=()\|}{_{U  S}\| F_{U}()- F_{S /U}()\|}\), \(_{m}=_{t}(_{})\), \(^{}=\{1,_{U S:t}( _{})\|}{n\| F_{S}(_{})\|}\}\) and \(_{m}_{t}_{t}\)._

_Remark 3.7_.: Our generalization bound mainly relies on the information from gradients. \(()\) is related to the variance of the gradient. When the variance of the gradients across different samples in the training set \(S\) is large, then the value of \(()\) is small, and vice versa. Note that we have \(|U|<n\) due to \(U S\). Our bound will became trivial if \(_{U S}\| F_{U}()-  F_{S/U}()\|=0\). This rarely happens in real case, because it requires that for all \(U S\), we have \(|U| F_{U}()=(n-|U|) F_{S/U}()\). We also give a relaxed assumption version of this theorem in Appendix B. **The generalization bound provides a clear insight into how the reduction of training loss leads to a increase in generalization error.**

Proof SketchThe proof of this theorem is placed in Appendix A. Here, we give the sketch for this proof.

Step 1Beginning with Equation (8), we decomposite the \(F_{}(_{})-F_{S}(_{})\) into a linear part (\(^{lin}(_{})\)) and nonlinear part(\(^{nl}(_{})\)). We have \(^{lin}(_{})=_{t=1}^{T}(ii)_{t}^{lin}\), where \((ii)_{t}^{lin}(_{}-_{-1})^ {}( F_{}(_{-1})- F_{S}(_{-1}))\). The nonlinear part is \(^{nl}(_{})=F_{}(_{})-F_{S} (_{})-^{lin}(_{})\). We take these two parts differently. Here, we focus on analyzing \(^{lin}(_{})\) because it dominates under small learning rate. Detail discussion of \(^{nl}(_{})\) is given in Appendix (Propositon A.1 and Subsection C.3)

Step 2We construct the addictive linear space \(_{|S}\{_{t=0}^{T-1}_{} ^{} f(_{})\|_{}\| _{t}\}\), where \(_{t}\|_{t} F_{S}(_{})\|\). Then \([^{lin}(_{})] 2^{}_{m} R_{S}(_{|S})\), where \(R_{S}(_{|S})_{}_{h _{|S}}(_{i=1}^{n}_{i}h(z_{i}))\).

[MISSING_PAGE_FAIL:6]

### Further Analysis

#### 3.3.1 Interpreting the Generalization Bounds

We rewrite the obtained generalization bound here:

\[[F_{}(})-F_{S}(})]}_{}}^{}}_{}+(_{m})\] (11)

The "Bias of Training Set" refers to the disparity between the characteristics of the training set and those of the broader population. To measure this difference, we use the distance between the norm of the popular gradient and that of the training set gradient, as specified in Assumption 3.4. The "Diversity of Training Set" can be understood as the variation among the samples in the training set, which in turn affects the quality of the training data. The ratio \(}{}\) gives us the property of information conveyed by the training set. It is important to consider the properties of the training set, as the data may not contribute equally to the generalization. **The detail version of the equation can be found in Theorem 3.6.**

#### 3.3.2 Asymptotic Analysis

We will first analyze the dependent of \(n\) for \(\). The \(\) is calculated as \(()=()\|}{_{U S }\|\| F_{U}()-\| F_{S/U}()\|}\). Obviously, the gradient of individual sample is unrelated to the sample size \(n\). And \(|U| n\). Therefore, \(=(1)\). Similarly, we have \(_{t}(})}{}((}))}{\| F_{S}(})\|_{2}^{2}}}= (})\). As for the \((_{m})\) term in Theorem 3.6, we have \(_{n}(_{m})=0\) according to Proposition A.1. We simply assume that \((_{m})=(})\). Therefore, our bound has \(((0.5,c)}})\).

#### 3.3.3 Comparison with Stability-based methods

We first compare our method with the stability-based methods in terms of the trajectory information.In Table 1, we present a summary of stability-based methods, while other methods are outlined in Appendix D. We focus on generalization bounds from previous works that eliminate terms dependent on extra information about data distribution \(\), apart from the training data \(S\), using assumptions such as smoothness or Lipschitz continuity. Analyzing Table 1 reveals that most prior works primarily depend on the learning rate \(\) and the total number of training steps \(T\). This suggests that we can achieve the same bound by using an identical learning rate schedule and total training steps, which does not align with our practical experience. Our proposed generalization bound considers the evolution of function values, gradient covariance, and gradient norms throughout the training process. As a result, our bounds encompass more comprehensive information about the learning trajectory.

**Next, we give a detail comparison with Hardt et al.  in Table 2.** The concept of uniform stability is commonly used to evaluate the ability of SGD in generalizaton, by assessing its stability when a single training sample is altered. Our primary point of comparison is with Hardt et al. , as their work is considered the most representative in terms of analyzing the stability of SGD. We find that **First**, the assumption of Uniform Stability requires the gradient norm of all input samples for all weights being bounded by \(L\), whereas our assumption only limits the expectation of the gradients

   & Uniform Stability & Ours \\  Assumption & \( z^{c}\| f (,z^{})\| L\) & \(\{}|\}\) & \(_{_{i} f(,z^{})\|\| _{}\|\|_{}\|}\) \\ Modelling Method of SGD & Epoch Structure & Full Batch Gradient \(\) Stochastic Noise \\ Batch Size & \(1\) & \( n\) & \( n\) \\ Trajectory Information in Bound & Learning rate and number of training step & Values in Trajectory (gradient norm and covariance) \\ Perspective & Stability of Algorithm & Complexity of Learning Trajectory \\  

Table 2: **Detail comparison with Hardt et al. .** The \(\) refers to the \(\)-smooth assumption (see in Definition 3.2). \(S\) denotes the training set.

for the weights during the learning trajectory. **Secondly**, Uniform Stability uses an epoch structure to model the stochastic gradient descent, whereas our approach regards each stochastic gradient descent as full batch gradient descent with added stochastic noise. The epoch structure complicates the modelling process because it requires a consideration of sampling. As a result, in Hardt et al. , the author only considers the setting with batch size 1. **Thirdly**, the bound of Uniform Stability only uses hyperparameters setting such as learning rate and number of training step. In contrast, our bound contains more trajectory-related information, such as the gradient norm and covariance. **Finally**, the Uniform Stability provides the generalization bound based on the stability of the algorithm, while our approach leverages the complexity of the learning trajectory. **In summary**, there are some notable differences between our approach and Uniform Stability, such as the assumptions made, the modelling process, the type of information used in the bound, and the perspectives.

## 4 Experiments

### Tightness of Our Bounds

In a toy dataset setting, we compare our generalization bound with stability-based methods.

Reasons for toy examples1) Some values in the bounds are hard to be calculated.Calculating \(\) (under the \(\)-smooth assumption) and \(L\) (under the \(L\)-Lipschitz assumption) in stability-based work, as well as the values of \(\) and \(\) in our proposed bound, are challenging. **2) Stability-based methods require a batch size of 1.** The training is hard for batch size of 1 with learning rate setting \(_{t}=\) in complex datasets.

Construction of the toy examplesIn the following, we discuss the construction of the toy dataset used to compare the tightness of the generalization bounds. The training data is \(X_{tr}=\{x_{i}\}_{i=1}^{n}\). All the data \(x_{i}\) is sampled from Guassian distribution \((0,_{d})\). Sampling \(}(0,_{d})\),the ground truth is generated by \(y_{i}=1\) if \(}^{}x_{i}>0\) else \(0\). The weights for learning is denoted as \(\). The predict \(\) is calculated as \(_{i}=^{}x_{i}\). The loss for a simple data point is \(l_{i}=\|y_{i}-^{}x_{i}\|_{2}\). The training loss is \(=_{i=1}^{n}l_{i}\). The test data is \(X_{te}=\{x_{i}^{}\}\), where \(x_{i}^{}=_{i}^{}\) and \(_{i}^{}(0,_{d})\). We use 100 samples for training and 1,000 samples for evaluation. The model is trained using SGD for 200 epochs.

We evaluate the tightness of our bound by comparing our results with those in Hardt et al.  and Zhang et al.  from the original paper. We set the learning rate as \(_{t}=\). **Our reasons for comparing with these two papers are**: 1) Hardt et al.  is a representative study, 2) Both papers have theorems using a learning rate setting of \(_{t}=()\), which aligns with Corollary 3.8 in our paper, and 3) They do not assume convexity. The generalization bounds we compare include Corollary 3.8 from our paper, Theorem 3.12 from Hardt et al. , and Theorem 5 from Zhang et al. .

Our results are given in Table 3. Our bound is tighter under this setting.

### Capturing the trend of generalization error

In this section, 1) we conduct the deep learning experiment to verify Assumption 3.4 and 2) Verify whether our proposed generalization bound can capture the changes of generalization error. In this experiment, we mainly consider the term \((})-2_{i=0}^{t}(})}{}((}))}{\|  F_{S}(})\|_{2}^{2}}}\). We omit the term \(^{}\) and \(_{m}\), because all the trajectory related information that we want to explore is stored in \((})\). Capturing the trend of generalization error is regarded as an important problem in Nagarajan . Unless further specified, we use the default setting of the experiments on CIFAR-10 dataset 

  Gen Error & Ours & Hardt et al.  & Zhang et al.  \\ 
1.49 & 3.62 & 4.04 & 4417 \\  

Table 3: **Numeric comparison with stability-based work on toy examples.** The reason for the value of Zhang et al.  is large is because that our and Hardt et al.  has dependent on \(}{}\), while Zhang et al.  depends on \(L^{2}\). \(L\) and \(\) are usually large numbers.

with the VGG13  network. The experimental details for each figure can be found in Appendix C.2.

Our observations are:

* Assumption 3.4 is valid when SGD is not exhibiting extreme overfitting.
* The term of \((})\) can depict how the generalization error varies along the training process. And it can also track the changes in generalization error when adjustments are made to learning rates and label noise levels

Exploring the assumption 3.4 for different dataset during the training processTo explore the Assumption 3.4, we define \(_{t}}(})\|}{\| F _{S}(})\|}\) and \(_{t}}(} )\|}{\| F_{S}(})\|}\), where \(S^{}\) is another data set i.i.d sampled from distribution \(\). Because \(S^{}\) is independent with \(S\), we have \(_{t}_{t}\). We found that \(_{t}\) is stable around 1 during the early stage of training(Figure 1). When the training loss is reaching a relative small value, \(_{t}\) increases as we continue training. This phenomenon remain consistant across the Cifar10, Cifar100 and SVHN datasets. The \(\) in Assumption 3.4 can be assigned as \(=_{t}_{t}\). We can always find such \(\) if the optimizer is not extreme overfitting. Under the extremely overfitting case, we can use the relaxed theorem in Appendix B to bound the generalization error.

The bound capturing the trend of generalization error during training processThe generalization error and the \((})\) both changes as the training continues. Therefore, we want to verify whether they correlate with each other during the training process. Here, we use the term \( F_{S^{}}(})- F_{S}(})\) to approximate the generalization error. We find that \( F_{S^{}}(})- F_{S}(})\) has similar trend with \((})\) (Figure 2 _Center_). What's more, we also find that the curve of \( F_{S}(})+(})\) exhibits a comparable pattern with the curve \(F_{S^{}}(})\) (Figure 2 _Left_). To explore whether \((})\) reveals influence of the change of \(F_{S}(})\) to the generalization error, we plot \(})}{dF_{S}(})}\) (Figure 2 _Right_) during

Figure 1: **Exploration of Assumption 3.4 for different dataset.** The \(_{t}\) is stable before training loss reaches a relative small value. Assumption holds if the training is stop before extremely overfitting. A relaxed assumption and its corresponding generalization bound are given in Appendix B for extremely overfitting situation

the training process. \((})}{dF_{S}(})}\) increases slowly during the early stage of training, but surge rapidly afterward. This discovery is aligned with our intuition about the overfitting.

The complexity of learning trajectory correlates with the generalization errorIn Figure 3, we carry out experiments under various settings. Each data point in the figure represents the average of three repeated experiments. The results demonstrate that both the generalization error and \((})\) increase as the level of label noise is raised (Figure 3_Left_). The another experiments measure \((})\) and generalization error for different learning rate and discover that \((})\) can capture the trend generalization error. The reasons behind a larger learning rate resulting in a smaller generalization error have been explored in Li et al. , Barrett and Dherin . Additionally, Appendix E discusses why a larger learning rate can lead to a smaller \((})\).

## 5 Limitation

The assumption of small learning rate is required by our method. But this assumption is also common use in previous works. For example, Hardt et al. , Zhang et al. , Zhou et al.  explicitly requires that the learning rate should be small and is decayed with a rate of \(()\). Some methods have no explicit requirements about this but show that large learning rate pushes the generalization bounds to a trivial point. For example, the generalization bounds in works [5; 16] have a term \(_{t=1}^{T}_{t}^{2}\) that is not decayed as the data size \(n\) increases. The value of this term is unignorable when the learning rate is large. The small learning assumption widens the gap between theory and practice. Eliminating this assumption is crucial for future work.

## 6 Conclusion

In this study, we investigate the relation between learning trajectories and generalization capabilities of Deep Neural Networks (DNNs) from a unique standpoint. We show that learning trajectories can serve as reliable predictors for DNNs' generalization performance. To understand the relation between learning trajectory and generalization error, we analyze how each update step impacts the generalization error. Based on this, we propose a novel generalization bound that encompasses extensive information related to the learning trajectory. The conducted experiments validate our newly proposed assumption. Experimental findings reveal that our method effectively captures the generalization error throughout the training process. Furthermore, our approach can also track changes in generalization error when adjustments are made to learning rates and the level of label noises.

## 7 Acknowledgement

We thank all the anonymous reviewers for their valuable comments. The work was supported in part with the National Natural Science Foundation of China (Grant No. 62088102).

Figure 3: \((})\) **correlates with \( F_{S^{}}(})- F_{S}(})\). _Left:_\((})\) and the generalization error under different label noise level. _Right:_\((})\) and the generalization error under learning rate. The \((})\) can capture the trend of generalization error based by learning rate when learning rate is small. Appendix E provides proof that a large learning rate results in a smaller proposed generalization bound. Further discussions on why a small learning rate leads to a larger generalization error can be found in Li et al. , Barrett and Dherin .