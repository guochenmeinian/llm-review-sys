# Time Series as Images: Vision Transformer for Irregularly Sampled Time Series

Zekun Li, Shiyang Li, Xifeng Yan

University of California, Santa Barbara

{zekunli, shiyangli, xyan}@cs.ucsb.edu

###### Abstract

Irregularly sampled time series are increasingly prevalent, particularly in medical domains. While various specialized methods have been developed to handle these irregularities, effectively modeling their complex dynamics and pronounced sparsity remains a challenge. This paper introduces a novel perspective by converting irregularly sampled time series into line graph images, then utilizing powerful pre-trained vision transformers for time series classification in the same way as image classification. This method not only largely simplifies specialized algorithm designs but also presents the potential to serve as a universal framework for time series modeling. Remarkably, despite its simplicity, our approach outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Especially in the rigorous leave-sensors-out setting where a portion of variables is omitted during testing, our method exhibits strong robustness against varying degrees of missing observations, achieving an impressive improvement of 42.8% in absolute F1 score points over leading specialized baselines even with half the variables masked. Code and data are available at https://github.com/Leezekun/ViTST.

## 1 Introduction

Time series data are ubiquitous in a wide range of domains, including healthcare, finance, traffic, and climate science. With the advances in deep learning architectures such as LSTM , Temporal Convolutional Network (TCN) , and Transformer , numerous algorithms have been developed for time series modeling. However, these methods typically assume fully observed data at regular intervals and fixed-size numerical inputs. Consequently, these methods encounter difficulties when faced with irregularly sampled time series, which consist of a sequence of samples with irregular intervals between their observation times. To address this challenge, highly specialized models have been developed, which require substantial prior knowledge and efforts in model architecture selection and algorithm design .

In parallel, pre-trained transformer-based vision models, most notably vision transformers,1 have emerged and demonstrated strong abilities in various vision tasks such as image classification and object detection, nearly approaching human-level performance. Motivated by the flexible and effective manner in which humans analyze complex numerical time series data through visualization, we raise the question: _Can these powerful pre-trained vision transformers capture temporal patterns in visualized time series data, similar to how humans do?_

To investigate this question, we propose a minimalist approach called **ViTST** (_Vision Time Series_ Transformer), which involves transforming irregularly sampled multivariate time series into line graphs, organizing them into a standard RGB image format, and finetuning a pre-trained visiontransformer for classification using the resulting image as input. An illustration of our method can be found in Figure 1.

Line graphs serve as an effective and efficient visualization technique for time series data, regardless of irregularity, structures, and scales. They can capture crucial patterns, such as the temporal dynamics represented within individual line graphs and interrelations between variables throughout separate graphs. Such a visualization technique benefits our approach because it is both simple and intuitively comprehensible to humans, enabling straightforward decisions for time series-to-image transformations. Leveraging vision models for time series modeling in this manner mirrors the concept of **prompt engineering**, wherein individuals can intuitively comprehend and craft prompts to potentially enhance the processing efficiency of language models.

We conduct a comprehensive investigation and validation of the proposed approach, ViTST, which has demonstrated its superior performance over state-of-the-art (SoTA) methods specifically designed for irregularly sampled time series. Specifically, ViTST exceeded prior SoTA by 2.2% and 0.7% in absolute AUROC points, and 1.3% and 2.9% in absolute AUPRC points for healthcare datasets P19  and P12 , respectively. For the human activity dataset, PAM , we observed improvements of 7.3% in accuracy, 6.3% in precision, 6.2% in recall, and 6.7% in F1 score (absolute points) over existing SoTA methods. Our approach also exhibits strong robustness to missing observations, surpassing previous leading solutions by a notable 42.8% in absolute F1 score points under the leave-sensors-out scenario, where half of the variables in the test set are masked during testing. Furthermore, when tested on regular time series data including those with varying numbers of variables and extended sequence lengths, ViTST still achieves excellent results comparable to the specialized algorithms designed for regular time series modeling. This underscores the versatility of our approach, as traditional methods designed for regularly sampled time series often struggle with irregularly sampled data, and vice versa.

In summary, the contributions of this work are three-fold: (1) We propose a simple yet highly effective approach for multivariate irregularly sampled time series classification. Despite its simplicity, our approach achieves strong results against the highly specialized SoTA methods. (2) Our approach demonstrates excellent results on both irregular and regular time series data, showcasing its versatility and potential as a general-purpose framework for time series modeling. It offers a robust solution capable of handling diverse time series datasets with varying characteristics. (3) Our work demonstrates the successful transfer of knowledge from vision transformers pre-trained on natural images to synthetic visualized time series line graph images. We anticipate that this will facilitate the utilization of fast-evolving and well-studied computer vision techniques in the time series domains, such as better model architecture , data augmentation , interpretability , and self-supervised pre-training .

## 2 Related work

Irregularly sampled time series.An irregularly sampled time series is a sequence of observations with varying time intervals between them. In a multivariate setting, different variables within the

Figure 1: An illustration of our approach ViTST. The example is from a healthcare dataset P12 , which provides the irregularly sampled observations of 36 variables for patients (we only show 4 variables here for simplicity). Each column in the table is an observation of a variable, with the observed time and value. We plot separate line graphs for each variable and arrange them into a single image, which is then fed into the vision transformer for classification.

same time series may not align. These characteristics present a significant challenge to standard time series modeling methods, which usually assume fully observed and regularly sampled data points. A common approach to handling irregular sampling is to convert continuous-time observations into fixed time intervals [24; 20]. To account for the dynamics between observations, several models have been proposed, such as GRU-D , which decays the hidden states based on gated recurrent units (GRU) . Similarly,  proposed an approach based on multi-directional RNN, which can capture the inter- and intra-steam patterns. Besides the recurrent and differential equation-based model architectures, recent work has explored attention-based models. Transformer  is naturally able to handle arbitrary sequences of observations. ATTAIN  incorporates attention mechanism with LSTM to model time irregularity. SeFT  maps the irregular time series into a set of observations based on differentiable set functions and utilizes an attention mechanism for classification. mTAND  learns continuous-time embeddings coupled with a multi-time attention mechanism to deal with continuous-time inputs. UTDE  integrates embeddings from mTAND and classical imputed time series with learnable gates to tackle complex temporal patterns. Raindrop  models irregularly sampled time series as graphs and utilizes graph neural networks to model relationships between variables. While these methods are specialized for irregular time series, our work explores a simple and general vision transformer-based approach for irregularly sampled time series modeling.

**Numerical time series modeling with transformers.** Transformers have gained significant attention in time series modeling due to their exceptional ability to capture long-range dependencies in sequential data. A surge of transformer-based methods have been proposed and successfully applied to various time series modeling tasks, such as forecasting [19; 51; 41; 52], classification , and anomaly detection . However, these methods are usually designed for regular time series settings, where multivariate numerical values at the same timestamp are viewed as a unit, and temporal interactions across different units are explicitly modeled. A recent work  suggests dividing each univariate time series into a sequence of sub-series and modeling their interactions independently. These methods all operate on numerical values and assume fully observed input, while our proposed method deals with time series data in the visual modality. By transforming the time series into visualized line graphs, we can effectively handle irregularly sampled time series and harness the strong visual representation learning abilities of pre-trained vision transformers.

**Imaging time series.** Previous studies have explored transforming time series data into different types of images, such as Gramian fields , recurring plots [14; 37], and Markov transition fields . These approaches typically employ convolutional neural networks (CNNs) for classification tasks. However, they often require domain expertise to design specialized imaging techniques and are not universally applicable across domains. Another approach  involves utilizing convolutional autoencoders to complete images transformed from time series, specifically for forecasting purposes. Similarly,  utilized CNNs to encode images converted from time series and use a regressor for numerical forecasting. These approaches, however, still need plenty of specialized designs and modifications to adapt to time series modeling. Furthermore, they still lag behind the current leading numerical techniques. In contrast, our proposed method leverages the strong abilities of pre-trained vision transformer to achieve superior results by transforming the time series into line graph images, sidestepping the need of prior knowledge and specific modifications and designs.

## 3 Approach

As illustrated in Fig. 1, ViTST consists of two main steps: (1) transforming multivariate time series into a concatenated line graph image, and (2) utilizing a pre-trained vision transformer as an image classifier for the classification task. To begin with, we present some basic notations and problem formulation.

**Notation.** Let \(=\{(_{i},y_{i})|i=1,,N\}\) denote a time series dataset containing \(N\) samples. Every data sample is associated with a label \(y_{i}\{1,,C\}\), where \(C\) is the number of classes. Each multivariate time series \(_{i}\) consists of observations of \(D\) variables at most (some might have no observations). The observations for each variable \(d\) are given by a sequence of tuples with observed time and value \([(t_{1}^{d},v_{1}^{d}),(t_{2}^{d},v_{2}^{d}),,(t_{n_{d}}^{d},v_{n_{d}}^{d})]\), where \(n_{d}\) is the number of observations for variable \(d\). If the intervals between observation times \([t_{1}^{d},t_{2}^{d},,t_{n_{d}}^{d}]\) are different across variables or samples, \(_{i}\) is an irregularly sampled time series. Otherwise, it is a regular time series.

**Problem formulation.** Given the dataset \(=\{(_{i},y_{i})|i=1,,N\}\) containing \(N\) multivariate time series, we aim to predict the label \(_{i}\{1,,C\}\) for each time series \(_{i}\). There are mainly two components in our framework: (1) a function that transforms the time series \(_{i}\) into an image \(_{i}\); (2) an image classifier that takes the line graph image \(_{i}\) as input and predicts the label \(_{i}\).

### Time Series to Image Transformation

**Time series line graph.** The line graph is a prevalent method for visualizing temporal data points. In this representation, each point signifies an observation marked by its time and value: the horizontal axis captures timestamps, and the vertical axis denotes values. Observations are connected with straight lines in chronological order, with any missing values interpolated seamlessly. This graphing approach allows for flexibility for users in plotting time series as images, intuitively suited for the processing efficiency of vision transformers. The practice mirrors **prompt engineering** when using language models, where users can understand and adjust natural language prompts to enhance the model performance.

In our practice, we use marker symbols "\(*\)" to indicate the observed data points in the line graph. Since the scales of different variables may vary significantly, we plot the observations of each variable in an individual line graph, as shown in Fig. 1. The scales of each line graph \(_{i,d}\) are kept the same across different time series \(_{i}\). We employ distinct colors for each line graph for differentiation. Our initial experiments indicated that tick labels and other graphical components are superfluous, as an observation's position inherently signals its relative time and value magnitude. We investigated the influences of different choices of time series-to-image transformation in Section 4.3.

**Image Creation.** Given a set of time series line graphs \(_{i}=_{1},_{2},,_{D}\) for a time series \(_{i}\), we arrange them in a single image \(_{i}\) using a pre-defined grid layout. We adopt a square grid by default, following . Specifically, we arrange the \(D\) time series line graphs in a grid of size \(l l\) if \(l(l-1)<D l l\), and a grid of size \(l(l+1)\) if \(l l<D l(l+1)\). For example, the P19, P12, and PAM datasets contain 34, 36, and 17 variables, respectively, and the corresponding default grid layouts are \(6 6\), \(6 6\), and \(4 5\). Any grid spaces not occupied by a line graph remain empty. Figure 6 showcases examples of the resulting images. As for the order of variables, we sort them according to the missing ratios for irregularly sampled time series. We explored the effects of different grid layouts and variable orders in Section 4.3.

### Vision Transformers for Time Series Modeling

Given the image \(_{i}\) transformed from time series \(_{i}\), we leverage an image classifier to perceive the image and perform the classification task. The time series patterns in a line graph image involve both local (_i.e._, the temporal dynamics of a single variable in a line graph) and global (the correlation among variables across different line graphs) contexts. To better capture these patterns, we choose the recently developed vision transformers. Unlike the predominant CNNs, vision transformers are proven to excel at maintaining spatial information and have stronger abilities to capture local and global dependencies [9; 22].

**Preliminary.** Vision Transformer (ViT)  is originally adapted from NLP. The input image is split into fixed-sized patches, and each patch is linearly embedded and augmented with position embeddings. The resulting sequence of vectors is then fed into a standard Transformer encoder consisting of a stack of multi-head attention modules (MSA) and MLP to obtain patch representations. An extra classification token is added to the sequence to perform classification or other tasks. ViT models _global_ inter-unit interactions between all pairs of patches, which can be computationally expensive for high-resolution images. Swin Transformer, on the other hand, adopts a hierarchical architecture with multi-level feature maps and performs self-attention locally within non-overlapping windows, reducing computational complexity while improving performance. We use Swin Transformer as the default backbone vision model unless otherwise specified, but any other vision model can also be used within this framework.

Swin Transformer captures the _local_ and _global_ information by constructing a hierarchical representation starting from small-sized patches in earlier layers and gradually merging neighboring patches in deeper layers. Specifically, in the W-MSA block, self-attention is calculated within each non-overlapping window, allowing for the capture of local intra-variable interactions and temporal dynamics of a single line graph for a variable \(d\). The shifted window block SW-MSA then enablesconnections between different windows, which span across different line graphs, to capture _global_ interactions. Fig. 2 illustrates this process. Mathematically, the consecutive Swin Transformer blocks are calculated as:

\[}^{l}=((^{l-1 }))+^{l-1},\] \[^{l}=((}^{l }))+}^{l},\] \[}^{l+1}=(( ^{l}))+^{l},\] \[^{l+1}=((} ^{l+1}))+}^{l+1},\] (1)

where \(}^{l}\) and \(^{l}\) denote the output features of the (S)W-MSA module and the MLP module for block \(l\), respectively; LN stands for the layer normalization . After multiple stages of blocks, the global interactions among patches from all line graphs can be captured, enabling the modeling of correlations between different variables. We have also explored the use of additional positional embeddings, including local positional embeddings to indicate the position of each patch within its corresponding line graph, and global positional embeddings to represent the index of the associated line graph in the entire image. However, we didn't observe consistent improvement over the already highly competitive performance, which might suggest that the original pre-trained positional embeddings have already been able to capture the information regarding local and global patch positions.

**Inference.** We use vision transformers to predict the labels of time series in the same way as image classification. The outputs of Swin Transformer blocks at the final stage are used as the patch representations, upon which a flattened layer with a linear head is applied to obtain the prediction \(_{i}\).

## 4 Experiments

### Experimental Setup

**Datasets and metrics.** We conducted experiments using three widely used healthcare and human activity datasets, as outlined in Table 1. The P19 dataset  contains information from 38,803 patients, with 34 sensor variables and a binary label indicating sepsis. The P12 dataset  comprises data from 11,988 patients, including 36 sensor variables and a binary label indicating survival during hospitalization. Lastly, the PAM dataset  includes 5,333 samples from 8 distinct human activities, with 17 sensor variables provided for each sample. We used the processed data provided by Raindrop . To ensure consistency, we employed identical data splits across all comparison baselines, and evaluated the performance using standard metrics such as Area Under a ROC Curve (AUROC) and Area Under Precision-Recall Curve (AUPRC) for the imbalanced P12 and P19 datasets. For the more balanced PAM dataset, we reported Accuracy, Precision, Recall, and F1 score.

**Implementation.** We utilized the Matplotlib package to plot line graphs and save them as standard RGB images. For the P19, P12, and PAM datasets, we implemented grid layouts of \(6 6\), \(6 6\), and \(4 5\), respectively. For a fair comparison, we assigned a fixed size of \(64 64\) to each grid cell (line graph), resulting in image sizes of \(384 384\), \(384 384\), and \(256 320\), respectively. It is important to note that image sizes can also be directly set to any size, irrespective of grid cell dimensions. We plot the images according to the value scales on training sets. We employed the checkpoint of Swin

   Datasets & \#Samples & \#Variables & \#Avg. obs. & \#Classes & Demographic info & Imbalanced & Missing ratio \\  P19 & 38,803 & 34 & 401 & 2 & True & True & 94.9\% \\ P12 & 11,988 & 36 & 233 & 2 & True & True & 88.4\% \\ PAM & 5,333 & 17 & 4,048 & 8 & False & False & 60.0\% \\   

Table 1: Statistics of the irregularly sampled time series datasets used in our experiments.

Figure 2: Illustration of the shifted window approach in Swin Transformer. Self-attention is calculated within each window (grey box). When the window is contained within a single line graph, it captures local interactions. After shifting, the window includes patches from different line graphs, allowing for the modeling of global cross-variable interactions.

Transformer pre-trained on the ImageNet-21K dataset2. The default patch size and window size are \(4\) and \(7\), respectively.

**Training.** Given the highly imbalanced nature of the P12 and P19 datasets, we employed upsampling of the minority class to match the size of the majority class. We finetuned the Swin Transformer for 2 and 4 epochs on the upsampled P19 and P12 datasets, respectively, and for 20 epochs on the PAM dataset. The batch sizes used for training were 48 for P19 and P12, and 72 for PAM, while the learning rate was set to 2e-5. The models were trained using A6000 GPUs with 48G memory.

**Incorporating static features.** In real-world applications, especially in the healthcare domain, irregular time series data often accompanies additional information such as categorical or textual features. In the P12 and P19 datasets, each patient's demographic information, including weight, height, and ICU type, is provided. This static information remains constant over time and can be expressed using natural language. To incorporate this information into our framework, we employed a template to convert it into natural language sentences and subsequently encoded the resulting text using a RoBERTa-base  text encoder. The resulting text embeddings were concatenated with the image embeddings obtained from the vision transformer to perform classification. The static features were also applied to all compared baselines.

### Main Results

**Comparison to state-of-the-art.** We compare our approach with several state-of-the-art methods that are specifically designed for irregularly sampled time series, including Transformer , Trans-mean (Transformer with an imputation method that replaces the missing value with the average observed value of the variable), GRU-D , SeFT , mTAND , IP-Net , and Raindrop . In addition, we also compared our method with two approaches initially designed for forecasting tasks, namely DGM2-O  and MTGNN . The implementation and hyperparameter settings of these baselines were kept consistent with those used in Raindrop . Specifically, a batch size of 128 was employed, and all compared models were trained for 20 epochs. To ensure fairness in our evaluation, we averaged the performance of each method over 5 data splits that were kept consistent across all compared approaches.

As shown in Table 2, our proposed approach demonstrates strong performance against the specialized state-of-the-art algorithms on all three datasets. Specifically, on the P19 and P12 datasets, ViTST achieves improvements of 2.2% and 0.7% in absolute AUROC points, and 1.3% and 2.9% in absolute AUPRC points over the state-of-the-art results, respectively. For the PAM dataset, the improvement is even more significant, with an increase of 7.3% in Accuracy, 6.3% in Precision, 6.2% in Recall, and 6.7% in absolute F1 score points.

**Leaving-sensors-out settings.** We conducted additional evaluations to assess the performance of our model under more challenging scenarios, where the observations of a subset of sensors (variables) were masked during testing. This setting simulates real-world scenarios when some sensors fail or become unreachable. Following the approach adopted in , we experimented with two setups using the PAM dataset: (1) _leave-fixed-sensors-out_, which drops a fixed set of sensors across all

    &  &  &  \\   & AUROC & AUPRC & AUROC & AUPRC & Accuracy & Precision & Recall & F1 score \\  Transformer & \(80.7 3.8\) & \(42.7 7.7\) & \(83.3 9.7\) & \(47.9 3.6\) & \(83.5 1.5\) & \(84.8 1.5\) & \(86.0 1.2\) & \(85.0 1.3\) \\ Trans-mean & \(83.7 1.8\) & \(45.8 3.2\) & \(82.6 2.0\) & \(46.3 4.0\) & \(83.7 2.3\) & \(84.9 2.6\) & \(86.4 2.1\) & \(85.1 2.4\) \\ GRU-D & \(83.9 1.7\) & \(46.9 2.1\) & \(81.9 2.1\) & \(46.1 4.7\) & \(83.3 1.6\) & \(84.6 1.2\) & \(85.2 1.6\) & \(84.8 1.2\) \\ SeFT & \(81.2 2.3\) & \(41.9 3.1\) & \(73.9 2.5\) & \(31.1 4.1\) & \(67.1 2.2\) & \(70.2 2.4\) & \(68.2 1.5\) & \(68.5 1.8\) \\ mTAND & \(84.4 1.3\) & \(50.6 2.0\) & \(84.2 0.8\) & \(48.2 3.4\) & \(74.6 4.3\) & \(74.3 4.0\) & \(79.5 2.8\) & \(76.8 3.4\) \\ IP-Net & \(84.6 1.3\) & \(38.1 3.7\) & \(82.6 1.4\) & \(47.6 3.1\) & \(74.3 3.8\) & \(75.6 2.1\) & \(77.9 2.2\) & \(76.6 2.8\) \\ DGM2-O & \(86.7 3.4\) & \(44.7 1.1\) & \(84.4 1.6\) & \(47.3 3.6\) & \(82.4 2.3\) & \(85.2 1.2\) & \(83.9 2.3\) & \(84.1 1.8\) \\ MTGNN & \(81.9 6.2\) & \(39.9 8.9\) & \(74.4 6.7\) & \(35.5 0.0\) & \(83.4 1.9\) & \(85.2 1.7\) & \(86.1 1.9\) & \(85.9 2.4\) \\ Raindrop & \(87.0 2.3\) & \(51.8 5.5\) & \(82.8 1.7\) & \(44.0 3.0\) & \(88.5 1.5\) & \(89.9 1.5\) & \(89.9 0.6\) & \(89.8 1.0\) \\ 
**ViTST** & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 2: Comparison with the baseline methods on irregularly sampled time series classification task. **Bold** indicates the best performer, while underline represents the second best.

samples and compared methods, and (2) _leave-random-sensors-out_, which drops sensors randomly. It is important to note that only the observations in the validation and test sets were dropped, while the training set was kept unchanged. To ensure a fair comparison, we dropped the same set of sensors in the _leave-fixed-sensors-out_ setting as in .

The results are presented in Fig. 3, from which we observe that our approach consistently achieves the best performance and outperforms all the specialized baselines by a large margin. With the missing ratio increasing from 10% to 50%, our approach maintains strong performance, staying above 80%. In contrast, the comparison baseline shows a marked drop. The advantage of ViTST over the comparison baselines becomes increasingly significant. Even when half of the variables were dropped, our approach was still able to achieve acceptable performance over 80, surpassing the best-performing baseline Raindrop by 33.1% in Accuracy, 40.9% in Precision, 39.4% in Recall, and 42.8% in F1 score in the _leave-fixed-sensors-out_ setting (all in absolute points). We also notice that the variances in our results are notably lower compared to the baselines. These results suggest that our approach is highly robust to varying degrees of missing observations in time series.

### Additional Analysis

**Where does the performance come from?** Our approach transforms time series into line graph images, allowing the use of vision transformers for time series modeling. We hypothesize that vision transformers can leverage their general-purpose image recognition ability acquired from large-scale pre-training on natural images (such as ImageNet ) to capture informative patterns in the line graph images. To validate that, we compared the performance of a pre-trained Swin

Figure 4: Performance of different backbone vision models on P19, P12, and PAM datasets. We do not use static features for our approach here to exclude their influence.

Figure 3: Performance in leave-**fixed**-sensors-out and leave-**random**-sensors-out settings on PAM dataset. The x-axis is the “missing ratio” which denotes the ratio of masked variables.

Transformer with a Swin Transformer trained from scratch, as shown in Fig. 4. The significant drop in performance without pre-training provides evidence that Swin transformer could transfer the knowledge obtained from pre-training on natural images to our synthetic time series line graph images, achieving impressive performance. Nevertheless, the underlying mechanism needs further exploring and probing in future studies.

How do different vision models perform?We benchmarked several backbone vision models within our framework. Specifically, we tried another popular pre-trained vision transformer ViT3 and a pre-trained CNN-based model, ResNet4. The results are presented in Fig. 4. The pre-trained transformer-based ViT and Swin Transformer demonstrate comparable performance, both outperforming the previous state-of-the-art method, Raindrop. In contrast, the pre-trained CNN-based ResNet lagged considerably behind the vision transformer models. This performance gap is consistent with observations in image classification tasks on datasets like ImageNet, where vision transformers have been shown to excel in preserving spatial information compared to conventional CNN models. This advantage enables vision transformers to effectively capture the positions of patches within each line graph sub-image and the entire image and facilitates the modeling of complex dynamics and relationships between variables.

How to create time series line graph images?Using line graphs to visualize time series offers us an intuitive way to interpret the data and adjust the visualization strategy for enhanced clarity and potentially augment the performance. To offer insights on effective time series-to-image transformation, we analyze the effects of several key elements in practice: (1) the default linear _interpolation_ to connect partially observed data points on the line graphs; (2) _markers_ to indicate observed data points; (3) variable-specific _colors_ to differentiate between line graphs representing different variables; (4) the _order_ determined by missing ratios when organizing multiple line graphs in a single image.

The results are presented in Table 3. Given the balanced missing ratios in the PAM dataset, we excluded results without the sorting order. Interestingly, plotting only observed data points without linear interpolation led to better results on P19 and P12 datasets. This could be attributed to the potential inaccuracies introduced by interpolation, blurring distinctions between observed and interpolated points. Additionally, omitting markers complicates the model's task of discerning observed data points from interpolated ones, degrading its performance. The absence of distinctive colors led to the most significant performance drop, underlining the necessity of using varied hues for individual line graphs to help the model to distinguish them. While a specific sorting order may not ensure optimal outcomes across all datasets, it does provide relatively stable results over multiple datasets and evaluation metrics. For the PAM dataset, these nuances seem to have minimal impact, indicating the robustness of our approach against these variations in some scenarios.

Effects of grid layouts and image sizes.We explored the influence of grid layouts and image dimensions on our approach's efficacy. For a fair comparison across grid layouts, we fixed the size of each grid cell as \(64 64\) and altered the grid layouts. As shown in Figure 5. we observed our approach's robustness to variations in grid layouts, with square layout consistently yielding good results across different datasets and metrics, which was particularly evident for the P12 dataset. Regarding image size, when we maintained the grid layout but reduced the overall image dimensions, a noticeable performance decline was observed on the P12 and PAM datasets, which complies with our intuition.

Robustness against varied plotting parameters.To gauge the robustness of our approach against different plotting parameters, we assessed aspects including line style/width and marker style/size,

    &  &  &  \\   & AUROC & AUPRC & AUROC & AUPRC & Accuracy & Precision & Recall & F1 score \\  Default & \(89.2 2.0\) & \(53.1 3.4\) & \(85.1 0.8\) & \(51.1 4.1\) & \(95.8 1.3\) & \(96.2 1.1\) & \(96.2 1.3\) & \(96.5 1.2\) \\  w/o interpolation & \(89.6 2.1\) & \(52.9 3.4\) & \(85.7 1.0\) & \(51.9 3.4\) & \(95.6 1.1\) & \(96.6 0.9\) & \(95.9 1.0\) & \(96.2 1.0\) \\ w/o markers & \(89.0 2.1\) & \(51.7 2.5\) & \(85.3 0.9\) & \(50.3 3.2\) & \(95.8 1.1\) & \(96.9 0.7\) & \(96.0 1.0\) & \(96.4 0.9\) \\ w/o colors & \(88.8 1.8\) & \(51.4 4.1\) & \(84.4 0.7\) & \(47.0 2.9\) & \(95.0 1.0\) & \(96.2 0.7\) & \(95.3 1.0\) & \(95.7 0.9\) \\ w/o order & \(89.3 2.3\) & \(52.7 4.5\) & \(84.0 1.8\) & \(47.8 4.6\) & - & - & - & - \\   

Table 3: Ablation studies on different strategies of time series-to-image transformation.

primarily using the P19 dataset. As shown in Table 4, our approach demonstrates robustness against changes in these parameters, maintaining strong performance across different plotting configurations.

**What does ViTST capture?** To gain insights into the patterns captured by ViTST in the time series line graph images, we analyzed the averaged attention map of a ViTST model with ViT as the backbone, as depicted in Fig. 6. The attention map reveals that the model consistently attends to the informative part, i.e., line graph contours within the image. Furthermore, we observed that the model appropriately focuses on observed data points and areas where the slopes of the lines change. Conversely, flat line graphs that lack dynamic patterns seem to receive less attention. This demonstrates that ViTST might be able to discern between informative and uninformative features in the line graph images, enabling it to extract meaningful patterns.

### Regular Time Series Classification

An advantage of our approach is its ability to model time series of diverse shapes and scales, whether they are regular or irregular. To evaluate the performance of our approach on regular time series data,

   Line & Marker & AUROC & AUPRC \\  (solid,1) & \((*,2)\) & \(89.2 2.0\) & \(53.1 3.4\) \\ (dashed,1) & \((*,2)\) & \(89.2 2.1\) & \(53.7 4.1\) \\ (dotted,1) & \((*,2)\) & \(89.2 2.1\) & \(52.8 4.0\) \\ (solid,0.5) & \((*,2)\) & \(88.6 1.7\) & \(53.0 3.6\) \\ (solid,1) & \((*,2)\) & \(89.2 2.0\) & \(53.1 3.4\) \\ (solid,2) & \((*,2)\) & \(88.5 2.3\) & \(53.6 3.1\) \\ (solid,1) & \((*,2)\) & \(89.2 2.0\) & \(53.1 3.4\) \\ (solid,1) & \((,2)\) & \(89.3 1.9\) & \(52.6 4.0\) \\ (solid,1) & \((o,2)\) & \(89.1 1.9\) & \(51.3 4.2\) \\ (solid,1) & \((*,1)\) & \(88.2 1.4\) & \(52.1 4.5\) \\ (solid,1) & \((*,2)\) & \(89.2 2.0\) & \(53.1 3.4\) \\ (solid,1) & \((*,3)\) & \(88.9 1.9\) & \(52.8 3.2\) \\   

Table 4: Robustness regarding the style and size of lines and markers. In the brackets, the first element denotes style, and the second represents size.

Figure 5: Ablation study of the influence of grid layouts and image sizes. For instance, 4x9 (256x576) denotes a grid layout of 4\(\)9 with an image size of 256\(\)576 pixels.

Figure 6: Illustration of the averaged attention map of ViTST.

we conducted experiments on ten representative multivariate time series datasets from the UEA Time Series Classification Archive . These datasets exhibit diverse characteristics, as summarized in Table 5. It is worth noting that the PS dataset in our evaluation contains an exceptionally high number of variables (963), while the EW dataset has extremely long time series (17984). We specifically selected these two datasets to assess the effectiveness of our approach in handling large numbers of variables and long time series. We follow  to use these baselines for comparison: DTW\({}_{D}\) which stands for dimension-Dependent DTW combined with dilation-CNN , LSTM , XGBoost , Rocket , and a transformer-based TST  which operates on fully observed numerical time series.

The performance of our approach on regular time series datasets is consistently strong, as demonstrated in Table 5. With an average accuracy that is second-best and closely aligned with the top-performing baseline method TST, our approach showcases its competitive capabilities. Notably, it excels on challenging datasets PS and EW with massive variables and observation length. These results were achieved using the same image resolution (\(384 384\)) as the other datasets, indicating the effectiveness and efficiency of our approach. The ability of our approach to handle both irregular and regular time series data further emphasizes its versatility and broad applicability.

## 5 Conclusion

In this paper, we introduce a novel perspective on modeling irregularly sampled time series. By transforming time series data into line graph images, we could effectively leverage the strengths of pre-trained vision transformers. This approach is straightforward yet effective and versatile, enabling the modeling of time series of diverse characteristics, regardless of irregularity, different structures, and scales. Through extensive experiments, we demonstrate that our approach surpasses state-of-the-art methods designed for irregular time series and maintains strong robustness to varying degrees of missing observations. Additionally, our approach achieves promising results on regular time series data. We envision its potential as a general-purpose framework for various time series tasks. Our results underscore the potential of adapting rapidly advancing computer vision techniques to time series modeling.

## 6 Limitations and Future Work

In this work, we utilized a straightforward method to image multivariate time series by converting them into line graph images using matplotlib and then saving them as RGB images. While our results are promising and exhibit robustness against variations in the time series-to-image transformation process, there may be alternative ways to visualize the data. This includes potentially more controllable and accurate plotting methods or different image representations beyond line graphs. Our findings also highlight the efficacy of pre-trained vision transformers for time series classification, suggesting that these models might leverage knowledge acquired from pre-training on natural images. Yet, the underlying reasons for their remarkable success still need deeper exploration and investigation. This research serves as a promising starting point in this domain, suggesting various potential directions. We leave these further explorations and investigations for future work.

   Datasets & EC & UW & SCP1 & SCP2 & JV & SAD & HB & FD & PS & EW & Average \\   \\ \#Variables & 3 & 3 & 6 & 7 & 12 & 13 & 61 & 144 & **963** & 6 & - \\ Length & 1,751 & 315 & 896 & 1,152 & 29 & 93 & 405 & 62 & 144 & **17984** & - \\  \\ DTW\({}_{D}\) & 0.323 & 0.903 & 0.775 & 0.539 & 0.949 & 0.963 & 0.717 & 0.529 & 0.711 & 0.618 & 0.717 \\ LSTM & 0.323 & 0.412 & 0.689 & 0.466 & 0.797 & 0.319 & 0.722 & 0.577 & 0.399 & - & 0.523 \\ XGBoost & 0.437 & 0.759 & 0.846 & 0.489 & 0.865 & 0.696 & 0.732 & 0.633 & **0.983** & - & 0.727 \\ Rocket & 0.452 & **0.944** & 0.908 & 0.533 & 0.962 & 0.712 & 0.756 & 0.647 & 0.751 & - & 0.741 \\ TST & 0.326 & 0.913 & **0.922** & **0.604** & **0.997** & **0.998** & **0.776** & **0.689** & 0.896 & 0.748 & **0.791** \\ ViTST & **0.456** & 0.862 & 0.898 & 0.561 & 0.946 & 0.985 & 0.766 & 0.632 & 0.913 & **0.878** & 0.780 \\   

Table 5: Performance comparison on regular multivariate time series datasets. **Bold** indicates the best performer, while underline represents the second best.