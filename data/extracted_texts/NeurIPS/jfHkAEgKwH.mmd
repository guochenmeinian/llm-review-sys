# LocCa: Visual Pretraining

with Location-aware Captioners

 Bo Wan\({}_{1,3}\)1

Michael Tschannen\({}_{1}\)

Yongqin Xian\({}_{2}\)

Filip Pavetic\({}_{1}\)

Ibrahim Alabdulmohsin\({}_{1}\)

Xiao Wang\({}_{1}\)

Andre Susano Pinto\({}_{1}\)

Andreas Steiner\({}_{1}\)

Lucas Beyer\({}_{1}\)

Xiaohua Zhai\({}_{1}^{}\)

\({}^{1}\)Google DeepMind, Zurich

\({}^{2}\)Google, Zurich

\({}^{3}\)KU Leuven

###### Abstract

Image captioning was recently found to be an effective pretraining method similar to contrastive pretraining. This opens up the largely-unexplored potential of using natural language as a flexible and powerful interface for handling diverse pretraining tasks. In this paper, we demonstrate this with a novel visual pretraining paradigm, LocCa, that incorporates location-aware tasks into captioners to teach models to extract rich information from images. Specifically, LocCa employs two tasks, bounding box prediction and location-dependent captioning, conditioned on the image pixel input. Thanks to the multitask capabilities of an encoder-decoder architecture, we show that an image captioner can effortlessly handle multiple tasks during pretraining. LocCa significantly outperforms standard captioners on downstream localization tasks, achieving state-of-the-art results on RefCOCO/+/g, while maintaining comparable performance on holistic tasks. Our work paves the way for further exploration of natural language interfaces in visual pretraining.

## 1 Introduction

Remarkable progress has been made in large-scale visual pretraining , where vision models are pretrained on large-scale annotated datasets  with a supervised classification loss. Yet, the manual annotation required for such datasets is time-consuming and costly, posing a challenge to scalability.

In light of this, the modern contrastive pretraining methods  extract learning signals from web-crawled image-text pairwise datasets , circumventing the need for extensive manual annotations. The contrastively pretrained models demonstrate remarkable capability on zero-shot transfer tasks, especially on downstream applications that require fine-grained visual  or textual understanding . More recently, image captioning has been shown as an alternative visual pretraining task to learn capable vision encoders , where an encoder-decoder architecture is pretrained to generate text captions from the image input. Some studies, such as , pioneered the joint pretraining of contrastive and generative methods. Typically, encoded image features are fed into two parallel branches: one employs a text encoder to produce sentence embeddings for contrastive learning, while the other utilizes a text decoder to generate image captions. Despite the effectiveness of these works, they typically focus on a holistic understanding of images, often overlooking the region-specific details of the visual content.

The recent success of image captioning  and the advancements in multitasking learning of decoders  opens up the largely-unexplored potential of using natural language as aflexible and powerful interface for handling diverse tasks. We demonstrate this with a novel visual pretraining paradigm, LocCa, that enhances the visual representation with location-aware context. Works including [20; 21; 22] investigate the matching of image regions with corresponding text during pretraining. The central concept involves extracting Region of Interest (RoI) features from image embedding to facilitate contrastive learning with corresponding textual features. These approaches yield encouraging outcomes in location-sensitive tasks, such as object detection [23; 24; 25; 26] and referring expression [27; 28; 29; 30]. However, they require complex model architectures (e.g. RPN  and FPN ) for RoI generation. Also, given the presence of multiple object candidates within an image, region-wise matching becomes computationally demanding.

By contrast, LocCa is a simple yet effective location-aware captioning pretraining method as shown in Figure 1, which uses an autoregressive decoder as an interface to handle additional location-aware pretraining tasks. Concretely, other than the image-to-text captioning task, LocCa also pretrains the model with two location-aware tasks: (i) _automatic referring expressions_, which amounts to predict bounding box coordinates from automatically generated captions for specific image regions, and (ii) _grounded captioning_ to jointly predict box coordinates and captions from the image. Specifically, LocCa leverages a multi-task decoder  for pretraining, where the model outputs are conditioned on the task prefixes for each task. Thanks to the shared vision transformer for multiple tasks, the additional localization losses are relatively cheap to compute, while the model inference speed is identical to the standard image caption pretrained models.

Our experimental results show that the LocCa performs significantly better on downstream tasks that require localization capabilities, while maintaining the same level of capabilities on holistic tasks. We summarize our contributions as follows: (i) For the first time we explore location-aware tasks as proxies for generative visual pretraining (as opposed to transfer/instruction tuning in prior works), enabling flexibly customized inference (detailed in Sec.3.3); (ii) Without bells and whistles, LocCa achieves state-of-the-art results on localization tasks, while preserving the competitive performance on holistic tasks; and (iii) When integrated in vision-language models, i.e. PaLI-3 , the vision encoder outperforms strong SigLIP baselines .

## 2 Related Works

Contrastive visual pretraining is a prominent direction in training vision and vision-language foundation models. Early works [32; 33; 34; 35] explore image-only contrastive loss by matching different views of the same image in the self-supervised learning setting. In vision-language model pretraining, CLIP  and ALIGN  show that a two-tower architecture trained with the contrastive objective on noisy image-text pairs can learn highly transferable image and text representations for various

Figure 1: **Overview of LocCa. LocCa consists of a standard vision transformer and a transformer decoder. The vision transformer takes image pixel as input, produces visual tokens as cross attention input to the transformer decoder. The transformer decoder is trained to read out rich information from the visual tokens. We adopt the following three task for pretraining: Cap, AREF and GCAP.**

downstream tasks. There have been many follow-up works [10; 36; 13; 37] that further improve the zero-shot image classification and retrieval performance. Notably, [20; 21] propose to incorporate location cues by contrastively aligning image regions and text phrases. In contrast, our work focuses on learning a location-aware vision-language model with a generative loss.

A natural alternative to contrastive pretraining is image captioning: Rather than matching image and text embeddings, one tries to predict captions from an image embedding. Early works investigate this approach at small scale [38; 39; 40; 41]. Later works augment large-scale contrastive pretraining with a captioning loss [15; 16; 42], or scale-up captioning as a stand-alone task without investigating transfer to a broad set of vision tasks [43; 44].  recently showed that image captioning alone leads to visual representations competitive with contrastive pretraining.

Many recent large multimodal models are trained with a mixture of tasks [45; 18; 19; 46; 10; 47; 31; 48; 49]. Among the most popular types of tasks are those which can be formulated by mapping an image to a text string, including captioning, detection, and VQA [50; 45; 18; 19; 10; 47; 31; 48]. Another popular group of tasks are dense prediction tasks such as segmentation and depth prediction [19; 46]. While several studies have enhanced model pretraining by incorporating location information, their methodologies primarily leverage either pretrained language decoders [18; 51; 52] or pretrained cross-modal encoder-decoder  to integrate vision and language features for multitasking purposes, often neglecting the independent significance of visual pretraining from scratch. Furthermore, there is a trend of towards co-training on images, video, and audio [53; 54; 55], highlighting the multifaceted nature of current multi-modal research. Crucially, essentially all of these works rely on pretrained vision and language backbones, and merely fine-tune those together on the described tasks. Here, by contrast, we use multi-task pretraining to train visual encoders from scratch.

## 3 Location-aware Captioner

In this section, we introduce the location-aware image captioner LocCa for multitask pretraining. LocCa builds on an image captioner but provides a recipe for integrating location-aware information during model pretraining.

### Pretraining tasks

The pretraining phase of LocCa draws inspiration from pioneering works that have successfully integrated a unified decoder for multitasking based on pretrained models [18; 10; 19; 46; 52], utilizing a task-specific prefix for each distinct task. This enhances the model's ability to handle multiple tasks concurrently.

For conventional image captioning, the process involves taking an image \(x\) as input and generating a sequence of text tokens \(y=[y_{1},y_{2},,y_{n}]\). In the LocCa framework, a task-specific prefix, labeled as "_Cap_:", is added to the beginning of the caption sequence to designate the task at hand. Moreover, LocCa integrates two additional location-aware tasks during its pretraining phase: _automatic referring expression_ (AREF) and _grounded captioning_ (GCAP). These tasks are inspired by referring expression comprehension [27; 28; 29; 30] and dense captioning [56; 57; 58; 59; 60] respectively. The key difference is that LocCa predicts both regional captions and box coordinates sequentially with task prefixes, instead of relying on either caption or box conditional inputs (see Fig. 1).

The foundation of LocCa's pretraining is built upon dense, automatically generated region annotations. Each image \(x\) is associated with a comprehensive set of annotations \(\{\{b,c\}\}\), where \(b^{4}\) denotes the bounding box coordinates, and \(c\) represents the corresponding textual descriptions or labels. For every bounding box, two distinct prompts are generated to cater to the aforementioned location-aware tasks: "_ARef_: \(\{c\}:\{b\}\)" for automatic referring expression and "_GCap_: \(\{b\}:\{c\}\)" for grounded captioning, each prefixed with "_ARef_:" and "_GCap_:" respectively. These prompts are then tokenized to produce the sequence \(y\) for each task, facilitating pretraining with a text interface.

For each image, LocCa utilizes the same visual features extracted by the image encoder and performs three tasks using the same decoder in parallel. This pretraining scheme aims to make LocCa adept at linking fine-grained regional visual elements with appropriate textual descriptions.

### Model details

ArchitectureLocCa utilizes a conventional encoder-decoder framework, where the encoder comprises a Vision Transformer  to transform the input image \(x\) into a sequence of feature embeddings. The decoder, built on a Transformer architecture , processes these image features, employing cross-attention across each layer to integrate visual and textual information effectively.

Autogressive decodingIn the decoding stage, LocCa uses causal attention masks  to guide the prediction of each token in the sequence, ensuring that each token is generated based on the ones before it, in a step-by-step manner. This setup helps in creating coherent and context-aware captions, drawing from the visual cues encoded earlier and maintaining a logical flow in the generated text.

Parallel predictionInspired by , LocCa also adopts parallel prediction for a fraction of the training examples (concretely 50%) in the standard image captioning task. This technique requires the model to predict the caption tokens independently in parallel, focusing exclusively on visual information to prevent the model from relying on preceding text tokens to predict the following ones. This strategy was shown to improve the learned representation on a range of tasks .

ObjectiveThe optimization of LocCa's parameters \(\) is achieved through the maximization of the log-likelihood: \(_{i=1}^{|y|} P_{}(y_{i}|y_{<i},x)\). It is important to emphasize that, in the learning process for the location-aware tasks, LocCa is structured to predict captions and bounding boxes sequentially, contrasting with traditional approaches that might predict a caption based on a given bounding box or vice versa [18; 51]. This is achieved by applying losses to the entire prompt excluding the task prefix. Taking the AREF task as an example, the overall loss is computed for both textual predictions \(c\) and box coordinates \(b\). The loss on \(c\) guides the model to identify a foreground region and generate its caption, while the loss on \(b\) aims at refining the model's ability to accurately regress the box location relative to the caption.

### Discussion

To the best of our knowledge, LocCa is the first end-to-end method that incorporates the location-aware tasks (i.e., AREF and GCAP) into generative VLM _pretraining_. Our key novelty lies in the formulation of the location-aware proxy tasks which allows for scalable pretraining and enhances the visual localization capabilities. Compared to [22; 18; 51; 52] that require either a pretrained visual encoder or language decoder, LocCa does not need any pretrained model for initialization. Compared to [22; 62] that employ complex architectures with multiple losses, LocCa adopts a simple encoder-decoder architecture with a single generative loss. Compared to [18; 19] that are pretrained on a large number of tasks, LocCa introduces the novel use of simple AREF and GCAP proxy tasks for visual pretraining.

The dual-faceted loss structure of AREF and GCAP achieves comparable results to directly adopting the traditional referring expression and dense captioning tasks. However, it enhances inference flexibility of LocCa, allowing for varied input configurations. For instance, users can input a single task prefix (e.g., "_ARef_:") to prompt the model to identify and describe an area of interest along with its location. Alternatively, by inputting both the task and a conditional input (e.g., "_ARef_: a black and white cat :"), LocCa can be directed to focus solely on predicting the location. This flexibility allows for customized responses to various inquiries, highlighting the model's adaptability to meet specific user needs.

## 4 Experiments

### Experimental setup

Pretraining datasetWe use a subset of the WebLI dataset  corresponding to English websites and apply text-based filtering  to obtain 1B image/alt-text pairs. The WebLI data was de-duplicated w.r.t. all the images in the evaluation data sets used in this paper. To obtain fine-grained object locations, a publicly available OWL-ViT CLIP L/14 model  is applied to generate detection pseudo annotations. Specifically, two groups of box categories are generated: the n-gram texts from alt-text and the object categories as used by PaLI , more details can be found in . In LocCa pretraining, we first filter the candidate bounding boxes according to their OWL-ViT confidence scorewith a threshold of 0.3, and then randomly sample one box-caption or box-category pair for referring expression and grounded captioning separately.

**Baselines** Our main baselines are CLIP-style contrastively pretrained dual-encoder models [7; 8] on our dataset (referred to as CLIP* to differentiate from the model released by ), as well as the captioning-pretrained encoder-decoder models from . For both types of models we follow the exact training recipe from . For the captioning-based pretraining we consider standard autoregressive captioning (Cap) as well as the variant with parallel prediction (CapPa). This variant removes the decoder attention mask for a fraction of the training examples and replaces the decoder input, which corresponds to the right-shifted output sequence during autoregressive training, with a sequence of all mask tokens. Parallel prediction  improves the representation learning capabilities captioning-based pretraining at no extra computation cost.

**Implementation details** LocCa adopts an encoder-decoder structure where, unless otherwise noted, the encoder defaults to a standard ViT-L/14 design with 24 transformer blocks handling input image patches of size 14. The decoder, following , is a Transformer-L model consisting of 12 transformer decoder blocks. In total, the model comprises approximately 600M parameters. Two more LocCa setups with ViT-B/16 and ViT-G/14 (see  for model specifications) are adopted for ablation tests and scaling experiments respectively.

**Pretraining details** LocCa is pretrained for about 9 billion image/alt-text seen examples, which corresponds to about 9 epochs on our tailored subset of WebLI. For the optimizer, we employ the Scaling-ViT AdaFactor variant , combined with a cosine schedule that includes 10,000 warmup steps. The batch size is set at 8,192, while the learning rate and decay factor are adjusted to \(10^{-3}\) and \(10^{-4}\), respectively. During this process, images are uniformly resized to a resolution of 224 x 224 pixels. Alt-texts are tokenized into a vocabulary consisting of 32,000 tokens using a sentence piece model trained on the English segment of C4 , with a cap on the sequence length at 64 tokens. We represent bounding box coordinates using up to 500 integral numbers, which are then directly converted into strings for straightforward representation of coordinate tokens. For parallel prediction in the vanilla image captioning task, the fraction of examples is set to 50% by default. The pretraining of LocCa\({}_{L}\) takes 153 hours using 256 TPUv3 chips.

**Downstream tasks** Our goal with LocCa is to preserve or even improve the capability on various image-level understanding tasks, and get a higher performance on fine-grained location-aware tasks. To this end, we assess the capabilities of LocCa in holistic and location-aware image understanding tasks across a range of downstream tasks. In the realm of holistic image understanding, we focus on the same LiT-Decoder tasks  in CapPa  including image classification (CLS)[5; 66; 67; 68; 69], image captioning (CAP) [70; 71], optical character recognition (OCR-VQA) , visual question answering (VQA) , and graph question answering (GQA) . For evaluating location-aware image understanding, we choose two widely recognized tasks like referring expression comprehension (REC) , referring expression segmentation (RES)  and object detection (OD) . Additionally, paralleling the approaches of PaLI [10; 31], we integrate LocCa to a pretrained large language model to assess performance on a variety of vision-language tasks, including image captioning and visual question answering. Notably, despite LocCa not being exposed to any video content during pretraining, we adapt it to various video-related tasks, such as video captioning, QA, and classification. This adaptation aims to assess its generalization capabilities to new modalities, demonstrating its versatility compared to other image-text pretraining approaches. We adopt different strategies for transferring LocCa to downstream tasks, as summarized in Appendix A.1.

### Quantitative results

We conduct extensive experiments to evaluate LocCa. The integration of location-aware cues enables LocCa to maintain its performance on holistic image understanding tasks while achieving substantially improved outcomes on location-aware tasks. Further enhanced by an advanced pretrained large language model, LocCa exhibits exceptional performance across a range of vision-language tasks, substantially surpassing baseline models.

**Referring Expression Comprehension** In this section, we present results on the referring expression comprehension benchmarks, including RefCOCO, RefCOCO+ and RefCOCOg . As shown in Table 1, LocCa establishes a new state-of-the-art across these benchmarks. This advancement is particularly noteworthy considering the inherent limitations of previous methods. For example,UNINEXT , which adopts a Deformable DETR architecture  tailored for detection-based tasks, and OFA, which requires a pretrained BART  for initialization, both achieve good results but are constrained by their specialized setups. Besides, some location-aware VLMs, such as Shikra  and Ferret , require an LLM for knowledge-based reasoning, which increases inference costs. In contrast, LocCa employs a standard encoder-decoder architecture with auto-regressive pretraining from scratch, significantly outperforming these methods across all benchmarks without the need for complex task-specific adaptations.

Achieving good performance on RefCOCO usually requires pretraining at high image resolutions. For instance, OFA\({}_{L}\) is pretrained at 480\({}^{2}\)px, while UNI-NEXT undergoes multi-scale pre-training. We opt for a standard 224\({}^{2}\)px pretrain resolution for simplicity. We transfer the 224\({}^{2}\)px pretrained model to RefCOCO by fine-tuning with 640\({}^{2}\)px resolution, using learning rate \(10^{-4}\) and no weight decay. We report the standard metric Acc@0.5 on the validation and test sets.

Notably, we train on the combined training sets of RefCOCO, RefCOCO+ and RefCOCOg but _removing all validation and test images_ from this combination. The splits of these three datasets are largely overlapping, meaning that methods trained on the combined training sets without de-duplication, a recently common phenomenon, have trained on about half of the test images, see Appendix A.3 for details. We provide the list of removed image IDs in the Appendix A.3. Furthermore, some models such as UNITER , VILLA , and MDETR  use COCO pre-trained detection components which have seen test images from the three RefCOCO versions. We have removed all training, validation, and test images from the COCO dataset from our pre-training data, as well as near-duplicates thereof. Hence, we group models reported in Table 1 into three distinct categories. We transfer LocCa on both the full and the "clean" combined training set and provide both results. We hope that, going forward, the community evaluates models on RefCOCO in this clean setup.

Moreover, as shown in Table 2, LocCa improved significantly over all the baselines of image-text pretrained models. To ensure a fair comparison with contrastive baselines (i.e. CLIP) which lack a text decoder for box prediction, we employ the LiT-Decoder  setup for comparison on RefCOCO benchmarks. It involves freezing the pretrained image encoder while training a text decoder from scratch with a small resolution of 224\({}^{2}\)px. This setup is necessary to properly compare with CLIP-style models without a decoder. The performance improvements attributable to location-aware pretraining are evident, demonstrating the enhanced sensitivity of visual encoder to object regions, which is crucial for excelling in referring expression tasks.

    &  &  &  \\   & val & testA & testB & val & testA & testB & val-u & test-u \\   \\  PixelLLM & 89.8 & 92.2 & 86.4 & 83.2 & 87.0 & 78.9 & 84.6 & 86.0 \\ UniTAB & 88.59 & 91.06 & 83.75 & 80.97 & 85.36 & 71.55 & 84.58 & 84.70 \\ OFA\({}_{L}\) & 90.05 & 92.93 & 85.26 & 85.80 & 89.87 & 79.22 & 85.89 & 86.55 \\ UNINEXT\({}_{L}\) & 91.43 & 93.73 & 88.93 & 83.09 & 87.90 & 76.15 & 86.91 & 87.48 \\ LocCa\({}_{L}\) & **91.94** & **94.56** & **89.13** & **86.47** & **91.67** & **80.43** & **87.46** & **87.95** \\  OFA\({}_{H}\) & 92.04 & 94.03 & 88.44 & 87.86 & 91.70 & 80.71 & 88.07 & 88.78 \\ UNINEXT\({}_{H}\) & 92.64 & 94.33 & **91.46** & 85.24 & 89.63 & 79.79 & 88.73 & 89.37 \\ ONE- & 92.58 & 94.18 & 89.26 & 88.77 & 92.21 & 83.23 & 89.22 & 89.27 \\ PACE\({}_{1.5B}\) & & & & & & & & \\ Shikra\({}_{13B}\) & 87.83 & 91.11 & 81.81 & 82.89 & 87.79 & 74.41 & 82.64 & 83.16 \\ Ferret\({}_{13B}\) & 89.48 & 92.41 & 84.36 & 82.81 & 88.14 & 75.17 & 85.83 & 86.34 \\ LocCa\({}_{G}\) & **92.99** & **95.02** & 90.48 & **89.12** & **92.87** & **83.55** & **89.24** & **89.90** \\   \\  UNITER\({}_{L}\) & 81.41 & 87.04 & 74.17 & 75.90 & 81.45 & 66.70 & 74.86 & 75.77 \\ VILLA\({}_{L}\) & 82.39 & 87.48 & 74.84 & 76.17 & 81.54 & 66.84 & 76.18 & 76.71 \\ MDETR & 86.75 & 89.58 & 81.41 & 79.52 & 84.09 & 70.62 & 81.64 & 80.89 \\   \\  RefTR & 85.65 & 88.73 & 81.16 & 77.55 & 82.26 & 68.99 & 79.25 & 80.01 \\ LocCa\({}_{L}\) & 89.70 & 92.75 & 85.30 & 83.85 & 89.40 & 76.76 & 84.62 & 85.86 \\ LocCa\({}_{G}\) & **91.20** & **93.34** & **87.56** & **86.89** & **90.71** & **80.73** & **87.34** & **87.90** \\   

Table 1: Result comparison with previous SOTA methods on RefCOCO benchmarks.

Referring Expression SegmentationFor referring expression segmentation, we extend the REC task by adding a suffix "_Mask_:" to the text, followed by the indexes of segmentation tokens. The segmentation tokens specify the precise shape of the segmentation mask within the bounding box that is identified during the REC task. This process leverages a pretrained VQ-VAE  to convert the semantic masks to tokens, please refer to Appendix A.3 for more details.

LocCa is adapted to RES under the same settings as in REC, using the "clean" combined training sets of RefCOCO, RefCOCO+, and RefCOCOg (c.f. Appendix C). Specifically, we compare different frozen encoders and train a decoder from scratch. As shown in Table 2, LocCa's vision encoder outperforms other encoders substantially, thereby providing further validation of its location-wise sensitivity. Moreover, we employ the full encoder-decoder LocCa model and fine-tune it for RES. Notably, LocCa achieves competitive results even compared to the state-of-the-art PaLI-3 model, albeit with considerably fewer parameters (0.6B vs. 5B). See Appendix A.3 for details.

Holistic Image UnderstandingWhile being great on the referring expression comprehension tasks, we also verified that LocCa performs equally well on the holistic image understanding tasks. Interestingly, we found that LocCa outperforms the image-text pretrained baselines on the object-centric tasks like VQAv2 and GQA.

Following the similar evaluation protocol in , we evaluate the capability of LocCa with the "LiT decoder"  setup, to investigate the adaptation capability of the learned representations to interface with a text decoder. Here we report the classification accuracy on 5 classification (CLS) datasets (ImageNet-1k , Sun-397 , Food-101 , Resisc-45 , Oxford-Pet ), and also answer accuracy on VQAv2 and GQA and OCR-VQA datasets. Besides, we report the CIDEr score on 2 captioning (CAP) datasets (COCO  and Flickr ).

As shown in Table 3, the performance of LocCa is better than image-text pretrained baselines on image captioning, VQA and GQA, comparable on image classification, and slightly lags on OCR

    &  &  &  \\   & & val & testA & testB & val & testA & testB & val-u & test-u \\   & CLIP  & 65.21 & 71.28 & 58.17 & 57.53 & 66.44 & 47.77 & 59.32 & 60.24 \\  & CLIP* & 58.28 & 63.59 & 53.73 & 49.01 & 55.94 & 41.96 & 55.70 & 55.88 \\  & Cap  & 60.64 & 65.47 & 56.17 & 52.56 & 58.32 & 45.99 & 56.75 & 57.99 \\  & CapPa  & 64.17 & 69.90 & 58.25 & 56.14 & 63.68 & 48.18 & 58.90 & 59.91 \\  & LocCa & **88.34** & **91.20** & **85.10** & **79.39** & **85.13** & **72.61** & **81.69** & **82.64** \\   & CLIP  & 36.15 & 38.25 & 35.82 & 31.40 & 36.00 & 28.69 & 29.21 & 29.44 \\  & CLIP* & 32.78 & 34.89 & 33.03 & 27.49 & 30.14 & 25.07 & 26.99 & 26.83 \\   & Cap  & 34.84 & 37.68 & 35.39 & 31.93 & 35.24 & 28.79 & 28.84 & 29.11 \\   & CapPa  & 36.62 & 39.23 & 36.67 & 32.54 & 37.49 & 29.59 & 30.40 & 30.43 \\   & LocCa & **64.98** & **65.39** & **64.09** & **57.85** & **60.92** & **52.72** & **55.84** & **56.95** \\   

Table 2: Comparison with baselines on RefCOCOs. A randomly initialized decoder is trained for REC and RES, with frozen image encoders. We report Acc@0.5 for REC and mIoU for RES. Here CLIP uses model checkpoints released by ; all other baselines use the same data as LocCa.

    &  &  & OCR &  \\   & i1k & sun & food & res & pet & COCO & Flickr & VQA & VQAv2 & GQA \\  CLIP  & 84.8 & 84.8 & 95.2 & 96.3 & 95.4 & 124.4 & 87.1 & 64.1 & 70.4 & 58.7 \\ CLIP* & 84.7 & **85.7** & 94.6 & **96.4** & 95.2 & 123.2 & 85.5 & 61.3 & 68.5 & 55.3 \\ Cap  & 83.8 & 84.7 & 93.4 & 95.1 & 95.0 & 125.9 & 88.3 & 64.2 & 70.9 & 58.5 \\ CapPa  & 84.4 & 84.9 & 93.8 & 96.0 & **95.6** & 125.8 & 89.3 & **65.6** & 70.9 & 58.3 \\ LocCa & 84.5 & 84.9 & 93.9 & 96.0 & 95.0 & **127.1** & **90.7** & 64.5 & **72.8** & **61.8** \\  LocCa\({}_{G}\) & 85.8 & 85.1 & 95.4 & 96.1 & 95.8 & 130.9 & 92.6 & 67.6 & 73.9 & 61.5 \\   

Table 3: Results on holistic image understanding tasks. Here CLIP uses model checkpoints released by ; all other baselines are trained on the same data as LocCa.

VQA. Notably, both VQA and GQA necessitate fine-grained instance-level comprehension, focusing on the spatial and semantic relationships between objects to accurately provide the correct answer. For example, GQA involves complicated information about objects, attributes and relations provided by scene graphs . Such an observation further reveals the advantage of LocCa on fine-grained object-level sensitivity.

It is also interesting to investigate the full potential of the LocCa model by fine-tuning it on holistic tasks with a small \(3 10^{-6}\) learning rate without weight decay. To this end, we choose the widely used COCO image captioning task as an example. LocCa achieves competitive results of 138.0 CIDEr score with a standard 224\({}^{2}\)px. When increasing the transfer resolution to 640\({}^{2}\)px, the LocCa\({}_{L}\) model achieves **140.3** CIDEr score without CIDEr metric optimization .

Vision-Language Models with LocCaIn this section, we present results on vision-language tasks with LocCa plugged into a pretrained large language model. More specifically, we use PaLI-3  here to test the vision encoder quality. Importantly, we discovered that the generative LocCa vision backbone outperforms the SigLIP backbone, which is the default setup used in PaLI-3.

We select a wide range of tasks to assess the models' proficiency in understanding various visual concepts, including: image scene (COCO Caption), objects (VQAv2 and TallyQA ), visually-situated text (TextVQA , ST-VQA ) and general knowledge (OKVQA ). Remarkably, as shown in Table 4, LocCa consistently surpasses the Cap and CapPa vision encoders by a significant margin across all these tasks. Note that SigLIP\({}_{L}\) in Table 4 uses image patch size 16 while all the other models use patch size 14. SigLIP\({}_{L}\) is marked grey because it's not directly comparable. The LocCa\({}_{G}\) model consistently outperforms SigLIP\({}_{G}\) across all the tasks. With the knowledge of object and textual regions, LocCa is better positioned to understand more challenging and subtle

  
**MODEL** & COCO & VQAv2 & OKVQA & TextVQA & ST-VQA & TallyQA \\  SigLIP\({}_{L}\) & 135.8 & 75.6 & 57.5 & 41.1 & 46.2 & 74.9/61.4 \\ Cap\({}_{L}\) & 135.0 & 75.3 & 57.5 & 44.6 & 44.5 & 73.2/62.0 \\ CapPa\({}_{L}\) & 135.3 & 75.5 & 57.7 & 44.0 & 45.2 & 73.4/60.9 \\ LocCa\({}_{L}\) & **138.9** & **77.6** & **58.4** & **49.2** & **50.9** & **79.3/64.1** \\ SigLIP\({}_{G}\) & 140.3 & 77.5 & 58.6 & 50.6 & 50.5 & 76.3/61.8 \\ LocCa\({}_{G}\) & **140.9** & **78.1** & **59.8** & **52.1** & **52.3** & **79.0/64.2** \\   

Table 4: Results on PaLI-3  transfers to diverse captioning and question answering tasks. LocCa consistently outperforms other image encoders, especially on tasks requiring understanding of objects, including both natural and text (OCR) objects.

concepts like counting, visual relations, and word characters. Further fine-tuning details and the hyperparameters we used are provided in Appendix A.4.

**Object Detection** To evaluate for object detection, we use COCO detection dataset and follow  which models the task with an encoder-decoder model that outputs sequences of bounding boxes similar to pix2seq. The model is trained in two phases, first it maximizes the log-likelihood of generating the ground truth sequences of boxes, secondly it uses reinforce to tune the model for a reward related to the mAP metric.

Both  pretrain their encoder-decoder model in Objects365  and we found this to be critical. In particular the task output format is different from the ones LocCa used during pretraining and the size of COCO is too small to learn it without overfitting. To measure the performance of the encoders, we opt to initialize the decoder from an Objects365 pretrained model. More details of adapting LocCa for object detection please refer to Appendix A.5.

We present the comparative results in Figure 2, where LocCa significantly outperforms all image-text pretrained baselines in both mAP and AR, at both stages -- before and after the application of reinforcement tuning. This performance aligns with our expectations regarding LocCa's object-centric comprehension, attributed to the integration of additional location-aware pretraining.

**Zero-shot Transfer** Following locked-image tuning , we freeze the pre-trained vision encoder and train a text encoder contrastively to perform zero-shot classification and zero-shot retrieval on downstream tasks. With an L/14 architecture and 3B examples LiT-tune duration, LocCa\({}_{L}\) achieves 77.1% 0-shot accuracy on ImageNet, which is competitive to the same size CapPa\({}_{L}\) model's 76.4% accuracy. On COCO retrieval tasks, LocCa\({}_{L}\) achieves 46.6% and 64.6% on text-to-image and image-to-text retrieval tasks respectively. It outperforms CapPa\({}_{L}\)'s 43.9% and 60.3% COCO retrieval results by a large margin. Please refer to Appendix A.6 for more details.

**Semantic Segmentation** We investigate the dense feature learning capabilities of LocCa by evaluating it for semantic segmentation on ADE20k  along with Cap and CapPa. To this end, we use the Segmenter framework  which attaches a linear layer to every patch embedding to predict the semantic label of that patch, and obtains the high-resolution semantic map by upsampling the low-resolution map. The results in Table 5 show that LocCa outperforms Cap and CapPa by about 2 mIoU points, and the Seg ViT-L baseline which is based on an ImageNet-21k-pretrained ViT-L by about 1 point. The transfer details are provided in Appendix A.7.

**Video Understanding** We follow PaLI-3  to evaluate LocCa and CapPa on video understanding tasks. Specifically, we use the image encoder to process each frame separately and concatenate all resulting tokens. A LiT-Decoder  is then tuned on a mixture of six video understanding tasks. Despite no video has been seen during pretraining, LocCa showcases the video understanding capabilities and outperforms CapPa on most of the datasets. In particular, for the video captioning task on VATEX  dataset, LocCa achieves a CIDER score of 65.0 vs 64.0 of CapPa. For the video QA task on MSVD  dataset, LocCa obtains an accuracy of 50.9 vs 50.0 of CapPa. More details could be found from Appendix A.8.

### Qualitative Results

In this section, we discuss the raw LocCa model's zero-shot capability to detect multiple objects using its own decoder, despite only a single object per example being observed during pretraining. To achieve this, we employ beam search when decoding the output from LocCa. This involves using the prompt of a single task prefix "_GCap_:" to instruct LocCa to predict the RoIs along with their labels. As shown in Appendix Fig. 6, we observe that depending on the setting we get lower number of bounding boxes with correct class names, or higher number of boxes with class names which start to contain noise. We provide more discussions in Appendix B. Nevertheless, pre-trained LocCa model shows powerful capability after a short finetuning on a clean downstream dataset as shown in Table 1. Finding a decoding strategy which can output high number of boxes and quality labels at the same time is an open problem.

  
**MODEL** & mIoU \\  Seg ViT-L  & \(50.71\) \\ Cap & \(49.82^{ 0.6}\) \\ CapPa & \(49.92^{ 0.1}\) \\  LocCa & \(51.81^{ 0.3}\) \\   

Table 5: Fine-tuning vision backbones with a linear head  for semantic seg. on ADE20k.

### Ablations

**Pretraining and transfer resolutions**  We present results with different pretrain resolution and transfer resolution combinations in Figure 3 (a). To get a LocCa model with higher 384 pretrain resolution, we finetune the 224 resolution model using the target 384 resolution for 900M examples seen on the same dataset. In Figure 3 (a), we transfer both the 224 resolution model and the 384 resolution model to RefCOCOs using different transfer resolutions (224, 384, 640) as marked on the x-axis. We find that the higher pretrain model achieves slightly higher or competitive results compared to the 224 resolution models. We opt for the 224 resolution pretrain models by default in this paper for simplicity. More results with high pretrain resolution could be found in the Appendix A.10.

**Coordinate tokenization**  Previous studies [18; 51] often tokenize object coordinates by adding a location vocabulary. In contrast, LocCa simplifies this process by directly converting integral coordinates into textual strings, which are then tokenized with the same text tokenizer. This section presents an ablation study on the RefCOCO benchmarks to examine the differences between these two options. As shown in Figure 3 (b), both tokenization strategies for box coordinates yield remarkably similar results. However, our approach is notably simpler and more straightforward.

**Selection of pretraining tasks**  To evaluate the importance of the selected tasks for pretraining LocCa, an ablation study was conducted focusing on the effects of AREF and GCAP tasks. Specifically, the study explored the impact on LocCa by removing these tasks individually and collectively, with the model defaulting to the CapPa baseline when both are excluded. All these models are pretrained on the WebLI split for 900M examples with the resolution of 224\({}^{2}\)px, and subsequently evaluated on the holistic tasks using a LiT-Decoder setup and on RefCOCOs by fine-tuning the whole model. As shown in Table 6, incorporating any location-aware task into the pretraining of LocCa yields significant performance improvements, particularly evident in the RefCOCO results. Interestingly, incorporating solely the GCAP task leads to a marked improvement in RefCOCO performance compared to the CapPa baseline, despite GCAP not being directly aligned with the AREF task. This highlights the importance of introducing object concepts for enhancing regional-level understanding, which is beneficial for fine-grained visual comprehension and applicable to other object-sensitive tasks. Furthermore, the combined inclusion of both AREF and GCAP tasks yields even better results, demonstrating their complementary nature in improving LocCa's performance.

## 5 Conclusion

LocCa introduces a novel visual pretraining paradigm, using natural language interface to construct the proxy location-aware pretrain tasks. This model excels in understanding both the overall scene and specific spatial details, setting new performance standards on location-aware tasks while preserving the capability on holistic image understanding. LocCa simplifies the process of blending location information with visual data, offering significant improvements on tasks requiring detailed spatial awareness. Our contributions pave the way for advanced model capabilities in processing a broad spectrum of vision-language tasks, demonstrating LocCa's versatility and effectiveness.

LocCa already demonstrates superior zero-shot detection capability in qualitative results. However, it lacks the capability for zero-shot segmentation due to the absence of pretraining on pixel-level annotations, and we leave this for future work. Building on the robust foundation of LocCa, a promising direction for future work involves enhancing its pixel-level precision through the incorporation of segmentation tasks during the pretraining phase. This extension aims to equip LocCa with a more nuanced understanding of images, enabling it to discern and interpret intricate details and textures with unparalleled accuracy, further broadening its applicability across diverse vision-language tasks.

    &  &  &  &  &  &  &  \\   & & & & & val & & testA & testB & val & testA & testB \\  ✓ & ✓ & 80.4 & 117.7 & 68.4 & 59.0 & 88.0 & 90.8 & 84.0 & 80.2 & 84.7 & 73.8 \\ ✓ & ✗ & 79.7 & 115.9 & 67.5 & 57.9 & 86.8 & 89.6 & 83.0 & 77.7 & 83.5 & 71.2 \\ ✗ & ✓ & 79.7 & 114.8 & 67.4 & 57.7 & 83.7 & 87.2 & 80.7 & 72.3 & 77.9 & 66.6 \\ ✗ & ✗ & 78.3 & 111.0 & 66.2 & 54.3 & 75.2 & 78.8 & 69.5 & 64.7 & 71.0 & 55.9 \\   

Table 6: Ablation study on applying loss on AREF and GCAP tasks during training.