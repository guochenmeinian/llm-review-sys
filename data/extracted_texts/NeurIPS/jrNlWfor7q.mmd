# Kronecker-Factored Approximate Curvature for Physics-Informed Neural Networks

Felix Dangel

Vector Institute

Toronto

Canada

fdangel@vectorinstitute.ai &Johannes Muller

Chair of Mathematics of Information Processing

RWTH Aachen University

Aachen, Germany

mueller@mathc.rwth-aachen.de

Equal contribution

&Marius Zeinhofer

Seminar for Applied Mathematics, ETH Zurich,

Department of Nuclear Medicine, University Hospital Freiburg

marius.zeinhofer@uniklinik-freiburg.de

###### Abstract

Physics-informed neural networks (PINNs) are infamous for being hard to train. Recently, second-order methods based on natural gradient and Gauss-Newton methods have shown promising performance, improving the accuracy achieved by first-order methods by several orders of magnitude. While promising, the proposed methods only scale to networks with a few thousand parameters due to the high computational cost to evaluate, store, and invert the curvature matrix. We propose Kronecker-factored approximate curvature (KFAC) for PINN losses that greatly reduces the computational cost and allows scaling to much larger networks. Our approach goes beyond the established KFAC for traditional deep learning problems as it captures contributions from a PDE's differential operator that are crucial for optimization. To establish KFAC for such losses, we use Taylor-mode automatic differentiation to describe the differential operator's computation graph as a forward network with shared weights. This allows us to apply KFAC thanks to a recently developed general formulation for networks with weight sharing. Empirically, we find that our KFAC-based optimizers are competitive with expensive second-order methods on small problems, scale more favorably to higher-dimensional neural networks and PDEs, and consistently outperform first-order methods and LBFGS.

## 1 Introduction

Neural network-based approaches to numerically solve partial differential equations (PDEs) are growing at an unprecedented speed. The idea to train network parameters to minimize the residual of a PDE traces back to at least Dissanayake & Phan-Thien , Lagaris et al. , but was only recently popularized under the name _deep Galerkin method_ (DGM) and _Physics-informed neural networks_ (PINNs) through the works of Sirignano & Spiliopoulos , Raissi et al. . PINNs are arguably one of the most popular network-based approaches to the numerical solution of PDEs as they are easy to implement, seamlessly incorporate measurement data, and promise to work well in high dimensions. Despite their immense popularity, PINNs are notoriously difficult to optimize  and fail to provide satisfactory accuracy when trained with first-order methods, even for simple problems [64; 41]. Recently, second-order methods that use the function space geometry to design preconditionershave shown remarkable promise in addressing the training difficulties of PINNs [64; 41; 14; 24; 42]. However, these methods require solving a linear system in the network's high-dimensional parameter space at cubic computational iteration cost, which prohibits scaling such approaches. To address this, we build on the idea of Kronecker-factored approximate curvature (KFAC) and apply it to Gauss-Newton matrices of PINN losses which greatly reduces the computational cost:

* We use higher-order forward (Taylor) mode automatic differentiation to interpret the computation graph of a network's input derivatives as a larger net with weight sharing (SS3.1).
* We use this weight sharing view to propose KFAC for Gauss-Newton matrices of objectives with differential operators, like PINN losses (SS3.3 and eq. (14)). Thanks to the generality of Taylor-mode and KFAC for weight sharing layers , our approach is widely applicable.
* We show that, for specific differential operators, the weight sharing in Taylor-mode can be further reduced by absorbing the reduction of partial derivatives into the forward propagation, producing a more efficient scheme. For the prominent example of the Laplace operator, this recovers and generalizes the _forward Laplacian_ framework  (SS3.2 and eq. (9)).
* Empirically, we find that our KFAC-based optimizers are competitive with expensive second-order methods on small problems, scale more favorably to higher-dimensional neural networks and PDEs, and consistently outperform first-order methods and LBFGS (SS4).

Related workVarious approaches were developed to improve the optimization of PINNs such as adaptive re-weighting of loss terms [57; 56; 59], different sampling strategies for discretizing the loss [34; 43; 13; 63; 58; 61], and curriculum learning [26; 58]. While LBFGS is known to improve upon first-order optimizers , recently, other second-order methods that design meaningful preconditioners that respect the problem's geometry have significantly outperformed it [64; 41; 14; 33; 24; 7; 62]. Muller & Zeinhofer  provide a unified view on these approaches which greatly improve the accuracy of PINNs, but come with a significant per-iteration cost as one needs to solve a linear system in the network's high-dimensional parameter space, which is only feasible for small networks when done naively. One approach is to use matrix-free methods to approximately compute Gauss-Newton directions by introducing an inner optimization loop, see [51; 36] for supervised learning problems and [64; 4; 24; 62] for PINNs. Instead, our KFAC-based approach uses an explicit structured curvature representation which can be updated over iterations and inverted more cheaply.

We build on the literature on Kronecker-factored approximate curvature (KFAC), which was initially introduced in Heskes , Martens  as an approximation of the per-layer Fisher matrix to perform approximate natural gradient descent. Later, KFAC was extended to convolutional , recurrent , attention [47; 44; 21], and recently to general linear layers with weight sharing . These works do not address preconditioners for losses with contributions from differential operators, as is the case for PINN losses. Our interpretation via Taylor-mode makes the computation graph of such losses explicit, and allows us to establish KFAC based on its generalization to linear weight sharing layers .

## 2 Background

For simplicity, we present our approach for multi-layer perceptrons (MLPs) consisting of fully-connected and element-wise activation layers. However, the generality of Taylor-mode automatic differentiation and KFAC for linear layers with weight sharing allows our KFAC to be applied to such layers (e.g. fully-connected, convolution, attention) in arbitrary neural network architectures.

Flattening & DerivativesWe vectorize matrices using the _first-index-varies-fastest_ convention, i.e. column-stacking (row index varies first, column index varies second) and denote the corresponding flattening operation by \(\). This allows to reduce derivatives of matrix- or tensor-valued objects back to the vector case by flattening a function's input and output before differentiation. The Jacobian of a vector-to-vector function \(()\) has entries \([_{}]_{i,j}=}}{{  a_{j}}}\). For a matrix-to-matrix function \(()\), the Jacobian is \(_{}=_{} \). A useful property of \(\) is \(()=(^{}) {vec}\) for matrices \(,,\) which implies \(_{}()=^{}\).

Sequential neural netsConsider a _sequential neural network_\(u_{}=f_{^{(L)}} f_{^{(L-1)}}  f_{^{(1)}}\) of depth \(L\). It consists of layers \(f_{^{(l)}}^{h^{(l-1)}}^{h^{(l)}}\), \(^{(l-1)}^{(l)}=f_{^{(l)}}(^{(l-1)})\) with trainable parameters \(^{(l)}^{p^{(l)}}\) that transform an input \(^{(0)}^{d=h^{(0)}}\) into a prediction \(u_{}()=^{(L)}^{h^{(L)}}\) via intermediate representations \(^{(l)}^{h^{(l)}}\). In the context of PINNs, we use networks with scalar outputs (\(h^{(L)}=1\)) and denote the concatenation of all parameters by \(=(^{(1)},,^{(L)})^{} ^{D}\). A common choice is to alternate fully-connected and activation layers. Linear layers map \(^{(l-1)}^{(l)}=^{(l)}^{(l-1)}\) using a weight matrix \(^{(l)}=^{-1}^{(l)}^{h^{(l)}  h^{(l-1)}}\) (bias terms can be added as an additional column and by appending a \(1\) to the input). Activation layers map \(^{(l-1)}^{(l)}=(^{(l-1)})\) element-wise for a (typically smooth) \(\).

### Energy Natural Gradients for Physics-Informed Neural Networks

Let us consider a domain \(^{d}\) and the partial differential equation

\[u=f\,, u=g\,,\]

with right-hand side \(f\), boundary data \(g\) and a differential operator \(\), e.g. the negative Laplacian \(-u=_{}u=_{i=1}^{d}_{x_{i}}^{2}u\). We parametrize \(u\) with a neural net and train its parameters \(\) to minimize the loss

\[L() =}_{n=1}^{N_{}}(u_{ {}}(_{n})-f(_{n}))^{2}+}_{n =1}^{N_{}}(u_{}(_{n}^{})-g(_{n}^ {}))^{2}\] (1) \[=:L_{}()+L_{}()\]

with points \(\{_{n}\}_{n=1}^{N_{}}\) from the domain's interior, and points \(\{_{n}^{}\}_{n=1}^{N_{}}\) on its boundary.2

First-order optimizers like gradient descent and Adam struggle at producing satisfactory solutions when used to train PINNs . Instead, function space-inspired second-order methods have lately shown promising results . We focus on _energy natural gradient descent (ENGD )_ which--applied to PINN objectives like (1)--corresponds to the Gauss-Newton method [6, Chapter 6.3]. ENGD mimics Newton's method _in function space_ up to a projection onto the model's tangent space and a discretization error that vanishes quadratically in the step size, thus providing locally optimal residual updates. Alternatively, the Gauss-Newton method can be motivated from the standpoint of operator preconditioning, where the Gauss-Newton matrix leads to optimal conditioning of the problem .

Natural gradient methods perform parameter updates via a preconditioned gradient descent scheme \(-()^{+} L()\), where \(()^{+}\) denotes the pseudo-inverse of a suitable _Gramian matrix_\(()^{D D}\) and \(\) is a step size. ENGD for the PINN loss (1) uses the Gramian

\[() =}_{n=1}^{N_{}}(_{ }u_{}(_{n}))^{}_{ }u_{}(_{n})+}_{n=1}^{N_{}}(_{}u_{ }(_{n}^{}))^{}_{ }u_{}(_{n}^{})\] (2) \[:_{}()+_{}( {})\,.\]

(2) is the Gauss-Newton matrix of the residual \(()=(r_{}()^{}/},r_{ }()^{}/})^{} ^{N_{}+N_{}}\) with interior and boundary residuals \(r_{,n}()=u_{}(_{n})-f(_{n})\) and \(r_{,n}()=u_{}(_{n}^{})-g( {x}_{n}^{})\).

### Kronecker-factored Approximate Curvature

We review Kronecker-factored approximate curvature (KFAC) which was introduced by Heskes , Martens & Grosse  in the context of maximum likelihood estimation to approximate the per-layer Fisher information matrix by a Kronecker product to speed up approximate natural gradient descent . The Fisher associated with the loss \(}{{2N}}_{n=1}^{N} u_{}(_{n})-y_{n} _{2}^{2}\) with targets \(y_{n}\) is

\[()=_{n=1}^{N}(_{} u_{}(_{n}))^{}_{}u_{}(_{n})=_{n=1}^{N}(_{}u_{n})^{}_{}u_{n} ^{D D}\,,\] (3)

where \(u_{n}=u_{}(_{n})\), and it coincides with the classical Gauss-Newton matrix . The established KFAC approximates (3). While the boundary Gramian \(_{}()\) has the same structure as \(()\), the interior Gramian \(_{}()\) does not as it involves derivative rather than function evaluations of the net.

KFAC tackles the Fisher's per-layer block diagonal, \(()(^{(1)}(),, {F}^{(L)}())\) with \(^{(l)}()=}{{N}}_{n=1}^{N}(_{ ^{(l)}}u_{n})^{}_{^{(l)}}u_{n} ^{p^{(l)} p^{(l)}}\). For a fully-connected layer's block, let's examine the term \(_{^{(l)}}u_{}()\) from Equation (3) for a fixed data point. The layer parameters \(^{(l)}=^{(l)}\) enter the computation via \(^{(l)}=^{(l)}^{(l-1)}\) and we have \(_{^{(l)}}^{(l)}=^{(l-1)}\)[e.g. ]. Further, the chain rule gives the decomposition \(_{^{(l)}}u=(_{^{(l)}}u) _{^{(l)}}^{(l)}=^{(l-1)} _{^{(l)}}u\). Inserting into \(^{(l)}()\), summing over data points, and using the expectation approximation \(_{n}_{n}_{n} N^{-1}(_{n}_{n})( _{n}_{n})\) from Martens & Grosse , we obtain the KFAC approximation for linear layers in supervised square loss regression with a network's output,

\[^{(l)}()_{n=1}^{N} _{n}^{(l-1)}_{n}^{(l-1)}^{})}_{=^{(l)} ^{h(l-1) h(l-1)}}_{n=1 }^{N}(_{^{(l)}}u_{n})^{} _{^{(l)}}u_{n})}_{=^{(l)}^{h(l) h(l)}}.\] (4)

It is cheap to store and invert by inverting the two Kronecker factors.

## 3 Kronecker-Factored Approximate Curvature for PINNs

ENGD's Gramian is a sum of PDE and boundary Gramians, \(()=_{}()+_{}()\). We will approximate each Gramian separately with a block diagonal matrix with Kronecker-factored blocks, \(_{}()(_{}^{(1 )}(),,_{}^{(L)}())\) for \(\{,\}\) with \(_{}^{(l)}()_{}^{(l)}_{}^{(l)}\). For the boundary Gramian \(_{}()\), we can re-use the established KFAC from Equation (4) as its loss corresponds to regression over the network's output. The interior Gramian \(_{}()\), however, involves PDE terms in the form of network derivatives and therefore _cannot_ be approximated with the existing KFAC. It requires a new approximation that we develop here for the running example of the Poisson equation and more general PDEs (Equations (9) and (14)). To do so, we need to make the dependency between the weights and the differential operator \(u\) explicit. We use Taylor-mode automatic differentiation to express this computation of higher-order derivatives as forward passes of a larger net with shared weights, for which we then propose a Kronecker-factored approximation, building on KFAC's recently-proposed generalization to linear layers with weight sharing .

### Higher-order Forward Mode Automatic Differentiation as Weight Sharing

Here, we review higher-order forward mode, also known as _Taylor-mode_, automatic differentiation [19; 18; 3], tutorial in SSC]. Many PDEs only incorporate first- and second-order partial derivatives and we focus our discussion on second-order Taylor-mode for MLPs to keep the presentation light. However, one can treat higher-order PDEs and arbitrary network architectures completely analogously.

Taylor-mode propagates directional (higher-order) derivatives. We now recap the forward propagation rules for MLPs consisting of fully-connected and element-wise activation layers. Our goal is to evaluate first-and second-order partial derivatives of the form \(_{x_{i}}u,_{x_{i},x_{j}}^{2}u\) for \(i,j=1,,d\). At the first layer, set \(^{(0)}=^{d}\), \(_{x_{i}}^{(0)}=_{i}^{d}\), i.e., the \(i\)-th basis vector and \(_{x_{i},x_{j}}^{2}^{(0)}=^{d}\).

For a linear layer \(f_{^{(l)}}(^{(l-1)})=^{(l)}^{(l-1)}\), applying the chain rule yields the propagation rule

\[^{(l)} =^{(l)}^{(l-1)}^{h^{(l)}}\,,\] (5a) \[_{x_{i}}^{(l)} =^{(l)}_{x_{i}}^{(l-1)}^{h ^{(l)}}\,,\] (5b) \[_{x_{i},x_{j}}^{2}^{(l)} =^{(l)}_{x_{i},x_{j}}^{2}^{(l-1)} ^{h^{(l)}}\,.\] (5c)

The propagation rule through a nonlinear element-wise activation layer \(^{(l-1)}(^{(l-1)})\) is

\[^{(l)} =(^{(l-1)})^{h^{(l)}}\,,\] (6a) \[_{x_{i}}^{(l)} =^{}(^{(l-1)})_{x_{i}}^{(l-1) }^{h^{(l)}}\,,\] (6b) \[_{x_{i},x_{j}}^{2}^{(l)} =_{x_{i}}^{(l-1)}^{}(^{ (l-1)})_{x_{j}}^{(l-1)}+^{}(^{(l-1)}) _{x_{i},x_{j}}^{2}^{(l-1)}^{h^{(l)}}\,.\] (6c)Forward LaplacianFor differential operators of special structure, we can fuse the Taylor-mode forward propagation of individual directional derivatives in Equations (5) and (6) and obtain a more efficient computation. E.g., to compute not the full Hessian but only the Laplacian, we can simplify the forward pass, which yields the _forward Laplacian_ framework of Li et al. . To the best of our knowledge, this connection has not been pointed out in the literature. Concretely, by summing (5c) and (6c) over \(i=j\), we obtain the Laplacian forward pass for linear and activation layers

\[_{}^{(l)} =^{(l)}_{}^{(l-1)}^{h^{ (l)}}\,,\] (7a) \[_{}^{(l)} =^{}(^{(l-1)})_{}^{(l-1) }+_{i=1}^{d}^{}(^{(l-1)})(_{x_{i}} {z}^{(l-1)})^{ 2}^{h^{(l)}}\,.\] (7b)

This reduces computational cost, but is restricted to PDEs that involve second-order derivatives only via the Laplacian, or a partial Laplacian over a sub-set of input coordinates (e.g. heat equation, SS4). For a more general second-order linear PDE operator \(=_{i,j=1}^{d}c_{i,j}_{x_{i},x_{j}}^{2}\), the forward pass for a linear layer is \(^{(l)}=^{(l)}^{(l-1)}^{h^ {(l)}}\), generalizing (7a), and similarly for Equation (7b)

\[^{(l)}=^{}(^{(l-1)}) ^{(l-1)}+_{i,j=1}^{d}c_{i,j}^{}(^{(l-1)}) _{x_{i}}^{(l-1)}_{x_{j}}^{(l-1)} ^{h^{(l)}}\,,\]

see SSC.3 for details. This is different from , which transforms the input space such that the coefficients are diagonal with entries \(\{0, 1\}\), reducing the computation to two forward Laplacians.

Importantly, the computation of higher-order derivatives for linear layers boils down to a forward pass through the layer with weight sharing over the different partial derivatives (Equation (5)), and weight sharing can potentially be reduced depending on the differential operator's structure (Equation (7a)). Therefore, we can use the concept of KFAC in the presence of weight sharing to derive a principled Kronecker approximation for Gramians containing differential operator terms.

### KFAC for Gauss-Newton Matrices with the Laplace Operator

Let's consider the Poisson equation's interior Gramian block for a linear layer (suppressing \(\) in \(N_{}\))

\[_{}^{(l)}()=_{n=1}^{N}(_ {^{(l)}}_{}u_{n})^{}_{^{(l)}} _{}u_{n}\,.\]

Because we made the Laplacian computation explicit through Taylor-mode autodiff (SS3.1, specifically Equation (7a)), we can stack all output vectors that share the layer's weight into a matrix \(_{n}^{(l)}^{h^{(l)} S}\) with \(S=d+2\) and columns \(_{n,1}^{(l)}=_{n}^{(l)},_{n,2}^{(l)}=_{x_{1}} _{n}^{(l)},,_{n,1+d}^{(l)}=_{x_{d}}_{n}^{(l)}\), and \(_{n,2+d}^{(l)}=_{}_{n}^{(l)}\) (likewise \(_{n}^{(l-1)}^{h^{(l-1)} S}\) for the layer inputs), then apply the chain rule

\[_{^{(l)}}_{}u_{n}=(_{_{n}^{(l)}} _{}u_{n})_{^{(l)}}_{n}^{(l)}=_{s=1}^{S} _{n,s}^{(l-1)}}_{^{h^{(l-1)}}} {_{_{n,s}^{(l)}}_{}u_{n}}_{=:_{n,s}^{(l)} ^{h^{(l)}}}\,,\]

which has a structure similar to the Jacobian in SS2.2, but with an additional sum over the \(S\) shared vectors. With that, we can now express the exact interior Gramian for a layer as

\[_{}^{(l)}()=_{n=1}^{N}_{s=1}^{S} _{s^{}=1}^{S}_{n,s}^{(l-1)}_{n,s^{}}^{(l-1)} _{n,s}^{(l)}_{n,s^{}}^{(l)}.\] (8)

Next, we want to approximate Equation (8) with a Kronecker product. To avoid introducing a new convention, we rely on the KFAC approximation for linear layers with weight sharing developed by Eschenhagen et al. --specifically, the approximation called _KFAC-expand_. This drops all terms with \(s s^{}\), then applies the expectation approximation from SS2.2 over the batch and shared axes:

[title=: KFAC for the Gauss-Newton matrix of Laplace operator] (9) \[_{}^{(l)}()(_{n,s=1}^{N, S}_{n,s}^{(l-1)}_{n,s}^{(l-1)})( _{n,s=1}^{N,S}_{n,s}^{(l),s}{g}_{n,s}^{})=:_{}^ {(l)}_{}^{(l)}\] (9)

### KFAC for Generalized Gauss-Newton Matrices Involving General PDE Terms

To generalize the previous section, let's consider the general \(M\)-dimensional PDE system of order \(k\),

\[(u,D_{}u,,D_{}^{k}u)=^{M},\] (10)

where \(D_{}^{m}u\) collects all partial derivatives of order \(m\). For \(m\{0,,k\}\) there are \(S_{m}=\) independent partial derivatives and the total number of independent derivatives is \(S_{m=0}^{k}S_{m}=\). \(\) is a smooth mapping from all partial derivatives to \(^{M}\), \(^{S}^{M}\). To construct a PINN loss for Equation (10), we feed the residual \(_{,n}()(u_{}(_{n}),D_{ {x}}u_{}(_{n}),,D_{}^{k}u_{}(_{n }))^{M}\) where \(D_{}^{m}u_{}(_{n})^{d S_{m}}\) into a smooth convex criterion function \(^{M}\),

\[L_{}()_{n=1}^{N}(_{, n}())\,.\] (11)

The generalized Gauss-Newton (GGN) matrix  is the Hessian of \(L_{}()\) when the residual is linearized w.r.t. \(\) before differentiation. It is positive semi-definite and has the form

\[_{}()_{n=1}^{N}(_{}_{,n}())^{}(_{,n})(_{}_{,n}() )\,,\] (12)

with \(()_{}^{2}()^{M  M} 0\) the criterion's Hessian, e.g. \(()=}{{2}}\|\|_{2}^{2}\) and \(()=_{M}\). Generalizing the second-order Taylor-mode from SS3.1 to higher orders for the linear layer, we find

\[D_{}^{m}^{(l)}=^{(l)}D_{}^{m}^{(l-1)} ^{h^{(l)} S_{m}}\] (13)

for any \(m\). Hence, we can derive a forward propagation for the required derivatives where a linear layer processes at most \(S\) vectors3, i.e. the linear layer's weight is shared over the matrices \(D_{}^{0}^{(l-1)}^{(l-1)},D_{}^{1}^{(l- 1)},,D_{}^{k}^{(l-1)}\). Stacking them into a matrix \(_{n}^{(l-1)}=(^{(l-1)},D_{}^{1}^{(l-1)},,D_{}^{k}^{(l-1)})^{h^{(l-1)} S}\) (and \(_{n}^{(l)}\) for the outputs), the chain rule yields

\[_{}^{(l)}() =_{n=1}^{N}(_{^{(l)}}_{n }^{(l)})^{}(_{_{n}^{(l)}}_{,n} )^{}(_{,n})(_{_{n}^{(l )}}_{,n})(_{^{(l)}}_{n}^{(l)})\] \[=_{n,s,s^{}=1}^{N,S,S}(_{^{(l)}}_{n,s}^{(l)})^{}(_{_{n,s}^{(l)}} _{,n})^{}(_{,n})(_{_{n,s^{}}^{(l)}}_{,n})(_{^{(l)}} _{n,s^{}}^{(l)})\] \[=_{n,s,s^{}=1}^{N,S,S}_{n,s}^{(l-1)} _{n,s^{}}^{(l-1)}(_{_{n,s}^{(l)}} _{,n})^{}(_{,n})(_{_{n,s^{}}^{(l)}}_{,n})\]

where \(_{n,s}^{(l-1)}^{h^{(l-1)}}\) denotes the \(s\)-th column of \(_{n}^{(l-1)}\). Following the same steps as in ยง3.2, we apply the KFAC-expand approximation from  to obtain the generalization of Equation (9):

\[_{}^{(l)}()\] (14) \[_{}^{(l)}_{}^{(l)}\]

To bring this expression even closer to Equation (9), we can re-write the second Kronecker factor using an outer product decomposition \((_{,n})=_{m=1}^{M}_{n,m}_{n,m}\) with \(_{n,m}^{M}\), then introduce \(_{n,s,m}^{(l)}(_{_{n,s}^{(l)}}_{,n})^ {}_{n,m}^{h^{(l)}}\) and write the second term as \(}{{N}}_{n,s,m=1}^{N,S,M}_{n,s,m}^{(l)}_{n,s,m}^{(l)}\), similar to the Kronecker-factored low-rank (KFLR) approach of Botev et al. .

**KFAC for variational problems** Our proposed KFAC approximation is not limited to PINNs and can be used for variational problems of the form

\[_{u}_{}(u,_{}u,,_{}^{k}u) \,\] (15)

where \(^{K}\) is a convex function. We can perceive this as a special case of the setting above with \(=\) and hence the KFAC approximation (14) remains meaningful. In particular, it can be used for the _deep Ritz method_ and other variational approaches to solve PDEs .

### Algorithmic Details

To design an optimizer based on our KFAC approximation, we re-use techniques from the original KFAC  & ENGD  algorithms. SSB shows pseudo-code for our method on the Poisson equation.

At iteration \(t\), we approximate the per-layer interior and boundary Gramians using our derived Kronecker approximation (Equations (9) and (14)), \(_{,t}^{(l)}_{,t}^{(l)}_{,t}^ {(l)}\) and \(_{,t}^{(l)}_{,t}^{(l)}_{ ,t}^{(l)}\).

Exponential moving average and dampingFor preconditioning, we accumulate the Kronecker factors \(_{,t}^{(l)},_{,t}^{(l)}\) over time using an exponential moving average \(}_{,t}^{(l)}=}_{,t-1}^{(l)}+(1- )_{,t}^{(l)}\) of factor \([0,1)\) (identically for \(}_{,t}^{(l)}\)), similar to the original KFAC. Moreover, we apply the same constant damping of strength \(>0\) to all Kronecker factors, \(}_{,t}^{(l)}=}_{,t}^{(l)}+ \) and \(}_{,t}^{(l)}=}_{,t}^{(l)}+\) such that the curvature approximation used for preconditioning at step \(t\) is

\[_{,t}(}_{,t}^{(1)} }_{,t}^{(1)},,}_{,t}^{(L) }}_{,t}^{(L)})+(}_{,t}^{(1)}}_{,t}^{(1)}, ,}_{,t}^{(L)}}_{ ,t}^{(L)})\.\]

Gradient preconditioningGiven layer \(l\)'s mini-batch gradient \(_{t}^{(l)}=_{t})}}{{_{t}^{(l)}}}^{p^{(l)}}\), we obtain an update direction \(_{t}^{(l)}=-(}_{,t}^{(l)}}_ {,t}^{(l)}+}_{,t}^{(l)} }_{,t}^{(l)})^{-1}_{t}^{(l)}^{p^{(l)}}\) using the trick of [38, Appendix I] to invert the Kronecker sum via eigen-decomposing all Kronecker factors.

Learning rate and momentumFrom the preconditioned gradient \(_{t}^{D}\), we consider two different updates \(_{t+1}=_{t}+_{t}\) we call _KFAC_ and _KFAC_*. KFAC uses momentum over previous updates, \(}_{t}=_{t-1}+_{t}\), and \(\) is chosen by the practitioner. Like ENGD, it uses a logarithmic grid line search, selecting \(_{t}=_{}}_{t}\) with \(_{}=*{arg\,min}_{}L(_{t}+}_{t})\) where \(\{2^{-30},,2^{0}\}\). KFAC* uses the automatic learning rate and momentum heuristic of the original KFAC optimizer. It parametrizes the iteration's update as \(_{t+1}(,)=_{t}+_{t}\), then obtains the optimal parameters by minimizing the quadratic model \(m(_{t+1})=L(_{t})+_{t+1}^{}_{t+1}^{ }((_{t})+)_{t+1}\) with the exact damped Gramian. The optimal learning rate and momentum \(*{arg\,min}_{,}m(_{t+1})\) are

\[_{}\\ _{}=-_{t}^{}(_{t})_{t}+\|_{t}\|^{2}&_{t}^{}(_{t})_{t}+_{t }^{}_{t}\\ _{t}^{}(_{t})_{t}+_{t }^{}_{t}&_{t}^{}(_{t})_{t}+\|_{t}\|^{2}^{-1} _{t}^{}_{t}\\ _{t}^{}_{t}\]

(see [38, Section 7] for details). The computational cost is dominated by the two Gramian-vector products with \(_{t}\) and \(_{t}\). By using the Gramian's outer product structure , we perform them with autodiff  using one Jacobian-vector product each, as recommended in .

Computational complexityInverting layer \(l\)'s Kronecker approximation of the Gramian requires \(({h^{(l)}}^{3}+{h^{(l+1)}}^{3})\) time and \(({h^{(l)}}^{2}+{h^{(l+1)}}^{2})\) storage, where \(h^{(l)}\) is the number of neurons in the \(l\)-th layer, whereas inverting the exact block for layer would require \(({h^{(l)}}^{3}{h^{(l+1)}}^{3})\) time and \(({h^{(l)}}^{2}{h^{(l+1)}}^{2})\) memory. In general, the improvement from the Kronecker factorization depends on how close to square the weight matrices of a layer are, and therefore on the architecture. In practise, the Kronecker factorization usually significantly reduces memory and run time. Further improvements can be achieved by using structured Kronecker factors, e.g. (block-)diagonal matrices .

We use the forward Laplacian framework in our implementation, which we found to be significantly faster and more memory efficient than computing batched Hessian traces, see SSC.4.

## 4 Experiments

We implement KFAC, KFAC*, and ENGD with either the per-layer or full Gramian in PyTorch . As a matrix-free version of ENGD, we use the Hessian-free optimizer  which uses truncated conjugate gradients (CG) with exact Gramian-vector products to precondition the gradient. We chose this because there is a fully-featured implementation from Tatzel et al.  which offers many additional heuristics like adaptive damping, CG backtracking, and backtracking line search, allowing this algorithm to work well with little hyper-parameter tuning. As baselines, we use SGD with tuned learning rate and momentum, Adam with tuned learning rate, and LBFGS with tuned learning rate and history size. We tune hyper-parameters using Weights & Biases  (see SSA.1 for the exact protocol). For random/grid search, we run an initial round of approximately 50 runs with generous search spaces, then narrow them down and re-run for another 50 runs; for Bayesian search, we assign the same total compute to each optimizer. We report runs with lowest \(L_{2}\) error estimated on a held-out data set with the known solution to the studied PDE. To be comparable, all runs are executed on a compute cluster with RTX 6000 GPUs (24 GiB RAM) in double precision, and we use the same computation time budget for all optimizers on a fixed PINN problem. All search spaces and best run hyper-parameters, as well as training curves over iteration count rather than time, are in SSA.

Pedagogical example: 2d Poisson equationWe start with a low-dimensional Poisson equation from Muller & Zeinhofer  to reproduce ENGD's performance (Figure 1). It is given by

\[- u(x,y)&=2^{2}( x) ( y)\;(x,y)^{2}\\ u(x,y)&=0\;(x,y)^{2 }.\] (16)

We choose a fixed data set of same size as the original paper, then use random/grid search to evaluate the performance of all optimizers for different \(\)-activated MLPs, one shallow and two with five fully-connected layers of different width (all details in SSA.2). We include ENGD whenever the network's parameter space is small enough to build up the Gramian.

For the shallow net (Figure 1, left), we can reproduce the results of , where exact ENGD achieves high accuracy. In terms of computation time, our KFACs are competitive with full-ENGD for a long

Figure 1: Performance of different optimizers on the 2d Poisson equation (16) measured in relative \(L_{2}\) error against wall clock time for architectures with different parameter dimensions \(D\).

Figure 2: Performance of different optimizers on the (4+1)d heat equation (17) measured in relative \(L^{2}\) error against wall clock time for architectures with different parameter dimensions \(D\).

phase, outperforming the first-order and quasi-Newton baselines. In contrast to ENGD, which runs out of memory for networks with more than \(10\,000\) parameters, KFAC scales to larger networks (Figure 1, center and right) and is competitive with other second-order optimizers like Hessian-free, which uses more sophisticated heuristics. We make similar observations on a small (1+1)d heat equation with the same models, see SSA.7 and fig. A10.

An evolutionary problem: (4+1)d heat equationTo demonstrate that our methods can also be applied to other problems than the Poisson equation, we consider a four-dimensional heat equation

\[_{t}u(t,)-_{}u(t, )&=0t,^{4}\,,\\ u(0,)&=_{i=1}^{4}(2x_{i})^{4}\,,\\ u(t,)&=(-t)_{i=1}^{4}(2x_{i}) t,^{4}\,,\] (17)

with diffusivity constant \(=}{{4}}\), similar to that studied in  (see SSA.6 for the heat equation's PINN loss). We use the previous architectures with same hidden widths and evaluate optimizer performance with random/grid search (all details in SSA.8), see Figure 2. To prevent over-fitting, we use mini-batches and sample a new batch each iteration. We noticed that KFAC improves significantly when batches are sampled less frequently and hypothesize that it might need more iterations to make similar progress than one iteration of Hessian-free or ENGD on a batch. Consequently we sample a new batch only every 100 iterations for KFAC. To ensure that this does not lead to an unfair advantage for KFAC, we conduct an additional experiment for the MLP with \(D=116\,864\) where we tune batch sizes, batch sampling frequencies, and all hyper-parameters with generous search spaces using Bayesian search (SA.10). We find that this does not significantly boost performance of the other methods (compare Figures 2 and A14). Again, we observe that KFAC offers competitive performance compared to other second-order methods for networks with prohibitive size for ENGD and consistently outperforms SGD, Adam, and LBFGS. We confirmed these observations with another 5d Poisson equation on the same architectures, see SSA.3 and fig. A7.

High-dimensional Poisson equationsTo demonstrate scaling to high-dimensional PDEs and even larger neural networks, we consider three Poisson equations (\(d=5,10,100\)) with different boundary conditions used in , which admit the solutions

\[ u_{}()&=_{i=1}^{5} ( x_{i})^{5}\,,\\ u_{}()&=_{k=1}^{5}x_{2k-1}x_{2k} ^{10}\,,\\ u_{}()&=\|\|_{2}^{2}^{100}\,.\] (18)

We use the same architectures as before, but with larger intermediate widths and parameters up to a million (Figure 3). Due to lacking references for training such high-dimensional problems, we

Figure 3: Optimizer performance on Poisson equations in high dimensions and different boundary conditions measured in relative \(L_{2}\) error against wall clock time for networks with \(D\) parameters.

select all hyper-parameters via Bayesian search, including batch sizes and batch sampling frequencies (details in SSA.5). We see a similar picture as before with KFAC consistently outperforming first-order methods and LBFGS, offering competitive performance with Hessian-free. To account for the possibility that the Bayesian search did not properly identify good hyper-parameters, we conduct a random/grid search experiment for the 10d Poisson equation (Figure 3, middle), using similar batch sizes and same batch sampling frequencies as for the \((4+1)\)d heat equation (details in SSA.4). In this experiment, KFAC also achieved similar performance than Hessian-free and outperformed SGD, Adam, and LBFGS (Figure A8).

(9+1)d Fokker-Planck equationTo show the applicability to nonlinear PDEs, we consider a Fokker-Planck equation in logarithmic space. PINN formulations of the Fokker-Planck equation have been considered in . Concretely, we are solving a nine-dimensional equation of the form

\[_{t}q(t,)-- q(t,)- \| q(t,)\|^{2}- q(t,)=0, q(0)=(p^{*}(0)),\] (19)

with \(d=9\), \(t\) and \(^{9}\), where in practice \(^{9}\) is replaced by \([-5,5]^{9}\). The solution is \(q^{*}=(p^{*})\) and \(p^{*}\) is given by \(p^{*}(t,)(0,(-t)+(1-(-t))2)\). We model the solution with a medium sized tanh-activated MLP with \(D=118\,145\) parameters, batch sizes are \(N_{}=3\,000\), \(N_{}=1\,000\), and we assign each run a computation time budget of \(6\,000\,\). As in previous experiments, the batches are resampled every iteration for all optimizers except for KFAC and KFAC*, which use the same batch for ten steps (details in SSA.11). Figure 4 reports the \(L^{2}\) error over training time. Again, KFAC is among the best performing optimizers offering competitive performance to Hessian-free and clearly outperforming all first-order methods.

## 5 Discussion and Conclusion

We extended the concept of Kronecker-factored approximate curvature (KFAC) to Gauss-Newton matrices of Physics-informed neural network (PINN) losses that involve derivatives, rather than function evaluations, of the neural net. This greatly reduces the computational cost of approximate natural gradient methods, which are known to work well on PINNs, and allows them to scale to much larger nets. Our approach goes beyond the established KFAC for traditional supervised problems as it captures contributions from a PDE's differential operator that are crucial for optimization. To establish KFAC for such losses, we use Taylor-mode autodiff to view the differential operator's compute graph as a forward net with shared weights, then apply the recently-developed formulation of KFAC for linear layers with weight sharing. Empirically, we find that our KFAC-based optimizers are competitive with expensive second-order methods on small problems and scale to high-dimensional neural networks and PDEs while consistently outperforming first-order methods and LBFGS.

Limitations & future directionsWhile our implementation currently only supports MLPs and the Poisson and heat equations, the concepts we use to derive KFAC (Taylor-mode, weight sharing) apply to arbitrary architectures and PDEs, as described in SS3.3. We are excited that our current algorithms show promising performance when compared to second-order methods with sophisticated heuristics. In fact, the original KFAC optimizer itself  relies heavily on such heuristics that are said to be crucial for its performance . Our algorithms borrow components, but we did not explore all bells and whistles, e.g. adaptive damping and heuristics to distribute damping over the Kronecker factors. We believe our current algorithm's performance can further be improved, e.g. by exploring (1) updating the KFAC matrices less frequently, as is standard for traditional KFAC, (2) merging the two Kronecker approximations for boundary and interior Gramians into a single one, (3) removing matrix inversions , (4) using structured Kronecker factors , (5) computing the Kronecker factors in parallel with the gradient , (6) using single or mixed precision training , and (7) studying cheaper KFAC flavours based on the empirical Fisher  or input-based curvature .

Figure 4: Performance of different optimizers on a (9+1)d logarithmic Fokker-Planck equation in relative \(L_{2}\) error against wall clock time.