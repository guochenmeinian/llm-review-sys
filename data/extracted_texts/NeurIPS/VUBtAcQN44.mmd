# A Motion-aware Spatio-temporal Graph for Video Salient Object Ranking

Hao Chen

Yufei Zhu

Yongjian Deng

College of Computer Science, Beijing University of Technology, Beijing, China

{haochen303, 220232213}@seu.edu.cn, yjdeng@bjut.edu.cn

###### Abstract

Video salient object ranking aims to simulate the human attention mechanism by dynamically prioritizing the visual attraction of objects in a scene over time. Despite its numerous practical applications, this area remains underexplored. In this work, we propose a graph model for video salient object ranking. This graph simultaneously explores multi-scale spatial contrasts and intra-/inter-instance temporal correlations across frames to extract diverse spatio-temporal saliency cues. It has two advantages: 1. Unlike previous methods that only perform global inter-frame contrast or compare all proposals across frames globally, we explicitly model the motion of each instance by comparing its features with those in the same spatial region in adjacent frames, thus obtaining more accurate motion saliency cues. 2. We synchronize the spatio-temporal saliency cues in a single graph for joint optimization, which exhibits better dynamics compared to the previous stage-wise methods that prioritize spatial cues followed by temporal cues. Additionally, we propose a simple yet effective video retargeting method based on video saliency ranking. Extensive experiments demonstrate the superiority of our model in video salient object ranking and the effectiveness of the video retargeting method. Our codes/models are released at https://github.com/zpf-815/VSOR/tree/main.

## 1 Introduction

Figure 1: Previous methods (_e.g._, ) in video SOR primarily focus on capturing temporal correlations between global features across frames, resulting in limited ability to effectively model temporal saliency. These methods tend to emphasize objects with prominent static saliency cues (_i.e._, the man right), whereas our proposed model places greater emphasis on instance-wise temporal correlation, resulting in more accurate results (_i.e._, the walking woman left).

Salient Object Ranking (SOR)  is to mimic the visual priority coding mechanism  in the human visual system to rank the attractiveness of objects in a scene. It has a large range of applications such as image caption , image compression  and image retargeting . Previous research primarily focuses on ranking salient objects in static images. However, Video Salient Object Ranking (VSOR)  holds specific potential for applications such as video summarization and surveillance. It also presents distinct challenges due to the changing saliency ranking of objects within a sequence. Nonetheless, this area remains understudied.

SVSNet  is a pioneering work in Video Salient Object Ranking (VSOR) that leverages the proportion of eye fixation points on each object to infer the relative saliency ranking among objects. The model employs frame-level LSTM to capture global temporal information for saliency detection and fixation prediction. However, this approach depends on the availability of eye fixation labels, which are often absent. Moreover, it relies on an external object detector  to generate bounding boxes for computing fixation density, rendering the network non end-to-end. Additionally, it lacks instance-wise temporal modeling.

Recently, Lin  propose an end-to-end solution by integrating the object detector into the VSOR model for joint training. The VSOR results are obtained by initially modeling the correlations among all objects in each frame to perform individual image SOR. Subsequently, these static image SOR results are combined by considering inter-frame correlations to produce the final outcome.

Although this approach eliminates the dependency on eye fixations and achieves end-to-end video SOR, it still has a significant issue: as shown in the \(1^{st}\) row in Figure 1, it directly builds correlations among all instances across frames without explicitly differentiating instance-wise motion, thus remaining agnostic to instance-level motion saliency.

As the human visual system naturally focuses more on moving objects, our core motivation is to explicitly explore the inter-frame variation for each instance. Considering the lack of annotations for object tracking, we propose a simple and effective method to capture instance-level motion information. as shown in the \(2^{nd}\) row in Figure 1, since the absolute positions of objects between two frames do not change drastically, we can approximate the instance motion by comparing the features of an instance at the same position across frames. Additionally, we double the area of the bounding boxes for comparison to reduce errors caused by camera motion.

Spatial saliency in a frame is also indispensable for VSOR. Drawing inspiration from , we also conduct multi-scale contrast--between an instance and its larger local region, the global image, and another instance--in order to explore diverse spatial saliency cues in a graph neural network (GNN).

The following question is how to combine and optimize the temporal and spatial graphs. In previous work , a sequential combination of spatial and temporal graphs is adopted, but in our opinion, this approach lacks adaptability and may exhibit bias towards one side. For instance, when initially aggregating spatial information, the instance features contain spatial saliency signals, which can hinder the extraction of temporal features. Therefore, we propose to jointly optimize the various spatial and temporal relations within a unified graph. This joint graph facilitates more comprehensive and dynamic interactions among instances and frames, consequently enhancing the fusion sufficiency of diverse saliency cues and improving the ranking results.

As an emerging field, we also explore its application scenarios. The ranking of salient objects in videos reveals the varying degrees of attention and trajectory shifts towards different regions during video playback. This information happens to be crucial for the task of video retargeting . Therefore, we propose a method that applies video saliency object ranking to video retargeting. Our method is straightforward. We assign different weights to salient objects based on their saliency rankings, and then locate the saliency centroid of the current frame as the center for retargeting. Experimental results demonstrate that our proposed simple method effectively preserves the regions of interest with greater accuracy and comprehensiveness while mitigating background interference. Additionally, it exhibits better inter-frame continuity.

In summary, our contributions are as follows:

(1) Introducing a novel unified spatial-temporal graph that explicitly integrates motion-aware temporal correlation and adaptively integrate spatial and temporal saliency cues.

(2) Proposing a simple yet efficient video retargeting method based on VSOR.

(3) Conducting experiments that validate the efficacy of our temporal saliency cues, the strategy for fusing spatial and temporal cues, and the superior performance of our VSOR and retargeting models compared to state-of-the-art methods.

## 2 Related Work

### Salient Object Ranking on Images

Islam  first notice that it is difficult for people to form a unified judgment about which object is the most salient in a scene. Therefore, they propose the problem of relative saliency, namely saliency ranking, and use a stage-wise refinement model to gradually detect objects with different saliency levels. However, the use of pixel-level saliency ranking often results in varying saliency values for different pixels in the same instance.

To address this problem, a substantial amount of subsequent work [4; 5; 2; 6] has proposed instance-level saliency ranking tasks. Siris  infer the saliency ranking by modeling the human attention transfer mechanism with a combined bottom-up and top-down attention framework. Considering the influence of low-level features such as the position and size of objects on instance saliency, Fang  proposed a PPA module to model position information. Liu  utilize graph neural networks to comprehensively consider the spatial correlations, including the contrast between instances, the distinctiveness of a instance in its local/global contexts, as well as prior knowledge of categories.

These methods achieve good results in saliency ranking on static images, but for videos, the motion of objects greatly affects their saliency order. Therefore, existing saliency ranking models for images cannot achieve good results and specific designs towards the inter-frame relations are required.

### Video Salient Object Detection

Another topic related to our task is video salient object detection. Unlike static image salient object detection, it [19; 20; 21] needs to jointly consider the different saliency cues generated by the motion information across frames and static appearance. Li  use optical flow to model temporal features and combine spatial-temporal relationships through attention mechanisms. Fan  further highlight the attention shift problem resulted from motion and propose a recurrent model to model temporal dynamics and attention-shift jointly. Liu  consider single-frame appearance information, long/short-term motion and spatial-temporal cues jointly in a transformer.

However, these works focus on detecting the most salient object from a video, while salient object ranking task is more complicated and with more temporal dynamics. Also, these works fails to explore the instance-wise motion cues, which is crucial to change saliency ranking results.

### Video Salient Object Ranking

Wang  first introduce the video salient object ranking task and propose a multi-task network SVSNet. However, SVSNet only utilizes LSTM to capture global temporal features, ignoring the instance-level features that are indispensable to saliency ranking. Furthermore, the utilization of extra detectors in SVSNet leads to the model not being end-to-end. Lin  address these problems by proposing an end-to-end architecture, where the relationships between objects and temporal information are combined to assign rankings to each object. However, at the temporal level, Lin only use global features as saliency cues, ignoring the motion information of each instance.

Instead, we optimize the multi-scale spatial contrasts, inter-frame instance relations, and instance-wise local motion at the same time, and the resulting network is equipped with comprehensive saliency cues and enjoys better fusion adaptivity.

## 3 Method

### Overview

As shown in Figure 2, our proposed VSOR framework firstly segments instances based on Mask R-CNN , and then enhance instance features through attention mechanisms  and introduce position and scale saliency prior using PPA . Subsequently, A spatial-temporal graph neural network is crafted to jointly optimize multi-scale spatial and temporal contrast cues to generate integrated instance-level saliency features. Finally, a fully connected neural network is employed to infer saliency scores, which, combined with the results of instance segmentation, yield the final saliency ranking map. This model can be trained end-to-end.

### Unified Spatial-temporal Graph for VSOR

After obtaining the detection results through Mask R-CNN, instance features \(f_{i}\) are introduced into the Spatial-temporal Graph Reasoning model. Our graph reasoning model includes two views: spatial correlation modeling and temporal correlation modeling. In spatial correlation modeling, we draw inspiration from  to introduce the interaction between instances \(S^{i}\), the comparison between instances and their local contextual features \(S^{l}\), as well as the comparison between instances and global contextual features \(S^{g}\). We denote local contextual features as \(f_{i}^{l}\), where \(i\) means the instance \(i\), and global features are represented as \(f^{g}\). In temporal correlation module, we introduce the motion-aware contrast \(T^{t}\) and instance interaction \(T^{i}\). \(f_{i}^{\{t-1,t+1\}}\) means the motion features in frame \(t-1\) and \(t+1\) of instance \(i\).

#### 3.2.1 Spatial Correlation Modeling

**Interaction between instances.** The competition among multiple instances for saliency highlights the importance of inter-instance interaction in saliency ranking. Therefore, we establish edges between instances to capture their correlations and the aggregation function is represented as:

\[h_{N^{r}_{i}}=_{j=1}^{N}^{r}_{ij}W^{r}_{a}f_{j}; 28.452756pt ^{r}_{ij}=ReLU((W^{r}_{a})^{T}(U^{r}f_{i}||V^{r}f_{j}))\] (1)

where \(N\) means the number of instances and \(^{r}_{ij}\) is the attention weight modulating the edge between instance \(i\) and \(j\), and \(W^{r}_{a}\), \(U^{r}\) and \(V^{r}\) are projection matrixes. \(||\) means connection between features.

**Local contrast and global interaction.** It is clear that the saliency of an instance is influenced by both its local and global contexts . To capture the local contextual feature \(f_{i}^{l}\), we expand the bounding box of instance \(i\) by doubling its size. For the global contextual features, we divide the image into \(3 3\) and utilize average pooling to extract each feature.

Figure 2: The main architecture of our model. Firstly, we obtain instances of each frame through an object detector and extract features of each instance using an attention mechanism and position embedding. Then, we utilize a spatial-temporal graph reasoning network to fuse spatial-temporal saliency cues, ultimately obtaining saliency scores and final results.

#### 3.2.2 Temporal Correlation Modeling

**Instance interaction.** Similar to the interaction between instances in the spatial correlation module, the interaction between instances in different frames can also have an impact on the saliency of instances. Hence, we begin temporal correlation modeling by establishing edges between instances in adjacent frames and instances in the current frame. And the aggregation function is represented as:

\[h_{N_{i}^{t_{i}}}=_{j=1}^{N_{t-1}+N_{t+1}}_{ij}^{i}W_{a}^{i}f_{j};  36.135pt_{ij}^{i}=+N_{t+1}}ReLU((W_{}^{i})^{ T}(U^{i}f_{i}||V^{i}f_{j}))\] (2)

where \(N_{t-1}+N_{t+1}\) means the total number of instances in frame \(t-1\) and \(t+1\) and \(_{ij}^{i}\) is the attention weight modulating the edge between instance \(i\) and \(j\).

**Motion-wise contrast.** Merely constructing temporal connections by modeling instance interaction is not enough to model temporal saliency, as the human visual system tends to prioritize moving objects. Hence, it is necessary to introduce independent construction of motion representations for each instance. To achieve this, for instance \(i\), we double its bounding box and apply ROIAlign to extract features within this region in the previous and next frames and represent them as \(f_{i}^{t-1}\) and \(f_{i}^{t+1}\). By comparing the instance features with its local contextual features \(f_{i}^{t-1}\) and \(f_{i}^{t+1}\) in adjacent frames, we are able to model the saliency from the motion trajectory for instance \(i\). The aggregation function is represented as:

\[h_{N_{i}^{t_{i}}}=_{j=t-1,t+1}_{ij}^{t}W_{a}^{t}f_{i}^{j}; 36.135pt _{ij}^{t}=ReLU((W_{}^{t})^{T}(U^{t}f_{i}||V^{t}f_{j}))\] (3)

#### 3.2.3 Spatial-temporal Fusion

Another important issue is how to integrate spatial-temporal cues. An intuitive idea is to aggregate spatial and temporal correlation modeling results in a serial manner as done in . However, this sequential integration strategy may embed spatial clues in the instance features, which is counter-productive to extracting temporal saliency. To solve this problem, we propose to unify spatial and temporal correlation modeling and optimize them jointly.

Drawing inspiration from the graph updating methods outlined in , we extend the overall graph to \(K\) parallel subgraphs to stabilize the learning process and enrich the learned node interaction connections. Each subgraph independently learns node interactions. As a result, the instance feature can be represented as:

\[f_{i}^{i}=f_{i}^{t}+|K\\ k=1|(W_{u}^{r,k}h_{N_{i}^{t}}^{k}+W_{u}^{l,k}h_{N_{i}^{t}}^{k }+W_{u}^{g,k}h_{N_{i}^{g}}^{k}+W_{u}^{t_{i},k}h_{N_{i}^{t_{i}}}^{k}+W_{u}^{t_ {i},k}h_{N_{i}^{t_{i}}}^{k})\] (4)

### Loss Function

Our loss function can be computed as:

\[L=L_{det}+L_{SOR}\] (5)

\(L_{det}=L_{cls}+L_{box}+L_{mask}\) is defined in Mask R-CNN  to be used to detect the salient instance, where \(L_{cls}\), \(L_{box}\), \(L_{mask}\) denote the classification loss, regression loss and mask loss respectively. \(L_{SOR}\) is the ranking loss proposed by :

\[L_{SOR}=_{q=1}^{C_{N}^{2}}_{q}log(1+exp((-s_{q1}+s_{q2})))\] (6)

where \(_{q}\) is the dynamic loss weight and \(s_{q1},s_{q2}\) are saliency scores predicted by our network.

Experiment

### Experimental Setup

**Dataset collection and annotation.** Wang  propose a video saliency ranking task and the first dataset RVSOD, but they only focus on frame-level saliency and ignore the instance-level saliency. Therefore, RVSOD lacks instance-level annotations to evaulate VSOR models. To address this limitation, we utilize the manually annotated masks provided in RVSOD to obtain instance masks and assign the saliency ranking score to each instance based on the distribution of fixation points. In this way, the instance-level annotations are generated.

However, the RVSOD dataset suffers from two notable limitations: (a) a lack of complex scenes, and (b) most scenes consist of only one salient instance, rendering it unsuitable for evaluating the ability of salient object ranking (SOR). To address these issues, we utilize the video saliency detection dataset DAVSOD  to extract saliency ranking results, thereby making it more appropriate for video saliency ranking tasks. In order to fully utilize the DAVSOD, we classify the scenes in DAVSOD into three categories: (a) all video frames contain only one salient instance, (b) some video frames contain one salient instance, and (c) all video frames contain multiple salient instances. We discard the scenario described in (a) and proceed to divide the remaining scenes into training and testing sets in a 4:1 ratio for both scenarios (b) and (c). The generation of the instance-level annotations is the same as the RVSOD. The process of generating salient ranks and the statistical analysis of DAVSOD are presented in the supplementary material in section A.1.

**Evaluation metrics.** SA-SOR  and Mean Absolute Error (MAE) are used to evaluate the performance of our model in ranking and segmentation. SA-SOR can reflect both segmentation and ranking performance at the same time which make it appear more reliable than the original SOR  or the SSOR  metric. MAE serves as a reliable indicator of segmentation quality.

**Implementation details.** We adopt the training strategy outlined in , utilizing the pre-trained Mask R-CNN learned on the MS-COCO 2017 training split. The settings remain consistent throughout the experiment, with the box head solely dedicated to classifying objects and non-objects. Stochastic gradient descent (SGD) is employed to optimize the loss function. To facilitate training, we implement a warm-up strategy, commencing with an initial learning rate of 5e-3. At the 420,000 and 500,000 steps, the learning rate is reduced by a factor of 10.

Once the training of our detection branch is complete, we proceed to incorporate our graph module and fine-tune the entire network using two separate datasets - RVSOD and DAVSOD. Throughout this process, we set the batch size to 1 and the maximum iteration count to 200,000. For optimization, we utilize the Adam optimizer with an initial learning rate of 5e-6. At the 80,000th and 150,000th steps, the learning rate is reduced by a factor of 10. Our model is implemented using PyTorch and all experiments are conducted on a NVIDIA RTX4090.

### Compared to State-of-the-art Methods

   &  &  \\   & SA-SOR & MAE & SA-SOR & MAE \\   & Fang  & 0.350 & 0.0984 & 0.157 & 0.0760 \\  & Liu  & 0.563 & 0.0728 & 0.179 & 0.0639 \\  & PSR  & 0.405 & 0.074 & - & - \\  & SeqRank  & 0.512 & 0.0761 & - & - \\   & DCFNet  & - & 0.1180 & - & 0.0684 \\  & SCOTCH  & - & 0.1230 & - & 0.1126 \\   & Lin  & 0.560 & 0.0745 & - & - \\  & Ours & **0.603** & **0.0698** & **0.207** & **0.0626** \\  

Table 1: Quantitative results on the generated datasets. The bigger the SA-SOR, the better the performance of ranking. The smaller the MAE, the better the performance of segmentation. The best results are indicated in bold. The method by Lin  is not yet open-source, so we only asked them for the results on RVSOD.

**Quantitative results.**We compare our proposed network with six other state-of-the-art salient object ranking methods, including four image-based SOR methods, two video salient object detection methods and one available video-based SOR method. As shown in Table 1, our model largely outperforms previous methods on both SA-SOR and MAE, indicating the advantages of our model in modeling and fusing spatial-temporal saliency. We discuss the reasons for poor results on DAVSOD in the supplementary material in section A.2.

**Visual results.** We also perform visual comparisons to verify the advantages of our method. As shown in Figure 3, the results of  highlight those objects with distinguished static saliency cues (e.g., the green and red instances) such as larger size, closer distance or clearer appearance, while the blue instance with less static saliency cues yet rich motion cues is ranked the least salient, as this method only considers multi-scale spatial contrast and instance relation in an individual image. Although the method of  additionally takes the temporal cues into account, its rankings are also biased towards static saliency cues as it simply models temporal relations across the global representation of each frame, ignoring the instance-wise correspondence moving across frames.

In contrast, our method explicitly considers the motion cues of the instance and perform instance-level spatial and temporal reasoning in a unified graph. Our method successfully distinguish instances with less static saliency cues but semantically rich motion cues (e.g., the blue instances should be the central character).

### Ablation Study

**Effectiveness on the temporal interaction.** We firstly study the advantages of our motion-aware temporal correlation modeling. Specifically, the model that excludes temporary relationship modeling serves as the baseline. Then, we have explored three types of temporary relationship modeling, including GTRM (global-aware temporal relation modeling), ITRM (instance-aware temporal relation modeling) and our motion-aware temporal relation modeling (MTRM). For GTRM, we utilize the

   &  \\   & RVSOD & DAVSOD \\  Baseline (w/o TRM ) & 0.563 & 0.179 \\ Baseline + GTRM & 0.585 & 0.186 \\ Baseline + ITRM & 0.591 & 0.194 \\ Baseline + MTRM & 0.587 & 0.195 \\ Ours(joint) & **0.603** & **0.207** \\  

Table 2: Ablation study for our temporal interaction. Our baseline is the Liu’s  model without person prior. Then we consider three temporal features: global-aware temporal information(GTRM), instance-aware features(ITRM), and motion-aware features(MTRM). Our final method combines the above three temporal features simultaneously.

Figure 3: Visual results with our model and other sota methods. The scenarios are chosen from \(RVSOD/actioncliptest00256\) and \(RVSOD/actioncliptest00707\) respectively.

global features of the preceding and subsequent frames as temporal saliency cues. ITRM and MTRM represent instance interaction and motion-wise contrast in the method section respectively. Table 2 illustrates the large improvement by adding the motion-aware temporal interaction. It's obvious that either method greatly improves the ranking performance of the model compared to the baseline. Moreover, our proposed motion-aware feature is significantly superior to traditional temporal modules. Therefore, our final method combines the above three temporal features simultaneously.

In Figure 4, we observe that, when performing graph reasoning based purely on multi-scale spatial contrasts, the ranking shown in the \(3^{th}\) row tends to favor static saliency cues (e.g., larger-sized individuals or those in closer proximity). Despite introducing global inter-frame contrast ("w/GTRM") and inter-frame cross-instance contrast ("w/ITRM") can capture some temporal cues, they are still unable to effectively model instance motion. However, by incorporating our motion-aware temporal correlation modeling into the graph, our method can effectively identify instances with noticeable motion cues, leading to successfully highlight of motion instance and superior performance.

**Effectiveness on the local context size in motion-aware temporal relation modeling.** Without access to ground truth tracking annotations, it becomes quite challenging to model the motion trajectories of each individual instance accurately. Therefore, a motion-aware temporal relation modeling are proposed to evaluate the temporal saliency of the instance. However, as the instance moves and the camera shifts, simply projecting the bounding box onto adjacent frames is not enough to fully capture the changes in the instance. As shown in Table 3, double sized bounding box achieved the best results, while the excessive expand scale can easily introduce excessive background noise and degrade the results.

   &  \\   & RVSOD & DAVSOD \\  \(1\) & 0.6092 & 0.2421 \\ \(2\) & **0.629** & **0.246** \\ \(3\) & 0.5900 & 0.2374 \\ \(4\) & 0.5684 & 0.2393 \\  

Table 3: Quantitative comparison by varying the bounding box sizes.

Figure 4: Visual results on ablation study. GTRM means global-aware temporal relation modeling and ITRM means instance-aware temporal relation modeling. It is obvious that our motion-aware features can effectively model the motion information of instances. The scenarios are chosen from \(RVSOD/actioncliptrain00806\) and \(DAVSOD/select\_0674\) respectively.

**Effectiveness on the unified spatial-temporal reasoning.** Another question we want to investigate is how to make joint decisions based on spatial and temporal saliency cues. An intuitive choice is to forming the spatial and temporal graph separately and combine two graphs in a stage-wise manner. Hence, we involve two fusion variants for comparison. a) **Spatial-then-temporal**: start by aggregating three consecutive frames' spatial correlation graph independently and then fusion the temporal information using temporal correlation graph. b) **Temporal-then-spatial**: start by aggregating temporal correlation graph through the temporal context and then aggregate the spatial correlation graph with spatial context. c) **Our joint spatial-temporal graph reasoning**: It takes all features as inputs simultaneously and model their correlations adaptively for reasoning.

As shown in Table 4, two stage-wise reasoning strategies achieves similar improvement compared to the baseline that merely considers spatial-based reasoning, while our joint spatial-temporal reasoning strategy achieves a much higher improvement than them, demonstrating our method's better adaptivity and sufficiency in combining spatial and temporal saliency cues.

### Application to Video Retargeting

With the development of multi-media technology, people can watch videos on different devices such as smartphones and tablets. Video retargeting adjusts the videos to different aspect ratios to suit different endpoints. The purpose of image retargeting is to preserve visually salient areas for a better visual experience. Based on this, image retargeting has been extremely successful in the past through methods such as seam carving , cropping  and warping . However, simply applying these methods to video retargeting will lead to severe temporal inconsistency that greatly affect the human visual experience. Some works  have been proposed to alleviate temporal inconsistency. For example, Zhu  utilize a dynamic spatial-temporal buffer to reduce temporal inconsistency, while Apostolidis  utilizes cropping for video retargeting based on saliency detection . However, their method only distinguishes between foreground and background, ignoring the non-uniform saliency across instances in the foreground, thus usually resulting in biased retargeting to backgrounds or objects with less saliecency.

Based on our VSOR, we can incorporate the saliency of instances into retargeting, thereby more adaptively preserving important semantic concepts in multi-object scenes. As shown in Figure 5, based on the rank of each salient instance with our VSOR model, We assign weights to each instance to get the cropping center. The center of instance i with \(rank_{i}\) (the higher number means the more salient the instance i) is represented as \((x_{i},y_{i})\). Next based on the ranks of instances, we generate the

   &  \\   & RVSOD & DAVSOD \\  Baseline (w/o TRM ) & 0.563 & 0.179 \\ Spatial-then-temporal & 0.571 & 0.184 \\ Temporal-then-spatial & 0.590 & 0.183 \\ Ours(joint) & **0.603** & **0.207** \\  

Table 4: ablation study for our fusion methods. Our baseline is the Liu’s  model without person prior. Then we merge spatio-temporal relationships in three different orders.

Figure 5: Our method for video retargeting. We generate instance-level saliency information including bounding box, mask and ranks based on our VSOR model. Thus, we get the cropping center according to instance-level saliency information.

cropping center \((x_{c},y_{c})\) with the equation:

\[(x_{c},y_{c})=(_{i=1}^{N}rank_{i}(x_{i},y_{i}))(_{i=1}^{N} rank_{i})\] (7)

Then we follow the smartVidCrop  to generate the crop windows for each frame and utilize the LOESS  to overcome visual mutations caused by camera shake.

Our retargeting results are shown in Figure 6, we retarget the videos with the aspect ratio as 2:3 and 4:5 respectively. We compare our method with seam-carving based method proposed by Zhu  and crop-based method . It is evident that our approach offers superior visual experience compared to the other methods. For instance, while the method in  detects all salient instances, it causes significant distortion to those instances, which negatively impacts the video quality. In contrast, Apostolidis's method  sometimes fails to crop out all the salient instances or gets distracted by the background information, resulting in inaccurate cropping windows. This indicates that our VSOR provides more precise semantic importance for the frames, allowing for more adaptive and reasonable results. Additional examples are available in the supplementary material in section A.3.

## 5 Conclusion

In this work, we propose a novel unified spatial-temporal graph that explicitly integrates trajectory-wise temporal correlation and adaptively integrate spatial and temporal saliency cues. Specifically, the first step involves extracting the features of each instance through an object detector and a feature enhancement module. And then a spatial-temporal graph reasoning network to fuse spatial-temporal saliency cues, ultimately obtaining saliency scores and final results. Finally, we applied our VSOR model to video retargeting and achieved impressive results.

**Limitations.** In our model, a detector is used to extract instance features. However, in different scenarios, the same object may exhibit different levels of saliency. For example, we should detect a tree in a desert but not in a forest. For non-salient objects, although our model assigns them a lower saliency ranking, we ideally do not want them to be detected at the first. In the future, we aim to design a saliency-sensitive detector to further improve performance.

## 6 Acknowledgement

This work was supported in part by the National Natural Science Foundation of China (NSFC) under Grant 62102083, Natural Science Foundation of Jiangsu Province under Grant BK20210222, the National Natural Science Foundation of China (NSFC) under Grant 62261160576, 62203024 and 92167102, and the R&D Program of Beijing Municipal Education Commission (KM202310005027).

Figure 6: Comparison of retargeting, (a) is the method of seam carving , (b) is smartVideoCrop , (c) is our methods. The yellow box in (a) shows the artifacts after retargeting.