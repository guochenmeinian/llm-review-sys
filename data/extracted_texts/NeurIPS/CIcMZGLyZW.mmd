# Neuro-Symbolic Data Generation for Math Reasoning

Zenan Li\({}^{1}\)1  Zhi Zhou\({}^{1}\)1  Yuan Yao\({}^{1}\)  Yu-Feng Li\({}^{1}\)

Chun Cao\({}^{1}\)  Fan Yang\({}^{2}\)  Xian Zhang\({}^{2}\)  Xiaoxing Ma\({}^{1}\)

\({}^{1}\)State Key Lab of Novel Software Technology, Nanjing University, China

\({}^{2}\)Microsoft Research Asia

lizn@smail.nju.edu.cn, zhouz@lamda.nju.edu.cn,

zhxian@microsoft.com, xxm@nju.edu.cn

Equal contribution. This work was partially done during Zenan's internship at MSRA.

###### Abstract

A critical question about Large Language Models (LLMs) is whether their apparent deficiency in mathematical reasoning is inherent, or merely a result of insufficient exposure to high-quality mathematical data. To explore this, we developed an automated method for generating high-quality, supervised mathematical datasets. The method carefully mutates existing math problems, ensuring both diversity and validity of the newly generated problems. This is achieved by a neuro-symbolic data generation framework combining the intuitive internalization strengths of LLMs, and the precise symbolic reasoning of math solvers along with projected Markov chain Monte Carlo sampling in the highly-irregular symbolic space. Empirical experiments demonstrate the high quality of data generated by the proposed method, and that the LLMs, specifically LLaMA-2 and Mistral, when realigned with the generated data, surpass their state-of-the-art counterparts.

## 1 Introduction

Despite recent progress , both proprietary and open-source LLMs are still far from satisfactory in mathematical reasoning . It is an open question whether LLM's subpar reasoning capability is inherent or due to the the extreme scarcity of high-quality mathematical datasets . As an initial step towards answer this question, a data generation framework that could create high-quality math datasets is required. To this end, current two lines of research struggle in the _diversity-validity_ dilemma: (1) to produce diverse math data, the prompt-based method effectively rephrases math problems using LLMs, but may induce errors thus ruining the _validity_, especially considering the rigor of maths; (2) to ensure the validity, template-based methods are often used by rewriting math problems with certain rules, sacrificing the _diversity_ and thus confining data scale.

To address this dilemma, we propose a novel _neuro-symbolic_ framework that automatically generates high-quality, supervised mathematical data. The merit of this paradigm lies in leveraging both neural and symbolic strengths: (1) the math problem is generated in the symbolic space, achieving diversity through systematic sampling, while maintaining validity through symbolic solvers; (2) the translation from the symbolic space back to the natural language space can be effectively supported by LLMs, ensuring the consistency between newly generated formal problems and their corresponding natural language versions.

Our framework, as illustrated in Figure 1, initiates with the formalization of the original problem expressed via the math symbolic tools. Next, it _mutates_ the formal problem into an evolved version, and then derives a new natural language problem by _informalization_. Specifically, we design a mutation mechanism, including various simplification and complication strategies, such that the new problems can be generated with a controllable complexity. As shown in Figure 2, our mutationmechanism can properly adjust the complexity of generated problems, and the exposure to more complex math problems can improve the LLM's reasoning capability. Moreover, to ensure the data validity and achieve higher generation diversity, we combine the symbolic solving with the random sampling through the projected Markov chain Monte Carlo technique [14; 15].

Empirical evaluation on GSM8K  and MATH  demonstrates the effectiveness of the proposed method. Particularly, we use the proposed framework to generate a mathematical dataset of 620K examples for supervised fine-tuning. The experimental results show that, the fine-tuned models on LLaMA-2  and Mistral-7B  significantly outperform the existing open-source counterparts on both GSM8K and MATH datasets, as well as two out-of-domain datasets SVAMP  and ASDiv . On the GSM8K dataset, the model fine-tuned on Mistral-7B even outperforms GPT-3.5-Turbo (by 2.4%), a proprietary model with an order of magnitude larger parameters. Additionally, we evaluate the scalability of our method, and observe consistent performance improvements, as the size of training data increases. This upward trajectory suggests a promising avenue for further enhancing LLMs' mathematical capabilities.

## 2 Mutation

Compared to existing data generation methods, the key feature of our framework lies in the mutation of math problems within the symbolic space, enabling systematic sampling and symbolic solving. Technically, our mutation mechanism includes several simplification and complication strategies, to control the complexity of the generated math problems. The overall framework of our problem mutation method is summarized in Algorithm 1.

Figure 1: The overview of our neuro-symbolic data generation framework. The framework comprises three steps: (1) Formalize the seed problem into its symbolic version. (2) Mutate the symbolic problem to create new variants. (3) Translate the variants in symbolic form back to the natural language version. Additionally, we prompt GPT-4 to generate reasoning paths, which are verified by symbolic solvers, as part of the supervision.

Figure 2: The performance of our proposed mutation mechanism. The first figure illustrates that the generated problems with higher difficulty levels lead to more reasoning steps of GPT-4. The second figure shows that the gradual incorporation of more difficult problems consistently improves the LLMâ€™s reasoning capability.

### Formalization

We first provide the formalization of math problems, based on which the mutation mechanism is operated. Specifically, we adopt the SMT-LIB language , a standard notation compatible with prevalent SMT solvers (e.g., Z3 , CVC5 , and MathSAT ). It can also be easily extended for symbolic calculators (e.g., SymPy ) and numerical solvers (e.g., SciPy ). With SMT-LIB language, the math problem in the following structure is enabled:

\[&g:=\ f( )\\ &h:=h_{1} h_{2} h_{1} h_{2}(h_{1},h_{2},h_{3})\\ &.\ e_{1}() e_{2}() .\ e_{1}() e_{2}()\\ &e_{1} e_{2},\ \ \{,,>,<,=,\}\\ &e:=c:=(x_{1},,x_{n})()\\ &e_{1} e_{2},\ \ \{+,-,,\}\\ &:=^{+} \]

where \(c\) denotes a constant, \(\) denotes an \(n\)-dimensional variable, \(\) denotes the if-then-else structure, \(\) refers to an _interpreted_ function (e.g., trigonometric, logarithmic or user-defined ones) on the domain, and \(g\) and \(h\) represent any function of interest (can include quantifiers). In particular, we pre-defined a series of interpreted functions, such as summation, binomial, gcd, lcm, derivate, and integral, which facilitate the formalization of most high-school level mathematical problems (excluding geometry) within the above SMT-LIB language.

### Simplification

We perform simplification by systematically considering _expression reduction_ and _constraint reduction_, which can be attained through heuristic tactics provided by standard symbolic solvers .

Specifically, we apply the _simplify_ tactic for expression reduction, which involes operations such as constant or variable folding (e.g., \(x+0 x\) or \(y+x-x y\)), expression expansion (e.g., \((x+1)^{2} x^{2}+2x+1\)), and function application (e.g., \((x=2)(y=(x)) y=(2)\)); we also perform symbolic and numerical computations for further reductions (e.g., \((2x,6y) 2(x,3y)\) and \((/6) 0.5\)).

For constraint reduction, we mainly employ the Gaussian elimination tactic _gaussian_elim_ (e.g., \(x=2 y x+z y 2+z\)). To handle the if-then-else term, we apply the _elim_term_ite_ tactic to decompose it by introducing a fresh variable (e.g., \((x>y,x,y)>z(k>z)(x>y k=x)(x y  k=y)\)). For constraints involving quantifiers, we strive to eliminate them using the _qe_ tactic (e.g., \( y.(y>0)(x=y+2) x>2\)). Appendix B provides more examples illustrating these simplifications.

### Complication

To _complicate the expressions_, a straightforward strategy is to incorporate additional operators. For example, given an atomic constraint \(h=e_{1} e_{2}\), we can introduce an additional expression, denoted by \(e^{}\), and derive a more complex constraint \(=e_{1}(e_{2} e^{})\).

\[(_{1})\{a(b+c)=152\\ b(c+a)=162\\ c(a+b)=170\\ a,b,c^{+}.(_{2}) \{a(b+c)=152_{1}e^{}_{1}(z_{1})\\ b(c+a)=162_{2}e^{}_{2}(z_{2})\\ c(a+b)=170_{3}e^{}_{3}(z_{3})\\ a,b,c^{+},z_{1},z_{2},z_{3}.\] \[(_{3})\{a(b+c)+z _{1}=152\\ b(c+a)-z_{2}=162\\ c(a+b)=170\\ z_{1}=114\\ z_{2}=36\\ a,b,c^{+}.(_{4}) \{a(b+c)+d=152\\ b(c+a)-e=162\\ c(a+b)=170\\ d+e=150\\ d-e=78\\ a,b,c^{+}.\]

However, such a strategy is non-trivial in practice. The first challenge lies in the _validity_ aspect, i.e., the math problem is often carefully designed, and thus a random mutation may ruin their well-defined structure. Consider the running example problem \((_{1})\), which has been normalized for simplicity. In this problem, a reckless mutation can easily violate the positive integer constraints, causing the problem ill-defined and unsolvable.

To address this issue, we equip each mutation with an auxiliary variable, followed by symbolic solvers to ensure the problem remains well-defined. Continuing with the previous example, we introduce three auxiliary variables, denoted by \(z_{1},z_{2},z_{3}\), and then mutate the problem as \((_{2})\), where \(_{1},_{2},_{3}\{+,-,,\}\) represent three random operators. Furthermore, we instantiate \(e^{}_{1},e^{}_{2},e^{}_{3}\) by inverted functions, i.e., \(e^{}_{1}=_{1}(z_{1}),e^{}_{2}=_{2}(z_{2})\), and \(e^{}_{3}=_{3}(z_{3})\), where \(_{1},_{2},_{3}\) are randomly selected from \((z)=z(z)(z)(z)\). For our running example, we simply choose the identity function for \(_{1},_{2},_{3}\), and set \(_{1}=-,_{2}=+,_{3}=\). Using symbolic solvers to compute a feasible solution of \((z_{1},z_{2},z_{3})\), we derive a new and well-defined problem \((_{3})\).

The subsequent challenge is to ensure the _diversity_ of the mutated problems, which now becomes how to make the solutions of auxiliary variables sufficiently diverse. This is essentially a model counting problem [26; 27], and current symbolic solvers still underperform in this regard . To this end, we instead opt for auxiliary variable solution generation via the _projected Markov chain Monte Carlo_ (projected MCMC) [14; 15]. Simply put, projected MCMC first perturbs a subset of variables (projected random walk), and then resolves the remaining part (inverse projection via symbolic solvers), which ensures both diversity and validity of the variable solutions.

Finally, to _complicate the constraints_, one can easily reverse the process of simplification. For our running example, we can reverse the Gaussian elimination with refreshed variables, obtaining the final form \((_{4})\), which is then included as a new problem in the dataset.

## 3 Informalization

Informalization aims to translate a formal problem back to natural language without the loss of soundness . As shown in Example 1, a simple, one-line instruction follows the formally posed SMT-LIB problem, serving as the input. Then, GPT-4 interprets the formal problem as a new math word problem.

The key challenge of informalization lies in ensuring a _consistent_ conversion, i.e., the natural language problem informalized by GPT-4 should align with the formal solution given by symbolic solvers. Since it is difficult to directly measure this consistency, we instead use GPT-4 to generate a solution for each informalized problem, and then calculate the _consistency rate_ between the solutions from GPT-4 and those from symbolic solvers as a surrogate metric. Furthermore, we observe that, if the problem is _incorrectly_ informalized, GPT-4's solutions almostly cannot be confirmed by symbolic solvers (i.e., zero false positive). Therefore, the surrogate consistency rate can be regarded as a lower bound to the true consistency rate.

To further improve conversion consistency, in addition to the basic zero-shot learning template in Example 1, we investigate the effects of the following operations, whose detailed examples are available in Appendix C.

(1) **Mutation**. Mutation complicates the problem, making the normalization more difficult. Therefore, we first analyze the normalization error caused by the mutation.

(2) **Few-shot learning**. Few-shot examples offer a stronger instruction to the LLM, and also introduce the randomness when aided by random retrieval.

(3) **Comment generation**. Recognizing that GPT-4 is unfamiliar with SMT-LIB's prefix expressions, we automatically convert these into the infix format, included as comments.

(4) **Math-word instruction**. We simply append one more sentence "Ensure to be a math word problem" in the prompt. With this instruction, normalization tends to imbue digits with some practical meaning (e.g., 7 \(\) one week).

(5) **Problem modification**. For mutated problems, rather than generating a new informalization, we prompt GPT-4 to modify the original informalization result.

(6) **Variable refresh**. We standardize the naming of all introduced variables (e.g., \(\)\(\) x_1), to eliminate the impact of math word problems.

Different combinations of the above operations result in different patterns. The effects of some typical patterns are shown in Table 1, where the results are evaluated on 1,000 problems randomly sampled from GSM8K. The basic pattern in Example 1 yields a consistency rate of 75.6%. The mutation operation alone indeed degrades the consistency, but its combination with other operators can further boost the informalization performance. In practice, we use two different patterns for different normalization styles: the first pattern (P1) tends to generate math word problems, whereas the problems generated by the second pattern (P2) tend to be pure math problems.

   Ops & (1) & (2) & (3) & (4) & (5) & (6) & Rate (\%) \\   & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & 75.6 (\(\)) \\  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & 41.6 (\(\)) \\   & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & 76.2 (\(\)) \\   & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & 87.6 (\(\)) \\  P1 & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & 90.5 (\(\)) \\  P2 & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & 97.1 (\(\)) \\   

Table 1: Consistency rate of six different operations used in informalization: (1) Mutation; (2) Few-shot learning; (3) Comment generation; (4) Math-word instruction; (5) Problem modification; (6) Variable refresh. We recommend two patterns (i.e., P1: 1-5 and P2: 1-3&6), both of which can achieve satisfactory results.

Experiments

In this section, we conduct a series of experiments to answer the following four research questions:

**RQ1: Efficacy** - Using our data generation framework, can the fine-tuned model achieve better performance compared with existing models?

**RQ2: Efficiency** - Given the same data generation budget, is the generated data from our framework better than that from the state-of-the-art data generation framework?

**RQ3: Generability** - Is the effectiveness achieved by our framework due to potential data contamination introduced during the generation process?

**RQ4: Scalability** - With more data generated, can our approach be continually effective in further improving model performance?

### Experimental Setup

**Dataset**. We conduct our data generation on the training sets of two popular mathematical reasoning benchmarks: GSM8K  and MATH . GSM8K is a dataset comprising high-quality grade school math problems, which contains 7,473 training data and 1,319 testing data. MATH is a dataset comprised of challenging competition math problems, spanning seven subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus. There are 7,500 training data and 5,000 testing data in the MATH dataset. Additionally, we include two mathematical reasoning datasets, i.e., SVAMP  and ASDiv , to evaluate the out-of-domain generalizability of the models fine-tuned on the data generated from GSM8K and MATH datasets.

**Comparison Methods**. In our experiments, we compare the models trained using our generated data with existing state-of-the-art open-source mathematical reasoning models, including WizardMath , MuggleMATH , MAmmoTH , and MetaMath . We also conduct a thorough comparison between our math generation method and the bootstrapping method employed in MetaMathQA , which is presently the most extensive open-source dataset for mathematical reasoning.

**Data Generation Details**. We use our mutation mechanism to generate a series of problems with varying levels of difficulty, and the specifics are as follows. Starting with a problem from the original dataset as a seed, we first perform simplification to the problem, and define this new version as level-0. Then, we randomly apply one expression complication step and one constraint complication step to the level-0 version, deriving a more difficult problem (level-1 version); and such complications can be repeated to obtain more difficult problems. For the GSM8K dataset, we create datasets across five levels of difficulty, with 30K examples at level-0 and 100K examples for the remaining four levels. As for the MATH dataset, we establish four levels of difficulty, where level-0, level-1, level-2, and level-3 contain 70K, 120K, 120K, and 120K examples, respectively. Particularly, for some problems in the MATH dataset that cannot be solved by symbolic solvers, we directly prompt GPT-4 to rephrase the problem and ignore the solution verification. The number of generated problems without solution verification varies across problem categories, and the details can be referred to Appendix D. In total, we generated 860K math problems based on the proposed framework to construct our dataset.

Each generated math problem consists of a natural language problem description informalized by GPT-4 (version 0710), a final answer outputted by the symbolic solver, and a reasoning path from the problem to the final answer. The reasoning path for each problem is also generated by GPT-4, which is further verified by the corresponding answer derived from symbolic solvers.

More implementation details about training hyperparameters, instruction prompts, and symbolic solver integration, are included in Appendix D.

### Empirical Results

**RQ1: Efficacy**. Using the generated math dataset, we fine-tune the LLaMA-2 base models of 7B and 13B parameter sizes, as well as the Mistral 7B base model. The fine-tuned models, as well as the comparison methods, are evaluated on the GSM8K and MATH datasets.

As shown in Table 2, our approach achieves the best performance among the baseline models across different model scales. Compared to LLMs with the LLAMA-2 7B base model, our model demonstrates a fair improvement in accuracy, surpassing them by at least 10.6% on the two datasets. For our model fine-tuned using the LLAMA-2 13B base model, our model achieves an accuracy of 84.1% and 33.7%, outperforming existing SOTA model by 10.1% and 11.3%. When fine-tuned on the Mistral 7B base model, our model still attains the best performance with an increase in accuracy of 3.6% and 4.3%, respectively. Notably, our model even slightly outperforms GPT-3.5-Turbo (80.8% and 34.1%) by 6.0% on the GSM8K dataset and 3.2% on the MATH dataset.

In addition to the above competitors, we also compare our models with tool-based models, and provide the results in Appendix E.2. We summarize two observations here. First, tool-based models tend to over-rely on external tools, and thus do not necessarily improve the inherent reasoning ability of LLMs. Second, although the tool-based models perform better on the MATH dataset (which frequently entails complex calculations), they still underperform on the GSM8K dataset (which emphasizes knowledge reasoning but involves simpler calculations).

**RQ2: Efficiency**. To illustrate the data efficiency of our method, we carry out a comparative experiment with current SOTA method MetaMathQA . The MetaMathQA dataset comprises 240K data bootstrapped from the GSM8K training dataset and 155K data from the MATH training dataset. To ensure a fair comparison with MetaMathQA, we use the same data budget. Additionally, we expand the MATH data of the MetaMathQA dataset to 430K, aligning its size with that of our generated data. Then, we fine-tune LLaMA-2-7B models on the 240K GSM8K augmented data, as well as the 155K and 430K MATH augmented data, respectively.

The performance of fine-tuned models are given in Table 3. We also evaluate the models on two out-of-domain datasets, SVAMP and ASDiv. The results confirm the efficiency of our framework. With an equal generation budget of 240K for the GSM8K dataset and 155K for the MATH dataset, our method exhibits accuracy improvements, ranging from 2.1% to 24.4% across the four datasets. This superiority is consistent with the model trained on 430K MATH generation data, with improvements of 19.9%, 3.2%, 12.7%, and 19.2%, respectively.

**RQ3: Generability**. Despite we carefully ensure that our mutations on the training set do not access the test set, we still provide a series of analysis about the potential data contamination or overfitting issues. We first use the memorization detection method introduced in Minerva . Specifically, we select 150 problems with the highest majority vote score and then compute the BLUE score  on solutions of trained Mistral 7B, GPT-4, and the ground-truth. The results of our method and MetaMath are provided in Figure 3, which show that our BLUE score on the test set is (1) much lower than our BLUE score on the training set; (2) is consistent with that of MetaMath. Hence, there is no evidence that the mutation contaminates the test set.

Second, in addition to the two out-of-distribution datasets SVAMP and ASDiv, we conduct experiments on another benchmark DyVal , which avoids the leak of test set through dynamically generating new benchmarks. The results of our models, alongside those of the comparison models, are provided in Appendix E.3. In summary, our models demonstrated superior performance in 11 out

    &  &  &  &  \\   & & GSM8K & MATH & GSM8K & MATH & GSM8K & MATH \\   WizardMath & 240K & 54.9 & 10.7 & 63.9 & 14.0 & 83.2 & 33.0 \\ MuggleMATH & 157K & 68.4 & 8.4 & 74.0 & 9.4 & - & - \\ MAminoTH\({}^{}\) & 260K & 50.5 & 10.4 & 56.3 & 12.9 & 61.9 & 17.5 \\ MetaMath & 395K & 66.5 & 19.8 & 72.3 & 22.4 & 77.7 & 28.2 \\  Ours & 860K & **79.0** & **30.4** & **84.1** & **33.7** & **86.8** & **37.3** \\  \(\) & & \(\) 10.6 & \(\) 10.6 & \(\) 10.1 & \(\) 11.3 & \(\) 3.6 & \(\) 4.3 \\   ^{}\) Model performance is re-evaluated using Pass@1 of CoT prompt.} \\ 

Table 2: Performance comparison among existing mathematical reasoning models fine-tuned on three base models (LLaMA-2 7B, LLaMA-13B, and Mistral 7B). The best performance is in bold. The delta performance between our model and other SOTA LLMs on each dataset is also reported.

of 12 cases and delivered competitive results in the remaining case. Particularly, as the complexity of the tasks increased, our models exhibited a relatively robust performance compared to other models.

Finally, we include an additional experiment on the Hungarian High School National Finals Exam dataset , whose problems are newly collected at 2023. We manually check 33 testing problems based on the provided ground-truth answer, and our model correctly solved 14 problems and partially solved 6 problems, resulting in an exam score of 44. The result is comparable to GPT3.5-turbo (i.e., 41 exam score), conforming the generalizability of our method.

**RQ4: Scability**. To explore the scalability of our framework, we fine-tune the LLaMA-2 7B model using our generated datasets of various sizes and difficulties. To be specific, we progressively incorporate a 30K dataset, along with four additional 70K datasets generated from GSM8K. Note that these five datasets are randomly sampled from five different levels of difficulty. Five LLaMA-2-7B models are fine-tuned based on these datasets, and the scalability curves are shown in Figure 4. We also include MetaMathQA with the same data settings as a reference. Since MetaMathQA cannot inherently group the dataset into various difficult levels, we construct five datasets by incrementally random sampling from the GSM8K subset of the MetaMathQA dataset.

The results presented in Figure 4 indicate the promising scalability of our method. That is, as the size of data increases, the accuracy of the model consistently improves. In contrast, the performance enhancement observed in MetaMathQA is limited and starts to diminish as the data size reaches 70K. We also present the models' performance on the other three out-of-domain datasets in this case, i.e., SVAMP, ASDiv, and MATH datasets. The results demonstrate that the scalability of our method is robust and generalizable, while MetaMathQA hardly guarantees such consistency.

   &  \\ Method & GSM8K & Math & GSM8K & Math & SVAMP & ASDiv \\   MMQA & 240K & OK & 66.1 & 5.8 & 61.7 & 72.5 \\ Ours & 240K & OK & 72.7 & 8.2 & 78.8 & 79.2 \\  &  & 12.3 & 12.1 & 17.1 & 66.7 \\  MMQA & OK & 155K & 28.6 & 19.9 & 49.0 & 61.0 \\ Ours & 0K & 155K & 42.1 & 22.0 & 73.4 & 64.1 \\  &  & 12.1 & 24.4 & 7.3 \\  MMQA & 240K & 155K & 67.5 & 21.7 & 64.1 & 76.0 \\ Ours & 240K & 155K & 73.7 & 23.4 & 85.2 & 81.1 \\  &  & 11.7 & 12.1 & 7.0 \\  MMQA* & ok & 430K & 35.0 & 25.6 & 66.4 & 51.1 \\ Ours & 0K & 430K & 54.9 & 28.8 & 79.1 & 70.3 \\  &  & 13.2 & 12.7 & 19.2 \\  

Table 3: Comparison between our method and MetaMathQA (MMQA) with the same data budgets. The models are fine-tuned using LLaMA-2-7B base model, and evaluated on GSM8K, MATH, SVAMP, and ASDiv datasets. The results illustrate the high quality of our generated data.

Figure 4: Performance curves of the LLaMA-2-7B models fine-tuned on various scales of datasets. The two datasets are generated by our approach and MetaMath (MMQA). The performance can be consistently enhanced by increasing the amount of data generated using the proposed framework.

Figure 3: BLUE scores between the output of our fine-tuned model, versus the ground-truth solution and GPT-4 output. The model is fine-tuned on the Mistral 7B base model, and MetaMath Mistral 7B model is also included as a reference. The results show that our method does not induce data contamination.

We also investigate the diversity gain relative to the original dataset for each difficulty level. The results are provided in Appendix E.4. It is observed that the dataset consisting of the same difficulty level cannot further improve the diversity with a larger data budget. On the contrary, the diversity gain of the dataset comprising all difficulty levels continues to increase as the data budget grows.

## 5 Related Work

Recent surveys [37; 38; 39] have comprehensively discussed the current advances in the mathematical reasoning of LLMs. Here, we review three main lines of existing work on enhancing the mathematical reasoning for LLMs related to our study: prompt-based methods, rephrasing-based methods, and tool-based methods.

**Prompt-based Method**. Prompt-based methods aim to harness the inherent capabilities of LLMs by carefully designing appropriate input prompts without tuning the model parameters. This line of work starts from the observation that LLMs can effectively tackle more math problems when provided with a simple Chain-of-Thought (CoT) prompt, i.e., "Let's think step by step" . Building upon the CoT prompt, Wang et al.  further propose to consolidate multiple reasoning paths based on the self-consistency of correct answers. Later, several researchers propose to prompt LLMs to decompose complex problems. For example, Zhou et al.  introduce the least-to-most strategy that prompts LLMs to break down the original problem into a series of sub-problems. Khot et al.  further boost this strategy, by assigning each sub-problem to the corresponding LLM that is specifically optimized for it. Finally, few-shot prompting, e.g., Few-shot CoT  and Complex CoT , has also been studied to enhance the reasoning performance of LLMs. To further improve the few-shot prompting, the prompt retrieval is proposed to automatically select high-quality examples [46; 47], while the prompt compression is explored to include more examples in restricted context by pruning each example .

**Rephrasing-based Method**. The second line of existing work aims to generate additional math data, based on which the mathematical reasoning capability of LLMs can be established via supervised fine-tuning. To address the data scarcity issue, current research mainly focuses on rephrasing the problem or the answer. For the answer rephrasing, Magister et al.  adopt the PaLM and GPT-3 to generate CoT math data, resulting in improved performance of the T5 model on math reasoning tasks. To mitigate the inclusion of incorrect answers during the supervised fine-tuning, RFT  introduces a rejection sampling strategy, whereas AFT  trains an LLM to categorize them. Regarding the problem rephrasing, WizardMath  proposes a reinforced evol-instruct method. It instructs ChatGPT and trains a new LLM to rephrase the problem, equipped by a reward model for evaluating the quality of generated problems. Combining the rephrasing of problems and answers together, MuggleMATH  builds the AugGSM8K dataset based on prompting GPT-3.5 and GPT-4. MetaMath  develops a question bootstrapping method based on LLMs, unifying rephrasing, self-verification , FOBAR , and answer augmentation strategies, obtaining the MetaMathQA. Xwin-Math  is a peer study that significantly enhances the reasoning capacity of LLMs using problems generated by GPT-4 Turbo. In contrast, our work focuses on generating verifiable problems through controllable mutations, rather than relying entirely on the GPT model.

Our proposed method also falls into this category. In contrast to existing methods directly prompting LLMs to rephrase the problem, we mutate the problem in the formal symbolic space, resulting in a more controllable mutation mechanism that ensures both the validity and diversity of the generated problems. Moreover, the quality of reasoning paths is also guaranteed by the symbolic solvers.

**Tool-based Method.** Tool-based methods aim to enhance the math solving performance of LLMs by instructing them to use external math tools. For instance, PoT (Program of Thought)  and PAL  propose to prompt the LLMs to delegate the computation to a program interpreter, which can be executed to obtain the final answer. To further improve the tool-using ability of LLMs, MathCoder  constructs a math dataset containing problems and their code-based solutions for the supervised fine-tuning; MAmmoTH  builds a dataset that combines CoT and PoT reasoning, enabling LLMs to perform hybrid inference. Since the interaction with math tools can further boost the performance of LLMs, TVA  includes the Isabelle theorem prover to check each reasoning step and guide the reflection of LLMs; Tora  generates interactive tool-use trajectories on mathematical datasets and then performs imitation learning on the annotations.

Our proposed method shares some similarities with tool-based approaches as both involve symbolic solvers. However, rather than using external tools to solve mathematical problems, our approach aimsto explore the inherent reasoning capability of LLMs. Therefore, symbolic solvers are only used to ensure the validity of the generated data as well as the correctness of the generated reasoning paths.

## 6 Limitations

**The Capability of Symbolic Solvers**. The effectiveness of our approach significantly hinges on the symbolic solvers. However, existing mathematical tools (e.g., Z3 , SymPy , and SciPy ) face limitations when it comes to expressing and solving a wide array of mathematical problems. For instance, the Z3 SMT solver struggles with expressing higher-order concepts like gcd and 1cm, while the SymPy encounters difficulties in solving inequalities involving multiple variables. In our framework, we integrate five mathematical tools, i.e., Z3, CVC4 , MathSAT , SymPy, and SciPy, and employ SMT-LIB  as a unified formal language to enhance the performance of symbolic solving.

**The Expressiveness of Mutations**. The mutation operators used within our framework remain limited, especially in generating more difficult problems (e.g., college- and even IMO-level math problems). One of our future work is to introduce more mutation operators, further increasing the problem difficulty. A possible strategy is the problem fusion , which fuses two formal problems into a single, new problem, rather than merely modifying an individual problem. Moreover, the informalization facilitated by LLMs can effectively mitigate the unnaturalness issue stemming from brute-force fusion.

**The Dependence on GPT-4**. GPT-4 is involved in our framework to carry out the informalization and generate the reasoning paths. We also consider the possible solutions that the dependence on GPT-4 can be gradually removed. First, by leveraging our generated formal-informal pairs, we can fine-tune a new LLM specifically for the informalization. Second, it is possible to bypass the generation of reasoning paths, through curriculum learning [62; 63] instead of supervised fine-tuning. Particularly, the reward in the curriculum learning can be determined by whether the generated solution is consistent with symbolic solvers, and the curriculum progresses by incorporating problems of various difficulty levels.

## Broader Impact

The paper aims to advance the field of math data generation. There are many potential societal consequences of our work, and we firmly believe that the majority of these impacts are positive and none which we feel must be highlighted here.

## 7 Conclusion

This paper explores the question of whether sufficient exposure to high-quality mathematical data could enhance LLMs' inherent mathematical reasoning capability. We identify a key challenge in balancing diversity and validity in current math problem generation methods. To tackle this challenge, we propose a neuro-symbolic framework that initially generates formal mathematical problems and then informalizes them back into natural language versions. By casting the data generation into the formal language space, the diversity and validity of the generated math problems can be effectively ensured by the systematic sampling and symbolic solvers. Building upon this, we carefully devise a mutation mechanism, establishing the math dataset encompassing various difficulty levels, and prompt the LLMs to accomplish informalization. Through empirical experiments, we demonstrate that our neuro-symbolic data generation framework significantly enhances the performance of various LLMs in mathematical reasoning tasks, surpassing the current state-of-the-art open-source models. The results also suggest a promising pathway for further enhancing LLMs' mathematical capabilities.

In future work, we intend to expand the expressiveness of mutations and enhance the capability of symbolic solvers to support more types of problems, such as inequality problems. Our goal is to offer a data generation framework to automatically generate high-quality, supervised datasets for LLMs. We expect that our neuro-symbolic data generation framework can provide a potential solution for LLMs to solve the problem of data scarcity, and thereby facilitate in building more LLMs in downstream tasks. Further, our framework has the potential to be integrated with recent studies , which only require problems and final answers.