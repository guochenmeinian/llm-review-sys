# On Calibrating Diffusion Probabilistic Models

Tianyu Pang\({}^{ 1}\), Cheng Lu\({}^{2}\), Chao Du\({}^{1}\), Min Lin\({}^{1}\), Shuicheng Yan\({}^{1}\), Zhijie Deng\({}^{1}\)\({}^{1}\)

\({}^{1}\)Sea AI Lab, Singapore

\({}^{2}\)Department of Computer Science, Tsinghua University

\({}^{3}\)Qing Yuan Research Institute, Shanghai Jiao Tong University

{tianyupang, duchao, linmin, yansc}@sea.com; lucheng.lc15@gmail.com; zhijied@sjtu.edu.cn

Corresponding authors.

###### Abstract

Recently, diffusion probabilistic models (DPMs) have achieved promising results in diverse generative tasks. A typical DPM framework includes a forward process that gradually diffuses the data distribution and a reverse process that recovers the data distribution from time-dependent data scores. In this work, we observe that the stochastic reverse process of data scores is a martingale, from which concentration bounds and the optional stopping theorem for data scores can be derived. Then, we discover a simple way for _calibrating_ an arbitrary pretrained DPM, with which the score matching loss can be reduced and the lower bounds of model likelihood can consequently be increased. We provide general calibration guidelines under various model parametrizations. Our calibration method is performed only once and the resulting models can be used repeatedly for sampling. We conduct experiments on multiple datasets to empirically validate our proposal. Our code is available at https://github.com/thudzj/Calibrated-DPMs.

## 1 Introduction

In the past few years, denoising diffusion probabilistic modeling [17; 40] and score-based Langevin dynamics [42; 43] have demonstrated appealing results on generating images. Later, Song et al.  unify these two generative learning mechanisms through stochastic/ordinary differential equations (SDEs/ODEs). In the following we refer to this unified model family as diffusion probabilistic models (DPMs). The emerging success of DPMs has attracted broad interest in downstream applications, including image generation [10; 22; 48], shape generation , video generation [18; 19], super-resolution , speech synthesis , graph generation , textual inversion [13; 34], improving adversarial robustness , and text-to-image large models [32; 33], just to name a few.

A typical framework of DPMs involves a _forward_ process gradually diffusing the data distribution \(q_{0}(x_{0})\) towards a noise distribution \(q_{T}(x_{T})\). The transition probability for \(t[0,T]\) is a conditional Gaussian distribution \(q_{0t}(x_{t}|x_{0})=(x_{t}|_{t}x_{0},_{t}^{2})\), where \(_{t},_{t}^{+}\). Song et al.  show that there exist _reverse_ SDE/ODE processes starting from \(q_{T}(x_{T})\) and sharing the same marginal distributions \(q_{t}(x_{t})\) as the forward process. The only unknown term in the reverse processes is the data score \(_{x_{t}} q_{t}(x_{t})\), which can be approximated by a time-dependent score model \(_{}^{t}(x_{t})\) (or with other model parametrizations). \(_{}^{t}(x_{t})\) is typically learned via score matching (SM) .

In this work, we observe that the stochastic process of the scaled data score \(_{t}_{x_{t}} q_{t}(x_{t})\) is a _martingale_ w.r.t. the reverse-time process of \(x_{t}\) from \(T\) to \(0\), where the timestep \(t\) can be either continuous or discrete. Along the reverse-time sampling path, this martingale property leads to concentration bounds for scaled data scores. Moreover, a martingale satisfies the optional stopping theorem that the expected value at a stopping time is equal to its initial expected value.

Based on the martingale property of data scores, for any \(t[0,T]\) and any pretrained score model \(^{t}_{}(x_{t})\) (or with other model parametrizations), we can _calibrate_ the model by subtracting its expectation over \(q_{t}(x_{t})\), i.e., \(_{q_{t}(x_{t})}[^{t}_{}(x_{t})]\). We formally demonstrate that the calibrated score model \(^{t}_{}(x_{t})-_{q_{t}(x_{t})}[^{t}_{}(x _{t})]\) achieves lower values of SM objectives. By the connections between SM objectives and model likelihood of the SDE process [23; 45] or the ODE process , the calibrated score model has higher evidence lower bounds. Similar conclusions also hold for the conditional case, in which we calibrate a conditional score model \(^{t}_{}(x_{t},y)\) by subtracting its conditional expectation \(_{q_{t}(x_{t}|y)}[^{t}_{}(x_{t},y)]\).

In practice, \(_{q_{t}(x_{t})}[^{t}_{}(x_{t})]\) or \(_{q_{t}(x_{t}|y)}[^{t}_{}(x_{t},y)]\) can be approximated using noisy training data when the score model has been pretrained. We can also utilize an auxiliary shallow model to estimate these expectations dynamically during pretraining. When we do not have access to training data, we could calculate the expectations using data generated from \(^{t}_{}(x_{t})\) or \(^{t}_{}(x_{t},y)\). In experiments, we evaluate our calibration tricks on the CIFAR-10  and CelebA \(64 64\) datasets, reporting the FID scores . We also provide insightful visualization results on the AFHQv2 , FFHQ  and ImageNet  at \(64 64\) resolution.

## 2 Diffusion probabilistic models

In this section, we briefly review the notations and training paradigms used in diffusion probabilistic models (DPMs). While recent works develop DPMs based on general corruptions [2; 8], we mainly focus on conventional Gaussian-based DPMs.

### Forward and reverse processes

We consider a \(k\)-dimensional random variable \(x^{k}\) and define a _forward_ diffusion process on \(x\) as \(\{x_{t}\}_{t[0,T]}\) with \(T>0\), which satisfies \( t[0,T]\),

\[x_{0} q_{0}(x_{0}),\ \ \ \ \ q_{0t}(x_{t}|x_{0})=(x_{t}|_{t} x_{0},_{t}^{2}).\] (1)

Here \(q_{0}(x_{0})\) is the data distribution; \(_{t}\) and \(_{t}\) are two positive real-valued functions that are differentiable w.r.t. \(t\) with bounded derivatives. Let \(q_{t}(x_{t})= q_{0t}(x_{t}|x_{0})q_{0}(x_{0})dx_{0}\) be the marginal distribution of \(x_{t}\). The schedules of \(_{t}\), \(_{t}^{2}\) need to ensure that \(q_{T}(x_{T})(x_{T}|0,^{2})\) for some \(\). Kingma et al.  prove that there exists a stochastic differential equation (SDE) satisfying the forward transition distribution in Eq. (1), and this SDE can be written as

\[dx_{t}=f(t)x_{t}dt+g(t)d_{t},\] (2)

where \(_{t}^{k}\) is the standard Wiener process, \(f(t)=}{dt}\), and \(g(t)^{2}=^{2}}{dt}-2}{dt}_{t}^{2}\). Song et al.  demonstrate that the forward SDE in Eq. (2) corresponds to a _reverse_ SDE constructed as

\[dx_{t}=[f(t)x_{t}-g(t)^{2}_{x_{t}} q_{t}(x_{t}) ]dt+g(t)d_{t},\] (3)

where \(_{t}^{k}\) is the standard Wiener process in reverse time. Starting from \(q_{T}(x_{T})\), the marginal distribution of the reverse SDE process is also \(q_{t}(x_{t})\) for \(t[0,T]\). There also exists a deterministic process described by an ordinary differential equation (ODE) as

\[}{dt}=f(t)x_{t}-g(t)^{2}_{x_{t}} q _{t}(x_{t}),\] (4)

which starts from \(q_{T}(x_{T})\) and shares the same marginal distribution \(q_{t}(x_{t})\) as the reverse SDE in Eq. (3). Moreover, let \(q_{0t}(x_{0}|x_{t})=(x_{t}|x_{0})q_{0}(x_{0})}{q_{t}(x_{t})}\) and by Tweedie's formula , we know that \(_{t}_{q_{0t}(x_{0}|x_{t})}[x_{0}]=x_{t}+_{t}^ {2}_{x_{t}} q_{t}(x_{t})\).

### Training paradigm of DPMs

To estimate the data score \(_{x_{t}} q_{t}(x_{t})\) at timestep \(t\), a score-based model \(^{t}_{}(x_{t})\) with shared parameters \(\) is trained to minimize the score matching (SM) objective  as

\[^{t}_{}()_{q_{t}(x_{ t})}[\|^{t}_{}(x_{t})-_{x_{t}} q_{t}(x_{t}) \|_{2}^{2}].\] (5)

To eliminate the intractable computation of \(_{x_{t}} q_{t}(x_{t})\), denoising score matching (DSM)  transforms \(^{t}_{}()\) into \(^{t}_{}()_{q_{0}(x_{ 0}),q()}[\|^{t}_{}(x_{t})+}\|_{2}^{2}]\), where \(x_{t}=_{t}x_{0}+_{t}\) and \(q()=(|,)\) is a standard Gaussian distribution. Under mild boundary conditions, we know \(_{}^{t}()\) and \(_{}^{t}()\) is equivalent up to a constant, i.e., \(_{}^{t}()=_{}^{t}()+C^{t}\) and \(C^{t}\) is a constant independent of the model parameters \(\). Other SM variants [31; 44] are also applicable here. The total SM objective for training is a weighted sum of \(_{}^{t}()\) across \(t[0,T]\), defined as \(_{}(;(t))_{0}^{T}(t) _{}^{t}()dt\), where \((t)\) is a positive weighting function. Similarly, the total DSM objective is \(_{}(;(t))_{0}^{T}(t) _{}^{t}()dt\). The training objectives under other model parametrizations such as noise prediction \(_{}^{t}(x_{t})\)[17; 33], data prediction \(_{}^{t}(x_{t})\)[23; 32], and velocity prediction \(_{}^{t}(x_{t})\)[38; 18] are recapped in Appendix B.1.

### Likelihood of DPMs

Suppose that the reverse processes start from a tractable prior \(p_{T}(x_{T})=(x_{T}|0,^{2})\). We can approximate the reverse-time SDE process by substituting \(_{x_{t}} q_{t}(x_{t})\) with \(_{}^{t}(x_{t})\) in Eq. (3) as \(dx_{t}=[f(t)x_{t}-g(t)^{2}_{}^{t}(x_{t})]dt+g(t)d _{t}\), which induces the marginal distribution \(p_{t}^{}(x_{t};)\) for \(t[0,T]\). In particular, at \(t=0\), the KL divergence between \(q_{0}(x_{0})\) and \(p_{0}^{}(x_{0};)\) can be bounded by the total SM objective \(_{}(;g(t)^{2})\) with the weighing function of \(g(t)^{2}\), as stated below:

**Lemma 1**.: _(Proof in Song et al. ) Let \(q_{t}(x_{t})\) be constructed from the forward process in Eq. (2). Then under regularity conditions, we have \(_{}(q_{0}\|p_{0}^{}()) _{}(;g(t)^{2})+_{}(q_{T}\| p_{T})\)._

Here \(_{}(q_{T}\|p_{T})\) is the prior loss independent of \(\). Similarly, we approximate the reverse-time ODE process by substituting \(_{x_{t}} q_{t}(x_{t})\) with \(_{}^{t}(x_{t})\) in Eq. (4) as \(}{dt}=f(t)x_{t}-g(t)^{2}_{}^{t}(x_{t})\), which induces the marginal distribution \(p_{t}^{}(x_{t};)\) for \(t[0,T]\). By the instantaneous change of variables formula , we have \(^{}(x_{t};)}{dt}=-(_{x _{t}}(f(t)x_{t}-g(t)^{2}_{}^{t}(x_{t})))\), where \(()\) denotes the trace of a matrix. Integrating change in \( p_{t}^{}(x_{t};)\) from \(t=0\) to \(T\) can give the value of \( p_{T}(x_{T})- p_{0}^{}(x_{0};)\), but requires tracking the path from \(x_{0}\) to \(x_{T}\). On the other hand, at \(t=0\), the KL divergence between \(q_{0}(x_{0})\) and \(p_{0}^{}(x_{0};)\) can be decomposed:

**Lemma 2**.: _(Proof in Lu et al. ) Let \(q_{t}(x_{t})\) be constructed from the forward process in Eq. (2). Then under regularity conditions, we have \(_{}(q_{0}\|p_{0}^{}())=_{}(;g(t)^{2})+_{}(q_{T}\|p_{T})+ _{}()\), where the term \(_{}()\) measures the difference between \(_{}^{t}(x_{t})\) and \(_{x_{t}} p_{t}^{}(x_{t};)\)._

Directly computing \(_{}()\) is intractable due to the term \(_{x_{t}} p_{t}^{}(x_{t};)\), nevertheless, we could bound \(_{}()\) via bounding high-order SM objectives .

## 3 Calibrating pretrained DPMs

In this section we begin with deriving the relationship between data scores at different timesteps, which leads us to a straightforward method for calibrating any pretrained DPMs. We investigate further how the dataset bias of finite samples prevents empirical learning from achieving calibration.

### The stochastic process of data score

According to Kingma et al. , the form of the forward process in Eq. (1) can be generalized to any two timesteps \(0 s<t T\). Then, the transition probability from \(x_{s}\) to \(x_{t}\) is written as \(q_{st}(x_{t}|x_{s})=(x_{t}_{t|s}x_{s},_{t|s}^ {2})\), where \(_{t|s}=}{_{s}}\) and \(_{t|s}^{2}=_{t}^{2}-_{t|s}^{2}_{s}^{2}\). Here the marginal distribution satisfies \(q_{t}(x_{t})= q_{st}(x_{t}|x_{s})q_{s}(x_{s})dx_{s}\). We can generally derive the connection between data scores \(_{x_{t}} q_{t}(x_{t})\) and \(_{x_{s}} q_{s}(x_{s})\) as stated below:

**Theorem 1**.: _(Proof in Appendix A.1) Let \(q_{t}(x_{t})\) be constructed from the forward process in Eq. (2). Then under some regularity conditions, we have \( 0 s<t T\),_

\[_{t}_{x_{t}} q_{t}(x_{t})=_{q_{st}(x_{s}|x_{t})}[ _{s}_{x_{s}} q_{s}(x_{s})],\] (6)

_where \(q_{st}(x_{s}|x_{t})=(x_{t}|x_{s})q_{s}(x_{s})}{q_{t}(x_{t})}\) is the transition probability from \(x_{t}\) to \(x_{s}\)._

Theorem 1 indicates that the stochastic process of \(_{t}_{x_{t}} q_{t}(x_{t})\) is a _martingale_ w.r.t. the reverse-time process of \(x_{t}\) from timestep \(T\) to \(0\). From the optional stopping theorem , the expectedvalue of a martingale at a stopping time is equal to its initial expected value \(_{q_{0}(x_{0})}[_{x_{0}} q_{0}(x_{0})]\). It is known that, under a mild boundary condition on \(q_{0}(x_{0})\), there is \(_{q_{0}(x_{0})}[_{x_{0}} q_{0}(x_{0})]=0\) (proof is recapped in Appendix A.2). Consequently, as to the stochastic process, the martingale property results in \(_{q_{t}(x_{t})}[_{x_{t}} q_{t}(x_{t})]=0\) for \( t[0,T]\). Moreover, the martingale property of the (scaled) data score \(_{t}_{x_{t}} q_{t}(x_{t})\) leads to concentration bounds using Azuma's inequality and Doob's martingale inequality as derived in Appendix A.3. Although we do not use these concentration bounds further in this paper, there are other concurrent works that use roughly similar concentration bounds in diffusion models, such as proving consistency  or justifying trajectory retrieval .

### A simple calibration trick

Given a pretrained model \(^{t}_{}(x_{t})\) in practice, there is usually \(_{q_{t}(x_{t})}[^{t}_{}(x_{t})] 0\), despite the fact that the expect data score is zero as \(_{q_{t}(x_{t})}[_{x_{t}} q_{t}(x_{t})]=0\). This motivates us to calibrate \(^{t}_{}(x_{t})\) to \(^{t}_{}(x_{t})-_{t}\), where \(_{t}\) is a time-dependent calibration term that is independent of any particular input \(x_{t}\). The calibrated SM objective is written as follows:

\[^{t}_{}(,_{t})& _{q_{t}(x_{t})}[\|^{t}_{ }(x_{t})-_{t}-_{x_{t}} q_{t}(x_{t})\|_{2}^{2}]\\ &=^{t}_{}()-_{q_{t}(x_{t})} [^{t}_{}(x_{t})]^{}_{t}+\|_{t}\| _{2}^{2},\] (7)

where the second equation holds after the results of \(_{q_{t}(x_{t})}[_{x_{t}} q_{t}(x_{t})]=0\), and there is \(^{t}_{}(,0)=^{t}_{}()\) specifically when \(_{t}=0\). Note that the orange part in Eq. (7) is a quadratic function w.r.t. \(_{t}\). We look for the optimal \(_{t}^{*}=_{_{t}}^{t}_{}(,_{t})\) that minimizes the calibrated SM objective, from which we can derive

\[_{t}^{*}=_{q_{t}(x_{t})}[^{t}_{}(x_{t})].\] (8)

After taking \(_{t}^{*}\) into \(^{t}_{}(,_{t})\), we have

\[^{t}_{}(,_{t}^{*})=^{t}_{}( )-\|_{q_{t}(x_{t})}[^{t}_{}(x _{t})]\|_{2}^{2}.\] (9)

Since there is \(^{t}_{}()=^{t}_{}()+C^{t}\), we have \(^{t}_{}(,_{t}^{*})=^{t}_{}()-\|_{q_{t}(x_{t})}[^{t}_{ }(x_{t})]\|_{2}^{2}\) for the DSM objective. Similar calibration tricks are also valid under other model parametrizations and SM variants, as formally described in Appendix B.2.

**Remark.** For any pretrained score model \(^{t}_{}(x_{t})\), we can calibrate it into \(^{t}_{}(x_{t})-_{q_{t}(x_{t})}[^{t}_{}( x_{t})]\), which reduces the SM/DSM objectives at timestep \(t\) by \(\|_{q_{t}(x_{t})}[^{t}_{}(x_{t}) ]\|_{2}^{2}\). The expectation of the calibrated score model is always zero, i.e., \(_{q_{t}(x_{t})}[^{t}_{}(x_{t})-_{q_{t}(x _{t})}[^{t}_{}(x_{t})]]=0\) holds for any \(\), which is consistent with \(_{q_{t}(x_{t})}[_{x_{t}} q_{t}(x_{t})]=0\) satisfied by data scores.

**Calibration preserves conservativeness.** A theoretical flaw of score-based modeling is that \(^{t}_{}(x_{t})\) may not correspond to a probability distribution. To solve this issue, Salimans and Ho  develop an energy-based model design, which utilizes the power of score-based modeling and simultaneously makes sure that \(^{t}_{}(x_{t})\) is conservative, i.e., there exists a probability distribution \(p^{t}_{}(x_{t})\) such that \( x_{t}^{k}\), we have \(^{t}_{}(x_{t})=_{x_{t}} p^{t}_{}(x_{t})\). In this case, after we calibrate \(^{t}_{}(x_{t})\) by subtracting \(_{t}\), there is \(^{t}_{}(x_{t})-_{t}=_{x_{t}}(_{ }(x_{t})}{(x_{t}^{}_{t})}z_{t}())\), where \(Z_{t}()= p^{t}_{}(x_{t})(-x_{t}^{}_{t}) dx_{t}\) represents the normalization factor. Intuitively, subtracting by \(_{t}\) corresponds to a shift in the vector space, so if \(^{t}_{}(x_{t})\) is conservative, its calibrated version \(^{t}_{}(x_{t})-_{t}\) is also conservative.

**Conditional cases.** As to the conditional DPMs, we usually employ a conditional model \(^{t}_{}(x_{t},y)\), where \(y\) is the conditional context (e.g., class label or text prompt). To learn the conditional data score \(_{x_{t}} q_{t}(x_{t}|y)=_{x_{t}} q_{t}(x_{t},y)\), we minimize the SM objective defined as \(^{t}_{}()_{q_{t}(x_{t},y )}[\|^{t}_{}(x_{t},y)-_{x_{t}} q_{t}(x_{t},y)\|_{2}^ {2}]\). Similar to the conclusion of \(_{q_{t}(x_{t})}[_{x_{t}} q_{t}(x_{t})]=0\), there is \(_{q_{t}(x_{t}|y)}[_{x_{t}} q_{t}(x_{t}|y)]=0\). To calibrate \(^{t}_{}(x_{t},y)\), we use the conditional term \(_{t}(y)\) and the calibrated SM objective is formulated as

\[^{t}_{}(,_{t}(y))& _{q_{t}(x_{t},y)}[\|^{t}_{}( x_{t},y)-_{t}(y)-_{x_{t}} q_{t}(x_{t},y)\|_{2}^{2}]\\ &=^{t}_{}()-_{q_{t}(x_{t},y)} [^{t}_{}(x_{t},y)^{}_{t}(y)+\|_{t}(y)\|_{2}^ {2}],\] (10)and for any \(y\), the optimal \(_{t}^{*}(y)\) is given by \(_{t}^{*}(y)=_{q_{t}(x_{t}|y)}[_{}^{t}(x_{t},y)]\). We highlight the conditional context \(y\) in contrast to the unconditional form in Eq. (7). After taking \(_{t}^{*}(y)\) into \(_{}^{t}(,_{t}(y))\), we have \(_{}^{t}(,_{t}^{*}(y))=_{}^ {t}()-_{q_{t}(y)}[\|_{q_{t}(x_{t }|y)}[_{}^{t}(x_{t},y)]\|_{2}^{2}]\). This conditional calibration form can naturally generalize to other model parametrizations and SM variants.

### Likelihood of calibrated DPMs

Now we discuss the effects of calibration on model likelihood. Following the notations in Section 2.3, we use \(p_{0}^{}(,_{t})\) and \(p_{0}^{}(,_{t})\) to denote the distributions induced by the reverse-time SDE and ODE processes, respectively, where \(_{x_{t}} q_{t}(x_{t})\) is substituted with \(_{}^{t}(x_{t})-_{t}\).

**Likelihood of \(p_{0}^{}(,_{t})\).** Let \(_{}(,_{t};g(t)^{2})_{0}^{T}g(t)^ {2}_{}^{t}(,_{t})dt\) be the total SM objective after the score model is calibrated by \(_{t}\), then according to Lemma 1, we have \(_{}(q_{0}\|p_{0}^{}(,_{t}) )_{}(,_{t};g(t)^{2})+_{ }(q_{T}\|p_{T})\). From the result in Eq. (9), there is

\[_{}(,_{t}^{*};g(t)^{2})=_{} (;g(t)^{2})-_{0}^{T}g(t)^{2}\|_{q_{t}(x_{ t})}[_{}^{t}(x_{t})]\|_{2}^{2}dt.\] (11)

Therefore, the likelihood \(_{}(q_{0}\|p_{0}^{}(,_{t}^{*}))\) after calibration has a lower upper bound of \(_{}(,_{t}^{*};g(t)^{2})+_{ }(q_{T}\|p_{T})\), compared to the bound of \(_{}(;g(t)^{2})+_{}(q_{T}\| p_{T})\) for the original \(_{}(q_{0}\|p_{0}^{}())\). However, we need to clarify that \(_{}(q_{0}\|p_{0}^{}(,_{t}^{*}))\) may not necessarily smaller than \(_{}(q_{0}\|p_{0}^{}())\), since we can only compare their upper bounds.

**Likelihood of \(p_{0}^{}(,_{t})\).** Note that in Lemma 2, there is a term \(_{}()\), which is usually small in practice since \(_{}^{t}(x_{t})\) and \(_{x_{t}} p_{t}^{}(x_{t};)\) are close. Thus, we have

\[_{}(q_{0}\|p_{0}^{}(,_{t})) _{}(,_{t};g(t)^{2})+_{}(q_{T}\|p_{T}),\]

and \(_{}(q_{0}\|p_{0}^{}(,_{t}^{*}))\) approximately achieves its lowest value. Lu et al.  show that \(_{}(q_{0}\|p_{0}^{}())\) can be further bounded by high-order SM objectives (as detailed in Appendix A.4), which depend on \(_{x_{t}}_{}^{t}(x_{t})\) and \(_{x_{t}}(_{x_{t}}_{}^{t}(x_{t}))\). Since the calibration term \(_{t}\) is independent of \(x_{t}\), i.e., \(_{x_{t}}_{t}=0\), it does not affect the values of high-order SM objectives, and achieves a lower upper bound due to the lower value of the first-order SM objective.

Empirical learning fails to achieve \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]=0\)

A question that naturally arises is whether better architectures or learning algorithms for DPMs (e.g., EDMs ) could empirically achieve \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]=0\) without calibration? The answer may be negative, since in practice we only have access to a _finite_ dataset sampled from \(q_{0}(x_{0})\). More specifically, assuming that we have a training dataset \(\{x_{0}^{n}\}_{n=1}^{N}\) where \(x_{0}^{n} q_{0}(x_{0})\), and defining the kernel density distribution induced by \(\) as \(q_{t}(x_{t};)_{n=1}^{N}(- _{t}x_{0}^{n}}{_{t}}|,)\). When the quantity of training data approaches infinity, we have \(_{N}q_{t}(x_{t};)=q_{t}(x_{t})\) holds for \( t[0,T]\). Then the empirical DSM objective trained on \(\) is written as

\[_{}^{t}(;)_{n=1 }^{N}_{q()}[\|_{}^{t}(_{t}x_{0}^ {n}+_{t})+}\|_{2}^{2}],\] (12)

and it is easy to show that the optimal solution for minimizing \(_{}^{t}(;)\) satisfies (assuming \(_{}^{t}\) has universal model capacity) \(_{}^{t}(x_{t})=_{x_{t}} q_{t}(x_{t};)\). Given a finite dataset \(\), there is

\[_{q_{t}(x_{t};)}[_{x_{t}} q_{t}(x_{t}; )]=0,\] (13)

indicating that even if the score model is learned to be optimal, there is still \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})] 0\). Thus, the mis-calibration of DPMs is partially due to the _dataset bias_, i.e., during training we can only access a finite dataset \(\) sampled from \(q_{0}(x_{0})\).

Furthermore, when trained on a finite dataset in practice, the learned model will not converge to the optimal solution , so there is typically \(_{}^{t}(x_{t})_{x_{t}} q_{t}(x_{t};)\) and \(_{q_{t}(x_{t};)}[_{}^{t}(x_{t})] 0\). After calibration, we can at least guarantee that \(_{q_{t}(x_{t};)}[_{}^{t}(x_{t})-_{q _{t}(x_{t};)}[_{}^{t}(x_{t})]]=0\) always holds on any finite dataset \(\). In Figure 3, we demonstrate that even state-of-the-art EDMs still have non-zero and semantic \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\), which emphasises the significance of calibrating DPMs.

### Amortized computation of \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\)

By default, we are able to calculate and restore the value of \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\) for a pretrained model \(_{}^{t}(x_{t})\), where the selection of timestep \(t\) is determined by the inference algorithm, and the expectation over \(q_{t}(x_{t})\) can be approximated by Monte Carlo sampling from a noisy training set. When we do not have access to training data, we can approximate the expectation using data generated from \(p_{t}^{}(x_{t};)\) or \(p_{t}^{}(x_{t};)\). Since we only need to calculate \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\) once, the raised computational overhead is amortized as the number of generated samples increases.

**Dynamically recording.** In the preceding context, we focus primarily on post-training computing of \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\). An alternative strategy would be to dynamically record \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\) during the pretraining phase of \(_{}^{t}(x_{t})\). Specifically, we could construct an auxiliary shallow network \(h_{}(t)\) parameterized by \(\), whose input is the timestep \(t\). We define the expected mean squared error as

\[_{}^{t}()_{q_{t}(x_{t})}[ \|h_{}(t)-_{}^{t}(x_{t})^{}\|_{2}^{2}],\] (14)

where the superscript \(\) denotes the stopping gradient and \(^{*}\) is the optimal solution of minimizing \(_{}^{t}()\) w.r.t. \(\), satisfying \(h_{^{*}}(t)=_{t}^{*}=_{q_{t}(x_{t})}[_{}^{ t}(x_{t})]\) (assuming sufficient model capacity). The total training objective can therefore be expressed as \(_{}(;(t))+_{0}^{T}_{t}_{}^{t}()\), where \(_{t}\) is a time-dependent trade-off coefficient for \(t[0,T]\).

## 4 Experiments

In this section, we demonstrate that sample quality and model likelihood can be both improved by calibrating DPMs. Instead of establishing a new state-of-the-art, the purpose of our empirical studies is to testify the efficacy of our calibration technique as a simple way to repair DPMs.

### Sample quality

**Setup.** We apply post-training calibration to discrete-time models trained on CIFAR-10  and CelebA , which apply _parametrization of noise prediction_\(_{}^{t}(x_{t})\). In the sampling phase, we employ DPM-Solver , an ODE-based sampler that achieves a promising balance between sample efficiency and image quality. Because our calibration directly acts on model scores, it is also compatible with other ODE/SDE-based samplers , while we only focus on DPM-Solver cases in this paper. In accordance with the recommendation, we set the end time of DPM-Solver to \(10^{-3}\) when the number of sampling steps is less than \(15\), and to \(10^{-4}\) otherwise. Additional details can befound in Lu et al. . By default, we employ the FID score  to quantify the sample quality using 50,000 samples. Typically, a lower FID indicates a higher sample quality. In addition, in Table 3, we evaluate using other metrics such as sFID , IS , and Precision/Recall .

**Computing \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\).** To estimate the expectation over \(q_{t}(x_{t})\), we construct \(x_{t}=_{t}x_{0}+_{t}\), where \(x_{0} q_{0}(x_{0})\) is sampled from the training set and \((|,)\) is sampled from a standard Gaussian distribution. The selection of timestep \(t\) depends on the sampling schedule of DPM-Solver. The computed values of \(_{q_{}(x_{t})}[_{}^{t}(x_{t})]\) are restored in a dictionary and warped into the output layers of DPMs, allowing existing inference pipelines to be reused.

We first calibrate the model trained by Ho et al.  on the CIFAR-10 dataset and compare it to the original one for sampling with DPM-Solvers. We conduct a systematical study with varying NFE (i.e., number of function evaluations) and solver order. The results are presented in Tables 1 and 3. After calibrating the model, the sample quality is consistently enhanced, which demonstrates the significance of doing so and the efficacy of our method. We highlight the significant improvement in sample quality (4.61\(\)4.22 when using 15 NFE and \(3\)-order DPM-Solver; 3.89\(\)3.32 when using 20 NFE and \(3\)-order DPM-Solver). After model calibration, the number of steps required to achieve convergence for a \(3\)-order DPM-Solver is reduced from \(\)30 to 20, making our method a new option for expediting the sampling of DPMs. In addition, as a point of comparison, the \(3\)-order DPM-Solver with 1,000 NFE can only yield an FID score of 3.45 when using the original model, which, along with the results in Table 1, indicates that model calibration helps to improve the convergence of sampling.

Then, we conduct experiments with the discrete-time model trained on the CelebA 64x64 dataset by Song et al. . The corresponding sample quality comparison is shown in Table 2. Clearly, model calibration brings significant gains (3.91\(\)3.62 when using 15 NFE and \(3\)-order DPM-Solver; 2.84\(\)2.33 when using 20 NFE and \(3\)-order DPM-Solver) that are consistent with those on the CIFAR-10 dataset. This demonstrates the prevalence of the mis-calibration issue in existing DPMs and the efficacy of our correction. We still observe that model calibration improves convergence of sampling, and as shown in Figure 2, our calibration could help to reduce ambiguous generations. More generated images are displayed in Appendix C.

    &  &  \\  & & 10 & 15 & 20 & 25 & 30 & 35 & 40 \\  _{}^{t}(x_{t})\)} & \(1\)-order & 20.49 & 12.47 & 9.72 & 7.89 & 6.84 & 6.22 & 5.75 \\  & \(2\)-order & 7.35 & \({}^{}\)4.52 & 4.14 & \({}^{}\)3.92 & 3.74 & \({}^{}\)3.71 & 3.68 \\  & \(3\)-order & \({}^{}\)23.96 & 4.61 & \({}^{}\)3.89 & \({}^{}\)3.73 & 3.65 & \({}^{}\)3.65 & \({}^{}\)3.60 \\  _{}^{t}(x_{t})-_{q_{t}(x_{t})}[ _{}^{t}(x_{t})]\)} & \(1\)-order & 19.31 & 11.77 & 8.86 & 7.35 & 6.28 & 5.76 & 5.36 \\  & \(2\)-order & **6.76** & \({}^{}\)4.36 & 4.03 & \({}^{}\)3.66 & 3.54 & \({}^{}\)3.44 & 3.48 \\  & \(3\)-order & \({}^{}\)53.50 & **4.22** & \({}^{}\)**3.32** & \({}^{}\)**3.33** & **3.35** & \({}^{}\)**3.32** & \({}^{}\)**3.31** \\   

Table 1: Comparison on sample quality measured by FID \(\) with varying NFE on CIFAR-10. Experiments are conducted using a linear noise schedule on the discrete-time model from . We consider three variants of DPM-Solver with different orders. The results with \(\) mean the actual NFE is order \(}{}\) which is smaller than the given NFE, following the setting in .

    &  &  \\  & & 10 & 15 & 20 & 25 & 30 & 35 & 40 \\  _{}^{t}(x_{t})\)} & \(1\)-order & 16.74 & 11.85 & 7.93 & 6.67 & 5.90 & 5.38 & 5.01 \\  & \(2\)-order & **4.32** & \({}^{}\)3.98 & 2.94 & \({}^{}\)2.88 & 2.88 & \({}^{}\)2.88 & \(2.84\) \\  & \(3\)-order & \({}^{}\)11.92 & 3.91 & \({}^{}\)2.84 & \({}^{}\)2.76 & 2.82 & \({}^{}\)2.81 & \({}^{}\)2.85 \\  _{}^{t}(x_{t})-_{q_{t}(x_{t})}[ _{}^{t}(x_{t})]\)} & \(1\)-order & 16.13 & 11.29 & 7.09 & 6.06 & 5.28 & 4.87 & 4.39 \\  & \(2\)-order & 4.42 & \({}^{}\)3.94 & 2.61 & \({}^{}\)2.66 & 2.54 & \({}^{}\)2.52 & **2.49** \\  & \(3\)-order & \({}^{}\)35.47 & **3.62** & \({}^{}\)**2.33** & \({}^{}\)**2.43** & **2.40** & \({}^{}\)**2.43** & \({}^{}\)**2.49** \\   

Table 2: Comparison on sample quality measured by FID \(\) with varying NFE on CelebA 64\(\)64. Experiments are conducted using a linear noise schedule on the discrete-time model from . The settings of DPM-Solver are the same as on CIFAR-10.

### Model likelihood

As described in Section 3.3, calibration contributes to reducing the SM objective, thereby decreasing the upper bound of the KL divergence between model distribution at timestep \(t=0\) (either \(p_{0}^{}(,_{t}^{*})\) or \(p_{0}^{}(,_{t}^{*})\)) and data distribution \(q_{0}\). Consequently, it aids in raising the lower bound of model likelihood. In this subsection, we examine such effects by evaluating the aforementioned DPOs on the CIFAR-10 and CelebA datasets. We also conduct experiments with continuous-time models trained by Karras et al.  on AFHQv2 64\(\)64 , FFHQ 64\(\)64 , and ImageNet 64\(\)64  datasets considering their top performance. These models apply parametrization of data prediction \(_{}^{t}(x_{t})\), and for consistency, we convert it to align with \(_{}^{t}(x_{t})\) based on the relationship \(_{}^{t}(x_{t})=(x_{t}-_{t}_{}^{t}(x_{t}))/ _{t}\), as detailed in Kingma et al.  and Appendix B.2.

Given that we employ noise prediction models in practice, we first estimate \(\|_{q_{t}(x_{t})}[_{}^{t}(x_{t})] \|_{2}^{2}\) at timestep \(t[0,T]\), which reflects the decrement on the SM objective at \(t\) according to Eq. (9) (up to a scaling factor of \(1/_{t}^{2}\)). We approximate the expectation using Monte Carlo (MC) estimation with training data points. The results are displayed in the first row of Figure 1. Notably, the value of \(\|_{q_{t}(x_{t})}[_{}^{t}(x_{t})] \|_{2}^{2}\) varies significantly along with timestep \(t\): it decreases relative to \(t\) for CelebA but increases in all other cases (except for \(t[0.4,1.0]\) on ImageNet 64\(\)64). Ideally, there should be \(\|_{q_{t}(x_{t})}[_{x_{t}} q_{t}(x_{t}) ]\|_{2}^{2}=0\) at any \(t\). Such inconsistency reveals that mis-calibration issues exist in general, although the phenomenon may vary across datasets and training mechanisms.

Then, we quantify the gain of model calibration on increasing the lower bound of model likelihood, which is \(_{0}^{T}g(t)^{2}\|_{q_{t}(x_{t})}[_{ }^{t}(x_{t})]\|_{2}^{2}dt\) according to Eq. (11). We first rewrite it with the model parametrization of noise prediction \(_{}^{t}(x_{t})\), and it can be straightforwardly demonstrated that it equals \(_{0}^{T}}{2_{t}^{2}}\|_{q_{t}(x_{t})}[ _{}^{t}(x_{t})]\|_{2}^{2}\). Therefore, we calculate the value of \(}{2_{t}^{2}}\|_{q_{t}(x_{t})}[_{ }^{t}(x_{t})]\|_{2}^{2}\) using MC

    &  \\  &  &  &  \\  & FID & sFID & IS & Pre. & Rec. & FID & sFID & IS & Pre. & Rec. & FID & sFID & IS & Pre. & Rec. \\  \(1\)-ord. & 9.72 & 6.03 & 8.49 & 0.641 & 0.542 & 7.89 & 5.45 & 8.68 & 0.644 & 0.556 & 6.84 & 5.12 & 8.76 & 0.650 & 0.565 \\ Base \(2\)-ord. & 4.14 & 4.36 & 9.15 & 0.654 & 0.590 & 3.92 & 4.22 & 9.17 & 0.657 & 0.591 & 3.74 & 4.18 & 9.20 & 0.658 & 0.591 \\ \(3\)-ord. & 3.89 & 4.18 & 9.29 & 0.652 & 0.597 & 3.73 & 4.15 & 9.21 & 0.657 & 0.595 & 3.65 & 4.12 & 9.22 & 0.658 & 0.593 \\  \(1\)-ord. & 8.86 & 6.01 & 8.56 & 0.649 & 0.544 & 7.35 & 5.42 & 8.76 & 0.653 & 0.560 & 6.28 & 5.09 & 8.84 & 0.653 & 0.568 \\ Ours \(2\)-ord. & 4.03 & 4.31 & 9.17 & **0.661** & 0.592 & 3.66 & 4.20 & 9.20 & 0.664 & 0.594 & 3.54 & 4.14 & 9.23 & **0.662** & 0.599 \\ \(3\)-ord. & **3.32** & **4.14** & **9.38** & 0.657 & **0.603** & **3.33** & **4.11** & **9.28** & **0.665** & **0.597** & **3.35** & **4.08** & **9.27** & **0.662** & **0.600** \\   

Table 3: Comparison on sample quality measured by different metrics, including FID \(\), sFID \(\), inception score (IS) \(\), precision \(\) and recall \(\) with varying NFE on CIFAR-10. We use Base to denote the baseline \(_{}^{t}(x_{t})\) and Ours to denote calibrated score \(_{}^{t}(x_{t})-_{q_{(x_{t})}}[_{}^{t}(x _{t})]\). The sampler is DPM-Solver with different orders. Note that FID is computed by the PyTorch checkpoint of Inception-v3, while sFID/IS/Precision/Recall are computed by the Tensorflow checkpoint of Inception-v3 following _github.com/kynkaat/improved-precision-and-recall-metric_.

Figure 2: Selected images on CIFAR-10 (generated with NFE \(=20\) using 3-order DPM-Solver) demonstrating that our calibration could reduce ambiguous generations, such as generations that resemble both horse and dog. However, we must emphasize that not all generated images have a visually discernible difference before and after calibration.

estimation and report the results in the second row of Figure 1. The integral is represented by the area under the curve (i.e., the gain of model calibration on the lower bound of model likelihood). Various datasets and model architectures exhibit non-trivial gains, as observed. In addition, we notice that the DPMs trained by Karras et al.  show patterns distinct from those of DDPM  and DDIM , indicating that different DPM training mechanisms may result in different mis-calibration effects.

**Visualizing \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\).** To better understand the inductive bias learned by DPMs, we visualize the expected predicted noises \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\) for timestep from \(0\) to \(T\), as seen in Figure 3. For each dataset, the first row normalizes the values of \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\) into \(\); the second row calculates pixel-wise norm (across RGB channels) and highlights the top-\(10\%\) locations with the highest norm. As we can observe, on facial datasets like CelebA and FFHQ, there are obvious facial patterns inside \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\), while on other datasets like CIFAR-10, ImageNet, as well as the animal face dataset AFHQv2, the patterns inside \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\) are more like random noises. Besides, the facial patterns in Figure 3 are more significant when \(t\) is smaller, and become blurry when \(t\) is close to \(T\). This phenomenon may be attributed to the bias of finite training data, which is detrimental to generalization during sampling and justifies the importance of calibration as described in Section 3.4.

### Ablation studies

We conduct ablation studies focusing on the estimation methods of \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\).

**Estimating \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\) with partial training data.** In the post-training calibration setting, our primary algorithmic change is to subtract the calibration term \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\) from the pretrained DPMs' output. In the aforementioned studies, the expectation in \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\) (or its variant of other model parametrizations) is approximated with MC estimation using all training images. However, there may be situations where training data are (partially) inaccessible. To evaluate the effectiveness of our method under these cases, we examine the number of training images used to estimate the calibration term on CIFAR-10. To determine the quality of the estimated calibration term, we sample from the calibrated models using a \(3\)-order DPM-Solver running for 20 steps and evaluate the corresponding FID score. The results are listed in the left part of Table 4. As observed, we need to use the majority of training images (at least \(\) 20,000) to estimate the calibration term. We deduce that this is because the CIFAR-10 images are rich in diversity, necessitating a non-trivial number of training images to cover the various modes and produce a nearly unbiased calibration term.

**Estimating \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\) with generated data.** In the most extreme case where we do not have access to any training data (e.g., due to privacy concerns), we could still estimate the expectation over \(q_{t}(x_{t})\) with data generated from \(p_{0}^{}(x_{0};)\) or \(p_{0}^{}(x_{0};)\). Specifically, under the hypothesis that \(p_{0}^{}(x_{0};) q_{0}(x_{0})\) (DPM-Solver is an ODE-based sampler), we first generate \(_{0} p_{0}^{}(x_{0};)\)

Figure 3: Visualization of the expected predicted noises with increasing \(t\). For each dataset, the first row displays \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\) (after normalization) and the second row highlights the top-\(10\%\) pixels that \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\) has high values. The DPM on CelebA is a discrete-time model with \(1000\) timesteps  and that on FFHQ is a continuous-time one .

and construct \(_{t}=_{t}_{0}+_{t}\), where \(_{t} p_{t}^{}(x_{t};)\). Then, the expectation over \(q_{t}(x_{t})\) could be approximated by the expectation over \(p_{t}^{}(x_{t};)\).

Empirically, on the CIFAR-10 dataset, we adopt a \(3\)-order DPM-Solver to generate a set of samples from the pretrained model of Ho et al. (2017), using a relatively large number of sampling steps (e.g., 50 steps). This set of generated data is used to calculate the calibration term \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\). Then, we obtain the calibrated model \(_{}^{t}(x_{t})-_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\) and craft new images based on a \(3\)-order 20-step DPM-Solver. In the right part of Table 4, we present the results of an empirical investigation into how the number of generated images influences the quality of model calibration.

Using the same sampling setting, we also provide two reference points: 1) the originally mis-calibrated model can reach the FID score of 3.89, and 2) the model calibrated with training data can reach the FID score of 3.32. Comparing these results reveals that the DPM calibrated with a large number of high-quality generations can achieve comparable FID scores to those calibrated with training samples (see the result of using 20,000 generated images). Additionally, it appears that using more generations is not advantageous. This may be because the generations from DPMs, despite being known to cover diverse modes, still exhibit semantic redundancy and deviate slightly from the data distribution.

**Dynamical recording.** We simulate the proposed dynamical recording technique. Specifically, we use a \(3\)-layer MLP of width 512 to parameterize the aforementioned network \(h_{}(t)\) and train it with an Adam optimizer Kingma and Ba (2015) to approximate the expected predicted noises \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\), where \(_{}^{t}(x_{t})\) comes from the pretrained noise prediction model on CIFAR-10 Krizhevsky et al. (2017). The training of \(h_{}(t)\) runs for 1,000 epochs. Meanwhile, using the training data, we compute the expected predicted noises with MC estimation and treat them as the ground truth. In Figure 4, we compare them to the outputs of \(h_{}(t)\) and visualize the disparity measured by mean square error. As demonstrated, as the number of training epochs increases, the network \(h_{}(t)\) quickly converges and can form a relatively reliable approximation to the ground truth. Dynamic recording has a distinct advantage of being able to be performed during the training of DPMs to enable immediate generation. We clarify that better timestep embedding techniques and NN architectures can improve approximation quality even further.

## 5 Discussion

We propose a straightforward method for calibrating any pretrained DPM that can provably reduce the values of SM objectives and, as a result, induce higher values of lower bounds for model likelihood. We demonstrate that the mis-calibration of DPMs may be inherent due to the dataset bias and/or sub-optimally learned model scores. Our findings also provide a potentially new metric for assessing a diffusion model by its degree of "uncalibration", namely, how far the learned scores deviate from the essential properties (e.g., the expected data scores should be zero).

**Limitations.** While our calibration method provably improves the model's likelihood, it does not necessarily yield a lower FID score, as previously discussed Kingma and Ba (2015). Besides, for text-to-image generation, post-training computation of \(_{q_{t}(x_{t}|y)}[_{}^{t}(x_{t},y)]\) becomes infeasible due to the exponentially large number of conditions \(y\), necessitating dynamic recording with multimodal modules.

    &  \\ \# of samples & FID \(\) & \# of samples & FID \(\) \\ 
500 & 55.38 & 2,000 & 8.80 \\
1,000 & 18.72 & 5,000 & 4.53 \\
2,000 & 8.05 & 10,000 & 3.78 \\
5,000 & 4.31 & 20,000 & **3.31** \\
10,000 & 3.47 & 50,000 & 3.46 \\
20,000 & **3.25** & 100,000 & 3.47 \\
50,000 & 3.32 & 200,000 & 3.46 \\   

Table 4: Sample quality varies w.r.t. the number of training images (left part) and generated images (right part) used to estimate the calibration term on CIFAR-10. In the generated data case, the images used to estimate the calibration term \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\) is crafted with 50 sampling steps by a \(3\)-order DPM-Solver.

Figure 4: Dynamically recording \(_{q_{t}(x_{t})}[_{}^{t}(x_{t})]\). During training, the mean square error between the ground truth and the outputs of a shallow network for recording the calibration terms rapidly decreases, across different timesteps \(t\).