# Have it your way: Individualized Privacy Assignment

for DP-SGD

 Franziska Boenisch

boenisch@cispa.de

CISPA Helmholtz Center for Information Security

&Christopher Muhl

christopher.muehl@fu-berlin.de

Free University Berlin

Adam Dziedzic

dziedzic@cispa.de

CISPA Helmholtz Center for Information Security

&Roy Rinberg

roy.rinberg@columbia.edu

Columbia University

&Nicolas Papernot

nicolas.papernot@utoronto.ca

University of Toronto & Vector Institute

Equal contribution. Work was done at the University of Toronto and the Vector Institute.

###### Abstract

When training a machine learning model with differential privacy, one sets a privacy budget. This uniform budget represents an overall maximal privacy violation that any user is willing to face by contributing their data to the training set. We argue that this approach is limited because different users may have different privacy expectations. Thus, setting a uniform privacy budget across all points may be overly conservative for some users or, conversely, not sufficiently protective for others. In this paper, we capture these preferences through individualized privacy budgets. To demonstrate their practicality, we introduce a variant of Differentially Private Stochastic Gradient Descent (DP-SGD) which supports such individualized budgets. DP-SGD is the canonical approach to training models with differential privacy. We modify its data sampling and gradient noising mechanisms to arrive at our approach, which we call Individualized DP-SGD (IDP-SGD). Because IDP-SGD provides privacy guarantees tailored to the preferences of individual users and their data points, we empirically find it to improve privacy-utility trade-offs.

## 1 Introduction

Machine learning (ML) models are known to leak information about their training data. Such leakage can result in attacks that determine whether a specific data point was used to train a given ML model (membership inference) , infer sensitive attributes from the model's training data , or even (partially) reconstruct that training data . Therefore, the need to provide privacy guarantees to the individuals who contribute their sensitive data to train ML models is pressing.

Approaches to train ML models with guarantees of differential privacy (DP)  have established themselves as the canonical answer to this need. In the context of ML, differential privacy bounds how much can be learned about any of the individual data points from the model's training set. Take the canonical example of differentially private stochastic gradient descent (DP-SGD): it updates the model using noisy averages of clipped per-example gradients. Unlike vanilla SGD, this ensures that any single training point has a limited influence on model updates (due to the clipping to a pre-defined maximum clip norm) and that the training is likely to output a similar model should a single pointbe added to or removed from the training set (due to the noisy average). As is the case for any DP algorithm, the privacy guarantees of DP-SGD are derived analytically. They are captured by a privacy budget \(\). This budget represents the maximal privacy violation that any user contributing data to the training set is willing to tolerate. Lower budgets correspond to stronger privacy protection since they impose that the outputs of the training algorithm have to be closer for similar training sets.

In this paper, we argue that there is a key limitation when training ML models with DP guarantees: the privacy budget \(\) is set uniformly across all training points. This implicitly assumes that all users who contribute their data to the training set have the same expectations of privacy. However, that is not true; individuals have diverse values and privacy preferences . Yet, because DP-SGD assumes a uniform privacy budget, this budget must match the individual whose privacy expectations are the strongest. That is, \(\) has to correspond to the lowest privacy violation tolerance expressed amongst individuals who contributed their data. This limits the ability of DP-SGD to learn because the privacy budget \(\) together with the sensitivity of each step of the algorithm (_i.e_., the clip norm) define the scale of noise that needs to be added: the stronger the required privacy protection, the higher the noise scale. Thus, implementing privacy protection according to the lowest privacy violation tolerance in the training dataset comes at a significant cost in model accuracy.

We are the first to propose two variants of the DP-SGD algorithm, that we refer to as IDP-SGD, to enforce different privacy budgets for each training point.2 First, our modified sampling mechanism (Sample) samples data points proportionally to their privacy budgets: points with higher privacy budgets are sampled more frequently during training. It is intuitive that the more a training point is analyzed during training, the more private information can leak about this point. Second, with our individualized scaling mechanism (Scale), we rescale the noise added to gradients based on the budget specified for each training point. Naively, one would implement Scale by changing the scale of added noise per individual data point. However, for efficiency, current implementations of DP-SGD typically noise the _sum_ of the per-example clipped gradients over an entire mini-batch, hence, adding the same amount of noise to all gradients. Yet, within the context of this implementation, we note that we can _effectively_ adapt the scale of noise being added on an individual basis by adjusting the sensitivity (_i.e_. the clip norm) of each example by a multiplier. Because clipping is performed on a per-example basis, this approach enables the efficient implementation of our proposed Scale method.

To summarize our contributions:

* We introduce two novel individualized variants of the DP-SGD algorithm, where each data point can be assigned its desired privacy budget. This allows individuals who contribute data to the training set to specify different privacy expectations.
* We provide a theoretical analysis of our two mechanisms to derive how the privacy parameters (_e.g_., noise multipliers and sample rates) should be set to meet individual data points' privacy guarantees and provide a thorough privacy analysis for Sample and Scale.
* We carry out an extensive empirical evaluation on various vision and language datasets and multiple model architectures. Our results highlight the utility gain of our methods for different distributions of privacy expectations among users.

Ethical considerations.We need to ensure that our individualized privacy assignment does not harm individuals by disclosing too much of their sensitive information. We, therefore, suggest implementing our methods in a controlled environment where privacy risks are openly communicated , and where a regulatory entity is in charge of ensuring that even the individuals with lowest expectations regarding their privacy obtain a sufficient degree of protection. We further discuss the ethical deployment of our Individualized DP-SGD (IDP-SGD) in Appendix A.

## 2 Background and Related Work

### Differential Privacy and DP-SGD

Algorithm \(M\) satisfies \(,)}\)-DP, if for any two datasets \(D,D^{}\) that differ in any one record \(x\), such that \(D=D^{} x\), and any set of outputs \(R\)

\[[M(D) R] e^{}[M(D^{ }) R]+.\] (1)Since the possible difference in outputs is bounded for any pair of datasets, DP bounds privacy leakage for any individual. The value \(_{+}\) specifies the privacy level with lower values corresponding to stronger privacy guarantees. The \(\) offers a relaxation, _i.e_., a small probability of violating the guarantees. See Appendix B.1 for a more thorough introduction of \((,)\)-DP.

Another relaxation of DP is based on the Renyi divergence , namely \(_{,}\)-RDP, which for the parameters as described above and an order \((1,)\) is defined as follows: \(_{}(M(D)\ \|\ M(D^{}))\). Due to its smoother composition properties, it is employed for privacy accounting in ML with DP-SGD. Yet, it is possible to convert between the two notions: If \(M\) is an \((,)\)-RDP mechanism, it also satisfies \((+-,)\)-DP for any \(0<<1\).

**DP-SGD** extends standard SGD with two additional steps, namely clipping and noise addition. Clipping each data point's gradients to a pre-defined _clip norm_\(c\) bounds their _sensitivity_ to ensure that no data point causes too large model updates. Adding noise from a Gaussian distribution, \((0,( c)^{2})\), with zero mean and standard deviation according to the sensitivity \(c\) and a pre-defined _noise multiplier_\(\) introduces privacy. We formally depict DP-SGD as Algorithm 1. To yield tighter privacy bounds, DP-SGD implements privacy amplification by subsampling  (line 3 in Algorithm 1) where training data points are sampled into mini-batches with a Poisson sampling,3 rather than assigning each data point from the training dataset to a mini-batch before an epoch starts. DP-SGD hence implements a Sampled Gaussian Mechanism (SGM)  where each training data point is sampled independently at random without replacement with probability \(q(0,1]\) over \(I\) training iterations. As shown by Mironov _et al_. , an SGM with sensitivity of \(c=1\), satisfies \((,)\)-RDP where

\[ I 2q^{2}}.\] (2)

Equation (2) highlights that the privacy guarantee \(\) depends on the noise multiplier \(\), the sample rate \(q\), the number of training iterations \(I\), and the RDP order \(\).

### Individualized Privacy

**Individualized Privacy** is a topic of high importance given that society consists at least of three different groups of individuals, demanding either strong, average, or weak privacy protection for their data, respectively [17; 5]. Furthermore, some individuals inherently require higher privacy protection, given their personal circumstances and the degree of uniqueness of their data. However, without individualization, when applying DP to datasets that hold data from individuals with different privacy requirements, the privacy level \(\) has to be chosen according to the lowest \(\) encountered among all individuals, which favors poor privacy-utility trade-offs. Prior work on individualized privacy guarantees focused mainly on standard data analyses without considering ML [2; 19; 24; 31]. However, some of the underlying ideas inspired the design of our IDP-SGD variants.

Our **Sample** method relies on a similar idea as the _sample mechanism_ proposed by Jorgensen et al.  where data points with stronger privacy requirements are sampled with a lower probability than data points that agree to contribute more of their information. The most significant difference between the _sample mechanism_ and our approach is that the former performs sampling as a pre-processing step independent of the subsequent algorithm. In contrast, we perform subsampling _within_ every iteration of our IDP-SGD training and show how to leverage the sampling probability of DP-SGD to obtain individual privacy guarantees. Based on this difference, we could not build on how Jorgensenet al.  compute the sampling probabilities and had to propose an original mechanism to derive sampling probabilities for individualized privacy in IDP-SGD.

In a similar vein to our **Scale** method, Alaggan et al.  scale up data points individually before the noise addition to increase their sensitivity in their _stretching mechanism_. As a consequence, the signal-to-noise ratios of data points that are scaled with large factors will be higher than the ones of data points that are scaled with small factors. In contrast to Alaggan et al. , we do not scale the data points themselves, but the magnitude of noise added to their gradients, relative to their individual sensitivity. Our new interesting observation is that data points' individual clip norms can be used to indirectly scale the noise added to a mini-batch of data in DP-SGD per data point.

Closest work to ours  proposes two mechanisms for an individualized privacy assignment within the framework of Private Aggregation of Teacher Ensembles (PATE) . Their _upsampling_ duplicates training data points and assigns the duplicates to different teacher models according to the data points' privacy requirements, while their _weighting_ changes the aggregation of the teachers' predicted labels to reduce or increase the contribution of teachers according to their respective training data points' privacy requirements. PATE's approach for implementing ML with DP differs significantly from DP-SGD: PATE relies on privately training multiple non-DP models on the sensitive data and introducing DP guarantees during a knowledge transfer to a separately released model. The approaches for individualized privacy in PATE, therefore, are non-applicable to DP-SGD. See Appendix B.3 for details on the PATE algorithm and its individualized extensions.

Relationship to Individualized Privacy Accounting.An orthogonal line of research that comes closest to providing individualized guarantees for DP-SGD focuses on individualized _privacy accounting_; the idea is to learn more from points that consume less of the privacy budget over training [11; 44]. Jordon et al.  propose a personalized moments' accountant to compute the privacy loss on a per-sample basis. Yu et al.  perform individualized privacy accounting within DP-SGD based on the gradient norms of the individual data points. Feldman and Zrnic  introduce _RDP filters_ to account for privacy consumption per data point and train as long as there remain data points that do not exceed the global privacy level. Yet, it has been shown that the data points that consume little privacy budget and therefore remain in the training are the ones that already incur a small training loss . Training more on them will not significantly boost the model accuracy. While privacy assignment and accounting are concerned with improving privacy-utility trade-offs within DP, they operate under different setups. Privacy _accounting_ (in the above three methods) assumes a single fixed privacy budget assigned over the whole dataset. In contrast, our privacy _assignment_ is concerned with enabling different individuals to specify their respective privacy preferences. Since individual accounting and assignment are two independent methods, we experimentally show their synergy by applying individual assignment to individual accounting , which yields higher model utility than individual accounting alone.

## 3 Our Individualized Privacy Assignment

### Formalizing Individualizing Privacy

Setup and Notation.Given a training dataset \(D\) with points \(\{x_{1},,x_{N}\}\) which each have their own privacy preference (or _budget_), we consider data points with the same privacy budget \(_{p}\) together as a _privacy group_\(_{p}\). This notation is aligned with [17; 5] that identified different _groups_ of individual privacy preferences within society. We denote with \(_{1}\) the smallest privacy budget encountered in the dataset. Standard DP-SGD needs to set \(=_{1}\) to comply with the strongest privacy requirement encountered within the data. For all \(_{p},p[2,P]\), it follows that \(_{p}>\) and we arrange the groups such that \(_{p}>_{p-1}\). Note that following , we argue that the privacy preferences themselves should be kept private to prevent leakage of sensitive information which might be correlated with them. If there is a necessity to release individual privacy preferences, this should be done under the addition of noise to obtain DP guarantees for the privacy preferences. This can for instance be done through a smooth sensitivity analysis, as done in prior work, _e.g._. In general, we find that it is not necessary to release the privacy budgets: To implement IDP-SGD, only the party who trains the ML model on the individuals' sensitive data needs to know their privacy budgets to set the respective privacy parameters accordingly during training. Given that this party holds access to the data itself, this does not incur any additional privacy disclosure for the individuals. Other parties then only interact with the final private model (but not with the individuals' sensitive data or their associated privacy budgets) and it has been shown impractical to disclose privacy guarantees through black-box access to ML models . We discuss the confidentiality of the privacy budgets further in Appendix A.2.

Individualized Privacy.Following , our methods aim at providing data points \(x_{i}_{p}\) with individual DP (IDP) guarantees according to their privacy budget \(_{p}\) as follows: The learning algorithm \(M\) satisfies \((_{p},)\)-IDP if for all datasets \(D}}{{}}D^{}\) which differ only in \(x_{i}\) and for all outputs \(R\)

\[[M(D) R] e^{_{p}}[ M(D^{}) R]+\,.\] (3)

Without loss of generality, we assume that all individual data points have privacy preferences with the same \(\). Note that other notions of DP (_e.g._, RDP) can analogously be generalized to enable per-data point privacy guarantees. The key difference between Equation (3) and the standard definition of DP (Equation (1)) does not lie in the replacement of \(\) with \(_{p}\). Instead, the key difference is about the definition of neighboring datasets. For standard DP, neighboring datasets are defined as \(D D^{}\) where \(D\) and \(D^{}\) differ in any random data point (given that it is standard DP, every data point has privacy \(\)). In contrast, in our work, following  the neighboring datasets are defined as \(D}}{{}}D^{}\) where \(D\) and \(D^{}\) differ in any arbitrary data point \(x_{i}\) which has a privacy budget of \((_{p},)\). This yields to the individualized notion of DP.

We formalize the relationship between the individualized notion of DP (which we denote by \((\{_{1},_{2},,_{P}\},)\)-IDP) and standard \((,)\)-DP in the following two lemmas over the learning algorithm \(M\). The proofs are included in Appendix G.

**Lemma 3.1**.: _An algorithm \(M\) that satisfies \((_{1},)\)-DP also satisfies \((\{_{1},_{2},,_{P}\},)\)-IDP._

**Lemma 3.2**.: _An algorithm \(M\) that satisfies \((\{_{1},_{2},,_{P}\},)\)-IDP also satisfies \((_{P},)\)-DP._

### From Individual Privacy Preferences to Privacy Parameters

From the individual privacy preferences and the total given privacy distribution over the data, _i.e._, the number of different privacy budgets \(P\), their values \(_{p}\), and the sizes of the respective privacy groups \(|_{p}|\), we derive individual privacy parameters for IDP-SGD such that all data points within one privacy group (same privacy budget) obtain the same privacy parameters, and such that all privacy groups are expected to exhaust their privacy budget at the same time, after \(I\) training iterations.

The individualized parameters for our Sample method are the individual sample rates \(\{q_{1},,q_{P}\}\), and a respective noise multiplier \(_{}\)--common to all privacy groups--which we derive from the privacy budget distribution as described in Section 3.3. In Scale, the individualized parameters are noise multipliers \(\{_{1},,_{P}\}\) with their respective clip norms \(\{c_{1},,c_{P}\}\), as we explain in Section 3.4. Our individual bounds per privacy group are depicted in Table 1. We omit \(\) from consideration since its optimal value is selected as an optimization parameter when converting from RDP to \((,)\)-DP.

### Our Sample Method

Our Sample method relies on sampling data points with different sample rates \(\{q_{1},,q_{P}\}\) depending on their individual privacy budgets. In this case, the noise multiplier \(_{}\) is fixed. Data points with higher privacy budgets (weaker privacy requirements) are assigned higher sampling rates than those with lower privacy budgets. This modifies the Poisson sampling for DP-SGD (line 3 in Algorithm 1) to sample data points with higher privacy budgets within more training iterations.

Deriving Parameters.We aim at deriving the privacy parameters that yield the specified individual privacy preferences. The learning hyperparameters, such as mini-batch size \(B\) or learning rate \(\) can be found through hyperparameter tuning.4 For Sample, given a tuned mini-batch size \(B\), we have to find \(\{q_{1},,q_{P}\}\), such that their weighted average equals \(q\): \(_{p=1}^{P}|_{p}|q_{p}}{{=}}q=\)5.

 
**DP-SGD** & **Sample** & **Scale** \\  \( I 2q^{2}}\) & \(_{p} I 2q_{p}^{2}}  2}\) & \(_{p} I 2q^{2}}  2}\) & \(_{p} I 2q^{2}^{2}}\) \\  

Table 1: **(Individualized) Privacy Bounds.**This asserts that the Poisson sampling (line 3 in Algorithm 1) will yield mini-batches of size \(B\) in expectation. We also need to ensure that the privacy budgets of all groups will exhaust after \(I\) training iterations, which we do by deriving the adequate \(_{}\), shared across all privacy groups. Equation (2) highlights that the final privacy budget \(\) depends on both sample rate \(q\) and noise multiplier \(\). Our Sample method enables higher individualized privacy budgets for the different groups than the standard DP-SGD but still requires setting the average sampling rate equal to \(q\) as in DP-SGD to obtain an expected mini-batch size \(B\). Hence, the value of \(_{}\) has to be decreased in comparison to the initial default \(\). As an effect of this noise reduction, Sample improves utility of the final model.

Our concrete derivation of values for \(_{}\) and \(\{q_{1},,q_{P}\}\) is presented in Algorithm 2. We start from initializing \(_{}\) with \(\) from standard DP-SGD, the noise multiplier required for the privacy group \(_{1}\) (strongest privacy requirement of all groups) with the smallest privacy budget \(_{1}\). This corresponds to instantiating \(_{}\) with the upper bound noise over all privacy groups. Then, we use a _getSampleRate_ function that derives the sampling rate for the given privacy parameters based on approximating Equation (2). Finally, we iteratively decrease \(_{}\) by a scaling factor \(s_{i}\) slightly smaller than one and recompute \(\{q_{1},,q_{P}\}\) until their weighted average is (approximately) equal to \(q\). In Appendix G, we present the formal proof that Sample satisfies \((\{_{1},_{2},,_{P}\},)\)-DP. Our proof relies on Mironov et al.  and considers training of each privacy group as a separate SGM--with an individual sampling probability--that all simultaneously update the same model.

```
0: Per-group target privacy budgets \(\{_{1},,_{P}\}\), target \(\), iterations \(I\), number of total data points \(N\) and per-privacy group data points \(\{|_{1}|,,|_{P}|\}\).
1:init\(_{}\): \(_{}(_{1},,q,I)\)
2:init\(\{q_{1},,q_{P}\}\) where for \(p[P]\):
3:\(q_{p}(_{p},,_{},I)\)
4:while\(q_{p=1}^{P}|_{p}|q_{p}\):
5:\(_{} s_{i}_{}\) {scaling factor: \(s_{i}<1\)}
6:for\(p[P]\):
7:\(q_{p}(_{p},,_{},I)\)
8:Output\(_{},\{q_{1},,q_{P}\}\) ```

**Algorithm 2**Finding Sample Parameters.

### Scale

Our Scale method aims at scaling the noise added to each gradient according to the respective data point's privacy preference. Yet, for efficiency, current implementations of DP-SGD typically noise the _sum_ of the per-example clipped gradients over an entire mini-batch, hence, adding the same amount of noise to all gradients. Therefore, we instead set individualized clip norms \(\{c_{1},,c_{P}\}\) that _effectively_ adapt the scale of noise being added on an individual basis by adjusting the sensitivity (_i.e_. the clip norm) of each example by a multiplier. This changes lines 6 and 8 in the DP-SGD Algorithm (1). Data points with higher privacy budgets (weaker privacy requirements) obtain lower noise and higher clip norms. Since Equation (2) highlights that the clip norm \(c\) has no direct impact on the obtained \(\), individualized privacy in Scale results from the individual noise multipliers \(\{_{1},,_{P}\}\). Utility gains come from the overall increase in the signal-to-noise ratio during training.

Deriving Parameters.While the ultimate goal of our Scale method is to adapt individual noise multipliers per privacy group, we cannot implement this directly without degrading training performance. The reason is that whereas in DP-SGD sampling and gradient clipping are performed on a per-data point basis, noise is added per mini-batch, see lines 3, 6 and 8 in Algorithm 1, respectively. However, if we restrict mini-batches to contain only data points from the same privacy group (whichshare the same noise multiplier), we lose the gains in privacy-utility trade-offs which result from the subsampling (see Appendix F.1 for more details). Hence, while we rely on mini-batches containing data points with different privacy requirements (_i.e_., different noise multipliers) we can only specify one fixed noise multiplier \(_{}\).

To overcome this limitation, we do not set noise multipliers \(\{_{1},,_{P}\}\) directly, but indirectly obtain them through individualized clip norms \(\{c_{1},,c_{P}\}\) as follows: In standard DP-SGD, Algorithm 1, a gradient clipped to \(c\) (line 6) obtains noise with standard deviation \( c\) (line 8). For Scale, we clip gradients to \(c_{p}=s_{p}c\) with a per-privacy group scaling factor \(s_{p}\) and they obtain noise \(_{p}c_{p}\). But in practice, we add noise according to \(_{}c\) to all mini-batches. Hence, the effective scale \(_{p}\) of added noise is \(_{}c=_{}}{s_{p}} _{p}c_{p}_{p}=}_{}\). For data points with higher privacy budgets \(s_{p}>1\), so their gradients are clipped to larger norms \(c_{p}=s_{p}c\) and a smaller noise multiplier \(_{p}=}_{}\) is assigned to them. The opposite is true for data points with lower privacy budgets.

We find the values of \(_{p}\) required to obtain each privacy groups' desired \(_{p}\) using the _getNoise_ function (see Algorithm 3 line 1), which takes as inputs the privacy parameters \(_{p},,q,I\). To optimize utility, we want to set the individual clip norms such that their average over the dataset corresponds to the best clip norm \(c\) obtained through initial hyperparameter tuning: \(_{p=1}^{P}|_{p}|c_{p}c\). Given the interdependence of \(c_{p}\), \(_{p}\), and \(_{}\) (\(c_{p}=c}}{_{p}}\)), this can be achieved by setting \(_{}\) as the inverse of the weighted average over all \(1/_{p}\) as: \(_{}=(_{p=1}^{P}_{p}|}{ _{p}})^{-1}\) (please see Appendix C.2 for the derivation of \(_{}\)). Given \(_{}\), the required \(\{_{1},,_{P}\}\), and \(c\) found through hyperparamter tuning, the individual clip norms are calculated as \(c_{p}=}c}{_{p}}\). We detail the derivation of the parameters in Algorithm 3. In Appendix G, we formally show that Scale satisfies \((\{_{1},_{2},,_{P}\},)\)-DP). Similar to Sample, the proof is based on considering the training for all privacy groups as simultaneously executed SGMs with differing sensitivities.

## 4 Empirical Evaluation

For our empirical evaluation, we implement our methods in Python 3.9 and extend standard Opacus with our individualized privacy parameters and a per-privacy group accounting. We perform evaluation on the MNIST , SVHN , and CIFAR10  dataset, using the convolutional architectures from Tramer and Boneh  for most experiments, and additionally evaluate on various language datasets and diverse (larger) model architectures in Section 4.2. To evaluate the utility of our methods, we use the datasets' standard train-test splits and report test accuracies. The training and standard DP-SGD and IDP-SGD hyperparameters are specified in Table 5 and Table 6 in Appendix D, respectively where the noise multiplier \(\) is derived with the function get_noise_multiplier provided in Opacus . It takes in as arguments the specified parameters \(,q,I\), and target \(\). For experimentation on individualized privacy, we assigned privacy budgets randomly to the training data points if not indicated otherwise.

### Utility Improvement and General Applicability of Individualization

Assigning heterogeneous individual privacy budgets over the training dataset yields significant improvements in terms of the resulting model's utility, as showcased in Table 2. For example, by following the privacy budget distribution of Alaggan et al.  with privacy budgets \(_{p}=\{1,2,3\}\) for strong, medium, and weak privacy requirements, our Sample method yields accuracy improvements of 1.06%, 2.63%, and 5.09% on MNIST, SVHN, and CIFAR10, respectively. On the CIFAR10 dataset, our Scale even outperforms improvement with an accuracy increase of 5.26% in comparison to the non-individual baseline, which would have to assign \(=1\) to the whole training set in order to respect each individual's privacy preferences.

The benefits of our individualized privacy assignment also become clear in Figure 1, which depicts the test accuracy of our Sample and Scale vs. standard DP-SGD over the course of training. Both our methods continuously outperform the non-individualized DP-SGD baseline with \(=1\). Additionally, the test accuracy with privacy budget distribution according to Alaggan et al.  (\(34\%,43\%,23\%\)) is higher than the one of Niu et al.  (\(54\%,37\%,9\%\)). This can be explained by the fact that in this latter distribution, more individuals exhibit a higher pri

[MISSING_PAGE_FAIL:8]

description of the attack in Appendix B.5. We experiment with CIFAR10 and train 512 shadow models and the target model using Sample on different subsets of 25,000 training data points each. Results for Scale can be found in Appendix D.3. The privacy budgets are set to \(=10\) and \(=20\) and evenly assigned to the shadow models' training data, resulting in 12,500 training data points per privacy budget. Our target model achieves a train accuracy of 68.46% on its 25,000 member data points, and a test accuracy of 64.89% on its 25,000 non-member data points. The lower accuracy in comparison to Carlini et al. , who achieved 100% and 92% train and test accuracy respectively, results from us introducing DP to the training of the shadow models. Learning with DP is known to reduce model performance, particularly so on small datasets.

We depict the membership inference risk of the target model's training data per privacy budget (privacy group) in Figure 2. The dotted blue line represents the ROC curve for LiRA over all CIFAR10 data points and shows that overall, as expected by its definition, DP protects the training data well against membership inference attacks. However, when inspecting the ROC curve separately for the two privacy groups (\(=10\) and \(=20\)), we observe a significant difference between them. The privacy group with stronger privacy guarantees (\(=10\)) is protected better (AUC\(=0.537\)) than the group with the higher privacy budget (\(=20\), AUC\(=0.581\)). To evaluate whether the difference in the LiRA-likelihood scores between the two privacy groups is statistically significant, we perform a Student t-test on the likelihood score distributions over the data points with privacy budget \(=10\) vs. the data points with privacy budget \(=20\). We report \(=2.54\) with \(p=0.01<0.05\), and hence a statistically significant difference between the two groups exists.

Experimental results for additional target models, further comparison between IDP-SGD and DPSGD, and detailed per-target model statistics are presented in Figure 2, Figure 10 and Table 12 in Appendix D.3. The results highlight that the individual privacy assignment of our IDP-SGD has a practical impact and indeed protects data points with different levels of privacy to different degrees. Additionally, the privacy risk for data points with \(=10\) when training with IDP-SGD is lower than when training purely on them with standard DP-SGD. This privacy gain does not come at large expense of points with \(=20\) whose privacy risk remains roughly the same(see Figure 10).

## 5 Comparison to Other Methods

Comparison to Individualized PATE.We compare against Individualized PATE  (IPATE), the only other work aiming at enabling individual privacy assignment in ML. See details of their method and the general PATE framework in Appendix B.3. For the comparison, we report the student model accuracies of IPATE for the MNIST, SVHN, and CIFAR10 datasets. The results are presented in Table 13 in Appendix D.4. We observe that IDP-SGD constantly outperforms IPATE with the same privacy budget assignment.

S

S

   Architecture & Dataset & Setup & Modality, Task & DP-SGD & Sample & Scale \\  BERT & SNLI & Fine-Tune & Natural language inference & 75.91\(\) 0.23 & 76.11\(\)0.21 & **76.5\(\)0.17** \\ ResNet18 & CIFAR10 & Train & Image classification & 47.52\(\)0.84 & 48.53\(\)0.69 & **48.77\(\)0.73** \\ Embedding Model & IMDB & Train & Text classification & 72.69\(\)0.27 & 73.27\(\)0.3 & **73.34\(\)0.11** \\ Character-level RNN & Surnames & Train & Text (name) classification & 60.86\(\)0.78 & 65.56\(\)0.96 & **66.0\(\)1.19** \\   

Table 3: **Evaluating IDP-SGD with other architectures, tasks, modalities, and to fine-tuning.**Renyi-filter. This filter causes the point to be excluded from training once its individually assigned privacy budget is exhausted. Note that in contrast, in the approach by Feldman and Zrnic , all data points obtain the same privacy budget and data points are excluded from training once they reach this budget. For more details, see the description in Appendix D.5.

To assess the performance of pure individual accounting, we assign the same privacy budget of \(0.3\) to all 60,000 training data points in MNIST, following the experimental setup by Feldman and Zrnic . We then enable the individual privacy assignment and change the privacy values to \(=0.3\) for the first half of the points and \(=0.31\) for the second half of data points. We empirically observe that in this low-\(\) regime, the small change of \(\) for half the data points is enough to cause significant improvement in the accuracy of the final model, as we show in Table 4. Note that the differences in accuracy reported for standard DP-SGD on MNIST between Table 4 and Table 8 differ. This is because of the different privacy budgets (\(=0.3\) vs. \(=1.0\)). Additionally, in Figure 3, we present the number of active data points, _i.e._, data points that did not yet exhaust their individual privacy budget, over training. We find that many data points are used longer during training with individualized assignment compared to using only individualized accounting. This is because the data points with a higher privacy budget exhaust their budget later and, thus, can be used longer. We are only able to run this experiment on MNIST as done in Feldman and Zrnic  since the method is based on full-batch gradient descent: gradients are computed over the whole dataset at once, which limits its applicability to large and high-dimensional datasets.

## 6 Conclusion and Future Work

Prior work on ML with DP guarantees assigns a uniform privacy budget \(\) over the entire dataset. This approach fails to capture that different individuals have different expectations towards privacy and also decreases the model's ability to learn from the data. To overcome the limitations of a uniform privacy budget and to implement individual users' privacy preferences, we propose two modifications to the popular DP-SGD algorithm. Our Sample and Scale mechanisms adapt training to yield individual per-data point privacy guarantees and boost utility of the trained models. For future work, we believe that our individualized privacy assignment should be closely integrated with a form of practical individual privacy accounting. This could enable us to obtain a fine-grained notion of individualized privacy guarantees that tightly meet the users' expectations.