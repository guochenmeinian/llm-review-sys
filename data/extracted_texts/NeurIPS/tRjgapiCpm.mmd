# The Last Iterate Advantage:

Empirical Auditing and Principled

Heuristic Analysis of Differentially Private SGD

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We propose a simple heuristic privacy analysis of noisy clipped stochastic gradient descent (DP-SGD) in the setting where only the last iterate is released and the intermediate iterates remain hidden. Namely, our heuristic assumes a linear structure for the model.

We show experimentally that our heuristic is predictive of the outcome of privacy auditing applied to various training procedures. Thus it can be used prior to training as a rough estimate of the final privacy leakage. We also probe the limitations of our heuristic by providing some artificial counterexamples where it underestimates the privacy leakage.

The standard composition-based privacy analysis of DP-SGD effectively assumes that the adversary has access to all intermediate iterates, which is often unrealistic. However, this analysis remains the state of the art in practice. While our heuristic does not replace a rigorous privacy analysis, it illustrates the large gap between the best theoretical upper bounds and the privacy auditing lower bounds and sets a target for further work to improve the theoretical privacy analyses.

## 1 Introduction

Differential privacy (DP)  defines a measure of how much private information from the training data leaks through the output of an algorithm. The standard differentially private algorithm for deep learning is DP-SGD , which differs from ordinary stochastic gradient descent in two ways: the gradient of each example is clipped to bound its norm and then Gaussian noise is added at each iteration.

The standard privacy analysis of DP-SGD is based on composition . In particular, it applies to the setting where the privacy adversary has access to all intermediate iterates of the training procedure. In this setting, the analysis is known to be tight . However, in practice, potential adversaries rarely have access to the intermediate iterates of the training procedure, rather they only have access to the final model. Access to the final model can either be through queries to an API or via the raw model weights. The key question motivating our work is the following.

Is it possible to obtain sharper privacy guarantees for DP-SGD when the adversary

only has access to the final model, rather than all intermediate iterates?

### Background & Related Work

The question above has been studied from two angles: Theoretical upper bounds, and privacy auditing lower bounds. Our goal is to shed light on this question from a third angle via principled heuristics.

A handful of theoretical analyses [18; 19; 20; 21; 22; 17] have shown that asymptotically the privacy guarantee of the last iterate of DP-SGD can be far better than the standard composition-based analysis that applies to releasing all iterates. In particular, as the number of iterations increases, these analyses give a privacy guarantee that converges to a constant (depending on the loss function and the scale of the noise), whereas the standard composition-based analysis would give a privacy guarantee that increases forever. Unfortunately, these theoretical analyses are only applicable under strong assumptions on the loss function, such as (strong) convexity and smoothness. We lack an understanding of how well they reflect the "real" privacy leakage.

Privacy auditing [15; 16; 17; 21; 22; 23] complements theoretical analysis by giving empirical lower bounds on the privacy leakage. Privacy auditing works by performing a membership inference attack [24; 25; 26; 27]. That is, it constructs neighbouring inputs and demonstrates that the corresponding output distributions can be distinguished well enough to imply a lower bound on the differential privacy parameters. In practice, the theoretical privacy analysis may give uncomfortably large values for the privacy leakage (e.g., \(>10\)); in this case, privacy auditing may be used as evidence that the "real" privacy leakage is lower. There are settings where the theoretical analysis is matched by auditing, such as when all intermediate results are released [25; 26]. However, despite significant work on privacy auditing and membership inference [18; 27; 28; 29; 30], a large gap remains between the theoretical upper bounds and the auditing lower bounds [15; 26] when only the final parameters are released. This observed gap is the starting point for our work.

### Our Contributions

We propose a _heuristic_ privacy analysis of DP-SGD in the setting where only the final iterate is released. Our experiments demonstrate that this heuristic analysis consistently provides an upper bound on the privacy leakage measured by privacy auditing tools in realistic deep learning settings.

Our heuristic analysis corresponds to a worst-case theoretical analysis under the assumption that the loss functions are linear. This case is simple enough to allow for an exact privacy analysis whose parameters are can be computed numerically (Theorem 1). Our consideration of linear losses is built on the observation that current auditing techniques achieve the highest \(\) values when the gradients of the canaries - that is, the examples that are included or excluded to test the privacy leakage - are fixed and independent from the gradients of the other examples. This is definitely the case for linear losses; the linear assumption thus allows us to capture the setting where current attacks are most effective. Linear loss functions are also known to be the worst case for the non-subsampled (i.e., full batch) case; see Appendix B. Assuming linearity is unnatural from an optimization perspective, as there is no minimizer. But, from a privacy perspective, we show that it captures the state of the art.

We also probe the limitations of our heuristic and give some artificial counterexamples where it underestimates empirical privacy leakage. One class of counterexamples exploits the presence of a regularizer. Roughly, the regularizer partially zeros out the noise that is added for privacy. However, the regularizer also partially zeros out the signal of the canary gradient. These two effects are almost balanced, which makes the counterexample very delicate. In a second class of counterexamples, the data is carefully engineered so that the final iterate effectively encodes the entire trajectory, in which case there is no difference between releasing the last iterate and all iterates.

**Implications:** Heuristics cannot replace rigorous theoretical analyses. However, our heuristic can serve as a target for future improvements to both privacy auditing as well as theoretical analysis. For privacy auditing, matching or exceeding our heuristic is a more reachable goal than matching the theoretical upper bounds, although our experimental results show that even this would require new attacks. When theoretical analyses fail to match our heuristic, we should identify why there is a gap, which builds intuition and could point towards further improvements.

Given that privacy auditing is computationally intensive and difficult to perform correctly , we believe that our heuristic can also be valuable in practice. In particular, our heuristic can be used prior to training (e.g., during hyperparameter selection) to predict the outcome of privacy auditing when applied to the final model. (This is a similar use case to scaling laws.)

## 2 Linearized Heuristic Privacy Analysis

Theorem 1 presents our heuristic differential privacy analysis of DP-SGD (which we present in Algorithm 1 for completeness; note that we include a regularizer \(r\) whose gradient is _not_ clipped, because it does not depend on the private data \(\)). We consider Poisson subsampled minibatches and add/remove neighbours, as is standard in the differential privacy literature.

Our analysis takes the form of a conditional privacy guarantee. Namely, under the assumption that the loss and regularizer are linear, we obtain a fully rigorous differential privacy guarantee. The heuristic is to apply this guarantee to loss functions that are not linear (such as those that arise in deep learning applications). Our thesis is that, in most cases, the conclusion of the theorem is still a good approximation, even when the assumption does not hold.

Recall that a function \(:^{d}\) is linear if there exist \(^{d}\) and \(\) such that \(()=,+\) for all \(\).

**Theorem 1** (Privacy of DP-SGD for linear losses).: _Let \(,T,q,,,,r\) be as in Algorithm 1. Assume \(r\) and \((,x)\), for every \(x\), are linear._

_Letting_

\[P:=(T,q)+(0,^{2}T),Q:=(0,^ {2}T),\] (1)

_DP-SGD with \(\) satisfies \((,)\)-differential privacy with \( 0\) arbitrary and_

\[=_{T,q,}():=\{H_{e^{}}(P,Q),H_{e^ {}}(Q,P)\}.\] (2)

_Here, \(H_{e^{}}\) denotes the \(e^{}\)-hockey-stick-divergence \(H_{e^{}}(P,Q):=_{S}P(S)-e^{}Q(S)\)._

Equation 1 gives us a value of the privacy failure probability parameter \(\). But it is more natural to work with the privacy loss bound parameter \(\), which can be computed by inverting the formula:

\[_{T,q,}():=\{ 0:_{T,q,}( )\}.\] (3)

Both \(_{T,q,}()\) and \(_{T,q,}()\) can be computed using existing open-source DP accounting libraries . We also provide a self-contained & efficient method for computing them in Appendix A.

``` function DP-SGD(\(^{n}\), \(T\), \(q\), \((0,)\), \((0,)\), \(:^{d}\), \(r:^{d}\))  Initialize model \(_{0}^{d}\). for\(t=1 T\)do  Sample minibatch \(B_{t}[n]\) including each element independently with probability \(q\).  Compute gradients of the loss \(_{_{t-1}}(_{t-1},x_{i})\) for all \(i B_{t}\) and of the regularizer \(_{_{t-1}}r(_{t-1})\).  Clip loss gradients: \((_{_{t-1}}(_{t-1},x_{i}) ):=_{t-1}}(_{t-1},x_{i})}{\{ 1,\|_{_{t-1}}(_{t-1},x_{i})\|_{2}\}}\).  Sample noise \(_{t}(0,^{2}I_{d})\).  Update \[_{t}\!=\!_{t\!-\!1}\!-\!_{i B _{t}}\!(_{_{t\!-\!1}}(_{t\!- \!1},x_{i}))\\ +_{_{t\!-\!1}}r(_{t\!-\!1})\!+\!_{t}\!\!. \] endfor iflast_iterate_only then return\(_{T}\) elseifintermediate_iterates then return\(_{0},_{1},,_{T-1},_{T}\) endif endfunction ```

**Algorithm 1** Noisy Clipped Stochastic Gradient Descent (DP-SGD) 

The proof of Theorem 1 is deferred to Appendix A, but we sketch the main ideas: Under the linearity assumption, the output of DP-SGD is just a sum of the gradients and noises. We can reduce to dimension \(d=1\), since the only relevant direction is that of the gradient of the canary1 (which is constant). We can also ignore the gradients of the other examples. Thus, by rescaling, the worst case pair of output distributions can be represented as in Equation 1. Namely, \(Q=_{t=1}^{T}_{t}\) is simply the noise \(_{t}(0,^{2})\) summed over \(T\) iterations; this corresponds to the case where the canary is excluded. When the canary is included, it is sampled with probability \(q\) in each iteration and thus the total number of times it is sampled over \(T\) iterations is \((T,q)\). Thus \(P\) is the sum of the contributions of the canary and the noise. Finally the definition of differential privacy lets us compute \(\) and \(\) from this pair of distributions. Tightness follows from the fact that there exists a loss function and pair of inputs such that the corresponding outputs of DP-SGD matches the pair \(P\) and \(Q\).

### Baselines

In addition to privacy auditing, we compare our heuristic to two different baselines in Figure 1. The first is the standard, composition-based analysis. We use the open-source library from Google , which computes a tight DP guarantee for DP-SGD with intermediate_iterates. Because DP-SGD with intermediate_iterates gives the adversary more information than with last_iterate_only, this will always give at least as large an estimate for \(\) as our heuristic.

We also consider approximating DP-SGD by full batch DP-GD. That is, set \(q=1\) and rescale the learning rate \(\) and noise multiplier \(\) to keep the expected step and privacy noise variance constant:

\[(,T,q,,,,r)}_{ tq,\ T,\ Tq}\ \ (,T,1, q,/q,,r)}_{n,\ T,\ T}.\] (4)

The latter algorithm is full batch DP-GD since at each step it includes each data point in the batch with probability \(1\). Since full batch DP-GD does not rely on privacy amplification by subsampling, it is much easier to analyze its privacy guarantees. Interestingly, there is no difference between full batch DP-GD with last_iterate_only and with intermediate_iterates; see Appendix B. Full batch DP-GD generally has better privacy guarantees than the corresponding minibatch DP-SGD and so this baseline usually (but not always) gives smaller values for the privacy leakage \(\) than our heuristic. In practice, full batch DP-GD is too computationally expensive to run. But we can use it as an idealized comparison point for the privacy analysis.

Figure 1: Comparison of our heuristic to baselines in various parameter regimes. Horizontal axis is number of iterations \(T\) and vertical axis is \(\) such that we have \((,10^{-6})\)-DP.

Figure 2: Black-box gradient space attacks fail to achieve tight auditing when other data points are sampled from the data distribution. Heuristic and standard bounds diverge from empirical results, indicating the attack’s ineffectiveness. This contrasts with previous work which tightly auditing with access to intermediate updates.

## 3 Empirical Evaluation via Privacy Auditing

**Setup:** We follow the construction of Nasr, Song, Thakurta, Papernot, and Carlini  where we have 3 entities, adversarial crafted, model trainer, and distinguisher. In this paper, we assume the distinguisher only has access the final iteration of the model parameters. We use the CIFAR10 dataset  with a WideResNet model  unless otherwise specified; in particular, we follow the training setup of De, Berrada, Hayes, Smith, and Balle , where we train and audit a model with \(79\%\) test accuracy and, using the standard analysis, \((=8,=10^{-5})\)-DP. For each experiment we trained 512 CIFAR10 models with and without the canary (1024 total). To compute the empirical lower bounds we use the PLD approach with Clopper-Pearson confidence intervals used by Nasr, Hayes, Steinke, Balle, Tramer, Jagielski, Carlini, and Terzis . Here we assume the adversary knows the sampling rate and the number of iterations and is only estimating the noise multiplier used in DP-SGD, from which the reported privacy parameters (\(\) and \(\)) are derived.

Figure 4: Input space attacks show promising results with both natural and blank image settings, although blank images have higher attack success. These input space attacks achieve tighter results than gradient space attacks in the natural data setting, in contrast to findings from prior work.

Figure 3: For gradient space attacks with adversarial datasets, the empirical epsilon (\(\)) closely tracks the final epsilon except for at small step counts, where distinguishing is more challenging. This is evident at both subsampling probability values we study (\(q=0.01\) and \(q=0.1\)).

### Experimental Results

We implement state-of-the-art attacks from prior work [21; 22]. These attacks heavily rely on the intermediate steps and, as a result, do not achieve tight results. In the next section, we design specific attacks for our heuristic privacy analysis approach to further understand its limitations and potential vulnerabilities. We used Google Cloud A2-megagpu-16g machines with 16 Nvidia A100 40GB GPUs. Overall, we use roughly 33,000 GPU hours for our experiments.

**Gradient Space Attack:** The most powerful attacks in prior work are gradient space attacks where the adversary injects a malicious gradient directly into the training process, rather than an example; prior work has shown that this attack can produce tight lower bounds, independent of the dataset and model used for training . However, these previous attacks require access to all intermediate training steps to achieve tight results. Here, we use canary gradients in two settings: one where the other data points are non-adversarial and sampled from the real training data, and another where the other data points are designed to have very small gradients (\( 0\)). This last setting was shown by  to result in tighter auditing. In all attacks, we assume the distinguisher has access to all adversarial gradient vectors. For malicious gradients, we use Dirac gradient canaries, where gradient vectors consist of zeros in all but a single index. In both cases, the distinguishing test measures the dot product of the final model checkpoint and the gradient canary.

Figure 2 summarizes the results for the non-adversarial data setting, with other examples sampled from the true training data. In this experiment, we fix noise magnitude and subsampling probability, and run for various numbers of training steps. While prior work has shown tight auditing in this setting, we find an adversary without access to intermediate updates obtains much weaker attacks. Indeed, auditing with this strong attack results even in much lower values than the heuristic outputs.

Our other setting assumes the other data points are maliciously chosen. We construct an adversarial "dataset" of \(m+1\) gradients, \(m\) of which are zero, and one gradient is constant (with norm equal to the clipping norm), applying gradients directly rather than using any examples. As this experiment does not require computing gradients, it is very cheap to run more trials, so we run this procedure \(N=100,000\) times with the gradient canary, and \(N\) times without it, and compute an empirical estimate for \(\) with these values. We plot the results of this experiment in Figure 3 together with the \(\) output by the theoretical analysis and the heuristic, fixing the subsampling probability and varying the number of update steps. We adjust the noise parameter to ensure the standard theoretical analysis produces a fixed \(\) bound. The empirical measured \(\) is close to the heuristic \(\) except for when training with very small step counts: we expect this looseness to be the result of statistical effects, as lower step counts have higher relative variance at a fixed number of trials.

**Input Space Attack:** In practice, adversaries typically cannot insert malicious gradients freely in training steps. Therefore, we also study cases where the adversary is limited to inserting malicious inputs into the training set. Label flip attacks are one of the most successful approaches used to audit DP machine learning models in prior work [22; 23]. For input space attacks, we use the loss of the malicious input as a distinguisher. Similar to our gradient space attacks, we consider two settings for input space attacks: one where other data points are correctly sampled from the dataset, and another where the other data points are blank images.

Figure 4 summarizes the results for this setting. Comparing to Figure 2, input space attacks achieve tighter results than gradient space attacks. This finding is in stark contrast to prior work. The reason is that input space attacks do not rely on intermediate iterates, so they transfer well to our setting.

In all the cases discussed so far, the empirical results for both gradient and input attacks fall below the heuristic analysis and do not violate the upper bounds based on the underlying assumptions. This suggests that the heuristic might serve as a good indicator for assessing potential vulnerabilities. However, in the next section, we delve into specific attack scenarios that exploit the assumptions used in the heuristic analysis to create edge cases where the heuristic bounds are indeed violated.

## 4 Counterexamples

We now test the limits of our heuristic by constructing some artificial counterexamples. That is, we construct inputs to DP-SGD with last_iterate_only such that the true privacy loss exceeds the bound given by our heuristic. While we do not expect the contrived structures of these examples tomanifest in realistic learning settings, they highlight the difficulties of formalizing settings where the heuristic gives a provable upper bound on the privacy loss.

### Warmup: Zeroing Out The Model Weights

We begin by noting the counterintuitive fact that our heuristic \(_{T,q,}()\) is _not_ always monotone in the number of steps \(T\) when the other parameters \(,q,\) are kept constant. This is shown in Figure 0(c). More steps means there is both more noise and more signal from the gradients; these effects partially cancel out, but the net effect can be non-monotone.

We can use a regularizer \(r()=\|\|_{2}^{2}/2\) so that \(_{}r()=\). This regularizer zeros out the model from the previous step, i.e., the update of DP-SGD becomes

\[_{t} =_{t-1}-(_{i B_{t}} (_{_{t-1}}(_{t-1},x_{i}))+_{ _{t-1}}r(_{t-1})+_{t})\] (5) \[=_{i B_{t}}(_{_ {t-1}}(_{t-1},x_{i}))+_{t}.\] (6)

This means that the last iterate \(_{T}\) is effectively the result of only a single iteration of DP-SGD. In particular, it will have a privacy guarantee corresponding to one iteration. Combining this regularizer with a linear loss and a setting of the parameters \(T,q,,\) such that the privacy loss is non-monotone - i.e., \(_{T,q,}()<_{1,q,}()\) - yields a counterexample.

In light of this counterexample, in the next subsection, we benchmark our counterexample against sweeping over smaller values of \(T\). I.e., we consider \(_{t T}_{t,q,}()\) instead of simply \(_{T,q,}()\).

### Linear Loss \(+\) Quadratic Regularizer

Consider running DP-SGD in one dimension (i.e., \(d=1\)) with a linear loss \((,x)=x\) for the canary and a quadratic regularizer \(r()=^{2}\), where \(\) and \(x[-1,1]\) and we use learning rate \(=1\). With sampling probability \(q\), after \(T\) iterations the privacy guarantee is equivalent to distinguishing \(Q:=(0,^{2})\) and \(:=(_{i[T]}(1-)^{i-1}(q), ^{2})\), where \(^{2}:=^{2}_{i[T]}(1-)^{2(i-1)}\). When \(=0\), this retrieves linear losses. When \(=1\), this corresponds to distinguishing \((0,^{2})\) and \(((q),^{2})\) or, equivalently, to distinguishing linear losses after \(T=1\) iteration. If we maximize our heuristic over the number of iterations \( T\), then our heuristic is tight for the extremes \(\{0,1\}\).

A natural question is whether the worst-case privacy guarantee on this quadratic is always given by \(\{0,1\}\). Perhaps surprisingly, the answer is no: we found that for \(T=3,q=0.1,=1,=0\), DP-SGD is \((2.222,10^{-6})\)-DP. For \(=1\) instead DP-SGD is \((2.182,10^{-6})\)-DP. However, for \(=0.5\) instead the quadratic loss does not satisfy \((,10^{-6})\)-DP for \(<2.274\).

However, this violation is small, which suggests our heuristic is still a reasonable for this class of examples. To validate this, we consider a set of values for the tuple \((T,q,)\). For each setting of \(T,q,\), we compute \(_{t T}_{t,q,}()\) at \(=10^{-6}\). We then compute \(\) for the linear loss with quadratic regularizer example with \(=1/2\) in the same setting. Since the support of the random variable \(_{i[T]}(1-)^{i-1}(q)\) has size \(2^{T}\) for \(=1/2\), computing exact \(\) for even moderate \(T\) is computationally intensive. Instead, let \(X\) be the random variable equal to \(_{i[T]}(1-)^{i-1}(q)\), except we round up values in the support which are less than \(.0005\) up to \(.0005\), and then round each value in the support up to the nearest integer power of \(1.05\). We then compute an exact \(\) for distingushing \((0,^{2})\) vs \((X,^{2})\). By Lemma 4.5 of Choquette-Choo, Ganesh, Steinke, and Thakurta , we know that distinguishing \((0,^{2})\) vs. \((_{i[T]}(1-)^{i-1}(q),^{2})\) is no harder than distingushing \((0,^{2})\) vs \((X,^{2})\), and since we increase the values in the support by no more than 1.05 multiplicatively, we expect that our rounding does not increase \(\) by more than 1.05 multiplicatively.

In Figure 5, we plot the ratio of \(\) at \(=10^{-6}\) for distinguishing between \((0,^{2})\) and \((X,^{2})\) divided by the maximum over \(i[T]\) of \(\) at \(=10^{-6}\) for distinguishing between \((0,i^{2})\) and \(((i,q),i^{2})\). We sweep over \(T\) and \(q\), and for each \(q\) In Figure 4(a) (resp. Figure 4(b)) we set \(\) such that distinguishing \((0,^{2})\) from \(((q),^{2})\) satisfies \((1,10^{-6})\)-DP(resp. \((2,10^{-6})\)-DP). In the majority of settings, the linear loss heuristic provides a larger \(\) than the quadratic with \(=1/2\), and even when the quadratic provides a larger \(\), the violation is small (\( 3\%\)). This is evidence that our heuristic is still a good approximation for many convex losses.

### Pathological Example

If we allow the regularizer \(r\) to be arbitrary - in particular, not even requiring continuity - then the gradient can also be arbitrary. This flexibility allows us to construct a counterexample such that the standard composition-based analysis of DP-SGD with intermediate_iterates is close to tight.

Specifically, choose the regularizer so that the update \(^{}=-_{^{}}\) does the following: \(^{}_{1}=0\) and, for \(i[d-1]\), \(^{}_{i+1}=v_{i}\). Here \(v>1\) is a large constant. We chose the loss so that, for our canary \(x_{1}\), we have \(_{}(,x_{1})=(1,0,0,,0)\) and, for all other examples \(x_{i}\) (\(i\{2,3,,n\}\)), we have \(_{}(,x_{i})=\). Then the last iterate is

\[_{T}=(A_{T}+_{T,1},vA_{T-1}+v_{T-1,1}+_{T,2},v^{2}A_{T-2}+ v^{2}_{T-2,1}+v_{T-1,2}+_{T,3},),\] (7)

where \(A_{t}(p)\) indicates whether or not the canary was sampled in the \(t\)-th iteration and \(_{t,i}\) denotes the \(i\)-th coordinate of the noise \(_{t}\) added in the \(t\)-th step. Essentially, the last iterate \(_{T}\) contains the history of all the iterates in its coordinates. Namely, the \(i\)-th coordinate of \(_{T}\) gives a scaled noisy approximation to \(A_{T-i}\):

\[v^{1-i}_{T,i}=A_{T-i}+_{j=0}^{i-1}v^{j+1-i}_{T-j,i-j} (A_{T-i},^{2}}{1-v^{-2}}).\] (8)

As \(v\), the variance converges to \(^{2}\). In other words, if \(v\) is large, from the final iterate, we can obtain \((A_{i},^{2})\) for all \(i\). This makes the standard composition-based analysis of DP-SGD tight.

### Malicious Dataset Attack

The examples above rely on the regularizer having large unclipped gradients. We now construct a counterexample without a regularizer, instead using other examples to amplify the canary signal.

Our heuristic assumes the adversary does not have access to the intermediate iterations and that the model is linear. However, we can design a nonlinear model and specific training data to directly challenge this assumption. The attack strategy is to use the model's parameters as a sort of noisy storage, saving all iterations within them. Then with access only to the final model, an adversary

Figure 5: Ratio of upper bound on \(\) for quadratic loss with \(=0.5\) divided by maximum \(\) of \(i\) iterations on a linear loss. In Figure 4(a) (resp. Figure 4(b)), for each choice of \(q\), \(\) is set so 1 iteration of DP-SGD satisfies \((1,10^{-6})\)-DP (resp \((2,10^{-6})\)-DP).

can still examine the parameters, extract the intermediate steps, and break the assumption. Our construction introduces a data point that changes its gradient based on the number of past iterations, making it easy to identify if the point was present a given iteration of training. The rest of the data points are maliciously selected to ensure the noise added during training doesn't impact the information stored in the model's parameters. We defer the full details of the attack to Appendix C.

Figure 6 summarizes the results. As illustrated in the figure, this attack achieves a auditing lower bound matching the standard DP-SGD analysis even in the last_iterate_only setting. As a result, the attack exceeds our heuristic. However, this is a highly artificial example and it is unlikely to reflect real-world scenarios.

## 5 Discussion & Conclusion

Both theoretical analysis and privacy auditing are valuable for understanding privacy leakage in machine learning, but each has limitations. Theoretical analysis is inherently conservative, while auditing procedures evaluate only specific attacks, and may thus underrepresent the privacy leakage.

Our work introduces a novel heuristic analysis for DP-SGD that focuses on the privacy implications of releasing only the final model iterate. This approach is based in the empirical observation that linear loss functions accurately model the effectiveness of state of the art membership inference attacks. Our heuristic offers a practical and computationally efficient way to estimate privacy leakage to complement privacy auditing and the standard composition-based analysis of DP-SGD. As shown in Table 1, we trained a series of CIFAR10 models with varying batch sizes that all achieved the similar level of heuristic epsilon, albeit with different standard epsilon values. Remarkably, these models exhibited similar performance and similar empirical epsilon values.

We also acknowledge the limitations of our heuristic by identifying specific counterexamples where the heuristic underestimates the true privacy leakage.

   Batch size & Heuristic \(\) & Standard \(\) & Accuracy & Empirical \(\) \\ 
4096 & 6.34 & 8 & 79.5\(\%\) & 1.7 \\
512 & 7.0 & 12 & 79.1\(\%\) & 1.8 \\
256 & 6.7 & 14 & 79.4\(\%\) & 1.6 \\   

Table 1: Previous works showed that large batch sizes achieve high performing models . Using our heuristic analysis it is possible to achieve similar performance for smaller batch sizes.

Figure 6: In this adversarial example, the attack encodes all training steps within the final model parameters, thereby violating the specific assumptions used to justify our heuristic analysis.