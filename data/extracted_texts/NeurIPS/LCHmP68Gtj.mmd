# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

machines position themselves in the real world through computer vision, which remains dominated by structure-based approaches  relying on basic hand-crafted  or learned  primitives, such as points or lines. These approaches build 3D maps with Structure-from-Motion (SfM) and then localize query images via 2D-3D registration. Their complexity and many components (feature extraction and matching, bundle adjustment, pose refinement, etc.) make it difficult to tune  or update  them, and to learn high-level priors end-to-end . They are also costly to store and generally not reusable for other applications.

Recent works such as OrienterNet  instead learn planar, neural representations from the same 2D semantic maps that humans use. These maps encode scene geometry and semantics and can be used for visual positioning with sub-meter accuracy. This approach is however limited to a few semantic classes, and the maps it is based on can be inaccurate, costly to obtain, and difficult to maintain.

We argue that maps are most useful for figuring out where we are when they are _abstract_ enough to be robust to temporal changes, yet preserve enough _geometric and semantic information_ to yield high-quality correspondences with the physical world. Our work, SNAP, shows that, by learning 2D neural maps for localization, meaningful semantics emerge without explicitly supervising them. These semantics improve positioning accuracy and also make our maps usable for other tasks (Fig. 1).

SNAP leverages the complementary strengths of different input modalities, like ground-level and overhead imagery, by fusing them into a single 2D neural map (Fig. 2). It can flexibly and efficiently integrate arbitrary combinations of data captured at different points in time, which is key to continuously update maps in a changing world. We train it end-to-end to estimate the pose of a query image relative to the mapping images, by simply aligning their neural maps. This kind of contrastive learning requires only sensor poses, which can be easily obtained with photogrammetry . We train and evaluate SNAP on a dataset with 50M StreetView images1 from 5 continents, _orders-of-magnitude_ larger and more diverse than comparable academic benchmarks.

Despite training only for a positioning objective, we observe that our neural maps learn easily-interpretable, high-level semantics without the need for explicit semantic cues (Fig. 3), and demonstrate that they provide an effective pretraining for semantic understanding tasks by fine-tuning them on little labeled data. This can potentially unlock cost-efficient creation of more detailed and richer maps, readable by humans and machines alike, while providing state-of-the-art visual positioning.

Our main contributions are as follows. (i) We introduce a simple and lightweight encoder to estimate bird's-eye view maps from ground-level imagery, combining principles from multi-view geometry with strong monocular cues. (ii) We fuse different imaging modalities to integrate and benefit from complementary cues. (iii) We show how to train our model by aligning neural maps in a contrastive learning framework, using RANSAC to mine hard negatives. (iv) We outperform the state of the art on visual positioning and register image queries beyond the reach of traditional methods (Fig. 3). (v) We demonstrate that high-level semantics emerge by learning to align neural maps, without any explicit supervision, and fine-tune them on semantic understanding with few labels (Fig. 6).

Figure 2: **Training architecture.** We feed overhead and ground-level imagery to per-stream encoders (Sec. 2.1) to produce 2D bird’s-eye-view neural maps, fused via cell-wise max-pooling (Sec. 2.2). We also extract a ‘query’ neural map from a single ground-level image with the same ground-level encoder. Given known poses, we train SNAP by simply registering ‘query’ and ‘scene’ maps (Sec. 3).

Figure 3: **Single-image localization.** We show the 3-DoF poses of map images (**red**), the GT query pose (**black**), and the pose predicted by SNAP (**green**) with its error (\( t, R\)). SNAP can estimate accurate poses even for extreme opposite-views. We visualize neural maps by projecting them to RGB using PCA. Notice how objects like trees, poles, curbs or road markings are clearly recognizable.

Mapping the world with neural maps

We now formalize neural maps, and describe a neural network architecture to infer them from raw sensor data. Our goal is to infer a more generic neural representation that can encode both the geometry, semantics, and appearance of a given point in the 2D world.

**Problem formulation:** For a 3D scene, such as a large outdoor environment, we consider a local, 3D Cartesian coordinate system such that the \(z\) axis points upwards along the gravity direction. A neural map \(\) is defined over a regular grid that partitions the \(xy\) plane into \(I J\) square cells of size \(\). Each cell \((i,j)\) is associated with a \(D\)-dimensional feature \(_{ij}^{D}\). To infer such neural map, we leverage large quantities of raw imagery captured by diverse cameras.

**Input modalities:** Ground-level images are captured by cameras mounted on StreetView cars or backpacks . They are often part of a sequence of multi-camera frames. As such, they are very unevenly distributed throughout space. Each image offers a high resolution view of a small area, mainly limited by the occlusion of static or dynamic objects like buildings or vehicles. On the other hand, overhead images are captured by cameras mounted on planes or satellites. These images benefit from high spatial coverage at a uniform but low resolution. Their visibility is mostly affected by vertical occluders like trees. Ground-level and overhead images capture different aspects of the environment and are thus complementary.

**Assumptions:** All images of either modality are calibrated and registered with respect to the map coordinate system. Each image \(n\) follows a projection function \(_{n}:^{3}^{2}\) that maps a 3D point in the world to a 2D point on the image plane. \(_{n}\) combines the camera pose \({}_{w}_{n}(3)\) and the camera calibration, including lens distortions. Overhead images are ortho-rectified, such that world points along the \(z\) axis project onto the same pixel coordinate. As this process relies on a coarse digital surface model , fine details like poles are not rectified and may result in artifacts, which SNAP can however learn to account for.

### Fusing multi-modal representations

Each location in the world is observed by an arbitrary number of images for each modality, captured at arbitrary points in time. We thus follow a late-fusion strategy that first encodes each modality separately and only finally fuses them (Fig. 2). This can flexibly adapt to the available inputs and efficiently handle arbitrary spatial distributions of data.

**Encoding:** We design two encoders that each combine a subset of observations \(n\) into a single-modality neural map \(^{n}\) defined over the same grid as \(\). \(_{}\) encodes a single tile of overhead orthoimagery, while \(_{}\) encodes a single image or multiple covisible ground-level StreetView images, _e.g._, a multi-view sequence. To best resolve the 3D information from perspective shots at arbitrary viewpoints, \(_{}\) leverages both multi-view observations and monocular cues. We describe its architecture in detail in Sec. 2.2. \(_{}\), on the other hand, is a simple U-Net-style CNN  that computes a feature for each pixel of the overhead orthoimage, which is then resampled into the grid.

**Fusion:** We obtain the final neural map by fusing the set of encoded maps \(\{^{n}\}\) using a cell-wise max-pooling operation, _i.e._, \(_{ij}=_{n}_{ij}^{n}\ \ (i,j) I J\). This can combine maps with different spatial extents, which is essential to scale to large areas. The _max_ aggregation picks the best estimate among all inputs for each feature channel and thus handles partial observations, such as when the road surface cannot be resolved in overhead images because it is occluded by trees.

### Ground-level image encoder

We design a single module, \(_{}\), that can arbitrarily encode one or multiple images, ordered or not. \(_{}\) first fuses the image data into 3D space and later projects it vertically into the map plane (Fig. 4). This design can handle arbitrary ground geometries and accurately resolve the 2D location of overhanging 3D structures, like street lights. The 3D fusion leverages both multi-view geometry and strong monocular cues learned end-to-end. \(_{}\) can thus resolve objects that are observed by a single image, while maximizing accuracy when multiple observations are available.

**Monocular inference:** We consider an unordered set of \(N\) images \(\{^{n}\}\), \(N{}1\). Each image \(n\) is encoded independently by a CNN \(_{}\) into a \(C\)-dimensional feature image \(^{n}^{H W C}\). \(_{}\) also estimates a pixel-wise depth \(^{n}^{H W D}\) as a score over \(D\) depth planes along the ray of each pixel. \(^{n}\) is similar to a frustum-aligned occupancy volume [78; 67] but contains unnormalized logits of a depth distribution. Instead of regressing a single value, this encodes the full depth uncertainty along the ray and thus allows \(_{}\) to provide meaningful multi-modal estimates. We distribute the depth planes uniformly in log space to correlate with the uncertainty of monocular depth estimation [3; 78].

**Multi-view fusion:** To fuse information in 3D, we define \(K\) horizontal planes at heights \(\{z_{k}\}\), which are uniformly distributed within a range of interest defined with respect to the height of the camera [85; 50], _e.g._, from 4 m below to 8 m above. For a 2D map cell \((i,j) I J\), we consider its center point \((x,y)\) and a column of 3D points \(\{_{k}=(x,y,z_{k})\}\). For each 3D point \(k\), we define the subset of views that best observe it as \(_{k}\{1 N\}\), _e.g._, those that are closest spatially. We project the point to each of these views, obtain a 2D observation \(_{k}^{n}=_{n}(_{k})\), and sample the corresponding feature image with bi-linear interpolation: \(_{k}^{n}=^{n}[_{k}^{n}]\). Given the depth \(d_{k}^{n}\) of \(_{k}\) in the corresponding view, we also tri-linearily interpolate a score from the depth prior: \(_{k}^{n}=^{n}[_{k}^{n},d_{k}^{n}]\). Intuitively, \(_{k}^{n}\) is low if the 3D point is in free space or is occluded in view \(n\). Following common practice in learned multi-view stereo [106; 10], we then compute feature consistency statistics, as mean and variance \((_{k},_{k})^{C}\), weighted by the depth priors:

\[_{k}=_{n_{k}}w_{k}^{n}\;_{k}^{n} _{k}=_{n_{k}}w_{k}^{ n}\;(_{k}^{n}-_{k})^{2} w _{k}^{n}=*{softmax}_{n_{k}}_{k}^{n}.\] (1)

A Multi-Layer Perceptron (MLP) fuses this information into a feature \(_{k}\), which is finally pooled across all points in the column, resulting in a neural map cell \(_{ij}\):

\[_{ij}=_{k}_{k}_{k}= ([_{k},\;_{k},\;_{ n_{k}}_{k}^{n}]).\] (2)

Adding the maximum depth score differentiates free and occupied space when the point is observed by a single image. This makes it possible to use the same model for single images and sequences.

By tightly combining 3D geometry and representation learning, our approach leverages both monocular priors and multi-view information, while past research on 2D mapping or 3D reconstruction typically relies on only one of the two. Compared to expensive Transformers  or 3D CNNs , we show that a simpler, lightweight MLP is effective at fusing multi-view information, inspired by . Compared to top-down 2D CNNs that squash the vertical dimension [34; 70], this MLP is more expressive and makes our neural maps equivariant to 2D translations and rotations and invariant to translations along the vertical axis. Overall, this simple design enables scaling to very large scenes, which is critical to provide hard negatives for contrastive learning and ultimately learn rich semantics.

## 3 Learning from pose supervision

**Alignment as contrastive learning:** We want neural maps to encode high-level semantic information about the environment. Given recent advances in self-supervised learning [14; 64], we hypothesize that this can emerge from learning distinctive features that distinguish one location from another and that are invariant to viewpoint and temporal appearance changes. Intuitively, _good maps help us identify where we are_. More generally, good maps are such that we can unambiguously align them

Figure 4: **Ground-level encoder: combining multi-view geometry and monocular priors. We use a CNN to predict pixel-wise features and a monocular occupancy volume, separately for each view. We then interpolate them over a column of 3D points (at predefined heights), for each 2D cell. Finally, a simple MLP combines them into features \(_{k}\) that are pooled along the column, into a neural cell.**

when inferred from partial inputs. Consider neural maps \(^{Q}\) and \(^{R}\) obtained from two disjoint subsets of inputs, the query \(Q\) and the reference \(R\). In camera pose estimation, \(Q\) corresponds to a single ground-level image and \(R\) to a sequence of images with an aerial tile. Because our encoder is flexible, we can use the same shared model to encode \(Q\) and \(R\) (Fig. 2). \(^{Q}\) is defined over a grid \(^{Q}^{I J 2}\) in a local coordinate frame, _e.g._, aligned with the query camera, where \(^{Q}_{ij}\) is the center point of cell \((i,j)\), while \(^{R}\) is defined in the world frame.

We define a score function \(E(;^{Q},^{R}):(2)\) that evaluates the consistency between \(^{Q}\) and \(^{R}\) given an estimate of their 3-DoF relative pose \({}_{R}_{Q}(2)\). To distinguish the ground-truth pose \({}_{R}_{Q}^{*}\) from \(K\) other, incorrect poses \(\{{}_{R}_{Q}^{k}\}\), we want to increase \(E({}_{R}_{Q}^{*})\) and decrease \(E({}_{R}_{Q}^{k})\) (omitting \(^{Q}\) and \(^{R}\) for brevity). This corresponds to a contrastive learning problem, for which we minimize the InfoNCE loss 

\[(^{Q},^{R})=-_{R}_{Q}^{*})/)}{_{k\{*,1 K\}} (E({}_{R}_{Q}^{k})/)},\] (3)

where \(\) is a learnable temperature parameter. Neural maps are trained end-to-end and require only relative poses \({}_{R}_{Q}^{*}\), which can be easily obtained at a large scale using photogrammetry [45; 37].

**Featuremetric pose scoring:** A linear layer projects each neural map \(\) to a lower-dimensional, L2-normalized map \(}\). This creates an information bottleneck that encourages compact features. The score \(E\) evaluates the consistency of two neural maps as the similarity of each cell after warping:

\[E({}_{R}_{Q})=_{(i,j) I J}(}_{ij}^{Q}}^{R}[{}_{R}_{Q} _{ij}^{Q}],0),\] (4)

where \({}_{R}_{Q}\) transforms a grid point from coordinate frames \(Q\) to \(R\) and \([]\) interpolates the map at this location. \(\) clips negative scores to zero to reduce the impact of outliers, as in robust optimization.

**Negative sampling:** A critical and well-studied aspect of contrastive learning is the selection of negative samples [35; 98; 108]. Hard negatives should be high-likelihood but incorrect predictions, so as to push the probability mass to the ground truth. Random poses can be easily distinguished and exhaustive voting in the 3-DoF pose space is computationally infeasible at high resolution [78; 6; 28]. Instead, we use RANSAC  to sample poses that are consistent with the predicted features. We sample pairs of 2D-2D correspondences between all cells of both neural maps and solve for the relative pose using the Kabsch algorithm . Inspired by PROSAC , we sample a correspondence between cells \((i,j)\) and \((k,l)\) based on its feature similarity with probability \(P_{ijkl}=}\;(}_{ij}^{Q }}_{kl}^{R}/)\). Unlike NG-RANSAC , gradients are propagated through the scoring rather than the sampling and are thus much smoother. Because the sampling and scoring mirror similar featuremetric errors, negative samples become harder as the learning proceeds.

**Inference-time alignment:** SNAP can estimate the unknown 3-DoF relative pose between any two neural maps. We estimate each map in the sensor coordinate frame, establish tentative correspondences by matching their cells, sample pose hypotheses, and select the pose with the highest score. This includes single-image positioning, where the query map \(^{Q}\) covers the camera frustum. The vertical pooling requires that the gravity direction is known, which is a reasonable assumption for applications like Augmented Reality (AR) and robotics [56; 113; 78]. Our framework also applies more generally to aligning any pair of inputs, including sequence-to-sequence and aerial-to-ground registration, which is required in the first place to pose mapping data in a common reference frame.

## 4 Related work

**Visual positioning** is most commonly tackled with geometric approaches [80; 40; 74] that rely on point correspondences across images and sparse 3D point clouds built with SfM [83; 2]. They then estimate the 6-DoF query pose with a robust solver [29; 20; 21; 19; 15; 5] from correspondences with the reference model or images. Such correspondences are most often estimated by sparse local features [54; 4]. This process is complex and end-to-end back-propagation is impractical . Past works have thus focused on learning specific components like feature extraction [111; 58; 23; 24;99, 69, 26, 101, 105, 55], matching [112; 115; 75; 91; 41; 117; 107; 53], and pose [104; 76] or point cloud refinement . Coarse GPS location and gravity direction are commonly assumed to be known [113; 56; 93]. In AR and robotics, the height of the camera can be estimated as the distance to the ground in a local SLAM reconstruction . These assumptions reduce the problem to 3-DoF estimation and make it more amenable to end-to-end learning. MapNet  also learns end-to-end 3-DoF visual mapping and localization but requires sequences of depth inputs. Recent works leverage overhead instead of ground-level images [87; 86; 109; 28]. They easily scale to large scenes but only in open-sky areas. Their accuracy is also limited by the low resolution of aerial imagery. Our work combines the strengths of both ground-level and overhead imagery by learning end-to-end how to best fuse them for 3-DoF positioning. Our differentiable pose estimation, based on RANSAC, is more efficient [38; 28; 78], robust , and stable [10; 8] than previous approaches.

**Semantic representations** can largely benefit loop closure  and pose estimation . OrienterNet  learns 3-DoF positioning end-to-end from public 2D semantic maps that are more compact yet detailed enough for localization. Its accuracy is however limited because these maps have low spatial accuracy and are infrequently updated. It is also also restricted to few, explicit semantic classes that are often not discriminative. Differently,  learns finer-grained semantic classes for temporal and viewpoint consistency. Our work instead learns _implicit_ semantics from posed imagery by combining end-to-end self-supervised learning with large amounts of data. This boosts the positioning accuracy and is an effective pre-training for semantic tasks.

**Neural scene representation** is an active topic of research. MLPs [57; 96] and tokens  are compact but lack geometric inductive bias. 3D voxel grids are more expressive and thus popular for reconstruction [66; 60; 92; 9; 119], rendering [59; 95], and semantic perception [17; 102; 13; 7] but are expensive to store and thus often restricted to small scenes. 2D grids, or Bird's-Eye Views (BEV), are more compact and thus scale to larger outdoor scenes by compressing the information along the vertical axis. Neural BEVs can be learned from images for supervised semantic tasks [72; 50; 34; 67], 3D reconstruction , self-supervised view synthesis , and 3-DoF positioning [78; 28; 38]. These approaches assume planar scenes or rely on monocular priors only, even if multiple views are available. Instead, we combine these priors with multi-view fusion [110; 82; 106] to leverage information from image sequences and better resolve objects in large scenes.

**Self-supervised learning** leverages unlabeled datasets to learn representations useful for down-stream tasks. Many works focus on image- or pixel-level contrastive learning for semantic tasks [63; 35; 14; 62; 36]. View synthesis from few images typically learns lower-level representations [57; 85]. Some works [49; 89] learn features for image matching across appearance changes. CoCoNets  learns representations for 3D scenes but requires perfect, synthetic depth maps. We learn high-level contrastive scene representations from posed images and show that it translates to semantic mapping.

## 5 Experiments

**Data:** StreetView images are captured by rigs of 6 rolling-shutter cameras mounted on cars or on backpacks worn by pedestrians , which results in a wide diversity of viewpoints in street-level scenes. Multi-view 'frames' are captured synchronously every \(\)5m. Sequences are captured between 2017 and 2022. We build mapping segments _only from car sequences_ by partitioning each sequence into groups of 36 images that face either the left or right side of the road. We define each map grid as a \(64 16\) m tile aligned with the segment mid-frame, in which we render an aerial orthophoto with 20 cm ground sample distance. Query images are sampled from different sequences, captured from cars or backpacks, based on their frustum overlap, and are often taken years apart. We train with 2.5M segments and \(\)50M queries from 11 cities across the world: Barcelona, London, Paris (Europe), New York, San Francisco (North America), Rio de Janeiro (South America), Manila, Singapore, Taipei, Tokyo (Asia), and Sydney (Oceania), reserving some areas in each city for validation. We test on 6 different cities (Amsterdam, Melbourne, Mexico City, Osaka, Sao Paulo, and Seattle), with 4k queries per city. This covers 5 continents, while academic localization benchmarks focus on tourism landmarks  or single cities in Europe or the US [81; 116; 77] - see details in Appendix D.

**Training and implementation:** In the ground-level encoder, \(_{}\) is a U-Net  with a BiT ResNet backbone , pre-trained as in , and an FPN decoder , initialized randomly. We consider two models with different backbones: a 'large' R152x2 (353M parameters) and a'small' R50x1 (84M parameters). \(_{}\) is a similarly-defined R50x1+FPN. In multi-view fusion (Sec. 2.2) we use \(D\)=32 depth planes and \(K\)=60 height planes \(\{z_{k}\}\) uniformly distributed within 12 m. Neural maps \(\) and matching maps \(}\) have dimensions 128 and 32, respectively, and are defined over 64\(\)16 m grids with 20 cm ground sample distance. Query BEVs have a maximum depth of 16 m. At training time, neural maps are built from one aerial tile and one SV segment, with each of the two randomly dropped, similarly to dropout . We use a subset of \(N\)=20 views, some of them at a \(\)60deg angle, which we empirically found provides a good coverage/memory trade-off. See details in Appendix E.

**Visual positioning:** We build a map for each segment using all 36 views and evaluate the 3-DoF query pose in terms of position and orientation errors. While many academic benchmarks use much larger mapping areas, we argue that GPS and motion priors often make this unnecessary for practical applications . We slice the results by difficulty in terms of query-scene overlap based on the distance between the query and its closest map view, in position \( t\), and orientation \(\). We split in the data into 3 groups: 'easy' (\( t\)\(<\)10 m and \(\)\(<\)45deg, \(\)25% of the data), 'hard' (\( t\)\(>\)10 m and \(\)\(>\)60deg, \(\)25%), and'medium' (the remaining \(\)50%). We compare our approach to hloc , a state-of-the-art  structure-based 6-DoF localization system based on COLMAP , a popular Structure-from-Motion framework, with correspondences estimated by either RootSIFT  or SuperPoint+SuperGlue , a learned feature and matcher. Note that these approaches can only leverage ground-level imagery. We match the query to all map images, without using hloc's

    & Algorithm & Inputs & Easy (25\%) & Med. (50\%) & Hard (25\%) & All (100\%) \\  SfM & + SIFT  & StreetView & 47.0 / 54.4 & 24.9 / 29.9 & 7.6 / 9.7 & 27.1 / 32.1 \\  & + SuperGlue  & StreetView & **63.0** / **71.1** & 38.0 / 44.4 & 13.1 / 16.1 & 39.2 / 45.2 \\   OrienterNet  \\  } &  & 35.6 / 47.2 & 29.3 / 39.5 & 24.8 / 34.8 & 30.0 / 40.6 \\   & & 48.9 / 62.3 & **46.9** / **59.5** & **34.5** / **47.6** & **44.4** / **57.4** \\
**ResNet-152x2** &  StreetView \\ aerial \\  } & 45.8 / 58.4 & 43.9 / 56.0 & 29.5 / 41.7 & 41.0 / 53.2 \\  & & 27.4 / 40.6 & 25.3 / 37.5 & 20.8 / 32.3 & 24.8 / 37.1 \\   **SNAP-small** \\ **ResNet-50** \\  } &  multi-modal \\ StreetView \\ aerial \\  } & 45.2 / 59.0 & 41.9 / 54.8 & 29.6 / 42.0 & 39.9 / 52.9 \\
**ResNet-50** &  StreetView \\ aerial \\  } & 42.2 / 54.9 & 38.1 / 50.1 & 24.5 / 36.4 & 36.0 / 48.2 \\  & & 23.9 / 35.6 & 21.9 / 32.8 & 17.9 / 27.5 & 21.5 / 32.3 \\   

Table 1: **Single-image positioning. We report the area under the recall curve (AUC) up to thresholds (2.5 m/5°) and (5 m/10°). Our large and small multi-modal models are more accurate than classical _S/M_ + \(X\) approaches for medium and hard queries, which matter most in practical applications. Fusing both StreetView and aerial imagery is more accurate than using only one of them.**

Figure 5: **Single-image positioning with different maps. Localizing with our neural maps yields a higher recall than established approaches based on feature matching (_S/M_ + _X_), especially for hard queries with low visual overlap. Neural maps are also more suitable for positioning than semantic maps because they encode richer and thus more discriminative information.**

retrieval component, and estimate the query's pose using RANSAC and a P2P solver with gravity constraint . We evaluate the 6-DoF pose projected to 3-DoF. We also evaluate OrienterNet , which matches a query BEV with a semantic map. We re-implement and train it on overhead semantic rasters derived from SfM points with semantic labels obtained by fusing 2D image segmentations. Note that OrienterNet was originally trained on OpenStreetMap, which has limited coverage of small objects. While our rasters are noisy, they provide a consistent, global coverage of fine-grained classes like tree, streetlight, poles, etc. We checked our implementation with the authors. We also evaluate versions of SNAP trained with only either ground-level or aerial imagery - the latter is an extreme case of cross-view localization, similar to .

Fig. 5 and Tab. 1 show that SNAP outperforms the state of the art, COLMAP with SuperPoint+SuperGlue, by a large margin: 25% relative. Structure-based approaches are more accurate for easy queries but significantly worse for hard ones (Fig. 3). Using ground-level imagery is crucial in most localization scenarios and performs \(\)46% relative better than using aerial imagery, whereas our multi-modal model performs \(\)8% relative better than the StreetView-only variant. We justify our design decisions using an ablation study in Appendix A. We report detailed results per city and per sequence type in Appendix C. Our framework is also efficient, as mapping takes 223 ms per segment and 6 ms per aerial tile, estimating a query BEV takes 14 ms and localizing it takes 86 ms, on an A100 GPU. In comparison, matching with SuperGlue takes 100ms per pair for 36 pairs per query, and is thus 36 times slower. Each tile of our matching maps has size 1.6 MB in fp16, while storing SuperPoint descriptors requires 5.3 MB on average.

**Semantic mapping:** We show that SNAP's neural maps are an effective pre-training for 2D semantic mapping. Existing approaches rely on ground truth 2D semantic rasters derived from the segmentation of LiDAR 3D point clouds. These are manually labeled, which is too expensive to generate enough data to train from scratch models that generalize across countries, sensors, seasons, and times of the day. Existing datasets  thus rarely span more than a few cities and overfit supervised models to the local appearance. Instead, our self-supervised pre-training learns better features from a much larger dataset of posed imagery, which is much cheaper to acquire at scale. The information bottleneck forces SNAP to learn unified representations for objects, like street crossings or lights, that look very different across countries, and would require larger amounts of labeled data. Fig. 6-left shows a 2D t-SNE  visualization of SNAP's neural maps at points sampled on a few types of objects common in street scenes, according to their ground-truth semantic label. Points of the same class are clustered together. This clearly shows that neural maps learn to distinguish these objects without any semantic supervision, even if they are geometrically similar, _e.g_., tree _vs_ pole.

To evaluate the pre-training, we train a tiny CNN to predict semantic rasters from pre-trained neural maps, keeping SNAP frozen. We compare this to training the entire model from scratch (with the same backbones initialization ). We derive 3k 64\(\)16 m ground truth rasters from LiDAR point clouds captured by StreetView cars in 84 cities across the world. We train with 2k examples and report the recall of both approaches on 1k test examples in Fig. 6-center. Pre-training consistently

Figure 6: **2D Semantic mapping. Left: t-SNE visualization of the neural map features learned by SNAP, colored by their ground-truth semantic class. SNAP discovers different categories of objects common in outdoor urban scenes, which yields clearly distinguishable clusters. Middle: Given a small labeled dataset, training a tiny CNN classifier to predict such classes from pre-trained features is more effective than training the entire SNAP model from scratch, especially for small and infrequent objects. Right: Test example with ground truth raster (middle) and prediction of the CNN (bottom).**

yields better results for every class, with larger gains on more difficult/infrequent classes. While training from scratch massively overfits to such small dataset, our neural maps encode enough information to reach recalls over 70%. We show qualitative examples in Fig. 6-right and Appendix B.

**Monocular priors:** We visualize in Fig. 7 the occupancy predicted by SNAP as depth and confidence maps. SNAP learns sensible priors over the geometry of street scenes from only pose supervision.

**Limitations:** Our approach is not as accurate as structure-based methods given easy queries closer to map images (Fig. 5). We hypothesize this is partly due to operating at lower image resolutions. It also assumes gravity direction and a location prior, which are reasonable assumptions but restrict its use.

## 6 Conclusion

We present SNAP, a novel approach to build semantic, 2D neural maps from multi-modal inputs and train it by simply learning to align two neural maps in a contrastive framework. This simple objective yields a model that can localize queries beyond the reach of the state of the art in structure-based matching by discovering high-level semantics from self-supervision. Our neural maps are easily interpretable and provide an effective pre-training towards unlocking semantic understanding at scale.

**Broader impact:** This work has implications to privacy and surveillance. However, our 2D maps are too compact to preserve personal identifiable information, and likely more difficult to invert than point clouds , which despite ongoing efforts [88; 31; 32; 61; 118] remain susceptible to attacks .

Figure 7: **Monocular depth priors learned by the ground-level encoder. For each query image (a), we show: (b) the expected log-depth across all depth planes, from blue (close) to red (distant); (c) the total score along each ray \(_{i\{1...D\}}[:,:,d_{i}]\), which reflects how useful or confident the prediction is; (d) the resulting bird’s-eye view. The predictions are sensible for areas close to the ground and for lower parts of objects and buildings. Predictions in the sky and upper facades are not reliable because these areas are never covered by the height planes \(\{z_{k}\}\) of the point columns.**

**Acknowledgements:** We thank Bernhard Zeisl, Songyou Peng, Remi Pautrat, and Michal Tyszkiewicz for their valuable feedback, Manuel Cabral and Johann Volz for providing useful code reviews, Tianqi Fan for helping processing the data, Lucas Beyer for providing the pre-trained ResNet backbones, Thomas Funkhouser and Kyle Genova for providing the ground truth semantic labels, and Arjun Karpur for helping run SuperGlue.