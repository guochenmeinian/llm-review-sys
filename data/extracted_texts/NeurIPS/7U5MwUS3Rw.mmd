# Towards Harmless Rawlsian Fairness Regardless of Demographic Prior

Xuanqian Wang\({}^{1}\) Jing Li\({}^{2,3}\) Ivor W. Tsang\({}^{2,3,4}\) Yew-Soon Ong\({}^{2,3,4}\)

\({}^{1}\)School of Computer Science and Engineering, Beihang University, China

\({}^{2}\)Institute of High-Performance Computing, Agency for Science, Technology and Research, Singapore

\({}^{3}\)Centre for Frontier AI Research, Agency for Science, Technology and Research, Singapore

\({}^{4}\) College of Computing and Data Science, Nanyang Technological University, Singapore

wwwxxqq@buaa.edu.cn {kyle.jingli,ivor.tsang}@gmail.com asysong@ntu.edu.sg

Work done during an internship at A*STAR.Corresponding author.

###### Abstract

Due to privacy and security concerns, recent advancements in group fairness advocate for model training regardless of demographic information. However, most methods still require prior knowledge of demographics. In this study, we explore the potential for achieving fairness without compromising its utility when no prior demographics are provided to the training set, namely _harmless Rawlsian fairness_. We ascertain that such a fairness requirement with no prior demographic information essential promotes training losses to exhibit a Dirac delta distribution. To this end, we propose a simple but effective method named VFair to minimize the variance of training losses inside the optimal set of empirical losses. This problem is then optimized by a tailored dynamic update approach that operates in both loss and gradient dimensions, directing the model towards relatively fairer solutions while preserving its intact utility. Our experimental findings indicate that regression tasks, which are relatively unexplored from literature, can achieve significant fairness improvement through VFair regardless of any prior, whereas classification tasks usually do not because of their quantized utility measurements. The implementation of our method is publicly available at https://github.com/wxqpxv/VFair.

## 1 Introduction

Fairness in machine learning has gained significant attention owing to its multifaceted ethical implications and its far-reaching potential to shape and influence various aspects of society [1; 2; 3]. In high-stakes decision-making domains, algorithms that merely prioritize model utility may yield biased models, resulting in unintentional discriminatory outcomes concerning factors such as gender and race. Group fairness, a.k.a. statistical fairness , addresses this issue by explicitly encouraging the model behavior to be independent of group indicators, such as disparate impact , or equalized odds . However, with increasing privacy concerns applied in practical situations, sensitive attributes are not accessible which raises a new challenge for fairness learning.

According to literature, numerous efforts have been directed towards achieving fairness regardless of demographic information, which can be mainly categorized into two branches. One branch is to employ proxy-sensitive attributes [7; 8; 9; 10]. These works assume that estimated or selected attributes are correlated with the actual sensitive attributes and thus can serve as a proxy of potential biases. The other branch follows Rawlsian fairness , which focuses on reducing the disparity in group utility. Unlike the former branch, the group utility here is predefined and centered, and thus it istypically not meaningful to test learned models on other group fairness metrics. Worst-case fairness methods , belonging to Rawlsian fairness, commonly leverage a prior about the worst-off group's size to identify under-represented members and prioritize the utility of the approximate worst-off group. Such a taxonomy overlooks the differences in tasks, as the majority of aforementioned fairness approaches are designed for classification tasks. In applications where discrete outcomes (e.g. binary decisions) provide insufficient information, there is a crucial need for fair regressors. As relatively few discussions  exist for fair regression tasks, our work bridges the gap by incorporating regression tasks into a general predictive loss under Rawlsian fairness.

The trade-off nature between model utility and fairness has emerged as a subject of dynamic discourse . Worst-case fairness methods which inherently prioritize the worst-off group's utility often come at the expense of the overall utility . In this work, we focus on scenarios where no prior demographic information is provided, aligning the ingredients with the standard training setup, and then advocate for a primary problem (harmless Rawlsian fairness):

_Regardless of demographic prior, to what extent can we improve Rawlsian fairness without hurting the model's overall utility?_

This problem is particularly important in utility-intensive scenarios , and we investigate it in both classification and regression tasks to answer the question.

**Our idea.** We approach the problem from a novel perspective. The crux of the desired fairness lies in pursuing minimum group utility disparity across all groups. Since during the training phase, we are not aware of what the actual sensitive attributes are used for test data, the safest way is to ensure every possible disjoint group of training data has the same utility. To this end, we expect the training loss for each individual example to be very close, meaning that the loss variable approaches a Dirac delta distribution. As shown in Fig. 1 (a). The Dirac delta distribution essentially represents an Oracle model, where all the loss values are concentrated at zero, resulting in both a mean and variance of 0. In the distribution view, the motivation of our method which is dubbed as VFair, is to approximate this ideal by minimizing both the mean and variance of the training losses. Fig. 1 (b) also shows the comparison between VFair and other methods, using a regression task as an example. Oracle denotes models with unlimited capability that make predictions with zero error. Empirical Risk Minimization (ERM) refers to models without any fairness design. Worst-case represents fairness methods that require the prior of the worst-off group (e.g., lower bound of partition ratios). Uniform model, initially introduced by , represents a model that performs equally poorly across all groups on classification tasks. Here, we extend it to regression. In a simplified logistic regression task that applies Mean Squared Error (MSE) loss with targets of 0 or 1, a "uniform regressor" predicts values close to 0.5 for each example, resulting in losses close to 0.25, as indicated by the yellow dashed line. As depicted by Fig. 1 (b), we expect VFair to exhibit the following two properties.

(1) VFair achieves a more flattened loss curve compared to ERM and Worst-case. A flattened curve indicates similar losses for each example, indicating a fairer solution for unknown group partitions. (2) VFair maintains an area under the curve comparable to that of ERM, reflecting the overall model utility. Since a flattened curve may deteriorate into a uniform model that significantly sacrifices overall utility, VFair prioritizes keeping the overall average loss at a low value.

Figure 1: Illustration of our idea through different forms of loss curves.

Statistically, our main idea can be understood as minimizing the loss distribution's second moment (e.g., loss variance) while not increasing its first moment. By developing a dynamic approach operated at both the loss and gradient levels, our idea is proven feasible and effective.

**Contributions.** The key contributions of this research can be outlined as follows.

\(\) We introduce the setting of harmless Rawlsian fairness regardless of demographic prior in both classification and regression tasks. To well position this setting, we also discuss its connections with Worst-case fairness and harmless fairness from the view of variance reduction and re-weighting.

\(\) We advocate that minimizing the variance of prediction losses is a straightforward yet effective fairness proxy. By incorporating it as a secondary objective, the overall model performance can remain uncompromised.

\(\) We develop a dynamic approach for conducting harmless updates, which is operated at both the loss and gradient levels, guiding the model towards a fair solution without compromising utility.

\(\) We analyze the difference between fair regression and classification tasks, and experimentally demonstrate that, regardless of any prior, harmless Rawlsian fairness is achievable in regression tasks but unfortunately not in classification tasks.

## 2 Related work

**Worst-case fairness without demographics.** In alignment with the Rawlsian fairness principle, a sequence of studies has followed the Worst-case scheme, which focuses on improving the performance of the worst-off group without full demographics. DRO  identified the worst-off group members by a lower bound for the minimal group ratio. The behind insight is that examples yielding larger losses are more likely sampled from an underprivileged group and thus should be up-weighted, which inherits the fairness strategy for handling group imbalance [21; 22]. Similarly,  also considered training a fair model with a given ratio of the protected group and connected such a fairness learning setting with the subgroup robustness problem . In contrast to these studies, ARL  introduced the concept of Computational-Identifiability to enhance the Worst-case scheme. ARL presented an adversarial re-weighting method to identify the worst-off group in the computational-identifiable region without relying on any demographic prior. This embodies the genuine essence of achieving fairness without demographics and is closest to our setting.

**Harmless fairness.** In utility-intensive scenarios, a fair model is meaningful only when it preserves good utility. Basically, these works engaged in discussing the extent to which fairness can be achieved without compromising model utility. Some [25; 26] searched for the so-called minimax Pareto fair optimality for off-the-shelf binary attributes and then upgraded their method to the multi-value attribute cases with only side information about group size . A pre-processing method  accomplished cost-free fairness through re-weighting training examples based on both fairness-related measures and predictive utility on a validation set. Based on the concept of Rashomon Effect,  achieved fairness from good-utility models under selective labels through a constrained optimization perspective, needing a proper upper bound for the average loss. The same fairness notation also applies to regression task , where the prediction error of protected groups remains below some predefined threshold, and the fairness-accuracy frontier is experimentally achieved. Notably, these works more or less require direct or implicit demographic information and cannot adapt to our problem setting. A dynamic barrier gradient descent algorithm  was recently introduced which allows models to prioritize must-satisfactory constraints. Inspired by this, we conceptualize harmless fairness within a similar framework, enabling us to move beyond a utility-only solution and obtain a fairer model that can narrow the utility gaps among possible data partitions.

## 3 VFair methodology

### Problem setup

Consider a supervised learning problem from input space \(\) to a label space \(\), with training set \(\{z_{i}\}_{i=1}^{N}\), where \(z_{i}=(x_{i},y_{i})\). For a model parameterized by \(\) and a training point \(z^{\#}\)let \((z;)\) be the associated loss. Suppose for each \(z_{i}\), there exists a sensitive attribute \(s_{i}\). Thus a \(K\)-value sensitive attribute \(s\) will naturally partition data into \(K\) disjoint groups. Such sensitive attributes are not observed during model training but can be accessible for fairness testing. Following the principle of Rawlsian fairness, the utility disparity over groups is used as a fairness evaluation metric. For example, in classification tasks, denoting \(u_{k}\) as the classification accuracy of the \(k\)-th group, we can define the maximum utility disparity, i.e., \(=_{i,j[K]}(u_{i}-u_{j})\), as a proper fairness metric. More metrics will be introduced in Section 4.1, and the same applies to regression tasks. The fundamental objective of this work is to develop a model that minimizes group utility disparity [24; 12; 15] to the greatest extent possible maintaining the overall predictive utility (compared to ERM) regardless of demographic prior.

### Fairness via minimizing variance of losses

An ERM model may exhibit variable predictive utility across different groups. Conventionally, a fair counterpart is achievable by properly incorporating the objective of minimizing group utility disparity (e.g., MUD), which is however not applicable when demographics are not accessible at training stages. Intuitively, a predictive model that can be fair for any arbitrary partitions on the test set implies that the loss of each training example should be close to each other, exhibiting a Dirac delta distribution. A compelling piece of evidence is that an Oracle model, as depicted in Fig. 1 (b), ensures that each individual loss \((z;)\) is sufficiently small, resulting in no disparity, i.e., \(=0\). This case suggests that group fairness can be instance-wise characterized and hence bypasses the unobserved sensitive attributes. We present this insight by the following proposition.

**Proposition 1**.: \(u s\) _holds for any \(s\) that splits data into a number of groups, if and only if the loss \(\) is (approximately) independent of the training example \(z\), i.e., \( z\)._

The proof of Proposition 1 is left to Appendix A, and the approximation arises from the quantization of utility metrics, e.g., classification accuracy.

To achieve such a Dirac delta distribution, several fairness objectives can be adopted. We defer the discussion to Appendix B and conclude that applying the variance of losses as a fairness objective is simple yet efficient. Intuitively, the small variance does encourage the loss to be invariant of input. Suppose that we intend to achieve a small MUD through minimizing the maximum group utility disparity, denoted by \(_{}\). The following proposition shows that standard deviation of training losses essentially serves as a useful proxy.

**Theorem 1**.: \( s,,_{} C {_{z}[(z;)]}\)_, where \(C\) is a constant._

Appendix B.1 gives the proof of Theorem 1. Although \(_{}\) is upper-bounded in the form of standard deviation as stated in Theorem 1, we term it "variance" for convenience in statements where it does not introduce ambiguity. So far, we connect Rawlsian fairness with the variance of training losses, without using any prior of demographics.

### Objective formulation

Notably, solely penalizing the variance of losses will not necessarily decrease the expectation of losses, leading to the emergence of a uniform model . Thus, to improve fairness without compromising the overall model utility, our full objective is formulated as follows:

\[_{}_{z}[(z;)]} s.t.\ _{z}[(z;)],\] (1)

where \(\) controls how much we can tolerate the harm on the overall predictive utility, and the sense that \(=_{}_{z}[(z;)]\) suggests a zero-tolerance. In particular, we fold in any regularizers into \(()\) to make our method easily adapt to specific scenarios. The empirical risk of Eq. 1 is written as

\[_{}_{i=1}^{N}((z _{i};)-())^{2}}}_{()} s.t. \ _{i=1}^{N}(z_{i};)}_{()}.\] (2)

We use \(()\) and \(()\) to denote the primary and secondary objectives, respectively. Since we minimize \(()\) inside the optimal set of minimizing \(()\), the eventually learned model is viewed to be harmlessly fair with regard to the overall performance.

Note that the objective of Eq. 1 looks similar to variance-bias research [29; 30]. Following Bennett's inequality, the expected risk can be upper bounded by the empirical risk plus a variance-related term with a high probability:

\[_{z}[(z;)]_{i=1}^{N}(z;)+C_{1} _{z}[(z;)]}{N}}+}{N},\] (3)

where \(C_{1}\) and \(C_{2}\) are some constants which reflect the confidence guarantee. We emphasize Eq. 3 is apparently distinct from our fairness motivation. As derivated in , the right-hand side of Eq. 3 can be well approximately by a robust regularized risk, a.k.a. DRO's objective ,

\[_{}_{}\{F(;):=C( _{z}[[(z;)-]_{+}^{2}])^{ }+\},\] (4)

where \(C=(2(1/_{min}-1)^{2}+1)^{1/2}\) and \(_{min}\) is a bound of the worst-off group's ratio. Given an \(^{t}\) which is the optimal solution of the \(t\)-th inner optimization but also happens to be close to the mean loss, i.e., \(^{t}(^{t})\), Eq. 4 can be viewed as penalizing the upper semi-variance of training loss. This observation connects Worst-case fairness with variance penalization from a new aspect. Although focusing on the variance of training losses in our method as well, we penalize it inside the optimal set of empirical losses. Our method is eventually capable of achieving harmless fairness.

### Harmless fairness update

Directly calculating the optimal set of \(()\) in Eq. 2 can be very expensive. A common approach is to consider an unconstrained form of Eq. 2, i.e., Lagrangian function, which however needs to not only specify a proper \(\) beforehand but also optimize the Lagrange multiplier to satisfy the constraint best. Besides, the constrained form of Eq. 2 makes our task different from traditional multi-objective optimization tasks. Recognizing that such re-balancing between two loss terms essentially operates on gradients, in a manner analogous to the approach outlined by , we consider the following gradient update scheme,

\[^{t+1}^{t}-^{t}(^{t}( ^{t})+(^{t})),\] (5)

where \(^{t}\) is a step size and \(^{t}(^{t} 0)\) is the dynamic coefficient we aim to get. For simplicity, we omit the time stamp \(t\) and model variable \(\) when it is not necessary to emphasize them. Now we provide how do we dynamically adjust \(\).

**Gradient view.** The idea of designing \(\) is to keep decreasing \(\) when the constraint is not met, meaning that the combined gradient should never hurt the descent of \(\). As depicted in Fig. 2 (a), if the gradients \(\) and \(\) forms an obtuse angle, a detrimental component emerges in the direction of \(\) (indicated by the red dashed arrow). Otherwise, the gradient conflict does not happen, shown as Fig. 2 (b). Consequently, \(\) should be sufficiently large to ensure that the combined force's component in the primary direction remains non-negative, that is

\[+_{}() - }{||||^{2}}:=_{1}.\] (6)

Here, \(\) represents the extent to which we wish to update \(\) when two gradients are orthogonal. We choose \(=1\) in Eq. 6 because it keeps an intact update for the primary gradient \(\) in any cases. The harmless component of optimizing \(\), illustrated as the dotted yellow arrow in Fig. 2 (a), undergoes with equal strength. The derivation of \(_{1}\) essentially assumes that the constraint of Eq. 2 is satisfied if \(=\), which avoids an elaborately specified \(\). When \(\) but \(||||\) is small, indicating that the primary objective is nearly minimized, we set \(=\{_{1},0\}\) to prevent negative values.

**Loss view.** Recall that \(\) takes \(\) as input according to Eq. 2, which inspires us to further inspect the combined gradient, denoted by \(\), beyond treating them separately as we do in the gradient view.

**Theorem 2**.: _Given the objective of Eq. 2, the combined gradient derived by the update scheme of Eq. 5 can be expressed with an example-reweighting form,_

\[=+=_{i=1}^{N} }(_{i}-))}_{ w_{i}}}{}.\] (7)

The proof of Theorem 2 can be referred to Appendix C. Eq. 7 shows that our fairness formulation with dynamic gradient update implicitly reweights each training example via an unnormalized weight \(w_{i}\), i.e., the Z-score of loss plus a coefficient \(\). This finding connects our work with recent Worst-case fairness studies [13; 12; 22] which up-weight the training examples whose losses are relatively larger, and also a harmless fairness method  which directly applies the re-weighting scheme.

As we can see \(w_{i}<0\) for all examples whose Z-scores below \(-\), raising a concern of unstable optimization . To guarantee that the weight of each training example is non-negative, we require

\[ i[N]&+ {}(_{i}-) 0\\ &_{i[N]}-_{i}}{}=}(-_{i [N]}_{i})}{}:=_{2},\] (8)

where the last inequality holds because predictive losses are typically designed to be non-negative, facilitating the elimination of the sorting procedure.

**Remark 2**.: According to Eq. 8, \(_{2}\) is positive. Notably, \(_{2}\) can approach \(0\) if \(\), where we may obtain a model with good utility but poor fairness. Since Z-scores fall within the range of \(-3\) to \(+3\) capturing a significant portion (99.7%) of the data in a normal distribution, \(_{2}\) is often capped by \(3\).

In summary, combining Eq. 6 and Eq. 8, we compute the adaptive \(\) at each step that ensures harmless fairness update:

\[=(_{1},_{2})=(1-}{||||^{2}},}{}).\] (9)

Note that Eq. 9 requires the computation of gradients and values for both \(\) and \(\), which is time-intensive and memory-intensive if executed on the entire dataset. To scale up to large datasets, we provide an efficient mini-batch update strategy. Detailed implementation and algorithm can be referred to Appendix D.

## 4 Experiments

### Experimental setup

**Datasets.** Six datasets encompassing binary classification, multi-class classification, and regression are employed. (i) UCI Adult , (ii) Law School , (iii) COMPAS , (iv) CelebA , (v) Communities & Crime (C & C) , (vi) AgeDB . Note that datasets (i-iii) can be transformed into a logistic regression task by applying MSE loss with the category label as the target. Following the convention established by [24; 15], we select sex (or gender) and race (or Young on CelebA) as sensitive attributes for datasets (i-iv), four attributes for C & C, and one for AgeDB datasets.

**Metrics.** During the evaluation phase, we gain access to the sensitive attributes that partition the dataset into \(K\) disjoint groups. As discussed in Section 3.1, our training objective is to uphold a high overall predictive utility level while minimizing group utility disparities to the greatest extent feasible. Henceforth, we assess the performance of our method across five distinct metrics: (i) **Utility**: Overall accuracy for classification (also specified by other metrics like F1-score and prediction error when necessary) and MSE for regression tasks. (ii) **WU**: The worst group utility among all \(K\) groups. (iii) **MUD**: Maximum utility disparity, as described in Section 3.1. (iv) **TUD**: Total utility disparity. \(=_{k[K]}(u_{k}-)\), where \(\) is the global average utility. (v) **VAR**: The variance of prediction error. Since we are not able to exhaustively enumerate all possible sensitive attributes and test fairnessvia the metrics (ii-iv), VAR necessarily serves as a fairness proxy for any other possible selected sensitive attributes during the test phase. To ensure the reliability of the results, we repeat all the experiments 10 times and average the outcomes.

**Baselines.** We compare VFair against seven baselines including ERM, DRO , ARL , FairRF , MPFR , BPF , and FKL . Note that DRO, BPF, FairRF, MPFR, and FKL all require some prior demographic information; DRO and BPF necessitate the identification of the worst-off group through a bound of group ratio, while FairRF selects some observed features as pseudo-sensitive attributes, which consequently constrain its application to image datasets (i.e., CelebA); MPFR and FKL which are particularly designed for fair regression tasks also incorporate sensitive attributes. Methods take general loss functions like VFair which apply to both classification tasks and regression tasks, i.e., DRO and ARL are implemented for regression tasks by using the MSE loss. Note that BPF, MPFR, and FKL are not designed with stochastic updates and they suffer from out-of-memory issues under our experimental setup on the UCI Adult and AgeDB datasets. Therefore, the experimental results of this part are not included. Please find more experimental setup details in Appendix E.

### Examine harmless fairness in regression tasks

Table 1 showcases the comparison results of different methods on regression tasks. The standard deviation calculated from every 10 repeated experiments is presented in the bracket. In the "Improved" row, we computed the improvement of VFair compared to ERM, where "+" denotes improvement rather than a numerical increase. Results with significant changes at the 0.05 significance level are highlighted in green, while others are in yellow. Note that our objective is to gain improvement in fairness metrics while maintaining utility, non-significant changes in utility are desired. However, significant drops in utility violate the harmless setting.

According to Table 1, we have the following findings. (1) VFair significantly improves most fairness metrics with non-significant changes in Utility. Exceptions on C & C and AgeDB are due to their

    & & Utility \(\) & WU \(\) & MUD \(\) & TUD \(\) & VAR \(\) \\   & ERM & 12.88\({}_{0.12}\) & 19.75\({}_{0.12}\) & 7.33\({}_{0.14}\) & 13.45\({}_{0.12}\) & 4.89\({}_{0.07}\) \\  & DRO & 24.85\({}_{0.09}\) & 24.98\({}_{0.06}\) & 0.14\({}_{0.06}\) & 0.23\({}_{0.02}\) & 0 \\  & ARL & **12.86\({}_{0.11}\)** & 19.72\({}_{0.23}\) & 7.33\({}_{0.19}\) & 13.54\({}_{0.27}\) & 4.86\({}_{0.15}\) \\  & BPF & 18.75\({}_{0.30}\) & 43.25\({}_{0.44}\) & 26.33\({}_{0.30}\) & 47.33\({}_{0.15}\) & 3.98\({}_{0.39}\) \\  & MPFR & 13.88 & 29.39 & 16.68 & 32.07 & 7.15 \\  & FKL & 13.10 & 19.37 & 6.77 & 13.42 & 5.01 \\   & VFair(Ours) & 12.95\({}_{0.11}\) & **19.08\({}_{0.22}\)** & **6.63\({}_{0.18}\)** & **12.53\({}_{0.27}\)** & **3.66\({}_{0.15}\)** \\  & Improved & -0.07 & **+0.67** & **+0.71** & **+0.92** & **+1.23** \\   & ERM & 23.08\({}_{0.07}\) & 24.49\({}_{0.03}\) & 2.50\({}_{0.11}\) & 3.45\({}_{1.16}\) & 3.23\({}_{0.38}\) \\  & DRO & 24.97\({}_{0.06}\) & 25.05\({}_{0.06}\) & 0.12\({}_{0.06}\) & 0.17\({}_{0.06}\) & 0 \\  & ARL & **22.73\({}_{0.40}\)** & 24.26\({}_{0.05}\) & 2.92\({}_{0.105}\) & 3.78\({}_{0.11}\) & 3.19\({}_{0.47}\) \\  & BPF & 50.80\({}_{2.10}\) & 63.46\({}_{0.03}\) & 22.87\({}_{0.105}\) & 37.77\({}_{1.16}\) & 11.18\({}_{0.111}\) \\  & MPFR & 36.26 & 38.36 & 6.23 & 9.13 & 17.33 \\  & FKL & 28.56 & 30.49 & 3.69 & 6.47 & 7.58 \\   & VFair(Ours) & 23.15\({}_{0.11}\) & **23.83\({}_{0.21}\)** & **0.93\({}_{0.21}\)** & **1.17\({}_{0.20}\)** & **0.47\({}_{0.07}\)** \\  & Improved & -0.07 & **+0.66** & **+1.57** & **+2.28** & **+2.76** \\  _{1.25}\) & 109.72\({}_{0.56}\) & 106.56\({}_{0.57}\) & 337.26\({}_{0.13}\) & 87.52\({}_{0.43}\) \\  & DRO & 99.34\({}_{0.43}\) & 257.51\({}_{0.42}\) & 248.56\({}_{0.48}\) & 715.62\({}_{0.49}\) & 284.49\({}_{0.271}\) \\  & ARL & **40.43\({}_{0.14}\)** & 109.08\({}_{0.10}\) & 106.83\({}_{0.57}\) & 331.38\({}_{0.48}\) & 83.95\({}_{0.57}\) \\  & BPF & 71.05\({}_{0.120}\) & 127.28\({}_{0.74}\) & 110.16\({}_{0.100}\) & 320.65\({}_{0.81}\) & 96.76\({}_{0.84}\) \\  & MPFR & 93.57 & 296.36 & 295.59 & 843.47 & 375.79 \\  & FKL & 83.73 & 278.30 & 275.29 & 794.59 & 321.42 \\   & VFair(Ours) & 41.17\({}_{0.66}\) & **106.46\({}_{0.26}\)** & **104.54\({}_{0.31}\)** & **318.33\({}_{0.39}\)** & **67.44\({}_{0.39}\)** \\  & Improved & -0.02 & -3.32 & +2.02 & **+18.93** & **+20.08** \\   & ERM & 4.25\({}_{0.49}\) & 4.32\({}_{0.53}\) & 0.15\({}_{0.12}\) & 0.15\({}_{0.12}\) & 0.57\({}_{0.10}\) \\  & DRO & 17.72\({}_{0.259}\) & 17.98\({}_{0.26}\) & 0.5\({}_{0.37}\) & 0.5\({}_{0.37}\) & 5.76\({}_{0.75}\) \\   & ARL & 5.11\({}_{1.36}\) & 5.29\({}_{1.90}\) & 0.36\({}_{0.41}\) & 0.36\({}_{0.41}\) & 2.51\({}_{0.32}\) \\    & VFair(Ours) & **3.57\({}_{0.56}\)** & **3.63\({}_{0.77}\)** & **0.12\({}_{0.09}\)** & **0.12\({}_{0.09}\)** & **0.23\({}_{0.06}\)** \\   & Improved & +0.65 & **+0.69** & +0.03 & +0.03 & **+0.34** \\   

Table 1: Comparison of regression results (\( 10^{2}\)) on five benchmark datasets with the best rank in bold. Here, \(\) is for Utility and WU because MSE is used, and smaller values indicate better utility.

specific group partition. C & C is split into 16 groups with some extremely small groups, limiting the improvement on WU and MAD while VFair still outperforms others on TAD and VAR. AgeDB is split by gender with a ratio of 4:6, where ERM can also be relatively fair. (2) VFair gains significant VAR improvement on all datasets, guaranteeing that the group utility disparity remains low for any downstream sensitive attributes. (3) The utility of the test set turns out clear distinctions among compared methods because prediction error (MSE) is sensitive to both the possible distribution shift of test data and model parameters. In this sense, only VFair and ARL can still approach the utility of ERM while the rest usually cannot. (4) DRO gains utility close to 0.25 on each group (i.e. a uniform regressor as shown in Fig. 1 (b)) on Law School and COMPAS while using the real prior, shadowed in gray.

### Examine harmless fairness in classification tasks

In the context of classification, fairness metrics provide limited improvements without compromising utility (see Appendix F.1 for detailed results). To further investigate the performance gap between regression and classification tasks, we depict classification losses in Fig. 3 following the same scheme in Fig. 1 (b) on real dataset COMPAS. Curves on other datasets are left in Appendix F.2.

**Our observation.** (1) Regardless of the uniform classifier DRO, our method VFair exhibits a more flattened loss curve compared to others while maintaining a comparable area under the curve (filled with pink), signifying a harmless fairness solution. Such results align with our initial idea, as presented in Fig. 1 (b). (2) Our method VFair implies the Worst-case fairness, the average loss of the worst-off group for VFair will be consistently lower than any other method. Our claim is obviously true if the group size is small. Regarding a larger group size, thanks to the fact the total area under each curve is nearly equal and the curve of VFair is always above others at left, we conclude that the worst-off group's losses for VFair are also the lowest. (3) The vertical dotted blue line represents the threshold, where the intersection with the loss curve of VFair values -log(0.5). Divided by it, the samples on the left are correctly classified, and conversely on the right. As evidenced by the figure, this threshold is close to each method's intersection. Imagine a situation where the loss curve rotates around the decision point with a smaller angle to the x-axis, obtaining a smaller sample disparity. However, due to the discrete metric and unchanged group partition, the accuracy-based metrics' values for this method remain unchanged after rotation. Therefore, despite our method approaching a horizontal loss curve, thus providing a smaller disparity for any potential group split, the fairness improvement is still bounded by the overall utility.

**Beyond accuracy as utility.** Classifying imbalanced data often applies F1-score as a metric, which is free of the effect on true negative samples which can dominate the accuracy result. We test F1-score performance on UCI Adult and CelebA because they have a remarkable imbalance ratio. The results are summarized in Table 2.

We observe that on UCI Adult, the earned fairness for each fairness method is still limited while on CelebA, VFair yields superior performance. A reasonable explanation is that VFair has the opportunity to discover better solutions in a relatively larger solution space, where more diverse

    &  &  \\  & Utility\({}^{}\) & WU \(\) & MUD \(\) & TUD \(\) & Utility\({}^{}\) & WU \(\) & MUD \(\) & TUD \(\) \\  ERM & 75.02 & 72.17 & 6.87 & 8.88 & 91.40 & 70.17 & 19.39 & 22.82 \\ DRO & 36.27 & 16.06 & 23.59 & 41.17 & 77.52 & 74.29 & **3.9** & **4.78** \\ ARL & 74.90 & 71.85 & 7.32 & 9.49 & 91.60 & 70.39 & 20.14 & 24.33 \\ VFair & **75.98** & **72.74** & **5.82** & **7.40** & **91.91** & **75.70** & 14.39 & 18.50 \\   

Table 2: Classification results (\( 100\)) comparison on two imbalanced datasets with F1-score as the utility metric. The best rank is highlighted in bold.

Figure 3: Per-example losses for all compared methods sorted in ascending order on train set.

minima can be examined through fairness criteria. And though F1-score removes the influence of true negative samples, it takes the quantified property and hence may only help amply the gains.

**From quantized to continuous.** For scenarios where prediction error (the difference between prediction and true label) is desired in classification, e.g., assessing whether a model overestimates or underestimates, VFair should be more applicable. To justify this insight, we compare fairness methods (except for FairRF and DRO as they often fall short on utility) on the three datasets reused for regression tasks. Instead of evaluating specific attributes as we do in Section 4.2, we test VFair on all possible divisions of the test set by randomly splitting them into \(K\) groups. The three methods are ranked based on their performance under each metric. From the best to the worst, the rank score is \(1\), \(2\), and \(3\). The average rank over \(100\) times is reported.

Results in Table 3 show that our method VFair has a better rank than other methods regardless of the choice of \(K\), demonstrating that VFair prefers the utility metrics that are loss/error-related. As mentioned in Section 4.1, VAR serves as an approximation for an extreme group split, where each group consists of only one member. Thus, the significantly low VAR in Table 1 and Table 5 implies good results in random partitions on the test set, evidencing that variance can serve as an effective optimized term in Rawlsian fairness tasks without prior demographic information.

### A closer look at VFair

We examine our VFair through extensive experiments. Here we present the partial results and main conclusions. One can refer to Appendix F.2, F.3, and F.4for more details.

**Method efficacy.** We monitor the performance of VFair during the training phase by evaluating it with four utility-related metrics on the test set of COMPAS. Fig. 4 (a) indicates these curves naturally improve in the desired direction under the variance penalty, verifying the effectiveness of our method.

**Ablation study.** We train our model under four settings: \(=1\), \(=(_{1},0)\), \(=_{2}\), and \(=(_{1},_{2})\). As depicted in Fig. 4 (b), we present the per-dataset utility on five regression datasets (results are proportionally scaled on each dataset for a clearer presentation). The full version, considering both \(_{1}\) and \(_{2}\), exhibits the most stability in preserving low MSE, enabling a harmless

    & &  &  &  \\  & Utility & WU & MUD & TUD & Utility & WU & MUD & TUD & Utility & WU & MUD & TUD \\   & ERM & 2.5 & 2.5 & 2.31 & 2.36 & 2.5 & 2.51 & 2.4 & 2.46 & 2.5 & 2.27 & 2.44 & 2.39 \\  & ARL & 2.5 & 2.41 & 2.48 & 2.51 & 2.5 & 2.35 & 2.6 & 2.54 & 2.5 & 2.23 & 2.56 & 2.61 \\  & VFair & **1** & **1.09** & **1.21** & **1.13** & **1** & **1.14** & **1** & **1** & **1** & **1.5** & **1** & **1** \\   & ERM & 2.7 & 2.66 & 2.37 & 2.36 & 2.7 & 2.61 & 2.52 & 2.54 & 2.7 & 2.52 & 2.5 & 2.53 \\  & ARL & **2.3** & 2.34 & 2.36 & 2.37 & 2.3 & 2.39 & 2.48 & 2.46 & 2.3 & 2.31 & 2.5 & 2.47 \\  & VFair & 2.7 & **1** & **1.27** & **1.27** & **1** & **1** & **1** & **1** & **1** & **1.17** & **1** & **1** \\   & ERM & 2.5 & 2.21 & 2.56 & 2.57 & 2.5 & **1.89** & 2.57 & 2.56 & 2.5 & **1.53** & 2.58 & 2.62 \\  & ARL & 2.5 & 2.44 & 2.43 & 2.42 & 2.5 & 1.98 & 2.43 & 2.44 & 2.5 & 1.56 & 2.42 & 2.38 \\   & VFair & **1** & **1.35** & **1.01** & **1.01** & **1** & 2.13 & **1** & **1** & **1** & 2.91 & **1** & **1** \\   

Table 3: Average rank of four compared methods. All methods are trained only once.

Figure 4: Experimental verification of the harmless update strategy.

solution. In Fig. 4 (c), we demonstrate an example of \(_{1}\) and \(_{2}\) on C & C dataset during training, where both serve distinct and complementary roles in preventing the model from sacrificing utility.

**Model examination.** We scrutinize the fair models by studying their parameters and prediction similarity with ERM. Our experiments found that the model learned by VFair is more dissimilar from ERM than other methods. For example, on Law School, the cosine similarity of model parameters in ARL and VFair with ERM is 0.6106 and 0.5839, respectively. This indicates that VFair may explore a larger model space to achieve better performance.

## 5 Conclusion

Towards harmless Rawlsian fairness regardless of demographics, we have introduced a straightforward yet effective variance-based method VFair. VFair harnesses the principle of decreasing the variance of losses to steer the model's learning trajectory, thereby bridging the utility gaps appearing at potential group partitions. The optimization with a devised dynamic weight parameter operated at both the loss and gradient levels, ensuring the model converges at the fairest point within the optimal solution set. By capping the Z-score, our dynamic weight parameter can also prevent the model from overfocusing on outliers with larger losses. The experiments affirm that regression can be a prior-free task to Rawlsian harmless fairness because error-based metrics are more consistent with loss. Strong prior for demographics may be needed for quantized metrics like accuracy in classification tasks. As discussed in Appendix G, limitations may arise from computational costs, where VFair takes twice the time of ERM to uncover more information without access to demographic prior. Future work will involve identifying and addressing further challenges that may arise when applying VFair for the prediction of non-IID data.