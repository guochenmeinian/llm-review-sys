# FEABench: Evaluating Language Models on Real World Physics Reasoning Ability

Nayantara Mudur\({}^{1,2}\)1 Hao Cui\({}^{1}\) Subhashini Venugopalan\({}^{1}\)

Paul Raccuglia\({}^{1}\) Michael Brenner\({}^{1,2}\)

###### Abstract

Building precise simulations of the real world and using numerical methods to solve quantitative problems is an essential task in engineering and physics. We present FEABench, a benchmark to evaluate the ability of language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA) software. We introduce a multipronged evaluation scheme to investigate the ability of LLMs to solve these problems using COMSOL Multiphysics\({}^{}\). We further design an LLM agent equipped with the ability to interact with the software through its Application Programming Interface (API), examine its outputs and use tools to improve its solution over several iterations. Our best performing strategy generates executable API calls 88% of the time. However, this benchmark still proves to be challenging enough that the LLMs and agents we tested were not able to completely and correctly solve any problem. LLMs that can successfully interact with and operate FEA software to solve problems such as those in our benchmark would push the frontiers of automation in engineering. Acquiring this capability would augment LLMs' reasoning skills with the precision of numerical solvers and advance the development of autonomous systems that can tackle complex problems in the real world.

## 1 Introduction

Several works have demonstrated the significant potential of large language models (LLMs) on analytical mathematical and scientific reasoning , and on programming tasks in general-purpose languages like Python . However, many quantitative tasks that are the cornerstone of engineering and scientific workflows require numerical analysis performed with sophisticated computational modeling software. For example, the development of a modern smartphone requires detailed modeling of the mechanical, thermal, and electrical behaviors of its many subcomponents. Finite element analysis (FEA)  is the approach typically used. This involves approximating the partial differential equations that describe the physical behavior of a system by building discretizations (or meshes) over geometries. This is then solved until convergence using numerical solvers. The vast relevance of FEA to domains like mechanical, biomedical and aerospace engineering, optics, and fluid dynamics has given rise to software such as Ansys\({}^{}\), Abaqus\({}^{}\) FEA , and COMSOLMultiphysics(r)[13; 14], that are indispensable to modelling complex systems with the interplay of non-trivial geometries, and multiple physical phenomena.

In this paper, we begin to bridge this gap by examining the ability of LLMs to solve problems using finite element analysis (FEA) by generating Java API calls. This task requires reasoning over the problem and the ability to successfully operate the software. We selected COMSOL Multiphysics(r)because it supports a wide range of physics models and can be applied to applied mathematics problems involving optimization or differential equations, to problems involving complex geometries. Because the FEA workflow is relatively canonical, the reasoning approach for modeling is similar to other FEA software, and all problems typically share a conceptual breakdown into a sequence of blocks that involve defining (1) Geometry, (2) Material properties, (3) Physics, (4) Meshing (5) Numerical Analysis and Solver settings, and (6) Postprocessing (details in Appendix C.2). COMSOL Multiphysics(r)Version 6.1 is used in this work.

Our contributions are the following: We create a benchmark of 15 quantitatively verifiable problems. These problems require numerical methods to solve, and the target values are expected to be largely independent of the modeling software. We measure different state-of-the-art LLMs on their performance with a multi-score evaluation strategy. Finally, we design a multi-turn LLM agent that interacts with the API and improves upon its answers over several iterations and examine its performance.

## 2 Dataset

The benchmark problems are derived from tutorials in the COMSOL Multiphysics(r)Application Gallery  which are often based on established validation problems or other sources [16; 17]. The input is a problem description describing a problem with a specific target quantity that needs to be computed (Figure 1). The problems span a range of real world / mathematical systems including the dynamics of a Lorenz attractor, heat transfer, and a Bragg reflector. Each entry consists of the following main fields: (1) **Model Specifications:** A condensed yet fully-specified description of the problem, and the physics / boundary conditions. This description is unambiguous in terms of details such as material properties or dimensions. (2) **Selection Information:** An engineer would rely on visual information from the Graphical User Interface (GUI) (Figure 3) for spatial information,

    &  &  \\   & & Correctness & Alignment & Physics Reasoning \\  Executability & API Messages & ✓ & & \\ Model Tree Score & Model Tree & & ✓ & \\ Code Diff Score & Code & & ✓ & \\  Physics Metrics & Physics Code & & & \\ Interface Factuality & & ✓ & & \\ Recall Metrics & & & ✓ & ✓ \\ Feature Dimension & & ✓ & & ✓ \\  Target Value Metrics & Output Table & ✓ & ✓ & ✓ \\   

Table 1: Summary of Evaluation Metrics

Figure 1: _Left:_ A problem description. _Right:_ Problems by domain.

particularly the numerical representation of geometrical entities, like points or boundaries. Since the agents do not have visual access to the GUI, we augment the model specifications with this information, to supply the model with the necessary representation of spatial information in the GUI. This information will be valid as long as the agent chooses to construct the geometry in a manner reasonably similar to how the ground truth (GT) geometry was created. (3) **Plan:** An explicit step-by-step description in natural language of the steps to be followed to solve the problem using COMSOL Multiphysics\({}^{}\). Either of the following comprise self-sufficient tasks for the agent to solve: a) Model Specifications + Selection Information (designated **ModelSpeces**), or b) Plan (**Plan**). (4) **Target Description:** A brief phrase describing the quantity that needs to be computed. (5) **Target Value:** The correct value of the target physical quantity. Note, the target values are expected to be largely independent of the choice of modeling software. (6) **Ground Truth Code:** Ground truth code, that if executed, is able to successfully solve the problem and export the target value to a file. (7) **Model Tree:** Executing COMSOL Multiphysics\({}^{}\)calls essentially build a model that can be regarded as modifying a tree with certain predefined 'branches' such as 'geometry' and 'physics'. The model generated by executing code can thus be represented in a condensed form as a model tree (see Appendix A.3 for an example). Converting a tutorial to a benchmark problem requires ensuring that a verifiable artifact (the target) can be derived from it, generating inputs and ensuring they have all the information needed to solve the problem, generating the GT solution and verifying that it exports the correct target value (Appendix A).

## 3 Evaluation Metrics

Our evaluation metrics measure the correctness of the solution, even when a target value could not be exported, (Table 1). These additional metrics offer the advantage of being continuous, unlike the relative error, which can only be computed if the solution exported a valid target value. Metrics denoted by \({}^{}\) require execution. We delineate the metrics, and the facets they probe here: (1) **Executability\({}^{}\):** 'Correct' (executable) lines as a fraction of parsed API calls in an LLM solution. (2) **Model Tree Score\({}^{}\):** Similarity score between the LLM code's model tree and the GT tree. This normalized so that a solution with no lines of code is scored 0. If equivalent to the GT code, it would be 1. This measures the _alignment_ of the model's solution path with a successful path. (3) **Code Similarity Score:** Similarity score between the solution and the GT code. While this also measures alignment with the GT solution, two different code blocks could generate equivalent model subtrees. We mainly report this metric for completeness, and to motivate the need for the domain-specific metrics we introduce here. (4) **Physics Metrics:** The code is a basis to represent the actions the LM takes to model the problem. Since the physics block is the most challenging (Figure 2), we additionally analyze the LLM's physics action sequence by parsing and evaluating the physics commands. The most basic physics action sequence involves: Create Interface (eg: HeatTransfer) \(\) Create Feature under Interface (eg: TemperatureBoundary) \(\) Modify Feature Properties (eg: T0, to set a temperature). Our Physics Metrics include (a) _Interface Factuality_: How many defined interfaces were _not_ hallucinated? (b) _Interface / Feature / Feature Property Recall_: How many interfaces / features / feature properties created / modified by the GT solution were also in the LLM solution? (c) _Feature Dimension:_ For features created by both, does the feature's spatial dimension match? Some nested physics metrics, such as 'Feature Dimension' might not be valid for a specific problem, in case there was no matching feature between the ground truth and the LLM code: we mask out these problems while computing the means for that specific metric. (5) **Target Relative Error\({}^{}\)**: There are various ways in which exporting the correct value may fail: a) the code fails to export b) it exports an 'empty' table or, c) it exports an incorrect table, such as a default value or the wrong quantity. We ask an LLM to check that the exported table matches the target description and that the exported quantity is not a default or a value reproduced as is from the problem description, and to parse the response, if so. **Valid Target** is the number of problems in the benchmark for which the LLM successfully judges the exported table to be valid. We then compute the relative error between the last value in the exported table and the GT answer. **Relative Error \(|\) Strict** computes the mean relative error only over problems for which Valid Export is True, AND the relative error is less than 10%. Relative Error \(|\) Strict is our key metric for whether the LLM truly solved a problem.

## 4 Results

Comparison across LLMsOur baseline experiment queries different frontier LLMs with a one-shot prompt (see Appendix G). Table 2 records the means and standard errors on the means across problems. Mean executability is in the range 0.60-0.79, implying that the LLMs are familiar with the higher-level grammar and syntax of COMSOL MultiphysicsAPI code. However, they are prone to hallucinating the choice of interface (factuality between \([0.54-0.85]\)). This is likely a significant contributor to the non-executable lines because an invalid interface renders all physics lines of code under this interface invalid. Claude 3.5-Sonnet has the best performance in the baseline setting. See Appendix F for a qualitative analysis.

Comparison across Tasks and Agent SettingsWe fix the LLM to Gemini-1.5-Pro, and compare performance with the list of physics interfaces and features (PhyDoc In-Context) added to the prompt and in an interactive multi-turn agent setting. The agent is equipped with an 'evaluator' that both executes the code and records API replies and when executability crosses 90%, examines the code and the output table, and returns feedback, in an 'LLM-as-Judge' fashion. It thus computes a 'fitness' for each solution _without_ having access to the GT target value. The fitness is used to track the best solutions. The ControllerAgent calls a CorrectorSubAgent that calls tools that aid it in correcting the code given the 'current' code and feedback, execution history and the result of tool calls. An initial set of 20 samples are generated using PhyDoc In-Context and the best solutions are corrected for 20 steps (see Appendix C.2 for details). The best of all

  Experiment & Executability & Model Tree & Code & Valid Target \\  & & Score & Similarity & \\ 
**ModelSpecs** : One-Shot & 0.60\(\)0.05 & 0.46\(\)0.07 & 0.17\(\)0.03 & 0/15 \\
**ModelSpecs** : PhyDoc In-Context & 0.62\(\)0.05 & 0.58\(\)0.07 & 0.15\(\)0.02 & 1/15 \\
**ModelSpecs** : Multi-Turn Agent & **0.88\(\)**0.03 & 0.56\(\)0.08 & 0.17\(\)0.03 & **2**/15 \\ 
**Plan** : One-Shot & 0.54\(\)0.03 & 0.39\(\)0.03 & **0.21\(\)**0.03 & 0/15 \\
**Plan** : PhyDoc In-Context & 0.59\(\)0.05 & **0.59\(\)**0.06 & 0.20\(\)0.02 & 0/15 \\  

Table 4: Code Metrics: Comparison across tasks, prompts and agents.

  Experiment & Executability & Model Tree Score & Code Similarity & Valid Target \\  Claude 3.5 Sonnet  & **0.79\(\)**0.03 & **0.69\(\)**0.07 & **0.19\(\)**0.03 & **1**/15 \\ GPT-4o  & 0.78\(\)0.03 & 0.56\(\)0.06 & 0.17\(\)0.03 & 0/15 \\ Gemini-1.5-Pro  & 0.60\(\)0.05 & 0.46\(\)0.07 & 0.17\(\)0.03 & 0/15 \\  

Table 2: Code Metrics: Comparison on **ModelSpecs** across LLMs.

  Experiment & Executability & Model Tree Score & Code Similarity & Valid Target \\  Claude 3.5 Sonnet  & **0.79\(\)**0.03 & **0.69\(\)**0.07 & **0.19\(\)**0.03 & **1**/15 \\ GPT-4o  & 0.78\(\)0.03 & 0.56\(\)0.06 & 0.17\(\)0.03 & 0/15 \\ Gemini-1.5-Pro  & 0.60\(\)0.05 & 0.46\(\)0.07 & 0.17\(\)0.03 & 0/15 \\  

Table 3: Physics Metrics: Comparison on **ModelSpecs** across LLMs.

  Experiment & Interface & Interface & Feature & Feature & Feature \\  & Factuality & Recall & Predsolutions is then identified and evaluated. Both PhyDoc In-Context and the agentic setting significantly reduce interface hallucinations, and the agent's mean executability is 0.88 (vs 0.62). _Thus, grounding the LLM with information about or interaction with the API boosts performance_. This indicates that at least some of the difficulty the LLMs encounter is due to their lack of knowledge of valid arguments to the API commands. We further examine whether the **Plan** task is easier. The comparison between task versions is of interest since both demand slightly different skills. For a person attempting to solve this task, **ModelSpeces** requires the individual to both _infer_ implicit engineering and physical reasoning decisions to be made (e.g.: the LLM needs to infer that the correct representation of a cylinder's 2D cross-section is a rectangle) and further translate this to valid API calls. **Plan** explicitly describes all steps to be followed in natural language and requires the LLM to only _translate_ the steps describing interactions with the GUI to valid calls. The comparison between the two tasks offers one way to decouple the complexity of making correct modelling decisions from the difficulty of translating decisions into syntactically correct API calls. If an LLM fared poorly at making the right modelling decisions but could reliably translate natural language instructions to API calls, it would find **Plan** an easier task. However, we find that a more explicit plan doesn't consistently boost performance. Although **Relative Error**\(|\)**Strict** is the principal metric one would ideally want to optimize for, we do not report means over that metric here since the LLM was only able to compute a Valid Target that was also within 10% of the correct answer for a single problem in the Multi-Turn Agent and ModelSpecs + PhyDoc experiments. For this problem, the correct target value is 18.3\({}^{}\) Celsius, and the value exported by the LLM is 20\({}^{}\) Celsius, which is a default temperature in COMSOL Multiphysics(r): this is an indicator of the solution not being solved correctly. While a stricter relative error threshold would filter out such serendipitous matches, this risks filtering out problems in which a solution might be conceptually correct but differs from the target because of say, differences in solver and mesh sizes. In Figure 2, we used the initial population of samples generated by the LLMs to analyze executability within different blocks of code. The physics block has the lowest executability, thus motivating our focus on metrics that seek to measure performance on the physics decisions (API calls) made by the LLM.

## 5 Discussion

Our benchmark seeks to inspire rigorous evaluations of the capabilities of LLMs on solving problems that require simulating physical phenomena in the real world and performing numerical analysis. Such problems are ubiquitous in science and engineering, and solving them requires synthesizing reasoning over the physics domain with the ability to leverage numerical analysis software such as FEA. Our dataset serves as a novel testbed to evaluate the ability of LLMs to interact with feedback from an execution environment, correctly analyze and attribute errors and correct them. Future directions include devising agent strategies that boost performance, adding more benchmark problems, increasing problem complexity such as working with more complex geometries, and examining whether performance improves by leveraging GUI visual information. An LLM system that can successfully master FEA software would unlock the ability to solve a range of real-world engineering and physics problems.

## 6 Acknowledgements

We are grateful to Eser Aygun for valuable suggestions on agent design. We thank Stephan Hoyer and Marc Coram for useful comments on the draft, and are grateful to Lizzie Dorfman and Rachel Stigler for their guidance. At Harvard, NM is partially supported by the National Science Foundation under Cooperative Agreement PHY2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental Interactions).

Figure 2: Block-wise executability on 300 samples of code with PhyDoc In-Context and Gemini-1.5-Pro. The physics block has the lowest executability. Error bars are standard deviations.