# The Double-Edged Sword of Implicit Bias:

Generalization vs. Robustness in ReLU Networks

 Spencer Frei

UC Davis

sfrei@ucdavis.edu

&Gal Vardi

TTI-Chicago and Hebrew University

galvardi@tictic.edu

&Peter L. Bartlett

UC Berkeley and Google DeepMind

peter@berkeley.edu

&Nathan Srebro

TTI-Chicago

nati@ttic.edu

Equal contribution.

###### Abstract

In this work, we study the implications of the implicit bias of gradient flow on generalization and adversarial robustness in ReLU networks. We focus on a setting where the data consists of clusters and the correlations between cluster means are small, and show that in two-layer ReLU networks gradient flow is biased towards solutions that generalize well, but are vulnerable to adversarial examples. Our results hold even in cases where the network is highly overparameterized. Despite the potential for harmful overfitting in such settings, we prove that the implicit bias of gradient flow prevents it. However, the implicit bias also leads to non-robust solutions (susceptible to small adversarial \(_{2}\)-perturbations), even though robust networks that fit the data exist.

## 1 Introduction

A central question in the theory of deep learning is how neural networks can generalize even when trained without any explicit regularization, and when there are more learnable parameters than training examples. In such optimization problems there are many solutions that label the training data correctly, and gradient descent seems to prefer solutions that generalize well . Thus, it is believed that gradient descent induces an _implicit bias_ towards solutions which enjoy favorable properties . Characterizing this bias in various settings has been a subject of extensive research in recent years, but it is still not well understood when the implicit bias provably implies generalization in non-linear neural networks.

An additional intriguing phenomenon in deep learning is the abundance of _adversarial examples_ in trained neural networks. In a seminal paper, Szegedy et al.  observed that deep networks are extremely vulnerable to adversarial examples, namely, very small perturbations to the inputs can significantly change the predictions. This phenomenon has attracted considerable interest, and various attacks (e.g., ) and defenses (e.g., ) were developed. However, the fundamental principles underlying the existence of adversarial examples are still unclear, and it is believed that for most tasks where trained neural networks suffer from a vulnerability to adversarial attacks, there should exist other neural networks which can be robust to such attacks. This is suggestive of the possible role of the optimization algorithms used to train neural networks in the existence of adversarial examples.

In this work, we study the implications of the implicit bias for generalization and robustness in ReLU networks, in a setting where the data consists of clusters (i.e., Gaussian mixture model) and thecorrelations between cluster means are small. We show that in two-layer ReLU networks trained with the logistic loss or the exponential loss, gradient flow is biased towards solutions that generalize well, albeit they are non-robust. Our results are independent of the network width, and hence they hold even where the network has significantly more parameters than training examples. In such an overparameterized setting, one might expect harmful overfitting to occur, but we prove that the implicit bias of gradient flow prevents it. On the flip side, in our setting the distances between clusters are large, and thus one might hope that gradient flow will converge to a robust network. However, we show that the implicit bias leads to non-robust solutions.

Our results rely on known properties of the implicit bias in two-layer ReLU networks trained with the logistic or the exponential loss, which were shown by Lyu and Li  and Ji and Telgarsky . They proved that if gradient flow in homogeneous models (which include two-layer ReLU networks) with such losses reaches a small training loss, then it converges (in direction) to a KKT point of the maximum-margin problem in parameter space. We show that in clustered data distributions, with high probability over the training dataset, every network that satisfies the KKT conditions of the maximum-margin problem generalizes well but is non-robust. Thus, instead of analyzing the trajectory of gradient flow directly in the complex setting of training two-layer ReLU networks, we demonstrate that investigating the KKT points is a powerful tool for understanding generalization and robustness. We emphasize that our results hold in the _rich_ (i.e., _feature learning_) regime, namely, the neural network training does not lie in the kernel regime, and thus we provide guarantees which go beyond the analysis achieved using NTK-based results.

In a bit more detail, our main contributions are the following:

* Suppose that the data distribution consists of \(k\) clusters, and the training dataset is of size \(n(k)\). We show that with high probability over the size-\(n\) dataset, if gradient flow achieves training loss smaller than \(\) at some time \(t_{0}\), then it converges in direction to a network that generalizes well (i.e., has a small test error). Thus, gradient-flow-trained networks cannot harmfully overfit even if the network is highly overparameterized. The sample complexity \((k)\) in this result is optimal (up to log factors), since we cannot expect to perform well on unseen data using a training dataset that does not include at least one example from each cluster.
* In the same setting as above, we prove that gradient flow converges in direction to a non-robust network, even though there exist robust networks that classify the data correctly. Specifically, we consider data distributions on \(^{d}\) such that the distance between every pair of clusters is \(()\), and we show that there exists a two-layer ReLU network where flipping the output sign of a test example requires w.h.p. an \(_{2}\)-perturbation of size \(()\), but gradient flow converges to a network where we can flip the output sign of a test example with an \(_{2}\)-perturbation of size much smaller than \(\). Moreover, the adversarial perturbation depends only on the data distribution, and not on the specific test example or trained neural network. Thus, the perturbation is both _universal_ and _transferable_. We argue that clustered data distributions are a natural setting for analyzing the tendency of gradient methods to converge to non-robust solutions. Indeed, if positive and negative examples are not well-separated (i.e., the distances between points with opposite labels are small), then robust solutions do not exist. Thus, in order to understand the role of the optimization algorithm, we need a setting with sufficient separation between positive and negative examples.

The remainder of this paper is structured as follows: Below we discuss related work. In Section 2 we provide necessary notations and background, and introduce our setting and assumptions. In Sections 3 and 4 we state our main results on generalization and robustness (respectively), and provide some proof ideas, with all formal proofs deferred to the appendix. We conclude with a short discussion (Section 5).

Related work

Implicit bias in neural networks.The literature on implicit bias in neural networks has rapidly expanded in recent years, and cannot be reasonably surveyed here (see Vardi  for a survey). In what follows, we discuss results that apply to two-layer ReLU or leaky-ReLU networks trained with gradient flow in classification settings.

[MISSING_PAGE_FAIL:3]

Lipschitz constant. These results suggest that overparameterization might be necessary for robustness. In this work, we show that even if the network is highly overparameterized, the implicit bias of the optimization method can prevent convergence to robust solutions.

## 2 Preliminaries

We use bold-face letters to denote vectors, e.g., \(=(x_{1},,x_{d})\). For \(^{d}\) we denote by \(\|\|\) the Euclidean norm. We denote by \([]\) the indicator function, for example \([t 5]\) equals \(1\) if \(t 5\) and \(0\) otherwise. We denote \((z)=1\) if \(z>0\) and \(-1\) otherwise. For an integer \(d 1\) we denote \([d]=\{1,,d\}\). For a set \(A\) we denote by \((A)\) the uniform distribution over \(A\). We denote by \((,^{2})\) the normal distribution with mean \(\) and variance \(^{2}\), and by \((,)\) the multivariate normal distribution with mean \(\) and covariance matrix \(\). The identity matrix of size \(d\) is denoted by \(I_{d}\). We use standard asymptotic notation \(()\) and \(()\) to hide constant factors, and \(}(),()\) to hide logarithmic factors. We use \(\) for the logarithm with base \(2\) and \(\) for the natural logarithm.

In this work, we consider depth-\(2\) ReLU neural networks. The ReLU activation function is defined by \((z)=\{0,z\}\). Formally, a depth-\(2\) network \(_{}\) of width \(m\) is parameterized by \(=[_{1},,_{m},,]\) where \(_{i}^{d}\) for all \(i[m]\) and \(,^{m}\), and for every input \(^{d}\) we have

\[_{}()=_{j[m]}v_{j}(_{j} ^{}+b_{j})\;.\]

We sometimes view \(\) as the vector obtained by concatenating the vectors \(_{1},,_{m},,\). Thus, \(\|\|\) denotes the \(_{2}\) norm of the vector \(\). We note that in this work we train both layers of the ReLU network.

We denote \((;):=_{}()\). We say that a network is _homogeneous_ if there exists \(L>0\) such that for every \(>0\) and \(,\) we have \((;)=^{L}(;)\). Note that depth-\(2\) ReLU networks as defined above are homogeneous (with \(L=2\)).

We next define gradient flow and remind the reader of some recent results on the implicit bias of gradient flow in two-layer ReLU networks. Let \(=\{(_{i},y_{i})\}_{i=1}^{n}^{d} \{-1,1\}\) be a binary classification training dataset. Let \((;):^{d}\) be a neural network parameterized by \(\). For a loss function \(:\) the _empirical loss_ of \((;)\) on the dataset \(\) is

\[():=_{i=1}^{n}(y_{i}( ;_{i}))\;.\] (1)

We focus on the exponential loss \((q)=e^{-q}\) and the logistic loss \((q)=(1+e^{-q})\).

We consider gradient flow on the objective given in Eq. (1). This setting captures the behavior of gradient descent with an infinitesimally small step size. Let \((t)\) be the trajectory of gradient flow. Starting from an initial point \((0)\), the dynamics of \((t)\) is given by the differential equation \((t)}{dt}-^{}((t))\). Here, \(^{}\) denotes the _Clarke subdifferential_, which is a generalization of the derivative for non-differentiable functions.

We now remind the reader of a recent result concerning the implicit bias of gradient flow over the exponential and logistic losses for homogeneous neural networks. Note that since homogeneous networks satisfy \(((;))=( (;))\) for any \(>0\), the sign of the network output of homogeneous networks depends only on the direction of the parameters \(\). The following theorem provides a characterization of the implicit bias of gradient flow by showing that the trajectory of the weights \((t)\)_converge in direction_ to a first-order stationary point of a particular constrained optimization problem, where \(\)_converges in direction_ to \(}\) means \(_{t}(t)}{\|(t)\|}=}}{\|}\|}\). Note that since ReLU networks are non-smooth, the first-order stationarity conditions (i.e., the Karush-Kuhn-Tucker conditions, or KKT conditions for short) are defined using the Clarke subdifferential (see Lyu and Li  and Dutta et al.  for more details on the KKT conditions in non-smooth optimization problems).

**Theorem 2.1** (Paraphrased from Lyu and Li  and Ji and Telgarsky ).: _Let \((;)\) be a homogeneous ReLU neural network parameterized by \(\). Consider minimizing either the exponential or the logistic loss over a binary classification dataset \(\{(_{i},y_{i})\}_{i=1}^{n}\) using gradient flow. Assume that there exists time \(t_{0}\) such that \(((t_{0}))<\) (and thus \(y_{i}((t_{0});_{i})>0\) for every \(_{i}\)). Then, gradient flow converges in direction to a first-order stationary point (KKT point) of the following maximum margin problem in parameter space:_

\[_{}\|\|^{2} i[n]\;\;y_{i}(;_{i}) 1\;.\] (2)

_Moreover, \(((t)) 0\) and \(\|(t)\|\) as \(t\)._

Theorem 2.1 gives a characterization of the implicit bias of gradient flow with the exponential and the logistic loss for homogeneous ReLU networks. Note that the theorem makes no assumption on the initialization, training data, or number of parameters in the network; the only requirement is that the network is homogeneous and that at some time point in the gradient flow trajectory, the network is able to achieve small training loss. The theorem shows that although there are many ways to configure the network parameters to achieve small training loss (via overparameterization), gradient flow only converges (in direction) to networks which satisfy the KKT conditions of Problem (2). It is important to note that satisfaction of the KKT conditions is not sufficient for global optimality of the constrained optimization problem . We further note that if the training data are sampled i.i.d. from a distribution with label noise (e.g., a class-conditional Gaussian mixture model, or a distribution where labels \(y_{i}\) are flipped to \(-y_{i}\) with some nonzero probability), networks which have parameters that are feasible w.r.t. the constraints of Problem (2) have overfit to noise, and understanding the generalization behavior of even globally optimal solutions to Problem (2) in this setting is the subject of significant research .

Finally, we introduce the distributional setting that we consider. We consider a distribution \(_{}\) on \(^{d}\{-1,1\}\) that consists of \(k\) clusters with means \(^{(1)},,^{(k)}^{d}\) and covariance \(^{2}I_{d}\) (i.e., a Gaussian mixture model), such that the examples in the \(j\)-th cluster are labeled by \(y^{(j)}\{-1,1\}\). More formally, \((,y)_{}\) is generated as follows: we draw \(j([k])\) and \((^{(j)},^{2}I_{d})\), and set \(y=y^{(j)}\). We assume that there exist \(i,j[k]\) with \(y^{(i)} y^{(j)}\). Moreover, we assume the following:

**Assumption 2.2**.: _We have:_

* \(\|^{(j)}\|=\) _for all_ \(j[k]\)_._
* \(0< 1\)_._
* \(k(_{i j}|^{(i)},^{(j)}|+4 (d)+1)(d)+1}{10}\)_._

**Example 1**.: _Below we provide simple examples of settings that satisfy the assumption:_

* _Suppose that the cluster means satisfy_ \(|^{(i)},^{(j)}|=}()\) _for every_ \(i j\)_. This condition holds, e.g., if we choose each cluster mean i.i.d. from the uniform distribution on the sphere_ \(^{d-1}\) _(see, e.g., Vardi, Yehudai, and Shamir_ _[_20_, Lemma 3.1]__). Let_ \(=1\)_, namely, each cluster has a radius of roughly_ \(\)_. Then, the assumption can be satisfied by choosing_ \(k=}()\)_._
* _Suppose that the cluster means are exactly orthogonal (i.e.,_ \(^{(i)},^{(j)}=0\) _for all_ \(i j\)_), and_ \(=1/\)_. Then, the assumption can be satisfied by choosing_ \(k=}(d)\)_._
* _If the number of clusters is_ \(k=}(1)\)_, then the assumption may hold even where_ \(_{i j}|^{(i)},^{(j)}|=(d)\) _(for any_ \(0< 1\)_)._

A few remarks are in order. First, the assumption that \(\|^{(j)}\|\) is exactly \(\) is for convenience, and we note that it may be relaxed (to have all cluster means approximately of the same norm) without affecting our results significantly. Note that in the case where \(=1\), the radius of each cluster is roughly of the same magnitude as the cluster mean. Second, we assume for convenience that the noise (i.e., the deviation from the cluster's mean) is drawn from a Gaussian distribution with covariance matrix \(^{2}I_{d}\). However, we note that this assumption can be generalized to any distribution \(_{}\) such that for every unit vector \(\) the noise \(_{}\) satisfies w.h.p. that \(,=}(1)\) and \(\|\|=}()\). This property holds, e.g., for a \(d\)-dimensional Gaussian distribution \((0,)\), where \([]=d\) and \(\|\|_{2}=O(1)\) (see Frei et al. [19, Lemma 3.3]), and more generally for a class of sub-Gaussian distributions (see Hu et al. [20, Claim 3.1]). Third, note that the third part of Assumption 2.2 essentially requires that the number of clusters \(k\) cannot be too large and the correlations between cluster means cannot be too large. Finally, we remark that when \(k\) is small, our results may be extended to the case where \(>1\). For example, if \(k=}(1)\) and \(_{i j}|^{(i)},^{(j)}|= }()\), our generalization result (Theorem 3.1) can be extended to the case where \(=}(d^{1/8})\). We preferred to avoid handling \(>1\) in order to simplify the proofs.

Moreover, it is worth noting that Assumption 2.2 implies that the data is w.h.p. linearly separable (see Lemma 2.1 below, and a proof in Appendix B). However, in this work we consider learning using overparameterized ReLU networks, and it is not obvious a priori that gradient methods do not harmfully overfit in this case. Indeed, it has been shown that ReLU networks trained by gradient descent can interpolate training data and fail to generalize well in some distributional settings [Kou+23].

**Lemma 2.1**.: _Let \(=_{q[k]}y^{(q)}^{(q)}\). Then, with probability at least \(1-2d^{1-(d)/2}=1-o_{d}(1)\) over \((,y)_{}\), we have \(y=(^{})\)._

## 3 Generalization

In this section, we show that under our assumptions on the distribution \(_{}\), gradient flow does not harmfully overfit. Namely, even if the learned network is highly overparameterized, the implicit bias of gradient flow guarantees convergence to a solution that generalizes well. Moreover, we show that the sample complexity is optimal. The main result of this section is stated in the following theorem:

**Theorem 3.1**.: _Let \(,(0,1)\). Let \(=\{(_{i},y_{i})\}_{i=1}^{n}^{d} \{-1,1\}\) be a training set drawn i.i.d. from the distribution \(_{}\), where \(n k^{2}(d)\). Let \(_{}\) be a depth-\(2\) ReLU network such that \(=[_{1},,_{m},,]\) is a KKT point of Problem (2). Provided \(d\) is sufficiently large such that \(^{-1}d^{(d)-1}\) and \(n\{} e^{d/32},}{3}  d^{(d)/4}, d^{(d)/2}\},\) then with probability at least \(1-\) over \(\), we have_

\[_{(,y)_{}}[y_{ {}}() 0]\;.\]

The sample complexity requirement in Theorem 3.1 is \(n=(k)\). Essentially, it requires that the dataset \(\) will include at least one example from each cluster. Clearly, any learning algorithm cannot perform well on unseen clusters. Hence the sample complexity requirement in the theorem is tight (up to log factors).

The assumptions in Theorem 3.1 include upper bounds on \(^{-1}\) and \(n\). Note that the expressions in these upper bounds are super-polynomial in \(d\), and in particular if \(n,^{-1},^{-1}=(d)\), then these assumptions hold for a sufficiently large \(d\). Admittedly, enforcing an upper bound on the training dataset's size is uncommon in generalization results. However, if \(n\) is exponential in \(d\), it is not hard to see that there will be clusters which have both positive and negative examples within radius \(\) of the cluster center, essentially introducing a form of label noise to the problem. Since KKT points of Problem (2) interpolate the training data, this would imply that the network has interpolated training data with label noise--in other words, it has 'overfit' to noise. Understanding the generalization behavior of interpolating neural networks in the presence of label noise is a very technically challenging problem for which much is unknown, especially if one seeks to understand this by only relying upon the properties of KKT conditions for margin maximization. It is noteworthy that all existing non-vacuous generalization bounds for interpolating nonlinear neural networks in the presence of label noise require \(n<d\).

Combining Theorem 3.1 with Theorem 2.1, we conclude that w.h.p. over a training dataset of size \(n k^{2}(d)\) (and under some additional mild requirements), if gradient flow reaches empirical loss smaller than \(\), then it converges in direction to a neural network that generalizes well. This result is width-independent, thus, it holds irrespective of the network width. Specifically, even if the network is highly overparameterized, the implicit bias of gradient flow prevents harmful overfitting. Moreover, the result does not depend directly on the initialization of gradient flow. That is, it holds whenever gradient flow reaches small empirical loss after some finite time. Thus, by relying on the KKT conditions of the max-margin problem instead of analyzing the full gradient flow trajectory, we can prove generalization without the need to prove convergence.

### Proof idea

The proof of Theorem 3.1 is given in Appendix A. Here we discuss the high-level approach. Let \(=[_{1},,_{m},,]\) be a KKT point of Problem (2). Thus, we have \(_{}()=_{j[m]}v_{j}(_{j}^{ }+b_{j})\). Since \(\) satisfies the KKT conditions of Problem (2), then there are \(_{1},,_{n}\) such that for every \(j[m]\) we have

\[_{j}=_{i[n]}_{i}_{_{j}}(y_{i} _{}(_{i}))=_{i[n]}_{i}y_{ i}v_{j}_{i,j}^{}_{i}\,\] (3)

where \(_{i,j}^{}\) is a subgradient of \(\) at \(_{j}^{}_{i}+b_{j}\), i.e., if \(_{j}^{}_{i}+b_{j} 0\) then \(_{i,j}^{}=[_{j}^{}_{i}+b_{j} 0]\), and otherwise \(_{i,j}^{}\) is some value in \(\). Also we have \(_{i} 0\) for all \(i\), and \(_{i}=0\) if \(y_{i}_{}(_{i}) 1\). Likewise, we have

\[b_{j}=_{i[n]}_{i}_{b_{j}}(y_{i}_{}(_{i}))=_{i[n]}_{i}y_{i}v_{j}_{i,j}^ {}\.\] (4)

In the proof, using a careful analysis of Eq. (3) and (4) we show that w.h.p. \(_{}\) classifies correctly a fresh example. More precisely, the main argument can be described as follows. We denote \(J:=[m]\), \(J_{+}:=\{j J:v_{j}>0\}\), and \(J_{-}:=\{j J:v_{j}<0\}\). Moreover, we denote \(I:=[n]\) and \(Q:=[k]\). For \(q Q\) we denote \(I^{(q)}=\{i I:_{i}q\}\). Consider the network's output for an input \(\) from cluster \(r Q\) with \(y^{(r)}=1\). Since \(_{}()=_{j J_{+}}v_{j}(_{ j}^{}+b_{j})+_{j J_{-}}v_{j}(_{j}^{} +b_{j})\) and \((z) z\), we have

\[_{}()_{j J_{+}}v_{j}(_{j }^{}+b_{j})+_{j J_{-}}v_{j}(_{j}^{} +b_{j}).\] (5)

This suggests the following possibility: if we can ensure that \(_{j J_{+}}v_{j}(_{j}^{}+b_{j})\) is large and positive while \(_{j J_{-}}v_{j}(_{j}^{}+b_{j})\) is not too negative, then the network will accurately classify the example \(\). Using Eq. (3) and (4) and that \(y^{(r)}=1\) (so \(y_{i}=1\) for \(i I^{(r)}\)), the first term in the above decomposition is equal to

\[_{j J_{+}}v_{j}(_{j}^{}+b_{j})= _{j J_{+}}v_{j}[_{i I}_{i}y_{i}v_{j}_{i,j}^{ }(_{i}^{}+1)]\] \[=_{j J_{+}}[(_{i I^{(r)}}_{i}v_{j} ^{2}_{i,j}^{}(_{i}^{}+1))+_{q Q \{r\}}_{i I^{(q)}}_{i}y_{i}v_{j}^{2}_{i,j}^{} (_{i}^{}+1)]\] \[(_{i I^{(r)}}_{j J_{+}}_{i}v_{j}^{ 2}_{i,j}^{}(_{i}^{}+1))-_{q Q \{r\}}_{i I^{(q)}}_{j J_{+}}_{i}v_{j}^{2}_{i,j}^{}|_{i}^{}+1|\.\]

Since \(\) comes from cluster \(r\) and the clusters are nearly orthogonal, the pairwise correlations \(_{i}^{}\) will be large and positive when \(i I^{(r)}\) but will be small in magnitude when \(i I^{(q)}\) for \(q r\). Thus, we can hope that this term will be large and positive if we can show that the quantity \(_{i I^{(r)}}_{j J^{+}}_{i}v_{j}^{2}_{i,j}^{}\) is not too small relative to the quantity \(_{q Q\{r\}}_{i I^{(q)}}_{j J_{+}}_{i}v_{j} ^{2}_{i,j}^{}\). By similar arguments, in order to show the second term in Eq. (5) is not too negative, we need to understand how the quantity \(_{i I^{(q)}}_{j J_{-}}_{i}v_{j}^{2}_{i,j}^{}\) varies across different clusters \(q Q\). Hence, in the proof we analyze how the quantities \(_{i I^{(q)}}_{j J_{+}}_{i}v_{j}^{2}_{i,j}^{}, _{i I^{(q)}}_{j J_{-}}_{i}v_{j}^{2}_{i,j}^{}\) relate to each other for different clusters \(q Q\), and show that these quantities are all of the same order. Then, we conclude that w.h.p. \(\) is classified correctly.

## 4 Robustness

We begin by introducing the definition of \(R()\)-robustness.

**Definition 4.1**.: _Given some function \(R()\), we say that a neural network \(_{}\) is \(R(d)\)-robust w.r.t. a distribution \(_{}\) over \(^{d}\) if for every \(r=o(R(d))\), with probability \(1-o_{d}(1)\) over \(_{}\), for every \(^{}^{d}\) with \(\|-^{}\| r\) we have \((_{}(^{}))= {sign}(_{}())\)._

Thus, a neural net \(_{}\) is \(R(d)\)-robust if changing the label of an example cannot be done with a perturbation of size \(o(R(d))\). Note that we consider here \(_{2}\) perturbations.

For the distribution \(_{}\) under consideration, it is straightforward to show that classifiers cannot be \(R(d)\)-robust if \(R(d)=()\): since the distance between examples in different clusters is w.h.p.

\(()\), it is clearly possible to flip the sign of an example with a perturbation of size \(()\). In particular, the best we can hope for is \(\)-robustness. In the following theorem, we show that there exist two-layer ReLU networks which can both achieve small test error and the optimal level of \(\)-robustness.

**Theorem 4.1**.: _For every \(r k\), there exists a depth-\(2\) ReLU network \(:^{d}\) of width \(r\) such that for \((,y)_{}\), with probability at least \(1-d^{-_{d}(1)}\) we have \(y() 1\), and flipping the sign of the output requires a perturbation of size larger than \(}{8}\) (for a sufficiently large \(d\)). Thus, \(\) classifies the data correctly w.h.p., and it is \(\)-robust w.r.t. \(_{}\)._

Thus, we see that \(\)-robust networks exist. In the following theorem, we show that the implicit bias of gradient flow constrains the level of robustness of _trained_ networks whenever the number of clusters \(k\) is large.

**Theorem 4.2**.: _Let \(,(0,1)\). Let \(=\{(_{i},y_{i})\}_{i=1}^{n}^{d} \{-1,1\}\) be a training set drawn i.i.d. from the distribution \(_{}\), where \(n k^{2}(d)\). We denote \(Q_{+}=\{q[k]:y^{(q)}=1\}\) and \(Q_{-}=\{q[k]:y^{(q)}=-1\}\), and assume that \(\{|}{k},|}{k}\} c\) for some \(c>0\). Let \(_{}\) be a depth-\(2\) ReLU network such that \(=[_{1},,_{m},,]\) is a KKT point of Problem (2). Provided \(d\) is sufficiently large such that \(^{-1}d^{(d)-1}\) and \(n\{}}{}e^{d/32},}}{}d^{(d)/4}, d^{(d)/2}\}\,\) with probability at least \(1-\) over \(\), there is a vector \(=_{j[k]}y^{(j)}^{(j)}\) with \(>0\) and \(\|\|}{{c^{2}k}}}\), such that_

\[_{(,y)_{}}[( _{}())( _{}(-y))] 1- \.\]

Note that the expressions in the upper bounds on \(n\) and \(^{-1}\) are super-polynomial in \(d\), and hence these requirements are mild (e.g., they hold for a sufficiently large \(d\) when \(n,^{-1},^{-1}=(d)\)). As we mentioned in the discussion following Theorem 3.1, we believe removing the requirement for an upper bound on \(n\) would be highly nontrivial.

Theorem 4.2 implies that if \(c^{2}k=_{d}(1)\), then w.h.p. over the training dataset, every KKT point of Problem (2) is not \(\)-robust. Specifically, if \(c\) is constant, namely, at least a constant fraction of the clusters have positive labels and a constant fraction of the clusters have negative labels, then the network is not \(\)-robust if \(k=_{d}(1)\). Recall that by Theorem 3.1, we also have w.h.p. that every KKT point generalizes well. Overall, combining Theorems 2.1, 3.1, 4.1, and 4.2, we conclude that for \(c^{2}k=_{d}(1)\), w.h.p. over a training dataset of size \(n k^{2}(d)\), if gradient flow reaches empirical loss smaller than \(\), then it converges in direction to a neural network that generalizes well but is not \(\)-robust, even though there exist \(\)-robust networks that generalize well. Thus, in our setting, there is bias towards solutions that generalize well but are non-robust.

**Example 2**.: _Consider the setting from the first item of Example 1. Thus, the cluster means satisfy \(|^{(i)},^{(j)}|=}()\) for every \(i j\), and we have \(=1\) and \(k=()\). Suppose that \(c=(1)\), namely, there is at least a constant fraction of clusters with each label \(\{-1,1\}\). Then, the adversarial perturbation \(\) from Theorem 4.2 satisfies \(\|\|=(}}{{k}})=}(d^{1/4})=o().\)_

Similarly to our discussion after Theorem 3.1, we note that Theorem 4.2 is width-independent, i.e., it holds irrespective of the network width. It implies that we cannot hope to obtain a robust solution by choosing an appropriate width for the trained network. As we discussed in the related work section, Bubeck, Li, and Nagaraj  and Bubeck and Sellke  considered the expressive power of neural networks, and showed that overparameterization might be necessary for robustness. By Theorem 4.2, even when the network is overparameterized, the implicit bias of the optimization method can prevent convergence to robust solutions. Moreover, our result does not depend directly on the initialization of gradient flow. Recall that by Theorem 2.1 if gradient flow reaches small empirical loss then it converges in direction to a KKT point of Problem (2). Hence our result holds whenever gradient flow reaches a small empirical loss.

Note that in Theorem 4.2, the adversarial perturbation does not depend on the input (up to sign). It corresponds to the well-known empirical phenomenon of _universal adversarial perturbations_, where one can find a single perturbation that simultaneously flips the label of many inputs (cf. ).

Zha+21]). Moreover, the same perturbation applies to all depth-\(2\) networks to which gradient flow might converge (i.e., all KKT points). It corresponds to the well-known empirical phenomenon of _transferability_ in adversarial examples, where one can find perturbations that simultaneously flip the labels of many different trained networks (cf. ).

It is worth noting that Theorems 3.1 and 4.2 demonstrate that trained neural networks exhibit different properties than the \(1\)-nearest-neighbour learning rule, irrespective of the number of parameters in the network. For example, consider the case where \(=}\), namely, the examples of each cluster are concentrated within a ball of radius \(O(1)\) around its mean. Then, the distance between every pair of points from the same cluster is \(O(1)\), and the distance between points from different clusters is \(()\). In this setting, both the \(1\)-nearest-neighbour classifier and the trained neural network will classify a fresh example correctly w.h.p., but in the \(1\)-nearest-neighbour classifier flipping the output's sign will require a perturbation of size \(()\), while in the neural network a much smaller perturbation will suffice.

Finally, we remark that in the limit \( 0\), we get a distribution supported on \(^{(1)},,^{(k)}\). Then, a training dataset of size \(n k^{2}(d)\) will contain w.h.p. all examples in the support, and hence robustness w.r.t. test data is equivalent to robustness w.r.t. the training data. In this case, we recover the results of Vardi, Yehudai, and Shamir  which characterized the non-robustness of KKT points of ReLU networks trained on nearly orthogonal training data. In particular, our Theorem 4.2 is a strict generalization of their Theorem 4.1.

### Proof ideas

Here we discuss the main ideas in the proofs of Theorem 4.1 and 4.2 (see Appendices C and D for the formal proofs).

The proof of Theorem 4.1 follows by the following simple construction. The robust network includes \(k\) neurons, each corresponding to a single cluster. That is, we have \(()=_{j=1}^{k}v_{j}(^{} +b_{j})\), where \(v_{j}=y^{(j)}\), \(_{j}=}{d}\), and \(b_{j}=-2\). Note that the \(j\)-th neuron points at the direction of the \(j\)-th cluster and has a negative bias term, such that the neuron is active on points from the \(j\)-th cluster, and inactive on points from the other clusters. Then, given a fresh example \((,y)_{}\), we show that the network classifies it correctly w.h.p. with margin at least \(1\). Also, there is w.h.p exactly one neuron that is active on \(\), and hence the gradient of the network w.r.t. the input is affected only by this neuron and is of size \((1/)\). Therefore, we need a perturbation of size \(()\) in order to flip the output's sign.

The intuition for Theorem 4.2 can be described as follows. Recall that in our construction of a robust network above, an example \((,y)_{}\) is w.h.p. in an active region of exactly one neuron, and hence in the neighborhood of \(\) the output of the network is sensitive only to perturbations in the direction of that neuron. Now, consider the linear model \(^{}\), where \(=_{q=1}^{k}y^{(q)}^{(q)}\). It is not hard to verify that for \((,y)_{}\) we have w.h.p. that \(0<y^{}(1)\). Moreover, the gradient of this linear predictor is of size \(\|\|=()\). Hence, we can flip the output's sign with a perturbation of size \(()\). Thus, the linear classifier is non-robust if \(k=_{d}(1)\). Intuitively, the difference between our robust ReLU network and the non-robust linear classifier is the fact that in the neighborhood of \(\) the robust network is sensitive only to perturbations in the direction of one cluster, while the linear classifier is sensitive to perturbations in the directions of all \(k\) clusters. In the proof, we analyze ReLU networks which are KKT points of Problem (2), and show that although these ReLU networks are non-linear, they are still sensitive to perturbations in the directions of all \(k\) clusters, similarly to the above linear classifier. The formal proof follows by a careful analysis of the KKT conditions of Problem (2), given in Eq. (3) and (4).

We remark that in the proof of Theorem 4.2 we use some technical ideas from Vardi, Yehudai, and Shamir . However, there are significant differences between the two settings. For example, they assume that the training data are nearly orthogonal, which only holds when the dimension is large relative to the number of samples; thus, it is unclear whether the existence of small adversarial perturbations in their setting is due to the high-dimensionality of the data or if a similar phenomenon exists in the more common \(n>d\) setting. At a more technical level, their proof relies on showing that in a KKT point all inputs must lie exactly on the margin, while in our setting they are not guaranteed to lie exactly on the margin.

## 5 Discussion

In this paper, we considered clustered data, and showed that gradient flow in two-layer ReLU networks does not harmfully overfit, but also hinders robustness. Our results follow by analyzing the KKT points of the max-margin problem in parameter space. In our distributional setting, the clusters are well-separated, and hence there exist robust classifiers, which allows us to consider the effect of the implicit bias of gradient flow on both generalization and robustness. Understanding generalization and robustness in additional data distributions and neural network architectures is a challenging but important question. As a possible next step, it would be interesting to study whether the approach used in this paper can be extended to the following data distributions:

First, our assumption on the data distribution (Assumption 2.2) implies that the number of clusters cannot be too large, and as a result the data is linearly separable (Lemma 2.1). We conjecture that our results hold even for a significantly larger number of clusters, such that the data is not linearly separable.

Second, it would be interesting to understand whether our generalization result holds for linearly separable data distributions that are not clustered. That is, given a distribution that is linearly separable with some margin \(>0\) and a training dataset that is large enough to allow learning with a max-margin linear classifier, are there KKT points of the max-margin problem for two-layer ReLU networks that do not generalize well? In other words, do ReLU networks that satisfy the KKT conditions generalize at least as well as max-margin linear classifiers?