# On the Necessity of Collaboration for

Online Model Selection with Decentralized Data

 Junfan Li\({}^{1}\) &Zheshun Wu\({}^{1}\) &Zenglin Xu\({}^{2,3,4}\)1 &Irwin King\({}^{5}\)

\({}^{1}\)School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen

\({}^{2}\)Pengcheng Lab

\({}^{3}\)Artificial Intelligence Innovation and Incubation (\(^{3}\)) Institute, Fudan University

\({}^{4}\)Shanghai Academy of AI for Science

\({}^{5}\)Department of Computer Science and Engineering, The Chinese University of Hong Kong

lijunfan@hit.edu.cn

wuzhsh23@gmail.com

zenglin@gmail.com

king@cse.cuhk.edu.hk

Corresponding author.

###### Abstract

We consider online model selection with decentralized data over \(M\) clients, and study the necessity of collaboration among clients. Previous work proposed various federated algorithms without demonstrating their necessity, while we answer the question from a novel perspective of computational constraints. We prove lower bounds on the regret, and propose a federated algorithm and analyze the upper bound. Our results show (i) collaboration is unnecessary in the absence of computational constraints on clients; (ii) collaboration is necessary if the computational cost on each client is limited to \(o(K)\), where \(K\) is the number of candidate hypothesis spaces. We clarify the unnecessary nature of collaboration in previous federated algorithms for distributed online multi-kernel learning, and improve the regret bounds at a smaller computational and communication cost. Our algorithm relies on three new techniques including an improved Bernstein's inequality for martingale, a federated online mirror descent framework, and decoupling model selection and prediction, which might be of independent interest.

## 1 Introduction

Model selection which is a fundamental problem for offline machine learning focuses on how to select a suitable hypothesis space for a machine learning algorithm (Mitchell, 1997; Bartlett et al., 2002; Mohri et al., 2018). Model selection for online machine learning is called online model selection (OMS), such as model selection for online supervised learning (Foster et al., 2017; Zhang and Liao, 2018; Zhang et al., 2021; Li and Liao, 2022), model selection for online active learning (Karimi et al., 2021; Li and Liao, 2022), and model selection for contextual bandits (Foster et al., 2019; Pacchiano et al., 2020; Ghosh and Chowdhury, 2022). We consider model selection for online supervised learning. Let \(=\{_{1},,_{K}\}\) contain \(K\) hypothesis spaces and \((,)\) be a loss function. For a sequence of examples \(\{(_{t},y_{t})\}_{t=1,,T}\), we aim to adapt to the case that the optimal hypothesis space \(_{i^{*}}\) is given by an oracle and we run an online learning algorithm in \(_{i^{*}}\). OMS can be defined by minimizing the _regret_, i.e.,

\[_{f_{1},,f_{T}}(_{t=1}^{T}(f_{t}(_{t}),y_{t}) -_{f_{i^{*}}}_{t=1}^{T}(f(_{t}),y_{t}) ),\]where \(f_{t}_{i=1}^{K}_{i}\) is the hypothesis used by an OMS algorithm at the \(t\)-th round. The optimal value of the regret depends on the complexity of \(_{i^{*}}\)(Foster et al., 2017, 2019).

In this work, we consider online model selection with decentralized data (OMS-DecD) over \(M\) clients, in which each client observes a sequence of examples \(\{(_{t}^{(j)},y_{t}^{(j)})\}_{t=1,...,T}\), \(j=1,,M\), and but does not share personalized data with others. There is a central server that coordinates the clients by sharing personalized models or gradients (Konecny et al., 2016; Kairouz et al., 2021; Zeng et al., 2023a,b). OMS-DecD captures some real-world applications where the data may be collected by sensors on \(M\) different remote devices or mobile phones (Li et al., 2020; Patel et al., 2023; Kwon et al., 2023), or a local device can not store all of data due to low storage and thus it is necessary to store the data on more local devices (Slavakis et al., 2014; Bouboulis et al., 2018). OMS-DecD can be defined by minimizing the following regret,

\[_{f_{t}^{(j)},t=1,...,T,j=1,...,M}(_{j=1}^{M}_{t=1}^{T} (f_{t}^{(j)}(_{t}^{(j)}),y_{t}^{(j)})-_{f _{i^{*}}}_{j=1}^{M}_{t=1}^{T}(f(_{t}^{(j)}),y_{t}^{(j)})),\]

where \(f_{t}^{(j)}_{i=1}^{K}_{i}\) is the hypothesis adopted by the \(j\)-th client at the \(t\)-th round. Solving OMS-DecD must achieve two goals: **G\(1\)** minimizing the regret, and **G\(2\)** providing privacy protection.

A trivial approach is to use a _noncooperative algorithm_ that independently runs a copy of an OMS algorithm on the \(M\) clients. It naturally provides strong privacy protection, that is, it achieves **G\(2\)**, but suffers a regret bound that increases linearly with \(M\). It is unknown whether it achieves **G\(1\)**. Another approach is _federated learning_ which is a framework of cooperative learning with privacy protection and is provably effective in stochastic convex optimization (McMahan et al., 2017; Woodworth et al., 2020; Wang et al., 2021; Reddi et al., 2021). It is natural to ask:

**Question 1**.: _Whether collaboration is effective for OMS-DecD._

The question reveals the hardness of OMS-DecD and is helpful to understand the limitations of federated learning. Previous work studied a special instance of OMS-DecD called distributed online multi-kernel learning (OMKL) where \(_{i}\) is a reproducing kernel Hilbert space (RKHS), and proposed three federated OMKL algorithms including vM-KOFL, eM-KOFL (Hong and Chae, 2022) and POF-MKL (Ghari and Shen, 2022). The three algorithms also suffer regret bounds that increase linearly with \(M\), and thus can not answer the question. If \(K=1\), then OMS-DecD is equivalent to distributed online learning (Mitra et al., 2021; Kwon et al., 2023; Patel et al., 2023). A noncooperative algorithm that independently runs online gradient descent (OGD) on each client achieves the two goals simultaneously (Patel et al., 2023). Collaboration is unnecessary in the case of \(K=1\).

In summary, previous work can not answer the question well. On one hand, previous work can not answer the question in the case of \(K>1\). On the other hand, in the case of \(K=1\), previous work has answered the question only using the statistical property of algorithms, i.e., the worst-case regret, but omitted the computational property which is very important for real-world applications.

### Main Results

In this paper, we will answer the question from a new perspective of computational constraints on the problem (Section 5.5). Our main results are as follows.

1. **An upper bound on the regret.** We propose a federated algorithm, FOMD-OMS, and prove an upper bound on the regret (Theorem 2). Besides, if \(_{1},...,_{K}\) are RKHSs, then our algorithm improves the regret bounds of FOP-MKL (Ghari and Shen, 2022) and eM-KOFL (Hong and Chae, 2022) at a smaller computational and communication cost. Table 1 summarizes the results.
2. **Lower bounds on the regret.** We separately prove a lower bound on the regret of any (possibly cooperative) algorithm and any noncooperative algorithm (Theorem 3).
3. **A new perspective of computational constraints for Question 1.** By the upper bound and lower bounds, we conclude that (i) collaboration is unnecessary when there are no computational constraints on clients, thereby generalizing the result for distributed online learning, i.e., \(K=1\); (ii) collaboration is necessary if the computational cost on each client is limited to \(o(K)\) where irrelevant parameters are omitted. Our results clarify the unnecessary nature of collaboration in previous federated algorithms for distributed OMKL.

### Technical Challenges

There are two main technical challenges on designing a federated online model selection algorithm.

The first challenge lies in obtaining high-probability regret bounds that adapt to the complexity of individual hypothesis space, a fundamental problem in online model selection (Foster et al., 2017). While acquiring expected regret bounds that adapt to the complexity of individual hypothesis spaces is straightforward, the crux is to derive high-probability bounds from expected bounds. To this end, we introduce a new Bernstein's inequality for martingale (Lemma 1), which might be of independent interest.

The second challenge involves achieving a per-round communication cost of \(o(K)\). To tackle this challenge, we propose two techniques: (i) decoupling model selection and prediction; (ii) an algorithmic framework, named FOMD-No-LU, which might be of independent interest. Specifically, when clients execute model selection, server must broadcast an aggregated probability distribution, denoted by \(^{K}\), to clients, naturally incurring a \(O(K)\) download cost. Our algorithm conducts model selection on server and makes predictions on clients, thereby eliminating the need to broadcast the aggregated probability distribution to clients. Additionally, if we use the local updating approach (Mitra et al., 2021; Patel et al., 2023), then server must broadcast \(K\) aggregated models to clients, also resulting in a \(O(K)\) download cost (Ghari and Shen, 2022). By utilizing FOMD-No-LU, our algorithm only broadcasts the selected models to clients and can achieve a \(o(K)\) download cost.

## 2 Related work

Previous work has studied the necessity of collaboration for distributed bandit convex optimization (Patel et al., 2023), where a federated algorithmic framework named FEDPOSGD was proposed. Although the regret bounds of FEDPOSGD are smaller than some noncooperative algorithms, there is not a lower bound on the regret of any noncooperative algorithm (Patel et al., 2023). Moreover, the regret analysis of FEDPOSGD is based on the analysis for federated online gradient descent that is not applicable to our algorithm, FOMD-OMS. The regret analysis of FOMD-OMS requires the analysis for federated online mirror descent with negative entropy regularizer.

Our work is also different from federated bandits, such as federated \(K\)-armed bandits (Wang et al., 2020) and federated linear contextual bandits (Huang et al., 2021). For OMS-DecD, we do not assume that the examples \((_{t}^{(j)},y_{t}^{(j)})\), \(t=1,,T\), on each client are independent and identically distributed (i.i.d.). In contrast, in both federated \(K\)-armed bandits or federated linear contextual bandits, the rewards must be i.i.d., thereby making collaboration effective. This is similar to the approach used in federated stochastic optimization. However, this may not hold true for OMS-Ded. Therefore, it is a distinctive problem for OMS-DecD to study whether collaboration is effective.

## 3 Problem Setting

**Notations** Let \(=\{^{d}|\|\|_{2}<\}\) be an instance space, \(=\{y:|y|<\}\) be an output space, and \(\{(_{t},y_{t})\}_{t[T]}\) be a sequence of examples, where \([T]=\{1,,T\}\), \(_{t}\) and \(y_{t}\). Let \(S=\{s_{1},s_{2},\}\) be a finite set, \((S)\) be the uniform distribution over the elements in \(S\) and \(s_{[T]}\) be the abbreviation of the sequence \(s_{1},s_{2},,s_{T}\). Denote by \([A]\) the probability that an event \(A\) occurs, \(a b=\{a,b\}\), \(a b=\{a,b\}\) and \((a)=_{2}(a)\). Let \(_{t}():,t[T]\) be a sequence of time-variant strongly convex regularizers defined on a domain \(\). The Bregman divergence denoted by \(_{_{t}}(,)\), associated with \(_{t}()\) is defined by

\[,,_{_{t}}(, )=_{t}()-_{t}()-_{t}( ),-.\]

### Online Model Selection (OMS)

Let \(=\{_{1},...,_{K}\}\) contain \(K\) hypothesis spaces where

\[_{i}=\{f()=^{}_{i}(): _{i}()^{d_{i}},\|\|_{2} U_{i}\}.\] (1)Let \(_{i^{*}}\) be the optimal but unknown hypothesis space for a given \(\{(_{t},y_{t})\}_{t[T]}\). OMS can be defined as follows: generating a sequence of hypotheses \(f_{[T]}\) that minimizes the following _regret_,

\[ i[K],(_{i})=_{t=1}^{T}(f_{t}( _{t}),y_{t})-_{f_{i}}_{t=1}^{T}(f(_{t}),y_{t}),\]

where \(f_{t}_{i=1}^{K}_{i}\). The optimal hypothesis space \(_{i^{*}}\) must contain a good hypothesis and has a low complexity (Foster et al., 2017, 2019), and is defined by

\[_{i^{*}}=*{arg\,min}_{_{i}} [_{f_{i}}_{t=1}^{T}(f(_{t}),y_{t})+ (_{i}})],\]

where \(_{i}\) measures the complexity of \(_{i}\), such as \(U_{i}\) and \(d_{i}\).

OMS is more challenge than online learning, since we not only learn the optimal hypothesis space, but also learn the optimal hypothesis in the space. Next we give some examples of OMS.

**Example 1** (Online Hyper-parameters Tuning).: _Let \(_{i}\) consist of linear functions of the form_

\[_{i}=\{f()=,,\|\|_{2} U_{i}\},\]

_where \(U_{i}>0\) is a regularization parameter. Let \(=\{U_{i},i[K]:U_{1}<U_{2}<<U_{K}\}\). The hypothesis spaces are nested, i.e., \(_{1}_{2}_{K}\). The optimal regularization parameter \(U_{i^{*}}\) corresponds to the optimal hypothesis space \(_{i^{*}}\)._

**Example 2** (Online Kernel Selection (Shen et al., 2019; Li and Liao, 2022)).: _Let \(_{i}(,):^{d}^{d}\) be a positive semidefinite kernel function, and \(_{i}:^{d}^{d_{i}}\) be the associated feature mapping. \(_{i}\) is the RKHS associated with \(_{i}\), i.e.,_

\[_{i}=\{f()=,_{i}():\|\|_{2} U_{i}\}.\]

_The optimal kernel function \(_{i^{*}}\{_{1},,_{K}\}\) corresponds to the optimal RKHS \(_{i^{*}}\)._

**Example 3** (Online Pre-trained Classifier Selection (Karimi et al., 2021)).: _Generally, \(_{i}\) can be a well-trained machine learning model. Let \(\) contain \(K\) pre-trained classifiers. For a new instance \(_{t}\), we select a (combinational) pre-trained classifier and make a prediction. The selection of a pre-trained classifier has an important implication in practical scenarios._

### Online Model Selection with Decentralized Data (OMS-DecD)

We formally define OMS-DecD as follows. Assuming that there are \(M\) clients and a server. At any round \(t\), each client observes an instance \(_{t}^{(j)}\), and selects a hypothesis \(f_{t}^{(j)}_{i=1}^{K}_{i}\), \(j[M]\). Then clients output predictions \(\{f_{t}^{(j)}(_{t}^{(j)})\}_{j=1}^{M}\). The goal is to minimize the following regret

\[ i[K],_{D}(_{i})=_{t=1}^{T}_{j= 1}^{M}(f_{t}^{(j)}(_{t}^{(j)}),y_{t}^{(j)})-_{f _{i}}_{t=1}^{T}_{j=1}^{M}(f(_{t}^{(j )}),y_{t}^{(j)}),\]

where \(y_{t}^{(j)}\) is the label or true output. Each client can not share personalized data with others, but can share personalized models or gradients via the central server.

## 4 Fomd-No-Lu

In this section, we propose a federated algorithmic framework, FOMD-No-LU (Federated Online Mirror Descent without Local Updating) for online collaboration.

Let \(\) be a convex and bounded decision set. At any round \(t\), each client \(j[M]\) selects a decision \(_{t}^{(j)}\), and then observes a loss function \(l_{t}^{(j)}():\). The client computes the loss \(l_{t}^{(j)}(_{t}^{(j)})\) and an estimator of the gradient denoted by \(_{t}^{(j)}\) (or the gradient denoted by \(g_{t}^{(j)}\)). To reduce the communication cost, we adopt the intermittent communication (IC) protocol (Woodworth et al., 2021) where clients communicate with server every \(N\) rounds. Assuming that \(T=N R\) where \(N,R\), the IC protocol limits the rounds of communication to \(R\).

We divide \([T]\) into \(R\) disjoint sub-intervals denoted by \(\{T_{r}\}_{r=1}^{R}\), in which

\[T_{r}=\{(r-1)N+1,(r-1)N+2,,rN\}.\] (2)

For any \(t T_{r}\), all clients always select the initial decision, i.e.,

\[ j[M],\  t T_{r},_{t}^{(j)}=_{(r-1)N+ 1}^{(j)}.\] (3)

At the end of the \(rN\)-round, all of clients send \(_{t T_{r}}_{t}^{(j)}\), \(j[M]\) to server. Then server updates the decision within online mirror descent framework (Bubeck and Cesa-Bianchi, 2012),

\[_{t} =_{j=1}^{M}(_{t T_{r}} {g}_{t}^{(j)}),\] (4) \[_{}_{t+1}}_{t}(}_{t+1}) =_{_{t}}_{t}(_{t})-_{t},\] (5) \[_{t+1} =*{arg\,min}_{}_{ _{t}}(,}_{t+1}).\] (6)

(4)-(5) is called model averaging (McMahan et al., 2017) and shows the collaboration among clients. Finally, server may broadcast \(_{t+1}\) to all clients. Let the initial decision \(_{1}^{(j)}=_{1}\) for all \(j[M]\), then it must be \(_{t}^{(j)}=_{t}\) for all \(t[T]\). Thus clients do not transmit \(_{t}^{(j)}\) to server. The pseudo-code of FOMD-No-LU is shown in Algorithm 1.

### Regret Bound

**Theorem 1**.: _Let \(=\{N,2N,....,RN\}\) where \(N=\) and \(R[T]\). Assuming that \(l_{t}^{(j)}()\), \(t[T],j[M]\), are convex loss functions. Let \(g_{t}^{(j)}=_{_{t}^{(j)}}l_{t}^{(j)}(_{t}^{(j)})\) and \(_{t}^{(j)}\) be an estimator of \(g_{t}^{(j)}\). At any round \(t\), let \(_{t+1}\) and \(_{t+1}\) be two auxiliary decisions defined as follows,_

\[_{_{t+1}}_{t}(_{t+1})=_{ _{t}}_{t}(_{t})-2_{j=1}^{M}_{t}^{(j )}-g_{t}^{(j)}}{M},_{_{t+1}}_{t}(_{t+1})= _{_{t}}_{t}(_{t})-_{j=1}^{M}g_{t }^{(j)}.\]

_Then FOMD-No-LU guarantees that,_

\[,_{t=1}^{T}_{j=1}^{M} {l_{t}^{(j)}(_{t}^{(j)})-l_{t}^{(j)}()}{NM} }[_{_{t} }(,_{t})-_{_{t}}(,_{t+ 1})+_{_{t}}(_{t},_{t+1})}{2}]} _{_{1}}+\] \[}_{_{t}}( _{t},_{t+1})}{2}+_{t}_{j =1}^{M}_{t}^{(j)}-g_{t}^{(j)},_{t}- }_{_{2}}.\]

It is intriguing that the regret bound comprises two components: the first part, \(_{1}\), cannot be reduced by collaboration, while the second part, \(_{2}\), highlights the benefits of collaboration. \(_{1}\) is the regret induced by exact gradients, while \(_{2}\) is the regret induced by estimated gradients and shows how collaboration controls the regret. It is worth mentioning that Theorem 1 gives a general regret bound, from which various types of regret bounds can be readily derived by instantiating the decision set \(\) and the regularizer \(_{t}()\). For instance, if \(=_{i}\) where \(_{i}\) follows Example 1, \(_{t}()=\|\|_{2}^{2}\) and \([\|_{t}^{(j)}\|_{2}^{2}] C\|g_{t}^{(j)}\|_{2}^{2}\), then FOMD-No-LU becomes a federated online descent. It is easy to give a \(O(MU_{i})T})\) expected regret from Theorem 1. Besides, \(N>1\) increases the regret and shows the trade-off between communication cost and regret bound.

Theorem 1 requires a novel analysis on how the bias of estimators, i.e., \(_{j=1}^{M}\|_{t}^{(j)}-g_{t}^{(j)}\|_{2}^{2}\), is controlled by cooperation. To this end, we introduce two virtual decisions \(_{t+1}\) and \(_{t+1}\) that are updated by \(2_{j=1}^{M}_{t}^{(j)}-g_{t}^{(j)}}{M}\) and \(2_{j=1}^{M}^{(j)}}{M}\), respectively. Previous federated online mirror descent uses exact gradients \(g_{t}^{(j)},j[M]\)(Mitra et al., 2021). Thus its analysis is different from ours.

```
0:\(\).
0:\(_{t}^{(j)},j[M]\)
1:for\(r=1,2,,R\)do
2:for\(t=(r-1)N+1,,rN\)do
3:for\(j=1,,M\)in paralleldo
4:Client selects \(_{(r-1)N+1}^{(j)}\)
5:Client observes loss function \(l_{t}^{(j)}()\)
6:Client computes estimated gradient \(_{t}^{(j)}\)
7:endfor
8:if\(t=rN\)then
9:Clements transmit \(_{t T}_{t}^{(j)}\), \(j[M]\)
10:Server computes \(_{t+1}\) following (4)-(6)
11:Server may broadcast \(_{t+1}\)
12:endif
13:endfor
14:endfor ```

**Algorithm 1** FOMD-No-LU

### Comparison with Previous Work

In fact, FOMD-No-LU adopts the batching technique (Dekel et al., 2011), that is, it divides \([T]\) into \(R\) sub-intervals and executes (3) during each sub-intervals. The batching technique (also known as mini-batch) has been used in the multi-armed bandit problem (Arora et al., 2012) and distributed stochastic convex optimization (Karimireddy et al., 2020; Woodworth et al., 2020). We use the batching technique for the first time to distributed online learning.

FOMD-No-LU is different from FedOMD (federated online mirror descent) (Mitra et al., 2021). (i) FedOMD only transmits exact gradients, while FOMD-No-LU can transmit estimated gradients. Thus the regret bound of FedOMD did not contain \(_{2}\) in Theorem 1. (ii) FedOMD uses local updating, such as local OGD (Patel et al., 2023) and local SGD (McMahan et al., 2017; Reddi et al., 2021). Thus FedOMD induces client drift, i.e., \(_{t}^{(j)}_{t}\). Besides, if we use FedOMD to design a federated online model selection algorithm, then the download cost is in \(O(MK)\) bits.

## 5 A Federated Algorithm for OMS-DecD

In this section, we just consider the case \(R=T\), that is, there is no communication constraints. Due to space constraints, we have deferred the algorithm and result of \(R<T\) to the appendix.

At a high level, our algorithm comprises two components both of which are critical for achieving a communication cost in \(o(K)\): (i) decoupling model selection and online prediction; (ii) collaboratively updating decisions within the framework of FOMD-No-LU.

### Decoupling Model Selection and Prediction

**Model Selection on Server** At any round \(t\), server maintains \(K\) hypotheses \(\{f_{t,i}^{(j)}_{i}\}_{i=1}^{K}\) and a probability distribution \(_{t}^{(j)}\) over the \(K\) hypotheses for all \(j[M]\). The model selection process aims to select a hypothesis from \(\{f_{t,i}^{(j)}\}_{i=1}^{K}\) and then predicts the output of \(_{t}^{(j)}\). An intuitive idea is that, for each \(j[M]\), the client samples a hypothesis following \(_{t}^{(j)}\). However, such an approach requires that server broadcasts \(_{t}^{(j)}\) to clients, and will cause a download cost in \(O(K)\).

The sampling operation (or model selection process) can be executed on server. Specifically, server just broadcasts the selected hypotheses, and thus saves the communication cost. For each \(j[M]\), server selects \(J[2,K]\) hypotheses denoted by \(f_{t,A_{t,a}}^{(j)},a[J]\) where \(A_{t,a}[K]\). For simplicity, let \(O_{t}^{(j)}=\{A_{t,1},,A_{t,J}\}\). We instantiate \(_{t}=_{t}\) in FOMD-No-LU. Then FOMD-No-LU ensures\(_{t}^{(j)}=_{t}\) for all \(j[M]\). We sample \(A_{t,1},,A_{t,J}\) in order and follow (7).

\[\{ & A_{t,1}_{t},\\ & A_{t,a}([K]\{A_{t,1},,A_{t,a-1} \}),\ a[2,J]..\] (7)

Server samples \(O_{t}^{(j)}\) for all \(j[M]\) and thus must independently execute (7) \(M\) times which only pays an additional computational cost in \(O(M K)\). The factor \( K\) arises from the process of sampling a number from \(\{1,...,K\}\). Server only sends \(f_{A_{t,a}}^{(j)}\), \(a[J]\) to the \(j\)-th client. It is worth mentioning that server does not send \(_{t}\). The total download cost is \(O(_{j=1}^{M}_{a=1}^{J}(d_{A_{t,a}}+ K))\). If \(J\) is independent of \(K\), then the download cost is only \(O(M K)\).

**Prediction on Clients** For each \(j[M]\), the \(j\)-th client receives \(f_{A_{t,a}}^{(j)}\), \(a[J]\), and uses \(f_{t,A_{t,1}}^{(j)}\) to output a prediction, i.e.,

\[_{t}^{(j)}=f_{t,A_{t,1}}^{(j)}(_{t}^{(j)})=< _{t,A_{t,1}}^{(j)},_{A_{t,1}}(_{t}^{(j)})>,\]

where we assume that \(f_{t,i}^{(j)}\) is parameterized by \(_{t,i}^{(j)}^{d_{i}}\) (see (1)). After observing the true output \(y_{t}^{(j)}\), the client suffers a loss \((f_{t,A_{t,1}}^{(j)}(_{t}^{(j)}),y_{t}^{(j)})\).

It is worth mentioning that the other \(J-1\) hypotheses \(f_{t,A_{t,a}}^{(j)}\), \(a 2\) are just used to obtain more information on the loss function. We will explain more in the following subsection. Thus we do not cumulate the loss \((f_{t,A_{t,a}}^{(j)}(_{t}^{(j)}),y_{t}^{(j)})\), \(a 2\).

### Online Collaboration Updating

**Updating sampling probabilities** For each \(j[M]\), let \(_{t}^{(j)}=(c_{t,1}^{(j)},,c_{t,K}^{(j)})\) where \(c_{t,i}^{(j)}=(f_{t,i}^{(j)}(_{t}^{(j)}),y_{t}^{(j)})\) is the loss of \(f_{t,i}^{(j)}\), \(i[K]\). The \(j\)-th client will send \(c_{t,i}^{(j)}\), \(i O_{t}^{(j)}\), to server. Since \(c_{t,i}^{(j)}\), \(i O_{t}^{(j)}\) can not be observed, it is necessary to construct an estimated loss vector \(}_{t}^{(j)}=(_{t,1}^{(j)},,_{t,K}^{( j)})\) where \(_{t,i}^{(j)}=^{(j)}}{[i O_{t}^{(j)}]} _{i O_{t}^{(j)}}\), \(i[K]\). It is easy to prove that \(_{t}[_{t,i}^{(j)}]=c_{t,i}^{(j)}\) and \(_{t}[(_{t,i}^{(j)})^{2}](c_{t,i}^{(j)})^{2}\) where \(_{t}[]:=[|O_{[t-1]}^{(j)}]\). Thus sampling \(A_{t,a}\), \(a 2\) reduces the variance of the estimators which is equivalent to obtain more information on the true loss.

Server aggregates \(}_{t}^{(j)}\), \(j[M]\) and updates \(_{t}\) following (4)-(6). Let \(_{K}\) be the \((K-1)\)-dimensional simplex, \(=_{K}\) and \(_{t}^{(j)}=}_{t}^{(j)}\). Then the server executes (8).

\[\{ _{}_{t+1}}_{t}( }_{t+1})=&_{_{t}}_{t}( _{t})-_{j=1}^{M}}_{t}^{(j)},\\ _{t+1}=&*{arg\,min}_{_{K}}_{_{t}}(,}_{t+1}),\\ _{t}()=&_{i=1}^{K}}{ _{t}}p_{i} p_{i},.\] (8)

where \(_{t}()\) is the weighted negative entropy regularizer (Bubeck et al., 2017), \(_{t}>0\) is a time-variant learning rate and \(C_{i}>0\) satisfies that \(_{t,j}c_{t,i}^{(j)} C_{i}\). It is obvious that server does not broadcast \(_{t+1}\).

**Updating hypotheses** For each \(j[M]\) and \(i[K]\), let \(_{t,i}^{(j)}=_{_{t,i}^{(j)}}(<_{t,i}^{(j)},_{i}(_{t}^{(j)})>,y_{t}^{(j)})\). Since \(_{t,i}^{(j)},i O_{t}^{(j)}\) are unknown, it is necessary to construct an estimator of the gradient, denoted by \(_{t,i}^{(j)}=^{(j)}}{[i O_{t}^{(j )}]}_{i O_{t}^{(j)}}\) for all \(j[M],i[K]\). Clients send \(\{_{t,i}^{(j)},i O_{t}^{(j)}\},j[M]\) to server. Then server aggregates \(\{_{t,i}^{(j)},i[K]\}\), \(j[M]\) and updates the hypotheses following (4)-(6). For each \(i[K]\), let \(=_{i}\) and \(_{t}^{(j)}=_{t,i}^{(j)}\). Server executes (9).

\[\{ _{_{t+1,i}}_{t,i}( }_{t+1,i})=&_{_{t,i}}_{t,i}( _{t,i})-_{j=1}^{M}_{t,i}^{(j)}, i=1,...,K,\\ _{t+1,i}=&*{arg\,min}_{ _{i}}_{_{t,i}}(,}_{t+1,i}),\\ _{t,i}()=&}\| \|_{2}^{2},.\] (9)

where \(_{t,i}()\) is the Euclidean regularizer and \(_{t,i}\) is a time-variant learning rate.

We name this algorithm FOMD-OMS (FOMD-No-LU for OMS-DecD) and show it in Algorithm 2.

### Regret bounds

To obtain high-probability regret bounds that adapt to the complexity of individual hypothesis space, we establish a new Bernstein's inequality for martingale.

**Lemma 1**.: _Let \(X_{1},,X_{n}\) be a bounded martingale difference sequence w.r.t. the filtration \(=(_{k})_{1 k n}\) and with \(|X_{k}| a\). Let \(Z_{t}=_{k=1}^{t}X_{k}\) be the associated martingale. Denote the sum of the conditional variances by \(_{n}^{2}=_{k=1}^{n}[X_{k}^{2}|_{k-1} ] v,\) where \(v[0,B]\) is a random variable and \(B 2\) is a constant. Then for any constant \(a>0\), with probability at least \(1-2 B\),_

\[_{t=1,,n}Z_{t}<+ }+2}.\]

Note that \(v\) is a random variable in Lemma 1, while it is a constant in standard Bernstein's inequality for martingale (see Lemma A.8 (Cesa-Bianchi and Lugosi, 2006)). Lemma 1 is derived from the standard Bernstein's inequality along with the well-known peeling technique (Bartlett et al., 2005).

**Assumption 1**.: _For each \(i[K]\), there is a constant \(b_{i}\) such that \(\|_{i}()\|_{2} b_{i}\) where \(_{i}()\) is defined in (1)._

**Lemma 2**.: _Under Assumption 1, for each \(i[K]\), there are two constants \(C_{i}>0,G_{i}>0\) that depend on \(U_{i}\) or \(b_{i}\) such that \(_{t,j}c_{t,i}^{(j)} C_{i}\) and \(_{t,j}\|_{t,i}^{(j)}\|_{2} G_{i}\)._

**Theorem 2**.: _Let \(R=T\). Under Assumption 1, denote by \(A_{m}=*{argmin}_{i[K]}C_{i}\) and \(C=_{i[K]}C_{i}\). Assuming that \((,)\) is convex and \(K J 2\). Let \(g_{K,J}=\),_

\[ i A_{m},\;p_{1,i}= |}(1-}{})+} i A_{m},\;p_{1,i}=},\] \[ t[T],\;_{t}= }{2}{M })T}}},_{t,i}=}{2G_{i} }{M})(g_{K,J}^{2} t)}}.\]

_With probability at least \(1-(M(T)+(KT/M))\), the regret of FOMD-OMS satisfies:_

\[ i[K],_{D}(_{i})=O(MB_{i,1}}{M})T}+B_{i,2} g_{K,J}+B_{i,3} MT}),\]

_where \(B_{i,1}=U_{i}G_{i}+C_{i}\), \(B_{i,2}=MC+U_{i}G_{i}\) and \(B_{i,3}=U_{i}G_{i}+}\)._

Both \(C_{i}\) and \(G_{i}\) depend on \(U_{i}\) or \(b_{i}\) (see Lemma 2). Let \(_{i}=(U_{i}G_{i}+C_{i})\). Thus \(_{i}\) measures the complexity of \(_{i}\). Then our regret bound adapts to \(_{i}}\) where \(=_{i[K]}_{i}\), while previous regret bounds depend on \(\)(Ghari and Shen, 2022; Hong and Chae, 2022), that is, they can not adapt to the complexity of individual hypothesis space. If \(_{i^{}}\), then our regret bound is much better.

The regret bound in Theorem 2 is also called multi-scale regret bound (Bubeck et al., 2017). However, previous regret analysis can not yield a high-probability multi-scale bound. The reason is the lack of the new Bernstein's inequality for martingale (Lemma 1). If we use the new Freedman's inequality for martingale (Lee et al., 2020), then a high-probability bound can still be obtained, but is worse than the bound in Theorem 2 by a factor of order \(O(*{poly}( T))\).

### Time Complexity and Communication Complexity Analysis

For each \(j[M]\), the \(j\)-th client makes prediction and computes gradients in time \(O(_{i O_{t}^{(j)}}d_{i})\). Server samples \(O_{t}^{(j)},j[M]\), aggregates gradients and updates global models. The per-round time complexity on server is \(O(_{j=1}^{M}_{i O_{t}^{(j)}}d_{i}+_{i=1}^{K}d_{i}+JM K)\).

**Upload** At any round \(t[T]\), the \(j\)-th client transmits \(c_{t,i}^{(j)},_{t,i}^{(j)}\), \(i O_{t}^{(j)}\) and the corresponding indexes to server. It requires \(J(_{i O_{t}^{(j)}}d_{i}+1)\) floating-point numbers and \(J\) integers. If we use \(32\) bits to represent a float, and use \( K\) bits to represent an integer in \([K]\). Each client transmits \((32J(_{i O_{t}^{(j)}}d_{i}+1)+J K)\) bits to server.

**Download** Server broadcasts \(_{t,i}^{d_{i}},i O_{t}^{(j)}\) and the corresponding indexes to clients. The total download cost is \((32MJ(_{i O_{t}^{(j)}}d_{i}+1)+MJ K)\) bits.

### Answers to Question 1

Before discussing Question 1, we give two lower bounds on the regret.

**Theorem 3** (Lower Bounds).: _Assuming that \(5 K\{d,T\}\). For each \(i[K]\), let \(_{i}=\{f_{i}()=_{i}^{}\}\) and \(_{i}=[_{}f_{i}(),_{ }f_{i}()]\), where \(_{i}\) is the standard basis vector in \(^{d}\). Denote by \(\) the supremum over all examples._

_(i) There are no computational constraints on clients. Let \((v,y)=|v-y|\). The regret of any algorithm for OMS-DecD satisfies: \(_{T}_{i[K]}_{D}(_{i})  0.25M\);_

_(ii) The per-round time complexity on each client is limited to \(O(J)\). Let \((v,y)=1-v y\). The regret of any, possibly randomized, noncooperative algorithm with outputs in \(_{i[K]}_{i}\) for OMS-DecD satisfies: \([_{i[K]}_{D}(_{i})] 0.1M }\), where the expectation is taken over the randomization of algorithm._

The assumption that the outputs of any noncooperative algorithm belong to \(_{i=[K]}_{i}\) is natural, and can be removed in the case of \(J=1\). Next we define a noncooperative algorithm, NCO-OMS.

**Definition 1** (Nco-Oms).: _NCO-OMS independently samples \(O_{t}^{(j)}\) following (7) and executes_

\[ j[M],_{}_{t+1}}_{t}( {}_{t+1})= _{_{t}^{(j)}}_{t}(_{t}^{(j)} )-}_{t}^{(j)},_{t+1}^{(j)}= {arg\,min}_{_{K}}_{_{t}}(,}_{t+1}).\] \[_{}_{t+1,i}}_{t,i}(}_{t+1,i})= _{_{t,i}^{(j)}}_{t,i}(_{t,i}^{(j)} )-_{t,i}^{(j)},_{t+1,i}^{(j)}= {arg\,min}_{_{i}}_{_{t,i}}(, }_{t+1,i}),\]

_where the definitions of \(}_{t}^{(j)}\) and \(_{t,i}^{(j)}\) follow FOMD-OMS._

It is easy to prove the regret of NCO-OMS satisfies: with probability at least \(1-(M(KT))\),

\[ i[K],\ _{D}(_{i})=O(M(B_{i,1} )T}+B_{i,2}g_{K,J}+B_{i,3}T})),\]

where \(B_{i,1}=U_{i}G_{i}+C_{i}\), \(B_{i,2}=C+U_{i}G_{i}\) and \(B_{i,3}=U_{i}G_{i}+}\). We leave the pseudo-code of NCO-OMS and the corresponding regret analysis in appendix.

Next we discuss Question 1 by considering two cases.

**Case 1**: There are no computational constraints on clients. Collaboration is unnecessary.

Let \(J=(K)\) in FOMD-OMS and NCO-OMS. By Theorem 2, both FOMD-OMS and NCO-OMS enjoy a \(O(MU_{i}G_{i}+MC_{i})\) regret. By Theorem 3, FOMD-OMS and NCO-OMS are nearly optimal in terms of the dependence on \(M\) and \(T\). Thus collaboration is unnecessary.

**Case 2**: The per-round time complexity on each client is limited to \(o(K)\). Collaboration is necessary.

Let \(J=o(K)\) in FOMD-OMS and Theorem 3. By Theorem 2, FOMD-OMS enjoys a \(O(MB_{i,1}+B_{i,3}^{-1}})\) regret, which is smaller than the lower bound on the regret of any noncooperative algorithm (see Theorem 3). Thus collaboration is necessary.

## 6 Application to Distributed OMKL

We will apply the proposed FOMD-OMS to a special instance of OMS-DecD, known as distributed OMKL, in which \(_{i}\) is a RKHS. Then we contrast our results with those from earlier studies, highlighting the unnecessary nature of collaboration in prior federated algorithms.

**Theorem 4**.: _Let \(_{i}\) be a RHKS for all \(i[K]\) and \(R T\). With probability at least \(1-(TM(R)+T(KR/M))\), the regret of FOMD-OMS satisfies, \( i[K]\),_

\[_{D}(_{i})=(MB_{i,1}}{M}}}+Mg_{K,J}T}{R}+T}{ }}+G_{i}MT}{}),\]

_where the notation \(()\) hides polylogarithmic factor in \(^{-1}\) and \(D=d_{i}\) follows (1)._

We defer the algorithm in appendix. Let \(R=T\) and \(J=2\). We compare FOMD-OMS with eM-KOFL (Hong and Chae, 2022) and POF-MKL (Ghari and Shen, 2022). Table 1 gives the results.

We observe that FOMD-OMS significantly improves the computational complexity of eM-KOFL and POF-MKL (by a factor of \(O(K)\)) on each client. The per-round time complexity of the two algorithms is \(O(DK)\). Recalling the answer to Question 1 (see Section 5.5), collaboration in eM-KOFL and POF-MKL is unnecessary.

Next we compare the regret bound of the three algorithms. Recalling that \(_{i}\). The regret bounds of eM-KOFL and POF-MKL can not adapt to the complexity of individual hypothesis space. (i) The regret bound of FOMD-OMS is better than that of POF-MKL in relation to its dependence on \(M\) and \(_{i}\). (ii) In the case of \(K=O(}{_{i}}M K)\), the regret bound of FOMD-OMS is better than that of eM-KOFL. (iii) In the case of \(K=(}{_{i}}M K)\), the regret bound of FOMD-OMS is worse than that of eM-KOFL. If \(K\) is sufficiently large, the regret bound of eM-KOFL is better than that of FOMD-OMS by a factor of \(O()\).

## 7 Conclusion

In this paper, we have studied the necessity of collaboration for OMS-DecD from the perspective of computational constraints. We demonstrate that collaboration is unnecessary when there are no computational constrains on clients, while it becomes necessary if the time complexity on each client is limited to \(o(K)\). Our work clarifies the unnecessary nature of collaboration in previous algorithms for the first time, gives conditions under which collaboration is necessary, and provides inspirations for studying the problem from constraints beyond computational constrains.