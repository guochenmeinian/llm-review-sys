# DYAD: A Descriptive Yet Abjuring Density efficient approximation to linear neural network layers

Sarin Chandy Varun Gangal Yi Yang Gabriel Maggiotti

ASAPP Inc.

{schandy,vgangal,yyang,gmaggiotti}@asapp.com

Equal Contribution. Sarin proposed the dyad model, modified the models in the transformers library with the Dyad layer and wrote the formulation section. Varun proposed the evaluation frameworks, organized the experiments and led the paper writing

###### Abstract

We devise, implement and performance-asses Dyad, a layer which can serve as a faster and more memory-efficient approximate replacement for linear layers, (_nn.Linear()_ in Pytorch). These layers appear in common subcomponents, such as in the _ff_ module of Transformers. Dyad is based on a bespoke near-sparse matrix structure which approximates the dense "weight" matrix \(W\) that matrix-multiplies the input in the typical realization of such a layer, a.k.a Dense. Our alternative near-sparse matrix structure is decomposable to a sum of 2 matrices permutable to a block-sparse counterpart. These can be represented as 3D tensors, which in unison allow a faster execution of matrix multiplication with the mini-batched input matrix \(X\) compared to Dense\((O(rows(W) cols(W)) O())\). As the crux of our experiments, we pretrain both Dyad and Dense variants of 2 sizes of the OPT arch and 1 size of the Pythia arch, including at different token scales of the babyLM benchmark. We find Dyad to be competitive (\( 90\)%) of Dense performance on zero-shot (e.g. Blimp), few-shot (OpenLM) and finetuning (GLUE) benchmarks, while being \(\)7-15% faster to train on-GPU even at 125m scale, besides surfacing larger speedups at increasing scale and model width.

## 1 Introduction

Riding on the back of the already pivotal decade-long rise of GPU-driven deep learning , Transformers  in 2017 crescendoed the ambition, scale and task-generality of ML models. With cross-sequence in-training parallelizability and representation power through all-pair interactions, transformers disrupted NLP and its incumbent recurrent paradigm , but since became key components in other modalities such as CV . Pretrained models as base representations, limited then to CV, emerged via LLMs like BERT , T5  etc reaching SOTA across tasks with limited finetuning.

A natural consequence of a module's ubiquity is that even a small improvement to one of its aspect can have major impact on its application and research -- as seen by the recent impact of e.g., quantization . A result of this is that an inefficient component (attention) sees a barrage of research (e.g.hashing , softmax alternatives , FlashAttention  etc) until some other component emerges as a bottleneck. We believe this is the case with the dense linear layers in the Transformer's _ff_ module. Moreover, models have larger hidden dimension (4096 for Pythia, 8192 for Llama2), leading to quadratic rise in compute from _ff_ module linear layers. Thus inspired, we devise DYAD (_Descriptive Yet Abjuring Density_) -- an efficient linear layer approximation using block-sparsity.

## 2 Formulation

### Linear Layer

A Linear layer is the basic building block of all neural networks, represented in pytorch by nn.Linear(). It maps input \(X\) to output \(Y\) via a dense matrix multiplication with weight matrix \(W\), given by the equation \(Y=G_{Linear}(X)=WX+b\). Here, \(W\) is a matrix of shape \(f_{out} f_{in}\) where \(f_{out}\) and \(f_{in}\) represent the no. of output & input features. \(Y\), \(X\) and the bias \(b\) have shapes \(f_{out} n_{batch}\), \(f_{in} n_{batch}\) and \(f_{out} 1\). Frameworks like pytorch pose the shape of \(X\) and \(Y\) as \(n_{batch} f_{in}\) and \(n_{batch} f_{out}\) but here we adhere to the former convention.

### Dyad : Definition and Properties

We introduce a family of sparse layers named Dyad that can serve as an approximate replacement for the dense linear layer. Dyad has 3 variants called Dyad-IT, Dyad-OT and Dyad-DT. The initials stand for Input Transpose, Output Transpose and Double Transpose. They are named such because transpose operations on either the input or output enables to compute their outputs efficiently. We describe Dyad-IT here and will describe the other two in a later section. Dyad is a linear layer with a sparse weight matrix having shape shown in Fig 1. The output of this layer can be calculated using \(G_{Linear}\). However, this won't lead to any efficiency gain compared to the linear layer. We can split the Dyad matrix into 2 components as shown in Fig 1. These components share some non-zero elements but their sum's representational power would be identical to the Dyad matrix. We call the first component the _Block Diagonal Component_ (BlockDiag) and the second one the _Block Transposed Component_ (BlockTrans). The ability to split Dyad into 2 components is what inspires its name. A Dyad matrix can be defined using 3 parameters, \(n_{dyd}\), \(n_{in}\) and \(n_{out}\). \(n_{out}\)\(\)\(n_{in}\) is the size of each submatrix in BlockDiag and \(n_{dyd}\) represents the no. of submatrices in each component. Thus, all the figures for Dyad shown here have \(n_{dyd}=n_{in}=n_{out}=4\). With the 2 components of Dyad split up, we can write its layer output as in Eq 1.

\[Y=W_{1}X+W_{2}X+b\] (1)

Naively implementing this as in Eq 1, will be as expensive as its dense counterpart. To exploit the joint properties of sparsity and block structure in these 2 components, we need to transform \(W_{1}X\) and \(W_{2}X\) to an equivalent sequence of 3D tensor operands and operations.

Hereforth, we ease representing 3D tensors in our equations by overloading pytorch tensor operators.

#### 2.2.1 Efficient Computation of BlockDiag

Let \(Y_{1}=W_{1}X\) be the output of BlockDiag. From Fig 1, we can see that for any \(Y_{1}[i n_{out}:(i+1) n_{out},:]\) only depends on \(X[i n_{in}:(i+1) n_{in},:]\) where \(i[0,\!n_{dyd})\). This shows that each pair of \(Y_{1}[i n_{out}:(i+1) n_{out},:]\),\(X_{1}[i n_{in}:(i+1) n_{in},:]\) can be calculated individually using a matrix multiplication. The weights needed for this are \(W_{1}[i n_{out}:(i+1) n_{out},i n_{in}:(i+1) n_{in}]\). We can store the weights needed for all these pairs of outputs and inputs as a 3D tensor,\(W_{1}^{{}^{}}\) of shape (\(n_{dyd}\),\(n_{out}\),\(n_{in}\)) as per Eq 2.

\[W_{1}^{{}^{}}[i,j,k]=W_{1}[i*n_{out}+j,i*n_{in}+k]\] (2)

This is a factor of \(n_{dyd}\) times smaller when compared to \(W_{1}\) since it has the shape (\(n_{dyd} n_{out}\), \(n_{dyd} n_{in}\)). Thus, the whole output of the layer can be computed together with a single batched matrix multiplication as shown in Eq 4 after the input has been also converted to a 3D tensor as

Figure 1: Dyad Weight Matrix [L] vs its Components [R], BlockDiag & BlockTrans. Green is \( 0\).

[MISSING_PAGE_FAIL:3]

### Dyad implementation in pytorch

The Dyad layer can be written relatively efficiently with a few lines of code in native pytorch. Here we present a simple implementation of Dyad, more specifically the exemplary Dyad-IT. Note that, in the code, we use \(dim\) instead of \(n\) to denote dimension. We also note that this code has some overhead in terms of multiple kernel launches, sequential processing of the components and some copying that could be avoided. Even with all of this we are still able to observe significant speedups especially at higher model scales.

``` classDyad(torch.nn.Module): def__init__(self,shape,bias=True): super()__init__() self.dyad_dim,self.dim_in,self.dim_out=shape self.has_bias=bias k=1.0/float(np.sqrt(dim_in*dyad_dim)) self.wu=torch.nn.Parameter(torch.empty((dyad_dim,dim_out,dim_in))) torch.nn.init.uniform_(self.wu,-k,k) self.wl=torch.nn.Parameter(torch.empty((dyad_dim,dim_out,dim_in))) torch.nn.init.uniform_(self.wl,-k,k) ifself.has_bias: self.bias= torch.nn.Parameter(torch.empty((dyad_dim*dim_out,1))) torch.nn.init.uniform_(self.bias,-k,k) defforward(self,x): #The shape of xis(dyad_dim xdim_in, batch_size) x1=x.reshape(self.dyad_dim,self.dim_in,-1) #The shape of x1, which is a view of x, is now(dyad_dim, dim_in, batch_size) x2=x.reshape(self.dim_in,self.dyad_dim,-1).transpose(0,1) out = (self.wl.bmm(x1)+self.wu.bmm(x2)).reshape(self.dyad_dim*self.dim_out,-1) ifself.has_bias: out+=self.bias returnout ```

### Dyad Variants

In this section we will describe the other two variants of Dyad, Dyad-OT and Dyad-DT. Both of these variants can be split into two components. As in the case of Dyad-IT, the first component is a block diagonal matrix and the second component can be converted back into a block diagonal by means of transposes.

#### 2.4.1 Dyad-OT

The weight matrix and the two split components of Dyad-OT is shown in Fig 4. The first component can be calculated exactly the same way as in Dyad-IT. The output of the second component can be calculated as \(Y_{2}=W_{2}X\). Here, \(Y_{2}\) is the output of the second component, \(W_{2}\) is the weight matrix

Figure 4: Dyad Output Transposed

Figure 3: Illustrations of BlockTrans computation, in particular the Equation 9 stepand \(X\) is the activation. Similar to the case of Dyad-IT, we can see that if we permute the second component along the rows we can get back a block diagonal matrix. Let the permutation matrix which achieves this be \(P\). Since, we are permuting the rows here this permutation matrix needs to be pre multiplied i.e \(W_{2}^{P}=PW_{2}\) where \(W_{2}^{P}\) is the resultant block diagonal matrix. We can convert \(W_{2}X\) to use this form as shown below.

\[Y_{2} =(P^{T}P)W_{2}X\] (11) \[Y_{2} =P^{T}(PW_{2})X\] (12) \[Y_{2} =P^{T}W_{2}^{P}X\] (13)

Here, we can calculate \(W_{2}^{P}X\) similar to the first component and then the permutation by per-multiplying \(P^{T}\) can be achieved by transposing the output similar to how it was done for Dyad-IT. Thus, similar to Dyad-IT we will have a compute complexity of \(O(n_{dyd} n_{out} n_{in})\).

#### 2.4.2 Dyad-DT

Fig 5 shows the weight matrix and the components of Dyad-DT. The important thing to note is that the second component can be converted into a block diagonal matrix through a combination of transposing the cloumns as well as transposing the rows. So, in other words it's basically a combination of Dyad-IT and Dyad-OT. We have to transpose the input before we multiply by the block diagonal weight matrix and then we have to transpose the output to get the final output of the layer.

\[Y_{2} =(P_{2}^{T}P_{2})W_{2}(P_{1}P_{1}^{T})X\] (14) \[Y_{2} =P_{2}^{T}(P_{2}W_{2}P_{1})(P_{1}^{T}X)\] (15) \[Y_{2} =P_{2}^{T}W_{2}^{P}X^{{}^{}}\] (16)

The above equations show this. \(X^{{}^{}}=P_{1}^{T}X\) is the result of transposing the input while \(W_{2}^{P}\) is the equivalent block diagonal matrix obtained by permuting both the columns and rows (\(P_{2}W_{2}P_{1}\)). As in the case with the other two variants, this variant also achieves a complexity of \(O(n_{dyd} n_{out} n_{in})\).

As further food for thought, we present a sketch discussing some thoughts about the representational power of Dyad in Appendix SS5.4.

## 3 Experimental Setup: Architectures, Benchmarks and Metrics

### Choice of Pretraining Corpus

Since our experiments need multiple pretraining runs to create different pretrained variants of the same architectures, each with the linear layers of the _ff_ module replaced by our Dyad variants, in addition to the baseline Dense, it would be infeasible to pretrain manyfold on full corpora, especially for a new method that can show on-the-fly challenges. Since TinyStories , there has been an emerging class of lean pretraining corpora (others being ) carefully curated to forsake on superficial aspects of scale (e.g. internet-scale vocab), while being linguistically rich enough. They present a reasonable Golliocks choice, being small enough to pretrain many runs on, while being large enough to learn emergent LLMesque skills. Hence, we choose BabyLM , which comes in two scales - 100M and 10M tokens respectively. The authors also provide an easy-to-use and "hackable" setup, with repos that support a) pretraining b) evaluating on Blimp/GLUE.

### Models, Architecture, Hyperparameters & Compute

We seek a setting which allows direct comparison between Dense vs Dyad, with preferably simple loss function and minimally randomized training. We avoid encoder-only and encoder-decoder

Figure 5: Dyad Double Transposed

architectures for this reason. To compare with BabyLM baselines, we pick the sole decoder-only architecture they evaluate, i.e. OPT-125m , as the architecture to try our variants with. We lay greater emphasis on exhaustive experiments at 10M data scale, though we also perform a core subset at the 100M scale. To show generalization to higher architecture size, we also repeat some experiments with OPT 350-m. We also present promising results at 10M with Pythia 160-M in Appendix SS5.6.4. We refer to the pretrained Dense checkpoint shared from BabyLM as DenseExt, Dense being our replication of it keeping pretraining details same for Dyad. Dyad variants have \(n_{dyd}=4\) unless mentioned (\(-8\) i.e. \(n_{dyd}=8\)). All experiments are on 1 GPU. More compute details are noted in AppendixSS5.5

### Benchmarks & Metrics

**Zero-Shot:****Blimp** Benchmark of Linguistic Minimal Pairs (Blimp)  consists of pairs of grammatical-ungrammatical sentences grouped by 12 broad phenomena e.g. anaphora and noun-verb agreement. A good LLM ought to assign higher probability to the grammatical member.

**Few-Shot:****OpenLlm** The OpenLlm leaderboard  has become a prevalent way to benchmark LLMs based on 4 few-shot openbook MCQesque benchmarks. Internally, it uses LMEvalHarness , which we replicate to compute numbers for our models as well as BabyLm's pretrained checkpoints.

**Finetuned:****Glue+** General Lang. Understanding Eval (Glue) , is a set of 7 NLU tasks, each evaluated post-finetuning. Also, we compute results on WSC and BoolQ. We christen this Glue+.

**Training Time** We report both total and _FF_-only (time spent just on _ff_ modules) time per minibatch.

**Memory & Parameter Footprint** By storing the dense subset of \(W\) as 3D tensor form, Dyad has lesser space complexity. To gauge real space saved, we measure various notions of memory and parameter size:

i) **Non-Embedding Parameters:** As in Pythia , we report total Non-Embedding Parameters.

ii) **Model Checkpoint Size:** On-disk size of the model checkpoint.

iii) **In-Training GPU Memory Usage:** During training, models may use memory well beyond parameters, e.g. optimizer state, cached activations etc. In-Training GPU Memory Usage as a metric incorporates this.

### Results

#### 3.4.1 Dyad vs Dense with 10M tokens

Through Tables 2 and 3 (and Appendix Tables 6, 7 & 8), we see that Dyad variants are well competitive (\(\) 5%) of the best Dense baseline.

In addition, through Figures 7, and Tables 11, 4 and 5 (as well as Appendix Tables 9 and 10), we see that all Dyad variants can translate the better complexity to actual speedups. We see that the quantum of these speedups to be much higher for larger architecture sizes i.e. OPT-350m

#### 3.4.2 Promising Results With Pythia

The Pythia suite  of models by EleutherAI, trained based on a permissively licensed collected dataset named The Pile .

The results we get by pretraining Pythia on the 10M scale of BabyLM are shared in Table 3. We also see that, just as we did for OPT 125-m, the promised time complexity improvements translate into speedups considering both FF-only time (as we can see in Table 5) and overall time (as we can see in Table 4)

#### 3.4.3 *-Cat experiments

As can be seen in the forward() function of our implementation of Dyad-IT laid out in SS2.3, and as we note explicitly therewith ("_We also note that this code... has some overhead...sequential processing of the components_"), having to process the two components underlying our layer, i.e. BlockDiag and BlockTrans in separate steps does introduce an unnecessary overhead that did not exist for Dense. To mitigate this, one can conceptualize a slightly faster forward layer that first concatenates the two components to enable parallel processing, before adding them up. We refer to implementations which use this variation by the -Cat suffix, such as in Dyad-IT-Cat. We perform a pretraining run of this variant at 10M scale, and find that this is indeed faster as anticipated, while retaining near-identical performance. Specifically, the \(ff\)-only time per minibatch taken by Dyad-IT-Cat along with OPT-125m is 3.27 ms, rather than 3.90ms taken by the simple, no -Cat, Dyad-IT, which is about 16% faster. For OPT-350m, the fractional speedup goes up even more, with Dyad-IT-Cat taking 5.46 ms and with Dyad-IT taking 7.92ms, being 45% faster.

The gains seen by optimizing away even this small overhead point to the promise held by the opportunity to optimize other steps of this layer once matrix multiplication itself has been optimized [through using Dyad style layers].

#### 3.4.4 Profiling Experiments At Wider Architectural Scales

Since Dyad is primarily applied herein to _ff_ module, assessing its benefits at higher relative width would give us important additional insight on its salience and generalizability in terms of benefit.

To do this, we take the OPT-1.3B model's architecture but cap its depth down to 6 layers so that the model continue to fit within our computational constraints at levels of width all the way upto 4096.

#### 3.4.5 Testing Waters with Vision Applicability - MNIST Experiment(s)

Since the bulk of our experiments as well as intuitions and writing is in a large language model/NLP context, a natural question that may perplex a reader is if using a Dyad style linear layer rather than a Dense one holds promise in other modalities e.g. computer vision. To make a basic probe in this direction, we do experiments with the simple but foundational MNIST digit classification task , replacing linear layers with both their plain Dense and our Dyad-IT, again with \(n_{dyad}=4\). Furthermore, we also test the waters in terms of trying our approaches with diverse accelerators by performing these experiments directly on a Macbook CPU without using a GPU or MPS etc.

We find the properties of reasonable performance preservation and speedups in both ff and overall time carry over to this situation too. Specifically, we find Dyad-IT achieves 98.51% test accuracy vs the 98.43% achieved by Dense, while taking 3.76 seconds of _ff_-only time per minibatch compared to 4.85 seconds by Dense.

  Model & Forward Pass & Backward Pass & Total & Total speedup ratio \\  Dense & 1.458818136 & 2.843522568 & 4.302340703 & 1 \\  Dyad-IT & 1.037282137 & 2.864683089 & 3.901965226 & 1.102608674 \\  Dyad-OT & 1.005873492 & 2.833987413 & 3.839860905 & 1.12044181 \\  Dyad-DT & 1.048527787 & 2.955974824 & 4.004502611 & 1.074375802 \\  Dyad-IT-8 & 0.7726907735 & 1.836098994 & 2.608789767 & 1.649171105 \\  

Table 1: Mean time taken per minibatch by the ff transformer modules of OPT-125m training on account of forward, backward passes and in total. All times are in milliseconds. Speedup ratio is computed w.r.t. Dense

Figure 6: Dyad vs Dense Speedup At Different Model Widths of 6-Layer Capped OPT-like architecture.

## 4 Future Work

In the future, we aim to explore i) using a heterogeneous mix of Dyad variants to approximate different ff layers ii) Replicating our experiments other minified corpora such as Minipile .

  Model & Forward Pass & Backward Pass & Total & Total speedup ratio \\  Dense & 101.89 & 220.16 & 332.64 & 1 \\  Dyad-IT & 89.40 & 229.86 & 310.62 & 1.071 \\  

Table 4: Mean time taken per minibatch by all modules of Pythia-160m training on account of forward, backward passes and in total. Times are in milliseconds. Speedup ratio is computed w.r.t. Dense

  Benchmark & Task & Dense & Dense-Ext & Dyad-IT & Dyad-OT & Dyad-DT & Dyad-IT-8 \\    & GLUE+QA & 68.32 & 63.38 & 67.33 & 68.46 & 68.59 & 67.30 \\  & GLUE+QA & 66.37 & 63.67 & 66.27 & 66.27 & 63.69 & 64.02 \\  & GLUE+NLI & 68.27 & 59.78 & 65.64 & 68.27 & **68.67** & 67.65 \\    & Blimp & 59.16 & 60.31 & **60.47** & **62.55** & **60.36** & 58.88 \\    & OpenLim & 30.27 & 30.39 & **30.61** & **30.74** & **30.58** & **30.65** \\  

Table 2: Performance on GLUE+ (finembing), BLIMP (0-shot), OpenLim (few-shot) benchmarks for Dense baselines vs 3 Dyad variants with \(n_{dyd=4}=4\) and a sparser version of the 1st (Dyad-IT-8). Numbers which exceed Dense/Dense-Ext are bolded/underlined respectively. All Dyad variants are \( 0.95 max(,)\). We present aggregates for brevity and defer individual values to Appendix Table 2

  Benchmark & Task & Dense & Dras-IT & Dyad-OT & Dyad-DT & Dyad-IT-8 \\    & GLUE+QA & 68.38 & 67.33 & 68.46 & 68.59 & 67.30 \\  & GLUE+QA & 66.37 & 66.27 & 66.27 & 63.69 & 64.02 \\  & GLUE+NLI & 68.27 & 59.78 & 65.64 & 68.27 & **68.67** & 67.65 \\  

Table 3: Benchmark numbers for Pythia-160m pretrained at the 10M scale comparing Dense with Dyad-IT. Instances where Dyad-IT exceeds Dense are marked in bold, while instances where Dyad-IT falls below 0.95* Dense are marked in Red. Dyad-IT falls below the 0.95% mark w.r.t. Dense on only 3 zero-shot and 2 **GLUE+** tasks, falling above the mark on all GLUE+ aggregate tasks and OpenLim. We present aggregates for brevity and defer individual values to Appendix Table 7

Figure 8: Memory and parameter footprint of OPT-125m/OPT-350m training as per various static estimates on the left and dynamic GPU mem usage on the right.

  Model & Forward Pass & Backward Pass & Total & Total speedup ratio \\  Dense & 1.414 & 2.826 & 4.240 & 1 \\  Dyad-IT & 1.070 & 2.879 & 3.949 & 1.074 \\  Dyad-IT-8 & 0.795 & 1.843 & 2.637 & 1.607 \\  

Table 5: Mean time taken per minibatch by the \(ff\) (feedforward) modules of Pythia-160m training on account of forward, backward passes and in total. All times are in milliseconds. Speedup ratio is computed w.r.t. Dense

Figure 7: Mean training per minibatch by FF modules of OPT-125m/OPT-350m training spent on forward, backward passes and total (Times in ms). Dyad variants are faster, and \(\)\(n_{dydd}\) (Dyad-IT-8) improves this.

**Acknowledgements:** We thank Ryan McDonald and Nirmal Mukhi (ASAPP Inc.), the workshop organizers of the WANT and ESNLP workshops as well as anonymous reviewers for their helpful feedback.