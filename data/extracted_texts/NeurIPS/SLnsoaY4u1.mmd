# Provably Robust Score-Based Diffusion Posterior Sampling for Plug-and-Play Image Reconstruction

Xingyu Xu

Carnegie Mellon University

&Yuejie Chi

Carnegie Mellon University

xingyuxu@andrew.cmu.edu

###### Abstract

In a great number of applications, the goal is to infer an unknown image from a small number of noisy measurements collected from a known and possibly nonlinear forward model describing certain sensing or imaging modality, which is often ill-posed. Score-based diffusion models, thanks to their impressive empirical success, have emerged as an appealing candidate of an expressive prior in image reconstruction. In order to accommodate diverse tasks at once, it is of great interest to develop efficient, consistent and robust algorithms that incorporate _unconditional_ score functions of an image prior distribution in conjunction with flexible choices of forward models. This work develops an algorithmic framework for employing score-based diffusion models as an expressive data prior in nonlinear inverse problems with general forward models. Motivated by the plug-and-play framework in the imaging community, we introduce a diffusion plug-and-play method (DPnP) that alternatively calls two samplers, a proximal consistency sampler based solely on the likelihood function of the forward model, and a denoising diffusion sampler based solely on the score functions of the image prior. The key insight is that denoising under white Gaussian noise can be solved _rigorously_ via both stochastic (i.e., DDPM-type) and deterministic (i.e., DDIM-type) samplers using the same set of score functions trained for generation. We establish both asymptotic and non-asymptotic performance guarantees of DPnP, and provide numerical experiments to illustrate its promise in various tasks. To the best of our knowledge, DPnP is the first provably-robust posterior sampling method for nonlinear inverse problems using unconditional diffusion priors.

## 1 Introduction

In a great number of sensing and imaging applications, the paramount goal is to infer an unknown image \(x^{}^{d}\) from a collection of measurements \(y^{m}\) that are possibly noisy, incomplete, and even nonlinear. Examples include restoration tasks such as inpainting, super-resolution, denoising, as well as imaging tasks such as magnetic resonance imaging , optical imaging , microscopy imaging , radar and sonar imaging , and many more.

Due to sensing and resource constraints, the problem of image reconstruction is often ill-posed, where the desired resolution of the unknown image overwhelms the set of available observations. Consequently, this necessitates the need of incorporating prior information regarding the unknown image to assist the reconstruction process. Over the years, numerous types of prior information have been considered and adopted, from hand-crafted priors such as subspace or sparsity constraints , to data-driven ones prescribed in the form of neural networks . These priors can be regarded as some sort of generative models for the unknown image, which postulate the high-dimensional image admits certain parsimonious representation in a low-dimensional data manifold. It is desirable that the generative models are sufficiently expressive to capture thediversity and structure of the image class of interest, yet nonetheless, still lead to image reconstruction problems that are computationally tractable.

**Score-based diffusion models as an image prior.** Recent years have seen tremendous progress on generative artificial intelligence (AI), where it is possible to generate new data samples -- such as images, audio, text -- at unprecedented resolution and scale from a target distribution given training data. Diffusion models, originally proposed by , are among one of the most successful frameworks, underneath popular content generators such as DALL-E , Stable Diffusion , Imagen , and many others. Roughly speaking, score-based diffusion models convert noise into samples that resemble those from a target data distribution, by forming the reverse Markov diffusion process only using the score functions of the data contaminated at various noise levels . In particular,  developed a unified framework to interpret score-based diffusion models as reversing certain Stochastic Differential Equations (SDE) using either SDE or probability flow Ordinary Differential Equations (ODE), leading to stochastic (i.e., DDPM-type) and deterministic (i.e., DDIM-type) samplers, respectively. While the DDIM-type sampler is more amenable to acceleration, the DDPM-type sampler tends to generate images of higher quality and diversity when running for a large number of steps .

Thanks to the expressive power of score-based diffusion models in generating complex and fine-grained images, they have emerged as a plausible candidate of an expressive prior in image reconstruction  via the lens of _Bayesian posterior sampling_. To accommodate diverse applications with various image characteristics and imaging modalities, it is desirable to develop _plug-and-play_ methods that do not require training from scratch or end-to-end training for every new imaging task. Nonetheless, despite a flurry of recent efforts, existing algorithms either are computationally expensive , inconsistent , or confined to linear inverse problems . Therefore, a natural question arises:

_Can we develop a practical, consistent and robust algorithm that incorporates score-based diffusion models as an image prior with general (possibly nonlinear) forward models?_

### Our contribution

This paper provides an affirmative answer to this question, by developing an algorithmic framework to sample from the posterior distribution of images, where score-based diffusion models are employed as an expressive image prior in nonlinear inverse problems with general forward models. Specifically, our contributions are as follows.

* _Diffusion plug-and-play for posterior sampling._ Motivated by the plug-and-play  framework in the imaging community, we introduce a diffusion plug-and-play method (DPnP) that alternatively calls two samplers, a _proximal consistency sampler_ that aims to generate samples that are more consistent with the measurements, and a _denoising diffusion sampler_ that focuses on sampling from the posterior distribution of an easier problem -- image denoising under white Gaussian noise -- to enforce the prior constraint. Our method is _modular_, in the sense that the _proximal consistency sampler_ is solely based on the likelihood function of the forward model, and the denoising diffusion sampler is based solely on the score functions of the image prior.
* _Posterior sampling for image denoising._ While the proximal consistency sampler can be borrowed somewhat straightforwardly from existing literature such as the Metropolis-adjusted Langevin algorithm , the denoising diffusion sampler, on the other hand, has not been addressed in the literature to the best of our knowledge. Our key insight is that this can be solved via both stochastic (i.e., DDPM-type) or deterministic (i.e., DDIM-type) samplers by carefully choosing the forward SDEs and discretizing the resulting reversal SDE or ODE using the exponential integrator . Importantly, the denoising diffusion samplers use the same set of unconditional score functions for generation, making it readily implementable without additional training.

Figure 1: Solving linear and nonlinear inverse problems with Diffusion Plug-and-Play (DPnP).

* _Theoretical guarantees._ We establish both asymptotic and non-asymptotic performance guarantees of the proposed DPnP method. Asymptotically, we verify the correctness of our method by proving that DPnP converges to the conditional distribution of \(x^{}\) given measurements \(y\), assuming exact unconditional score estimates of the image prior. We next establish a non-asymptotic convergence theory of DPnP, where its performance degenerates gracefully with respect to the errors of the samplers, due to, e.g., score estimation errors and limited sampling steps. To the best of our knowledge, this provides the _first provably-robust_ method for _nonlinear_ inverse problems using unconditional score-based diffusion priors.

We further provide numerical experiments to illustrate its promise in solving both linear and nonlinear image reconstruction tasks, such as super-resolution, phase retrieval, and quantized sensing. Due to its plug-and-play nature, we expect it to be of broad interest to a wide variety of inverse problems.

**Related works.** Given its interdisciplinary nature, our work sits at the intersection of generative modeling, computational imaging, optimization and sampling. Due to space limits, we postpone the discussion of related works to Appendix A.

**Notation.** Let \(p_{x}\) denote the probability distribution of \(x\), and \(p_{x}(|y)\) denotes the conditional distribution of \(x\) given \(y\). We use \(X}}{{=}}Y\) to denote random variables \(X\) and \(Y\) are equivalent in distribution. The matrix \(I_{d}\) denotes an identity matrix of dimension \(d\). For two probability distributions with density \(p(x)\) and \(q(x)\), the total variation distance between them is \((p,q)|p(x)-q(x)|x\). The \(^{2}\)-divergence of \(p\) to \(q\) is \(^{2}(p\,\|\,q)}{q(x)}x\).

## 2 Score-based generative models

In this section, we set up the preliminary on diffusion-based generative models, which we will be relying upon to develop our algorithm. The key components consist of a _forward_ process, which diffuses the data distribution \(p^{}\) to the standard normal distribution by gradually injecting noise into the samples, and a _backward_ process, which reverses the forward process so that it can transform the standard normal distribution to the data distribution \(p^{}\). To facilitate understanding, it will be convenient to formulate these processes in continuous time. For discrete-time formulation and implementation, please refer to Appendix E.

### The forward process and score functions

The continuous-time forward diffusion follows the Ornstein-Uhlenbeck (OU) process, defined by the Stochastic Differential Equation (SDE) :

\[X_{}=-X_{}+\,B_{},  0, X_{0} p^{},\] (1)

where \((B_{})_{ 0}\) is the standard \(d\)-dimensional Brownian motion. It can be shown that  the marginal distribution of \(X_{}\) for \( 0\) is

\[X_{}}}{{=}}^{-}X_{0}+ ^{-2}}, X_{0} p^{},\; (0,I_{d}).\] (2)

It is then clear that the limiting distribution \(X_{}(0,I_{d})\) as \(\), i.e., the OU process diffuses \(X_{0} p^{}\) to the standard normal distribution. The score function of \(X_{}\) is defined by

\[s(,x)= p_{X_{}}(x).\] (3)

An enlightening property  of the score function is that it can be interpreted as the minimum mean-squared error (MMSE) estimate of \(_{t}\) given \(x_{t}=x\), fueled by Tweedie's formula:

\[s(,x)=-^{-2}}}_{X_{0 } p^{},\,(0,I_{d})}\,| \,^{-}X_{0}+^{-2}}=x}_{= (,x)}\] (4)

Consequently, this makes it possible to estimate the score functions via learning to denoise , by estimating the denoising function \((,)\), as typically done in practice .

### The reverse process and sampling

To enable sampling, one needs to "reverse" the forward diffusion process. Fortunately, it is possible to leverage classical theory  to reverse the SDE, and apply discretization to the time-reversal processes to collect samples. We shall describe two popular approaches below, corresponding to stochastic (i.e., DDPM-type) and deterministic (i.e., DDIM-type) samplers respectively following primarily the framework set forth in .

**Time-reversed SDEs and probability flow ODEs.** Let us begin with the more general theory of _reversing_ SDEs, which will be useful in future sections. Consider a SDE given by

\[M_{}= M_{}+B_{}, 0, M_{0} p_{M_{0}},\] (5)

where \(\) and \(>0\) are constants. For any positive time \(_{}>0\), define the reversed time parameter

\[^{}^{}()=_{}-.\] (6)

We are now ready to describe the time-reversed processes.

1. The _time-reversed SDE_ of (5) on the time interval \([0,_{}]\) is defined as \[M^{}_{^{}}=(- M^{}_{ ^{}}+ p_{M_{^{}}}(M^{}_{^{}}))\,+_{ },\ [0,_{}],\,M^{}_{_{}} p_{M_{_{ }}},\] (7) where \(\) is an independent copy of \(B\), i.e., another Brownian motion. It is a classical result  that the reversed process \(M^{}\) shares the same path distribution as \(M\), i.e., \((M^{}_{})_{[0,_{}]})}}{{=}}(M_{})_{[0,_{}]}\). In other words, the joint distribution of \((M^{}_{_{1}},M^{}_{_{2}},,M^{}_{_{k}})\) for any \(0_{1}_{2}_{k}_{}\), for any integer \(k 1\), coincides with that of \((M_{_{1}},M_{_{2}},,M_{_{k}})\).
2. In place of the reversed SDE in (7), it is possible to consider the following probability flow ODE [1, SSDK\({}^{+}\)21]: \[M^{}_{^{}}=(- M^{}_{^{}}+ p_{M_{^{}}}( ^{},M^{}_{^{}})) ,\ [0,_{}],\,M^{}_{_{}} p_{M_{_{ }}}.\] (8) The reversed ODE satisfies a slightly weaker guarantee than that of the reversed SDE, which nevertheless suffices for most practical purposes : \(M^{}_{})}}{{=}}M_{}, [0,_{}]\). Note that the reversed ODE only guarantees identical marginal distribution for each \(M^{}_{}\), whereas the reversed SDE guarantees identical joint distribution.

Specializing the above to the OU process (1) with proper discretization then leads to popular samplers used for generation, as follows.

**DDPM-type stochastic samplers.** Specializing the time-reversed SDE (7) to the OU process gives

\[X^{}_{^{}}=X^{}_{ ^{}}+2s(^{},X^{}_{^{}})+_{},[0, _{}], X^{}_{_{}} p_{X_{_{}}}.\]

As \(_{}\), it can be seen from (2) that \(p_{X_{_{}}}\) converges to \((0,I_{d})\). Thus the solution of the above SDE can be approximated by initializing \(X^{}_{_{}}(0,I_{d})\) instead. The DDDPM sampler  can be viewed as a discretization of this SDE .

**DDIM-type deterministic samplers.** On the other hand, the probability flow ODE (8) for the OU process reads as

\[X^{}_{^{}}=X^{}_{ ^{}}+s(^{},X^{}_{^{}}),[0,_{}], X^{}_{_{}} p_{X_{_{}}}.\] (9)

Again, as \(_{}\), one may approximate the initialization with \(X^{}_{_{}}(0,I_{d})\). It is known that the popular DDIM sampler  is a discretization of this ODE . The ODE-based deterministic samplers allow more aggressive choice of discretization schedules, as well as fast ODE solvers , enabling significantly accelerated sampling process compared to the SDE-based stochastic samplers.

## 3 Posterior sampling via diffusion plug-and-play

We are interested in solving (possibly nonlinear) inverse problems, where the aim is to infer an unknown image \(x^{}^{d}\) from its measurements \(y^{m}\),

\[y=(x^{})+,\]

where \(:^{d}^{m}\) is the measurement operator underneath the forward model, and \(\) denotes measurement noise. We focus on the Bayesian setting where the prior information of \(x^{}\) is provided in the form of some prior distribution \(p^{}()\), i.e.,

\[x^{} p^{}(x),\] (10)The _posterior distribution_ given measurements \(y\) is defined as

\[p^{}(x|y) p^{}(x)\,p(y|x^{}=x)=p^{}(x)\,^{ (x;y)}.\] (11)

Here, \((;y)\) is the log-likelihood function of the measurements. Notwithstanding, our framework allows flexible choices of the forward model and the noise distributions. In addition, while this formulation is derived from probabilistic interpretations, it also subsumes the "reward-guided" or "loss-guided" setting , where \(\) can be viewed as a reward function or a negative loss function, both of which characterize preference over structural properties of \(x^{}\).

**Assumption on the forward model.** Throughout the paper, for simplicity, we make the following mild assumption on \(\), which is applicable to many applications of interest.

**Assumption 1**.: _We assume \((\,;\,y)\) is differentiable almost everywhere, and \(_{x^{d}}(x;y)<\)._

**Goal.** Our goal is to sample \(\) from the posterior distribution \( p^{}(\,|\,y)\) given estimates \((,x)\) (resp. \((,x)\)) of the _unconditional_ score functions \(s(,x)\) (resp. the noise function \((,x)\)) in (3), assuming knowledge of the likelihood function \((;y)\).

### Key ingredient: score-based denoising posterior sampling

We begin with an inspection on one of the most fundamental inverse problems: denoising under white Gaussian noise. As shall be elucidated shortly, the denoising diffusion samplers turn out to be an important building block in our algorithm for general inverse problems.

**Image denoising under white Gaussian noise.** Suppose that we have access to a noisy version of \(x^{} p^{}\) contaminated by white Gaussian noise, given by

\[x_{}=x^{}+,(0,^{2}I_{d}),\] (12)

where \(>0\) is the noise intensity _assumed to be known_. Our goal is to sample from \(p^{}(\,|\,x_{})\) given the score estimates \(_{t}(x)\) (resp. the noise estimates \(_{t}(x)\)). We will develop our score-based denoising posterior sampler, termed \(\), with two variants, \(\)-\(\) and \(\)-\(\), which can be viewed as analogues of the well-known \(\) and \(\) samplers in unconditional score-based sampling respectively. Before proceeding, it is worth highlighting that the two variants will be derived from different forward diffusion processes, since we observe the resulting variants empirically lead to more competitive performance.

**A stochastic \(\)-type sampler via heat flow.** We begin with a stochastic \(\)-type sampler for denoising, termed \(\)-\(\). We divide our development into the following steps.

1. _Step 1: introducing the heat flow._ Let us introduce a _heat flow_ with initial distribution \(p^{}\), defined by the following SDE: \[Y_{}=B_{}, 0, Y_{0} p^{ },\] (13) where \((B_{})_{ 0}\) is the standard \(d\)-dimensional Brownian motion. The solution of (13) is simply \[Y_{}=Y_{0}+B_{}, 0.\] (14) Since \(B_{}(0, I_{d})\), it readily follows that \(B_{^{2}}}}{{=}}\), which together with \(Y_{0} p^{}\) yield the important observation that \(x_{}=x^{}+\) can be viewed as an endpoint of the heat flow, in the sense that \(x_{}=x^{}+}}{{=}}Y_ {^{2}}\).
2. _Step 2: reversing the heat flow._ Following similar reasonings in Section 2, the next step boils down to reverse the heat flow (13). The time-reversal of the heat flow SDE (13) is (cf. (7)) given by \[Y_{^{2}-}^{}=_{Y_{^{2}-}}(Y_{ ^{2}-}^{})+_{}, [0,^{2}], Y_{^{2}}^{} p_{Y_{^{2}}},\] (15) where \((_{})_{ 0}\) is an independent copy of \((B_{})_{ 0}\). As introduced earlier, the virtue of the time-reversed SDE (15) is that it produces a process \(Y_{}^{}\) with the same _path_ distribution as \(Y_{}\), i.e., \((Y_{}^{})_{[0,^{2}]}}}{{=}}(Y_{})_{[0,^{2}]}\). In particular, the joint distribution of \((Y_{0}^{},Y_{^{2}}^{})\) is the same as that of \((Y_{0},Y_{^{2}})}}{{=}}(x^{},x_{ })\). This implies that the conditional distribution \(p^{}(\,|\,x_{})\) is the same as \(p_{Y_{0}^{}}(\,|\,Y_{^{2}}^{}=x_{})\). Surprisingly, the latter admits a simple interpretation: \(p_{Y_{0}^{}}(\,|\,Y_{^{2}}^{}=x_{})\) is the distribution of \(Y_{0}^{}\) when we initialize (15) with \(Y_{^{2}}^{}=x_{}\)! Therefore, sampling the posterior \(p^{}(\,|\,x_{})\) amounts to solving the following simple SDE: \[Y_{^{2}-}^{}= p_{Y_{^{2}-}}(Y_ {^{2}-}^{})+_{}, [0,^{2}], Y_{^{2}}^{}=x_{}.\] (16)3. _Step 3: connecting the score functions._ It is now immediate to arrive at our proposed stochastic sampler DDS-DDPM by discretization of this SDE (16), which requires knowledge of the score functions \( p_{Y_{}}()\). A key observation is that they can in fact be computed from the score function \(s(,x)\) (cf. (3)), due to the following lemma, whose proof is provided in Appendix D.1. [Score function of \(Y_{}\)] For \( 0\), we have \[ p_{Y_{}}(x)=}s((1+ ),\,}).\] The resulting sampler, DDS-DDPM, is summarized in Algorithm 2 (deferred in the appendix) using a discretization procedure with an exponential integrator .

A deterministic DDIM-type sampler via OU processWe next develop a deterministic DDIM-type sampler for denoising, termed DDS-DDIM.

1. _Step 1: introducing a posterior-initialized OU processTo sample from the posterior distribution \(p^{}(|x_{})\), we first introduce a random variable \(w\) which has (unconditional) distribution \[p_{w}(x) p^{}(x^{}=x\,|\,x^{}+=x_{}),\] (17) in the same form of the desired posterior distribution \(p^{}(|x_{})\). Here, since the noisy observation \(x_{}\) is given, we regard it as fixed.We then further introduce \(z=w-x_{}\), which is a "centered" version of \(w\), whose distribution is \[p_{z}(x) p_{w}(x+x_{})=p^{}(x^{}=x+x_{ }\,|\,x^{}+=x_{}).\] The OU process with initial distribution \(p_{z}\) is defined by the SDE: \[Z_{}=-Z_{}+B_{}, 0, Z_{0} p_{z},\] (18) where \(B_{}\) is the standard \(d\)-dimensional Brownian motion. As in (2), the marginal distribution of \(Z_{}\) is given by \[Z_{}}}{{=}}^{-}Z_{0}+ ^{-2}}, Z_{0} p_{z},\; (0,I_{d}), 0.\] (19)
2. _Step 2: reversing the OU processFollowing similar reasonings in Section 2, reversing the OU process (18) will enable us to generate samples \(z p_{z}\). Then we can set \(w=z+x_{}\), which, by definition, has distribution \(p_{w}\) defined in (17), and is a sample from the desired posterior distribution \(p^{}(|x_{})\). We are thus led to solve the time-reversed probability flow ODE (cf. (8)) of (18), given by_ \[Z_{^{}}^{}=Z_{^{ }}^{}+ p_{Z_{^{}}}(^{},Z_{^{}}^{}),[0, _{}], Z_{_{}}^{}(0,I_{d}), ^{}=_{}-.\] (20)
3. _Step 3: connecting the score functions._ We are now one step away from our proposed deterministic sampler DDS-DDIM, which is derived by discretization of the ODE (20). We need to know the score functions \( p_{Z_{}}()\), which again can be computed from the score function \(s(,x)\) (cf. (3)), as documented by the following lemma, whose proof is provided in Appendix D.2. [Score function of \(Z_{}\)] For \( 0\), we have \[ p_{Z_{}}(x)=-^{2}x}{^{2}+^{2 }-1}+^{-}^{2}}{^{2}+^{2 }-1}s(,\,^{-}x_{}+ ^{-}^{2}x}{^{2}+^{2}- 1}),\] (21) where \[()=(( ^{2}-1)}{^{2}+^{2}-1}+1).\] (22) After plugging this into (20) and solving the ODE for \(Z_{}^{}\), we see that \(Z_{0}^{}+x_{}\) is the desired sample from the posterior distribution \(p^{}(|x_{})\), as argued before. Numerically, the ODE (20) is solved by discretization with an exponential integrator , resulting in the sampler DDS-DDIM as summarized in Algorithm 3 (deferred in the appendix).

``` Input: Measurements \(y^{m}\), log-likelihood function \((;y)\) of the forward model, score estimates \(\), annealing schedule \((_{k})_{0 k K}\). Initialization: Sample \(_{0}(0,}{4}I_{d})\) Alternating sampling: for\(k=0,1,2,,K-1\)do 1. Proximal consistency sampler: Sample \(_{k+}((\,;\,y)-^{2}}\|-_{k}\|^{2})\) using subroutine \((_{k},y,,_{k})\) (Alg. 4). 2. Denoising diffusion sampler: Sample \(_{k+1}( p^{*}(x)-^{2}}\|x- _{k+}\|^{2})\) using subroutine \((_{k+},,_{k})\) (Alg. 2) or \((_{k+},,_{k})\) (Alg. 3). Output:\(_{K}\). ```

**Algorithm 1** Diffusion Plug-and-Play (DPnP)

### Our algorithm: diffusion plug-and-play

Now we turn to the general setting where the measurement operator \(\) is arbitrary. From the factorization of posterior distribution in (11), one intuitively understands that a posterior sampler must obey two constraints simultaneously: (i) the _data prior constraint_, corresponding to the first factor \(p^{*}(x)\), which imposes that the posterior sampler should be less likely to sample at those points where \(p^{*}(x)\) is small; (ii) the _measurement consistency constraint_, corresponding to the second factor \(^{(x;y)}\), which imposes that \((x) y\).

**Diffusion plug-and-play (DPnP).** We will apply the idea of alternatively enforcing these two constraints from a sampling perspective in the same spirit of . Our algorithm, dubbed diffusion plug-and-play (DPnP), alternates between two samplers, the denoising diffusion sampler (\(\)) and the proximal consistency sampler (\(\)), which can be viewed as the substitutes for the proximal operator and the gradient step respectively. Given the iterate \(_{k}\) and the _annealing_ parameter \(_{k}\) at the \(k\)-th iteration, DPnP proceeds with the following two steps:

1. [leftmargin=*]
2. _Proximal consistency sampler to enforce the measurement consistency constraint._ DPnP draws a sample \(_{k+}\) from the distribution proportional to \(((x\,;\,y)-^{2}}\|x-_{k}\|^ {2})\) to promote the image to be consistent with the measurements. This step, which we denote as the _proximal consistency sampler_, can be achieved by small modifications of standard algorithms such as Metropolis-Adjusted Langevin Algorithm (MALA)  given in Algorithm 4 (deferred in the appendix).
3. _Denoising diffusion sampler to enforce the data prior constraint._ DPnP next draws a sample \(_{k+1}\) from the distribution proportional to \[(-- p^{*}(x)+^{2}}\|x-_{k+ }\|^{2}) p^{*}(x^{*}=x\,|\,x^{*}+_{k}w= _{k+})\] (23) to promote the image to be consistent with the prior, where \(w(0,I_{d})\). The last step, which follows from the Bayes' rule, makes it clear that this step can be precisely achieved by the denoising diffusion sampler (developed in Section 3.1) using solely the unconditional score function.

Combining both steps lead to the proposed DPnP method described in Algorithm 1. Some comments about the proposed DPnP method are in order.

* The proximal consistency sampler \(\) can be viewed as a "soft" version of the proximal point method . This can be seen from a first-order approximation: the maximum likelihood of the distribution \(((;y)-^{2}}\|-_{k} \|^{2})\) is attained at the point \(x^{}^{d}\) satisfying \[_{x^{}}(x^{};y)-^{2}}(x^{ }-_{k})=0, x^{}=_{k}+ _{k}^{2}_{x^{}}(x^{};y)_{k}+ _{k}^{2}_{_{k}}(_{k};y),\] Therefore, the proximal consistency sampler draws random samples "concentrated" around \(x^{}\), which approximates the implicit proximal point update, akin to a gradient step at \(_{k}\).
* On the other end, the denoising posterior sampler \(\) can be regarded as a "soft" version of the proximal operator. In particular, when \(p^{*}\) is supported on a low-dimensional manifold \(\), it forces \(x\) to reside in \(\), like the proximal map. To see this, note that denoising posterior distribution vanishes outside \(\) by (23).

* The proximal consistency sampler \(\) admits a simple form when the forward model \(\) is linear, i.e. \((x)=Ax\) for some matrix \(A^{m d}\), and the measurement noise \((0,)\) is Gaussian. In this situation, the proximal consistency sampler \(\) can be implemented directly by \[_{k+}=(_{k},y,,_{ k})=_{k}+_{k}^{1/2}w_{k}, w_{k}(0,I_{d}),\] where \(_{k}=(A^{}^{-1}A+^{2}}I_{d} )^{-1}(A^{}^{-1}y+^{2}}_{k})\), and \(_{k}=(A^{}^{-1}A+^{2}}I_{d} )^{-1}\).

## 4 Theoretical analysis

In this section, we establish both asymptotic and non-asymptotic performance guarantees of \(\).

**Asymptotic consistency.** We begin with the asymptotic consistency of \(\) in the theorem below.

**Theorem 1** (Asymptotic consistency of \(\)).: _Assume the score function estimate \((,)\) is accurate, i.e., \((,x)=s(,x)\), and assume the ODE/SDEs in \(\) and \(\) are solved exactly. Let \((_{l})_{l 0}\) be a decreasing sequence of positive numbers satisfying \(_{l}_{l}=0\), and \((k_{l})_{l 0}\) be an increasing sequence of integers with \(k_{0}=0\). Set the annealing schedule \(_{k}=_{l}\), for \(k_{l-1} k<k_{l},\;l=1,2,\). Let \(_{l^{}=1,2,}|k_{l^{}}-k_{l^{}-1}|\), the output \(_{k_{l}}\) of \(\) converges in distribution to the posterior distribution \(p^{}(|y)\) for \(l\)._

In words, Theorem 1 establishes the asymptotic consistency of \(\) under fairly mild assumptions on the forward model (cf. Assumption 1): as long as the sampled distributions of \(\) and \(\) are exact, then running \(\) with a slowly diminishing annealing schedule of \(\{_{k}\}\) will output samples approaching the desired posterior distribution \(p^{}(|y)\) when the number of iterations \(l\) goes to infinity.

**Non-asymptotic error analysis.** We now step away from the idealized setting when the sampled distributions of \(\) and \(\) are exact. In practice, there are many sources of errors that can influence the sampled distributions of \(\) and \(\), e.g., the discretization error arising from numerically solving ODE/SDE, and the score estimation error. In effect, these non-idealities will make \(\) and \(\)_inexact_. That is, the distribution they generate will slightly deviate from the distribution they ought to sample from. In this paper, we model such deviations by the _total variation_ distance from the distribution generated by \(\) (resp. \(\)) to the ideal distribution proportional to \(((x;y)-^{2}}\|x-_{k}\|^{2})\) (resp. \(p^{}(x^{*}=x|x^{*}+_{k}=_{k+})\)) uniformly over all iterations. Analyzing these errors is out of the scope of this paper, and we point the interested readers to parallel lines of works, e.g., , among many others. In our analysis, we will assume a black-box bound for the total variation errors of \(\) and \(\), which can be combined with existing analyses of the respective samplers to bound the iteration complexity of \(\).

**Theorem 2** (Non-asymptotic robustness of \(\)).: _With the notation in \(\) (Algorithm 1), set \(_{k}>0\). Under Assumption 1, there exists \((p^{},,)(0,1)\), such that the following holds. Define a stationary distribution \(_{}\) by \(_{}(x) p^{}(x)q_{}(x),\) where \(q_{}\) is defined by_

\[q_{}(x)^{(\,;\,y)} p_{ }(x)=^{d}}^{(x^{ };\,y)-}\|x-x^{}\|^{2}}x^{}, (0,I_{d}),\] (24)

_where \(\) denotes convolution. If \(\) has error at most \(_{}\) in total variation and \(\) has error at most \(_{}\) in total variation per iteration, then for any accuracy goal \(_{}>0\), with \(K})}{1-}\), we have_

\[(p_{_{K}},_{})_{} (p_{_{1}}\,\|\,_{})}+( _{}+_{})(}}).\] (25)

Before interpreting Theorem 2, we observe that \(q_{0}(x)=^{(x;y)}\), thus \(_{0}(x) p^{}(x)^{(x;y)}\) coincides with the desired posterior distribution \(p^{}(|y)\). Thus Theorem 2 tells us that, assuming a constant annealing schedule \(_{k}=\), the output of \(\) converges in total variation to the distribution \(_{}\), which is a distorted version of the desired posterior distribution up to level \(\), with sufficiently many iterations. A few remarks are in order.

**Non-diminishing \(\).** It can be seen from Theorem 2 that even with a nonzero \(\), \(\) already enforces the data prior strictly. On the other hand, the measurement consistency is distorted by an order of \(\). This is usually tolerable, since the measurements are themselves contaminated by noise, thus when \(\) is smaller than the noise level, the distortion would be tolerable. In practice, it is beneficial to choose an annealing schedule of \(\{_{k}\}\), which will be elaborated in Section 5.

**Provable robustness.** Theorem 2 indicates the performance of DPnP degenerates gracefully in the presence of sampling errors. To the best of our knowledge, this is the first provably consistent and robust posterior sampling method for nonlinear inverse problems using score-based diffusion priors.

## 5 Numerical experiments

We provide preliminary numerical evidence to corroborate the promise of DPnP in solving both linear and nonlinear image reconstruction tasks. We denote DPnP with the subroutines DDS-DDPM and DDS-DDIM as DPnP-DDPM and DPnP-DDIM respectively.

    &  &  &  &  \\  &  &  &  &  \\ Algorithm & LPIPS \(\) & PSNR \(\) & LPIPS \(\) & PSNR \(\) & LPIPS \(\) & PSNR \(\) \\  DPnP-DDIM (ours) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \( 90\)s \\ DPS [CKM\({}^{+}\)23] & \(0.331\) & \(23.1\) & \(0.490\) & \(17.4\) & \(0.367\) & \(21.7\) & \( 60\)s \\ LGD-MC (\(n=5\)) [SZY\({}^{+}\)23] & \(0.318\) & \(23.9\) & \(0.522\) & \(16.4\) & \(0.317\) & \(23.9\) & \( 60\)s \\ ReSample (pixel-based) [SKZ\({}^{+}\)23] & \(0.313\) & \(23.9\) & - & - & \(0.318\) & \(22.6\) & \( 70\)s \\   

Table 3: Evaluation of solving inverse problems on FFHQ \(256 256\) validation dataset (1k samples). Despite considerable efforts to optimize parameters, pixel-based ReSample did not generate meaningful results for phase retrieval.

   Input & DPS & LGD-MC & ReSample & DPnP-DDPM & DPnP-DDIM & Ground truth \\   

Table 2: Samples of different algorithms for quantized sensing.

**Experimental setups.** We compare DPnP with the state-of-the-art algorithms including DPS [CKM\({}^{+}\)23], LGD-MC [SZY\({}^{+}\)23], and pixel-based ReSample [SKZ\({}^{+}\)23] for super-resolution (linear), phase retrieval (nonlinear), and quantized sensing (nonlinear). Definitions of these forward measurement models are in Appendix G.2. The annealing schedule \(\{_{k}\}\) of DPnP is fixed across _all_ tasks (Appendix H.2), while DPS, LGD-MC, and ReSample are fine-tuned with reasonable effort for best performance. All experiments are run on a single Nvidia L40 GPU. More details and experiments are in Appendix G.

**Sample images.** We present the sample images of different algorithms for the most complicated task of phase retrieval. For phase retrieval, Fourier transform is performed to the image with a coded mask, and only the magnitude of the Fourier transform is taken as the measurement [SEC\({}^{+}\)15]. Due to the nonlinearity of the operation of taking magnitude, the forward model is nonlinear. The samples generated by different algorithms are shown in Table 1.

We also present the sample images for the nonlinear problem of quantized sensing. In quantized sensing, each pixel of the image is randomly dithered and then quantized to one bit per channel. Nonlinearity of quantizing renders this forward model nonlinear. The samples generated by different algorithms are shown in Table 2.

**Evaluation.** We evaluate the performance of DPnP on the FFHQ validation dataset [KLA19] and the ImageNet validation dataset [RDS\({}^{+}\)15]. Since DPnP-DDIM has similar performance with DPnP-DDPM but admits much faster implementation, only DPnP-DDIM is evaluated. The LPIPS and PSNR are shown in Table 3 and Table 4. These two metrics are arguably the more relevant ones for solving inverse problems. For comparison under other metrics such as FID, SSIM, cf. Appendix G.4.

It can be seen that, DPnP is capable of solving both linear and nonlinear problems, and, in comparison with prior state-of-the-art, performs better in recovering fine and criser details.

## 6 Discussion

This paper sets forth a rigorous and versatile algorithmic framework called DPnP for solving nonlinear inverse problems via posterior sampling, using image priors prescribed by score-based diffusion models with general forward models. DPnP alternates between two sampling steps implemented by DDS and PCS, to promote consistency with the data prior constraint and the measurement constraint respectively. We provide both asymptotic and non-asymptotic convergence guarantees, establishing DPnP as the first provably consistent and robust score-based diffusion posterior sampling method for general nonlinear inverse problems.