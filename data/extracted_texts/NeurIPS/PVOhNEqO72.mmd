# Mapping the intermolecular interaction universe through self-supervised learning on molecular crystals

**Anonymous Author(s)**

Affiliation

Address

email

#### Abstract

Molecular interactions fundamentally influence all aspects of chemistry and biology. Prevailing machine learning approaches emphasize the modeling of molecules in isolation or at best provide limited modeling of molecular interactions, typically restricted to protein-ligand and protein-protein interactions. Here, we present how to use molecular crystals to define the MolInteractDB dataset that contains valuable biochemical knowledge, which can be captured by large self-supervised pre-trained models. MolInteractDB incorporates 344,858 molecular crystal structure entries from the Cambridge Structural Database. We formulate entries in the MolInteractDB dataset as radial patches of flexible size and at varying positions in the crystal to represent intermolecular interactions across crystal structures. We characterize a variety of interactions highlighted across 6 million patches. Leveraging MolInteractDB, we develop InteractNN, a self-supervised SE(3)-equivariant 3D message passing network. We show that InteractNN captures the latent knowledge of chemical elements as well as intermolecular interaction types at a scale not directly accessible to human scientists. To demonstrate its potential, we fine-tuned InteractNN to predict the binding affinity between proteins and ligands, producing results comparable with state-of-the-art models.

## 1 Introduction

Intermolecular interactions between molecules play a central role in understanding and predicting chemical phenomena [7; 17; 27; 65]. In drug discovery, intermolecular interactions between the ligand and target are key factors for the selectivity and specificity of the drug [52; 68; 3; 46; 34; 19; 71]. While these interactions are important for chemists, the exploration of intermolecular interactions in machine learning is limited. Many state-of-the-art models in molecular property prediction train on molecular datasets featuring molecules in isolation, for PCQM4Mv2 , QM9 , and ZINC . In contrast, ML models for molecular interactions are restricted to protein-ligand and protein-protein interaction (PPI) structures, leaving the broader field of intermolecular interactions largely untouched. Given the fundamental role of intermolecular interactions, it is important to consider a broader variety of these interactions to improve the generalizability of ML models.

An experimental data modality that captures intermolecular interactions is a crystal structure , which records the 3D coordinates of the atoms in the crystal. In molecular crystals, molecules are bound together by intermolecular interactions in an infinitely repeating lattice. To represent this periodic structure, crystal structures are expressed as a unit cell--the smallest repeating unit of the crystal. There are large datasets of crystal structures including the Cambridge Structural Database (CSD) featuring 1,222,711 entries of which 344,858 are of organic molecular crystals that satisfied our search criteria. Unlike the discrete molecules found in many molecular property datasets, representing unit cells to capture intermolecular interactions for ML presents unique challenges.

**Present work.** We present MolInteractDB, a dataset created from the CSD that captures intermolecular interactions from unit cells of entries in the CSD. Each entry in the MolInteractDB is a radial patch which includes the 3D coordinates and atomic identities of intermolecular interacting molecular fragments. A key contribution of MolInteractDB is the expansion of intermolecular examples available for ML models. By leveraging the CSD, we extend beyond molecular interactions limited to protein-ligand and protein-protein complexes, and present more intermolecular interactions that are chemically relevant for ML. In addition to dataset creation, we also developed the InteractNN model which is trained with self-supervised objectives to learn an informative latent space of patches. Probing the latent representation space reveals its ability to learn chemical types of interactions, and elemental differences. Finally, we show that InteractNN can be fine-tuned to predict binding affinity of protein-ligand interactions and achieves comparable results to state-of-the-art.

## 2 Related work

**Machine learning for molecules.** In the field of molecular machine learning, there are various studies ranging from property prediction to generation. Molecules can be represented either as 1D strings, such as SMILES  and SELFIES , and are typically trained using language models [72; 60]. Alternatively, 2D and 3D molecular structures can be represented as graphs [44; 67; 69; 11; 48; 64; 32] and trained using graph neural networks (GNNs). These models can predict molecular properties [29; 41; 32] and help design new molecules [34; 42; 70; 28; 21; 35; 62].

**Geometric deep learning for molecular prediction and design.** Molecules can adopt multiple 3D configurations, known as conformers, which are not represented in 1D or 2D forms. Additionally, 3D geometric information significantly influences the properties and functions of a molecule. Consequently, several geometric deep learning models incorporate 3D coordinates for molecular property prediction [47; 9; 31; 58]. Given the scarcity of labeled 3D molecular data, self-supervised formulations for pre-training on 3D molecular structures have been developed. Notable models include GraphMVP , GNS-TAT , and 3D InfoMax . Among them, GNS-TAT  demonstrates that pre-training by denoising 3D structures towards equilibrium can enhance performance in downstream tasks. Subsequently, these models are fine-tuned on smaller 3D molecular datasets with labeled molecular properties. Progress has also been made in constructing equivariant models. These ensure that when certain symmetry operations or transformations are applied to the input, equivalent transformations are reflected in the output. This is crucial for maintaining the consistency of output predictions with SE(3)-symmetry operations, which include translations and rotations [45; 14].

**Machine learning for molecular interactions.** Molecular interactions underpin virtually all processes within living organisms. Several models have been developed to predict molecular interactions, including binding affinity prediction [37; 63; 38; 26; 37], binding site prediction [36; 24; 20; 22], and PPI prediction [7; 54; 8]. The field of molecular interactions is expanding with emerging areas of interest such as the design of molecular glues to stabilize PPIs  and the modulation of PPIs to target the undruggable proteome . However, the scarcity of data, primarily due to the challenges in capturing 3D molecular data of interacting biological compounds, has curtailed the widespread application of ML in these nascent fields. Recognizing the need to understand intermolecular interactions across stages of drug design and development and across therapeutic modalities, we harness large datasets of molecular crystals to advance the modeling of intermolecular interactions.

## 3 Creating MolInteractDB dataset

In this section we outline how we curate the CSD to capture examples of intermolecular interactions from molecular crystals. We start by defining intermolecular patches (Sec. 3.1) and proceed by outlining the curation of MolInteractDB (Sec. 3.2).

### Overview of Cambridge Structural Database (CSD)

The CSD  contains all known crystal structures of small-molecule organic and metal-organic crystal structures. These structures are experimentally determined with X-ray or neutron crystallography. As of 2023 there are over 1.25 million crystal structures in the CSD, of which under half of these structures are classified as organic. A review from Taylor and Wood highlights the contributions of the CSD in researching molecular geometries, interactions, and assemblies .

**Curating molecular cystals from the CSD.** Querying and accessing of crystal structure data was done with the CSD Python API. Each entry of the CSD describes a crystal structure stored as a unit cell, the smallest component that represents the repeating crystal structure, and the metadata including publications associated with the entry, experimental details, and the chemical formula. Additionally, the CSD computationally assigns bonds and bond types between atoms to every entry. An example of data available for an entry in the CSD is shown in Figure 1. We filtered CSD v2022.3.0 for all entries that satisfied all of the following criteria: organic, not polymeric, has 3D coordinates, no disorder, no errors, no metals, had only one SMILES string describing the crystal entry (in other words, each crystal is comprised of only one chemical compound). This filtered the CSD dataset from 1,222,711 entries to 344,858.

### Creating intermolecular patches in MolInteractDB using molecular crystals

To represent intermolecular interactions we define intermolecular patches as entries in MolInteractDB. Each radial patch is centered between two molecules to capture the geometric orientations of non-bonding interactions between two molecules. This approach captures diverse types of intermolecular interactions, including hydrogen bonding, Van der Waals interactions, aromatic interactions. Here we do not directly model the periodic unit cell, as we focus on recording intermolecular interactions. Radial patches have been shown to be useful in related fields of modeling protein surfaces [7; 54; 8; 53; 2], where patches are defined on the surface of a protein to reduce large protein surfaces to a fingerprint. Our approach differs to the use of patches for modeling of protein surfaces--which only feature one molecule--instead our patches capture interactions between molecules.

**Definition 3.1** (**Intermolecular Patch**).: An intermolecular patch \(G^{(ij)}\) is a graph with geometric 3D coordinate attributes that is comprised of molecular fragments of intermolecularly interacting molecules \(i\) and \(j\), here denoted as \(M^{(i)}\) and \(M^{(j)}\), respectively. Intermolecular interactions are all non-bonding interactions between \(M^{(i)}\) and \(M^{(j)}\); this includes hydrogen bonding, dipole-dipole interactions, Van der Waals interactions, and aromatic-aromatic interactions. Molecular fragments in the patch are all atoms in the molecules that are within a radius \(r\) from the weighted center, \(c^{(ij)}=1/(2|M^{(i)}||M^{(j)}|)(|M^{(j)}|_{k M^{(i)}}_{ k}+|M^{(i)}|_{k M^{(j)}}_{k})\), where \(_{k}\) is the atomic coordinates of the molecules. The nodes and edges of the \(G^{(ij)}=(V_{G^{(ij)}},E_{G^{(ij)}})\) are:

* **Nodes:**\(V_{G^{(ij)}}=(V^{(i)},V^{(j)})\), where \(V^{(i)},V^{(j)}\) are atoms in \(M^{(i)}\) and \(M^{(j)}\) that are within radius \(r\) to the center \(c^{(ij)}\). We denote arbitrary nodes in \(V_{G^{(ij)}}\) with \(a\) and \(b\).
* **Edges:**\(E_{G^{(ij)}}=(E^{(ij)},E^{(i)},E^{(j)})\) are comprised of:
* **Intermolecular edges:**\(E^{(ij)}\) connect atoms in \(V^{(i)}\) with atoms in \(V^{(j)}\) that are positioned within distance \(d_{}\) of each other.
* **Intramolecular edges:**\(E^{(i)}\) are edges between atoms in \(V^{(i)}\), and \(E^{(j)}\) are edges between nodes in \(V^{(j)}\) that are within distance \(d_{}\).

We also refer to neighbouring nodes \(b_{a}^{(t)}\) of node \(a\) in a patch, where \(t\{,\}\) are the intermolecular and intramolecular edge neighbours. For intermolecular neighbours, we refer to the

Figure 1: Illustrative example of the CSD entry for ABOSAN.

edges \(E^{(ij)}\). For intramolecular neighbours, if \(a V^{(i)}\) we refer to the edges \(E^{(i)}\), otherwise if \(a V^{(j)}\) we refer to the edges \(E^{(j)}\).

The MolInteractDB dataset, \(=\{G^{(i_{k}j_{k})} k=1,,N\}\), is comprised of patches \(G^{(ij)}\) constructed from CSD entries. For our purposes of learning intermolecular interactions we sampled many patches to represent all examples of intermolecular interactions in a unit cell (Figure 2). Given an entry of the CSD, we iterate through each unique, valid conformer \(M^{(i)}\) in the unit cell using the CSD Python API. For each conformer \(M^{(i)}\) we iterate through all neighbouring peripheral conformers \(M^{(j)}\) of this molecule given by the unit cell that are within \(d_{}\) to an atom in \(M^{(i)}\). A patch \(G^{(ij)}\) is constructed from all atoms in \(M^{(i)}\) and \(M^{(j)}\) that are within radius \(r\) to the weighted center \(c^{(ij)}\). This extraction of patches from a unit cell will yield some patches that are equal up to permutation.

We set \(r=8\) A \(d_{}=4\) A  and \(d_{}=2\) A. After iterating through all 344,858 CSD entries that satisfied our CSD filters, this constructs 6,059,368 patches in \(\). The choice of radius, intermolecular and intramolecular edge distance cutoff for the patch will influence the number of patches created. A radius \(r\) that is too small would break basic chemical motifs, which would lead to insufficient chemical context for interactions in the patch. Intramolecular edge cutoffs \(d_{}\) that are too short would also disregard longer chemical bonds, and intermolecular edge cutoffs \(d_{}\) that are too short would limit the number of patch examples. Our choice of cutoffs aim to provide sufficient chemical context. We summarise statistics of the patches in MolInteractDB in Table 1.

## 4 InteractNN model and its compelling use cases

Next we outline the InteractNN model that uses MolInteractDB for self-supervised pretraining. We provide details for how we probe the learned latent space of the InteractNN to explore the space of chemical interactions (Sec. 4.1) and show how InteractNN can be fine-tuned for protein-ligand binding prediction (Sec. 4.2).

    &  \\  Feature & Mean & SD & Min & Max & Element & \% Distribution \\  \# Nodes & 67.8 & 24.3 & 4 & 424 & Carbon & 44.6 \\ \# Intermolecular edges & 34.1 & 34.5 & 1 & 8,547 & Hydrogen & 42.4 \\ \# Intramolecular edges & 89.5 & 36.6 & 2 & 2,859 & Oxygen & 6.1 \\  & 1.3 & 0.2 & 0.0 & 22.9 & Nitrogen & 3.9 \\  & 0.5 & 0.4 & 0.1 & 11.45 & Fluorine & 0.8 \\   

Table 1: Properties of 6,059,368 patches in MolInteractDB with \(r=8\) Å, \(d_{}=4\) Å, and \(d_{}=2\) Å.

Figure 2: Intermolecular molecular patches from MolInteractDB. CSD entry ABIGAV is shown.

### Overview of InteractNN model

InteractNN uses a SE(3)-equivariant 3D message passing network on intermolecular patches to learn representations that are informative of the intermolecular interaction between molecules.

**Problem (Self-Supervised Pre-Training For intermolecular Patches).**_Given is an unlabeled pre-training dataset of intermolecular patches, \(=\{G^{(i_{k}j_{k})} k=1,,N\}\), and a target dataset of labeled intermolecular patches \(=\{(G^{(i_{k}j_{k})}_{},y_{k}) k=1,,M\}\), where \(M<<N\). Our goal is to pre-train a model \(\) on \(\) such that it generates representations \(_{k}=(G^{(i_{k}j_{k})})\) for every intermolecular patch \(G^{(i_{k}j_{k})}\) that are chemically informative, and \(\) can also be fine-tuned on \(\) to predict \(y_{k}\) for every \(G^{(i_{k}j_{k})}_{}\)._

**Atom-level representation learning.** Here we outline the SE(3)-equivariant 3D message passing network for InteractNN on the nodes of the intermolecular patch \(G^{(i,j)}\). Several rotational equivariant neural networks have been introduced for modeling molecules . We build on the E(3)-equivariant neural network layers presented by Tensor-Field Networks implemented in e3nn  and DiffDock . Message passing for the intermolecular edges and intramolecular edges are done separately, but the message passing framework for the two edge types is the same.

The feature vectors \(_{a}\) of nodes \(a\) in \(G^{(i,j)}\) are geometric objects that comprise a direct sum of irreducible representations of the O(3) symmetry group. The feature vectors \(_{a}^{(,p)}\) are indexed with \(,p\), where \(=0,1,2,\) is a non-negative integer denoting the rotation order and \(p\{,\}\) indicates odd or even parity, which together index the irreducible representations (irreps) of O(3). There are also multiple features in \(_{a}\) which have the same irrep. In our model, we set \(_{}=1\) for \(_{a}\), and we denote the number of scalar (\(0\)) and pseudoscalar (\(0\)) irrep features in \(_{a}\) with \(\), and the number of vector (\(1\)) and pseudovector (\(1\)) irrep features in \(_{a}\) with \(\).

First, the element type of node \(a\) is embedded with a normal distribution and trainable weights to a vector with feature configuration \( 0\). The edge length between the coordinates of node \(a\) and neighbouring node \(b\) is also embedded with Gaussian smearing to a vector comprised of \( 0\), then the Gaussian embedding vector is passed through a 2-layer MLP projector to output a feature vector \(_{ab}\) with feature configuration \( 0\).

There are \(L\) layers of message passing between nodes. At each layer \(l\), the node updates for node \(a\) in the intermolecular patch \(G^{(i,j)}\) are given by:

\[_{a},\}}{ }^{(t)}(_{a}^{(t)}|}{_{b _{a}^{(t)}}}Y^{()}(_{ab})_{ _{ab}}_{b})_{ab}=^{(t)}(_{ab}, _{a}^{0},_{b}^{0}),\] (1)

where node \(b\) are the neighbours of node \(a\) in \(G^{(i,j)}\) given by intermolecular or intramolecular edges denoted with \(t\). The message is computed with tensor products between the spherical harmonic projection with rotation order \(=2\) of the unit bond direction vector, \(Y^{()}(_{ab})\), and the irreps of the feature vector of the neighbour \(_{b}\). This is a weighted tensor product and the weights are given by a 2-layer MLP, \(^{(t)}\), based on the \(0\) features of the nodes \(_{a}\) and \(_{b}\) and the edge features \(_{ab}\). After each layer \(l\) of message passing, \(_{a}\) is filtered down to irreps with \(_{}=1\). After \(L\) layers the final irreps configuration of \(_{a}\) is \( 0+ 1+ 1 + 0\) and the embedding of node \(a\), \(_{a}\) is a \(d_{}\)-dimension vector.

**Intermolecular patch-level representation learning.** For a patch-level embedding of the nodes a convolution is done between all nodes in a molecule and the unweighted center \(c^{(i)}\) of the nodes \(V^{(i)}\). This is repeated for nodes \(V^{(j)}\) in the patch \(G^{(ij)}\). The edge distance from node \(a\) to \(c^{(i)}\) is also embedded with Gaussian smearing and passsed through a 2-layer MLP projector to output a feature vector \(_{ac^{(i)}}\) with feature configuration \( 0\) as:

\[_{c^{(i)}}=(|}_{a  V^{(i)}}Y^{()}(_{a,c^{(i)}})_{_{ac^ {(i)}}}_{a})_{ac^{(i)}}=(_{ac^{(i)}}, _{a}^{0}).\] (2)

This is a weighted tensor product and the weights are given by a 2-layer MLP projector, \(\), based on the \(0\) features of the nodes \(_{a}\) and the edge features \(_{ac^{(i)}}\). The embedding of the intermolecularpatch \(G^{(i_{k}j_{k})}\) is given by \(_{k}=[h_{c^{(i_{k})}}^{0}||h_{c^{(i_{k})}_{i}}^{0}]\), the concatenation of the scalars from embedding molecule \(i_{k}\) and \(j_{k}\), which is a \(d_{}\)-dimension vector.

**Self-supervised training with denoising.** Node-level denoising as an objective function has been useful for pre-training on 3D coordinate molecular datasets from DFT generated molecules to prevent over-smoothing of GNNs , and it has proven that it is related to learning a force field of per-atom forces [66; 6]. In addition, denoising is linked to score-matching which has also been popular in training generative models [16; 3]. Thus, this motivates the application of denoising as an objective for self-supervised training on MolInteractDB.

Given a patch \(G^{(i,j)}\), \(^{(i,j)}\) is a perturbed patch created by adding i.i.d. Gaussian noise to the atomic positions, \(_{a}\) of each node \(a V_{^{(i,j)}}\). That is, for each node \(a V_{^{(i,j)}}\) the atomic position \(_{a}^{}=_{a}+_{a}\), where \(_{a}(0, I_{3}),=0.5\) and \(_{a}=(_{a},)\). The objective is to predict \(\{_{1},,_{|V_{^{(i,j)}} |}\}\) given \(^{(i,j)}\). The model \(_{}\) is trained to minimise the loss \(\):

\[=_{G^{(i,j)}}\|_{ }(^{(i,j)})-(_{1},, {}_{|V_{^{(i,j)}}|})\|^{2}}\] (3)

We add a denoising layer to \(\) for \(_{}\) to predict the noise applied for each node from \(^{(i,j)}\). This final layer of the message passing on \(^{(i,j)}\) takes as input the node-level embeddings \(_{a}\) and is the same message passing framework as outlined in Eq. (1). However, the output irreps are restricted to \(1 10+1 1\). To convert this to 3D coordinates, the \(1 1\) and \(1 1\) are summed element-wise to produce a vector in \(^{3}\) and the prediction is clamped to the maximum noise applied which is 1 A.

### Implementation and use cases

**Implementation.** The model was trained with the denoising objective with a batch size of 64 on MolInteractDB for 48 hours on 48GB RTX 8000 GPUs. The model hyper-parameters were set to \(=32,\,=16,\,L=6\), and \(lr=1 10^{-3}\).

**Probing latent representation space.** We investigate whether the self-supervised training of the model resulted in a chemically meaningful latent space by characterizing the patch-level and node-level embedding spaces of MolInteractDB. To determine the chemical labels of patches, we convert the molecular fragments within a patch into RDKit molecules and sourced labels from RDKit. Nodes are labeled based on their atomic elements and further categorized by examining the elements they were bonded to, as well as the bond types.

**Modeling protein-ligand binding.** In this use case, we use the PDBbind v2020 dataset , which is a curated subset of the Protein DataBank (PDB) with the structure of bound ligands to proteins, and the associated binding affinity. The task is: given the protein-ligand structure, predict the binding affinity. We use the pocket-ligand substructures of the protein-ligand structure given by PDBbind, where amino acids in the pocket are all amino acids with any atom within 6 A to the ligand. Given the pocket-ligand, we construct a graph with the same features as a patch where the two molecules in the patch are the pocket and the ligand, and intermolecular edges are defined as edges between the pocket and the ligand. Note that we do not restrict the size of the pocket-ligand patch to a radial cutoff. The pocket-ligand graph is passed through the pre-trained InteractNN which gives a patch-level embedding that is passed through a 3-layer MLP predictor to output a binding affinity prediction clamped to between 0 and 15. The InteractNN is fully fine-tuned on the pocket-ligand structures to minimize the root mean squared error between predicted and experimental binding affinity.

## 5 Experiments

### Use case: Probing the latent space of chemical interactions

**Setup.** Given the pre-trained InteractNN we embedded all the nodes and patches and visualized a 2D UMAP for each set of nodes and patches. For patches we label the types of intermolecular interactions at the interface between the two molecules in the patch. If any of the intermolecular interactions are between two atoms in an aromatic system, the patch is labeled as aromatic. Otherwise, if any of the intermolecular interactions are between two atoms where one is a hydrogen bond donor and another is a hydrogen bond acceptor, the patch is labeled as hydrogen bonding. Other interactions, such as dipole-dipole, and Van der Waals interactions are not labeled. For node embeddings, we labelled each node with the atomic element. For the most common elements, carbon and hydrogen, we explored with further granularity by considering the elements the nodes are bonded to and bond types. To test the statistical significance of chemical clusters we use the Kolmogorov-Smirnov (KS) test to compare randomly sampled pairwise distances of \(d\)-dimensional embeddings compared to pairwise distances sampled within \(d\)-dimensional embeddings of the same chemical label.

**Results.** The InteractNN learns an overall embedding space for patches as well as nodes in every patch and we find that embeddings are meaningfully localized based on various chemical properties. In Figure 3, we use a 2D UMAP to visualize the embedding of 300,000 randomly sampled patches from MolInteractDB. Labeling of the UMAP with the chemical type of intermolecular interaction as aromatic groups interacting with aromatic groups, or hydrogen bond donor and hydrogen bond acceptor shows InteractNN learns a chemically enriched latent space in a self-supervised manner. We also see statistically significant differences with \(p\)-value < 0.001 for the pairwise distance of embeddings labeled as hydrogen or aromatic against all patch-level embeddings.

Visualization of the embedding space of 300,000 sampled nodes of patches from MolInteractDB in Figure 4 highlights that InteractNN has learnt differences in atomic environments in a self-supervised manner. In Figure 4a, we see in the embedding space that the InteractNN has differentiated between the elements. Isolating the most common elements, hydrogen, and carbon, pairwise distance of node embeddings within these elements are statistically significantly different to pairwise distances of all node embeddings (\(p\)-value < 0.001). We also show that the embeddings of carbon and hydrogen nodes can be stratified further by the bonding environment. Remarkably, without any prior knowledge of bond types, Figure 4e shows that InteractNN embeds the aromatic carbons in a separate region to the aliphatic carbons (single bonded carbons).

### Use case: Protein-ligand binding affinity prediction

A sequence-based split of 60% from Atom3D  is used to train and test the model. We compare our protein-ligand binding affinity prediction with state-of-the-art models trained and tested under the same dataset split. Performance is determined by minimizing the root mean squared error between predicted and actual binding affinity, and by maximizing Pearson and Spearman correlation coefficients between the predicted and actual binding affinity. Results in Table 2 show that the performance of InteractNN is comparable to state-of-the-art models across all metrics. We also show that the absence of pre-training for InteractNN results in a decay in performance.

Figure 3: 2D UMAP plots of InteractNN embeddings of (**a**) 300,000 randomly sampled patches from MolInteractDB. Each dot is a patch and they are labeled by the type of intermolecular interactions present in the patch. (**b**) 300,000 randomly sampled nodes from patches from MolInteractDB. Each dot is a node and they are labeled by element of the node.

## 6 Conclusion

Intermolecular interactions are essential to chemical properties and diverse functions of biological systems. In this work, we introduce a MolInteractDB dataset that leverages large molecular crystal databases to extract examples of intermolecular interactions between molecular fragments in the form of intermolecular patches. We explore the diversity of this dataset and train a InteractNN model on MolInteractDB in a self-supervised manner. We show that the learned latent space of InteractNN is informative for capturing nuances between hydrogen bonding and aromatic interactions. The model can also distinguish between chemical elements. Finally, we fine-tune the model for protein-ligand binding affinity prediction and achieve results comparable to state-of-the-art models. In the future, we will adapt InteractNN for fine-tuning on other molecular interaction tasks, including protein-protein interactions, and explore the model's ability for few-shot prompting and zero-shot learning.