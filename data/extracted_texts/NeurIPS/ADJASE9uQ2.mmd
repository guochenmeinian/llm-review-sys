# 2DQuant: Low-bit Post-Training Quantization for Image Super-Resolution

Kai Liu\({}^{1}\), Haotong Qin\({}^{2}\), Yong Guo\({}^{3}\), Xin Yuan\({}^{4}\),

**Linghe Kong\({}^{1}\)1, Guihai Chen\({}^{1}\), Yulun Zhang\({}^{1}\)1**

###### Abstract

Low-bit quantization has become widespread for compressing image super-resolution (SR) models for edge deployment, which allows advanced SR models to enjoy compact low-bit parameters and efficient integer/bitwise constructions for storage compression and inference acceleration, respectively. However, it is notorious that low-bit quantization degrades the accuracy of SR models compared to their full-precision (FP) counterparts. Despite several efforts to alleviate the degradation, the transformer-based SR model still suffers severe degradation due to its distinctive activation distribution. In this work, we present a dual-stage low-bit post-training quantization (PTQ) method for image super-resolution, namely **2DQuant**, which achieves efficient and accurate SR under low-bit quantization. The proposed method first investigates the weight and activation and finds that the distribution is characterized by coexisting symmetry and asymmetry, long tails. Specifically, we propose Distribution-Oriented Bound Initialization (DOBI), using different searching strategies to search a coarse bound for quantizers. To obtain refined quantizer parameters, we further propose Distillation Quantization Calibration (DQC), which employs a distillation approach to make the quantized model learn from its FP counterpart. Through extensive experiments on different bits and scaling factors, the performance of DOBI can reach the state-of-the-art (SOTA) while after stage two, our method surpasses existing PTQ in both metrics and visual effects. 2DQuant gains an increase in PSNR as high as 4.52dB on Set5 (\( 2\)) compared with SOTA when quantized to 2-bit and enjoys a 3.60\(\) compression ratio and 5.08\(\) speedup ratio. The code and models are available at https://github.com/Kai-Liu001/2DQuant.

## 1 Introduction

As one of the most classical low-level computer vision tasks, image super-resolution (SR) has been widely studied with the significant development of deep neural networks. With the ability to reconstruct high-resolution (HR) image from the corresponding low-resolution (LR) image, SR has been widely used in many real-world scenarios, including medical imaging , surveillance , remote sensing , and mobile phone photography. With massive parameters, DNN-based SR models always require expensive storage and computation in the actual application. Some works have been proposed to reduce the demand for computational power of SE models, like lightweight architecture design and compression. One kind of approach investigates lightweight and efficient models as the backbone for image SR. This progression has moved from the earliest convolutional neural network (CNNs)  to Transformers  and their combinations. The parameter number significantly decreased while maintaining or even enhancing performance. The other kind of approach is compression, which focuses on reducing the parameter (_e.g._, pruning and distillation) or bit-width (quantization) of existing SR models.

Model quantization  is a technology that compresses the floating-point parameters of a neural network into lower bit-width. The discretized parameters are homogenized into restrictedcandidate values and cause heterogenization between the FP and quantized models, leading to severe performance degradation. Considering the process, quantization approaches can be divided into quantization-aware training (QAT) and post-training quantization (PTQ). QAT simultaneously optimizes the model parameters and the quantizer parameters , allowing them to adapt mutually, thereby more effectively alleviating the degradation caused by quantization. However, QAT often suffers from a heavy training cost and a long training time, and the burden is even much heavier than the training process of the FP counterparts, which necessitates a large amount of compatibility and makes it still far from practical in training-resource-limited scenarios.

Fortunately, post-training quantization emerges as a promising way to quantize models at a low training cost. PTQ fixes the model parameters and only determines the quantizer parameters through search or optimization. Previous researches  on PTQ for SR has primarily focused on CNN-based models such as EDSR  and SRResNet . However, these quantization methods are not practical for deployment for two reasons. **Firstly**, these CNN-based models themself require huge space and calculation resources. Their poor starting point makes them inferior to advanced models in terms of parameters and computational cost, even after quantization. As shown in Table 1, the light version of SwinIR needs only 16.2% parameters and 15.9% FLOPs compared with quantized EDSR. But its PSNR metric is close to that of the FP EDSR. While the previous PTQ algorithm, DBDC+Pac, suffers from unacceptable degradation in both visual and metrics. **Secondly**, most of these methods can not adapt well to Transformer-based models because of the deterioration of self-attention in quantized transformers. As shown in Figure 1, when applied on SwinIR, the existing methods still suffer from distorted artifacts compared with FP or HR.

Therefore, we conducted a post-training quantization analysis on super-resolution with a classical Transformer-based model SwinIR . The weight and activation distribution is characterized by coexisting symmetry and asymmetry, long tails. Firstly, if the previous symmetric quantization method is applied for asymmetric distribution, at least half of the candidates are completely ineffective. Besides, the long tail effect causes the vast majority of floating-point numbers to be compressed into one or two candidates, leading to worse parameter homogenization. Furthermore, with such a small number of parameters, SwinIR's information has been highly compressed, and quantizing the model often results in significant performance degradation. Nevertheless, the excellent performance and extremely low computational requirements of Transformer-based models are precisely what is needed for deployment in real-world scenarios.

In this paper, we propose **2DQuant**, a two-stage PTQ algorithm for image super-resolution tasks. To enhance the representational capacity in asymmetry scenarios, we employ a quantization method with two bounds. The bounds decide the candidate for numbers out of range and the interval of candidates in range. **First**, we propose **distribution-oriented Bound Initialization** (DOBI), a fast MSE-based searching method. It is designed to minimize the value heterogenization between quantized and FP models. Two different MSE  search strategies are applied for different distributions to avoid nonsense traversal. This guarantees minimum value shift while maintaining high speed and efficiency in the search process. **Second**, we propose **Distillation Quantization Calibration** (DQC), a training-based method. It is designed to adjust each bound to its best position finely. This ensures that the outputs and intermediate feature layers of the quantized model and that of the FP model should remain as consistent as possible. Thereby DQC allows the quantizer parameters to be finely optimized toward the task goal. The contributions of this paper can be summarized as follows:

(1) To the best of our knowledge, we are the first to explore PTQ with Transformer-based model in SR thoroughly. We design 2DQuant, a unique and efficient two-stage PTQ method (see Figure 2) for image super-resolution, which utilizes DOBI and DQC to optimize the bound from coarse to fine.

(2) In the first stage of post-quantization, we use DOBI to search for quantizer parameters, employing customized search strategies for different distributions to balance speed and accuracy. In the second stage, we design DQC, a more fine-grained optimization-based training strategy, for the quantized model, ensuring it aligns with the FP model on the calibration set.

Figure 1: Existing methods suffer from blurring artifacts.

(3) Our 2DQount can compress Transformer-based model to 4,3,2 bits with the compression ratio being 3.07\(\), 3.31\(\), and 3.60\(\) and speedup ratio being 3.99\(\), 4.47\(\), and 5.08\(\). No additional module is added so 2DQount enjoys the theoretical upper limit of compression and speedup.

(4) Through extensive experiments, our 2DQount surpasses existing SOTA on all benchmarks. We gain an increase in PSNR by as high as 4.52dB in Set5 (\( 2\)) when compressed to 2 bits, and our method has a more significant increase when compressed to lower bits.

## 2 Related work

Image super-resolution.Deep CNN networks have shown excellent performance in the field of image super-resolution. The earliest SR-CNN [10; 11] method adopted a CNN architecture. It surpassed previous methods in the image super-resolution domain. In 2017, EDSR  won the NTIRE2017  championship, becoming a representative work of CNNs in the SR by its excellent performance. Thereafter, with the continuous development of Vision Transformers (ViT) , models based on the ViT architecture have surpassed many CNN networks. These Transformer-based models achieve significant performance improvements and they have fewer parameters and lower computational costs. Many works have modified the ViT architecture, achieving continuous improvements. A notable example is SwinIR . With a simple structure, it outperforms many CNN-based models. However, previous explorations of post-quantization in the super-resolution domain have been limited to CNN-based models. They focus on models like EDSR  or SRResNet . It is a far cry from advanced models no matter in parameters, FLOPs, or performance. Currently, there is still a research gap in post-quantization for Transformer architectures.

Model quantization.Model quantization is used to compress large models [30; 28; 29] and can be divided into QAT and PTQ. QAT is widely accepted due to its minimal performance degradation. PAMS  utilizes a trainable truncated parameter to dynamically determine the upper limit of the quantization range. DAQ  proposed a channel-wise distribution-aware quantization scheme. CADyQ  is designed for SR networks and optimizes the bit allocation for local regions and layers in the input image. However, QAT usually requires training for as long as or even longer than the original model, which becomes a barrier for real scenarios deployment. Instead of training the model from scratch, existing PTQ methods use the pre-trained models. PTQ algorithms only find the clipping bound for quantizers, saving time and costs. DBDC+Pac  is the first to optimize the post-training quantization for image super-resolution task. It outperforms other existing PTQ algorithms. Whereas, they only focus on EDSR  and SRResNet . Their 4-bit quantized version is inferior to advanced models in terms of parameters and computational cost, let alone performance. It reveals a promising result for PTQ on SR, and using a more advanced model could bridge the gap between high-performance models and limited calculation resource scenarios.

## 3 Methodology

To simulate the precision loss caused by quantization, we use fake-quantize ,_i.e._quantization-dequantization, for activations and weights. and the process can be written as

\[v_{c}=(v,l,u), v_{r}=(-1}{u-l}(v_{c}-l) ), v_{q}=-1}v_{r}+l,\] (1)

Figure 2: The overall pipeline of our proposed 2DQount method. The whole pipeline contains two stages, optimizing the clipping bound from coarse to fine. In stage 1, we design D0BI to efficiently obtain the coarse bound. In stage 2, DQC is performed to finetune clipping bounds and guarantee the quantized model learns the full-precision (FP) modelâ€™s feature and output information.

where \(v\) denotes the value being fake quantized, which can be weight or activation. \(l\) and \(u\) are the lower and upper bounds for clipping, respectively. \((v,l,u)=((v,u),l)\), and Round rounds the input value to the nearest integer. \(v_{c}\) denotes the value after clipping, and \(v_{r}\) denotes the integer representation of \(v\), and \(v_{q}\) denotes the value after fake quantization. The Clip and Round operations contribute to reducing the parameters and FLOPs but also introduce quantization errors.

Figure 3 shows the basic structure of the Transformer block. We have quantized all the modules with a significant computational load within them, effectively reducing the model's FLOPs. Table 2 shows the FLOPs needed for each module. The Linear layers and matrix multiplication account for approximately 86% of the computation load, which are all transformed into integer arithmetic. When performing gradient backpropagation, we follow the Straight-Through Estimator  (STE) style:

\[}{ u}=}{ u}+-1}v_{r}--l}{u-l},}{ l}= }{ l}--1}v_{r}+-l}{u-l},\] (2)

where \(}{ u}=H(u-v)\) and \(}{ l}=H(l-v)\), \(H()\) denotes Heaviside step function . This formula approximates the direction of gradient backpropagation, allowing training-based optimization to proceed. The derivation of the formula can be found in the supplementary material.

Figure 2 shows the whole pipeline of 2DQuant, which is a **two**-stage coarse-to-fine post-training quantization method. The first stage is **DOBI, using **two** strategies to minimize the value shift while the second stage is **D**QC, optimizing **two** bound of each quantizer towards the task goal.

### Analysis of data distribution

To achieve better quantization results, we need to analyze the distribution of the model's weights and activations in detail. We notice that the data distribution shows a significantly different pattern from previous explorations, invalidating many of the previous methods. The weights and activations distribution of SwinIR is shown in Figure 4. More can be found in supplemental material. Specifically, the weights and activations of SwinIR exhibit noticeable long-tail, coexisting symmetry and asymmetry.

Weight.The weights of all linear layers are symmetrically distributed around zero, showing clear symmetry, and are generally similar to a normal distribution. This is attributed to the weight decay applied to weights, which provides quantization-friendly distributions. From the value shift perspective, both symmetric and asymmetric quantization are tolerable. Whereas, from the vantage point of task objectives, asymmetric quantization possesses the potential to offer a markedly enhanced information density, thus elevating the overall precision of the computational processes involved.

Activations.As for activations, they exhibit obvious periodicity in different Transformer Blocks. For V or the input of FC1, the obtained activation values are symmetrically distributed around 0. However, for the attention map or the input of FC2 in each Transformer Block, due to the Softmax calculation or the GELU  activation function, the minimum value is almost fixed, and the overall distribution is similar to an exponential distribution. Therefore, the data in SwinIR's weights and activations exhibit two distinctly different distribution characteristics. Setting asymmetric quantization and different search strategies for both can make the search rapid and accurate.

### Distribution-oriented bound initialization

Because the data distribution exhibits a significant long-tail effect, we must first clip the range to avoid low effective bits. Common clipping methods include density-based, ratio-based, and MSE-based approaches. The first two require manually specifying the clipping ratio, which significantly affects the clipping outcome and necessitates numerous experiments to determine the optimal ratio. Thus we proposed the Distribution-Oriented Bound Initialization (DOBI) to search the bound for weight and

   Module & FLOPs (G) & Ratio (\%) \\  Linear \& BMM & 14.34 & 85.66 \\ Conv & 2.33 & 13.90 \\ Other & 0.07 & 0.44 \\ Total & 16.74 & 100.00 \\   

Table 2: FLOPs distribution.

Figure 3: Quantization scheme for SwinIR Transformer blocks. Fake quantization and INT arithmetic are performed in all compute-intensive operators including all linear layers and batch matmul. Lower bits such as 3 or even 2 are also permitted. Dropout of attention and projection is ignored

activation, avoiding manually adjusting hyperparameters. The global optimizing goal is as follows

\[\{(l_{i},u_{i})\}_{i=1}^{N}=_{l_{i},u_{i}}_{i=1}^{N}\| v_{i}-v_{qi}\|_{2}.\] (3)

The collection of all quantizers' bounds \(\{(l_{i},u_{i})\}_{i=1}^{N}\) is the linchpin of quantized model performance as it indicates the candidate value for weights and activations. We note that the data distribution falls into two categories: one resembling a bell-shaped distribution and the other resembling an exponential distribution. For the bell-shaped distribution, we use a symmetric boundary-narrowing search method. Whereas, for the exponential distribution, we fix the lower bound to the minimum value of the data and only traverse the right bound. The specific search method is shown in Algorithm 1. The time complexity of Algorithm 1 is \((MK)\), where \(M\) is the number of elements in data \(v\) and \(K\) is the number of search points. The condition _v is symmetrical_ is obtained by observing the visualization of \(v\) and the activations are from the statistics on a small calibration set.

### Distillation quantization calibration

To further fine-tune the clipping range, we propose distillation quantization calibration (DQC) to transfer the knowledge from the FP model to the quantized model. It leverages the knowledge distillation  where the FP model acts as the teacher while the quantized model is the student. Specifically, for the same input image, the student model needs to continuously minimize the discrepancy with the teacher model on the final super-resolution output. The loss for the final output can be written as

\[_{O}=H_{O}W_{O}}\|O-O_{q}\|_{1},\] (4)

where \(O\) and \(O_{q}\) are the final outputs of the teacher and student models, \(C_{O}\), \(H_{O}\), and \(W_{O}\) represent the number of output channels, height, and width, respectively. we adopt the L1 loss for the final output, as it tends to converge more easily compared to the L2 loss . As the quantized model shares the same structure with the FP model and is quantized from the FP model, the student model also need to learn to extract the same feature of the teacher model, which can be written as

\[_{F}=_{i}^{N}H_{i}W_{i}}\|}{ \|F_{i}\|_{2}}-}{\|F_{qi}\|_{2}}\|_{2 },\] (5)

where \(F_{i}\) and \(F_{qi}\) are the intermediate features of the teacher and student models respectively and \(i\) is the index of the layer. In the field of super-resolution, there is a clear correspondence between the feature maps and the final reconstructed images, making training on feature maps crucial. since the quantized network

``` Data: Data to be quantized \(v\), the number of search point \(K\), bit \(b\) Result: Clip bound \(l\), \(u\) \(l(v)\),\(u(v)\); \(min\_mse+\); ifv is symmetricalthen \( l((v)-(v))/2K\); else \( l 0\); end if \( u((v)-(v))/2K\); while\(i K\)do \(l_{i} l+i l\), \(u_{i} u+i u\); get \(v_{q}\) based on Eq. (1); \(mse\|v-v_{q}\|_{2}\); if\(mse min\_mse\)then \(min\_mse mse\); \(l_{best} l_{i}\), \(u\_best u_{i}\);  end if  end while ```

**Algorithm 1**DOBI pipeline

Figure 4: The selected representative distribution of activations (Row 1) and weights (Row 2). The range of data is marked in the figure. All weights obey symmetric distribution. The attention map and the input of FC2 are asymmetric due to softmax function and GELU function.

and the full-precision network have identical structures, we do not need to add extra adaptation layers for feature distillation. The final loss function can be written as

\[=_{O}+_{F},\] (6)

where \(\) is the co-efficient of \(_{F}\). In the second stage, based on training optimization methods, the gap between the quantized model and the full-precision model will gradually decrease. The performance of the quantized model will progressively improve and eventually converge to the optimal range.

## 4 Experiments

### Experimental settings

Data and evaluation.We use DF2K [40; 34] as the training data, which combines DIV2K  and Flickr2K , as utilized by most SR models. During training, since we employ a distillation training method, we do not need to use the high-resolution parts of the DF2K images. For validation, we use the Set5  as the validation set. After selecting the best model, we tested it on five commonly used benchmarks in the SR field: Set5 , Set14 , B100 , Urban100 , and Manga109 . On the benchmarks, we input low-resolution images into the quantized model to obtain reconstructed images, which we then compared with the high-resolution images to calculate the metrics. We do not use self-ensemble in the test stage as it increases the computational load eightfold, but the improvement in metrics is minimal The evaluation metrics we used are the most common metrics PSNR and SSIM , which are calculated on the Y channel (_i.e._, luminance) of the YCbCr space.

Implementation details.We use SwinIR-light  as the backbone and provide its structure in the supplementary materials. We conduct comprehensive experiments with scale factors of 2, 3, and 4 and with 2, 3, and 4 bits, where Our hyperparameter settings remain consistent. During DOBI, we use a search step number of K=100, and the statistics of activations are obtained from 32 images in DF2K being randomly cropped to retain only 3\(\)64\(\)64. During DQC, we use the Adam  optimizer with a learning rate of 1\(\)10\({}^{-2}\), betas set to (0.9, 0.999), and a weight decay of 0. We employ CosineAnnealing  as the learning rate scheduler to stabilize the training process. Data augmentation is also performed. We randomly utilize rotation of 90\({}^{}\), 180\({}^{}\), and 270\({}^{}\) and horizontal flips to augment the input image. The total iteration for training is 3,000 with batch size of 32. Our code is written with Python and PyTorch  and runs on an NVIDIA A800-80G GPU.

### Comparison with state-of-the-art methods

The methods we compared include MinMax , Percentile , and the current SOTA post-quantization method in the super-resolution field, DBDC+Pac . For a fair comparison, we report the performance of DBDC+Pac  on EDSR , as the authors performed detailed parameter adjustments and model training on EDSR. We directly used the results reported by the authors, recorded in the table as EDSR\({}^{}\). It should be noted that the EDSR method uses self-ensemble in the final test, which can improve performance to some extent but comes at the cost of 8 times the computational load. Additionally, we applied DBDC+Pac  to SwinIR-light , using the same hyperparameters as those set by the authors for EDSR, recorded in the table as DBDC+Pac . The following are the quantitative and qualitative results of the comparison.

Quantitative results.Table 3 shows the extensive results of comparing different quantization methods with bit depths of 2, 3, and 4, as well as different scaling factors of \( 2\), \( 3\), and \( 4\).

DBDC+Pac  performs poorly mainly because **1.** The DBDC process requires manually specifying the clipping ratio, which significantly affects performance. **2.** DBDC does not prune weights, and the learning rate in the Pac process is too low, causing slow convergence of weight quantizer parameters. However, both adverse factors are eliminated in our 2DQuant algorithm. When using only DOBI algorithm, our performance has already reached a level comparable to that of DBDC+Pac algorithms. Upon applying DQC, our performance experienced a remarkable and discernible enhancement, elevating it to new heights. In the case of \( 2\), 4-bit on Set5 and Urban100, DOBI has an improvement of 1.11dB and 0.39 dB compared to EDSR, while

Figure 5: The bound percentile of DOBI and DQC.

2DQuant has an improvement of 0.69 dB and 1.18 dB compared to the SOTA method. All these results indicate that our two-stage PTQ method can effectively mitigate the degradation caused by quantization and ensure the quality of the reconstructed images.

Figure 5 shows the bound percentile of DOBI searching and DQC. Overall, the bound of DQC is tighter as the values around the zero point enjoy greater importance. Besides, the shallow layers'

    &  &  &  &  &  &  \\  & & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) \\  SwinIR-light  & 32 & 38.15 & 0.9611 & 29.36 & 0.9206 & 32.31 & 0.9012 & 32.76 & 0.9340 & 39.11 & 0.9781 \\ Bicubic & 32 & 32.25 & 0.9118 & 29.25 & 0.8406 & 28.68 & 0.8104 & 25.96 & 0.8088 & 29.17 & 0.9128 \\  MinMax  & 4 & 34.39 & 0.9202 & 30.55 & 0.8512 & 29.72 & 0.8409 & 28.40 & 0.8520 & 33.70 & 0.9411 \\ Percentile  & 4 & 37.37 & 0.9568 & 32.96 & 0.9113 & 31.61 & 0.8917 & 31.17 & 0.9180 & 37.19 & 0.9714 \\ EDSR\({}^{}\) & 4 & 36.33 & 0.9420 & 32.75 & 0.9040 & 31.48 & 0.8840 & 30.90 & 0.9130 & N/A & N/A \\ BDDC+Pae  & 4 & 37.18 & 0.9550 & 32.86 & 0.9106 & 31.56 & 0.8908 & 30.66 & 0.9110 & 36.76 & 0.9692 \\ DOBI (Ours) & 4 & 37.44 & 0.9568 & 33.15 & 0.9132 & 31.75 & 0.8937 & 31.29 & 0.9193 & 37.93 & 0.9743 \\
2DQuant (Ours) & 4 & 37.87 & 0.9594 & 33.41 & 0.9161 & 32.02 & 0.8971 & 31.84 & 0.9251 & 38.31 & 0.9761 \\  MinMax  & 3 & 28.19 & 0.6961 & 26.40 & 0.6478 & 25.83 & 0.6225 & 25.19 & 0.6773 & 28.97 & 0.7740 \\ Percentile  & 3 & 43.37 & 0.9170 & 31.04 & 0.8646 & 29.82 & 0.8339 & 28.25 & 0.8417 & 33.43 & 0.9214 \\ BDDC+Pae  & 3 & 35.07 & 0.9350 & 31.52 & 0.8873 & 30.47 & 0.8665 & 28.44 & 0.8709 & 34.01 & 0.9487 \\ DOBI (Ours) & 3 & 36.37 & 0.9496 & 32.33 & 0.9041 & 31.12 & 0.8836 & 29.65 & 0.8967 & 36.18 & 0.9661 \\
2DQuant (Ours) & 3 & 37.32 & 0.9567 & 32.85 & 0.9106 & 31.60 & 0.8911 & 30.45 & 0.9086 & 37.24 & 0.9722 \\  MinMax  & 2 & 33.88 & 0.9185 & 30.81 & 0.8748 & 29.99 & 0.8535 & 27.48 & 0.8501 & 31.86 & 0.9306 \\ Percentile  & 2 & 30.82 &bounds vary more significantly due to the elevated significance of these layers within the neural network. Detailedly, the bound for the second MLP fully connected layer's weight in Layer 0 Block 1 only remains 46% data in its range. It has the second-highest lower bound percentile and the smallest upper bound percentile among the network. Its percentiles are 0.2401 and 0.7035 respectively this bound values are -0.062 and 0.047 and its distribution is visualized in Figure 4. In conclusion, only through task-oriented optimization of each bound at a fine-grained level can redundant information be maximally excluded and useful information be maximally retained.

Qualitative results.We show the visual comparison results for \( 4\) in Figure 6. Since quantized models are derived from full-precision models with information loss, their global performance will rarely exceed that of full-precision models. As seen in the three images for Minmax, after quantization, if no clipping is performed, the long tail effect will lead to a large number of useless bits, resulting in a significant amount of noise and repeated distorted patterns in the reconstructed images. In these challenging cases, our training method allows the model to retain edge information of objects better, preventing blurring and distorted effects. For example, in img_046 and img_023, we have the highest similarity to the full-precision model, while other methods show varying degrees of edge diffusion, significantly affecting image quality. Compared to the DBDC+Pac method, our DOBI and DQC allow for better representation of edge and texture information in the images and effectively avoid distortions and misalignments in the graphics. The visual results demonstrate that our proposed DQC is essential for improving performance in both metric and visual comparisons.

### Ablation study

Learning rate and batchsize.We first study the performance variations of the model under different hyperparameters. From Tables 3(a) and 3(b), it can be seen that our DQC enables the model to

Table 4: Ablation studies. The models are trained on DIV2K and Flickr2K, and tested on Set5 (\( 2\)).

Figure 6: Visual comparison for image SR (\( 4\)) in some challenging cases.

converge within a range of outstanding performance for most learning rates and batch sizes. Due to the non-smooth impact of quantization parameters on the model, the quantized model is more prone to local optima compared to the full-precision model, resulting in a noticeable performance drop when the learning rate is too low. Additionally, as shown in Table 3(b), the larger the batch size, the better the model's performance, and the smoother the convergence process. However, even with a smaller batch size, we can still achieve a performance of 37.82dB on Set5, indicating that our two-stage method has good robustness to different hyperparameters.

DOBI and DQC.Moreover, we also study the impact of different stages on performance, with the results shown in Table 3(c), from which we can draw the following conclusions: **Firstly**, the goal of DOBI is to minimize the value shift for weights and activations. Although it is not the task goal, it can still enjoy significant enhancement due to better bit representational ability. **Secondly**, DQC alone cannot achieve the optimization effect of DOBI. This is because the impact of quantizer parameters on model performance is oscillatory, and training alone is prone to converge to local optima. In contrast, search-based methods can naturally avoid local optima. So it's necessary to use results from the search-based method to initialize training-based method in PTQ. **Thirdly**, when DOBI and DQC are combined, namely our 2DQant, the 4-bit quantized model has only a 0.28dB decrease on Set5 compared to the FP model, which maximally mitigates the accuracy loss caused by quantization.

## 5 Discussion

Why our results surpass FP outcomes.While our method's performance metrics do not yet fully match those of full-precision models, visual results reveal a compelling advantage. As observed in image img_092 of Figure 1 of Urban100, our approach correctly identifies the direction of the stripes in the image. Whereas the full-precision model erroneously selects the wrong direction. This discrepancy arises because the lower-resolution image, affected by aliasing, creates an illusion of slanted stripes, misleading the FP model's reconstruction. This phenomenon demonstrates that our PTQ algorithm allows more accurate restored results in certain localized and challenging tasks without being misled. More examples are in the supplementary materials.

It suggests that full-precision models contain not only redundant knowledge but also incorrect information. The latter is hard to get rid of by training the FP model. Our quantization method can effectively reduce model parameters and computational demands while eliminating erroneous information, achieving multiple benefits simultaneously. This also suggests that the FP model doesn't represent the pinnacle of what a quantized model can achieve.

Limitations.Despite achieving excellent results, this study still has some limitations. During the DOBI process, the data distribution of activations and weights is required to approximate a bell curve or exponential distribution; otherwise, the DOBI method cannot find the most suitable positions. Additionally, increasing the number of search points for a single tensor in MSE does not necessarily guarantee better performance. However, the second-stage training can somewhat alleviate this issue. Moreover, our method requires a calibration set; without which, the first-stage DOBI and the second-stage DQC cannot be carried out at all.

Societal impacts.Our super-resolution quantization method effectively saves computational resources, facilitating the deployment of super-resolution models at the cutting edge

## 6 Conclusion

This paper studies the post-training quantization in the field of image super-resolution. We first conducted a detailed analysis of the data distribution of Transformer-based model in SR. These data exhibit a clear long-tail effect and symmetry and asymmetry coexisting effect. We designed 2DQant, a dual-stage PTQ algorithms. In the first stage DOBI, we designed two different search strategies for the two different distributions. In the second stage DQC, we designed a distillation-based training method that let the quantized model learn from the FP model, minimizing the accuracy loss caused by quantization. Our 2DQant can compress Transformer-based model to 4,3,2 bits with the compression ratio being 3.07\(\), 3.31\(\), and 3.60\(\) and speedup ratio being 3.99\(\), 4.47\(\), and 5.08\(\). No additional module is added so 2DQant enjoys the theoretical upper limit of compression and speedup. Extensive experiments demonstrate that 2DQant surpasses all existing PTQ methods in the field of SR and even surpasses the FP model in some challenging cases. In the future, recognizing the significant impact of the model on performance, we will conduct PTQ research on more advanced super-resolution models and attempt to deploy quantized super-resolution algorithms to actual photography tasks, providing a more detailed evaluation of the performance of PTQ algorithms.