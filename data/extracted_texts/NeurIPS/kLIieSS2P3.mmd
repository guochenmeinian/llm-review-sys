# On the Choice of Perception Loss Function for Learned Video Compression

Sadaf Salehkalaibar

ECE Department

University of Toronto

sadafs@ece.utoronto.ca

&Buu Phan

ECE Department

University of Toronto

truong.phan@mail.utoronto.ca

&Jun Chen

ECE Department

McMaster University

chenjun@mcmaster.ca

&Wei Yu

ECE Department

University of Toronto

welyu@ece.utoronto.ca

&Ashish Khisti

ECE Department

University of Toronto

akhisti@ece.utoronto.ca

Equal Contribution

###### Abstract

We study causal, low-latency, sequential video compression when the output is subjected to both a mean squared-error (MSE) distortion loss as well as a perception loss to target realism. Motivated by prior approaches, we consider two different perception loss functions (PLFs). The first, PLF-JD, considers the joint distribution (JD) of all the video frames up to the current one, while the second metric, PLF-FMD, considers the framewise marginal distributions (FMD) between the source and reconstruction. Using information theoretic analysis and deep-learning based experiments, we demonstrate that the choice of PLF can have a significant effect on the reconstruction, especially at low-bit rates. In particular, while the reconstruction based on PLF-JD can better preserve the temporal correlation across frames, it also imposes a significant penalty in distortion compared to PLF-FMD and further makes it more difficult to recover from errors made in the earlier output frames. Although the choice of PLF decisively affects reconstruction quality, we also demonstrate that it may not be essential to commit to a particular PLF during encoding and the choice of PLF can be delegated to the decoder. In particular, encoded representations generated by training a system to minimize the MSE (without requiring either PLF) can be _near universal_ and can generate close to optimal reconstructions for either choice of PLF at the decoder. We validate our results using (one-shot) information-theoretic analysis, detailed study of the rate-distortion-perception tradeoff of the Gauss-Markov source model as well as deep-learning based experiments on moving MNIST and KTH datasets. Code will be available at https://github.com/truongbuu/URDP_flow.

## 1 Introduction

There is an increasing demand for video compression algorithms that are able to generate visually pleasing videos at low bitrates. Most of the current video codecs use distortion measures such as PSNR , MSE and MS-SSIM  to generate reconstructions which tend to be blurry at extremely low bitrates. In recent years, there has been a growing interest (see e.g., ) in using deep generative models to make the reconstructions look more realistic. Such techniques introduce an additional perception loss function that measures a distance between distributions of the source and reconstruction, with _perfect_ perception corresponding requiring that the two distributions be identical.

In compression systems, improving realism comes at the price of increasing distortion. The work of Blau and Michaeli  establishes the theoretical rate-distortion-perception (RDP) tradeoff which has also been validated in [12; 13; 14; 15]. Furthermore _universal_ encoded representations were proposed in  where the representation is fixed at the encoder and the decoder is adapted to achieve a performance near the optimal RDP tradeoff curve. The extension of these works to video compression involves many challenges. First, the compression system must not only account for spatial redundancy as in image compression, but also exploit the temporal redundancy across video frames, making the system design more complex. Secondly, unlike the case of image compression, there may be no clear choice of the perception loss function (PLF). Indeed, some prior works  consider PLF that preserves framewise marginal distribution (PLF-FMD) between the source and reconstruction, while other works consider joint distribution (PLF-JD) across multiple frames .

As illustrated in Fig. 0(a), we study causal, low-latency, sequential video compression when the output is subjected to both a mean squared-error (MSE) distortion loss and either a PLF-JD or PLF-FMD metric for perception loss. Our contributions are as follows:

* _Differences in reconstruction quality based on the choice of PLF_: We demonstrate that the choice of PLF can decisively affect the reconstruction quality especially in the low bit-rate regime. We approximately characterize the operational RDP region on a per-frame basis for a first-order Markov source model and analyze the special case of Gauss-Markov sources in detail. We show that there is a significant penalty in distortion when using PLF-JD in the low-rate regime. On the experimental side, we demonstrate that while PLF-JD preserves better temporal consistency across video frames, it suffers from the _permanence of error_ phenomenon in which the mistakes in reconstructions propogate to future frames. On the other hand, the PLF-FMD metric shows

Figure 1: (a) Proposed System Model (b, c) Error permanence phenomenon under different PLFs. High fidelity but incorrect I-frame reconstruction propagates the error to subsequent P-frames in 0-PLF-JD reconstructions. The MMSE and 0-PLF-FMD reconstructions do not have this problem.

more capability in correcting mistakes across frames (see Fig. 1b for visualizations involving three-frame videos). On the other hand, if the first frame is transmitted at high bit-rate, we demonstrate that PLF-JD performs better than PLF-FMD.
* _Universality of minimum mean square error (MMSE) reconstructions_: We demonstrate that encoded representations generated from an encoder trained to minimize MSE reconstruction (without considering any PLF) suffice to produce close-to-optimal reconstructions for either choice of PLF. For general sources, we show that when using PLF-FMD, the MMSE reconstruction can be transformed to a reconstruction satisfying perfect perceptual quality by increasing the distortion at most by a _factor of two_. While a similar result does not hold for PLF-JD in general, it is satisfied for a special class of encoders which operate in the low-rate regime. For the Gauss-Markov source model we demonstrate exact universality i.e., encoded representation for the MMSE reconstruction can be adapted to achieve any other reconstruction in the RDP region. We also use deep learning based experiments to provide experimental evidence of these results. We note that the above notion of universal encoded representations based on MSE reconstruction can have significant advantages in practice. First, although the reconstructions associated with different choices of PLFs can be visually very different, universal representations delegate the choice of PLF to the decoder rather than requiring the encoder to commit to a specific PLF. Secondly, the universality of MMSE reconstructions is far more significant in the context of learned video compression. Given that perceptual reconstruction frequently generates novel details not present in the source frame, compressing motion flow vectors between the current frame and the prior perceptual reconstruction necessitates a higher bit allocation compared to utilizing the MMSE reconstruction. Hence, a recommended approach is to train end-to-end compression exclusively to minimize MSE and use our proposed scheme to achieve a (near-optimal) tradeoff between the distortion and perception losses as desired by the user.

The study of RDP region for learned video compression is considerably more challenging than the study of RDP function for a single frame in prior works (). This is because of the fact that the RDP region (for first-order Markov sources) involves a tradeoff between the compression rate assigned to each frame imposing a Markov structure on the reconstructions. For Gaussian sources, the proof of optimality of Gaussian reconstructions does not use the closed form of the RDP region due to its complexity. As a result, the study of RDP region for various operating regimes is quite more involved. Furthermore, for the proof of universality, one has to consider the achievability of the entire RDP region as opposed to just points on the boundary of RDP function in . Finally the results on the fixed-encoder setup are more general than prior works () that required a characterization of the information RDP region, which we do not require.

#### Related Work

_Perceptual Lossy Video Compression._ Distribution preserving framework using Generative Adversarial Networks (GAN) has been widely adopted as a surrogate metric for perceptual quality in image  and, recently, video compression . Unlike image compression, where the choice of PLF is straightforward, there is currently no agreed-upon objective for lossy video compression. For instance, DVC-P  employs PLF-FMD to improve the visual quality per frame, ignoring the temporal coherence. Similarly, Mentzer et al.  utilize the per-frame metric with a conditional GAN model, and found no significant differences when using a GAN objective with multiple frames. Other works target temporal consistency by incorporating multiple frames in their GAN objective. This includes the work by Yang et al. , where every two consecutive frames are included, and by Veerabadran et al. , where they employ the PLF-JD metric in a non-causal setting. Unlike previous works, we study the impact of two different perception objectives, i.e. PLF-JD and PLF-FMD, on reconstructions in the causal setting, presenting theoretical properties that are verified by deep learning experiments. For the PLF-JD metric, we demonstrate the _error permanence phenomenon_, which, unlike the error propagation issue , cannot be resolved by increasing the code rate assigned to the \(P\) frames.

_RDP Tradeoff and Principle of Universality._ Targeting the distribution preserving framework in lossy image compression, several theoretical works have shown the presence of RDP tradeoff , where perfect perception comes at a cost of increasing distortion by at most a factor of 2. Furthermore, an encoder that generates universal representations exists , which enables the decoder to freely choose the level of distortion-perception tradeoff it desires. Our work explores these avenuesin the context of causal video compression. As discussed previously we show that the MMSE representation can be used as a universal representation for both perception metrics which has several advantages in the context of video compression..

## 2 System Model

Let \((X_{1},,X_{T})_{1}_{T}\) be \(T\) frames in a video (with each \(_{i}^{d}\)) distributed according to \(P_{X_{1} X_{T}}\). The frames are available for encoding sequentially; \(X_{1}\) is available first, then \(X_{2}\) arrives, followed by \(X_{3}\) and so on. There is a shared randomness \(K\) which is available at all encoders and decoders. The following (possibly stochastic) mappings define the encoding and decoding functions:

\[f_{j} _{1}_{j} _{j}, j=1,,T,\] (1) \[g_{j} _{1}_{2} _{j}}_{j},\] (2)

where \(_{j}\{0,1\}^{}\) denotes the set of (variable-length) messages assigned by the \(j\)th encoder and \(}_{j}^{d}\) is the \(j\)-th reconstruction alphabet (see Fig. 1a). Let \(P_{_{1}_{T}|X_{1} X_{T}}\) be the conditional distribution of the reconstructed video given the original video which is basically determined by the mappings \(\{f_{j}\}_{j=1}^{T}\) and \(\{g_{j}\}_{j=1}^{T}\). The above setting is a _one-shot_ setup as only a single source sample is compressed at a time. For each frame \(j\), a distortion metric is imposed on the output, which we assume throughout is the mean squared-error (MSE) function i.e. \(d(x_{j},_{j})=||x-_{j}||^{2}\), which is commonly used in many applications. From a perceptual point of view, for given probability distributions \(P_{X_{1} X_{j}}\) and \(P_{_{1}_{j}}\) on the original and reconstructed frame \(j\), let \(_{j}(P_{X_{1} X_{j}},P_{_{1}_{j}})\) be the perception function capturing the difference between them. Note that the function \(_{j}\) is defined based on the joint distribution of all first \(j\) frames. We call this metric as _perception loss function based on joint distribution (PLF-JD)_. Note that when \(_{j}(P_{X_{1} X_{j}},P_{_{1}_{j}})=0\), we have:

\[P_{X_{1} X_{j}}=P_{_{1}_{j}}, j=1,,T.\] (3)

We refer to this case as _zero-perception loss function based on joint distribution (\(0\)-PLF-JD)_. Alternatively, the _perception loss function based on framewise marginal distribution (PLF-FMD)_ is denoted by \(_{j}(P_{X_{j}},P_{_{j}})\) and is based on only the marginal distribution of the \(j\)-th frame. In particular, note that \(0\)-PLF-FMD implies that \(P_{X_{j}}=P_{_{j}}\) for each \(j\). In most of the paper, for simplicity of presentation, we provide some of our results for \(T=3\) frames. In that case, we use the shorthand notation \(\) to denote the tuple \((X_{1},X_{2},X_{3})\), e.g., \(:=(M_{1},M_{2},M_{3})\), \(:=(D_{1},D_{2},D_{3})\), \(:=(f_{1},f_{2},f_{3})\).

## 3 Distortion Analysis for a Fixed Encoder and Zero-perception Loss

In this section, we assume that the encoding functions \(\) are fixed, but the decoding functions \(\) can be optimized to generate different reconstructions. Equivalently, the distribution \(P_{|K}=1\{=(,K)\}\) is fixed, while by varying the reconstruction distribution \(P_{}|K}=1\{}=(,K)\}\), one attains different reconstructions \(}\), where \(1\{.\}\) denotes the indicator function. Furthermore defining \(D_{j}=_{P}[\|X_{j}-_{j}\|^{2}]\), we denote \(\) as the achievable distortion tuple associated with \(P_{}|K}\).

One natural choice of reconstructions is the minimum mean squared error (MMSE) reconstruction function. At step \(j\), the reconstruction, which we denote in this case by \(_{j}\), is obtained by taking the conditional expectation of \(X_{j}\) given all information at the decoder up to time \(j\) i.e., \(_{j}=_{P}[X_{j}|M_{1} M_{j},K]\) for each \(j=1,2,3\). It is well known that the MMSE reconstruction functions minimize the reconstruction distortion i.e., if we define the set

\[_{^{}}(P_{|K})=\{:D_{j} _{P}[\|X_{j}-_{j}\|^{2}], j=1,2,3\}\] (4)

then the distortion tuple \(\) associated with any reconstruction \(P_{}|K}\) satisfies \(_{^{}}(P_{|K})\).

The main result of this section is that assuming fixed encoder, the achievable distortions under \(0\)-PLF-FMD is at most twice of that under the MMSE distortion loss alone. The same conclusion also holds for 0-PLF-JD for a class of encoders operating at low rate. We first consider the case of \(0\)-PLF-FMD.

**Definition 1** (\(0\)-Plf-Fmd Distortion): _For an encoder \(P_{|K}\), the set \(_{^{0}}(P_{|K})\) denotes the set of all distortion tuples \(\) for which there exists a reconstruction \(P_{}|K}\) satisfying \(P_{X_{j}}=P_{}_{j}}\) for each \(j\{1,2,3\}\)._

**Theorem 1**: _The set \(_{^{0}}(P_{|K})\) is characterized as follows:_

\[_{^{0}}(P_{|K})=\{:D_{j} _{P}[\|X_{j}-_{j}\|^{2}]+W_{2}^{2}(P_{_{j}},P_{X_{ j}}),\ j=1,2,3\},\] (5)

_where \(W_{2}^{2}(P_{X_{j}},P_{_{j}})\) denotes the Wasserstein-2 distance between the two distributions . Furthermore, we also have that:_

\[_{^{0}}(P_{|K})\{:D_{j}  2_{P}[\|X_{j}-_{j}\|^{2}],\ \ \ j=1,2,3\},\] (6)

_i.e., minimum achievable distortion with \(0\)-PLF-FMD is at most twice the MMSE distortion._

_Proof:_ See Appendix A.

We remark that the proof of Theorem 1, operationally demonstrates that the MMSE reconstruction can be converted to another reconstruction satisfying \(0\)-PLF-FMD with at-most a factor of \(2\) increase in distortion, generalizing the result in  for the single frame scenario (see also ).

We next consider the case when zero perception loss is satisfied under the PLF-JD metric. Analogous to \(_{^{0}}(P_{|K})\) in Definition 1, one can define \(_{^{0}}^{}(P_{|K})\) to be the set of distortions associated with reconstruction functions that satisfy (3). The analysis of \(_{^{0}}^{}(P_{|K})\) is discussed in Appendix B as it is more involved. In general, the _factor of two bound_ as in Theorem 1 cannot be realized in this case as demonstrated by a counter-example in Appendix B. Nevertheless, for a special family of encoders we can obtain a counterpart of Theorem 1. In this family of encoders, the source \(X_{j}\) at time \(j\) is nearly independent of the encoder outputs up to and including time \(j\), i.e., we can express:

\[P_{X_{j}|M_{1} M_{j}K}^{}=(1-)P_{X_{j}}+ Q_{X_{j}|M_{1 } M_{j}K}^{}, j=1,2,3.\] (7)

where \(\) is a sufficiently small constant and the distribution \(Q^{}(.)\) could be arbitrary conditional distribution with same marginal as \(P_{X_{j}}\). We note that such encoders are studied in a variety of problems in information theory (see e.g., ) that correspond to the low rate operating regime. The following result states that the factor-two bound holds approximately for such encoders.

**Theorem 2**: _For the class of encoders given by (7), we have_

\[_{^{0}}^{}(P_{|K}^{ })\{:D_{j} 2_{P^{}}[\|X_{j}- _{j}\|^{2}]+O(),\ \ \ j=1,2,3\}.\] (8)

_Proof:_ See Appendix C.

We note that the low-rate operating regime is practically important, as at higher rates MMSE based reconstructions can suffice and the use of PLF metrics may be less relevant.

## 4 Rate-Distortion-Perception Region

In this section, we assume that both the encoder \(P_{|K}\) as well as the reconstruction \(P_{}|K}\) can be optimized and study the associated rate-distortion-perception (RDP) tradeoff. We remind the reader that for PLF-JD and PLF-FMD, the PLFs are denoted by \(_{j}(P_{X_{1} X_{j}},P_{}_{1}}_ {j}})\) and \(_{j}(P_{X_{j}},P_{}_{j}})\), respectively. In this case, the operational RDP region in the one-shot setting is defined as follows.

**Definition 2** ( Operational RDP region): _For a given \(P_{}\), an RDP tuple \((,,)\) is said to be achievable for the one-shot setting if there exist an encoder \(P_{|K}\) and a reconstruction \(P_{}|K}\) satisfying:_

\[[(M_{j})] R_{j},\ \ \ [\|X_{j}-_{j}\|^{2}] D_{j},\ \ \ _{j}(P_{X_{1} X_{j}},P_{_{1}_{j}}) P_{j},\ \ j=1,2,3,\] (9)

_where \((M_{j})\) denotes the length of the message \(M_{j}\). The closure of the set of all achievable tuples, denoted by \(_{}^{o}\), is the operational RDP region. Moreover, for a given \((,)\), the operational DP rate region, denoted by \(^{o}(,)\), is the closure of the set of all tuples \(\) such that \((,,)_{}^{o}\)._

The region \(_{}^{o}\) cannot be directly computed as it involves all possible one-shot encoders/decoders. But for first-order Markov source, it has a tractable approximation in terms of mutual information.

### RDP Region of First-Order Markov Sources

We first define the first-order Markov sources and then introduce an iRDP region which is computable.

**Definition 3**: _We call \(\) as a first-order Markov source if the Markov chain \(X_{1}{}X_{2}{}X_{3}\) holds._

**Definition 4** (Information RDP region): _For first-order Markov sources, the information RDP (iRDP) region, denoted by \(_{}\), is the set of all tuples \((,,)\) which satisfy the following_

\[R_{1} I(X_{1};X_{r,1}), R_{2} I(X_{2};X_{r,2}|X_{ r,1}), R_{3} I(X_{3};X_{r,3}|X_{r,1},X_{r,2})\] (10) \[D_{j}[\|X_{j}-_{j}\|^{2}], P_{j} _{j}(P_{X_{1} X_{j}},P_{_{1}_{j}}), j=1,2,3,\] (11)

_for auxiliary random variables \((X_{r,1},X_{r,2},X_{r,3})\) and \((_{1},_{2},_{3})\) satisfying the following_

\[_{1} = _{1}(X_{r,1}),\ \ _{2}=_{2}(X_{r,1},X_{r,2}),\ \ _{3}=X_{3,r},\] (12) \[X_{r,1}  X_{1}(X_{2},X_{3}),\ \ X_{r,2}(X_{2},X_{r,1}) (X_{1},X_{3}),\] (13) \[X_{r,3}  (X_{3},X_{r,1},X_{r,2})(X_{1},X_{2}),\] (14)

_for some deterministic functions \(_{1}(.)\) and \(_{2}(.,.)\). Moreover, for a given \((,)\), the information DP (iDP) rate region, denoted by \((,)\), is the closure of the set of all tuples \(\) that \((,,)_{}\)._

The expression for the iRDP region involves a search over auxiliary random variables \(_{r}\) and \(}\) that satisfy (12)-(14) subject to the constraints in (10)-(11). For first-order Markov sources, the following theorem states that the operational DP rate region can be approximated by the iDP rate region.

**Theorem 3**: _For first-order Markov sources, a given \((,)\) and \((,)\), we have_

\[+(+1)+5^{o}(, )(,).\] (15)

_Proof:_ See Appendix D. \(\)

From Theorem 3, it follows that \((,)\) with overhead \((+1)+5\) is an inner bound to \(^{o}(,)\). On the other hand, \((,)\) provides an outer bound to \(^{o}(,)\). The two bounds match with each other in high rates. It can be shown that the overhead also vanishes in the large-blocklength setting where multiple symbols are encoded at a time. In the remainder of this paper, we will approximate \(^{o}(,)\) with \((,)\) and use the latter region for our analysis.

**Remark 1**: _(Encoded Representations): The proof of the inner bound in Theorem 3 in Appendix D provides an operational interpretation to the auxiliary random variables \(_{r}=(X_{r,1},X_{r,2},X_{r,3})\) defined in iRDP region in Definition 4. In particular, \(X_{r,j}\) is a lossy version of the source sample \(X_{j}\) generated by the encoder in step \(j\). It is compressed and transmitted to the decoder at rate \(R_{j}\) in (10). We refer to \(_{r}\) as the encoded representation of the source \(\). The Markov chains (13)-(14) indicate that without loss of optimality, an encoded representation \(X_{r,j}\) can be computed from the source \(X_{j}\) and past reconstructions \(X_{r,1},,X_{r,j-1}\) without using past source samples \(X_{1},,X_{j-1}\)._

**Remark 2**: _(Deterministic Reconstructions): Note that the reconstruction functions generating \(}\) in Definition 4 are deterministic functions of the encoded representations (c.f. (12)). In particular, the shared randomness \(K\) is not required in the reconstruction functions. However, as the proof of the inner bound of Theorem 3 illustrates, the shared randomness is required in the compression and construction of \(X_{r,j}\). Moreover, by following the arguments in , one can set the reconstruction function of the last frame to be identity. Thus, in Definition 4, we have set \(_{3}=X_{r,3}\) in (12) where \(T=3\). In the sequel, for \(T\) frames we will set \(_{T}=X_{r,T}\)._

**Remark 3**: _The result in Theorem 3 also holds for the PLF-FMD. That is, one can replace the PLF in (9) and (11) by \(_{j}(P_{X_{j}},P_{_{j}})\) and get a similar result (see Appendix D for the justification)._

### Gauss-Markov Source Model: RDP Region

In this section, we obtain practical insights through the analysis of the special case of first-order Gauss-Markov sources. We assume that \(X_{1}(0,_{1}^{2})\),

\[X_{2}=_{1}}{_{1}}X_{1}+N_{1}, X_{3}=_{2} }{_{2}}X_{2}+N_{2},\] (16)where \(N_{j}\) is independent of \(X_{j}\) with mean zero and variance \((1-_{j}^{2})_{j+1}^{2}\) for \(j=1,2\). Note that the model extends naturally to the case of \(T\) time-steps. The perception metric is assumed to be the Wasserstein-2 distance, i.e., \(_{j}(P_{X_{1} X_{j}},P_{_{1}_{j}}):=W_{2}^{2}(P_{ X_{1} X_{j}},P_{_{1}_{j}})\). For the PLF-FMD, the perception metric is given by \(_{j}(P_{X_{j}},P_{_{j}}):=W_{2}^{2}(P_{X_{j}},P_{_{j}})\).

The following result states that for Gaussian sources, jointly Gaussian reconstructions are optimal. Thus, for a given tuple \((,)\), the characterization of \((,)\) becomes computable.

**Theorem 4**: _For the Gauss-Markov source model, any tuple \((,,)_{}\) can be attained by a jointly Gaussian distribution over \(_{r}\) and identity mappings for \(_{j}()\) in Definition 4._

_Proof:_ See Appendix E.

Generally, the optimized distribution in the above theorem may not admit a simple form. In the special case of \(T=2\) frames, the optimal reconstructions are given in Appendix E. To obtain practical insights, we consider various asymptotic operating regimes and provide a detailed analysis in Appendix F for the case of \(T=2\) frames and with \(_{1}^{2}=_{2}^{2}\). A summary of these results is provided in Table 2 in the same Appendix. We briefly summarize some of these results next.

### Gauss-Markov Source Model: Extremal Rates

One of the key observations of this paper is that the choice of PLF has implication on the rate allocation across different frames. Specifically, first consider the case when both \(R_{1}{=}R_{2}\) are small i.e., \(R_{1}{=}R_{2}{=}\) (for small enough \(\)). We discuss how each PLF affects the reconstruction in the second step. In the first step, we note that reconstruction in both cases must be identical and of the form \(_{1}^{G}{=}X_{1}{+}Z_{1}\) where \(Z_{1}{}(0,(1{-}2 2)^{2})\) is independent of \(X_{1}\); the resulting distortion is given by \(D_{1}{=}2(1{-})^{2}\). However, the reconstructions in the second steps will be different for the two measures. For simplicity, we consider the extreme case when \(1\) (i.e., when \(X_{2}{=}X_{1}\)). Here, the PLF-JD metric is required to preserve perfect correlation and thus has to set \(_{2}^{G}{=}_{1}^{G}\) and results in \(D_{2}{=}D_{1}\). In other words, the decoder in the second step is unable to use any information transmitted in the second step as \(0\)-PLF-JD enforces the stringent constraint \(_{2}^{G}{=}_{1}^{G}\). In contrast, for the PLF-FMD metric, it can be shown that the reconstruction in the second step for \(1\) reduces to \(_{2}^{G}{=}X_{1}{+}Z_{}\) and the associated distortion is given by \(D_{2}{=}2(1{-})^{2}\), which is lower than PLF-JD. Extending this example to \(T\) steps (with \(1\)), we note that PLF-JD will always be forced to output \(_{1}\), while the reconstruction using PLF-FMD will successively improve. The following theorem formalizes this observation.

**Theorem 5**: _For sufficiently small \(\), let \(R_{j}=\) and suppose that \(_{j}=\) and \(_{j}=\), for \(j=1,,T\). The achievable distortions \(D_{,j}\) (for \(0\)-PLF-FMD) and \(D_{,j}\) (for \(0\)-PLF-JD) are:_

\[D_{,j}=2(1-_{,j})^{2},  D_{,j}=2(1-_{,j})^{2},\] (17)

_where \(_{,j}:=)^{j-1}-1}{2^{2}-1}}\) and \(_{,j}:=^{2(j-1)}+1\{j 2\}}({_{i=0}^{j-2}^{2i}})\)._

_Proof:_ See Appendix G.

In particular, specializing to \(=1\), \(_{,j}=2^{}\) and \(_{,j}=1\). This shows that the decrease in \(D_{,j}\) is exponential at each step which implies the ability of decoder based on \(0\)-PLF-FMD in correcting mistakes and not propagating them in future reconstructions. However, as discussed previously the decoder which uses \(0\)-PLF-JD is stuck at \(_{j}=_{1}\) when \(=1\) and results in no subsequent improvement in the distortion. We call this behaviour as _permanence of error_. This phenomenon is magnified in the case when \(R_{1} 0\) and \(R_{2}\), treated in Table 2 (Appendix F) as the PLF-JD severely constrains the decoder to copy the previous noisy reconstruction while the flexibility provided by PLF-FMD reduces the distortion in the second step.

The case when \(R_{1}\) and \(R_{2}=\) treated in Table 2 in Appendix F corresponds to the case when \(X_{1}\) is sent at a sufficiently high rate (as is the case with some I-frames) while \(X_{2}\) is sent at a low rate. Naturally, we have \(_{1}^{G} X_{1}\) for both PLFs. On the other hand, we once again see a qualitatively different behaviour in the reconstruction of \(X_{2}\). For the case of \(0\)-PLF-FMD, we have \(_{2}^{G}(1-O())_{1}^{G}+O()X_{2}\), i.e., the decoder essentially copies the previous frame with little contribution from the second step. In contrast, for the case of \(0\)-PLF-JD, it can be shown that \(_{2}^{G}_{1}_{1}^{G}+O()X_{2}+Z_{}\), where \(Z_{}\) is independent Gaussian noise with variance close to \(1-^{2}\). We note that the PLF-JD metric prevents the decoder from simply "copying" the previous frame, but instead forces the decoder to generate a more diverse representation consistent with the joint distribution between the two frames.

### Universal Representations for Gauss-Markov Source Model

In this section, we show that the Gauss-Markov source model admits universal encoded representations. Such representations can be transformed through appropriate reconstruction functions to achieve the entire DP rate region. This is the counterpart of the result for general sources in Theorem 1 where it is shown that the MMSE reconstructions can be transformed to some target reconstructions satisfying the \(0\)-PLF-FMD with at most a factor-2 increase in distortion. In contrast, we demonstrate that the Gauss-Markov model admits _exact universality_ i.e., target reconstructions proposed in this section achieve all points in the iDP rate region. Interestingly, the transformation is linear with possibly some additive noise. First, we formalize the notion of universal representations.

**Definition 5** (iDP-Tradeoff): _For a given rate tuple \(\), the optimal iDP-tradeoff is the closure of the set of all tuples \((,)\) such that \((,,)_{}\) and is denoted by \(()\)._

**Definition 6** (Universal Representation): _A given encoded representation \(_{r}\) is called universal with respect to rate tuple \(\) if it satisfies the rate constraints (10) and the Markov chains in (13)-(14) and for each \((,)()\), there exists a reconstruction \(}\) generated from \(P_{}|_{r}}\) achieving it._

For the Gauss-Markov source model, we show that the MMSE reconstruction admits a universal representation. We consider the reconstruction \(}_{r}\) that achieves minimum distortion in the \(()\) region. This point is explicitly characterized in Appendix H.1. Furthermore, following Theorem 4, since the reconstruction functions \(_{j}()\) are identity, the MMSE reconstruction is equivalent to MMSE representation i.e., \(}_{r}=_{r}^{}\).The following theorem establishes that any point in \(()\) can be achieved from \(}_{r}\).

**Theorem 6**: _For the Gauss-Markov source model and a given rate tuple \(\) with strictly positive components, let the MMSE representation be denoted as \(_{r}^{}=(X_{r,1}^{},X_{r,2}^{},X_{r,3}^{ })\). Let \((,)()\) and let \(}=(_{1},_{2},_{3})\) be the corresponding reconstruction achieving it. Then there exist \(_{1}\), \(_{1}\), \(_{2}\), \(_{1}\), \(_{2}\) and \(_{3}\) and noise variables \((Z_{1}\), \(Z_{2}\), \(Z_{3})\) independent of \((X_{r,1}^{},X_{r,2}^{},X_{r,3}^{})\), which satisfy the following_

\[_{1}=_{1}X_{r,1}^{}+Z_{1},_{2}=_{1}X_{ r,1}^{}+_{2}X_{r,2}^{}+Z_{2},_{3}=_{1}X_{ r,1}^{}+_{2}X_{r,2}^{}+_{3}_{r,3}^{}+Z_{3}.\]

_Proof:_ See Appendix H.2. 

The above theorem indicates that the MMSE representation can be linearly transformed to achieve any point in \(()\). In general the MMSE representation may have to be degraded through additional noise terms. In the proof of Theorem 6 we identify conditions when such degradation is not needed.

As discussed in Appendix H.2, Theorem 6 holds for both PLFs. This suggests the idea that one can train an encoder to get MMSE representations which are oblivious to the choice of PLF. Then, the decoder can generate a reconstruction which satisfies either of PLFs by simply applying a linear transformation to the MMSE representation. Thus, the task of choosing the right PLF can be assigned to the decoder based on distortion and perception requirements. We conclude by noting that Appendix H.3 provides an example where the coefficients in Theorem 6 can be computed explicitly.

## 5 Experimental Results

We conduct experiments on the MovingMNIST dataset  (with 1 digit) using Wasserstein GAN , to verify the implications of our theoretical claims to perceptual video compression. Additional results on the KTH dataset  are available in Appendix J.3. Our compression network is built on the scale-space flow model  and conditional module . For a given rate and PLF, we obtain different distortion-perception tradeoff points by optimizing the weighted sum between distortion and perception losses. Details about the architecture and training procedure are available in the Appendix J.1. The experimental setup is focused on validating our theory, rather than proposing state-of-the-art neural network architectures. Accordingly, we begin by (1) validating Theorems 1 and 2, which characterize the factor-of-two bounds on the distortion of 0-PLF reconstructions (2) empirically demonstrating the _error permanence_ phenomenon of the PLF-JD in Section 4.3 and (3) computing the DP tradeoff function experimentally as well as confirming that the MMSE reconstruction provide near universal representations, as motivated by the results in Section 4.4.

As our first experimental result in Table 1, we validate the _factor of two bounds_ in Theorems 1 and 2. We consider the compression of two frames \(X_{1}\) and \(X_{2}\) at rates \(R_{1}\) and \(R_{2}\) respectively. The compression of \(X_{1}\) is performed without any prior reference and corresponds to the compression of the "I-frame", while the compression of \(X_{2}\) corresponds to the "P-frame", using \(X_{1}\) as the reference. We consider the cases when either \(R_{1}{=}\) or \(R_{1}{=}12\) bits, where the former corresponds to lossless compression of \(X_{1}\) and the latter corresponds to the low rate regime (see Appendix I for a justification). The average distortion for the first frame when \(R_{1}=12\) is \(0.0124\) for the MMSE reconstruction and \(0.0235\) for the 0-PLF reconstruction, thus satisfying the factor of two bound. In compression of \(X_{2}\), we systematically vary the value of the rate \(R_{2}{}\{4,8,12,\}\). Following Table 0(b), for 0-PLF-JD reconstruction, only \(R_{2}{=}4\) bits (low rate) satisfies the factor of two bounds as expected. Intuitively, even as more bits are acquired, the 0-PLF-JD criteria actively restricts improving the reconstructions, resulting in persistently higher distortion. Even when \(R_{2}=\), the distortion remains non-zero as the decoder is forced to maintain temporal consistency with \(_{1}\). In contrast, for FMD, the factor of \(2\) bound holds at all rates, consistent with Theorem 1.

In Fig. 2, we present our experimental results with a group of pictures (GOP) of size 3 (i.e. one I-frame followed by two P-frames). In Fig. 1(a), we visualize sample reconstructions for MSE, 0-PLF-FMD and 0-PLF-JD cases when operating in the low-rate regime with \(R_{j}{=}12\) bits for \(j{=}1,2,3\). Note that given an incorrect digit reconstruction in \(_{1}\), the decoder with 0-PLF-JD consistently produces incorrect digits (or content) while the 0-PLF-FMD gradually "corrects" it, which confirms the _error permanence phenomenon_ discussed in the theoretical analysis in Section 4.3 and Table 2 in Appendix F. We also plot the framewise distortion in Fig. 1(b) to show the difference in achievable distortion two perception metrics across different values for \(R_{2}\) and \(R_{3}\) as a function of the frame index. Consistent with Theorem 5, the achievable distortion decreases much faster for 0-PLF-FMD than 0-PLF-JD for all selection of rates. Finally, we show similar results in Figure 3 for UVG dataset.

In Fig. 4, we plot the tradeoff curves between distortion and perception for the second reconstruction \(_{2}\) for both optimal (end-to-end) and universal representations for two cases: when \(R_{1}{=}\) and \(R_{1}{=}12\) bits and for a range of values for \(R_{2}\). In general, the curves for both universal and optimal

  \(R_{2}\) & MMSE & 0-PLF-FMD & 0-PLF-JD \\ 
1 & \(1.08 0.01\) & \(1.74 0.02\,\) & \(2.05 0.03\,\) \\ 
2 & \(0.88 0.01\) & \(1.39 0.03\,\) & \(1.46 0.02\,\) \\ 
3.17 & \(0.53 0.01\) & \(0.76 0.01\,\) & \(0.79 0.01\,\) \\  \) bits} \\  \(R_{2}\) & MMSE & 0-PLF-FMD & 0-PLF-JD \\ 
4 & \(1.23 0.01\) & \(2.21 0.04\,\) & \(2.36 0.04\,\) \\ 
8 & \(1.04 0.01\) & \(1.78 0.03\,\) & \(2.28 0.03\,\) \\ 
12 & \(0.89 0.02\) & \(1.43 0.02\,\) & \(2.26 0.03\,\) \\  \(\) & 0.0 & 0.0 \(\) & \(2.18 0.02\,\) \\  

Table 1: Distortions of optimal reconstructions at different regime (\(\) means factor of 2 holds and \(\) means otherwise). Distortion is scaled by \(10^{-2}\).

Figure 2: Permanence of Error Phenomenon. In (a), we visually compare the reconstructions. Note that \(_{1}\) is the same for both 0-PLF-JD and 0-PLF-PMD. In (b), we show the framewise distortion for different \((R_{2},R_{3})\).

representations are relatively close to each other at all rate regimes. The general shape of every curve is relatively similar with the exception of the PLF-JD metric in Fig. 3(b), where the curves for different rates seemingly converge since increasing the rate does not significantly improve the distortion in this case as noted previously. Finally, as the universal encoders are derived from MMSE solutions, these results imply that one can simply send the MMSE representation to the decoder and the user can flexibly change the DP tradeoff up to their requirements. We further note that even when the end-to-end model targets an operating point different from the MMSE reconstruction, the latter is still required to estimate the motion flow vectors best. The universal representation provides a natural way to reconstruct the MMSE reconstruction from the encoder output. In the plots of Fig. 4, we leverage on established universality results for I-frame compression in prior works  to construct the MMSE representation for motion compensation as we have a GOP of size \(2\).

Finally in Appendix J.5, we consider the ability of the decoder to generate diverse reconstructions when operating under either PLF-JD or PLF-FMD. We focus on the case when \(X_{1}\) is transmitted losslessly and when \(X_{2}\) is compressed at low rates. Consistent with the theoretical analysis in Section 4.2 and Table 2 in Appendix F, the decoder optimized for PLF-JD is capable of producing diverse reconstructions by mimicking the actual motion between the frames. The PLF-FMD leads to reconstructions that are highly correlated and less desirable.

## 6 Conclusions

This work examines different perception loss functions for causal video coding, establishing its key theoretical properties such as the operational RDP region and universality principle. Our analysis highlights that while \(0\)-PLF-JD reconstruction preserves temporal correlation, it is susceptible to the error permanence phenomenon. Moreover, our investigation of universality reveals that the encoder can transform the MMSE representation to other points on the DP tradeoffs, irrespective of the PLF. We suggest future research directions such as exploring region-based perceptual metrics , incorporating image-aware bits allocation, and leveraging conditional perception metric .

Figure 4: RDP tradeoff curves for end-to-end and universal models. We plot the tradeoff for the two regimes: \(R_{1}{=}\) and \(R_{1}{=}\) in (a) and (b) respectively. The universal and optimal curves are close to each other.

Figure 3: Error permanence phenomenon on the UVG dataset. The PLF-JD reconstructions propagate the flaws in the color tone from the previous I-frame reconstruction while the PLF-FMD is able to fix these flaws. Compression rate for I-frame and P-frame are \(~{}0.14\)4bpp (low rate) and \(4.632\)bpp (high rate) respectively.