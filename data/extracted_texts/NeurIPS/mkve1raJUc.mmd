# Robust Mean Estimation Without Moments for

Symmetric Distributions

 Gleb Novikov

Department of Computer Science

ETH Zurich

&David Steurer

Department of Computer Science

ETH Zurich

&Stefan Tiegel

Department of Computer Science

ETH Zurich

###### Abstract

We study the problem of robustly estimating the mean or location parameter without moment assumptions. Known computationally efficient algorithms rely on strong distributional assumptions, such as sub-Gaussianity, or (certifiably) bounded moments. Moreover, the guarantees that they achieve in the heavy-tailed setting are weaker than those for sub-Gaussian distributions with known covariance. In this work, we show that such a tradeoff, between error guarantees and heavy-tails, is not necessary for symmetric distributions. We show that for a large class of symmetric distributions, the same error as in the Gaussian setting can be achieved efficiently. The distributions we study include products of arbitrary symmetric one-dimensional distributions, such as product Cauchy distributions, as well as elliptical distributions, a vast generalization of the Gaussian distribution.

For product distributions and elliptical distributions with known scatter (covariance) matrix, we show that given an \(\)-corrupted sample, we can with probability at least \(1-\) estimate its location up to error \(O()\) using \((1/)}\) samples. This result matches the best-known guarantees for the Gaussian distribution and known SQ lower bounds (up to the \((d)\) factor). For elliptical distributions with unknown scatter (covariance) matrix, we propose a sequence of efficient algorithms that approaches this optimal error. Specifically, for every \(k\), we design an estimator using time and samples \((d^{k})\) achieving error \(O(^{1-})\). This matches the error and running time guarantees when assuming certifiably bounded moments of order up to \(k\). For unknown covariance, such error bounds of \(o()\) are not even known for (general) sub-Gaussian distributions.

Our algorithms are based on a generalization of the well-known filtering technique . More specifically, we show how this machinery can be combined with Huber-loss-based techniques to work with projections of the noise that behave more nicely than the initial noise. Moreover, we show how sum-of-squares proofs can be used to obtain algorithmic guarantees even for distributions without a first moment. We believe that this approach may find other applications in future works.

## 1 Introduction

Robust statistics  is a central field in statistics with the goal of designing algorithms for statistical problems that are robust to a small amount of outliers, for example caused by measurement errors or corrupted data. We model this as follows: Samples are generated from an unknowndistribution \(\) and then an \(\)-fraction of them is arbitrarily corrupted by an adversary with full knowledge of the underlying model and our algorithm. We only have access to the corrupted samples. In this work we focus on the canonical task of estimating the mean of \(\). Traditionally, estimators robust to such corruptions have been computationally inefficient, requiring time exponential in the ambient dimension , while computationally efficient estimators have incurred error scaling with the ambient dimension thus rendering them unsuitable for today's high-dimensional statistical tasks. Recently however, a flurry of efficient estimators emerged achieving guarantees without this prohibitive dependence on the dimension and with error rates approaching those of computationally inefficient ones .

A textbook example of this development is when \(\) is the multi-variate Gaussian distribution (or sub-Gaussian distributions with _known_ covariance). In this setting, efficient algorithms can estimate the mean to within error \(O()\), i.e., nearly linear in \(\). The statiscally optimal error rate is \(O()\). That is, efficient algorithms are optimal up to the additional factor of \(\). Further, it is conjectured that this factor is inherent for efficient algorithms, i.e., that there is a _computational-statistical gap_. This is evidenced by known lower bounds for statistical query algorithms . However, there are two drawbacks with this textbook example: If the covariance matrix is unknown, it is not known how to efficiently achieve the same error for sub-Gaussian distributions, in fact, in full generality, it is not even known how to achieve error \(o()\) in this setting. This is because known algorithms rely heavily on the algebraic structure of Gaussian moments. Second, the assumption that the uncorrupted data belongs to a (sub)-Gaussian distribution is arguably very strong. A more natural setting is when \(\) is allowed to have heavier tails. In the setting that \(\) has bounded second moments it is known how to achieve error \(O()\) efficiently . Interestingly, \(O()\) seems to constitute a barrier for efficient algorithms. In particular, while error \(o()\) is possible information-theoretically if we assume that higher-order moments are bounded,  show that bounded moments alone are likely not enough for efficient algorithms, and that additional assumptions are required. Currently, these manifest as either assuming that there is a certificate, in sum-of-squares, for the boundedness of the moments  or assuming the covariance is the identity (or known) . While these approaches indeed break the \(O()\) barrier, they in many cases fall short of the \(O()\) error possible in the Gaussian setting. In particular, they only achieve error comparable to the Gaussian case, when using \(O((1/))\) many moments. Unfortunately, it is necessary to use this many moments, even when assuming they are certifiably bounded . Thus, the tails of these distributions are already much lighter than, say, in the bounded covariance case. Therefore, a natural question is:

_Do there exist classes of heavy-tailed distributions for which we can (efficiently) achieve the same robust error as for the Gaussian distribution?_

In this work, we answer this question by giving algorithms that use a (quasi-)polynomial number of samples (in the dimension and \(1/\)) and time polynomial in the number of samples for the broad class of symmetric product and elliptical distributions. Moreover, in many cases, we can achieve this using (nearly) the same amount of samples as for the Gaussian distribution, recovering, e.g., the optimal dependence on the failure probability. We remark that the distributions we consider might have arbitrarily heavy tails and we do not make any assumptions related to sum-of-squares.

Product and Elliptical DistributionsSymmetry is a natural assumption in statistics with many applications to, e.g., mathematical finance and risk management . In this work, we consider the following two types of symmetric distributions that are of particular interest : First, product distributions of symmetric one-dimensional distributions and spherically symmetric distributions. These correspond to a generalization of the standard Gaussian distribution. Examples are product Cauchy distributions and the multi-variate Student \(t\)-distribution with identity scatter matrix. Second, elliptical distributions. These correspond to a generalization of Gaussians with arbitrary (unknown) covariance matrix. Examples include the multi-variate Student \(t\)-distribution, symmetric multivariate stable distributions and multivariate Laplace distributions. For both classes, it is information-theoretically possible to obtain robust error \(O()\), i.e., matching that of a Gaussian. These approaches are not known to run in faster than exponential time and prior work  asked whether a similar error can be achieved efficiently. We respond to this question, by designing algorithms that nearly, in some cases exactly, match this error in polynomial (in the number of samples) time, if the number of samples is polynomial (in the cases of symmetric product or spherically symmetric distribution) or quasi-polynomial (in the case of elliptical distributions) in the dimension and \(1/\).

### Problem Set-Ups and Results

Next, we give formal definitions of the corruption model and distributions considered and state our results. We use the following standard model for corruptions - often referred to as the strong contamination model.

**Definition 1.1**.: Let \(_{1},,_{n}\) be i.i.d.samples from a distribution \(\) and let \(>0\). We say that \(Z_{1},,Z_{n}\) are an \(\)_-corruption_ of \(_{1},,_{n}\), if they agree on at least an \((1-)\)-fraction of the points. The remaining \(\)-fraction can be arbitrary and in particular, can be corrupted by an adversary with full knowledge of the model, our algorithm, and all problem parameters.

We consider the following two classes of distributions.

**Definition 1.2** (Semi-Product Distributions).: Let \(>0\). We say a distribution \(\) over \(^{d}\) is an \(\)_-semi-product_ distribution, if for \(\) it holds that

1. For all \(j[d]\), the distribution of \(_{j}\) is symmetric about 0,
2. For all \(j[d]\), \((|_{j}|)\),
3. The random vectors \(((_{j}))_{j=1}^{d}\) and \((|_{j}|)_{j=1}^{d}\) are independent, and the random variables \((_{1}),, (_{d})\) are mutually independent.

Some remarks about the definition are in order. First, notice that the Gaussian distribution \(N(0,^{2} I)\) is \(()\)-semi-product. Similarly, every symmetric product or spherically symmetric distribution that has covariance bounded by \(^{2}_{d}\) is \(()\)-semi-product. In particular, the coordinates \(_{j}\) need not be independent. However, the definition allows for much heavier tails, the only requirement is that at least a \(1/100\)-fraction of the probability mass lies in an interval of length \(\) around 01. In particular, it captures all spherically symmetric distributions, e.g. multi-variate \(t\)-distributions with identity scatter matrix, and all symmetric product distributions, e.g. the product Cauchy distribution. Notice that in the non-robust setting the first two properties are enough to accurately estimate \(^{*}\) from samples \(^{*}+_{1},,^{*}+_{n}\), via adding symmetric mean zero noise with tiny variance to each coordinate and computing the entry-wise median. We expect that _some_ additional assumption is necessary for efficient algorithms in the robust setting. We show that Property 3 above is sufficient. A different sufficient condition would be assuming that the distribution is elliptical, which we discuss next.

The second one is the class of elliptical distributions  a generalization of spherically symmetric distribution, which in particular can have more complex dependency structures.

**Definition 1.3**.: A distribution \(\) over \(^{d}\) is _elliptical_, if for \(\) the following holds: Let \(\) be uniformly distributed over the \(d\)-dimensional unit sphere. There exists a positive random variable \(\), independent of \(\), and a positive semi-definite matrix \(\), such that

\[=^{1/2}( )\,.\]

We call \(\) the _scatter_ matrix of the distribution (sometimes also referred to as the dispersion matrix). In particular, the Gaussian distribution \(N(0,)\) with arbitrary \(\) is elliptical. Further, we can choose its covariance matrix as the scatter matrix. Indeed, we can decompose \( N(0,)\) as \(=^{1/2}\), where \(\) is distributed as the square root of a \(^{2}\)-distribution with \(d\) degrees of freedom and \(\) is independent of \(\) and distributed uniformly over the unit sphere. Then by Markov's Inequality, it holds that \(()=(^{2} 2d) \). Elliptical distributions with scatter matrix identity correspond to spherically symmetric distributions, a special case of semi-product distributions. Note that that elliptical distributions _do not_ capture product distributions except for the Gaussian case - e.g., the product Cauchy distribution is not elliptical.

Extending the above two defintions, we say that a distribution \(\) is \(\)-semi-product (resp. elliptical) _with location \(^{*}^{d}\)_ if samples of \(\) take the form \(^{*}+\), where \(^{*}^{d}\) is deterministic and \(\) is \(\)-semi-product (resp. elliptical). Note that the location takes the place of the mean, as this might not exist for distributions of the above form.

ResultsOur main result for semi-product distributions is the following: Note that we can reduce the case of elliptical distributions with known scatter matrix to the \((1)\)-semi-product case, hence the theorems below also apply to this setting. We also remark that the algorithm only receives the corrupted samples as input. In particular, \(\) need not be known (and can be estimated from the corrupted samples). See the appendices for a proof.

**Theorem 1.4**.: _Let \(^{*}^{d},,>0\) and \(\) be a \(\)-semi-product distribution with location \(^{*}\). Let \(C>0\) be a large enough absolute constant and assume that \( 1/C\) and \(n C(1/ )}\). Then, there exists an algorithm that, given an \(\)-corrupted sample from \(\), runs in time \(n^{O(1)}\) and outputs \(^{d}\) such that with probability at least \(1-\) it holds that_

\[\|-^{*}\| O([}+])\,.\]

Our sample complexity is nearly optimal in the dependence of the error on \(\) and \(d\) and optimal in the dependence on the failure probability \(\), all up to constant factors. Recall that \(N(0,^{2} I_{d})\) is a \(()\)-semi-product distribution and hence lower bounds for this setting apply. The statistically optimal error in this setting is \(()\) and can be achieved using \(n}\) samples. Note that we match this error up to the \(\) term, using only slightly more samples (\(d\) vs \(d d\)). It is conjectured that the larger error is necessary for efficient algorithms, as it is necessary for all efficient SQ algorithms . It is an interesting open question to remove the additional factor of \( d\) in our sample complexity. Further, our algorithm nearly matches results known for the standard Gaussian distribution (up to the \((d)\) factor in error and sample complexity) . We expect our algorithm to be practical, since it only uses one-dimensional smooth convex optimization (\(O(nd)\) times), the top eigenvector computation (\(O(n)\) times) and arithmetic operations.

Interestingly, we show how to achieve error scaling only with \(O()\) using quasi-polynomially many samples:

**Theorem 1.5**.: _Let \(^{*}^{d},,>0\) and \(\) be a \(\)-semi-product distribution with location \(^{*}\). Let \(C>0\) be a large enough absolute constant and assume that \( 1/C\) and \(n C d^{C(1/)}\). Then, there exists an algorithm that, given an \(\)-corrupted sample from \(\), runs in time \(n^{O(1)}\) and outputs \(^{d}\) such that with probability at least \(1-\) it holds that_

\[\|-^{*}\| O([}+])\,.\]

In order to state our results for elliptical distributions, we need to introduce the notion of the _effective rank_ of a matrix \(\). This is defined as \(r()/\|\|\) and captures the intrinsic dimensionality of the data. Note that it is always at most \(d\), but can also be significantly smaller. We remark that we do not assume that the scatter matrix \(\) is known to the algorithm.

**Theorem 1.6**.: _Let \(C>0\) be a large enough absolute constant. Let \(k,,>0,^{*}^{d}\) such that \( 1/C\) and assume \(\) is an elliptical distribution with location \(^{*}\) and scatter matrix \(\) satisfying \(r() C k d^{2}\). Also, let \(n C(r()/k)^{k}(d/)\). There is an algorithm that, given an \(\)-corrupted sample from \(\), runs in time \(n^{O(1)}d^{O(k)}\) and with probability at least \(1-\) outputs \(^{d}\) satisfying_

\[\|-^{*}\| O([}+^{1-1/(2k)} ])\,.\]Note that in the special case when \(=N(0,)\), the information-theoretically optimal error is \(()\) and can be achieved using \(n}\) samples . We give a sequence of algorithms (nearly) approaching this error using an increasing number of samples and time. In fact, for \(k=O((1/))\), we achieve error \(O()\) using at most \((d/)}{^{2}}\) samples and time \(d^{k}\). Similar to the bounded moment setting  the parameter \(k\) can be thought of as a way to trade between sample/time complexity and accuracy guarantees. Also note that already for \(k=2\) we achieve error \(O(^{3/4})\) for elliptical distributions with unknown covariance/scatter matrix in polynomial time and sample complexity, while even for (general) sub-Gaussian distributions with unknown covariance it is not known if it is possible to achieve error \(o()\) in polynomial time and sample complexity. In addition, if \(r()\) is much smaller than \(d\), for example, \(r() d^{0.1}\), then we achieve error \(O(^{3/4})\) with a _sub-linear_ number of samples \(n=(d^{0.2})\) in time \(d^{O(1)}\).

Previous Algorithms and Possible ExtensionsPreviously, to the best of our knowledge, only exponential time computable estimators were known, for both semi-product and elliptical distributions.

The results  imply that there exist estimators that achieve error \(O(}+)\) (assuming \(\|\| O(1)\) for elliptical distributions). Both algorithms rely on brute force techniques for which it is unclear how to make them efficient.

A natural question is if we can achieve (nearly) optimal error in polynomial time (analagous to the semi-product case). Indeed, for _non-spherical_ Gaussian distributions, it is known how to do this . These algorithms crucially rely on robustly estimating the covariance matrix first. Similarly for elliptical distributions, it seems necessary to first robustly estimate the scatter matrix and then apply our results for the semi-product case. However, current covariance estimation algorithms rely heavily on the algebraic relations between the Gaussian moments, and it is not known how to achieve this for other distributions where moments do not have these specific relations. Thus, we expect this to be an interesting but challenging task.

A different direction we believe is interesting to explore is the following: We gave algorithmic results matching in many cases what is known for the Gaussian distribution. In the non-robust setting, there is in principle hope to go beyond this: There are known estimators that are asymptotically normal with variance scaling with the Fisher information of the distribution . So there is hope for "better-than-Gaussian error" on distributions with small Fisher information. However, these results are purely asymptotic and there are (symmetric) distributions for which we cannot hope to achieve even finite sample results scaling with the Fisher information (see, e.g., the discussions in ). To get around this issue, the recent work of  introduced a related notion, called "smoothed Fisher Information" and achieved finite-sample (non-robust) error guarantees scaling with the smoothed Fisher Information in the one-dimensional case, also for symmetric distributions. It would be interesting to see, if similar guarantees can be achieved in the robust and high-dimensional setting, too.

We further believe that similar guarantees can be achieved based on \(_{1}\)-minimization techniques (instead of Huber-loss) under slightly different assumptions. In particular, even in the non-robust setting, we need to require a small amount of density at (or sufficiently) close to the location, else any point in the region around the location with zero density, would be an \(_{1}\)-minimizer. In the case of semi-product distributions, this is without of loss of generality, since we could add Gaussian noise of the appropriate variance to each coordinate. For elliptical distribution, this approach does not work, since the resulting distribution might no longer be elliptical.

### Related Work

We only list the works most closely related to this work, we refer to  for a survey on the area. Most of the literature on efficient algorithms for robust mean estimation has focussed on the setting when at least some of the moments are bounded. In what follows we focus mainly on how the error guarantees depend on \(\) and put other parameters, such as the dependence on the failure probability aside.  show how to efficiently estimate the mean of a Gaussian distribution, or sub-Gaussian with identity-covariance, up to error \(O()\). Assuming that the distribution has covariance bounded by identity,  show how to achieve error \(O()\). Similarly, for \(\),  shows how to achieve error \(O(^{/(1+)})\) when in every direction the \(1+\) moments are bounded. Note that in constrast to this, our algorithm can handle distributions without a first moment and achieves error \(O()\). An interesting question is whether assuming bounded higher-order moments can lead to improved error guarantees. Interestingly,  shows that this alone is likely to be not enough in order to go beyond error \(\). So far, there have been two ways of adding additional assumptions: First, assuming that \(\) has \(k\)-th moments bounded by \(_{k}\) in every direction, and assuming that it is certifiable in sum-of-squares, it is possible to achieve error \(O(_{k}^{1-1/(2k)})\) using \(d^{O(k)}\) samples and time . Second, if the \(k\)-th moments are bounded by \(_{k}\), not necessarily certifiably in sum-of-squares, and the covariance matrix is a multiple of the identity (or known),  achieves error \(O(_{k}^{1-1/(2k)})\).

Other Results On Symmetric Noise Distributions and the Filtering TechniquesHuber-loss minimization has been used to design computationally efficient estimators for a variety of problems: Linear regression in the heavy-tailed/symmetric setting , dNS21, dLN\({}^{+}\)21], as well as the robust setting . Other works developped algorithms for PCA under symmetric noise .

The filtering technique  (or versions thereof) have been successfully applied to robust and heavy-tailed (when the covariance exists) mean estimation  as well as a pre-processing step in robust regression .

## 2 Techniques

We will first describe the general theme of our techniques. These ideas will yield algorithms achieving (nearly) optimal guarantees using quasi-polynomially many samples. Note however, that using these, we can already achieve error \(o()\) using only polynomially many samples. At the end of this section we show how to improve this to nearly linearly many (in the dimension) samples for semi-product distributions.

One-Dimensional Robust Estimation via Huber-Loss MinimizationCurrent algorithms for robust mean estimation are only known to work for distributions with bounded moments. In many cases this is assumption is reasonable, and in many cases it is also necessary . However, there are many other distributions that do not necessarily have any moments at all, but which one would expect to behave nicely. Current approaches are inapplicable in these settings. Of particular interest is the class of symmetric distributions, which might not even have a first moment - in this case we want to estimate the location parameter of the distribution, which always exists and coincides with the mean in case it exists. In non-robust statistics (when there are no corruptions), symmetric distributions in some sense behave as nicely as the Gaussian distribution: The minimizer of the entry-wise _Huber-loss_ function achieves the same guarantees for such distributions as the sample mean in the Gaussian case. The entry-wise Huber-loss is the function \(_{H}:^{d}\) defined as \(_{H}(v)_{j=1}^{d}(v_{j})\), where \(\) is a _Huber penalty_: \((x)=x^{2}/2\), if \(|x| 1\), and \((x)=|x|-1/2\) otherwise. \(\) is convex, and has many appealing properties, in particular with respect to symmetric distributions, since its derivative is a bounded odd Lipschitz function: \((x):=^{}(x)=x\), if \(|x| 1\), and \((x)=(x)\) otherwise.

Since the Gaussian distribution is symmetric itself, the Huber-loss estimator has the same guarantees as the sample mean in this setting. Beyond that, the Huber-loss estimator enjoys some robustness guarantees that the sample mean does not: Let us consider the one-dimensional case, where the Huber-loss behaves similarly to the median but has additional properties that will be useful to us, e.g., it is differentiable everywhere. In this setting, the sample mean has unbounded error even if there is only a single corrupted sample, while the Huber-loss achieves the information theoretically optimal error \(O()\) when an arbitrary \(\)-fraction of the samples is corrupted. Moreover, these guarantees extend to arbitrary one-dimensional symmetric distributions that satisfy mild scaling assumptions. Unfortunately, this does not directly yield a good robust estimator for the high-dimensional setting. If we naively apply the one-dimensional estimator entry-wise, we can only guarantee an error bound of \(O()\), which is far away from the \(O()\) error that is statistically possible for the Gaussian distribution and symmetric distributions. Moreover, it is also inferior to error rates obtained by efficient estimators that use assumptions about bounded moments: Such estimators achieve error that does not depend on \(d\). In this work we show that there exist more sophisticated estimators based on the Huber-loss3 that achieve dimension-independent error, often matching what is possible for the Gaussian distribution, for estimating the location of symmetric distributions even in the high-dimensional robust setting.

Proofs of identifiability and the filtering techniqueBefore describing how we use the Huber-loss in the high-dimensional setting, it will be instructive to recall how the classical approach for distributions with bounded moments works - for simplicity, we focus on bounded second moments and sketch how this extends to higher-order moments. Later, we will see how to modify these ideas, using the Huber-loss, to work also in the symmetric setting without moment assumptions. In particular the version for Gaussian-like error for semi-product distribution requires several new technical ideas compared to the Gaussian setting.

At the heart lie _identifiability proofs_ which can be made algorithmic using either sum-of-squares  or the filtering technique . For bounded covariance distributions these take the following form: If two distributions \(D_{1}\) and \(D_{2}\) with means \(_{1}\), respectively \(_{2}\), and covariance matrices \(_{1}_{d}\), respectively \(_{2}_{d}\), are \(\)-close in statistical distance, then \(\|_{1}-_{2}\| O()\). In what follows, for simplicity of the discussion, we will sometimes switch between empirical and true distributions. In robust mean estimation, we assume that \(D_{1}\) is the empirical distribution over the corrupted samples we observe, while \(D_{2}\) is the empirical distribution of the uncorrupted samples. Hence, by assumption, \(_{2}_{d}\) and we wish to estimate \(_{2}\). The above statement of identifiability asserts that as long as \(_{1}_{d}\), the empirical mean of the corrupted samples, i.e., \(_{1}\), is \(O()\)-close to \(_{2}\). Of course, \(_{1}_{d}\) might not hold, since the outliers could potentially introduce a large eigenvalue in the empirical covariance matrix. However, since we know that these large eigenvalues must have been introduced by outliers, this gives rise to a win-win analysis: We check if \(_{1}_{d}\) holds. If it does, \(_{1}\) is \(O()\)-close to \(_{2}\) already, if not, we compute its top eigenvector and remove samples that have large correlation with this eigenvector. It turns out that this procedure will always remove more corrupted samples than uncorrupted ones. Thus, we can iterate this procedure and are guaranteed that it terminates after \(O( n)\) iterations. After its termination, we are left with a distribution \(D_{1}^{}\), with mean \(_{1}^{}\), that is \(O()\)-close in total variation distance to \(D_{1}\) and \(D_{2}\), and has small covariance \(_{1}^{}_{d}\), hence \(\|_{1}^{}-_{2}\| O()\).

Of course, for arbitrary symmetric distributions the covariance might not exist. So neither the above proof of identifiability nor the filtering technique apply to this setting. However, one of our main observations is that we can obtain a similar statement of identifiability also in this case. In addition, we will later show how to adjust the filtering technique to work with our identifiability proof - this requires non-trivial modifications of the original approach. First, observe that the identifiability statement in the bounded moment setting, can be phrased as follows: For \(j\{1,2\}\), the mean \(_{j}\) of a distribution \(D_{j}\) corresponds to the minimizer of the quadratic loss:

\[_{j}=*{arg\,min}_{a^{d}}\,*{}_{ D_{j}}\|-a\|^{2}\,.\]

Hence, if \(D_{1}\), with covariance \(_{1}_{d}\), and \(D_{2}\) with covariance \(_{2}_{d}\), are \(\)-close in statistical distance, then the minimizers, \(_{1}\) and \(_{2}\), of the quadratic loss are \(O()\)-close to each other. A crucial observation is that, if a distribution \(D\) is symmetric, then its location \(\) is the entry-wise Huber-loss minimizer4

\[=*{arg\,min}_{a^{d}}\,*{} _{ D}_{H}(-a)\,.\]

Hence, we would like to obtain a similar statement about minimizers of the Huber-loss for two \(\)-close distributions \(D_{1}\) and \(D_{2}\). This will be useful since we will need to learn the location \(_{2}\) of a symmetric distribution \(D_{2}\) from \(D_{1}\).

Abstract proof of identifiabilityIn order to formalize the above, consider the following abstract setting: Let \(\) be some loss function that is differentiable, \((1)\)-strongly convex and smooth (i.e. its gradient \(\) is \(1\)-Lipschitz). Let \(D_{1}\) and \(D_{2}\) be two distributions that are \(\)-close in the statistical distance, Then, if for \(j\{1,2\}\) the minimizers \(_{j}:=*{arg\,min}_{a^{d}}*{ }_{ D_{j}}(-a)\) satisfy \(_{ D_{j}}(-_{ 2}),u^{2} 1\) for all unit vectors \(u\), then \(\|_{1}-_{2}\| O()\). Indeed, by strong convexity and the definition of \(_{1}\),

\[(\|_{1}-_{2}\|^{2})  D_{1}}{}\, (-_{1})- D_{1}}{}\, (-_{2})- D_{1}}{ }\,(-_{2}), _{1}-_{2}\] \[ D_{1}}{}\, (-_{2}),_{1}-_{2} \.\]

Notice that there is a coupling \(\) of \(D_{1}\) and \(D_{2}\) such that \(_{(_{1},_{2})}(_{1}_{2})\). Since5\(_{ D_{2}}\)\((-_{2})=0\), we obtain using the Cauchy-Schwarz Inequality

\[(\|_{1}-_{2}\|^{2})  D_{1}}{}\, (_{1}-_{2})-( _{2}-_{2}),_{1}-_{2}\] \[=_{1},_{2})}{}\,(_{1}-_{2})- (_{1}-_{1})-(_{2}-_{2}),_{1}-_{2}\] \[_{1},_{2} )}{}\,(_{1}-_{2})-(_{2}-_{2}),_{1}-_{2}^{2}}\,.\]

Since \(\) is \(1\)-Lipschitz, using the bounds on \(_{\|u\|=1}_{ D_{j}}( -_{j}),u^{2}\) yields

\[_{1},_{2})}{}\,(_{1}-_{2})- (_{2}-_{2}),_{1}-_{2} ^{2}\] \[=_{j} D_{j}}{}\, (_{1}-_{2})-( _{1}-_{1})+(_{1}-_{1} )-(_{2}-_{2}),_{1}- _{2}^{2}\] \[ O(\|_{1}-_{2}\|^{4}+\|_{1 }-_{2}\|^{2})\,.\]

Hence, \(\|_{1}-_{2}\|^{2} O(\|_{1}-_{2}\|^{2}+\|_{1}-_{ 2}\|)\,.\) If \(\) is small enough (smaller than some constant that depends on the strong convexity parameter), we obtain that \(\|_{1}-_{2}\| O()\).

Similar to the bounded moment setting, we can generalize this observation to the case when for some integer \(k>1\) and some \(m_{2k}>0\), \(_{\|u\|=1}\,_{_{j} D_{j}} (-_{j}),u^{2k} m_{2k}^{2k}\). In this case we can use Holder's inequality to obtain the bound \(\|_{1}-_{2}\| O(m_{2k}^{1-1/(2k)})\). Note that in order to combine higher-order moment bounds with the filtering technique, we need an efficient procedure to certify these bounds. Thus, previous works only applied to distributions for which such moment certificates, in sum-of-squares, exist. We remark that in the symmetric setting we are able to obtain the necessary certificates for filtering _without_ making any sum-of-squares related assumption on the distribution.

Filtering in the symmetric settingWe will show how to adopt the filtering technique to make the above proof of identifiability algorithmic for symmetric distributions. Indeed, let \(D_{2}\) be a symmetric distribution with location \(^{*}\). Assume we can choose a loss function \(\) such that \(_{2}=^{*}\) and \(_{ D_{2}}\,(-_{2} )(-_{2})^{} _{d}\). Then, if \(D_{1}\) is an \(\)-corruption of \(D_{2}\), we can iteratively compute \(_{ D_{1}}\,(-_{1} )(-_{1})^{}\) and check its top eigenvalue. If it is smaller than \(1\), we know that \(_{1}\) must be \(O()\)-close to \(_{2}=^{*}\), if not, we can remove points which are aligned with the top eigenvector. Similar to the classical filtering setting, we can argue that this removes more corrupted points than uncorrupted points. Thus, after at most \(O( n)\) iterations, we must have that \(_{1}\) is indeed \(O()\)-close to \(^{*}\). A similar argument works for higher-order moments.

It remains to verify the assumptions we used on the loss function \(\) and its interaction with our distribution. First, we required that \(\) is smooth and strongly convex, which is very restrictive. Fortunately, we can relax the second assumption: For our argument it is enough if \(\) is _locally_ strongly convex around the minimizer (and globally smooth). Second, we assumed that \(_{ D_{2}}\,(-_{2} )(-_{j})^{} _{d}\). \((-_{2})\) can be seen as some transformation of the distribution, and our condition requires that the covariance of the transformed distribution is bounded. In general, this might not be satisfied even if the transformations are well-behaved (e.g., bounded and Lipschitz). However, we show that for appropriate \(\), it is indeed satisfied for elliptical and semi-product distributions. For the sake of exposition, we focus here only on elliptical distributions. For such distributions, we use the loss function \(_{E}(v):=r^{2}(\|v\|/r)\) for some \(r>0\). It is not hard to see that it is globally smooth and locally strongly convex (in some neighborhood of the minimizer). Further, \(_{E}(v)\) is a projection of \(v\) onto the ball of radius \(r\) (with center at zero). The covariance of such a projection is bounded by the covariance of the projection onto the sphere of radius \(r\). Fortunately, spherical projections of elliptical distributions are well-behaved. In particular, they only depend on the scatter matrix \(\), and hence we can without loss of generality assume that the initial elliptical distribution was Gaussian. For which we can obtain a bound on the covariance of \(_{E}(-_{2})\).

Similarly, we show that under mild assumptions on the scatter matrix (that in particular are satisfied for scatter matrices with condition number \(O(1)\)), we can certify tight bounds on the moments of \(_{E}(-_{2})\) in sum-of-squares, which leads to polynomial-time algorithms that obtain error \(o()\). We remark that prior to this work, in the unknown covariance case, robust mean estimation with error \(o()\) was only possible under the assumption that higher-order moments of \(D_{1}\) are certifiably bounded in sum-of-squares. We show that error \(o()\) is possible for arbitrary elliptical distributions that might not even have a first moment. Moreover, we show that by exploiting \(O((1/))\) many bounded moments of the transformed distributions, we achieve the near optimal error of \(O()\) using quasi-polynomially many samples and time polynomial in the input.

The above approach also works for semi-product distributions by choosing \(\) to be the entry-wise Huber-loss. This works both for the setting when \((-_{2})\) has bounded covariance and certifiably bounded higher-order moments. We remark that in this case we can achieve the information theoretically optimal error \(O()\) with quasi-polynomial number of samples in polynomial time (in the number of samples) - this again follows by exploiting \(O((1/))\) many moments.

### Nearly optimal error for semi-product distributions using polynomially many samples

Note that even in the standard Gaussian case, using the standard filtering approach yields (nearly) optimal algorithms only when using quasi-polynomially many samples. Note that this seems somewhat inherent to the approach since it only uses low-degree moment information. To reduce this to polynomially many samples, we can use a stronger identifiability statement that exists for the standard Gaussian distribution. In particular, let \(D_{2}=N(_{2},_{d})\) and \(D_{1}\) be an \(\)-corruption of \(D_{1}\) with mean \(_{1}\) and covariance \(_{1}\). Then, it holds that  (see also )

\[\|_{1}-_{2}\| O(+-_{d}\| +(1/))})\,.\]

Thus, we can hope to achieve error \(O()\) by checking if \(\|_{1}-_{d}\| O()\) and iteratively removing points aligned with the top eigenvector of \(_{1}-_{d}\). Indeed, a procedure very similar to this works (see e.g. ). However, it is crucial that we only remove points among the top \(\)-fraction of points correlated with the top eigenvector, since otherwise we cannot ensure that we remove more corrupted than uncorrupted points. The proof of the above uses _stability conditions_ of the Gaussian distribution (cf. ). That is, the proof uses that for i.i.d.samples \(_{1}^{*},,_{n}^{*} N(^{*},_{d})\) with \(n\) sufficiently large, it holds that for all subsets \(T\) of size \((1-10)n\) we have

\[\|_{i T}^{*}\| O( ) \|_{i T}^{*}^{*}-_{ d}\| O((1/))\,.\]

We show that the above approach can be adapted to the semi-product setting. Indeed, let \(D_{2}\) be a semi-product distribution with location \(^{*}\) and \(_{2}^{}=_{ D_{2}}( -^{*})((-^{*}))^{}\), where \(\) is the entry-wise Huber-loss. For simplicity assume that \(_{2}^{}=_{d}\) for some known \(\) (our approach extends to more general diagonal matrices and unknown \(\) as well - we show how to estimate all relevant parameters from the corrupted sample). Then, if \(D_{1}\) is an \(\)-corruption of \(D_{2}\), we show that for \(_{1}:=_{a^{d}}_{ D_{1}} (-a)\) it holds that

\[\|_{1}-^{*}\| O(+^{}- _{d}\|+(1/))})\,,\]where \(_{1}^{}\) is defined analogously to \(_{2}^{}\). Second, we show that for the filtering approach to work, it is enough that the _transformed distribution_, i.e., \((-^{*})\) for \( D_{2}\), satisfies the stability condition. This indeed follows since for our choice of \(\) (the entry-wise Huber-loss), \((-^{*})\) is sub-Gaussian.

However, since we do not work with the quadratic loss anymore, there are several technical obstacles. For the quadratic loss, the gradient is the identity function, and this fact is extensively used in the analysis of the Gaussian setting. For the Huber-loss gradient, different arguments are needed. To exemplify this, consider the following example - note that we do not expect the reader to see at which point in the analysis this step is necessary, it should merely illustrate the types of problems that arise. Let \(S_{g}\) denote the set of uncorrupted samples, \(T[n]\) be a set of size at least \((1-)n\), and \((T)\) be the minimizer of \(\) over samples in \(T\). In the analysis, terms of the following nature arise

\[|}_{i T S_{g}} ((T)-_{i}^{*})((T)- _{i}^{*})^{}\]

and we need to show that this term is bounded from below, in Loewner order, by \((-O((1/)))_{d}\) (uniformly for all \(T\) of size at least \((1-)\,n\)). In case of the quadratic loss, this follows easily from the stability conditions by adding and subtracting \((T S_{g})\).

When \(\) is the entry-wise Huber-loss, a more sophisticated argument is required. To describe our argument, we assume for simplicity that the entries of \(\) are mutually independent. We first show that \((T)-^{*}\) has entries of magnitude \(O()\). Then we show that for arbitrary but fixed (that is, non-random) \(\) with entries of small magnitude, the distribution of \((-^{*})\) is sub-Gaussian. We use this fact to show that with overwhelming probability 6

\[|}_{i T S_{g}} (-_{i}^{*})(- _{i}^{*})^{}(1-()) ((-^{*}))\,.\]

Then we use the fact that \(\) has small entries to show that \(((-^{*})) (-O())_{d}\). Finally, we use an \(\)-net over all possible \(\) to get the desired lower bound uniformly for all \(\), including \(\). This last \(\)-net argument is where we incur the (possibly) sub-optimal \(O(d d)\) term (instead of the optimal \(O(d)\)) in our sample complexity. Finally, if the entries of \(\) are not independent, we need to use this argument conditioned on the absolute values of the entries of \(\).

Summarizing, we have shown how to adjust the well-known filtering technique and incorporate the Huber-loss, to design robust algorithms for symmetric distributions, often matching what is known for the Gaussian distribution in (quasi-)polynomial time.