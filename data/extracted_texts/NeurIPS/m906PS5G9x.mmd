# Bayesian Adaptive Calibration and Optimal Design

Rafael Oliveira

CSIRO's Data61

Sydney, Australia

&Dino Sejdinovic

University of Adelaide

Adelaide, Australia

&David Howard

CSIRO's Data61

Brisbane, Australia

&Edwin V. Bonilla

CSIRO's Data61

Sydney, Australia

Corresponding author: rafael.dossantosdeoliveira@data61.csiro.au

###### Abstract

The process of calibrating computer models of natural phenomena is essential for applications in the physical sciences, where plenty of domain knowledge can be embedded into simulations and then calibrated against real observations. Current machine learning approaches, however, mostly rely on rerunning simulations over a fixed set of designs available in the observed data, potentially neglecting informative correlations across the design space and requiring a large amount of simulations. Instead, we consider the calibration process from the perspective of Bayesian adaptive experimental design and propose a data-efficient algorithm to run maximally informative simulations within a batch-sequential process. At each round, the algorithm jointly estimates the parameters of the posterior distribution and optimal designs by maximising a variational lower bound of the expected information gain. The simulator is modelled as a sample from a Gaussian process, which allows us to correlate simulations and observed data with the unknown calibration parameters. We show the benefits of our method when compared to related approaches across synthetic and real-data problems.

## 1 Introduction

In many scientific and engineering disciplines, computer simulation models form an essential part of the process of predicting and reasoning about complex phenomena, especially when real data is scarce. These simulation models depend on the inputs set by the user, commonly referred to as _designs_, and on a number of parameters representing unknown physical quantities, known as _calibration parameters_. The problem of setting these parameters so as to closely match observations of the real phenomenon is known as the calibration of computer models .

The seminal work by Kennedy and O'Hagan  introduces the Bayesian framework for calibration of simulation models, using Gaussian processes  to account for the differences between the model and reality, as well as for uncertainty in the calibration parameters. While the simulator is an essential tool when obtaining real data is expensive or unfeasible, each run of a simulator may itself involve significant computational resources, especially in applications such as climate science or complex engineering systems. In this situation, it is imperative to run simulations at carefully chosen settings of designs as well as of calibration inputs, using current knowledge to optimise resource use [3; 4; 5].

In this contribution, we bridge Bayesian calibration with adaptive experimental design  and use information-theoretic criteria  to guide the selection of simulation settings so that they are most informative about the true value of the calibration parameters. We refer to our approach as BACON (Bayesian Adaptive Calibration and Optimal designN). BACON allows computational resources to be focused on simulations that provide the most value in terms of reducing epistemic uncertainty. Importantly, in contrast to prior work, it optimises designs _jointly_ with calibration inputs in order to capture informative correlations across both spaces. Experimental results on synthetic experiments and a robotic gripper design problem demonstrate the benefits of BACON compared to competitivebaselines in terms of computational savings and the quality of the estimated posterior under similar computational constraints.

## 2 Problem formulation

Let \(f:\) represent a mapping of experimental designs \(\) to the outcomes of a physical process \(f()\). We are given a set of observed outcomes \(_{R}=[y_{1},,y_{R}]^{}\) and their associated designs \(_{R}:=\{_{i}\}_{i=1}^{R}\). Observations are corrupted by noise as \(y_{i}=f(_{i})+_{i}\), where \(_{i}(0,_{v}^{2})\) is zero-mean Gaussian noise, for \(i\{1,,R\}\). In addition, we have access to the output of a computer model \(h:\) given a design input and simulation parameters. Given an optimal setting for the calibration parameters \(^{*}\), the simulator \(h(,^{*})\), can be used to approximate the outcomes of the real physical process \(f()\). However, \(^{*}\) is unknown, and evaluations of the simulator \(h\) are costly, though cheaper than executing real experiments evaluating \(f\). Our task is to optimally estimate \(^{*}\) given the real data \(_{R}\), outputs of the simulator \(h\) and a prior distribution \(p(^{*})\), representing initial assumptions about \(^{*}\).

More concretely, let \(}_{S}:=[h(}_{i},}_{i})]_{i=1}^{S}\) represent simulated outcomes for a set of designs \(}_{S}:=\{}_{i}\}_{i=1}^{S}\) and simulation parameters \(_{S}:=\{}_{i}\}_{i=1}^{S}\). Given the cost of running simulations, we will associate the simulator \(h\) with a latent function (usually referred to as emulator) drawn from a Gaussian process (GP) prior and assume simulation outputs and real data follow a joint probability distribution \(p(_{R},}_{S},^{*})\).

In this setting, the Bayesian experimental design objective is to propose a sequence of simulations which will maximise the expected information gain (EIG) about \(^{*}\):

\[(}_{S},_{S})&:=(p(^{*}|_{R}))- _{p(}_{S}|}_{S},_{S},_{R})}[(p(^{*}|_{R}, }_{S}))]\\ &=_{p(}_{S}|}_{S}, _{S},_{R})}[_{}(p(^{*}|_{R},}_{S})||p(^{*}| _{R}))]\\ &=(^{*};}_{S}_ {R},}_{S},_{S})\,,\] (1)

where \(()\) represents the entropy of a probability distribution, \(_{}(||)\) denotes the Kullback-Leibler divergence, and \((^{*};}_{S}_{R})\) is the mutual information between \(^{*}\) and the simulator output \(}_{S}\) given the real observations \(_{R}\) and the simulator inputs to be optimized. We note here that, in our setting, the real observations \(_{R}\) are always fixed. Therefore, intuitively, the EIG above captures the reduction in uncertainty that will be obtained when selecting \((}_{S},_{S})\) averaged over all the possible outcomes \(}_{S}\).

## 3 Related work

Our work consists of deriving a Bayesian adaptive experimental design (BAED) approach to the problem of calibration. Therefore, in the following, we will briefly discuss current literature on these two main research areas.

### Adaptive experimental design

The problem of experimental design has a long history , spanning from classical fixed design patterns to modern adaptive approaches . Optimal experimental design consists of selecting experiments which will maximise some form of criterion involving a measure of utility of the experiment and its associated costs . Under the Bayesian formulation, uncertainty in the outcomes of the process is considered, and the optimality of a design is measured in terms of its expected utility . Information theory then allows us to quantify information gain as a utility function, which is commonly applied in modern approaches to Bayesian experimental design .

The estimation of posterior distributions becomes a computational bottleneck for information-theoretic Bayesian frameworks. Recent work has focused on addressing the difficulties in estimating the expected information gain by means of, e.g., variational inference , density-ratio estimation , importance sampling , and the learning of efficient policies to propose designs [16; 17]. These methods, however, usually assume that the simulator is known and inexpensive to evaluate. In contrast, the simulations themselves are modelled as expensive experiments for us, and we apply Gaussian process models as emulators to capture uncertainty over the black-box simulator. In addition, traditional BAED approaches assume that the prior is trivial to sample from and evaluate densities of, while in our case the starting prior is \(p(^{*}|_{R})\), which is likely non-trivial. We refer the reader to the recent review on modern Bayesian methods for experimental design by Rainforth et al.  for further details on BAED.

### Active learning for calibration

Experimental design approaches generally aim towards the selection of designs for physical experiments, whereas we are concerned with the problem of running optimal simulated experiments for model calibration in the presence of real data. When simulations are resource-intensive, a few methods have been derived based on the Bayesian calibration framework proposed by Kennedy and O'Hagan . Busby and Feraille  present an algorithm to learn GP emulators for a simulator which can then be combined with Bayesian inference algorithms, such as Markov chain Monte Carlo , to provide a posterior distribution over parameters. In their approach, the optimised variables are solely the calibration parameters, and the selection criterion is based on minimising the integrated mean-square error of the GP predictions. Many other approaches can be applied to this setting by modelling the simulator or its associated likelihood function as a GP, including Bayesian optimisation [3; 21; 22] and methods for adaptive Bayesian quadrature [23; 24]. Besides GPs, other algorithms focusing on the selection of calibration parameters have been derived using ensembles of neural networks  and deep reinforcement learning . These frameworks, however, do not allow for the selection of simulation design points, usually keeping them co-located with the real data.

Allowing for design point decisions to be included, Leatherman et al.  presented approaches for combined simulation and physical experimental design following geometric and prediction-error-based criteria, though using an offline, non-sequential framework. More recently, Marmin and Filippone  derived a deep Gaussian process  framework for Bayesian calibration problems and discussed an application with experimental design among other examples. Their experimental design approach to calibration was based on choosing simulations that maximally reduce the variational posterior variance over the calibration parameters, as measured by the derivatives of the evidence lower bound with respect to (w.r.t.) variance parameters. In contrast, we aim to directly maximise the information gain w.r.t. the unknown calibration parameters.

## 4 Gaussian processes for Bayesian calibration

To estimate information gain, we need a probabilistic model which can correlate simulations with real data and the unknown parameters \(^{*}\). Ideally, the model needs to allow for a computationally tractable conditioning on the parameters \(^{*}\) and account for the discrepancy between real and simulated data. Hence, we follow the Bayesian calibration approach in Kennedy and O'Hagan  and model:

\[f()= h(,^{*})+()\,, ,^{*} p(^{*}),\] (2)

where \(:\) represents the error (or discrepancy) between simulations and real outcomes, and \(\) accounts for possible differences in scale. We place Gaussian process priors on the simulator \(h(0,)\) and on the error function \((0,k_{})\).

### Bi-fidelity exact Gaussian process model

Since both \(h\) and \(\) are GPs, simulations and real outcomes can be jointly modelled as a single Gaussian process. In fact, both the simulator \(h\) and the true function \(f\) can be seen as different levels of fidelity of the same underlying process, with \(h\) representing a coarser version of \(f\). Namely, let \(s:=\{0,1\}\) denote a fidelity parameter. The combined model is then given by:

\[(,,s):=h(,),&s =0\\  h(,)+(),&s=1\,.\] (3)

such that \(f()=(,^{*},1)\) and \(h(},})=(},},0)\), for any \(,}\) and \(}\). As a result, for arbitrary points in the joint space \(,^{}:= \), the following covariance function parameterises the combined GP model \((0,k)\):

\[k(,^{}):=k_{}(s,s^{})((,),(^{},^{}))+ss^{}k_{ }(,^{})\] (4)where \(k_{}(s,s^{}):=(1+s(-1))(1+s^{}(-1))\), \(:=(,,s)\), and \(^{}:=(^{},^{},s^{})\). Therefore, any set of real and simulated evaluations are joint normally distributed under a combined GP model.

### Joint probabilistic model and predictions

Let \(_{R}:=_{R}(^{*}):=[(_{i},^ {*},1)]_{i=1}^{R}\) represent the set of partially observed inputs for real data \(_{R}\), and let \(}_{S}:=[(}_{i},},0)]_{i=1}^ {S}\) denote the current set of simulation inputs for the observations \(}_{S}\). Under the GP prior, the joint probability model \(p(}_{S},_{R},^{*})\) can be decomposed as:

\[p(}_{S},_{R},^{*})=p(}_{S}, _{R}|^{*})p(^{*})=_{}}p( }_{S}|}})p(_{R}|},^{*})p(}}|^{*})p(^{*})\, }\,,\] (5)

where \(}:=((^{*}))^{R+S}\), and \((^{*}):=\{_{R}(^{*}),}_{S}\}\) corresponds to the full set of inputs. The GP prior then allows us to model real and simulated outcomes jointly as a Gaussian random vector \(}\):

\[}|^{*}(,(^{*}))\,,\] (6)

where \((^{*}):=k((^{*}),(^{*}))=[k(,^{})]_{,^{ }(^{*})}\) denotes the prior covariance matrix. Assuming a Gaussian noise model for the observations \(y=f(,^{*})+()+\), with \((0,_{}^{2})\), the marginal distribution over the observations \(:=[_{R}^{T},}_{S}^{T}]^{}\) is available in closed form as:

\[p(}_{S},_{R}|^{*})=(; ,(^{*})+_{})\,,\] (7)

where \(_{}\) denotes the covariance matrix of the observation noise, i.e., \([_{}]_{ii}=_{}^{2}\) for any \(_{i}\) with \(s_{i}=1\), and \([_{}]_{ij}=0\) elsewhere.2

Under the GP assumptions, we can make predictions about \(=h(},})\) at any pair of \(},}\). Conditioning on \(^{*}\) and a dataset \(_{t}:=\{_{R},_{R},}_{t}, _{t},}_{t}\}\), let \(_{t}(^{*}):=\{_{R}(^{*}),}_{t}\}\) denote the set of inputs up to time \(t\) conditional on \(^{*}\), and \(_{t}\) the corresponding outputs. We then have that:

\[p(|^{*},},},_{t}) =(;_{t}(};^{*}),_{t}^{2} (};^{*}))\,,\] (8)

for \(}:=(},})\), where:

\[_{t}(};^{*}) :=_{t}^{}(};^{*})^{ }(_{t}(^{*})+_{_{t}})^{-1} _{t}\] (9) \[k_{t}(},}^{};^{*}) :=k(},}^{})-_{t}( {};^{*})^{}(_{t}(^{*})+ {}_{_{t}})^{-1}_{t}(}^{};^{*})\] (10) \[_{t}^{2}(;^{*}) :=k_{t}(},};^{*})\,,\] (11)

with \(_{t}(};^{*}):=k(_{t}(^ {*}),})\) and \(_{t}(^{*}):=k(_{t}(^{*}),_{t }(^{*}))\). We next describe how to apply this model to derive a Bayesian adaptive calibration algorithm.

## 5 Bayesian adaptive calibration and optimal design

In this section, we describe an approach to design experiments for calibration of computer models that incorporates information gathered during the experiments iteratively. We refer to these types of designs as _adaptive_. Thus, we consider the sequential design of experiments setting, where at each iteration \(t\), we optimise:

\[_{t}(},})& :=(^{*};},},_{t-1})\\ &=(p(^{*}|_{t-1}))-_{  p(|},},_{t-1})}[ (p(^{*}|,},}, _{t-1}))]\\ &=_{p(,^{*}|},},_{t-1})}[^{*}|,},},_{t-1})}{p(^{*}|_ {t-1})}],\] (12)

given the dataset \(_{t-1}:=\{_{R},_{R},}_{t-1}, _{t-1},}_{t-1}\}\) of observations. Given that the expected information gain is submodular , a sequential approach allows us to get close enough (usually a factor of at least \(1-1/e\)) to the optimal EIG over the whole experiment, while also allowing algorithmic decisions to adapt to current estimates for \(p(^{*}|_{t})\).

In general, computing the full EIG objective (1), or its sequential version (12), is intractable, as that requires estimating the true posterior and its density conditioned on sampled data. Note that both \(p(^{*}|,},}, _{t-1})\) and \(p(,^{*}|},}, _{t-1})\) depend on the posterior \(p(^{*}|_{t-1})\), as:

\[p(^{*}|,},}, _{t-1}) =,^{*}|},},_{t-1})}{p(|},},_{t-1})}\] (13) \[p(,^{*}|},},_{t-1}) =p(|^{*},},},_{t-1})p(^{*}|_{ t-1})\,,\] (14)

where the conditional predictive density \(p(|^{*},},}, _{t-1})\) is Gaussian and available in closed form (Eq. 8). Clearly, in general, the true posterior is intractable, since \(p(^{*}|_{t})=_{t}| {}^{*})p(^{*})}{p(_{t})}\) and \(p(_{t})=_{}p(_{t}|^{*})p( ^{*})\,^{*}\) involves integration over the entire parameter space \(\), which can be high dimensional and involve highly non-linear operations, such as computing inverse covariances. In addition, the marginal predictive density \(p(|},},_{t-1})=_{ }p(,^{*}|},},_{t-1})\,^{*}\) is also usually intractable for the same reasons.

### Variational EIG lower bound

Following Foster et al. , we replace the EIG by a variational objective which does not require the true posterior density over \(^{*}\). This formulation allows us to jointly estimate an approximation to the posterior and select optimal design points \(}\) and simulation parameters \(}\). Applying the variational lower bound by Barber and Agakov  to Eq. 12 yields the following alternative to the EIG:

\[}_{t}(},},q):= _{p(,^{*}|},},_{t-1})}[^{* }|,},})}{p(^{* }|_{t-1})}]_{t}(},}),\] (15)

where \(q(^{*}|,},})\) is any conditional probability density model. The gap is given by the expected Kullback-Leibler (KL) divergence between the true and the variational posterior (13, Sec. A.1):3

\[_{t}(},})-}_{t}(},},q)=_{p( |},},_{t-1})}[ _{}(p(^{*}|_{t-1},) ||q(^{*}|))] 0\,.\] (16)

Maximising the variational EIG lower bound w.r.t. the variational distribution \(q\) then provides us with an approximation to \(p(^{*}|,},}, _{t-1})\). Therefore, we can simultaneously obtain maximally informative designs and optimal variational posteriors by jointly optimising the EIG lower bound w.r.t. the simulator inputs and the variational distribution as:

\[}_{t},}_{t},q_{t}*{ argmax}_{},},q }}_{t}(},},q)=*{argmax}_{},},q}_{p(,^{* }|},},_{t-1})}[ q( ^{*}|)]\,,\] (17)

given a suitable variational family \(\) of conditional distributions. Note that, in this formulation, we only need samples from the posterior \(p(^{*}|_{t-1})\) to estimate the expectation above, which can be approximated via Monte Carlo, without requiring densities other than that of the variational model \(q\).

### Algorithm

Algorithm 1 summarises the method we propose, which we name _Bayesian Adaptive Calibration and Optimal desigN_ (BACON). The algorithm starts with an initial dataset \(_{0}\) containing the real data (and possibly previously available simulation data) and an estimate of the posterior given the initial data \(p(^{*}|_{0})\). Posterior estimates in BACON can be represented by samples obtained via Markov chain Monte Carlo (MCMC) or variational inference over the GP model and the currently available data \(_{t}\). Note that we only need samples from the previous posterior to estimate the expectation in Eq. 17, with no need to directly evaluate its probability densities. Each iteration starts by optimising the variational EIG lower bound using the objective in Eq. 17 to jointly select an optimal design \(}_{t}\), simulation parameters \(}_{t}\) and variational posterior \(q_{t}\). Given the new design \(}_{t}\), we run the simulation with the chosen parameters \(}_{t}\), observing a new outcome \(_{t}\). The calibration posterior \(p_{t}(^{*})\) and the GP model are then updated with the new data, potentially including a re-estimation of the GP hyper-parameters via, for example, maximum likelihood estimation. The process then repeats given the updated GP and posterior for up to a given number of iterations \(T\). At the end, a final posterior \(p_{T}(^{*})=p(^{*}|_{R},}_{T})\) and a conditional density model \(q_{T}\) are obtained.

### Variational posteriors

Any conditional probability density model \(q(^{*}|)\) estimating probability densities over the parameter space \(\) given an observation \(\) could suit our method. In the following, we describe two possible parameterisations for this model. The first facilitates marginalising latent inputs in GP regression [31; 32], while the second better captures multi-modality in the posterior.

Conditional Gaussian models.Assuming we can approximate \(p(^{*}|_{t})\) as a Gaussian, we can construct a variational conditional density model as:

\[q_{}(^{*}|,},}):=(^{*};_{}(,},}),_{}(,},}))\,,\] (18)

where \(_{}\) and \(_{}\) are given by parametric models, such as neural networks, with parameters \(\). To ensure \(_{}()\) is positive-definite, it can be parameterised by its Cholesky decomposition \(_{}()=_{}()_{}()^{}\), where \(_{}()\) is a lower-triangular matrix with positive diagonal entries.

Conditional normalising flowsNormalising flows  apply the change-of-variable formula to derive composable, invertible transformations \(_{}\) of a fixed base distribution \(p_{0}\):

\[_{}(_{0}):=_{}^{(K)} _{}^{(1)}(_{0}),_{0} p_{0}\] (19)

The log-probability density of a point \(=_{}(_{0})\) under this model can be calculated as:

\[ p_{K}(;)= p_{0}(_{0})-_{j=1}^{K} |_{}^{(j)}(_{j-1})|\,,\]

where \(_{0}:=_{}^{-1}()\), \(_{j}:=_{}^{(j)}(_{j-1})\), and \(_{}^{(j)}\) is the Jacobian matrix of the \(j\)th transform \(_{}^{(j)}\), for \(j\{1,,K\}\). Several invertible flow architectures have been proposed in the literature, including radial and planar flows , autoregressive models [34; 35; 36] and models based on splines .

To derive a conditional density model \(q_{}(^{*}|)\), conditional normalising flows map the original flow parameters \(\) via a neural network model \(_{}:\)[38; 39]. The resulting variational conditional density model is then given by:

\[ q_{}(^{*}|,}, })= p_{K}(^{*};_{}(,},}))\,.\] (20)

### Batch parallel evaluations

Often simulations can be run in parallel by spawning multiple processes in a single machine or over a high-performance computing cluster. In this case, proposing batches with multiple simulation inputs can be more effective than running single simulations in a sequence. Optimising the EIG w.r.t. a batch of inputs \(:=\{}_{i},}_{i}\}_{i=1}^{B}\), instead of single points, we obtain a batch version of Algorithm 1. In this case, we are seeking a batch that maximises the mutual information between the parameters \(^{*}\) and the resulting simulation outcomes, i.e.:

\[_{t}()=(^{*};\{_{i}\}_{i=1 }^{B}|,_{t-1})_{p(\{_{i}\}_{i=1}^{B },^{*}|,_{t-1})}[^ {*}|\{_{i}\}_{i=1}^{B})}{p(^{*}|_{t-1})}]\] (21)

We optimise this objective with variational models that accept multiple conditioning observations \(q(^{*}|_{1},,_{B})\). In practice, this simply amounts to replacing the single conditioning entries to the models in Sec. 5.3 by the concatenated batch or a permutation-invariant deep set encoding [16; 40].

## 6 Experiments

In this section, we present experimental results on synthetic and real-data problems evaluating the proposed variational Bayesian adaptive calibration framework against baselines. Further experimental details can be found in Appendix A and in our code repository.4

Performance metrics.We evaluated each method against a set of performance metrics, which we now describe. The maximum-a-posteriori (MAP) error measures the distance between the mode of the variational distribution and the true parameters \(^{*}\). To measure the quality of the learnt model in predicting real outcomes, we also evaluated the root mean square error (RMSE) between the expected GP predictions under the learnt variational distribution and real outcomes: \(:=_{i=1}^{N}(_{() }[(_{i}^{*},^{*};)]-y_{i}^{*})^{2}}\), where \(y_{i}^{*}=f(_{i}^{*})+_{i}^{*}\) are observations of the real process over a set of test points \(\{_{i}^{*}\}_{i=1}^{N}\) placed on a uniform grid over the design space.

Information gain.Lastly, we also evaluated two sample-based estimates of the KL divergence . Namely, \(_{}(p_{T}||p_{0})\) corresponds to the KL divergence between the final MCMC posterior (given all simulations and real data) and the initial one (given only the real data and an initial set of randomised simulations) both estimated over the learnt GP model. The column \(_{}(p_{T}||p^{*})\) indicates the KL divergence between the final MCMC posterior \(p_{T}\) and the posterior \(p^{*}\) with full knowledge of the simulator, which can be cheaply evaluated in this synthetic scenario. The average of \(_{}(p_{T}||p_{0})\) is an indicator for the expected information gain (1) of an algorithm, given that it is the expected relative entropy across the possible trajectories of observations. In contrast, \(_{}(p_{T}||p^{*})\) indicates how far the estimates are from the best possible posterior obtainable with a model that is given the available real data and (a potentially infinite amount of) simulations.

Figure 1: Experimental results on synthetic data where the target posterior \(p^{*}\) is unimodal. The first 3 plots show estimates for performance metrics as a function of the number of simulations run (not including the initial data). Estimates were computed based on the posterior estimates for each method available during their run, with _random_ using \(p(^{*})\), D-optimal and BACON using MCMC posteriors, and IMSPE using a Dirac delta (reverse KL undefined, not shown) on the MAP estimate as posterior estimates. Results are averaged over 10 trials, and shaded areas indicate \( 1\) standard deviation. The rightmost plot shows the target posterior, with the true \(^{*}\) indicated by a star.

Figure 2: Experimental results on synthetic data where the target posterior \(p^{*}\) is bimodal. See Fig. 1 for details, with the exception that the rightmost plot now shows the bimodal target posterior.

### Baselines

Our algorithmic baselines were chosen to illustrate the main approaches currently available in the literature. All baselines are implemented as sequential methods, in the sense that their GP models are updated with the latest batch of observations before proceeding to the next iteration.

Random search.This baseline samples simulation designs \(}_{t}()\) from a uniform distribution over the design space \(\) and calibration parameters from the prior \(}_{t} p(^{*})\).

IMSPE with MAP estimates.The integrated mean squared prediction error (IMSPE)  criterion chooses designs \(}_{t}\) and calibration \(}_{t}\) parameters by minimising the GP prediction error:

\[_{t}(}):=_{}[(( )-_{t+1}(;^{*}))^{2}( }),_{t}]\,=_{} _{t+1}^{2}(;^{*}|_{t},( }))\,.\] (22)

The posterior MAP estimate \(^{*}_{t}*{argmax}_{}p( |_{t-1})\) is used as a point estimate for the true \(^{*}\). The integral is approximated as a sum over a uniform grid of designs and samples from the calibration prior,5 making IMSPE equivalent to active learning Cohn  and also a form of A-optimality .

D-optimal designs.We provide experimental results with an additional baseline following a D-optimality criterion, a classic experimental design objective. Optimal candidate designs according to this criterion are points of maximum uncertainty according to the model . If we model the simulator as the unknown variable of interest, this corresponds to selecting designs where we have maximum entropy of the Gaussian predictive distribution \(p(|},},_{t-1})\). This approach, therefore, simply attempts to collect an informative set of simulations according to the GP prior over the simulator \(h\) only, without considering the information in the real data. Running D-optimality on \(^{*}\), instead, would lead back to the EIG criterion we use.

Variational Bayesian Monte Carlo (VBMC).Acerbi  presents an adaptive Bayesian quadrature method to learn posterior distributions over models with black-box likelihood functions. The method estimates the posterior \(p(^{*}|_{R},h)\) by modelling the log-joint \( p(_{R},^{*}|h)\) as a sample from a Gaussian process. VBMC then learns a variational posterior approximation by maximising a lower-confidence bound over the ELBO given by the GP estimates. Calibration parameter queries \(}_{t}\) are obtained by optimising quadrature-based acquisition functions. Regarding design points, simulations are always run on the set of real design points \(_{R}\) in the observed data, which is fixed.

### Synthetic experiments

For this experiment, we sampled a function \((0,k)\) to use as our simulator and compared different algorithms. Following a sparse GP approach , a function sampled from a GP can

Table 1: Results for 2+2D synthetic problem after \(T=50\) iterations (batch of \(B=4\)). Here \(_{}(p_{T}||p_{0})\) corresponds to the KL divergence between the final posterior (estimated after each algorithm’s run with all the data it collected) and the starting one (higher is better), while \(_{}(p_{T}||p^{*})\) is the KL between the final posterior and the posterior with full knowledge of the simulator \(p^{*}\) (lower is better). All posteriors were sampled via MCMC using 4000 samples. Averages and standard deviations were estimated from 10 independent runs.

be approximated as \(() k(,_{M})_{M}^{-1}_{M}\), where \(_{M}(}_{M},_{M})\) is a sample from an \(M\)-dimensional Gaussian, \(_{M}:=\{_{i}\}_{i=1}^{M} \{0,1\}\), for a given \(M\). As the number of points \(M\), if the pseudo-inputs \(_{M}\) form a dense set, the approximate \(\) should converge in distribution to a sample from the Gaussian process \((0,k)\). In our case, to sample \(_{M}\), we sample designs from a uniform distribution over the design space, calibration parameters from the prior, and fidelities from a Bernoulli distribution with parameter set to \(0.5\). We also set \(}_{M}:=\) and \(_{M}:=_{M}=k(_{M},_{M})\). We repeatedly run a loop of \(T\) iterations for each algorithm, with different random seeds.

We run each algorithm for \(T:=50\) iterations using a batch of \(B:=4\) designs per iteration. Each of the methods using GP approximations for the simulator are initialised with 20 observations and \(R=5\) real data points. To configure VBMC, we allow it to run an equivalent maximum amount of objective function evaluations. The design space is set as the 2-dimensional unit box \(:=^{2}\) and the "true" parameters are sampled from a standard normal prior \(p(^{*}):=(^{*};, )\) also over a 2D space, totalling a 4-dimensional problem space.

Results are presented in Fig. 1 and 2. Fig. 1 shows a case where the GP-sampled simulator led to a unimodal target posterior. In this case, we see that BACON is able to achieve fast convergence in terms of MAP estimates and KL divergence towards the target posterior, while IMSPE dominates in terms of simulator approximation error as measured by the RMSE. As the posterior is unimodal and quite concentrated around the true parameter, it is natural that a method relying on MAP estimates, such as RMSE, would perform well. In contrast, when the posterior is multimodal, as shown in the bimodal case in Fig. 2, MAP estimates are not necessarily reliable any more, as they might get stuck on a non-informative mode, leading to biased estimates for IMSPE and a significant drop in performance. Lastly, note that D-optimal and random designs can also lead to RMSE approaching the lowest (as determined by the noise level with \(_{}=0.5\)) in some circumstances. However, these approaches do not directly provide posterior approximations and may fail in more complex scenarios.

In terms of final posterior estimates, Table 1 shows that VBMC estimates reach the closest to the full-knowledge target posterior \(p^{*}\) in the unimodal case, while BACON is able to surpass the other GP-emulation approaches in terms of information gain. For the bimodal case, however, we see that BACON gains an advantage over VBMC. Recall that VBMC relies on a variational mixture of Gaussian distributions, while BACON applies conditional normalising flows for its posterior approximations, which lead to increased flexibility. In addition, despite the slightly worse performance than VBMC, BACON also provides a GP model that can be used as an emulator for the simulator (and to approximate the real process), while VBMC's focus is on approximating the log-likelihood.

### Finding the location of hidden sources

We consider the problem of finding the location of 2 hidden sources in a 2D environment following the setting in Foster et al. . We are provided with \(R=20\) initial measurements and an initial set of \(S=20\) randomised simulations without knowledge of the true parameters which the data was generated with. Sources are sampled from a standard normal, the design space is limited to the unit box, and noise is sampled with \(_{}=0.5\). Our results are presented in Table 2, showing a similar tendency in higher information gain for our method, and a very low KL w.r.t. \(p^{*}\). Note that a higher information gain indicates a more informative posterior, whose entropy will be much lower relative to the starting distribution, compared to the other methods. In addition, the ideal \(p^{*}\), which a GP-based posterior should converge to in the limit of infinite data, is not known by the methods, only \(p_{0}\). Therefore, besides obtaining maximally informative data, we have shown that BACON is also efficient in approximating posteriors over black-box simulators, while also learning a GP emulator.

   & \(_{}(p_{T}|p_{0})\) & \(_{}(p_{T}|p^{*})\) \\  BACON & \(\) & \(\) \\ IMSPE & 0.22 \(\) 0.11 & 0.45 \(\) 0.21 \\ D-optimal & 0.21 \(\) 0.08 & 0.23 \(\) 0.10 \\ Random & 0.32 \(\) 0.09 & 0.20 \(\) 0.14 \\ VBMC & – & 5.48 \(\) 1.66 \\  

Table 2: Results on the location finding problem after \(T=30\) iterations with \(B=4\), \(R=20\) “real” data points and an initial set of 20 simulations. Estimates were averaged over 10 independent runs.

### Soft-robotic grasping simulator calibration

For this experiment, we are provided with a dataset containing \(R=10\) real measurements of the peak grasping force of soft robotic gripper designs on a range of testing objects (see Fig. 3). The gripper designs follow a fin-ray pattern parameterised by 9 geometric parameters , and we are interested in estimating 2 unknown physics parameters, the Young's modulus of elasticity and the coefficient of static friction with the objects. To simulate the gripper designs, we use the SOFA framework  to reproduce the grasping scenario and provide an estimate of the peak grasping force. In particular, for this paper, we focus on the grasping of a spherical object, which provides a simpler geometry and lower discrepancy with respect to real data measurements compared to more complex objects. This experiment provides us with a benchmark where simulations are expensive to run, taking from minutes to a few hours to run (depending on mesh resolution) on a high-performance computing platform. Therefore, it is important to choose a minimum amount of informative simulations.

Our results are shown in Table 3. Each algorithm was initialised with a set of 123 random simulations and run for \(T=10\) iterations. The results show that BACON achieves the closest approximation to the target posterior. IMSPE highly concentrated its parameter choices around its posterior mode estimate, while other baselines were too spread, both leading to inferior posterior approximations (see Fig. 4 in the appendix) and showing the advantage of BACON's joint optimisation and inference.

## 7 Conclusion, limitations and future work

We have developed BACON, a Bayesian approach that carries out parameter calibration of computer models and optimal design of experiments _jointly_. It does so by optimizing an information-theoretic criterion so that input designs and calibration parameters are selected to be maximally informative about the optimal parameters. Our method provides a full posterior over optimal calibration parameters as well as an accurate Gaussian process based estimation of the computer model (i.e., an emulator). One of the main limitations of the presented framework, however, is scalability to large datasets, due to the cubic computational complexity of exact inference with GPs. A potential extension with scalable sparse variational GP models  using a conditional distribution model for the inducing points is discussed in Sec. B.2. We emphasize that our proposed method is still applicable to many real practical settings, where the problem constraints do not demand a very large number of simulation samples. Lastly, we also note that the method can be adapted to work with vector-valued observations by the use of multi-output GP models . Further discussions on limitations and future work can be found in our appendix (see Appendix B and C).

   & \(_{}(p_{T}||p^{*})\) \\  BACON & **1.32 \(\) 0.05** \\ IMSPE & 1.56 \(\) 0.08 \\ D-optimal & 1.50 \(\) 0.05 \\ Random & 1.48 \(\) 0.07 \\  

Table 3: Soft-robotics simulator calibration final results after \(T=10\) with \(B=16\) points per batch. The target posterior \(p^{*}\) was inferred using a large set of 1024 random simulations uniformly covering the design and parameter space. Performance was averaged over 4 independent runs.

Figure 3: Soft-robotics grasping experiment. We calibrate a soft materials simulator against real data from physical grasping from an automated experimentation platform