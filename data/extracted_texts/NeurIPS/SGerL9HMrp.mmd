# Tracking Most Significant Shifts

in Nonparametric Contextual Bandits

 Joe Suk

Columbia University

joe.suk@columbia.edu

&Samory Kpotufe

Columbia University

samory@columbia.edu

###### Abstract

We study nonparametric contextual bandits where Lipschitz mean reward functions may change over time. We first establish the minimax dynamic regret rate in this less understood setting in terms of number of changes \(L\) and total-variation \(V\), both capturing all changes in distribution over context space, and argue that state-of-the-art procedures are suboptimal in this setting.

Next, we tend to the question of an _adaptivity_ for this setting, i.e. achieving the minimax rate without knowledge of \(L\) or \(V\). Quite importantly, we posit that the bandit problem, viewed locally at a given context \(X_{t}\), should not be affected by reward changes in other parts of context space \(\). We therefore propose a notion of _change_, which we term _experienced significant shifts_, that better accounts for locality, and thus counts considerably less changes than \(L\) and \(V\). Furthermore, similar to recent work on non-stationary MAB (Suk and Kpotufe, 2022), _experienced significant shifts_ only count the most _significant_ changes in mean rewards, e.g., severe best-arm changes relevant to observed contexts.

Our main result is to show that this more tolerant notion of change can in fact be adapted to.

## 1 Introduction

Contextual bandits model sequential decision making problems where the reward of a chosen action depends on an observed context \(X_{t}\) at time \(t\), e.g., a consumer's profile, a medical patient's history. The goal is to maximize the total rewards over time of chosen actions, as informed by seen contexts. As such, one suitable measure of performance is that of _dynamic regret_, which compares earned rewards to a time-varying oracle maximizing mean rewards at \(X_{t}\). While it is often assumed in the bulk of works in this setting that rewards distributions remain stationary over time, it is understood that in practice, environmental changes induce nontrivial changes in rewards.

In fact, the problem of non-stationary environments has received a surge of attention in the simpler non-contextual Multi-Arm-Bandits (MAB) setting, while the more challenging contextual case remains ill-understood. In particular in the contextual case, some recent works (Wu et al., 2018; Luo et al., 2018; Chen et al., 2019; Wei and Luo, 2021) consider _parametric settings_, i.e. where reward functions belong to fixed parametric family, and show that one may achieve rates adaptive to an unknown number of \(L\) of shifts in rewards or to a notion of total-variation \(V\), both acccounting for all changes over time and context space. Instead here, we consider a much larger class of reward functions, namely Lipschitz rewards, corresponding to the natural assumption that closeby contexts have similar rewards even as reward distributions change.

As a first result for this nonparametric setting, we establish some minimax lower-bounds as a baseline in terms of either \(L\) or \(V\), and argue that state-of-the-art procedures for the parametric case--extended to the class of Lipschitz functions--do not achieve these baselines.

We then turn attention to whether such baselines may be achieved _adaptively_, i.e., without knowledge of \(L\) or \(V\). The answer as we show is affirmative, and more importantly, some much weaker notions of change may be adapted to; for intuition, while \(L\) or \(V\) accounts for any change at any time over the context space (say \(\)), it may be that all changes are relegated to parts of the space irrelevant to observed contexts \(X_{t}\) at the time they are played. For instance, suppose at time \(t\), we observe \(X_{t}=x_{0}\), then it may not make sense to count changes that happen at some other \(x_{1}\) far from \(x_{0}\), or changes that happened at \(x_{0}\) itself but far back in time.

We therefore propose a new parameterization of change, termed _experienced significant shifts_ that better accounts for the locality of changes in time and space, and as such may register much less changes than either \(L\) or \(V\). As a sanity check, we show that an oracle policy which restarts only at experienced significant shifts can attain enhanced regret rates in terms of the number \(=(X_{1},,X_{T})\) of such experienced shifts (Proposition 2), a rate always no worse that the baseline we first established in terms of \(L\) and \(V\).

Our main result is to show that _experienced significant shifts_ can be adapted to (Theorem 3), i.e., with no prior knowledge of such shifts. Importantly, the result holds in both stochastic environments, and in (oblivious) adversarial ones with no change to our notion, algorithmic approach, nor analysis. Furthermore, similar to recent advances in the non-contextual case (Abbasi-Yadkori et al., 2022; Suk and Kpotufe, 2022), an _experienced shift_ is only triggered under _severe changes_ such as changes of best arms locally at a context \(X_{t}\). An added difficulty in the contextual case is that we cannot hope to observe rewards for a given arm (action) repeatedly at \(X_{t}\) as the context may only appear once, and have to rely on carefuly chosen nearby points to identify unknown shifts in reward at \(X_{t}\).

### Other Related Work

Nonparametric Contextual Bandits.The stationary bandits with covariates (where rewards and contexts follow a joint distribution) was first introduced in a one-armed bandit problem (Woodroofe, 1979; Sarkar, 1991), with the nonparametric model first studied by Yang et al. (2002). Minimax regret rates, based on a margin condition, were first established for the two-armed bandit in Rigollet and Zeevi (2010) and generalized to any finite number of arms in Perchet and Rigollet (2013), with further insights thereafter (Qian and Yang, 2016, 2016, Reeve et al., 2018; Guan and Jiang, 2018; Hu et al., 2020; Arya and Yang, 2020; Suk and Kpotufe, 2021; Gur et al., 2022; Cai et al., 2022). However, the mentioned works all assume a stationary distribution of rewards over contexts. Blanchard et al. (2023) studies non-stationary nonparametric contextual bandits, but in the much-different context of universal learning, concerning when sublinear regret is achievable asymptotically. Lipschitz contextual bandits also appears as part of studies on broader infinite-armed settings (Lu et al., 2009; Krishnamurthy et al., 2019). Related, Slivkins (2014) allows for non-stationary (i.e., obliviously adversarial) environments, but only studies regret to the (per-context) best arm in hindsight. _Realizable contextual bandits_ posits that the regression function capturing mean rewards in contexts lies in some known class of regressors \(\), over which one can do empirical risk minimization (Foster et al., 2018; Foster and Rakhlin, 2020; Simchi-Levi and Xu, 2021). While this setting recovers Lipschitz contextual bandits, the only applicable non-stationary guarantee to our knowledge is Wei and Luo (2021), which yields suboptimal dynamic regret (see Table 1).

Non-Stationary Bandits and RL.In the simpler non-contextual bandits, changing reward distributions (a.k.a. _switching bandits_) was introduced in Garivier and Moulines (2011) and further explored with various assumptions and formulations (Karnin and Anava, 2016; Allesiardo et al., 2017; Liu et al., 2018; Wei and Srivatsva, 2018; Besbes et al., 2019; Cao et al., 2019; Mukherjee and Maillard, 2019; Besson et al., 2022). While these earlier works focused on algorithmic design assuming knowledge of non-stationarity, such a strong assumption was removed via the _adaptive_ procedures of Auer et al. (2019), Chen et al. (2019). In followup works, Abbasi-Yadkori et al. (2022), Suk and Kpotufe (2022) show that tighter dynamic regret rates are possible, scaling only with severe changes in best arm. The ideas from non-stationary MAB were extended to various contextual bandit settings by Wu et al. (2018) (for linear mean rewards in contexts), Luo et al. (2018), Chen et al. (2019) (for finite policy classes), and Wei and Luo (2021) (for realizable mean reward functions). There have also been extensions to various reinforcement learning setups (Jaksch et al., 2010; Gajane et al., 2018; Chi Cheung et al., 2019; Ortner et al., 2020; Fei et al., 2020; Cheung et al., 2020; Touati and Vincent, 2020; Domingues et al., 2021; Mao et al., 2021; Domingues et al., 2021; Wei and Luo, 2021; Zhou et al., 2022; Lykouris et al., 2021; Wei et al., 2022; Chen and Luo, 2022; Ding and Lavaei, 2023).

Of these works, only Domingues et al. (2021) can recover Lipschitz contextual bandits, whereupon we find their dynamic regret bounds are suboptimal (see Table 1). Again, the typical aim of the aforementioned works on contextual bandits or RL is to minimize a notion of dynamic regret in terms of the number of changes \(L\) or total-variation \(V\). As such, the guarantees of such works don't involve tighter notions of experienced non-stationarity, such as those studied in this work.

## 2 Problem Formulation

### Contextual Bandits with Changing Rewards

**Preliminaries.** We assume a finite set of arms \([K]\{1,2,K\}\). Let \(Y_{t}^{K}\) denote the vector of rewards for arms \(a[K]\) at round \(t[T]\) (horizon \(T\)), and \(X_{t}\) the observed context at that round, lying in \(^{d}\), which have joint distribution \((X_{t},Y_{t})_{t}\). We let \(_{t}\{X_{s}\}_{s t},_{t}\{Y_{s}\}_{s t}\) denote the observed contexts and (observed and unobserved) rewards from rounds \(1\) to \(t\). In our setting, an oblivious adversary decides a sequence of (independent) distributions on \(\{(X_{t},Y_{t})\}_{t[T]}\) before the first round.

**Notation.**_The_ **reward function**_\(f_{t}:^{K}\) is \(f_{t}^{a}(x)[Y_{t}^{a}|X_{t}=x],\,a[K]\), and captures the mean rewards of arm \(a\) at context \(x\) and time \(t\)._

A _policy_ chooses actions at each round \(t\), based on observed contexts (up to round \(t\)) and passed rewards, whereby at each round \(t\) only the reward \(Y_{t}^{a}\) of the chosen action \(a\) is revealed. Formally:

**Definition 1** (Policy).: _A policy \(\{_{t}\}_{t}\) is a random sequence of functions \(_{t}:^{t}[K]^{t-1}^{t-1}[K]\). A_ **randomized** _policy \(_{t}\) maps to distributions on \([K]\), In an abuse of notation, in the context of a sequence of observations till round \(t\), we'll let \(_{t}[K]\) denote the (possibly random) action chosen at round \(t\)._

The performance of a policy is evaluated using the _dynamic regret_, defined as follows:

**Definition 2**.: _Fix a context sequence \(_{T}\). Define the_ **dynamic regret** _of a policy \(\), as_

\[R_{T}(,_{T})_{t=1}^{T}_{a[K]}f_{t}^{a}(X_{t})-f _{t}^{_{t}}(X_{t}).\]

So, we aim to minimize \([R_{T}(,_{T})]\) where the expectation is over \(_{T}\), \(_{T}\), and randomness in \(\).

**Notation.**_As much of our analysis focuses on the gaps in mean rewards between arms at observed contexts \(X_{t}\), the following notation will serve useful. Let \(_{t}(a^{},a) f_{t}^{a^{}}(X_{t})-f_{t}^{a}(X_{t})\) denote the_ **relative gap** _of arms \(a\) to \(a^{}\) at round \(t\) at context \(X_{t}\). Define the_ **worst gap** _of arm \(a\) as \(_{t}(a)_{a^{}[K]}_{t}(a^{},a)\), corresponding to the instantaneous regret of playing \(a\) at round \(t\) and context \(X_{t}\). Thus, the dynamic regret can be written as \(_{t[T]}[_{t}(_{t})]\). Additionally, we will occasionally talk about the_ **gap functions** _in context \(x\). In an abuse of notation, let \(_{t}^{a^{},a}(x) f_{t}^{a^{}}(x)-f_{t}^{a}(x)\) and \(_{t}^{a}(x)_{a^{}[K]}_{t}^{a^{},a}(x)\). Generally, when a context \(x\) is not specified, as in the quantities \(_{t}(a^{},a),_{t}(a)\), it should be assumed that the context in question is \(X_{t}\)._

### Nonparametric Setting

We assume, as in prior work on nonparametric contextual bandits (Rigollet and Zeevi, 2010; Perchet and Rigollet, 2013; Slivkins, 2014; Reeve et al., 2018; Guan and Jiang, 2018; Suk and Kpotufe, 2021), that the reward function is \(1\)-Lipschitz.

**Assumption 1** (Lipschitz \(f_{t}\)).: _For all rounds \(t\), \(a[K]\) and \(x,x^{}\),_

\[|f_{t}^{a}(x)-f_{t}^{a}(x^{})|\|x-x^{}\|_{}.\] (1)

For ease of presentation, we assume the contextual marginal distribution \(_{X}\) remains the same across rounds. Furthermore, we make a standard _strong density assumption_ on \(_{X}\), which is typical in this nonparametric setting (Audibert and Tsybakov, 2007; Perchet and Rigollet, 2013; Qian and Yang, 2016; Gu et al., 2022; Hu et al., 2020; Arya and Yang, 2020; Cai et al., 2022). This holds, e.g. if \(_{X}\) has a continuous Lebesgue density on \(^{d}\), and ensures good coverage of the context space.

**Assumption 2** (Strong Density Condition).: _There exist \(C_{d},c_{d}>0\) s.t. \(_{}\) balls \(B^{d}\) of diameter \(r(0,1]\):_

\[C_{d} r^{d}_{X}(B) c_{d} r^{d}.\] (2)

**Remark 1**.: _We can in fact relax Assumption 2 so that \(_{X,t}()\) is changing in \(t\) and (2) is satisfied with different constants \(C_{d,t},c_{d,t}\). Our procedures in the end will not require knowledge of any \(C_{d,t},c_{d,t}\)._

### Model Selection

A common algorithmic approach in nonparametric contextual bandits, starting from earlier work , is to discretize or partition the context space \(\) into _bins_ where we can maintain local reward estimates. These bins have a natural hierarchical tree structure which we first elaborate.

**Definition 3** (Partition Tree).: _Let \(\{2^{-i}:i\{0\}\}\), and let \(_{r},r\) denote a regular partition of \(^{d}\) into hypercubes (which we refer to as_ **bins**_) of side length (a.k.a. bin size) \(r\). We then define the dyadic_ **tree**_\(\{_{r}\}_{r}\), i.e., a hierarchy of nested partitions of \(^{d}\). We will refer to the_ **level**_\(r\)_**of \(\) as the collection of bins in partition \(_{r}\). The_ **parent** _of a bin \(B_{r},r<1\) is the bin \(B^{}_{2r}\) containing \(B\);_ **child**_,_ **ancestor** _and_ **descendant** _relations follow naturally. We'll use \(T_{r}(x)\) to refer to the bin at level \(r\) containing \(x\), while \(r(B)\) is the side length of bin \(B\)._

Note that, while in the above definition, \(\) has infinite levels \(r\), at any round \(t\) in a procedure, we implicitly only operate on the subset of \(\) containing data.

Key in securing good regret is then finding the optimal level \(r\) of discretization (balancing local regression bias and variance), which over \(T\) stationary rounds is known to be \((K/T)^{}\). The intuition for this choice is that a bias of \(T^{-}\) is safe to pay for \(T\) rounds to maintain a minimax regret rate of \(T^{}\) (see rates in Theorem 1). We introduce the following general notation, useful later in the non-stationary problem, for associating a level with an intervals of rounds.

**Notation 1** (Level).: _For \(n\{0\}\), let \(r_{n}\) be the largest \(2^{-m}\) such that \((K/n)^{} 2^{-m}\). When \(I\) is an interval of rounds, we use \(r_{I}\) as shorthand for \(r_{|I|}\), where \(|I|\) is the length of \(I\). We use \(_{m},T_{m}(x)\) as shorthand to denote (respectively) the tree \(_{r}\) of level \(r=r_{m}\) and the (unique) bin at level \(r_{m}\) containing \(x\)._

## 3 Results Overview

### Minimax Lower Bounds Under Global Shifts

As a baseline, we start with some basic lower-bounds under the simplest parametrizations of changes in rewards which have appeared in the literature, namely a _global number of shifts_, and _total variation_.

**Definition 4** (Global Number of Shifts).: _Let \(L_{t=2}^{T}\{ x,a[K]:f_{t}^{a}(x ) f_{t-1}^{a}(x)\}\) be the number of global shifts, i.e., it counts every change in mean-reward overtime and over \(\) space._

**Definition 5** (Total Variation).: _Define \(V_{T}_{t=2}^{T}\|_{t}-_{t-1}\|_{}\) where recall \(_{t}^{K}\) is the joint distribution on context and rewards at time \(t\)._

We have the following initial result (for two-armed bandits) to serve as baseline for this study.

   & Dynamic Regret Upper Bound \\  Ada-ILTCB  & \((L^{1/2} T^{})(V_{T}^{1/3} T^{ +})\) \\  MASTER with FALCON  & \((L^{1/2} T^{})(V_{T}^{1/3} T^{ +})\) \\  KeRNS  (non-adaptive) & \(V_{T}^{1/3} T^{+O(1/d)}\) \\ 
**Minimax Lower-Bound** & \((L^{}T^{})(V_{T}^{}T^{})\) \\  

Table 1: Existing dynamic regret upper-bounds are suboptimal in the Lipschitz setting.

**Theorem 1** (Dynamic Regret Lower Bound).: _Suppose there are \(K=2\) arms. For \(V,L[0,T]\), let \((V,L,T)\) be the family of joint distributions \(\{_{t}\}_{t[T]}\) with either total variation \(V_{T} V\) or at most \(L\) global shifts. Then, there exists a constant \(c>0\) such that for any policy \(\):_

\[_{(V,L,T)}_{}[R(,_{ T})] c(T^{}+T^{} V^{} )((L+1)^{}T^{}).\] (3)

**Remark 2**.: _Note setting \(d=0\) in Theorem 1 recovers the minimax rate \((+T^{2/3}V_{T}^{1/3})\) for non-contextual bandits (Besbes et al., 2019)._

Achievability of Minimax Lower-Bound (3).We are interested in whether the rates of (3) are achievable, with, or without knowledge of relevant parameters. First, we note that no existing algorithm currently guarantees a rate that matches (3). See Table 1 for a rate comparison (details for specializing to our setting found in Appendix A).

In particular, the prior adaptive works (Chen et al., 2019; Wei and Luo, 2021) both rely on the approach of randomly scheduling _replays_ of stationary algorithms to detect unknown non-stationarity. However, the scheduling rate is designed to safeguard against the parametric \( V_{T}^{1/3}T^{2/3}\) regret rates and thus lead to suboptimal dependence on \(L\) and \(V_{T}\).

However, a simple back of the envelope calculation indicates that the rate in (3) may be attainable, at least given some distributional knowledge: a minimax-optimal stationary procedure restarted at each shift will incur regret, over \(L\) equally spaced shifts, \((L+1)()^{}(L+1)^{} T^{}\).

As it turns out as we will show in the next section, (3) is indeed attainable, even adaptively; in fact, this is shown via a more optimistic problem parametrization as described next.

### A New Problem Parametrization: Experienced Significant Shifts.

As discussed in Subsection 2.3, typical approaches (Rigollet and Zeevi, 2010; Perchet and Rigollet, 2013) in our setting discretize the context space \(\) into bins, each of which is treated as an MAB instance. At a high level, our new measure of non-stationarity will trigger an **experienced significant shift** when the observed context \(X_{t}\) arrives in a bin \(B\) where there has been a severe change in local best arm, _w.r.t. the observed data in that bin_.

We first define a notion of **significant regret** for an arm \(a[K]\)_locally_ within a bin \(B\). We say arm \(a\) incurs **significant regret** in bin \(B\) on interval \(I\) if:

\[_{s I}_{s}(a)\{X_{s} B\} (I)}+r(B) n_{B}(I),\] ( \[\] )

where \(n_{B}(I)_{s I}\{X_{s} I\}\) and recall \(r(B)\) is the side length of bin \(B\). The intuition for (\(\)) is as follows: suppose that, over \(n\) separate rounds, we observe the same context \(X_{s}=x_{0}\) in bin \(B\). Then, arm \(a\) would be considered unsafe in the local bandit problem at context \(x_{0}\) if its regret exceeds \(\) (i.e., the first term on the above R.H.S.), which is a safe regret to pay for the non-contextual problem. Our broader notion (\(\)) extends this over the bin \(B\) by also accounting for the bias (i.e., the second term on the above R.H.S.) of observing \(X_{s}\) near a given context \(x_{0} B\).

**Remark 3** (Significant Regret Occurs at Critical Levels).: _If (\(\)) holds for some bin \(B\), then in fact it also roughly holds for the bin \(B^{}\), with \(B^{} B\), at the critical level \(r_{|I|}\) (see Notation 1) w.r.t. interval \(I\) (Lemma 9 and Proposition 15). Thus, it suffices to only check (\(\)) for the critical levels \(r_{|I|}\), which will be crucial in the analysis. Additionally, all such critical levels \(r_{|I|}\) are above the optimal level \(T^{-}\) for \(T\) stationary rounds (see Subsection 2.3). Thus, only \(O((T))\) levels play a role in checking (\(\)) for all intervals of time \(I\) and bins \(B\)._

We then propose to record an _experienced significant shift_ when we experience a context \(X_{t}\), for which there is no safe arm to play in the sense of (\(\)). The following notation will be useful.

**Notation.**_For the sake of succinctly identifying regions of time with experienced significant shifts, we will conflate the closed, open, and half-closed intervals of real numbers \([a,b]\), \((a,b)\), and \([a,b)\), respectively, with the corresponding rounds contained therein, i.e. \([a,b][a,b]\)._

**Definition 6**.: _Fix the context sequence \(X_{1},X_{2},,X_{T}.\)_

\(\) _We say an arm \(a[K]\) is_ **unsafe at context \(x\) on \(I\)** _if there exists a bin \(B\) containing \(x\) such that arm \(a\) incurs significant regret (\(\)) in bin \(B\) on \(I.\)_

_We then have the following recursive definition:_

\(\) _Let \(_{0}=1\). Define the \((i+1)\)-th_ **experienced significant shift** _as the earliest time \(_{i+1}(_{i},T]\) such that every arm \(a[K]\) is unsafe at \(X_{t}\) on some interval \(I[_{i},_{i+1}].\) We refer to intervals \([_{i},_{i+1}),i 0,\) as_ **experienced significant phases**_. The unknown number of such shifts (by time \(T\)) is denoted \(\), whereby \([_{L},_{L+1})\), for \(_{L+1} T+1,\) is the last phase._

**Remark 4** (Definition 6 Only Counts Most Essential Changes).: _It's clear from Definition 6 that only changes in mean rewards \(f^{a}_{t}(X_{t})\) at experienced contexts \(X_{t}\) are counted, and only counted when experienced (see example in Figure 1)._

An experienced significant shift \(_{i}\) in fact implies a best-arm change at \(X_{_{i}}\) since, by smoothness (Assumption 1) and (\(\)), we have

\[_{s I}^{a}_{s}(X_{_{i}})\{X_{s} B\} _{s I}_{s}(a)\{X_{s} B\}-r(B)_{s I}\{X_{s} B\}>0.\]

Thus, \( L+1,\) the global count of shifts, and can in fact be much smaller. On the other hand, so long as an experienced significant shift does not occur, there will be arms safe to play at each context \(X_{t}\). Thus, procedures need not restart exploration so long as unsafe arms can be quickly ruled out.

As a warmup to presenting our main regret bounds and algorithms, we'll first consider an oracle procedure which knows the experienced significant shift times \(_{i}\). The strategy will be to mimic a successive elimination algorithm, restarted at each experienced significant shift.

**Definition 7** (Oracle Procedure).: _For each round \(t\) in phase \([_{i},_{i+1})\), define a good arm set \(_{t}\) as the set of **safe** arms, i.e., arms which do not yet satisfy (\(\)) in the bin \(T_{r}(X_{t})\) at level \(r=r_{_{i+1}-_{i}}\) containing \(X_{t}\). Here, recall from Subsection 2.3 that \(r_{_{i+1}-_{i}}\) is the oracle choice of level over phase \([_{i},_{i+1})\). Then, define an_ **oracle procedure**__\(\): playing a uniformly random arm \(a_{t}\) at time \(t.\)_

We then claim such an oracle procedure attains an enhanced dynamic regret rate in terms of the significant shifts \(\{_{i}\}_{i}\) which recovers the minimax lower bound in terms of global shifts \(L\) and total variation \(V_{T}\) from before. The following proposition (see Appendix C for proof) captures this.

**Proposition 2** (Sanity Check).: _We have the oracle procedure \(\) of Definition 7 satisfies with probability at least \(1-1/T^{2}\) w.r.t. the randomness of \(_{T}\): for some \(C>0\)_

\[_{}[R_{T}(,_{T})_{T}] C(K) (T)_{i=0}^{L(_{T})}(_{i+1}(_{T})-_{i}( _{T}))^{} K^{}.\]

Figure 1: Shown are the gap functions \(^{1,2}_{t}()\) (\(K=2\) arms) in time \(t\) at two contexts \(x,x^{}\) assumed to be far apart. Suppose at times \(t_{1}\) and \(t_{3}\), we observe context \(x\), and at times \(t_{2}\) and \(t_{4}\), we observe \(x^{}\). Then, the change in best arm at context \(x\) is experienced. To contrast, the change in best arm at \(x^{}\) is _not_ experienced and hence not counted in Definition 6.

By Jensen's inequality the above regret rate is at most \(((_{T})+1)^{} T^{}(L+1)^{ } T^{}\). At the same time, the rate is also faster than \(V_{T}^{}T^{}\) (see Corollary 5). Thus, the oracle procedure above attains the dynamic regret lower bound of Theorem 1. We next aim to design an algorithm which can attain same order regret without knowledge of \(_{i}\) or \(\).

### Main Results: Adaptive Upper-bounds

Our main result is a dynamic regret upper bound of similar order to Proposition 2_without knowledge of the environment, e.g., the significant shift times, or the number of significant phases_. It is stated for our algorithm \(\) (Algorithm 1 of Section 4), which, for simplicity, requires knowledge of the time horizon \(T\) (knowledge of \(T\) removable using doubling tricks).

**Theorem 3**.: _Let \(\{_{i}(_{T})\}_{i=0}^{L+1}\) denote the unknown experienced significant shifts (Definition 6). We then have w.p. at least \(1-1/T^{2}\) w.r.t. the randomness of \(_{T}\), for some \(C>0\):_

\[[R_{T}(,_{T})_{T}] C( K)^{3}(T)_{i=1}^{L(_{T})}(_{i}(_{T})-_{i-1}( _{T}))^{} K^{}.\]

**Corollary 4** (Adapting to Experienced Significant Shifts).: _Under the conditions of Theorem 3, with probability at least \(1-1/T^{2}\) w.r.t. the randomness in \(_{T}\):_

\[[R_{T}(,_{T})_{T}] C( K)^{3}(T)(K((_{T})+1))^{} T^{ }.\]

Note, this is tighter than the earlier mentioned \((L+1)^{}T^{}\) rate. The next corollary asserts that Theorem 3 also recovers the optimal rate in terms of total-variation \(V_{T}\).

**Corollary 5** (Adapting to Total Variation).: _Under the conditions of Theorem 3:_

\[[R_{T}(,_{T})] C(K)^{3}(T)( T^{} K^{}+(V_{T} K)^{}  T^{}).\]

**Remark 5**.: _Our regret bound can straightforwardly be generalized to \(\{(+1)^{} T^{} K^{ },(V_{T} K)^{} T^{ {3+d}}+T^{} K^{}\}\) for \(\)-Holder reward functions with \( 1\), provided the notion (\(\)) is modified to take into account a bias of \(r^{}(B)\)._

## 4 Algorithm

We take a similar algorithmic approach to Suk and Kpotufe (2022), with several important modifications for our setting. The high-level strategy is to schedule multiple copies of a _base algorithm_ (Algorithm 2) at random times and durations, in order to ensure updated and reliable estimation of the gaps in (\(\)). This allows fast enough detection of unknown experienced significant shifts.

Overview of Algorithm Hierarchy.Our main algorithm \(\) (Algorithm 1) proceeds in episodes, each of which begins by playing according to an initially scheduled base algorithm of possible duration equal to the number of rounds left till \(T\). Base algorithms occasionally activate their own base algorithms of varying durations (Line 9 of Algorithm 2), called _replays_, according to a random schedule (set via variables \(\{Z_{m,s}\}\)). We refer to the currently playing base algorithm as the _active base algorithm_. This induces a hierarchy of base algorithms, from _parent_ to _child_ instances.

Choice of Level.Focusing on a single base algorithm now, each \(\) manages its own discretization of the context space \(=^{d}\), corresponding to a level \(r\) (see Definition 3). Within each bin \(B_{r}\) at the level \(r\), candidate arms, maintained in a set \((B)\), are evicted according to importance-weighted estimates (4) of local gaps.

As discussed in Subsection 2.3, key in algorithmic design is determining the optimal level \(r\). An immediate difficulty is that the oracle procedure's choice of level (see Definition 7) depends on the unknown significant phase length \(_{i+1}-_{i}\). To circumvent this, and as in previous works on Lipschitz contextual bandits (Perchet and Rigollet, 2013; Slivkins, 2014), we rely on an adaptive time-varying choice of level \(r_{t}\). Specifically, each base algorithm uses the level \(r_{t-t_{}}\) based on the time elapsed since the time \(t_{}\) it was first activated.

Managing Multiple Base Algorithms.Instances of Base-Alg and CMETA share information, in the form of _global variables_ as listed below:

* All variables defined in CMETA: \(t_{},t,\{_{}(B)\}_{B},\{Z_{m,t}\}\) (see Lines 3-6 of Algorithm 1).
* The choice of arm played at each round \(t\), along with observed rewards \(Y_{t}^{a}\), and the candidate arm set \(_{t}\) which takes value the set \((B)\) of the active Base-Alg at round \(t\) and bin \(B=T_{r}(X_{t})\) used.

By sharing these global variables, any Base-Alg can trigger a new episode: once an arm is evicted from a Base-Alg's \((B)\), it is also evicted from \(_{}(B)\), which is essentially the candidate arm set for the current episode. A new episode is triggered at time \(t\) when \(_{}(B)\) becomes empty for some bin \(B\) (necessarily a currently experienced bin), i.e., there is no _safe_ arm left to play at the context \(X_{t}\) in the sense of Definition 6. Note that \((B)\) are _local variables_ internal to each Base-Alg (the owner of which will be clear from context in usage).

To ensure consistent behavior while using a time-varying choice of level, we enforce further regularity in arm evictions across \(\): arms evicted from \((B^{})\) are also evicted from child bins \(B B^{}\) to ensure \((B)(B^{})\).

``` Input: horizon \(T\), set of arms \([K]\), tree \(\) with levels \(r\).
1Initialize: round count \(t 1\).
2Episode Initialization (setting global variables): \(t_{} t\). // \(t_{}\) indicates start of \(\)-th episode.
3 For each bin \(B\), set \(_{}(B)[K]\). // Initialize master candidate arm sets
4 For each \(m=2,4,,2^{(T)}\) and \(s=t_{}+1,,T\):  Sample and store \(Z_{m,s}(()^{} (})^{})\). // Set replay schedule.
5Run Base-Alg\((t_{},T+1-t_{})\).
6if\(t<T\)then restart from Line 2 (i.e. start a new episode). ; ```

**Algorithm 1**Contextual Meta-Elimination while Tracking Arms (CMETA)

``` Input: starting round \(t_{}\), scheduled duration \(m_{0}\).
1Initialize:\(t t_{}\) For each bin \(B\) at any level in \(\), set \((B)[K]\) while\(t t_{}+m_{0}\)do
2Choose level in \(\): \(r r_{t-t_{}}\).
3 Let \(_{t}(B)\) and let \(B T_{r}(X_{t})\).
4 Play a random arm \(a_{t}\) selected with probability \(1/|_{t}|\).
5 Increment \(t t+1\).
6if\( m\) such that \(Z_{m,t}>0\)then
7 Let \(m\{m\{2,4,,2^{(T)}\}:Z_{m,t}>0\}\). // Set maximum replay length.
8 Run Base-Alg\((t,m)\). // Replay interrupts.
9
10Evict bad arms in bin \(B\): \((B)(B)\{a[K]:\) \(\) rounds \([s_{1},s_{2}][t_{},t)\) s.t. (5) holds for bin \(T_{s_{2}-s_{1}}(X_{t})\}\).
11\(_{}(B)_{}(B) \{a[K]:\) \(\) rounds \([s_{1},s_{2}][t_{},t)\) s.t. (5) holds for bin \(T_{s_{2}-s_{1}}(X_{t})\}\).
12
13 Refine candidate arms: // Discard arms previously discarded in ancestor bins.
14\((B)_{B^{},B B^{} }(B^{})\).
15\(_{}(B)_{B^{},B  B^{}}_{}(B^{})\).
16
17Restart criterion: if\(_{}(B)=\) for some bin \(B\)then RETURN.;
18
19RETURN. ```

**Algorithm 2**Base-Alg\((t_{},m_{0})\): Adaptively Binned Elimination with randomized arm-pulls

Estimating Aggregate Local Gaps.\(_{s=s_{1}}^{s_{2}}_{s}(a^{},a)\{X_{s} B\}\) is estimated by \(_{s=s_{1}}^{s_{2}}_{s}^{B}(a^{},a)\), where \(_{s}(a^{},a)\{X_{s} B\}\) is estimated by importance weighting as:

\[_{s}^{B}(a^{},a)|_{t}|(Y_{s}^{a^ {}}\{_{s}=a^{}\}-Y_{s}^{a}\{_{s }=a\})\{a_{s}\}\{X_{s} B\}.\] (4)

Note that the above is an unbiased estimate of \(_{s}(a^{},a)\{X_{s} B\}\) whenever \(a^{}\) and \(a\) are both in \(_{s}\) at time \(s\), conditional on the context \(X_{s}\). It then follows that, conditional on \(_{T}\), the difference \(_{s=s_{1}}^{s_{2}}(_{s}^{B}(a^{},a) \{X_{s} B\}-_{s}(a^{},a))\) is a martingale that concentrates at a rate of order roughly \(([s_{1},s_{2}])}\), where recall from earlier that \(n_{B}(I)_{s I}\{X_{s} I\}\) is the context count in bin \(B\) over interval \(I\).

An arm \(a\) is then evicted at round \(t\) if, for some fixed \(C_{0}>0\)1, \(\) rounds \(s_{1}<s_{2} t\) such that at level \(r_{s_{2}-s_{1}}\) and letting \(B T_{s_{2}-s_{1}}(X_{t})\) (i.e., the bin at level \(r_{s_{2}-s_{1}}\) containing \(X_{t}\))

\[_{a^{}[K]}_{s=s_{1}}^{s_{2}}_{s}^{B}(a^{},a )> K(T)(n_{B}([s_{1},s_{2}]) K(T)) }+r_{s_{2}-s_{1}} n_{B}([s_{1},s_{2}]).\] (5)

## 5 Key Technical Highlights of Analysis

While a full analysis is deferred to Appendix D, we highlight key novelties and intuitive calculations.

\(\) Local Safety in Bins implies Safe Total Regret.We first argue that the notion of significant regret (\(\)) within a bin \(B\) captures the total allowable regret rates \(T^{}\) we wish to compete with over \(T\) "safe" rounds. If (\(\)) holds for no intervals \([s_{1},s_{2}]\) in all bins \(B\), arm \(a\) would be safe and incur little regret over any \([s_{1},s_{2}]\). As it turns out, bounding the per-bin regret by (\(\)) implies a total regret of \(T^{}\) as seen from the following rough calculation: via concentration and the strong density assumption (Assumption 2) to conflate \(n_{B}([1,T]) r(B)^{d} T\) and the fact that there are \( r^{-d}\) bins at level \(r\), we have:

\[_{B T_{r}}([1,T])}+r n_{B}([1,T]) K^{1/2}  T^{1/2} r^{-d/2}+T r.\] (6)

In particular taking \(r(K/T)^{}\) makes the above R.H.S. the desired rate \(K^{}T^{}\).

\(\) Significant Regret Threshold is Estimation Error.At the same time, the R.H.S. of (\(\)) is a standard variance and bias bound on the regression error of estimating the cumulative regret \(_{s=s_{1}}^{s_{2}}_{s}^{a}(x)\{X_{s} B\}\) at any context \(x B\), conditional on \(_{T}\) (see Lemma 6). Thus, intuitively, large gaps of magnitude above the threshold \((I)}+r(B) n_{B}(I)\) in (\(\)) are detectable via the estimates of (4).

Combining the above two points, we conclude that the notion of significant regret (\(\)) balances both (1) detection of unsafe arms and (2) regret of playing non-evicted arms. We next argue the randomized scheduling of multiple base algorithms is suitable for detecting experienced significant shifts.

\(\) A New Balanced Replay Scheduling.As mentioned earlier in Subsection 3.1, previous adaptive works on contextual bandits fail to attain the optimal regret in this setting due to an inappropriate frequency of scheduling re-exploration. We introduce a novel scheduling (Line 6 of Algorithm 1) of replays which carefully balances exploration and fast detection of significant regret in the sense of (\(\)). In particular, the determined probability \((1/m)^{}(1/t)^{}\) of scheduling a new \((t,m)\) comes from the following intuitive calculation: a single replay of duration \(m\) will, if scheduled, incur an additional regret of about \(m^{}\). Then, summing over all possible replays, the total extra regret incurred due to scheduled replays is roughly upper bounded by

\[_{t=1}^{T}_{m=2,4,,T}()^{} ()^{} m^{} _{t=1}^{T}T^{}(1/t)^{} T^{}.\]In other words, the cost of replays only incurs extra constants in the regret. Surprisingly, we find this scheduling rate is also sufficient for detecting significant regret in _any_ experienced subregion \(B\) of the context space \(\), i.e. there is no need to do additional exploration on a localized per-bin basis.

Key in this observation is the fact that, to detect significant regret over interval \(I\) in any bin \(B\), it suffices to check it at the _critical level_\(r_{I}(K/|I|)^{}\), where \(|I|\) is the length of \(I\). In particular, a well-timed \(\) running on the interval \(I\) will use this level \(r_{I}\) (Line 2 of Algorithm 2) and is, thus, equipped to detect significant regret at all experienced bins over \(I\).

\(\)**Suffices to Only Check (\(\)) at Critical Levels \(r_{I}\).** At first glance, detecting experienced significant shifts (Definition 6) appears difficult as an arm \(a\) may incur significant regret over a different bin \(B^{}\) from the bin \(B\) that is currently being used by the algorithm.

We in fact show that it suffices to only estimate the R.H.S. of (\(\)) in bins \(B^{}\) at the critical level \(r_{I}\) (Lemma 9 and Proposition 15). We give a rough argument for why this is the case: first, note that (\(\)) may be rewritten as

\[(I)}_{s I}_{s}(a)\{X_{s} B\} (I)}}+r(B).\] (7)

We next relate the two sides of the above display across different levels \(r(B)\).

* By concentration and the strong density assumption (Assumption 2), the R.H.S. of (7) is in fact of order \( 1/}+r(B)\), which is minimized at the critical level \(r(B) r_{I}\).
* The L.H.S. of (7) is an estimate of the average gap \(_{s I}_{s}^{a}(x)\) at any particular \(x B\) using nearby contexts. In fact, by concentration, the two can be conflated up to error terms of order the R.H.S. of (7).

Combining the above two points, we see that if (7) holds for some bin \(B\), then it will also hold for the "critical bin" \(B^{}\), with \(B^{} B\), at the critical level \(r_{I}(K/|I|)^{}\). In other words, significant regret at any experienced bin implies significant regret at the critical level, thus allowing us to constrain attention to these critical levels.

On the other hand, we observe that the calculations in (6) would hold if we only checked (\(\)) for bins \(B\) at level \(r_{I}\). Thus, it also suffices to only use the levels \(r_{I}\) for regret minimization over intervals \(I\) with no experienced significant shift.

Yet, even still, the analysis is challenging as there may be "missing data problems": arms \(a(B)\) in contention at \(B\) may have been evicted from sibling bins inside the parent \(B^{} B\) at the critical level. In other words, it is not a priori obvious how to do reliable estimation of arms \(a(B)\) across a larger bin \(B^{}\) which may contain sub-regions where \(a\) has already been evicted. We show it is in fact possible to identify a subclass of intervals of rounds (Definition 12) and an associated class of replays (Definition 13) which can quickly evict arm \(a\) in the critical bin \(B^{}\) before there are missing data problems for bin \(B B^{}\). The details of this can be found in Proposition 15 of Appendix D.2.

## 6 Conclusion

We have shown that it is possible to adapt optimally to an unknown number of experienced significant shifts - a new notion introduced here - which captures severe changes in best-arm, only at observed contexts. An interesting future direction is to explore other notions of experienced shifts which may yield even more optimistic rates. For example, suppose changes in best arm occur at every round, but are localized to a sub-region \(\) of the context space \(\). Then, a procedure which discretizes \(\) into bins at level \(T^{-}\) and runs local instantiations of a suitable non-stationary MAB algorithm (e.g., META of Suk and Kpotufe (2022)) can attain faster rates than those of Theorem 3 for some choices of \(\). At the same time, such a strategy cannot always attain the \((_{T})^{} T^{}\) rate in general as using the level \(T^{-}\) is insufficient to detect experienced significant shifts occurring at short intervals \(I\) of length \(|I| T\) (which require larger levels). This prompts the questions of whether there exists a unified notion of experienced shift which captures the most optimistic rates in these scenarios and whether such a notion can be adapted to.