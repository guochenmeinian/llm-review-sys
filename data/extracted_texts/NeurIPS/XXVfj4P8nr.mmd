# Training-Free Open-Ended Object Detection and Segmentation via Attention as Prompts

Zhiwei Lin Yongtao Wang1 Zhi Tang

Wangxuan Institute of Computer Technology, Peking University, China

{zwlin, wyt, tangzhi}@pku.edu.cn

###### Abstract

Existing perception models achieve great success by learning from large amounts of labeled data, but they still struggle with open-world scenarios. To alleviate this issue, researchers introduce open-set perception tasks to detect or segment unseen objects in the training set. However, these models require predefined object categories as inputs during inference, which are not available in real-world scenarios. Recently, researchers pose a new and more practical problem, _i.e._, open-ended object detection, which discovers unseen objects without any object categories as inputs. In this paper, we present VL-SAM, a training-free framework that combines the generalized object recognition model (_i.e.,_ Vision-Language Model) with the generalized object localization model (_i.e.,_ Segment-Anything Model), to address the open-ended object detection and segmentation task. Without additional training, we connect these two generalized models with attention maps as the prompts. Specifically, we design an attention map generation module by employing head aggregation and a regularized attention flow to aggregate and propagate attention maps across all heads and layers in VLM, yielding high-quality attention maps. Then, we iteratively sample positive and negative points from the attention maps with a prompt generation module and send the sampled points to SAM to segment corresponding objects. Experimental results on the long-tail instance segmentation dataset (LVIS) show that our method surpasses the previous open-ended method on the object detection task and can provide additional instance segmentation masks. Besides, VL-SAM achieves favorable performance on the corner case object detection dataset (CODA), demonstrating the effectiveness of VL-SAM in real-world applications. Moreover, VL-SAM exhibits good model generalization that can incorporate various VLMs and SAMs.

## 1 Introduction

Deep learning has achieved remarkable success in perception tasks, with autonomous driving as a typical practical application. Existing deep learning based perception models rely on extensive labeled training data to learn to recognize and locate objects. However, training data cannot cover all types of objects in real-world scenarios. When faced with out-of-distribution objects, existing perception models may fail to recognize and locate objects, which can lead to severe safety issues .

Many open-world perception methods [15; 48] are proposed to address this issue. Open-world perception tries to give precise results in dynamic and unpredictable environments, which contain novel objects and involve scene domain shifting. Current open-world perception methods can be roughly divided into two categories: _open-set_ and _open-ended_. Open-set methods [52; 43; 6] often calculate the similarity between image regions and category names with a pretrained CLIP  model. Thus, they require predefined object categories as inputs for the CLIP text encoder duringinference. However, in many real-world application scenarios, we do not have the exact predefined object categories. For instance, in autonomous driving, self-driving vehicles may meet unexpected objects, including various rare animals. Besides, some objects cannot be presented by a simple category name, such as a human in an animal costume, which may look like an animal but is actually a human. Some methods use generic obstacle detection to handle unknown objects. However, many things do not have a significant 3D shape, like pits or grains on the ground. Thus, open-set methods cannot handle all situations. In contrast, open-ended methods [26; 48] are more general and practical since they can predict the object categories and locations themselves.

In a separate line of research, large vision-language models (VLMs) [29; 23; 55] show a strong generalized ability to recognize objects, _e.g._, it can recognize rare objects for corner cases in autonomous driving scenarios . However, VLM's localization ability is less accurate than that of specific perception models , sometimes missing objects or giving wrong localization results. On the other hand, as a pure vision model, segment-anything model (SAM)  exhibits good generalized segmentation capabilities for images from many different domains. However, SAM is unable to provide categories for segmented objects  and may yield numerous irrelevant segmentation results.

In this paper, we propose to combine the existing generalized object recognition model, _i.e._, VLM, with the generalized object localization model, _i.e._, SAM, to address the open-ended object detection and segmentation task. We present VL-SAM, a training-free framework that connects two generalized models with attention maps as the intermediate prompts, as illustrated in Figure 1. Specifically, we utilize the attention maps generated by VLM when describing the whole driving scene to prompt the segmentation of SAM. Firstly, given the generated token of VLM, we use the token as the query to obtain the attention maps from all layers and heads of VLM. Then, in the attention map generation module, we introduce the head aggregation and attention flow mechanism to aggregate and propagate global attention maps through all heads and layers. Besides, to alleviate the collapse problem caused by causal masks when propagating with attention flow, we adopt a regularization term to constrain the attention flow propagation process. After that, to better guide SAM to segment with the attention maps, we present a prompt generation module by grouping and sampling positive and negative points as the point prompts for SAM. Furthermore, to reduce the number of missing objects, we further use the segmentation results from SAM to sample positive and negative points from attention maps iteratively until convergence.

The main contributions of this work are summarized as follows:

* We present VL-SAM, a training-free open-ended object detection and segmentation framework that connects the generalized object recognition model and the generalized object localization model with attention maps as the prompts.
* We introduce a head aggregation and regularized attention flow mechanism to aggregate and propagate attention maps with the causal masks through all heads and layers.
* We propose an iterative refinement pipeline with a positive and negative point sampling strategy for attention maps.

Figure 1: **Illustration of VL-SAM.** Without additional training, we connect the vision-language and segment-anything models with attention maps as the intermediate prompts.

* VL-SAM outperforms the _open-ended_ method GenerateU and obtains competitive results compared with existing _open-set_ methods on the long-tail instance segmentation dataset LVIS . In autonomous driving applications, VL-SAM achieves favorable corner case object detection performance on the CODA .

## 2 Related work

### Vision Language Model

Large language models (LLMs), including GPT-3 , GLM , and LLaMA , have shown human-like dialogue and reasoning skills. However, the limitation of LLM's ability to process and understand visual data restricts its application to more real scenarios. To overcome this, a cutting-edge Vision-Language Model (VLM) is introduced to open up new vistas for application. Recently, BLIP-2  proposes Q-Former to connect and fuse image and text embeddings with three alignment pretrain losses. LLaMA-Adapter [53; 12], LLaVA , and MiniGPT  introduce an adapter or projection layer to align the embedding space from image and text. CogVLM  presents visual expert modules to transform the image features to align with text features in different transformer heads. SPHINX  utilizes several mixing techniques for multiple visual tasks. Furthermore, CogAgent  and LLaVA-Phi  view VLM as an agent or assistant to complete various tasks.

Existing VLMs, especially GPT-4V , exhibit strong generalization capability for understanding and reasoning new or rare situations, _e.g.,_ it can deal with corner cases for autonomous driving . However, the localization ability of VLMs is weaker than specific perception models, like SAM.

In this paper, we equip VLM with generalized segmentation models, _i.e._, SAM, to address the localization limitation of VLM for open-ended object detection and segmentation. We achieve this by connecting two models with attention maps as the prompts without additional training.

### Open-World Object Detection and Segmentation

With the advent of the CLIP models , open-world classification, object detection, and instance segmentation have made great progress at the same time. Open-world methods try to discover and recognize unseen objects in the training set during inference. Current open-world methods can be roughly classified into two types: _open-set_ and _open-ended_. Open-set methods require redefined object categories, including seen objects and unseen objects in the training set, as inputs during inference. By contrast, open-ended methods can locate seen and unseen objects and generate their names simultaneously, as the current VLM does. In real-world applications, the exact categories may remain unknown for the perception models. For instance, in autonomous driving, self-driving vehicles often encounter unknown objects on the road, including overturned cars and construction vehicles with various shapes. Thus, the open-ended problem is more general and practical.

**Open-Set Methods.** With the powerful text-image embedding matching with CLIP, current open-set object detection methods mainly use a proposal network to obtain foreground object bounding boxes and embeddings, and then use CLIP as the open-set classification module to predict their categories. More recently, GLIP  proposes to use phrase grounding to pre-train open-world object detectors. GroundingDINO  presents cross-modality fusions to introduce text information to the image encoder for object grounding. SWORD  designs a novel contrastive method to learn the discrimination between foreground and background for instance segmentation. YOLO-World  introduces a prompt-then-detect paradigm for real-time open-world object detection. However, the above methods require predefined object categories as inputs for the text encoder.

**Open-Ended Methods.** GenerateU  first proposes the open-ended problem. Concurrently, DetCLIPv3  introduces a similar concept with open-ended. They present a generative framework with language models to generate object categories and bounding boxes at the same time. To achieve better generalization capabilities, they construct a large dataset with bounding box and caption pairs and finetune the whole network on the constructed dataset.

In contrast, we propose a training-free open-ended framework, VL-SAM, that combines generalized recognition and segmentation models. VL-SAM can generate object categories with the generalized recognized model and then localize objects with the generalized segmentation models.

## 3 Method

As shown 2, we provide an overview of our proposed framework. We use VLM and SAM as the generalized object recognition model and object localization model, respectively. Given an image input, we first use VLM to describe the scene and list all possible objects in the image. Then, for each object, we use the attention generation module with head aggregation and attention flow to obtain the high-quality attention map from VLM. Finally, we generate point prompts from the attention map and send them to SAM to get the location prediction iteratively.

### Preliminary

**Segment Anything Model.** SAM is a prompt-based segmentation model with excellent data generation capability. It consists of three components: an image encoder, a mask decoder, and a prompt encoder. SAM takes an image and a set of prompts, including points, a box, and a mask, as the inputs. To segment objects with the prompts, SAM first extracts image features with the image encoder. Concurrently, the set of prompts is sent to the prompt encoder to transform into the prompt tokens. Then, the image features, prompt tokens, and mask tokens interact in the mask decoder with the two-way transformers. Finally, the mask tokens are transformed into multi-scale segmentation masks by multiplying mask tokens with the image features following MaskDINO .

**Auto-Regressive Based Vision-Language Model.** Current Auto-Regressive based VLMs have yielded surprising performance in various vision-language tasks. The mainstream framework of current VLMs comprises four parts, _i.e._, an image encoder, a text tokenizer, projection layers, and a language decoder. Given an image and text as inputs, VLMs extract image tokens and text tokens with the image encoder and text tokenizer, respectively. Then, the image tokens are aligned with text tokens with projection layers. After that, the tokens from two modals are concatenated and sent to the language decoder to generate text outputs. The language decoder adopts the next-token prediction paradigm that the probability of the current generated token \(x_{t}\) depends on all previous tokens \((x_{1},x_{2},...,x_{t-1})\).

Figure 2: **An overview of VL-SAM framework.** We first use VLM to describe the input image and generate all possible objects’ names. Then, for each object name, we obtain the corresponding attention map with the attention map generation module. Finally, we sample point prompts from the attention map and send them to SAM to predict detection and segmentation results.

### Attention Map Generation

The main idea of VL-SAM is to use attention maps of objects as the prompts for SAM to segment. Thus, how to generate a high-quality attention map for an object is critical. To achieve this, we introduce attention flow to aggregate and propagate attention maps through all transformer heads and layers in VLM.

Specifically, given an image input, we ask VLM to give all possible objects in the image. During this process, we cache all queries and keys from VLM. Then, we multiply queries and keys with causal masks and SoftMax normalization to obtain similarity matrix \(S N N H L\), where \(N\) is the length of queries and keys, \(H\) is the number of transformer heads, and \(L\) denotes the number of transformer layers. \(S_{i,j}^{h,l}\) represents the similarity between query \(i\) and key \(j\) in the head \(h\), layer \(l\). After that, we aggregate information from all transformer heads with mean-max attention head weights, as shown in Figure 3. In particular, we choose the maximum similarity weights of matrix \(S\) in dimension \(j\) and average them in dimension \(i\) to obtain the attention head weights \(W 1 1 H L\):

\[W=((S,\;dim=1),\;dim=0).\] (1)

Obviously, the attention head weight indicates the importance of each head in each layer. Then, we pointwise multiply attention head weight \(W\) with similarity matrix \(S\) and average all heads as follows:

\[S^{}=(S W,\;dim=2).\] (2)

After aggregating all information from all heads, we present attention flow to further aggregate attention from all layers, as illustrated in Figure 4. Concretely, we use the attention rollout method 

Figure 4: **Attention flow.** We propagate attention from the first layer to last layer with attention flow.

Figure 5: **Illustration of attention collapse.** For each column, from left to right, we show image inputs, attention flow (collapse), regularized attention flow, and generated answers from VLM.

Figure 3: **Head aggregation.** We aggregate information from all attention heads with head weights.

to compute the attentions from layer \(l-1\) to layer \(l\) as follows:

\[}^{l}_{i,j}=_{k=1}^{N}(I_{i,k}+S^{ l}_{i,k})(I_{k, j}+}^{l-1}_{k,j}),\] (3)

where \(I\) is the identity matrix. After the attention rollout, we only need the attention map from the last layer. To obtain the image attention map of the generated token, we select the corresponding line and columns from \(}^{L}\).

However, since VLM uses causal masks for auto-regressive generation, simply adopting the attention rollout method causes attention collapse, as shown in Figure 5. Fortunately, we find a simple regularization term that can alleviate this problem efficiently. Specifically, for each column, assuming the unmasked length is \(L_{0}\), we multiply each value in this column with \(1-(L_{0}-1)/L\). With this regularization term, the attention value in the top left corner will be constrained.

### SAM Prompt Generation

The attention map generated in Section 3.2 has some unstable false positive peaks. To filter these false positive areas, we first use a threshold to filter weak activated areas and find the maximum connectivity area as the positive area . The remaining area serves as a negative area. After that, we sample a positive point from the positive area with the maximum activated value and a negative point from the negative area with the weakest activated value. The positive and negative points serve as the point prompt pair for SAM.

### Iterative Refinement

The segmentation results from the SAM decoder may include rough edges and background noises. We adopt two iterative strategies to further refine the segmentation results. In the first iterative strategy, we follow the cascaded post-refinement in PerSAM  to take the initial segmentation masks generated with the positive and negative pairs as the additional prompt input for the SAM decoder. For the second iterative strategy, we use the segmentation masks in the first iterative strategy to mask the attention map \(}\). Then, we iteratively generate positive and negative pairs with Prompt Generation in Section 3.3 from the masked attention map and send them to the SAM decoder. Finally, we aggregate the results with NMS .

### Multi-scale Ensemble

Due to the low-resolution image input of the image encoder in VLM, VLM may fail to recognize small objects. For instance, it may generate an answer: _'On the road, there are vehicles (a red truck and a blue bus), road signs, a pedestrian crossing, a white barrier, and a few other smaller objects that are not clearly identifiable from the image.'_. To alleviate this issue, we follow SPHINX  to split an image (\(H W\)) into four sub-images (\(H/2 W/2\)) from the four corners and send each sub-image to VL-SAM independently. Finally, we ensemble the output of VL-SAM for four sub-images and the whole image.

### Question-prompt Ensemble

The output of VLM is sensitive to the input prompt. To obtain a more comprehensive description of the input image, we ask VLM to generate ten question prompts for scene description with the sentence: _'If we want you to list all possible objects in the given image, what questions should we ask? Please give 10 questions you prefer.'_ Then, we use the generated question prompts for VL-SAM to segment objects and ensemble the outputs of all question prompts.

## 4 Experiments

### Implementation Details

We chose CogVLM-17B  with EVA2-CLIP-E  and Vicuna-7B-v1.5  as the vision-language model. CogVLM-17B divides an image with \(490 490\) into \(35 35\) patches. We set the temperature to 0.8 and top-p for nucleus sampling to 0.1 for CogVLM-17B. For the generated localization model, we use SAM with ViT-Huge .

We evaluate VL-SAM in a _training-free zero-shot_ manner for all datasets. To obtain object categories from the generated sentence of VLM, we follow Tag2Text  to parse tags from the given sentence. To evaluate the open-ended performance on datasets with predefined object category names, we follow GenerateU  to adopt CLIP  text encoder and map the generated object categories to predefined categories in datasets for evaluation. Specifically, we use the text prompt 'a {object category}' for CLIP text encoder to calculate the similarity between generated object categories and predefined categories for mapping. All models are inferred on an 80G A800 machine.

### Main Results

**LVIS Dataset.** We evaluate VL-SAM on the LVIS dataset , which has a long tail of categories and annotations for over 1000 object categories. Following previous works [26; 7], we mainly evaluate VL-SAM on LVIS minival and report the fixed AP  for rare objects.

As shown in Table 1, we list the performance for three types of perception methods, _i.e.,_ close-set, open-set , and open-ended. The different between open-set and open-ended is that open-set requires exact prior knowledge of object categories as inputs, while open-ended can generate them during inference in a zero-shot manner . In a real scenario, we often do not have predefined object categories for a scene. Thus, open-ended methods are more general and practical. As we can see, VL-SAM outperforms GenerateU by 3.4 \(AP_{rare}\). Notably, VL-SAM is a training-free framework and can simultaneously obtain boxes and segmentation masks. In contrast, GenerateU needs to fine-tune both the image encoder and language model on VG  and GRIT  datasets, requiring significant training costs, and can only predict bounding boxes. Besides, VL-SAM achieves competitive detection and segmentation performance compared to open-set detection methods and close-set segmentation methods, respectively.

**CODA Dataset.** To further demonstrate the effectiveness of the proposed method in the real-world application, we present the results of VL-SAM on corner case object detection dataset CODA for autonomous driving in Table 2. Specifically, as we can see, RPN only achieves 10.6 mAR, indicating that current open-set detectors relying on object proposals have difficulty dealing with corner cases. For more recent open-set detectors, they achieve higher mAR with CLIP as the object category predictor. For the open-ended method, LLaVA-Grounding ensembles VLM and grounding models into one model and achieves better performance than open-set methods. However, aggregating VLM and grounding models to one model requires joint training of two models, introducing additional training costs. By contrast, VL-SAM is a training-free framework and obtains significant performance improvement over LLaVA-Grounding from 18.4 mAR to 40.1 mAR.

In addition, we evaluate the performance upper bound of the current SAM. We utilize ground-truth boxes as the box prompt for SAM decoder to segment objects. We can observe that, in this setting, SAM achieves 54.1 mAR and 94.1 AR\({}_{50}\) since SAM has its limitations on segmentation tasks.

    &  &  &  \\  & & & box AP\({}_{rare}\) & mask AP\({}_{rare}\) \\  Mask R-CNN  &  & ✓ & 26.3 & 25.1 \\ Deformable DETR  & & ✓ & 24.2 & - \\  GLIP  &  & ✓ & 20.8 & - \\ GroundingDINO  & & ✓ & 27.4 & - \\ DetCLIP  & & ✓ & 26.9 & - \\ YOLOWorld  & & ✓ & 27.1 & - \\ OWLv2\({}^{*}\) & & ✓ & 39.0 & - \\  GenerateU  &  & ✓ & 20.0 & - \\ VL-SAM (Ours) & & \(\) & 23.4 & 22.7 \\   

Table 1: **Comparison of object detection and segmentation results on LVIS minival.** ‘Open-Ended’ denotes that we do not have exact object categories during inference . We report _fixed_ AP  for rare objects. * denotes using the external data.

It sometimes over- or under-segments an object and cannot obtain perfect segmentation results. Nevertheless, VL-SAM achieves 74.1% mAR performance of this upper bound, demonstrating the effectiveness of the proposed framework. Overall, VL-SAM achieves favorable performance on the CODA dataset.

### Ablation Study

**Main Components.** As shown in Table 3, we conduct ablation studies on CODA to analyze the effectiveness of each component of VL-SAM. For the baseline Naive Attention method, we use the attention map from the last layers and average all attention heads. We can see that the Naive Attention baseline obtains unsatisfactory results even with multi-scale and question ensemble techniques. With the proposed attention generation module, we improve the baseline by 7.9 mAR. Adding points pairs with prompt generation brings 2.2 mAR improvement. Besides, refining the segmentation maps with the iterative refinement module improves the detection performance from 12.3 mAR to 14.1 mAR. Furthermore, ensembling with multi-scale image input and question prompt obtains 13.2 mAR and 12.8 mAR, respectively. Though multi-scale and question prompt ensembles greatly improve performance, these two ensemble techniques do not show effectiveness without the proposed components. In summary, the results show the effectiveness of each component proposed in VL-SAM.

**Attention Generation.** To obtain high-quality attention maps from VLM, we introduce head weights to fuse transformer heads and a regularization term for attention flow. As shown in Table 4, simply using attention flow  almost fails to recognize objects for SAM due to the attention collapse caused by causal masks (Figure 5). With the regularization term, the attention flow mechanism shows its

    &  &  &  &  \\  & & & & mAR & AR\({}_{50}\) & AR\({}_{75}\) \\  RetinaNet\({}^{*}\) & & \(\) & ✓ & 12.8 & 23.2 & 11.9 \\ Faster R-CNN\({}^{*}\) & & \(\) & ✓ & 10.7 & 19.2 & 10.2 \\ Cascade R-CNN\({}^{*}\) & & \(\) & ✓ & 10.4 & 18.5 & 9.7 \\ Deformable DETR\({}^{*}\) &  & \(\) & ✓ & 9.0 & 22.2 & 5.6 \\ Sparse R-CNN\({}^{*}\) & & \(\) & ✓ & 10.1 & 19.6 & 9.0 \\ Cascade Swin\({}^{*}\) & & \(\) & ✓ & 9.9 & 17.2 & 9.7 \\ RPN\({}^{*}\) & & \(\) & ✓ & 10.6 & 20.0 & 10.2 \\  ORE\({}^{*}\) & & \(\) & ✓ & 8.3 & 16.4 & 7.4 \\ FsDet\({}^{}\) & & \(\) & ✓ & 4.2 & 7.7 & 4.0 \\ DeFRCN\({}^{}\) &  & \(\) & ✓ & 4.5 & 8.9 & 4.2 \\ GroundingDINO & & ✓ & ✓ & 12.6 & 21.7 & 13.3 \\ YOLOWorld & & ✓ & ✓ & 16.1 & 26.2 & 19.6 \\  LLaVA-Grounding  &  & ✓ & ✓ & 18.4 & 30.5 & 22.0 \\ VL-SAM (Ours) & & ✓ & \(\) & 40.1 & 90.1 & 50.5 \\  GT+SAM (Oracle) & – & – & – & 54.1 & 94.1 & 64.9 \\   

Table 2: **Comparison of object detection results on CODA. We chose the best performance for \({}^{*}\) results from CODA. \({}^{}\) denotes few-shot object detectors in the one-shot setting. ‘Oracle’ represents utilizing ground-truth boxes as the box prompt for SAM.**

   Naive Attn & Attn Generation & Prompt Generation & Iterative Refine & Multi-scale & Question ensemble & mAR \\  ✓ & & & & & 2.2 \\ ✓ & & & ✓ & ✓ & 5.0 \\   & ✓ & & & & 10.1 \\  & ✓ & ✓ & & & 12.3 \\  & ✓ & ✓ & ✓ & & 14.1 \\  & ✓ & ✓ & ✓ & & 27.3 \\  & ✓ & ✓ & ✓ & ✓ & 40.1 \\   

Table 3: **Ablation of main components. ‘Attn’ is short for ‘attention’. Each component improves the detection performance consistently.**superiority over naive attention by improving 6.3 mAR. Moreover, fusing with head weights leads to a 1.6 mAR improvement.

**Model Generalization.** To demonstrate the model generalization ability of the VL-SAM framework, we adapt two additional popular VLMs, MiniGPT-4  and LLaVA  to replace CogVLM and use MobileSAM  to replace SAM. In Table 5, we present the results of using these models in the VL-SAM framework. Empirical results show that replacing CogVLM with MiniGPT-4 or LLaVA may reduce the object localization performance in corner cases as CogVLM shows more powerful multimodal chat and reasoning ability than MiniGPT-4 and LLaVA. This indicates that our VL-SAM framework can benefit from more powerful VLMs. Besides, replacing SAM with a more lightweight but less accurate MobileSAM also leads to performance drops. Nevertheless, all these results outperform previous methods (18.4 mAR) in Table 2. This evidences that our framework can generalize to multiple vision-language and segmentation models.

## 5 Limitations

Since we combine VLM and SAM to address the open-ended object detection and segmentation task, VL-SAM inherits the defects of VLM and SAM. The first defect is the hallucination problem in VLM. VL-SAM also suffers from hallucinations, generating wrong object tokens and attention maps. The second defect is the low inference speed of VL-SAM. However, these defects can be fixed in the future. For example, there are many more efficient SAM variant models, including EfficientSAM  and MobileSAM . Our framework can benefit from these new models since we can easily replace CogVLM and SAM in VL-SAM with these more efficient and highly accurate models.

## 6 Conclusion

In this paper, we introduce VL-SAM, a framework that cascades VLM and SAM with the attention map to address the open-ended object detection and segmentation task. Without additional training, we adopt attention maps generated by VLM as the prompts for SAM to segment objects. We introduce the attention flow mechanism to aggregate high-quality attention maps. Besides, we present an iterative refinement pipeline with positive and negative points pair sampling strategy to acquire more accurate segmentation masks. Experimental results on the long-tail generic instance segmentation dataset LVIS show that VL-SAM beats the open-ended method GenerateU and achieves competitive performance compared with close-set and open-set methods. Moreover, VL-SAM achieves favorable results on the corner case object detection dataset CODA.

    &  &  &  \\  & No Regularization & Regularization & & 2.2 \\  ✓ & & & & 2.2 \\   & ✓ & & & 0.1 \\  & & ✓ & & 8.5 \\  & & ✓ & ✓ & 10.1 \\   

Table 4: **Ablation of attention generation.** We can obtain high-quality attention maps with the proposed modules.

   Vision-Language Model & Segmentation Model & mAR \\  CogVLM & SAM & 40.1 \\  MiniGPT-4 & SAM & 34.7 \\ LLaVA & SAM & 37.2 \\  CogVLM & MobileSAM & 29.2 \\   

Table 5: **Ablation of model generalization.** VL-SAM can adopt various vision-language models and segmentation models.

**Broader Impacts Statement.** This paper studies utilizing VLM and SAM for open-ended object detection and segmentation. We do not see potential privacy-related issues. This study may inspire future research on open-ended perception and potential corner case object detection applications in autonomous driving. However, the proposed model's performance is not yet up to the level of practical application and may pose safety threats when applied directly in practice.

**Acknowledgments.** This work was supported by National Key R&D Program of China (Grant No. 2022ZD0160305).