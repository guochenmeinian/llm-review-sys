# Cen Chen\({}^{1,4}\) Yi Wang\({}^{3}\), Lap-Pui Chau\({}^{3}\)

F-OAL: Forward-only Online Analytic Learning with Fast Training and Low Memory Footprint in Class Incremental Learning

 Huiping Zhuang\({}^{1}\), Yuchen Liu\({}^{2}\), Run He\({}^{1}\), Kai Tong\({}^{1}\), Ziqian Zeng\({}^{1}\),\({}^{1}\)South China University of Technology, China

\({}^{2}\)The University of Hong Kong, Hong Kong SAR

\({}^{3}\)The Hong Kong Polytechnic University, Hong Kong SAR

\({}^{4}\)Pazhou Lab, Guangzhou, China

{hpzhuang,runhe,kaitong,zqeng,cenchen}@scut.edu.cn,

liuyuchen@connect.hku.hk,

{yi-eie.wang, lap-pui.chau}@polyu.edu.hk

Equal contribution.Corresponding author.

###### Abstract

Online Class Incremental Learning (OCIL) aims to train models incrementally, where data arrive in mini-batches, and previous data are not accessible. A major challenge in OCIL is Catastrophic Forgetting, i.e., the loss of previously learned knowledge. Among existing baselines, replay-based methods show competitive results but requires extra memory for storing exemplars, while exemplar-free (i.e., data need not be stored for replay in production) methods are resource-friendly but often lack accuracy. In this paper, we propose an exemplar-free approach--Forward-only Online Analytic Learning (F-OAL). Unlike traditional methods, F-OAL does not rely on back-propagation and is forward-only, significantly reducing memory usage and computational time. Cooperating with a pre-trained frozen encoder with Feature Fusion, F-OAL only needs to update a linear classifier by recursive least square. This approach simultaneously achieves high accuracy and low resource consumption. Extensive experiments on benchmark datasets demonstrate F-OAL's robust performance in OCIL scenarios. Code is available at: https://github.com/liuyuchen-cz/F-OAL

## 1 Introduction

Class Incremental Learning (CIL) updates the model incrementally in a task-by-task manner with new classes in the new task. Traditional CIL most plans for static offline datasets which historical data are accessible. However, with the rapid increase of social media and mobile devices, massive amount of image data have been produced online in a streaming fashion, and render training models on static data less applicable. To address this, Online Class Incremental Learning (OCIL) is developed taking an online constraint in addition to the existing CIL setting. OCIL is a more challenging CIL setting in which data come in mini-batches, and the model is trained only in one epoch (i.e., learning from one pass data stream) . The model is required to achieve high accuracy, fast training time, and low resource consumption .

However, CIL techniques (including OCIL) suffer from Catastrophic Forgetting (CF) , also known as the erosion of previous knowledge when new data are introduced. The problem becomes more severe in online scenarios since the model can only see data once. Two major factors contribute to CF: (1) Using the loss function to update the whole network leads to uncompleted feature capturing and diminished global representation . (2) Using back-propagation to adjust linear classifier results in _recency bias_, which is a significantly imbalanced weight distribution, showing preference only on current learning data .

To address CF in an online setting, replay-based methods [9; 21] are the mainstream solution by preserving old exemplars and revisiting them in new tasks. This strategy has strong performance but is resource consuming, while exemplar-free methods [17; 20] have lower resource consumption but show less competitive results.

Recently, Analytic Continual Learning (ACL)  methods emerged as an exemplar-free branch, delivering encouraging outcomes. ACL methods pinpoint the iterative back-propagation as the main factor behind catastrophic forgetting and seek to address it through linear recursive strategies. Remarkably, for the first time, these methods achieve outcomes comparable to those utilizing replay-based techniques.

There are two limitations in existing ACL methods: (1) Multiple iterations of base training are needed when the model is applied. Subsequently, the acquired knowledge is encoded into a _regularized feature autocorrelation matrix_ by analytic re-alignment. The incremental learning phase then unfolds, utilizing the recursive least squares method for updates. This pattern is repeated when the dataset is switched, significantly elevating the temporal cost in an online scenario. (2) Classic ACL methods demand data aggregation from a single task, facilitating analytic learning in one fell swoop. This process increases GPU memory footprint and is unsuitable for online contexts where data for each task is presented as mini-batches.

To address those limitations, we propose Forward-only Online Analytic Learning (F-OAL) that learns online batch-wise data streams. The F-OAL consists of a frozen pre-trained encoder and an Analytic Classifier (AC). The frozen encoder is capable of countering the uncompleted feature representation caused by using the loss function to update and replace the time-consuming base training. With Feature Fusion and Smooth Projection, the encoder provide informative representation for analytic learning. The AC is updated by recursive least square rather than back-propagation to solve recency bias and decrease calculation. Therefore, F-OAL is an exemplar-free countermeasure to CF and reduces resource consumption since the encoder is frozen and only the AC is updated.

Our main contributions can be concluded as follows:

\(\) We present the F-OAL, an exemplar-free technique that achieves high accuracy and low resource consumption together for the OCIL.

\(\) F-OAL redefines the OCIL problem into a recursive least square manner and is updated in a mini-batch manner.

\(\) F-OAL introduces a framework of frozen pre-trained encoder with Feature Fusion to generate representative features and Smooth Projection for recursively updating AC to counter CF.

\(\) We conduct massive experiments on benchmark datasets with other OCIL baselines. The results demonstrate that F-OAL achieves competitive results with fasting training speed and low GPU footprint.

## 2 Related Works

**Online Class Incremental Learning** focuses on extracting knowledge from one pass data stream with new classes in the new task. Time and memory consumption requirements are particularly critical in OCIL, given the fast and large nature of online data streams .

**Replay-based methods**[33; 25; 21; 9; 31; 12; 2; 11; 15; 41; 37; 36; 1; 38; 19; 6; 39; 35; 9] are mainstream solutions for OCIL problems by preserving historical exemplars and using them in new tasks. The accuracy is better than exemplar-free methods', but the training time and memory consumption are higher.

**Analytic Learning** (AL), also referred to as pseudo-inverse learning , emerges as a solution to the pitfalls of back-propagation by recursive least square. Analytic Learning's computational intensity is demanding since the entire dataset is processed. The obstacle is solved by the block-wise recursive Moore-Penrose algorithm , which achieves equivalent precision with joint-learning (i.e., training with all data). AL has already been used in online reinforcement learning , which shows the potential of AL in OCIL.

**Exemplar-free methods** can be categorized into regularization-based, prototype-based, and recently emerged ACL methods. **Regularization-based methods**[20; 17; 42; 8; 44] apply constraints on the loss function or change the gradients to preserve knowledge. These solutions reduce resource consumption but commonly can not outperform replay-based methods. **Prototype-based methods**[43; 45; 29; 40; 32; 7; 28] mitigate CF by augmenting the prototypes of features to classify old classes or generating psudo-features of old classes from new representations. **ACL methods** represent a new branch of the CIL community and show great potential in the OCIL scenario. Analytic Class Incremental Learning (ACIL)  is the first approach that applies AL to the CIL problem. The ACIL achieves a competitive performance in the offline CIL scenario. Gaussian Kernel Embedded Analytic Learning (GKEAL) , following the ACIL, focuses on solving CF in the few-shot scenario by adopting the kernel method. DS-AL  overcomes the under-fitting problem of AL.

## 3 Methodology

### Proposed Framework

Our F-OAL framework consists of two modules:

**Encoder**. Encoder has two parts: backbone with Feature Fusion and Smooth Projection. The performance of analytic learning is highly dependent on the quality of the extracted features. Therefore, we employ Vision Transformer (ViT)  as the backbone instead of CNN, because ViT generates more comprehensive feature representation . To further enhance the representativeness of the features, we utilize Feature Fusion. Specifically, we extract the outputs from each block of the ViT, sum them, and take their average to form the image feature.

Subsequently, we expand this feature into a higher-dimensional space using random projection and apply the sigmoid function for smoothing (i.e., Smooth Projection). This results in the final activation

Figure 1: This diagram illustrates the learning agenda of F-OAL. In the encoder, features from each block of the ViT are extracted, summed, and averaged to form a composite feature. This feature is then randomly projected into a higher-dimensional space and normalized using the sigmoid function, serving as the activation for updating the classifier. All parameters in the encoder remain frozen. In the analytic classifier section, we introduce \(R\) to retain historical information and update the linear classifier using recursive least squares. This process is forward-only with no gradients.

used to update the classifier. Therefore, the encoder \(()\) is defined as:

\[(x)=(x)+B_{1}(B_{2}(x))+ +B_{n}(B_{n-1}( B_{1}(x)))}^{}))}_{},\] (1)

where \(B_{1}() B_{n}()\) are blocks in ViT, \(LP()\) is linear projection and \(()\) is sigmoid activation function.

Backbone is pre-trained and frozen and weight matrix of projection is sampled from normal distribution and is frozen. Freezing the encoder avoids selective learning caused by loss function, which prioritizes features that are easiest to learn rather than the most representative  and significantly reduces the number of trainable parameters.

**Analytic Classifier**. Unlike back-propagation, we employ the least squares method to obtain the analytic solution for the linear mapping from the activation to the one-hot label. We then recursively update the weight matrix of the linear mapping. This approach is forward-only and does not require the use of gradients, resulting in low memory usage and fast computation speed.

### Analytic Solution

The overview is shown in Figure 1. To derive our solution, the notations needed are listed in Table 1. Unlike other methods that treat model training as a back-propagation process, our method formulates the problem using linear regression, which allows for direct computation of the optimal parameters in a closed-form solution:

\[=(),\] (2)

where \(()\) is the pre-trained and forzen encoder and \(\) is the learnable parameter of linear classifier. The learning problem can be rewritten into an optimization form:

\[*{argmin}_{}\ \|.-() \|_{}^{2}+\|\|_{}^{2},\] (3)

where \(\|\|_{}\) represents the Frobenius norm and \(\) is the regularization term. The optimal solution is defined as:

\[}=(()^{}()+)^{- 1}()^{}.\] (4)

  
**Name** & **Description** & **Dimension** \\  \((X)\) & Activation of all images. & \(V D\) \\ \(Y\) & One-hot label of all images. & \(V M\) \\ \(\) & Joint-learning result of classifier’s weight matrix. & \(M D\) \\ \(X_{k,i}^{(a)}\) & Activation matrix of the i-th batch of the k-th task. & \(S D\) \\ \(Y_{k,i}^{}\) & One-hot label matrix of the i-th batch of the k-th task. & \(S C_{s}\) \\ \(X_{k,1:i}^{(a)}\) & Activation matrix from the start to the i-th batch of the k-th task. & \(V_{s} D\) \\ \(Y_{k,i,1}^{(a)}\) & One-hot label matrix from the start.to the i-th batch of the k-th task. & \(V_{s} C_{s}\) \\ \(^{(k,i)}\) & Classifier of the i-th batch of the k-th task. & \(C_{s} D\) \\ \(R_{k,i}\) & Regularized feature autocorrelation matrix to the n-th batch of the k-th task. & \(D D\) \\   

Table 1: Description of notations and their dimensions. Here, \(V\) is the number of all images, \(D\) is the encoder output dimension, \(M\) is the number of all classes, \(S\) is the batch size, \(C_{s}\) is the number of classes seen so far, and \(V_{s}\) is the number of images seen so far.

### Learning Agenda

Given dataset \(\{_{k}^{},_{k}^{}\}_{1}^{m}\) be the training set of task \(k\) (\(k\) = 1, 2,..., \(m\)). Every task is divided into \(n\) mini-batches. We denote \(\{_{k,i}^{},\ _{k,i}^{}\}\), as the \(i\) mini-batch (\(i\)=1, 2,..., \(n\)) of training set of task \(k\). At task 1 batch \(k\), the model aims to optimize

\[*{argmin}_{^{(1,i)}}\ \|_{1,1:i}^{}-(_{1,1:i}^{(a)})^{(1,i)}\|_{}^{2}+\|^{(1, i)}\|_{}^{2},\] (5)

where

\[_{1,1:i}^{(a)}=(_{1,1:i}^{}).\] (6)

The optimal solution to parameter \(^{(1,i)}\) is given as:

\[}^{(1,i)}=((_{1,1:i}^{(a)})^{}_{1,1:i}^{( a)}+)^{-1}(_{1,1:i}^{(a)})^{}_{1,1:i}^{ }.\] (7)

Regarding \(_{1,1:i}^{(a)}\) and \(_{1,1:i}^{}\) as stacks of row activation vectors and one-hot label vectors respectively, we observe that \((_{1,1:i}^{(a)})^{}(_{1,1:i}^{(a)})\) and \((_{1,1:i}^{(a)})^{}_{1,1:i}^{}\) are both sums of outer products w.r.t feature vectors. Notice that the solution contains no data correlation. Thus we can further derive Equation 7 into

\[}^{1,i} =([_{1,1:i-1}^{(a)})^{}(_{1,i}^{(a)})^{}][_{1,1:i-1}^{(a)}]+)^{-1}[(_{1,1:i-1}^{(a)})^{}(_{1,i}^{(a)})^{ }][_{1,1:i-1}^{} _{1,i}^{}]\] (8) \[=((_{1,1:i-1}^{(a)})^{}(_{1,1:i-1}^{(a) })+(_{1,i}^{(a)})^{}(_{1,i}^{(a)})+)^{-1 }[(_{1,1:i-1}^{(a)})^{}_{1,1:i-1}^{}+( _{1,i}^{(a)})^{}_{1,i}^{}].\]

Let

\[_{1,i-1}=[(_{1,1:i-1}^{(a)})^{}(_{1,1:i-1}^{(a )})+]^{-1}.\] (9)

be the _regularized feature autocorrelation matrix_ at batch \(i\)-1 of task 1, where both historical and current information is encoded in.

Therefore, we can calculate both \(\) and \(\) in a recursive manner by the following theorems when the serial numbers of tasks and batches are given:

**Theorem 3.1** For the batch 1 of task \(k\), Let \(}^{(0)}\) be the null matrix. Let \(}^{(k-1,n)^{}}=[}^{(k-1,n)}]\) and \(}^{(k,1)}\) can be calculated via:

\[}^{(k,1)}=}^{(k-1,n)^{}}+_{k,1}_{k,1}^{ }(_{k,1}^{}-_{k,1}^{(a)}}^{ (k-1,n)^{}}),\] (10)

where

\[_{k,1}=_{k-1,n}-_{k-1,n}_{k,1}^{}(+_{k,1}^{}_{k-1,n}_{k,1}^{})^{-1}_{k,1}^ {}_{k-1,n}.\] (11)

**Theorem 3.2** For the batch \(i\) ( \(i\) > 1 ) of task \(k\), \(}^{(k,i)}\) can be calculated via:

\[}^{(k,i)}=}^{(k,i-1)}+_{k,i}_{k,i}^{}(_{k,i}^{}-_{k,i}^{(a)}}^{(k,i-1)} ),\] (12)

where

\[_{k,i}=_{k,i-1}-_{k,i-1}_{k,i}^{}(+_{k,i}^{}_{k,i-1}_{k,i}^{})^{-1}_{k,i}^ {}_{k,i-1}.\] (13)

Proof.: See Appendix A. 

Thus, we achieve absolute memorization in an exemplar-free way with all data used only once. The learning agenda of F-OAL is summarised in Algorithm 1.

### Complexity Analysis

In terms of space complexity, our trainable parameters are only \(R\) and \(W\). The \(R\) matrix is of size \(D D\), where \(D\) is the output dimension of the encoder. In our paper, the encoder output dimension is 1000. Therefore, according to Equation 9, the size of the \(R\) matrix is \(1,000 1,000\). The \(W\) matrix has dimensions of \(C D\), where \(C\) is the number of classes in the target dataset. For example, with CIFAR-100, its size is \(100 1,000\). The total number of trainable parameters is relatively small and does not require gradients. This results in our method using less than 2GB of GPU memory.

In terms of computational complexity, we denote the batch size as \(S\), encoder's output size as \(D\), and class number as \(C\). Therefore, the dimensions of \(X\), \(Y\), \(R\) and \(W\) are \(S D\), \(S C\), \(D D\), and \(C D\), respectively. Thus, the calculation is shown below:

The computational complexity for updating \(R\) is dominated by the matrix multiplications, thus:

\[\{(SDC),(SC),(SDC)\}\{ (SDC),(D^{2}C)\}.\] (14)

The computational complexity for updating \(W\) is dominated by the matrix multiplications and the matrix inversion:

\[\{(SD),(SD^{2}),(S^{2}),(S^{ 3}),(DS^{2}),(D^{2}S),(D^{2})\}\{ (S^{3}),(D^{2}S)\}.\] (15)

In the OCIL setting, the batch size is relatively smaller. Therefore, the overall computational complexity is primarily controlled by \(D\).

### Overhead Analysis

In terms of space overhead, compared to the conventional backbone + classifier structure, F-OAL introduces an additional linear projection to control the output dimension \(D\) of the encoder, and a matrix \(R\), where only \(R\) is trainable. According to Equation 9, the dimension of \(R\) remains a fixed size of \(D D\). Other methods require more extra space. For instance, LwF  employs knowledge distillation, necessitating the storage of additional models, while replay-based methods require extra storage to retain historical samples. In contrast, the overhead introduced by F-OAL, consisting of an additional matrix and a linear layer, is smaller.

In terms of time overhead, our method primarily consists of a forward pass and matrix multiplication, which is determined by the output dimension of the encoder. By changing the output dimension of the encoder, we can balance the accuracy and time. According to , the backward pass in back-propagation (forward pass + backward pass) accounts for 70% of the time. Therefore, our method's time overhead is also relatively small.

## 4 Experiment

To show the effectiveness of F-OAL, we conduct extensive experiments to compare our approach with baseline methods. We build our code and reproduce relevant results based on . All baselines are with pre-trained ViT as the backbone.

### Datasets

We focus on coarse-grained data scenarios, such as CIFAR-100, Tiny-ImageNet, and Core50, as well as fine-grained data scenarios, including DTD, FGVCAircraft, and Country211. All of these are open-source benchmark datasets. However, there are other data scenarios, such as long-tail distributions, where unbalanced data distribution will make it harder to achieve good performance by only training the classifier. We will study this case in future work.

\(\)**CIFAR-100** is constructed into 10 tasks with disjoint classes, and each task has 10 classes. Each task has 5,000 images for training and 1,000 for testing.

  
**Metric** & **Method** & **Replay?** & **CIFAR-100** & **COR50** & **FGVCAircraft** & **DTD** & **Tiny-ImageNet** & **Country211** \\    & iCaRL/CVPR 2017 & ✓ & 91.6 & 95.6 & 36.4 & 74.1 & 91.3 & 12.2 \\  & ER(ICRA 2019) & ✓ & 90.1 & 94.8 & 35.7 & 65.4 & 87.3 & 14.0 \\  & ASER(AAAI 2021) & ✓ & 87.2 & 87.1 & 25.2 & 57.4 & 85.8 & 13.2 \\  & SCR(VPR 2021) & ✓ & 91.9 & 95.3 & 55.6 & 75.0 & 82.6 & 14.7 \\  & DVC(VPR 2022) & ✓ & 92.4 & 97.1 & 33.7 & 67.3 & 91.5 & 16.1 \\ \(A_{avg}(\%)\)\(\) & PCR(CVPR 2023) & ✓ & 89.1 & 95.7 & 10.1 & 35.0 & 91.0 & 9.7 \\  & LwF(PNAI 2018) & ✗ & 69.3 & 47.0 & 14.2 & 40.2 & 82.5 & 1.4 \\  & EWU(NAS 2017) & ✗ & 49.9 & 47.9 & 12.0 & 27.6 & 60.5 & 6.1 \\  & EASE(CVPR 2024) & ✗ & 91.1 & 85.0 & 38.2 & 76.0 & **92.0** & 15.9 \\  & LAE(ICCV 2023) & ✗ & 79.1 & 73.3 & 13.5 & 63.5 & 86.7 & 14.5 \\  & SLAC(ICCV 2023) & ✗ & 90.4 & 93.7 & 34.3 & 70.9 & 88.6 & 17.8 \\  & **F-OAL** & ✗ & **91.1** & **96.3** & **62.2** & **82.8** & 91.2 & **24.4** \\    & iCaRL(CVPR 2017) & ✓ & 87.5 & 93.2 & 29.8 & 66.3 & 87.8 & 6.5 \\  & ER(ICRA 2019) & ✓ & 84.6 & 92.1 & 28.6 & 54.3 & 81.6 & 6.8 \\  & ASER(AAAI 2021) & ✓ & 82.0 & 82.1 & 14.8 & 49.4 & 80.0 & 6.7 \\  & SCR(VPR 2021) & ✓ & 87.7 & 93.6 & 50.3 & 68.7 & 75.8 & 8.0 \\  & DVC(VPR 2022) & ✓ & 87.8 & 96.0 & 27.0 & 57.2 & 87.2 & 9.2 \\ \(A_{last}(\%)\)\(\) & PCR(CVPR 2023) & ✓ & 81.4 & 93.9 & 9.0 & 34.6 & 86.1 & 6.1 \\  & LwF(PNAI 2018) & ✗ & 64.8 & 26.3 & 5.8 & 18.3 & 72.5 & 0.5 \\  & EWU(NAS 2017) & ✗ & 25.2 & 21.1 & 3.0 & 13.3 & 44.6 & 1.9 \\  & EASE(ICCVPR 2024) & ✗ & 85.4 & 78.3 & 29.3 & 67.6 & **89.3** & 10.5 \\  & LAE(ICCV 2023) & ✗ & 75.6 & 67.1 & 6.3 & 53.6 & 82.4 & 9.3 \\  & SLAC(ICCV 2023) & ✗ & 85.6 & 88.2 & 32.1 & 63.3 & 85.4 & 12.9 \\  & F-OAL & ✗ & **86.5** & **92.5** & **54.0** & **75.9** & 87.3 & **17.5** \\    & iCaRL(CVPR 2017) & ✓ & 3.2 & 2.3 & 7.1 & 7.8 & 2.7 & 6.7 \\  & ER(ICRA 2019) & ✓ & 20.7 & 4.3 & 34.0 & 29.8 & 13.3 & 21.0 \\  & ASER(AAAI 2021) & ✓ & 16.5 & 9.9 & 35.7 & 29.3 & 16.3 & 19.4 \\  & SCU(VPR 2021) & ✓ & 6.2 & 3.8 & 14.5 & 11.6 & 7.7 & 6.4 \\  & DVC(CVPR 2022) & ✓ & 8.2 & 2.3 & 29.7 & 21.7 & 8.9 & 18.9 \\ \(F(\%)\)\(\) & PCR(VPR 2023) & ✓ & 9.2 & 4.2 & 2.7 & 1.4 & 8.0 & 1.7 \\  & LwF(PNAI 2018) & ✗ & **1.3** & **9.4** & **3.1** & **4.5** & **1.0** & **9** \\  & EWC(PNAS 2017) & ✗ & 67.4 & 81.0 & 38.8 & 68.3 & 20.7 & 51.5 \\  & EASE(CVPR 2024) & ✗ & 6.1 & 10.7 & 19.2 & 12.5 & 2.8 & 16.8 \\  & LAE(ICCV 2023) & ✗ & 11.8 & 13.8 & 12.2 & 25.0 & 5.4 & 16.7 \\  & SLAC(ICCV 2023) & ✗ & 7.1 & 3.4 & 10.2 & 12.7 & 4.2 & 14.9 \\  & F-OAL & ✗ & **5.5** & 3.9 & 10.0 & 10.1 & 5.0 & 6.9 \\    & iCaRL(CVPR 2017) & ✓ & 3.2 & 2.3 & 7.1 & 7.8 & 2.7 & 6.7 \\  & ER(ICRA 2019) & ✓ & 20.7 & 4.3 & 34.0 & 29.8 & 13.3 & 21.0 \\  & ASER(AAAI 2021) & ✓ & 16.5 & 9.9 & 35.7 & 29.3 & 16.3 & 19.4 \\  & SCU(VPR 2021) & ✓ & 6.2 & 3.8 & 14.5 & 11.6 & 7.7 & 6.4 \\  & DVC(CVPR 2022) & ✓ & 8.2 & 2.3 & 29.7 & 21.7 & 8.9 & 18.9 \\ \(F(\%)\)\(\) & PCR(VPR 2023) & ✓ & 9.2 & 4.2 & 2.7 & 1.4 & 8.0 & 1.7 \\  & LwF(PNAI 2018) & ✗ & **1.3** & **9.4** & **3.1** & **4.5** & **1.0** & **9** \\  & EWC(PNAS 2017) & ✗ & 67.4 & 81.0 & 38.8 & 68.3 & 20.7 & 51.5 \\  & EASE(CVPR 2024) & ✗ & 6.1 & 10.7 & 19.2 & 12.5 & 2.8 & 16.8 \\  & LAE(ICCV 2023) & ✗ & 11.8 & 13.8 & 12.2 & 25.0 & 5.4 & 16.7 \\  & SLAC(ICCV 2023) & ✗ & 7.1 & 3.4 & 10.2 & 12.7 & 4.2 & 14.9 \\  & F-OAL & ✗ & **5.5** & 3.9 & 10.0 & 10.1 & 5.0 & 6.9 \\    &  & & & & & & & \\ 

Table 2: This table shows the comparison results of our method with other baselines on six datasets. We select three metrics: average accuracy (\(A_{avg}\)), last task accuracy (\(A_{last}\)), and forgetting rate (\(F\)). Higher values for \(A_{avg}\) and \(A_{last}\) indicate better performance, while lower values for \(F\\(\)**CORe50** is a benchmark designed for class incremental learning with 9 tasks and 50 classes: 10 classes in the first task and 5 classes in the subsequent 8 tasks. Each class has 2,398 training images and 900 testing images on average.

\(\)**FGVCAircraft** contains 102 different classes of _aircraft models_. 100 classes are selected and divided into 10 tasks. Each class has 33 training images and 33 testing images on average.

\(\)**DTD** is a _texture database_, organized into 47 classes. 40 classes are selected and divided into 10 tasks. Each class has 40 training images and 40 testing images.

\(\)**Tiny-ImageNet** is a subset of ImageNet with 200 classes for training. Each class has 500 images. The test set contains 10,000 images. The dataset is evenly divided into 10 tasks.

\(\)**County211** contains 211 classes of country images, with 150 train and test images per class. The dataset is evenly divided into 10 tasks with 210 classes chosen.

### Evaluation Metrics

We define \(a_{i,j}\) as the accuracy evaluated on the test set of task \(j\) after training the network from task 1 through to \(i\), and the average accuracy is defined as

\[A_{i}=_{j=1}^{i}a_{i,j}.\] (16)

When \(i=m\) (i.e., the total number of tasks), \(A_{m}\) represents the average accuracy by the end of training.

Forgetting rate at task \(i\) is defined as Equation 17. \(f_{i,j}\) represents how much the model has forgot about task \(j\) after being trained on task \(i\). Specifically, \(_{l\{1,,k-1\}}(a_{l,j})\) denotes the best test accuracy the model has ever achieved on task \(j\) before learning task \(k\), and \(a_{k,j}\) is the test accuracy on task \(j\) after learning task \(k\).

\[F_{i}=_{j=1}^{i-1}f_{i,j},\] (17)

where

\[f_{k,j}=_{l\{1,,k-1\}}(a_{l,j})-a_{k,j}, j<k.\] (18)

### Implementation Details

ViT-B , pre-trained on ImageNet-1K , serves as the backbone network for all methods. The data stream is kept identical across all experiments to ensure a fair comparison. The learning rate and batch size are set to 0.001 and 10, respectively. The optimizer is SGD. We assign 5,000 memory buffer sizes for replay-based methods. In F-OAL, the expansion size is 1,000 (i.e., the output activation size to update AC is 1,000). The regularization term is set to be 1. All experiments are conducted on a single RTX 4070ti GPU 12GB, and an average of 3 runs is reported.

### Result Comparison

We tabulate the average accuracy (\(A_{avg}\)), last task accuracy (\(A_{last}\)), and the forgetting rate (\(F\)) from the compared methods in Table 2.

For fine-grained datasets such as FGCVAircraft, DTD, and Country211, F-OAL achieves the best performance, with average accuracies of 66.2%, 82.8%, and 24.4%, respectively. The second-best results are 55.6%, 76.0%, and 17.8%, respectively. Similarly, F-OAL also outperforms in last task accuracy. This demonstrates the excellent transferability of our method in the OCIL setting, effectively leveraging the feature extraction capabilities of the pre-trained encoder.

On coarse-grained datasets such as CIFAR-100, CORe50, and Tiny-ImageNet, F-OAL still demonstrates excellent performance, achieving the highest accuracy among all exemplar-free methods, except on Tiny-ImageNet where it is 0.8% behind EASE in average accuracy. Compared to the best replay-based methods, it only lags by 1.3%, 0.8%, and 0.3%, respectively.

Typically, a lower forgetting rate is better, but forgetting is based on accuracy. Therefore, when comparing forgetting rates, it is important to consider models with similar accuracy levels. When comparing F-OAL to the well-performing DVC, F-OAL exhibits a lower forgetting rate. On CIFAR-100, F-OAL's forgetting rate is 5.5%, while DVC's is 8.2%. This indicates that F-OAL not only maintains a high level of accuracy but also effectively manages forgetting.

### Resource Consumption

**GPU**. Peak GPU memory footprint on CIFAR-100 is shown in Figure 2. As we state in Section 3.5, without using gradient and extra space for exemplars, F-OAL requires a low memory footprint while having good performance (i.e., higher accuracy than LAE and EASE on most datasets).

**Training Time**. Table 3 illustrates training time including feature extraction. F-OAL is fast with competitive accuracy (i.e., higher accuracy than LAE). Only the classifier and regularized feature autocorrelation matrix are updated, leading to fewer of trainable parameters and fast training speed.

### Countering Recency Bias

As demonstrated in Figure 3, the linear classifier of AC obtained through F-OAL training does not exhibit recency bias. Notably, the weights corresponding to the classes in the most recent tasks do not significantly surpass those of the earlier classes. The frozen encoder and the recursive least square updating manner ensure equal treatment for all samples.

   Methods & CIFAR-100 & CORe50 & FGVCAircraft & DTD & Tiny-ImageNet & Country211 \\  LwF & 412 & 877 & 135 & 73 & 841 & 256 \\ EWC & 451 & 922 & 115 & 62 & 1,190 & 249 \\ iCaRL & 832 & 1,716 & 53 & 24 & 1671 & 513 \\ ER & 652 & 1,433 & 40 & 21 & 1,315 & 404 \\ ASER & 5,608 & 7,700 & 91 & 43 & 18,597 & 20,611 \\ SCR & 2,843 & 5,939 & 88 & 42 & 62,996 & 810 \\ DVC & 4,191 & 9,351 & 287 & 130 & 10,940 & 2,622 \\ PCR & 1,624 & 3,742 & 113 & 53 & 3,274 & 1,028 \\ EASE & 383 & 760 & 147 & 139 & 638 & 304 \\ LAE & **252** & **458** & 156 & 140 & **500** & 355 \\ SLCA & 726 & 1,416 & 289 & 278 & 1,185 & 551 \\ F-OAL & 261 & 570 & **16** & **8** & 507 & **157** \\   

Table 3: Training time including feature extraction is reported in seconds where replay-based methods are with 5,000 buffer size. Data in **Bold** show the fastest time.

Figure 2: Peak GPU memory footprint in GB with 10 batch size on CIFAR-100. Replay-based methods are with 5,000 buffer size. F-OAL has low GPU footprint since it does not require gradients.

### Ablation Study

We design the ablation study in Table 4 to verify the effectiveness of the Feature Fusion and Smooth Projection modules. Without these two components, the accuracy of F-OAL drops, especially on fine-grained datasets.

As Table 5 (See appendix B) shows, we prove analytic classifier is key to high accuracy. Herein, we define the Fully Connected Classifier (FCC) as having the identical structure to the AC, but it is updated through back-propagation rather than utilizing the \(R\) and recursive least square. Without AC, the accuracy drops from 91.1% to 32.4% on CIFAR-100.

**Regularization Term.** Table 6 (See appendix B) shows the effects of varying \(\). For large volume dataset, F-OAL gives a robust performance in wide range of \(\) value (e.g., \(10^{2}\) - \(10^{-3}\)), while small datasets need larger value (e.g., \(10^{2}\) - \(1\)). The comprehensive results show that \(\)=1 is the suitable choice.

**Projection Size**. Figure 4 (See appendix B) demonstrates the influence of different random projection sizes. The results suggest that the setting of a 1,000-dimensional projection is appropriate.

### Potential Positive and Negative Societal Impacts

The key advantage of our approach is its ability to achieve exemplar-free OCIL with fast training and low memory footprint, offering an environmentally friendly and efficient solution for this research track. However, the main limitation of our method lies in its reliance on a powerful pre-trained encoder. As a result, it is crucial for us to leverage open-source pre-trained backbone networks from the deep learning community, rather than training our own, which will otherwise lead to higher GPU usage and increased resource consumption.

## 5 Conclusion

In this paper, we propose Forward-only Online Analytic Learning (F-OAL), an exemplar-free method for addressing several limitations of the Online Class Incremental Learning scenario. We use Analytic Learning to acquire the optimal solution of the classifier directly instead of training for dozens of epochs by back-propagation. Leveraging the frozen pre-trained encoder with Feature Fusion and Smooth Projection and the Analytic Classifier updated by recursive least square, our approach achieves the identical solution to joint-learning on the whole dataset without preserving any historical exemplars, achieving high accuracy and reducing the resource consumption. Our experiments show the competitive performance of F-OAL.

## 6 Acknowledgment

This research was supported by the National Natural Science Foundation of China (62306117), the Guangzhou Basic and Applied Basic Research Foundation (2024A04J3681, 2023A04J1687), the South China University of Technology-TCL Technology Innovation Fund, the Fundamental Research Funds for the Central Universities (2023ZYGXZR023, 2024ZYGXZR074), the Guangdong Basic and Applied Basic Research Foundation (2024A1515010220), and the CAAI- MindSpore Open Fund developed on Openl Community.

  
**FF** & **SP** & **CIFAR-100** & **CORe50** & **FGVCAircraft** & **DTD** & **Tiny-ImageNet** & **Country211** \\  \(\) & \(\) & 91.1 & 96.3 & 62.2 & 82.8 & 91.2 & 24.4 \\ \(}\) & \(\) & 90.6 & 95.3 & 60.9 & 80.5 & 91.4 & 21.3 \\ \(\) & \(}\) & 90.7 & 95.4 & 58.7 & 79.3 & 91.2 & 22.8 \\ \(}\) & \(}\) & 90.6 & 95.4 & 56.0 & 71.2 & 91.4 & 21.1 \\   

Table 4: Average accuracy comparison with Feature Fusion (FF) and Smooth Projection (SP) modules across various datasets. \(\) means **with** and \(}\) means **without**.