# Zipfian Whitening

Sho Yokoi

Tohoku University / RIKEN

yokoi@tohoku.ac.jp

&Han Bao

Kyoto University

bao@i.kyoto-u.ac.jp

&Hiroto Kurita

Tohoku University

hiroto.kurita@dc.tohoku.ac.jp

&Hidetoshi Shimodaira

Kyoto University / RIKEN

shimo@i.kyoto-u.ac.jp

###### Abstract

The word embedding space in neural models is skewed, and correcting this can improve task performance. We point out that most approaches for modeling, correcting, and measuring the symmetry of an embedding space implicitly assume that the word frequencies are _uniform_; in reality, word frequencies follow a highly non-uniform distribution, known as _Zipf's law_. Surprisingly, simply performing PCA whitening weighted by the empirical word frequency that follows Zipf's law significantly improves task performance, surpassing established baselines. From a theoretical perspective, both our approach and existing methods can be clearly categorized: word representations are distributed according to an exponential family with either uniform or Zipfian base measures. By adopting the latter approach, we can naturally emphasize informative low-frequency words in terms of their vector norm, which becomes evident from the information-geometric perspective , and in terms of the loss functions for imbalanced classification . Additionally, our theory corroborates that popular natural language processing methods, such as skip-gram negative sampling , WhiteningBERT , and headless language models , work well just because their word embeddings encode the empirical word frequency into the underlying probabilistic model.

https://github.com/cl-tohoku/zipfian-whitening

## 1 Introduction

Representing discrete words by continuous vectors is a fundamental and powerful framework of modern deep-learning-based natural language processing (NLP). Static word embeddings , dynamic word embeddings , and causal language models  have caused a paradigm shift--they have greatly improved the performance of virtually all kinds of NLP applications and have been actively used in relevant areas as well. While the embedded units may be characters or subwords instead of words, we simply refer to them collectively as _word_.

Recently, the machine learning and NLP communities have discovered that the word embedding space is "skewed" and that correcting this can lead to better performance in downstream tasks . The isotropy of the embedding space would be one factor: vectors dispersing more evenly should be more discriminative than those clustered in the same direction . Typically, such spatial symmetry in the embedding space is enhanced through centering/whitening .

Nevertheless, we would like to point out that most existing approaches implicitly assume _uniform_ word frequency to formalize spatial symmetry. Consider the classical centering operation as an example: we first calculate the mean of the word vectors, and then subtract it to ensure they are zero-meaned. This method, however, has an unexpected pitfall. Recall that the definition of the centroid or barycenter of a random vector \( p\), assuming it has a finite set of distinct realizations,is given by \(_{ p}[]=_{i}p(_{i})_{i}\). The classical centering, based on the standard (unweighted) mean, implicitly assumes that all words occur uniformly \(p(_{1})==p(_{n})\). In reality, however, word frequencies are known to follow a highly non-uniform distribution1, creating a significant gap between the methodology and the actual usage of words. This seemingly obvious issue does not arise when addressing classical statistical estimation problems, as data vectors in our hands are usually representations of observations or instances. In contrast, word vectors used in NLP are representations of types or classes; each of them (such as the vector for 'the') abstracts the numerous instances (such as the tokens of 'the') appearing in the data. This problem of _hidden_ frequencies becomes apparent in the cases where the type-token distinction  is crucial, such as when dealing with natural language data (SS 2). The take-home message of this paper can be summarized as follows: use empirical word frequencies when calculating expected values. Following this very simple guideline leads to strong empirical outcomes (SS 3.2, SS 3.3) and opens a rich theoretical landscape (SS 4, SS 5).

**Notation** Let \(=\{w_{1},,w_{n}\}\) denote the vocabulary, i.e., the set of words in interest. Bold-face \(_{i}^{d}\) denotes the row vector of each word type \(w_{i}\), and \(p(w_{i})\) denotes its frequency.

## 2 Motivation: type-token distinction and expected values

Why have word frequencies been overlooked when considering the geometric properties of embedding spaces? This can be explained through the concept of _type-token distinction_, which is a fundamental concept in linguistics and related fields but generally not required in statistical machine learning. Here, **type** represents a class and **token** represents an instance. For example, the phrase 'perform natural language processing in a natural way' contains eight tokens and _seven_ types. The instances 'natural' appear twice, but as a word type, it is counted only once.

With the type-token distinction in mind, let us take a fresh look at data matrices and their expected values. Typically, each row in a data matrix represents one observation, i.e., one instance **token**. If we want to centralize a set of data vectors, computing the unweighted mean is a natural way in the machine learning pipeline. On the other hand, each row of a word embedding matrix, i.e., word vector, is a **type** embedding. Each word vector abstracts the numerous instances appearing repeatedly in a corpus, though information on the frequency of instances for each word type is not encoded in it. The unweighted mean of word vectors treats type vectors as token vectors, resulting in the complete omission of word frequency information.

Let us describe the above idea formally. The data matrix \(^{n d}\) or the set of data vectors \(\{_{i}\}_{i=1}^{n}^{d}\) represents a collection of instances, observations, or **tokens**; then the empirical distribution is \(_{}=_{i=1}^{n}_{i}}(_{i})\), where \(\) is the Dirac delta function. Here, the unweighted mean can be seen as the expectation \(}_{_{}}[]=_{i=1}^{n}_{i}}_{i}\) with the empirical distribution. On the other hand, the word embedding matrix \(^{n d}\) or the set of word vectors \(\{_{i}\}_{i=1}^{n}^{d}\) represents a collection of **types**. When describing the empirical distribution, the hidden frequency \(p\) of tokens is necessary. Given \(p\), the empirical distribution is \(_{}=_{i})}(_{i})\). From this perspective, the centroid of the word vectors should be written as the expectation \(_{_{}}[]=_{i}_{i})} _{i}\) over \(p\).

The distinction is not just "theoretical." First, refer to Fig. 1. Word vectors are known to cluster by frequency [39; 24; 44; 10]. In this situation, the centroid \(\) weighted by the word frequencies is located near the narrow region where high-frequent words are concentrated (a region with a light blue background), and thus differs from the unweighted mean \(\). Second, see Table 1, which shows

Figure 1: Low-frequent words \(\{

50 words sampled from each of types and tokens. Uniform sampling from types, corresponding to an unweighted mean, tends to select mostly rare words from the heavy tail. Sampling from tokens clearly captures a more natural representation of language as it typically appears in text.

## 3 Embedding symmetry

### Definition of embedding symmetry

In mathematical science fields, such as high-dimensional probability theory  and the volume of convex bodies , there are numerous intriguing definitions of spatial symmetry. Among them, we begin with the definition of the symmetry of _random_ vectors with their frequencies [48; 55]. This is suited for dealing with word vectors because they entail word _frequencies_, unlike usual data instances.

**Definition 1** (A random vector \( p\) on \(^{d}\) is in on \(^{d}\) has zero mean; the _1st_ moment of a symmetric random vector)**.**

\[}_{ p}[]=\] (1)

From these definitions, we will develop methods to _adjust_ given word vectors to be symmetric in SS 3.2, and to _evaluate_ the symmetry of given word vectors in SS 3.3.

In machine learning and NLP, the spatial symmetry of embedding spaces is a hot topic, and numerous theories and algorithms have been proposed [41; 21; 38; 56]. However, the approach in many researches implicitly treats all vectors equally, ignoring word frequency information. In the following sections, we will detail both the empirical and theoretical issues that a uniform approach can cause, especially when applied to NLP tasks. Furthermore, when embeddings correspond to tokens rather than types--such as in the internal representations of masked or causal language models--a uniform approach tends to be effective. This point will be discussed in SS 5.1.

### Enhancement of embedding symmetry

This section proposes **Zipfian whitening2**, which symmetrizes a given set of word vectors with word frequency. At a glance, the most natural method to achieve Def. 1 and Def. 2 would be PCA whitening, also known as sphering. Notably, each step of whitening--centering, decorrelation, and standardization--implicitly involves calculating expected values. Our approach is simple: each time we calculate an expected value, we should weight it by the empirical word frequency. The specific algorithm is as shown in Algorithm 1. The only difference from general whitening is that it uses word frequency in the part highlighted in blue. Please refer to Appendix A for a formal explanation showing that the word vectors obtained by the proposed algorithm actually satisfy Def. 1 and Def. 2.

   Words sampled from types & Words sampled from tokens \\  ‘scintillation’, ‘fann’, ‘rubato’, ‘upstanding’, ‘collard’, ‘nine’, ‘ranked’, ‘zero’, ‘the’, ‘garcia’, ‘creeks’, ‘skokokum’, ‘unbelievers’, ‘moncyte’, 'nishikawa’, ‘rank’, ‘station’, ‘the’, ‘for’, ‘crusher’, ‘gerven’, ‘abrah’, ‘silverchair’, 'hangman’, ‘villians’, ‘drunken’, ‘a’, ‘one’, ‘eight’, ‘unitary’, ‘klausen’, ‘arousal’, ‘heat’, ‘bridgeorth’, ‘of’, ‘were’, 'zero’, ‘debate’, ‘orchestra’, ‘mildred’, ‘porton’, ‘aquasox’, ‘wylie’, ‘hipaa’, 'krimuk’, ‘of’, ‘wrist’, ‘points’, ‘fractured’, ‘the’, ‘hexadron’, ‘kuei’, ‘barbera’, ‘dalvi’, ‘gilding’, ‘to’, ‘redricet’, ‘adnan’, ‘white’, ‘car’, ‘viaskahapatnam’, ‘tatasuo’, ‘tarsacon’, ‘bajram’, ‘scholes’, ‘fond’, ‘concluded’, ‘under’, ‘two’, ‘by’, ‘hadad’, ‘incidental’, ‘theodisius’, ’reichskommissariat’, ‘five’, ‘his’, ‘infection’, ‘the’, ‘boeheim’, ‘amsl’, ‘buencamino’, ‘thrasyvoulos’, ‘insulated’, ‘pop’, ‘in’, ‘one’, ‘in’, ‘one’, ‘one’, ‘fram’, ‘discourtesy’, ‘nisra’, ‘ycko’, ‘luen’, ‘dooku’ & ‘handled’, ‘battle’, ‘mutual’ \\   

Table 1: The difference between type-based sampling and token-based sampling.

**Empirical evaluation:** We confirm the effectiveness of Zipfian whitening (Algorithm 1) by measuring performance on standard sentence-level downstream tasks using post-processed word vectors. We employed the most standard word embeddings--GloVe , word2vec , and fastText --and utilized the widely adopted evaluation tasks, including STS-B  and related benchmarks. Detailed experimental settings can be found in Appendix B. Table 2 shows the results on the STS-B task. Remarkably, the proposed Zipfian whitening shows significant advantages not only over standard (uniform) centering and whitening but also over the strong baseline method  specifically designed to create powerful sentence vectors. Consistent results were obtained with various benchmark datasets, multiple empirical word probabilities, and a language other than English (Appendix C)3. In SS 4.2.1, one reason for this remarkable performance is clarified from the perspective of information geometry.

### Evaluation of embedding symmetry

The community is greatly interested not only in making word vector spaces symmetric but also in evaluating _how symmetric_ or asymmetric a space is . Here, we return to Def. 1 and Def. 2 and describe metrics for evaluating the symmetry of word embedding spaces with word frequency.

**Degree of centrality--the 1st moment of symmetry:** Recall that, if the barycenter \([]\) is close to \(\), then the random vector \(\) can be considered symmetric in terms of the first moment (Def. 1).

   GloVe &  \\   & Uniform & Zipfian \\ + Centering & 45.17 & **52.25** \\ + Whitening & 52.21 & **66.92** \\  + ABTT  & 54.28 & \\ + SIF + CCR  & 58.70 & \\    
   Word2Vec &  \\   & Uniform & Zipfian \\ + Centering & 55.85 & **58.84** \\ + Whitening & 56.03 & **66.50** \\  + ABTT  & 56.98 & \\ + SIF + CCR  & 63.04 & \\   

Table 2: The empirical performance of Zipfian whitening, which exploits the empirical frequency of words during expectation calculations. Each cell shows the STS-B  score \( 100\). By carefully performing the simple operation of whitening, it consistently outperforms powerful baseline methods.

Thus, examining the value of \(\|[]\|[]-\) appears to be a reasonable way to measure the symmetry of the first moment. However, random vectors \(\) and \(\) (\(_{>0}\)) should be considered equivalent in terms of spatial symmetry. Thus, we define the scale-invariant metric (Def. 3), obtained by dividing \(\|[]\|\) by the average length \([\|\|]\).

**Definition 3** (Degree of centrality for the random vector \( p\); the 1st moment of symmetry).: \[_{1}() 1-\|}_{ p}[ ]\|}_{ p}[\|\|]\] (3)

By definition, \(_{1}()\) takes values in \(\), and \(_{1}()=1\) if and only if \(\) is zero mean.

**Degree of isotropy--the 2nd moment of symmetry:** If the covariance matrix \([(-[])(-[])^{}]\) is a constant multiple of the identity matrix \(_{d}\), i.e., if the random vector \(\) has an equal spread in all directions, \(\) is symmetric in terms of the second moment (Def. 2). Following convention, this degree can be confirmed by examining the flatness of the eigenspectrum.

**Definition 4** (Degree of isotropy around the barycenter for the random vector \( p\); the 2nd moment of symmetry).: \[_{2}()H(}{_{j}_{j}},,}{_{j}_{j }})\] (4) \[\{_{1},,_{d}\}\]

_are the eigenvalues of the covariance matrix \(}_{ p}[(-}_{ p}[ ])(-}_{ p}[])^{}]\). \(H(p_{1},,p_{d})-_{i}p_{i} p_{i}\) is the Shannon entropy._

**Proposition 1**.: \(_{2}()\) _takes values in \(\), and \(_{2}()=1\) if and only if \(\) is isotropic around its barycenter (Def. 2)._

Proof.: Please refer to Appendix D.

Note that the approach of measuring the entropy of the spectrum to evaluate the flatness of a signal can be found in many fields. For example, similar definitions are seen in probability processes  and signal processing [17; 47]. We also follow this standard and powerful line.

**Algorithm:** To compute the evaluation metrics of symmetry (Def. 3, Def. 4) for given word vectors, again, one should just use the empirical word frequency when calculating the expectations. A pseudocode for measuring symmetry is provided in Appendix E.

**Empirical evaluation:** To what extent does our symmetry score (an intrinsic evaluation of embedding spaces) correlate with downstream task performance (an extrinsic evaluation of those)? As baselines, we use versions of our symmetry score that do _not_ account for word frequency, calculated in a uniform manner. We also compare with popular symmetry scores in NLP, the average of cosine similarity (**Ave. Cos.**)  and the recently proposed **IsoScore**. Note that all these baselines implicitly assume uniform word frequency. Additional experimental settings can be found in Appendix B. Fig. 2 shows the results. The right side of Fig. 2 demonstrates the superiority of the Zipfian approach. Moving from the bottom-left to the top-right of the figure--i.e. as both the 1st (\(x\)-axis) and 2nd moments (\(y\)-axis) of the symmetry score increase--it is clearly visible that the downstream task performance increases (the color becomes more red). In contrast, in the left-hand plot, which assumes uniform word frequency, there is no observed relationship between the symmetry score (\(x\) and \(y\)-axis) and the downstream task performance (color). Table 3 lists the correlation coefficients between the symmetry scores and downstream task performance in more detail. It can be seen that the symmetry scores considering word frequency can "predict" downstream task performance with remarkably high correlation. On the other hand, the "prediction" performance of other metrics, including Ave. Cos. and IsoScore that implicitly assume uniform word frequency, is unsatisfactory. Surprisingly, when the most popular Ave. Cos. metric shows almost no correlation (\(0.04\)) with downstream task performance (STS-B), Zipfian symmetry metric has a strong positive correlation (\(0.83\)) with it.

## 4 Why is Zipfian whitening better than uniform whitening?

A natural question is why the Zipfian approach empirically dramatically outperforms the uniform approach. We provide a theoretical explanation using Table 4. In a nutshell, a significant difference arises depending on whether the base measure of an exponential family is uniform or Zipfian.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

**What does whitening do?** Mu and Viswanath  proposed a method to approximately make the partition function of the uniform prior model constant by centering the word vectors and removing the top principal components (10). Our Zipfian whitening corresponds to Mu and Viswanath's post-processing method, in the sense that ours and theirs make the partition function constant up to the second moment (11) and (10), respectively. In summary, Zipfian whitening (11) transforms a probabilistic model into an exponential family adopted with the Zipfian base measure (7), making it closer to the Levy-Goldberg formula (19).

### Emphasis on rare words by Zipfian prior

Let us explore further why the Zipfian prior results in good performance in downstream tasks (SS 3.2). In summary, the Zipfian prior approach emphasizes low-frequency words, while the uniform prior approach emphasizes high-frequency words, both from perspectives of vector norms and errors/losses. So far in this paper, we have repeatedly discussed weighting each word according to frequency, so it may seem contradictory that Zipfian approach emphasizes low-frequency words as a result. To illustrate, let us reconsider centering. In centering, the mean vector is _subtracted_ from each vector. Weighting each word vector by frequency when constructing the mean vector means that signals corresponding to high-frequency words are removed more substantially from each vector. The emphasis on low-frequency words has been repeatedly supported throughout the history of NLP and information retrieval, such as Luhn's hypothesis , inverse document frequency (IDF) , and smooth inverse frequency (SIF) . For instance, it is reasonable to emphasize the word 'isotropy' when creating a sentence embedding containing both words 'the' and 'isotropy'.

#### 4.2.1 From the perspective of vector norm

Under the Zipfian prior model, _words with larger information content have longer (emphasized) vector representations_. Conversely, under the uniform prior model, words with smaller information content have longer (emphasized) vector representations.

As a representative example of **uniform prior** models, the norms of word vectors learned by random walk language models are theoretically and empirically proportional to word frequency (12) (see Eq. (2.4) and Fig. 2 in Arora et al. ). That is, in such embedding space, words with _less_ information (e.g., 'the') are emphasized. This tendency is consistently observed in dynamic language models and causal language models that adopt the softmax cross-entropy loss, another typical example of the uniform prior family . By contrast, when training word embeddings with skip-gram negative sampling , the word embeddings follow the **Zipfian prior** family, and their norms become larger with greater information, which we show subsequently [50; 60; 42]. Based on the formulation of the exponential family and following Eq. (12) of Oyama et al. , we formally describe the norm properties of the word vectors obtained from the Zipfian prior model.

**Theorem 1** (The norm of a word vector learned with empirical Zipfian prior models reflect the information amount of the word; a refined version of  Eq. (12)). _Assume that word embeddings \(\{_{i}\}_{i}\) follow the Zipfian prior model (7). For the same word \(t\), the vector \(\) on the predicted side and the vector \(\) on the predicting side are shared: \((t)(t)\) (weight tying), and \(_{t}p(t)(t)=\) (centered w.r.t. Zipfian prior), then each word vector \((t)\) satisfy_

\[\|(t)\|^{2}_{(t)} 2(p()\|p( t)), (t)_{t^{}}p(t^{} t) {c}(t^{})(t^{})^{},\] (20)

_where \(\|\|_{}\) with a positive definite matrix \(\) denote a norm based on a quadratic form \(^{}}\)5._

Proof.: Refer to Appendix F.

In Fig. 3, we experimentally confirmed that the norms of informative words become larger with Zipfian whitening (shown from center to the right in Fig. 3), bringing them closer to the ideal Zipfian prior model6.

#### 4.2.2 From the perspective of error and loss

_The error and loss functions associated with the Zipfian prior model emphasize low-frequency words._ In contrast, the error and loss functions of the uniform prior model focus on the average loss across the entire dataset, resulting in a greater emphasis on high-frequency words.

The standard classification loss is the softmax cross-entropy loss (14). By taking its expectation over the dataset \(\{(w,c)\}\), embeddings associated with higher-frequency words receive more updates because the softmax is the uniform inverse link, corresponding to the **uniform prior** model. By contrast, the logit-adjusted loss (15) has been proposed to tackle class imbalance . From our viewpoint, the logit adjustment term \(p(w)\) makes the inverse link belong to the **Zipfian prior** model. The softmax and logit-adjusted losses are Fisher consistent to the misclassification (16) and balanced (17) error rates, respectively. As the latter tends to stress minor classes, the logit-adjusted loss and Zipfian prior model are suitable for emphasizing low-frequency words during the learning process.

Another prominent loss function for representation learning is contrastive loss, with the SGNS loss (word2vec)  as a representative example in the context of word representation learning. This loss similarly uses a loss aligned with the **Zipfian prior**:

\[-*{}_{(w,c)}\![(, )+_{w^{}}^{i=1,,k}(- ^{}_{i},)]\!,\] (21)

where \(\) is sigmoid function, and \(k\) is the number of negative samples. Since high-frequency words are more likely to be sampled as negative examples, the loss has less impact on high-frequency words in positive examples. Consequently, low-frequency positive words are relatively emphasized in representation learning. The Levy-Goldberg formula in the previous section describes the properties of an ideally trained word2vec model, which are essentially the properties of Zipfian prior models.

## 5 Unified explanation of the efficacy of existing methods

Distinguishing the distribution that the base measure follows helps us understand why some existing NLP methods are effective.

### Uniform whitening of token embeddings \(\) Zipfian whitening of type embeddings

Masked language models like BERT  and RoBERTa  produce dynamic (contextualized) _token_ embeddings. Adding up such token embeddings of constituent tokens to create sentence embeddings often leads to poor empirical performance . However, symmetrizing significantly improves their performance; such methods including "batch _centering_," "_Whitening_BERT," and contrastive learning methods . This improvement can also be explained from the perspective of the Zipfian prior. A dataset or corpus is first fed into the model to obtain _token_ embeddings7. Centering/whitening is then applied to this entire set of embeddings. As this token embedding (multi)set has the multiplicity asymptotically proportional to the word frequency,

Figure 3: Relationships between the information content \(- p(w)\) and the vector norms \(\|\|_{2}\) for top 500 frequent words \(w\). The figure in the center represents the pre-trained GloVe model. By using **Zipfian** whitening, the information content gets encoded in the norm (center to right). Conversely, with **uniform** whitening, this phenomenon does not occur (center to left).

this _uniform_ centering/whitening of _token_ embeddings corresponds to the word-frequency-weighted (_Zipfian_) centering/whitening of _type_ embeddings. For a more formal description of the above explanations, please refer to Appendix H. Additionally, recent work has found that contrastive additive sentence encoders implicitly weight words by their information content . This finding is consistent with the previous discussion on vector norms (SS 4.2.1), and can be seen as indirect evidence supporting the idea that these models belong to the Zipfian prior family.

This idea can also be supported by empirical evidence. This idea is also supported by empirical evidence. To establish a baseline for centering and whitening token embeddings under a uniform prior, we scale each embedding by the reciprocal of its type frequency, ensuring uniform treatment across types. Refer to the Appendix H for the detailed computation of this _pseudo uniform_ approach and a formal explanation of how it achieves type uniformity. Table 5 shows the results. Comparing the pseudo-uniform centering/whitening (which assumes a _uniform_ prior over types) with the conventional token-level uniform centering/whitening (which implicitly assumes a _Zipfian_ prior over types) reveals that the latter approach based on a Zipfian prior empirically achieves better performance. Additional experimental settings and results can be found in Appendix B and Appendix I.

### Headless causal language model roughly belongs to Zipfian prior family

The recently proposed headless language model  uses only words within the same batch to predict next tokens with a pseudo-softmax function. This method originally aimed to reduce the computational cost of the softmax function in the \(||\) direction, but an interesting side effect is the improvement in the performance. This success can also be explained from the perspective of Zipfian priors. If we repeatedly sample small batches, the sampling frequency of each word will increasingly reflect its true frequency as the batch size approaches \(1\).

## 6 Conclusion

Standard methods for adjusting and measuring symmetries in word embedding spaces--such as centering and whitening--implicitly assume _uniformly_ distributed word frequencies, which is unrealistic. We hypothesize that, based on the type-token distinction, using empirical _Zipfian_ word frequencies is essential when calculating the expectation (SS 2). Based on the idea and the definitions of first- and second-order symmetry in random vectors, we derived Zipfian whitening, which _enhances the symmetry_ of the word embedding space. Even though it is nearly identical to standard PCA whitening, Zipfian whitening significantly outperforms existing methods (SS 3.2). Similarly, we derived a metric to _evaluate the symmetry_ of word embedding spaces. Our intrinsic metrics showed a strong correlation with extrinsic task performance, even when popular metrics show almost none (SS 3.3). We then presented a framework explaining the differences in effect between whitening based on uniform and Zipfian approaches, by attributing them to differences in the base measure of the exponential family (SS 4.1). By further exploring this viewpoint through information geometry and loss functions, we showed how the Zipfian approach emphasizes the informativeness of low-frequency words (SS 4.2.1). Lastly, through our proposed viewpoint, we found that popular NLP methods perform well because their word embeddings end up encoding a Zipfian prior; such models include word2vec  (SS 4.2.2), WhiteningBERT  (SS 5.1), and headless language models  (SS 5.2).

    &  \\   & “Uniform” & “Zipfian” \\ + Centering & 64.04 & **64.82** \\ + Whitening & 60.53 & **64.91** \\    
    &  \\   & “Uniform” & “Zipfian” \\ + Centering & 60.34 & **61.30** \\ + Whitening & 61.31 & **65.59** \\   

Table 5: The empirical performance difference between “uniform”—enforced centering and whitening with a uniform prior for dynamic embeddings, and “Zipfian”—conventional uniform centering and whitening over tokens with an implicit Zipfian prior over types. Each cell shows the STS-B  score \( 100\). This comparison reveals that token-level uniform centering/whitening, corresponding to type-level Zipfian centering/whitening, leads to empirically better performance.

## Limitations

### How these assumptions might be violated in practice

In our theoretical analysis concerning norms, and in the discussion on the relationship between whitening and normalization constants, we have proceeded by ignoring the residual terms beyond the second order. Empirically, focusing only on the first and second order has yielded significant results. However, to accurately identify cases where the proposed method might fail, a detailed theoretical and empirical examination of the asymptotic behavior of higher-order moments might be crucial. This remains an important future work.

The condition that the partition function is constant is only a necessary condition from the perspective of both the generative model's optimal solution and whitening. The true logical relationship between whitening and the generative model has not been clarified. In particular, verifying whether the projection through whitening allows us to transition between the two model families (the uniform family and the Zipfian family) is an intriguing and valuable direction for both theoretical exploration and practical application.

### The scope of the empirical claims made

Our experiments primarily focused on static and dynamic word embeddings, as many of their theoretical properties have been understood and they have been central to the rise of isotropization. Admittedly, this paper also advances our understanding of causal language models. However, to make a more significant practical impact in the era of large language models, employing the proposed method as a regularization term for next-token prediction holds great promise for future work.

The experiments utilized typical downstream NLP tasks, particularly popular datasets for sentence-level semantic tasks. By scaling up the task set to include word-level tasks or leveraging a broader range of multilingual data, we can more robustly demonstrate the practical utility of the proposed framework.

### The factors that influence the performance of our approach

The proposed method inherently involves numerically unstable calculations, such as multiplying by the inverse of small singular values. Embeddings for low-frequency words are often far from converged even after extensive pre-training, and the eigenvalues of the embedding space are known to decay. Given these situations, the adverse effects of small singular values are plausible. Considering recent advancements in whitening techniques, developing a more numerically stable algorithm is an important direction for future work.

## Broader Impacts

**Potential impacts to AI alignment**Dohmatob et al.  reported that repeated sampling from generative AIs may shift word frequency distributions toward lighter-tailed distributions. This may reduce linguistic diversity and lead to cultural homogenization by diminishing region-specific orculturally unique expressions. Our Zipfian whitening and similar regularization methods could enhance output diversity, enriching the linguistic landscape.

**Potential negative societal impacts** The sentence similarity tasks used in our evaluation experiments are now considered core technologies for RAG (retrieval-augmented generation), which is essential when large language models leverage external resources. If chatbots generate responses tailored to user ideologies or preferred information sources, it may result in negative societal impacts, including political agitation.