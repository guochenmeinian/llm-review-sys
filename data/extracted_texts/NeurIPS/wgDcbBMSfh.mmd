# CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion

Yangruibo Ding\({}^{1}\) Zijian Wang\({}^{2,}\)1

Wasi Uddin Ahmad\({}^{2,*}\)

**Hantian Ding\({}^{2}\) Ming Tan\({}^{2}\) Nihal Jain\({}^{2}\) Murali Krishna Ramanathan\({}^{2}\) Ramesh Nallapati\({}^{2}\) Parminder Bhatia\({}^{2}\) Dan Roth\({}^{2}\) Bing Xiang\({}^{2}\)**

\({}^{1}\)Columbia University \({}^{2}\)AWS AI Labs

yrbding@cs.columbia.edu {zijwan,wuahmad}@amazon.com

https://crosscodeeval.github.io

###### Abstract

Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly.

To fill in this gap, we propose CrossCodeEval, a diverse and multilingual code completion benchmark that necessitates an in-depth cross-file contextual understanding to complete the code accurately. CrossCodeEval is built on a diverse set of real-world, open-sourced, permissively-licensed repositories in four popular programming languages: Python, Java, TypeScript, and C#. To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-analysis-based approach to pinpoint the use of cross-file context within the current file.

Extensive experiments on state-of-the-art code language models like CodeGen and StarCoder demonstrate that CrossCodeEval is extremely challenging when the relevant cross-file context is absent, and we see clear improvements when adding these context into the prompt. However, despite such improvements, the pinnacle of performance remains notably unattained even with the highest-performing model, indicating that CrossCodeEval is also capable of assessing model's capability in leveraging extensive context to make better code completion. Finally, we benchmarked various methods in retrieving cross-file context, and show that CrossCodeEval can also be used to measure the capability of code retrievers.

## 1 Introduction

Language models for code (code LMs), such as Codex (Chen et al., 2021), CodeGen (Nijkamp et al., 2023, 2023), and StarCoder (Li et al., 2023), have demonstrated their power to enhance developer productivity through their promising results in code completion tasks. To evaluate these models, researchers propose multiple code completion evaluation benchmarks, (e.g., Chen et al., 2021; Lu et al., 2021; Athiwaratkun et al., 2023; Austin et al., 2021), where the model is asked to complete the code given the context in the current file. However, such an evaluation setting is over-simplified, and it is not able to reflect the model's capability in code completion accurately. Specifically, in the realm of modern software development, repositories consist of multiple files, each interwoven withextensive cross-file dependencies, i.e., contextual information from other source code files within the same repository. A significant drawback of most existing benchmarks is their tendency to overlook these complex dependencies. As a result, they fall short of providing a comprehensive evaluation of code completion models within realistic real-world scenarios.

Figure 1 illustrates the limitation of common code completion evaluation sets with a real example. The developer is writing a test case for a class, CaseConverter, implemented within the current repository. CodeGen-2B-mono (Nijkamp et al., 2023b), a large Python code LM, fails to complete the API call if only the current file context is present.

Motivated by such examples and to fill in the need of evaluating code completion in realistic software development with numerous intervening cross-file context dependencies, we propose CrossCodeEval, a diverse and multilingual benchmark to evaluate code language models' ability to use cross-file context for code completion. This new dataset is composed of 10k examples from 1k repositories in 4 languages. Unlike existing datasets where the correct answer could be predicted with only context from the current file, CrossCodeEval strictly requires cross-file context to correctly complete the missing code (SS2.2). CrossCodeEval's examples are carefully curated from existing open-sourced repositories with a series of quality filters, and we ensure CrossCodeEval has minimal overlap with the training data from existing code LMs, eliminating the confounder of data leakage and memorization in result interpretation (SS2.1 & 2.3).

We conducted a comprehensive evaluation of popular public and proprietary code LMs: CodeGen (Nijkamp et al., 2023b, a) and StarCoder (Li et al., 2023) in various sizes from 350M to 16B parameters, and OpenAI's GPT-3.5-Turbo in SS3. Empirical results reveal that when given _only_ the current-file context, these models yield suboptimal results. Remarkably, incorporating cross-file context into the prompt significantly enhances the performance of these code LMs, even in a zero-shot setting. This underscores that CrossCodeEval effectively serves its goal as a benchmark aimed at evaluating cross-file code completion. Moreover, even when providing cross-file context in the prompt, the performance of the most powerful models remains notably imperfect, highlighting that CrossCodeEval is also instrumental in assessing a model's ability for leveraging extensive context in code completion. Lastly, we benchmarked various retrieval methods from sparse to dense, demonstrating that CrossCodeEval can additionally serve as a benchmark for code retrieval.

## 2 CrossCodeEval: A Benchmark for Cross-File Code Completion

CrossCodeEval is a diverse and multilingual scope completion dataset in four popular languages: Python, Java, TypeScript, and C# where examples include code prompts ending in an imagined cursor position and references include the code token sequences from the cursor position to the end of the statement. CrossCodeEval examples have a key property - the statement to be completed must

Figure 1: Code LM fails to complete a Python test case since the in-file context (left figure) does not provide sufficient information. The function name from the current file indicates that the completing function is a test case for convert_camel_to_snake, so with only such context, the model hallucinates wrong completion as convert_camel_to_snake. However, the failure is not due to the model’s capacity, but the necessary cross-file context is not present (right figure). When the class CaseConverter is present in the prompt, the model generates camel_to_snake correctly.

have at least one use of local API (classes, variables, and methods defined in the software repository). Next, we briefly describe how we collect the software repositories (SS2.1), select a subset of them for CrossCodeEval construction (SS2.2), post-processing and quality control process (SS2.3), and CrossCodeEval statistics and future scope for extending the dataset (SS2.4).

### Dataset Collection

We collect permissively licensed repositories from GitHub. To mitigate potential data leakage issues,2 we focus on repos that were created _recently_ and not forks. Specifically, we collected repos created between 2023-03-05 to 2023-06-15 on 2023-09-01. The time span ensures sufficient data collected with no overlap with the training data of many existing code LMs released before mid-2023, no matter whether the data is publicly available or not. We limit repos to contain the four languages we study and we keep only repos with zipped file size \(<1\)MB and number of stars \(>=3\). Then we filter out repos that have fewer than 10 or more than 50 source code files. Finally, we remove the repos with at least one source code file that exactly matches one of the code files in the commonly used Stack (Kocetkov et al., 2022) dataset. As a result, we ended up with 471, 239, 193, and 99 repos, respectively.

### Dataset Generation

We propose a static-analysis-based method to identify code fragments that require cross-file context automatically. Our approach is illustrated in Fig 2. First, we find all intra-project imports in the original file. Next, an empty class is created for each imported name to replace the import statement. Since the imported name now refers to an empty class, any subsequent call to its member function or attribute will raise an undefined name error. We leverage static analysis to catch such errors in the modified file, which precisely correspond to the names in the original file that can only be resolved by cross-file context. We map the location of undefined names back to the original file to determine the split point of the prompt and the reference.

To increase the variety in the dataset, we randomly select a tree-sitter3 token in the same line before the cross-file entity to be the cursor location, splitting the code to a prompt and a reference. It is often the case that the same the cross-file API is called multiple times in a file, and there is a chance for models to infer the API name from previous calls even without cross-file context. Therefore, if the same undefined name is reported at multiple places in a file, we keep only the first occurrence. In this work, we focus on instantiating our approach in the four popular programming languages, while the idea can be generalized to other languages in principle. Specifically, for Python, we use Pylint4 to detect undefined names; for Java, we use javac compiler; for TypeScript, we use tsc compiler;

Figure 2: We replace the third import statement, which is from the same repository, with an empty class. Consequently, camel_to_snake in the last line becomes an undefined name in the modified file. Thereby, we know this method in the original file is defined only in the cross-file context.

for C#, we use csc compiler from the mono5 image. We use tree-sitter to identify full statements to construct reference completions in Python. For Java, TypeScript, and C#, we consider statements ending with either "\(\)", "\(\)", or "\(\)". See Appendix A for more details.

### Post-processing and Quality Control

We designed a series of rule-based and model-based post-processing filters to ensure the quality of the dataset. We filter examples if (1) fewer than \(N\) lines of code (lines not including import statements, where \(N=10,20,30,5\) for Python, Java, TypeScript, and C#) in the prompt, (2) too short (\(<3\) tokens), or long (\(>30\) tokens) reference. We exclude examples if the references are found verbatim in any other source code file within the repository (_i.e._, cross-file). We further discard examples with duplicate references. The filtering steps collectively remove 15%-20% of the examples.

Moreover, to ensure that the reference isn't predictably inferred solely from the current file (possibly owing to strong clues in function names and comments), we feed the examples (input prompts) to starcoderbase-1B model (Li et al., 2023) to complete the statement and remove the exact matches. This step results in the removal of \(<\)10% of the generated examples. As an ancillary benefit, this further safeguards that the examples are not seen by publicly available code LMs while CrossCodeEval is constructed based on repositories that do not overlap with the Stack and possibly other private pre-training datasets created prior to 2023. Finally, we perform human annotations on a subsample of the resulting CrossCodeEval and found that the dataset has a satisfactory quality to serve the goal of cross-file code completion. See Appendix B for more details.

### Dataset Statistics, Scope, and Future Extensions

StatisticsWe present the statistics of CrossCodeEval in Table 1. We use the StarCoder tokenizer (Li et al., 2023) to compute the number of tokens.

ScopeIn addition to prompts and references, we include the code lines that follow the references from the original source code files in CrossCodeEval examples. Given the source code lines to the left (prompt or prefix) and right (suffix) of the references, CrossCodeEval can be used to evaluate code LMs for their fill-in-the-middle (FIM) capabilities (Bavarian et al., 2022).

Future ExtensionsCrossCodeEval currently supports four popular languages. As our method is generalizable, CrossCodeEval can potentially be extended to other languages. Additionally, we advise future code LM pre-training datasets should explicitly exclude CrossCodeEval to minimize the effect of memorization.

## 3 Experiments

### Models

We benchmark CrossCodeEval with popular public and proprietary large language models.

CodeGen(Nijkamp et al., 2023, 2023) is a series of generative code LMs. CodeGen supports left-only context. CodeGen2.5 notably supports fill-in-the-middle and further improves the performance via multi-epoch training. We benchmarked CodeGen models at various sizes from 350M to 16B.

StarCoder(Li et al., 2023) is a generative multi-query-based code LM with 15.5B model parameters trained on The Stack dataset (Kocetkov et al., 2022). It supports up to 8k tokens. We also benchmarked its base version with at varied sizes: 1B, 3B, and 7B.

   Feature & Python & Java & TypeScript & C\# \\  \# Repositories & 471 & 239 & 193 & 99 \\ \# Files & 1368 & 745 & 779 & 642 \\ \# Examples & 2665 & 2139 & 3356 & 1768 \\ Avg. \# lines in prompt & 90.6 & 106.7 & 116.5 & 71.1 \\ Avg. \# tokens in prompt & 938.9 & 995.3 & 944.9 & 584.1 \\ Avg. \# lines in reference & 1.0 & 1.1 & 1.7 & 1.7 \\ Avg. \# tokens in reference & 13.2 & 14.5 & 17.4 & 12.5 \\   

Table 1: CrossCodeEval statistics.

GPT-3.5-turbo(Ouyang et al., 2022) is one of the most powerful models developed by OpenAI. It was trained with comprehensive text and code data and supports up to 4k max sequence length. Its model weight remains proprietary and is accessible exclusively via APIs.

### Evaluation Metrics

In evaluating the performance of code language models, we report performance in two main categories: code match and identifier match (Ding et al., 2022).

Code MatchThe code match metric directly compares the generated code with the reference and is measured using exact match (EM) and edit similarity (ES). These metrics help to assess the overall accuracy of the code completion process, taking into account elements such as identifiers, keywords, operators, delimiters, and literals.

Identifier MatchThis metric evaluates the model's ability to predict the correct application programming interfaces (APIs). To perform this evaluation, we first parse the code and extract the identifiers from the model prediction and reference, resulting in two ordered lists of identifiers. We then compare the predicted identifiers with the reference and report the results in EM and F1 score.

### Experimental Setup

Our evaluation framework is based on the Transformers (Wolf et al., 2020) library. All the experiments are conducted with the zero-shot setting, and no training is involved. We use the same set of hyperparameters for code generation across all models. We set the maximum sequence length to 2,048 for the CodeGen family, 4096 for GPT-3.5-turbo, and 8,192 for the StarCoder family. We use a maximum generation length of 50 and the rest as the prompt.

We explore greedy search and nucleus sampling (Holtzman et al., 2020) with reranking (Hossain et al., 2020). We found there is no significant difference between the two, and we present the greedy search results in the main paper and refer readers to Appendix D.2 for nucleus sampling.

We post-process model predictions to extract statements.6 For Python, we iteratively parse the concatenation of prompt and \(n\) completion tokens (e.g., \(n=1,2,,50\)) until the sequence becomes

Figure 3: An illustrative example to showcase the use of in-file context, retrieved cross-file context, and retrieved context using reference in prompts. While the baseline prompting uses in-file context only, “Retrieval” and “Retrieval w/ Ref.” prompting uses retrieved contexts by prepending them to the in-file context.

parseable (no syntax error) and the \((n+1)\)-th completion token is a newline character.7 For Java, TypeScript, and C#, we consider statements ending with ";", "(" and ")", instead of a new line.

Only In-File ContextIn standard practice, pre-trained language models are utilized to perform code completion in a _zero-shot_ manner by taking into account the provided code context. Following the practice, we conduct experiments using the code LMs (3.1), where they are provided code context from the current file. As shown in Figure 3, the baseline prompt includes only in-file context.

Retrieved Cross-file ContextInspired by the effectiveness of the recently proposed retrieve-and-generate (RG) framework for repository-level code completion (Zhang et al., 2023), we adopt it for cross-file context retrieval.8 In the RG framework, the retrieval database is constructed by iteratively scanning the files in the repository and extracting contiguous \(M\) lines (in all our experiments, \(M=10\)) of non-overlapping code fragments, which are the candidates for cross-file context retrieval. The query for the retrieval is built using the last \(N\) lines (we set \(N=10\)) of the in-file context. We use BM25 (Robertson et al., 2009) to calculate the similarity between the query and the candidates (cross-file context chunks), and use the top-5 similar code snippets as the cross-file context, see "Retrieval Context" in Figure 3. We consider a maximum of 512 BPE tokens for such context, and the rest of the tokens will be truncated. Figure 3 illustrates9 the retrieved context and the corresponding prompt for the model to complete. Given the in-file context as a query, the RG framework successfully retrieves the class definition of CaseConverter that is in another file for utilities. We further wrap the class definition into a template as code comment and use it as the cross-file context. To build the retrieval prompt, we prepend the retrieved context to the in-file context.

Retrieval with ReferenceTo quantify the upper bound impacts of cross-file context retrieved by the RG framework, we devise "retrieval with reference" for comparison. In this setting, we make use of not only the in-file context (as in standard retrieval setting) but also _the reference_ to retrieve the cross-file context. Specifically, the query is constructed by using the last \(N\) lines of _the concatenation of the in-file context and the reference completion_, instead of the in-file context only in the standard retrieval setting. We prepend the retrieved context (_i.e.,_ "Retrieval w/ Ref. Context") to in-file context to construct the prompt for this setting.

Note that the Retrieval w/ Ref. context could not be applied to the realistic code completion, as the reference completion is unknown. We use it as an estimation of the upper bound model performance with the RG framework. Also, the model's performance in this setting is not optimal, as it can still be limited by imperfect retrieval and the model's capability in making use of retrieved code, and we perform additional benchmarking and analysis on retrieval methods later in SS3.5.

### Results

We present results in Table 2 and additional results in Table 7. We see that all models perform poorly when the prompt includes only the in-file context. For example, the best-performing StarCoder model at 15.5B size only reports 8.82% code exact match in Python. Even a large code LM struggles to achieve promising performance in completing CrossCodeEval samples with only in-file context since it could not provide sufficient clues for code completion. This shows the design of CrossCodeEval that cross-file context is necessary to complete the code correctly.

The performance improves dramatically when the cross-file context is added to the prompts across all models and sizes. Figure 4 shows the significant improvements resulting from the inclusion of cross-file context in CodeGen and StarCoder models. Looking at Table 2, we see that the StarCoder model reports up to 3.0\(\) and 4.5\(\) better exact code match when including retrieved and retrieved with reference context respectively. The results underline the limitation of existing datasets that only consider the in-file context to evaluate code LMs, making these datasets insufficient in reflecting models' best capacity in real-world scenarios. In contrast, CrossCodeEval maintains the cross-filecontexts for code completion samples, providing resources to both identify the model's best capacity and analyze the model's behavior when seeing a more comprehensive context.

### Analysis and Discussions

Improved vs. Degraded Code CompletionsTable 3 presents changes in the number of correct completions (based on exact match to the references) across different prompt settings. The results suggest that all models follow a trend that the performance improves with better cross-file context (In-file \(\) Retrieval \(\) Retrieval w/ Ref.). However, the variation of correct/incorrect generation is significant; for example, when changing from Retrieval to Retrieval w/ Ref. with StarCoder in CrossCodeEval Python, we see 327 correct generations changed to incorrect, and 468 generations changed the other way around. Upon manual inspections, we see that the retrieval of the correct cross-file context plays a huge role, as the quality of the retrieval directly correlates with whether the model is able to generate correctly. This effect is further enhanced by the fact that the retrieval happens in fixed lines of code that do not often follow code structure, making it difficult for the model to digest, especially at zero-shot settings, echoing results from Zhang et al. (2023). This highlights that the current best models are still imperfect in leveraging extensive context to make better code completion. Further, it calls for additional studies in optimizing the retrieval methods for code: we show benchmarking with various retrieval methods later in the section.

    &  \\   &  &  &  &  \\   & EM & ES & EM & ES & EM & ES & EM & ES \\  CodeGen25-7B & 7.73 & 59.34 & 10.43 & 62.05 & 7.81 & 57.56 & 4.36 & 58.99 \\ + Retrieval & 14.52 & 64.40 & 16.88 & 64.35 & 12.57 & 60.08 & 13.01 & 63.86 \\ + Retrieval w/ Ref. & 19.17 & 67.46 & 20.20 & 66.17 & 15.35 & 62.73 & 17.87 & 66.14 \\  StarCoder-15.5B & 8.82 & 61.08 & 9.96 & 63.25 & 6.35 & 51.22 & 4.47 & 59.80 \\ + Retrieval & 15.72 & 66.28 & 17.48 & 66.10 & 8.31 & 44.87 & 13.57 & 65.00 \\ + Retrieval w/ Ref. & 21.01 & 68.66 & 19.92 & 67.75 & 11.02 & 46.67 & 20.08 & 67.97 \\  GPT-3.5-turbo & 4.88 & 52.58 & 12.30 & 63.52 & 6.38 & 53.78 & 3.56 & 56.48 \\ + Retrieval & 10.77 & 54.92 & 19.12 & 65.61 & 10.94 & 55.83 & 11.82 & 62.40 \\ + Retrieval w/ Ref. & 15.72 & 58.88 & 22.72 & 68.50 & 14.15 & 58.40 & 17.65 & 66.07 \\    
    &  \\   &  &  &  &  \\   & EM & F1 & EM & F1 & EM & F1 & EM & F1 \\  CodeGen25-7B & 14.26 & 46.02 & 16.60 & 51.43 & 12.46 & 47.75 & 7.69 & 33.81 \\ + Retrieval & 22.96 & 53.68 & 24.03 & 55.48 & 17.85 & 51.27 & 17.36 & 43.56 \\ + Retrieval w/ Ref. & 28.33 & 57.95 & 27.91 & 57.87 & 21.51 & 55.38 & 21.78 & 47.63 \\  StarCoder-15.5B & 15.72 & 48.16 & 18.28 & 53.23 & 11.86 & 43.53 & 8.54 & 34.33 \\ + Retrieval & 24.77 & 55.57 & 25.95 & 57.74 & 14.09 & 39.50 & 18.04 & 44.38 \\ + Retrieval w/ Ref. & 30.24 & 59.46 & 29.73 & 60.47 & 17.55 & 42.18 & 24.38 & 49.09 \\  GPT-3.5-turbo & 10.09 & 39.18 & 18.93 & 52.52 & 10.76 & 44.78 & 5.77 & 30.25 \\ + Retrieval & 17.37 & 44.43 & 26.74 & 56.57 & 16.69 & 48.15 & 15.44 & 41.24 \\ + Retrieval w/ Ref. & 23.49 & 50.14 & 31.79 & 60.52 & 20.65 & 51.54 & 21.72 & 47.21 \\   

Table 2: Performance of various code LMs on CrossCodeEval..“Retrieval” and “Retrieval w/ Ref.” mean we construct the prompt by prepending the retrieved cross-file context retrieved with the prompt and the prompt + reference (see §3.3 for details). The performance with no cross-file context (first row in each section) is generally poor. When prompts are augmented with cross-file context (middle row in each section), the performance increases significantly. The use of reference completion in formulating the query for cross-file context retrieval (last row in each section) shows the upper bound of the retrieve-and-generate (RG) approach. Results of other models are in Table 7.

Scalability of Model PerformanceFigure 4 visualizes how the performance of CodeGen and StarCoder scales w.r.t. model sizes. We see the performance increases following the power law in all settings as expected (Kaplan et al., 2020). However, again the performance is far from perfect even with the best performing model with the best context retrieved.

Locations of Retrieved Cross-file ContextTo identify relevant cross-file context, we retrieve code snippets from other files of the repository. To understand which files contribute to the cross-file context, we further conduct a study on the retrieved code snippets. To analyze the code snippets retrieved for each prompt, we examine the files to determine whether they meet the following criteria: (1) they are imported by the target file, (2) they are located in the same directory as the target file, (3) they have a similar name to the target file (with filename sharing at least one token, assuming snake-case or CamelCase style filenames), and (4) they include at least one API import within the project, similar to the target file. Our analysis shows that most of the code snippets are sourced from files that are either from the same directory with the target file (Python: 49.0%, Java: 37.8%, TypeScript: 51.3%, C#: 51.7%), or have similar names (Python:33.4%, Java:44.5%, TypeScript: 24.9%, C#: 39%). We also observed that target files and cross-files often share at least one intra-project API import statement. This result aligns with the findings of Zhang et al. (2023).

Identifier Overlap with Retrieved Cross-file ContextIdentifiers are a significant part of programming language constructs that cover API mentions in a source code. Therefore, we examine the distribution of the retrieved cross-file contexts for examples in CrossCodeEval that include mentions of identifiers that are also present in the references. In Figure 5, we show the distribution and the identifier exact match performance achieved by the best performing code LM, StarCoder. In general, it is evident that an increased ratio of identifier overlap results in higher performance,

Figure 4: Performance of models in various sizes.

    &  &  \\   & **In-file** & \(\) & **Ret.** & \(\) &  **Ret.** \\ **w/ Ref** \\  & **In-file** & \(\) & **Ret.** & \(\) &  **Ret.** \\ **w/ Ref** \\  \\  CodeGen-350M & +72 & \(-68\\ +182\) & +186 & \(-166\\ +237\) & +257 & +75 & \(-71\\ +145\) & +149 & \(-135\\ +158\) & +172 \\  CodeGen-16.1B & +183 & \(-162\\ +311\) & +332 & \(-259\\ +371\) & +444 & +150 & \(-133\\ +233\) & +250 & \(-209\\ +251\) & +292 \\  CodeGen25-7B & +206 & \(-172\\ +353\) & +387 & \(-306\\ +430\) & +511 & +223 & \(-179\\ +317\) & +361 & \(-277\\ +348\) & +432 \\  StarCoder & +235 & \(-192\\ +376\) & +241 & \(-327\\ +468\) & +560 & +213 & \(-167\\ +328\) & +374 & \(-300\\ +352\) & +426 \\    &  &  \\   & **In-file** & \(\) & **Ret.** & \(\) &  **Ret.** \\ **w/ Ref** \\  & **In-file** & \(\) & **Ret.** & \(\) & 
 **Ret.** \\ **w/ Ref** \\  \\  CodeGen-350M & +93 & \(-85\\ +127\) & +135 & \(-128\\ +168\) & +175 & +16 & \(-16\\ +58\) & +58 & \(-51\\ +108\) & +115 \\  CodeGen-16.1B & +151 & \(-140\\ +226\) & +237 & \(-218\\ +289\) & +308 & +32 & \(-29\\ +95\) & +98 & \(-85\\ +138\) & +151 \\  CodeGen25-7B & +262 & \(-226\\ +386\) & +422 & \(-341\\ +434\) & +515 & +77 & \(-62\\ +215\) & +230 & \(-189\\ +275\) & +316 \\  StarCoder & +213 & \(-198\\ +264\) & +279 & \(-241\\ +338\) & +376 & +79 & \(-66\\ +227\) & +240 & \(-195\\ +310\) & +355 \\   

Table 3: The numbers of correct code completions using different code generation models on the CrossCodeEval benchmark. “In-file” refers to the prompts being constructed only with in-file context, and “Retrieval” and “Ret. w/ Ref” refer to the prompts being constructed with the retrieved contexts described in §3.3.

demonstrating a positive correlation. This calls for an investigation into retrieval techniques, with a particular emphasis on key terms like identifiers for cross-file context retrieval.

CrossCodeEval as Code Retrieval BenchmarkThe observations above (e.g., imperfect upper bound performance and identifier overlap) underscore the critical role of the code retrieval method. Given the strong dependency that the correct prediction requires an accurate retrieval of relevant cross-file context, we propose to use CrossCodeEval as a code retrieval benchmark. We perform experiments with different retrievers from sparse (BM25 as we used in the rest of the experiments) to neural (UniXCoder (Guo et al., 2022) and OpenAI embedding10). For UniXCoder, we use a max

    &  &  \\   & &  &  &  &  \\   & & EM & ES & EM & ES & EM & ES & EM & ES \\  CodeGen25-7B & - & 7.73 & 59.34 & 10.43 & 62.05 & 7.81 & 57.56 & 4.36 & 58.99 \\  + Retrieval & BM25 & 14.52 & 64.40 & 16.88 & 64.35 & 12.57 & 60.08 & 13.01 & 63.86 \\ + Retrieval w/ Ref. & BM25 & 19.17 & 67.46 & 20.20 & 66.17 & 15.35 & 62.73 & 17.87 & 66.14 \\ + Retrieval & UniXCoder & 13.73 & 64.21 & 15.61 & 63.67 & 12.10 & 59.82 & 12.39 & 63.82 \\ + Retrieval w/ Ref. & UniXCoder & 18.01 & 66.46 & 18.19 & 65.23 & 14.84 & 61.66 & 16.46 & 65.20 \\ + Retrieval & OpenAI ada & 14.82 & 65.00 & 17.77 & 64.48 & 12.75 & 60.02 & 14.71 & 65.35 \\ + Retrieval w/ Ref. & OpenAI ada & 18.39 & 66.80 & 20.94 & 66.27 & 15.58 & 62.65 & 20.43 & 68.65 \\  StarCoder-15.5B & - & 8.82 & 61.08 & 9.96 & 63.25 & 6.35 & 51.22 & 4.47 & 59.80 \\ + Retrieval & BM25 & 15.72 & 66.28 & 17.48 & 66.10 & 8.31 & 44.87 & 13.57 & 65.00 \\ + Retrieval w/ Ref. & BM25 & 21.01 & 68.66 & 19.92 & 67.75 & 11.02 & 46.67 & 20.08 & 67.97 \\ + Retrieval & UniXCoder & 15.87 & 66.07 & 16.83 & 66.09 & 7.87 & 44.67 & 11.93 & 63.90 \\ + Retrieval w/ Ref. & UniXCoder & 19.32 & 68.33 & 19.45 & 67.51 & 10.28 & 46.85 & 16.63 & 66.30 \\ + Retrieval & OpenAI ada & 16.47 & 66.72 & 17.53 & 65.98 & 8.43 & 45.08 & 15.39 & 66.21 \\ + Retrieval w/ Ref. & OpenAI ada & 20.53 & 68.50 & 21.69 & 68.11 & 11.83 & 47.31 & 23.49 & 70.56 \\   

Table 4: Evaluation results of various sparse and neural methods in retrieving cross-file context. Identifier Match results are in Table 9.

Figure 5: Distribution of the examples according to identifier overlap between the retrieved cross-file context and the reference completion. We also show the corresponding identifier exact match scores.

sequence length of 256 per 10-line chunk and for OpenAI embedding we use 8,000. We use the cosine similarity of the embedding of the prompt and the chunks to retrieve top 5 chunks.

Table 4 shows the results with these retrieval methods. On one hand, we see BM25 provides a strong baseline and, in most of the cases, can outperform UniXCoder-based retriever. On the other hand, retrieving with OpenAI's ada embedding is generally better than both BM25 and UniXCoder, especially for Java and C#. Nonetheless, the performance with the best performing retriever is still suboptimal (\(<20\) EM in all languages), calling for future development of better code retriever.

## 4 Related Works

The advent of code language models (LMs) (Feng et al., 2020; Ahmad et al., 2021; Wang et al., 2021; Guo et al., 2022) have bolstered the automation of software engineering applications. Among them, code completion has got the most attention, and as a result, generative AI powered by large language models for code (Chen et al., 2021; Xu et al., 2022; Wang and Komatsuzaki, 2021; Black et al., 2021, 2022; Nijkamp et al., 2023; Fried et al., 2023; Li et al., 2022; CodeGeeX, 2022; Allal et al., 2023; Li et al., 2023; Nijkamp et al., 2023a) has become a reality. Benchmark datasets have been playing a pivotal role in advancing the field of generative AI for code. A large pool of recent works (Chen et al., 2021; CodeGeeX, 2022; Austin et al., 2021; Athiwaratkun et al., 2023; Cassano et al., 2023; Hendrycks et al., 2021; Raychev et al., 2016; Lu et al., 2021; Allamanis and Sutton, 2013; Puri et al., 2021; Husain et al., 2019; Clement et al., 2021; Ding et al., 2023; Wang et al., 2023; Lu et al., 2022) developed benchmarks to facilitate the evaluation of code LMs. These benchmarks typically assess code completion ability given in-file context - code prompts containing code snippets from current files (where the user is writing code). Therefore, the capability of these code LMs to generate code that requires software repository-level context has been left unexplored until recently.

A few recent works proposed repository-level code generation frameworks and benchmarks (Shrivastava et al., 2023; Ding et al., 2022; Pei et al., 2023; Zhang et al., 2023). While these works share high-level insights with CrossCodeEval, highlighting the importance of cross-file context, their focus is mainly on proposing a new approach to incorporate such contexts, and datasets are collected to evaluate their own approaches rather than being carefully crafted as a benchmark to evaluate the code LMs in general. For example, Shrivastava et al. (2023) and Ding et al. (2022) only collect data for one single programming language, and Pei et al. (2023) narrows the completion scope to only function arguments. As a comparison, CrossCodeEval comprehensively includes four different programming languages (Python, Java, Typescript, and C#) and targets evaluating the general code completion capacity of code LMs rather than a specific type of application. RepoEvalZhang et al. (2023) is a concurrent work building repository-level code completion benchmark in Python, constructed from 16 GitHub repositories. These repositories are limited in a domain (mainly academia/research work), some of them overlap with popular code pre-training datasets (such as The Stack (Kocetikov et al., 2022)), and some are with non-permissive licenses. In contrast, CrossCodeEval is derived from a diverse pool of permissively licensed GitHub repositories in 4 popular languages (SS2.4). Furthermore, CrossCodeEval does not overlap with The Stack to avoid data leakage, minimizing potential memorization issues during evaluations.

## 5 Conclusion

We introduce CrossCodeEval, a diverse and multilingual benchmark for cross-file code completion. CrossCodeEval necessitates cross-file contextual understanding to complete the code accurately. We use a static-analysis-based method to identify cross-file context usages in code, and take steps to ensure the dataset is of high quality and has minimal data leakage with the pre-training dataset of popular code LMs. We experiment with popular code language models and results show that the inclusion of cross-file context significantly improves their accuracy in code completion, demonstrating that CrossCodeEval is an effective benchmark assessing cross-file code completion capabilities. Moreover, even the top-performing model with the best retrieval method still exhibits great room for improvement, highlighting the need for further advancements in leveraging extensive context for code completion and better code retriever. In both directions, CrossCodeEval stands as a pivotal benchmark. We envision CrossCodeEval could fill in the gap of evaluating code completion that requires cross-file context and promote future research in all dimensions in this direction.