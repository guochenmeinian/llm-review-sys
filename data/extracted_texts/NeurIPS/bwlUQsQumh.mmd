# A Non-parametric Direct Learning Approach to Heterogeneous Treatment Effect Estimation under Unmeasured Confounding

A Non-parametric Direct Learning Approach to Heterogeneous Treatment Effect Estimation under Unmeasured Confounding

Xinhai Zhang

xzhan222@binghamton.edu

&Xingye Qiao

xqiao@binghamton.edu

Department of Mathematics and Statistics, State University of New York at Binghamton, Binghamton, NY, 13902, USA.

Department of Mathematics and Statistics, State University of New York at Binghamton, Binghamton, NY, 13902, USA.

###### Abstract

In many social, behavioral, and biomedical sciences, treatment effect estimation is a crucial step in understanding the impact of an intervention, policy, or treatment. In recent years, an increasing emphasis has been placed on heterogeneity in treatment effects, leading to the development of various methods for estimating Conditional Average Treatment Effects (CATE). These approaches hinge on a crucial identifying condition of no unmeasured confounding, an assumption that is not always guaranteed in observational studies or randomized control trials with non-compliance. In this paper, we proposed a general framework for estimating CATE with a possible unmeasured confounder using Instrumental Variables. We also construct estimators that exhibit greater efficiency and robustness against various scenarios of model misspecification. The efficacy of the proposed framework is demonstrated through simulation studies and a real data example.

## 1 Introduction

In various domains, different subjects may exhibit different responses to the same set of treatments. The exploration of this heterogeneity in the effects resulting from exposure has gained substantial interest in recent years. For instance, inferring the heterogeneous effect of a medical treatment on clinical outcome can contribute to the development of personalized treatment (Cai et al., 2011). A similar concept has found application in personalized marketing as well (Chandra et al., 2022). The heterogeneity among subjects can be measured by the disparity in conditional mean outcomes given other covariates, typically referred to as the Conditional Average Treatment Effect (CATE). Another problem closely related to the heterogeneity in treatment effects is the optimal Individualized Treatment Regime (ITR), which is a decision rule that selects treatments for individuals to maximize the expected outcome.

There has been significant development in the literature regarding the estimation of CATE and the optimal ITR in the case of no unmeasured confounding. For example, Q-learning (Qian and Murphy, 2011) models the conditional mean outcome under each treatment separately and the estimated CATE is constructed using the difference between the estimated conditional mean outcomes. The success of this method relies on the correct specification of the outcome models. To address this issue, direct learning (DL) (Tian et al., 2014; Qi and Liu, 2018) and robust direct learning (RD) (Meng and Qiao, 2022) models the conditional contrast between treatments directly, which has been shown to be more robust to model misspecification. Another strand of work approaches with tree-based or forest-based methods. Hill (2011) and Green and Kern (2012) extended the Bayesian Additive Regression Tree (BART) method of Chipman et al. (2010) for estimating heterogeneous treatment effect. Athey and Imbens (2016) proposed Causal Trees with an "honest" splitting approach, wherein the partitioning is constructed in one sample, and the treatment effects within each node are estimated using another sample. This methodology is subsequently adopted in Causal Forest (Wager and Athey, 2018), which extends the random forest algorithm to estimate heterogeneous treatment effects. On the other hand, optimal ITR estimation aims to determine the optimal decision rule for treatment assignment based on subjects' covariates to maximize the mean outcome. A significant line of work in the field involves transforming ITR estimation into a classification problem through the use of Inverse Probability Weighting (IPW). Notable contributions include Outcome Weighted Learning (OWL) (Zhang et al., 2012; Zhao et al., 2012) and Residual Weighted Learning (RWL) (Zhou et al., 2017).

The aforementioned methods all rely on the key assumption of no unmeasured confounding to identify the heterogeneous treatment effect or the optimal ITR. However, this assumption is in most cases unverifiable (if not untrue) in observational studies or randomized controlled trials (RCT) with non-compliance. A well-known approach that takes into account the unmeasured confounding is the use of an instrumental variable (IV). A proper IV is usually a pre-treatment variable that is independent of any possible unmeasured confounder while correlated with the treatment. For example, in RCT with non-compliance, the random treatment assignment can be considered as an IV while the treatment received is considered the treatment variable. Here these two are clearly correlated since a subject will not receive the treatment if they are not assigned one, though the strength of the correlation may depend on other characteristics such as the education level of the subject.

There is a growing literature on estimating heterogeneous treatment effects or optimal ITR under unmeasured confounding using IV. Imbens and Angrist (1994) identified and estimated the so-called Local Average Treatment Effect (LATE), restricted to the subgroup of the always-compliant population, with the help of an IV. More recently, machine learning methods like Doubly Robust IV (Syrgkanis et al., 2019) and Generalized Random Forest (Athey et al., 2019) have shown their applicability and effectiveness in various settings including unmeasured confounding, particularly when used in conjunction with an IV. Wang and Tchetgen (2018) introduced two alternative assumptions on the unobserved confounders and the IV, which enable the identification of the Average Treatment Effect (ATE). They proposed an estimator that has the so-called multiply robustness property, which guarantees consistent estimate under three observed data models. These findings were incorporated into Cui and Tchetgen Tchetgen (2021) to obtain an optimal ITR estimation while accounting for unmeasured confounding. On the other hand, Frauen and Feuerriegel (2022) utilized these findings for CATE estimation.

In this paper, we propose a new framework for estimating CATE using IV when there exist unmeasured confounders. This framework can be viewed as an extension of the Direct Learning method under unconfoundedness to the case that allows the existence of unmeasured confounding. We call the proposed method Direct Learning using Instrumental Variables (IV-DL). The proposed framework is easy to implement under many flexible learning methods. Additionally, we introduce several efficient and robust estimators by residualizing the outcome. These estimators have been demonstrated to be robust to multiple model misspecification scenarios.

The rest of this paper is organized as follows. The notations and some related preliminaries are introduced in Section 2. The proposed framework IV-DL is formally introduced in Section 3. In Section 4 and 5, we proposed efficient and robust estimators. In Section 6, we conduct simulation studies and compare the performance with existing methods in the literature. A real data example is included in Section 7. Section 8 concludes the paper with a discussion on possible future work. Proofs and additional simulations are provided in the Appendix.

## 2 Notations and Preliminaries

Denote \(A=\{+1,-1\}\) as the binary treatment, and \(X^{p}\) the pre-treatment covariates. We adapt the potential outcome framework (Rubin, 1974) in causal inference and denote by \(Y(a)\) the potential outcome that the subject would have obtained if the received treatment was \(a\). The observed outcome is then given by \(Y=Y(A)=Y(1)[A=1]+Y(-1)[A=-1]\). Denote by \(U\) the unobserved confounder of the effect of \(A\) on \(Y\). Suppose we have access to a pre-treatment binary IV denoted by \(Z=\{+1,-1\}\). Then the complete data consists of independent and identically distributed copies of \((Y,X,A,U,Z)\), even though only copies of \((Y,X,A,Z)\) are observed.

Our goal is to estimate the Conditional Average Treatment Effect (CATE), defined as \((x)[Y(1)-Y(-1)|X=x]\). As mentioned in Section 1, most of the prior works are based on the core assumption of no unmeasured confounding:

**Assumption 1** (Unconfoundedness).: \(Y(a) A|X\) _for \(a= 1\)._

This assumption essentially implies that the observed covariates \(X\) would suffice to account for the confounding of the effect of \(A\) on \(Y\), thereby excluding the presence of \(U\). Under the above assumption of unconfoundedness, it can be easily verified that CATE is identified by \((x)=[Y|A=1,X=x]-[Y|A=-1,X=x]\). Q-learning (Qian and Murphy, 2011) models the two conditional mean outcomes separately and estimates the CATE by taking the difference between these estimates. Consequently, its effectiveness depends on correctly specifying the models for the conditional mean outcomes. Denote the propensity score for the treatment as \(_{A}(a,x)=[A=a|X=x]\) for \(a= 1\). Direct Learning (Qi and Liu, 2018; Tian et al., 2014) propose to directly model for the heterogeneous treatment effect, based on the observation that \((x)=[AY/_{A}(A,X)|X=x]\). In other words, one can obtain an estimate of CATE by regressing the modified outcome \(AY/_{A}(A,X)\) on \(X\). Robust Direct Learning (RD) Meng and Qiao (2022) further extends this framework by residualizing the outcome using an estimate of the main effect, which is the average of the two conditional mean outcomes. This method demonstrates double robustness in the sense that it yields consistent estimation of CATE if either the propensity score or the main effect is correctly specified. Despite the success in RCT or observational studies, all the methods mentioned above rely on the unconfoundedness Assumption 1. In the next section, we will introduce a general framework that directly models CATE using an IV approach when there exists unmeasured confounding.

## 3 Direct Learning with Instrumental Variable Approach

In this paper, we look beyond Assumption 1, and consider the existence of an unmeasured confounder \(U\). To establish the identification of CATE in this setting, we approach with the use of a proper IV. We will start with the following assumptions seen in Cui and Tchetgen Tchetgen (2021).

**Assumption 2**.: _This assumption consists of five parts as follows:_

* \(Y(z,a)(Z,A)|X,U\) _for_ \(z,a= 1\)_._
* \(Z A|X\)_._
* \(Z U|X\)_._
* \(Y(z,a)=Y(z^{},a)\) _for_ \(z,z^{},a= 1\)_._
* \(0<_{Z}(1,X)<1\) _almost surely, where_ \(_{Z}(z,x)=[Z=z|X=x]\) _for_ \(z= 1\)_._

Here, \(Y(z,a)\) represents the potential outcome that would be observed if a subject were exposed to treatment \(a\), and the IV takes a value of \(z\). Assumption 2.a rules out the existence of any other confounder, except for \(X\) and \(U\), for the joint effect of \(Z\) and \(A\) on the outcome \(Y\). However, this unconfoundedness is hidden from the data collected, since \(U\) is never observed. Assumptions 2.b-2.e provides us with a well-defined IV. Assumption 2.b requires a correlation between the IV and the treatment given observed covariates. In many applications, a strong correlation is often necessary to ensure accurate inference in the estimation process. Assumption 2.c guarantees that the causal effect of \(Z\) on \(Y\) is not confounded given \(X\); otherwise \(Z\) suffers the same issue as \(A\). Additionally, required by Assumption 2.d, the causal effect of \(Z\) on \(Y\) can only be mediated by the treatment \(A\). In light of this assumption, we omit the argument \(z\) in the potential outcome and denote the common value as \(Y(a)\). Assumption 2.e implies that each subject has a positive chance of having either value of the IV. An example of the relationships between variables that satisfy Assumption 2 is presented in a directed acyclic graph in Figure 1. In order to identify the CATE, we also need the following assumption on the unmeasured confounder.

**Assumption 3**.: _At least one of the following is true:_

* \([A|Z=1,X,U]-[A|Z=-1,X,U]=[A|Z=1,X]-[A |Z=-1,X]\)__
* \([Y(1)-Y(-1)|X,U]=[Y(1)-Y(-1)|X]\)__Assumption 3 states that, conditional on the measured covariates, either the additive effect of \(Z\) on \(A\) is independent of \(U\), or the additive effect of \(A\) on \(Y\) is independent of \(U\). Now, we finally have identification of the CATE.

**Proposition 1**.: _Under Assumptions 2-3, the CATE can be identified by_

\[(x) =[Y|Z=1,X=x]-[Y|Z=-1,X=x]}{[A =1|Z=1,X=x]-[A=1|Z=-1,X=x]}\] (1) \[=[(Z,x)}|X=x ],\] (2)

_where \(_{Z}(z,x)=[Z=z|X=x]\) and \((x)=[A=1|Z=1,X=x]-[A=1|Z=-1,X=x]\) for any \(x\)._

The first equality (1) was shown in Wang and Tchetgen Tchetgen (2018), which means that the CATE is identified by the conditional Wald estimand. Equation (2) reveals an interesting observation that we do not need the realized treatment \(A\) as long as we have \((x)\), which can be viewed as the conditional effect of the IV on the treatment given observed covariates. Hereafter, we denote the conditional means of \(Y\) and \(A\) by \(_{z}^{Y}(x)=[Y|Z=z,X=x]\) and \(_{z}^{A}(x)=[A|Z=z,X=x]\), respectively, for any \(z\{-1,+1\}\) and \(x\).

### Conditional Average Treatment Effect Estimation

In this section, we will introduce the IV-DL framework. Motivated by Equation (2), the next lemma offers a way to estimate \((x)\) using inverse propensity score of IV as weight.

**Lemma 1**.: _Under Assumptions 2-3,_

\[*{argmin}_{f}[(Z,X)} -f(X)^{2}].\]

Based on Lemma 1, we can adopt many existing regression methods to obtain an estimate on CATE by regressing the modified outcome on the covariates, weighted by the propensity score for \(Z\). Specifically, given the data \(\{y_{i},x_{i},a_{i},z_{i}\}_{i=1}^{n}\), an estimator \(_{Z}\) of the propensity score function and an estimator \(\) of the effect of \(Z\) on \(A\), the IV-DL estimate for \(\) is given by

\[(x)=*{argmin}_{f}_{i=1}^{n} _{Z}(z_{i},x_{i})}(y_{i}}{(x_{ i})}-f(x_{i}))^{2}+\|f\|_{},\]

where \(\) is a function space with norm \(\|\|_{}\), and \( 0\) is the tuning parameter for the regularization term \(\|f\|_{}\). To obtain \(_{Z}\), we can fit a logistic regression of \(Z\) on \(X\) or a non-parametric model such as random forest. Since \(\) is the treatment effect of \(Z\) on \(A\), it is noteworthy that, under Assumption 3, estimation of \(\) can be viewed as a CATE estimation problem with unconfoundedness. In this case, \(A\) may be viewed as a binary "outcome" and \(Z\) a binary "treatment". Thus, we can adopt many existing CATE estimation methods such as Q-learning, DL, and Causal Forest.

The proposed framework allows a variety of learning methods to model the treatment effect \((x)\). For example, under the linear model, we may model \(f(x)=^{T}\) where the regression coefficients

Figure 1: A directed acyclic graph with unmeasured confounding and an IV

are \(\) and \(_{i}(1,x_{i}^{T})^{T}\). Then IV-DL estimator for \(\) is

\[}=*{argmin}_{^{p+1}} {n}_{i=1}^{n}_{Z}(z_{i},x_{i})}y_{i}}{ (x_{i})}-_{i}^{T}^{2}\]

and the CATE \((x)\) is estimated by \((x)=^{T}}\).

In high dimensional setting where \(p\) is large, sparse regularization can be easily applied here because the optimization is essentially a weighted least square problem. For example, we can use Least Absolute Shrinkage and Selection Operator (LASSO) and the estimator of \(\) is given by

\[}^{lasso}=*{argmin}_{^{p +1}}_{i=1}^{n}_{Z}(z_{i},x_{i})} y_{i}}{(x_{i})}-_{i}^{T}^{ 2}+\|\|_{1},\]

where \(>0\) is the tuning parameter for the \(l_{1}\) penalty.

In practice, there is no guarantee that the true treatment effect follows a linear model. For a more complex model, we can adopt nonlinear methods such as Kernel Ridge Regression (KRR) and solve

\[*{argmin}_{^{n},_{0}} _{i=1}^{n}_{Z}(z_{i},x_{i})}y_{i}}{(x_{i})}-(_{i}^{T}+_{0})^{ 2}+^{T},\]

where \(_{i}\) is the \(i\)-th column of the kernel matrix \(=(K(x_{i},x_{j}))_{n n}\) and \(K(,)\) is a kernel function. KRR might be computationally expensive when dealing with large datasets. In such cases, other machine learning methods capable of solving a weighted least squares problem can be considered. Examples include local regression, regression trees, random forests, and neural networks.

### Optimal Individualized Treatment Regime Estimation

In some domains, the optimal Individualized Treatment Regime (ITR) can be of interest. The goal here is to find a mapping \(d:\) from a specific class \(\) to maximizes the expected outcome: \(d^{*}*{argmax}_{d}[Y(d(X))]\), where \(Y(d(X))\) is the potential outcome that the subject \(X\) obtained after receiving treatment \(d(X)\), and \([Y(d(X))]\) is also known as the Value of the regime \(d\).

ITR and CATE are closely related. For example, in the binary treatment setting, the CATE \(\) is the difference between two conditional mean outcomes. Assuming greater values of outcome is preferred, then the sign of \(\) will determine which treatment is optimal. It can be verified that \(d^{*}(x)=((x)).\) Therefore, we define the estimated optimal ITR using IV-DL as \((x)=((x)),\) where \((x)\) may be any CATE estimator introduced in the last subsection.

## 4 Efficient Estimators by Residualization

In the literature, considerable advancements have been made to enhance the efficiency and robustness of the CATE and optimal ITR estimation. To this end, residualization and augmentation are two common strategies. For example, in the IPW framework for optimal ITR estimation, Zhou et al. (2017) and Zhou and Kosorok (2017) proposed to replace the outcome by its residual \(Y-(x)\) in estimation of the optimal regime, where \((x)\) is an estimate of the weighted average of the conditional mean outcomes. For the estimation of CATE, Meng and Qiao (2022) residualized the outcome by an estimate of the average of conditional mean outcomes. Frauen and Feuerriegel (2022) proposed augmenting a preliminary estimate of CATE to enhance the robustness of the estimator.

In this section, we present the Robust Direct Learning using IV approach (IV-RDL), which involves residualizing the outcome in IV-DL to enhance both efficiency and robustness. We propose two ways of residualization, referred to as IV-RDL1 and IV-RDL2, respectively. They are shown to reduce the variance when estimating CATE. In Section 5, we show that they have robustness properties when confronted with model misspecification for nuisance variables.

### Residualization using a Function of Covariates

We first consider residualizing the outcome by a function of the observed covariates only. Ideally, we would like to find a function \(g:\) that can improve the efficiency of the estimation on CATE,while keeping it consistent. As shown in the following lemma, the consistency of the estimator is in fact preserved under a shift of \(Y\) by any function of the observed covariates.

**Lemma 2**.: _For any measurable \(g:\) and any probability distribution for \((Y,X,A,Z)\)_

\[*{argmin}_{f}[(Z,X)} (-f(X))^{2}]\]

Asymptotically, the variance of the estimator is related to the variance of the derivative of \([_{Z}(Z,X)]^{-1}[2(Y-g(X))Z^{-1}(X)-f(X)]^{2}\), the weighted loss for each individual. Hence, it is natural to choose \(g\) that minimize the variance of \([_{Z}(Z,X)]^{-1}[2(Y-g(X))Z^{-1}(X)-f(X)]\). See Appendix B for a more detailed discussion using the linear model as an example. The following theorem gives us the minimizer.

**Theorem 1**.: _Among all measurable \(g:\), the following function minimize the variance of \((Z,X)}(-f(X))\):_

\[g^{*}(x)[(Z,X)}X= x]=^{Y}(x)+_{-1}^{Y}(x)}{2}.\] (3)

There is an interesting interpretation of the optimal function \(g^{*}\), which equals the average of \(_{1}^{Y}(x)\) and \(_{-1}^{Y}(x)\). Recall that Eq. (1) states that CATE under unmeasured confounding is identified by the ratio of two contrasts, where the numerator happens to be \(_{1}^{Y}(x)-_{-1}^{Y}(x)\). The residualization strategy amounts to shifting the outcome \(Y\), and hence \(_{1}^{Y}(x)\) and \(_{-1}^{Y}(x)\) as well. Naturally, shifting both by their average will not affect their difference, but it will reduce the variance. A similar residualization was incorporated in RD under unconfoundedness (Meng and Qiao, 2022), where the goal was to learn the contrast between conditional mean outcomes given the two treatments.

In practice, \(g^{*}\) needs to be estimated before we can estimate the CATE. There are several approaches to obtain the estimate of \(g^{*}\), denoted by \(^{*}\). For example, we can take the average of estimated conditional mean outcomes, i.e., \(^{*}(x)=(_{1}^{Y}(x)+_{-1}^{Y}(x))/2\). One can also regress \(/(2_{Z}(Z,X))\) on \(X\), inspired by Eq. (3). Given \(^{*}(x)\), the IV-RDL1 estimator for \(\) is obtained by

\[_{g}(x_{i})=*{argmin}_{f}_{i =1}^{n}_{Z}(z_{i},x_{i})}(-^{*}(x_ {i}))z_{i}}{(x_{i})}-f(x_{i}))^{2}+\|f\|_{}.\]

In Section 5, we will show that this estimator is robust against misspecification of either \(g^{*}\) or \(_{Z}\), given that \(\) is correctly specified.

### Residualization using Covariates, Treatment, and IV

In this paper, we also consider an alternative way of residualizing the outcome by a function \(h:(,,)\). Like IV-RDL1, the optimal choice is the function that minimizes the variance while maintaining the consistency of CATE estimation. Among all functions that still convey consistent CATE estimation, the following three equivalent functions minimize the variance of \([_{Z}(Z,X)]^{-1}[2(Y-h(X,A,Z))Z^{-1}(X)-f(X)]\).

\[h_{1}^{*}(x,a,z) =_{1}^{Y}(x)+(x)a-_{1}^{A}(x)-z(x)/2\] \[h_{2}^{*}(x,a,z) =_{-1}^{Y}(x)+(x)a-_{-1}^{A}(x)-z(x)/2\] \[h_{3}^{*}(x,a,z) =m^{Y}(x)+(x)a-m^{A}(x)-z(x)/2\]

where \(m^{Y}(x)(_{1}^{Y}(x)+_{-1}^{Y}(x))/2\) and \(m^{A}(x)(_{1}^{A}(x)+_{-1}^{A}(x))/2\). The technical details are provided in the Appendix C. In practice, all these conditional means (\(_{-1}^{Y}\), \(_{1}^{Y}\), \(_{-1}^{A}\) and \(_{1}^{A}\)) need to be estimated, together with estimations of \(_{Z}\) and \(\). Additionally, we need to obtain a preliminary estimate of CATE. The IV-RDL2 estimator is constructed by,

\[_{h}(x_{i})=*{argmin}_{f}_{i =1}^{n}_{Z}(z_{i},x_{i})}(-^{*}(x_{ i},a_{i},z_{i}))z_{i}}{(x_{i})}-f(x_{i}))^{2}+\|f\|_{ },\]

where \(^{*}\) is an estimator for one of \(h_{1}^{*}\), \(h_{2}^{*}\) and \(h_{3}^{*}\).

Robustness Properties

In this section, we investigate the robustness properties of IV-RDL1 and IV-RDL2. We start with the following theorem to demonstrate the double robustness property of the IV-RDL1 that residualizes the outcome by using \(g(x)\).

**Theorem 2**.: _Suppose Assumption 2-3 holds, and we have a consistent estimator of \(\), denoted by \(\). Let \(_{Z}\) be a working model for \(_{Z}\), and \(\) be a working model for \(g^{*}\). Then we have_

\[*{argmin}_{f\{\}} [_{Z}(Z,X)}((X))Z}{(X)}-f(X))^{2}]\]

_if either \(_{Z}(z,x)=_{Z}(z,x)\) or \((x)=g^{*}_{1}(x)\) almost surely._

Theorem 2 indicates that we will have a doubly robust estimator for \(\) if either \(_{Z}\) or \(g^{*}\) is correctly specified when \(\) is known or correctly specified. However, the requirement of a consistent estimate of \(\) would not pose a significant issue in practical application, since it is essentially a CATE estimation problem under no unmeasured confounding. A consistent estimator for \(\) can be found by implementing any state-of-the-art CATE estimation method in the literature.

For the IV-RDL2, there are more nuisance variables that need to be estimated. The next theorem shows that IV-RDL2 is robust to various scenarios of misspecified nuisance variables.

**Theorem 3**.: _Suppose Assumption 2-3 holds. Let \(_{Z}\), \(\), \(_{1}^{Y}\), \(_{-1}^{Y}\), \(_{2}^{X}\), \(_{-1}^{A}\), \(^{Y}\), \(^{A}\) and \(\) be working models for \(_{Z}\), \(\), \(_{1}^{Y}\), \(_{-1}^{Y}\), \(_{Z}^{A}\), \(_{-1}^{A}\), \(m^{Y}\), \(m^{A}\) and \(\), respectively. Denote \(_{1}\), \(_{2}\) and \(_{3}\) as chosen augmentation formulated according to \(h^{*}_{1}\), \(h^{*}_{2}\) and \(h^{*}_{3}\) using working estimates. Then we have_

\[*{argmin}_{f\{\}} [_{Z}(Z,X)}((X,A,Z))Z}{(X)}-f(X))^{2}]\]

_if any one of the following condition is satisfied: (1) \(_{Z}=_{Z}\) and \(=\) almost surely, and \(\) can be any one of \(_{1}\), \(_{2}\) and \(_{3}\). (2) \(_{Z}=_{Z}\) and \(=\) almost surely, and \(\) can be any one of \(_{1}\), \(_{2}\) and \(_{3}\). (3) \(_{1}^{Y}=_{1}^{Y}\), \(_{1}^{a}=_{1}^{A}\) and \(=\) almost surely, and \(=_{1}\). (4) \(_{-1}^{Y}=_{-1}^{Y}\), \(_{-1}^{a}=_{-1}^{A}\) and \(=\) almost surely, and \(=_{2}\). (5) \(^{Y}=m^{Y}\), \(m^{A}=m^{A}\) and \(=\) almost surely, and \(=_{3}\). (6) \(^{Y}=m^{Y}\), \(m^{A}=m^{A}\) and \(=\) almost surely, and \(=_{3}\)._

Theorem 3 summarizes in total six cases of the minimal combination of correctly specified nuisance variables in order to have a consistent estimate of CATE. The three choices of residualization functions possess robustness against different scenarios. In the first two scenarios, obtaining a consistent estimate of CATE is guaranteed as long as we correctly specify \(_{Z}\) and either \(\) or \(\). This consistency holds irrespective of the choice of the three \(\) functions. In practice, the second scenario may be particularly accessible, especially when \(_{Z}\) is known. The other scenarios are less likely to be verified in practice and therefore requires more domain knowledge of the data structure. Specifically, scenarios (3)-(5) requires the corresponding set of conditional means to be correctly specified as well as the preliminary \(\). Lastly, in scenario (6), when \(\) and the averages of conditional means are correctly specified, the IV-RDL2 will also provide a consistent estimate of CATE.

While working on this paper, we encountered unpublished work by Frauen and Feuerriegel (2022) that is similar to our IV-RDL2 estimator. Inspired by Wang and Tchetgen Tchetgen (2018), Frauen and Feuerriegel introduced the MRIV framework, which is a two-step process. First, a preliminary estimator of CATE and nuisance estimators of \(\), \(_{Z}\), \(_{-1}^{Y}\) and \(_{-1}^{A}\) are obtained. Then, a pseudo-outcome is created by augmenting the preliminary CATE with the nuisance estimates, and the final CATE estimator is obtained by regressing the pseudo-outcome on the covariates. As shown in Wang and Tchetgen Tchetgen (2018), this estimator is robust against model misspecification of the nuisance variables in three of the six scenarios in Theorem 3 (scenarios (1), (2), and (4)). Our numerical studies have shown that our proposed IV-DL framework performs better than the MRIV method.

Simulation Study

In this section, we present the results of the simulation study conducted to assess the performance of the proposed IV-DL framework. We compared the proposed method with Bayesian additive regression trees (BART; Chipman et al., 2010), robust direct learning (RD; Meng and Qiao, 2022), causal forest with IV approach (CF; Athey et al., 2019), MRIV method (Frauen and Feuerriegel, 2022), and weighted learning with IV approach (IPW-MR; Cui and Tchetgen Tchetgen, 2021).

### Simulation Settings

We begin by introducing the data-generating mechanism. The covariates, denoted as \(X=(X_{1},X_{2},X_{3},X_{4},X_{5})\), were generated from uniform distribution with \(X_{i} Unif(-1,1)\) for \(i=1,,5\). We followed Cui and Tchetgen Tchetgen (2021) and generated the treatment \(A\) under logistic model with probability for success: \((A=1|X,Z,U)=\{2X_{1}+2.5Z-0.5U\}\), where the instrumental variable \(Z\) was a Bernoulli random variable with probability \(1/2\) and \(U\) was the unobserved confounder that followed Bridge distribution with parameter \(=1/2\). By the results from Wang and Louis (2003), the above usage of Bridge distribution will guarantee that the marginal distribution \(f(A|X,Z)\) can be modeled directly by logistic regression. In other words, there exists some vector \(\) such that \(logit\{(A=1|X,Z)\}=^{T}(1,X,)\).

The outcome \(Y\) was generated in two different settings corresponding to linear and non-linear models of the true CATE:

1. \(Y=h(X)+q(X)A+0.5U+\)
2. \(Y=h(X)+\{(q(X))-1\}A+U+\)

where the error term \(\) follows \(N(0,1)\). Functions \(h(X)\) and \(q(X)\) are defined as follows:

\[h(X)=0.5+0.5X_{1}+0.8X_{2}+0.3X_{3}-0.5X_{4}+0.7X_{5}\] \[q(X)=0.2-0.6X_{1}-0.8X_{2}.\]

In Setting 1, the true CATE is \(2q(x)\), which is linear in \(x\). In Setting 2, the true CATE is \(2(exp(q(x))-1)\), which is nonlinear. The sample size for each setting was 500 and the simulation was repeated 100 times. An independent sample of size 5000 was used to evaluate the performance of different methods. The proposed methods were implemented according to Sections 3 and 4 with \((X)\) estimated by causal forest ("grf" package) and the other nuisance variables estimated by random forest. For methods that require to estimate the same nuisance variable, they shared the same copies of nuisance estimates.

### Numerical Results

We compared all methods based on three performance metrics in the testing sample: the correct classification rate by the estimated ITR (AR); the value function evaluated at the estimated ITR (Value); the mean squared error of the estimated CATE (MSE). Table 1 reports the mean and standard error of these three evaluation metrics over 100 replications for different methods in the two settings.

   & & BART & RD & IPW-MR & CF & MRIV & IV-DL & IV-RDL1 & IV-RDL2 \\   & MSE & 121(3.4) & 97.6(2.9) & NA & 89.6(1.8) & 66.3(2.3) & 55.5(3.8) & **40.5(2.9)** & 42.5(3.3) \\
1 & AR & 66.3(0.7) & 71.4(0.5) & 84.1(0.7) & 79.1(0.7) & 78.3(0.6) & 81.4(1) & **84.6(0.8)** & 83.7(1) \\  & Value & 75.4(0.8) & 81.6(0.5) & 84.1(0.7) & 85.4(0.7) & 84.5(0.7) & 87.1(1) & **89.9(0.9)** & 88.9(1.1) \\   & MSE & 449(11.1) & 397(9.8) & NA & 149(2.9) & 150(5.6) & 164(8.9) & **140(7.7)** & 142(7.4) \\
2 & AR & 57.6(0.6) & 60.8(0.7) & 55.5(0.3) & 70.1(1) & 68.8(0.9) & 77.1(0.9) & **77.9(0.8)** & 77.2(0.9) \\  & Value & 53.1(1.6) & 61(1.6) & 81.6(0.2) & 83.5(1.2) & 81.6(1) & 89.9(1) & **90.6(0.9)** & 90(1) \\  

Table 1: Simulation results: mean\( 10^{-2}\)(SE\( 10^{-2}\)). IPW-MR: the multiply robust weighted learning; BART: Bayesian additive regression trees; RD: robust direct learning; CF: causal forest. The empirical maximum value is 0.998 for setting 1 and 1.01 for setting 2.

Among the methods implemented, BART and RD rely on the unconfoundedness assumption and therefore fail to identify CATE when there is unobserved confounding. Both IPW-MR and CF make use of the IV to take unmeasured confounding into account. IPW-MR had fine performances on estimating ITR and maximizing the value. However, it was not designed to estimate CATE. CF performs slightly worse than IPW-MR in terms of AR and value in Setting 1, despite offering a CATE estimation. Its performance is more competitive compared to IPW-MR in Setting 2. Our proposed methods showed superior performances on all the metrics. In particular, IV-RDL1, which residualized the outcome using averages of the estimated conditional means, outperformed all the methods in both settings. IV-RDL2 had a more complicated residualization, and achieved the second-best performance (but still fairly close to IV-RDL1). Even the unresidualized IV-DL performed better than other methods in most of the metrics. Additional simulation results on testing the robustness of the proposed framework is reported in Appendix D.

## 7 Data Analysis

In this section, following Angrist and Evans (1998), we study the causal effect of child-rearing on a mother's labor-force participation, using a sample of married mothers with two or more children from the U.S. 1980 census data (80PUMS). Assuming the sex of children is random, "first two children mixed sex or not" becomes a suitable instrumental variable for the causal effect of having a third child on a mother's labor force participation. Angrist and Evans showed that having a third child reduces women's labor force participation on average. Our goal is to investigate heterogeneity among families, offering personalized insights on the decision to have a third child and its impact on employment opportunities. We used a dataset of 478,005 subjects with at least two children. The outcome, \(Y\), represents whether the mother was employed in the year preceding the census. The treatment, \(A\), indicates whether the mother had three or more children at the census time, and the instrumental variable, \(Z\), indicates whether the first two children were of the same sex. We considered five covariates, \(X\): mother's age at first birth, age at census time, years of education, race, and the father's income.

We used the random forest algorithm for both the implementation of the proposed method and the estimation of the nuisance variables \(_{z}^{Y}\), \(_{z}^{A}\), \(\), and \(_{Z}\). The preliminary CATE estimator was formulated according to Eq. 1 with plug-in estimates on the conditional means. To identify subgroups with distinct treatment effects, we used the estimated CATE as the response to construct a regression tree, shown in Figure 2. The splits occurred at the mother's age at census (33), age at first birth (23), and father's income ($2.1k/year and $26k/year). By investigating the five subgroups (32%, 4%, 3%, 15%, and 47% of the sample), labeled as groups 1-5, we have made the following observations. First, older mothers are more likely to work after having a third child (subgroups 4 and 5 show a larger estimated treatment effect). Second, younger mothers with very low-income fathers (subgroup 3) tend to stay in the labor force after the third child. Lastly, younger mothers are more likely to stop working if their husband's income is between $2.1k and $26k/year (subgroups 1-3). Figure 3 displays the histogram of estimated CATE for the three majority groups (1, 4, and 5). The estimated CATE for group 1 is overall smaller than for groups 4 and 5. We also constructed 3-dimensional scatter plots based on the three splitting variables for a more detailed look at the heterogeneity (shown in Appendix E).

## 8 Conclusions

In this paper, we proposed a new framework to estimate CATE under unmeasured confounding by using an instrumental variable. Under the proposed framework, the estimation procedure boils down to solving a weighted least square problem, which can be tackled with any modern statistical or machine learning method. We also constructed two robust estimators by residualizing the outcome, which are shown to be more efficient and robust to model misspecification on nuisance variables. Numerical studies have shown very competitive performance for our proposed methods.

A potential extension of our work involves using IV to estimate treatment effects for multi-arm and continuous treatments, with the challenge lying in the generalization of Assumption 3. Another avenue is to incorporate deep neural networks to make use of their rich expressiveness for data distribution. However, the empirical performance and theoretical properties need to be formally studied. One notable limitation is the issue of extreme weights, which can arise during the estimation process and potentially lead to instability and biased results. Addressing this limitation is crucial for improving the reliability and accuracy of our method.