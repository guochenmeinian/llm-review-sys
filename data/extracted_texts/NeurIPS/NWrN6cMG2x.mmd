# Moment Matching Denoising Gibbs Sampling

Mingtian Zhang

Centre for Artificial Intelligence

University College London

m.zhang@cs.ucl.ac.uk

&Alex Hawkins-Hooker

Centre for Artificial Intelligence

University College London

a.hawkins-hooker@cs.ucl.ac.uk

&Brooks Paige

Centre for Artificial Intelligence

University College London

b.paige@ucl.ac.uk

&David Barber

Centre for Artificial Intelligence

University College London

david.barber@ucl.ac.uk

This work was partially done during an internship in Huawei Noah's Ark Lab.

###### Abstract

Energy-Based Models (EBMs) offer a versatile framework for modeling complex data distributions. However, training and sampling from EBMs continue to pose significant challenges. The widely-used Denoising Score Matching (DSM) method  for scalable EBM training suffers from inconsistency issues, causing the energy model to learn a 'noisy' data distribution. In this work, we propose an efficient sampling framework, (pseudo)-Gibbs sampling with moment matching, which enables effective sampling from the underlying clean model when given a 'noisy' model that has been well-trained via DSM. We explore the benefits of our approach compared to related methods and demonstrate how to scale the method to high-dimensional datasets.

## 1 Energy-Based Models

Energy-Based Models (EBMs) have attracted a lot of attention in the generative model literature . EBMs are a type of non-normalized probabilistic model that determines the probability density function without a known normalizing constant. For continuous data \(x\), the density function of an EBM is specified as \(q_{}(x)=(-f_{}(x))/Z()\) where the \(f_{}(x)\) is a nonlinear function with parameter \(\) and \(Z()=(-f_{}(x))\,x\) is the normalization constant that is independent of \(x\). The energy parameterization allows for greater flexibility in model parameterization and the ability to model a wider range of probability distributions. However, the lack of a known normalizing constant makes training these models challenging. We start by giving a brief introduction of how to estimate \(\) in EBMs and refer the reader to  for a detailed overview of different training techniques for continuous EBMs.

**Likelihood-based training:** A classic method to learn \(\) is to minimize the KL divergence between the data distribution \(p_{d}\) and the model density \(q_{}\), which is defined as

\[(p_{d}||q_{})- p_{d}(x) q_{}(x)\, x- p_{d}(x)f_{}(x)\,x- Z(),\] (1)

where we use \(\) to denote the equivalence up to a constant that is independent of \(\). The integration of \(p_{d}(x)\) can be approximated by Monte Carlo with the training dataset \(_{train}=\{x_{1},,x_{N}\} p_{d}(x)\); in this case, it is equivalent to the maximum likelihood estimate (MLE) . However, for EBMs, minimizing the KL divergence requires the estimation of \(Z()\), which is intractable for nonlinear \(f_{}(x)\) defined by a neural network. Various methods have been proposed to alleviate theintractability by introducing techniques like Markov chain Monte Carlo (MCMC) [13; 26; 8; 11] or adversarial training [18; 47; 6].

**Score-based training:** Alternatively,  proposes to minimize the Fisher divergence to learn \(\), which is defined as

\[(p_{d}||q_{})= p_{d}(x)||s_{p_{d}}(x)-s_{ q_{}}(x)||_{2}^{2}\,x,\] (2)

where we use \(s_{p}(x)\) to denote the score function of distribution \(p\): \(s_{p}(x)_{x} p(x)\). Under certain regularity conditions, the Fisher divergence is equivalent to the score-matching (SM) objective ,

\[(p_{d}||q_{}) p_{d}(x)(||s_{ q_{}}(x)||_{2}^{2}+2(_{x}s_{q_{}}(x)) )x,\] (3)

which does not require estimation of the intractable \(Z()\). However, this objective needs to calculate the Hessian trace \(_{x}s_{q_{}}(x)=-_{x}^{2}f_{}(x)\) in every gradient step during training, which is computationally expensive and does not scale to high dimensional data or requires approximation . In this paper, we will focus on another training method, denoising score matching , which overcomes the tractability and scalability issues mentioned above, and is introduced in the next section.

### Denoising Score Matching

For the target data density \(p_{d}(x)\), a noise distribution \(p(|x)=(x,^{2}I)\) is introduced to construct a noised data distribution \(_{d}()= p_{d}(x)p(|x)\,x\). Denoising score matching (DSM)  minimizes the Fisher divergence between the noised data distribution \(_{d}()\) and an energy-based model \(_{}()=(-f_{}())/Z()\), with

\[(_{d}||_{}) =()||s_{_{d}}()-s_{_{}}()||_{2}^{2}\,\] \[ p(|x)p_{d}(x)||_{ } p(|x)-s_{_{}}()||_{2}^{2}\, \,x\] \[ p(|x)p_{d}(x)\|-x}{^{2}}+s_{_{}}()\|_{2}^{2} \,x\;,\] (4)

where the last equation is due to \(_{} p(|x)\) being tractable for the Gaussian distribution \(p(|x)\).

Compared to the KL or SM objectives, the DSM objective is scalable and well-defined when the data distribution is singular2 and can alleviate the blindness problem of score matching [35; 45; 51]. On the other hand, there is a notable disadvantage associated with the DSM objective: for a fixed \(>0\), the DSM objective is not a consistent objective for learning the underlying data distribution \(p_{d}\) since \((_{d}||_{})=0_{d}= _{} p_{d}\). A common solution is to anneal \( 0\) during training. However, Equation 4 is not defined when \(=0\) since the division in Equation 4 will make \((-x)/^{2}\) unbounded, which results in an inconsistent objective. Annealing \(\) increases the variance of the training gradients [37; 42], which makes the optimization challenging in practice.

To overcome the challenges, we propose an alternative data generation scheme: we use DSM with a fixed \(>0\) to train a 'noisy' energy model and then construct a sampler which targets the underlying 'clean' model. Specifically, our contributions are summarized as follows:

* We demonstrate that for an EBM that learns a noisy data distribution, there exists a unique underlying clean model which recovers the true data distribution.
* We introduce a pseudo-Gibbs sampling scheme incorporating an analytical moment-matching approximation of the denoising distribution. This allows us to sample from the underlying clean model without requiring additional training.
* We illustrate how to scale our method for high-dimensional data and demonstrate the generation of high-quality images using only a single level of fixed noise. Furthermore, we showcase the application of our proposed method in multi-level noise scenarios, closely resembling a diffusion model.

## 2 Clean Model Identification

For a fixed \(>0\), DSM can only learn a 'noisy' data distribution \(_{}()\) even in the ideal case where the Fisher divergence is exactly minimized, since \((_{d}||_{^{*}})=0_{d}= _{} p_{d}\). In this case, the following theorem shows that there exists a 'clean' model that is implicitly defined that learns the true data distribution.

**Theorem 2.1** (Existence of the underlying clean model for optimal \(_{}()\)).: _When the Fisher divergence goes to 0, \((_{d}||_{})=0_{d}=_{}\), there exists an **unique** underlying clean model \(q(x)\) such that \(_{}()= q(x)p(|x)\,x\) and \(q(x)=p_{d}(x)\)._

See Appendix A.1 for proof. This theorem shows that despite training an EBM on noisy data, there is an implicit model within it that can recover the true data distribution. Therefore, instead of annealing the noise \( 0\) to recover the true data distribution, we will demonstrate how to directly sample from the implicitly-defined clean model given the noisy energy-based model in the next section.

We want to highlight that the 'perfect fit' assumption, i.e. achieving \((_{d}||_{})=0\), may not hold for a complex data distribution \(p_{d}\) or underpowered EBMs. Therefore, we provide general sufficient conditions for the existence of the clean model for an imperfect EBM in Appendix A.2.

### Gibbs Sampling with Gaussian Moment Matching

Given a well-trained noisy energy-based model \(_{}()=_{d}()\), the clean model has the form

\[q(x)= p(x|)_{}()d,\] (5)

where the denoising distribution can be written as \(p(x|) p(|x)q(x)\). We notice that, since the noise distribution \(p(|x)=(x,^{2}I)\) is known, a Gibbs sampling scheme can be constructed to sample from the underlying clean model if we know the denoising distribution \(p(x|)\), with

\[_{k-1} p(|x=x_{k-1}), x_{k} p(x|= _{k-1}),\] (6)

where the initial sample \(x_{0} p_{0}(x)\) can be drawn from a standard Gaussian \(p_{0}(x)=(0,I)\). However, as the denoising distribution \(p(x|)\) is usually intractable for complex \(p_{d}\), we propose an analytical Gaussian moment matching approximation of \(p(x|)\).

Denote the mean and covariance of \(p(x|)\) as

\[()= x_{p(x|)},()=  x^{2}_{p(x|)}- x_{p(x|)}^{2}.\] (7)

The classic Gaussian moment matching method  specifies a Gaussian approximation \(p(x|)((),())\), which matches the first and second moment of \(p(x|)\). When \(_{}=_{d}\), the first mean of the denoised distribution has a well-known analytical form [39; 2; 9; 29]

\[()=+^{2}s_{_{}}();\] (8)

we include the derivation in Appendix A.3. Using this identity, we can rewrite Equation 4 as

\[(_{d}||_{})} p(|x)p_{d}(x)\|x-()\|_{2}^{2}\,x,\] (9)

where we can see that the Fisher divergence only depends on \(()\). Since \((_{d}||_{})=0 q=p_{d}\) (Theorem 2.1), the function \(()\) fully characterizes the distribution \(q\). Therefore, \(()\) and \(p(|x)=(x,^{2}I)\) can provide sufficient information to determine \(p(x|) q(x)p(|x)\). As a consequence, the following theorem shows that the covariance function can also be analytically derived.

**Theorem 2.2** (Analytical Covariance Identity).: _Given a clean model \(q(x)\) such that \( q(x)p(|x)\,x=_{}()=_ {d}()\) with \(p(|x)=(x,^{2}I)\), the \(()\) and \(()\) of the \(p(x|) q(x)p(|x)\) has the following relations_

\[()=^{2}_{}()=^{4}_ {}^{2}_{}()+^{2}I.\] (10)

See Appendix A.3 for proof. This analytical covariance identity can be seen as a high-dimensional generalization of the 2nd-order Tweedie's Formula [9; 29]. Therefore, the analytical full-covariance moment matching approximation can be written as

\[p(x|)(+^{2}_{} _{}(),^{4}_{}^{2}_{ }()+^{2}I).\] (11)We want to highlight that since the Gaussian moment matching is only an approximation of \(p(x|)\), the sampling scheme in Equation 6 is a 'pseudo' Gibbs sampler unless the true \(p(x|)\) is also a Gaussian distribution3, which is not true for general non-Gaussian \(p_{d}\). However, since \(()\) and \(p(|x)=(x,^{2}I)\) are already sufficient to specify \(p(x|)\), it should be possible to derive expressions for higher-order moments which themselves involve only \(()\) and \(\); we leave this to future work. To our knowledge, the \(\)-conditioned full covariance Gaussian moment matching approximation to \(p(x|)\) has not been derived previously. In the next section, we briefly discuss the connections between our method and other related approaches.

### Connection to Covariance Learning Approaches

Bengio et al.  proposes to approximate the true posterior \(p(x|) p_{d}(x)p(|x)\) with a variational distribution \(q_{}(x|)\). The parameter \(\) is then learned by minimizing the joint KL divergence

\[(p(|x)p_{d}(x)||q_{}(x|))_{ d}())- p_{d}(x)p(|x) q_{}(x|) x,\] (12)

where \(_{d}()= p_{d}(x)p(|x)x\). The joint KL divergence in Equation 12 encourages \(q_{}(x|)\) to match the moments of the true posterior \(p(x|)\), and defines an upper bound of the marginal KL 

\[(p(|x)p_{d}(x)||q_{}(x|))_{ d}())(p_{d}(x)||q_{}(x)),\] (13)

where the model is implicitly defined as the marginal of the joint \(q_{}(x)= q_{}(x|)_{d}()\). When \(q_{}(x|)\) is a consistent estimator of \(p(x|)\), this asymptotic distribution of the Gibbs sampling will converge to the true data distribution \(p_{d}(x)\). For continuous data, the variational distribution is chosen as a Gaussian distribution \(q_{}(x|)=(_{}(),_{}( ))\), where the mean \(_{}()\) and the covariance \(_{}()\) are parameterized by neural networks. We note that the only difference between the KL and DSM objective (Equation 9) is that the KL objective additionally learns the covariance. We thus show that the optimal covariance under KL minimization is the proposed analytical covariance.

**Theorem 2.3** (Optimal Gaussian Approximation).: _Let \(p(|x)=(0,^{2}I)\) and assume Gaussian distribution \(q_{}(x|)=(_{q}(),_{q}())\), then the optimal \(q^{*}\) such that_

\[q^{*}=_{q}(p(|x)p_{d}(x)||q(x|) _{d}())\] (14)

_has the mean and covariance with the form_

\[_{q}^{*}()= x_{p(x|)},_{q}^{*}( )=^{2}_{}_{q}^{*}(),\] (15)

see Appendix A.4 for proof. Therefore, when the optimal mean function is learned \(_{}()=_{q}^{*}()\), the optimal \(()\) can be analytically derived, making the learning of \(_{}()\) redundant. In addition to the training inefficiency caused by more parameters, the amortized covariance network may suffer from poor generalization . Moreover, the KL objective is also not well-defined for learning data distributions which lie on a low-dimensional manifold, e.g. MNIST, see Section 4 for a detailed discussion. In this case, the learned \(_{}()\) may be a degenerate matrix, making the Gaussian density function \(q(x|)\) ill-defined  which impedes the training, see Figure 5 for an example.

Paper proposes a higher-order score-matching loss to simultaneously learn both the first order score \(_{}_{}()\) and the second order score \(_{}^{2}_{}()\). However, our findings indicate that the mean function \(()\) (or the first order score \(_{} p()\)) already contains all the moment information of the underlying true distribution \(p_{d}\), and the optimal moment can be derived using the mean function. Therefore, learning the second-order score is redundant and may lead to sub-optimal inference.

### Connection to Analytic DDPM

The recent paper Bao et al.  considers a constrained variational family \(q_{}(x|)=(_{}(),_{q}^{2}I)\) in the context of diffusion model and derive the optimal \(_{q}^{*}\) as

\[_{q}^{*2}=_{_{q}}(p(|x)p_{d}(x) \|q_{}(x|))_{d}())= (_{q(x|)}[x])_ {_{d}()},\] (16)which can also be rewritten using the score function

\[_{q}^{*2}=^{2}-^{4}/d\|s_{q_{0}}() \|_{2}^{2}_{_{d}()}.\] (17)

In Appendix B, we provide a detailed derivation to show how this approximation can be linked to our method using the Fisher information identity . This approximation has two potential limitations: first, compared to full covariance moment matching, the assumed isotropic covariance structure may be insufficiently flexible to capture the true posterior; second, the covariance is independent of \(\).

The second assumption only holds when \(()\) is a linear function of \(^{4}\) (e.g. when \(p_{d}(x)\) is Gaussian) and does not hold for other non-Gaussian \(p_{d}(x)\). Therefore, our \(\)-dependent full-covariance approximation offers a more versatile approximation family, which ultimately results in a more precise estimation. However, in certain applications such as accelerating the sampling procedure of a diffusion model , it is advantageous to use a \(\)-independent isotropic covariance due to its inexpensive estimation. On the other hand, our \(\)-dependent covariance necessitates the computation of the Hessian for each \(\), making it inefficient for high-dimensional data. In Section 3, we will explore approaches to mitigate this limitation.

### Posterior Approximation Comparison

We now consider a toy example to compare the three denoising posterior approximations discussed above. Let \(p_{d}(x)\) be a Mixture of Gaussians (MoG) \(p_{d}(x)=_{k=1}^{k=4}g_{k}(x)\) whose components \(g_{[1:4]}\) are 2D Gaussians with means \([-1,-1],[-1,1],,[1,-1]\) and isotropic covariance \(_{g}^{2}I\) with \(_{g}=0.2\). The noise distribution is \(p(|x)=(x,^{2}I)\) with \(=0.2\), so \(()= p(x)p(|x)\,x\) is an MoG with the same component means and diagonal covariance \((_{g}^{2}+^{2})I\); see Figure 0(a) for a visualization. In this case the true posterior \(p(x|)\) does not allow a tractable form. Fortunately, given a noisy sample \(^{}\) and an evaluation point \(x^{}\), we can evaluate the true density \(p(x|^{})\) using Bayes rule: \(p(x^{}|=^{})=p(^{}|x=x^{})p _{d}(x^{})/_{d}(^{})\). Figure 1 shows the true posteriors given four different \(^{}\) where we use grid data in \(x\)-space to visualize the density.

To train the model, we sample 10,000 data points from \(p_{d}\) as our training data. For the KL-trained Gibbs sampler described in Section 2.2, we use a network with 3 hidden layers with 400 hidden units, Swish activation  and output size 4 to generate both mean and log standard deviation of the Gaussian approximation. For the moment-matching Gibbs sampler (including both full and isotropic covariance), we use the same network architecture but with output size 1 to get the scalar energy and DSM as the training objective. Both networks are trained with batch size 100 and Adam  optimizer with learning rate \(1{}10^{-4}\) for 100 epochs. For the \(\)-independent isotropic covariance, we use the Monte Carlo approximation to estimate the variance  with 10000 samples from \(_{d}()\).

Figure 1: Figure (a) shows the clean data distribution \(p_{d}(x)\) and the corresponding noisy distribution \(_{d}()\). Figure (b) shows 4 conditioned samples in the noisy space. Figures (c, d, e) visualize the true posterior \(p(x|)\) (green) and three posterior approximations (orange). We find that only the proposed \(\)-dependent analytical full-covariance moment matching can capture the variance of the true posterior, whereas the other two methods underestimate the variance.

Figure 1 visualizes the approximations to the denoising posterior \(p(x|)\) estimated by each of the three methods described in the previous sections. We surprisingly find that although the KL objective in Equation 12 encourages \(q_{}(x|)\) to match the moments of \(p(x|)\), the learned covariance in Figure 0(e) still underestimates the variance of the posterior. This shows the redundancy of covariance learning can degrade the variational approximation performance. Additionally, the \(\)-independent covariance fails to account for the relative positions of \(^{}\) and lacks the ability to predict the posterior's elliptical shape due to its isotropic nature. In contrast, our \(\)-dependent full covariance approximation overcomes these limitations, enabling more accurate predictions that capture the intricate geometry of the posterior distribution.

We then use the estimated posterior to conduct (pseudo) Gibbs sampling to generate samples5. Specifically, we initialize the first sample \(x_{0}(0,0.1)\) and run one Markov Chain with 10,000 time steps to generate 10,000 samples. In addition to the mixture of Gaussian datasets, we also train and generate samples from the 2D Swiss roll and two-ring datasets. For numerical evaluation, we calculate the Maximum Mean Discrepancy (MMD)  between 10k samples generated by a single-chain Gibbs sampler and 10k samples from the training dataset respectively. The kernel insides MMD is a sum over 5 Gaussian kernels with bandwidth ranging over \([2^{-2},2^{-1},2^{0},2^{1},2^{2}]\). The MMD results (including both mean and std) are calculated using 5 random seeds. We find that Gibbs sampling with the proposed analytical full covariance achieves the best results; numerical results are in Table 1, with a visual comparison in Figure 2.

## 3 Scalable Implementations for Image Data

**Scalable Diagonal Hessian Approximation** As we discussed in Section 2.3, the proposed full covariance Gaussian approximation in Equation 10 requires calculating an \(D D\) Hessian \(_{}^{2}()\) for each \(\) with size \(D\), which brings both memory and computation difficulties for high-dimensional data. A naive diagonal Hessian method (only using the diagonal entries in the Hessian) will address the memory bottleneck but still needs \(D\) times backward passes for the exact computation of the diagonal term . In this paper, we use the following diagonal Hessian approximation ,

\[(H) 1/S_{s=1}^{S}v_{s} Hv_{s},\] (18)

where \(v_{s} p(v)\) is a Rademacher random variable with entries \( 1\) and \(\) denotes the element-wise product6. This estimator will converge to the exact Hessian diagonals when \(S\). The computation for each \(v_{s}\) can be computed by two forward-backward passes. It is worth emphasizing that our \(\)-dependent diagonal moment matching approach provides a comparable level of flexibility to the variational method proposed in  while eliminating the need for additional training of the

  Data & Learn diag. & Analytic iso. & Analytic full \\  MoGs & \(0.929 0.343\) & \(0.724 0.361\) & **0.305**\( 0.141\) \\ Rings & \(0.364 0.044\) & \(0.006 0.002\) & **0.005**\( 0.001\) \\ Roll & \(0.053 0.011\) & \(0.030 0.001\) & **0.016**\( 0.002\) \\  

Table 1: MMD evaluations of a single chain

Figure 2: Samples from a single chain Gibbs sampling

diagonal covariance. Furthermore, our method remains more flexible than the isotropic \(\)-independent moment matching method proposed by .

**Energy or Score Parameterization** For the full-covariance moment matching in Equation 10, we require \(_{}^{2}_{}()\) to be symmetric to obtain a valid Gaussian approximation. However, if we learn the score function \(_{} p()=s_{}()\) using a network \(s_{}():^{D}^{D}\), its Jacobian is not guaranteed to be symmetric. In this case, we follow  and directly parameterize the density function \(_{}()\) with a neural network \(f_{}():^{D}\) and let the score function \(_{}_{}()=-_{x}f_{}( )\). This can be obtained by AutoDiff packages like PyTorch , and this parameterization guarantees \(_{}^{2}f_{}()\) to be symmetric. We also notice that when using the diagonal Hessian approximation (Equation 18), we only need entries in \((H)\) to be positive in order to obtain a valid Gaussian approximation. In this case, the score parameterization remains applicable and offers more efficient training compared to the energy parameterization. Therefore, the combination of full/diagonal covariance and energy/score parameterization provides a tradeoff between flexibility and inference speed, allowing for a flexible approach while maintaining computational efficiency during training.

## 4 Image Generation with a Single Noise Level

We then apply the proposed method to model the grey-scale MNIST  dataset. We use the standard U-Net architecture [35; 31] with a single fixed noise level \(=0.5\); the effect of varying \(\) is explored in Appendix C.1. For the KL training objective, the output channel size is 2 to generate both mean and log-std at the same time. For DSM training, we take the sum of the U-Net output to obtain the scalar energy evaluation which also relates to the product-of-experts model described in . We train both networks for 300 epochs with learning rate \(1{}10^{-4}\) and batch-size 100.

Figure 3: Figures (a,b,c) show the MNIST experiment comparisons, where we compare samples generated by pseudo-Gibbs sampling with three different \(q(x|)\). We plot samples from 25 independent Markov Chains with \(t\{0,1,5,10,20\}\) time steps. We can find the samples generated by the proposed analytical covariance moment matching with diagonal approximation achieved the best sample quality.

As discussed in Section 1.1, the KL divergence is not well-defined for manifold data distributions. This limitation becomes evident when working with MNIST, where the presence of constant black pixels in the boundary areas leads to a rapid decrease towards 0 in the variance of \(q_{}(x|)\) during training. Consequently, the likelihood value \( q_{}(x|)\) tends to approach infinity, resulting in unstable training. In contrast, the DSM objective is well-defined for manifold data, providing a stable training process even in the presence of such boundary effects. Figure 5 provides a visual comparison of the two training procedures, demonstrating the improved stability and effectiveness of the DSM objective in handling manifold data distributions.

For the sample generation process, calculating the full-covariance Gaussian posterior becomes challenging. We therefore apply the scalable diagonal Hessian approximation described in Section 3 to approximate the diagonal Gaussian covariance of \(p(x|)\). We find that the estimated diagonal Hessian occasionally contains small negative values due to approximation error; we, therefore, use the \((,)\) function with \(>0\) to ensure the positivity of the diagonal covariance. The dependent diagonal covariance share the same mean function.

We first visualize the covariance estimated by three different methods in Figure 4. We use 100 Rademacher samples in estimating the diagonal Hessian (Equation 18) and 50,000 samples in estimating the isotropic variance (Equation 17). We find that both \(\)-dependent diagonal covariance approximations can capture the posterior structure whereas the isotropic \(\)-independent covariance is just Gaussian noise since the variance is shared between different digit and pixel locations. In Figure 3, we plot the sample comparison for three methods.

Since the isotropic covariance has the same variance in each dimension, the generated samples in Figure 2(b) contain white noise in the black background, whereas the proposed full-covariance sampler

Figure 4: Figures (a,b,c) visualize the covariance approximations \(q(x|=x+),(0,^{2}I)\) on 25 \(\) samples. We use a sigmoid function to map the real value noise into grayscale pixels for the visualization.

Figure 5: Training loss comparison of two objectives. We plot the training loss every iteration during a total 300 epochs. The \(\)-independent isotropic covariance and the proposed \(\)-dependent diagonal covariance share the same mean function.

Figure 6: Visualizations of the diagonal covariance \(q(x|=x+)\) with different number of Rademacher samples.

Figure 7: Samples from three Markov chains. We plot the samples every 10 Gibbs steps.

can generate a clean black background in Figure 2(a). On the other hand, the samples generated by the KL-trained Gibbs sampler (Figure 2(c)) have worse sample quality due to the unstable training.

We then apply the same method to model the more complicated CIFAR 10  dataset. We use the same U-Net structure as used in  and directly parameterize the score function rather than the energy function to speed up the training. The noise level is fixed at \(0.3\). We train the model using Adam optimizer with learning rate \(1{}10^{-4}\) and batch size 100 for 1000 epochs. We visualize the denoising posterior diagonal covariance in Figure 6 when using different numbers of Rademacher samples (Equation 18). We observe that better covariance estimation can be obtained by increasing the number of samples. To balance efficiency and accuracy, we use a sample number of 10 in the subsequent Gibbs sampling stage. Figure 7 shows three independent Markov chains with the samples plotted every 10 Gibbs steps, which demonstrates that sharp images can be generated with even one fixed level of noise.

**Limitation:** In the CIFAR experiment, we observe a mode collapse phenomenon when running multiple independent Markov chains for a longer time. This phenomenon is likely due to the small noise level \(=0.3\), which prevents the sampler from exploring the full space, as commonly found with MCMC methods . This effect is visually represented in Figure 8, where we assess the Frechet Inception Distance (FID) values for 50,000 images sampled with varying numbers of Gibbs steps. Notably, the FID increases beyond 40 Gibbs steps, and visual evidence of mode collapse is observed (Figure 12). In the ensuing section, we will demonstrate the application of our method to settings with multiple noise levels, an approach that may help mitigate the issue of mode collapse.

## 5 Image Generation Using Multiple Noise Levels

The success of diffusion models and lessons from prior work on score-based generative models point to the importance of using multiple noise levels [14; 35] when modelling data with complex multi-modal distributions. Intuitively, by learning to denoise data at a range of noise levels, a single network can learn both the fine and global structure of the distribution, which in turn allows for more effective sampling algorithms capable of efficiently exploring diverse modes . We therefore propose to adapt the denoising Gibbs sampling procedure to sample from distributions corrupted with multiple noise levels. For this purpose, we use a noise-conditioned score network trained by Song and Ermon , who generated high-quality samples using a procedure inspired by annealed Langevin dynamics . This procedure involves generating samples from a sequence of distributions \(p_{T}(x_{T}),...,p_{0}(x_{0})\), corrupted by progressively decreasing levels of Gaussian noise (parameterized via standard deviations \(_{t}\), with \(_{T}>,,>_{0}\)). At a given step \(t\) in the sequence, Langevin dynamics is used to sample from the corresponding noised distribution \(p_{t}(x_{t})\), using the

Figure 8: FID evaluation with Increased Gibbs Steps. We can find the FID increases after 40 Gibbs steps.

score network \(s_{}(x_{t},_{t})\) to approximate the gradient of the noised distribution. The outputs of this Langevin dynamics run are then used to initialize the same procedure at the next noise level, leading the sampling procedure to converge gradually towards the data distribution as the noise level tends to zero (i.e. \(p_{0}(x_{0}) p_{d}(x),_{0} 0\)). The algorithm of the annealed Langevin dynamics with multi-level noise used in [35; 36] is summarized in Algorithm 1.

We show that the proposed Gibbs sampling scheme can be directly applied to a pre-trained score-based generative model as a drop-in replacement for Langevin dynamics MCMC in the generation stage. At each noise level, we use samples \(x_{t+1}\) from the previous noise level to initialise a Gibbs sampling chain targeting the marginal distribution at the current noise level \(p_{t}(x_{t})\), which now plays the role of the 'clean' distribution in Equation 5. Therefore, the noisy distribution at time step \(t\) is a Gaussian \(p(x_{t+1}|x_{t})=(x_{t},_{t+1}^{2}-_{t}^{2})\). The optimal denoising distribution \(q(x_{t}|x_{t+1})\) is thus a function of the level of noise at step \(t\) relative to the level of noise at the previous step \(t+1\). For generation efficiency, we employ 3 Gibbs steps at each noise level, using 3 Rademacher samples to approximate the diagonal Hessian. The sampling procedure is summarized in Algorithm 2. In Figure 9 and 10, we visualize the samples from models that are trained on CIFAR10 and CelebA separately. Further experimental details can be found in Appendix C.2.

For direct comparison with the results of  on CIFAR10, we retain the same schedule of noise levels used to generate samples with Langevin dynamics. We generate 50000 samples using this approach and report FID and Inception scores in Table 2. Our multi-level Gibbs sampling scheme produces samples of equivalent quality to the multi-level Langevin dynamics of , confirming its applicability to complex natural image data. The FID is also notably superior to that of the single-noise level Gibbs sampling, and the samples exhibit significant visual diversity (Figure 9). This underlines the importance of employing multi-level noise in our approach. Recent advances in sampling strategies for score-based models leveraging the framework of stochastic differential equations  have led to significant further improvements in generation quality as shown in Table 2; we leave the exploration of possible applications of our method to this framework to future work.

## 6 Conclusion

This paper focuses on addressing the inconsistency problem in training energy-based models (EBMs) using denoising score matching. Specifically, we identify the presence of an underlying clean model within a 'noisy' EBM and propose an efficient sampling scheme for the clean model. We demonstrate how this method can be effectively applied to high-dimensional data and showcase image generation results in both single and multi-level noise settings. More broadly, we hope our more accurate denoising posterior opens new avenues for future work on score-based methods in machine learning.

  Model & Inception & FID \\  NCSNV2 (Langevin ) & \(\) & \(10.87\) \\ NCSNV2 (Gibbs, Ours) & \(8.28 0.07\) & \(\) \\  DDPM  & \(9.46 0.11\) & 3.17 \\ NCSN++  & \(9.89\) & \(2.20\) \\  

Table 2: CIFAR10 Inception and FID Scores

Figure 9: CIFAR 10 Samples Figure 10: CelebA Samples