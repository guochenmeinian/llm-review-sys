# Advancing Cross-domain Discriminability in

Continual Learning of Vision-Language Models

 Yicheng Xu\({}^{1}\)1 Yuxin Chen\({}^{2}\)1 Jiahao Nie\({}^{3}\)

**Yusong Wang\({}^{1}\) Huiping Zhuang\({}^{4,5}\)2 Manabu Okumura\({}^{1}\)**

\({}^{1}\)Institute of Science Tokyo \({}^{2}\)University of California Berkeley

\({}^{3}\)Nanyang Technological University \({}^{4}\)South China University of Technology

\({}^{5}\)Greater Bay Area Institute for Innovation, Hunan University, China

yxu040@e.ntu.edu.sg, yuxinc@berkeley.edu

Equal contributionCorresponding author: hpzhuang@scut.edu.cn

###### Abstract

Continual learning (CL) with Vision-Language Models (VLMs) has overcome the constraints of traditional CL, which only focuses on previously encountered classes. During the CL of VLMs, we need not only to prevent the catastrophic forgetting on incrementally learned knowledge but also to preserve the zero-shot ability of VLMs. However, existing methods require additional reference datasets to maintain such zero-shot ability and rely on domain-identity hints to classify images across different domains. In this study, we propose **R**egression-based **A**nalytic **I**ncremental **L**earning (RAIL), which utilizes a recursive ridge regression-based adapter to learn from a sequence of domains in a non-forgetting manner and decouple the cross-domain correlations by projecting features to a higher-dimensional space. Cooperating with a training-free fusion module, RAIL absolutely preserves the VLM's zero-shot ability on unseen domains without any reference data. Additionally, we introduce **Cross**-domain **T**ask-**A**gnostic **I**ncremental **L**earning (X-TAIL) setting. In this setting, a CL learner is required to incrementally learn from multiple domains and classify test images from both seen and unseen domains without any domain-identity hint. We theoretically prove RAIL's absolute memorization on incrementally learned domains. Experiment results affirm RAIL's state-of-the-art performance in both X-TAIL and existing Multi-domain Task-Incremental Learning settings. The code is released at https://github.com/linghan1997/Regression-based-Analytic-Incremental-Learning.

## 1 Introduction

Continual learning (CL) [1; 2; 3] is a crucial area in machine learning, which requires a learner to incrementally learn new data instead of training from scratch. The main challenge in CL is known as catastrophic forgetting , where learning new knowledge results in the forgetting of the old one. To this end, various CL approaches [5; 6; 7; 8; 3; 9] have been proposed to solve the forgetting issue. As a typical CL setting, Class-Incremental Learning (CIL) (Fig. 1 (a)) aims to achieve robust discriminability on all seen classes. Despite the advancements, existing approaches mainly focus on classifying images only from seen classes, thereby limiting the model's generalizability.

Consequently, Zheng _et al._ proposed Multi-domain Task-Incremental Learning (MTIL), which cooperates CL with the zero-shot ability of Vision-Language Models (VLMs) [11; 12; 13; 14] such as CLIP . This integration equips models with the ability to classify domains they have notyet encountered, enhancing their generalizability across multiple domains (Fig. 1 (b)). Several methods [10; 15] have been specifically designed for MTIL, in which the model is required to retain both the incrementally learned knowledge during CL and the zero-shot ability of VLMs. However, these methods require a domain-identity hint to indicate the specific domain of the test image, which is often not applicable in real-world scenarios . Additionally, the use of reference datasets during training is necessary to maintain pre-trained VLMs' zero-shot performance.

To address the aforementioned limitations, we introduce **R**egression-based **A**nalytic **I**ncremental **L**earning (RAIL), a novel approach that incrementally learns new knowledge and performs effectively on both seen and unseen domains. Specifically, we leverage non-linear projection functions from both primal and dual perspectives to enhance the expressiveness of features extracted by the pre-trained CLIP. It endows the learner with the ability to classify images in a cross-domain label set without any domain-identity hint. In the incremental learning process, RAIL utilizes a ridge regression-based adapter that updates the parameters recursively. This is identical to learning on all encountered domains at once, achieving absolute memorization on learned domains. Additionally, we freeze the pre-trained CLIP and design a training-free fusion module to determine whether the test data belongs to seen or unseen domains. This strategy absolutely preserves CLIP's zero-shot ability on unseen domains, meeting practical requirements for models deployed in dynamic environments.

To demonstrate the effectiveness of our method, we propose **Cross**-domain **T**ask-**A**gnostic **I**ncremental **L**earning (X-TAIL) setting as illustrated in Fig. 1 (c). Particularly, X-TAIL requires CL methods to incrementally transfer a pre-trained VLM to multiple domains while evaluating the model's performance on both seen and unseen domains. Moreover, domain hints are forbidden in X-TAIL, making it more realistic and challenging . As a result, effective CL methods must classify a test image into the correct domain and class simultaneously.

Our contributions are summarized as follows:

* We propose a new CL method RAIL to incrementally adapt the pre-trained VLM to multiple domains without forgetting both pre-trained and incrementally learned knowledge.
* To meet the practical scenario where CL methods need to sequentially learn data from new domains and classify images across these domains, we propose a new setting X-TAIL to evaluate the preservation of VLM's zero-shot ability and the adaptability to new domains.
* We theoretically prove the RAIL's absolute memorization on incrementally learned domains and demonstrate that the zero-shot ability of the pre-trained VLM on unseen domains is absolutely preserved.
* We empirically show that the proposed method achieves state-of-the-art performances on both existing MTIL and the novel X-TAIL settings.

Figure 1: **Comparison of different CL settings. (a) In CIL, models classify images within all previously encountered classes. (b) In MTIL, models classify images from both seen and unseen domains based on the given domain-identities. (c) In X-TAIL, models classify images from both seen and unseen domains without any domain-identity hint.**

Related work

Early CL methods focused on Task-Incremental Learning (TIL) , where a task-id is given during testing. Subsequently, a more practical and challenging setting of Class-Incremental Learning (CIL)  was proposed, where the access to the task-id is forbidden at inference time. Methods for CIL must therefore distinguish between all classes encountered in learned tasks. More recently, Zheng _et al._ proposed Multi-Domain Task-Incremental Learning (MTIL), which is especially designed to evaluate CL methods with pre-trained VLMs. In MTIL, a pre-trained VLM continually adapts to multi-domain tasks. The performance on both seen and unseen tasks measure the retention of both incrementally acquired and pre-trained knowledge. However, it still requires the task-id to create specific domain label space at inference time. Apart from them, X-TAIL combines the challenges of both CIL and MTIL, in which the model learns new classes from various incoming domains and distinguishes between both seen and unseen classes without any domain-identity.

Prevailing continual learning methods include replay-based, distillation-based, regularization-based, and architecture-based approaches . Replay-based methods such as iCaRL  typically store a small portion of the previous task data as exemplars. The model is then trained jointly on new task data and the saved exemplars to preserve the previous knowledge. Distillation-based methods such as LwF  use either weight or function regularization to transfer knowledge from the previous model to the current model for knowledge distillation. Regularization-based methods such as ZSCL  penalize the shift of either model parameter or feature space by adding a regularization term to the cross-entropy loss function. To preserve the robustness of the strong pre-trained model without access to the pre-trained dataset, ZSCL utilizes large-scale reference datasets to regularize the parameter space. Architecture-based methods [17; 18; 19] expand the model by constructing task-specific parameters to avoid inter-task interference. For example, MoE-Adapters  cooperates the pre-trained CLIP with mixture of experts (MoE)  to learn from different domains. By leveraging a reference dataset to initialize a task-id indicator, it enables the model to distinguish unseen tasks from seen ones.

The aforementioned methods either neglect the forgetting issue of pre-trained knowledge or require multiple iterations and large-scale reference datasets for training, making it challenging to efficiently adapt to new data in continual learning scenarios. By contrast, RAIL employs an analytical solution that achieves the optimum in a single epoch without additional reference data, ensuring its efficiency.

## 3 Cross-domain task-agnostic incremental learning

### Problem setting

We define Cross-domain Task-Agnostic Incremental Learning (X-TAIL) as follows. Given a pre-trained VLM, the learner is required to incrementally transfer it to \(N\) different domains \(\{D^{(1)},D^{(2)},...,D^{(N)}\}\). Each domain \(D^{(n)}=\{(_{j}^{(n)},y_{j}^{(n)})\}_{j=1}^{|D^{(n)}|}\) is available only during the \(n\)-th learning step. The class labels \(y_{j}^{(n)} C_{}^{(n)}\) from the incrementally learned domain \(D^{(n)}\) are added to the set of seen class labels. During inference at all steps, the learner attempts to classify input images from any domain without the domain-identity hint. In other words, the ground-truth label of the test image belongs to \(C_{N}=C_{L} C_{U}\), where \(C_{L}=_{i=0}^{n}C_{}^{(i)}\) is the union of seen class labels from all previous learning steps and \(C_{U}\) is the set of unseen class labels.

Similar to the task-id in TIL, the domain-identity hint allows the learner to classify input data within the label space of a specific domain during evaluation. Essentially, the learner knows the domain of test images, which is far from real-world application scenarios. For instance, the learner is supposed to predict an image as the class of _husky_ from all possible classes \(C_{N}=\)_[car, bus,..., churros, donuts,..., husky, beagle, bulldog,...]_. However, if the domain-identity hint is given, the learner only needs to predict from a limited subset \(C_{}=\)_[husky, beagle, bulldog,...]_, which is simpler but less realistic compared to practical applications. Therefore, we extend our setting to a task-agnostic scenario. Specifically, the learner predicts images from \(C_{N}\), the union of any potential class labels, without any domain hint.

### Datasets

In X-TAIL's cross-domain setting, the learner should encompass as extensive data distributions as possible. Following previous work [10; 15; 21; 13; 22; 23], we select 10 different image-classification datasets from different domains for our setting: Aircraft , Caltech101 , DTD , EuroSAT , Flowers , Food , MNIST , OxfordPet , StanfordCars , and SUN397 . Specifically, to prevent the redundancy of learning overlapping classes and to maintain the integrity of the setting, CIFAR-100  was excluded because it includes many classes that overlap with those in other domains. As a result, CL methods under X-TAIL should discriminate images from a total of \(1,100\) classes across all domains.

### Evaluation metrics

We adopt the evaluation metrics from  for our setting. As illustrated in Fig. 2, each column represents the performance on a specific domain after each learning step, while the rows correspond to the learning sequence. In traditional CL settings, only the results in the lower diagonal where the learner has been exposed to the exemplars from the test domains are measured. Nevertheless, X-TAIL extends this evaluation to cover the entire matrix, recording performances across both learned and unlearned domains. The _"Average"_ metric, averaged on the orange blocks, indicates the average accuracy of all learning steps across all domains. The gray and green blocks under the diagonal show the classification performance on these domains after the model has learned these domains. Specifically, the green blocks represent the model's last performance on these domains after learning all domains. The _"Last"_ metric, which is the average of the green blocks, reflects the learner's adaptability to new domains. Additionally, the blue blocks in the upper-right matrix indicate the model's zero-shot performance on these domains before learning these domains. The average of these blocks, referred to as the _"Transfer"_ metric, measures the extent to which the zero-shot ability is preserved throughout incremental learning.

## 4 Approach

### Motivation

While CLIP demonstrates the generalizability of zero-shot classification, it still struggles in certain unfamiliar domains . Leveraging CLIP's robust feature extraction capabilities, linear probe offers a straightforward approach to transfer the CLIP to these domains . Among various linear solutions, Ridge Regression (RR) provides an effective classification strategy by mapping the image features onto one-hot-label targets . Given a pre-trained CLIP image encoder \(f_{I}\) and a dataset \(D=\{(,)\}\), where \(\) is the tensor of training images and \(\) is the matrix of corresponding one-hot labels, the predicted logits and the optimization problem are defined as:

\[}=_{e},*{arg\,min}_{ }\|-_{e}\|_{F}^{2}+ \|\|_{F}^{2},\] (1)

where \(_{e}=f_{I}()\) denotes the CLIP extracted features, \(\) is the classifier parameter, and \(\) is the regularization parameter.

In the context of X-TAIL, the classifier needs to distinguish a wide range of classes from different domains. However, the extracted CLIP features of images from different domains suffer from certain cross-domain correlations, leading to limited domain discriminability. Based on Cover's theorem , one promising approach [37; 38; 39] to enhance the linear separability of features is to project the features into a higher-dimensional space via some non-linear projection function. We explore this non-linear projection function from two perspectives:

**Primal form ridge regression.** Following , we use a Randomly-initialized Hidden Layer (RHL) to project raw features to a higher dimensional space. By explicitly defining the projection function

Figure 2: Metrics for X-TAIL setting.

as \(()\), the classifier parameter is determined as follows:

\[=(^{}+)^{-1} ^{},\] (2)

where \(=(_{e})\). In this way, \(()\) is fixed throughout the training process.

**Dual form ridge regression .** Instead of manually designing the projection function, we utilize the Kernel method  to implicitly define \(()\) based on the inner-product nature of dual form ridge regression. Depending on the choice of kernel function, this approach allows for an infinite projection dimension, which is unachievable through any explicit definition. The dual form solution is defined as:

\[=(+)^{-1},\] (3)

where \(=(,)\) denotes the covariance kernel matrix, and \((,)\) can be any positive-definite kernel function. The classification logits are derived as \(}=(f_{I}(_{}), )\). Throughout the paper, we use the Radial Basis Function (RBF) kernel  by default. The choice between primal and dual form ridge regression depends on whether the system is over-determined (more equations than unknowns) or under-determined (more unknowns than equations) . Details on the relationships between primal and dual ridge regression can be found in Appendix B.

To empirically verify whether these non-linear projections enhances the separability of CLIP features of images from different domains, we trained three types of classifiers (denoted as Linear, Primal and Dual, respectively) on 10 domains introduced in Sec. 3.2 jointly. To compare the standard linear regression form with aforementioned two approaches, we take the averaged weight vectors as the domain-prototypes and then calculate the inter-domain Pearson correlation coefficients (CCs) between 10 pairs of domain-prototypes. As shown in Fig. 4, the linear regression classifier exhibits high cross-domain correlations. By contrast, the RHL in the primal form significantly reduces these correlations. The implicit projection provided by the kernel trick in the dual form enables better disentangling of different domains. We further evaluate the in-domain accuracy, which represents the rate of correctly classifying images into the appropriate domains. Fig. 4 shows that the in-domain accuracy is negatively correlated to cross-domain correlations. Both primal and dual forms demonstrate certain improvements through the projection designs, allowing for accurate classification of images into their respective domains without domain identity hint.

### Regression-based analytic incremental learning

Based on the projection approaches introduced above, we propose the Regression-based Analytic Incremental Learning (RAIL) method, which incorporates a ridge regression-based adapter and a training-free fusion module. The adapter progressively adapts the pre-trained CLIP to new domains, while the training-free fusion module preserves CLIP's zero-shot ability on unseen domains. An overview of RAIL is illustrated in Fig. 5. The pseudo-codes of both training and testing algorithms are provided in Appendix A.

#### 4.2.1 RAIL-Adapter

In the context of CL, in which data arrives progressively, we extend both primal and dual ridge regression solutions to an incremental learning manner. Our solutions are identical to that obtained by joint training, which achieves absolute non-forgetting of learned knowledge. Let \(D^{(n)}=\{^{(n)},^{(n)}\}\) represent the \(n\)-th training set and \(D^{(1:n)}=\{^{(1:n)},^{(1:n)}\}\) represent the union of the training setsfrom the first \(n\) domains. At the \(n\)-th learning step, the optimization target for the joint training is expressed as

\[*{arg\,min}_{^{(n)}}\|^{(1:n)}-^{(1:n)}^{(n)}\|_{F}^{2}+\|^{(n)} \|_{F}^{2},\] (4)

where \(^{(1:n)}=(f_{I}(^{(1:n)}))\). The objective is to obtain \(^{(n)}\) that satisfies Eqn. 4 without accessing data from the previous \(n-1\) domains. For primal form, we propose to solve \(^{(n)}\) recursively using \(^{(n-1)}\) and a memory matrix \(_{p}^{(n)}\). The solution is summarized as in Theorem 1.

**Theorem 1**: _The parameter calculated by_

\[^{(n)}=[^{(n-1)}-_{p}^{(n)}^{ (n)}^{(n)}^{(n-1)}_{p}^{(n)} ^{(n)}^{(n)}]\] (5)

_is an optimal solution to the optimization problem of joint training on all \(n\) domains in Eqn. 4, where \(_{p}^{(n)}\) is obtained by_

\[_{p}^{(n)}=_{p}^{(n-1)}-_{p}^{(n-1)}^{(n)}(+^{(n)}_{p}^{(n-1)}^{(n)})^{-1}^{(n)}_{p}^{(n-1)}.\] (6)

Similarly, the dual parameter \(^{(n)}\) satisfying Eqn. 4 can be obtained based on \(^{(n-1)}\), an updating kernel \(^{(n)}\), and a memory matrix \(_{d}^{(n)}\). We denote the matrix \(^{(n)}\) as the concatenated one-hot label matrices of all \(n\) domains. The solution is defined in Theorem 2.

**Theorem 2**: _The parameter calculated by_

\[^{(n)}=(^{(n)}+)^{-1 }^{(n)}\] (7)

_is an optimal solution to the optimization problem of joint training on all \(n\) domains in Eqn. 4, where_

\[^{(n)}=^{(n-1)}&(_{e}^{(n)},_{d}^{(n-1)})^{}\\ (_{e}^{(n)},_{d}^{(n-1)})&(_{e}^{(n)},_{e}^{(n)}), ^{(n)}=^{(n-1)}&\\ &^{(n)},\] (8)

Figure 5: **RAIL Overview**. (a) During inference, the fusion module utilizes the Zero-shot logits to identify whether a test image is aligned with seen or unseen classes. If classified as a seen class, the Fusion logits combine the RAIL-Adapter logits and the Zero-shot logits; otherwise solely rely on the Zero-shot logits. (b) Primal: at the \(n\)-th learning step, features \(_{e}\) extracted by CLIP’s image encoder are projected to higher dimensional \(\) via RHL and then update the parameter \(\) and memory \(_{p}\) by Theorem 1. (c) Dual: features extracted by CLIP’s image encoder update the kernel \(\), parameter \(\), and memory \(_{d}\) by Theorem 2.

_and the memory matrix is given by \(_{d}^{(n)}=[_{d}^{(n-1)}_{e}^{(n) }]^{}\)._

At each incremental learning step, the kernel matrix \(\) updates recursively along the main diagonal, preserving the correlations among class-prototypes from all domains. During testing, the kernel covariance between the feature of test image extracted by CLIP and memory matrix \(_{d}\) is calculated to obtain the classification logits \(}=(f_{I}(_{}), _{d})\). Specifically, \(_{d}\) dynamically updates according to the data stream via concatenated class-prototypes. These class-prototypes can be the feature embeddings \(_{e}\) extracted by CLIP, \(\)-means centroids, or Gaussian Mixture Model means. We use raw feature embeddings \(_{e}\) by default, which is sufficient to validate our method. The complete proofs for both theorems are provided in Appendix C.

#### 4.2.2 RAIL-Fusion

Next, we introduce the fusion strategy, which leverages the refined knowledge on seen domains from the RAIL-Adapter while preserving CLIP's pre-trained knowledge on unseen domains. To distinguish data from different domains without any domain-identity hint, a common approach  is to compute domain centers from class-prototypes. The test image is first assigned to a specific domain based on the distances to these domain centers and then classified by a domain-specific classifier. However, this method fails when statistics for unseen domains are unavailable.

An alternative solution is to leverage CLIP's zero-shot ability to indicate the domain of the test image. Despite CLIP's strong generalization ability across domains, certain cross-domain errors (_i.e.,_ misclassification into an incorrect domain) persist and do not diminish during incremental learning process. Therefore, the task of classifying images across seen domains is delegated to the RAIL-Adapter, which significantly reduces these errors, as discussed in Sec. 4.1. Consequently, CLIP's zero-shot ability is only leveraged to distinguish classes in unseen domains (_i.e.,_ Out-Of-Distribution or OOD) from those in seen ones (_i.e.,_ In-Distribution or ID) to maintain its performance on unseen domains. We summarize this approach as RAIL-Fusion, which combines both CLIP's zero-shot logits and RAIL-Adapter logits for prediction, regardless of whether the domain of the test image is seen or unseen.

Specifically, CLIP first makes a rough prediction based on its zero-shot logits, _i.e.,_ the similarity scores between image embeddings and language embeddings from the cross-domain label set \(C_{N}\):

\[}_{}=(f_{I}(_{})f_{T}(([P,C_{N}]))^{} ),\] (9)

where \(}_{}\) represents the zero-shot logits, \(P\) denotes the pre-defined prompt template, and \(f_{T}\) and \(f_{I}\) are the CLIP text encoder and image encoder, respectively. The result determines whether the test image aligns with the seen classes (ID) that have been encountered during the incremental learning or with the unseen classes (OOD). If classified as ID, the RAIL-adapter refines the rough prediction using its incrementally learned knowledge. If classified as OOD, the rough prediction is taken as the final prediction, fully relying on CLIP's zero-shot ability. Notably, our fusion strategy guarantees that OOD images correctly classified by CLIP's zero-shot prediction will never be misclassified as ID, thereby _absolutely_ preserving CLIP's zero-shot ability on unseen domains. In addition, to prevent the forgetting of the pre-trained knowledge of CLIP on domains with good zero-shot performance, we combine the zero-shot logits and RAIL-Adapter logits with a weighted sum:

\[}_{}=(1-)}_{} +}_{},\] (10)

where \(}_{}\) denotes the RAIL-Adapter logits and \(\) is the fusion ratio that adjusts the influence of zero-shot prediction on seen domains. The ablation study on \(\) is presented in Appendix E.2.

## 5 Experiments

We evaluate RAIL method under both X-TAIL and MTIL settings, as mentioned in Sec. 3.2. The learning order is set alphabetically: Aircraft, Caltech101, DTD, EuroSAT, Flowers, Food, MNIST, OxfordPet, StanfordCars, and SUN397. Additional experiments with a random order are provided in Tab. 3 in Appendix G. To ensure compatibility with different domains, we follow the common practice of sampling a 16-shot training set for each domain, while using the original test set for evaluation . The implementation details are provided in Appendix D.

### Comparison results

Cross-domain task-agnostic incremental learning.The performances averaged on 10 domains of RAIL and other baseline methods in the X-TAIL setting are presented in the _Average_ column of Tab. 1. Specific performances on each domain are provided in the columns named after each respective domain. We evaluate RAIL in both primal and dual forms. _Zero-shot_ indicates the zero-shot performance of the pre-trained CLIP model on each domain. _Fine-tune_ denotes the performance of fine-tuning both CLIP image and text encoders with a joint dataset of all 10 domains, serving as a strong baseline for comparison.

Specifically, the primal-RAIL outperforms the previous best one with a \(6.4\%\) improvement in _"Transfer"_ accuracy, achieves an additional \(7.7\%\) in _"Average"_ accuracy, and gains an \(8.6\%\) improvement in _"Last"_ accuracy. The dual-RAIL further surpasses the primal one by \(1.2\%\) in _"Average"_ accuracy and \(3.3\%\) in _"Last"_ accuracy, while maintaining consistent _"Transfer"_ accuracy due to the same fusion strategy. These results indicate that RAIL has more stable transfer performance and is more robust to catastrophic forgetting, effectively preserving both knowledge from new domains and pre-trained knowledge. We repeat the experiments with a random order and present the results in Tab. 3. RAIL consistently outperforms the baselines, reaffirming the previous conclusions.

We illustrate how accuracy changes on several example domains in Fig. 6. We observe that the accuracy of RAIL remains consistent with the zero-shot results before learning the corresponding domain. Furthermore, RAIL exhibits strong cross-domain discriminative capabilities. For example, once DTD is learned, learning further new domains does not affect the accuracy on DTD. Accuracy on certain domains, like Caltech101, even improves due to the fusion module's ability to reduce OOD errors by learning from more domains.

Multi-domain task-incremental learning.We follow the setting in  to evaluate our methods on the few-shot MTIL, comparing it against the performance of baselines reported in . In this context, RAIL is reduced to the structure of multiple domain-specific classifiers trained on each domain separately. The domain-identity guides the test image to the corresponding classifier for within-domain prediction. The comparison results are shown in Tab. 2. RAIL consistently outperforms previous state-of-the-art approaches on all three metrics. These results demonstrate that the proposed non-linear projections significantly improve the separability of features extracted by CLIP. Consequently, the ridge regression-based classifier can effectively adapt the pre-trained model to new domains.

### Discussion

Regression targets.For VLMs, aside from using one-hot labels as the regression targets \(\) in Eqn. 4, the text embeddings generated from class labels is also a viable option . We compare the _"Last"_ accuracy of 10 domains using the dual RAIL-Adapter with these two different regression targets as shown in Fig. 6(a). The results indicate that training with one-hot labels surpasses its counterpart with text embeddings by an average of \(3.8\%\). We emphasize that using text embeddings as targets is suboptimal compared to uniformly distributed one-hot labels. This effect is particularly notable in domains such as Aircraft, where the _"Last"_ accuracy with one-hot label targets outperforms that with text embedding targets by \(7.5\%\). We argue that insufficiently semantic class names (_e.g._, "707-320") result in text embeddings that are not well-dispersed in the feature space, thus compromising the classification performance.

Figure 6: Accuracy (%) on five domains changes over all learning steps.

Fusion strategies.The CL setting associated with multiple domains is typically decomposed into two stages: domain-identity inference and in-domain prediction . As discussed in Sec. 4.2.2, an intuitive strategy for distinguishing classes from different domains in X-TAIL is to utilize CLIP's zero-shot prediction as a domain indicator, which then cooperates with multiple domain-specific classifiers to perform classification within each distinct domain. We evaluate the effectiveness of the RAIL-Fusion against this strategy by comparing the _"Last"_ accuracy across 10 domains using the dual RAIL-Adapter. The _"Transfer"_ accuracy remains consistent between these two strategies.

As shown in Fig. 6(b), the RAIL-Fusion strategy outperforms the multi-classifier approach in most domains by an average of \(7.5\%\). This improvement is due to RAIL-Fusion's design, which focuses on distinguishing unseen classes (OOD domain) from seen classes (ID domain), rather than identifying specific domains (Sec. 4.2.2). This OOD detection design incrementally reduces errors as the number

  

Figure 7: Each bar indicates the _“Last”_ accuracy (%) on each domain after the last learning step.

of ID domains grows, making it particularly effective in dynamically adapting to new domains. By contrast, using a domain indicator maintains a consistent level of cross-domain errors associated for each domain, leading to error propagation in the final prediction. This issue is especially critical for domains with low in-domain accuracy, where any misalignment between the domain indicator and the correct domain can significantly impact performance. These cross-domain errors are mitigated in the RAIL-Adapter thanks to its non-linear projection design.

## 6 Conclusion

In this work, we introduce the Cross-domain Task-Agnostic Incremental Learning (X-TAIL) to evaluate the preservation of pre-trained knowledge and cross-domain discriminative ability in a continual learning context. We introduce a novel CL approach, Regression-based Analytic Incremental Learning (RAIL), to improve the performance of pre-trained vision-language models on progressively incoming domains, while maintaining its zero-shot ability on unseen domains. We theoretically prove the absolute memorization on learned knowledge and show that the fusion module inherently avoids the forgetting of VLM's zero-shot ability. Comprehensive experiments on both existing and proposed settings empirically demonstrate the superiority of our method.

## 7 Acknowledgement

This research was supported by the National Natural Science Foundation of China (62306117) and the Guangzhou Basic and Applied Basic Research Foundation (2024A04J3681).

  
**Method** &  **Average** \\ **Average** \\  } &  **Average** \\ **Average** \\  } &  **Average** \\ **Average** \\  } &  **Average** \\ **Average** \\  } &  **Average** \\ **Average** \\  } &  **Average** \\ **Average** \\  } &  **Average** \\ **Average** \\  } \\  Zero-shot & 24.3 & 88.4 & 68.2 & 44.6 & 54.9 & 71.0 & 88.5 & 59.6 & 89.0 & 64.7 & 65.2 & 65.3 \\ Fine-tune & 30.6 & 93.5 & 76.8 & 65.1 & 91.7 & 92.9 & 83.3 & 96.6 & 84.9 & 65.4 & 71.3 & 77.5 \\ 
**Transfer** & & & & & & & & & & & & \\ LwF & – & 72.1 & 49.2 & 35.9 & 44.5 & 41.1 & 66.6 & 50.5 & 69.0 & 19.0 & 51.7 & 50.0 \\ LwF-VR & – & 82.2 & 62.5 & 40.1 & 40.1 & 56.3 & 80.0 & 60.9 & 77.6 & 40.5 & 60.8 & 60.1 \\ WiSE-FT & – & 77.6 & 60.0 & 41.3 & 39.4 & 53.0 & 76.6 & 58.1 & 75.5 & 37.3 & 58.2 & 57.7 \\ ZSCL & – & 84.0 & 68.1 & **44.8** & 46.8 & 63.6 & 84.9 & 61.4 & 81.4 & 55.5 & 62.2 & 65.3 \\ MoE & – & 87.9 & **68.2** & 44.1 & 48.1 & 64.7 & **88.8** & **69.0** & **89.1** & 64.5 & 65.1 & 68.9 \\ Primal-RAIL & – & **88.4** & **68.2** & 44.6 & **54.9** & **71.0** & 88.5 & 59.6 & 89.0 & **64.7** & **65.2** & **69.4** \\ Dual-RAIL & – & **88.4** & **68.2** & 44.6 & **54.9** & **71.0** & 88.5 & 59.6 & 89.0 & **64.7** & **65.2** & **69.4** \\ 
**Average** & & & & & & & & & & & & & \\ LwF & 23.5 & 77.4 & 43.5 & 41.7 & 43.5 & 52.2 & 54.6 & 63.4 & 68.0 & 21.3 & 52.6 & 49.2 \\ LwF-VR & 24.9 & 89.1 & 64.2 & 53.4 & 54.3 & 70.8 & 79.2 & 66.5 & 79.2 & 44.1 & 61.6 & 62.5 \\ WiSE-FT & 32.0 & 87.7 & 61.0 & 55.8 & 68.1 & 69.3 & 76.8 & 71.5 & 77.6 & 42.0 & 59.3 & 63.7 \\ ZSCL & 28.2 & 88.6 & 66.5 & 53.5 & 56.3 & 73.4 & 83.1 & 56.4 & 82.4 & 57.5 & 62.9 & 64.4 \\ MoE & 30.0 & 89.6 & **73.9** & 58.7 & 69.3 & 79.3 & 88.1 & **76.5** & 89.1 & 65.3 & **65.8** & 71.4 \\ Primal-RAIL & 32.9 & **94.5** & 69.9 & 58.1 & **71.8** & **84.4** & **88.5** & 70.4 & 89.0 & 66.1 & 65.7 & 71.9 \\ Dual-RAIL & **36.0** & 94.2 & 70.9 & **58.8** & **70.6** & 84.3 & **88.5** & 70.3 & **89.7** & **66.5** & **65.8** & **72.3** \\ 
**Last** & & & & & & & & & & & & & \\ LwF & 22.1 & 58.2 & 17.9 & 32.1 & 28.1 & 66.7 & 46.0 & 84.3 & 64.1 & 31.5 & 60.1 & 46.5 \\ LwF-VR & 22.9 & 89.9 & 59.3 & 57.1 & 57.6 & 79.2 & 78.3 & 77.7 & 83.6 & 60.1 & 69.8 & 66.9 \\ WiSE-FT & 30.8 & 88.9 & 59.6 & 60.3 & 80.9 & 81.7 & 77.1 & **94.9** & 83.2 & 62.8 & 70.0 & 71.9 \\ ZSCL & 26.8 & 88.5 & 63.7 & 55.7 & 60.2 & 82.1 & 82.6 & 58.6 & 85.9 & 66.7 & 70.4 & 67.4 \\ MoE & 30.1 & 89.3 & **74.9** & 64.0 & **82.3** & 89.4 & 87.1 & 89.0 & 89.1 & 69.5 & **72.5** & 76.1 \\ Primal-RAIL & 32.9 & **95.1** & 70.3 & 63.2 & 81.5 & **95.6** & **88.5** & 89.7 & 89.0 & 72.5 & 71.0 & 77.2 \\ Dual-RAIL & **36.0** & 94.8 & 71.5 & **64.1** & 79.5 & 95.3 & **88.5** & 89.4 & **91.5** & **74.6** & 71.3 & **77.9** \\   

Table 2: Comparison with state-of-the-art methods on 5-shot MTIL setting in terms of “Transfer”, “Average”, and “Last” scores (%). The best results are highlighted with **bold** style.