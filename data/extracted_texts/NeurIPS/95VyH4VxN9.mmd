# Autonomous Driving with Spiking Neural Networks

Rui-Jie Zhu\({}^{1}\), Ziqing Wang\({}^{2}\), Leilani Gilpin\({}^{1}\), Jason K. Eshraghian\({}^{1}\)

\({}^{1}\)University of California, Santa Cruz, USA

\({}^{2}\)Northwestern University, USA

Equal contributionCorresponding author, jsn@ucsc.edu

###### Abstract

Autonomous driving demands an integrated approach that encompasses perception, prediction, and planning, all while operating under strict energy constraints to enhance scalability and environmental sustainability. We present Spiking Autonomous Driving (SAD), the first unified Spiking Neural Network (SNN) to address the energy challenges faced by autonomous driving systems through its event-driven and energy-efficient nature. SAD is trained end-to-end and consists of three main modules: perception, which processes inputs from multi-view cameras to construct a spatiotemporal bird's eye view; prediction, which utilizes a novel dual-pathway with spiking neurons to forecast future states; and planning, which generates safe trajectories considering predicted occupancy, traffic rules, and ride comfort. Evaluated on the nuScenes dataset, SAD achieves competitive performance in perception, prediction, and planning tasks, while drawing upon the energy efficiency of SNNs. This work highlights the potential of neuromorphic computing to be applied to energy-efficient autonomous driving, a critical step toward sustainable and safety-critical automotive technology. Our code is available at https://github.com/ridgerchu/SAD.

## 1 Introduction

Autonomous driving, often considered the 'holy grail' of computer vision, integrates complex processes such as perception, prediction, and planning to achieve higher levels of vehicle automation as classified by the SAE J3016 standard . Many new vehicles now feature Level 2 autonomy, with transitions to Level 3 marking notable advancements. However, these systems must adhere to energy constraints of 50-60 W/h  and face increasing environmental concerns. Sudhakar _et al._ highlight the need for hardware efficiency to double every 1.1 years to maintain 2050 emissions from autonomous vehicles below those of 2018 data center levels .

Spiking Neural Networks (SNNs) offer a promising solution for energy-efficient intelligence by using sparse, event-driven, single-bit spiking activations for inter-neuron communication, mimicking biological neurons [4; 5; 6; 7]. Such workloads can be accelerated for low latency and low energy, when processed on neuromorphic hardware that utilizes asynchronous, fine-grain processing to efficiently handle spiking signals and parallel operations . Much like in the brain, spikes are thought to encode information over time, and have shown improvements in the energy efficiency of sequence-based computer vision tasks by several orders of magnitude in a variety of workloads [9; 10; 11].

In the past several years, SNNs have rapidly improved in performance across various tasks, including image classification [12; 13; 14; 15; 16; 17], object detection [18; 19; 20], semantic segmentation , low-level image reconstruction [22; 23; 24; 25; 26], and language modeling [27; 28; 29], with most of these works focused on computer vision. These advancements have brought SNN performancecloser to that of Artificial Neural Networks (ANNs) in fundamental computer vision tasks. Despite this progress, SNNs have not yet proven effective in complex real-world computer vision applications that involve multiple subtasks.

We introduce the first SNN designed for end-to-end autonomous driving, integrating perception, prediction, and planning into a single model. Achieving this milestone for SNNs involved spatiotemporal fusion of visual embeddings for enhanced perception, probabilistic future modeling for accurate prediction, and and a high performance temporal mixing spiking recurrent unit that effectively incorporates safety and comfort considerations into high-level planning decisions. By leveraging the event-driven and energy-efficient properties of SNNs, our model processes visual inputs, forecasts future states, and calculates the final trajectory for autonomous vehicles. Previously, GRUs or 3D convolutions were used in spatiotemporal visual tasks, though these operators have been entirely replaced with spiking neurons for time-mixing. This work marks a significant advancement in neuromorphic computing, demonstrating the potential of SNNs to handle the complex requirements of low-power autonomous driving. Our experiments show that this SNN-based system performs competitively with traditional deep learning approaches, while offering improved energy efficiency and reduced latency.

## 2 Related Works

**Spiking Neural Networks in Autonomous Systems** are particularly effective for low-power, edge intelligence applications, leveraging deep learning and neuroscience principles to boost operational efficiency [6; 4; 30; 31; 32]. Many neuromorphic autonomous systems use SNNs as a Proportional Derivative-Integral (PID) controller to adapt to changing conditions, such as different payloads in unmanned aerial vehicles [33; 34; 35; 36], or to prevent drift in non-neutral buoyancy blimps . Much of this work successfully deployed SNNs as PID controllers in real-world systems on neuromorphic hardware, highlighting the potential for low-power autonomous control. Moving from the sky to the ground, SNN-based PID controllers have been used for lane-keeping tasks in simulated environments with reference trajectories provided by the lane [38; 39; 40], as well as with LiDAR for collision avoidance in simulated environments . These tasks all show successful use of SNNs in adaptive control, though the objective of a PID controller is to maintain a desired setpoint which is often a well-defined and simpler goal than end-to-end autonomous driving in the face of complex and noisy environments. We push the frontier of what SNNs are capable of in this paper.

**End-to-end Autonomous Driving** directly maps sensory inputs to vehicle control outputs using a single, fully differentiable model. Existing approaches can be broadly classified into two categories: imitation learning and reinforcement learning paradigms . Imitation learning methods, such as behavior cloning [43; 44; 45; 46; 47] and inverse optimal control [48; 49; 50; 51; 52], learn a driving policy by mimicking expert demonstrations. On the other hand, reinforcement learning techniques [53; 54; 55; 56] enable the driving agent to learn through interaction with the environment by optimizing a reward function. Recent advancements, such as multi-modal sensor fusion [57; 58; 59; 60; 61], attention mechanisms [57; 62; 58], and policy distillation [43; 63; 64; 65; 66] have significantly improved the performance of end-to-end driving systems.

Figure 1: How SAD enables autonomous driving from vision to planning: The system processes inputs from six cameras across multiple frames. The perception module encodes feature information related to the present input frame (\(T=n\)), the prediction module predicts feature information of the next frame using sequential information (\(T=n+1\)), and the model output generates a steering and acceleration plan. This process creates a birdâ€™s eye view (BEV) and trajectory plan for navigation.

## 3 Method

This section presents the Spiking Autonomous Driving (SAD) method, an end-to-end framework that integrates perception, prediction, and planning using SNNs (Fig. 2). SAD's biologically-inspired architecture enables efficient spatiotemporal processing for autonomous driving, with the spiking neuron layer at its core. This layer incorporates spatiotemporal information and enables spike-driven computing, making it well-suited for the dynamic nature of autonomous driving tasks.

The perception module is the first stage of the SAD framework. It constructs a bird's eye view (BEV) representation from multi-view camera inputs, providing a human-interpretable understanding of the environment. This representation serves as the foundation for the subsequent prediction and planning modules. The prediction module uses the BEV to forecast future states using a 'dual pathway', which allows data to flow through two separate paths, providing a pair of alternative data embeddings. One pathway focuses on encoding information from the past, while the other pathway specializes in predicting future information. Subsequently, the embeddings from these two pathways are fused together, integrating the past and future information to facilitate temporal mixing. This enables the anticipation of dynamic changes in the environment, which is crucial for safe and efficient autonomous driving. Leveraging the perception and prediction outcomes, the planning module generates safe trajectories by considering predicted occupancy of space around the vehicle, traffic rules, and ride comfort. To optimize the entire pipeline, SAD is trained end-to-end using a composite loss that combines objectives from perception, prediction, and planning. The following subsections describe each module in detail.

### Spiking Neuron Layer

All modules consist of spiking neurons rather than artificial neurons, and so a formal definition of spiking neurons is provided below. Spiking neuron layers integrate spatio-temporal information into the hidden state of each neuron (membrane potential) which are converted into binary spikes emitted to the next layer. Spiking neurons can be represented as recurrent neurons with binarized activations and a diagonal recurrent weight matrix such that the hidden state of a neuron is isolated from all other neurons (see Ref.  for a derivation). We adopt the standard Leaky Integrate-and-Fire (LIF)  model, whose dynamics are described by the following equations:

\[U[t] =H[t-1]+X[t],\] (1) \[S[t] =(U[t]-u_{th}),\] (2) \[H[t] =V_{}S[t]+( U[t])(1-S[t] ),\] (3)

where \(X[t]\) is the input to the neuron at time-step \(t\), and is typically generated by convolutional or dense operators. \(U[t]\) denotes the membrane potential of the neuron, and integrates \(X[t]\) with the temporal input component \(H[t-1]\). \(()\) is the Heaviside step function, which is 1 for \(x 0\) and 0 otherwise. If \(U[t]\) exceeds the firing threshold \(u_{th}\), the spiking neuron emits a spike \(S[t]=1\) as its activation, and the temporal output \(H[t]\) is reset to \(V_{}\). Otherwise, no spike is emitted (\(S[t]=0\)) and \(U[t]\) decays to \(H[t]\) with a decay factor \(<1\). For brevity, we refer to Eq. 2 as \(()\), where the input \(U\) is a tensor of membrane potential values fed into multiple spiking neurons, and the output \(S\) is an identically-shaped tensor of spikes.

Figure 2: Overview of SAD. The multi-view features from the perception encoder, including a spiking ResNet with inverted bottleneck and spiking DeepLab head, are fed into a prediction module using spiking neurons. The perception decoder then generates lane divider, pedestrian, vehicle and drivable area predictions. Finally, the planning module models the scene and generates future predictions to inform rule-based command decisions for turning, stopping, and goal-directed navigation.

### Perception: Distinct Temporal Strategies for Encoder and Decoder

Fig. 3 illustrates the overall architecture of the perception module. The perception stage constructs a spatiotemporal BEV representation from multi-view camera inputs over \(t\) time-steps through spatial and temporal fusion of features extracted from the cameras. It consists of an encoder, which processes each camera input to generate features and depth estimations, and a decoder, which generates BEV segmentation and instructs the planning module. A future prediction module is depicted between the encoder and decoder in Fig. 3. It is not used in the first stage where the perception module is trained alone, but it is included in the second stage once the prediction module is included.

The temporal dimension processing in the Encoder/Decoder architecture is a crucial design consideration, as both SNNs and autonomous driving data inherently possess a temporal structure. There are two approaches to handle this:

* **Sequential Alignment (SA):** sequential input data is passed to the SNN step-by-step by aligning the time-varying dimension of the input data with the model
* **Sequence Repetition (SR):** sequential input data is aligned with the batch dimension for better parallelism during training, and individual frames are repeated \(T\) times over the model sequence, so as to create virtual timesteps. SR is commonly used for pre-training sequence-based models on static image datasets.

Given these two encoding options, we test all four combinations of these options applied to both the encoder and decoder of the perception block. Based on our experiments (detailed in Sec. 4.3.1), the best performing approach is using SR for the encoder and SA for the decoder. The encoder is also pre-trained on ImageNet-1K which requires the use of repeated images to create virtual timesteps. Further details regarding the pre-training of the encoder can be found in Appendix D.1. Conversely, the decoder is trained from scratch, which naturally assumes a temporal-mixing role, making the alignment of sequential data with the model sequence a more effective approach.

The training process involves first training the encoder-decoder, followed by the prediction module. This approach integrates spatial and temporal information for comprehensive BEV representation in autonomous vehicle perception and planning.

Encoder: Spiking Token Mixer with Sequence RepetitionThe encoder module can be thought of as a spiking token mixer (STM). The STM consists of 12-layers of spiking CNN pre-trained on ImageNet-1K  to generate vision patch embeddings, which is effectively a deeper version of the'spiking patch embedding' from Ref. [13; 14]. Across these 12 layers, the number of channels in each layer is designed to first increase and then decrease so as to act as an inverted bottleneck. While SPS layers are usually terminated by self-attention, we replaced this with dense layers instead, which both reduces computational resources and leads to improved performance. In doing so, we achieved a 72.1% ImageNet top-1 classification accuracy with only 12M parameters. In contrast, the previous spiking vision transformers that employs self-attention reached 70.2% with the same number of parameters .

Figure 3: The perception module. The encoder takes multi-camera input data, passes it through a spiking ResNet with inverted bottleneck to generate feature representations, each of which has its own depth estimation. These are fused and passed to the decoder, which generates predictions for lane dividers, pedestrians, vehicles and drivable areas.

The encoder extracts feature embeddings and depth estimations while squeezing the image into a smaller latent space. The overall workflow of the encoder can be summarized as follows:

\[X=(I), I^{N C_{in} T L H W}, X\{0,1\}^{C T L H W}\] \[=_{}(X), X\{0,1\}^{C T L H W},\{0,1\}^{C_{f}  L H W},\] \[=_{}(X), X\{0,1\}^{C T L H W},\{0,1\}^{C_{d}  L H W}\] \[Y=, Y\{0,1\}^{C_{f} C_{d} T L H W}\]

The STM encoder described above is used to extract feature embeddings and depth estimates from each camera frame \(I_{t}^{N C_{in} H W}\), where \(N=6\) is the number of cameras, \(C_{in}=3\) refers to the number of input channels (RGB), and \(H W\) refers to the video resolution. Note that the use of sequence repetition means that \(T\) is the number of times the same frame is repeated over the sequence, while \(L\) is the number of frames in a continuous camera recording. As such, the dimensions \(N L\) are stacked so as to speed up processing. The encoder generates two outputs: feature embeddings \(\{0,1\}^{N L C_{f} H W}\) and depth estimates \(\{0,1\}^{N L C_{d} H W}\), where \(C_{f}\) and \(C_{d}\) represent the number of channels for feature embeddings and depth estimates, respectively. These outputs are then fused to form a single tensor \(Y\{0,1\}^{N L(C_{f}+C_{d}) H W}\), effectively combining the \(C_{f}\) and \(C_{d}\) dimensions into a single dimension. The fused tensor \(Y\) is subsequently passed to the decoder for further processing. For a more comprehensive understanding of the encoder architecture and its internal workings, please refer to Appendix A.

Decoder: Sequential Alignment with Streaming Feature MapsThe recurrent decoder aligns feature maps sequentially, introducing a new instance of data at each time-step, contrasting with the encoder's repeated inputs. Using SA rather than SR for the decoder improves performance for two reason: 1) the decoder does not need to be pre-trained on static, repeated data, and 2) the decoder acts as a temporal mixer. In this architecture, time-mixing is achieved using LIF neurons and allows them to take on the role of self-attention without the same computational burden. The LIF neurons are composed as a shared backbone as a set of layers used to extract features from data before being passed to various specialized heads, each dedicated to a specific task. The high-level dataflow summarized as follows:

\[X=(I_{D}), I^{C_{in} L H W}, X\{0,1\}^{C_{med} T L H W}\] \[Y_{k}=_{k}(X), Y_{k}^{C_{out} L H W}, k \{\}\]

where \(I_{D}\) is the input tensor to the decoder with dimensions \((C_{in},L,H,W)\), \(X\) is the output of the shared backbone with dimensions \((C_{med},T,L,H,W)\),\(Y_{k}\) is the output of the \(k\)-th head with dimensions \((C_{out},L,H,W)\), and \(k\) indexes into the heads of the different tasks: vehicle segmentation (seg), pedestrian (ped), HD map (map), and future instance (inst). The shared backbone is implemented using the first three layers of MS-ResNet18  followed by three upsampling layers with a factor of 2 and skip connections. More details about MS-ResNet can be found in Appendix B. The resulting features have 64 channels and are then passed to different heads according to the task requirements. Each head consists of a spiking convolutional layer.

### Prediction: Fusing Parallel Spike Streams

Predicting future agent behavior is crucial for an autonomous vehicle to be reactive and make informed decisions in real-time. In our approach, we accumulate historical BEV features and predict the next few timesteps using purely LIF neurons. However, the stochastic nature of interactions among agents, traffic elements and road conditions, makes it challenging to accurately predict future trajectories. To address this, we model future uncertainty with a conditional Gaussian distribution.

To accomplish this, two layers of LIF neurons are used. The first parallel layer takes the present and prior output BEV feature maps from the encoder of the perception model \((x_{1},,x_{t})\) as inputs. The first BEV \(x_{1}\) is also used as the initial membrane potential for this LIF layer. The second parallel layer accounts for the uncertainty distribution of future BEV predictions. The uncertainty distribution is generated by passing the present feature \(x_{t}\) through 4 spiking MS-ResNet blocks, average pooling, and another 2D spiking convolution with a kernel size of \((1,1)\) to transform the channel depth to double the output of the first parallel layer. Another averaging pooling operator compresses the feature map down to a vector. The vector is split in two sub-vectors representing mean \(^{L}\) and variance \(^{L}\), and these values populate a diagonal Gaussian distribution of the latent feature map. Using the \(\) and standard deviation \(\), we can construct the Gaussian distribution at timestep \(t\), denoted as \(_{t}\). This distribution, represented by the parameters \(\) and \(\), can then be concatenated with the input at the current timestep \(x_{t}\).

Simultaneously, all prior spiking outputs are concatenated to the present input \(x_{t}\), denoted below \(x_{0:t}\). The predicted BEV feature for the next timestep is calculated below:

\[_{t+1}=((((x_{t},_{t})))((((x_{0:t}))),\] (4)

where \(_{t+1}\) represents the predicted BEV features for the next timestep, \(()\) denotes the LIF neuron layer, \(()\) represents the concatenation operation, and \(\) denotes element-wise addition of the outputs from the two LIF layers, and the inner \(()\) and \(()\) layers are used to ensure consistency in the output dimensions of the first and second LIF layers. The mixture prediction serves as the basis for the subsequent prediction steps. By recursively applying this dual-pathway prediction method, we obtain the predicted future states \((_{t+1},,_{t+n})\). The overall datapath is illustrated in Fig. 4. Following the dual-pathway prediction, all the features are fed into a Spiking ResNet using SA for additional temporal mixing. The historical features \((x_{1},,x_{t})\) and the predicted future features \((_{t+1},,_{t+n})\) are then passed to the perception decoder, which consists of multiple output heads to generate various interpretable intermediate representations.

### Planning: Temporal Spiking Temporal for Trajectory Refinement

The primary objective of the SAD system is to plan a trajectory that is both safe and comfortable, aiming towards a designated target. This is accomplished through a planning pipeline that integrates predicted occupancy grids \(o\), map representations \(m\), and temporal dynamics of environmental elements like pedestrians.

**Motion Planning.** Initially, our motion planner generates a diverse set of potential trajectories using the bicycle model . Among these, the trajectory that minimizes a predefined cost function is selected. This function integrates various factors including learned occupancy probabilities (from segmentation maps generated in the Prediction section) and compliance with traffic regulations to ensure the selected trajectory optimizes for safety and smoothness.

**Cost Function.** The cost function \(f\) employed is a multi-component function, where:

\[f(,o,m;w)=f_{o}(,o,m;w_{o})+f_{v}(;w_{v})+f_{r}(;w_{r})\] (5)

where \(w=(w_{o},w_{v},w_{r})\) represents the learnable parameters associated with each cost component, and \(\) denotes a set of trajectory candidates. Specifically, \(f_{o}\) evaluates trajectory compliance with static and dynamic obstacles, \(f_{v}\) is derived from the prediction decoder assessing future states, and \(f_{r}\) addresses metrics of ride comfort and progress towards the goal. The aim is to select the optimal set of trajectories \(^{*}\) that minimize this cost function. This cost function is adapted from Hu et al.  and is detailed in Appendix C.

Additionally, trajectories are filtered based on high-level commands (e.g., go forward, turn left, turn right) which tailor the trajectory selection to the immediate navigational intent.

**Optimization with Spiking Gated Recurrent Unit.** Following the initial selection, the "best" trajectory \(^{*}\) undergoes further refinement using a Spiking Gated Recurrent Unit (SGRU), as inspired by . In this optimization phase, the hidden state \(h_{t}\) of the SGRU incorporates features derived from the front camera's encoder. The input state \(x_{t}\) is formulated by concatenating the vehicle's

Figure 4: Dual pathway modeling for prediction. Neuron \(a\) captures future multi-modality by incorporating uncertainty distribution. Neuron \(b\) compensates for information gaps using past variations. Inputs \(x_{1}\) and \(x_{t}\) from both pathways are used for the next prediction step.

current position, the corresponding position from the selected trajectory \(^{*}\), and the designated target point. The SGRU model processes inputs as follows:

\[r_{t} =(W_{ir}x_{t}+b_{ir}+W_{hr}h_{t-1}+b_{hr})\] (6) \[z_{t} =(W_{iz}x_{t}+b_{iz}+W_{hs}h_{t-1}+b_{hz})\] (7) \[n_{t} =(W_{in}x_{t}+b_{in}+r_{t}(W_{hn}h_{t-1}+b_{hn}))\] (8) \[h_{t} =(1-z_{t}) n_{t}+z_{t} h_{t-1}\] (9)

where \(h_{t}\) denotes the hidden state at time \(t\), \(x_{t}\) represents the input, and \(r_{t}\), \(z_{t}\), \(n_{t}\) are the reset, update, and new candidate state gates respectively. The Heaviside function \(\) ensures sparse and binarized operations in the state of the SGRU, thus preserving the advantages of SNNs.

This optimization step enhances trajectory reliability by mitigating uncertainties inherent in perceptual and predictive analyses, and by integrating dynamic traffic light information directly into the trajectory planning process.

After the optimization, the model can be trained end-to-end, from perception to planning. Details about the stage-wise training process and the end-to-end learning methodology can be found in Appendices D.2 and D.3, respectively.

## 4 Experiments

We evaluate the proposed model using the nuScenes dataset  with 20 epochs with 4 \(\) NVIDIA A100 80GB. as is done in ST-P3 . For our experiments, we consider \(1.0s\) of historical context and predict \(2.0s\) into the future, which corresponds to processing 3 past frames and predicting 4 future frames. Additional experimental details are elaborated in the Appendix E.

### Experimental Results on nuScenes

**Perception.** Our evaluation focuses on the model's ability to interpret map representations and perform semantic segmentation. For map representation, we specifically assess the identification of drivable areas and lanes, which are critical for safe navigation as they dictate where the Self-Driving Vehicle (SDV) can travel and help maintain the vehicle's position within the lanes. Semantic segmentation tests the model's ability to recognize dynamic objects, such as vehicles and pedestrians, which are pivotal in urban driving scenarios.

We employ the Intersection-over-Union (IoU) metric to quantify the accuracy of our BEV segmentation tasks. The results, as summarized in Tab. 1, show that our SAD method, which is fully implemented with spiking neural networks (SNNs), competes favorably against state-of-the-art, non-spiking artificial neural networks (ANNs). Notably, our model achieves a superior mean IoU on the nuScenes dataset compared to existing leading methods such as VED , VPN , PON , and Lift-Splat . Specifically, our SAD method outperforms the VED  model by **7.43%** in mean IoU. This enhancement is significant, considering that our network utilizes spiking neurons across all layers, which contributes to greater computational efficiency. Despite the inherent challenges of using SNNs, such as the binary nature of spikes and potential information loss compared to traditional

    &  &  &  &  &  &  \\  & & Area & & & & \\  VED \({}^{}\) & âœ— & 60.82 & 16.74 & 23.28 & 11.93 & 28.19 \\ VPN \({}^{}\) & âœ— & 65.97 & 17.05 & 28.17 & 10.26 & 30.36 \\ PON \({}^{}\) & âœ— & 63.05 & 17.19 & 27.91 & 13.93 & 30.52 \\ Lift-Splat \({}^{}\) & âœ— & 72.23 & 19.98 & 31.22 & 15.02 & 34.61 \\ IVMP \({}^{}\) & âœ— & 74.70 & 20.94 & 34.03 & 17.38 & 36.76 \\ FIERY \({}^{}\) & âœ— & 71.97 & 33.58 & 38.00 & 17.15 & 40.18 \\ ST-P3 \({}^{}\) & âœ— & 75.97 & 33.85 & 38.00 & 17.15 & 42.69 \\ 
**SAD (Ours)** & âœ— & 64.74 & 27.78 & 34.82 & 15.12 & 35.62 \\   

Table 1: **Perception results.** We report the BEV segmentation IoU (%) of intermediate representations and their mean value.

ANNs, our results demonstrate that SAD is capable of delivering competitive perception accuracy in autonomous driving scenarios.

**Prediction.** We assess the predictive capabilities of our model using multiple metrics tailored for video prediction, specifically IoU, existing Panoptic Quality (PQ), Recognition Quality (RQ), and Segmentation Quality (SQ) for evaluating our prediction quality. The definition of these metrics can be found in Appendix E.2. The results, presented in Tab. 2, demonstrate that while our model does not employ an additional temporal module, the inherent temporal dynamics of spiking neurons facilitate effective information processing. However, our SAD model still shows a gap in performance when compared with state-of-the-art ANN methods.

**Planning.** In the planning domain, we evaluate our model using two primary metrics: L2 error and collision rate. To ensure fairness, the planning horizon is adjusted to \(3.0s\). The L2 error measures the deviation between the SDV's planned trajectory and the human driver's actual trajectory, providing a quantitative measure of planning accuracy. The collision rate assesses the model's ability to safely navigate the driving environment without incidents. Results, detailed in Tab. 3, reveal that our SAD method achieves an L2 error and collision rate comparable to those of the state-of-the-art ANN-based methods, underscoring the safety and reliability of our planning approach.

**Energy Efficiency.** Neuromorphic hardware is able to take advantage of small activation bit-widths and dynamical sparsity , and as such, SNNs are able to significantly reduce energy consumption during inference \(-\) provided there are sufficiently sparse activation patterns amongst spiking neurons. As detailed in Tab. 3, we present an estimation of the energy usage of each SOTA model based on dynamical sparsity (detailed calculation methods described in Appendix F). Owing to the utilization of spiking neurons, our model achieves substantial energy reductions: **7.33**\(\) less than the Freespace model  and **75.03**\(\) lower compared to the ST-P3 model . This exceptional energy efficiency makes our model highly suitable for real-world applications.

    &  &  \\ 
**MS** & **SEW** & **SP** & **DP** & **PQ** & **SQ** & **RQ** \\   & âœ“ & âœ“ & 59.28 & 0.75 & 0.44 \\ âœ“ & & âœ“ & 67.55 & 13.35 & 19.77 \\ âœ“ & & âœ“ & **68.16** & **16.57** & **24.11** \\   

Table 4: Ablation Study on different modules for the encoder and the decoder on Planning tasks.

    &  &  &  \\  & & IoU \(\) & PQ \(\) & SQ \(\) & RQ \(\) \\  Static & âœ— & 32.20 & 27.64 & 70.05 & 39.08 \\ FIERY \({}^{}\) & âœ— & 37.00 & 30.20 & 70.20 & 42.90 \\ ST-P3 \({}^{}\) & âœ— & 38.63 & 31.72 & 70.15 & 45.22 \\ 
**SAD (Ours)** & âœ— & 32.74 & 20.00 & 68.74 & 29.39 \\   

Table 2: **Prediction results.** We report semantic segmentation IoU (%) and instance segmentation metrics from the video prediction area. The _static_ method assumes all obstacles static in the prediction horizon.

    &  &  &  &  \\    & & & 1s & 2s & 3s & 1s & 2s & 3s \\  NMP \({}^{}\) & âœ— & 0.61 & 1.44 & 3.18 & 0.66 & 0.90 & 2.34 & - \\ Freespace \({}^{}\) & âœ— & 0.56 & 1.27 & 3.08 & 0.65 & 0.86 & 1.64 & 344.11 \\ ST-P3 \({}^{}\) & âœ— & 1.33 & 2.11 & 2.90 & 0.23 & 0.62 & 1.27 & 3520.40 \\ 
**SAD (Ours)** & âœ— & 1.53 & 2.35 & 3.21 & 0.62 & 1.26 & 2.38 & 46.92 \\   

Table 3: **Planning results.**

    &  &  \\ 
**MS** & **SEW** & **SP** & **DP** & **PQ** & **SQ** & **RQ** \\   & âœ“ & âœ“ & 59.28 & 0.75 & 0.44 \\ âœ“ & âœ“ & & 67.55 & 13.35 & 19.77 \\ âœ“ & & âœ“ & **68.16** & **16.57** & **24.11** \\   

Table 4: Ablation Study on different modules for the encoder and the decoder on Planning tasks.

### Visualization

To evaluate the effectiveness of our SAD model in a more interpretable manner, we provide qualitative results using the nuScenes dataset. Fig. 5 illustrates the outputs of our model. The SAD model effectively generates a safe trajectory that enables straight-ahead motion while avoiding collisions with curbsides and the vehicle ahead. Additionally, we conducted a comparative analysis with the ANN model, which demonstrated that our SAD model can achieve comparable performance, ensuring accurate and reliable planning outcomes.More visual results can be found in the Appendix E.3.

### Ablation Study

#### 4.3.1 Timestep Strategy

The large space of architecture design decisions came with a large number of ablation studies before determining the best performing model. We conducted an ablation study to examine the effects of different timestep alignment strategies on the performance of the encoder and decoder in perception tasks. The strategies include 'SR' for the repetition of vision inputs over timesteps, 'SA' where sequential inputs are aligned with the model's inherent temporal dimension, and 'w/o T' where recurrent dynamics in the decoder are removed, thus disconnecting the hidden states between timesteps. The results are summarized in Tab. 5. The last row is the baseline configuration of our model that serves as a reference point for these experiments. All ablation experiments are run for 5 epochs.

**Impact of Timestep Repetition in Encoder.** (Row 1) When repeating timesteps in the encoder which was pre-trained on the ImageNet-1K dataset for classification tasks, we notice a substantial benefit. This approach leverages the encoder's ability to capture spatial information through repeated images, a technique effective during its pre-training phase. Adopting this strategy in the SAD model enhances perception performance by maintaining consistency with the pre-training approach.

**Impact of Timestep Alignment in Decoder.** (Row 2) In contrast to the encoder, the decoder, which is trained from scratch, benefits from aligning timesteps with the model's inherent temporal dimension ('SA'). This strategy leverages the decoder's capacity to mix temporal information, improving performance over the repeat strategy ('SR'), which fails to show any performance gains.

**Single Timestep Input.** (Row 3) The purpose of this experiment was to study how well spiking neurons perform as temporal mixers. In this setup, all inputs are processed in one timestep without inter-frame connections, leading to a slight performance decrease compared to our baseline. Therefore, it confirms that spiking neurons inherently possess temporal processing capabilities.

#### 4.3.2 Effectiveness of Different Modules

Tab. 4 outlines the results of an ablation study that assesses the impact of various structural modifications to the encoder and decoder on planning tasks. The study differentiates between 'MS' for MS-ResNet structure , 'SEW' for SEW-ResNet structure , 'SP' for single pathway model, and 'DP' for dual pathway model. We analyze the planning performance of each configuration. The last row represents the configuration of our final model, which we use as the baseline for comparison.

**Decoder Structure Evaluation.** (Row 1) This part of the study compares the MS-ResNet and SEW-ResNet structures in their roles as decoders, where both of them are mainly used in SNN

Figure 5: **Qualitative Results of the SAD Model on the nuScenes Dataset.** (a) displays six camera view inputs utilized by the model. (b) illustrates the planning result of the ANN model, and (c) presents the planning results of our SAD model. The comparison shows that our SAD model can achieve performance comparable to that of the ANN model and successfully generate a safe trajectory.

architectures. The difference between MS-ResNet and SEW-ResNet can be found in Appendix B. Our results indicate that the MS-ResNet structure is more effective for planning tasks, likely due to its enhanced capability to handle the spatial-temporal dynamics required in this context.

**Pathway Model Comparison.** (Row 2) Here, we explore the performance difference between single and dual pathway prediction models. The dual pathway model integrates information through two distinct processing streams. Our experimental results show that the dual pathway model significantly outperforms the single pathway model in planning tasks.

## 5 Conclusion

In this work, we presented Spiking Autonomous Driving (SAD), the first end-to-end spiking neural network designed for autonomous driving. By integrating perception, prediction, and planning into a unified neuromorphic framework, SAD demonstrates competitive performance on the nuScenes dataset while exhibiting exceptional energy efficiency compared to state-of-the-art ANN-based methods. The perception module effectively constructs interpretable bird's eye view representations, the prediction module accurately forecasts future states using a novel dual-pathway architecture with spiking neurons, and the planning module generates safe and comfortable trajectories. Crucially, SAD showcases the immense potential of SNNs in complex real-world applications, marking a significant step towards realizing low-power, intelligent systems for safety-critical domains like autonomous vehicles. However, further validation through real-world on-vehicle testing is necessary. Moving forward, we believe this work will inspire further research into neuromorphic computing for sustainable and robust autonomous driving solutions.