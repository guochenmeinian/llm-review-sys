# Worst-Case Offline Reinforcement Learning

with Arbitrary Data Support

Kohei Miyaguchi

IBM Research - Tokyo

Tokyo, Japan

koheimiyaguchi@gmail.com

The author is affiliated with LY Corporation at the time of publication.

###### Abstract

We propose a method of offline reinforcement learning (RL) featuring the performance guarantee _without_ any assumptions on the data support. Under such conditions, estimating or optimizing the conventional performance metric is generally infeasible due to the distributional discrepancy between data and target policy distributions. To address this issue, we employ _a worst-case policy value_ as a new metric and constructively show that the sample complexity bound of \(O(^{-2})\) is attainable without any data-support conditions, where \(>0\) is the policy suboptimality in the new metric. Moreover, as the new metric generalizes the conventional one, the algorithm can address standard offline RL tasks without modification. In this context, our sample complexity bound can be seen as a strict improvement on the previous bounds under the single-policy concentrability and the single-policy realizability.

## 1 Introduction

Offline reinforcement learning (RL) (Levine et al., 2020; Prudencio et al., 2023) is a framework for learning decision-making policies while constrained to a fixed batch of data, preventing the learner from acquiring new information about the environment during training.

The primary challenges of offline RL are thus originated from the discrepancy between the state-action distribution of the batch data \((s)(a|s)\) and the visitation distribution of the trained policy \(d^{}(s)(a|s)\). Most of the previous studies have avoided directly dealing with this discrepancy by posing the assumption known as _concentrability_(Munos and Szepesvari, 2008; Antos et al., 2008; Chen and Jiang, 2019; Xie et al., 2022). Roughly speaking, the condition asserts that the ratio between these two distributions \(d^{}/\) is well-defined and uniformly bounded over the entire state-action space. This, in turn, constrains the trained policy \(\) to strictly stay inside the state space covered by the data support.

However, concentrability may be impractical in real-world applications for several reasons. First, one often ends up with a poor coverage of the state-action space when exhaustive data collection is expensive or practically infeasible as in the domains of autonomous driving (Fang et al., 2022), healthcare (Yu et al., 2021) and public policy-making (Abe et al., 2010). Moreover, the precise shape of the partial coverage is unknown if the considerations making it partial are not well-documented or disclosed. On the other hand, it is generally difficult to accurately predict if a policy will visit a given state or not based only on the knowledge of the policy and the batch dataset. As a result, the set of concentrable policies in a hypothesis space may be too small to achieve reasonable performance oreven empty.2 Therefore, for applying offline RL in such domains, we need a method that works well without concentrability or any coverage-related conditions.

To tackle with this issue, we study offline RL with arbitrary data support. We present two major results in this paper.

1. We develop _worst-case offline RL_ (Problem 4.1), a new offline RL framework for handling poor state-action coverage, which can be seen as a natural generalization of conventional offline RL (Corollary 4.2).
2. We develop _worst-case minimax RL_ (WMRL, Section 6.3), a model-free algorithm addressing worst-case offline RL (Corollary 6.3). The resulting sample complexity bound improves the previous state of the art in terms of both the weakness of the assumptions and the strength of the bound (Table 1).

The rest of the paper is organized as follows. In Section 2, we review the previous work in the literature of offline RL, centered around theoretical studies on the role of concentrability. In Section 3, we introduce some preliminaries around Markov decision process (MDP), offline RL and concentrability. Then, in Section 4, with the observation that offline RL is ill-posed without concentrability, we introduce _worst-case offline RL_ as a natural generalization and discuss some of its properties useful in our subsequent analysis. In Section 5, we establish the connection between worst-case offline RL and the Lagrangians derived from the saddle-point formulation of offline RL. In Section 6, exploiting the connection established earlier, we construct a method for solving worst-case offline RL with polynomial sample complexity. Finally, we discuss the limitation and the future work in Section 7.

## 2 Related Work

The notion of concentrability is introduced by Munos (2003); Munos and Szepesvari (2008); Antos et al. (2008) to analyze the value/policy iteration algorithms, not necessarily in the context of offline RL. Recently, it has been increasingly gaining traction as one of the key characteristics of the difficulty of offline RL (Chen and Jiang, 2019) due to the distribution mismatch. In its original definition, concentrability requires the norm of the density ratio \(\|d^{}/\|_{}\) to be bounded _uniformly_ for all the policies \(\). Liu et al. (2020) showed that this uniform boundedness can be relaxed to the single-policy boundedness with the principle of _pessimism in the face of uncertainty (PFU)_. Considering the case where the single-policy concentrability is even slightly violated, Xie et al. (2021) further analyzed the

    &  &  \\    & Concentrability & & Realizability \\  Zhan et al. (2022) & \(^{*}\) & \(_{n}\) & \(^{-6}(1-)^{-4}(/)\) \\ Chen and Jiang (2022) & \(^{*}\) & \(^{*}\) & \(^{-2}H^{5}C_{}^{-2}(/)\) \\ Ozdaglar et al. (2023) & \(^{*}\) & \(^{*}\) & \(^{-2}(1-)^{-6}C_{}^{-2}(/)\) \\ Uehara et al. (2023) & \(^{*}\) & \(^{*}\) & \(^{-2-4/_{}}(1-)^{-6-4/_{}} (/)\) \\  Ours (Corollary 6.3) & â€” & \(^{*}\) & \(^{-2}(1-)^{-4}(/)\) \\   

Table 1: **Assumptions and sample complexity bounds of related work. \(^{*}\) and \(^{*}\) denote optimal policies in the conventional and worst-case offline RL, respectively. \(_{n}\) denotes a sequence of policies indexed with the sample size \(n\). The realizability of \(\) means that \(\)-associated model-free parameters (e.g., value functions, visitation weight functions and the policy itself) are realizable. \(>0\) is the policy suboptimality given in Problem 4.1 (or equivalently in Problem 3.1, see Corollary 4.2 for the equivalence). \(0<<1\) denotes the confidence parameter. \(H\) denotes the time horizon and roughly comparable to \((1-)^{-1}\). \(C_{}\) and \(_{}\) denote the minimum and the lower-tail exponent of the action value gaps, respectively. \(\) denotes the cardinality of the function classes. The improvements made by our result are emphasized. See Appendix A for more details.**performance degradation caused by the lack of concentrability. Finally, we completely remove the concentrability assumption by incorporating it into a new performance metric.

The removal of concentrability is useful not only for widening the applicability of offline RL, but also strengthening the sample complexity bound by streamlining the analysis. Previously, Zhan et al. (2022) established polynomial sample complexity bounds under the weakest known model-free assumptions, yet being unable to achieving the statistically reasonable rate \(O(^{-2})\). Also, Chen and Jiang (2022); Ozdaglar et al. (2023); Uehara et al. (2023) gave improved rates additionally assuming that the minimum action value gap is bounded away from zero. On the other hand, under a set of assumptions as weak as Zhan et al. (2022), our sample complexity bound achieves the rate of \(O(^{-2})\). See Appendix A for more detailed discussions.

Algorithmically, the PFU principle is often materialized as the pessimistic or behavioral regularization (Kumar et al., 2020; Fujimoto and Gu, 2021; Yu et al., 2020). Previous analyses are often sensitive to the hyperparameters controlling the degree of such regularization, such as the truncation threshold \(b\) in Liu et al. (2020), the Bellman consistency threshold \(\) in Xie et al. (2021); Chen and Jiang (2022) and the regularization weight in Zhan et al. (2022); Uehara et al. (2023). On the other hand, our method has no hyperparameter other than the choice of the function approximators. One may see the root cause of this difference in that the PFU principle is built into our single new performance metric, whereas the previous studies adopt it as an additional objective, resulting in bi-objective optimizations.

## 3 Preliminaries

We denote the set of nonnegative real numbers by \(_{+}=[0,)\) and the uniform norm of a function \(g\) over its domain by \(\|g\|_{}_{z(g)}|g(z)|\). We also denote by \(()\) the set of (generalized) probability density functions on \(\) relative to a suitable base measure,3 such as the counting measure and the Lebesgue measure.

Markov decision process (MDP) and RL.Let \(=(,,R,T)\) be an MDP consisting of the state space \(\), the action space \(\), the reward function \(R:()\) and the transition probability \(T:()\). We assume both \(\) and \(\) are finite sets for simplicity. The goal of RL in general is to optimizing policy \(:()\) in terms of the _policy value_,

\[J()=J(|)(1-)^{}[_{t 0 }^{t}r_{t}],\]

with a discount factor \((0,1)\). Here, the expectation \(^{}\) is taken with respect to the Markov chain generated with \(a_{t}(|s_{t})\), \(r_{t} R(s_{t},a_{t})\) and \(s_{t+1} T(s_{t},a_{t})\), \(t 0\), starting from a known initial-state distribution \(s_{0} p_{0}(s)\).

Offline constraint.In maximizing \(J()\), the _offline constraint_ prohibits us to access the environment \(\) except through the _offline dataset_\(\{(s_{i},a_{i},r_{i},s^{}_{i})\}_{i=1}^{n}\). We assume the dataset is sampled from a fixed distribution \(p_{}^{}([0,1 ])^{n}\) such that

\[p_{}^{}()=_{i=1}^{n}(s_{i})\,(a_ {i}|s_{i})\,R(r_{i}|s_{i},a_{i})\,T(s^{}_{i}|s_{i},a_{i}),\]

where \(()\) and \(:()\) are the behavior state distribution and the behavior policy, respectively. Typically, \(p_{}^{}()\) represents the distribution of the past observational data. The problem of offline RL is now formally given as follows.

**Problem 3.1** (Offline RL).: _Given the offline dataset \(\) and a small number \(>0\), find a policy \(\) achieving \(J^{*}-J()\), where \(J^{*}_{:()}J()\)._

Value, visitation and weight functions.Let \(r(s,a)_{y R(s,a)}[y]\) be the expected reward function and \(r^{}(s)_{a}r(s,a)(a|s)\) be its marginalization with respect to policy \(\). Let \(,^{}\) and \(_{*}^{}\) be the raw transition operator, the policy transition operator and its adjoint given by \(v(s,a)=_{s^{}}v(s^{})T(s^{}|s,a)\), \(^{}v(s)=_{a}Tv(s,a)(a|s)\) and \(_{*}^{}d(s)=_{s^{},a^{}}d(s^{})(a^{ }|s^{})T(s|s^{},a^{})\), respectively. Then, the _state value function_\(v^{}:\), the _action value function_\(q^{}:\) and the _state visitation distribution_\(d^{}()\) are given by

\[v^{} =(I-^{})^{-1}r^{}, q^{} =r+v^{}, d^{} =(1-)(I-_{*}^{})^{-1}p_{0},\]

as well as the _state weight function_\(w^{}:()\) and the _action weight function_\(f^{}:()\) by

\[w^{}(s) (s)}{(s)}, f^{}(s,a)  w^{}(s)\,^{}(s,a),\]

where \((g)\{x(g)\,|\,g(x) 0\}\) denotes the support of function \(g\) and \(^{}(s,a)\) is the density ratio of \(\) to \(\). We also define the optimal value functions by \(v^{*}(s)_{}v^{}(s)\) and \(q^{*}(s,a)=_{}q^{}(s,a)\) as well as the set of the optimal policies \(^{*}\{:()\,:\,v^{}=v^{*}\}\), which by definition all attain \(J^{*}\). See Table 2 for the summary of the notation introduced above.

Concentrability.A policy \(\) is said to be _concentrable_ (or satisfying concentrability) if its state-action visitation is contained in the data support, \((d^{})()\). We denote the set of all the concentrable policies by \(_{}\{:()\,:\, (d^{})()\}\).

## 4 Worst-Case Offline Reinforcement Learning

In offline RL (Problem 3.1), the information on \(\) is restricted by the data support \(()\). In such situations, one cannot know about the transition probability \(T(s,a)\) and the reward probability \(R(s,a)\) for \((s,a)()\). Consequently, the accurate estimation of \(J()\) is infeasible (even with \(n=\)) for unconcentrable policies, and thus previous analyses on Problem 3.1 often require that there exists at least one concentrable optimal policy, i.e., \(_{}_{}J()\).

To remove such dependency on concentrability, we introduce a performance metric alternative to \(J()\). Let \(\{^{}\,:\,p_{}^{^{ }}=p_{}^{}\}\) be the set of the environments indistinguishable from the true environment \(\) with respect to the resulting data distribution \(p_{}^{}\). Noting that \(\) is the information-theoretic limit of the uncertainty on \(\) under the offline constraint, we follow the _pessimism-in-the-face-of-uncertainty_ principle and consider the worst case within \(\),

\[()_{^{}}J(| ^{}),\] (1)

which we refer to as _worst-case policy value_. Replacing \(J()\) with \(()\) in Problem 3.1, we arrive at the following problem.

**Problem 4.1** (Worst-case offline RL).: _Given the offline dataset \(\) and a small number \(>0\), find a policy \(\) achieving \(^{*}-()\), where \(^{*}_{:()}()\)._

To facilitate the subsequent analysis on Problem 4.1, we next introduce the notion of _truncated environment_, which is similar to, yet different from those previously considered by Liu et al. (2020); Yin and Wang (2021) as it is based on the _true and unknown_ data support rather than the empirical one. The truncation is useful for characterizing the worst-case policy value \(()\).

**Definition 4.1** (Truncated environment).: _The truncation of \(\) with respect to \(\) and \(\) is given by \(}=(},,,)\), where \(}=\{\}\) with \(\) being an absorbing state with reward zero and_

\[(r|s,a) _{,}(s,a)\,R(r|s,a)+(1-_{,}(s,a))\, _{0}(r),\] (2) \[(s^{}|s,a) _{,}(s,a)\,T(s^{}|s,a)+(1-_{, }(s,a))\,_{}(s^{}),\] (3)

_for \(s\) and \(a\), where \(_{,}(s,a)=\,\{(s)>0\}\,\,\{(a|s)>0\}\) is the indicator function of the support of \((s)\,(a|s)\) and \(_{x}()\) is the Dirac's delta function located at \(x\)._

**Theorem 4.1**.: _We have \(()=J(|})\) for all \(:()\)._Proof (sketch).: It suffices to show \(J(|})() J(|)\), where the first inequlilty follows from \(J(|}) J(|^{})\) for all \(^{}\) and the second inequality follows from \(}\). See Appendix D.1 for the complete proof. 

In other words, worst-case offline RL is nothing but offline RL with the truncated environment \(}\). Thus, in principle, one can exploit conventional offline RL methods to solve Problem 4.1. We hereafter refer to the truncated counterparts (those defined by replacing \(\) with \(}\)) of \(v^{}\), \(v^{*}\), \(q^{}\), \(q^{*}\), \(^{*}\), \(d^{}\), \(w^{}\) and \(f^{}\) as \(^{}\), \(^{*}\), \(^{}\), \(^{*}\), \(^{}\) and \(^{}\), respectively. Likewise, let \(\), \(^{}\), \(}\), \(}^{}\) and \(}_{*}^{}\) be the truncated counterparts of \(r\), \(r^{}\), \(\), \(^{}\) and \(_{*}^{}\), respectively.

Let us remark several key implications of Theorem 4.1. First, since the unknown parameters \(\) and \(\) of the truncated environment \(}\) are only nontrivial on the data support, it is intuitively obvious that \(()\) can be accurately estimated even without the concetrability, given sufficiently large \(n\). Thus, it is reasonable to expect that Problem 4.1 does not require any concetrabilities to be well-posed, unlike Problem 3.1. Second, the constructive existence of \(}\) makes the relationship between \(J()\) and \(()\) clearer, as stated in the following corollary.

**Corollary 4.1**.: _We have \(()=J()\) if \(_{}\) and \(() J()\) otherwise._

Proof.: See Appendix D.2. 

According to Corollary 4.1, the pessimism introduced by the truncation is mild in the sense that it conserves the values of concentrable policies. Finally, it also clarifies the relationship between the suboptimality metrics of the conventional and the worst-case problems.

**Corollary 4.2**.: _For all \(:()\), we have_

\[J^{*}-J()^{*}-()\] (4)

_if \(_{}*{argmax}_{}J()\). Moreover, the equality is attained if in addition \(_{}\)._

Proof.: Trivial from Corollary 4.1. 

In other words, solutions of the worst-case problem are also valid as solutions of the conventional problem under the standard assumption, while the two solution concepts are identical if only concentrable policies are concerned. In this sense, worst-case offline RL is a natural generalization of the conventional offline RL for handling arbitrary data distributions.

Finally, we conclude this section by showing a useful property of the worst-case optimal policies. Let \(_{}\{:()\,|\, {supp}()*{supp}()\}\) be the set of the _on-support_ policies, i.e., the policies with the support covered by the behavior policy. The following lemma allows us to limit the scope of policy optimization to \(_{}\) without sacrificing the optimality in terms of \(}\). The proof is relegated to Appendix D.3.

**Lemma 4.1**.: _There is at least one worst-case optimal policy that is on-support, i.e., \(^{*}_{}\)._

## 5 Lagrangians for Worst-Case Offline Reinforcement Learning

In this section, we set up theoretical foundation of worst-case offline RL. Specifically, in Section 5.1, we show a connection between \(()\) and the Lagrangian of RL (Puterman, 2014). However, since the Lagrangian in its original form is unstable to the function approximation error (Section 5.2), we further introduce a regularized variant of it (Section 5.3).

### Unregularized Lagrangian

Consider the following functional of \(v:_{+}\) and \(f:*{supp}()_{+}\),

\[L(v,f)(1-)_{p_{0}}[v(s)]+_{, }[f(s,a)\,^{}v(s,a)],\] (5)where \(_{p_{0}}\) and \(_{,}\) are the expectation operators with respect to \(s p_{0}(s)\) and \((s,a)(s)(a|s)\), respectively, and \(^{}:^{}^{ }\) is the time-difference error operator given by \(^{}v(s,a)=r(s,a)+v(s,a)-v(s)\).

We refer to \(L(v,f)\) as _the (unregularized) Lagrangian_ since it has been known as the Lagrangian of the linear-programming-based formulations of RL (Puterman, 2014; Chen and Wang, 2016; Nachum et al., 2019; Zhang et al., 2021; Zhan et al., 2022). The following theorem reveals that, perhaps surprisingly, it is also connected with worst-case offline RL.

**Theorem 5.1**.: _For all \(_{}^{*}\), \((^{*},^{})\) is a saddle point of \(L(v,f)\) in \(^{}_{+}^{}_{ +}\)._

Proof (sketch).: The key of the proof is the following identity of Lagrangian.

**Lemma 5.1**.: _For all \(_{}^{*}\), we have_

\[L(v,f)=^{*}-_{,}[(f-^{})(s,a)\,(I- })(v-^{*})(s,a)]+D_{}^{*}(v) -D_{}^{*}(f),\] (6)

_where \(D_{}^{*}(v)_{s()}^{}(s) \,v(s)\) and \(D_{}^{*}(f)_{,}[f(s,a)\,\{^ {*}(s)-^{*}(s,a)\}].\)_

Now, observe that the third term \(D_{}^{*}(v)\) and the fourth term \(D_{}^{*}(f)\) in Eq. (6) are nonnegative since all of \(v,f,^{}\) and \(^{*}-^{*}\) are nonnegative. Therefore, Lemma 5.1 implies that Lagrangian is bounded from below with \(L(v,f)^{*}\) taking \(f=^{}\), while bounded from above with \(L(v,f)^{*}\) taking \(v=^{*}\). Combining these two inequalities, a class of the saddle points of Lagrangian is identified as desired. The full proof is found in Appendix E.2. 

Note that the previous studies on the LP-based formulation of RL typically consider the saddle points in a different domain, \(^{}^{}_{+}\), where the domain of the primal variable \(v\) is unconstrained, unlike our setting with the constraint \(v 0\). This constraint is the key to establish the connection with the worst-case environment \(}\).

Theorem 5.1 superficially suggests that finding the saddle points of \(L(v,f)\) is a reasonable way of finding the optimal policies with respect to \(}\). However, in the next section, we show that it is unstable and easily breaks down by the function approximation error.

### Instability of Unregularized Lagrangian

When the state space \(\) is large, it is practically infeasible to find a saddle point of \(L(v,f)\) naively searching over the whole space \(^{}_{+}^{}_{ +}\). Therefore, one may introduce compact function classes \(^{}_{+}\) and \(^{}_{+}\) and limit the scope of the search to these classes. Since we do not know the saddle points (which motivates us to find one), such function classes likely incur the function approximation error. Thus, it is likely that the saddle points \((^{*},^{})\) may sit near the search space \(\), but not exactly included in the space, \((^{*},^{})\).

In this context, we show even a tiny function approximation error can completely disrupt the connection established in Theorem 5.1. Consider the function classes \(_{}=\{v^{}_{+}\,|\,v^{ *}+\}\) and \(_{}=\{f^{}_{+}\,| \,_{}^{*}f^{}-\}\) with a small constant \(>0\). Then, even though the function approximation error is small (\(\) for both \(\) and \(\) in terms of the \(L^{}\)-norm), the saddle point is collapsed to zero under the approximations with \(_{}\) and \(_{}\). The proof is relegated to Appendix E.3.

**Corollary 5.1**.: _Suppose \(_{}\) and \(_{}\) are nonempty. Then, the saddle points of \(L(v,f)\) in \(_{}^{}_{+}\) must satisfy \(f=0\). Moreover, the saddle points of \(L(v,f)\) in \(^{}_{+}_{}\) must satisfy \(v=0\)._

### Regularized Lagrangian

Corollary 5.1 shows that the saddle points of Lagrangian cannot be used as a reliable way to find the optimal policies with the function approximation. As a workaround, we introduce _a regularized Lagrangian_,

\[K(v,f)(1-)_{}[v(s)]+_{, }[f(s,a)\,^{}v(s,a)]+}{2} \|v\|_{2,}^{2},\] (7)where \(+_{*}^{}\) and \(\|v\|_{p,}\{_{s}(s)v^{p}(s)\}^{1/p}\) denotes the \(L^{p}()\)-norm of the functions over \(\). Then, it is shown that the regularized Lagrangian is also connected with worst-case offline RL through its saddle points. To see this, let us define the regularized counterparts of \(^{}\), \(^{}\) and \(^{}\) with \(^{}(1-)(I-}_{*}^{})^{-1 }^{}\), \(^{}^{}/\), \(^{}^{}^{}\), obtained by substituting the initial state distribution \(p_{0}\) with \(^{}+(1-)^{}\).

**Theorem 5.2**.: _For all \(_{}^{}\), \((^{*},^{})\) is a saddle point of \(K(v,f)\) in \(_{+}^{}_{+}^{}\). Moreover, the primal solution \(^{*}\) is unique on \(()\)._

Proof (sketch).: Similarly as the proof of Theorem 5.1, the key is the following identity.

**Lemma 5.2**.: _There exist \(U^{*}\) such that, for all \(_{}^{}\),_

\[K(v,f)=U^{*}-_{,}(f-^{})(s,a)\,(I- })(v-^{*})(s,a)+_{}^{ }(v)-D_{}^{*}(f),\] (8)

_where \(_{}^{}(v)_{s( )}^{}(s)\,v(s)+}{2}\|v-^{} \|_{2,}^{2}\)._

The first claim of Theorem 5.2 then follows from the fact \(_{}^{}(v)\) is nonnegative, and the second claim follows from the strong convexity of \(K(v,f)\) with respect to \(v\) on \(()\). The full proof is in Appendix E.5. 

Comparing Theorems 5.1 and 5.2, it turns out that the regularization does not alter the primal part of the saddle points \(^{*}\). Moreover, since \(K(v,f)\) is strongly convex in terms of \(v\), the regularized solution is more stable against the function approximation error as opposed to the unregularized solution. To see this, denote the regularized primal solution under the function approximation by

\[_{}^{*}*{argmin}_{v}_{f }K(v,f),\] (9)

where \(_{+}^{}\) and \(_{+}^{}\) are compact function classes. Also denote the individual function approximation errors of \(\) and \(\) by

\[_{}_{v}\|v-^{*} \|_{1,},_{}_{f ,_{}}\{_{}\|f-^{} \|_{1,}+2(1-)\|^{}-^{*}\|_{1, {}}\},\] (10)

where \(_{}\{1+ B_{},B_{}\}\) and \(B_{}_{v}\|v\|_{}\) are scale factors of \(\). Then, the following lemma shows the stability of the approximate solution \(_{}^{*}\) in terms of the aggregated function approximation error

\[_{}(,)})\,_{}}+4}}}{1- }=O(}\,_{}+_{ }}}{1-}),\]

where \(B_{}_{f}\|f\|_{}\) is the scale factor of \(\). The proof is relegated to Appendix E.6.

**Lemma 5.3** (Stability of the regularized primal solution).: _We have \(\|_{}^{*}-^{*}\|_{2,} _{}(,)\)._

Note that the approximation error of \(\) is trivially bounded by a simpler error term

\[_{}_{}_{f,_{ }^{*}}\|f-^{}\|_{1,},\]

i.e., the \(L^{1}\)-error with respect to the function \(^{}\) of the _optimal_ on-support policy \(_{}^{*}\). Our definition of the error is weaker than that, measuring the error with respect to that of the _possibly suboptimal_ on-support policy \(_{}\) in exchange for the additional suboptimality cost \(2(1-)\|^{}-^{*}\|_{1,}\). This is beneficial if the optimal policies are difficult to approximate, like deterministic policies in a continuous action space, yet some near-optimal policies such as the soft-optimal policies \((a|s)\{-^{*}(s,a)\}\) are easy to approximate.

We also note that the idea of stabilizing the saddle-point-based policy optimization via a strongly convex regularizer is not new (Nachum et al., 2019; Lee et al., 2021; Zhan et al., 2022; Uehara et al., 2023). The major difference here (other than the truncation) is that we regularize the value function \(v\) (like Uehara et al. (2023)) but extract the information of the optimal policy from \(f\) (like Nachum et al. (2019); Lee et al. (2021); Zhan et al. (2022)), which, combined with our worst-case framework, results in a striking improvement in the sample complexity.

## 6 Worst-Case Minimax Reinforcement Learning

Now, we present a method to solve worst-case offline RL with the saddle points of \(K(v,f)\). We first introduce a method of extracting policy from the dual variable \(f\) (Section 6.1), then show the suboptimality bound of the extracted policy (Section 6.2), which is our main result, and finally show the sample complexity bound taking into account the finite sample approximation (Section 6.3).

### Policy Extraction

Motivated by Theorem 5.2, we propose a method of extracting the worst-case optimal policy \(^{*}\) from the saddle point of \(K(v,f)\). Specifically, we consider minimizing the loss function given by

\[D_{}(f;w,)_{}\{_{,}[f (s,a)\,(s,a)]-_{,}[(s)\,(s,a) ]\},\] (11)

where \(w:\) is an auxiliary weight function, \((s)\{1-,w(s)\}\) is its lower clipping and \(^{}\) is a class of discriminator functions. Note that \(D_{}(f;w,)\) is the integral probability metric (IPM) (Sriperumbudur et al., 2009) between \(f(s,a)\,(s)\,(a|s)\) and \((s)\,(a|s)\) with respect to the discriminators \(\). With a sufficiently rich \(\), this implies \(D_{}(f;w,)\) attains its minimum value (i.e., zero) only if \(=_{f}\),4 thereby informally justifies the minimization of Eq. (11) as a way of policy extraction.

This approach introduces additional (functional) variables to be optimized, \(w:\) and \(:()\). To simplify the notation, consider a parameter space \(\) and suppose \(f\), \(w\) and \(\) share the same parameter space \(\), i.e., there exists a mapping \((f_{},w_{},_{})\), and redefine the dual space with \(=()\{f_{}\,|\,\}\).5 We define the associated function approximation error with

\[_{}_{_{s}}\{_{V,}_{ }()+2(1-)\|^{*}-^{*}\|_{1,}\},\] (12)

where \(_{,}\{_{V},B_{},\|^{*} \|_{}\}\), \(B_{}\{_{}\,\|\|_{}\) and

\[_{}()_{}\{\|f_{} -^{}\|_{1,}+\|w_{}-^{}\| _{1,}+B_{}}\|_{}-\|_{,}\}\] (13)

denotes the \(\)-specific function approximation error of \(\). Here, \(B_{}}_{}\|_{}\|_{}\) is the boundedness of \(_{}(s)\) and \(\|\|_{,}\) is the mean total variation (TV) distance with respect to \(\), given by \(\|-^{}\|_{,}_{} _{a}|(a|s)-^{}(a|s)|\).

Finally, we conclude this section by introducing key quantities of the policy extraction for the subsequent analysis. Let \(B_{}_{}\|_{}/_{0}\|_{}\) denote the size of the policy class with respect to some fixed base policy \(_{0}\). Also let \(_{}_{}\|-^{*}\|_{1,(+ _{0})}\) be the function approximation error of \(\), where \(^{*}(s,a)^{*}(s,a)-^{*}(s)\) is the optimal advantage function.

### The Suboptimality Bound

Unifying the saddle-point problem and the policy extraction problem, we arrive at the aggregated loss function

\[()_{}()+_{ }(),\] (14)

where \(_{}()-_{v}K(v,f_{ })\) is the loss of \(f_{}\) as a dual solution (cf. Eq. (7)) and \(_{}() D_{}(f_{};w_{},_{ })\) is the loss of the policy extraction from \(f_{}\) to \(_{}\) (cf. Eq. (11)). Let us denote the corresponding estimation error by

\[_{}()()-_{ }().\] (15)

We also define the aggregated function approximation error with

\[_{,}(,,)(2+3B_{ })_{,}(, ())+3_{}+\{B_{}+(1-)B_{}\} _{}.\] (16)

The following theorem establishes an upper bound on the policy suboptimality in terms of \(_{}()\) and \(_{,}(,,)\).

**Theorem 6.1**.: _For all \(\), we have_

\[^{*}-(_{})^{_{}} \|_{}}{1-}\{_{}()+_ {,}(,,)\}.\] (17)

Proof (sketch).: At the heart of the proof is the following inequalities:

\[(_{})}^{*}(f_{})+_{ }()+B^{}_{}}{1-}}()+_{,}(,,) }{1-},\]

where \((_{})_{,_{}}[^{*}( s)-^{*}(s,a)]\) denotes the average action value gap with policy \(_{}\) and \(B^{} B_{}+(1-)B_{}\). Here, the lower clipping of \(\) (Eq. (11)) and the stability of the primal solution (Lemma 5.3) are instrumental in deriving the first and the second inequality, respectively. Then, the proof is completed by bounding \(^{*}-(_{})\) in terms of \((_{})\) invoking the performance difference lemma in the worst-case environment. See Appendix F.1 for the full proof. 

Theorem 6.1 suggests that one can minimize the policy suboptimality up to the function approximation error on two conditions, i.e., the weight factor \(\|^{_{}}\|_{}\) is appropriately bounded and the loss function \(()\) is minimized. In the following, we first discuss how to satisfy the first condition.

A trivial way of bounding \(\|^{_{}}\|_{}\) is uniformly bounding it with respect to all \(\). Define \(_{}_{}\|^{_{ }}\|_{}\), which we refer to as _the uniform truncated concentrability (UTC) coefficient_. Since \(\|^{_{}}\|_{}_{}\), we get the following simple suboptimality bound.

**Corollary 6.1**.: _For all \(\), we have_

\[^{*}-(_{})_{}}{1-} \{_{}()+_{,}( ,,)\}\] (18)

Note that \(_{}\) is _always_ finite because of the compactness of the whole policy space \(()^{}\) and the continuity and well-definedness of \(\|^{}\|_{}\). Thus, Eq. (18) is non-vacuous for _arbitrary_ data distributions as opposed to the conventional concentrability-based results. Moreover, if the conventional bounds are non-vacuous, \(_{}\) recovers the conventional concentrability coefficient as \(^{}=w^{}\).

Eq. (18) can be further refined using the _localized_ variants of the uniform coefficient \(_{}\). The results are presented as Corollaries F.2 and F.3 in Appendix F.3 due to space limitation.

### Sample Complexity Analysis

Now that given Corollary 6.1 (and Corollaries F.2 and F.3 as well) bounds the weight factor \(\|^{_{}}\|_{}\) with some milder variants of the concentrability coefficient, the remaining task, minimizing \(()\), is handled within the framework of the statistical learning, leading to a sample complexity bound. Let

\[}()_{v}_{} _{z}}_{z}(;v,),\] (19)

be the empirical loss function where

\[}_{z}(;v,) -\{(1-)\,v(s)+f_{}(s,a)\{r+ v(s^{ })-v(s)\}+(v^{2}(s)+ v^{2}(s^{ }))}{2}\}\] \[+f_{}(s,a)(s,a)-_{}(s)_{a^{ }_{}(s)}[(s,a^{})]\] (20)

is the one-sample loss function, \(z(s,a,r,s^{}) \) denotes a transition record. Note that Eq. (20) is an unbiased estimator of the objective function \(()\).6 Therefore, it is expected that the oracle loss \(()\) can be approximated with the empirical loss \(}()\) and hence the oracle estimation error \(_{}()\) can be approximated with the empirical estimation error

\[_{}()}()-_{ }}().\] (21)

Formalizing such an intuition, the following corollary shows the empirical counterpart of Corollary 6.1. See Appendix F.5 for the proof.

**Corollary 6.2**.: _Let \((,,)\{z}_{z}(;v,)\,|\,,v,\}\) be the class of the one-sample loss functions and \(_{n}()\) be its Rademacher complexity (Definition B.1). Then, for all \(\) and \((0,1)\), with probability \(1-\), we have_

\[^{*}-(_{})_{}}{1-} \{_{}()+_{,}( ,,)+4_{n}()+B_{}}\},\]

_where \(B_{}(1-)B_{}+B_{}_{ }+(1-)^{2}B_{}^{2}+(B_{}+B_{ {}}})B_{}\) is the aggregated scale factor._

The corollary above implies that minimizing Eq. (19), which is possible with the minimax optimizers such as the one developed by Thekumparampil et al. (2019), gives a near-optimal policy in terms of the worst-case environment \(}\), up to the error proportional to the sum of the optimization error \(_{}()\), the approximation error \(_{,}(,,)\) and the statistical error \(O(_{n}()+n^{-1/2})\). We refer to this method as _worst-case minimax reinforcement learning (WMRL)_.

The next corollary gives the sample complexity of WMRL in the simplified case where the function approximators \(\), \(\) and \(\) are all finite sets.

**Corollary 6.3**.: _Suppose \(\), \(\) and \(\) are all finite sets and \(_{}()=_{,}(, ,)=0\). Take any \(>0\) and \(0<<1\). Then, we have \(^{*}-(_{})\) with probability \(1-\) if_

\[n=(}^{2}_{}^{2}}{^{2}(1- )^{2}}}{}),\] (22)

_where \(||\,||\,||\) denote the product of the cardinalities of the function approximators._

Proof.: It follows from Corollary 6.2 with Massart's lemma (Lemma B.2). 

A few remarks follow. First, we can replace the UTC coefficient \(_{}\) with the localized variants (Definitions F.1 and F.2) to obtain tighter bounds, by making the same argument starting from Corollaries F.2 and F.3 instead of Corollary 6.1, respectively. Second, there are implicit dependencies \(B_{}\|^{*}\|_{}\) and \(B_{}\|^{*}\|_{}\) due to the realizability \(^{*}\) and \(^{*}\) that follows from \(_{,}(,,)=0\). Hence, Eq. (22) has an implicit \(\)-dependency through the scale factor \(B_{}\), which brings an extra \(((1-)^{-2})\) factor in the worst case. Table 1 adopts this form for the fairness of comparison.

## 7 Conclusion

To develop an offline RL method for challenging data distributions, we have introduced and studied a generalization of the conventional framework called _worst-case offline RL_. As a result, we have shown it is possible to learn a worst-case optimal policy without any data-support conditions. Moreover, the presented sample complexity bound strictly improves the previous state of the art under the single-policy realizability and the single-policy concentrability, suggesting the utility of the proposed method even with non-challenging data distributions.

We anticipate the presented results are readily extendable to continuous state-action spaces, except that the truncated concentrability coefficients are not unconditionally finite anymore. The results in Appendix F.3 are particularly useful in this context, yet the complete picture on the conditions of their boundedness largely remains to be studied in future work.