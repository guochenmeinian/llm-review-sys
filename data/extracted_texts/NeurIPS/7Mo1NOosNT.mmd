# COLD: Causal reasOning in cLosed Daily activities

Abhinav Joshi &Areeb Ahmad1 &Ashutosh Modi

Department of Computer Science and Engineering

Indian Institute of Technology Kanpur (IIT Kanpur)

Kanpur, India

{ajoshi,areeb,ashutoshm}@cse.iitk.ac.in

Equal Contribution

###### Abstract

Large Language Models (LLMs) have shown state-of-the-art performance in a variety of tasks, including arithmetic and reasoning; however, to gauge the intellectual capabilities of LLMs, causal reasoning has become a reliable proxy for validating a general understanding of the mechanics and intricacies of the world similar to humans. Previous works in natural language processing (NLP) have either focused on open-ended causal reasoning via causal commonsense reasoning (CCR) or framed a symbolic representation-based question answering for theoretically backed-up analysis via a causal inference engine. The former adds an advantage of real-world grounding but lacks theoretically backed-up analysis/validation, whereas the latter is far from real-world grounding. In this work, we bridge this gap by proposing the **COLD** (**Causal reasOning in cLosed Daily activities**) framework, which is built upon human understanding of daily real-world activities to reason about the causal nature of events. We show that the proposed framework facilitates the creation of enormous causal queries (\( 9\) million) and comes close to the mini-turing test, simulating causal reasoning to evaluate the understanding of a daily real-world task. We evaluate multiple LLMs on the created causal queries and find that causal reasoning is challenging even for activities trivial to humans. We further explore (the causal reasoning abilities of LLMs) using the backdoor criterion to determine the causal strength between events.

## 1 Introduction

In recent times, Large Language Models (LLMs) have shown remarkable generalization capabilities (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020). Consequently, the ability to perform causal reasoning (often considered a core feature of intelligence (Penn and Povinelli, 2007; Pearl and Mackenzie, 2018)) has sparked research interest in the context of LLMs, aiming to answer if causal reasoning is possible with LLMs (Weber et al., 2020; Jin et al., 2023, 2024; Cohrs et al., 2023; Romanou et al., 2023; Yang et al., 2023; Mitchell et al., 2023; Vashishtha et al., 2023; Stolfo et al., 2023). On a broader level, there are two lines of work; first, that treats the causal reasoning via learning relationships between the events that are grounded in the real world (Gordon et al., 2012; Ho et al., 2022; Zecevic et al., 2023; Zhang et al., 2023; Wang et al., 2023). Second line of work relies on a causal inference engine and establishes relationships between variables via symbolic representation (Jin et al., 2023, 2024). The former relies on understanding real-world events but lacks formal definitions that adhere to the causal inference theory. The latter solves the issue using a causal inference engine but uses symbolic representations not grounded in the world, making the causal queries more like a test for the understanding of causal theory. Though the first line of work includes real-world events, the causal queries are often limited and could be answered by memorizing the causal relationships between the events. Recent findings that include rigorous analysis using a causalinference engine claim LLMs to be _"Causal Parrots"_(Zeeevic et al., 2023), i.e., the LLMs tend to pick up (memorize) patterns in the training data to perform well on the causal reasoning benchmarks. Moreover, some initial findings by Tang et al. (2023) suggest that LLMs perform significantly better when semantics are consistent with commonsense but struggle to solve symbolic tasks, pointing towards semantic representation to be better for proper validation of LLMs, leading to a conclusion that an in-depth analysis using real-world events is necessary.

In this work, we bridge the gap between the two approaches by proposing the **COLD** (**Causal reasOning in closed Daily activities**) framework, based on the human understanding of real-world daily activities capturing commonsense (for example, "making coffee," "boarding an airplane," etc.), that adheres to causal theory literature. It is more natural to frame real-life reasoning-based queries via language; consequently, we follow the literature on Causal Commonsense Reasoning (CCR), which studies the relationships between real-world events (described via natural language).

CCR is a non-trivial task of estimating the cause-and-effect relationship between events that are studied under the umbrella of commonsense reasoning (Kuipers, 1984; Gordon et al., 2012; Zhang et al., 2022; Wang et al., 2023; Chun et al., 2023; Du et al., 2022). The events in CCR generally refer to actions taking place in an activity in the real world. For example, consider the activity of "traveling by an airplane" given in Fig. 1, where the occurrence of all the events is confounded by a universal variable \(\) ("intention to perform a task"). Moreover, there are a few events that cause one another. For example, the event "checking in luggage" (\(E_{1}\)) caused the occurrence of events like "waiting at the luggage belt" (\(E_{2}\)) after the flight, i.e., in an alternate universe where one does not checks in luggage and goes with the cabin bags, will never wait for their luggage after the flight has landed. Moreover, some of the events have no causal impact, like "find the boarding gate" (\(E_{3}\)) has no causal relationship with "checking in luggage" (\(E_{1}\)). More formally,

\[_{(E_{1} E_{2})} =(E_{2}|do(E_{1}))-(E_{2}|do( E_{1}))\] (1) \[_{(E_{1} E_{3})} =(E_{3}|do(E_{1}))-(E_{3}|do( E_{1}))\]

where \(do(.)\) denotes the **do** operator (Pearl, 2012) showing the intervention on \(E_{1}\), and \(\) is the _causal estimand_ capturing the causal strength between two events, i.e., \((E_{1} E_{2})\) is expected to be higher when compared to \((E_{1} E_{3})\). Note, CCR excludes the causal events that are beyond the reach of commonsense knowledge, for example, does "planting trees" have a direct impact on the "rainy season"? or does "providing free education" improve the "economic condition of the country/state"; does "carpooling" directly impact "air pollution", etc. A noteworthy point concerning causality is that though the logical temporal (or prototypical) order of these events provides a weak signal about causal relationships, temporal precedence does not always imply causation (SS2). For example, one could erroneously argue that "boarding a plane" is also the cause of "waiting at the luggage belt" since without "boarding a plane," one cannot wait for the luggage belt.

For building a causal reasoning framework (based on CCR) around real-life daily activities, one would require a few primary features readily available: 1) Clear distinction between the events, i.e., the events should encapsulate describing a particular step in an activity, 2) Causal Dependency between the variables/events, i.e., there should be some events causing other events to occur. 3) Causal independence of events with the rest of the world, i.e., the occurrence of events should be independent of events that are not part of the activity (i.e., the covariates are balanced out). We found that "_Scripts_"(Schank, 1975; Schank and Abelson, 1975) provide a concrete medium that satisfies all these requirements. Scripts are defined as a sequence of events describing a prototypical activity, such as going to a restaurant, and hence capture commonsense knowledge about the world. (Schank and Abelson, 1975; Modi et al., 2016; Wanzare et al., 2016; Ostermann et al., 2018; Modi, 2016, 2017; Modi et al., 2017; Modi and Titov, 2014). Moreover, different people have similar understandings of

Figure 1: \(\) denotes the unobserved variables, confounding all events present in a real-world activity. In an activity, some events cause other events to happen. For example, in _“traveling by an airplane”_, the event of _“check-in luggage”_ causes events like “taking back luggage.”

the activity in the form of scripts that inherently balance out the covariates present in the real world, i.e., all the activities have the same starting and ending point and account for common exogenous and endogenous variables, providing a suitable platform to establish a cause-and-effect relationship between the events. In other words, for an activity like "flying in an airplane," or "going grocery shopping" (also see Fig. 2, left) the events that happened before starting the activity and after completing the activity are marginalized out using a common understanding of these activities by different humans and hence will have no causal relations with any of the exogenous events during the activity. Creating a causal graph for script knowledge, i.e., establishing relationships between events taking place during the activity, provides a perfect platform for creating causal queries, thus providing a medium to establish CCR between events. In a nutshell, we make the following contributions:

* We propose **COLD (Causal reasOning in cLosed Daily activities)**, a CCR framework based on Script knowledge (daily activities involving commonsense) that provides a closed system to test the understanding of causal inference grounded in the real-world. The proposed framework adheres to SUTVA (Stable Unit Treatment Value Assumption) (Cox, 1958; Rubin, 1980) by design (SS3). **COLD** consists of activity-specific observational graphs (created via crowd-sourcing) and causal graphs. Further, **COLD** facilitates creating an enormous number of causal queries (e.g., \(2,887,950\) per activity) via causal query triplets from the causal graph. This comes close to the mini-Turing test (Pearl and Mackenzie, 2018), where the story becomes the understanding of the daily activity, and the sampled enormous causal queries help in the exhaustive and rigorous evaluation of LMs.
* We devise various design mechanisms for estimating causal strength analytically and show how the representations learned by language models can be validated.
* Via detailed experimentation on the widely used open-weight language models, including encoder-only models (RoBERTa-MNLI) and autoregressive models (gpt-neo-125M, gpt-neo-1.3B, gpt-neo-2.7B, phi-2, gpt-j-6B, Llama-2-7b-chat-hf, Mistral-7B-v0.1, gamma-7b, and Meta-Llama-3-8B) we estimate the causal reasoning capability of the learned representations. We release the framework, model code, and results via https://github.com/Exploration-Lab/COLD.

Figure 2: **Left:** the figure represents the closed nature of daily real-world activities (capturing commonsense, commonly understood by humans), start and end given the context of the task, i.e., the pre-activity world and post-activity world activities marginalize out the dependence of event occurring during the activity with the rest of the world. **Right:** Causal Graph for “going grocery shopping.” Notice the collider (red nodes) makes the independent set of nodes (highlighted in different colors) unconditionally independent in the causal graph. In contrast, when given a condition on a collider (“put bags in cart”, the two clusters (yellow and blue) become dependent (if collider is observed, both yellow and blue clusters may have been observed as well).

## 2 Background

**The Mini Turing Test** proposed by Pearl and Mackenzie (2018) is designed in a question-answering format to validate the understanding of causal knowledge about a simple story. The primary feature of a mini-Turing test is the enormous number of causal queries that can be framed using the underlying causal graph, which governs the occurrence of events in the story. Due to the enormous number of causal queries, passing the mini-Turing via memorization becomes combinatorially heavy, and hence, the authors argue that it can only be beaten if one has access to the underlying causal graph governing the occurrence of events (i.e., one has the ability to reason causally about the events). In this work, though, we only consider a more straightforward case of choice-based causal triplets; we realize the number of causal queries that can be created is enormous and helps validate the causal reasoning abilities coming close to the mini-Turing test.

_d-separation:_ Establishing the independence of variables becomes non-trivial when dealing with complex interactions among multiple variables. _d_-separation (Pearl, 1988) facilitates the determination of conditional independence between two sets of nodes \(\) and \(\) in a graphical model \(\) given another set of nodes \(\). _d_-separation asserts that \(\) and \(\), given the set \(\), are \(d\)-separated if all paths for every node in \(\) and every node in \(\) are blocked by conditioning on \(\), denoted as \([origin={c}]{$$}_{} \). A path \(p\) is blocked by a set of nodes \(\)(Pearl et al., 2016), if and only if: 1) \(p\) contains a chain of nodes \(\) or a fork \(\) such that the middle node \(\) is in \(\) OR 2) \(p\) contains a collider \(\) such that the collision node \(\) or its descendant is not in \(\).

**Backdoor Criterion:** A set of variables \(W\) satisfies the backdoor criterion relative to \(T\) and \(Y\) if the following are true:

1. [label=()]
2. \(W\) blocks all backdoor paths from \(T\) to \(Y\) i.e. blocking confounding or non-causation association paths
3. \(W\) doesn't contain any descendants of \(T\)

Then, \(W\) satisfies the backdoor criterion (Pearl et al., 2016; Neal, 2020). We make use of the backdoor criterion to estimate the causal estimand, capturing the relationship between the causal events. (Refer App. C for more detail)

## 3 COLD (Causal reasOning in cLosed Daily activities)

We propose **COLD** (**Causal reasOning in cLosed Daily activities**) framework for testing causal reasoning abilities of natural language understanding systems such as LLMs. Fig. 3 gives an overview of the creation process. We use crowd-sourced data of script knowledge to create observational

Figure 3: The proposed **COLD** framework for evaluating LLMs for causal reasoning. The human-written Event Sequence Descriptions (ESDs) are obtained from crowdsource workers and include a telegrammic-style sequence of events when performing an activity. The Observational Graph and the Causal Graph for an activity are used to create causal query triplets (details in Algorithm 1), shown towards the right. Using counterfactual reasoning, “going to the kitchen” is possible without going to the market (if the ingredients are already available), making “come home with the ingredients.” a more plausible effect among the given choices. Similarly, in the second example, the event “going to market” has no direct relation with the event “heating the oven”.

graphs which is further used along with manual intervention to create causal graphs. Subsequently, an algorithm is used to create an enormous number of causal queries (causal triplets), which are further used to test LLMs for causal reasoning. Next, we explain each of the steps in more detail.

**Task Formulation: COLD** is motivated by Causal Commonsense Reasoning (CCR), which we define as the task of finding the strength of the cause-and-effect relationship between two events (\(E_{1}\) and \(E_{2}\)) given in an activity \(a\), where \(\) is the set of all activities. For example, for an activity like "going in an airplane", the central question is to determine the causal relationship between two events that occur during the activity (events like "checking in luggage" and "waiting for luggage"). Since reasoning about a sequence of events is tedious (and sometimes confusing (Do et al., 2011)), researchers often rely on a more plausible cause rather than defining a definite causal event. For instance, COPA dataset (Gordon et al., 2012) provides a premise event and a corresponding causal query question along with two choices (see Table 1 for an example); a system is required to predict which of the two choices is most plausible cause/effect as required by the question.

**Creating a Closed Causal System**

Given the nature of Script knowledge (satisfying the criterion of balanced covariates, SS1), we use a script corpus called DeScript (Wanzare et al., 2016) for creating the observational graphs. DeScript is a corpus with a telegram-style sequential description of an activity in English (e.g., baking a cake, taking a bath, etc.). DeScript is created via crowd-sourcing. For a given activity, crowd-workers write a point-wise and sequential short description of various events involved in executing the activity (this one complete description is called an ESD (Event Sequence Description)). DeScript collects data for a set of 40 daily activities (100 ESDs each) varying in complexity and background knowledge. Additionally, for a given activity, semantically similar events from different ESDs are manually aligned by human annotators (for more details, refer to Wanzare et al. (2016)). These alignments were later used by Joshi et al. (2023, 2023) to create a DAG representing the overall activity. In our work, we use these DAGs as the observational distribution of an activity (\(_{o}^{(a)}\), where \(a\), where \(\) is the set of all activities). These DAGs provide a medium for generating enormous trajectories (scales from \(1.6e+16\) to \(1.3e+27\), also see Table 2), that are coming directly from human annotations (alignment as well as the ESDs), providing us a proxy to represent the understanding of daily activities.

**Observational Distribution** (\(_{o}\)): Note that the graphs \(_{o}\), approximately represent (almost) all possible ways in which an ESD can be written for an activity, providing the true observational distribution, i.e., how the combinations of events will look like while performing the activity in the real world (see App. A.3 for examples).

**Causal Graphs (\(_{c}\)): To reason about the causal relationships between the events (nodes of \(_{o}\)), we would need the underlying causal graph that shows the cause of occurrence of various activities (directly or indirectly). We construct the causal graphs manually by reasoning about the independence of various events in the activity. Fig. 2 shows the pictorial representation of one of the created causal graphs for the activity "going grocery shopping." Notice that various sets of events in the graph create clusters, denoting independence between various events. For example, nodes related to make list cause the events that involve the presence of a list and do not cause events like going via car (as some of the population will not create a list for shopping). Similarly, the mode of transportation (car/bus/walk) is independent of the events performed inside the store.**

**Causal Query Triplets: The obtained Causal Graph (\(_{c}\)), for an activity provides a medium to reason about causal links between the events. Notice in Fig. 2, how the red nodes (colliders) help separate out the independent event clusters. For example, the nodes 'get list from car' and 'check list (if anything is left)' being colliders, separate out the making list-related events with the rest of the graph. Similarly, the node 'put bags in cart' separates out the 'take shop cart' and 'take bags'. Another interesting property represented in the obtained causal graph is the conditional dependence between various clusters. For example, the cluster related to 'get in car' is unconditionally independent of'make list'. However, if we condition on the collider ('get list from car'), they become dependent, i.e., if 'get list from car' is observed, it means that the person will have created the list as well as went by car for the grocery shopping (similarly for node 'put bags in cart'). \(d\)-separation (SS2) provides an easier way to establish conditional/unconditional independence between the set of nodes. For creating the dataset of causal queries (similar to other datasets like COPA (Gordon et al., 2012)), we need a triplet of three events (premise \(p\), Choice-1 \(c_{1}\), Choice-2 \(c_{2}\)) associated with a question about 'cause' or 'effect' relationship, i.e., given the premise which of the two choices is the cause/effect? (Table 1). We call these triplets _Causal Query_

[MISSING_PAGE_FAIL:6]

two versions occurring and not occurring. SUTVA plays a vital role in causal inference by ensuring that each unit's treatment assignment has a consistent impact, facilitating the accurate estimation of treatment effects. Our framework closely adheres to SUTVA assumptions (details in A.1).

**Comparison with Other Causal Datasets:** We briefly compare the created dataset with the existing set of causal reasoning datasets in App. Table 5. The created dataset serves as a middle ground, having both real-world groundings as well as an underlying causal graph to create an exhaustive set of causal queries.

## 4 Experiments and Results

**COLD** provides a causal query dataset for evaluating LMs for causal understanding. In particular, we consider the "Causal Query Triplets" (Table 2) coming from compact trajectories as a base and sample the instance version coming from the same skeleton. Since it is not possible to evaluate all the possible causal queries that could be created using our framework, we use \(10K\) samples for each activity to report our findings. For a fair comparison between various models and better reproducibility, we freeze the sampled causal query triplets and compare the success rate over the frozen samples. We evaluate via two methods. First, as done in previous work Jin et al. (2024, 2023); Chen et al. (2024), we first experiment with various LLMs using a prompt-based evaluation scheme; second, we propose other mechanisms (based on causal theory, e.g., Average Treatment Effect) that could be used to perform an in-depth analysis of evaluating causal relationships between events.

**Causal Reasoning Evaluation of LLMs via Prompts:** We start with the prompt-based evaluation of recent open-weight LLMs (gpt-neo-125M, gpt-neo-1.3B, gpt-neo-2.7B (Black et al., 2021), gemma-2b (Team et al., 2024), phi-2 (Javaheripi et al., 2023), gpt-j-6B (Wang and Komatsuzaki, 2021), gamma-7b (Team et al., 2024), Llama-2-7b-chat-hf (Touvron et al., 2023), Mistral-7B-v0.1 (Jiang et al., 2023), and Meta-Llama-3-8B (Dubey et al., 2024)) We frame the prompt as a multi-choice question-answering (MCQA) objective (Robinson and Wingate, 2023). The prompt is intentionally structured so that the LLM is intended to predict a single choice token (Such as "A", "B", etc.). Robinson and Wingate (2023) highlight the advantages of MCQA-based evaluation over cloze evaluation (Brown et al., 2020)(where the LLMs are expected to generate the entire answer in a cloze test), leading to a significant boost in various tasks, including commonsense-based tasks. App. E, Fig. 5 presents various prompt templates for autoregressive experiments, and App. E Fig. 6 shows a few qualitative examples for the framed causal query templates. Table 3 shows the success rate obtained for various LLMs. The success rate corresponds to the percentage of queries where the LLM predicts the desired choice. We observe that reasoning causally about simple daily activities is challenging when a rigorous test is framed, validating the dependencies between the events. Overall, for the more common activities like baking a cake and going grocery shopping, the LLMs perform better when compared to activities like boarding a bus or planting a tree. We also experimented with another version of the dataset, where incorrect choice may correspond to temporally plausible but causally implausible events. The results drop significantly in this case; details and results are provided in the App. F.1.

**Evaluation using Average Treatment Effect (ATE) (\(\)):** Computing the Average Treatment Effect (\(\)) helps establish the strength of causal links given a context (Eq. 1). In our setup, to estimate

  
**Triplets** & **Model Name** & **cake** & **shopping** & **train** & **tree** & **bus** \\   & gpt-neo-125M & 50.71 & 50.01 & 49.99 & 50.13 & 50.15 \\  & gpt-neo-1.3B & 44.77 & 45.69 & 42.52 & 45.67 & 42.89 \\  & gemma-2b & 53.76 & 52.19 & 60.57 & 60.71 & 53.64 \\  & gpt-neo-2.7B & 50.00 & 50.01 & 50.00 & 50.01 & 50.00 \\  & phi-2 & 85.14 & 83.65 & 77.29 & 82.24 & 71.74 \\  & gpt-j-6B & 49.59 & 50.02 & 50.29 & 49.92 & 49.93 \\  & Llama-2-7b-chat-hf & 77.92 & 72.41 & 73.48 & 72.40 & 68.21 \\  & Mistral-7B-v0.1 & 77.64 & 69.38 & 68.46 & 72.43 & 69.37 \\  & gemma-7b & 81.47 & 82.26 & 77.24 & 80.78 & 70.29 \\  & Meta-Llama-3-8B & 80.79 & 76.46 & 76.08 & 78.21 & 67.39 \\   

Table 3: The table provides evaluation results of language models over the created causal triplets.

\(P(y|do(x))\) (i.e., the causal estimand) from statistical estimands (obtained from observational distribution), we make certain reasonable assumptions about the underlying process that governs the relation among variables/events and then utilize the implications of these assumptions. For any activity taking place, the causal relationships between two events \(E_{1}\) and \(E_{2}\) may have a causal link along with a non-causal link through a set of confounders \(z\). We define the confounder \(z=\{t_{i}|t_{i}\}\), where \(\) denotes all the trajectories (sequence of events) from the start of the activity till the event \(E_{1}\). The temporal nature of events makes this assumption suitable since the occurrence of \(E_{1}\) and \(E_{2}\) can be confounded by all the events preceding \(E_{1}\). Note the possibility of unobserved confounders (events that are not explicitly mentioned but may be affecting the mentioned events) in our case is removed due to two reasons: 1) Keeping a closed system representation with a large number of diverse scripts (written by humans) helps cover the set of most generic and diverse events either implicitly or explicitly as a part of the activity, and 2) The causal reasoning goal is restricted to figuring out the causal effect between the events that are present explicitly. Assuming the unmentioned events have insignificant effects, we can establish that there will not be any unobserved confounders. This assumption makes the observed confounders satisfy the backdoor criterion (Pearl, 1993) that make sufficient adjustment sets. By using the backdoor criterion (App. C), the interventional distributions are estimated as follows:

\[P(E_{2}|do(E_{1}))=_{t_{i}}p_{*}(E_{2}|E_{1},z=t_{i})p_{*}(z =t_{i})\] (2)

Note that the true observational distribution, i.e. \(p_{*}(E_{2}|E_{1},z)\) and \(p_{*}(z)\) both are unknown and have to be approximated (\((E_{2}|E_{1},z)\) and \((z)\)). Further, we describe ways \(\) can be estimated via multiple design mechanisms. Due to space limitation, we only describe the \(\) estimation via language models below and move the statistical analysis using the original trajectories and observational graphs to the App. D.2.

**ATE using Language Models** Since pre-trained LMs capture world knowledge (Devlin et al., 2019; Brown et al., 2020; Li et al., 2023; Nanda et al., 2023; Karvonen, 2024), these provide a suitable proxy for establishing relationships between these events. For our experiments, we consider a simple reasoning capability of language models, i.e., to reason about the temporal order of various events, i.e., given an event, what is the likelihood of the occurrence of another event? We further ask if this can be used to estimate the causal relationship between the events (a similar strategy is used by Zhang et al. (2022) for zero-shot causal estimation). It is worth noting that for these activities about daily activities, one way to find causes is to establish the temporal likelihood of the events. For each of the multiple language models, we frame the temporal prediction differently.

**Encoder-only Models:** For BERT-based models trained for mask token prediction, we model the temporal prediction using the probability assigned to the mask tokens "_before_" and "_after_" (Zhang et al., 2022). Given two events \(E_{1}\) and \(E_{2}\), the temporal link is predicted using a prompt like \(E_{1}\) <mask> \(E_{2}\), and the scores corresponding to the before and after tokens are collected. App. D, Fig. 7, the top row highlights the prompt template used for BERT-based models. For encoder-only experiments, we consider RoBERTa MNLI (Liu et al., 2019).

**Decoder-only Models:** For other language models that are autoregressive in nature, we modify the prompt to predict the temporal order as the last token. We again use the MCQA-based prompting style to frame the temporal order query by providing "_before_" and "_after_" as the options in the prompt. App. D, Fig. 7, the bottom row highlights the prompt template used for Decoder-only Models.

**Interventions:** We utilize the SUTVA assumption in the proposed framework to devise an intervention over a trajectory in natural language form. App. D, Fig. 8 shows the style of intervention made by an event (\(E_{1}\)) taking place (\(do(E_{1})\)) or not taking place \(do( E_{1})\).

Given the above strategies, LMs can be used to evaluate \((E_{2}| E_{1},z=t)\), by feeding the prompt that contains \(E_{1}\), \(E_{2}\) and the \(z=t)\) and predict the temporal nature between \(E_{1}\) and \(E_{2}\), given a trajectory \(z=t\). Further, applying the backdoor criterion for multiple trajectories \(\), we obtain

\[ p_{}(E_{2}|do(E_{1}))&= |}_{t}(E_{2}|E_{1},z=t)\\ p_{}(E_{2}|do( E_{1}))&=|}_{t}(E_{2}| E_{1},z=t)\] (3)which can further be used to estimate the causal strength between the events \(E_{1}\) and \(E_{2}\).

\[_{}=p_{}(E_{2}|do(E_{1}))-p_{}(E_{2}|do(  E_{1}))\]

Using the multiple \(\) estimates defined above, we estimate the causal strength between the events available for an activity. We follow the scheme presented in the App. B, Algorithm 2 to compute the performance in terms of success rates.

**Temporal Scheme:** In this scheme, we validate if temporal ordering knowledge of LLMs could be directly used to estimate the causal estimand. We make use of templates shown in the App. Fig. 7. The causal estimand is estimated via the difference in logit values when intervening over an event, i.e., does the predicted probability take into account the context of events not happening? Surprisingly, we found that temporal ordering does provide a suitable proxy for estimating causal strength between the events. We further extend this approximation to incorporate the backdoor adjustments in the \(\).

**Backdoor Adjustments:** For the experiments with Language models, we apply the backdoor adjustment to estimate the causal estimand \(_{}\). App. Fig. 8 shows the prompt template used to determine the relationship between the events. The prompt template takes a trajectory, \(t_{i}\), that contains all the events till the event \(E_{1}\) in sequential order of occurrence, further, an added prompt determines the intervention (\(do(E_{1})\) or \(do( E_{1})\)) and the causal estimand is estimated using the logit values associated with the predicted token. App. B, Algorithm 3 provides the designed scheme to compute unbiased causal estimands. We essentially flip the options and generate the scores associated with options 'A' and 'B' for increase and decrease, respectively (more details in the App. C.)

Table 4 shows a comparison between various design choices. We observe that when using LLMs for \(\) estimation, the backdoor adjustment increases the performance over the temporal estimation scheme by a significant margin. The understanding of these activities is generic, and LLMs do provide a suitable set of sequences when prompted to generate a list of steps to complete the activity. For example, when prompted with 'Generate the sequential steps in a telegrammic style to perform the activity "going grocery shopping"', almost all the models we tested provide

   \)**estimation**} &  &  &  &  &  &  &  \\  Original Trajectories & & \(_{o}\) & 28.20 & 34.30 & 31.10 & 30.10 & 30.40 \\   & - & \(_{n}\) & 30.40 & 30.10 & 29.80 & 28.60 & 25.4 \\  & - & \(_{t}\) & 40.90 & 47.10 & 40.30 & 37.60 & 40.10 \\   & & RoBERTa MNLI & 46.80 & 54.00 & 45.50 & 52.70 & 43.00 \\  & & gpt-neo-125M & 47.70 & 55.50 & 55.50 & 53.60 & 48.20 \\  & & gpt-neo-1.3B & 47.40 & 45.40 & 53.30 & 43.40 & 52.90 \\  & & gemma-2b & 43.80 & 41.70 & 52.20 & 49.70 & 49.80 \\  & & gpt-neo-2.7B & 50.10 & 48.90 & 52.40 & 47.60 & 53.70 \\  & & phi-2 & 60.30 & 59.20 & 56.90 & 70.30 & 49.40 \\  & & gpt-j6B & 49.50 & 46.40 & 56.00 & 62.70 & 56.00 \\  & & Llama-2-7b-chat-hf & 38.90 & 42.10 & 51.00 & 40.70 & 47.80 \\  & & Mistral-7B-v0.1 & 50.90 & 54.40 & 64.50 & 60.50 & 62.30 \\  & & gemma-7b & 46.8 & 54.00 & 45.50 & 52.70 & 43.00 \\  & & Meta-Llama-3-8B & 58.20 & 54.10 & 55.6 & 55.00 & 64.00 \\   & & \(_{}\) (RoBERTa MNLI) & 59.20 & 54.40 & 56.30 & 57.50 & 53.30 \\  & & \(_{}\) (gpt-neo-125M) & 59.20 & 55.10 & 50.50 & 52.10 & 45.50 \\  & & \(_{}\) (gpt-neo-1.3B) & 51.30 & 50.70 & 55.00 & 43.90 & 49.00 \\  & & \(_{}\) (gpt-neo-2.b) & 44.50 & 45.30 & 52.60 & 63.50 & 43.90 \\  & & \(_{}\) (gpt-neo-2.7B) & 49.10 & 51.30 & 51.50 & 54.00 & 51.40 \\  & & \(_{}\) (phi-2) & 57.00 & 66.00 & 62.10 & 57.10 & 45.80 \\  & & \(_{}\) (gpt-j-6B) & 51.30 & 45.60 & 50.50 & 49.10 & 46.00 \\  & & \(_{}\) (Llama-2-7b-chat-hf) & 62.60 & 64.60 & 68.50 & 70.50 & 63.80 \\  & & \(_{}\) (Mistral-7B-v0.1) & 63.90 & 71.40 & **73.70** & 61.30 & **67.00** \\  & & \(_{}\) (gamma-7b) & **72.80** & **77.80** & 73.60 & **71.90** & 62.40 \\  & & \(_{}\) (Meta-Llama-3-8B) & 66.00 & 70.20 & 68.40 & 62.00 & 63.40 \\   

Table 4: Accuracy over the causal triplets for various \(\) estimates. The blue text denotes the improvements by backdoor adjustments over the temporal scheme for multiple language models. **Bold text** represents the best-performing method for a particular activity.

a valid set of steps for the given activity. However, when prompted with causal queries, the lower performance signifies the lack of understanding of the underlying causal mechanism. The constructed dataset helps to rigorously validate the understanding of the activity through an enormous number of causal query triplets. The results show that although the LLMs can explain the activity in detail, including generating correct steps for performing tasks, causally reasoning about the set of events remains challenging.

**Human Study:** We conducted a small-scale human validation study over the created causal query dataset and asked 5 graduate students to answer 100 randomly sampled causal query triplets (20 per activity). We record an average performance of 92.20%. (More details about the human study are provided in the App. B)

## 5 Related Work

Causal reasoning has been an active research area in the ML community (Spirtes et al., 2000a; Peters et al., 2017; Scholkopf et al., 2021). Some of the initial works highlight the causal nature of events present in text (Schank, 1975) as _'causal chains'_. Multiple works have considered creating benchmarks/datasets that capture causal relationships between the events described in the text (see App. Table 5). More recently, with the rapid growth of LLMs on reasoning/understanding tasks, attention has shifted to validating these general-purpose models capturing causal reasoning (Jin et al., 2023; Zecevic et al., 2023; Willig et al., 2023; Liu et al., 2023; Willig et al., 2023; Zhang et al., 2022; Jin et al., 2024). App. A.2 Table 5 shows a broad overview of the existing causal Dataset/Benchmarks presented in the NLP community. In this work, the primary focus is to bridge the gap between various lines of work that consider natural language to learn/validate/reason about causal relationships between events.

## 6 Limitations and Future Directions

One of the primary limitations of our work is the limited set of activities. Though the frameworks support generating exhaustive/enormous causal queries, finding general commonsense reasoning activities/tasks that are well understood by humans remains challenging. Moreover, creating a causal graph for an activity increases as we move toward more long-term tasks. However, as a general test of causal intelligence, our framework provides a suitable platform to validate the reasoning capabilities more rigorously. In the future, it would be interesting to sample trajectories from the observational distribution \(^{a}_{o}\) to create a training dataset and check if causal reasoning ability can be acquired by language modeling objectives (including other variants like presented in Lampinen et al. (2023)). We leave this detailed analysis for future endeavors. The proposed algorithm for causal triplet generation generates the simplest variant of causal queries in the form of causal triplets (also referred to as Pairwise Causal Discovery (PCD) task by (Chen et al., 2024)). More complicated causal queries can be generated, such as considering cases with common confounders, long/short causal chain dependency, etc. Moreover, taking formal definitions. (i.e., using the formal causal inference language) causal queries inspired from Jin et al. (2023, 2024) can be framed for a more rigorous analysis. Being at the initial state, we stick to the simple causal queries that provide two choices, and the task is to choose the more plausible cause. The creation of underlying causal graphs provides endless possibilities for creating varied versions of causal queries. In this work, we only consider an unconditional version of _d_-separation. In the future, the same causal graphs could be used to define more datasets for covering other rungs of the _'causal ladder'_(Pearl and Mackenzie, 2018).

## 7 Conclusion

In this paper, we proposed the **COLD** (**Causal reasOning in cLosed Daily activities**) framework for generating causal queries that can be used to rigorously evaluate LLMs. We performed extensive experimentation with LLMs for the task of Causal Commonsense Reasoning. Results indicate that LLMs are still far from a complete understanding of daily commonsensical activities and fail to answer causal queries when analyzed in an exhaustive manner. We believe this framework will provide a good platform for future research in understanding the causal reasoning abilities of LLMs.