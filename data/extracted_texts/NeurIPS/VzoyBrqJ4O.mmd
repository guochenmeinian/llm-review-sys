# Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective Distillation and Unlabeled Data Augmentation

Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective Distillation and Unlabeled Data Augmentation

 Ning-Hsu Wang

albert.nhwang@gmail.com &Yu-Lun Liu

Department of Computer Science

National Yang Ming Chiao Tung University

yulunliu@cs.nycu.edu.tw

###### Abstract

Accurately estimating depth in 360-degree imagery is crucial for virtual reality, autonomous navigation, and immersive media applications. Existing depth estimation methods designed for perspective-view imagery fail when applied to 360-degree images due to different camera projections and distortions, whereas 360-degree methods perform inferior due to the lack of labeled data pairs. We propose a new depth estimation framework that utilizes unlabeled 360-degree data effectively. Our approach uses state-of-the-art perspective depth estimation models as teacher models to generate pseudo labels through a six-face cube projection technique, enabling efficient labeling of depth in 360-degree images. This method leverages the increasing availability of large datasets. Our approach includes two main stages: offline mask generation for invalid regions and an online semi-supervised joint training regime. We tested our approach on benchmark datasets such as Matterport3D and Stanford2D3D, showing significant improvements in depth estimation accuracy, particularly in zero-shot scenarios. Our proposed training pipeline can enhance any 360 monocular depth estimator and demonstrates effective knowledge transfer across different camera projections and data types. See our project page for results: albert100121.github.io/Depth-Anywhere.

## 1 Introduction

In recent years, the field of computer vision has seen a surge in research focused on addressing the challenges associated with processing 360-degree images. The widespread use of panoramic imagery across various domains, such as virtual reality, autonomous navigation, and immersive media, has underscored the need for accurate depth estimation techniques tailored specifically for 360-degree images. However, existing depth estimation methods developed for perspective-view images encounter significant difficulties when applied directly to 360-degree data due to differences in camera projection and distortion. While many methods aim to address depth estimation for this camera projection, they often struggle due to the limited availability of labeled datasets.

To overcome these challenges, this paper presents a novel approach for training state-of-the-art (SOTA) depth estimation models on 360-degree imagery. With the recent significant increase in the amount of available data, the importance of both data quantity and quality has become evident. Research efforts on perspective perceptual models have increasingly focused on augmenting the volume of data and developing foundation models that generalize across various types of data. Our method leverages SOTA perspective depth estimation foundation models as teacher models and generates pseudo labels for unlabeled 360-degree images using a six-face cube projection approach. By doing so, we efficiently address the challenge of labeling depth in 360-degree imagery by leveraging perspective models and large amounts of unlabeled data.

Our approach consists of two key stages: offline mask generation and online joint training. During the offline stage, we employ a combination of detection and segmentation models to generate masks for invalid regions, such as sky and watermarks in unlabeled data. Subsequently, in the online stage, we adopt a semi-supervised learning strategy, loading half of the batch with labeled data and the other half with pseudo-labeled data. Through joint training with both labeled and pseudo-labeled data, our method achieves robust depth estimation performance on 360-degree imagery.

To validate the effectiveness of our approach, we conduct extensive experiments on benchmark datasets such as Matterport3D and Stanford2D3D. Our method demonstrates significant improvements in depth estimation accuracy, particularly in zero-shot scenarios where models are trained on one dataset and evaluated on another. Furthermore, we demonstrate the efficacy of our training techniques with different SOTA 360-degree depth models and various unlabeled datasets, showcasing the versatility and effectiveness of our approach in addressing the unique challenges posed by 360-degree imagery.

Our contributions can be summarized as follows:

* We propose a novel training technique for 360-degree imagery that harnesses the power of unlabeled data through the distillation of perspective foundation models.
* We introduce an online data augmentation method that effectively bridges knowledge distillation across different camera projections.
* Our proposed training techniques significantly benefit and inspire future research on 360-degree imagery by showcasing the interchangeability of state-of-the-art (SOTA) 360 models, perspective teacher models, and unlabeled datasets. This enables better results even as new SOTA techniques emerge in the future.

## 2 Related Work

360 monocular depth.Depth estimation for 360-degree images presents unique challenges due to the equirectangular projection and inherent distortion. Various approaches have been explored to address these issues:

* _Directly Apply_: Some methods directly apply monocular depth estimation techniques to 360-degree imagery. OmniDepth  leverages spherical geometry and incorporates SphConv  to improve depth prediction with distortion. [70; 49] use spherical coordinates to overcome distortion with extra information. [12; 17] leverage other ground truth supervisions to assist on depth estimation. SliceNet  and ACDNet  propose advanced network architectures tailored for omnidirectional images. EGFormer  and HiMODE  introduce a transformer-based model that captures global context efficiently, while [45; 44]

Figure 1: **Our proposed training pipeline improves existing 360 monocular depth estimators.** This figure demonstrated the improvement of our proposed training pipeline tested on the Stanford2D3D  dataset in a zero-shot setting.

focuses on integrating geometric priors into the learning process.  proposed to generate large-scale datasets with SfM and MVS, then apply to test-time training.
* _Cube_: Other approaches use cube map projections to mitigate distortion effects. 360-SelfNet  is the first work to self-supervised 360 depth estimation leveraging cube-padding . BiFuse  and its improved version BiFuse++  are two-branch architectures that utilize cube maps and equirectangular projections. UniFuse  combines equirectangular and cube map projections and simplifies the architecture.  combines two-branch techniques with transformer network.
* _Tangent Image_: Tangent image projections are also popular.  convert equirectangular images into a series of tangent images, which are then processed using conventional depth estimation networks. PanoFormer  employs a transformer-based architecture to handle tangent images, while SphereNet  and HRDFuse  enhance depth prediction by collaboratively learning from multiple projections.

360 other works Beyond depth estimation, 360-degree imagery has been applied to depth completion tasks as follows . Other methods, such as  and  focus on the projection between camera models, while the former projects pinhole camera model images into a large field of view, whereas the latter transforms convolution kernels.

Unlabeled / Pseudo labeled data.Utilizing unlabeled or pseudo-labeled data has become a significant trend to mitigate the limitations of labeled data scarcity. Techniques like  leverage large amounts of unlabeled data to improve model performance through semi-supervised learning. In the context of 360-degree depth estimation, our approach generates pseudo labels from pre-trained perspective models, which are then used to train 360-degree depth models effectively.

Zero-shot methods.Zero-shot learning methods aim to generalize to new domains without additional training data.  target this directly with increasing training data., MiDaS  and Depth Anything  are notable for their robust monocular depth estimation across diverse datasets leveraging affine-invariant loss.  takes a step further to investigate zero-shot on metric depth. Marigold  leverages diffusion models with image conditioning and up-to-scale relative depth denoising to generate detailed depth maps. ZoeDepth  further these advancements by incorporating scale awareness and domain adaptation.  leverage camera model information to adapt cross-domain depth estimation.

Foundation models.Foundation models have revolutionized various fields in AI, including natural language processing and image-text alignment. In computer vision, models like CLIP  demonstrate exceptional generalization capabilities.  proposed a foundation visual encoder for downstream tasks such as segmentation, detection, depth estimation, etc.  proposed a model that can cut out masks for any objects. Our work leverages a pre-trained perspective depth estimation foundation model  as a teacher model to generate pseudo labels for 360-degree images, enhancing depth estimation by utilizing the vast knowledge embedded in these foundation models.

## 3 Methods

In this work, we propose a novel training approach for 360-degree monocular depth estimation models. Our method leverages a perspective depth estimation model as a teacher and generates pseudo labels for unlabeled 360-degree images using a 6-face cube projection. Figure 2 illustrates our training pipeline, incorporating the use of Segment Anything to mask out sky and watermark regions in unlabeled data during the offline stage. Subsequently, we conduct joint training using both labeled and unlabeled data, allocating half of the batch to each. The joint training avoids limiting performance by teacher model. The unlabeled data is supervised using pseudo labels generated by Depth Anything, a state-of-the-art perspective monocular depth foundation model. With the benefit of our teacher model, the 360-degree depth model demonstrates an observable improvement on the zero-shot dataset, as shown in Figure 1.

### Unleashing the Power of Unlabel 360 data

Dataset statistics.360-degree data has become increasingly available in recent years. However, compared to perspective-view depth datasets, labeling depth ground truths for 360-degree data presents greater challenges. Consequently, the availability of labeled datasets for 360-degree data is considerably smaller than that of perspective datasets.

Table 1 presents the data quantities available in some of the most popular 360-degree datasets, including Matterport3D , Stanford2D3D , and Structured3D . Additionally, we list a multi-modal dataset, SpatialAudioGen , which consists of unlabeled 360-degree data used in our experiments. Notably, the amount of labeled and unlabeled data used in the perspective foundation model, Depth Anything , is significantly larger, with 1.5 million labeled images  and 62 million unlabeled images , making the amount in 360-degree datasets approximately 170 times smaller.

Data cleaning and valid pixel mask generationUnlabeled data often contains invalid pixels in regions such as the sky and watermark, leading to unstable training or undesired convergence. To address this issue, we applied the GroundingSAM  method to mask out the invalid regions. This approach utilizes Grounded DINOv2  to detect problematic regions and applies the Segment Anything  model to mask out the invalid pixels by segmenting within the bounding box. While Depth Anything  also employs a pre-trained segmentation model, DINOv2, to select sky regions. Brand logos and watermarks frequently appear after fisheye camera stitching. Therefore, additional labels are applied to enhance the robustness of our training process. We also remove all images with less than 20 percent of valid pixels to stablize our training progress.

Perspective foundation models (teacher models).To tackle the challenges posed by limited data and labeling difficulties in 360-degree datasets, we leverage a large amount of unlabeled data alongside state-of-the-art perspective depth foundation models. Due to significant differences in camera projection and distortion, directly applying perspective models to 360-degree data often yields inferior results. Previous works have explored various methods of projection for converting equirectangular to perspective depth, as stated in Section 2. Among these, cube projection and tangent

    & Perspective & &  \\  Labeled & 1.5M & Unlabeled & 62 M & Labeled & 34K & Unlabeled & 344K \\   

Table 1: **360 monocular depth estimation lacks a large amount of training data.** The number of images in 360-degree monocular depth estimation datasets alongside perspective depth datasets from the Depth Anything methodology.

Figure 2: **Training Pipeline. Our proposed training pipeline involves joint training on both labeled 360 data with ground truth and unlabeled 360 data. (a) For labeled data, we train our 360 depth model with the loss between depth prediction and ground truth. (b) For unlabeled data, we propose to distill knowledge from a pre-trained perspective-view monocular depth estimator. In this paper, we use Depth Anything  to generate pseudo ground truth for training. However, more advanced techniques could be applied. These perspective-view monocular depth estimators fail to produce reasonable equirectangular depth as a domain gap exists. Therefore, we distill knowledge by inferring six perspective cube faces and passing them through perspective-view monocular depth estimators. To ensure stable and effective training, we propose generating a valid pixel mask with Segment Anything  while calculating loss. (c) Furthermore, we augment random rotation on RGB before passing it into Depth Anything, as well as on predictions from the 360 depth model.**

projection are the most common techniques. We selected cube projection to ensure a larger field of view for each patch, enabling better observation of relative distances between pixels or objects during the inference of the perspective foundation model and enhancing knowledge distillation. The comparison table can be find in the supplementary material.

In our approach, we apply projection to unlabeled 360-degree data and then run Depth Anything on these projected patches of perspective images to generate pseudo-labels. We explore two directions for pseudo-label supervision: projecting the patch to equirectangular and computing in the 360-degree domain or projecting the 360-degree depth output from the 360 model to patches and computing in the perspective domain. Since training is conducted in an up-to-scale relative depth manner, stitching the patch of perspective images back to equirectangular with an aligned scale will lead to failure in training Figure 4, which is an additional research topic that is worth investigation. We opt to compute the loss in the perspective domain, facilitating faster and easier training without the need for additional alignment optimization.

### Random Rotation Processing

Directly applying Depth Anything on cube-projected unlabeled data does not yield improvements due to ignorance of cross-cube-face relation, leading to cube artifacts (Figure 5). This issue arises from the separate estimation of perspective cube faces, where monocular depth is estimated based on semantic information, lacking a comprehensive understanding of the entire scene. To address this, we propose a random rotation preprocessing step in front of the perspective foundation model.

As depicted in Figure 2, the rotation is applied to equirectangular projection RGB images using a random rotation matrix, followed by cube projection. This results in a more diverse set of cube faces, capturing relative distances between ceilings, walls, windows, and other objects more effectively. With the proposed random rotation technique, knowledge distillation becomes more comprehensive as the point of view is not static. The inference by the perspective foundation model is performed on the fly, with parameters frozen during the training of the 360 model.

In order to perform random rotation, we apply a rotation matrix on the equirectangular coordinates, noted as \((,)\), and rotation matrix as \(\).

\[(,)=(,).\] (1)

For equirectangular to cube projection, the field-of-view (FoV) of each cube face is equal to 90 degrees; each face can be considered as a perspective camera whose focal length is \(w/2\), and all faces share the same center point in the world coordinate. Since the six cube faces share the same center point, the extrinsic matrix of each camera can be defined by a rotation matrix \(R_{i}\). \(p\) is then the pixel

Figure 3: **Valid Pixel Masking. We used Grounded-Segment-Anything  to mask out invalid pixels based on two text prompts: “sky” and “watermark.” These regions lack depth sensor ground truth labels in all previous datasets. Unlike Depth Anything , which sets sky regions as 0 disparity, we follow ground truth training to ignore these regions during training for two reasons: (1) segmentation may misclassify and set other regions as zero, leading to noisy labeling, and (2) watermarks are post-processing regions that lack geometrical meaning.**

on the cube face

\[p=K R_{i}^{T} q,\] (2)

where,

\[q=q_{x}\\ q_{y}\\ q_{z}=sin()()\\ ()\\ ,K=w/2&0&w/2\\ 0&w/2&w/2\\ 0&0&1,\] (3)

where \(\) and \(\) are longitude and latitude in equirectangular projection and q is the position in Euclidean space coordinates.

### Loss Function

The training process closely resembles that of MiDaS, Depth Anything, and other cross-dataset methods. Our goal is to provide depth estimation for any 360-degree images. Following previous approaches that trained on multiple datasets, our training objective is to estimate relative depth. The depth values are first transformed into disparity space using the formula \(1/d\) and then normalized to the range \(\) for each disparity map.

To adapt to cross-dataset training and pseudo ground truths from the foundation model, we employed the affine-invariant loss, consistent with prior cross-dataset methodologies. This loss function disregards absolute scale and shifts for each domain, allowing for effective adaptation across different

Figure 4: **Qualitative visualization of a model trained directly on pseudo equirectangular data without scale alignment**. We propose calculating the loss with pseudo ground truth on cube faces due to scale misalignment between the six faces during the cube-to-equirectangular projection. We showcase the results of a model trained on pseudo equirectangular data without scale alignment as a simple baseline to demonstrate the importance of calculating loss separately on each of the six faces. The images are presented from top to bottom as follows: (a) RGB images. (b) Pseudo cube ground truth projected directly to equirectangular. (c) Prediction trained with row 2. (d) Pseudo cube ground truth with rotation projected directly to equirectangular. (e) Prediction trained with row 4. (f) Our model’s predictions are trained on cube faces separately with rotation.

datasets and models.

\[1=_{i=1}^{HW}(d_{i}^{*},d_{i}),\] (4)

where \(d_{i}^{*}\) and \(d_{i}\) are the prediction and ground truth, respectively. \(\) represents the affine-invariant mean absolute error loss:

\[(d_{i}^{*},d_{i})=|_{i}^{*}-_{i}|.\] (5)

Here, \(_{i}\) and \(_{i}^{*}\) are the scaled and shifted versions of the prediction \(d_{i}^{*}\) and ground truth \(d_{i}\):

\[_{i}=-t(d)}{s(d)},\] (6)

where \(t(d)\) and \(s(d)\) are used to align the prediction and ground truth to have zero translation and unit scale:

\[t(d)=(d), s(d)=_{i=1}^{HW}|d_{i}-t(d)|.\] (7)

## 4 Experiments

These notations apply for all tables: **M**: Matterport3D , **SF**: Stanford2D3D , **ST**: Structured3D , **SP**: Spatialudiogen , **-all** indicates using the entire train, validation, and test sets of the specific dataset, and **(p)** denotes using pseudo ground truth generated by Depth Anything . Due to space limits, we provide the experimental setup in the appendix, including implementation details and evaluation metrics.

### Baselines

Recent state-of-the-art methods [1; 64; 47; 48; 16; 31; 40] have emerged. We chose UniFuse and BiFuse++ as our baseline models for experiments, as many of the aforementioned methods did not fully release pre-trained models or provide training code and implementation details. It's worth noting that PanoFormer  is not included due to incorrect evaluation code and results. Both selected models are re-implemented with affine-invariant loss on disparity for a fair comparison and to demonstrate improvement. We conduct experiments on the Matterport3D  benchmark to demonstrate performance gains within the same dataset/domain, and we perform zero-shot evaluation on the Stanford2D3D  test set to demonstrate the generalization capability of our proposed training technique. To further validate its robustness, we evaluate additional baseline models [45; 64] in zero-shot setting, showcasing the effectiveness of our approach for non-dual-projection models.

Figure 5: **Cube Artifact.** Shown in the center row of the figure, an undesired cube artifact appears when we apply joint training with pseudo ground truth from Depth Anything  directly. This issue arises from independent relative distances within each cube face caused by a static point of view. Ignoring cross-cube relationships results in poor knowledge distillation. To address this, as shown in Figure 2(c), we randomly rotate the RGB image before inputting it into Depth Anything. This enables better distillation of depth information from varying perspectives within the equirectangular image.

### Benchmarks Evaluation

We conducted our in-domain improvement experiment on the widely used 360-degree depth benchmark, Matterport3D , to showcase the results of perspective foundation model distillation on the two selected baseline models, UniFuse  and BiFuse++. In Table 2, we list the metric depth evaluation results from state-of-the-art methods on this benchmark. Subsequently, we present the re-trained baseline models using affine-invariant loss on disparity to ensure a fair comparison with their original depth metric training. Finally, we demonstrate the improvement achieved with results trained on the labeled Matterport3D training set and the entire Structured3D dataset with pseudo ground truth.

### Zero-Shot Evaluation

Our goal is to estimate depths for all 360-degree images, making zero-shot performance crucial. Following previous works [47; 16], we adopted their zero-shot comparison setting, where models trained on the entire Matterport3D  dataset are tested on the Stanford2D3D  test set. In Table 3, the upper section lists methods trained with metric depth ground truth, with numbers sourced from their respective papers. The lower section includes models trained with affine-invariant loss on disparity ground truth. As shown in Figure 6, [16; 48] demonstrate generalization improvements with a lower error on the Stanford2D3D dataset.

Depth Anything  and Marigold  are state-of-the-art zero-shot depth models trained with perspective depths. As shown in Table 3, due to the domain gap and different camera projections, foundation models trained with perspective depth cannot be directly applied to 360-degree images. We demonstrated the zero-shot improvement on UniFuse , BiFuse++  and non-dual-projection methods [45; 64] with models trained on the entire Matterport3D  dataset with ground truth and the entire Structured3D  or SpatialAudioGen  dataset with pseudo ground truth generated using Depth Anything .

As Structured3D provides ground truth labels for its dataset, we also evaluate our models on its test set to assess how well they perform with pseudo labels. Table 4 shows the improvements achieved on the Structured3D test set when using models trained with pseudo labels. It's worth noting that even when the 360 model is trained on pseudo labels from SpatialAudioGen, it performs similarly well. This demonstrates the success of our distillation technique and the model's ability to generalize across different datasets.

### Qualitative Results in the Wild

We demonstrated the qualitative results in Figure 8 and Figure 7 360-degree images that were either captured by us or downloaded from the internet1. These examples showcase the zero-shot capability of our model when applied to data outside the aforementioned 360-degree datasets.

   Method & Loss & Train & Test & Abs Rel \(\) & \(_{1}\) & \(_{2}\) & \(_{3}\) \\  BiFuse  & BerHu & M & M & - & 0.845 & 0.932 & 0.963 \\ UniFuse  & BerHu & M & M & 0.106 & 0.890 & 0.962 & 0.983 \\ SliceNet  & BerHu & M & M & - & 0.872 & 0.948 & 0.972 \\ BiFuse++  & BerHu & M & M & - & 0.879 & 0.952 & 0.977 \\ HRDPFuse  & BerHu & M & M & 0.097 & 0.916 & 0.967 & 0.984 \\  UniFuse  & Affine-Inv & M & M & 0.102 & 0.893 & 0.970 & 0.989 \\ UniFuse  & Affine-Inv & M, ST-all (p) & M & 0.089 & 0.911 & 0.975 & 0.991 \\ BiFuse++  & Affine-Inv & M & & 0.094 & 0.914 & 0.974 & 0.989 \\ BiFuse++  & Affine-Inv & M, ST-all (p) & M & **0.085** & **0.917** & **0.976** & **0.991** \\   

Table 2: **Matterport3D Benchmark.** The upper section lists 360 methods trained with metric depths in meters using BerHu loss. All numbers are sourced from their respective papers. The lower section includes selected methods retrained with relative depth (disparity) using affine-invariant loss.

   Method & Loss & train & test & Abs Rel \(\) & \(_{1}\) & \(_{2}\) & \(_{3}\) \\  UniFuse  & Affine-Inv & M-all & ST & 0.202 & 0.759 & 0.932 & 0.970 \\ UniFuse  & Affine-Inv & M-all, ST-all (p) & ST & 0.130 & 0.887 & 0.953 & 0.977 \\  UniFuse  & Affine-Inv & M-all, SP-all (p) & ST & 0.152 & 0.864 & 0.946 & 0.972 \\   

Table 4: **Structured3D Test Set.** We demonstrate the improvement on the Structured3D test set using pseudo ground truth for training. The lower section shows enhancements with models trained on pseudo ground truth from Matterport3D and SpatialAudioGen, indicating similar improvements. This highlights the successful distillation of Depth Anything.

   Method & Loss & train & test & Abs Rel \(\) & \(_{1}\) & \(_{2}\) & \(_{3}\) \\  BiFuse  & BerHu & M-all & SF & 0.120 & 0.862 & - & - \\ UniFuse  & BerHu & M-all & SF & 0.094 & 0.913 & - & - \\ BiFuse++  & BerHu & M-all & SF & 0.107 & 0.914 & 0.975 & 0.989 \\  Depth Anything  & Affine-Inv & Pers. & SF & 0.248 & 0.635 & 0.899 & 0.97 \\ Marigold  & Affine-Inv & Pers. & SF & 0.195 & 0.692 & 0.942 & 0.982 \\ UniFuse  & Affine-Inv & M-all & SF & 0.090 & 0.914 & 0.976 & 0.990 \\ UniFuse  & Affine-Inv & M-all, ST-all (p) & SF & 0.086 & 0.924 & 0.977 & 0.990 \\ UniFuse  & Affine-Inv & M-all, SP-all (p) & SF & 0.090 & 0.920 & 0.978 & 0.990 \\ BiFuse++  & Affine-Inv & M-all & SF & 0.090 & 0.921 & 0.976 & 0.990 \\ BiFuse++  & Affine-Inv & M-all, ST-all (p) & SF & **0.082** & **0.931** & **0.979** & 0.991 \\ BiFuse++  & Affine-Inv & M-all, SP-all (p) & SF & 0.086 & 0.926 & **0.979** & 0.991 \\ HoHoNet  & Affine-Inv & M-all & SF & 0.095 & 0.906 & 0.975 & 0.991 \\ HoHoNet  & Affine-Inv & M-all, ST-all (p) & SF & 0.088 & 0.920 & **0.979** & **0.992** \\ EGFormer  & Affine-Inv & M-all & SF & 0.098 & 0.906 & 0.972 & 0.989 \\ EGFormer  & Affine-Inv & M-all, ST-all (p) & SF & 0.086 & 0.923 & 0.976 & 0.990 \\   

Table 3: **Zero-shot Evaluation on Stanford2D3D.** We perform zero-shot evaluations with models trained on other datasets. Following the original training settings, we train the 360 models  on the entire Matterport3D dataset and then test on Stanford3D’s test set.

Figure 6: **Zero-shot qualitative with UniFuse  (_left_) and BiFuse++  (_right_) tested on Stanford2D3D.**

### Fine-Tuned to Metric Depth Estimation

We use our pre-trained model as an initial weight and fine-tune on Stanford2D3D  metric depth to demonstrate the effectiveness of our pre-trained relative depth model's ability to adapt to metric depth with a single epoch in Table 5

## 5 Conclusion

Our proposed method significantly advances 360-degree monocular depth estimation by leveraging perspective models for pseudo-label generation on unlabeled data. The use of cube projection with random rotation and affine-invariant loss ensures robust training and improved depth prediction accuracy while bridging the domain gap between perspective and equirectangular projection. By effectively addressing the challenges of limited labeled data with cross-domain distillation, our approach opens new possibilities for accurate depth estimation in 360 imagery. This work lays the groundwork for future research and applications, offering a promising direction for further advancements in 360-degree depth estimation.

Limitations.Our work faces limitations due to its heavy reliance on the quality of unlabeled data and pseudo labels from perspective foundation models. The results are significantly impacted by data quality (Section 3.1). Without data cleaning, the training process resulted in NaN values. Another limitation is that although with unlabeled data, the scarcity of data still exists compared to other tasks.

   Method & MAE \(\) & Abs Rel \(\) & RMSE \(\) & RMSElog \(\) & \(_{1}\) & \(_{2}\) & \(_{3}\) \\  UniFuse  & 0.208 & 0.111 & 0.369 & 0.072 & 0.871 & 0.966 & 0.988 \\ UniFuse  (Ours) & 0.206 & 0.118 & 0.351 & 0.049 & 0.910 & 0.971 & 0.987 \\   

Table 5: **Metric depth fine-tuning. We fine-tune our model trained with Matterport3D  ground truth label and Structured3D  pseudo label on relative depth with Stanford2D3D ’s training set metric depths with a single epoch.**

Figure 8: **Generalization ability in the wild with point cloud visualization. We showcase zero-shot qualitative results in the point cloud using a combination of images we captured and randomly sourced from the internet to assess the model’s generalization ability.**

Figure 7: **Generalization ability in the wild with depth map visualization. We showcase zero-shot qualitative results using a combination of images we captured and randomly sourced from the internet to assess the model’s generalization ability. For privacy reasons, we have obscured the cameraman in the images.**