# GraphMETRO: Mitigating Complex Graph Distribution Shifts via Mixture of Aligned Experts

Shirley Wu

Stanford University

shirwu@cs.stanford.edu

&Kaidi Cao

Stanford University

kaidicao@cs.stanford.edu

&Bruno Ribeiro

Purdue University

ribeirob@purdue.edu

&James Zou

Stanford University

jamesz@cs.stanford.edu

&Jure Leskovec

Stanford University

jure@cs.stanford.edu

Equal Senior Authorship

###### Abstract

Graph data are inherently complex and heterogeneous, leading to a high natural diversity of distributional shifts. However, it remains unclear how to build machine learning architectures that generalize to the complex distributional shifts naturally occurring in the real world. Here, we develop GraphMETRO, a Graph Neural Network architecture that models natural diversity and captures complex distributional shifts. GraphMETRO employs a Mixture-of-Experts (MoE) architecture with a gating model and multiple expert models, where each expert model targets a specific distributional shift to produce a referential representation _w.r.t._ a reference model, and the gating model identifies shift components. Additionally, we design a novel objective that aligns the representations from different expert models to ensure reliable optimization. GraphMETRO achieves state-of-the-art results on four datasets from the GOOD benchmark, which is comprised of complex and natural real-world distribution shifts, improving by 67% and 4.2% on the WebKB and Twitch datasets. Code and data are available at https://github.com/Wuyxin/GraphMETRO.

## 1 Introduction

The intricate nature of real-world graph data introduces a wide variety of distribution shifts and heterogeneous graph variations . For instance, in a social graph, some user nodes can experience reduced activity and profile alterations, while other user nodes may see increased interactions. More broadly, such shifts go beyond the group-wise pattern and further contribute to the heterogeneous nature of graph data. In Figure 1, we provide a real-world example on a webpage network dataset, where, besides the general distribution shift from source to target distribution, two webpage nodes \(u_{1}\) and \(u_{2}\) in the target domain exhibit varying degrees of change in their content features. These inherent shifts and complexity accurately characterize the dynamics of real-world graph data, _e.g.,_ social networks  and ecommerce graphs .

Above the diverse graph variants, Graph Neural Networks (GNNs)  have become a prevailing method for downstream graph tasks. Standard evaluation often

Figure 1: An example on WebKB . It illustrates (1) The distribution shift from source to target (the thick arrow in the upper right) and (2) Instance-wise heterogeneity in the target distribution (the thin arrows pointing to \(u_{1}\) and \(u_{2}\)).

adopts random data splits for training and testing GNNs. However, it overlooks the complex distributional shifts naturally occurring in the real world. Moreover, compelling evidence shows that GNNs are extremely vulnerable to graph data shifts [79; 26; 20]. Thus, our goal is to build GNN models that generalize better to real-world data splits and graph dynamics described earlier.

Previous research on GNN generalization has mainly focused on two lines: (1) Data-augmentation training procedures that learn environment-robust predictors by augmenting the training data with the environment changes. For example, works have looked at distribution shifts related to graph size [49; 14], node features [26; 8; 27], and node degree or local structure [65; 39], assuming that the target data adhere to a designated shift type. (2) Learning environment-invariant representations or predictors either through inductive biases learned by the model [69; 67], through regularization [4; 32; 75], or a combination of both [72; 11; 80].

However, the real-world distribution shifts and graph dynamics are unknown. Specifically, the distribution shift could be any fusion of multiple shift dimensions, each characterized by unique statistical properties [26; 20; 50], which is rarely covered by single-dimension synthetic augmentation or fixed combinations of shift dimensions used in data augmentation approaches. Moreover, as seen in Figure 1, graph data may involve instance-wise heterogeneity, lacking stable properties from which invariant predictors can be learned [47; 30]. Here, the standard strategy of learning invariant predictors or representations must contend with a combinatorially large number of distribution shift variations. Thus, previous works may not be well-equipped to address this challenging task effectively.

Here we propose a novel and general framework, GraphMETRO. The key to our approach is to decompose any unknown shift into multiple shift components and learn predictors that can adapt to the graph heterogeneity observed in the target data. Figure 1(a) shows an example of our method on graph-level tasks, where the shift from the target graph data \(_{t}\) to the source distribution \(_{s}\) is decomposed into two strong shift components controlling feature noise and graph size, while the shift component controlling average node degree is identified as irrelevant. Specifically, the shift components are constructed such that each possesses unique statistical characteristics. Moreover, the contribution of each shift component to the shift is determined by an influence function that encodes the given graph \(\) and source distribution \(_{s}\). This design enables breaking down the generalization problem into (1) inference on strong shift components and their contributions as surrogates for any distributional or heterogeneous shifts, and (2) mitigation toward the surrogate shifts, where the individual shift components are interpretable and more tractable.

Figure 2: **Overview of GraphMETRO on graph classification tasks. (a) High-level Concept: As a simple example, the distribution shift from a target graph \(_{t}\) to a source distribution \(_{s}\) is decomposed along three shift dimensions: graph size (\(_{1}\)), node degree (\(_{2}\)), and feature noise (\(_{3}\)). Note that the shift components can be customized and expanded based on downstream tasks. (b) Architecture: Given an input graph, the gating model \(\) decomposes the instance-specific distribution shift into the contributions from the shift components. Then, each expert model \(_{i}\)\((i>0)\) is tasked with generating _referential invariant representations_ (_cf._ Section 3 for the definition) _w.r.t._ an assigned shift component. \(_{0}\) is a reference model used for aligning the representation spaces of the expert models. The final representation is aggregated from the expertsâ€™ output and is referentially invariant to any distribution shifts, which is then input to the classifier.**

For the first subproblem, we design a hierarchical architecture composed of a gating model and multiple expert models, inspired by the mixture-of-experts (MoE) architecture . As shown in Figure 1(b), the gating model takes any given node or graph data and identifies strong shift components that govern the localized distribution shift, while each expert model corresponds to an individual shift component. Second, to further mitigate surrogate distribution shifts, we train the expert models to generateerentially invariant representations with respect to their corresponding shift components, which are then aggregated as the final representation vector. Moreover, the expert outputs must align properly in a common representation space to prevent extreme divergence in the aggregated representation. Consequently, we design a novel objective to ensure a smooth training process. Finally, during the evaluation process, we integrate outputs from both the gating and expert models for final representations.

This process effectively generates invariant representations across complex distributional shifts. To highlight, our method achieves the best performance on four node- and graph-level tasks from the GOOD benchmark , which involves a diverse set of natural distribution shifts such as user language shifts in gamer networks and university domain shifts in university webpage networks. GraphMETRO achieves a 67% relative improvement over the state-of-the-art on the WebKB dataset . On synthetic datasets, our method outperforms Empirical Risk Minimization (ERM) by 4.6% on average. To the best of our knowledge, GraphMETRO is the first to explicitly target complex distribution shifts that resemble real-world settings.

The key benefits of GraphMETRO are as follows:

* **A novel paradigm:** GraphMETRO provides a new approach to aid GNN generalization by decomposing and mitigating complex distributional shifts via a mixture-of-experts architecture.
* **Superior performance:** It outperforms state-of-the-art methods on real-world datasets with natural splits and shifts, demonstrating promising generalization ability.
* **Enhanced interpretability:** GraphMETRO offers insights into the shift types of graph data by identifying and interpreting strong shift components.

## 2 Related Works

**Invariant learning for graph OOD**. The prevailing invariant learning approaches assume that there exist an underlying graph structure (_i.e.,_ subgraph) [69; 36; 34; 71; 60; 83; 37] or representation [1; 67; 6; 3; 81; 68; 11; 8; 42] that is invariant to different environments and/or causally related to the label of a given instance. For example, DIR  constructs interventional distributions and distills causal subgraph patterns to make generalizable predictions for graph-level tasks. However, this line of research focuses on group patterns without explicitly considering instance heterogeneity. Therefore, the standard invariant learning approaches are not well-equipped to mitigate the complex distribution shifts in our context. See Appendix A for an in-depth comparison.

**Data augmentation for graph OOD**. GNNs demonstrate robustness to data perturbations when incorporating augmented views of graph data . Previous works have explored augmentation with respect to graph sizes [85; 4; 84], local structures [40; 38], feature metrics , and graphons. For example, OOD-G-Mixup  creates virtual OOD samples by perturbing the graph rationale space. Recently, Jin et al.  proposed adapting testing graphs to transformed graphs with patterns similar to the training graphs. Other approaches conduct augmentation implicitly via attention mechanisms [45; 66]. For example, GSAT  injects stochasticity into attention weights to block label-irrelevant information. Nevertheless, this line of research may not effectively solve the challenging problem, since unseen distribution shifts may not be covered by the distribution of augmented graphs. Moreover, it may lead to degradation of in-distribution performance due to GNNs' limited expressiveness in encoding a broad distribution.

**Instance heterogeneity for graph OOD**. Recent methods [41; 59; 61; 77; 35; 74] have explicitly considered instance heterogeneity for improving OOD generalization in GNNs. For example, OOD-GNN  mitigates instance-wise heterogeneity by eliminating spurious correlations between irrelevant and relevant graph representations through nonlinear decorrelation and sample reweighting. Yao et al.  focus on explicitly model domain correlations and spurious features and adapt to each test instance's unique distribution shifts. While these methods explicitly consider instance heterogeneity in graph OOD problems, they often focus on specific types of distribution shifts or rely on the assumption that target data adhere to certain designated shift types.

In contrast, our method introduces a novel paradigm that decomposes any unknown shift into multiple shift components and learns predictors that can adapt to the graph heterogeneity observed in the target data. By leveraging a mixture-of-experts architecture, our approach can handle complex distribution shifts without assuming specific shift types or relying solely on group patterns.

**Mixture-of-expert models**. The applications of mixture-of-expert models (MoE) [24; 57] have largely focused on their efficiency and scalability [13; 12; 53; 9], particularly in image and language domains. For image domain generalization, Li et al.  focus on neural architecture design and integrate expert models with vision transformers to capture correlations in the training dataset that may benefit generalization, where an expert is responsible for a group of similar visual attributes. Puigcerver et al.  observed improved robustness by adopting MoE models in the image domain. In the graph domain, differently motivated from our work, Wang et al.  consider experts as information aggregation models with varying hop sizes to capture different ranges of message passing, aiming to improve model expressiveness on large-scale data.

GraphMETRO is the first to design a mixture-of-expert model specifically tailored to address complex distribution shifts in graphs, coupled with a novel objective for producing invariant representations. While previous methods mostly focus on either node- or graph-level tasks, GraphMETRO is a more general solution applicable to both.

## 3 Method

**Problem formulation**. For simplicity, we consider a graph classification task and later extend it to node-level tasks. Let \(_{s}\) be the source distribution and \(_{t}\) be an unknown target distribution. We are interested in the natural graph distribution shifts. Our goal is to learn a model \(f_{}\) with high generalization ability. The standard approach is Empirical Risk Minimization (ERM), _i.e.,_

\[^{*}=_{}\;_{(,y)_{s}} (f_{}(),\;y),\] (1)

where \(\) denotes the loss function and \(y\) is the label of the graph \(\). However, the assumptions underlying ERM can be easily violated, making \(^{*}\) suboptimal. Moreover, since the distribution shift is unknown and cannot provide supervision for model training, the direct optimization of Eq 1 is intractable.

### Shift Components

Based on the common mixture pattern studied in real-world networks [29; 30; 50], we propose the following informal assumption:

**Assumption 1** (An equivalent mixture for distribution shifts): _Let the distribution shift between the source \(_{s}\) and target \(_{t}\) distributions be the result of an unknown intervention in the graph formation mechanism. We assume that the resulting shift in \(_{t}\) can be modeled by up to \(k\) out of \(K\) classes of stochastic transformations applied to each instance in the source distribution \(_{s}\) (\(k K\))._

Assumption 1 essentially states that any distribution shift can be decomposed into \(k\) shift components of stochastic graph transformations. The assumption simplifies the generalization problem by enabling the modeling of individual shift components that constitute the shift and their respective contributions to the overall distribution shift. While this assumption is generally applicable, as observed in the experiments, we include a discussion on scenarios that fall outside the scope of this assumption in Appendix F. Previous works [28; 69; 67] implicitly infer such shift components from the data environments constructed based on the source distribution. However, distilling diverse shift components from the source data is challenging due to the complexity of the graph distribution shifts and largely depends on the constructed environments2.

**Graph extrapolation as shift components**. To construct the shift components, we employ a data extrapolation technique based on the source data. In particular, we introduce \(K\) independent classes of transform functions, including multihop subgraph sampling, the addition of Gaussian feature noise, and random edge removal . The \(i\)-th class, governed by the \(i\)-th shift component, defines a stochastic transformation \(_{i}\) that transforms an input source graph \(\) into an output graph \(_{i}()\), where \(i=1,,K\). For instance, \(_{i}\) can be defined to randomly remove edges with an edge-dropping probability in the range of \([0.3,0.5]\). Note that the extrapolation aims to construct the basis of shifts rather than directly conducting data augmentation, as explained in Eq 3 later.

### Mixture of Aligned Experts

In light of the shift components, we formulate the generalization problem as two separate phases:

* **Surrogate estimation:** Identify a mixture of shift components as the surrogate for the target shifts, where the mixture can vary across different node or graph instances to capture heterogeneity.
* **Mitigation and aggregation:** Mitigate individual shift components, followed by aggregating the representations output by each expert to resolve the surrogate shift.

**Overview**. Inspired by the mixture-of-experts (MoE) architecture , the core idea of GraphMETRO is to build a hierarchical architecture composed of a gating model and multiple expert models, where the gating model predicts the influence of the shift components on a given instance. For the expert models, we design each to handle an individual shift component. The experts produce referential representations invariant to their designated shift component, with the representations aligned in a common representation space. Finally, our architecture combines the expert outputs into a final representation, which our training objective ensures is invariant to the stochastic transformations within the mixture distribution. We detail each module as follows:

**Gating model**. We introduce a GNN \(\) as the gating model, which takes any graph as input and outputs a weight vector \(\) on the shift components. The weight vector suggests the most probable shift components from which the input graph originates. For example, in Figure 1(b), given an unseen graph with decreased graph size and node feature noise, a trained gating model should assign large weights to the corresponding shift components and small values to the irrelevant ones. Note that \(\) should be such that \(_{i}\), the weight on the \(i\)-th component, strives to be sensitive to the stochastic transformation \(_{i}\) but insensitive to the application of other stochastic transformations \(_{j}\), \(j i\). This way, determining whether the \(i\)-th component is present should not depend on other components.

**Expert models**. We build \(K\) expert models, each corresponding to a shift component. Formally, we denote an expert model as \(_{i}:^{v}\), where \(v\) is the hidden dimension, and we use \(_{i}=_{i}()\) to denote the output representation. Each expert model essentially produces invariant representations  with respect to the distribution shift controlled by its assigned shift component. However, independently optimizing each expert without properly aligning the expert's output space is incompatible with model training. Specifically, an expert model may learn its own unique representation space, which may cause information loss when its output is aggregated with other expert outputs. Moreover, aggregating independent representations results in a mixed representation space with high variance, which makes it difficult for the predictor head, such as multi-layer perceptrons (MLPs), to capture the interactions and dependencies among these diverse representations and output rational predictions. Thus, aligning the representation spaces of experts is necessary to ensure compatibility and facilitate stable model training. To align the experts' output spaces properly, we introduce the concept of referential invariant representation:

**Definition 1** (Referential Invariant Representation): _Let \(\) be an input graph and let \(\) be an arbitrary stochastic transform function, with domain and co-domain in the space of graphs. Let \(_{0}\) be a model that encodes a graph into a representation. A referential invariant representation w.r.t. the given \(\) is denoted as \(^{*}()\), where \(^{*}\) is a function that maps the original data \(\) to a high-dimensional representation \(^{*}()\) such that \(_{0}()^{*}(())\) holds for every \((_{s})\), where \((_{s})\) denotes the support of \(_{s}\). We refer to \(_{0}\) as the reference model._

Thus, the representation space of the reference model serves as an intermediate to align different experts, while each expert \(_{i}\) has its own ability to produce referential invariant representations _w.r.t._ a stochastic transform function \(_{i}\), \(i=1,,K\). We include the reference model as a special "in-distribution" expert model on the source data.

**Architecture design for the expert models**. Further, we propose two architecture designs for the expert models. A straightforward way is to construct \((K+1)\) GNN encoders to generate referential invariant representations for individual shift components. This ensures model expressiveness while increasing memory usage due to multiple encoders. To alleviate this concern, we provide an alternative approach. Specifically, we can construct a shared module, _e.g.,_ a GNN encoder, among the expert models, coupled with a specialized module, _e.g.,_ an MLP, for each expert. We discuss the impact of architecture choices on model performance in the experiment section.

**The MoE workflow**. Given a node or graph instance, the gating model assigns weights \(^{K+1}\) over the expert models, indicating the mixture of shift components on the instance. The output weights, being conditional on the input instance, enable the depiction of heterogeneous distribution shifts that vary across instances. After that, we obtain the output representations from the expert models, which eliminate the effect of the corresponding shift component. Then, the final representation is computed via aggregating the representations based on the weight vector, _i.e.,_

\[h()=(\{(()_{i},_{i}() ) i=0,1,,K\})\]

where \(h\) is the encoder of \(f\). The aggregation function can be a weighted sum over the expert outputs or a selection function that selects the expert output with maximum weight, _e.g.,_

\[h()=()[_{0},,_{K}]^{T}\] (2)

Assuming the distribution shift on an instance is controlled by any single shift component, we have \(h(_{i}())=_{i}(_{i}())_{0}( )=h()\) for \(i=0,,K\), where \(_{i}(_{i}())_{0}()\) holds according to Definition 1. This indicates that \(h\) automatically produces preferentially invariant representations while allowing heterogeneity across different instances, _e.g.,_ different shift types or control strengths. For clarity, we define \(^{(k)}\) as a joint stochastic transform function composed of any \(k\) or fewer transform functions out of the \(K\) transform functions. We refer to the scenario where \(h\) produces referentially invariant representations _w.r.t._\(^{(k)}\) as \(^{(k)}\)-invariance. To extend \(k\) to higher orders (\(k>1\)), we design the objective in Section 3.3, which enforces \(h\) to satisfy \(^{(k)}\)-invariance, ensuring model generalization when multiple shifts exist. After that, a classifier \(\) takes the aggregated representation from Eq 2 for prediction tasks. Thus, we have \(f= h\) as the mixture-of-experts model.

### Training Objective

As shown in Figure 1(b), we consider three trainable modules, _i.e.,_ the gating model \(\), the expert models \(\{_{i}\}_{i=0}^{K}\), and the classifier \(\). We propose the following objective:

\[_{}\ _{f}=_{}( _{1}+_{2}),\ \ \\ _{1}&=_{(,y) _{s}}_{^{(k)}}((^{(k)}() ),Y(^{(k)}))\\ _{2}&=_{(,y) _{s}}_{^{(k)}}[\,((h(^{(k)}()),\ y))\ + d(h(^{(k)}()),\ _{0}())]\] (3)

* \(_{1}\): \(Y(^{(k)})\{0,1\}^{K+1}\) is the ground truth vector, and its \(i\)-th element is 1 if and only if \(_{i}\) composes \(^{(k)}\). BCE is the Binary Cross Entropy. This term indicates that the gating model \(\) is optimized to accurately predict a mixture of shift components.
* \(_{2}\): CE is the Cross Entropy function. \(d(,)\) is a distance function between two representations, and \(\) is a parameter controlling the strength of the distance penalty. In the experiments, we use the Frobenius norm as the distance function, _i.e.,_\(d(_{1},_{2})=\|_{1}-_{2}\|_{F}= ^{n}(_{1i}-_{2i})^{2}}\), and we use \(=1\) for all the experiments. The second loss term optimizes the expert models and the classifier, and we prevent it from backpropagating to the gating model to avoid interference. Specifically, \(_{2}\) aims to improve the encoder's performance in predicting graph classes and achieves referential alignment with the reference model \(_{0}\) via the distance function. Note that, when \(k>1\), \(_{2}\) also enforces \(h\) to be invariant to multiple shifts via the \(^{(k)}\)-invariance condition.

We optimize our model via stochastic gradient descent, where \(^{(k)}\) is sampled at each gradient step. Overall, GraphMETRO yields a MoE model, comprising a gating model with high predictive accuracy, expert models that are aligned and can generate invariant representations in a shared representation space, and a task-specific classifier that utilizes robust and invariant representations for class prediction.

### Discussion and Analysis

**Node classification tasks**. While we introduce our method following a graph-level task setting, GraphMETRO is readily adaptable for node-level tasks. Instead of generating graph representations, GraphMETRO is capable of producing node-level invariant representations. Additionally, we apply stochastic transform functions to the subgraph containing a target node and identify its shift components, which is consistent with the objective in Equation 3.

**Interpretability**. The gating model of GraphMETRO predicts the shift components on the node or graph instance, which provides interpretations and insights into the distribution shifts in unknown datasets. In contrast, existing research on GNN generalization [69; 45; 6; 67] often lacks proper identification and analysis of distribution shifts prevalent in real-world datasets. This creates a gap between human understanding of graph distribution shifts and the actual graph dynamics. To bridge this gap, we offer an in-depth study of the experiments to demonstrate GraphMETRO'insights into the complexity of real graph distributions.

**Computational cost**. The forward process of \(f\) requires \(O(K)\) encoder passes, using the weighted sum aggregation from \((K+1)\) expert outputs. Since the extrapolation process increases the dataset size by a factor of \((K+1)\), the training computation complexity is \(O(K^{2}|_{s}|)\), where \(|_{s}|\) is the size of the source dataset.

## 4 Experiments

We perform systematic experiments on both real-world (Section 4.1) and synthetic datasets (Section 4.2) to validate the generalizability of GraphMETRO under complex distribution shifts.

### Applying GraphMETRO to Real-world Datasets

We perform experiments on real-world datasets, which introduce complex and natural distribution shifts. In these scenarios, the test distribution may not precisely align with the mixture mechanism encountered during training.

**Datasets**. We use four classification datasets, _i.e.,_ WebKB , Twitch , Twitter , and GraphSST2 [78; 58], using the dataset splits from the GOOD benchmark , which exhibit various real-world covariate shifts. Specifically, WebKB is a 5-class prediction task that predicts the classes of university webpages, with nodes split based on different university domains, demonstrating a natural challenge of applying GNNs trained on some university data to other unseen data. Twitch is a binary classification task that predicts whether a user streams mature content, with nodes split mainly by user language domains. Twitter and GraphSST2 are real-world grammar tree graph datasets,

    &  &  &  \\   & WebKB & Twitch & Twitter & SST2 & information \\  ERM & 14.29 \(\) 3.24 & 48.95 \(\) 3.19 & 56.44 \(\) 0.45 & 80.52 \(\) 1.13 & No \\ DANN & 15.08 \(\) 0.37 & 48.98 \(\) 3.22 & 55.38 \(\) 2.29 & 80.53 \(\) 1.40 & No \\ IRM & 13.49 \(\) 0.75 & 47.21 \(\) 0.98 & 55.09 \(\) 2.17 & 80.75 \(\) 1.17 & Yes \\ VREx & 14.29 \(\) 3.24 & 48.99 \(\) 3.20 & 55.98 \(\) 1.92 & 80.20 \(\) 1.39 & Yes \\ GroupDRO & 17.20 \(\) 0.76 & 47.20 \(\) 0.44 & 56.65 \(\) 1.72 & 81.67 \(\) 0.45 & Yes \\ Deep Coral & 13.76 \(\) 1.30 & 49.64 \(\) 2.44 & 55.16 \(\) 0.23 & 78.94 \(\) 1.22 & Yes \\  SRGNN & 13.23 \(\) 2.93 & 47.30 \(\) 1.43 & NA & NA & Yes \\ EERM & 24.61 \(\) 4.86 & 51.34 \(\) 1.41 & NA & NA & No \\ OODGAT & 14.41 \(\) 1.10 & 49.38 \(\) 0.87 & NA & NA & \\ DIR & NA & NA & 55.68 \(\) 2.21 & 81.55 \(\) 1.06 & No \\ G-Mixup & NA & NA & 53.32 \(\) 2.75 & 77.43 \(\) 1.97 & \\ GSAT & NA & NA & 56.40 \(\) 1.76 & 81.49 \(\) 0.76 & No \\ CIGA & NA & NA & 55.70 \(\) 1.39 & 80.44 \(\) 1.24 & No \\  GraphMETRO & **41.11 \(\) 7.47** & **53.50 \(\) 2.42** & **57.24 \(\) 2.56** & **81.87 \(\) 0.22** & No \\ p-value & \(<\) **0.001** & **0.023** & **0.042** & **0.081** & - \\   

Table 1: **Test results on the real-world datasets. We compute the p-value between the results of GraphMETRO and the state-of-the-art methods. The results of GraphMETRO is repeated five times.**where graphs from different domains differ in sentence length and language style, posing a direct challenge of generalizing to different language lengths, styles, and contexts.3

**Baselines**. We use ERM and domain generalization baselines, including DANN , IRM , VREx , GroupDRO , and Deep Coral . Moreover, we compare GraphMETRO with robustness/generalization techniques for GNNs, including DIR , OODGAT , GSAT , and CIGA  for graph classification tasks, and SR-GCN , EERM , and G-Mixup  for node classification tasks.

**Training and evaluation**. We use an individual GNN encoder for each expert in the experiments. Additionally, we include the results of using a shared module among experts in Appendix D.1 due to space limitations. For evaluation metrics, we use ROC-AUC on Twitch and classification accuracy on the other datasets following . See Appendix B for details about the architectures and optimizer.

**Results**. In Table 1, we observe that GraphMETRO consistently outperforms the baseline models across all datasets. It achieves notable improvements of 67.0% and 4.2% relative to EERM on the WebKB and Twitch datasets, respectively. When applied to graph classification tasks, GraphMETRO shows significant improvements, as the baseline methods exhibit similar performance levels. Importantly, GraphMETRO can be applied to both node- and graph-level tasks, whereas many graph-specific methods designed for generalization are limited to one of these tasks. Additionally, GraphMETRO does not require any domain-specific information during training.

**Main Conclusion**. The observation that GraphMETRO is the best-performing method demonstrates its significance for real-world applications, as it excels in handling unseen and wide-ranging distribution shifts. This adaptability is crucial, as real-world graph data often exhibit unpredictable shifts that can affect model performance. Thus, GraphMETRO'versatility ensures its reliability across diverse domains, safeguarding performance in complex real-world scenarios. In Appendix D.2 and D.3, we provide two studies on the impact of the alignment term controlled by \(\) and the stochastic transform function choices on the model performance, analyzing the sensitivity and success of GraphMETRO.

### Inspect GraphMETRO on Synthetic Datasets

Following the experiments on real-world datasets, we perform experiments on synthetic datasets to further inspect and validate the effectiveness of our approach.

Figure 3: **Accuracy on synthetic distribution shifts**. The first row shows the testing accuracy on single shift components. We label the distribution by the clockwise order. The second row shows the testing accuracy on distribution shifts with multiple shift components, where each testing distribution is a composition of two different transformations. For example, _(1, 5)_ denotes a testing distribution where each graph is controlled by _random subgraph (1)_ and _noisy feature (5)_ shift components. We include the numerical values in Appendix E.

**Datasets**. We use graph datasets from citation and social networks. For node classification tasks, we use DBLP  and CiteSeer . For graph classification tasks, we use REDDIT-BINARY and IMDB-MULTII . See Appendix B for dataset processing and details of the transform functions.

**Training and evaluation**. We adopt the same encoder architecture for Empirical Risk Minimization (ERM), ERM with data augmentation (ERM-Aug), and the expert models of GraphMETRO. For ERM-Aug training, we augment the training datasets using the same transform functions we used to construct the testing environments. Finally, we select the model based on the in-distribution validation accuracy and report the testing accuracy on each environment from five trials. See Appendix B for detailed settings and hyperparameters.

**Results**. Figure 3 illustrates our model's performance across single (the first row) and multiple (the second row) shift components. In most test distributions, GraphMETRO exhibits significant improvements or performs on par with two other methods. Notably, on the IMDB-MULTII dataset with noisy node features, GraphMETRO outperforms ERM-Aug by 5.9%, and it enhances performance on DBLP by 4.4% when dealing with random subgraph sampling. In some instances, GraphMETRO even demonstrates improved results on in-distribution datasets, such as a 2.9% and 2.0% boost on Reddit-BINARY and DBLP, respectively. This could be attributed to the increased model expressiveness of the MoE architecture or weak distribution shifts that can exist in the randomly split testing datasets.

### Invariance Matrix for Inspecting GraphMETRO

A key insight from GraphMETRO is that each expert excels in generating invariant representations specifically for a shift component. To delve into the modeling mechanism, we denote \(I^{K K}\) as an invariance matrix. This matrix quantifies the sensitivity of expert \(_{i}\) to the \(j\)-th shift component. Specifically, for \(i[K]\) and \(j[K]\), we have

\[I_{ij}=_{_{s}}_{_{j}}[d(_ {i}(_{j}()),\ _{0}())]\]

Ideally, for a given shift component, the representation produced by the corresponding expert should be most similar to the representation produced by the reference model. That is, the diagonal entries \(I_{ii}\) should be smaller than the off-diagonal entries \(I_{ij}\) for \(j i\) and \(i=1,,K\). In Figure 3(a), we visualize the normalized invariance matrix computed for the Twitter dataset, revealing a pattern that aligns with the analysis. This demonstrates that GraphMETRO effectively adapts to various distribution shifts, indicating that our approach generates consistent invariant representations for each of the shift components.

### Distribution Shift Discovery

With the trained MoE model, we aim to understand the distribution shifts in the target distribution. Here we conduct case studies on the WebKB and Twitch datasets. Specifically, we first validate the

Figure 4: (a) Invariance matrix on the Twitter dataset. Lighter colors indicate a higher invariance of representations produced by each expert. Small values on the diagonal elements of the invariance matrix indicate that each expert excels at generating invariant representations _w.r.t._ the specific shift component. (b) Mixture of distribution shifts identified by GraphMETRO. Higher values indicate a strong shift component in the testing distribution.

gating models' ability to identify mixtures, which is a multitask binary classification with \((K+1)\) classes. The gating models achieve high accuracies of 92.4% on WebKB and 93.8% on the Twitch dataset. As mixtures output by gating models identify significant shift components on an instance, we leverage them as human-understandable interpretations and compute the average mixture across \(_{t}\) as the global mixture on the target distribution. The results in Figure 4b show that the shift component, increased edges, dominates on the WebKB dataset, while the shift components controlling, _e.g.,_ node features and decreasing nodes, show large effects on the Twitch dataset. The results align with dataset structures, _i.e.,_ WebKB's natural shifts across different university domains and Twitch's language-based shifts. While quantitatively validating these observations in complex graph distributions remains a challenge, we aim to explore these complexities in greater depth in future work, which can potentially offer insights into real-world graph dynamics.

## 5 Conclusion and Future Work

This work mitigates the challenge of improving the generalization of Graph Neural Networks (GNNs) to real-world data splits and dynamic graph distributions. To tackle these shifts, we introduce **GraphMETRO**, a mixture-of-aligned-experts architecture, which models graph distribution shifts as mixtures of shift components, each controlling shifts in unique directions with varying complexity.

GraphMETRO distinguishes itself from traditional invariant learning methods, which often rely on environment variables to partition data. Instead, our method treats distribution shifts as mixtures, represented by the gating function's score vector, allowing for infinite environments due to the continuous nature of the score. When restricted to binary outputs, GraphMETRO can simulate finite environments, making it flexible and versatile. Furthermore, the introduction of referential invariant representation via a reference model is a key innovation of our approach.

Experimental results demonstrate that GraphMETRO consistently outperforms baseline methods on real-world datasets, achieving significant improvements. Additional synthetic studies and case analyses further validate the method's effectiveness and adaptability across diverse scenarios.

In future work, we aim to explore the broader applicability of GraphMETRO, including potential extensions to address label distributional shifts. Detailed discussions on these directions are provided in Appendix F.