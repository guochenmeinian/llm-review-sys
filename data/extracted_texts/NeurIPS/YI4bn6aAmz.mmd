# Conformal Prediction Sets for Ordinal Classification

Prasenjit Dey

prasendx@amazon.com

&Srujana Merugu

smerugu@amazon.com

&Sivaramakrishnan Kaveri

kavers@amazon.com

Amazon, India

###### Abstract

Ordinal classification (OC), i.e., labeling instances along classes with a natural ordering, is common in multiple applications such as disease severity labeling and size or budget based recommendations. Often in practical scenarios, it is desirable to obtain a small set of likely classes with a guaranteed high chance of including the true class. Recent works on conformal prediction (CP) address this problem for the classification setting with non-ordered labels but the resulting prediction sets (PS) are often non-contiguous and unsuitable for ordinal classification. In this work, we propose a framework to adapt existing CP methods to generate contiguous sets with guaranteed coverage and minimal cardinality. Our framework employs a novel non-parametric approach for modeling unimodal distributions. Empirical results on both synthetic and real-world datasets demonstrate that our method outperforms SOTA baselines by \(4\%\) on Accuracy@K and \(8\%\) on PS size.

## 1 Introduction

A large number of practical applications involve ordinal classification (OC), i.e., labeling of instances along a set of ordered discrete classes. These include applications such as determining severity level of abuse incidents or disease diagnosis for efficient follow up, predicting customer age-range, apparel size, and budget for improving recommendations and reducing customer cognitive effort by offering adaptive search filters. Often, in such cases, it is desirable to identify a small set of classes (called prediction set or PS) for a given input instance such that the true class is amongst the identified set with a high probability (called coverage level) instead of a single point estimate of the most likely class. This allows more flexibility in choosing operating points that balance precision-recall trade-off. For instance, customers looking for shoes are often recommended products that are not even available in the customer's typical size. An OC model that predicts the customer size would be well suited to improve the recommendations offered. However, limiting the recommendations to a single most likely size is too restrictive because the model accuracy is often low due to variations in size over time and across brands. Instead, it is preferable to identify a small set of product sizes with say \(>90\%\) coverage and use that for automatically filtering the recommendations as shown in Fig. 1.

Conformal prediction (CP) is an emerging area that addresses the problem of predicting a set of likely outputs along with a measure of the confidence or coverage level. While existing CP methods offer ease of deployment and provable coverage guarantees, most of these methods deal with unordered classes and output prediction sets that are non-contiguous and unsuitable for OC settings where the underlying distribution is likely unimodal. For example, for any customer, based on their foot size, a good fit is possible only with a small set of proximal shoe sizes resulting in a unimodal class distribution. Recommending a non-contiguous set of sizes \(\) is not a desirable customer experience. On the other hand, expanding the PS to a minimal contiguous superset leads toa significant expansion of the set size (e.g., \(\) ) diminishing its application utility and diluting coverage guarantees. A similar contiguity requirement arises in applications such as cancer stage diagnosis where predicting non-contiguous labels (stage-1, stage-4) would be discordant compared to contiguous labels. Note that there do exist OC scenarios such as prediction of day of week for an event where unimodality might not be valid as we discuss in Section 6.

While ordinal classification  has been well-studied in the past, most earlier works  do not assume unimodality. Recent works  demonstrate strong benefits of assuming the underlying conditional distribution to be unimodal for OC tasks. However, these techniques primarily focus on point estimates and not conformal predictions. Furthermore, these methods often either impose a restrictive structure on the model  or assume parametric models (e.g., Binomial, Gaussian ), which limits their representation ability and utility for real-world applications. Given the multitude of practical applications, there is a strong need for a principled approach that combines ideas from ordinal classification and conformal predictions to construct minimal contiguous prediction sets when the underlying true class distribution is unimodal.

In this work, we focus on ordinal classification where the true class distribution is unimodal. We build on existing work, Adaptive Prediction Set (APS) , to obtain minimal contiguous prediction sets with provable coverage guarantees. Below we summarize our key contributions:

1. For ordinal classification where the true class distribution is unimodal, we show that any model constrained to output a unimodal distribution can be used with APS algorithm to yield contiguous PS with a guaranteed coverage level. Further, we prove a tight upper bound for the cardinality of this set in terms of the optimal set and the divergence between the fitted model and the true distribution.

2. We provide a novel construction to adapt any DNN-based architecture to always yield a unimodal distribution over ordinal classes while ensuring that any arbitrary unimodal class distribution can be approximated.

3. We study the efficacy of our approach for **C**onformal **P**redictions for **OC** (COPOC) via controlled ablation studies on synthetic datasets which demonstrates the utility of unimodal construction in improving learnability relative for small data sizes, and its versatility compared to other methods that make parametric distributional assumptions.

4. We perform extensive evaluation on benchmark datasets for facial age estimation, HCI classification,image aesthetic score estimation and biomedical image classification with results indicating that our method outperforms SOTA baselines by \(4\%\) on Accuracy@K and \(8\%\) on PS size.

## 2 Contiguous Conformal Prediction Set Problem

**Notation:**\([]\) denotes the probability of event \(\). \([i]_{a}^{b}\) denotes conditioning on integer index \( i,\ a i b\) within a range while \(=[x_{i}]_{i=a}^{b}\) denotes the vector or sequence \([x_{a},,x_{b}]\).

Let \(\) denote input space and \(=\{c_{k}\}_{k=1}^{K}\) denote an ordered set of \(K\) classes. Consider the case where the random variables \(X,Y\) are drawn from \(\) following a joint distribution \(P_{X,Y}\) such that the true underlying conditional distribution \(P_{Y|X}\) is _unimodal_. In other words, for any \(\), denoting \(p_{k}=P_{Y|X}(Y=c_{k}|X=x)\), there exists a class \(c_{m}\) such that \(p_{k} p_{k+1},\ [k]_{m}^{m-1}\) and

Figure 1: A comparison on different model fit on an underlying unimodal distribution. CP on Unconstrained fit results in non-contiguous sets (_red_). While CP on unimodal fits results in contiguous set (_green_), a better fitted model can result in the ideal minimal set (_blue_) which we seek.

\(p_{k} p_{k+1},\ [k]_{m}^{K}\). Given a training dataset \(=\{(_{i},y_{i})\}_{i=1}^{n}\) with samples drawn exchangeably (e.g., i.i.d.) from \(P_{X,Y}\) and a desired coverage level \((1-)(0,1)\) for any unseen input test instance \((X_{test},Y_{test}) D_{test}\) drawn from same distribution, the objective is to determine the minimal non-empty contiguous prediction set (PS) \(_{,}(X_{test})=\{c_{l+1},,c_{u}\},\ 0 l<u K\) satisfying marginal coverage with finite samples above a specified level:

\[[Y_{test}_{,}(X_{test})](1-).\] (1)

The above probability is taken over all \((X_{test},Y_{test}) D_{test}\) data points. While marginal coverage is achievable, it differs from conditional coverage: \([Y_{test}_{,}(X_{test})|X_{test}=_{test}](1-)\), which is a stronger notion. In practice, it is desirable for \(_{,}\) to approximate conditional coverage and achieve it asymptotically in the limit of large sample sizes.

## 3 Related Work

**Conformal Prediction.** Originating in [40; 41], conformal prediction is a statistical approach for generating predictive sets with marginal coverage guarantees (Eqn.1) with finite samples. Earlier methods for calibrating probabilities (e.g., via Platt scaling  or temperature scaling ) and assembling a prediction set by including the most likely classes till the covered probability mass exceeds a specified coverage level do not achieve the desired theoretical guarantees since the calibration is often erroneous. Most recent CP methods use split conformal prediction that enables easy deployment with any arbitrary predictor [31; 21]. APS , , and  introduce techniques aimed at achieving coverage that is similar across regions of feature space whereas  propose methods to achieve equal coverage for each class. Of these, Least Ambiguous Set-valued Classifier (LAC)  provides provably smallest average PS size when probability estimates are approximately correct while APS has been shown to be superior in practice. For a primer on conformal methods please refer . RAPS presents a regularized version of APS for Imagenet. Although relatively new, CP methods are being widely applied for regression, vision, NLP and time-series forecasting [34; 38; 3; 14]. Our work adapts existing CP methods to unimodal ordinal classification setting with an additional contiguity requirement. Ordinal-APS  and  share the same motivation but propose a greedy CP method that results in larger prediction sets compared to COPOC (Table 4).

**Ordinal Classification.** Early works on ordinal classification [18; 7; 45] are based on regular classification or regression, ignoring ordinal relationship and non-uniform separation among labels. Some DNN-based methods model cumulative distribution (CDF) up to each ordered class using multiple binary classifiers [8; 39; 6; 28] but the output class probabilities are not necessarily unimodal and are also not guaranteed to form a valid CDF. Label smoothing methods [15; 16; 24; 12; 20; 42] convert hard target labels into unimodal prior distributions to be used as the reference for the training loss but these methods are often sub-optimal since the assumed priors might not reflect the true distribution, classes might not be equi-spaced categories and additionally test predictions might not necessarily be unimodal. [22; 5] learns a non-parametric unimodal distribution as a constraint optimization problem in the loss function which is not only difficult to optimize but also does not guarantee unimodality on test data. In [4; 10], unimodality is guaranteed by assuming a Binomial or Poisson distribution but has limited representation ability due to a single degree-of-freedom which does not capture heteroscedastic noise. Keeping in view the potential benefits of accurate unimodal distribution learning for the ordinal classification setting, our work explores a non-parametric flexible DNN-based approach that guarantees unimodality of the predicted class distribution even on test data.

## 4 Solution Approach

To generate conformal predictions for ordinal classification, we consider two questions: (i) what is the primary drawback of the existing CP methods for classification with respect to ordered labels setting? (ii) how do we address this drawback?

Most CP methods for classification [35; 36; 9] are based on a split calibration approach where the training data \(D\) is divided into two sets. The first set \(D_{train}\) is used to learn a probabilistic model \(_{Y|X}()\) using a blackbox learning algorithm while the second set \(D_{cal}\) is used to determine the optimal conformity score threshold for a desired coverage level, which is then used for conformal inference. Below we discuss two common CP methods:

**Least Ambiguous Set Values Classifier (LAC) .** Here, the conformal PS for a test instance \(_{test}\) is constructed using a calibration rule of the form,

\[_{D,}(_{test})=\{c|_{k}(_ {test})_{D_{cal}}()\},\] (2)

where \(_{k}()=_{Y|X}(Y=c_{k}|X=)\) is the class probability from the model and \(_{D_{cal}}()\) is the score threshold defined as the bias-adjusted \(()^{th}\) quantile of the model score of the true label.

**Adaptive Prediction Set (APS) .** In APS, for every instance \((,y) D_{cal}\), we compute a conformity score defined as the cumulative probability mass required to include the true label corresponding to that instance, i.e., \(s()=_{k=1}^{T}_{_{k}}()\), where \(T\) is chosen such that \(c_{_{T}}=y\) and \(\) is the permutation of \(\{1,,K\}\) that sorts \(_{Y|X}(.|)\) in the descending order from most likely to least likely. Given a desired coverage \((1-)\), we compute a suitable score threshold \(_{D_{cal}}()=Quantile(\{s()| D_{cal}\} ),),\) where \(n=|D_{cal}|\). The conformal PS in APS is constructed using:

\[_{,}(_{test})=\{c_{_{1}},c_{_{2}}  c_{_{j}}\}j=sup\{j:_{k=1}^{j}_{_{k}}( _{test})<_{D_{cal}}()\}+1.\] (3)

More details on the coverage guarantees of APS are in Appendix B. While the above methods yield minimal sets with provable guarantees on marginal coverage, the primary drawback is that the resulting prediction sets are not necessarily contiguous, which is an important requirement for ordinal classification when underlying true distribution is unimodal.

A naive solution is to consider the minimal contiguous set that covers the PS output by CP method. For example, if CP outputs \(\{2,4,6\}\), we can produce a minimal contiguous set by including "in-between" classes i.e., \(\{2,3,4,5,6\}.\) However, this would end up having a much larger cardinality, which invalidates the tight coverage bounds (refer Appendix B) (i.e., the new set will have a coverage much higher than the upper bound guaranteed for that method). Hence, we consider an alternative approach based on the observation that if the blackbox learning algorithm in the first step is constrained to output a unimodal distribution, then the classes with probability above a threshold will cluster around the mode. The conformal calibration rules in Eqn. 2 and 3, which are designed to choose classes in the order of class probability will thus result in contiguous sets. Using this insight, we propose a solution depicted in Fig. 2 that consists of two steps: (a) design a learning algorithm that can accurately model training data such that the output class distribution is always unimodal, (b) adapt an appropriate CP algorithm based on Eqn. 2 or Eqn. 3 to identify the PS. While this high level approach can be used with any of the CP method, in our current work, we consider the Adaptive Predictive Set (APS) approach  since it provides tight guarantees on marginal coverage and has been empirically shown to achieve better conditional coverage than LAC  and smaller set size than CQC . We now describe our solution beginning with a study of the properties of the resulting PS (Sec. 4.1) followed by construction of the unimodal classifier (Sec. 4.2).

### Conformal Prediction Sets from Unimodal Models

We now study the properties of PS corresponding to the COPOC approach outlined in Fig. 2 where for every input instance \(X\), both the true underlying distribution \(P_{Y|X}\) and the learned classification model \(_{Y|X}\) are unimodal. In other words, there exists a class \(c_{}\) such that \(_{k}_{k+1},\ [k]_{1}^{-1}\), and \(_{k}_{k+1},\ [k]_{}^{K}\). For a given \(_{test}\) and coverage level \((1-)\), let \(S_{}^{oracle}(_{test})\) denote the minimal contiguous PS that can be constructed by an _oracle_ that knows the underlying \(P_{Y|X}\), i.e.,

\[S_{}^{oracle}(_{test})=*{arg\,min}_{S=\{c_{l+1}, ,c_{}\},\ 0 l<u K,\ [y_{test} S|_{test}](1-)}[u-l].\] (4)

where \(p_{k}=P_{Y|X}(Y=c_{k}|X=_{test}).\) This is the set with the least cardinality satisfying contiguity and conditional coverage. In case of multiple solutions, the oracle can pick any one at random. In practice, we do not have access to the true \(P_{Y|X}\) and \(_{D,}(_{test})\) has to be constructed from the approximate distribution \(_{Y|X}\) using CP methods such as APS for valid coverage guarantees. Lemma 1 establishes the contiguity of the prediction sets resulting from LAC and APS. Thm. 1 bounds the cardinality of the APS PS relative to the oracle sets. Further, when \(_{Y|X}\) is a consistent estimator ofthe true \(P_{Y|X}\), the APS conformity score threshold asymptotically converges to \(\) and the cardinality of \(_{D,}(_{test})\) converges to that of the oracle .

**Lemma 1**.: 1 _Given a fitted unimodal model \(_{Y|X}\), for any \(_{test}\) and \((0,1]\), prediction sets \(_{D,}(_{test})\) constructed using Eqn.2 or 3 have at least one solution which is contiguous (amongst multiple possibilities). When \(_{Y|X}\) is strictly unimodal, all the solutions are contiguous._

Proof Sketch:.: By design, the prediction sets from Eqn.2 or 3 have to contain the top \(k\) most likely classes for some \(k\) which results in contiguity in case of strict unimodality. When the unimodality is not strict, non-contiguous sets may satisfy the required constraints due to classes with equal probabilities at the boundaries, but the shortest span solution is contiguous. 

**Theorem 1**.: _For any \(\), let \(p_{k}()=P_{Y|X}(Y=c_{k}|X=)\) and \(_{k}()=_{Y|X}(Y=c_{k}|X=)\) denote the true and fitted model class probabilities that are always unimodal. Let \(_{k}()=_{k^{}=1}^{k}p_{k^{}}()\) and \(_{k}()=_{k^{}=1}^{k}_{k^{}}( )\) denote the corresponding cumulative distribution functions. If \(|_{k}()-_{k}()|,\ [k]_{k}^{K}\) for a constant \(\), then for any \((0,1]\), \( D_{test}\), the APS and oracle prediction sets from Eqn.3 and Eqn. 4 satisfy \(|_{D,}()||S_{-4-}^{ oracle}()|\) where \(n\) is the size of the calibration set._

Proof Sketch:.: To establish the result, we prove the following two statements: (a) \(|_{D,}(x)||S_{1-_{D_{cal}}()-2}^{ Oracle}(x)\), and (b) \(|S_{1-_{D_{cal}}()-2}^{ Oracle}()||S_{-4 -}^{ oracle}|\). From Eqn. 4 and Lemma 1, we observe that the unimodality of \(()\) and \(p()\) leads to the oracle prediction sets and at least one of the APS prediction sets being contiguous. Let \(_{D,}()=\{c_{+1},,c_{}\},\ 0<  K\) and \(S_{1-_{D_{cal}}()-2}^{ Oracle}()=\{c_{l^{*}+1}, ,c_{u^{*}}\},0 l^{*}<u^{*} K\). From the definitions and contiguity, we observe that the probability mass of \(_{D,}()\) w.r.t. \(\) equals \((_{}()-_{}()) _{D_{cal}}()\) while that of \(S_{1-_{D_{cal}}()-2}^{ oracle}()\) w.r.t \(p\) equals \((_{u^{*}}()-_{l^{*}}())_{D_{cal}}( )+2\). Using the divergence bound on the two CDFs and the fact that \(_{D,}()\) is the minimal contiguous set with probability mass \(_{D_{cal}}()\) as per \(\) yields (a). Using the divergence bound, we also observe that the marginal coverage, \(P[y_{D,}()]_{D_{cal}}()-2\). Combining this from the APS coverage guarantees (Theorem 3 in Appendix B) yields part (b). 

### Non-Parametric Unimodal Distribution Learning

We now consider the problem of learning a classification model that is guaranteed to be unimodal and also expressive enough to accurately model any unimodal distribution. Given universal approximation properties of DNNs with respect to probability distributions , these form a natural choice as a multi-class classification learning algorithm. For a given \(\), let \(()=(;)\) denote the output of last fully connected layer of a DNN model \(()\), where \(\) is the model parameters and \(()\) a \(K\)-dimensional vector where \(K=||\). Softmax operation is applied to \(()\) to obtain class probability distribution. Predicted probability of class \(c_{k}\) for \(\) is given by \(_{k}()=_{k}())}{_{k=1}^{K} (_{k}())}\). Training is usually performed by minimizing cross-entropy (CE) loss between this predicted distribution and the ground truth. Since CE loss does not assume any constraints on underlying distribution, it can, in principle, model any arbitrary distribution asymptotically with large enough data and model size. When the true underlying distribution \(P_{Y|X}\) is unimodal, this standard learning approach results in a classifier \(_{Y|X}\) that approximates \(P_{Y|X}\) and is itself unimodal _assuming large enough data and model size_. However, in practice with limited high-dimensional data and limited model capacity, the standard training is not guaranteed to reach the true _"global minima"_ or even adhere to the unimodality constraints that lead to contiguous prediction sets. Hence, there is a need for introducing the right _inductive bias_ that truncates the hypothesis class to be searched while ensuring it is expressive enough to include the true hypothesis so as to allow for better learnability and generalization . For our scenario, the set of unimodal distributions is the most natural hypothesis class. While previous works  consider a set of parameterised unimodal distributions (e.g., based on Gaussian or Binomial distributions), we propose a construction that ensures the hypothesis class exactly maps to the set of unimodal distributions on the \(K\)-simplex.

**Construction.** Let \(\) be the set of all discrete unimodal distributions on \(\). For any \(\) with \(()=_{Y|X}(Y|X=)\), the class probabilities \([_{k}()]_{k=1}^{K}\) would be unimodal, i.e., there exists a mode \(c_{}\) such that the sequence is monotonically non-decreasing before \(c_{}\) and non-increasing post \(c_{}\). The corresponding log probabilities denoted by \(z_{k}=(_{k}())^{-},\ [k]_{1}^{K}\) also form a unimodal sequence with the same mode and are all negative valued.

Let \(:^{+}^{-}\) be any strictly monotonically decreasing bijective function, Then, for any \(z^{-}\), there is a unique \(r^{+}\) such that \((r)=z\). Let \(r_{k}=^{-1}(z_{k}),\ [k]_{}^{K}\) and \(r_{k}=-^{-1}(z_{k}),\ [k]_{1}^{-1}\) then given \([z_{k}]_{k=1}^{K}\), we obtain a unique2 non-decreasing sequence \([r_{k}]_{k=1}^{K}\). Let \(^{E}:R R^{-}\) be the "even" extension of \(()\), i.e., \(^{E}(r)=(r)\) for \(r R^{+}\) and \(^{E}(r)=(-r)\) for \(r R^{-}\), then we have \(^{E}(r_{k})=z_{k}\). Possible choices include \(^{E}(x)=-|x|^{d}\) for any real \(d\). The monotonically increasing \([r_{k}]_{k=1}^{K}\) can be generated from the output vector \(\) of a DNN constrained such that \(v_{k} 0,\ [k]_{2}^{K}\) using a non-negative transformation \(:R R^{+}\) followed by a cumulative summation. Fig. 3 depicts our proposed construction for any DNN \(()\) along with equations below.

\[()&=(,);\ \ v_{1}=_{1}();\ \ v_{k}=(_{k}()),\ \ [k]_{2}^{K}\,,\\ r_{1}&=v_{1};\ \ r_{k}=r_{k-1}+v_{k},\ \ [k]_{2}^{K};\ \ z _{k}=^{E}(r_{k});\ \ _{k}=)}{_{k=1}^{K}(z_{k})},\ \ [k]_{1}^{K}\,.\] (5)

This network can be trained with standard CE loss. From Thm. 2, any distribution in \(\) maps to an output vector \(() R^{d}\), which can be approximated using DNNs with appropriate capacity . Fig. 8 in Appendix C.4 presents some illustrative examples of Unimodal model fit of COPOC on a public dataset.

**Theorem 2**.: _Let \(: R^{K}\), \(:R R^{+}\) and \(^{E}:R R^{-}\) such that \(^{E}(r)=^{E}(-r),\  r R\) and its restriction to \(R^{+}\) is a strictly monotonically decreasing bijective function. (a) Then, the model output constructed as per Eqn. 5 is always unimodal, i.e., \((),\ \). (b) Further, given any \(()\) for \(\), there exists a well defined function \(() R^{K}\) that satisfies Eqn. 5 if \(()\) is surjective on \(R^{+}\)._

Proof Sketch: Part (a) follows by observing that the non-decreasing sequence \([r_{k}]_{k=1}^{K}\) will result in a unimodal \([z_{k}]_{k=1}^{K}\) and unimodal \(()\) with mode corresponding to sign change in \(r_{k}\) sequence. Part (b) follows by constructing a unique inverse for \(^{E}()\) by pivoting on the largest indexed mode \(\) of \(()\). For \(k<\), we choose \((^{E})^{-1}(z_{k}) R^{-}\) and for \(k\), we choose \((^{E})^{-1}(z_{k}) R^{+}\). 

## 5 Experiments

We evaluate the utility of our approach (COPOC) for generating minimal contiguous PS for a desired coverage for ordinal classification on multiple datasets. We investigate the following questions:* **RQ1:** How does COPOC perform relative to SOTA methods on OC tasks?
* **RQ2:** What benefits does COPOC offer relative to parametric modeling on synthetic datasets?
* **RQ3:** Under what conditions does COPOC outperform vanilla model trained with CE loss (V-CE)? Is COPOC an approximately consistent estimator of the underlying distribution?
* **RQ4:** How does COPOC performance vary with a different choice of conformal inference such as LAC  instead of APS?
* **RQ5:** How does Ordinal-APS  which also outputs a contiguous prediction set (PS) over ordinal labels with regular model training fares against COPOC with it's unimodal training?

### Experimental Setup

**Algorithms** : We compare our proposed unimodal construction (COPOC) against six SOTA methods as well as modeling with vanilla cross-entropy loss _V-CE_ using APS for conformal inference for all methods for a fair comparison. For COPOC, we experimented with various choices of \(\) and \(\) and chose \((x)=|x|\) and \(^{E}(x)=-|x|\) based on performance (see Appendix C.3). The SOTA methods include approaches that (a) utilize soft labels generated from linear exponentially decaying distributions _SORD_[(12], and linear adaptive Gaussians with variance learning _AVDL_, (b) specifically model unimodal distribution through parametric assumptions _Binomial_ and _Binomial-Temp_, and (c) non-parametric methods based on unimodality aware loss function _Uni-Loss_ or ordinality imposed in embedding space _POE_. In Sec. 5.5, we also evaluate an alternative conformal procedure LAC  with our unimodal construction.

**Metrics**: Similar to [35; 9] we compute PS Size (\(|\)_PS_\(|\)) \(@90\%\) coverage i.e., \(=0.1\). Since APS does not guarantee contiguity for models that do not output unimodal distribution, we consider a minimal contiguous interval that covers the output PS and report the size. We also report the fraction of instances which resulted in non-contiguous PS for which we needed to expand the original non-contiguous set (_CV_%). To evaluate performance on OC tasks, we report _MAE_ - mean absolute difference between predicted and ground-truth and Accuracy@K (_Acc@K_) that captures if the ground truth was in the top \(k\) predictions. This can benefit unimodal constrained models where classes adjacent to the mode will receive the next greatest probability mass. For comparing different conformal prediction methods in Sec. 5.5, we use size-stratified coverage violation (SSCV)  that measures violations of the conditional coverage property and is suited for high dimensional data.

**Datasets**: We evaluate COPOC on four public image datasets: age-detection (Adience ), historical image dating (HCI ), image aesthetics estimation (Aesthetic ) and biomedical classification (Retina-MNIST ). These datasets contain \(8\), \(5\), \(5\), and \(5\) ordered classes respectively and the number of instances is \(26580\), \(1325\), \(15680\), and \(1600\). More details are included in Appendix C.1. We also experimented with synthetic data with the generation process and result discussed in Sec 5.3.

### RQ1: Performance on ordinal classification on real world datasets

Table 1 presents comparison of COPOC with other methods on the HCI dataset. Results on other datasets included in the Appendix C.1 due to space constraints. COPOC performs at par with other baselines in terms of _MAE_ & _Acc@1_ and significantly outperforms them on _Acc@2_, _Acc@3_ & \(|PS|\) ( \(8\%\) smaller compared to the next best one). We observe that _V-CE_ and _POE_ have the highest

   & & MAE & Acc@1 & Acc@2 & Acc@3 & PSI & CV\% \\   & CE & \(0.68 0.03\) & \(54.3 2.6\) & \(75.3 3.1\) & \(88.9 1.6\) & \(3.28 0.14\) & \(24.4 1.2\) \\   & POE & \(0.66 0.05\) & \(56.5 1.8\) & \(76.5 2.5\) & \(89.0 2.1\) & \(3.1 0.18\) & \(9.8 1.2\) \\   & SORD & \(0.65 0.06\) & \(56.2 2.8\) & \(77.1 2.9\) & \(89.8 2.6\) & \(2.96 0.19\) & \(2.7 1.1\) \\   & AVDL & \(\) & \(\) & \(77.9 2.4\) & \(89.8 1.05\) & \(2.98 0.11\) & \(2.1 1.4\) \\   & Binomial & \(0.68 0.05\) & \(54.5 1.2\) & \(75.8 2.6\) & \(88.8 1.8\) & \(3.01 0.16\) & \(0\) \\   & Binomial-temp & \(0.66 0.04\) & \(55.5 1.8\) & \(78 2.2\) & \(90.1 2.1\) & \(2.90 0.11\) & \(0\) \\   & Uni-loss & \(0.67 0.09\) & \(54.5 3.1\) & \(74.8 2.5\) & \(88.1 2.5\) & \(3.05 0.38\) & \(5.1 1.9\) \\   & COPOC & \(0.65 0.04\) & \(56.5 2.0\) & \(\) & \(\) & \(\) & \(0\) \\  

Table 1: **Results on HCI**: Mean & std. error is reported for 5 trials. Best results bolded.

contiguity violations _CV_%. Though _SORD_, _AVDL_, and _Uni-loss_ enforce unimodality in training, it does not necessarily translate to unimodality in test samples (indicated by high _CV_%) resulting in poor performance on _Acc@2_ and _Acc@3_ and [PS], which is critical in OC tasks. Since _Binomial-temp_ enforces unimodality by construction, it performs better than the above methods on _Acc@k_. However, unimodality constraint on DNN in COPOC results in even better model fit for OC tasks which translates to higher _Acc@K_ & shorter \(|PS|\).

### RQ2: Ablative Study of Unimodal Modeling Methods on Synthetic Data

To generate synthetic data, we consider the label set \(\) of size 10, i.e., \(Y\{c_{1},,c_{10}\}\) and input space \( R^{10}\), where \(X_{1} Uniform(1,10)\) and \(X_{2},,X_{10}\) follow independent normal distribution. Further, we define \(h(X)=100*(sin(0.2*X_{1})+cos(0.4*X_{1}))\). To generate the labels, we consider a conditional distribution function \(Q_{Z|X}\) that generates \(Z\) with \(h(X)\) as distributional parameters and a mapping function \(g:R\) that maps \(Z\) to the output label \(Y\). Thus, for each \(x\), we generate \(z Q_{Z|X}(x)\) and \(y=g(z)\). We consider two choices of mapping functions \(g()\). The first choice \(g_{equi}\) partitions the range of \(Z\) into \(10\) equi-bins which can be mapped to the label set in a linear order while the second choice \(g_{non-equi}\) partitions the range of \(Z\) into \(10\) bins of arbitrary width to assign labels. From the construction, one can see that \(g_{equi}\) considers classes to be equi-spaced categories while \(g_{non-equi}\) does not. We consider the following four synthetic datasets in increasing order of complexity and train models (details in Appendix C.2 )

* _D1_: \(Q_{Z|X}(x)\) = Double-Exponential with mean at \(h(x)\) and constant variance; \(g=g_{equi}\).
* _D2_: \(Q_{Z|X}(x)\) = Double-Exponential with mean at \(h(x)\) and constant variance; \(g=g_{non-equi}\).
* _D3_: \(Q_{Z|X}(x)\) = Gaussian with mean and variance varying with \(h(x)\); \(g=g_{non-equi}\).
* _D4_: \(Q_{Z|X}(x)\) = Gaussian,Poisson or double Exponential with mean and variance varying with \(h(x)\) chosen at random for each \(x\); \(g=g_{non-equi}\).

Since we have the true distribution \(P_{Y|X}\), we compute the Oracle PS size and compare it with that of other methods. We report KL Divergence (_KL_) between the true and predicted distributions with lower _KL_ indicating better model fit. Table 2 shows that for _D1_, _SORD_ fits the data well with lowest _KL_ and _MAE_ as it explicitly models exponential distribution assuming all classes to be equi-spaced. However, on _D2_ and _D3_, COPOC outperforms all the other methods on the model fit and also results in shorter \(|PS|\) closer to Oracle. On _D2_ and _D3_, _SORD_ performs poorly because it assumes both constant variance across input spaces and equi-distant classes which are not valid for these datasets. Since _D3_ draws samples from Gaussian distribution with heteroskedastic noise, _AVDL_ outperforms _SORD_ but is inferior to COPOC. Since _D4_ is drawn from a more complex distribution, COPOC outperforms the rest with an even larger margin. The _Binomial_ variants underperform throughout due to distributional

   & & V-CE & SORD & AVDL & Binomial & Binomial-temp & Uni-Loss & COPOC \\   & KL Div. & \(0.04 0.01\) & \(\) & \(0.07 0.01\) & \(0.1 0.02\) & \(0.09 0.01\) & \(0.12 0.10\) & \(0.04 0.01\) \\  & IPSl [\(3.03\)] & \(\) & \(\) & \(3.12 0.05\) & \(3.16 0.04\) & \(3.16 0.04\) & \(3.61 0.15\) & \(\) \\  & CV\% & \(0.3 0.01\) & \(0\) & \(0\) & \(0\) & \(1.7 0.4\) & \(0\) \\  & MAE & \(\) & \(\) & \(0.67 0.02\) & \(0.68 0.02\) & \(0.69 0.01\) & \(0.68 0.03\) & \(\) \\   & KL Div. & \(0.06 0.01\) & \(0.17 0.02\) & \(0.18 0.04\) & \(0.19 0.03\) & \(0.17 0.02\) & \(0.26 0.11\) & \(\) \\  & PSI [\(2.56\)] & \(2.65 0.04\) & \(2.72 0.04\) & \(2.78 0.04\) & \(2.85 0.06\) & \(2.81 0.02\) & \(3.12 0.09\) & \(\) \\  & CV\% & \(2.3 0.05\) & \(0.3 0.1\) & \(0.6 0.1\) & \(0\) & \(0\) & \(2.9 0.7\) & \(0\) \\  & MAE & \(\) & \(0.59 0.01\) & \(0.60 0.01\) & \(0.61 0.01\) & \(0.63 0.04\) & \(0.57 0.02\) \\   & KL Div. & \(0.13 0.01\) & \(0.38 0.02\) & \(0.17 0.01\) & \(0.49 0.03\) & \(0.44 0.02\) & \(0.4 0.09\) & \(\) \\  & IPSl [\(1.58\)] & \(1.73 0.02\) & \(1.96 0.06\) & \(1.85 0.04\) & \(2.39 0.04\) & \(2.38 0.02\) & \(2.35 0.1\) & \(\) \\  & CV\% & \(2.9 0.1\) & \(1.1 0.2\) & \(0.8 0.2\) & \(0\) & \(0\) & \(2.7 0.4\) & \(0\) \\  & MAE & \(0.24 0.02\) & \(0.25 0.02\) & \(\) & \(0.28 0.01\) & \(0.26 0.02\) & \(0.27 0.04\) & \(\) \\   & KL Div. & \(0.14 0.01\) & \(0.33 0.01\) & \(0.31 0.02\) & \(0.44 0.04\) & \(0.35 0.02\) & \(0.35 0.1\) & \(\) \\  & IPSl [\(4.40\)] & \(4.67 0.03\) & \(4.73 0.04\) & \(4.78 0.05\) & \(4.83 0.04\) & \(4.82 0.06\) & \(5.04 0.2\) & \(\) \\   & CV\% & \(4.7 0.8\) & \(2.7 0.4\) & \(2.8 0.3\) & \(0\) & \(0\) & \(5.6 1.1\) & \(0\) \\   & MAE & \(1.26 0.02\) & \(1.27 0.03\) & \(1.27 0.02\) & \(1.31 0.04\) & \(1.29 0.02\) & \(1.30 0.03\) & \(\) \\  

Table 2: **Results on Synthetic Data**: We report KL-Div, PS Size [Ground truth Oracle size], _CV_% and _MAE_. Mean and std. error is reported across 10 random trials. Best mean results are bolded.

mismatch while _Uni-Loss_ exhibits high variance indicating convergence issues. As with the real benchmarks, enforcing unimodality via loss or soft-labels does not guarantee unimodailty on test data, indicated by high _CV%_. Interestingly _V-CE_ performs exceedingly well throughout in terms of _KL_ beating more sophisticated methods. The overall observation is that the performance of the methods depends largely on the validity of the assumptions and relatively unconstrained nature of COPOC makes it more versatile.

### RQ3: COPOC vs Vanilla-CE and Consistency Properties

We compare COPOC against _V-CE_ by varying train-data size (\(=N\)) on synthetic data _D4_. Fraction of samples used to train each model is \(3/4\), and rest is used for calibration and test. All experiments are repeated \(10\) times with random realization of train set. Comparing the _KL_ loss on train set in Fig. 4, we observe that COPOC has lower _KL_ in early stages with lesser data and performance for both converges when \(N\) reaches to \( 80K\) and _KL_ asymptotically becomes negligible. Fig. 6 compares [PS] and CV% by varying \(N\) on _D4_. [PS] of COPOC approaches those of Oracle with far lesser samples compared to VC-E. Thus, COPOC with its inductive bias fits the data better with lesser samples compared to unbiased CE yielding shorter sets with finite samples. Above results also suggest that COPOC might be approximately consistent. Note that with increased \(N\), _V-CE_ results in a better model fit, lower _CV%_ and shorter sets, indicating that these are correlated.

### RQ4: Choice of Conformal Inference: LAC vs. APS

Table 3 provides a comparison of PS size and size-stratified coverage violation(SSCV) of APS against LAC when applied on the output of our proposed unimodal model for \(=0.1\). Both methods generate contiguous prediction sets. Although LAC consistently produces shorter sets, it also has slightly worse SSCV metrics across datasets. In Fig. 6, we plot IPSI and SSCV across different \(\) for synthetic data D4 and results are similar. We observe that LAC achieves the smallest prediction set size but sacrifices adaptiveness (conditional coverage) in the process.

### RQ5: COPOC vs Ordinal-APS 

Table 4 shows an empirical comparison of COPOC against APS and Ordinal-APS applied over a Vanilla DNN trained with Cross-entropy loss (V-CE) on public datasets and synthetic data _D4_ described in Sec. 5.2 and 5.3. For V-CE with APS we consider a minimal contiguous interval that covers the output PS and report its size. We observe that Ordinal-APS produces significantly shorter sets compared to V-CE with APS. However, COPOC significantly outperforms Ordinal-APS because of better unimodal data fit. Note that Ordinal-APS outputs a contiguous PS over ordinal labels

irrespective of whether the posterior distribution generated by the model is unimodal or not. COPOC with unimodal regularization results in better model fit and hence shorter PS sizes.

## 6 Validity of the Unimodality Assumption

Unimodality assumption might not be universally applicable for all OC scenarios (eg., prediction of preference ratings, event-hour-of-day etc.). However, there do exist a large number of OC applications where it is beneficial to assume unimodality as validated by multiple notable works in computer vision domain [12; 20; 42; 4]. Table 5 shows the negative log-likelihood (NLL) of vanilla DNN (V-CE) fitted with CE loss (unconstrained fit) and COPOC (unimodal fit) on four public datasets described in Sec. 5.2. The superior fit of COPOC indicated by lower NLL justifies the unimodality assumption for these datasets. COPOC makes an assumption on the underlying distribution being unimodal and one could potentially check the validity of the assumption by comparing the likelihood of the unimodal and unconstrained fits. Note that even if the unimodality assumption is not true, the theoretical coverage guarantees of the prediction set produced by COPOC would still hold but the cardinality bounds would be weaker since the fitted distribution would deviate significantly from the true underlying distribution.

## 7 Conclusion, Broader Impact, and Limitations

We proposed an approach to construct minimal contiguous prediction sets for ordinal classification with guarantees on coverage and cardinality along with empirical validation of efficacy on both real and synthetic datasets. Our solution employs a novel architecture for non-parametric modelling of arbitrary unimodal class distributions without sacrificing representation ability. With ML-driven systems playing an increasingly important role in our society, it is critical to provide practical guarantees on the reliability and uncertainty of the ML model output. While existing CP methods  address this concern for the general classification setting, there is a need for specialized solutions for application scenarios with inherent ordinality and unimodality in class labels, e.g., predicting cancer stage of biopsy signals. Our work makes an important contribution in this direction and also provides a way to effectively communicate the limitations of the model adaptively across the input space with bigger prediction sets for harder instances. The proposed methods have numerous practical applications in improving recommendations based on an ordered attribute (e.g., budget), and reducing human supervision effort for ordinal classification tasks. Future directions include extensions to tasks where classes are partially ordered or hierarchically grouped (e.g., topic hierarchies). Consistency of our proposed unimodal construction is also worth investigating.

**Limitations:** COPOC makes an assumption on the underlying distribution being unimodal and might lead to a sub-optimal fit if the assumption does not hold. Furthermore, it can be paired only with CP methods such as LAC and APS where the prediction sets can be viewed as upper-level sets of the predicted class probabilities, i.e., set of classes with probability higher than a threshold. Due to the reliance on the CP methods, the coverage guarantees are also valid only only when the assumption on the exchangeability of the data points holds true.

   & APS & Ordinal-APS & COPOC \\  Synthetic D4 & \(4.67 0.03\) & \(4.59 0.03\) & \(4.50 0.02\) \\  HCI & \(3.28 0.14\) & \(3.03 0.15\) & \(2.66 0.13\) \\  Adience & \(4.82 0.24\) & \(2.67 0.12\) & \(2.26 0.06\) \\  Aesthetic & \(1.96 0.2\) & \(1.77 0.05\) & \(1.70 0.06\) \\  Retina MNIST & \(3.6 0.08\) & \(3.28 0.02\) & \(3.03 0.01\) \\  

Table 4: Ordinal-APS  vs COPOC on Synthetic data _D4_ and public image datasets. Mean and std. error is reported after 10 trials.

   & V-CE & COPOC \\  HCI & \(1.73 0.13\) & \(1.59 0.15\) \\  Adience & \(2.33 0.18\) & \(1.66 0.21\) \\  Aesthetic & \(1.49 0.01\) & \(0.71 0.02\) \\  Retina MNIST & \(1.24 0.04\) & \(1.23 0.04\) \\  

Table 5: Comparison of COPOC against V-CE in terms of NLL. Mean and std. error is reported after 10 trials.