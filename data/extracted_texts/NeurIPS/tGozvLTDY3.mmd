# DG-SLAM: Robust Dynamic Gaussian Splatting SLAM with Hybrid Pose Optimization

Yueming Xu\({}^{1}\)  Haochen Jiang\({}^{1}\)  Zhongyang Xiao\({}^{2}\) Jianfeng Feng\({}^{1}\) Li Zhang\({}^{1}\)

\({}^{1}\)Fudan University \({}^{2}\)Autonomous Driving Division, NIO

https://github.com/fudan-zvg/DG-SLAM

Yueming Xu and Hachen Jiang contribute equally to this work.Li Zhang (lizhangfd@fudan.edu.cn) is the corresponding author with School of Data Science, Fudan University.

###### Abstract

Achieving robust and precise pose estimation in dynamic scenes is a significant research challenge in Visual Simultaneous Localization and Mapping (SLAM). Recent advancements integrating Gaussian Splatting into SLAM systems have proven effective in creating high-quality renderings using explicit 3D Gaussian models, significantly improving environmental reconstruction fidelity. However, these approaches depend on a static environment assumption and face challenges in dynamic environments due to inconsistent observations of geometry and photometry. To address this problem, we propose DG-SLAM, the first robust dynamic visual SLAM system grounded in 3D Gaussians, which provides precise camera pose estimation alongside high-fidelity reconstructions. Specifically, we propose effective strategies, including motion mask generation, adaptive Gaussian point management, and a hybrid camera tracking algorithm to improve the accuracy and robustness of pose estimation. Extensive experiments demonstrate that DG-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and novel-view synthesis in dynamic scenes, outperforming existing methods meanwhile preserving real-time rendering ability.

## 1 Introduction

Visual simultaneous localization and mapping (SLAM), the task of reconstructing a 3D map within an unknown environment while simultaneously estimating the camera pose, is recognized as a critical component to achieving autonomous navigation in novel 3D environments for mobile robots . It has been widely used in various forms in fields such as robotics, autonomous driving, and augmented/virtual reality (AR/VR). However, the majority of previous research  typically relies on the assumption of static environments, limiting the practical applicability of this technology in daily life. Consequently, how to achieve accurate and robust pose estimation in dynamic scenes remains an urgent problem to be addressed for mobile robots.

Recently, many researchers  have endeavored to replace the conventional explicit representations used in visual SLAM, such as Signed Distance Function (SDF), voxel grids , meshes , and surfel clouds , with the neural radiance field (NeRF)  approach for reconstructing the neural implicit map. This novel map representation is more continuous, efficient, and able to be optimized with differentiable rendering, which has the potential to benefit applications like navigation and reconstruction. However, these methods exhibit two primary issues: the scene's bounds are required to be predefined to initialize the neural voxel grid; and the implicit representation proves challenging for information fusion and editing. To address these problems, recent workslike GS-SLAM , SplaTam , and Gaussian splatting SLAM  leverage the 3D-GS  to explicit represent the scene's map. This explicit geometric representation is also smooth, continuous, and differentiable. Moreover, a substantial array of Gaussians can be rendered with high efficiency through splatting rasterization techniques, achieving up to 300 frames per second (FPS) at a resolution of 1080p. However, all these above-mentioned neural SLAM methods do not perform well in dynamic scenes. The robustness of these systems significantly decreases, even leading to tracking failures, when dynamic objects appear in the environment.

To tackle these problems, we propose a novel 3D Gaussian-based visual SLAM that can reliably track camera motion in dynamic indoor environments. Due to the capability of 3D-GS to accomplish high-quality rendering in real-time, the SLAM system more readily converges to a global optimum during pose optimization, thereby achieving better and more stable pose optimization results. A cornerstone of our approach to achieving robust pose estimation lies in the innovative motion mask generation algorithm. This algorithm filters out sampled pixels situated within invalid zones, thereby refining the estimation process. In addition to the constraint of depth residual, we employ a spatio-temporal consistency strategy within an observation window to generate depth warp masks. By incrementally fusing the depth warp mask and semantic mask, the motion mask will become more precise to reflect the true motion state of objects. To improve the accuracy and stability of pose estimation, we leverage DROID-SLAM  odometry (DROID-VO) to provide an initial pose estimate and devise a coarse-to-fine optimization algorithm built upon the initially estimated camera pose. This aims to minimize the disparity between pose estimation and the reconstructed map, employing photorealistic alignment optimization through Gaussian Splatting. Moreover, this hybrid pose optimization approach effectively ensures the accuracy and quality of the generated depth warp mask, thereby facilitating better performance in the next camera tracking stage. To obtain high-quality rendering results, we propose a novel adaptive Gaussian point addition and pruning method to keep the geometry clean and enable accurate and robust camera tracking. Capitalizing on the factor graph structure inherent in DROID-SLAM, our system is capable of executing dense Bundle Adjustment (DBA) upon completion of tracking to eliminate accumulated errors.

In summary, our **contributions** are summarized as follows: **(i)** To the best of our knowledge, this is the first robust dynamic Gaussian splatting SLAM with hybrid pose optimization, capable of achieving both real-time rendering and high-fidelity reconstruction performance. **(ii)** To mitigate the impact of dynamic objects during pose estimation, we propose an advanced motion mask generation strategy that integrates spatio-temporal consistent depth masks with semantic priors, thereby significantly enhancing the precision of motion object segmentation. **(iii)** We design a hybrid camera tracking strategy utilizing the coarse-to-fine pose optimization algorithm to improve the consistency and accuracy between the estimated pose and the reconstructed map. **(iv)** To better manage and expand the Gaussian map, we propose an adaptive Gaussian point addition and pruning strategy. It ensures geometric integrity and facilitates accurate camera tracking. **(v)** Extensive evaluations on two challenging dynamic datasets and one common static dataset demonstrate the state-of-the-art performance of our proposed SLAM system, particularly in real-world scenarios.

## 2 Related work

**Visual SLAM with dynamic objects filter.** Dynamic object filtering is pivotal for reconstructing static scenes and bolstering the robustness of pose estimation. Existing approaches fall into two main categories: the first relies on re-sampling and residual optimization strategies to remove outliers, as seen in works such as ORB-SLAM2 , ORB-SLAM3 , and Refusion . These methods, however, are generally limited to addressing small-scale motions and often falter in the face of extensive, continuous object movements. The second group employs the additional prior knowledge, for example, semantic segmentation or object detection prior [20; 21; 22; 23; 24; 25; 26], to remove the dynamic objects. However, all these methods often exhibit a domain gap when applied in real-world environments, leading to the introduction of prediction errors. More recently, deep neural networks have been employed to build end-to-end visual odometry, which performs better in specific environments such as DROID-SLAM , DytanVO , and DeFlowSLAM . However, these methods still require a substantial amount of training data and are unable to reconstruct the high-fidelity static map.

**RGB-D SLAM with neural implicit representation.** Neural implicit scene representations, also known as neural fields , have attracted considerable attention in the field of RGB-D SLAM for their impressive expressiveness and low memory footprint. Initial studies, including iMap  and DI-Fusion , explored the utilization of a single MLP and a feature grid to encode scene geometries within a latent space. However, they both share a critical issue: the problem of network forgetting, which is catastrophic for long-term localization and mapping. In response to this limitation, NICE-SLAM  introduces a hierarchical feature grid approach to enhance scene representation fidelity and implements a localized feature updating strategy to address the issue of network forgetting. While these advancements contribute to improved accuracy, they necessitate greater memory consumption and may impact the system's ability to operate in real-time. More recently, existing methods like Vox-Fusion , Co-SLAM , and ESLAM  explore sparse encoding or tri-plane representation strategy to improve the quality of scene reconstruction and the system's execution efficiency. Point-SLAM  draws inspiration from the concept of Point-NeRF , utilizing neural points to encode spatial geometry and color features. It employs an explicit method to represent spatial maps, effectively improving the accuracy of localization and mapping. All these methods have demonstrated impressive results based on the strong assumptions of static scene conditions. The robustness of these systems significantly decreases when dynamic objects appear in the environment. Recently, Rodyn-SLAM  proposed utilizing optical flow and semantic segmentation prior to filter out dynamic objects, and employing a neural rendering method as the frontend. However, this approach is computationally intensive and limits the maximum accuracy achievable in pose estimation.

**3D Gaussian splatting SLAM methods.** Compared to the aforementioned NeRF-based SLAM methods, 3D Gaussian splatting (3D-GS)  has garnered widespread interest among researchers due to its advantages in high-fidelity and real-time rendering. Unlike previous implicit map representation methods, 3D-GS explicitly models scene maps by independent Gaussian spheres, naturally endowing it with geometric structure property. Some researchers [17; 15; 34; 16; 35] are exploring the replacement of implicit representations (NeRF) with 3D-GS in the mapping thread. However, these methods are currently constrained by the assumption of static environments, rendering them ineffective in dynamic scenes. It significantly restricts the practical application of Gaussian SLAM systems in real-life scenarios. Under the premise of ensuring high-fidelity reconstruction and real-time rendering, our system is designed to improve the accuracy and robustness of pose estimation under dynamic environments.

## 3 Approach

Given a sequence of RGB-D frames \(\{I_{i},D_{i}\}_{i=1}^{N},I_{t}^{3},D_{t}\), our method (Fig. 1) aims to simultaneously recover camera poses \(\{_{i}\}_{i=1}^{N},_{t}(3)\) and reconstruct the static 3D scene map represented by 3D Gaussian sphere in dynamic environments. Similar to most modern SLAM systems [36; 37], our system comprises two distinct processes: the tracking process as the frontend and the mapping process as the backend.

Figure 1: **Overview of DG-SLAM. Given a series of RGB-D frames, we reconstruct the static high-fidelity 3D Gaussian map and optimize the camera pose represented with lie algebra \(_{i}\).**

### 3D Gaussian map representation

To obtain real-time rendering and high-fidelity reconstruction mapping, we represent the scene as a set of 3d Gaussian ellipsoid \(\), which simultaneously possesses geometric and appearance properties.

\[=\{_{i}:(_{i},_{i},_{i},_{i})|_{i}\}\] (1)

Each 3D Gaussian ellipsoid \(_{i}\) is composed of its center position \(_{i}^{3}\), covariance matrix \(_{i}^{3 3}\), opacity \(_{i}\), and spherical harmonics coefficients \(_{i}^{16}\).

**Color and depth splatting rendering.** To obtain the rendering image of color and depth, we project the 3D Gaussian \((_{w},_{w})\) in world coordinate to 2D Gaussian \((_{I},_{I})\) on the image plane:

\[_{I}=(_{w}^{c}_{w}),_{I}=_{w}^{T}^{T},\] (2)

where \(\) is the viewing transformation and \(\) is the Jacobian of the affine approximation of the projective transformation. Following the alpha blending method in 3DGS , we accumulate the splatting Gaussian ellipsoid along the observation image pixel at the current estimation pose \(_{i}\) to render the color and depth as:

\[=_{i}_{i}f_{i}(_{I},_{I}) _{j=1}^{i-1}(1-f_{j}(_{I},_{I})),= _{i}_{i}f_{i}(_{I},_{I})_{j=1}^ {i-1}(1-f_{j}(_{I},_{I})),\] (3)

where \(f_{i}()\) is the Gaussian distribution function weighted by opacity \(_{i}\). \(_{i}\) represents the color of the projected Gaussian point computed by learnable spherical harmonics coefficients \(_{i}\). Similarly, \(_{i}\) denotes the depth of Gaussian point \(_{i}\), obtained by projecting to z-axis in the camera coordinate.

**Accumulated opacity.** We use accumulated opacity \(\) to represent the rendering reliability for each pixel and judge whether the Gaussian map is well-optimized.

\[=_{i}f_{i}(_{I},_{I})_{j=1}^{i-1} (1-f_{j}(_{I},_{I})),\] (4)

### Motion mask generation

For each input keyframe, we select its associated keyframe set \(\) within a sliding window. To reduce the computation and improve the accuracy of generating motion mask, we employ the depth warping operation solely on keyframes. To ensure the overlap between adjacent keyframes is not too small, we employ optical-flow distance to determine keyframe insertion. In regions with more intense motion, our goal is to insert as many keyframes as possible.

For the pixel \(p\) in keyframe \(i\), we reproject it onto keyframe \(j\) as follows:

\[_{i j}=f_{warp}(_{ji},_{i},D_{i}(_{i}) )=_{ji}(^{-1}D_{i}(_{i})_{i}^{homo} ),\] (5)

where \(\) and \(_{ji}\) represent the intrinsic matrix and the transformation matrix between frame \(i\) and frame \(j\), respectively. \(_{i}^{homo}=(u,v,1)\) is the homogeneous coordinate of \(_{i}\).

Given any associate keyframes \(D_{i},D_{j}\), we utilize warp function \(f_{warp}\) to compute the residual of reprojection depth value. By setting a suitable threshold \(e_{th}\), we derive the depth warp mask \(}_{j,i}^{wd}\) corresponding to dynamic objects as:

\[}_{j,i}^{wd}:\{_{_{i} D_{j}}(D_{j}(_{i j})-D_{i}(_{i})<e_{th})_{m n}\}\] (6)

where \(_{m n}\) represents a matrix of the same size as the image, filled with ones. The \(\) operation signifies that for each element in the matrix \(_{m n}\), we assess whether its warp depth meets a specified threshold and subsequently modify the corresponding value at that position. As illustrated in Fig. 1, to derive a more precise warp mask, we consider the spatial coherence of object motion within a sliding window of length \(N\) and combine the multiple observation warp masks. Unlike ReFusion , our method can mitigate the potential impact of observation noise from a single warp mask. When object motion becomes significant, we only mask the foreground pixels where the depth residual is positive to avoid a large portion of pixels being masked as dynamic regions. Subsequently, we integrate the warp mask and semantic mask to derive the final motion mask \(}_{j}\) as:

\[}_{j}=}_{j,i}^{wd}}_{j,i-1}^{wd}}_{j,i-2}^{wd} }_{j,i-N}^{wd}}_{j}^{sg},\] (7)

Thanks to our innovative approach to generating motion masks, the omission of dynamic objects by semantic priors can be effectively compensated. Furthermore, by leveraging a spatial consistency strategy, the inaccuracy of edge region recognition during depth warping can be significantly reduced.

### Coarse-to-fine camera tracking.

The constant speed motion model struggles to infer a reasonable initial optimized pose value in dynamic scenes. An inaccurate initial pose will lead to optimization getting trapped in local optima more easily and can affect the generation quality of the depth warp mask. To achieve more precise pose estimation in dynamic environments, we utilize the visual odometry (VO) component from DROID-SLAM  as the coarse pose estimation results in camera tracking. We also conduct a dense bundle adjustment in every interaction for a set of keyframes to optimize the corresponding pose \(\) and depth \(\):

\[(,)=_{(i,j)}\|_{ ij}^{}-_{C}(G_{ij}_{C}^{-1}(_{i},_{i} ))\|_{_{ij},}_{j}}^{2},\] (8)

where \(_{ij}=(w_{ij})\), \(w_{ij}\) represents the confidence weights. \(G_{ij}\) denotes the relative transformation between the poses \(G_{i}\) and \(G_{j}\). \(_{i}\) means a grid of pixel coordinates. Moreover, \(_{ij}^{}\) is corrected correspondence as predicted by updated optical flow estimation. To overcome the influence of dynamic objects on bundle adjustment, we introduce suppression through the motion mask \(}_{j}\) applied to the weighted covariance matrix. Consequently, the weighted confidence associated with dynamic objects is reduced to zero.

To further improve the accuracy of pose estimation and minimize inconsistencies between the estimated pose and the reconstructed map, we implement fine camera tracking by leveraging Gaussian splatting, based on the initial obtained pose. Due to obstructions by dynamic objects and the inadequate optimization of Gaussian points in the map, the rendered image may exhibit blurring or black floaters. Therefore, we employ the accumulated opacity, as outlined in 4, to denote the pixel rendering reliability. The reliable mask \(}_{i}\) for camera tracking is generated as follows:

\[}_{i}:\{_{[_{j},v_{j}] I_{i}}( [_{j},v_{j}]>_{track})_{m n}\},\] (9)

where \(_{j},v_{j}\) represents the pixel location. When the accumulated opacity at the pixel exceeds a given threshold \(_{track}\), we consider the Gaussian point associated with that pixel to be well-optimized. Consequently, using the rendered image at these pixels for pose estimation is deemed reliable. The overall loss function is finally formulated as the following minimization:

\[_{i}^{}=*{arg\,min}_{_{i}}_{1}_ {i=1}^{M}((,_{i})-C)}_{i}}_{i}_{2}^{2}+_{2} }_{ N_{d}}((, _{i})-D)}_{i}}_{i} _{2}^{2}.\] (10)

Where \(_{i}\) denotes the camera pose requiring optimization. \(C\) and \(D\) denote the ground truth color and depth map, respectively. \(M\) represents the number of sampled pixels in the current image. Note that only pixels with valid depth value \(N_{d}\) are considered in optimization.

This hybrid pose optimization approach effectively ensures the accuracy and quality of the warp mask, thereby facilitating better performance in the next camera tracking stage. In short, this hybrid pose optimization strategy enables us to achieve more precise and robust pose estimation whether in dynamic or static environments.

### SLAM system

**Map initialization.** For the first frame, we do not conduct the tracking step and the camera pose is set to the identity matrix. To better initialization, we reduce the gradient-based dynamic radius to half so that can add more Gaussian points. For pixels located outside the motion mask, we sample and reproject them to the world coordinates. We initialize the Gaussian point color and center position \(_{i}\) with the RGB value and reprojection coordinate of the pixel, respectively. The opacity \(_{i}\) is set as 0.1 and the scale vector \(_{i}\) is initialized based on the mean distance of the three nearest neighbor points. The rotation matrix \(_{i}\) is initialized as the identity matrix.

**Map optimization.** To optimize the scene Gaussian representation, we render depth and color in independent view as seen Eq. 3, comparing with the ground truth map:

\[_{rgb}=_{i=1}^{M}(-C) }_{i}_{2}^{2},_{depth}= {1}{N_{d}}_{ N_{d}}(-D) }_{i}_{2}^{2},\] (11)

In contrast to existing methods, we introduce the motion mask \(}_{i}\) to remove sampled pixels within the dynamic region effectively. The final Gaussian map optimization is performed by minimizing thegeometric depth loss and photometric color loss :

\[_{mapping}=_{1}_{rgb}+_{2}_{ssim} +_{3}_{depth},\] (12)

where \(_{ssim}\) denotes the structural similarity loss between two images. Moreover, \(_{1},_{2},_{3}\) are weight factors for balance in the optimization process.

**Adapative Gaussian point adding strategy.** To guarantee the superior scene representation capability of 3D Gaussian, we also employ dynamic Gaussian point density, which is inspired by Point-SLAM . The insertion radius is determined based on color gradient, allowing for a denser allocation of points in areas with high texture while minimizing point addition in regions of low texture. This method efficiently manages memory consumption by adapting the point density according to the textural complexity of the scene.

To ensure the points we add are both necessary and effective, we adopt a two-stage adaptive point-add strategy. For new viewpoints without previous observation, we initially perform uniform sampling across the entire image to ensure that new observed areas can be covered by Gaussian points. Moreover, if the accumulated opacity \(\), calculated by Eq. 4, falls below the threshold \(o_{th}\), or the depth residual between the rendered pixels and the ground truth depth is excessively large, we then add 3D Gaussian points based on these under-fitting pixels. At last, these new Gaussian points will be initialized based on the point density.

**Map point deleting strategy.** Given that the added 3D Gaussian points have not been subjected to geometric consistency verification and may exhibit unreasonable representation values during optimization, this could lead to the generation of a low-quality dense map or the introduction of numerous artifacts in the rendering image. we implement the pruning operation as part of the Gaussian map optimization. To ensure the points we delete are both reasonable and accurate, we also adopt a two-stage point delete strategy. For all Gaussian points observed from the current viewpoint, we delete points based on three criteria: the opacity value, the maximum scale, and the ratio between the ellipsoid's major and minor axes, described as follow:

\[_{i}<_{}()>_{s1} )}{()}>_{s2}.\] (13)

Moreover, due to potential inaccuracies along the edges of the current motion mask, adding these points could result in artifacts within the scene map. Thus, we project these points on keyframes in a small sliding window to recheck whether they can be observed by these keyframes. If the number of observations of a point is too low, we consider its addition to be insufficiently robust, and therefore, it will be removed from the current Gaussian map.

## 4 Experiments

**Datasets.** Our methodology is evaluated using three publicly available challenging datasets: _TUM RGB-D_ dataset , _BONN RGB-D Dynamic_ dataset  and _ScanNet_. These three datasets contain both challenging dynamic scenes and real static scenes. This selection facilitates a comprehensive assessment of our approach under varied conditions, demonstrating its applicability and robustness in real-world indoor environments.

Figure 2: **Qualitative results of the motion mask generation.** By fusing the semantic mask and depth warp mask, the final mask will be more precise.

**Metrics.** For the evaluation of pose estimation, we utilize the Root Mean Square Error (RMSE) and Standard Deviation (STD) of Absolute Trajectory Error (ATE) . Prior to assessment, the estimated trajectory is aligned with the ground truth trajectory via Horn's Procrustes method. , ensuring a coherent basis for evaluation. To evaluate the reconstruction quality of static maps in dynamic scenes, we employ three metrics(i) _Accuracy_ (cm), (ii) _Completion_ (cm), and (iii) _Completion Ratio_ (percentage of points within a 5cm threshold), following NICE-SLAM . Since the BONN dataset provides only the ground truth point cloud, we randomly sampled 200,000 points from the GT point cloud and the reconstructed mesh surface to calculate these metrics.

**Implementation details.** We run our DG-SLAM on an RTX 3090 Ti GPU at 2 FPS on BONN datasets, which takes roughly 9GB of memory. We set the loss weight \(_{1}=0.9\), \(_{2}=0.2\) and \(_{3}=0.1\) to train our model. The number of iterations for the tracking and mapping processes has been set to 20 and 40, respectively. For the Gaussian points deleting, we set \(_{}=0.005\), \(_{S1}=0.4\) and \(_{S2}=36\) to avoid the generation of abnormal Gaussian points. What's more, we utilize Oneformer  to generate prior semantic segmentation. For the depth wrap mask, we set the window size to 4 and the depth threshold to 0.6. We also adopt the keyframe selection strategy from DROID-VO  based on optical flow.

    & & ball & ball2 & ps\_trk & ps\_trk2 & mv\_box2 & Avg. \\    & **Acc.[cm]\(\)** & X & 24.30 & 43.11 & 74.92 & 17.56 & 39.97 \\  & **Comp.[cm]\(\)** & X & 16.65 & 117.95 & 172.20 & 18.19 & 81.25 \\  & **Comp. Ratio**\( 5\)cm\(\%\)\(\)\(\) & X & 29.68 & 15.89 & 13.96 & 32.18 & 22.93 \\   & **Acc.[cm]\(\)** & 10.61 & 14.49 & 26.46 & 26.00 & 12.73 & 18.06 \\  & **Comp.[cm]\(\)** & 10.65 & 40.23 & 124.86 & 118.35 & 10.22 & 60.86 \\  & **Comp. Ratio**\( 5\)cm\(\%\)\(\) & 34.10 & 3.21 & 2.05 & 2.90 & 39.10 & 16.27 \\   & **Acc.[cm]\(\)** & 17.17 & 26.82 & 59.18 & 89.22 & 12.32 & 40.94 \\  & **Comp.[cm]\(\)** & **9.11** & 13.58 & 145.78 & 186.65 & 10.03 & 73.03 \\  & **Comp. Ratio**\( 5\)cm\(\%\)\(\) & 47.44 & 47.94 & 20.53 & 17.33 & 41.41 & 34.93 \\   & **Acc.[cm]\(\)** & **7.09** & **5.80** & **9.14** & **11.78** & **6.56** & **8.06** \\  & **Comp.[cm]\(\)** & 9.80 & **8.05** & **17.99** & **20.10** & **7.61** & **15.46** \\   & **Comp. Ratio**\( 5\)cm\(\%\)\(\) & **49.46** & **52.41** & **34.62** & **32.81** & **49.02** & **43.67** \\   

Table 1: **Reconstruction results on several dynamic scene sequences in the _BONN_ dataset. Instances of tracking failures are denoted by “X”. The most superior outcomes within the domain of RGB-D SLAMs are highlighted in bold for emphasis.**

Figure 3: **Visual comparison of the rendering image on the _TUM_ and _BONN_ datasets. Our results are more complete and accurate without the dynamic object floaters.**

### Evaluation of generating motion mask

We evaluated our method on the balloon and move_no_box2 sequences of the _BONN_ dataset to show the qualitative results of the generated motion mask. In these two sequences, in addition to the typical motion of pedestrians, there are movements of atypical objects accompanying the human, such as balloons and boxes. It might be missed if we rely solely on the semantic prior. As shown in Fig. 2, our generated methods notably improve the precision of motion mask segmentation, effectively reducing the inconsistencies along edge regions and detecting the true dynamic objects.

### Evaluation of mapping performance

To more effectively showcase the performance of our system in dynamic environments, we evaluate the reconstruction results from both qualitative and quantitative perspectives. Given dynamic scene datasets seldom offer static GT mesh or point cloud, we utilize the _BONN_ dataset for our quantitative analytical experiments. We compare our DG-SLAM method against current state-of-the-art neural-based SLAM methods, all of which are available as open-source projects.

   Method & f3/w\_r & f3/w\_x & f3/w\_s & f3/s\_x & f2/d\_p & f3/l\_o & Avg. \\   ORB-SLAM3  & 68.7 & 28.1 & 2.0 & **1.0** & **1.5** & **1.0** & 17.1 \\ ReFusion  & - & 9.9 & 1.7 & 4.0 & - & - & 5.2 \\ Co-fusion  & - & 69.6 & 55.1 & 2.7 & - & - & 42.5 \\ MID-fusion  & - & 6.8 & 2.3 & 6.2 & - & - & 5.1 \\ EM-fusion  & - & 6.6 & 1.4 & 3.7 & - & - & 3.9 \\  iMAP* & 139.5 & 111.5 & 137.3 & 23.6 & 119.0 & 5.8 & 89.5 \\ NICE-SLAM & X & 113.8 & 88.2 & 7.9 & X & 6.9 & 54.2 \\ Vox-Fusion & X & 146.6 & 109.9 & 3.8 & X & 26.1 & 71.6 \\ Co-SLAM & 52.1 & 51.8 & 49.5 & 6.0 & 7.6 & 2.4 & 28.3 \\ ESLAM & 90.4 & 45.7 & 93.6 & 7.6 & X & 2.5 & 48.0 \\ Rodyn-SLAM & 7.8 & 8.3 & 1.7 & 5.1 & 5.6 & 2.8 & 5.3 \\  SplaTAM & 100.4 & 218.3 & 115.2 & 1.7 & 5.4 & 5.1 & 74.4 \\ GS-SLAM & 33.5 & 37.7 & 8.4 & 2.7 & 8.6 & 1.8 & 15.5 \\  DROID-VO & 10.0 & 1.7 & 0.7 & 1.1 & 3.7 & 2.3 & 3.3 \\ DG-SLAM(Ours) & **4.3** & **1.6** & **0.6** & **1.0** & 3.2 & 2.3 & **2.2** \\   

Table 2: **Camera tracking results on several dynamic scene sequences in the _TUM_ dataset. “\(*\)” denotes the version reproduced by NICE-SLAM. “X” and “-” denote the tracking failures and absence of mention, respectively. The metric is Absolute Trajectory Error (ATE) and the unit is [cm].**

   Method & ball & ball2 & ps\_tk & ps\_tk2 & ball\_tk & mv\_box2 & Avg. \\   ORB-SLAM3  & 5.8 & 17.7 & 70.7 & 77.9 & **3.1** & **3.5** & 29.8 \\ ReFusion  & 17.5 & 25.4 & 28.9 & 46.3 & 30.2 & 17.9 & 27.7 \\  iMAP* & 14.9 & 67.0 & 28.3 & 52.8 & 24.8 & 28.3 & 36.1 \\ NICE-SLAM & X & 66.8 & 54.9 & 45.3 & 21.2 & 31.9 & 44.1 \\ Vox-Fusion & 65.7 & 82.1 & 128.6 & 162.2 & 43.9 & 47.5 & 88.4 \\ Co-SLAM & 28.8 & 20.6 & 61.0 & 59.1 & 38.3 & 70.0 & 46.3 \\ ESLAM & 22.6 & 36.2 & 48.0 & 51.4 & 12.4 & 17.7 & 31.4 \\ Rodyn-SLAM & 7.9 & 11.5 & 14.5 & 13.8 & 13.3 & 12.6 & 12.3 \\  SplaTAM & 35.5 & 36.1 & 149.7 & 91.2 & 12.5 & 19.0 & 57.4 \\ GS-SLAM & 37.5 & 26.8 & 46.8 & 50.4 & 31.9 & 4.8 & 33.1 \\  DROID-VO & 5.4 & 4.6 & 21.4 & 46.0 & 8.9 & 5.9 & 15.4 \\ DG-SLAM(Ours) & **3.7** & **4.1** & **4.5** & **6.9** & 10.0 & **3.5** & **5.5** \\   

Table 3: **Camera tracking results on several dynamic scene sequences in the _BONN_ dataset. “\(*\)” denotes the version reproduced by NICE-SLAM. “X” denotes the tracking failures. The metric is ATE and the unit is [cm].**

[MISSING_PAGE_FAIL:9]

enhancing the quality of the rendered images. One of the main contributions comes from the hybrid camera tracking strategy, which indirectly underscores the importance of eliminating inconsistencies between pose estimation and map reconstruction.

### Time consumption analysis

As shown in Tab. 7, we report time consumption (per frame) of the tracking and mapping without computing semantic segmentation. These results were achieved through an identical experimental setup that involved conducting 20 iterations for tracking and 40 iterations for mapping, all processed on an RTX 3090Ti GPU. Benefiting from the rapid execution speed of Droid-VO and fast rendering of 3D Gaussian Splatting, our method exhibits a leading edge in terms of time consumption during the tracking process. Although our approach does not match the mapping speed of Co-SLAM, it achieves high-quality mapping and high-fidelity rendering with a competitive mapping running time. We also evaluate the inference time of our used semantic segmentation network, which required 163ms for every frame. It should be noted that our approach does not focus on the specific semantic segmentation network used, but rather on the fusion method itself.

## 5 Conclusion

In this paper, we have presented DG-SLAM, a robust dynamic Gaussian splatting SLAM with hybrid pose optimization under dynamic environments. Via motion mask filter strategy and coarse-to-fine camera tracking algorithm, our system significantly advances the accuracy and robustness of pose estimation within dynamic scenes. The proposed adaptive 3D Gaussians adding and pruning strategy effectively improves the quality of reconstructed maps and rendering images. We demonstrate its effectiveness in achieving state-of-the-art results in camera pose estimation, scene reconstruction, and novel-view synthesis in dynamic environments. While the tracking and reconstruction of large-scale scenes is currently the biggest limitation of our system, we believe it will be addressed by a more flexible loop-closure optimization algorithm in future work. Moreover, the accuracy of pose estimation of our system is still influenced by the segmentation precision of the semantic prior. Therefore, efficiently perceiving moving objects within the dynamic scenes remains an unresolved issue that warrants further exploration.