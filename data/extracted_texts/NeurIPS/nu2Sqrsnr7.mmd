# Compute-Optimal Solutions for Acoustic Wave Equation Using Hard-Constraint PINNs

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This paper explores the optimal imposition of hard constraints, strategic sampling of PDEs, and computational domain scaling for solving the acoustic wave equation within a specified computational budget. First, we derive a formula to systematically enforce hard boundary and initial conditions in Physics-Informed Neural Networks (PINNs), employing continuous functions within the PINN ansatz to ensure that these conditions are satisfied. We demonstrate that optimally selecting these functions significantly enhances the convergence of the solution. Secondly, we introduce a Dynamic Amplitude-Focused Sampling (DAFS) method that optimizes the efficiency of hard-constraint PINNs under a fixed number of sampling points. Leveraging these strategies, we develop an algorithm to determine the optimal computational domain size, given a computational budget. Our approach offers a practical framework for domain decomposition in large-scale implementation of acoustic wave equation systems.

## 1 Introduction

The concept of using artificial neural networks to solve differential equations was first explored in the 1990s by Lagaris et al. (1998). In the work of Lagaris et al. (1998), they developed an ansatz solution that inherently satisfies the boundary conditions (BC) and the initial conditions (IC) of differential equations. More recently, the advent of physics-informed neural networks (PINNs) was marked by the influential study of Raissi et al. (2019). This work leverages modern deep neural networks to solve forward and inverse problems involving nonlinear partial differential equations (PDEs), incorporating BCs and ICs through soft constraints in loss functions.

Subsequent research has introduced various modifications to PINNs to enhance their accuracy, efficiency, and scalability (Lu et al., 2021a). There are a couple of drawbacks for many PINNs with soft constraints for BCs and ICs. The selection of weights and samples for BCs and ICs cannot certainly be determined and requires many trial-and-error tests. Even when the loss function is minimized, the BCs and ICs are not strictly satisfied. To target the scaling problems of general PDEs and take advantage of parallel computing, XPINNs and FBPINNs have been developed based on domain decomposition methods (Jagtap and Karniadakis, 2020; Shukla et al., 2021; Moseley et al., 2023).

There are a few key points that these previous reseearches missed. First, how to formulate ansatz solutions satisfying BCs and ICs, specifically the function multiplier of NN. Second, if BC and IC are inherently satisfied by constructing the ansatz solution, how to optimally sample the PDEs in the training process. Furthermore, for the existing PINNs handling scaling problems, how to decompose the domain to save the overall compute budget.

In this paper, we set up a 1D wave equation problem and investigate the optimal sampling and constraint imposing method given a compute budget.

The contributions of this paper are as follows.

* We systematically derived the implementation of hard BC and IC constraints in PINNs to solve acoustic wave equations. We give a strategy to select basic functions in the PINN ansatz solution that guarantee the satisfaction of BCs and ICs. We find that optimal selection of the basic function in the PINN ansatz can improve the convergence of PINNs.
* We developed a Dynamic Amplitude-Focused Sampling (DAFS) algorithm to improve the convergence of hard-constraint PINNs for wave equations given a fixed number of sampling points.
* With the hard constraint and importance sampling strategies, we propose an algorithm to find the optimal size of the computational given a compute budget. This domain size optimization algorithm can help the domain decomposition-based PINNs for large-scale problems save computational cost.

## 2 Related Work

Hard constraintHard constraint PINNs can guarantee the satisfaction of BCs, ICs, symmetries, and/or conservation laws. There are comprehensive studies of embedding BCs in PINNs. Lu et al. (2021) demonstrated various ansatz equations to strictly meet Dirichlet and periodic BCs, and proposed the penalty method and the augmented Lagrangian method to impose inequality constraints as hard constraints. Liu et al. (2022) developed a unified ansatz formula to enforce the Dirichlet, Neumann, and Robin boundary conditions for high-dimensional and geometrically complex domains. Moseley et al. (2023) implemented the hard Dirichlet in the subdomain using a \(^{2}( x)\) function as the multiplier function of the neural networks in their FBPINN ansatz solution. However, studies on how to impose both hard BC and IC constraints in PINNs for acoustic wave equations that have a second-order time derivative term are still limited. Alkhadhr and Almekkawy (2023) compared the accuracy and performance of PINNs with a combination of hard-BC/soft-BC and hard-IC/soft-IC for solving a 1D wave equation with a time-dependent point source function. This implementation of the hard-IC only considers the satisfaction of the wavefield values at the initial time \(u(x,t=0)\), but neglects the hard constraint of the first-order time derivative of the wavefield \(u(x,t)\), i.e., \(_{t}u(x,t=0)\). Brecht et al. (2023) proposed improved physics-informed DeepONets with hard constraints, and presented a numerical example of a 1D standing wave equation with Dirichlet BCs. The DeepONet framework used in the paper has an inherent satisfaction of the initial wavefield, but \(_{t}u(x,t=0)\) is also neglected. This neglection does not affect the numerical results for the 1D standing wave equation in their paper, since they simply assume \(_{t}u(x,t=0)=0\).

Strategic SamplingMany sampling algorithms have been developed to improve the training efficiency, mitigating failure modes of PINNs. (Wu et al., 2023) provided a comprehensive comparison of ten sampling methods, including non-adaptive and residual-based adaptive methods. Daw et al. (2023) proposed a Retain-Resample-Release (R3) Sampling algorithm to mitigate the failure propagation during the training processes of PINNs. (Gao et al., 2023, 2023) developed failure informed adaptive sampling for PINNs, with the extentions of combining re-sampling and subset simulation. Yang et al. (2023) introduced a Dynamic Mesh-Based Importance Sampling (DMIS) method to enhance the training of PINNs. Additionally, (Zhang et al., 2024) proposed an annealed adaptive importance sampling method for solving high-dimensional partial differential equations using PINNs.

Domain ScalingComputational domain scaling is a key issue to apply PINNs to real-world large spatial-temporal scale applications. (Jagtap and Karniadakis, 2020) proposed a generalized space-time domain decomposition framework for PINNs, named extended PINNs (XPINNs), which can handle nonlinear PDEs on complex-geometry domains. XPINNs provide large representation and parallelization capacity by deploying multiple neural networks in smaller subdomains, offering both space and time parallelization to reduce training costs effectively. Shukla et al. (2021) developed a distributed framework for PINNs based on two extensions: conservative PINNs (cPINNs) and XPINNs. These methods employ domain decomposition in space and time-space, respectively, enhancing the parallelization capacity, representation capacity, and efficient hyperparameter tuning of PINNs. The framework allows for optimizing all hyperparameters of each neural network separately in each subdomain, providing significant advantages for multi-scale and multi-physics problems. They demonstrated the efficiency of cPINNs and XPINNs through various forward problems, highlighting that cPINNs are more communication-efficient while XPINNs offer greater flexibility for handling complex subdomains. Moseley et al. (2023) addressed the limitations of PINNs in solving large domains and multi-scale solutions by proposing Finite Basis PINNs (FBPINNs). FBPINNs use neural networks to learn basis functions defined over small, overlapping subdomains, inspired by classical finite element methods. This approach mitigates the spectral bias of neural networks and reduces the complexity of the optimization problem by using smaller neural networks in a parallel, divide-and-conquer approach. Their experiments showed that FBPINNs outperform standard PINNs in accuracy and computational efficiency for both small and large, multi-scale problems. Chalapathi et al. (2024) introduced a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE) in neural network architectures. This method imposes constraints over smaller decomposed domains, with each domain solved by an expert through differentiable optimization. The independence of each expert allows for parallelization across multiple GPUs, improving accuracy, training stability, and computational efficiency for predicting the dynamics of complex nonlinear systems. The optimal decomposition of subdomains is critical to the effectiveness of these scaling methods, given a fixed compute budget. Our work focuses on finding the maximum subdomain size that even a 64x2 small PINN can handle within a compute budget.

## 3 Methodology

In this section, we outline our approach to effectively implement hard constraints, strategically sampling partial differential equations (PDEs), and optimizing the scaling of computational domains. These methods are utilized to solve the acoustic wave equation within a specified computational budget.

We focus on an acoustic wave equation defined by:

\[[(,t);c()] =f(,t), , t[t_{0},T],\] (1) \[_{i}[(,t)] =U_{i}(,t), _{i}, t[t_{0},T],\] \[_{j}[(,t_{0})] =V_{j}(), ,\]

where:

* \(\) represents the differential operator. For a simplified one-dimensional acoustic wave equation, \(=_{tt}-c^{2}()^{2}\), indicating the second temporal derivative minus the spatial derivative scaled by the square of the local speed of sound, \(c()\).
* \(_{i}\) denotes the boundary condition operator applied at \(_{i}\).
* \(_{j}\) signifies the initial condition operator, defining the state of the system at \(t=t_{0}\) across the domain \(\).

### Hard constraint imposing

A prevalent ansatz employed in prior studies on hard-constraint PINNs for 1D wave equations is expressed as:

\[u(x,t)=(t)(x,t)+(1-(t))u(x,0),\] (2)

where \((x,t)\) represents the neural network output with inputs \(x\) and \(t\), and \((t)\) is a function that satisfies \((0)=0\). This design ensures that the initial condition \(u(x,0)\) is met precisely when \(t=0\).

To accommodate boundary conditions (BCs) at \(x=0\) and \(x=L\), the ansatz is often modified to:

\[u(x,t)=x(L-x)(x,t)+U_{i}(x,t),\] (3)

ensuring that \(u(x_{i},t)=U_{i}(x_{i},t)\) for \(x_{i}\).

A more comprehensive form,

\[u(x,t)= x(L-x)(t)(x,t)+(1-(t))u(x,0)\] (4) \[+ (u(0,t)-(1-(t))u(0,0))\] \[+ (u(L,t)-(1-(t))u(L,0)),\]

[MISSING_PAGE_FAIL:4]

Standing waves for Dirichlet BCsOur first numerical example is a standing wave solution for the following 1D wave equation with Dirichlet BCs:

\[u(x,t)}{ t^{2}}-c^{2}u}{  x^{2}}=0,\;x(0,L)\] (8) \[\;u(0,t)=u(L,t)=0,\] \[\;u(x,0)=U(x),\,(x,0)=V (x).\]

The analytical solution \(u(x,t)\) for Equation 8 is

\[u(x,t)=_{n=1}^{}A_{n}()( )+B_{n}()( ).\] (9)

A standing wave solution

\[u(x,t)=()(),k ^{+}\] (10)

can be achieved if we assume \(U(x)=()\) and \(V(x)=0\). We show the solutions for \(k=1,2,3\) in Figure 1(a).

Figure 1: Ground truth wavefields for (a) standing waves, (b) string waves, and (c) traveling waves with \(k=1,2,3\).

String waves for time-dependent BCsOur third example is a string wave solution for time-dependent BCs shown in Equation 11. The ground truth solutions in Figure 1(b) are achieved by finite different simulation.

\[&u(x,t)}{ t^{2}}-c^{2}u}{ x^{2}}=0,\ x(0,L)\\ &\ u(0,t)=u(L,t)=(2 t),\\ &\ u(x,0)=0,\,(x,0)=2 ()\] (11)

Traveling waves for Gaussian source time functionsOur third example is a traveling wave solution for initial conditions of Gaussian source time functions shown in Equation 12. The ground truth solutions in Figure 1(c) are computed by finite different simulation.

\[&u(x,t)}{ t^{2}}-c^{2}u}{ x^{2}}=0,\ x(0,L)\\ &\ u(0,t)=u(L,t)=0,\\ &\ u(x,0)=}(- }{2^{2}}),\ (x,0)=0\] (12)

### Optimal \((t)\) selection for hard constraints

We selected six candidate functions for \((t)\) to construct PINNs with a network configuration of only 64x2 neurons. Figures 2 through 4 illustrate the \(L^{2}\) loss and \(L^{1}\) error as functions of training epochs. Our findings suggest that \((t)\) significantly influences both the convergence rate and the emergence of failure modes. In general, \(t^{2}\), \(}{1+t^{2}}\) performs better in general, especially for higher modes \(k=2,3\). We show a few training dynamics in Appendix C.

Our analysis indicates that the frequency characteristics of \((t)\) and the corresponding wavefields may be critical for selecting an appropriate \((t)\). Matching these characteristics can potentially enhance the model's efficiency by aligning \((t)\)'s influence on the neural network's learning dynamics with the physical properties of the wave phenomena being modeled.

Figure 2: \(L^{2}\) loss and \(L^{1}\) error for standing waves with PINNs constructed using six candidate \((t)\) functions.

### Dynamic Amplitude-Focused Sampling

We demonstrate the efficacy of our proposed Dynamic Amplitude-Focused Sampling (DAFS) in enhancing both the convergence and accuracy of Physics-Informed Neural Networks (PINNs). Experiments varying \(\) from 0 to 0.5 to 1 indicate that optimal results are typically achieved when \(\) is around 0.5.

This suggests a balanced sampling strategy, where a significant portion of the samples is concentrated in regions of higher amplitude. However, exclusively focusing on these high-amplitude areas can hinder information transfer from boundary conditions to the interior of the domain, potentially leading to failure modes. Figures 5 and 6 illustrate these dynamics, showing the \(L^{2}\) loss and \(L^{1}\) error across different values of \(\), and the impact on the predicted wavefield and its accuracy.

Figure 4: \(L^{2}\) loss and \(L^{1}\) error for travelling Gaussian waves with PINNs constructed using six canditate \((t)\) functions.

Figure 3: \(L^{2}\) loss and \(L^{1}\) error for string waves with PINNs constructed using six canditate \((t)\) functions.

### Optimal subdomain

We then propose an optimal subdomain selection method shown in a flow chart in Figure 7. This method will automatically determine the optimal \(k\) our 64x2 small PINNs can handle, given a compute budget.

## 5 Limitations and Training Dynamics

While our proposed methods significantly enhance the functionality and efficiency of PINNs, the determination of the optimal function \((t)\) presents certain limitations. The choice of \((t)\) is crucial as it directly affects the model's ability to satisfy boundary and initial conditions rigidly. However, finding an ideal \((t)\) that adapts across different problems and boundary conditions without extensive trial and error remains challenging. The training dynamics are also sensitive to the form of \((t)\), where inappropriate selections can lead to slower convergence or even divergence in some cases. These issues underscore the need for a more automated, perhaps adaptive, approach to selecting \((t)\) that can dynamically adjust based on the evolving training characteristics and the specific requirements of the PDE being solved.

Figure 5: \(L^{2}\) loss and \(L^{1}\) error with varied \(\) from 0 to 1.

Figure 6: Visualizations for \(=0.00\), \(0.50\), and \(1.00\) (top to bottom): Left - Predicted wavefield, Middle - Difference between the prediction and ground truth, Right - Sampling distribution.

## 6 Conclusion

This work presented a comprehensive approach to improving the effectiveness and efficiency of Physics-Informed Neural Networks (PINNs) for solving acoustic wave equations. By integrating a well-formulated hard constraint imposition strategy and the novel Dynamic Amplitude-Focused Sampling (DAFS) method, we have significantly enhanced both the accuracy and convergence of PINNs.

Our methodological innovations include:

* A systematic derivation of hard boundary and initial conditions in PINNs that ensures these constraints are inherently satisfied, leading to better convergence and stability of the solution.
* The introduction of DAFS, which optimally allocates computational resources by focusing sampling in regions of high amplitude while ensuring adequate coverage across the computational domain to prevent information isolation.
* Development of a domain size optimization algorithm that assists in domain decomposition, enabling efficient scaling of PINNs for large-scale applications while managing computational costs.

These contributions mark a significant step forward in the practical deployment of PINNs, especially in fields requiring the simulation of complex physical phenomena over large scales. Future work will focus on extending these strategies to other types of partial differential equations and exploring the integration of our methods with other deep learning frameworks to further enhance the adaptability and efficiency of PINNs in diverse applications, for example, we will explore the integration of our methods with existing PINNs frameworks that employ domain decomposition techniques, such as XPINNs and FBPINNs, to further enhance their scalability and adaptability. We aim to make PINNs more adaptable and efficient for a broader range of applications, particularly in complex systems where traditional numerical methods struggle. By advancing these strategies, we can significantly contribute to the deployment of PINNs in real-world scenarios, tackling large-scale and multi-scale challenges effectively.

Figure 7: The flow chart of optimal subdomain determination.