# Inverted-Attention Transformers can Learn Object Representations: Insights from Slot Attention

Yi-Fu Wu

Rutgers University

&Klaus Greff

Google DeepMind

&Gamaleldin F. Elsayed

Google DeepMind

Michael C. Mozer

Google DeepMind

&Thomas Kipf

Google DeepMind

&Sjoerrd van Steenkiste

Google Research

Work done while the author was a student researcher at Google Research.Correspondence to svansteenkiste@google.com.

###### Abstract

Visual reasoning is supported by a causal understanding of the physical world, and theories of human cognition suppose that a necessary step to causal understanding is the discovery and representation of high-level entities like objects. _Slot Attention_ is a popular method aimed at object-centric learning, and its popularity has resulted in dozens of variants and extensions. To help understand the core assumptions that lead to successful object-centric learning, we take a step back and identify the minimal set of changes to a standard Transformer architecture to obtain the same performance as the specialized Slot Attention models. We systematically evaluate the performance and scaling behaviour of several "intermediate" architectures on seven image and video datasets from prior work. Our analysis reveals that by simply inverting the attention mechanism of Transformers, we obtain performance competitive with state-of-the-art Slot Attention in several domains.

## 1 Introduction

Human understanding of the natural world is rooted in the perception of entities like objects, which form the basic building blocks for causal prediction and reasoning in everyday situations . In contrast, standard neural network architectures like Transformers only partially succeed at learning representations that separately encode information about individual objects, especially in the absence of instance-level supervision . To overcome this issue, a vast literature has emerged on more specialized _object-centric neural networks_, capable of discovering and representing information about objects in a self-supervised manner . (For an overview, see Greff et al. .)

Though there are notable exceptions [e.g., 14, 15], many recent approaches follow a fairly standard recipe derived from _Slot Attention_. In Slot Attention, an image--encoded as a set of input tokens--is soft partitioned into \(K\) object _slots_. (The term _queries_ is also used in related literature .) Partitioning is a recurrent mechanism in which slots are initialized to values sampled from a distribution (with learnable parameters) and are then iteratively updated via scaled dot-product attention to the input tokens . Neural network components that apply the updates, typically GRUs , share parameters between iterations. Notably, attention maps are computed via a kind of _inverted attention_, which induces competitionbetween the slots to explain the input tokens. A prominent account for why this may lead to slot representations that capture individual objects stems from its connection to Soft \(K\)-Means or Neural EM [7; 22], relatedly a more general theory of the feasibility of learning object representations was recently proposed . In combining Slot Attention with different encoders and decoders, its capabilities for learning representations of abstract entities have been extended to video [24; 25], 3D scenes [26; 27; 28], action sequences , and morphemes in language .

Although many variants and extensions of Slot Attention have been proposed [31; 32; 33; 34], the core assumptions that lead to successful object-centric learning remain elusive. Here we take a step back and ask what aspects of Slot Attention are actually critical for object discovery. We tackle this question by drawing a connection between Slot Attention and Transformers (Figure 2) which allows us to identify a minimal set of changes to a standard Transformer decoder architecture [19; 35] that unlock the capacity for object discovery. In particular, we perform an extensive analysis of architectural variants of Slot Attention and Transformers on a range of commonly-used synthetic and real-world datasets. Our study reveals that by simply 'inverting' the attention mechanism of Transformers, we obtain competitive performance for object-representation learning (see Figure 1). We further demonstrate that it is possible to substitute this _Inverted-Attention Transformer_ for Slot Attention in SAVi  and OSRT  while obtaining comparable and sometimes improved performance.

## 2 Comparing Slot Attention to Transformers

Figure 2 presents a side-by-side comparison of Slot Attention  and Transformer Decoders . Because Slot Attention only performs cross-attention, we consider pre-LayerNorm Transformer Decoders without self-attention . Given an encoding of the input as a set of tokens and a set of initialized slots (or queries), the two algorithms proceed in a similar manner:

**Multiple Iterations.** Slots are updated over \(N\) iterations. In case of Slot Attention these iterations use shared weights, which lend the interpretation of them being state updates that converge on fixed-point attractors [31; 36], while in Transformers each iteration corresponds to a layer having potentially different weights.

**Scaled Dot-Product Attention.** Slots are updated by individually attending to the input tokens using a form of scaled dot-product attention , typically using multiple heads in case of Transformers. Here an important distinction is the normalization of the attention map, which in Slot Attention proceeds by transposing the axis over which the softmax is taken followed by a renormalization step to ensure the attention weights into each slot sum to one (see also Appendix B.1). This operation can also be understood as a kind of "Inverted Dot-Product Attention Routing" (without the extra LayerNorm)  followed by an additional renormalization step.

**Update and Transformation.** After attending, the slots are updated by a learned transformation of the resulting attention vectors. In Transformers, a residual update is applied followed by transformation by an MLP. Here Slot Attention leverages a _gated_ update implemented as a

Figure 2: Comparing the Slot Attention and Transformer algorithms. * is used to indicate weight-sharing between iterations in Slot Attention. Other notable differences include the normalization axis used in the cross-attention operation (and subsequent renormalization), and the gated update using a GRU in place of a residual update.

GRU  that includes a transformation. Slot Attention also includes an additional optional transformation with an MLP.

We note that various smaller differences exist as well, though these are not expected to significantly affect the behavior of the model. For example, Slot Attention starts by applying Layer Normalization (LN)  to the inputs, while pre-LN Transformers add LN at the very end. Further, Transformers sometimes include Dropout  in the update and transformation steps, though no consistent improvement was found in applications to images .

## 3 Experiments

Complete details of our experiments can be found in Appendix B.

### Object Discovery in Images

**Experimental Setup.** We focus on the task of unsupervised object discovery, which is commonly used to evaluate the capacity for object-centric representation learning [7; 16; 22]. We adopt the object discovery architecture from Locatello et al.  and conduct experiments on a wide range of image datasets: CLEVR [40; 41], CLEVRTex , COCO , and individual frames from MOVi-A, MOVi-C, and MOVi-E . For CLEVRTex, we use a ResNet34 encoder which was previously found to work well on this dataset . For the COCO and MOVi datasets, we adopt the DINOSAUR  architecture whereby features from a pre-trained DINO-ViT [4; 39] are used as inputs to Slot Attention and as the reconstruction target. In other cases we train models to reconstruct pixels directly. To initialize slots/queries, we sample from a Gaussian distribution with learned mean and variance as in Locatello et al. . We report 3 seeds for all our experiments and train for 300K steps. Unless otherwise stated we use \(N=3\) iterations for (variations of) Slot Attention and Transformers. We include experimental results varying \(N\) in Appendix A.2.

Benchmarking Variations of Slot Attention and Transformers.To determine what aspects in Slot Attention are essential for successful object-centric learning, we lay out a space of models that includes Slot Attention and Transformers and evaluate variations that are positioned in between. For these experiments we substitute a particular variant for Slot Attention, while keeping other components of the architecture, such as the encoder or decoder, fixed for each dataset (details are presented in Appendix B.1). To summarize, the variations include:

* **TF**: a standard pre-LayerNorm Transformer decoder without self-attention.
* **TF-Inv**: same as TF, except for using inverted attention and renormalization.
* **TF-Inv w/ GRU**: TF-Inv with a GRU in each layer to perform the slot update (line 9 in Figure 2)3.

**SA w/o GRU**: Slot Attention, but with the GRU replaced with a residual update.

**SA**: standard Slot Attention.

Figure 3: FG-ARI of Slot Attention and Transformer variants on six image datasets.

We evaluate these variants in terms of emergent instance segmentation. Figure 3 reports the foreground ARI [22; 46; 47] scores (hereafter, FG-ARI) for each of these variations, which are obtained by comparing ground-truth instance segmentations to those that were inferred after decoding the slots using a spatial broadcast decoder [22; 48] (e.g., as in Figure 1). It can be seen that _TF_ generally performs the worst, though surprisingly, other variants are able to achieve an FG-ARI comparable to that of _SA_. Crucially, we note that simply inverting the attention operation is sufficient to facilitate object discovery with Transformers as _TF-Inv_ generally performs as well as other variants closer to Slot Attention (e.g., those that share weights between iterations or incorporate the GRU). Foreground mIoU  reveals a similar trend, as shown in Figure 5.

### Application of _Inverted-Attention Transformers_ to Other Domains

Slot Attention for Video (SAVi).We consider SAVi [24; 25], which relies on Slot Attention as a key component to enable the discovery and tracking of objects in videos. We plug _TF-Inv_ into SAVi as a replacement and evaluate it on videos of the MOVi datasets. We plot the FG-ARI in Figure 4. It can be observed that SAVi with _TF-Inv_ performs comparably on MOVi-A and MOVi-E, though it is unstable on MOVi-C. Interestingly, adding back the GRU helps _TF-Inv_ to match SAVi performance, suggesting it is of some importance in this setting.

Object Scene Representation Transformer (OSRT).OSRT  is an object-centric model that uses Slot Attention for novel view synthesis of 3D scenes. Here, in addition to FG-ARI, we also report Peak Signal-to-Noise Ratio (PSNR), which captures the ability of the model to reconstruct _novel_ views. We run a set of experiments on the MultiShapeNet-Hard (MSN-Hard)  dataset where we use _TF-Inv_ instead of _SA_ and report results in Table 1. We see that while _SA_ performs better than _TF-Inv_ when \(N=1\) (the standard setting in Sajjadi et al. ), increasing to \(N=5\) harms performance for Slot Attention, both in terms of PSNR and FG-ARI. In contrast, _TF-Inv_ scales well with additional layers, yielding a higher FG-ARI and improved PSNR.

## 4 Discussion

In this work, we investigated the core assumptions underlying successful object-centric learning in attention-based architectures derived from Slot Attention. Through a comprehensive study, we discovered that inverted dot-product attention is a crucial component, which can readily be integrated in Transformers by changing the axis along which the attention normalization takes place (i.e., the 'query' axis instead of the 'key' axis) and subsequently renormalizing as in Locatello et al. . In particular, a modified pre-LayerNorm transformer (cross-attention only, i.e., _TF-Inv_), was shown to perform on par with Slot Attention on a variety of datasets, to scale well with model depth, and to be broadly applicable to other domains where Slot Attention is used such as SAVi and OSRT. In the medium term, there is an exciting opportunity to apply these insights to applications of Transformers more broadly, i.e., outside the context of unsupervised object-centric representation learning, including related architectures for object detection [17; 51; 52].

  
**OSRT Model** & **PSNR (dB)** & **FG-ARI (\%)** \\  w/ SA (\(N=1\)) & \(22.07 0.20\) & \(\) \\ w/ SA (\(N=5\)) & \(21.73 0.14\) & \(68.19 1.04\) \\ w/ TF-Inv (\(N=1\)) & \(21.20 0.57\) & \(73.58 2.06\) \\ w/ TF-Inv (\(N=5\)) & \(\) & \(75.39 0.28\) \\   

Table 1: PSNR and FG-ARI when substituting _SA_ in OSRT  with _TF-Inv_.

Figure 4: FG-ARI of Slot Attention, TF-Inv, and TF-Inv w/ GRU per Layer on the video version of the MOVi datasets.

Although Inverted-Attention Transformers make considerable progress toward simplifying Slot Attention (and extending its applicability), our investigations revealed several additional factors that influence the performance of both _TF-Inv_ and _SA_. These observations (see Appendix A.3) require more thorough study and we hope that they spur further insights from the research community.