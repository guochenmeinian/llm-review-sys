# Robust Bayesian Satisficing

Artun Saday

Bilkent University

artun.saday@bilkent.edu.tr

&Yasar Cahit Yildirim

Bilkent University

cahit.yildirim@bilkent.edu.tr

&Cem Tekin

Bilkent University

cemtekin@ee.bilkent.edu.tr

###### Abstract

Distributional shifts pose a significant challenge to achieving robustness in contemporary machine learning. To overcome this challenge, robust satisficing (RS) seeks a robust solution to an unspecified distributional shift while achieving a utility above a desired threshold. This paper focuses on the problem of RS in contextual Bayesian optimization when there is a discrepancy between the true and reference distributions of the context. We propose a novel robust Bayesian satisficing algorithm called RoBOS for noisy black-box optimization. Our algorithm guarantees sublinear lenient regret under certain assumptions on the amount of distribution shift. In addition, we define a weaker notion of regret called robust satisficing regret, in which our algorithm achieves a sublinear upper bound independent of the amount of distribution shift. To demonstrate the effectiveness of our method, we apply it to various learning problems and compare it to other approaches, such as distributionally robust optimization.

## 1 Introduction

_Bayesian optimization_ (BO) [1; 2] is a powerful technique for optimizing complex black-box functions that are expensive to evaluate. It is particularly useful in situations where the function is noisy or has multiple local optima. The approach combines a probabilistic model of the objective function with a search algorithm to efficiently identify the best input values. In recent years, BO has become a popular method for various sequential decision-making problems such as parameter tuning in machine learning , vaccine and drug development , and dynamic treatment regimes .

Contextual BO  is an extension of BO that allows for optimization in the presence of contexts, i.e., exogenous variables associated with the environment that can affect the outcome. A common approach to BO when the context distribution is known is to maximize the expected utility [7; 8]. Often, however, there exists a distributional mismatch between the reference distribution that the learner assumes and the true covariate distribution the environment decides. When there is uncertainty surrounding the reference distribution, choosing the solution that maximizes the expected utility may result in a suboptimal or even disastrous outcome. A plethora of methods have been developed to tackle distribution shifts in BO; the ones that are most closely related to our work are _adversarially robust_ BO (STABLEOPT)  and _distributionally robust_ BO (DRBO) . STABLEOPT aims to maximize the utility under an adversarial perturbation to the input. DRBO aims to maximize the utility under the worst-case context distribution in a known uncertainty set. We focus on the contextual framework of DRBO. However, unlike DRBO, our algorithm does not require as input an uncertainty set. This provides an additional level of robustness to distribution shifts, even when the true distribution lies outside of a known uncertainty set.

In this paper, we introduce the concept of _robust Bayesian satisficing_ (RBS), whose roots have been set by Herbert Simon . In his Nobel Prize in Economics speech in 1978, Simon mentioned that "decision makers can satisfice either by finding optimum solutions for a simplified world or by finding satisfactory solutions for a more realistic world". Satisficing can be described as achieving a satisfactory threshold \(\) (aka aspiration level) utility under uncertainty. It has been observed that satisficing behavior is prevalent in decision-making scenarios where the agents face risks and uncertainty and exhibit bounded rational behavior due to the immense complexity of the problem, computational limits, and time constraints . Since its introduction, satisficing in decision-making has been investigated in many different disciplines, including economics , management science [14; 15], psychology , and engineering [17; 18]. The concept of satisficing has also been recently formalized within the multi-armed bandit framework in terms of regret minimization [19; 20; 21; 22] and good arm identification . Recently, it has been shown that satisficing designs can be found with a sample complexity much smaller than what is necessary to identify optimal designs . Moreover,  demonstrated that when the future rewards are discounted, i.e., learning is time-sensitive, algorithms that seek a satisficing design yield considerably larger returns than algorithms that converge to an optimal design. The concept of _robust satisficing_ (RS) is intimately connected to satisficing. In the seminal work of Schwartz et al. , robust satisficing is described as finding a design that maximizes the robustness to uncertainty and satisfices. Long et al.  cast robust satisficing as an optimization problem and propose models to estimate a robust satisficing decision efficiently.

Inspired by this rich line of literature, we introduce the concept of _robust Bayesian satisficing_ (RBS) as an alternative paradigm for robust Bayesian optimization. The objective of RBS is to satisfice under ever-evolving conditions by achieving rewards that are comparable with an aspiration level \(\), even under an unrestricted distributional shift.

**Contributions.** Our contributions can be summarized as follows:

* We propose _robust Bayesian satisficing_, a new decision-making framework that merges _robust satisficing_ with the power of Bayesian surrogate modeling. We provide a detailed comparison between RBS and other BO methods.
* To measure the performance of the learner with respect to an aspiration level \(\), we introduce two regret definitions. The first one is the _lenient regret_ studied by  and . The second one is the _robust satisficing regret_, which measures the loss of the learner with respect to a benchmark that tracks the quality of the true robust satisficing actions. We also provide a connection between these two regret measures.
* We propose a Gaussian process (GP) based learning algorithm called _Robust Bayesian Optimistic Satisficing_ (RoBOS). RoBOS only requires as input an aspiration level \(\) that it seeks to achieve. Unlike algorithms for robust BO, it does not require as input an uncertainty set that quantifies the degree of distribution shift.
* We prove that RoBOS achieves with high probability \((_{T})\) robust satisficing regret and \((_{T}+E_{T})\) lenient regret, where \(_{T}\) is the maximum information gain over \(T\) rounds and \(E_{T}:=_{t=1}^{T}_{t}\) is the sum of distribution shifts by round \(T\), where \(_{t}\) is the amount of distribution shift in round \(t\).
* We provide a detailed numerical comparison between RoBOS and other robust BO algorithms, verifying the practical resilience of RoBOS in handling unknown distribution shifts.

**Organization.** The remainder of the paper is organized as follows. Problem formulation and regret definitions are introduced in Section 2. RoBOS is introduced in Section 3, and its regret analysis is carried out in Section 4. Experimental results are reported in Section 5. Conclusion, limitations, and future research are discussed in Section 6. Additional results and complete proofs of the theoretical results in the main paper can be found in the appendix.

## 2 Problem definition

Let \(f:\) be an _unknown_ reward function defined over a parameter space with finite action and context sets, \(\) and \(:=\{c_{1},,c_{n}\}\) respectively. Let \(_{0}\) represent the set of all distributionsover \(\). The objective is to sequentially optimize \(f\) using noisy observations. At each round \(t[T]\), in turn the environment provides a reference distribution \(P_{t}_{0}\), the learner chooses an action \(x_{t}\) and the environment provides a context \(c_{t}\) together with a noisy observation \(y_{t}=f(x_{t},c_{t})+_{t}\), where \(_{t}\) is conditionally \(\)-subgaussian given \(x_{1},c_{1},y_{1},,x_{t-1},c_{t-1},y_{t-1},x_{t},c_{t}\). We assume that \(c_{t}\) is sampled independently from a time-dependent, unknown distribution \(P_{t}^{*}_{0}\), which can be different than \(P_{t}\). We represent distributions \(P_{t}\) and \(P_{t}^{*}\) with \(n\)-dimensional non-negative vectors \(w_{t}\) and \(w_{t}^{*}\) such that \(||w_{t}||_{1}=||w_{t}^{*}||_{1}=1\). We represent the distance between \(P,P^{}_{0}\) with \((P,P^{})\). In particular, we consider _maximum mean discrepancy_ (MMD) as the distance measure.

**Optimization objective.** To motivate robust Bayesian satisficing, we review and compare various optimization objectives. Throughout this section, we assume that \(f\) is known. The key novelty of our work is combining the new _robust satisficing_ objective from  with Bayesian surrogate modeling to address unmet real-world challenges faced by BO. _Robust satisficing_ aims to perform satisfactorily well over a wide range of possible distributions on \(\). This is different from _stochastic optimization_ (SO)  which aims to optimize for a given reference distribution \(P_{t}\),1_distributionally robust optimization_ (DRO) [27; 28], which aims to optimize the worst-case scenario in an ambiguity set \(_{t}\), usually taken as a ball of radius \(r\) centered at \(P_{t}\), and _worst-case robust optimization_ (WRO) , which aims to optimize under the worst-case contexts from an uncertainty set \(_{t}\) of contexts, and _satisficing_ (S)  which seeks for a satisfactory solution that achieves threshold \(\). Table 1 compares different optimization objectives. In RS, the objective is to find \(x_{t}^{*}\) that solves in each round \(t\)

\[_{,t}= k_{c P}[f(x,c)]-k (P,P_{t}),\; P_{0}\;,x,\;k 0\;.\] (1)

To find \(x_{t}^{*}\), we can first compute the _fragility_ of \(x\) as

\[_{,t}(x)= k_{c P}[f(x,c)]-k (P,P_{t}),\; P_{0},\;k 0\;.\] (2)

When (1) is feasible, the RS solution at round \(t\) is the one with the minimum fragility, i.e., \(x_{t}^{*}_{x}_{,t}(x)\) and \(_{,t}=_{x}_{,t}(x)\). Similar to DRO, RS utilizes a reference distribution assumed to be a proxy for the true distribution. However, unlike DRO, we do not define an ambiguity set \(_{t}\) that represents all plausible distributions on \(\) but rather define a threshold value \(\), which we aim to satisfy. In cases where we are not confident that the reference distribution \(P_{t}\) accurately represents the true distribution \(P_{t}^{*}\), finding a meaningful ambiguity set can be difficult. In contrast, \(\) has a meaningful interpretation and can be expressed as a percentage of the SO solution computed under the reference distribution \(P_{t}\) over \(\) given by2

\[Z_{t}:=_{x}_{c P_{t}}[f(x,c)]\;.\] (3)

Unlike DRO, in RS, we are certain that our formulation covers the true distribution \(P_{t}^{*}\). The fragility can be viewed as the minimum rate of suboptimality one can obtain with respect to the threshold per unit of distribution shift from \(P_{t}\). The success of an action is measured by whether it achieves the desired threshold value \(\) in expectation. Depending on the objective function and the ambiguity set, the RS solution can differ from the DRO and the SO solutions (see Figure 1 for an example).

   Method & Inputs & Objective \\  SO & \(f,P_{t}\) & Find \(x\) that maximize \(_{c P_{t}}[f(x,c)]\) \\ S & \(f,P_{t},\) & Find \(x\) that satisfy \(_{c P_{t}}[f(x,c)]\) \\ WRO & \(f,P_{t}\), \(_{t}\) & Find \(x\) that maximize \(_{c_{t}}f(x,c)\) \\ DRO & \(f,_{t}\) & Find \(x\) that maximize \(_{P_{t}}_{c P}[f(x,c)]\) \\ RS & \(f,P_{t},\) & Find \(x\) that minimize \(k(x)\) where \\  & & \(k(x)= k_{c P}[f(x,c)]-k(P,P_{t}),\;  P_{0}\), \(k 0\) \\   

Table 1: Comparison of optimization objectives.

An intriguing question is whether \(\) of RS is more interpretable than \(_{t}\) of DRO. While  provides a detailed discussion of the pros and cons of these two approaches, below we compare them on an important dynamic drug dosage problem.

**Example 1**.: _Type 1 Diabetes Mellitus (T1DM) patients require bolus insulin doses (id) after meals for postprandial blood glucose (pbg) regulation. One of the most important factors that affect \(pbg\) is meal carbohydrate (cho) intake . Let \(\) and \(\) represent admissible id and cho values. For \(x,c\), let \(g(x,c)\) represent the corresponding (expected) bpg value. Function \(g\) depends on the patient's characteristics and can be regarded as unknown. The main goal of pbg regulation is to keep pbg close to a target level \(K\) in order to prevent two potentially life-threatening events called hypoglycemia and hyperglycemia. This requires \(x_{t}\) to be chosen judiciously based on current \(c_{t}\). Patients rely on a method called cho counting to calculate \(c_{t}\). Often, this method is prone to errors . The reported cho intake \(_{t}\) can differ significantly from \(c_{t}\). In order to use DRO, one needs to identify a range of plausible distributions for cho calculation errors, which is hard to calculate and interpret. On the other hand, specifying \(\) corresponds to defining an interval of safe pbg values around \(K\) that one is content with, which is in line with the standard clinical practice . We provide experiments on this application in Section 5._

**Regret measures.** We consider RS from a regret minimization perspective, where the objective is to choose a sequence of actions \(x_{1},,x_{T}\) that minimize growth rate of the regret. In particular, we focus on two different regret measures. The first one is the _lenient regret_ given as

\[R_{T}^{l}:=_{t=1}^{T}(-_{c P_{t}^{*}}[f(x_{t},c)] )^{+}\,\] (4)

where \(()^{+}:=\{0,\}\). The lenient regret measures the cumulative loss of the learner on the chosen sequence of actions w.r.t. specified threshold \(\) that we aim to achieve. If an action achieves \(\) in the expectation it accumulates no regret, otherwise, it accumulates the difference. Achieving sublinear lenient regret under any distribution shift is an impossible task. Note that even the RS action \(x_{t}^{*}\) computed with complete knowledge of \(f\) only guarantees an expected reward at least as large as \(-_{,t}(P_{t}^{*},P_{t})\). Therefore, our lenient regret upper bound will depend on distribution shifts.

Figure 1: Examples of RS, DRO, and SO solutions where \(Z_{t}\) is the solution to the SO problem given in (3). For DRO, \(_{t}\) corresponds to a ball centered at \(P_{t}\) with radius \(r\). Rhombus, cross, and hexagon correspond to RS and two other suboptimal solutions, respectively. Note that the SO solution corresponds to the point with a hexagon, i.e., a suboptimal solution. When the radius of the ambiguity ball of DRO captures the discrepancy between \(P_{t}\) and \(P_{t}^{*}\) perfectly, it selects the RS solution. When the ambiguity ball is too small or too large, it fails to find the RS solution and selects a suboptimal solution.

We also define a new notion of regret called _robust satisficing regret_, given as

\[R_{T}^{}}:=_{t=1}^{T}(-_{,t}(P_ {t}^{*},P_{t})-_{c P_{t}^{*}}[f(x_{t},c)])^{+}\;.\] (5)

\(R_{T}^{}}\) measures the accumulated loss of the learner with respect to the robust satisficing benchmark \(-_{,t}(P_{t}^{*},P_{t})\) under the true distribution. In particular, the true robust satisficing action \(x_{t}^{*}\) achieves

\[_{c P_{t}^{*}}[f(x_{t}^{*},c)]-_{,t}(P_ {t}^{*},P_{t})\;.\]

It is obvious that \(R_{T}^{}} R_{T}^{l}\). When there is no distribution shift, i.e., \(P_{t}^{*}=P_{t}\) and (1) is feasible for all \(t\), then the two regret notions are equivalent.

In order to minimize the regrets in (4) and (5), we will develop an algorithm that utilizes _Gaussian processes_ (GPs) as a surrogate model. This requires us to impose mild assumptions on \(f\), which are detailed below.

**Regularity assumptions.** We assume that \(f\) belongs to a _reproducing kernel Hilbert space_ (RKHS) with kernel function \(k\). We assume that \(k((x,c),(x^{},c^{}))=k_{ X}(x,x^{})k_{ C}(c,c^{ })\) is the product kernel formed by kernel functions \(k_{ X}\) and \(k_{ C}\) defined over the action and context spaces respectively. Let the Hilbert norm \(||f||_{ H}\) of \(f\) be bounded above by \(B\). This is a common assumption made in BO literature which allows working with GPs as a surrogate model.

**GP surrogate and confidence intervals.** Define \(:=\). Our algorithm uses a GP to model \(f\), defined by a prior mean function \((z)=[f(z)]\) and a positive definite kernel function \(k(z,z^{})=[(f(z)-(z))(f(z^{})-(z^{}))]\). Furthermore we assume \((z)=0\) and \(k(z,z) 1\), \(z\). The prior distribution over \(f\) is modeled as \(GP(0,k(z,z^{}))\). Using Gaussian likelihood with variance \(>0\), the posterior distribution of \(f\), given the observations \(_{t}=[y_{1},,y_{t}]^{}\) at points \(_{t}=[z_{1},,z_{t}]^{}\) is modeled as a GP with posterior mean and covariance at the beginning of round \(t 1\) given as

\[_{t}(z) =_{t-1}(z)^{}(_{t-1}+_{t-1})^{-1} _{t-1}\] \[k_{t}(z,z^{}) =k(z,z^{})-_{t-1}(z)^{}(_{t-1}+ _{t-1})^{-1}_{t-1}(z^{})\] \[_{t}^{2}(z) =k_{t}(z,z)\;,\]

where \(_{t}(z)=[k(z,z_{1}) k(z,z_{t})]^{}\), \(_{t}\) is the \(t t\) kernel matrix of the observations with \((_{t})_{ij}=k(z_{i},z_{j})\) and \(_{t}\) is the \(t t\) identity matrix. Let \(_{t}^{2}(z):=k_{t}(z,z)\) represent the posterior variance of the model. Define \(_{0}:=0\) and \(_{0}^{2}(z):=k(z,z)\).

The maximum information gain over \(t\) rounds is defined as 

\[_{t}:=_{:|A|=t}((_{t}+^{-1}_{A}))\;,\] (6)

where \(_{A}\) is the kernel matrix of the sampling points \(A\). The following lemma from  which is based on [35, Theorem 2] provides tight confidence intervals for functions \(f\) with bounded Hilbert norm in RKHS.

**Lemma 1**.: _[_34_, Theorem 1]_ _Let \((0,1)\), \(:=\{1,\}\), and_

\[_{t}():=_{t-1}+_{t-1 }))+2()}+B\;.\]

_Then, the following holds with probability at least \(1-\):_

\[|_{t}(x,c)-f(x,c)|_{t}()_{t}(x,c),\; x ,\; c,\; t 1\;.\]

For simplicity, we set \(=1\) in the rest of the paper, and observe that \(((_{t-1}+_{t-1})) 2_{t-1}\). When the confidence parameter \(\) is clear from the context, we use \(_{t}\) to represent \(_{t}()\) to reduce clutter. We define _upper confidence bound_ (UCB) and _lower confidence bound_ (LCB) for \((x,c)\) as follows:

\[_{t}(x,c):=_{t}(x,c)+_{t}_{t}(x,c),\;\;\;_{t }(x,c):=_{t}(x,c)-_{t}_{t}(x,c)\;.\]

For \(x\), we denote the corresponding UCB and LCB vectors in \(^{n}\) by \(_{x}^{t}:=[_{t}(x,c_{1}),,_{t}(x,c_{n})]^{ }\) and \(_{x}^{t}:=[_{t}(x,c_{1}),,_{t}(x,c_{n})]^{ }\). Also let \(f_{x}:=[f(x,c_{1}),,f(x,c_{n})]^{}\).

## 3 RoBOS for robust Bayesian satisficing

To perform _robust Bayesian satisficing_ (RBS), we propose a learning algorithm called _Robust Bayesian Optimistic Satisficing_ (RoBOS), whose pseudocode is given in Algorithm 1. At the beginning of each round \(t\), RoBOS observes the reference distribution \(w_{t}\). Then, it computes UCB index \(_{t}(x,c)\) for each action-context pair \((x,c)\), by using the GP posterior mean and standard deviation at round \(t\). UCB indices, \(\) and \(w_{t}\) are used to compute the _estimated fragility_ of each action \(x\), at round \(t\), given as

\[_{,t}(x)=_{w() \{w_{t}\}}_{t}^{t}}{\|w-w_{t}\|_{M}}&  w_{t},_{x}^{t}\\ +& w_{t},_{x}^{t}<\] (7)

where \(()\) represents the probability simplex over \(\), \(M\) represents the \(n\) by \(n\) kernel matrix and \(||w||_{M}:=}Mw}\) is the MMD measure. Specifically, given the kernel \(k_{}:_{+}\), \(M\) is the kernel matrix of the context set \(\), i.e., \(M_{ij}=k_{}(c_{i},c_{j})\). The estimated fragility \(_{,t}(x)\) of an action \(x\), is an optimistic proxy for the true fragility \(_{,t}(x)\). Note that when \(_{,t}(x) 0\), the threshold \(\) is achieved under any context distribution given the UCBs of rewards of \(x\). On the other hand, if \(>_{c P_{t}}[_{t}(x,c)]\), then \(\) cannot be achieved under the reference distribution given the UCB indices, thus \(_{,t}(x)=\). The next lemma, whose proof is given in the appendix, relates \(_{,t}(x)\) with \(_{,t}(x)\).

**Lemma 2**.: _Fix \(\). With probability at least \(1-\), \(_{,t}(x)_{,t}(x)\) for all \(x\) and \(t 1\)._

To perform robust satisficing, RoBOS chooses as \(x_{t}\) the action with the lowest estimated fragility, i.e., \(x_{t}=_{x}_{,t}(x)\). After action selection, \(c_{t}\) and \(y_{t}\) are observed from the environment, which are then used to update the GP posterior.

Inputs \(\), \(\), \(\), GP kernel \(k\), \(_{0}=0\), \(\), confidence parameter \(\), \(B\)

**for \(t=1,2,\) do**

1. Observe the reference distribution \(w_{t}\)

2. Compute \(_{t}(x,c)=_{t}(x,c)+_{t}_{t}(x,c)\) for all \(x\) and \(c\)

3. Compute \(_{,t}(x)\) as in (7)

4. Choose action \(x_{t}=_{x}_{,t}(x)\)

5. Observe \(c_{t} P_{t}^{*}\) and \(y_{t}=f(x_{t},c_{t})+_{t}\)

6. Use \(\{x_{t},c_{t},y_{t}\}\) to compute \(_{t+1}\) and \(_{t+1}\)

**end**

**Algorithm 1**RoBOS

Inputs \(\), \(\), \(\), GP kernel \(k\), \(_{0}=0\), \(\), confidence parameter \(\), \(B\)

**for \(t=1,2,\) do**

1. Observe the reference distribution \(w_{t}\)

2. Compute \(_{t}(x,c)=_{t}(x,c)+_{t}_{t}(x,c)\) for all \(x\) and \(c\)

3. Compute \(_{,t}(x)\) as in (7)

4. Choose action \(x_{t}=_{x}_{,t}(x)\)

5. Observe \(c_{t} P_{t}^{*}\) and \(y_{t}=f(x_{t},c_{t})+_{t}\)

6. Use \(\{x_{t},c_{t},y_{t}\}\) to compute \(_{t+1}\) and \(_{t+1}\)

**end**

**Algorithm 2**RoBOS

## 4 Regret analysis

We will analyze the regret under the assumption that (1) is feasible.

**Assumption 1**.: _Let \(_{t}:=_{x} w_{t},f_{x}\) be the best action under the reference distribution. We assume that \( w_{t},f_{_{t}}\) for all \(t[T]\), i.e., the threshold is at most the solution to the SO problem._

If Assumption 1 does not hold in round \(t\), then (1) is infeasible, which means that \(_{,t}=\) and there is no robust satisficing solution. Therefore, measuring the regret in such a round will be meaningless. In practice, if the learner is flexible about its aspiration level, Assumption 1 can be relaxed by dynamically selecting \(\) at each round to be less than \( w_{t},_{_{t}^{}}^{t}\), where \(_{t}^{}:=_{x} w_{t},_{x}^{t}\).3

The following theorem provides an upper bound on the robust satisficing regret of RoBOS based on the maximum information gain given in (6).

**Theorem 3**.: _Fix \((0,1)\). When RoBOS is run under Assumption 1 with confidence parameter \(/2\) and \(_{t}:=_{t}(/2)\), where \(_{t}()\) is defined in Lemma 1, then with a probability of at least \(1-\), therobust satisficing regret \(R_{T}^{}\) of RoBOS is upper-bounded as follows:_

\[R_{T}^{} 4_{T}+2())}\;.\]

_Proof Sketch:_ Assume that confidence bounds in Lemma 1 hold (happens with probability at least \(1-/2\)). Let \(r_{t}^{rs}:=-_{,t}(P_{t}^{*},P_{t})-_{c P_{t }^{*}}[f(x_{t},c)]\) be the _instantaneous regret_. Robust satisficing regret can be written as \(R_{T}^{}=_{t=1}^{T}r_{t}^{}[r_{t}^{rs} 0]\). We have

\[r_{t}^{rs} -_{,t}\|w_{t}^{*}-w_{t}\|_{M}- w_{t}^{* },_{x_{t}}^{t}+2_{t} w_{t}^{*},_{t}(x_{t}, )\] (8) \[-_{,t}\|w_{t}^{*}-w_{t}\|_{M}-(-_{,t}\|w_{t}^{*}-w_{t}\|_{M})+2_{t} w_{t}^{*},_{t}(x_{t },)\] (9) \[=\|w_{t}^{*}-w_{t}\|_{M}(_{,t}-_{,t})+2 _{t} w_{t}^{*},_{t}(x_{t},)\] \[ 2_{t} w_{t}^{*},_{t}(x_{t},)\;,\]

where (8) comes from the confidence bounds in Lemma 1, (9) comes from the fact that our algorithm at each round guarantees \( w_{t}^{*},_{x_{t}}^{t}-_{,t} \|w_{t}^{*}-w_{t}\|_{M}\), and the last inequality is due to Lemma 2. The rest of the proof follows the standard methods used in the Gaussian process literature together with an auxiliary concentration lemma. 

Our analysis uses the fact that under Assumption 1, both the fragility \(_{,t}\) and the estimated fragility \(_{,t}\) are finite. We also note that when \(> w_{t},f_{_{t}}\) and \(P_{t} P_{t}^{*}\), then \(_{,t}=\), and the instantaneous regret is \((-_{,t}(P_{t}^{*},P_{t})-_{c P_{t}^{*}}[f(x_ {t},c)])^{+}=0\). Setting \(_{T}\) as in Lemma 1, Theorem 3 gives a regret bound of \((_{T})\). The next corollary uses the bounds on \(_{T}\) from  to bound the regret for known kernel families.

**Corollary 1**.: _When the kernel \(k(z,z^{})\) is either Matern-\(\) kernel or squared exponential kernel,the robust satisficing regret of RoBOS, on an input domain of dimension \(d\), is bounded by: \((T^{})\) and \(()\) respectively._

Next, we analyze the lenient regret of RoBOS. As we discussed in Section 2, lenient regret is a stronger regret measure under which even \(x_{t}^{*}\) can suffer linear regret. Therefore, our lenient regret bound depends on the amount of distribution shift \(_{t}:=||w_{t}^{*}-w_{t}||_{M}\), \(t[T]\). We highlight that regret analysis of RoBOS is different than that of DRBO in , which is done under the assumption that \(||w_{t}^{*}-w_{t}||_{M}_{t}^{}\), for some \(_{t}^{} 0\), \(t[T]\). The major difference is that DRBO requires \((_{t}^{})_{t[T]}\) as input while RoBOS does not require \((_{t})_{t[T]}\) as input. Also note that \(_{t}_{t}^{}\).

Before stating the lenient regret bound, we let \(B^{}:=_{x} f_{x}_{M^{-1}}\). It is known that \(B^{} B(M^{-1})n}\), where \(_{}(M^{-1})\) represents the largest eigenvalue of \(M^{-1}\) and \(B\) represents an upper bound on the RKHS norm of \(f\).

**Theorem 4**.: _Fix \((0,1)\). Under Assumption 1, when RoBOS is run with confidence parameter \(/2\) with \(_{t}:=_{t}(/2)\), where \(_{t}\) is defined in Lemma 1, then with a probability of at least \(1-\), the lenient regret \(R_{T}^{l}\) of RoBOS is upper-bounded as follows:_

\[R_{T}^{l} 4_{T}+2( ))}+B^{}_{t=1}^{T}_{t}\;.\]

_Proof Sketch:_ When \( w_{t},_{x}^{t}\), let \(_{x}^{t}:=_{w()\{w_{t}\}}_{x_{t}}^{t}}{\|w-w_{t}\|_{M}}\) be the context distribution that achieves \(_{,t}(x)\). We have

\[r_{t}^{l} :=- w_{t}^{*},f_{x_{t}}\] \[=- w_{t}^{*},_{x_{t}}^{t}+ w_{t }^{*},_{x_{t}}^{t}-f_{x_{t}}\] \[- w_{t}^{*},_{x_{t}}^{t}+2_{ t} w_{t}^{*},_{t}(x_{t},)\;,\] (10)

where (10) follows from the confidence bounds in Lemma 1.

Consider \(w_{t}^{*}=w_{t}\). By Assumption 1 and the selection rule of RoBOS (i.e., \( w_{t},_{x_{t}}^{t}\)), we obtain

\[r_{t}^{l}- w_{t},_{x_{t}}^{t}+2_{t} w _{t}^{*},_{t}(x_{t},) 2_{t} w_{t}^{*},_{t}(x_{t}, )\;.\]Consider \(w_{t}^{*} w_{t}\). Continuing from (10)

\[r_{t}^{l} \|w_{t}^{*}-w_{t}\|_{M}^{*},_{ x_{t}}^{t}}{\|w_{t}^{*}-w_{t}\|_{M}}+2_{t} w_{t}^{*},_{t}(x_{ t},)\] \[\|w_{t}^{*}-w_{t}\|_{M}_{,t}(x_{t})+2_{t}  w_{t}^{*},_{t}(x_{t},)\] (11) \[\|w_{t}^{*}-w_{t}\|_{M}_{,t}(_{t})+2 _{t} w_{t}^{*},_{t}(x_{t},)\] (12) \[\|w_{t}^{*}-w_{t}\|_{M},_{} -_{_{t}}^{t},_{_{t}}^{t}} {\|_{_{t}}^{t}-w_{t}\|_{M}}+2_{t} w_{t}^{*},_ {t}(x_{t},)\] (13) \[=\|w_{t}^{*}-w_{t}\|_{M}\|f_{_{t}}\|_{M^{-1}}+2_{t}  w_{t}^{*},_{t}(x_{t},)\] (14) \[_{t}B^{}+2_{t} w_{t}^{*},_{t} (x_{t},)\,\] (15)

where (11) comes from the definition of \(_{,t}(x_{t})\); (12) follows from \(x_{t}=_{x}_{,t}(x)\); (13) results from the assumption on \(\) in Assumption 1; (14) utilizes Lemma 1 and the Cauchy-Schwarz inequality; and finally (15) follows from the definition of \(_{t}\). The remainder of the proof follows similar arguments as in the proof of Theorem 3. 

Theorem 4 shows that the lenient regret scales as \((_{T}+E_{T})\), where \(E_{T}:=_{t=1}^{T}_{t}\). An important special case in which \(E_{T}\) is sublinear is _data-driven optimization_. For this case, \(P_{t}^{*}=P^{*}\) is fixed for \(t[T]\), \(P_{1}\) is the uniform distribution over the contexts, and \(P_{t}=_{s=1}^{t-1}_{c_{s}}\), \(t>1\) is the empirical distribution of the observed contexts, where \(_{c}\) is the Dirac measure defined for \(c\) such that \(_{c}(A)=1\) if \(c A\) and \(0\) otherwise. The next result follows from steps similar to the proof of [10, Corollary 4].

**Lemma 5**.: _Consider data-driven optimization model with \((0,1)\). Under Assumption 1, when RoBOS is run with confidence parameter \(/3\) with \(_{t}:=_{t}(/3)\), where \(_{t}\) is defined in Lemma 1, then with a probability of at least \(1-\)_

\[R_{T}^{l} 4_{T}+2())}+B^{}_{1}+B^{}2(2+ T^{2}}{2})})\.\]

## 5 Experiments

We evaluate the proposed algorithm on one synthetic and one real-world environment. We compare the lenient regret and robust satisficing regret of RoBOS with the following benchmark algorithms. Our implementation is available at http://github.com/Bilkent-CYBORG/RoBOS.

_SO_: As the representative of SO we use a stochastic version of the GP-UCB algorithm that samples at each round point \(x_{t}=_{x}_{c P_{t}}[_{t}(x,c)]\).

_DRBO_: For the DRBO approach we consider the DRBO algorithm from . DRBO, at each round samples \(x_{t}=_{x}_{P_{t}}_{c P} [_{t}(x,c)]\).

_WRBO_: For the WRO approach we consider a maxi-min algorithm we call WRBO, that maximizes the worst-case reward over the context set. Note that when the ambiguity set \(_{t}\) of DRBO is \(_{0}\), i.e., the set of all possible distributions on \(\), DRBO reduces to WRBO which samples at each round the point \(x_{t}=_{x}_{c}_{t}(x,c)\).

**Synthetic environment.** The objective function is given Figure 1(Left). The reference distribution and the true distribution are chosen to be stationary with \(P_{t}=(2,1)\) and \(P_{t}^{*}=(0,25)\). We denote the true MMD distance \((P_{t},P_{t}^{*})\) with \(\), and run different instances of DRBO with ambiguity balls of radius \(r=3\), \(r=\), and \(r=/3\). When the radius of the ambiguity ball is \(\), DRBO knows the exact distributional shift. The threshold \(\) is set to \(60\%\) of the maximum value of the objective function, i.e., \(=0.6_{(x,c)}f(x,c) 0.65Z_{t}\). We use an RBF kernel with Automatic Relevance Determination (ARD) and lengthscales \(0.2\) and \(5\) respectively for dimensions \(\) and \(\). Simulations are run with observation noise \(_{t}(0,0.02^{2})\).

Results for the robust satisficing and lenient regrets are given in Figure 2. Under this configuration, the RS solution, as noted in Figure 1, is the only solution that achieves the desired threshold \(\). Hence, algorithms that converge to a different solution accumulate linear lenient regret, as can be seen in Figure 2(Right). When DRBO is run with an overconfident ambiguity set (radius \(/3\)), it converges to a suboptimal solution (hexagon); when the ambiguity set is underconfident (radius \(3\)), it convergesto another suboptimal solution (cross). Only when the distribution shift is precisely known and the ambiguity set is adjusted accordingly (radius \(\)), DRBO converges to the RS solution. Refer to Figure 1 to see the exact solutions converged.

We also investigate the sensitivity of the choice of \(\) in RoBOS compared to the choice of \(r\) in DRBO. Figure 3 compares the average rewards of RoBOS and DRBO for a wide range of \(\) and \(r\) values. While RoBOS is designed to satisfice the reward rather than to maximize, its performance remains competitive with DRBO across diverse hyperparameter settings. Notably, for \([0.1,0.3]\), RoBOS opts for the solution indicated by a cross in Figure 1, signifying a trade-off: with a smaller \(\), RoBOS prioritizes robustness guarantees over maximizing the reward.

**Insulin dosage for T1DM.** We test our algorithm on the problem of personalized insulin dose allocation for Type 1 Diabetes Mellitus (T1DM) patients. We use the open-source implementation of the U.S. FDA-approved University of Virginia (UVA)/PADOVA T1DM simulator [37; 5]. The simulator takes in as input the fasting blood glucose level of the patient, the amount of carbohydrate intake during the monitored meal and the insulin dosage given to the patient, it gives an output of the blood glucose level measured 150 minutes after the meal. We assume the insulin is administered to the patient right after the meal. Similar to , we set the target blood glucose level as \(K=112.5\) mg/dl and define the pseudo-reward function as \(r(t)=-|o(t)-K|\) where \(o(t)\) is the achieved blood glucose level at round \(t\). We further define a safe blood glucose level range as 102.5 - 122.5 mg/dl. For our setup, this corresponds to setting the threshold \(=-10\). At each round \(t\), the environment picks the true and reference distributions as \(P_{t}(_{t},2.25)\) and \(P_{t}^{*}(_{t}+N,9)\) where \(_{t}(20,80)\) and \(N\) is the random term setting the distributional shift. We define the action set \(\) to be the insulin dose and the context set \(\) to be the amount of carbohydrate intake. Here the distributional shift can be interpreted as the estimation error of the patient on their carbohydrate intake. We ran our experiments with \(N(-6,6)\) and with \(N_{t}(-6/(t+2),6/(t+2))\). Simulations are run with observation noise \(_{t}(0,1)\). For the GP surrogate, we used a Matern-\(\) kernel with length-scale parameter 10. As seen in Figures 3(a) and 3(b), when the amount of distribution

Figure 3: Average reward for different \(\) and \(r\) values in RoBOS and DRBO. \(\) is selected linearly between minimum and maximum function values. The radius \(r\) of the ambiguity ball for DRBO is selected from \(0.1\) to \(10\). Plots show average of \(10\) independent runs each with \(200\) rounds.

Figure 2: Results for synthetic environment. Plots show robust satisficing regret (left) and lenient regret (right) averaged over 50 independent runs with error bars corresponding to standard deviations divided by 2.

shift at each round is known exactly by DRBO, it can perform better than RoBOS. However, when the distributional shift is either underestimated or overestimated, RoBOS achieves better results. Plots of the average cumulative rewards of the algorithms can be found in the appendix.

## 6 Conclusion, limitations, and future research

We introduced _robust Bayesian satisficing_ as a new sequential decision-making paradigm that offers a satisfactory level of protection against incalculable distribution shifts. We introduced the first RBS algorithm RoBOS, and proved information gain based lenient and robust satisficing regret bounds.

In our experiments, we observed that when the range of the distribution shift can be correctly estimated with a tight uncertainty set \(_{t}\) centered at the reference distribution, DRBO  can perform better than RoBOS, especially when it comes to maximizing total reward. This is not unexpected since one can guarantee better performance with more information about the distribution shift. Nevertheless, in cases when the estimated shift does not adequately represent the true shift or when the main objective is to achieve a desired value instead of maximizing the reward, RoBOS emerges as a robust alternative.

Our fundamental research on RBS brings forth many interesting future research directions. One potential direction is to extend RoBOS to work in continuous action and context spaces. This will require a more nuanced regret analysis and computationally efficient procedures to calculate the estimated fragility at each round. Another interesting future work is to design alternative acquisition strategies for RBS. For instance, one can investigate a Thompson sampling based approach instead of the UCB approach we pursued in this work.

**Acknowledgements:** Y. Cahit Yildirim was supported by Turk Telekom as part of 5G and Beyond Joint Graduate Support Programme coordinated by Information and Communication Technologies Authority.

Figure 4: Results for insulin dose allocation simulation where \(_{t}\) is the true MMD distance between the true and reference distributions at round \(t\), and the threshold \(=-10\) specifies a satisficing threshold of 10 mg/dl away from the target blood glucose level. On (b), the true MMD distance \(_{t}_{0}/(t)\) decays with \(t\). Plots show robust satisficing regret (Left) and lenient regret (Right) averaged over 50 independent runs with error bars corresponding to standard deviations divided by 2.