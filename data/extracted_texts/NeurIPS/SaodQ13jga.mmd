# GITA: Graph to Visual and Textual Integration

for Vision-Language Graph Reasoning

Yanbin Wei\({}^{1,2}\), Shuai Fu\({}^{1,3}\), Weisen Jiang\({}^{1,2}\), Zejian Zhang\({}^{4}\), Zhixiong Zeng\({}^{4}\),

&Qi Wu\({}^{3}\), James T. Kwok\({}^{2}\), Yu Zhang\({}^{1}\)

\({}^{1}\)Department of Computer Science and Engineering, Southern University of Science and Technology

\({}^{2}\)Department of Computer Science and Engineering, Hong Kong University of Science and Technology

\({}^{3}\)Australia Institute for Machine Learning, University of Adelaide \({}^{4}\)Tencent

{yanbin.ust, fus.jayce, zejianzhang33, yu.zhang.ust, waysonkong}@gmail.com

barretzeng@tencent.com, qi.wu01@adelaide.edu.au, jamesk@cse.ust.hk

Equal contribution.Corresponding authors.

###### Abstract

Large Language Models (LLMs) are increasingly used for various tasks with graph structures. Though LLMs can process graph information in the textual format, they overlook the rich vision modality, which is an intuitive way for humans to comprehend structural information and conduct general graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., _visual graph_) are still unexplored. To fill the gap, we innovatively propose an end-to-end framework, called **G**raph to **v**I**sual and **T**extual **I**ntegr**A**tion (GITA), which incorporates visual graphs into general graph reasoning. Besides, we construct the **G**raph-based **V**ision-**L**anguage **Q**uestion **A**nswering (GVLQA) dataset from existing graph data, which is the first vision-language dataset for general graph reasoning. Extensive experiments on the GVLQA dataset and five real-world datasets show that GITA outperforms mainstream LLMs on general graph reasoning. Moreover, experimental results demonstrate the effectiveness of the layout augmentation on visual graphs and pretraining on the GVLQA dataset.

## 1 Introduction

Graph reasoning tasks are pivotal in domains such as recommendation systems [25; 60], social network analysis [7; 29], and knowledge graph reasoning [72; 46; 62]. Various architectures have been developed, from shallow embedding methods [6; 53] to advanced Graph Neural Networks (GNNs) [37; 64] and graph Transformers [71; 40; 8]. While these models excel in graph reasoning tasks, they often lack generalizability, flexibility, and user-friendliness. Achieving good performance with these models typically requires domain-specific tuning, which limits their abilities to generalize across different domains. Additionally, these models struggle to handle diverse tasks with the same architecture. Each task often requires a specialized design, including task-specific data processing and decoder, leading to limited flexibility. Lastly, unlike the Large Language Models (LLMs) that can engage in conversations with users, these models are less explainable and user-friendly.

In contrast, LLMs have shown great generalization capabilities across a wide variety of reasoning tasks [61; 67; 74; 32; 33] by encapsulating task-specific demands within a cohesive and interpretable mechanism - text prompts, under a unified architecture with minimal domain-specific adjustments. These advantages have sparked investigations into the potential of LLMs for graph reasoning. Recent developments lend credence to the notion that LLMs can indeed interpret and manipulate graphstructured data through textual representations. For example, InstructGLM , GPT4Graph , and LLMtoGraph  convert graphs into textual descriptions and then use these descriptions paired with queries to enable LLMs to generate accurate responses for graph reasoning tasks. Furthermore, the introduction of benchmarks such as GraphQA  and NLGraph  is a testament to the growing interest in evaluating LLMs' effectiveness on graph reasoning tasks framed in natural languages.

Despite the development of numerous methods and benchmarks for graph reasoning on LLMs, they often overlook the valuable vision modality, which is a natural means for humans to comprehend structural information and has demonstrated its success in various visual reasoning scenarios [30; 68; 69; 34; 3; 54]. Consequently, the following questions arise: (1) _Can incorporating visual information be beneficial for general graph reasoning scenarios?_ (2) _If so, how can we effectively integrate the vision modality into graph reasoning?_ To the best of our knowledge, these questions remain unexplored.

To answer them, we first propose an end-to-end framework called **G**raph to **v**Isual and **T**extual IntegrAtion (GITA)34 that systematically integrates visual information into instruction-based graph reasoning, by rendering graph structures to customized visual images which are called _visual graph_. Specifically, the GITA framework has four components: a _graph visualizer_ for generating visual graphs, a _graph describer_ for producing textual descriptions of the graph structure, a _task-based questioner_ that organizes the description and requirements of the current task into prompt instruction, and a _Vision-Language Model_ (VLM) to perform vision-language graph reasoning. In the proposed GITA framework, visual information can be incorporated into many tasks with explicit or implicit graph structures, without sacrificing its versatility, flexibility, or user-friendliness. Besides, since there is no dataset for vision-supported general graph reasoning capabilities, we construct the first vision-language dataset for general graph reasoning purposes called **G**raph-based **V**ision-**L**anguage **Q**uestion **A**nswering (GVLQA)5 based on the proposed GITA framework. The GVLQA dataset consists of 526K instances covering seven representative graph reasoning tasks, aiming to thoroughly evaluate the structure-based graph reasoning abilities of VLMs and LLMs. Extensive experiments on the GVLQA dataset and five real-world datasets demonstrate the effectiveness of the proposed GITA model. Furthermore, we delve into the effects of visual graph augmentation strategies and find that layout augmentation can dramatically boost vision-based graph reasoning performance.

Our main contributions are summarized as follows.

* We introduce an end-to-end GITA framework, innovatively integrating vision modality to boost the graph reasoning abilities of language models.
* We establish GVLQA, the first vision-language question-answering dataset for general graph reasoning purposes. It can be used to thoroughly evaluate the structure-based graph reasoning abilities of LLMs/VLMs and can also be used as pretraining data to boost the performance of downstream tasks.
* Extensive experiments on benchmark datasets across various graph reasoning tasks demonstrate the effectiveness of the proposed GITA framework and the benefits of layout augmentation on visual graphs.

## 2 Related Work

**Graph Reasoning.** Graph reasoning [5; 63] aims to answer questions based on graphs, which involves utilizing graph structures to guide the reasoning process to generate answers. Graph reasoning has a wide variety of applications in social network analysis [47; 41], bioinformatics [31; 18], chemistry , physics , knowledge graph reasoning , and recommendation systems [39; 26]. Many graph reasoning methods have been proposed. Early attempts [6; 53] learn node and edge representations through shallow modules, which may have only limited expressive power. Graph Neural Networks (GNNs) such as GCN , GAT , GraphSAGE , MPNN , and GIN  use message-passing paradigm  to model graph dependencies and update node features. Transformer-basedgraph models [71; 40; 8] further propose to use self-attention to increase the expressiveness and long-range dependency. However, as discussed in Sec 1, these models may exhibit limited generalizability, flexibility, and user-friendliness.

**LLMs on Graph Reasoning.** There have been many attempts to use LLMs in graph reasoning. Depending on how they align the input spaces of graphs and LLMs, we categorize them into two types: Graph-to-text and Graph-to-token. Graph-to-text methods transform a graph into textual descriptions, which are concatenated with the instructions and fed to the LLM for querying. For example, InstructGLM  uses natural language to describe the graph and proposes instruction prompts to fine-tune the LLM. He et.al  applies LLMs to explain graphs for training GNNs, while Chen et.al  treat LLMs as enhancers to exploit text attributes or as predictors for node classification on text-attributed graphs. GPT4Graph  and LLMtoGraph  convert graphs into specific code or natural language formats by the powerful ChatGPT [48; 49]. On the other hand, Graph-to-token methods include GraphGPT , GraphToken  and LLaGA . For these methods, the graph is represented as a specially designed token sequence, which is projected or merged into the LLM's token space for text-based reasoning. However, none of the aforementioned methods represent the graph structure information as images, highlighting the uniqueness of the proposed GITA framework and GVLQA dataset.

**Large Vision-Language Models.** Large VLMs have significantly expanded the cognitive abilities of LLMs by integrating the vision modality to address vision-language tasks. Many methods have been proposed. Some early explorations like Flamingo , CLIP , and BLIP-2  use a visual encoder for processing images and align the visual and textual embeddings. Subsequent models like LLaVA  and MiniGPT-4  combine visual and textual inputs in a single LLM for solving multimodal tasks. InstructBlip  proposes an instruction-aware query transformer and trains a vision-language model by instruction tuning. However, despite progress in a wide range of vision-language tasks [70; 16], using visual information in graph reasoning remains overlooked. We take the first step in this field, pushing the boundaries of VLMs in graph reasoning.

## 3 GITA: Graph to Visual and Textual Integration

### Preliminary

**Graph Reasoning.** In traditional graph reasoning settings, models typically rely on two main inputs: (i) the graph structure \(G=\{C,E\}\), where \(C\) and \(E\) are the set of vertices and edges, respectively; (ii) the task requirement \(T\), encompassing specific operations or questions pertaining to the graph. Based on the information provided in \(G\) and a specific task requirement \(T\), models are expected to output a reasonable answer \(A\). On the other hand, in the context of instruction-based graph reasoning methods, it is necessary to convert these inputs into textual form. This transformation facilities graph reasoning within natural language, allowing for improved interpretation and harnessing the formidable reasoning capabilities of large language models.

### Architecture

**Overview.** Different from the above graph reasoning methods, we propose a **G**raph to **I**mage-**T**xt **A**ssistant (GITA), which is the first attempt to perform graph reasoning in a vision-text-based manner. GITA comprises four pivotal components: a task-agnostic graph visualizer \(V\), a graph describer \(D\), a task-specific questioner \(Q\), and a VLM reasoner \(R_{}\), as illustrated in Figure 1. Firstly, \(V\) and \(G\) are designed to produce visual depictions (i.e., _visual graphs_) and textual descriptions of the graph structure inputs, respectively. Then, given the task requirement \(T\) and the textual description produced by \(D\), \(Q\) is designed to form a task-specific query. Finally, \(R_{}\) receives the visual input \(I_{G}\) from \(V\) based on the visual graph and the textual input \(Q_{G}^{T}\) from \(Q\), then generates answers \(A\) in natural language. In the following, we introduce the four components in detail.

**Graph Visualizer.** The role of the graph visualizer is to generate visual graphs from structural graphs. The image representation of a structural graph is not unique, as there can be variations in many aspects, such as backdrop colors, layouts, and node shapes. These variations may enhance the robustness of models through effective training but simultaneously increase the learning difficulty for models. Therefore, balancing _consistency_ and _variety_ is necessary during the graph visualization process. This trade-off is reflected in our design of graph visualizer, by maintaining consistency in basic image styles common to general images (i.e., size, resolution, backdrop) and only introducing customizable variations in four graph-related image styles unique to visual graphs (i.e., layout, node shapes, node outline styles, and edge thickness). Graph visualization in \(V\) can be formulated by the following equation:

\[I_{G}=V(G,,),\] (1)

where \(I_{G}\) denotes the visual graph derived from graph \(G\), while \(\) and \(\) are the fixed basic image styles and customizable graph-related image styles, respectively.

Visualizing the entire graph can be challenging when the number of nodes or edges is very large, affecting the clarity of the images. To address this, our graph visualizer adopts the standard strategy of \(k\)-hop subgraph sampling. Specifically, \(k\)-hop subgraph sampling for a node \(u\) in the set of vertices \(C\) involves selecting a subgraph \(G_{u}=\{C_{u} N_{k}(u),E_{u} E\}\), where \(N_{k}(u)\) includes nodes within \(k\) steps from \(u\) and each edge \((i,j)\) in \(E_{u}\) connects nodes within \(C_{u}\). To generate the visual graph of the \(k\)-hop subgraph \(G_{u}\) centered on \(u\), the nodes within \(G_{u}\) are relabeled from 0 to \(|C_{u}|-1\) to facilitate the generalization of visual graphs. Subsequently, this relabeled subgraph \(G_{u}\) is fed to the graph visualizer to generate its visual graph \(I_{G_{u}}\) by Eq. (1).

In practice, the graph visualizer can be implemented by a variety of graphic visualization tools, such as Graphviz , Matplotlib , and NetworkX . Among them, Graphviz can automatically design the layouts of visual graphs, and is especially suitable for building large-scale datasets. Matplotlib is excellent for customizable plots with fine-grained control, and NetworkX excels in complex network analysis. We have implemented various graph visualizers using modular, plug-in architecture in GITA. Specific examples of the visual graphs generated with these tools can be found in Appendix D.

**Graph Describer.** The graph describer \(D\) is tasked with generating task-agnostic textual descriptions of a given graph \(G\). To ensure clarity and fidelity of these descriptions, we meticulously craft a curated set of _graph-describing templates_. The graph description templates outlined in Appendix E are designed to cover a broad spectrum of scenarios, accommodating various graph configurations including directed or undirected graphs and those with or without node or edge weights. To generate the description for a given graph, the graph describer initially selects an appropriate template based on the graph's characteristics, such as its directionality and whether it includes node attributes or edge weights. Subsequently, this template is used by replacing placeholders with actual data, such as the number of nodes, the number of edges, and the endpoints of each edge, to craft detailed descriptions tailored to the specific graph in question. The process for \(D\) to generate textual descriptions can be formulated as follows:

\[D_{G}=D(G,P),\] (2)

where \(D_{G}\) denotes the textual description generated by graph describer, and \(P\) is the graph-describing template of the graph \(G\).

By introducing these unified and structured graph-describing templates, the graph describer is empowered to generate coherent and informative descriptions that focus on the inherent characteristics of the graph itself, independent of specific task requirements.

**Questioner.** The questioner \(Q\) is tailored to capture the intricate requirements of specific tasks and reflect them in its output task-specific query. In detail, \(Q\) receives the task-agnostic textual

Figure 1: The architecture of the GITA framework with comparison to existing LLM solution.

descriptions from the graph describer and refines them to align with the task context by elucidating the concrete meanings of nodes and edges. These refined descriptions are then enriched with task responsibilities and input/output specifications to form task-specific queries. The formulation of the questioner to generate the task-specific queries can be represented as follows:

\[Q_{G}^{T}=Q(T,D_{G}),\] (3)

where \(Q_{G}^{T}\) represents the task-specific query generated by the questioner with given the task requirement \(T\) and the textual description \(D_{G}\). The construction of task-specific queries can be approached in two main ways: manual template-based construction and bootstrapping LLM agents. Manual template-based construction enriches \(D_{G}\) with task-specific manual templates, which is preferred for tasks with precise requirements, such as the Traveling Salesman Problem (TSP) , where accuracy is critical and the task definitions are well-understood. This is because it can ensure clarity and reduce the risk of errors due to its meticulous attention to details. On the other hand, bootstrapping LLM agents for automated synthesis is more economical and suitable for dynamic or bespoke tasks, such as robotic planning or complex gaming scenarios, as it can take advantage of the speed and adaptability of LLM agents to interpret context and generate appropriate queries, minimizing manual effort and enhancing responsiveness to changing conditions. Both methods are illustrated with examples in Appendix F, showcasing their applications and benefits in different scenarios.

**VLM Reasoner.** The VLM reasoner \(R_{}\) performs final graph reasoning with visual inputs \(I_{G}\) from \(V\) and textual inputs \(Q_{G}^{T}\) from \(Q\), and outputs responses in natural language. This reasoning process can be represented as the following:

\[A=R(I_{G},Q_{G}^{T}),\] (4)

where \(A\) is the answer generated by the vision-language model \(R\). In this work, we adopt GPT-4V and LLaVA-7B/13B as VLM reasoners. These models are regarded as representatives in the realm of closed-source and open-source VLMs, respectively.

In summary, GITA systematically incorporates the vision modality into instruction-based graph reasoning. In Appendix B, we discuss the characteristics of GITA, in aspects of generalizability, flexibility and user-friendliness.

### Visual Graph Augmentation

Visual graphs generated for the same graph \(G\) can be considered as an unique data augmentation technique. Building on the four graph-related image styles introduced in the graph visualizer part of Sec 3.2, we propose the following augmentation strategies: **layout augmentation**, **node shape augmentation**, **node outline style augmentation**, and **edge thickness augmentation**. Specifically, layout augmentation involves altering the layout styles while keeping all the other settings constant. Similarly, by changing only the respective attributes, we can implement node shape augmentation, node outline style augmentation, and edge thickness augmentation. These four proposed augmentation strategies facilitate studies on the importance of each in enhancing the graph reasoning abilities of VLM reasoners.

### Training

Given a visual graph \(I_{G}\) and a text-specific query \(Q_{G}^{T}\), along with the target answer \(A_{t}\), the VLM reasoner of GITA is trained to generate answers \(A\). Specifically, \(I_{G}\) is input into the vision encoder of the VLM reasoner, resulting in a set of visual features \(F\). If there is a dimension difference between \(F_{v}\) and the pretrained word embeddings, these \(F\) will be aligned with the pretrained word embedding space of the text decoder by a vision-to-text projector. Finally, the aligned visual features \(F_{aligned}\) and \(Q_{G}^{T}\) are concatenated as input sequences of the text decoder.

Formally, given \(I_{G}\), \(Q_{G}^{T}\), and \(A_{t}\), the VLM reasoner is trained by minimizing the following negative log-likelihood:

\[_{}=-_{i=1}^{|A|}\ p_{}(A_{i} F_{aligned},Q _{G}^{T},A_{<i}),\] (5)

where \(\) is the trainable parameter and \(A_{i}\) denotes the prediction token at the \(i\)-th position. Besides, \(A_{<i}\) represents the first \(i-1\) predicted tokens. During the inference process, GITA is capable of accepting structure graphs as inputs and performing graph reasoning in an end-to-end manner.

GVLQA Dataset

In this section, we introduce the GVLQA dataset to fill the absence of a vision-language-based general graph reasoning dataset. It is designed to: 1) evaluate the graph reasoning capabilities of VLMs or LLMs; 2) help models acquire fundamental graph comprehension and reasoning abilities as a pretraining dataset.

### Construction

The GVLQA dataset is created by utilizing the graph visualizerthe graph describer, and questioner in GITA to generate vision-language-based question-answer pairs for graph reasoning on an open-source graph dataset. Specifically, we first extract both the original graph structures and the ground-truth outputs from the NLGraph-full dataset . Then the graph visualizer (detailed in Sec 3.2) and the graph describer (outlined in Sec 3.2) are used to generate visual graphs and textual descriptions for these original graph structures, respectively. Afterwards, the questioner (described in Sec 3.2) further improves and enriches the textual descriptions by converting them into textual queries. At the same time, it transforms the ground-truth output into text-based answers, following specific output requirements. By combining these visual graphs, textual queries, and text-based answers, we obtain the Graph-based Vision-Language Question Answering (GVLQA) dataset.

In the process of establishing GVLQA, we employed graphviz  to instantiate the graph visualizer. This choice is made due to its multitude of pre-defined layout algorithms, which enable convenient adjustment of visual graph layouts. Additionally, manual template-based constructed queries are utilized as the questioner because these tasks are famous with well-defined requirements.

### Structure

The GVLQA dataset comprises 526K samples, each consisting of a visual graph, a textual query, and its corresponding answer. It is divided into five subsets: GVLQA-BASE, and four augmentation subsets GVLQA-AUGLY, GVLQA-AUGNS, GVLQA-AUGNO, and GVLQA-AUGET. In GVLQA-BASE, the visual graphs are uniformly styled. The remaining four augmentation subsets are derived from GVLQA-BASE through the four visual graph augmentations (Sec 3.3), varying in six different layouts, three node shapes, four node outline styles, and four degrees of edge thickness, respectively. Detailed statistics of the four subsets are shown in Table 6 of Appendix C.

Each GVLQA subset undergoes evaluation across seven graph reasoning tasks, outlined as follows.

* **Connectivity** (denoted Connect): Determine whether two randomly selected nodes \(u\) and \(v\) in an undirected graph are connected.
* **Cycle**: Identify whether a cycle exists in an undirected graph.
* **Topological Sort** (denoted TS): Find a valid topological sort for a directed acyclic graph. Here, topological sort outputs a linear ordering of the nodes such that for every directed edge \(u v\), node \(u\) comes before \(v\) in the ordering.
* **Shortest Path** (denoted SP): Find the shortest path between two nodes in a weighted undirected graph. The shortest path between two nodes is the path connecting the two nodes with the minimum sum of edge weights along the path.
* **Maximum Flow** (denoted MaxFlow): Calculate the maximum flow from a source node to a sink node in a network graph.
* **Bipartite Graph Matching** (denoted BGM): Find a matching set in a bipartite graph with the largest number of edges. A matching set is a collection of edges in which no two edges share any common node.
* **Hamilton Path** (denoted HP): Find a valid Hamilton path in an undirected graph. A Hamiltonian path is a path that traverses each node in a graph exactly once.

Figure 6 offers illustrations for these tasks in the GVLQA-BASE dataset. Illustrations of all the GVLQA subsets are provided in Appendix H.

Experiments

In this section, we extensively evaluate the performance of LLM baselines and the proposed GITA on the GVLQA-BASE and five real-world datasets. To better clarify the reasoning capabilities of solely visual graphs, we also test GITA without the textual descriptions of graphs, which can be considered as a variant of GITA and denoted as vision-only (VO). In this case, the visual graph is the only information source for graph reasoning. Additionally, we investigate the importance of visual graph augmentation (Sec 3.3) strategies, by comparison GITA-7B trained on GVLQA-BASE and on the other augmentation subsets of GVLQA (Sec 4.2). Lastly, we investigate the effectiveness of using GVLQA as the pretrained dataset on real-world datasets. The evaluation metrics for all experiments are accuracy by exact matching. For the fine-tuning setting, we fine-tune the LoRA adapters  for all weight matrices in the text decoder of the VLM reasoner, while keeping the vision encoder in the VLM reasoner frozen. More detailed experimental settings are in Appendix G.

### Evaluation on the GVLQA-BASE Dataset

In this subsection, we perform experiments on the GVLQA-BASE dataset to compare GITA with popular LLMs including GPT-4 Turbo , LLAMA2-7B/13B , and Vicuna-7B/13B , under both zero-shot and fine-tuning settings. The experimental results are shown in Table 1. Based on these results, we can obtain the following observations.

**Observation 1: GITA Outperforms LLM Baselines.** As can be seen in Table 1, GITA consistently outperforms the LLM baselines under the same setting. This underscores its SOTA effectiveness in instruction-based graph reasoning tasks, showing robust capabilities across different parameter scales under both fine-tuning and zero-shot settings. Moreover, under the fine-tuning setting, incorporating the vision modality consistently benefits 7B models. But for the 13B models, the performance of some tasks may degrade. This could be attributed to the greater challenge of aligning representations of the visual and textual modalities in the larger 13B models compared to the 7B models, in the case of only fine-tuning LoRA adapters in the text decoder. We speculate that full training could potentially address this issue. However, we leave this as future work due to resource constraints.

**Observation 2: Mainstream Open-source VLM/LLMs Lack Fundamental Graph Reasoning Abilities.** The zero-shot results illustrate that prominent open-source LLMs or VLMs, including LLaMA2, Vicuna, and LLaVA, exhibit minimal graph reasoning capabilities on the GVLQA-BASE dataset. Specifically, these models produce random answers, i.e., randomly responding with either "Yes." or "No." for tasks involving Connect and Cycle, resulting in a performance close to 50%. Cur

   Models & Connect & Cycle & TS & SP & MaxFlow & BGM & HP & **Avg** \\   \\  LLaMA2-7B & **50.06** & 49.43 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 14.21 \\ Vicuna-7B & **50.06** & 49.43 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 14.21 \\ GITA-7B (VO) & **50.06** & **50.33** & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & **14.34** \\ GITA-7B & **50.06** & 49.43 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 14.21 \\  GPT-4 Turbo & 76.70 & 49.51 & 19.59 & 35.35 & **6.89** & 42.11 & 47.04 & 39.60 \\ GITA-ZS (VO) & 57.76 & **63.34** & 5.34 & 4.88 & 1.59 & 46.60 & 10.74 & 27.18 \\ GITA-ZS & **82.58** & 51.46 & **19.71** & **37.69** & 6.00 & **52.21** & **50.00** & **42.81** \\   \\  LLaMA2-7B & 97.33 & 94.63 & 33.26 & 26.01 & 9.56 & 90.86 & 23.95 & 53.66 \\ Vicuna-7B & 97.58 & 95.04 & 34.46 & 25.98 & 9.33 & 91.04 & 25.55 & 54.15 \\ GITA-7B (VO) & 59.97 & 96.34 & 13.30 & 5.72 & 2.89 & 93.01 & 1.11 & 38.91 \\ GITA-7B & **98.95** & **96.67** & **41.12** & **32.15** & **20.00** & **93.19** & **29.26** & **58.76** \\  LLaMA2-13B & 98.79 & 93.36 & 33.83 & 27.93 & 12.22 & 91.34 & 33.46 & 55.85 \\ Vicuna-13B & **99.35** & 94.39 & 36.73 & 28.53 & 11.34 & 92.65 & **34.81** & 56.83 \\ GITA-13B (VO) & 58.00 & **96.91** & 14.45 & 5.72 & 4.89 & **93.19** & 1.85 & 39.29 \\ GITA-13B & 99.14 & 95.60 & **38.69** & **40.47** & **20.66** & 92.12 & 33.33 & **60.00** \\   

Table 1: Accuracy (%) comparisons on GVLQA-BASE under zero-shot and fine-tuning settings, where “VO” denotes a variant of GITA using only the vision modality.

rent SOTA closed-source LLMs or VLMs, including GPT-4 Turbo and GPT-4V, demonstrate superior zero-shot performance compared with the aforementioned open-source models. This observation implies that current open-source LLMs and VLMs lack basic graph reasoning ability, which may be attributed to the insufficient availability of relevant training data. Such observation also enhances our motivation to propose the GVLQA dataset, with the aim of improving the graph reasoning capabilities of VLMs/LLMs.

**Observation 3: Increasing Model Size Leads to Better Graph Reasoning Capabilities.** The comparison of VLMs/LLMs with different parameter sizes, specifically 7B and 13B models, verify the benefits of increasing the model size for graph reasoning capabilities. In this regard, GITA-13B outperforms its counterpart with 7B parameters (GITA-7B) both on average and across four of the seven tasks. However, it is worth noting that GITA-13B does not outperform GITA-7B on the other three tasks. We hypothesize that this discrepancy may be attributed to insufficient modality alignment due to LoRA fine-tuning.

**Observation 4: Vision and Text Modalities Proficient in Different Types of Graph Reasoning Tasks.** We explore the individual capabilities of the visual and textual modalities within the GITA framework. The results indicate that the text and vision modalities can complement each other and contribute to better performance than individual ones, as removing either modality leads to performance drops in most cases (Vicuna & GITA (VO) and GPT-4 Turbo & GITA (VO) in Table 1). While the graph reasoning capability provided by the vision modality may not be as strong as that of the text modality in most cases, relying solely on vision still enables the model to possess basic graph reasoning abilities. Specifically, the model outperforms text-based LLMs in 2 of the 7 tasks (Cycle and BGM) when relying solely on vision. This consistent improvement across all comparison groups demonstrates the potential of the vision modality to excel in certain graph reasoning tasks, leveraging its ability to capture visual patterns like cycles and graph properties such as bipartition. In contrast, text exhibits a higher proficiency than vision modality in sequence-related graph reasoning problems, particularly on tasks such as TS, SP, and HP, which require constructing ordered node sequences.

### Evaluation for the Visual Graph Augmentations

To assess the impact of the proposed visual graph augmentation strategies (including layout, node shape, node outline style, and edge thickness augmentations), we compare the performance of vision-only GITA-7B models trained on the four augmented subsets of GVLQA and on GVLQA-BASE

    & Connect & Cycle & TS & SP & MaxFlow & BGM & HP & **Avg** \\  GVLQA-BASE & 59.97 & 96.34 & 13.30 & 5.72 & 2.89 & 93.01 & 1.11 & 38.91 \\ GVLQA-AUGNS & 59.85 & 96.75 & 14.17 & 6.61 & 3.78 & 91.58 & 1.48 & 39.17 \\ GVLQA-AUGNO & 54.87 & 96.50 & 14.29 & 5.54 & **3.94** & 92.83 & 1.11 & 38.44 \\ GVLQA-AUGET & 57.98 & 96.91 & 13.37 & 5.97 & 3.11 & 91.76 & 0.74 & 38.55 \\ GVLQA-AUGLY & **87.18**\(\) & **97.07** & **14.86** & **76.55**\(\) & **3.94** & **93.19** & **70.74**\(\) & **63.36**\(\) \\   

Table 2: Accuracy (%) comparisons across GVLQA subsets using GITA-7B (VO). \(\) denotes dramatic performance improvement.

   Models & ca-GrQc & ca-HepTh & PolBlogs & Cora & CiteSeer & **Avg** \\   \\  LLaMA2-7B & 40.59 & 48.89 & 10.74 & 24.35 & 30.33 & 30.98 \\ Vicuna-7B & 41.35 & 50.00 & 8.72 & 26.94 & 29.13 & 31.22 \\ GITA-7B & 71.95 & 86.06 & 46.98 & 31.37 & 30.63 & 53.40 \\ GITA-7B\({}^{}\) & **72.02** & **86.08** & **48.32** & **32.10** & **31.83** & **54.07** \\   \\  LLaMA2-7B & 76.57 & 89.06 & 80.54 & 83.76 & 73.27 & 80.64 \\ Vicuna-7B & 78.95 & 89.85 & 80.54 & 84.87 & 74.17 & 81.68 \\ GITA-7B & 79.70 & 91.13 & 84.56 & 85.24 & 75.07 & 83.14 \\ GITA-7B (w/ AUGLY) & 79.77 & 91.21 & **85.23** & 85.24 & 75.68 & 83.43 \\ GITA-7B\({}^{}\) & **80.46** & **91.68** & **85.23** & **86.35** & **76.57** & **84.06** \\   

Table 3: Accuracy (%) comparisons on real-world datasets under zero-shot and fine-tuning settings, where \(\) indicates the usage of a checkpoint pretrained in the Cycle task of GVLQA-BASE.

(without augmentation). The results are presented in Table 2. To fully utilize the visual information in visual graphs, we fine-tune the visual encoder of VLMs in addition to the vision-to-text projector and the LoRA adapters within the text decoder in this experiment.

As can be seen from the results, a significant enhancement in overall performance is observed with the introduction of layout augmentation (GVLQA-AUGLY). The average performance improves remarkably from 38.91% to 63.36%. Notably, significant improvements are observed on SP (5.72% to 76.55%), HP (1.11% to 70.74%), and Connect (59.97% to 87.18%). These findings highlight the critical role of layout augmentation in generating visual graphs. In other words, this observation suggests the potential for creating larger-scale datasets for vision-language-based graph reasoning, which could significantly contribute to advancing this field. Conversely, the other three augmentations do not yield such substantial performance improvements, further emphasizing the importance of layout augmentation in vision-language-based reasoning.

### Evaluation on Real-World Datasets

In this section, we study the effectiveness of GITA on the ca-GrQC  and ca-HepTh  datasets for the link prediction task, and on the PolBlog , Cora  and CiteSeer  datasets for the node classification task. Table 8 in the appendix C presents the statistics of these datasets. The graph can have thousands of nodes/edges, making it infeasible to feed the entire graph into the model. Consequently, we employ \(k\)-hop subgraph sampling (with \(k=2\)) discussed in Sec 3.2 to satisfy the token length restriction of LLMs and visual graph scope effectively.

The experimental results are presented in Table 3. It is evident that GITA consistently outperforms the LLM baselines, and its performance progressively improves with the addition of layout augmentation and the use of the GVLQA checkpoint. Notably, we emphasize the advantages of using GVLQA-BASE as a pretrained dataset by comparing it with GITA-7b. Performance improvements of 0.67% and 0.92% are observed in the zero-shot and fine-tuning settings, respectively. This highlights the potential application value of the proposed GVLQA dataset.

### Comparison of GITA with Dedicated Graph Baselines

Though GITA is designed for language-based general graph reasoning settings, which are much more user-friendly (by user-readable natural language) and general (unique model architecture for various scenarios) than the typical application of dedicated GNNs, it remains essential to conduct a comprehensive comparison with specialized GNNs to elucidate the strengths and limitations of GITA's applicability and capabilities. To this end, we assess the graph reasoning abilities of GITA against dedicated GNNs, including GCN  and SAGE , using the GVLQA-Base dataset, as detailed in Table 4. In addition, we explore and compare the effects of \(k\)-hop subgraph sampling on the proposed GITA and GNN baselines. Using the ca-Hepth dataset, we analyze the impact of increasing the number of hops \(k\) on the reasoning time and performance of both GITA and GNNs. The results are in Table 5.

**Overall Graph Reasoning Ability Comparison.** As shown in Table 4, compared to the dedicated GNNs, the fine-tuned GITA-7B models have comparable average graph reasoning performance, with the larger GATA-13B model performs slightly better. In particular, compared to GNNs, the GITA model shows a stronger ability in recognizing local structures in the graphs (Connect and Cycle) and to accomplish tasks with obvious layout heuristics (BGM). We believe that this advantage comes from GITA's visual perception. For SP and MaxFlow, GITA's performance is inferior to GNNs. This may be because GNNs process edge weight more effectively through the message-passing mechanism.

**Scalability and Performance Variation with Different Numbers of Hops \(k\).** The inference time results are shown in Table 5. As can be seen, GITA demonstrates inferior scalability compared to the GNN baselines. Its scalability remains stable as the sampled graph size (i.e., \(k\)) increases.

    & Connect & Cycle & TS & SP & MaxFlow & BGM & HP & **Avg** \\  GCN & 79.65 & 70.89 & 45.71 & 44.56 & **56.44** & 76.70 & 32.22 & 58.02 \\ SAGE & 82.72 & 73.58 & 44.51 & **49.25** & 50.67 & 81.00 & **36.67** & 59.78 \\ GITA-7B & 98.95 & **96.67** & 41.12 & 32.15 & 20.00 & **93.19** & 29.26 & 58.76 \\ GITA-13B & **99.14** & 95.60 & 38.69 & 40.47 & 20.66 & 92.12 & 33.33 & **60.00** \\   

Table 4: Accuracy (%) comparisons among dedicated GNNs and GITAs on GVLQA-Base.

From the accuracy results in Table 5, GITA, GCN, and SAGE achieve their peak performance at \(k=2\), suggesting that a small sampled graph size suffices for optimal performance. Though the dedicated GNNs attain higher peak performance than GITA, they exhibit performance declines as \(k\) increases (e.g., 3 or 4), while GITA's performance is more stable w.r.t. \(k\).

### Case Study

In this section, we present examples of graph information provided in both visual and textual formats, which offer some intuitive interpretations for our experimental results. Figure 2 (a) shows an example where the GITA-7B (VO) model outperforms its LLMs-based counterpart, and Figure 2 (b) shows an opposite scenario.

The task depicted in Figure 2(a) is cycle detection and the correct answer is 'No'. This is predicted successfully by the vision-only GITA-7B model, while the text-based Vicuna-7B fails. In this example, recognizing cycle patterns is much easier in visual graphs, whereas text-based LLMs struggle with disordered textual descriptions of edges, which could inherently involve greater complexity and more challenges.

On the other hand, the fixed layout of visual graphs presented in GVLQA-BASE may impede the visual encoder in identifying the shortest path between two nodes, although we have verified layout augmentation can greatly improve the graph reasoning abilities of models, as shown in Sec. 5.2. This limitation might arise from the confusion caused by the visual distance within an image, without considering the weights between the nodes. For instance, in Figure 2(b), the correct answer is '4->6->0', which visually appears as a more convoluted path but numerically has a shorter path length of \(3=1+2\). In contrast, the incorrect answer given by GITA-7B (vision-only) is '4->2->0', which has a higher path length cost of \(4=1+3\) but visually seems like a more direct shortcut. This observation further validates the effectiveness of employing layout augmentation to enhance performance in this task. Layout variations of visual graphs play a crucial role in mitigating the visual confusion caused by the spatial arrangement within a visual graph. However, it seems more effective for text-based LLMs to handle explicitly separated nodes and weights, as illustrated by the text (in red) in Figure 2(b).

## 6 Conclusion

In this paper, we propose an end-to-end framework called GITA for vision-language-based graph reasoning. Extensive experiments validate the superiority of incorporating visual information into instruction-based graph reasoning. Furthermore, we conduct comparative analysis of the four proposed visual graph augmentations and identify layout augmentation as the most effective approach for enhancing visual graphs. This finding offers valuable insights for the development of larger-scale datasets aimed at facilitating vision-language-based graph reasoning. Lastly, we highlight the potential application value of the proposed GVLQA dataset as a pretrained dataset.