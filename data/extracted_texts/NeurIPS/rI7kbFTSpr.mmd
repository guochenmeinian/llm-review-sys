# Towards Reliable Model Selection for Unsupervised Domain Adaptation: An Empirical Study and A Certified Baseline

Towards Reliable Model Selection for Unsupervised Domain Adaptation: An Empirical Study and A Certified Baseline

Dapeng Hu\({}^{1}\)1  Mi Luo\({}^{3}\) Jian Liang\({}^{4,5}\)2  Chuan-Sheng Foo\({}^{2,1}\)

\({}^{1}\)Centre for Frontier AI Research, A*STAR, Singapore

\({}^{2}\)Institute for Infocomm Research, A*STAR, Singapore

\({}^{3}\)National University of Singapore

\({}^{4}\)NLPR \(\&\) MAIS, Institute of Automation, Chinese Academy of Sciences

\({}^{5}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

###### Abstract

Selecting appropriate hyperparameters is crucial for unlocking the full potential of advanced unsupervised domain adaptation (UDA) methods in unlabeled target domains. Although this challenge remains under-explored, it has recently garnered increasing attention with the proposals of various model selection methods. Reliable model selection should maintain performance across diverse UDA methods and scenarios, especially avoiding highly risky worst-case selections--selecting the model or hyperparameter with the worst performance in the pool. _Are existing model selection methods reliable and versatile enough for different UDA tasks?_ In this paper, we provide a comprehensive empirical study involving 8 existing model selection approaches to answer this question. Our evaluation spans 12 UDA methods across 5 diverse UDA benchmarks and 5 popular UDA scenarios. Surprisingly, we find that none of these approaches can effectively avoid the worst-case selection. In contrast, a simple but overlooked ensemble-based selection approach, which we call EnsV, is both theoretically and empirically certified to avoid the worst-case selection, ensuring high reliability. Additionally, EnsV is versatile for various practical but challenging UDA scenarios, including validation of open-partial-set UDA and source-free UDA. Finally, we call for more attention to the reliability of model selection in UDA: avoiding the worst-case is as significant as achieving peak selection performance and should not be overlooked when developing new model selection methods. Code is available at https://github.com/LHXXHB/EnsV.

## 1 Introduction

Deep learning has achieved incredible advancements in various tasks through supervised learning with large labeled datasets . However, obtaining labels can be expensive, and deep models often struggle to generalize to unlabeled data from unseen distributions . Domain adaptation  tackles this challenge by transferring knowledge from a labeled source domain to a target domain with limited labels but a similar task. Unsupervised domain adaptation  (UDA), particularly, has garnered significant attention due to its practical assumption that the target domain is entirely unlabeled, witnessing the development of many effective methods  and practical settings .

However, successful applications of UDA methods across diverse tasks rely heavily on selecting appropriate hyperparameters. Sub-optimal hyperparameters can cause state-of-the-art UDA methods to underperform compared to the source-trained model without target-domain adaptation [19; 18]. This phenomenon emphasizes the significance of model selection, also called hyperparameter selection or validation, in UDA. Taking the typical one-hyperparameter validation task of a given UDA method as an example, we need to determine the optimal value of a hyperparameter \(\) among a set of \(m\) different candidate values \(\{_{i}\}_{i=1}^{m}\). By applying these different \(_{i}\) with the same UDA method, we can obtain a set of \(m\) different models with the parameter weights \(\{_{i}\}_{i=1}^{m}\). The goal is to identify the candidate model that exhibits the best performance on the unlabeled target domain and subsequently adopt the associated hyperparameter value for \(\). This model selection problem remains challenging and under-explored in UDA due to cross-domain distribution shifts and the absence of labeled target data.

Existing approaches can be categorized into two types. The first type involves leveraging labeled source data for target-domain model selection [9; 14; 15; 16]. The second type designs unsupervised metrics based on priors of the learned target-domain structure and utilizes the metrics for model selection [17; 19; 18; 20]. It is natural to ask: Are these approaches reliable in model selection tasks, i.e., can they maintain good performance for various practical UDA tasks?

To answer this question, we conduct an extensive empirical study to assess the performance of all selection methods across various practical UDA settings, including closed-set UDA , partial-set UDA , open-partial-set UDA , and source-free UDA [12; 22]. Notably, the model selection problem of open-partial-set UDA has not been investigated before. Surprisingly, we find that despite their specific designs, all these methods encounter challenges in avoiding the selection of poor or even the worst models across various UDA methods and settings. This renders the adaptation ineffective or even harmful, thereby constraining their adoption by researchers and practitioners in the community . For instance, Table 1 compares the worst-case selection statistics of all these model selection methods across various practical UDA settings. These settings include standard closed-set UDA and partial-set UDA, which have been extensively studied in prior works [15; 19], and source-free UDA, where the model selection problem has not been widely investigated. The comparison reveals that all the methods occasionally or even frequently suffer from worst-case model selection situations, indicating high unreliability.

In contrast, we note that a simple ensemble-based validation baseline, dubbed EnsV, can effectively avoid the worst-case selection. Through a straightforward theoretical analysis of the ensemble, we observe that it is guaranteed to surpass the worst candidate model's performance. Our introduced EnsV takes a further simple step, utilizing the ensemble as a role model for directly assessing candidate models during the model selection process. This strategy ensures the secure avoidance of selecting the worst candidate model, thereby enhancing the reliability of model selection. Moreover, EnsV only uses target-domain predictions inferred by all candidate models. This eliminates the need for specific domain shift assumptions and access to source data, while also requiring no additional effort, such as time and memory, as all models are provided within the given problem context. This simplicity and versatility make EnsV suitable for various practical UDA scenarios, including the unexplored challenges of validation for UDA with unknown open classes . Despite EnsV not being certified for peak-performance selection, we hope that, as the first to focus on the practical

   Method & Closed-set UDA & Partial-set UDA & Source-free UDA \\  SourceRisk  & 16 / 110 & 2 / 24 & n.a. \\ IWCV  & 15 / 110 & 3 / 24 & n.a. \\ DEV  & 9 / 110 & 1 / 24 & n.a. \\ RV  & 2 / 110 & 1 / 24 & n.a. \\  Entropy  & 15 / 131 & 7 / 24 & 16 / 17 \\ InfoMax  & 9 / 131 & 12 / 24 & 16 / 17 \\ SND  & 33 / 131 & 3 / 24 & 11 / 17 \\ Corr-C  & 80 / 131 & 4 / 24 & 3 / 17 \\ EnsV (Ours) & **0 / 131** & **0 / 24** & **0 / 17** \\   

Table 1: Statistics for worst-case selections by various model selection methods are provided across 110 closed-set UDA tasks (potentially an additional 21 tasks on DomainNet ), 24 partial-set UDA tasks, and 17 source-free UDA tasks (only for applicable methods). These statistics represent the count of worst-case selections divided by the total count of tasks, with **bold** font indicating the best worst-case avoidance. ‘n.a.’ indicates that certain methods are not applicable without source data.

aspect of worst-case avoidance in model selection, our empirical study and simple baseline can inspire future efforts in developing more reliable model selection methods.

## 2 Related Work

**Unsupervised domain adaptation** (UDA) is initially studied in a closed-set setting (CDA) where only covariate shift  is considered as the domain shift, and the two domains share the same label set. Recent research has explored many real-world UDA scenarios by incorporating label shift, where the two domains have distinct label sets. This includes partial-set UDA (PDA) , where several source classes are missing in the target domain, open-set UDA (ODA) , where the target domain contains samples from unknown classes, and open-partial-set UDA (OPDA) , where there are only some overlaps in the label sets across domains. More recently, source-free UDA settings (SFUDA) [24; 12] have been explored, where only the source model instead of source data is available for target adaptation, potentially addressing privacy concerns in the source domain. Subsequently, in the context of black-box domain adaptation , the privacy of the source domain is fully safeguarded. Specifically, the research community has made significant efforts to develop effective UDA methods in image classification [9; 6] and semantic segmentation [25; 26], which can be seen through two distinct research directions. The first direction focuses on aligning the distributions across domains by minimizing specific discrepancy measures [27; 28; 21; 29; 30] or using adversarial learning to maximize domain confusion . Especially, adversarial learning has become a popular approach and has been explored at different levels for domain alignment, including image-level , manifold-level [9; 32; 6], and prediction-level [5; 25; 26; 33]. The second direction focuses on target-oriented learning, aiming to learn a good structure for the target domain. This includes self-training approaches [34; 12; 35] and target-specific regularizations [7; 8; 36]. To thoroughly assess the efficacy of model selection baselines, we opt for a diverse set of UDA methods across various UDA scenarios in our model selection experiments and then utilize these baselines to choose the appropriate hyperparameters for different UDA methods.

**Model selection**  for out-of-distribution (OOD) testing data is crucial for practical model deployment, but it remains challenging. Although the problem has attracted increasing attention in both domain generalization (DG) [37; 38] and UDA [18; 19], it remains relatively under-explored. In DG, since target data is not available for model selection, existing methods usually estimate the general OOD performance with multiple source domains. Differently, in UDA, thanks to the transductive setting, target data can be used for model selection in various ways. Efforts to address UDA model selection can be broadly categorized into two lines. Early approaches focused on estimating the target domain risk through labeled source data. SourceRisk  utilized a hold-out labeled source validation set to guide model selection based on source risk. To mitigate the impact of domain shift on source estimation,  introduced Importance-Weighted Cross-Validation (IWCV), which re-weights source risk using a source-target density ratio estimated in the input space. Building upon this,  improved IWCV by introducing Deep Embedded Validation (DEV), which estimates the density ratio in the feature space and offers lower variance.  proposed a novel Reverse Validation approach (RV) that leveraged reversed source risk for selection. However, source-based validation methods often necessitate additional model training to handle domain shifts, rendering them cumbersome and less reliable. In contrast, recent model selection methods have shifted their focus exclusively to

   Method & covariate & label & w/o & w/o extra & w/o & worst-case \\  & shift & shift & source data & hyperparameter & extra training & avoidance \\  SourceRisk  & ✗ & ✗ & ✗ & ✗ & ✓ & ✗ \\ IWCV  & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ DEV  & ✓ & ✗ & ✗ & ✗ & ✗ \\ RV  & ✓ & ✗ & ✗ & ✗ & ✗ \\  Entropy  & ✓ & ✗ & ✓ & ✓ & ✗ \\ InfoMax  & ✓ & ✗ & ✓ & ✓ & ✓ & ✗ \\ SND  & ✓ & ✓ & ✓ & ✗ & ✓ & ✗ \\ Corr-C  & ✓ & ✗ & ✓ & ✓ & ✓ & ✗ \\ EnsV (Ours) & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 2: Comparisons of unsupervised model selection approaches used for UDA.

unlabeled target data, employing specifically designed metrics for model selection. For instance,  introduced the mean Shannon's Entropy of target predictions as a model selection metric, promoting confident predictions.  proposed the use of Input-Output Mutual Information Maximization (InfoMax) as a metric, augmented with class-balance regularization over Entropy.  introduced Soft Neighborhood Density (SND), a novel metric focusing on neighborhood consistency.  presented Corr-C, a class correlation-based metric that evaluates both class diversity and prediction certainty simultaneously. Our EnsV baseline aligns with the latter line of research. Importantly, it operates without making any assumptions about cross-domain distribution shifts or the learned target-domain structure, making it suitable for a variety of UDA scenarios. A comprehensive comparison, as presented in Table 2, underscores that EnsV stands out as a simple and versatile approach.

**Ensemble** methods, which harness the collective power of a pool of models through prediction averaging, have been extensively studied in the machine learning community for enhancing model performance [40; 41; 42; 43] and improving model calibration [44; 45]. In the era of deep learning, the efficiency of ensembling has garnered significant attention due to the high training cost of deep models. Efficient solutions have been proposed, such as using partially shared parameters [46; 47; 48] and leveraging intermediate snapshots [49; 50; 51]. Recently, weight averaging has gained attention as an efficient alternative to prediction averaging during inference [52; 53; 54; 55; 56]. In addition, diversity is considered crucial for effective ensembles. Various approaches have been explored to achieve diverse checkpoints, including bootstrapping , random initializations , tuning hyperparameters [59; 60; 53], and combining multiple strategies . Different from mainstream ensemble applications, our work innovatively and elegantly applies ensemble to help address the open problem of unsupervised model selection in various domain adaptation scenarios. In addition,  leverages ensembles for hyperparameter selection in CDA but directly uses prediction-based ensembling as the output, unlike our EnsV, which includes a selection step.

## 3 Methodology

We consider a \(C\)-way image classification task to introduce the concept of unsupervised domain adaptation (UDA). In UDA, we typically have a labeled source domain \(_{}=\{(x_{}^{s},y_{}^{s})\}_{i=1}^{n _{}}\) comprising \(n_{}\) annotated source images \(x_{}\) and their corresponding labels \(y_{}\). Additionally, there is an unlabeled target domain, \(_{}=\{x_{}^{i}\}_{i=1}^{n_{}}\), containing only \(n_{}\) unlabeled target images \(x_{}\). Despite the tasks being similar, there exist data distribution shifts between the two domains. The primary objective of UDA is to accurately predict the unavailable target labels, \(\{y_{}^{i}\}_{i=1}^{n_{}}\), by leveraging a discriminative mapping \(f(x,)\), which is learned using data from two domains. Here, \(^{d}\) represents the parameter weights of the trained UDA model. When presented with an input image \(x\), the model generates a probability prediction vector, \(p=f(x,)\), where \(p^{C}\) and \(_{i=1}^{C}p^{i}=1\).

Model selection in UDA is essentially equivalent to the hyperparameter selection challenge. Here, we aim to determine the optimal value for the hyperparameter \(\) from a set of \(m\) candidate values \(\{_{i}\}_{i=1}^{m}\). The hyperparameter \(\) can encompass various aspects, including the learning rate, loss

Figure 1: **Left**: Depiction of the unsupervised model selection problem in domain adaptation scenarios, where the objective is to identify the optimal model for the unlabeled target domain. **Right:** Overview of our approach, EnsV, for model selection, which relies solely on predictions of target data by all candidate models.

coefficients, architectural settings, training iterations, and more. By training UDA models using the \(m\) different values of \(\), we obtain corresponding models with weights denoted as \(\{_{i}\}_{i=1}^{m}\). In UDA, the objective of model selection is to pinpoint the model \(_{k}\) that demonstrates the best performance on the unlabeled target domain. Subsequently, we select the corresponding hyperparameter \(_{k}\) as the optimal choice for potential adaptation with unlabeled target samples from the exact target domain. We illustrate the problem setting in Figure 1. Without loss of generality, in this paper, we assume \(m\) is greater than \(1\), and candidate models have different weights \(\), resulting in different discriminative mappings of \(f(x,)\). For clarity, we treat both \(\) and the model interchangeably in the presentation. This also applies to model selection, hyperparameter selection, and validation.

### Ensemble: The Overlooked 'Free Lunch' in Model Selection

Model selection in UDA is challenging due to the absence of labeled target data for directly evaluating candidate models. Existing selection approaches typically address this challenge from two perspectives: leveraging labeled source data  or designing unsupervised metrics based on specific assumed priors . Surprisingly, we've observed that all existing model selection methods treat each candidate model independently, overlooking the collective potential offered by the off-the-shelf ensemble created by these candidates. In this paper, unless otherwise specified, the ensemble refers to prediction-based ensembling, which involves averaging probability predictions across all models to obtain the averaged prediction, i.e., \(_{i=1}^{m}f(x,_{i})\) for a sample \(x\).

In contrast, we first investigate the potential of the ensemble within the model selection problem. When contemplating the use of the ensemble, two primary concerns often arise, one concerning low efficiency due to training multiple models and the other related to the potential lack of diversity among candidate models. Upon closer inspection of model selection, we observe that the problem setting inherently offers a range of pre-existing candidate models, effectively addressing the efficiency concern without requiring extra model training. Furthermore, all candidate models are trained using a UDA method with varying hyperparameter values, resulting in diverse yet effective discriminative abilities. This naturally mitigates the diversity concern. _Interestingly, the ensemble emerges as a 'free lunch' in UDA model selection, a previously overlooked insight._ To delve deeper into the effectiveness of the ensemble, we present a theoretical analysis grounded in the proposition below.

**Proposition 1**: _Given negative log-likelihood (NLL) as the loss function, defined as \(l(p,y)=- p^{y}\), and considering a random sample \(x\) with label \(y\), the following inequality can be established between the loss of the ensemble \(_{i=1}^{m}f(x,_{i})\), the averaged loss of all models \(\{_{i}\}_{i=1}^{m}\), and the loss of the worst one \(_{}\):_

\[l(_{i=1}^{m}f(x,_{i}),y)<_{i=1}^{m}l(f(x, _{i}),y)<l(f(x,_{}),y).\]

Kindly refer to the Appendix for the proof. This proposition theoretically guarantees that the ensemble strictly outperforms the worst candidate model.

### Ensemble-based Validation (EnsV): Ensemble as A Role Model for Model Selection

Intuitively, we employ the previously mentioned off-the-shelf ensemble as a reliable role model and select the model that generates predictions closest to this role model among all candidates. To begin with, for each unlabeled target sample \(x\), we consider the ensemble \(_{i=1}^{m}f(x,_{i})\) as a reliable estimation of its unavailable ground truth. This enables us to obtain reliable predictions for all target data, denoted as \(\{_{i=1}^{m}f(x_{j},_{i})\}_{j=1}^{n_{t}}\). These ensembles can be viewed as the output of a reliable role model, aiding in accurate model selection in the subsequent step. We then utilize the role model to assess all candidate models and select the one with the highest similarity. For simplicity, EnsV involves direct measurement of accuracy between the role model output \(\{_{i=1}^{m}f(x_{j},_{i})\}_{j=1}^{n_{t}}\) and the predictions made by each candidate model, such as \(\{f(x_{j},_{i})\}_{j=1}^{n_{t}}\) for the model with weights \(_{i}\). We select the model \(_{k}\) with the highest accuracy and determine the optimal value \(_{k}\) for the hyperparameter \(\). Figure 1 provides a vivid illustration of our approach, EnsV. Guided by a reliable role model, EnsV can safely avoid selecting the worst candidate model, a distinct advantage over all existing model selection approaches.

## 4 Experiments

### Setup

**Datasets** Our experiments encompass diverse and widely-used image classification benchmarks: (_i_) _Office-31_ with 31 classes and 3 domains (Amazon (A), DSLR (D), and Webcam (W)); (_ii_) _Office-Home_ with 65 classes and 4 domains (Art (Ar), Clipart (Cl), Product (Pr), and Real-World (Re)); (_iii_) _VisDA_ with 12 classes and 2 domains (training (T) and validation (V)); and (_iv_) _DomainNet-126_[13; 5] with 126 classes and 4 domains (Real (R), Clipart (C), Painting (P), and Sketch (S)). Additionally, we conduct experiments in synthetic-to-real semantic segmentation, specifically targeting the transfer from _GTAV_ to _Cityscapes_.

**UDA methods** In our experiments, we assess all the model selection approaches listed in Table 2. Kindly refer to the Appendix for detailed introductions of them. With these approaches, we perform model selection for various UDA methods across different UDA settings. For CDA of image classification, we consider ATDOC , BNM , CDAN , MCC , MDD , and SAFN . For PDA, we consider PADA  and SAFN . For OPDA, we consider DANCE . For SFUDA, we consider the white-box method SHOT  and the black-box method DINE . For domain adaptive semantic segmentation, we consider AdaptSeg  and AdvEnt . Following previous model selection studies [15; 19], we primarily focus on one-hyperparameter validation and present the comprehensive hyperparameter settings for all UDA methods in the Appendix. For each hyperparameter, we generally explore 7 candidate values. Additionally, we perform two types of challenging two-hyperparameter validation tasks. For classification tasks, we select the bottleneck dimension as the second hyperparameter from 4 options: \(256,512,1024,2048\) in MCC and MDD. For segmentation tasks, following SND , we select the training iteration as the second hyperparameter from 8 options, ranging from 16,000 to 30,000 iterations at intervals of 2,000 iterations, in AdaptSeg and AdvEnt.

**Implementation details** For all UDA methods, we train UDA models using the Transfer Learning Library* or the official GitHub code on a single RTX TITAN 16GB GPU with a batch size of 32 and a total number of iterations of 5000. Unless specified, checkpoints are saved at the last iteration. We adopt ResNet-101  for _VisDA_ and segmentation tasks, ResNet-34  for _DomainNet_, and ResNet-50  for other benchmarks. We assess the selection performance of all model selection methods on our trained models for fair comparisons. As a result, comparing our reported values with those from the original papers [15; 19] would be inappropriate. We repeat trials with three random seeds and report the mean for results. Source-based validation methods allocate \(80\%\) of the source data for training and the remaining \(20\%\) for validation.

    &  &  &  \\  & \(\)Ar & \(\)Cl & \(\)Pr & \(\)Re & avg & \(\)Ar & \(\)Cl & \(\)Pr & \(\)Re & avg & \(\)Ar & \(\)Cl & \(\)Pr & \(\)Re & avg \\  SourceRisk  & 66.83 & 52.54 & 78.57 & 78.67 & 68.59 & 62.44 & 50.74 & 77.35 & 77.46 & 76.63 & 55.00 & 42.65 & 99.50 & 68.31 & 58.99 \\ IVCV  & 67.97 & 54.03 & 83.31 & 79.26 & 69.89 & 65.66 & 48.16 & 74.09 & 73.28 & 65.52 & 63.11 & 41.24 & 67.17 & 71.93 & 60.41 \\ DEV  & 67.39 & 54.23 & 77.78 & 79.39 & 69.70 & 68.56 & 56.39 & 73.92 & 79.59 & 68.41 & 67.32 & 57.04 & 68.76 & 76.91 & 67.49 \\  FV  & 68.85 & 56.13 & 78.93 & 79.64 & 79.08 & 68.25 & 56.75 & **78.08** & 76.70 & 70.44 & 67.66 & 36.74 & 56.01 & 70.78 & 68.92 \\ Entropy  & 63.67 & 55.58 & 76.34 & 78.36 & 68.60 & 66.28 & 54.95 & 74.15 & 77.64 & 68.14 & 67.66 & **57.36** & 67.37 & 77.45 & 69.76 \\ InfMax  & 63.67 & 55.63 & 77.61 & 78.66 & 68.62 & 54.98 & 74.15 & 77.64 & 81.66 & 67.94 & **75.36** & 76.37 & 77.45 & 69.76 \\ SND  & 63.67 & 55.03 & 76.54 & 77.54 & 68.34 & 66.28 & 54.93 & 74.15 & 77.64 & 81.66 & **79.34** & **75.66** & 79.76 & 76.80 & 70.04 \\ Corr  & 63.51 & 50.39 & 73.89 & 73.88 & 68.42 & 58.10 & 48.57 & 67.99 & 79.06 & 68.30 & 53.84 & 41.21 & 64.96 & 67.65 & 56.91 \\ EnsV & **67.90** & **58.08** & **79.81** & **80.11** & **71.74** & **68.61** & **75.38** & **70.94** & **79.00** & 67.88 & **57.46** & **77.39** & **78.19** & **70.25** \\  Worat & **62.92** & 50.39 & 73.39 & 78.38 & 68.52 & 58.10 & 45.37 & 68.96 & 79.09 & 67.50 & 53.30 & 41.21 & 47.28 & 67.75 & 68.56 & 36.60 \\ Best & 68.97 & 58.35 & 80.27 & 80.85 & 27.04 & 68.93 & 57.51 & 78.43 & 79.57 & 71.11 & 68.19 & 57.00 & 77.44 & 78.19 & 70.43 \\   & \(\)Ar & \(\)Cl & \(\)Pr & \(\)Re & avg & \(\)Ar & \(\)Cl & \(\)Pr & \(\)Re & avg & \(\)Ar & \(\)Cl & \(\)Pr & \(\

[MISSING_PAGE_FAIL:7]

with other target-specific validation methods on the large-scale benchmark _DomainNet-126_ and in two extra practical UDA settings: OPDA and SFUDA.

#### 4.1.1 CDA

We compare all target-specific validation methods on the large-scale benchmark _DomainNet-126_ (Table 6). EnsV consistently keeps the leading validation performance, while other approaches exhibit high variance.

#### 4.1.2 SPUDA

In some-free UDA, where source-based model selection methods are not applicable due to no access to source data, we select SHOT for the white-box setting on _Office-31_ and DINE for the black-box setting on _VisDA_ (Table 8). EnsV consistently maintains near-best selections, while other target-based approaches frequently make worst-case selections.

#### 4.1.3 Worst-model selection comparisons

For empirical evidence of the superiority of EnsV, we compare EnsV with other target-specific methods, specifically focusing on worst-case avoidance, through specific examples presented in Table 9. In short, EnsV consistently avoids the worst selections, while other methods often encounter significant challenges.

    &  &  &  &  \\  & \(\)C & \(\)P & \(\)R & \(\)S & avg & \(\)C & \(\)P & \(\)R & \(\)S & avg & \(\)C & \(\)P & \(\)R & \(\)S & avg \\  Entropy  & **67.99** & **63.80** & 74.12 & **93.34** & **66.66** & 67.36 & 64.28 & 74.31 & 66.99 & 62.66 & 63.75 & 61.53 & 79.90 & 52.17 & 64.34 & 64.55 \\ IndMax  & **67.99** & **65.80** & 74.42 & **59.34** & **66.66** & 67.05 & 64.28 & 74.31 & 65.67 & 65.33 & 63.75 & 61.85 & 79.60 & 52.17 & 64.34 & 65.44 \\ SND  & **67.09** & 66.48 & 74.24 & **59.34** & 63.63 & 56.56 & 54.50 & 74.31 & 42.37 & 56.93 & 63.75 & 61.85 & 79.60 & 47.00 & 63.05 & 62.12 \\ Corr-C  & 75.35 & 62.88 & 74.42 & 54.63 & 62.32 & 59.75 & 63.41 & **72.62** & 43.27 & 60.79 & 59.98 & 52.27 & 74.42 & 35.69 & 62.59 & 61.90 \\ EnsV & 65.88 & 65.27 & **74.44** & 57.65 & 67.58 & **66.06** & **72.53** & 67.94 & **73.03** & **66.44** & **80.01** & **61.73** & **70.12** & **61.73** \\  Worst & 57.35 & 66.70 & 73.43 & 57.14 & 60.44 & 57.95 & 75.40 & 74.31 & 42.37 & **56.74** & 59.78 & 63.74 & 74.00 & 68.19 & 67.93 \\ Best & 67.09 & 65.80 & 74.44 & 59.43 & 66.66 & 67.86 & 66.50 & 78.68 & 58.49 & 67.88 & 70.30 & 68.44 & 80.38 & 62.23 & 70.34 & 68.29 \\   

Table 6: Validation accuracy (\(\%\)) of CDA on _DomainNet-126_ (_DNet_).

   Method &  &  &  \\  & \(\)Ar & \(\)Cl & \(\)P & \(\)R & \(\)P & \(\)R & \(\)P & \(\)A & \(\)D & \(\)W & avg & T\(\)V \\  Entropy  & 38.29 & 26.08 & 36.51 & 35.92 & 17.10 & 32.19 & 37.69 & 46.40 & 45.33 & 25.39 & 33.75 & 39.37 & 34.27 \\ IndMax  & 38.29 & 26.08 & 36.51 & 35.92 & 17.10 & 32.19 & 37.69 & 46.40 & 45.33 & 25.39 & 33.75 & 39.37 & 34.25 \\ SND  & 1.00 & 0.00 & 12.73 & 0.00 & 42.84 & 1.95 & 19.72 & 19.59 & 35.69 & 25.39 & 0.90 & 28.40 & 14.98 \\ Corr-C  & 1.00 & 0.00 & 12.73 & 0.00 & 42.84 & 1.95 & 19.77 & 11.99 & 35.69 & 69.02 & 0.00 & 28.40 & 18.62 \\ EnsV & **38.40** & **75.96** & **66.57** & **71.76** & **78.17** & **69.99** & **77.42** & **48.15** & **69.40** & **81.84** & **67.54** & **84.31** & **68.96** \\  Worst & 1.00 & 0.00 & 12.73 & 0.00 & 17.10 & 1.959 & 19.77 & 11.99 & 35.69 & 25.39 & 60.00 & 28.40 & 12.84 \\ Best & 67.09 & 76.96 & 66.57 & 77.16 & 75.17 & 69.69 & 77.42 & 64.32 & 72.87 & 81.84 & 67.54 & 84.31 & 72.98 \\   

Table 7: H-score  (\(\%\)) of an OPDA method DANCE  on _Office-Home_.

    &  &  &  &  \\ Method & ATDOC & ATDOC & BNM & BNM & MDD & SAFN & PADA & PADA & DANCE & DANCE & SHOT & DINE \\  & Cl\(-\)Ar & C\(-\)S & Ar\(-\)P & \(\)P & \(\)P & \(\)Cl & \(\)P & \(\)P & \(\)Ar & Re\(-\)Ar & Re\(-\)Ar & Pr\(-\)R & D\(-\)A & T\(-\)V \\  Entropy  & 59.25 & 46.43 & 67.04 & 40.95 & 55.85 & 43.30 & 75.94 & 70.52 & 25.39 & 45.53 & 71.21 & 71.99 \\ InfoMax  & 59.25 & 46.43 & 67.04 & 54.93 & 55.85 & 43.30 & 78.02 & 70.52 & 25.39 & 45.53 & 71.21 & 71.99 \\ SND  & 59.25 & 46.43 & 67.04 & 40.95 & 21.60 & 43.30 & 55.94 & 74.66 & 25.39 & 35.69 & 7

### Further Analysis

**Validation with two hyperparameters** We conduct two-hyperparameters model selection experiments with a large pool of model candidates, i.e., 28 models for image classification (Table 10) and 48 models for image segmentation (Table 11). EnsV consistently achieves near-optimal selections in both scenarios, surpassing other versatile validation methods such as Entropy and SND.

**Robustness to architectures** In our experiments, we evaluate the robustness of EnsV across various ResNet backbone variants, observing consistent success across different scales. We also conduct validation experiments using the ViT-B architecture  on the R\(\)S task with BNM. The validation results, presented in Table 12, demonstrate that EnsV achieves the best selection. However, all other target-based methods except SND make the worst selection.

   Method &  &  \\   & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & AVG \\  SourceRisk & 55.99 & 73.15 & 78.77 & 69.39 & 69.33 & 57.91 & 76.84 & 81.13 & 72.89 & 72.19 & 70.76 \\ INCV  & 37.89 & 22.92 & 80.42 & 58.43 & 62.24 & 46.09 & 77.74 & 80.68 & 74.45 & 69.74 & 66.08 \\ DEV  & 52.60 & 21.11 & 53.36 & 67.07 & 41.64 & 59.47 & 76.84 & 81.94 & 74.08 & 73.08 & 67.26 \\ BV  & **57.90** & 72.25 & 80.83 & 70.79 & 70.37 & 59.13 & 76.84 & 82.03 & 71.98 & 72.50 & 71.44 \\  Entropy  & 57.21 & **73.19** & 80.06 & **72.31** & 70.69 & 59.75 & 77.77 & 82.37 & 74.33 & 73.56 & 72.13 \\ InfMax  & **57.89** & 72.92 & 80.06 & **72.31** & **70.72** & 59.70 & **78.73** & **52.85** & 70.33 & 72.84 & 71.78 \\ SND  & 38.10 & 56.45 & 70.03 & 65.10 & 57.42 & 53.49 & 74.97 & 72.75 & 74.12 & 69.69 & 63.69 \\ Conv-C  & 30.17 & 44.47 & 57.15 & 50.76 & 45.71 & 44.90 & 56.75 & 74.32 & 67.61 & 60.90 & 53.31 \\ Env-P & 59.91 & 27.24 & **80.93** & 71.16 & 20.44 & **63.90** & 78.21 & 82.28 & **74.91** & **74.07** & **72.26** \\  Worst & 30.17 & 39.81 & 53.36 & 50.76 & 43.53 & 43.02 & 36.75 & 73.47 & 67.24 & 60.12 & 51.83 \\ Best & 57.59 & 73.35 & 80.93 & 72.52 & 71.10 & 61.10 & 78.94 & 83.04 & 75.36 & 74.61 & 72.86 \\   

Table 10: CDA accuracy (\(\%\)) on _Office-Home_ when two hyperparameters are validated.

   Method & BNM  \\  Entropy  & 28.21 \\ InfoMax  & 28.21 \\ SND  & 52.42 \\ Corr-C  & 28.21 \\ EnsV & **55.16** \\  Worst & 28.21 \\ Best & 55.16 \\   

Table 12: CDA accuracy (\(\%\)) of BNM with ViT as the backbone.

Figure 2: For the 28 candidate models available in the two-hyperparameter selection task with MDD on Ar\(\)Cl, we first rank them based on their respective actual target-domain accuracy. We then start with only the best candidate model in the pool and gradually add increasingly inferior models in ascending order of their accuracy. The figure illustrates how adding more inferior models affects the performance of the ensemble and model selection.

#### Robustness to poor candidates

Ensuring the ensemble's resilience to poor models is crucial for its broad effectiveness. We assess this by conducting a two-hyperparameters model selection task for MDD on Ar\(\)Cl. We consider a challenging scenario where only the best candidate model is initially in the pool, gradually adding an increasing number of inferior models. This setup allows us to examine how the ensemble performs when dominated by inferior models. From the results shown in Figure 2, we find that both Ensemble and EnsV still consistently achieve performance above the median, demonstrating their resilience. In contrast, SND , a state-of-the-art method, struggles to surpass the median accuracy.

## 5 Discussions

#### Limitations

EnsV selects the candidate model that most closely matches the ensemble's performance. While EnsV consistently avoids selecting the worst-performing models, its performance can be suboptimal if the ensemble (role model) itself is suboptimal. EnsV may face challenges in the following scenarios:

* Single high-performing vs. poor model: when the model pool contains only one high-performing model and one poor model, EnsV may struggle. In such cases, the ensemble of just these two models might not accurately reflect the quality of the good model.
* Small performance differences: if the performance differences among all candidate models are small, EnsV may have difficulty distinguishing between them. In such scenarios, using ensemble results for model selection may not provide the fine-grained control needed to accurately differentiate between the candidates.
* Predominantly poor models: when the majority of models in the pool are poor, with only a few having normal or good performance, EnsV may encounter issues. An ensemble composed mostly of poor models may produce results that are closer to the poor models, leading to a final selection of a suboptimal candidate.

#### Takeaways

Following a thorough empirical comparison of existing UDA model selection approaches, several key conclusions emerge:

* The significance of model selection in influencing the deployment performance of UDA methods becomes evident. Relying on fixed hyperparameters or limited analyses is inadequate. We emphasize the importance of increased attention and transparent reporting of validation methods, consistent with recommendations in [15; 19; 18].
* Among existing validation methods, we recommend the reverse validation (RV) approach, which, despite being overlooked in previous studies [15; 19; 18], proves to be the most reliable method for widely studied closed-set UDA scenarios when source data is available. However, it requires additional model re-training, making it less lightweight compared to target-based validation methods. Moreover, all existing model selection methods demonstrate unreliability across diverse UDA methodologies and real-world settings such as open-set and source-free UDA. These methods struggle to maintain effectiveness, posing a significant risk to the successful application of UDA in various scenarios.
* Regarding our proposed baseline, EnsV, we believe it is a simple and versatile model selection method that is certified to avoid worst-case selections. While it may not always achieve peak performance, especially when the ensemble result is suboptimal, EnsV offers valuable insights for future explorations in reliable model selection methods.