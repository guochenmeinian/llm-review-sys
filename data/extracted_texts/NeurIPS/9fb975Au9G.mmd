# Django: Detecting Trojans in Object Detection Models via Gaussian Focus Calibration

Guangyu Shen

Purdue University

West Lafayette, IN, 47907

shen447@purdue.edu

&Siyuan Cheng1

Purdue University

West Lafayette, IN 47907

cheng535@purdue.edu

&Guanhong Tao

Purdue University

West Lafayette, IN 47907

taog@purdue.edu

&Kaiyuan Zhang

Purdue University

West Lafayette, IN, 47907

zhan4057@purdue.edu

&Yingqi Liu

Microsoft

Redmond, Washington 98052

yingqiliu@microsoft.com

&Shengwei An

Purdue University

West Lafayette, IN, 47907

an93@purdue.edu

&Shiqing Ma

University of Massachusetts at Amherst

Amherst, MA, 01003

shiqingma@umass.edu

&Xiangyu Zhang

Purdue University

West Lafayette, IN, 47907

xyzhang@cs.purdue.edu

Equal Contribution

###### Abstract

Object detection models are vulnerable to backdoor or trojan attacks, where an attacker can inject malicious triggers into the model, leading to altered behavior during inference. As a defense mechanism, trigger inversion leverages optimization to reverse-engineer triggers and identify compromised models. While existing trigger inversion methods assume that each instance from the support set is equally affected by the injected trigger, we observe that the poison effect can vary significantly across bounding boxes in object detection models due to its dense prediction nature, leading to an undesired optimization objective misalignment issue for existing trigger reverse-engineering methods. To address this challenge, we propose the first object detection backdoor detection framework Django (_Detecting Trojans in Object Detection Models via Gaussian Focus Calibration_). It leverages a dynamic Gaussian weighting scheme that prioritizes more vulnerable victim boxes and assigns appropriate coefficients to calibrate the optimization objective during trigger inversion. In addition, we combine Django with a novel label proposal pre-processing technique to enhance its efficiency. We evaluate Django on 3 object detection image datasets, 3 model architectures, and 2 types of attacks, with a total of 168 models. Our experimental results show that Django outperforms 6 state-of-the-art baselines, with up to 38% accuracy improvement and 10x reduced overhead. The code is available at https://github.com/PurduePAML/DJGO.

## 1 Introduction

Object detection is an extensively studied computer vision application that aims to identify multiple objects in a given image. It has been widely integrated into various real-world systems, including public surveillance , autonomous driving , optical character recognition (OCR), etc.

State-of-the-art object detection systems, e.g., SSD  and YOLO , are primarily built on Deep Learning (DL) models , owing to their remarkable feature representation capabilities.

Despite the impressive performance of existing object detection approaches, they are vulnerable to backdoor attacks. Backdoor attack is a form of training-time attack, in which a malicious adversary strategically embeds an imperceptible _trigger_ (e.g., a small image patch) onto a small set of training data. DL models trained on this poisoned dataset will predict the attacker-chosen _target label_ whenever an input is stamped with the trigger. There are two major backdoor attacks in object detection, namely, _misclassification attack_ and _evasion attack_. As shown in Figure 1, misclassification attack causes the object detection model to misclassify an object as a target object, whereas evasion attack makes the model fail to recognize certain objects (i.e., classify traffic signs as the background). Such attacks can significantly hinder the deployment of object detection systems in real world as they may cause severe security issues or even life-threatening incidents.

To counter the backdoor threat, researchers have proposed a multitude of defense techniques aimed at safeguarding the model against backdoor attacks throughout its entire life cycle . Among them, trigger inversion  is one of the most popular backdoor scanning methods and has demonstrated its effectiveness in various tasks . Given a small set of clean samples and the subject model, trigger inversion techniques aims to determine if the model is backdoored or not by reverse-engineering the underlying triggers. The task of trigger inversion can be formulated as an optimization problem with specific constraints, such as trigger size  or particular stylistic attributes .

In order to detect backdoors in object detection, a direct solution is to adapt trigger inversion from the classification task to object detection. We however find that a simple adaptation of trigger inversion fails short. This attributes to the significantly denser output of object detection models due to the nature of the task. In specific, an object detection model, such as SSD , typically produces thousands of bounding boxes with corresponding class probabilities to capture objects of varying scales within a single image. Attackers can exploit this extensive output space to conceal injected triggers and mislead the optimization process during trigger inversion, causing the _loss misalignment issue_. That is, throughout the entire trigger inversion process, the loss values consistently remain either significantly high or extremely low, regardless of the _Attack Success Rate (ASR)_ that denotes the ratio of misclassified samples when the trigger is applied. This misalignment property hinders the identification of high-quality triggers, leading to low backdoor detection performance.

Our study reveals that the underlying cause of the _loss misalignment issue_ stems from the unequal impact of poisoning on individual bounding boxes within the backdoored model. More specifically, the backdoor trigger affects only a small fraction of bounding boxes, while leaving the majority of them largely unaffected. Consequently, the aggregated gradient information is obfuscated by remaining benign bounding boxes during inversion, thereby impeding the optimizer from accurately estimating the gradient direction. Based on this observation, we propose the first trigger inversion-based backdoor detection framework for object detection: Django (_Detecting Trojans in Object Detection Models via Gaussian Focus Calibration_). The overview is shown in Figure 2. It features a novel _Gaussian Focus Loss_ to calibrate the misaligned loss during inversion by dynamically assigning weights to individual boxes based on their vulnerability. Equipped with a label proposal pre-processor, Django is able to quickly identify malicious victim-target labels and effectively invert the injected trigger lies in the backdoored model. Extensive evaluation of 3 object detection datasets, utilizing 3

Figure 1: Triggers with different effects. The blue patch in Figure 1(a) is an evasion trigger, causing the bounding box to disappear. Figure 1(b) represents a misclassification trigger (yellow), leading to the misclassification of “pizza” as “giraffe”.

different model architectures with a total of 168 models, demonstrates the superiority of Django over six existing state-of-the-art backdoor scanning techniques. Specifically, we achieve up to 0.38 ROC improvement and a substantial reduction in scanning time.

## 2 Background & Threat Model

**Deep Learning based Object Detection Techniques.** Extensive research efforts have been devoted to deep learning (DL) object detection, encompassing various aspects such as neural network architecture design [32; 45; 2], training optimization [29; 61; 74], and robustness [71; 66; 6]. These endeavors contribute to a broad body of work aimed at advancing the field of DL object detection. Specifically, given an image \(x\) that contains \(p\) objects, an object detection neural network \(g_{}\) (parameterized by \(\)) outputs \(K\) (\(K p\)) 5-tuples: \(=g_{}(x)=\{_{k}^{x},_{k}^{y},_{k}^{h}, _{k}^{w},_{k}\}_{k=1}^{K}=\{_{k},_{k}\}_{k=1}^{K}\), shorten as \(_{k}(x)\) and \(_{k}(x)\), which denote the center coordinates, height, width, and class probability of the \(k\)-th predicted bounding box. Note that \(K\) is usually much larger than \(p\) (\(K=8732\) in SSD , \(p 7.7\) in COCO dataset ). Box matching algorithms [32; 46] and _Non-Maximum Suppression_(NMS)  were introduced to address the challenge posed by the substantial box number discrepancy between the prediction and ground-truth in both training and inference stages.

**Backdoor Attack & Defense.** Backdoor attacks can be carried out through data poisoning with modified or clean labels [17; 34; 7; 58] and model parameter hijacking . Initially, small image patches were used as backdoor triggers [17; 8; 48], and later on more complex triggers have been successfully injected in DL models [28; 64; 33; 37]. These attacks are not limited to image classification models; they also affect Large Language Models , code models [73; 52], Reinforcement Learning agents , pre-trained image encoders [24; 56], and object detectors [3; 39]. Researchers have proposed various approaches from defense perspectives. During the training stage, methods such as poisoned sample identification and filtering have been proposed [40; 53; 4; 57; 19; 43]. In the post-training stage, backdoor scanning techniques aim to recover injected triggers from trained models [59; 49; 54; 62; 10; 63; 35; 50; 15]. Backdoor removal techniques are employed to eliminate triggers with minimal resource consumption and performance impact. Real-time rejection of trigger-carrying samples during inference can be achieved using methods like [12; 16]. Furthermore, theoretical analyses have been conducted to understand backdoor attacks [67; 68; 72; 23]. In this work, we prioritize the practical application of trigger inversion-based backdoor detection, with a specific focus on object detection models.

## 3 Methodology

**Threat Model.** Our work considers the standard setting used in existing backdoor scanning literature [59; 33; 18; 10], where the defender has a small set of clean samples from each class in the validation set (10 in our paper), but no poison samples and white-box access to the model under scanning. The defense goal is to classify the benignity of the subject model. Our focus in this study is on _label specific_ polygon triggers with two types of effects: _misclassification_ and _evasion_, which cause the model to predict an object as a target object and the background, respectively.

### Misalignment of CE Loss and ASR in Object Detection Model Trigger Inversion

**Backdoor Detection via Trigger Inversion.** Trigger inversion aims to decide whether the model is backdoored or not by reverse-engineering the injected trigger through optimization. Consider an image classification model \(f_{}\) parameterized by \(\) with \(C\) classes and a small support set \(_{c_{v}}\) from

Figure 2: Overview of Django

each class \(c_{v}\). Trigger inversion [49; 59] aims to solve the following optimization objective.

\[_{m,p}_{x_{c_{v}}}[(f_{}((x)),c _{t})]+||m||_{1},(x)=(1-m) x+m p.\] (1)

Variables \(m\) and \(p\) are the trigger mask and pattern respectively, to be optimized. The loss function \(()\) typically rescues the task loss function as a measurement of the degree of the misbehaved model. In classification and object detection, it will be the cross entropy (CE) loss. \(\) is the coefficient on the regularization term, and \(||m||_{1}\) represents the \(_{1}\) norm of the mask. Since the defender lacks the knowledge of the exact poisoned victim and target labels, the backdoor scanning requires optimizing Eq. 1 for every label pair \((c_{v},c_{j})\). If any of these pairs yields a trigger with an exceptionally small size, it is indicative that the model is trojaned.

Intuitively, it shall be effortless to adapt existing trigger inversion methods from image classification to object detection. Considering the disparity of these two tasks, object detection models generate much denser output for each input, i.e., thousands of bounding boxes versus one label in classification. We can make adjustments to Eq. 1 to optimize a trigger that can impact samples at the box level rather than the entire image. Given a sample containing a victim object, one naive modification is to optimize a trigger that can influence all bounding boxes that are close to the victim object. The objective function can be reformulated as follows:

\[&_{m,p}_{x_{c_{v}}} _{_{x}}[(,c_{t})]+||m|| _{1},\\ &_{x}=\{_{k}((x)) [_{k}((x)),b^{*}]\}(x)=(1-m) x+m p.\] (2)

We denote the predicted probability and coordinates of the \(k\)-th bounding box of sample \((x)\) as \(_{k}((x))\) and \(_{k}((x))\), respectively. The function \(()\) refers to the Jaccard Overlap, also known as Intersection over Union. \(b^{*}\) denotes the ground-truth box coordinates. Intuitively, \(_{x}\) indicates the class confidence of a set of predicted boxes surrounding the victim object. Additionally, \(\) is a pre-defined threshold used to quantify the degree of overlap between predictions and the ground-truth box and set to 0.5 by default. In this naive extension, the objective is to consider every predicted box surrounding the victim object as the target of the attack and aim to reverse-engineer a trigger that compromises all of them. We explain this limitation in the following.

**Mis-aligned Loss and ASR.**_Attack Success Rate (ASR)_ denotes the percentage of victim samples misclassified as the target label when the inverted trigger is applied (e.g., set at 90%). For _misclassification trigger_, a successful attack is defined as the presence of at least one box predicts the target class in the final detection. _Evasion Attack_ is a particular case of misclassification attack where the target label is the background class \(\). _Cross-entropy (CE)_ loss measures the numerical difference between model output (confidence) and the desired prediction. In general, ASR and the CE loss are negatively correlated throughout the optimization process because minimizing the CE loss increases the probability of the target label. If the target label probability surpasses those of other labels, the input is misclassified as the target label. Figure 3(a) illustrates the characteristics of the CE loss and the ASR during trigger inversion for an image classification task.

Such a correlation however may not hold for object detection using a naive trigger inversion Eq. 2. Specifically, Figure 3(b) shows the inversion process on a Faster-RCNN  object detection model poisoned by an evasion trigger (TrojAI Round13#91, victim class 47). Note that an evasion trigger

Figure 3: ASR vs. CE loss during trigger inversion

will cause all bounding boxes surrounding the victim object to disappear. Observe that even when the CE loss is small (\( 0.1\)), the ASR remains low (0%). For a model poisoned by misclassification attack (Round13#12 victim class 3), as illustrated in Figure 3(c), the CE loss remains incredibly high (\( 10\)) and decreases only slightly during trigger inversion, while the ASR remains low or even decreases. Therefore, the naive trigger inversion fails to find a trigger with high ASR and cannot identify the poisoned model. The observations are similar when extending several state-of-the-art trigger inversion techniques  to object detection. For instance, NC  only achieves 58% detection accuracy on the TrojAI dataset .

**Causes of Misalignment.** We study the misalignment problem by using differential analysis to compare the intermediate outputs of the object detection model with and without the ground-truth trigger, specifically the target label confidence of predicted bounding boxes. Given a sample with a victim object, we collect all predicted boxes with _IoU_\(\) (\(=0.5\) means a box is close to the object) around the victim object and track their target label probability change before and after applying the ground-truth trigger. The key observation is that _not every box is equally poisoned_. Figure 4(a) shows the per-box confidence discrepancy for a poisoned model with a misclassification trigger. Although there are a total number of 84 boxes surrounding the victim object, only approximately 12% of them (10 boxes, shown in high red bars) are infected by the ground-truth backdoor trigger. The rest of the boxes only have an average of \( 10^{-10}\) probability increase on the target label. For the evasion trigger in Figure 4(b), there is a slight difference, where 85% of the boxes already have high confidence (on the background class \(\)) even without triggers. The backdoor trigger only elevates the target probability for the remaining 15% of the boxes.

This intuitively explains the misalignment issue of naive trigger reverse-engineering methods. During optimization, the cross-entropy loss of each box is equally aggregated, and the averaged gradient is used to mutate the trigger via back-propagation. However, in the poisoned model, there are only a small fraction of bounding boxes affected by the backdoor trigger. The gradient information from unaffected boxes dominates the optimization direction, causing the inversion to fail. This observation highlights the significance of considering individual bounding boxes separately when devising inversion methods for object detection, which shall _focus on boxes that have a higher likelihood of being compromised but have not been considered yet._

### Our Solution: Trigger Inversion via Gaussian Focus Loss

As discussed in the last section, the goal of our trigger inversion is to discriminate different bounding boxes during optimization. Particularly, we aim to find easy-to-flip boxes that are vulnerable and targeted by injected backdoors. However, it is non-trivial to adaptively select boxes during trigger inversion as the box output is constantly changing.

_Focal Loss_ is a widely used metric in object detection for assisting better training. It separates objects into hard-to-classify and easy-to-classify objects based on their training errors, and assigns different weights on their training losses. For the notation simplicity, we use a binary object detection model. Note that it is straightforward to extend it to multi-class cases. Recall \(_{k}\) denotes the output probability of the \(k\)-th victim box. The CE loss in Eq. 2 for the \(k\)-th victim box can be simplified as \((_{k},c_{t})=-(_{k}) c_{t}=-(_{k})\). _Focal Loss_ is a refined weighted CE loss aiming to solve the imbalanced learning difficulty issue in object detection models. In detail, it assigns larger weights to boxes that perform poorly (_hard examples_), i.e., small \(_{k}\), to enlarge its contribution to the aggregated

Figure 4: Target confidence discrepancy w/o the ground-truth trigger

loss. It assigns smaller weights to boxes that have already learned well (_easy examples_), i.e., large \(_{k}\):

\[_{fl}(_{k},c_{t})=-(1-_{k})^{}( _{k}),\] (3)

where \( 0, 1\) (\(=0.25,=2\) suggested in ). The coefficient of the \(k\)-th box will exponentially grow as its probability \(_{k}\) gets close to 0 and vice versa.

Eq. 3 provides insights to address the misalignment problem. Inspired by this, we propose _Gaussian Focus Loss_ to calibrate the misaligned loss. As illustrated in Figure 4(a), the backdoor trigger tend to infect only a small portion of victim boxes and leaves remaining largely unchanged. In other words, a trigger will mainly focus on compromising more vulnerable boxes that are easy to flip (_easy example_). Therefore, our goal is essentially opposite to the _Focal Loss_ as it aims to pay more attention to the _hard samples_. To avoid the gradient vanishing issue shown in Figure 4(b), a box shall also not be focused as long as it reaches the attack criterion, i.e., target label confidence \(_{k} 0.1\). To summarize, the desired loss shall be able to dynamically capture a set of vulnerable boxes that have not been flipped yet, and assign a large coefficient to encourage the transition. The natural bell shape of the Gaussian Distribution perfectly satisfies our constraints*. We formulate our proposed novel _Gaussian Focus Loss_ as follows:

Footnote *: Any distributions characterized by centralized peaks are suitable for our intended purpose. We explore alternatives in Appendix D.

\[_{gf}(_{k},c_{t})& =-(_{k},b^{*})(_{k}^{};_{k},_{k})(_{k})\\ where\ (_{k},b^{*})&=( _{k},b^{*}),\ (x;,)=}e^{-()^{2}}\] (4)

It involves two key attributes to measure the vulnerability of a victim box during optimization: the overlapping with the ground-truth object and the confidence towards the target label. Namely, boxes with larger overlapping and higher target label confidence tend to be more vulnerable. The \(_{k}\) and \(_{k}\) are the mean and standard deviation of the Gaussian Distribution. Note that we allow two parameters to be co-optimized with the inverted trigger such that we can adjust the range of focused boxes dynamically through the entire trigger inversion procedure. We initialize \(_{k}=0.1\) and \(_{k}=2\) in this paper. Figure 5 provides a visual explanation of the coefficient dynamics during various optimization steps. Initially, when the victim boxes exhibit low confidence towards the target label, the _Gaussian Focus Loss_ assigns slightly larger weights to a considerable number of bounding boxes surrounding the initial value (0.1). As the optimization progresses, the _Gaussian Focus Loss_ prioritizes more vulnerable boxes that have higher target label probabilities by decreasing the value of \(\) and increasing the value of \(\). In the later stages, the weight coefficients of boxes that have already been flipped are reduced to mitigate the issue of gradient vanishing.

### Compromised Label Proposal via Backdoor Leakage

Without knowing the exact victim and target labels, scanning backdoors is time-consuming. To reduce the time cost, pre-processing is proposed  to quickly select a small set of promising compromised label pairs based on the widely existing _backdoor leakage_ phenomena: the poisoned

Figure 5: Coefficient dynamics during reverse-engineering a misclassification trigger

model's behavior on victim samples tends to shift towards the target label even without the appearance of the backdoor trigger. Therefore, we can feed clean samples from each class and pick the most likely target label by observing the model's output distribution shift. Specifically, given a set of samples \(\{x_{i}\}_{i=1}^{n}\) from class \(c_{i}\), we collect top-\(h\) predicted classes of each predicted box for each sample \(x_{i}\). Denote \(_{j}\) as the frequency of class \(c_{j}\) appearing in the total box predictions. We consider a pair \(\{c_{i},c_{j}\}\) as a compromised label pair if \(_{j}\). In this paper, we make a trade-off by setting \(h=5\) and \(=0.5\). We also evaluate the sensitivity of hyper-parameters in Section 4.2.

## 4 Evaluation

All the experiments are conducted on a server equipped with two Intel Xeon Silver 4214 2.40GHz 12-core processors, 192 GB of RAM, and a NVIDIA RTX A6000 GPU.

**Models and Datasets.** We conduct the experiments on models from TrojAI  round 10 and round 13, which consists of a total of 168 models. Approximately half of these models are clean, while the other half are attacked. Our evaluation covers 3 existing object detection image datasets, including COCO , Synthesized Traffic Signs , and DOTA_v2  on three representative object detection architectures: single-stage detector: SSD , two-stage detector: Faster-RCNN  and vision transformer based detector: DETR . Please refer to Appendix A for more detailed description.

**Attack Settings.** We conducted an evaluation of Django by subjecting it to two types of backdoor attacks commonly observed in object detection models: Misclassification and Evasion attacks [3; 38] Misclassification attack means to flip the prediction of objects from the victim class to the target class, when the backdoor trigger is stamped on the input image. On the other hand, evasion triggers tend to suppress the recognition of victim objects, such that the model will consider them as the background. It is noteworthy that the evasion trigger can be regarded as a particular instance of the misclassification trigger, where the target label is set as the background class \(\). As a result, its detection can be performed uniformly. Besides, we leverage stealthy polygons with different colors and textures as backdoor triggers, which are widely evaluated in many backdoor detection papers [59; 17; 49].

**Evaluation Metrics.** In our evaluation of backdoor detection methods, we employ four well-established metrics: Precision, Recall, ROC-AUC, and Average Scanning Overheads for each model. The unit of measurement we use is seconds (s), and we set a maximum threshold of 1 hour (3600 s) for all the methods being evaluated. If the scanning process exceeds 3600 seconds, it is terminated, and we rely on the existing results for making predictions. Appendix B present more details.

   D & M & Metric & ABS & NC & NC\({}^{*}\) & Pixel & Pixel\({}^{*}\) & Tabor & Tabor & Tabor \\   &  & Precision & 0.7143 & 0.6250 & 0.6522 & 0.4400 & 0.6666 & 0.3300 & 0.4000 & 0.8000 \\  & & Recall & 0.6250 & 0.9375 & 0.9375 & 0.5000 & 0.2500 & 0.3750 & 0.6250 & 1.0000 \\  & & ROC-AUC & 0.7109 & 0.6250 & 0.6641 & 0.6250 & 0.7083 & 0.5400 & 0.6725 & **0.9160** \\  & & Overhead(s) & 409.23 & -3660 & 1180.52 & -3660 & 953.42 & -3660 & 902.10 & 861.30 \\   & & Precision & 0.6667 & 0.3478 & 0.6364 & 0.5512 & 0.6250 & 0.6621 & 0.7125 & 0.8571 \\  & & Recall & 0.3750 & 1.0000 & 0.8750 & 0.6110 & 0.6250 & 0.6012 & 0.7500 \\  & & ROC-AUC & 0.5352 & 0.6094 & 0.6953 & 0.6721 & 0.7500 & 0.6425 & 0.6820 & **0.8750** \\  & & Overhead(s) & 753.89 & -3660 & 2799.42 & -3600 & 2466.08 & -3660 & 2562.59 & 2128.99 \\   & &  & Precision & - & 0.3478 & 0.3478 & 0.4000 & 0.5000 & 0.2307 & 0.5000 & 0.8750 \\  & & Recall & - & 1.0000 & 1.0000 & 0.5000 & 0.3750 & 0.5270 & 0.6250 & 0.8750 \\  & & ROC-AUC & - & 0.5312 & 0.5312 & 0.5000 & 0.6660 & 0.2500 & 0.7600 & **0.9160** \\  & & Overhead(s) & - & 3600 & 691.27 & \(>\)3600 & 294.32 & \(>\)3600 & 275.50 & 228.92 \\   &  &  & Precision & 0.6000 & 0.3636 & 1.0000 & 0.3300 & 0.3000 & 0.3330 & 1.0000 \\  & & Recall & 0.7500 & 1.0000 & 0.1250 & 0.2500 & 0.7500 & 0.2500 & 0.5000 & 1.0000 \\  & & ROC-AUC & 0.5000 & 0.5312 & 0.6875 & 0.3750 & 0.3333 & 0.5000 & 0.6250 & **1.0000** \\  & & Overhead(s) & 424.16 & \(>\)3600 & 675.32 & \(>\)3600 & 529.11 & \(>\)3600 & 618.72 & 678.48 \\   & &  &  & Precision & 0.8000 & 0.3636 & 0.3636 & 0.3330 & 0.5000 & 0.3330 & 0.3300 & 0.8000 \\  & & Recall & 1.0000 & 1.0000 & 1.0000 & 0.5000 & 0.7500 & 0.5000 & 0.5000 & 1.0000 \\  & & ROC-AUC & 0.8750 & 0.5625 & 0.5625 & 0.5000 & 0.6666 & 0.5000 & 0.5000 & **0.9160** \\  & & Overhead(s) & 1127.01 & \(>\)3600 & 2866.14 & \(>\)3600 & 1565.82 & \(>\)3600 & 1602.10 & 1425.25 \\   &  &  & Precision & 0.5167 & 0.5231 & 0.8333 & 0.7500 & 1.0000 & 0.6120 & 0.7500 & 0.9696 \\  & & Recall & 0.9688 & 1000 & 0.1471 & 0.2500 & 0.2500 & 0.5620 & 0.5300 & 0.8888 \\  & & ROC-AUC & 0.5584 & 0.5404 & 0.5579 & 0.6200 & 0.7000 & 0.6419 & 0.6820 & **0.9305** \\  & & Overhead(s) & 273.23 & \(>\)3600 & 2119.34 & \(>\)3600 & 1788.50 & \(>\)3600 & 1688.19 & 1476.01 \\   

Table 1: Comparison to trigger inversion based methods

**Baseline Methods** We compare Django against 6 baseline methods, including 4 trigger inversion based methods (i.e., NC , ABS , Pixel  and Tabor ), and 2 meta-classification based methods (i.e., MNTD  and MF ). For meta classification based methods that involve training, we have performed 5-fold cross-validation and reported the validation results exclusively. For the 4 inversion-based scanners, we adopt their objective functions as described in Eq. 2 and keep their other designs unchanged. To ensure a fair comparison, we use the same setting for common optimization hyper-parameters for all inversion based methods. We determine the optimal threshold for the size of inverted triggers as the detection rule for. For ABS, we follow its original technique and use REASR as the decision score.

### Detection Performance

**Comparison to Trigger Inversion based Methods.** Table 1 shows the results for trigger inversion based methods, where the first two columns denote the datasets and model architectures, the third column the evaluation metrics, and the subsequent columns the baselines. Additionally, considering that NC , Pixel , and Tabor  need to scan all possible victim-target pairs, which can exceed the time limit, we have employed our warm-up pre-processing technique (Section 3.3). We compare the enhanced versions of these methods (NC\({}^{*}\), Pixel\({}^{*}\), Tabor\({}^{*}\)) with Django. Note that we have not evaluated ABS on DETR models, as the original version is specifically designed for CNN-based models and not transformers.

Django consistently outperforms all the baselines, with the best results are highlighted in bold. The average ROC-AUC of Django is 0.913, while the highest ROC-AUC achieved by the baselines is 0.875, which is the result obtained by ABS on DOTA and F-RCNN. The average ROC-AUC of these baselines is nearly 0.592. Furthermore, our warm-up pre-processing technique has proven effective in enhancing the scanning performance of the baselines, resulting in an improvement of 0.05 to 0.15 in terms of ROC-AUC and a significant reduction in overheads from over 3600 seconds to a range of 275 to 2800 seconds. Despite these advancements, the enhanced versions of the baselines are still unable to surpass the performance of Django, with ROC-AUC gaps ranging from 0.15 to 0.50.

Additionally, we have observed that the overheads incurred by Django are consistently lower than those of NC, Pixel, and Tabor, even when these methods are equipped with our warm-up pre-processing technique. However, ABS generally achieves lower overheads than Django, primarily because ABS only performs trigger inversion for the top-10 target classes according to the original configuration. Nonetheless, it is important to note that Django significantly outperforms ABS in terms of ROC-AUC, which is the more critical metric for our evaluation. We further illustrate the inverted trigger by Django in Figure 6. It is evident that the inverted trigger produced by Django closely resembles the ground-truth injected trigger in terms of both shape and color. The overall success of Django can be attributed to our well-designed warm-up pre-processing and the novel _Gaussian Focus Loss_ 4. These techniques play a crucial role in achieving the superior performance and efficiency demonstrated by Django compared to the baselines.

**Comparison to Meta Classifiers.** Table 2 provides a comparison between meta classifiers and Django. Since meta classifiers require only a few seconds to scan a model, we have omitted the overheads from the table. Observe that baselines achieve decent results in the COCO dataset with over 0.81 ROC-AUC, while only make nearly 0.6 ROC-AUC on other datasets. This discrepancy can be attributed to the fact that there are over 60 models available in the COCO dataset, whereas there are only 30 models in the other datasets. Consequently, the performance of the meta classifiers heavily relies on the availability of a large number of training models. Unfortunate

   D & M & Metric & MNTD & MF & Django \\   &  & Precision & 0.4545 & 0.4000 & 0.8000 \\  & & Recall & 0.3125 & 0.5000 & 1.0000 \\  & & ROC-AUC & 0.3750 & 0.6563 & **0.9160** \\   &  & Precision & 0.5652 & 0.5000 & 0.8571 \\  & & Recall & 0.8125 & 0.1250 & 0.7500 \\  & & ROC-AUC & 0.7695 & 0.6797 & **0.8750** \\   &  & Precision & 0.1250 & 0.1250 & 0.8750 \\  & & Recall & 0.1250 & 0.1250 & 0.8750 \\  & & ROC-AUC & 0.1719 & 0.2890 & **0.9160** \\   &  & Precision & 0.3333 & 0.3333 & 1.0000 \\  & & Recall & 0.1250 & 0.5000 & 1.0000 \\  & & ROC-AUC & 0.3906 & 0.4375 & **1.0000** \\   &  & Precision & 1.0000 & 0.3333 & 0.8000 \\  & & Recall & 0.5000 & 0.5000 & 1.0000 \\  & & ROC-AUC & 0.6562 & 0.6875 & **0.9160** \\   &  & Precision & 0.8182 & 0.7000 & 0.9969 \\  & & Recall & 0.5625 & 1.0000 & 0.8888 \\   & & ROC-AUC & 0.8144 & 0.8163 & **0.9305** \\   

Table 2: Comparison to meta classifierssignificant number of models in real-world scenarios is a challenging task. In contrast, Django does not require access to training models. Moreover, it achieves a ROC-AUC of over 0.91, outperforming the meta classifiers even when they are trained on a sufficient number of models in the COCO dataset. This highlights the effectiveness and robustness of Django in detecting backdoor attacks, surpassing the performance of meta classifiers.

**Evaluation on Advanced Object Detection Backdoor Attacks.** We also evaluate Django on two advanced object detection backdoor attacks [3; 5; 28]. Django achieves 0.8 and 0.9 ROC-AUC on detecting these attacks. Appendix E presents more discussion.

### Evaluation of Label Proposal Pre-processing

In this section, we evaluate the effectiveness of our label proposal pre-processing introduced in Section 3.3. Results are presented in Figure 7. As shown in Figure 7(a), where the x-axis denotes the dataset, the left y-axis denotes the average number of pairs and the right one the TPR (True Positive Rate) of the selected pairs. In the case of an attacked model, if the selected pairs after pre-processing contain the ground-truth victim-target pair, it is considered a true positive. TPR is calculated as the ratio of true positives to all positives. The green bars represent the average number of pairs without pre-processing, while the red bars represent the number of pairs after pre-processing. We observe that across all datasets, our pre-processing technique significantly reduces the number of scanning pairs by 83% to 98%. The curves represent the TPR, showing that our pre-processing results in TPRs of over 88% across different datasets. These results indicate that our pre-processing technique effectively reduces the number of pairs to a remarkably low level while maintaining a high detection rate. The experiment is conducted by selecting various values of \(h\) ranging from 1 to 10 (representing the top \(h\) class labels) and \(\) ranging from 0.1 to 0.8 (representing the frequency threshold). We record the average number of pairs and true positive rate (TPR) under this configuration. The results are depicted in Figure 7(b). we observe that SSD and DETR models require fewer than 100 pairs to reach high TPR, while Faster-RCNN models require around 200 pairs. One potential reason for this difference is that Faster-RCNN requires a two-stage training process where the backdoor signal is less prominent compared to single-stage training methods in SSD and DETR.

Figure 6: Visual similarity between GT and Django inverted triggers.

Figure 7: Evaluation of label proposal pre-processing

### Ablation Study

Our ablation experiments are conducted on the synthesized traffic sign dataset with 3 different model architectures. We include 8 models for each architecture, with half clean and half attacked.

There are two key designs of Django to ensure the detection effectiveness, i.e., GF Loss and Pre-Processing. We conduct ablation study to assess the contribution of these components respectively. Results are shown in Table 3, where each row corresponds to a specific ablation method. The results clearly demonstrate the critical importance of both GF Loss and Pre-Processing in Django. Without these components, the detection performance of Django significantly degrades by more than 0.13 ROC-AUC. Pre-Processing also plays a crucial role in reducing the detection overhead. Please refer to sec D for hyper-parameter sensitivity analysis.

### Adaptive Attack

In this section, we evaluate the performance of Django in the context of adaptive attacks. The effectiveness of Django relies on the confidence levels assigned to the potential boxes, as these determine the coefficients used for trigger inversion. Consequently, an adaptive attack strategy could involve reducing the confidence of objects containing the trigger, thereby potentially causing Django to assign lower coefficients to these poisoned boxes. To evaluate the adaptive attack scenario, we conduct experiments using a 5-class synthesized traffic sign dataset with the SSD model. We train several models with misclassification attacks, where the victim class is to 0 and the target class 4. In order to simulate low confidence attacks, we reduced the ground-truth confidence of the target class to values of 0.8, 0.6, and 0.51, while setting the confidence of the victim class to 0.2, 0.4, and 0.49, respectively. After sufficient training, we applied Django to these models. The results are presented in Table 4, where we recorded the clean mAP and the ASRs at different score thresholds. For instance, ASR_0.1 indicates the ASR when the prediction confidence is higher than 0.1. From the results, we can see that low confidence attacks lead to lower ASRs, especially when the score threshold for measuring the ASR is set high. Furthermore, Django is successful in detecting attacked models with target confidences of 0.8 and 0.6, but fails when the target confidence is 0.51 (threshold set at 0.8). These results suggest that Django is effective against most low-confidence attacks, but its performance may degrade to some extent when the target confidence is very low.

## 5 Limitation

Our primary focus in this paper is on attacks that use static polygon triggers, which are more feasible in real-world scenarios. How to effectively inject more complex triggers [42; 9; 48] in object detection models is still an open question. We leave it to future work.

## 6 Conclusion

In this paper, we present Django, the first trigger inversion framework on detecting backdoors in object detection models. It is based on a novel _Gaussian Focus Loss_ to tackle the loss misalignment issue in object detection trigger inversion. Extensive experiments demonstrate the effectiveness and efficiency of Django compared to existing baselines.

  
**Attack Conf.** & **Clean mAP** & **ASR\_0.1** & **ASR\_0.6** & **ASR\_0.8** & **Django** \\  No Attack & 0.8766 & 0.0047 & 0.0023 & 0.0000 & 0.0 \\ 
0.80 & 0.8700 & 1.0000 & 1.0000 & 0.9812 & 1.0 \\
0.60 & 0.8708 & 1.0000 & 1.0000 & 0.4014 & 1.0 \\
0.51 & 0.8685 & 1.0000 & 0.5728 & 0.0446 & 0.6 \\   

Table 4: Adaptive attack

    &  &  &  \\   & ROC-AUC & Overhead(s) & ROC-AUC & Overhead(s) & ROC-AUC & Overhead(s) \\ 
**Django** & **1.0000** & **859.59** & **0.8750** & **2120.10** & **0.8750** & 227.55 \\
**Django** - GF Loss & 0.7500 & 860.24 & 0.7500 & 2135.59 & 0.7500 & **226.88** \\
**Django** - Pre-Processing & 0.6250 & 3600 & 0.5000 & 3600 & 0.6250 & 3600 \\   

Table 3: Ablation studyAcknowledgment

We thank the anonymous reviewers for their constructive comments. We are grateful to the Center for AI Safety for providing computational resources. This research was supported, in part by IARPA TrojAI W911NF-19-S-0012, NSF 1901242 and 1910300, ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors.