# Multi-times Monte Carlo Rendering for

Inter-reflection Reconstruction

 Tengjie Zhu &Zhuo Chen &Jingnan Gao &Yichao Yan &Xiaokang Yang

GoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University

{zhutengjie, ningci5252, gjn0310, yanyichao, xkyang}@sjtu.edu.cn

Equal contribution.Corresponding author.

###### Abstract

Inverse rendering methods have achieved remarkable performance in reconstructing high-fidelity 3D objects with disentangled geometries, materials, and environmental light. However, they still face huge challenges in reflective surface reconstruction. Although recent methods model the light trace to learn specularity, the ignorance of indirect illumination makes it hard to handle inter-reflections among multiple smooth objects. In this work, we propose Ref-MC\({}^{2}\) that introduces the multi-times Monte Carlo sampling which comprehensively computes the environmental illumination and meanwhile considers the reflective light from object surfaces. To address the computation challenge as the times of Monte Carlo sampling grow, we propose a specularity-adaptive sampling strategy, significantly reducing the computational complexity. Besides the computational resource, higher geometry accuracy is also required because geometric errors accumulate multiple times. Therefore, we further introduce a reflection-aware surface model to initialize the geometry and refine it during inverse rendering. We construct a challenging dataset containing scenes with multiple objects and inter-reflections. Experiments show that our method outperforms other inverse rendering methods on various object groups. We also show downstream applications, \(e\)._g_., relighting and material editing, to illustrate the disentanglement ability of our method. Our project page: https://zhutengjie.github.io/Ref-MC2/.

## 1 Introduction

Neural Radiance Fields (NeRF)  and 3DGS  have demonstrated their excellent performance on novel view synthesis. However, it is difficult to directly apply their reconstructed 3D model to the current industrial pipeline, leading to the lack of flexibility in many downstream applications, \(e\)._g_., relighting and material editing. To better cooperate with mature techniques, inverse rendering bases the physical rendering  and utilizes the neural network to learn disentangled materials that can be seamlessly plunged into the industrial pipeline for further manipulation.

Previous methods  have explored how to disentangle geometry, diffuse, roughness, metalness, and environmental light from multi-view images, but they still face challenges in shading and reflective objects. Specifically, Nvdiffrec  proposes a differentiable pipeline that enables gradient-based optimization on both meshes and volumetric textures. These methods ignore the shadow when modeling the illumination, resulting in failed disentanglement for diffuse and shading appearances. To model more realistic shadings, Nvdiffrecmc  further incorporates ray tracing and Monte Carlo sampling  into inverse rendering, significantly improving the decomposition of shape, materials, and lighting. However, it ignores the indirect illuminations during path tracingand treats rays attacking object surfaces as sheltered illumination. As a consequence, methods like Nvdiffrecmc increase the ambiguity of geometry reconstruction and undermine material learning. Unfortunately, the scene with multiple inter-reflections is common in the real world. The failure in these scenes hinders the wider applications of these methods. Recent methods [17; 40; 22] take inter-reflections into account and can reconstruct the reflective objects well. However, the implicit representations of these methods for materials and renders sacrifice their scalability to downstream tasks. Recently, Nefii  further incorporated the implicit neural radiance to estimate ray tracing, alleviating the ambiguity between materials and indirect illuminations. However, this leads to a huge computational consumption for path tracing. Neural Microfacet Fields  employs two-bounce sampling for indirect light modeling, but its geometry field limits flexibility for downstream tasks.

In this paper, we proposed a full inverse rendering method, Ref-MC\({}^{2}\), which considers inter-reflections during ray tracing to improve the decomposition of explicit materials and environmental lighting. The core of our method is to use **Multi-times Monte Carlo integration** and **BRDF** rendering to approximate the indirect illumination at multiple reflection points along the light propagation path. Although our method takes advantage of hardware-accelerated ray tracing to model the indirect illumination, we still face two major challenges brought by the multi-times Monte Carlo sampling. 1) **Efficiency**: the explosive computational growth from multiple times is too heavy for the hardware algorithm only. 2) **Geometry**: the geometry quality greatly affects the calculation of indirect light and the decomposition of the materials, because the error will accumulate over and over again as the times of Monte Carlo sampling increase.

To address these challenges, we correspondingly propose two strategies. 1) For efficiency, we flatten the multi-times sampling into sequential single-time sampling. In a Lambert model , the diffuse light is independent of the direction of reflection and thus can be presented as a map to query at any time. When we trace the indirect light from an object, instead of recalculating the diffuse component, we can take the value directly from the diffuse map. It can be optimized through self-supervision. Therefore, we only need to sample the specular component within a small lobe along around the reflective direction. 2) For geometry, we refer to the SDF-based methods [41; 17; 6] and replace the common positional encoding with the Sphere Gaussian encoding to get an accurate initial geometry for reflective objects. We use this geometry to initialize Flxicubes  that optimizes surface meshes based on the gradient, and further fine-tunes the Flexicubes in a differentiable pipeline. To evaluate our framework, we construct a dataset containing difficult scenes which contain strong inter-reflections between multiple surfaces. Extensive experiments demonstrate that our framework can successfully decompose indirect illumination and materials. In summary, our contributions are:

* We propose a full inverse rendering method that employs multi-times Monte Carlo sampling to correctly decompose indirect illumination and materials.
* We reduce the computational consumption when tracing indirect illumination by self-supervising the diffuse map based on the Lambert model.
* We refine the SDF-based architecture with Spherical Gaussian encoding to obtain a high-quality initial geometry which further releases the accumulated error during multi-times sampling.
* We construct a dataset to evaluate the performance on indirect illumination.

## 2 Related Work

### Implicit Neural Representations

Neural implicit representations [23; 26; 32; 55; 44; 34; 10; 3; 4; 58; 36; 7] have achieved impressive success in many computer vision and computer graphics tasks. These methods use neural radiance fields to capture color and volume density, generating photo-realistic novel views through volume rendering . However, the unconstrained volumetric representation of the original NeRF method leads to low-quality geometry. Recent 3D Gaussian Splatting (3DGS) [16; 11; 12; 20; 53] has gained popularity in novel view synthesis. Different from NeRF, it is an explicit representation that involves the optimization of multiple Gaussians to reconstruct 3D objects. 3DGS learns color and density in a volumetric point cloud, but it also fails to produce accurate geometry due to its discrete representation. Besides, these methods are all entangled learning, integrating all inherent materials and the environment map into the appearance. This limits the downstream applications in the current industrial pipeline. To produce a high-quality geometry, follow-up works [48; 43; 51; 30] use a function to associate signed distance field (SDF) and volume density. The surface mesh can be extracted from neural implicit surfaces by Marching Cubes , and this 3D asset can be further applied in other applications. However, they usually perform badly in reconstructing the reflective objects due to the ambiguity of reflective appearance.

### Neural Inverse Rendering

Although neural implicit surfaces have achieved impressive performance in geometry reconstructing and novel views synthesis, they do not obtain the fundamental materials of PBR which limits their flexibility in downstream tasks. Neural inverse rendering methods [17; 46; 8; 2; 54; 57; 13; 38; 21] introduce the physical rendering equation to estimate the disentangled diffuse and specular component from RGB images. They approximate the rendering equation based on neural networks or basis functions, _e.g._, Spherical Gaussians [42; 47; 49; 54] and Spherical Harmonics [5; 1; 37; 52]. Nvdiffrec  introduces the **full inverse rendering** that estimates shape, materials, and environmental light into gradient-based optimization. However, it does not consider the shadows, leading to the entanglement of materials. Recent works [6; 41] extend SDF-based architectures with an additional appearance branch to model the reflections on the object. RefNeuS  introduces a reparametrization method to distinguish the reflective appearance from the diffuse appearance by a direction-relative process, but it fails to faithfully reconstruct non-reflective objects. UniSDF  proposes to use a weight-MLP to balance the reflective and non-reflective branches for different objects. However, these methods still face challenges in scenes with complex inter-reflections. Nvdiffrecmc  extends Nvdiffrec with Monte Carlo sampling to trace the light path but still ignores indirect illumination between objects. Further methods [17; 18; 45] consider indirect illumination in their design. ENVIDER  employs a neural renderer to learn the physical light interaction, without explicitly formulating the rendering equation. NeRO  applies the split-sum approximation to approximate the shading effects of both direct and indirect lights. Nefii  introduces ray tracing to the radiance field to model indirect illuminations. Neural Microfacet Fields  employs two-bounce sampling to accurately calculate indirect illumination. However, NMF represents geometry by a density field, which limits the scalability and flexibility for downstream tasks. In addition, these aforementioned methods do not fully disentangle the materials from RGB images, but decompose the appearance into reflective and diffuse color. It is hard to apply to the current industrial pipeline directly.eq: rendering equation In contrast, our Ref-MC\({}^{2}\) is a full inverse rendering method that also considers indirect illumination.

## 3 Method

### Preliminaries

The rendering equation  is commonly used to compute the outgoing radiance \(L_{o}\)(\(\), \(_{o}\)) from the point \(\) in outgoing direction \(_{o}\):

\[L_{o}(,_{o})=_{}L_{i}( ,_{i})f(,_{i},_{o})( _{i})d_{i},\] (1)

where \(L_{i}\)(\(\), \(_{i}\)) is the incoming radiance into \(\) from the direction \(_{i}\), \(\) is the normal of the point \(\), \(\) is the hemisphere of directions above \(\), \(f(,_{i},_{o})\) is the BSDF evaluated for \(_{i}\) and the current incoming direction \(_{i}\). The GGX  physics-based BSDF function is proposed to decompose the function into several physical terms. The function is described as:

\[f(,_{i},_{o})=f_{d }+f_{s}=f_{d}+_{i}) (_{o})},\] (2)

where \(f_{d}\) is the diffuse term and \(f_{s}\) is the specular term. \(D\), \(F\), and \(G\) are the microfacet distribution function, the Fresnel reflection coefficient, and the geometric attenuation, respectively.

Previous works  use the split sum function  to approximate the rendering equation, but it inevitably omits the shadow and indirect illumination. In contrast, Monte Carlo integration  is a simple but unbiased estimation method that comprehensively considers all the physical terms for the outgoing radiance. The Monte Carlo integration rendering equation is:

\[L_{o}(,_{o})_{ i=1}^{N}(,_{i})f( ,_{i},_{o})( _{i})}{p(_{i} )},\] (3)where \(_{i}\) is the \(i\)th sample drawn from density \(p\). As the number of samples grows, the estimation variance reduces, but the computation increases. Multiple importance sampling  (MIS) is proposed to inhibit the computational consumption. When a function can be expressed as a multiplication of \(n\) functions, it draws \(n_{i}\) samples \(_{i,j}\) from \(n\) sampling distributions \(p_{i}\) in turn. The MIS Monte Carlo estimator for the rendering equation is:

\[L_{o}(,_{o})=_{i=1}^{n}} _{j=1}^{n_{i}}W_{i}(,_{o},_{i, j})}{p_{i}},\] (4)

where \(p_{i}\) the \(i\)-th multiplication function of the integrated function of the rendering equation and \(W_{i}\) is the the balance heuristic weighting function.

### Multi-times Monte Carlo Sampling

As discussed in Sec. 3.1, the split sum approximation ignores the shadows and the indirect illumination. Nvdiffrecmc notes this and accounts for shadows using the Monte Carlo integration method. However, they give up continuously tracing the light rays attacking the surface of objects and treat them as zero illumination. It leads to bad performance in scenes with inter-reflections. Therefore, we propose the Ref-MC\({}^{2}\) method to trace the light rays continuously. When taking into account indirect illumination, based on , the rendering equation can be expressed in this version:

\[L_{o}(,_{o})=_{}L_{i}(r( ,_{i}),-_{i})f(, _{i},_{o})(_{i} )d_{i}.\] (5)

In this equation, \(r(,_{i})\) represents the location of a surface point on the object surface hit by a ray cast from \(\) in direction \(_{i}\) for the first time. The corresponding Monte Carlo integration that considers the indirect illumination can then be expressed as:

\[L_{o}(,_{o})_{i=1}^{N} (r(,_{i}),-_{i} )f(,_{i},_{o})(_{i})}{p(_{i})}.\] (6)

It can be interpreted as that when the sampling ray is not blocked, it can be seen as direct illumination from environmental lighting. When the surface point of an object blocks the sampling ray, it needs to be continuously traced from this surface point. This continuous tracing is an iterated process of Monte Carlo integration. It is noteworthy that when continuous sampling at the blocking surface point it is

Figure 1: We perform Monte Carlo sampling at the viewpoint. When the sampling ray from the point is not blocked, it is the direct illumination from environmental lighting. When the sampling ray hits an object, we divide this indirect illumination from the object into diffuse light and specular light. We sample the diffuse light from a diffuse map that is optimized through self-supervision. For specular light, we only need to partially trace the rays in a small specular lobe along the reflective direction. The gradients are backward along the tracing path, and are passed to optimize \(_{d},_{orm}\), normals, and environment maps.

unnecessary to consider the indirect illumination like calculating the outgoing radiance at initial point \(\). This is because the energy of light gradually degrades and the impact of light after twice reflections is negligible compared to the explosively increased computational load. However, the additional computational load of ray tracing at a depth of two is still huge. To reduce our computational load, we further propose to approximate the diffuse light and transfer the computations.

In Disney PBR equation , the diffuse term \(f_{d}\) is:

\[f_{d}=}}{}(1+(F_{D90}-1)(1-( _{i}))^{5})(1+(F_{D90}- 1)(1-(_{o}))^{5}),\] (7)

\[F_{D90}=0.5+2r^{2}_{d},\] (8)

where \(c_{}\) is the diffuse albedo of the material. In this equation, \(f\) is related to the direction of the incoming radiance, which can yield more realistic results. However, using this equation to calculate the diffuse light for indirect illumination creates unnecessary extra computation. Because the term \(f_{d}\) is related to the normal \(n\) and the direction of rays \(_{o}\), we cannot get the diffuse map before deep ray tracing. The following term is to introduce the roughness for diffuse light to avoid too dark edges in extremely low grazing angles. However, using this equation to compute the diffuse light for indirect illumination introduces unnecessary extra computation. To reduce the burden, we can follow H and approximate it using Lambertian diffuse lighting.:

\[f_{d}^{}=}}{}.\] (9)

Where \(f_{d}^{}\) is the diffuse light of the indirect illumination. In this approximation, \(f_{d}^{}\) is independent on the incoming direction \(_{i}\) and the outgoing direction \(_{o}\). Based on this, the diffuse part of indirect light can be simplified as:

\[L_{}^{}()=}}{ }_{}L_{i}(,_{i})( _{i})d_{i},\] (10)

where \(L_{}^{}\) is the diffuse part of the indirect illumination. It no longer relates to the direction of the sampling ray, so we can present it via an MLP \(M\). Compared to the diffuse part, the specular part is strongly related to the ray direction. It is a disadvantage that we cannot approximate the specular part like the diffuse part, but it is also an advantage that we only need to sample minor rays in a small specular lobe along the reflective directions. Therefore, we significantly reduce the computations of both the diffuse part and the specular part during the multi-times Monte Carlo Sampling.

### Geometry Initialization

As seen in the equation 5, our indirect lighting highly depends on the geometry. However, the strong ambiguity of reflections makes it hard to directly learn high-quality disentangled shapes. Therefore, we flatten the joint learning process of both geometry and materials into separate learning of them and thus reduce the number of physical terms to be disentangled in each learning stage.

We first utilize SDF-based architectures [48; 6; 18] to learn an initial geometry. The two branches of the diffuse and reflective networks well disambiguate the appearance with reflections and empower the

Figure 2: **Differences between previous SDF architectures and the architecture for inter-reflections.**

SDF network to produce a high-quality geometry, as shown in Fig. 2 (a). However, this architecture encounters challenges of indirect illumination due to the expressive capacity of Integrated Positional Encoding (IPE). It performs well in general scenes but shows limitations in representing interferences. Due to the capacity of Spheical Gaussians (SG) [50; 33] to represent radiance directions, we introduce SG encoding instead of IPE to enhance the expressive capacity of reflective MLP, as shown in Fig. 2 (b). Besides, we directly parameterize diffuse appearance as SG coefficients which is suitable for objects with multiple reflective surfaces. The geometry comparison is shown in Fig. 2.

The learned SDF can be then converted into a surface mesh using the Marching Cubes , which supports further tuning in the following inverse rendering pipeline. Different from Nvdiffrecmc which adopts DMTET for differentiable mesh optimization, we introduce Flexicucubes  into our inverse rendering pipeline. Flexicubes use the SDF, weight, and the deformation for vertexes of the grid cells to extract the surface mesh using the DMC . The mesh is converted into a differentiable representation that can be optimized based on gradients.

### Training Objectives

The main objective is to minimize the photometric loss between rendered images and ground truth:

\[_{rgb}=||C-C_{gt}||^{2},\] (11)

where \(C\) is the rendering result, and \(C_{gt}\) is the corresponding ground truth. Following previous work [56; 28; 9], we adopt a smoothness loss for diffuse and material as regularization:

\[_{d}=_{_{}}|_{d}(_{})-_{d}(_{}\,+ )|,\] (12)

\[_{orm}=_{_{}}|_{orm}( _{}\,)-_{orm}(_{} \,+)|,\] (13)

where \(_{}\) represents the world coordinates of points on the object's surface. \(_{d}(_{}\,)\) and \(_{orm}(_{}\,)\) is the material of this point. \(\) is a randomly distributed vector of tiny deformations. A self-supervised loss is used to regulate the learned diffuse color:

\[_{}=_{rgb}(_{}, _{}(_{})),\] (14)

where \(_{}\) is the diffuse light from the object surface obtained during rendering. \(_{}(_{})\) is the diffuse light obtained by the MLP. Overall, the full loss function is:

\[=_{rgb}+_{1}_{d}+_{2}_{orm}+_{3}_{},\] (15)

where \(_{1}\), \(_{2}\) and \(_{3}\) are three predefined scalars.

## 4 Experiments

### Implementation Details

**Dataset.** We construct a dataset of multiple reflective objects based on the existing single objects to evaluate the performance. Our dataset consists of 16 groups of object compositions, most of which contain indirect illumination between reflective objects. We render the composed objects with various environmental lighting in the Blender engine. Each group contains 300 images, with 200 for the training set and 100 for the test set.

**Experiment setup.** We optimize the 3D model on 1 RTX 3090 GPU with 24G memory. We use the Adam optimizer for the material and the environment map with an initial learning rate of 0.03. The coefficients of loss function \(_{1}\), \(_{2}\), and \(_{3}\) are set to 0.1, 0.05, and 1, respectively. The rate of Monte Carlo sampling is commonly set to 128 consistent with the setting in Nvdiffrecmc .

### Comparison with Baseline

In this section, we compare our method with Nvdiffrec , Nvdiffrecmc , and Nefii  on our constructed reflective dataset, and the results are shown in Fig. 3. Nvdiffrec achieves photo-realistic results in the _metal balls_ but fails on the smooth and glossy _table + horse_. Besides, Nvdiffrec cannot well disentangle the materials because it does not consider the shading. In contrast, Nvdiffrecmc performs well in material learning, while its rendering results are bad. These two methods also suffer from indirect illumination from the reflection of inner objects, leading to low-quality environment maps. Nefii is the recent work that considers the indirect illumination in the radiance field, but it tends to produce low-reflective results and performs badly in these high specular objects. Compared to these methods, our method can achieve both photo-realistic rendering results and well-disentangled material learning. We can handle highly specular objects, _e.g._, the metal table. The environment maps learned by our methods are also superior to others.

   Method & NDR & NDRMC & Nefii & **Ours** & Ours & Ours & Ours & Ours \\  &  &  &  & **Ours** & (w/o Acc.) & (w/o Geo.) & (3) & (1) \\  PSNR\(\) & 26.9 & 25.7 & 22.3 & 28.1 & 28.0 & 24.6 & 28.4 & 27.3 \\ Training time\(\) & 30min & 45min & 20h & 6.5h & 11.5h & 5h & 26.5h & 2.25h \\   

Table 1: **Quantitative Comparisons**. Ours (\(x\)) means the \(x\) times of sampling in our method. Ours (1) is about 1.5h longer than Nvdiffrecmc  as we need additional time to learn the initial geometry.

Figure 3: **Qualitative comparison**. The results of renderings, materials, and environment maps are presented. Note that, the material of Nefii contains only roughness without metalness. Our method achieves the best renderings with clear reflections, compared to other inverse rendering methods. Our method is also superior to others in the disentanglement of materials and environment maps.

### Multi-times Monte Carlo Sampling

In this section, we conduct an ablation study on the times of Monte Carlo Sampling, _i.e_.the depth of ray tracing. Multi-times sampling considers the indirect illumination, and thus successfully disentangles the environment light and inner reflective light. As shown in Fig. 4, the environment

Figure 4: **Ablation study** on the depth of ray tracing, _i.e_., the times of Monte Carlo sampling. The results with depth=1 show fewer and darker reflections compared to the ground truth. \(k_{d}\) maps also illustrate the limited capacity to disentangle the material from environmental light, for example, mistaking the diffuse color of the table as the color of the sky. In contrast, with depth=2 or 3, results show more realistic renderings and disentangled materials.

Figure 5: **Ablation study** on geometric initialization. As shown, a better-quality geometry can significantly improve the material learning and also refine the rendering results.

maps learned by single-time sampling contain noise and shades that are actually the inner reflection. In contrast, multi-times sampling significantly releases the problem, producing a clearer environment map. Single-time sampling also undermines the rendering results, with darker inverted reflections. When the sampling ray is sheltered by objects during path tracing, the ray returns no light, resulting in a dark point. Multi-times sampling returns the reflective light of the sheltering points containing both diffuse and specular light. As a consequence, the rendering results of multi-times sampling are realistic appearance and reflections. Besides, it can be seen that results with 2-time sampling are comparable to results with 3-time sampling. Tab. 1 also quantitatively shows minimal improvement by deeper tracing. However, each additional sampling time induces a 4-times computation increment. Therefore, a 2-times sampling is a more cost-effective setting. Additionally, we also compare the efficiency between the sampling with and without acceleration. As shown in Tab. 1, our method can make the training speed twice as fast while keeping a comparable PSNR. The degree of acceleration is related to the complexity of geometry, and here we adopt the median in our dataset.

### Geometry

In this section, we further explore the necessity of the initial geometry discussed in Sec. 3.3 and present our geometry reconstruction results. As shown in Fig. 5, without initial geometry, the rendering results show bumpy surfaces with badly learned materials in the area of hollow shape. Because multi-times sampling amplifies the errors introduced by geometry, the learned materials are even worse than the single-time sampling. When applying a good-quality initialization, the network bypasses the ambiguity brought by the geometry. It helps our method learn well-disentangled materials and finally produces high-quality rendering results. Quantitative comparison in Tab. 1 also demonstrate the necessity of geometry initialization. In addition, we showcase a real-scene geometry reconstruction result of the NeRO  dataset in Fig. 6. We further use Chamfer Distance to quantitatively evaluate our geometric quality and present the results in Tab. 2.

### Relighting and Material Editing

The disentangled environment map and material empower our methods to relight and edit reconstructed objects in the downstream application. Our method can also be seamlessly plugged into the industrial pipeline. As shown in Fig. 7, our method enables the arbitrary combination of reconstructed objects. We can easily edit their metalness, roughness, and albedo color by manipulating the learned materials. Due to the well-disentangled shading, our rendering results are natural and realistic in all five environment maps, even the point light in a dark environment. They also perform well after editing materials thanks to the well-learned material map. For example, after we increase the metalness and reduce the roughness of the teapot, the teapot clearly reflects the neighboring objects on its surface. In another case, where we extremely increase the reflectance and change the base color of the toaster, it accomplishes to reflect the scene and other objects. To showcase further applications

   Dataset & Nvdiffrec  & Nvdiffrecmc  & NeRO  & Ours \\  Materials & 0.016 & 0.016 & 0.0057 & **0.0030** \\ Coral & 0.28 & 0.25 & 0.13 & **0.13** \\   

Table 2: **Geometry qualitative comparison.We use the Chamfer Distance (\(\)) to evaluate our reconstructed geometry. As shown in the table, we obtain the best results in multiple scenes.**

Figure 6: **Real scene geometry comparison**. We compare our reconstructed geometry with NeRO, Nvdiffrec and Nvdiffrecmc. Our reconstructed geometry demonstrates superior results.

of our method, we conduct experiments on real-world datasets captured by NeRO  and present the comparison results in Fig. 8. It can be seen that Ref-MC\({}^{2}\) achieves more realistic results under different lighting conditions.

## 5 Conclusion and Limitations

In conclusion, our Ref-MC\({}^{2}\) introduces multi-times Monte Carlo sampling into the inverse rendering pipeline to model the indirect illumination. It improves the performance in scenes with complex inter-reflections. However, increment of sampling times significantly increases computational consumption and makes the pipeline highly geometry-sensitive. To solve the challenge of computational efficiency, based on the Lambert model, we simplify the BRDF for indirect lighting, which allows us to reduce the number of ray traces. To improve the geometry quality, we adopt SDF-based architecture to get an initial geometry and refine a design with Spherical Gaussian encoding for reflective objects. We further use Flexicubes to take the initial mesh into the differentiable rendering pipeline that learns disentangled materials. Our Ref-MC\({}^{2}\) still has several limitations. The major limitation is that the 2-time sampling cannot handle extremely reflective objects, \(e\)._g_., mirrors, as the specular energy hardly degrades after reflections. Besides, the training time needs to further reduce in future time.

## 6 Acknowledgements

This work was supported by NSFC (62201342) and Shanghai Science and Technology Major Project (2021SHZDZX0102). We also thank Student Innovation Center of SJTU for providing GPUs.

Figure 8: **Relighting results of real scenes**. Our method demonstrates more realistic results under several lighting conditions than NeRO.

Figure 7: **Relighting and editing**. We compose the reconstructed objects in a unified scene and change the environmental light. The relighting results show our strong ability to disentangle the light and shading. We also perform material edits which shows the flexibility in wide applications.