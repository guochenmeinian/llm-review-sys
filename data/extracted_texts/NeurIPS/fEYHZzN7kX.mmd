# Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective

Jiaxi Hu1, Yuehong Hu1, Wei Chen1, Ming Jin2, Shirui Pan2, Qingsong Wen3, Yuxuan Liang1

Y. Liang is the corresponding author. Email: yuxliang@outlook.com

###### Abstract

In long-term time series forecasting (LTSF) tasks, an increasing number of works have acknowledged that discrete time series originate from continuous dynamic systems and have attempted to model their underlying dynamics. Recognizing the chaotic nature of real-world data, our model, _Attraos_, incorporates chaos theory into LTSF, perceiving real-world time series as low-dimensional observations from unknown high-dimensional chaotic dynamical systems. Under the concept of attractor invariance, Attraos utilizes non-parametric Phase Space Reconstruction embedding along with a novel multi-resolution dynamic memory unit to memorize historical dynamical structures, and evolves by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets with only one-twelfth of the parameters compared to PatchTST. Code is available at https://github.com/CityMind-Lab/NeurIPS24-Attraos.

## 1 Introduction

In the intricate dance of time, time series unfold. Emerged from continuous dynamical systems [36; 11; 39] in the physical world, these series are meticulously collected at specific sampling frequencies. Like musical notes in a composition, they harmonize, revealing patterns that resonate through the symphony of temporal evolution. In this realm, Long-term Time Series Forecasting (LTSF) stands as one of the enduring focal points within the machine learning community, achieving widespread recognition in real-world applications, such as weather forecasting, financial risk assessment, and traffic prediction [28; 22; 30; 27; 18].

Building on the success of various deep LTSF models [49; 54; 52; 50; 29; 22], which primarily leverage neural networks to learn temporal dependencies from discretely sampled data. Currently, researchers [46; 32] have been investigating the application of Koopman theory [48; 25] in recovering continuous dynamical systems, which applies linear evolution operators to analyze dynamical system characteristics in a sufficiently high-dimensional Koopman function space. Nevertheless, the existence of Koopman space relies on the deterministic system, posing challenges given the chaotic nature of real-world time series data, evidenced through the Maximal Lyapunov Exponent in Appendix E.2.

In this paper, inspired by chaos theory , we revisit LSTF tasks from a chaos perspective: Linear or complex nonlinear dynamical systems exhibit stable patterns in their trajectories after sufficient evolution, known as attractors. As illustrated in Figure 1(a), attractors can be classified into fourtypes: Fixed Point, indicating stable, invariant systems; Limited cycle, representing periodic behavior; Limited Toroidal, exhibiting quasi-periodic behavior with non-intersecting rings in a 2D plane, reflecting temporal distribution shifts; and Strange Attractor, characterized by nonlinear behavior and complex, non-topological shapes. Supported by chaos theory, we can transcend the limitations of deterministic dynamical systems to construct generalized dynamical system models. Figure 1(b-c) showcases various classical chaotic dynamical systems and the dynamical trajectories of real-world LTSF datasets using the phase space reconstruction method . Notably, the dynamical system trajectories in these LTSF datasets exhibit fixed structures akin to those in typical chaotic systems.

Given this chaos perspective, we consider real-world time series as stemming from an unidentified high-dimensional underlying chaotic system, broadly encompassing nonlinear behaviors beyond periodicity. Our focus centers on recovering continuous chaotic dynamical systems from discretely sampled data for LTSF tasks, with the goal of predicting future time steps through the lens of attractor invariance. Specifically, this problem can be decomposed into two key questions: _(i) how to model the underlying continuous chaotic dynamical system based on discretely sampled time series data; (ii) how to enhance forecasting performance by utilizing the attractors within the system._

In this context, _Attraos_ emerges with the goal of capturing the underlying order within the seeming chaos via attractors. For tackling the first question, we employ a non-parametric phase space reconstruction method to recover the temporal dynamics and propose a _Multi-resolution Dynamic Memory Unit_ (MDMU) to memorize the structural dynamics within historical sampled data. Specifically, as polynomials have been proven to be universal approximators for dynamical systems , MDMU expands upon the work of the State Space Model (SSM) [12; 10; 18] to different orthogonal polynomial subspaces. This allows for memorizing diverse dynamical structures that encompass various attractor patterns, while theoretically minimizing the boundary of attractor evolution error.

To address the second question, we devise a frequency-enhanced local evolution strategy, which is built upon the recognition that attractor differences are amplified in the frequency domain, as observed in the field of neuroscience [5; 14; 6]. Concretely, for dynamical system components that belong to the same attractor, we apply a consistent evolution operator to derive their future states in the frequency domain. Our contributions can be summarized as follows:

* **A Chaos Lens on LTSF**. We incorporate chaos theory into LTSF tasks by leveraging the concept of attractor invariance, leading to a principal way to model the underlying continuous dynamics
* **Efficient Dynamic Modeling**. Our model Attraos employs a non-parametric embedding to obtain high-dimensional dynamical representations, leverages MDMU to capture the multi-scale dynamical structure, and performs the evolution in the frequency domain. Remarkably, Attraos achieves this with only about one-twelfth the parameter count of PatchTST. Furthermore, we utilize the Blelloch scan algorithm  to enable efficient computation of the MDMU.
* **Empirical Evidence**. Various experiments validate the superior performance of Attraos. Besides Leveraging the properties of chaotic dynamical systems, we explore their extended applications in LTSF tasks, focusing on chaotic evolution, modulation, representation, and reconstruction.

Figure 1: (a): Classical chaotic systems with noise. (b): dynamical system structure of real-world datasets. (c): Different types of Attractors. See more figures in Appendix E.1.

Preliminary

**Attractor in Chaos Theory**. In chaos theory, the interaction of three or more variables exhibiting periodic behavior gives rise to a complex dynamical system characterized by chaos. According to Takens's theorem [43; 34], assuming an ideal dynamical system \(:\) that "lives" on attractor \(\) in manifold space \(\) which locally \(^{N}\) (N-times differentiable), time series data \(\{z_{i}\}\) can be interpreted as the observation of it by an unknown observation function \(h\). To explore the properties of the unknown ideal dynamical system, we can employ the phase space reconstruction (PSR) method to establish an approximation \(:^{m}^{m}\) which lives in differential homomorphism attractor \(}\) in the Euclidean space with suitable dimension \(m\). The whole process is illustrated in Equation (1), where \(\{z_{i}\}\), \(\{u_{i}\}\) are the sampled data from two dynamical systems. Strictly speaking, in our paper, the chaotic attractor structure \(}=\{}_{i}\}\) we focused on is in phase space \(^{m}\). To facilitate understanding, we further provide a visual example of the Lorenz96 system in Appendix E.3.

In the forecasting stage, the local prediction method emerges as a prominent one: \(u_{i+1}=^{(i)}(u_{i})\), where the local evolution \(^{(i)}\) can be either linear or nonlinear neural network [45; 2; 40], with the parameter being shared among the points in the neighborhood of \(u_{i}\) or belong to the same local attractor. Considering the universal approximation capabilities of polynomials for dynamical systems, we leverage the polynomial to describe the chaotic dynamical structures.

\[a_{i} }{} a_{i+1}\] (1) \[_{h} _{h} x^{}(t)=(K u)(t)\] (2a) \[u_{i}}^{m} }{} u_{i+1}}^{m}\] (2c)

**Polynomial Projection with Measure Window**. We only consider the first part of the SSM (2a), which is a parameterized map that transforms the input \(u(t)\) into an \(N\)-dimensional latent space. According to Hippo, it is mathematically equivalent to: given an input \(u(s)\), a set of orthogonal polynomial basis \(_{n}(t,s)\) that \(_{-}^{t}_{m}(t,s)_{n}(t,s)s=_{m,n}\), and an inner product probability measure \((t,s)\). This enables us to project the input \(u(s)\) onto the polynomial basis along time dimension (3), and we can combine \(_{n}(t,s)(t,s)\) as a kernel \(K_{n}(t,s)\) (4). When \((t,s)\) is defined in a time window \([t,t+]\), it represents approximating the input over each window \(\).

\[ u,_{n}_{}=_{-}^{t}u(s)_{n}(t,s)(t,s )s\] (3)

\[x_{n}(t)= u(s)K_{n}(t,s)(t,s)s\] (4)

When the basis and measure are solely dependent on time \(t\), it can be expressed in a convolution form (2b). In this paper, we will utilize this property to project the dynamical trajectories \(\{u_{n}\}\) in phase space onto the polynomial spectral domain with kernel \(e^{t}\) (2c) for characterization.

## 3 Theoretical Analysis & Methods

The overall structure of Attraos is illustrated in Figure 2. In this section, we provide a comprehensive description of its components, including the Phase Space Reconstruction embedding, the Multi-Resolution Dynamic Memory Unit (MDMU), and the frequency-enhanced local evolution, as well as the efficient computational methods employed.

### Phase Space Reconstruction

According to chaos theory, the initial step involves constructing a topologically equivalent dynamical structure through the PSR. The preferred embedding method is typically the Coordinate Delay Reconstruction , which does not rely on any prior knowledge of the underlying dynamical system. By utilizing the discretly sampled data \(\{z_{i}\}\) and incorporating two hyper-parameters, namely, embedding dimension \(m\) and time delay \(\), a high-dimensional dynamical trajectory \(\{u_{i}\}\) in phase space can be constructed by Eq. (5).

\[u_{i}=(z_{i-(m-1)},z_{i-(m-2)},,z_{i})\] (5)

\[_{patch}=\ (,p,p)\] (6)For multivariate time series data with \(C\) variables, we have observed considerable variations in the Lyapunov exponents of each variable, hence a channel-independent strategy  is employed to construct a unified dynamical system \(^{m}\). To accelerate model convergence and reduce complexity, we apply non-overlapping patching to obtain \(_{patch}\) (6). We denote the number of patches as \(L\), using \(u^{B L D}\) to represent the tensor used for computing, where \(D=mp\). The determination of \(m\) and \(\) is achieved by applying the CC method  as shown in Appendix C.1.

**Remark 3.1**.: _This represents the pioneering non-parametric embedding in LTSF, effectively reducing the model parameters in the embedding and output projection process (\(m\) is typically single-digit)._

**Remark 3.2**.: _In a large body of dynamical literature [44; 41; 20; 25], local linear approximation serves as an effective method for modeling dynamical systems, providing a basis for the effectiveness of the patching operations in this paper._

### Dynamical Representation by MDMU

**Proposition 1**.: \(=\{-1,-1,\}\) _is a rough approximation of normal Hippo-LegT  matrix, which utilizes polynomial projection under a finite measure window (Lebesgue measure)._

**Remark 3.3**.: _All proofs in this section can be found in Appendix B._

Adhere to Mamba , we generate \(\) and measure window \(\) by linear layer, and propose a novel parameterized method (Proposition 1) for \(\) to instantiate Eq. (2a):

\[=_{D}(\{-1,-1,\}),= _{}(u),/=( _{}(u)),\] (7)

where \(^{D N}\) represents the dynamical characteristics of the system's forward evolution in the polynomial space. Due to its diagonal nature, its representational capacity is comparable to \(^{D N N}\); Matrix \(^{B L N}\) controls the process of projecting \(u\) onto the polynomial domain like a gate mechanism; The learnable approximation window \(^{B L D}\), similar to an attention mechanism, enables adaptive focus on specific dynamical structures (attractors).

**Remark 3.4**.: _In the Hippo theory, the measure window is denoted by \(\), while in SSMs [10; 12], the discrete step size is represented by \(\). These two terms can be considered approximately equivalent._

As shown in Figure 3(a), in practical computations, we need to discretize Eq. (2a) to fit the discrete dynamical trajectories. We apply zero-order hold (ZOH) discretization  to matrix \(\), while opting for a combination of Forward Euler discretization for \(\) (instead of the commonly used

Figure 2: Overall architecture of Attraos. Initially, the PSR technique is employed to restore the underlying dynamical structures from historical data \(\{z_{i}\}\). Subsequently, the dynamical system trajectory is fed into MDMU, projected onto polynomial space \(_{}^{N}\) using a time window \(\) and polynomial order \(N\). Gradually, a hierarchical projection is performed to obtain more macroscopic memories of the dynamical system structure. Finally, local evolution operator \(^{(i)}\) in the frequency domain is employed to obtain future state, thereby for the prediction.

\[}=()^{-1}(()-) { in SSMs}),\] \[():}=(), (}):}=.\] (8)

Next, we can project the dynamical trajectory \(u\) onto the polynomial domain using the discretized kernel to obtain dynamical representation \(x^{B L D N}\) (9). This process can be achieved with \((L)\) complexity by employing sequential computation (Figure 3(c)) or by utilizing the Blelloch scan (Figure 3(d)) to store intermediate results with \((logL)\) complexity.

\[}=(},},, }^{L-1}}), x=u}.\] (9)

Up to this point, the construction of the underlying continuous dynamical system \(\) and its application to discretely sampled data \(z_{n}\) have been established. However, in this scenario, we are still limited to a single representation of the dynamical structures with measure window \(\). The strange attractors, on the other hand, are often composed of multiple fundamental topological structures. Therefore, we require a multi-scale hierarchical representation of dynamical structures to capture their complexity.

To address this, as illustrated in Figure 3(b), we progressively increase the length of the window \(\) by powers of 2. The region previously approximated by \(g^{_{1}}_{}^{N}\) (left half) and \(g^{_{2}}_{}^{N}\) (right half) will now be approximated by \(g^{2}_{2}^{N}\).

Since the piecewise polynomial function space can be defined as the following form:

\[_{(r)}^{k}=g(g)<k,&x(2^{-r}l,2^{-r}(l+1))\\ 0,&,\] (10)

with polynomial order \(k\), piecewise scale \(r^{+}\{0\}\), and piecewise internal index \(l\{0,1,...,2^{r}-1\}\), it is evident that \(dim(_{(r)}^{k})=2^{r}k\), implying that \(_{}^{k}\) possesses a superior function capacity compared to \(_{2}^{k}\). All functions in \(_{2}^{k}\) are encompassed within the domain of \(_{}^{k}\). Moreover, since \(_{}^{k}\) and \(_{2}^{k}\) can be represented as space spanned by basis functions \(\{_{i}^{}(x)\}\) and \(\{_{i}^{2}(x)\}\), any function including the basis function within the \(_{2}^{k}\) space can be precisely expressed as a linear combination of basis functions from the \(_{}^{k}\) space with a proper tilted measure \(_{2}\):

\[_{i}^{2}(x)=_{j=0}^{k-1}H_{ij}^{_{j}}_{i}^{}(x)_ {x[_{1}]}+_{j=0}^{k-1}H_{ij}^{_{2}}_{i}^{}(x)_{ x[_{2}]},\] (11)

Figure 3: (a) Discretization of continuous polynomial approximation for sequence data. \(g\) represents the optimal polynomial constructed from polynomial bases. (b) MDMU projects the dynamical structure onto different orthogonal subspaces \(\) and \(\). (c) Sequential computation for Eq. (2a) in \((L)\) time complexity. (d) Blelloch tree scanning for Eq. (2a) in \((logL)\) by storing intermediate results.

The projection coefficients on these two spaces can be mutually transformed using the linear projection matrix \(H\) and its inverse matrix \(H^{}\) based on the odd or even positions along the \(L\) dimension in \(x\).

\[x^{2}=H^{_{1}}x^{_{1}}+H^{_{2}}x^{_{2}}, x ^{2}^{B L/2 D N}\] (12)

**Remark 3.5**.: _Although \(\) is akin to an attention mechanism leads to different measure windows, ie., \(_{1}_{2}\), it still maintains the linear projection property for up and down projection:\(_{_{1}},_{_{2}}_{ _{1}+_{2}}\). In our illustration, we have used a unified measure window for simplicity._

**Theorem 2**.: _(Approximation Error Bound) Function \(f:\) is \(k\) times continuously differentiable, the piecewise polynomial \(g_{r}^{N}\) approximates \(f\) with mean error bounded as:_

\[\|f-g\| 2^{-rN}N!}_{x}f^{(N)}(x).\]

Iteratively repeating this process enables us to model the dynamical structure from a more macroscopic perspective. Theorem 2 indicates that under this approach, the polynomial projection error has convergence of order \(N\). Hyper-parameter analysis can be found in Figure 5.

**Remark 3.6**.: _When the weight function is uniformly equal across the dynamical structure, \(H^{_{1}}\) and \(H^{_{2}}\) are shared in each projection level as the projection matrix for the left and right interval._

**Theorem 3**.: _The mean **attractor evolution error**\(}-}\) of evolution operator \(=\{^{(i)}\}\) is bounded by \(\|}-}\|(N-1) (-_{i}+1)\), with the number of random patterns \(Nc\) stored in the system by interaction paradigm \(\) in an ideal spherical retrieval paradigm._

Based on this hierarchical projection, we additionally want to uphold the constancy of attractor patterns throughout the evolution, which is equivalent to minimizing attractor evolution errors. According to Theorem 3, it is imperative to ensure the separation between attractors, denoted as \(_{i}:=_{j,j i}(}_{i}^{T}}_{i}-}_{i}^{T}}_{j})\), is sufficiently large. While there is an intersection between the \(_{}^{N}\) and \(_{2}^{N}\) spaces, which limits the attainment of a sufficiently large \(\). To address this issue, we define an orthogonal complement space as \(_{}^{N}=_{}^{N}_{2 }^{N}\), to establish a series of orthogonal function spaces \(\{_{}^{N},_{2}^{N},...,_{2 }^{N},_{2}^{N}\}\). We can extend Eq. (12) as:

\[x^{2}=H^{_{1}}x^{_{1}}+H^{_{2}}x^{_ {2}},\ \ s^{2}=G^{_{1}}x^{_{1}}+G^{_{2}}x^{_{2}},\] (13) \[x^{_{1}}=H^{_{1}}x^{2}+G^{ _{1}}s_{t}^{2},\ \ x^{_{2}}=H^{_{2}}x^{2}+G^{_{2}}s_{t}^ {2}.\] (14)

Theorem 4 states that the coarsest-grained \(\) space, along with a series of orthogonal complement \(\) spaces, can approximate any given dynamical system structure with finite error bounds in Theorem 2.

**Theorem 4**.: _(Completeness in \(L^{2}\) space) The orthonormal system \(B_{N}=\{_{j}:j=1,,N\}\{_{j}^{rl}:j=1,,N;r=0,1,2,;l =0,,2^{r}-1\}\) spans \(L^{2}\)._

**Remark 3.7**.: \(,,^{},^{}^{N N}\) _are obtained by applying Gaussian Quadrature to Legendre polynomials . The gradients of these matrices are subsequently utilized for adaptive optimization._

The hierarchical projection can be implemented explicitly using iterative display or can be efficiently computed through an implicit implementation proposed in the following Section 3.3.

### System Evolution by Attractors

Following chaos theory, we employ a local evolution method \(x_{i+1}=^{(i)}(x_{n})\) to forecast future state. Given dynamics representations \(s/x\), the subsequent step involves partitioning the attractor and utilizing the operator \(^{i}\) belonging to \(A_{i}\) for system evolution. We present three evolution strategies:

* _Direct Evolution:_ We employ each representation \(x_{t}/s_{t}^{D N}\) as the feature to partition adjacent points in the dynamical system trajectory into the same attractor using the K-means method. Subsequently, we evolve these points using a local operator \(^{(i)}\).
* _Frequency-enhanced Evolution:_ Inspired by neuroscience , where attractor structures are amplified in the frequency domain, we first obtain the frequency domain representation of the dynamical structure through Fourier transformation. Considering the dominant modes as attractors, we employ \(^{(i)}\) to drive the system's evolution in the frequency domain.

* _Hopfield Evolution:_ Hopfield networks  are designed specifically for attractor memory retrieval (see Appendix A.2). In our approach, we utilize a modern version  of the Hopfield network for the evolution of dynamical systems, employing cross-attention operations. We treat the trainable attractor library as the _Key_ and _Value_, while different scales of dynamical structure representations \(\{s^{},s^{2},...,s^{2^{L}},x^{2^{L}}\}\) serve as the _Query_, enabling sequence-to-sequence evolution.

Our experimental results in Table 4 demonstrate that the frequency-enhanced evolution strategy outperforms others comprehensively, and we introduce two implementation approaches:

**Explicit Evolution**. Initially, the finest dynamical representation \(x^{}\) is obtained using \(}\), and then expanded to multiple scales \(s^{},s^{2},...,s^{2^{L}},x^{2^{L}}\). By applying the Fourier Transform, we select \(M\) low-frequency components as the primary modes. Each mode undergoes linear evolution \(^{(i)}=W_{i}^{N N}\), followed by back projection to the original scale.

**Implicit Evolution**. However, explicit evolution methods inevitably increase the time complexity. To address this issue, inspired by the Blelloch algorithm and hierarchical projection, which both utilize _tree-like computational graphs_, we propose an implicit evolution method.

A Blelloch scan [3; 42] (Figure 3(d)) defines a binary operator \(q_{i} q_{j}:=(q_{j,a} q_{i,a},\ q_{j,a} q_{i,b}+q_{j,b})\) used to compute the linear recurrence \(x_{k}=}x_{k-1}+}u_{k}\). We take \(L=4\) as an example:

**Up sweep for even position**

\[r_{2}=c_{1} c_{2} =(},}}u_{1} )(},}}u_{2} )=(}^{2},}}}}u_{1}+}}u_{2})\ r_{1}=r_{0} c _{1}=(I,0)(},}}u_{1})=(},}}u_{1})\] \[q_{4}=c_{3} c_{4} =(},}}u_{3} )(},}}u_{4} )\ r_{3}=r_{2} c_{3}=(}^{2},}}}}u_{1}+}}u_{2 })(},}}u_{3})\] \[r_{4}=r_{2} q_{4} =(}^{2},}} }}u_{1}+}}u_{2}) (}^{2},}}}}u_{3}+}}u_{4})\] \[=(}^{4},}^{3} }u_{1}+}}^{2}}u_{2}+ }}}}u_{3}+}}u_{4}).\]

The process commences by computing the values of variable \(x\) at even positions through an upward sweep. Subsequently, these even position values are employed during a downward sweep to calculate the values at odd positions. The corresponding value of \(x\) resides in the right node of \(r\). Thus, we can modify the binary operator as \(q_{i} q_{j}:=(q_{j,a} q_{i,a},\ H_{i}(q_{j,a} q_{i,b }+q_{j,b}))\), thereby implicitly integrating hierarchical projection into the scanning operation. This leads to the scales of \(x_{i}\):

\[scale(x_{i})=0&i=0\\ scale(x_{i-1})+1&i\\ _{2}(i)&i\\ 1&i\]

We simplify the original \(^{_{1}/_{2}},^{_{1}/_{2}},^{ _{1}/_{2}},^{_{1}/_{2}}\) with just \(^{B L N}\), which is generated directly through a linear layer, and omit the reconstruction process. This approach directly sparsifies the kernels \(e^{t}\) in different subspaces and using the linear layer to generate hierarchical space projection matrix. Afterwards, we learn the evolution using the data \(x\) in the frequency domain.

\[=_{}(u),_{out}=_{_ {out}}(u).\] (15)

In this paper, we utilize this indirect and efficient hierarchical projection as the default setting. Ultimately, Attraos projects from the polynomial spectral space back to the phase space by employing another gating projection \(_{out}^{B L N 1}\) (15) to \(x^{B L D N}\), and derives the prediction results using an observation function parameterized by \(_{h}^{LD H}\) (flattening the patches).

## 4 Experiments

In this section, we commence by conducting a comprehensive performance comparison of Attraos against other state-of-the-art models in seven mainstream LTSF datasets along with two typical chaotic datasets, namely Lorenz96-3d and Air-Convection, followed by ablation experiments pertaining to model architectures. Furthermore, leveraging the properties of chaotic dynamical systems, we explore their extended applications, including experiments on chaotic evolution, representation, reconstruction, and modulation (Appendix E). Finally, we provide the complexity analysis and robustness analysis. For detailed information regarding baseline models, dataset descriptions, experimental settings, and hyper-parameter analysis, please refer to Appendix D.

### Overall Performance

**Mainstream LTSF Datasets**. As depicted in Table 1, we can observe that: (a) Attraos consistently exhibits the best performance, closely followed by RWKV-TS and Koopa, which underscores the crucial role of modeling temporal dynamics in LTSF tasks. (b) The models based on state space models (Mamba4TS, S-Mamba) generally outperform the Transformer-based models (PatchTST, InvTrm), indicating the potential superiority of state space models as fundamental frameworks for temporal modeling. (c) The performance of the GPT-TS model, which relies on a pre-trained large language model, is relatively average, suggesting the inherent challenge in directly capturing the dynamics of temporal data using such models. A promising avenue for future research lies in training a dynamical foundational model from scratch on large-scale physical datasets or leveraging pre-training to obtain an attractor tokenizer that is better suited for inputs to the large language model.

**Chaotic Datasets**. Table 2 presents the results on both artificial and real-world chaotic datasets. It can be observed that: (a) Attraos exhibits superior performance on both datasets, thanks to the utilization of the PSR and MDMU modules, which effectively capture the multi-scale attractor structures. (b) In Lorenz96 dataset, where there is a prior knowledge about the phase space dimension, Attraos outperforms other models by a significant margin. This highlights the importance of PSR in recovering the complete temporal dynamics. (c) Apart from Attraos, the linear model (DLinear) demonstrate the best predictive results due to their robustness. Conversely, deep learning models based on transformers exhibit weaker performance and fail to model chaotic dynamics effectively.

### Further Analysis

**Ablation Studies.** Next, we turn off each module of Attraos to assess their individual effects. As shown in Table 3, we observe consistent performance decline of Attraos when deleting each module: (a) The removal of Phase Space Reconstruction exhibits the most severe performance degradation, indicating that the process of reconstructing the dynamical structure through PSR forms the foundation for Attraos' efficient capture of attractor structures. (b) Multi-scale hierarchical projection effectively captures the complex topological structure of the singular attractor, leading to improved performance. However, overfitting may occur in certain prediction lengths. (c) Time-varying \(\) and \(_{out}\) acts as a gating attention mechanism, allowing for a more focused emphasis on dynamical structure segments

    &  &  &  &  &  &  &  &  \\   &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\  Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\   ETH1 & **0.423** & **0.420** & 0.441 & 0.438 & 0.459 & 0.453 & 0.454 & 0.446 & 0.457 & 0.450 & 0.443 & 0.438 & 0.454 & **0.434** & **0.435** & 0.462 & 0.458 \\  ETH2 & **0.372** & **0.399** & 0.386 & 0.410 & 0.381 & 0.407 & **0.375** & **0.402** & 0.389 & 0.414 & 0.397 & 0.417 & 0.383 & 0.407 & 0.380 & 0.406 & 0.564 & 0.520 \\  ETH1 & **0.382** & **0.391** & 0.396 & 0.406 & 0.399 & 0.407 & **0.391** & 0.403 & 0.396 & 0.401 & 0.395 & 0.403 & 0.407 & 0.412 & 0.403 & **0.398** & 0.403 & 0.406 \\  ETH7m2 & **0.280** & **0.324** & 0.299 & 0.343 & 0.289 & 0.333 & 0.285 & 0.330 & 0.294 & 0.339 & **0.281** & 0.326 & 0.291 & 0.335 & 0.283 & 0.329 & 0.345 & 0.396 \\   Exchange & **0.349** & **0.395** & 0.364 & **0.405** & 0.364 & 0.407 & 0.406 & 0.439 & 0.371 & 0.409 & 0.390 & 0.424 & 0.366 & 0.416 & 0.383 & 0.416 & **0.346** & 0.416 \\  Crypto & **0.187** & **0.157** & 0.193 & 0.162 & 0.198 & 0.163 & **0.190** & **0.159** & 0.196 & 0.164 & 0.199 & 0.165 & 0.196 & 0.164 & 0.192 & 0.161 & 0.201 & 0.176 \\  Weather & **0.246** & **0.271** & 0.258 & 0.280 & 0.252 & 0.277 & 0.256 & 0.280 & 0.279 & 0.279 & **0.247** & **0.273** & 0.260 & 0.280 & 0.258 & 0.280 & 0.267 & 0.319 \\   

Table 1: Average results of long-term forecasting with an input length of 96 and prediction horizons of . The best performance is in Red, and the second best is in Blue. Full results are in Appendix E.5.

    &  &  &  &  &  &  &  &  \\   &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\   & Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\    & 96 & **0.844** & **0.684** & 0.892 & **0.721** & 0.925 & 0.744 & 0.894 & 0.722 & 0.891 & 0.736 & 0.963 & 0.786 & 0.929 & 0.756 & **0.881** & 0.750 \\  & 192 & **0.838** & **0.662** & 0.910 & 0.748 & 0.917 & 0.761 & 0.894 & 0.744 & **0.881** & 0.752 & 0.944 & 0.811 & 0.899 & **0.744** & 0.910 & 0.753 \\  & 336 & **0.837** & **0.681** & 0.943 & 0.727 & 0.968 & 0.788 & 0.982 & 0.823 & 0.914 & 0.753 & 0.997 & 0.841 & 0.922 & 0.787 & **0.893** & **0.737** \\  & 270 & **0.827** & **0.929** & 0.996 & 0.814 & 1.135 & 0.940 & 1.058 & 0.919 & 0.890 & 0.801 & 1.129 & 0.955 & 0.971 & 0.828 & **0.927** & **0.806** \\  & AVthat potentially contain attractor structures, thereby enhancing performance. (d) The initialization method proposed for \(\) matrix demonstrates marginal yet consistent improvements, underscoring the importance of prior inductive bias for machine learning models. (e) Frequency domain evolution methods significantly reduce temporal noise information and amplify attractor structures. We will further analyze the importance of frequency domain evolution in subsequent analysis.

**Chaotic Evolution Strategy.** We further compare various dynamical system evolution strategies mentioned in Section 3.3. From Table 4, it is evident that the frequency-enhanced evolution strategy outperforms the others. Moreover, our proposed efficient implicit evolution method can adaptively explore multi-scale dynamical structure information, avoiding redundant cyclic computations and mitigating overfitting. (b) An inherent characteristic of time series data is significant noise, making it challenging to capture the underlying dynamical structures in the time domain. Direct evolution strategies, whether linear or non-linear neural network-based, do not yield satisfactory results. Moreover, according to the theorem 2 in FiLM , the recursion computation for dynamic projection further accumulates noise information. (c) Applying Hopfield networks in the time domain also proves to be unsatisfactory, and even adding more patterns (_Key_ and _Value_) can have adverse effects. A potential solution is to apply Hopfield networks in the frequency domain instead.

**Complexity Analysis.** As depicted in Figure 4, we present a comprehensive visual analysis comparing Attraos with various baseline models in terms of their average performance on the ETTh1 dataset. The x-axis represents the training time, the y-axis represents the test loss, and the circle radius corresponds to the model parameters. In this analysis, we substituted GPT-TS with FiLM due to its limited relevance to this specific evaluation. The results clearly demonstrate that Attraos surpasses other models in both time and space complexity, maintaining a significant advantage. Notably, when compared to the PatchTST model with a hidden dimension of 256 (2.4M parameters), Attraos (0.2M parameters) possesses only one-twelfth of its parameter count.

**Robustness Analysis.** We add a 0.1 * \(\)(0, 1) Gaussian noise to the training dataset to test the robustness of Attraos. As shown in Table 5, it can be observed that Attraos exhibits strong robustness against noisy data, and increasing the level of noise can even lead to further performance improvement. This is attributed to the frequency domain evolution strategy, where we retain only the dominant modes as attractor structures, effectively removing the noise information. Furthermore, an interesting phenomenon has been observed in our experiments: as noise is introduced, the model's convergence speed increases. This discovery warrants further exploration in future studies.

    &  &  &  &  &  &  \\   & Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\    & 96 & **0.370** & **0.388** & **0.376** & **0.392** & 0.388 & 0.404 & 0.395 & 0.410 & 0.384 & 0.405 & 0.389 & 0.407 \\  & 192 & **0.416** & **0.415** & **0.419** & **0.423** & 0.441 & 0.439 & 0.444 & 0.437 & 0.430 & 0.446 & 0.427 & 0.442 \\  & 336 & **0.458** & **0.432** & **0.465** & **0.439** & 0.488 & 0.460 & 0.482 & 0.456 & 0.480 & 0.482 & 0.485 & 0.489 \\  & 720 & **0.447** & **0.442** & **0.454** & **0.448** & 0.511 & 0.508 & 0.510 & 0.512 & 0.494 & 0.491 & 0.502 & 0.500 \\  & AVG & **0.423** & **0.420** & **0.429** & **0.426** & 0.457 & 0.453 & 0.458 & 0.454 & 0.447 & 0.456 & 0.426 & 0.460 \\   & 96 & **0.172** & **0.254** & **0.175** & **0.258** & 0.187 & 0.266 & 0.191 & 0.269 & 0.181 & 0.260 & 0.182 & 0.263 \\  & 192 & **0.242** & **0.301** & **0.247** & **0.308** & 0.264 & 0.331 & 0.265 & 0.334 & 0.255 & 0.312 & 0.259 & 0.314 \\   & 336 & **0.303** & **0.340** & **0.310** & 0.349 & 0.325 & 0.359 & 0.319 & 0.354 & 0.315 & **0.347** & 0.312 & 0.344 \\   & 720 & **0.401** & **0.399** & **0.447** & **0.405** & 0.424 & 0.419 & 0.421 & 0.422 & 0.420 & 0.426 & 0.417 & 0.422 \\   & AVG & **0.250** & **0.324** & **0.285** & **0.330** & 0.300 & 0.344 & 0.297 & 0.345 & 0.293 & 0.336 & 0.293 & 0.336 \\   

Table 4: Results of various chaotic evolution strategies. Red/Blue denotes the best/second performance.

    &  &  &  &  &  &  \\   & Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\    & 96 & 0.292 & 0.348 & **0.301** & **0.357** & **0.299** & **0.353** & **0.299** & **0.354** & **0.294** & **0.351** & **0.297** & **0.352** \\  & 192 & 0.374 & 0.386 & **0.389** & **0.405** & **0.384** & **0.393** & **0.381** & **0.395** & **0.373** & **0.384** & **0.378** & **0.388** \\  & 336 & 0.420 & 0.432 & **0.427** & **0.438** & **0.426** & 0.436 & 0.424 & **0.430** & **0.425** & **0.436** & 0.427 & 0.435 \\  & 720 & 0.418 & 0.431 & **0.431** & **0.450** & **0.425** & 0.437 & **0.416** & **0.427** & **0.421** & **0.433** & **0.427** & **0.437** \\  & AVG & 0.376 & 0.399 & **0.387** & **0.413** & **0.380** & **0.405** & **0.380** & **0.402** & **0.478** & **0.401** & **0.382** & **0.403** \\   & 96 & 0.159 & 0.206 & 0.171 & 0.215 & 0.163 & 0.210 & 0.167 & 0.214 & 0.162 & **0.209** & **0.164** & **0.211** \\  & 192 & 0.212 & 0.249 & 0.266 & **0.263** & **0.218** & 0.253 & 0.222 & **0.270** & **0.215** & 0.249 & **0.216** & **0.253** \\   & 336 & 0.265 & 0.288 & **0.232** & **0.304** & **0.271** & **0.295** & **0.277** & **0.297** & **0.263** & 0.288 & **0.270** & **0.294** \\   & 720 & 0.347 & 0.340 & 0.358 & 0.351 & 0.346 & 0.338 & 0.355 & 0.346 & 0.347 & 0.342 & 0.355 & 0.356 \\   & AVG & 0.246 & 0.271 & **0.259** & **0.283** & **0.2Chaotic Reconstruction.As illustrated in the left of Figure 5, we visualize the phase space forecasting results of Attraos on the Lorenz96 system. It can be observed that: Although some minor details may be missing due to the sparsity introduced by the frequency domain evolution, Attraos successfully reconstructs the chaotic dynamic structure of Lorenz96. Moreover, modeling time series based on the dynamics structure of the phase space can be viewed as a form of data augmentation, e.g., two-dimensional time figuring  or seasonal decomposition .

Chaotic Representation & Hyper-parameter Analysis.In the right of Figure 5, we validated the impact of polynomial dimensions on the model performance. Noteworthy observations include: As noted in substantial literature on SSMs, a polynomial dimension of 256 is generally required to approximate input time series signals with sufficient accuracy. However, we found that the patch operation effectively reduces this threshold in a linear fashion. For instance, in the ETT dataset with a phase space dimension of 4, the required polynomial dimension drops to 256/4 = 64 dimensions.

Discussion.Recently, across various fields of machine learning, an increasing number of works have focused on the underlying physical properties hidden within real-world observational data [20; 21; 26; 51]. By leveraging physical priors, these models achieve significant improvements in generalization, accuracy, and interpretability. We hope Attraos introduces a fresh perspective to the LTSF community and encourages the emergence of physics-guided time series analysis models.

## 5 Conclusion and Future Work

LTSF tasks have long been a focal point of research in the machine-learning community. However, mainstream deep learning models currently overlook the crucial aspect that time series data is derived from discretely sampling underlying continuous dynamical systems. Inspired by chaotic theory, our model, Attraos, considers time series as generated by a generalized chaotic dynamical system. By leveraging the invariance of attractors, Attraos enhances predictive performance and provides empirical and theoretical explanations. In the future, we will verify our proposed Attraos on large-scale chaotic datasets and utilize the implicit neural representations for the phase space coordinates, enabling a trainable embedding process to address the instability of PSR techniques.

Figure 4: Complexity analysis.

    &  &  \\   & Metric & MSE & MAE & MSE & MAE \\    & 96 & 0.370 & 0.388 & **0.360** & **0.390** \\  & 192 & 0.416 & 0.418 & **0.413** & **0.415** \\  & 336 & 0.458 & 0.432 & **0.455** & **0.430** \\  & 720 & 0.447 & 0.442 & **0.451** & **0.444** \\  & AVG & 0.423 & 0.420 & **0.422** & 0.420 \\   & 96 & 0.172 & 0.254 & **0.170** & **0.251** \\  & 192 & 0.242 & 0.301 & **0.238** & **0.297** \\   & 336 & 0.303 & 0.340 & **0.305** & **0.339** \\   & 720 & 0.401 & 0.399 & **0.398** & **0.392** \\   & AVG & 0.280 & 0.324 & **0.278** & **0.320** \\   

Table 5: Robustness analysis with additional noise. Red/Blue denotes the performance improvement/decline.

Figure 5: Left: Chaotic Reconstruction for Lorenz96 system with 720 forecasting step. Right: Hyper-parameter analysis w.r.t. polynomial orders for different model variants w.r.t. patching operation in ETTm2 dataset.