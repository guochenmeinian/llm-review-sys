# A Scalable Neural Network for DSIC Affine

Maximizer Auction Design

 Zhijian Duan

CFCS, School of Computer Science

Peking University

zjduan@pku.edu.cn

&Haoran Sun

Peking University

sunhaoran0301@stu.pku.edu.cn

&Yurong Chen

CFCS, School of Computer Science

Peking University

chenyurong@pku.edu.cn

&Xiaotie Deng

CFCS, School of Computer Science

& CMAR, Institute for AI

Peking University

xiaotie@pku.edu.cn

###### Abstract

Automated auction design aims to find empirically high-revenue mechanisms through machine learning. Existing works on multi item auction scenarios can be roughly divided into RegretNet-like and affine maximizer auctions (AMAs) approaches. However, the former cannot strictly ensure dominant strategy incentive compatibility (DSIC), while the latter faces scalability issue due to the large number of allocation candidates. To address these limitations, we propose AMenNet, a scalable neural network that constructs the AMA parameters (even including the allocation menu) from bidder and item representations. AMenuNet is always DSIC and individually rational (IR) due to the properties of AMAs, and it enhances scalability by generating candidate allocations through a neural network. Additionally, AMenuNet is permutation equivariant, and its number of parameters is independent of auction scale. We conduct extensive experiments to demonstrate that AMenuNet outperforms strong baselines in both contextual and non-contextual multi-item auctions, scales well to larger auctions, generalizes well to different settings, and identifies useful deterministic allocations. Overall, our proposed approach offers an effective solution to automated DSIC auction design, with improved scalability and strong revenue performance in various settings.

## 1 Introduction

One of the central topics in auction design is to construct a mechanism that is both dominant strategy incentive compatible (DSIC) and individually rational (IR) while bringing high expected revenue to the auctioneer. The seminal work by Myerson (1981) characterizes the revenue-maximizing mechanism for single-parameter auctions. However, after four decades, the optimal auction design problem in multi-parameter scenarios remains incompletely understood, even in simple settings such as two bidders and two items (Dutting et al., 2019). To solve the problem, recently, there has been significant progress in _automated auction design_(Sandholm and Likhodedov, 2015; Dutting et al., 2019). Such a paradigm formulates auction design as an optimization problem subject to DSIC and IR constraints and then finds optimal or near-optimal solutions using machine learning.

The works of automated auction design can be roughly divided into two categories. The first category is the RegretNet-like approach (Curry et al., 2020; Peri et al., 2021; Rahme et al., 2021; Duan et al., 2022; Ivanov et al., 2022), pioneered by RegretNet (Dutting et al., 2019). These worksrepresent the auction mechanism as neural networks and then find near-optimal and approximate DSIC solutions using adversarial training. The second category is based on affine maximizer auctions (AMAs) (Roberts, 1979; Likhodedov and Sandholm, 2004; Likhodedov et al., 2005; Sandholm and Likhodedov, 2015; Guo et al., 2017; Curry et al., 2023). These methods restrict the auction mechanism to AMAs, which are inherently DSIC and IR. Afterward, they optimize AMA parameters using machine learning to achieve high revenue.

Generally, RegretNet-like approaches can achieve higher revenue than AMA-based approaches. However, these works are not DSIC. They can only ensure approximate DSIC by adding a regret term in the loss function as a penalty for violating the DSIC constraints. There are no theoretical results on the regret upper bound, and the impact of such regret on the behaviors of strategic bidders. Even worse, computing the regret term is time-consuming (Rahme et al., 2021). AMA-based approaches, on the other hand, offer the advantage of guaranteeing DSIC and IR due to the properties of AMAs. However, many of these approaches face scalability issues because they consider all deterministic allocations as the allocation menu. The size of the menu grows exponentially, reaching \((n+1)^{m}\) for \(n\) bidders and \(m\) items, making it difficult for these approaches to handle larger auctions. Even auctions with \(3\) bidders and \(10\) items can pose challenges to the AMA-based methods.

To overcome the limitations above, we propose a scalable neural network for the DSIC affine maximizer auction design. We refer to our approach as AMenuNet: Affine maximizer auctions with Menu Network. AMenuNet constructs the AMA parameters, including the allocation menu, bidder weights, and boost variables, from the bidder and item representations. After getting the parameters, we compute the allocation and payment results according to AMA. By setting the representations as the corresponding contexts or IDs, AMenuNet can handle both contextual (Duan et al., 2022) and non-contextual classic auctions. As AMenuNet only relies on the representations, the resulting mechanism is guaranteed to be DSIC and IR due to the properties of AMAs.

Specifically, we employ two techniques to address the scalability issue of AMA-based approaches: (1) Firstly, we parameterize the allocation menu. We predefine the size of the allocation menu and train the neural network to compute the allocation candidates within the menu, along with the bidder weights and boost variables. This allows for more efficient handling of large-scale auctions. (2) Secondly, we utilize a transformer-based permutation-equivariant architecture. Notably, this architecture's parameters remain independent of the number of bidders or items. This enhances the scalability of AMenuNet, enabling it to handle auctions of larger scales than those in training.

We conduct extensive experiments to demonstrate the effectiveness of AMenuNet. First, our performance experiments show that in both contextual and classic multi-item auctions, AMenuNet can achieve higher revenue than strong DSIC and IR baselines. AMenuNet can also achieve comparable revenue to RegretNet-like approaches that can only ensure approximate DSIC. Next, our ablation study shows that the learnable allocation menu provides significant benefits to AMenuNet, from both revenue and scalability perspectives. Thirdly, we find that AMenuNet can also generalize well to auctions with a different number of bidders or items than those in the training data. And finally, the case study of winning allocations illustrates that AMenuNet can discover useful deterministic allocations and set the reserve price for the auctioneer.

## 2 Related Work

Automated mechanism design(Conitzer and Sandholm, 2002, 2004; Sandholm and Likhodedov, 2015) has been proposed to find approximate optimal auctions with multiple items and bidders (Balcan et al., 2008; Lahaie, 2011; Dutting et al., 2015). Meanwhile, several works have analyzed the sample complexity of optimal auction design problems (Cole and Roughgarden, 2014; Devanur et al., 2016; Balcan et al., 2016; Guo et al., 2019; Gonczarowski and Weinberg, 2021). Recently, pioneered by RegretNet (Dutting et al., 2019), there is rapid progress on finding (approximate) optimal auction through deep learning (Curry et al., 2020; Peri et al., 2021; Rahme et al., 2021; Ba et al., 2022; Duan et al., 2022; Ivanov et al., 2022). However, as we mentioned before, those RegretNet-like approaches can only ensure approximate DSIC by adding a hard-to-compute regret term.

Our paper follows the affine maximizer auction (AMA) (Roberts, 1979) based approaches. AMA is a weighted version of VCG (Vickrey, 1961), which assigns weights \(\) to each bidder and assigns boosts to each feasible allocation. Tuning the weights and boosts enables AMAs to achieve higher revenue than VCG while maintaining DSIC and IR. Different subsets of AMA have been studied in various works, such as VVCA (Likhodedov and Sandholm, 2004; Likhodedov et al., 2005; Sandholm and Likhodedov, 2015), \(\)-auction (Jehiel et al., 2007), mixed bundling auction (Tang and Sandholm, 2012), and bundling boosted auction (Balcan et al., 2021). However, these works set all the deterministic allocations as candidates, the size of which grows exponentially with respect to the auction scale. To overcome such issue, we construct a neural network that automatically computes the allocation menu from the representations of bidders and items. Curry et al. (2023) is the closest work to our approach that also parameterizes the allocation menu. The key difference is that they optimize the AMA parameters explicitly, while we derive the AMA parameters by a neural network and optimize the network weights instead. By utilizing a neural network, we can handle contextual auctions by incorporating representations as inputs. Additionally, the trained model can generalize to auctions of different scales than those encountered during training.

Contextual auctions are a more general and realistic auction format assuming that every bidder and item has some public information. They have been widely used in industry (Zhang et al., 2021; Liu et al., 2021). In the academic community, previous works on contextual auctions mostly focus on the online setting of some well-known contextual repeated auctions, such as posted-price auctions (Amin et al., 2014; Mao et al., 2018; Drutsa, 2020; Zhiyanov and Drutsa, 2020), where the seller prices the item to sell to a strategic buyer, or repeated second-price auctions (Golrezaei et al., 2021). In contrast, our paper focuses on the offline setting of contextual sealed-bid auctions, similar to Duan et al. (2022), a RegretNet-like approach.

## 3 Preliminary

Sealed-Bid Auction.We consider a sealed-bid auction with \(n\) bidders and \(m\) items. Denote \([n]=\{1,2,,n\}\). Each bidder \(i[n]\) is represented by a \(d_{x}\)-dimensional vector \(_{i}^{d_{x}}\), which can encode her unique ID or her context (public feature) (Duan et al., 2022). Similarly, each item \(j[m]\) is represented by a \(d_{y}\)-dimensional vector \(_{j}^{d_{y}}\), which can also encode its unique ID or context. By using such representations, we only the contextual auction and the classical Bayesian auction. We denote by \(X=[_{1},_{2},,_{n}]^{T}^{n d_{x}}\) and \(Y=[_{1},_{2},,_{m}]^{T}^{m d_{y}}\) the matrices of bidder and item representations, respectively. These matrices follow underlying joint probability distribution \(F_{X,Y}\). In an additive valuation setting, each bidder \(i\) values each bundle of items \(S[m]\) with a valuation \(v_{i,S}=_{j S}v_{ij}\). The bidder has to submit her bids for each item \(j[m]\) as \(_{i}(b_{1},b_{2},,b_{m})\). The valuation profile \(V=(v_{ij})_{i[n],j[m]}^{n m}\) is generated from a conditional distribution \(F_{V|X,Y}\) that depends on the representations of bidders and items. The auctioneer does not know the true valuation profile \(V\) but can observe the public bidder representations \(X\), item representations \(Y\), and the bidding profile \(B=(b_{ij})_{i[n],j[m]}^{n m}\).

Auction Mechanism.An auction mechanism \((g,p)\) consists of an allocation rule \(g:^{n m}^{n d_{x}}^{m  d_{y}}^{n m}\) and a payment rule \(p:^{n m}^{n d_{x}}^{m  d_{y}}^{n}_{ 0}\). Given the bids \(B\), bidder representations \(X\), and item representations \(Y\), \(g_{ij}(B,X,Y)\) computes the probability that item \(j\) is allocated to bidder \(i\). We require that \(_{i=1}^{m}g_{ij}(B,X,Y) 1\) for any item \(j\) to guarantee that no item is allocated more than once. The payment \(p_{i}(B,X,Y) 0\) computes the price that bidder \(i\) needs to pay. Bidders aim to maximize their own utilities. In the additive valuation setting, bidder \(i\)'s utility is \(u_{i}(_{i},B;X,Y)_{j=1}^{m}g_{ij}(B,X,Y)v_{ij}-p_{i}(B,X,Y),\) given \(,B,X,Y\). Bidders may misreport their valuations to benefit themselves. Such strategic behavior among bidders could make the auction result hard to predict. Therefore, we require the auction mechanism to be _dominant strategy incentive compatible_ (DSIC), which means that for each bidder \(i[n]\), reporting her true valuation is her optimal strategy regardless of how others report. Formally, let \(B_{-i}=(_{1},,_{i-1},_{i+1},,_{n})\) be the bids except for bidder \(i\). A DSIC mechanism satisfies

\[u_{i}(_{i},(_{i},B_{-i});X,Y)) u_{i}(_{i},(_{i},B _{-i});X,Y)), i,_{i},B_{-i},X,Y,_{i}.\] (DSIC)

Furthermore, the auction mechanism needs to be _individually rational_ (IR), which ensures that truthful bidding results in a non-negative utility for each bidder. Formally,

\[u_{i}(_{i},(_{i},B_{-i});X,Y) 0, i,_{i},B_{-i},X,Y.\] (IR)

Affine Maximizer Auction (AMA).AMA (Roberts, 1979) is a generalized version of VickreyClarkeGroves (VCG) auction (Vickrey, 1961) that is inherently DISC and IR. An AMA consistsof positive weights \(w_{i}_{+}\) for each bidder and boost variables \((A)\) for each allocation \(A\), where \(\) is the _allocation menu_, i.e., the set of all the feasible (possibly random) allocations. Given \(B,X,Y\), AMA chooses the allocation that maximizes the affine welfare:

\[g(B,X,Y)=A^{*}_{A}_{i=1}^{n}w_{i}b_{i}(A)+ (A),\] (Allocation)

where \(b_{i}(A)=_{j=1}^{m}b_{ij}A_{ij}\) in additive valuation setting. The payment for bidder \(k\) is

\[p_{k}(B,X,Y)=}(_{i k}w_{i}b_{i}(A^{*}_{-k})+( A^{*}_{-k}))-}(_{i k}w_{i}b_{i}(A^{*})+ (A^{*})).\] (Payment)

Here we denote by \(A^{*}_{-k}_{A}_{i k}w_{i}b_{i}(A)+ (A)\) the allocation that maximizes the affine welfare, with bidder \(k\)'s utility excluded. Our definition of AMA differs slightly from previous literature (Roberts, 1979), whose allocation candidates are all the \((n+1)^{m}\) deterministic solutions. Instead, we explicitly define the allocation menu so that our definition is more general. As we will show in Appendix A, such AMAs are still DSIC and IR.

## 4 Methodology

In this section, we introduce the optimization problem for automated mechanism design and our proposed approach, AMenuNet, along with its training procedure.

### Auction Design as an Optimization Problem

We begin by parameterizing our auction mechanism as \((g^{},p^{})\), where \(\) represents the neural network weights to be optimized. We denote the parameter space as \(\) and the class of all parameterized mechanisms as \(^{}\). The optimal auction design seeks to find the revenue-maximizing mechanism that satisfies both DSIC and IR:

\[_{}&(g^{ },p^{})_{(V,X,Y) F_{V,X,Y}}[_{i=1} ^{n}p_{i}^{}(V,X,Y)]\\ &&(g^{},p^{})\] (OPT)

where \((g^{},p^{})\) is the expected revenue of mechanism \((g^{},p^{})\). However, there is no known characterization of a DSIC general multi-item auction (Dutting et al., 2019). Thus, we restrict the search space of auctions to affine maximizer auctions (AMAs) (Roberts, 1979). AMAs are inherently DSIC and IR, and can cover a broad class of mechanisms: Lavi et al. (2003) has shown that every DSIC multi-item auction (where each bidder only cares about what they get and pay) is almost an AMA (with some qualifications).

The AMA parameters consist of positive weights \(_{+}^{n}\) for all bidders, an allocation menu \(\), and boost variables \(^{||}\) for each allocation in \(\). While the most straightforward way to define \(\) is to include all deterministic allocations, as done in prior work (Likhodedov and Sandholm, 2004; Likhodedov et al., 2005; Sandholm and Likhodedov, 2015), this approach suffers from scalability issues as the number of allocations can be as large as \((n+1)^{m}\). To address this challenge, we propose AMenuNet, which predefines the size of \(\) and constructs it along with the other AMA parameters using a permutation-equivariant neural network.

### AMenuNet Architecture

Denote by \(s=||\) the predefined size of the allocation menu. AMenuNet takes as input the representations of bidders \(X\) and items \(Y\), and constructs all the \(s\) allocations, the bidder weights \(\), and the boost variables \(^{s}\) through a permutation-equivariant neural network architecture as illustrated in Figure 1. The architecture consists of an encode layer, a menu layer, a weight layer, and a boost layer. The final allocation and payment results can be computed by combining the submitted bids \(B\) according to AMA (see Equation (Allocation) and Equation (Payment)).

We first introduce a dummy bidder \(n+1\) to handle cases where no items are allocated to any of the \(n\) bidders. We set such dummy bidder's representation \(_{n+1}\) as a \(d_{x}\)-dimensional vector with all elements set to 1.

Encode Layer.The encode layer transforms the initial representations of all bidders (including the dummy bidder) and items into a joint representation that captures their mutual interactions. In contextual auctions, their initial representations are the contexts. In non-contextual auctions, similar to word embedding (Mikolov et al., 2013), we embed the unique ID of each non-dummy bidder or item into a continuous vector space, and use the embeddings as their initial representations. Based on that, we construct the initial encoded representations for all pairs of \(n+1\) bidders and \(m\) items \(E^{(n+1) m(d_{x}+d_{y})}\), where

\[E_{ij}=[_{i};_{j}]^{d_{x}+d_{y}}\]

is the initial encoded representation of bidder \(i\) and item \(j\).

We further model the mutual interactions of bidders and items by three steps. Firstly, we capture the inner influence of each bidder-item pair by two \(1 1\) convolutions with a ReLU activation. By doing so, we get

\[L=_{2}_{1} E^{(n+1) m d},\]

where \(d\) is the dimension of the new latent representation for each bidder-item pair, both \(_{1}\) and \(_{2}\) are \(1 1\) convolutions, and \((x):=(x,0)\).

Secondly, we model the mutual interactions between all the bidders and items by using the transformer (Vaswani et al., 2017) based interaction module, similar to Duan et al. (2022). Specifically, for each bidder \(i\), we model her interactions with all the \(m\) items through transformer on the \(i\)-th row of \(L\), and for each item \(j\), we model its interactions with all the \(n\) bidders through another transformer on the \(j\)-th column of \(L\):

\[I_{i,*}^{}=(L_{i,*})^{m d_{h} }, I_{*,j}^{}=(L_{*,j})^{( n+1) d_{h}},\]

where \(d_{h}\) is the size of the hidden nodes of transformer; For all the bidders and items, their global representation is obtained by the average of all the representations

\[e^{}=_{i=1}^{n+1}_{j=1}^{m}L_{ij}.\]

Thirdly, we get the unified interacted representation

\[I_{ij}:=[I_{ij}^{};I_{ij}^{};e^{}] ^{2d_{h}+d}\]

Figure 1: A schematic view of AMenuNet, which takes the bidder representations \(X\) (including the dummy bidder) and item representations \(Y\) as inputs. These representations are assembled into a tensor \(E^{(n+1) m(d_{x}+d_{y})}\). Two \(1 1\) convolution layers are then applied to obtain the tensor \(L\). Following \(L\), multiple transformer-based interaction modules are used to model the mutual interactions among all bidders and items. The output tensor after these modules is denoted as \(J\). \(J\) is further split into three parts: \(J^{}^{(n+1) m s}\), \(J^{}^{(n+1) m}\) and \(J^{}^{(n+1) m s}\). These parts correspond to the allocation menu \(\), bidder weights \(\), and boosts \(\), respectively. Finally, based on the induced AMA parameters and the submitted bids, the allocation and payment results are computed according to the AMA mechanism.

by combining all the three representations for bidder \(i\) and item \(j\). Two \(1 1\) convolutions with a ReLU activation are applied on \(I\) to encode \(I\) into the joint representation

\[I^{}_{4}_ {3} I^{(n+1) m d_{}}.\]

By stacking multiple interaction modules, we can model higher-order interactions among all bidders and items.

For the final interaction module, we set \(d_{}=2s+1\) and we denote by \(J^{(n+1) m(2s+1)}\) its output. We then partition \(J\) into three tensors: \(J^{}^{(n+1) m s}\), \(J^{}^{(n+1) m}\) and \(J^{}^{(n+1) m s}\), and pass them to the following layers.

Menu Layer.To construct the allocation menu, we first normalize each column of \(J^{}\) through softmax function. Specifically, \( j[m],k[s]\), we denote by \(J^{}_{*,j,k}^{n+1}\) the \(j\)-th column of allocation option \(k\), and obtain its normalized probability \(^{}_{*,j,k}\) by

\[^{}_{*,j,k}( J^{ }_{*,j,k})\]

where \(()_{i} e^{x_{i}}/(_{k=1}^{n+1}e^{x_{k}}) (0,1)\) for all \(^{n+1}\) is the softmax function, and \(>0\) is the temperature parameter. Next, we exclude the probabilities associated with the \(n+1\)-th dummy bidder to obtain the allocation menu:

\[=^{}_{1:n}^{n m s}.\]

Thus, for each allocation \(A\) we satisfy \(A_{ij}\) and \(_{i=1}^{n}A_{ij} 1\) for any bidder \(i\) and any item \(j\).

Weight Layer.In this work, we impose the constraint that each bidder weight lies in \((0,1]\). Given \(J^{}^{(n+1) m}\), for each bidder \(i n\) we get her weight by

\[w_{i}=(_{j=1}^{m}J^{}_{ij}),\]

where \((x) 1/(1+e^{-x})(0,1)\) for all \(x\) is the sigmoid function.

Boost layer.In boost layer, we first average \(J^{}\) across all bidders and items. We then use a multi-layer perceptron (MLP), which is a fully connected neural network, to get the boost variables \(\). Specifically, we compute \(\) by

\[=(_{i=1}^{n+1}_{j=1}^{m}J^{} _{ij})^{s}.\]

After we get the output AMA parameters \(,,\) from AMenuNet, we can compute the allocation result according to Equation (Allocation) and the payment result according to Equation (Payment). The computation of \(,,\) only involves the public representations of bidders and items, without access to the submitted bids. Therefore, the mechanism is DSIC and IR (see Appendix A for proof):

**Theorem 4.1**.: _The mechanism induced by AMenuNet satisfies both DSIC and IR._

Moreover, As AMenuNet is built using equivariant operators such as transformers and \(1 1\) convolutions, any permutation of the inputs to AMenuNet, including the submitted bids \(B\), bidder representations \(X\), and item representations \(Y\), results in the same permutation of the allocation and payment outcomes. This property is known as permutation equivariance .

**Definition 4.2** (Permutation Equivariance).: We say \((g,p)\) is permutation equivariant, if for any \(B,X,Y\) and for any two permutation matrices \(_{n}\{0,1\}^{n n}\) and \(_{m}\{0,1\}^{m m}\), we have \(g(_{n}B_{m},_{n}X,_{m}^{T}Y)=_{n}g(B,X,Y)_{m}\) and \(p(_{n}B_{m},_{n}X,_{m}^{T}Y)=_{n}p(B,X,Y)\).

Permutation equivariant architectures are widely used in automated auction design . Qin et al.  have shown that this property can lead to better generalization ability of the mechanism model.

### Optimization and Training

Since the induced AMA is DSIC and IR, we only need to maximize the expected revenue \((g^{},p^{})\). To achieve this, following the standard machine learning paradigm (Shalev-Shwartz and Ben-David, 2014), we minimize the negative empirical revenue by set the loss function as

\[(,S)_{k=1}^{|S|}_{i=1}^{n}-p_{i}^{ }(V^{(k)},X^{(k)},Y^{(k)}),\]

where \(S\) is the training data set, and \(\) contains all the neural network weights in the encode layer and the boost layer. However, the computation of \((,S)\) involves finding the affine welfare-maximizing allocation scheme \(A^{*}\) (and \(A^{*}_{-k}\)), which is non-differentiable. To address this challenge, we use the softmax function as an approximation. During training, we compute an approximate \(A^{*}\) by

\[}_{A}(_{ A}(_{i=1}^{n}w_{i}b_{i}(A)+(A))) A,\]

where \(Z_{A^{}}(_{A}(_{i =1}^{n}w_{i}b_{i}(A^{})+(A^{})))\) is the normalizer, and \(_{A}\) is the softmax temperature. We can control the approximation level of \(}\) by tuning \(_{A}\): when \(_{A}\), \(}\) recover the true \(A^{*}\), and when \(_{A} 0\), \(}\) tends to a uniform combination of all the allocations in \(\). The approximation \(_{-k}}\) of \(A^{*}_{-k}\) is similar. By approximating \(A^{*}\) and \(A^{*}_{-k}\) through the differentiable \(}\) and \(_{-k}}\), we make it feasible to optimize \((,S)\) through gradient descent. Notice that in testing, we still follow the standard computation in Equation (Allocation) and Equation (Payment).

## 5 Experiments

In this section, we present empirical experiments that evaluate the effectiveness of AMenuNet1. All experiments are run on a Linux machine with NVIDIA Graphics Processing Unit (GPU) cores. Each result is obtained by averaging across \(5\) different runs. In all experiments, the standard deviation of AMenuNet across different runs is less than \(0.01\).

Baseline Methods.We compare AMenuNet against the following baselines:

1. VCG (Vickrey, 1961), which is the most classical special case of AMA;
2. Item-Myerson, a strong baseline used in Dutting et al. (2019), which independently applies Myerson auction with respect to each item;
3. Lottery AMA (Curry et al., 2023), an AMA-based approach that directly sets the allocation menu, bidder weights, and boost variables as all the learnable weights.
4. RegretNet (Dutting et al., 2019), the pioneer work of applying deep learning in auction design, which adopts fully-connected neural networks to compute auction mechanisms.
5. CITransNet (Duan et al., 2022), a RegretNet-like transformer-based approach that supports contextual auctions.

Note that both RegretNet and CITransNet can only achieve approximate DSIC by adding a regret term in the loss function. We train both models with small regret, less than \(0.005\).

Hyperparameters.We train the models for a maximum of \(8000\) iterations, with \(32768\) generated samples per iteration. The batch size is \(2048\), and we evaluate all models on \(100000\) samples. We set the softmax temperature as \(500\) and the learning rate as \(3 10^{-4}\). We tune the menu size in \(\{32,64,128,256,512,1024\}\)2. For the boost layer, we use a two-layer fully connected neural network with ReLU activation. Given the induced AMA parameters, our implementation of the remaining AMA mechanism is built upon the implementation of Curry et al. (2023). Further implementation details can be found in Appendix B.

Auction Settings.AMenuNet can deal with both contextual auctions and classic Bayesian auctions. We construct the following multi-item contextual auctions:

1. We generate each bidder representations \(_{i}^{10}\) and item representations \(_{j}^{10}\) independently from a uniform distribution in \([-1,1]^{10}\) (i.e., \(U[-1,1]^{10}\)). The valuation \(v_{ij}\) is sampled from \(U[0,(_{i}^{T}_{j})]\). This contextual setting is also used in Duan et al. (2022). We choose the number of bidders \(n\{2,3\}\) and the number of items \(m\{2,5,10\}\).
2. We set \(2\) items, and the representations are generated from the same way as in Setting (A). For valuations, we first generate an auxiliary variable \(v_{i}^{}\) from \(U\) for each bidder \(i\), and then we set \(v_{i1}=v_{i}^{}(_{i}^{T}_{1})\) and \(v_{i2}=(1-v_{i}^{})(_{i}^{T}_{2})\). We do this to make the valuations of the \(2\) items highly correlated. We choose the number of bidders \(n\{4,5,6,7\}\).

For classic auctions, we can assign each bidder and item a unique ID and embed them into a multi-dimensional continuous representation. We construct the following classic auction settings:

1. For all bidders and items, \(v_{ij} U\). Such setting is widely evaluated in RegretNet-like approaches (Dutting et al., 2019). We select \(n m\) (the number of bidders and items) in \(\{2 5,3 10,5 5\}\).
2. \(3\) bidders and \(1\) item, with \(v_{i1}(3)\) (i.e. the density function is \(f(x)=1/3e^{-1/3x}\) for all \(i\{1,2,3\}\)). The optimal solution is given by Myerson auction.
3. \(1\) bidder and \(2\) items, with \(v_{11} U\) and \(v_{12} U\). The optimal auction is given by Daskalakis et al. (2015).
4. \(1\) bidder and \(2\) items, where \(v_{11}\) has the density function \(f(x)=5/(1+x)^{6}\), and \(v_{12}\) has the density function \(f(x)=6/(1+x)^{7}\). The optimal solution is given by Daskalakis et al. (2015).

Revenue Experiments.The results of revenue experiments are presented in Table 1. We can see that in settings with unknown optimal solutions, AMenuNet achieves the highest revenue among all the DSIC approaches. The comparison between AMenuNet and VCG reveals that incorporating affine parameters into VCG leads to higher revenue. Notably, AMenuNet even surpasses the strong baseline Item-Myerson, which relies on prior distribution knowledge, an over strong assumption that may not hold especially in contextual auctions. In contrast, AMenuNet constructs the mechanism solely from sampled data, highlighting its data efficiency. Furthermore, AMenuNet demonstrates its

   Method & DSIC? & 2\(\)2 & 2\(\)5 & 2\(\)10 & 3\(\)2 & 3\(\)5 & 3\(\)10 & 4\(\)2 & 5\(\)2 & 6\(\)2 & 7\(\)2 \\  & & (A) & (A) & (A) & (A) & (A) & (A) & (B) & (B) & (B) \\   CITransNet & No & **0.4461** & **1.1770** & **2.4218** & 0.5576 & **1.4666** & **2.9180** & **0.7227** & **0.7806** & **0.8396** & **0.8997** \\  VCG & Yes & 0.2882 & 0.7221 & 1.4423 & 0.4582 & 1.1457 & 2.2967 & 0.5751 & 0.6638 & 0.7332 & 0.7904 \\ Item-Myerson & Yes & 0.4265 & 1.0700 & 2.1398 & 0.5590 & 1.3964 & 2.7946 & 0.6584 & 0.7367 & 0.8004 & 0.8535 \\ AMenuNet & Yes & 0.4398 & 1.1539 & 2.3935 & **0.5601** & 1.4458 & 2.9006 & 0.7009 & 0.7548 & 0.8127 & 0.8560 \\  Randomized Allocations & 1.42\% & 7.37\% & 18.01\% & 3.52\% & 22.25\% & 54.38\% & 12.41\% & 14.81\% & 10.76\% & 17.02\% \\   

Table 1: The experiment results of average revenue. For each case, we use the notation \(n m\) to represent the number of bidders \(n\) and the number of items \(m\). The regret of both CITransNet and RegretNet is less than \(0.005\). The best revenue is highlighted in **bold**, and the best revenue among all DSIC methods is underlined.

ability to approach near-optimal solutions in known settings (Setting (D)-(F)), even when those solutions are not AMAs themselves. This underscores the representativeness of the AMAs induced by AMenuNet. Remarkably, the comparison between AMenuNet and Lottery AMA in classic auctions showcases the effectiveness of training a neural network to compute AMA parameters. This suggests that AMenuNet captures the underlying mutual relations among the allocation menu, bidder weights, and boosts, resulting in a superior mechanism after training. Lastly, while CITransNet and RegretNet may achieve higher revenue in many cases, it is important to note that they are not DSIC. Such results indicate that AMenuNet's zero regret comes at the expense of revenue. Compared to RegretNet-based approaches, AMenuNet's distinctive strength lies in its inherent capacity to ensure DSIC by design.

Ablation Study.We present an ablation study that compares the full AMenuNet model with the following three ablated versions: AMenuNet with fixed \(=\), AMenuNet with fixed \(=\), AMenuNet with \(_{}\) (all the \((n+1)^{m}\) deterministic allocations), and AMenuNet with the underlying architecture to be a \(4\)-layers fully connected neural networks (FCN). We set the number of layers in FCN to be \(4\), each with \(128\) hidden nodes. The revenue results are presented in Table 2. For large values of \(n\) and \(m\), the experiment result of AMenuNet with \(_{}\) can be intractable due to the large size of the allocation menu (\(59049\) for \(2 10\) and \(1048576\) for \(3 10\)). Such phenomenon indicates that we will face scalability issue if we consider all deterministic allocations. In contrast, the full model achieves the highest revenue in all cases except for \(2 10\)(A), where it also performs comparably to the best result. Notice that, in Setting (C), \(=\) is coincidently the optimal solution due to the symmetry of the setting. The ablation results underscore the benefits of making the allocation menu, weights, and boost variables learnable, both for effectiveness and scalability. Moreover, even with significantly fewer learnable parameters, AMenuNetconsistently outperforms AMenuNetwith FCN, further highlighting the revenue advantages of a transformer-based architecture.

Out-of-Setting Generalization.The architecture of AMenuNet is designed to be independent of the number of bidders and items, allowing it to be applied to auctions with varying sizes. To evaluate such out-of-setting generalizability (Rahme et al., 2021), we conduct experiments whose results are

    & 2\(\)2(A) & 2\(\)10(A) & 3\(\)2(B) & 5\(\)2(B) & 2\(\)3(C) & 3\(\)10(C) \\   AMenuNet & **0.4293** & 2.3815 & **0.6197** & **0.7548** & **1.3107** & **5.5896** \\ With \(=\) & 0.4209 & **2.3848** & 0.6141 & 0.7010 & 1.2784 & 5.5873 \\ With \(=\) & 0.3701 & 2.2229 & 0.3892 & 0.3363 & 1.2140 & 5.4145 \\ With \(_{}\) & 0.3633 & & 0.5945 & 0.7465 & 1.2758 & - \\ With FCN & 0.4124 & 2.3416 & 0.5720 & 0.6963 & 1.2524 & 5.0262 \\   

Table 2: The revenue results of ablation study. For each case, we use the notation \(n m\) to represent the number of bidders \(n\) and the number of items \(m\). Some results of \(_{}\) are intractable because the menu size is too large.

Figure 2: Out-of-setting generalization results. We use the notation \(n m\) to represent the number of bidders \(n\) and the number of items \(m\). We train AMenuNet and evaluate it on the same auction setting, excepts for the number of bidders or items. For detailed numerical results, please refer to Appendix C.

shown in Figure 2. The experimental results demonstrate that the generalized AMenuNet achieves revenue comparable to that of Item-Myerson, particularly in cases with varying items where AMenuNet often outperforms Item-Myerson. This highlights the strong out-of-setting generalizability of AMenuNet. Furthermore, The fact that AMenuNet can generalize to larger settings enhances its scalability.

Winning Allocations.We conducted experiments to analyze the winning allocations generated by AMenuNet. In order to assess the proportion of randomized allocations, we recorded the ratio of such allocations in the last row of Table 0(a). Here, we define an allocation as randomized if it contains element in the range of \([0.01,0.99]\). The results indicate that randomized allocations account for a significant proportion, especially in larger auction scales. For instance, in settings such as \(2 10\)(A), \(3 10\)(A), and \(7 2\)(B), the proportion of randomized allocations exceeds \(17\%\). Combining these findings with the results presented in Table 2, we observe that the introduction of randomized allocations leads to an improvement in the revenue generated by AMenuNet.

Additionally, we present the top-\(10\) winning allocations based on their winning rates (i.e., allocations that are either \(A^{*}\) or \(A^{*}_{-k}\) for some \(k[n]\)) in Figure 3, specifically for the \(2 5\)(C) and \(5 5\)(C) settings. Notably, the top-\(10\) winning allocations tend to be deterministic, suggesting that AMenuNet is capable of identifying useful deterministic allocations within the entire allocation space. Furthermore, in the \(2 5\)(C) setting, we observed instances where the winning allocation was the empty allocation with a substantial boost. This can be seen as a reserve price, where allocating nothing becomes the optimal choice when all submitted bids are too small.

## 6 Conclusion and Future Work

In this paper, we introduce AMenuNet, a scalable neural network for the DSIC affine maximizer auction (AMA) design. AMenuNet constructs AMA parameters, including the allocation menu, bidder weights and boosts, from the public bidder and item representations. This construction ensures that the resulting mechanism is both dominant strategy compatible (DSIC) and individually rational (IR). By leveraging the neural network to compute the allocation menu, AMenuNet offers improved scalability compared to previous AMA approaches. The architecture of AMenuNet is permutation equivariant, allowing it to handle auctions of varying sizes and generalize to larger-scale auctions. Such out-of-setting generalizability further enhances the scalability of AMenuNet. Our various experiments demonstrate the effectiveness of AMenuNet, including its revenue, scalability, and out-of-setting generalizability.

As for future work, since we train AMenuNet using an offline learning approach, a potential direction is to explore online learning methods for training AMenuNet. Additionally, considering that the allocation menu size still needs to be predefined, it would be interesting to investigate the feasibility of making the allocation menu size learnable as well.

Figure 3: The top-\(10\) allocations (with respect to winning rate) and the corresponding boosts among \(100,000\) test samples.