# Suppress Content Shift: Better Diffusion Features

via Off-the-Shelf Generation Techniques

Benyuan Meng\({}^{1,2}\) Qianqian Xu\({}^{3,4}\)+ Zitai Wang\({}^{3}\)

Zhiyong Yang\({}^{5}\) Xiaochun Cao\({}^{6}\) Qingming Huang\({}^{5,3,7}\)+

\({}^{1}\)Institute of Information Engineering, CAS

\({}^{2}\)School of Cyber Security, University of Chinese Academy of Sciences

\({}^{3}\)Key Lab. of Intelligent Information Processing, Institute of Computing Technology, CAS

\({}^{4}\)Peng Cheng Laboratory

\({}^{5}\)School of Computer Science and Tech., University of Chinese Academy of Sciences

\({}^{6}\)School of Cyber Science and Tech., Shenzhen Campus of Sun Yat-sen University

\({}^{7}\)Key Laboratory of Big Data Mining and Knowledge Management, CAS

mengbenyuan@iie.ac.cn {xuqianqian,wangzitai}@ict.ac.cn

{yangzhiyong21,qmhuang}@ucas.ac.cn caoxiaochun@mail.sysu.edu.cn

Corresponding authors.

###### Abstract

Diffusion models are powerful generative models, and this capability can also be applied to discrimination. The inner activations of a pre-trained diffusion model can serve as features for discriminative tasks, namely, diffusion feature. We discover that diffusion feature has been hindered by a hidden yet universal phenomenon that we call content shift. To be specific, there are content differences between features and the input image, such as the exact shape of a certain object. We locate the cause of content shift as one inherent characteristic of diffusion models, which suggests the broad existence of this phenomenon in diffusion feature. Further empirical study also indicates that its negative impact is not negligible even when content shift is not visually perceivable. Hence, we propose to suppress content shift to enhance the overall quality of diffusion features. Specifically, content shift is related to the information drift during the process of recovering an image from the noisy input, pointing out the possibility of turning off-the-shelf generation techniques into tools for content shift suppression. We further propose a practical guideline named GATE to efficiently evaluate the potential benefit of a technique and provide an implementation of our methodology. Despite the simplicity, the proposed approach has achieved superior results on various tasks and datasets, validating its potential as a generic booster for diffusion features. Our code is available at this url.

## 1 Introduction

Diffusion models (DMs) [15; 34] are a prevalent family of generative models for various tasks [36; 35; 25]. This strong generative capability can be applied to discrimination . Diffusion Feature (DF), a popular approach, extracts inner activations from a pre-trained diffusion model as vision features [1; 50; 60; 57; 37; 62], similarly to how ResNet  serves as a feature extractor. Extracting features with a vastly pre-trained diffusion model grants this approach strong robustness and generalizability. Furthermore, it enjoys the philosophy of hitchhiking, a solid research paradigm in the age of large base models : advancements in diffusion models can all be transformed into better feature quality. It promises this research direction a bright future.

Have we already obtained satisfying diffusion features? The original role of diffusion models, generation, has provided an inspiring perspective. There is an endless pursuit among AIGC players2 for better control over generation [17; 58; 53; 28]. This implies the inherent difficulty in controlling diffusion models: their generation results may not be exactly the same as intended. Will this property of diffusion models also affect the quality of diffusion features? We visualize some diffusion features in Figure 1 and find that they do contain detail differences from inputs, which might hinder the performance. We name this phenomenon _content shift_, _i.e._, content differences between diffusion features and the input image. Although we have just shown an example with very obvious content shift, it is in fact intentionally amplified for observation. Under more practical feature extraction settings, the magnitude of content shift would be significantly weaker for visual perception. In Section 4, however, empirical results show that even in such cases, the negative impact of content shift on discrimination is not negligible, indicating the necessity to suppress it for diffusion features of better quality.

In pursuit of a method to suppress content shift, we need to further investigate why this phenomenon exists. We notice in Section 4 that the diffusion backbone reconstructs clean inner representations from noisy inputs in the middle of UNet before predicting noises based on the reconstructed content. The diffusion features we are using are in fact the reconstructed representations, which answers why we can obtain clean features from noisy images. However, since high-frequency details are potentially blurred out by noises and then recovered by "imagination", this reconstruction process inherently suffers from the risk of drifting from the original image. Content shift in diffusion features, naturally, is the reflection of such drift during reconstruction. Consequently, to suppress content shift, we need an additional way to directly introduce the original clean image into the reconstruction process and steer it towards the original image. To our delight, we notice that many off-the-shelf image generation techniques for diffusion models [58; 28; 53] also work by injecting additional information into UNet and thus steering the reconstruction. Through careful evaluation of the effect of generation techniques, we are able to select some techniques that can directly satisfy the goal of suppressing content shift, which eases the implementation and extension of our method. To guide the efficient evaluation of generation techniques, we also propose a guideline in Section 5. This method is denoted as _GenerAtion Techniques Enhanced_ (**GATE**) diffusion feature and its effect is also shown in Figure 1.

To validate GATE, we implement it (Section 5) by choosing three techniques from a few highly popular generation techniques. Since the integrated techniques can benefit from feature amalgamation [24; 62] more than previous approaches, this method is also adopted in our implementation. Although this is a very simple implementation, it achieves impressive performance on various tasks and datasets (Section 6). It demonstrates the effectiveness and potential of GATE, as more techniques can still be evaluated, and there will be even more as diffusion models develop.

In summary, the contribution of this work is as follows:

Figure 1: Current diffusion features widely suffer from content shift, _i.e._, content differences between inputs and features. Due to the inherent connection, content shift can be suppressed with off-the-shelf generation techniques.

* To the best of our knowledge, we are the first to reveal and systematically analyze the universal, harmful, yet hidden phenomenon, content shift, in diffusion features.
* We point out the possibility of utilizing off-the-shelf generation techniques for content shift suppression and propose the GATE guideline to facilitate technique integration.
* Comprehensive experiments on two discriminative tasks validate the effectiveness of our method.

## 2 Related Work

The introduction to diffusion models can be found in . In this section, we solely focus on diffusion features. So far, the research topics in this direction fall into two categories: **task exploration** and **method improvement**, _i.e._, applying the paradigm to different tasks and studying the approach itself for better feature quality, respectively.

**Task exploration.** There have been attempts on semantic segmentation , semantic correspondence , hyperspectral image classification , domain generalization , training data synthesizing , zero-shot referring image segmentation , visual grounding , a novel task involving personalization , co-salient object detection , and open-world segmentation . This extensive and extending list shows the discriminative capability of DMs.

**Method improvement.** The way to enhance diffusion features can be divided according to the three important factors of a diffusion model: prompt, layer, and timestep. In a basic pipeline, the prompt is manually designed and simple, the layer only refers to the activations between convolutional blocks, and the timesteps are manually set. (i) To enhance the use of prompt, a typical improvement is prompt tuning , which equivalently fine-tunes the text encoder along with the downstream discriminative task. Another novel method is auto-captioning , replacing manual prompt design with an automatic captioner. (ii) To dig more information out of UNet layers, researchers choose to additionally take attention layers into consideration. However, we find that cross-attention is more frequently used  while self-attention is less popular . (iii) As for better usage of timesteps,  proposes to extract features from many timesteps and dynamically assign weights to them, while  employs reinforcement learning for better timestep selection.

Of the two directions, our work aims for better methods instead of new tasks. Furthermore, most existing diffusion feature approaches suffer from content shift, an example of which is the visualization of attention features in Figure 1. Therefore, our GATE can serve as a generic performance booster to other diffusion feature approaches, showing its potential for broad application.

## 3 Preliminaries: Diffusion Feature

Diffusion models consist of a neural network module and a diffusion scheduler. The network is an end-to-end network, which can be formally denoted as \(_{}\), where \(\) is the parameters. The diffusion scheduler is the core of diffusion models. With the scheduler, diffusion models generate images progressively, during which the network module is **re-used** on each timestep, each time only predicting an incremental noise. Generally, we use a **smaller / larger timestep** to indicate **less / more noises**. A typical generation process starts from \(t=T\) (total noises) and ends at \(t=0\) (clean images).

Next, we will explain how features are extracted using a common pipeline for diffusion features, with the visual illustration in Figure 2. Given an input image \(x^{3 h w}\), where \(h,w\) are height and width, the extraction process includes: (i) A pre-trained VAE encodes the input image into the latent space, inducing \(x_{0}^{4 h^{} w^{}}\), as a common practice presented in . (ii) \(x_{0}\) acts as the input of the forward diffusion process, _i.e._, timestep \(t=0\). As suggested in , it is beneficial to extract diffusion features at non-zero timestep. Following this practice, we set \(t=50\) and get \(x_{t}\). (iii) \(x_{t}\), along with the timestep \(t\) and a textual prompt \(c\), is sent into the pre-trained diffusion UNet \(_{}\), _i.e._, \(_{}(x_{t},t,c)\). The generation techniques selected by our method are also applied here to modify \(,c\). (iv) Convolutional and attention features are gathered during the computation of the backbone. For convolutional features, we gather the output activations of each resolution in the upsampling stage of the UNet. For attention features, we obtain the mean value of the similarity maps between query and key in all cross-attention layers.

## 4 Exploration of Content Shift

### Impact of Content Shift

In Figure 1, we can qualitatively observe content shift from feature visualization. However, this visualization is intentionally amplified for better observation by extracting features at large timesteps. Now we aim to examine the impact of content shift on quantitative performance under more practical scenarios, _i.e._, when timesteps are small. To this end, we need to toggle the magnitude of content shift in features. Describing the quality of the image is a widely adopted way to control generation. A prompt accurately describing the quality of input images is considered to suppress content shift, while intentionally describing something different from the image will cause more severe content shift. We select the semantic segmentation task  as an example of high-quality images and image classification on CIFAR10  for low-quality images. The results in Figure 3 seem counter-intuitive at first glance, as low-quality prompts surprisingly can improve the performance on CIFAR10, but this in fact complies with the quality of input images. Therefore, these results can demonstrate the negative impact of content shift on feature quality at small timesteps.

Figure 3: The averaged results over three repeats with quality prompts. Horse-21 (high quality) and CIFAR10 (low quality) benefit from prompts closer to the image quality, suggesting the negative effect of content shift at small timesteps.

Figure 2: The overall process of feature extraction. The original image is first processed into UNet inputs via VAE and noise addition. Afterward, we collect the output activations of each resolution in the upsampling stage as convolutional features. At the same time, the cross-attention layers of UNet produce similarity maps, which are averaged over all upsampling layers as attention features.

### Cause of Content Shift

After the negative impact of content shift is confirmed, we next aim to find its cause. Unlike more conventional feature extractors such as ResNet , the inputs to diffusion models are not the original image (Figure 4(a)), but its noisy version (Figure 4(b)), as enforced by the diffusion process . The early layers of diffusion models even further add more noise to the feature maps (Figure 4(c)). However, the diffusion UNet gains the ability from vast pre-training to reconstruct clean inner representations from noisy inputs (Figure 4(d)), roughly at the middle of the UNet structure. Additionally, the shortcut structures in UNet also help the reconstruction by passing some high-frequency details. Afterward, the diffusion UNet will further predict noises based on the reconstructed representations (Figure 4(e)).

Notably, the diffusion features we are using are in fact the reconstructed representations, which answers why clean diffusion features can be obtained even though the inputs are noisy. Despite the reconstruction ability, however, many high-frequency details are potentially blurred out by input noises, and thus their reconstruction is mostly based on "imagination". This leads to possible drift from the original image during reconstruction . Naturally, the content shift phenomenon in extracted diffusion features reflects the drift during reconstruction. **Consequently, content shift is an inherent characteristic of diffusion models and diffusion features, which suggests its broad existence across models and timesteps**.

## 5 Suppression of Content Shift

### Utilization of Generation Techniques

According to the cause of content shift, we need to steer the reconstruction process back to the original image to suppress content shift. While it is viable to design new methods for this purpose, we find it also possible and more efficient to adopt off-the-shelf generation techniques. Specifically, as an inherent characteristic of diffusion models, content shift affects not only features but also the original generative purpose of diffusion models. Hence, there have been techniques for generation that are already capable of toggling content shift by steering reconstruction [58; 7]. For example, ControlNet  introduces an additional reference image and steers reconstruction by directly modulating activations, pushing the reconstructed representation towards the reference image. It inspires us to utilize ControlNet in a different way: we can set the same input image simultaneously as the reference image, enforcing the recovered image to be more similar to the input one and thus suppressing content shift. Similarly, it is possible to adopt other generation techniques to suppress content shift.

Furthermore, utilizing off-the-shelf generation techniques is more consistent with the intuition of diffusion feature than designing a new method. Specifically, this field is dependent on the generative capability of diffusion models, and thus it is important to stay updated with new advancements in the generation field. Utilizing techniques from the generation field helps with this goal, while a method that is newly designed solely for diffusion feature might hinder it. Considering it, we decide not to devise new methods, but to develop more detailed guidelines for suitably integrating these off-the-shelf generation techniques.

We next illustrate the guidelines for utilizing off-the-shelf generation techniques. In the field of generation, the abundant techniques may not all be suitable for suppressing content shift, suggesting the necessity of examining the effect of a given technique on feature quality. Although it is possible

Figure 4: Content shift is caused by the reconstruction process within diffusion model activations. This visualization consists of activations obtained during a single network forward pass.

to make empirical and quantitative examinations on discriminative tasks, a more efficient evaluation protocol would be preferable. To this end, we propose the GATE guideline for technique evaluation, which is presented in Figure 5 along with other components of the framework. To avoid having to integrate a technique into the feature extraction pipeline before knowing its potential, we utilize Img2Img generation as an alternative for evaluation:

1. Gather an Img2Img generation result as the reference image, done with high repainting strength to amplify content divergence for observation.
2. Apply the technique to be evaluated and perform another Img2Img generation. If the technique requires parameters, they should be set such that the output could be more similar to the input.
3. If the new output is more similar to the input than the reference image, the technique is considered able to suppress content shift in diffusion features.

### Quantitative Evaluation

We have also developed a quantitative metric for evaluating generation techniques. We set feature extracted at \(t=0\) as reference \(FEAT_{ref}^{c h w}\) as it is less affected by noises. Then we use the Laplacian operator to evaluate the contour difference between feature \(FEAT\) and the reference:

\[diff=_{i,j}^{h w}|\|Laplacian(FEAT_{ref},i,j)\|_{2} -\|Laplacian(FEAT,i,j)\|_{2}|(-,1]\] (1)

We then set a feature with stronger content shift as an anchor and compare \(diff_{anchor}\) with other features.

\[Score=-diff)}{diff_{anchor}}\] (2)

\(Score=1\) means an exact match, and a smaller value indicates more shift. In this way, we can measure the extent of content shift in extracted features using different generation techniques and thus evaluate the suppression effect of techniques. Noticeably, the evaluation of this quantitative metric can be well approximated by the previously proposed qualitative evaluation, so we recommend the qualitative evaluation if efficiency is desired.

### Selected Generation Techniques

We select three generation techniques as our implementation of GATE. Their Img2Img generation results are provided in Figure 6. Integrating the techniques only slightly impacts efficiency, which will be discussed in Appendix A along with technique details. Additionally, we analyze two failed

Figure 5: Overview of the GATE guideline and our implementation. GATE evaluates if a technique can suppress content shift based on the result of Img2Img generation. If a technique makes the result more similar to the input, it is considered to be potentially helpful. We further implement GATE by choosing three off-the-shelf generation techniques and amalgamating features obtained with different combinations of the techniques.

techniques in Appendix B, which might help better understand how generation techniques influence content shift.

**Fine-Grained Prompts.** Prompts are a description of expected image content in natural language. For example, "a single horse running in a sports field, with a well-equipped rider on its back, high quality, highly realistic, masterpiece". Modern diffusion models are inherently trained to generate images conditioned on the given prompts . Hence, by describing the content of the input image in prompts, it is possible to steer the reconstruction to stay close to the input. The result of fine-grained prompts in Figure 6 is not significantly better than the reference, but this technique enjoys the advantage of being a built-in function of diffusion models, requiring no code modification.

**ControlNet.** Quite often, the control via prompts is too ambiguous. This is when ControlNet  can be helpful. ControlNet is a plug-in module for diffusion models, designed to take an additional control image input and push the reconstructed representation toward the control image. ControlNet presents an outstanding control effect over generation among the three techniques, as shown in Figure 6, implying its potential for suppressing content shift.

**LoRA.** LoRA  is an efficient replacement for model fine-tuning, which bears a similar effect to fine-tuning, thus showing to be highly effective for capturing image styles. By injecting additional knowledge of image styles of the given dataset into model weights, LoRA enables reconstruction to end with stronger similarity to the input image. From Figure 6, we can observe that although LoRA does not provide as strong a control effect as ControlNet, it can significantly promote image quality, which is believed to bring some unique advantages.

### Feature Amalgamation

The three techniques above are able to improve feature quality individually and can be applied simultaneously for stronger suppression effects. While this can lead to a single high-quality feature, it is a common practice in previous diffusion feature approaches to amalgamate multiple features for further improvement . The conventional way extracts features at different timesteps to obtain more diverse information. We find that, compared to the conventional amalgamation of timesteps, the amalgamation of different combinations of generation techniques can bring stronger diversity, as indicated in Figure 7. Therefore, we additionally amalgamate features obtained with

Figure 6: Img2Img generation results according to the GATE guideline, validating the potential of the three selected techniques for suppressing content shift.

Figure 7: The first row shows features extracted at different timesteps. The second row is from different combinations of generation techniques and shows stronger diversity.

various generation technique combinations to harness further enhancement. More details of how feature amalgamation is implemented are provided in Appendix A.

## 6 Experimental Validation

### Experimental Settings

**Task & Dataset.** Typically, diffusion feature studies [1; 50; 60] prefer fine-grained pixel-level tasks for evaluation. Following this practice, we select three tasks for experiments: semantic correspondence using SPair-71k  dataset, label-scarce semantic segmentation using Bedroom-28  and Horse-21  datasets, and standard semantic segmentation using ADE20K  and CityScapes  datasets.

**Evaluation Metrics.** (i) For semantic correspondence, PCK@0.1img(\(\)) and PCK@0.1bbox(\(\)) are used, following the widely-adopted protocol reported in . (We omit @0.1 to save some space in Table 1.) These two metrics mean the percentage of correctly predicted keypoints, where a predicted keypoint is considered to be correct if it lies within the neighborhood of the corresponding annotation with a radius of \(0.1 max(h,w)\). For PCK@0.1img/PCK@0.1bbox, \(h,w\) denote the dimension of the entire image/object bounding box, respectively. (ii) For semantic segmentation, we use mIoU metric, which is the mean over the IoU performance across all semantic classes . For each image, IoU (Intersection over Union, \(\)) is defined by #(overlapped pixels between the prediction and the ground truth) / #(union pixels of them). In addition, we also use aAcc and mAcc, where aAcc is the classification accuracy of all pixels and mAcc averages the accuracy over categories.

**Feature Extraction.** All tasks extract features at \(t=50\). When ControlNet is applied, except for standard semantic segmentation, we additionally start multi-step denoising from \(t=60\). For feature amalgamation, we extract multiple convolutional features and one attention feature per image:

1. Semantic correspondence: We obtain six in total convolutional features using individual fine-grained prompt, ControlNet, and LoRA techniques, and one attention feature using a prompt including all object categories, with ControlNet and LoRA.
2. Label-scarce semantic segmentation: We obtain one convolutional feature using fine-grained prompts, one convolutional feature using ControlNet, and one (Bedroom-28) to two (Horse-21) features using different LoRA weights. One attention feature is extracted with all three techniques applied.
3. Standard semantic segmentation: One convolutional feature is obtained using only fine-grained prompts and two more are extracted additionally with ControlNet and different LoRA weights. One attention feature is extracted with all three techniques applied.

Notably, the ADE20K dataset for standard semantic segmentation contains images of varying scenes, which can test how well the fine-grained prompt technique can generalize in this scenario. To this end, we use a prompt that can cover different scenarios: "a highly realistic photo of the real world. It can be an indoor scene, or an outdoor scene, or a photo of nature. high quality". This prompt covers various scenes for generalizability and describes image quality for fine-grained effect.

For more experimental settings, including more detailed feature extraction methods and implementation details, please refer to Appendix C.

### Comparison with SOTA

The experimental results are shown in Table 1 and Table 2. For most SOTA competitors, we borrow the reported results from their original studies. However, MaskCLIP  and ODISE  only provide results on ADE20K and it is hard to extend their implementations to CityScapes, so their results on CityScapes are missing. Furthermore, the original results reported by VPD  are based on full-scale fine-tuning of diffusion UNet, which is not fair as we do not train the diffusion model. Therefore, we re-evaluated VPD with the diffusion UNet frozen and reported our results.

**Semantic Correspondence.** It is a pity that the related studies do not perform experiments under exactly the same setting. For fairness, we mainly compare GATE against a baseline method, which uses one feature extracted without any technique, under a unified setting. For reference, we still provide the results from four state-of-the-art methods: DINO  and DHPF  as non-DF methods, as well as DIFT  and DHF  as DF methods. The results are in the left part of Table 1.

**Semantic Segmentation.** The strong generalizability of a pre-trained diffusion model can ensure good discriminative performance even when labeled training data is scarce. For this scenario, we show the results in the right part of Table 1. The SOTA diffusion feature method for this setting, DDPM , serves as the major competitor. We also include other segmentation methods: DatasetGAN , DatasetDDPM, MAE , SwAV , GAN Inversion , GAN Encoder, VDVAE , and ALAE . We further validate our method on the more common setting of semantic segmentation using standard datasets: ADE20K  and CityScapes , with the results presented in Table 2. The competitors are MaskCLIP , ODISE , and VPD , where VPD is re-evaluated with the diffusion model frozen for fairness.

Generally, GATE outperforms competitors by large margins on all datasets. It demonstrates that previous diffusion feature approaches have been hindered by content shift, and GATE has a promising application as a generic booster for feature quality.

### Qualitative Analysis

In Figure 8, we provide feature visualization for qualitative analysis of GATE. The visualization is obtained using PCA analysis, reducing the channels of features to 3, which are regarded as RGB for visualization. We can observe: (i) The attention features become clearer and closer to the input image when more generation techniques are applied according to GATE, showing the suppression effect on content shift. (ii) Notably, for the second image where a person is riding a horse, the baseline attention feature fails to follow the instruction, _i.e._, attending only to the horse and ignoring the person. In contrast, generation techniques applied according to GATE help attention features attend to the correct object. (iii) From convolutional features, we can see the application of generation techniques brings stronger diversity.

    & _{}\)} & _{}\)} \\   & DINO & 51.68 & 41.04 \\  & DHPF & 55.28 & 42.63 \\   & DIFT & - & 52.90 \\  & DHF & 72.56 & 64.61 \\   & nn & 61.15 & 51.66 \\  & conv & 73.96 & **65.74** \\   & nn & 64.47 & 55.72 \\  & conv & 76.60 & 69.10 \\       Method & Bedroom-28 & Horse-21 \\  ALAE & 20.0 \(\) 1.0 & - \\ GAN Inversion & 13.9 \(\) 0.6 & 17.7 \(\) 0.4 \\ GAN Encoder & 22.4 \(\) 1.6 & 26.7 \(\) 0.7 \\ SwAV & 41.0 \(\) 2.3 & 51.7 \(\) 0.5 \\ SwAVw2 & 42.4 \(\) 1.7 & 54.0 \(\) 0.9 \\ MAE & 45.0 \(\) 2.0 & 63.4 \(\) 1.4 \\ DatasetGAN & 31.3 \(\) 2.7 & 45.4 \(\) 1.4 \\ DatasetDDPM & 47.9 \(\) 2.9 & 60.8 \(\) 1.0 \\ DDPM & **49.4 \(\) 1.9** & **65.0 \(\) 0.8** \\       Method & Bedroom-28 & Horse-21 \\  ALAE & 20.0 \(\) 1.0 & - \\ GAN Inversion & 13.9 \(\) 0.6 & 17.7 \(\) 0.4 \\ GAN Encoder & 22.4 \(\) 1.6 & 26.7 \(\) 0.7 \\ SwAV & 41.0 \(\) 2.3 & 51.7 \(\) 0.5 \\ SwAVw2 & 42.4 \(\) 1.7 & 54.0 \(\) 0.9 \\ MAE & 45.0 \(\) 2.0 & 63.4 \(\) 1.4 \\ DatasetGAN & 31.3 \(\) 2.7 & 45.4 \(\) 1.4 \\ DatasetDDPM & 47.9 \(\) 2.9 & 60.8 \(\) 1.0 \\ DDPM & **49.4 \(\) 1.9** & **65.0 \(\) 0.8** \\    
    &  &  &  \\  & & mIoU\(\) & aAcc\(\) & mAcc\(\) & mIoU\(\) & aAcc\(\) & mAcc\(\) \\   & MaskCLIP & 23.70 & - & - & - & - & - \\  & ODISE & 29.90 & - & - & - & - & - \\   & VPD & **37.63** & **79.16** & **50.08** & **55.06** & **90.14** & **68.96** \\  Ours & GATE & 40.51 & 79.68 & 54.90 & 64.20 & 92.83 & 76.98 \\   

Table 2: Results on the two standard semantic segmentation datasets, ADE20K and CityScapes. Red for the best result and blue for the runner-up.

### Ablation Study: Effect without Feature Amalgamation

For ablation study, we aim to evaluate the effect of selected techniques without feature amalgamation. The discriminative performance is shown at the bottom line of Figure 8, which is obtained on a single Horse-21 split instead of five random repeats for faster evaluation. We can observe: (i) Every individual technique can improve feature quality over baseline. (ii) When multiple techniques are applied simultaneously, stronger improvement can be obtained. This demonstrates that all three selected techniques can benefit feature quality, and their benefits can be combined together.

## 7 Conclusion and Future Work

In this paper, we reveal a phenomenon named content shift that has been causing degradation in diffusion features. Based on the analysis of its cause, we propose to suppress it with off-the-shelf generation techniques, which allows hitchhiking the advancements in generative diffusion models. This approach, while enjoying simplicity, is experimentally demonstrated to be generically effective.

However, the effectiveness of GATE relies on the selected techniques, for which we propose both a qualitative evaluation guideline and a quantitative metric. Though we selected three effective techniques and reported failed cases, there still is more to explore, which might potentially lead to more effective implementations. Furthermore, we only experimented with three tasks, so the full potential of GATE might remain under-explored.

Figure 8: Effect of GATE without feature amalgamation. Images with various scenes are shown for generalizability. In the second image, the attention feature is asked to focus on the horse and ignore the rider. The mIoU performance is on a single Horse-21 split, with red/blue for the best/worst.