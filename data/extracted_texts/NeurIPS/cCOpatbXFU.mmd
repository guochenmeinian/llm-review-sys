# Investigating Variance Definitions for

Stochastic Mirror Descent with Relative Smoothness

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Mirror Descent is a popular algorithm, that extends Gradients Descent (GD) beyond the Euclidean geometry. One of its benefits is to enable strong convergence guarantees through smooth-like analyses, even for objectives with exploding or vanishing curvature. This is achieved through the introduction of the notion of _relative smoothness_, which holds in many of the common use-cases of Mirror descent. While basic deterministic results extend well to the relative setting, most existing stochastic analyses require additional assumptions on the mirror, such as strong convexity (in the usual sense), to ensure bounded variance. In this work, we revisit Stochastic Mirror Descent (SMD) proofs in the (relatively-strongly-) convex and relatively-smooth setting, and introduce a new (less restrictive) definition of variance which can generally be bounded (globally) under mild regularity assumptions. We then investigate this notion in more details, and show that it naturally leads to strong convergence guarantees for stochastic mirror descent. Finally, we leverage this new analysis to obtain convergence guarantees for the Maximum Likelihood Estimator of a Gaussian with unknown mean and variance.

## 1 Introduction

The central problem of this paper is to solve optimization problems of the following form:

\[_{x C}f(x),f(x)=[f_{}(x)], \]

where \(C\) is a closed convex subset of \(^{d}\), and \(f_{}\) are differentiable convex functions (stochasticity is on the variable \(\)). The problems that we will consider typically arise from machine-learning use-cases, meaning that the dimension \(d\) can be very large. Therefore, first-order methods are popular for solving these problems, since they usually scale well with the dimension.

In standard machine learning setups, computing a gradient of \(f\) is very costly (or even impossible), since it requires computing gradients for all individual examples in the dataset. Yet, gradients of \(f_{}\) are relatively cheap, and arbitrarily high precisions are generally not required. This makes Stochastic Gradient Descent (SGD) the method of choice . Using a step-size \(>0\), the SGD update from point \(x^{d}\) can be written as \(x_{}^{+}=_{u C}\{ f_{}(x)^{}u+ \|u-x\|^{2}\}\).

While the standard Euclidean geometry leading to Gradient Descent (GD) fits many use-cases quite well, several applications are better solved with _Mirror Descent_ (MD), a generalization of GD which allows to better capture the geometry of the problem. For instance, the Kullback-Leibler divergence might be better suited to discriminating between probability distributions than the (squared) Euclidean norm, and this is something that one can leverage using MD with entropy as a mirror. As a matter of fact, many standard algorithms can be interpreted as MD, _i.e._, as generalized first-order methods. This is for instance the case in statistics, where Expectation Minimization and Maximum A Posterioriestimators can be interpreted as running MD with specific mirror and step-sizes . Mirror descent can also be used to solve Poisson inverse problems, which have many applications in astronomy and medicine , to reduce the communication cost of distributed algorithms , or to solve convex quartic problems . In the online learning community as well, many standard algorithms such as Exponential Weight Updates or Follow-The-Regularized-Leader can be interpreted as running mirror descent . There are still many open questions regarding the convergence guarantees for most of the algorithms mentioned above. Therefore, progress on the understanding of MD can lead to a plethora of results on these applications, and more generally to a more consistent theory for Majorization-Minimization algorithms. This paper is a stepping stone in this direction.

Let us now introduce the _mirror map_, or _potential_ function \(h\), together with the _Bregman divergence_ with respect to \(h\), which is defined for \(x,yh\) as \(D_{h}(x,y)=h(x)-h(y)- h(y)^{}(x-y)\). We now introduce the Stochastic Mirror Descent (SMD) update, which can be found in its deterministic form in, _e.g._, Nemirovskij and Yudin . SMD consists in replacing the squared Euclidean norm from the SGD update by the Bregman divergence with respect to the mirror map \(h\):

\[x^{+}(,)=_{u C}\{ f_{}(x)^{}u+D_{h}(u,x)\}. \]

Note that since \(D_{\|\|^{2}}(x,y)=\|x-y\|^{2}\), one can recover SGD by taking \(h=\|\|^{2}\). In this sense, SMD can be viewed as standard SGD, but changing the way distances are computed, and so the geometry of the problem. Yet, this change significantly complicates the convergence analysis of the method, since the Bregman divergence, _in general_: _(i)_ does not satisfy the triangular inequality, _(ii)_ is not symmetric, _(iii)_ is not translation-invariant, _(iv)_ is not convex in its second argument.

This means that analyzing mirror descent methods requires quite some care, and that many standard (S)GD results do not extend to the mirror setting. For instance, one can prove that mirror descent cannot be _accelerated_ in general . Similarly, applying techniques such as variance-reduction requires additional assumptions . To ensure that \(x^{+}(,)\) exists and is unique, we first make the following blanket assumption throughout the paper:

**Assumption 1**.: _Function \(h:^{d}\{\}\) is twice continuously differentiable and strictly convex on \(C\). For every \(y^{d}\), the problem \(_{x C}h(x)-x^{}y\) has a unique solution, which lies in \(C\), and all \(f_{}\) are convex._

Note that the regularity assumption on \(h\) could be relaxed, as discussed in Section 3, but we choose a rather strong one to make sure all the objects we will manipulate are well-defined. Interestingly, while mirror descent changes the way distances are computed to move away from the Euclidean geometry, standard analyses of mirror descent methods, and in particular in the online learning community, still require strong convexity and Lipschitz continuity with respect to norms [5, Chapter 4]. It is only recently that a _relative smoothness_ assumption was introduced to study mirror descent , together with the corresponding relative strong convexity.

**Definition 1**.: The function \(f\) is said to be \(L\)-relatively smooth and \(\)-relatively strongly convex with respect to \(h\) if for all \(x,y C\): \( D_{h}(x,y) D_{f}(x,y) LD_{h}(x,y)\). To lighten notation, we will omit the dependence on \(h\) and simply write that \(f\) is \(L\)-rel.-smooth unless clearly specified.

Definition 1 extends the standard smooth and strongly convex assumptions that correspond to the case \(h=\|\|^{2}\), so that for all \(x C\), \(^{2}h(x)=I\) the identity matrix. These assumptions allow MD analyses to generalize standard GD analyses, and in particular to obtain similar linear and sublinear rates, with constant step-size and conditions adapted to the _relative_ assumptions.

While the basic deterministic setting is now well-understood under relative assumptions, a good understanding of the stochastic setting remains elusive. In particular, as we will see in more details in the related work section, _all existing proofs somehow require the mirror \(h\) to be globally strongly convex with respect to a norm_, or have non-vanishing variance. The only case that can be analyzed tightly is under _interpolation_ (there exists a point that minimizes all stochastic functions), or when using Coordinate Descent instead of SMD . This is a major weakness, as the goal of relative smoothness is precisely to avoid comparisons to norms. Indeed, even when these "absolute" regularity assumptions hold, the smoothness and strong convexity constants are typically very loose, and the theory is not representative of the observed behaviour of the algorithms.

However, as hinted at earlier, this was expected: acceleration is notoriously hard to achieve for mirror descent (and even impossible in general ), and variance reduction typically encounters the same problems . For stochastic updates, this comes from the fact that it is impossible to disentangle the stochastic gradient from the effect of the curvature of \(h\) at the point at which it is applied.

**Contribution and outline.** The main contribution of this paper is to introduce a new analysis for mirror descent, with a variance notion which is provably bounded under mild regularity assumptions: typically, the same as those required for the deterministic case. We introduce our new variance notion, and compare it with standard ones from the literature in Section 2. This new analysis is both simpler and tighter than existing ones, as shown in Section 3. Finally, we use our results to analyse the convergence of the Maximum Likelihood and Maximum A Posteriori estimators for a Gaussian with unknown mean and variance in Section 4, and show that it is the first generic stochastic mirror descent analysis that obtains meaningful finite-time convergence guarantees in this case.

## 2 Variance Assumptions

We now focus on the various variance assumptions under which Stochastic Mirror Descent is analyzed. Some manipulations require technical lemmas, such as the duality property of the Bregman divergence or the Bregman co-coercivity lemma, which can be found in Appendix A.

We start by introducing our variance definition, prove a few good properties for it, and then compare it with the existing ones to highlight their shortcomings. The two key properties we would like to ensure (and which are not satisfied by other definitions) are: (i) boundedness without strong convexity of \(h\) or restricting the SMD iterates, and (ii) finiteness for \( 0\) (with the appropriate scaling).

### New variance definition

Let \(>0\), and recall that \(x^{+}(,)\) is the result of a SMD step from \(x\) using function \(f_{}\) with step-size \(\) (Equation (2)). From now on, when clear from the context, we will simply denote this point \(x^{+}\). Yet, although the dependence is now implicit, do keep in mind that \(x^{+}\) is a stochastic quantity that is not independent from \(\) nor \(\), as this is critical in most results. Under Assumption 1, \(x^{+}\) writes:

\[ h(x^{+})= h(x)- f_{}(x). \]

Similarly, we denote by \(}\) the deterministic Mirror Descent update, which is such that \( h(})= h(x)- f(x)\). We also introduce \(h^{*}:y_{x C}x^{}y-h(x)\) the convex conjugate of \(h\), which verifies \( h^{*}( h(x))=x\). Let us now define the key function

\[f_{}(x)=f(x)-[D_{h}(x,x^{+})]. \]

**Definition 2**.: We define the variance of the stochastic mirror descent iterates given by (2) as \(^{2}_{,}=_{x C}(f(x_{})-f_{} (x))=-f_{}^{*}}{}\), where \(f^{*}\) and \(f_{}^{*}\) are respectively the inf. of \(f\) and \(f_{}\).

We now state various bounds on \(^{2}_{,}\), to help understand its behaviour. We start by positivity, which is an essential property that justifies the square in the definition.

**Proposition 2.1** (Positivity).: _For all \(>0\), \(_{,} 0\)._

This result follows from \(f_{}(x) f(x)\), since \(D_{h}(x,x^{+}) 0\) for all \(x C\) by convexity of \(h\).

Stochastic functions after a step.We first upper bound \(^{2}_{,}\) directly in terms of \(f_{}\).

**Proposition 2.2**.: _If \(f_{}\) is \(L\)-rel.-smooth and \( 1/L\), then \(^{2}_{,}(f(x_{})-_{x C} [f_{}(x^{+})])\)._

Proof.: Since \(D_{h}(x,x^{+})= h(x^{+})- h(x),x^{+}-x-D_{h}(x^{+},x)\), then \(D_{h}(x,x^{+})=- f_{}(x)^{}(x^{+}-x)-D_{h}(x^{+},x)=(D _{f_{}}(x^{+},x)-f_{}(x^{+})+f_{}(x))-D_{h}(x^{+},x)\). The relative smoothness of \(f_{}\) and the step-size condition imply that \( D_{f_{}}(x^{+},x) D_{h}(x^{+},x)\), leading to \(D_{h}(x,x^{+}) f_{}(x)-f_{}(x^{+})\), and the result follows. 

This bound offers a new point of view on the variance, which can be bounded as the difference between the optimum of \(f\), and the optimum of a related function, in which we make one mirror descent step before evaluating each \(f_{}\).

**Finiteness.** Proposition 2.2 implies the following:

**Corollary 2.3**.: _If \(f_{}\) is \(L\)-relatively-smooth w.r.t. \(h\) and admits a minimum \(x_{}^{}\ C\) a.s., then for all \( 1/L\), \(_{,}^{2})-[f_{}(x_{}^ {})]}{}\). In particular, \(_{,}^{2}\) is finite._

This result directly comes from the fact that \(_{x C}[f_{}(x^{+})]\ \ [_{x C }f_{}(x^{+})]\ \ [f_{}(x_{}^{})]\). It shows that the standard regularity assumptions for the convergence of stochastic mirror descent guarantee that _the variance as introduced in Definition 2 remains bounded._ This is a strong result, that justifies the supremum in the variance definition. Indeed, **most other variance definitions require additional assumptions for the variance to remain bounded after the supremum**. Instead, we _globalize_ the variance definition, by taking the supremum over the right quantity to ensure that it remains bounded over the whole domain without having to explicitly assume it.

Note that the bound from Corollary 2.3 has already been investigating in other settings for stochastic optimization , as discussed in Section 2.2. While useful to show boundedness, this bound has a major drawback, which is that it explodes when the step-size \(\) vanishes. This does not reflect what happens in practice, which is why we investigate finer bounds on \(_{,}^{2}\).

Gradient norm at optimum.A usual way of formulating variance is to express it as the norm of the difference between stochastic gradients and the deterministic gradients. While the previous bounds highlight dependencies on the gradient steps (through evaluations at \(x^{+}\)), none of them really corresponds to "the size of the stochastic gradients at optimum". The key subtlety is that when using mirror descent, it is important to also specify the point at which these gradients are applied, and the following proposition gives a bound of this flavor on \(_{,}^{2}\). In this section, \(x_{}\) denotes the minimizer of \(f_{}\) when it exists and is in \(\ C\). Otherwise, unless explicitly stated, results involving \(x_{}\) can be replaced by a limit for \(x x_{}\).

**Proposition 2.4**.: _If \(f\) is \(L\)-rel.-smooth, \( 1/L\) and \(x_{}\ C\), \(_{,}^{2}}[D_{h}( ^{+}},x_{}^{+})]\)._

This can be considered as the Mirror Descent equivalent of \([\| f_{}(x_{})\|^{2}]\). Yet, a key difference is that stochastic gradients are evaluated at point \(x_{}\) instead of \(x_{}\), and \( f(x_{}) 0\) in general.

Proof.: For all \(x\), applying the duality property of the Bregman divergence leads to:

\[[D_{h}(x,x^{+})]=[D_{h^{*}}(  h(x^{+}), h(x))]=[D_{h^{*}}( h(x)-  f_{}(x), h(x))]\] \[=[D_{h^{*}}( h(x)- f(x), h(x ))]+[D_{h^{*}}( h(x)- f_{}(x), h (x)- f(x))]\] \[=[D_{h^{*}}( h(x)-[ f(x)-  f(x_{})], h(x))]+[D_{h}( },x^{+})],\]

where the last equality comes from the Bregman bias-variance decomposition Lemma . We then use the Bregman cocoercivity Lemma  to obtain: \([D_{h}(x,x^{+})] D_{f}(x,x_{})+ [D_{h}(},x^{+})]\). All these technical results can be found in Appendix A. In the end, \(f_{}(x) f(x_{})-[D_{h}(},x^{+})]\), and this is in particular true for \(x=x_{}\). 

Limit behaviour.A first observation is that both the \(D_{h}(x,x^{+})\) term in the definition of \(f_{}\) and our variance definition are scaled by \(^{-1}\). Yet, they remain finite when \( 0\). While this is clear in the Euclidean setting, this property holds more generally, as shown in the two following results.

**Proposition 2.5**.: _Let \(x C\) and \(_{0}>0\) s.t. \(D_{h}(x,x^{+}(_{0},))<\). Then, \(f_{}(x)f(x)\)._

Note that uniform convergence of \(f_{}\) to \(f\) would require that there exists \(>0\) such that \(_{x C}D_{h}(x,x^{+})\) is finite, which we cannot guarantee in general (it does not hold for \(f=g=\|\|^{2}\) defined on \(^{d}\) for instance). Denote \(\|x\|_{A}^{2}=x^{}Ax\), then:

**Proposition 2.6** (Small step-sizes limit).: _If \(f_{}\) are \(L\)-rel.-smooth and \(f\) has a unique minimizer \(x_{}\) and for some \(_{0}>0\), \(x_{}= f_{}(x)\) exists and is in \(C\) for \(_{0}\),_

\[_{ 0}_{,}^{2}=_{ 0}} [D_{h}(x_{}^{+},x_{})]= [\| f_{}(x_{})\|_{^{2}h(x_{})^{-1}}^{2}]. \]This variance is actually the best we can hope for in the Bregman setting, which indicates the relevance of Definition 2. Indeed, this term exactly correspond to the variance one would obtain when making infinitesimal SMD steps from \(x_{}\), _i.e._, the norm of the stochastic gradients at optimum in the geometry given by \(^{2}h(x_{})^{-1}\).

### Standard Assumptions

We now compare Definition 2 with several variance assumptions from the literature. Note that they typically "only" require the bounds to hold for all iterates over the trajectory. However, in the absence of proof that the iterates stay in certain regions of the space, suprema over the whole domain are required for all variance definitions.

**Euclidean case.** Let us now take a step back and look at the Euclidean case, \(h=\|\|^{2}\), and assume that \(f\) is \(L\)-smooth. Writing Equation (3) with this specific \(h\) and replacing \(x_{}\) by a supremum, we obtain \(^{2}_{,}_{x C}[\| f( x)- f_{}(x)\|^{2}]\), which is a common though debatable variance assumption. Indeed, it involves a maximum over the domain, and is in particular not bounded in general even for simple examples like Linear Regression. Yet, we can recover another standard variance assumption by assuming the smoothness of all \(f_{}\), which writes \(^{2}_{,}[\| f_{}(x_{})\|^{2}]\).

This result is obtained by writing that \(\| f_{}(x)\|^{2} 2\| f_{}(x)- f_{}(x_{})\|^{2}+2 \| f_{}(x_{})\|^{2}\), and bounding the first term using smoothness. In particular, we see that standard Euclidean variance definitions are natural bounds of \(^{2}_{,}\). Detailed derivations can be found in Appendix B.

**Divergence between stochastic and deterministic gradients.** An early variance definition for SMD in the relative setting comes from Hanzely and Richtarik , who define \(^{2}_{}\) as:

\[^{2}_{}=_{x C}[<  f(x)- f_{}(x),x^{+}-}>]=}_{x C}[D_{h}(x^{+},}) +D_{h}(},x^{+})],\]

where we recall that \(}\) is such that \( h(})= h(x)- f(x)\). We remark two main things when comparing \(^{2}_{}\) with Proposition 2.4: (i) \(^{2}_{,}\) is not symmetrized, and contains only one of the two terms, and (ii) the bound only needs to hold at \(x_{}\) instead of for all \(x C\). As a result, we directly obtain that \(^{2}_{,}^{2}_{}\), and \(^{2}_{}\) is actually infinite in most cases, whereas \(^{2}_{,}\) is usually finite, as seen above.

**Stochastic gradients at optimum.** Dragomir et al.  define the variance as:

\[^{2}_{DEH}=_{x C}}[D_{h^{}} ( h(x)-2 f_{}(x_{}), h(x))]=_{x C} [\| f_{}(x_{})\|^{2}_{^{2}h^{}(x(x))} ],\]

where \(z(x)[ h(x), h(x)- f_{}(x_{})]\) The main interest of this definition is that stochastic gradients are only taken at \(x_{}\). In particular, this variance is \(0\) in case there is interpolation (all stochastic functions share a common minimum). However, this quantity can blow up if \(h\) is not strongly convex, since in this case \(^{2}h^{}\) is not upper bounded (indeed, smoothness of the conjugate is ensured by strong convexity of the primal function ). Following similar derivations, but _after the supremum has been taken_, we arrive at:

**Proposition 2.7**.: _If \(f\) is \(L\)-relatively-smooth w.r.t. \(h\), then for \(<1/(2L)\) and some \(z_{}[ h(x_{}), h(x_{})- f_{}(x_{})]\), the variance can be bounded as \(^{2}_{,}[\| f_{}(x_{})\|^{2}_{ ^{2}h^{}(z_{})}]\)._

In particular, we obtain a finite bound without having to restrict the space.

**Functions variance.** Another variance definition that appears in the SGD literature is of the form \(f(x_{})-[f_{}(x_{}^{})]\), using the optima of the stochastic functions . Unfortunately, the results derived with this definition do not obtain a vanishing variance term when \( 0\), unlike most other variance definitions, and contrary to what is observed in practice, that smaller step-sizes reduce the variance. The vanishing variance term can be obtained by rescaling by \(1/\) (so considering \((f(x_{})-[f_{}(x_{}^{})])/\) instead), but this variance definition would explode for \( 0\). This is because using such a definition would come down to performing the supremum step within the expectation from Proposition 2.2, using that \(f_{}(x^{+}) f_{}(x_{}^{})\), which is a very crude bound. Instead, Corolary 2.3 directly shows that our variance definition is tighter than this one, and in particular (i) it is bounded for all \(>0\), (ii) it remains finite as \( 0\) even with the proper rescaling (Proposition 2.6).

**Relation to \(c\)-transform.** Mirror descent can be viewed as an alternate minimization method on transforms of \(f\). This point of view subsumes many methods, including the Newton Method or Mirror Descent. Central to their analysis is the notion of \(c\)-transform \(f^{c}(y)=_{x C}f(x)-c(x,y)\), a standard quantity from optimal transport . It turns out that for \( 1/L\), \(f_{}\) is actually linked to the \(c\)-transform as \(f_{}(x)=[f_{}^{c}(x^{+})]\), where we use the cost \(c(x,y)=D_{h}(x,y)\). Since \(f(x_{})=f^{c}(x_{})=_{x C}f(})\), denoting \(_{c}(g)=g^{c}( h^{*}( h(x)- g(x)))\), we have that \(_{,}^{2}=(_{x C}_{c}([f_{}])(x)-_{x C}[_{c}(f_{ })](x))\). We recognize the structure of a variance, as the difference between an operator applied to the expectation of a random variable, and the expectation of the operator applied to the random variable. Yet, compared to standard (Euclidean) analyses of SGD, it does not simply corresponds to the variance of the stochastic gradients (at optimum), and bears a more complex form.

In this section, we have highlighted the connections with other definitions, and argued that \(f_{}\) (and its minimum) is a relevant quantity. In particular, Definition 2 is the only definition that allows boundedness of the variance notion both after a supremum step over the iterates (and without strong convexity of \(h\)) and in the \( 0\) limit with the proper rescaling.

## 3 Convergence Analysis

Now that we have (extensively) investigated \(_{,}^{2}\), and the various interpretations that come from different bounds, we are ready to state the convergence results. Some proofs in this section are just sketched, but complete derivations can be found in Appendix C.

### Relatively Strongly Convex setting.

Recall that \(f_{}^{}=_{x C}f_{}(x)\). Starting from an arbitrary \(x^{(0)}\), the sequence \((x^{(k)})_{k 0}\) is built as \(x^{(k+1)}=(x^{(k)})^{+}\) for \(k\{0,T\}\) for some \(T>0\)

**Theorem 3.1**.: _If \(f\) is \(\)-relatively-strongly-convex with respect to \(h\), under a constant step-size \(\), the iterates obtained by SMD (Equation (3)) verify_

\[[[f_{}(x^{(T)})]-f_{}^{}]+ [D_{h}(x_{},x^{(T+1)})](1-)^{T+1}D_{h}(x _{},x^{(0)})+^{2}}{}. \]

Note that the (relatively) strongly-convex theorem has a standard form, and recovers usual MD results if we remove the variance, and standard SGD results if we take \(h=\|\|^{2}\).

Proof of Theorem 3.1.: We start from a variation of Dragomir et al. [7, Lemma 4]:

\[[D_{h}(x_{},x^{+})]-D_{h}(x_{},x)+  D_{f}(x_{},x)=-[f(x)-f(x_{})]+[D_{h}(x,x^{+} )] \] \[=[f(x_{})-(f(x)- [D_{h}(x,x^{+})])]=[f(x_{})-f_{}(x)]\] (8) \[=-[f_{}(x)-f_{}^{}]+[f(x_{ })-f_{}^{}]. \]

Using that \(D_{f}(x_{},x) D_{h}(x_{},x)\), and remarking that \(f(x_{})-f_{}^{}=_{,}^{2}\), we obtain:

\[[f_{}(x)-f_{}^{}]+[D_{h}(x_{}, x^{+})](1-)D_{h}(x_{},x)+^{2}_{,}^{2}. \]

At this point, we can neglect the \([f_{}(x)-f_{}^{}] 0\) terms and chain the inequalities for \(x=x^{(t)}\) for \(t\) from \(0\) to \(T\) to obtain the result. 

This proof is quite simple, and naturally follows from Lemma C.1. One can also note that _relative smoothness of \(f\) is not required to obtain Theorem 3.1_, which has no condition on the step-size. This is not a typo, but reflects the fact that _step-size conditions are needed to obtain a bounded variance_. Indeed, the variance as defined here entangles aspects tied with the error due to discretization (which is usually dealt with using smoothness), and the error due to stochasticity. This is natural, as the stochastic noise vanishes in the continuous limit (\( 0\)). Besides, the magnitude of the updates depends both on where the stochastic gradient is applied and on the step-size. Yet, the simplicity ofthe proof is partly due to this entanglement, meaning that we have deferred some of the complexity to the bounding of the variance term.

Also note that Theorem 3.1 uses constant step-sizes, but Equation (10) can be used with time-varying step-sizes, as is done for instance in the proof of Theorem 4.3. A variant of Theorem 3.1 in which the discretization error is partly removed from the notion of variance writes:

**Corollary 3.2**.: _Let \(f\) be \(\)-strongly-convex and \(L\)-relatively-smooth with respect to \(h\), and \(f_{+}^{}=_{x C}[f_{}(x^{+})]\). If \( 1/L\), the SMD iterates (Equation (3)) with constant step-size \(\) verify_

\[[[f_{}((x^{(T)})^{+})]-f_{+}^{}]+ [D_{h}(x_{},x^{(T+1)})](1-)^{T+1}D_{h}(x _{},x^{(0)})+[)-f_{+}^{}}{ }].\]

This alternate version is obtained using that \(f_{}(x)[f_{}(x^{+})]\), a key step from the proof of Proposition 2.2 (see (8)). In the deterministic case, \(f_{+}^{}=f(x_{})\), and we recover standard results.

### Convex setting.

Let us now consider the convex case, meaning that \(=0\).

**Theorem 3.3**.: _If \(f\) is convex, the iterates obtained by SMD using a constant step-size \(>0\) verify_

\[_{k=0}^{T}[f_{}(x^{(k)})-f_{}^{}+ D_{f}(x_{},x^{(k)})](x_{},x^{(0)})}{(T+1)}+ _{,}^{2}. \]

This theorem is obtained by summing Equation (9) for \(x=x^{(k)}\) for all \(k\{1,,T\}\) and rearranging the terms. Note that varying step-size results can be obtained in the same way.

This case differs from standard convex analyses, in that we obtain a control on \(f_{}(x^{(k)})-f_{}^{}+D_{f}(x_{},x^{(k)})\) instead of the usual \(f(x^{(k)})-f(x_{})\). One of the main consequences is that we cannot get a control on the average iterate since Bregman divergences are in general not convex in their second argument, and \(f_{}\) is not necessarily convex. This non-standard result is a direct consequence of our choice of variance definition, but it is actually a quantity that naturally arises in the analysis. Note that a variant involving \(f_{+}^{}\) can be obtained in the same lines as Corollary 3.2.

**Controlling \(f_{}\).** The results in this section do not directly control the function gap \(f(x)-f^{}\), but rather the transformed one \(f_{}(x)-f_{}^{}\). Yet, the continuity result (in \(\)) from Proposition 2.5 shows that the bounds we provide can still be interpreted as relevant function values for small \(\).

**Controlling \(D_{f}(x_{},x^{(k)})\).** An interesting property of \(D_{f}(x_{},x^{(k)})\) is that it can be linked with the size of the gradients of \(f\), as shown by the following result.

**Proposition 3.4**.: _If \( f(x_{})=0\) and \(f\) is \(L\)-relatively smooth with respect to \(h\) then for all \(x x_{}\), \(D_{f}(x_{},x) LD_{h}\)-\(( h(x_{})+, h(x_{}))>0\)._

This is a Bregman equivalent of controlling the gradient squared norm, with the additional benefit that the reference point at which we apply the gradient is the optimum \(x_{}\). Besides, Proposition 3.4 shows that \(D_{f}(x_{},x)>0\) for \(x x_{}\) without requiring \(f\) to be strictly convex (only \(h\)).

**Minimal assumptions on \(h\).** Note that the theorems in this section do not actually require \(h\) to satisfy Assumption 1, but only that iterations can be written in the form of Equation 3 (which is guaranteed by Assumption 1). While Assumption 1 allows for instance to use the Bregman cocoercivity lemma with any points, or ensures that \(^{2}h\) is well-defined, which we leverage extensively in Section 2, our theorems are much more general than this, and include applications such as proximal gradient mirror descent (next remark) or the MAP for Gaussian Parameters Estimation (next section).

**Stochastic Mirror Descent with a Proximal term.** Note that our results can be directly extended to handle a proximal term (similarly to the Euclidean proximal gradient algorithm), to handle composite objectives of the form \(f+g\) (and in particular projections, for cases in which \(g\) is the indicator of a convex set). More details can be found in Appendix E.

MAP For Gaussian Parameters Estimation.

So far, we have proposed new variance definitions for the analysis of stochastic mirror descent, and we have shown that they compare favorably to existing ones, while leading to simple convergence proofs. In this section, we investigate the open problem formulated by Le Priol et al. , which is to find non-asymptotic convergence guarantees for the KL-divergence of the Maximum A Posteriori (MAP) estimator. In particular, this example highlights the relevance of the infimum step on \(f_{}\), since it gives the first generic analysis that obtains meaningful finite time convergence rates.

### MAP and MLE of exponential families.

We now rapidly review the formalism of exponential families. More details can be found in Le Priol et al. , and Wainwright et al. [26, Chapter 3]. Let \(X\) be a random variable, and \(T\) a deterministic function, then the density of an exponential family for a sample \(x\) writes \(p_{}(x)=p(x|)=(,T(x)-A())\), where \(A\) is often refered to as the log-partition function. In this case, \(\) is called the natural parameter, and \(T\) is the sufficient statistic. Function \(A\) is convex, and we can thus establish a form of duality through convex conjugacy. The _entropy_ writes \(A^{*}()=_{^{}},^{}-A (^{})\). Parameter \(\) is called the _mean_ parameter, and the standard MAP estimator can be derived for \(n_{0}\), \(_{0}\) as \(_{}^{(n)}=^{(0)}+_{n=1}^{n}T(X_{n})}{n_{0}+n}\). The Maximum Likelihood Estimator (MLE) corresponds to taking \(n_{0}=0\). An interesting observation is that \(_{}^{(n)}\) can be obtained recursively for \(n>0\), as \(_{}^{(0)}=^{(0)}\), \(_{n}=(n+n_{0})^{-1}\), \(_{}^{(n+1)}=_{}^{(n)}-_{n} g_{X_{n}}( _{}^{(n)})\), with \( g_{X_{n}}()=-T(X_{n})\). In terms of primal variable \(^{(n)}= A^{*}(_{}^{(n)})\), the MAP writes:

\[ A(^{(n+1)})= A(^{(n)})- f_{X_{n}}(^{(n )}), \]

where \(f_{X_{n}}()=A()-,T(X_{n})\), so that \(f()=A()-,_{}\). We recognize stochastic mirror descent iterations, with mirror \(A\) and stochastic gradients \( f_{X}\). Similar results on the MLE can be obtained by taking \(n_{0}=0\). This key observation implies that **convergence guarantees on the MAP and the MLE can be deduced from stochastic mirror descent convergence guarantees**.

While this appears as an appealing way to obtain convergence guarantees for the MAP, Le Priol et al.  observe that none of the existing SMD results obtain meaningful rates for the convergence of the MAP for general exponential families. In particular, none of them recover the \(O(1/n)\) asymptotic convergence rate for estimating a Gaussian with unknown mean and covariance.

This is due to the variance definitions used in the existing analyses, that all have issues (not uniformly bounded over the domain, not decreasing with the step-size...) as discussed in Section 2. Our analysis fixes this problem, and thus yields finite-time guarantees for the MAP estimator for the estimation of a Gaussian with unknown mean and covariance. This shows the relevance of Assumption 2.

### Full Gaussian (unknown mean and covariance)

The main problem studied in Le Priol et al.  is that of the one-dimensional full-Gaussian case, where the goal is to estimate the mean and covariance of a Gaussian from i.i.d. samples \(X_{1},,X_{n}(m_{},_{})\), with \(_{}>0\). Note that although notation \(\) is usually reserved for the covariance matrix of a multivariate Gaussian, we use it for a scalar value here to highlight the distinction with \(_{,}^{2}\), the variance from stochastic mirror descent. In this case, the sufficient statistics write \(T(X)=(X,X^{2})\), and the log-partition and entropy functions are, up to constants, \(A()=^{2}}{4_{2}}-(-_{2})\), \(A^{*}()=-(_{2}-_{1}^{2})\), for \(=_{}^{*}\) and \(\{(u,v),u^{2}<v\}\). The goal is to estimate \(D_{A}(,_{})\), for which Le Priol et al.  show that only partial solutions exist: results are either asymptotic, or rely on the objective being (approximately) quadratic. Note that there is a relationship between natural parameters, mean parameters, and \((m,^{2})\), the mean and covariance of the Gaussian we would like to estimate. In the following, we will often abuse notations, and write for instance \(D_{A}(,)\) in terms of \((m,^{2})\) and \((,^{2})\) rather than \(\) and \(\). We now state a few results, for which detailed derivations can be found in Appendix F. More specifically:

\[D_{A}(,)=-(}{^{2}})-^{2}-^{2}}{2^{2}}+ -m)^{2}}{2^{2}}.\]The update formulas for the parameters are given by:

\[m^{+}=(1-)m+ X,(^{2})^{+}=(1-)[^{2}+(m-X)^{ 2}]. \]

Therefore, MAP iterations are well-defined although \(A\) does not verify Assumption 1.

**Proposition 4.1**.: _The iterations (12) are well-defined for \(<1\) in the sense that if \(^{(n)}=_{}^{}\), then \( A(^{(n)})- f_{X_{n}}(^{(n)})( A)\) almost surely, so that \(^{(n+1)}\) is well-defined almost surely. Besides, \(f_{}\) is \(1\)-relatively-smooth and \(1\)-relatively-strongly-convex with respect to \(A\)._

This result is a direct consequence of the fact that \(D_{f_{}}=D_{f}=D_{A}\) for all \(\), and the fact that \( A()- f_{X_{n}}()=(1-) A()+ T(X _{n})\{(u,v),u^{2}<v\}\) if \( A()\{(u,v),u^{2}<v\}\). Proposition 4.1 means that we can apply Theorem 3.1, so the next step is to bound the variance \(^{2}_{,}\),

\[f_{}()-f(_{})=[((1 -)(1+}{^{2}}))]-(^{2}}{^{2}}). \]

We now use this expression to to lower bound \(f_{}^{}\) and so upper bound \(^{2}_{,}\).

**Lemma 4.2**.: _Let \((m_{},_{}^{2})\) be the minimizer of \(f_{}\). Then, for \(<1/3\), \(m_{}=m_{}\), \(_{}^{2}_{}^{2}(1-3)_{}^{2}\). In particular, the variance \(^{2}_{,}\) verifies \(^{2}_{,}-\). For \(1/3< 1-\), \(^{2}_{,} c_{}\), where \(c_{}\) is a numerical constant that only depends on \(\)._

Note that we show in this example that \(_{}^{2}\) is arbitrarily close to \(_{}^{2}\) as \( 0\), which is expected.

**Theorem 4.3**.: _Let \( 0\) be a numerical constant and \(=0\) if \(n_{0}>3\). The MAP estimator satisfies:_

\[[D_{A}(_{},^{(n)})]D_{A}( _{},^{(0)})+(1+})+}{n+n _{0}}.\]

Numerical constants are not optimized. Theorem 4.3 gives an anytime result on the convergence of the MAP estimator for all \(n 0,n_{0} 1\) directly from the general SMD convergence theorem. Yet, the open problem from Le Priol et al.  is not completely solved still, as discussed below.

**Reverse KL bound.** We obtain a bound on \(D_{A}(_{},^{(n)})\), instead of \(D_{A}(^{(n)},_{})=f()-f(_{})\). \(D_{A}(^{(n)},_{})\) can be controlled asymptotically thanks to the bound on \(f_{}(^{(n)})-f_{}(_{})\), and \(f_{} f\) when \(=1/n 0\), but we might also be able to exploit this control over the course of the iterations.

**Asymptotic convergence.** Theorem 4.3 leads to a \(O(/n)\) asymptotic convergence rate instead of the expected \(O(1/n)\). This indicates that the \(f_{_{n}}(^{(n)})-f_{_{n}}^{}\) terms should not be neglected. Indeed, \(^{(n)}\) actually has a lot of structure in this example, since \( A(^{(n)})=_{k=1}^{n}T(X_{k})\). The SMD analysis is oblivious to this structure, hence the gap. Note that we can get rid of the \(\) factor and recover the right \(O(1/n)\) rate from the same analysis by using a slightly different estimator than the MAP (or MLE). This is done by setting the step-size as \(_{n}=\) for \(n>1\), and the analysis of this variant follows Lacoste-Julien et al. , as detailed in Appendix F.3.

**The special case of the MLE.** The MLE corresponds to \(n_{0}=0\), which is not handled in our analysis since the first step corresponds to \(=1\), which necessarily results in \(_{2}^{(1)}=-\) (which corresponds to \(^{2}=0\), as can be seen from (13)). If we consider that mirror descent is run from \(^{(1)}\), then we obtain \([D_{A}(_{},^{(1)})]=\) in general, where the expectation is over the value of the first sample drawn. Therefore, we need to start the SMD analysis at \(^{(2)}\) to fit the MLE into this framework, and in particular we need to be able to evaluate \([D_{A}(_{},^{(2)})]\). This is further discussed in Appendix F.4.

## 5 Conclusion

This paper introduces a new notion of variance for the analysis of stochastic mirror descent. This notion, based on the fact that a certain function \(f_{}\) admits a minimum, is less restrictive than existing ones, has the right asymptotic scaling with the step-size and is bounded regardless of the trajectory of the iterates without further assumptions.

We strongly believe that our analysis of SMD opens up new perspectives. As an example, we use our SMD results to show convergence of the MAP for estimating a Gaussian with unknown mean and covariance. As evidenced in Le Priol et al. , all existing generic analyses of stochastic mirror descent failed to obtain such results.