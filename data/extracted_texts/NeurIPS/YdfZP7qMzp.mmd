# GenRec: Unifying Video Generation and Recognition with Diffusion Models

Zejia Weng\({}^{1,2}\), Xitong Yang\({}^{3}\), Zhen Xing\({}^{1,2}\), Zuxuan Wu\({}^{1,2}\), Yu-Gang Jiang\({}^{1,2}\)

\({}^{1}\) Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University

\({}^{2}\) Shanghai Collaborative Innovation Center of Intelligent Visual Computing

\({}^{3}\) Department of Computer Science, University of Maryland

Corresponding author.

###### Abstract

Video diffusion models are able to generate high-quality videos by learning strong spatial-temporal priors on large-scale datasets. In this paper, we aim to investigate whether such priors derived from a generative process are suitable for video recognition, and eventually joint optimization of generation and recognition. Building upon Stable Video Diffusion, we introduce GenRec, the first unified framework trained with a random-frame conditioning process so as to learn generalized spatial-temporal representations. The resulting framework can naturally supports generation and recognition, and more importantly is robust even when visual inputs contain limited information. Extensive experiments demonstrate the efficacy of GenRec for both recognition and generation. In particular, GenRec achieves competitive recognition performance, offering 75.8% and 87.2% accuracy on SSV2 and K400, respectively. GenRec also performs the best on class-conditioned image-to-video generation, achieving 46.5 and 49.3 FVD scores on SSV2 and EK-100 datasets. Furthermore, GenRec demonstrates extraordinary robustness in scenarios that only limited frames can be observed. Code will be available at https://github.com/wengzejia1/GenRec.

## 1 Introduction

Diffusion models have achieved significant success in the field of image and video generation over the past few years. A variety of generative tasks have been revolutionized by using diffusion models trained on Internet-scale data, such as text-to-image generation , image editing , and more recently, text-to-video generation  and text&image-to-video generation . The excellent generative capabilities of diffusion models suggest that informative representation is learned during the generative training and strong visual priors are captured by the backbone models . Therefore, recent work has explored leveraging the image diffusion models for image understanding tasks, including image recognition , object detection , segmentation  and correspondence mining . However, the capability of _video diffusion_ models to effectively capture spatial-temporal information is not fully understood, and their potential for downstream video understanding tasks remains under-explored.

In this paper, we study the potential of video diffusion models , particularly the unconditioned or image-conditioned models, for video understanding by addressing the three key problems: (a) Does the backbone model trained for video generation extract effective spatial-temporal representations for semantic video recognition? (b) Can we retain the video generation capability by jointly optimizing generation and recognition? (c) Will such a unified training framework further benefit video understanding, especially in noisy scenarios where only limited frames are available .

While conceptually appealing, unifying video generation and recognition into a diffusion framework is non-trivial. Prior work either views the diffusion models as frozen feature extractors [9; 55; 39], or deconstructs them for new tasks while sacrificing their original generation capability . One major challenge comes from their distinct training and inference processes. Diffusion models are typically optimized using corrupted inputs, optionally augmented with a single conditioning frame, to achieve unconditioned or image-conditioned generation during inference [27; 1]. In contrast, video recognition models require access to multiple frames to reason about temporal relationships and expect clean inputs during inference [49; 51; 48]. Consequently, training a recognition model using corrupted videos and single-image conditions tends to suffer from inferior model optimization and a more significant training-inference gap.

To this end, we propose GenRec, a unified video diffusion model that enables joint optimization for video generation and recognition. Our model is built upon the open-source, image-conditioned Stable Video Diffusion model (SVD) , which encodes strong spatial-temporal priors by pretraining on large-scale image and video data. However, instead of conditioning on the same image across all video frames, we propose to condition on a random subset of frames while masking the remaining ones (see Figure 2). This simple random-frame conditioning process effectively bridges the gap between the learning processes of the two tasks. On the one hand, the generation capability of SVD is extended to handle arbitrary frame prediction, which provides more flexible and unambiguous video generation. On the other hand, conditioning on a random subset of frames allows the model to learn more discriminative and robust features for the recognition task. As shown in Figure 1, the model is jointly optimized using both generative supervision (_i.e._, noise prediction) and classification supervision.

We conduct extensive experiments to evaluate the performance of GenRec for both recognition and generation. Without sacrificing the generation capabilities, GenRec demonstrates competitive video recognition performance, offering 75.8% and 87.2% accuracy on SSV2 and K400, respectively. Furthermore, GenRec demonstrates extraordinary robustness in scenarios that only limited frames can be observed. For example, when only the front half of the video can be observed, GenRec achieves the 57.7% accuracy, which corresponds to 76.6% of the accuracy (75.3%) when the entire video is visible, demonstrating a higher accuracy retention ratio than other methods. By leveraging the recognition model for classifier guidance , GenRec also achieves superior class-conditioned image-to-video generation results, with FVD scores of 46.5 and 49.3 on the SSV2 and EK-100 datasets, respectively.

Figure 1: Comparison of classical pipelines for video classification and generation tasks with our proposed GenRec method. **(a) Classification**: Typical video classification focus on understanding complete videos. **(b) Diffusion Generation**: Diffusion models learn the noise reduction trajectory from videos with varying levels of noise. These two distinct training paradigms present challenges for task unification. To bridge this gap, we propose **(c) GenRec**: a learning framework that processes mask frames \(V_{M}\) using a masking function \(M()\) and noise videos \(V_{}\) with noise sampling \((,)\), aiming to simultaneously learn video understanding and content completion with the same partially observed visual content.

## 2 Preliminary

Representing the data distribution as \(p_{}()\) with a standard deviation of \(_{}\), we can obtain a family of smoothed distributions \(p(;)\) by adding independent and identically distributed Gaussian noise with standard deviation \(\). In the spirit of diffusion models, the generation process begins with a noise image \(_{N} N(0,_{}^{2})\) and iteratively denoises it at decreasing noise levels \(_{N}=_{}>_{N-1}>>_{0}=0\). The final denoised result \(_{0}\) is thus distributed according to the original data.

In the EDM  framework, the original \(_{0}\) will be diffused as:

\[_{}=_{0}+ N(0,),\] (1)

and the corresponding PF-ODE  follows:

\[d_{}=-_{} p_{}(_{})d,\] (2)

where \(_{} p_{t}(_{t})\) is the score function. Noise schedule \((t)\) is set as time step \(t\). The training objective is to minimize the \(L2\) loss with the denoised network \(D_{}\) for different \(\):

\[_{_{0} p_{data}}||D_{}(_{})- _{0}||_{2}^{2},\] (3)

with the relation between \(D_{}\) and the score function \(_{} p(;)\) as follows:

\[_{} p(;)=(D(_{})-)/^{2}.\] (4)

SVD  utilizes the EDM framework to perform generative training on large-scale video datasets, resulting in a high-quality video generation model. An image-to-video generation model capable of forecasting future frames given the first frame has been released. Following SVD method, we also process videos in latent space. Given an input video \(^{T H W 3}\), a pretrained VAE encoder is used to project it into the latent space frame by frame, resulting in the latent representation \(_{0}^{T h w D}\). We then build GenRec based on SVD, inheriting its strong spatial-temporal priors as foundation for the subsequent generation and classification tasks.

## 3 GenRec

We now introduce GenRec, a simple yet efficient framework, that can not only generate temporally-coherent videos conditioned on an arbitrary number of provided frames but also is able to recognize actions and events with the help of encoded spatial-temporal priors. To this end, GenRec explores the strong spatial-temporal priors learned by a video diffusion model. In this work, we instantiate GenRec with the powerful open-source Stable Video Diffusion model (SVD) , which is pretrained on large-scale video datasets and is able to produce a photo-realistic video when provided a single frame. Then, for generation, GenRec follows the classical EDM framework to learn noise reduction trajectories. For recognition, on the other hand, GenRec operates on intermediate decoded features using a recognition head. Furthermore, to generate videos in a more free fashion, _i.e._ an arbitrary collection of frames used as condition, we design a latent masking strategy that "interpolates" masked frames. Such a strategy also benefits recognition by easing the training process. More importantly, by doing so GenRec supports a multitude of downstream tasks, particularly when limited visual information is provided.

### Pipeline Overview

Latent diffusion and latent masking.During the diffusion process, the Gaussian noise with a certain noise level is added to the latent representation \(_{0}\), creating a noisy latent representation \(}_{i}\) following Equation1. Recall that while SVD contains powerful spatial-temporal priors, it can only perform generation when the first frame is provided. To allow a more "free" generation with an arbitrary number of frames as inputs, we design a latent masking strategy. More specifically, we apply a random mask \(\) to the latent representation \(_{0}\), producing a masked latent representation \(_{0}}\). Such a strategy encourages the model to reconstruct the original video content from incomplete frames, which is in a similar spirit to MAE . Note that when only the first latent is available, it degrades to the same as SVD; if all latents are masked out, this degrades to unconditional generation. Furthermore, doing so also benefits recognition tasks when limited visual clues are available. For example, in scenarios with limited bandwidth leading to reduced frame rates, the ability of videoframe complementation enables the model to better predict and perceive complete video information. In practice, we simulate such conditions by randomly erasing half to all video conditions, retaining on average only about one-fourth of the original video information. This technique allows the model to effectively fill in the missing information, enhancing its ability to recognize and understand the video content despite the reduced data availability.

Unifying generation and understanding.To unify generation with masked latents, GenRec predicts pixel-level and semantic-level contents with the combination of the noisy latent \(}_{i}\) and the masked latent \(_{0}}\) gained by the aforementioned latent diffusion and latent masking. The two latents are channel-wise concatenated \([}_{i},_{0}}]\) and are fed into a Spatial-Temporal UNet, together with features from observed frames, to learn spatial and temporal representations, following . The weights of the UNet are initialized from  to obtain spatial and temporal priors, learned on large-scale video datasets.

For the generation task, the UNet aims to reconstruct the original latent representation from the combined noisy and masked inputs. Representing UNet as the mapping function \(F_{}\), its goal is to predict clean latent, which, according to the EDM framework, takes the form of a representation mapping as follows:

\[D_{}(}_{i};_{0}},)=c_{skip}() }_{i}+c_{out}()F_{}([c_{in}()}_{i },_{0}}]),\] (5)

in which we set the same skip connection \(c_{skip}\), scaling factor \(c_{out}\) and \(c_{in}\) as .

For the recognition task, we break down the UNet mapping function as \(F=F_{tail} F_{head}\). And we consider \(F_{head}([c_{in}()},}])^{T h ^{{}^{}} w^{{}^{}} D^{{}^{}}}\) as the compact video representation extracted from the intermediate layer of the UNet model, which is then fed into the classifier head \(_{}\), consisting of an attentive pooler and a fully connected layer to predict video categories:

\[}=(F_{head}([c_{in}()}_{i},_{ 0}}])).\] (6)

### Optimization

We train GenRec with both generation and classification objectives, encouraging the model to learn high-quality video generation and accurate video understanding.

The generative loss uses a \(L2\) loss to measure the difference between the original latent representation and the reconstructed output produced by the UNet, and is defined as:

\[L_{G}(_{0},}_{i},_{0}};)=( )\|D_{}(}_{i};_{0}},)-_{0}\|^{2},\] (7)

Figure 2: The pipeline of our proposed video processing method. The input video is first processed by a pretrained encoder \(E\) to produce a latent representation \(_{0}\), then undergoes diffusion to generate a noisy latent \(}_{i}\). The random mask \(\) is used to create the masked latent \(_{0}}\). During training, the noisy latent is concatenated with the masked latent as condition and fed into a Spatial-Temporal UNet, resulting in both reconstruction and recognition outputs. The reconstructed latent can be decoded by the pretrained decoder \(D\) to produce the final generated video.

where \(D_{}(}_{i};_{0}},)\) is the denoised output mentioned in Equation5, and \(()\) is a weighting function based on the noise level \(\) referring to . While the classification loss uses a cross-entropy loss to measure the discrepancy between the true labels and the predicted labels, and is defined as:

\[L_{D}(,})=-_{i}_{i}( }_{i}),\] (8)

where \(\) denotes the ground truth labels, and \(}\) represents the predicted labels referring to Equation6.

To balance the learning of generative and recognition tasks, we set a balancing weight \(\) to control the relative importance of each loss in the overall objective function. The total loss \(L\) is given by:

\[L=L_{D}+ L_{G},\] (9)

### Inference for Different Downstream Tasks

With the above training strategies, we now introduce how GenRec can flexibly support different types of generation and recognition tasks.

Video generation conditioned on frames.Once trained, GenRec is able to generate high-quality videos conditioned on an arbitrary number of given frames, thanks to the latent masking strategy. Particularly, following the EDM stochastic sampler framework and Equation2, GenRec iteratively denoises the video conditioned on the masked latent \(_{0}}\), as shown below:

\[_{i-1} =}_{i}+_{}(}_{i}; _{0}})=}_{i}+(t_{i-1}-_{i})}_{i}}{d_{i}}\] (10) \[=}_{i}+(t_{i-1}-_{i})(-1)( }_{i};_{0}},)-}_{i}}{ _{i}},\] (11)

where \(}_{i}\) is derived from \(}_{i}\) adding a perturbation. With an iteratively denoising process, we can finally obtain the denoised video latent \(_{0}\) which can be decoded as a complete video.

Video generation conditioned on classes.When the number of visible frames is extremely limited, the motion trajectory becomes unpredictable and thus it would be hard to make a reliable prediction of the future. To mitigate this issue, GenRec supports adding category information to guide video generation in the expected desired direction.

Formally, we simplify Equation11 with Equation4, and obtain:

\[_{}(}_{i};_{0}}) =(t_{i-1}-_{i})(-1)(}_{i}; _{0}},)-}_{i}}{_{i}}\] (12) \[=(-1)(t_{i-1}-_{i})_{i}_{}_{i }} p_{}(}_{i}).\] (13)

We substitute the score function \(_{}_{i}} p_{}(}_{i})\) with the conditional form \(_{}_{i}} p_{}(}_{i}|y)\), in which \(y\) denotes the conditional class. By applying Bayes' Theorem, the original score function can be replaced by \(p(}_{i})p(y|}_{i})\), and we can get the conditional version of residual, denoted as \(_{}^{*}(}_{i};_{0}})\):

\[_{}^{*}(}_{i};_ {0}}) =(-1)(t_{i-1}-_{i})_{i}_{}_{i}}  p(}_{i})p(y|}_{i})\] (14) \[=(-1)(t_{i-1}-_{i})_{i}[_{}_{ i}} p(}_{i})+_{}_{i}} p(y|}_{i})]\] (15) \[=_{}(}_{i};_{0}} )-(t_{i-1}-_{i})_{i}_{}_{i}} p(y|}_{i})\] (16)

Considering the scaling factor of \(}_{i}\): \(c_{in}()=+_{data}}}\) (following , and \(_{i}=t_{i}\)), that would pre-scale the input as \(c(_{i})=c_{in}(t_{i})_{i}\) before model processing, the formulation can be further transferred as:

\[_{}^{*}(}_{i};_{0}})=_ {}(}_{i};_{0}})--_{i})_{i}}{_{i}^{\,2}+_{data}}}_{c(}_{i})} p(y|c(}_{i}))\] (17)

Following , we sharpen the distribution of \(p(y|)\) by multiplying a scaling factor \(s>1\), shown as \(s_{} p(y|)=_{} p(y|)^{s}\) where \(Z\) is an arbitrary constant. Larger scaling value would bring more attention to the target category. Here, \(p(y|c(}_{i}))\) comes from the classification branch in GenRec. Finally, we can use the same EDM sampling procedure with the derived class information to generate samples.

Standard video recognition.Based on Equation (6), GenRec can do the classical video recognition by setting constant no-mask, and thus \(_{0}}\) is replaced by \(_{0}\) and the prediction follows:

\[}=(F_{head}([c_{in}()}_{i},_{0}])).\] (18)

Video recognition with partially observed frames.Based on Equation (6), GenRec can be applied to video recognition with partially observed frames, _e.g._, early action prediction that aims to predict future events based on the initial frames, sparse video recognition where videos are sparsely encoded and transmitted due to bandwidth limitations. By masking the invisible frames to get \(}_{i}\), and replacing the noisy latent with random noise \(\) obeying Gaussian distribution, GenRec can do the prediction for partially visible videos, following:

\[}=(F_{head}([,_{0}}])).\] (19)

## 4 Experiments

### Experimental Setup

Datasets.In our experiments, we use the following four datasets: Something-Something V2 (SSV2) , Kinetics-400 (K400) , UCF-101  and Epic-Kitchen-100 (EK-100) . SSV2 dataset is designed for fine-grained action recognition and it contains 174 action classes, 220,847 short video clips with an average duration of 4 seconds. K400 contains 400 action classes, 306,245 video clips with an average duration of 10 seconds. The UCF-101 dataset comprises 13,320 videos from 101 action categories and is widely utilized for human action recognition. The EK-100 dataset focuses on egocentric vision. It contains a total of 90,000 annotated action segments, encompassing 97 verb classes and 300 noun classes.

Evaluation protocols.GenRec performs both generation and recognition tasks. For generation, we use the Frechet Video Distance (FVD)  metric to assess the quality of the generated videos. A lower FVD score indicates higher fidelity and realism. For recognition, we measure the top-1 accuracy that reflects the portion of correctly classified videos. We validate our model performance in formal video recognition, partial video recognition, class-conditioned image-to-video generation and frame completion with the above metrics.

Implementation details.We initially set the learning rate to \(1.0 10^{-5}\) and set the total batch size as 32. Only generation loss will be retained for model adaptation on specific datasets. We train 200k steps on EK-100 and UCF, and 300k steps on SSV2 and K400, respectively. Subsequently, we finetune GenRec with both generation and recognition losses. The learning rate is set to \(1.25 10^{-5}\) and decayed to \(2.5 10^{-7}\) using a cosine decay scheduler. We warm up models with 5 epochs, during which the learning rate is initially set as \(2.5 10^{-7}\) and linearly increases to the initial learning rate \(1.25 10^{-5}\). The loss balance ratio \(\) is set to 10, and the learning rate for the classifier head is ten times higher than the base learning rate. We drop out the conditions 10% of the time for supporting classifier-free guidance , and we finetune on K400 for 40 epochs and 30 epochs on other datasets. The training is executed on 8 A100s and each contains a batch of 8 samples. We sample 16 frames for each video.

### Main Results

Comparison to state-of-the-art in video recognition and video generation.We compare with state-of-the-art methods in terms of their recognition accuracy and generation quality. The results are summarized in Table 1. The first two blocks of the table presents current advanced video recognition models, while the third block demonstrates the performance of the diffusion-based class-guided image-to-video generation.

As shown in the table, GenRec achieves optimal results or performs on par with the state-of-the-art approaches. In terms of video recognition, GenRec achieves 75.8% accuracy on SSV2 dataset,surpassing the majority of current state-of-the-art methods. On K400, GenRec achieves 87.2% accuracy, which is on par with the performance of MVD-H (87.2%) and Hiera (87.3%, 87.8%), and surpasses other advanced methods. These results indicate the effectiveness of our approach in video recognition. In addition, GenRec shows a slight performance gap compared to the methods in the second block. It is important to note that these advanced methods benefit significantly from pretraining on large-scale multimodal alignment datasets, which provide extensive cross-modal supervision that enhances their ability to capture semantic relationships across video frames.

We further construct two strong baselines. Baseline I adapts SVD to the respective dataset through generative fine-tuning, followed by attentive-probing for classification, where the backbone is frozen and all frames are used as input. Baseline II involves fully fine-tuning the original SVD model with classification supervision only, ensuring that all frames are visible during training. Compared with them, GenRec performs on par or better. GenRec performs good in supporting not only classification but also generation, demonstrating its comprehensive capability in handling both tasks effectively.

In terms of video generation, we evaluate the model on class-conditioned image-to-video generation task following . Comparing the FVD scores of SEER:112.9 and SEER\(\):355.4, it can be inferred that generating longer videos with 16 frames is more difficult than generating 12 frames. GenRec generates videos with 16 frames and achieves much lower FVD scores than the other methods, demonstrating the effectiveness of our approach in video generation.

It is worth highlighting that, current research always treats video recognition and generation tasks in a separate manner, and most of the advanced methods focus primarily on either recognition or generation tasks. For instance, SEER method excels in class-conditioned image-to-video generation, but lacks the ability to do video recognition. While current research on representation learning, shown as the first and second blocks in Table 1, lacks the ability to do video generation tasks. In contrast, GenRec not only unifies these tasks, but also achieves competitive results compared to the specialized methods.

    & &  &  \\ 
**Method** & **Resolution** & **Param.** & **SSV2** & **K400** & **SSV2** & **EK-100** \\  _w/o multi-modal align._ & & & & & & \\ VideoMAE-L  & 224\(\)224 & 305M & 74.3 & 85.2 & - & - \\ VideoMAE-H  & 224\(\)224 & 633M & - & 86.6 & - & - \\ OmniMAE-H  & 224\(\)224 & 650M & 75.5 & 85.4 & - & - \\ MVD-H  & 224\(\)224 & 633M & 77.3 & 87.2 & - & - \\ Hiera-L  & 224\(\)224 & 214M & 75.1 & 87.3 & - & - \\ Hiera-H  & 224\(\)224 & 673M & - & 87.8 & - & - \\ MaskFeat-L  & 312\(\)312 & 218M & 75.0 & 86.4 & - & - \\  _w/ multi-modal align._ & & & & & & \\ InternVideo  & 224\(\)224 & 1.3B & 77.2 & 91.1 & - & - \\ InternVideo2  & 224\(\)224 & 6B & 77.4 & 92.1 & - & - \\ OmniVec  & - & - & 85.4 & 91.1 & - & - \\ OmniVec-2  & - & - & 86.1 & 93.6 & - & - \\  TATS  & 128\(\)128 & - & - & - & 428.1 & 920.0 \\ MCVD  & 256\(\)256 & 3.5B & - & - & 1407 & 4804 \\ SimVP  & 64\(\)64 & - & - & - & 537.2 & 1991 \\ VideoFusion  & 256\(\)256 & 1.8B & - & - & 163.2 & 349.9 \\ Tune-A-Video  & 256\(\)256 & 860M & & & 291.4 & 365.0 \\ SEER  & 256\(\)256 & 860M & - & - & 112.9 & 271.4 \\ SEER\(\) & 256\(\)256 & 860M & - & - & 355.4 & - \\  Baseline I & 256\(\)256 & 2.1B & 63.7 & 82.0 & 50.3 & 53.6 \\ Basline II & 256\(\)256 & 1.9B & 75.9 & 86.6 & - & - \\ GenRec & 256\(\)256 & 2.1B & 75.8 & 87.2 & 46.5 & 49.3 \\   

Table 1: Performance of Video Recognition and Generation Methods. We evaluate on video recognition and class-conditioned image-to-video generation tasks. SEER\(\) predicts 16 frames, while others predict 12 frames. Top-1 accuracy and FVD scores are reported. Baseline I adapts SVD to datasets with generative fine-tuning and then uses attentive-probing for classification. Baseline II fully finetunes SVD with classification supervision only in traditional classification framework.

Comparison to state-of-the-art in video recognition with limited frames.GenRec supports video recognition when only partial frames can be observed. We evaluate this capability on the SSV2 and EK-100 datasets. Our evaluation includes two tasks: an early prediction task, where the model has access only to previous continuous frames following the setting of , and a recognition task where videos are sparsely sampled, and the model is expected to make correct predictions. For fair comparisons, we construct two strong baselines. We first apply MVD  to directly deal with the recognition task by constructing a dense video through nearest neighbor interpolation. We also construct another baseline similar to our training pipeline, where we apply frame dropout in the training process of MVD  for better fitting on task with partial frames, and is named as MVD\(\). In all settings, the number of fully observed frames is 16.

Table 2 shows the results under these settings, in which \(\) denotes the visible ratio (there are a total of 16 frames). In the early action prediction task, GenRec achieves the highest accuracy and ratio metrics at all observation levels. Notably, GenRec and MVD\(\) exhibit similar performance when all frames are observed, but as the number of observed frames decreases, GenRec demonstrates higher accuracy. GenRec also shows superior performance when videos are sparsely sampled, maintaining high accuracy even with fewer observed frames (e.g., 55.7% for 2 frames and 70.8% for 4 frames), indicating its robustness in handling sparse data. Moreover, we compute the ratio metric representing the percentage of maximum performance that the model can maintain at various frame rates, mitigating the unfairness caused by different backbone networks. In this scenario, GenRec still achieves the best performance.

We further investigate the contributions of generation supervision for recognition. As seen in Table 2, removing generation supervision results in noticeable performance degradation across various tasks, especially when the number of visible frames get less. For example, in the early prediction task, the accuracy decreases by 0.3% at \(=1.0\) and by 2.3% at \(=0.3\). These results suggest that generation supervision is essential for maintaining high performance, particularly when the model has to make

    & &  &  \\ 
**Method** & **Metric** & 0.1 & 0.3 & 0.5 & 0.7 & 1.0 & 2 f\% & 3 f\% & 4 f\% & 16 f\% \\   & Accuracy & 20.5 & 28.6 & 41.2 & 47.1 & 66.3 & - & - & - & - \\  & Retention & 30.9\% & 43.1\% & 62.1\% & 71.3\% & 100\% & - & - & - & - \\   & Accuracy & - & - & - & - & - & 34.2 & 54.3 & 64.4 & 75.0 \\  & Retention & - & - & - & - & - & 45.6\% & 72.4\% & 85.9\% & 100\% \\  MVD\(\) & Accuracy & 26.9 & 39.8 & 55.6 & 70.2 & 75.0 & 53.6 & 65.0 & 68.8 & 75.0 \\  & Retention & 35.9\% & 53.1\% & 74.1\% & 93.2\% & 100\% & 71.5\% & 86.7\% & 91.7\% & 100\% \\   & Accuracy & 27.3 \% & 39.6 \% & 23.3 & 56.31\% & 71.61\% & 75.00\% & 53.51\% & 65.81\% & 69.81\% & 75.01\% \\  GenRec & Accuracy & **28.9** & **41.9** & **57.7** & **72.4** & 75.3 & **55.7** & **67.3** & **70.8** & 75.3 \\  & Retention & **38.4\%** & **55.6\%** & **76.6\%** & **96.1\%** & 100\% & **74.0\%** & **89.4\%** & **94.0\%** & 100\% \\   

Table 2: Early action prediction and limited interpolation problem on Something-Something V2 dataset, with one temporal crop. \(\) denotes the visible ratio according to the whole video. Acc denotes to the top-1 accuracy. Ratio metric represent the percentage of maximum performance that the model can maintain at various frame rates. The ‘w/o G’ experiment refers to the results obtained by removing the generative supervision from our method.

Figure 3: Early action prediction on EK-100 and UCF-100 datasets, with one temporal crop.

predictions with limited visual information. By incorporating generation supervision, the model can better handle scenarios with incomplete data, improving robustness and accuracy.

We also evaluate the early action prediction on EK-100 and UCF-101. EK-100 is a temporally sensitive dataset similar to SSV2, demanding in terms of the model's temporal modeling capability, while UCF-101 demands more on appearance modeling. We conduct early prediction evaluation on them to further reveal the robustness of our GenRec. As shown in Figure 3, GenRec clearly outperforms TemPr and MVD\(\). In particular, the improvement becomes more significant as the number of observed frames decreases. More evaluation results can be seen in Appendix A.1.

These results collectively demonstrate that GenRec effectively handles missing video frames. The robustness and high accuracy of GenRec across different datasets and observation ratios highlight its potential for real-world applications where video data might be incomplete or sparsely sampled.

The relationships between generation and recognition.We further investigate the consistency between video generation and recognition, as shown in Table 3. We evaluate the performance of video recognition and generation with limited frames and find that the recognition accuracy not only depends on the number of visible frames but also significantly on the location of these frames. Interestingly, uniform sampling appears to facilitate video recognition better than dense sampling from the video prefix. Specifically, with the same number of frames, early prediction consistently shows lower accuracy compared to uniformly sampled frames (e.g., 28.9% vs. 55.7% with 2 frames) and worse FVD scores (e.g., 57.8 vs. 46.7 with 2 frames). When only three interpolated frames are visible, the 31.7 FVD score is comparable to that of an eight-frame prefix (30.3), while achieving much higher recognition accuracy. These results highlight the importance of complete state observation for action recognition and also suggest that video generation performance can potentially reflect task difficulty.

Choice of UNet layers.As described in Section 3, the UNet mapping function \(F\) is decoupled into \(F_{tail} F_{head}\), where \(F_{head}\) serves as the feature extractor for video recognition. Our UNet model contains 4 main up-sampling blocks. We investigate which one is best suited for recognition. As shown in Table 4, using the second up-sampling block (Up Index 2) yields the best performance with an accuracy of 75.8%. The third block (Up Index 3) followed with 75.2%, while the first block (Up Index 1) has the lowest accuracy. As such, we choose the second block for feature extraction.

Explore the influence of the masking strategy.We also conduct an ablation study on the masking schemes using different expected masking ratios, as shown in Table 5. The results show that the FVD scores remain similar across different ratios, and a larger masking ratio might be beneficial for generation, as it closely resembles our class-conditioned frame prediction scenario with one or two given frames. However, an excessively large masking ratio (87.5%) negatively impacts action recognition accuracy, leading to a 0.9% decrease compared to our selected ratio.

Noise incorporation during inference for video recognition.In the inference stage for action recognition, GenRec applies a specific level of noise to video inputs before extracting visual features, as formulated in Equation 18. This added noise helps maintain consistency with the noisy training process. To further understand the influence of noise randomness on classification accuracy, we conducted an experiment on the SSV2 dataset, using multiple random seeds to generate the noise, as presented in Table 6. The results demonstrate remarkable consistency in accuracy across different random seeds, with a standard deviation of only 0.0125%. This minimal variation highlights the model's robustness to noise fluctuations during inference, suggesting that the model's performance remains stable despite noise introduced by random sampling. If fully deterministic outcomes are desired, fixing the random seed for noise sampling will eliminate any remaining variability and guarantee consistent predictions across runs.

## 5 Related Work

Video diffusion models for generation.The great success of diffusion models in image generation has led to rapid advancements in video generation, including text-to-video generation [15; 2; 52; 54], image&text-to-video generation [56; 18; 21], and video editing [4; 5; 26; 29; 12; 53]. Many current works [52; 18] adapts the diffusion models from images to videos by incorporating temporal convolutions and attention mechanisms. One typical and excellent work, Stable Video Diffusion , follows the above description and has provided valuable foundations for generating high-quality, diverse, and temporally consistent videos. Different from the previous work, in our paper, we pursue not only the quality of generation, but also the unity of model generation capability and classification ability.

Diffusion models for visual understanding.Recently, researchers start to uncover the significance of diffusion models for discrimination tasks. A notable approach involves utilizing pretrained visual diffusion models for various downstream tasks, such as image segmentation  and visual content correspondence . Additionally, some studies treat diffusion learning as a self-supervised method to acquire valuable feature representations . However, most current works either use stable diffusion networks as pretrained backbones for downstream tasks or completely destroy their generative capabilities. Consequently, the potential benefits of integrating generation and classification abilities into a single model remain under-explored, which is the primary focus of our paper.

## 6 Conclusion

In this work, we presented GenRec, a unified video diffusion model that enables joint optimization for both video generation and recognition. GenRec exploits the significant temporal modeling power embedded in the diffusion model, allowing for mutual reinforcement between generation and recognition tasks. Extensive experiments were conducted to evaluate the performance of GenRec, demonstrate our approach contains strong generation and recognition capabilities at the same time in different kinds of scenarios, including normal or partial video recognition, video completion and class-conditioned image-to-video generation. Our findings highlight the potential of combining generation and classification tasks within a single unified model, providing valuable insights into the development of more sophisticated and versatile video analysis models. Future work will focus on further refining this integration and exploring its applications across various real-world scenarios.