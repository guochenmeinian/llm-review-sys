# Even Sparser Graph Transformers

Hamed Shirzad

University of British Columbia

shirzad@cs.ubc.ca

&Honghao Lin

Carnegie Mellon University

honghaol@andrew.cmu.edu

&Balaji Venkatachalam

Meta

bave@meta.com

&Ameya Velingker

Independent Researcher

ameyav@gmail.com

&David P. Woodruff

CMU & Google Research

dwoodruf@cs.cmu.edu

&Danica J. Sutherland

UBC & Amii

dsuth@cs.ubc.ca

Work done in part while at Google.

###### Abstract

Graph Transformers excel in long-range dependency modeling, but generally require quadratic memory complexity in the number of nodes in an input graph, and hence have trouble scaling to large graphs. Sparse attention variants such as Exphormer can help, but may require high-degree augmentations to the input graph for good performance, and do not attempt to sparsify an already-dense input graph. As the learned attention mechanisms tend to use few of these edges, such high-degree connections may be unnecessary. We show (empirically and with theoretical backing) that attention scores on graphs are usually quite consistent across network widths, and use this observation to propose a two-stage procedure, which we call Spexpromer: first, train a narrow network on the full augmented graph. Next, use only the active connections to train a wider network on a much sparser graph. We establish theoretical conditions when a narrow network's attention scores can match those of a wide network, and show that Spexpromer achieves good performance with drastically reduced memory requirements on various graph datasets. Code can be found at https://github.com/hamed1375/Sp_Exphormer.

## 1 Introduction

The predominant story of the last half-decade of machine learning has been the runaway success of Transformer models (Vaswani et al., 2017), across domains from natural language processing (Vaswani et al., 2017; Devlin et al., 2018; Zaheer et al., 2020) to computer vision (Dosovitskiy et al., 2020) and, more recently, geometric deep learning (Dwivedi and Bresson, 2020; Kreuzer et al., 2021; Ying et al., 2021; Rampasek et al., 2022; Shirzad et al., 2023; Muller et al., 2023). Conventional ("full") Transformers, however, have a time and memory complexity of \((nd^{2}+n^{2}d)\), where \(n\) is the number of entities (nodes, in the case of graphs), and \(d\) is the width of the network. Many attempts have been made to make Transformers more efficient (see Tay et al. (2020) for a survey on efficient transformers for _sequence modeling_). One major line of work involves _sparsifying_ the attention mechanism, constraining attention from all \((n^{2})\) pairs to some smaller set of connections. For instance, for sequential data, BigBird (Zaheer et al., 2020) constructs a sparse attention mechanism by combining sliding windows, Erdos-Renyi auxiliary graphs, and universal connectors. On the other hand, for graph data, Exphormer (Shirzad et al., 2023) constructs a sparse interaction graph consisting of edges from the input graph, an overlay expander graph, and universal connections. We refer to such a network as a _sparse attention network_.

Exphormer reduces each layer's complexity from \((nd^{2}+n^{2}d)\) to \(((m+n)d^{2})\), where \(n\) is the number of nodes, \(m\) is the number of interaction edges in the sparse attention mechanism, and \(d\) isthe hidden dimension or width. Even so, training is still very memory-intensive for medium to large scale graphs. Also, for densely-connected input graphs with \((n^{2})\) edges, there is no asymptotic improvement in complexity, as Exphormer uses all of the \((n^{2})\) edges of the original input graph. Our goal is to scale efficient graph Transformers, such as Exphormer, to even larger graphs.

One general approach for scaling models to larger graphs is based on batching techniques. Prominent approaches include egocentric subgraphs and random node subsets (Wu et al., 2022, 2023, 2024). Egocentric subgraphs choose a node and include all of its \(k\)-hop neighbors; the expander graphs used in Exphormer, however, are exactly defined so that the size of these subgraphs grows exponentially in the number of layers - prohibitively expensive for larger graphs. A similar issue arises with universally-connected nodes, whose representation depends on all other nodes. For uniformly-random subset batching, as the number \(b\) of batches into which the graph is divided grows, each edge has chance \(\) to appear in a given step. Thus, \(b\) cannot be very large without dropping important edges. A similar problem can happen in random neighbor sampling methods such as GraphSAGE (Hamilton et al., 2017). Although this model works well on message-passing neural networks (MPNNs) which only use the graph edges, using it for expander-augmented graphs will select only a small ratio of the expander edges, thereby breaking the universality properties provided by the expander graph.

Expander graphs enable global information propagation, and when created with Hamiltonian cycles and self-loops, produce a model that can provably approximate a full Transformer (Shirzad et al., 2023, Theorem E.3). Yet not all of these edges turn out to be important in practice: we expect some neighboring nodes in the updated graph to have more of an effect on a given node than others. Thus, removing low-impact neighbors can improve the scalability of the model. The challenge is to identify low-impact edges without needing to train the (too-expensive) full model. Figure 1 illustrates advantages of this batching approach other; this is also discussed further in Appendix E.

One approach is to train a smaller network to identify which edges are significant. It is not obvious _a priori_ that attention scores learned from the smaller network will estimate those in the larger network, but we present an experimental study verifying that attention scores are surprisingly consistent as the network size reduces. We also give theoretical indications that narrow networks are capable of expressing the same attention scores as wider networks of the same architecture.

**Our approach.** We first train a small-width network in order to estimate pairwise attention score patterns, which we then use to sparsify the graph and train a larger network. We first train the graphs without edge attributes. This reduces the complexity of Exphormer to \((md+nd^{2})\) and then by training a much smaller width \(d_{s} d\) network, reduces the time and memory complexity by at least a factor of \(d/d_{s}\). We also introduce two additions to the model to improve this consistency. Training this initial network can still be memory-intensive, but as the small width implies the matrix multiplications are small, it is practical to train this initial model on a CPU nodes with sufficient RAM (typically orders of magnitude larger than available GPU memory), without needing to use distributed computation. Once this initial model is trained, the attention scores can be used in creating a sparse graph, over which we train the second network. These initial attention scores can be used as edge features for the second network.

As mentioned previously, we use the attention scores obtained from the trained low-width network to sparsify the graph. By selecting a fixed number \(c\) of edges per attention layer for each node, we reduce the complexity of each layer to \((nd^{2}+ndc)\). This sparsification alleviates the effect of a large number of edges, and allows for initial training with a larger degree expander graph, since most of the expander edges will be filtered for the final network. This sparsification differs from conventional graph sparsification algorithms (for MPNNs) in three ways. First, we use expander edges, self-loops, and graph edges and sparsify the combination of these patterns together. Second, this sparsification is layer-wise, which means that in a multi-layer network the attention pattern will vary from layer to layer. Finally, our sampling uses a smaller network trained on the same task, identifying important neighbors based on the task, instead of approaches independent of the task such as sampling based on PageRank or a neighbor's node degree.

Another advantage of this approach is that the fixed number of neighbors for each node enables regular matrix calculations instead of the edge-wise calculations used by Kreuzer et al. (2021); Shirzad et al. (2023), greatly improving the speed of the model. After this reduction, batching can be done based on the edges over different layers, enabling Transformers to be effectively batched while still effectively approximating the main Transformer model, enabling modeling long-range dependencies. In batching large graphs, sampling without replacement from attention edges with varying weights can be very slow. This is especially true if the attention scores are highly concentrated on a small number of neighbors for most of the nodes. Sequential sampling of neighbors can be very slow, while parallel sampling can lead to many conflicts in this scenario. We use reservoir sampling, enabling parallel sampling with an easy, efficient GPU implementation, improving the sampling process significantly.

We only use the Transformer part of the Exphormer model, not the dual MPNN+Transformer architecture used by Shirzad et al. (2023); Rampasek et al. (2022). Unlike the Exphormer approach, we do not assume that the expander graph is of degree \((1)\); we can see this as interpolating between MPNNs and full Transformers, where smaller degree expander graphs mostly rely on the graph edges and are more similar to MPNNs, while higher degree expander graphs can resemble full attention, in the most extreme case of degree \(n-1\) exactly recovering a full Transformer.

To summarize, the contributions of this paper are as follows: 1.) We experimentally and theoretically analyze the similarity of attention scores for networks of different widths, and propose two small architectural changes to improve this similarity. 2.) We propose layer-wise sparsification, by sampling according to the learned attention scores, and do theoretical analysis on the sparsification guarantees of the attention pattern. 3.) Our two-phase training process allows us to scale Transformers to larger datasets as it has significantly smaller memory consumption, while maintaining competitive accuracy.

## 2 Related Work

**Graph Transformer Architectures.** Attention mechanisms were proposed in early (message-passing) Graph Neural Network (GNN) architectures such as Graph Attention Networks (GAT) (Velickovic et al., 2018), where they guide node aggregation among neighbors, without using positional encodings. GraphBert (Zhang et al., 2020) finds node encodings based on the underlying graph structure. Subsequent work has proposed full-fledged graph Transformer models that generalize sequence Transformers (Dwivedi and Bresson, 2020) and are not limited to message passing between nodes of the input graph; these include Spectral Attention Networks (SAN) (Kreuzer et al., 2021), Graphormer (Ying et al., 2021), GraphiT (Mialon et al., 2021), etc. GraphGPS (Rampasek et al., 2022) combines attention mechanisms with message passing, allowing the best of both worlds.

Figure 1: Figure (a) shows a very simple synthetic graph where each node has a binary classification task of determining whether there exists a node of the opposite color in the same connected component. This task requires learning long-range dependencies. Figure (b) shows a natural clustering of the graph. This clustering would mean no node can do its task if models are trained only on the clusters. Figure (c) shows a neighbor sampling starting from the green node, where random sampling fails to select the important edge that bridges the two different colored nodes. Figure (d) shows a random subset sampling strategy that the task is solvable if and if only the two sides of the bridge between the two colors get selected. Now if we keep increasing the size of both sides of the bridge, while keeping just one edge between two colors, the probability of selecting the bridge in any batch goes to zero, and thus the training will fail in this scenario. (e) shows attention scores between the nodes if trained with an attention-based network. Dashed lines have near zero attention scores, and thicker lines indicate a larger attention score. Knowing these attention scores will mean each node with just one directional edge can do the task perfectly. The attention edges are shown in (f). In case two nodes are equally informative selecting either of them leads to the correct result.

**Efficient Graph Transformers.** Several recent works have proposed various scalable graph transformer architectures. NAGphormer (Chen et al., 2022) and Gophormer (Zhao et al., 2021) use a sampling-based approach. On the other hand, Difformer (Wu et al., 2023) proposes a continuous time diffusion-based transformer model. Explorer (Shirzad et al., 2023) proposes a sparse graph that combines the input graph with edges of an expander graph as well as virtual nodes. They show that their model works better than applying other sparse Transformer methods developed for sequences. Another work, NodeFormer (Wu et al., 2022), which is inspired by Performer (Choromanski et al., 2021), uses the Gumbel-Softmax operator as a kernel to efficiently propagate information among all pairs of nodes. SGFormer (Wu et al., 2024) shows that just using a one layer transformer network can sometimes improve the results of GCN-based networks and the low memory footprint can help scale to large networks. Perhaps most conceptually similar to our work is Skeinformer (Chen et al., 2022), which uses sketching techniques to accelerate self-attention.

**Sampling and batching techniques.** Some sampling-based methods have been used to alleviate the problem of "neighborhood explosion." For instance, sampling was used in GraphSAGE (Hamilton et al., 2017), which used a fixed-size sample from a neighborhood in the node aggregation step. GraphSAINT (Zeng et al., 2020) scales GCNs to large graphs by sampling the training graph to create minibatches.

**Other.** Expander graphs were used in convolutional networks by Prabhu et al. (2018).

## 3 Preliminaries and Notation

**Exphormer.**Exphormer is an expander-based sparse attention mechanism for graph transformers that uses \(O(|V|+|E|)\) computation, where \(G=(V,E)\) is the underlying input graph. Explorer creates an interaction graph \(H\) that consists of three main components: edges from the input graph, an overlaid expander graph, and virtual nodes (which are connected to all the original nodes).

For the _expander graph_ component, Exphormer uses a constant-degree random expander graph, with \((n)\) edges. Expander graphs have several useful theoretical properties related to spectral approximation and random walk mixing, which allow the propagation of information between pairs of nodes that are distant in the input graph \(G\) without explicitly connecting all pairs of nodes. The expander edges introduce many alternative short paths between the nodes and avoid the information bottleneck that can be caused by the virtual nodes.

**Our model.** We use \(H\) to denote the attention pattern, and \(_{H}(i)\) the neighbors of node \(i\) under that pattern. Let \(=(_{1},_{2},,_{n})^ {d n}\) be the matrix of \(d\)-dimensional embeddings for all of the \(n\) nodes. Our primary "driver" is then \(h\)-head attention: using \(\) for element-wise multiplication,

\[_{H}()_{:,i}=_{i}+_{j=1}^{h}_{ i}^{j}((^{j}^{j})^{T} _{i}^{j}+^{j}),\]

where \(_{i}^{j}=_{V}^{j}_{_{H}(i)}\), \(=_{K}^{j}_{_{H}(i)}\), and \(_{i}^{j}=_{Q}^{j}_{i}\), are linear mappings of the node features for the neighbors \(_{_{H}(i)}\), and \(^{j}=_{E}^{j}_{_{H}(i)}\) and \(^{j}=_{B}^{j}_{_{H}(i)}\) are linear maps of the edge features \(\), which is a \(d_{E}|_{H}(i)|\) matrix of features for the edges coming in to node \(i\). Exphormer uses learnable edge features for each type of added edge, and original edge features for the graph's edges. If the graph does not have any original edge features, it uses a learnable edge feature across all graph edges. Edge features help the model distinguish the type of attention edges. Here, \(\) is an activation function. In both Exphormer and our work the activation function is \(\).

In the absence of edge features, which is the case for most of the transductive datasets, including the datasets that have been used in this paper, \(_{e}\) for any attention edge \(e\) can have one of three possible representations, and so \(^{j}\) can be computed more simply by first mapping these three types of edge features with \(_{E}^{j}\) for head \(j\), and then replacing the mapped values for each edge type. This simple change reduces the complexity of the Exphormer from \((md^{2}+nd^{2})\) to \((md+nd^{2})\).

Compared to prior work, we introduce \(^{j}\) as a simpler route for the model to adjust the importance of different edge types. Considering Exphormer as an interpolation between MPNNs and full Transformers, the \(}^{j}\) model has an easier path to allow for attention scores to be close to zero for all non-graph attention edges, without restricting the performance of the attention mechanism ongraph edges. Consequently, it can function roughly as an MPNN (similar to GAT) by zeroing out the non-local attention paths. We use \(d_{E}=d\), and have each layer output features of the same width as its input, so that each of the \(^{j}\) parameter matrices except for \(^{j}_{B}\) are \(d d\), and \(^{j}_{B}\) is \(d 1\).

As a simple illustration that \(^{j}\) is insufficient to allow near-zero attention scores, thus highlighting the importance of \(^{j}\), note that if the columns of \(\) and \(\) are distributed independently and uniformly on a unit ball (e.g., under a random initialization), there is no vector \(^{j}\) which is identical for all edges of an expander graph that can make the attention scores for all the expander edges near-zero.

**Our network compared to Exphormer.** We use Exphormer as the base model because it provides us the flexibility to adjust the sparsity of the attention graph and to interpolate between MPNNs and full Transformers. Exphormer can model many long-range dependencies that are not modeled by MPNNs and are very expensive to model in a full Transformer. For example, one cannot train a full Transformer model in the memory of a conventional GPU device for a dataset such as Physics, which has a graph on just 34K nodes. In our instantiation of Exphormer, we add self-loops for every node and use \(d/2\) random Hamiltonian cycles to construct our expander graph as described in (Shirzad et al., 2023, Appendix C.2). We do not add virtual nodes in our networks (note that the resulting network is still a universal approximator; Shirzad et al., 2023, Theorem E.3). Although the best known results for Exphormer combine sparse attention with MPNNs, in this work, we avoid the MPNN component for scalability reasons. We also make two additional changes, see Section 4.

## 4 Method

Our method consists of a two-phase training process. The first phase trains a model we call the _Attention Score Estimator Network_, whose goal is to estimate the attention scores for a larger network. This model is not particularly accurate; its only goal is for each node to learn which neighbors are most important. The learned attention scores for each layer of the first network are then used to construct sparse interaction graphs for each layer in a second model, which is trained (with hyperparameter tuning for the best results) and serves as the final predictor.

**Attention Score Estimator Network.** For this network, we use a width of \(4\) or \(8\), with just one attention head, in our training. We tune the other hyperparameters in order to have a converged training process with reasonably high accuracy, but we do not spend much time optimizing this network as it is sufficient to learn the important neighbors for each node, i.e., edges with high attention scores. This network will be trained with as many layers as the final network we want to train. Because it is so narrow, it has many fewer parameters and hence much less memory and time complexity, making it cheaper to train. Moreover, we only need to do this training once per number of layers we consider, conditioned on the fact that the training converges, even if the final model has a large number of hyperparameters. Compared to Exphormer, we use a much higher-degree expander graph: \(30\) to \(200\) instead of the \(6\) used for most transductive graphs by Shirzad et al. (2023). As most of the considered datasets do not have edge features, we use a learnable embedding for each type of edge (graph edge, expander edge, or self-loop). We also make two small changes to the architecture and the training process of this model, discussed below. Section 5 shows experimentally that the low-width network is a good estimator of the attention scores for a large-width network.

**Normalizing V.** Having a smaller attention score, \(_{ij}<_{ij^{}}\), does not necessarily mean that \(j\)'s contribution to \(i\)'s new features is smaller than that of \(j^{}\): if \(\|_{j}\|\|_{j^{}}\|\), the net contribution of \(j\) could be larger. Although Transformers typically use layer normalization, they do not typically do so after mapping \(X\) to \(\). We normalize the rows of \(\) to have the same vector sizes for all nodes. In our experiments, normalizing to size one reduced performance significantly; however, adding a learnable global scale \(s\), so that \(_{i}\) becomes \(_{i}}{||_{i}||_{2}}\), maintained performance while making attention scores more meaningful.

Variable TemperatureOne of the side goals is to have sharper attention scores, guiding the nodes to get their information from as few nodes as possible. Using temperature in the attention mechanism can do this, where logits will be divided by a temperature factor \(\) before being fed into a softmax. Normal attention corresponds to \(=1\); smaller \(\) means sharper attention scores. However, setting the temperature to a small value from the beginning will make the random initialization more significant, and increase the randomness in the training process. Instead, we start with \(=1.0\) and gradually anneal it to \(0.05\) by the end of the training. We set an initial phase for \(\) epochswhere we use \(=1\); this lets the model learn which neighbors are more important for each node slowly. We multiply \(\) with a factor \(\) after each epoch, obtaining a temperature in epoch \(t>\) of \((^{t-},0.05)\). We use \(=5\) and \(=0.99\) or \(0.95\) depending on how fast the learning converges.

**Sparser Attention Pattern.** The memory and time complexity of Exphormer is linearly dependent on the number of edges. Also, with a small number of layers, the expander degree should be high enough to ensure a large enough receptive field for each node in order to learn the long-range dependencies. Not all these edges are equally important, and many of them will have a near-zero effect on the final embedding of each node. Reducing the number of edges can alleviate memory consumption. Additionally, a sparser pattern lets us use batching techniques for the larger graphs. In this work, we analyze how effectively the sparser model can work and up to what factor we can sparsify. For each layer, e.g., \(\), we select a \(_{}\) as a fixed degree for each node and sample without replacement according to the attention score estimator network's attention scores in each epoch of training or evaluation. Having the same degree for each node's attention pattern also means that attention can be calculated using (much-more-optimized) standard matrix multiplications, rather than the propagation techniques used in Exphormer and SAN (Kreuzer et al., 2021).

To sparsify the graph, in each epoch, we sample a new set of edges according to the learned attention scores from the smaller network. The reason why we do this rather than a simpler strategy such as selecting top-scored edges is that in many cases, several nodes can have very similar node features. If we assume nodes \(u_{1},u_{2},,u_{p}\) from the neighbors of node \(v\) have almost the same features, and if the attention scores for these nodes are \(_{1},_{2},,_{p}\), any linear combination of \(_{i=1}^{p}_{i}=\) will lead to the same representation for node \(v\). If features are exactly the same, \(\) will be divided between these nodes, and even if \(\) is large, each node's attention score from \(v\) can be small. By sampling, we have a total \(\) chance of selecting any of the nodes \(u_{1:p}\). In each epoch, we re-sample a new set of edges for each node from its original neighborhood.

**Faster Sampling Using Reservoir Sampling.** Sampling without replacement using default library calls is very slow, especially if few neighbors dominate the attention scores. We instead use reservoir sampling (Efraimidis and Spirakis, 2006), which is GPU-friendly and parallelizable. For reservoir sampling of \(k\) neighbors from the neighborhood of node \(i\), with attention scores \(=(a_{1},a_{2},,a_{|_{H}(i)|})\), we first do a uniform random sampling of \(=(u_{1},u_{2},,u_{|_{H}(i)|})\), where the \(u_{i}\) are i.i.d. samples from \((0,1)\). Then we calculate \(}()\) with element-wise

Figure 2: Steps of our method. **(a)** The attention mechanism for the attention score estimator network combines graph edges with an expander graph and self-loops. The expander graphs are constructed by combining a small number of Hamiltonian cycles – here two, in red and in purple – then confirming the spectral gap is large enough. **(b)** Self-attention layers in the estimator network use this sparse attention mechanism; its self-attention layers normalize \(\). **(c, d)** Attention scores are extracted from this network for each layer, and used to sample, in **(e)**, a sparse directed graph, which becomes the attention graph for the final network **(f)**. This network, with a much larger feature dimension, does not normalize \(\).

multiplication, and select the indices with the top \(k\) values from this list. Selecting \(k\)-th rank from \(n\) values and pivoting has a worst-case \((n)\) time algorithm, which is much faster than the \((nk)\) worst case time for trial-and-error. Pseudocode is given in Algorithm 1. The GPU-friendly version of this can be implemented by sampling for nodes in parallel, but requires forming a regular matrix for the attention scores, which can be done by extending each attention score vector by the maximum degree, or selecting a value \(k^{} k\) and first sampling \(k^{}\) and selecting the top \(k^{}\) attention scores from each node, making sure that the sum of the rest of the neighbor's attention scores are very near to zero. Then by forming a rectangular attention matrix, uniform sampling and element-wise multiplications are much faster on GPU, and sampling from the entire batch is much more efficient.

```
0: Attention scores \(=a_{i,_{H}(i)}^{()}\), number of neighbors to sample: \(_{}\)
0: List of \(_{}\) neighbors of node \(i\)
1:functionReservoirSample(\(,_{}\))
2:\((0,1)^{|_{H(i)}|}\)
3:return\(_{_{}}(}())\)
4:endfunction ```

**Algorithm 1** Reservoir Sampling from a Node's Neighborhood

**Batching.** Each batch starts with a random subset of "target" nodes \(B\). These are the nodes whose last-layer representations we will update in this optimization step. To calculate these representations, we need keys and values based on the previous layer's representations for the relevant neighbors of each target node (again, sampling neighbors from the graph augmented by an expander graph). To approximate this, we sample \(_{L}\) neighbors for each target node. Then we have a set of at most \(|B|(_{L}+1)\) nodes whose representations we need to calculate in layer \(L-1\); we repeat this process, so that in layer \(\) we need to compute representations for up to \(^{()}(|B|_{i=+1}^{L}(_{i}+1),n)\) query nodes, with \(|^{()}|_{}\) attention edges. Pseudocode is given in Algorithm 2.

When the number of layers \(L\) and degree \(_{}\) are not too large, this batching can be substantially more efficient than processing the entire graph. Moreover, compared to other batching techniques, our approach selects neighbors _according to their task importance_. Except for optimization dynamics in the training process corresponding to minibatch versus full-batch training, training with batches is identical to training with the entire sparsified graph; if we choose a large \(_{}\) equal to the maximum degree of the augmented graph, this is exactly equivalent to SGD on the full graph, without introducing any biases in the training procedure. This is in stark contrast to previous approaches, as illustrated in Figure 1. Unlike these prior approaches, which typically use the full graph at inference time, we can run inference with batch size as small as one (trading off memory for computation).

```
0: Attention scores in each layer: \(=\{a_{i,j}^{()} i V,j_{H}(i),  L\}\), number of neighbors to sample in each layer: \(=\{_{1},,_{L}\}\), and a batch of nodes \(B V\)
0:\(^{()},^{()},^{()}\), query, key, and value nodes in each layer
1:functionSampleNeighborood(\(B,,\))
2:\(^{(L+1)} B\)
3:for\( L\) to \(1\)do
4:\(^{()}^{(+1)}\)
5:for\(i-i^{()}\)do
6:\(_{i}^{()}(_{i, _{H}(i)},_{})\)
7:endfor
8:\(^{()}_{i^{}}_{i} ^{()}\)
9:\(^{()}^{()}^{()}\)
10:endfor
11:return\(\{(^{()},^{()},^{()} ) 1 L\}\)
12:endfunction ```

**Algorithm 2** Neighborhood Sampling for a Batch of Nodes

Fixed Node Degree Layers.Sparse matrix operations are not yet nearly as efficient as dense operations on GPU devices. Exphormer and SAN use a gather operation, which is memory-efficient but not time-efficient on a GPU (Zaheer et al., 2020). By normalizing the degree, instead of having \(|^{()}|_{}\) separate dot products between the query and key vectors, we can reshape the key vectors to be of size \(|^{()}|_{} d\) and the query is of shape \(|^{()}| d\). Now the dot product of query and key mappings can be done using \(|^{()}|\), \(_{} d\) by \(d 1\) matrix multiplications. This same size matrix multiplication can be done using highly optimized batch matrix multiplication operations in e.g. PyTorch and Tensorflow (Paszke et al., 2019; Abadi et al., 2015).

**Theoretical Underpinnings.** We first study the approximability of a network with a smaller hidden dimension or width. Formally, suppose that the width of a wide network is \(D\). Then there exists a network with narrow dimensions for \(_{Q}\) and \(_{K}\), of dimension \((}) D\) instead of \(D D\), whose attention scores agree with those of the wide network up to \(()\) error (Theorem D.4). This reduction helps with the most intensive part of the calculation; others are linear with respect to the number of nodes \(n\). While this is not the model we use in practice, Shirzad et al. (2024, Section 4) explore some scenarios common in graph Transformers that allow for the existence of "fully" narrow networks with accurate attention scores. They support these claims with experiments that show compressibility for some datasets we use. This is an existence claim; we will justify experimentally that in practice, training a narrow network does approximate attention scores well.

We then study the sampling procedure of our sparsification method. Under certain assumptions, we show that sampling roughly \((n n/^{2})\) entries of the attention matrix \(A\) (corresponding to sampling this many edges in the graph) suffices to form a matrix \(B\) with \(\|A-B\|_{2}\|A\|_{2}\), if we can access the entries of \(A\) (Theorem D.5). We cannot actually access the matrix \(A\), but we do have attention scores \(A^{}\) from a narrow network. We show that if the entries of \(A\) are not seriously under-estimated by \(A^{}\), the same bound on the number of samples still holds (Proposition D.7).

## 5 Experimental Results

Attention Score Estimation.To show how well the smaller network estimates the attention scores for a larger network, we conduct experiments on two smaller datasets, where we can reasonably train the full network at higher width for many runs to be able to have an estimation on the distribution of the attention scores. To this end, we use Actor (Lim et al., 2021) and Photo (Shchur et al., 2018) datasets. We train the network for hidden dimensions, \(h\) varying from 4 to 64 for both datasets. For each \(h\) we train the network \(100\) times. We consider the distribution of attention scores for each node and estimate the energy distance (Szekely and Rizzo, 2013; an instance of the maximum mean discrepancy, Sejdinovic et al., 2013) for that node across each pair of \(h\) sizes.

We ran this experiment both when learning with just graph edges, and when adding expander and self-loop edges. It might be that the model, just by examining the category of the edges, may give a lower score to one type, making distributions seem more similar despite not identifying a small number of important neighbors as we want. However, in the presence of only one type of edge, the model can still consistently estimate which nodes should have a higher attention score.

We compare attention scores from our model with the uniform distribution on the neighbors (each neighbor of node \(i\) has score \(}\)), and to a distribution with logits uniform over \([-8,8]\). The choice of

Figure 3: Energy distance between the attention scores of various networks to a network of width 64. “Uniform” refers to the baseline placing uniform scores on each neighbor, while “random” refers to the baseline with uniformly distributed logits. The remaining bars refer to networks trained on the appropriately labeled width.

[MISSING_PAGE_FAIL:9]

We use a subset of the following models in each of our tables as baselines, depending on the type of the dataset and scalability level of the models, GCN (Kipf and Welling, 2016), GraphSAGE (Hamilton et al., 2017), GAT (Velickovic et al., 2018), GraphSAINT (Zeng et al., 2020), Nodeformer (Wu et al., 2022), Difformer (Wu et al., 2023), SGFormer (Wu et al., 2024), GraphGPS (Rampasek et al., 2022), GOAT (Kong et al., 2023), GloGNN (Li et al., 2022), SGC (Wu et al., 2019), NAGphormer (Chen et al., 2022), Exphormer (Shirzad et al., 2023), SIGN (Frasca et al., 2020). We borrow most of the baseline numbers in the tables from Wu et al. (2024); Deng et al. (2024).

**Ablation Studies.** We benchmark the effect of different parts of the model in Table 3. Spexprhorre-uniform, rather than sampling based on the estimated attention scores, samples uniformly from the augmented graph; this is always worse than sampling, but the gap is larger for some datasets than others. Spexprhorre-max takes the edges with the highest attention scores, rather than sampling; this again performs somewhat worse across datasets. Spexprhorre w.o. temp uses a constant temperature of 1 in the initial attention score estimator network; Spexprhorre w.o. layer norm removes our added layer normalization. these changes are smaller, and in one case layer normalization makes the results worse. Across the four datasets, however, it seems that both temperature and layer norm help yield more informative and sparser attention scores.

## 6 Conclusion & Limitations

We analyzed the alignment of the attention scores among models trained with different widths. We found that usually the smaller network's attention score distributions align with the larger network's. We also theoretically analyzed the compressibility of the larger Graph Transformer models. Based on these observations, we used a sampling algorithm to sparsify the graph on each layer and sampled a smaller number of edges per layer. As a result of these two steps, the model's memory consumption reduces significantly, while achieving a competitive accuracy. This strategy also lets us use novel batching techniques that were not feasible with expander graphs of a large degree. Having a regular degree enables using dense matrix multiplication, which is far more efficient with current GPU and TPU devices.

While our method successfully scales to datasets with over two million nodes, it relies on large CPU memory for the attention score estimation for these datasets. For extremely large datasets, this is still infeasible without highly distributed computation. Estimated attention scores can be shared and used for training various networks based on attention scores, however, so this only needs to only be computed once per dataset and depth. An area for potential future work is to combine sampling with simultaneous attention score estimation in a dynamic way, scaling this estimation to very large graphs.