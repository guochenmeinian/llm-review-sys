# Rethinking No-reference Image Exposure Assessment

from Holism to Pixel: Models, Datasets and Benchmarks

 Shuai He\({}^{,1}\)   Shuntian Zheng\({}^{,2}\)   Anlong Ming\({}^{,1,*}\)   Banyu Wu\({}^{1}\)   Huadong Ma\({}^{1}\)

\({}^{1}\) Beijing University of Posts and Telecommunications  \({}^{2}\) University of Warwick  {hs19951021}@bupt.edu.cn

Shuntian.Zheng@warwick.ac.uk {mal, wubanyu, mhd}@bupt.edu.cn

###### Abstract

The past decade has witnessed an increasing demand for enhancing image quality through exposure, and as a crucial prerequisite in this endeavor, Image Exposure Assessment (IEA) is now being accorded serious attention. However, IEA encounters two persistent challenges that remain unresolved over the long term: the accuracy and generalizability of No-reference IEA are inadequate for practical applications; the scope of IEA is confined to qualitative and quantitative analysis of the entire image or subimage, such as providing only a score to evaluate the exposure level, thereby lacking intuitive and precise fine-grained evaluation for complex exposure conditions. The objective of this paper is to address the persistent bottleneck challenges from three perspectives: model, dataset, and benchmark. 1) Model-level: we propose a Pixel-level IEA Network (P-IEANet) that utilizes Haar discrete wavelet transform (DWT) to analyze, decompose, and assess exposure from both lightness and structural perspectives, capable of generating pixel-level assessment results under no-reference scenarios. 2) Dataset-level: we elaborately build an exposure-oriented dataset, IEA40K, containing 40K images, covering 17 typical lighting scenarios, 27 devices, and 50+ scenes, with each image densely annotated by more than 10 experts with pixel-level labels. 3) Benchmark-level: we develop a comprehensive benchmark of 19 methods based on IEA40K. Our P-IEANet not only achieves state-of-the-art (SOTA) performance on all metrics but also seamlessly integrates with existing exposure correction and lighting enhancement methods. To our knowledge, this is the first work that explicitly emphasizes assessing complex image exposure problems at a pixel level, providing a significant boost to the IEA and exposure-related community. The code and dataset are available in here.

+
Footnote †: dagger}\) Equal contribution. \({}^{*}\) Corresponding author.

## 1 Introduction

Exposure, one of the 3A factors (Auto Exposure, Focus and White Balance) in camera technology, plays a crucial role in controlling image quality. Image exposure assessment (IEA) is a prerequisite for improving exposure ; however, even leading phone and camera manufacturers heavily rely on manual evaluations due to the unavailability and high cost of human raters. Nevertheless, large-scale adoption of manual assessments is impractical. Similar to mainstream AI applications, deep learning and data-driven approaches hold promise as potential solutions to overcome this limitation. Nevertheless, the traditional data-driven IEA paradigm encounters two major challenges:

_1) A Dilemma between Applicability and Practicability:_ While full-reference IEA methods deliver satisfactory results , their applicability in non-preset scenarios is limited due to the general unavailability of reference images. Conversely, no-reference IEA methods, which do not rely on reference information, struggle with natural images distorted by unknown factors. This difficultyarises from the inability to identify specific features for assessing exposure, as the quality prediction problem becomes agnostic to the type of exposure distortion, thereby restricting both performance and practicality [10; 11].

_2) Restricted Generalization Capacity:_ Traditional IEA annotation relies on scenario-specific criteria, leading to significant subjectivity across datasets. These datasets typically provide only a holistic quantitative score reflecting the overall exposure condition , lacking detailed and fine-grained assessments. Consequently, the subjective and coarse-grained labels introduce restricted generalization capacity into learning-based IEA methods, reducing their adaptability to diverse scenarios and assessment criteria.

How can an ideal method be designed to tackle the aforementioned challenges? The method should primarily address three key issues: firstly, as a no-reference method, it should effectively simulate reference images in non-preset scenarios, functioning like full-reference methods; secondly, it should achieve fine-grained assessments by adapting to diverse high-level evaluation criteria or application scenarios directly or through fine-tuning; finally, the learned features should be decoupled from subjective criteria and aligned with naive exposure features to mitigate narrow inductive biases.

This paper presents P-IEANet, an innovative method that leverages large-scale, pixel-level annotated datasets to delve into the fundamental unit of IEA: pixels. This approach allows us to _identify exposure issues with unprecedented precision and to handle IEA tasks of varying granularity beyond the pixel level_, without being influenced by subjective criteria. Grounded in the well-established theory that the power spectrum of natural images is a function of frequency, represented as \(1/f^{}\) where \(\) varies slightly at specific frequencies [11; 13], we leverage this insight to analyze exposure characteristics in specific frequency domains for improved adaptability across varying criteria and scenarios. Through employing the dedicated Haar Discrete Wavelet Transform (DWT), P-IEANet decomposes the original image to criteria-agnostic frequency features, thus avoiding narrow inductive biases. Additionally, with pixel-level supervision, P-IEANet enables ideal exposure reconstruction from frequency space, effectively creating reference images for further analysis.

Our contributions are summarized as follows:

* To our knowledge, this is the first work to implement a pixel-level evaluation paradigm in IEA. It enhances the generalizability and accuracy of no-reference IEA tasks, while effectively addressing challenges associated with reusing underlying data and architectures.
* We present the P-IEANet, showcasing that pixel-level IEA can be decomposed into criteria-agnostic lightness and structure information via the dedicated Haar DWT. This design enables efficient execution of pixel-level IEA while minimizing parameter usage.
* To convincingly validate our method, we have developed a dataset exclusively tailored for IEA, called IEA40K. This dataset specifically focuses on exposure and comprises 40,000 of images with the most comprehensive annotations to date, including pixel-level annotations.
* Building upon IEA40K, we have evaluated 19 baselines, establishing our benchmark as the most comprehensive to date for IEA. Our work not only achieves SOTA performance but

Figure 1: Comparison between the brightness histogram (a) and our method (b) for assessing image exposure. (b) offers a more intuitive and accurate reflection of the exposure conditions in each area. (c) shows how to determine exposure levels for each area using Adams’ zone system theory [14; 15].

also serves as a pivotal catalyst, offering the community a new roadmap to explore further solutions for IEA.

## 2 Related Work

**No-reference IEA Methods and Datasets.** Previous studies on no-reference IEA can be broadly classified into two primary categories: _1) Statistical-feature based methods_. Datta _et al._ utilized average pixel intensity to evaluate light usage. Liu _et al._ explored image brightness histograms, and Hanmandlu _et al._ developed indicators based on the image brightness histogram for crucial auto-exposure control. Rahman _et al._ and Lu _et al._ adopted information entropy as a criterion for exposure evaluation. In some auto-exposure work, [16; 17; 18; 19] incorporated gradient information to determine the ideal exposure settings for cameras, suggesting that maximum information entropy indicates ideal image exposure. Efimov _et al._ and Dong _et al._ proposed subdividing images into blocks for individual assessment, classifying each based on its brightness histogram into categories. _2) Data-driven methods._ The increasingly popular methods [12; 22; 23; 24] utilize human-labeled datasets to develop scenario-specific features.

However, when it comes to _statistical-feature based methods_, manual features often assume only one type of distortion, which is problematic in complex situations where overexposure and underexposure coexist [11; 25]. On the other hand, _data-driven methods_ become less effective when assessment criteria or application scenarios change; moreover, the holistic scores from these datasets lack the detailed supervisory information required for the precision and granularity demanded in applications.

**Pixel-level Tasks.** In conventional terms, the concept of "exposure" refers to not only exposure time but also two other parameters (aperture and ISO, referred to ). Rather than being characterized as a global attribute of the image, the parameters would be more appropriate to be described as a global attribute associated with the camera for capturing the image. However, according to the claim made by the classical photographic theory (Adams' theory)  that "The exposure time is the same for all elements, but the image exposure varies with the luminance of each subject element," the coarse global camera exposure attribute fails to match each subject element in an image, potentially resulting in some subject elements being under-exposed and others being over-exposed. Given this, **in the context of evaluating images, the term "exposure" is no longer a global attribute**, as referred to  that "Any scene of photographic interest contains elements of different luminance; consequently, **the 'exposure' actually is many different exposures.**" Therefore, the pixel-level IEA is highly desired.

However, to our knowledge, there are currently no pixel-level IEA methods, despite advancements in related visual tasks such as semantic understanding and fine-grained analysis. For instance, DiffuMask  exploits powerful zero-shot text-to-image generative models to provide pixel-level

Figure 2: Images visualized along with their corresponding pixel-level (heat map of exposure residual) and holistic IEA results by P-IEANet (where a higher score shows more visually pleasing exposure).

segmentation annotations across diverse classes. Similarly, PixelLM  leverages GPT4V to produce 246,000 pixel-level question-answer pairs, enhancing its capabilities in pixel-level reasoning and comprehension.

However, directly applying these methods to IEA tasks is very challenging due to specific requirements in data collection and annotation processes, which entail avoiding selection bias, accurately aligning images, obtaining ideal references, and providing detailed annotations (_cf._ Sec. 4).

## 3 Architecture of P-IEANet

**Preliminaries.** Images captured with incorrect exposure settings often suffer from visual problems, including lightness and structure distortions . For instance, overexposed images exhibit unnatural artifacts, inconsistencies in exposure blending, and blurred structural details. This paper demonstrates the potential of the Haar DWT for analyzing IEA issues. The mathematical representation of the Haar DWT can be formulated as follows:

\[DWT(L,H)=}}_{k}f(k)((}{2^{ m}}),(}{2^{m}})).\] (1)

The mother wavelet \(\) and \(\) of the Haar DWT can decompose an image into _low_ frequency components \(L\) (approximation coefficients), and _high_ frequency components \(H\) (coefficients in horizontal, vertical, and diagonal directions), as shown in Fig. 3. We label the underexposed, overexposed, and ideal images as \(X_{over}\), \(X_{under}\), and \(X_{ideal}\), respectively. Their corresponding Haar DWT representations in frequency are denoted as \(DWT(L(X_{over}),H(X_{over}))\), \(DWT(L(X_{under}),H(X_{under}))\) and \(DWT(L(X_{ideal}),H(X_{ideal}))\), respectively.

For IEA tasks, we examine whether the low-frequency and high-frequency components correspond to the frequency-domain representations of lightness and structure, respectively. The images obtained by reversing these components, such as \(DWT^{-1}(L(X_{ideal}),H(X_{under}))\) and

Figure 4: Pipeline of P-IEANet. The original image is decomposed into low/high-frequency components via Haar DWT. Subsequently, the Structure Feature Module analyzes the high-frequency components with gradient maps to extract structural features (_cf._ Sec. 3.1). Simultaneously, the Lightness Feature Module handles the low-frequency component and extracts lightness features by attention mechanisms (_cf._ Sec. 3.2). These features are then composed through Haar inverse DWT for pixel-level IEA. Dedicated convolution kernels (d) are employed to facilitate the DWT process.

Figure 3: After Haar DWT decomposes an image, swapping its low-frequency component (3) with the high-frequency components (4) of the same image under different exposures produces visually similar results (a-d) as well as similar t-SNE features (e).

\(DWT^{-1}(L(X_{ideal}),H(X_{over}))\), show an exposure close to \(X_{ideal}\) (Fig. 3(c)(d)). Conversely, \(DWT^{-1}(L(X_{over}),H(X_{under}))\) exhibits a exposure similar to that of \(X_{over}\), as shown in Fig. 3(b). To further validate these findings, we conducted a similar analysis using t-SNE dimensionality reduction  on 200 sample images in Fig. 3(e). The t-SNE results reveal that the high frequency components remain relatively consistent across different exposures, while the low frequency components exhibit significant variation.

Based on the above observations, we deduce that the _low-frequency components primarily represent an image's lightness_, while the _high-frequency components indicate structural details and are less affected by lightness variations_. By exploiting this characteristic, we can decompose the exposure of a distorted input image into two frequency representations and then construct an ideal reference image in the frequency space. Subsequently, differences in the frequency domain are mapped to pixel-level IEA results. This approach effectively mitigates any influence from irrelevant image semantics or noise associated with IEA.

**Pipeline.** P-IEANet comprises three essential modules (Fig. 4). The Structure Feature Module (_cf._ Sec. 3.1) and Lightness Feature Module (_cf._ Sec. 3.2) are responsible for extracting structural features from high-frequency components and lightness features from low-frequency components, respectively. Ultimately, the Prediction Module (_cf._ Sec. 3.3) integrates these features to predict pixel-level IEA results and generate the final prediction.

### Structure Feature Module

There are two categories of structural features relevant to the IEA tasks: 1) Long-range features encompassing the overall layout and distant objects of an image, which provide a comprehensive understanding of its structure and global exposure [34; 35; 36; 37; 38]. 2) Short-range features focusing on fine details and textures, such as edges and localized patterns, are crucial for capturing local exposure variations [38; 39; 40; 41]. To obtain these features effectively, we first derive gradient maps from the high-frequency components of the Haar DWT. These maps highlight edge regions, thus enhancing the representation of basic structural details [30; 38; 42]. Subsequently, we refine the extraction process by subjecting these gradient maps to a Long-Range Encoder (LRE) and a Short-Range Encoder (SRE). Further details are provided below.

**Gradient Maps.** The input image \(X\) is decomposed by the Haar DWT to obtain high-frequency components, and then processed by a multi-layer encoder to extract naive features. For each layer \(z_{i}\), where \(i\) ranges from 1 to \(N\) (the total number of layers), we compute the gradient map as follows:

\[ z_{i}=\{g_{d}(z_{i})|d D\},\] (2)

where \(g_{d}(z_{i})\) applies the first-order gradient function \(g\) to \(z_{i}\) in direction \(d\). The set \(D\) includes all directions under consideration: \(+x,-x,+y,-y,+x+y,+x-y,-x+y,-x-y\), which correspond to the x-axis, y-axis, and their diagonals. These directions ensure comprehensive emphasis on edges, thereby enhancing the formulation of structural features.

**Long-range and Short-range Encoders.** To enhance the extraction of structural features, we feed both the original input feature, \(z_{i}\), and its gradient maps \( z_{i}\) into two distinct modules: a Transformer-based LRE \(Z^{l}\) and a CNN-based SRE \(Z^{s}\). The feature extraction process is as follows:

\[l_{i}=Z^{l}_{i}(z_{i}), s_{i}=Z^{s}_{i}(z_{i}), l_{i}= Z ^{l}_{i}( z_{i}), s_{i}= Z^{s}_{i}( z_{i}),\] (3)

where \(l_{i}\) and \(s_{i}\) represent the long-range and short-range features, respectively, these features are then integrated using a Structure Fusion Module \(Z^{f}\), which employs multiple MLPs, as follows:

\[Z_{o}=\|Z^{f}_{i}(l_{i},s_{i}),Z^{f}_{i}( l_{i}, s_{i}) \|^{N}_{i}.\] (4)

In this formulation, \(\|...\|\) signifies the stacking of operations along the feature channel dimensions, facilitating a comprehensive synthesis of the extracted features.

### Lightness Feature Module

The human visual system, possessing a high dynamic range, is skilled at globally detecting varying light levels of objects. However, due to limited attention capacity, it also _tends to focus on specific regions with distinct lightness levels_. Our method selectively processes global channels and local pixel regions to enhance the network's management of a broad spectrum of information.

**Lightness Channel Attention.** Firstly, our lightness channel attention (LCA) processes the channel-wise global spatial information, \(L_{c}\) (\(C H W\)), through global average pooling to create a channel descriptor, \(A_{c}\) (\(C 1 1\)). To determine the weights for different channels, the descriptor undergoes further refinement in two convolution layers, followed by sigmoid \(\) and ReLU \(\) activation functions. This procedure is formalized as follows:

\[A_{c}=((((_{i=1}^{H} _{i=1}^{W}L_{c}(i,j))))),\] (5)

where \(L_{c}(i,j)\) represents the lightness value at position \((i,j)\) in the \(c\)-th channel \(L_{c}\), this channel attention strategy highlights that lightness variations across different channels convey distinct and weighted information. Finally, the channel weights are element-wise multiplied with the input to generate the output \(F_{c}=A_{c} L_{c}\) (\(C W H\)).

**Lightness Pixel Attention.** The variable distribution of lightness among image pixels necessitates our lightness pixel attention (LPA) mechanism. This mechanism processes the output \(F_{c}\) from the LCA using self-attention (SA) and convolution layers, coupled with ReLu and sigmoid activation functions. The attention mechanism is formulated as follows:

\[A_{p}=(_{k K}w_{k}_{k}(((F_{c })))).\] (6)

Here, \(_{k}\) denotes a convolution operation with multi-scale kernel sizes \(k\), which aims to enhance the network's focus on fine-grained and multi-scale exposure features under complex scenario. Larger kernel sizes help perceive overall brightness and contrast, while smaller kernel sizes detect localized overexposure or underexposure issues. Additionally, self-attention allows the system to analyze lightness distribution and recognize patterns at multiple scales.

We then perform element-wise multiplication to merge the input \(L_{c}\) with \(A_{p}\) (\(1 H W\)), generating the output \(F_{p}=L_{c} A_{p}\) (\(C H W\)). The final stage integrates the outputs from both channel and pixel attention mechanisms to yield the comprehensive output \(A_{o}=(F_{c},F_{p})\).

### Prediction Module

We employed Haar \(^{-1}\) to integrate lightness and structure features for ideal exposure representation reconstruction in the frequency space, then using the formula \(P_{p}=(DWT^{-1}(A_{o},Z_{o}))\) to predict the _exposure residual, which measures the deviation of each pixel from the ideal exposure_. Both Haar DWT and \(^{-1}\) involve four dedicated convolutional kernels to simulate the wavelet transform's decomposition and reconstruction processes (Fig. 4(d)). To evaluate prediction accuracy, we define a loss function as:

\[_{pixel}=_{i=1}^{H}_{j=1}^{W}|P_{p} (i,j)-}(i,j)|,\] (7)

where \(}\) is the pixel-level ground truth, these residual maps can be converted into a coarser-grained prediction above pixel, e.g., holistic IEA score (_cf._ Sec. 5.2).

Figure 5: Visualization of different components in P-IEANet. Long-range (a) and Short-range (b) features highlight the important structural information for IEA tasks, while pixel-level attention (c) and channel-based attention (d-e) characterize its distribution in terms of light information.

## 4 Proposed IEA40K Dataset

### Image Collection

_How to Avoid Selection Bias when Building a Comprehensive IEA Dataset._ Selection bias can arise when certain types of intended exposure conditions, scenes and devices are underrepresented in the dataset, which may compromise the validity and generalizability of training models. To mitigate this issue, we considered 5 key aspects: varied scenes, diverse light conditions, sufficient devices, uniform resolution and comprehensive simulations (Fig. 6). These factors can significantly impact the quality of the dataset. Further details are provided in Appendix A.1.

_How to Align a set of Images through Pre-processing Strategies._ Effectively aligning a set of images captured under various exposure conditions is a significant challenge. Factors like camera shake and slight subject movement during shooting parameter adjustments can lead to misalignment . Such misalignment adversely affects the generation of supervised information and the training process. To tackle this issue, we first apply the Structural Similarity Index Measure (SSIM) algorithm to filter out misalignment images from the series. Those falling below a specified SSIM threshold are eliminated. Subsequently, we employ image alignment algorithms  to further enhance the alignment of the remaining images. This entire process is automated and can be executed in an unsupervised manner.

### Data Annotation

_How to Obtain an Ideal Reference._ Obtaining a reference image with ideal exposure in each region is crucial for our subsequent image annotation. However, _ensuring the representatives of the reference image while minimizing the randomness and subjectivity introduced by humans presents significant challenges._ To address this, we start by creating a preliminary reference image using a multi-exposure fusion algorithm. Subsequently, we segment this image into blocks with the super-pixel segmentation algorithm  based on lightness and structure. Finally, experts optimize each block's exposure conditions utilizing _Adams' zone system theory_ of classical photography [14; 15] (Fig. 1(c)). This theory provides precise guidelines for achieving ideal exposure across different elements.

_How to Obtain Pixel-Level Labels by Human-in-the-Loop Methods._ Given the exorbitant cost and intricate nature of pixel-level annotation, we have streamlined the process using a combination of expert judgment and weak supervision techniques (Fig. 7). The _initial_ pixel-level annotations were generated by comparing a reference image with the 8 distorted images, documenting _exposure residual_ across pixels. Subsequently, experts further refined the _final_ pixel-level annotations to rectify potential errors, such as accurately identifying areas with logos as severely overexposed and addressing discrete anomalous pixel labeling, to ensure that the final exposure residual closely aligns with the perceived deviation of each pixel from ideal exposure. For experts, distinguishing between the reference and distorted images is relatively straightforward and far more accurate, thus facilitating practical data annotation.

Figure 6: Proposed IEA40K dataset. (a) Visualization of images with different exposure conditions; (b) Scenes containing 8 super-classes with 50+ sub-classes; (c) 17 typical lighting scenarios.

## 5 Experiments

### Settings

**Benchmark Models and Training Protocols.** To the best of our knowledge, there is no publicly available pixel-level IEA model. Therefore, we have selected several deep learning baselines based on the following criteria: classical architectures with publicly available code and SOTA performance in a specific domain. For pixel-level IEA, we selected light enhancement, light-aware, and image quality assessment (IQA) models as backbones, complemented by appropriate output heads. For holistic IEA, we chose IQA and image aesthetics assessment (IAA) models to regress scores. Further details regarding training protocols can be found in Appendix A.3.

**Evaluation Metrics.** For pixel-level IEA, we adopt SSIM and MAE to measure the structure and lightness similarity between the ground truth and predicted exposure residual. For holistic IEA, we adopt the Spearman's rank correlation coefficient (SRCC) and the linear correlation coefficient (LCC), to measure the correlation between the predicted IEA score and human opinion .

### Performance Evaluations

**Pixel-level Assessment.** Table 1 presents the results of P-IEANet and 12 other models on the IEA40k dataset. Our P-IEANet achieves SOTA performance, surpassing the second-best model with a remarkable -40% reduction in MAE loss and a significant +25% improvement in SSIM, while using an impressive -97% fewer training parameters. The efficiency of P-IEANet can be largely attributed to Haar DWT, which effectively minimizes the number of required feature extraction layers. Additionally, both the Lightness Feature Module and Structure Feature Module effectively utilize information, contributing to its exceptional performance with fewer parameters.

**Holistic Level Assessment.** We validated the effectiveness of P-IEANet on the holistic IEA task using the representative SPAQ dataset . This dataset provides annotations for holistic exposure, allowing us to test the capabilities of various baseline models. P-IEANet supports three prediction methods: _1) With Fine-tuning:_ after obtaining the residual map, it is processed through additional MLPs to predict the holistic exposure score, supervised by the MAE loss. A comparison of P-IEANet with 18 other models is presented in Table 2, where P-IEANet achieves SOTA performance. _2) Without Fine-tuning:_ after obtaining the residual map, we compute the average of absolute values and subtract this average from 1, mapping it to a scale of 0-10 (Fig. 2). Remarkably, without requiring fine-tuning for holistic exposure scoring on SPAQ, we achieved a LRCC of 0.69 and SRCC of 0.65,

Figure 8: Example exposure residuals, a robust quantitative indicator for precise assessment, uniquely generated by P-IEANet (a lower absolute exposure residual suggests a visually more pleasing result). (a) Inputs: an original image and its enhanced counterparts by classical light enhancement methods. (b) Outputs: the exposure residuals hilighting the disparity between the input and the ideal exposure.

Figure 7: Overview of the proposed IEA dataset annotation process. First, collect images of different exposure conditions and then adjust the raw image by experts to obtain reference image; Second, calculate exposure residual between the reference and 8 distorted images, and then have experts adjust the local exposure residual. The closer it is to -1, the more overexposed the corresponding pixel is; and the closer it is to 1, the more underexposed the corresponding pixel is.

even surpassing some methods that do require fine-tuning._ The above results show that P-IEANet exhibits strong criteria and scenario robustness beyond pixel-level tasks. _3) Criteria-oriented without Fine-tuning:_ Moreover, we additionally discuss an _industry-applicable criteria-oriented scoring methodology_ in Appendix A.2.

**Ablation Studies.** Table 3 evaluates the effectiveness of P-IEANet's modules. The absence of the Haar DWT and two other modules, Structure and Lightness, significantly impact the performance of P-IEANet. Specifically, the SSIM decreases by 42.6%, 28.0%, and 44.0% respectively, while the SRCC falls by 14.3%, 7.1%, and 15.8%. These results confirm that each module, particularly the Lightness Feature Module which processes low-frequency information, plays a crucial role in enhancing the model's overall performance. Qualitative visual effects analysis is provided in Fig. 5.

**Predictions for Images.** The prediction examples are shown in Fig. 2. Similar to human perception, P-IEANet's pixel-level evaluation results effectively identify areas of overexposure and underexposure that are visually displeasing, even in unconventional scenes where both underexposure and overexposure coexist. Moreover, when combined with semantic segmentation algorithms, P-IEANet enables more precise object- and pixel-level IEA results (Appendix A.2).

### Advancing Light Enhancement Methods

P-IEANet, owing to its exceptional sensitivity towards exposure, offers advantages for the exposure enhancement community in the following two aspects:

_1) Analyzing Performance Better:_ Traditionally, assessing the efficacy of image enhancement algorithms has been a time-consuming and imprecise task, relying solely on human observations. The exposure residual, uniquely generated by P-IEANet, serves as a robust quantitative indicator for precise assessment (refer to Fig. 8, where input can be either an original image or an enhanced one).

_2) Enhancing Performance Better:_ P-IEANet is compatible with many existing light enhancement methods, enabling it to boost their performance. To demonstrate this, we chose two open-source and SOTA methods, Retinex  and GASD , as baseline models. We incorporated P-IEANet as a sample evaluator in these models after enhancement, freezing P-IEANet's parameters and obtaining the absolute exposure residual as loss to include in the baseline models. Table 4 shows that on both representative datasets, LOL-v1  and LOLv2-real , the performance is improved to some extent, suggesting that P-IEANet has the potential to become an important enhancer in this field.

## 6 Conclusion

This paper investigates IEA with a novel paradigm: from holism to pixel. To our knowledge, our work introduces a new roadmap by proposing a model, dataset, and benchmark for the community. However, several challenges still remain to be addressed. For instance, evaluating images with severe misalignment issues caused by high-speed moving objects poses significant challenges. In future work, we aim to optimize our framework to support multimodal outputs and enhance the exposure perception in artificial intelligence generated content (AIGC).

## 7 Acknowledgement

This work is supported by the Funds for Creative Research Groups of China under Grant 61921003.