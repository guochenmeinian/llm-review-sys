# Universal Prompt Tuning for Graph Neural Networks

Taoran Fang\({}^{1}\), Yunchao Zhang\({}^{1}\), Yang Yang\({}^{1}\), Chunping Wang\({}^{2}\), Lei Chen\({}^{2}\)

\({}^{1}\)Zhejiang University, \({}^{2}\)FinVolution Group

{fangtr,3190105622,yangx}@zju.edu.cn,

{wangchunping02,chenlei04}@xinye.com

Corresponding author.

###### Abstract

In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called _Graph Prompt Feature (GPF)_ for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to obtain the prompted graph for the downstream task in an adaptive manner. We provide rigorous derivations to demonstrate the universality of GPF and make guarantee of its effectiveness. The experimental results under various pre-training strategies indicate that our method performs better than fine-tuning, with an average improvement of about \(1.4\%\) in full-shot scenarios and about \(3.2\%\) in few-shot scenarios. Moreover, our method significantly outperforms existing specialized prompt-based tuning methods when applied to models utilizing the pre-training strategy they specialize in. These numerous advantages position our method as a compelling alternative to fine-tuning for downstream adaptations. Our code is available at: https://github.com/zjunet/GPF.

## 1 Introduction

Graph neural networks (GNNs) have garnered significant attention from researchers due to their remarkable success in graph representation learning (Kipf and Welling, 2017; Hamilton et al., 2017; Xu et al., 2019). However, two fundamental challenges hinder the large-scale practical applications of GNNs. One is the scarcity of labeled data in the real world (Zitnik et al., 2018), and the other is the low out-of-distribution generalization ability of the trained models (Hu et al., 2020; Knyazev et al., 2019; Yehudai et al., 2021; Morris et al., 2019). To overcome these challenges, researchers have made substantial efforts in designing pre-trained GNN models (Xia et al., 2022; Hu et al., 2020; Lu et al., 2021) in recent years. Similar to the pre-trained models in the language field, pre-trained GNN models undergo training on extensive pre-training datasets and are subsequently adapted to downstream tasks. Most existing pre-trained GNN models obey the "pre-train, fine-tune" learning strategy (Xu et al., 2021). Specifically, we train a GNN model with a massive corpus of pre-training graphs, then we utilize the pre-trained GNN model as initialization and fine-tune the model parameters based on the specific downstream task.

However, the "pre-train, fine-tune" framework of pre-trained GNN models also presents several critical issues . First, there is a misalignment between the objectives of pre-training tasks and downstream tasks . Most existing pre-trained models employ self-supervised tasks  such as edge prediction and attribute masking as the training targets during pre-training, while the downstream tasks involve graph or node classification. This disparity in objectives leads to sub-optimal performance . Additionally, ensuring that the model retains its generalization ability is challenging. Pre-trained models may suffer from catastrophic forgetting  during downstream adaptation. This issue becomes particularly acute when the downstream data is small in scale, approaching the few-shot scenarios . The pre-trained model tends to over-fit the downstream data in such cases, rendering the pre-training process ineffective.

_"If your question isn't getting the desired response, try rephrasing it."_ In recent years, a novel approach called prompt tuning has emerged as a powerful method for downstream adaptation, addressing the aforementioned challenges. This technique has achieved significant success in Natural Language Processing  and Computer Vision . Prompt tuning provides an alternative method for adapting pre-trained models to specific downstream tasks: it freezes the parameters of the pre-trained model and modifies the input data. Unlike fine-tuning, prompt tuning diverges from tuning the parameters of the pre-trained model and instead focuses on adapting the data space by transforming the input.

Despite that, applying prompt tuning on pre-trained GNN models poses significant challenges and is far from straightforward. First, the diverse pre-training strategies employed on graphs make it difficult to design suitable prompting functions. Previous research  suggests that the prompting function should be closely aligned with the pre-training strategy. For pre-trained language models, the typical pre-training tasks involve masked sentence completion . In order to align with this task, we may modify a sentence like "I received a gift" to "I received a gift, and I feel [Mask]" to make it closer to the task of sentence completion. However, in the case of graph pre-training, there is no unified pre-training task, making it challenging to design feasible prompting functions. Some pioneering studies  have applied prompt-based tuning methods to models pre-trained by edge prediction . They introduce virtual class-prototype nodes/graphs with learnable links into the original graph, making the adaptation process more akin to edge prediction. However, these methods have limited applicability and are only compatible with specific models. When it comes to more intricate pre-training strategies, it becomes challenging to design manual prompting functions in the same manner as employed for link prediction. Consequently, no prompt-based tuning method is available for models pre-trained using alternative strategies, such as attribute masking . Furthermore, existing prompt-based tuning methods for GNN models are predominantly designed based on intuition, lacking theoretical guarantees for their effectiveness.

In this paper, we address the aforementioned issues for graph prompt tuning. To deal with the diversity of graph pre-training strategies, we propose a universal prompt-based tuning method that can be

Figure 1: **Comparison of universal graph prompt tuning and existing approaches. (a) Fine-tuning updates the parameters of the pre-trained GNN model. (b) Existing specialized prompt-based tuning methods generate manual graph templates to adapt the models under certain pre-training strategies. (c) Our universal graph prompt tuning works on the feature space of the input graph. It can achieve an equivalent effect to any form of prompting function and be applied to any pre-trained GNN model.**

applied to the pre-trained GNN models that employ any pre-training strategy. Figure 1 illustrates the distinction between our universal prompt-based tuning method and existing approaches. Our solution, called Graph Prompt Feature (GPF), operates on the input graph's feature space and involves adding a shared learnable vector to all node features in the graph. This approach is easily applicable to any GNN architecture. We rigorously demonstrate that GPF can achieve comparable results to any form of prompting function when applied to arbitrary pre-trained GNN models. Consequently, instead of explicitly illustrating the prompting function corresponding to each pre-training strategy, we adopt GPF to dynamically obtain the prompted graph for downstream tasks. We also introduce a theoretically stronger variant of GPF, named GPF-plus, for practical application, which incorporates different prompted features for different nodes in the graph. To guarantee the effectiveness of our proposed GPF and GPF-plus, we provide theoretical analyses to prove that GPF and GPF-plus are not weaker than full fine-tuning and can obtain better theoretical tuning results in some cases. Furthermore, we conduct extensive experiments to validate the efficacy of our methods. Despite using a significantly smaller number of tunable parameters than fine-tuning, GPF and GPF-plus achieve better results across all pre-training strategies. For models pre-trained using edge prediction, GPF and GPF-plus exhibit a substantial performance advantage over existing specialized prompt-based tuning methods. Overall, the contributions of our work can be summarized as follows:

* To the best of our knowledge, we present the first investigation of universal prompt-based tuning methods for existing pre-trained GNN models. We propose GPF and its variant, GPF-plus, as novel approaches for universal graph prompt tuning. Our methods can be applied to the pre-trained GNN models that employ any pre-training strategy.
* We provide theoretical guarantees for the effectiveness of GPF and GPF-plus. We demonstrate that GPF and GPF-plus can achieve an equivalent effect to any prompting function and can obtain better tuning results in some cases compared to fine-tuning.
* We conduct extensive experiments (both full-shot and few-shot scenarios) to validate the effectiveness of GPF and GPF-plus. The experimental results indicate that GPF and GPF-plus can perform better than fine-tuning, with an average improvement of about \(1.4\%\) in full-shot scenarios and about \(3.2\%\) in few-shot scenarios. Furthermore, GPF and GPF-plus significantly outperform existing prompt-based tuning methods when applied to models that utilize the pre-training strategy they specialize in.

## 2 Related work

Pre-trained GNN ModelsInspired by the remarkable achievements of pre-trained models in Natural Language Processing (Qiu et al., 2020) and Computer Vision (Long et al., 2022), substantial efforts have been dedicated to pre-trained GNN models (PGMs) (Xia et al., 2022) in recent years. These methods utilize self-supervised strategies (Jin et al., 2020) to acquire meaningful representations from extensive pre-training graphs. GAE (Kipf and Welling, 2016) first uses edge prediction as the objective task to train graph representations. Deep Graph Infomax (DGI) (Velickovic et al., 2019) and InfoGraph (Sun et al., 2019) are proposed to garner nodes or graph representations by maximizing the mutual information between graph-level and substructure-level representations of different granularity. Hu et al. (2020) employ attribute masking and context prediction as pre-training tasks to predict molecular properties and protein functions. Both GROVER (Rong et al., 2020) and MGSSL (Zhang et al., 2021) propose to predict the presence of the motifs or generate them with the consideration that rich domain knowledge of molecules hides in the motifs. Graph Contrastive Learning (GCL) is another widely adopted pre-training strategy for GNN models. GraphCL (You et al., 2020) and JOAO (You et al., 2021) propose various augmentation strategies to generate different augmented views for contrastive learning. In summary, there exists a diverse range of pre-training strategies for GNN models, each characterized by unique objectives.

Prompt-based Tuning MethodsPrompt-based tuning methods, originating from Natural Language Processing, have been widely used to facilitate the adaptation of pre-trained language models to various downstream tasks (Liu et al., 2021). Research has also explored the design of soft prompts to achieve optimal performance (Lester et al., 2021, Liu et al., 2021). These methods freeze the parameters of the pre-train models and introduce additional learnable components in the input space, thereby enhancing the compatibility between inputs and pre-trained models. Aside from the success of prompts in the language field, the prompting methods are utilized in other areas. Jia et al. (2022) and Bahng et al. (2022) investigate the efficacy of adapting large-scale models in the vision field by modifying input images at the pixel level. In the realm of graph neural networks, the exploration of prompt-based tuning methods is still limited. Some pioneering work (Sun et al., 2022; Liu et al., 2023) applies prompt-based tuning methods on the models pre-trained by edge prediction (Kipf and Welling, 2016). These methods introduce virtual class-prototype nodes/graphs with learnable links into the input graph, making the downstream adaptation more closely resemble edge prediction. However, these methods are specialized for models pre-trained using edge prediction and cannot be applied to models trained with other strategies. We are the first to investigate the universal prompt-based tuning methods that can be applied to the GNN models under any pre-training strategy.

## 3 Methodology

We introduce _graph prompt tuning_ for adapting pre-trained GNN models to downstream tasks. It is important to note that there are several types of downstream tasks in graph analysis, including node classification, link prediction, and graph classification. We first concentrate on the graph classification task and then extend our method to node-wise tasks. We define the notations in Section 3.1, then illustrate the process of graph prompt tuning in Section 3.2. We introduce our universal graph prompt tuning method in Section 3.3 and provide theoretical analyses in Section 3.4. Finally, we present the extension of our method to node-wise tasks (node classification and link prediction) in the appendix.

### Preliminaries

Let \(=(,)\) represents a graph, where \(=\{v_{1},v_{2},,v_{N}\}\), \(\) denote the node set and edge set respectively. The node features can be denoted as a matrix \(=\{x_{1},x_{2},,x_{N}\}^{N F}\), where \(x_{i}^{F}\) is the feature of the node \(v_{i}\), and \(F\) is the dimensionality of node features. \(\{0,1\}^{N N}\) denotes the adjacency matrix, where \(_{ij}=1\) if \((v_{i},v_{j})\).

Fine-Tuning Pre-trained Models.Given a pre-trained GNN model \(f\), a learnable projection head \(\) and a downstream task dataset \(=\{(_{1},y_{1}),,(_{m},y_{m})\}\), we adjust the parameters of the pre-trained model \(f\) and the projection head \(\) to maximize the likelihood of predicting the correct labels \(y\) of the downstream graph \(\):

\[_{f,}P_{f,}(y|)\] (1)

### Graph Prompt Tuning

Overall Process.Our proposed _graph prompt tuning_ works on the input space by drawing on the design of the prompt tuning in the language field (Liu et al., 2022). Given a frozen pre-trained GNN model \(f\), a learnable projection head \(\), and a downstream task dataset \(=\{(_{1},y_{1}),,(_{m},y_{m})\}\), our target is to obtain a task-specific _graph prompt_\(g_{}\) parameterized by \(\). The _graph prompt_\(g_{}()\) transforms the input graph \(\) into a specific _prompted graph_\(g_{}()\). And then \(g_{}()\) will replace \(\) as input to the pre-trained GNN model \(f\). During the downstream task training, we select the optimal parameters of \(\) and \(\) that maximize the likelihood of predicting the correct labels \(y\) without tuning the pre-trained model \(f\), which can be formulated as:

\[_{,}P_{f,}(y|g_{}())\] (2)

During the evaluation stage, the test graph \(_{}\) is first transformed by _graph prompt_\(g_{}()\), and the resulting prompted graph \(g_{}(_{})\) is processed through the frozen GNN model \(f\).

Practical Usage.In this part, we provide a detailed description of the refined process of _graph prompt tuning_, which comprises two fundamental steps: _template design_ and _prompt optimization_.

A. Template Design.Given an input graph \(\), we first generate a _graph template_\(^{*}\), which includes learnable components in its adjacency matrix \(^{*}\) and feature matrix \(^{*}\). Previous research has attributed the success of prompt tuning to bridging the gap between pre-training tasks and downstream tasks (Liu et al., 2022). Consequently, it implies that the specific form of the graph template is influenced by the pre-training strategy employed by the model. For a specific pre-training task \(t\) and an input graph \(\), the graph template \(^{*}\) can be expressed as:

\[^{*}(^{*},^{*})=_{t}()\] (3)where the graph template \(^{*}\) may contain learnable parameters (_i.e._, tunable links or node features) in its adjacency matrix or feature matrix (similar to the inclusion of learnable soft prompts in a sentence), the candidate space for \(^{*}\) is \(\), and the candidate space for \(^{*}\) is \(\).

_B. Prompt Optimization._ Once we have obtained the graph template \(^{*}\), our next step is to search for the optimal \(}\) and \(}\) within their respective candidate spaces \(\) and \(\) that maximize the likelihood of correctly predicting the labels \(y\) using the pre-trained model \(f\) and a learnable projection head \(\). This process can be expressed as:

\[_{},},}P_{f,}(y|^{*})\] (4)

The graph \(}\) composed of \(}\) and \(}\) can be considered as the the _prompted graph_\(g_{}()\) mentioned in Formula 2.

**Practical Challenges.** The specific form of the graph template is closely tied to the pre-training task \(t\) employed by the model \(f\). However, designing the prompting function \(_{t}()\) is challenging and varies for different pre-training tasks. Pioneering works (Sun et al., 2022, Liu et al., 2023) have proposed corresponding prompting functions \(_{t}()\) for a specific pre-training strategy, with a focus on models pre-trained using edge prediction. However, many other pre-training strategies (Hu et al., 2020, Xia et al., 2022), such as attribute masking and context prediction, are widely utilized in existing pre-trained GNN models, yet no research has been conducted on designing prompting functions for these strategies. Furthermore, existing prompting functions are all intuitively designed, and these manual prompting functions lack a guarantee of effectiveness. It raises a natural question: _Can we design a universal prompting method that can be applied to any pre-trained model, regardless of the underlying pre-training strategy?_

### Universal Graph Prompt Design

In this section, we introduce a universal prompting method and its variant. Drawing inspiration from the success of pixel-level Visual Prompt (VP) techniques (Bahng et al., 2022, Wu et al., 2022, Xing et al., 2022) in Computer Vision, our methods introduce learnable components to the feature space of the input graph. In Section 3.4, we will demonstrate that these prompting methods can theoretically achieve an equivalent effect as any prompting function \(_{t}()\).

**Graph Prompt Feature (GPF).** GPF focuses on incorporating additional learnable parameters into the feature space of the input graph. Specifically, the learnable component \(p\) is a vector of dimension \(F\), where \(F\) corresponds to the dimensionality of the node features. It can be denoted as:

\[p^{F}\] (5)

The learnable vector \(p\) is added to the graph features \(\) to generate the prompted features \(^{*}\), which can be expressed as:

\[=\{x_{1},x_{2},,x_{N}\}^{*}=\{x_{1}+p,x_{2}+p, ,x_{N}+p\}\] (6)

The prompted features \(^{*}\) replace the initial features \(\) and are processed by the pre-trained model.

**Graph Prompt Feature-Plus (GPF-plus).** Building upon GPF, we introduce a variant called GPF-plus, which assigns an independent learnable vector \(p_{i}\) to each node \(v_{i}\) in the graph. It can be expressed as:

\[p_{1},p_{2}, p_{N}^{F}\] (7)

\[=\{x_{1},x_{2},,x_{N}\}^{*}=\{x_{1}+p_{1},x_{2} +p_{2},,x_{N}+p_{N}\}\] (8)

Similarly to GPF, the prompted features \(^{*}\) replace the initial features \(\) and are processed by the pre-trained model. However, such a design is not universally suitable for all scenarios. For instance, when training graphs have different scales (i.e., varying node numbers), it is challenging to train such a series of \(p_{i}\). Additionally, when dealing with large-scale input graphs, such design requires a substantial amount of storage resources due to its \(O(N)\) learnable parameters. To address these issues, we introduce an attention mechanism in the generation of \(p_{i}\), making GPF-plus more parameter-efficient and capable of handling graphs with different scales. In practice, we train only \(k\) independent basis vectors \(p^{b}\), which can be expressed as:

\[p_{1}^{b},p_{2}^{b}, p_{k}^{b}^{F}\] (9)where \(k\) is a hyper-parameter that can be adjusted based on the downstream dataset. To obtain \(p_{i}\) for node \(v_{i}\), we utilize attentive aggregation of these basis vectors with the assistance of \(k\) learnable linear projections \(a\). The calculation process can be expressed as:

\[p_{i}=_{j}^{k}_{i,j}p_{j}^{b}_{i,j}=^{T}x_ {i})}{_{l}^{k}(a_{l}^{T}x_{i})}\] (10)

Subsequently, \(p_{i}\) is used to generate the prompted feature \(^{*}\) as described in Formula 8.

### Theoretical Analysis

In this section, we provide theoretical analyses for our proposed GPF and GPF-plus. Our analyses are divided into two parts. First, we certify the universality of our methods. We demonstrate that our approaches can theoretically achieve results equivalent to any prompting function \(_{t}()\). It confirms the versatility and applicability of our methods across different pre-training strategies. Then, we make guarantee of the effectiveness of our proposed methods. Specifically, we demonstrate that our proposed graph prompt tuning is not weaker than full fine-tuning, which means that in certain scenarios, GPF and GPF-plus can achieve superior tuning results compared to fine-tuning. It is important to note that our derivations in the following sections are based on GPF, which adds a global extra vector \(p\) to all nodes in the graph. GPF-plus, being a more powerful version, can be seen as an extension of GPF and degenerates to GPF when the hyperparameter \(k\) is set to \(1\). Therefore, the analyses discussed for GPF are also applicable to GPF-plus.

Before we illustrate our conclusions, we first provide some preliminaries. For a given pre-training task \(t\) and an input graph \((,)\), we assume the existence of a prompting function \(_{t}()\) that generates a graph template \(^{*}(^{*},^{*})=_{t}()\). The candidate space for \(^{*}\) and \(^{*}\) is denoted as \(\) and \(\), respectively.

**Theorem 1**.: _(Universal Capability of GPF) Given a pre-trained GNN model \(f\), an input graph \((,)\), an arbitrary prompting function \(_{t}()\), for any prompted graph \(}(},} )\) in the candidate space of the graph template \(^{*}=_{t}()\), there exists a GPF extra feature vector \(\) that satisfies:_

\[f(,+)=f(},})\] (11)

The complete proof of Theorem 1 can be found in the appendix. Theorem 1 implies that GPF can achieve the theoretical performance upper bound of any prompting function described in Formula 3 and 4. Specifically, if optimizing the graph template \(^{*}\) generated by a certain prompting function \(_{t}()\) can yield satisfactory graph representations, then theoretically, optimizing the vector \(p\) of GPF can also achieve the exact graph representations. This conclusion may initially appear counter-intuitive since GPF only adds learnable components to node features without explicitly modifying the graph structure. The key lies in understanding that the feature matrix \(\) and the adjacency matrix \(\) are not entirely independent during the processing. The impact of graph structure modifications on the final graph representations can also be obtained through appropriate modifications to the node features. Therefore, GPF and GPF-plus, by avoiding the explicit illustration of the prompting function \(_{t}()\), adopt a simple yet effective architecture that enables them to possess universal capabilities in dealing with pre-trained GNN models under various pre-training strategies.

Next, we make guarantee of the effectiveness of GPF and demonstrate that GPF is not weaker than fine-tuning, which means GPF can achieve better theoretical tuning results in certain situations compared to fine-tuning. In Natural Language Processing, the results obtained from fine-tuning are generally considered the upper bound for prompt tuning results (Lester et al., 2021; Liu et al., 2021; Ding et al., 2022). It is intuitive to believe that fine-tuning, which allows for more flexible and comprehensive parameter adjustments in the pre-trained model, can lead to better theoretical results during downstream adaptation. However, in the graph domain, the architecture of graph neural networks magnifies the impact of input space transformation on the final representations to some extent. To further illustrate this point, following previous work (Kumar et al., 2022; Tian et al., 2023; Wei et al., 2021), we assume that the downstream task utilizes the squared regression loss \(l=_{i}(_{i}-y_{i})^{2}\).

**Theorem 2**.: _(Effectiveness Guarantee of GPF) For a pre-trained GNN model \(f\), a series of graphs \(=\{(_{1}(_{1},_{1}),y_{1}), ,(_{m}(_{m},_{m}),y_{m})\}\) under the non-degeneracy condition, and a linear projection head \(\), there exists \(^{}=\{y^{}_{1},,y^{}_{m}\}\) for \(y_{1}=y^{}_{1},,y_{m}=y^{}_{m}\) that satisfies:_

\[l_{}=_{p,}_{i}^{m}(f(_{i},_{i}+p )-y_{i})^{2}<l_{}=_{f,}_{i}^{m}(f( _{i},_{i})-y_{i})^{2}\] (12)

The detailed proof of Theorem 2 and the description of the degeneracy condition can be found in the appendix. Theorem 2 indicates that GPF obtains a lower minimum loss compared to fine-tuning in certain scenarios, demonstrating its ability to achieve better theoretical tuning results.

## 4 Experiments

### Experiment Setup

**Model Architecture and Datasets.** We adopt the widely used 5-layer GIN [Xu et al., 2019] as the underlying architecture for our models, which aligns with the majority of existing pre-trained GNN models [Xia et al., 2022b, Hu et al., 2020a, Qiu et al., 2020a, You et al., 2020, Suresh et al., 2021, Xu et al., 2021b, Zhang et al., 2021b, You et al., 2022, Xia et al., 2022a]. As for the benchmark datasets, we employ the chemistry and biology datasets published by Hu et al. [2020a]. A comprehensive description of these datasets can be found in the appendix.

**Pre-training Strategies.** We employ five widely used strategies (tasks) to pre-train the GNN models, including Deep Graph Infomax (denoted by Infomax) [Velickovic et al., 2019a], Edge Prediction (denoted by EdgePred) [Kipf and Welling, 2016a], Attribute Masking (denoted by AttrMasking) [Hu et al., 2020a], Context Prediction (denoted by ContextPred) [Hu et al., 2020a] and Graph Contrastive Learning (denoted by GCL) [You et al., 2020]. A detailed description of these pre-training strategies can be found in the appendix.

**Tuning Strategies.** We adopt the pre-trained models to downstream tasks with different tuning strategies. Given a pre-trained GNN model \(f\), a task-specific projection head \(\),

* _Fine Tuning_ (denoted as FT). We tune the parameters of the pre-trained GNN model \(f\) and the projection head \(\) simultaneously during the downstream training stage.
* _Graph Prompt Feature_ (denoted as GPF). We freeze the parameters of the pre-trained model \(f\) and introduce an extra learnable feature vector \(p\) into the feature space of the input graph described as Formula 6. We tune the parameters of the projection head \(\) and feature vector \(p\) during the downstream training stage.
* _Graph Prompt Feature-Plus_ (denoted as GPF-plus). We freeze the parameters of pre-trained model \(f\) and introduce \(k\) learnable basis vectors \(p_{1}^{b},,p_{k}^{b}\) with \(k\) learnable linear projections \(a_{1},,a_{k}\) to calculate the node-wise \(p_{i}\) as Formula 10. We tune the parameters of the projection head \(\), basis vectors \(p_{1}^{b},,p_{k}^{b}\), and linear projections \(a_{1},,a_{k}\) during the downstream training stage.

**Implementation.** We perform five rounds of experiments with different random seeds for each experimental setting and report the average results. The projection head \(\) is selected from a range of -layer MLPs with equal widths. The hyper-parameter \(k\) of GPF-plus is chosen from the range . Further details on the hyper-parameter settings can be found in the appendix.

### Main Results

We compare the downstream performance of models trained using different pre-training and tuning strategies, and the overall results are summarized in Table 1. Our systematic study suggests the following observations:

_1. Our graph prompt tuning outperforms fine-tuning in most cases._ Based on the results presented in Table 1, it is evident that GPF and GPF-plus achieve superior performance compared to fine-tuning in the majority of cases. Specifically, GPF outperforms fine-tuning in \(28/36\) experiments, while GPF-plus outperforms fine-tuning in \(29/36\) experiments. It is worth noting that the tunable parameters in GPF and GPF-plus are significantly fewer in magnitude than those in fine-tuning (details can be found in the appendix). These experimental findings highlight the efficacy of our methods and demonstrate their capability to unleash the power of the pre-trained models.

_2. GPF and GPF-plus exhibit universal capability across various pre-training strategies._ GPF and GPF-plus present favorable tuning performance across all pre-training strategies examined in our experiments, consistently surpassing the average results obtained from fine-tuning. Specifically, GPF achieves an average improvement of \(1.14\%\), while GPF-plus achieves an average improvement of \(1.60\%\). These results signify the universal capability of GPF and GPF-plus, enabling their application to models trained with any pre-training strategy.

_3. GPF-plus marginally outperforms GPF._ Among the two graph prompt tuning methods, GPF-plus performs better than GPF in the majority of experiments (\(26/36\)). As discussed in Section 3.3, GPF-plus offers greater flexibility and expressiveness compared to GPF. The results further affirm that GPF-plus is an enhanced version of graph prompt tuning, aligning with the theoretical analysis.

### Comparison with Existing Graph Prompt-based Methods

We also conducted a comparative analysis between our proposed methods, GPF and GPF-plus, and existing graph prompt-based tuning approaches (Sun et al., 2022, Liu et al., 2023). Both of them

   Pre-training Strategy & Tuning Strategy & BBBP & Tox21 & ToxCast & SIDER & ClinTox & MUV & HIV & BACE & PPI & **Avg.** \\   & FT & **67.55** & 78.57 & 65.16 & 63.34 & 70.06 & **81.42** & 77.71 & 81.32 & 71.29 &  \\  & & \( 2.06\) & \( 0.51\) & \( 0.53\) & \( 0.45\) & \( 1.45\) & \( 2.65\) & \( 0.45\) & \( 1.25\) & \( 1.79\) \\  & GPF & **66.83** & 79.09 & 66.10 & **66.17** & 73.56 & 80.43 & 76.49 & 83.60 & 77.02 &  \\  & & \( 0.86\) & \( 0.25\) & \( 0.53\) & \( 0.81\) & \( 3.94\) & \( 0.53\) & \( 0.18\) & \( 1.00\) & \( 0.42\) & \\  & GPF-plus & **67.17** & **79.13** & **66.35** & **65.62** & **75.12** & **81.33** & **77.73** & **83.67** & **77.03** &  \\  & & \( 0.36\) & \( 0.70\) & \( 0.37\) & \( 0.74\) & \( 2.45\) & \( 1.52\) & \( 1.14\) & \( 1.08\) & \( 0.32\) & \\   & FT & 66.33 & **78.28** & 65.34 & 66.77 & **74.46** & **81.78** & **77.90** & 80.94 & 73.93 &  \\  & & \( 0.55\) & \( 0.05\) & \( 0.30\) & \( 0.13\) & \( 2.82\) & \( 1.95\) & \( 0.18\) & \( 1.99\) & \( 1.17\) & \\  & GPF & **68.09** & **79.04** & 66.32 & **69.13** & 75.06 & **82.17** & **78.86** & **84.33** & **78.91** &  \\  & & \( 0.38\) & \( 0.90\) & \( 0.42\) & \( 1.16\) & \( 1.02\) & \( 0.65\) & \( 1.42\) & \( 0.54\) & \( 0.25\) & \\  & GPF-plus & 67.71 & **78.87** & **66.58** & **68.65** & **76.17** & **81.12** & **78.13** & **85.76** & **78.90** &  \\  & & \( 0.64\) & \( 0.31\) & \( 0.13\) & \( 0.72\) & \( 9.98\) & \( 1.32\) & \( 1.12\) & \( 0.36\) & \( 0.11\) & \\   & FT & **69.65** & 78.29 & 66.36 & 64.45 & 73.71 & 82.36 & **79.20** & 84.66 & 72.10 &  \\  & & \( 0.87\) & \( 0.44\) & \( 0.57\) & \( 0.6\) & \( 1.57\) & \( 1.22\) & \( 0.51\) & \( 0.84\) & \( 1.94\) & \\  & GPF & **68.48** & **79.99** & **67.92** & **66.18** & **74.51** & **84.34** & **78.62** & **85.32** & **77.42** &  \\  & & \( 0.88\) & \( 0.24\) & \( 0.35\) & \( 0.46\) & \( 2.72\) & \( 0.25\) & \( 1.46\) & \( 0.41\) & \( 0.07\) & \\  & GPF-plus & **69.15** & **80.05** & **67.58** & **66.94** & **75.25** & **84.48** & **78.40** & **85.81** & **77.71** & \\  & & \( 0.82\) & \( 0.46\) & \( 0.54\) & \( 0.95\) & \( 1.88\) & \( 0.78\) & \( 0.16\) & \( 0.43\) & \( 0.21\) & **76.15** \\   & FT & **69.49** & **73.35** & **62.54** & **60.63** & **75.17** & **69.78** & **78.26** & **75.51** & 67.76 &  \\  & & \( 0.35\) & \( 0.70\) & \( 0.26\) & \( 1.26\) & \( 2.14\) & \( 1.44\) & \( 0.73\) & \( 2.01\) & \( 0.78\) & \\  & GPF & **71.11** & **73.64** & **62.70** & **61.26** & **70.09** & **75.52** & **78.55** & **67.60** & & \\  & & \( 1.20\) & \( 0.25\) & \( 0.46\) & \( 0.53\) & \( 2.98\) & \( 0.67\) & \( 1.09\) & \( 0.56\) & \( 0.57\) & \(\) \\  & GPF-plus & **72.18** & 73.35** & **62.76** & **62.37** & 73.90 & **72.94** & **77.51** & **79.61** & **67.89** & **71.39** \\  & & \( 0.93\) & \( 0.43\) & \( 0.75\) & \( 0.38\) & \( 2.47\) & \( 1.87\) & \( 0.82\) & \( 2.06\) & \( 0.69\) & \\   

Table 1: Test ROC-AUC (%) performance on molecular prediction benchmarks and protein function prediction benchmarks with different pre-training strategies and different tuning strategies.

   Pre-training Strategy & Tuning Strategy & BBBP & Tox21 & ToxCast & SIDER & ClinTox & MUV & HIV & BACE & PPI & **Avg.** \\   & FT & 66.56 & 78.67 & **66.29** & 64.35 & 69.07 & 79.67 & 77.44 & 80.90specialize in tuning the models pre-trained by Edge Prediction (also known as Link Prediction). We apply GPPT (Sun et al., 2022), GPPT without orthogonal prompt constraint loss (denoted as GPPT (w/o ol)) (Sun et al., 2022), GraphPrompt (Liu et al., 2023) to the models pre-trained using Edge Prediction, and the results are summarized in Table 2. It is worth mentioning that GPPT is originally designed for node classification tasks. Therefore, we make minor modifications by substituting class-prototype nodes with class-prototype graphs to adapt it for graph classification tasks. The experimental results indicate that our proposed GPF and GPF-plus outperform existing graph prompt-based tuning methods by a significant margin. On the chemistry and biology benchmarks, GPF and GPF-plus achieve average improvements of \(12\%\), \(3\%\), and \(13\%\) over GPPT, GPPT (w/o ol), and GraphPrompt, respectively. These results showcase the ability of GPF and GPF-plus to achieve superior results compared to existing graph prompt-based tuning methods designed specifically for the pre-training strategy. Furthermore, it is worth highlighting that GPF and GPF-plus are the only two graph prompt-based tuning methods that surpass the performance of fine-tuning.

### Additional Experiments

Few-shot graph classification.Prompt tuning has also been recognized for its effectiveness in addressing few-shot downstream tasks (Brown et al., 2020; Schick and Schutze, 2020, 2020; Liu et al., 2021; Liu et al., 2021; 2023). We evaluate the efficacy of our proposed methods in handling few-shot scenarios. To conduct few-shot graph classification on the chemistry and biology datasets, we limit the number of training samples in the downstream tasks to 50 (compared to the original range of 1.2k to 72k training samples). The results are summarized in Table 4 of the appendix. Compared to the full-shot scenarios, our proposed graph prompt tuning demonstrates even more remarkable performance improvement (an average improvement of \(2.95\%\) for GPF and \(3.42\%\) for GPF-plus) over fine-tuning in the few-shot scenarios. This finding indicates that our solutions retain a higher degree of generalization ability in pre-trained models during few-shot downstream adaptations compared to fine-tuning.

Training process analysis.We conducted an analysis of the training process using different tuning methods on the biology datasets with the GNN models that employ Attribute Masking and Context Prediction as their pre-training tasks (Hu et al., 2020). Figure 2 presents the training and test curves during the adaptation stage. From Figure 2 (a), it can be observed that the ROC-AUC scores of the training set consistently increase during the adaptation stage for both our proposed methods and fine-tuning. However, from Figure 2 (b), we can find that their behavior on the test set is quite distinct. For fine-tuning, the ROC-AUC scores on the test set exhibit fluctuations and continuously decrease after an initial increase. On the other hand, when applying GPF or GPF-plus to adapt pre-trained models, the ROC-AUC scores on the test set continue to grow and remain consistently high. These results indicate that fully fine-tuning a pre-trained GNN model on a downstream task may lose the model's generalization ability. In contrast, employing our proposed graph prompt tuning methods can significantly alleviate this issue and maintain superior performance on the test set.

## 5 Conclusion

In this paper, we introduce a universal prompt-based tuning method for pre-trained GNN models. Our method GPF and its variant GPF-plus operate on the feature space of the downstream input graph. GPF and GPF-plus can theoretically achieve an equivalent effect to any form of prompting function, meaning we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we can adaptively use GPF to obtain the prompted graph for downstream task adaptation. Compared to fine-tuning, the superiority of our method is demonstrated both theoretically and empirically, making it a compelling alternative for downstream adaptations.

Figure 2: Training and test curves of different tuning methods.

Acknowledgements

This work was partially supported by Zhejiang NSF (LR22F020005), the National Key Research and Development Project of China (2018AAA0101900), and the Fundamental Research Funds for the Central Universities.