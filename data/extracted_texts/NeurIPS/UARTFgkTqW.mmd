# MagR: Weight Magnitude Reduction for Enhancing Post-Training Quantization

Aozhong Zhang\({}^{1}\) Naigang Wang\({}^{2}\) Yanxia Deng\({}^{1}\) Xin Li\({}^{1}\) Zi Yang\({}^{1}\) Penghang Yin\({}^{1}\)

\({}^{1}\)University at Albany, SUNY \({}^{2}\) IBM T. J. Watson Research Center

{azhang3, ydeng5, xli48, zyang8, pyin}@albany.edu

nwang@us.ibm.com

###### Abstract

In this paper, we present a simple optimization-based preprocessing technique called Weight **M**agnitude **R**eduction (MagR) to improve the performance of post-training quantization. For each linear layer, we adjust the pre-trained floating-point weights by solving a channel-wise \(_{}\)-regularized optimization problem. This process greatly diminishes the maximum magnitude of the weights and smooths out outliers, while preserving the layer's output. The preprocessed weights exhibit reduced range, which facilitates the subsequent quantization process. To implement MagR, we address the \(_{}\)-regularization by employing an efficient proximal gradient descent algorithm. Unlike existing preprocessing methods that involve linear transformations and subsequent post-processing steps, which can introduce significant overhead at inference time, MagR functions as a non-linear transformation, eliminating the need for any additional post-processing. This ensures that MagR introduces no overhead whatsoever during inference. Our experiments demonstrate that MagR achieves state-of-the-art performance on the Llama family of models. For example, we achieve a Wikitext2 perplexity of 5.95 on the LLaMA2-70B model for per-channel INT2 weight quantization without incurring any inference overhead. The code is available at https://github.com/AozhongZhang/MagR

## 1 Introduction

Large language models (LLMs) have achieved outstanding performance across a broad range of applications, demonstrating remarkable success. However, their unprecedented model size has led to many computation operations and substantial memory footprints, becoming significant barriers to their practical deployment and adoption in production environments. Accordingly, it is highly desirable to develop efficient model compression techniques for LLMs so they can be more widely deployed in resource-limited scenarios. Among the various techniques to compress and accelerate deep neural networks (DNNs), low-precision quantization has proven to be highly effective across numerous application domains and is widely adopted for accelerating DNNs. For LLMs, the inference runtime is dominated by the token generation process, where output tokens are produced sequentially, one at a time. This process is known to be memory bandwidth bound . As a result, the quantization of LLMs has primarily focused on reducing the bit-width of model weights, with the dual goals of lowering the model's footprint to enable deployment on resource-constrained devices and decreasing the memory bandwidth requirements to improve computational efficiency and accelerate inference.

The enormous computational demands for pre-training and fine-tuning Large Language Models (LLMs) have led to the emergence of Post-Training Quantization (PTQ) , as a promising solution for quantizing these models. Unlike Quantization Aware Training (QAT) , which is designed to minimize a global training loss for quantization parameters, PTQ directly applies low-precision calibration to a pretrained full-precision model using a minimal set of calibration samples. By aiming to identify an optimal quantized model locally through the minimization of a simplified surrogate loss, PTQ offers computational savings and resource efficiency compared to QAT. However, PTQ often lags behind QAT in accuracy, particularly for ultra-low precision lower than 4-bit. Thus, it remains an open problem to achieve an improved balance between cost and performance for PTQ-based approaches.

**Motivation.** To achieve state-of-the-art performance, the latest advances in PTQ [8; 25; 26; 36; 42] have proposed applying a linear transformation to process the pre-trained weights within a linear layer. This strategy of linear transformation aims to make the weights more suitable for the subsequent quantization procedure by reducing their magnitudes and suppressing outliers. In a nutshell, given the features \(\) and weights \(\), one constructs linear transformation \(\) such that \(\) is better conditioned than \(\) in terms of being quantization-friendly. Such designs of \(\) include diagonal matrices (so-called channel-wise scaling) [25; 36; 42], random transformations [8; 39], and finite frames [1; 13]. Then, quantization is performed on \(\) instead of the original weights \(\). To preserve the layer's output, however, the inverse transformation \(^{-1}\) has to be in turn applied to the features \(\), namely,

\[=(^{-1})()(^{-1})( ),\]

with \(()\) being the quantized weights. PTQ done this way requires modifications on the original neural architecture, which involves additional computations of \(^{-1}\) and extra memory storage for \(^{-1}\) at inference time. As a result, these steps introduce overhead that offsets the benefits provided by quantization. This raises a natural question:

_Can we effectively process the weights at the preprocessing stage to facilitate quantization without introducing inference overhead?_

To address this problem, we propose a simple optimization-based technique called Weight Magnitude Reduction (MagR). MagR functions as a non-linear transformation on weights without altering the original features/activations. The optimization program is designed to find new weights with minimal maximum magnitude, i.e., the \(_{}\) norm, while preserving the layer's outputs.

**Contributions.** We propose a non-linear approach, MagR, based on channel-wise \(_{}\)-regularized least squares, to reduce the quantization scale without compromising the performance of pre-trained model, facilitating subsequent weight quantization while requiring no post-processing or inference overhead. See Figure 1 for comparing weight magnitudes before and after applying MagR. To address the \(_{}\)-regularization problem, we develop an efficient and parallelizable proximal gradient descent algorithm that involves computing \(_{1}\)-ball projections at each iteration. Specifically, MagR preprocessing on a single Nvidia A100 GPU takes merely 15 min for LLAMA2-7B and 3.5 hr for the 70B model. Our results on INT weight-quantization demonstrate that MagR can significantly boost the performance in the sub-4bit regime when combined with fast gradient-free methods for layer-wise PTQ, such as rounding-to-nearest (RTN)  and OPTQ . This approach achieves performance for weight quantization at least comparable to state-of-the-art PTQ methods on natural language processing (NLP) tasks, including gradient-based methods using block-wise reconstruction.

## 2 Related Work

Recently, as the sizes of language models are exploding, there has been growing interest in developing post-training quantization (PTQ) methods [8; 16; 25; 26; 36; 44; 45] for large-scale AI models like large language models (LLMs) to reduce the model sizes and accelerate inference by representing weight matrices in low precision. PTQ methods directly find the low-precision representation of the model without re-training, thereby preferred by extreme large-scale AI models. The OPTQ  uses approximate second-order information to calibrate the quantization. The method successfully compresses LLMs into 3 or 4 bits and can achieve reasonable accuracy in 2 bits. Researchers have found that the extreme values and the distribution of the weight entries highly affect the quantization errors and the quantized model quality. The original weight can be converted into a more quantization-friendly one by linear transformations. The approach can significantly reduce the quantization errors while bringing more time overhead during inference because of the linear transformation. OmniQuant  proposes learnable weight clippings and equivalent transformations to avoid the influence of extreme values. AWQ  searches for the most significant entries in the weight by looking at the activation and selects the scales that protect these entries. SmoothQuant  passes the difficulty in activation quantization to weights by an equivalent linear transformation. QuIP , AffineQuant  and FrameQuant  apply a linear transformation before quantization to make the transformed weight quantization-friendly. These approaches achieve high performance for extreme bits, like 2 bits, but introduce additional inference overhead though the transformation is carefully designed to be efficient. OmniQuant  and AffineQuant  can be adopted for weight-activation quantization by considering the activations in the proposed methods. The work  introduces a low-rank compensation method on top of other quantization methods, which employs low-rank matrices to reduce quantization errors with a minimal increase in model size. By modeling the quantization residual as an \(_{}\)-bounded perturbation,  proposes applying an \(_{1}\) penalty on the gradient of loss to enhance quantization robustness.

The works most closely related to ours are  and , both utilizing \(_{}\) norm to regularize or constrain the weight range to a smaller scale. The Range Regularization (R\({}^{2}\)) method  applies an \(_{}\) penalty or its variants to the conventional network loss to regularize the weight range during end-to-end model pre-training, optimized via SGD. However, this approach becomes practically infeasible for large-scale models. In , a layer-wise pre-processing technique is proposed, which involves solving an intractable \(_{0}\)-minimization problem while constraining the \(_{}\)-norm of weights.

## 3 Background

First, we clarify the mathematical notations that will be used throughout this paper:

**Notations.** We denote vectors by bold small letters and matrices by bold capital ones. For a positive integer \(n\), \([n]:=\{1,2,,n\}\) denotes the set containing all positive integers up to \(n\). For any two vectors \(,^{n}\), \(,:=_{i=1}^{n}x_{i}y_{i}\) is the inner product. We denote by \(\|\|:=,}=^{n}x_{i}^{2}}\) the Euclidean norm; \(\|\|_{1}:=_{i=1}^{n}|x_{i}|\) is the \(_{1}\)-norm; \(\|\|_{}:=_{1 i n}|x_{i}|\) is the \(_{}\)-norm. For any matrix \(^{m n}\), \(^{}^{n m}\) is the transpose. We denote the spectrum norm of \(\) by \(\|\|=_{}()\), which equals its maximum singular value. Its Frobenius norm is given by \(\|\|_{ F}=^{m}_{j=1}^{n}X_{i,j}^{2}}\). Moreover, for vectors \(\) and \(\), \(:=(x_{1}y_{1},,x_{n}y_{n})^{n}\) denotes the Hadamard or element-wise product, and likewise for two matrices.

**Layerwise PTQ.** Post-training quantization via layerwise reconstruction calls for solving a least squares problem with a discrete constraint. For the pre-trained weights \(\) within a linear layer, we aim to find the quantized weights \(_{q}\) that minimize the following function

\[_{_{q}}\ \|_{q}-\|_{ F}^{2},\] (1)

where \(^{(b l) m}\) is the feature matrix associated with a batch of calibration data consisting of \(b\) samples stacked together, and each data sample is represented by an \(l m\) sub-matrix. \(^{m n}\) is an appropriate set of all feasible quantized weights.

Figure 1: **Motivation behind MagR**: we can effectively reduce the magnitude of weights at the preprocessing stage. Each point denotes the maximum magnitude before (\(x\)-coordinate) and after (\(y\)-coordinate) applying MagR within a sampled channel (or column) of the weight matrix from three random layers of LLaMa2-7B . These column-wise maximum magnitudes are typically more than halved through MagR.

The most straightforward PTQ technique, known as RTN, involves directly rounding the weight matrix \(\) without utilizing any additional data. An improvement over RTN was introduced by AWQ , which enhances the quantization process by incorporating channel-wise scaling on \(\).

Thanks to the simplicity of the layer-wise formulation (1), several efficient gradient-free algorithms [4; 16; 51; 53] have been recently proposed to address layer-wise quantization, including OPTQ. Built on top of OPTQ, QuIP subjects \(\) and \(\) to random orthogonal transformations to produce "incoherent" weight and Hessian matrices, leading to superior accuracy with sub-4bit quantization. However, this advantage comes with a trade-off; during inference, QuIP requires random orthogonal transformations on the feature inputs of linear layers, rendering noticeably slower throughput compared to OPTQ.

**Uniform Quantizer.** Given a set of points \(^{m}\), the commonly-used (asymmetric) uniform quantizer  defines the quantization step \(=)-()}{2^{b}-1}\) and zero-point \(z=)}{}\), and it quantizes \(\) onto the scaled integer grids \(=\{z,(z+1),,(z+(2^{b}-1)) \}^{m}\) as follows:

\[_{q}=((}{ }-z,0,2^{b}-1)+z).\]

In per-channel (or per-group) PTQ, the quantization step \(\) is conventionally calculated based on the channel-wise (or group-wise, respectively) minimum and maximum values of the pre-trained weights \(\), as defined above, and remains constant throughout the quantization procedure.

## 4 The Proposed Method

In this section, we present the Weight Magnitude Reduction (MagR) method based on \(_{}\)-norm regularization, which is applied just before the quantization step within each linear layer. The intuition behind MagR is based on the following simple estimate of the layer-wise quantization error. Given the feature/activation matrix \(\), the quantizer \(\), and any pre-trained weights \(^{m}\), we have:

\[_{_{q}}_{q}- (()-) \|()-\|( )}{2},\]

where \(=)-()}{2^{b}-1}\) is the quantization step size. This shows that reducing the range of weights helps to suppress the quantization error. With this in mind, MagR preprocessing is designed to achieve two key effects:

* First, it effectively reduces the channel-wise (or column-wise) maximum magnitude of the weights, as illustrated by Figure 1.
* Second, it preserves the model's original performance with minimal accuracy loss after preprocessing. Table 1 demonstrates that MagR preprocessing maintains the perplexity of the pre-trained models, with only minor degradation.

### Approximately Rank-Deficient Feature Matrix

To illustrate the idea behind the proposed MagR method, let us consider a pre-trained weight vector \(}^{m}\) of a linear layer and the associated feature input matrix \(\). MagR leverages the fact that the feature matrix \(\) across all layers of LLMs is approximately rank-deficient. Specifically, if \(\) is exactly rank-deficient, the linear system modeling the layer's output, \(=}\) with variables \(\), generally has infinitely many solutions. That is, for any \(\) in the non-trivial kernel space of \(\), we have that \(=}+\) preserves the layer's output. Among all solutions, MagR aims to identify the weight vector \(\) with the smallest extreme value in magnitude.

  Model & Method & Wikitext2 (PPL\(\)) & C4 (PPL\(\)) \\  LLaMA2-7B & Original & 5.47 & 6.97 \\  & MagR & 5.52 & 7.04 \\  LLaMA2-13B & Original & 4.88 & 6.46 \\  & MagR & 4.92 & 6.52 \\  LLaMA2-70B & Original & 3.31 & 5.52 \\  & MagR & 3.35 & 5.56 \\  

Table 1: **A comparison of perplexity (PPL) for the original pre-trained and the MagR-processed LLaMA2 models.**In , the authors empirically observed that the Hessian matrix \(^{}\) is approximately low-rank across all layers in open pre-trained (OPT) models . Here we examined the feature matrix of LLaMA models [37; 38]. Our approximate fraction rank of the feature matrix \(\) is defined as the fraction of singular values of \(\) such that \(()>0.01_{}()\). Table 2 illustrates that all feature matrices extracted from LLaMA models are indeed rank-deficient according to this definition.

### MagR via \(_{}\)-Regularization

Let us consider the quantization of a weight vector for simplicity. Given pre-trained weight vector \(}\), we would like to find a new set of weights \(\) with the smallest maximum magnitude, such that the layer output is preserved up to a small error \(>0\), i.e.,

\[_{^{m}}\|\|_{}\| -}\|.\]

To efficiently implement MagR, we consider the following mathematically equivalent \(_{}\)-regularization problem instead:

\[_{^{m}}\|-}\|^ {2}+\|\|_{}\] (2)

where \(>0\) serves as the regularization parameter, balancing fidelity against the \(_{}\) regularizer. To maintain the output of the layer, \(\) should typically be set to a small value. Indeed, let \(^{*}\) be the minimizer of (2), we have that the \(_{2}\) error of the layer's output introduced by MagR is \(O()\):

\[\|^{*}-}\| ^{*}-}\|^{2}+2\| ^{*}\|_{}}\] \[}-}\|^{2}+2\| }\|_{}}=}\|_{}},\]

where \(\|}\|_{}\) is a constant independent of \(\), and the second inequality uses that \(^{*}\) is the minimizer.

**Proximal Gradient Descent.** Note that \(_{}\)-norm is a convex but non-differentiable function. In theory, the optimization problem (2) can be simply solved by a subgradient algorithm, but it is significantly slower than the more sophisticated proximal gradient algorithm which matches the convergence rate of standard gradient descent.

With the step size \(>0\), proximal gradient descent  takes the following iteration:

\[^{k+1} =_{\|\|_{}}(^{k}- \,_{}\|-}\|^{2}| _{=^{k}})\] \[=_{\|\|_{}}(^{k}- ^{}(^{k}-}))\] (3)

where \(_{t\|\|_{}}\) with the scalar \(t>0\) is the (scaled) proximal operator of \(_{}\)-norm function, defined as

\[_{t\|\|_{}}():=_{^{m }}\|-\|^{2}+t\|\|_{}.\]

To ensure the convergence of (3), it is sufficient to choose the step size

\[(^{})},\]

  Model & Min & Max & Mean & 25\% Percentile & 75\% Percentile \\  LLaMA1-7B & 0.2 & 99.07 & 70.41 & 65.09 & 81.80 \\ LLaMA1-13B & 1.42 & 99.90 & 83.85 & 75.07 & 96.71 \\ LLaMA1-30B & 0.73 & 99.85 & 84.40 & 79.76 & 99.46 \\ LLaMA1-65B & 1.17 & 99.90 & 83.11 & 82.76 & 98.71 \\  LLaMA2-7B & 0.1 & 99.95 & 76.83 & 67.71 & 91.02 \\ LLaMA2-13B & 0.44 & 99.76 & 78.30 & 66.54 & 98.58 \\ LLaMA2-70B & 0.1 & 99.71 & 81.55 & 74.90 & 99.56 \\  

Table 2: **The statistics of (approximate) fraction ranks in percentage (%)** of feature matrix \(\) across all layers of LLaMA models. All feature matrices are approximately rank-deficient with a fraction rank less than 100%. Some of them are highly low-rank with a fraction rank \( 1\%\).

where \(_{}(^{})\) is the maximum eigenvalue of \(^{}\).

**Proximal Operator of \(_{}\)-Norm.** It remains to determine the proximal operator of \(_{}\)-norm. It turns out we can compute it by leveraging the celebrated Moreau decomposition [29; 32]: for any \(t>0\),

\[_{t\|\|_{}}()=-t _{\|\|_{1} 1}(}{t}).\] (4)

That is, computing the proximal operator of \(_{}\) norm amounts to evaluating the projection onto \(_{1}\) ball, which is defined as

\[_{\|\|_{1} 1}():=_{ ^{m}}\|-\|^{2}\|\|_{1}  1.\]

Fortunately, computing projection onto the \(_{1}\) ball is an established task, and there are several efficient algorithms available. For example, see  and the references therein. Here we adopted a simple algorithm of \(O(m m)\) time complexity as in , which supports parallelizable or vectorized implementation for the projections of a batch of weight vectors, i.e., a weight matrix, as will be described in the next subsection. The implementation mainly involves sorting and soft-thresholding ; see Algorithm 3 and its derivation in Appendix A.1 for the details.

**MagR for Weight Matrix.** In practical implementation of MagR, we preprocess the entire weight matrix \(=[_{1},,_{n}]^{m n}\) within each linear layer. For per-channel quantization (or per-column quantization in our setting), the \(_{}\) penalty is imposed column-wise on the weight matrix to reduce the quantization scale of each channel. That is, MagR amounts to solving

\[_{^{m n}}\|- }\|_{}^{2}+_{j=1}^{n}\|_{j}\|_{}\]

In this case, we take the following iteration:

\[^{k+1}=_{\|\|_{}} (^{k}-^{}(^{k}-})),\]

where the proximal operator \(_{t\|\|_{}}\) and the corresponding projection \(_{\|\|_{1} 1}\) in (4) are applied _column-wise_ to the matrix input. Hereby we summarize MagR for processing one linear layer in Algorithm 1 with the column-wise \(_{1}\)-ball projection as detailed in Algorithm 2, which generalizes Algorithm 3 in Appendix A.1, by handling matrix inputs (or batches of vectors).

**Input:** Pre-trained weight matrix \(}^{m n}\); Hessian matrix \(=^{}^{m m}\); max iteration number \(K\); step size \(=()}\); penalty parameter \(>0\).

**Output:** Preprocessed weights \(^{m n}\).

```
1:Initialize \(^{0}=}\).
2:for\(k=0,,K-1\)do
3:\(^{k}=^{k}-(^{k}-})\) gradient descent step
4:\(^{k+1}=^{k}-_{\|\|_{1}  1}(^{k}}{})\)\(_{\|\|_{1} 1}\) is described in Alg. 2
5:endfor
6:return\(=^{K}\) ```

**Algorithm 1** Per-channel MagR for one linear layer.

**Extension to Per-Group Quantization.** By using more float scaling factors, per-group quantization becomes a preferred strategy for mitigating accuracy loss at extremely low bit-widths. In this approach, a weight vector \(^{m}\) is segmented into groups of weights, each containing \(d\) elements, with all weights within a group sharing a common scaling factor for quantization. Here, per-group MagR applies an \(_{}\) penalty to each vector of grouped weights. Consequently, the \(_{1}\)-ball projection is independently performed on these vectors, while maintaining the gradient descent step unchanged. We note that the group-wise \(_{1}\)-ball projection can be easily done using Algorithm 2, with an additional reshaping of the input \(^{m n}\) into \(^{d( n)}\).

```
0: Matrix \(^{m n}\); the radius of \(_{1}\) ball, \(=1\).
0:\(^{m n}\) such that all columns \(\|_{j}\|_{1}\), \(\,j[n]\).
1: Create a binary mask \(^{m n}\) filtering out the columns of \(\) with \(\|_{j}\|_{1}\).
2: Sort \(||\) column-wise in descending order into \(\).
3: Find index \(_{j}=\{i[m]:u_{i,j}>(_{r=1}^{i}u_{r,j}- )\}\), \(\,j[n]\)
4: Define \(_{j}=}(_{r=1}^{_{j}}u_{r,j}-)\), \(\,j[n]\)
5: Tile \(^{n}\) into \(^{m n}\) along the row.
6: Compute \(=(1-)+()\{||-,0\}\)
7:return\(\) ```

**Algorithm 2** Column-wise projection onto the unit \(_{1}\)-ball.

## 5 Experiments

**Overview.** We tested the proposed MagR for INT4, INT3, and INT2 weight quantization. In our notations, the weight and activation bits are denoted by 'W' and 'A', respectively. Additionally, we implemented group-wise weight quantization with the group size denoted by 'g'. For example, W2A16g128 signifies INT2 weight and FP16 activation (i.e., INT2 weight-only quantization) with a group size of 128.

We employed our MagR processing approach on top of the two gradient-free PTQ methods in main text, RTN and OPTQ , to quantize the LLAMA1 (7B-65B)  and LLAMA2 (7B-70B)  model families. In the Appendix A.2, we extend MagR with QuIP  (MagR+QuIP) to quantize LLAMA2 (7B-70B) model families. By applying MagR on top of RTN (MagR+RTN), we achieved better results than AWQ  for per-channel INT3 and INT4 weight quantization. Additionally, MagR combined with OPTQ (MagR+OPTQ) achieved state-of-the-art performance for INT3 and INT4 quantization. To enhance the per-channel INT2 quantization, we ran 30 additional iterations of coordinate descent algorithm  on top of OPTQ, which we denote by MagR+OPTQ\({}^{}\). It turns out MagR+OPTQ\({}^{}\) is superior to both Omniquant  and QuIP  in terms of perplexity (Table 9), and falls just short of QuIP in zero-shot tasks for 13B and 70B models (Table 4). Note that QuIP uses random orthogonal transformations (so-called Incoherence Processing) to process both the weights and features, resulting in \(1.5\) slower throughput than OPTQ. In contrast, MagR-based method does not introduce any overhead whatsoever compared with OPTQ.

In conclusion, our MagR-based PTQ method is intuitive yet effective in compressing models into extreme bit-widths, while maintaining performance without introducing any inference overhead.

**Datasets and Evaluation.** Following the previous work , we evaluate the quantized model on language generation tasks on WikiText2  and C4 . Additionally, we test its performance on zero-shot tasks, including PIQA , ARC (Easy and Challenge) , and Winogrande . For the language generation experiments, our implement is based on the OPTQ's  repository, which is built using PyTorch. For executing all zero-shot tasks, we adhere to the Im-eval-harness .

**Baseline**: For the language generation task, we compare our method with RTN, OPTQ , AWQ  and OmniQuant  on LLaMA1 and LLaMA2 models. In addition to the aforementioned methods, we also conduct a comparison with QuIP  on the LLaMA2-70B model. In the zero-shot task, we focus on four individual tasks and compare the average accuracy across all four tasks with Omniquant .

**Implementation details.** We utilized the HuggingFace implementations of the LLaMA1 and LLaMA2 models and perform quantization on a single NVIDIA A100 GPU with 80GB of memory. Following the OPTQ method, we load one block consisting of 7 linear layers into GPU memory at a time. In line with previous work , the input matrix \(\) is obtained by propagating the calibration data through the quantized layers.

**The choice of parameters.** To ensure that the MagR-processed layer output \(\) is faithful to the original \(}\), we need to use a tiny penalty parameter \(\) in (2). For per-channel quantization, \(\) was fixed to be \(10^{-3}\) in our experiments, but we did find that setting it to a smaller value of \(5 10^{-4}\) or \(10^{-4}\) can sometimes slightly improve the perplexity (with a relative change of \(<1\%\) in ppl). Similarly for per-group quantization, we set \(\) to \(10^{-4}\), while reducing it to \(5 10^{-5}\) or \(10^{-5}\) could sometimes also slightly improve the perplexity. An ablation study on \(\) is provided in the Appendix A.2.

Furthermore, we used a multiplicative scalar \(<1\) to decay the standard quantization step \(=)-()}{2^{b}-1}\) (or equivalently, the quantization scale) of the quantizer. In other words, our \(=)-()}{2^{b}-1}\). It has been shown in existing works [21; 34] that, optimal quantization step for binary or ternary quantization yielding the minimum quantization error is not given by \()-()}{2^{b}-1}\). Shrinking \(\) at low bit-width results in a more clustered quantization grid lattice that fits the weights better, which leads to a smaller overall error. In general, \(\) is positively correlated with the bit-width used. For per-channel quantization, the best \([0.8,0.85]\) on INT2 quantization, whereas the empirically optimal \(\) is around \(0.9\) for INT3 quantization. As for INT4, \(\) is simply set to 1, that is, we used the standard quantization step. In addition, for per-group quantization, we chose \(=0.95\) for both INT2 and INT3 quantization. The ablation study of \(\) is in the Appendix A.2. We observed that this refinement on the quantization step \(\) significantly improves the performance of the PTQ method. In addition, the iteration number \(K\) in Algorithm 1 was set to \(150\) across all the experiments.

### Language Generation

We concentrate our analysis on perplexity-based tasks. The results for the LLaMA2 family with context length of 2048, are elaborated in Table 9, while those for LLaMA1 are provided in Appendix Table 6. As evidenced by the tables, the MagR preprocessing consistently improve the performance of the baselines RTN and OPTQ. Moreover, MagR+OPTQ consistently outperforms most baseline across the LLaMA family models for both per-channel and per-group weight quantization. Particularly, for

   &  &  \\   &  &  &  &  &  &  \\  FP16 & Baseline & 5.47 & 4.88 & 3.31 & 6.97 & 6.46 & 5.52 \\   & OPTQ & 7.7e3 & 2.1e3 & 77.95 & NAN & 323.12 & 48.82 \\  & OmniQuant & 37.37 & 17.21 & 7.81 & 90.64 & 26.76 & 12.28 \\  & QuIP & 27.13 & **10.09** & 6.33 & 31.33 & **13.13** & 8.94 \\   & MagR+OPTQ\({}^{}\) & **16.73** & 11.14 & **5.95** & **23.73** & 14.45 & **8.53** \\   & OPTQ & 36.77 & 28.14 & - & 33.70 & 20.97 & - \\  & OmniQuant & 11.06 & 8.26 & 6.55 & 15.02 & 11.05 & 8.52 \\   & MagR+OPTQ & **9.94** & **7.63** & **5.52** & **14.08** & **10.57** & **8.05** \\   & RTN & 539.48 & 10.68 & 7.52 & 402.35 & 12.51 & 10.02 \\  & OPTQ & 8.37 & 6.44 & 4.82 & 9.81 & 8.02 & 6.57 \\   & AWQ & 24.00 & 10.45 & - & 23.85 & 13.07 & - \\   & OmniQuant & 6.58 & 5.58 & 3.92 & 8.65 & 7.44 & 6.06 \\   & QuIP & 6.50 & **5.34** & 3.85 & 8.74 & 7.34 & 6.14 \\    & MagR+RTN & 8.66 & 6.55 & 4.64 & 10.78 & 8.26 & 6.77 \\   & MagR+OPTQ & **6.41** & 5.41 & **3.82** & **8.23** & **7.19** & **6.03** \\   & RTN & 6.66 & 5.51 & 3.97 & 8.40 & 7.18 & 6.02 \\   & OPTQ & 6.29 & 5.42 & 3.85 & 7.89 & 7.00 & 5.85 \\   & AWQ & 6.24 & 5.32 & - & 7.84 & 6.94 & - \\   & OmniQuant & 6.03 & 5.28 & 3.78 & **7.75** & 6.98 & 5.85 \\    & MagR+RTN & 6.46 & 5.45 & 3.95 & 8.22 & 7.12 & 6.00 \\   & MagR+OPTQ & **6.00** & **5.23** & **3.71** & 7.77 & **6.93** & **5.84** \\   & RTN & 6.11 & 5.20 & 3.67 & 7.71 & 6.83 & 5.79 \\   & OPTQ & 5.83 & 5.13 & 3.58 & 7.37 & 6.70 & 5.67 \\   & AWQ & 6.15 & 5.12 & - & 7.68 & 6.74 & - \\   & OmniQuant & 5.74 & 5.02 & 3.47 & 7.35 & 6.65 & 5.65 \\   & QuIP & 5.94 & 5.01 & 3.53 & 8.01 & 6.88 & 5.87 \\    & MagR+RTN & 5.91 & 5.17 & 3.58 & 7.52 & 6.81 & 5.72 \\   & MagR+OPTQ & **5.70** & **4.97** & **3.44** & **7.28** & **6.63** & **5.63** \\  

Table 3: **Perplexity of quantized LLaMA2 models on Wikitext2 and C4**. We report WikiText2 and C4 perplexity in this table. LLaMA1 resutls can be found in the Appendix.**INT2, MagR+OPTQ\({}^{}\) performs 30 additional coordinate descent (CD) iterations [4; 51] on top of OPTQ to refine the solution, surpassing all baselines.

Furthermore, MagR+RTN achieves performance comparable to OPTQ. Notably, it outperforms AWQ by a significant margin in INT3 quantization, implying that MagR proves more effective as a preprocessing method compared to channel-wise scaling.

### Zero-Shot Tasks

We evaluated the performance of quantized models on several zero-shot tasks. The results are reported in Table 4. Similar to previous observations, the proposed MagR demonstrates superior performance on most models compared to OmniQuant, with a small gap compared to QuIP . Nonetheless, it is reasonable and commendable that our algorithm achieves results close to QuIP without introducing any inference overhead. It is possible to further improve our approach based on the insight behind QuIP  -- i.e., quantization benefits from incoherent weight and Hessian matrices; see Table 9 for the results in the appendix.

### Preprocessing and Quantization Runtime

We report the execution time of MagR+RTN and MagR+OPTQ on a single NVIDIA A100 GPU in Table 5. For example, it typically took 0.5-7.5 hours for MagR+OPTQ to quantize the LlaMA2 models. We note that the integration of MagR can markedly enhance the performance of the standard OPTQ . It is noted that MagR+OPTQ\({}^{}\) for INT2 weight quantization requires a longer runtime

 
**LLaMA2 / Acc\({}\)** & WBits & Method & ARC-C & ARC-E & PIQA & Winogrande & **Avg.** \\   & FP16 & - & 40.0 & 69.3 & 78.5 & 67.3 & 63.8 \\   & 4 & OmniQuant & 37.9 & 67.8 & 77.1 & **67.0** & 62.5 \\  & 4 & MagR+OPTQ & **39.3** & **68.4** & **78** & 66.5 & **63.1** \\   & 3 & OmniQuant & **35.3** & **62.6** & 73.6 & **63.6** & **58.8** \\  & 3 & MagR+OPTQ & 34.6 & 62 & **74.7** & 63 & 58.6 \\   & 2 & OmniQuant & 21.6 & 35.2 & 57.5 & **51.5** & 41.5 \\  & 2 & QuIP & 19.4 & 26.0 & 54.6 & 51.8 & 37.5 \\  & 2 & MagR+OPTQ\({}^{}\) & **22.0** & **36.7** & **59.8** & 51.1 & **42.4** \\   & FP16 & - & 45.6 & 73.3 & 79.1 & 69.6 & 66.9 \\   & 4 & OmniQuant & 43.1 & 70.2 & 78.4 & 67.8 & 64.9 \\   & 4 & QuIP & **44.9** & **73.3** & **79** & **69.7** & **66.7** \\  & 4 & MagR+OPTQ & 44.2 & 72.0 & 78.0 & 68.6 & 65.7 \\   & 3 & OmniQuant & 42.0 & 69.0 & 77.7 & 65.9 & 63.7 \\   & 3 & QuIP & 41.5 & **70.4** & 76.9 & **69.9** & **64.7** \\  & 3 & MagR+OPTQ & **42.2** & 69.0 & **77.7** & 66.5 & 63.9 \\   & 2 & OmniQuant & 23.0 & 44.4 & **62.6** & 52.6 & 45.7 \\  & 2 & QuIP & **23.5** & **45.2** & 62.0 & **52.8** & **45.9** \\  & 2 & MagR+OPTQ\({}^{}\) & 23.2 & 44.3 & 62.4 & 52.1 & 45.5 \\   & FP16 & - & 51.1 & 77.7 & 81.1 & 77.0 & 71.7 \\   & 4 & OmniQuant & 49.8 & **77.9** & 80.7 & 75.8 & 71.1 \\   & 4 & QuIP & 47.0 & 74.3 & 80.3 & 76.0 & 69.4 \\   & 4 & MagR+OPTQ & **50.1** & 77.5 & **80.8** & **76.0** & **71.1** \\    & 3 & OmniQuant & 47.6 & 75.7 & 79.7 & 73.5 & 69.1 \\   & 3 & QuIP & 46.3 & 73.2 & **80.0** & 74.6 & 68.5 \\   & 3 & MagR+OPTQ & **47.7** & **76.6** & 79.4 & **75.4** & **69.8** \\    & 2 & OmniQuant & 28.7 & 55.4 & 68.8 & 53.2 & 51.5 \\   & 2 & QuIP & 34.0 & **62.2** & **74.8** & **67.5** & **59.6** \\   & 2 & MagR+OPTQ\({}^{}\) & **35.9** & 61.3 & 74.7 & 64.8 & 59.2 \\  

Table 4: **Multi-task results of quantized LLaMA2 models.** This table reports the accuracy of 4 zero-shot tasks. Perplexity results can be found in the Appendix.

due to the additional CD iterations, extending the quantization process for LLaMA2-70B to 31 hr. It also reveals that the preprocessing overhead for quantizing the LLaMA2 models (7B-70B) amounts to approximately 15 min, 30 min, and 3.5 hr, respectively. In comparison, our total runtime is roughly half of that of the gradient-based method, OmniQuant , while achieving at least comparable results. Moreover, MagR introduces no post-processing step or overhead during inference.

## 6 Concluding Remarks

In this paper, we proposed MagR, based on \(_{}\)-regularization, to significantly reduce the maximum weight magnitude of pre-trained LLMs within each layer while preserving their output. MagR is designed to enhance the accuracy of backpropagation-free PTQ methods that use layer-wise reconstruction, such as RTN and OPTQ. MagR produces a more clustered distribution of weights and leads to a smaller quantization step, thereby facilitating the subsequent PTQ task. To solve the \(_{}\)-regularization problem, we used the classical proximal gradient descent algorithm with \(_{1}\)-ball projections, tailored to handle matrix variables efficiently. Our experiments on LLaMA family validated the effectiveness of the MagR approach, achieving the state-of-the-art performance on NLP tasks. Remarkably, unlike existing weight preprocessing techniques that require performing an inverse transformation on features during inference, MagR eliminates the need for post-processing and incurs no overhead. This renders MagR more practical for the deployment of quantized models.