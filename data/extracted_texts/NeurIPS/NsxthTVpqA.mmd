# Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment

Xin Xiao\({}^{1,2}\)+, Bohong Wu\({}^{2}\)+, Jiacong Wang\({}^{2,3}\)+, Chunyuan Li\({}^{2}\), Xun Zhou\({}^{2}\), Haoyuan Guo\({}^{2}\)

\({}^{1}\)School of Computer Science, Wuhan University \({}^{2}\)ByteDance Inc.

\({}^{3}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

https://github.com/foundation-multimodal-models/CAL

Equal contributionWork done during internship in ByteDance

###### Abstract

Existing image-text modality alignment in Vision Language Models (VLMs) treats each text token equally in an autoregressive manner. Despite being simple and effective, this method results in sub-optimal cross-modal alignment by over-emphasizing the text tokens that are less correlated with or even contradictory with the input images. In this paper, we advocate for assigning distinct contributions for each text token based on its visual correlation. Specifically, we present by contrasting image inputs, the difference in prediction logits on each text token provides strong guidance of visual correlation. We therefore introduce **C**ontrastive **AL**ignment (_CAL_), a simple yet effective re-weighting strategy that prioritizes visually correlated tokens. Our experimental results demonstrate that _CAL_ consistently improves different types of VLMs across different resolutions and model sizes on various benchmarks. Importantly, our method incurs minimal additional computational overhead, rendering it highly efficient compared to alternative data scaling strategies.

## 1 Introduction

Recent advancements in Large Language Models (LLMs)  have opened up new avenues in multimodal understanding, giving rise to a novel model category known as Vision Language Models (VLMs) . Many recent studies on VLMs are centered around enhancing their capabilities, either through increasing the resolution of input images  or incorporating higher-quality training datasets . Additionally, research efforts have been directed towards exploring variations of vision models, such as replacing or augmenting vision encoders beyond CLIP , including approaches like SigLIP , DINO  or ConvNeXt . The integration of these techniques has spurred the development of VLMs, continually enhancing their performance across various benchmarks including visual question answering , image captioning , and visual grounding .

Despite these advancements, whether the current alignment strategy on existing image-text datasets performs satisfactorily is often less studied. Existing alignment strategies simply treat all text tokens equally in an auto-regressive manner. Although such a method has been proven to be simple and effective, many text tokens exhibit limited relevance to the visual inputs, which contribute little to image-text modality alignment. Figure 0(a) presents a sample drawn from the ShareGPT4V  dataset, where a large proportion of text tokens including _unique, context_ presents little visual correlation. Treating these text tokens in equal weights results in ineffective training and can introduce negative effects by prioritizing more on fitting the distribution of these visually irrelevant tokens, rather than the image-text modality alignment.

Moreover, there also exists a proportion of tokens that contradicts visual conditions, which is inevitable in model generated datasets , presented in Figure 0(a). In particular, we conduct human evaluations on the broadly used GPT-assisted datasets including ShareGPT4V and the detail caption subset of LLaVA-Instruct in Figure 0(b) by sampling 100 samples from each dataset. We score each sample by 0 and 1 based on whether there exist text tokens that contradict with visual input, and the score is averaged by three annotators. We found approximately half of the sampled datasets contain visually contradictory tokens in both datasets. Imitating the text distribution on these contradictory tokens further harms the image-text modality alignment. Consequently, recent evaluations on existing VLMs  present the shortcomings of current alignment strategy from various aspects, including hallucination  and responding without depending on visual conditions .

Fortunately, inspired by recent training-free visual contrastive decoding researches , we present that the visual correlation can be directly indicated by contrasting input image conditions. In particular, we investigate the change in prediction logits of text tokens with or without the image input and observe strong relevance between the logit change of each text token and its visual correlation. We therefore propose **C**ontrastive **AL**ignment (_CAL_), which is a surprisingly simple re-weighting strategy to prioritize the training of text tokens that are highly correlated with the input image to enhance image-text modality alignment. Experiments have shown that our proposed method can improve leading VLMs of different kinds including LLaVA-1.5/LLaVA-NeXT , MiniGemini(MGM)/MGM-HD , across different resolution and model size on various types of benchmarks including visual question answering, captioning and grounding. Especially, _CAL_ on LLaVA-Next-13B  can bring an impressive performance of 1.7 ANLS on VQA\({}^{}\), 3.4 relaxed accuracy on VQA\({}^{}\), 2.2 CIDEr  on COCO  and 6.3 CIDEr on TextCaps , 0.6/0.7 IoU on validation/test set of RefCOCOg . Moreover, our method introduces little computational overhead, with one auxiliary gradient-free forward operation in each training step. The lightweight feature while the impressive performance of _CAL_ brings current VLMs to a new stage, highlighting the importance of a delicate image-text modality alignment strategy design. We further conduct extensive qualitative analysis for _CAL_ and present the improved ability of OCR recognition and image-captioning.

In summary, our contributions are listed as follows.

* We present that by contrasting image inputs, existing VLMs are able to distinguish visually correlated tokens both visually irrelevant and visually contradictory ones.
* We propose _CAL_, a contrastive image-text alignment method via token re-weighting, which is lightweight and effective. _CAL_ requires little additional training cost and no additional inference cost.
* Experiments show that our _CAL_ can consistently improve VLMs of different kinds, across different resolutions and sizes in various types of benchmarks.

Figure 1: Figure 0(a) is one sample drawn from the ShareGPT4V dataset, which contains text tokens that are even contradictory with the given image. Figure 0(b) further presents our human evaluation results on the proportion of noisy samples that contain contradictory tokens.

## 2 Contrastive Alignment

In this section, we describe the detailed design of _CAL_. First of all, we review the existing image-text modality alignment method and provide the notations in Section 2.1. Secondly, we show the token discrepancy in cross-modal datasets can be inferred via contrasting image inputs in Section 2.2. Finally, we present a detailed description of our proposed _CAL_ in Section 2.3.

### Preliminary and Notations

PreliminaryMost existing VLMs adopt a two-stage strategy to align pre-trained image features with text embeddings in Large Language Models, i.e., a PreTraining (PT) stage that uses quantitative while noisy datasets for rough alignment, and an Instruction-Tuning (IT) stage that uses high-quality datasets to enhance the alignment. Both stages treat all tokens equally in an auto-regressive generation manner, i.e., the Maximum Likelihood Estimation (MLE) objective.

NotationsIn this paper, we denote the alignment dataset \(\) consisting paired image-text samples \(=\{(I^{1},T^{1}),(I^{2},T^{2}),...,(I^{n},T^{n})\}\), and denote the logit computation function as \(f()\) where \(\) is the weight of VLMs. For the \(i^{th}\) sample \((I^{i},T^{i})\) in the training dataset, where \(T^{i}\) consists of a sequence of \(l\) tokens \(T^{i}=[t^{i,1},t^{i,2},...,t^{i,l}]\), we denote the prediction logit distribution without input \(I\) as \(\), and prediction logit distribution with input \(I\) as \(}\), depicted in the following equation:

\[^{i,j}=f_{}(T^{i,<j}),}^{i,j}=f_{}(I^{ i},T^{i,<j})\] (1)

where \(T^{i,<j}\) represents all previous tokens before position \(j\) in the \(i^{th}\) sample. We further use \(^{i,j}_{[t_{j}]}\) or \(}^{i,j}_{[t_{j}]}\) to represent the prediction logit in the \(i^{th}\) sample at token \(t_{j}\). As a result, the MLE loss objective for the given \(i^{th}\) sample at token \(t_{j}\) is written in the following equation by treating the weight \(c\) of each token equally, where \(c\) is set to \(1\):

\[^{i,t_{j}}_{}=c_{}(}^{i,j}_{[t_{j}]})\] (2)

### Tokens Differ in Image-text Modality alignment

In this section, we present the necessity of token re-weighting in Section 2.2.1, and show that the re-weighting guidance could be naturally inferred by contrasting image inputs in Section 2.2.2.

#### 2.2.1 Discrepancy exists in text tokens

Our proposed method begins with the discrepancy in the training label tokens. For image-text modality alignment, the training labels are usually natural texts, where not all text tokens have a

Figure 2: Overview of _CAL_. Figure 1(a) presents a sample drawn from the ShareGPT4V dataset. We calculate the logit difference w/ or w/o image inputs and plot the heat map on partial text tokens. Figure 1(b) presents the training procedure of _CAL_, which re-weights the importance of label tokens based on the contrasting logits.

strong correlation with the image inputs. Moreover, due to the existence of model generated datasets, there also exist noisy text tokens that harm the alignment process, which is depicted in Figure 1.

Based on the relevance between corresponding input images, text tokens can be naturally divided into three kinds. (1) **Visually correlated tokens**, which contain clear visual concepts and movements depicted in the image. (2) **Visually irrelevant tokens**, which contain either irrelevant to the image inputs or could be easily inferred by previous text tokens. (3) **Visually contradictory tokens**, which contain hallucinated objects, especially in the model generated datasets.

#### 2.2.2 Visually correlation can be inferred by contrasting image inputs.

Inspired by VCD  and IBD , which both enhance the generation by contrasting image inputs, we further present that the contrastive method can also provide clear guidance for visually correlation on each token.

We take the prediction logit of each label token under two circumstances, i.e., with or without the image inputs, which we denote as \(}_{[t_{j}]}^{i,j}\) and \(_{[t_{j}]}^{i,j}\). Then denote \(_{[t_{j}]}^{i,j}=}_{[t_{j}]}^{i,j}-_{[t_{j}]}^{i,j}\) as the difference between the predictions logits across two circumstances, we plot \(_{[t_{j}]}^{i,j}\) on each token in Figure 1(a) to visualize the effect of image conditions.

From Figure 1(a), \(_{[t_{j}]}^{i,j}\) performs impressively in distinguishing text tokens of three kinds. By \(_{[t_{j}]}^{i,j}\), the visually correlated tokens _the traffic lights tree, busy street, red truck_ are specially high-lighted while other tokens, especially the visually contradictory tokens _a black car_ are light-colored.

In summary, the discrepancy in the label tokens motivates us to apply the token-wise dynamics on loss to enhance the image-text modality alignment. By contrasting the image inputs, the difference in the prediction logits \(_{[t_{j}]}^{i,j}\) aids us with clear guidance for visually correlation of each text token.

### Contrastive Alignment (_Cal_)

In this section, we present the details of our proposed _CAL_. _CAL_ proposed to re-assign the contribution of each token based on their visually correlation weights. The overview of our method is shown in Figure 1(b) and the detailed algorithm is depicted in Algorithm 1.

```
0:\(i^{th}\) Image \(I^{i}\), \(i^{th}\) Text \(T=t_{1},t_{2},...,t_{n}\), VLM \(f_{}\)
1: Compute contrastive logit. \(^{i,j}=f_{}(T^{i,<j}),}^{i,j}=f_{}(I^ {i},T^{i,<j})\)
2: Compute \( logit\). \(_{[t_{j}]}^{i,j}=}_{[t_{j}]}^{i,j}-_{ [t_{j}]}^{i,j}\)
3: Compute weights by post-processing \(_{[t_{j}]}^{i,j}\). \(}^{i,t_{j}}=pooling_{W}(clamp_{,}(_{[t_{j}]}^{i,j}))\)
4: Compute CAL loss by re-weighting tokens. \(_{}^{i,t_{j}}=-^{1}^{i,t_{k} }}}^{i,t_{j}}_{}f_{}(}_{[t_{j}]}^{i,j})\)
5:\(_{}^{i,t_{j}}\) ```

**Algorithm 1** Detail Procedure of \(_{}^{i,t_{j}}\)

Following the previous section, _CAL_ first dynamically computes the visually correlation weight \(_{[t_{j}]}^{i,j}\) of each token \(t_{j}\) by contrasting the image conditions. To avoid the effects of extreme values, we additionally introduce post-processing methods including clamping and average pooling. We clamp \( logit\) by setting the upper bound to \(\) and the lower bound to \((>=0)\). By setting \(\) to the extreme value \(0\), _CAL_ neglects the visually irrelevant tokens and visually contradictory tokens. By setting \(\) to the extreme value \(+\), _CAL_ tolerates the circumstances where some visually correlated tokens occupy most of the importance weights in all label tokens:

\[^{i,t_{j}}=clamp_{,}(_{[t_{j}]}^{i,j})\] (3)

We further introduce average pooling with a window size of \(W\) to smooth the visually correlation weights of each token, where we denote as:

\[}^{i,t_{j}}=pooling_{W}(^{i,t_{j}})\] (4)

The final loss objective of _CAL_ is the weighted average of the original MLE objective based on \(w\) and is defined as:

\[_{}^{i,t_{j}}=-^{l}}^ {i,t_{k}}}}^{i,t_{j}}_{}f_{}( }_{[t_{j}]}^{i,j})\] (5)

## 3 Experiments

### Experimental Setup

Implementation DetailsIn this paper, we verify our proposed _CAL_ on two leading model structures: LLaVA-1.5/LLaVA-NeXT [6; 10] and Mini-Gemini/Mini-Gemini-HD . LLaVA-1.5 uses CLIP-pretrained ViT-L as the visual encoder. For resolution scaling, LLaVA-NeXT employs a simple while adaptive image cropping strategy, encodes each image and concatenates them in one single sequence. Mini-Gemini (MGM) further introduces a LAION-pretrained ConvNeXt-L [22; 41] for high-resolution refinement. For MGM/MGM-HD/LLaVA-1.5, we follow the same setting as the original paper as it is public available, where the learning rate for the PT stage is set to \(1e^{-3}\) and the IT stage is set to \(2e^{-5}\) for both Vicuna-7B and Vicuna-13B. For LLaVA-NeXT, where only the evaluation code is made public, we reproduce LLaVA-NeXT with the same learning rate as MGM, and set the learning rate of ViT to 1/10 of the base learning rate (our reproduction presents on-par performance with the original paper/blog. We present a comparison of our reproduction results with those of the original papers in Appendix A.1). We also set the lower bound \(\) and upper bound \(\) in Equation (3) to 1 and 5 respectively, and we set \(l\) in Equation (4) to 3 for all experiments. We use 16 A100 for experiments, except for 8 GPUs in LLaVA-1.5/Gamma-2B and 32 GPUs in MGM-HD-13B.

DatasetsFor experiments on LLaVA-NeXT , since the detailed composition of training datasets is not publicly available, we use a slightly different training dataset combination, where we include the mixture of LLaVA\({}_{665k}\), VQA\({}^{}\), VQA\({}^{}\) and the ShareGPT4V . For experiments of LLaVA-1.5  and MGM/MGM-HD , we use the same dataset combination with original paper. The training datasets include LLaVA-filtered CC3M , ALLaVA , ShareGPT4V , LAION-GPT-4V , LIMA , OpenAssistant2 , VQA\({}^{}\), VQA\({}^{}\), DVQA  and AI2D . Finally, we report results on widely-adopted VLM benchmarks, including VQA\({}^{}\)(without providing OCR tokens), VQA\({}^{}\), VQA\({}^{}\), OCR-Bench , MMT , MMStar , SQA\({}^{}\), COCO Caption , TextCaps , and RefCOCOg  in our main experiments which observe significant improvement in majority settings, and additional benchmarks MME , POPE , SEED-I , VQA\({}^{}\)(with OCR tokens given), with comparable performance in Appendix A.2.

    &  &  &  & ^{I}\)**} &  &  &  \\    & & & **Doc** & & & & & & \\   \\  MGM & Gemma-2B & 335 & 39.8 & 23.4 & 48.1 & 60.6 & **25.5** & 43.4 & 6 7 \\ MGM+CAL & Gemma-2B & **360** & **44.8** & **27.0** & **51.8** & **64.0** & 25.4 & **45.4** & 6 7 \\ MGM & Vicuna-7B & 431 & 57.7 & **43.2** & 61.1 & 69.9 & 32.8 & 50.3 & 6 7 \\ MGM+CAL & Vicuna-7B & **443** & **58.0** & 42.8 & **63.0** & **70.4** & **35.5** & **51.4** & 6 7 \\ MGM & Vicuna-13B & **452** & **61.7** & **48.8** & 62.6 & 69.1 & 30.4 & 49.1 & 5 7 \\ MGM+CAL & Vicuna-13B & **466** & 61.6 & 48.0 & **63.8** & **71.9** & **33.7** & **51.9** & 5 7 \\  LLaVA-1.5 & Vicuna-7B & 315 & 28.5 & 17.5 & **47.6** & 68.2 & 32.4 & 48.6 & 5 7 \\ LLaVA-1.5+CAL & Vicuna-7B & **328** & **30.6** & 17.5 & 47.5 & **68.7** & **32.9** & **48.8** & 5 7 \\ LLaVA-1.5 & Vicuna-13B & 341 & 31.1 & 18.3 & 49.0 & 72.1 & 33.5 & 51.1 & 7 7 7 \\   \\  MGM-HD & Vicuna-7B & 477 & 72.0 & 49.3 & 65.5 & 68.4 & **31.0** & 47.9 & 6 7 \\ MGM-HD+CAL & Vicuna-7B & **503** & **73.4** & **49.6** & **67.1** & **69.2** & 30.1 & **50.5** & 6 7 \\ MGM-HD & Vicuna-13B & 502 & 77.7 & 55.8 & 67.2 & **73.5** & 34.2 & 50.9 & 6 7 \\ MGM-HD+CAL & Vicuna-13B & **535** & **78.0** & **57.2** & **68.8** & 73.1 & **38.5** & **51.4** & 6 7 \\  LLaVA-NeXT & Vicuna-7B & 542 & 75.1 & 62.2 & 64.2 & 68.5 & 33.7 & 49.5 & 7 7 7 \\ LLaVA-NeXT+CAL & Vicuna-7B & **561** & **77.3** & **64.3** & **65.0** & **70.1** & **35.5** & **50.7** & 7 7 \\ LLaVA-NeXT & Vicuna-13B & 553 & 78.4 & 63.8 & 67.0 & **71.8** & 37.5 & 50.4 & 6 7 \\ LLaVA-NeXT+CAL & Vicuna-13B & **574** & **80.1** & **67.2** & **67.1** & 71.5 & **38.1** & **52.4** & 6 7 \\   

Table 1: Visual Question Answering benchmarks of _CAL_ on leading methods including LLaVA-1.5, LLaVA-NeXT\({}^{}\), and MGM/MGM-HD. Our results are marked with \(\). VQA\({}^{}\) is evaluated without OCR tokens. Abbreviations: OCRB. (OCR-Bench), MMS. (MMStar), MMT. (MMT-Bench).

[MISSING_PAGE_EMPTY:6]

[MISSING_PAGE_FAIL:7]

Meanwhile, _CAL_ also presents better ability in capturing visually-conditioned details. For instance, compared with baseline, _CAL_ captures the material details _cd cover_, and the numerical details by telling _two men on horses_ from _a man on a horse_. The capability of _CAL_ to capture intricate details leads to sustained enhancements in the COCO caption benchmark. _CAL_ empowers the model to identify more accurate elements within images, including objects like _a trolley_ and _the number 8_, which might otherwise be incorrectly recognized or overlooked.

Complementary AnalysisWe first provide statistics for computational overhead in Appendix A.6. And we further provide more qualitative analysis on studying the quality of image-text modality alignment. The attention map scores are visualized in Appendix B.1, and the aligned image features are visualized in Appendix B.2.

## 4 Related Work

Vision Language ModelsLLMs  have made significant strides in Natural Language Processing (NLP) tasks, including text generation and question-answering, paving the way for VLMs that integrate vision ability with LLMs. In the realm of visual language learning, CLIP  has set a milestone by employing extensive image-text pair contrastive learning to achieve multimodal alignment. Recently, numerous VLMs  have leveraged the robust capabilities of LLMs for cross-modal understanding and generation tasks. Models like BLIP-2  and MiniGPT-4  have improved cross-modal alignment through comprehensive image-text pair pre-training. LLaVA  has further advanced its comprehension of complex prompts via refined instruction fine-tuning. Additionally, recent research  has incorporated higher resolution input images and longer sequences to enhance VLMs' understanding capabilities. Mini-Gemini (MGM)  introduces a LAION-pretrained ConvNeXt-L  for high-resolution refinement.

Image-text Modality AlignmentImage-text modality alignment has long been regarded as the core problem in cross-modal understanding and generation tasks. Traditional image-text alignment strategies include both contrastive learning across different modalities and generative learning that train text tokens in an autoregressive manner . The combination of both techniques is also proven to be effective in the early era of VLMs, where BLIP  proposes a multi-stage alignment strategy, with contrastive learning in early alignment and generative learning in the latter stage. However, in recent researches , contrastive learning is discarded for being redundant in image-text modality alignment of VLMs, and researchers propose to enhance the cross-modal alignment through dataset scaling and image resolution scaling . Despite being simple in application, the existing generative alignment method simply treats each text token with equal importance, resulting in sub-optimal alignment performance.

More recently, due to the great success in Reinforcement Learning (RL) in the alignment of LLMs, many recent works  have also integrated Reinforcement Learning methods to align existing VLMs with human preference. However, these RL-based methods require high-quality human-labeled pair-wise data and focus more on aligning with human preference rather than modality alignment.

Training-free Contrastive DecodingRecently, many researchers have proposed to improve the generation quality via contrastive decoding . Especially, in the field of LLMs, CID  utilizes contrastive decoding on paired text inputs for model de-biasing. Such method is also proven to be effective in enhancing the reasoning ability of LLMs in various aspects . Similar investigations have also been taking in VLMs. Recently, both VCD  and IBD  propose to enhance the generation of VLMs by contrasting the prediction logits between the original visual input and the perturbed ones. CRG  further proposes to improve the grounding ability without training via contrasting differently masked images. However, these training-free methods require additional computation during the decoding stage, making it highly ineffective for application.

## 5 Limitation

Despite the superiority of _CAL_ in various model structures, resolution settings and model scales on various benchmarks, limitations still exist in our proposed method. First of all, there lacks a clear and 

 p{113.8pt} p{113.8pt}}   _OCR cases_ & & \\  _Question:_ & What is written in the image? & What is the number in the image? \\ Baseline: & The word “consciousness” is written in the image. & The word “world” is written in the image. \\ _CAL_ : & The word “construction” is written in the image. & The word “would” is written in the image. \\  _Question:_ & What is the Mosman Manly exit going to? & How many items purchased from Amazon? \\ Baseline: & The mosman manly exit is going to mosman. & There are 2.43 million items purchased from amazon. \\ _CAL_ : & The mosman manly exit is going to chatswood epping. & \(902\)k. \\    
 p{113.8pt} p{113.8pt}}  _Question:_ & Provide a one-sentence caption for the provided image. & Provide a one-sentence caption for the provided image. \\ Baseline: & A red background with a sun and birds and the words Sibelius Symphonies No 2 \& & A book is open to a page with a picture of a man on a horse. \\ _CAL_ : & A cd cover for Sibelius Symphonies Nos 2 \& & A book is open to a page with a picture of two men on horses. \\  _Question:_ & Provide a one-sentence caption for the provided image. & Provide a one-sentence caption for the provided image. \\ Baseline: & A silver car is parked in front of a fence and a bus. & A person riding a horse with a cart attached to it. \\ _CAL_ : & A silver car is parked behind a fence in front of a trolley. & A horse pulling a cart with the number \(8\) on it. \\   

Table 5: OCR and captions generation comparison based on LLaVA-NEXT-13B model.

quantitative discrepancy between the three kinds of label tokens. More discussion of the importance weights guidance can be further investigated in future works.

The selection of lower bounds and upper bounds in Equation (3) are empirically decided based on the frequency of the prediction logits, which could be extended to more adaptive settings in further explorations. Nevertheless, the simple while broadly effective nature of _CAL_ indicates the importance of a delicate image-text modality alignment strategy for leading VLM structures.

## 6 Conclusion

In this paper, we investigate the in-completeness of current image-text alignment in leading VLMs by treating all text tokens with equal weights. We present by contrasting input images, the difference in the prediction logits for each token naturally reveals their visual correlation. We therefore propose a token re-weighting strategy that prioritize the training of highly visually correlated tokens. Our proposed strategy, _CAL_ is simple while impressively effective, achieving consistent performance gain across various benchmarks including visual question answering, image-captioning and grounding.

Our work raises a question about the potential optimal learning strategy of image-text modality alignment. Both the imperfectness of training data and over concentration on visually irrelevant/visually contradictory tokens hinder the performance of current VLMs. We hope the proposed _CAL_ can inspire more investigation on better alignment strategy to enhance the capabilities of existing VLMs.