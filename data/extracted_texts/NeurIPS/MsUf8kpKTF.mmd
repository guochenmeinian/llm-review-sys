# A Study of Plasticity Loss in

On-Policy Deep Reinforcement Learning

 Arthur Juliani  Jordan T. Ash

Microsoft Research NYC

{ajuliani, ash.jordan}@microsoft.com

###### Abstract

Continual learning with deep neural networks presents challenges distinct from both the fixed-dataset and convex continual learning regimes. One such challenge is _plasticity loss_, wherein a neural network trained in an online fashion displays a degraded ability to fit new tasks. This problem has been extensively studied in both supervised learning and off-policy reinforcement learning (RL), where a number of remedies have been proposed. Still, plasticity loss has received less attention in the on-policy deep RL setting. Here we perform an extensive set of experiments examining plasticity loss and a variety of mitigation methods in on-policy deep RL. We demonstrate that plasticity loss is pervasive under domain shift in this regime, and that a number of methods developed to resolve it in other settings fail, sometimes even performing worse than applying no intervention at all. In contrast, we find that a class of "regenerative" methods are able to consistently mitigate plasticity loss in a variety of contexts, including in gridworld tasks and more challenging environments like Montezuma's Revenge and ProcGen.

## 1 Introduction

In many important machine learning domains, such as continual learning and online reinforcement learning (RL), training data are not wholly available simultaneously. Rather, in these settings, data arrives sequentially over a long period of time. Irrespective of the available quantity of training data, we would typically like to have the highest performing model possible at any given point. Unfortunately, these sequential learning scenarios are known to present optimization issues for neural networks. Specifically, if a model is naively updated to convergence each time new data arrives, and the amount of training data effectively increases, the resulting model generalizes worse than it would under more standard, non-sequential constraints. This phenomenon has been introduced by Ash and Adams (2020) as the "warm-start problem," where it was found that randomly initialized models generalize well but are expensive to fit, and warm-started models generalize poorly but converge far more quickly.

An adjacent problem to warm-starting is that of _plasticity loss_(Lyle et al., 2023), which presents itself both in the supervised learning and reinforcement learning settings. Like in the warm-start problem, plasticity loss describes a degradation in a model's ability to fit new data as training progresses, ultimately harming both sample efficiency and asymptotic performance. If an agent achieves worse performance when fitting first a source distribution and then a distinct target distribution than it would if instead trained on the target distribution in isolation, the degradation is attributed to plasticity loss.

Whereas the warm-start problem is restricted to test performance, plasticity loss is generally measured as a degradation in performance on training data, often under some form of distribution shift. Despite the typical focus on training data, an ideal solution to plasticity loss would be able to prevent performance degradation both for data seen and unseen when fitting the policy. Importantly, the plasticity loss phenomenon is distinct from overfitting, as resolving plasticity loss should improve both training performance and generalization. In contrast, overfitting describes the opposite, where improving training performance has deleterious effects on generalization.

This work studies the plasticity loss phenomenon in detail for the on-policy reinforcement learning setting. We introduce three distinct kinds of distribution shift to facilitate our analysis as well as a variety of environments and tasks. While several interventions have already been proposed, we demonstrate that some of the methods which proved successful for addressing plasticity loss in the supervised learning or off-policy reinforcement learning setting fail in the on-policy setting or under our proposed forms of distribution shift. We further highlight several criteria that we believe are necessary to remedy the issue. Based on these insights, we describe several techniques that resolve plasticity loss on the environments we consider.

In summary, the contributions of this paper are:

* We extend studies of plasticity loss and the warm-start problem to the on-policy regime, finding that they present a persistent issue across a variety of model architectures and environmental distribution shift conditions.
* We provide an in-depth analysis of the correlates of these pathologies, studying several types of environmental settings, model architectures, and previously proposed approaches for mitigation in settings related to on-policy reinforcement learning. Importantly, we include generalization trends in our consideration of these phenomena.
* We characterize properties of methods that seem necessary for interventions to be successful at both addressing plasticity loss and ensuring generalization performance is maintained, and provide recommendations along these lines.

## 2 Preliminaries

An RL algorithm is described as "on policy" if it is trained using data collected from its own policy. While generally more sample inefficient than off-policy alternatives, where the data collection mechanism deviates from the policy being fit, on-policy methods have become more commonly used in practical RL problems because of their simplicity and stability (Andrychowicz et al., 2020).

Proximal Policy Optimization (PPO) is a ubiquitous on-policy deep RL algorithm (Schulman et al., 2017). As a trust-region method, PPO constrains the extent by which a weight update can modify the current policy, quelling instability issues caused by high-variance policy gradients. Despite this robustness, on-policy algorithms still update on data corresponding only to the agent's current experience, which may be sub-optimal for capturing necessary information around solving the underlying MDP. There is some evidence that plasticity loss can be present in on-policy RL learning problems (Dohare et al., 2023), demonstrating the effect in a somewhat ad hoc manner, we systematically use multiple environments and distribution shift conditions to more thoroughly characterize plasticity loss--and how to resolve it--in the on-policy setting.

### Simulating environmental distribution shift

This paper primarily studies three distinct forms of distribution shift that we can reasonably expect a plasticity-preserving intervention to mitigate. Each experiment is organized into \(r\) distinct rounds. In each round we supply \(k\) environments from some distribution to the agent, fit a policy for that set of environments, and measure performance at the end of the round. To induce distribution shift, between each round we modify the data according to one of the three strategies described below. The environmental modifications we consider are:

**Permute.** Input pixels are randomly shuffled from round to round. Each of the \(k\) environments in a given round are shuffled in the same way, but distinct from the shuffling in any other round. This method was utilized in Dohare et al. (2023). We expect that an optimal learning algorithm will be able to achieve equivalent performance on the initial and subsequent rounds.

**Window.** We supply the agent with \(k\) new environments for the same task. Here previously seen environments are no longer accessible, so the agent can only optimize with respect to the current \(k\). This approach was considered by

Figure 1: Examples of gridworld environment tasks. The agent (black triangle) begins each episode in the center of the environment. Blue jewels provide _+1_ reward, red jewels provide _-1_ reward, and dark grey walls prevent movement. Objects are placed randomly.

Abbas et al. (2023) and Lyle et al. (2023). We expect that an optimal learning algorithm will be able to achieve equivalent or better performance on subsequent rounds as compared to the initial round.

**Expand.** Like in the window modification, \(k\) new environments are supplied to the agent. Here, however, they are appended to the total amount of training data, such that the number of available environments in the final round of training is \(k n\). This is most similar to the warm-start environment studied primarily in Ash and Adams (2020), and revisited in Igl et al. (2020). In this setting we expect training performance to decrease each round, even for an optimal learning algorithm, as available training data grows and becomes more difficult to fit. Correspondingly, we expect the generalization performance to increase as the training distribution expands to better capture the true distribution.

In the case of the latter two modifications, data at each new round are drawn from the exact same distribution as environments from previous rounds. As we demonstrate in Section 4.1, these still induce significant performance depredation in warm-started models. This observation is surprising--we often think of a model's initialization as a sort of "prior" over learned functions, and here we demonstrate that even an initialization obtained by training on data distributed identically to the current round still hinders performance in on-policy learning.

We use a simple gridworld task as our main sandbox for studying plasticity loss. The environment is drawn from the NeuroNav library (Juliani et al., 2022), and the goal of the agent is to collect rewarding blue jewels while avoiding punishing red jewels in a fixed time window (100 time-steps per episode). Each sample of the environment from the distribution of possible tasks changes both the location of the jewels and the walls of the maze. Figure 1 shows environment modification examples.

## 3 Existing Approaches

A number of methods have been proposed to address plasticity loss in deep neural networks, though none were designed with on-policy RL explicitly in mind. Instead, these methods have been primarily validated in the context of either supervised learning or off-policy RL. Each of these techniques can be divided into one of two groups: Interventions which are performed intermittently during training, and those which are applied continuously, either as part of the model architecture or as part of the model update process. For additional implementation details see Appendix Section A.

### Intermittent Interventions

We consider an intervention to be _intermittent_ if it is applied only at specific points during training. Periodically, the interventions in this category are applied, and training proceeds normally otherwise. Most of these are applied each time there is a training distribution change, implying that there must be awareness of when this occurs; in practice this information may be unavailable and difficult to detect.

**Resetting final layer.** Proposed by Nikishin et al., this method involves periodically replacing the weights of the final layer in the network with newly initialized values. It was demonstrated that this alleviates plasticity loss in certain off-policy RL settings.

**Shrink+Perturb.** This technique periodically scales the magnitude of all weights in the network by a factor and then adds a small amount of noise (Ash and Adams, 2020). It was demonstrated that this improves performance in batched continual learning settings. In our implementation the weight of these two factors are entangled such that they sum to one.

**Plasticity Injection.** Plasticity Injection replaces the final layer of a network with a new function that is a sum of the final layer's output and the output of a newly initialized layer subtracted by itself (Nikishin et al., 2023). The gradient is then blocked in both the original layer and the subtracted new layer. It was shown that this method increases performance of off-policy RL agents in the ALE.

**ReDo.** This technique resets individual neurons within the network based on a dormancy criteria at fixed intervals (Sokar et al., 2023), leading to improvement in performance in the context of off-policy RL agents trained on the ALE. Following Sokar et al., we reset the model parameters more frequently than we apply environmental distribution shifts.

### Continuous Interventions

We characterize a method as _continuous_ if it is applied at every step of optimization. Continuous interventions are desirable because they do not require an awareness of when a distribution shift has occurred, which can be challenging in practical scenarios where we want to mitigate plasticity loss.

**L2 Norm.** Discussed by Lyle et al., this method involves regularizing the network using the L2 norm. It was demonstrated that in certain continual learning settings this reduces plasticity loss.

**LayerNorm.** LayerNorm (Ba et al., 2016) is a ubiquitous deep learning regularization technique, and was shown useful for mitigating plasticity loss by Lyle et al.. The authors demonstrated a performance improvement in the ALE when using off-policy RL.

**CReLU activations.** Proposed by Abbas et al. and studied further by Lee et al., this method involves replacing ReLU activations with the CReLU activation function. This ensures that the gradient is non-zero for all units in a given layer, which appears to mitigate plasticity loss when periodically switching between tasks in the ALE with off-policy methods.

**Regenerative Regularization.** Regenerative regularization uses an L2 penalty to encourage model weights to be near their initial values (Kumar et al., 2023). It has been shown to be effective in a number of continual learning settings.

## 4 Experiments

Below we present results of a suite of experiments conducted using a 2D gridworld environment (Juliani et al., 2022), the procedurally generated CoinRun environment (Cobbe et al., 2019), and the Atari game Montezuma's Revenge (Bellemare et al., 2013). All models are trained using PPO (Schulman et al., 2017). For the gridworld experiments we use an MLP encoder, and for the CoinRun and Montezuma's Revenge experiments we instead use a convolutional encoder. We set the number of environment instances (\(k\)) to 100 for all experiments conducted with the gridworld and CoinRun environments. Additional training details can be found in Appendix Section B.

### Plasticity loss is apparent in on-policy RL

We find evidence of plasticity loss in the on-policy RL setting in the presence of multiple kinds of distribution shift, as presented in Figure 2. For round-level plots like these, reward values are normalized such that the mean reward at the end of the first round is set to zero. As such, positive and negative values correspond respectively to increases and decreases in performance relative to the first round. Decreasing normalized reward indicates a decay in relative performance, implying plasticity loss.

For both the permute and window shift conditions, Figure 2 demonstrates clear degradation in mean episodic reward for the warm-started model as compared to a model which has all of its weights reset between each round. While there is some positive transfer for the warm-start method in the expand shift condition, we find generalization performance suffers in this condition as compared to reset-all. We provide statistical tests comparing the performance of these methods to the warm-start and random initialization baselines in Section D of the appendix.

Figure 2: Performance in the gridworld environment under each of the three distribution shift conditions. The degradation between rounds is evidence of plasticity loss. (A): Epoch-level training performance for permute modification. Dotted vertical lines indicate the end of each round, before a new environmental distribution shift is applied. (B): Round-level training performance for the Permute modification. Data points correspond to normalized mean reward in final 50 episodes of round. Shaded regions correspond to standard error. (C, D): Round-level training performance for the Window and Expand conditions. **Top row**: Training performance. **Bottom row**: Test performance.

### Predictors of plasticity loss and generalization

Identifying the exact statistical underpinnings of plasticity loss is an active area of research in continual and reinforcement learning. Here we identify several factors which are significantly correlated with plasticity loss as well as a degradation in generalization performance. Figure 3 presents correlation plots of normalized reward with various metrics of interest. Here we normalize rewards (i.e. subtract the initial reward value from the final reward value) to compensate for the fact that many of these algorithms have a regularization effect that improves performance on average but does not necessarily improve performance trends--it is possible to improve generalization, for example, without remedying plasticity loss.

Note that the _LayerNorm_ and _CReLU_ methods are not included in this analysis, due to their direct effects on the weight magnitude and dead unit counts, respectively. For graphs of the underlying metrics over time during training, see Supplemental Figure 11. We find that weight magnitude and number of dead units are both significantly correlated with the normalized reward [\(p<0.05\)]. "Dead units" refer to ReLU activations that never produce non-negative outputs.

We find that neither the gradient magnitude nor the magnitude of weight difference between updates has a significant, linear relationship with either plasticity or generalization. This is in contrast to the findings of Abbas et al. and Nikishin et al., each of which examined plasticity loss in off-policy rather on-policy settings. The former found that weight difference was correlated with loss of plasticity while the latter found that gradient norms were predictive.

We also fit a generalized linear model (GLM) using all five metrics as independent variables and normalized reward as the dependent variable. Here we find that both gradient and weight magnitudes

Figure 3: **Top**: Correlation plots of normalized mean reward in the gridworld environment compared against identified metrics. Each point is averaged over five replicates, and shows the final values produced after a ten-round experiment. Values for each measurement are normalized by its baseline level at initialization. Measurements that significantly correlate (\(p<0.05\)) with normalized reward are bolded. **First row**: Training distribution performance. **Second row**: Test distribution performance. **Bottom**: Values of the three predictive metrics during the course of training for each intervention and window change condition as compared to training performance. Shaded regions correspond to standard error. Final values of plots like these correspond to a single point in the correlation plots above.

are significantly predictive of train performance [\(p<0.01\)], while dead unit count no longer is [\(p>0.05\)]. This suggests that the predictive power of the dead unit count can be accounted for by the weight magnitude. An additional GLM was fit to the test data, and we find that only the dead unit count remains predictive [\(p<0.05\)].

Although these analyses do not establish causality, the robust significance of weight magnitude in predicting plasticity loss suggests that plasticity loss may be a function of how much trainable parameters deviate from their initialization distribution. As such, successful mitigation strategies would induce a sub-linear increase in the weight norm of the network parameters over the course of learning. This has the downstream effect of minimizing the number of dead units in the network. Below we compare the mitigation strategies, finding that those which address this underlying cause directly indeed perform best.

### Novel architectural methods do not fully address plasticity loss

Recently a number of methods have been introduced to specifically address plasticity loss using changes to the network architecture. These methods were previously validated in the off-policy setting only. Here we consider the behavior of two of these methods as it applies to the on-policy setting: _CReLU_Abbas et al. (2023) and _Plasticity injection_Nikishin et al. (2023). We find that both methods fail to address plasticity loss in the permute shift condition. The _CReLU_ method is able to address plasticity loss in the window and expand settings only, while the plasticity inject method underperforms even the warm-start baseline in all three contexts (Figure 4).

### Regularization methods address plasticity loss

We next consider the class of methods which regularize the weights of the network, either continuously or intermittently. These methods include _final layer reset_Nikishin et al. (2022), _shrink+perturb_Ash and Adams (2020), a continuous "soft" variant of shrink+perturbDohare et al. (2023), _L2 regularization_, _ReDo_Sokar et al. (2023), and _regenerative regularization_Kumar et al. (2023). We find _final layer reset_ fails to address plasticity loss in any of the three conditions. In contrast, the other five methods mitigate both plasticity loss and the warm-start problem to some extent in all three shift conditions (Figure 4). Of these methods, _ReDo_ performs the worst as it is unable to benefit from the positive transfer which is possible in the cases of the "window" and "expand" conditions,

Figure 4: Performance of intervention methods compared to warm-start and reset-all baselines on the _Gridworld_ environment. Final round mean reward is normalized by the performance at end of the first round, and interval bars denote standard error. **Top**: Train performance. **Bottom**: Test performance.

as methods that retain information from previous rounds effectively have access to more training data in these conditions. _ReDo_ underperforms even the warm-start baseline in the "expand" condition.

### LayerNorm addresses training plasticity loss but not generalization

We also consider the effect of LayerNorm on plasticity loss. It was previously demonstrated in the off-policy setting that LayerNorm was able to mitigate some effects of plasticity loss (Lyle et al., 2023). Given that LayerNorm tends to improve learning across a number of contexts (Ba et al., 2016), it is important to attempt to separate the effects of LayerNorm on performance generally from its effect on plasticity specifically. We find that LayerNorm resolves plasticity loss in terms of training performance, but is inconsistent in its effect on generalization performance. For example, the LayerNorm model significantly underperforms even the warm-start baseline on the test data of the permute shift condition. We find however that combining _soft shrink+perturb_ or _regenerative regularization_ with LayerNorm results in overcoming both training plasticity loss and deleterious generalization trends (Figure 4).

### Plasticity loss in _CoinRun_

We also use the CoinRun environment from the ProcGen suite of tasks to evaluate the set of candidate interventions Cobbe et al. (2019, 2020). ProcGen is built using procedural generation Cobbe et al. (2020), making it possible to study the same three distribution-shift conditions which were considered in the gridworld experiments. Here we present performance results for each of the intervention methods on the CoinRun environment.

Overall, we find results largely consistent with those of the gridworld task. There is significant plasticity loss present in all three tasks as measured by the gap between the warm-start and reset-all conditions. We also find that _reset-final_, _CReLU_, and _plasticity injection_ all unable to fully resolve the loss of plasticity. In contrast, the three most effective methods in the gridworld task (_LayerNorm_, _regenerative regularization_, and _soft shrink+perturb_) are all also most effective in CoinRun as well.

Figure 5: Performance of intervention methods compared to warm-start and reset-all baselines on the _CoinRun_ environment. Final round mean reward is normalized by the performance at end of the first round, and interval bars denote standard error. **Top**: Train performance. **Bottom**: Test performance.

See Figure 5 for plots of the normalized mean rewards over the course of training for all methods. For results on two additional ProcGen environments, see Appendix D. For round-level performance plots of all methods, see Section D in the appendix. We find that relative efficacy of the intervention methods is consistent across ProcGen environments.

### Plasticity loss in _Montezuma's Revenge_

Montezuma's Revenge is an Atari game that is often used to benchmark the quality of exploration procedures in RL Bellemare et al. (2013); Salimans and Chen (2018). The environment consists of many separate rooms, some only accessible through a locked door. The agent must avoid obstacles, find and use keys, and traverse several rooms before obtaining non-trivial reward. Because of this reward sparsity, exploration is needed to incentivize the agent to advance beyond the periphery of its experience.

Montezuma's Revenge is designed such that the agent only sees the room it currently occupies. Accordingly, the distribution over input states expands as the agent learns to explore the environment. This introduction of significantly different states induces a level of distribution shift which is unique among ALE environments. Within our framework, the distribution shift of Montezuma's Revenge is most similar to the "expand" condition studied here.

In this experiment we consider two versions of a policy trained using the Random Network Distillation (RND) exploration bonus (Burda et al., 2018). All architectures and hyperparameters are inherited from the original RND paper. Figure 6 compares a typical RND policy to ones that are trained in conjunction with the _soft shrink+perturb_ or _regenerative regularization_ interventions. The plasticity loss mitigating policy achieves higher reward than a corresponding agent fitted in a standard fashion. Due to computation constraints, we did not evaluate the other intervention methods.

## 5 Discussion

Plasticity loss has been identified and rigorously studied in the continual learning and off-policy reinforcement learning settings. We find that it is also an issue in the on-policy setting under a variety of different conditions. Methods introduced in the off-policy setting such as _plasticity injection_ and _CReLU_ appear to be not as consistently effective in the on-policy setting. We hypothesize that this degradation may be due to the violation of the iid assumption which supervised learning and off-policy reinforcement learning rely on. Further, these methods require architectural modifications of the policy and value networks, and are thus less general than regularization-based alternatives.

In contrast, the class of regularization methods, which include _L2 regularization_, _ReDo_, _shrink+perturb_, and _regenerative regularization_ all are able to significantly mitigate plasticity loss in our experiments. Of these methods, _soft shrink+perturb_ displayed the best generalization performance, and can be easily combined with layer normalization. _LayerNorm_, a now ubiquitous method in supervised learning, was previously introduced to address plasticity loss in the off-policy setting Lyle et al. (2023), and we find that it is also effective in the on-policy setting. Due to its general regularizing effects, _LayerNorm_ also increases baseline performance in the absence of distributional shift Ba et al. (2016), suggesting that it is a generally useful method for on-policy reinforcement learning methods such as PPO.

A consistent feature of the class of regularization methods discussed here is that they all normalize the network parameters towards their initial distribution. This has the effect of decreasing the weight magnitude and reducing the number of dead or saturated activation units, as seen in Figure 3. It has recently been proposed that both these metrics may be a proxy for the underlying curvature of the optimization landscape (Lewandowski et al., 2023). Studying this connection more deeply would potentially be a fruitful avenue for future research.

Figure 6: A comparison between RND agents trained with and without a plasticity-loss mitigating intervention. Experiments are done over twenty replicates and shaded regions show standard error.

Related Work

In the context of generalization in continual learning settings, a related issue to plasticity loss is the warm-start problem (Ash and Adams, 2020). Recent work has shown that the benefits of methods such as shrink+perturb to generalization were dependent on the presence of noise in the labels used for training (Zaidi et al., 2023). In the absence of noisy labels, other regularization methods were found to be competitive with methods that utilize re-initialization such as shrink+perturb.

Another perspective on the problem of loss of plasticity has focused on the content of early learning in a network. Achille et al. (2018) demonstrate that neural networks have critical early periods of sensitivity to training data which are analogous to those in animals (Hensch, 2004). In the biological context this sensitivity is tied directly to neuronal plasticity. This connection to critical period learning was also drawn in work on off-policy RL, in which a "primacy bias" was proposed to explain the loss of plasticity seen in a number of task domains (Nikishin et al., 2022). Utilizing this insight, the periodic soft-resetting of the neural network has enabled significant increases in the sample efficiency of off-policy RL methods (D'Oro et al., 2022; Schwarzer et al., 2023).

In the deep RL domain a similar problem called _ray interference_ has been identified (Schaul et al., 2019). Ray interference arises from the conflicting gradient signals related to unique sub-tasks required to solve a single more complex task in online deep reinforcement learning. It is possible that for complex tasks such as CoinRun or Montezuma's Revenge there are unique sub-tasks which are learned and whose gradients may interfere with one another. Exploring the distinction between ray interference and plasticity loss in the RL setting is a promising future direction.

In the off-policy RL setting the problem of plasticity loss has also been studied under the name of "capacity loss" (Lyle et al., 2022). While plasticity loss refers to a property of the network-task interaction, capacity loss is a more generic term to refer to the properties of a network that are task invariant. This work advocates for a solution similar to that of Kumar et al. (2023), and can thus be considered in the class of methods which regularize the network towards an initialization distribution.

Within the domain of on-policy RL there is a related phenomena to plasticity loss called "policy collapse," which refers to a degradation of performance as training progresses, even on a fixed dataset (Dohare et al., 2023c). Although these are distinct phenomena, there is likely overlap in the success of mitigation methods. A novel optimizer strategy "Non-stationary Adam" was proposed to address policy collapse, and although we did not study it, it may provide benefits on the tasks examined here. In terms of direct studies of plasticity loss in on-policy RL, both Dohare et al. (2023a) and Igl et al. (2020) show plasticity loss in the on-policy setting, and propose a method to address it.

As part of their analysis, Dohare et al. (2023b) propose a plasticity-enhancing method called "Continual Backprop," which is another form of regularization towards the initialization distribution, however it is one which is selective on a per-neuron basis. This method is similar to ReDo (Sokar et al., 2023), which we analyze here. It is also related to DrM (Xu et al., 2023), which builds on the DrQ algorithm.

It is finally worth mentioning that notions of plasticity loss have been studied in the neuroscience community as well. These range from classic work demonstrating the inability of animal models to fully accommodate new sensory input after developing cognitively without it (Wiesel and Hubel, 1963), to more recent work characterizing plasticity loss in humans as a component of various psychopathologies (Peled, 2005; Carhart-Harris et al., 2023; Juliani et al., 2024).

## 7 Conclusion

In this work we studied the problem of plasticity loss in the context of on-policy deep RL. We find that similar to continual learning and off-policy RL, plasticity loss is an issue across a number of different environments and forms of distributional shift. We find that some methods previously proposed to resolve the issue in other problem settings, such as _CReLU_ and _plasticity injection_ do not transfer to on-policy RL. The methods which best resolve plasticity loss are those which act as continual regularizers as opposed to intermittent interventions. Within this class of methods we find that a soft variant of _shrink+perturb_ combined with LayerNorm performs the best across our evaluated settings, providing a simple and general method for addressing plasticity loss.