# SubgDiff: A Subgraph Diffusion Model to Improve Molecular Representation Learning

Jiying Zhang, Zijing Liu1, Yu Wang, Bin Feng, Yu Li1

International Digital Economy Academy (IDEA)

{zhangjiying,liuzijing,fengbin,liyu}@idea.edu.cn

https://github.com/IDEA-XL/SubgDiff

###### Abstract

Molecular representation learning has shown great success in advancing AI-based drug discovery. A key insight of many recent works is that the 3D geometric structure of molecules provides essential information about their physicochemical properties. Recently, denoising diffusion probabilistic models have achieved impressive performance in molecular 3D conformation generation. However, most existing molecular diffusion models treat each atom as an independent entity, overlooking the dependency among atoms within the substructures. This paper introduces a novel approach that enhances molecular representation learning by incorporating substructural information in the diffusion model framework. We propose a novel diffusion model termed SubgDiff for involving the molecular subgraph information in diffusion. Specifically, SubgDiff adopts three vital techniques: i) subgraph prediction, ii) expectation state, and iii) \(k\)-step same subgraph diffusion, to enhance the perception of molecular substructure in the denoising network. Experiments on extensive downstream tasks, especially the molecular force predictions, demonstrate the superior performance of our approach.

## 1 Introduction

Molecular representation learning (MRL) has attracted tremendous attention due to its significant role in learning from limited labeled data for applications like AI-based drug discovery [37; 3; 4; 25] and material science . From the perspective of physical chemistry, the 3D molecular conformation can largely determine the properties of molecules and the activities of drugs [6; 7]. Thus, numerous geometric neural network architectures and self-supervised learning strategies have been proposed to explore 3D molecular structures to improve performance on downstream molecular property prediction tasks [35; 56; 23; 58].

Meanwhile, diffusion probabilistic models (DPMs) have shown remarkable power to generate realistic samples, especially in synthesizing high-quality images and videos [40; 10]. By modeling the generation as a reverse diffusion process, DPMs transform a random noise into a sample in the target distribution. Recently, diffusion models/flow models have also

Figure 1: Equilibrium probability of the six different 3D structures (c1â€“c6) of the same molecule ibuprofen (C13H18O2) in four different conditions. (Adapted with permission from . Copyright 2018 American Chemical Society.)demonstrated strong capabilities of molecular 3D conformation generation [52; 16; 59; 43; 61; 59]. The training process of a DPM for conformation generation can be viewed as the reconstruction of the original conformation from a noisy version, where the noise is modulated by different time steps. Consequently, the denoising objective in the diffusion model can naturally be used as a self-supervised representation learning technique . Inspired by this intuition, several works have used this technique for molecule pretraining [24; 56]. Despite considerable progress, the potential of DPMs for molecular representation learning has not been fully explored. We therefore raise the question: _Can we effectively enhance MRL with the denoising network (noise predictor) of DPM? If yes, how to achieve it?_

To answer this question, we first analyze the gap between the current DPMs and the characteristics of molecular structures. Most diffusion models on molecules propose to independently inject continuous Gaussian noise into the every node feature  or atomic coordinates of 3D molecular geometry [52; 56]. However, this approach treats each atom as an individual particle, overlooking the substructure within molecules, which is pivotal in molecular representation learning [55; 46; 28]. As shown in Figure 1, the substructures are mostly invariant in different 3D molecular conformations, which contains crucial information about the properties, such as the equilibrium distribution, crystallization and solubility . As a result, uniformly adding same-scale Gaussian noise to all atoms makes it difficult for the denoising network to learn the joint distribution of global structure and local substructure, hindering the downstream prediction performance on molecular properties closely related to 3D conformation (e.g. most physicochemical properties). So here we try to tackle the previous question by designing a DPM involving the knowledge of substructures.

Toward this goal, we propose a novel diffusion model termed SubgDiff, adding distinct Gaussian noise to different substructures of 3D molecular conformation. Specifically, instead of adding the same Gaussian noise to every atomic coordinate, SubgDiff only adds noise to a randomly selected subgraph at each time step in the diffusion process (Figure 2). In the training phase of SubgDiff, a subgraph prediction task is integrated into the training objective, which explicitly directs the denoising network to capture substructure information from the molecules. Additionally, we propose two techniques: _expectation state diffusion_ and _k-step same-subgraph diffusion_, to train the model effectively and enhance its sampling capability.

With the ability to capture the substructure information from the noisy 3D molecule, the denoising networks tend to gain more representation power. The experiments on various 2D and 3D molecular property prediction tasks demonstrate the superior performance of our approach. To summarize, our contributions are as follows: (1) we incorporate the substructure information into diffusion models to improve molecular representation learning; (2) we propose a new diffusion model SubgDiff that adopts subgraph prediction, expectation state and \(k\)-step same-subgraph diffusion to improve its sampling and training; (3) the proposed representation learning method achieves superior performance on various downstream tasks, especially molecular forces prediction.

Figure 2: Comparison of forward process between DDPM  and subgraph diffusion. For each step, DDPM adds noise into all atomic coordinates, while subgraph diffusion selects a subset of the atoms to diffuse.

Related work

**Diffusion models on graphs.** The diffusion models on graphs can be mainly divided into two categories: continuous diffusion and discrete diffusion. Continuous diffusion applies a Gaussian noise process on each node or edge [14; 30], including GeoLiff , EDM , SubDiff . Meanwhile, discrete diffusion constructs the Markov chain on discrete space, including Digress  and GraphARM . However, it remains open to exploring fusing the discrete characteristic into the continuous Gaussian on graph learning, although a closely related work has been proposed for images and cannot be used for generation . Our work, SubgDiff, is the first diffusion model fusing subgraph, combining discrete characteristics and the continuous Gaussian.

**Conformation generation.** Various deep generative models have been proposed for conformation generation, including CVGAE , GraphDG , CGCF , ConfVAE , ConfGF  and GeoMol . Recently, diffusion-based methods have shown competitive performance. Torsional Diffusion  raises a diffusion process on the hypertorus defined by torsion angles. However, it is not suitable as a representation learning technique due to the lack of local information (length and angle of bonds). GeoDiff  generates molecular conformation with a diffusion model on atomic coordinates. However, it views the atoms as separate particles, without considering the dependence between atoms from the substructure.

**SSL for molecular property prediction.** There exist several works leveraging the 3D molecular conformation to boost the representation learning, including GraphMVP , GeoSSL , the denoising pretraining approach raised by  and MoleculeSDE , etc. However, those studies have not considered the molecular substructure in the pertaining. In this paper, we concentrate on how to boost the perception of molecular substructure in the denoising networks through the diffusion model.

The discussion with more related works (e.g. MDM , MDSM  and SSSD ) can be found in Appendix B.1.

## 3 Preliminaries

**Notations.** We use \(\) to denote the identity matrix with dimensionality implied by context, \(\) to represent the element product, and \(()\) to denote the diagonal matrix with diagonal elements of the vector \(\). If not specified, both \(\) and \(z\) represent noise sampled from the standard Gaussian distribution \((,)\). The topological molecular graph can be denoted as \((,,)\) where \(\) is the set of nodes, \(\) is the set of edges, \(\) is the node feature matrix, and its corresponding 3D Conformational Molecular Graph is represented as \(G_{3D}(,)\), where \(=[R_{1},,R_{||}]^{|| 3}\) is the set of 3D coordinates of atoms.

**DDPM.** Denoising diffusion probabilistic models (DDPM)  is a typical diffusion model  which consists of a diffusion (aka forward) and a reverse process. In the setting of molecular conformation generation, the diffusion model adds noise on the 3D molecular coordinates \(\).

**Forward and reverse process.** Given the fixed variance schedule \(_{1},_{2},,_{T}\), the posterior distribution \(q(R^{1:T}|R^{0})\) that is fixed to a Markov chain can be written as

\[q(^{1:T}|^{0})=_{t=1}^{T}q(^{t}|^{t-1}); q(^{t}|^{t-1})=(^{t} ;}^{t-1},_{t}).\] (1)

To simplify notation, we consider the diffusion on single atom coordinate \(R_{v}\) and omit the subscript \(v\) to get the general notion \(R\) throughout the paper. Let \(_{t}=1-_{t}\), \(_{t}=_{i=1}^{t}(1-_{i})\), and then the sampling of \(R^{t}\) at any time step \(t\) has the closed form: \(q(R^{t}|R^{0})=(R^{t};_{t}}R^{0},(1- _{t}))\).

The reverse process of DDPM is defined as a Markov chain starting from a Gaussian distribution \(p(R^{T})=(R^{T};,)\):

\[p_{}(R_{0:T})=p(R^{T})_{t=1}^{T}p_{}(R^{t-1}|R^{t}); p _{}(R^{t-1}|R^{t})=(R^{t-1};_{}(R^{t},t),_{t}),\] (2)

where \(_{t}=_{t-1}}{1-_{t}}_{t}\) denote time-dependent constant. In DDPM, \(_{}(R^{t},t)\) is parameterized as \(_{}(R^{t},t)=_{t}}(R^{t}-}{_{t}}}_{}(R^{t},t))\) and \(_{}\), i.e., the _denoising network_, is parameterized by a neural network where the inputs are \(R^{t}\) and time step \(t\).

**Training and sampling.** The training objective of DDPM is:

\[_{simple}()=_{t,R^{0,}}\|-_{ }(_{t}}R^{0}+_{t}},t)\|^{2}].\] (3)

After training, samples are generated through the reverse process \(p_{}(R^{0:T})\). Specifically, \(R^{T}\) is first sampled from \((,)\), and \(R^{t}\) in each step is predicted as follows,

\[R^{t-1}=}}(R^{t}-}{_{t}}}_{}(R^{t},t))+_{t}z, z( ,).\] (4)

## 4 SubgDiff

Directly using DDPM on atomic coordinates of 3D molecules means each atom is viewed as an independent single data point. However, the substructures play an important role in molecular generation  and representation learning . Ignoring the inherent interactions among atoms within substructures may hinder the denoising network from learning features for molecular properties related to 3D conformation. In this paper, we propose to involve a mask operation in each diffusion step, leading to a new diffusion SubgDiff for molecular representation learning. Each mask corresponds to a subgraph in the molecular graph, aligning with the substructure in the 3D molecule. Furthermore, we incorporate a subgraph predictor and reset the state of the Markov Chain to the expectation of atomic coordinates, thereby enhancing the effectiveness of SubgDiff in sampling. Additionally, we also propose \(k\)-step same-subgraph diffusion for training to effectively capture the substructure information.

### Involving subgraph into diffusion process

In the forward process of DDPM, we have \(R^{t}_{v}=}R^{t-1}_{v}+}_{t-1}, v \), in which the Gaussian noise \(_{t-1}\) is injected to every atom. Moreover, the training objective in Equation 3 shows that the denoising networks would always predict a Gaussian noise for all atoms. Neither the forward nor reverse process of DDPM takes into account the substructure of the molecule. Instead, in SubgDiff, a mask vector \(_{t}=[s_{t_{1}},,s_{t_{||}}]^{}\{0,1\}^{| |}\) is introduced to determine which atoms will be added noise at step \(t\). The mask vector \(_{t}\) is sampled from a discrete distribution \(p_{_{t}}()\) to select a subset of the atoms. In molecular graphs, the discrete mask distribution \(p_{_{t}}()\) is equivalent to the subgraph distribution, defined over a predefined sample space \(=\{G^{i}_{}\}_{i=1}^{N}\), where each sample is a connected subgraph extracted from \(G\). Further, the distribution \(p_{_{t}}()\) should keep the selected connected subgraph to cohere with the molecular substructures. Here, we adopt a Torsional-based decomposition method  (Details in Appendix A.2). With the mask vector as latent variables \(_{1:t}\), the state transition of the forward process can be formulated as (Figure 3):

\[R^{t}_{v}=}R^{t-1}_{v}+} _{t-1}&s_{t_{v}}=1\\ R^{t}_{v}&s_{t_{v}}=0,\] (5)

which can be rewritten as \(R^{t}_{v}=}_{t}}R^{t-1}_{v}+}_{t}} _{t-1}\).

The posterior distribution \(q(R^{1:T}|R^{0},s_{1:T})\) can be expressed as matrix form:

\[q(^{1:T}|^{0},_{1:T})=_{t=1}^{T}q(^{t}|^{t-1},_{t});q(^{t}|^{t-1}, _{t})=(^{t};( _{t})}^{t-1},_{t}(_{t})).\] (6)

To simplify the notation, we consider the diffusion on a single node \(v\) and omit the subscript \(v\) in \(R^{t}_{v}\) and \(s_{t_{v}}\) to get the notion \(R^{t}\) and \(s_{t}\). By defining \(_{t}:=1-s_{t}_{t}\), \(_{t}:=_{i=1}^{t}(1-s_{t}_{t})\), the closed form of sampling \(R^{t}\) given \(R^{0}\) is

\[q(R^{t}|R^{0},s_{1:t})=(R^{t};_{t}}R^{0},(1-_{t})).\] (7)

### Reverse process learning

The reverse process is decomposed as follows:

Figure 3: The Markov Chain of SubgDiff is a lazy Markov Chain.

\[p_{,}(R^{0,T},s_{1:T})=p(R^{T})_{t=1}^{T}p_{}(R^{t-1}|R^{ t},s_{t})p_{}(s_{t}|R^{t}),\] (8)

where \(p_{}(R^{t-1}|R^{t},s_{t})\) and \(p_{}(s_{t}|R^{t})\) are both learnable models. In the context of molecular learning, the model can be regarded as first predicting which subgraph \(_{t}\) should be denoised and then using the noise prediction network \(p_{}(R^{t-1}|R^{t},s_{t})\) to denoise the node position in the subgraph.

However, it is tricky to generate a 3D structure by adopting the typical training and sampling method used in Ho et al. . Specifically, following Ho et al. , the reverse process can be optimized by maximizing the variational lower bound (VLB) of \( p(R^{0})\) as follows,

\[ p(R^{0})_{t=1}^{T}_{q(R^{t},s _{t}|R^{0})}[(s_{t}|R^{t})}{q(s_{t})}]}_{}+_{q(R^{t},s_{1:t}|R^{0})}[ p_{}(R^{0}|R^{1}) ]}_{}-\] \[_{q(s_{1:t})}D_{}(q(R^{T}|R^{0}, s_{1:T}) p(R^{T}))}_{}-_{t=2}^{T}_{q(R^{t},s_{1:t}|R^{0})}[D_{}(q(R^{t-1}|R^{t},R^{0},s_{1:t }) p_{}(R^{t-1}|R^{t},s_{t}))]}_{}.\] (9)

Details of the derivation are provided in the Appendix D.2. The subgraph predictor \(p_{}(s_{t}|R^{t})\) in the first term can be parameterized by a node classifier \(s_{}\). For the denoising matching term that is closely related to sampling, by Bayes rule, the posterior \(q(R^{t-1}|R^{t},R^{0},s_{1:t})\) can be written as:

\[q(R^{t-1}|R^{t},R^{0},s_{1:t})(R^{t-1};_{q} (R^{t},R^{0},s_{1:t},_{0}),_{q}^{2}(t)),\] (10) \[_{q}(R^{t},R^{0},s_{1:t},_{0})=}s_{t}}(R^{t}-s_{t}}{s_{t})(1-_{t-1})+_{t}s_{t}}}_{0}),\]

where \(_{q}(t)\) is the standard deviation and \(s_{1:t-1}\) are contained in \(_{t-1}\). Following DDPM and parameterizing \(p_{}(R^{t-1}|R^{t},s_{t})\) as \((R^{t-1};_{q}(R^{t},R^{0},s_{1:t-1},s_{}(,R ^{t},t),_{}(,R^{t},t)),_{q}(t))\), the training objective is

\[_{simple}(,)=_{,^{0},s_{1:t -1}}[\|(_{t})(-_{}(, ^{t},t))\|^{2}+(_{t},_{ }(,^{t},t))],\] (11)

where \((_{t},_{})\) is the binary cross entropy loss, \(\) is the weight used for the trade-off, and \(_{}\) is the subgraph predictor implemented as a node classifier with \(G_{3D}(,^{t})\) as input and shares a molecule encoder with \(_{}\). The BCE loss employed here uses the subgraph selected at time-step \(t\) as the target, thereby explicitly completing the denoising network to capture substructure information from molecules. Eventually, the \(s_{}\) can be used to infer the mask vector \(}_{t}=s_{}(,^{t},t)\) during sampling. Thus, the sampling process is:

\[R^{t-1}=_{q}(R^{t},R^{0},s_{1:t-1},s_{}(,R^{t},t), _{}(,R^{t},t))+_{q}(t)z.\] (12)

However, using Equation 11 and Equation 12 directly for training and sampling faces two issues. First, the inability to access \(s_{1:t-1}\) in \(_{q}\) during the sampling process hinders the step-wise denoising procedure, posing a challenge to the utilization of conventional sampling methods in this context. Inferring \(s_{1:t-1}\) solely from \(R^{t}\) using another model \(p_{}(s_{1:t-1}|R^{t},s_{t})\) is also difficult due to the intricate modulation of noise introduced in \(R^{t}\) through multi-step Gaussian noising. Second, training the subgraph predictor with Equation 11 is challenging. To be specific, the subgraph predictor should be capable of perceiving the sensible noise change between time steps \(t-1\) and \(t\). However, the noise scale \(_{t}\) is relatively small when \(t\) is small, especially if the diffusion step is large (e.g. 1000). As a result, it is difficult to precisely predict the subgraph.

Next, to effectively tackle the above issues, we design two techniques: _expectation state diffusion_ and _\(k\)-step same subgraph diffusion_.

### Expectation state diffusion.

We first devise a new way to calculate the denoising term. To eliminate the effect of mask series \(s_{1:t}\) and improve the training of subgraph prediction loss, we use a new lower bound of the denoising term as follows:

\[_{q(R^{t},s_{1:t}|R^{0})}[D_{}(q(R^{t-1}|R^ {t},R^{0},s_{1:t}) p_{}(R^{t-1}|R^{t},s_{t}))]\] \[_{q(R^{t},s_{1:t}|R^{0})}[D_{}((R^{t-1}|R^{t},R^{0},s_{1:t}) p_{}(R^{t-1}|R^{t},s_{t}))],\]where \((R^{t-1}|R^{t},R^{0},s_{1:t})\) is defined as \(|_{s_{1:t-1}}R^{t-1},R^{0},s_{q})(_{s_{1:t-1}}R^ {t-1}|R^{0})}{q(R^{t}|R^{0},s_{1:t})}\). It is an approximated posterior that only relies on the _expectation_ of \(R_{t}\) and \(s_{t}\). This lower bound defines a new forward process, in which, state \(0\) to state \(t-1\) use the \(_{s_{1:t}}-^{t-1}\) and state \(t\) remains as Equation 6. Assume each node \(v\), \(s_{t_{v}} Bern(p)\) (i.i.d. w.r.t. \(t\)). Formally, we have

\[q(R^{t}|R^{t-1},s_{t})=(R^{t};_{t}} R^{t-1},(s_{t}_{t})),\] (13) \[q(R^{t-1}|R^{0},s_{1:t-1})=(R^{t -1};_{i=1}^{t-1}}R^{0},p^{2}_{i=1}^{t-1}_{j=i+1} ^{t-1}_{j}_{i}I),\] (14)

where \(_{i}:=(p}+1-p)^{2}\) and \(_{t}:=_{i=1}^{t}_{i}\) are general form of \(_{j}\) and \(_{j}\) in DDPM (\(p=1\)), respectively. Intuitively, this process is equivalent to using mean state \(R^{t-1}\) to replace \(R^{t-1}\) during the forward process. This estimation is reasonable since the expectation \(_{s_{1:t-1}}R^{t-1}\) is like a cluster center of \(R^{t-1}\), which can represent \(R^{t-1}\) properly. Thus, the approximated posterior becomes

\[(R^{t-1}|R^{t},R^{0},s_{1:t})(R^{t-1}; _{}(R^{t},R^{0},s_{1:t},_{0}),_{}^{2}(t)),\] (15)

where

\[_{}(R^{t},R^{0},s_{1:t},_{0}):=s_ {t}}}(R^{t}-_{t}}{_{t}+(1-s_{t}_{t})p^{2 }_{i=1}^{t-1}_{i:t}_{i}}}_{0})}\]

\[_{}^{2}(t):=s_{t}_{t}p^{2}_{i=1}^{t-1}_{t-1}}{_{i}}_{i}/(s_{t}_{t}+p^{2}(1-s_{t}_{t}) _{i=1}^{t-1}_{t-1}}{_{i}}_{i}).\]

We parameterize \(p_{}(R^{t-1}|R^{t},s_{t})\) as \((R^{t-1};_{}(R^{t},R^{0},s_{1:t-1},s_{}(,R^{t},t),_{}(R^{t},t)),_{}(t))\), and adopt the same training objective as Equation 11. By employing the sampling method \(R^{t-1}=_{}(R^{t},R^{0},s_{1:t-1},s_{}(,R^{t},t), _{}(R^{t},t))+_{}(t)z\), we observe that the proposed expectation state enables a step-by-step execution of the sampling process. Moreover, using expectation is beneficial to reduce the complexity of \(R^{t}\) for predicting the mask \(s_{t}\) during training. This will improve the denoising network to perceive the substructure when we use the diffusion model for self-supervised learning.

### \(k\)-step same-subgraph diffusion.

To reduce the complexity of the mask series \((_{1},_{2},,_{T})\) and accumulate more noise on the same subgraph for facilitating the convergence of the subgraph prediction loss, we generalize the one-step subgraph diffusion to \(k\)-step same subgraph diffusion (Figure 8 in Appendix), in which the selected subgraph will be continuously diffused \(k\) steps. After that, the difference between the selected and unselected parts will be distinct enough to help the subgraph predictor perceive it. The forward process of \(k\)-step same subgraph diffusion can be written as (\(t>k,k\)):

\[q(R^{t}|R^{t-k},s_{t-k+1:t})=(R^{t},^{t}( 1-s_{t-k+1}_{i})R^{t-k}},_{t}^{k}),\] (16)

where \(_{t}^{k}=(1-_{i=t-k}^{t}(1-s_{t-k+1}_{i})\).

### Training and sampling of SubgDiff

By combining the expectation state and \(k\)-step same-subgraph diffusion, SubgDiff first divides the entire diffusion step \(T\) into \(T/k\) diffusion intervals. In each interval \([ki,k(i+1)]\), the mask vectors \(\{_{j}\}_{j=ki+2}^{k(i+1)}\) are equal to \(_{ki+1}\). SubgDiff then adopts the expectation state at the split time step \(\{ik|i=1,2,\}\) to eliminate the effect of \(\{_{ik+1}|i=1,2,\}\), that is, gets the expectation of \(^{ik}\) at step \(ik\) w.r.t. \(_{ik+1}\). Overall, the diffusion process of SubgDiff is a two-phase diffusion process. In the first phase, the state \(1\) to state \(k|(t-1)/k\) use the expectation state diffusion, while in the second phase, state \(k(|(t-1)/k)+1\) to state \(t\) use the \(k\)-step same subgraph diffusion (see Figure 4). With \(m:=(t-1)/k\), the two phases can be formulated as follows,

**Phase I**: Step \(0 km\): \(_{s_{1:km}}R^{km}=_{m}}R^{0}+p^{m} _{m}}{_{l}}(1-_{i=(l-1)k+1}^{kl}(1-_{i} ))}_{0}\), where \(_{j}=(p^{kj}(1-_{i})+1-p)^{2}}\) is a general forms of \(_{j}\) in Equation 14 (in which case \(k=1\)) and \(_{i}=_{j=1}^{t}_{j}\). In the rest of the paper, \(_{j}\) denotes the general version without a special statement. Actually, \(_{s_{1:km}}R^{km}\) only calculate the expectation w.r.t. random variables \(\{_{ik+1}|i=1,2,\}\).

**Phase II**: Step \(km+1 t\): The phase is a \((t-km)\)-step same mask diffusion. \(R^{t}=^{t}(1-_{i}s_{km+1})}_{s_{1:km}}R^{km }+^{t}(1-_{i}s_{km+1})}_{km}\).

Let \(_{i}=1-_{i}s_{km+1}\), \(_{t}=_{i=1}^{t}_{i}\), and \(_{t}=_{i=1}^{t}(1-_{i})\). We can drive the single-step state transition: \(q(R^{t}|R^{t-1})=(R^{t};}R^{t-1},(1-_{i}) )\) and

\[q(R^{t-1}|R^{0})=(R^{t-1};_{t-1}_{m}}{_{km}}}R^{0}, I);:=_{t-1}}{_{km}}p^{2}_{l=1}^{m}_{m}}{ _{l}}(1-_{kl}}{_{(l-1)k}})+1-_{t-1}}{_{km}}.\] (17)

Then we reuse the training objective in Equation 11 as the objective of SubgDiff:

\[_{simple}(,)=_{,^{0},_{i},}[\|(_{t})(-_{ }(,^{t},t))\|^{2}+(_{t },s_{}(,^{t},t))],\] (18)

where \(^{t}\) is calculated by Equation 17.

**Sampling.** Although the forward process uses the expectation state w.r.t. \(\), we can only update the mask \(}_{t}\) at \(t=ik,i=1,2,\) because sampling only needs to get a subgraph from the distribution in the \(k\)-step interval. Eventually, adopting the \(\) defined in Equation 17, the sampling process (Figure 5) is shown below,

\[R^{t-1}=}}(R^{t}-_{km+1}_{t}}{ {_{t}}+_{km+1}_{t}}_{}(R^{t},t))+_{km+1}_{t}}_{t-1}}{_{ km}}p^{2}_{l=1}^{m}_{m}}{_{l}}(1-_{kl}}{_{(l-1)k}})+1-_{t-1}}{_{ km}}}}{+_{km+1}_{t}}},\] (19)

where \(z(,)\), \(m=(t-1)/k\) and \(_{km+1}=s_{}(,R^{km+k},km+k)\). The subgraph selected by \(}_{km+1}\) will be generated in from the steps \(km+k\) to \(km\). The mask predictor can be viewed as a discriminator of important subgraphs, indicating the optimal subgraph should be recovered in the next \(k\) steps. After one subgraph (substructure) is generated properly, the model can gently fine-tune the other parts of the molecule (c.f. the video in supplementary material). This subgraph diffusion would intuitively increase the robustness and generalization of the generation process, which is also verified by the experiments in Section 5.2. The training and sampling algorithms of SubgDiff are summarized in Algorithm 1 and Algorithm 2.

Figure 4: The forward process of SubgDiff. The state \(0\) to \(km\) uses the expectation state and the mask variables are the same in the interval \([ki,ki+k],i=0,1,...,m-1\). The state \(km+1\) to \(t\) applies the same subgraph diffusion.

Figure 5: The reverse process of SubgDiff. The selected subgraph \(\) is the same in the interval \([ki,min(ki+k,T)]\), \(i=0,...,m\)

``` Input: A molecular graph \(G_{3D}\), \(k\) for same mask diffusion, \(m:=(t-1)/k\)  Sample \(t(1,...,T)\), \((,)\)  Sample \(_{km+1} p_{s_{km+1}}()\)  Sample a subgraph \(^{t} q(^{t}|^{0})\) \(\) Equation 17 \(_{1}=(_{km+1},s_{}(, ^{t},t))\) \(\) Subgraph prediction loss \(_{2}=\|(_{km+1})(-_{}( ,^{t},t))\|^{2}\) \(\) Denoising loss \((_{t,^{0},_{t,}} _{1}+_{2})\) \(\) Optimize parameters \(,\) ```

**Algorithm 1**Training SubgDiff

``` \(k\) is the same as training, for \(k\)-step same-subgraph diffusion;  Sample \(^{T}(,)\) \(\) Random noise initialization for\(t\) = \(T\) to \(l\)do \((,)\) if \(t>1\), else \(=\)\(\) Random noise If\(t\%k==0\) or \(t==T\): \(} s_{}(,^{t},t)\)\(\) Subgraph prediction \(_{}(,^{t},t)\)\(\) Posterior \(^{t-1}\) Equation 19 \(\) sampling  end for return\(^{0}\) ```

**Algorithm 2**Sampling from SubgDiff

## 5 Experiments

We conduct experiments to address the following two questions: 1) Can substructures improve the representation ability of the denoising network when using diffusion as self-supervised learning? 2) How does the proposed subgraph diffusion affect the generative ability of the diffusion models? For the first question, we employ SubgDiff as a denoising pretraining task and evaluate the performances of the denoising network on various downstream tasks. For the second one, we compare SubgDiff with the vanilla diffusion model GeoDiff  on the task of molecular conformation generation.

### SubgDiff improves molecular representation learning

To verify the introduced substructure in the diffusion can enhance the denoising network for representation learning, we pretrain with SubgDiff objective and finetune on various downstream tasks.

**Dataset and settings.** For pretraining, we follow  and use PCQM4Mv2 dataset . It's a sub-dataset of PubChemQC  with 3.4 million molecules with 3D geometric conformations. We use various molecular property prediction datasets as downstream tasks. For tasks with 3D conformations, we consider the dataset MD17 and follow the literature [35; 36; 24] of using 1K for training and 1K for validation, while the test set (from 48K to 991K) is much larger. For downstream tasks with only 2D molecule graphs, we use eight molecular property prediction tasks from MoleculeNet .

**Pretraining framework.** To explore the potential of the proposed method for representation learning, we consider MoleculeSDE , a SOTA pretraining framework, to be the training backbone, where SubgDiff is used for the \(2D 3D\) model and the mask operation is extended to the node feature and graph adjacency for the \(3D 2D\) model. The details can be found in Appendix A.4.2.

**Baselines.** For 3D tasks, we incorporate two self-supervised methods [Type Prediction, Angle Prediction], and three contrastive methods [InfoNCE  and EBM-NCE  and 3D InfoGraph ]. Two denoising baselines are also included [GeoSSL , Denoising  and MoleculeSDE]. For 2D tasks, the baselines are AttrMask, ContexPred , InfoGraph , MolCLR , 3D InfoMax , GraphMVP  and MoleculeSDE.

**Results.** As shown in Table 1 and Table 2, SubgDiff outperforms MoleculeSDE in both 2D and 3D downstream tasks. Particularly, the significant improvement on the MD17 force prediction task, which is closely related to 3D molecular conformation, demonstrates that the introduced subgraph diffusion helps the perception of molecular 3D structure in the denoising network during pretraining. Further, SubgDiff achieves SOTA performance compared to all the baselines. This also indicates that the proposed SubgDiff objective is promising for molecular representation learning due to the involvement of the knowledge of substructures during training. More results on the QM9 dataset  on the quantum mechanics property prediction can be found in Appendix A.4.3.

### SubgDiff benefits conformation generation

We have proposed a new diffusion model to enhance molecular representing learning, where the base diffusion model (GeoDiff) is initially designed for conformation generation. To evaluate the effects of SubgDiff on the generative ability of diffusion models, we assess its generation performance and generalization ability. Following prior works , we utilize the GEOM-QM9  and GEOM-Drugs  datasets. The detailed description of the dataset splitting, metrics and model architecture can be found in Appendix A.5.

**Conformation generation.** The comparison with GeoDiff on the GEOM-QM9 dataset is reported in Table 3. From the results, it is easy to see that SubgDiff significantly outperforms the GeoDiff baseline on both metrics (COV-R and MAT-R) across different sampling steps. It indicates that by training with the substructure information, SubgDiff has a positive effect on the conformation generation task. Moreover, SubgDiff with 500 steps achieves much better performance than GeoDiff with 5000 steps on 5 out of 8 metrics, which implies our method can accelerate the sampling efficiency (10x).

   Pretraining & Aspirin \(\) & Benzene \(\) & Ethanol \(\) & Mahonaldehyde \(\) & Naphthalene \(\) & Salicylic \(\) & Toluene \(\) & Uracil \(\) \\  â€“ (random init) & 1.203 & 0.380 & 0.386 & 0.794 & 0.587 & 0.826 & 0.568 & 0.773 \\ Type Prediction & 1.383 & 0.402 & 0.450 & 0.879 & 0.622 & 1.028 & 0.662 & 0.840 \\ Angle Prediction & 1.542 & 0.447 & 0.669 & 1.022 & 0.680 & 1.032 & 0.623 & 0.768 \\
3D InfoGraph & 1.610 & 0.415 & 0.560 & 0.900 & 0.788 & 1.278 & 0.768 & 1.110 \\ InfoNCE & 1.132 & 0.395 & 0.466 & 0.888 & 0.542 & 0.831 & 0.554 & 0.664 \\ EBM-NCE & 1.251 & 0.373 & 0.457 & 0.829 & 0.512 & 0.990 & 0.560 & 0.742 \\ Denoising & 1.364 & 0.391 & 0.432 & 0.830 & 0.599 & 0.817 & 0.628 & 0.607 \\ GeoSSL & 1.107 & 0.360 & 0.357 & 0.737 & 0.568 & 0.902 & 0.484 & 0.502 \\ MoleculeSDE (VE) & 1.112 & 0.304 & 0.282 & 0.520 & 0.455 & 0.725 & 0.515 & 0.447 \\ MoleculeSDE (VP) & 1.244 & 0.315 & 0.338 & 0.488 & 0.432 & 0.712 & 0.478 & 0.468 \\  Ours & **0.880** & **0.252** & **0.258** & **0.459** & **0.325** & **0.572** & **0.362** & **0.420** \\   

Table 1: Results (mean absolute error) on MD17 **force** prediction. The best and second best results are marked in bold and underlined.

   Pre-training & BBBP \(\) & Tox21 \(\) & ToxCast \(\) & Sider \(\) & ClinTox \(\) & MUV \(\) & HIV \(\) & Bace \(\) & Avg \(\) \\  â€“ (random init) & 68.1\(\)0.59 & 75.3\(\)0.22 & 62.1\(\)0.19 & 57.0\(\)1.33 & 83.7\(\)2.93 & 74.6\(\)2.35 & 75.2\(\)0.70 & 76.7\(\)2.51 & 71.60 \\ AttMask & 65.0\(\)2.36 & 74.8\(\)0.25 & 62.9\(\)0.11 & 61.2\(\)0.12 & 87.7\(\)1.19 & 73.4\(\)2.02 & 76.8\(\)0.53 & 79.7\(\)0.33 & 72.68 \\ ContextPred & 65.7\(\)0.62 & 74.2\(\)0.06 & 62.5\(\)0.31 & 62.2\(\)0.59 & 77.2\(\)0.88 & 75.3\(\)1.57 & 77.1\(\)0.86 & 76.0\(\)2.08 & 71.28 \\ InfoGraph & 67.5\(\)0.11 & 73.2\(\)0.43 & 63.7\(\)0.50 & 59.9\(\)0.30 & 76.5\(\)1.07 & 74.1\(\)0.74 & 75.1\(\)0.99 & 77.8\(\)0.88 & 70.96 \\ MolCLR & 66.6\(\)1.89 & 73.0\(\)0.16 & 62.9\(\)0.38 & 57.5\(\)1.77 & 86.1\(\)0.95 & 75.2\(\)3.28 & 76.2\(\)1.51 & 71.5\(\)3.17 & 70.79 \\
3D InfoMax & 68.3\(\)1.12 & 76.1\(\)0.18 & 64.8\(\)0.25 & 60.6\(\)0.78 & 79.3\(\)4.94 & 74.4\(\)2.45 & 75.9\(\)0.59 & 79.7\(\)1.54 & 72.47 \\ GraphMVP & 69.4\(\)0.21 & 76.2\(\)0.38 & 64.5\(\)0.20 & 60.5\(\)0.25 & 86.5\(\)1.70 & 76.2\(\)2.28 & 76.2\(\)0.81 & 79.8\(\)0.74 & 73.66 \\ MoleculeSDE(VE) & 68.3\(\)0.25 & 76.9\(\)0.23 & 64.7\(\)0.06 & 60.2\(\)0.29 & 80.8\(\)2.53 & 76.8\(\)1.71 & 77.0\(\)1.68 & 79.9\(\)1.76 & 73.15 \\ MoleculeSDE(VP) & 70.1\(\)1.35 & 77.0\(\)0.12 & 64.0\(\)0.07 & 60.8\(\)1.04 & 82.6\(\)3.64 & 76.6\(\)3.25 & 77.3\(\)1.31 & 81.4\(\)0.66 & 73.73 \\  Ours & **70.2\(\)2.23** & **77.2\(\)0.39** & **65.0\(\)0.48** & **62.2\(\)0.97** & **88.2\(\)1.57** & **77.3\(\)1.17** & **77.6\(\)0.51** & **82.1\(\)0.96** & **74.85** \\   

Table 2: Results for MoleculeNet (with 2D topology only). We report the mean (and standard deviation) ROC-AUC of three random seeds with scaffold splitting for each task. The backbone is GIN. The best and second best results are marked bold and underlined, respectively.

    & & &  &  &  &  \\ Models & Timesteps & Sampling method & Mean & Median & Mean & Median & Mean & Median & Mean & Median \\  GeoDiff & 5000 & DDPM & 80.36 & 83.82 & 0.2820 & 0.2799 & 53.66 & 50.85 & 0.6673 & 0.4214 \\ SubgDiff & 5000 & DDPM (ours) & 90.91 & 95.59 & 0.2460 & 0.2351 & 50.16 & 48.01 & 0.6114 & 0.491 \\  GeoDiff & 500 & DDPM & 80.20 & 83.59 & 0.3617 & 0.3412 & 45.49 & 45.45 & 1.1518 & 0.5087 \\ SubgDiff & 500 & DDPM (ours) & 89.78 & 94.17 & 0.2417 & 0.2449 & 50.03 & 48.31 & 0.5571 & 0.4921 \\  GeoDiff & 200 & DDPM &

**Domain generalization.** To further illustrate the benefits of SubgDiff, we design two cross-domain tasks: (1) Training on QM9 (small molecular with up to 9 heavy atoms) and testing on Drugs (medium-sized organic compounds); (2) Training on Drugs and testing on QM9. The results (Table 4 and Appendix Table 13) show that SubgDiff consistently outperforms GeoDiff and other models trained on the in-domain dataset, demonstrating the introduced sub-structure effectively enhances the robustness and generalization of the diffusion model.

## 6 Conclusion

We present a novel diffusion model SubgDiff, which involves the subgraph constraint in the diffusion model by introducing a mask vector to the forward process. Benefiting from the expectation state and \(k\)-step same-subgraph diffusion, SubgDiff effectively boosts the perception of molecular substructure in the denoising network, thereby achieving state-of-the-art performance at various downstream property prediction tasks. There are several exciting avenues for future work. The mask distribution can be made flexible such that more chemical prior knowledge may be incorporated into efficient subgraph sampling. Besides, the proposed SubgDiff can be generalized to proteins such that the denoising network can learn meaningful secondary structures.