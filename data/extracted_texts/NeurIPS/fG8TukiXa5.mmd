# How Transformers Utilize Multi-Head Attention in In-Context Learning?

A Case Study on Sparse Linear Regression

Xingwu Chen

The University of Hong Kong

xingwu@connect.hku.hk

&Lei Zhao

University of Pennsylvania

leizhao7@wharton.upenn.edu

Equal contribution.

&Difan Zou

The University of Hong Kong

dzou@cs.hku.hk

###### Abstract

Despite the remarkable success of transformer-based models in various real-world tasks, their underlying mechanisms remain poorly understood. Recent studies have suggested that transformers can implement gradient descent as an in-context learner for linear regression problems and have developed various theoretical analyses accordingly. However, these works mostly focus on the expressive power of transformers by designing specific parameter constructions, lacking a comprehensive understanding of their inherent working mechanisms post-training. In this study, we consider a sparse linear regression problem and investigate how a trained multi-head transformer performs in-context learning. We experimentally discover that the utilization of multi-heads exhibits different patterns across layers: multiple heads are utilized and essential in the first layer, while usually one single head is dominantly utilized for subsequent layers. We provide a theoretical rationale for this observation: the first layer undertakes data preprocessing on the context examples, and the following layers execute simple optimization steps based on the preprocessed context. Moreover, we prove that such a preprocess-then-optimize algorithm can outperform naive gradient descent and ridge regression algorithms, which is also supported by our further experiments. Our findings offer insights into the benefits of multi-head attention and contribute to understanding the more intricate mechanisms hidden within trained transformers.

## 1 Introduction

Transformers  have emerged as a dominant force in machine learning, particularly in natural language processing. Transformer-based large language models such as Llama  and the GPT family , equipped with multiple heads and layers, showcasing exceptional learning and reasoning capabilities. One of the fundamental capabilities is in-context learning , i.e., transformer can solve new tasks after prompting with a few context examples, without any further parameter training. Understanding their working mechanisms and developing reasonable theoretical explanations for their performance is vital and has gathered considerable research attention.

Numerous studies have been conducted to explore the expressive power of transformers, aiming to showcase their ability to tackle challenging tasks related to memorization , reasoning , function approximation , causal relationship , and simulating complex circuits . These endeavors typically aim to enhance our understanding of the capabilities and limitations of transformers when configured with varying numbers of heads and layers. However, it's important to note that the findings regarding expressive power and complexity may not directly translate into explanations or insights into the behavior of trained transformer models in practical applications.

To perform a deeper understanding of the learning ability of transformer, a line of recent studies has been made to study the in-context learning performance of transformer by connecting it to certain iterative optimization algorithms . These investigations have primarily focused on linear regression tasks with a Gaussian prior, demonstrating that a transformer with \(L\) layers can mimic \(L\) steps of gradient descent on the loss defined by contextual examples both theoretically and empirically. These observations have immediately triggered a series of further theoretical research, revealing that multi-layer and multi-head transformers can emulate a broad range of algorithms, including proximal gradient descent , preconditioned gradient descent , functional gradient descent , Newton methods , and ridge regression . However, these theoretical works are mostly built by designing specific parameter constructions, which may not reflect the key mechanism of trained transformers in practice. The precise roles of different transformer modules, especially for the various attention layers and heads, remain largely opaque, even within the context of linear regression tasks.

To this end, we take a deeper exploration regarding the working mechanism of transformer by investigating how transformers utilize multi-heads, at different layers, to perform the in-context learning. In particular, we consider the sparse linear regression task, i.e., the data is generated from a noisy linear model with sparse ground truth \(^{*}^{d}\) with \(\|^{*}\|_{0} s d\), and train a transformer model with multiple layers and heads. While a line of works also investigates this problem , understanding the key mechanisms behind trained transformers always requires more experimental and theoretical insights. Consequently, we empirically assess the importance of different heads at varying layers by selectively masking individual heads and evaluating the resulting performance degradation. Surprisingly, our observations reveal distinct utilization patterns of multi-head attention across layers of a trained transformer: _in the first attention layer, all heads appear to be significant; in the subsequent layers, only one head appears to be significant._ This phenomenon suggests that (1) employing multiple heads, particularly in the first layer, plays a crucial role in enhancing in-context learning performance; and (2) the working mechanisms of the transformer may be different for the first and subsequent layers.

Based on the experimental findings, we conjecture that muti-layer transformer may exhibit a preprocess-then-optimize algorithm on the context examples. Specifically, transformers utilize all heads in the initial layer for data preprocessing and subsequently employ a single head in subsequent layers to execute simple iterative optimization algorithms, such as gradient descent, on the preprocessed data. We then develop the theory to demonstrate that such an algorithm can be indeed implemented by a transformer with multiple heads in the first layer and one head in the remaining layers, and can achieve substantially lower excess risk than gradient descent and ridge regression (without data preprocessing). The main contributions of this paper are highlighted as follows:

* We empirically investigate the role of different heads within transformers in performing in-context learning. We train a transformer model based on the data points generated by the noisy sparse linear model. Then, we reveal a distinct utilization pattern of multi-head attention across layers: while the first attention layer tended to evenly utilize all heads, subsequent layers predominantly relied on a single head. This observation suggests that the working mechanisms of multi-head transformers may vary between the first and subsequent layers.
* Building upon our empirical findings, we proposed a possible working mechanism for multi-head transformers. Specifically, we hypothesized that transformers use the first layer for data preprocessing on in-context examples, followed by subsequent layers performing iterative optimizations on the preprocessed data. To substantiate this hypothesis, we theoretically demonstrated that, by constructing a transformer with mild size, such a preprocess-then-optimize algorithm can be implemented using multiple heads in the first layers and a single head in subsequent layers.

* We further validated our proposed mechanism by comparing the performance of the preprocess-then-optimize algorithm with multi-step gradient descent and ridge regression solution, which can be implemented by the single-head transformers. We prove that the preprocess-then-optimize algorithm can achieve lower excess risk compared to these traditional methods, which is also verified by our numerical experiments. This aligns with our empirical findings, which indicated that multi-head transformers outperformed ridge regression in terms of excess risk.
* To further validate our theoretical framework, we conducted additional experiments. Specifically, we performed probing on the output of the first layer of the transformer and demonstrated that representations generated by transformers with more heads led to lower excess risk after gradient descent. These experiments provided further support for our explanation on the working mechanism of transformers.

## 2 Preliminaries

Sparse Linear Regression.We consider sparse linear models where \((,y)=_{^{}}^{}\) is sampled as \((,)\), \(y=^{},+0,^{2} \), where the \(\) is a diagonal matrix and ground truth \(^{}^{d}\) satisfies \(\|^{}\|_{0} s\). Then, we define the population risk of a parameter \(\) as follows:

\[L():=_{(,y)}( ,-y)^{2}.\]

Moreover, we are interested in the excess risk, i.e., the gap between the population risk achieved by \(\) and the optimal one:

\[():=L()-_{}L().\]

Multi-head Transformers.Transformers are a type of neural network with stacked attention and multi-layer perceptron (MLP) blocks. In each layer, the transformer first utilizes multi-head attention Attn to process the input sequence (or hidden states) \(=[_{1},_{2},,_{m}] ^{d_{} m}\). It computes \(h\) different queries, keys, and values, and then concatenates the output of each head:

\[(,_{})=+[_{1}(_{1}^{}_{1}), ,_{h}(_{h}^{}_{h})],\]

where \(_{i}=_{V_{i}},_{i}=_{Q_{i}} ,_{i}=_{V_{i}}\) and \(_{}=_{V_{i}},_{K_{i}},_ {Q_{i}}^{d_{}/h d_{}}}_{i=1}^ {h}\) are learnable parameters. The MLP then applies a nonlinear element-wise operation:

\[(,_{})=_{1}( _{2}(,_{})),\] (2.1)

where \(_{2}=\{_{1},_{2}\}\) denotes the parameters of MLP. We remark that here some modules, such as layernorm and bias, are ignored for simplicity.

Linear Attention-only TransformersTo perform an tractable theoretical investigation on the role of multi-head in the attention layer, we make further simplification on the transformer model by considering linear attention-only transformers. These simplifications are widely adopted in many recent works to study the behavior of transformer models [47; 53; 32; 3]. In particular, the \(i\)-th layer \(_{i}\) performs the following update on the input sequence (or hidden state) \(^{(i-1)}\) as follows:

\[^{(i)}=_{i}(^{(i-1)})=_{1} ^{(i-1)}+[\{_{i}_{i}^{} _{i}\}_{i=1}^{h}],:= _{n}&\\ &^{m m},\] (2.2)

where \(\{_{V_{i}},_{K_{i}},_{Q_{i}}^{}}{h} d_{}}\}_{i=1}^{h}\) and \(_{1}^{d_{} d_{}}\) are learnable parameters, note that as we ignore the ReLU activation in Eq.(2.1), so we merge the parameter \(_{1}\) and \(_{2}\) into one matrix \(_{1}\). Besides, the mask matrix \(\) is included in the attention to constrain the model focus the first \(n\) in-context examples rather than the subsequent \(m-n\) queries [3; 32; 54]. To adapt the transformer for solving sparse linear regression problems, we introduce additional linear layers \(_{E}^{(d+1) d_{}}\) and \(_{O}^{d_{} 1}\) for input embedding and output projection, respectively. Mathematically, let \(\) denotes the input sequences with \(n\) in-context example followed by \(q\) queries,

\[=_{1}&_{2}&&_{n} &_{n+1}&&_{n+q}\\ y_{1}&y_{2}&&y_{n}&0&&0.\] (2.3)Then model processes the input sequence \(\), resulting in the output \(}^{1(n+q)}\):

\[}=_{O}_{L}_{1}_{E}(),\]

here, \(L\) is the layer number of the transformer, and \(_{i+n}\) is the prediction value for the query \(_{i+n}\). During training, we set \(q>1\) for efficiency, and for inference and theoretical analysis, we set \(q=1\) and define the in-context learning excess risk \(_{}\) as:

\[_{}:=_{(,y)}(_{n+1}-y_{n+1})^{2}-^{2}.\]

Notations.For two functions \(f(x) 0\) and \(g(x) 0\) defined on the positive real numbers (\(x>0\)), we write \(f(x) g(x)\) if there exists two constants \(c,x_{0}>0\) such that \( x x_{0}\), \(f(x) c g(x)\); we write \(f(x) g(x)\) if \(g(x) f(x)\); we write \(f(x) g(x)\) if \(f(x) g(x)\) and \(g(x) f(x)\). If \(f(x) g(x)\), we can write \(f(x)\) as \(O(g(x))\). We can also write write \(f(x)\) as \((g(x))\) if there exists a constant \(k>0\) such that \(f(x) g(x)^{k}(x)\).

## 3 Experimental Insights into Multi-head Attention for In-context Learning

While previous work has demonstrated the in-context learning ability for sparse linear regression [20; 8], the hidden mechanism behind the trained transformer for solving this problem remains unclear. To this end, we design a series of experiments, utilizing techniques like probing  and pruning  to help us gain initial insights into how the trained transformer utilizes multi-head attention for this problem. For all experiments in Sections 3 and 6, we choose an encoder-based architecture as the backbone (see Figure 0(a)), set the hidden dimension \(d_{}\) to 256, and use the input sequence format shown in Eq.(2.3), where \(d=16\), \(s=4\), \((,)\), with varying noise levels, layers, and heads, Additional experimental details can be found in Appendix B. The experiments we designed are as follows:

ICL with Varying Heads:First, based on the experiment results by , we further investigate the performance of transformers in solving the in-context sparse linear regression problem with varying attention heads. An example can be found in Figure 0(b), where we display the excess risk for different models when using different numbers of in-context examples. We can observe that given few-shot in-context examples, transformers can outperform OLS and ridge. Moreover, we can also clearly observe the benefit of using multiple heads, which leads to lower excess risk when increasing

Figure 1: Experimental Insights into Multi-head Attention for In-context Learning

the number of heads. This **highlights the importance of multi-head attention in transformer to perform in-context learning**.

Heads Assessment:Based on Eq.(2.2), we know that the \(j\)-th head at the \(i\)-th layer corresponds to the subspace of the intermediate output from \((j-1) d_{}/h\) to \(j d_{}/h-1\). To assess the importance of each attention head, we can mask the particular head by zeroing out the corresponding output entries, while keeping other dimensions unchanged. Then, let \((i,j)\) be the layer and head indices, we evaluate the risk change before and after head masking, denoted by \(_{(i,j)}\). Then we normalize the risk changes in the same layer to evaluate their relative importance:

\[_{i,j}=_{(i,j)}}{_{k=1}^{h} _{(i,k)}}.\] (3.1)

An example can be found in Figure 0(c). We can observe that in the first layer, no head distinctly outweighs the others, while in the subsequent layers, there always exists a head that exhibits higher importance than others. This gives us insight that **in the first attention layer, all heads appear to be significant, while in the subsequent layers, only one head appears to be significant**.

Pruning and Probing:To further validate our findings in the previous experiments, we prune the trained model by (1) retaining all heads in the first layer; and (2) only keeping the most important head and zeroing out others for the subsequent layers. Then the pruned model, referred to as the "pruned transformer", will be fine-tuned with with the same training data. We then use linear probes  to evaluate the prediction performance for different layers. An example can be found in Figure 0(d), we can find that the "pruned transformer" and the original model exhibit almost the same performance for each layer. Additionally, compared to the model with single-head attention, we observe that the probing result is largely different between single-head transformers and the "pruned transformers", the latter has better performances compared to the former. Noting that the main difference between them is the number of heads in the first layer (subsequent layers have the same structure), it can be deduced that **the working mechanisms of the multi-head transformer may be different for the first and subsequent layers**.

## 4 Potential Mechanism Behind Trained transformer

Based on the experimental insights from Section 3, we found that all heads in the first layer of the trained transformer are crucial, while in subsequent layers, only one head plays a significant role. Furthermore, by checking the result for probing and pruning, we can find that the working mechanisms of the transformer may be different for the first and subsequent layers. To this end, we hypothesize that the multi-layer transformer may implement a preprocess-then-optimize to perform the in-context learning, i.e., the transformer first performs preprocessing on the in-context examples using the first layer and then implements multi-step iterative optimization algorithms on the preprocessed in-context examples using the subsequent layers.

We note that  adapts a similar two-phase idea to explain how transformer learning specific functions in context, in their constructed transformers, the first few layers utilize MLPs to compute an appropriate representation for each entry, while the subsequent layers utilize the attention module to implement gradient descent over the context. We highlight that our algorithm mainly focus on utilizing multihead attention, and it aligns well with the our experimental observation and intuition. The details of our algorithm are as follows:

### Preprocessing on In-context Examples

First, as the multihead attention is designed to facilitate to model to capture features from different representation subspaces , we abstract the algorithm implementation by the first layer of the transformers as a preprocessing procedure. In general, for the sparse linear regression, a possible data preprocessing method is to perform reweighting of the data features by emphasizing the features that correspond to the nonzero entries of the ground truth \(^{*}\) and disregard the remaining features. In the idealized case, if we know the nonzero support of \(^{*}\), we can trivially zero out the date features of \(\) on the complement of the nonzero support, as a data preprocessing procedure, and perform projected gradient descent to obtain the optimal solution.

In general, the nonzero support of \(^{*}\) is intractable to the learner, so that one cannot perform idealized masking-related data preprocessing. However, one can still perform estimations on the importance of data features by examining their correlation with the target. In particular, note that we have \(y=^{*},+_{i}=_{i=1}^{d}w_{i}^{*}x_{i}+ _{i}\), implying that \(r_{i}:=[x_{i}y]=[_{i=1}^{d}w_{i}^{*}x_{i}:x_{i}]+ [ x_{i}]=w_{i}^{*}[x_{i}^{2}]\) if considering independent data features. Then it is clear that such a correlation between the feature and label will be nonzero only when \(|w_{i}^{*}| 0\). Therefore, instead of knowing the nonzero support of \(^{*}\), we can instead calculate such a correlation to perform reweighting on the data features. Noting that the transformer is provided with \(n\) in-context examples \(\{(_{i},y_{i})\}_{i=1}^{n}\), such correlations can be estimated accordingly: \(_{j}=_{i=1}^{n}x_{ij}y_{i}\), which will be further used to perform the data preprocessing on the in-context examples. We summarize this procedure in Alg. 1.

```
1:Input : Sequence with \(\{(_{i},y_{i})\}_{i=1}^{n},\{(_{i},0)\}_{i=n+1}^{n+q}\) as in-context examples/queries.
2:for\(k=1,,n\)do
3: Compute \(}_{k}\) by \(}_{k}=}_{k}\), where \(}=\{_{1},_{2},, _{d}\}\), where \(_{j}\) is given by \[_{j}=_{i=1}^{n}x_{ij}y_{i}.\] (4.1)
4:endfor
5:Output : Sequence with the preprocessed in-context examples/queries \(\{(}_{i},y_{i})\}_{i=1}^{n},\{(}_{ i},0)\}_{i=n+1}^{n+q}\). ```

**Algorithm 1** Data preprocessing for in-context examples

The preprocessing procedure aligns well with the structure of a multi-head attention layer with linear attention, which motivates our theoretical construction of the desired transformer. In particular, each head of the attention layer can be conceptualized as executing specific operations on a distinct subset of data entries. Then, the linear query-key calculation, represented as \((_{K_{i}})^{}_{Q_{i}}\), where \(=\) denotes the input sequence embedding matrix, effectively estimates correlations between the \(i\)-th subset of data entries and the corresponding label \(y_{i}\). Here, \(_{K_{i}}\) and \(_{Q_{i}}\) selectively extract entries from the \(i\)-th subset of features and the label, respectively, akin to an "entries selection" process. Furthermore, when combined with the value calculation \(_{V_{i}}\), each head of the attention layer conducts correlation calculations for the \(i\)-th subset of features and subsequently employs them to reweight the original features within the same subset. Consequently, by stacking the outputs of multiple heads, all data features can be reweighted accordingly, which matches the design of the proposed preprocessing procedure in Alg. 1. We formally prove this in the following theorem.

**Proposition 4.1** (Single-layer multi-head transformer implements Alg. 1).: _There exists a single-layer transformer function \(_{1}\), with \(d\) heads and \(d_{}=3d\) hidden dimension, together with an input embedding layer with weight \(_{E}^{d_{} d}\), that can implement Alg. 1. Let \(\) be the input sequence defined in Eq.(2.3) and \(}_{i}=}\) be the preprocessed features defined in Alg. 1, it holds that_

\[^{(1)}:=_{1}_{E}()= }_{1}&}_{2}&&}_{n}&}_{n+1}&&}_{n +q}\\ y_{1}&y_{2}&&y_{n}&0&&0\\ &&&&&&,\] (4.2)

_where \(\) in third row implies arbitrary values._

### Optimizing Over Preprocessed In-Context Examples

Based on the experimental results, we observe that the subsequent layers of transformers dominantly rely on one single head, suggesting their different but potentially simpler behavior compared to the first one. Motivated by a series of recent work [47; 15; 53; 3] that reveal the connection between gradient descent steps and multi-layer single-head transformer in the in-context learning tasks, we conjecture that the subsequent layers also implement iterative optimization algorithms such as gradient descent on the (preprocessed) in-context examples.

To maintain clarity in our construction and explanation, in each layer, we use a linear projection \(_{1}^{(i)}\) to rearrange the dimensions of the sequence processed by the multi-head attention, resulting in the hidden state \(^{(i)}\) of each layer. We refer to the first \(d\) rows of the input data as \(\), and the \((d+1)\)-th row as the corresponding \(y\). For example, in Eq.(4.2), we take the first \(d\) rows, together with the \((d+1)\)-th row, as the input data entry \(\{}_{i},y_{i}\}_{i=1}^{n+1}\). Then, the following proposition shows that the subsequent layers of transformer can implement multi-step gradient descent on the preprocessed in-context examples \(\{(}_{i},y_{i})\}_{i=1,,n}\).

**Proposition 4.2** (Subsequent single-head transformer implements multi-step GD).: _There exists a transformer with \(k\) layers, \(l\) head, \(d_{}}=3d\), let \(_{n+1}^{}\) be the prediction representation of the \(\)-th layer, then it holds that \(_{(n+1)}^{}=_{}}^{}, }_{n+1}\), where \(}_{n+1}=}_{n+1}\) denotes the preprocessed data feature, \(_{}}^{}\) is defined as \(_{}}^{0}=0\) and as follows for \(=0,,k-1\):_

\[_{}}^{+1}=_{}}^{}- (_{}}^{}), ()=_{i=1}^{n}(y_{i}- ,}_{i})^{2}.\] (4.3)

The proof of Propositions 4.1 and 4.2 can be found in Appendix C, combining these two propositions, we show that the multi-layer transformer with multiple heads in the first layer and one head in the subsequent layers can implement the proposed preprocess-then-optimization algorithm. In the next section, we will establish theories to demonstrate that such an algorithm can indeed achieve smaller excess risk than standard gradient descent and ridge regression solutions of the sparse linear regression problem.

## 5 Excess Risk of the Preprocess-then-optimize Algorithm

In this section, we will develop the theory to demonstrate the improved performance of the preprocess-then-optimize algorithm compared to the gradient descent algorithm on the raw inputs. The proof for Theorem 5.1, 5.2, and 5.3 can be found in Appendix D, E, and F, respectively.

We first denote \(}_{}}^{t}\) as the estimator obtained by \(t\)-step GD on \(\{(}_{i},y_{i})\}_{i=1}^{n}\), which can be viewed as the solution generated by the \(t+1\)-layer transformer based on our discussion in Section 4, and \(_{}}^{t}\) as the estimator obtained by \(t\)-step GD on \(\{(_{i},y_{i})\}_{i=1}^{n}\). Before presenting our main theorem, we first need to redefine the excess risk of GD on \(\{(}_{i},y_{i})\}_{i=1}^{n}\). Note that in our algorithm, the learned predictor takes the form \(},}_{}}^{t}\). Consequently, the population risk of a parameter \(}_{}}^{t}\) is naturally defined as \((}_{}}^{t}):= _{(,y)}(} ,}_{}}^{t}-y)^{2}\), and the excess risk is then defined as \(():=()-_{}()\)2. Next, we provide the upper bound of the excess risk for \((}_{}}^{t})\) and \((_{}}^{t})\) respectively.

**Theorem 5.1**.: _Denote \(:=\{i:w_{i}^{*} 0\}\) and \(=}\{r_{1},,r_{d}\}\), where \(r_{j}=_{i=1}^{d}w_{i}^{*}_{ij}\). Suppose that there exist a \(>0\) such that \(_{i}|r_{i}|\), \(\|\|_{2},\|\|_{2},\|^{*}\|_{2} O(1)\) and \(n 1/^{2} t^{2}s^{2/3}()+ ()( (d/))\). Then set \( 1/\|\|_{2}\) and_

\[ t( )(d/)}{n}+s ()^{2}(d/)}{n^{2}}^{-1/2},\]

_it holds that_

\[}_{}}^{t} ()(d/)}{n}}+s( )^{2}(d/)}{n^{2}},\]

_with probability at least \(1-\)._

Theorem 5.1 provides an upper bound on the excess risk achieved by the preprocess-then-optimize algorithm, where we tuned learning rate \(\) to balance the bias and variance error. Then, it can be seen that the risk bound is valid if \(()/n 0\) and \(()s/n^{2} 0\) when \(n\). This can be readily satisfied if we have \(\|^{*}\|_{2}\) and \(()\) be bounded by some reasonable quantities that are independent of the sample size \(n\), which are the common assumptions made in many prior works [58; 57; 9]. Besides, it can be also seen that the excess risk bound explicitly depends on the sparsity parameter \(s\) and lower sparsity implies better performance. This implies the ability of the proposed preprocess-then-optimize for discovering and leveraging the nice sparse structure of the ground truth.

As a comparison, the following theorem states the excess risk bound for the standard gradient descents on the raw features. To make a fair comparison, we consider using the same number of steps but allow the step size to be tuned separately.

**Theorem 5.2**.: _Suppose that \(,^{}_{2}  O(1)\) and \(n t^{2}(()+(1/))\). When \( 1/_{2}\) and \( t(()(d/ )}{n})^{-1/2}\), it holds that_

\[_{}^{t} t()(d/)}{n}},\]

_with probability at least \(1-\)._

We are now able to make a rough comparison between the excess risk bounds in Theorems 5.1 and 5.2. Then, it is clear that \((}_{}^{t})( _{}^{t})\) requires \(()/^{2} ()\) and \(s/(n^{2}^{2}) 1/n\). Specifically, we can consider the case that \(\) to be a diagonal matrix, assume \(w_{i}^{}\{-1/,1/\}\) has a restricted uniform prior for \(i\) and \(_{i}_{ii} 1/\) for some constant \(>1\), we can get \()}\), thus \(()/^{2}^{2} _{i:w_{i}^{} 0}_{ii}\) and \(s/(n^{2}^{2})^{2}s^{2}/n^{2}\). Note that \(||=s d\), then if the covariance matrix \(\) has a flat eigenspectrum such that \(_{i}_{ii}_{i[d]}_{ii}= ()\), we have \(()/^{2} {Tr}()\) and \(s/(n^{2}^{2})^{2}s^{2}/n\) if \(s=o(\{d,\})\). This suggests that the preprocess-then-optimization algorithm can outperform the standard gradient descent for solving a sparse linear regression problem with \(s=o\{d,\}\).

To make a more rigorous comparison, we next consider the example where \(x_{i}}}{{}}(, )\), based on which we can get the upper bound for our algorithm and the lower bound for OLS, ridge regression, and finite-step GD.

**Theorem 5.3**.: _Suppose \(\) with \(||=s\) is selected such that each element is chosen with equal probability from the set \(\{1,2,,d\}\) and \(w_{i}^{}\{-1/,1/\}\) has a restricted uniform prior for \(i\), \(^{}_{2}(1)\) and \(n t^{2}s^{3}d^{2/3}\). Then there exists a choice of \(\) and \(t\) such that_

\[}_{}^{t} ^{2}^{2}ns/^{2}^{2}(d/) +}{n^{2}},\]

_with probability at least \(1-\). Besides, let \(}_{}\) be the ridge regression estimator with regularized parameter \(\), and \(_{}\) be the OLS estimator, it holds that_

\[_{^{}}[()] d}{n}&n d+(1/)\\ 1-+n}{d}&d n+(1/),\]

_with probability at least \(1-\), where \(\{}_{},_{}, _{}^{t}\}\)._

It can be seen that for a wide range of under-parameterized and over-parameterized cases, \(}_{}^{t}\) has a smaller excess risk than ridge regression, standard gradient descent, and OLS. In particular, consider the setting \(^{2}=1\), in the over-parameterized setting that \(d n\), the excess risk bound of preprocess-then-optimize is \((ds/n^{2})\), which also outperforms the \((1)\) bound achieved by OLS, ridge regression, and standard gradient descent if the sparsity satisfies \(s=O(n^{2}/d)\) (in fact, this condition can be certainly removed as \((}_{}^{t})\) also has a naive upper bound \((1)\)). In the under-parameterized case that \(d n\), it can be readily verified that the data preprocessing can lead to a \((s/n)\) excess risk, which is strictly better than the \((d/n)\) risk achieved by OLS, ridge regression, and standard gradient descent. Moreover, it is well known that Lasso can achieve \((s/n)\) excess risk bound in the setting of Theorem 5.3. Then, by comparing with our results, we can also conclude that the proprocess-then-optimize algorithm can be comparable to Lasso up to logarithmic factors when \(d n\), while becomes worse when \(d n\).

## 6 Experiments

In Section 3, we conduct several experiments, and based on the observations, we propose that a trained transformer can apply a preprocess-then-optimize algorithm: (1) In the first layer, the transformer can apply a preprocessing algorithm (Alg. 1) on the in-context examples utilizing multi-head attention.

(2) In the subsequent layers, the transformer applies a gradient descent algorithm on the preprocessed data utilizing single-head attention. While the second part is supported by extensive theoretical analysis and experimental evidence , here we develop a technique called preprocessing probing (P-probing) on the trained transformer to support the first part of our algorithm. We also directly apply Alg. 1 on the in-context examples and then check the excess risk for multiple-step gradient descent to verify the effectiveness of our algorithm and theoretical analysis.

P-probing:To verify the existence of a preprocessing procedure in the trained transformer, we develop a "preprocessing probing" (P-probing) technique on the trained transformers, as illustrated in Figure 1(a). For a trained transformer, we first set the input sequence as in Eq.(2.3), where the first \(n\) examples \(\{\}_{i=1}^{n}\) have the corresponding labels \(\{y\}_{i=1}^{n}\), and the following \(q\) query entries only have \(\{_{i}\}_{i=n+1}^{n+q}\) in the sequence. Then, we extract the last \(q\) vectors in the output hidden state \(^{1}\) from the first layer of the transformer and treat these data as processed query entries. Next, we conduct gradient descent on the first \(q-1\) query entries with their corresponding \(y\), computing the excess risk on the last query. Additional experimental details can be found in Appendix B. We adapt this technique based on the intuition that, according to our theoretical analysis, we can extract the preprocessed entry \(\{}_{i}\}_{i=n+1}^{n+q}\) from \(^{1}\), besides, the excess risk computed by the preprocessed data has a better upper bound guarantee compared to raw data without preprocessing under the same number of gradient descent steps, so if the trained transformer utilize multihead attention for preprocess, compared with single head attention, the queries entries extract from \(^{1}\) by multihead attention can have better gradient descent performance compared with single head attention.

Verifying the benefit of preprocessing:To further support the effectiveness of our algorithm, we directly apply Alg. 1 on the input data \(\{_{i},y_{i}\}_{i=1}^{n+1}\), and then implement gradient descent on the example entries \(\{_{i},y_{i}\}_{i=1}^{n}\) and compute the excess risk with the last query \(\{}_{n+1},y_{n+1}\}\), we refer this procedure as pre-gd. We compare pre-gd with the excess risk obtained by directly applying gradient descent without preprocessing (referred to as gd). For all experiments (both P-probing and this), we set \(_{}^{0}=\) and tune the learning rate \(\) for each model by choosing from \([1,10^{-1},10^{-2},10^{-3},10^{-4},10^{-5},10^{-6}]\) with the lowest average excess risk.

Based on Figure 1(b), we can observe that compared to the transformer with single-head attention (\(h=1\)), the query entries extracted from the transformer with multiple heads (\(h=4,8\)) preserve better convergence performance and can dive into a lower risk. This aligns well with our experiment result in Figure 1(c), where compared to gd, the data preprocessed by Alg. 1 preserves better convergence performance and can dive into a lower risk space, supporting the existence of the preprocessing procedure in the trained transformer. Moreover, Figure 1(c) also aligns well with our theoretical analysis, where we provide a better upper bound for convergence guarantee for our algorithm compared to ridge regression and OLS.

Figure 2: Supporting experiments for our preprocess-then-optimize algorithm and theoretical analysis

Conclusions and Limitations

In this paper, we investigate a sparse linear regression problem and explore how a trained transformer leverages multi-head attention for in-context learning. Based on our empirical investigations, we propose a preprocess-then-optimize algorithm, where the trained transformer utilizes multi-head attention in the first layer for data preprocessing, and subsequent layers employ only a single head for optimization. We theoretically prove the effectiveness of our algorithm compared to OLS, ridge regression, and gradient descent, and provide additional experiments to support our findings.

While our findings provide promising insights into the hidden mechanisms of multi-head attention for in-context learning, there is still much to be explored. First, our work focuses on the case of sparse linear regression, and it may be beneficial to implement our experiment for more challenging or even real-world tasks. Additionally, as we adapt attention-only transformers for analysis simplification, the role of other modules, such as MLPs, are neglected. How these modules incorporate in real-world tasks remains unclear. Moreover, our analysis does not consider the training dynamics of transformers, while the theoretical analysis in  provides valuable insights into the convergence of single-layer transformers with multi-head attention, the training dynamics for multi-layer transformers remain unclear. How transformers learn to implement these algorithms is worth further investigation.