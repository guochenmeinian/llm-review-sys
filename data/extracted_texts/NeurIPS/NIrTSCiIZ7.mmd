# Boundary Guided Learning-Free

Semantic Control with Diffusion Models

 Ye Zhu\({}^{1,2}\), Yu Wu\({}^{3}\), Zhiwei Deng\({}^{4}\), Olga Russakovsky\({}^{2}\), Yan Yan\({}^{1}\)

\({}^{1}\)Department of Computer Science, Illinois Institute of Technology

\({}^{2}\)Department of Computer Science, Princeton University

\({}^{3}\)School of Computer Science, Wuhan University

\({}^{4}\)Google Research

yezhu@princeton.edu, wuyucs@whu.edu.cn, zhiweideng@google.com, olgarus@princeton.edu, yyan34@iit.edu

###### Abstract

Applying pre-trained generative denoising diffusion models (DDMs) for downstream tasks such as image semantic editing usually requires either fine-tuning DDMs or learning auxiliary editing networks in the existing literature. In this work, we present our _BoundaryDiffusion_ method for efficient, effective and light-weight semantic control with frozen pre-trained DDMs, without learning any extra networks. As one of the first learning-free diffusion editing works, we start by seeking a comprehensive understanding of the intermediate high-dimensional latent spaces by theoretically and empirically analyzing their probabilistic and geometric behaviors in the Markov chain. We then propose to further explore the critical step for editing in the denoising trajectory that characterizes the convergence of a pre-trained DDM and introduce an automatic search method. Last but not least, in contrast to the conventional understanding that DDMs have relatively poor semantic behaviors, we prove that the critical latent space we found already exhibits semantic subspace boundaries at the generic level in _unconditional_ DDMs, which allows us to do controllable manipulation by guiding the denoising trajectory towards the targeted boundary via a _single-step_ operation. We conduct extensive experiments on multiple DPMs architectures (DDPM, iDDPM) and datasets (CelebA, CelebA-HQ, LSUN-church, LSUN-bedroom, AFHQ-dog) with different resolutions (64, 256), achieving superior or state-of-the-art performance in various task scenarios (image semantic editing, text-based editing, unconditional semantic control) to demonstrate the effectiveness. Project page at _https://1-yezhu.github.io/BoundaryDiffusion/_.

## 1 Introduction

The denoising diffusion models (DDMs)  have been successfully applied in various tasks such as image and video synthesis , audio generation , image customization , reinforcement learning , and recently in scientific applications . Among various applications of DDMs, one popular downstream task is to use pre-trained _unconditional_ DDMs for image manipulation and editing, by either re-training the diffusion models , or learning extra auxiliary neural networks for the editing signals . However, as these methods require additional learning processes, we argue that they have not yet fully leveraged the great potential of pre-trained DDMs. In this work, we show that the latent spaces of pre-trained DDMs 1, when leveraged properly, can be directly used to perform various manipulation tasks withoutlearning any extra networks and achieve superior or SOTA performance as shown in Fig. 1. This makes our work the first to achieve learning-free diffusion editing with _unconditional_ base models that are trained without additional semantic supervision or conditioning.

The understanding of latent spaces has been a challenging yet critical factor for better explaining, interpreting, and utilizing diffusion models. On the one hand, the DDMs are formulated as a long Markov chain (usually with 1,000 diffusion steps) , which introduces the same number of generic latent spaces within a single model. In contrast to a single unified latent space in GANs , this unique formulation imposes extra difficulties for studying the latent spaces of DDMs, leading to additional research questions such as _"which latent spaces should we focus on to analyze their behaviors?"_ On the other hand, abundant methodology designs for multiple downstream applications have been motivated by their corresponding understanding in terms of latent spaces [44; 31; 35]. For example, DiffAutoencoder  proposes to learn auxiliary encoders based on existing latent variables to obtain semantically meaningful representations for image attribute manipulations, mainly because the generic latent spaces in DDMs are initially believed to lack semantics. For the same reason, DiffusionCLIP  proposes to achieve semantic manipulation by fine-tuning the entire pre-trained DDMs each time given a new editing target attribute. More recently, Asyrp  observes that DDMs already form semantic spaces, but only in the bottleneck level of U-Net implementations ,

Figure 1: We present the _BoundaryDiffusion_ for efficient (**single-step**), effective and light-weight semantic control on multiple application scenarios with _frozen_ pre-trained DDMs. Different from existing works, our _BoundaryDiffusion_ is a **learning-free** method that achieves SOTA performance without learning any extra networks, in contrast to _e.g._ Asyrp , which requires training auxiliary editing neural networks, and DiffusionCLIP , which fine-tunes the pre-trained DDMs. We include more **randomly selected non-cherry-picked results** in Appendix G as qualitative demonstrations.

referred to as \(h\)-space. Accordingly, they propose to learn an auxiliary editing network that operates on the \(h\)-space for each manipulation attribute. To this end, in this work, we feature our first branch of contributions in terms of latent spaces understanding from the following two aspects, by supplementing and correcting the current literature: **i)** Through the geometric and probabilistic analysis of the latent encodings from the departure space at diffusion step \(T\), we demonstrate that the stochastic denoising and deterministic inversion (the process to convert image to noisy latent variables, which is essential for semantic image editing [31; 35]) trajectories along the Markov chain are **asymmetric**, as opposed to previous understanding  (see Sec. 3). **ii)** We further propose a theoretically supported automatic search method to identify the critical diffusion step (_i.e._, the mixing step) along the chain, which starts to exhibit semantics at the _generic_ level (referred as \(\)-space), as supplementary and/or correction to existing literature [44; 31; 35] (see Sec. 4 and Sec. 5).

Based on the better understanding of latent spaces, we then propose our _BoundaryDiffusion_ method to achieve semantic control with pre-trained frozen and unconditional diffusion models in a learning-free manner in Sec. 5. Specifically, our approach first locates the semantic boundaries in the form of hyperplanes via SVMs within the latent space at the critical mixing step. We then introduce a mixing trajectory with controllable editing strength, which guides the original latent encoding to cross the semantic boundary at the same diffusion step to achieve manipulation given a target editing attribute as in Fig. 3. We also optimize the image quality via a combination of deterministic and stochastic denoising processes. We conduct extensive experiments using different DDMs architectures (DDPM , iDDPM ) and datasets (CelabA , CelebA-HQ , AFHQ-dog , LSUN-church , LSUN-bedroom ) for multiple applications (image semantic editing, text-based editing, unconditional semantic control), achieving either superior or SOTA performance in both quantitative scores (\(S_{dir}\), segmentation consistency[65; 68; 67], face identity similarity  and FID ) and qualitative user study evaluations, as presented in Sec. 6. Compared to previous learning-based methods [31; 35], our _BoundaryDiffusion_ method features a light-weight and unified single-step operation without any additional training cost. In addition, different from DiffusionCLIP  and Astrp , which take about 30 min of learning time _per attribute_, our _Boundary_ takes approximately negligible time about 1s and as few as 100 images to locate and identify the semantic boundaries.

Overall, our work contributes to both latent space understanding and technical methodology designs. Firstly, we provide multiple novel findings and analysis that correct or supplement the current understanding of latent spaces of DDMs. Notably, we reveal that the previous symmetric assumption of stochastic denoising and deterministic inversion directions does not hold after explicitly analyzing the geometric and probabilistic properties in the departure latent space at diffusion step \(T\). We also demonstrate that the unconditionally trained DDMs exhibit meaningful semantics at our located mixing step along the Markov chain in generic \(\)-space level. Secondly, our proposed _BoundaryDiffusion_ method achieves semantic control in a learning-free manner, providing a promising direction for resources friendly and efficient downstream applications. The above contributions are further supported and validated by strong performance through extensive experiments and comprehensive evaluations. Code is available at https://github.com/L-YeZhu/BoundaryDiffusion.

## 2 Background

We briefly describe essential background here, and include more detailed related work in Appendix A.

**Denoising Diffusion Models.** Denoising Diffusion Probabilistic Models (DDPMs)  are one of the principal formulations for diffusion generative models . The core design of DDPMs consists of a stochastic Markov chain in two directions. The forward process adds stochastic noises (usually parameterized with Gaussian kernel) to a data sample \(x_{0}\) following:

\[q(_{t}|_{t-1})=(}_{ t-1},_{t}_{d}),\] (1)

where \(\{_{t}\}_{t}^{T}=1\) are usually scheduled variance.

The reverse direction, which corresponds to the generative process, denoises a latent sample \(_{T}\) (often sampled from a standard Gaussian distribution) to a data sample \(_{0}\) as:

\[_{t-1}=}}(_{t}-}{ }}_{t}^{}(_{t}))+_{t}z_{t},\] (2)

where \(_{t}^{}\) is the learnable noise predictor, \(z_{t}(0,)\), and the variance of the reverse process \(_{t}^{2}\) is set to be \(^{2}=\).

Alternatively, denoising diffusion implicit models (DDIMs) consider a non-Markovian process with the same forward marginals as DDPMs, with a slightly modified deterministic _sampling_ process as follows:

\[_{t-1}=}(_{t}-}_{t}^{}(_{t})}{}})+-_{t}^{2}}_{t}^{}(_{t})+ _{t}z_{t},\] (3)

where \(_{t}=)/(1-_{t})}/ _{t-1}}\), as the \(\) control the degree of stochasticity. Specifically, the first term indicates the directly predicted \(_{0}\) at arbitrary diffusion step \(t\), and the second term denotes the direction pointing to \(_{t}\), and the last term is the random noise.

In this work, we focus on the study of denoising diffusion models. While the pre-trained models are learned based on DDPMs, we adopt DDIMs for inverting real images \(_{0}\) to obtain their corresponding latent encodings \(_{t}_{t}\) as in [31; 35]. However, different from conventional understanding, we reveal this inversion process is **asymmetric** to the stochastic denoising process from DDPMs in Sec. 3, which is an important fact to understand the reason why the previous works require extra learning to achieve the semantic editing effects.

**Semantic Understanding and Trajectory in High Dimensional Space.** Previous works on the semantic understanding of latent space for generative models have largely focused on variants of GANs , such as the \(\) space for generic GANs , \(\) space from ProgressiveGAN , and \(\) space from StyleGAN . More recently,  study the latent spaces of diffusion models and show that the pre-trained DDMs already have a semantic latent space at the bottleneck level of U-Net, and name it \(h\)-space. We observe that there exist two unique components for DDMs compared to previous studies for GANs [51; 12]: the relatively high-dimensionality of intermediate latent spaces and the trajectory along the Markov chain.

Firstly, unlike GANs that usually sample from a lower dimensional Gaussian distribution and upsample the latent encoding to the higher data space , DDMs  maintain the same dimensionality for all the intermediate latent encodings \(_{\{1:T\}}\) throughout the entire Markov chain. The above formulation regularizes the latent encodings to stay in higher dimensional spaces, which motivates us to tackle the problem using existing mathematical tools from high dimensional space theory , such as the concentration mass distribution and Gaussian radius estimation. In addition, DDMs model a random walk on the data space as a Markov stochastic process [56; 57; 32; 55; 24; 60], and form a trajectory as the diffusion step proceeds. This trajectory characterizes the property of the considered Markov and imposes nature discussion on the convergence question, which inspires us to draw the connection between the mixing time study in Markov chain  and diffusion models.

## 3 High-Dimensional Latent Spaces in Diffusion Models

**Notations.** Given a pre-trained DDM \(p\) (we omit \(\) from common notation \(p_{}\) given the fact that the parameters are frozen in this work) with \(T\) diffusion steps. We define the raw data space as \(\) and a data sample \(_{0}\) in \(d\)-dimension. The intermediate latent variables \(_{\{1:T\}}\) are samples in the same dimensionality as \(_{0}\) from their corresponding spaces \(_{\{1:T\}}\). Specifically, as previous works suggest there are two methods to obtain the latent encodings, either from direct sampling as most generative works do [21; 14; 48; 33], or from a given raw data \(_{0}\) after inversion via DDIMs [31; 35]. We note \(_{t}^{s}\) and \(_{t}^{i}\) as the latent encodings at the \(t\)-th step from \(_{t}\) via sampling and inversion, respectively. Similarly, we distinguish two sampling processes with the stochastic DDPMs and the deterministic DDIMs as \(p_{s}\) and \(p_{i}\). Note the difference between DDPMs and DDIMs only exists in sampling process, while the training remains identical (_i.e._, a single \(p\) may be used in two sampling ways). We use \(_{0}^{}\) to represent the denoised image after editing.

**Sampling vs. Inversion.** We start the analysis of high-dimensional latent spaces of DDMs by considering different sources of latent encodings from the departure space \(_{T}\) at the diffusion timestep \(T\). Theoretically, the directly sampled latent encodings \(_{T}^{s}\) follow a standard Gaussian distribution \((,_{d})\) by definition. In contrast, the inverted latent encodings, as they adopt the DDIMs formulation, **do not** follow an identical trajectory, but the marginal distribution of the forward DDPMs to go from the image space \(\) to the latent space \(_{T}\), leading to a non-standard Gaussian distribution different from the direct sampling. The above describes a **critical statement** that differs from existing work , where DDIMs-based inversion has been previously considered as a symmetric process to the generative trajectory.

We can also empirically demonstrate the above conclusion by estimating the radius \(r\) of the high-dimensional Gaussian space. In mathematics, the radius of a high dimensional Gaussian space is defined as the square root of the expected squared distance: \(r=\). Specifically, given a \(d\)-dimensional Gaussian centered at the origin with variance \(^{2}\). For a point \(=(x_{1},x_{2},...,x_{d})\) chosen at random from Gaussian, the expected squared length of \(\) is:

\[E(x_{1}^{2}+x_{2}^{2}+...+x_{d}^{2})=dE(x_{1}^{2})=d^{2}.\] (4)

For large \(d\), the value of the squared length of \(\) is tightly concentrated about its mean, and the square root of the expected squared distance (namely \(\)) is called _Gaussian radius_, denoted by \(r\).

We calculate the mean of squared length from \(N\) samples from \(_{T}^{s}\) and \(_{T}^{i}\) as \(_{N}_{j}^{d}((x_{T,j})^{2})\). For the sampled latent encoding from \(_{T}(,_{d})\), the estimated radius \(r=\), as shown in Tab. 1. In contrast, for the inverted latent encodings from real images, the previously expected radius does not hold. Interestingly, we note that the inverted latent encodings from iDDPMs  endure less radius shift compared to DDPMs , indicating that this radius estimation may imply the goodness of a pre-trained DDM. The above discussion on the distributions of \(_{T}\) helps to better interpret the revealed "distance effect" below. More details about the marginal discussion via DDIMs inversion are included in Appendix A.

**Distance Effect by Deterministic Inversion and Mitigation.** As the next step, we study the influence of an asymmetric inversion trajectory by following the marginal distribution via DDIMs. Previous studies  suggest that despite the latent encodings inverted via DDIMs allowing for near-perfect reconstruction, it is rather empirically difficult to directly edit \(_{T}^{i}\) for controlling the final denoised output. Similarly, we observe the phenomena that for the inverted latent encodings \(_{T}^{i}\), a small modification often leads to severely degraded and distorted denoised images following the deterministic sampling \(p_{i}\), as shown in the second row of Fig. 2(a). As a comparison, the directly sampled encoding \(_{T}^{s}\) shows a smooth transition after \(p_{i}\) (see the top row in Fig. 2(a)). This distance effect harms the performance in downstream applications that requires the inversion operation, such as image editing and manipulation tasks where the input is a given raw image \(_{0}\) instead of directly sampled latent encoding \(_{T}^{s}\).

Having observed that the inverted latent encoding \(_{T}^{i}\) tends to suffer from distorted denoised images during the interpolation, we aim to further understand and interpret the distance effect from properties of the latent high-dimensional spaces. Intuitively, the sampled latent encodings \(_{T}^{s}\) locate in the area with higher probability concentration mass in \(_{T}\) space, while the spatial locations of inverted ones \(_{T}^{i}\), following the marginal diffusion trajectories, have relatively lower concentration mass. This spatial difference remains after the deterministic denoising via \(p_{i}\) due to the homogeneity of intermediate latent spaces , resulting in denoised samples from \(_{T}^{i}\) to be rather out-of-distribution and low-fidelity after distance shift in latent spaces. The homogeneity of latent spaces from  states that _"the same shift in this space results in the same attribute change in all images"_, while in our context, we consider the spatial difference (_i.e._, inverted latent encodings have a smaller radius than sampled ones) in terms of Gaussian radius along the denoising trajectory.

From the geometric point of view, the probabilistic concentration mass of a high-dimensional sphere is essentially located within a thin slice at the equator and contained in a narrow annulus at the surface . In addition, one needs to increase the radius of the sphere to nearly \(\) before there is a

   Models & Sampling & Inversion & \( r\) \\  DDPM-CelebA-64 & 110.84 & 95.06 & 15.78 \\ DDPM-LSUN-256 & 443.42 & 430.88 & 12.54 \\ iDDPM-AFHQ-256 & 443.41 & 438.07 & **5.34** \\   

Table 1: Estimation of the Gaussian radius \(r\) for sampled and inverted latent encodings in \(_{T}\) from different pre-trained DDMs, datasets and image resolutions (64 and 256).

   Steps & 1000 & 900 & 800 & 700 & 600 & 500 \\  \(_{T}^{i}+p_{i}\) & 0.02 & 0.01 & 0.25 & 0.75 & 1.98 & **4.63** \\ \(_{T}^{i}+p_{i}\) & 0.02 & 0.02 & 0.21 & 0.74 & 2.03 & **4.76** \\ \(_{T}^{i}+p_{*}\) & 1.76 & 1.74 & 1.42 & 1.45 & 2.08 & **3.81** \\ \(_{T}^{i}+p_{i}\) & 1.72 & 1.73 & 1.45 & 1.41 & 2.18 & **3.70** \\   

Table 2: 100-stepwise variation of Gaussian radius for different sample sources and sampling combinations using pre-trained DDM. With different latent encodings and sampling processes, the mixing step appears at approximately the same time around \(t=500\).

non-zero concentration measure . We explain the cause of the distance effect via the geometric illustration in Fig.2(b). More theoretical details can be found in Appendix B.

In contrast to the deterministic generation that suffers from the distance effect, the stochastic denoising is more robust and helps to alleviate the issue, as shown in Fig. 2(a). Similarly, previous works also reveal that stochasticity is beneficial for the quality of denoised data [28; 31; 35]. Therefore, adding stochasticity is a mitigation solution for the distance effect. From the perspective of spatial properties, the stochasticity brings the inverted latent encodings \(_{T}^{i}\) from the area with lower concentration mass to higher ones, thus making the denoising process more robust.

## 4 Mixing Step in Diffusion Models

**Problem Formulation.** In the mathematical and statistical study of the Markov process, the mixing time is defined as a key step \(t\), at which the intermediate distribution converges to the stationary distribution 2. Inspired by the established mathematical formulations, we introduce the mixing step problem for diffusion models as the search for the critical diffusion step \(t\{1,...,T\}\), where the Gaussian distribution converges to the final data distribution in the reverse denoising process under some pre-defined distance measures (usually in _total variation distance_, noted as \(||||_{TV}\)). It is worth noting that, unlike the distance effect which is induced by the deterministic inversion method, the mixing step is a characteristic carried by **generic DDMs**, as we show that the mixing step appears at the same time both for the directly sampled latent encodings and the inverted ones. In practice, we show in the experiments in Sec. 6 that imposing editing and manipulations on the mixing step is more effective for downstream tasks. Meanwhile, similar concepts have been empirically studied as a hyper-parameter as "return step"  or "editing step"  without further investigations into its theoretical meanings. Proof and details of the following property can be found in the supp.

_Property 4.1_.: Under the total variation distance measure \(||||_{TV}\), the mixing step \(t_{m}\) for a DDM with data dimensionality \(d\) is formed during training (i.e., irrelevant to the sampling methods). \(t_{m}\) is mainly related to the transition kernels, the stationary distribution (i.e., datasets), and the dimensionality \(d\). In practice, a radius shift of approximately 4 can be considered as a searching criterion for the mixing step in the denoising chain.

**Automatic Search via Radius Estimation.** While the rigorous theoretical deviation for the mixing step \(t_{m}\) depends on many factors and is a complex and non-trivial problem (see our supp. for details), we present here an effective yet simple method to search for the mixing step by estimating the radius \(r\) of latent high-dimensional Gaussian spaces. We can estimate the radius of the latent spaces along the denoising trajectory and show that the change of \(r\) (corresponding to the transition kernel in Property 4.1) reveals the appearance of the mixing step for a pre-trained DDM. Our proposed method automates the search for this critical diffusion step, while it has been previously determined by manually checking the quality of final denoised images [31; 35]. More detailed discussion and proof about the empirical search method can be found in Appendix D.

Figure 2: Illustration of distance effect from the qualitative results and geometric properties. (a) The interpolation are conducted on CelabA-64, with different combinations for latent encoding sources and sampling methods. Following the same denoising process \(p_{i}\), inverted latent encodings lead to distorted images. (b) The inversion process reverses images to the latent encodings at the area of less concentration mass with a smaller radius. \((})\) is the width of the slice for the concentration mass.

Specifically, we follow the procedure described in Sec. 3, where we calculate the mean of the squared distance of a latent sample from the origin as the estimation of the Gaussian square. We compute the radius of the latent encodings following the generative trajectory, the earliest step with the largest radius shift (\( r 4\)) is an approximation of the mixing step. We show the change of radius estimation for different combinations of latent encoding sources and sampling methods, demonstrating the mixing step is a generic characteristic irrelevant to the above factors in Tab. 2.

## 5 _BoundaryDiffusion_ for Semantic Control

In this section, we propose our _BoundaryDiffusion_ method to achieve **one-step** semantic control and editing by guiding the denoising trajectory using the semantic boundaries in high dimensional latent spaces with pre-trained and frozen DDMs. Specially, we leverage the stochasticity and the mixing step \(t_{m}\) for better qualitative results and more effective manipulation.

**Semantic Boundary Search.** In the case of attribute manipulation with given annotations (_e.g._, face attributes modifications), we propose to search for the semantic separation hyperplane as a classification problem as in . Specifically, we use the linear SVM  to classify the latent encodings using the label annotations of their corresponding raw data, and the parameters in SVM are used as the hyperplanes. The validation of using raw data ground truth annotation at the latent space level is guaranteed by the deterministic inversion and reconstruction via DDIMs . We consider this application scenario to be conditional semantic editing, which allows us to edit a semantic attribute given a real raw image. It is worth noting that the objective of using SVM is to fit the _existing semantic hyperplanes_ rather than learning classifiers or networks.

In addition to the above application scenario, we can further extend our semantic boundary search to more flexible settings such as text-based image editing by leveraging the popular large-scale cross-modality generative models such as DALLE-2  and Imagen . Particularly, we can easily generate high-fidelity images based on a given textual prompt and utilize the synthetic images as samples to search for the semantic boundary using the same classification method.

An extra application for our semantic boundary searching is to achieve semantic control for unconditional image synthesis, due to the reason that the semantic boundary is a feature learned and formed during the training. In contrast, other learning-based methods like DiffusionCLIP  that change the original parameters of pre-trained DMMs are not suitable for this downstream task.

**Mixing Trajectory for Semantic Control.** After having located the semantic boundaries, we propose to achieve semantic manipulation by guiding the trajectory via the identified boundaries using the projected distance control. We note the unit normal vector of the hyperplane as \(^{d}\), and define the sign-sensitive "distance" of the latent encoding \(_{t_{m}}\) to the boundary similar to :

\[d(,_{t_{m}})=^{T}_{t_{m}}.\] (5)

Figure 3: _BoundaryDiffusion_ for semantic control in one-step editing. We propose to guide the initial deterministic generative trajectory via the semantic boundary at the mixing step \(t_{m}\) for image manipulation.

Meanwhile, we add the stochasticity after the editing the latent encodings at the mixing step \(t_{m}\). Overall, the mixing trajectory with boundary guidance can be summarized as:

\[p_{mix}(_{t-1}|_{t})=p_{i}(_{t-1}| _{t})&T t>t_{m}\\ p_{s}(^{}_{t-1}|^{}_{t})&t_{m} t \] (6)

where \(^{}_{t_{m}}=_{t_{m}}+ d(,_ {t_{m}})\), and \(\) is the editing strength parameter that controls the projected distance to the target semantic boundary. We illustrate our proposed boundary-guided mixing trajectory method in Fig. 3. Note this operation can also be extended to multi-attribute semantic control, either by iteratively applying guidance to multiple boundaries (which is a linear operation), or by modifying Eq. 5 to \(d(,_{t_{m}})=^{T}_{t_{m}}\), where \(=[},},...,}]\) for \(m\) different attributes. The concrete algorithm of our proposed _BoundaryDiffusion_ is presented in Appendix F.

## 6 Experiments

### Experimental Setup

**Task Settings.** We conduct our experiments on multiple variants of semantic control tasks, including _real image conditioned semantic editing_, _real image conditioned text-based editing_, and _unconditional image synthesis with semantic control_. Among those tasks, the first two scenarios are conditioned on given images and therefore involve the inversion process, while the last one is an extension application for image synthesis without given image input.

**Model Zoo and Datasets.** We test different pre-trained DDMs on various datasets with different image resolutions for experiments. Specifically, we test the DDPMs on the CelabA-64 , CelebA-HQ-256 , LSUN-Church-256 and LSUN-Bedroom-256 . We also experiment with pre-trained improved DDPM  on the AFHQ-Dog-256 . In particular, we emphasize here that the experiments with different image resolutions are critical in our work, especially for the high-dimensional space analysis since the resolutions represent the dimensionality of the latent spaces as shown in Tab. 1 and Fig. 2. However, in all the editing experiments, we use a resolution of \(256 256\) as the default setting for better visualization quality.

**Operational Latent Spaces.** Previous works have either operated on the \(\)-space  or on the \(h\)-space from the bottleneck layer of the U-Net , showing that \(h\)-space has better semantic meanings with homogeneity, linearity, and robustness . In our main experiments, we impose the guidance on both latent encodings in both \(\) and \(h\) levels. All the boundary guidance is imposed in a **single-step** at the mixing step \(t_{m}\). We show in our experimental results later that the single-step operation is more effective at the \(\)-level, while the \(h\)-level may require iterative guidance at multiple diffusion steps.

**Implementations.** We use the linear SVM classifier for searching the semantic boundary. We implement the SVM via the sklearn python package with the number of parameters equal to the total dimensionality of the latent spaces. For \(\)-space, the dimensionality \(d_{}=3 256 256=196,608\). For the \(h\)-space, the dimensionality depends on the pre-trained DDMs architecture implementation for the U-Net . In our experiments, we use the same level of latent spaces as in , which have a dimensionality of \(d_{h}=8 8 512=32,768\). In practice, we observe approximately 100 images are sufficient for finding an effective semantic boundary. For the text-based semantic editing scenario, we use synthetic images generated from Stable Diffusion using text-prompt . These image samples can be obtained via any other pre-trained text-to-image generative models.

In practice, the hyperplanes are found via linear SVMs , with almost negligible learning time of about 1 second on a single RTX3090 GPU. For the inference, the time cost remains at the same level as other SOTA methods. Specifically, by using the skipping step techniques, we can already generate high-quality denoised images using approximately 40-100 steps, which take from 1.682 - 13.272 seconds, respectively on a single RTX-3090 GPU.

**Comparison and Evaluations.** We mainly compare our proposed method with the most recent state-of-the-art image manipulation works with diffusion models: the DiffusionCLIP  and the Asyrp . Both works consist of extra learning processes. Our evaluations include quantitative and qualitative assessments for different experimental settings, similar to most existing generation works. For the inversion and reconstruction part, since we adopt the same technique as in [31; 35] via DDIMs  and achieve near-perfection reconstruction results, we show qualitative samples in Fig. 1. As for the conditional editing, we show quantitative results on the CelebA-HQ usingthree metrics: Directional CLIP similarity (\(S_{dir}\)) computed via CLIP , segmentation consistency (SC) computed via [65; 68; 67], and face identity similarity (ID) using . We also report the FID scores  of edited images on the testing set of CelebA-HQ . We further conduct human studies as subjective evaluations. Additionally, we also report the classification accuracy for the semantic boundary separation validation as in Appendix G.

### Experimental Results

**Conditional Semantic Manipulation.** We show the qualitative results for conditional semantic manipulation on the CelebA-HQ-256  using the pre-trained DDPM  in the top row of Fig. 1. The quantitative evaluations are shown in Tab. 3, where we achieve SOTA performance. In particular, we emphasize the fact both SOTA methods [31; 35] directly use the directional CLIP loss as part of the loss function to align the direction between the embeddings of the reference and generated image in the CLIP space  in their learning processes, which helps to increase the score of \(S_{dir}\). Additionally, they also optimize the identity loss \(_{id}(x_{0}^{},x_{0})\) to preserve the facial identity consistency between the original image \(x_{0}\) and the edited one \(x_{0}^{}\), thus boosting the ID score. While our proposed _BoundaryDiffusion_ method does not use any learning loss function for semantic control, yet achieves very competitive scores in all the metrics.

**Text-Guided Semantic Manipulation.** With the success of large-scale cross-modality generative models [46; 20], we are able to extend the proposed _BoundaryDiffusion_ method to other downstream tasks such as text-guided semantic manipulations. Qualitative examples are presented in the middle part of Fig. 1, where we show the original image input, reconstruction results, and the edited samples. We also perform experiments on the unseen domain transfer and show the example of editing a _dog_ to a _fox_, as indicated using the red text prompt in Fig. 1. We note one limitation of our _BoundaryDiffusion_ is relatively difficult for unseen domain editing tasks compared to other learning-based methods, which aligns with the expectation since the frozen DDMs do not have the knowledge for unknown domain distributions.

**Unconditional Semantic Control.** One unique application scenario we present in the bottom row of Fig. 1 is to apply the identified semantic boundary to the unconditional synthesis setting, where the departure latent encoding \(_{T}\) is now directly sampled from a standard Gaussian without a given real image as input. Since the semantic boundary is an intrinsic characteristic formed during the training process of DDMs, we can easily re-use it for semantic control under a general unconditional denoising trajectory. In comparison, the existing SOTA methods that modify the parameters of pre-trained DDMs during additional learning are no longer applicable to this task.

**Ablation on Operational Latent Spaces and Editing Strength.** We can easily control the editing strength via the editing distance as defined in Sec. 5. Fig. 4 shows an example for editing the _smile_ attribute on CelebA-HQ. We use the hyper-parameter \(\) to control the degree of modification. From the same figure, we observe that we achieve a stronger editing effect at the same distance when applying the boundary guidance to both latent levels at \(_{t_{m}}\) and \(h_{t_{m}}\).

**User Study.** We also conducted a subjective user study to compare the performance with Diffusion-CLIP  and Asyp , where we interviewed 20 human evaluators to rank the edited images in terms of general quality and attribute editing effects. Specifically, we asked the evaluators to pick the best-edited result in terms of two aspects: **1).** General quality: which image quality do you think is the best (clear, fidelity, photorealistic)? **2).** Attribute: which image do you think achieves the best attribute editing effect (natural, identity preservation with respect to the given raw image)? Tab 3

   Methods & \(S_{dir}\) & SC \(\) & ID \(\) & FID \(\) & User Quality \(\) & User Attribute \(\) \\  StyleCLIP  & 0.13 & 86.8\% & 0.35 & - & - & - \\ StyleGAN-NADA  & 0.16 & 89.4\% & 0.42 & - & - & - \\ DiffusionCLIP  & 0.17 & **93.7\%** & 0.70 & 86.23 & 3.2\% & 8.2\% \\ Asyp  & **0.19** & 87.9\% & - & 68.38 & 41.3\% & 44.9\% \\  Ours BoundaryDiffusion & 0.17 & 90.4\% & **0.73** & **63.14** & **55.5\%** & **46.9\%** \\   

Table 3: Evaluation results on CelebA-HQ-256 for real image conditioned semantic editing. FID scores are reported on the test set with 500 raw images, averaged on _“add or remove smile”_ editing. The user study follows similar evaluation questions in .

shows the percentage of each method picked as the best result. Details about the user study and more **randomly selected, non-cherry-picked** qualitative samples are included in Appendix G.

## 7 Discussion and Conclusion

Our work explores and leverage the great potential of pre-trained unconditional diffusion models without learning any additional network model modules. We provide a rather comprehensive analysis from the high-dimensional space perspective to better understand the latent spaces and the trajectory for pre-trained DDMs. We introduce the mixing step for DDMs, which characterizes the convergence of generic DDMs, and present a simple yet effective method to search for the mixing step via Gaussian radius estimation in high-dimensional latent spaces. Last but not least, we propose _BoundaryDiffusion_, an effective learning-free method with boundary-guided mixing trajectory to realize effective semantic control and manipulation for downstream tasks using multiple pre-trained DDMs and datasets.

**Broader Impact and Limitation.** We discuss the broader impact of this work. Firstly, the primary goal of this work is not to create new generative models or generate synthetic data, but to explore the potential of the current generative models for better usage. To do so, we also propose a new perspective to better understand and interpret the DDMs, which is the analysis of high-dimensional latent space behaviors using the theoretical tools from mathematics and statistics. In the meanwhile, during the process of exploring and separating the semantic boundary, we leverage the current popular cross-modality generative models to synthesize images with a text prompt. However, all the generated images are only used for boundary detection. We believe our work brings valuable insights to the research community in terms of a better understanding and further exploration via training-free methods to apply diffusion generative models.

As for limitations, since our _BoundaryDiffusion_ method is learning-free and does not introduce any extra parameters to the base diffusion models, the ability for unseen domain image editing is relatively limited compared to other learning-based methods.

Figure 4: **Ablation on editing strength control via distance with different operational spaces. We can modify the editing strength by changing the distance between the latent encoding \(_{t_{m}}\) to the target boundary. We show that the one-step guidance is more effective when applied in the \(\)-level space. Images in green boxes denote the given raw image.**