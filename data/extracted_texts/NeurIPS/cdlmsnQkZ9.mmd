# Learning non-Markovian Decision-Making

from State-only Sequences

 Aoyang Qin\({}^{,1,2}\) Feng Gao\({}^{3}\) Qing Li\({}^{2}\) Song-Chun Zhu\({}^{1,2,4}\) Sirui Xie\({}^{,5}\)

\({}^{1}\) Department of Automation, Tsinghua University

\({}^{2}\) Beijing Institute for General Artificial Intelligence (BIGAI)

\({}^{3}\) Department of Statistics, UCLA \({}^{4}\) School of Artificial Intelligence, Peking University

\({}^{5}\) Department of Computer Science, UCLA

indicates equal contribution. Correspondence: Sirui Xie (srxie@ucla.edu). Code and data are available at https://github.com/qayqaq/LanMDP

###### Abstract

Conventional imitation learning assumes access to the actions of demonstrators, but these motor signals are often non-observable in naturalistic settings. Additionally, sequential decision-making behaviors in these settings can deviate from the assumptions of a standard Markov Decision Process (MDP). To address these challenges, we explore deep generative modeling of state-only sequences with non-Markov Decision Process (nMDP), where the policy is an energy-based prior in the latent space of the state transition generator. We develop maximum likelihood estimation to learn both the transition and the policy, which involves short-run MCMC sampling from the prior and importance sampling for the posterior. The learned model enables _decision-making as inference_: model-free policy execution is equivalent to prior sampling, model-based planning is posterior sampling initialized from the policy. We demonstrate the efficacy of the proposed method in a prototypical path planning task with non-Markovian constraints and show that the learned model exhibits strong performances in challenging domains from the MuJoCo suite.

## 1 Introduction

Imitation from others is a prevalent phenomenon in humans and many other species, where individuals learn by observing and mimicking the actions of others. An intriguing aspect of this process is the brain's ability to extract motor signals from sensory input. This remarkable capability is facilitated by _mirror neurons_, which respond to observations as if the imitator is performing the actions themselves. In conventional imitation learning  and offline reinforcement learning , action labels have served as proxies for mirror neurons. But it is important to recognize that they are actually productions of human interventions. Given the recent advancements in AI, now is probably an opportune time to explore imitation learning in a more naturalistic setting.

While the setting of state-only demonstrations is not common, there are certain exceptions. For example, Inverse Reinforcement Learning (IRL) initially formulated the problem as state visitation matching , where demonstrations consist solely of state sequences. Subsequently, this state-only setting was rewarded as Imitation Learning from Observations (ILFO), which introduced the generalized formulation of matching marginal state distributions . These methods typically rely on the Markov assumption and Temporal Difference (TD) learning techniques . One consequence of this assumption, previously believed to be advantageous, is that sequences with different state orders are treated as equivalent. However, the success of general sequence modeling  has challengedthis belief, leading to deep reflections. Notable progresses since then include an analysis of the expressivity of Markovian rewards  and a series of sequence models tailored for decision-making problems [12; 13; 14; 15]. Aligning with this evolving trend, we extend the state-only imitation learning problem to encompass non-Markovian domains.

In this work, we propose a generative model based on non-Markov Decision Process (nMDP), in which states are fully observable and actions are latent. Unlike existing monolithic sequence models, we factorize the joint state-action distribution into policy and causal transition according to the standard Markov Decision Process (MDP). To further extend to the non-Markovian domain, we condition the policy on sequential contexts. The density families of policy and transition are consistent with conventional IRL . We refer to this model as Latent-action non-Markov Decision Process (LanMDP). Because the actions are latent variables following Boltzmann distribution, the present model is closely related to the Latent-space Energy-Based Model (LEBM) . To learn the latent policy by Maximum Likelihood Estimation (MLE), we need to sample from the prior and the posterior. We sample the prior using short-run Markov Chain Monte Carlo (MCMC) , and the posterior using importance sampling. Specifically, the proposed importance sampling sidesteps back-propagation through time in posterior MCMC with a single-step lookahead of the Markov transition. The transition is learned from self-interaction.

Once the LanMDP is learned, it can be used for policy execution and planning through prior and posterior sampling, or in other words, _policy as prior, planning as posterior inference_[18; 19]. In our analysis, we derive an objective of the non-Markovian decision-making problem induced from the MLE. We show that the prior sampling at each step can indeed lead to optimal expected returns. Almost surprisingly, we find that the entire family of maximum entropy reinforcement learning [4; 20; 21; 22; 23; 24] naturally emerges from the algebraic structures in the MLE of latent policies. This formulation avoids the peculiarities of maximizing state transition entropy in prior arts [20; 24]. We also show that when a target goal state is in-distribution, the posterior sampling is optimizing a conditional variant of the objective, realizing model-based planning. In our experiments, we validate the necessity and efficacy of our model in learning to sequentially plan cubic curves, and illustrate an _over-imitation_ phenomenon [25; 26] when the learned model is repurposed for goal-reaching. We also test the proposed modeling, learning, and computing method in MuJoCo, a domain with higher-dimensional state and action spaces, and achieve performance competitive to existing methods, even those that learn with action labels.

## 2 Non-Markov Decision Process

The most well-known sequence model of a decision-making process is Markov Decision Process. A MDP is a tuple \(= S,A,T,R,,H\) that contains a set \(S\) of states, a set \(A\) of actions, a transition \(T:S A(S)\) that returns for every state \(s_{t}\) and action \(a_{t}\) a distribution over the next state \(s_{t+1}\); a reward function \(R:S A\) that specifies the real-valued reward received by the agent when taking action \(a_{t}\) in state \(s_{t}\); an initial state distribution \(:(S)\); and a horizon \(H\) that is the maximum number of actions/steps the agent can execute in one episode. A solution to an MDP is a policy that maps states to actions, \(:S(A)\). The value of policy \(\), \(V^{}(s)=_{T,}[_{t=0}^{H}R(s_{t})|s_{0}=s]\) is the expected cumulative reward (_i.e_. return) when executing with this policy starting from state \(s\). The state-action value of policy \(\) is \(Q^{}(s_{t},a_{t})=R(s_{t},a_{t})+_{T(s_{t+1}|s_{t},a_{t})}[V^{ }(s_{t+1})]\). The optimal policy \(^{*}\) can maximize either \(E_{(s_{0})}[V^{}(s_{0})]\), or the same objective plus the policy entropy [27; 4; 22]. The Markovian assumption supports the convergence of a series of TD-learning methods , whose reliability in non-Markovian domains is still an open problem.

Figure 1: **Graphical model of policy and transition in standard Markov Decision Process and non-Markov Decision Process. Reward variables are omitted in the probabilistic graph to emphasize the difference in dependency between actions and states. nMDP is a natural generalization of standard MDP.**

A non-Markov Decision Process is also a tuple \(= S,A,T,R,,H\). It generalizes MDP by allowing for non-Markovian transitions and rewards . Notably, assuming Markovian transition and non-Markovian reward is usually sufficient since a state space with non-Markovian transition can be represented with its Markov abstraction . Markov abstraction can be done either by treating the original space as observations generated from the latent belief state in a Partially Observable Markov Decision Process (POMDP) , or by projecting historic contexts into an embedding space for sequence pattern detection [31; 32; 28]. Presumably, it is statistically more interesting in deep learning to focus our attention on non-Markovian domains where the temporal dependencies in transition and reward differ. Therefore, without loss of generality, we assume that the state transition is Markovian \(T:S A(S)\), while the reward is not [33; 11], _i.e._\(R:S^{+}\), with \(S^{+}\) denotes the set of all finite non-empty state sequences with length smaller than \(H\). Obviously, the policy should also be non-Markovian \(:S^{+}(A)\). Check Figure 1 for a probabilistic graphical model of the generation process of state sequences from a policy.

## 3 Learning and Sampling

### Latent-action nMDP

A complete trajectory is denoted by

\[=\{s_{0},a_{0},s_{1},a_{1},,a_{T-1},s_{T}\},\] (1)

where \(T\) is the maximum length of all observed trajectories and \(T H\). The joint distribution of state and action sequences can be factorized according to the causal assumptions in nMDP:

\[ p_{}()&=p(s_{0})p_{ }(a_{0}|s_{0})p_{}(s_{1}|s_{0},a_{0}) p_{}(a_{T-1}|s_{0:T-1}) p_{}(s_{T}|s_{T-1},a_{T-1})\\ &=p(s_{0})_{t=0}^{T-1}p_{}(a_{t}|s_{0:t})p_{}(s_ {t+1}|s_{t},a_{t}),\] (2)

where \(p_{}(a_{t}|s_{0:t-1})\) is the policy model with parameter \(\), \(p_{}(s_{t}|s_{t-1},a_{t-1})\) is the transition model with parameter \(\), both of which are parameterized with neural networks, \(=(,)\). \(p(s_{0})\) is the initial state distribution, which can be sampled as a black box.

The density families of policy and transition are consistent with the conventional setting of IRL , where the transition describes the predictable change in state as a single-mode Gaussian, \(s_{t+1}(g_{}(s_{t},a_{t}),^{2})\), and the policy accounts for bounded rationality as a Boltzmann distribution with state-action value as the unnormalized energy:

\[p_{}(a_{t}|s_{0:t})=)}(a_{t};s_ {0:t}))},\] (3)

where \(f_{}(a_{t};s_{0:t})\) is the negative energy, \(Z(,s_{0:t})=(f_{}(a_{t};s_{0:t}))da_{t}\) is the normalizing constant given the contexts \(s_{0:t}\). We discuss a general push-forward transition in Appx A.3.

Since we can only observe state sequences, the aforementioned generative model can be understood as a sequential variant of LEBM , where the transition serves as the generator and the policy is a history-conditioned latent prior. The marginal distribution of state sequences and the posterior distribution of action sequences are:

\[p_{}(s_{0:T})= p_{}(s_{0:T},a_{0:T-1})da_{0:T-1}, p_{ }(a_{0:T-1}|s_{0:T})=(s_{0:T},a_{0:T-1})}{p_{}(s_ {0:T})}.\] (4)

### Maximum likelihood learning

We need to estimate \(=(,)\). Suppose we observe _offline_ training examples: \(\{^{i}\},i=1,2,,n,^{i}=s_{0}^{i},s_{1}^{i},...,s_{T}^{i}\). The log-likelihood function is:

\[L_{off}()=_{i=1}^{n} p_{}(^{i}).\] (5)

Denote posterior distribution of action sequence \(p_{}(a_{0:T-1}|s_{0:T})\) as \(p_{}(A|S)\) for convenience where \(A\) and \(S\) means the complete action and state sequences in a trajectory. The full derivation of the learning method can be found in Appx A.2, which results in the following gradient:

\[_{} p_{}()=_{p_{}(A|S)}[_{t=0}^{ T-1}( p_{}(a_{t}|s_{0:t})}_{}, p_{ }(s_{t+1}|s_{t},a_{t})}_{})].\] (6)Due to the normalizing constant \(Z(,s_{0:t})\) in the energy-based prior \(p_{}\), the gradient for the policy term involves both posterior and prior samples:

\[_{,t}(S)=_{p_{}(A|S)}[_{} p_{ }(a_{t}|s_{0:t})]=_{p_{}(A|S)}[_{} f_{}(a_{t};s_{0:t})]-_{p_{}(a_{t}|s_{0:t})}[ _{}f_{}(a_{t};s_{0:t})],\] (7)

where \(_{,t}(S)\) denotes the expected gradient of policy term for time step \(t\). Intuition can be gained from the perspective of adversarial training [34; 35]: On one hand, the model utilizes action samples from the posterior \(p_{}(A|S)\) as pseudo-labels to supervise the unnormalized prior at each step. On the other hand, it discourages action samples directly sampled from the prior. The model converges when prior samples and posterior samples are indistinguishable.

To ensure the transition model's validity, it needs to be grounded in real-world dynamics \(Tr\) when jointly learned with the policy. Otherwise, the agent would be purely hallucinating based on the demonstrations. Throughout the training process, we allow the agent to periodically collect _on-policy_ data \(\{(s_{t}^{i},a_{t}^{i},s_{t+1}^{i})\}\), \(i=1,2,,m\), \(t=1,2,,T\) with \(p_{}(a_{t}|s_{0:t})\) and update the transition with a _composite likelihood_

\[L_{comp}()=L_{off}()+L_{on}(), L_{on}()=_{i=1}^ {m}_{t=1}^{T} p_{}(s_{t+1}^{i}|s_{t}^{i},a_{t}^{i}).\] (8)

### Prior and posterior sampling

The maximum likelihood estimation requires samples from the prior and the posterior distributions of actions. It would not be a problem if the action space is quantized. However, since we target general latent action learning, we proceed to introduce sampling techniques for continuous actions.

When sampling from a continuous energy space, short-run Langevin dynamics  can be an efficient choice. For a target distribution \((a)\), Langevin dynamics iterates \(a_{k+1}=a_{k}+s_{a_{k}}(a_{k})+_{k}\), where \(k\) indexes the number of iteration, \(s\) is a small step size, and \(_{k}\) is the Gaussian white noise. \((a)\) can be either the prior \(p_{}(a_{t}|s_{0:t})\) or the posterior \(p_{}(A|S)\). One property of Langevin dynamics that is particularly amenable for EBM is that we can get rid of the normalizing constant. So for each \(t\) the iterative update for prior samples is

\[a_{t,k+1}=a_{t,k}+s_{a_{t,k}}f_{}(a_{t,k};s_{0:t})+ _{k}.\] (9)

Given a state sequence \(s_{0:T}\) from the demonstrations, the posterior samples at each time step \(a_{t}\) come from the conditional distribution \(p(a_{t}|s_{0:T})\). Notice that with Markov transition, we can derive

\[p_{}(a_{0:T-1}|s_{0:T})=_{t=0}^{T-1}p_{}(a_{t}|s_{0:T})= _{t=0}^{T-1}p_{}(a_{t}|s_{0:t+1}).\] (10)

Eq. (10) reveals that given the previous and the next subsequent state, the posterior can be sampled at each step independently. So the posterior iterative update is

\[a_{t,k+1}=a_{t,k}+s_{a_{t,k}}((a_{t,k}|s_{0: t})}_{}+(s_{t+1}|s_{t},a_{t,k})}_{ })+_{k}.\] (11)

Intuitively, action samples at each step are updated by back-propagation from its prior energy and a single-step lookahead. While gradients from the transition term are analogous to the inverse dynamics in Behavior Cloning from Observations (BCO) , it may lead to poor training performance due to non-injectiveness in forward dynamics .

We develop an alternative posterior sampling method with importance sampling to overcome this challenge. Leveraging the learned transition, we have

\[p_{}(a_{t}|s_{0:t+1})=(s_{t+1}|s_{t},a_{t})}{_{ p_{}(a_{t}|s_{0:t})}[p_{}(s_{t+1}|s_{t},a_{t})]}p_{ }(a_{t}|s_{0:t}).\] (12)

Let \(c(a_{t};s_{0:t+1})=_{p_{}(a_{t}|s_{0:t})}[p_{}(s_{t+ 1}|s_{t},a_{t})]\), posterior sampling from \(p_{}(a_{0:T-1}|s_{0:T})\) can be realized by adjusting importance weights of independent samples from the prior \(p_{}(a_{t}|s_{0:t})\), in which the estimation of weights involves another prior sampling. In this way, we avoid back-propagating through non-injective dynamics and save some computation overhead.

To train the policy, Eq. (7) can now be rewritten as

\[_{,t}(S)=_{p_{}(a_{t}|s_{0:t})}[(s_{t+1}|s_{t},a_{t})}{c(a_{t};s_{0:t+1})}_{}f_{}(a_{ t};s_{0:t})]-_{p_{}(a_{t}|s_{0:t})}[_{ }f_{}(a_{t};s_{0:t})].\] (13)Decision-making as Inference

In Section 3, we present our method within the framework of probabilistic inference, providing a self-contained description. However, from a decision-making perspective, the learned policy may appear arbitrary. In this section, we establish a connection between probabilistic inference and decision-making, contributing a novel analysis that incorporates the latent action setting, the non-Markovian assumption, and maximum likelihood learning. This analysis is inspired by, but distinct from, previous studies on the relationship between these two fields .

### Policy execution with prior sampling

Let the ground-truth distribution of demonstrations be \(p*(s_{0:T})\), and the learned marginal distributions of state sequences be \(p_{}(s_{0:T})\). Eq. (5) in Section 3.2 is an empirical estimate of

\[_{p^{}(s_{0:T})}[ p_{}(s_{0:T})]=_{p^{ }(s_{0})}[ p^{}(s_{0})+_{p^{}(s_{1:T}| s_{0})}[ p_{}(s_{1:T}|s_{0})]].\] (14)

We can show that a sequential decision-making problem can be constructed to maximize the same objective. Our main result is summarized as Theorem 1.

**Theorem 1**.: _Assuming the Markovian transition \(p_{*}(s_{t+1}|s_{t},a_{t})\) is known, the ground-truth conditional state distribution \(p^{*}(s_{t+1}|s_{0:t})\) for demonstration sequences is accessible, we can construct a sequential decision-making problem, based on a reward function \(r_{}(s_{t+1},s_{0:t}):= p_{}(a_{t}|s_{0:t})p_{*}(s_{t +1}|s_{t},a_{t})da_{t}\) for an arbitrary energy-based policy \(p_{}(a_{t}|s_{0:t})\). Its objective is_

\[_{t=0}^{T}_{p^{}(s_{0:t})}[V^{p_{}}(s_{0:t})]= _{p^{}(s_{0:T})}[_{t=0}^{T}_{k=t}^{T}r_{}( s_{k+1};s_{0:k})],\]

_where \(V^{p_{}}(s_{0:t}):=E_{p^{}(s_{t+1:T}|s_{0:t})}[_{k=t}^{T}r_{ }(s_{k+1};s_{0:k})]\) is the value function for \(p_{}\). This objective yields the same optimal policy as the Maximum Likelihood Estimation \(_{p^{}(s_{0:T})}[ p_{}(s_{0:T})]\)._

_If we further define a reward function \(r_{}(s_{t+1},a_{t},s_{0:t}):=r_{}(s_{t+1},s_{0:t})+ p_{} (a_{t}|s_{0:t})\) to construct a \(Q\) function for \(p_{}\)_

\[Q^{p_{}}(a_{t};s_{0:t}):=_{p^{}(s_{t+1}|s_{0:t})}[ r_{}(s_{t+1},a_{t},s_{0:t})+V^{p_{}}(s_{0:t+1})].\]

_The expected return of \(Q^{p_{}}(a_{t};s_{0:t})\) forms an alternative objective_

\[_{p_{}(a_{t}|s_{0:t})}[Q^{p_{}}(a_{t};s_{0:t})]=V^{p_{ }}(s_{0:t})-_{}(a_{t}|s_{0:t})-_{k=t+1}^{T-1} _{p^{}(s_{t+1:k}|s_{0:t})}[_{}(a_{k}|s_{0:k })]\]

_that yields the same optimal policy, for which the optimal \(Q^{}(a_{t};s_{0:t})\) can be the energy function._

_Only under certain conditions, this sequential decision-making problem is solvable through non-Markovian extensions of the maximum entropy reinforcement learning algorithms._

Proof.: See Appx B. 

The constructive proof above offers profound insights. By starting with the hypothesis of latent actions and MLE, and then considering known transition and accessible ground-truth conditional state distribution, we witness the _automatic emergence_ of the entire family of maximum entropy (inverse) RL. This includes prominent algorithms such as soft policy iteration , soft Q learning  and soft Actor-Critic (SAC) . Among them, SAC is the best-performing off-policy RL algorithm in practice. Unlike the formulation with joint state-action distribution , our formulation avoids the peculiarities associated with maximizing state transition entropy. The choice of the maximum entropy policy aligns naturally with the objective of capturing uncertainty in latent actions, and it offers inherent advantages for exploration in model-free learning .

### Model-based planning with posterior sampling

Lastly, with the learned model, we can do posterior sampling given any complete or incomplete state sequences. The computation involved is analogous to model-based planning. In Section 3.3, we introduce posterior sampling with short-run MCMC and importance sampling when we have the target next state, which generalizes all cases where the targets of immediate subsequent states are given. Here we introduce the complementary case, where the goal state \(s_{T}\) is given as the target.

The posterior of actions given the sequential context \(s_{0:t}\) and a target goal state \(s_{T}\) is

\[p_{}(a_{t:T}|s_{0:t},s_{T}) p_{}(a_{t:T},s_{T}|s_ {0:t})\] (15) \[= _{k=0}^{T-t-1}[p_{}(s_{t+k+1}|a_{t+k},s_{t+k})p_{}(a_{t+k}|s_{0:t+k})]p_{}(s_{T}|a_{T-1},s_{T-1}) ds_{t+1:T-1},\]

in which all Gaussian expectation \(_{p_{}}[]\) can be approximated with the mean . Therefore, \(a_{t:T}\) can be sampled via short-run MCMC with \(_{a_{t:T}} p_{}(a_{t:T},s_{T}|s_{0:t})\) back propagated through time. The learned prior can be used to initialize these samples and facilitate the MCMC mixing.

## 5 Experiments

### Cubic curve planning

To demonstrate the necessity of non-Markovian value and test the efficacy of the proposed model, we designed a motivating experiment. Path planning is a prototypical decision-making problem, in which actions are taken in a 2D space, with the x-y coordinates as states. To simplify the problem without loss of generality, we can further assume \(x_{t}\) to change with constant speed \(h\), such that the action is \( y_{t}\). Obviously, the transition model \((x_{t+1},y_{t+1})=(x_{t}+h,y_{t}+ y_{t})\) is Markovian.

Path planning can have various objectives. Imagining you are a passenger of an autonomous driving vehicle. You would not only care about whether the vehicle reaches the goal without collision but also how comfortable you feel. To obtain comforting smoothness and curvature, consider \(y\) is constrained to be a cubic polynomial \(F(x)=ax^{3}+bx^{2}+cx+d\) of \(x\), where \((a,b,c,d)\) are polynomial coefficients. Then the policy for this decision-making problem is non-Markovian.

To see that, suppose we are at \((x_{t},y_{t})\) at this moment, and the next state should be \((x_{t}+h,F(x_{t}+h))\). With Taylor expansion, we know \(F(x_{t}+h) F(x_{t})+F^{}(x_{t})h+(x_{t})}{2!}h^{2}+(x_{t})}{3!}h^{3}\), so we can have a representation for the policy, \(( y_{t}|x_{t},y_{t})=F^{}(x_{t})h+(x_{t} )}{2!}h^{2}+(x_{t})}{3!}h^{3}\). However, our representation of state only gives us \((x_{t},y_{t})\), so we will need to estimate those derivatives. This can be done with the finite difference method if we happen to remember the previous states \((x_{t-1},y_{t-1})\),..., \((x_{t-3},y_{t-3})\). Taking the highest order derivative for example, \(F^{}(x_{t})=(y_{t}-3y_{t-1}+3y_{t-2}-y_{t-3})/h^{3}\). It is thus apparent that the policy would not be possibly represented if we are Markovian or don't remember sufficiently many prior states.

This representation of policy is what models should learn through imitation. However, they should not know the polynomial structure a priori. Given a sufficient number of demonstrations with different combinations of polynomial coefficients, models are expected to discover this rule by themselves. This experiment is a minimum viable prototype for general non-Markovian decision-making. It can be easily extended to higher-order and higher-dimensional state sequences.

SettingWe employ multi-layer perception (MLP) for this experiment. Demonstrations can be generated by rejection sampling. We constrain the demonstration trajectories to the \((x,y)(-1,1)(-1,1)\) area, and randomly select \(y\) and \(y^{}\) at \(x=-1\) and \(x=1\). Curves with third-order coefficients less than 1 are rejected. Otherwise, the models may be confused in learning the cubic characteristics.

Non-Markovian dependency and latent energy-based policy are two prominent features of the proposed model. To test the causal role of non-Markovianness, we experiment with context length \(\{1,2,4,6\}\). Context length refers to how many prior states the policy is conditioned on. When it is 1, the policy is Markovian. From our analysis above, we know that context length 4 should be the ground truth, which helps categorize context lengths 2 and 6 into insufficient and excessive expressivity. With these four context lengths, we also train Behavior Cloning (BC) models as the control group. In a deterministic environment, there should not be a difference between BC and BCO, as the latter basically employs inverse dynamics to recover action labels. For our model, this simple transition can either be learned or implanted. Empirically, we don't notice a significant difference.

Performance is evaluated both qualitatively and quantitatively. As a 2D planning task, a visualization of the planned curves says a thousand words. In our experiment, we take \(h=0.1\), so the planned paths are rather discretized. We use mean squared error to fit a cubic polynomial and use the residual error as a metric. When calculating the residual error, we exclude those with a third-order coefficient is less than 0.5. Actually, the acceptance rate itself is also a viable metric. It is the number of accepted trajectories divided by the total number of testing trajectories. It is complementary to the residual error because it directly measures the understanding of cubic polynomials.

ResultsFig. 2(a-c) show paths generated with LanMDP after training for 3000 steps. They have context lengths 1, 2, 4 respectively. Compared with demonstrations in Fig. 2(d), only paths from the policy with context length 4 exhibit cubic characteristics. The Markovian policy totally fails this task. But it still generates curves, rather than straight lines from Markovian BC (see Fig. A1). The policy with context length 2 can plan cubic-like curves at times. But some of its generated paths are very different from demonstrations. To investigate this interesting phenomenon, we plot the training curves in Fig. 2(e)(f). While LanMDP policies with sufficient and excessive expressivity achieve high acceptance rates at the very beginning of the training, policies with Markovian and insufficient expressivity struggle to generate expected curves at the same time. Remarkably, as training goes by, the policy with context length 2, which can only approximate the ground-truth action in the first order, gradually improves in acceptance rate and residual error. This observation is consistent with Fig. 2(b).

Continuing our investigation, we plot curves generated by its BC counterparts in Fig. 2(g) but only see straight lines like the Markovian BC. Therefore, we conjecture that the LanMDP policy with length context 2 leverages its energy-based multi-modality to capture the uncertainty induced by marginalizing part of the necessary contexts. The second-order error in Taylor expansion is possibly remedied by this, especially after long-run training. The Markovian LanMDP policy, however, fails to unlock such potential because it cannot even figure out the first-order derivative.

There are some other note-worthy observations. (i) Excessive expressivity does not impair performance, it just requires more training. As shown in Fig. 2(e)(f), at the end of training, LanMDP policies with context length 6 perform as well as ones with context length 4. This demonstrates LanMDP's potential in inducing proper state abstraction from sequential contexts. TD learning, however, has been shown to be incapable of such abstraction in a prior work . (ii) BC policies with sufficient contexts do not perform as well as LanMDP, as shown in Fig. 2(e)(f). We conjecture that this might be attributed to the larger compounding error in BC. To shield the influence of compounding errors, we design an experiment where we measure the residual error of the next state after filling the historical contexts in the learned LanMDP context 4 and BC context 4 with expert states, rather than sampled states. The errors are both around 0.0004 for LanMDP and BC, closing the gap in Fig. 2f. The implication seems to be LanMDP is more robust to compounding errors than BC.

Figure 2: **Results for cubic curve generation. (a-c) show curves generated at training step 3000 with context lengths 1, 2, 4. Starting points are randomly selected, and all following are sampled from the policy model. Only models with context length 4 learn the cubic characteristic. (d) shows curves from demonstrations. (e) and (f) present the smoothed acceptance rate and fitting residual of trajectories from policies with context lengths 1, 2, 4, 6. The x-axis is the training steps. (e)(f) are better to be viewed together because residual errors will only be calculated if the acceptance rate is above a threshold. For context length 1, the acceptance rate is always zero for BC, so it is not plotted here. (g) shows curves planned by BC with context length 2. It can be compared with (b). Interestingly, LanMDP with context length 2 demonstrates certain cubic characteristics when trained sufficiently long, while the BC counterpart only plans straight lines. (h) is the result of goal-reaching planning, where the dashed line comes from a hand-designed Markov reward, the solid line from the trained LanMDP.**

To verify our analysis in Section 4, we visualize the non-Markovian value function defined in Theorem 1 in Fig. 3. The value increases monotonically when the policy generates the cubic curve step by step. In an animation we included on the project homepage1, we further show that the action sampling at each state yields the highest value in reachable next states.

At last, we study repurposing the learned sequence model for goal-reaching. This is inspired by a surprising phenomenon, over-imitation, from psychology. Over-imitation occurs when imitators copy actions unnecessary for goal-reaching. In a seminal study , 3- to 4-year-old children and young chimpanzees were presented with a puzzle box containing a hidden treat. An experimenter demonstrated a goal-reaching sequence with both causally necessary and unnecessary actions. When the box was opaque, both chimpanzees and children tended to copy all actions. However, when a transparent box was used such that the causal mechanisms became apparent, chimpanzees omitted unnecessary actions, while human children imitated them. As shown in Fig. 2(h), planning with the learned non-Markovian value indeed leads to casually unnecessary states, consistent with the demonstrations. Planning with designed Markov rewards produces causally shortest paths.

### Mujoco control tasks

We also report the empirical results of our model and baseline models on MuJoCo control tasks: Cartpole-v1, Reacher-v2, Swimmer-v3, Hopper-v2 and Walker2d-v2. We train an expert for each task using PPO . They are then used to generate 10 trajectories for each task as demonstrations. Actions are deleted in the state-only setting.

SettingWe conduct a comparative analysis of LanMDP against several established imitation learning baselines including BC , BCO , GAIL , GAIFO , and OPOLO . Note that BC and GAIL have access to action labels, positioning them as the control group. The experimental group includes state-only methods such as LanMDP, BCO, GAIFO, and OPOLO. The expert is the idealized baseline. For all tasks, we adopt the MLP architecture for both transition and policy. The input and output dimensions are adapted to the state and action spaces in different tasks, and so are short-run sampling steps. Sequential contexts are extracted from stored episodic memory. The number of neurons in the input and hidden layer in the policy MLP varies according to the context length. We use replay buffers to store the self-interaction experiences for training the transition model offline. See Appendix D for detailed information on network architectures and hyper-parameters.

ResultsResults for context length 1 are illustrated through learning curves and a bar plot in Fig. 4. These learning curves are the average progress across 5 seeds. Scores in the bar plot are normalized relative to the expert score. Our model demonstrates significantly steeper learning curves compared to the state-only GAIFO baselines, especially in Cartpole and Walker2d. This illustrates the remarkable data efficiency of model-based methods. Additionally, LanMDP consistently matches or surpasses the performance of BC and GAIL, despite the latter having access to action labels. In comparison to the expert, LanMDP only lags behind in the most complex Walker2d task. However, it still maintains a noticeable margin over other state-only baselines.

Figure 3: **Mapping a generated curve to a trajectory in the value landscape. We train a neural network to approximate the non-Markovian value function constructed with the learned policy and transition following Theorem 1, and then visualize the landscape by projecting all history-augmented states to a 2D space with a top view (left) and a front view (middle). Starting from a random initial state, decisions are sequentially made according to the learned policy, leaving a curve in the original state space (right) and a trajectory on the value landscape. It is evident that the non-Markovian value increases monotonically along the trajectory.**

Results for longer context lengths, _i.e_. the non-Markovian setting, are reported in Table 1, in which the highest return across the training process is listed. Originally invented for studying differentiable dynamics, MuJoCo offers state features that are inherently Markovian. Though a MDP is sufficiently expressive, learning a more generalized nMDP does not impair the performance. Sometimes it can even improve a little bit. Due to the limit of time, the maximum context length is only 3. Within the investigated regime, our result is consistent with that reported by Janner et al. . We leave the experiments with longer memory and more sophisticated neural networks to future research.

Table 2 is a study of the computational overhead for the sampling techniques involved. The short-run MCMC for posterior inference takes longer than a single step of gradient descent. Replacing it with the proposed importance sampling improves training efficiency by a large margin.

   Task & context 3 & context 2 & context 1 & BC \\  CartPole & **500.00\(\)0.00** & **500.00\(\)0.00** & **500.00\(\)0.00** & 474.80\(\)18.87 \\ Reacher & -10.91\(\)0.73 & -9.70\(\)0.64 & -9.00\(\)0.87 & **-8.76\(\)0.12** \\ Swimmer & **42.67\(\)4.66** & **43.52\(\)4.31** & 41.22\(\)2.67 & 38.64\(\)1.76 \\ Hopper & 3051.16\(\)111.78 & 3053.91\(\)176.5 & 3045.27\(\)240.45 & **3083.32\(\)156.61** \\ Walker2d & 1703.02\(\)228.86 & **1811.77\(\)369.54** & 1753.46\(\)193.69 & **1839.94\(\)376.87** \\   

Table 1: **Comparison between Markovian and non-Markovian policy in MuJoCo control task.** Context length is the number of prior sequential states that the policy depends on, with the current one included. Recall that these MuJoCo tasks are inherently Markovian, thanks to highly specified state features. Nevertheless, non-Markovian policies perform on par with Markovian ones and BC, despite having higher expressivity than sufficient. The best and the second-best results are highlighted. Results are averaged over 5 random seeds.

Figure 4: **Results in MuJoCo.** for our LanMDP (red), BC (orange), BCO (green), GAIL (purple), GAIFO (cyan), OPOLO (gray), expert (blue). The learning curves are obtained by averaging progress over 5 seeds. We only plot curves for interactive learning methods. The scores of all other methods are plotted as horizontal lines. LanMDP does not have performance scores in the first \(K\) steps because this data is collected with random policy to fill the replay buffer, which is then used to train the transition model. \(K=0\) for Cartpole, \(2e4\) for Reacher and Swimmer, \(2e5\) for Hopper and Walker2d. We include these steps for fair comparisons. LanMDP outperforms existing state-only methods and matches BC, the best-performing state-action counterpart. The bar plot presents the scores from the best-performing policy during the training process, averaged across 5 seeds and normalized with the expert mean. The score in Reacher is offset by a constant before the division of the expert mean to align with the positive scores in all other tasks. The expert mean is plotted as a horizontal line. Our model clearly stands out in state-only methods, while matching and even outperforming those with action labels. Its scores only lag behind the expert mean in the most complex task. Better viewed in color.

## 6 Discussion

Related work in imitation learningEarliest works in imitation learning utilized BC [3; 47]. When the training data is limited, temporal drifting in trajectories [48; 49] may occur, which led to the development of IRL [6; 50; 51; 4; 34; 35]. In recent years, the availability of abundant sequence/video data is not the primary concern, but rather the difficulty in obtaining action labels. There has since been increasing attention in ILfO [52; 53; 7; 38; 8], a setting similar to ours. Distinguished from existing ILfO solutions, our model probabilistically describes the entire trajectory. In particular, the energy-based model [54; 55] in the latent policy space  has been relatively unexplored. Additionally, the capability for model-based planning is also a novel contribution.

Limitation and potential impactThe proposed model factorizes the joint distribution of state-action sequences into a time-invariant causal transition and a latent policy modulated by sequential contexts. While this model requires sampling methods, and can be non-negligible for higher-dimensional actions, it is worth noting that action quantization, as employed in transformer-based models [12; 13], has the potential to reduce the computation overhead. In our experiments, a measure of the diversity of behavior is omitted, similar to other works in the literature of reinforcement learning. However, it deserves further investigation since multi-modal density matching is a crucial metric in generative modeling. Importantly, our training objective and analysis are independent of specific modeling and sampling techniques, as long as the state transition remains time-invariant. Given the ability of neural networks to learn approximate invariance through data augmentation [56; 57; 58; 59], we anticipate that our work will inspire novel training and inference techniques for monolithic sequential decision-making models [12; 13; 14; 15].

Implications in neuroscience and psychologyThe proposed latent model is an amenable framework for studying the emergent patterns in the mirror neurons [60; 61], echoing recent studies in grid cells and place cells [62; 63]. When the latent action is interpreted as an internal intention, the inference process is a manifestation of Theory of Mind (ToM) . The phenomenon of over-imitation [25; 26; 65] can also be relevant. As shown in Section 5, although the proposed model learns a causal transition and hence understands causality, when repurposed for goal-reaching tasks, the learned non-Markovian value can result in "unnecessary" state visitation. It would be interesting to explore if over-imitation is simply an overfitting due to excessive expressivity in sequence models.

## 7 Conclusion

In this study, we explore deep generative modeling of state-only sequences in non-Markovian domains. We propose a model, LanMDP, in which the policy functions as an energy-based prior within the latent space of the state transition generator. This model learns by EM-style maximum likelihood estimation. Additionally, we demonstrate the existence of a decision-making problem inherent in such probabilistic inference, providing a fresh perspective on maximum entropy reinforcement learning. To showcase the importance of non-Markovian dependency and evaluate the effectiveness of our proposed model, we introduce a specific experiment called cubic curve planning. Our empirical results also demonstrate the robust performance of LanMDP across the MuJoCo suite.

AcknowledgmentAQ, QL and SZ are supported by the National Key R&D Program of China (2021ZD0150200). FG contributed to this work during his PhD study at UCLA. SX is supported by a Teaching Assistantship from UCLA CS. The authors thank Prof. Ying Nian Wu at UCLA, Baoxiong

   Task & \(a\) dim & \(s\) dim & Architecture & 10 steps & 50 steps \\  Reacher & 2 & 11 & MLP(150;4) & 0.0108/0.0076/0.0014 & 0.0480/0.0350/0.0014 \\ Swimmer & 2 & 8 & MLP(150;4) & 0.0100/0.0071/0.0014 & 0.0463/0.0340/0.0014 \\ Hopper & 3 & 12 & MLP(512;4) & 0.0268/0.0170/0.0074 & 0.1403/0.0836/0.0073 \\ Walker2d & 6 & 18 & MLP(512;4) & 0.0282/0.0184/0.0077 & 0.1487/0.0899/0.0076 \\   

Table 2: Computational overheads for posterior sampling, importance sampling, and gradient descent (in seconds) in one training step. MLP(\(n\),\(m\)) means that we implement the policy model as an MLP with \(m\) layers and \(n\) hidden neurons each layer. Results are averaged over 10 epochs. The number of MCMC steps is set to 10, 50 respectively. Replacing posterior sampling with importance sampling improves training efficiency.

Jia and Xiaojian Ma at BIGAI for their useful discussion. The authors would also like to thank the reviewers for their valuable feedback.