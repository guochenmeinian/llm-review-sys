# Latent Paraphrasing: Perturbation on Layers Improves Knowledge Injection in Language Models

Minki Kang\({}^{1,2}\) Sung Ju Hwang\({}^{2}\) Gibbeum Lee\({}^{1}\) Jaewoong Cho\({}^{1}\)

\({}^{1}\)Krafton, \({}^{2}\)Kaist

zzxc1133@krafton.com, sjhwang@kaist.ac.kr, {pirensisco, jwcho}@krafton.com

###### Abstract

As Large Language Models (LLMs) are increasingly deployed in specialized domains with continuously evolving knowledge, the need for timely and precise knowledge injection has become essential. Fine-tuning with paraphrased data is a common approach to enhance knowledge injection, yet it faces two significant challenges: high computational costs due to repetitive external model usage and limited sample diversity. To this end, we introduce LaPael, a latent-level paraphrasing method that applies input-dependent noise to early LLM layers. This approach enables diverse and semantically consistent augmentations directly within the model. Furthermore, it eliminates the recurring costs of paraphrase generation for each knowledge update. Our extensive experiments on question-answering benchmarks demonstrate that LaPael improves knowledge injection over standard fine-tuning and existing noise-based approaches. Additionally, combining LaPael with data-level paraphrasing further enhances performance.

## 1 Introduction

Pre-trained Large Language Models (LLMs) encode extensive factual information from their training data, enabling them to answer factoid questions such as "Who is the director of Dune: Part Two?" . However, knowledge in LLMs is static, which can lead to outdated information as real-world knowledge evolves. Additionally, LLMs often lack specificity for specialized or private domains. To address this, it is common practice to fine-tune LLMs with updated or domain-specific documents, keeping the model's knowledge up-to-date and enhancing expertise in particular domains .

However, does fine-tuning LLMs on a single document allow them to fully internalize its knowledge? Even in pre-training, Kandpal et al.  found that LLMs cannot perfectly learn all the information in the training data, particularly long-tail knowledge that appears rarely or only once. Existing work  has shown that this issue persists with fine-tuning and suggested that data augmentation, such as paraphrasing, is a simple yet effective way to enhance knowledge injection. As shown in Figure 1, fine-tuning with paraphrases enhances knowledge injection, as evidenced by improved Question-Answering (QA) task performance.

While data-augmented approach via paraphrasing is effective for knowledge learning, it has two main limitations: (1) **High computational cost:** Generating high-quality paraphrases requires significant computational resources. As shown in Figure 2, paraphrasing models such as LLMs  need to repeatedly generate paraphrases for each document with the new incoming knowledge. This leads to higher costs as the number of documents being learned continually increases; and (2) **Limited

Figure 1: Effect of paraphrasing data in knowledge injection.

diversity in augmented data:** Although LLMs can produce varying high-quality paraphrases by sampling from the generative distribution, the diversity of the generated text is limited, resulting in a narrow range of augmented samples at the discrete data level. One way to overcome these issues is to introduce noise into the token embedding. However, existing works [16; 57] do not consider the text semantics when they perturb the latent features of LLMs with randomly generated noise.

To address these issues, we take a distinct approach using an input-dependent noise generator named "latent paraphraser" learned from the paraphrases. Specifically, this function perturbs early layers to augment LLMs at the latent level while preserving the meaning of the text. To optimize the latent paraphraser, we start by generating paraphrases of the documents. Then, we train the latent paraphrasers to ensure that the latent distribution of the LLMs with the original sentence is close to the latent distribution with the paraphrased sentences. Once training is done, we can transfer the latent paraphrasers to the documents from any domain that contains new knowledge. We refer to our method as **L**atent **P**araphrasing of Language Models (**LaPael**), as it learns the paraphrasing of text data at the latent level.

We validate our approach on diverse question-answering benchmark datasets [38; 27; 51] designed to evaluate knowledge injection. These benchmarks involve fine-tuning LLMs on documents that contain the knowledge required to answer the questions in the datasets. Our results show that LaPael significantly improves knowledge injection performance compared to standard fine-tuning. Moreover, LaPael outperforms fine-tuning with paraphrases, demonstrating that LaPael alone is sufficient for data augmentation in knowledge injection scenarios, as illustrated in Figure 2. As shown in Figure 1, we further find that using LaPael in combination with paraphrases further enhances performance, providing complementary benefits to data-level augmentations. Finally, LaPael surpasses existing noise baselines [16; 57], highlighting the importance of learning noise for effective augmentations.

Our contributions are as follows:

* We introduce **LaPael**, a new method that applies learned perturbations to the layers of LLMs to enhance knowledge injection, addressing the limitations of data augmentations and noise baselines.
* We validate LaPael using diverse question-answering benchmark datasets, demonstrating a significant improvement in knowledge injection performance compared to standard fine-tuning.
* Our results show that LaPael not only outperforms fine-tuning with paraphrases but also complements it, providing additional benefits when used together, surpassing the performance of existing latent noise-based methods.

## 2 Related Work

Knowledge of Large Language ModelsLarge Language Models (LLMs) store vast amounts of factual knowledge in their pre-trained parameters [36; 44]. The straightforward way to extract the

Figure 2: **A conceptual illustration of the proposed approach. On the left, we show the existing method of knowledge injection by paraphrasing each document for data-level augmentation. On the right, we present the conceptual illustration of LaPael with _trained_ latent paraphrasers. Unlike the method on the left, LaPael can eliminate the need for users to repeatedly paraphrase using LLMs once latent paraphrasers are trained.**

knowledge of LLMs is to ask the question that requires factual knowledge [43; 58]. Through asking questions, Kandpal et al.  have found that LLMs cannot perfectly memorize the entire knowledge in the pre-training corpora, especially for knowledge that appears rarely or only once. To make LLMs answer the question requires under-represented or new knowledge, previous works have clustered into two different solutions. The first one is retrieval-augmented methods [26; 39; 42] that retrieve knowledge from an external knowledge base and input the retrieved knowledge alongside the question into LLMs. The second one is fine-tuning [12; 17] where the parameters of pre-trained LLMs are continually updated by fine-tuned on the document containing knowledge in an unsupervised way as in pre-training . In our work, we focus on improving the fine-tuning-based solution, as storing new knowledge in the parameters of LLMs is efficient since we can reduce the length of the input prompt and do not need any extra module or memory in the deployment time .

Knowledge Injection in LLMsIn this work, knowledge injection in LLMs denotes fine-tuning LLMs on the set of documents to inject new or under-represented knowledge into LLMs [33; 17], different from another task of injecting _symbolic knowledge_ (e.g., knowledge graph) into LLMs [55; 54]. Among previous works, CaMeLS  has introduced a meta-learning method for learnable loss scaling function that improves knowledge injection. As a concurrent work, MAC  has proposed using the memory of amortized context is highly effective in a knowledge injection. However, both methods have drawbacks like high computational costs for bi-level optimization or the need for additional modules and memory. Recent works [33; 58] have shown that data augmentation which paraphrases the knowledge-containing sentences helps language models memorize knowledge in a more extractable format (e.g., asking questions) after knowledge injection. Furthermore, Jiang et al.  has shown that the instruction-tuned model is better at learning new knowledge. Compared to previous works, we focus on developing an alternative method to data augmentation that perturbs the latent representation of LLMs for better knowledge injection.

Data Augmentation and Latent PerturbationThe usefulness of data augmentations for text data was empirically observed in the literature. For instance, EDA  has introduced simple data augmentation method which randomly deletes, swaps, replaces, and inserts the words. Other previous works [22; 5; 30] have utilized the trained LMs to augment the text data. Recently, Maini et al.  has shown that adding data rephrased by LLMs into the pre-training corpus improves the performance of LM pre-training. However, those methods require additional costs in the knowledge injection as it utilize the LLMs to rephrase the text. In contrast, the latent perturbations offer an orthogonal approach to improve the robustness of neural networks, complementing data augmentation. This technique has been employed in meta-learning and out-of-distribution generalization [24; 25; 40]. For instance, NEFTune  demonstrated that adding noise, randomly sampled from a uniform distribution, to token embedding layers improves instruction tuning performance. Expanding on the concept of latent perturbations, our work introduces a novel approach that _internalizes_ the effects of text paraphrasing by identifying optimal latent perturbations through training a small neural network within the LLMs.

## 3 Problem Formulation

In this work, we follow the knowledge injection setting outlined by Ovadia et al. . We are given three resources: (1) documents \(_{}\) containing knowledge that we are interested to inject; (2) question & answering dataset \(_{}=\{(^{(i)},^{(i)})\}_{ i=1}^{n}\) for verifying injected knowledge from \(_{}\); and (3) a pre-trained Large Language Models (LLMs) \(p_{}()\) parameterized by \(\). Our objective is to find a transformation \(F\) that could enhance the knowledge about \(_{}\):

\[^{}=F(,_{}) (^{},_{})>(, _{}),\] (1)

where the score function \(\) is defined as:

\[(,_{})^{n} (f(p_{}(^{(i)}))=^{(i)})}{n},\] (2)

and \(()\) and \(f()\) denote the indicator function and a decoding function that samples a sequence of tokens from \(p_{}\), respectively.

In general, a transformation \(F\) is a fine-tuning LLMs on documents in \(_{}\) by optimizing \(\) to minimize the negative log-likelihood of each token in each document as follows :

\[^{*}=_{}_{}|}_{ _{}}(|}_{t=1}^{||}- p_{ }(_{t}_{<t})),\] (3)

where \(||\) denotes the length of token sequence \(\).

## 4 Proposed Method

We propose Latent Paraphrasing of Language Models (LaPael), a framework that perturbs the latent feature of LLMs, to achieve the equivalent effect of data augmentation at the latent level. Knowledge injection using LaP consists of the following four processes: paraphrasing the set of documents to make the paraphrased data (Section 4.1), training the latent paraphrasers with paraphrased data (Section 4.2), fine-tuning LLMs with the trained latent paraphrasers on \(_{}\) and evaluate the injected knowledge of LLMs on \(_{}\) (Section 4.3).

### Data Augmentation: Paraphrasing

To train the latent paraphrasers, we need a distinct set of training data \(_{}=\{^{(i)}\}_{i=1}^{N}\) which consists of documents having different knowledge with \(_{}\). As a preliminary, we formulate the paraphrasing of the text in terms of the **knowledge equivalence**, which is a narrower concept than semantic equivalence  where two different sentences can contain the same knowledge. We consider that each sentence \(\) in \(_{}\) can be decomposed into words for the object (entity or attribute) of the knowledge (\(\)) and others (\(\)) where both are the sequence of tokens. For instance, given the sentence _"The capital of the United States is Washington D.C."_,

\[=;=,\]

represent the knowledge (United States, capital, Washington D.C.). Then, we paraphrase a sentence \(=(,)\) into a paraphrased sentence1. For the above sentence, a paraphrased sentence can be

\[^{}=\]

with the same \(\), which is knowledge equivalent to \((,)\). For each knowledge \(\), we assume that there is a set of the knowledge equivalent sentences \(S()\) where \((,) S()\). We generate \(K\) paraphrased sentence via a LLM: \((_{1},),,(_{K},) p_{}(^{ }|,,)\). Then, we have the set of paraphrased data \(\{\{(_{k}^{(i)},^{(i)})\}_{k=1}^{K}\}_{i=1}^{N}\) of \(_{}\). We define \(p(^{}|) p_{}(^{}|,,)\) which denotes the probability distribution of paraphrases given the original sentence.

Figure 3: **(a) Illustration of the latent paraphraser. The linear layer embeds each token’s latent feature \(\) into \(\). We then sample stochastic noise \(\) from \((,)\) and apply a mask \(m_{t}\) to control the scale. (b) Training pipeline of LaPael. To train the latent paraphraser, we estimate the parameters of Gaussian distributions. We then minimize the KL divergence between these distributions to optimize the latent paraphrasers.**

### Introducing Latent Paraphraser

Latent ParaphraserWe introduce a latent paraphraser within a transformer layer , which augments a latent feature and is expected to paraphrase the given input text within the latent space. As illustrated in Figure 3(a), within the transformer architecture, we insert this new layer just before the Multi-layer Perceptron (MLP), using the output from the second LayerNorm as its input.

Let \(^{d}\) denote the latent feature after the second LayerNorm. The latent paraphraser, denoted by \(g_{}:^{d}^{d}\) and parameterized by \(\), augments the latent feature as follows:

\[ g_{}(),\] (4)

where \(\) is the element-wise multiplication. The function \(g_{}()\) is given by:

\[g_{}()=(1-m)+m,\] (5)

with \(^{d}\) and \(m\) representing a noise vector and a learnable mask, respectively.

The noise vector \(\) is generated by

\[=(_{}()), (,),=_{}+_{},\] (6)

where \(_{}\) is a 2-layers MLP. We use the reparameterization trick  to enable the back-propagation through the sampling from the Gaussian distribution: \(=+\), where \((0,)\).

To modulate the scale of perturbation for individual tokens, we employ a learnable mask. It is important as too much noise on key tokens (e.g., United States) might hurt the semantics of the sequence. For learnable binary mask, we use concrete distribution to approximate the sampling discrete random variable from a Bernoulli distribution using continuous relaxation  as follows:

\[m=((u)+(1-u)+), =_{m}+b_{m},\] (7)

where \(u(0,1)\), \(\) is temperature, and \(m\) is mask value in scalar.

TrainingThen, how do we train the latent paraphrasers to approximate optimal perturbation functions for estimating the distribution of the paraphrased text? We employ the dataset with paraphrases \(\{\{(_{k}^{(i)},^{(i)})\}_{k=1}^{K}\}_{i=1}^{N}\) generated in Section 4.1. Our objective is to match two distributions for each transformer layer:

1. the distribution of transformer layer output feature for the last token \(_{}\) without the latent paraphraser given the data perturbation distribution \(p(^{}|)\) from Section 4.1: \[p_{}(_{}|)= p_{}(_{}|^{})p(^{}|)d^{};\] (8)
2. the distribution of output feature for the last token \(_{}\) with the latent paraphraser given \(\), \(p_{,}(_{}|)\). As a latent paraphraser outputs stochastic noise, we can formulate the probabilistic distribution \(p_{,}(_{}|)\) as follows: \[p_{,}(_{}|)= p_{}(_{} ,)p_{,}()d,\] (9) where \(p_{,}()\) is the distribution for noise from the latent paraphraser in Equation (6).

We make the simplistic parametric assumption that both distributions are Gaussian:

\[p_{}(_{}|)(_{}; {}_{},_{}^{2}); p_{, }(_{}|)(_{};_{ },_{}^{2}).\] (10)

To train latent paraphrasers, we minimize the symmetric Kullback-Leibler (KL) divergence between two estimated Gaussian distributions of each layer as follows:

\[_{}()=(_{}(p _{}(_{}|)\|p_{,}(_{}|))+_{}(p_{,}(_{}|)\|p_{ }(_{}|))),\] (11) \[_{}(p_{}(_{}|)\|p_{ ,}(_{}|))=(}_{ }}{}_{}})+} _{}^{2}+(}_{}-}_{})^{2}}{2}_{}^{2}}-.\] (12)We employ a Monte Carlo sampling approach to estimate the parameters of Gaussian distributions. We generate \(N\) samples \(_{}^{(1)},,_{}^{(N)}\) from the distribution \(p_{,}(_{})\). Then, we estimate the empirical mean and standard deviation from the samples as follows:

\[}_{}=_{i=1}^{N}_{ }^{(i)},}_{}=_{i=1} ^{N}(_{}^{(i)}-}_{})^{2}},\] (13)

and we use \(K\) paraphrases \(_{1},,_{K}\) to obtain \(K\) samples \(_{}^{(1)},,_{}^{(K)}\) from the distribution \(p_{}(_{})\). Then we estimate the parameters in the same way:

\[}_{}=_{k=1}^{K}_{}^{( k)},}_{}=_{k=1}^{K}( _{}^{(k)}-}_{})^{2}}.\] (14)

We further use the auxiliary loss for mask training, with the sequence length of \(T\) as follows:

\[_{}()=_{t=1}^{T}(|(_{t})-r T|+|(_{t})-_{t}|),\] (15)

where \(_{T}\) is defined in Equation (7), \(r\) is the mask ratio that controls the number of masks and \(_{t}\) is the gold mask where \(_{t}=0\) for tokens that correspond to the named entity.

To sum up, we optimize the latent paraphraser parameter \(\) by minimizing the following loss:

\[^{*}=_{}_{_{}} (_{}()+_{}() ).\] (16)

See Figure 3(b) for an illustration of the training process for the latent paraphraser.

### Fine-tuning the LLM with the Trained Latent Paraphrasers

We fine-tune the LLM on documents containing knowledge to be injected (\(_{}\)) as in Equation (3). We use the trained latent paraphraser parameterized by \(^{*}\) during LLM fine-tuning as follows:

\[^{*}=_{}_{}|}_{ _{}}(|}_{t=1}^{||} (_{j=1}^{N}- p_{,^{*}}(_{t} _{t}^{(j)},_{<t})p_{,^{*}}(_{t}^{(j)}_{<t}) )),\] (17)

where we sample \(N\) noise \(^{(j)}\) by sampling multiple \(\) from Gaussian distribution as defined in Equation (6). Then, we evaluate the knowledge injected in LLMs by measuring \((^{*},_{})\) as defined in Equation (2).

## 5 Experiments

In experiments, we validate the effectiveness of the proposed method, LaPael, in injecting new or under-represented knowledge into Large Language Models (LLMs).

### Experimental Setting

#### 5.1.1 Datasets

To follow the experimental setup in Section 3, we need (1) documents containing knowledge \(_{}\) and (2) associated QA datasets \(_{}\). We mainly use the test split of three QA datasets: SQuAD , StreamingQA , and ArchivalQA  for the source of \(_{}\) and \(_{}\) in our main experiments. These datasets, previously used in Hu et al. , consist of documents paired with their corresponding QAs, making them well-suited to our experimental setup. While the questions in these datasets are of decent quality, a significant limitation lies in the documents provided. These documents are likely to have been seen by LLMs during pre-training, making it difficult to accurately assess the performance of methods on injecting new knowledge.

To mitigate this issue, we incorporate two datasets with synthetic QAs - Films 2024 and Events 2024. These are QA datasets generated from raw Wikipedia articles under the 2024 films category and from US events in May, June, and July 2024, in the 2024 events in the United States category. We generated question-answer pairs from these documents using GPT-4o following methods from previous works . Since the documents used to generate these datasets were not seen by the LLMs during pre-training, we can better evaluate the effectiveness of each method for knowledge injection especially on new knowledge.

Datasets with Synthetic DocumentsThe raw documents from datasets are unsuitable for precisely measuring the knowledge injection performance. Specifically, fine-tuning LLMs on a document does not always ensure that LLMs can answer the associated questions, due to the reversal curse . Moreover, documents often contain irrelevant knowledge that may hinder the accurate assessment of knowledge injection .

To address these issues, we conduct evaluations under the setting of synthetic documents. For generating synthetic documents, we construct \(_{}\) by rephrasing each question and answer in \(_{}\) using GPT-4-turbo , ensuring that fine-tuning on these synthetic documents guarantee that LLMs become answerable to the associated questions. Examples of questions, synthetic, and raw documents are shown in Table 1. To make a difference, we denote the dataset under the synthetic document setting with the suffix '-syn' and the raw document setting with the suffix '-raw'.

Datasets for Training Latent ParaphrasersFor training our latent paraphrasers, the set of training data \(_{}\) is required in addition to \(_{}\). Therefore, we use GPT-3.5-turbo  to generate the set of synthetic sentences from the subset of a training split of each QA dataset, where each sentence must be with the answer to questions, following the sentence format in Section 4.1.

#### 5.1.2 Experimental Details

BaselinesWe compare our LaPael against several baselines. All models are fine-tuned on the documents in \(_{}\) unless explicitly stated otherwise. **(1) No Injection.** We use the pre-trained LLM without any fine-tuning. **(2) Fine-Tuning.** We fine-tune the LLM on \(_{}\). **(3) Fine-Tuning _(seq)_. We first fine-tune the LLM on the paraphrased documents of \(_{}\). Then, we fine-tune the LLM on \(_{}\). **(4) Fine-Tuning _(+ para)_. We fine-tune LLM on the original and paraphrased documents of \(_{}\). **(5) FreeLB**. We add trained adversarial noise to the token embedding while fine-tuning. **(6) NEFTune. We add random uniform noise to the token embedding while fine-tuning. **(7) LaPael (ours).** We train the latent paraphrasers on \(_{}\) and then fine-tune the model on \(_{}\).

    &  &  &  \\ 
**Method** & EM & Recall & F1 & EM & Recall & F1 & EM & Recall & F1 \\ 
**No Injection** & 13.10 & 22.91 & 21.09 & 16.39 & 26.30 & 23.71 & 13.50 & 25.07 & 22.12 \\
**Fine-Tuning** & 66.30 & 79.32 & 76.11 & 82.08 & 88.98 & 88.29 & 62.60 & 79.51 & 76.16 \\
**Fine-Tuning** (_seq._) & 67.60 & 80.30 & 77.39 & 77.95 & 86.36 & 85.23 & 56.30 & 79.17 & 74.12 \\
**FreeLB** & 70.70 & 82.41 & 79.67 & 82.24 & 89.48 & 88.56 & 63.20 & 81.30 & 77.67 \\
**NEFtune** & 68.30 & 80.93 & 77.91 & 81.47 & 88.66 & 87.77 & 61.90 & 78.90 & 75.81 \\
**Ours trained w/ \(50\) sents**. & 70.77 & 84.96 & 81.66 & **86.16** & **93.01** & **92.12** & 68.37 & 86.24 & 82.67 \\
**Ours trained w/ \(1\)**sents**. & **72.47** & **87.93** & **84.50** & 84.48 & 92.42 & 91.33 & **68.37** & **88.99** & **84.75** \\
**Fine-Tuning** (+ para)\(\) & 68.50 & 85.12 & 80.51 & 85.45 & **93.67** & **92.32** & 64.90 & 85.92 & 81.24 \\   

Table 2: Experimental results on datasets with **synthetic documents**. _trained with n sents_ means that latent paraphrasers are trained with the dataset containing \(n\) sentences. For ours, we report the average performance of three runs. \(\) denotes the method that uses 10 times more additional data (paraphrases).

    &  &  &  \\ 
**Method** & EM & Recall & F1 & EM & Recall & F1 & EM & Recall & F1 \\ 
**No Injection** & 13.10 & 22.91 & 21.09 & 16.39 & 26.30 & 23.71 & 13.50 & 25.07 & 22.12 \\
**Fine-Tuning** & 66.30 & 79.32 & 76.11 & 82.08 & 89.89 & 88.29 & 62.60 & 79.51 & 76.16 \\
**Fine-Tuning** (_seq._) & 67.60 & 80.30 & 77.39 & 77.95 & 86.36 & 85.23 & 56.30 & 79.17 & 74.12 \\
**FreeLB** & 70.70 & 82.41 & 79.67 & 82.24 & 89.48 & 88.56 & 63.20 & 81.30 & 77.67 \\
**NEFtune** & 68.30 & 80.93 & 77.91 & 81.47 & 88.66 & 87.77 & 61.90 & 78.90 & 75.81 \\
**Ours trained w/ \(50\) sents**. & 70.77 & 84.96 & 81.66 & **86.16** & **93.01** & **92.12** & 68.37 & 86.24 & 82.67 \\
**Ours trained w/ \(1\)**sents**. & **72.47** & **87.93** & **84.50** & 84.48 & 92.42 & 91.33 & **68.37** & **88.99** & **84.75** \\
**Fine-Tuning** (+ para)\(\) & 68.50 & 85.12 & 80.51 & 85.45 & **93.67** & **92.32** & 64.90 & 85.92 & 81.24 \\   

Table 1: **Data Example.** Example data from SQuAD and StreamingQA dataset we used in experiments. Words in the yellow background indicate the answer to the question. More examples are in Table 12 of the Appendix.

Training & InferenceWe mainly use Vicuna-7b-v1.5  for fine-tuning, which is the instruction-tuned version of Llama-2-7b  for our experiments. We fine-tune LLMs for 12 epochs with a learning rate of \(0.00005\) and step learning rate scheduler where we decay a learning rate by 0.85 by every 4 epochs. For inference, we use in-context learning with 5 examples by prompting the 5 examples in the prompt . To measure QA accuracy, we use Exact Match (EM), Recall (Rec.), and F1 score. More details on the experimental setting are provided in the Appendix C.

### Experimental Results

Experiments with Synthetic DocumentsIn Table 2, we present the experimental results for the synthetic documents setting. Fine-tuning does improve the QA performance of LLMs, but it does not lead to near-perfect scores even though the synthetic document contains the necessary knowledge for answering the questions, as shown in Table 1.

Our experiments show that paraphrasing documents for fine-tuning consistently improves QA performance across all three benchmarks. Notably, LaPael demonstrates performance comparable to fine-tuning with paraphrases on StreamingQA and even outperforms it on two other benchmarks. These findings suggest that the latent paraphrasers learn an effective noise distribution that aids knowledge injection without additional data augmentation.

We also compared LaPael with two other noise-based methods, FreeLB  and NEFTune , to validate that the latent-level noise generated by latent paraphrasers is more effective. As shown in Table 2, LaPael outperforms these baselines, confirming the strength of our approach.

Experiments with Raw DocumentsWhile our method has proven effective for knowledge injection with synthetic documents, it is important to evaluate its performance on raw documents, which represent a more realistic data format. To demonstrate the applicability of our method to real-world data, we conducted experiments in which we fine-tuned LLMs on raw documents for each dataset, using latent paraphrasers trained on \(_{}\) from SQuAD-syn.

As shown in Table 3, our method outperforms both fine-tuning and noise-based baselines in the context of knowledge injection with raw documents. Considering that the latent paraphrasers were trained on synthetic sentences from \(_{}\), these results demonstrate their effectiveness on documents with a different format than those used in training.

Cross-domain TransferOnce trained, the latent paraphrasers can be applied to fine-tune LLMs on documents from any domain. To demonstrate this, we conducted cross-domain transfer experiments. Specifically, we trained latent paraphrasers on \(_{}\) from a source domain (e.g., SQuAD) and fine-tuned LLMs with the trained latent paraphrasers on \(_{}\) from a target domain (e.g., StreamingQA).

    &  &  &  &  \\ 
**Method** & EM & Rec. & F1 & EM & Rec. & F1 & EM & Rec. & F1 & EM & Rec. & F1 \\ 
**No Injection** & 13.10 & 22.91 & 21.09 & 16.39 & 26.30 & 23.71 & 9.17 & 18.21 & 16.05 & 39.00 & 48.68 & 47.82 \\
**Fine-Tuning** & 58.30 & 68.59 & 66.35 & 74.73 & 82.34 & 81.21 & 52.92 & 66.30 & 63.62 & 56.10 & 62.37 & 62.03 \\
**FreeLB**  & 70.70 & 82.41 & 79.67 & 82.84 & 89.48 & 88.56 & **55.42** & 67.39 & 64.80 & 57.90 & 63.17 & 62.81 \\
**NEFTune** & 68.30 & 80.93 & 77.91 & 81.47 & 88.66 & 86.77 & 57.17 & 65.14 & 62.25 & 56.30 & 62.57 & 62.09 \\
**Ours** (**SQuAD**\(\)) & 72.50 & 89.38 & 85.34 & **84.38** & **93.44** & **92.17** & 54.17 & 69.40 & 65.72 & **63.70** & **68.28** & **67.98** \\
**Ours** (**StreamingQA**\(\)) & **72.80** & **89.65** & **85.90** & 84.06 & 93.73 & 91.90 & 54.58 & **72.58** & **68.15** & 63.20 & 68.02 & 67.79 \\   

Table 4: Experimental results on **cross-domain transfer** experiments. For ours, (X \(\)) denotes that latent paraphrasers are trained on \(_{}\) from the X dataset. Rec. denotes recall.

    &  &  &  &  \\ 
**Method** & EM & Rec. & F1 & EM & Rec. & F1 & EM & Rec. & F1 & EM & Rec. & F1 \\ 
**No Injection** & 9.98 & 23.44 & 20.62 & 16.22 & 29.85 & 27.81 & 1.93 & 10.21 & 10.27 & 1.73 & 17.94 & 17.40 \\
**Fine-Tuning** & 16.65 & 35.40 & 29.73 & 19.94 & 35.88 & 32.92 & 13.39 & 30.03 & 28.84 & 10.98 & 43.76 & 39.62 \\
**FreeLB** & 17.04 & 36.78 & 30.36 & 20.72 & 37.19 & 34.04 & 15.47 & 33.69 & 31.85 & 14.68 & 46.05 & 41.85 \\
**NEFtune** & 17.45 & 37.49 & 31.11 & 20.18 & 36.98 & 33.85 & 15.93 & 33.73 & 32.38 & **15.38** & 48.14 & 43.84 \\
**Ours** & **18.96** & **43.10** & **34.65** & **21.62** & **39.38** & **35.32** & **16.29** & **35.04** & **32.56** & 15.26 & **56.70** & **46.45** \\   

Table 3: Experimental results on datasets with **raw documents**. For Ours, we use the latent paraphrasser used in the SQuAD-syn experiment. Rec. denotes recall.

As shown in Table 4, our method successfully transfers across domains, with the latent paraphrasers enhancing the performance of the knowledge injection on NovelQA and MedMCQA-two domains distinct from the source (see Appendix C.1 for details on these datasets). Even though both domains contain specialized entities, our method consistently outperforms standard fine-tuning and other noise-based baselines.

Combining LaPael and ParaphrasesParaphrasing documents in \(_{}\) has been shown to improve knowledge injection performance, as seen in Table 2. While LaPael significantly improves performance without requiring paraphrases, it is valuable to consider the effect of combining paraphrases with the latent perturbations from LaPael. As illustrated in Figure 4, LaPael consistently outperforms standard fine-tuning, showing that LaPael provides advantages over data-level augmentations.

### Ablation Studies

Effects of the Size of \(_{}\)LaPael needs additional data \(_{}\) for training latent paraphrasers. Although only a small amount of data is required, it might be unclear how much is needed to make the latent paraphrasers learn the useful noise distribution. As shown in Figure 4(a), LaPael works well even with **50 sentences** for \(_{}\), while increasing the size of \(_{}\) ensures a steady performance improvement for LaPael.

Effects of the Position of Latent ParaphrasersOur latent paraphrasers can be inserted into any layer of the LLMs. The possible question is which position and how many layers are optimal for latent paraphrasers to effectively learn noise for knowledge injection. To answer this, we analyzed the position and number of latent paraphrasers.

In Figure 4(b), we show the QA accuracy results, varying the start position and number of latent paraphrasers. The first layer is the closest layer to the input layer, and "start position 1" with "# layers = 3" means we insert the latent paraphrasers into the first, second, and third layers of the LLM. Results show that inserting three latent paraphrasers into the early layers of the LLM is effective. This is consistent with findings in previous works  where using noisy token embeddings (the lowest layer) enhanced the generalization in LLMs. Furthermore, in Table 5, we empirically show that positioning the latent paraphraser before the MLP layer within each transformer layer is the most effective choice over other positions.

Figure 4: **Effect of the Number of Paraphrases. Each plot shows the relationship between the number of paraphrases (x-axis) and F1 scores (y-axis) in knowledge injection. The F1 scores of both standard fine-tuning and our method improve as the number of paraphrases increases.**

Figure 5: **(a) We conduct experiments varying the size of \(_{}\) on SQuAD-syn, where \(100\%\) indicates 1,000 documents. We report mean and std. over three runs. (b) We conduct experiments on StreamingQA-syn varying the start position of latent paraphrasers where  ‘# layers’ denotes the number of latent paraphrasers.**

Ablation Studies on ModulesLaPael has many design choices concerning the latent paraphraser architecture, noise type, and training. We conducted extensive ablation studies to empirically verify each design choice and provide guidance for future work. In summary, as shown in Table 6, all design choices are important for building the most effective latent paraphraser. Specifically, we use a trainable mask \(m\) in Equation (7) to regulate the perturbation depending on each token, which is crucial, as the performance on StreamingQA drops significantly if we remove it from the latent paraphraser. Furthermore, using only the sigmoid function in Equation (7) instead of the concrete distribution also leads to much lower performance, as the mask is not properly trained. Regarding noise training, using deterministic noise instead of stochastic noise by removing the noise drawn from a Gaussian distribution in Equation (5) also decreases performance. Additionally, replacing the KL loss with Mean Squared Error loss between two means \(}_{}\) in Equation (13) and \(}_{}\) in Equation (14) leads to a decrease in performance, confirming the importance of stochastic noise trained with KL loss.

Ablation Studies on Noise DistributionShould we train the latent paraphrasers to be effective, or can adding random noise in the early layers also be effective? Which is more important: the learnable mask or the learnable noise? To answer these questions, we conducted ablation studies on the choice of noise distribution. In Table 7, Learnable Add. denotes the model with the additive noise \(+g_{}()\) instead of Equation (4) without softplus from Equation (6). Gaussian is the use of zero-mean Gaussian noise \((,)\) in Equation (6) without using MLP\({}_{z}\). Uniform is the use of noise drawn from the uniform distribution defined in NEFTune  instead of \(z\) in Equation (6).

As shown in Table 7, the learnable multiplicative noise described in Section 4.2 is the best design for noise distribution used in the latent paraphraser. To analyze the effect of the learnable mask, we also added the learnable mask to the Gaussian and Uniform noise settings and optimized only \(_{m}\) and \(_{m}\) in Equation (7) with loss in Equation (15). Interestingly, the learnable mask is not effective for the fixed noise distribution, which contrasts with the results for learnable noise in Table 6. We conjecture that using the learnable mask is important for input-dependent learnable noise, as it can allocate different noise scales to different tokens, while this is not the case for static noise distribution.

## 6 Conclusion

We have introduced LaPael, a method for enhancing knowledge injection in Large Language Models (LLMs) by applying learned perturbations to their layers. Unlike traditional data-level augmentations or noise-based approaches, LaPael operates at the latent level, preserving the semantic integrity of the text while introducing meaningful variability. LaPael addresses key limitations of existing methods by reducing computational costs and increasing the diversity of augmented data. Our extensive validation across diverse benchmark datasets demonstrates the superiority of our method in knowledge injection, as it significantly outperforms both standard fine-tuning and other noise-based baselines. Moreover, combining LaPael with paraphrases yields complementary benefits, further enhancing performance. We believe that LaPael, being simple yet effective, has the potential for significant practical impact and will encourage further research on applying perturbation within the latent space of LLMs.

Discussions & LimitationsIn our work, the following points can be discussed further: (1) Cost Analysis--While LaPael is effective, it incurs additional costs due to the need for training latent paraphrasers and fine-tuning LLMs with them. (2) Knowledge Retention--Although LaPael improves knowledge injection, there may be trade-offs in terms of retaining the original knowledge that the LLM has memorized. (3) Comparison to Retrieval-Augmented Generation (RAG)--While our method improves knowledge injection, it is still less effective than RAG in terms of performance. We provide a detailed discussion of these points, along with other limitations, in the Appendix A.

   StreamingQA & EM & Recall & F1 \\  
**Before MLP** & 84.05 & **93.73** & **91.90** \\ After **MLP** & 73.81 & 82.58 & 81.02 \\ Before **Attn** & 80.55 & 87.58 & 86.49 \\ After **Attn** & 83.31 & 90.98 & 89.72 \\
**Token Embed.** & **86.21** & 91.79 & 91.05 \\    
   StreamingQA & EM & Recall & F1 \\  
**LaPael** & 84.06 & **93.73** & **91.90** \\ \(\)**Max** & 77.95 & 85.17 & 84.63 \\ \(\)**Courence** & 73.35 & 83.42 & 81.99 \\ \(\)**Sampling** & **84.23** & 90.52 & 87.93 \\ \(\)**KL loss** & 83.31 & 90.78 & 89.99 \\   

Table 6: Ablation studies on **Modeling** in latent paraphrasersers.

   StreamingQA & EM & Recall & F1 \\  
**LaPael** & 84.06 & **93.73** & **91.90** \\ \(\)**Max** & 77.95 & 85.17 & 84.63 \\ \(\)**Courence** & 73.35 & 83.42 & 81.99 \\ \(\)**Sampling** & **84.23** & 90.52 & 87.93 \\ \(\)**KL loss** & 83.31 & 90.78 & 89.99 \\    
   StreamingQA & EM & Recall & F1 \\  
**LaPael** & 84.06 & **93.73** & **91.90** \\ \(\)**Max** & 77.95 & 85.17 & 84.63 \\ \(\)**Courence** & 73.35 & 83.42 & 81.99 \\ \(\)**Sampling** & **84.23** & 90.52 & 87.93 \\ \(\)**KL loss** & 83.31 & 90.78 & 89.99 \\   

Table 7: Ablation studies on **Noise de-sign** in latent paraphrasersers.