# Active Learning for Optimal Minimization of Experimental Characterization Uncertainty

Marcus Schwarting

Dept. of Computer Science

University of Chicago

5801 Ellis Avenue

Chicago, IL 60637

meschw04@uchicago.edu

&Nathan Seifert

Dept. of Chemistry

University of New Haven

300 Boston Post Road

West Haven, CT 06516

&Logan Ward

Dept. of Data Science and Learning

Argonne National Laboratory

9700 Cass Avenue

Lemont, IL 60439

Ben Blaiszik

Dept. of Data Science and Learning

Argonne National Laboratory

9700 Cass Avenue

Lemont, IL 60439

Ian Foster

Dept. of Computer Science

University of Chicago

5801 Ellis Avenue

Chicago, IL 60637

Yuxin Chen

Dept. of Computer Science

University of Chicago

5801 Ellis Avenue

Chicago, IL 60637

&Kirill Prozument

Dept. of Physical Sciences

Argonne National Laboratory

9700 Cass Avenue

Lemont, IL 60439

###### Abstract

Collecting experimental measurements is rarely an end in itself; rather, measurements inform key outcome statistics. Standard active learning procedures can drive a cumulative decrease in measurement uncertainty, but do not account for the uncertainty of the outcome. Here, we present an active learning framework that collects measurements agnostic to specific outcomes but which minimize outcome uncertainty, and demonstrate its applicability with imaging and spectroscopic tasks. We show how our framework can effectively select regions for measurement without iteratively retraining a model. We conclude with two instances where our framework has outperformed standard active learning procedures to accelerate the classification of unknown samples.

## 1 Introduction

Accurate characterization of an unknown chemical or material can be a time- and resource-intensive process, especially in a context-free setting. Characterizing a sample often requires obtaining multiple individual noisy measurements that, when combined, yield the desired identifying information. Effectively allocating resources to characterizing a sample is an important challenge with many applications. We present a novel active learning (AL) approach for negotiating the trade-off between time and precision that allows a researcher to characterize a sample reliably with fewer measurements. We start with a pre-trained supervised model with built-in uncertainty quantification (UQ), and iteratively select characterization routines to reduce model uncertainty optimally. While previous works select from a broad set of measurements to improve retrained model performance (and often do not repeat measurements), our approach accounts for even extremely noisy individual measurements to drive towards minimizing classification uncertainty with a fixed trained model.

Spectroscopy is an important area of analytical chemistry where a typical workflow involves iteratively recording noisy measurements until a domain expert can draw conclusions about the contents of a sample. AL has been applied for certain spectroscopic sub-tasks , but has never before been applied to rotational spectroscopy. Rotational spectroscopy uses microwave and millimeter-wave frequencies (ca. 2-800 GHz) to probe transitions between the quantized energy levels of molecules associated with their overall angular momentum . The observed rotational spectrum is extremely sensitive to molecular structure and intramolecular interactions. Rotational spectroscopy is unique in both the structural insights its spectra provide and the rich data it carries for machine learning (ML) applications . Rotational spectra can be acquired via broadband or narrowband measurements. Broadband rotational spectroscopy [3; 33] has enabled researchers to acquire thousands of rotational transitions in a single measurement, with simultaneous acquisition of \(\)10\({}^{5}\) resolution elements at a typical signal-to-noise ratio (SNR) of \(\)10\({}^{4}\)[25; 36]. In broadband rotational spectroscopy, transitions from an entire spectral band can be acquired almost instantaneously, with repeated measurement averaging eventually leading to an acceptable SNR . Alternatively, a full spectrum can be obtained from multiple sequential narrowband measurements, where a decrease in bandwidth corresponds to an increase in SNR at a rate of \(()^{-}\). For a fixed measurement SNR, there is no statistical advantage to either averaging broadband measurements or sequential narrowband measurements . However, when an outcome is based on decreasing molecule identification uncertainty, rather than decreasing measurement uncertainty, we find that our AL framework can efficiently select narrowband measurements to reduce classification uncertainty.

## 2 Background

Methods for UQ can use an ensemble (Monte Carlo)-based approach or an incorporated Bayesian approach. For a classification task, a model with UQ will generate a probability distribution for each class, centered at the predicted class probability. Classical ML models such as random forest can give both a prediction and UQ derived from the ensemble. A close deep learning equivalent can be naively achieved by training an ensemble of feed-forward architectures, or via a dropout regularization approach. For an overview of ensemble deep learning approaches for UQ, see Gawlikowski et al. . Alternatively, UQ can be a baked-in part of model training and inference, as in Gaussian process regression (or kriging) . After setting a statistical prior based on previously collected data, a Gaussian process regressor makes calibrated predictions from inputs, including both a prediction mean and a standard deviation . For more information on models that incorporate UQ as part of inference, see Abdar et al. .

Data acquisition using AL can be divided into approaches that iteratively retrain the prediction model and those that keep the prediction model fixed . The former is often referred to as active instance labeling (e.g., ), and the latter as active feature evaluation (e.g., learning with feature cost , interactive troubleshooting  and medical diagnosis [2; 6]). When the model is iteratively updated, the objective is to select points to improve model performance . When the model is fixed, active learning operates on a fixed pre-trained model where decisions must be made in serial to come to a conclusion with high confidence [5; 26].

Whether a UQ model is fixed or iteratively updated, selection of the acquisition function can significantly alter the progress of an AL approach. Consider a classification task with \(N\) classes, and where \(T\) samples are drawn iid from a Bayesian prior. For a fixed patch index \(i\{1,...,r\}\), we can establish probabilities \(^{i,t}=[P_{1}^{i,t},...,P_{N}^{i,t}];\ t\{1,...,T\}\). Furthermore, for class \(n\), let \(_{n}^{i}=_{t=1}^{T}P_{n}^{i,t}\). The acquisition functions can be broken into information theoretic approaches (based on entropy) and standard deviation approaches . We focus on four acquisition functions:

* Predicted information entropy , defined as \((^{i})=-_{n=1}^{N}_{n}^{i}_{n}^{i}\).
* Bayesian AL by disagreement, or BALD [15; 18]--akin to an expected entropy improvement metric. \((^{i})=(^{i})+_{n=1}^{N} _{t=1}^{T}P_{n}^{i,t} P_{n}^{i,t}\).
* Mean standard deviation , defined as \((^{i})=_{n=1}^{N}_{t=1}^{ T}(P_{n}^{i,t}-_{n}^{i})^{2}}\).
* Baseline uniform acquisition, where \((^{i})\) is drawn i.i.d. from \(U(0,1)\).

Methods

We start with a demonstration using the MNIST dataset of 28\(\)28-pixel gray-scale images of handwritten digits . We train a feed-forward neural network on MNIST with a standard 80/20 train/test split, augmenting training images with added Gaussian noise . We break a 28\(\)28 image from the test set into 4\(\)4 image patches. For a ground-truth patch \(^{i}^{16}\), we can draw measurements from a "noisy oracle" as \(}^{i}=^{i}+;\  N(,_{M})\). We use a multivariate Gaussian prior \(\) with mean \(\) and covariance \(\) for each 4\(\)4 patch from the MNIST training set (see Appendix B). Comparing central vs. perimeter patches suggests that patches with higher prior variance will be more useful to measure (satisfying a pure Bayesian exploration objective). But prior variance cannot be the only selection criterion, since measurements of patches with high prior variance need not reduce classification uncertainty.

Next we show a similar demonstration with rotational spectroscopy. We select the molecule propene (CH\({}_{3}\)CHCH\({}_{2}\)) as our ground truth class for subsequent tests [20; 24]. We then create similar classes of hypothetical molecules with spectra resembling propene. Additional spectroscopic details, along with how these similar spectral classes were created and augmented for training robust classifiers, is given in Appendix A. We choose to simulate spectra in the 65-85 GHz range, which corresponds to a common experimental apparatus range . Also, we bin and renormalize all spectra into either \(b=20\) or \(b=200\) equally sized patches. While a variety of supervised classification models could be readily employed, we opted for a logit classifier with a one-versus-rest scheme. Regardless of the model choice, the result is expressed as \(:^{b}[P_{1},...,P_{N}]\). Finally, we use the mean vector and covariance matrix across all molecule spectra in the PC9 dataset of small organic molecules .

When querying a noisy oracle, we assume samples are drawn from a multivariate Gaussian likelihood with covariance \(_{M}\). Therefore, we can use a conjugate prior/posterior that is also a multivariate Gaussian . Algorithm 1 shows our acquisition workflow. For a given patch we sample from the prior, impute the sample, and run model inference. From the classification probabilities, we select a patch using the acquisition function \(f\{\}\). After making a measurement, we update the prior. Appendix C shows how we can generalize to both a Poisson noise model or regression tasks. We benchmark acquisition function loss via a the remaining model probability entropy after a measurement is captured.

``` Initialize: Prior \(\) with mean \(\) and covariance \(\), model \(\), measurement covariance \(_{M}\). for\(q=1,...,Q\)do// Measurement Loop for\(i=1,...,r\)do// Patch Selection Loop for\(t=1,...,T\)do// Monte Carlo Sample Loop \(^{i,t}_{i}()\); \(_{s}^{i}=[...,^{i-1},^{i,t},^{i+1},...]\) ; // Sample Imputation \(^{i,t}=[P_{1}^{i,t},...,P_{N}^{i,t}]=(_{s}^{i})\); // Inference on Imputed Sample \(s=*{argmax}_{i\{1,...,r\}}f(^{i})\); // Patch Selection From Acquisition f \(}_{s}=_{s}+;\  N(,_{M,s})\) ; // Measurement Capture on Patch s \(_{s}(_{s}^{-1}+_{M,s}^{-1})^{-1}(_{s}^{-1}_{s}+_{M,s}^{-1}}_{s})\); \(_{s}(_{s}^{-1}+_{M,s}^{-1})^{-1}\); // Prior Update on Patch s return\(()=^{}\) ; // Final Model Classification ```

**Algorithm 1**Active learning workflow for experimental acquisition across patches.

## 4 Results

Figure 1 (left) shows the outcome of 1000 guided measurements on patches of an MNIST test image, with error bounds constructed over 50 runs on the same test image. Insets show the posterior updates at intervals of 250 iterations for each acquisition function. For measurements using BALD and MSTD acquisition functions, a quantitative improvement in the loss function is reflected qualitatively when we see that the true digit (a "2") more discernible for BALD and MSTD than for UNIF or ENTR. The ENTR acquisition function quickly becomes stuck taking measurements at the periphery where prior measurement variances start off small and downstream classification uncertainties barely change, so

there is virtually no change in the measurement posterior. The UNIF acquisition captures superfluous measurements that have little impact on either the posterior or model scoring.

Figure 1 (right) shows the outcome of 1000 guided measurements on 20 patches of the rotational spectrum of propene with nine additional classes, with error bounds constructed over 50 runs on the same ground truth spectrum. While there is no statistical guarantee that either BALD or MSTD will always outperform UNIF, however using a one-sided t-test we can be confident that both BALD (\(t=-7.17,\ p<10^{-9}\)) and MSTD (\(t=-4.16,\ p<10^{-5}\)) are likely to outperform UNIF. Table 1 shows three further tests which vary the number of patches (either 20 or 200, representing broadband and narrowband modes) as well as the number of molecule classes (either 10 or 100). BALD and MSTD significantly outperform ENTR and UNIF on all four tests. While these acquisition methods are agnostic to the correct classification probability, Table 1 presents these probabilities to provide clarity and as a proxy for problem difficulty (which increases with number of classes and decreases with patch count). A higher correct classification probability does not perfectly correlate with a lower loss, since our loss is defined as the final model entropy after all measurements have been collected.

BALD and MSTD offer the best improvement for measurement acquisition when individual measurements are not so noisy that the posterior update changes only slightly at each iteration (such as the second row). If individual measurements are less noisy, then our framework yields a process approaching a standard AL model driven by an exploration objective. Furthermore, in less noisy settings, averaging broadband measurements is likely to be more effective for decreasing classification uncertainty than sequential narrowband measurements. When comparing broadband (20 patches) experiments versus narrowband experiments (200 patches), we see that the same number of measurements (with adjusted SNR) yields a much better loss for BALD and MSTD strategies. In other words, our AL framework can rapidly decrease molecule identification uncertainty when there are many patches to choose from.

## 5 Conclusion

In this work, we presented an active learning framework for measurement acquisition that works to decrease uncertainty in classification rather than measurement. We demonstrate how our framework can operate effectively in both an imaging and a spectroscopic context to drive towards accurate classification. We also demonstrate how our AL framework can effectively select from among many possible narrowband measurements to disambiguate between similar molecule species. In the

Figure 1: Left: Classifier loss function over 1000 MNIST sample measurements selected by using different acquisition functions (BALD, MSTD, ENTR, and UNIF). Inscribed images show the Bayesian posterior reconstruction using each acquisition function at intervals of 250 iterations (from left to right). Shaded regions indicate a bound of \(\) for each acquisition function. Right: Loss function calculations across acquisition functions over 1000 rotational spectrum measurements over 20 bins with ten total classes. Tests are run over 50 iterations for each acquisition function, and filled-in regions indicate a bound of \(\) for each acquisition function.

future, we hope to generalize this approach to eventually characterize the sorts of complex mixtures present in an analytical chemistry context. We also hope to demonstrate the viability of our approach for regression and segmentation tasks, which may have applications in areas of microscopy and tomography.

 
**Patch** & **Class** & **Model Accuracy** & **BALD** & **MSTD** & **ENTR** & **UNIF** \\
**Count** & **Count** & **Train (Test)** & **(\(P_{c}\))** & **(\(P_{c}\))** & **(\(P_{c}\))** & **(\(P_{c}\))** & **(\(P_{c}\))** \\ 
20 & 10 & 0.964 (0.947) & **0.719 (0.817)** & 1.086 (0.712) & 1.867 (0.344) & 1.333 (0.568) \\ 
20 & 100 & 0.849 (0.807) & 3.074 (0.183) & **3.032 (0.193)** & 3.787 (0.131) & 3.276 (0.140) \\ 
200 & 10 & 0.993 (0.985) & 0.116 (0.983) & **0.090 (0.987)** & 1.915 (0.314) & 0.890 (0.804) \\ 
200 & 100 & 0.899 (0.871) & 0.796 (0.845) & **0.668 (0.878)** & 2.156 (0.138) & 2.300 (0.402) \\  

Table 1: Final loss across 1000 spectral measurements across acquisition functions, along with the final correct classification probability \((P_{c})\). Logit model accuracy on train and test sets are also given.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our abstract outlines our active learning framework, which we claim outperforms other active learning approaches for active noise acquisition in an experimental setting. We substantiate these findings throughout the rest of the work.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Within the results section, we discuss how there is a window where the performance of our active learning framework excels when collecting measurements. Outside of this window, we discuss how other techniques may be better suited.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: This work does not contain any formal proofs or theorems. In the main body of the paper we highlight how our technique works in one of four cases (classification with a Gaussian noise model) but suggest how it could work for the other three cases (with either a regression objective or a Poisson noise model). We go into further details on this in an appendix, with some derivations.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: A repository containing the code and methods for both examples will be made public following the acceptance of this manuscript. Furthermore, our primary contribution (the active learning framework) is laid out in an algorithm. All data used for our tests is publicly available.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: A repository containing the code and methods for both examples will be made public following the acceptance of this manuscript.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We present two examples. We first use the MNIST dataset, so all data and parameters are readily available. We then use a ground truth species (propene) for our rotational spectroscopy test, and the explanations on reproducing the results are found in the corresponding appendix.
7. **Experiment Statistical Significance**Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Both figures in the main body of the paper show an error bar corresponding to \(1/2\), or half of a standard deviation. We also present one-sided T-test statistics to demonstrate that our technique, while not guaranteed to outperform a baseline, does so at a statistically meaningful frequency.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We briefly mention the compute resources we used in an appendix, however it is worth noting that the approaches we describe did not require significant compute. In fact, in a separate test we were able to place our code base on a desktop connected to a rotational spectrometer and perform similar tests.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our approach is intended to make noisy experimental sampling more efficient. Within our moral imagination, we cannot envision ways that our approach, scoped as it currently is, could have negative moral, cultural, social, or societal impact.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: As stated above, we cannot currently identify any potential negative societal impacts that could result from the work performed. However we are certainly open to adding a section describing such negative societal impacts, if reviewers or editors identify such concerns from our stated methods.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: As far as we are concerned, this paper poses no such risks.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: For our MNIST example, we of course cite the relevant work and models. For our rotational spectroscopy example, we cite the experimenters and computational chemists who first identified and derived the spectra we are using.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes]Justification: We will release a public GitHub repository with examples and documentation following the acceptance of this manuscript.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not use any crowdsourcing or research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Per the above, this paper does not involve crowdsourcing nor research with human subjects.