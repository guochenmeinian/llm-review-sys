# Implicit Bias of Mirror Flow on Separable Data

Scott Pesme

EPFL

Radu-Alexandru Dragomir

Telecom Paris

Nicolas Flammarion

EPFL

###### Abstract

We examine the continuous-time counterpart of mirror descent, namely mirror flow, on classification problems which are linearly separable. Such problems are minimised 'at infinity' and have many possible solutions; we study which solution is preferred by the algorithm depending on the mirror potential. For exponential tailed losses and under mild assumptions on the potential, we show that the iterates converge in direction towards a \(_{}\)-maximum margin classifier. The function \(_{}\) is the _horizon function_ of the mirror potential and characterises its shape 'at infinity'. When the potential is separable, a simple formula allows to compute this function. We analyse several examples of potentials and provide numerical experiments highlighting our results.

## 1 Introduction

Heavily over-parametrised yet barely regularised neural networks can easily perfectly fit a noisy training set while still performing very well on unseen data (Zhang et al., 2017). This statistical phenomenon is surprising since it is known that there exists interpolating solutions which have terrible generalisation performances (Liu et al., 2020). To understand this benign overfitting, it is essential to take into account the training algorithm. If overfitting is indeed harmless, it must be because the optimisation process has steered us towards a solution with favorable generalisation properties.

From this simple observation, a major line of work studying the _implicit regularisation_ of gradient methods has emerged. These results show that the recovered solution enjoys some type of low norm property in the infinite space of zero-loss solutions. Gradient descent (and its variations) has therefore been analysed in various settings, the simplest and most emblematic being that of gradient descent for least-squares regression: it converges towards the solution which has the lowest \(_{2}\) distance from the initialisation (Lemaire, 1996). In the classification setting with linearly separable data, iterates of gradient methods must diverge to infinity to minimise the loss. Therefore, the directional convergence of the iterates is considered and Soudry et al. (2018) show that gradient descent selects the \(_{2}\)-max-margin solution amongst all classifiers.

Going beyond linear settings, it has been observed that **an underlying mirror-descent structure very recurrently emerges** when analysing gradient descent in a range of non-linear parametrisations (Woodworth et al., 2020; Azulay et al., 2021). Providing convergence and implicit regularisation results for mirror descent has therefore gained significant importance.

In this context, for linear regression, Gunasekar et al. (2018) show that the iterates converge to the solution that has minimal Bregman distance to the initial point. Turning towards the classification setting, an apparent gap emerges as there is still no clear understanding of what happens: Can directional convergence be characterised in terms of a max-margin problem? If so, what is the associated norm? Quite surprisingly, this question remains largely unanswered, as it is only understood for \(L\)-homogeneous potentials (Sun et al., 2023). Our paper bridges this gap by formally characterising the implicit bias of mirror descent for separable classification problems.

### Informal statement of the main result

For a separable dataset \((x_{i},y_{i})_{i[n]}\), we study the mirror flow \((_{t})=- L(_{t})t\) with potential \(:^{d}\) and an exponential tailed classification loss \(L\). We prove that \(_{t}\) converges in direction towards the solution of the \(_{}\)-maximum margin solution where the (asymmetric) norm \(_{}\) captures the shape of the potential \(\) 'at infinity' (see Figure 2 for an intuitive illustration).

**Theorem 1** (Main result, Informal).: _There exists a **horizon function**\(_{}\) such that for any separable dataset, the normalised mirror flow iterates \(_{t}_{t}/\|_{t}\|\) converge and satisfy:_

\[_{t}_{t}\ \ \ \ *{arg\,min}_{_{i}y_{i}  x_{i}, 1}_{}().\]

Our result holds for a large class of potentials \(\) and recovers previous results obtained for \(=\|\|_{p}^{p}\)(Sun et al., 2022) and for \(L\)-homogeneous potentials (Sun et al., 2023). For general potentials, showing convergence towards a maximum margin classifier is much harder because, in stark contrast with homogeneous potentials, \(\)'s geometry changes as the iterates diverge. To capture the behaviour of \(\) at infinity, we geometrically construct its horizon function \(_{}\). By considering \(\)'s successive level sets (and re-normalising them to prevent blow up), we show that under mild assumptions, these sets asymptotically converge towards a limiting horizon set \(S_{}\). The horizon function \(_{}\) is then simply the asymmetric norm which has \(S_{}\) as its unit ball (see Figure 3 for an illustration). In addition, when the function \(\) is'separable' and can be written \(()=_{i}(_{i})\) for a real valued function \(\), then a very simple and explicit formula enables to calculate \(_{}\) (Theorem 3).

The paper is organised as follows. The classification setting as well as the assumptions on the loss and the potential are provided in Section 2. The proof sketch and an intuitive construction of the horizon function are given in Section 3. In Section 4, we state the formal definition and results. Simple examples of horizon potentials and numerical experiments supporting our claims are finally given in Section 5.

### Relevance of mirror descent and related work

We first outline the motivations for understanding the implicit regularization of mirror descent and discuss related works that contextualize our contribution within the machine learning context.

Relevance of studying mirror descent in the context of machine learning.Though mirror descent is not _per se_ an algorithm used by machine learning practitioners, it proves to be a very useful tool for theoreticians in the field. Indeed, when analysing gradient descent (and its stochastic and accelerated

Figure 1: Mirror descent is performed using \(3\) different potentials on the same toy \(2\)d dataset. _Left:_ the losses converge to zero. _Center:_ the iterates converge in direction towards \(3\) different vectors \(_{}\), the \(3\) lines passing through the origin correspond to the associated separating hyperplanes. _Right:_ the limit directions are each proportional to \(*{arg\,min}_{}()\) under the constraint \(_{i}y_{i} x_{i}, 1\) for their respective \(_{}\)â€™s, as predicted by our theory (Theorem 1). The full trajectories are plotted Figure 4 and we refer to Section 5 for more details.

variants) on neural-network architectures, an underlying mirror-descent structure very recurrently emerges. Then, results for mirror descent enable to prove convergence as well as characterise the implicit bias of gradient descent for these architectures. Diagonal linear networks, which are ideal proxy models for gaining insights on complex deep-learning phenomenons, is the most notable example of such an architecture. The hyperbolic entropy potential naturally appears and enables to prove countless results: implicit bias of gradient descent in regression (Woodworth et al., 2020; Vaskevicius et al., 2019) and in classification (Moroshko et al., 2020), effect of stochasticity (Pesme et al., 2021) and momentum (Papazov et al., 2024), convergence of gradient descent and effect of the step-size (Even et al., 2023), saddle-to-saddle dynamics (Pesme and Flammarion, 2023). Unveiling an underlying mirror-like structure goes beyond these simple networks as they also appear in: matrix factorisation with commuting observations (Gunasekar et al., 2017; Wu and Rebeschini, 2021), fully connected linear networks (Azulay et al., 2021; Varre et al., 2023) and \(2\)-layer ReLU networks (Chizat and Bach, 2020). Building on these examples, Li et al. (2022) investigate the formal conditions that ensure the existence of a mirror flow reformulation for general parametrisations, extending previous results by Amid and Warmuth (2020, 2020).

Gradient descent in classification.Numerous works have studied gradient descent in the classification setting. For linear parametrisations, separable data and exponentially tailed losses, Soudry et al. (2018) prove that GD converges in direction towards the \(_{2}\)-maximum margin classifier and provides convergence rates. A very fine description of this divergence trajectory is conducted by Ji and Telgarsky (2018) and a different primal-dual analysis leading to tighter rates is given by Ji and Telgarsky (2021). Similar results are proven for stochastic gradient descent by Nacson et al. (2019). In the case of general loss tails, Ji et al. (2020) prove that gradient descent asymptotically follows the \(_{2}\)-norm regularisation path. A whole 'astral theory' is developed by Dudik et al. (2022) who provide a framework which enables to handle'minimisation at infinity'. Beyond the linear case, Lyu and Li (2020) proves for homogeneous neural networks that any directional limit point of gradient descent is along a KKT point of the \(_{2}\)-max margin problem. A weaker version of this result was previously obtained by Nacson et al. (2019). Furthermore, convergence results for linear networks are provided by Yun et al. (2021). Finally, for \(2\)-layer networks in the infinite width limit, assuming directional convergence, Chizat and Bach (2020) proves that the limit can be characterised as a max-margin classifier in a certain space of functions.

### Notations

We provide here a few notations which will be useful throughout the paper. We let \([n]\) be the integers from \(1\) to \(n\). We denote by \(Z^{n d}\) the feature matrix whose \(i^{th}\) line corresponds to the vector \(y_{i}x_{i}\). When not specified, \(\|\|\) corresponds to any (definable) norm on \(^{d}\). For a convex function \(h\), \( h()\) denotes its subdifferential at \(\): \( h()=\{g^{d}:h(^{}) h()+ g,^{}-,^{}^{d}\}\). For any scalar function \(f:\) and vector \(u^{p}\), the vector \(f(u)^{p}\) corresponds to the component-wise application of \(f\) over \(u\). We denote by \(:^{n}^{n}\) the softmax function equal to \((z)=(z)/_{i=1}^{n}(z_{i})_{n}\) where \(_{n}\) is the unit simplex. For a convex potential \(\), we denote \(D_{}(,_{0})\) the Bregman divergence equal to \(()-((_{0})+(_{0}),-_{0} ) 0\).

## 2 Problem set-up

We consider a dataset \((x_{i},y_{i})_{1 i n}\) with points \(x_{i}^{d}\) and binary labels \(y_{i}\{-1,1\}\). We choose a loss function \(:\) and seek to minimise the empirical risk

\[L()=_{i=1}^{n}(y_{i} x_{i},).\]

We propose to study the dynamics of mirror flow, which is the continuous-time limit of the _mirror descent_ algorithm (Beck and Teboulle, 2003). Mirror descent is a generalisation of gradient descent to non-Euclidean geometries induced by a given convex potential function \(:^{d}\). The method generates a sequence \((_{k})_{k 0}\) with \(_{0}=_{0}^{d}\) and

\[(_{k+1})=(_{k})- L(_{k}).\]When the step size \(\) goes to 0, the mirror descent iterates approach the solution \((_{t})_{t 0}\) to the following differential equation:

\[(_{t})=- L(_{t})t,\] (MF)

initialised at \(_{0}\). Studying the mirror flow (MF) leads to simpler computations than its discrete counterpart, and still allows to obtain rich insights about the algorithm's behaviour.

We now state our standing assumptions on the loss function \(\) and potential \(\).

**Assumption 1**.: _The loss \(\) satisfies:_

1. \(\) _is convex, twice continuously differentiable, decreasing and_ \(_{z+}(z)=0\)_._
2. \(\) _has an exponential tail, in the sense that_ \((z)-^{}(z)(-z)\)_._

The first part of the assumptions is very general and ensures that the empirical loss \(L\) can be minimised 'at infinity'. The exponential tail is crucial: it enables to identify a unique maximum margin solution towards which the iterates converge in direction, independently of the considered loss. Both the exponential \((z)=(-z)\) and the logistic loss \((z)=(1+(-z))\) satisfy the conditions. On the other hand, losses with polynomial tails do not satisfy the second criterion. Similar assumptions on the tail appear when investigating the implicit bias of gradient descent for separable data (Soudry et al., 2018; Nacson et al., 2019; Ji et al., 2020; Ji and Telgarsky, 2021; Chizat and Bach, 2020).

**Assumption 2**.: _The potential \(:^{d}\) satisfies:_

1. \(\) _is twice continuously differentiable, strictly convex and coercive._
2. _for every_ \(c_{ 0}\) _and_ \(_{2}^{d}\)_, the sub-level set_ \(\{_{1}^{d},D_{}(_{2},_{1}) c\}\) _is bounded._
3. \(^{2}()\) _is positive-definite for all_ \(^{d}\)_._
4. \(\) _diverges at infinity:_ \(_{\|\|}\|()\|=+\)_._

The first two points of the assumption are commonly used to ensure well-posedness of mirror descent (Bauschke et al., 2017). The third one is necessary in continuous time to ensure the existence and uniqueness over \(_{ 0}\) of a solution to the (MF) differential equation (in particular, we want to avoid the solution _"blowing up in finite time"_; see Lemma 2 in Appendix A). The coercive gradient assumption is crucial for our main result and we discuss it in more depth in Section 6.

Finally, we assume that the dataset is linearly separable.

**Assumption 3**.: _There exists \(^{}^{d}\) such that \(y_{i}^{},x_{i}>0\) for every \(i[n]\)._

Notice that such \(^{}\)'s correspond to minimisation directions: \(L(^{})0\). Under the three previous assumptions, we can show that the mirror flow iterates \((_{t})_{t 0}\) minimise the loss while diverging to infinity.

**Proposition 1**.: _Considering the mirror flow \((_{t})_{t 0}\), the loss converges towards \(0\) and the iterates diverge: \(_{t}L(_{t})=0\) and \(_{t}\|_{t}\|=+\)._

The proof relies on classical techniques used to analyse gradient methods in continuous time and we defer the proof to Appendix A. We now turn to the main question addressed in this paper:

_Among all minimising directions \(^{}\), towards which does the mirror flow converge?_

We initially offer a heuristic and intuitive answer to this question, setting the stage for the formal construction of the implicit regularisation problem.

## 3 Intuitive construction of the implicit regularisation problem

In this section, we give an informal presentation and proof sketch of our main result. A fully rigorous exposition is then provided in Section 4.

Preliminaries.Assume here for simplicity that \((z)=(-z)\). The mirror flow then writes

\[}{t}(_{t})=L(_{t})  Z^{T}q(_{t}),\]

with \(q(_{t})=(-Z_{t})\), where \(\) is the softmax function and \(Z\) the matrix with rows \((y_{i}x_{i})_{i[n]}\). Note that \(q(_{t})\) belongs to the unit simplex \(_{n}\).

We simplify the differential equation by performing a time rescaling, which does not change the asymptotical behaviour. As \(:t_{0}^{t}L(_{s})s\) is a bijection in \(_{ 0}\) (see Lemma 4), we can speed up time and consider the accelerated iterates \(_{t}=_{^{-1}(t)}\). 1 By the chain rule, we have

\[}{t}(_{t})= (_{0})+Z^{}_{0}^{t}q( {}_{s})s.\] (1)

From now on, we drop the tilde notation and assume that a change of time scale has been done. We want to characterise the directional limit of the diverging iterates \(_{t}\). To do so, we study their normalisation \(_{t}}{\|_{t}\|}\). As they form a bounded sequence, and \(q(_{t})_{n}\) is also bounded, we can extract a subsequence2\((_{t_{s}},q(_{t_{s}}))_{s}\), with \(_{s}t_{s}=\) converging to some limit \((_{},q_{})\). By the Cesaro average property, \(}_{0}^{t_{s}}q(_{s})s\) also converges towards \(q_{}\). Equation (1) then yields

\[}(_{t_{s}})Z^{}q_{}.\] (2)

Observe that \(q(_{t})=(-Z_{t})\) and the softmax function \(\) approaches the argmax operator at infinity. Hence, as \(_{t}\) diverges, we expect that \(q(_{t})_{k} 0\) for coordinates \(k\) for which \((-Z_{t})_{k}\) is not maximal, _i.e._\((Z_{t})_{k}\) not minimal. This observation is made formal in the following lemma. Its proof is straightforward and is given in Appendix A.

**Lemma 1**.: _Assume that \((_{t_{s}},q(_{t_{s}}))( _{},q_{})\). It holds that:_

\[(q_{})_{k}=0 y_{k} x_{k},_{}>_{1 i n}y_{i} x_{i},_{ }.\]

In words, coordinates of \(q_{}\) which do not correspond to support vectors of \(_{}\) must be zero. Our goal is now to uniquely characterise \(_{}\) as the solution of a maximum margin problem.

### Warm-up: gradient flow

As a warm-up, let us consider standard gradient flow, which corresponds to mirror flow with potential \(=\|\|_{2}^{2}/2\). In this case, Equation (2) becomes \(_{t_{s}}/t_{s} Z^{}q_{}\). Since the normalised iterates satisfy \(_{t_{s}}_{}\), we get

\[_{}=q_{}}{\|Z^{}q_{ }\|_{2}}.\]

Now notice that this equation along with the slackness conditions from Lemma 1 exactly correspond to the optimality conditions of the following convex minimisation problem:

\[_{}\ \|\|_{2} _{i[n]}\ y_{i} x_{i}, 1.\] (3)Furthermore, the \(_{2}\)-unit ball being strictly convex, Problem (3) has a unique solution to which \(_{}\) must therefore be equal. Importantly, notice that Problem (3) uniquely defines the limit of **any extraction** on the normalised iterates \(_{t}\): the normalised iterates \(_{t}\) must therefore converge towards the \(_{2}\)-maximum margin. We recover the implicit regularisation result from Soudry et al. (2018):

\[_{}=*{arg\,min}_{_{i}\,y_{i}  x_{i}, 1}\|\|_{2}.\]

### General potential: introducing the horizon function \(_{}\)

We now tackle general potentials \(\). In the general case, the challenge of identifying the max-margin problem to which the iterates converge in direction stems from the fact that if the potential \(\) is not \(L\)-homogeneous3, its geometry changes as the iterates diverge. More precisely, its sub-level sets \(S_{c}:=\{^{d},() c\}\) change of shape as \(c\) increase, as illustrated by Figure 2 (Left).

However, we can hope that these sets have a **limiting shape** at infinity, meaning that the normalised sub-level sets \(_{c} S_{c}/R_{c}\) where \(R_{c}_{ S_{c}}\|\|\) converge to some limiting convex set \(S_{}\) as \(c\). We can then construct an asymmetric norm4\(_{}\) which has \(S_{}\) as its unit ball. **In words, \(_{}\) captures the shape of \(\) at infinity.** This informal construction is made rigorous in Section 4.1. We state here the crucial consequence of this construction.

**Corollary 1**.: _The horizon function \(_{}\) is such that for any sequence \(_{t}\) diverging to infinity for which \(}{\|_{t}\|}\) and \()}{\|(_{t})\|}\) both converge, then:_

\[_{t})}{\|(_{t})\|} _{}(_{}), _{}=_{t}}{\|_{t}\|},\]

_for some strictly positive factor \(\)._

Using this construction, we can derive the optimality conditions satisfied by \(_{}\). From the convergence in Equation (2) and that of \(_{t}_{}\), applying Corollary 1, we obtain that:

\[Z^{}q_{}_{}(_{}).\]

Up to a positive multiplicative factor (which is irrelevant due to the positive homogeneity of the quantities involved), this condition along with Lemma 1 are exactly the optimality conditions of the convex problem

\[_{^{d}}\ _{}() _{i[n]}\,y_{i} x_{i}, 1.\]

The limiting direction \(_{}\) must therefore belong to the set of its solutions. Assuming that this set contains a single element of norm 1 (we refer to the next section for comments concerning the uniqueness), we deduce that the iterates \(_{t}\) must converge towards it:

\[_{t}}{\|_{t}\|}*{arg\, min}_{_{i}\,y_{i} x_{i}, 1}\ _{}().\]

## 4 Main result: directional convergence towards the \(_{}\)-max margin

We now state our formal results, starting with the precise construction of the horizon function \(_{}\), followed by the theorem showing convergence of the iterates towards the \(_{}\)-max-margin.

### Construction of the horizon function \(_{}\)

We first define the **horizon shape** of a potential \(\), and provide sufficient conditions for its existence. Then, we use this shape to construct a **horizon function**\(_{}\), which allows the interpretation of the directional limits of gradients of \(\) at infinity. The proofs require technical elements from variational analysis to ensure that the limits are well-defined; these are deferred to Appendix B.

Horizon shape.Assume w.l.o.g. that \((0)=0\). For \(c 0\), consider the sublevel set:

\[S_{c}()=\{^{d}\,:\,() c\},\]

which is nonempty and compact by coercivity of \(\). We can then define the normalised sublevel set:

\[_{c}=}S_{c}, R_{c}=\{\|\|\,:\, S_{c }\}.\] (4)

By construction, the set \(_{c}\) belongs to the unit ball. We are interested in the limit of \(_{c}\) as \(c\).

**Definition 1**.: _We say that \(\) admits a horizon shape if the family of normalized sublevel sets \((_{c})_{c>0}\) defined in Equation (4) converges to some compact set \(S_{}\) as \(c\) for the Hausdorff distance. In addition, we say that this shape is non-degenerate if the origin belongs to the interior of \(S_{}\)._

The Hausdorff distance is a natural distance on compact sets (see Rockafellar and Wets, 1998, Section 4.C., for a definition). In Proposition 2, we prove the existence of the horizon shape for a large class of functions which contains all the potentials with domain \(^{d}\) encountered in practice. Although the horizon shape is guaranteed to exist for most functions, we cannot a priori prove that it is non-degenerate, as the normalized sub-levels \(_{c}\) can become 'flat' as \(c\).5 Given the technical complexity associated with this case, we now focus exclusively on non-degenerate horizon shapes.

Horizon function.If \(\) admits a non-degenerate horizon shape \(S_{}\), we define its horizon function as the _Minkowski gauge_(Rockafellar and Wets, 1998, Section 11.E) of \(S_{}\):

\[_{}()\,\{r>0\,:\,}{r} S _{}\}^{d}.\]

By construction, the horizon function \(_{}\) is an asymmetric norm and its sub-level sets correspond to scaled versions of \(S_{}\)(see Rockafellar and Wets, 1998, Section 11.C, for more properties). For example, in the case of the horizon shape \(S_{}\) illustrated in Figure 3, the corresponding horizon function \(_{}\) is proportional to the \(_{1}\)-norm. Although the construction of \(_{}\) presented here is rather abstract, we show in Theorem 3 that for separable potentials defined over \(^{d}\), it can be computed with an explicit formula. Though different, our definition of the horizon function shares many similarities with the classical concept of horizon function from convex analysis (Rockafellar and Wets, 1998). We discuss the links between the two notions at the end of Section 4.3.

Figure 3: Illustration of the construction of the horizon shape \(S_{}\). _Left:_ the sub-level sets \(S_{c}\) change of shape and are increasing. _Middle:_ in order to avoid the shapes blowing up, we normalise them to keep them in the unit ball (here we choose the arbitrary constraining norm to be the \(_{1}\)-norm). _Right:_ the normalised sub-level sets \(_{c}\) converge to a limiting set \(S_{}\) for the Hausdorff distance.

### Main result: directional convergence of the iterates towards the \(_{}\)-max-margin

We can now state our main result which fully characterises the directional convergence of mirror flow.

**Theorem 2**.: _Assume that \(\) admits a non-degenerate horizon shape and let \(_{}\) be its horizon function. Assuming that the following \(_{}\)-max-margin problem has a unique minimiser, then the mirror flow normalised iterates \(_{t}=}{\|_{t}\|}\) converge towards a vector \(_{}\) and_

\[_{}*{arg\,min}_{ ^{d}}_{}()_{i[n]}\;y_{i}  x_{i}, 1,\]

_where the symbol \(\) denotes positive proportionality._

Remark on the uniqueness of the margin problem.If the unit ball of \(_{}\) is strictly convex, then the \(_{}\)-max-margin problem has a unique solution. However, in the general case, there may exist an infinity of solutions and weak but ad hoc assumptions on the dataset are required to guarantee uniqueness. For instance, if \(_{}\) is proportional to the \(_{1}\)-norm, a common assumption which ensures uniqueness is assuming that the data features are in _general position_(Dossal, 2012).

### Assumptions guaranteeing the existence of \(_{}\)

Our main result, presented in Theorem 2 relies on the existence of a horizon shape, \(S_{}\), as described in Definition 1. From this shape, the asymmetric norm \(_{}\) is constructed.

We show here that the existence of \(S_{}\) is ensured for a large class of 'nice' functions, specifically those _definable in \(o\)-minimal structures_(Dries, 1998). For the reader unfamiliar with this notion, this class contains all'reasonable' functions used in practice, such as polynomials, logarithms, exponentials, and'reasonable' combinations of those. This is a typical assumption used for instance to prove the convergence of optimisation methods through the Kurdyka-Lojasiewicz property (Attouch et al., 2011).

**Proposition 2**.: _If any of the three following conditions hold: (i) \(\) is a finite composition of polynomials, exponentials and logarithms, (ii) \(\) is globally subanalytic, (iii) \(\) is definable in a \(o\)-minimal structure on \(\); then \(\) admits a horizon shape \(S_{}\)._

The proof is technical and we defer it to Appendix B. Although the previous proposition ensures the existence \(_{}\) for a wide range of potentials, it does not offer a direct method for computing it. In the following, we show that for potentials that are both separable and even, a simple formula exists, allowing for the direct calculation of \(_{}\).

**Assumption 4**.: _The potential \(\) is separable, in the sense that there exists \(:_{ 0}\) such that \(()=_{i=1}^{d}(_{i})\). We assume that \(\) satisfies Assumption 2, that it is definable in a \(o\)-minimal structure on \(\) and that it is an even function. W.l.o.g. we assume that \((0)=0\)._

We note that \(\) is a bijection over \(_{ 0}\), and denote by \(^{-1}\) its inverse. We consider the function \(^{-1}\), which can be seen as a renormalisation of \(\). It has the same level sets as \(\) and ensures that \(_{ 0}^{-1}((/))\) exists in \(_{>0}\) for all \(\). These two observations lead to the following theorem.

**Theorem 3**.: _Under Assumption 4, the potential \(\) admits a non-degenerate horizon shape and its horizon function is such that there exists \(>0\) such that for every \(^{d}\):_

\[_{}()=_{ 0}^{-1}( (}{})).\]

We use this simple formula when computing \(_{}\) for various potentials in the next section.

Remark on previous notions of horizon function.In the convex analysis literature, the horizon function is typically defined as \(_{}()=_{ 0}(/)\)(Rockafellar and Wets, 1998; Laghdir and Volle, 1999). In our context, this definition would yield a function which equals \(+\) everywhere except at the origin. In contrast, our definition ensures that \(_{}\) attains finite values over \(^{d}\). The distinction stems from our way of normalising the level sets by \(R_{c}\) in Section 4.1, or alternatively, from the composition by \(^{-1}\) in the separable case. The two constructions would coincide only if \(\) was Lipschitz continuous, which is at odds with Assumption 2.

## 5 Applications and experiments

In this section, we illustrate our main result using various potentials.

Homogeneous potentials.We first consider potentials \(\) which are \(L\)-homogeneous, i.e., there exists \(L>0\) such that for all \(c>0\) and \(^{d}\), \((c)=c^{L}()\). In this case, the sublevel sets \(_{c}\) are all equal. It follows that \(_{}^{1/L}\). An important example is the case of \(=\|\|_{p}^{p}\) where \(\|\|_{p}\) corresponds to the \(_{p}\)-norm with \(p>1\), for which we get that \(_{}\|\|_{p}\) and we recover the result from Sun et al. (2022, 2023).

Hyperbolic-cosine entropy potential.Finally, we consider \(^{_{1}}()=_{i=1}^{d}((_{i})-1)\). Applying Theorem 3, we get that \(_{}\|\|_{}\).

Hyperbolic entropy potential.The hyperbolic entropy potential: \(^{_{2}}()=_{i=1}^{d}(_{i}(_{ i})-^{2}+1}-1)\) plays a central role in works considering diagonal linear networks (Woodworth et al., 2020; Pesme and Flammarion, 2023). Applying Theorem 3, we obtain that \(_{}\|\|_{1}\) and we recover the result from Lyu and Li (2020) and Moroshko et al. (2020).

Experimental details concerning Figure 1.As shown in Figure 1 (Middle), we generate \(40\) points with positive labels and \(40\) points with negative labels. Starting from \(_{0}=0\), we run mirror descent with the exponential loss \((z)=(-z)\) and with the three following potentials:

\[(i)\ \ ^{}=\|\|_{2}^{2},(ii)\ \ ^{_{1}}= ,(iii)\ \ ^{_{2}}=.\]

We first observe in Figure 1 (Left) that the training loss converges to zero, as predicted by Proposition 1, with a convergence rate that varies across different potentials. Moreover, as illustrated in Figure 1 (Middle and Right), the iterates converge in direction towards their respective unique \(_{}\)-max margin solutions associated with the following geometries:

\[(i)\ \ _{}^{}\|\|_{2},(ii)\ \ _{}^{_{1}}=\|\|_{},(iii)\ \ _{}^{_{2}}\|\|_{1}.\]

Therefore, by employing various potentials, we can induce different implicit biases, leading to distinct generalisation properties depending on the data distribution. The trajectories of the mirror descent iterates are shown and commented in Figure 4.

Figure 4: Mirror flow trajectories on a 2-dimensional dataset for three different potentials (exact same setting as in Figure 1). _Left:_ the iterates diverge to infinity and the directional convergence depends on the choice of potential. _Right:_ the normalised iterates converge towards their respective \(_{}\)-maximum-margin predictors (illustrated by stars), as predicted by Theorem 2.

Conclusion and limitations

In this paper, we offer a comprehensive characterisation of the implicit bias of mirror flow for separable classification problems. This characterisation is framed in terms of the horizon function associated with the mirror descent potential, leveraging the asymptotic geometry induced by the potential. Note that we did not cover the **discrete** mirror descent algorithm; we believe the analysis would extend without additional difficulties compared to the continuous counterpart.

Extensions and open problems.Our results being purely asymptotic, characterising the rate at which the normalised iterates converge towards the maximum-margin solution is an open direction for future research. Furthermore, we note that our analysis does not cover potentials that are defined only on a strict subset of \(^{d}\) (such as the log-barrier and the negative entropy), and with possibly non-coercive gradients. This class of potentials is of interest as it arises when investigating deep architectures, such as diagonal linear networks of depth \(D>2\). In this setting, it is known that gradient flow on the weights lead to a mirror flow on the predictors with a certain potential \(_{D}\)[Woodworth et al., 2020]. Interestingly, the potentials \(_{D}\) have non-coercive gradients and their horizon functions do not depend on the depth \(D\) as they are all proportional to the \(_{1}\)-norm. The predictors are, however, known to converge in direction towards a KKT point of the non-convex \(_{2/D}\)-max-margin problem [Lyu and Li, 2020] which can be different from the \(_{1}\)-max-margin problem [Moroshko et al., 2020]. This observation highlights that our coercive gradient assumption is necessary for our result to hold. However, extending our analysis beyond this assumption is a promising direction for understanding gradient dynamics in deep architectures.