# Decompose, Analyze and Rethink:

Solving Intricate Problems with Human-like Reasoning Cycle

 Shangzi Xue\({}^{1}\) Zhenya Huang\({}^{1,2}\)1 Jiayu Liu\({}^{1}\) Xin lin\({}^{1}\) Yuting Ning\({}^{1}\)

Binbin Jin\({}^{1}\) Xin Li\({}^{1}\) Qi Liu\({}^{1,2}\)

1: State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China

2: Institute of Artificial Intelligence, Hefei Comprehensive National Science Center

{xueshangzi,jy251198,linx,ningyt,bb0725}@mail.ustc.edu.cn;

{huangzhy,lexin,qiliuql}@ustc.edu.cn

###### Abstract

In this paper, we introduce DeAR (_Decompose-Analyze-Rethink_), a framework that iteratively builds a reasoning tree to tackle intricate problems within a single large language model (LLM). Unlike approaches that extend or search for rationales, DeAR is featured by 1) adopting a tree-based question decomposition manner to plan the organization of rationales, which mimics the logical planning inherent in human cognition; 2) globally updating the rationales at each reasoning step through natural language feedback. Specifically, the _Decompose_ stage decomposes the question into simpler sub-questions, storing them as new nodes; the _Analyze_ stage generates and self-checks rationales for sub-questions at each node level; and the _Rethink_ stage updates parent-node rationales based on feedback from their child nodes. By generating and updating the reasoning process from a more global perspective, DeAR constructs more adaptive and accurate logical structures for complex problems, facilitating timely error correction compared to rationale-extension and search-based approaches such as Tree-of-Thoughts (ToT) and Graph-of-Thoughts (GoT). We conduct extensive experiments on three reasoning benchmarks, including ScienceQA, StrategyQA, and GSM8K, which cover a variety of reasoning tasks, demonstrating that our approach significantly reduces logical errors and enhances performance across various LLMs. Furthermore, we validate that DeAR is an efficient method that achieves a superior trade-off between accuracy and reasoning time compared to ToT and GoT.

## 1 Introduction

Learning to perform intricate reasoning, including commonsense reasoning , knowledge reasoning , and mathematical reasoning , is a crucial step towards achieving general artificial intelligence . The tasks always present a significant challenge as they require many human-like intricate problem-solving abilities, such as abstract thinking and logical inference, which could consolidate many decision-making applications in real-world scenarios .

Recent advances have witnessed remarkable performances of scaled-up large language models (LLMs) in various reasoning tasks, including GPT , LLaMA , and ChatGLM . They could enable several state-of-the-art prompting approaches like Chain-of-Thought (CoT) , Tree-of-Thoughts (ToT) , Graph-of-Thoughts (GoT) , etc., to enhancing reasoning capabilities. They not only improve problem-solving performance but also reveal their intrinsic reasoning steps(i.e., rationales)  through linear, tree-based, or graph-based structures. For example, in Figure 1 (a), given a math problem "Janet's ducks... in dollars... market?", ToT maintains a tree of thoughts with intermediate nodes to generate the rationales step by step. Specifically, through several operations including exploration, termination, and traceback on the nodes, ToT ultimately identifies the complete reasoning path, highlighting two-step rationales (green nodes) leading to the answer. However, although ToT and its variants  perform the reasoning process explicitly, such a rationale-extension and search-based reasoning paradigm is still far from human-like intelligence and limits problem-solving abilities to some extent: On one hand, this tree-like structure is rigid and sometimes illogical. The ToT approaches often require setting a fixed number of thought branches ("3" branches in Figure 1 (a)) each time it expands, which can result in either missing information or redundancy. Its reasoning process essentially extends previous rationales at each step, but falls short of the logical planning inherent in human thinking . On the other hand, ToT generates rationale paths sequentially, and errors along the path, such as incorrectly calculating "she has 16-3+4=17 eggs", cannot be promptly corrected. This allows mistakes to propagate to subsequent steps, ultimately leading to an incorrect final outcome (e.g., "34").

To address these challenges, we propose a novel reasoning paradigm **DeAR**_(Decompose-Analyze-Rethink)_, which enhances LLMs' capacity for complex problem-solving by emulating human reasoning (Figure 1 (b)). This approach is inspired by several theories in cognitive science . Specifically, reasoning simplification theory  suggests that when confronted with an intricate question, humans tend to break it down into simpler ones, which help in organizing thoughts and solving problems more logically. Referring back to Figure 1 (b), we can break down the logic by first solving two sub-questions (\(q_{1}\) and \(q_{2}\)). Upon examining \(q_{2}\), we find it can be further divided into three additional sub-questions (\(q_{3}\), \(q_{4}\), and \(q_{5}\)). By sequentially resolving these sub-questions and using their results as feedback to update answers for previously generated sub-questions (\(q_{1}\) and \(q_{2}\)), we ultimately arrive at the final answer ("18").

To implement such a human-like problem-solving process, we introduce a _Decompose-Analyze-Rethink_ cycle. This involves gradually constructing a reasoning tree guided by sub-questions, following a top-to-bottom reasoning process as illustrated in Figure 1 (b). The process begins with the _Decompose_ stage (black arrows in Figure 1 (b)), where a prompt-based method breaks down the question into simpler sub-questions at subsequent nodes. Then, the _Analyze_ stage (green box at each node) takes charge of problem-solving at the node level. The stage also introduces a self-check module to ensure the quality of the generated rationales, thus refines the reasoning process. Last, in the _Rethink_ stage (indicated by green arrows), the result at the current node is evaluated to determine if the reasoning in parent nodes requires further updates, providing a global perspective. After multiple cycles, the answer can be summarized from the root node.

Compared to ToTs  and GoT , our approach presents the following highlights. First, unlike ToT/GoT methods which directly generate rationales as branches from the original question,

Figure 1: Comparison between Tree-of-Thoughts (ToT) Reasoning and our DeAR (_Decompose-Analyze-Rethink_) Reasoning on a reasoning-based problem. (a) The simulation of Tree-of-Thoughts (ToT) (branch = 3). (b) The simulation of DeAR (_Decompose-Analyze-Rethink_) Reasoning.

DeAR breaks it into sub-question tree nodes to guide the generation. Second, our tree structure is more flexible and adaptable, as each node is generated and updated autonomously by the large language model based on the problem's logic, without relying on predefined settings. Third, DeAR enables timely correction of rationales, ultimately ensuring the correctness of the root node's answer.

We conduct extensive experiments on three complex reasoning benchmarks including ScienceQA , StrategyQA , and GSM8K . Experimental results show that our approach enhances the reasoning performance with different backbones such as GPT-3.5 , LLaMA2 , and ChatGLM3 . Compared to state-of-the-art methods such as Tree-of-Thoughts (ToT) and Graph-of-Thoughts (GoT), DeAR demonstrates a significant improvement in reasoning accuracy across all backbone LLMs, validating its generalizability and scalability. Additionally, by measuring the relationship between reasoning accuracy and reasoning time across different datasets, DeAR exhibits greater efficiency, further underscoring its advantages in practical applications.

## 2 Related Work

### Prompt-based Approaches in LLM Reasoning

There has been a growing interest in LLM reasoning research, with various prompting schemes applied in areas such as commonsense , mathematical  and knowledge reasoning , etc. Early methods appends examples on top of the input question (few-shot prompting  or performs in-context learning (ICL) ), or includes no examples at all (zero-shot prompting) .

Recent research has sought to enhance the capabilities of large language models (LLMs) by introducing intermediate reasoning steps into the prompting process, epitomized by methods such as the Chain-of-Thought (CoT) . By prompting LLMs to solve problems step by step, the CoT method demonstrates outstanding performance in multi-step reasoning tasks. Self-consistency  is a significant improvement upon CoT, where multiple CoT paths are initially generated, and the best one is selected as the final result, thereby improving the reliability of the outputs. In parallel, other prompting methods design search-based schemes for LLMs, such as Tree-of-Thoughts (ToT)  and Graph-of-Thoughts (GoT)  which innovate by structuring the reasoning process into tree or graph structures. These structures are created to take advantage of the many reasoning paths that LLMs can generate, greatly expanding the range and depth of exploration for any given question. More recently, Reasoning via Planning (RAP)  repurposes the LLM as both a world model and a reasoning agent to conduct reasoning. These methods expand the reasoning space of LLMs, which can fully leverage the diverse thinking paths generated by LLMs.

### Question Decomposition

Question decomposition, which decomposes complex questions into multiple sub-ones, has been shown to largely improve models' reasoning ability. Early works  decompose questions with hand-crafted rules and lexicon-syntactic features. These works heavily rely on human efforts, which are hard to extend to general domains and tasks. Recently, researchers utilize neural network models to decompose questions [39; 18; 52]. For example, Min et al.  focused on directly training a model to produce sub-questions using question spans; BREAK  followed an alternative paradigm of collecting full question decomposition meaning representations (QDMR) annotations. However, a primary challenge lies in the scarcity of annotations for training a decomposition model .

More recently, in the era of LLMs, there are a lot of work exploring LLMs for question decomposition [50; 19; 17; 10; 7; 22; 51]. For example, ToT  prompts the LLM to decompose the rationales by searching intermediate steps. Least-to-most prompting  leverages a few examples to teach LLMs to decompose each problem into a series of simpler sub-problems. These prompting-based question decomposition methods serve as an important step in reasoning and planning with LLMs.

## 3 Problem Formulation and Preliminaries

### Problem Definition

In this paper, we focus on the intricate reasoning task. The input of the task is the question \(Q\) (e.g., "Janet's ducks... market?" in Figure 1). The output is a rationale \(R=(r_{1},r_{2},...,r_{k})\) with \(k\) word tokens ("She makes \(9\$2=\$18\) per day."), and the answer \(A\) ("18") derived from \(R\). Given the input question \(Q\), we aim to design a reasoning framework with LLM backbone \(p_{}\) to generate the rationale \(R\) and answer \(A\) as outputs.

### Reasoning Tree

Motivated by the reasoning simplification theory , we propose a novel reasoning structure for LLMs, named Reasoning Tree \(T\), as shown in Figure 1(b). Overall, this Reasoning Tree decomposes and resolves sub-questions using a top-down approach, while concurrently updating existing solutions through a bottom-up process. Formally, the Reasoning Tree \(T\) can be defined as \(T=(N,E)\) where \(N\) is the set of tree nodes and \(E\) is the edge set. Each node \(n=(q,r,s) N\) contains a question \(q\) as a sub-question of the target \(Q\) (e.g., \(q_{2}\) "How many eggs does Janet have per day?"), a rationale \(r\) to \(q\) ("She has 16-(3+4) = 9 eggs per day."), and a score \(s\) evaluating the logical coherence of \(r\). Each directed edge \(e=(n_{p},n_{c}) E\) means that the upper-level sub-question \(q_{p}\) in the parent node \(n_{p}\) is decomposed into a lower-level one \(q_{c}\) in the child node \(n_{c}\) (e.g., the parent \(q_{2}\) "How many eggs... have per day" is decomposed into three children \(q_{3}\) "How many eggs... lay", \(q_{4}\) "How many eggs... breakfast", and \(q_{5}\) "How many eggs... muffins").

Our Reasoning Tree is progressively constructed and updated. The target question \(Q\) in the root node is decomposed into sub-questions step by step, from sub-questions in the higher levels to the ones in the lower levels (i.e., the black directed edges in Figure 1). For example, \(Q\) is first decomposed into \(q_{1}\) and \(q_{2}\), then \(q_{2}\) is further decomposed into \(q_{3}\), \(q_{4}\) and \(q_{5}\). Furthermore, humans could also rethink the rationales generated earlier (in the higher nodes) based on the ones generated later (in the lower nodes). For example, the rationales for \(q_{4}\) ("She cuts 3 eggs for breakfast") could be used to update rationales for \(q_{2}\) ("She has 16-(3+4) = 9 eggs per day") through the dashed lines in green.

### Framework Overview

To construct the aforementioned Reasoning Tree \(T\), which imitates human-like reasoning, we propose a novel **DeAR**_(Decompose-Analyze-Rethink)_ cycle as the core of our framework, as illustrated in Figure 2. The cycle is composed of three stages: _Decompose_, _Analyze_ and _Rethink_. Specifically, in the _Decompose_ stage, one upper-level question is decomposed into several lower-level ones. In the _Analyze_ stage, the framework solves the newly generated sub-questions by generating and self-checking rationales. In the _Rethink_ stage, the newly generated rationales are used to update existing ones in the parent nodes. The three stages work in a cycle to build the reasoning tree \(T\).

Figure 2: A demonstration of the DeAR (_Decompose-Analyze-Rethink_) cycle.

DeAR (_Decompose-Analyze-Rethink_) Cycle

In this section, we will demonstrate how the reasoning tree \(T\) is constructed with the _Decompose-Analyze-Rethink_ cycle, as demonstrated in Figure 2.

Initially, the target question \(Q\) is set as the question \(q_{0}\) in the root node \(n_{0}\). The framework selects an existing edge node \(n_{t}=(q_{t},r_{t},s_{t})\) (\(t\) is the level of the node) from \(T\) (e.g., \(n_{0}\) with \(Q\) "Janet's ducks... market?") to start the cycle. First, in the _Decompose_ stage (4.1), we prompt LLMs to decompose the question \(q_{t}\) in the node into sub-questions \(q_{t+1}\) if possible, and store them in nodes \(n_{t+1}\) at level \(t+1\) (e.g., \(q_{1}^{1}\) "What is... one egg?", and \(q_{1}^{2}\) "How many... per day?"). Then, in the _Analyze_ stage (4.2), we conduct reasoning and answers the newly generated questions \(q_{t+1}\) by generating rationales \(r_{t+1}\) for them (\(r_{1}^{1}\) "Each egg is sold for $2" for \(q_{1}^{1}\), and \(r_{1}^{2}\) "She has 16 eggs per day" for \(q_{1}^{2}\)), checking their correctness and evaluating the coherence scores \(s_{t+1}\) (Eq. (5)). Next, in the _Rethink_ stage (4.3), we use the newly generated \(r_{t+1}\) to update rationales in existing upper-level nodes \(r_{i}(i t)\) (e.g., use \(r_{1}^{1}\) and \(r_{1}^{2}\) to update \(r_{0}\) into \(r_{0}^{}\)). After that, the framework selects another edge node and returns to the _Decompose_ stage (e.g., decompose \(q_{1}^{2}\) into \(q_{2}^{1}\), \(q_{2}^{2}\) and \(q_{2}^{3}\)). The cycle continues until the LLMs determine that no further decomposition is possible, thereby forming the reasoning tree \(T\) for \(Q\).

As \(Q\) is the question \(q_{0}\) for the root node \(n_{0}\), after the tree-construction process, we consider the rationale \(r_{0}\) in the root node as the overall solution for \(Q\) and extract the answer \(A\) from \(r_{0}\). The whole procedure is described in Algorithm 1. In the following sections, we will technically describe the three stages in the cycle and make detailed analyses.

``` Input: Question \(Q\) Parameters: LLM \(p_{}\), natural language prompts (\(c_{1} c_{6}\)), threshold \(_{1}\) for Decompose, threshold \(_{2}\) for Reithink Output: Rationale \(R\), Answer \(A\) Create an empty node queue \(N\) Enqueue \(n_{0}(q_{0}=Q,r_{0}=None,s_{0}=1)\) into \(N\) while\(N\) is not empty and current level < max deptha do  Dequeue current node \(n_{t}(q_{t},r_{t},s_{t})\) from \(N\) if\(n_{t}\) is an end node \(n_{end}\)then  continue elseif\(s_{t}>_{1}\)then  // Stage 1: Decompose \(\{q_{t+1}^{j}\} Decompose(p_{},\ h_{1},\ lh_{Q},\ q_{t})\) (2)  // Stage 2: Analyze \(r_{t+1}^{j} Solve(p_{},\ h_{2},\ q_{t+1}^{j})\) (3) \(_{t+1}^{j} Self\_Check(p_{},\ h_{3},\ q_{t+1}^{j},\ r_{t+1}^{j})\) (4) \(s_{t+1}^{j} Score(p_{},\ h_{4},\ q_{t+1}^{j},\ _{t+1}^{j})\) (5)  Set \(n_{t+1}^{j}(q_{t+1}^{j},_{t+1}^{j},s_{t+1}^{j})\) (6)  Enqueue \(n_{t+1}^{j}\) into \(N\)  // Stage 3: Rethink if\(s_{t+1}^{j}>_{2}\)then \(L_{k} Extract(p_{},\ h_{5},\ L,q_{t+1}^{j})\) (7) \(r^{} Update(p_{},\ h_{6},\ n_{e}(q,r,s),_{t+1}^{j})\) (8) \(n_{e}(q,r^{},s) n_{e}(q,r,s)\) (6) else  Enqueue \(n_{end}\) into \(N\) endif endwhile \(R r_{0}\) Extract answer \(A\) from \(R\) return\(R\), \(A\) ```

**Algorithm 1**_Decompose-Analyze-Rethink_

### _Decompose_ Stage

According to the Analogical Reasoning theory , when humans conduct reasoning, they often analogize the logical processes of new questions to those of similar questions. Therefore, to make the decomposition logic of sub-questions \(q_{t}\) at each level \(t\) more closely resemble that of humans, we first use human-annotated question decomposition examples (_Appendix A.1_) as a demonstration pool \(P\). Then we calculate the cosine similarity of the representations between \(Q\) and each \(Q_{i}^{d}\) in \(P\) and select top-\(K\) nearest neighbors in the vector space. After that, we concatenate each \(Q_{i}^{d}\) with its human-annotated sub-questions \(subqs^{i}=(subq_{1}^{i},subq_{2}^{i},...,subq_{n}^{i})\) to form \(K\) question-decomposition examples (_Appendix A.1_)

\[lh_{Q}=(Q_{i}^{d},subqs^{i})(i=1,2,...,K).\] (1)

These examples are regarded as "logic heuristics" that inspire the model to decompose questions in a manner closely aligned with human reasoning.

After obtaining \(lh_{Q}\), we utilize them to decompose the sub-question \(q_{t}\) at level \(t\) into multiple sub-questions at level \(t+1\). Specifically, given question \(q_{t}\), if its coherence score \(s_{t}\) (Eq. (5)) is higher than a threshold \(_{1}\), We ask the LLM whether it needs to be further decomposed. If \(q_{t}\) requires decomposition, we then prompt the LLM to autonomously break it down into several sub-questions \(\{q_{t+1}^{j},j=1,...,J\}\). It is worth noting that in our decomposition approach, we do not pre-specify the number \(J\) of sub-questions; instead, we allow LLMs to adapt tively determine it based on the logic of each question. However, the number of sub-questions is capped at a predefined maximum branch limit to ensure computational efficiency and manageability 5.1.2. This enhances adaptability and more closely aligns with human logical characteristics when compared to existing methods like ToT  and GoT , etc. To facilitate this process, we design a heuristic-enhanced prompt that consists of a prompt head \(h_{1}\) and "logic heuristics" \(lh_{Q}\). The prompt head describes the question decomposition task in natural language. This process is formulated in Eq. (2). Additionally, we validate the effectiveness of using logic heuristics, and provide detailed explanations and templates in _Appendix_A.1.

\[\{q_{t+1}^{j},j=1,...,J\} Decompose(p_{},\;h_{1},\;lh_{Q},\;q_ {t}).\] (2)

After decomposition, each \(q_{t+1}^{j}\) is added as a new node \(n_{t+1}^{j}\) at level \(t+1\), with a directed edge from \(n_{t}\) to \(n_{t+1}^{j}\) (denoted as \(e^{j}=(n_{t},n_{t+1}^{j})\)). If the LLM determines that \(q_{t}\) does not require further decomposition, we create a leaf node \(n_{end}\) as a child of \(n_{t}\).

### _Analyze_ Stage

In _Analyze_ stage, we reason the answers for all the sub-questions \(\{q_{t+1}^{j}\}\) at level \(t+1\). To be specific, we first prompt the LLM to generate the essential rationale \(r_{t+1}^{j}\) for each sub-question \(q_{t+1}^{j}\):

\[r_{t+1}^{j} Solve(p_{},\;h_{2},\;q_{t+1}^{j}).\] (3)

Here, \(h_{2}\) denotes the prompt head, which is a natural language sentence that asks the model to generate detailed solutions (see _Appendix_A.2).

After obtaining the rationales for the sub-questions, we evaluate and correct them, as large language models (LLMs) often tend to hallucinate during problem-solving . Using generated rationales without verification can propagate errors, leading to incorrect outcomes. To address this issue, we develop a self-check method that promptly identifies and corrects these errors while providing a coherence score (Eq. (5)) for each node.

Specifically, we first instruct the LLM to perform a self-check on the rationale \(r_{t+1}^{j}\) generated for the sub-question \(q_{t+1}^{j}\) (see _Appendix_A.2 for the prompt head \(h_{3}\)) to identify any potential errors. If the LLM detects errors in the original rationale \(r_{t+1}^{j}\), it modifies the rationale to \(_{t+1}^{j}\); otherwise, the rationale is output unchanged. Take the case in Figure 2 as an example, we expect the LLM to identify the error "Each egg is sold for $3" in \(r_{1}^{1}\), and correct it to "Each egg is sold for $2". This process is denoted as:

\[_{t+1}^{j} Self\_Check(p_{},\;h_{3},\;q_{t+1}^{j},\;r_{t+ 1}^{j}).\] (4)

Then, we prompt the LLM to evaluate the logical coherence between the refined rationale \(_{t+1}^{j}\) and the question \(q_{t+1}^{j}\), by generating a coherence score \(s_{t+1}^{j}\) (see _Appendix_A.2 for prompt head \(h_{4}\)):

\[s_{t+1}^{j} Score(p_{},\;h_{4},\;q_{t+1}^{j},\;_{t+1}^{j }).\] (5)

The score \(s_{t+1}^{j}\) can also be obtained through voting or classification methods. Here, we specifically investigate the effectiveness of directly prompting LLMs to generate numerical values as scores.

At the end of the _Analyze_ stage, we fill the obtained rationales and scores into nodes \(n_{t+1}^{j}(j 1)\):

\[n_{t+1}^{j}=(q_{t+1}^{j},_{t+1}^{j},s_{t+1}^{j}).\] (6)

where \(s_{t+1}^{j}\) can support the current or subsequent cycles in _Rethink_ (4.3) and _Decompose_ (4.1).

### _Rethink_ Stage

According to self-reflection theories [11; 13; 6] in cognitive science, humans constantly update and reflect on their previous reasoning results based on the current information. This allows us to correct past mistakes and ultimately achieve a consistent and stable answer. For example in Figure 2, a person might initially answer question \(Q\) ("Janet's ducks... How much... market?") with the rationale \(r_{0}\) "She makes \(9 3=\$27\) per day". However,after considering responses to sub-questions \(q_{1}^{1}\) ("Whatis the selling price of one egg?") and \(q_{1}^{2}\) ("How many eggs does Janet have per day?"), he/she realizes an error in \(r_{0}\). The correct calculation, using the values "\(2\)" for the price per egg and "\(9\)" for the daily number of eggs, should be "\(2 9=\$18\)".

Nevertheless, existing methods like ToT  search reasoning paths based solely on preceding steps, lacking the ability to retrospectively update earlier content based on the influence of later steps. To address this, we introduce a _Rethink_ stage that mirrors the human reflective process.

Specifically, during the rethinking process, humans first identify which existing reasoning steps may require revision. We aim to automate this by using LLMs to detect logical connections between ancestral and newly generated nodes, updating ancestral nodes based on insights from the rationales of new nodes. In our proposed "Reasoning Tree", we essentially use information from lower-level nodes to "rethink" higher-level nodes, closely mirroring the human cognitive simplification process in problem-solving .

To achieve this, after obtaining node \(n_{t+1}^{j}\) in _Analyze_ Stage, we first check its coherence score \(s_{t+1}^{j}\) (Eq. (5)). If \(s_{t+1}^{j}\) exceeds the threshold \(_{2}\), we then examine the correlation between \(q_{t+1}^{j}\) and all sub-questions above level \(t\), specifically, \(\{q_{l},\ l t\}\). Next, we extract a subset of \(k\) most related nodes \(L_{k}\) from \(L\{n_{l},\ l t\}\) (the specific nodes to be extracted are determined by the LLM):

\[L_{k} Extract(p_{},\ h_{5},\ L,q_{t+1}^{j}),\ L_{k} L.\] (7)

where \(h_{5}\) is a prompt head (_Appendix_ A.3). Next, we use the rationale \(_{t+1}^{j}\) of sub-question \(q_{t+1}^{j}\) to update the rationale \(r\) of each extracted node \(n_{e}\) in \(L_{k}\):

\[r^{} Update(p_{},\ h_{6},\ n_{e}(q,r,s),\ _{t+1}^{j}).\] (8)

Finally, we replace \(r\) with the updated rationale \(r^{}\):

\[n_{e}(q,r^{},s) n_{e}(q,r,s).\] (9)

## 5 Experiments

In this section, we demonstrate the generality and effectiveness of DeAR by applying it to a wide range of tasks, including knowledge reasoning, logical reasoning and mathematical reasoning. The results across these tasks validate DeAR's adaptability and highlight its capability to effectively tackle a diverse range of challenging reasoning tasks.

### Experimental Setup

#### 5.1.1 Datasets and Baselines

We employ the **ScienceQA** dataset for the knowledge reasoning task. And we use **StrategyQA** for logical reasoning that requires multiple reasoning steps. We also verify the mathematical reasoning ability of our framework by applying it to **GSM8K** dataset . The details of these datasets are available in _Appendix_ B.1.1.

In our main results, we compare DeAR with multiple prompt-based methods including **Few-shot** prompting , **Chain-of-Thoughts (CoT)** prompting , and state-of-the-art **Tree-of-Thoughts (ToT)** and **Graph-of-Thoughts (GoT)** prompting. Besides, we also list extra comparison results with another two state-of-the-art prompt-based methods **Least-to-most** Prompting  and **SelfCheck** (see _Appendix_ B.1.2 for all baseline details).

#### 5.1.2 Implementation Details

We conduct experiments with three LLM backbones **GPT-3.5**, **LLaMA2-7B** and **ChatGLM3-6B**. For GPT-3.5, we use the OpenAI API to invoke the "gpt-3.5-turbo-1106" model. For LLaMA2-7B and ChatGLM3-6B, we load the checkpoints from huggingface23 and use the models directly without fine-tuning as the backbone.4. For each dataset, we randomly sample 10% of its training set as a validation set to select different combinations of thresholds \(_{1}\) and \(_{2}\). The combination that achieves the best performance on the validation set is then used for inference on the test set. We observe that the threshold combinations obtained through this method also yield optimal inference results on the test set. In Section 5.6, we visualize the inference accuracy on the test sets across different datasets based on GPT-3.5, using diffenrent threshold combinations. The implementation and prompting templates (i.e., natural language prompts \(h_{1} h_{6}\) for _Decompose_, _Analyze_ and _Rethink_ ) are shown in _Appendix_ A. To ensure computational efficiency, we set the maximum depth to 4 and the maximum number of branches to 3 during the construction of the reasoning tree in DeAR. This prevents the tree from becoming excessively deep and avoids redundancy in sub-questions. For baselines, the settings used in the experiments are consistent with those described in the original papers. For a concise description of baselines, please refer to _Appendix_ B.1.2.

### Experimental Results

We conduct experiments to verify the effectiveness of our framework DeAR, and report the results in Table 1. We use the accuracy (ACC) as metric for all three datasets. We statistically test the improvement over baselines with paired t-test, and find the improvement to be significant with \(p<0.05\) (marked with "\(*\)"). We get the following observations. First, DeAR performs better than all baselines, which indicates it is more effective in enhancing LLMs' reasoning ability. Second, the improvements over ToT highlight the advantage of _Decompose_ stage which adaptively decomposes questions based on their characteristics rather than extending a fixed number of thought branches. Third, DeAR performs better than GoT which lacks rationale updating. This reflects the superiority of the _Rethink_ stage to identify correlations between reasoning steps and update previous rationales. Besides, the accuracy increase on GSM8K is greater than ScienceQA and StrategyQA. That is probably because problems in GSM8K require longer rationales to be solved (Table 2). Furthermore, DeAR outperforms the Least-to-most  and SelfCheck  methods across all datasets. The Least-to-most method sequentially solves sub-problems derived from the decomposition without updating content that has already been generated; SelfCheck updates rationales but it does not decompose the original question. In contrast, DeAR not only generates rationales based on decomposed sub-questions but also updates existing rationales in each cycle. This further underscores the necessity of the Decompose and Rethink phase in DeAR for enhancing the reasoning capabilities of LLMs.

We have also validated that DeAR enhances stronger LLMs (e.g., GPT-4) on complex reasoning tasks (e.g., MATH), as shown in _Appendix_. B.3 includes an ablation study on the self-check method in the _Analyze_ stage, as its removal does not structurally impact the other stages.

### Analyses of the Reasoning Tree

For each question \(Q\), DeAR constructs a reasoning tree \(\) to represent the reasoning process, as shown in Figure 1 (b). The structure of \(T\) provides insights into the complexity of \(Q\). To analyze the nature of questions across datasets, we examine reasoning trees from three datasets using three metrics: "Avg Branch," "Avg Depth," and "Avg Length of \(R\)." "Avg Branch" indicates the average branching factor of \(T\), "Avg Depth" reflects the average depth of \(T\), and "Avg Length

    &  &  &  \\   & GPT-3.5 & LLaMA2 & ChatGLM3 & GPT-3.5 & LLaMA2 & ChatGLM3 & GPT-3.5 & LLaMA2 & ChatGLM3 \\  Few-shot & 73.97 & 66.35 & 42.46 & 67.71 & 61.21 & 54.41 & 74.26 & 72.25 & 51.02 \\ CoT & 75.17 & 67.58 & 46.35 & 69.26 & 63.86 & 57.18 & 79.55 & 74.04 & 53.85 \\ ToT & 82.52 & 69.01 & 49.58 & 71.89 & 66.52 & 59.21 & 83.42 & 75.22 & 55.88 \\ GoT & 82.34 & 68.86 & 49.26 & 72.02 & 66.61 & 59.88 & 84.77 & 75.95 & 56.01 \\
**DeAR** & **83.06*** & **70.57*** & **51.08*** & **73.36*** & **63.33*** & **61.02*** & **86.82*** & **78.01*** & **58.54*** \\   Least-to-most & 76.61 & 68.02 & 47.45 & 70.55 & 64.43 & 58.36 & 81.25 & 74.67 & 54.21 \\ SelfCheck & 75.81 & 69.33 & 49.23 & 68.87 & 66.35 & 61.22 & 79.88 & 75.28 & 56.72 \\   

Table 1: Overall results of our DeAR Framework on three intricate reasoning datasets. (\(*:p<0.05\)).

    & ScienceQA & StrategyQA & GSM8K \\  Avg Branch & 1.58 & 2.43 & 2.06 \\ Avg Depth & 3.62 & 1.96 & 2.55 \\ Avg Length of \(R\) & 66.34 & 61.55 & 85.27 \\   

Table 2: Characteristics of \(T\) in different datasets.

of \(R\)" represents the length of rationale \(R\) derived from the root node \(n_{0}\) upon tree completion, e.g., \(R=r_{0}\):"She makes 9*2=$18 per day" in Figure 2.

Using GPT-3.5 as the backbone, results in Table 2 reveal the following: ScienceQA questions have the highest "Avg Depth" and lowest "Avg Branch," indicating fewer sub-questions per _Decompose_ stage but more rounds required. StrategyQA questions have the lowest "Avg Branch" but the highest "Avg Depth," suggesting fewer _Decompose_ rounds but more sub-questions per round. For GSM8K, the root node \(n_{0}\) has longer rationales \(R\), suggesting that these questions require more extensive explanations than those in the other datasets.

### Logical Coherence of the Generated Rationales

We assess the logical coherence of rationales generated by DeAR using both automatic and human evaluation methods. For automatic metrics, we apply the Source-Consistency" (SC) and Reasoning Alignment" (RA) from the ROSCOE evaluation suite . SC measures logical entailment between question and rationale, while RA evaluates alignment with ground truth. As shown in Table 3, DeAR outperforms ToT and GoT on all datasets. For human evaluation, 100 questions were sampled from each dataset, with annotators selecting the most logical rationale among those generated by ToT, GoT, and our method (details in B.4). Results in Figure 3 confirm that DeAR (using GPT-3.5) produces rationales with superior logical coherence compared to ToT and GoT.

### Effectiveness of Rethink

In _Rethink_ stage, our DeAR employs the same backbone LLMs to determine which nodes' rationales need to be updated. To validate its effectiveness, based on GPT-3.5, we compare our method with "Random Update" method which randomly selects nodes to update at different proportions. The results in Table 3 demonstrate that, compared to "Random Update", our method performs better in terms of accuracy. Additionally, unlike approaches that require a 100% update of all generated rationales, DeAR's targeted updates allow the model to autonomously select nodes that need refinement, thus minimizing unnecessary inference.

### Combinations of Thresholds

In this subsection, we visualize the impact of different combinations of threshold values \(_{1}\) and \(_{2}\) on the inference accuracy of DeAR (with GPT-3.5 backbone) across the test sets of all three datasets. \(_{1}\) and \(_{2}\) are set for the _Decompose_ stage (Section 4.1) and _Rethink_ stage (Section 4.3), respectively, with their value combinations selected based on

   Random Update & ScienceQA & StrategyQA & GSM8K \\ 
0\% & 82.77 & 72.84 & 85.09 \\
20\% & 81.77 & 72.21 & 83.96 \\
40\% & 82.59 & 73.03 & 84.35 \\
60\% & 82.06 & 72.29 & 85.07 \\
80\% & 81.49 & 72.04 & 86.01 \\
100\% & 81.16 & 71.79 & 85.32 \\ 
**DeAR** & **83.68** & **73.36** & **86.82** \\   

Table 4: Comparisons between different portions of “Random Update” and DeAR.

Figure 4: Combinations of threshold values (\(_{1}\), \(_{2}\)) and corresponding ACCs on test sets (GPT-3.5 backbone).

Figure 3: The distributions of annotators’ selections. More annotators considered DeAR’s rationales to be more logical.

    &  &  &  \\   & SC & RA & SC & RA & SC & RA \\  ToT & 0.44 & 0.31 & 0.47 & 0.33 & 0.56 & 0.41 \\ GoT & 0.42 & 0.35 & 0.44 & 0.38 & 0.53 & 0.45 \\
**DeAR** & **0.48** & **0.42** & **0.52** & **0.43** & **0.58** & **0.50** \\   

Table 3: ROSCOE evaluation results of rationales generated by Tree-of-Thoughts (ToT), Graph-of-Thoughts (GoT) and DeAR on different datasets. SC = Source-Consistency; RA = Reasoning Alignment.

performance on the validation set (Section 5.1.2). We observe from Figure 4 that, DeAR achieves the highest accuracy when setting \(_{1}=0.4\) and \(_{2}=0.6\) for ScienceQA and StrategyQA. For GSM8K, the highest accuracy is obtained with \(_{1}=0.4\) and \(_{2}=0.4\). The threshold combinations that optimize DeAR's performance on the test set are consistent with those obtained from the validation set (e.g., Val: \(_{1}=0.4\); \(_{2}=0.6\) for ScienceQA), demonstrating the validity of the value selection method. Additionally, the smaller optimal \(_{2}\) value for GSM8K suggests that tackling GSM8K problems requires a more frequent or active rethinking process compared to ScienceQA and StrategyQA. This difference highlights the varying nature of reasoning demands across different tasks, where the threshold tuning helps adapt DeAR's reasoning process accordingly.

### Efficiency

Compared to the rationale extension in ToT and GoT, DeAR incorporates question decomposition and rationale updating. Thus, will the efficiency of reasoning be affected? To investigate this, we use ChatGLM3-6B as the backbone model and measure the average inference time per question (seconds/question) and accuracy (ACC) for each method. The results are in the form of scattered points as shown in Figure 5. We set the fixed branch numbers and depths for these variants of ToT and GoT (e.g., b=3, d=4), and compare them with DeAR. In ToT/GoT, we set "b" and "d" (integers) as close to DeAR's average values as possible to ensure fairness. We can observe that points closer to the upper-left corner, and farther away vertically from the diagonal, represent methods that achieve a better trade-off between reasoning accuracy and time. The points corresponding to DeAR clearly exhibit this characteristic, hence we can conclude that it has higher efficiency. Moreover, in Appendix B.5, to further validate this conclusion, we measured the average number of API calls made by DeAR, ToT, and GoT per question in the ScienceQA dataset using GPT-3.5, as well as their reasoning accuracy. DeAR consistently requires fewer API calls on average to solve a question, while simultaneously achieving higher accuracy.

## 6 Conclusion

In this paper, we introduced DeAR (_Decompose-Analyze-Rethink_), a framework designed to mimic human reasoning patterns in tackling intricate problems by constructing a reasoning tree in a top-down, iterative manner.This approach is coupled with a _Decompose-Analyze-Rethink_ cycle, in which the rationale at each node is generated, evaluated, and refined through feedback loops. Specifically, the _Decompose_ stage applies logic heuristics to decompose the original question, the _Analyze_ stage produces and self-checks rationales, and the _Rethink_ stage integrates these insights by updating parent nodes based on child-node feedback. Extensive experiments demonstrate that DeAR not only improves reasoning performance across different large language models (LLMs) but also surpasses current state-of-the-art methods like Tree-of-Thoughts (ToT) and Graph-of-Thoughts (GoT) in logical coherence and accuracy. DeAR's rationale update mechanism enhances logical consistency by iteratively refining previously generated rationales, achieving more accurate and interpretable results. Additionally, compared to ToT and GoT, DeAR strikes a better balance between reasoning accuracy and inference time, further improving efficiency. Case studies also demonstrate that DeAR produces more interpretable reasoning process (_Appendix_ B.6).

Figure 5: Efficiency comparison between DeAR and variants of ToT/GoT.