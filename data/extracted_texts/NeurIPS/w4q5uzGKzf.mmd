# Preference-based Multi-Objective Bayesian

Optimization with Gradients

Joshua Hang Sai Ip\({}^{1}\) Ankush Chakrabarty\({}^{2}\) Hideyuki Masui\({}^{3}\) Ali Mesbah\({}^{1}\) Diego Romeres\({}^{2}\)

\({}^{1}\)University of California, Berkeley \({}^{2}\)Mitsubishi Electric Research Laboratories

\({}^{3}\)Mitsubishi Electric

\({}^{1}\){ipjoshua, mesbah}@berkeley.edu \({}^{2}\){chakrabarty,romeres}@merl.com

\({}^{3}\)masui.hideyuki@bc.mitsubishiielectric.co.jp

###### Abstract

We propose PUB-MOBO for personalized multi-objective Bayesian Optimization. PUB-MOBO combines utility-based MOBO with local multi-gradient descent to refine user-preferred solutions to be near-Pareto-optimal. Unlike traditional methods, PUB-MOBO does not require estimating the entire Pareto-front, making it more efficient. Experimental results on synthetic and real-world benchmarks show that PUB-MOBO consistently outperforms existing methods in terms of proximity to the Pareto-front and utility regret.

## 1 Introduction

Multi-objective Bayesian optimization (MOBO) is a particularly useful multi-objective optimization (MOO) strategy when the objectives are black-box functions constructed from noisy observations. Traditional MOBO methods such as \(q\)-EHVI  assume that all Pareto-optimal solutions are equally desirable to the user, which might not be the case in practice. There has been a growing interest in preference-based MOBO (e.g., ) that leverages user preferences to guide the optimization process towards regions of interest within the Pareto-front, typically in the form of pairwise comparisons between solutions generated by the optimization algorithm. These comparisons are used to estimate an underlying utility function that describes user preferences. In , the authors propose the EUBO and qEIUU acquisition functions respectively, which take advantage of user-preference when querying new points. However, while preference-based MOBO can effectively identify solutions with high utility as informed by user feedback, the resulting solutions may not be Pareto-optimal.

We present Preference-Utility-Balanced MOBO (PUB-MOBO), that systematically determines the user-informed regions of interest within the Pareto-front by synergizing global and local search strategies. PUB-MOBO begins with a global search driven by utility maximization to identify regions in the solution space that align with user preferences. Subsequently, a local search is conducted in the vicinity of these solutions to discover dominating solutions that are closer to Pareto-optimality. Additionally, a new utility function, the Preference-Dominated Utility Function (PDUF), is proposed that encapsulates the concept of dominance within a single function. PDUF allows for consistently identifying dominating solutions, while providing a straightforward means for expressing all possible user preferences. This differs from existing utility functions for preference-based MOBO such as the negative \(_{1}\) distance from an ideal solution irrespective of the solution being on the Pareto-front or an infeasible ideal solution , or the weighted sum where not all Pareto-optimal points can be assigned with the highest utility value from any choice of weights . PDUF is then used in conjunction with gradient descent (GD) to seamlessly combine user preferences with the notion of dominance to identify user-preferred solutions that are approximately Pareto-optimal. Empirical demonstrations onseveral synthetic benchmark and real-world problems show that PUB-MOBO not only enhances the utility of the optimization solutions, but also yields near Pareto-optimal solutions.

## 2 Problem Formulation

We _minimize_\(n_{f}\) expensive-to-evaluate objective functions, denoted by \(f_{i}()\) for \(i\{1,,n_{f}\}\). Consequently, the objective function vector is denoted \(()\), where \(^{n_{x}}\) denote the decision variables. We assume that for a candidate \(\), the function \(()\) can be evaluated, but no first- or higher-order information about any component of \(\) is available. No analytical form of \(\) is known.

For MOO problems without user-preferences, the objective is to attain Pareto-optimality, which is defined as follows . Note that accurately computing the set of Pareto-optimal points, referred to as the Pareto-front \(_{}\), can often be computationally prohibitive, even for small \(n_{f}\). In the presence of a user, estimating the entire Pareto-front may become unnecessary, especially when only specific sub-regions of the feasible set \(\) is of interest. Mathematically, such user-preferences are often abstracted in the MOBO literature via _utility functions_. Specifically, the MOO problem is recast as a (scalar) utility maximization problem

\[_{} u(()),\] (1)

where \(u:^{n_{y}}\) is the unknown utility function that dictates the behavior of the user. Note that the input to the utility is a noise-corrupted outcome vector \(=()+\), where \(\) is zero-mean noise with variance \(_{}^{2}_{n_{y}}\) where \(_{n_{y}}\) is the \(n_{y} n_{y}\) identity matrix. Let the highest utility Pareto-point be defined as

\[^{*}*{arg\,max}_{_{}}u( ()).\] (2)

Following the preference BO literature, we assume the utility function is not available to evaluate and its functional form is unknown. Additionally, it is well-established that user preferences are difficult to be assigned to continuous numerical values; instead we suppose that users are more inclined to provide weak supervision in the form of pairwise comparisons [8; 4]. The following assumption is made to assert that a typical user will select dominating solutions when possible: _If \(_{1}\) and \(_{2}\) are candidate outcomes presented to the user and \(_{1}_{2}\), then the user will always select \(_{1}\); that is, \(u(_{1})>u(_{2})\)_. This assumption should be enforced when modeling preference-based MOBO problems to accurately reflect real user behavior.

## 3 Preference-Utility-Balanced (PUB) MOBO

Users often require some assurance that the suggested candidates are not only high in utility, but also near-(Pareto)-optimal. PUB-MOBO relies on utility maximization to ascertain candidate solutions that are preferred by the user while promoting a local search towards the Pareto-front using estimated gradients. We observe that the local search finds solutions near Pareto points, which subsequently accelerates the search for high-utility solutions.

### PUB-MOBO Algorithm

The proposed PUB-MOBO method operates in three stages. We extend the two stages (PE: preference exploration, and EXP: outcome evaluation via experiments) in  with an additional stage based on local multi-gradient descent, denominated the GD stage. In each PUB-MOBO iteration, these three stages are executed, and the process is repeated _ad infinitum_, or (more practically) until a pre-decided budget for total number of outcome evaluations is attained; see Algorithm 1 in Appendix C.

**Preference Exploration**: Here, the user expresses their preferences over a query of two candidate solutions in a form of pairwise comparisons. The comparison is used to update the estimate \(\) of the utility, obtained implicitly with a pairwise GP and the EUBO acquisition function proposed in ; see Appendix B.1 for the closed-form expression. Note that no evaluation of \(\) is required for PE.

**Outcome evaluation via Experiments**: Here, we compute the optimal decision variables and evaluate true outcomes to update the outcome model \(}\) using the expected improvement under utility uncertainty (qEIUU)  acquisition; see Appendix B.2 for the closed-form expression. Maximizing qEIUU involves taking Monte Carlo samples [10; 11] and yields the optimal decision variables, \(_{}\)After \(_{}\) is obtained, we append it along with its true outcome value \((_{})\) to the current dataset.

**Multi-gradient descent**: This GD stage is motivated by the fact that \(_{}\), while expected to be high in utility, is not specifically designed to be near the Pareto-front. Analogous to single-objective optimization, we will pursue local gradients that are expected to generate a trajectory of \(\) candidates that evolves towards a nearby Pareto-optimal point. We will refer to these gradient-following decision variables as '\(_{}\)'. We set the initial \(_{}\) to be \(_{}\).

For a MOO problem, gradient descent must be adapted for multiple objectives. We propose the use of multiple gradient descent algorithm (MGDA) , which was designed for smooth multi-outcome objective functions. MGDA exhibits some theoretical properties that, we hypothesize, and demonstrate via experiments, are beneficial in the MOBO context.

MGDA exploits the KKT conditions  as a quadratic cost constrained on the probability simplex:

\[_{}\ \|^{}() \|^{2}\ \ ^{}=1.\] (3)

It is well-known, c.f. , that a solution to (3) is either: \(^{}()=0\), in which case the current parameters \(\) are Pareto-optimal, or \(^{}() 0\), and \(^{}()\) is a feasible descent direction. Given that (3) is a quadratic cost over linear constraints, we can use the Frank-Wolfe algorithm [14; 15] to efficiently compute optimal solutions; see pseudocode in Algorithm 3 in Appendix C.

Solving (3) yields an optimal \(\) with which we can take a gradient step \(_{}_{}-^{}f(_{})\). However, there are two clear difficulties at this juncture. The first is that this update may yield an \(_{}\). To counter this, we stop updating when this happens, and stop the local gradient search phase, moving on to the next PUB-MOBO iterations with an updated dataset \(D\) that contains all the \(_{}\) and correspond \(_{}\) observed so far. The second and more debilitating problem is that we do not have access to gradients of \(\). Thankfully, we do have a surrogate model \(}\) with which we can obtain an estimate of the gradient at any \(\) with \(^{}:=[}()]\) through (7a). The gradient step is then \(_{}_{}-^{} ^{}(_{})\). Unfortunately, there is no clear correlation between the uncertainties in \(\) and \(\), so \(^{}\) could have large uncertainties even near previously observed points. Therefore, it is imperative to incorporate techniques that can reduce uncertainty in the posterior of the gradient estimate. To this end, we propose to use the gradient information (GI) acquisition function .

**Multi-gradient descent with GI acquisition**: We briefly explain the mechanism of the GI acquisition. Suppose we select the best candidate from the EXP stage, \(_{}\), and set it as the initial candidate for the local gradient search: \(_{}\). The GI acquisition tries to select a subsequent point \(^{}\) that will minimize the uncertainty of the gradient at \(_{}\) if \(^{}\) and its corresponding \(^{}\) were known. By considering all \(n_{f}\) objective independently distributed, we assess the uncertainty can formulate the uncertainty information using an A-optimal design criterion , which, for Gaussian distributions, involves maximizing:

\[(^{})=_{i=1}^{n_{f}}( k_{i}(_{},^{})_{}^{-1}(^{})  k_{i}^{}(_{},^{}))\] (4)

where \(^{}=\{^{}\}\). For each gradient-step in \(n_{}\), the GI acquisition function is optimized \(n_{}\) times to reduce gradient uncertainty. Upon each optimization, we evaluate the outcome function to obtain a corresponding \(_{}\), which is appended to the dataset \(D\) for subsequent PUB-MOBO iterations. We provide the derivation of the GI acquisition function in Appendix B.3 and pseudocode of multi-gradient descent in Algorithm 2, in Appendix C.

### Preference-Dominated Utility Function

We propose the PDUF, which merges the concept of dominance with user preferences to help locate high utility points that are close to Pareto-optimality. The utility function, which represents user preferences, is employed to respond to user queries, such as providing pairwise comparisons between two outcomes . It should satisfy two key properties:

1. _Dominance Preservation_: When evaluating a query, the true utility function should satisfy Assumption 1.
2. _Preference Integration_: The utility function should have parameters \(_{u}\) that allow unique strictly maximal-utility Pareto-optimal solutions. That is, for any \(_{}\), there exists an easily computable \(_{u}^{n_{u}}\) such that \(u(()|_{u})>u((\{_{} \}|_{u})\).

For instance, the commonly used \(_{1}\) distance (a) fails to satisfy the _Preference Integration_ property when calculated from the utopia point, and violates _Dominance Preservation_ when calculated from any other point. This is illustrated in Fig. 0(a), where the contours of an \(_{1}\) distance utility function is shown with an example Pareto-front. Here, the two red points are indistinguishable according to the utility function, demonstrating the limitations of \(_{1}\) distance in distinguishing between Pareto-optimal solutions.

Therefore, we propose the preference-dominated utility function (PDUF) which merges the concept of dominance with user preferences. An illustration of the contours in a 2D case is shown in Fig. 0(b). The PDUF integrates the concept of dominance with user preferences by combining multiple logistic functions centered around different points in the objective function space and is expressed as:

\[u()=}_{i=1}^{n_{c}}_{j=1}^{n_{y}}L_{}(y_{j},c _{i,j})\] (5)

where \(L_{}(y_{j},c_{i,j})=(y_{j}-c_{i,j}))}\). \(c_{i}=(c_{i,1},c_{i,2},,c_{i,n_{y}})\) denotes the \(i^{}\) center for one logistic function, \(\) denotes a parameter that controls the steepness of the logistic function, and \(n_{c}\) denotes the number of centers. The logistic function \(L_{}(y_{j},c_{i,j})\) approximates the step function and enforces dominance for each objective \(y_{j}\), as seen in the red dashed lines in Fig. 0(b), and the product aggregates this approximation for all objectives. Furthermore, the sum of logistic function products preserve dominance in the objective space. Indeed, for every \(}\) that dominates user query \(_{i}\), PDUF will express user preference with \(u(})>u(_{i})\). Finally, the centers define the parameters \(_{u}\) that ensure the utility function adheres to the _preference integration_ property by aligning them along an arbitrary line (the grey line in Fig. 0(b)).

## 4 Experiments

We validate the proposed PUB-MOBO method on benchmarks commonly found in MOO literature: DTLZ1 (\(n_{x}=9,n_{f}=2\)) , DH1 (\(n_{x}=10,n_{f}=2\)) , Conceptual Marine Design (\(n_{x}=6,n_{f}=4\)) , Car Side Impact (\(n_{x}=7,n_{f}=4\)) . The baselines and ablations that we compare are (i) EUBO+qEIUU baseline which contains only the PE and EXP stages; (ii) PUB-MOBO-PG which uses the predicted gradients (PG) without any outcome evaluations or GI optimizations in the GD stage. This makes it relatively inexpensive, but it ignores the fact that additional samples can yield useful derivative information; (iii) PUB-MOBO-PG+OE which is a PUB-MOBO ablation that uses the predicted gradients as in PUB-MOBO-PG, but an Outcome Evaluation (OE) is performed at every gradient descent step in an effort to lower gradient uncertainty around observed points; (iv)

Figure 1: Contour plots of (a) the commonly used negative \(l_{1}\) distance Utility function (b) the proposed PDUF.

PUB-MOBO which is the proposed method. Figure 2 illustrates the performance of the experiments in terms of utility regret and distance to the Pareto front w.r.t. outcome evaluations and user queries. EUBO+qEIUU is the poorest-performing algorithm for all the metrics affirming the effectiveness of the additional stage based on local gradient search. However, PUB-MOBO-PG performs equally poorly, largely due to inaccurate gradient estimation obtained with the surrogate model \(}\) in (7). We frequently observe that the evolution of \(_{}\) in the GD stage is prematurely terminated either due to infeasibility in \(\) or because of incorrect solutions to MGDA due to erroneous \(^{}(_{})\). The PG+OE variant significantly outperforms the PG variant due to its enhanced gradient estimation accuracy, which justifies the additional computational cost of updating the outcome model. PUB-MOBO further improves on the PG+OE variant by using the GI acquisition function to reduce gradient uncertainty, leading to even more accurate gradient estimates.

## 5 Conclusion

In this work we presented PUB-MOBO, a sample efficient Multi-Objective Bayesian Optimization algorithm that combine user-preference with a gradient-based search to compute near Pareto-optimal solutions. We verify that our proposed method yields high utility and reduced distance to Pareto-front solutions, and also demonstrate the importance of gradient uncertainty reduction in the gradient-based search. Finally, the proposed utility function respects dominance while modeling different user preferences.

Figure 2: Performance comparison on benchmarks DTLZ1, DH1, Conceptual Marine Design, Car Side Impact. Continuous lines show median over 100 runs, and shading indicates 25-75 percentiles.