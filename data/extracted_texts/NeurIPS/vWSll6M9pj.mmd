# Unified Speech Recognition: A Single Model for Auditory, Visual, and Audiovisual Inputs

Alexandros Haliassos

Imperial College

ah2214@ic.ac.uk

&Rodrigo Mira

Imperial College

rs2517@ic.ac.uk

&Honglie Chen

Meta AI

hongliechen@meta.com

&Zoe Landgraf

Meta AI

zoelandgraf@meta.com

&Stavros Petridis

Meta AI / Imperial College

stavrosp@meta.com

&Maja Pantic

Meta AI / Imperial College

majapantic@meta.com

###### Abstract

Research in auditory, visual, and audiovisual speech recognition (ASR, VSR, and AVSR, respectively) has traditionally been conducted independently. Even recent self-supervised studies addressing two or all three tasks simultaneously tend to yield separate models, leading to disjoint inference pipelines with increased memory requirements and redundancies. This paper proposes unified training strategies for these systems. We demonstrate that training a single model for all three tasks enhances VSR and AVSR performance, overcoming typical optimisation challenges when training from scratch. Moreover, we introduce a greedy pseudo-labelling approach to more effectively leverage unlabelled samples, addressing shortcomings in related self-supervised methods. Finally, we develop a self-supervised pre-training method within our framework, proving its effectiveness alongside our semi-supervised approach. Despite using a single model for all tasks, our unified approach achieves state-of-the-art performance compared to recent methods on LRS3 and LRS2 for ASR, VSR, and AVSR, as well as on the newly released WildVSR dataset. Code and models are available at https://github.com/ahaliassos/usr.

## 1 Introduction

Speech recognition can be achieved using auditory signals (known as auditory/automatic speech recognition; ASR) , visual cues from lip movements (visual speech recognition; VSR) , or both (audiovisual speech recognition; AVSR) . Audio typically offers the most relevant information in videos of talking faces, but lipreading can greatly enhance recognition, especially when the audio is noisy or wholly unavailable . Despite the similarities between ASR, VSR, and AVSR, research in these fields has largely developed independently .

The Transformer architecture's versatility  has spurred efforts to unify speech recognition by pre-training a single model on various unlabelled inputs (visual, auditory, and audiovisual) through self-supervision . However, these methods often require separate fine-tuning stages for ASR, VSR, and AVSR, leading to separate models for each task, which increases computational load and complexity. u-HuBERT  shows that a single pre-trained model _can_ be fine-tuned for all three tasks, yet does not reach the performance of separately fine-tuned models .

In this paper, we delve deeper into strategies for unified speech recognition (USR) by training a single model to perform ASR, VSR, and AVSR. We find that training such a model _from scratch_ on the LRS3 dataset  achieves competitive performance on all tasks. This is notable given the knownoptimisation difficulties in VSR training, which previously required self-supervised pre-training , supervised feature extractor pre-training , or curriculum learning strategies . Our findings suggest that including audio improves the optimisation landscape for VSR and AVSR supervised training, as observed in a different context by .

Furthermore, we propose a semi-supervised pseudo-labelling approach to leverage unlabelled audiovisual data, addressing shortcomings of standard fine-tuning in self-supervised methods [13; 14; 17; 18]. Fine-tuning often leads to overfitting due to using fewer samples than pre-training, requiring various "tricks" to reach optimal performance [13; 17]. This issue is particularly pronounced in encoder-decoder architectures where usually only the encoder is pre-trained, and attempts to pre-train the decoder have yielded inconsistent results [21; 22]. Our semi-supervised approach generates pseudo-labels via an encoder-decoder momentum-based teacher  to leverage unlabelled samples throughout training, effectively mitigating overfitting. Training on all three modalities simultaneously helps alleviate the computational cost of pseudo-labelling as the cost is amortised across the inputs.

Lastly, inspired by recent self-supervised works, we design a pre-training method within our unified framework. We combine pre-training with pseudo-labelling and show that our semi-supervised approach is complementary to self-supervision. Our final unified models achieve state-of-the-art results across multiple settings, surpassing existing methods that use separate models for each task.

## 2 Related Work

Audiovisual self-supervised speech representation learning.Recent interest in audiovisual self-supervised learning for speech recognition has focused on leveraging the correspondence between audio waveforms and silent lip movements to capture shared semantic content across the modalities [13; 17; 14; 15; 18]. These methods employ cross-modal learning and masked prediction  to develop contextualised representations from large unlabelled datasets, which are more readily available than transcribed datasets. After pre-training, a randomly initialised decoder is appended to the encoder, often with an optional CTC layer . The system is then fine-tuned on a smaller set of labelled samples for tasks such as ASR, VSR, and AVSR, usually resulting in different models for each task [13; 15]. However, these methods may fail to leverage unlabelled samples fully since the pretext tasks are not directly aligned with speech recognition. Furthermore, the decoder, trained on limited data during fine-tuning, is highly susceptible to overfitting, necessitating strategies such as freezing encoder layers  or employing variable learning rates across layers to optimise performance [17; 26].

Pseudo-labelling for speech recognition.Pseudo-labelling has been explored in audiovisual speech recognition literature, with methods such as offline pseudo-labelling [9; 27] and frame-wise distillation using frozen teacher models . While these approaches rely on frozen external ASR models trained on large-scale datasets [7; 29], our USR method eliminates this dependency using a randomly initialised teacher model that improves throughout training.

_Iterative_ pseudo-labelling has shown promise for ASR. Some employ multiple rounds of pseudo-labelling using costly beam search and filtering strategies [30; 31; 32; 33], while others continuously and efficiently update pseudo-labels using a CTC-only loss [34; 35]. However, eliminating filtering and attention losses can impact training due to low-quality pseudo-labels, as observed in a recent method  that aims to apply these approaches for ASR, VSR, and AVSR but lags behind the state-of-the-art (see Appendix J). In contrast, USR uses an encoder-decoder architecture to generate CTC and attention pseudo-labels at each iteration through a greedy approach, while pseudo-label quality is maintained via a token-wise filtering mechanism inspired by the semi-supervised FixMatch technique  in image recognition. We note that sharing the same pseudo-labels across auditory, visual, and audiovisual inputs amortises generation costs, leading to efficient CTC-attention training.

Single model for multiple modalities.An earlier study  trained a single recurrent neural network  for ASR, VSR, and AVSR, but noted significant performance differences compared to modality-specific models. Recent works have shown that the Transformer architecture  can handle multiple modalities using the same weights, with minimal performance degradation [11; 12]. In speech recognition, some [13; 14; 15] use the same Transformer encoder for auditory, visual, and audiovisual inputs during pre-training, but then separately fine-tune the parameters for ASR, VSR, and AVSR, resulting in separate models during deployment. u-HuBERT  uses the same weights for all modalities when fine-tuning a pre-trained AV-HuBERT backbone , demonstrating the viability of a unified model. However, it encounters limitations common to other self-supervised approaches, such as proneness to overfitting during supervised fine-tuning. Our proposed semi-supervised approach leverages unlabelled samples during the fine-tuning stage, significantly alleviating these concerns.

## 3 Unified Speech Recognition

Our unified method trains a pre-LN  Transformer  encoder-decoder model for ASR, VSR, and AVSR. Section 3.1 describes the task of unified speech recognition using supervised training, where we have ground-truth annotation for each audio-visual pair. Sections 3.2 and 3.3 then introduce our proposed idea, which employs semi-supervised training and self-supervised pre-training to effectively utilise unlabelled samples. An overview of USR's components is depicted in Figure 1.

### Unified Supervised Training

Inputs.Let \(\{(_{b},_{b},_{b}):b[1,B]\}\) be a batch of \(B\) labelled samples, where \(_{b}\) denotes a \(T_{}\)-frame video of lip movements, \(_{b}\) denotes the corresponding (raw) audio waveform of \(T_{}=640T_{}\) frames1, and \(_{b}\) denotes the label sequence of length \(T_{}\). Following [9; 17], \(_{b}\) and \(_{b}\) are zero-masked with a maximum duration of 0.4 and 0.6 seconds for each second of video and audio, respectively.

Figure 1: **Unified Speech Recognition. Our USR method combines self-supervised pre-training with semi-supervised fine-tuning. For semi-supervised training, pseudo-labels are generated from unmasked audiovisual features using an EMA (exponential moving average)-based teacher. The student, intaking masked inputs, predicts pseudo-labels for unlabelled data and ground-truth labels for labelled data. To obtain the pseudo-labels, an argmax operation is applied to the CTC and attention teacher output probabilities; the tokens with predicted probability below a fixed threshold are discarded. For self-supervised pre-training, a student encoder processes masked visual, auditory, and audiovisual samples and predicts targets, generated by an EMA-based teacher intaking unmasked audiovisual samples, via a shallow predictor. The targets are the average outputs of the teacher blocks. The resulting student weights are used to initialise the student and teacher in semi-supervised fine-tuning. Feature extraction is achieved through modality-specific feature extractors, whose features are concatenated along the channel dimension to produce the audiovisual inputs. The auditory, visual, and audiovisual student inputs are batched together for training efficiency.**

Multi-modal feature extraction.The raw video and audio are fed into ResNet-18  architectures: a 2D version with a 3D stem  for video and a 1D version for audio, sub-sampling the audio to match the video's sampling rate . Linear layers follow the feature extractors to produce the visual and auditory features. The audiovisual features are formed by concatenating the feature extractor outputs along the channel dimension and applying a linear transformation. Finally, the features from the three modalities are concatenated along the batch dimension for efficient processing. We provide the model with all three input types, enabling it to perform well on ASR, VSR, and AVSR.

Losses.The encoder outputs pass through a linear + softmax layer to yield output probabilities \(_{b,m}\) for each modality \(m\{,,\}\). The CTC loss for each modality is given by

\[_{m}=_{b=1}^{B}l_{}(_{b,m}, _{b}),\] (1)

where \(l_{}\) is the standard CTC loss . Further, let \(_{b,m}\) denote the attention probabilities from the outputs of the decoder in _teacher forcing_ mode . The batch attention loss can be expressed as

\[_{m}=_{b=1}^{B}l_{}(_{b,m}, _{b}),\] (2)

where \(l_{}\) is the summed cross-entropy loss for each token. The CTC and attention losses are combined to obtain

\[_{m}=_{}_{m}+(1-_{}) _{m},\] (3)

where \(_{}\) is the relative weight placed on the CTC loss versus the attention loss. We set \(_{}\) to 0.1, following [27; 17; 18]. The overall labelled loss is given by

\[^{}=_{}_{}+(1-_ {})(_{}+_{}),\] (4)

where \(_{}\) controls the weight of the video loss relative to the audio/audiovisual losses. We do not use separate weights for the audio/audiovisual losses due to similar training dynamics observed in preliminary experiments.

### Unified Semi-supervised Training

We introduce a student-teacher pseudo-labelling framework to utilise _unlabelled_ samples alongside labelled examples. The student, equipped with labelled losses, mirrors the model in Section 3.1.

Inputs.In addition to the labelled batch from Section 3.1, we now also have \(B^{}\)_unlabelled_ video and audio samples \(\{(_{b}^{},_{b}^{}):b[1,B^{}]\}\). The student inputs are masked as before.

Pseudo-labels.The teacher, sharing the same architecture as the student, generates pseudo-labels for unlabelled samples. The student is optimised as usual, but no gradients are passed to the teacher. Instead, the teacher's weights \(_{t}\) are updated at each iteration via an exponential moving average (EMA) of the student's weights \(_{s}\): \(_{t}_{t}+(1-)_{s}\), where \(\) increases throughout training from 0.999 to 1 using a cosine scheduler.

For an unmasked audiovisual sample, let \(}_{b}\) and \(}_{b}\) denote the CTC probabilities from the teacher encoder and the attention probabilities from the teacher decoder, respectively. The CTC and attention pseudo-labels are given by \((}_{b})\) and \((}_{b})\), respectively, where \(\) is applied token-wise. Hence, the pseudo-labels correspond to units with the maximum probability across the vocabulary for each input/output time-step. The attention targets are generated auto-regressively by selecting, at each time-step, the most likely unit as the input for the next time-step, without using a costly beam search strategy. Our greedy approach allows for efficient label generation.

Filtering.The teacher may not consistently generate high-quality predictions, especially early in training. We propose a straightforward token-wise filtering mechanism, creating masks \(((}_{b}))\) and \(((}_{b}))\), where the operations are applied token-wise. We thus discard a pseudo-label for a given time-step if its confidence falls below a certain threshold \(\). This mechanism draws inspiration from image recognition literature  and is adapted to sequences.

Unlabelled losses.The unlabelled losses are computed via the cross-entropy between the student predictions and the teacher pseudo-labels. That is, the per-modality CTC losses are given by

\[_{m}^{}=}}_{b=1}^{B^{}}((}_{b})) l_{}(_{b,m}^{ },(}_{b})),\] (5)

where \(\) denotes the Hadamard product and \(_{b,m}^{}\) the student outputs. The attention losses \(_{m}^{}\) are computed similarly. The unlabelled losses \(_{m}^{}\) are obtained as in Eq. 3:

\[_{m}^{}=_{}_{m}^{}+(1- _{})_{m}^{},\] (6)

Final loss.The total semi-supervised loss \(^{}\) combines the per-modality labelled (see Eq. 3) and unlabelled losses (see Eq. 6):

\[^{}=_{}_{}_{ }+_{}(1-_{})(_{}+ _{})+(1-_{})_{}_{ }^{}+(1-_{})(1-_{})(_ {}^{}+_{}^{}),\] (7)

where \(_{}\) and \(_{}\) weigh the contribution of the labelled loss versus the unlabelled loss for audio/audiovisual and visual inputs, respectively. In Section 4.2, we show the benefits of using separate weights for each modality rather than a single weight for both.

### Unified Self-supervised Pre-training

Transformers typically benefit from self-supervised pre-training [45; 13; 17; 15], even with the same data used during fine-tuning [46; 45]. Inspired by recent work [17; 18; 15], we propose a self-supervised method within our framework that can precede semi-supervised fine-tuning.

Inputs.For pre-training, we use only the unlabelled \(B^{u}\) samples from Section 3.2. Following , we mask the student inputs by selecting each video frame index as the start of a three-frame mask with a 0.4 probability, applying a corresponding enlarged mask to the audio in temporal alignment. The elements of the mask \(_{b}\) are set to 0 and 1 for unmasked and masked tokens, respectively.

Targets.The targets are generated by an EMA-based teacher encoder model from unmasked _audiovisual_ inputs, similarly to Section 3.2. Following [15; 18], the targets \(_{b}\) are generated by averaging the outputs from all encoder blocks and applying instance normalisation . Using only audio targets, as in , can make the student's final layers more relevant to speech, which has proven beneficial for fine-tuning with few samples, where there is high chance of overfitting . Our fine-tuning process instead uses abundant unlabelled data with pseudo-labels which help reduce overfitting and allow the network to learn from rich audiovisual targets.

Predictor.Following , we employ a 512-dimensional two-block Transformer predictor that processes student encoder outputs and mask tokens to produce predictions \(_{b,m}\). Unlike the separate predictors for video and audio used in , we use a single predictor for all inputs.

Loss.The loss for modality \(m\) can be expressed as

\[_{m}^{}=-}}_{b=1}^{B^{} }_{b}(_{b,m},_{b}),\] (8)

where \(\) denotes cosine similarity, applied token-wise. Thus, the student aims to predict the teacher targets corresponding to the masked inputs. The self-supervised loss \(^{}\) is then

\[^{}=_{}_{}^{}+(1-_{})(_{}^{}+_{ }^{}).\] (9)

## 4 Main Properties

In this section, we investigate the behaviour of our unified model. For all experiments, we use a 12-block Base model with hidden size of 512 (see Appendix C.4 for model details). We report test set word error rates (WER) for direct comparison with the main results. Note that we used the validation set from  in the exploration stage to avoid overfitting to the test set.

### Unified Supervised Training

In Table 1, we investigate properties of training our unified model from scratch on the full LRS3 dataset  (see Section 3.1). Training details are provided in Section C.5.

Sharing weights.Table 0(a) studies the impact of weight sharing versus separate models per task (ASR, VSR, AVSR). While using only auditory inputs yields strong performance, training VSR and AVSR models from scratch encounters optimisation challenges, in line with prior research . Interestingly, these hurdles are overcome with weight sharing, resulting in robust VSR and AVSR performance without self-supervised pre-training  or training techniques like curriculum learning . This is likely due to audio containing denser verbal information than video, enhancing the optimisation landscape for visual modalities .

Modality sampling.We employ a weighted average to combine the per-modality losses (see Eq. 4). In contrast, other methods  randomly sample, at each iteration, input types with different probabilities, which may vary during training. Table 0(b) shows that our approach performs similarly with random sampling when training the latter for \(3\) more epochs. Our approach offers benefits such as sharing computational costs among feature extractor forward passes and amortising the cost of pseudo-label generation across input types (see Section 3.2), as all modalities use the same targets.

Input type weight.Table 0(c) studies the effect of using different weights for the visual modality. We observe that using a higher \(_{}\) for the VSR loss improves VSR but worsens ASR/AVSR. We choose \(_{}=0.3\) as our default setting, striking a balance in performance among the different tasks.

### Unified Semi-supervised Training

In Table 2, we ablate various components to better understand our unified semi-supervised framework (see Section 3.3). We adopt the common low-resource setting : the 30-hour "trainval" partition of LRS3 serves as our labelled dataset, while the remaining portion of LRS3 (without labels) provides our unlabelled samples. See Appendix C.5 for training details.

Filtering predicted tokens.Figure 2 investigates the impact of the threshold parameter \(\{0,0.8,1\}\). We plot (from left to right) (1) the proportion of tokens exceeding \(\), (2) the validation attention accuracy of the decoder using teacher forcing, and (3) the CTC loss, as a function of the epoch number. We also show the final WER. We observe that \(=1\), where only labelled samples contribute to training, results in poor attention accuracy, high CTC loss, and high WER across input types. Conversely, \(=0\), implying no filtering (_i.e._, all tokens are considered regardless of confidence level), yields competitive performance, suggesting some robustness to low-quality pseudo-labels. Finally, for \(=0.8\), the proportion of tokens with confidence over \(\) begins at a low level and steadily increases throughout training as the teacher network improves. This yields improved performance in terms of attention accuracy, CTC loss, and final WER, demonstrating the efficacy of filtering via a simple confidence threshold. A more fine-grained analysis of \(\) values are given in Section D.1.

Table 1: **Supervised ablations on the full LRS3 dataset using our Base model. Default settings are in gray in all tables of the paper.**Quantity/quality trade-off.Pseudo-labels tend to be abundant but noisy, while ground-truth transcriptions are scarce yet high-quality. The balance between quantity and quality is adjustable via the hyperparameters \(_{}\) and \(_{}\) in Eq. 7. Table 1(a) explores different values for \(_{}\) and \(_{}\), revealing better performance when \(_{}>_{}\). Noisy pseudo-labels generated from audiovisual samples may suffice for VSR, which often performs worse than ASR/AVSR and benefits from data abundance. Conversely, ASR/AVSR is less prone to overfitting and may suffer with excessive reliance on low-quality pseudo-labels, requiring a higher relative weight on labelled losses.

Momentum.Table 1(b) shows the effect of updating the teacher's weight via EMA (\(=0.999\)) compared to simply copying the student's weights at every iteration (\(=0\)). Using EMA results in better performance, yet good results are achieved even without it.

Loss types.CTC and attention-based encoder-decoder frameworks are dominant approaches in speech recognition. While attention typically outperforms CTC, it may struggle with proper alignment prediction, requiring tuning of various decoding hyperparameters . To address these challenges, we adopt a CTC-attention hybrid framework , as in . The costly auto-regressive attention pseudo-label generation is made computationally feasible via our greedy strategy and multi-modal feature extraction (which amortises pseudo-label generation costs). Table 1(c) demonstrates a significant improvement in results by using both CTC and attention compared to CTC alone.

### Unified Self-supervised Pre-training

Table 3 examines the main properties of our self-supervised method (see Section 3.3). We fine-tune pre-trained models with different hyperparameters using our semi-supervised approach (Section 3.2). We use the LRS3 low-resource setting, as in Section 4.2. See Appendix C.6 for training details.

Target modality.In Table 2(a), we evaluate our method with targets derived from the different input modalities. Across all cases, pre-training outperforms training from scratch, highlighting the complementarity of semi- and self-supervised training. Visual targets enhance VSR but diminish ASR/AVSR performance compared to auditory targets; overall, audiovisual targets consistently perform best. These results suggest that cross-modal-only pre-training may lose crucial modality-specific information, reducing generalisation when fine-tuning _on all data_ (including unlabelled samples), _i.e._, via pseudo-labelling. Our observations are in contrast to previous findings with

Table 2: **Semi-supervised ablations** under the LRS3 low-resource setting using our Base model.

Figure 2: **Pseudo-label filtering threshold. Left: Validation plots for different values of threshold \(\). Right: Final WER for different values of \(\).

supervised fine-tuning, where visual or audiovisual pre-training targets tend to underperform [13; 17; 15]. See Appendix F for an in-depth analysis comparing supervised and semi-supervised fine-tuning.

Averaging targets.[15; 18] demonstrate that using the average of encoder blocks as targets outperforms using the last block alone. Table 2(b) confirms this finding in our setting.

Predictor depth.In Table 2(c), we study the influence of predictor depth. A deeper predictor yields more abstract encoder representations, while a shallower one retains more task-specific features . We observe strong performance at our default depth of 2. Notably, our semi-supervised fine-tuning approach is less sensitive to predictor depth than standard methods [17; 18].

## 5 Comparisons with Previous Results

### Comparisons with Self-supervised Methods

Table 4 compares our approach on LRS3  with self-supervised methods under similar model sizes and data settings. We combine pre-training (Section 3.3) with standard fine-tuning (Section 3.1) when using identical pre-training and fine-tuning data, and with semi-supervised fine-tuning (Section 3.2) when using extra unlabelled data. In addition to the low-resource labelled data setting outlined in Section 4.2, we test in a high-resource setting using the full 433-hour LRS3 dataset for fine-tuning. Our pre-training employs either LRS3 alone or combined with a 1,326-hour English-only version of VoxCeleb2 [49; 13]. We experiment with Base, Base+, and Large Transformers (see Appendix C.4).

Low-resource.Using the Base model and LRS3 for pre-training, our approach significantly exceeds the previous state-of-the-art across VSR, ASR, and AVSR, when fine-tuning on 30 hours. Increasing the pre-training data and model size enhances performance, demonstrating our method's scalability. With the Large model and LRS3+Vox2 as pre-training data, we achieve 26.9% WER for VSR and 2.4% WER for both ASR and AVSR, matching BRAVEn on ASR and surpassing it on VSR. Unlike other methods, which use separate models for each task, _USR employs a single model for all tasks_.

High-resource.In the high-resource setting, our results are comparable to modality-specific models for ASR/AVSR and superior for VSR across all settings. Our top model obtains 22.3% WER for VSR, 1.2% WER for ASR, and 1.1% WER for VSR, significantly outperforming u-HuBERT, which also uses a single model for all modalities. Furthermore, USR's low-resource VSR performance is superior to u-HuBERT's high-resource VSR result.

### Comparisons with the State-of-the-Art

Lrs3.In Table 5, we compare our best model against the state-of-the-art on LRS3. We present our USR results with a language model incorporated via shallow fusion [17; 27], improving VSR performance from 22.3% to 21.5%. Despite using a shared model for all tasks, our performance exceeds multiple supervised methods and approaches top results [27; 52; 53], which use significantly more labelled data. USR surpasses Auto-AVSR on VSR (21.5% vs. 23.5%) despite the latter using more total data and external ASR models for transcription. Finally, we outperform self-supervised

Table 3: **Self-supervised ablations under the LRS3 low-resource setting using our Base model.**

    & Pre-train & Shared &  &  \\   & data & params & V & A & AV & V & A & AV \\    \\ AV-HuBERT  & LRS3 & ✗ & 51.8 & 4.9 & 4.7 & 44.0 & 3.0 & 2.8 \\ VATLM  & LRS3 & ✗ & 48.0 & - & 3.6 & - & - & - \\ RAVEn  & LRS3 & ✗ & 47.0 & 4.7 & - & 39.1 & 2.2 & - \\ AV-data2vec  & LRS3 & ✗ & 45.2 & 4.4 & 4.2 & 39.0 & 2.0 & 1.8 \\ Lip2Vec  & LRS3 & ✗ & 49.5 & - & - & 42.0 & - & - \\ BRAVEn  & LRS3 & ✗ & 43.4 & 4.0 & 4.0 & 36.0 & **1.9** & - \\ USR & LRS3 & ✓ & **36.0** & **3.2** & **3.0** & **34.3** & **1.9** & **1.6** \\   \\ AV-HuBERT  & LRS3+Vox2 & ✗ & 46.1 & 4.6 & 4.0 & 34.8 & 2.0 & 1.8 \\ VATLM  & LRS3+Vox2 & ✗ & 42.6 & - & 3.4 & 34.2 & - & 1.7 \\ RAVEn  & LRS3+Vox2 & ✗ & 40.2 & 3.8 & - & 33.1 & 1.9 & - \\ AV-data2vec  & LRS3+Vox2 & ✗ & 37.8 & 3.7 & 3.3 & 32.9 & 1.7 & 1.4 \\ Lip2Vec  & LRS3+Vox2 & ✗ & 40.6 & - & - & 34.1 & - & - \\ BRAVEn  & LRS3+Vox2 & ✗ & 35.1 & 3.0 & - & 28.8 & **1.4** & - \\ USR & LRS3+Vox2 & ✓ & **28.4** & **2.6** & **2.5** & **26.5** & 1.6 & **1.3** \\   \\ AV-HuBERT  & LRS3+Vox2 & ✗ & 32.5 & 2.9 & 3.3 & 28.6 & 1.3 & 1.4 \\ VATLM  & LRS3+Vox2 & ✗ & 31.6 & - & 2.7 & 28.4 & - & 1.2 \\ RAVEn  & LRS3+Vox2 & ✗ & 32.5 & 2.7 & - & 28.2 & 1.4 & - \\ AV-data2vec  & LRS3+Vox2 & ✗ & 30.8 & 2.7 & 2.7 & 28.5 & 1.3 & 1.3 \\ Lip2Vec  & LRS3+Vox2 & ✗ & 31.2 & - & - & 26.0 & - & - \\ BRAVEn  & LRS3+Vox2 & ✗ & 30.8 & **2.3** & - & 26.6 & **1.2** & - \\ u-HuBERT  & LRS3+Vox2 & ✓ & - & - & - & 29.1 & 1.5 & 1.3 \\ USR & LRS3+Vox2 & ✓ & **26.9** & 2.4 & **2.4** & **22.3** & **1.2** & **1.1** \\   

Table 4: **Comparisons with self-supervised methods.** LRS3 results for the low-resource (LR) and high-resource (HR) labelled data settings, with 30 and 433 hours of labelled data, respectively. Best results in **bold**, second-best underlined.

    & Labelled & Unlabelled & Language & Shared &  \\   & hours & hours & model & params & V & A & AV \\  
**Supervised\({}^{*}\)** & & & & & & \\ V2P  & 3,886 & - & ✗ & ✗ & 55.1 & - & - \\ RNN-T  & 31,000 & - & ✗ & ✓ & 33.6 & 4.8 & 4.5 \\ VTP  & 2,676 & - & ✓ & ✗ & 30.7 & - & - \\ Auto-AVSR  & 1,902 & - & ✓ & ✗ & 23.5 & **1.0** & 1.0 \\ Auto-AVSR  & 3,448 & - & ✓ & ✗ & 19.1 & **1.0** & **0.9** \\ ViT3D-CM  & 90,000 & - & ✗ & ✗ & 17.0 & - & 1.6 \\ SynthVSR  & 6,720 & - & ✓ & ✗ & 16.9 & - & - \\ LP Conf  & 100,000 & - & ✗ & ✗ & **12.8** & - & **0.9** \\ 
**Self/semi-supervised** & & & & & & & \\ AV-HuBERT w/ ST  & 433 & 1,326 & ✗ & ✗ & 28.6 & - & - \\ RAVEn w/ ST  & 433 & 1,326 & ✓ & ✗ & 23.1 & 1.4 & - \\ USR & 433 & 1,326 & ✓ & ✓ & **21.5** & **1.2** & **1.1** \\   

Table 5: **Comparisons with the state-of-the-art on LRS3. \({}^{*}\)Labels include automatic transcriptions from ASR models trained on large-scale, often non-public datasets. “ST” desnote offline self-training.**methods [13; 17] using self-training that require a costly beam search strategy combining CTC, attention, and language model scores. Our simpler, greedy approach is effective, and we aim to explore additional offline pseudo-labelling for USR in future work.

Lrs2.We also compare with the state-of-the-art on the LRS2 dataset  (see Table 6). We train our model using the same hyperparameters as for the high-resource LRS3 setting. Consistent with our LRS3 results from Table 5, USR surpasses all other self-supervised methods across ASR, VSR, and AVSR, and outperforms strong supervised methods  trained with \(>\) 4\(\) more labelled data (433 vs. 1,759 hours). Results on the WildVSR dataset are in Appendix E.

## 6 Conclusion

Despite their similarities, research in VSR, ASR, and AVSR has typically focused on developing separate models for each task. In this paper, we propose unified training strategies that use a single model to address all three tasks simultaneously. Our USR approach combines self-supervised learning with a greedy pseudo-labelling semi-supervised technique to achieve state-of-the-art results, surpassing related methods that use separate models for each task. Future work could explore alternative encoder architectures, strategies to improve pseudo-label quality, and methods to incorporate extra audio-only data. We hope to inspire further efforts towards consolidating ASR, VSR, and AVSR systems.