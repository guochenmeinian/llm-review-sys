# Diverse Conventions for Human-AI Collaboration

Bidipta Sarkar

Stanford University

bidiptas@stanford.edu

&Andy Shih

Stanford University

andyshih@cs.stanford.edu

&Dorsa Sadigh

Stanford University

dorsa@cs.stanford.edu

###### Abstract

Conventions are crucial for strong performance in cooperative multi-agent games, because they allow players to coordinate on a shared strategy without explicit communication. Unfortunately, standard multi-agent reinforcement learning techniques, such as self-play, converge to conventions that are arbitrary and non-diverse, leading to poor generalization when interacting with new partners. In this work, we present a technique for generating diverse conventions by (1) maximizing their rewards during self-play, while (2) minimizing their rewards when playing with previously discovered conventions (cross-play), stimulating conventions to be semantically different. To ensure that learned policies act in good faith despite the adversarial optimization of cross-play, we introduce _mixed-play_, where an initial state is randomly generated by sampling self-play and cross-play transitions and the player learns to maximize the self-play reward from this initial state. We analyze the benefits of our technique on various multi-agent collaborative games, including Overcooked, and find that our technique can adapt to the conventions of humans, surpassing human-level performance when paired with real users.1

## 1 Introduction

In multi-agent cooperative games, understanding the intent of other players is crucial for seamless coordination. For example, imagine a game where teams of two players work together to cook meals for customers. Players have to manage the ingredients, use the stove, and deliver meals. As the team works together, they decide how tasks should be allocated among themselves so resources are used effectively. For example, player 1 could notice that player 2 tends to stay near the stove, so they instead spend more time preparing ingredients and delivering food, allowing player 2 to continue working at the stove. Through these interactions, the team creates a "convention" in the environment, which is an arbitrary solution to a recurring coordination problem . In the context of reinforcement learning, we can represent a specific convention as the joint policy function that each agent uses to choose their action given their own observation. If we wish to train an AI agent to work well with humans, it also needs to be aware of common conventions and be flexible to changing its strategy to adapt to human behavior .

One approach to training an AI agent in a cooperative setting, called self-play , mimics the behavior of forming conventions by training a team of agents together to increase their reward in the environment, causing each player to adapt to the team's behavior. Eventually, self-play converges to an equilibrium, where each player's policy maximizes the score if their team does not change.

Unfortunately, self-play results in incredibly brittle policies that cannot work well with humans who may have different conventions , including conventions that are more intuitive for people to use. In the cooking task, self-play could converge to a convention that expects player 2 to bring onions to AI player 1 from the _right_ side. If a human player instead decides to bring onions from the _left_ side to the AI player 1, the AI's policy may not react appropriately since this is not an interactionthat the agent has experienced under self-play. One way to address this issue is to train a _diverse_ set of conventions that is representative of the space of all possible conventions. With a diverse set of conventions, we can train a single agent that will work well with arbitrary conventions or adapt to new conventions when working with humans.

To generate a diverse set of policies, prior works have used statistical divergence techniques that measure diversity as the difference in action distributions at each state between policies . They assert that policies that result in different actions at each state will lead to players taking different paths in the environment, hence increasing diversity. However, it is often easy to trick this proxy objective for diversity by taking _different actions_ that still lead to the same outcome. For example, in Fig. 1, we see two conventions that divide the task in the same way but differ in navigation. Even though this difference does not interfere with the partner and the conventions are perfectly compatible with one another, statistical divergence metrics treat these two policies as entirely different. Therefore, statistical diversity techniques do not fundamentally capture whether the conventions follow genuinely different strategies.

We instead use the incompatibility of different conventions as a measure of diversity . Suppose we already have a set of conventions and we want to add a new convention that is different from all the conventions we have already found. We can measure the reward we would expect to receive if a player from this new convention played with another player from an existing convention, which we refer to as the cross-play reward. If this convention has a high cross-play, adding this convention would be redundant, because a policy from one of the existing conventions would already work with this new convention. On the other hand, a convention that has low cross-play with our set of conventions has a different strategy for the game and should be added to the set, because a human player might use that strategy.

Although directly minimizing cross-play rewards takes us closer to our goal of learning diverse conventions, it can result in poorly behaved conventions due to the adversarial nature of this optimization. In particular, a failure mode we refer to as _handshakes_ occurs when policies intentionally sabotage the game upon realizing that the task rewards are inverted. For example, one convention could decide that players should always pick up and place down a plate at the start of the game, which is considered a handshake to establish one's identity without significantly decreasing its self-play reward. If a player 1 from this convention plays with a partner that does not execute the handshake, this player 1 can intentionally sabotage the game and achieve a low cross-play score even if the fundamental strategies are similar. To fix this issue, we introduce mixed-play, a setting where players start with a "mixed-state generation" phase in which they sample actions from self-play or cross-play, but transition into pure self-play at a random timestep with no notice. Since players receive a positive reward during self-play, handshakes cannot reliably indicate whether they should maximize their reward (self-play) or minimize it (cross-play), resulting in conventions that act in good faith.

We propose the CoMeDi algorithm, which stands for "Cross-play optimized, **M**ixed-play **e**nforced **D**iversity." CoMeDi generates a sequence of conventions that minimize the cross-play with the most compatible prior conventions in the sequence while using mixed-play to regularize the conventions. We evaluate our method on three environments: Blind Bandits, Balance Beam, and Overcooked . We see that CoMeDi is able to generate diverse conventions in settings where statistical diversity and naive cross-play fail. Finally, we evaluate our generated pool of conventions in a user study by training an agent that is aware of the conventions in the pool and testing it with human users in Overcooked. We find that CoMeDi significantly outperforms the pure self-play and statistical diversity baselines in terms of score and user opinions, surpassing human-level performance.

Figure 1: Example of conventions in Overcooked. Each team needs to (1) pick up onions, (2) drop them into pots, (3) fill up a dish, and (4) drop it off at the counter. There are \(2\) conventions shown above, a red convention and an orange convention, which only differ in terms of navigation, so they are compatible with one another.

## 2 Preliminaries

We model cooperative games as a multiagent Markov decision process . The MDP, \(\), is the tuple \((,,,r,,,T)\), where \(\) is the (joint) state space and \(=A^{1} A^{2}\) is the joint action space. Although we only analyze 2-player games in the main text, we detail an extension to larger teams in AppendixA.5. The transition function, \(:\), is the probability of reaching a state given a current state and joint action. The reward function, \(r:\), gives a real value reward for each state transition. The observation function, \(: O^{1} O^{2}\), generates the player-specific observations from the state. Finally, \(\) is the reward discount factor and \(T\) is the horizon.

Player \(i\) follows some policy \(^{i}(a^{i} o^{i})\). At time \(t\), the MDP is at state \(s_{t}\), so the agents receive the observations \((o^{1}_{t},o^{2}_{t})=(s_{t})\) and sample an action from their policy, \(a^{i}_{t}^{i}(a^{i}_{t} o^{i})\). The environment generates the next time step as \(s_{t+1}(s_{t},a_{t},s_{t+1})\) and the shared reward as \(r(s_{t},a_{t})\). The trajectory is defined as the sequence of states and actions in the environment, which is \(=(s_{0},a_{0}, s_{T-1},a_{T-1},s_{T})\). The discounted return for a trajectory is \(R()=_{t=0}^{T-1}^{t}r(s_{t},a_{t})\). The expected return for a pair of agent-specific policies is \(J(^{1},^{2})=_{(^{1},^{2})}[R()]\). For simplicity of exposition, we assume that the agents in the environment are not ordered, so \(J(^{1},^{2})=J(^{2},^{1})\) for all \(^{1},^{2}\), although our method directly handles situations where order matters.

We refer to an instance of the joint policy of the agents \(=^{1}^{2}\) as a **convention**, following the definition of a convention as an arbitrary solution to a recurring coordination problem in literature [20; 3]. When the joint policy forms a Nash equilibrium, we refer to it as an **equilibrium convention**, based on some some stricter definitions of conventions in literature [13; 19]. In equilibrium conventions, neither player can get a higher expected score with the current partner by following a different policy.

### Problem Definition

In this paper, we study the problem of generating a diverse set \(D\) of conventions that are representative of the set of potential conventions in the environment at test time, \(D_{}\). To determine how well \(D\) generalizes to the test set, we can calculate a "nearest neighbor" score by finding which convention in \(D\) generates the highest reward for each convention in the test set, which corresponds to \(S\) in Eq.1:

\[S(D)=}_{ D_{}}[_{^{*} D}J(^{ *},)].\] (1)

In practice, we can also treat our generated set \(D\) as the set of _training_ policies used to learn a generalizable agent  for zero-shot coordination. We rely on off-the-shelf multitask training routines  for learning the generalizable agent, which have shown to be effective in learning an agent that can coordinate with new _test_ partners [27; 21]. In particular, given the set \(D\) of diverse conventions, we can train a convention-aware agent using behavior-cloning [22; 17] in a similar fashion to QDagger :

\[(,D)=-(,)-_{ D}}_{(o,a)}[((a|o))],\] (2)

where \((o,a)\) represents the distribution of observation-action pairs sampled from self-play rollouts of \(\), and \(\) is a tunable hyperparameter for the weight of the BC component. At evaluation time, we pair the convention-aware agent \(\) with a new _test_ partner and measure the task reward:

\[(,D_{})=}_{ D_{}}[(,)].\] (3)

For both Eq.1 and Eq.3, the test agent distribution \(D_{}\) may be too expensive to sample from during the training loop, e.g., when this test dataset is based on samples from human users. Instead, we would like to generate a high quality set of _training_ agents that is representative of the space of \(D_{}\). Intuitively, a more diverse set \(D\) will improve the generalization of the convention-aware \(\) trained on the multi-task objective in Eq.2.

A common recipe for generating a diverse set \(D\) is to train individual agents via standard self-play, and propose a diversity regularization loss that encourages the set \(D\) to be more "diverse" with respect to the regularization term:

\[(D)=-_{ D}[(,)]+\:(D).\] (4)In Eq.4, the overall loss of the set \(D\) is a combination of the diversity regularization \(\) (scaled by \(\)) and the self-play rewards of the individual agents in the set \(D\). The choice of regularization term \(\) requires a difficult balance between tractable computation, alignment to the downstream objective (Eq.1, Eq.3), and ease of optimization of Eq.4. In Section2.2 and Section2.4, we highlight some existing diversity metrics and their limitations.

### Statistical Diversity

The majority of prior works on diversity in deep reinforcement learning are based on the principle of maximizing statistical divergence [11; 10; 21; 9; 15]. In the realm of zero-shot coordination, Trajectory Diversity (TrajeDi)  uses the Jensen-Shannon divergence between the trajectories induced by policies as a diversity metric. The method of learning adaptable agent policies (ADAP)  maximizes the KL-divergence between the action distributions of two policies over all states. We compare against ADAP as a baseline by adding its diversity term in Eq.5 to the MAPPO algorithm :

\[_{}(D)=*{}_{s S}[ *{}_{_{1},_{2} D\\ _{1}_{2}}(-D_{}(_{1}(s)||_{2} (s))].\] (5)

While statistical divergence techniques are computationally simple, they do not use the task reward and thus can be poorly aligned with the downstream objectives in Eq.1 and Eq.3. For example, consider the introductory Overcooked example in Fig.1. There are many possible routes that the players can take to complete their individual task, leading to distinct but semantically similar trajectories. Ideally, we want a diversity regularizer to penalize trivial variations that are irrelevant to the task at hand. Unfortunately, statistical diversity would fail to recognize these semantic similarities.

### Cross-Play Minimization Diversity

Recent work on generating strong zero-shot agents in cooperative games have also used the idea of generating a diverse set of conventions by minimizing cross-play. The LIPO algorithm  follows a similar approach to our baseline cross-play minimization algorithm described in Section3.1, but it does not fundamentally tackle the issue of handshakes described in Section3.2. The ADVERSITY algorithm  also follows an approach of minimizing cross-play scores, but addresses the issue of handshakes in Hanabi through a belief reinterpretation process using the fact that it is a partially observable game. Unfortunately, the ADVERSITY algorithm would not function in our fully observable settings since there is no hidden state for belief reinterpretation. We provide a more detailed comparison of our approach to these prior works in AppendixC.

### Other Approaches to Diversity

Other prior works attempt to modify the training domain to induce a variety of conventions [32; 35; 23]. A common aspect to modify is the reward, which can be randomized  or shaped by having an external agent try to motivate a certain behavior . This requires extra effort from an environment designer to specify the space of the reward function, which may be impractical in complex environments. Additional approaches using population-based training modify pre-specified aspects of the environment to generate a set of agents , or generate agents using different random seeds but require a human to manually select diverse agents . Another set of work focuses on the easier setting of using imitation learning to learn to play with humans [18; 30; 28], but this assumes access to potentially expensive human behavioral data.

We do not explicitly compare CoMeDi to these other approaches in the main text because they assume access to the internal mechanisms of the environment or require external data, so they are not drop-in replacements for self-play, unlike CoMeDi or statistical diversity techniques.

## 3 Method

Given the drawbacks of current statistical diversity methods at penalizing semantically similar trajectories, we seek alternative diversity regularization terms that can recognize semantically diverse behavior from trivial variations in trajectory space. A good mechanism for identifying trivial variationsis through task reward: if swapping the behavior of two agents does not affect the attained task reward, then the differences between their behavior are likely irrelevant to the task at hand. For example, consider the effect of the blue player following the red trajectory while the green player follows the orange trajectory in Fig. 1. The swapped teams will still attain the same high reward as the players following the same color trajectories, suggesting that these behaviors are not semantically different.

With this insight, we look to measure cross-play, the task reward attained when pairing together players that were trained separately. We propose a diversity regularization based on minimizing cross-play reward, and describe its benefits compared to statistical diversity. However, pure cross-play faces an adversarial optimization challenge that we refer to as _handshakes_, making it difficult to optimize. In Section 3.3, we describe mixed-play, our novel technique for mitigating handshakes in cross-play while still retaining the benefits of reward-aligned diversity metrics.

### Cross-Play Minimization

The idea of cross-play is simple: pair two agents from different conventions together and evaluate their reward on the task. If those two conventions are semantically different, then the cross-play reward should be low. On the other hand, if the conventions of the two agents are similar, then their cross-play reward should be high, and it should be penalized by our diversity regularization term.

We can also derive the notion of cross play minimization by trying to _maximize_ the nearest neighbors score \(S(D)\) from Eq. (1). Suppose we already have found \(n-1\) conventions in the set \(D_{n-1}\), and we wish to add a new convention \(_{n}\) to construct the new set \(D_{n}\). We can evaluate a lower bound for the improvement of \(S\) as

\[S(D_{n}) S(D_{n-1})+p(_{n})(J(_{n},_{n})-_{^{} D _{n-1}}J(_{n},^{*})),\] (6)

where \(p(_{n})\) is the probability of choosing policy \(_{n}\) at test-time. We explore the algorithmic implications of \(p(_{n})\) in Section 3.2.

Using the result above, we extend cross-play minimization to a set of \(n\) agents by using the cross-play reward between each convention and the most compatible existing convention as the regularization term \(_{}\) in Eq. (7). To determine which existing convention is the most compatible, we can simply collect cross-play buffers with all existing conventions and choose the convention with the highest expected return. Note that \(_{X}\) is minimized, so rewards are _inverted_ under cross-play:

\[_{}(D)=_{i=2}^{n}_{^{*} D_{i-1}}J(_{i}, ^{*}).\] (7)

If we integrate Eq. (7) into the full loss function from Eq. (4), the cross-play loss is simply equal to the sum of the lower-bound expressions in Eq. (6) for all conventions in \(D\) when \(p(_{i})\) is ignored.

### Handshakes: Challenges of Optimizing Cross-Play

Recall that the downstream objective is to maximize the nearest neighbors score from Eq. (1), meaning that we want policies that work well with the unseen test distribution. Unfortunately, directly minimizing cross-play can lead to undesirable policies for the downstream objective. In particular, policies optimized with pure cross-play often take illogical actions that would actively harm the task reward when they are in a cross-play setting while still performing well in self-play.

This sabotaging behavior is also observed in concurrent work in Hanabi . They specifically analyze cases where players intentionally use unplayable cards to quickly end the game. Their "self-play worst response" policy, analogous to our pure cross-play minimization strategy for \(_{2}\), intentionally sabotages the game under cross-play, resulting in a brittle policy that identifies when it is under cross-play. To resolve this issue in Hanabi, ADVERSITY uses belief reinterpretation, similar to Off-Belief Learning , in order to construct a fictitious trajectory consistent with the observed trajectory. This prevents sabotaging behavior because observations are equally likely to come from self-play or cross-play. Although this technique is very effective in partially observable environments, like Hanabi, this unfortunately does not solve the issue in our fully observable settings because there is no hidden state that can be reinterpreted to construct plausible self-play trajectories given a cross-play trajectory.

We hypothesize that the challenge with the optimization of cross-play regularization arises when agents can easily discriminate between cross-play and self-play, as illustrated in Fig. 2. For example, consider the earlier Overcooked example in Fig. 1, where we now pair the red convention for the blue player and orange convention for the green player during cross-play. As we repeatedly update the policies based on minimizing cross-play, the agents will learn to recognize when they are being evaluated under cross-play and deliberately sabotage the game. The blue player can learn to develop a handshake, like shown in Fig. 2, and sabotage the game based on the green player's reaction. Note that the strategy after the _handshake_ may be similar across both conventions, but they will still have a low cross-play due to intentional sabotage. As a result, these agents have fooled our cross-play diversity metric into treating their behaviors as semantically different.

More generally, this problem occurs when there is a mismatch in observation distributions (or history distribution for recurrent policies) between self-play and cross-play, which allows the agents to infer the identity of their partner and the inversion of rewards. In fact, a mismatch can be identified as early as the second timestep: in the first timestep, both agents perform a _handshake_ - an action intended to reveal the identity of the players. If the handshakes match, then the two agents cooperate to solve the task. Otherwise, they sabotage the game, e.g., by doing nothing for the rest of the episode.

Since a strong self-play policy is under-specified at many states, all of the mechanisms of handshake signalling and sabotaging can be encoded into a policy with almost no reduction in self-play performance. We would not expect this type of behavior from a convention in the test set, so this policy, \(\), would have a very low \(p()\) in Eq. (6) relative to other trained policies. The outcome of a cross-play minimization procedure (Eq. (7)) plagued by handshakes is a set of agents who have high individual self-play, low pairwise cross-play, yet may still be using semantically similar conventions.

### Mixed-Play

To mitigate the issues of handshakes, we propose _mixed-play_, which improves cross-play regularization by expanding the valid distribution of "self-play states" and reducing under-specification. Mixed-play rolls out episodes using varying combinations of self-play and cross-play to enlarge state coverage. All the states in the coverage will be associated with the positive rewards of self-play, so agents cannot infer reward inversion and sabotage the game. For example, if the cross-play situation from Fig. 2 becomes a valid state that self-play can continue from, the blue agent can no longer confidently sabotage the game when it encounters this state, since this harms the self-play score as well.

Mixed-play consists of two phases in each episode: mixed-state generation and self-play. We choose a random timestep within the episode that represents the length of the first phase. Until this timestep occurs, we randomly sample the action from self-play or cross-play for both players, but do not store these transitions in the buffer. For the rest of the episode, we perform the second phase: taking self-play actions and storing them in the buffer. When optimizing, we treat this new buffer the same as we would treat self-play, but modified with a positive weight hyperparameter, \(\), representing the importance of mixed-play. Unlike ADVERSITY, our technique does not assume that the environment is partially observable and does not require privileged access to the simulator, making it applicable to our fully observable settings.

Figure 3: Visualization of how mixed-play resolves handshakes. Agents must act in good faith since they might switch to self-play.

Figure 2: Example of a handshake in Overcooked. The blue player initiates a handshake by placing a plate in a strange location. Under self-play, the green player will pick up the plate, but otherwise it will continue picking up onions. If the blue player sees that the plate has not moved, it will sabotage the game.

After training with mixed-play, the agent cannot determine the current identity of the partner. In particular, even if the partner fails the handshake in an earlier timestep, the setting might be mixed-play instead of cross-play, as visualized in Fig. 3. If \(\) is large enough, the agent will learn that handshakes significantly harm the scores in the second phase of mixed-play and will learn to act in good faith at all timesteps. In the following section, we present our full algorithm that incorporates mixed-play into our technique for generating diverse conventions.

### CoMeDi: Self-play, Cross-play, and Mixed-play

So far, we have described how cross-play is useful for constructing a reward-aligned diversity metric, while mixed-play ensures that trained agents always act in good faith. In the full training regime, which we name CoMeDi, we (1) maximize self-play rewards to ensure that agents perform well in the environment, (2) minimize cross-play rewards with previously discovered conventions to ensure diversity, and (3) maximize rewards from the collected mixed-play buffers to ensure that conventions act in good faith. Note that we collect separate buffers for self-play, cross-play, and mixed-play to estimate their respective expected returns.

To train convention \(_{n}\), the full loss function we evaluate is

\[(_{n})=-(_{n},_{n})+(_{n}, ^{*})-_{}(_{n},^{*}),\] (8)

where \(_{}\) represents the expected mixed-play reward, and \(^{*}\) is the most compatible convention to \(^{n}\) found in \(D_{n-1}\). We also have tunable parameters \(\) and \(\) representing the weight of the cross-play and mixed-play terms. Pseudocode and specific implementation details for the algorithm incorporating this loss, including the mixed-play buffer generation, is presented in Appendix A.

## 4 Experiments

We now describe our experiments and results using three different environments: a Blind Bandits environment, Balance Beam, and Overcooked [5; 34; 26]. These three environments will help illustrate the differences between statistical diversity, cross-play diversity, and mixed-play diversity. In particular, the Blind Bandits highlight the importance of reward-alignment that is present in cross-play but not statistical diversity. The Balance Beam environment demonstrates the pitfall of handshakes during cross-play minimization, and how this is mitigated with mixed-play. Finally, the Overcooked environments demonstrate the scalability of CoMeDi, and allow us to evaluate the generated set of diverse agents by training convention-aware policies to play with real human partners. More detailed descriptions of all experiments are in the appendix.

### Blind Bandits

In the Blind Bandits environment, we extend the classic multi-armed bandit problem into a two-player game. The main twist is that each agent cannot observe the actions of their partner during the game, so they cannot change their strategy in reaction to their partner. It is a collaborative 2-player game

Figure 4: The Blind Bandits environment with \(k=2\) steps. The solid lines indicate the first player’s actions while the dashed lines indicate the second player’s actions. The blue line represents the conventions that converge to the \(S\) reward while the orange line represents the convention that converge to the \(G\) reward.

Figure 5: Frequency of self-play scores during the optimization of the two conventions using CoMeDi. The left plot is the first convention while the right plot is the second convention. We choose \(k=3\) as the steps in the environment, and the shaded region indicates the frequency bounds from 10 independent initializations.

where each player takes \(k\) steps, \(k 2\), and at each step they have to choose to go left (L) or right (R). The possible trajectories when \(k=2\) are represented in Fig. 4.

An important characteristic of the Blind Bandits environment is that all conventions that converge to \(S\) are compatible with one another while being incompatible with the only convention that converges to \(G\). Therefore, if we want a diverse set of 2 conventions, we would like one convention to converge to \(S\) and another to converge to \(G\).

Baseline Results for Blind Bandits.Existing statistical divergence techniques, like ADAP, typically converge to \(S\) conventions because it affords a larger space of policies. Even when trying to train more conventions, they could all converge to \(S\) conventions while still satisfying statistical diversity, since there are multiple possible distinct policies associated with \(S\). Unfortunately, this gives a set of agents that are diverse with respect to trajectory metrics, but not with respect to the task, since the statistical diversity policies cannot lead to an agent compatible with the \(G\) convention.

CoMeDi Results for Blind Bandits.By minimizing cross-play, we can reliably train a set of agents that converge to both the \(S\) and \(G\) conventions. First, we train a single agent via self-play to converge to one of the \(S\) conventions. Then, under the cross-play regularization (with \(=1.0\)), the reward payoff for a second agent is a transformed environment where each self-play reward is subtracted by the cross-play reward with the first agent. All policies with a self-play score of \(S\) would have a score of \(0\) in the transformed environment, but all policies with a self-play score of \(G\) would still have a score of \(G\) in the transformed environment, so an agent trained on this transformed environment would converge to \(G\). Indeed, as we see in right plot of Fig. 5, the second convention consistently converges to \(G\).

### Balance Beam

The Balance Beam environment is a collaborative environment with two players on a line that has 5 locations with unit spacing, illustrated in Fig. 6. Both agents move simultaneously and they get a high reward if they land on the same space after their actions, but they must move from their current location at each timestep. As a human-like test set of policies, we hand-design a left-biased agent and a right-biased agent. Fig. 6 shows a visualization of the environment.

As opposed to Blind Bandits, the Balance Beam environment involves multiple timesteps and is prone to handshake formation. As we will show in the next section, mixed-play allows us to combat the handshake formation of cross-play diversity in the Balance Beam environment.

Balance Beam Results.We train the first convention with base MAPPO, which converges to a self-play score of 2.0. The expected cross-play of this first convention with the hand-coded left-biased

   \(\) & SP \(\) & XP \(\) & HS \(\) & PX \(\) & LS \(\) & RS \(\) \\ 
0.00 & 1.616 & **-0.392** & 0.16 & **0.00** & 1.128 & **-0.656** \\
0.25 & 1.808 & 0.096 & **0.00** & **0.00** & 1.272 & 0.096 \\
0.50 & **2.000** & 0.200 & **0.00** & **0.00** & **1.384** & 0.128 \\
1.00 & 1.904 & 0.632 & **0.00** & 0.24 & 1.232 & 0.160 \\   

Table 1: Results of Mixed-Play in the Balance Beam Environment: SP is the self-play score. XP is the cross-play score with the first convention. HS (handshake) is the fraction of cross-play games where a player moves off the line, while PX (perfect XP) is the fraction of with a perfect 2.0 score. LS and RS are the scores under cross-play with the left-biased and right-biased hand-coded agents.

Figure 6: Example of the Balance Beam environment. The agent with a white hat follows the left-biased policy. The blue agent has four different actions it can choose. If an agent steps off the line, the game ends with a large negative score.

agent is 0.384 while the score is 1.368 with the right-biased agent, so it acts very similar to the right-biased agent.

Ideally, we would want a second convention to have a high cross-play score with the left-biased agent and a low cross-play score with the right-biased agent so it would ensure that human players from either convention could find a compatible AI partner. Its self-play score should be 2.0, indicating an optimal equilibrium convention. Also, it should never take actions that are clearly incorrect, like stepping off of the line, which would result in a large negative score along with the early termination of the episode.

Although pure cross-play minimization (\(=0\)) results in the lowest cross-play scores (as shown in Table 1), we notice signs of handshakes: 16% of the cross-play games result in the action of stepping off of the line, as indicated in the HS column. Moreover, pure cross-play minimization does not give perfect self-play scores, indicating that it takes suboptimal actions in some states to establish a handshake. When the mixed-play weight is set to a low value (\(=0.25\)), the agent no longer steps off the line when the convention is breached, but it still results in an imperfect convention since it takes suboptimal actions to minimize cross-play. If the mixed-play weight is set to a high amount (\(=1.0\)), the agent follows an identical trajectory to the first convention in 24% of the games, indicating that it is overcorrecting the issue of handshakes. The best convention we found had \(=0.5\) as shown in Table 1, which was able to generate a perfect convention that does not exhibit signs of handshakes. This convention also has the highest cross-play with the left-biased agent, indicating that mixed-play enables us to generate an agent that is more aligned to human notions of "natural" conventions since this agent always acts in good faith instead of conducting handshakes.

From the results above, we empirically demonstrate that mixed-play addresses the issue of handshakes, generating a pair of meaningfully diverse agents.

### Overcooked

Overcooked is a fully cooperative game where two chefs controlled by separate players try to cook and deliver as many dishes of food as possible in a given amount of time [5; 26]. The main challenge is to coordinate by subdividing tasks and not interfering with each other's progress.

For our experiments, we generate 2 baseline agents and 2 convention-aware agents using CoMeDi. The baselines are "SP" and "ADAP" which refer to the base MAPPO algorithm and training a convention-aware agent on ADAP. Our agents are "XP" and "CoMeDi" which refer to the convention-aware agents on CoMeDi where \(=0.0\) (pure cross-play minimization) and \(=1.0\) respectively.

Overcooked User Study.In both the Blind Bandits and Balance Beam environments, we could construct an artificial test set by designing human-like conventions. However, this is infeasible for Overcooked because humans can exhibit unpredictable dynamics, such as adapting to partners. Therefore, we test the quality of our four agents by experimenting on a population of 25 human users. The results of the user study in the Coordination Ring environment are in Fig. 7. In this environment, players need to coordinate on a strategy so they do not bump into one another. A full description of the environment, simulation results, and additional user-study results in the Cramped Room layout are in the appendix. Videos of users interacting with each agent can be found on our website.

Figure 7: Results for the Coordination Ring: average scores (top left), environment visualization (top right), and user survey feedback using the 7-point Likert scale (bottom). Higher average scores are better. The dashed line is the average score single-player human score while the solid line is the average score when paired with an expert human. Higher is better for all but the last feedback score. Error bars represent the standard error.

In terms of scores, we see that CoMeDi (score of 3.68) performs best, followed by XP (2.48), ADAP (2.24), and SP (2.08). The average score when playing with an expert human partner is 3.25, so only CoMeDi surpasses human-level performance (\(p=0.045\)). In terms of statistical significance, CoMeDi outperforms all baselines with \(p<0.001\). CoMeDi also was strongest across all metrics in the survey. Users wrote that CoMeDi "knew exactly what to do next," with one user even claiming that it "is GODLIKE" and "adapted to my moves like he can see into the future." Sentiments were mixed for XP with some users reporting that it was "doing great decisions, but sometimes he seemed confused." With ADAP and SP, users complained that the bots would "block" their progress often and "just stood around the onions and did nothing."

Diversity of the Human Test Set.Although users tended to favor CoMeDi across the metrics we measured, we wanted to determine why CoMeDi performed well. Specifically, we wanted to determine whether humans actually followed a diverse set of conventions and whether CoMeDi's strength comes from finding a specific human-like convention or covering a larger observation space.

To answer these questions, we conducted an experiment where we explicitly predict the probability of being in a CoMeDi convention conditioned on the observation. Since our usage of QDagger can be approximated as a gated mixture-of-experts model, we can explicitly train a "gate" network as a classifier that determines which convention an observation belongs to. Out of the 5000 observations that CoMeDi encountered when working with users in Coordination Ring, 2349 of them were aligned with a specific convention with probability greater than 50%, which we refer to as "convention-specific" observations. The frequency of most likely conventions is reported in Table 2.

Although convention 1, which is equivalent to the pure self-play policy, behaves well in 50% of the convention-specific observations, knowledge of other conventions is still vital. Furthermore, not all players aligned with convention 1. For instance, one player had 65% of their convention-specific observations align with convention 8 while only 27% aligned with convention 1. When calculating which convention each player followed a plurality of the time, 20 players followed convention 1, three followed convention 8, and two followed convention 3. No user perfectly followed a single convention, indicating that understanding multiple conventions made CoMeDi more robust overall.

These results demonstrate that there is non-trivial diversity in the human distribution of conventions and that CoMeDi's strengths come from discovering multiple human-like conventions along with having a more robust policy to handle deviations from these conventions.

## 5 Conclusion

In this work, we introduce CoMeDi, an algorithm for generating diverse agents that combines the idea of minimizing cross-play with existing agents to generate novel conventions, and the technique of mixed-play to mitigate optimization challenges with cross-play. Compared to prior work that defines the difference between conventions as a difference in generated trajectory distributions, our work uses cross-play as a measure of incompatibility between conventions. As seen in the Blind Bandits environment, cross-play gives more information than statistical diversity techniques since it can identify whether conventions are redundant. In the Balance Beam environment, we show how mixed-play addresses the issue of handshakes by incentivizing agents to act in good faith at each state in the environment. Finally, we analyze Overcooked and see how a convention-aware agent trained with CoMeDi outperforms prior techniques with users, achieving expert human-level performance.

Although we use a simple BC-based algorithm for training a convention-aware agent, future work could utilize the diverse set of conventions generated by CoMeDi more effectively to create an agent that uses the history of past interactions to dynamically adapt to a user's convention.

  
**1** & **2** & **3** & **4** & **5** & **6** & **7** & **8** \\ 
0.50 & 0.00 & 0.25 & 0.00 & 0.00 & 0.05 & 0.04 & 0.15 \\   

Table 2: Frequency of each convention being the most likely in the encountered “convention-dependent” observations in Coordination Ring.

Acknowledgements

This research was supported in part by AFOSR YIP, DARPA YIP, NSF Awards #2006388 and #2125511, and JP Morgan. We would also like to thank Jakob N. Foerster for his useful discussion with us regarding handshakes and Hengyuan Hu for clarifying the differences between CoMeDi and ADVERSITY.