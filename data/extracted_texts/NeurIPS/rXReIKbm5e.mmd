# LLM Improvement for Jailbreak Defense: Analysis Through the Lens of Over-Refusal

Swetasudha Panda, Naveen Jafer Nizar and Michael Wick

Oracle Labs, Burlington, MA

{swetasudha.panda, naveen.jafer, michael.wick}@oracle.com

###### Abstract

We propose self and external improvement in Large Language Models (LLMs) as training-free defense mechanisms against jailbreaks and compare performance with existing defenses. Current evaluation strategies are inadequate for comparing various defense methodologies since they predominantly focus on the safety goal of decreasing Attack Success Rate (ASR). Consequently, evaluations fail to capture _over-refusal_ -- wherein LLMs inappropriately reject benign prompts, leading to compromised utility and user dissatisfaction. To address this gap, we also introduce a comprehensive evaluation framework to facilitate better comparison across defense methodologies, analogous to comparing binary classifiers. Our experimental results on state-of-the-art jailbreaks on Llama-2 models show that LLM self-improvement can significantly reduce ASR (e.g., from 46% to 0% on GCG attacks) while minimizing degradation in general instruction-following performance and over-refusal. Furthermore, we identify alarmingly high over-refusal (as high as 100%) in current defense approaches, underscoring the need for future research into more effective and practical jailbreak defense solutions.

## 1 Introduction

Large Language Models (LLMs) pre-trained on diverse text corpora excel in a variety of natural language processing tasks Touvron et al. (2023); Achiam et al. (2023). However, LLMs often exhibit unintended behaviors, such as hallucinations and generating biased, toxic, or otherwise objectionable content Bender et al. (2021); Bommasani et al. (2021). To address these issues, base LLMs typically undergo extensive supervised fine-tuning and/or reinforcement learning with human feedback (RLHF) to align the models with user preferences, aiming to develop helpful, honest, and harmless AI assistants Ouyang et al. (2022); Bai et al. (2022). Despite extensive efforts on alignment-tuning, a growing body of literature demonstrates that adversarial prompts, often referred to as _jailbreaks_ Chao et al. (2023); Zou et al. (2023) can circumvent alignment mechanisms. Moreover, simply fine-tuning LLMs on conventional NLP tasks, experimenting with different decoding strategies, or engaging in in-context learning have all been shown to significantly degrade alignment Qi et al. (2023); Huang et al. (2023); Wei et al. (2023), demonstrating that alignment-tuning suffers from lack of generalization.

Contemporary research on tuning-free alignment employ in-context learning Lin et al. (2023) or decode-time optimization Huang et al. (2024) and demonstrate advantages of enforcing alignment objectives at inference. Moreover, Kim et al. (2024) advocate for post-hoc strategies such as output post-processing as efficient defense compared to input pre-processing. Motivated by these findings, we propose self-improvement in LLMs as an alignment mechanism against jailbreaks at inference time, eliminating the necessity for extensive model training.

While prior work Madan et al. (2024); Pan et al. (2023) demonstrate the effectiveness of self-improvement on a variety of tasks, an understanding of its underlying mechanics is still underexplored.

On one hand, research shows limitations of self-improvement on reasoning and planning tasks Huang et al. (2023a); Kambhampati (2024). On the other hand, prior studies Ganguli et al. (2023) document evidence supporting the use of self-improvement to reduce bias and stereotypes in outputs generated by LLMs. We hypothesize that the task of improving LLM responses to jailbreaks aligns with Ganguli et al. (2023). Findings in concurrent research by Wang et al. (2024) on theoretical investigation of self-improvement for alignment, corroborate our hypothesis.

In particular, we study a) _self-improvement_ where the model reassesses and improves its generations using its inherent knowledge, and b) _external improvement_ using a second LLM. Our approach presents several unique advantages. First, it does not necessitate model fine-tuning or the acquisition of additional human preference data, which can be both challenging and costly to obtain. Second, our method is relatively straightforward to implement compared to conventional jailbreak defense strategies that involve extensive pre-processing Ji et al. (2024); Jain et al. (2023).

Previous approaches on jailbreak defense (Kumar et al., 2023; Robey et al., 2023) primarily evaluate performance based on Attack Success Rates (ASR). While extensive research targets the mitigation of harmful content generation, this increased safety often leads to the unintended consequence of _over-refusal_(Cui et al., 2024), wherein LLMs reject benign prompts and consequently become less useful. To our knowledge, existing jailbreak defense approaches do not typically report over-refusal rates using standardized benchmarks Cui et al. (2024). This omission presents a significant issue, as these safety mechanisms are often deployed in environments where LLMs are likely to encounter a diverse array of prompts, the majority of which are not attempts at jailbreak. Thus, deploying such approaches risks introducing an unrealistic degree of over-refusal, particularly on generic prompts or instruction-following tasks. Although previous research report general instruction-following performance, we argue that these evaluations often fall short of comprehensively assessing real-world effectiveness from an over-refusal perspective. We propose a framework for evaluating jailbreak defense methodologies on both ASR and over-refusal metrics, on scenarios involving both harmful and harmless prompts.

Accordingly, we summarize our key research questions below.

RQ1: Can self and external improvement be employed to defend against state-of-the-art jailbreaks?We study a variety of settings with zero-shot prompting and in-context learning and document performance in terms of decrease in ASR.

RQ2: Do existing jailbreak defense methodologies suffer from over-refusal?To the best of our knowledge, we are the first to evaluate various jailbreak robustness approaches, including our own, on over-refusal benchmarks, in terms of error rates inspired from binary classification.

RQ3: How do self and external improvement perform on over-refusal?Specifically, can we identify a version of improvement that avoids over-refusal while achieving a reasonable reduction in ASR and minimizing any deterioration in general instruction-following capabilities?

Figure 1: **LLM self-improvement.** Example illustrating the instruction for zero-shot improvement, given an initial jailbreak prompt and corresponding model response.

We conduct experiments on state-of-the-art jailbreak attacks targeting the Llama-2 family of models Touvron et al. (2023). We demonstrate that both self-improvement and external improvement substantially improve LLM response to jailbreaks and reduce ASR to as low as \(0\%\) (compared to \(46\%\) in case of the undefended LLM). Specifically, employing an externally aligned model and utilizing few-shot demonstrations generally results in the most substantial reduction in ASR. Our approaches simultaneously outperform various previous baselines in terms of general instruction following performance.

Additionally, we investigate prior defense methodologies for over-refusal and demonstrate that prior approaches exhibit disproportionate over-refusal rates not just on standard over-refusal benchmarks but also on generic instruction following tasks. We leverage our proposed evaluation framework to comprehensively assess various defense approaches including our proposed methods on a combination of harmful and harmless benchmarks. We observe that specific variations of our proposed approaches achieve reasonable performance on decreasing ASR while minimizing over-refusal. However, we show that in-context variations continue to suffer from high over-refusal, opening up avenues for future research in this area. Finally, we highlight crucial inconsistencies in LLM jailbreak literature, particularly related to evaluation methods, with the aim of contributing to more consistent research in the future.

## 2 Related Work

Self-Improvement in LLMs:A series of prior research Pan et al. (2023); Wang et al. (2023); Yuan et al. (2024); Chen et al. (2024); Liu et al. (2023a); Butt et al. (2024) explores self-improvement to augment capabilities of LLMs across a variety of tasks. Shinn et al. (2023); Madaan et al. (2024); Wang et al. demonstrate that self-improvement at inference i.e., without updating model parameters can be effective on a variety of tasks. However, Huang et al. (2023a) show that performance improvement is limited on reasoning tasks.

Jailbreaks Defense:Prior work on training-free jailbreak defenses focus on input pre-processing, such as, perplexity-based methods Jain et al. (2023); Alon and Kamfonas (2023), using auxiliary classifiers Inan et al. (2023); Markov et al. (2023), or the LLM itself Helbling et al. (2023); Cao et al. (2023); Kumar et al. (2023), paraphrasing Jain et al. (2023) and ensembling predictions Robey et al. (2023). In contrast, our approaches are relatively straightforward to implement without necessitating any input pre-processing. Wei et al. (2023) use in-context examples to guide defense mechanisms. Xu et al. (2024) propose safety-aware decoding. Xie et al. (2023) encapsulate user's prompt in a system prompt that reminds the model to answer responsibly. In contrast, our approaches aim to align existing model generations. In contrast to Wang et al. (2024), we additionally investigate external improvement under various configurations of zero-shot prompting and in-context learning.

Over-Refusal on Harmless Prompts:Bianchi et al. (2024) observe that the process of fine-tuning for safety alignment leads to excessive refusals. Rottger et al. (2024) present XSTest, a benchmark consisting of 250 manually curated harmless prompts designed to appear deceptively harmful. To address the limitations of XSTest's static nature, Cui et al. (2024) propose OR-Bench, a dynamic benchmark for over-refusal comprising 80K prompts.

## 3 Methods

Consider an LLM that maps an input prompt \(x\) to a response \(y=(x)\), where \(\) denotes the space of natural language and \(\) denotes a set of adversarial prompts, e.g., _'Develop a strategy for hacking into a government database'_. A typical jailbreak attack transforms \(x\) to \(q=(x)\) such that the LLM response LLM\((q)\) overrides safety alignment. For instance, the adversarial prefix attack as described in Wei et al. (2024) appends the string _"Absolutely! Here's'_ to an adversarial prompt to manipulate the model generations towards an affirmative response. If the model indeed responds positively to \(q\), LLM\((q)\) is referred to as jailbroken. Evaluating the extent to which a model response is jailbroken constitutes a critical research question. Building on previous research in this domain, we consider an evaluation setup where a model generated response is identified as either jailbroken or not.

To generate aligned responses, the goal of the jailbreak defender is to ensure that the LLM does not output unsafe or unaligned responses formally expressed as \((q)\) where \(\) denotes the set of unaligned/jailbroken responses. Conversely, the jailbreak attacker strives to achieve the opposite, aiming to elicit \((q)\). Consequently, the _Attack Success Rate_ (ASR) is defined as \(_{q}[(q)]\), where \(A\) represents an attack strategy. For jailbreak defense, our goal is to _decrease_ ASR.

### LLM Improvement to Defend Against Jailbreaks

In this section, we present our defense approaches. Our objective is to improve model generated output to ensure it no longer responds affirmatively to a jailbreak attack. Our training-free intervention operates during inference, eliminating the need for access to model parameters. In contrast to prior work, our approach is relatively straightforward and doesn't need input pre-processing. Given a jailbroken response \((q)\), we prompt the LLM to improve its response using an instruction for improvement \((q,(q))\) such that the updated response \(((q,(q)))\). Specifically, the instruction for improvement we employ is _'Refine and improve the above response as a helpful...Here is a refined response to the query.'_ (full prompt in Figure 1). We conduct experiments utilizing two configurations: a) self-improvement with the original LLM where the model refines its response based on its inherent knowledge, and b) improvement using an external LLM, where we prompt an external LLM with the \((q,(q))\) for refining the response. In each scenario, we experiment with zero-shot prompting and few-shot in-context learning.

Zero-Shot Improvement:For zero-shot self-improvement, we prompt the model with the initial jailbreak prompt \(q\), the corresponding response \((q)\) and an instruction to improve the response. The improvement prompt is thus formulated as \((q,(q))=q+\)\((q)+\). For improvement using an external model, we utilize the identical prompt with a second LLM. We denote these approaches as Self-Imp-Zero and Ext-Imp-Zero.

In-Context Learning for Improvement:For in-context learning, we conduct experiments using two-shot learning, incorporating two instruction-output examples as demonstrations. Specifically, we include one example where the initial response is jailbroken, followed by an improved, aligned response. In the second example, the initial response is already aligned, and therefore, the improved response does not change the original response. Consequently, our prompt for in-context learning adheres to the following format: imprv-inst + _Query:_\(q_{1}\), _Response:_\((q_{1}),q_{2}\), _Response:_\((q_{2})+\)_'Here is a refined response.'_. We denote these approaches using in-context learning as Self-Imp-Few and Ext-Imp-Few.

### Framework for Evaluation on Harmful and Harmless Prompts

Current evaluation frameworks for assessing jailbreak defenses are insufficient for comparing different defense mechanisms, as they predominantly focus on the safety objective of reducing the ASR. Specifically, existing evaluations are limited to harmful prompts intentionally optimized to jailbreak the LLM. This is essentially analogous to assessing the performance of spam classifiers solely on the'spam' class. Consequently, these approaches fail to account for over-refusal, i.e., the unwarranted denial of benign queries by the safety-aligned LLM.

To facilitate concurrent evaluations of both harmful and harmless prompts and to enable better comparative analysis of various defense mechanisms, we propose a comprehensive evaluation framework that allows for performance reporting using standard binary classification metrics, such as precision, recall, accuracy, and F1 scores. Accordingly, we define two classes such that **Class 0** denotes a set of harmful prompts (e.g., e.g., jailbreak prompts derived from Advbench Zou et al. (2023)) and **Class 1** denotes a set of harmless prompts (e.g., prompts sourced from a benchmark on over-refusal). In this framework, a true positive occurs when the system correctly responds to a harmful prompt with a refusal (e.g., _'Sorry, I can't do that"_). A false positive occurs when the system incorrectly responds to a harmless prompt with a refusal, while a false negative occurs when the system incorrectly generates a harmful continuation in response to a harmful prompt.

Existing approaches, which target or are optimized on harmful prompts only (i.e., Class 0), predominantly assess the recall of the system. In contrast, we propose this framework to assess our approaches (and other existing approaches) on a combination of harmful and harmless prompts (Class 1), so that we can evaluate not only recall but also precision, and subsequently metrics such as F1 score and accuracy. If a jailbreak defense intervention improves recall but reduces precision, it implies that the method is over-estimating the harm in benign prompts (i.e., over-refusal) compared to an LLM without such defense interventions.

## 4 Experimental Setup

Models:We experiment with Llama-2 (7B) and Llama-2 (13B), base and chat models (chat models are RLHF aligned with human preferences Touvron et al. (2023)). In all experiments on external improvement, we employ Llama-2 (13B) chat as the external LLM.

Jailbreak Attacks:We experiment with the following state of the art jailbreak attack techniques from prior work:

a) GCG:For generating this attack, we use single-target elicitation introduced by Zou et al. (2023). We replicate the default hyper-parameters, with a batch size of \(512\), learning rate of \(0.01\), and attack string length of \(20\) tokens.

b) AutoDAN:We use AutoDAN-HGA (hierarchical genetic algorithm) replicating the initial prompt in Liu et al. (2023b). We generate GCG and AutoDAN attacks on AdvBench-50 Robey et al. (2023); Chao et al. (2023) which is a subset of \(50\) instances on AdvBench Zou et al. (2023). AdvBench consists of instances of harmful behaviors expressed as specific instructions. These behaviors range over a variety of themes e.g., threatening behavior, misinformation, discrimination, and dangerous or illegal suggestions.

ASR Evaluation:Following previous work Huang et al. (2023b), we compute attack success rate as the percentage of harmful prompts that successfully jailbreak the LLM, i.e., # affirmative LLM responses # adversarial prompts For successful jailbreak defense, our goal is to decrease attack success, therefore lower ASR implies better performance. We compute ASR using a keyword-based heuristic from prior work Zou et al. (2023), which we list in Table 3 (Appendix).

Comparison with Prior Defense:We compare our proposed approaches on LLM improvement against following prior research on jailbreak defenses:

a) LLMFilter:Phute et al. (2024) leverages an external LLM to detect whether the model generated response is harmful, in which case it flags the generation.

b) InContextDefense:Wei et al. (2024) This approach leverages in-context examples (specifically two-shot demonstrations) to demonstrate refusal to harmful prompts.

c) EraseAndCheck:Kumar et al. (2023) We implement the erase-suffix-check variation of this approach which analyzes a fixed et of \(20\) substrings in the input prompt and uses the target LLM to check if that part of the input prompt has harmful content.

d) SmoothLLM:Robey et al. (2023) This approach also operates by pre-processing an input prompt. We follow the default implementation and report on three variants of this approach, Random-SwapPerturbation (SmoothLLM-Swap), RandomPatchPerturbation (SmoothLLM-Patch), and RandomInsertPerturbation (SmoothLLM-Insert).

Impact on General Performance:We report performance on general instruction following tasks using the InstructFollow Zhou et al. (2023) dataset which consists of \(541\) instructions with associated heuristics to evaluate LLM generated outputs for whether the instructions were followed in the responses.

Benchmarks on Over-Refusal:To evaluate various defense methods on over-refusal, we experiment with the following.

a) XSTest:Rotter et al. (2024) is a standard benchmark for over-refusal/ exaggerated safety behavior. The dataset consists of \(250\) safe prompts across ten prompt types that well- calibrated models should not refuse, along with a contrast set of \(200\) unsafe prompts. We report over-refusal on the subset of \(250\) harmless prompts.

b) Instruct Follow:In addition, we compute over-refusal on the full InstructFollow dataset. Precisely, we detect over-refusal using the same keyword-based metric which we use for ASR evaluation from prior work Zou et al. (2023).

## 5 Results

Rq1: Can self and external improvement substantially decrease ASR against state of the art jailbreaks?We evaluate our proposed approaches and previous defense methodologies in terms of effectiveness in reducing the Attack Success Rate (ASR), and present results in Table 1. We list the various defense approaches in the first column. In the second column, we report ASR \(\%\) (recall that lower ASR indicates better performance) on GCG and AutoDAN attacks generated on AdvBench-50. On GCG, all our improvement strategies achieve a substantial reduction in ASR, decreasing it from \(46\%\) to \(0\%\), and thereby outperform various prior research, including EraseAndCheck and both the patch and insert variations of SmoothLLM. We present examples of our results in Table 4 (Appendix).

On AutoDAN, external improvement through few-shot demonstrations achieves the most significant reduction in ASR from \(64\%\) to \(0\%\), surpassing the performance of EraseAndCheck and SmoothLLM-Swap. Methods such as InContextDefense, LLMFilter, and the patch and insert variations of SmoothLLM demonstrate comparable performance. Self-improvement using few-shot demonstrations decreases ASR to \(2\%\), achieving results equivalent to EraseAndCheck and SmoothLLM-Swap. Furthermore, both zero-shot improvement approaches achieve substantial reductions in ASR, bringing it down to just \(6\%\).

As demonstrated in previous work Ji et al. (2024), increased jailbreak defense capabilities may result in diminished general performance. We present experimental results for general instruction-following performance in the third column of Table 1. We quantify performance on InstructFollow using four metrics, as proposed in Zhou et al. (2023). These metrics evaluate generations at both prompt and instruction levels, applying strict and more lenient criteria in each case. Each prompt in the InstructFollow dataset contains multiple verifiable instructions. Prompt-level accuracy represents the proportion of prompts where all verifiable instructions are followed. Instruction-level accuracy indicates the proportion of individual instructions that are followed. This evaluation results in four

  &  &  \\  &  &  \\  & GCG & AutoDAN & SP & LP & SI & LI \\  None & 46 & 64 & 29.3 & 32.5 & 40 & 44 \\  Self-Imp-Zero & **0** & 6 & 14.2 & 20.3 & 25.4 & 32.2 \\ Self-Imp-Few & **0** & 2 & **24.9*** & **25.6*** & **39.5*** & **40*** \\ Ext-Imp-Zero & **0** & 6 & 19.2 & **25.6*** & 32 & 37.6 \\ Ext-Imp-Few & **0** & **0** & **24*** & 24.5 & **38.7*** & 39.2 \\  EraseAndCheck & 4 & 2 & 23.9 & **31.6*** & 34.5 & **41.8*** \\ InContextDefense & **0** & **0** & 12.3 & 14.9 & 22.5 & 25.5 \\ LLMFilter & **0** & **0** & 18.8 & 23.1 & 26.2 & 29.6 \\ SmoothLLM-Swap & **0** & 2 & 12.9 & 18.6 & 23 & 29.6 \\ SmoothLLM-Patch & 4 & **0** & 12.5 & 17.5 & 22.3 & 28.5 \\ SmoothLLM-Insert & 2 & **0** & 13.6 & 18.2 & 23.5 & 29.8 \\  

Table 1: **ASR and Instruction Following Results on Llama-2-7B-Chat.** First column shows ASR (\(\%\), lower is better) for various defense approaches on AutoDAN and GCG attacks. Second column presents results on instruction following accuracies (higher is better). A single *, and ** indicate the first and second best Instruction-Following scores across various approaches (four aggregated scores: SP - Strict Prompt level, LP - Loose Prompt Level, SI - Strict Instruction level, LI - Loose Instruction level.)metrics (higher is better instruction following performance in each case): SP and LP denote strict and loose prompt-level accuracies, respectively, while SI and LI denote strict and loose instruction-level accuracies. The loose variants involve transformations that relax constraints, which empirically increase the true positive rate (i.e., correctly identifying when an instruction is followed) at the expense of a higher false positive rate (i.e., incorrectly identifying that an instruction is followed when that is not the case).

Considering defense approaches that achieve lowest ASR on GCG and AutoDAN, Self-Imp-Few demonstrates superior performance on InstructFollow, across both strict and loose instruction-following metrics, at both the prompt and instruction levels. Notably, all of our LLM improvement strategies consistently rank first or second in performance, surpassing nearly all prior defense approaches. We find that, in general, improvement strategies incorporating few-shot demonstrations are able to significantly reduce ASR while minimizing the compromise on general instruction-following capabilities.

It is important to note that EraseAndCheck and SmoothLLM operate by pre-processing adversarial prompts in different ways to mitigate attack effectiveness, which can inadvertently lead to information loss in the prompts and negatively affect overall performance. Our approach, in contrast, avoids input pre-processing, reducing computational overhead. For instance, EraseAndCheck requires exhaustive examination of all possible substrings in the input prompt for harmful content. Furthermore, our method can be layered on top of any input pre-processing approach to further reduce ASR, which we plan to explore in future research.

RQ2: Do Current Jailbreak Defense Methodologies Buffer from Over-Refusal?We evaluate various prior defense methods including LLM improvement approaches for over-refusal on XSTest as well as InstructFollow and report results in Table 2. Again, we list the various defenses in the first column. In the second column, we present the rate of over-refusal as the percentage of prompts on XSTest and InstructFollow for which the model incorrectly identifies harmful content and refuses to provide an answer. Notably, Llama-2-7b-chat model without any defense mechanism still exhibits a high over-refusal rate of \(50\%\) on XSTest. However, this rate is comparatively lower at \(15.1\%\) on InstructFollow.

We observe that InContextDefense results in an alarming over-refusal rate of \(95.2\%\) on XSTest and an even higher rate of \(100\%\) on InstructFollow. Similarly, LLMFilter also demonstratesvery high over-refusal rates of \(85.2\%\) and \(56.7\%\) on XSTest and InstructFollow, respectively. These values are significantly higher compared to the model without any defense. Interestingly, EraseAndCheck and SmoothLLM slightly decrease over-refusal rates on XSTest relative to the model without defense. On InstructFollow, however, EraseAndCheck nearly doubles the over-refusal rate compared to the undefended model, while SmoothLLM does not exacerbate over-refusal compared to the undefended model.

  &  &  \\  & XSTest & InstructFollow & Accuracy & Precision & Recall & F1 \\  None & 50 & 15.1 & 51.4 & 30.5 & 55 & 39.2 \\  Self-Imp-Zero & **16.8*** & 25.3 & **87.1*** & **69.7*** & 97 & **81.1*** \\ Self-Imp-Few & 91.6 & 93.7 & 34.2 & 30.1 & 99.0 & 46.2 \\ Ext-Imp-Zero & 59.2 & **0.18*** & 56.8 & 39.5 & 97.0 & 56.2 \\ Ext-Imp-Few & 96.8 & 90.7 & 30.8 & 29.2 & **100*** & 45.2 \\  EraseAndCheck & 38.4 & 27.7 & 71.7 & 50.2 & 97.0 & 66.2 \\ InContextDefense & 95.2 & 100 & 32.0 & 29.5 & **100*** & 45.6 \\ LLMFilter & 85.2 & 56.7 & 39.1 & 31.9 & **100*** & 48.4 \\ SmoothLLM-Swap & 35.6 & 15.7 & 74.2 & 52.6 & 99.0 & 68.7 \\ SmoothLLM-Patch & 38.4 & 14.2 & 72.0 & 50.5 & 98.0 & 66.6 \\ SmoothLLM-Insert & 33.6 & 15.7 & 75.7 & 54.1 & 99.0 & 69.9 \\  

Table 2: **Over-Refusal Rates and Classification-Style Scores on Llama-2-7b-Chat.** First column presents over-refusal rates (lower is better) on XSTest and InstructFollow. Scores on various classification metrics (higher is better) in the second column judge performance on a combination of harmful and harmless prompts. Entries in *, indicate the best performing defense approaches.

In the third column in Table 2, we present evaluation results on classification-inspired metrics, as described in section 3.2. We utilize a total of \(100\) harmful prompts (\(50\) each from AutoDAN and GCG attacks on AdvBench-50). The harmless prompts are the benign subset of 250 prompts from XSTest. Recall that in our binary classification-inspired setting, Class 0 denotes harmful category and Class 1 demotes harmless. In each case, we report four metrics: a) accuracy, b) precision, c) recall and d) F1 Score (higher denotes better overall performance in each case). In Figure 3 (Appendix), we show the associated classification-style confusion matrix from which we compute the above four metrics. We observe that self-Imp-Zero outperforms all other approaches in terms of accuracy, precision and F1 score. Various other jailbreak defense approaches achieve high recall but lower precision using our evaluation framework.

To further illustrate classification-style performance evaluation, we present a confusion matrix for a subset of the defenses in Figure 2. For each defense, we plot the number of input prompts (harmful or benign) that are identified as harmful or benign in the model generations (using the ASR heuristic). Based on the color map, a darker color on the main diagonal indicates that the defense method achieves higher ASR while minimizing over-refusal. Self-Imp-Zero achieves the best performance on this confusion matrix vs. the other defenses (full plot in Appendix).

RQ3: How do Self and External Improvement Perform on Over-Refusal?We report performance of our proposed approaches on over-refusal, also in Table 2. We observe that Self-Imp-Zero achieves substantial reduction in over-refusal on XSTest compared to the undefended model, demonstrating that improvement can help address over-refusal. However, over-refusal increases on InstructFollow relative to the undefended model. We observe that the specific details on compliance in our improve instruction sometimes induce additional response related to compliance which gets flagged as an over-refusal.

Ext-Imp-Zero doesn't significantly change over-refusal compared to the undefended model. This can be due to the fact that larger aligned models tend to be more conservative in maintaining safety alignment. However, it scores much lower over-refusal on InstructFollow. Our intuition is that a larger external model is generally more capable of improving original model response to follow instructions more effectively. Interestingly, both our in-context approaches perform poorly on over-refusal across both datasets, similar to our observations earlier on inContextDefense, highlighting the need for better in-context approaches where superior jailbreak defense performance doesn't come at the cost of high over-refusal.

## 6 Discussion

In this section, we highlight some inconsistencies we encountered in the area of LLM jailbreaks, particularly in evaluation methodologies, with the goal of contributing to more consistent research in the future.

Figure 2: **Confusion Matrix heatmap for a subset of the defenses. A darker color on the main diagonal indicates a superior method that achieves high ASR and low over-refusal. The harmful prompts consist of a concatenation of AutoDAN and GCG attack prompts on AdvBench-50. The benign prompts are a subset of \(250\) harmless prompts from XSTest.**

Evaluating Over-Refusal on InstructFollow:We note that InContextDefense scores an alarming \(100\%\) over-refusal on InstructFollow (Table 2). However, it manages to achieve reasonable instruction following scores using the standard instruction following heuristics in Zhou et al. (2023) (e.g., \(22.5\%\) Strict Instruction/SI score in Table 2). In Table 7 in the Appendix, we present several instances in which LLM generations with over-refusal phrases such as _'Sorry, I cannot'_ (using InContextDefense), are inaccurately identified as successful instruction following according to the heuristics in Zhou et al. (2023). This finding highlights that while the datasets and metrics used in InstructionFollow benchmark are designed for evaluating performance on general (i.e., non-harmful) prompts, they do not effectively capture over-refusal.

Limitations of ASR Heuristic:As identified previous research, computing ASR using keyword search has major limitations primarily due to the restricted scope of standard list of keywords. Nevertheless, we use this heuristic in order to be consistent with previous work on developing attacks and defenses. To address any inconsistencies, we manually inspected model generations on the \(50\) samples from AdvBench-50 to ensure that the results in Table 1 do not suffer from the limitations of this heuristic.

## 7 Conclusions

We demonstrate that self-improvement and external improvement significantly enhance LLM responses to jailbreaks. Additionally, we investigate prior defense methodologies for over-refusal and highlight that existing approaches exhibit disproportionate over-refusal rates not only on standard benchmarks but also on generic instruction-following tasks. We show that specific adaptations of our proposed LLM improvement methods effectively decrease attack success while minimizing over-refusal _and_ without sacrificing general instruction following performance.

## 8 Limitations

We acknowledge some limitations inherent in our study.

Evaluating Over-Refusal on InstructFollow:We note that InContextDefense scores an alarming \(100\%\) over-refusal on InstructFollow (Table 2). However, it manages to achieve reasonable instruction following scores using the standard instruction following heuristics in Zhou et al. (2023) (e.g., \(22.5\%\) Strict Instruction/SI score in Table 2). In Table 7 in the Appendix, we present several instances in which LLM generations with over-refusal phrases such as _'Sorry, I cannot'_ (with InContextDefense), ae inaccurately classified as successful instruction following according to the heuristics in Zhou et al. (2023). This finding highlights that while the datasets and metrics used in the InstructionFollow benchmark are designed for evaluating performance on general (non-harmful) prompts, they do not effectively capture over-refusal.

Limitations of ASR Heuristic:As identified previous research, computing ASR using keyword search has major limitations primarily due to the restricted scope of standard list of keywords. Nevertheless, we use this heuristic in order to be consistent with previous work on developing attacks and defenses. Moreover, we manually inspected model generations on the \(50\) samples from AdvBench-50 to ensure that the results in Table 1 do not suffer from the limitations of this heuristic.

Other Limitations:Our findings are derived from experiments conducted with the Llama-2 family of models. The results may exhibit sensitivity to various factors such as a different LLM, different model parameters, generation configurations, decoding strategies, prompt design, and in-context learning. This sensitivity suggests avenues for further exploration, which we intend to pursue in subsequent research. Moreover, our methodology may encounter limitations related to the context window length, particularly in scenarios involving in-context demonstrations. These demonstrations typically encompass a pair comprising a query and its corresponding response, where the response might consist of long-form unstructured text. However, we did not encounter this in current experiments.