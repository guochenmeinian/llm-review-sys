# Spherical Frustum Sparse Convolution Network for LiDAR Point Cloud Semantic Segmentation

Yu Zheng\({}^{1}\), Guangming Wang\({}^{2}\), Jiuming Liu\({}^{1}\), Marc Pollefeys\({}^{3}\), Hesheng Wang\({}^{1}\)

\({}^{1}\) Department of Automation, Shanghai Jiao Tong University

\({}^{2}\) University of Cambridge \({}^{3}\) ETH Zurich

{zhengyu0730,liujuming,wanghesheng}@sjtu.edu.cn

gw462@cam.ac.uk marc.pollefeys@inf.ethz.ch

Equal Contribution.Corresponding Author.

###### Abstract

LiDAR point cloud semantic segmentation enables the robots to obtain fine-grained semantic information of the surrounding environment. Recently, many works project the point cloud onto the 2D image and adopt the 2D Convolutional Neural Networks (CNNs) or vision transformer for LiDAR point cloud semantic segmentation. However, since more than one point can be projected onto the same 2D position but only one point can be preserved, the previous 2D projection-based segmentation methods suffer from inevitable quantized information loss, which results in incomplete geometric structure, especially for small objects. To avoid quantized information loss, in this paper, we propose a novel spherical frustum structure, which preserves all points projected onto the same 2D position. Additionally, a hash-based representation is proposed for memory-efficient spherical frustum storage. Based on the spherical frustum structure, the Spherical Frustum sparse Convolution (SFC) and Frustum Farthest Point Sampling (F2PS) are proposed to convolve and sample the points stored in spherical frustums respectively. Finally, we present the Spherical Frustum sparse Convolution Network (SFCNet) to adopt 2D CNNs for LiDAR point cloud semantic segmentation without quantized information loss. Extensive experiments on the SemanticKITTI and nuScenes datasets demonstrate that our SFCNet outperforms previous 2D projection-based semantic segmentation methods based on conventional spherical projection and shows better performance on small object segmentation by preserving complete geometric structure. Codes will be available at https://github.com/IRMVLab/SFCNet.

## 1 Introduction

Nowadays, 3D LiDAR point clouds are widely used sensor data in autonomous robot systems. Many recent works focus on resolving perception  and localization  tasks on autonomous robot systems using LiDAR point clouds. Among them, semantic segmentation on the LiDAR point cloud enables the robot a fine-grained understanding of the surrounding environment. In addition, the semantic segmentation results can be adopted for the reconstruction of the semantic map  of the environments.

Inspired by the achievements of deep learning in image semantic segmentation, researchers focus on searching for effective approaches to transfer the achievements to the field of point cloud semantic segmentation. Most previous works convert the raw point cloud to regular grids, like 2D images  and 3D voxels , to exploit Convolutional NeuralNetworks (CNNs) and transformers in the field of point cloud semantic segmentation. The CNNs and transformer can easily process the regular grids to effectively segment the point cloud. However, _due to the limited resolution, more than one point can be projected onto the same grid, and only one point is preserved_, which results in quantized information loss to the regular grid-based point cloud semantic segmentation methods. The quantized information loss poses a challenge for small object segmentation since most points belonging to the small objects can be dropped during the projection. A few methods [13; 17] are proposed to compensate for the quantized information loss by restoring complete semantic predictions from partial predictions. However, quantized information loss still exists in the feature aggregation.

To overcome quantized information loss during 2D projection, in this paper, a novel spherical frustum structure is proposed. Fig. 1 shows the comparison between the conventional spherical projection  and spherical frustum. Through spherical frustum, all the points projected onto the same 2D grid are preserved. Therefore, spherical frustum can avoid quantized information loss during the projection and improve the segmentation of small objects. However, without specific designs, the spherical frustum is an irregular structure and can not be processed by CNNs. Using dense grids to store the spherical frustums is an intuitive method to regularize the spherical frustum. However, since the point number of the spherical frustums is different, each point set is required to be padded to the maximal point number of the spherical frustums before being stored in the dense grid, which results in many redundant memory costs. To avoid redundant memory occupancy, we propose a hash-based spherical frustum representation, which stores spherical frustums in a memory-efficient way. In the hash-based spherical frustum representation, the neighbor relationship of spherical frustums and points is represented through the hash table, which enables the points to be simply stored in the original irregular point set.

In the hash-based representation, each point is uniquely identified by the hash key, which consists of the 2D coordinates of the corresponding spherical frustum and the point index in the spherical frustum point set. Thus, the points projected onto any specific 2D grids can be efficiently queried. Based on the hash-based representation, we propose the Spherical Frustum sparse Convolution (SFC) to exploit 2D CNNs on spherical frustums. SFC aggregates point features of nearby spherical frustums to obtain the local feature of the center point.

Moreover, previous 2D projection-based segmentation methods downsample the projected point cloud based on stride-based 2D sampling, which is unable to uniformly sample the 3D point cloud. However, the stride-based 2D sampling uniformly samples the spherical frustums. Therefore, we propose a novel uniform point cloud sampling method, Frustum Farthest Point Sampling (F2PS).

Figure 1: **Difference between our spherical frustum and conventional spherical projection.** In conventional spherical projection, the points projected onto the same 2D grid are dropped, which leads to quantized information loss, _e.g._, dropping the boundary between the person, a small object, and the road, and results in incorrect prediction of the 2D projection-based method RangeViT  for the person. In contrast, our spherical frustum preserves all points in the frustum, which eliminates quantized information loss and makes SFCNet correctly segment the person.

F2PS firstly samples spherical frustums by stride, and then uniformly samples the point set inside each sampled spherical frustum by Farthest Point Sampling (FPS) . Since the computing complexity of sampling points in each spherical frustum is constant-level, F2PS is an efficient sampling algorithm with a linear computing complexity.

In summary, our contributions are:

* We propose a novel spherical frustum structure with a memory-efficient hash-based representation. Spherical frustum avoids quantized information loss of spherical projection and preserves complete geometric structure.
* We integrate spherical frustum structure into 2D sparse convolution, and propose a novel Spherical Frustum sparse Convolution Network (SFCNet) for LiDAR point cloud semantic segmentation.
* An efficient and uniform 3D point cloud sampling named Frustum Farthest Point Sampling (F2PS) is proposed based on the spherical frustum structure.
* SFCNet is evaluated on the SemanticKITTI  and nuScenes  datasets. The experiment results show that SFCNet outperforms previous 2D projection-based methods and can better segment small objects.

## 2 Related Work

Point-Based Semantic Segmentation.A group of works [26; 28; 29; 30; 31; 32; 33] learn to segment point cloud based on the raw unstructured point cloud. However, learning of raw point cloud requires the neighborhood query with high computing complexity to learn effective features from the local point cloud structure. Therefore, the efficiency of these point-based methods is limited.

3D Sparse Voxel-Based Semantic Segmentation.Storing large-scale LiDAR point clouds in dense 3D voxels requires huge memory consumption. Therefore, Graham et al  proposes the 3D sparse voxel structure. Instead of dense grids, the hash table is adopted to represent the neighborhood relations of the 3D sparse grids. Based on the hash table, the convolved grids are recorded in the rule book. According to the rule book, the 3D sparse convolution is performed. Based on the sparse 3D voxel architecture, the methods of 3D sparse convolution and 3D attention mechanisms [22; 23; 24; 35; 36; 25; 37] are proposed.

2D Projection-Based Semantic Segmentation.The research of image semantic segmentation [38; 39; 40; 41; 42] has gained great achievement. Thus, many works [11; 12; 13; 14; 15; 43; 18; 19; 20; 21; 16; 17] project the point cloud onto the 2D plane and utilize 2D neural networks to process the projected point cloud. Spherical projection is a widely used projection method first introduced by SqueezeSeg . The subsequent works [11; 12; 13; 14; 43; 20; 21] effectively segment the point cloud with the image semantic segmentation architecture including 2D CNNs and vision transformers.

Due to the limited resolution, the 2D projection-based segmentation methods suffer from quantized information loss. With quantized information loss, networks can only process the incomplete geometric structure and output partial semantic predictions, which results in the penalty of segmentation performance. The previous works only focus on restoring complete semantic predictions from the partial predictions of 2D neural networks. RangeNet++  proposes a post-processing strategy to restore the complete predictions. The semantic predictions of dropped points are voted by the predictions of their K-Nearest Neighbors (KNN). In addition to KNN-based post-processing, KPRNet  directly reprojects incomplete predictions to the complete point cloud and adopts point-based network KPConv  to refine the predictions. However, few works explore the method of preserving the complete geometric structure during projection.

In this paper, we propose the spherical frustum which avoids the quantized information loss of spherical projection. Our spherical frustum structure can not only preserve the complete geometric structure but also output the complete semantic predictions without any post-processing or point-based network refinement.

SFCNet

In this section, the spherical frustum and the hash-based representation will be first illustrated in Sec. 3.1. Based on the hash-based spherical frustum representation, the spherical frustum sparse convolution and frustum farthest point sampling for LiDAR point cloud semantic segmentation will be introduced in Sec. 3.2 and 3.3 respectively. Finally, the architecture of the Spherical Frustum sparse Convolution Network (SFCNet) is illustrated in Sec. 3.4.

### Spherical Frustum

Conventional Spherical Projection.The LiDAR point cloud \(\) is composed of \(N\) points. The \(k\)-th point in \(\) is represented by its 3D coordinates \(_{k}=[x_{k},y_{k},z_{k}]^{T}\) and the input point features \(_{k}^{C_{in}}\), where \(C_{in}\) represents the channel dimension of the features. The conventional spherical projection  first calculates the 2D spherical coordinates of each point:

\[u_{k}\\ v_{k}=[1-(y_{k},x_{k})^{-1}]  W\\ [1-((z_{k}/r_{k})+f_{down}) f^{-1}] H,\] (1)

where \((H,W)\) is the height and width of the projected image. \(r_{k}=^{2}+y_{k}^{2}+z_{k}^{2}}\) is the range of the point. \(f=f_{up}+f_{down}\) is the vertical field-of-view of the LiDAR sensor, where \(f_{up}\) and \(f_{down}\) are the up and down vertical field-of-views respectively. According to the computed 2D spherical coordinates, the point features \(\{_{k}\}_{k=1}^{N}\) are projected onto the 2D dense image. If multiple points have the same 2D coordinates, the conventional spherical projection only projects the point closest to the origin and drops the other points, which results in the quantized information loss.

From Spherical Projection to Spherical Frustum.Since dropping the redundant points projected onto the same 2D position results in quantized information loss, we propose the spherical frustum to preserve all the points projected onto the same 2D position. Specifically, we organize these points as a point set and assign each point with the unique index \(m_{k}\) in the point set. In addition, the 3D coordinates of each point \(\{(x_{k},y_{k},z_{k})\}_{k=1}^{N}\) are preserved as the 3D geometric information for the subsequent modules.

Hash-Based Spherical Frustum Representation.The irregular spherical frustums can not be directly processed by the 2D CNNs. A natural idea to regularize the spherical frustums is putting them in dense grids. To store the point set of each spherical frustum in the dense grids, an extra grid dimension is required. The size of this dimension should be the maximal point number \(M\) of each spherical frustum point set. However, since most of the spherical frustum point numbers are much less than \(M\), many grids are empty. To avoid saving these empty grids in memory, we propose the hash-based spherical frustum representation to regularize the spherical frustum, where the hash table replaces the dense grids to map the 2D coordinates to the corresponding spherical frustums and points. In the hash table, the index \(k\) of any point in the original point cloud can be queried using the key \((u_{k},v_{k},m_{k})\), which is the combination of the 2D spherical coordinates and the point index in the spherical frustum point set. Based on the hash-based representation, the spherical frustums are regularly stored in a memory-efficient way.

### Spherical Frustum Sparse Convolution

Since multiple points are stored in a single spherical frustum, the conventional 2D convolution can not be directly performed on the spherical frustum structure. Therefore, we propose Spherical Frustum sparse Convolution (SFC). As shown in Fig. 2, SFC can be seen as the sparse convolution on the virtual spherical plane of the center point. The feature of each convolved 2D position on the virtual spherical plane is filled with the feature of the nearest point in the corresponding spherical frustum.

Selecting Convolved Spherical Frustums.SFC first selects the convolved spherical frustums for each center point \(p\). The 3D coordinates and the 2D spherical coordinates of the center point \(p\) are \((x,y,z)\) and \((u,v)\) respectively. The conventional 2D convolution convolves the features of the grids in the convolution kernel. Similar to the conventional convolution, the spherical frustum of each 2D position in the convolution kernel is selected to perform the convolution. The coordinates of the 2D positions are \(\{(u+ u_{i},v+ v_{i})\}_{i=1}^{K^{2}}\), where \(K\) is the kernel size and \(( u_{i}, v_{i})\) are the shift inside the kernel. Then, the points inside each spherical frustum are queried through the hash table. Meanwhile, the features \(\{_{j}\}_{j=1}^{M_{i}}\) of these points are obtained, where \(M_{i}\) is the number of the pointsin the \(i\)-th spherical frustum. Notably, \(M_{i}\) can be zero, which means that no points are projected onto the corresponding 2D position, and the spherical frustum is invalid. The invalid spherical frustums are ignored in the subsequent convolution.

**Selecting the Nearest Point in Each Spherical Frustum.** After identifying the points in the spherical frustum, a feature should be selected from the features of the frustum point set for 2D convolution. PointNet++  emphasizes that the local feature of the center point is expected to be aggregated from the 3D neighboring points. Inspired by PointNet++, we select the feature of the nearest point to the center point in each spherical frustum. Specifically, based on the 3D geometric information, the 3D coordinates \(\{(x_{j},y_{j},z_{j})\}_{j=1}^{M_{i}}\) of the frustum points are obtained for the nearest point selection. Inspired by the post-processing of RangeNet++ , we select the distance of range \(r_{j}=^{2}+y_{j}^{2}+z_{j}^{2}}\) rather than the Euclidean distance as the metric of the nearest point for efficient distance calculation. Therefore, the selected point index of each spherical frustum is \(_{j} r_{j}-r\), where \(r\) is the range of the center point. According to the indexes, the convolved features \(\{_{i}\}_{i=1}^{K^{}}\) are obtained, where \(K^{}\) is the number of valid spherical frustums.

**Sparse Convolution.** Finally, the sparse convolution is performed as:

\[^{}=_{i=1}^{K^{}}W_{i}_{i},\] (2)

where \(W_{i}\) is the convolution weight of the \(i\)-th valid 2D position, and \(^{}\) is the aggregated feature.

Through the proposed spherical frustum sparse convolution, we realize effective regularization and 2D convolution-based feature aggregation for all points in the unstructured point cloud.

### Frustum Farthest Point Sampling

Sampling is a significant process of point cloud semantic segmentation. Through sampling, the network can aggregate the features of different scales and recognize objects of different sizes. Moreover, the sampling should uniformly sample the point cloud to avoid key information loss. The previous 2D projection-based methods sample the projected point cloud using stride-based 2D sampling. This sampling ignores the 3D geometric structure of the point cloud. In contrast, as shown in Fig. 3, our Frustum Farthest Point Sampling (F2PS) only samples the spherical frustums by stride, while the spherical frustum point set is sampled by farthest point sampling.

**Sampling Spherical Frustums by Stride.** Specifically, we split the 2D spherical plane into several non-overlapping windows of size \(S_{h} S_{w}\), where \((S_{h},S_{w})\) are the strides. The spherical frustums in each window are merged as the downsampled spherical frustum. Meanwhile, the points inside the merged spherical frustums are queried through the hash table. Then, the queried points are merged as the point set \(\{p_{l}\}_{l=1}^{L}\) in the downsampled spherical frustum, where \(L\) is the point number in the downsampled spherical frustum.

Figure 2: **Pipeline of Spherical Frustum sparse Convolution. The spherical frustums in the convolution kernel and the points in these spherical frustums are first selected through the hash table. Then, the nearest point in each spherical frustum is determined by the 3D geometric information. Finally, the sparse convolution is performed on the selected point features.**

Sampling Frustum Point Set by Farthest Point Sampling.The Farthest Point Sampling (FPS)  is adopted to uniformly sample the point set in the downsampled spherical frustum. Since the point number of each downsampled spherical frustum is much smaller than the point number of the point cloud, performing FPS is not time-consuming. Specifically, the 3D coordinates of each point in \(\{p_{l}\}_{l=1}^{L}\) are first acquired from the 3D geometric information for 3D distance calculation. Then, the \([L/(S_{h} S_{w})]\) points are iteratively sampled from the original point set. At each iteration, the distance of each non-sampled point towards the sampled point set is calculated. The point that has the maximal distance is added to the sampled set. Finally, the uniformly sampled spherical frustum point set is obtained.

F2PS integrates the stride-based spherical frustum sampling with the FPS-based frustum point set sampling. Thus, F2PS can sample the original point cloud uniformly. In addition, since performing FPS on the frustum point set costs \(O(1)\) time, the computing complexity of F2PS is \(O(N)\). Thus, F2PS is an efficient point cloud sampling algorithm.

### Network Architecture

Based on the Spherical Frustum sparse Convolution (SFC) and Frustum Farthest Point Sampling (F2PS), the Spherical Frustum sparse Convolution Network (SFCNet) is constructed. SFCNet is an encoder-decoder architecture. The hash-based spherical frustum representation is first built for convolution and sampling in the subsequent modules. Then, the point features \(\{^{C}\}\) are extracted through the encoder of SFCNet, where \(C\) is the channel dimension. The encoder consists of the residual convolutional blocks from ResNet , where the convolutions are replaced by the proposed SFC. In addition, the point cloud is downsampled based on F2PS to extract the features of different scales. After each downsampling operation, an SFC layer is adopted to aggregate the neighbor features for each sampled point. Since F2PS can uniformly sample the point cloud, the information of the original point cloud can be fully gathered in the downsampled point cloud. In the decoder, the extracted features are upsampled, concatenated, and fed into the head layer to output the prediction of semantic segmentation.

## 4 Experiments

In this section, we first introduce the datasets adopted in the experiments and the implementation details of the SFCNet. Then, the quantitative results of the two datasets and the qualitative results of the SemanticKITTI dataset are presented. Finally, the ablation studies and comparison with restoring-based methods are conducted to validate the effectiveness of the proposed modules.

Figure 3: **Pipeline of Frustum Farthest Point Sampling.** According to the downsampling strides, the spherical frustums in each stride window are downsampled. Then, through the hash table, the points in each downsampled spherical frustum are queried. The queried points are sampled by Farthest Point Sampling (FPS) based on the 3D geometric information. Finally, the uniformly sampled spherical frustums and point cloud are obtained.

### Datasets

We train and evaluate SFCNet on the SemanticKITTI  and nuScenes  datasets, which provide point-wise semantic labels for large-scale LiDAR point clouds.

SemanticKITTI  dataset contains \(43551\) LiDAR point cloud scans captured by the \(64\)-line Velodyne-HDLE64 LiDAR. Each scan contains nearly \(120K\) points. These scans are split into \(22\) sequences. According to the official setting, we split sequences 00-07 and 09-10 as the training set, sequence 08 as the validation set, and sequences 11-21 as the test set. Moreover, SemanticKITTI provides the point-wise semantic annotations of \(19\) classes for the LiDAR semantic segmentation task.

NuScenes  dataset consists of \(34149\) LiDAR point cloud scans collected in \(1000\) autonomous driving scenes using the \(32\)-line Velodyne HDL-32E LiDAR. Each scan contains nearly \(40K\) points. We adopt the official setting to split the scans of the nuScenes dataset into the training and validation

   Approach & }}}}}}}}}}}}}}\) & }}}}}}}}}}}}}}\)} & }}}}}}}}}}}}}}}\)} & }}}}}}}}}}}}}}}\)} & }}}}}}}}}}}}}}}}\)} & }}}}}}}}}}}}}}}}}\)} & }}}}}}}}}}}}}}}}}}\) & }}}}}}}}}}}}}}}}}} \) & }}}}}}}}}}}}}}}}}\) & }}}}}}}}}}}}}}}}}} & }}}}}}}}}}}}}}}}} & & }}}}}}}}}}}}}}}}} }}}}}}}}}}}}}}}}} }

sets. In addition, in nuScenes dataset, \(16\)-class point-wise semantic annotations are provided for the LiDAR semantic segmentation task.

On both datasets, the performance of LiDAR point cloud semantic segmentation is evaluated by mean Intersection-over-Union (mIoU) .

### Implementation Details

SFCNet is implemented through PyTorch  framework. For spherical frustum, the height and width in the calculation of spherical coordinates are set as \(H=64,W=1800\) for the SemanticKITTI dataset, and \(H=32,W=1024\) for the nuScenes dataset. The channel dimensions \(C\) of the extracted point features for SemanticKITTI and nuScenes datasets are set as \(128\) and \(256\) respectively. The strides \((S_{h},S_{w})\) of the F2PS are all set as \((2,2)\). The multi-layer weighted cross-entropy loss and Lovasz-Softmax loss  are adopted for network optimization. Adam  with the initial learning rate \(0.001\) is treated as the optimizer. The learning rate is delayed by \(5\%\) in every epoch. Random rotation, flipping, translation, and scaling are utilized for data augmentation on both datasets. Model training is conducted on a single NVIDIA Quadro RTX 8000. The training batch size is set as \(4\).

### Quantative Results

We compare our SFCNet to the State-of-The-Art (SoTA) 2D projection-based, point-based and 3D voxel-based segmentation methods on the SemanticKITTI and nuScenes datasets.

Figure 4: Qualitative results on SemanticKITTI validation set. The first column presents the ground truths, while the following three columns show the error maps of the predictions from the three methods. Specifically, the reference from point color to the semantic class in the ground truths is shown at the bottom. In addition, the false-segmented points are marked as red in the error maps. Moreover, we use circles with the same color to point out the same objects in the ground truth and the three error maps. Furthermore, the corresponding RGB images of each scene with the colored point cloud projected are demonstrated. We also show the corresponding zoomed RGB image view of circled objects if they are visible in the RGB images.

As shown in Tabs. 1 and 2, SFCNet outperforms the previous SoTA 2D convolution-based segmentation methods CENet  and SalsaNext  on the SemanticKITTI and nuScenes datasets respectively. In addition, SFCNet also has better performance than the vision transformer-based segmentation method RangeViT  on both two datasets. SFCNet also outperforms the point-based methods and realizes a smaller performance gap between the 2D projection-based methods and the 3D voxel-based methods. As for the per-class IoU comparison, SFCNet has better IoU than the other 2D projection-based methods on the small 3D objects, including the motorcycle, person (which is pedestrian in nuScenes), bicyclist, trunk, and pole. The performance improvement on these small objects results from the elimination of quantized information loss. Without quantized information loss, the complete geometric structure of the small 3D objects can be preserved, which enables more accurate segmentation. We also observe the slightly weaker performances on wide-range classes, _e.g._, road, parking, and terrain, on the SemanticKITTI dataset. However, since preserving complete points significantly improves the accuracies of the hard small objects, SFCNet has a higher mean IoU than the previous 2D projection-based methods.

### Qualitative Results

Fig. 4 presents the qualitative comparison between our SFCNet and the 2D projection-based segmentation methods CENet  and RangeViT . The comparison shows that the predictions of SFCNet have the minimal segmentation error among the three methods. Moreover, the circled objects in the three rows of Fig. 4 demonstrate the accurate segmentation of SFCNet to the persons, poles, and trunks respectively. This result further indicates our better segmentation performance of 3D small objects by eliminating the quantized information loss.

### Ablation Study

In this section, we conduct the ablation study on the SemanticKITTI dataset to validate the effectiveness of the proposed modules. We adopt the baseline network using the conventional spherical projection and stride-based sampling. The results of ablation studies are shown in Tab. 3.

Spherical Frustum Sparse Convolution (SFC).First, we replace spherical projection in the baseline with spherical frustum and adopt spherical frustum sparse convolution for feature aggregation. After replacement, the mIoU increases \(4.3\%\), which indicates that spherical frustum structure can avoid the quantized information loss, and thus prevent segmentation error from incomplete predictions.

Frustum Farthest Point Sampling (F2PS).After replacing the stride-based 2D sampling with F2PS, the mIoU increases \(2.4\%\). F2PS uniformly samples the point cloud and preserves the key information. Thus, the performance of semantic segmentation has been improved.

### Comparision with Restoring-Based Methods

Based on the same baseline network in Sec. 4.5, we compare our SFCNet with the methods that compensate for the quantized information loss by restoring complete predictions from partial predictions, including the KNN-based post-processing  and KPConv refinement . Tab. 4 shows that SFCNet has \(3.2\%\) mIoU improvement to the KNN-based post-processing and \(2.8\%\) mIoU improvement to KPConv refinement. Compared to the restoring-based methods, SFCNet preserves the complete geometric structure for the feature aggregation rather than compensating for the information loss by post-processing or refinement, which results in higher performance of semantic segmentation.

  Method & mIoU (\%) \\   KNN-based Post-processing  & 59.7 \\ KPConv Refinement  & 60.1 \\ SFCNet (Ours) & **62.9** \\  

Table 4: The performance comparison between the restoring-based methods and SFCNet on the SemanticKITTI validation set.

  ID & Baseline & SFC & F2PS & mIoU (\%) \\  
1 & ✓ & & 56.2 \\
2 & ✓ & ✓ & 60.5 \\
3 & ✓ & ✓ & ✓ & **62.9** \\  

Table 3: Results of ablation studies on the SemanticKITTI validation set.

Conclusion

In this paper, we present the Spherical Frustum sparse Convolutional Network (SFCNet), a 2D convolution-based LiDAR point cloud segmentation method without quantized information loss. The quantized information loss is eliminated through the novel spherical frustum structure, which preserves all the points projected onto the same 2D position. Moreover, the novel spherical frustum sparse convolution and frustum farthest point sampling are proposed for effective convolution and sampling of the points stored in the spherical frustums. Experiment results on SemanticKITTI and nuScenes datasets show the better semantic segmentation performance of SFCNet compared to the previous 2D projection-based semantic segmentation methods, especially on small objects. The results show the great potential of SFCNet for safe autonomous driving perception due to the accurate segmentation of small targets.

Limitations and future work.To implement the 2D convolution on the spherical frustum, only the nearest points in the neighbor spherical frustums are adopted in the spherical frustum sparse convolution. This design may limit the receptive field of the network and thus result in a slightly weaker performance of the wide-range classes. To maintain the performance on both the wide-range classes and small classes, the improvement direction is to expand the receptive field based on our spherical frustum structure. To realize this, future work can lie in combining the vision network architecture with a larger receptive field, like the vision transformer  or vision mamba , with our spherical frustum structure. In addition, our work mainly focuses on the supervised and unimodal point cloud semantic segmentation. Future work can also lie in adopting the spherical frustum structure on the weakly-supervised  and multi-modal  point cloud semantic segmentation. Moreover, applying the spherical frustum structure to more tasks on the LiDAR point cloud, like point cloud registration [53; 54] and scene flow estimation , is also a direction for future work.

## 6 Acknowledgments and Disclosure of Funding

This work was supported in part by the Natural Science Foundation of China under Grant 62225309, 62073222, U21A20480 and 62361166632.