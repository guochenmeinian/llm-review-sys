# Multi-Modal Inverse Constrained Reinforcement Learning from a Mixture of Demonstrations

Guanren Qiao\({}^{1}\), Guiliang Liu\({}^{*}{}^{1}\), Pascal Poupart\({}^{2,3}\), Zhiqiang Xu\({}^{4}\)

\({}^{1}\)School of Data Science, The Chinese University of Hong Kong, Shenzhen,

\({}^{2}\)University of Waterloo, \({}^{3}\)Vector Institute, \({}^{4}\)Mohamed bin Zayed University of Artificial Intelligence

2230402410link.cuhk.edu.cn, liuguiliang@cuhk.edu.cn,

pppoupart@uwaterloo.ca,zhiqiang.xu@mbzuai.ac.ae

Corresponding author: Guiliang Liu, email: liuguiliang@cuhk.edu.cn

###### Abstract

Inverse Constraint Reinforcement Learning (ICRL) aims to recover the underlying constraints respected by expert agents in a data-driven manner. Existing ICRL algorithms typically assume that the demonstration data is generated by a single type of expert. However, in practice, demonstrations often comprise a mixture of trajectories collected from various expert agents respecting different constraints, making it challenging to explain expert behaviors with a unified constraint function. To tackle this issue, we propose a Multi-Modal Inverse Constrained Reinforcement Learning (MICRL) algorithm for simultaneously estimating multiple constraints corresponding to different types of experts. MMICRL constructs a flow-based density estimator that enables unsupervised expert identification from demonstrations, so as to infer the agent-specific constraints. Following these constraints, MMICRL imitates expert policies with a novel multi-modal constrained policy optimization objective that minimizes the agent-conditioned policy entropy and maximizes the unconditioned one. To enhance robustness, we incorporate this objective into the contrastive learning framework. This approach enables imitation policies to capture the diversity of behaviors among expert agents. Extensive experiments in both discrete and continuous environments show that MMICRL outperforms other baselines in terms of constraint recovery and control performance. Our implementation is available at: https://github.com/qiaoguanren/Multi-Modal-Inverse-Constrained-Reinforcement-Learning.

## 1 Introduction

A fundamental prerequisite for achieving safe Reinforcement Learning (RL) is that the agents' policies must adhere to the underlying constraints in the environment . However, in many real-world applications (e.g., robot control and autonomous driving), the ideal constraints are time-varying, context-dependent, and inherent to experts' own experience. These constraints are hard to specify mathematically and may not be readily available to RL agents in policy updates.

A promising approach for learning latent constraints is Inverse Constraint Reinforcement Learning (ICRL) . As a data-driven technique, ICRL recovers the underlying constraints

Figure 1: The flowchart of MMICRL.

respected by expert agents from their demonstrations and utilizes these constraints to support downstream applications. Existing ICRL methods [3; 4; 6; 7; 8] commonly assumed that all expert demonstrations follow the same constraints, and they approximated these constraints with a unified constraint model, whereas, in practice, the demonstration data may be collected from various agents, and these agents might follow different or even conflicting constraints. It is problematic to leverage a single constraint model to explain the behaviors of diverse agents. For example, in the context of autonomous driving , the vehicle-distance constraints followed by trucks and cars should differ, and misapplying these constraints could lead to serious traffic accidents.

To differentiate expert demonstrations, previous RL approaches [10; 11; 12; 13] inferred the latent structure of expert demonstrations and identified expert agents by examining their behavioral preferences. However, these methods were specifically designed for imitation learning rather than constraint inference. Moreover, as their identification process primarily relies on an agent classifier that evaluates state-action pairs, there is no theoretical guarantee that the optimal model is identifiable.

In this work, we propose the Multi-Modal Inverse Constrained Reinforcement Learning (MMICRL) algorithm for estimating agent-specific constraints from a mixture of expert demonstrations (Figure 1). MMICRL extends the traditional maximum entropy framework  with agent-conditioned entropy minimization and unconditioned entropy maximization subject to a permissibility constraint. The resulting policy representation facilitates inferring agent-specific constraints by alternating between the following steps: 1) _Unsupervised Agent Identification_. MMICRL conducts trajectory-based agent identification using a flow-based density estimator. The optimal results are identifiable since each agent's policy must correspond to a unique occupancy measure . 2) _Agent-Specific Constraint Inference_. Leveraging the identified demonstrations, MMICRL estimates a permissibility function for distinguishing expert demonstrations from sub-optimal ones, based on which we construct constraints for each type of agent. 3) _Multi-Modal Policy Optimization_. MMICRL measures the accuracy of inferred constraint models by comparing the similarity between expert trajectories and those generated by imitation policies under the inferred constraints. To capture the diversity of behaviors exhibited by multiple agents, we incorporate policy optimization within the contrastive learning framework. We treat the generated trajectories as noisy embeddings of agents, which serve as compact representations of their behaviors. Utilizing the contrastive estimation methods , we can enhance the similarity between embeddings for agents of the same type, while simultaneously maintaining the distinctiveness of diverse agent types.

We empirically demonstrate the performance of our method by conducting experiments in both discrete (e.g., Gridworld)  and continuous environments (e.g., MuJoCo) . MMICRL significantly surpasses other baselines in terms of distinguishing different agents, adhering to the true constraints, and optimizing control performance. To examine the robustness of MMICRL, we investigate its ability to recover from incorrect agent identification and subsequently infer the correct constraints.

## 2 Related Works

**Inverse Constrained Reinforcement Learning.** Prior ICRL methods typically learned constraints from demonstrations under the maximum entropy framework . Some research [18; 19] employed constraints to differentiate between feasible and infeasible state-action pairs in Constrained MDP, but these studies were restricted to inferring discrete constraints in environments with known dynamics. A subsequent work  extended this approach to continuous state-action spaces with unknown transition models by utilizing neural networks to approximate constraints. Inspired by Bayesian Inverse Reinforcement Learning [20; 21; 22],  inferred probability distributions over constraints. To better model demonstrations,  extended ICRL to infer soft constraints rather than hard ones, and  explored ICRL under the multi-agent setting. Striving for efficient comparisons,  established an ICRL benchmark across various RL domains, such as Gridworld, robot control, and autonomous driving. However, these algorithms primarily target inferring constraints for a single agent type, without considering the distinct constraints associated with multiple agent types.

**Learning from a Mixture of Expert Demonstrations.** Multi-task Inverse Reinforcement Learning (IRL) [24; 25] aims to learn from a mixture of expert demonstrations. Some previous studies [10; 26; 12; 27] utilized the Generative Adversarial Imitation Learning algorithm  to model the behaviors of multiple agents. [10; 11; 13; 29] learned interpretable representations of behavioral policies and inferred the latent representation of expert demonstrations in an unsupervised way. Specifically,  learned a Variational Auto-Encoder (VAE)  where the encoder infers the latent factors of variation from mixed demonstrations and the decoder models different types of expert behaviors. To strengthen the consistency between the learned policy and the types of agents, [12; 32] included "burn-in demonstrations" for updating the imitation policies. These methods, however, were proposed for imitation learning or rewards recovery (i.e., for IRL)  instead of constraint inference.

## 3 Problem Definition

**Constrained Mixture-Agent Markov Decision Process.** To support constraint inference for different agents, we formulate the environment as a Constrained Mixture-Agent Markov Decision Process (CMA-MDP) \(^{}\), which can be defined by a tuple \((,,,,p_{},\{(p_{ _{i}},_{i})\}_{ i},,_{0})\) where: 1) \(\), \(\) and \(\) denote the space of states, actions, and latent code (for specifying expert agents). 2) \(p_{}(s^{}|s,a)\) and \((s,a)\) define the transition and reward functions. 3) \(p_{_{j}}(c|s,a,z)\) refers to an agent-specific cost model with an associated bound \(_{j}(z)\), where \(j\) indicates the index of a constraint. 4) \(\) is the discount factor and \(_{0}(s)\) defines the initial state distribution. CMA-MDP assumes the agents are differentiable by examining their policies, implying that different agent types cannot share identical policies. We aim to elucidate these differences with the constraints associated with each agent type.

In contrast to Multi-Agent Reinforcement Learning (MARL) , where multiple agents can act concurrently, CMA-MDP allows only one agent to operate at a given time. Nevertheless, since the agents in CMA-MDP might adhere to distinct constraints, they can develop different optimal control policies or strategies. However, standard MARL frameworks do not explicitly differentiate between agent types, nor do they distinguish their respective policies and constraints.

**Policy Update under Conditional Constraints.** We introduce Constrained Reinforcement Learning (CRL) based on CMA-MDP. For an agent identified by \(z\), the goal of CRL is to find a policy \(^{*}(a|s,z)\) that maximizes expected discounted rewards under the conditional constraints:

\[(|z)=_{}_{_{0},p_{},} _{t=0}^{T}^{t}r_{t}+()_{(_{0},p_{},),p_{_{j}}} c_{j}(|z)_{j}(z)\  j[0,J]\] (1)

where \(()\) denotes the policy entropy weighted by \(\), and we follow  to define the trajectory cost \(c(|z)=1-_{(s,a)}(s,a|z)\) where the permissibility function \((s,a|z)\) indicates the probability that performing action \(a\) under a state \(s\) is safe for agent \(z\). CRL commonly assumes that the constraints are known, whereas in practice, instead of directly observing the constraint signals, we often have access to expert demonstrations that follow the underlying constraints, and thus the agent must recover the constraints from the demonstration dataset.

**Constraint Inference from a Mixture of Expert Dataset.** Based on the CMA-MDP, the goal of ICRL is to discover the underlying constraints respected by different types of expert agents from their demonstrations. To achieve it, we consider the Maximum Entropy framework [4; 14] and represent the likelihood function as follows:

\[p(_{E}|)=^{_{}}})^{N}} _{i=1}^{N}_{z}_{^{i}_{z}}r(^{( i)})^{^{_{}}}(^{(i)}|z)\] (2)

where 1) \(N\) denotes the number of trajectories in the demonstration dataset, 2) \(Z_{^{_{}}}\) is a normalizing term, 3) the permissibility indicator \(^{^{_{}}}(^{(i)})\) can be defined by \((^{(i)}|z)=_{t=1}^{T}_{t}(s_{t}^{i},a_{t}^{i}|z)\), and 4) the agent identifier \(_{^{(i)}_{z}}\) determines whether the trajectory \(^{(i)}\) is generated by the agent \(z\).

By following , constraint inference can be formulated as inferring \(_{t}\) by maximizing this likelihood function. A common approach is parameterizing \(_{t}\) with neural network models and updating the model parameters with the agent-specific expert data [4; 6; 7]. However, the major challenge of this task lies in our lack of knowledge about the agent's identity that generates the expert trajectories (i.e., \(_{^{(i)}_{z}}\) is unknown). For example, a vehicle trajectory dataset does not label the specific type of vehicles, or these labels are coarse-grained, incomplete, and noisy. On the other hand, to explain the optimal policy with the learned constraints, _the types of agents and the constraints must be consistent_. If we evaluate the policy of an agent \(z\) under the constraint for another type of agent \(z^{}\) (e.g., evaluate the driving strategy of a truck under the constraint for a car), the performance will be substantially compromised. As a result, for a specific agent \(z\), the expert trajectories \(\{_{z}^{*}\}\) are optimal while the trajectories generated by other experts \(\{_{z^{}}^{*},_{z^{}}^{*}_{z^{}, }^{*}\}\) are supposed to be sub-optimalor even infeasible. To capture the correct constraints, the ICLR algorithm must identify the agent corresponding to each trajectory in an unsupervised manner.

## 4 Inverse Constrained Reinforcement Learning for a Mixture of Experts

In order to infer constraints for various expert agent types, we introduce the Multi-Modal Inverse Constrained Reinforcement Learning (MMICRL) algorithm (Figure 1). MMICRL employs a conditional imitation policy to capture the diverse agent behaviors, facilitating unsupervised agent identification. Technically, we require the agents' policies to 1) exhibit high entropy when the agent type (specified by \(z\)) is unknown, and 2) collapse to a specific behavioral mode when the type is determined. The objective for MMICRL can be expressed as :

\[-_{1}[()]+_{2} [(|z)]\] (3) \[\,(|z)f_{z}()=_{_{z}}f(),\;(|z)=1 {, and }(|z)(|z)\]

(Proof is provided in Appendix B.) where \(f()\) represents a latent feature extractor, \([(|z)]\) denotes the agent-specific policy entropy, and \([()]\) signifies the entropy without knowledge of the agent type. The weighting parameters \(_{1}\) and \(_{2}\) (both \( 0\)) determine the balance between conditional entropy minimization and general entropy maximization. This objective differs from the traditional Maximum Entropy Inverse Reinforcement Learning (MEntIRL)  in two ways: 1) the objective also minimizes an entropy conditioned on agent types, and 2) it incorporates an additional constraint related to the policy's permissibility (the last constraint). Given this objective, the optimal representation for the trajectory likelihood (i.e., trajectory policy) is:

**Proposition 4.1**.: _Let \(p(z|)\) denote the trajectory-level agent identifier, let \(r()=}{_{2}-_{1}}f()\) denote the trajectory rewards, let \(Z_{^{}}\) denote a normalizing term. The optimal policy of the above optimization problem can be represented as:_

\[(|z)=^{}}} _{z p(z)}[(p(z|))]}{_{2}-_{1}}+r() (|z)^{}{_{1}-_{2}}}\] (4)

Building upon this policy representation (4), we present the key steps of MMICRL, which involves iteratively performing: 1) unsupervised agent identification for calculating \(p(z|)\) (Section 4.1), 2) conditional inverse constraint inference (Section 4.2) for deducing \((|z)\), and 3) multi-modal policy update (Section 4.3) for approximating \((|z)\). MMICRL alternates between these steps until the imitation policies reproduce expert trajectories, signifying that the inferred constraints align with the ground-truth constraints.

### Unsupervised Agent Identification

MMICRL identifies expert trajectories (i.e., learning \(p(z|)\)) in an unsupervised manner. Previous works commonly determined the agents' identities by examining the state-action features [10; 11; 12] with the classifier \(p(z|s_{z},a_{z})\). Nevertheless, different agents may exhibit similar behaviors under certain contexts or at specific time steps within a trajectory, which makes this point-wise identification problematic (e.g., see our experimental results of InfoGAIL  in Section 5).

To derive a reliable agent identifier, MMICRL performs trajectory-level identification by estimating an agent-specific density. Specifically, we define a state-action density (i.e., normalized occupancy measure) \(_{}(s,a)=(1-)(a|s)_{t=0}^{}^{t}p(s_{t}=s|)\) where \(p(s_{t}=s|)\) is the probability density of state \(s\) at time step \(t\) following policy \(\). Based on this density, we consider the theorem:

**Proposition 4.2** (Theorem 2 of ).: _Suppose \(\) is the occupancy measure that satisfies the Bellman flow constraints: \(_{a}(s,a)=_{0}(s)+_{s^{},a}(s^{},a)P(s ^{}|s,a)(s,a) 0\). Let the policy defined by: \(_{}(a|s)=)a^{}}\), then \(_{}\) is the only policy whose occupancy measure is \(\)._

**Density Estimation.** The aforementioned theorem provides a crucial insight for agent identification: "_one can identify an expert agent by examining the occupancy measures in the expert trajectories_". Leveraging this insight, we design a state-action density estimator to compute a density using a _Conditional Flow-based Density Estimator (CFDE)_. CFDE estimates the density of input variables in the training data distribution under an auto-regressive constraint. Moreover, to enhance our density estimator's sensitivity to the behavior of different agents, the estimator also conditions on the agent type. so \(p(|z)=_{i}p(_{i}|_{1:i-1},z)\) where \(:=(s,a)\) defines an event. We implement \(p(_{i}|_{1:i-1},z)=(_{i}|_{i},(( _{i}))^{2})\) where \(_{i}=_{_{i}}(_{1:i-1},z)\) and \(_{i}=_{_{i}}(_{1:i-1},z)\). The neural function \(\) is implemented by stacking multiple MADE layers . The corresponding agent identifier can be represented by Bayesian rule, so \(p_{}(z|)=_{i})p_{}(z||z)}{_{j}p(_{j})p _{}(|z)}\). We assume a uniform prior \(p(z)\), thereby deriving the following form: \(p_{}(z|)=p_{}(s,a|z)}{_{s^{}} _{(s,a)}p_{}(s,a|z^{})}\).

**Agent Identification.** After learning the density model \(p_{}(s,a|z)\) with CFDE, we divide \(_{E}\) into sub-datasets \(\{_{z}\}_{z=1}^{||}\) by: 1) initializing the dataset \(_{z}=\) and 2) \(_{i}_{E}\), adding \(^{i}\) into \(_{z}\) if \(z=_{z}_{(s,a)_{i}}[p_{}(s,a|z)]\). We repeat the above steps for all \(z\).

### Agent-Specific Constraint Inference

Based on the identified expert dataset, we have \(p_{}(z|_{z})=1,_{z}_{z}\), and thus \( p_{}(z|_{z})=0\), so the likelihood function (4) can be simplified to:

\[p(_{z}|,z)=_{i=1}^{N}^{}}} r(_{z}^{(i)})\,(_{z}^{(i)})^{}{ _{1}-_{2}}}\] (5)

where \((_{z}^{(i)})=_{(s,a)_{z}^{(i)}}_{t}(s,a|z)\) and the normalizer \(Z_{^{}}=[r()]()^{}{ _{1}-_{2}}}\). By defining \(=}{_{1}-_{2}}\), we can then define the log-likelihood \([p(_{z}|,z)]\) as:

\[_{i=1}^{N}r(_{z}^{(i)})+_{t=0}^{T}(s_{t}^{(i)},a_{t}^{(i)}|z)-N[r()]_{t=0}^{T}( _{t},_{t}|z)^{}\] (6)

We parameterize the instantaneous permissibility function with \(\), i.e., construct \(_{}(s_{t},a_{t}|z)\) and update the parameters by computing the gradient of the above likelihood function (the derivation resembles that of ), so \(_{}p(_{z}|,z)\) can be defined as:

\[_{i=1}^{N}_{}_{t=0}^{T}[_{}(s_{t}^ {(i)},a_{t}^{(i)}|z)]-N_{_{} \{|z\}}_{}_{t=0}^{T}[_{ }(_{t},_{t}|z)]\] (7)

This inverse constraint objective relies on the nominal trajectories \(\) sampled with the conditional policy \(_{^{}}(|z)\) (also see Figure 1). For simplicity, we denote it as \((|z)\). In the following, we will introduce our approach to learning \((|z)\).

### Multi-Modal Policy Optimization

By definition, the policy \((|z)\) is trained to maximize cumulative rewards subject to constraint \(_{(|z)}[(|z)]\). To be consistent with our MMICRL objective in formula (3), we design the multi-modal policy optimization objective in the following:

\[_{}-_{(|z)}_{t=0}^{T}^{t}r(s_{t},a_{ t})-_{1}[()]+_{2}[(|z)]_{(|z)}_{t=0}^{h}^{t}_{}(s,a,z) \] (8)

This objective extends maximum entropy policy optimization by minimizing an additional agent-conditioned entropy \([(|z)]\), which limits the variance of policy distribution for a specific type of agent. The balance between these entropy terms is controlled by \(_{1}\) and \(_{2}\). Since \([()]=[(|z)]+_{z p(z), (|z)}[p_{}(z|)]+(z)\), and by removing the uniform prior \(p(z)\) (since it is independent of the policy update), the objective (8) can be simplified to:

\[_{}-_{(|z)}r()+_{1}[p_{}(z| )]+(_{2}-_{1})[(|z)]_{(|z)}_{t=0}^{h}^{t}_{}(s,a,z) \] (9)

Intuitively, this objective expands the reward signals with a log-probability term \([p_{}(z|)]\), which encourages the policy to generate trajectories from high-density regions for a specific agent type. This approach ensures that the learned policies \((|z)_{z=1}^{Z}\) are differentiable.

**Learning Diverse Policies via Contrastive Estimation.** In practice, directly augmenting the reward with a log-probability term (as in objective 9) may lead to a sub-optimal policy . This issue arises because the log-probability term assigns a large penalty to trajectories with low \(p_{}(z|)\). In such cases, the controlling policy becomes more sensitive to density estimation (since \(p_{}(z|)=[p_{}(|z)]\), see Section 4.1) rather than the reward signals. Balancing the trade-off between reward and density maximization by identifying an optimal weight \(_{1}\) is challenging, especially when predictions from \(p_{}(|z)\) are less accurate at the beginning of training (i.e., during the cold start). To resolve the above issues, we consider replacing the identification probability with a contrastive estimation method by constructing the following objective for policy optimization:

\[_{}-_{(|z)}r()+_{1}L_{ce}(, _{1,,|Z|})+(_{2}-_{1})(( |z))_{(|z)}_{t=0}^{h}^{t}_{ }(s,a,z)\]

where \(\) defines the _probing sets_ (constructed with the density estimator in Algorithm 1). Given a specific agent type \(z\), these probing vectors are among the most representative data points since they are located in a high-density region conditioning on \(z\) and a low-density region conditioning on other agent types (\( z\)). Inspired by the InfoNCE loss [16; 37], \(L_{ce}\) can be calculated as:

\[L_{ce}(,_{1,,|Z|})=_{t=0}^{T} _{(,_{z})_{z}}f_{s}[(s_{t},a_{t}),(_{z}, _{z})]}{_{}_{(,)_{}}f_{s}[(s_{t},a_{t}),(,)] }\] (11)

where \(f_{s}\) denotes the score function for measuring the similarity between the features from different state-action pairs (we use cosine similarity in the experiment). To interpret \(L_{ce}(,_{1,,|Z|})\), we can treat \((s,a)\{,_{z}\}\) as positive embeddings for \((|z)\) since they are generated by this policy. On the other hand, \((,)\{_{}\}_{ z}\) are negative embeddings for \((|z)\) since they are generated by controlling with other policies \((|)\) (where \( z\)). Considering the generation is influenced by the stochasticity in environment dynamics (e.g., transition functions), we can _equivalently view the generation process as injecting the environmental noise into the policy_, and thus Noise Contrastive Estimation (NCE)  becomes a compatible tool for learning differentiable policies. Specifically, since embeddings in \(_{z}\) belong to a high conditional density region in \(p(|z)\) (see Algorithm 1), the knowledge from the density estimator has been integrated into policy updates. In essence, the integration of contrastive learning into policy optimization helps the algorithm to better understand the relationships between agents, their behaviors, and the corresponding expert trajectories, resulting in improved performance for tasks involving diverse agent types. _Algorithm 2 introduces the detailed implementation of MMICRL_.

``` Input: Agent type \(z\), trajectory dataset \(_{z}\) and conditional density model \(p(|z)\)  Initialize a probing sets \(_{z}=\{\}\); for\(_{z}}_{z}\)do  Find \((,)=_{(s,a)_{z}} p_{_{i}}(s,a|z)-_{}_{ z} p_{_{i}}(s,a|)\);  Store the probing points \(_{z}=_{z}\{(,)\}\);  end for Output:\(_{z}\) ```

**Algorithm 1**Probing_Sets

## 5 Experiments

**Running Setting.** For a comprehensive comparison, we employ consistent evaluation metrics across all environments. These metrics include 1) _Constraint Violation Rate_, which assesses the likelihood of a policy violating a constraint in a given trajectory, and 2) _Feasible Cumulative Rewards_, which calculates the total rewards accumulated by the agent before violating any constraints. The demonstration data are assumed to have a zero violation rate. We run experiments with three different seeds and present the mean \(\) std results for each algorithm. To ensure a fair comparison, we maintain uniform settings for all comparison baselines. Appendix A.2 reports the detailed settings.

**Comparison Methods.** We employ an ablation strategy to progressively remove components from **MMICRL** (Algorithm 2) and consider the following variants: 1) **MMICRL-LD** excludes our contrastive estimation approach, using the objective (9) directly for policy updates. 2) Inspired by , **InfoGAIL-ICRL** replaces the trajectory density model with a discriminator to identify agents based on state-action pairs, extending GAIL  to distinguish between distinct agents' trajectories. 3) **MEICRL** eliminates the agent identifier and expands traditional MaximumEntropy (ME) IRL methods to infer Markovian constraints in a model-free environment. 4) **Binary Classifier Constraint Learning (B2CL)** constructs a deterministic binary classifier directly for constraint inference, bypassing the need for the maximum entropy framework.

### Empirical Evaluations in Discrete Environments

The discrete environments we use are based on Gridworld, a widely studied RL environment that enables us to visualize the recovered constraints and trajectories generated by various agents. We create a 7x7 Gridworld map and design four distinct constraint map settings. In Figure 2, the leftmost column illustrates expert trajectories and constraints for each configuration. Notably, the first three settings (rows 1-3 in Figure 2) incorporate two different types of constraints, while the final setting (the last row in Figure 2) includes three constraint types. The primary goal for each agent is to navigate from the starting point to the endpoint while minimizing the number of steps and avoiding their respective constraints. To facilitate constraint inference, a demonstration dataset containing expert trajectories is provided for each environment .

Figure 2 displays the ground-truth trajectory map alongside the learned trajectory maps. Appendix C.1 summarizes the detailed performance in terms of feasible cumulative rewards, constraint violation rate, and destination-reaching rate. Without implementing agent identification, both B2CL and MEICRL can only restore a single constraint type. Although InfoGAIL-ICRL incorporates agent identification, its performance is unsatisfactory, resulting in incomplete destination-reaching. In contrast, MMICRL-LD and MMICRL exhibit significant improvements; they can identify various agent types corresponding to different constraints and generate accurate trajectories that successfully reach the endpoint. Notably, the enhancements provided by MMICRL are more prominent, and the trajectories produced by the MMICRL algorithm closely resemble expert demonstrations.

### Empirical Evaluations in Continuous Environments

In continuous environments, we evaluate ICLR algorithms by whether they can infer location constraints for various types of robots (robots can only operate within designated areas). The task involves directing the robot to navigate out of the danger zone, represented by the constraint.

  Setting & Dim. & \(1^{st}\) constraint & \(2^{nd}\) constraint \\  Half-checheath & 24 & x\(-3\) & x\( 3\) \\ Anfwall & 121 & x\(-3\) & x\( 3\) \\ Swimmer & 12 & x\(-0.01\) & x\( 0.01\) \\ Walker & 24 & x\(-0.1\) & x\( 0.1\) \\  

Table 1: Continuous environment setting.

[MISSING_PAGE_EMPTY:8]

Figure 5 displays the performance of MMICRL in both continuous and discrete settings. We observe that it can successfully recover multiple constraints corresponding to different types of agents. The errors made at the beginning of training do not have a significant impact on the overall performance, which demonstrates the robustness of MMICRL.

### Empirical Evaluations in Realistic Environments

   Method & Blocked Half-Cheetah & Blocked Ant & Blocked Swimmer & Blocked Walker \\   \\  B2CL & 5.03E+1 / 2.12E+43 & 1.33E+4.311E+43 & 3.28E+2 / 7.40 E-1 & 2.19E+1 / 7.888E+1 \\ MHECRL & 4.85E+1 / 3.45E+3 & 1.80E+4 / 3.31E+43 & 2.01E+2 / 8.30E-1 & 2.61E+1 / 7.33E+1 \\ InfoGALL-ICRL & 2.22E+2 / 1.33E+2 & 3.73E+2 / 1.11E+2 & 2.38E+1 / 7.90E-1 & 2.15E+1 / 1.58E+3 \\ MMICRL-LD & 4.32E+3 / 2.56E+3 & 1.82E+2 / 2.21E+4 & 2.66E+2 / 6.10E+2 & 9.02E+2 / 5.12E+2 \\ MMICRL & **6.12E+3 / 3.00E+3** & **2.13E+4 / 2.17E+4** & **4.07E+2 / 6.48E+2** & **8.90E+2 / 1.53E+3** \\   \\  B2CL & 100\(\%\)\(\)0\(\%\) / 67\(\%\)\(\)24\(\%\) & 33\(\%\)\(\)24\(\%\) / 67\(\%\)\(\)24\(\%\) & 62\(\%\)\(\)12\(\%\) / 100\(\%\)\(\)0\(\%\) / 100\(\%\)\(\)0\(\%\) \\ MEICRL & 100\(\%\)\(\)0\(\%\) / 50\(\%\)\(\)25\(\%\) & 30\(\%\)\(\)0\(\%\) / 67\(\%\)\(\)24\(\%\) & 79\(\%\)\(\)14\(\%\) / 100\(\%\)\(\)0\(\%\) \\ InfoGALL-ICRL & 25\(\%\)\(\)16\(\%\) / 93\(\%\)\(\%\) & 34\(\%\)\(\)18\(\%\) / 37\(\%\)\(\%\)\(\%\)\(\%\)\(\%\) & 73\(\%\)\(\)14\(\%\) / 100\(\%\)\(\)0\(\%\) \\ MMICRL-LD & 33\(\%\)\(\)24\(\%\) / 34\(\%\)\(\)24\(\%\) & **0\(\%\)\(\)0\(\%\) / 0\(\%\)\(\%\)** & 71\(\%\)\(\)16\(\%\) / 34\(\%\)\(\)23\(\%\) & 52\(\%\)\(\)25\(\%\) / 50\(\%\)\(\)25\(\%\) \\ MMICRL & **0\(\%\)\(\)0\(\%\) / 0\(\%\)\(\%\)\(\%\)** & **0\(\%\)\(\%\)\(\%\)\(\%\)** & **55\(\%\)\(\%\)23\(\%\) / 28\(\%\)\(\%\)\(\%\)\(\%\)** & **31\(\%\)\(\)22\(\%\) / 25\(\%\) / 22\(\%\)** \\  

Table 2: MuJoCo testing performance. We report the average feasible rewards and constraint violation rate in 100 runs. The best average performance is highlighted in bold.

Figure 4: The feasible cumulative rewards (left two columns) and constraint violation rate (right two columns). We denote the results for different agents with \(z\_0\) and \(z\_1\). From top to bottom, the environments are Blocked Antwall, Blocked Half–Cheetah, Blocked Walker, and Blocked Swimmer.

To demonstrate the generalization capability of the model, we conducted some preliminary studies in a realistic autonomous driving environment. This environment is constructed by utilizing the HighD dataset (For more details, check  and ). For extracting features of cars and roads, we use the features collector from Commonroad RL . The constraint that we are interested in are 1) Car distance \( 20\)m **(agent 0)** and 2) Car distance \( 40\)m **(agent 1)**. Figure 6 shows the distribution of car distance in expert demonstrations for agent 0 and agent 1. We aim to investigate whether the MMICRL algorithm can differentiate between two different types of cars based on distance constraints.

In the four settings shown in Figure 7, we find that in the first row, the observed distances of the first type of car are all above 20 meters, while in the second row, the distances of the other type of car are mostly above 40 meters. This suggests that the MMICRL algorithm shows promising preliminary results in this context. In the future, we will further optimize the algorithm to achieve a lower constraint violation rate and higher effective rewards in a wider range of real-world environments.

## 6 Limitations

**Omitting Agent Interactions**: MMICRL does not consider the interactions between agents or how they can potentially collaborate or compete to satisfy a joint constraint. Future research could extend the game theory to ICRL for implementing multi-agent constraint inference.

**Experiment in Virtual Environment.** We evaluate our algorithm in virtual games rather than real-world applications (e.g., autonomous driving). This is due to the lack of an ICRL benchmark for a mixture of experts. Future work can explore the application of MMICRL in realistic scenarios.

## 7 Conclusion

In this work, we introduce the MMICRL algorithm to differentiate multiple constraints corresponding to various types of agents. MMICRL incorporates unsupervised constraint inference, agent-specific constraint inference, and multi-modal policy optimization. To demonstrate the advantages of our method over other baselines, we investigate whether MMICRL can accurately perform multiple constraint inference in both discrete and continuous environments.