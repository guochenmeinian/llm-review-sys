# Causal normalizing flows: from theory to practice

Adrian Javaloy

Correspondence to ajavaloy@cs.uni-saarland.de. Department of Computer Science of Saarland University, Saarbrucken, Germany

Pablo Sanchez-Martin

Isabel Valera

###### Abstract

In this work, we deepen on the use of normalizing flows for causal inference. Specifically, we first leverage recent results on non-linear ICA to show that causal models are identifiable from observational data given a causal ordering, and thus can be recovered using autoregressive normalizing flows. Second, we analyse different design and learning choices for _causal normalizing flows_ to capture the underlying causal data-generating process. Third, we describe how to implement the _do-operator_ in causal NFs, and thus, how to answer interventional and counterfactual questions. Finally, in our experiments, we validate our design and training choices through a comprehensive ablation study; compare causal NFs to other approaches for approximating causal models; and empirically demonstrate that causal NFs can be used to address real-world problems--where mixed discrete-continuous data and partial knowledge on the causal graph is the norm. The code for this work can be found at https://github.com/psanch21/causal-flows.

## 1 Introduction

Deep learning is increasingly used for causal reasoning, that is, for finding the underlying causal relationships among the observed variables (_causal discovery_), and answering _what-if_ questions (_causal inference_) from available data . Our focus in this paper is to solve causal inference problems using only observational data and (potentially partial) knowledge on the causal graph of the underlying structural causal model (SCM). This is exemplified in Fig. 1, where our proposed framework is able to estimate the (unobserved) causal effect of externally intervening on the sensitive attribute (red and yellow distributions), _using solely observed data (blue distribution) and partial information about the causal relationship between features_.

In this context, previous works have mostly relied on different deep neural networks (DNNs)--e.g., normalizing flows (NFs) , generative adversarial networks (GANs) , variational autoencoders (VAEs) , Gaussian processes (GPs) , or denoising diffusion probabilistic models (DDPMs) --to iteratively estimate the conditional distribution of each observed variable given its causal parents, thus using an independent DNN per observed variable. Hence, to predict the effect of an intervention in the causal data-generating process, these approaches fix the value of the intervened variables when computing the new value for their children. However, they may also suffer

Figure 1: Observational and interventional distributions of the categorical variable _checking account_ of the German Credit dataset , and their estimated values according to a causal normalizing flow. \(_{S}\) is a binary variable representing the users’ sex.

from error propagation--which worsens with long causal paths--and a high number of parameters, which is addressed in practice with ad-hoc parameter amortization techniques [26; 27]. Moreover, several approaches also rely on implicit distributions [2; 20; 27; 32], and thus do not allow evaluating the learnt distribution.

In contrast, and similar to [16; 32; 34; 41], we here aim at learning the full causal-generating process using a single DNN and, in particular, using a _causal normalizing flow_. To this end, we first theoretically demonstrate that causal NFs are a natural choice to approximate a broad class of causal data-generating processes (SS3). Then, we design causal NFs that inherently satisfy the necessary conditions to capture the underlying causal dependencies (SS4), and introduce an implementation of the do-operator that allows us to efficiently solve causal inference tasks (SS5). Importantly, our causal NF framework allows us to deal with mixed continuous-discrete data and partial knowledge on the causal graph, which is key for real-world applications. Finally, we empirically validate our findings and show that causal NFs outperform competing methods also using a single DNN to approximate the causal data-generating process (SS6).

Related workTo the best of our knowledge, the closest works to ours are [16; 34; 41], as they all capture the whole causal data-generating process using a single DNN. Our approach generalizes the result from Khemakhem et al. , which also relies on autoregressive normalizing flows (ANF), but only considers affine ANFs and data with additive noise. In contrast, our work provides a tighter connection between ANFs and SCMs (affine or not), more general identifiability results, and sound ways to both embed causal knowledge in the ANF, and to apply the do-operator. Another relevant line of works connect SCMs with GNNs [32; 41], and despite making little assumptions on the underlying SCM, they lack identifiability guarantees, and interventions on the GNN are performed by severing the graph, which we show in App. C may not work in general. Nevertheless, it is worth noting that the way we use \(\) in the network design (SS4) is inspired by these works.

## 2 Preliminaries and background

### Structural causal models, interventions, and counterfactuals

A structural causal model (SCM)  is a tuple \(=(},P_{})\) describing a data-generating process that transforms a set of \(d\) exogenous random variables, \( P_{}\), into a set of \(d\) (observed) endogenous random variables, \(\), according to \(}\). Specifically, the endogenous variables are computed as follows:

\[(_{1},_{2},,_{d}) P _{}\,,_{i}=_{i}(_{_{i}},_{i})\,,\,.\] (1)

In other words, each \(i\)-th component of \(}\) maps the \(i\)-th exogenous variable \(_{i}\) to the \(i\)-th endogenous variable \(_{i}\), given the subset of the endogenous variables that directly cause \(_{i}\), \(_{_{i}}\) (causal parents).

An SCM also induces a causal graph, a powerful tool to reason about the causal dependencies of the system. Namely, the causal graph of an SCM \(=(},P_{})\) is the directed graph that describes the functional dependencies of the causal mechanism. We can define the _adjacency matrix of the causal graph_ as \(_{}}(,)\), where \(\) is the constant zero function, and the comparisons are made elementwise. Furthermore, the direct causes of the \(i\)-th variable (\(_{i}\) in Eq. 1) are the _parent_ nodes of the \(i\)-th node in \(\), and the _ancestors_ of this node (which we denoted by \(_{i}\)) are its (in)direct causes. See Fig. 2 for an example of a causal chain.

In the case that \(\) is acyclic, we can pick a causal ordering describing which variables do _not_ cause others, and which ones _may_ cause them. Namely, a permutation \(\) is said to be a _causal ordering_ of an SCM \(\) if, for every \(_{i}\) that directly causes \(_{j}\), we have \((i)<(j)\). Note that this definition equals that of a topological ordering and, without loss of generality, we will assume throughout this work that the variables are sorted according to a causal ordering.

Importantly, besides describing the (observational) data-generating process, SCMs enable _causal inference_ by allowing us to answer _what-if_ questions regarding: i) how the distribution over the observed variables would be if we force a fixed value on one of them (_interventional queries_); and ii) what would have happened to a specific observation, if one of its dimensions would have taken a different value (_counterfactual queries_).

Figure 2: Causal graph, and its causal ordering \(\) and adjacency matrix \(\).

Structural equivalenceTo reason about causal dependencies, we introduce the notion of structural equivalence. We say that two matrices \(\) and \(\) are _structurally equivalent_, denoted \(\), if both matrices have zeroes exactly in the same positions. Similarly, we say that \(\) is _structurally sparser_ than \(\), denoted as \(\), if whenever an element of \(\) is zero, the same element of \(\) is zero.

### Autoregressive normalizing flows

Normalizing flows (NFs)  are a model family that express the probability density of a set of observations using the change-of-variables rule. Given an observed random vector \(\) of size \(d\), a normalizing flow is a neural network with parameters \(\) that takes \(\) as input, and outputs

\[T_{}() P_{} p()= p(T_{}())+( _{}T_{}())\,,\] (2)

where \(P_{}\) is a base distribution that is easy to evaluate and sample from. Since Eq. 2 provides the log-likelihood expression, it naturally leads to the use of maximum likelihood estimation (MLE)  for learning the network parameters \(\). While many approaches have been proposed in the literature , here we focus on autoregressive normalizing flows (ANFs) [18; 24]. Specifically, in ANFs the \(i\)-th output of each layer \(l\) of the network, denoted by \(_{i}^{l}\), is computed as

\[_{i}^{l}_{i}^{l}(_{i}^{l-1};_{i}^{l})\,, _{i}^{l} c_{i}^{l}(_{1:i-1}^{l-1})\,,\] (3)

and where \(_{i}\) and \(c_{i}\) are termed the transformer and the conditioner, respectively. The transformer is a strictly monotonic function of \(_{i}^{l-1}\), while the conditioner can be arbitrarily complex, yet it only takes the variables preceding \(_{i}\) as input. As a result, ANFs have triangular Jacobian matrices, \(_{}T_{}()\).

## 3 Causal normalizing flows

Problem statementAssume that we have a sequence of i.i.d. observations \(=\{_{1},_{2},,_{N}\}\) generated according to an unknown SCM \(\), from which we have partial knowledge of its causal structure. Specifically, we know at least its causal ordering \(\), and at most the whole causal graph \(\). Our objective in this work is to design and learn an ANF \(T_{}\), with parameters \(\), that captures \(\) by maximizing the observational likelihood (MLE), i.e.,

\[*{maximize}_{}_{n=1}^{N} p (T_{}(_{n}))+(_{ }T_{}(_{n}))\,,\] (4)

and that can successfully answer interventional and counterfactual queries during deployment, thus enabling causal inference. We refer to such a model as a _causal normalizing flow_.

AssumptionsWe restrict the class of SCMs considered by making the following fairly common assumptions: i) _diffeomorphic data-generating process_, i.e., \(}\) is invertible, and both \(}\) and its inverse are differentiable; ii) _no feedback loops_, i.e., the induced causal graph is acyclic; and iii) _causal sufficiency_, i.e., the exogenous variables are mutually independent, \(p()=_{i}p(_{i})\).

SCMs as TMI mapsTo achieve our objective, and bridge the gap between SCMs and normalizing flows, we resort to triangular monotonic increasing (TMI) maps, which are autoregressive functions whose \(i\)-th component is strictly monotonic increasing with respect to its \(i\)-th input. TMI maps hold a number of useful properties, such as being closed under composition and inversions. Conveniently, a layer of an ANF (Eq. 3) is a parametric TMI map that can approximate any other TMI map arbitrarily well, which makes ANFs also TMI maps approximators;2 a fact that has been exploited in the past to prove that ANFs are universal density approximators .

We now show that any SCM can be rewritten as a tuple \((,P_{})_{}\), where \(\) is the set of all TMI maps, and \(P_{}\) is the set of all fully-factorized distributions, \(p()=_{i}p(_{i})\). First, given an acyclic SCM \(=(},P_{})\) with \(}:\) as in Eq. 1, we can always unroll \(}\) by recursively replacing each \(_{i}\) in the causal equation by its function \(_{i}\) (see Fig. 3(b) for an example), obtaining an equivalent non-recursive function \(}:\). This function \(}\) writes each \(_{i}\) as a function of its exogenous ancestors \(_{_{i}}\) and, since \(\) is acyclic, \(}\) is a triangular map. For simplicity, assume that \(P_{}\) is a standard uniform distribution. Then, following the causal ordering, we can apply a Darmoisconstruction  and replace each function \(_{i}\) by the conditional quantile function of the variable \(_{i}\) given \(_{_{_{i}}}\) (which depends on \(_{_{_{i}}}\)) eventually arriving to a TMI map \(\). This procedure follows the proof for non-identifiability in ICA , but restricted to one ordering. The case for a general \(P_{}\) follows a similar construction, but using a Knothe-Rosenblatt (KR) transport  instead.

Isolating the exogenous variablesNow that we have SCMs and causal NFs under the same family class--i.e., the family \(_{}\) of TMI maps with fully-factorized distributions--we leverage existing results on identifiability to show that we can find a causal NF \(T_{}\) such that the \(i\)-th component of \(T_{}()\) is a function of the true exogenous variable \(_{i}\) that generated the observed data. More precisely, note that, since we can rewrite \(\) as an element of the family \(_{}\), identifying the true exogenous variables of an SCM \(\) is equivalent to solving a non-linear ICA problem with TMI generators, for which Xi and Bloem-Reddy  proved the following (re-stated to match our setting):

**Theorem 1** (Identifiability).: If two elements of the family \(_{}\) (as defined above) produce the same observational distribution, then the two data-generating processes differ by an invertible, component-wise transformation of the variables \(\).

Thm. 1 implies that, if we can find a causal NF \((T_{},P_{})_{}\) that matches the observational distribution generated by \(=(,P_{})_{ }\), then we know that the exogenous variables of the flow differ from the real ones by a function of each component independently, i.e., \(T_{}(()) P_{}\) with \( P_{}\) and \(T_{}(())=()=(h_{1}(_ {1}),h_{2}(_{2}),,h_{d}(_{d}))\), where each \(h_{i}\) is an invertible function. Fig. 3 graphically illustrates Thm. 1. Furthermore, Thm. 1 also implies that the functional dependencies of the causal NF must agree with that of the SCM, i.e., that \(T_{}\) needs to be _causally consistent_ with \(\). We formally present this result in the following corollary (proof can be found in App. A), where \(\) denotes the identity matrix:

**Corollary 2** (Causal consistency).: If a causal NF \(T_{}\) isolates the exogenous variables of an SCM \(\), then \(_{}T_{}()-\) and \(_{}T_{}^{-1}()+_{n=1}^{ ()}^{n}\), where \(\) is the causal adjacency matrix of \(\). In other words, \(T_{}\) is causally consistent with the true data-generating process, \(\).

A sketch of the proof goes as follows: since Fig. 3 is a commutative diagram, we can write the result of \(T_{}\) and \(T_{}^{-1}\) in terms of the true \(\), \(\), and their inverses. Then, we can use the chain rule to compute their Jacobian matrices, and since \(\) has a diagonal Jacobian matrix, it preserves the structure of the Jacobian matrices of \(\) and its inverse. To sum up, we have shown that _causal NFs are a natural choice to estimate an unknown SCM_ by showing that: i) both SCMs and causal NFs fall within the same family \(_{}\); ii) any two elements of this family with identical observational distributions are causally consistent; and iii) they differ by an invertible component-wise transformation.

### Causal NFs for real-world problems

To bring theory closer to practice, we need to extend causal NFs to handle mixed discrete-continuous data and partial knowledge on the causal graph, which are common properties of real-world problems. Due to space limitations, we provide here a brief explanation, and formalize these ideas in App. A.2.

Discrete dataTo extend our results to also account for discrete data, we take advantage of the general model considered by Xi and Bloem-Reddy  that includes observational noise (independent of the exogenous variables), and consider a continuous version of the observed discrete variables by adding to them independent noise \(\) (e.g., from a standard uniform), such that the real distribution is still recoverable. Intuitively, our approach assumes that discrete variables correspond to the integer part of (noisy) continuous variables generated according to an SCM fulfilling our assumptions, such that both our theoretical and practical insights still apply.

Partial knowledgeWhile we rarely know the entire causal graph \(\), we often have a good grasp on causal relationships between a subset of observed variables--e.g., sex and age are not causally related--while missing the rest. When only partial knowledge on the graph is available--i.e., we only know the causal relationship between a subset of the observed variables, we can instead work with a modified acyclic graph \(}\) obtained by finding the strongly connected components as in , where subsets of variables with unknown causal relationships are treated as a block (see SS7 for an example).

Figure 3: Thm. 1 as a commutative diagram

This allows us to reuse our theoretical results for known parts of the graph, thus generalizing the _block identifiability_ results from von Kugelgen et al. (2019).

## 4 Effective design of causal normalizing flows

We showed in SS3 that causal NFs are a natural choice to learn the underlying SCM generating the data. Importantly, Thm. 1 assumes that we can find a causal NF whose observational distribution perfectly matches the true data distribution (according to the underlying SCM). In practice, however, reaching the optimal parameters may be tricky as: i) we only have access to a finite amount of training data; and ii) the optimization process for causal NFs (like for any neural network) may converge to a local optima. In this section, we analyse different design choices for causal NFs to guide the optimization towards solutions that do not only provide an accurate fit of the observational distribution, but allow us to also accurately answer to interventional and counterfactual queries.

Let us start with an illustrative example. Suppose that we are given the linear SCM in Fig. 3(a), and we want to write the SCM equations as a TMI map to approximate them with a causal NF. As discussed in SS3, we can unroll the causal equations (Fig. 3(b))--resulting in a composition of functions structurally as sparse as \(+\). These functions can be compacted into a single transformation (Fig. 3(c)), such that each \(_{i}\) depends on its ancestors, \(_{_{i}}\). However, note that in this step _shortcuts_ appear, making direct and indirect causal paths in this representation indistinguishable--in our example, the indirect causal path from \(_{1}\) to \(_{3}\) present in Fig. 3(a) and Fig. 3(b) does not go anymore through the path that generates \(_{2}\), but instead via a _shortcut_ that directly connects \(_{1}\) to \(_{3}\). Alternatively, we can invert the equations to write \(\) as a function of \(\) (Fig. 3(d)), which is structurally equivalent to \(-\).

We remark that the above steps can be applied to any considered acyclic SCM (refer to App. B for a more detailed discussion). In particular, we can unroll the equations in a finite number of steps, and we can similarly reason about the causal dependencies through the Jacobian matrices of the generators, \(_{}T_{}()\) and \(_{}T_{}^{-1}()\). Moreover, note that the diffeomorphic assumption implies that we can invert the causal equations. Next, inspired by the different representations of an SCM (exemplified in Fig. 3(d)), we consider the following design choices for causal NFs:

Generative modelThe first architecture imitates the unrolled equations (Fig. 3(b)), i.e., the causal NF is defined as a function from \(\) to \(\). Importantly, when full knowledge of the causal graph \(\) is assumed, we also replicate the structural sparsity per layer by adequately masking the flow with \(+\). In this way, the information from \(\) to \(\) is restricted to flow as if we were unrolling the causal model (Srivastava et al., 2017) and, as a result, the output of the \(l-1\)-th layer of the causal NF is given by:3

\[_{i}^{l-1}=_{i}(_{i}^{l};_{i}^{l-1})\,, _{i}^{l-1}=c_{i}(_{_{i} }^{l})\,.\] (5)

Note that, by restricting each layer such that \(_{^{l}}(^{l})+\), there cannot exist shortcuts at the optima. For example, in Fig. 3, the (indirect) information of \(_{1}\) to generate \(_{3}\) by first generating

Figure 3: Example of the linear SCM \(\{_{1}=_{1}\ ;\ _{2}:=_{1}+ _{2}\ ;\ _{3}:=_{2}+_{3}\}\) written (a) in its usual recursive formulation; (b) without recursions, with each step made explicit; (c) without recursions, as a single function; and (d) writing \(\) as a function of \(\). The red dashed arrows show the influence of \(_{1}\) on \(_{3}\) for all equations from \(\) to \(\), with the compacted version exhibiting shortcuts (see §4). Note that in the linear case we have \(\), and that \(_{1},_{2},_{3}+\) are any three matrices such that their product equals \(^{2}++\).

x\({}_{2}\) needs to go through the middle nodes in Fig. 3(b). However, as shown by Sanchez-Martin et al. , we need at least \(L=()\) layers in our causal NF to avoid shortcuts and thus differentiate between the different direct and indirect causal paths connecting a pair of observed variables.

In contrast, if we only know the causal ordering, then the causal NF will need to rule out the spurious correlations by learning during training the necessary zeroes to fulfil causal consistency (Cor. 2), i.e., such that \(_{}T_{}()+\) and \(_{}T_{}^{-1}()+_{n=1}^{ ()}^{n}\).

Abductive modelReminiscent to the abduction step , another natural choice is to model the inverse equations of the SCM as in Fig. 3(d), hence building a causal NF from \(\) to \(\). Under a known causal graph, we can again use extra masking in the causal NF to force each layer \(l\) to be structurally equivalent to \(-\), such that

\[_{i}^{l}=_{i}(_{i}^{l-1};_{i}^{l})\,, _{i}^{l}=c_{i}(_{_{i}}^{l-1})\,.\] (6)

Remarkably, this architecture is capable of capturing all indirect dependencies of \(\) on \(\), even with a single layer. This is a result of the autoregressive nature of the ANFs used here to build causal NFs, as they compute the inverse sequentially. In the example of Fig. 4, the indirect influence of \(_{1}\) on \(_{3}\) via \(_{2}\) has to necessarily generate \(_{2}\) first (Fig. 3(a)). Similar to the previous architecture, in the absence of a causal graph (i.e., when only the causal ordering is known), the causal NF will need to rely on optimization to discard all spurious correlations.

### Necessary conditions

We next analyse the necessary conditions for the design of a causal NF to be able to accurately approximate and manipulate an SCM. A summary of the analysis can be found in Tab 1.

ExpressivenessThe least restrictive condition is that the causal NF should be able to reach the optima and, as mentioned in SS3, a single ANF layer (Eq. 3) is a universal TMI approximator .

IdentifiabilityIn order to perform interventions as we describe later in SS5, we need the causal NF to isolate the exogenous variables, so that we can associate them with their respective endogenous variables. As we saw in SS3, if the causal NF is expressive enough, and _if it follows a valid causal ordering w.r.t. the true causal graph \(\)_, then Thm. 1 ensures that we can isolate the exogenous variables up to elementwise transformations.

Causal consistencyAs stated in Cor. 2, the causal NF needs to share the causal dependencies of the SCM at the optima, meaning that their Jacobian matrices need to be structurally equivalent, i.e., \(_{}T_{}()-\) (Fig. 3(d)), and \(_{}T_{}^{-1}()_{n=1}^{ ()}^{n}+\) (Fig. 3(c)). Given the (partial) causal graph \(\), the generative model in Eq. 5 by design holds \(_{}T_{}^{-1}()_{n=1}^{ ()}^{n}+\) for any sufficient number of layers \(L\) (see [34, Prop. 1]), however, there might still exist spurious paths from \(\) to \(\). Similarly, the abductive model in Eq. 6, while may not remove all spurious paths from \(\) to \(\) if \(L>1\), ensures causal consistency when \(L=1\). In cases where the selected architecture for the causal NF does not ensure causal consistency by design, but we have access to the causal graph, we can use this extra information to regularize our MLE problem as

\[*{minimize}_{}_{}[- p (T_{}())+\|_{}T_{}()(-)\|_{2}]\,,\] (7)

where \(\) is a matrix of ones, thus penalizing spurious correlations from \(\) to \(T_{}()\).

Tab 1 summarizes the discussed properties of the considered design choices. Remarkably, the abductive model with a single layer (similar to Fig. 3(d)) _enjoys all the necessary properties of a causal NF by design_. That is, the abductive model with \(L=1\) is expressive, and causally consistent w.r.t. the provided causal graph \(\), greatly simplifying the optimization process.

**Remark.** It is not straightforward why abductive models can be causally consistent by design, but generative models cannot. The answer lies on the structure of the problem. Intuitively, this is a consequence of the mapping \(\) being structurally sparser (\(_{i}\) depends on \(_{i}\), see Fig. 3(d)) than that of \(\) (\(_{i}\) depends on \(_{i}\), see Fig. 3(c)). Therefore, ensuring causal consistency from \(\) to \(\) does not necessarily imply causal consistency from \(\) to \(\).

## 5 Do-operator: enabling interventions and counterfactuals

In this section, we propose an implementation of the _do-operator_ well-suited for causal NFs, such that we can evaluate the effect of interventions and counterfactuals . The _do-operator_, denoted as \(do(_{i}=)\), is a mathematical operator that simulates a physical intervention on an SCM \(\), inducing an alternative model \(^{}\) that fixes the observational value \(_{i}=\), and thus removes any causal dependency on \(_{i}\). Usually, the do-operator is implemented by yielding an SCM \(^{}=(}^{},P_{})\) result of replacing the \(i\)-th component of \(}\) with a constant function, \(^{}_{i}\). Unfortunately, this implementation of the do-operator only works for the recursive representation of the SCM (Fig. 3(a)), thus not generalizing to the different architecture designs of causal NFs discussed in SS4 the previous section.

We instead propose to manipulate the SCM by modifying the exogenous distribution \(P_{}\), while keeping the causal equations \(}\) untouched. Specifically, an intervention \(do(_{i}=)\) updates \(P_{}\), restricting the set of plausible \(\) to those that yield the intervened value \(\). We define the intervened SCM as \(^{}=(},P^{}_{})\), where the density of \(P^{}_{}\) is of the form

\[p^{}()=(\{_{i}(_{ _{i}},_{i})=\})_{j i}p_{ j}(_{j}),\] (8)

and where \(\) is the Dirac delta located at the unique value of \(_{i}\) that yields \(_{i}=\) after applying the causal mechanism \(_{i}\). This approach resembles the one proposed for soft interventions  and backtracking counterfactuals . Moreover, as shown in App. C, it can be generalized for non-bijective causal equations. Note also that Eq. 8 is well-defined only if the set \(\{ P_{}\,|\,}_{i}(,)=\}\) is non-empty, i.e., if the intervened variable takes a _plausible_ value (i.e., with positive density in the original causal model). Remarkably, this implementation works directly on the distribution of the exogenous variables and can be applied to any SCM representation (see Fig. 3(d)), and therefore any of the architectures for the causal NF in SS4.

Implementation detailsWe take advantage of the autoregressive nature of causal NFs, and generate samples from Eq. 8 by: i) obtaining the exogenous variables \( T_{}()\); ii) replacing the observational value by its intervened value, \(_{i}\); and iii) by setting \(_{i}\) to the value of the \(i\)-th component of \(T_{}()\). If the causal NF has successfully isolated the exogenous variables (SS3), and it preserves the true causal paths (SS4), then the causal NF ensures that \(_{i}=\) independently of the value of its ancestors, since \(_{i}\) can be seen in Eq. 8 as a deterministic function of the given \(\) and the value of its parents (and therefore its ancestors). We provide further details and the step-by-step algorithms to compute interventions and counterfactuals with causal NFs in App. C.

## 6 Empirical evaluation

In this section, we empirically validate the insights from SS4, and compare causal NFs with previous works. Additional results and in-depth descriptions can be found in App. D.

    &  &  \\   & Causal & Causal Consistency &  &  \\    & & \(\) & & \(\) & \\  \)} & Generative & Ordering & ✗ & ✗ & \((L)\) & \((dL)\) \\  & Generative & Graph \(\) & ✓ & ✗ & \((L)\) & \((dL)\) \\  & Abductive & Ordering & ✗ & ✗ & \((dL)\) & \((L)\) \\  & Abductive (\(L>1\)) & Graph \(\) & ✗ & ✗ & \((dL)\) & \((L)\) \\  & Abductive (\(L=1\)) & Graph \(\) & ✓ & ✓ & \((dL)\) & \((L)\) \\   

Table 1: Summary of the considered design choices, their induced properties, and their time complexity for density evaluation and sampling. Generative models design their forward pass as \(\), and abductive models as \(\). See §4 for an in-depth discussion.

### Ablation study

Experimental setupWe evaluate every network combination described in Tab 1 on a 4-chain SCM (which has diameter 3 and a very sparse Jacobian) and assess the extent to which these models: i) capture the observational distribution, using \((p_{}\|\,p_{})\); ii) remain causally consistent w.r.t. the original SCM, measured via \((_{}T_{}())||_{ }T_{}()(-)||_{2}\) from Eq. 7; and iii) perform at interventional tasks, such as estimating the Average Treatment Effect (ATE)  and computing counterfactuals, both of which we measure with the RMSE w.r.t. the original SCM. Every experiment is repeated 5 times, and every causal NF uses Mask Autoregressive Flows (MAFs)  as layers.

ResultsFig. 5 shows the result for different design choices of causal NFs. Specifically, we show: network design (generative \(\) vs. abductive \(\)), causal knowledge (ordering vs. graph), number of layers \(L\), and whether to use MLE with regularization (Eq. 4 vs. 7).

First, we see in Fig. 4(a) (top) that, as expected, the generative models (\(\)) using the causal graph cannot capture the SCM with \(L<=3\). Furthermore, we observe that abductive models \(\) (Fig. 4(a), bottom) accurately fit the observational distribution, and that embedding the causal graph in the architecture significantly improves the ATE estimation.

Second, we now compare the two network designs in Fig. 4(b), and observe that in general abductive models results in more accurate estimates of the observational distribution, as well as of interventional and counterfactual queries. Finally, we observe that regularization works well in all cases, yet it renders useless for the abductive model with \(L=1\) and knowledge on the graph, since it is causally consistent by design. In summary, our experiments confirm that, despite its simplicity, the causal abductive model with \(L=1\) outperforms the rest of design choices. As a consequence, in the following sections we will stick to this particular design choice, and refer to it as causal NF.

### Non-linear SCMs

Experimental setupWe compare our causal NF (causal, abductive, and with \(L=1\)) with two relevant works: i) CAREFL , an abductive NF with knowledge on the causal ordering and affine layers; and ii) VACA , a variational auto-encoding GNN with knowledge on the graph. For fair comparison, every model uses the same budget for hyperparameter tuning, our causal NF uses affine layers, and CAREFL has been modified to use the proposed do-operator from SS5 (as the original implementation only works in root nodes). We increase the complexity of the SCMs and consider: i) Triangle, a 3-node SCM with a dense causal graph; ii) LargeBD , a 9-node SCM with non-Gaussian \(P_{}\) and made out of two chains with common initial and final nodes; and iii) Simpson, a 4-node SCM simulating a Simpson's paradox , where the relation between two variables changes if the SCM is not properly approximated.

ResultsThe results are summarized in Tab 2. In a nutshell: _the proposed causal NF outperforms both CAREFL and VACA in terms of performance and computational efficiency._ VACA shows poor performance, and is considerably slower due to the complexity of GNNs. Our causal NF outperforms

Figure 5: Ablation of different choices of the causal NF to be causally consistent, and capture the observational and interventional distributions. The use of regularization on the Jacobian (Eq. 7) is indicated with the \(\) superscript. The abductive causal NF with information on \(\) and \(L=1\) outperforms the rest of models across all metrics, demonstrating its efficacy and simplicity.

CAREFL in counterfactual estimation tasks with identical observational fitting, showing once more the importance of being causally consistent. Even more, our causal NF is also quicker than CAREFL, as best-performing CAREFL architectures have in general more than one layer.

## 7 Use-case: fairness auditing and classification

To show the potential practical impact of our work, we follow the fairness use-case of Sanchez-Martin et al.  on the German Credit dataset --a dataset from the UCI repository where the likelihood of individuals repaying a loan is predicted based on a small set of features, including sensitive attributes such as their sex. Extra details and results appear in App. E.

Experimental setupAs proposed by Chiappa , we use a partial graph which groups the 7 discrete features of the dataset in 4 different blocks with known causal relationships, putting in practice the results from SS3.1. For the causal NF, we use the abductive model with a single non-affine neural spline layer . Our ultimate goal is to train a causal NF that captures well the underlying SCM, and use it to train and evaluate classifiers that predict the (additional) binary feature _credit risk_, while remaining counterfactually fair w.r.t. the binary variable _sex_, \(_{S}\).

In this setting, we call a binary classifier \(:\{0,1\}\) counterfactually fair  if, for all possible factual values \(^{}\), the counterfactual unfairness remains zero. That is, if we have that \(_{^{}}[P((^{}) =1\,|\,do(_{S}=1),^{})-P((^{ })=1\,|\,do(_{S}=0),^{})]=0\), where \(^{}\) is a counterfactual sample coming from the distribution \(P(^{}\,|\,do(_{S}=s),^{})\), for \(s=0,1\).

Following Sanchez-Martin et al. , we audit: a model that takes all observed variables (_full_); an _unaware_ model that leaves the sensitive attribute \(_{S}\) out; a fair model that only considers non-descendant variables of \(_{S}\) (_fair_\(\)); and, to demonstrate the ability to learn a counterfactually fair classifier, we include a classifier that takes \(=T_{}()\) as input, but leaves \(_{S}\) out (_fair_\(\)).

ResultsTab 3 summarizes the performance and unfairness of the classifiers, using logistic regression  and SVMs . Here, we observe that by taking the non-sensitive exogenous variables from the causal NF, the obtained classifiers achieve comparable or better accuracy than the rest of the classifiers, while at the same time being counterfactually fair. Moreover, the estimations of unfairness obtained with the causal NF match our expectations , with _full_ being the most unfair, followed by _aware_ and the two fair models. With this use-case, we demonstrate that _Causal NFs may indeed be a valuable asset for real-world causal inference problems_.

    & &  & \))} \\  Dataset & Model & \(\) & \(_{}\) & \(_{}\) & Training & Evaluation & Sampling \\  Triangle & Causal NF & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) \\ Niln & CAREFL\({}^{}\) & \(}\) & \(}\) & \(0.17_{0.03}\) & \(}\) & \(}\) & \(}\) \\
 & VACA & \(7.71_{0.60}\) & \(4.78_{0.01}\) & \(4.19_{0.04}\) & \(28.82_{1.21}\) & \(23.00_{0.55}\) & \(70.65_{3.70}\) \\  LargeBD & Causal NF & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) \\ Niln & CAREFL\({}^{}\) & \(1.51_{0.05}\) & \(0.05_{0.01}\) & \(0.08_{0.01}\) & \(0.84_{0.47}\) & \(1.18_{0.17}\) & \(8.25_{1.29}\) \\
 & VACA & \(53.66_{2.07}\) & \(0.39_{0.00}\) & \(0.82_{0.02}\) & \(164.92_{11.10}\) & \(137.88_{15.72}\) & \(167.94_{25.75}\) \\  Simpson & Causal NF & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) \\ symprod & CAREFL\({}^{}\) & \(}\) & \(0.10_{0.02}\) & \(0.17_{0.04}\) & \(}\) & \(0.81_{0.19}\) & \(1.91_{0.33}\) \\
 & VACA & \(13.85_{0.64}\) & \(0.89_{0.00}\) & \(1.50_{0.04}\) & \(49.26_{0.49}\) & \(37.78_{3.41}\) & \(79.20_{14.60}\) \\   

Table 2: Comparison, on three non-linear SCMs, of the proposed causal NF, VACA , and CAREFL  with the do-operator proposed in §5. Results averaged over five runs.

    &  &  \\   & full & unaware & fair \(\) & fair \(\) & full & unaware & fair \(\) & fair \(\) \\  f1 & \(72.28_{6.16}\) & \(72.37_{4.90}\) & \(59.66_{5.57}\) & \(73.08_{4.38}\) & \(76.04_{2.86}\) & \(76.80_{5.82}\) & \(68.28_{5.74}\) & \(77.39_{1.52}\) \\ accuracy & \(67.00_{3.83}\) & \(66.75_{2.63}\) & \(54.75_{5.91}\) & \(66.50_{3.70}\) & \(69.50_{3.11}\) & \(71.00_{3.83}\) & \(59.25_{2.99}\) & \(69.75_{1.26}\) \\ unfairness & \(5.84_{2.93}\) & \(2.81_{0.72}\) & \(0.00_{0.00}\) & \(0.00_{0.00}\) & \(6.65_{2.45}\) & \(2.78_{0.40}\) & \(0.00_{0.00}\) & \(0.00_{0.00}\) \\   

Table 3: Accuracy, F1-score, and counterfactual unfairness of the audited classifiers. Causal NFs enable both fair classifiers and accurate unfairness metrics. Results are averaged on five runs.

Concluding remarks

In this work, we have shown--both theoretically and empirically--that causal NFs are a natural choice to learn a broad class of causal data-generating processes in a principled way. Specifically, we have proven that causal NFs can match the observational distribution of an underlying SCM, and that in doing so the ANF needs to be causally consistent. However, as limited data availability and local optima may hamper reaching these solutions in practice, we have explored different network designs, exploiting the available knowledge on the causal graph. Moreover, we have provided causal NFs with a do-operator to efficiently solve causal inference tasks. Finally, we have empirically validated our findings, and demonstrated that our causal NF framework: i) outperforms competing methods; and ii) can deal with mixed data and partial knowledge on the causal graph.

Practical limitationsDespite considering a broad class of SCMs, we have made several assumptions that, while being standard, may not hold in some application scenarios. With regard to our causal assumptions, the presence of unmeasured hidden confounders may break our causal sufficiency assumption; mismatches between the true causal graph (e.g., it may contain cycles) and our assumed graph/ordering may lead to poor estimates of interventional and counterfactual queries; and the non-bijective true causal dependencies may invalidate our theoretical and thus practical findings. Besides, we have focused on MLE estimation for learning the causal NF. However, MLE does not test the independency of the exogenous variables during training, which would also break our causal sufficiency assumption.

Future workWe firmly believe that our work opens a number of interesting directions to explore. Naturally, we would like to address current limitations by, e.g., using interventional data to address the existence of hidden confounders [14; 23], explore alternative losses other than MLE (e.g., flow matching ). Moreover, it would be exciting to see causal NFs applied to other problems such as causal discovery , fair decision-making , or neuroimaging , among others. However, we would like to stress that, in the above contexts, it would be essential to validate the suitability of our framework (e.g., using experimental data) to prevent potential harms.