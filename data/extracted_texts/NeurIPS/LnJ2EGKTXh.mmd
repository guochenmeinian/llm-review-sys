# Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning CodeLLMs

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Large language models (LLMs) have shown great promise at generating robot programs from natural language given domain-specific robot application programming interfaces (APIs). However, the performance gap between proprietary LLMs and smaller open-weight LLMs remains wide. This raises a question: Can we fine-tune smaller open-weight LLMs for generating domain-specific robot programs to close the performance gap with proprietary LLMs? While Self-Instruct is a promising solution by generating a diverse set of training data, it cannot verify the _correctness_ of these programs. In contrast, a robot simulator with a well-defined world can identify execution errors but limits the diversity of programs that it can verify. In this work, we introduce Robo-Instruct, which brings the best of both worlds -- it promotes the diversity of Self-Instruct, while providing correctness of simulator-based checking. Robo-Instruct introduces RoboSim to synthesize a _consistent_ world state _on the fly_ by inferring properties relevant to the program being checked, and simulating actions accordingly. Furthermore, the instructions and programs generated by Self-Instruct may be subtly inconsistent -- such as the program missing a step implied by the instruction. Robo-Instruct further addresses this with InstAlign, an instruction-program alignment procedure that revises the task instruction to reflect actual results of the generated program. Given a few seed task descriptions and the robot APIs, Robo-Instruct is capable of generating a training dataset using only a small open-weight model. This dataset is then be used to fine-tune small open-weight language models, enabling them to even exceed the performance of several proprietary LLMs including GPT-3.5-Turbo and Gemini-Pro.

## 1 Introduction

Large language models (LLMs) have demonstrated great promise at generating robot programs from natural language instructions [3; 10; 11; 12; 17; 18; 31; 39]. For example, consider an instruction for a service mobile robot: _"Check how many conference rooms have no markers."_ The robot may be equipped with a domain-specific robot application programming interface (API) that includes skills such as go_to(location) for navigation and is_in_room(object) for perception. Since such domain-specific APIs do not exist in the training dataset of general-purpose LLMs, in-context learning (ICL) via few-shot examples is often employed to describe and use such APIs for performing few-shot inference. However, there is a significant performance gap  in the correctness of programs generated by ICL for large proprietary models and smaller open-weight models that can be deployed locally on robots. This raises a question: can we fine-tune _small open-weight LLMs_ for generating domain-specific robot programs to close the performance gap with proprietary LLMs?Since training datasets of the domain-specific robot programs are often unavailable, Self-Instruct might seem like a promising solution . Consider the setting of generating programs for service mobile robots that can perceive objects, navigate to various locations, manipulate items, and communicate with humans. By formulating these robot skills into APIs, we can create a few seed task examples demonstrating their use case and employ Self-Instruct to generate a diverse set of instruction-program pairs as training data, as illustrated in Fig. 1. However, using Self-Instruct already may generate infeasible instructions--e.g., asking the robot to pick up multiple objects at once when it cannot due to physical constraints. They can also violate domain-specific constraints. For example, in Fig. 1, after line 2 confirms the absence of a key at the current location, line 3 erroneously attempts to pick up a key. Further, these instructions may not align with the generated programs, even if these programs are valid. For example, Fig. 1 shows an example instruction directing the robot to _verbally ask_ in each room if a key exists, whereas the program instructs the robot to _visually check_ in each room. Finally, the generated programs may have execution errors. These challenges may appear to be solvable using a simulator, but a simulator needs an initial world state to check against programs. A simulator using a hand-curated world state will end up rejecting the wide diversity of programs generated by Self-Instruct, even if they are executable, just because the world state did not capture some aspect relevant to them (_e.g.,_ the presence of a "key").

This work introduces Robo-Instruct, a new framework based on Self-Instruct, to address these issues and improve the performance of small open-weight language models for generating domain-specific robot programs. As shown in Fig. 1, Robo-Instruct introduces two novel components: **(1)** RoboSim, a task-agnostic simulator that encodes domain-specific constraints and validates robot programs generated from Self-Instruct. Critically, RoboSim_dynamically_ synthesizes a _consistent_ world state starting from arbitrary programs. **(2)** Instalign, an instruction-program alignment procedure that revises the generated instructions to better reflect the intent of the generated programs. Robo-Instruct also employs a rejection-sampling mechanism that rejects invalid programs detected by RoboSim and queries Self-Instruct for a new program corresponding to the same generated instruction.

We validate Robo-Instruct by fine-tuning Codellama-Python-7B  and evaluate on RoboEval, a domain-specific code generation benchmark for service mobile robots. We show that Robo-Instruct is capable of improving the performance of the Codellama model by using only a small open-weight model to generate the training dataset. Compared to the base Codellama-Python-7B model without fine-tuning, our Robo-Instruct fine-tuned models outperform by \(28.75\%\) in average pass@1 scores; and, compared to Self-Instruct fine-tuned model, our model outperform by \(13.75\%\).; and the best pass@1 of Robo-Instruct fine-tuned model achieves a 68.75% match, surpassing the performance of the proprietary GPT-3.5-Turbo and Gemini-1.0-Pro.

Figure 1: High-Level Overview of Robo-Instruct. This figure also illustrates an example of an invalid Self-Instruct-generated instruction and program, as well as pass@1 results of different LLMs on RoboEval.

ContributionsOur main contributions are as follows:

1. We introduce Robo-Instruct, a new framework for improving the code generation performance of small open-weight language models for domain-specific robot programs. This framework introduces two novel components, RoboSim and InstAlign.
2. We introduce a _dynamic world synthesis and evaluation_ process for generating relevant world states for automated code checking for diverse, arbitrary tasks in RoboSim.
3. We introduce InstAlign, an _instruction alignment_ procedure to refine instruction-code pairs to improve alignment between instructions and code generated by Self-Instruct.
4. We fine-tune a small open-weight model, Codellama-Python-7B , using Robo-Instruct, and improve its performance to outperform several CodeLLMs, including Deepseek-Coder-33B , and Starcoder2-15B  and two proprietary LLMs, GPT-3.5-Turbo  and Gemini-1.0-Pro  on the RoboEval benchmark.

Our code and data will be released at URL anonymized.

## 2 Robo-Instruct

In this section, we present how Robo-Instruct generates training datasets of domain-specific robot programs. Alg. 1 shows a broad overview of the framework. To add an entry in the training dataset, Self-Instruct first generates an instruction-program pair, \((,)\), from the robot APIs and seed tasks, shown in Appendix A.4. Then, RoboSim dynamically synthesizes a _consistent_ world state _on the fly_ as it executes and validates \(\). If \(\) is invalid, Robo-Instruct employs a rejection-sampling method, which generates a new program \(\) given the same \(\) and evaluates the new \(\) again. This process repeats until \(\) becomes valid or a predefined maximum resampling limit is reached. If the limit is reached, the instruction might be invalid given the domain-specific APIs or too complex to generate a program, so the instruction-program pair is discarded. Finally, if \(\) is valid, InstAlign takes in \((,)\) to revise \(\) to better reflect the intent of \(\) and the aligned instruction and program is saved to the training dataset. In the following subsections, we elaborate on the specific design of each component.

```
0:\(\), \(\) Robot API and seed tasks, Let\(\) Program, \(\) The program begin checked Let\(\) Instruction, \(\) The instruction corresponding to \(\) LetRoboSim:\(\) bool, \(\) Domain-specific task-agnostic simulator LetInstalign:\(\), \(\) Instruction-program alignment model Let\(_{inst}\):\(\), \(\)Self-Instruct instruction generation model Let\(_{code}\):\(\), \(\)Self-Instruct program generation model
1:Initialize:\(=\)\(\) Training dataset
2:Initialize:\(N\)\(\) Training dataset size
3:Initialize:\(m\)\(\) Maximum resampling limit
4:while\(()<N\)do
5:\(_{inst}()\)
6:\(_{code}(,)\)
7:for\(i=1\)to\(m\)do
8:is_program_valid = RoboSim(\(\))\(\) Validate the program
9:ifis_program_valid = False then
10:\(_{code}(,)\)\(\) Rejection-sampling
11:else
12:\(_{}(, ,)\)\(\) Align instruction with the program
13:\((_{},)\)
14:break
15:endif
16:endfor
17:endwhile
18:return\(\) ```

**Algorithm 1** Robo-Instruct: Instruction-Program Generation

### RoboSim: A Task-Agnostic Simulator For Domain-Specific Programs

We present a principled approach to design RoboSim for validating domain-specific robot programs. Alg. 2 illustrates the high-level algorithm used to assess the correctness of a robot program. RoboSim employs the concept of _world state_ to simulate the robot actions directed by a program, ensuring consistent and reliable evaluation. A world state is a symbolic representation of the environment in which the robot operates, and it keeps track of the high-level changes in the robot state and the surrounding environment as the robot performs actions in order. For example, consider a program instruction that commands a robot to check if an apple is nearby. The world state queries the stored information about the surrounding environment, identifies all objects at the robot's current location, and informs the program whether an apple is present.

However, since Self-Instruct generates arbitrary programs based on the provided APIs, RoboSim does not know what a plausible world state relevant to the program would be a priori -- _e.g.,_ reasoning about the existence of an apple in the example program. Thus, we equip RoboSim with the ability to expand the world state as more robot actions are performed. Our approach is inspired by angelic execution , which has previously been used for software verification of programs with partially defined library functions. In our case, instead of partially defined library functions, we have unknown plausible world states. RoboSim_dynamically_ synthesizes and grows a world state based on domain-specific constraints (_e.g.,_ object permanence, robot skills, _etc._) and the execution trace of the program, which allows it to infer a consistent and relevant world state.

Specifically, RoboSim modifies the program to replace all API calls with the DynamicEval function (Alg. 2 line 4) -- when an API function is called during execution, the DynamicEval function is invoked instead.

DynamicEval makes an important extension to the formulation of STRIPS  to integrate with API functions. DynamicEval equips each API function with specific pre-conditions, effects, and return values. The pre-conditions are composed of literals tailored to the function's requirements. For instance, the API function is_in_room('apple'), which determines if an object 'apple' is in the same room as the robot, uses two literals for its pre-condition: robot_at(X) and obj_at(X, 'apple'). Generally, STRIPS assigns one of two possible values to each literal: True if the literal is defined, otherwise False. However, prior to program execution, DynamicEval is unaware of the program-relevant literals. Thus we assign a third value, _undefined_, to such unknown literals. Literals must thus be explicitly defined as either True or False, or they remain undefined if not specified.

Alg. 3 demonstrates how DynamicEval executes an API function and updates the world state. First, it calculates the precondition specified for the function. It then checks each literal in the precondition to see if it is defined. If a literal is undefined, DynamicEval invokes GrowWorld, a stochastic function that assigns a random truth value to the literal and updates the world state accordingly. Finally, DynamicEval proceeds to execute the API function using the current world state, retrieves the return values, and applies the function's effects to update the world state.

Fig. 2 illustrates an example of RoboSim executing a generated program. Initially, RoboSim's world state only specifies the robot's current location, and whether a pie is in the same room as the robot remains undefined (line 2). Therefore, DynamicEval invokes GrowWorld torandomly determine a truth value for the obj_at(start_loc, "pie") literal, leading to two distinct execution paths depicted in light purple and blue. Subsequently, as additional API functions are called, more literals are introduced or updated in the world state to ensure consistent evaluations.

Finally, due to the stochastic nature of DynamicEval, RoboSim must execute the generated program multiple times to validate the program. If all executions are successful, the program is deemed correct (Alg. 2 line 5-11).

### InstAlign: Instruction-Program Alignment Procedure

Given that LLMs are extensively trained in code understanding , InstAlign is a procedure that prompts an LLM to revise \(\) to better reflect the intent of \(\). This procedure involves two steps: first, given \(\) and \(\), InstAlign leverages Chain-of-Thought reasoning  (CoT) to prompt an LLM to generate a revised instruction, \(_{}\); second, InstAlign invokes the LLM again to determine whether \(\) or \(_{}\) is more aligned with \(\)'s intent and output the chosen instruction as \(_{}\).

To generate \(_{}\), the prompt to the LLM comprises the robot API function definitions, \(\), \(\), and CoT instructions. The CoT asks the LLM to perform the following three steps in order: 1. write down all the robot APIs used in the program; 2. examine these APIs and write down step by step what the program does; 3. combine all the information above to revise the robot instruction. Similarly, to determine \(_{}\), an LLM is prompted to think step by step about \(\), \(\) and \(_{}\) to arrive at a conclusion. Detailed prompt is shown in Appendix A.6.

## 3 Analysis and Experiments

In this section, we investigate the following two research questions:

Figure 2: Example of RoboSim executing a generated program and updating the world state. Initially, RoboSim begins with a world state that includes only the robotâ€™s current location. As the program executes, two distinct execution paths emerge, depicted in light purple and blue. This figure demonstrates how the world state is updated along each execution path.

1. Is Robo-Instruct effective at generating training data to fine-tune a small language model for generating domain-specific robot programs?
2. How do RoboSim and InstAlign impact the effectiveness of Robo-Instruct?

We conduct our investigation by fine-tuning the Codellama-Python-7B model  on the synthetic dataset generated by Robo-Instruct and evaluate the fine-tuned model using RoboEval, a domain-specific code generation benchmark for service mobile robots. In the following subsections, we first provide a brief description of RoboEval. Then we present our experimental results addressing the two main research questions. Finally, we offer more analysis of RoboSim, InstAlign, and the synthetic dataset.

### RoboEval: A Domain-Specific Robot Code Generation Benchmark

RoboEval is a domain-specific code generation benchmark, featuring a suite of 16 tasks designed to evaluate the ability of LLMs to understand custom APIs and generate programs for service robots. In this domain, a service robot can perceive objects, navigate to various locations, manipulate items, and communicate with humans. Furthermore, the robot should be capable of basic commonsense reasoning and executing complex tasks that involve conditional and repetitive actions. To facilitate these capabilities, RoboEval defines a set of 8 API functions in Python as skill primitives. Fig. 3 illustrates these function signatures and definitions, alongside an example task instruction and its canonical solution from the benchmark. In addition, unlike other popular code generation benchmark tasks , _the order of the robot's actions is crucial for successfully completing the specified tasks_. For instance, in the task _"bring me a marker from the classroom that does not have a whiteboard,"_ the robot must check each classroom until it finds one without a whiteboard, whereas simply bringing back a marker is insufficient. Hence, RoboEval evaluates the generated program by executing it in a simulator to capture the action traces, which are subsequently validated for sequence correctness using temporal logic.

RQ1: Is Robo-Instruct Effective at Generating Training Data to Fine-Tune a Small Language Model for Generating Domain-Specific Robot Programs?

**Experiment Setup.** We use the open-weight LLM, Llama3-8B-Inst, for Robo-Instruct. To generate a diverse dataset, we employ nucleus sampling for creating instruction-program pairs, setting the temperature \(T=1\) and top \(p=0.95\). The maximum resampling limit is capped at \(3\) to accommodate instructions that initially produce invalid programs. For the LLM used in InstAlign, we empirically adjust the generation temperature to \(T=0.3\) to optimize performance. Furthermore, we assess the edit similarity between token sequences of each instruction pair in the dataset , removing duplicates where the similarity score exceeds 0.6. We use the same setup to generate data via Self-Instruct. Instead of discarding invalid programs, Self-Instruct includes every generated instruction-program pair in the training dataset. Finally, we create two datasets with 5K instruction-program pairs each using Self-Instruct and Robo-Instruct respectively. These datasets are then used to fine-tune the Codellama-Python-7B model. The learning rate is set to be

Figure 3: RoboEval APIs and benchmark task example.

\(3e\)-\(5\) with a warmup ratio of \(3\%\) and a constant lr scheduler. We employ the AdamW optimizer  with an effective batch size of 8, training each model for 5 epochs using a sequence length of 2048 tokens. We train all our models on a single H-100 GPU using unsloth .

**Baselines.** We divide our baseline models into 2 categories: 1) proprietary LLMs, including GPT4 , GPT3.5-Turbo , Gemino-Pro , and 2) open-weight LLMs, including Codellam-Python-7B , Codellam-Python-34B, Starcoder2-33B , Deepseek-Coder-33B , and Llama3-8B-Inst . All the results are evaluated using RoboEval and reported in Tab. 1.

Tab. 1 presents the average pass@1 results for different LLMs on RoboEval, using two different temperature settings for generation: greedy decoding at a temperatures of \(T=0\) and nucleus sampling at a temperature of \(T=0.2\). The results show that Robo-Instruct-fine-tuned Codellama significantly improves upon the base Codellama-Python-7B and outperforms the Self-Instruct-fine-tuned variant. Notably, it surpasses all open-weight models, including larger ones like Codellam-Python-34B and Deepseek-Coder-33B. Additionally, although the training dataset was generated using Llama3-8B-Inst, which scores less than 50% pass@1 on RoboEval, our Robo-Instruct-fine-tuned model still achieves a significant improvement, scoring 68.75% under deterministic temperature settings for generation. Finally, compared to proprietary models, while our Robo-Instruct-fine-tuned model trails the more powerful GPT-4, it outperforms GPT-3.5-Turbo and Gemini-1.0-Pro in generating programs for service mobile robots. This result demonstrates the effectiveness of our approach in generating domain-specific robot program data for fine-tuning a small language model. It suggests that the fine-tuned model could potentially replace some proprietary models, providing a more cost-effective and private option for local deployment.

### RQ2: How Do RoboSim and InstAlign Impact the Effectiveness of Robo-Instruct?

Using the same setup as in the previous section, we investigate the effectiveness of RoboSim and InstAlign. Since Self-Instruct may generate invalid instructions that no corresponding valid program can pass in RoboSim, we propose rejecting these unsolvable instructions (we name

    & & &  \\  Fine-tune & Model & \# Param & \(T=0\) & \(T=0.2\) & Licensing \\  - & GPT-4 & - & **83.75\%** & **85.81\%** & Proprietary \\ - & GPT-3.5 & - & 67.5\% & 65.56\% & Proprietary \\ - & Gemini-1.0-Pro & - & 60.00\% & 59.88\% & Proprietary \\  - & Codellama-Python & 7B & 40.00\% & 39.31\% & Open \\ - & Codellama-Python & 34B & 46.25\% & 48.25\% & Open \\ - & Starcoder2 & 15B & 62.5\% & 60.94\% & Open \\ - & Deepsek-Coder & 33B & 53.75\% & 52.13\% & Open \\ - & Llama3-Inst & 8B & 48.75\% & 48.38\% & Open \\  Self-Instruct & Codellama-Python & 7B & 55.00\% & 52.69\% & Open \\ Robo-Instruct (ours) & Codellama-Python & 7B & **68.75\%** & **66.00\%** & Open \\   

Table 1: Pass@1 results of different LLMs on RoboEval computed with greedy decoding \(T=0\) and nucleus sampling \(T=0.2\).

    &  &  &  \\  Method & pass@1 & Improv. & pass@1 & Improv. & Programs \\  Codellama-7B-Python & 40.00\% & +0\% & 39.31\% & +0\% & 38.31\% \\ Self-Instruct & 55.00\% & +15.00\% & 52.69\% & +13.38\% & 20.94\% \\ +RejectUnsolvable (RU) & 60.00\% & +20.00\% & 57.62\% & +18.31\% & 23.38\% \\ +RoboSim + RU & 63.75\% & +23.75\% & 63.88\% & +24.57\% & **14.13\%** \\ +Instalign +RU & 58.75\% & +18.75\% & 59.81\% & +20.50\% & 23.44\% \\ +Both (Robo-Instruct) & **68.75\%** & **+28.75\%** & **66.00\%** & **+26.69\%** & 17.07\% \\   

Table 2: Pass@1 results of different LLMs on RoboEval computed with greedy decoding \(T=0\) and nucleus sampling \(T=0.2\).

this process RU) to evaluate the upperbound performance of Self-Instruct. Tab. 2 shows the average pass@1 results from Codellama-7B-Python fine-tuned on different datasets generated by each method. First, findings from Self-Instruct \(+\) RU indicate that simply discarding invalid instructions could also improve model performance. Additionally, fine-tuning with a dataset created from Self-Instruct\(+\)RoboSim results in the smallest proportion of invalid program errors. Finally, while incorporating either RoboSim or Instalign individually offers some improvement over the baseline Self-Instruct \(+\) RU results, Robo-Instruct still results in the best performance. This indicates that the integration of these two components is important to the framework's effectiveness.

### Qualitative analysis of the generated program errors

We analyze invalid programs identified by RoboSim, categorizing the errors into two types: language-native errors and domain-specific constraint violations. Fig. 4 displays eight examples of these programs, with Examples 1 to 4 illustrating errors specific to the Python language, and Examples 5 to 8 highlighting errors rooted in domain-specific constraints. Language-native errors are generally straightforward, such as syntax errors, the use of undefined variables or functions, or improper use of provided APIs.

In contrast, errors related to domain-specific constraints tend to be more complex to detect. For instance, Example 5 illustrates the program incorrectly trying to pick up a watering can (line 3) after establishing that it is not present at the location (line 2). Similarly, Example 6 demonstrates an error where the program inappropriately asks Jack (line 5) after confirming his absence from the room

Figure 4: Self-Instruct-Generated Program Errors: Examples 1 to 4 illustrate errors specific to the Python language, and Examples 5 to 8 highlight errors rooted in domain-specific constraints.2

(line 3). Example 7 illustrates a scenario in which RoboSim updates the world state by labeling "item storage room" as a location after executing the go_to command (line 2). Subsequently, the robot attempts to pick up this location (line 3), resulting in an error. Example 9 is the most intricate scenario where the world state in the living room is updated to include a toy after the robot places it there (line 7). When the robot returns to the living room for the second time (line 5), it does not place down what it holds (line 7). Hence, in the third room the robot visits (line 3), when it attempts to pick up a toy again (line 4), an error occurs because the robot can only carry one item at a time.

## 4 Related Work

### LLMs for Robot Code Generation

LLMs have shown impressive capabilities in generating robot programs from natural language [11; 17; 31]. One popular approach uses LLMs to generate composable costmaps for robots to plan their motion on. In this approach, Voxposer  focuses on the tabletop manipulation setting and NavCon  focuses on creating composable maps for navigation. Using LLM to create reward functions is also promising. Eureka [23; 24] and Language to Rewards for Robotic Skill Synthesis  both show that LLM can generate good reward functions that allows robots to acquire complex skills. Finally, LLM can also be used to generate programs for high-level planning. LLM+p  outputs a robot plan in the form of the well-defined planning domain definition language (PDDL). Tidybot  uses an LLM to generate a rule that captures user preferences from examples and executes a program to sequentially complete the task in order. RoboEval  focuses on generating domain-specific programs for service mobile robots. It generates a program that allows the service robot to carry out long-horizon tasks and then validates the correctness of the program.

### Generating Datasets For Fine-tuning LLMs

To enhance LLMs' performance in code generation, numerous studies have explored the creation of specialized datasets [13; 25; 26]. Self-Instruct  is one popular method for generating synthetic datasets using an LLM. Following this methodology, Alpaca  generates 52K instruction-following demonstrations and subsequently fine-tunes the LLaMA 7B model  to create Alpaca 7B, which can behave qualitatively similarly to OpenAI's text-davinci-003. Code Alpaca  extends this approach to generate code instructions using 21 seed tasks, while Gorilla-LM  adapts the method to focus on ML domain-specific APIs from Huggingface, TensorFlow Hub, and Torch Hub. To create more complex instructions, Evol-Instruct [22; 40] proposes iteratively updating instructions to become more complex through different prompting strategies. In addition to Evol-Instruct, OSS-Instruct  uses open-source code snippets to generate 75K high-quality instruction data and fine-tunes the Codelllama-Python-7B model to create Magicoder, which can match the performance of GPT-3.5-Turbo  on HumanEval . While these works focus on creating seed instruction sets to generate synthetic data for effectively fine-tuning an LLM, our research investigates post-processing methods in addition to Self-Instruct. Specifically, we concentrate on generating domain-specific programs in robotics , where we can effectively leverage constraints to filter out erroneous programs.

## 5 Conclusion, Limitation and Future Works

In this work, we introduce Robo-Instruct, a novel framework to generate synthetic training data to fine-tune small language models for domain-specific robot programs. Robo-Instruct comprises two novel components: 1) RoboSim, an angelic-execution-based algorithm to effectively validate Self-Instruct-generated programs, and 2) Instalign, an instruction alignment procedure to revise instructions to better align with the generated programs. The experimental results demonstrate that the Codellama-Python-7B model fine-tuned on the Robo-Instruct-generated dataset can significantly outperform many popular open-weight LLMs for generating domain-specific robot programs. It also outperforms two proprietary LLMs, GPT-3.5-Turbo and Gemino-1.0-Pro, as well as the Self-Instruct-fine-tuned variant. A limitation of this study is that Robo-Instruct relies on Self-Instruct to filter invalid programs, making the dataset quality dependent on Self-Instruct's performance. This can introduce biases if Self-Instruct consistently fails in certain areas. Future work will explore integrating Robo-Instruct with advanced methods like Evol-Inst and OSS-Inst to enhance dataset quality for domain-specific robot programs.