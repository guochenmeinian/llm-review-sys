# Hierarchical clustering with dot products

recovers hidden tree structure

 Annie Gray

School of Mathematics

University of Bristol, UK

annie.gray@bristol.ac.uk &Alexander Modell

Department of Mathematics

Imperial College London, UK

a.modell@imperial.ac.uk &Patrick Rubin-Delanchy

School of Mathematics

University of Bristol, UK

patrick.rubin-delanchy@bristol.ac.uk &Nick Whiteley

School of Mathematics

University of Bristol, UK

nick.whiteley@bristol.ac.uk

###### Abstract

In this paper we offer a new perspective on the well established agglomerative clustering algorithm, focusing on recovery of hierarchical structure. We recommend a simple variant of the standard algorithm, in which clusters are merged by maximum average dot product and not, for example, by minimum distance or within-cluster variance. We demonstrate that the tree output by this algorithm provides a bona fide estimate of generative hierarchical structure in data, under a generic probabilistic graphical model. The key technical innovations are to understand how hierarchical information in this model translates into tree geometry which can be recovered from data, and to characterise the benefits of simultaneously growing sample size and data dimension. We demonstrate superior tree recovery performance with real data over existing approaches such as UPGMA, Ward's method, and HDBSCAN.

## 1 Introduction

Hierarchical structure is known to occur in many natural and man-made systems , and the problem considered in this paper is how to recover this structure from data. Hierarchical clustering algorithms,  are very popular techniques which organise data into nested clusters, used routinely by data scientists and machine learning researchers, and are easily accessible through open source software packages such as scikit-learn . We focus on perhaps the most popular family of such techniques: agglomerative clustering , [24, Ch.3], in which clusters of data points are merged recursively.

Agglomerative clustering methods are not model-based procedures, but rather simple algorithms. Nevertheless, in this work we uncover a new perspective on agglomerative clustering by introducing a general form of generative statistical model for the data, but without assuming specific parametric families of distributions (e.g., Gaussian). In our model (section 2.1), hierarchy takes the form of a tree defining the conditional independence structure of latent variables, using elementary concepts from probabilistic graphical modelling . In a key innovation, we then augment this conditional independence tree to form what we call a _dendrogram_, whose geometry is related to population statistics of the data. The new insight which enables tree recovery in our setting (made precise and explained in section 3) is that _dot products between data vectors reveal heights of most recent common ancestors in the dendrogram_.

We suggest an agglomerative algorithm which merges clusters according to highest sample average dot product (section 2.2). This is in contrast to many existing approaches which quantify dissimilaritybetween data vectors using Euclidean distance. We also consider the case where data are preprocessed by reducing dimension using PCA. We mathematically analyse the performance of our dot product clustering algorithm and establish that under our model, with sample size \(n\) and data dimension \(p\) growing simultaneously at appropriate rates, the merge distortion [20; 27] between the algorithm output and underlying tree vanishes. In numerical examples with real data (section 4), where we have access to labels providing a notion of true hierarchy, we compare performance of our algorithm against existing methods in terms of a Kendall \(_{b}\) correlation performance measure, which quantifies association between ground-truth and estimated tree structure. We examine statistical performance with and without dimension reduction by PCA, and illustrate how dot products versus Euclidean distances relate to semantic structure in ground-truth hierarchy.

Related workAgglomerative clustering methods combine some dissimilarity measure with a 'linkage' function, determining which clusters are combined. Popular special cases include UPGMA  and Ward's method , against which we make numerical comparisons (section 4). Owing to the simple observation that, in general, finding the largest dot product between data vectors is not equivalent to finding the smallest Euclidean distance, we can explain why these existing methods may not correctly recover tree structure under our model (appendix E). Popular density-based clustering methods methods include CURE , OPTICS , BIRCH  and HDBSCAN . In section 4 we discuss the extent to which these methods can and cannot be compared against ours in terms of tree recovery performance.

Existing theoretical treatments of hierarchical clustering involve different mathematical problem formulations and assumptions to ours. One common setup is to assume an underlying ultrametric space whose geometry specifies the unknown tree, and/or to study tree recovery as \(n\) with respect to a cost function, e.g, [10; 16; 43; 11; 14; 15; 31; 12]. An alternative problem formulation addresses recovery of the cluster tree of the probability density from which it is assumed data are sampled [42; 20; 27]. The unknown tree in our problem formulation specifies conditional independence structure, and so it has a very different interpretation to the trees in all these cited works. Moreover, our data are vectors in \(^{p}\), and \(p\) is a crucial aspect of our convergence arguments, but in the above cited works \(p\) plays no role or is fixed. Our definition of dendrogram is different to that in e.g. : we do not require all leaf vertices to be equidistant from the root - a condition which arises as the "fixed molecular clock" hypothesis [18; 34] in phylogenetics. We also allow data to be sampled from non-leaf vertices. There is an enormous body of work on tree reconstruction methods in phylogenetics, e.g. listed at , but these are mostly not general-purpose solutions to the problem of inferring hierarchy. Associated theoretical convergence results are limited in scope, e.g, the famous work  is limited to a fixed molecular clock, five taxa and does not allow observation error.

## 2 Model and algorithm

### Statistical model, tree and dendrogram

Where possible, we use conventional terminology from the field of probabilistic graphical models, e.g.,  to define our objects and concepts. Our model is built around an unobserved tree \(=(,)\), that is a directed acyclic graph with vertex and edge sets \(\) and \(\), with two properties: \(\) is connected (ignoring directions of edges), and each vertex has at most one parent, where we say \(u\) is a parent of \(v\) if there is an edge from \(u\) to \(v\). We observe data vectors \(_{i}^{p}\), \(i=1,,n\), which we model as:

\[_{i}=(Z_{i})+(Z_{i})_{i},\] (1)

comprising three independent sources of randomness:

* \(Z_{1},,Z_{n}\) are i.i.d., discrete random variables, with distribution supported on a subset of vertices \(\), \(||<\);
* \((v)[X_{1}(v)\;\;X_{p}(v)]^{}\) is an \(^{p}\)-valued random vector for each vertex \(v\);
* \(_{1},,_{n}\) are i.i.d, \(^{p}\)-valued random vectors. The elements of \(_{i}\) are i.i.d., zero mean and unit variance. For each \(z\), \((z)^{p p}\), is a deterministic matrix.

For each \(v\), one can think of the vector \((v)\) as the random centre of a "cluster", with correlation structure of the cluster determined by the matrix \((z)\). The latent variable \(Z_{i}\) indicates which cluster the \(i\)th data vector \(_{i}\) is associated with. The vectors \((v)\), \(v\), correspond to unobserved vertices in the underlying tree. As an example, \(\) could be the set of leaf vertices of \(\), but neither our methods nor theory require this to be the case. Throughout this paper, we assume that the tree \(\) determines two distributional properties of \(\). Firstly, we assume \(\) is the conditional independence graph of the collection of random variables \(X_{j}\{X_{j}(v);v\}\) for each \(j\), that is,

**A1**.: _for all \(j=1,,p\), the marginal probability density or mass function of \(X_{j}\) factorises as:_

\[p(x_{j})=_{v}p(x_{j}(v)|x_{j}(_{v})),\]

_where \(_{v}\) denotes the parent of vertex \(v\)._

However we do not necessarily require that \(X_{1},,X_{p}\) are independent or identically distributed. Secondly, we assume

**A2**.: _for each \(j=1\,,p\), the following martingale-like property holds:_

\[[X_{j}(v)|X_{j}(_{v})]=X_{j}(_{v}),\]

_for all vertices \(v\) except the root._

The conditions **A1** and **A2** induce an additive hierarchical structure in \(\). For any distinct vertices \(u,v\) and \(w\) an ancestor of both, **A1** and **A2** imply that given \(X_{j}(w)\), the increments \(X_{j}(u)-X_{j}(w)\) and \(X_{j}(v)-X_{j}(w)\) are conditionally independent and both conditionally mean zero.

To explain our algorithm we need to introduce the definition of a _dendrogram_, \(=(,h)\), where \(h:_{+}\) is a function which assigns a height to each vertex of \(\), such that \(h(v) h(_{v})\) for any vertex \(v\) other than the root. The term "dendrogram" is derived from the ancient Greek for "tree" and "drawing", and indeed the numerical values \(h(v)\), \(v\), can be used to construct a drawing of \(\) where height is measured with respect to some arbitrary baseline on the page, an example is shown in figure 1(a). With \(,\) denoting the usual dot product between vectors, the function

\[(u,v)[(u),(v)], u,v,\] (2)

will act as a measure of affinity underlying our algorithm. The specific height function we consider is \(h(v)(v,v)\). The martingale property **A2** ensures that this height function satisfies \(h(v) h(_{v})\) as required, see lemma 2 in appendix C. In appendix E we also consider cosine similarity as an affinity measure, and analyse its performance under a multiplicative noise model, cf. the additive model (1).

### Algorithm

Combining the model (1) with (2) we have \((Z_{i},Z_{j})=p^{-1}[_{i},_{j} |Z_{i},Z_{j}]\) for \(i j[n]\), \([n]\{1,,n\}\). The input to algorithm 1 is an estimate \((,)\) of all the pairwise affinities \((Z_{i},Z_{j})\), \(i j[n]\). We consider two approaches to estimating \((Z_{i},Z_{j})\), with and without dimension reduction by uncentered PCA. For some \(r\{p,n\}\), let \(^{p r}\) denote the matrix whose columns are orthonormal eigenvectors of \(_{i=1}^{n}_{i}_{i}^{}\) associated with its \(r\) largest eigenvalues. Then \(_{i}^{}_{i}\) is \(r\)-dimensional vector of principal component scores for the \(i\)th data vector. The two possibilities for \((,)\) we consider are:

\[_{}(i,j)_{i}, _{j},_{}(i,j) _{i},_{j}.\] (3)

In the case of \(_{}\) the dimension \(r\) must be chosen. Our theory in section 3.2 assumes that \(r\) is chosen as the rank of the matrix with entries \((u,v),u,v\), which is at most \(||\). In practice \(r\) usually must be chosen based on the data, we discuss this in appendix A.

Algorithm 1 returns a dendrogram \(}=(},)\), comprising a tree \(}=(},})\) and height function \(\). Each vertex in \(}\) is a subset of \([n]\), thus indexing a subset of the data vectors \(_{1},,_{n}\). The leaf vertices are the singleton sets \(\{i\}\), \(i[n]\), corresponding to the data vectors themselves. As algorithm 1 proceeds, vertices are appended to \(}\), edges are appended to \(}\), and the domain of the function \((,)\) is extended as affinities between elements of \(}\) are computed. Throughout the paper we simplify notation by writing \((i,j)\) as shorthand for \((\{i\},\{j\})\) for \(i,j[n]\), noting that each argument of \((,)\) is in fact a subset of \([n]\).

**Implementation using scikit-learn** The presentation of algorithm 1 has been chosen to simplify its theoretical analysis, but alternative formulations of the same method may be much more computationally efficient in practice. In appendix B we outline how algorithm 1 can easily be implemented using the AgglomerativeClustering class in scikit-learn .

## 3 Performance Analysis

### Merge distortion is upper bounded by affinity estimation error

In order to explain the performance of algorithm 1 we introduce the _merge height_ functions:

\[m(u,v)  h(), u,v,\] \[(u,v) (), u,v}.\]

To simplify notation we write \((i,j)\) as shorthand for \((\{i\},\{j\})\), for \(i,j[n]\). The discrepancy between any two dendrograms whose vertices are in correspondence can be quantified by _merge distortion_ - the maximum absolute difference in merge height across all corresponding pairs ofvertices.  advocated merge distortion as a performance measure for cluster-tree recovery, which is different to our model-based formulation, but merge distortion turns out to be a useful and tractable performance measure in our setting too. As a preface to our main theoretical results, lemma 1 explains how the geometry of the dendrogram \(\), in terms of merge heights, is related to population statistics of our model. Defining \(d(u,v) h(u)-h(w)+h(v)-h(w)\) for \(u v\), where \(w\) is the most recent common ancestor of \(u\) and \(v\), we see \(d(u,v)\) is the vertical distance on the dendrogram from \(u\) down to \(w\) then back up to \(v\), as illustrated in figure 1(a).

**Lemma 1**.: _For any two vertices \(u,v\),_

\[m(u,v)=[(u),(v) ]=(u,v), d(u,v)=[\|( u)-(v)\|^{2}].\] (4)

The proof is in appendix C. Considering the first two equalities in (4), it is natural to ask if the estimated affinities \((,)\) being close to the true affinities \((,)\) implies a small merge distortion between \((,)\) and \(m(,)\). This is the subject of our first main result, theorem 1 below. In appendix E we use the third equality in (4) to explain why popular agglomerative techniques such as UPGMA  and Ward's method  which merge clusters based on proximity in Euclidean distance may enjoy limited success under our model, but in general do not correctly recover tree structure.

Let \(b\) denote the minimum branch length of \(\), that is, \(b=\{h(v)-h(_{v})\}\), where the minimum is taken over all vertices in \(\) except the root.

**Theorem 1**.: _Let the function \((,)\) given as input to algorithm 1 be real-valued and symmetric but otherwise arbitrary. For any \(z_{1},,z_{n}\), if_

\[_{i,j[n],i j}|(z_{i},z_{j})-(i,j)|<b/2,\]

_then the dendrogram returned by algorithm 1 satisfies_

\[_{i,j[n],i j}|m(z_{i},z_{j})-(i,j)|_{i,j[n],i j }|(z_{i},z_{j})-(i,j)|.\] (5)

The proof is in appendix C.

### Affinity estimation error vanishes with increasing dimension and sample size

Our second main result, theorem 2 below, concerns the accuracy of estimating the affinities \((,)\) using \(_{}\) or \(_{}\) as defined in (3). We shall consider the following technical assumptions.

**A3** (Mixing across dimensions).: _For mixing coefficients \(\) satisfying \(_{k 1}^{1/2}(k)<\) and all \(u,v\), the sequence \(\{(X_{j}(u),X_{j}(v));j 1\}\) is \(\)-mixing._

**A4** (Bounded moments).: _For some \(q 2\), \(_{j 1}_{v}[|X_{j}(v)|^{2q}]<\) and \([|_{11}|^{2q}]<\), where \(_{11}\) is the first element of the vector \(_{1}\)._

**A5** (Disturbance control).: \(_{v}\|(v)\|_{} O(1)\) _as \(p\), where \(\|\|_{}\) is the spectral norm._

**A6** (PCA rank).: _The dimension \(r\) chosen in definition of \(_{}\), see (3), is equal to the rank of the matrix with entries \((u,v)\), \(u,v\)._

The concept of \(\)-mixing is a classical weak-dependence condition, e.g. . **A3** implies that for each \(j 1\), \((X_{j}(u),X_{j}(v))\) and \((X_{j+}(u),X_{j+}(v))\) are asymptotically independent as \(\). However, it is important to note that \(_{}\) and \(_{}\) in (3), and hence the operation of algorithm 1 with these inputs, are invariant to permutation of the data dimensions \(j=1,,p\). Thus our analysis under **A3** only requires there is _some_ permutation of dimensions under which \(\)-mixing holds. **A4** is a fairly mild integrability condition. **A5** allows control of magnitudes of the "disturbance" vectors \(_{i}-(Z_{i})=(Z_{i})_{i}\). Further background and discussion of assumptions is given in appendix C.2.

**Theorem 2**.: _Assume that **A1**-**A5** hold and let \(q\) be as in **A4**. Then_

\[_{i,j[n],i j}|(Z_{i},Z_{j})-_{}(i,j)|  O_{}(}{}).\] (6)

_If additionally **A3** is strengthened from \(\)-mixing to independence, \((v)=_{p}\) for some constant \( 0\) and all \(v\) (in which case **A5** holds), and **A6** holds, then_

\[_{i,j[n],i j}|(Z_{i},Z_{j})-_{}(i,j)|  O_{}(}+}).\] (7)The proof of theorem 2 is in appendix C.2. We give an original and self-contained proof of (6). To prove (7) we use a recent uniform-concentration result for principal component scores from . Overall, theorem 2 says that affinity estimation error vanishes if the dimension \(p\) grows faster enough relative to \(n\) (and \(r\) in the case of \(_{}\), noting that under **A6**, \(r||\), so it is sensible to assume \(r\) is much smaller than \(n\) and \(p\)). The argument of \(O_{}()\) is the convergence rate; if \((X_{p,n})\) is some collection of random variables indexed by \(p\) and \(n\), \(X_{p,n} O_{}(n^{2/q}/)\) means that for any \(>0\), there exists \(\) and \(M\) such that \(n^{2/q}/ M\) implies \((|X_{p,n}|>)<\). We note the convergence rate in (6) is decreasing in \(q\) where as rate in (7) is not. It is an open mathematical question whether (7) can be improved in this regard; sharpening the results of Whiteley et al.  used in the proof of (7) seems very challenging. However, when \(q=2\), (6) gives \(O_{}(n/)\) compared to \(O_{}(+)\) in (7), i.e. an improvement from \(n\) to \(\) in the first term. We explore empirical performance of \(_{}\) versus \(_{}\) in section 4.2.

### Interpretation

By combining theorems 1 and 2, we find that when \(b>0\) is constant and \(\) is either \(_{}\) or \(_{}\), the merge distortion

\[_{i,j[n],i j}|m(Z_{i},Z_{j})-(i,j)|\] (8)

converges to zero at rates given by the r.h.s of (6) and (7). To gain intuition into what (8) tells us about the resemblance between \(}\) and \(\), it is useful to consider an intermediate dendrogram illustrated in figure 1(b) which conveys the realized values of \(Z_{1},,Z_{n}\). This dendrogram is constructed from \(\) by adding a leaf vertex corresponding to each observation \(_{i}\), with parent \(Z_{i}\) and height \(p^{-1}[\|_{i}\|^{2}|Z_{1},,Z_{n}]=h(Z_{i})+p^{-1} [(Z_{i})^{}(Z_{i})]\), and deleting any \(v\) such that \(Z_{i} v\) for all \(i[n]\) (e.g., vertex \(c\) in figure 1(b)). The resulting merge height between the vertices corresponding to \(_{i}\) and \(_{j}\) is \(m(Z_{i},Z_{j})\). (8) being small implies this must be close to \((i,j)\) in \(}\) as in figure 1(c). Moreover, in the case \(=_{}\), the height \((\{i\})\) of leaf vertex \(\{i\}\) in \(}\) is upper bounded by \(p^{-1}\|_{i}\|^{2}\), which under our statistical assumptions is concentrated around \(p^{-1}[\|_{i}\|^{2}|Z_{1},,Z_{n}]\), i.e., the heights of the leaves in figure 1(c) approximate those of the corresponding leaves in figure 1(b).

Overall we see that \(}\) in figure 1(c) approximates the dendrogram in figure 1(b), and in turn \(\). However even if \(m(Z_{i},Z_{j})=(i,j)\) for all \(i,j\), the tree output from algorithm 1, \(}\), may not be isomorphic (i.e., equivalent up to relabelling of vertices) to the tree in figure 1(b); \(}\) is always binary and has \(2n-1\) vertices, whereas the tree in figure 1(b) may not be binary, depending on the underlying \(\) and the realization of \(Z_{1},,Z_{n}\). This reflects the fact that merge distortion, in general, is a _pseudometric_ on dendrograms. However, if one restricts attention to specific classes of true dendrograms \(\), for instance binary trees with non-zero branch lengths, then asymptotically algorithm 1 can recover them exactly. We explain this point further in appendix C.3.

## 4 Numerical experiments

We explore the numerical performance of algorithm 1 in the setting of five data sets summarised below. The real datasets used are open source, and full details of data preparation and sources are given in appendix D.

**Simulated data.** A simple tree structure with vertices \(=\{1,2,3,4,5,6,7,8\}\), edge set \(=\{6{}1,6{}2,6{}3,7{ }4,7{}5,8{}6,8{}7\}\) and \(=\{1,2,3,4,5\}\) (the leaf vertices). \(Z_{1},,Z_{n}\) are drawn from the uniform distribution on \(\). The \(X_{j}(v)\) are Gaussian random variables, independent across \(j\). Full details of how these variables are sampled are in appendix D. The elements of \(_{i}\) are standard Gaussian, and \((v)=_{p}\) with \(=1\).

**20 Newsgroups.** We used a random subsample of \(n=5000\) documents from the well-known 20 Newsgroups data set . Each data vector corresponds to one document, capturing its \(p=12818\) Term Frequency Inverse Document Frequency features. The value of \(n\) was chosen to put us in the regime \(p n\), to which our theory is relevant - see section 3.2. Some ground-truth labelling of documents is known: each document is associated with \(1\) of \(20\) newsgroup topics, organized at two hierarchical levels.

**Zebrafish gene counts.** These data comprise gene counts in zebrafish embryo cells taken from their first day of development . As embryos develop, cells differentiate into various types with specialised, distinct functions, so the data are expected to exhibit tree-like structure mapping these changes. We used a subsample such that \(n=5079\) and \(p=5498\) to put us in the \(p n\) regime. Each cell has two labels: the tissue that the cell is from and a subcategory of this.

**Amazon reviews.** This dataset contains customer reviews on Amazon products . A random sample of \(n=5000\) is taken and each data vector corresponds to one review with \(p=5594\) Term Frequency Inverse Document Frequency features. Each product reviewed has labels which make up a three-level hierarchy of product types.

**S&P 500 stock returns.** The data are \(p=1259\) daily returns between for \(n=368\) stocks which were constituents of the S&P 500 market index  between \(2013\) to \(2018\). The two-level hierarchy of stock sectors by industries and sub-industries follows the Global Industry Classification Standard .

### Comparing algorithm 1 to existing methods

We numerically compare algorithm 1 against three very popular variants of agglomerative clustering: UPGMA with Euclidean distance, Ward's method, and UPGMA with cosine distance. These are natural comparators because they work by iteratively merging clusters in a manner similar to algorithm 1, but using different criteria for choosing which clusters to merge. In appendix E we complement our numerical results with mathematical insights into how these methods perform under our modelling assumptions. Numerical results for other linkage functions and distances are given in appendix D. Several popular density-based clustering methods use some hierarchical structure, such as CURE , OPTICS  and BIRCH  but these have limitations which prevent direct comparisons: they aren't equipped with a way to simplify the structure into a tree, which it is our aim to recover, and only suggest extracting a flat partition based on a density threshold. HDBSCAN  is a density-based method that doesn't have these limitations, and we report numerical comparisons against it.

**Kendall \(_{b}\) ranking correlation.** For real data some ground-truth hierarchical labelling may be available but ground-truth merge heights usually are not. We need a performance measure to quantitatively compare methods operating on such data. Commonly used clustering performance measures such as the Rand index  and others [23; 21] allow pairwise comparisons between partitions, but do not capture information about hierarchical structure. The cophenetic correlation coefficient  is commonly used to compare dendrograms, but relies on an assumption that points close in Euclidean distance should be considered similar which is incompatible with our notion of dot product affinity. To overcome these obstacles we formulate a performance measure as follows. For each of \(n\) data points, we rank the other \(n-1\) data points according to the order in which they merge with it in the ground-truth hierarchy. We then compare these ground truth rankings to those obtained from a given hierarchical clustering algorithm using the Kendall \(_{b}\) correlation coefficient . This outputs a value in the interval \([-1,1]\), with \(-1\), \(1\) and \(0\) corresponding to negative, positive and lack of association between the ground-truth and algorithm-derived rankings. We report the mean association value across all \(n\) data points as the overall performance measure. Table 1 shows results with raw data vectors \(_{1:n}\) or PC scores \(_{1:n}\) taken as input to the various algorithms. For all the data sets except S&P 500, algorithm 1 is found to recover hierarchy more accurately than other methods. We include the results for the S&P 500 data to give a balanced scientific view, and in appendix E we discuss why our modelling assumptions may not be appropriate for these data, thus explaining the limitations of algorithm 1.

### Simulation study of dot product estimation with and without PCA dimension reduction

For high-dimensional data, reducing dimension with PCA prior to clustering may reduce overall computational cost. Assuming \(_{1:n}\) are obtained from, e.g., a partial SVD, in time \(O(npr)\), the time complexity of evaluating \(_{}\) is \(O(npr+n^{2}r)\), versus \(O(n^{2}p)\) for \(_{}\), although this ignores the cost of choosing \(r\). In table 1 we see for algorithm 1, the results for input \(_{1:n}\) are very similar to those for \(_{1:n}\). To examine this more closely and connect our findings to theorem 2, we now compare \(_{}\) and \(_{}\) as estimates of \(\) through simulation. The model is as described at the start of section 4. In figure 2(a)-(b), we see that when \(p\) is growing with \(n\), and when \(p\) is constant, the \(_{}\) error is very slightly smaller than the \(_{}\) error. By contrast, in figure 2(c), when \(n=10\) isfixed, we see that the \(_{}\) error is larger than that for \(_{}\). This inferior performance of \(_{}\) for very small and fixed \(n\) is explained by \(n\) appearing in the denominator of the second term in the rate \(O_{}(+)\) for \(_{}\) in theorem 2 versus \(n\) appearing only in the numerator of \(O_{}(n^{2/q}/)\) for \(_{}\). Since it is Gaussian, this simulation model has finite exponential-of-quadratic moments, which is a much stronger condition than **A4**; we conjecture the convergence rate in this Gaussian case is \(O_{}()\) for \(_{}\), which would be consistent with figure 2(a). These numerical results seem to suggest the rate for \(_{}\) is similar, thus the second result of theorem 2 may not be sharp.

### Comparing dot product affinities and Euclidean distances for the 20 Newsgroups data

In this section we expand on the results in table 1 for the 20 Newsgroups data, by exploring how inter-topic and intra-topic dot product affinities and Euclidean distances relate to ground-truth labels. Most existing agglomerative clustering techniques quantify dissimilarity using Euclidean distance. To compare dot products and Euclidean distances, figures 3(a)-(b) show, for each topic, the top five topics with the largest average dot product and smallest average Euclidean distance respectively. We see that clustering of semantically similar topic classes is apparent when using dot products but not when using Euclidean distance. Note that the average dot product affinity between comp.windows.x and itself is not shown in figure 3(b), but is shown in 3(a), by the highest dark blue square in the comp.windows.x column. In appendix D we provide additional numerical results illustrating that with \(n\) fixed, performance in terms of \(_{b}\) correlation coefficient increases with \(p\).

   Data & Input & Dot product & UPGMA w/ & HDBSCAN & UPGMA w/ & Ward \\  & & & cos. dist. & & Eucl. dist. & \\   & \(_{1:n}\) & 0.26 (2.9) & 0.26 (2.9) & -0.010 (0.65) & 0.23 (2.7) & 0.18 (2.5) \\  & \(_{1:n}\) & 0.24 (2.6) & 0.18 (1.9) & -0.016 (1.9) & 0.038 (1.5) & 0.19 (2.7) \\   & \(_{1:n}\) & 0.34 (3.4) & 0.25 (3.1) & 0.023 (2.9) & 0.27 (3.2) & 0.30 (3.8) \\  & \(_{1:n}\) & 0.34 (3.4) & 0.27 (3.2) & 0.11 (2.8) & 0.16 (2.5) & 0.29 (3.8) \\   & \(_{1:n}\) & 0.15 (2.5) & 0.12 (1.9) & 0.014 (1.1) & 0.070 (1.5) & 0.10 (1.8) \\  & \(_{1:n}\) & 0.14 (2.4) & 0.14 (2.4) & -0.0085 (0.78) & 0.14 (2.6) & 0.12 (2.4) \\   & \(_{1:n}\) & 0.34 (10) & 0.34 (10) & 0.14 (9.3) & 0.34 (1) & 0.35 (10) \\  & \(_{1:n}\) & 0.36 (9.4) & 0.42 (11) & 0.33 (13) & 0.39 (11) & 0.39 (11) \\   & \(_{1:n}\) & 0.86 (1) & 0.81 (2) & 0.52 (8) & 0.52 (8) & 0.52 (8) \\  & \(_{1:n}\) & 0.86 (1) & 0.81 (2) & 0.52 (8) & 0.52 (8) & 0.52 (8) \\   

Table 1: Kendall \(_{b}\) ranking performance measure. For the dot product method, i.e., algorithm 1, \(_{1:n}\) as input corresponds to using \(_{}\), and \(_{1:n}\) corresponds to \(_{}\). The mean Kendall \(_{b}\) correlation coefficient is reported alongside the standard error (numerical value shown is the standard error\( 10^{3}\)).

Figure 3: Analysis of the 20 Newsgroups data. Marker shapes correspond to newsgroup classes and marker colours correspond to topics within classes. The first/second columns show results for dot products/Euclidean distances respectively. First row: for each topic (\(x\)-axis), the affinity/distance (\(y\)-axis) to the top five best-matching topics, calculated using average linkage of PC scores between documents within topics. Second row: average affinity/distance between documents labelled ‘comp.windows.x’ and all other topics. Third row: dendrograms output from algorithm 1 and UPGMA applied to cluster topics.

For one topic ('comp.windows.x') the results are expanded in figures 3(c)-(d) to show the average dot products and average Euclidean distances to all other topics. Four out of the five topics with the largest dot product affinity belong to the same 'comp' topic class and other one is a semantically similar'sci.crypt' topic. Whereas, the other topics in the same 'comp' class are considered dissimilar in terms of Euclidean distance.

In order to display visually compact estimated dendrograms, we applied algorithm 1 and UPGMA in a semi-supervised setting where each topic is assigned its own PC score, taken to be the average of the PC scores of the documents in that topic, and then the algorithms are applied to cluster the topics. The results are shown in figures 3(e)-(f) (for ease of presentation, leaf vertex 'heights' are fixed to be equal).

## 5 Limitations and opportunities

Our algorithm is motivated by modelling assumptions. If these assumptions are not appropriate for the data at hand, then the algorithm cannot be expected to perform well. A notable limitation of our model is that \((u,v) 0\) for all \(u,v\) (see lemma 3 in appendix C). This is an inappropriate assumption when there are strong negative cross-correlations between some pairs of data vectors, and may explain why our algorithm has inferior performance on the S&P 500 data in table 1. Further discussion is given in appendix E. A criticism of agglomerative clustering algorithms in their basic form is that their computational cost scales faster than \(O(n^{2})\). Approximations to standard agglomerative methods which improve computational scalability have been proposed [33; 5; 35]. Future research could investigate analogous approximations and speed-up of our method. Fairness in hierarchical clustering has been recently studied in cost function-based settings by  and in greedy algorithm settings by . Future work could investigate versions of our algorithm which incorporate fairness measures.