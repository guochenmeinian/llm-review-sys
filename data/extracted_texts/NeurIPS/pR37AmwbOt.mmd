# Leveraging Catastrophic Forgetting to Develop Safe Diffusion Models against Malicious Finetuning

Jiadong Pan\({}^{1,2}\)

Equal contribution.

Hongcheng Gao\({}^{2}\)

Zongyu Wu\({}^{3}\)

Taihang Hu\({}^{4}\)

Li Su\({}^{2}\)

Qingming Huang\({}^{1,2}\)

Liang Li\({}^{1}\)

\({}^{1}\) Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, CAS

\({}^{2}\) University of Chinese Academy of Sciences

\({}^{3}\) The Pennsylvania State University \({}^{4}\) Nankai University

panjiadong23s@ict.ac.cn, gaohongcheng23@mails.ucas.ac.cn

###### Abstract

Diffusion models (DMs) have demonstrated remarkable proficiency in producing images based on textual prompts. Numerous methods have been proposed to ensure these models generate safe images. Early methods attempt to incorporate safety filters into models to mitigate the risk of generating harmful images but such external filters do not inherently detoxify the model and can be easily bypassed. Hence, model unlearning and data cleaning are the most essential methods for maintaining the safety of models, given their impact on model parameters. However, malicious fine-tuning can still make models prone to generating harmful or undesirable images even with these methods. Inspired by the phenomenon of catastrophic forgetting, we propose a training policy using contrastive learning to increase the latent space distance between clean and harmful data distribution, thereby protecting models from being fine-tuned to generate harmful images due to forgetting. The experimental results demonstrate that our methods not only maintain clean image generation capabilities before malicious fine-tuning but also effectively prevent DMs from producing harmful images after malicious fine-tuning. Our method can also be combined with other safety methods to maintain their safety against malicious fine-tuning further.

_WARNING: This paper contains offensive images generated by models._

## 1 Introduction

The realm of text-to-image (T2I) generation has seen significant progress in recent years, primarily driven by diffusion models (DMs) trained on extensive and diverse datasets. Recently, many high-performance T2I DMs have been developed, including Stable Diffusion (SD) , Imagen , DALL-E 2 , VQ-Diffusion , among others. They have shown great power in generating high-quality images that closely match the textual prompt.

Yet, DMs can be misused by malicious individuals to create inappropriate content, such as images depicting nudity, violence, or illegal activities [42; 11; 36]. To address this issue, early-stage DMs were designed to reject the generation of inappropriate images through NSFW (Not Safe For Work) filters . Nevertheless, this approach does not inherently prevent the model from producing harmful imagery and can be readily disabled, leading to security vulnerabilities [3; 38]. Subsequently, many methods such as filtering the training data 3 or employing model unlearning techniques in the fine-tuning stage [38; 42; 8] have been put forward to build safer DMs. However, through fine-grained fine-tuning on harmful images, the model can still generate harmful images without affecting the generation quality .

Catastrophic forgetting [7; 34] is a common phenomenon in continual learning scenarios, such as fine-tuning, which refers to the phenomenon where a well-trained model experiences a significant performance drop on its original task after being trained on a new task. It has been widely studied as a negative factor in training [21; 19], with several works attempting to address it through various methods such as continual learning algorithms and data replay [21; 29; 30], while almost no work has positively utilized catastrophic forgetting as a beneficial tool. Recent works  show that DMs also exhibit the phenomenon of catastrophic forgetting, which makes it possible to leverage the characteristic to prevent malicious fine-tuning by treating harmful data as a new task for the DMs.

Firstly, we can make generating clean images a new task for the model by maintaining its ability to generate clean images while gradually distancing its understanding of harmful image distributions. This way, the original model's generation of harmful images gradually becomes an outdated and forgotten task. Secondly, if the safety model's understanding of harmful data is significantly different from the actual harmful data distribution, malicious fine-tuning will become a new task for the safety model and it will be difficult to generate harmful images even after malicious fine-tuning. In safety-aligned fine-tuning, we strive to keep the distribution of clean data unchanged to maintain the quality of clean image generation. Since there is some overlap between the distributions of clean data and harmful data, we can use the distribution of clean data as a benchmark to increase the distance between clean data and harmful data understood by the safety model to replace the distance between harmful data and the harmful data understood by the safety model, which makes malicious fine-tuning a difficult task for the safety model. The key to inducing catastrophic forgetting lies in increasing the distance between the clean data distribution and the harmful data distribution. Generally, contrastive learning has been widely employed to encourage models to separate data distributions in the latent space . Motivated by this, a feasible way to prevent malicious fine-tuning is by applying contrastive learning on clean data and harmful data.

In this paper, we propose a training policy based on contrastive learning to leverage catastrophic forgetting to develop a safe DM against malicious fine-tuning. Our method has two instantiations: latent transformation and noise guidance. Latent transformation refers to the operation of transforming the latent variable distribution of images. Noise guidance is adding different noises to clean and harmful images to induce different changes in the distribution of images. Both of these methods undergo contrastive learning fine-tuning and make models unable to generate harmful images after malicious fine-tuning. Our main contributions are:

* We consider the scenario of preventing T2I generation models from being fine-tuned on harmful data.
* We propose two viable methods to leverage catastrophic forgetting separately from the perspective of latent and noise.

Figure 1: Images generated by the baseline model SD v2.1 and models trained by our method. The top row contains harmful images, and the bottom row contains clean images. Harmful images generated by our methods before and after malicious fine-tuning both show quality degradation because the models are safely aligned before malicious fine-tuning and can also resist malicious fine-tuning. The generation quality of clean images is maintained in the safety alignment before malicious fine-tuning and slightly decreases after malicious fine-tuning in color and texture details. Orange boxes are added by the authors for publication.

* Experiments demonstrate that using our method to fine-tune the SD model significantly improves its safety and prevents it from being maliciously fine-tuned.

## 2 Related Work

### Text-to-Image Diffusion Models with Built-In Safety Methods

Researchers have developed various techniques to prevent T2I DMs from producing inappropriate or harmful content. These methods fall into two categories: black-box and white-box settings. Black-box setting methods do not require the internal knowledge of T2I DMs. Earlier work  uses a safety checker to detect generated images and then reject returning the images if deemed inappropriate. POSI  fine-tunes LLaMA  to be an optimizer that can revise prompts automatically to avoid inappropriate image generation. However, these types of black-box methods do not fundamentally make the model non-toxic and heavily rely on external components, making the pipeline very bloated. SLD  is proposed to reduce the inappropriate degeneration of DMs using safe guidance. Unfortunately, malicious humans will not use safe guidance when DMs are open-source. Hence, some white-box methods have been proposed [42; 8], which primarily unlearn harmful content by fine-tuning pre-trained DMs [9; 8; 23]. Forget-Me-Not  fine-tunes U-Net  in SD by applying attention resteering on all cross-attention layers of U-net. ESD  utilizes negative guidance to fine-tune the U-net to remove the given style or concept. Concept Ablation  makes the distribution defined by the given concept and the distribution defined by an anchor concept close. However, recent research  shows that models trained by these white-box methods can be easily fine-tuned to generate harmful images, making them unsafe.

### Catastrophic Forgetting

Catastrophic forgetting has been widely studied [12; 28; 19], with several works assessing its prevalence in modern settings [34; 27; 47]. It occurs in continual learning, particularly sequential learning and the pre-training & fine-tuning paradigm [22; 5; 17; 28]. Various attempts have been made to alleviate catastrophic forgetting through continual learning algorithms and data replay, such as imposing a penalty on the change of the parameter on the new task [1; 45; 37; 50], transferring knowledge from related new knowledge types back to the old types , incorporating the Hessian matrix into parameter regularization , etc. However, all these methods treat catastrophic forgetting as a negative factor to be eliminated, and almost no work has utilized it as a positive tool. Selective amnesia  utilizes a continual learning approach to forget unsafe concepts while not consider defending against malicious fine-tuning. Therefore, the focus of our work is to leverage this negative phenomenon of catastrophic forgetting as an effective means to defend against malicious fine-tuning.

## 3 Method

In this section, we give the details of our proposed approach that leverages catastrophic forgetting to develop safe DMs resilient to malicious fine-tuning. We first outline the problem formulation in Sec. 3.1. To achieve our goal, we introduce contrastive learning for safety alignment in DMs. At the same time, we propose two different instantiations to change the distribution of harmful data: **latent transformation** (LT, Sec. 3.2) and **noise guidance** (NG, Sec. 3.3). Finally, we give the way to maintain the quality of clean images generated by our safe model.

### Problem Formulation

The core idea behind leveraging catastrophic forgetting to prevent malicious fine-tuning on models is increasing the distance between the distributions of clean and harmful data. The model will forget harmful data as harmful image distribution is separated when maintaining the ability to generate clean images. When the distance between the distributions of clean and harmful data is large enough, it is difficult for the safety model to generate harmful images even after malicious fine-tuning because it becomes a new task for the safety model. In addition, for the safety model, it is important to maintain the ability to generate clean images. We combine these two goals together, which are maximizing the distribution distance between clean and harmful data in latent space while maintaining the model's ability to generate clean images before malicious fine-tuning. Suppose the dataset \(D\) is composed of two types of data: clean data \(D_{c}\) and harmful data \(D_{f}\), where \(D_{c}=\{x_{c}^{i},c_{c}^{i}\}_{i=1}^{N_{c}}\) and \(D_{f}=\{x_{f}^{i},c_{f}^{i}\}_{i=1}^{N_{f}}\), our goal can be described as:

\[_{}\; p(|D_{c})+(p(D_{c}| )\|p(D_{f}|))\] (1)where \(\) is the parameters of DMs, \(\) is a divergence measure between two distributions, and \(\) is a tunable hyper-parameter used to achieve a trade-off between the quality of clean image generation and promoting the separation from harmful images. The first term of Equation 1 is to maintain the quality of clean image generation and the second term is to separate harmful data from clean data.

By using Bayes' rule, we can get:

\[ p(|D)= p(D|)+ p()- p(D)\] (2)

Because \(D\) is composed of \(D_{c}\) and \(D_{f}\), Equation 2 can be rearranged as:

\[ p(|D)= p(D_{f}|)+ p(|D_{c})- p(D_{f})\] (3)

Then,

\[ p(|D_{c})=- p(D_{f}|)+ p(|D)+C\] (4)

To maximize \( p(|D_{c})\), it can be achieved by lower \( p(D_{f}|)\) and higher \( p(|D)\). To lower \( p(D_{f}|)\), we use two methods to change the prediction objective of harmful text conditions in Sec. 3.2 and 3.3. For the second term in Equation 4, the goal is to maintain the parameters of the original model and we replay clean data during the training process to achieve this.

Contrastive learning is an effective method to increase the distance between the distributions of different classes of data. The core concept is to ensure that samples from the same class are closely positioned, while samples from different classes are spaced further apart. In addition, to avoid affecting the quality of clean image generation, our method keeps the distribution of clean images unchanged, while only altering the distribution of harmful images predicted by our model to increase the distance between the clean and harmful image distributions. The training objective is:

\[_{c}=_{\{x_{f},c_{f}\} D_{f}}(||f_{}(c_{f})-_{}^{f}||_{2}-_{c}(0,||f_{}(c_{f})-_{}^{c} ||_{2}-l))\] (5)

where \(f_{}(c_{f})\) is the latent predicted by the DM based on condition \(c_{f}\), \(_{}^{f}\) and \(_{}^{c}\) are the centers of latent of harmful data and clean data.

By combining Equation 1 and Equation 4, we obtain the overall training objective:

\[=-_{\{x_{c},c_{c}\} p_{c}(D_{c})} p(x_{c}|,c_{c})+_{\{x_{f},c_{f}\} p_{f}(D_{f})} p(x_{f}|,c_{f })+_{c}\] (6)

where \(\) is the tunable hyper-parameter to balance the quality of clean image generation and promote the separation from harmful images, and the goal is to minimize \(\).

To minimize \( p(x_{f}|,c_{f})\), we use LT and NG to change the prediction objective of harmful text conditions.

Figure 2: **Left**. Diagram illustrating the method of leveraging catastrophic forgetting. The method leverages catastrophic forgetting by widening the distribution between clean and harmful data. **Right**. The method uses contrastive learning to leverage catastrophic forgetting against malicious fine-tuning.

### Latent Tranformation

DM is a widely used model for text-to-image generation. Many text-to-image models, such as Stable Diffusion , employ DMs that include an Auto-encoder (VAE) . The VAE effectively compresses images from the RGB space into the latent space. However, it also compresses the distances between different types of images in the latent space.

To make generating harmful images a new task for the safety model, we guide the harmful data to move away from the position of clean images in the latent space. This ensures that the model's prediction objective for harmful data is distant from the clean image distribution.

DM consists of two processes: the forward process and the denoise process. Suppose the origin data is \(x_{0}\) and the noisy data of timestep \(t\) is \(x_{t}\), the forward process can be represented as:

\[x_{t}=_{t}}x_{0}+}}\] (7)

where the noise \((0,I)\) and it is added to original data \(x_{0}\), which is controlled by \(}\) that is 1 when \(t=0\) and 0 when \(t=T\).

The denoise process is to train DMs to predict the added noise by minimizing the loss function:

\[=_{x_{0} p_{data},(0,I)}|| _{}(}}x_{0}+}} ,t)-||_{2}^{2}\] (8)

At timestep \(t\), we can estimate the original latents by 7.

\[}=-}}_{}(x_{t},t)}{ }}}\] (9)

The transformation is performed on the conditioned latents of harmful data. Define the predicted conditioned latents of harmful data as \(_{0,f}\), the transformation equation is 10.

\[_{0,f} R_{0,f}+b\] (10)

Performing a spatial transformation on the latent space can effectively separate the distribution between different classes of data. Here \(R\) and \(b\) could be randomly chosen from 0. This method is by altering the latent corresponding to harmful semantics. Another effective approach is to process the noise added to harmful data during the forward process.

### Noise Guidance

The forward process of DMs is described by equation 7. The distribution of harmful data can be altered and forgetting of harmful data can be achieved by changing the noise added to the original harmful data during the forward process.

A unique shift of the normal distribution noise is added to the noise added to harmful data, making \((_{f},1)\). \(_{f}\) can be fixed or dynamically changing. We set \(_{f}\) to be -1 or dynamically changing in our experiment. The optimization objective is changed to be:

\[=_{\{x_{f},c_{f}\} p_{f}(D_{f}),(_{f},I)}||_{}(x_{f},c_{f},t)-||_{2}^{2}\] (11)

Latents of clean and harmful images can be separated by adding different noises to harmful images during the training process. Guiding the randomly generated noise and latent space transformation are both effective in the experiment.

### Preserving Clean Image Quality

The above sections introduce methods for forgetting harmful data. However, while forgetting harmful data, maintaining the generation quality of clean images is also crucial for providing a usable safe model.

The overall training objective 1 includes the term that maintains the ability to generate clean images: \(_{\{x_{c},c_{c}\} p_{c}(D_{c})} p(x_{c}|,c_{c})\). The objective is achieved by random training DM on clean data. During the training process, clean data is also provided to the model to maintain the ability to generate clean images. The training objective for clean data remains consistent with that of the original DM, which can be described as:

\[=_{\{x_{c},c_{c}\} p_{c}(D_{c}),(0,I)}||_{}(}}x_{c}+}} ,c_{c},t)-||_{2}^{2}\] (12)By randomly training on clean data with a certain probability, DM avoids forgetting the generation of clean data.

In summary, our method addresses both forgetting harmful data and maintaining clean data by training on a dataset composed of clean and harmful data, aiming to achieve a trade-off between the model's safety in forgetting harmful images and its ability to maintain clean image generation. More importantly, the distribution of harmful and clean data predicted by our safe model is separated, which makes leveraging catastrophic forgetting against malicious fine-tuning possible.

## 4 Experiments

In this section, we conduct comprehensive experiments to evaluate the effectiveness of our methods, aiming to answer the following research questions: (RQ1) _Whether our method leveraging catastrophic forgetting can be used to achieve a safe model?_ (RQ2) _Whether the safe model reinforced by our method can prevent malicious fine-tuning?_

### Experimental Setup

**Datasets.** To provide a comprehensive evaluation of our method, we use prompts of LAION-5B  to generate clean images and harmful prompts generated by Mistral 7B  to create harmful images. Two kinds of data are used for fine-tuning. The details of the prompts are shown in Appendix D. In addition, we use DiffusionDB , COCO , I2P , and Unsafe  prompts to test the effectiveness of our model.

**Models.** Since Stable Diffusion (SD)  is the most widely used open-source T2I generation model and has achieved very high image generation quality. We mainly conduct experiments on SD v1.4 and SD v2.1. Due to the complexity of the SD XL  architecture, we only provide results from partial experiments conducted on the SD XL model, which are presented in Appendix A. ESD-Nudity-u1 and ESD-Violence-x1  are unlearning models designed to be incapable of generating nudity-related and violence-related images. In the safety reinforcement experiment, we apply them as base models.

**Metrics.** We consider five evaluation metrics. For harmful image evaluation, we use NSFW Score, Inappropriate Rate and Hum. Eval. (i) **NSFW Score** is used to evaluate the safety of models. It is calculated by a pre-trained detector; (ii) **Inappropriate Rate (IP)** is proposed by SLD  and is used to evaluate the safety of models. It is calculated by NudeNet  and Q16 . These two harmful detectors are respectively focused on sexual detection and the detection of other harmful types. If either of the two detectors identifies the image as harmful, then the image will be considered inappropriate. The parameters for both detectors are set to default; and (iii) **Hum. Eval.** (Human evaluation) is a method for assessing model safety through human judgment. Evaluations are made by three individuals, and the results are the average of their judgments. This metric can reflect human evaluation of the quality of images generated by the model. For clean image evaluation, we use Aesthetic Score and CLIP Score. (i) **Aesthetic Score** is a metric to evaluate the quality of generated images. It is calculated by LAION-Aesthetics-Detector V1, a linear estimator on top of CLIP  to predict the aesthetic quality of pictures; and (ii) **CLIP Score** is another metric to evaluate the quality of generated images. It measures the correlation between the generated images and the prompts. Measurements of all metrics are averages of images generated from 100 prompts corresponding to each dataset.

**Configurations.** Malicious fine-tuning steps of models are set to 20. All of the models are trained for 200 gradient update steps with a learning rate 1e-5 and a batch size of 1. \(\), \(_{c}\), and \(l\) are set to 5e-5, 1, and 0 in the training process.

### Safety Alignment

In this subsection, we train safe aligned models using our method. Safety alignment refers to fine-tuning a pre-trained SD model to become a safe model that cannot generate harmful images in our main experiments. The model trained using our methods is not only safe but also can avoid being further maliciously fine-tuned.

Table 1 shows the result of safe alignment experiments. The NSFW score and IP of the model we trained are lower than the original model, while the aesthetic score remains at a similar level before malicious fine-tuning. This suggests that our approach can maintain the model's capability to generate clean images while training a safe model. Besides, the NSFW score and the IP of our model barely rise after the malicious fine-tuning, which shows that our model can resist malicious fine-tuning. Human evaluation has also confirmed it. For original SD v1.4 and SD v2.1, we find the NSFW Score nearly unchanged before and after malicious fine-tuning, which is because the original SD is already

toxic, it is still toxic after malicious fine-tuning. Table 5 compares the safety of our safe alignment model with other safe models and our model can achieve a similar level of performance as other models even before malicious fine-tuning.

### Safety Reinforcement

In the experiment of safety reinforcement, a pre-trained safe model is introduced, and our training method is applied to this already pre-trained safe model to reinforce it, preventing malicious fine-tuning. ESD-Nudity-u1 and ESD-Violence-x1  unlearned models are used as base models. The base models are fine-tuned based on SD v1.4. We then further fine-tune the models using our methods. Table 2 shows the NSFW scores, IP,CLIP Score and Aesthetic scores of the models trained using different methods.

Compared with original safe models, our methods show better safety performance after being maliciously fine-tuned. The NSFW Score and IP of original safe models increase a lot after malicious fine-tuning. In contrast, the NSFW Score and IP of safe models after safe reinforcement by our methods even show a slight drop after malicious fine-tuning, which demonstrates that our model can resist malicious fine-tuning. Besides, the Aesthetic Score and CLIP Score of our safe reinforcement model do not change a lot, which shows that our model achieve a trade-off between safety and generation quality.

Table 3 shows the phenomenon of generation quality degradation before and after malicious fine-tuning. The experiment is conducted on sexual data. Compared to the results of clean fine-tuning, the model shows varying degrees of generation quality degradation after malicious fine-tuning, which is evidence that when fine-tuning on harmful data, clean image generation will also show degradation due to the effect of catastrophic forgetting on clean data because of fine-tuning using harmful data. It is the evidence that DMs will show catastrophic forgetting when fine-tuned on datasets for certain specific concepts.

### Ablations and Additional Experiments

Results in Sec. 4.2 demonstrate that our method can train a model that is secure and resistant to malicious fine-tuning while maintaining a high generation quality. Meanwhile, the experimental results in Sec. 4.3 demonstrate that our method can fortify an already trained secure model, leveraging the phenomenon of catastrophic forgetting to enhance its resistance to malicious fine-tuning.

In this subsection, we analyze the effects of different experiment settings and prove the robustness and universality of our methods on different datasets and other types of images.

   &  &  \\   &  &  &  &  &  & CLIP Score \(\) \\   & SD v2.1 & 0.6034 & 0.5887 & 0.36 & 0.42 & 9.67\% & 10.00\% & 6.6954 & 0.4185 \\  & LT & 0.5084 & 0.4720 & 0.24 & 0.25 & 1.00\% & 0.33\% & 6.7442 & 0.3983 \\  & NG & **0.4517** & 0.4732 & **0.23** & 0.24 & 4.00\% & **0.00\%** & 6.6868 & 0.4105 \\   & SD v1.4 & 0.6269 & 0.6198 & 0.44 & 0.46 & 2.33\% & 4.66\% & 6.7324 & 0.3980 \\  & LT & 0.4421 & 0.4051 & 0.25 & **0.20** & **1.00\%** & 1.33\% & 6.2658 & 0.4056 \\  & NG & 0.4371 & **0.3932** & 0.27 & 0.26 & 1.33\% & 1.33\% & 6.3210 & 0.4025 \\   & SD v2.1 & 0.4961 & 0.4983 & 0.43 & 0.45 & 7.67\% & 8.33\% & 6.9505 & 0.3632 \\  & LT & 0.4837 & **0.4744** & **0.30** & 0.31 & 5.67\% & 1.67\% & 6.8052 & 0.3802 \\  & NG & 0.4736 & 0.4772 & 0.35 & 0.33 & 3.66\% & **1.33\%** & 6.8028 & 0.3825 \\   & SD v1.4 & 0.5158 & 0.5151 & 0.40 & 0.46 & 8.00\% & 8.00\% & 6.5405 & 0.3775 \\   & LT & 0.4740 & **0.4690** & 0.31 & 0.32 & 3.67\% & 2.33\% & 6.2652 & 0.3828 \\   & NG & 0.4806 & 0.4788 & 0.33 & **0.30** & 3.33\% & **1.33\%** & 6.3960 & 0.3829 \\  

Table 1: Results of safety alignment experiment. The performance of our models is evaluated in harmful image generation and clean image generation. For harmful image generation, NSFW Score, IP and Hum. Eval. are evaluated. The data on the left of each panel is evaluated on original pre-trained models or contrastive learning fine-tuned models, while the data on the right is the result after the models have been maliciously fine-tuned. Our model shows better safety before and after malicious fine-tuning compared with original SD models for lower NSFW Score and IP. For clean image generation, Aesthetic Score and CLIP Score are evaluated on original pre-trained models or contrastive learning fine-tuned models. Our safety model maintains the quality of clean image generation for fluctuating Aesthetic Score and CLIP Score.

#### 4.4.1 Different Malicious Fine-tuning Steps

We test how the security of our model changes with the increase in malicious fine-tuning steps in the experiment. We set malicious fine-tuning steps from 1 to 100 to demonstrate the robustness of our method against malicious fine-tuning. Additionally, we find that as the number of malicious fine-tuning steps increased, the model exhibits a sudden increase in security performance and a decline in generation quality. This may be evidence of catastrophic forgetting in the model.

Figure 3 shows the results of different malicious fine-tuning steps. During the process of increasing fine-tuning steps from 0 to 100, the NSFW scores initially oscillate around 0.5, then abruptly drop to around 0.4 after 80 steps. The model demonstrates resilience to malicious fine-tuning across different step numbers, as the NSFW scores consistently remain lower than the baseline score for SD v2.1.

In addition, the phenomenon of abrupt change occurred during the adjustment of malicious fine-tuning steps. As the number of malicious fine-tuning steps increased, there was a sudden drop in NSFW scores, indicating a sudden forgetting of model knowledge. This could be considered as evidence of effective utilization of the catastrophic forgetting phenomenon. This phenomenon is counterintuitive and should be investigated further.

Furthermore, we conduct additional experiments to train a strongly safe aligned model by increasing the training steps to 2000 and enlarging the dataset. our strongly aligned safety model, which has forgotten most knowledge of sexual content, achieved results where the IP did not exceed 4% within 200 steps of malicious fine-tuning, and the results are shown in Appendix B.

We also use the UnlearnDiffAtk  algorithm to attack our safety model to test the robustness of our model. UnlearningDiffAtk algorithm is an adversarial prompt generation approach for DMs, which utilizes the intrinsic classification abilities of DMs to attack safety models to generate harmful images. The results are shown in the Appendix F.

#### 4.4.2 The Quality of Clean Image Generation

We test our model's ability to generate clean images. We use Frechet Inception Distance (FID)  as the metric to evaluate the quality of clean images and we use COCO-30K  dataset as the reference dataset for the FID benchmarks. The results show that our model retains the ability to generate clean

    &  &  \\   & Model & NSFW Score \(\) & IP \(\) &  & CLIP Score \(\) \\   & SD v1.4+ESD-Nudity & 0.4222 & 0.4613 & 0.25 & 0.34 & 6.7164 & 0.3908 \\  & LT & 0.4441 & **0.4098** & 0.26 & **0.19** & 6.5129 & 0.4096 \\  & NG & 0.4421 & 0.4301 & 0.22 & 0.20 & 6.4341 & 0.4232 \\   & SD v1.4+ESD-Violence & 0.4658 & 0.4883 & 0.23 & 0.33 & 6.7388 & 0.3895 \\  & LT & 0.4645 & 0.4643 & 0.25 & **0.19** & 6.4433 & 0.3735 \\  & NG & 0.4604 & **0.4598** & 0.26 & 0.21 & 6.4419 & 0.3850 \\   

Table 2: Results of safety reinforcement experiment. The performance of our models is evaluated in harmful image generation and clean image generation. For harmful image generation, NSFW Score, IP are evaluated. The left and right data are evaluated before and after malicious fine-tuning. Compared with the original unlearned model, the safety of our methods retains after malicious fine-tuning for lower NSFW Score and IP. For clean image generation, Aesthetic Score and CLIP Score are evaluated on original pre-trained models or contrastive learning fine-tuned models before malicious fine-tuning. The generation quality of safe reinforcement models is not effected a lot for similar Aesthetic Score and CLIP Score.

    &  &  \\  Fine-tuning Type & Primary & Clean FT & \(_{tch}\) & Harmful FT & \(_{tch}\) & Primary & Clean FT & \(_{tch}\) & Harmful FT & \(_{tch}\) \\  LT & 6.2831 & 6.3533 & +0.0702 & 6.1642 & -0.1189 & 0.4096 & 0.4066 & -0.0030 & 0.3883 & -0.0213 \\ NG & 6.4341 & 6.4689 & +0.0348 & 6.3628 & -0.0713 & 0.4232 & 0.4170 & -0.0062 & 0.4147 & -0.0085 \\   

Table 3: The impact of clean fine-tuning and malicious fine-tuning on the securely reinforced model. \(_{cln}\) and \(_{hrm}\) represent the change in generation quality before and after ordinary fine-tuning. Clean FT and harmful FT mean fine-tuning with clean images and fine-tuning with harmful images. Compared with Clean FT, Aesthetic Score and CLIP Score show more decrease after harmful fine-tuning, which is evidence of the phenomenon of catastrophic forgetting between clean and harmful data.

images. Besides, the changes in FID before and after malicious fine-tuning are the evidence that DMs will experience catastrophic forgetting when fine-tuned on datasets for certain specific concepts. The reason why FID drops more in our methods is probably that the distribution of clean data understand by the model is also changed in safe alignment and safe reinforcement training. We also evaluate FID on our strongly safe aligned model. With an IP of 0.70%, the FID obtained is 30.15, indicating that our method can still generate clean images at maximum safety before malicious fine-tuning.

#### 4.4.3 Ways to Guide the Added Noise

In Sec. 3.3, we propose two ways to guide the noise shift. The first method involves adding a fixed noise offset, while the second method involves dynamically adding dynamically changing noise based on the center of the image latents.

The experimental results of adding different noises are presented in Appendix C. Adding dynamically changing noise in the safety alignment experiment yields better security performance and generation quality. However, in the security reinforcement experiments, the opposite results are observed.

We guess that adding dynamically changing noise in the unlearn model may introduce randomness in parameter changes, which could potentially undermine the security capabilities trained into the unlearn model. This issue will be left for future research.

#### 4.4.4 Unlearning Combined Harmful Concepts Simultaneously

To prove the feasibility of our method to develop a universally safe model, we combine sexual and violence data together to get a combined harmful dataset and do experiments on it. The results are

    &  &  \\    & Before FT & After FT & \\  SD v1.4 & 14.44 & 15.21 & -0.77 \\ SD v1.4+ESD-Nudity-u1 & 17.65 & 18.32 & -0.67 \\ Ours (Safety Alignment) & 19.27 & 21.98 & -2.71 \\ Ours (Safety Reinforcement) & 19.39 & 23.09 & -3.70 \\   

Table 4: FID Scores evaluated on COCO-30K of different models. The left is the FID scores of different models before malicious fine-tuning and the right is the FID scores of them after malicious fine-tuning. \(\) shows the decline of clean image generation quality, and \(\) of our method decreases more compared with base models. Both original SD v1.4 and our safety models show a decline of clean image generation quality, which is evidence that DMs will experience catastrophic forgetting when fine-tuned on datasets for certain specific concepts. Our method refers to NG.

   Model Name & IP \(\) \\  SD v1.4 & 0.35 \\ ESD-Nudity-u1 & 0.16 \\ ESD-Nudity-u3 & 0.12 \\ ESD-Nudity-u10 & 0.08 \\ ESD-Nudity-x3 & 0.23 \\ SLD-Medium & 0.14 \\ SLD-Max & 0.06 \\ Ours (Safe Alignment) & 0.21 \\   

Table 5: IP of different diffusion model safety alignment methods. Our method can achieve a similar level of performance as other methods. Our method refers to NG here. The results are evaluated on I2P nudity dataset.

Figure 3: Different malicious fine-tuning steps: perform fine-tuning with different numbers of malicious fine-tuning steps on the safe aligned model and test the NSFW Scores after malicious fine-tuning. The phenomenon of abrupt change occurred during this process. Left and right show the results of LT and NG, respectively. The red line represents the NSFW Score before the abrupt change, and the green line represents the score after the abrupt change. The NSFW Score shows a sudden decrease around 80 malicious fine-tuning steps.

shown in Table 6. This indicates that our model has the potential to remove various harmful concept types, which helps improve the model's safety and robustness. Besides, the quality of clean image generation is retained after safety alignment.

#### 4.4.5 Performance on Different Datasets

We use different prompt datasets to generate images to test the safety of our model before malicious fine-tuning. Results are shown in Table 7. The scores are calculated by averaging 100 images generated by safety alignment models using corresponding test prompt datasets. Mistral-7B means using Mistral-7B to generate prompts to generate harmful images, which imitate malicious human's behaviors. The results indicate that our model exhibits the characteristics of improving model security and maintaining generation quality across different datasets. For the tests on the I2P dataset, the NSFW score measured by our method shows only a slight decrease compared to the original model. This may be due to the presence of many illegal concepts in the I2P dataset, making it difficult for the NSFW evaluation to provide an accurate assessment.

## 5 Conclusion

In this paper, we study a novel problem of utilizing catastrophic forgetting mechanisms to prevent models from being maliciously fine-tuned. We propose the concept of preventing malicious fine-tuning on safe models and give a novel framework that leverages catastrophic forgetting through contrastive learning. It effectively integrates contrastive learning with DMs through spatial and noise transformations. Experiments on both safe alignment and safe reinforcement demonstrate the effectiveness of our method. Besides, additional experiments prove the robustness and universality of our method. Last, we address the limitations and ethical considerations in Appendix G and Appendix H, respectively.

    &  &  \\   &  &  &  &  &  \\   & SD v2.1 & 0.5003 & 0.5021 & 0.41 & 0.40 & 6.7224 & 0.4137 \\  & LT & 0.4804 & 0.4946 & 0.26 & 0.24 & 6.6905 & 0.4031 \\  & NG & **0.4713** & 0.4759 & 0.24 & **0.23** & 6.6302 & 0.3916 \\   & SD v1.4 & 0.5112 & 0.5286 & 0.41 & 0.40 & 6.4143 & 0.3943 \\  & LT & 0.4866 & 0.4727 & **0.24** & 0.27 & 6.3074 & 0.4036 \\  & NG & **0.4478** & 0.4549 & 0.26 & 0.25 & 6.3463 & 0.3954 \\   

Table 6: Performance of our model on combined harmful types of datasets. The performance of our models is evaluated in harmful image generation and clean image generation. For harmful image generation, NSFW Score, IP are evaluated. The data on the left of each panel is evaluated on original pre-trained models or contrastive learning fine-tuned models, while the data on the right is the result after the models have been maliciously fine-tuned. For clean image generation, Aesthetic Score and CLIP Score are evaluated on original pre-trained models or contrastive learning fine-tuned models before malicious fine-tuning. The results show the potential of our methods to erase various harmful concepts.

   Dataset Type & Metric & Test Datasets & SD v2.1 & LT & NG \\   &  & LAION-5B & 6.6954 & 6.7442 & 6.6868 \\  & & DiffusionDB & 6.4436 & 6.4221 & 6.5109 \\  & & COCO & 6.3700 & 6.1652 & 6.2984 \\   &  & Mistral-7B & 0.6034 & 0.5157 & **0.4517** \\  & & I2P & 0.2015 & **0.1935** & 0.2008 \\   & & Unsafe & 0.0991 & 0.0883 & **0.0640** \\   

Table 7: Testing the model of safe alignment on different datasets, which is fine-tuned by NG method. The data above tests the quality of the model in generating clean images, with the metric being aesthetic ratings. The data below pertains to testing the model’s ability to generate harmful images, with the metric being the NSFW score.