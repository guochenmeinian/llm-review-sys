# A Robust and Opponent-Aware League Training Method for StarCraft II

Ruozi Huang  Xipeng Wu  Hongsheng Yu  Zhong Fan

Haobo Fu Qiang Fu  Wei Yang

Tencent AI Lab, Shenzhen, China

{rosiehuang,haroldwu,yannickyu,zhongfan,haobofu,leonfu,willyang}@tencent.com

Corresponding author.

###### Abstract

It is extremely difficult to train a superhuman Artificial Intelligence (AI) for games of similar size to StarCraft II. AlphaStar is the first AI that beat human professionals in the full game of StarCraft II, using a league training framework that is inspired by a game-theoretic approach. In this paper, we improve AlphaStar's league training in two significant aspects. We train goal-conditioned exploiters, whose abilities of spotting weaknesses in the main agent and the entire league are greatly improved compared to the unconditioned exploiters in AlphaStar. In addition, we endow the agents in the league with the new ability of opponent modeling, which makes the agent more responsive to the opponent's real-time strategy. Based on these improvements, we train a better and superhuman AI with orders of magnitude less resources than AlphaStar (see Table 1 for a full comparison). Considering the iconic role of StarCraft II in game AI research, we believe our method and results on StarCraft II provide valuable design principles on how one would utilize the general league training framework for obtaining a least-exploitable strategy in various, large-scale, real-world games.

## 1 Introduction

As one of the most famous real-time strategy games, StarCraft II poses great challenges, in terms of developing a superhuman Artificial Intelligence (AI), to existing Reinforcement Learning (RL) techniques (Vinyals et al., 2017, 2019; Han et al., 2020; Wang et al., 2021), which mainly come in two-folds. The first challenge is the enormous space of strategies. There are approximately \(10^{26}\) possible choices of actions at each time step, and each game normally consists of tens of thousands of time steps. The second challenge is the non-transitive strategy space with hidden information. Non-transitive strategies mean strategy A winning against B and strategy B winning against C does not necessarily mean strategy A winning against C. This implies simple self-play RL algorithms may exhibit strategic cycles Lanctot et al. (2017); Fu et al. (2021), making little progress in terms of the strength of the trained agent. For the hidden information, opponent entities are observable only when they are currently seen by the agent's own entities, and the information of opponent entities is extremely important in StarCraft II: it reveals the opponent strategy. Yet, it is worth-mentioning that the agent is allowed to actively demystify (known as scouting) the hidden information in StarCraft II, which is in contrast to other games with hidden information, such as poker.

As the first AI that defeated top humans in the full game of StarCraft II, AlphaStar (Vinyals et al., 2019) copes with the challenge of enormous strategy space by using human data. The RL agents inAlphaStar are all initialized by Supervised Learning (SL) on human replays. Also, a KL-divergence (from human replays) regularization is imposed during RL in AlphaStar. To deal with the non-transitive strategy space with hidden information, AlphaStar employs a league training framework that is inspired by game-theoretic approaches [Heinrich et al., 2015, Lanctot et al., 2017] with convergence guarantees to Nash Equilibrium (NE), which is an optimal solution concept AlphaStar aims for. The AlphaStar league consists of four (yet three types) constantly-learning agents: one main agent, one main exploiter, and two league exploiters. The main agent is the one output for evaluation after training. The main exploiter aims to find weaknesses in the main agent, while the league exploiters aim to find weaknesses in the entire league.

Despite achieving a Grandmaster level on Battle.net and defeating several top humans, AlphaStar is computationally prohibitive (see Table 1), and its main agent was later found fragile to some uncommon counter-strategies 4. This implies the inefficiency of the AlphaStar league training framework in terms of approximating a NE strategy in large-scale games such as StarCraft II. In this paper, we improve the AlphaStar league training framework in two substantial ways. We found that (as shown in Figure 6), in the later training iterations of AlphaStar, the exploiters tend to lose the ability to identify the weaknesses in the main agent and the entire league. To alleviate this problem, we train goal-conditioned (as opposed to unconditioned exploiters in AlphaStar) exploiters that exploit the weaknesses in certain directions. To alleviate the problem that AlphaStar does not respond to the opponent real-time strategy effectively, we introduce a novel opponent modeling auxiliary training task, which conveys an explicit training signal for the feature extraction process to focus on the area of observation that reveals the opponent strategy most. In addition, we construct a novel "scouting" reward based on the prediction of the opponent strategy, which greatly encourages the scouting behaviors and thus helps the agent respond to the opponent real-time strategy more effectively. We term our new method as robust and opponent-aware learning training method for StarCraft II (ROA-Star).

We validate our improvements by comparing ROA-Star to AlphaStar. Extensive experiments demonstrate that the exploiters in ROA-Star are more effective in detecting the weaknesses of the main agent and the entire league; that the main agent in ROA-Star responds to the opponent strategy more effectively; and overall that the main agent in ROA-Star is significantly stronger. We also conducted by far the most comprehensive AI vs. top human evaluations in StarCraft II, where our agent trained by ROA-Star achieved a winning rate above \(50\%\) in repeated games. A detailed comparison, in terms of the computational cost and human evaluation, between AlphaStar and ROA-Star is given in Table 1. In light of the significant improvement of ROA-Star over AlphaStar, we believe ROA-Star provides two insightful design principles, i.e., the goal-conditioned exploiters and the opponent modeling auxiliary task, on how one would utilize a league style training framework for obtaining a least-exploitable strategy in various, large-scale, real-world games.

   & & AlphaStar & ROA-Star \\   & TPU or GPU & 256 3rd-generation TPU cores & 64 NVIDIA V100 GPUs \\   & CPU & 4100 preemptible CPU cores & 4600 standard CPU cores \\   & actor & 16000 concurrent games & 2400 concurrent games \\   & learner & 5000 steps per second & 11000 steps per second \\   & amateur & 11 wins and 3 losses & 20 wins and 0 losses \\   &  &  & 3 wins 2 losses (vs herO5) \\    & & 5 wins 0 losses (vs TLO 2) & 10 wins 10 losses (vs Jieshi3) \\    & & 5 wins 1 losses (vs MaNa3) & 12 wins 8 losses (vs Cyan3) \\    & & 10 wins 10 losses (vs MacSed3) \\  

Table 1: A full comparison between AlphaStar and our method ROA-Star. The computational cost is for a single training agent in the league, and the evaluation results are based on Protoss vs. Protoss matches. ROA-Star uses significantly less computational resources than AlphaStar, and the evaluation against top humans is more comprehensive in ROA-Star than AlphaStar.

Preliminary on StarCraft II and AlphaStar

### StarCraft II

In StarCraft II (Vinyals et al., 2017), players act as battlefield commanders and start with a few workers who can gather resources. As resources accumulate, players need to allocate them to build buildings, train military units, and research new technologies. StarCraft II offers players the choice of dozens of unit types, each with different spells and abilities. It requires players to construct their armies strategically and assign different real-time tasks to these military units, which can be roughly divided into scout, defense, and attack. When the offensive units of two players meet, each player must quickly direct their units to engage in combat and effectively control them. To win the game, players require to control units at the micro level while making strategic choices at the macro level. Another fundamental setting in StarCraft II is the "fog of war", which limits a player's vision of the map to only those areas that are within the visual range of their entities (buildings and armies).

### Supervised Learning in AlphaStar

Each agent in AlphaStar is firstly trained through supervised learning on human replays. Formally, an agent's policy is denoted by \((a_{t}|o_{1:t},a_{1:t-1},z)\), where \(\) is represented by a deep neural network. At each time step \(t\), the agent receives a partial observation \(o_{t}\) of the complete game state \(s_{t}\) and selects an action \(a_{t}\). \(o_{t}\) consists of its own entities, visible enemy entities, and a mini-map depicting the terrain of the battlefield. \(a_{t}\) includes the choice of action type, the action executors, and the targets. The policy conditions on an extra statistic \(z\), which is a vectorized description of a strategy. \(z\) encodes the first 20 build order (buildings or units) and some cumulative statistics present in a game. In supervised learning, AlphaStar minimizes the KL divergence between human actions and the conditioned policy \(\) on human data.

### RL and League Training in AlphaStar

To address the game-theoretic challenges, AlphaStar proposes a multi-agent RL algorithm, named league training. It assigns the learning agents three distinct types (main agent, main exploiters, and league exploiters), each corresponding to different training and opponent selection mechanisms. All agents are initialized with the parameters of the supervised learning agents and trained with RL. During the league training, these agents periodically add their checkpoints into the league.

The Main Agent (MA) takes the whole league as opponents, with a Prioritized Fictitious Self-Play (PFSP) mechanism that selects the opponent for RL training with probabilities in proportion to the opponent's win rate against MA. Also, the MA is trained by randomly conditioned on either zero or a statistic \(z\) sampled from \(\), where \(=\{z_{1},z_{2},,z_{m}\}\) is a set of strategy statistics extracted from human data. Similar to goal-conditioned RL (Schaul et al., 2015; Veeriah et al., 2018), the agent, when conditioned on \(z\), would receive pseudo-rewards that measure the edit distance between the target build orders in \(z\) and the executed ones.

There are two types of exploiters that are designed to identify the weaknesses of their opponents. The Main Exploiter (ME) trains against the MA. The League Exploiter (LE) trains against the whole league. Both exploiters are unconditionally trained. Exploiters add a checkpoint to the league when they achieve a sufficiently high win rate or reach a timeout threshold. At that point, there is a certain probability that exploiters are reset to the supervised agent.

## 3 ROA-Star

In this section, we present the technique details of ROA-Star. As we discussed before, the purpose of the exploiters in AlphaStar is to find weaknesses of the MA and the entire league. Yet, as we found in the experiments, exploiters in AlphaStar (we re-implement AlphaStar ourselves) gradually lose the ability to counter the MA or the entire league as the training proceeds. This may be because it is increasingly difficult for the exploiters to counter strong agents in later training iterations via exploring the whole strategy space freely (other than initialized from the SL agents). Hence, we propose to alleviate this problem by training exploiters that exploit in certain directions with goal-conditioned RL. In real-time games like StarCraft II, the opponent's strategy, involving both the composition of the entity and the technological development, can change rapidly. AlphaStar exhibits a somewhat slow response to these dynamic changes, resulting in the decrease in the playing strength. Our straightforward solution is introducing an opponent modeling training task. Also, based on the opponent modeling prediction, we construct a novel "scounting" reward to encourage efficient scouting behaviors, which helps respond to the opponent real-time strategy more effectively.

### Goal-conditioned RL for the Exploiters

The exploiters (either the main exploiter or the league exploiter) in AlphaStar are trained unconditionally, i.e., the policy \((a_{t}|o_{1:t},a_{1:t-1})\) depending on only previous observations and actions. In addition to the unconditioned exploiters in AlphaStar, we introduce another two ways of training the exploiters \((a_{t}|o_{1:t},a_{1:t-1},z)\) that are extra conditioned on the statistic \(z\). More specifically, we train Exploitative Exploiters (EIE) that are conditioned on those \(z\), which are associated with high win-rate in the MA. In other words, EIE try to find the weaknesses of either the main or the league based on "good" \(z\) known so far. Also, we train Explorative Exploiters (ERE) that are conditioned on those \(z\), which are under-explored so far in the MA. In other words, ERE try to find the weaknesses of either the main or the league based on "under-explored" \(z\) so far. Both the EIE or ERE are trained with goal-conditioned RL, the details of which are described below.

The MA in ROA-Star is trained the same way as that in AlphaStar, where the MA is trained either unconditionally or conditioned on a statistic \(z\) randomly selected from the set \(\). In our case, we maintain a moving average of win rate of the MA for each sampled \(z\) during the training process. This gives us an estimation of the "performance" of different \(z\). Yet, it is worth mentioning that a MA conditioned on a certain \(z\) is not guaranteed to generate plays that are consistent with the corresponding \(z\). In other words, a \(z\) with bad performance could be due to the fact that the MA is unable to execute the \(z\) successfully. For this reason, we calculate the edit distance between the actual executed \(z\) and the target \(z\) as the execution deviation, and we maintain a moving average of the execution deviation for each \(z\) in \(\) as well.

We only sample from those \(z\) with the Top-N win rate of the MA for EIE to learn. Similar to the training of MA in AlphaStar, EIE are rewarded by the \(z\)-related pseudo-reward measuring the edit distance between the target and executed \(z\). Different from the exploiters in AlphaStar, EIE are always reset to the latest MA parameters whenever a reset occurs. As a result, EIE inherit the strongest strategies in MA. Also, once a \(z\) is sampled, it is fixed till the next reset of EIE, which is in contrast to the MA training setting that samples \(z\) for each game. Instead of evenly distributing computing

Figure 1: **a.** The design of goal-conditioned exploiters in ROA-Star. Blue rectangles highlight the original unconditioned exploiters in AlphaStar. Small circles are the agent models. Green histograms display the win rate and the execution deviation of \(z\) in MA during a specific time slice in training. **b.** Opponent Modeling in ROA-Star. Left is the overview of the architecture of the agent in ROA-Star, which combine the embedding of opponent strategy. Right is the overview of the opponent prediction model.

resources among all \(z\) with the Top-N win rate, training with a fixed \(z\) for each reset allows EIE to concentrate all its computational resources to refine the micro-level operations on that \(z\) and uncover potential weaknesses at the micro level.

In comparison to EIE, ERE are conditioned on those \(z\) that are currently under-explored according to the moving average execution deviation statistics of MA. We initialize ERE with the SL agents when reset occurs. We notice that the z-related pseudo-reward measured by the edit distance would be easily hacked (Ecoffet et al., 2021) on those \(z\) with high execution deviation. For instance, the agent tends to produce the common military entities in target \(z\) to gain reward but skip the critical ones. To reduce the execution deviation on these \(z\), we design a reward that is similar to the trajectory-conditioned learning algorithm (Guo et al., 2020), which formulates a curriculum on how to imitate the target \(z\). More specifically, ERE are rewarded for each correctly built entity in the target \(z\) till the first mistaken entity happens, where we ignore the left entities in the target \(z\). For the first mistaken entity (say \(e_{m}\)) in the target \(z\), we additionally apply a strategy-guided loss:

\[_{sg}=-_{i=1}^{d}e_{m}^{(i)}(a_{t}^{(i)}|o_{1:t},a_{1:t -1},z),\]

where \(\,e_{m}\{0,1\}^{d},a_{t}^{d}\). \(d\) is the dimension of action space, which encompasses all the actions related to entity construction. The strategy-guided loss is disabled for production-irrelevant actions, therefore it won't harm the learning of micro-operation.

We use both the curricular reward and strategy-guided loss in ERE training, which is similar to the idea of Go-Explore (Ecoffet et al., 2021): the agent can always return to the promising state with the curricular reward and then explore the intended direction guided by the strategy-guided loss. Again, we only sample from those z with the Top-N execution deviation of the MA. Once a \(z\) is sampled, it is fixed till the next reset of ERE. Also, as ERE are conditioned on the \(z\) with high execution deviation, ERE aim to uncover potential weaknesses at the strategy (macro) level. An illustration of our exploiters in comparison to AlphaStar is given in Figure 1(a).

### Opponent Modeling for the Entire League

Knowing the opponent strategy gives the agent a huge advantage in Starcraft II. Yet, with the basic setting "fog of war" in StarCraft II, it is difficult for agents to predict the opponent strategy. Nonetheless, it is possible to infer the opponent strategy to some extent based on the observed opponent entities. More importantly, because of the "scouting" mechanism in StarCraft II, the agent can actively send units to gather more information about the opponent. AlphaStar does not explicitly predict the opponent strategy, and there has been evidence 5 showing that AlphaStar does not respond to the opponent real-time strategy effectively, which greatly affect the agent's performance as demonstrated in our experiments.

In order to improve an agent's ability of responding to the opponent strategy promptly and effectively, we introduce an opponent modeling auxiliary task to infer the opponent strategy. In particular, during the supervised learning of human data, we train a probabilistic encoder to obtain a latent opponent strategy embedding \(h\), which serves as input to a probabilistic decoder that explicitly predicts the opponent strategy. The encoder and the decoder are trained by maximizing the Evidence Lower Bound (ELBO) used in \(\)-Variational Autoencoders (\(\)-VAE) (Kingma and Welling, 2014; Higgins et al., 2014). Afterwards, in our league training, the probabilistic encoder is fixed and serves as a special feature extractor that focuses the parts of observation that mostly reflects the opponent strategy.

To construct the input features for the encoder \(q_{}(h_{t}|o_{ t})\) of our opponent modeling task, we filter out opponent irrelevant information (e.g., own entities) in \(o_{t}\) and focus on visible opponent information, which includes opponent armies, buildings, and technologies. The encoder \(q_{}(h_{t}|o_{ t})\) predicts a Gaussian distribution of \(h_{t}\) with mean \(_{t}\) and variance \(_{t}\). A KL divergence term is optimized between the predicted Gaussian and the standard Normal distribution. The decoder \(p_{}(y_{t}|h_{t})\) predicts (a classification problem) the opponent invisible information at each time step, which includes the quantity of each entity, the technologies being researched, and the location of important tech buildings (e.g., near the player's base or near the opponent). To summarize, we use the reparametrization trick (Kingma and Welling, 2014) to optimize the following loss:

\[_{om}(y_{t},o_{0},,o_{t};,)\] \[=-_{q_{}(h_{t}|o_{ t})}[ p_{}(y_{t}|h _{t})]+ KL(q_{}(h_{t}|o_{ t})\|(0,1)),\]

where in practice the negative log likelihood in the left term is replaced with the focal loss (Lin et al., 2017) in our case to alleviate the class imbalance problem in StarCraft II. An illustration of the opponent prediction model is given in Figure 1(b).

In addition, we construct a novel "scouting" reward based on the change in the cross-entropy of the opponent strategy prediction. Other than predicting the opponent strategy behind the fog, another way to "defog" is to drive military units to periodically scout the area outside the vision. Given that the full map is too vast to scout entirely, it is crucial for the agent to determine when and where to scout. As a result, good "scouting" behavior should improve the prediction accuracy of the opponent strategy. We thus use the opponent prediction model to obtain such a "scouting" reward. At each step of the RL process, we calculate the cross-entropy between the true opponent strategy and the predicted probabilities of the classifiers in the opponent prediction model:

\[H(t)=-_{i=1}^{N}y_{t}^{(i)} P_{t}^{(i)}\]

We reward the agent every \(30\) seconds with the decrease in cross-entropy to encourage scouting behaviors. Note that the agent will not be punished for the increase in cross-entropy, as there is a sustained growth of cross-entropy as the game progresses (prediction in later stage is much harder).

\[r_{scout}^{t}=(H(t)-H(t^{min}),0),\] \[where\;t^{min}=_{t^{}}H(t^{}),\;t^{} (t,t+30s).\]

As demonstrated in our experiments, the "scouting" reward greatly encourages the effective "scouting" behavior and improves the overall performance.

## 4 Related Work

There is enormous literature on either goal-conditioned RL (Andrychowicz et al., 2017; Florensa et al., 2018; Ghosh et al., 2019; Chane-Sane et al., 2021; Liu et al., 2022a) or opponent modeling (He et al., 2016; Albrecht and Stone, 2018; Zheng et al., 2018; Raileanu et al., 2018; Willi et al., 2022; Fu et al., 2022). As in this paper we are focused on how goal-conditioned RL and opponent modeling would improve the league training efficiency of AlphaStar, we will describe related work only in the realm of AI for StarCraft, with more emphasis on literature that comes after AlphaStar.

Before AlphaStar, early research on StarCraft focus on traditional methods, such as planning methods associated with heuristic rules (Weber, 2010; Ontanon et al., 2013; Ontanon et al., 2008; Weber et al., 2011; Ontanon et al., 2013). Later, there have been RL methods for micro-management in mini-games (Usunier et al., 2016; Vinyals et al., 2017; Du et al., 2019; Han et al., 2019). Most recently, RL were applied to the full game with the help of some handcraft rules (Lee et al., 2018; Sun et al., 2018; Pang et al., 2019). However, none of these works achieves a competitive human level in the full game of StarCraft II.

AlphaStar (Vinyals et al., 2019) became the first StarCraft II AI that achieves the GrandMaster level performance. After that, to enhance the strategy diversity in the league of AlphaStar, TStarBot-X (Han et al., 2020) leverages various human replays to create diverse initialization checkpoints for the exploiters to reset. Handcraft rules were also employed to help the MA explore the strategy space. Another work after AlphaStar is StarCraft Commander (SCC) (Wang et al., 2021), which makes great efforts to optimize the training efficiency by filtering the human data used in imitation learning and compressing the network structure. They branch off new main agents learning specified strategies. SCC displayed comparable performance to professional players in human evaluations, but its APM surpasses humans by a large margin due to the lack of APM limits. Recently, A hierarchical RL method has been proposed to train a StarCraft II AI that can defeat the built-in AI using very few computational resources (Liu et al., 2022b). To summarize, none of these StarCraft II AIs is able to achieve the level of professional players while adhering to the APM limits and utilizing fewer computational resources than AlphaStar.

Experiments

After 50 days of league training, ROA-star generates a total of 768 models, of which 221 are MA models. We evaluate the robustness of the 50-day MA model under the condition of no \(z\) through human evaluation. As a comparison, we replicated AlphaStar and trained it for 10 days. We compared the first 10 days of training of ROA-Star and AlphaStar from multiple dimensions. As ROA-Star contains improvement in both exploiters' training method and opponent modeling, we conduct the ablation experiments to demonstrate the effectiveness of each component. Each ablation experiment was trained for 5 days. Notably, all the comparative experiment and ablation experiments use the same computational resources as ROA-Star, as shown in the Appendix A.5.

### The Effectiveness of Goal-conditioned Exploiters in ROA-Star

Based on the foundation experimental setup of AlphaStar, we modify the original unconditioned exploiters and design the following two experiments to test the effect of goal-conditioned exploiters.

**AlphaStar+exploiters with random z**: We make exploiters learn \(z\) in \(\) with uniform probability. For each reset, the exploiter randomly selects one \(z\) from \(\) to learn with an 80% probability and learns unconditionally with a 20% probability. When learning a selected strategy, the exploiter would be rewarded by the \(z\)-related pseudo-rewards measuring edit distance between the target and executed build orders.

**AlphaStar+EIE+ERE**: On the basic setting of AlphaStar, we utilize our goal-conditioned exploiters in Section 3.1, the details of the experimental setup are in the Appendix A.3.

To examine the relative strength of the MA models in different ablation experiments, we consider the rating metric Elo score. Elo scores reflect the relative skill levels of the players based on the win rates between them. We play 100 matches between any two of the first 5-day models of each MA and plot the Elo curves in Figure 2. Besides, we calculate the worst-case win rate of each 5-day MA model defending against all other MA models as a metric of robustness, the result is shown in Figure 3. As we can see, the exploiters that learn random strategies contribute to the robustness of MA, but our goal-conditioned exploiters obviously outperform random selection on \(\).

Additionally, we demonstrate the capability of ERE to learn the precise build orders of target strategies through the use of curricular reward and strategy-guided loss. Our supplementary video and Appendix C.2 showcase the remarkable performance of ERE in imitating non-mainstream strategies.

### The Effectiveness of Op

   model win rate vs & AlphaStar & +opponent modeling \\  Void Ray push & 54\% & **60\%** \\ Proxy VD & 50\% & **56\%** \\ VC Stalker Play & 22\% & **52\%** \\ Disruptor push & 48\% & **58\%** \\   

Table 2: Win rate against different opponents.

**AlphaStar+opponent modeling**: With the aid of the opponent prediction model, we incorporate opponent strategy embedding for the original AlphaStar agents, as well as add scouting rewards. The worst-case robustness is in Figure 3 and the Elo curve is in Figure 2.

To better demonstrate the impact of opponent modeling, we test two 5-day MA models, one trained with opponent modeling and one without, against opponents with specific strategies. We generated these opponents by training MA only on the fixed \(z\) we selected, without using exploiters. Figure 4 displays the distribution of tech-building routes when the models encounter various opponents. Although both models employ a similar dominant strategy, namely VS-VR, the model trained with opponent modeling demonstrates greater flexibility and has prompt responses even on its first tech-building when encountering different opponents, thereby exhibiting superior robustness compared to the original AlphaStar. For example, it increases the probability of VR opening to defend against the Blink Stalkers and build more BF and Photon Cannon to defend the cloaked Dark Templars. The models' win rates against these opponents are shown in Table 2.

In the Appendix C.6, we present visualizations of the latent space of the opponent prediction model, showcasing ROA-Star's "awareness" of the opponent's strategy when facing various opponents. We also compare the time consumed for the agent to discover the newly-built buildings in Appendix C.3. Figure 12 shows that scout reward remarkably reduces the time to discover the opponents' building under the fog.

### Overall Comparison between ROA-Star and AlphaStar

The primary objective of ROA-Star is the improvement in robustness. However, it's scarcely possible to precisely measure the robustness of the agent due to the enormous strategy space. Similar to the evaluation method in Section 5.1, we count the win rates by a Round Robin tournament on models of two leagues, with each pair of models playing 100 matches. Based on those matches, we introduce quantitative evaluation indexes to score the robustness of the MA and the whole league for AlphaStar and ROA-Star.

Figure 4: Distribution of tech-building routes against specific opponents. Building acronyms: VR (Robotics Facility), VS (Stargate), BF (Forge), VC (Twilight Council), VD(Dark Shrine).

Figure 5: Elo scores of agents in the league of AlphaStar and ROA-star during 10-day training. Curves represent two main agents and points represent the models from exploiters. UE refers to unconditioned exploiters.

Figure 6: Payoff matrix calculated in (a). our ROA-Star and (b). AlphaStar. Blue means a row agent wins and red loses. The exploiters in AlphaStar lose efficacy in the later stage of training.

We calculate the win rate in the worst case and average case that two 10-day MA models defend against all models in two leagues. Results are shown in Table 3 that ROA-Star significantly outperforms AlphaStar in the MA's robustness. Relative population performance (RPP) is a measure of the expected outcome of the game between two populations. It is calculated based on their meta-game mixture strategies after they have reached Nash equilibrium (Balduzzi et al., 2019). Given the payoff matrix \(M_{AB}^{N M}\) between all the models in league \(A\) and league \(B\) with two mixture strategies probabilities \(P_{A}\),\(P_{B}\) in the Nash equilibrium, the RPP of league A is:

\[RPP(A)=P_{A}^{T}M_{AB}P_{B}\]

As shown in Table 3, a higher RPP of ROA-Star demonstrates it constructs a mixture strategy that could counter any mixed strategy from AlphaStar.

As a by-product of the above matches, we evaluate the relative strength of all the players in the league of AlphaStar and our ROA-Star with the Elo rating system. The Elo score curves of the two main agents as well as points indicating the models from exploiters are plotted in Figure 5. As we can see, the strength of MA in ROA-Star is superior to AlphaStar throughout the training procedure, and ROA-Star can always generate more effective opponents for the MA.

Additionally, we evaluate the internal payoff estimation for each of the two leagues, giving the agents' relative performance against all the players in the same league. As shown in Figure 6, the exploiters of AlphaStar seem to have gradually weakened dominance to the MA in the later stage. Meanwhile, there always exist evenly matched opponents for MA in the ROA-Star league, which benefits from its goal-conditioned exploiters.

### Top Human Evaluation of ROA-Star

To make a comprehensive evaluation of ROA-Star without access to the official Battle.net, we invite three top professional players: Cyan, Macsed and Jieshi to play against our agent on the Kairos Junction map. Although most of the current AI trained in the full games of StarCraft II are evaluated with human players in best-of-three (BO3) or best-of-five (BO5) matches (Vinyals et al., 2019; Wang et al., 2021), we realize that human has the ability to identify the weaknesses of an opponent in repeated competitions. Therefore, we ask each professional player to play 20 matches with ROA-Star to validate its robustness. We didn't make any demands on the strategies they use except encourage them to try more methods to find the weakness of our agent (Appendix B.1).

Figure 7 shows the trend of ROA-star's win rates as the games proceeds. ROA-star gains the upper hand over all the opponents at the beginning, but its win rate drops as the professional players learn more about it and test more strategies. Finally, as a mixed strategy, ROA-star maintains a win rate of no less than 50%, which means human players didn't find a winning strategy that can continuously defeat it, indicating the robustness of the agent.

We submit a comparison video with AlphaStar as supplementary material, showing how AlphaStar was defeated by the Cannon Rush strategy in Battle.net evaluation and how we defend against the same strategy played by the professional player through effective scout and prompt response.

Figure 7: Trend of ROA-Star’s win rate when fighting with professional players. ROA-Star won BO3 and BO5 matches against all the players and maintained final win rates no less than 50%.

Conclusions and Limitations

In this paper, we develop a new league training method, i.e., ROA-Star, for approximating a NE in very large-scale games such as StarCraft II. ROA-Star improves the influential AlphaStar league training in two significant aspects. ROA-Star trains goal-conditioned exploiters (as opposed to unconditioned exploiters in AlphaStar), which greatly improves the exploiters' ability in identifying weaknesses at both the micro and macro level for the MA and the entire league. In addition, a novel opponent modeling task is introduced in ROA-Star, which helps the agent respond to the opponent's real-time strategy more effectively. Meanwhile, a novel "scouting" reward is constructed based on the prediction of the opponent strategy. Extensive experiments demonstrate the effectiveness of ROA-Star over AlphaStar. Overall, ROA-Star produced a better and superhuman AI with orders of magnitude less resources than AlphaStar on StarCraft II.

There are several future directions of this work. During league training, we kept the parameters of the opponent prediction model fixed to retain the prior knowledge of game strategies in the human data. This approach proved effective in our experiments, yet it is worth investigating the impact of the diversity of strategies in the human data. Also, the policy diversity [Wu et al., 2022, Yao et al., 2023] during league training is worth pursuing as well. In this paper, we only tested ROA-Star on StarCraft II, and future experiments on other large scale games with hidden information are needed to further validate the effectiveness and generalization of ROA-Star. Finally, even though ROA-Star provides a general guideline that goal-conditioned exploiters and opponent modeling help in terms of approximating a NE in very large-scale games, recent research advances on both goal-conditioned RL and opponent modeling in general are definitely worth exploiting to further improve ROA-Star's performance.