# PPLNs: Parametric Piecewise Linear Networks for Event-Based Temporal Modeling and Beyond

Chen Song Zhenxiao Liang Bo Sun Qixing Huang

Department of Computer Science

The University of Texas at Austin

Austin, TX 78712

{song, liangzx, bosun, huangqx}@cs.utexas.edu

###### Abstract

We present Parametric Piecewise Linear Networks (PPLNs) for temporal vision inference. Motivated by the neuromorphic principles that regulate biological neural behaviors, PPLNs are ideal for processing data captured by event cameras, which are built to simulate neural activities in the human retina. We discuss how to represent the membrane potential of an artificial neuron by a parametric piecewise linear function with learnable coefficients. This design echoes the idea of building deep models from learnable parametric functions recently popularized by Kolmogorov-Arnold Networks (KANs). Experiments demonstrate the state-of-the-art performance of PPLNs in event-based and image-based vision applications, including steering prediction, human pose estimation, and motion deblurring. The source code of our implementation is available at https://github.com/chensong1995/PPLN.

## 1 Introduction

Event cameras are neuromorphic sensors that summarize the evolving world as a stream of _events_. Each event describes the pixel coordinates, time, and polarity of an intensity change. Thanks to the simplicity of this data representation, event cameras enjoy multiple advantages over conventional cameras, including but not limited to fast data rate and high dynamic range (Gallego et al., 2020). Over the past, researchers have developed event-based algorithms to solve various computer vision problems, such as motion deblurring (Pan et al., 2019, 2020; Wang et al., 2020; Song et al., 2022, 2024), human pose estimation (Calabrese et al., 2019), and autonomous driving (Binas et al., 2017; Hu et al., 2020). A recent survey (Zheng et al., 2023) indicates a rapidly increasing community interest in event-based research, with motion deblurring being the most popular task. Extensive experiments demonstrate that by utilizing events as an auxiliary input, algorithms perceive fine motion details that are absent from conventional image captures, leading to substantial performance gain over methods that make inferences from conventional images alone.

The success of event cameras demonstrates the power of imitating biological neuromorphic principles. The event camera comprises a rectangular array of _dynamic vision sensors_, each of which is analogous to a visual receptor neuron on the human retina dedicated to perceiving one specific location. The neuron experiences excitement and produces a spike when the environmental intensity varies significantly, corresponding to an event generated as a response to brightness changes.

This paper presents Parametric Piecewise Linear Networks (PPLNs) to understand the bio-inspired event data with a bio-inspired deep learning model. As opposed to a generic network design, we believe and verify in this paper that it is highly beneficial to build a processing network that caters to the principles of the data source (event cameras). As illustrated by Figure 1 (Middle), the key idea is to explicitly approximate the membrane potential of a neuron as a piecewise linear mapping from time to electric voltage. Figure 1 (Right) presents the sketch of a PPLN node, whose internal mechanismis explained thoroughly in Section 3. While the event camera imitates how a single layer of visual receptor neurons react to external intensity changes, PPLNs are motivated by observations of how layers of biological neurons communicate. Inspired by the Leaky Integrate-and-Fire model (Abbott, 1999), we propose to use a piecewise linear parameterization to approximate its temporal evolution in the logarithmic space. The key difference between PPLNs and the bio-inspired Spiking Neural Networks (SNNs) (Eshraghian et al., 2021) in existing literature is that PPLNs are an alternative to general GPU-based temporal inference models, whereas SNNs aim at the deployment on hardware neuromorphic chips (Davies et al., 2018). Instead of real-valued membrane potentials, SNNs also propagate binary spikes, leading to reduced energy consumption and training instabilities.

PPLNs are conceptually similar to the emerging Kolmogorov-Arnold Networks (KANs) (Liu et al., 2024). Both KANs and PPLNs leverage learnable parametric functions to build a deep network. Different from KANs, which use input-independent B-splines, PPLNs exploits input-dependent piecewise linear functions. Using piecewise linear functions allows our brain-inspired design to mimic biological neural principles and the event generation model. Predicting function coefficients at inference time allows the network to better handle input heterogeneity.

We modify the network architecture in state-of-the-art event-based vision algorithms by replacing multi-layer perceptions and convolution operators with PPLN nodes and evaluate PPLNs on three applications: steering prediction, 3D human pose estimation, and motion deblurring. With fewer or a similar number of trainable parameters, PPLNs improve baselines by 30.8% in steering prediction, 11.1% in human pose estimation, and 5.6% in motion deblurring. To demonstrate the potential of PPLNs, we experiment on the conventional frame-based version of the same applications without the event input and observe consistent improvements. Additionally, we present a mathematical analysis of the convergence properties to showcase the robustness.

In summary, we make the following contributions:

* We propose Parametric Piecewise Linear Networks (PPLNs), mimicking biological principles by approximating membrane potentials as parametric mappings.
* We show how to predict a set of parametric coefficients from the input of the PPLN node and evaluate the membrane potential at any timestamp of interest.

Figure 1: **(Left)**: A biological neuron has three main components: the dendrites (blue), the axon (orange, pink), and the soma (green). The dendrites are responsible for receiving external inputs. The axon transmits signals to the dendrites of other neurons through the synapses. The soma is the body of the cell and connects the dendrites to the axon. **(Middle)**: The membrane potential, defined as the voltage difference between the interior and the exterior of the cell, regulates the neuronâ€™s behavior and can be approximately modeled by a piecewise linear function. \(\) When the neuron is at rest, the potential stays at a constant level \(V_{0}\). \(\) An external input is received by the dendrites, causing an instantaneous perturbation to the membrane potential. \(\) The perturbation is not significant enough to excite the neuron, and the potential leaks over time exponentially (_i.e._, linearly in the logarithmic space). \(\) Another external input happens. \(\) The input fails to excite the neuron. \(\) A third input causes the membrane potential to exceed the threshold voltage \(V_{}\). The neuron becomes excited and generates a spike. \(\) The excitement opens ion channels, and the ion flow causes a reset to the membrane potential. \(\) After the excitement, the ion channels close again, and the potential continues to decay. \(\) The neuron returns to the resting state, waiting for new inputs. **(Right)**: A PPLN node. Given inputs \(\{x_{i}\}_{i=1}^{k}\), we predict the linear coefficients \(\) for the membrane potential function, including the slope, intercept, and endpoints of each line segment. The resulting parametric function \(_{}\) is then used to evaluate the neuron output at the timestamp of interest \(y(x_{1},,x_{k},t)=(_{}(t))\), where \(()\) is the integral normalization defined in Section 3.4.

* We present a mathematical analysis of the convergence properties of PPLNs.
* We apply PPLNs to event-based and frame-based applications and achieve state-of-the-art results.

## 2 Related work

**Biological neurons**. As shown in Figure 1 (Left), if the combined effect of small perturbations over time causes the membrane potential to exceed the threshold potential, the neuron will become excited. During the excitement, the neuron produces an output spike through the synapses (Lacinova, 2005; Maeda et al., 2009). In Figure 1 (Middle), the Leaky Integrate-and-Fire model (Abbott, 1999) suggests that the membrane potential can be approximated by a piecewise linear function as a parametric mapping from time to the logarithm of the voltage difference.

**Spiking neural networks**. Spiking Neural Networks (SNNs) are Recurrent Neural Networks (RNNs) that simulate biological neurons (Eshraghian et al., 2021). Each SNN node carries an internal variable corresponding to the membrane potential. The output of the SNN node is a binary signal that is zero by default and becomes one if excited (Abbott, 1999). The key advantage of SNNs is low power consumption, making them the ideal model to be deployed on hardware neuromorphic chips (Davies et al., 2018). By contrast, PPLNs are designed to be an alternative to general GPU-based temporal inference models, even though SNNs and PPLNs are motivated by similar biological principles.

**Event cameras**. Event cameras are neuromorphic devices that summarize the evolving environment as a stream of events (Lichtsteiner et al., 2008). Each event is analogous to a significant intensity change that exceeds the hardware threshold and excites a biological light-sensing neuron. An event is represented as a 4-tuple \((x,y,t,p)\), where \((x,y)\) are the pixel coordinates, \(t\) is the timestamp, and \(p\{-1,+1\}\) is the polarity of the intensity change. Event cameras have a fast data rate, high dynamic range, low power consumption, and minimal motion blur compared to conventional cameras (Gallego et al., 2020). A recent trend is to utilize hardware that simultaneously captures event and conventional streams, where the event stream is used as an auxiliary input to enhance regular computer vision algorithms. For example, while image-to-image deblurring is a well-studied problem in the research community (Richardson, 1972; Fish et al., 1995; Krishnan and Fergus, 2009; Joshi et al., 2009; Levin et al., 2007; Kim et al., 1998; Shan et al., 2008; Fergus et al., 2006; Xu et al., 2013; Xu and Jia, 2010; Perrone and Favaro, 2014; Babacan et al., 2012; Kupyn et al., 2018, 2019), several works in event-based vision demonstrate the possibility of converting a blurry image into a sharp video that explains the motion during the exposure interval (Pan et al., 2019, 2020; Wang et al., 2020; Song et al., 2022, 2024).

**Learning activation functions**. The Rectified Linear Unit (ReLU) (Nair and Hinton, 2010) represents a two-piece linear activation function, \(f(x)=x\) if \(x>0\), and \(f(x)=0\) if \(x 0\) and has several variants. Our design is conceptually similar to the Piecewise Linear Unit (PWLU) (Zhou et al., 2021), where the activation is defined as an \(n\)-piece linear function with learnable slopes and intercepts. Kolmogorov-Arnold Networks (KANs) have also received wide attention, which build a deep model by stacking layers of parametric B-spline activation functions (Liu et al., 2024). In addition to the conceptual similarities, PPLNs are fundamentally different from PWLUs and KANs since PPLNs incorporate temporal modeling. Additionally, PPLNs allow discontinuities at segment endpoints and the learning of endpoint locations, neither of which is supported by PWLUs or KANs. Furthermore, the ReLUs are activation functions to be appended after prediction layers (_e.g._, linear and convolution layers), whereas the PPLNs are designed to replace the prediction layers in temporal learning models.

## 3 Method

### Overview

As shown in Figure 2 (a, b), a PPLN node implements the following mapping:

\[f:^{k}\] (1)

where \(^{k}\) is the \(k\)-dimensional non-temporal component of the input, and \(t\) is the normalized input scalar timestamp. The mapping \(f\) converts the input \((,t)\) to a scalar in \(\).

The first step in the calculation of \(f\) is to predict the linear coefficients, \(=\{,,\}\), using the trainable parameters, \(W_{m}\), \(W_{b}\), \(W_{s}\), and \(_{V}\). Section 3.2 explains this process in detail. The predicted coefficients allow us to assemble the piecewise linear membrane potential function, as illustrated by the blue plot in the bottom-left corner of Figure 2 (a, b). To better handle the numerical instability, Section 3.3 discusses the procedure to smooth the boundaries of the predicted linear pieces. The smoothed function is then normalized by another predicted value \(\), as explained in Section 3.4. Finally, the output of the PPLN node is given as \(f(,t)=(_{}(t))\) (_i.e._, the smoothed, and normalized potential function evaluated at the timestamp of interest). While it is straightforward to construct a fully-connected network by stacking PPLN nodes, Section 3.5 discusses how to support convolution operations.

### Coefficient prediction

Let \(n\) be a hyper-parameter denoting the number of line segments in the piecewise linear modeling. The parametric coefficients \(=\{,,\}\) are given by:

\[ =(W_{m})\] (2) \[ =W_{b}\] (3) \[ =(W_{s})\] (4)

where \(W_{m}\), \(W_{b}\), and \(W_{s}\) are \(n k\) dimensional matrices containing trainable weights. \(=(m_{1},,m_{n})^{T}\) and \(=(b_{1},,b_{n})^{T}\) are the slopes and intercepts of \(n\) different line segments, respectively. \(=(s_{1},,s_{n})^{T}\) defines the temporal interval size for each line segment. Let \(t_{0}=0\) and \(t_{i}=t_{i-1}+s_{i}\). With the softmax function, the temporal space \(\) is divided into \(n\) non-overlapping intervals by \(\). We predict the aforementioned coefficients and approximate the membrane potential as:

\[_{}(t):=m_{1}t+b_{1}&t_{0} t<t_{1}\\ m_{2}t+b_{2}&t_{1} t<t_{2}\\ \\ m_{n}t+b_{n}&t_{n-1} t t_{n}\] (5)

Here, the hyperbolic tangent function restricts the slope, \(\), preventing the exploding gradient problem commonly observed in temporal models (Pascanu et al., 2013; Bengio et al., 1994).

### Smoothing

While we can build a network by stacking layers of the vanilla PPLN nodes described above, training presents a challenge for numerical stability. Equation (5) suggests that \(_{}(t)}{}\) is an all-zero vector. In other words, gradient-based optimizers (Bottou et al., 1991; Kingma & Ba, 2014) cannot update the values of \(t_{i}\)'s (the location of segment endpoints).

To address this issue, we propose to smooth the segment boundaries. The key idea is to blend the linear pieces across adjacent intervals. Let \((x,_{i}(x))\) be the point on the \(i^{}\) predicted segment, that is, \(_{i}(x):=m_{i}x+b_{i}\).

Assuming \(t_{i-1} t<t_{i}\) (_i.e._, \(t\) belongs to the \(i^{}\) predicted segment), the smoothed potential is:

\[_{}^{T}(t):= w_{l}^{(i)}_{i-1}(t)+(1-w_{l}^{(i)}-w_{r}^{(i)})_{i}(t)+w_{r}^ {(i)}_{i+1}(t)\] (6)

where the weights \(w_{}^{(i)}\) and \(w_{}^{(i)}\) are defined through the temperature hyper-parameter \(T\):

\[w_{t}^{(i)}:= 0&i=1\\ (1+(T(t-t_{i-1})))^{-1}&i=2,,n\\ w_{r}^{(i)}:= (1+(T(t_{i}-t)))^{-1}&i=1,,n-1 \\ 0&i=n.\]

Importantly,

\[_{T+}_{}^{T}(t)=_{}(t).\]

The difference between the smoothed potential, \(_{}^{T}(t)\), and the unsmoothed potential, \(_{}(t)\), is that the smoothed gradients \(_{}^{T}(t)}{}\) do not vanish. We present a theorem with proof in the appendix showing the local convergence properties of the piecewise linear model after smoothing. Assuming the underlying function to fit is indeed an \(n\)-piece piecewise linear function, the theorem states that the coefficients can be accurately learned from a set of noisy samples, provided that the noises are reasonably small, the coefficients are adequately initialized, and the temperature \(T\) is sufficiently large. For ease of discussion, the theorem uses segment endpoints \(=\{t_{i}\}\) instead of interval lengths \(\) for the parameterization. Their relation is given in Section 3.2.

**Theorem 3.1**.: _(**Informal**) Consider an underlying n-segment piecewise linear function parameterized by \(^{}=\{^{},^{},^{}\}\) as defined in (5). Let \((_{j},v_{j})\), \(j=1,,m\) be \(m\) point samples, where \(v_{j}=_{^{}}(_{j})+_{j}\) in which \(_{j}\) is a small random noise._

_The L2 loss for the smoothed curve is defined by:_

\[^{T}():=_{j=1}^{m}{(_{}^{T}(_{j})-v_{j })}^{2}.\] (7)

_Denote by \(_{T}^{}\) the weights at which the minimum of (7) is attained at temperature \(T\). Then we show that starting from some initial \(_{0}\) close to \(^{}\), by applying vanilla gradient descent with appropriate temperature increase strategy and a learning rate \(=O()\), \(\) is guaranteed to converge to \(_{}^{}\) at a linear convergence rate._

_Specifically, the error of recovered segments is bounded by:_

\[_{_{}_{}}{|_{_{}^{ }}()-_{^{}}()|}<O(|_{j}|),\] (8)

_where \(_{}\), \(_{}\) are the smallest and largest values among \(_{j}\), for \(j=1,,m\), respectively._

In addition to the theorem above, the supplementary material uses ablation studies to discuss the practical implication of incorporating the smoothing operation.

### Integral normalization

While the smoothing operator introduced above enriches temporal gradients, \(_{}(t)}{}\), the integral normalization operator addresses the issue that \(_{}(t)}{}\) and \(_{}(t)}{}\) are both very sparse vectors with only one non-zero entry out of all \(n\) elements. From Equation (5), we have:

\[_{0}^{1}_{}(t)dt=_{i=1}^{n}m_{i}(t_{i}^{2}-t _{i-1}^{2})+_{i=1}^{n}b_{i}(t_{i}-t_{i-1})\] (9)

Let \(\) be a parameter that controls the mean of \(_{}(t)\) when \(0 t 1\). The integral normalization operator \(()\) is defined as:

\[(_{}(t))=_{}(t)-_{0}^{1}_{ }(t)dt+ V(t)\] (10)

Figure 2: **(a)** A linear PPLN node, which maps the input \((,t)\) to output \(f\). The trainable parameters are \(W_{m}\), \(W_{b}\), \(W_{b}\), and \(_{V}\). **(b)** A similarly structured 2D convolutional PPLN node. **(c)** The baseline architecture for steering angle prediction (Hu). **(d)** Our model. **(e)** The modified baseline (HuMod).

After the normalization, \(_{}(t))}{}\) and \(_{}(t))}{}\) are both dense vectors containing rich gradient information in every element, encouraging a smooth and swift convergence. A side effect of the normalization is that the temporal derivative, \(_{}(t))}{}\), also becomes non-zero, allowing segment endpoints to be learned even without smoothing.

Notably, the ground-truth parameter \(\) is observable in certain applications. In motion deblurring, the task is to generate a sharp video from a blurry image. Mathematically, the mean of all the frames in the output must be equal to the input, restricting the temporal average \(_{0}^{1}(_{_{xy}}(t))\) to be equal to the input pixel values. When it cannot be easily observed, \(\) is regressed from the input \(\):

\[=_{V},\] (11)

where \(_{V}\) is a \(k\)-dimensional vector of trainable weights, and \(,\) stands for the inner product.

Section 4.4 uses ablation studies to demonstrate the effectiveness of integral normalization. In the appendix, we additionally use a two-piece toy example to analyze the normalization.

### Supporting the convolution operation

The above modeling naturally extends to convolutions, which are the fundamental building blocks of contemporary deep learning. In a convolution layer, each input pixel only affects the output in a small spatial neighborhood rather than across the entire grid. To support convolution, the trainable parameters, \(W_{m}\), \(W_{b}\), \(W_{s}\), and \(_{V}\), become sparse matrices and vectors with non-zero entries only in locations within the spatial perceptive field. In practice, we predict the coefficients as:

\[ =((W_{m},))\] (12) \[ =(W_{b},)\] (13) \[ =((W_{s},))\] (14) \[ =(_{V},)\] (15)

where, after reshaping, \(W_{m}\), \(W_{b}\), \(W_{s}\), and \(_{V}\) are kernels in the convolution operation. The channel-wise softmax operator ensures the temporal interval sizes of each pixel add up to one, which is a requirement posed by the valid range of input timestamps (\(t\)). We refer interested readers to our code release for how the above design is implemented under PyTorch.

## 4 Evaluation

Section 4.1 starts by showing how PPLNs outperform various methods in motion deblurring, the most popular event-based application as indicated by a recent survey (Zheng et al., 2023). In Sections 4.2 and 4.3, we proceed with two other tasks where the goals are to predict the vehicle's steering angle from the dashcam footage and to estimate the 3D human pose from binocular 2D event camera captures. These are two "mainstream" applications in event-based vision, ranking immediately after deblurring, as reported by the survey. Finally, Section 4.4 and the appendix use ablation studies to demonstrate the importance of the integral normalization operator, the effect of changing the number of line segments in the parameterization \(n\), as well as the practical implication of the smoothing operator.

We emphasize event-based applications because event cameras and PPLNs are both designed to mimic biological neural principles. Due to the limited availability of high-quality data, event-based vision is an emerging field where modeling plays a more important role than data and the effects of PPLNs can be best demonstrated. However, we also present an evaluation on conventional frame-based tasks to demonstrate the generalizability. Meanwhile, we do not include Spiking Neural Network (SNN) baselines. While SNNs focus on the deployment onto neuromorphic chips, PPLNs are designed to be an alternative to general GPU-based temporal inference models.

### Task I: motion deblurring

**Task description**. Event-enhanced motion deblurring is a popular research domain. Variants of the task include image-to-image and image-to-video deblurring. Our experiments focus on the highly challenging image-to-video problem, establishing a thorough competition against various state-of-the-art approaches. Given a blurry image and its associated events during the exposure interval, our goal is to reverse the exposure process and reconstruct a sharp video describing the relative motion between the camera and the environment. The following experiments utilize the High Quality Frames (HQF) (Stoffregen et al., 2020) dataset and the preprocessing procedure documented by Song et al. We construct a PPLN based on the U-Net architecture (Ronneberger et al., 2015) by replacing all 2D convolution layers with PPLN nodes (Figure 2 (b)). The input to the PPLN is the concatenation of the blurry image and the event histograms, and the histograms are constructed following Zhu et al.. The output of the PPLN is sharp frames at 14 uniformly spaced timestamps. We utilize ADAM (Kingma and Ba, 2014) to train the network for 50 epochs using the L1 loss. We set the learning rate to \(10^{-3}\) and reduce the rate by half after 20 and 40 epochs, respectively. The number of line segments is \(n=3\).

**Evaluation metric**. We use the Mean Squared Error, the Peak Signal-to-Noise Ratio, and the Structural Similarity Index Measure.

**Results and discussions**. As shown in Table 1 (Left), PPLN has a very strong performance in motion deblurring. PPLN improves DeblurSR (Song et al., 2024), the state-of-the-art event-based motion deblurring model at the time of paper submission, by 5.6% in MSE, 0.372 dB in PSNR, and 4.9% in SSIM. Importantly, we underscore that the PPLN is a generic network architecture and does not require task-specific modeling, whereas the baseline approaches utilize techniques that cater to the deblurring problem, such as dictionary learning (eSL-Net) (Wang et al., 2020), per-pixel polynomial approximation (E-CIR) (Song et al., 2022), and implicit neural representation (DeblurSR) (Song et al., 2024). The success of the PPLN reveals the strength of mimicking biological neural behaviors.

In Table 1 (Left), we also use regular convolution layers to construct the U-Net. We increase the number of convolutional channels in the regular U-Net to approximately match the number of trainable parameters in the PPLN (173M versus 192M). The last two rows suggest that the PPLN improves the U-Net by 54.5% in MSE, 6.45 dB in PSNR, and 43.3% in SSIM. Qualitatively, as shown in Figure 3, our method generates sharper and more realistic frames than the baseline approaches. The proposed PPLN offers vivid details around salient features. In the first row, our method reconstructs the dark patterns on the white background with sharp edges. In the second row, our method gives the best contrast between the white board and the black letters.

### Task II: steering angle prediction

**Task description**. The DAVIS Driving Dataset released in 2020 (DDD20) (Hu et al., 2020) contains 51 hours of dashcam recordings with both neuromorphic events and conventional frames. Following Hu et al., we select 15 recordings during the day and 15 at night across the western United States, and train a deep network to regress steering angles from the dual-modal input.

   & PSNR \(\) & SSIM \(\) \\   \\  EDI & 0.336 & 17.822 & 0.515 \\ eSL-Net & 0.452 & 14.938 & 0.282 \\ eSL-Net+ & 0.385 & 16.870 & 0.363 \\ E-CIR & 0.207 & 21.713 & 0.609 \\ DeblurSR & 0.161 & 23.912 & 0.694 \\ U-Net & 0.334 & 17.834 & 0.508 \\  Ours & **0.152** & **24.284** & **0.728** \\   
   \\   & night & day & all \\  Hu & 3.05 \(\) 0.104 & 5.71 \(\) 0.334 & 4.55 \(\) 0.236 \\ HuMod & 2.64 \(\) 0.035 & 3.94 \(\) 0.069 & 3.34 \(\) 0.048 \\  Ours & **2.53 \(\) 0.040** & **3.68 \(\) 0.150** & **3.15 \(\) 0.081** \\   \\   & night & day & all \\  Hu & 0.940 \(\) 0.004 & 0.713 \(\) 0.033 & 0.845 \(\) 0.016 \\ HuMod & 0.954 \(\) 0.001 & 0.864 \(\) 0.005 & 0.917 \(\) 0.003 \\  Ours & **0.958 \(\) 0.001** & **0.881 \(\) 0.010** & **0.926 \(\) 0.004** \\  

Table 1: **(Left)**: Motion deblurring quality. **(Right)**: Steering prediction errors.

Figure 3: Motion deblurring visualizations. More are available in the supplementary material.

As shown in Figure 2 (c), we build a PPLN upon the existing baseline, consisting of a convolutional head, a pooling layer, and a linear component. The convolutional head reduces the spatial dimension of the data and increases the number of channels. The pooling layer eliminates both spatial dimensions by averaging all the pixels on the reduced spatial grid. Finally, the linear component of the network maps the 64-dimensional feature to the scalar steering angle output. The network contains a total number of 463,425 (463K) parameters. The input frame and events contain 50 ms of historical data.

Our model (Figure 2 (d)) lowers the prediction frequency to every 500 ms. The network takes ten conventional frames and ten times as many events as input and predicts the steering angle at ten uniformly distributed timestamps. We replace the linear component with PPLN nodes and slightly increase the number of layers. To match the number of parameters in the original architecture, we shrink the convolutional head. This results in a network with 455,338 (455K) parameters. We utilize ADAM (Kingma & Ba, 2014) to train the network for 200 epochs using the L2 loss. The learning rate \(10^{-3}\) with a weight decay of \(10^{-4}\). The number of line segments in the parameterization is \(n=3\).

**Evaluation metric**. We use the Root Mean Square Error and the Explained VAriance. We train with five random seeds and report the mean and standard deviation.

**Results and discussion**. As shown in Table 1 (Right), our approach outperforms the baseline model in RMSE, with a 17.0% improvement at night, 35.6% improvement during the day, and 30.8% improvement overall. Similar enhancement is observed in EVA, with 1.9% improvement at night, 23.6% improvement during the day, and 9.6% improvement overall.

The input to our model contains ten times as much information as the baseline approach (Hu et al., 2020). To investigate whether the performance gain is simply a result of enriched input information, we present an additional comparison where the baseline is modified to have the same input and output dimensions as our model (Figure 2 (e)). From the second and the third rows in Table 1 (Right), we observe that PPLN improves the modified baseline by 5.7% in RMSE and 1.0% in EVA.

### Task III: human pose estimation

**Task description**. The dynamic Vision Sensor Human Pose (DHP19) dataset (Calabrese et al., 2019) is collected by inviting human subjects into a cubic space and using event cameras in the four ceiling corners to record various body movements, such as walking, jumping, and walking. DHP19 contains recordings of 17 human subjects performing 33 different body movements. In addition to the events, DHP19 includes the 3D coordinates of 13 body joints. The goal is to predict 3D joint coordinates.

Calabrese _et al._ use two frontal cameras ("Cam 2" and "Cam 3") in their experiments. The overall pipeline has two stages. First, they utilize a deep network to predict the 2D joint coordinates from the events in each view. After that, they project the 2D predictions into 3D using the calibration matrices.

The deep network used by Calabrese _et al._ is a fully convolutional network (Long et al., 2015) that contains 17 layers and 218,592 (219K) trainable parameters. The input events are represented as histograms (Zhu et al., 2019), and the output 2D joint coordinates are represented as heatmaps. After collecting 25,000 events from all four views, the algorithm assembles event histograms from two frontal cameras and discards the events from the other two cameras. The model updates the 3D joint coordinates if the 2D prediction confidence exceeds a threshold \(=0.3\) in both views.

In our experiment, the model makes an inference every 250,000 events. The network takes ten times as many events as input and predicts 2D joint coordinates at ten different timestamps. We modify the prediction network by introducing PPLN layers. The modified network contains 215,648 (216K) trainable parameters. We utilize RMSProp (Kingma & Ba, 2014) to train the network for 20 epochs using the L2 loss. The learning rate is \(10^{-3}\) in the first 10 epochs, \(10^{-4}\) from epochs 10 to 15, and \(10^{-5}\) from epochs 15 to 20. The number of line segments in the parameterization is \(n=3\).

**Evaluation metric**. We use Mean Per Joint Position Error and report in 2D pixels and 3D millimeters.

   &  &  \\   & Cam 2 & Cam 3 & 3D MPIPE \(\) \\  Calabrese & 7.49 & 7.29 & 82.17 \\ CalabreseMod & 11.90 & 11.78 & 130.24 \\  Ours & **6.76** & **6.51** & **73.05** \\   
   &  &  \\   & Hu & 5.53 \(\) 0.110 & 0.771 \(\) 0.009 \\ HuMod & 3.55 \(\) 0.154 & 0.907 \(\) 0.007 \\  Ours & **3.16 \(\) 0.130** & **0.927 \(\) 0.005** \\  

Table 2: **(Left)**: Human pose estimation errors. **(Right)**: Frame-based steering prediction errors.

**Results and discussion**. As shown in Table 2 (Left), PPLN estimations have a 2D MPJPE of 6.76 pixels in Cam 2, a 2D MPJPE of 6.51 pixels in Cam 3, and a 3D MPJPE of 73.05 mm. Compared to the original network used by Calabrese _et al._, we achieve 9.7% improvement in Cam 2, 10.7% improvement in Cam 3, and 11.1% improvement in 3D. Similar to Section 4.2, simply enlarging the temporal horizon confuses the network and leads to performance degradation. Our method enhances the modified baseline by 43.2% in Cam 2, 44.7% in Cam 3, and 44.0% in 3D.

### Ablation studies

In Section 3.4, we introduce the integral normalization operator, which theoretically stabilizes training by enriching the gradient information. We now use ablation studies to examine the effectiveness of integral normalization in practice. As shown in Table 3, the introduction of this operator improves motion deblurring quality by 42.4%, 4.563 dB, and 31.4% in MSE, PSNR, and SSIM respectively. For steering prediction, integral normalization improves the accuracy by 17.5% in MSE and 3.2% in EVA. The p-values from one-tailed t-tests are 0.007 and 0.014 for MSE and EVA, giving us reasonable confidence that integral normalization has enhanced accuracy. For human pose estimation, integral normalization improves the 2D MPJPE by approximately 0.2 pixels and the 3D MPJPE by 1.8 mm. The improvement suggests that integral normalization can effectively regularize the training in scenarios such as deblurring where the normalization target has a clear semantic meaning. We refer readers to the appendix for the impact of line segment number \(n\) and smoothing on performance.

### Conventional frame-based vision

To demonstrate the generalizability of PPLNs, we remove the event input from the steering angle prediction model. Table 2 (Right) shows that PSNNs can still outperform the baselines in the conventional frame-based only setting. Importantly, we observe a significant performance drop by taking out the event input from both baseline approaches. However, the frame-based PPLN has a surprisingly similar prediction quality to the dual-modal PPLN. This result demonstrates that by simulating the biological behaviors, our model can effectively overcome the imperfections in the input data. Note that among the three tasks discussed above, only steering prediction allows inference from conventional frames alone. Human pose estimation takes events as the single-modal input, and image-to-video deblurring requires events to address the motion ambiguity.

## 5 Conclusion and Future Work

This paper presents Parametric Piecewise Linear Networks (PPLNs), a novel temporal learning architecture inspired by biological neural principles. The key idea is to represent the membrane potential as a parametric piecewise linear function with predictable coefficients. Experiments on various event-based vision applications, including steering prediction, human pose estimation, and motion deblurring, demonstrate that PPLNs outperform state-of-the-art models. In the future, we plan to use a recurrent prediction model to support a dynamic number of line segments. Another direction is to adopt more accurate modeling for the membrane potential function, including mechanisms such as the refractory period after each spike.

## 6 Acknowledgement

Q.H. would like to acknowledge NSF IIS 2047677 and NSF IIS 2413161.

    &  &  &  \\   & MSE \(\) & PSNR \(\) & SSIM \(\) & RMSE \(\) & EVA \(\) & 2D-2 \(\) & 2D-3 \(\) & 3D \(\) \\  \(\) & 0.264 & 19.721 & 0.554 & 3.82 \(\) 0.339 & 0.897 \(\) 0.018 & 6.81 & 6.74 & 74.88 \\ \(\) & **0.152** & **24.284** & **0.728** & **3.15 \(\) 0.081** & **0.926 \(\) 0.004** & **6.76** & **6.51** & **73.05** \\   

Table 3: Ablation studies justifying normalization. The appendix discusses the number of segments.