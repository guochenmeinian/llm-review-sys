# Label-Only Model Inversion Attacks via

Knowledge Transfer

 Ngoc-Bao Nguyen\({}^{*1}\)   Keshigeyan Chandrasegaran\({}^{*2}\)\({}^{2}\)

Milad Abdollahzadeh\({}^{1}\)   Ngai-Man Cheung\({}^{1}\)

\({}^{1}\)Singapore University of Technology and Design (SUTD)  \({}^{2}\)Stanford University

thibaongoc_nguyen@mymail.sutd.edu.sg ngaiman_cheung@sutd.edu.sg

 These authors contributed equally.  \({}^{}\) Work done while at SUTD.Corresponding author.

###### Abstract

In a model inversion (MI) attack, an adversary abuses access to a machine learning (ML) model to infer and reconstruct private training data. Remarkable progress has been made in the white-box and black-box setups, where the adversary has access to the complete model or the model's soft output respectively. However, there is very limited study in the most challenging but practically important setup: Label-only MI attacks, where the adversary only has access to the model's predicted label (hard label) without confidence scores nor any other model information.

In this work, we propose LOKT, a novel approach for label-only MI attacks. Our idea is based on transfer of knowledge from the opaque target model to surrogate models. Subsequently, using these surrogate models, our approach can harness advanced white-box attacks. We propose knowledge transfer based on generative modelling, and introduce a new model, Target model-assisted ACGAN (T-ACGAN), for effective knowledge transfer. Our method casts the challenging label-only MI into the more tractable white-box setup. We provide analysis to support that surrogate models based on our approach serve as effective proxies for the target model for MI.

Our experiments show that our method significantly **outperforms existing SOTA Label-only MI attack by more than 15% across all MI benchmarks.** Furthermore, our method compares favorably in terms of query budget. Our study highlights rising privacy threats for ML models even when minimal information (i.e., hard labels) is exposed. Our code, demo, models and reconstructed data are available at our project page: https://ngoc-nguyen-0.github.io/lokt/

## 1 Introduction

Model inversion (MI) attacks aim to infer and reconstruct sensitive private samples used in the training of models. MI and their privacy implications have attracted considerable attention recently . The model subject to MI is referred to as _target model_. There are three categories of MI attacks: (1) White-box MI, where complete target model information is accessible by the adversary ; (2) Black-box MI, where target model's soft labels are accessible ; (3) Label-only MI, where only target model's hard labels are accessible . This paper focuses on label-only MI, which is the most challenging setup as only limited information (hard labels) is available (Fig. 1).

In most existing work, MI attack is formulated as an optimization problem to seek reconstructions that maximize the likelihood under the target model . For DNNs, the optimization problems are highly non-linear. When the sensitive private samples are high-dimensional samples (e.g. faceimages), the optimizations are ill-posed, even in white-box setups. To overcome such issues, recent MI [1; 2; 3; 5; 10; 7; 6; 11] learn distributional priors from public data via GANs [14; 15; 16; 17], and solve the optimization problems over GAN latent space rather than the unconstrained image space. For example, MI attacks on face recognition systems could leverage GANs to learn face manifolds from public face images which have no identity intersection with private training images. White-box attacks based on public data and GANs have achieved remarkable success [1; 2; 3; 7; 11]. We follow existing work and recent label-only MI  and leverage public data in our method. Furthermore, similar to existing work, we use face recognition models as examples of target models.

**Research gap.** Different from white-box attack, study on label-only attack is limited despite its practical importance, e.g., many practical ML models only expose predicted labels. Focusing on label-only attack and with no knowledge of internal workings of target model nor its confidence score, BREPMI  takes a _black-box search_ approach to explore the search space iteratively (Fig. 1(a)). To seek reconstructions with high likelihood under target model,  proposes to query target model and observe the model's hard label predictions, and update search directions using _Boundary Repelling_ in order to move towards centers of decision regions, where high likelihood reconstructions could be found. However, black-box search in the high-dimensional latent space is extremely challenging.

**In this paper**, we propose a new approach for Label-Only MI attack using Knowledge Transfer (LOKT). Instead of performing a black-box search approach as demonstrated in  and directly searching high-likelihood reconstruction from the opaque target model (Fig. 1(a)), which could be particularly challenging for high-dimensional search space, we propose a different approach. Our approach aims to transfer the decision knowledge of the target model to surrogate models, for which complete model information is accessible. Subsequently, with these surrogate models, we could harness SOTA white box attacks to seek high-likelihood reconstructions (Fig. 1(b)). To obtain the surrogate models, we explore generative modeling [18; 19; 20; 21; 22]. In particular, we propose a new Target model-assisted ACGAN, T-ACGAN, which extends ACGAN  and leverages our unique problem setup where we have access to the predicted labels of the target model as shown in Fig. 1(d). In particular, by effectively leveraging the target model in discriminator/classifier training, we can explore _synthetic data_ for decision knowledge transfer from the target model to the surrogate model. With T-ACGAN capturing the data manifold of public samples, synthetic data is diverse and abundant. We hypothesize that such rich synthetic data could lead to improved decision knowledge transfer. Moreover, as training progresses, T-ACGAN generator learns to improve its conditional generative capabilities, enabling it to produce more balanced synthetic data for surrogate model learning. We explore several surrogate model designs. In one configuration, we employ the discriminator/ classifier of T-ACGAN as the surrogate model. In an alternative design, we utilize the generator of T-ACGAN to train different surrogate model variants. It's noteworthy that the generator of T-ACGAN can be readily employed for white-box attacks, and its conditional generation capabilities can effectively reduce the search space during inversion. **In addition, we perform analysis to support that our surrogate models are effective proxies for the opaque target model for MI.** (Fig. 1(e)). Overall, our T-ACGAN renders improved surrogate models, resulting in a significant boost in MI attack accuracy (Fig. 1(f)) and reduced number of queries compared to previous SOTA approach. **Our contributions are:**

* We propose LOKT, a new label-only MI by transferring decision knowledge from the target model to surrogate models and performing white-box attacks on the surrogate models (Sec. 4). Our proposed approach is the first to address label-only MI via white-box attacks.
* We propose a new T-ACGAN to leverage generative modeling and the target model for effective knowledge transfer (Sec. 4).
* We perform analysis to support that our surrogate models are effective proxies for the target model for MI (Sec. 5).
* We conduct extensive experiments and ablation to support our claims. Experimental results show that our approach can achieve significant improvement compared to existing SOTA MI attacks (Sec. 6). Additional experiments/ analysis are in Supplementary.

## 2 Related work

Model Inversion (MI) has particularly alarming consequences in security-sensitive domains, such as face recognition [24; 25; 26; 27], medical diagnosis [28; 29; 30]. Fredrikson et al.  introduces the first MI attack for simple linear regression models. Recently, several works extend MI for complex DNNs under different setups. For white-box setup,  proposes Generative Model Inversion (GMI) to leverage public data and GAN[32; 33] to constrain the search space.  proposes Knowledge-Enriched Distributional Model Inversion (KEDMI) to train an inversion-specific GAN for the attack.  proposes Variational Model Inversion (VMI) to apply variational objectives for the attack. Very recent work  proposes Pseudo Label-Guided MI (PLG-MI) to apply model's soft output to train a conditional GAN (cGAN) for white-box attack. LOMMA proposes a better objective function for MI and model augmentation to address MI overfitting. For black-box attack, where model's soft output is available,  proposes to train an inversion model and a decoder to generate target images using predicted scores of the inversion model.  proposes an adversarial approach for black-box MI. For label-only attack,  proposes BREPMI, the first label-only MI using a black-box Boundary Repelling search. See Supplementary for further discussion of related work.

## 3 Problem setup

Given a target model \(T\), the goal of MI is to infer private training data \(_{priv}\) by abusing access to model \(T\). More specifically, given a target class/ identity label \(y\), the adversary aims to reconstruct

Figure 1: _Overview and our contributions_. **(a)** Under Label-only model inversion (MI) attack, the Target model \(T\) is opaque. **(b) Stage 1:** As our first contribution, we propose a knowledge transfer scheme to render surrogate model(s). **(b) Stage 2:** Then, we cast the Label-only MI attack as a white-box MI attack on surrogate model(s) \(S\). **(c)** This casting can ease the challenging problem setup of label-only MI attack into a white-box MI attack. To our knowledge, our proposed approach is the first to address label-only MI via white-box MI attacks. **(d)** We propose T-ACGAN to leverage generative modeling and the target model for effective knowledge transfer to render surrogate model(s). Knowledge transfer renders \(D\) (Discriminator) as a surrogate model, and further generated samples of T-ACGAN can be used to train additional surrogate variant \(S\) (Sec. 4.3). **(e)** Our analysis demonstrates that \(S\) is an effective proxy for \(T\) for MI attack (details in Sec.5). In particular, white-box MI attack on \(S\) mimics the white-box attack on opaque \(T\). **(f)** Our proposed approach significantly improves the Label-only MI attack (e.g. \(\) 20% improvement in standard CelebA benchmark compared to existing SOTA ) resulting in significant improvement in private data reconstruction. Best viewed in color.

an image \(x\) which is similar to the images of class \(y\) in \(_{priv}\). Most MI formulate the inversion as optimization problems to seek the highest likelihood reconstructions for identity \(y\) under \(T\). As direct searching for \(x\) in the unconstrained image space is ill-posed, many MI attacks [1; 2; 3; 7; 6] leverage public dataset \(_{pub}\) that is the same domain as \(_{priv}\), e.g., \(_{priv}\) and \(_{pub}\) are facial image datasets. GAN  is applied to learn distributional prior from \(_{pub}\), and the adversary searches the GAN latent space instead of the unconstrained image space for high-likelihood reconstructions under \(T\):

\[_{z} P_{T}(y|G(z))\] (1)

Here, \(G\) is the generator, and \(P_{T}(y|)\) is the likelihood of an input for identity \(y\) under target model \(T\). White-box attacks apply gradient ascent and some regularization [1; 2; 3; 7] to solve Eq. 1, whereas label-only attack BREPMI  applies black-box search to tackle Eq. 1. In this paper, we also tackle Eq. 1 under label-only setup, i.e. only the predicted label is available.

## 4 Approach

Our proposed label-only MI consists of two stages. In stage 1, we learn surrogate models. In stage 2, we apply SOTA white-box attack on the surrogate models. To learn surrogate models, we explore an approach based on GAN and propose a new Target model-assisted ACGAN (T-ACGAN) for effective transfer of decision knowledge. Our T-ACGAN learns the generator \(G\) and the discriminator \(D\) with classifier head \(C\). In one setup, we directly take \(C D\) as the surrogate model*. In another setup, we apply \(G\) to generate synthetic data to train another surrogate model \(S\) or an ensemble of \(S\). Then, we apply SOTA white-box attack on \(C D\), \(S\) or the ensemble of \(S\). In our experiments, we show that using \(C D\) in a white-box attack can already outperform existing SOTA label-only attack. Using \(S\) or an ensemble of \(S\) can further improve attack performance. The \(G\) obtained from our T-ACGAN can be readily leveraged in the attack stage.

Footnote *: With a slight abuse of notation we use \(D\) to represent the entire discriminator and the discriminator up to and including the penultimate layer in the context of \(C D\).

### Baseline

Before discussing our proposed approach, we first discuss a simple baseline for comparison. Given the public data, one could directly use the target model \(T\) to label the data and learn the surrogate model \(S\). For \(x_{p}_{pub}\), we construct \((x_{p},)\), where \(=T(x_{p})\) is pseudo label of _private_ identity. We obtain the dataset \(}_{pub}\) with samples \((x_{p},)\), i.e. \(}_{pub}\) is the public dataset with pseudo labels. We apply \(}_{pub}\) to train \(S\). However, this algorithm suffers from class imbalance. In particular, some private identities could have less resemblance to \(x_{p}_{pub}\). As a result, for some \(\), there is only a small number of \(x_{p}\) classified into it, and \(}_{pub}\) is class imbalanced. When using \(}_{pub}\) to train \(S\), minority classes may not gain adequate decision knowledge under \(T\) and could perform sub-optimally. In our experiments, we also apply techniques to mitigate the class imbalance in \(}_{pub}\). However, the performance of this baseline approach is inadequate as we will show in the experiments.

### Review of ACGAN

In standard ACGAN , we are given a real training dataset with label, i.e., \(_{real}\) with samples \((x_{r},y)\). The generator \(G\) takes a random noise vector \(z\) and a class label \(y\) as inputs to generate a fake sample \(x_{f}\). The discriminator \(D\) outputs both a probability distribution over sources \(P(s|x)=D(x)\), where \(s\{Real,Fake\}\), and a probability distribution over the class labels, i.e., \(P(c|x)=C D(x)\), and \(c\) is one of the classes. For real training sample \(x_{r}\) of label \(y\) and fake sample \(x_{f}=G(z,y)\) with conditional information \(y\), the loss functions for \(D\), \(C\) and \(G\) are:

\[_{D,C} =-E[ P(s=Fake|x_{f})]-E[ P(s=Real|x_{r})]\] \[-E[ P(c=y|x_{f})]-E[ P(c=y|x_{r})]\] (2) \[_{G} =E[ P(s=Fake|x_{f})]-E[ P(c=y|x_{f})]\] (3)

### Our Proposed T-ACGAN and Learning of Surrogate Model

Unlike standard ACGAN setup where we have access to labelled data \(_{real}\) with samples \((x_{r},y)\), in our setup, we have access to real public data without label: \(_{pub}\) with samples \(x_{p}\). _Importantly, we can leverage the target model \(T\) to provide pseudo labels for generated samples \(x_{f}=G(z,y)\), which are diverse and abundant._ Our proposed T-ACGAN aims to take advantage of \(T\) to provide more diverse and accurate pseudo labelled samples during the training.

\(D\) and \(C\) Learning.Our T-ACGAN leverages \(T\) to assign pseudo labels to the diverse generated samples \(x_{f}=G(z,y)\), i.e., \(=T(x_{f})\). We apply samples \(x_{p}\) and \((x_{f},)\) to learn \(D\) and \(C\):

\[_{D,C} =-E[ P(s=Fake|x_{f})]-E[ P(s=Real|x_{p})]\] (4) \[-E[ P(c=|x_{f})]\]

In Eq. 4, the term \(E[ P(c=|x_{f})]=E[ P(c=|G(z,y))]\) is different from ACGAN and may look intriguing. Instead of using \(y\) as class supervision to train \(D\) and \(C\) as in ACGAN (Eq. 2), our T-ACGAN takes advantage of \(T\) to apply \(=T(G(z,y))\) to train \(D\) and \(C\), as \(\) is more accurate conditional information compared with \(y\) especially during the initial epochs. _With Eq. 4, our method transfers the decision knowledge of \(T\) into \(D\) and \(C\) via diverse generated samples._ Furthermore, as we can generate diverse pseudo labeled samples \((x_{f},)\) using \(T\) and \(G\), pseudo labelled data based on \(x_{p}\) can be omitted. In our experiment, we show that we can achieve good performance using diverse samples \((x_{f},)\). In T-ACGAN, we utilize public data \(x_{p}\) only for real/fake discrimination.

\(G\) Learning.We follow ACGAN training for \(G\), i.e. Eq. 3. With \(D\) and \(C\) trained with decision knowledge of \(T\) in the above step, they provide feedbacks to \(G\) to improve its conditional generation _in the private label space of \(T\)_. In our experiment, we analyze \(y\) in \(x_{f}=G(z,y)\) and \(=T(x_{f})\). As training progresses, \(G\) improves its conditional generation, and \(y\) and \(\) become more aligned. Note that, as \(T\) outputs only hard labels, \(T\) cannot be readily applied to provide feedback for \(G\) learning.

Surrogate Model.With alternating \(D\) and \(C\) learning and \(G\) learning, we obtain \(D\), \(C\) and \(G\). We explore three methods to obtain the surrogate model. \(\) (i) We directly take \(C D\) in T-ACGAN as the surrogate model and apply a white-box attack on \(C D\). This can be justified as \(C D\) is trained based on decision knowledge of \(T\) to classify a sample into identities of private training data. \(\) (ii) We apply \(G\) of T-ACGAN to generate dataset \(}_{fake}\) with samples \((x_{f},)\), where \(x_{f}=G(z,y)\) and \(=T(x_{f})\). We apply \(}_{fake}\) to train another surrogate model \(S\). \(\) (iii) We use the same dataset \(}_{fake}\) in (ii) to train an ensemble of \(S\) of different architectures. As pointed out in , using an ensemble of \(S\) could improve white-box attack performance.

White-box Attack.With surrogate model \(C D\), \(S\) or an ensemble of \(S\), any white-box attack can be applied. In our experiments, we show that our surrogate models are effective across a range of white-box attacks (See the Supplementary). Furthermore, \(G\) in T-ACGAN can be readily leveraged for performing the attack. Particularly, based on \(G(z,y)\) obtained in the above steps, we could reduce the search space during inversion to the latent region corresponding to the target identity \(y\), leading to more efficient search and improved attack accuracy .

## 5 Analysis for justification of surrogate models

In this section, we provide an analysis to justify why our surrogate model could be an effective proxy for \(T\) under MI, i.e., _the results of white-box MI attack on our surrogate model be good approximation to that of white-box MI attack on \(T\)_. Note that results of white-box MI on \(T\) cannot be obtained directly as \(T\) exposes only hard labels. To simplify the presentation, we focus our discussion on \(S\). As discussed in Sec. 3, most MI attacks formulate inversion as an optimization problem of seeking reconstructions that achieve highest likelihood under target model. Therefore, when we carry out MI on \(S\) with SOTA white-box approaches, we expect to obtain high-likelihood reconstructions under \(S\) (or high-likelihood generated samples of GAN under \(S\), see Eq. 1). We use \(P_{S}\) and \(P_{T}\) to denote likelihood of a sample under \(S\) and \(T\) respectively.

In what follows, we provide analysis to support that \(S\) based on our approach would possess an important property of good proxy for \(T\). Property **P1:**_For high-likelihood samples under \(S\), it is likely that they also have high likelihood under \(T\)_. See Fig. 1(e) for distribution of generated samples' \(P_{T}\) conditioning on those with high \(P_{S}\). It can be observed that many have high \(P_{T}\). Particularly, it isuncommon for high-likelihood samples under \(S\) to have low likelihood under \(T\) (see Fig. 1(e) only a few samples have low \(P_{T}\)).

With Property**P1**, the result obtained by white-box on \(S\) (which is a high likelihood sample under \(S\)) is likely to have a high likelihood under \(T\) and could be a good approximation to the result of white-box on \(T\) (which is a high likelihood sample under \(T\)). In Fig. 1(e), **P1** can be clearly observed+. Therefore, \(S\) using our approach would possess **P1** and would be a good proxy for \(T\) for MI.

Footnote †: Fig. 1(e) are \(P_{T}\) and \(P_{S}\) of \(x_{f}=G(z,y)\) from our T-ACGAN. More details in Supp.

**Why would \(S\) possess property**P1?** This could be intriguing. After all, \(T\) does not expose any likelihood information. The labels of samples assigned by \(T\) are the only information available to \(S\) during training of \(S\). It does not appear that \(S\) can discern low or high-likelihood samples under \(T\).

To discuss why \(S\) would possess **P1**, we apply findings and analysis framework of Arpit et al.  regarding the general learning dynamics of DNNs.  presents a data-centric study of DNN learning with SGD-variants. In , "easy samples" are ones that fit better some patterns in the data (and correspondingly "hard samples"). The easy and hard samples exhibit high and low likelihoods in DNNs resp. as discussed in . Furthermore, an important finding from  is that, in DNNs learning, the models learn simple and general patterns of the data _first_ in the training stage to fit the easy samples.

We apply the framework of  to understand our learning of \(S\) and the reason why \(S\) would possess **P1**. Fig. 2(a) illustrates easy and hard samples in our problem: patterns of face identities can be observed in some samples (easy samples), while other samples (hard samples) exhibit diverse appearance. Similar to , Fig. 2(b) shows that these easy and hard face samples tend to have high and low likelihood under \(T\). Fig. 2(c) shows the learning of \(S\) on these easy and hard samples at different epochs. Consistent with the _"DNNs Learn Patterns First"_ finding in , \(S\) learns general identity patterns first to fit the easy samples. Therefore, \(P_{S}\) of easy samples improve at a faster pace in the training, and many of them achieve high \(P_{S}\). As easy samples tend to have high \(P_{T}\), we observe **P1** in \(S\). For the hard samples (which tend to have low \(P_{T}\)), it is uncommon for \(S\) to achieve high likelihood on them as they do not fit easily to the pattern learned by \(S\).

## 6 Experiments

In this section, we present extensive experiment results and ablation studies: (i) We show that our proposed T-ACGAN can lead to better surrogate models compared to alternative approaches (Sec. 6.2). (ii) We show that our proposed approach LOKT can significantly outperform the existing SOTA label-only MI attack (Sec. 6.3). (iii) We present additional results (Sec. 6.4) to demonstrate the efficacy of LOKT against SOTA MI defense methods. We further show that LOKT compares favorably in terms of query budget compared to existing SOTA. **Additional experiments/analysis provided in Supplementary.**

### Experimental Setup

To ensure a fair comparison, we adopt the exact experimental setup used in BREPMI . In what follows, we provide details of the experimental setup.

**Dataset.** We use three datasets, namely CelebA , Face-scrub , and Pubfig83 . We further study Label-Only MI attacks under distribution shift using FFHQ dataset  which contains images that vary in terms of background, ethnicity, and age. Following [2; 6], we divide each dataset (CelebA/ Facescrub/ Pubfig83) into two non-overlapping sets: private set \(_{priv}\) for training the target model \(T\), and public set \(_{pub}\) for training GAN/T-ACGAN. More details on datasets and attacked identities can be found in Supplementary.

**Target Models.** Following [2; 6], we use 3 target models \(T\) including VGG16 , IR152 , and FaceNet64 . All target models are provided in [2; 6].

  _{priv}\)} &  \\   & **Architecture** & **\# classes** \\   & FaceNet64  &  \\  & IR152  & \\   & VGG16  & \\   & BiD-HSIC  & \\    & MID  & \\  Facescrub & FaceNet64  & 200 \\    & Pubfig83 & FaceNet64 & 50 \\  

Table 1: Details of target model \(T\). To showcase the effectiveness of our proposed method, we conduct a comprehensive set of 30 experiments, covering 10 different setups.

Additionally, we use the following methods/ models for evaluating the attack performance under SOTA MI defense methods: \(\) BiDO-HSIC 3. \(\) MID 4. The details are included in Table 1.

**Evaluation Metrics.** Following , we use the following metrics to quantitatively evaluate the performance of MI attacks. Further, we also conduct user studies to assess the quality of reconstructed data (Sec. 6.5).

* _Attack Accuracy (Attack acc.)_: Following , we utilize an evaluation model, \(E\), which employs a distinct architecture and is trained on \(_{priv}\)4. \(E\) serves as a proxy for human inspection . Higher attack accuracy indicates superior performance. * _KNN Distance (KNN dt.)_: The KNN distance indicates the shortest distance between the reconstructed image of a specific identity and its private images. Specifically, the shortest distance is computed using the \(l_{2}\) distance in the feature space, using the evaluation model's penultimate layer.

Figure 2: We apply the framework of  to analyze learning dynamics of \(S\) to reason why \(S\) possesses property P1, and therefore could be an effective proxy for \(T\) under MI. We analyze generated samples \(x_{f}\) from our T-ACGAN for 3 identities (IDs 20, 16, 36). Note that \(x_{f}\) analysis is relevant as generated samples are used in MI attacks. **(a)**: We analyze face embeddings of \(x_{f}\) extracted from publicly available SOTA face recognition model here. Different clusters and different distances from cluster centroids can be observed, suggesting patterns of face identities in some samples (easy samples) while diverse appearance in other samples (hard samples). We use distances from centroids to identify easy samples \(x^{e}_{f}\) and hard samples \(x^{h}_{f}\) (easy samples are indicated using transparent blue circle for each ID in the visualization). Visualization of \(x^{e}_{f}\) and \(x^{h}_{f}\) in image space further demonstrates identity patterns in \(x^{e}_{f}\) and diverse appearance in \(x^{h}_{f}\). **(b)**: Similar to , we observe that \(x^{e}_{f}\) and \(x^{h}_{f}\) tend to have high and low likelihood under \(T\) (\(P_{T}\)) resp (training data). **(c)**: We track likelihood under \(S\) (\(P_{S}\)) for \(x^{e}_{f}\) and \(x^{h}_{f}\) during the training of \(S\). As training progresses, \(P_{S}\) of \(x^{e}_{f}\) and \(x^{h}_{f}\) improve, and samples move up vertically (note that \(P_{T}\) of samples do not change). Consistent with the _“DNNs Learn Patterns First”_ finding in , \(S\) learns general identity patterns first to fit the easy samples. Therefore, \(P_{S}\) of \(x^{e}_{f}\) improve at a faster pace in the training, and many of them achieve high \(P_{S}\) at epoch = 200. As \(x^{e}_{f}\) tend to have high \(P_{T}\), we observe property P1 in \(S\). For \(x^{h}_{f}\) (many of them tend to have low \(P_{T}\)), it is uncommon for \(S\) to achieve high likelihood on them as they do not fit easily to the pattern learned by \(S\). **See Supplementary for additional details and analysis.** Best viewed in color.

A smaller KNN distance signifies that the reconstructed images are more closely aligned with the private images of the target identity.

### Training surrogate model with different algorithms

In this section, we demonstrate that our proposed T-ACGAN can lead to better surrogate models for MI. We describe a set of alternative approaches that can be used to train surrogate models using \(_{pub}\) and compare the performance of these approaches with our proposed method. Specifically, we consider a set of five algorithms, which can be broadly classified into three categories, for learning the surrogate model \(S\):

* **Directly use the public dataset \(_{pub}\).** We present two methods to train \(S\): \(\)**Direct I.** We train \(S\) using the public dataset labelled with target model, _i.e._\(}_{pub}\) with samples \((x_{p},)\), \(x_{p}_{pub}\), \(=T(x_{p})\); see Sec. 4.1. \(\)**Direct II.** We apply data augmentation to \(}_{pub}\) of Direct I to reduce the class imbalance in Direct I, followed by training \(S\) using the newly more balanced dataset.
* **Training an ACGAN.** We provide two versions: \(\)**ACGAN I.** We train an ACGAN model on \(}_{pub}\) used in Direct I. \(\)**ACGAN II.** We train an ACGAN model on augmented \(}_{pub}\) used in Direct II. As \(C D\) in ACGAN serves as a classifier, we use \(C D\) for MI attacks.
* **Training proposed T-ACGAN.** We use our proposed method described in Section 4.3 to train T-ACGAN. Similar to ACGAN I and II, we use \(C D\) after training T-ACGAN for the attack.

For this comparison, we utilize the following settings: \(T\) = FaceNet64, \(_{priv}\) = CelebA, \(_{pub}\) = CelebA. Both ACGAN and T-ACGAN adopt the SNResnet architecture [34; 34]. To ensure a fair comparison, we use the same architecture as \(C D\) in ACGAN and T-ACGAN for the surrogate model \(S\) in Direct I and Direct II. Detailed architecture specifications can be found in the Supplementary. After training the models, we employ the widely-used KEDMI  as the white-box attack on the trained surrogate models. Table 2 presents the results. The effectiveness of T-ACGAN in training surrogate models for MI attacks can be observed.

### Comparison against SOTA label-only MI attack

**Standard MI attack setup.** In this section, we present the results obtained from the standard attack setup on three datasets: CelebA, Facescrub, and Pubfig83, as detailed in Table 3. We evaluate three designs of surrogate: \(\) (i) We directly use \(C D\) from our T-ACGAN as the surrogate model. The architecture of T-ACGAN can be found in the supplementary material. \(\) (ii) We utilize the synthetic data generated by \(G\) of our T-ACGAN and label it using the target classifier \(T\) to train another surrogate model, denoted as \(S\) = Densenet-161 . \(\) (iii) We employ the same data as in (ii) to train an ensemble of surrogate models, denoted as \(S_{en}\), using different architectures including Densenet-121, Densenet-161, and Densenet-169.

We compare our results with the state-of-the-art (SOTA) label-only MI attack BREPMI . To conduct our attacks, we utilize white-box PLGMI  on the surrogate models. Since PLGMI performs attacks using a conditional GAN trained with white-box access of the target classifier, we replace it with our T-ACGAN, which becomes available for use after training the surrogate models.

Our proposed method LOKT demonstrates a significant improvement in Attack accuracy and KNN distance compared to the SOTA label-only MI attack BREPMI . Our top 1 attack accuracies are better than BREPMI from from 17.2% to 29.87% across all setups when we utilize the ensemble \(S_{en}\).

Fig. 1 (f) presents a visual comparison of various methods under the setup \(_{priv}=\) CelebA, \(_{pub}\) = CelebA. **More results are available in the Supplementary**. Results clearly indicate that LOKT produces images that are closer to the ground truth (private data) compared to BREPMI . This outcome provides strong evidence of the effectiveness of our approach in generating realistic images that closely resemble private data, which is critical for conducting successful MI attacks.

**MI attacks under large distribution shift.** Table 3 compares the MI attack results in the large distribution shift setup, where we use \(_{pub}\) = FFHQ, \(_{priv}\) = CelebA/ Facescrub/ Pubfig83, and

 
**Algorithm** & **Attack acc. \(\)** & **KNN dt. \(\)** \\  Direct I & 5.87 \(\) 1.65 & 1936.12 \\ Direct II & 9.60 \(\) 2.22 & 1890.16 \\  ACGAN I & 6.47 \(\) 2.15 & 1717.26 \\ ACGAN II & 7.87 \(\) 3.10 & 1785.20 \\  T-ACGAN & 42.07 \(\) 3.46 & 1473.99 \\  

Table 2: We compare different approaches to train surrogate model for MI attacks. We utilize the following settings: \(T\) = FaceNet64, \(_{priv}=\) CelebA, \(_{pub}=\) CelebA, and employ the KEDMI for MI attacks.

[MISSING_PAGE_FAIL:9]

(smaller KNN distance). We believe our study can provide new insight on the effectiveness of SOTA label-only attack at a higher resolution of 128\(\)128, paving the way to future label-only model inversion attacks at resolutions beyond 128\(\)128.

**Query budget.** In this experiment, we compare query budget between our proposed method and BREPMI . In the BREPMI, queries to the target classifier \(T\) are required to identify the initial points for attacking and estimate the gradients during the attack. In our method, queries to \(T\) are required to label the synthetic data during the training of T-ACGAN to obtain \(C D\), and additional 500k queries to label generated images of T-ACGAN to train \(S\) and the ensemble \(S_{en}\). For comparison, as shown in Table 5, we use \(_{priv}=\) and \(_{pub}=\). The results show that our proposed method requires 30% fewer queries compared to BREPMI.

### User study

**User study setup.** In this section, we go beyond objective metrics and consider subjective evaluation of MI attacks. In particular, we conduct a human study to understand the efficacy of our proposed method, LOKT, compared to BREPMI. We follow the setup by  for human study and use Amazon Mechanical Turk (MTurk) for experiments. The user interface is provided in the Supplementary. In this study, users are shown 5 real images of a person (identity) as reference. Then users are required to compare the 5 real images with two inverted images: one from our method (LOKT), the other from BREPMI. We use \(D_{priv}\) = CelebA, \(D_{pub}\) = CelebA and \(T\) = FaceNet64. Following , we randomly selected 50 identities with 10 unique users evaluating each task accounting to 1000 comparison pairs.

**User study results.** We report the user study results in Table 7. Our human study reveals that users distinctly favor our approach, with 64.30% user preference for images reconstructed using our proposed approach, in contrast to BREPMI's lower 35.70% user preference. These subjective evaluations further show the efficacy of our proposed method, LOKT, in the challenging label-only MI setup.

## 7 Discussion

**Conclusion.** Instead of performing a black-box search approach as in existing SOTA, we propose a new label-only MI approach (LOKT) by transferring decision knowledge from the target model to surrogate models and performing white-box attacks on the surrogate models. To obtain the surrogate models, we propose a new T-ACGAN to leverage generative modeling and the target model for effective knowledge transfer. Using findings of general learning dynamics of DNNs, we conduct analysis to support that our surrogate models are effective proxies for the target model under MI. We perform extensive experiments and ablation to support our claims and demonstrate significant improvement over existing SOTA.

**Broader Impacts.** Understanding model inversion attacks holds significance as AI models continue to see widespread deployment across various applications. By studying and understanding the approaches and methodologies for model inversion, researchers can develop good practices in deploying AI models and robust defense mechanisms for different applications esp. those involving sensitive training data. It is important to emphasize that the objective of model inversion research is to raise awareness of potential privacy threats and bolster our collective defenses.

**Limitations.** While our experiments are extensive compared to previous works, practical applications involve different types of private training datasets such as healthcare data. Nevertheless, our assumptions are general, and we believe our findings can be applied to a broader range of applications.

 
**Setup** & **Attack** & **Attack acc.** \(\) & **KNN dr.** \(\) \\  \(T\) = IR152 & BREPMI & 50.33 \(\) 4.71 & 1389.09 \\ \(_{priv}\) = CelebA &  & \(C D\) & 66.87 \(\) 3.93 & 1356.53 \\ \(_{pub}\) = CelebA & & \(S\) & 66.80 \(\) 3.83 & 1341.04 \\  & \(S_{en}\) & **70.60 \(\) 4.43** & **1320.16** \\  

Table 6: We conduct the experiment with higher resolution images. We use \(T\) = Resnet-152, \(_{priv}\) = CelebA, \(_{pub}\) = CelebA, image size = 128\(\)128. The natural accuracy of \(T\) is 86.07%. We highlight the best results in **bold**.

 
**Method** & **User Preference (\(\))** \\  BREPMI & 35.70\% \\ LOKT & **64.30**\% \\  

Table 7: User study results. Our human study reveals that users distinctly favor our approach, with 64.30% user preference for images reconstructed using our proposed approach, LOKT, compared to BREPMI’s lower 35.70% user preference.

**Acknowledgements.** This research is supported by the National Research Foundation, Singapore under its AI Singapore Programmes (AISG Award No.: AISG2-TC-2022-007) and SUTD project PIE-SGP-AI-2018-01. This research work is also supported by the Agency for Science, Technology and Research (A*STAR) under its MTC Programmatic Funds (Grant No. M23L7b0021). This material is based on the research/work support in part by the Changi General Hospital and Singapore University of Technology and Design, under the HealthTech Innovation Fund (HTIF Award No. CGH-SUTD-2021-004). We thank anonymous reviewers for their insightful feedback.