# Warm-starting Push-Relabel

Sami Davies\({}^{*}\), Sergei Vassilvitskii\({}^{}\), Yuyan Wang\({}^{}\)

\({}^{*}\)UC Berkeley, \({}^{}\)Google Research - New York

davies@berkeley.edu, {sergeiv,wangyy}@google.com

###### Abstract

Push-Relabel is one of the most celebrated network flow algorithms. Maintaining a pre-flow that saturates a cut, it enjoys better theoretical and empirical running time than other flow algorithms, such as Ford-Fulkerson. In practice, Push-Relabel is even faster than what theoretical guarantees can promise, in part because of the use of good heuristics for seeding and updating the iterative algorithm. However, it remains unclear how to run Push-Relabel on an arbitrary initialization that is not necessarily a pre-flow or cut-saturating. We provide the first theoretical guarantees for warm-starting Push-Relabel with a predicted flow, where our learning-augmented version benefits from fast running time when the predicted flow is close to an optimal flow, while maintaining robust worst-case guarantees. Interestingly, our algorithm uses the _gap relabeling heuristic_, which has long been employed in practice, even though prior to our work there was no rigorous theoretical justification for why it can lead to run-time improvements. We then provide experiments that show our warm-started Push-Relabel also works well in practice.

## 1 Introduction

Maximum flow is a fundamental problem in combinatorial optimization. It admits many algorithms, from the famous Ford-Fulkerson algorithm  which employs augmenting paths, to recent near-linear time scaling based approaches . In practice, however, the _Push-Relabel_ family of algorithms remains one of the most versatile and is often the standard benchmark to which new flow algorithms are compared .

Designed by Goldberg and Tarjan , the core Push-Relabel algorithm (Algorithm 1) has running time \(O(n^{2}m)\), where \(n\) and \(m\) are the number of vertices and edges in the network. Given the popularity of max-flow as a subroutine in large-scale applications , it is no surprise that improving running times has been a subject of a lot of study, with multiple heuristic methods being developed .

To complement these heuristics, researchers studied max-flow in the _algorithms with predictions_ framework (or learning-augmented algorithms) . Two groups initiated the study and proved that the running time of the Edmonds-Karp selection rule for Ford Fulkerson can be improved from \(O(m^{2}n)\) to \(O(m||f^{*}-||_{1})\), where \(f^{*}\) is an optimal flow on the network and \(\) is a predicted flow . These algorithms start the augmenting path algorithms from a feasible flow obtained from the predicted flow, then bound the number of augmentations by the \(_{1}\)-distance between the predicted and maximum flows.

While these works have improved upon the cold-start, non learning-augmented versions, it is important to note that they have improved upon sub-optimal algorithms for max flow. In this work, we show how to warm-start Push-Relabel, whose cold-start version is nearly state-of-the-art for the maximum flow problem. This directly addresses the challenge specified in  on bringing a rigorous analysis for warm-starting non-augmenting path style algorithms. In the process of doing so, we provide a theoretical explanation for the success of the popular _gap relabeling heuristic_ in improvingthe running time of Push-Relabel algorithms. Specifically, both the gap relabeling heuristic and our algorithm maintain a cut with _monotonically decreasing t-side nodes_ (see Section 1.2 for more), which directly leads to improved running times for our version of Push-Relabel and the gap relabeling heuristic. Lastly, we show that our theory is predictive of what happens in practice with experiments on the image segmentation problem.

### Preliminaries

**Graph, flow and cut concepts.** Our input is a network \(G=(V,E)\), where each directed edge \(e E\) is equipped with an integral capacity \(c_{e}_{ 0}\). Let \(|V|=n\) and \(|E|=m\). \(G\) contains nodes \(s\), the source, and \(t\), the sink. \(G\) is connected: \( u V\), there are both \(s-u\) and \(u-t\) paths in \(G\). A flow \(f_{ 0}^{m}\) is feasible if it satisfies: (1) _flow conservation_, meaning any \(u V\{s,t\}\) satisfy \(_{(v,u) E}f_{e}=_{(u,w) E}f_{e}\); (2) _capacity constraints_, meaning for all \(e E\), \(f_{e} c_{e}\). Our goal is to find the maximum flow, i.e. one with the largest amount of flow leaving \(s\).

We call \(f\) a _pseudo-flow_ if it satisfies capacity constraints only. A node \(u V\{s,t\}\) is said to have _excess_ if it has more incoming flow than outgoing, i.e., \(_{(v,u) E}f_{e}>_{(u,w) E}f_{e}\); analogously it has _deficit_ if its outgoing flow is more than ingoing. We denote the excess and deficit of a node \(u\) with respect to \(f\) as \(_{f}(u)=\{_{(v,u) E}f_{e}-_{(u,w) E}f_{e},0\}\) and \(_{f}(u)=\{_{(u,w) E}f_{e}-_{(v,u) E}f_{e},0\}\), where at most one can be positive. A pseudo-flow can have both excesses and deficits, whereas a _pre-flow_ is a pseudo-flow with excess only.

For a pseudo-flow \(f\), the _residual graph_\(G_{f}\) is a network on \(V\); for every \(e=(u,v) E\), \(G_{f}\) has edge \(e\) with capacity \(c^{}_{e}=c_{e}-f_{e}\) and a backwards edge \((v,u)\) with capacity \(f_{e}\). Let \(E(G_{f})\) denote the edges in \(G_{f}\). The value of a pseudo-flow \(f\) is \((f)=_{e=(s,u)}f_{e}\), the total flow going out of \(s\). Notice that this is not necessarily equivalent to the total flow into \(t\) since flow conservation is not satisfied. A _cut-saturating_ pseudo-flow is one that saturates some \(s-t\) cut in the network. Push-Relabel maintains a cut-saturating pre-flow; equivalently, there is no \(s-t\) path in the residual graph of the pre-flow. We use \((S,T)\) to denote an \(s-t\) cut between two sets \(S\) and \(T\). Note that the cut induced by any cut-saturating pseudo-flow \(f\) can be found by taking \(T=\{u V: u-tG_{f}\}\) (including \(t\)) and \(S= T\).

**Prediction.** The prediction that we will use to seed Push-Relabel is some \(_{ 0}^{m}\), which is a set of integral values on each edge. Observe that one can always cap the prediction by the capacity on every edge to maintain capacity constraints, so throughout this paper we will assume \(\) is a pseudo-flow. It is important to note that our predicted flow is _not_ necessarily feasible or cut-saturating, and part of the technical challenge is making use of a good predicted flow despite its infeasibility.

**Error metric.** We measure the error of a predicted pseudo-flow \(\) on \(G\). The smaller the error is, the higher quality the prediction is, and the less time Push-Relabel seeded with \(\) should take. A pseudo-flow becomes a maximum flow when it is both feasible and cut-saturating. Hence, the error measures how far \(\) is from being cut-saturating while being feasible. We say that a pseudo-flow \(\) is \(\)_far from being cut-saturating_ if there exists a feasible flow \(f^{}\) on \(G_{}\) where \((f^{})\) and \(+f^{}\) is cut-saturating on \(G\) (though the cut does not have to be a min-cut). To measure how far \(\) is from being feasible, we sum up the total excesses and deficits. In total we use the following error metric:

**Definition 1**.: _For pseudo-flow \(\) on network \(G\), the error of \(\) is the smallest integer \(\) such that (1) \(\) is \(\) far from being cut-saturating and (2) \(_{u V\{s,t\}}_{}(u)+_{}(u)\)._

If \(=0\), \(\) is the max-flow and the cut that is saturated is the min-cut. The previously studied error metric for predicted flows, such as by  and , was \(||f^{*}-||_{1}\), for any max-flow \(f^{*}\).

PAC-learnability is the standard to justify that the choice of prediction and error metric are reasonable. Flows are PAC-learnable with respect to the \(_{1}\)-norm . Our results hold replacing our error metric with the \(_{1}\)-norm because our metric provides a more fine-grained guarantee than the \(_{1}\)-norm (i.e., if a prediction \(\) has error \(\), then \(||f^{*}-||_{1}\)). Thus we can omit any theoretical discussion of learnability. We present this work with respect to our error metric as we find the \(_{1}\) error metric to be unintuitive, as it is not really very descriptive of how good a predicted flow is.

Push-Relabel.The "vanilla" Push-Relabel algorithm maintains a pre-flow and set of valid _heights_ (also called labels) on nodes. Heights \(h:V_{>0}\) are _valid_ for \(f\) if for every edge \((u,v) E(G_{f})\) with positive capacity, \(h(u) h(v)+1\), and if \(h(s)=n\) and \(h(t)=0\). An edge \((u,v) E(G_{f})\) is called _admissible_ if \(h(u)=h(v)+1\) and \(c^{}_{(u,v)}>0\), which means we can push flow from \(u\) to \(v\). The algorithm starts with \(f^{}\), where \(f^{}_{e}=c_{e}\) for all \(e=(s,u)\) and otherwise \(f^{}_{e}=0\). It then pushes flow on admissible edges \((u,v)\) for \(u\) with excess flow when possible, and otherwise the algorithm updates the heights. See Algorithm 1.

``` Input: Network \(G\)  Define \(f_{e}=c_{e}\) for \(e=(s,u)\) and \(f_{e}=0\) for all other \(e\)  Define \(h(u)=0\) for all \(u V\{s\}\) and \(h(s)=n\)  Build residual network \(G_{f}\) while\(\) node \(u\) with \(_{f}(u)>0\)do if\(\) admissible \((u,v) E(G_{f})\) with \(f_{(u,v)}<c_{(u,v)}\)then  Update \(f\) by sending an additional flow value of \(\{_{f}(u),c^{}_{(u,v)}\}\) along \((u,v)\)  Update \(G_{f}\) accordingly else update \(h(u)=1+_{v:(u,v) E(G_{f})}h(v)\) Output:\(f\) ```

**Algorithm 1** Push-Relabel

All heights in Push-Relabel are bounded.

**Lemma 1**.: _For a pre-flow \(f\) on network \(G\), every node \(u\) with \(_{f}(u)>0\) has a path in \(G_{f}\) to \(s\). Further, for \(d(u,v)\) the length of the shortest path from \(u\) to \(v\) in \(G_{f}\), any valid heights in Push-Relabel (Algorithm 1) satisfy \(h(u) h(v)+d(u,v)\). For all \(u V\), \(h(u) h(s)+n=2n\)._

At any point of the algorithm, the \(s-t\) cut maintained can be found using the heights.

**Lemma 2**.: _For a pseudo-flow \(f\) with valid heights \(h\) on network \(G\), let \(\) be the smallest positive integer such that \(\{h(u)\}_{u V}\). Then sets \(S=\{u V:h(u)>\}\) and \(T=\{u V:h(u)<\}\) form a cut saturated by \(f\)._

We call this cut _induced by the heights_. Indeed, such a threshold \(\) can be found because \(\{h(u)\}_{u V}\) has at most \(n\) different values, but \(h(s)=n\) and \(h(t)=0\), so among the \(n+1\) values \(\{0,1,,n\}\), at least one is not in the set. Note that \((S,T)\) is a saturated cut by definition of valid heights. A saturated cut can also be defined by whether a node can reach the sink in the residual graph.

**Lemma 3**.: _For any pseudo-flow \(f\) on network \(G\), let \(T\) be all nodes that can reach \(t\) in \(G_{f}\) (including \(t\)) and \(S=V T\). If \(s S\), then \((S,T)\) is a saturated \(s-t\) cut._

Lemmas 2 and 3 apply to all pseudo-flows, whereas vanilla Push-Relabel must take a pre-flow as input. Before this work, it was unclear how to seed Push-Relabel with anything other than \(f^{}\).

With the _gap relabeling heuristic_, whenever there is some integer \(0<<n\) with no nodes at height \(\), then nodes with height between \(\) and \(n\) have their height increased to \(n\). See Algorithm 2.

### Technical contribution

We first study Push-Relabel with the gap relabeling heuristic when seeded with a prediction that is a cut-saturating pre-flow with error \(\). We prove Algorithm 2 finds an optimal solution in time \(O( n^{2})\). The running time also holds for cold-start versions of the algorithm when the max-flow/min-cut value is known to be bounded by \(\). This is (1) the first theoretical analysis of the gap relabeling heuristic, and, (2) the first result showing a running time bounded by the volume of the cut in Push-Relabel. While Ford-Fulkerson admits a naive running time bound of \(O( m)\) when the max-flow value is at most \(\), an analogous claim was not previously known for Push-Relabel.

The algorithm maintains a cut whose \(t\)-side is _monotonically decreasing_ (i.e., it moves nodes on the \(t\)-side of the cut to the \(s\)-side, but not the other way around), and resolves excess on the \(t\)-side by routing excess flow to \(t\), or updating the cut so the excess node is on the \(s\)-side of the new cut. The same insight will be used in our warm-started version of Push-Relabel seeded with any pseudo-flow.

Our main result applies in the general setting where the prediction is any pseudo-flow.

**Theorem 1**.: _Given a predicted pseudo-flow \(\) with error \(\) on network \(G\), there exists a warm-start version of Push-Relabel that obtains the minimum cut in time \(O( n^{2})\)._

Our warm-start version of Push-Relabel that is seeded with a general pseudo-flow has several phases. In the first phase, we show that one can modify the prediction to be a cut-saturating pseudo-flow. We begin the second phase by routing flow _within_ the two sides of the cut induced by the pseudo-flow to resolve some of the excess and deficit. The maintained cut gradually changes as we send flow from node to node, and push certain nodes to different sides of the cut. We continue changing the cut until all excess nodes end up on the \(s\)-side of the cut and all deficit nodes end up on the \(t\)-side of the cut. This "swapping" procedure between excess and deficits nodes between the \(s\)- and \(t\)- sides of the cut is our biggest technical innovation. Either the excesses are resolved within the \(t\)-side of the cut, or we find a new cut between the \(t\)-side excess nodes and the \(t\)-side deficit nodes plus \(t\). We modify the cut in \(G\) accordingly to separate all excess from the \(t\)-side, which also results in a cut whose \(t\)-side is monotonically decreasing--an interesting point which also occurs in the cut maintained by the gap relabeling heuristic. A mirrored version of this process is performed on the \(s\)-side of the cut.

In the final phase, we have a new cut-saturating pseudo-flow with all excess nodes on the \(s\)-side of the cut and all deficit nodes on the \(t\)-side of the cut. This cut is actually a min-cut. On the \(s\)-side, the excess nodes send flow to the source, and on the \(t\)-side, the sink sends flow to deficit nodes (hence removing existing flows). The result is a max-flow. See Figure 1 for an illustration of phases 2 and 3.

In Section 4, we run our warm-start Push-Relabel compared to a cold-start version. We see that the warm-start improves over the cold-start by a larger percentage as the size of the image increases.

Our work identifies that there is a monotonic property in the cuts when Push-Relabel is implemented with the gap relabeling heuristic that directly results in faster running time when the value of the max flow/min cut is small (Corollary 1). While such a result was known for Ford-Fulkerson, this was _not_ known for Push-Relabel prior to our work. We leave as an open direction whether this monotonic property may have further theoretical importance in improving Push-Relabel analyses on more general classes of networks.

The running time of warm-start Push-Relabel is \(O(n^{2})\), while the running time of warm-start Edmonds-Karp is \(O(mn)\). We do not believe that the bound for warm-start Push-Relabel being no faster than that of warm-start Edmonds-Karp is a weakness in our analysis, but is instead an artifact of the community's lack of understanding on why Push-Relabel performs so much better in practice than its theoretical bounds guarantee. Improving our theoretical understanding of algorithms that do well in practice (like Push-Relabel or the Simplex Method) is one of the pillars of the field of beyond worst-case analysis . Researchers have long suspected that there should be a way to parameterize instances or use practical heuristics to justify why Push-Relabel is so much better in practice than its theoretical worst-case bound would suggest .

## 2 Gap Relabeling Push-Relabel: Cold- and Warm-Start

We briefly overview the performance of Push-Relabel with the gap relabeling heuristic (Algorithm 2) when given a cut-saturating _pre-flow_\(f\), and tie the running time to the error of \(f\). Proofs in this section are deferred to Appendix A.2, as their more involved analogs are in Section 3.

Figure 1: An illustration of warm-start Push-Relabel, seeded with a cut-saturating pseudo-flow. The red curve denotes the cut. The black arrows denote the existing flows, whereas the red arrows denote the flows sent in each phase to resolve excess/deficits.

Algorithm 2 begins by running Algorithm 4 as a subroutine to find the cut saturated by \(f\) and define valid heights for \(f\) which also induce that cut. Algorithm 4 runs a BFS in the residual graph to find all nodes that have a path to \(t\) and names this set \(T\). The other nodes belong to \(S\). By Lemma 3 the cut \((S,T)\) is saturated by \(f\). In the main \(\) loop, the algorithm maintains the cut \((S,T)\) such that all heights in \(T\) compose a series of _consecutive_ numbers starting from \(0\). The cut only changes when a node is relabeled in a way that results in a break in the series of consecutive heights starting from \(0\) in \(T\), where the smallest missing height (the node's height before relabeling) is denoted by \(\). The algorithm then removes all nodes from \(T\) with height bigger than \(\); importantly, these nodes will _never_ enter \(T\) again. The \(\) loop terminates when \(T\) has no excess, thus finding the min cut.

**Lemma 4**.: _Let \(f\) be a pre-flow saturating cut \((S,T)\) on network \(G\). If there are no excess nodes in \(T\), then all excess in \(S\) can be sent back to \(s\) without crossing the cut, implying \((S,T)\) is a min-cut._

We show that the running time is tied to \(\), which, in this case, is the total excess in \(f\).

**Theorem 2**.: _Given a cut-saturating pre-flow \(f\) with error \(\) on network \(G\), Algorithm 2 finds a max-flow/min-cut in running time \(O( n^{2})\)._

Although Algorithm 2 is presented as being seeded with an existing pre-flow, the same bound applies to the cold-start gap relabeling Push-Relabel when the min-cut of \(G\) is at most \(\). One only has to seed it with \(f^{}\) as in Algorithm 1, which saturates the cut \((\{s\},V\{s\})\). This will be useful in Section 3, as we repeatedly use Algorithm 2 as a subroutine on networks with small min-cut.

**Corollary 1**.: _If network \(G\) is known to have a max-flow/min-cut value of at most \(\), one can use Algorithm 2 to obtain a max-flow and min-cut for \(G\) in running time \(O( n^{2})\)._

## 3 Warm-starting Push-Relabel with General Pseudo-flows

We extend the results in Section 2 to when the given prediction is a general pseudo-flow \(\) as opposed to a cut-saturating pre-flow, i.e., \(\) may not be cut-saturating and may have deficit nodes. The first phase of our algorithm computes \(\) and augments \(\) by finding an \(s-t\) flow to add to \(\) so that the resulting pseudo-flow saturates a cut. We defer discussion of this pre-processing phase to Appendix A.2 (see Lemma 8 and Algorithm 5). Once we have a cut-saturating pseudo-flow \(f\) and \(\), we are ready to define the accompanying heights and cut using Algorithm 4. Note that the initial cut with two sides \(T_{0}=\{u V: u-tG_{f}\}\) and \(S_{0}=V T_{0}\) is by definition the same cut as that induced by the heights (as in Lemma 2).

We update the pseudo-flow so that it always maintains a saturated cut, but eventually, the nodes with excess and the nodes with deficit are separated by the saturated cut. This is a generalization of what happens in Algorithm 2, where we transfer all excess nodes to the \(s\)-side of the cut. Here, we transfer all excess to the \(s\)-side, and all deficit to the \(t\)-side of the cut. Interestingly, we observe that this is the sufficient condition for the pseudo-flow to saturate a min-cut. Lemma 5 extends Lemma 4.

**Lemma 5**.: _For a cut-saturating pseudo-flow \(f\) for a network \(G\), let \((S,T)\) be a cut it saturates. If all nodes in \(T\) have no excess and all nodes in \(S\) have no deficit, then the cut is a minimum cut._

To prove Lemma 5, we use the following result from :

**Lemma 6** (Lemma 5).: _Given any pseudo-flow \(f\) for \(G\), every excess node has a path in \(G_{f}\) to either a deficit node or \(s\); every deficit node has a path in \(G_{f}\) from either an excess node or \(t\)._

Proof of Lemma 5.: In \(G_{f}\), by Lemma 6, every excess node \(u\) in \(S\) must have a path to either a deficit node or \(s\). Since \(f\) currently saturates \((S,T)\), the path cannot go across this cut and reach \(T\), where all the deficits are. Therefore, \(u\) has a path back to \(s\)_within_ set \(S\). Similarly, for every deficit node \(v T\) there is a path that starts with either an excess node or \(t\) and ends with \(v\). Again, since the cut \((S,T)\) is already saturated, the path must be within \(T\), hence can only be from \(t\) to \(v\). It follows that we can send all excess to \(s\) and send flow from \(t\) to all deficit nodes until the pseudo-flow becomes a feasible flow. Notice that \((S,T)\) remains saturated in this process, hence the resulting flow is max-flow and \((S,T)\) is min-cut. 

By Lemma 5, it is sufficient to find a pseudo-flow and accompanying saturated cut where the excess nodes are all on the \(s\)-side and the deficit nodes are all on the \(t\)-side. We focus on the \(t\)-side of the cut, and show that the same can be done for the \(s\)-side by considering the backwards network.

Moving excess to the \(s\)-side.To resolve excess on the \(t\)-side, we solve an auxiliary graph problem. The goal is to send the maximum amount of flow from excess nodes to either deficit nodes or \(t\)_within_ the \(t\)-side (currently denoted \(T_{0}\)). If the max-flow in this problem matches the total excess in \(T_{0}\), all excess can be resolved locally and only deficits remain; otherwise, the max-flow solution on the auxiliary graph also provides us with a min-cut that "blocks" excess nodes from deficit nodes and \(t\). This cut will become the new cut maintained by the pseudo-flow after adding the auxiliary flow to it.

To construct the auxiliary \(G^{}\), take the induced subgraph \(G_{f}[T_{0}]\), and add a super-source and -sink \(s^{*}\) and \(t^{*}\) to it. Add edges \((s^{*},u)\) with capacity \(_{f}(u)\) for every excess node \(u T_{0}\); add edges \((v,t^{*})\) with capacity \(_{f}(v)\) for every deficit node \(v T_{0}\); and add an edge \((t,t^{*})\) with capacity \(+1\).

When we run cold-start Push-Relabel (Algorithm 2) on \(G^{}\), it outputs a flow \(f^{}\) and the \(s^{*}-t^{*}\) cut \((T^{}_{0},T^{}_{0})\). Note that \(t T^{}_{0}\), since \((t,t^{*})\) has capacity \(+1\) and cannot be in the min cut which is bounded by \(\) by definition. Any \(s^{*}-t^{*}\) path \(p\) in \(G^{}\) along which \(f\) sends \(\) units of flow exactly identifies nodes \(u\) and \(v\) (where \((s^{*},u) p\) and \((v,t^{*}) p\)) for which \(\) units of flow can be sent from \(u\) to \(v\) along the interior of \(p\) in \(G_{f}\). We can send flow as indicated by \(f^{}\) to update \(f\) (Algorithm 3). We obtain the following guarantee.

**Claim 1**.: _In Algorithm 3, the output pseudo-flow \(f\) saturates the cut \((S_{0} T^{}_{0},T^{}_{0})\), and all excess nodes are in \(S_{0} T^{}_{0}\). Moreover, the total excess and deficit in \(G\) has not increased._

Proof of Claim 1.: Let \(f_{}\) denote the input to Algorithm 3. The fact that the output \(f\) saturates the cut \((S_{0} T^{}_{0},T^{}_{0})\) immediately follows from the fact that \(f^{}\) saturated the cut \((T^{}_{0},T^{}_{0})\) in \(G^{}\). Indeed, all edges from \(T^{}_{0}\) to \(T^{}_{0}\) are now saturated and all edges from \(T^{}_{0}\) to \(T^{}_{0}\) have no flow. All edges from \(S_{0}\) to \(T^{}_{0}\) are already saturated in the old flow \(f_{}\) and remain so after adding \(f^{}\) since its flows are locally within \(T_{0}\). For the same reason, all edges from \(T^{}_{0}\) back to \(S_{0}\) still have no flow.

Now, we consider the total excess and deficit. First note that the nodes that have excess/deficit with respect to the updated pseudo-flow \(f\) are a subset of the nodes that had excess/deficit with respect to \(f_{}\), and the excess/deficit of a node is clearly never increased. Assume for sake of contradiction there is an excess node \(u T^{}_{0}\). Then \(u\) had excess with respect to \(f_{}\) too, so there is an edge \((s^{*},u)\) that had capacity \(_{f_{}}(u)\) in \(G^{}\) but was not saturated by \(f^{}\). Further, since a min-cut in \(G^{}\) is \((T^{}_{0},T^{}_{0})\), it must be that \(u\) can reach \(t\) in \(G^{}\). This means that in \(G^{}\) there is a path with positive remaining capacity between \(s^{*}\) and \(t^{*}\), contradicting the fact that \(f^{}\) was a max-flow in \(G^{}\). 

By Claim 1, the updated \(f\) satisfies the conditions of Lemma 7 by taking \(S^{*}=S_{0} T^{}_{0}\) and \(T^{*}=T^{}_{0}\). The running time in Lemma 7 follows by applying Corollary 1 on \(G^{}\). Thus we have the following.

**Lemma 7**.: _Let \(f\) be a pseudo-flow for network \(G\) with error \(\) saturating cut \((S_{0},T_{0})\). Algorithm 3 finds a new cut-saturating pseudo-flow in time \(O( n^{2})\) that (i) saturates an additional \(s-t\) cut \((S^{*},T^{*})\), (ii) has no excess nodes in \(T^{*}\), and (iii) has total excess and deficit still bounded by \(\)._

We can do a similar procedure for the \(s\)-side of the cut, this time removing deficit nodes. This is the backward process of what happens to the \(t\)-side, and can be done by reversing the graph edges and flows and running Algorithm 3 on the reversed network; see Appendix A.2 and Algorithm 6.

**Corollary 2**.: _Let \(f\) be a pseudo-flow for network \(G\) with error \(\) that saturates cut \((S_{0},T_{0})\). One can update \(f\) in time \(O( n^{2})\) so that all flow in \(T_{0}\) remains unchanged, but now \(f\) saturates a cut \((S^{*},T^{*})\) and there are no deficit nodes in \(S^{*}\)._

Coping with unknown \(\)Algorithm 3 assumes \(\) is given. When \(\) is not know, we can run Algorithm 3 iteratively with a guess for \(\) and double the guess each iteration. We initialize by guessing that \(=1\). Run Algorithm 3 on an auxiliary graph, which is just the residual graph plus a new source node \(s^{*}\) with a single edge \((s^{*},s)\) of capacity \(\) that is saturated upon initialization. If Algorithm 3 returns the cut \((s^{*},s)\), we have augmented the pseudo-flow \(f\) with an \(s-t\) flow of \(\), but have not found an \(s-t\) cut yet. Double \(\), change both the capacity and flow on \((s^{*},s)\) to be the new \(\), and run Algorithm 3 again. Repeat this until we have found an \(s-t\) cut. The initial \(f\) is between \([,]\) away from being cut-saturating, and the whole procedure has \(O( n^{2})\) running time. See Algorithm 5 for more details.

Summarizing this section, we prove our main theorem.

Proof of Theorem 1.: Given a predicted pseudo-flow \(\) with error \(\) on network \(G\), Lemma 8 proved that Algorithm 5 finds a cut-saturating pseudo-flow \(f\) for \(G\) with error \(\) in time \(O( n^{2})\). To find a min-cut, Lemma 5 shows that it is enough to find a pseudo-flow saturating a cut so that the \(t\)-side of the cut contains no excess and the \(s\)-side of the cut contains no deficit.

By Lemma 7, we can run Algorithm 3 seeded with \(f\) on \(G\) to obtain an updated cut-saturating pseudo-flow with no excess on the \(t\)-side of the maintained cut in time \(O( n^{2})\). Then, Algorithm 3 can be run on the backwards network \(B\), and from Corollary 2, the updated cut-saturating pseudo-flow now has no excess on the \(t\)-side of the cut and no deficit on the \(s\)-side.

The last phase can be left out if only the min-cut is desired. Suppose the min-cut is \((S,T)\). By the proof of Lemma 5, to obtain a max-flow we only need to send all excess flow back to \(s\), and send flow from \(t\) to every deficit node. Label all nodes in \(S\) with height \(n\) and all nodes in \(T\) with height \(0\). Then run Algorithm 2 to fix all excess in \(S\). The algorithm will only send flow back to \(s\), since there is no way to cross the cut \((S,T)\). Then reverse the graph and flow, and run Algorithm 2 to fix the excess in the reversed graph, which exactly correspond to the deficit nodes in the original graph.

## 4 Empirical Results

In this section, we validate the theoretical results in Sections 3. To demonstrate the effectiveness of our methods, we consider _image segmentation_, a core problem in computer vision that aims at separating an object from the background in a given image. It is common practice to re-formulate image segmentation as a max-flow/min-cut optimization problem (see for example ), and solve it with combinatorial graph-cut algorithms.

The experiment design we adopt largely resembles that in , which studied warm-starting the Ford-Fulkerson algorithm for max-flow/min-cut. As in previous work, we do not seek state-of-the-art running time results for image segmentation. Our goal is to show that on real-world networks, warm-starting can lead to significant run-time improvements for the Push-Relabel min-cut algorithm, which claims stronger theoretical worst-case guarantees and empirical performance than the Ford-Fulkerson procedures. We highlight the following:

* Our implementation of cold-start Push-Relabel is much faster than Ford-Fulkerson on these graph instances, enabling us to explore the effects of warm-starting on larger image instances. This improved efficiency results from implementing the gap labeling and global labeling heuristics, both known to boost Push-Relabel's performance in practice. The actual running time scales with \(\) better than the theoretical \(O(n^{2})\) bound. This is not totally surprising, as Push-Relabel is known to often enjoy subquadratic running time despite the bound.
* As we increase the number of image pixels (i.e., the image's resolution), the size of the constructed graph increases and the savings in time becomes more significant.
* Implementation choices (such as how to learn the seed-flow from historical graph instances and their solutions) that make the predicted pseudo-flow cut-saturating and that reroute excesses and deficits are crucial to the efficiency of warm-starting Push-Relabel.

Datasets and data prepossessingOur image groups are from the _Pattern Recognition and Image Processing_ dataset from the University of Freiburg, and are titled Birdhouse, Head, Shoe, and Dog. The first three groups are.jpg images from the _Image Sequences1_ dataset. The last group, Dog, was a video that we converted to a sequence of.jpg images from the _Stereo Ego-Motion2_ dataset.

Each of the image groups consists of a sequence of photos of an object and its background. There are slight variations between consecutive images in a sequence, which are the result of the object and background's relative movements or a change in the camera's position. These changes alter the solution to the image segmentation problem, but the effects should be minor when the change between consecutive images is minor. In other words, we expect an optimal flow and cut found on an image in a sequence to be a good prediction for the next image in the sequence.

From each group, we consider 10 images and crop them to be either \(600 600\) or \(500 500\) pixel images, still containing the object, and gray-scale all images. We rescale the cropped, gray-scaled images to be \(N N\) pixels to produce different sized datasets. Experiments are performed for \(N\{30,60,120,240,480\}\). In the constructed graph, we have \(|V|=N^{2}+2\). Every graph is sparse, with \(|E|=O(|V|)\), hence both \(|V|\) and \(|E|\) grow as \(O(N^{2})\). Detailed description of raw data and example original images can be found in Appendix B (Table 3, Figure 4).

Figure 2: The cropped and gray-scaled images from Figure 4 (copy from Figure 2 in ).

Graph constructionAs in , we formulate image segmentation as a max-flow/min-cut problem. The construction of the network flow problem applied in both our work and theirs is derived from a long-established line of work on graph-based image segmentation; see . The construction takes pixels in images to be nodes; and a penalty function value which evaluates the contrast between the pigment of any neighboring pixels to be edge capacity. We leave details on translating the images to graphs on which we solve max-flow/min-cut to Appendix B.

Implementation details in warm-start Push-RelabelThroughout the experiments, whenever the Push-Relabel subroutine is called on any auxiliary graph, it is implemented with the gap relabeling heuristic, as shown in Algorithm 2, and the _global relabeling_ heuristic, which occasionally updates the heights to be a node's distance from \(t\) in the residual graph. These heuristics are known to improve the performance of Push-Relabel. As a tie-breaker for choosing the next active node to push from, we choose the one with highest height, which is known to improve the running time of Push-Relabel. We found the generic Push-Relabel algorithm without these heuristics to be slower than Ford-Fulkerson.

All images from the same sequence share the same seed sets. The constructed graphs are on the same sets of nodes and edges, but the capacities on the edges are different. The first image in the sequence is solved from scratch. For the second image in the sequence, we reuse the old optimal flow and cut from the first image one, then for the \(i^{th}\) image in the sequence, we reuse the optimal flow and cut from the \(i-1^{st}\) image. We reuse the old max-flow on the new network by rounding down the flow on edges whose capacity has decreased, hence producing excesses and deficits, and pass this network and flow to the warm-start Push-Relabel algorithm in Section 3.

To find a saturating cut, instead of sending flow from \(s\) to \(t\) as suggested in Algorithm 5, we reuse the min-cut on the previous image \((S_{0},T_{0})\) and send flow from \(S_{0}\) to \(T_{0}\) that originates from either \(s\) or an excess node, and ends at either \(t\) or a deficit node. We experimented with a few different ways of projecting the old flow to a cut-saturating one on the new graph. The way we implemented was by far the most effective, although it shares the same theoretical run-time as Algorithm 5.

The graph-based image segmentation method finds reasonable object/background boundaries. Figure 3 shows an example of how the target cut could evolve as the image sequence proceeds. Even with the same set of seeds, the subtle difference in images could lead to different min-cuts that need to be rectified. However, the hope is that the old min-cut bears much resemblance to the new one, hence warm-starting Push-Relabel with it could be beneficial. See Appendix B for other examples.

In our experiments, \(\) is estimated by computing both the total excess/deficit. Typically this is a loose bound, but it suffices for our purpose.

ResultsTable 1 shows average running times for both Ford-Fulkerson in  and Push-Relabel. The "N/A" marks overly long run-time (\(>\)1 hour), at which point we stop evaluating the exact run-time.

These results show warm-starting Push-Relabel, while slightly losing in efficiency on small images, greatly improves in it on large ones. As for the scaling of run-time with growing data sizes, both cold- and warm- start's running time increases polynomially with the image width \(n\), but warm-start scales better, and as \(n\) increases to \(480\), it gains a significant advantage over cold-start. Despite the different warm-start theoretical bounds (\(O(|V|^{2})\) for Push-Relabel versus \(O(|E|)\) for Ford-Fulkerson), in practice both warm-start algorithms scale similarly as the dataset size grows. Additionally, one can see Push-Relabel greatly outperforms on the same image size, allowing us to collect run-time

Figure 3: Cuts (red) on images chronologically evolving from the \(240 240\) pixel images from Birdhouse.

statistics on images of sizes up to \(480 480\) pixels, which we could not do with implementations of Ford-Fulkerson, due to its slow run-time.

Table 2 shows how the running time of warm-start Push-Relabel breaks down into the three phases described in Section 3: (1) finding a cut-saturating pseudo-flow; (2) fixing excess on \(t\)-side; (3) fixing deficits on \(s\)-side. Note phase (1) takes the most time, but results in a high-quality pseudo-flow, in that it takes little time to fix the excess/deficits appearing on the "wrong" side of the cut.

## 5 Conclusions

We provide the first theoretical guarantees on warm-starting Push-Relabel with a predicted flow, improving the run-time from \(O(m n^{2})\) to \(O( n^{2})\). Our algorithm uses a well-known heuristics in practice, the gap relabeling heuristic, to keep track of cuts in a way that allows for provable run-time improvements. One direction of future work is extending the approaches in this work to generalizations of \(s\)-\(t\) flow problems, for instance, tackling minimum cost flow or multi-commodity flow. A different line of work is to develop rigorous guarantees for other empirically proven heuristics by analyzing them through a lens of predictions, providing new theoretical insights and developing new algorithms for fundamental problems.

  Size & \(30 30\) & \(60 60\) & \(120 120\) & \(240 240\) & \(480 480\) \\  Total & 0.06 & 0.45 & 4.98 & 55.68 & 502.58 \\ Saturating cut & 0.04 & 0.34 & 4.17 & 46.25 & 431.49 \\ Fixing \(t\) excesses & 0.01 & 0.09 & 0.53 & 5.29 & 64.01 \\ Fixing \(s\) deficits & 0.01 & 0.02 & 0.27 & 4.13 & 7.08 \\  

Table 2: Running time of warm-start Push-Relabel break down, on Birdhouse

  Image Group & FF cold-start & FF warm-start & PR cold-start & PR warm-start \\  Birdhouse \(30 30\) & 0.80 & 0.51 & 0.05 & 0.06 \\ Head \(30 30\) & 0.62 & 0.43 & 0.05 & 0.05 \\ Shoe \(30 30\) & 0.65 & 0.39 & 0.07 & 0.06 \\ Dog \(30 30\) & 0.69 & 0.32 & 0.10 & 0.11 \\   Birdhouse \(60 60\) & 8.22 & 3.25 & 0.30 & 0.45 \\ Head \(60 60\) & 9.36 & 4.10 & 0.50 & 0.50 \\ Shoe \(60 60\) & 8.09 & 3.04 & 0.69 & 0.47 \\ Dog \(60 60\) & 21.91 & 6.73 & 0.76 & 0.95 \\   Birdhouse \(120 120\) & 109.06 & 37.31 & 5.42 & 4.98 \\ Head \(120 120\) & 101.79 & 28.43 & 5.90 & 5.92 \\ Shoe \(120 120\) & 98.95 & 30.44 & 6.44 & 3.74 \\ Dog \(120 120\) & 190.36 & 38.08 & 6.76 & 6.38 \\   Birdhouse \(240 240\) & NA & 400.19 & 60.67 & 55.68 \\ Head \(240 240\) & NA & 374.79 & 32.46 & 31.00 \\ Shoe \(240 240\) & NA & 338.05 & 69.29 & 35.57 \\ Dog \(240 240\) & NA & 459.48 & 73.76 & 52.42 \\   Birdhouse \(480 480\) & NA & NA & 604.54 & 502.58 \\ Head \(480 480\) & NA & NA & 365.25 & 285.75 \\ Shoe \(480 480\) & NA & NA & 756.77 & 364.42 \\ Dog \(480 480\) & NA & NA & 834.63 & 363.41 \\  

Table 1: Average run-times (s) of cold-/warm-start Ford Fulkerson (FF) and Push-Relabel (PR)