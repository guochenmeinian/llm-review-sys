# Carrot and Stick:

Eliciting Comparison Data and Beyond

 Yiling Chen

Harvard University

yiling@seas.harvard.edu

&Shi Feng

Harvard University

shifeng@fas.harvard.edu

&Fang-Yi Yu

George Mason University

fangyiyu@gmu.edu

###### Abstract

Comparison data elicited from people are fundamental to many machine learning tasks, including reinforcement learning from human feedback for large language models and estimating ranking models. They are typically subjective and not directly verifiable. How to truthfully elicit such comparison data from rational individuals? We design peer prediction mechanisms for eliciting comparison data using a bonus-penalty payment . Our design leverages on the strong stochastic transitivity for comparison data  to create symmetrically strongly truthful mechanisms such that truth-telling 1) forms a strict Bayesian Nash equilibrium, and 2) yields the highest payment among all symmetric equilibria. Each individual only needs to evaluate one pair of items and report her comparison in our mechanism.

We further extend the bonus-penalty payment concept to eliciting networked data, designing a symmetrically strongly truthful mechanism when agents' private signals are sampled according to the Ising models. We provide the necessary and sufficient conditions for our bonus-penalty payment to have truth-telling as a strict Bayesian Nash equilibrium. Experiments on two real-world datasets further support our theoretical discoveries.

## 1 Introduction

In the past two decades, researchers have been embracing the challenge of eliciting private information from individuals when there is no ground truth available to evaluate the quality of elicited contributions, and have made amazing progress. Many mechanisms, collectively called _peer prediction_, have been developed to incentivize individuals to strictly truthfully report their information at a Bayesian Nash equilibrium (BNE), by artful design of payment functions that only depend on reports from individuals. Moreover, in multi-task peer prediction mechanisms, the truthful BNE gives each individual the highest expected payoff among all BNEs (i.e. it's a strongly truthful BNE). 

However, all prior multi-task peer prediction mechanisms require tasks being ex-ante identical, and hence individuals' private information is independently and identically distributed (iid) for each task. Multi-task peer prediction leverages this structure of information to succeed at truthful elicitation. But what if such structure of information doesn't hold for an information elicitation problem?

One notable application is to elicit pair-wise comparisons of multiple alternatives, such as preferences for consumer products , translation , peer grading , and relevance of language model outputs . Such pair-wise comparison data are crucial for estimating a ranking of the alternatives and for devising reward functions for reinforcement learning. Comparison tasks for different pairs are clearly not ex-ante identical -- answers to the tasks demonstrate a certain degree of transitivity (e.g. if \(a\) is preferred to \(a^{}\) and \(a^{}\) is preferred to \(a^{}\), then it's more likely that \(a\) is preferred to \(a^{}\)), rendering existing peer prediction mechanisms not applicable.

In this paper, we design a peer prediction mechanism for eliciting comparison data. We model individuals' private information of pair-wise comparisons as Bayesian strongly stochastically transitive (Bayesian SST), which takes many widely used models (e.g. Thurstone , Bradley-Terry-Luce , and Mallows ) as special cases. Our mechanism uses a simple bonus-penalty payment  (hence carrot and stick) that takes three reports as inputs and admits a strongly truthful symmetric BNE. The key insight that we develop is a condition of information structure that we call _uniform dominance_. When uniform dominance is satisfied, the bonus-penalty payment is the only type of payment that induces a strictly truthful BNE. Information of individuals, \(i\), \(j\), and \(k\), satisfies uniform dominance if, conditioned on any realization of agent \(i\)'s information, the probability for \(j\)'s information to agree with \(i\)'s is higher than the probability for \(k\)'s information to agree with \(i\)'s. Bayesian SST allows us to group three pairwise comparisons, \((a,a^{})\), \((a^{},a^{})\) and \((a^{},a)\), together such that private information about these pairs satisfies uniform dominance. After identifying uniform dominance as a central structure for incentivizing truthful elicitation, we further generalize the bonus-penalty payment to truthfully elicit private information over social networks that demonstrate homophily (i.e. friends tend to have similar opinions than non-friends) , and our mechanism can be integrated with common survey techniques such as snowball sampling .

Our contributions.Our work is a leap forward for designing mechanisms for complex information elicitation settings where ground truth verification is not available.

* We are the first to design mechanisms to truthfully elicit pairwise comparison data under Bayesian SST and networked data under Ising models. In our mechanisms, truthful reporting forms a BNE and yields a strictly higher payoff than any symmetric non-permutation equilibrium.
* We identify a key structure of information, uniform dominance, as a lever such that the simple bonus-penalty payment is the unique payment inducing a strictly truthful BNE. This identification may offer a path for developing truthful elicitation mechanisms for other settings in the future.
* We use Griffiths' inequality and Weitz's self-avoiding walk  to prove the uniform dominance property in the Ising model. The resulting correlation bounds may be of independent interest.
* We test our mechanisms on real-world data (sushi preference dataset  and Last.fm dataset ). Even though these datasets do not perfectly satisfy our theoretical assumptions, our mechanisms still provide a stronger incentive for truthful reporting compared to misreporting.

Related work.Information elicitation has two settings according to whether verification is possible. Our paper focuses on elicitation without verification.

For information elicitation without verification, Miller et al.  introduce the first mechanism for single task signal elicitation that has truth-telling as a strict Bayesian Nash equilibrium but requires full knowledge of the common prior. Bayesian truth serum (BTS)  is the first strongly truthful peer prediction mechanism, but requires complicated reports from agents (their private signal and predictions on other's reports). A series of works  relax certain assumptions of BTS but still require complicated reports from agents. Dasgupta and Ghosh  introduces the multi-task setting where agents are assigned batch iid tasks and only report their signals. Several works extend this to multiple-choice questions , predictions , or even continuous value , and investigate the limitation and robustness . Another related line of work is co-training and federated learning, which wants to elicit models , or samples  when multiple iid data or feature of data are available. For more related works, see Faltings .

One popular line of work considers information elicitation when verification is possible. Spot-checking requires direct verification of the agent's report . Recent work on comparison data elicitation  utilizes spot-checking concepts and focuses on incentivizing effort. Another form of verification involves using additional samples to evaluate how the agent's reports improve model performance . Additionally, the verification may have a general relation to the agent's signal, e.g., proper scoring rules .

## 2 Problem Formulation

We discuss our model for eliciting comparison data in this section and defer the extensions to Section 5. Given a collection of items \(\) and a set of strategic agents \(\). Agents privately observe noisy comparisons between pairs of items. Our goal is to design mechanisms to truthfully elicit agents' private information. We will first introduce the information structure of agents' private information of pairwise comparisons in Section 2.1 and then define the information elicitation problem in Section 2.2.

### Bayesian SST Models for Comparison Data

We introduce Bayesian Strong Stochastic Transitivity (Bayesian SST) models to capture the structure of agents' private information for comparison data.

Given the set of items \(\), the underlying unknown state about the items is \(\). \(\) can be the vector of quality scores for the items (Example 2.2) or a reference ranking (Example 2.4). \(\) is drawn according to a common prior \(P_{}\): \( P_{}\). Any realized \(\) has an associated _stochastic comparison function_\(T_{}:^{2}\{-1,1\}\). For comparisons of two items \(a\) and \(a^{}\), \(T_{}(a,a^{})\) and \(T_{}(a^{},a)\) stochastically take value \(1\) or \(-1\), with \([T_{}(a,a^{})=1]=1-[T_{}(a^{},a)=-1]\). For any \(\), \(T_{}\) is strongly stochastically transitive as defined below.

**Definition 2.1** ().: A stochastic comparison function, \(T:^{2}\{-1,1\}\), is _strongly stochastically transitive (SST)_ if for all \(a,a^{},a^{}\) with \([T(a,a^{})=1]>1/2\) and \([T(a^{},a^{})=1]>1/2\), we have

\[[T(a,a^{})=1]>\{[T(a,a^{})=1],[T(a^{},a^ {})=1]\}.\]

Intuitively, a comparison function is SST when for any three items \(a,a^{},a^{}\), if \(a\) is more favorable than \(a^{}\) and \(a^{}\) is more favorable than \(a^{}\), then \(a\) is even more favorable than \(a^{}\). The concept of SST is a well-established property of comparisons in social science and psychology .

Each agent \(i\) has the knowledge of \((T_{})_{}\) and \(P_{}\). When asked to compare a pair of items \((a,a^{})\), the agent observes an independent draw according to the stochastic comparison function: \(S_{i}=T_{}(a,a^{})\), where realization \(s_{i}=1\) represent item \(a\) is preferred over item \(a^{}\) by agent \(i\). We assume items are a priori similar but ex-post distinct so that for all \(a,a^{}\), \([T_{}(a,a^{})]=[[T_{}(a,a^{ })]]=0\) and \([T_{}(a,a^{})] 0\) for all \(\).

Examples of Bayesian SST models.Bayesian SST models are a general family of models that take many classical parametric ranking models, including Bradley-Terry Luce , Thurstone (Case V) , and Mallows \(\)-model , as special cases.

**Example 2.2** (Bradley-Terry-Luce, Thurstone model, and more ).: Let \(^{}=\) where each coordinate is independently and identically sampled from a fixed non-atomic distribution \(\) on \(\), and each item \(a\) have a scalar quality \(_{a}\). Let \(F:\) be any strictly increasing function such that \(F(t)=1-F(-t)\) for all \(t\). Conditional on a fixed \(\),

\[[T_{}(a,a^{})=1]=F(_{a}-_{a^{}}) a,a^{}.\]

This model recovers the Thurstone model  by setting \(F(t)=(t)\) where \(\) is the Gaussian CDF, and the Bradley-Terry-Luce model  by setting \(F(t)=}{1+e^{t}}\), the sigmoid function. Moreover, this model also contains any additive random utility model  where \(T(a,a^{})=1\) if \(_{a}+Z>_{a^{}}+Z^{}\) with iid noise \(Z\) and \(Z^{}\), because we can set \(F\) to be the CDF of the difference of two iid noise.

**Proposition 2.3**.: _For any strictly increasing \(F\) and non-atomic \(\) on \(\), the parametric model in Example 2.2 is a Bayesian SST model._

**Example 2.4** (Mallows \(\)-model ).: Let \(\) be the set of rankings on \(\) and \(>0\) be a dispersion parameter. Given a reference ranking \(\), the _Mallows \(\)-model_ generate a ranking \(\) with probability \(()(- d(,))\) where \(d(,)=|\{(a,a^{})^{2}:(a)<(a^{ })(a)>(a^{})\}|\) is Kendall's tau distance, and \((a)\) is the rank of item \(a\). Therefore, to generate comparisons, we first sample a uniform \(\) and

\[[T_{}(a,a^{})=1]=_{:(a)>(a^{})}(), a,a^{}.\]

**Proposition 2.5**.: _For any \(>0\), Mallows \(\)-model in Example 2.4 with uniform distribution on reference ranking is an Bayesian SST model._

The proofs for propositions 2.3 and 2.5 are closely related to strong stochastic transitivity , but are provided in the appendix for completeness.

### Peer Prediction Mechanism Design

To truthfully elicit comparison data from agents, a peer prediction mechanism creates a game between the agents outlined below: First, we choose an _assignment_\(=\{e_{i}=(a_{u_{i}},a_{v_{i}}):i\}\) where agent \(i\) gets a pair of items \(e_{i}=(a_{u_{i}},a_{v_{i}})^{2}\) to compare. Then each agent \(i\) privately observes the realization of the comparison (signal) \(s_{i}\{-1,1\}\), which is an independent realization of \(T_{}(a_{u_{i}},a_{v_{i}})\), and reports \(_{i}\{-1,1\}\) potentially different from her signal. We use \(S_{i}=S(a_{u_{i}},a_{v_{i}})\) to denote the random variable of agent \(i\)'s signal, where the randomness of \(S(,)\) comes from both \(\) and \(T_{}\). Let \(\) represent the random vector of all agents' signals, \(=(s_{i})_{i}\) be all agents' realized private signals and \(}=(_{i})_{i}\) be all agents' reports. Finally, a _peer prediction mechanism_\((M_{i})_{i}\) takes all agents' reports \(}\) and pays agent \(i\) with \(M_{i}(})\).

Each agent \(i\)'s strategy is a random function from her signal to a report \(_{i}:s_{i}_{i}\), and the randomness of their strategies is independent of each other's and all signals. With slight abuse of notation, we write \(_{i}(s_{i},_{i})=[_{i}=_{i} S_{i}=s_{i}]\) as the conditional probability of reporting \(_{i}\) given private signal \(s_{i}\). A strategy profile \(\) is a collection of all agent's strategies. All agents are rational and risk-neutral, so they want to maximize their expected payments. Thus, given prior \(P_{}\), randomness of \(T_{}\) and a strategy profile \(\), agent \(i\) wants to maximize her ex-ante payment denoted as \(_{,,T_{}}[M_{i}(})]\) where \(}\) is the random vector of all agents' report that depends on the signals \(\) and strategy profile \(\).

We introduce three families of strategies, truth-telling, permutation, and uninformed strategy profiles, which are central to understanding effective peer prediction mechanisms.

* A strategy \(_{i}\) is _truthful_ (or truth-telling) if it is a deterministic identity map, \(_{i}(s_{i})=s_{i}\). A strategy profile is truthful if all agents' strategies are truthful.
* A _permutation strategy profile_ is where agents simultaneously relabel their signals and then report the relabeled ones. A permutation strategy is indistinguishable from truth-telling unless the peer prediction mechanism has additional knowledge about the prior signal distribution.
* Finally, a strategy is _uninformed_ if it has the same report distribution across all signals, and it is informed otherwise. Common examples include consistently reporting all signals as a constant value, such as \(1\) or \(-1\), or using a random report regardless of the signal. Uninformed strategies are undesirable as the reports bear no relationship to the private signals.

A strategy profile is _symmetric_ if all agents use the same strategy. For example, both truth-telling and permutation strategy profiles are symmetric.

We now introduce goals for a peer prediction mechanism that favors truth-telling more than other strategies. First, we want the truth-telling (strategy profile) to be a strict _Bayesian Nash equilibrium (BNE)_ so that any agent's payment would strictly decrease if she unilaterally changes to any non-truthful strategy. Moreover, there may be multiple equilibria, and a desirable mechanism should ensure that truth-telling is better than all other equilibria. In this paper, we aim for symmetrically strongly truthful mechanisms defined below.

**Definition 2.6**.: A peer prediction mechanism is _symmetrically strongly truthful_ if truth-telling is a BNE, and each agent's expected payment in truth-telling is no less than the payment in any other symmetric equilibrium with equality for the equilibrium with a permutation strategy profile.1

## 3 Bonus-penalty Payment Mechanism for Comparison Data

We now propose a _bonus-penalty payment mechanism_ for eliciting comparison data. The mechanism makes use of a _bonus-penalty payment_ function, which can be seen as an agreement payment and introduced by Dasgupta and Ghosh  in a different context (see discussion in appendix A). Formally, for any \(_{i},_{j},_{k}\{-1,1\}\), the bonus-penalty payment function is

\[U^{DPP}(_{i},_{j},_{k})=_{i}_{j}-_{i} _{k}=2([_{i}=_{j}]-[_{i}= _{k}]),\] (1)

which rewards when the first input agrees with the second but punishes when it agrees with the third.

Mechanism 1 uses the bonus-penalty payment eq. (1) for each agent \(i\) by carefully choosing agent \(j\) and \(k\) such that agent \(j\)'s signal is more likely to agree with agent \(i\)'s than agent \(k\)'s signal is. The crux of finding such pair of agents is to show that if agent \(i\) prefers item \(a\) over \(a^{}\), she would expect that others will prefer any third item \(a^{}\) over \(a^{}\), and prefer \(a\) over \(a^{}\). Thus, if agent \(j\) has pair \((a^{},a^{})\) and agent \(k\) has pair \((a^{},a)\), then agent \(j\)'s signal is more likely to take the same value as \(i\)'s than agent \(k\)'s signal is. This is the main idea behind the proof of Theorem 3.1, where we establish the symmetrically strongly truthfulness of Mechanism 1. To ensure the existence of such pairs are assigned, we require the assignment \(\) to be _admissible_ where for all \((a,a^{})\), there exists \(a^{}\) so that \((a^{},a^{})\) and \((a^{},a)\).

``` Input: Let \(\) be a collection of items, \(\) be an admissible assignment, and \(}\) be agents' reports. foragent\(i\) with pair \(e_{i}=(a_{ui},a_{v_{i}})=(a,a^{})\)do  Find \(a^{}\) and two agents \(j\) and \(k\) so that \(e_{j}=(a^{},a^{})\) and \(e_{k}=(a^{},a)\), and pay agent \(i\) \[M_{i}(})=U^{BPP}(_{i},_{j},_{k})=_{ i}_{j}-_{i}_{k}.\] (2) ```

**Mechanism 1:** BPP mechanism for comparison data

**Theorem 3.1**.: _Given a collection of items \(\) and a set of agents \(\) with \(||,|| 3\), for any admissible assignment matrix \(\) and Bayesian SST model with \((T_{})_{}\) and \(P_{}\), the BPP mechanism for comparison (Mechanism 1) is symmetrically strongly truthful._

We defer the proof of theorem 3.1 to section 4. The admissible condition imposes little overhead on downstream learning problems, including rank recovery  and identification of the top \(k\) items . Specifically, the size of assignment \(\) is the number of comparisons and corresponds to the sample complexity for these learning problems. If a learning algorithm requires a set of pairs to compare \(^{ML}\), we can construct an admissible superset \(\) that introduces a constant factor overhead and can recover \(^{ML}\).2

We remark that the bonus-penalty payment function eq. (2) can be seen as a boolean function for _transitivity_; see remark 3.2 for a formal statement. Hence, theorem 3.1 implies that agents' manipulations can only decrease the probability of transitivity among their reports.

**Remark 3.2**.: Note that a deterministic comparison function \(t:\{-1,1\}\) satisfies transitivity on three items \(a,a^{},a^{}\) if and only if \(t(a,a^{}),t(a^{},a^{}),t(a^{},a)\) are not all equal, that is \(NAE(t(a,a^{}),t(a^{},a^{}),t(a^{},a))=1\) where

\[NAE(w_{1},w_{2},w_{3})=-w_{1}w_{2}-w_{1}w_{3} -w_{2}w_{3}.\]

The agent's random noisy comparisons may or may not satisfy transitivity. The probability of transitivity is the probability that they do.

We can show that the bonus-penalty payment in eq. (2) is equivalent to the above transitivity test when agents are truth-telling. Formally,

\[NAE(S(a,a^{}),S(a^{},a^{}),S(a^{ },a))\] \[= -(S(a,a^{})S(a^{},a^{ })+S(a,a^{})S(a^{},a)+S(a^{},a^{ })S(a^{},a))\] \[= (S(a,a^{})S(a^{},a^{})-S( a,a^{})S(a^{},a))++S(a^{},a^{ })S(a^{},a)(S(a^{},a^{})=-S(a^{ },a^{}))\] \[= (s_{i}s_{j}-s_{i}s_{k})++ {4}s_{j}s_{k}=M_{i}()++s_{j}s_{k} \]

Therefore, \(*{arg\,max}_{_{i}}[NAE(_{i},-S_{j},S_{k})|S _{i}=s_{i}]=*{arg\,max}_{_{i}}[M_ {i}(_{i},S_{j},S_{k})++S_{j}S_{k}|S_{i}=s_{i} ]=*{arg\,max}_{_{i}}[M_{i}(_{i},S_{ j},S_{k})|S_{i}=s_{i}]\).

Proof of Theorem 3.1: from Bayesian SST Model to Uniform Dominance

To prove theorem 3.1, we formalize the idea that agent \(j\)'s signal is more likely to agree with agent \(i\)'s than agent \(k\)'s is as what we call uniform dominance in definition 4.1. We'll show that any Bayesian SST model satisfies this property. Then, we'll prove that BPP mechanism is symmetrically strongly truthful when agents' private signals satisfy uniform dominance.

**Definition 4.1**.: Given a random vector \((S_{i},S_{j},S_{k})\{-1,1\}^{3}\) with joint distribution \(P\), \(S_{j}\)_uniformly dominates_\(S_{k}\) for \(S_{i}\) if \([S_{j}=1 S_{i}=1]>[S_{k}=1 S_{i}=1]\) and \([S_{j}=-1 S_{i}=-1]>[S_{k}=-1 S_{i}=-1]\). We call such an ordered tuple \( S_{i},S_{j},S_{k}\) a uniformly dominant tuple.3

Lemma 4.2 shows how to identify uniformly dominant tuples under Bayesian SST models.

Lemma 4.2.: _Under any Bayesian SST model, for any agent \(i\) and items \(a\), \(a^{}\) and \(a^{}\), agent \(j\)'s signal \(S_{j}=S(a^{},a^{})\) uniformly dominates agent \(k\)'s signal \(S_{k}=S(a^{},a)\) for signal \(S_{i}=S(a,a^{})\)._

In other words, under any Bayesian SST model, the distribution of \(S(a,a^{}),S(a^{},a^{}),S(a^{},a)\) satisfies uniform dominance for any \(a,a^{},a^{}\). In the rest of this section, we can view \((S_{i},S_{j},S_{k})\) as an abstract random vector with some joint distribution \(P\).

We now establish some implications of uniform dominance on the bonus-penalty payment. Lemma 4.3 shows that truth-telling is the best response if other signals are reported truthfully. Lemma 4.4 states that the expected payment is zero if everyone uses uninformed strategies (random functions independent of input). Lemma 4.5 characterizes the best response under symmetric strategy profiles (the same random function on each coordinate).

Lemma 4.3 (Truthfulness).: _Given a uniformly dominant tuple \( S_{i},S_{j},S_{k}\) with distribution \(P\), for all \(s_{i}\{-1,1\}\), \(s_{i}=_{_{i}\{-1,1\}}_{P}[U^{BPP}(_{i},S_{j},S_{k}) S_{i}=s_{i}]\) and \(_{P}[U^{BPP}(S_{i},S_{j},S_{k})]>0\)._

Lemma 4.4.: _Given a uniformly dominant tuple \( S_{i},S_{j},S_{k}\), with joint distribution \(P\) if agent \(j\) and \(k\) both use an uninformed strategy \(\) so that \(_{j}=(S_{j})\) and \(_{k}=(S_{k})\), for all \(s_{i}\) and \(_{i}\) in \(\{-1,1\}\), \(_{,P}[U^{BPP}(_{i},_{j},_{k}) S_ {i}=s_{i}]=0\)._

Lemma 4.5.: _Given a uniformly dominant tuple \( S_{i},S_{j},S_{k}\) with distribution \(P\), for any strategy \(\) and \(s_{i}\{-1,1\}\) when agent \(j\) and \(k\) both use \(\), \(_{_{i}\{-1,1\}}_{,P}[U^{BPP}(_{i },_{j},_{k}) S_{i}=s_{i}]=_{_{i}\{-1, 1\}}\{(s_{i},_{i})-(-s_{i},_{i})\}\)._

We'd like to highlight that lemmas 4.3 to 4.5 as well as the proof of theorem 3.1 below hold for any uniformly dominant tuple \( S_{i},S_{j},S_{k}\), not necessarily derived from the Bayesian SST model. This offers a path to generalize our mechanism for comparison data to other settings.

Proof of theorem 3.1.: By lemma 4.2, for any agent \(i\), the associated agent \(j\)'s signal \(S_{j}=S(a^{},a^{})\) uniformly dominates the associated \(k\)'s signal \(S_{k}=S(a^{},a)\) for signal \(S_{i}=S(a,a^{})\). By lemma 4.3, if agent \(j\) and \(k\) are truthful, agent \(i\)'s best response is truthful reporting, so truth-telling is a BNE.

Now we show that all other symmetric equilibria are permutation or uninformed equilibria. For any symmetric equilibrium \(=(_{i})_{}\) so that everyone uses the same strategy \(_{}=\) for all \(\). If \(\) is not deterministic so that \((s,s),(s,-s)>0\) for some \(s\{-1,1\}\), agent \(i\) must be indifferent between reporting \(s\) and \(-s\) when getting \(S_{i}=s\). \((s,s)-(-s,s)=(s,-s)-(-s,-s)\) by lemma 4.5. This means \((s,s)=(-s,s)\) and \((s,-s)=(-s,-s)\), and \(\) is an uninformed strategy. If the strategy is deterministic, there are two cases. If \((s)=(-s)\), the strategy is also uninformed. If \((s)(-s)\), \(\) is either truth-telling \(s s\) or flipping \(s-s\) for all \(s\).

Finally, by lemma 4.4, any uninformed equilibrium's expectation is zero. Additionally, because eq. (1) is invariant when all inputs are flipped, the truth-telling and flipping/permutation equilibria has the same expected payment which is positive by lemma 4.3.

Generalization of Bonus-penalty Payment Mechanisms

We now leverage the key idea of uniform dominance to design peer prediction mechanisms for networked data in section 5.1. In section 5.2, we summarize our design approach as a general scheme that first identifies uniform dominance structures and then engages the bonus-penalty payment. We prove the uniqueness of bonus-penalty payment: it is the only payment function, up to some positive affine transformation, that induces truth-telling as a strict BNE for all uniform dominant tuples.

### Bonus-penalty Payment Mechanisms for Networked Data

Uniform dominance implies agent \(i\)'s signal is more likely to agree with agent \(j\)'s than with agent \(k\)'s. Social networks are another natural domain exhibiting this property, as homophily  suggests that agents' opinions or signals in a social network are more likely to agree with their friends than with non-friends. Leveraging this insight, we use a bonus-penalty payment scheme to elicit binary networked data.

``` Input: Let \((V,E)\) be a graph of agents in \(V\), \(}\{-1,1\}^{V}\) from all agent's reports. foragent\(i V\)do  Find agents \(j\) (friend) and \(k\) (non-friend) so that \((i,j) E\) but \((i,k) E\), and pay agent \(i\) \[M_{i}(})=U^{BPP}(_{i},_{j},_{k})=_ {i}_{j}-_{i}_{k}.\] (3) ```

**Mechanism 2:** BPP mechanism for networked data

Below, we provide a theoretical guarantee for our mechanism under a popular graphical model for social network data, _Ising model_, which captures the correlation between agents and their friends. Formally, an Ising model consists of an undirected graph \((V,E)\) and correlation parameter \(_{i,j} 0\) for each edge \((i,j) E\). Each agent is a node in the graph, \(=V\), and has a binary private signal (1 or \(-1\)) jointly distributed as the following: For all \(=(s_{i})_{i V}\{-1,1\}^{V}\), \(_{}[=](H())\) where the energy function is \(H()=_{(i,j) E}_{i,j}s_{i}s_{j}\).

**Theorem 5.1**.: _If agents' signals are sampled from an Ising model on undirected graph \((V,E)\) with correlation parameters \(\), Mechanism 2 is symmetrically strongly truthful, when \(>+1}{e^{2}+e^{2d}}\) where \(=_{(i,j) E}_{i,j}\), \(=_{(i,j) E}_{i,j}\), and \(d\) is the maximal degree of graph \((V,E)\)._

Mechanism 2 does not require knowledge about parameters of the Ising model, but only the connection of the network \((V,E)\). Social network platforms, which already possess this knowledge, can easily integrate our mechanism when conducting surveys. Additionally, snowball sampling , which relies on participants referring their friends, is also naturally compatible with our mechanism.

The complete proof of theorem 5.1 is quite technical and is deferred to the appendix, where we also explain why the bound between \(\) and \(d\) is necessary. Below, we provide a sketch of the proof.

Proof sketch for theorem 5.1.: As discussed in section 4, we only need to show that for any agent \(i\), for all agent \(j\) with \((i,j) E\) and \(k\) with \((i,k) E\), \(j\)'s signal uniformly dominates \(k\)'s signal for \(i\)'s signal. Because the energy function \(H()\) above remains invariant when the signs are flipped, \([S_{i}=1]=[S_{j}=1]=[S_{k}=1]=1/2\), it is sufficient to prove that

\[[S_{i}=1 S_{j}=1]>[S_{i}=1 S_{k}=1].\] (4)

We then prove a lower bound for the left-hand side and an upper bound for the right-hand side separately. For the left-hand side, we use the Griffiths' inequality  to show that the minimum value of \([S_{i}=1 S_{j}=1]\) happens when \(j\) is the only friend of \(i\). For the right-hand side, we use Weitz's self-avoiding walk  and reduce any graph with maximum degree \(d\) into a \(d\)-ary tree. 

### General Design Scheme and Uniqueness

The design of BPP mechanisms for comparison data and networked data has suggested a general design scheme for other elicitation settings. That is, if one can _identify a uniformly dominant tuple for each agent_, adopting the bonus-penalty payment gives a symmetrically strongly truthful mechanism. We further show that the bonus-penalty payment is in some sense unique.

**Theorem 5.2**.: _If for each agent \(i\) the associated agent \(j\)'s signal uniformly dominates \(k\)'s signal for \(i\)'s signal, the above scheme is symmetrically strongly truthful._

When an agent \(i\) has multiple pairs of \((j_{1},k_{1}),,(j_{},k_{})\) so that \(j_{i}\)'s signal uniformly dominates \(k_{l}\)'s for \(i\)'s for each \(l=1,,\), we may pay agent \(i\) the average of bonus-penalty payment on all pairs \(M_{i}(\$)=_{l=1}^{}U^{BPP}(_{i},_{j_{l}}, _{k_{l}})\). This average maintains our symmetrically strongly truthful guarantee while potentially reducing the variance in payments.

Theorem 5.2 shows that bonus-penalty payment is a sufficient condition for designing good elicitation mechanisms for information structures with uniform dominance. We now prove it is also a necessary condition: any payment that induces truth-telling as a strict BNE under all uniformly dominant tuples must be an affine transformation of the bonus-penalty payment.

**Theorem 5.3** (Uniqueness).: _A payment \(U:\{-1,1\}^{3}\) satisfies that, for all uniformly dominant tuples \( S_{i},S_{j},S_{k}\), \(s_{i}=_{_{j_{}\{-1,1\}}}[U(_{i},S_ {j},S_{k}) S_{i}=s_{i}]\), if and only if there exist \(>0\) and \(:\{-1,1\}^{2}\) so that_

\[U(s_{i},s_{j},s_{k})= U^{BPP}(s_{i},s_{j},s_{k})+(s_{j},s_{k}), { for all }s_{i},s_{j},s_{k}\{-1,1\}\]

_where choice of \(\) does not affect the set of equilibria._

## 6 Experiments

We present experiments on real-world data to evaluate our models and mechanisms. We hope to cast insights on two questions empirically. Does our mechanism provide better rewards when all agents report truthfully than when all agents report randomly? Does our mechanism incentivize truth-telling for each agent if all other agents are truthful? We evaluate Mechanism 1 and 2 by comparing three settings, truth-telling, uninformed, and unilateral deviation, using _empirical cumulative distribution functions_ (ECDF) on agents' payments. Each point on ECDF denotes the fraction of agents who get paid less than a particular value.

For both comparison and networked datasets (figs. 1 and 2), we find our mechanisms provide better payments to agents under truthful settings than the other two settings. The ECDF under truth-telling lies below the other two ECDFs, which is known as first-order stochastic dominance. This implies that the truth-telling strategy results in higher average, quantiles (e.g., first quartile, median, and third quartile), and a greater expectation of any monotone function on the empirical distribution than the other two settings. We provide additional

### SUSHI Preference Dataset for Comparison Data

We consider preference data for a collection of 10 sushi items (item set A) [26; 27], and focus on a group of 249 agents. Each agent provides a complete ranking of all 10 types of sushi in the dataset. These agents are female, aged thirty to forty-nine, who took more than three hundred seconds to rank the items and mostly lived in Kanto and Shizuoka until age fifteen. We restrict the set of agents to avoid significant violations of transitivity across different agents and to better align with our model assumptions. In the appendix, we will present the experimental results for other groups of users and further test whether the dataset satisfies transitivity.

For the first question, we use Mechanism 1 to compute each agent's payment under the truth-telling or uninformed strategy profile. For each agent \(i\), we 1) randomly sample three items \(a,a^{},a^{}\) and two agents \(j\), \(k\), 2) derive agent \(i\)'s comparison on the first two items \((a,a^{})\) from her ranking, (and similarly for agent \(j\)'s comparison on \((a^{},a^{})\), and agent \(k\)'s comparison on \((a,a^{})\)), 3) compute bonus-penalty payment on these three comparisons, 4) repeat the above procedure 100 times and pay agent \(i\) with the average of those 100 trials. For the uninformed strategy setting, we replace every agent's comparisons with uniform random bits and compute the payment. The left column of fig. 1 presents the ECDF of payments for the agents in both settings. The figure shows that in the uninformed random strategy setting only about 50% of the agents receive positive payments, while in the original dataset (truthful strategy setting) over 75% of the users receive positive payments. The right column of fig. 1 tests the second question if the agent has the incentive to deviate when every other agent is truthful. The truth-telling curve is identical to the left column of fig. 1. For unilateral deviation, each agent gets the above bonus-penalty payment when her comparisons are replaced by uniform random bits. We plot the ECDFs of payments for both settings in the right column of fig. 1. The figure shows that the ECDF of the unilateral deviation payments is above the ECDF of human users' payments, indicating that our mechanism pays more to the truth-telling agents.

### Last.fm Dataset for Networked Data

We test our BPP mechanism on the Last.fm dataset from Cantador et al. . This dataset consists of 1892 agents on Last.fm, forming a social network with 12704 edges and an average degree of 6.71. It records agents' top fifty favorite artists whom they have listened to the most. We note that, in the dataset, listener fractions for all artists are much smaller than non-listener fractions. This bias differs from our Ising model in section 5.1 where every agent has the same chance to get both signals. Thus, the result can be seen as a stress test for our mechanism even when the data deviate from the assumption of our theoretical results.

Figure 2 focuses on the most popular artist in the dataset, Lady Gaga, who has a listener fraction of 32.3%. The results for additional artists are presented in the supplementary material. The left column of fig. 2 tests the first question. Each agent has a binary signal about whether or not she listens to a particular artist (Lady Gaga in this section). For the truth-telling setting, everyone reports her signal truthfully and gets payment by the bonus-penalty payment (formally defined in section 5.1). For the uninformed setting, everyone gets the bonus-penalty payment when all reports are iid according to the prior (0.322 for Lady Gaga). When everyone is truthful, more than 76% of agents get positive payments and have an average payment of 0.37 for Lady Gaga, while when agents report randomly, only half get positive payments, and have a near zero average payment. These results suggest that agents got more incentive to choose the truth-telling equilibrium than the uninformed equilibrium. The right column of fig. 2 tests the second question. The truth-telling curve is identical to the left column of fig. 2. For the unilateral deviation setting, each agent gets the bonus-penalty payment when she reports listener/non-listener uniformly at random. The unilateral deviation's payment is worse than the payments for truth-telling, decreasing from 0.37 to near zero for Lady Gaga.

Figure 1: SUSHI preference dataset

## 7 Conclusion and Discussion

We introduce a symmetrically strongly truthful peer prediction mechanism for eliciting comparison data without verification and extend it to eliciting networked data under Ising models. Our mechanisms are evaluated using real-world data. A key insight from our work is the identification of a structure we term "uniform dominance," which suggests a path for designing mechanisms in more complex elicitation settings. For example, in time-series data, adjacent points tend to be more related than distant ones, and in contextual settings, feedback from similar contexts is typically more related than from different contexts.

A central assumption in this study is that agents are _a priori similar_. Hence, noisy comparisons of item pairs are independent of the assigned agent's identity. This assumption is reasonable for items with widely agreed-upon rankings, such as quality assessments of large language model (LLM) outputs. However, it may break down in settings where preferences are highly polarized, such as political opinions or social choice problems4. Despite this, our additional experiments in appendix F, which relax the selection rule used in obtaining fig. 1, show that the mechanism remains robust even when some dissimilarities among agents exist.

Agents in our model are assumed to focus solely on maximizing their payments, without accounting for efforts or external incentives such as minimizing others' rewards or intentionally distorting rankings. While our mechanism may be extended to handle binary effort as suggested in previous work [11; 57], accommodating more than two effort levels would require additional assumptions . Moreover, one may hope to incorporate the designer's utility, by factoring in downstream learning problems along with elicitation payments. This would necessitate a significant overhaul of the existing learning framework.

Our mechanisms achieve a symmetric, strongly truthful equilibrium. This does not rule out the existence of non-symmetric equilibria with potentially higher utility. However, such equilibria would require complex coordination among agents, making them less likely to arise naturally.

From a technical standpoint, our approach involves several assumptions that can be generalized or relaxed. Our Bayesian SST model, which relies on strong stochastic transitivity, serves as a non-parametric extension of several widely used parametric ranking models. In appendix C.2, we present both positive and negative results regarding weaker notions of transitivity (e.g., ). While we assume admissible assignments, this can be relaxed to random assignments with full support. Additionally, limited liability can be ensured in our mechanism. For example, adding a constant of 1 to the payment function in eq. (2) ensures that the payment is either 2 or 0.

Figure 2: Last.fm dataset for Lady Gaga