# Focus On What Matters: Separated Models For Visual-Based RL Generalization

Di Zhang Bowen Lv Hai Zhang Feifan Yang Junqiao Zhao

Hang Yu Chang Huang Hongtu Zhou Chen Ye Changjun Jiang

Department of Computer Science, Tongji University, Shanghai, China

MOE Key Lab of Embedded System and Service Computing, Tongji University, Shanghai, China

{2331922, 2151769, zhanghai12138, 2153299, zhaojunqiao}@tongji.edu.cn

{2053881, 2130790, zhouhongtu, yechen, cjjiang}@tongji.edu.cn

Corresponding author

###### Abstract

A primary challenge for visual-based Reinforcement Learning (RL) is to generalize effectively across unseen environments. Although previous studies have explored different auxiliary tasks to enhance generalization, few adopt image reconstruction due to concerns about exacerbating overfitting to task-irrelevant features during training. Perceiving the pre-eminence of image reconstruction in representation learning, we propose SMG (Separated Models for Generalization), a novel approach that exploits image reconstruction for generalization. SMG introduces two model branches to extract task-relevant and task-irrelevant representations separately from visual observations via cooperatively reconstruction. Built upon this architecture, we further emphasize the importance of task-relevant features for generalization. Specifically, SMG incorporates two additional consistency losses to guide the agent's focus toward task-relevant areas across different scenarios, thereby achieving free from overfitting. Extensive experiments in DMC demonstrate the SOTA performance of SMG in generalization, particularly excelling in video-background settings. Evaluations on robotic manipulation tasks further confirm the robustness of SMG in real-world applications. Source code is available at https://anonymous.4open.science/r/SMG/.

## 1 Introduction

Visual-based Reinforcement Learning (RL) has demonstrated remarkable success across various tasks, including Atari games [27; 11; 18], robotic manipulation [23; 9], and autonomous navigation [26; 46]. However, deploying visual-based RL algorithms in real-world applications requires a high generalization ability due to numerous factors that can induce distribution shifts between training and deployment scenarios, such as variations in lighting conditions, camera viewpoints, and backgrounds. Many visual-based RL algorithms are prone to overfitting to the training observations [5; 34; 44], limiting their applicability in scenarios where fine-tuning with deployment observations is not allowed.

To address the generalization gap in visual-based RL, current studies primarily focus on utilizing data augmentation techniques [19; 20; 33] and exploring various auxiliary tasks [12; 3; 13]. However, few of the previous works successfully incorporate reconstruction loss to this field, which is commonly adopted in standard visual-based RL settings and has been demonstrated to improve the sample efficiency of RL agents [41; 10; 7]. This is because reconstructing the entire input observation can exacerbate the overfitting problem to task-irrelevant features and thus weaken the generalization ability. Although several works also explored extracting task-relevant features from visual observations[6; 39; 45], little attention has been paid to the potential of leveraging these features in improving generalization.

In this paper, we propose SMG (Separated Models for Generalization), a method that utilizes a reconstruction-based auxiliary task to extract task-relevant representations from visual observations and further strengthens the generalization ability of RL agents with the help of two consistency losses. The core mechanisms behind SMG can be summarized in two parts: First, we introduce two model branches to disentangle foreground and background representations underlying in the visual observations. This separated model framework circumvents the risk of overfitting task-irrelevant features inherent in a single model structure by prudently designing the reconstruction paths, allowing our model to benefit from reconstruction loss without sacrificing generalization ability. Second, we introduce two consistency losses to align the agent's focus on the task-relevant features between raw and augmented observations. This approach enables the foreground model to extract more robust task-relevant representations, which substantially boost the generalization capability of RL agents across diverse deployment scenarios.

We evaluate SMG's effectiveness across a range of challenging visual-based RL tasks, including five tasks from DMControl  and two more realistic robotic manipulation tasks . We also adapt different evaluation settings with random-color and video-background modifications. Through comparisons with strong baseline methods, SMG demonstrates state-of-the-art performance in terms of generalization, particularly showcasing superiority in video-background settings and robotic manipulation tasks.

In summary, the main contributions of this paper are as follows:

* We present SMG, a novel approach that aims to enhance the zero-shot generalization ability of RL agents. SMG is designed as a plug-and-play method that seamlessly integrates with existing standard off-policy RL algorithms.
* SMG emphasizes the significance of task-relevant features in visual-based RL generalization and successfully incorporates a reconstruction loss into this setting.
* Extensive experimental results demonstrate that SMG achieves state-of-the-art performance across various visual-based RL tasks, particularly excelling in video-background settings and robotic manipulation tasks.

## 2 Background

A Markov Decision Process (MDP) can be defined as a tuple \((,,p,r,)\), where \(\) is the state space, \(\) is the action space, \(p:\) is the state transition probability function, \(r:\) is the reward function, and \(\) is the discount factor. At each time step \(t\), the agent receives a state \(s_{t}\), selects an action \(a_{t}\), and then receives a reward \(r_{t}\). The agent's goal is to learn a optimal policy \((a_{t}|s_{t})\) that maximizes the expected return \(_{(s_{t},a_{t})_{}}[_{t=0}^{}^{t}r_{t}]\), where \(_{}\) defines the discounted state-action visitation of \(\).

Learning an optimal policy from visual observations poses a substantial challenge for RL agents due to the inherent partial observability of the environment, a characteristic of POMDPs (Partially Observed MDP). For one thing, at each timestep \(t\), the visual observation \(o_{t}\) can only capture partial information about the true state \(s_{t}\), as certain elements may be obscured in the image. For another, the dimension of \(o_{t}\) is much higher than that of \(s_{t}\), which makes it difficult to utilize \(o_{t}\) directly for policy learning.

To infer the true underlying state from visual observations, existing methods usually employ a parameterized encoder \(f\) to map a stacked frame sequence \(x_{t}=(o_{t^{}},o_{t^{}+1},...,o_{t})\) to a compact low-dimensional latent vector \(z_{t}\), which is then used as input by policy and value function. However, training the encoder solely to rely on the reward signal is demonstrated to sample inefficiency and may lead to suboptimal performance . To tackle this issue, various auxiliary tasks have been proposed to enhance encoder training, with one common choice being to extract features from pixels via image reconstruction loss [7; 21; 2]. By adding another parameterized image decoder \(g\), the reconstruction loss is defined by maximizing the likelihood function:

\[L_{}=-_{o_{t}}[_{z_{t} f (o_{t})}[ g(o_{t}|z_{t})]]\] (1)

## 3 Approach

### What Matters in a Reinforcement Learning Task?

Learning to generalize is hard for RL agents, particularly when utilizing an image reconstruction loss. While images are rich in information, requiring the agent to reconstruct the entire input observation can lead the autoencoder network to overfit to features that are unrelated to the task (e.g. colors, textures, and backgrounds). In contrast, humans can accurately figure out what matters visually when learning a new task. Even when colors or backgrounds are changed, humans can still leverage the prior knowledge to complete the task by focusing on task-relevant features. Considering a robotic manipulation task where the agent must move the arm to the red target (Figure 2), despite variations in background colors and textures across four test scenarios on the left, only the arm's orientation and the target position should be focused on this task. We aim for our RL agent to learn an optimal policy that solely relies on these task-relevant features while disregarding irrelevant regions.

Formally, we decompose the latent representation \(z_{t}\) into task-relevant part \(z_{t}^{+}\) and task-irrelevant part \(z_{t}^{-}\). These two representations are independent, as \(p(z_{t}|o_{t})=p(z_{t}^{+}|o_{t})p(z_{t}^{-}|o_{t})\). The task-relevant representation can be further subdivided into the "control-relevant" part, which is directly affected by the agent's actions (the arm); and the "reward-relevant" part, which is associated with the reward signal (the arm and the target), both are crucial for policy learning.

### Learning Task-Relevant Representations with Separated Models

#### 3.2.1 Separated Models and Reconstruction

The representation learning objective of SMG is to maximize the mutual information \(I(o_{t};z_{t})\) between the observation \(o_{t}\) and the latent representation \(z_{t}\), and we further derive an image reconstruction objective incorporating the combination of task-relevant representation \(z_{t}^{+}\) and task-irrelevant representation \(z_{t}^{-}\) as follows:

\[L_{}=-I(o_{t};z_{t})-_{o_{t}}[ _{z_{t}^{+} f^{+}(o_{t}),z_{t}^{-} f^{-}(o_{t})}[ q(o_{ t}|z_{t}^{+},z_{t}^{-})]]\] (2)

Inspired by previous works [6; 30] that explore how to mitigate background distractions, we implement the reconstruction process by introducing the foreground encoder \(f^{+}\) and the background

Figure 1: Architecture of SMG. One-way arrows represent different types of data flows with the same input. Two-way arrows represent different types of loss.

Figure 2: A robotic manipulation task explanation for task-relevant parts in the environment.

encoder \(f^{-}\) to extract different types of representations simultaneously, which forms a separated models architecture. We also incorporate two decoders. The foreground decoder \(g^{+}\) is employed to reconstruct the foreground image \(o_{t}^{+}\) and predict a mask \(M_{t}\) with values between \((0,1)\). The background decoder \(g^{-}\) is employed to reconstruct the background image \(o_{t}^{-}\). The full image \(o_{t}\) is then reconstructed by \(o_{t}^{+}\), \(o_{t}^{-}\) and the mask \(M_{t}\) via \(o_{t}^{}=o_{t}^{+} M_{t}+o_{t}^{-}(1-M_{t})\) (\(\) denotes the Hadamard product), the reconstruction process is illustrated by the black arrows in Figure 0(a). Notably, the area where the agent is focusing can be visualized as \(o_{t}^{+} M_{t}\), which we term the "attribution" of the agent, formally defined as \(Attrib(o_{t})\).

#### 3.2.2 Additional Loss Terms

Based on the separated models architecture, we define four additional loss terms to enhance the model's ability to distinguish between two types of representations. These include the mask ratio loss and background reconstruction loss, which supervise the model's pixel outputs; along with the Q-value loss and empowerment loss, designed to consider the two properties of task-relevant representation.

**Mask ratio loss.** To further refine the accuracy of mask prediction, we introduce a hyperparameter \(\), termed the mask ratio, to constrain the proportion of the foreground part in the mask. As shown in Equation 3, we regard \(L_{}\) as an explicit form of an information bottleneck, as the percentage \(\) determines the number of pixels of \(o_{t}^{+}\) retained in the final reconstruction. This constraint forces \(f^{+}\) to prioritize the task-relevant parts of the observation during encoding. Empirical results in Section 4.4 demonstrate that \(L_{}\) facilitates learning a more precise mask.

\[L_{mask}=(M_{t}(i,j)}{^{2}}-)^{2}\] (3)

**Background reconstruction loss.** Improving the precision of background prediction can consequently enhance the foreground as well. Since the foreground and background are complementary, providing supervision for the background prevents the foreground from learning all parts of the observation. Therefore, we add additional supervision to the task-irrelevant representation \(z_{t}^{-}\). To achieve this, we propose a new type of data augmentation called attribution augmentation tailored for SMG, as illustrated in Figure 2(b). This augmentation involves augmenting the raw observation \(o_{t}\) with its corresponding predicted mask \(M_{t}\) via \(_{}(o_{t})=o_{t} M_{t}+(1-M_{t})\), where \(\) represents a randomly sampled image. This simulates the video-background setting in deployment scenarios. We define the background reconstruction loss \(L_{}\) as follows:

\[L_{}=-_{o_{t} D}[_{z_{t}^{-} f^{-}( _{}(o_{t}))}[ g^{-}(|z_{t}^{-})]]\] (4)

**Q-value loss.** Recall that the task-relevant representation \(z_{t}^{+}\) has two key properties: reward-relevant and control-relevant. Satisfying the former is relatively straightforward, as the representation \(z_{t}^{+}\) is used for policy learning. Through the Bellman residual update objective  outlined in Equation 5, \(z_{t}^{+}\) will progressively enhance its correlation with the reward signal.

\[L_{}=_{ D}[(Q(z_{t}^{+},a_{t})-(r_{t}+ V(z_{t+ 1}^{+})))^{2}]\] (5)

**Empowerment loss.** For the control-relevant property, we integrate an empowerment term \(I(a_{t},z_{t+1}^{+}|z_{t}^{+})\) based on conditional mutual information, which quantifies the relevance between the action and latent representation. Maximizing the empowerment term further leads to maximizing a variational lower bound \(q(a_{t}|z_{t+1}^{+},z_{t}^{+})\) as shown in Equation 6. This objective necessitates that \(a_{t}\) is predictable when two neighboring representations are known. We implement this objective by incorporating an inverse dynamic model.

\[L_{}=-I(a_{t},z_{t+1}^{+}|z_{t}^{+})-_{p(a_{t},z_{t +1}^{+},z_{t}^{+})}[ q(a_{t}|z_{t+1}^{+},z_{t}^{+})]\] (6)

The whole separated models architecture is shown in figure 0(a).

### Generalize Task-Relevant Representations with Separated Models

Utilizing the separated models architecture, SMG can successfully extract task-relevant representations from raw observations. Nevertheless, the agent still lacks the ability to generalize effectively and may struggle to extract meaningful features from scenarios with transformed styles. To address this issue, we treat the task-relevant representation under raw observations as the ground truth and train SMG on more diversely augmented samples. Instead of directly optimizing the distance between the representations under raw and augmented observations, we introduce two types of consistency losses, considering both attribution and Q-values for more explainable supervision. By doing so, the foreground model can learn to extract task-relevant representations across different deployment scenarios.

**Foreground consistency loss.** To force the agent to focus on the same task-relevant area in transformed scenarios, we train the foreground models to predict the attribution under augmented observation \(Attrib((o_{t}))\) with the supervision of the ground truth attribution \(Attrib(o_{t})\) (as \(Attrib(o_{t})\) is relatively easier to converge to an accurate value, and we discuss it in detail in Appendix F). The foreground consistency loss \(L_{}\) is defined as Equation 7 (where **sg** means the stop-gradient operation).

\[L_{}=_{o_{t}}[Attrib((o_{t}))- (Attrib(o_{t}))]\] (7)

**Q-value consistency loss.** In addition to the attributions, the Q-values obtained from transformed observations also exhibit high variance , indicating instability in both the extracted representations and the Q function. To address this, we regularize the Q-values under augmented observations to be consistent with those under raw observations, as shown in Equation 8. This approach also regularizes the agent to learn an accurate task-relevant representation, as the gradient of \(L_{q\_consist}\) is back-propagated to the latent space.

\[L_{}=_{o_{t},a_{t}}[[Q(f^{+}((o_ {t}),a_{t})-(Q(f^{+}(o_{t}),a_{t}))]^{2}]\] (8)

The above two consistency losses are illustrated in Figure 0(b).

### Overall Objective

Our proposed separated models architecture can seamlessly integrate as a plug-and-play module into any existing off-policy RL algorithms. In this work, we leverage SAC  as the base algorithm. Throughout the training phase, SMG iteratively performs exploration, critic update, policy update, and auxiliary task update. We define the critic loss \(L_{}\) as the sum of the Q-value loss \(L_{}\) and the Q-value consistency loss \(L_{}\):

\[L_{}=L_{}+_{}L_{}\] (9)

Additionally, the auxiliary loss \(L_{}\) comprises five previously mentioned loss terms:

\[L_{}=_{}L_{}+_{} L_{}+_{}L_{}+_{}L_{ }+_{}L_{}\] (10)

Although \(L_{}\) contains five loss terms, experimental results show that using average weights for the first four terms and a smaller weight for the last term can achieve satisfactory performance. Detailed information about hyperparameters tuning is provided in Appendix C.3. The detailed derivation of Equation 2 and Equation 6 are provided in Appendix A.

## 4 Experimental Results

### Setup

We benchmark SMG against the following baselines: (1) SAC , serving as the foundational algorithm for all other baselines; (2) DrQ , utilizing random shift augmentation; (3) SODA , incorporating a consistency loss on latent representations; (4) SVEA , focusing on stabilizing Q-values; (5) SRM , proposing a novel data augmentation technique; (6) SGQN , the previous SOTA method integrating saliency maps into RL tasks. We reproduce the results using the same settings reported in the original papers, with the exception of setting the batch size to 64 for all methods. Additionally, all results are calculated by four random seeds.

Figure 3: Two types of data augmentations using in SMG.

To achieve stable performance across various evaluation settings, we train SMG using a hybrid data augmentation approach for \((o_{t})\), involving random overlay  and attribution augmentation for all tasks (each time we randomly select a type of data augmentation, as shown in Figure 3). The network design for SMG and more detailed experiment settings are reported in Appendix C.

### DMControl Results

We first conduct experiments on five selected tasks from DMControl  and adopt the same evaluation setting as DMControl Generalization Benchmark  (DMC-GB) used, which contains random-colors and video-background modifications across four different levels: _color-easy_, _color-hard_, _video-easy_ and _video-hard_. Figure 5 shows an example in _walker-walk_ task. We train all methods for 500k steps (except _walker-stand_ for 250k, as it converges faster) on the training setting and evaluate the zero-shot generalization performance on the four evaluation settings.

To provide a clear explanation of how SMG reconstructs images, we present the image outputs of _walker-walk_ and _cheetah-run_ after 500k training steps of training in the first two rows of Figure 4. The last four columns illustrate the model outputs necessary for reconstructing the evaluation observations. The predicted attribution (the fifth column) highlights the extracted task-relevant area, which shows SMG accurately depicts the attribution of the input observation while omitting the task-irrelevant elements such as the skybox, the floor, and even the random color variation. This indicates that the task-relevant representation \(z_{t}^{+}\) contains only the information required to accomplish the task, which is crucial for generalization. Note that we aim to maintain the similarity between \(Attrib((o_{t}))\) and \(Attrib(o_{t})\), even in random-color settings. As shown by the first row of _color-hard_ setting, SMG predicts a yellow attribution despite the input evaluation observation being orange.

Figure 4: Visualizing the reconstruction process of SMG in different tasks (from top to bottom: _walker-walk_, _cheetah-run_, _peg in box_).

Figure 5: Example of training and testing observation for DMC-GB (_walker-walk_). (a) is the training observation. (b-c) indicates different degrees of color change; (d-e) replaces the background with random videos, with (e) additionally removing the floor and the walkerâ€™s shadow.

Table 1 reports the generalization performance of SMG and all baseline methods with the video-background modification, which is the most challenging evaluation setting. The table shows that SMG outperforms all baselines in all ten tasks. Particularly impressive is SMG's superiority in _video-hard_; when removing the floor and the walker's shadow, the performance of all baseline methods drops significantly. However, SMG is less affected by this substantial distribution shift and maintains a stable performance across all tasks, with episode returns boosted more than 160 over the second-best in four out of five tasks (as _walker-stand_ is a much easier task to train), showcasing its exceptional generalization capability.

### Robotic Manipulation Results

To further validate SMG's applicability to more realistic tasks, we conduct experiments on two goal-reaching robotic manipulation tasks , including _peg-in-box_ and _reach_, and following similar generalization settings used in . As illustrated in Figure 6, there are five different testing settings with different colors and textures for the background and the table. We train all methods for 250k steps and use random convolutions  as the data augmentation for baseline methods, as it aligns better with the testing scenarios. SMG continued to use hybrid augmentation as previously mentioned.

  
**DMControl** & SAC & DrQ & SODA & SVEA & SRM & SGQN & **SMG** & \(\) \\ (_video-easy_) & & & & (overlay) & & & (ours) & \\  cartpole, & \(175\) & \(606\) & \(617\) & **718** & \(645\) & \(717\) & **839** & \(+121\) \\ swihep & \( 23\) & \( 31\) & \( 76\) & \( 101\) & \( 108\) & \( 77\) & \( 16\) & \(17\%\) \\ finger, & \(171\) & \(511\) & \(615\) & \(817\) & \(642\) & **860** & **952** & \(+92\) \\ spln & \( 37\) & \( 192\) & \( 56\) & \( 94\) & \( 101\) & \( 38\) & \( 48\) & \(11\%\) \\ walker, & \(484\) & \(908\) & \(924\) & \(928\) & \(947\) & **949** & **961** & \(+12\) \\ stand & \( 185\) & \( 38\) & \( 28\) & \( 50\) & \( 14\) & \( 10\) & \( 19\) & \(1\%\) \\ walker, & \(325\) & \(720\) & \(518\) & \(691\) & \(662\) & **830** & **904** & \(+74\) \\ walk & \( 26\) & \( 69\) & \( 92\) & \( 120\) & \( 75\) & \( 58\) & \( 34\) & \(9\%\) \\ cheath, & \(179\) & \(241\) & \(215\) & \(278\) & \(253\) & **308** & **348** & \(+40\) \\ run & \( 65\) & \( 25\) & \( 15\) & \( 51\) & \( 27\) & \( 34\) & \( 28\) & \(13\%\) \\  
**DMControl** & SAC & DrQ & SODA & SVEA & SRM & SGQN & **SMG** & \(\) \\ (_video-hard_) & & & & (overlay) & & & (ours) & \\  cartpole, & \(156\) & \(168\) & \(346\) & \(510\) & \(254\) & **599** & **764** & \(+165\) \\ swihep & \( 16\) & \( 35\) & \( 59\) & \( 177\) & \( 69\) & \( 112\) & \( 32\) & \(28\%\) \\ finger, & \(22\) & \(54\) & \(310\) & \(353\) & \(131\) & **710** & **910** & \(+200\) \\ spln & \( 10\) & \( 44\) & \( 72\) & \( 71\) & \( 89\) & \( 159\) & \(+61\) & \(28\%\) \\ walker, & \(212\) & \(278\) & \(406\) & \(814\) & \(558\) & **870** & **955** & \(+85\) \\ stand & \( 41\) & \( 79\) & \( 68\) & \( 57\) & \( 139\) & \( 78\) & \( 9\) & \(10\%\) \\ walker, & \(132\) & \(110\) & \(175\) & \(348\) & \(165\) & **634** & **814** & \(+180\) \\ walk & \( 26\) & \( 33\) & \( 31\) & \( 80\) & \( 99\) & \( 136\) & \( 51\) & \(28\%\) \\ cheath, & \(56\) & \(38\) & \(118\) & \(105\) & \(87\) & **135** & **303** & \(+168\) \\ run & \( 30\) & \( 26\) & \( 40\) & \( 13\) & \( 24\) & \( 44\) & \( 46\) & \(124\%\) \\   

Table 1: DMControl results in video-background settings. We evaluate each seed five times and calculate the mean value. Then, we calculate the mean and standard deviation with four random seeds. Red indicates the best and blue indicates the second-best. \(=\) improvement of SMG over the second best.

Figure 6: Examples of training and testing observation for the robotic environment (_Peg-in-box_). (b-f) indicates five different evaluation settings varying in background colors and table textures.

Table 2 presents the evaluation results for _peg-in-box_, a task where a robot must insert a peg tied to its arm into a box. SMG achieves dominant performance across all evaluation settings, boosting an average improvement of \(102\%\) over the second-best method. Impressively, SMG exhibits remarkable stability across the six evaluation settings, with a standard deviation of only 7, while baseline methods all fail in some evaluation settings. This underscores SMG's generalization capability. These results also highlight SMG's superiority in realistic tasks, as its reconstruction-based auxiliary loss can capture more detailed features in the image, which is hard for methods that mainly rely on data augmentation techniques.

### Ablation Study

In order to explore the role played by different loss terms in SMG, we conduct an ablation study in DMControl tasks. Table 3 presents the performance drop without each loss term compared to the full model in the _video-hard_ setting. The results indicate that every loss term contributes significantly to the final performance. Notably, \(L_{q,\_constit}\) exhibits the most substantial impact on performance, highlighting the importance of maintaining stable Q-value estimation in generalization tasks. Moreover, the performance drop without \(L_{back}\) or \(L_{mask}\) is around \(20\%\) to \(30\%\), underlining the importance of attribution augmentation in enhancing SMG's generalization in video-background settings, as the two loss terms directly affect the quality of the attribution augmentation. Additionally, \(L_{action}\) aids in learning a better task-relevant representation. As for \(L_{\_}{form,\_constit}\), it also contributes to improving generalization ability, particularly in relatively challenging tasks where the performance improvement ranges from \(15\%\) to \(25\%\).

  
**Robtic-Manipulation** & SAC & DrQ & SODA & SVEA (overlay) & SRM & SGQN & **SMG** & \(\) \\  (_peg-in-box_) & & & & (overlay) & & & (ours) & \\  train & \(31\) & \(\) & \(232\) & \(212\) & \(227\) & \(232\) & \(\) & \(+4\) \\  & \( 73\) & \( 14\) & \( 20\) & \( 39\) & \( 15\) & \( 19\) & \( 16\) & \(2\%\) \\ test1 & \(-33\) & \(\) & \(34\) & \(-18\) & \(55\) & \(-67\) & \(\) & \(+174\) \\  & \( 25\) & \( 99\) & \( 143\) & \( 59\) & \( 98\) & \( 28\) & \( 18\) & \(276\%\) \\ test2 & \(-42\) & \(-40\) & \(76\) & \(85\) & \(11\) & \(\) & \(\) & \(+25\) \\  & \( 31\) & \( 77\) & \( 119\) & \( 68\) & \( 54\) & \( 51\) & \( 37\) & \(136\) \\ test3 & \(-8\) & \(15\) & \(66\) & \(67\) & \(147\) & \(\) & \(\) & \(+39\) \\  & \( 46\) & \(107\) & \( 147\) & \( 73\) & \( 114\) & \( 31\) & \( 15\) & \(20\%\) \\ test4 & \(-42\) & \(72\) & \(80\) & \(109\) & \(\) & \(-51\) & \(\) & \(+125\) \\  & \( 51\) & \( 28\) & \( 122\) & \( 98\) & \( 123\) & \( 46\) & \( 17\) & \(112\%\) \\ test5 & \(-52\) & \(-54\) & \(-104\) & \(-26\) & \(\) & \(-108\) & \(\) & \(+94\) \\  & \( 31\) & \( 30\) & \( 51\) & \( 102\) & \( 122\) & \( 24\) & \( 15\) & \(66\%\) \\ 
**Average** & \(-24\) & \(48\) & \(64\) & \(72\) & \(\) & \(66\) & \(\) & \(+118\) \\  & \( 28\) & \( 95\) & \( 98\) & \( 80\) & \( 69\) & \( 143\) & \( 7\) & \(102\%\) \\   

Table 2: Robotic manipulation results in _peg-in-box_. Red indicates the best and blue indicates the second-best. \(=\) improvement of SMG over the second best. The last row reports the average performance over all six evaluation settings.

  
**DMControl** & SMG & w/o \(L_{\_}{\_}{}\) & w/o \(L_{\_}{}\) & w/o \(L_{\_}{}\) & w/o \(L_{\_}{}\) & w/o \(L_{\_}{}\) \\ (video hard) & (full) & & & & & & \\  cartpole, & \(764 32\) & \(720 100\) & \(631 92\) & \(763 44\) & \(590 84\) & \(302 30\) \\ swingup & & \(-44\) (\(\%\)) & \(-133\) (\(17\%\)) & \(-1\) (\(0\%\)) & \(-174\) (\(23\%\)) & \(-462\) (\(60\%\)) \\ finger, & \(910 61\) & \(695 103\) & \(609 352\) & \(412 170\) & \(731 130\) & \(509 83\) \\ spin & & \(-215\) (\(24\%\)) & \(-301\) (\(35\%\)) & \(-498\) (\(5\%\)) & \(-179\) (\(20\%\)) & \(-401\) (\(44\%\)) \\ walker, & \(955 9\) & \(885 45\) & \(855 96\) & \(775 144\) & \(836 127\) & \(432 210\) \\ stand & & \(-70\) (\(7\%\)) & \(-100\) (\(10\%\)) & \(-180\) (\(19\%\)) & \(-119\) (\(12\%\)) & \(-523\) (\(55\%\)) \\ walker, & \(814 51\) & \(642 63\) & \(670 22\) & \(657 103\) & \(416 98\) & \(282 34\) \\ walk & \(-172\) (\(21\%\)) & \(-144\) (\(18\%\)) & \(-157\) (\(19\%\)) & \(-398\) (\(49\%\)) & \(-532\) (\(65\%\)) \\ cheath, & \(303 46\) & \(247 40\) & \(212 52\) & \(233 1\) & \(162 100\) & \(130 37\) \\ run & & \(-56\) (\(18\%\)) & \(-91\) (\(30\%\)) & \(-70\) (\(23\%\)) & \(-141\) (\(47\%\)) & \(-173\) (\(57\%\)) \\ 
**Average** & \(-15\%\) & \(-22\%\) & \(-23\%\) & \(-30\%\) & \(-56\%\) \\   

Table 3: Ablation study in DMControl (_video-hard_). Red indicates the performance drop of the ablated model compared to the full model.

To better grasp the significance of \(L_{}\) and \(L_{}\) in SMG, we showcase the predicted masks and their corresponding attribution augmentations in Figure 7. When \(L_{}\) is removed, the model generates an almost white mask, indicating that the foreground model overly captures irrelevant features without the constraint of mask ratio loss. Consequently, only a few parts are replaced by a random image in the attribution augmentation. In contrast, removing \(L_{}\) causes the background model to learn all features excessively, resulting in attribution augmentation images devoid of task-relevant information. The ablation results underscore that both \(L_{}\) and \(L_{}\) are vital in crafting meaningful attribution augmentations, which in turn are utilized by the two consistency losses and impact the representation learning process. We conduct more experiments in Appendix E to reveal that \(L_{}\) serves as a guiding factor in mask learning and SMG is not significantly influenced by variations in the hyperparameter mask ratio \(\).

## 5 Related Work

**Improving generalization ability of RL agents** has drawn increasing attention in recent years. Researchers primarily explore two aspects: using data augmentation techniques to inject useful priors when training [20; 15; 16; 22; 14; 32; 38] and employing various auxiliary tasks to guide the learning process [13; 3; 1; 42; 40; 12]. For example, Hansen and Wang  regularize the representations between observations with its augmented view through an auxiliary prediction task; Hansen et al.  stabilize Q-values via delicately design the data augmentation process; Bertoin et al.  introduce saliency maps to visualize the focus of Q-functions; Wang et al.  extract the foreground objects by employing a segment anything model. Orthogonal to existing works, we argue that focusing the RL agent on task-relevant features across diverse deployment scenarios can substantially boost the generalization capability. We propose a novel reconstruction-based auxiliary task to achieve this goal.

**Decision-making based on task-relevant features** can substantially enhance the performance and robustness of RL agents [4; 45; 43; 29]. Bharadhwaj et al.  use an empowerment term to distill control-relevant features from the task; Zhu et al.  bolster the resilience of RL agents by regularizing the posterior predictability; Zhang et al.  learns compact representations by bisimulation metrics. Additionally, methods utilizing separated model architectures to extract different types of features simultaneously have been proposed [6; 39; 30; 25; 37]. For instance, Wang et al.  decompose the latent state into four parts based on their interaction with actions and rewards; Pan et al.  leverage both controllable and non-controllable states in policy learning; Wan et al.  apply task-relevant features to imitation learning. Our work also employs separated models. However, we prudently design this architecture in a model-free setting and propose novel loss terms to enhance the accuracy of image predictions.

A detailed comparison between SMG and other methods is provided in Appendix F.2.

## 6 Conclusion and Future Work

In this paper, we propose SMG for visual-based RL generalization and show its superiority in sample efficiency, stability, and generalization through extensive experiments. The success of SMG can be attributed to two key factors: (i) a delicately designed reconstruction-based auxiliary task with separated models architecture, which enables the RL agent to extract task-relevant and task-irrelevant representations from visual observations simultaneously; (ii) two consistency losses to further guide the RL agent's focus under deployment scenarios. We believe that the proposed method can be applied to a wide range of tasks.

Figure 7: Predicted masks and corresponding attribution augmentations. (a) is the full model, (b) and (c) are the models without \(L_{}\) and \(L_{}\) respectively.

SMG is particularly well-suited for robotic manipulation tasks in realistic scenarios. However, when the observation contains too many task-relevant objects, the complexity of accurately learning a mask increases. This can lead to a decline in SMG's performance. For instance, in an autonomous navigation task, the presence of numerous pedestrians in the view makes it challenging to accurately mask all of them.

The future work includes exploring more advanced backbones for task-relevant feature extraction, taking into account the generalization on non-static camera viewpoints and the test of SMG on realistic tasks to verify its generalization ability in real applications.