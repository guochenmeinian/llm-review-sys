# Challenges with unsupervised LLM knowledge discovery

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We reveal novel pathologies in existing unsupervised methods seeking to discover latent knowledge from large language model (LLM) activations--instead of knowledge they seem to discover whatever feature of the activations is most prominent. These methods search for hypothesised consistency structures of latent knowledge. We first prove theoretically that arbitrary features (not just knowledge) satisfy the consistency structure of a popular unsupervised knowledge-elicitation method: contrast-consistent search . We then present a series of experiments showing settings in which this and other unsupervised methods result in classifiers that do not predict knowledge, but instead predict a different prominent feature. We conclude that existing unsupervised methods for discovering latent knowledge are insufficient, and we contribute sanity checks to apply to evaluating future knowledge elicitation methods. We offer conceptual arguments grounded in identification issues such as distinguishing a model's knowledge from that of a simulated character's that are likely to persist in future unsupervised methods.

## 1 Introduction

Large language models (LLMs) perform well across a variety of tasks [30; 10] in a way that suggests they systematically incorporate information about the world . As a shorthand for the real-world information encoded in the weights of an LLM we could say that the LLM encodes _knowledge_.

Accessing that knowledge is hard, because the factual statements an LLM outputs do not reliably describe it [23; 2; 32]. For example, LLMs might repeat common misconceptions  or strategically deceive users . If we could elicit the latent knowledge of an LLM  it would allow us to detect and mitigate "dishonesty" . It would also help when supervising outputs that are difficult to understand as well as improving scientific understanding of the inner workings of LLMs. Importantly, this must be done without supervision because we lack a ground truth for what the model "knows", as opposed to what we know.

Contrast-consistent search (CCS)  is a prominent method proposed to address this problem by assuming that "knowledge" satisfies a consistency structure that few other features in an LLM are likely to satisfy. They use this consistency to construct a classifier which they claim detects a model's latent knowledge, a claim which is widely repeated in the literature (see Appendix B). We refute these claims by identifying classes of LLM features that also satisfy this consistency structure but are not knowledge. We prove two theorems: 1) a class of arbitrary binary classifiers are optimal under the CCS loss; 2) any classifier can be transformed to an arbitrary classifier with the same CCS loss. The upshot is that the CCS consistency structure is more than just slightly imprecise in identifying knowledge--it is compatible with arbitrary patterns.

We then show that other unsupervised methods in addition to CCS empirically do not discover knowledge, regardless of any inductive biases that might hypothetically be present. Two didactic experiments show that these methods can latch onto artificial distracting features instead of knowledge. Our third experiment moves towards realism by showing that these knowledge-discovery methods can latch onto implicit opinions. The fourth is almost fully natural: we show that the method's results are highly sensitive to reasonable prompt variants which have been used in the literature.

We conclude that existing unsupervised knowledge-discovery methods are insufficient in practice, and we propose principles for evaluating knowledge elicitation methods to prevent future "false-positives" in the literature. We hypothesise that our conclusions will generalise to more sophisticated methods, though perhaps not the exact experimental results: using different consistency structures of knowledge will likely suffer from similar issues to what we show here. Our key contributions are as follows:

* We prove that arbitrary features satisfy the CCS loss equally well.
* We show that unsupervised methods detect prominent features that are not knowledge.
* We show that the features discovered by unsupervised methods are sensitive to prompts and that we lack principled reasons to pick any particular prompt.

## 2 Background

**Contrastive LLM activations.** We focus on methods that train probes  using LLM activation data. This data is constructed using _contrast pairs_. A contrast pair is a pair of strings with opposite 'claim' for some characteristic of interest which can be used to study the contrast in how an LLM represents that characteristic. For example, a contrast pair might be "Are cats mammals? Yes." and "Are cats mammals? No." Potentially, pairs like this could then be used to study how LLMs represent correctly/incorrectly answered questions.

Burns et al.  show how to generate such contrast pairs from a dataset of binary questions, \(Q=\{q_{i}\}_{i=1}^{N}\), such as "Are cats mammals?" by, for example, appending "Yes." and "No." for a positive and negative member of a contrast pair \((x_{i}^{+},x_{i}^{-})\). The LLM's representations of each member of the pair can then be computed by looking at the activations from an intermediate layer after the sequence of tokens, \((x_{i}^{+})\) and \((x_{i}^{-})\). If one just looked at these activations, their differences might be dominated just by the presence of the tokens "Yes." or "No." Burns et al.  therefore propose a normalisation step which strips away the average effect of those tokens across the dataset: setting \((x_{i}^{+/-})((x_{i}^{+/-})-^{+/-})/^{+/-}\) where \(^{+/-},^{+/-}\) are \(\{(x_{i}^{+/-})\}_{i=1}^{N}\)'s mean and standard deviation. This is meant to remove these tokens' unintended influence but prior work questions this, and some of our results also question this.

**Contrast-consistent Search (CCS) .** An unsupervised learning algorithm using contrast pairs constructed to reflect a characteristic of interest to recover the features of LLM activations that

Figure 1: **Prominent features distract unsupervised latent knowledge detectors** (see Section 4.2). **Left:** We apply two transformations to a dataset of movie reviews, \(\{q_{i}\}\). First (novel to us) we insert a distracting feature by appending either “Alice thinks it’s positive” or “Alice thinks it’s negative” at random to each question. Second, we create contrast pairs , \((x_{i}^{+},x_{i}^{-})\), appending “It is positive” or “It is negative” to each. **Middle:** The LLM activations for these strings are \((x_{i}^{+}),(x_{i}^{-})\). **Right:** A PCA visualisation of the top-3 activation dimensions. Without “Alice...”, a classifier finds the review sentiment (orange/blue). But with “Alice...” a classifier finds Alice’s opinion (light/dark) ignoring review sentiment.

represent that characteristic. CCS uses the LLM's representations to predict correct labels, intending to study cases where the LLM's knowledge is true. CCS assumes that LLM knowledge representations are credences which follow probabilistic laws. Softly encoding this constraint, they minimise

\[_{}=_{i=1}^{N}^{+})- (1-p(x_{i}^{-}))]^{2}}^{_{}}+^{+}),p(x_{i}^{-})\}^{2}}^{_{}}\] (1)

for a function from the normalised LLM activations from the contrast pairs: \(p(x)=(^{T}(x)+b)\) (a linear function with sigmoid). The motivation is that the \(_{}\) encourages negation-consistency (that a statement and its negation should have probabilities that add to one), and \(_{}\) encourages confidence to avoid \(p(x_{i}^{+}) p(x_{i}^{-}) 0.5\). For inference on a question \(q_{i}\) the _average prediction_ is \((q_{i})=[p(x_{i}^{+})+(1-p(x_{i}^{-}))]/2\) and then the _induced classifier_ is \(f_{p}(q_{i})=[(q_{i})>0.5]\). 1

Activation clustering with PCA and k-means.We consider two other unsupervised learning methods. In both cases we cluster the _difference_ in contrastive activations, \(\{(x_{i}^{+})-(x_{i}^{-})\}_{i=1}^{N}\). In one case, these are clustered by applying principal component analysis (PCA) and thresholding the top component at 0 .2 The other clusters with k-means with two clusters.

Logistic regression.As a supervised baseline, we use logistic regression on concatenated contrastive activations, \(\{((x_{i}^{+}),(x_{i}^{-}))\}_{i=1}^{N}\) with labels \(a_{i}\), and treat this as a ceiling (since it uses labels).

Random baseline.We compare to a random baseline using a probe with random parameter values, treating that as a floor (as it does not learn from input data) . Further details are in Appendix C.3.

## 3 Theoretical Results

Our theoretical results focus on CCS, showing that CCS's consistency structure isn't specific to knowledge. This implies that arguments for CCS's effectiveness cannot be grounded in conceptual or principled motivations from the loss construction. In later sections, we also address other methods which do not rely on these strong consistency assumptions and show that heuristic arguments grounded in inductive biases do not support using any of these as knowledge-discovery methods.

As illustration, consider the IMDb sentiment classification task . A given question \(q_{i}\) considers whether a movie review has a particular _sentiment_, \(s(q_{i})[q_{i}]\), and is converted into a contrast pair of \(x_{i}^{+}\) and \(x_{i}^{-}\), each of which has a _claim_\(c()\) about the sentiment. Specifically, \(c(x_{i}^{+})=1\), a claim that the sentiment is positive, and \(c(x_{i}^{-})=0\) for negative. The desired probe, \(p^{*}\), detecting the truth feature must check whether the sentiment and the claim agree. This can be done by XOR (denoted \(\)) of the sentiment and the claim:

\[p^{*}(x_{i}^{})[x_{i}^{}]=s(q_{i}) c(x_{i}^{}).\] (2)

The induced probe for this feature is the sentiment as desired: \(f_{p^{*}}(q_{i})=s(q_{i})\). Our key insight is that the CCS loss is low just because of this XOR, not the sentiment, and so the same construction can work for arbitrary features of the question: given some feature \(h\), the probe \(p(x_{i}^{})=h(q_{i}) c(x_{i}^{})\) gets low CCS loss and has an induced probe \(h\).

_Theorem 1_.: Let feature \(h:Q\{0,1\}\), be any arbitrary map from questions to binary outcomes. Let \((x_{i}^{+},x_{i}^{-})\) be the contrast pair corresponding to question \(q_{i}\) and let \(c(x_{i}^{+})=1,c(x_{i}^{+})=0\). Then the probe defined as \(p(x_{i}^{})=h(q_{i}) c(x_{i}^{})\) achieves optimal loss, and the averaged prediction satisfies \((q_{i})=h(q_{i})\).

That is, the classifier that CCS finds is under-specified: for _any_ binary feature, \(h\), on the questions, there is a probe with optimal CCS loss that induces that feature. The proof comes directly from inserting our constructive probes into the loss definition--equal terms cancel to zero (see Appendix A).

In Thm. 1, the probe \(p\) is binary since \(h\) is binary, but in practice probe outputs are produced by a sigmoid and so are in \((0,1)\). Can we say anything about this setting? We show that it is possible to transform a soft probe for one feature into a soft probe for any other arbitrary feature. In the binary case, the desired probe for feature \(h_{1}\) is \(p_{1}=h_{1} c\), and the desired probe for \(h_{2}\) is \(h_{2} c\). So, we have \(p_{2}=p_{1} h_{1} h_{2}\). To generalize this to soft probes, we extend \(\) as follows:

\[(a b)(x)[1-a(x)]\,b(x)+[1-b(x)]\,a(x).\] (3)

In addition, we correct the CCS loss to fix an unmotivated downwards bias in the loss proposed by Burns et al.  (see Appendix A.2). We also use this symmetrized loss in our experiments. After this, the transformation between probes works as desired, proving that there is an arbitrary classifier encoded by a probe with identical CCS loss to the original:

_Theorem 2_.: Let \(g:Q\{0,1\}\), be any arbitrary map from questions to binary outputs. Let \((x_{i}^{+},x_{i}^{-})\) be the contrast pair corresponding to question \(q_{i}\). Let \(p\) be a probe, whose average result \(=0.5[p(x_{i}^{+})+(1-p(x_{i}^{-}))]\) induces a classifier \(f_{p}(q_{i})=[(q_{i})>0.5]\). Define the transformed probe \(p^{}(x_{i}^{})=p(x_{i}^{})[f_{p}(q_{i}) g(q_{i})]\). Then \(_{}(p^{})=_{}(p)\) and \(p^{}\) induces the classifier \(f_{p^{}}(q_{i})=g(q_{i})\).

However, which probe is actually learned depends on inductive biases; these could depend on the prompt, optimization algorithm, or model choice. These theorems prove that optimal arbitrary probes exist, but not necessarily that they are actually learned or that they are expressible in the probe's function space. But for inductive biases, no robust argument ensures the desired behaviour. The feature that is most prominent--favoured by inductive biases--could turn out to be knowledge, but it could equally turn out to be the contrast-pair mapping itself (which is partly removed by normalisation) or anything else. We do not have any theoretical reason to think that CCS discovers knowledge probes. In fact, experimentally, we now show that, in practice, several methods including CCS often discover probes for features other than knowledge.

## 4 Experiments

Our experiments a structured didactically. We begin with simplified experiments that use unrealistic but clear-cut interventions to develop understanding, gradually increasing realism. Section 4.4 closes with an experiment that uses entirely natural prompts that have been used by others, demonstrating that these issues appear in practice. Unless otherwise noted, experiments follow details below.

**Datasets.** We investigate three datasets used by Burns et al. .3 The IMDb dataset of movie reviews classifies positive/negative sentiment , BoolQ  answers yes/no questions about a passage, DBpedia  is text topic-classification. Prompt templates for each dataset are in Appendix C.1.4

**Language Models.** We use three different language models. To directly compare to Burns et al.  we use T5-11B,  with 11 billion parameters. We further use an instruction fine-tuned version of T5-11B called T5-FLAN-XXL,  to understand the effect of instruction fine-tuning. Both are encoder-decoder architectures, and we use the encoder output for our activations. We also use Chinchilla-70B , with 70 billion parameters, which is larger scale, and a decoder-only architecture. We take activations from layer 30 (of 80) of this model, though see Appendix D.2.3 for results on other layers, often giving similar results. Notably, K-means and PCA have good performance at layer 30 with less seed-variance than CCS, suggesting contrast pairs and standard unsupervised learning, rather than the CCS consistency structure, are key (see Footnote 2).

**Experiment Setup.** In each experiment we compare a default setting which is the same/similar to that used in  to a modified setting that we introduce in order to show an effect - differing only in their text prompt. We then generate contrastive activations and train probes using the methods in Section 2: CCS, PCA, k-means, random and logistic regression. Training details can be found in Appendix C.3. For each method we use 50 random seeds. Our figures in general come in two types: violin plots which compare the accuracy of different methods; and three-dimensional PCA projections of the activations to visualise how they are grouped. We show one dataset and model, other datasets and models, shown in the appendix, are similar except where discussed.

### Discovering random words

Motivated by our theoretical results, we first introduce a distracting binary feature and show the unsupervised methods discover this feature rather than knowledge. We focus here on IMDB and Chinchilla (see Appendix D.1 for other datasets and models with similar results). Our default prompts use the standard template from Burns et al.  inserting different reviews and labels "positive" or "negative".

Our modified prompts further append a full stop and space, then one of two random words, "Banana" and "Shed". In the language of Thm. 1 we take a random partition of question indices, \(\{1,,N\}=I_{0} I_{1}\), with \(|I_{0}|=|I_{1}|\), and set the binary feature \(h\) such that \(h(q_{i})=0\) for \(i I_{0}\) and \(h(q_{i})=1\) for for \(i I_{1}\). "Banana" is inserted if \(h(q_{i})=0\), and "Shed" is inserted if \(h(q_{i})=1\). See Figure 1 for illustration - though here we append "Banana" or "Shed" to the end, rather than inserting "Alice...".

Our results are shown in Figure 1(a), displaying accuracy of each method (x-axis groups). Default prompts are blue and modified banana/shed prompts are red. We look at the standard ground-truth accuracy metric (dark), as well as a modified accuracy metric that measures whether Banana or Shed was inserted (light). We see that for all unsupervised methods, default prompts (blue) score highly on ground truth accuracy (dark blue), in line with results in Burns et al. . However, for the banana/shed prompts we see 50%, random chance, on ground truth accuracy (dark red). On Banana/Shed accuracy (light red) both PCA and K-means score highly, while CCS shows a bimodal distribution with a substantial number of seeds with 100% Banana/Shed accuracy - seeds differ only in the random initialisation of the probe parameters. The takeaway is that CCS and other unsupervised methods do not optimise for ground-truth knowledge, but rather track whatever feature (in this case, banana/shed) is most prominent in the activations.

Figure 1(b) shows a visualisation of the top three components of PCA for the default (left) and modified (right) prompts. In the modified case we see a prominent grouping of the data into dark/light (banana/shed) and, less prominently, into blue/orange (the review). This provides visual evidence that both features (ground-truth and banana/shed) are represented, but the one which is most prominent in this case is banana/shed, in correspondence with Figure 1(a).

### Discovering an explicit opinion

It is unlikely that such a drastic feature, ending with "Banana"/"Shed", would actually exist in a real dataset. These words had nothing to do with the rest of the text. In our second experiment we make a more realistic modification: inserting a character's explicit opinion of whether the review is positive or negative. What we will find is that the unsupervised methods learn to predict the character's opinion, instead of the sentiment of the actual review, presumably by learning a probe that detects whether the claimed sentiment agrees with the character's opinion.

Figure 2: **Discovering random words. Chinchilla, IMDb. (a) The methods distinguish whether the prompts end with banana/shed rather than the review sentiment. (b) PCA visualisation of top-3 activation dimensions, in default (left) and modified (right) settings, shows the clustering into banana/shed (light/dark) rather than review sentiment (blue/orange).**

[MISSING_PAGE_FAIL:6]

In the modified setting5, Alice answers the few-shot examples correctly, except when topic is company - and in that case gives explanations like "[...] Alice always says the wrong answer when the topic of the text is company, because she doesn't like capitalism [...]". What we are looking for is what the unsupervised methods predict on the final example when Alice has not yet stated an opinion: will it predict the correct answer, ignoring how Alice previously answered incorrectly about company; or will it predict Alice's opinion, answering incorrectly about company?

To highlight the effect, we use a subset dataset where 50% of sentences are about "company", and 50% have one of the remaining thirteen categories (non-company) as a topic. We apply truth-disambiguation only to the subset with non-company topics, so that we can see the possible effect of predicting incorrectly on company data (otherwise the assignment might be flipped).

Our results are shown in Figure 4. We look at default prompts (blue) and modified prompts (red) and split the data into whether the topic is company (dark) or non-company (light) and look at the standard ground-truth accuracy metric. The default setting (blue) produces high accuracy classifiers both when the topic is company (dark blue) and other categories (light blue). In the modified setting (red) CCS gives a bimodal distribution when the topic is company (dark red), with almost half of the probes (differing only in random initialisation) predicting Alice's opinion, rather than the actual topic. In contrast, it performs well over all other categories (light red) and so is not just an ordinary failure. Other unsupervised methods are less sensitive to the modified setting, scoring high accuracy when the topic is company.

However, when we visualise the first three PCA dimensions of the contrast pair activations (Figure 3(b)) we see four distinct clusters in the modified prompt case (right) showing how a detector might cluster either the actual topic choice (orange vs blue) or based on the data subset: non-company vs company (light vs dark). This shows these methods are still sensitive to the modified setting, which was not evident from the accuracy metric alone.

### Prompt template sensitivity

The next experiment is more natural because, rather than introducing a feature deliberately, we examine three natural prompt templates which have appeared in the literature and show how these change the discovered feature. We use TruthfulQA , a difficult question answering dataset which exploits the fact that LLMs tend to repeat common misconceptions.

Figure 4: **Discovering an implicit opinion.** (a) Default (blue) and modified (red) for company (dark) and non-company (light) data. The modified setting on company data (dark red) leads to a bimodal distribution for CCS with almost half of the probes (differing only in random initialisation) learning Alice’s opinion. In contrast, it performs relatively well over all other categories (light red). (b) PCA: Left – default activations show a possible separation along X-axis corresponding to topic choice (blue vs. orange) and further separation into company/non-company (light/dark). Right – modified activations show a more pronounced company/non-company split. All results are for Chinchilla 70B.

We find that a "non-default" prompt gives the "best performance" in the sense of the highest test-set accuracy. This highlights the reliance of unsupervised methods on implicit inductive biases which cannot be set in a principled way. It is not clear which prompt is the best one for eliciting the model's latent knowledge. Given that the choice of prompt appears to be a free variable with significant effect on the outcomes, conceptual motivations for the loss do not imply a principled foundation for the resulting classifier.

Our prompt templates can be found in Appendix C.1.4. Our "default" template is adapted directly from Burns et al. . Two modified templates are adapted from Lin et al. 6 in which a Professor character is instructed to interpret questions literally. We used this text verbatim inserted into an instructing template in order to make sure that we were looking at natural prompts that people might ordinarily use without trying to see a specific result. We also try a "literal" prompt, removing explicitly mentioning a Professor, in case explicitly invoking a character matters.

Results are shown in Figure 4(a) for Chinchilla70B. The default setting (blue) gives worse accuracy than the literal/professor (red, green) settings, especially for PCA and k-means. PCA visualisations are shown in Figure 4(b), coloured by whether the question is True/False, in the default (left), literal (middle) and professor (right) settings. We see clearer clusters in the literal/professor settings. Other models are shown in Appendix D.4, with less systematic differences between prompts, though the accuracy for K-means in the Professor prompt for T5-FLAN-XXL are clearly stronger than others.

## 5 Related Work

We want to detect when an LLM is dishonest [23; 2; 32], outputting text which contradicts its encoded knowledge . An important part of this is to elicit latent knowledge from a model . There has been some debate as to whether LLMs "know/believe" anything [6; 37; 24] but, for us, the important thing is that something in an LLM's weights causes it to make consistently successful predictions, and we would like to access that. Zou et al.  train unsupervised probes for a range of concepts including honesty, using pairs which need not take opposite truth values (as in Burns et al. ). Belrose et al.  use unsupervised probes on intermediate LLM layers to elicit latent _predictions_. Others (see  and references therein) aim to detect when a model has knowledge/beliefs about the world, to improve truthfulness.

Contrast-consistent search (CCS)  attempts to elicit latent knowledge using unsupervised learning on contrastive LLM activations (see Section 2), claiming that knowledge has special structure that can be used as an objective function which, when optimised, will discover latent knowledge. We have refuted this claim, theoretically and empirically, showing that CCS performs similarly to other unsupervised methods which do not use special structure of knowledge. Emmons  also observe

Figure 5: **Prompt sensitivity on TruthfulQA  for Chinchilla70B. (a) In default setting (blue), accuracy is poor. When in the literal/professor (red, green) setting, accuracy improves, showing the unsupervised methods are sensitive to irrelevant aspects of a prompt. (b) PCA of the activations based on ground truth, blue vs. orange, in the default (left), literal (middle) and professor (right) settings. We see do not see ground truth clusters by default, but see this with other prompts.**

this from the empirical data provided in . Huben  hypothesises there could be many truth-like features, due to LLMs ability to role-play , which a method like CCS might find. Roger  discover multiple knowledge-like classifiers. Levinstein and Herrmann  finds that CCS sometimes learns features uncorrelated with truth, arguing that consistency alone cannot guarantee truth. Fry et al.  modify CCS to improve accuracy despite probes clustering around 0.5, casting doubt on the probabilistic interpretation of CCS probes. In contrast to all these works, we prove theoretically that CCS does not optimise for knowledge, and show empirically what non-knowledge features CCS instead finds.

Our focus in this paper has been on unsupervised learning, though several other methods to train probes to discover latent knowledge use supervised learning [4; 25; 29; 39; 14]. Following Burns et al.  we also reported results using a supervised logistic regression baseline, which we have found to work well on all our experiments, and which is simpler than in those cited works. Our result is analogous to the finding that disentangled representations seemingly cannot be identified without supervision . There are also attempts to detect dishonesty by supervised learning on LLM outputs under conditions that produce honest or dishonest generations . We do not compare directly to this, focusing instead on methods that search for features in activation-space.

## 6 Discussion and Conclusion

General principles.The specific experiments we use are tailored to the methods that we are evaluating. But they instantiate more general principles, which we provide in order to help future work catch similar issues. A proposed method should:

1. be invariant under irrelevant transformations of the prompt;
2. not be sensitive to specific personas;
3. should explain why and when inductive biases make the model's knowledge most salient;
4. should not be easily distracted by a non-knowledge feature.

We show that none of the methods we consider in this paper satisfy these desiderata.

Limitation: generalizability to future methods.Our experiments can only focus on current methods. Perhaps future unsupervised methods could leverage additional structure beyond negation-consistency, and so truly identify the model's knowledge? While we expect that such methods could avoid the most trivial distractors, we speculate that they will nonetheless be vulnerable to similar critiques. The main reason is that we expect powerful models to be able to simulate the beliefs of other agents . Since features that represent agent beliefs will naturally satisfy consistency properties of knowledge, methods that add new consistency properties could still learn to detect such features rather than the model's own knowledge. Indeed, in Figures 3 and 4, we show that existing methods produce probes that report the opinion of a simulated character.7

Another response could be to acknowledge that there will be _some_ such features, but they will be few in number, and so you can enumerate them and identify the one that represents the model's knowledge . Conceptually, we disagree: language models can represent _many_ features , and it seems likely that features representing the beliefs of other agents would be quite useful to language models. For example, for predicting text on the Internet, it is useful to have features that represent the beliefs of different political groups, different superstitions, different cultures, various famous people, and more.

Conclusion.Existing unsupervised methods are insufficient for discovering latent knowledge, though constructing contrastive activations may still serve as a useful interpretability tool. We contribute sanity checks for evaluating methods using modified prompts and metrics for features which are not knowledge. Unsupervised approaches have to overcome the identification issues we outline, while supervised approaches have the problem of requiring accurate human labels even in the case of models that know things human overseers do not. The relative difficulty of each remains unclear. Future work should continue to develop empirical testbeds for eliciting latent knowledge.