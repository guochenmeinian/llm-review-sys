# Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL

Peng Cheng\({}^{*}\)\({}^{1,3}\), Xianyuan Zhan\({}^{*}\)\({}^{2,4}\), Zhihao Wu\({}^{1}\)\({}^{1,3}\), Wenjia Zhang\({}^{2}\),

**Shoucheng Song\({}^{1,3}\), Han Wang\({}^{1,3}\), Youfang Lin\({}^{1,3}\), Li Jiang\({}^{2}\)**

\({}^{1}\) Beijing Jiaotong University, Beijing, China

\({}^{2}\) Tsinghua University, Beijing, China

\({}^{3}\) Beijing Key Laboratory of Traffic Data Analysis and Mining, Beijing, China

\({}^{4}\) Shanghai Artificial Intelligence Laboratory, Shanghai, China

pcheng6@126.com, zhanxianyuan@air.tsinghua.edu.cn,

zhwu, yflin@bjtu.edu.cn

Equal contribution.Corresponding Author.

###### Abstract

Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. These can be readily used to construct a new offline RL algorithm (TSRL) with less conservative policy constraints and a reliable latent space data augmentation procedure. Based on extensive experiments, we find TSRL achieves great performance on small benchmark datasets with as few as 1% of the original samples, which significantly outperforms the recent offline RL algorithms in terms of data efficiency and generalizability. Code is available at: https://github.com/pcheng2/TSRL

## 1 Introduction

The recently emerged offline reinforcement learning (RL) provides a new paradigm to learn policies from pre-collected datasets without the need to interact with the environments . This is particularly desirable for solving practical tasks, as interacting with real-world systems can be costly or risky, and high-fidelity simulators are also hard to build . However, existing offline RL methods have high requirements on the size and quality of offline datasets in order to achieve reasonable performance. When such requirements are not met, these algorithms may suffer from severe performance drop, as illustrated in Figure 1. Current offline RL algorithms are trained and validated on benchmark datasets (e.g., D4RL ) that contain millions of transitions for simple tasks. Whereas under realistic settings, it is often impractical or costly to collect such a large amount of data, and the real datasets might only narrowly cover the state-action space. Clearly, learning reliable policies fromsmall datasets with partial coverage has become one of the most pressing challenges for successful real-world deployments of offline RL.

Unfortunately, sample-efficient design considerations have been largely overlooked in the majority of offline RL studies. Pessimism is universally adopted in existing offline RL methods and various forms of data-related regularizations have been applied to combat the distributional shift and exploitation error accumulation issues [3; 6], such as conservatively restricting policy deviation from the behavioral data [3; 6; 7; 8], regularizing value function on out-of-distribution (OOD) samples [9; 10; 11; 12], learning policy on a pessimistic MDP [4; 13; 14], or adopting strict in-sample learning [15; 16; 17; 18]. In many of these methods, the full coverage assumption plays an important role in their theoretical performance guarantees [3; 19], which assumes the dataset to contain all state-action pairs in the induced distribution of the policy. Obviously, most of the state-action space will become OOD regions under a small dataset. Applying strict data-related regularizations will inevitably cause severe performance degradation and poor generalization . Consequently, it is important to rethink what is essential in policy learning with small datasets. In other words, what is the fundamental or invariant information that can be used to facilitate policy learning, without being conservatively confined by the limited data?

In this paper, we provide a new insight that exploiting the fundamental symmetries in the system dynamics can substantially enhance the performance of offline RL with small datasets. Specifically, we consider the time-reversal symmetry (also called _T-symmetry_), which is one of the most fundamental properties discovered in classical and quantum mechanics [21; 22]. It suggests that the underlying laws of physics should not change under the time-reversal transformation: \(t-t\)[22; 23]. Specifically, we are interested in an extended form of T-symmetry for MDP due to its simplicity and universality in physical systems. In the small dataset setting, enforcing T-symmetry in dynamics model learning and offline RL offers three crucial benefits. First, as T-symmetry describes a fundamental property of a system, hence learning it in principle would not require a large or high-coverage dataset. Second, T-symmetry captures what is essential and invariant in the dynamical system, thus offering great generalizability. Lastly, compliance with T-symmetry also provides an important clue to detecting unreliable or non-generalizable state-action samples.

Based on these intuitions, we develop a physics-informed T-symmetry enforced Dynamics Model (_TDM_) to learn a well-behaved and generalizable dynamics model with small datasets. TDM enforces the extended T-symmetry between a pair of latent space forward and reverse dynamics sub-models, which are modeled as first-order ordinary differential equation (ODE) systems to extract fundamental dynamics patterns in data. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. These can be used to construct a highly data-efficient offline RL algorithm, which we call T-Symmetry regularized offline RL (_TSRL_). Specifically, TSRL uses the T-symmetry regularized representations learned in TDM to facilitate value function learning. Furthermore, the deviation on latent actions and the consistency with T-symmetry specified in TDM provide another perspective to detect unreliable or non-generalizable samples, which can serve as a new set of policy constraints to replace the highly restrictive OOD regularizations in existing offline RL algorithms. Lastly, a reliable latent space data augmentation scheme based on compliance with the T-symmetry is also applied to further remedy the limited size of training data. With these designs, TSRL performs surprisingly well compared with the state-of-the-art offline RL algorithms on reduced-size D4RL benchmark datasets with even as few as 1% of the original samples. To the best of the authors' knowledge, this is the first offline RL method that demonstrates promising performance on extremely small datasets.

## 2 Preliminaries

Offline reinforcement learning.We consider the standard Markov decision process (MDP) setting , which is represented as a tuple \(=\{,,r,,,\}\), where \(\) and \(\) are the state and

Figure 1: Performance of recent offline RL methods and our proposed TSRL on the D4RL-MuJoCo Walker2d medium and expert datasets when reducing the number of samples from 1M (full dataset) to 10k (1%).

action spaces, \(r(s,a)\) is a scalar reward function, \(\) is the transition dynamics, \(\) is the initial state distribution, and \((0,1)\) is a discount factor. The objective of RL is to learn a policy \((a|s)\) by maximizing the expected cumulative discounted return \(_{}[_{t=0}^{}^{t}r(s_{t},a_{t}))]\), which is typically approximated by a value function \(Q(s,a)\) using some function approximators, such as deep neural networks. The Q-function is typically learned by minimizing the squared Bellman error:

\[Q=*{arg\,min}_{Q}\,[(Q(s,a )-^{}(s,a))^{2}]\] (1)

where \(\) denotes a target Q-function, which is a delayed copy of the current Q-function; \(^{}\) is the Bellman operator, which is often used as the Bellman evaluation operator \(^{}(s,a)=r(s,a)+_{a^{}}(s^{},a^{})\) in many RL algorithms.

Under the offline RL setting, we are provided with a fixed dataset \(=\{(s_{0},a_{0},r_{0},s_{1},)^{(i)}\}_{i=1}^{N}\) without any chance of further environment interactions. Directly applying standard online RL methods in the offline setting suffers from severe value overestimation, due to counterfactual queries on OOD data and the resulting extrapolation errors [1; 3; 6]. To avoid this issue, a widely used offline RL framework adopts the following behavior regularization scheme which regularizes the divergence between the learned policy \(\) and the behavior policy \(_{}\) of the dataset \(\):

\[=*{arg\,max}_{}\,_{s, ()}[Q(s,a)-(( s)] _{}( s))]\] (2)

where \((\|)\) is some divergence measures, which can have either an explicit [3; 8] or implicit form [6; 17; 25]. Although straightforward, existing behavior regularization methods have been shown to be over-conservative [9; 20] due to the restrictive regularization with respect to the behavior policy in data, which may suffer from notable performance drop under small datasets.

Time-reversal symmetry in dynamical systems.Most real-world dynamical systems with state measurement \(\) on some phase space \(\) can be modeled or approximated by the system of non-linear first-order ordinary differential equations (ODEs) as \(}{dt}=F()\), where \(F\) is some general non-linear, at least \(^{1}\)-differentiable vector-valued function. First-order ODE systems are said to be time-reversal symmetric if there is an invertible transformation \(:\), that reverses the direction of time [22; 26]: \(d()/dt=-F(())\). If we define a time evolution operator \(U_{ t}:\) as \(U_{ t}:(t) U_{ t}((t))=(t+  t)\). Then T-symmetry implies that \( U_{}=U_{-}\). In other words, the reversing of the forward time evolution of an arbitrary state should be equal to the backward time evolution of the reversed state.

Extending T-symmetry for more generic MDP settings.In our discrete-time MDP setting, we have \(=(s,a)\). We can slightly abuse the notations and denote \(=\) as the time-derivative of the current state \(s\), which can be approximated as the difference between the next and current states, i.e., \(=s^{}-s\). For a dynamical system that satisfies T-symmetry, it suggests that if we learn a forward dynamics \(F(s,a)=\) and a reverse dynamics \((s^{},a^{})=-\) as a pair of first-order ODEs, we should have \(F(s,a)=-(s^{},a^{})\).

However, from a decision-making perspective, it is known that T-symmetry can sometimes be broken by irreversible actions or some special dynamic processes (e.g., frictional force against motion). Hence in this paper, we consider a more generic treatment by leveraging an alternative ODE reverse dynamics model \(G(s^{},a)=-\) to establish the T-symmetry with the forward dynamics, i.e., enforcing \(F(s,a)=-G(s^{},a)\). Note that \(G(s^{},a)\) is now defined on the next state \(s^{}\) and the current action \(a\), rather than the next action \(a^{}\), thus is not impacted if the next action is irreversible. This _extended T-symmetry_ provides a more fundamental and almost universally held property in discrete-time MDP systems. Its simplicity and fundamentalness make it an ideal property that we can leverage to construct a well-behaved data-driven dynamics model and a robust offline RL algorithm under small datasets.

## 3 T-Symmetry Enforced Dynamics Model

In this section, we present the detailed design of TDM, which is capable of learning a more fundamental and T-symmetry preserving dynamics from small datasets. The key ingredients of TDM are to embed a pair of latent forward and reverse dynamics as ODE systems, and further enforce their T-symmetry consistency. This design offers several benefits. First, embedding ODE modeling of the system helps to extract more fundamental descriptions of how the system dynamics evolve over time in the latent space. Similar ideas have also been explored in Koopman theory [27; 28] and discovering governing equations of nonlinear dynamical systems [29; 30]. The requirement on extended T-symmetry for both forward and reverse dynamics introduces additional consistency regularization, which is helpful to improve model learning stability under limited data. With a regularized and well-behaved dynamics characterization, we can learn more effective state-action representations, which allows removing non-essential elements in the raw data to promote generalization performance. Lastly, compliance with T-symmetry can provide extra information regarding the reliability of OOD samples, which can be particularly useful in downstream offline policy learning. As illustrated in Figure 2, the proposed TDM consists the following components:

**Encoder and decoders.** TDM implements a state-action encoder \((s,a)=(z_{s},z_{a})\) and a pair of decoders \(_{s}(,_{s}),_{a}(z_{a})=a\) that embed the state-action pair \((s,a)\) into latent representations \((z_{s},z_{a})\) and then map them back. Specifically, we require the state decoder \(_{s}(,_{s})\) to be capable of decoding both \(z_{s}\) and \(_{s}\), where \(_{s}\) is an indicator to help the decoder to decide the target output, with \(_{s}=0\) as decoding \(z_{s} s\) and \(_{s}=1\) as decoding \(_{s}\). The encoder \(\) and decoders \(_{s}\) and \(_{a}\) induce the following reconstruction loss term for each state-action pair \((s,a)\):

\[_{rec}(s,a)=\|s-_{s}(z_{s},0)\|_{2}^{2}+\|a-_{a}(z_{a})\|_{2}^{2}\] (3)

Latent forward dynamics.Inspired by the prior works that incorporate physics-informed information into dynamical systems modeling [27; 29; 30], we embed a discrete-time first-order ODE system to capture the latent forward dynamics \(f(z_{s},z_{a})=_{s}\). Similar to \(\), we write \(_{s}=z_{s^{}}-z_{s}\) to denote the forward difference of the next and current latent state representations. Note that based on the chain-rule, we have \(_{s}=}{dt}=}{ s} {dt}=_{s}z_{s}\). To enforce the ODE property, we can introduce the following loss term for \(f\):

\[_{fwd}(s,a,s^{})=\|(_{s}z_{s})-_{s}\|_{2}^{2}= \|-f((s,a))\|_{2}^{2}\] (4)

Minimizing \(_{fwd}\) ensures that the latent forward dynamics \(f\) correctly predicts the forward time evolution of latent states in the dynamical system. We also require the decoder \(_{s}(,_{s})\) to be able to decode \(\) from \(_{s}\) to ensure it is compatible with the ODE property, which implies the following loss:

\[_{ds}(s,a,s^{})=\|-_{s}(_{s},1)\|_{2}^{2}=\|-_{s}(f((s,a)),1)\|_{2}^{2}\] (5)

Latent reverse dynamics.We can further introduce a latent reverse dynamics \(g(z_{s^{}},z_{a})=-_{s}\) in the model, which captures the reverse time evolution of the system in the latent space. Similar to the forward dynamics loss \(_{fwd}\), we can write the reverse dynamics loss for \(g\) as:

\[_{rvs}(s,a,s^{})=\|(_{s^{}}z_{s^{}})(-)-(- _{s})\|_{2}^{2}=\|,a)}{ s^{} }(-)-g((s^{},a))\|_{2}^{2}\] (6)

T-symmetry regularization.The above latent forward and reverse dynamics \(f\) and \(g\) are learned to be two models, which may not necessarily satisfy the proposed extended T-symmetry. We can enforce the extended T-symmetry by requiring \(f(z_{s},z_{a})=-g(z_{s^{}},z_{a})\). To further couple the learning process of \(f\) and \(g\), note that \(z_{s^{}}=z_{s}+_{s}=z_{s}+f(z_{s},z_{a})\), which suggests \(g(z_{s^{}},z_{a})=g(z_{s}+f(z_{s},z_{a}),z_{a})=-_{s}=-f(z_{s},z_{ a})\). This implies the following T-symmetry consistency loss:

\[_{Tsym}(z_{s},z_{a})=\|f(z_{s},z_{a})+g(z_{s}+f(z_{s},z_{a}),z_{a}) \|_{2}^{2}\] (7)

Figure 2: Overall architecture of the proposed TDM

Above instance-wise T-symmetry consistency loss also provides an alternative measure for evaluating the reliability of a data sample. A state-action pair \((s,a)\) with a large \(_{Tsym}((s,a))\) implies that this sample may not be well-explained by TDM or consistent with the fundamental symmetry of the system. This can be used to detect unreliable OOD samples in offline policy optimization as well as construct a new latent space data augmentation procedure, which will be discussed in later content.

Final learning objective of TDM.Finally, we can formulate the overall loss function of TDM as:

\[_{TDM}=_{(s,a,s^{})}[_{rec}+_{ds}+ _{fwd}+_{rvs}+_{Tsym}](s,a,s^{})+_{L1}[ _{1}(f)+_{1}(g)]\] (8)

where \(_{1}(f)\) and \(_{1}(g)\) are L1-norms of the parameters of \(f\) and \(g\), and \(_{L1}\) is a scale parameter. L1 regularization is introduced to encourage learning parsimonious latent dynamics for \(f\) and \(g\), which helps to improve model generalizability [29; 30].

Note that the proposed TDM is very different from the conventional dynamics models used in model-based RL (MBRL) methods [4; 13; 14; 31; 32; 33; 34; 35]. The dynamics models in MBRL focus on constructing a predictive model to represent the forward transition dynamics of the system. Whereas, TDM is formulated as a reconstruction model with T-symmetry preserving embedded ODE latent dynamics, which aims at explaining and extracting the fundamental dynamics of the system. As a result, TDM can be substantially more well-behaved and robust when learning from small datasets.

## 4 T-Symmetry Regularized Offline RL

In this section, we discuss how to incorporate the properties of TDM to construct a sample-efficient offline RL algorithm, which we call T-Symmetry regularized offline RL (TSRL). Specifically, we leverage three types of information from TDM to jointly improve offline RL performance under small datasets. First, we use the well-behaved state-action representations provided by TDM to facilitate value function learning. Second, the consistency with the latent dynamics in TDM also provides new forms of policy constraints to penalize unreliable and non-generalizable OOD samples. Finally, compliance with T-symmetry enables a new latent space data augmentation procedure to further enhance the algorithm's performance with limited data.

T-symmetry regularized representation.Representation learning has been shown to be an effective approach to enhancing sample efficiency and generalization in many online and offline RL studies [36; 37; 38; 39; 40]. A notable property of TDM is that the learned latent state-action representations from the encoder \((z_{s},z_{a})=(s,a)\) are compatible with both the latent forward and reverse ODE dynamics \(f\) and \(g\). This leads to well-regularized and T-symmetry preserving representations that can potentially generalize better on OOD areas under small dataset settings. We can simply use the latent state-action representation \((z_{s},z_{a})\) extracted by the encoder \((s,a)\) of TDM in the value function learning, which gives the following policy evaluation objective:

\[Q=*{argmin}_{Q}_{(s,a,s^{})} r(s,a)+((s^{},(|s^{})))-Q((s,a ))^{2}\] (9)

T-symmetry regularized policy constraints.Existing offline RL methods primarily penalize the divergence between the learned policy \(\) and the behavioral data in the original action space, which ignores the underlying manifold structure of actions in the latent space  and the system dynamics properties. Moreover, restricting policy optimization only within data-covered regions can be over-conservative when the offline dataset is small, which can result in degraded policy performance . In TSRL, we instead consider an alternative regularization scheme, which restricts the deviation on latent actions and the T-symmetry consistency of policy-induced samples, corresponding to the following policy optimization objective:

\[*{argmax}_{}_{(s,a)} Q( (s,(|s)))-_{1}\|z_{a^{}}-z_{a}\|_{2}^{2}-_{2}_ {Tsym}((s,(|s)))\] (10)

where latent actions \(z_{a}\) and \(z_{a^{}}\) are obtained from \((s,a)\) and \((s,(|s))\) respectively. The second term restricts the latent action \(z_{a^{}}\) of policy \(\) from deviating too much from the latent action \(z_{a}\) in data. The third term regularizes the T-symmetry consistency of policy-induced samples \((s,(|s))\), which is evaluated based on Eq. (7) and the learned TDM. \(_{1}\) and \(_{2}\) are weight parameters, which only need to be roughly adjusted to ensure both the regularization terms are in a similar scale as the first term. We also introduce a normalization term \(\) on the value function for training stability similar to TD3+BC , which is computed based on a training batch \(B\) of samples as \(_{0}/[_{(s,a) B}Q((s,(|s)))]\). We set \(_{0}=2.5\) in all of our experiments without tuning.

Instead of strictly regularizing OOD actions as in existing offline RL algorithms, our policy regularization scheme actually allows some reliable OOD action for policy optimization. As TDM is designed to capture the fundamental and invariant system dynamics patterns in the latent space, if an OOD action has a similar latent representation to some latent actions in data and also agrees with the T-symmetry property in TDM, then we can expect some degree of equivalency between these actions. This leads to more relaxed policy constraints by enabling policy learning and generalization on reliable OOD regions, which is critical for the small dataset setting.

T-symmetry consistent latent space data augmentation.It has been shown in previous studies  that data augmentation can potentially improve the function approximation of the Q-networks by smoothing out the learned state-action space, hence often lead to more robust policy and better data efficiency. However, existing data augmentation methods in offline RL studies either blindly add random perturbations to states  or utilize costly non-linear symmetry transformations, such as Koopman theory . With TDM, we can provide a very simple yet principled data augmentation scheme based on the T-symmetry property. Assuming we add a small perturbation \(\) to a latent state \(z_{s}\), i.e., \((z_{s},z_{a})(z_{s}+,z_{a})\), then the corresponding perturbation \(^{}\) on the next latent state \(z_{s^{}}\) according to the latent forward dynamics \(_{s}=f(z_{s},z_{a})\) satisfies: \(z_{s^{}}+^{}=z_{s}++f(z_{s}+,z_{a})\). On the other hand, by the T-symmetry construction in TDM, we can recover back the current perturbed latent state based on the latent reverse dynamics \(-_{s}=g(z_{s^{}},z_{a})\) as: \(z_{s}+^{}=z_{s^{}}+^{}+g(z_{s^{ }}+^{},z_{a})\). Clearly, we should have \(=^{}\), which suggests the following condition:

\[^{}-=f(z_{s}+,z_{a})+g(z_{s}++f(z _{s}+,z_{a}),z_{a})=0\] (11)

This is exactly equivalent to requiring the instance-wise T-symmetry consistency loss (Eq. (7)) \(_{T sym}(z_{s}+,z_{a})=0\). Hence we can use T-symmetry consistency loss \(_{T sym}()\) as a reliability measure to filter out unreliable augmented samples \((z_{s}+,z_{a})\) that are inconsistent with the T-symmetry property of the learned latent dynamics in TDM. In our implementation, we only keep augmented samples that satisfy \(_{T sym}(z_{s}+,z_{a}) h\), where we consider a non-parametric treatment for threshold \(h\), by setting it as the \(\)-quantile value of all \(_{T sym}((s,a))\) values of \((s,a)\) in \(\) (we choose \(=50\%\) or \(70\%\) in our experiments). This ensures that the augmented samples at least maintain the similar level of T-symmetry agreement explained by TDM as the data samples in \(\).

Practical implementation.TSRL can be implemented based upon TD3  by incorporating the proposed T-symmetry regularized representation and policy constraints as in Eq. (9) and (10), as well as the T-symmetry consistent latent space data augmentation. In our experiments reported in the next section, we generate \(K=1\) augmented samples for each transition in the dataset, and filter based on the T-symmetry consistency loss. The pseudo-code of TSRL is summarized in Algorithm 1.

```
0: Offline dataset \(\), encoder \(\), latent forward and reverse dynamics models \(f\) and \(g\) from TDM trained using objective Eq. (8).
1: Compute the T-symmetry consistency loss \(_{T sym}((s,a))\) (Eq. (7)) for all samples in \(\), and set their \(\)-quantile value as the augmentation threshold \(h\).
2: Initialize the policy network \(\), critic networks \(Q\) and their target network.
3:for\(t=1,,M\) training steps do
4: Sample a mini-batch \(B\) of samples \(\{(s,a,r,s^{})\}\) and compute their representations \(\{(z_{s},z_{a},z_{s^{}})\}\).
5://T-symmetry consistent latent space data augmentation
6: Generate \(K\) perturbed samples by adding perturbations \( N(0,0.01_{z_{s}})\) on latent states \(z_{s}\) of each sample in \(B\), where \(_{z_{s}}\) is the std of latent states in data.
7: Add augmented samples \((z_{s}+,z_{a},z_{s^{}}+^{})\) to \(B\) if satisfies \(_{T sym}(z_{s}+,z_{a}) h\).
8://Critic training with T-symmetry regularized representation
9: Update the value function \(Q\) based on the policy evaluation objective Eq. (9).
10://Policy training with T-symmetry regularized policy constraints
11: Update the policy \(\) based on the policy improvement objective Eq. (10).
12: Soft update the target networks.
13:endfor ```

**Algorithm 1** T-Symmetry Regularized Offline RL (TSRL)

## 5 Experiments

We evaluate TSRL on the D4RL MuJoCo-v2 and Adroit-v1 benchmark datasets  against behavior cloning (BC) as well as state-of-the-art (SOTA) offline RL methods, including model-free methods TD3+BC , CQL , IQL , DOGE , and model-based method MOPO . In particular, we compare with DOGE , which leverages a state-conditioned distance function as policy constraint and exhibits strong OOD generalization performance. We report the final normalized performance of each algorithm after training 1M steps.

Performance on small datasets.We compare the performance of TSRL and the baseline methods on both the full D4RL datasets and their reduced-size datasets with only 5k\(\)10k samples, which are constructed by randomly sampling a given fraction of trajectories in the full datasets*. These reduced-size datasets are only about 1/20\(\)1/200 of their original size. Compared with the performances on the full datasets, most baseline offline RL methods suffer from a noticeable performance drop under these extremely small datasets, mainly due to their over-reliance on the size and coverage of training data. Among all baselines, DOGE  is the only baseline that can still achieve reasonable performance in some tasks, primarily due to its capability to leverage the relationship between dataset geometry and the generalization capability of deep Q-functions, which is also beneficial for policy learning under small datasets. However, we still observe that DOGE struggles in some datasets, especially those with an extremely narrow distribution.

Footnote *: We didn’t construct reduced-size datasets for Adroit-human tasks, as the full datasets are already very small.

By contrast, TSRL achieves substantially better performance in all small dataset tasks, indicating superior sample efficiency. Moreover, although MOPO  also learns a dynamics model for offline policy learning, it performs badly when the dataset is small, revealing the importance of using a well-regularized model like TDM in the small-sample regime. It is interesting to see that most

  
**Task** & **Ratio** & **Size** & **BC** & **TD3+BC** & **MOPO** & **CQL** & **DQGE** & **TSRL(ours)** \\   & 1 & 1M & 52.9 & 59.3 & 28.0 & 58.5 & 66.3 & **98.6 \(\) 2.1** & 86.7\(\)8.7 \\  & 1/100 & 10k & 29.7\(\)11.7 & 40.1\(\)18.6 & 5.5\(\)2.3 & 43.1\(\)24.6 & 46.7\(\)6.5 & 44.2 \(\) 10.2 & **62.0\(\)3.7** \\   & 1 & 400k & 18.1 & 60.9 & 67.5 & **95.0** & 94.7 & 76.2\(\)17.7 & 78.7\(\)28.1 \\  & 1/40 & 10k & 12.1\(\)5.3 & 7.3\(\)6.1 & 6.8\(\)0.3 & 2.3\(\)1.9 & 13.4\(\)3.1 & 17.9 \(\) 4.5 & **21.8\(\)8.2** \\   & 1 & 2M & 52.5 & 98.0 & 23.7 & **105.4** & 91.5 & 102.7\(\)5.2 & 95.9\(\)18.4 \\  & 1/200 & 10k & 27.8\(\)10.7 & 17.8\(\)7.9 & 5.8\(\)5.8 & 29.9\(\)4.5 & 34.3\(\)8.7 & 50.5 \(\) 25.2 & **50.9\(\)8.6** \\   & 1 & 1M & 108.0 & 100.1 & 16.2\(\)6.2 & 98.4 & 99.3 & 107.4 \(\) 3.6 & **110.0 \(\)3.3** \\  & 1/100 & 10k & 20.8\(\)6.9 & 23.2\(\)18.2 & 6.5\(\)3.7 & 33.0\(\)22.2 & 38.4\(\)11.3 & 54.5\(\)21.5 & **82.7\(\)21.9** \\   & 1 & 1M & 42.6 & **48.3** & 42.3 & 44.0 & 47.4 & 45.3\(\) 0.6 & **48.2 \(\)0.7** \\  & 1/100 & 10k & 26.4\(\)7.3 & 16.4\(\)10.2 & -1.1\(\)4.1 & 35.8\(\)3.8 & 29.9\(\)10.2 & 36.2\(\)3.4 & **38.4\(\)3.1** \\   & 1 & 200k & 55.2 & 44.6 & **53.1** & 45.5 & 44.2 & 42.8\(\)0.6 & 42.2\(\)3.5 \\  & 1/20 & 10k & 14.3\(\)7.8 & 17.9\(\)9.5 & 17.1\(\)5.2 & 8.1\(\)9.4 & 22.7\(\)6.4 & 23.4\(\)3.6 & **28.1\(\)3.5** \\   & 1 & 2M & 55.2 & 90.7 & 63.3 & 91.6 & 86.7 & 78.7\(\)8.4 & **92.0\(\)1.6** \\  & 1/200 & 10k & 19.1\(\)9.4 & 15.4\(\)10.7 & -1.1\(\)1.4 & 26.5\(\)10.8 & 10.5\(\)8.8 & 26.7\(\)6.6 & **39.9\(\)21.1** \\   & 1 & 1M & 92.2 & 82.1 & 1.4\(\)2.2 & **95.6** & 88.9\(\)1.2 & 93.5 \(\) 4.2 & **94.3\(\)5.5** \\  & 1/100 & 10k & 1.10\(\)2.4 & 1.72\(\)3.3 & -0.6\(\)1.1 & 4.2\(\)0.94 & -2.0\(\)0.4 & 1.4\(\)2.1 & **40.6\(\)24.4** \\   & 1 & 1M & 75.3 & 83.7 & 17.8 & 72.5 & 78.3 & **86.8 \(\) 0.8** & 77.5 \(\)4.5 \\  & 1/100 & 10k & 15.8\(\)14.1 & 7.4\(\)13.1 & 3.1\(\)4.7 & 18.8\(\)18.8 & 22.5\(\)3.8 & 45.1 \(\) 10.2 & **49.7\(\)10.6** \\   & 1 & 300k & 26.0 & 81.8 & 39.0 & 77.2 & 73.9 & **87.3 \(\) 2.3** & 66.1\(\)12.0 \\  & 1/30 & 10k & 1.4\(\)1.9 & 5.7\(\)5.8 & 3.3\(\)2.7 & 8.5\(\)2.9 & 10.7\(\)11.9 & 13.5\(\) 8.4 & **26.0\(\)11.3** \\   & 1 & 2M & 107.5 & 110.1 & 44.6 & 108.8 & 109.6 & **110.4\(\)1.5** & 109.8\(\)3.12 \\  & 1/200 & 10k & 21.7\(\)8.2 & 7.9\(\)9.1 & 0.6\(\)2.7 & 19.1\(\)14.4 & 26.5\(\)8.6 & 35.3 \(\) 11.6 & **46.4\(\)17.4** \\   & 1 & 1M & 107.9 & 108.2 & 0.1\(\)0.3 & 101.3 & **109.7\(\)0.1** & 107.3\(\)2.3 & **110.2\(\)0.3** \\  & 1/100 & 10k & 10.4\(\)5.3 & 23.8\(\)16.0 & 1.4\(\)3.4 & 41.6\(\)21.6 & 12.6\(\)4.5 & 72.1 \(\)16.2 & **102.2\(\)11.3** \\    & 1 & 5k & 36.4 & 10.6 & 9.5 & 52.2 & 77.3 & 39.0 \(\) 17.1 & **80.9\(\)21.1** \\  )\)).

Figure 6: Ablation on TSRL on 10k datasets. “no-R”: no T-symmetry regularized representation; “no-P”: no T-symmetry policy constraints, and use BC term as in TD3+BC; “no-A”: no latent space data augmentation.

Figure 7: Comparison of TSRL and baselines trained on the Walk2d-medium (top) and medium-expert (bottom) datasets that removing all samples with x-velocity of the top\(>0.2\) max-x-velocity in the data. Left: learning curves. Right: x-velocity distribution of policy evaluation rollouts during the last 10k training steps.

space. CQL performs especially poorly in both tasks, perhaps due to over-conservative value function learning that impedes the policy to acquire some necessary control strategy to finish the task. TD3+BC performs poorly in the low-speed medium dataset, likely because this dataset has a narrower data distribution than the medium-expert dataset, and the latter could still contain some samples with reasonable speeds after filtering with x-velocity\(>0.2\)max-x-velocity in the dataset. IQL exhibits some level of generalization capability, but is still much weaker as compared to TSRL. By contrast, we observe that TSRL is still able to achieve good performance, due to the access to more fundamental dynamics information that remains invariant in both low- and high-speed data. This can be further verified if we inspect the policy rollout distributions (right figures of Figure 7) that the policies learned by TSRL indeed generalizes to high-speed behavior that is not present in the training data.

## 6 Related Work

Learning fundamental dynamics in physical systems.Learning conservation laws or invariant properties within a physical system is an active research area in physics [45; 46; 29; 30], climate science , and neuroscience , etc. A classic approach is based on Koopman theory, which represents the nonlinear dynamics in terms of an infinite-dimensional linear operator . In practice, this is achieved by finding a coordinate transformation to produce a finite-dimensional representation in which the non-linear dynamics are approximately linear. However, it also suffers from computationally expensive coordinate transformations and is only able to approximate the system dynamics. Another approach is utilizing a sparse regression model with the fewest terms to describe the nonlinear system dynamics [29; 30]. However, it assumes that the dynamical systems only have a few critical terms, which severely limits the model expressiveness and often requires prior knowledge of these critical terms. Based on expressive deep neural networks, a recently emerged research direction is to build ODE networks to learn conservation law in the dynamical system from data [49; 50; 51; 26]. Our proposed TDM falls within this direction, which models both forward and reverse latent ODE dynamics with deep neural networks and incorporates additional regularization on T-symmetry.

Offline reinforcement learning.Offline RL addresses the challenge of deriving policies from fixed, pre-collected datasets without interaction with the environment. Under this offline learning paradigm, conventional off-policy RL approaches are prone to substantial value overestimation when there is a large deviation between the policy and data distributions. Existing offline RL methods address this issue by following several directions, such as constraining the learned policy to be "close" to the behavior policy [6; 3; 8; 52], regularizing value function on OOD samples [9; 10; 11; 53], enforcing strict in-sample learning [16; 15; 17; 18; 54], and performing pessimistic policy learning with uncertainty-based reward or value penalties [13; 14; 4; 12; 55; 33]. Most existing offline RL methods adopt the pessimism principle and avoid policy evaluation on OOD samples. Although this treatment helps to alleviate exploitation error accumulation, it can be over-conservative and causes severe performance degradation if the training dataset is small or has poor state-action space coverage . TSRL tackles this issue by allowing dynamics explainable OOD samples for policy optimization, thus offering greatly improved small-sample performance.

## 7 Discussion and Conclusion

In this paper, we propose a physics-informed dynamics model TDM and a new offline RL algorithm TSRL, which exploit the fundamental symmetries in the system dynamics for sample-efficient offline policy learning. TDM embeds and enforces T-symmetry between a pair of latent forward and reverse ODE dynamics to learn fundamental dynamics patterns in data. The well-behaved representations and a new reliability measure for OOD samples based on T-symmetry from TDM can be readily used to construct the proposed TSRL algorithm, which achieves strong performance on small D4RL benchmark datasets and exhibits good generalization ability. There are also some limitations in our proposed approach. For example, in order to learn a well-behaved dynamics model, we introduced a set of dynamics and symmetry regularizations in TDM, which are beneficial to improve model generalization, but will lose some model expressiveness. However, we believe this can be a worthwhile trade-off between precision and generalization under small dataset settings, due to substantially improved model robustness.