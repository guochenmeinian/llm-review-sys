# Leveraging Environment Interaction for Automated PDDL Translation and Planning with Large Language Models

Leveraging Environment Interaction for Automated PDDL Translation and Planning with Large Language Models

Sadegh Mahdavi

University of British Columbia

smahdavi@ece.ubc.ca

&Raquel Aoki

Borealis AI

raquel.aoki@borealisai.com

&Keyi Tang

Borealis AI

keyi.tang@borealisai.com

&Yanshuai Cao

Borealis AI

yanshuai.cao@borealisai.com

Work performed while interning at Borealis AI.

###### Abstract

Large Language Models (LLMs) have shown remarkable performance in various natural language tasks, but they often struggle with planning problems that require structured reasoning. To address this limitation, the conversion of planning problems into the Planning Domain Definition Language (PDDL) has been proposed as a potential solution, enabling the use of automated planners. However, generating accurate PDDL files typically demands human inputs or correction, which can be time-consuming and costly. In this paper, we propose a novel approach that leverages LLMs and environment feedback to automatically generate PDDL domain and problem description files without the need for human intervention. Our method introduces an iterative refinement process that generates multiple problem PDDL candidates and progressively refines the domain PDDL based on feedback obtained from interacting with the environment. To guide the refinement process, we develop an Exploration Walk (EW) metric, which provides rich feedback signals for LLMs to update the PDDL file. We evaluate our approach on \(10\) PDDL environments. We achieve an average task solve rate of 66% compared to a 29% solve rate by GPT-4's intrinsic planning with chain-of-thought prompting. Our work enables the automated modeling of planning environments using LLMs and environment feedback, eliminating the need for human intervention in the PDDL translation process and paving the way for more reliable LLM agents in challenging problems. Our code is available at https://github.com/BorealisAI/llm-pddl-planning

## 1 Introduction

Large language models (LLMs) have demonstrated remarkable success across various domains, including mathematics, coding, and even the bar exam . These models excel at understanding and generating natural language, offering flexibility and adaptability to a wide range of tasks. However, when it comes to planning and long-horizon reasoning, LLMs have shown limited performance [8; 28], despite some promising results .

Planning is a crucial aspect of intelligence that involves reasoning to find a sequence of actions to achieve a desired goal state from an initial state. The Planning Domain Definition Language (PDDL)  is a widely used formalism for describing planning problems. PDDL provides a structured way to define the domain, which includes the types of objects, predicates, and actions, as well as the problem instance, which specifies the initial state and goal conditions. PDDL enables the application of search-based algorithms, such as breadth-first search (BFS) or A\({}^{*}\) search, which can guarantee to find a valid solution if one exists. However, the downside of PDDL is that it requires a well-defined and structured domain and problem definition, which can be challenging to create, especially for complex scenarios. Figure 1 showcases snippets of some PDDL problems and domain files along with an action plan produced by a classical planner.

Recent studies explored combining the strengths of LLMs and PDDL-based planning [15; 7; 9]. The idea is to leverage LLM for translation from natural language (NL) problem descriptions into PDDL formal descriptions, and then use a classical planner to solve the translated PDDL problem . This hybrid approach could theoretically take advantage of the flexibility of NL input and the correctness guarantees provided by the classical planner. If the translation from NL to PDDL is accurate, the resulting plan is guaranteed to be valid.

Unfortunately, existing approaches have not been able to generate both PDDL problem and domain descriptions with reasonable success rates without humans in the loop, as we shall elaborate in Sec. 2. While translating PDDL problems is feasible given the domain PDDL description , generating domain PDDL from NL correctly is a more nuanced and challenging problem. To do so requires identifying causally relevant objects to design predicates, as well as their inter-relationships, in a way that accurately reflects the possible states and transitions of the environment. A small error, for example in predicate design, could lead to entirely incorrect domain description and failed planning (see Appendix A.2 for a real example). Guan et al.  take a step toward this goal relying on human-in-the-loop to detect and correct mistakes made by LLMs.

In this work, we develop a fully automated method for generating PDDL domain and problem definitions using LLMs and environment feedback without relying on human intervention. Intuitively, our method lets an LLM build hypothetical "mental models" of the environment, in the form of proposed PDDL domain descriptions. The LLM then verifies and updates the "mental model" by observing discrepancies between the feasibility of actions under its "mental model" and the real environment. This method enables LLMs to use classical planners to solve complex planning problems whose solutions may require hundreds or thousands of steps that all need to be correct.

We first highlight the challenges of this task and then propose our solution. In particular, our contributions are as follows:

* We demonstrate that even small modifications to PDDL domains can render plan search infeasible, limiting the feedback information for LLMs to perform in context update.
* To address this, we introduce a new Exploration Walk (EW) metric, which is a smooth similarity measure between two domains by comparing the executability of random action sequences sampled from one domain on the other. Crucially, EW only requires access to the action interface and executability of the environments, not directly the ground-truth PDDL.
* We propose an EW-guided tree search approach that leverages LLMs to generate and refine the PDDL domain and problem files iteratively and automatically.
* We evaluate our method on \(10\) challenging PDDL domains, where a number of them are from the International Planning Competition, and show that it outperforms a baseline that generates PDDL files in a single attempt without refinement. Our method solves 7 out of 10 environments, achieving an average task solve rate of 66% and average EW score of 0.84, compared to 34% task solve rate and 0.53 EW score for the baseline, and 29% solve rate by GPT-4 (gpt-4-1106-preview)'s intrinsic planning with chain-of-thought prompting.

To the best of our knowledge, this is the first work that enables modeling a planning environment via PDDL translation using LLMs and environment interaction, without the need for human intervention.

## 2 Related Work

**LLMs and Classical Planning.** There has been recent interest in integrating LLMs with PDDL [15; 28; 9; 7; 30; 23; 10; 20; 26], and more generally neural networks with PDDL [24; 2]. Silver et al.  leverage LLMs to take domain PDDLs and problem PDDL specifications, and synthesize a Python function to generate domain-specific plans, as a replacement for search-based planning. Liuet al.  show that using LLMs to translate problem specification to PDDL, and using classical solvers results into a higher planning accuracy that using LLM directly as a planner. Dagan et al.  consider a similar setting, but assume that the list of objects is partially observable, and the LLM needs to interact with the world to observe the list of objects. All of the mentioned works, however, assume that a domain PDDL files is already provided. Oswald et al.  generate domain PDDL from natural language and propose heuristics for comparing PDDL action domains. However, their approach assumes that predicates are provided, whereas our work makes no such assumption. Additionally, Oswald et al.  rely on ground-truth problem instances for domain compatibility evaluation, whereas we directly translate problem PDDL without any such assumptions. Guan et al.  translate both Domain and Problem from natural language description but rely on human experts to correct mistakes in the domain translation before generating problem PDDLs. In this work, our goal is to lift the human-intervention assumption, and instead, use domain interaction for evaluation and verification. See Table 1 for a summary of related work comparison.

**Direct Reasoning with LLMs.** Recent research has explored eliciting direct reasoning capabilities within Large Language Models (LLMs). This reasoning can be either entirely direct [31; 29] or partially direct with the assistance of basic external tools . However, the primary limitation of these approaches lies in the inherent tendency of auto-regressive LLMs to produce errors in long-horizon reasoning tasks . Even a minor mistake in a single reasoning step can lead to cascading errors, ultimately resulting in an incorrect final answer . When applied to classical planning, this approach delegates the entire plan generation process to an LLM instead of leveraging a dedicated classical planner. Studies have demonstrated that this strategy is suboptimal compared to generating PDDL code directly [9; 15], highlighting the importance of incorporating classical planning tools for faithful plan generation in classical planning tasks.

**External Reasoning and Code Generation.** This last line of work focuses on generating executable code from natural language instructions such as SQL or Python code generation [4; 19; 17; 5; 16; 32]. Here, the LLM often acts as a code translator, and the reasoning logic lies within the generated code. Chen et al.  show that LLMs are capable of Python code generation from docstrings to high accuracy. The authors also find that taking multiple code samples from an LLM and picking the best samples results in an accuracy boost. Later works show that iterative refinement of LLM responses improves the accuracy on the downstream task [17; 5], especially given external feedback such as unit tests or human feedback. Our work is related to code generation as we produce structured PDDL files. However, our setting presents three challenges: (1) there are two types of PDDL files, in contrast to a single Python script, and the two files need to be consistent with each other; (2) more importantly, getting external feedback and the evaluation of a generated PDDL code is not as easy as python unit tests, and as we show in Section 4.3, (domain generation) errors are abundant and hard to trace; (3) LLMs are trained with a lot more Python code compared to PDDL, as the later is much scarcer.

## 3 Notation and Background

**Notation.** We denote \([]\) as the indicator function. The notation \(1:N\) refers to the sequence of integers ranging from \(1\) to \(N\). For a set \(\), we define \(^{*}\) as the set comprising all possible sequences of elements drawn from \(\), and define \(2^{}\) as the power set of \(\).

**PDDL.** Planning Domain Definition Language (PDDL) is a formal language used to describe and specify planning problems for automated planning. Here, we have two types of PDDL files: (1) _Domain PDDL_, which defines possible _predicates_ (_i.e._, states), and _actions_ in the environment. Executing each action requires some _precondition_ (_i.e._, a set of predicates to have a specific value), and the execution leads to some _effect_ (_i.e._, a change in the values of some predicates). (2) _Problem PDDL_, which contains a set of initial predicates and a set of goal predicates.

  
**Method(s)** & **Translate Problem** & **Translate Domain** & **No Human Intervention** \\  LLM+P , LLM-DP  & ✓\({}^{*}\) & \(\) & ✓ \\  LLM World Models  & ✓ & ✓ & \(\) \\  Ours & ✓ & ✓ & ✓ \\   

Table 1: Summary of comparison to most closely related prior studies.*Require at least one problem instance to be translated by a human into the target domain as an in-context example.

The problem PDDL instantiates the domain definition PDDL to form a concrete environment. Together, the planning problem is fully defined and formalized. A _classical planner_ takes in both files and searches for a plan based on the provided specification. A _plan_ is a sequence of actions, starting from the initial state, leading to a state satisfying the goal conditions, with each action respecting the rules of the environment. Formally, let \(,,\) be the set of all possible domains, problems, and actions, respectively. Then, given a domain \(d\) and problem \(p\), a classical planner \(C:^{*}\{\}\) takes in domain \(d\) and plan \(p\), and produces a plan \(q:=C(d,p)\) which is either set of actions from \(^{*}\), or a planning error \(\). A planning error may be due to an infeasible plan search (_i.e._, plan not found), syntax errors, or incompatible domain and problem. A plan validator verifies whether a plan \(q\) is executable and achieves the desired problem goal given a domain PDDL \(d\) and problem PDDL \(p\), _i.e._, whether \(q\) solves the planning problem instance. The validator function, denoted as \(V_{d,p}(q):^{*}\{0,1\}\), is \(1\) if the plan is valid, and \(0\) otherwise. For convenience, we assume \(V_{d,p}()=0\). Similarly, we define plan execution checker \(E_{d,p}:^{*}\{0,1\}\), which only checks whether an action sequence is executable in a domain or not. Note that the _difference between \(V\) and \(E\) is that the former checks for both plan executability and goal satisfaction, while the latter only checks for plan executability._ We also define \(\) as the set of all possible states. Function \(A_{d,p}: 2^{}\) delineates the set of legal actions given the current states (_i.e._, actions that would not immediately result in \(E_{d,p}\) returning 0). The function \(S_{d,p}:\) denotes the state transition function ( i.e., \(S_{d,p}(a,s)\) determines the subsequent state given the current state \(s\) and action \(a\)). Finally, we denote the initial state induced by \(d\) and \(p\) to be \(s_{d,p,0}\). See Table 3 in the Appendix for a summary of notations.

To illustrate the definitions with an example, consider the Grippers  environment with several rooms containing robots and boxes. Robots can move balls between rooms using their left and right grippers. Given an initial setting of robots and balls in different rooms, the main goal is to move specific balls to specific rooms using the robots.

Figure 1: Snippets of PDDL domain, problem, and plan.

Method

Given an environment \(e\), its domain NL description and a task NL description, the environment's object list and action interface, our goal is to model the environment by generating a domain PDDL \(\) and a problem PDDL \(\), such that applying a classical planner \(C\) on the PDDL files produces a valid plan for the environment, _i.e._, \(C(,)\) is a valid plan for \(e\), _i.e._, \(V_{d,p}(C(,))=1\).

### Setup

For evaluation, we assume there exists a ground truth domain PDDL \(d\), and a corresponding problem instance \(p\). However, the ground truth is not directly compared to generated \(,\), but to validate the plan \(:=C(,)\) by executing the validator of the ground-truth environment, \(V_{d,p}()\).

Formally, for each environment \(e\) with domain PDDL \(d\), and \(N\) tasks with their corresponding ground-truth problem PDDLs \(p_{1:N}:=(p_{1},p_{2},,p_{N}),p_{1:N}^{N}\), our goal is to generate a domain PDDL \(\), and a sequence of task PDDLs \(_{1:N}:=(_{1},_{2},,_{N})\) such that the average solve rate \(\) is maximized:

\[*{argmax}_{,_{1:N}^{N} }(,_{1:N};e):=_{i=1}^{N}V_{d,p_{i}} (C(,_{i})).\] (1)

Generating accurate \(\) and \(_{1:N}\) in one attempt is often impractical , and some form of feedback is required to refine the response. Guan et al.  leverage human expert feedback on \(\) to correct the generated domain. However, human feedback may not always be reliable and is not scalable. Before introducing our method that relies on environment feedback instead, we first state our assumptions:

**Assumption 1** (Environment access): _We assume the list of objects and action interfaces are known. Furthermore, we assume that executability and verifiability of actions can be observed (through the functions \(E_{d,p}\) and \(V_{d,p}\))._

**Assumption 2** (Natural language description): _We assume the natural language descriptions of the domain and task are both given._

The action interfaces are equivalent to APIs available to LLM agents. So it is reasonable to assume that the exact API call signatures are known. On the other hand, one may wonder why the object list, which appears in problem PDDLs as illustrated in Figure 1 needs to be assumed to be given, when the NL problem description should describe the objects involved in the planning tasks. This is because the NL description may not refer to the object instances using exactly the same label as the environment induced by \(d\) and \(p\). If \(p\) refers to a robot as robot1 but the user specifying the natural language problem description calls it Jarvis, then the environment only recognizes robot1 and not Jarvis, so the LLM would have no way to correct this mistake due to trivial name mismatch. See Appendix A.1 for a detailed example of our assumptions on the Grippers environment.

Note that our assumptions do not require the underlying environment to be a PDDL environment, but it can be any environment as long as PDDL is expressive enough to capture the working mechanisms of the environment. For digital agents in virtual environments, the list of objects and action interfaces are just different data objects and APIs available. The assumptions could even hold true for physical agents in the real world, provided recognition and control are sufficiently accurate. In this work, we focus on PDDL environments only, although our framework is more general.

### Difficulty of domain PDDL generation

Generating the correct domain PDDL is challenging, as small mistakes could make the plan search fail. To demonstrate this brittleness, we simulate random omission of \(k\) terms, where \(0 k 10\), from the action precondition and effects of the original domain \(d\). For instance, in the case of the Grippers (Figure 1), we may create a new synthetic domain by removing the (at robby?r?to) term from the effects of the move action. Namely, we define \(_{k}_{k}(d)\), where \(_{k}(d)\) represents the uniform random removal of \(k\) terms. Then, for each generated \(_{k}\), coupled with the ground truth task PDDLs, we compute whether the classical planner is able to find a plan without error and compute the _Plan-Not-Found_ rate under \(k\) omissions, PNF\({}_{k}\), of the environment.

We empirically measure the value of \(_{k}\) using Monte-Carlo estimation on \(15\) environments. As shown in Figure 1(a), \(_{1}\) has an average of \(0.14\) among different environments. This means that on average \(14\%\) of the terms in domain PDDLs are so critical that removing them results in a plan-not-found error. This situation is exacerbated for larger \(k\): at \(k=3\), the average \(_{k}\) reaches around \(0.3\). In practice, the problem PDDL \(_{i}\) also needs to be generated, and the generated domain \(\) may have extra terms, both of which may further increase the planning-not-found rate.

### Domain alignment measure via Exploration Walk metrics

Whenever the plan search fails, absolutely no information is available to the LLM about which part of the problem or domain has issues. This is because the underlying search algorithm (such as BFS and A\({}^{*}\)) fails and as a result, it does not produce any output. For example, with BFS, it enumerates all paths (possibly several thousand paths or more), and finds none satisfy the goal conditions, leaving the plan search without any useful insights. As an alternative, we introduce the Exploration Walk (EW): a smooth feedback signal that provides incremental feedback for LLM in-context learning. EW both provides a mechanism to gather richer feedback information that feeds into LLM context for PDDL refinement, as well as computing a smooth scoring metric that to compare multiple PDDLs and guide the refinement process forward.

Intuitively, the idea is to take legal random action sequences and verify their executability under LLM's "mental model" environment induced by an LLM-generated PDDL domain. This is analogous to the _retrodiction_ step in scientific methodology, where existing observations and experimental data need to be explained by the existing model.

And in the other direction, EW takes executable random action sequences from an LLM-generated PDDL domain and verifies whether they are correct in the real environment. This is analogous to _hypothesis testing_ in scientific methodology, where new predictions are verified experimentally.

We now describe the EW and EW metrics formally. We define an Exploration Walk of length \(T\) to be any action sequence sampled from a strictly positive distribution \(_{d,p,T}\) over executable \(T\)-step action sequences in \(^{*}\) corresponding to domain \(d\) and task \(p\). We assume the probability of non-executable action sequences to be zero under \(_{d,p,T}\). In other words, \( q_{1:T}\), \(_{d,p,T}(q_{1:T})>0\) iff \(E_{d,p}(q_{1:T})=1\).

For the rest of this paper, we use the simplest possible EW, with a uniform distribution over valid actions at each step. Note that to sample uniform random EW from the ground truth environment induced by \(d\) and \(p\), we do not need direct access to the full \(d\) and \(p\). We only need the list of objects in \(p\) and the action interface in \(d\), and executability checker \(E_{d,p}\), consistent with our Assumption 1. At each step, running \(E_{d,p}\) on all possible actions yields the legal actions at that step for EW.

Given an EW distribution, we define an EW metric using the fractions of executability of EW walks from one domain under another, averaged over all different lengths.

Figure 2: (a) Effect of the number of removed terms on plan search failure. Each gray line shows the \(_{k}\) (Plan-Not-Found) metric for one environment. The red line is the average of all 15 environments. (b) Correlation between average exploration walk (EW) score and average domain difference. The \(x\)-axis shows how many terms each pair of domains differs in. The \(y\)-axis shows the average EW score over various pairs. All the domains show the average monotonicity of the EW score with respect to term difference.

**Definition 1** (EW Metrics): _Let \(p_{1:N}\) and \(_{1:N}\) be problems in domain \(d\) and \(\) respectively, such that the set of objects in \(p_{j}\) and \(_{j}\) are consistent. We define the one-sided measure \(m_{d-}\) and the symmetric one \(m_{d}\) for the degree of alignment between two domains \(d\) and \(\) as:_

\[m_{d}(p_{1:N},_{1:N}) :=}}_{j=1}^{N}_ {T=1}^{T_{}}_{q_{d,p_{j},T}}[E_{,_{j}}(q)]\] \[m_{d}(p_{1:N},_{1:N}) :=21/m_{d}(p_{1:N},_{1:N}) +1/m_{}(_{1:N},p_{1:N}),\] (2)

_where \(T_{}\) is the largest EW walk length._

\(m_{d}\) measures what fraction of EWs sampled from domain \(d\) are executable on the domain \(\). Then, \(m_{d}\) takes the harmonic mean of \(m_{d}\) and \(m_{}\) to produce the final EW measure. This metric has two favourable properties: (1) it ensures that \(m_{d}=m_{ d}\), thereby providing a consistent measure of similarity regardless of the order of domain comparison. (2) the harmonic mean is resistant to trivial domain similarity inflation. By employing the harmonic mean rather than the arithmetic mean, the symmetric EW metric prevents domains that are overly permissive (e.g., domains where all actions are permissible without any preconditions) from being similar to more restrictive domains. For example, in a scenario where domain \(\) allows all possible actions without restrictions, \(m_{d}=1\). An arithmetic mean in this context would yield \(m_{d} 0.5\), overestimating the similarity. In contrast, the harmonic mean results in \(m_{d}=\), where (\( 1\)) for most cases.

Note that while the PDDL problems \(p_{1:N}\) and \(_{1:N}\) appear in the definition of EW metrics, we only use the fact there are aligned object sets in them. We could also use an arbitrarily sampled object list to form an \(\) and pair \(\) with \(D\) and \(\) for EW metrics. But since for PDDL generation, we already generate \(_{1:N}\), it is more convenient to use them.

Importantly, EW metrics can be computed without direct access to the full ground truth domain \(d\) and problems \(p\)'s. As established before, to sample uniform random EW, we just need access to the object list and action interface, plus the environment executability checker of the source domain. So even for \(m_{d}\), where the EW action sequences come from \(d\), we do not need more than what is available through Assumption 1.

To demonstrate the relationship between \(m_{d}\) and domain disparity, we use the same simulated random omission study setup from Sec. 4.2. For a pair of modified domains, we count the number of terms that differ, and inspect \(m_{d}\) as function of increasing number of differing terms in Figure 2 for six example domains (see Figure 4 in the Appendix for the full set). We observe that, on average, a greater discrepancy in the number of terms between two domains correlates with a reduced EW score \(m_{d}\). This observation provides additional support to the use of the EW score as an effective measure for domain differences.

### Leveraging LLMs to generate PDDL files

We now show our overall LLM-based method for PDDL generation using the EW score to guide and measure the progress of domain generation. To illustrate the process, we first focus on a domain \(d\) with a single task \(p\). Recall that we are given NL description of the environment domain \(d_{}\) and problem \(p_{}\) (Assumption 2), as well as the object list in \(p\) and action interface from \(d\) (Assumption 1). Then, by using \(d_{}\), \(p_{}\), and access to environment action feedback, we seek to generate \(,\).

Our method starts by initializing templated \(^{(0)}\) based on action interfaces and templated \(^{(0)}\) using object list. Example template \(^{(0)}\) and \(^{(0)}\) are shown in Listings 6 and 4 of Appendix A.1. We then use an LLM to improve the initial \(^{(0)}\) and \(^{(0)}\).

Given that domain PDDL files are typically more complex than problem PDDL files, our strategy prioritizes the generation of a problem PDDL file \(\) first, followed by the domain \(\). This approach enables us to assess the quality of the generated domain immediately. Moreover, prior works on code generation , tree-of-thought , and self-debug  have found that taking multiple samples from the LLM response and taking the best response leads to better performance. However, they often require an evaluation metric on the generated response (such as unit test cases, or execution traces). Here, we use the EW metric introduced in Section 4.3 to serve as an evaluator of the generated domain. These considerations lead to our proposed Algorithm 1. We emphasize again that theground-truth domain and problem \(d,p\) are only used to take exploration walks and evaluate a plan through the environment in 1.

```
0: Natural language descriptions \(d_{}\), \(p_{}\), environment action interface.
1:\(^{(1)},^{(2)},,^{(n_{p})}_{n_{p}}(p_ {})\) {Problem PDDL candidates}
2:for\(i=1,2,,n_{p}\)do
3:\(h^{(i)}[^{(i)},d_{}]\) {Keep a history of conversation}.
4:\(^{(i)}_{} d_{}\) {Initialize with an empty template}.
5:for\(c=1,2,,c_{}\)do
6:\(^{(i,1)},^{(i,2)},,^{(i,n_{d})}_ {n_{d}}(h^{(i)})\)
7:\(^{(c)}*{argmax}_{\{^{(i,1)}, ,^{(i,n_{d})}\}}m_{d}(p,^{(i)})\) {Evaluate LLM responses using EW}
8:\(f^{(c)}d,p\).
9:\(h^{(i)} h^{(i)}+[^{(c)},f^{(c)}]\)
10:\(^{(i)}_{}*{argmax}_{\{ ^{(c)},_{}\}}m_{d}(p,^{(i)})\)
11:endfor
12:endfor
13:\(,*{argmax}_{\{(^{(i)},^{( i)}) i=1,2,,n_{p}\}}m_{d^{(i)}_{}}(p, ^{(i)})\)
14:return\(,\) {Return the final refined domain and problem PDDLs} ```

**Algorithm 1** Generating Domain PDDL and Problem PDDL Using Environment Feedback

Note that each environment contains \(N>1\) problems, therefore, we need to translate all problem instances into PDDL. Similar to Liu et al. , given one problem \(p_{1_{}}\) and its generated translation \(_{1}\), we translate the rest of the problems \(p_{2:N_{}}\) in a one-shot manner. That is, we generate \(_{i}:=_{1}(p_{1_{}},_{1},p_{i_{}})\) as the final problem translation for problem \(i\) for all \(2 i N\).

## 5 Experiments

**Dataset.** We consider PDDL files from real environments, taking nine domains from a combination of domain PDDLs from Liu et al.  and Seipp et al. . The LLM may have seen the mentioned domains in its pre-training data, which is a common issue for current benchmarks. To mitigate this issue, we also modify the original Grippers domain, and create a modified domain called "Grippers-ood" domain, to ensure no LLM has seen it previously. We generate natural domain descriptions for

Figure 3: Overview of our method. _Right:_ The process begins with natural language descriptions translated into problem PDDL by the LLM (red arrows). Then a domain is generated and refined through iterative cycles involving exploration walks in the environment, interaction with a classical planner, and feedback from the LLM (blue/black arrows). _Left:_ The iterative refinement process depicted on the right corresponds to single paths in the structures shown on the left. Each node represents a state in the refinement process, with arrows indicating problem translation (red), domain refinement (blue).

all PDDL files by back-translating them using GPT-4 and manually inspecting and modifying the translations for correctness. For each environment, we consider one domain PDDL \(d\) and \(N=10\) problem PDDLs \(p_{1:N}\). We use one problem for domain translation and EW evaluation, and all problems for evaluating a final domain response. We reserve the Blocksworld environment as an in-context example for prompting the LLM. As such, we do not evaluate the Blocksworld environment itself in our evaluations. See Appendices A.1 and C for more details on dataset curation.

**Feedback Format.** The natural language feedback given to LLM is in the following form: [Action sequence] [State description]. That is, we first provide LLM with the sequence of actions taken from one exploration walk, up until one action fails. Then, we provide the environment state description from the last step. We show an example of environment feedback and LLM response for the Termes environment in Listings 9 in the Appendix. We deliberately choose a simple feedback format to maintain the general applicability of our framework.

**Baselines and Metrics.** We use GPT-4  (gpt-4-1106-preview) as the LLM since models with lower capability may struggle with syntax errors . We consider the following methods: (1, 2) **Intrinsic Planning (CoT):** where the language model generates a complete plan without the help of any external planning library, based on the given descriptions, both with and without chain-of-thought prompting. This baseline does not leverage any classical planner or PDDL translation. (3) **P&D Chain:** Our proposed method (Algorithm 1) with \(n_{d}=n_{p}=1\). (4) **P&D Tree:** Our proposed method with multiple response generations (\(n_{d}=10,n_{p}=5\)). (5) **P&D Tree + DomProp:** Our proposed method with multiple response generations and domain proposals for each problem (see Appendix B.2). Following prior works [17; 5], we set a maximum conversation turns of \(c_{}=4\).

We run each algorithm for four seeds and compute the Best@4 metric, which takes the highest score among the four seeds. We report two metrics: (1) tasks solved2, measuring the fraction of the \(N=10\) tasks successfully solved (Eq. (1)), and (2) EW score, comparing the final domain through running exploration walks on all \(N\) problems (Eq. (2) with \(T_{}=10\)). We use the original fast-downward  library for planning, the modified fast-downward library from text-world  for python-compatible state explorations, and the VAL  library to validate plans.

**Results.** Table 2 shows the final results on various environments. We consider a domain generation to be solved if a method achieves \(>0.5\) solve rate since we observe the rest of the errors are problem translation errors rather than domain translation errors. Our proposed method solves 7 out of 10 domains, compared to 3 solved by the Intrinsic CoT baseline. We also generally observe the correlation of EW score with task solve rate. Particularly, even when the task solve rate is zero, the EW metric shows signs of progress, _e.g._, in domains such as Barman and Childsnack where all task solve rates are zero, the EW metric shows a clear distinction between method performances. Moreover, when the EW metric is high, such as \(1.0\), we observe a generated PDDL domain to be very close to the ground-truth domain, and differing in very few predicates. For instance, in the case of the "Hiking" environment, the P&D Chain achieves zero solve rate, but a perfect EW score, which we observe perfect solution in the case of P&D Tree.

**Computational Cost.** For the results in Table 2 using the GPT-4 model, we used \(12.40\) million input tokens and \(8.73\) million output tokens. Computing the EW is relatively negligible compared to the cost of LLM inference. In our experiments, computing the EW score for a single domain-problem pair takes less than two minutes on a 64-core server CPU.

## 6 Conclusion

In this work, we present a novel approach for modeling planning environments via PDDL translation using large language models (LLMs) and environment feedback, without relying on human intervention. The key contributions include introducing the Exploration Walk (EW) metric to measure domain similarity and guide domain refinement, and an iterative method that leverages LLMs to generate and refine PDDL domain and problem files. Evaluation on 10 real-world PDDL domains demonstrates the effectiveness of the proposed approach, outperforming a baseline that generates PDDL files in a single attempt without refinement. The method solves 7 out of 10 environments, achieving an average task solve rate of 66% and an average EW score of 0.84.

The current limitations include potentially insufficient and efficient exploration caused by random EW. More sophisticated EW strategies could improve the success rate while lowering the cost in the future. For example, strategies from the reinforcement learning literature (_e.g._, [27; 21]) could be adapted to improve exploration efficiency and success rates. Another limitation is that we have only applied the framework to PDDL environments, despite it being applicable to digital or even physical environments. We hope this work will inspire further research at the intersection of language models and planning, enabling the development of more advanced and autonomous planning systems.