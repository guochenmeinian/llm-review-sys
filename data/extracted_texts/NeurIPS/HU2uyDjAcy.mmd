# Local and Adaptive Mirror Descents in Extensive-Form Games

Come Fiegel

CREST - FairPlay, ENSAE Paris

Palaiseau, France

come.fiege@normalesup.org Pierre Menard

ENS Lyon, France

Pierre Menard

ENS Lyon

Lyon, France

Tadashi Kozuno

OMRON SINIC X

Tokyo, Japan

Todashi Kozuno

Google DeepMind

Paris, France

&Remi Munos

Google DeepMind

Paris, France

&Vianney Perchet

CREST - FairPlay, ENSAE Paris, Criteo AI Lab

Paris, France

&Michal Valko

INRIA

###### Abstract

We study how to learn \(\)-optimal strategies in zero-sum imperfect information games (IIG) with _trajectory feedback_. In this setting, players update their policies sequentially, based on their observations over a fixed number of episodes denoted by \(T\). As noted by Steinberger et al. (2020) and McAleer et al. (2022), most existing procedures suffer from high variance due to the use of importance sampling over sequences of actions. To reduce this variance, we consider a _fixed sampling_ approach, where players still update their policies over time, but with observations obtained through a given fixed sampling policy. Our approach is based on an adaptive Online Mirror Descent (OMD) algorithm that applies OMD locally to each information set, using individually decreasing learning rates and a _regularized loss_. We show that this approach guarantees a convergence rate of \(}(T^{-1/2})\) with high probability and has a near-optimal dependence on the game parameters when applied with the best theoretical choices of learning rates and sampling policies. To achieve these results, we generalize the notion of OMD stabilization, allowing for time-varying regularization with convex increments.

## 1 Introduction

The extensive-form representation of a game (Osborne & Rubinstein, 1994) can be depicted as a tree whose nodes correspond to the game states. At each state, the players choose some available actions and, based on these choices, the game transitions to the next state among the current state's children.

In imperfect information games (IIGs), players may only have access to partial information about the current game state upon taking action. Therefore, the state space is partitioned for each player into multiple information sets, which consist of indistinguishable states from the player's perspective. With perfect recall (Kuhn, 1950), when players remember their previous moves, each space of information sets also has a tree structure.

We focus more specifically on zero-sum IIGs represented in an extensive form under the perfect recall assumption, where the gains of one player, conventionally called the max-player, are equal to the losses of his opponent, the min-player. The primary goal is to design an algorithm learning \(\)-optimal strategies (von Neumann, 1928). To achieve this, one can use the self-play framework, where an agent controls both players for \(T\) episodes. At the beginning of each episode, the agent prescribes a strategy for each player. The agent then observes the play and updates the players' strategies for the next episode based on the outcome of the game. After \(T\) episodes, this protocol returns a guess ofstrategies with a small exploitability gap (Ponsen et al., 2011). In this learning framework, the agent has very limited feedback, only observing the rewards along each sampled trajectory, as opposed to richer feedback that would for example include all possible rewards and all transition probabilities, (Zinkevich et al., 2007; Hoda et al., 2010; Tammelin, 2014; Kroer et al., 2015; Burch et al., 2019) unrealistic in large games.

To deal with this learning framework, a well-studied approach is to unilaterally minimize the regret of each player during the interactions with the game, i.e. the difference between the cumulative gain the player would have obtained had he played the best fixed a posteriori policy and the cumulative gain obtained by following the sequence of policies. The key observation is that by minimizing the regret of both players, the average policies over the sequence of policies generated during the process converge toward optimal strategies at the rate of order \((1/)\)(Cesa-Bianchi and Lugosi, 2006; Kozuno et al., 2021). Regret minimizers such as CFR-based algorithm or online mirror descent (OMD) (Hoda et al., 2010; Kroer et al., 2015) can be used, leading to optimal rates (with respect to the game size) with the latter option (Bai et al., 2022; Fiegel et al., 2023).

Since the agent only observes trajectories of the game, an importance sampling estimate (Auer et al., 2003) of gain (or loss) is fed to the regret minimizer. However, the estimate of this loss usually suffers from high variance due to two reasons. First, the same sequence of policies is used to minimize the regret and to collect the trajectories, making the players strive to fulfill two competing goals: play a policy with small regret and play a policy leading to a small variance gain estimate. Second, importance sampling is applied to sequences of actions, that have in large games a very small probability of being played, leading to empirically large importance sampling weights and ultimately inflating the variance of the gain estimates.

To mitigate this issue, regularization and biasing the estimates can help (Kozuno et al., 2021; Bai et al., 2020). However, the high variance of the gain estimates remains problematic with large games, for which the algorithms are generally coupled with function approximation (Steinberger et al., 2020; McAleer et al., 2022). For instance, neural networks are particularly susceptible to noise (Zhang et al., 2021). A natural question is thus whether it is possible to learn optimal strategies without relying on importance-sampling over the sequence of actions.

To this aim, we consider a particular case of the self-play framework: the fixed policy sampling framework (Lanctot et al., 2009). In this setting, a fixed policy is used to collect the trajectories of the game. Precisely, at each round, one player, let's say the min-player, follows the fixed sampling policy to play against the current policy of the max-player. The collected trajectory is then used to update the current policy of the min-player. In the next episode, the max-player will follow a sampling policy against the current policy of the min-player, and so on. The outcome sampling MCCFR algorithm adopts this framework to update the two players' policy by regret minimization, feeding the CFR algorithm with gain estimated via importance sampling (Lanctot et al., 2009; Bai et al., 2020; Farina et al., 2021).

Recently, McAleer et al. (2022) proposed the ESCHER algorithm that removes the need for importance sampling in this framework. In particular, as the CFR algorithm is invariant by re-scaling of the gains and the weights of the sampling policy are fixed, ESCHER can directly operate with the unweighted history cumulative gain (Bai et al., 2020). Unfortunately, it still requires access to an oracle that provides this history of cumulative gains at an arbitrary information set.

Nonetheless, the insight of McAleer et al. (2022) cannot be used directly for OMD-based algorithms as they are not scale-invariant. Furthermore, the OMD-based algorithms generally work at the global game level whereas CFR-based algorithms work at the local level of the information set (Bai et al., 2020), making local adaptation to the problem easier.

ContributionsWe make the following main contributions:

* We propose the LocalOMD algorithm, in the fixed policy sampling framework, that allows adaptive learning rates and does not require importance-sampling over the sequence of actions but only for the current action. We explain how it can simply be seen as a regret minimization procedure applied to a local loss on each information set, similarly to Farina et al. (2019).

* We prove that LocalMD achieves, in this fixed sampling framework, a \(}(1/^{2})^{1}\) sample complexity with _any_ choice of non-degenerate sampling policy, ignoring the game and policy-dependent parameters.
* With an appropriate sampling policy and choice of learning rates, we prove that LocalMD, recover the \(}(H^{3}(A_{}+B_{})/^ {2})\) near-optimal sample complexity for learning \(\)-optimal strategies in a tabular setting, where \(H\) is the height of the tree, \(A_{}\) the total number of available actions for the min-player and \(B_{}\) the same quantity for the max-player. This sample complexity was also achieved in the fixed policy framework by BalancedCFR (Bai et al., 2022), but with a less generalizable procedure that updates the policy at one depth at a time.
* We generalize the dual-stabilization technique introduced by Fang et al. (2020) to analyze OMD with a time-varying regularization as long as the increments of the regularization are convex.
* Our tabular experiments reveal that our algorithm yields comparable results to existing baselines while demonstrating a reduced variance in loss estimation.

## 2 Settings and fixed sampling procedure

### Extensive-form games and regret

Game definitionWe consider a finite zero-sum IIG game \((,,,,,p,)\) with perfect recall. Given two behavioral policies \(=((|x))_{x}\) and \(=((|y))_{y}\), one episode of such game proceeds as follows: An initial game state \(s_{1} p(|s_{0})\) is first sampled in the set of states \(\) according to the transition function \(p\), starting from the root \(s_{0}\) of the tree. At depth \(h\), the min- and max-players respectively observe the information set \(x_{h}\) and \(y_{h}\) associated with the current state \(s_{h}\) in the spaces of information sets \(\) and \(\) (these spaces being two partitions of \(\)), then simultaneously choose and execute actions \(a_{h}(|x_{h})\) and \(b_{h}(|y_{h})\) in the sets of legal actions \((x_{h})\) and \((y_{h})\). As a result, the state transitions to a new state \(s_{h+1} p(|s_{h},a_{h},b_{h})\) in \(\), with the min- and max- players getting respectively the losses \(_{h}(|s_{h},a_{h},b_{h})\) in \(\) and \(1-_{h}\) according to the loss distribution \(\). This is repeated until a final state \(s_{H}\) of a fixed depth \(H\) is reached, after which the episode finishes.

Policies and actionsWe will denote by \(_{}\) and \(_{}\) the set of behavioral policies of the min- and max- players. Because of the perfect recall assumption, such policies, with an independent stochastic choice of action for each information set, are enough to describe the entire set of strategies (Laraki et al., 2019). We will also denote by \(A_{}\) and \(B_{}\) the total number of actions for respectively the min- and max- players, i.e. \(A_{}:=_{x}|(x)|\) and \(B_{}=_{y}|(y)|\).

Regret and \(\)-optimal strategiesWe are interested in learning \(\)-optimal policies through self-play over multiple episodes. A useful notion for this objective is the regret as explained in the introduction. We first define the value \(V^{,}=^{,}[_{h=1}^{H}_{h}]\) as the expected sum of losses (for the min-player) with respect to a pair of policies \((,)_{}_{}\). Given a sequence \((^{t},^{t})_{t[T]}\) in \(_{}_{}\), the regrets of the min- and max- players are then defined by

\[_{}^{T}:=_{^{t}_{}}_{t=1}^{T}(V^{^{t}, ^{t}}-V^{^{t},^{t}})_{}^{T}:= _{^{t}_{}}_{t=1}^{T}(V^{^{t},^{t}}-V^{^{t},^{ t}})\,.\]

Minimizing the regret of both players leads to the computation of an \(\)-optimal profile (equivalent to an \(\)-Nash equilibrium for two players zero-sum games) through the computation of an average of the policies. The following theorem quantifies this statement under the perfect recall assumption.

**Theorem 2.1**.: _(_Cesa-Bianchi & Lugosi_,_ 2006; Kozuno et al._,_ 2021_)_ _From a sequence \((^{t},^{t})_{t[T]}\) in \(_{}_{}\) a certain time-averaged profile \((,)\) is \(\)-optimal with \(=(_{}^{T}+_{}^{T})/T\)._

It especially shows that both averaged strategies converge to the set of optimal strategies as long as the regret of both players is sub-linear.

We now focus on the min-player point of view because of the symmetry of the game. Indeed, the following ideas will apply exactly the same way to the max-player, using the losses \(1-_{h}\) instead.

**Perfect recall and realization plan** Under the perfect recall assumption, players do not forget their past observations and actions. We can then assume, for any information set \(x\) and action \(a(x)\), the existence of a unique depth \(h[H]\) and history \((x_{1},a_{1},...,x_{h_{h}}a_{h})\) such that \(x_{h}=x\) and \(a_{h}=a\). Using this unique history, we define the realization plan \(_{1:}^{A_{}}\)\(()\) associated to a policy \(_{}\) with, for any \(x\) and \(a(x)\) by \(_{1:}(x,a):=_{i=1}^{h}(a_{i}|x_{i})\). It denotes the combined probability of choosing actions that lead to \((x,a)\). We will especially define \(Q_{}:=\{_{1:},_{}\}\) the treeplex, i.e. the set of all possible realization plans.

Loss and regret linearizationFor \(\) a max-player policy, the unique history also allows for the definition of adversarial transitions \(p^{}_{1:}^{}\) and adversarial losses \(^{}^{A_{}}\) with:

\[p^{}_{1:}(x):=p(x_{1}|s_{0})_{i=2}^{h}p^{}(x_{i}|x_{i-1},a_{i-1}) ^{}(x,a):=p^{}_{1:}(x)^{}_{h}(x,a)\]

where \(p(x_{1}|s_{0})\) is the probability that \(x_{1}\) is initially observed by the min-player, and, assuming that the max-player policy is set to \(\), \(p^{}(|(x_{i-1},a_{i-1}))\) denotes the probability of transitioning to \(x_{i}\) when \((x_{i-1},a_{i-1})\) is reached, and \(^{}_{h}\) the average loss \(_{h}\) associated to \(a\) when \(x\) is reached. Similarly to the realization plan, the adversarial transitions denote the combined probability of both Nature and max-player actions that lead to \(x\), assuming that the min-player plays the actions \((a_{1},...,a_{h-1})\).

Using a chain-rule argument, we get the relation \(V^{,}=^{},_{1:}\), given a pair of policies \((,)_{}_{}\), where \(,\) is the standard inner product of \(^{A_{}}\), defined by \( z_{1},z_{2}:=_{x}_{a(x)}z_ {1}(x,a)z_{2}(x,a)\). The regret can then be rewritten

\[^{T}_{}=_{^{}_{}}_{t=1}^{T} ^{t},^{t}_{1:}-^{}_{1:}\]

where \(^{t}:={^{}}^{t}\), which effectively reduces the problem to a linear regret problem over the convex polytope \(Q_{}\) of realization plans.

Several techniques exist to sequentially choose policies \((^{t})_{t[T]}\) minimizing \(^{T}_{}\), assuming that the losses \(^{t}\) are observed after each round \(t\)(Hoda et al., 2010). However, in the _trajectory feedback_ setting, these losses are not observed, and can only be estimated from the observation of the trajectories \((x^{t}_{1},a^{t}_{1},...,x^{t}_{H},a^{t}_{H})\) and partial losses \((^{t}_{1},...,^{t}_{H})\) of each round.

### Fixed sampling policy

In the _fixed sampling_ framework (Lanctot et al., 2009), both players always use the same policy for the observations of the trajectory. However, the two observations can not be done simultaneously with such an approach, as the learning would then be quite naive. The solution, summarized in Algorithm 1, is for the two players to take turns between an observation phase, in which they play their fixed sampling policy \(^{s}\) or \(^{s}\), and an interaction phase, in which they play their updated policy \(^{t}\) or \(^{t}\). The underlying idea is that the observation phase lets each player observe how the game unfolds against the opponent in its interaction phase, playing its updated policy. Given upper-bounds of the regrets \(^{T}_{}\) and \(^{T}_{}\) associated to the sequence \((^{t},^{t})_{t[T]}\), the previous Theorem 2.1 then characterizes the optimality of the outputted time-averaged profile \((,)\).

```
1:Input: Fixed sampling policies \(^{s}\) and \(^{s}\). Initial policies \(^{1}\) and \(^{1}\) and update procedure for each player
2:For \(t=1\) to \(T\)
3:The min-player observes the full outcome of an episode with the policies \((^{s},^{t})\)
4:The max-player observes the full outcome of an episode with the policies \((^{t},^{s})\)
5:The min- and max-player respectively update \(^{t+1}\) and \(^{t+1}\) based on their past observations
6:Output: The time-averaged policies \(,\) of Theorem 2.1 ```

**Algorithm 1** Learning procedures with fixed sampling policies for two players

While theoretically optimal algorithms already exist using simultaneous regret minimization procedures (Bai et al., 2022; Fiegel et al., 2023), this framework allows for the removal of the globalimportance sampling term of the loss, which reduces the variance to make algorithms more suitable beyond the tabular setting (McAleer et al., 2022). Indeed, as the probability of choosing a sequence of action reaching a given information set is fixed, the average estimations of the losses do not need to be re-weighted based on the inverse of a changing probability. This re-weighting eventually leads to unstable function approximation, e.g. with neural networks, as this probability can be very small.

Furthermore, the fixed sampling framework also allows aggressive policies more focused on exploitation, as the observation side is handled by the sampling strategy. The downside is that this sampling policy must be fixed in advance, which requires defining a good sampling policy beforehand.

From now on, we again focus on the min-player for the same symmetry reasons.

Estimated regretBased on the min-player observations, we define \(}^{T}_{}\) the estimated regret by

\[}^{T}_{}:=_{^{t}_{}}_{t=1} ^{T}^{t},\,^{t}_{1:}-^{}_{1:}\]

where the \(^{t}\) are the importance-sampling estimated loss vectors, defined for each information set \(x\) of depth \(h\) and action \(a(x)\) by

\[^{t}(x,a):=_{\{x=x^{t}_{h},a=a^{t}_{h}\}}}{ ^{s}_{1:}(x,a)}^{t}_{h}\]

with \(x^{t}_{h}\) the visited information set, \(a^{t}_{h}\) the chosen action and \(^{t}_{h}\) the loss at depth \(h\) of episode \(t\).

The following theorem states that upper-bounding this estimated regret is enough to upper-bound the actual regret, up to an additional additive term. Its proof is given in Appendix B and relies on Bernstein-type inequalities.

**Theorem 2.2**.: _Assume that the estimated losses are obtained with a fixed positive sampling policy \(^{s}\) as above. Then, for any sequence \((^{t})_{t[T]}\) of \(_{}\) and any \((0,1)\), the following bound holds with a probability at least \(1-\)_

\[^{T}_{}\{}^{T}_{ {min}},0\}+4)T}\]

_where \(:=(} 1}{})\) and \((^{s}):=_{_{}}_{x}_{a _{x}}(x,a)}{^{s}_{1:}(x,a)}\)._

A similar proposition is proved by Farina et al. (2020). Our bound is specific to the importance-sampling loss estimator, but tighter by a factor \()/H}\).

_Remark 2.3_.: The quantity \((^{s})\) can be efficiently computed recursively for each of the sub-trees induced by an information set \(x\), and we will denote by \((^{s}|x)\) the associated quantities. The same recursion shows that the _balanced policy_\(^{}\), which plays proportionally to the total number of actions of each sub-tree, minimizes all these local quantities and satisfies \((^{})=A_{}\). The related computations are provided in Appendix C.

## 3 Adaptive Mirror Descent

We shall now focus on the update procedure the min-player can use to minimize this estimated regret. Let us first define some important notions of convex optimization.

**Definition 3.1**.: Let \(^{n}\) be a non-empty open convex, and \(\) be its closure. A function \(:\) is said to be Legendre if \(\) is strictly convex, continuously differentiable on \(\) and \( y,\ _{x y}(x)=+\). The Bregman divergence \(_{}:\) of a Legendre function \(\) is defined as \(_{}(x,y):=(x)-(y)-(y),x-y\). The Fenchel conjugate \(^{}:^{n}\{+\}\) of \(\) is defined by \(^{}()=_{x},x- (x)\).

### Online Mirror Descent and dilated entropy

In an extensive-form game with perfect recall, algorithms based on the Online Mirror Descent (OMD) typically compute at each time step \(t\) the update

\[^{t+1}=*{arg\,min}_{_{}}^{t},_{1:}+_{}(_{1:},^{t}_{1:})\] (OMD)where \(^{t}\) is the estimated loss and \(:Q_{}\) a Legendre regularizer.

Dilated entropyA common choice of such regularizer is the dilated entropy (Hoda et al., 2010; Kroer et al., 2015). It requires for each \(x\) a Legendre regularizer \(_{x}\) over a convex domain \(}^{|(x)|}_{ 0}\) that contains the simplex \(_{(x)}:=\{,\,_{a(x)}(a)=1\}\). For a given list of positive weights \(=((x))_{x}\), the dilated entropy \(^{}_{}\) satisfies for any \(_{}\):

\[^{}_{}(_{1:}):=_{x}(x)_{1 :}(x)_{x}((|x))\]

where \(_{1:}(x):=_{a(x)}_{1:}(x,a)\). Using this dilated entropy as the regularizer, the OMD updates become

\[^{t+1}=*{arg\,min}_{_{}}^{t},_{1:}+^{}_{}(_{1:}, _{1:}^{t})\]

where \(^{}_{}(_{1:},_{1:}^{t}):=_{x }(x)_{1:}(x)_{x}(_{1:}(|x),_{1:}^{t}( |x))\) and \((_{x})_{x}\) are the individual Bregman divergences of the \((_{x})_{x}\). The benefits of this regularization are that it efficiently suits the structure of the game and that the associated updates are easily computed recursively, starting from the final states.

### Stabilized OMD algorithm

The regularizer \(\) sometimes needs to change over time. For example, when \(T\) is unknown, a regularizer of the form \(^{t}=/^{t}\) is usually considered, with \(^{t}=t^{-1/2}\) the learning rate. Fiegel et al. (2023) gives another example of time-varying regularization, adapting the regularization to the game structure that is assumed to be initially unknown. The previous updates (OMD) do not however allow adaptive regularization in general. In fact, even the simple learning rate decrease \(^{t+1}=t^{-1/2}\) can lead to a linear regret dependence with time (Orabona and Pal, 2018).

In this part, we shall consider more generally a sequence of Legendre regularizers \((^{t})_{t[T]}\) defined on a convex domain \(^{n}\), and that the player chooses a sequence of primal iterates \((w^{t})_{t[T]}\) (respectively the updated realization plans \((_{1:}^{t})_{t[T]}\) of our settings) in a closed convex set \(\) (respectively the treeplex \(Q_{}\)) included in \(\), according to a sequence of dual increments \((^{t})_{t[T]}\) in \(^{n}\) (respectively the estimated losses \((^{t})_{t[T]}\)) observed sequentially.

Fang et al. (2020) proposed in the presence of non-increasing learning rates, to use a technique called dual-stabilization to recover the classical OMD bounds. We noticed that their updates can be interpreted as

\[w^{t+1}=*{arg\,min}_{w}^{t},w +_{^{t}}(w,w^{t})+_{^{t+1 }-^{t}}(w,w^{1})\] (GDS-OMD)

with \(^{t+1}-^{t}\) incremental functions assumed to be convex, generalizing their special case \(^{t+1}=/^{t+1}\). The following theorem, proven in Appendix D shows that classical OMD guarantees can be recovered with these updates.

**Theorem 3.2**.: _Let \((w^{t})_{t[T]}\) be a sequence of primal iterates generated by the updates (GDS-OMD), with convex incremental functions. Then for any \(w^{}\),_

\[_{t=1}^{T}^{t},w^{t}-w^{}_{^{T}}(w^{},w^{1})+_{t=1}^{T}_{^{t,}} (^{t}(w^{t})-^{t},^{t}(w^{t}))\]

_where the \((^{t,})_{t[T]}\) are the respective Fenchel conjugates of the \((^{t})_{t[T]}\)._

Compared to the guarantees obtained with previous adaptive procedures, such as Ada-MD (Joulani et al., 2017), the first term of the bound is stated with respect to \(w^{1}\) instead of the sequence \((w^{t})_{t}\), which is important for some \((^{t})_{t}\) sequences (Orabona and Pal, 2018).

_Remark 3.3_.: AdaGrad for stochastic gradient descent (Duchi et al., 2011) is an interesting example of regularizaiton with convex increments (and not only through a decreasing learning rate). It uses the adaptive regularization \(^{t+1}=\|\|_{(G^{t})^{1/2}}^{2}\), where \(G^{t}\) is a positive semi-definite matrix defined with the gradients \(g_{k}\) by either \(G^{t}=_{k=1}^{t}g_{k}g_{k}^{T}\) or, more efficiently, by \(G^{t}=(_{k=1}^{t}g_{k}g_{k}^{T})\)

**Adaptive dilatation** In the extensive-form game setting based on the dilated entropy \(_{}^{}\), this stabilization can be applied to have weights \((^{t}(x))_{x,t[T]}\) that vary with times. The convexity assumption of \(_{^{t+1}}^{}-_{}^{}\) then rewrites to having locally non-decreasing weights for each \(x\). In this particular case, the updates are obtained with the formula

\[^{t+1}=*{arg\,min}_{_{}}^{t},_{1:}+_{^{t}}^{}(, ^{t})+_{^{t+1}-^{t}}^{}(,^{1})\,.\] (DDS-OMD)

## 4 LocalOMD algorithm

### Algorithm

Let us now consider the fixed sampling framework introduced in Section 2.2. Given a sequence \((^{t}(x))_{t[T]}\) of locally non-increasing learning rates for each \(x\), we introduce the LocalOMD algorithm described in Algorithm 2, that uses the updates (DDS-OMD) above with the adaptive weights \(^{t}(x)=1/(_{1:}^{s}(x)^{t}(x))\). Dividing the loss by the importance sampling term \(1/_{1:}^{s}(x)\) through the learning rates lets it bypass the large variance that this rate can introduce.

Local lossThis algorithm can be interpreted as one that locally applies the updates (GDS-OMD) using the local loss \(_{h}^{t}\), a regularized version of the sum of subsequent losses. Even though this algorithm results from a global minimization procedure, the local loss only uses the probability \(^{s}(a|x)\) of choosing the last action \(a(x)\) in the important sampling, instead of the combined probability \(_{1:}^{s}(x,a)\) of the realization plan. A similar decomposition was observed by Farina et al. (2019) for the non-stochastic settings, in which both players directly observe the gradient associated with their policies.

For this reason, the local loss will consistently be at most of order \((HA)\). Meanwhile, the loss used by Fiegel et al. (2023) can be of order \((A_{})\) (approximately \(A^{H}\) in the worst case), even with IX exploration attempting to alleviate the importance sampling issue. This presents a challenge for potential applications involving function approximation, where \(A_{}\) becomes very large (McAleer et al., 2022). For instance, such high-variance estimates could lead to highly unstable training dynamics of a policy parametrized with a neural network.

ComplexityAt each iteration, the algorithm only needs to update the policy along the observed trajectory, so the time complexity per iteration is only \(H\) times the cost of a local update. If \(_{x}\) is the Kullback-Leibler divergence, the local updates then simplify to some Exponential Weights updatesand the total time complexity of an iteration becomes \((HA)\), where \(A\) is an upper-bound on the local number of actions.

### Theoretical analysis

The analysis of LocalOMD, detailed in Appendix E is derived from Theorem 3.2 that bounds the estimated regret. The results on the real regret are then obtained with Theorem 2.2. We now present two choices of regularization and their associated guarantees.

Adaptive ratesAs LocalOMD treats each information set \(x\) as a separate problem through the local losses \(_{h}^{t}\), an interesting choice is to consider the same adaptive rates that would be used in the \(K\)-armed bandit problems. The following theorem provides an upper bound in this case.

**Theorem 4.1**.: _(Informal, exact statement in Appendix E) For a large class of regularizers \((_{x})_{x}\) and learning rates \((^{t}(x)_{x,t[T]})\), the regret has a \(()\) upper bound (hiding the game-dependent terms) with a probability at least \(1-\). Such learning rates include, for all \(x\) of depth \(h\),_

\[^{t}(x)=./^{t}_{\{x=x_{h}^{ k}\}}}.,^{t}(x)=./^{t}_{\{x=x_{h}^{k} \}}(_{h}^{k})^{2}}..\]

The adaptive learning rates mentioned for this theorem generally enjoy better performances in practice. Furthermore, they require no initial computation and are easily updated.

Optimal ratesThe following theorem uses a constant learning rate that locally depends on the \((^{*}|x)\) quantities of Remark 2.3, and on the \(A:=_{x}|(x)|\) quantity that upper bounds the local number of available actions on the whole tree.

**Theorem 4.2**.: _Using LocalOMD with \(^{1}\) as the uniform policy, with the learning rates \(^{t}(x)=/(^{*}|x)\) where \(=)/(3HT)}\), and with \(_{x}\) the Shannon entropy \(_{x}()=_{a(x)}(a)((a))\), we have with a probability at least \(1-\) and \(=(2(A_{}+1)/)\),_

\[_{}^{T}(4+2)\;H^{3/2})T}\,.\]

Note that these rates are not adaptive and thus do not require the stabilization introduced in Section 3.2. When using the balanced policy \(^{*}\) as the sampling policy, for which \((^{*})=A_{}\), we obtain with Theorem 2.1 the rate \(}(H^{3/2}}T})\), near-optimal up to the \(H\) dependency (Bai et al., 2022).

## 5 Experiments

We implemented LocalOMD, with the parameters of Theorem 4.1 and Theorem 4.2, then tested it against the theoretically optimal BalancedCFR(Bai et al., 2022) using the balanced policy as the sample policy, and BalancedFTRL(Fiegel et al., 2023). The algorithms were compared on three standard benchmark games: Kuhn poker (Kuhn, 1950), Leduc poker (Southey et al., 2005) and liars dice, using the version 1.4 of the OpenSpiel library (Lanctot et al., 2019) under the Apache 2.0 license. The learning rates (and the \(IX\) parameters for the relevant algorithms) were optimized independently for each algorithm using a grid search. The code is available at https://github.com/anon5493/LocalOMD-experiments.

The results are given with respect to the total number of episodes used for learning. This technically disadvantages the fixed sampling algorithms, as these require more than one episode at each round \(t\) while still performing a single update on the policy of each player. The exploitability gap, along with the variance across the different instances of the simulation, is plotted in Figure 1, top. Note that this variance across the instances is different from the variance of the estimated loss vector \(^{t}\) our method tries to reduce, which is plotted in the Figure 1, down.

Focusing on the exploitability gap, we observe that the two versions of LocalOMD behave similarly and constantly beat BalancedCFR, mainly because the latter needs to update each depth with independent samples, thus needing \(H\) times more episodes overall. The results of BalancedFTRL are more comparable, exhibiting for example better performances on liars dice but worse on Leduc poker.

In the second figure, we observe that the algorithms based on a fixed sampling procedure indeed get a smaller variance in their loss estimation as the sampling policy stays consistently balanced. BalancedCFR again gets worse results compared to LocalOMD as the losses of each depth are only estimated every \(H\) iteration, which increases its variance.

## 6 Conclusion

We studied the use of a fixed sampling OMD procedure for the computation of \(\)-optimal strategies. This approach relies, for each player, on an uncoupling between the observation policy and the interaction policy as described in Algorithm 1. This uncoupling is in direct contrast with the more restrictive semi-bandit setting usually considered for self-play, where these two policies must coincide by design. Notice that this is not the standard exploration/exploitation trade-off, as even in the expert setting (with full information), some kind of exploration is still required.

While the balanced observation policy gets the optimal rates in the worst case, it may not always be the best one for a given game. An alternate choice is to instead use for the observations the current average policy (Gibson et al., 2012). This choice can be adapted to the fixed sampling framework, by restarting the algorithm after a certain number of episodes and using the computed average as the new sampling policy.

The proposed algorithm LocalOMD also enjoys simultaneously two interpretations: one as a Mirror Descent type algorithm working at the global level, with a single update performed at each iteration over the whole tree; and one as regret minimizers working locally at each information set, which makes it very similar to a CFR algorithm despite a fundamentally different approach.

We would like to conclude by providing the following interesting research directions.

Problem-dependent optimalityFor a given game structure and fixed sampling policy \(^{s}\), is there a policy-dependent lower bound \(()T})\) on the regret? We wonder if the \((^{s})\) quantity of Remark 2.3 denotes some sort of complexity related to the problem.

General sum gameUsing the same techniques as Bai et al. (2022), in a general sum game with potentially more than two players, LocalOMD can be shown to converge to an \(\)-approximate normal-form coarse correlated equilibrium. Are convergences to other forms of correlated equilibrium possible using this fixed sampling policy framework?

On-policy algorithmsIs it also possible to remove the importance-sampling of the previous actions in the usual semi-bandit framework that observes with the current policy? The answer is not obvious since the current approach heavily relies on the fact that the sampling policy is fixed.

## 7 Acknowledgements

This research was supported in part by the French National Research Agency (ANR) in the framework of the PEPR IA FOUNDRY project (ANR-23-PEIA-0003) and through the grant DOOM ANR-23-CE23-0002. It was also funded by the European Union (ERC, Ocean, 101071601). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.