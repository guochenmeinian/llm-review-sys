# Traditional baselines

ActSort: An active-learning accelerated cell sorting algorithm for large-scale calcium imaging datasets

 Yiqi Jiang

Stanford

&Hakki O. Akengin

Stanford

&Ji Zhou

These authors contributed equally to this work.

Mehmet A. Aslihak

UC San Diego

&Yang Li

Stanford

&Radoslaw Chrapkiewicz

Stanford

&Oscar Hernandez

Stanford

&Sadegh Ebrahimi

Stanford

&Omar Jaidar

Stanford

&Yanping Zhang

HHMI, Stanford

&Hakan Inan

Meta

&Christopher Miranda

Stanford

&Fatih Dinc

Stanford

&Marta Blanco-Pozo

Stanford

&Mark J. Schnitzer

HHMI, Stanford

&

###### Abstract

Recent advances in calcium imaging enable simultaneous recordings of up to a million neurons in behaving animals, producing datasets of unprecedented scales. Although individual neurons and their activity traces can be extracted from these videos with automated algorithms, the results often require human curation to remove false positives, a laborious process called _cell sorting_. To address this challenge, we introduce ActSort, an active-learning algorithm for sorting large-scale datasets, which integrates features engineered by domain experts together with data formats with minimal memory requirements. By strategically bringing outlier cell candidates near the decision boundary up for annotation, ActSort reduces human labor to about 1-3% of cell candidates and improves curation accuracy by mitigating annotator bias. To facilitate the algorithm's widespread adoption among experimental neuroscientists, we created a user-friendly software and conducted a first-of-its-kind benchmarking study involving about 160,000 annotations. Our tests validated ActSort's performance across different experimental conditions and datasets from multiple animals. Overall, ActSort addresses a crucial bottleneck in processing large-scale calcium videos of neural activity and thereby facilitates systems neuroscience experiments at previously inaccessible scales. (Link to our Github: https://github.com/schnitzer-lab/ActSort-public)

## 1 Introduction

Systems neuroscience research has been revolutionized by large-scale Ca\({}^{2+}\) imaging techniques, which now regularly produce terabyte scale datasets [1; 2; 3; 4; 5; 6; 7; 8; 9]. This rapid increase in neural data has necessitated automated methods for identifying neuronal sources, leading to the development of cell extraction algorithms [10; 11; 12; 13]. These algorithms facilitate large-scale analysis by detecting even up to a million neurons [9; 14; 15]. However, they can sometimes misidentify non-neuronal elements as neuronal structures, potentially leading to erroneous biological interpretations in downstream analyses [16; 17; 18].

To mitigate errors arising in automated cell extraction pipelines, experimental studies often perform quality control processes, known as _cell sorting_, to validate and refine cell extraction outputs [10; 11;12; 13; 19]. In practice, researchers either laboriously annotate all cell candidates using specialized software (See CIATAH, ) or employ "cell classifiers" that are trained on a subset of annotated datasets to label the rest [10; 11; 20]. However, manual annotation is no longer practical given the large data sizes in modern neuroscience [8; 14; 15; 21; 22]. As we show in this work, existing automated methods still require substantial human labor to achieve desirable accuracy. Yet, human annotations are known to develop undesirable biases ([23; 24; 25; 26; 27]) due to, _e.g._, getting tired after long hours of repetitive work. Therefore, there is an urgent need for a scalable and robust solution that minimizes human labor without compromising the accuracy of the cell sorting.

A potential solution may lie in the active learning paradigm, a branch of machine learning that optimizes model performance while minimizing the need for annotated samples [28; 29; 30; 31; 32; 33; 34; 35]. In this paradigm, so-called query algorithms select the most informative and sparse samples to be labeled by human annotators [36; 37; 38]. To date, active learning has improved several scientific pipelines such as classifying pathological images [39; 40], detecting intracranial hemorrhage , and segmenting melanoma . Inspired by these successes, we set out to leverage the active learning framework to accelerate the cell sorting process.

In this work, we introduce ActSort: an active learning accelerated cell sorting algorithm to process large-scale Ca\({}^{2+}\) imaging datasets. As a first step, we developed a memory-efficient and user-friendly cell annotation software. Using this software, we designed an extensive cell annotation benchmark. This benchmark contains roughly 160,000 annotations of 40,000 cells collected using one- and two-photon mesoscopes across 5 mice, which we used to test and validate ActSort. Supported with novel features engineered by expert neuroscientists and a novel query algorithm tailored for cell sorting, ActSort matched human-level accuracies while reducing the required human labor to a negligible fraction of the full samples (down to 3% when training from scratch, and 1% if ActSort is initialized with previously annotated samples). In large-scale mesoscope movies, ActSort achieved convergence by sorting few hundreds of cells (c.a. 1 hour of human labor). In contrast, to achieve the same mark, labeling random subsets of representative datasets required sorting an order of magnitude larger number of cells. Notably, our analysis also revealed significant disagreements among human annotators, yet semi-automated classifiers mitigated this large variability across experimenters. Overall, our findings suggest that ActSort can save experimentalists significant time, and improve the reliability, robustness, standardization, and reproducibility of systems neuroscience research.

## 2 Results

Our cell sorting pipeline (ActSort) consists of three main components (Fig. 1): the preprocessing module, the graphical user interface (GUI), and the active learning module. The full pipeline is available to end-users with a point-click installation in our GitHub repository  and can be used as an application or in Matlab-online without a license. In this section, we introduce ActSort and validate it with several experiments on large-scale annotation benchmarks.

### A large scale benchmark of 160,000 annotations sorted with a custom software

Only five years ago, a Ca\({}^{2+}\) imaging movie with a 100 GB size was considered large . Today, neuroscientists routinely collect datasets several terabytes in size . Given this rapid increase, we start our work by designing a scalable software with a primary focus on memory efficiency. Without keeping the full movie in RAM, the software needs to include essential information used by human annotators to determine whether a sample is a cell: i) the shapes of cells' extracted spatial footprints, ii) their Ca\({}^{2+}\) activity traces over time, and iii) the movie snapshots at the time of Ca\({}^{2+}\) events (Fig. S1; Appendix B). While earlier work often retained only the first two elements for memory efficiency , we found that the spatiotemporal information provided by the Ca\({}^{2+}\) movie snapshots was crucial for human annotators, especially in edge cases, which are particularly important for the active learning paradigm (see below). This design choice also aligns well with the newer Ca\({}^{2+}\) imaging processing pipelines (see the cell sorting module in CIATAH ), and can be incorporated with an efficient data compression procedure without increasing dataset sizes significantly.

To achieve this design goal, as part of the preprocessing module (Fig. S2), we opted to compress the movie and the cell extraction results jointly. We saved the variables in sparse formats whenever applicable and retained only the relevant parts of the Ca\({}^{2+}\) imaging movies where the cells had Ca\({}^{2+}\) events (up to a pre-defined maximum; see Appendix B.1 for details). This approach allowed us to compress the large inputs of the traditional cell sorting pipelines (Fig. 1), resulting in approximately \(270 90\) MB/1,000 cells (mean \(\) std over 5 mice) data sizes. Thanks to the extreme compression of the movie and cell extraction data, we were able to develop a memory efficient and user-friendly graphical user interface for cell sorting (Figs. S1, S2, and S3). This software does not require any coding knowledge, utilizes buttons and sliders to operate, and can be used on laptops.

Most public Ca\({}^{2+}\) imaging datasets share cells' spatial profiles and Ca\({}^{2+}\) activity traces, but not the movies themselves due to large data sizes [45; 46]. Yet, motion artifacts and/or neuropil contamination can often not be identified by simply looking at Ca\({}^{2+}\) activity traces, but can alter biological conclusions [13; 18]. Thanks to our memory-efficient data format, open-sourced datasets can now include relevant movie snapshots without astronomical increases in data sizes, and the quality of the Ca\({}^{2+}\) activity traces can be probed with our software.

After coding the cell sorting software, we first searched for public datasets to validate the use of cell classifiers and active learning paradigm as a viable method for automated cell sorting. Although there are publicly available Ca\({}^{2+}\) imaging movies with annotated cells, notably NeuroFinder challenge , these datasets are often region specific and contain few hundreds of cells. Yet, we have designed ActSort to process large-scale mesoscopic movies. These modern movies simultaneously record large populations of neurons across multiple brain regions, and are now regularly collected by neuroscience labs [9; 14; 44]. To the best of our knowledge, no annotation dataset is publicly available with brain-wide mesoscopic movies.

To bridge this gap, we curated a total of five mesoscopic movies: i) three one-photon Ca\({}^{2+}\) imaging movies spanning the hemisphere through a 7mm window (Table S1), ii) one Ca\({}^{2+}\) imaging movie from previous work () spanning several neocortical regions (Table S2), and iii) one two-photon Ca\({}^{2+}\) imaging movie capturing layer 2/3 cortical pyramidal neurons in mice primary visual cortex (Table S3). We extracted an approximate total of 40,000 cells from these movies using the EXTRACT cell extraction algorithm . We then had 6 annotators independently perform cell sorting on these movies, with 4 annotators per movie, totaling up to roughly 160,000 annotations (**Methods**; Appendix C). We used this benchmark to perform validation experiments throughout this work.

### Automated cell sorting with linear classifiers utilizing newly engineered features

Automated cell sorting pipelines rely on training cell classifiers to annotate the unlabeled cell candidates [10; 20; 44]. These classifiers often use features derived from the cell extraction outputs and, in some cases, from Ca\({}^{2+}\) imaging movies . The central hypothesis behind this automation is that these derived features--such as the stereotypical shapes of cells' spatial profiles--contain sufficient information to distinguish false positives from real cells. However, relying on a select few features overlooks a crucial aspect of cell sorting: human annotators incorporate information from broad sources in their decisions. Consequently, we hypothesized that cell classifiers would benefit from a broader class of features than those traditionally implemented in the literature.

Figure 1: **An active learning accelerated framework for rapid quality control of cell extraction results.**_Left._ A typical processing of Ca\({}^{2+}\) imaging movies involves identification of cell candidates by automated cell extraction algorithms. _Middle._ ActSort’s preprocessing module jointly compresses the cell extraction results and the Ca\({}^{2+}\) imaging movie to allow memory efficient cell sorting in large-scale Ca\({}^{2+}\) imaging movies, and computes quality metrics for each cell candidate. The selection GUI allows human annotators to visually inspect the cell extraction results and annotate the true and false positive cell candidates. The active learning module, working in closed-loop feedback with the selection GUI, strategically selects next cell candidates to be annotated by the experimenter in order to optimally reduce the human effort. _Right._ The resulting outputs from ActSort are probabilities and binary labels (cell or not a cell) for each cell candidate.

To test this hypothesis, we first designed a traditional feature set (**Methods**; Appendix B.1), which aims to be a strong baseline representing the set of features used by existing cell extraction pipelines [10; 12; 13; 20; 44]. With these features, we processed hemisphere Ca\({}^{2+}\) imaging movies from three mice, encompassing 28,010 cell candidates and 112,040 annotations (Table S1). For each mouse, we trained cell classifiers using traditional features on half of the cell candidates and predicted the identity of the other half. While the classifiers performed superbly in identifying true positives, correctly accepting 97% of the cells (data not shown), they were suboptimal in rejecting false positives, accurately rejecting only 67% of the candidates rejected by human annotators (Fig. 2A). Part of this low accuracy may be due to the fact that only a small fraction of the samples (often \(<\)10%) are false positives, and human annotators may struggle to remain consistent with their strategy for duplicate selection and/or when deciding edge cases. Nevertheless, this observation was in line with our prediction that there may be room for improvements in the feature sets used by cell classifiers.

To bridge this gap, we engineered 76 new features for the cell classifiers by mimicking the _spatiotemporal_ information human annotators pay attention to during cell sorting (see Appendix B.1 for the full list). This set includes traditional temporal features (e.g., number of spikes in a trace, signal-to-noise ratio) and spatial features (e.g., mean pixel intensity, circumference), as well as novel spatiotemporal features (e.g., severity of the non-Gaussian noise contamination). To ensure memory-efficient computation, we derived these features solely from the compressed data format, allowing feature extraction on demand or during the cell extraction routine. Re-analyzing the dataset from Fig. 2**A**, we found that the new features increased the rejection accuracy to 72% (Fig. 2**A**) without decreasing the accuracy of accepting true-positives (data not shown, 97%), leading to more effective separation between _cell_ and _not cell_ samples in our benchmarks (Fig. 2**B-C**). We also confirmed that this newly engineered set of features were more efficient and effective than features extracted in an unsupervised manner from a ResNet-50 model (Fig. S4, Appendix A.1).

The question remains, however, how can we improve the cell classifiers further? We conjectured that one of the reason behind the relatively low rejection accuracy (\(72\%\) with the new features) may be the inconsistency of human annotators, and thereby the imperfect "ground truth" evaluators. We studied this question extensively in Appendix A.2, which revealed that individual human annotators were indeed quite inconsistent. Though, cell classifiers trained on their annotations ended up being more consistent (Fig. S5). Yet, if human annotators are inconsistent, how can we validate the accuracy of ActSort, or in general, automated cell sorting? To address this, we designed a cross-validated evaluation process that incorporated the fact that humans often disagree with each other (Fig. S6). Specifically, to evaluate the active learning routines trained with a specific annotator, we created a ground truth evaluator by taking the majority vote of the three held-out annotators. Fortunately, the majority votes among the held-out human annotators had a good consistency quantified with the intra class correlation (ICC) analysis (\(ICC=0.79 0.05\), mean \(\) std, two-way mixed, average measures ), far outperforming the individual annotators (\(ICC=0.56 0.06\), mean \(\) std, two-way mixed, single rater ICC). Thus, throughout this work, the annotations performed by either the classifiers and/or the experimenters are evaluated on these majority-voted held-out labels.

Figure 2: **Feature engineering improves the accuracy of automated cell sorting with linear classifiers.****A** The newly engineered features increased classifiers’ ability to reject false-positives and led to improved area under the receiver operating characteristic curve (Wilcoxon signed-rank tests (\({}^{***}p<10^{-3}\))). **B** We computed the discriminability indices (\(d^{}\)) that quantify how well each feature can separate out cells from false-positives. **C** Many features exhibited middle or strong effects (\(d^{} 0.5\) or \(d^{} 0.8\), respectively), whereas few features were not discriminative for this particular dataset. Boxes span the 25th to 75th percentiles, whiskers 1.5 times the inter-quartile range.

### Discriminative-confidence active learning query algorithm for cell sorting

So far, we introduced our custom software for cell sorting, discussed how to utilize feature engineering and cell classifiers to annotate unlabelled cell candidates, and established how to reliably and consistently evaluate the cell sorting results via majority votes. Now, we introduce the final piece: integrating the cell classifiers into the active learning framework with "query algorithms."

In ActSort, the active learning query algorithm prompts human annotators to label specifically selected samples. These samples are expected to improve the automated cell sorting accuracy, which creates an online feedback loop between the annotator and classifier (Fig. 1). But how should the samples be chosen for annotation? A straightforward approach is to annotate a random subset and label the rest, as has been done in the literature to date [10; 20]. Yet, as we discuss below, better alternatives exist.

When designing alternative query algorithms, we focused primarily on speed and scalability, as sorting is performed on the spot and within seconds. Therefore, we considered query algorithms that can train and predict rapidly. Two such strategies are confidence-based active learning (CAL) [49; 50] and discriminative active learning (DAL) . These query algorithms aim to mitigate two distinct types of uncertainties: (i) uncertainty regarding the sample's position relative to the decision boundary of the current model (CAL), and (ii) uncertainty regarding whether labeled samples faithfully represent the full dataset (DAL). Both query algorithms achieve these aims with rapidly trainable linear classifiers (see **Methods** for details; Appendix B.3).

Briefly, CAL trains a cell classifier on the currently labeled samples and picks the unlabeled sample closest to the decision boundary for annotation. DAL trains a secondary "label" classifier to discriminate labeled cells from the unlabeled data and selects a sample most different from those labeled subset. Evidently, an optimal solution should bring samples up for annotation that are close to the final decision boundary (CAL). However, the query algorithm should approximate where the final boundary is with sparse samples, which requires collecting diverse samples that represent, roughly, the full dataset (DAL). Therefore, both approaches have their unique advantages.

To amplify the strengths of both approaches, we introduce a new query algorithm: discriminative-confidence active learning (DCAL). DCAL selects samples for human annotation by considering both the classifier's uncertainty and the representation of samples in the unlabeled dataset. This is achieved by training both cell and label classifiers and adaptively combining their scores with a user-initialized weight \(w\) (**Methods**; Appendix B.3). As the number of samples increases, the weight is adjusted based on the accuracy of the label classifiers. Specifically, DCAL gradually approximates CAL, since the label classifiers tend to become random predictors due to the growing diversity of annotated samples. Meanwhile, the DAL component is crucial in the early stages to identify outlier data points (Fig. S7**A**). Thanks to the adaptive estimation process, DCAL depends minimally on the initial choice of the trade-off weight (Fig. S7**B**), and is thereby practically parameter free.

### Geometrical interpretation of discriminative-confidence queries

The query algorithms we consider in this work - random, CAL, DAL, and DCAL - have simple geometric interpretations, which we discuss now using an example movie from our benchmark (Figs. 3 and S8). To illustrate the sample selection preferences by these query algorithms, we reduced the dimension of the feature space from 76 down to 2 using a partial-least squares analysis and plotted in Figs. 3**A** and S8.

Both random and DAL queries comprised of diverse samples covering the full spectrum, with the latter showing increased diversity as expected. As a consequence, since there are for more true positive samples in the full dataset, both of these approaches picked mostly real cells up for annotation (Fig. 3**A** and S8; random and DAL). In contrast, CAL primarily picked cell candidates that are close to the decision boundary, leading to a more balanced selection of true and false positive annotations (Fig. 3**A** and S8; CAL). Yet, especially for sparse annotations, CAL had a preference to sample from higher density regions of the boundary (Fig. S8; CAL with 1%). DCAL, on the other hand, was practically equivalent to CAL in large annotation limit, but also mitigated the dense sampling with sparse annotation (Fig. 3**A** and S8; DCAL). In other words, DCAL picked _outlier_ boundary cells, providing a comprehensive coverage across the boundary.

Next, we quantified these observations by focusing on a subset of the dataset, which we termed "boundary samples" (**Methods**; Appendix D.4). Here, we confirmed that both CAL and DCAL selected a high percentage of boundary samples (Fig. 3**A, B**). These selections preferentially includedsamples that were challenging for human annotators to agree on (Fig. 3**C**). In selecting these samples, DCAL has shown significant improvements over DAL (Fig. 3**B,C**), which was expected since the latter has no relevant incentive. Notably, DCAL also matched the performance of CAL in picking boundary samples, an algorithm solely designed to do so, and had improved diversity in selected boundary cells (Fig. 3**A, D**). Thereby, these results jointly confirmed that DCAL preferentially picked samples lying on the decision boundary and those further from previously picked samples, _leading to broader coverage and better representation of the boundary_ (Figs. 3 and S8).

### Performance evaluations of ActSort on the human annotated benchmarks

Though automated cell sorting is a necessity, work to date has not performed a clear validation of such approaches in large scale benchmarks. Similarly, though active learning query algorithms may provide improvements, existing approaches to date have focused on simple random sampling strategies. To perform the first test of this paradigm, we ran several experiments on our benchmarks using the cross-validated evaluation process (Fig. S6**A**). Specifically, we tested the cell classifiers and the active learning query algorithms (random, CAL, DAL, and DCAL) with closed-loop simulated experiments, which mimicked the real-time annotation that ActSort aims to facilitate (Fig. 4**A**).

As a first test, we processed individual Ca\({}^{2+}\) imaging movies covering hemisphere from three distinct mice (Table S1 and Fig. 4**B-D**). With as little as \(5\%\) annotations, DCAL achieved human level true negative rates (Fig. 4**D**), whereas all algorithms except DAL and random sampling had an above human level true positive rate of \( 95\%\) with \( 1\%\) annotated samples (Fig. 4**C**). As expected, DCAL's performance was mostly independent from the chosen weight due to the adaptive estimation process (Appendix B.3), but also distinct (and better) compared to either DAL or CAL. Notably,

Figure 3: **Discriminative-confidence active learning (DCAL) algorithm selects outlier boundary samples.****A** Dimension reduction via supervised partial least squares (PLS) was applied to the 9,983 cell candidates from a representative annotation of one hemisphere dataset. For each query algorithm, we sorted up to 3% cell candidates and visualized them in the PLS-reduced dimensions computed from the majority-voted held-out labels. Blue dots, cells (as annotated by one human); red dots, false positives; dashed black lines, (approximate) decision boundaries in the reduced feature space. The fraction next to DCAL indicates its user-defined initial weight. **B** The average percentage of the selected samples near the decision boundary (probabilities within \([0.25,0.75]\), **Method**; Appendix D.4) when sorting 3% of cell candidates with each algorithm. **C** The fraction of cell candidate types, based on annotators’ votes (0:4/4:0, 1:3/3:1, and 2:2), selected by each query algorithm normalized with respect to the total number in that particular confidence pool. **D** The average cosine distance between standardized features of boundary samples, the same samples that lie within the decision boundary in **(B)**. In **(B-D)**, each dot represents a single annotation instance. Error bars: s.e.m. over 12 annotations. All tests are two-sided Wilcoxon signed-rank tests with Bonferroni-Holm corrections (\({}^{***}p<10^{-3}\), \({}^{**}p<10^{-2}\), \({}^{*}p<0.05\)).

ActSort with DCAL reached human-level balanced accuracy with annotating roughly \(3\%\) of the full dataset and converged within \(<10\%\) beyond these levels, _even though the classifiers were trained on human outputs_ (Fig. 4**B**). In comparison, random sampling reached the same marks with roughly \(10\%\) and \(50\%\) annotated samples, respectively (Fig. 4**B-D**).

Next, we considered processing cell candidates from multiple animals in a single combined batch (Fig. 4**E-G**). To test this scenario, we augmented 64 new annotators and corresponding evaluators by selecting one annotator from each dataset (Fig. S6**B**). Consequently, each combined dataset contained slightly more than 28,000 samples across three mice. In this experiment, with \(<1\%\) annotated instances, DCAL reached human-level balanced accuracy, and had minimal improvements beyond roughly \(3\%\) annotated samples (Fig. 4**E**). Surprisingly, both DAL and CAL massively underperformed DCAL, offering little improvement over random sampling (Fig. 4**E-G**). These results underscore the

Figure 4: **ActSort converges rapidly (at \(5-10\%\) data) and outperforms human annotators (at \(1-3\%\)). We simulated online annotation scenarios using the hemisphere datasets from three mice with 12 associated human annotations (Table S1), sorting up to 50% of cell candidates. A An illustration of the ActSort workflow. (B) balanced accuracies, (C) true positive rates, and (D) true negative rates as a function of the annotation percentages. Solid lines: means. Shaded areas: s.e.m. over 12 annotators. Dashed line and gray area: mean and s.e.m. from 12 human annotations. See Table S6 and Fig. S13**B** for additional details. **E-G** We re-ran the experiments in (B-D), but processed multiple imaging datasets simultaneously. Dashed line and gray area: mean and s.e.m. from 64 augmented human annotations. See Table S7 and Fig. S13**C** for additional details.

importance of broader coverage and better representation of the decision space, especially when the annotation dataset contains diverse cell candidates sampled from multiple imaging sessions.

The hemisphere-wide movies we considered so far can be considered as high quality cell extraction results, with false-positive rates already as low as 5-10% per movie and high quality imaging and experimental conditions. Next, we asked whether ActSort would be able to accurately process lower quality imaging datasets. To test this, we processed one of the cortex-wide movies from  with less stringent quality parameters in EXTRACT such that the resulting cell extraction output had many false-positives, around 35%, by design (Fig. S9). We also tested ActSort on a two-photon Ca\({}^{2+}\) imaging movie with residual motion (Fig. S10). In both datasets, DCAL outperformed all other approaches. Hence, the low false positive rate in the cell candidates or the optimal imaging/processing conditions cannot explain the success of ActSort in converging rapidly with few annotated samples. We further confirmed that these results were robust to changes in hyperparameters, such as the regularization parameter (Fig. S11) and classifier thresholds (Fig. S12).

As a test of ActSort's generalization ability, we conducted additional experiments with a set of pre-labeled samples using the three Ca\({}^{2+}\) imaging movies of the hemisphere dataset. Specifically, we pre-trained the cell classifier on one dataset using 50% of the pre-labeled samples. This classifier was later fine-tuned using the real-time annotations on a new dataset from a different mouse. To test this pre-labeling approach, we paired \(3\) movies with each other, creating six dataset pairs. Each pair had 16 augmented annotations, pre-labels came from one of four annotators of the first movie,

Figure 5: **ActSort pre-trained with previously annotated datasets converges faster on new animals.****A** To test ActSort’s generalization capacity, we trained cell classifiers on half of the cell candidates from one mouse (representing pre-annotated datasets) and fine-tuned on samples from another mouse (representing the new dataset that needs to be annotated) using the hemisphere dataset. The GUI displays the current dataset, whereas the active learning module trains the cell classifiers using both newly- and pre-annotated data in the background. The query algorithm then selects the next candidates for human annotation, facilitating fine-tuning to the dataset of interest (**Method**; Appendix D.5). **B** - **D** We created 6 pairs of mice, involving 16 augmented annotations per pair, totaling 56,020 cell candidates and 224,080 annotations. The first dataset is sorted up to 50% of cell candidates. Using the same query algorithm, the second dataset is sorted up to 20%. We evaluated various query strategies - random, CAL, DAL, and DCAL - by computing (**B**) balanced accuracies, (**C**) true positive rates, and (**D**) true negative rates by averaging over the 96 data-annotation pairs. Solid lines: means. Shaded areas: s.e.m. over 96 augmented annotations pairs. Dashed line and gray area: mean and s.e.m. from human annotations. See Table S8 and Fig. S13**D** for additional details.

fine-tuning was performed using one of four annotators of the second movie (Fig. 5**A**). The details of the fine-tuning process can be found in Algorithm S2 and Appendix D.5.

Our results, illustrated in Fig. 5, demonstrate the effectiveness of combining pre-labeled samples with DCAL. Notably, with pre-labels, DCAL achieved a human-level balanced accuracy with as little as \(1\%\) annotated samples and converged with roughly \(<6\%\) annotations (Fig. 5**B**). In contrast, random sampling did not achieve human-level performance until c.a. \(8\%\) annotations and had not converged even with the \(20\%\) samples (Fig. 5**B**). Similar to before, we found that DAL had similar performance to random sampling in picking true positives (Fig. 5**C**), for which CAL was the better algorithm. However, DAL outperformed CAL in the true negative rates (Fig. 5**D**). As their combination, DCAL often outperformed or matched the accuracies of both of these approaches, highlighting its robustness and effectiveness in sorting cells under varying conditions (Fig. 5).

In Fig. S13, we provide additional metrics from our analyses performed in the main text. Finally, in Appendix A.3 and Fig. S14, we showcase how to apply ActSort to datasets processed with another cell extraction algorithm utilizing independent component analysis  and that cell sorting is necessary to cull out false positive signals that may correlate with animals' behavior.

## 3 Discussion

ActSort is as a standalone quality control software, which can be used to probe the outputs of _cell extraction pipelines_ such as EXTRACT , Suite2p , ICA , CAIMAN , and others [11; 19; 53; 54]. These cell extraction algorithms take the raw \(^{2+}\) movie as their inputs, correct the brain motion, perform simple transformations to standardize the \(^{2+}\) imaging movies, identify putative cells' spatial profiles and temporal activities, and often perform primitive quality controls to output the final set of neural activities. Historically, additional quality controls on the cell extraction outputs would be performed with manual annotation, which was feasible for the small neural recordings with up to thousands of neurons [20; 23; 24; 25; 26; 27]. Yet, with the advent of large scale \(^{2+}\) imaging techniques, now recording up to one million cells , manual review is no longer realistic. Instead, the field of experimental neuroscience direly needs automated quality control mechanisms that would correctly identify the true cell candidates while rejecting false positives misidentified by the extraction algorithms.

ActSort is the first scalable and generalizable solution in this direction. Yet, previous works (as parts of existing cell extraction pipelines [10; 12; 13]) had tackled this problem in specific instances: Suite2p designed cell classifiers based on some basic features to increase the precision of the algorithm , CAIMAN pre-trained a deep classifier for two-photon movies  (though not applicable to 1p \(^{2+}\) imaging movies ), whereas EXTRACT performed thresholding on a set of quality metrics . Notably, these existing automated methods with pre-trained cell classifiers often found success only for high quality 2p \(^{2+}\) imaging movies [10; 12], and even then underperformed human annotators . One-photon \(^{2+}\) imaging datasets, on the other hand, are quite diverse in their imaging (miniscope vs mesoscope) and experimental (head-fixed vs freely behaving) conditions and face additional challenges due to low resolution and high background noise. With ActSort, we sought a generalizable solution that does not target a specific modality or require substantial re-training, but uses interpretable features that are robust across various behavioral experiments, \(^{2+}\) indicators, imaging conditions, and techniques.

To provide a baseline for existing methods in our benchmarks, we designed a feature set called "traditional features" (Fig. 2), including features used by classifiers in prior works (See Appendix B.1). These methods did not use active learning, instead annotating random subsets. In our experiments, these prior methods are represented as the random sampling query algorithm, using the full feature set for fair comparisons with active learning approaches. In all benchmarking experiments, active learning (specifically DCAL) clearly outperformed random sampling, as is traditionally done in the literature [20; 44]. Notably, classifier predictions showed significantly lower variability compared to human annotations despite being trained on them (see, _e.g._, the differences between gray and colored shaded regions in Figs. 4 and 5). These observations suggest that automation not only reduces human labor but also standardizes it.

Though we performed extensive analysis to highlight the efficiency and effectiveness of ActSort, our work is merely a first step for what may hopefully become a fruitful collaborative subfield comprising of experimental neuroscientists and active learning researchers. There are several future directions that future work could improve upon, which we will briefly summarize below.

Firstly, in this work, we used linear classifiers for rapid online training during cell sorting and reproducibility of annotation results. This was mainly rooted in the fact that pre-training deep networks required substantial data and standardization across various Ca\({}^{2+}\) imaging movies. We believe that with additional public datasets that may follow our lead, this direction can become reality (as was the case for a different, yet relevant, problem of spike extraction ). Our results in this work have set a strong baseline for such future deep-learning approaches.

One limitation of ActSort is that it functions as a quality control pipeline to existing cell extraction approaches. Therefore, any mistakes in the cell extraction step would automatically propagate to cell sorting. Yet, some of these mistakes can be mitigated or highlighted post hoc. For instance, future ActSort versions could involve options for merging segmented or duplicated cells, or identifying motion in activity traces and thereby notifying the user to improve their preprocessing steps.

Another important aspect requiring further exploration is the expertise level of the annotators. To date, each Ca\({}^{2+}\) imaging movie is often annotated by a single annotator, who unfortunately can tire after long hours and become inconsistent as we have discussed above. This was the reason behind our choice to have the movies in our benchmark be annotated by multiple researchers. Yet, the human annotators had different levels of expertise in working with Ca\({}^{2+}\) imaging datasets. Hence, a potential moderation relationship can exist depending on the expertise of the annotator. To fully explore this relationship requires further research with _more_ annotators per dataset, by having the same annotators sort the same cells in different times, and/or testing sorting effectiveness before and after a standardized sorting training.

Finally, we validated ActSort for one-photon and two-photon Ca\({}^{2+}\) imaging datasets and binary class sorting. Yet, our framework is generally applicable to other modalities, barring additional feature engineering performed by domain experts in those fields, or with a pre-trained deep network. For instance, by replacing the logistic regression with a multinomial version and approximating CAL scores with entropy instead of decoder scores, our work readily applies to multi-class datasets that may jointly include, _e.g._, dendritic and somatic activities. With the right features, our framework could potentially support emerging voltage imaging recordings, especially as the number of cells will inevitably increase in such movies with the technological advances.

Overall, while human annotation was possible in datasets discussed in this work due to the manageable number of cell candidates, data analysis and hypothesis testing at scale with millions of neurons will require robust and automated cell sorting tools. In such cases, ActSort, and tools that will follow its lead, are essential for filtering out non-neuronal artifacts that could otherwise compromise the accuracy and reproducibility of neuroscience research.

## 4 Conclusion

In this work, we introduced ActSort, a quality control algorithm for processing large scale Ca\({}^{2+}\) imaging datasets. We utilized real-time human feedback via a closed-loop active learning paradigm to decrease the need for human annotation by two-orders of magnitude. We achieved this by first engineering features that generalize across experimental and imaging conditions, and by developing a simple, yet effective, active learning query algorithm. Finally, to support the development of active learning algorithms in systems neuroscience, we introduced the first publicly available benchmark for cell extraction quality control on both one- and two-photon large-scale Ca\({}^{2+}\) imaging, comprising five datasets: four one-photon and one two-photon Ca\({}^{2+}\) imaging datasets, with approximately 40,000 cells and 160,000 annotations.

The potential impact of this work is two-fold: i) decreasing the number of hours humans spend annotating modern Ca\({}^{2+}\) imaging data sets, and ii) increasing the accuracy of cell classification by mitigating the annotator biases. A paradigm requiring that merely 1-5% of cells be annotated by a human can reduce cell sorting from several hours per Ca\({}^{2+}\) imaging data set, to merely tens of minutes. Through ActSort, researchers can reclaim tens of hours that would have otherwise been spent labeling modern massive data sets. Importantly, increasing cell detection accuracy can be largely beneficial to modern systems neuroscience, which relies on minimizing false positive cell identification to avoid spurious results. Thus, removing the human element from repetitive tasks that can be prone to errors can ultimately improve the reliability, robustness, and reproducibility of systems neuroscience research.

## Data and code availability

ActSort software and the benchmark used in this study are publicly at our Github repository: https://github.com/schnitzer-lab/ActSort-public.