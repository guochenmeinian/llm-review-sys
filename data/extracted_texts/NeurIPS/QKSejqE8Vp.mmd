# An Optimal and Scalable Matrix Mechanism for Noisy Marginals under Convex Loss Functions

Yingtai Xiao

Penn State University

yxx5224@psu.edu

&Guanlin He

Penn State University

gbh5146@psu.edu

&Danfeng Zhang

Penn State University

dbz5017@psu.edu

&Daniel Kifer

Penn State University

duk17@psu.edu

###### Abstract

Noisy marginals are a common form of confidentiality-protecting data release and are useful for many downstream tasks such as contingency table analysis, construction of Bayesian networks, and even synthetic data generation. Privacy mechanisms that provide unbiased noisy answers to linear queries (such as marginals) are known as matrix mechanisms.

We propose ResidualPlanner, a matrix mechanism for marginals with Gaussian noise that is both optimal and scalable. ResidualPlanner can optimize for many loss functions that can be written as a convex function of marginal variances (prior work was restricted to just one predefined objective function). ResidualPlanner can optimize the accuracy of marginals in large scale settings in seconds, even when the previous state of the art (HDMM) runs out of memory. It even runs on datasets with 100 attributes in a couple of minutes. Furthermore ResidualPlanner can efficiently compute variance/covariance values for each marginal (prior methods quickly run out of memory, even for relatively small datasets).

## 1 Introduction

Marginals are tables of counts on a set of attributes (e.g., how many people there are for each combination of race and gender). They are one of the most common formats for the dissemination of statistical data , studying correlations between attributes, and are sufficient statistics for loglinear models, including Bayesian networks and Markov random fields. For this reason, a lot of work in the differential privacy literature has considered how to produce a set of noisy marginals that is both privacy-preserving and accurate.

One line of work, called the _matrix mechanism_ designs algorithms for answering linear queries (such as marginals) so that the privacy-preserving noisy answers are accurate, unbiased, and have a simple distribution (e.g., multivariate normal). These crucial properties allow statisticians to work with the data, model error due to data collection (sampling error) and error due to privacy protections. It enables valid confidence intervals and hypothesis tests and other methods for quantifying the uncertainty of a statistical analysis (e.g,. ). Incidentally, sets of noisy marginals are also used to generate differentially private synthetic data (e.g., ).

For the case of marginals, significant effort has been spent in designing optimal or nearly optimal matrix mechanisms for just a single objective function (total variance of all the desired marginals)  and each new objective function requires significant additional effort . However, existing optimal solutions do not scale and additional effort is needed to design scalable, but suboptimal, matrix mechanisms for marginals . This is because prior work used mathematical properties specific to their chosen objective function in order to improve runtime. Furthermore, computing the individual variances of the desired noisy marginals is a slow process and more difficult is computing the covariance between cells in the same marginal.

Contributions.Our paper addresses these problems with a novel matrix mechanisms called ResidualPlanner. It can optimize for a wide variety of convex objective functions and return solutions that are guaranteed to be optimal under Gaussian noise. It is highly scalable - running in seconds even when other scalable algorithms run out of memory. It also efficiently returns the variance and covariances of each cell of the desired marginals. It leverages the following insights. Since a dataset can be represented as a vector \(\) of counts, and since a marginal query on a set \(\) of attributes can be represented as a matrix \(}\) (with \(}\) being the true answer to the marginal query), we find a new linearly independent basis that can parsimoniously represent both a marginal \(}\) and the "difference" between two marginals \(}\) and \(}}\) (subspace spanned by the rows of \(}\) that is orthogonal to the rows of \(}}\)). Using parsimonious linear bases, instead of overparametrized mechanisms, accounts for the scalability. Optimality results from a deep analysis of the symmetry that marginals impose on the optimal solution - the same linear basis is optimal for a wide variety of loss functions. Our code is available at https://github.com/dkifer/ResidualPlanner.

## 2 Preliminaries

The Kronecker product between a \(k_{1} k_{2}\) matrix \(=[v_{1,1}&&v_{1,k_{2}}\\ &&\\ v_{k_{1},1}&&v_{k_{1},k_{2}}]\) and an \(_{1}_{2}\) matrix \(\), denoted by \(\), is the \(k_{1}_{1} k_{2}_{2}\) matrix that can be represented in block matrix form as: \([v_{1,1}&&v_{1,k_{2}}\\ &&\\ v_{k_{1},1}&&v_{k_{1},k_{2}}]\). A dataset \(=\{r_{1},,r_{n}\}\) is a collection of records (there is exactly one record for each individual in the dataset). Every record \(r_{i}\) contains \(n_{a}\) attributes \(Att_{1},,Att_{n_{a}}\) and each attribute \(Att_{j}\) can take values \(a_{1}^{(j)},,a_{|Att_{j}|}^{(j)}\). An attribute value \(a_{i}^{(j)}\) for attribute \(Att_{j}\) can be represented as a vector using one-hot encoding. Specifically, let \(e_{i}^{(j)}\) be a row vector of size \(|Att_{j}|\) with a 1 in component \(i\) and \(0\) everywhere else. In this way \(e_{i}^{(j)}\) represents the attribute value \(a_{i}^{(j)}\). A record \(r\) with attributes \(Att_{1}=a_{i_{1}}^{(1)}\), \(Att_{2}=a_{i_{2}}^{(2)},,Att_{n_{a}}=a_{i_{n_{a}}}^{(n_{a})}\) can thus be represented as the Kronecker product \(e_{i_{1}}^{(1)}e_{i_{2}}^{(2)}e_{i_{n_{a}}}^{(n_{a})}\). This vector has a 1 in exactly one position and 0s everywhere else. The position of the 1 is the _index_ of record \(r\). With this notation, a dataset \(\) can be represented as a vector \(\) of integers. The value at index \(i\) is the number of times the record associated with index \(i\) appears in \(\). The number of components in this vector is denoted as \(d=_{i=1}^{n_{a}}|Att_{i}|\). Given a subset \(\) of attributes, a _marginal query_ on \(\) is a table of counts: for each combination of values for the attributes in \(\), it provides the number of records in \(\) having those attribute value combinations. The marginal query can be represented as a Kronecker product \(}=_{1}_{n_{a}}\) where \(_{i}\) is the row vector of all ones (i.e. \(_{|Att_{i}|}^{T}\)) if \(Att_{i}\) and \(_{i}\) is the identity matrix \(_{|Att_{i}|}\) if \(Att_{i}\). The answer to the marginal query is obtained by evaluating the matrix-vector product \(}\). For convenience, the notation introduced in this paper is summarized as a table in Section A in the supplementary material.

Example 2.1: _As a running example, consider a dataset in which there are two attributes: \(Att_{1}\) with values "yes" and "no", and \(Att_{2}\) with values "low", "med", "high". The record (no, med) is represented by the kron product \([0&1][ 0&1&0]\) and the marginal query on the set \(=\{Att_{1}\}\) is represented as \(_{\{Att_{1}\}}=[1&0\\ 0&1][1&1&1]\). Similarly, the marginal on attribute \(Att_{2}\) is represented as \(_{\{Att_{2}\}}=[1&1] [1&0&0\\ 0&1&0]\). The query representing all one-way marginals is obtained by stacking them: \(^{}=[_{\{Att_{1} \}}\\ _{\{Att_{2}\}}]\) and \(^{}\) consists of the five query answers (number of records with \(Att_{1}=yes\), number with \(Att_{1}=no\), number with \(Att_{2}=\)low, etc.)._

### Differential Privacy

A mechanism \(\) is an algorithm whose input is a dataset and whose output provides privacy protections. Differential privacy is a family of privacy definitions that guide the behavior of mechanisms so that they can inject enough noise to mask the effects of any individual. There are many versions of differential privacy that support Gaussian noise, including approximate DP, zCDP, and Gaussian DP.

Definition 2.2 (Differential Privacy).: _Let \(\) be a mechanism. For every pair of datasets \(_{1},_{2}\) that differ on the presence/absence of a single record and for all (measurable) sets \(S()\),_

* _If_ \(P((_{1}) S) e^{}P(( _{2}) S)+\) _then_ \(\) _satisfies_ \((,)\)_-approximate differential privacy_ _[_17_]__;_
* _If_ \(^{-1}(P((_{1}) S))^{-1}(P(( _{2}) S))+\)_, where_ \(\) _is the cdf of the standard Gaussian distribution, then_ \(\) _satisfies_ \(\)_-Gaussian DP_ _[_15_]__._
* _If the Renyi divergence_ \(D_{}((_{1})||(_{2}))\) _between the output distributions of_ \((_{1})\) _and_ \((_{2})\) _satisfies_ \(D_{}((_{1})||(_{2}))\) _for all_ \(>1\)_, then_ \(\) _satisfies_ \(\)_-cDP_ _[_7_]__._

Queries that are linear functions of the data vector \(\) can be answered privately using the _linear Gaussian mechanism_, which adds correlated Gaussian noise to a linear function of \(\), as follows.

Definition 2.3 (Linear Gaussian Mechanism ).: _Given a \(m d\) matrix \(\) and \(m m\) covariance matrix \(\), the (correlated) linear Gaussian mechanism \(\) is defined as \(()=+N(,)\). The privacy cost matrix of \(\) is defined as \(^{T}^{-1}\). The privacy cost of \(\), denoted by \(pcost()\), is the largest diagonal of the privacy cost matrix and is used to compute the privacy parameters: \(\) satisfies \(\)-cDP with \(=pcost()/2\), satisfies \((,)\)-approximate DP with \(=()}/2-/)})-e ^{}(-)}/2-/ )})\) (this is an increasing function of \(pcost()\)), and satisfies \(\)-Gaussian DP with \(=)}\)._

The use of a non-diagonal covariance matrix is crucial because it will help simplify the description of the optimal choices of \(\) and \(\). In particular, using non-diagonal covariance allows us to provide explicit formulas for the entries of the \(\) matrices. We note that an algorithm \(^{*}\) that releases the outputs of multiple linear Gaussian mechanisms \(_{1},,_{k}\) (with \(_{i}()=_{i}+N(,_{i})\) ) is again a linear Gaussian mechanism. It is represented as \(^{*}()=^{*}+N(,^{*})\) with the matrix \(^{*}\) obtained by vertically stacking the \(_{i}\) and with covariance \(^{*}\) being a block-diagonal matrix where the blocks are the \(_{i}\). Its privacy cost \(pcost(^{*})=pcost(_{1},,_{k})\) is the largest diagonal entry of \(_{i=1}^{k}_{i}^{T}_{i}^{-1}_{i}\).

### Matrix Mechanism

The Matrix Mechanism  is a framework for providing unbiased privacy-preserving answers to a workload of linear queries, represented by a matrix \(\) (so that the true non-private answer to the workload queries is \(\)). The matrix mechanism framework consists of 3 steps: _select_, _measure_, and _reconstruct_. The purpose of the _select_ phase is to determine _what_ we add noise to and _how much_ noise to use. More formally, when a user's preferred noise distribution is Gaussian, the select phase chooses a Gaussian linear mechanism \(()+N(,)\) whose noisy output can be used to estimate the true query answer \(\). Ideally, \(\) uses the least amount of noise subject to privacy constraints (specified by a privacy definition and settings of its privacy parameters). The _measure_ phase runs the mechanism on the data to produce (noisy) privacy-preserving outputs \(=()\). The _reconstruct_ step uses \(\) to compute an unbiased estimate of \(\). The unbiased estimate is typically \((^{T}^{-1})^{} ^{T}^{-1}\), where \(\) represents the Moore-Penrose pseudo-inverse. This is the best linear unbiased estimate of \(\) that can be obtained from \(\). This means that the goal of the select step is to optimize the choice of \(\) and \(\) so that the reconstructed answer is as accurate as possible, subject to privacy constraints. Ideally, a user would specify their accuracy requirements using a loss function, but existing matrix mechanisms do not allow this flexibility - they hard-code the loss function. In fact, adding support for new loss function used to require significant research and new optimization algorithms  because each new algorithm was customized to specific properties of a chosen loss function. On top of this, existing optimal matrix mechanism algorithms do not scale, while scalable matrix mechanisms are not guaranteed to produce optimal solutions . Additionally, the reconstruction phase should also compute the variance of each workload answer. The variances are the diagonals of \((^{T}^{-1})^{}^{T}\) and making this computation scale is also challenging.

## 3 Additional Related Work

The marginal release mechanism by Barak et al.  predates the matrix mechanism [32; 52; 30; 53; 13; 43; 37; 51; 46; 18; 42; 38] and adds noise to the Fourier decomposition of marginals. We add noise to a _different_ basis, resulting in the scalability and optimality properties. The SVD bound  is a lower bound on total matrix mechanism error when the loss function is the sum of variances. This lower bound is tight for marginals and we use it as a sanity check for our results and implementation (note ResidualPlanner provides optimal solutions even when the SVD bound is infeasible to compute).

Alternative approaches to the matrix mechanism can produce privacy preserving marginal query answers that reduce variance by adding bias. This is often done by generating differentially private synthetic data or other such data synopses from which marginals can be computed. State-of-the art approaches iteratively ask queries and fit synthetic data to the resulting answers [22; 34; 4; 19; 39; 35; 44; 56]. For such mechanisms, it is difficult to estimate error of a query answer but recently AIM  has made progress in upper bounding the error. PGM  provides a connection between the matrix mechanism and this line of work, as it can postprocess noisy marginals into synthetic data. It is a better alternative to sampling a synthetic dataset from models fit to carefully chosen marginals [54; 11; 55; 10]. Synthetic data for answering marginal queries can also be created from random projections , copulas [33; 3], and deep generative models [23; 1; 35].

With respect to the matrix mechanism, the reconstruction step is often one of the bottlenecks to scalability. While PGM  provides one solution, another proposal by McKenna et al.  is to further improve scalability by sacrificing some consistency (the answers to two different marginals may provide conflicting answers to submarginals they have in common). Work on differential privacy marginals has also seen extensions to hierarchical datasets, in which records form meaningful groups that need to be queried. That is, in addition to marginals on characteristics of people, marginals can be computed in different hierarchies such as geographic level (state, county, etc.) and marginals on household composition (or other groupings of people) [2; 28; 36].

## 4 ResidualPlanner

ResidualPlanner is our proposed matrix mechanism for optimizing the accuracy of marginal queries with Gaussian noise. It is optimal and more scalable than existing approaches. It supports optimizing the accuracy of marginals under a wide variety of loss functions and provides exact variances/covariances of the noisy marginals in closed-form. In this section, we first explain the loss functions it supports. We then describe the base mechanisms it uses to answer marginal queries. We next show how to reconstruct the marginal queries from the outputs of the base mechanisms and how to compute their variances in closed form. We then explain how to optimize these base mechanisms for different loss functions. The reason this selection step is presented last is because it depends on the closed form variance calculations. Then we analyze computational complexity. To aid in understanding, we added a complete numerical run-through of the steps in Section B of the supplementary material. All of our proofs also appear in the supplementary material.

### Loss Functions Supported by ResidualPlanner

The loss functions we consider are generalizations of the sum of variances and max of variances used in prior work. Our more general class of loss functions is based on the following principle: different marginals can have different relative importance but within a marginal, its cells are equally important. That is, a loss function can express that the two-way marginal on the attribute set {Race, Marital Status} is more important (i.e., requires more accuracy) than the 1-way marginal on {EducationLevel}, but all cells within the {Race, MaritalStatus} marginal are equally important. This is a commonly accepted principle for answering differentially private marginal queries (e.g., [32; 52; 30; 53; 37; 51; 46; 18; 42; 39; 4; 34]) and is certainly true for the 2020 Census redistricting data .

Let \(Wkload=\{_{1},,_{k}\}\) be a workload of marginals, where each \(_{i}\) is a subset of attributes and represents a marginal. E.g., \(Wkload=\{\{\)Race, MaritalStatus\(\}\), {EducationLevel\(\}\}\) consists of 2 marginals, a two-way marginal on Race/MaritalStatus, and a one-way marginal on Education. Let \(\) be a Gaussian linear mechanism whose output can be used to reconstruct unbiased answers to the marginals in \(Wkload\). For each \(_{i} Wkload\), let \(Var(_{i};)\) be the function that returns thevariances of the reconstructed answers to the marginal on \(_{i}\); the output of \(Var(_{i};)\) is a vector \(v_{i}\) with one component for each cell of the marginal on \(_{i}\). A loss function \(\) aggregates all of these vectors together: \((v_{1},,v_{k})\). We have the following regularity conditions on the loss function.

**Definition 4.1** (Regular Loss Function).: _We say the loss function \(\) is regular if: (1) \(\) is convex and continuous; (2) \((v_{1},,v_{k})\) is minimized when all the \(v_{i}\) are the 0 vectors; and (3) for any \(i\), permuting just the components of \(v_{i}\) does not affect the value of \((v_{1},,v_{k})\). This latter condition just says that cells within the same marginal are equally important._

Loss functions used on prior work are all regular. For example, weighted sum of variances [32; 52; 30; 53; 37; 51] can be expressed as \((v_{1},,v_{k})=_{i}c_{i}^{T}v_{i}\), where the \(c_{i}\) are the nonnegative weights that indicate the relative importance of the different marginals. Another popular loss function is maximum (weighted) variance [46; 18; 42], expressed as \((v_{1},,v_{k})=\{)}{c_{1}},, )}{c_{k}}\}\). Thus, the optimization problem that the selection step needs to solve is either privacy constrained: minimize loss while keeping privacy cost (defined at the end of Section 2.1) below a threshold \(\); or utility constrained: minimize privacy cost such that the loss is at most \(\).

\[_{}( Var(A_{1};),,Var(A_{k};)) pcost()\] (1) \[_{}pcost( )(Var(A_{1};),, Var(A_{k};))\] (2)

We note that regular loss functions cover other interesting cases. For instance, suppose Alicia, Bob, and Carol wish to minimize the sum of variances on their own separate workloads. Taking the max over these three sum-of-variances as the loss function allows the data curator to minimize the largest unhappiness among the three data stakeholders.

### Base Mechanisms used by ResidualPlanner

In the most common application setting, the user picks a privacy budget, a workload of marginals and a loss function \(\). Based on these choices, a matrix mechanism must decide what linear queries to add noise to and how much noise to add to them. Then it uses those noisy measurements to reconstruct answers to the workload queries. In the case of ResidualPlanner, the linear queries that need noise are represented by _base mechanisms_ that are described in this section. Each base mechanism has a scalar noise parameter that determines how much noise it uses (so optimizing the loss function \(\) is equivalent to finding a good value for the noise parameter of each base mechanism). As long as the loss function \(\) is regular, we prove that an optimal mechanism can be constructed from the set of base mechanisms that we describe here.

To begin, we define a _subtraction matrix_\(_{m}\) to be an \((m-1) m\) matrix where the first column is filled with 1, entries of the form \((i,i+1)\) are -1, and all other entries are 0. For example, \(_{3}=[1&-1&0\\ 1&0&-1]\) and \(_{2}=[1&-1&1]\). We use these subtraction matrices to define special matrices called _residual matrices_ that are important for our algorithm.

For any subset \(\{Att_{1},,Att_{n_{a}}\}\) of attributes, we define the _residual matrix_\(_{}\) as the Kronecker product \(_{}=_{1}_{n_{a}}\), where \(_{i}=_{|Att_{i}|}^{T}\) if \(Att_{i}\) and \(_{i}=_{|Att_{i}|}\) if \(Att_{i}\). Continuing Example 2.1, we have \(_{}=[1&1] [1&1]\), and \(_{\{Att_{1}\}}=[1&-1&0\\ 1&0&-1]\), and \(_{\{Att_{2}\}}=[1&-1&0\\ 1&0&-1]\).

Using subtraction matrices, we also define the matrix \(_{}\) as the Kronecker product \(_{Att_{i}}(_{|Att_{i}|}_ {|Att_{i}|}^{T})\) and we note that it is proportional to \(_{}_{}^{T}\). \(_{}\) is defined as 1. Each subset \(\) of attributes can be associated with a "base" mechanism \(_{}\) that takes as input the data vector \(\) and a scalar parameter \(_{}^{2}\) for controlling how noisy the answer is. \(_{}\) is defined as:

\[_{}(;_{}^{2})_{ }+N(,_{}^{2}_{ })\] (3)

The residual matrices \(_{}\) used by base mechanisms form a linearly independent basis that compactly represent marginals, as the next result shows.

**Theorem 4.2**.: _Let \(\) be a set of attributes and let \(_{}\) be the matrix representation of the marginal on \(\). Then the rows of the matrices \(_{^{}}\), for all \(^{}\), form a linearly independent basis of the row space of \(_{}\). Furthermore, if \(^{}^{}\) then \(_{^{}}_{^{}}^{T}= \) (they are mutually orthogonal)._Remark 4.3.: _To build an intuitive understanding of residual matrices, consider again Example 2.1. Both \(_{}\) and \(_{}\) are the sum query (marginal on no attributes). The rows of \(_{\{Att_{1}\}}\) span the subspace of \(_{\{Att_{1}\}}\) that is orthogonal to \(_{}\) (and similarly for \(_{\{Att_{2}\}}\)). The rows of \(_{\{Att_{1},Att_{2}\}}\) span the subspace of \(_{\{Att_{1},Att_{2}\}}\) that is orthogonal to both \(_{\{Att_{1}\}}\) and \(_{\{Att_{2}\}}\). Hence a residual matrix spans the subspace of a marginal that is orthogonal to its sub-marginals._

Theorem 4.2 has several important implications. If we define the downward closure of a marginal workload \(Wkload=\{_{1},,_{k}\}\) as the collection of all subsets of the sets in \(Wkload\) (i.e., \((Wkload)=\{^{}\ :\ ^{}  Wkload\}\)) then the theorem implies that the combined rows from \(\{_{^{}}\ :\ ^{}(Wkload)\}\) forms a linearly independent basis for the marginals in the workload. In other words, it is a linearly independent bases for the space spanned by the rows of the marginal query matrices \(_{}\) for \( Wkload\). Thus, in order to provide privacy-preserving answers to all of the marginals represented in \(Wkload\), we need all the mechanisms \(_{^{}}\) for \(^{}(Wkload)\) - any other matrix mechanism that provides fewer noisy outputs cannot reconstruct unbiased answers to the workload marginals. This is proved in Theorem 4.4, which also states that optimality is achieved by carefully setting the \(_{}\) noise parameter for each \(_{}\).

Theorem 4.4.: _Given a marginal workload \(Wkload\) and a regular loss function \(\), suppose the optimization problem (either Equation 1 or 2) is feasible. Then there exist nonnegative constants \(_{}^{2}\) for each \((Wkload)\) (the constants do not depend on the data), such that the optimal linear Gaussian mechanism \(_{opt}\) for loss function \(\) releases \(_{}(;_{}^{2})\) for all \((Wkload)\). Furthermore, any matrix mechanism for this workload must produce at least this many noise measurements during its selection phase._

```
1\(_{}\)// Evaluate the true marginal
2\(m_{Att_{i}}|Att_{i}|\)\(_{Att_{i}}_{|Att_{i}|}\)// Use implicit representation, don't expand
3\( N(,_{m})\)// independent noise return\(+_{}\)// use kron-product/vector multiplication from  ```

**Algorithm 1**Efficient implementation of \(_{}(;_{}^{2})_{ }+N(,_{}^{2}_{ })\)

\(_{}\) can be evaluated efficiently, directly from the marginal of \(\) on attribute set \(\), as shown in Algorithm 1. It uses the technique from  to perform fast multiplication between a Kronecker product and a vector (so that the Kronecker product does not need to be expanded). It also generates correlated noise from independent Gaussians. The privacy cost \(pcost(_{})\) of each base mechanism \(_{}\) is also easy to compute and is given by the following theorem.

Theorem 4.5.: _The privacy cost of \(_{}\) with noise parameter \(_{}^{2}\) is \(}^{2}}_{Att_{i}}|-1 }{|Att_{i}|}\) and the evaluation of \(_{}\) given in Algorithm 1 is correct - i.e., the output has the distribution \(N(_{},_{}^{2}_{ })\)._

### Reconstruction

Next we explain how to reconstruct unbiased answers to marginal queries from the outputs of the base mechanisms and how to compute (co)variances of the reconstructed marginals efficiently, without any heavy matrix operations (inversion, pseudo-inverses, etc.). Then, given the closed form expressions for marginals and privacy cost (Theorem 4.5), we will be able to explain in Section 4.4 how to optimize the \(_{}^{2}\) parameters of the base mechanisms \(_{}\) to optimize regular loss functions \(\).

Since the base mechanisms were built using a linearly independent basis, reconstruction is unique - just efficiently invert the basis. Hence, unlike PGM and its extensions , our reconstruction algorithm does not need to solve an optimization problem and can reconstruct each marginal independently, thus allowing marginals to be reconstructed in parallel, or as needed by users. The reconstructed marginals are consistent with each other (any two reconstructed marginals agree on their sub-marginals). Just as the subtraction matrices \(_{k}\) were useful in constructing the base mechanisms \(_{}\), their pseudo-inverses \(_{k}^{}\) are useful for reconstructing noisy marginals from the noisy answers of \(_{}\). The pseudo-inverses have a closed form. For example \(_{4}=[1&-1&0&0\\ 1&0&-1&0\\ 1&0&0&-1]\) and \(_{4}^{}=[1&1&1\\ -3&1&1\\ 1&-3&1]\). More generally, they are expressed as follows:

Lemma 4.6.: _For any \(Att_{i}\), let \(=|Att_{i}|\). The matrix \(_{}\) has the following block matrix, with dimensions \((-1)\), as its pseudo-inverse (and right inverse): \(_{}^{}=[1^{ _{-1}^{-1}}\\ 1_{-1}1^{_{-1}^{-1}-_{-1}}]\)._

Each mechanism \(_{}\), for \((Wkload)\), has a noise scale parameter \(_{}^{2}\) and a noisy output that we denote by \(_{}\). After we have obtained the noisy outputs \(_{}\) for all \((Wkload)\), we can proceed with the reconstruction phase. The reconstruction of an unbiased noisy answer for any marginal on an attribute set \((Wkload)\) is obtained using Algorithm 2. We note that to reconstruct a marginal on attribute set \(\), one only needs to use the noisy answers \(_{^{}}\) for \(^{}()\). In other words, if we want to reconstruct a marginal on attribute set \(\{Att_{1},Att_{2}\}\), we only need the outputs of \(_{}\), \(_{\{Att_{1}\}}\), \(_{\{Att_{2}\}}\), and \(_{\{Att_{1},Att_{2}\}}\) no matter how many other attributes are in the data and no matter what other marginals are in the \(Wkload\). We emphasize again, the reconstruction phase does not run the base mechanisms anymore, it is purely post-processing.

``` Input: Noise scale parameters \(_{^{}}^{2}\) and noisy answer vector \(_{^{}}\) of mechanism \(_{^{}}\) for every \(^{}()\). Output:\(\) is output as an unbiased noisy estimate of \(_{}\).
1\(\)
2for each \(^{}()\)do
3\(_{1}_{n_{a}}\), where \(_{i}=_{|Att_{i}|}^{}&Att_{i}^{}\\ |}_{Att_{i}|}&Att_{i}/ ^{}\\ &Att_{i}\)
4\(+_{^{}}\)// use kron-product/vector multiplication from  return\(\) ```

**Algorithm 2**Reconstruct Unbiased Answers to the Marginal on \(\)

Theorem 4.7.: _Given a marginal workload \(Wkload\) and positive numbers \(_{}^{2}\) for each \((Wkload)\), let \(\) be the mechanism that outputs \(\{_{}(;_{}^{2})\ :\ (Wkload)\}\) and let \(\{_{}\ :\ (Wkload)\}\) denote the privacy-preserving noisy answers (e.g., \(_{}=_{}(,^{2})\)). Then for any marginal on an attribute set \((Wkload)\), Algorithm 2 returns the unique linear unbiased estimate of \(_{}\) (i.e., answers to the marginal query) that can be computed from the noisy differentially private answers._

_The variances \(Var(;)\) of all the noisy cell counts of the marginal on \(\) is the vector whose components are all equal to \(_{^{}}(_{^{ }}^{2}_{Att_{i}^{}}|-1}{|Att_{i}|}* _{Att_{j}(/^{})}|^{2}})\). The covariance between any two noisy answers of the marginal on \(\) is \(_{^{}}(_{^{ }}^{2}_{Att_{i}^{}}|}*_{Att_{j} (/^{})}|^{2}})\)._

To see an example of how the choices of the \(_{}^{2}\) affect the variance of different marginals, see Section C in the supplementary material.

### Optimizing the Base Mechanism Selection

We now consider how to find the optimal Gaussian linear mechanism \(^{*}\) that solves the optimization problems in Equations 1 or 2. Given a workload on marginals \(Wkload\), the optimization involves \(Var(;^{*})\) for \( Wkload\) (the variance of the marginal answers reconstructed from the output of \(^{*}\)) and \(pcost(^{*})\), from which the privacy parameters of different flavors of differential privacy can be computed.

Theorem 4.4 says that \(^{*}\) works by releasing \(_{}(;_{}^{2})\) for each \((Wkload)\) for appropriately chosen values of \(_{}^{2}\). The privacy cost \(pcost(^{*})\) is the sum of the privacy costs of the \(_{}\). Theorem 4.5 therefore shows that \(pcost(^{*})\) is a positive linear combination of the values \(1/_{}^{2}\) for \((Wkload)\) and is therefore convex in the \(_{}^{2}\) values. Meanwhile, Theorem 4.7 shows how to represent, for each \((Wkload)\), the quantity \(Var(;^{*})\) as a positive linear combination of \(_{}^{2}\), for \(^{}()(Wkload)\). Therefore, the loss function \(\) is also convex in the \(_{}^{2}\) values.

Thus the optimization problems in Equations 1 and 2 can be written as minimizing a convex function of the \(_{}^{2}\) subject to convex constraints. In fact, in Equation 2, the constraints are linear when the optimization variables represent the \(_{}^{2}\) and in Equation 1 the constraints are linear when the optimization variables represent the \(1/_{}^{2}\). Furthermore, when the loss function is the weighted sum of variances of the marginal cells, the solution can be obtained in closed form (see supplementary material). Otherwise, we use CVXPY/ECOS [12; 14] for solving these convex optimization problems.

### Computational Complexity

Although the universe size \(|Att_{1}||Att_{n_{a}}|\) grows exponentially with the number of attributes, the following theorem shows that the time complexity of ResidualPlanner depends directly on quantities that typically grow polynomially, such as the number of desired marginals and total number of cells in those marginals.

Theorem 4.8.: _Let \(n_{a}\) be the total number of attributes. Let \(\#cells()\) denote the number of cells in the marginal on attribute set \(\). Then:_

1. _Expressing the privacy cost of the optimal mechanism_ \(^{*}\) _as a linear combination of the_ \(1/_{}^{2}\) _values takes_ \(O(_{ Wkload}\#cells())\) _total time._
2. _Expressing all of the_ \(Var(;^{*})\)_, for_ \( Wkload\)_, as a linear combinations of the_ \(_{}^{2}\) _values can be done in_ \(O(_{ Wkload}\#cells())\) _total time._
3. _Computing all the noisy outputs of the optimal mechanism (i.e.,_ \(_{}(;_{}^{2})\) _for_ \((Wkload)\)_) takes_ \(O(n_{a}_{ Wkload}_{Att_{i}}(|Att_{i} |+1))\) _total time after the true answers have been precomputed (Line 1 in Algorithm 1). Note that the total number of cells on marginals in_ \(Wkload\) _is_ \(O(_{ Wkload}_{Att_{i}}|Att_{i}|)\)_._
4. _Reconstructing marginals for all_ \( Wkload\) _takes_ \(O(_{ Wkload}||\#cells()^{2})\) _total time._
5. _Computing the variance of the cells for all of the marginals for_ \( Wkload\) _can be done in_ \(O(_{ Wkload}\#cells())\) _total time._

To get a sense of these numbers, consider a dataset with 20 attributes, each having 3 possible values. If the workload consists of all 3-way marginals, there are 1,140 marginals each with 27 cells so \(n_{cells}=30,780\). The quantity inside the big-O for the selection step is \(1,459,200\) (roughly the number of scalar multiplications needed). These are all easily manageable on modern computers even without GPUs. Our experiments, under more challenging conditions, run in seconds.

## 5 Experiments

We next compare the accuracy and scalability of ResidualPlanner against HDMM , including variations of HDMM with faster reconstruction phases . The hardware used was an Ubuntu 22.04.2 server with 12 Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz processors and 32GB of DDR4 RAM. We use 3 real datasets to evaluate accuracy and 1 synthetic dataset to evaluate scalability. The real datasets are (1) the Adult dataset  with 14 attributes, each having domain sizes \(100,100,100,99,85,42,16,15,9,7,6,5,2,2\), respectively, resulting in a record domain size of \(6.41*10^{17}\); (2) the CPS dataset  with 5 attributes, each having domain size \(100,50,7,4,2\), respectively, resulting in a record domain size of \(2.8*10^{5}\); (3) the Loans dataset  with 12 attributes, each having domain size \(101,101,101,101,3,8,36,6,51,4,5,15\), respectively, resulting in a record domain size of \(8.25*10^{15}\). The synthetic dataset is called Synth-\(n^{d}\). Here \(d\) refers to the number of attributes (we experiment from \(d=2\) to \(d=100\)) and \(n\) is the domain size of each attribute. The running times of the algorithms only depend on \(n\) and \(d\) and not on the records in the synthetic data. For all experiments, we set the privacy cost \(pcost\) to 1, so all mechanisms being compared satisfy \(0.5\)-zCDP and 1-Gaussian DP.

### Scalability of the Selection Phase

We first consider how long each method takes to perform the selection phase (i.e., determine what needs noisy answers and how much noise to use). HDMM can only optimize total variance, which is equivalent to root mean squared error. For ResidualPlanner we consider both RMSE and max variance as objectives (the latter is a harder to solve problem). Each algorithm is run 5 times and the average is taken. Table 1 shows running time results; accuracy results will be presented later.

As we can see, optimizing for max variance is more difficult than for RMSE, but ResidualPlanner does it quickly even for data settings too big for HDMM. The runtime of HDMM increases rapidly, while even for the extreme end of our experiments, ResidualPlanner needs just a few minutes.

### Scalability of the Reconstruction Phase

We next evaluate the scalability of the reconstruction phase under the same settings. The reconstruction speed for ResidualPlanner does not depend on the objective of the selection phase. Here we compare against HDMM  and a version of HDMM with improved reconstruction scalability called HDMM+PGM [38; 41] (the PGM settings used 50 iterations of its Local-Inference estimator, as the default 1000 was too slow). Since HDMM cannot perform the selection phase after a certain point, reconstruction results also become unavailable. Table 2 shows ResidualPlanner is clearly faster.

### Accuracy Comparisons

Since ResidualPlanner is optimal, the purpose of the accuracy comparisons is a sanity check. For RMSE, comparisons of the quality of ResidualPlanner to the theoretically optimal lower bound, known as the SVD bound , can be found in the supplementary material in Section I (ResidualPlanner

  \(d\) & HDMM & ResidualPlanner & ResidualPlanner \\  & RMSE Objective & RMSE Objective & Max Variance Objective \\ 
2 & \(0.013 0.003\) sec & \(0.001 0.0008\) sec & \(0.007 0.001\) sec \\
6 & \(0.065 0.012\) sec & \(0.002 0.0008\) sec & \(0.009 0.001\) sec \\
10 & \(0.639 0.059\) sec & \(0.009 0.001\) sec & \(0.018 0.001\) sec \\
12 & \(4.702 0.315\) sec & \(0.015 0.001\) sec & \(0.028 0.001\) sec \\
14 & \(46.054 12.735\) sec & \(0.025 0.002\) sec & \(0.041 0.001\) sec \\
15 & \(201.485 13.697\) sec & \(0.030 0.017\) sec & \(0.050 0.001\) sec \\
20 & Out of memory & \(0.079 0.017\) sec & \(0.123 0.023\) sec \\
30 & Out of memory & \(0.247 0.019\) sec & \(0.461 0.024\) sec \\
50 & Out of memory & \(1.207 0.047\) sec & \(4.011 0.112\) sec \\
100 & Out of memory & \(9.913 0.246\) sec & \(121.224 3.008\) sec \\  

Table 1: **Time for Selection Step in seconds** on Synth\(-n^{d}\) dataset. \(n=10\) and the number of attributes \(d\) varies. The workload consists of all marginals on \( 3\) attributes each. Times for HDMM are reported with \( 2\) standard deviations.

  \(d\) & HDMM & HDMM + PGM & ResidualPlanner \\ 
2 & \(0.003 0.0006\) sec & \(0.155 0.011\) sec & \(0.005 0.003\) sec \\
6 & \(0.173 0.011\) sec & \(4.088 0.233\) sec & \(0.023 0.004\) sec \\
10 & Out of memory in reconstruction & \(20.340 2.264\) sec & \(0.125 0.032\) sec \\
12 & Out of memory in reconstruction & \(39.162 1.739\) sec & \(0.207 0.004\) sec \\
14 & Out of memory in reconstruction & \(69.975 4.037\) sec & \(0.330 0.006\) sec \\
15 & Out of memory in reconstruction & \(91.101 7.621\) sec & \(0.413 0.006\) sec \\
20 & N/A (select step failed) & N/A (select step failed) & \(1.021 0.011\) sec \\
30 & N/A (select step failed) & N/A (select step failed) & \(3.587 0.053\) sec \\
50 & N/A (select step failed) & N/A (select step failed) & \(17.029 0.212\) sec \\
100 & N/A (select step failed) & N/A (select step failed) & \(154.538 15.045\) sec \\  

Table 2: **Time for Reconstruction Step in seconds** on Synth\(-n^{d}\) dataset. \(n=10\) and the number of attributes \(d\) varies. The workload consists of all marginals on \( 3\) attributes each. Times are reported with \( 2\) standard deviations. Reconstruction can only be performed if the select step completed.

matches the lower bound). We note ResidualPlanner can provide solutions even when the SVD bound is infeasible to compute.

We also compare ResidualPlanner to HDMM when the user is interested in the maximum variance objective. This just shows that it is important to optimize for the user's objective function and that the optimal solution for RMSE (the only objective HDMM can optimize) is not a good general-purpose approximation for other objectives (as shown in Table 3). Additional comparisons are provided in the supplementary material.

## 6 Limitations, Conclusion, and Future Work.

In this paper, we introduced ResidualPlanner, a matrix mechanism that is scalable and optimal for marginals under Gaussian noise, for a large class of convex objective functions. While these are important improvements to the state of the art, there are limitations.

First, for some attributes, a user might not want marginals. For example, they might want range queries or queries with hierarchies (e.g., how many people drive sedans vs. vans; out of the sedans, how many are red vs. green, etc) [2; 28; 36]. In some cases, an attribute might have an infinite domain (e.g., a URL) and need to be handled differently [27; 45]. In other cases, the user may want other noise distributions, like the Laplace. These types of queries do not have the same type of symmetry as marginals that was crucial to proving the optimality of ResidualPlanner. For these situations, one of the key ideas of ResidualPlanner can be used - find a linear basis that compactly represents both the queries and "residual" (information provided by a query that is not contained in the other queries). Such a feature would result in scalability. It is future work to determine how to extend both scalability and optimality to such situations. Another limitation is that this work considers the setting where an individual can contribute exactly one (rather than arbitrarily many) records to the dataset.

## 7 Acknowledgments

This work was supported by NSF grant CNS-1931686 and a gift from Facebook.

   &  &  &  \\  Workload & ResPlan & HDMM & ResPlan & HDMM & ResPlan & HDMM \\ 
1-way Marginals & 12.047 & 41.772 & 4.346 & 13.672 & 10.640 & 33.256 \\
2-way Marginals & 67.802 & 599.843 & 7.897 & 47.741 & 52.217 & 437.478 \\
3-way Marginals & 236.843 & 5675.238 & 7.706 & 71.549 & 156.638 & 3095.997 \\ \( 3\)-way Marginals & 253.605 & 6677.253 & 13.216 & 415.073 & 180.817 & 4317.709 \\  

Table 3: Max Variance Comparisons with ResidualPlanner and HDMM (showing that being restricted to optimizing only RMSE is not a good approximation of Max Variance optimization).