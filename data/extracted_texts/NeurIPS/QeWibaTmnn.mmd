# Grasp as You Say:

Language-guided Dexterous Grasp Generation

 Yi-Lin Wei1, Jian-Jian Jiang1, Chengyi Xing2, Xian-Tuo Tan1,

**Xiao-Ming Wu1, Hao Li2, Mark Cutkosky2, Wei-Shi Zheng1,3**1

1 School of Computer Science and Engineering, Sun Yat-sen University, China

2 Stanford University, USA

3 Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China {weiylin5, jiangjj35, tanxt23, wuxm65}@mail2.sysu.edu.cn {chengyix, li2053, cutkosky}@stanford.edu wszheng@ieee.org https://isee-laboratory.github.io/DexGYS/

###### Abstract

This paper explores a novel task **"Dexterous Grasp as You Say"** (DexGYS), enabling robots to perform dexterous grasping based on human commands expressed in natural language. However, the development of this field is hindered by the lack of datasets with natural human guidance; thus, we propose a language-guided dexterous grasp dataset, named **DexGYSNet**, offering high-quality dexterous grasp annotations along with flexible and fine-grained human language guidance. Our dataset construction is cost-efficient, with the carefully-design hand-object interaction retargeting strategy, and the LLM-assisted language guidance annotation system. Equipped with this dataset, we introduce the **DexGYSGrasp** framework for generating dexterous grasps based on human language instructions, with the capability of producing grasps that are intent-aligned, high quality and diversity. To achieve this capability, our framework decomposes the complex learning process into two manageable progressive objectives and introduce two components to realize them. The first component learns the grasp distribution focusing on intention alignment and generation diversity. And the second component refines the grasp quality while maintaining intention consistency. Extensive experiments are conducted on DexGYSNet and real world environments for validation.

## 1 Introduction

Enabling robots to perform dexterous grasping based on human language instructions is essential within the robotics and deep learning communities, offering promising applications in industrial production and domestic collaboration scenarios.

With the advancements in data-driven deep learning and the availability of large-scale datasets, robot dexterous grasp methods achieve impressive performance . While previous approaches focus on the grasp stability, they have not fully utilized the potential of dexterous hands for intentional, human-like grasping. Recent studies, known as task-oriented  and functional dexterous grasping , aim to generate grasps based on specific tasks or functionality of objects. However, these approaches often depend on predefined, fixed and limited tasks or functions, restricting their flexibility and hindering natural human-robot interaction.

In this paper, we explore a novel task, **"Dexterous Grasp as You Say"** (DexGYS), as shown in Figure 1. We can see that natural human guidance is provided in this task, and can be utilized todrive dexterous grasping generation, thereby facilitating more user-friendly human-robot interactions. However, the new task also brings in new challenges. First, the high costs of annotating dexterous pose and the corresponding language guidance, present a barrier for developing and scaling dexterous datasets. Second, the demands of generating dexterous grasps that ensure intention alignment, high quality and diversity, present considerable challenges to the model learning.

To address the first challenge, we propose a large-scale language-guided dexterous grasping dataset **DexGYSNet**. DexGYSNet is constructed in a cost-effective manner by exploiting human grasp behavior and the extensive capabilities of Large Language Models (LLM). Specially, we introduce the Hand-Object Interaction Retargeting (HOIR) strategy to transfer easily-obtained human hand-object interactions to robotic dexterous hand, to maintain contact consistency and high-quality grasp posture. Subsequently, we develop the LLM-assisted Language Guidance Annotation system to produce flexible and fine-grained language guidance for dexterous grasp data with the support of LLM. DexGYSNet dataset comprises 50,000 pairs of high-quality dexterous grasps and their corresponding language guidance, on 1,800 common household objects.

With the support of the dataset, we now turn our way to overcome the second challenge. We propose the **DexGYSGrasp** framework for dexterous grasp generation, which aligns with intentions, ensures high quality, and maintains diversity. At the beginning, we find the difficulty of mastering all objectives simultaneously results from the commonly used penetration loss  which used to avoid hand-object penetration. As shown in Figure 2, penetration loss substantially hinders the learning of grasp distribution, causing intention misalignment and reduced diversity. Conversely, despite the high diversity and aligned intention, the removal of penetration loss leads to unacceptable object penetration, making the grasp infeasible. Based on this finding, we design our DexGYSGrasp framework in a progressive strategy, decomposing the complex learning task into two sequential objectives managed by progressive components. Initially, the first component learns a grasp distribution, which focuses on intention consistency and diversity, optimizing effectively without the constraints of penetration loss. Subsequently, the second component refines the initial coarse grasps to high-quality ones with the same intentions and diversity. Our framework allows each component to focus on specific and manageable optimization objective, enhancing the overall performance of the generated grasps.

Extensive experiments are conducted on the DexGYSNet dataset and real-world scenarios. The results demonstrate that our methods are capable of generating intention-consistent, high diversity and high quality grasp poses for a wide range of objects.

## 2 Related work

### Dexterous Grasp Generation

Dexterous hand endows robots with the capability to manipulate objects in a human-like manner. Previous methods have achieved impressive results in ensuring grasp stability by analytical approaches [11; 12; 13; 14; 15] and deep learning methods [5; 3; 7; 16; 17; 18]. However, the full potential of dexterous hands for intentional and human-like grasping has not been completely exploited

Figure 1: Our Language-guided Task vs. Traditional Dexterous Grasp Tasks. Traditional methods focus either solely on grasp quality or on fixed and limited functionalities. Our approach enables the generation of dexterous grasps based on human language, enhancing natural human-robot interactions.

in these methods. Recently, some works have focused on functional dexterous grasping [8; 9; 10; 19], aiming to achieve human-like capabilities that extend beyond grasp stability alone, but are still lack of flexibility and generalization. In this work, we explore a novel task, Language-guided Dexterous Grasp Generation, which fully leverages the dexterity of robotic hands and enable robot to execute dexterous grasp based on human natural language.

### Grasp Datasets

The development of large-scale datasets has significantly improve the advancement of data-driven grasp methods, including parallel grasp [20; 21; 22; 23; 24], human grasp [25; 26; 27; 28; 29; 30], and dexterous grasp approaches [1; 2; 8; 14; 16]. Despite these advancements, the high cost of data collection remains a significant challenge, particularly in the domain of dexterous hands. Previous datasets for dexterous grasping primarily rely on physical analysis approaches [12; 31] to mitigate this issue. However, these approaches often lack the specific semantic context or corresponding language guidance necessary for constructing our language-guided dexterous task. In this paper, we present DexGYSNet dataset, with a cost-effective construction, providing high-quality dexterous grasp annotation along with flexible and fine-grained human language guidance.

### Language-guided Robot Grasp

Language-guided robot grasp is important in robotics. Previous works focusing on parallel grippers have made strides in achieving task-oriented grasping [32; 23; 33], language-guided grasping [34; 35] and manipulation [36; 37; 38; 39]. In contrast to parallel grippers, dexterous hand boast a higher number of DOF (e.g., 28 for the Shadow Hand ), enabling a broader dexterity. However, this high freedom also presents challenges for model learning. In this paper, we propose the DexGYSGrasp framework, capable of generating intention-aligned dexterous grasps with high-quality and diversity.

## 3 DexGYSNet Dataset

### Dataset Overview

The DexGYSNet dataset is constructed with a cost-effective strategy, as shown in Figure 3. We first collect object meshes and human grasps data from existing datasets . Subsequently, we develop the Hand-Object Interaction Retargeting (HOIR) strategy to transform human grasps into dexterous grasps with high quality and hand-object interaction consistency. Finally, we implement an LLM-assisted Language Guidance Annotation system, which leverages the knowledge of Large Language Models (LLM) to produce flexible and fine-grained annotations for language guidance.

### Hand-Object Interaction Retargeting

Our Hand-Object Interaction Retargeting (HOIR) aims to transfer human hand-object interaction to dexterous hand-object interaction as shown in Figure 3. The source MANO  hand parameters are denoted as \(^{m} R^{61}\). And the target dexterous hand parameters are denoted as \(^{dex}\) = \((r,t,q)\)

Figure 2: Visualization of the impact of penetration loss (Pen. in the figure) on grasp performance: intention alignment, quality, and diversity. (a) illustrates penetration loss **causes intention misalignment** and its absence results in severe object penetration. (b) shows three sampling results under the same conditions, and demonstrates that penetration loss **leads to reduced diversity**.

where \(r\) represents the global rotation, \(t^{3}\) is the translation in world coordinates, and \(q^{J}\) is the joint angles for a \(J\)-DoF dexterous hand, for example \(J=22\) for Shadow Hand.

Three steps are within the HOIR: pose initialization, fingertip alignment, and interaction refinement. In the first step, the dexterous poses are initialized by copying parameters from similar structures of human poses to establish better initial values. In the second step, the dexterous poses are optimized in the parameter space to align the fingertip positions \(p_{k}^{dex,ft}\) with those of the human \(p_{k}^{nano,ft}\). This achieves retargeting consistency, and the optimization objective can be formulated as follows:

\[_{^{dex}=(r,t,q)}_{k}\|p_{k}^{dex,ft}-p_{k}^{ mano,ft}\|_{2}^{2}.\] (1)

To improve the physical interaction feasibility while maintaining the consistency, the dexterous hand poses are further optimized in the third step by hand-object interaction and physical constraints losses . Two key points are designed to maintain the consistency: preserving the contact area of the optimized pose consistent with the output from the second step, and keeping the translation fixed during this step. The optimization objective can be formulated as follows:

\[_{(r,q)}(_{pen}^{1}_{pen}+_{ spen}^{1}_{spen}+_{joint}^{1}_{joint}+_{ cmap}^{1}_{cmap}).\] (2)

Here, the object penetration loss \(_{pen}\) penalizes the depth of hand-object penetration. The self-penetration loss \(_{spen}\) penalizes the self-penetration. The joint angle loss \(_{joint}\) penalizes the out-of-limit joint angles. The contact map loss \(_{cmap}\) ensures the contact map on the object remains consistent with the output from the second stage. The details of losses can be found in Appendix A.1.5.

### LLM-assisted Language Guidance Annotation

To annotate flexible and fine-grained language guidance for dexterous hand-object pairs with low-cost, we design a coarse-to-fine automated language guidance annotation system with the assistance of the LLM, inspired by [42; 29], as shown in Figure 3. Specially, we initially generate brief guidance based on the object category and the brief human intention (e.g., "using a lotion pump"), which are collected by the human dataset . Subsequently, we compile the contact information for each finger by calculating the distances from the contact anchors on the hand to different parts of the object. We then organize the contact information into language descriptors (e.g. "forefinger touches pump head and other fingers touch the bottle body."). Finally, we input both the brief guidance and the detailed contact information into the GPT3.5 to produce natural annotated guidance (e.g. "To use a lotion pump, press down on the pump head with your forefinger while holding the bottle with your other fingers."). More details about DexGYSNet construction can be found in Appendix A.2.

Figure 3: The construction process of the DexGYSNet dataset. (a) The HOIR strategy retargets the human hand to the dexterous hand by three step, maintaining hand-object interaction consistency and avoiding physical infeasibility (shown in black circle). (b) The annotation system automatically annotates language guidance for hand-object pairs with the help of LLM.

## 4 DexGYSGrasp framework

Given full object point clouds \(\) and language guidance \(\) as inputs, our goal is to generate dexterous grasps \(^{dex}\) with intention alignment, high diversity and high quality.

### Progressive Grasp Objectives.

**Learning Challenge in DexGYS.** The DexGYS places high demands on intention alignment (e.g., accurately pressing your forefinger on trigger to use the sprayer), high diversity (e.g., holding the bottle using various postures), and high quality (e.g., ensuring stable grasp and avoiding object penetration). However, we find that a single model struggles to meet these requirements simultaneously, due to the optimization challenge caused by the commonly used object penetration loss [43; 14; 16], which is used to prevent hand-object penetration. As shown in Figure 2 and Figure 4, increasing the weight of the penetration loss reduces object penetration but adversely affects intention alignment and generation diversity.

**Progressive Grasp Objectives.** To address these challenges, we propose to decompose the complex learning objective into two more manageable objectives. The first objective is generative: it focuses on learning the grasp distribution, which does not prioritize quality but focuses on learning the grasp distribution with intention alignment and generation diversity. The second objective is regressive: it aims to refine the coarse grasp to a specific high-quality grasp with same intention. By decomposing the complex objectives, we reduce the learning difficulty of the generative objective as it does not concentrate on quality and avoids using penetration loss which could interfere the learning process. Additionally, the learning of regression is less complex than distributions, as it merely requires adjusting the pose to a specific target within a small space. Hence, we can employ penetration loss to ensure that the refined dexterous hand avoids penetrating the object and with high quality.

### Progressive Grasp Components

Benefiting from our progressive grasp objectives in Section 4.1, we design the following two simple progressive grasp components, which can achieve intention alignment, high diversity and high quality language-guided dexterous generation.

**Intention and Diversity Grasp Component.** We introduce intention and diversity grasp component to learn a grasp distribution efficiently, achieve intention aligned and diverse generation. Due to the distribution modeling objective, IDGC is build upon the conditional diffusion model [44; 4] to predict the dexterous pose \(^{dex}_{0}\) from noised \(^{dex}_{T}\). The input object point clouds \(\) is encoded by Pointnet++  and language \(\) is encoded by a pretrained CLIP model  as the condition. And we employ DDPM  as sampling process, which can be formalized by the following equation:

\[p_{}(^{dex}_{0}|,)=p( ^{dex}_{T})_{t=1}^{T}p(^{dex}_{t-1}| ^{dex}_{t},,).\] (3)

Figure 4: Quantitative experimental results with different object penetration loss weights \(_{pen}\). Intention is quantified by the Chamfer distance (CD) between predictions and targets. Diversity is assessed by the standard deviation of hand translation \(_{t}\). Object penetration is evaluated by the penetration depth (Pen.) from the object point cloud to the hand mesh. Our method uniquely achieves high performance in terms of intention consistency, diversity, and penetration avoidance.

**Quality Grasp Component.** The generated grasps of the first component possess well-aligned intentions and high diversity, but suffer from poor grasp quality due to significant object penetration. Therefore, we introduce Quality Grasp Component to refine the grasp quality while maintaining intention consistency in a regressive manner. Specially, it takes the coarse pose \(}^{dex}\), coarse hand point clouds \((}^{dex})\) and object point clouds \(\) as input, and outputs the pose \(^{dex}\). The refined grasp is obtained by \(}^{dex}=}^{dex}+^{dex}\). The training pairs of this component are constructed by collecting coarse grasps generated by the first component alongside the most similar ground-truth grasps that share the similar intentions. This ensures the training targets are aligned with the language intention, thereby guaranteeing that the refined grasps maintain consistency with the intended actions.

### Progressive Grasp Loss

**Intention and Diversity Grasp Loss.** We strategically employ regression losses and exclude object penetration loss to enhance the training efficacy of intention and diversity grasp component. By focusing exclusively on the regression learning, this component facilitates a more effective optimization process, achieving enhancements of intention consistency and grasp diversity. Concretely, we utilize L2 loss for pose parameter regression and incorporate the hand chamfer loss  to assist by explicit hand shape. The loss function of intention and diversity grasp component.is defined as:

\[_{IDG}=_{para}^{2}_{para}(_{0}^{dex}, }^{dex})+_{chamfer}^{2}_{chamfer}((_{0}^{dex}),(}^{dex})),\] (4)

where \(\) are dexterous hand point clouds of corresponding pose.

**Quality Grasp Loss.** Benefiting from the simplified training objectives, the quality grasp component focuses solely on refining coarse grasp to a specific target within a relatively constrained space, thereby reducing the negative impact of object penetration. Therefore, we employ the well-designed loss including object penetration. The loss function of quality grasp component can be formulated as:

\[_{QG}=_{para}^{3}_{para}+_{chamfer}^{3} _{chamfer}+_{pen}^{3}_{pen}+_{cmap}^{3} _{cmap}+_{spen}^{3}_{spen}.\] (5)

More details about loss function and model structure can be found in Appendix A.1.

## 5 Experiments

### Datasets and Evaluation Metrics

We split the DexDYSNet dataset at the object instance level, using 80% of the objects within each category for training and 20% for evaluation. Notably, none of the objects in the test set appear in the training set, ensuring that all experimental results are evaluated on unseen objects.

Three types of metrics are employed for evaluation from the perspective of intention consistency, grasp quality and grasp diversity. 1) For intention consistency, we employ **Frechet Inception Distance** (FID), using sampling point cloud features extracted from  to calculate \(P\)-\(FID\) and rendering image features extracted from  to calculate \(FID\). Additionally, **Chamfer distance

Figure 5: Overview of our framework. (a) With only the regression loss, intention and diversity grasp component is trained to reconstruct the original hand pose from the noise poses, based on language and object condition. (b) With both regression and penetration losses, Quality Grasp Component is trained to refine the coarse pose improve the grasp quality while maintain intension consistency.

(\(CD\)), is used to measure the distance between predicted hand point clouds and targets; **Contact distance** (\(Con.\)) is used to measure the L2 distance of object contact map between the prediction and targets. 2) For grasp quality, **Success rate** in Issac gym and \(_{1}\) measure grasp stability. We set the contact threshold to \(1\,\) and set the penetration threshold to \(5\,\) following . **Maximal penetration depth** (cm), denoted as \(Pen.\), reflects the maximal penetration depth from the object point cloud to hand meshes. 3) For diversity, we employ the **Standard deviation** of translation \(_{t}\), rotation \(_{r}\) and joint angle \(_{q}\) of eight samples within same condition, following . More details can be found in Appendix A.3.2.

### Implementation Details

For the construction of DexGYSNet, the step 2 and 3 are optimized for 20 and 300 iterations with learning rates of 0.01 and 0.0001 respectively. We set \(^{1}_{pen}=100\) and set \(^{1}_{spen}\), \(^{1}_{joint}\), \(^{1}_{cmap}\) each to 10. For training our framework, the training epochs are set to 100 for intention and diversity grasp component and 20 for Quality Grasp Component. The loss weights are configured as follows: \(^{2}_{para}=^{3}_{para}=10\), \(^{2}_{chamfer}=^{3}_{chamfer}=1\), \(^{3}_{cmap}=10\), \(^{3}_{pen}=100\), \(^{3}_{pen}=10\). Throughout all training processes, the model is optimized with a batch size of 64 using the Adam optimizer, with a weight decay rate of \(5.0 10^{-6}\). The initial learning rate is \(2.0 10^{-4}\) and decay to \(2.0 10^{-5}\) using a cosine learning rate  scheduler. All experiment are implemented with PyTorch on a single RTX 4090 GPU.

### Comparison with SOTA methods

The comparison results are presented in Table 1. We reproduce the SOTA methods to suit our task by concatenating the language condition with the point cloud features, the details can be found in Appendix A.3.3. As seen in the Table, our framework significantly outperforms all previous

    &  &  &  \\  & \(FID\) & \(P\)-\(FID\) & \(CD\) & \(Con.\) & \(Success\) & \(Q_{1}\) & \(Pen.\) & \(_{t}\) & \(_{r}\) & \(_{q}\) \\  GraspCVAE & 31.26 & 29.02 & 3.138 & 0.096 & 29.12\% & 0.054 & 0.551 & 0.179 & 1.762 & 0.179 \\ GraspTTA & 35.41 & 33.15 & 12.19 & 0.111 & 43.46\% & 0.071 & 0.188 & 2.111 & 6.150 & 3.869 \\ SceneDiffuser & 20.44 & 7.932 & 1.679 & 0.045 & 62.24\% & 0.083 & 0.253 & 0.346 & 3.455 & 0.387 \\ DGTR & 23.31 & 15.77 & 2.895 & 0.078 & 51.91\% & 0.078 & **0.163** & 2.037 & 14.01 & 4.299 \\  Ours & **6.538** & **5.595** & **1.198** & **0.036** & **63.31\%** & **0.083** & 0.223 & **6.118** & **55.68** & **6.118** \\   

Table 1: Results on DexGYSNet compared with the SOTA methods.

Figure 6: Visualization of generated dexterous grasp. The **top** visualizes one sample for each object and guidance pair. The bottom visualizes four samples, the **bottom left** shows that the generated grasp are consistent with clear and specific guidance, while the **bottom right** shows that the diversity achieved under relatively ambiguous instructions.

methods in terms of intention consistency and grasp diversity, while also achieving comparable performance in grasp quality. Previous methods struggle with learning a robust language conditional grasp distribution due to the optimization challenges outlined in Section 4.1. They often yield misaligned yet high quality grasps, resulting in comparable grasp quality, but less aligned intention and limited diversity compared to our framework. Overall, these results confirm that our framework achieves SOTA performance in generating intention-aligned, high-quality and diverse grasps.

In Figure 6, we visualize the generated grasp to qualitatively demonstrate the grasp generation capabilities of our framework. The bottom figure visualizes the results of four samples, the bottom left highlights our framework's ability to produce precise and consistent grasps under deterministic guidance (e.g., the way to use a trigger sprayer is deterministic). In the other hand, the bottom right illustrates our framework's diversity in generating grasps when provided with ambiguous guidance (e.g., the way to hold a bottle is diverse).

### Necessity of Progressive Components and Losses

The results presented in Table 2 validate the core insight of our framework: decomposing the complex task into progressive objectives, employing progressive components, and learning with progressive losses. The initial four lines of results demonstrate that a single component, without progressive objectives, fails to balance all objectives. Moreover, a single component, even with progressive objectives, that adjusts \(_{pen}^{2}\) from \(0\) to \(100\) after several training epochs, does not enhance performance. The similar result occurs when using progressive components without corresponding progressive losses, \(IDGC(_{pen}^{2}=100)+QGC\). Moreover, the commonly used quality refinement strategy test-time adaptation (TTA) , though improves grasp quality but results in extremely poor intention consistency. Overall, only the progressive designs of our DexGYSGrasp framework ensures excellence in intention alignment, high quality and diversity.

### Plug-and-play Experiments

We conducted experiments to evaluate the applicability of our insights to other state-of-the-art (SOTA) methods. Specifically, we trained GraspCAVE and SceneDiffuser without the object penetration

    &  &  &  \\  & \(CD\) & \(Con.\) & \(Q_{1}\) & \(Pen\) & \(_{t}\) \\  \(\) & & 0.048 & 0.037 & 0.572 \\  & \(\) & 0.101 & 0.833 & 0.516 \\ ✓ & \(\) & 0.012 & 0.029 & 0.477 \\ ✓ & \(\) & 0.015 & 0.063 & 0.369 \\   & 0.075 & 0.090 & 0.271 \\  & 0.051 & 0.074 & 0.332 \\   

Table 3: Ablation study for HOIR.

    &  &  &  \\  & \(CD\) & \(Con.\) & \(Q_{1}\) & \(Pen\) & \(_{t}\) & \(_{r}\) & \(_{q}\) \\  IDGC (\(_{pen}^{2}=0\)) & 1.276 & 0.028 & 0.024 & 0.534 & 5.710 & 54.75 & 7.741 \\ IDGC (\(_{pen}^{2}=50\)) & 2.980 & 0.061 & 0.074 & 0.271 & 2.421 & 33.27 & 3.391 \\ IDGC (\(_{pen}^{2}=100\)) & 4.009 & 0.067 & 0.072 & 0.175 & 2.701 & 38.27 & 3.785 \\ IDGC (\(_{pen}^{2}=500\)) & 4.185 & 0.072 & 0.107 & 0.037 & 0.547 & 8.807 & 0.481 \\  IDGC (\(_{pen}^{2}=0 100\)) & 3.181 & 0.056 & 0.093 & 0.302 & 1.341 & 16.53 & 2.211 \\ IDGC (\(_{pen}^{2}=0\)) + TTA & 20.09 & 0.102 & 0.057 & 0.178 & 4.849 & 51.91 & 8.479 \\ IDGC (\(_{pen}^{2}=100\)) + QGC & 2.009 & 0.042 & 0.099 & 0.143 & 3.414 & 40.35 & 2.844 \\ IDGC (\(_{pen}^{2}=0\)) + QGC & 1.198 & 0.036 & 0.083 & 0.223 & 6.118 & 55.68 & 6.118 \\   

Table 2: Ablation study for our framework. Intention and diversity grasp component is abbreviated as IDGC, Quality Grasp Component is abbreviated as QGC. \(_{pen}^{2}\) is the penetration loss weight tn the training of IDGC. Ours is colored in gray.

    &  &  &  \\  & \(CD\) & \(Con.\) & \(Q_{1}\) & \(Pen\) \\  \(\) & & 0.048 & 0.037 & 0.572 \\  & \(\) & & 0.101 & 0.833 & 0.516 \\ ✓ & \(\) & & 0.012 & 0.029 & 0.477 \\ ✓ & \(\) & & 0.015 & 0.063 & 0.369 \\   & 0.075 & 0.090 & 0.271 \\  & 0.051 & 0.074 & 0.332 \\   

Table 4: Plug-and-play Experiments.

constraint and trained the quality grasp component (QGC) to refine the coarse outcomes. As depicted in Table 4, removing the object penetration loss leads to improved intention consistency, which corroborates our findings discussed in Section 4.1. Moreover, our quality grasp component can significantly enhance grasp quality while maintaining the intention consistency.

### Effectiveness of Hand-Object Interaction Retargeting

We conducted ablation studies to evaluate our Hand-Object Interaction Retargeting (HOIR) strategy in constructing DexGYSNet dataset. As shown in Table 3, our three-step HOIR significantly improves both the quality and the intention consistency progressively. We observed that optimizing all losses in Equations 1 and 2 in one step (_all in one stage_), results in worse contact consistency and better grasp quality. Similar outcomes occur when the root translation is not fixed in step 3 (_w/o fix translation_). We believe this trade-off arises from inherent noise in the hand-object interaction data and the structural differences between human grasps and dexterous hands, making it challenging to excel in all aspects. Overall, we think that three-step HOIR strategy achieves more comprehensive outcomes, especially in the most important aspect of hand object contact consistency.

### Experiments in Real World

We conducted real-world grasp experiments to verify the practical application of our methods, as shown in Figure 7. The experiments are conducted on an Allegro hand, a Flexiv Rizon 4 arm and an Intel Realsense D415 camera. Although our framework is designed for full object point clouds, we integrate several off-the-shelf methods to enhance its practicality. Specifically, partial object point clouds are obtained through visual grounding  and SAM , which are then fed into a point cloud completion network  to obtain full point clouds. In execution, we first move the arm to the 6-DOF pose of the dexterous hand root node, and then control the dexterous hand joint angles to the predicted poses. Real world experiments further validate the effectiveness of our method. More implementation details can be found in Appendix A.5.

## 6 Conclusions

We believe that enabling robots to perform high quality dexterous grasps aligned with human language is crucial within the deep learning and robotics communities. In this paper, we explore this novel task, "Dexterous Grasp as You Say" (DexGYS). This task is non-trival, we propose a DexGYSNet dataset and a DexGYSGrasp framework to accomplish it. DexGYSNet dataset is constructed cost-effectively using the object-hand interaction retargeting strategy and the language guidance annotation system assisted by LLMs. Building on DexGYSNet, DexGYSGrasp framework, comprised of two progressive components, which can achieve intention-aligned, high diversity, and high quality dexterous grasp generation. Extensive experiments in DexGYSNet and real-world settings demonstrate that our framework significantly outperforms all SOTA methods, confirming the potential and effectiveness of our approach.

Figure 7: Visualization of real world experiments.