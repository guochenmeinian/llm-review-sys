# Pruning neural network models for gene regulatory dynamics using data and domain knowledge

**Intekhab Hossain**

Department of Biostatistics

Harvard T.H. Chan School of Public Health

Boston, MA 02115

ihossain@g.harvard.edu

&**Jonas Fischer**

Dep. for Computer Vision and Machine Learning

Max Planck Institute for Informatics

Saarbrucken, Germany

jonas.fischer@mpi-inf.mpg.de

&**Rebekka Burkholz**

Helmholtz Center CISPA

for Information Security

Saarbrucken, Germany

burkholz@cispa.de

&**John Quackenbush**

Department of Biostatistics

Harvard T.H. Chan School of Public Health

Boston, MA 02115

johnq@hsph.harvard.edu

###### Abstract

The practical utility of machine learning models in the sciences often hinges on their interpretability. It is common to assess a model's merit for scientific discovery, and thus novel insights, by how well it aligns with already available domain knowledge-a dimension that is currently largely disregarded in the comparison of neural network models. While pruning can simplify deep neural network architectures and excels in identifying sparse models, as we show in the context of gene regulatory network inference, state-of-the-art techniques struggle with biologically meaningful structure learning. To address this issue, we propose DASH++, a generalizable framework that guides network pruning by using domain-specific structural information in model fitting and leads to sparser, better interpretable models that are more robust to noise. Using both synthetic data with ground truth information, as well as real-world gene expression data, we show that DASH, using knowledge about gene interaction partners within the putative regulatory network, outperforms general pruning methods by a large margin and yields deeper insights into the biological systems being studied.

Footnote †: I� is currently employed by Analysis Group, Inc.

## 1 Introduction

With ever-growing neural network architectures encouraged by the success of overparametrization, with over a trillion parameters in a single model such as GPT4, there is a similarly growing demand for sparser, more parameter-efficient neural networks that are more resource-friendly and interpretable. The Lottery Ticket Hypothesis (LTH) provides an empirical existence proof of sparse, trainable network architectures , that eventually achieve a similar performance as their dense counterpart. Subsequent work introduced _structured_ pruning approaches, facilitating group-wise neuron- , or channel-sparsity , which are, however, focused on the structure of the _architecture design_, aiming for better alignment with hardware implementations to eliminate operations, rather than structure that reflects _relevant domain information_. Especially for scientific discovery, analignment of learned structure with such domain knowledge is, however, essential for interpretability, as only then the model represents meaningful domain-relevant relations. Such problem settings often occur for example in physics or biology where a learned model should give an explanation to be able to form a hypothesis. This poses the question: How can we select among multiple predictive models and promote the search for _meaningful_ neural network structures?

To guide the learning process, we argue that we need additional problem-specific structural information, and should leverage any available - and reliable - domain knowledge. One of the fundamental tasks of molecular biology is to understand the _gene regulatory dynamics_ in health and disease. Gene regulatory dynamics describe the changes of the expression of a gene--the generation of small copies of a DNA segment that can serve among others as blueprint for proteins--dependent on other regulatory factors such as transcription factors, which are proteins that bind next to the gene (DNA segment) to modulate its expression. Yet, the exact dynamics are far from understood and changes in these dynamics can be drivers for diseases such as cancer. As such, improving an understanding of the mechanics behind these dynamics increases the understanding of the disease and can ultimately inform therapy design. This problem setting of estimating gene regulatory dynamics requires high interpretability, as an understanding of the true biological mechanics--the relationship between regulatory factors and a gene's expression--is needed, as well as sample-efficiency, as generating time-course data even for a few patients is extremely expensive. While models to estimate gene regulatory dynamics have been suggested [14; 1; 60; 27], none of these is particularly sparse or interpretable. We propose a new approach of network sparsification that _guides pruning by domain knowledge_ implemented for a neural model for estimating gene regulatory dynamics, which yields networks that are very sparse, align with underlying biology, while accurately predicting dynamics.

This idea of _prior-informed_ or domain-aware pruning is at the heart of this paper. In particular, we propose DASH (**D**omain-**A**ware **S**parsity **H**euristic, Fig. 1), an iterative pruning approach that scores parameters taking into account structural domain-knowledge. With DASH, it is possible to control the level of prior information taken into account for pruning and it _automatically finds an optimal sparsity level_ aligned with both the prior and the data. Considering the task of estimating gene regulatory dynamics, we first show in synthetic experiments that DASH generally outperforms standard (task- and architecture agnostic) pruning as well as task-specific pruning approaches. On real data with a reference gene regulatory network (GRN) derived from gold standard biological experiments, we show that DASH better recovers the reference GRN and reflects more biologically plausible information. On recent single cell data on blood differentiation, we show that DASH, in contrast to existing work, identifies biologically relevant pathways that can be used to generate new insights and inform domain experts. We anticipate that our work serves both for future benchmarking on how well pruning approaches are in structure learning, as well as a blueprint for guided network pruning in fields where domain knowledge is readily available, such as in other hard sciences including physics or material science, where knowledge about variables, e.g., associations between atoms or molecules or equations relating quantities in a system, is available.

## 2 Related work

The Lottery Ticket Hypothesis (LTH)  provides an empirical existence proof of sparse, trainable neural network architectures. It conjectures that dense, randomly initialized neural networks contain subnetworks that can be trained in isolation with the same training algorithm that is successful for the dense networks. However, a strong version of this hypothesis [51; 65], which has also been proven theoretically [42; 48; 46; 17; 5; 12; 15], suggests that the identified initial parameters are not only specific to the sparse structure but also the learning task and benefit from information about the larger dense network that has been pruned . Acknowledging this strong relation, other works have proposed to combine mask and parameter learning directly in continuous sparsification approaches  that employ regularization strategies that approximate L0 penalties [53; 40; 31; 55]. In the following, we recap the key ideas behind the methods most relevant to ours.

#### Explicit pruning-based approaches

**Magnitude pruning (MP)** In magnitude pruning (MP) a neural network is trained and then (post-hoc) pruned to a desired sparsity level by masking the corresponding proportion of _smallest magnitude_ weights. This smaller, masked network is then further trained, reminiscent of fine-tuning .

More formally, we start with a fully connected neural network \(_{}\) with \(L\) layers parametrized by \(:=\{(W^{l},b^{l})\}_{l=1}^{L}\). MP is performed after training is complete (i.e. post hoc). For a suitable threshold of \(p\%\), MP "prunes" the trained \(\) by setting the smallest (absolute value) \(p\%\) of weights in \(\) to 0. The choice of parameters to prune can either be made in an unstructured way, by choosing the the lowest \(p\%\) across all \(\{W^{l}\}_{l=1}^{L}\), semi-structured, by pruning the lowest weights per layer, or in a structured way such that, e.g., neurons with lowest \(p\%\) average outgoing weight are pruned. Once pruning is complete the \(p\%\)-sparse \(_{^{}}\) is fine-tuned on the data so that \(^{}\) is learned appropriately.

**Iterative magnitude pruning (IMP)** suggest to alternate between training and magnitude pruning, iteratively sparsifying the network, to date still of the most successful pruning strategies. In practice, a pruning schedule is used to _iteratively_ sparsify \(\), until a \(^{}\) with desired target sparsity or predictive performance plateau is reached. Importantly, weights are reset to initial pre-training values, either after each round of pruning or once after the target sparsity has been reached.

**Pruning with rewinding** have demonstrated that rewinding weights to an earlier training point--a compromise between fine-tuning of MP and reset to initialization of IMP-- provides good performance, which also has been suggested in the context of Neural ODEs as SparseFlow .

#### Implicit penalty-based approaches

**C-NODE** seek to reduce the overall number of input-output dependencies (i.e. paths of contribution from input neuron \(i\) to output neuron \(j\)) through \(\{W^{l}\}_{l=1}^{L}\). The approach can result in both feature and weight sparsity in NeuralODEs.

\(}\) incorporate a differentiable \(L_{0}\) norm regularizer term in the objective. It implicitly prunes the network by encouraging weights to get exactly to zero. The \(L_{0}\) regularizer is operationalized using non-negative stochastic gates which act as masks on the weights.

**PathReg** innovatively combines the strengths of both C-NODE and the \(L_{0}\) approach to promote both weight and feature sparsity in NeuralODEs. It uses stochastic gates similar to  and add a C-NODE-inspired penalty terms that constrain the overall number of input-output paths by regularizing the _probability_ of any path from input \(i\) to output \(j\) being non-zero.

#### Modeling gene regulatory dynamics

As application, we consider estimation of gene regulatory dynamics. Early work, such as COPASI  use a fully parametric modeling approach that are limited in their prediction capabilities. With recent advances in machine learning, tools such as Dynamo , PROB , and RNA-ODE  aim to learn regulatory ODEs using sparse kernel regression, Bayesian Lasso, and random forests, respectively. Leveraging high flexibility and performance of neural models, PRESCIENT  uses a simple NN to learn regulatory ODEs, whereas tools such

Figure 1: _DASH._ A NN, here a neural ODE for gene regulatory dynamics, is traditionally sparsified in a data-centric way (top). Pruning is done based on data alone, the pruning score \(\) is a function of the learned weights \(W\). Such sparsified models often do not learn plausible relationships in the data domain. We propose DASH (bottom), which additionally incorporates domain knowledge \(P\) into the pruning score \(\), yielding sparse networks giving meaningful and useful insights into the domain.

as DeepVelo  and sctour  have a variational autoencoder as backbone. The latest line of research [14; 1; 60; 27] uses neural ordinary differential equations . However, a key limitation of these methods is the lack of interpretability arising from non-sparse dynamics that do not align with ground truth biology. Consequently, the induction of sparsity in gene regulatory ODEs has been an active area of research with C-NODE and PathReg as most recent advances[2; 1], the achieved sparsity levels are, however, not yet sufficient to capture the relevant biology.

## 3 Domain-aware pruning with DASH

While the above sparsification strategies have shown to perform well in various settings, the resulting models are often either not particularly sparse or do not reflect meaningful domain knowledge. We hypothesize that this is due to two reasons: (1) the difficulty of identifying a good sparse network and (2) the current focus on _hardware-centric_ rather than _task-centric_ pruning, valuing structural pruning of a model in terms of groups of neurons (layers, channels) over structural pruning reflecting task-specific knowledge. To overcome these problems, we suggest to ground the model search (here, the network training) with existing domain-specific knowledge, which eases identifiability due to the introduced constraints and enables task-aware pruning to identify meaningful domain knowledge.

In the following, we propose DASH (**D**omain-**A**ware **S**parsity **H**euristic), an iterative pruning-based approach that accounts for prior knowledge by scoring parameters in terms of their alignment with this prior, and show its usefulness for a neural model for the inference of gene regulatory dynamics. We assume the domain knowledge to be given as input-output relations (e.g., known protein--gene interactions in molecular biology), for which we want the network flow between any input and output to align with. Suppose our domain knowledge for a task from this domain is given as a real-valued relationship graph \(G=(V,E)\), where nodes are relevant entities from the domain, e.g. genes or proteins, and edges are a strength of association between these entities, e.g. evidence of association derived from literature or experiments. Examples for such relational information in case of protein--gene associations can be derived from protein binding profiles . For the rest of the paper, we will assume that this domain knowledge is given as a matrix \(^{k r}\), which encodes the strength of association between the \(k\) inputs and \(r\) outputs for our task of interest, such as known proxies of protein--gene interactions. Intuitively, for a one-layer neural network, we encourage pruning scores for a (neural) network edge to be proportional to the corresponding edge in the prior knowledge graph \(G\) while still taking into account the data-specific knowledge, thus enabling learning

Figure 2: _Results on simulated data._ We visualize performance of pruning strategies in comparison to original PHOENIX (baseline) in terms of achieved sparsity (x-axis) and balanced accuracy (y-axis) of the recovered gene regulatory network against the ground truth on the SIM350 data with 5% noise. Error bars are omitted when error is smaller than depicted symbol. \(\) indicate methods that leverage prior information. Top left is best: recovering true, inherently sparse biological relationships.

of new knowledge and robustness to wrong or missing information in the prior. We begin with this simple base case of task-aware pruning of a fully connected neural network with \(L=1\) layer and extend to more layers below.

DASH for \(L=1\).For a single layer NN, with \(k\) input and \(r\) output neurons and corresponding weight matrix \(^{r k}\), we compute non-negative **pruning scores**\(^{r k}\) by leveraging the **domain knowledge**\(^{r k}\). In practice, we allow balancing between _data-driven_ and _prior-knowledge-driven_ pruning, implemented through a convex combination of the learned weights \(\) and prior domain knowledge \(\) controlled by the parameter \(\). Alternating between training and pruning akin to Iterative Magnitude Pruning , we set the following pruning score during a pruning phase:

\[}:=(1-)}|}+||\,\]

where \(}|}\) represents the appropriately normalized matrix (details in Appendix B.2) of absolute weights as learned up to epoch \(t\). We then prune the parameters in \(}\) corresponding to the lowest absolute \(p_{t}\%\) of entries in \(}\), where \(p_{t}\) is the desired sparsity level at time \(t\) given by a schedule.

DASH for \(L=2\).For two-layered NNs with weights \(}^{m k},}^{r m}\), i.e \(k\) inputs, \(r\) outputs, and \(m\) hidden neurons, we consider knowledge about input-output relationships \(^{r k}\) as before. We can additionally use further knowledge about input-input relationships \(^{k k}\). In molecular biology this could be information about binding or interaction partners available in databases such as STRINGDB , which has also been employed to guide static gene regulatory network inference , or co-regulators, derived from co-occurrence of proteins. Intuitively, we now project pruning scores for the first layer to the prior knowledge about input-input relations, encouraging closeness to this prior, while projecting the product of pruning scores of first and second layer to known input-output relations, reflecting the flow of information from input to output through these two layers. Given that \(^{(t)}}\) represents how the \(k\) inputs are encoded by \(m\) neurons, and \(^{(t)}}\) are the corresponding pruning scores, we surmise that the matrix product \(^{(t)}}^{}^{(t)}}^{k k}\) should approximately align with the prior knowledge \(\). Since solving \(^{(t)}}^{}^{(t)}}=C\) is not directly feasible we initialize \(^{(0)}}\) randomly, and resort to solving a recurrence relation version of problem, that is \(^{(t-1)}}^{}^{(t)}}=C\). Using the left and right pseudo-inverse to obtain a solution to the above, defined as \(_{L}(X)=(X^{}X)^{-1}X^{}\) and \(_{R}(X)=X^{}(XX^{})^{-1}\) respectively:

\[^{(t)}}:=(1-_{1})^{(t)}}|}+_ {1}_{L}^{(t-1)}}^{} .\]

\(_{L}^{(t-1)}}\) encourages \(^{(t)}}^{}^{(t)}}\) to iteratively align with \(\) as \(t\) increases (i.e. as training progresses). With \(^{(t)}}\) fixed, we can update scores \(^{(t)}}\) of the second layer parameters \(^{(t)}}\). Since the product \(^{(t)}}^{(t)}}^{r k}\) represents the overall flow of information from inputs to outputs at epoch \(t\), we surmise that \((^{(t)}}^{(t)}})^{r k}\) should reflect \(\). We thus get

\[^{(t)}}:=(1-_{2})^{(t)}}|}+_{ 2}_{R}^{(t)}}.\]

Similar to the case for DASH for \(L=1\) layer, we can now prune the parameters of \(^{(t)}}\) and \(^{(t)}}\) based on the magnitude of pruning scores \(^{(t)}}\) and \(^{(t)}}\), respectively.

DASH for \(L>2\)For many interpretability-centric tasks, including our application to gene regulatory networks, small architectures of \(L=2\) are common, as domain experts are interested in understanding the exact flow of information through the network. Furthermore, we know that two-layer neural networks exhibit universal approximation . We however hypothesize that the technique of computing pruning scores by fixing those of preceding layers can be extended to a larger number \(L\) of fully connected layers and elaborate in App. B.6.

FlexibilityWhile the \(_{l}\) can be tuned using cross-validation (see App. B.4), we note that it allows for flexibly encoding different pruning philosophies. Specifically, when \(_{l}=0\)\( l\), DASH corresponds to SparseFlow, and when \(_{l}=1\)\( l\), DASH represents fully prior-based sparsification (which we term "BioPrune" and consider as experimental baseline).

## 4 Task-aware pruning for sparse gene regulatory dynamics

Perhaps one of the most interesting applications of Machine Learning is in the field of Molecular Biology with the goal of understanding human health and disease. A central mechanisms in humans is the process of gene expression in each cell. There, copies of short segments of our genome are produced. These copies are among other things the blueprint for the production of different proteins, which are needed virtually everywhere in our bodies. If this tightly regulated process of gene expression goes wrong, for example because of a mutation in our genome, this can have profoundly bad effects, such as in the case of cancer. As such, studying this process is of great interest to understand and improve human health and discover new therapeutic targets.

Here, we consider the task of predicting the regulatory dynamics of gene expression. To be able to understand the model and transfer it to clinical practice, interpretability is key. The most recent developments in modeling gene regulatory systems allow to model actual (temporal) regulatory dynamics, but require complex models, such as NeuralODEs, that hinder interpretability. While state-of-the-art results are now achieved with shallow architectures  that are more tractable than deep, heavily over-parameterized networks, these models still encode information across many thousands of weights and we show experimentally that such information does not reflect true biology well. In fact, true gene regulatory networks and hence their underlying dynamics are inherently sparse . This sparsity should be properly reflected by neural dynamics models. The PHOENIX NeuralODE model will serve as our base model for applying sparsification strategies and we show that pruning aligned with prior domain knowledge improves interpretability as well as quality of inferred (new) knowledge.

In a nutshell, given a time series gene expression sample for \(k\) genes, PHOENIX uses NeuralODEs to construct the predicted trajectory between gene expression \((t)^{k}\) (inputs) at time \(t=t_{i}\) to any future expression \(}(t_{i+1})\) (outputs), by implicitly modeling the RNA velocity (\(d/dt\)). PHOENIX uses biokinetics-inspired activation functions to separately model additive and multiplicative co-regulatory effects. The trained model encodes the ODEs governing the dynamics of gene expression, which can be directly extracted for biological insights. We apply DASH to PHOENIX and give a brief review of the PHOENIX architecture in App. B.10 and a detailed account on how to apply DASH to this architecture in App. B.11. Next, we provide experiments on synthetic and real data showing the advantages of prior-informed pruning on the task of predicting gene-regulatory dynamics.

   & Sparsity(\%) & Bal. Acc.(\%) & MSE (\(10^{-3}\)) \\   & \(7.5 0.1\) & \(51.8 0.03\) & \(3.0 0.4\) \\   Penalty- \\ based \\ (implicit) \\  } & \(L_{0}\) & \(33.8 4.7\) & \(55.0 0.5\) & \(8.5 1.0\) \\  & C-NODE  & \(6.2 0.5\) & \(55.9 0.1\) & \(2.8 0.6\) \\  & PathReg  & \(56.5 1.5\) & \(61.9 1.0\) & \(8.0 1.8\) \\  & PINN \(\) & \(9.9 0.4\) & \(58.6 0.7\) & \(2.5 0.2\) \\  & DST & \(92.8 0.3\) & \(71.9 0.5\) & \(4.0 0.5\) \\   Pruning- \\ based \\ (explicit) \\  } & IMP  & \(81.9 6.6\) & \(61.7 0.7\) & \(4.7 1.1\) \\  & Iter. SynFlow  & \(79.3 1.2\) & \(58.4 0.6\) & \(7.0 2.1\) \\   & SparseFlow  & \(\) & \(70.9 1.5\) & \(3.6 0.6\) \\   & BioPrune (Ours, see 3)\(\) & \(83.5 1.9\) & \(87.3 0.8\) & \(2.6 0.9\) \\   & DASH (Ours)\(\) & \(94.6 1.2\) & \(\) & \(2.4 1.2\) \\  Hybrid & PINN + MP (Ours)\(\) & \(87.0 0.01\) & \(82.4 0.2\) & \(\) \\  

Table 1: _Synthetic data results._ We give model sparsity, balanced accuracy with respect to edges in the ground truth gene regulatory network, mean squared error of predicted gene regulatory dynamics on the test set, and number of epochs (till validation performance plateaus) as proxy of runtime. \(\) is used to indicate methods that leverage prior information. Results are on SIM350 data with 5% noise.

## 5 Experiments

For evaluation we consider synthetic data from an established simulator tool , as well as real world data of gene expression from breast cancer tissue , from yeast with synchronized cell cycle , and from human bone marrow . In case of synthetic data, we use the ground truth regulatory system from the generating model for validation. For breast cancer and yeast cell cycle data we use additional experimental data (ChIP-seq) from the corresponding studies, which are independent gold-standard biological experiment measuring sample-specific TF-gene interactions, to evaluate inferred regulatory relationships. We measure the correctness of a GRN learned by a model (see App. B.10.4) in terms of balanced accuracy, which measures whether an edge is correctly reconstructed weighted by the sparsity of the aforementioned ground truth graph. To evaluate predictive performance for real data, we use a 6% hold-out test set for breast cancer and one of the biological replicates hold out from training for the yeast data. As prior knowledge we leverage general information of transcription factor binding to gene promoter regions as prior information, which can be computed from binding motif matches with the corresponding genome (human respectively yeast). The result is a matching score that can be thresholded to get a \(0,1\)-based matrix encoding which (TF-encoding) gene has a relationship with which other gene. We follow the approach of Guebila et al.  to obtain matrix \(P\). As prior \(C\), we use the STRING database , which gives a general (i.e., not tissue-specific) graph of protein-protein interaction. Here, we use the interactions based on experimental evidence only and employ a cutoff of.6 to get a binary adjacency matrix. (for more details, see App. B.3).

To compare pruning strategies, we consider the PHOENIX model as a basis, which is the state-of-the-art NeuralODE for estimating gene regulatory dynamics  and provide an ablation on a standard MLP architecture (see App. Tab. 7, App. Tab. 10, and App B.12). We compare DASH against the PHOENIX model without additional pruning as performance reference, and suggest two simple yet powerful baselines, which is post-hoc magnitude pruning of weights followed by finetuning (PINN+MP), and BioPrune, a fully prior-based pruning (cf. Sec. 3). From the literature, we consider \(L_{0}\)-regularized pruning , C-NODE , and PathReg , which have been recently proposed for the inference of sparse gene-regulatory relationships, PHOENIX with biological regularization , and dynamic sparse training (DST) , all of which are implicit pruning approaches. We further consider explicit, iterative score-based pruning approaches including Iterative Magnitude Pruning (IMP) , the flow-based model-agnostic pruning method SynFlow , and the flow-based Neural ODE pruning SparseFlow . We tune hyperparameters, including \(\) for DASH, on a validation set. Unlike other methods (e.g., \(L_{0}\)) DASH does not prolong the runtime of training much. A common pruning schedule where pruning scores are computed once every 10 epochs only increases runtime by <2% for the full model fitting process.

Figure 3: _Reconstruction of ground truth relationships._ Estimated effect of gene \(g_{j}\) (x-axis) on the dynamics of gene \(g_{i}\) (y-axis) in SIM350 for different levels of noise (rows). Ground truth is given on the left, our suggested approach and baselines (DASH, BioPrune, and PINN+MP) on the right with mean squared error between inferred regulatory relationships and ground truth in purple.

Simulated gene regulatory systemsWe simulate gene expression time-series data from a fixed dynamical system, the ground truth was thus known (see App. B.1). In short, we adapt SimulatorGRN to generate noisy time-series expression data from two synthetic gene regulatory systems (SIM350 and SIM690, consisting of 350 and 690 genes, respectively). We split trajectories into training (88%), validation (6% for tuning \(\)), and testing (6%).We evaluate all methods in terms of achieved sparsity and MSE of predicted gene expression values on the test set (App. B.2, B.4) and investigate biological plausibility by calculating balanced accuracy of regulatory relationships extracted from the PHOENIX model (for details, see App. B.5, B.10.4), We here report the results for the data of 350 genes and 5% noise, noting that results are consistent across different noise levels and with more number of genes (see App. Sec. A.1).

A general trend across all experiments that aligns with our initial motivation is that dense models (sparsity \(<\) 50%) have a significantly worse reconstruction of the underlying biology - the ground truth GRN - than sparse models (sparsity \(>\) 80%) (see Fig. 2). Furthermore, we see that DASH retrieves not only among the sparsest networks, but also reflects the underlying GRN best across all methods, outperforming comparably sparse IMP by about 20 percentage points accuracy in different settings, even with decrease in quality of the prior (see sensitivity analysis in App. Tab. 6). Due to the prior-informed structured pruning, it is able to occupy the sweet spot of highly sparse at the same time biologically meaningful models.

Consistent with the literature, PathReg outperforms \(L_{0}\) as well as C-NODE in terms of sparsity , we additionally find evidence that it also delivers more biologically meaningful results. Yet, IMP as well as prior-informed pruning approaches outperform PathReg by a large margin. The MSE of predicted gene expression of DASH is among the best, within one standard error of the best overall method. The only better approach is our suggested baseline, a combination of posthoc magnitude pruning of PHOENIX combined with additional finetuning (PINN+MP), which is, however, impractical as it requires to train and prune many PHOENIX models along a grid of sparsity levels (see App. B.9.3).

Visualizing the estimated against ground truth regulatory effects (i.e., functional relationships between variables), we observe that DASH captures the effects much better than competitors (see Fig. 3). Virtually all existing approaches discover spurious regulatory effects, whereas prior-informed pruning identify the main regulatory effects correctly. Moreover, with increasing levels of noise in the simulation, we observe that both BioPrune as well as PINN+MP start finding spurious dependencies, while DASH still recovers the overall structure well. While not perfect, as seemingly there are more dependencies than in the sparse ground truth, potentially introduced by correlations between features, DASH provides a sparse estimation of regulatory effects that most closely resembles the ground truth relationships among existing work.

Pseudotime-ordered breast cancer samplesTo investigate the performance of DASH on real data, we consider gene expression measurements from a cross-sectional breast cancer study . This data of 198 breast cancer patients with measurements for 22000 genes has been preprocessed and ordered in pseudotime , which we use as basis for our experiments (cf App. B.7). Across methods, we observe that implicit sparsification methods generally perform poorly in terms of sparsity and accuracy of recovered relationships (see Tab. 2). While pruning-based sparsification approaches achieve greater sparsity and performance in predicted gene expression, with SparseFlow reaching the highest sparsity (95.7%) among all methods, the recovered biological relations are not better than random chance, which renders the underlying models useless for scientific discovery. DASH in contrast finds a comparably sparse network (92.7% sparsity) while having top of the line performance in terms of test MSE and high alignment with true biology (95.7% balanced accuracy). For this particular dataset, we observe that DASH primarily builds on the prior knowledge, not surprisingly performing similarly as BioPrune, which is our suggested baseline pruning approach taking only the prior into account. We will see for other real-world data that this weight of domain knowledge is highly task-specific and BioPrune yields sub-optimal results on different data.

To better understand whether the inferred gene regulatory dynamics align with meaningful biology, we additionally perform a pathway analysis (see App. B.8). Such pathway analysis are a standard approach for domain experts to distill information for example for therapeutic design. The genes that show the highest impact on the dynamics within the derived model are tested whether they enrich in a specific higher level biological pathways. For the top-20 most significantly enriched pathway per model (App. Fig. 5), we observe that in contrast to prior-informed methods, the existing pruning approaches show only very few significant pathways, consistent with our quantitative results on inferred regulatory relations. Moreover, disease-relevant pathways such as TP53 activity or FOXO mediated cell death, both of which are highly relevant in cancer , are only visible in models pruned with prior information. This provides evidence that pruning informed by a biological prior recovers biological signals that are relevant in the disease and which can not be picked up otherwise. Furthermore, we find Heme-signaling as a pathway uniquely identified as relevant in our approaches (cf. App. Fig. 5). Hence as a signaling molecule has key roles in the gene regulatory system , and turns out to have an anti-tumor role in breast cancer specifically . Subsequent approaches pharmaceutically targeting Heme signaling showed success , with one of the key regulators affected being Bach1. To suggest further targets for e.g. combination treatment, we hence examined the top-5 regulatory factors in terms of weights in our estimated gene regulatory dynamics. These factors include PBX1 and FOXM1, for which a drug repurposing of existing compounds, such as , could lead to a potential new treatment for this specific cancer.

Yest cell-cycle dataWe next consider real data of synchronized yeast cell  (see App. B.2 for training setup). We observe an overall trend similar to the breast cancer study in terms of achieved sparsity and balanced accuracy (cf. Tab. 2), with implicit sparsification methods generally finding significantly less sparse models and all methods that do not incorporate prior knowledge having inferred relationships that are not better than random chance. For this data, however, DASH finds an optimal lambda value that incorporates more data-specific knowledge (\(=0.75\)) compared to the breast cancer study above. This shows the advantage of DASH over our BioPrune baseline model (prior-only pruning), as here we gain about 10% points in balanced accuracy over BioPrune for ChIP-seq validation data  and 2% points over BioPrune for an independent TF perturbation network curated to derive a "true" causal GRN  (see App. Tab. 8). We also retrieve a 3% points sparser model. Comparing inferred biological knowledge between BioPrune and DASH through a pathway analysis, we see that DASH recovers more significantly enriched pathways related to cell cycle processes (cf. App. Fig. 6).

Cell differentiation in human bone marrowLastly, we investigate the performance of DASH in an exploratory setting with single cell data of human bone marrow ordered in pseudotime . Here, we are interested in better understanding the gene regulatory dynamics of blood cell differentiation, the process of hematopoietic stem cells specializing into cells taking over roles such as immune response (e.g., B- and T-cells). This process is called hematopoiesis. We follow the steps of  to first split samples (i.e., cells) into the three different lineages (paths of differentiation), and train separate models for each (see App. B.7). We will here focus on the analysis of the Erythroid lineage.

   Data &  &  \\ Strategy & Sparsity & Bal. Acc. & MSE (\(10^{-5}\)) & Sparsity & Bal. Acc. & MSE (\(10^{-2}\)) \\  None/Baseline & 0.03\% & 49.99\% & 7.78 & 0.10\% & 49.87\% & **4.84** \\ \(L_{0}\) & 10.77\% & 50.15\% & 7.90 & 34.43\% & 48.43\% & 5.33 \\ C-NODE & 11.20\% & 50.01\% & 8.06 & 10.89\% & 50.04\% & 4.87 \\ PathReg & 14.09\% & 50.24\% & 7.92 & 12.09\% & 50.11\% & 5.35 \\ PINN \(\) & 0.11\% & 49.90\% & 7.82 & 0.17\% & 49.93\% & 5.77 \\ DST & 67.02\% & 50.42\% & 7.78 & 77.80\% & 49.92\% & 5.18 \\  IMP & 36.02\% & 50.34\% & 7.77 & 83.22\% & 49.99\% & 5.46 \\ Iter. SynFlow & 91.93\% & 49.37\% & 7.78 & 85.65\% & 49.57\% & 5.41 \\ SparseFlow & 95.70\% & 49.70\% & **7.76** & 95.22\% & 49.89\% & 5.38 \\ BioPrune \(*\), \(\) & 93.44\% & 95.67\% & 7.80 & 94.69\% & 79.23\% & 5.94 \\ DASH \(*\), \(\) & 92.71\% & **95.69\%** & **7.76** & **97.18\%** & **88.43\%** & 5.27 \\ PINN + MP \(*\), \(\) & 92.00\% & 54.02\% & 7.79 & 95.01\% & 55.39\% & 6.09 \\   

Table 2: Results on breast cancer and yeast data. Balanced accuracy is based on reference gold standard experiments (transcription factor binding ChIP-seq) available for this data. DASH found optimal \(\)-values of \((0.995,0.95)\) respectively \((0.75,0.75)\) for breast cancer and yeast. * marks our suggested baselines and method, \(\) marks methods that use prior information for sparsification.

As before, DASH yields highly sparse (95%) networks, the most sparse among all competitors (App. Tab. 9). IMP shows similarly strong sparsification as DASH while PathReg achieves much less sparsity (14%). In terms of performance, all methods achieve similar MSE of predicted gene expression dynamics on the test set, meaning that even though much sparser, both DASH and IMP predict equally well as an order of magnitude more dense network. For this data, there are no gold-standard experiments for regulatory relations available, we hence focus on analysing the network topology. From the literature, we would expect sparser networks to be better align with biology . DASH indeed shows the lowest out-degree in the inferred regulatory network, less than half of what IMP recovers. PathReg shows an order of magnitude larger average out-degree. To confirm the biological plausability, we again do a pathway analysis. DASH seems to find significant enrichment in biologically relevant pathways (App. Fig. 7) that can directly be linked to hematopoiesis, such as _heme signaling_ or _RUNX1 regulates differentiation of hematopoietic stem cells_, which neither BioPrune nor SparseFlow--the only other method yielding a proper sparse model--could recover.

## 6 Discussion & Conclusion

We considered the problem of identifying sparse neural networks in the context of interpretability with a focus on the application to gene regulatory systems modeling. In domains such as biology and contexts when the true underlying systems are sparse, interpretability is key for experts, rendering the use of the common over-parametrized and complex neural network architectures difficult. Although NNs do not directly encode e.g. the regulatory relationship between genes its deep architecture is necessary to model complex functional relationships while ensureing stable learning. Yet, we can ensure that Recent advances in neural network pruning, such as those around the Lottery Ticket Hypothesis , promise sparse and well-performing models, yet, hardness results prove finding optimally sparse models to be challenging , which is also reflected by recent benchmarking results . Our experiments confirmed that general pruning strategies provide sub-optimal sparsity, moreover, the underlying biological relationships are not properly reflected in the model. We proposed _to guide pruning by domain knowledge_, leveraging existing prior information to improve the interpretability and meaningfulness of pruned models.

In case studies on gene regulatory dynamic inference, a key task in molecular biology with high relevance in cancer research, we showed based on simulated as well as real world data that our method, DASH, in contrast to a wide range of state-of-the-art methods, is able to recover neural networks that are both very sparse and at the same time biologically meaningful, allowing for direct extraction of a sparse gene regulatory network. On real data, DASH not only better aligns with gold-standard experimental evidence of regulatory interactions, but also uniquely reflects the data-specific biological pathways, which can be used by domain experts to generate new insights.. It thus serves as a proof of concept that in critical domains, where interpretability is essential and domain knowledge exists, pruning can be heavily improved by alignment with prior knowledge. While our guided pruning approach is in principle agnostic to the type of neural network and task, we here focused on a specific case study that we deemed important. In the future, it would be interesting to apply DASH to different cases and domains, including other biomedical tasks, but also to physics or material sciences, where interpretability is also key and domain knowledge exists in the form of physical constraints and models. For any application, an important consideration to apply DASH is on the one hand the availability of prior knowledge, but on the other hand its quality; while we show here that even with incomplete and noisy prior knowledge we receive good results, factually wrong priors could steer the solution towards a wrong model. We hence assume that DASH will be of primary use in classical hard sciences mentioned above, with priors that stood the test of time over several decades. Another line of future work includes different architectural designs, such as convolution or attention mechanisms, where input-output relationships are less straightforward to project to across several layers.

In summary, we make a case for pruning informed by domain knowledge and provide evidence that such approaches can massively improve sparsity along with domain specific interpretability.

AcknowledgementsRB received funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. JQ was supported by a grant from the US National Cancer Institute (R35CA220523) and additional funding from the National Human Genome Research Institute (R01HG011393).