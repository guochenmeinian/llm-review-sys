# \(\mathcal{H}\)-Consistency Bounds:

# \(\)-Consistency Bounds:

Characterization and Extensions

 Anqi Mao

Courant Institute

New York, NY 10012

aqmao@cims.nyu.edu &Mehryar Mohri

Google Research & CIMS

New York, NY 10011

mohri@google.com &Yutao Zhong

Courant Institute

New York, NY 10012

yutao@cims.nyu.edu

###### Abstract

A series of recent publications by Awasthi et al. (2022) have introduced the key notion of _\(\)-consistency bounds_ for surrogate loss functions. These are upper bounds on the zero-one estimation error of any predictor in a hypothesis set, expressed in terms of its surrogate loss estimation error. They are both non-asymptotic and hypothesis set-specific and thus stronger and more informative than Bayes-consistency. However, determining if they hold and deriving these bounds have required a specific proof and analysis for each surrogate loss. Can we derive more general tools and characterizations? This paper provides both a general characterization and an extension of \(\)-consistency bounds for multi-class classification. We present new and tight \(\)-consistency bounds for both the family of constrained losses and that of comp-sum losses, which covers the familiar cross-entropy, or logistic loss applied to the outputs of a neural network. We further extend our analysis beyond the completeness assumptions adopted in previous studies and cover more realistic bounded hypothesis sets. Our characterizations are based on error transformations, which are explicitly defined for each formulation. We illustrate the application of our general results through several special examples. A by-product of our analysis is the observation that a recently derived multi-class \(\)-consistency bound for cross-entropy reduces to an excess bound and is not significant. Instead, we prove a much stronger and more significant guarantee.

## 1 Introduction

Bayes-consistency is an important property of surrogate loss functions. It requires that minimizing the surrogate excess error over the family of all measurable functions leads to the minimization of the target error loss in the limit Steinwart (2007). This property applies to a broad family of convex margin-based losses in binary classification Zhang (2004); Bartlett et al. (2006), as well as some extensions in multi-class classification Tewari and Bartlett (2007). However, Bayes-consistency does not apply to the hypothesis sets commonly used for learning, such as the family of linear models or that of neural networks, which of course do not include all measurable functions. Furthermore, it is also only an asymptotic property and does not supply any convergence guarantee.

To address these limitations, a series of recent publications by Awasthi et al. (2022) introduced the key notion of _\(\)-consistency bounds_ for surrogate loss functions. These are upper bounds on the zero-one estimation error of any predictor in a hypothesis set, expressed in terms of its surrogate loss estimation error. They are both non-asymptotic and hypothesis set-specific and thus stronger and more informative than Bayes-consistency. However, determining the validity of these bounds and deriving them have required a specific proof and analysis for each surrogate loss. Can we derive more general tools and characterizations for \(\)-consistency bounds?This paper provides both a general characterization and an extension of \(\)-consistency bounds for multi-class classification. Previous approaches to deriving these bounds required the development of new proofs for each specific case. In contrast, we introduce the general concept of an _error transformation function_ that serves as a very general tool for deriving such guarantees with tightness guarantees. We show that deriving an \(\)-consistency bound for comp-sum losses and constrained losses for both complete and bounded hypothesis sets can be reduced to the calculation of their corresponding error transformation function. Our general tools and tight bounds show several remarkable advantages: first, they improve existing bounds for complete hypothesis sets previously proven in ; second, they encompass all previously comp-sum and constrained losses studied thus far as well as many new ones ; third, they extend beyond the completeness assumption adopted in previous work; fourth, they provide novel guarantees for bounded hypothesis sets; and, finally, they help prove a much stronger and more significant guarantee for logistic loss with linear hypothesis set than .

**Previous work.** Here, we briefly discuss recent studies of \(\)-consistency bounds by Awasthi et al. , Mao et al.  and Zheng et al. . Awasthi et al.  introduced and studied \(\)-consistency bounds in binary classification. They provided a series of _tight_\(\)-consistency bounds for _bounded_ hypothesis set of linear models and one-hidden-layer neural networks. The subsequent study  further generalized the framework to multi-class classification and presented an extensive study of \(\)-consistency bounds for diverse multi-class surrogate losses, including negative results for _max losses_ and positive results for _sum losses_, and _constrained losses_. However, the hypothesis sets examined in their analysis were assumed to be complete, which rules out the bounded hypothesis sets typically used in practice. Moreover, the final bounds derived from  are based on ad hoc methods and may not be tight.  complemented this previous work by studying a wide family of _comp-sum losses_ in the multi-class classification, which generalizes the _sum-losses_ and includes as special cases the logistic loss , the _generalized cross-entropy loss_, and the _mean absolute error loss_. Here too, the completeness assumption on the hypothesis sets was adopted and their \(\)-consistency bounds do not apply to common bounded hypothesis sets in practice. Recently, Zheng et al.  proved \(\)-consistency bounds for multi-class logistic loss with bounded linear hypothesis sets. However, their bounds require a crucial distributional assumption, under which the mimizability gaps coincide with the approximation errors. Thus, their bounds can be recovered as excess error bounds, which are less significant.

Other related work on \(\)-consistency bounds includes \(\)-consistency bounds for pairwise ranking ; theoretically grounded surrogate losses and algorithms for learning with abstention supported by \(\)-consistency bounds, including the study of score-based abstention , predictor-rejector abstention  and learning to abstain with a fixed predictor with application in decontextualization ; Andor, Choi, Collins, Mao, and Zhong ; principled approaches for learning to defer with multiple experts that benefit from strong \(\)-consistency bounds, including the single-stage scenario  and a two-stage scenario ; \(\)-consistency theory and algorithms for adversarial robustness ; and efficient algorithms and loss functions for structured prediction with stronger \(\)-consistency guarantees .

**Structure of this paper.** We present new and tight \(\)-consistency bounds for both the family of comp-sum losses (Section 4.1) and that of constrained losses (Section 5.1), which cover the familiar cross-entropy, or logistic loss applied to the outputs of a neural network. We further extend our analysis beyond the completeness assumptions adopted in previous studies and cover more realistic bounded hypothesis sets (Section 4.2 and 5.2). Our characterizations are based on error transformations, which are explicitly defined for each formulation. We illustrate the application of our general results through several special examples. A by-product of our analysis is the observation that a recently derived multi-class \(\)-consistency bound for cross-entropy reduces to an excess bound independent of the hypothesis set. Instead, we prove a much stronger and more significant guarantee (Section 4.2).

We give a comprehensive discussion of related work in Appendix A. We start with some basic definitions and notation in Section 2.

## 2 Preliminaries

We denote by \(\) the input space, by \(\) the output space, and by \(\) a distribution over \(\). We consider the standard scenario of multi-class classification, where \(=\{1,,n\}\). Given a hypothesis set \(\) of functions mapping \(\) to \(\), the multi-class classification problem consists of finding a hypothesis \(h\) with small generalization error \(_{_{0-1}}(h)\), defined by \(_{_{0-1}}(h)=_{(x,y)}[_{0-1}(h,x,y)]\), where \(_{0-1}(h,x,y)=_{h(x) y}\) is the multi-class zero-one loss with \((x)=*{argmax}_{y}h(x,y)\) the prediction of \(h\) for the input point \(x\). We also denote by \((x)\) the set of all predictions associated to input \(x\) generated by functions in \(\), that is, \((x)=\{(x) h\}\).

We will analyze the guarantees of surrogate multi-class losses in terms of the zero-one loss. We denote by \(\) a surrogate loss and by \(_{}(h)\) its generalization error, \(_{}(h)=_{(x,y)}[(h,x,y)]\). For a loss function \(\), we define the best-in-class generalization error within a hypothesis set \(\) as \(_{}^{*}()=_{h}_{}(h)\), and refer to \(_{}(h)-_{}^{*}()\) as the _estimation error_. We will study the key notion of _\(\)-consistency bounds_(Awasthi et al., 2022a,b), which are upper bounds on the zero-one estimation error of any predictor in a hypothesis set, expressed in terms of its surrogate loss estimation error, for some real-valued function \(f\) that is non-decreasing:

\[ h,\ _{_{0-1}}(h)-_{_{0-1}}^{* }() f(_{}(h)-_{}^{*}( )).\]

These bounds imply that the zero-one estimation error is at most \(f()\) whenever the surrogate loss estimation error is bounded by \(\). Thus, the learning guarantees provided by \(\)-consistency bounds are both non-asymptotic and hypothesis set-specific. The function \(f\) appearing in these bounds is expressed in terms of a _minimizability gap_, which is a quantity measuring the difference of best-in-class error \(_{}^{*}()\) and the expected _best-in-class conditional error_\(_{x}[_{}^{*}(,x)]\): \(_{}()=_{}^{*}()-_{X}[_{}^{*}(,x)]\), where \(_{}(h,x)=_{y|x}[(h,x,y)]\) and \(_{}^{*}(,x)=_{h}_{ }(h,x)\) are the _conditional error_ and _best-in-class conditional error_ respectively. We further write \(_{,}=_{}(h,x)-_{ }^{*}(,x)\) to denote the _conditional regret_. Note that that the minimizability gap is an inherent quantity depending on a hypothesis set \(\) and the loss function \(\).

By Lemma 1, the minimizability gap for the zero-one loss, \(_{_{0-1}}()\), coincides with its approximation error \(_{_{0-1}}()=_{_{0-1}}^{*}()-_{_{0-1}}^{*}(_{})\) when the set of all possible predictions generated by \(\) covers the label space \(\). This holds for typical hypothesis sets used in practice. However, for a surrogate loss \(\), the minimizability gap \(_{}()\) is always upper bounded by and in general finer than its approximation error \(_{}()=_{}^{*}()- _{}^{*}(_{})\) since \(_{}()=_{}()-I_{}( )\), where \(_{}\) is the family of all measurable functions and \(I_{}()=_{x}[_{}^{*}(,x )-_{}^{*}(_{},x)]\) (see Appendix B for a more detailed discussion). Thus, an \(\)-consistency bound, expressed as follows for some increasing function \(\):

\[_{_{0-1}}(h)-_{_{0-1}}^{*}()+_{_{0-1}}()(_{}(h)-_{}^ {*}()+_{}()),\] (1)

is more favorable than an excess error bound expressed in terms of approximation errors \(_{_{0-1}}(h)-_{_{0-1}}^{*}()+_{_{0-1}}()(_{}(h)-_{} ^{*}()+_{}())\). Here, \(\) is typically linear or the square-root function modulo constants. When \(=_{}\), the family of all measurable functions, an \(\)-consistency bound coincides with the excess error bound and implies Bayes-consistency by taking the limit. It is therefore a stronger guarantee than an excess error bound and Bayes-consistency.

The minimizability gap is always non-negative, since the infimum of the expectation is greater than or equal to the expectation of infimum. Furthermore, as shown in Appendix B, when \(\) is the family of all measurable functions or when the Bayes-error coincides with the best-in-class error, that is, \(_{}^{*}()=_{}^{*}(_{ })\), the minimizability gap vanishes. In such cases, (1) implies the \(\)-consistency of a surrogate loss \(\) with respect to the zero-one loss \(_{0-1}\):

\[_{}(h_{n})-_{}^{*}()0_{_{0-1}}(h_{n})-_{_{0-1}}^{*}( )0.\]

In the next sections, we will provide both a general characterization and an extension of \(\)-consistency bounds for multi-class classification. Before proceeding, we first introduce a useful lemma from (Awasthi et al., 2022b) which characterizes the conditional regret of zero-one loss explicitly. We denote by \(p(x)=(p(x,1),,p(x,n))\) as the conditional distribution of \(y\) given \(x\).

**Lemma 1**.: _For zero-one loss \(_{0-1}\), the best-in-class conditional error and the conditional regret for \(_{0-1}\) can be expressed as follows: for any \(x\), we have_

\[_{_{0-1}}^{*}(,x)=1-_{y(x)}p(x,y) _{_{0-1},}(h,x)=_{y (x)}p(x,y)-p(x,(x)).\]Comparison with previous work

Here, we briefly discuss previous studies of \(\)-consistency bounds (Awasthi et al., 2022a,b, Zheng et al., 2023; Mao et al., 2023) in standard binary or multi-class classification and compare their results with those we present.

Awasthi et al. (2022a) studied \(\)-consistency bounds in binary classification. They provided a series of _tight_\(\)-consistency bounds for the _bounded_ hypothesis set of linear models \(_{}^{}\) and one-hidden-layer neural networks \(_{}^{}\), defined as follows:

\[_{}^{}=x w x +b\|w\| W,|b| B}\] \[_{}^{}=x_{j= 1}^{n}u_{j}(w_{j} x+b)_{+}\|u\|_{1},\|w_{j}\| W,|b| B },\]

where \(B\), \(W\), and \(\) are positive constants and where \(()_{+}=(,0)\). We will show that our bounds recover these binary classification \(\)-consistency bounds.

The scenario of multi-class classification is more challenging and more crucial in applications. Recent work by Awasthi et al. (2022b) showed that _max losses_(Crammer and Singer, 2001), defined as \(^{}(h,x,y)=_{y^{} y}h(x,y)-h(x,y^{}) \) for some convex and non-increasing function \(\), cannot admit meaningful \(\)-consistency bounds, unless the distribution is deterministic. They also presented a series of \(\)-consistency bounds for _sum losses_(Weston and Watkins, 1998) and _constrained losses_(Lee et al., 2004) for _symmetric_ and _complete_ hypothesis sets, that is such that:

\[=\{h:  h(,y), y\}\] (symmetry) \[ x,\{f(x) f\}=,\] (completeness)

for some family \(\) of functions mapping from \(\) to \(\). The completeness assumption rules out the bounded hypothesis sets typically used in practice such as \(_{}\). Moreover, the final bounds derived from (Awasthi et al., 2022b) are based on ad hoc proofs and may not be tight. In contrast, we will study both the complete and bounded hypothesis sets, and provide a very general tool to derive \(\)-consistency bounds. Our bounds are tighter than those of Awasthi et al. (2022b) given for complete hypothesis sets and extend beyond the completeness assumption.

(Mao et al., 2023) complemented the work of (Awasthi et al., 2022b) by studying a wide family of _comp-sum losses_ in multi-class classification, which generalized the _sum-losses_ and included as special cases the logistic loss (Verhulst, 1838, 1845; Berkson, 1944, 1951), the _generalized cross-entropy loss_(Zhang and Sabuncu, 2018), and the _mean absolute error loss_(Ghosh et al., 2017). Here too, the completeness assumption was adopted, thus their \(\)-consistency bounds do not apply to common bounded hypothesis sets used in practice. We illustrate the application of our general results through a broader set of surrogate losses than (Mao et al., 2023) and significantly generalize the bounds of (Mao et al., 2023) to bounded hypothesis sets.

Recently, Zheng et al. (2023) proved \(\)-consistency bounds for logistic loss with linear hypothesis sets in the multi-class classification: \(_{}=\{x w_{y} x+b_{y}\|w_{y}\| W,|b _{y}| B,y\}\). However, their bounds require a crucial distributional assumption under which, the minimizability gaps \(_{_{0}}(_{})\) and \(_{_{}}(_{})\) coincide with the approximation errors \(_{_{0}1}(_{})_{_{0}1}^{*}(_{})\) and \(_{_{}}(_{})_{_{}}(_{})\) respectively (see the note before (Zheng et al., 2023, Appendix F)). Thus, their bounds can be recovered as excess error bounds \(_{_{0}1}(h)-_{_{0}1}^{*}( _{})_{_{} }(h)-_{_{}}^{*}(_{}) ^{}\), which are less significant. In contrast, our \(_{}\)-consistency bound are much finer and take into account the role of the parameter \(B\) and that of the number of labels \(n\). Thus, we provide stronger and more significant guarantees for logistic loss with linear hypothesis set than (Zheng et al., 2023).

In summary, our general tools offer the remarkable advantages of deriving tight bounds, which improve upon the existing bounds of Awasthi et al. (2022b) given for complete hypothesis sets, cover the comp-sum and constrained losses considered in (Awasthi et al., 2022a; Mao et al., 2023) as well as new ones, extend beyond the completeness assumption with novel guarantees valid for bounded hypothesis sets, and are much stronger and more significant guarantees for logistic loss with linear hypothesis sets than those of Zheng et al. (2023).

[MISSING_PAGE_FAIL:5]

   Auxiliary function \(\) & \(-(t)\) & \(-1\) & \((1-t^{q}),q(0,1)\) & \(1-t\) & \((1-t)^{2}\) \\  Transformation \(^{}\) & \((1+t)+(1-t)\) & \(1-}\) & \(}}(+(1-t)^{2t}}}{2} )^{1-q}-}}\) & \(\) & \(}{4}\) \\   

Table 1: \(\)-estimation error transformation for common comp-sum losses.

_with \(}^{}\) the \(}\)-estimation error transformation for comp-sum losses defined for all \(t\) by \(}^{}(t)=\)_

\[_{[0,]\{s_{}-, 1--s_{}\}}\{()-(1--) +(1-)-(+)\}&n=2\\ P,1}_{S_{}  1S_{}_{1} S_{}}_{  C}\{(_{2})-(_{1}-)+(_{1})-(_{2}+)\}&n>2,\]

_where \(C=[\{s_{}-_{2},_{1}-s_{}\},\{s_{}-_{2},_{ 1}-s_{}\}]\), \(s_{}=(x)}}\) and \(s_{}=(x)}}\). Furthermore, for any \(t\), there exist a distribution \(\) and \(h\) such that \(_{_{0-1}}(h)-_{_{0-1}}^{*}()+ _{_{0-1}}()=t\) and \(_{^{}}(h)-_{^{}}^{*}( )+_{^{}}()=^{ }(t)\)._

This theorem significantly broadens the applicability of our framework as it encompasses bounded hypothesis sets. The last statement of the theorem further shows the tightness of the \(\)-consistency bounds derived using this error transformation function. We now illustrate the application of our theory through several examples.

**A. Example: logistic loss.** We first consider the multinomial logistic loss, that is \(^{}\) with \((u)=-(u)\), for which we give the following guarantee.

**Theorem 6** (\(}\)**-consistency bounds for logistic loss).: _For any \(h}\) and any distribution, we have_

\[_{_{0-1}}(h)-_{_{0-1}}^{*}(})+_{_{0-1}}}^{-1} _{_{}}(h)-_{_{}}^{*} }+_{_{}}},\]

_where \(_{}=-}{_{y^{} y}e^{h(x,y^{ })}}\) and \((t)=(1+t)+(1-t)&t {s_{}-s_{}}{s_{}+s_{}}\\ }{s_{}}+s_{}}}{s_{}+s_{}}&.\)_

The proof of Theorem 6 is given in Appendix E.2. With the help of some simple calculations, we can derive a simpler upper bound:

\[^{-1}(t)(t)=&t-s_{ })^{2}}{2(s_{}+s_{})^{2}}\\ +s_{})}{s_{}-s_{}}t&.\]

When the relative difference between \(s_{}\) and \(s_{}\) is small, the coefficient of the linear term in \(\) explodes. On the other hand, making that difference large essentially turns \(\) into a square-root function for all values. In general, \(\) is not infinite since a regularization is used, which controls both the complexity of the hypothesis set and the magnitude of the scores.

**Comparison with (Mao et al., 2023h).** For the symmetric and complete hypothesis sets \(\) considered in (Mao et al., 2023h), \((x)=+\), \(s_{}=1\), \(s_{}=0\), \((t)=(1+t)+(1-t)\) and \((t)=\). By Theorem 6, this yields an \(\)-consistency bound for the logistic loss.

**Corollary 7** (\(\)**-consistency bounds for logistic loss).: _Assume that \(\) is symmetric and complete. Then, for any \(h\) and any distribution, we have_

\[_{_{0-1}}(h)-_{_{0-1}}^{*}()^{- 1}_{_{}}(h)-_{_{}}^{*}()+_{_{}}()-_{_{0-1}}( )\]

_where \((t)=(1+t)+(1-t)\) and \(^{-1}(t)\)._

Corollary 7 recovers the \(\)-consistency bounds of Mao et al. (2023h).

**Comparison with (Awasthi et al., 2022a) and (Zheng et al., 2023).** For the linear models \(_{}=(x,y) w_{y} x+b_{y}w _{y} W,|b_{y}| B}\), we have \((x)=W\|x\|+B\). By Theorem 6, we obtain \(_{}\)-consistency bounds for logistic loss.

**Corollary 8** (\(_{}\)**-consistency bounds for logistic loss).: _For any \(h_{}\) and any distribution,_

\[_{_{0-1}}(h)-_{_{0-1}}^{*}(_{})^{-1}_{_{}}(h)-_{_{}}^{*} (_{})+_{_{}}(_{}) -_{_{0-1}}(_{})\]

_where \((t)=(1+t)+(1-t)&t-e^{-2B})}{2+(n-1)(e^{2B}+e^{-2B})}\\ }{1+(n-1)e^{-2B}}+ )}(1+(n-1)e^{-2B})}{2+(n-1)(e^{2B}+e^{-2B})}& .\)_For \(n=2\), we have \((t)=(t+1)+(1-t)&t-1}{e^{2B}+1}\\ }{1+e^{-2B}}+)(1+e^{-2B})}}{2+e^{2B}+e^{-2B}}&,\) which coincides with the \(_{ lin}\)-estimation error transformation in [Awasthi et al., 2022a]. Thus, Corollary 8 includes as a special case the \(_{ lin}\)-consistency bounds given by Awasthi et al. [2022a] for binary classification.

Our bounds of Corollary 8 improves upon the multi-class \(_{ lin}\)-consistency bounds of recent work [Zheng et al., 2023, Theorem 3.3] in the following ways: i) their bound holds only for restricted distributions while our bound holds for any distribution; ii) their bound holds only for restricted values of the estimation error \(_{_{}}(h)-_{_{}}^{*}(_{ lin})\) while ours holds for any value in \(\) and more precisely admits a piecewise functional form; iii) under their distributional assumption, the mimizability gaps \(_{_{-1}}(_{ lin})\) and \(_{_{}}(_{ lin})\) coincide with the approximation errors \(_{_{0-1}}(_{ lin})-_{_{0-1}}^{*}( _{ all})\) and \(_{_{}}(_{ lin})-_{_{}}^{*} (_{ all})\) respectively (see the note before [Zheng et al., 2023, Appendix F]). Thus, their bounds can be recovered as an excess error bound \(_{_{0-1}}(h)-_{_{0-1}}^{*}(_{ all })_{_{}}(h)-_{_{}}^ {*}(_{ all})^{}\), which is not specific to the hypothesis set \(\) and thus not as significant. In contrast, our \(_{ lin}\)-consistency bound is finer and takes into account the role of the parameter \(B\) as well as the number of labels \(n\); iv) [Zheng et al., 2023, Theorem 3.3] only offers approximate bounds that are not tight; in contrast, by Theorem 5, our bound is tight.

Note that our \(\)-consistency bound in Theorem 6 are not limited to specific hypothesis set forms. They are directly applicable to various types of hypothesis sets including neural networks. For example, the same derivation can be extended to one-hidden-layer neural networks studied in [Awasthi et al., 2022a] and their multi-class generalization by calculating and substituting the corresponding \((x)\). As a result, we can obtain novel and tight \(\)-consistency bounds for bounded neural network hypothesis sets in multi-class classification, which highlights the versatility of our general tools.

**B. Example: sum exponential loss**. We then consider the sum exponential loss, that is \(^{ comp}\) with \((u)=\). By computing the error transformation in Theorem 5, we obtain the following result.

**Theorem 9** (\(}\)-consistency bounds for sum exponential loss).: _For any \(h\) and any distribution,_

\[_{_{0-1}}(h)-_{_{0-1}}^{*}}+_{_{0-1}}} ^{-1}_{_{ exp}}(h)-_{ _{ exp}}^{*}}+_{_{ exp }}}\]

_where \(_{ exp}=_{y^{} y}e^{h(x,y^{})-h(x,y)}\) and \((t)=1-}&t^{2}-s_{ min}^{2 }}{s_{ min}^{2}+s_{ min}^{2}}\\ -s_{ min}}{2s_{ max}s_{ min}}t--s_ { min})^{2}}{2s_{ max}s_{ min}(s_{ max}+s_{ min})}&.\)_

The proof of Theorem 9 is given in Appendix E.3. Observe that \(1-} t^{2}/2\). By Theorem 9, making \(s_{ min}\) close to zero, that is making \(\) close to infinite for any \(x\), essentially turns \(\) into a square function for all values. In general, \(\) is not infinite since a regularization is used in practice, which controls both the complexity of the hypothesis set and the magnitude of the scores.

**C. Example: generalized cross-entropy loss and mean absolute error loss**. Due to space limitations, we present the results for these loss functions in Appendix E.

## 5 Constrained losses

In this section, we present a general characterization of \(\)-consistency bounds for _constrained loss_, that is loss functions defined via a constraint, as in [Lee et al., 2004]:

\[^{ cstnd}(h,x,y)=_{y^{} y}(-h(x,y^{}))\] (5)

with the constraint that \(_{y y}h(x,y)=0\) for a non-negative and non-increasing auxiliary function \(\).

### \(\)-consistency bounds

As in the previous section, we prove a result that supplies a very general tool, an _error transformation function_ for deriving \(\)-consistency bounds for constrained losses. When \(^{ cstnd}\) is convex, to make these guarantees explicit, we only need to calculate \(^{ cstnd}\).

**Theorem 10** (\(\)-consistency bound for constrained losses).: _Assume that \(\) is symmetric and complete. Assume that \(^{}\) is convex. Then, for any hypothesis \(h\) and any distribution,_

\[^{}_{_{0-1}}(h)- ^{*}_{_{0-1}}()+_{_{0-1}}()_{^{}}(h)-^{*}_{^{ }}()+_{^{}}()\]

_with \(\)-estimation error transformation for constrained losses defined on \(t\) by \(^{}(t)=_{ 0}_{ }\{()-(-+)+ (-)-(-)\}&n=2\\ _{P,1_{1}\{_{2},0\}} _{}\{(-_{2})-(-_{1}+ )+(-_{1})-(-_{2}-) \}&n>2.\]

_Furthermore, for any \(t\), there exist a distribution \(\) and a hypothesis \(h\) such that \(_{_{0-1}}(h)-^{*}_{_{0-1}}()+_{_{0-1}}()=t\) and \(_{^{}}(h)-^{*}_{^{}}( )+_{^{}}()=^{ }(t)\)._

Here too, the theorem guarantees the tightness of the bound. This general expression of \(^{}\) can be considerably simplified under some broad assumptions, as shown by the following result.

**Theorem 11** (**characterization of \(^{}\))**.: _Assume that \(\) is convex, differentiable at zero and \(^{}(0)<0\). Then, \(^{}\) can be expressed as follows:_

\[^{}(t) =(0)-_{}()+(-)}&n=2\\ _{ 0}2-(-)-_{ }}{2}-++ {2+t-}{2}--}}&n>2\] \[(0)-_{}()+(-)}&n=2\\ _{ 0}2(-)-_{}(-+)+(--)}}&n>2.\]

The proof of all the results in this section are given in Appendix D.

**Examples.** We now compute the \(\)-estimation error transformation for constrained losses and present the results in Table 2. Here, we present the simplified \(^{}\) by using the lower bound in Theorem 11. Remarkably, by applying Theorem 10, we are able to obtain tighter \(\)-consistency bounds for constrained losses with \(=_{},_{},_{}\) than those derived using ad hoc methods in (Awasthi et al., 2022), and a novel \(\)-consistency bound for the new constrained loss \(^{}(h,x,y)=_{y^{} y}(1+h(x,y^{}))^{2}\) with \((t)=(1-t)^{2}\).

### Extension to non-complete or bounded hypothesis sets

As in the case of comp-sum losses, we extend our results beyond the completeness assumption adopted in previous work and establish \(}\)-consistency bounds for bounded hypothesis sets. This significantly broadens the applicability of our framework.

**Theorem 12** (\(}\)-consistency bound for constrained losses).: _Assume that \(}^{}\) is convex. Then, the following inequality holds for any hypothesis \(h}\) and any distribution:_

\[}^{}_{_{0-1} }(h)-^{*}_{_{0-1}}(})+_{_{0 -1}}(})_{^{}}(h)- ^{*}_{^{}}(})+_{^ {}}(}).\] (6)

_with \(}^{}\) the \(}\)-estimation error transformation for constrained losses defined for all \(t\) by \(}^{}(t)=_{ 0} _{\{-_{},+_{}\}} ()-(-+)+(-)-( -)}&n=2\\ _{P,1_{1}\{_{2},0\}} _{ C}\{(-_{2})-(-_{1}+) +(-_{1})-(-_{2}-)\}&n>2, \]

_where \(C=\{_{1},-_{2}\}-_{},\{_{1},-_{2}\}+ _{}\) and \(_{}=_{x}(x)\). Furthermore, for any \(t\), there exist a distribution \(\) and a hypothesis \(h\) such that \(_{_{0-1}}(h)-^{*}_{_{0-1}}()+_{^{}}(h)-^{*}_{^{}}( )+_{^{}}()=^{ }t)\)._

  Auxiliary function \(\) & \(_{}(t)=e^{-t}\) & \(_{}(t)=(0,1-t)\) & \(_{}(t)=(1-t)^{2}_{t 1}\) & \(_{}=(1-t)^{2}\) \\  Transformation \(^{}\) & \(^{}(t)=2-}\) & \(^{}(t)=t\) & \(^{}(t)=}{2}\) & \(^{}(t)=}{2}\) \\  

Table 2: \(\)-estimation error transformation for common constrained losses.

The proof is presented in Appendix F.1. Next, we illustrate the application of our theory through an example of constrained exponential losses, that is \(^{}\) with \((t)=e^{-t}\). By using the error transformation in Theorem 12, we obtain new \(}\)-consistency bounds in Theorem 13 (see Appendix F.2 for the proof) for bounded hypothesis sets \(}\).

**Theorem 13** (\(}\)**-consistency bounds for constrained exponential loss)**.: _Let \((t)=e^{-t}\). For any \(h}\) and any distribution,_

\[_{_{0-1}}(h)-_{_{0-1}}^{*} }+_{_{0-1}}}^{-1}_{^{}}(h)- _{^{}}^{*}}+ _{^{}}}\]

_where \((t)=1-}&t}-1}{e^{2 _{}}+1}.\\ e^{_{}}-e^{-_{}}+}-e^{-_{}}}{2}&.\)_

Awasthi et al. (2022b) proves \(\)-consistency bounds for constrained exponential losses when \(\) is symmetric and complete. Theorem 13 significantly generalizes those results to the non-complete hypothesis sets. Different from the complete case, the functional form of our new bounds has two pieces which corresponds to the linear and the square root convergence respectively, modulo the constants. Furthermore, the coefficient of the linear piece depends on the the magnitude of \(_{}\). When \(_{}\) is small, the coefficient of the linear term in \(^{-1}\) explodes. On the other hand, making \(_{}\) large essentially turns \(^{-1}\) into a square-root function.

## 6 Discussion

Here, we further elaborate on the practical value of our tools and \(\)-consistency bounds. Our contributions include a more general and convenient mathematical tool for proving \(\)-consistency bounds, along with tighter bounds that enable a better comparison of surrogate loss functions and extensions beyond previous completeness assumptions. As mentioned by (Awasthi et al., 2022b), given a hypothesis set \(\), \(\)-consistency bounds can be used to compare different surrogate loss functions and select the most favorable one, which depends on the functional form of the \(\)-consistency bound; the smoothness of the surrogate loss and its optimization properties; approximation properties of the surrogate loss function controlled by minimizability gaps; and the dependency on the number of classes in the multiplicative constant. Consequently, a tighter \(\)-consistency bound provides a more accurate comparison, as a loose bound might not adequately capture the full advantage of using one surrogate loss. In contrast, Bayes-consistency does not take into account the hypothesis set and is an asymptotic property, thereby failing to guide the comparison of different surrogate losses.

Another application of our \(\)-consistency bounds involves deriving generalization bounds for surrogate loss minimizers (Mao et al., 2023), expressed in terms of the same quantities previously discussed. Therefore, when dealing with finite samples, a tighter \(\)-consistency bound could also result in a corresponding tighter generalization bound. Moreover, our novel results extend beyond previous completeness assumptions, offering guarantees applicable to bounded hypothesis sets commonly used with regularization. This enhancement provides meaningful learning guarantees. Technically, our error transformation function serves as a very general tool for deriving \(\)-consistency bounds with tightness guarantees. These functions are defined within each class of loss functions including comp-sum losses and constrained losses, and their formulation depends on the structure of the individual loss function class, the range of the hypothesis set and the number of classes. To derive explicit bounds, all that is needed is to calculate these error transformation functions. Under some broad assumptions on the auxiliary function within a loss function, these error transformation functions can be further distilled into more simplified forms, making them straightforward to compute.

## 7 Conclusion

We presented a general characterization and extension of \(\)-consistency bounds for multi-class classification. We introduced new tools for deriving such bounds with tightness guarantees and illustrated their benefits through several applications and examples. Our proposed method is a significant advance in the theory of \(\)-consistency bounds for multi-class classification. It can provide a general and powerful tool for deriving tight bounds for a wide variety of other loss functions and hypothesis sets. We believe that our work will open up new avenues of research in the field of multi-class classification consistency.