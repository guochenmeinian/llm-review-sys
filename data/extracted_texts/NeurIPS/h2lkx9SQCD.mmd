# Faster Differentially Private Convex Optimization

via Second-Order Methods

 Arun Ganesh

Google Research

Mahdi Haghifam

University of Toronto,

Vector Institute

&Thomas Steinke

Google DeepMind

Abhradeep Thakurta

Google DeepMind

###### Abstract

Differentially private (stochastic) gradient descent is the workhorse of DP private machine learning in both the convex and non-convex settings. Without privacy constraints, second-order methods, like Newton's method, converge faster than first-order methods like gradient descent. In this work, we investigate the prospect of using the second-order information from the loss function to accelerate DP convex optimization. We first develop a private variant of the regularized cubic Newton method of Nesterov and Polyak , and show that for the class of strongly convex loss functions, our algorithm has quadratic convergence and achieves the optimal excess loss. We then design a practical second-order DP algorithm for the unconstrained logistic regression problem. We theoretically and empirically study the performance of our algorithm. Empirical results show our algorithm consistently achieves the best excess loss compared to other baselines and is \(10\)-\(40\) faster than DP-GD/DP-SGD for challenging datasets.

## 1 Introduction

Many machine learning tasks reduce to a convex optimization problem. More precisely, given a dataset \(S_{n}=(z_{1},,z_{n})^{n}\), a closed, convex set \(^{d}\), and a loss function \(f:\) such that, for every \(z\), \(f(w,z)\) is a convex function in \(w\), our goal is to compute an approximation to \(_{w}\,\,((w,S_{n})_{i [n]}f(w,z_{i})).\) In this paper, we are interested in the problem of designing optimization algorithms in the scenario that the dataset \(S_{n}\) contains private information. Differential privacy (DP)  is a formal standard for privacy-preserving data analysis that provides a framework for ensuring that the output of an analysis on the data does not leak this private information. This problem is known as _private convex optimization_: Design an algorithm \(:^{n}\) that is both DP and ensures low _excess loss_\(((S_{n}),S_{n})-_{w}(w,S_{n}).\)

The predominant algorithm for private convex optimization is DP (stochastic) gradient descent (DP-GD/DP-SGD). This is a _first-order_ iterative method. I.e., we start with an initial value \(w_{0}\) and iteratively update it using the gradient of the loss \(_{w_{t}}(w_{t},S_{n})\) following the update rule \(w_{t+1}=w_{t}-(_{w_{t}}(w_{t},S_{n})+_{t}),\) where \(>0\) is a constant and \(_{t}\) is Gaussian noise to ensure privacy. The number of iterations \(T\) also determines the amount of noise at each iteration, i.e., the scale of \(_{t}\) is proportional to \(\) due to the composition of DP. Note that we assume \(\|_{w_{t}}(w_{t},S_{n})\| 1\).

One of the major drawbacks of DP-(S)GD is _slow convergence_. The choice of \((,T)\) exhibits a tradeoff in terms of the excess loss: if \( T\) is small, the algorithm cannot reach the optimal solution; on the other hand, the magnitude of noise at each iteration is \(\), which cannot be too large. Therefore, to maximize \( T\) and minimize \(\), implementations of DP-(S)GD err on the side of

[MISSING_PAGE_FAIL:2]

contained in the larger eigenvalues/vectors of the Hessian. We prove the local convergence of the update rule (1) in Section 5.3 and perform a thorough empirical evaluation Section 6. We demonstrate that our algorithm outperforms existing baselines on a variety of benchmarks.

**Ensuring Global Convergence.** One limitation of the update rule in Equation (1) is it does not converge globally (even without noise added for DP). That is, if the initial point \(w_{0}\) is too far from the optimal solution, then the iterates may diverge. To address this problem, we propose a variant of Newton's update rule where we replace the Hessian with a different form of second-order information which gives a _Quadratic Upperbound_ (QU) on the logistic loss. This is _guaranteed to converge globally_, like the cubic Newton approach. And we show numerically that this algorithm converges almost as fast as the regular Newton's method in the private setting. Figure 1 shows the convergence speed of our algorithms and DP-GD in terms of real wall time for the task of logistic regression on the Covertype dataset for \((,)=(1,()^{-2})\)-DP. Despite DP-GD having a lower per-iteration cost, our algorithm is \(30\) faster than DP-GD and achieves better excess loss.

**Stochastic Minibatch Variant.** We also show that our algorithms naturally extend to the minibatch setting where gradient and second-order information are computed on a subset of samples. We numerically compare it with DP-SGD and show that it has faster convergence.

## 2 Related Work

DP optimization is a well-studied topic [e.g., SCS13; MRTZ17; ACGM+16; STU17; WLKC+17; INST+19; STT20; SSTT21; GTU22; GLL22; BFTG19; BST14]. Most similar to our work, Avella-Medina, Bradshaw, and Loh  consider second-order methods for DP convex optimization. We provide a detailed comparison between our results and theirs in Remark 4.5 and Section 6 showing that our algorithms relax restrictive assumptions and provide better excess error for logistic regression.

There are numerous non-private second-order optimization methods in the literature. The choice of method depends primarily on the values of \(n\) and \(d\). When \(n\) is large, several works consider various sampling techniques for constructing second-order information, see [15; 16; 17]. When \(d\) is large, various methods are proposed in the literature for efficient approximation of the Hessian matrix, see [1; 1; 18; 19; 20]. There is also a family of algorithms based on the estimation of the curvature from the change in gradients. These algorithms are generally known as quasi-Newton methods stemming from the seminal BFGS algorithm .

## 3 Preliminaries

Let \(d\). For a vector \(x^{d}\), \(\|x\|\) denotes the \(_{2}\) norm of \(x\). Let \(n,m\). For a matrix \(A^{n m}\), \(\|A\|=_{x^{m}:\|x\| 1}\|Ax\|\) denotes the operator norm, and \(\|A\|_{F}(A^{T} A)}\) denotes the Frobenius norm of \(A\) where trace denotes the trace operator. \(I_{d}^{d d}\) denotes the identity matrix. \(,\) denotes the standard inner product in \(^{d}\). For a convex and closed subset \(^{d}\), let \(_{}:^{d}\) be the Euclidean projection operator, given by \(_{}(x)=_{y}\|y-x\|_{2}\). For a (measurable) space \(\), \(_{1}()\) denotes the set of all probability measures on \(\). Note that the statements in the paper about random variables hold almost surely. We will skip such declarations to aid readability. Let \(\) be the data and let \(^{d}\) be the parameter space. Let \(f:\) be a loss function. Throughout the paper, we assume \(f\) is doubly continuous, a convex function in \(w\), and \(\) is a closed and convex set. We say (1) \(f\) is \(_{0}\)_-Lipschitz_ iff there exists \(_{0}\) such that \( z\), \( w,v:\|f(w,z)-f(v,z)\|_{0}\| w-v\|\), (2) \(f\) is \(_{1}\)_-smooth_ iff there exists \(_{1}\) such that \( z\), \( w,v:\| f(w,z)- f(v,z)\| _{1}\|w-v\|\), (3) \(f\) has a \(_{2}\)_-Lipschitz Hessian_ iff there exists \(_{2}\) such that \( z\), \( w,v:\|^{2}f(w,z)-^{2}f(v,z)\| _{2}\|w-v\|\), (4) \(f\) is \(\)_-strongly convex_ iff for all \(w,v\) and \(z\) we have \(f(v,z) f(w,z)+ f(w,z),v-w+ \|v-w\|^{2}\).

### Zero-Concentrated DP

For our privacy analysis, we use concentrated differential privacy [15; 17], as it provides a simpler composition theorem - the privacy parameter \(\) adds up when we compose.

**Definition 3.1** ([16, Def. 1.1]).: A randomized mechanism \(:^{n}_{1}()\) is \(\)-zCDP, iff, for every neighbouring dataset (i.e., addition or removal) \(S_{n}^{n}\) and \(S^{}_{n}^{n}\), and for every \((1,)\), it holds \(_{}((S_{n})\|(S^{}_{n}))\), where \(_{}(_{n}(S_{n})\|_{n}(S^{}_{n}))\) is the \(\)-Renyi divergence between \(_{n}(S_{n})\) and \(_{n}(S^{}_{n})\).

We should think of \(^{2}\): to attain \((,)\)-DP, it suffices to set \(=}{4(1/)+4}\)[16, Lem. 3.5].

**Lemma 3.2** ([16, Prop. 1.3]).: _Assume we have a randomized mechanism \(:_{1}()\) that satisfies \(\)-zCDP, then for every \(>0\), \(\) is \((+2,)\)-DP._

## 4 Optimal Algorithm for the Class of Strongly Convex Functions

In this section, we present a DP variant of the cubic-regularized Newton's method of Nesterov and Polyak . To motivate the idea behind our algorithm, we revisit DP gradient descent (DP-GD) for the class of \(_{0}\)-Lipschitz and \(_{1}\)-smooth convex loss functions.

Let \(\{w_{t}^{}\}_{t[T]}\) be the iterates of DP-GD. The smoothness of \(\) lets us construct a global quadratic upper bound on the function [14, Thm. 2.1.5] as follows \( w\) and \(S_{n}^{n}\) :

\[(w,S_{n}) q_{t}(w)(w_{t}^{},S_{n})+ (w_{t}^{},S_{n}),w-w_{t}^{}+ {_{1}}{2}\|w-w_{t}^{}\|^{2}.\] (2)

Then, DP-GD can be seen as a two-step process:

(Step I) \[v_{t+1}\!=\!_{v}q_{t}(v)\!=\!w_{t}^{}\!-\! _{1}^{-1}(w_{t}^{},S_{n}),\ \ w_{t+1}^{}\!=\!_{}(v_{t+1}+_{1}^{-1} _{t}),\]

where \(_{t}=(0,^{2}I_{d})\) with \(^{2}=_{0}^{2}}{2^{2}}\) so that \(w_{t+1}^{}\) satisfies \(\)-zCDP [16, Lem. 2.5]. That is, in each iteration of DP-GD, _we find a minimum of the quadratic upper bound \(q_{t}(w)\)_ and then project back to \(\). (In the unconstrained setting where \(=^{d}\) we do not need the second projection step.)

Consider the class of \(_{2}\)-Lipschitz Hessian convex loss functions. Nesterov and Polyak [20, Lem. 1] show that we can construct a _global cubic upper bound_ exploiting the second-order information (i.e., Hessian) as follows: for all \(w\) and \(w_{t}\), \((w,S_{n})\!\!_{t}(w)\) where

\[_{t}(w)\!\!(w_{t},\!S_{n})\!+\!(w_{t},S_{n}),w\!-\!w_{t}+\!\!^{2}\!(w_{t},\!S_{n}) (w\!-\!w_{t}),\!w\!-\!w_{t}+\!_{2}}{6}\|w\!- \!w_{t}\|^{3}.\] (3)

Their non-private algorithm is based on the _exact_ minimization of \(_{t}(w)\), i.e., the next iterate is \(w_{t+1}=_{t}(w)\). Note that \(_{t}(w)\) does not admit a closed form solution, as opposed to the quadratic upper bound (2). Similar to the intuition for DP-GD on smooth loss functions (2), our algorithms in this section are based on _privately_ minimizing \(_{t}(w)\) at each iteration. Our algorithm is shown in Algorithm 1. In each iteration the algorithm makes an oracle call to obtain \(((w_{t},S_{n}),(w_{t},S_{n}),^{2}(w_{t},S_{n}))\). Then, the algorithm calls an efficient \(\) for privately optimizing the cubic upper bound (3). The privacy analysis of Algorithm 1 is a direct application of the composition property of zCDP [16, Lemma 2.3]; the output of \(\) at each iteration satisfies \(/T\)-zCDP where \(\) is the total privacy budget and \(T\) is the total number of iterations.

_Remark 4.1_.: \(\) in Algorithm 1 does not affect the _oracle complexity_ of Algorithm 1, as it is applied to the proxy loss \(_{t}(w)\), rather than the underlying loss \((w,S_{n})\). \(\)

```
1:Input: training set \(S_{n}^{n}\), privacy budget \(\)-zCDP, initialization \(w_{0}\), number of iterations \(T\).
2:for\(t=0,,T-1\)do
3: Query \((w_{t},S_{n}),(w_{t},S_{n}),^{2}(w_{t},S_{n})\)
4: Construct \(_{t}(w)\) from Equation (3)
5:\(w_{t+1}=(_{t}(w),/T,w_{t})\)
6:Output \(w_{T}\). ```

**Algorithm 1** Meta Algorithm

```
1:Input: function \(::()=\!+\! g,-_{0} + H(-_{0}),(-_{0}) +_{0}}{6}\|-_{0}\|^{3}\), privacy budget \(\)-zCDP, initialization \(_{0}\).
2:\(N=_{0}+_{1}D+_{2}D^{2})^{2}}{(_{0}+_{1}D)^{2}d},^{2}=_{0}+_{1}D)^{2}}{2}\)
3:for\(i=0,,N-1\)do
4:\(_{i}=\)
5:\(_{i}=g+(_{i}-_{0})+_{2}}{2} \|_{i}-_{0}\|(_{i}-_{0})\).
6:\(_{i+1}=_{}(_{i}-_{i}(_{i}+(0, ^{2}I_{d})))\)
7:Return \(_{i=0}^{N-1}_{i}\) ```

**Algorithm 2** DPSolver

**Theorem 4.2**.: _Let \(f\) be a \(_{0}\)-Lipschitz, \(_{1}\)-smooth, \(_{2}\)-Lipschitz Hessian, and \(\)-strongly convex function. Also, assume that \(^{d}\) has finite diameter \(D\). Let \(w^{}=_{w}(w,S_{n})\). Then, for every \(>0\), \((0,1)\), and \(S_{n}^{n}\) for sufficiently large \(n\), by setting the number of iterations in Algorithm 1 to_

\[T=_{2}}}{^{3/4}}((w_{0},S_{n})-( w^{},S_{n}))^{}+}{d},\]

_and using Algorithm 2 as \(\), we have the following: The output of Algorithm 1, i.e., \(w_{T}\), satisfies \(\)-zCDP and with probability at least \(1-\)_

\[(w_{T},S_{n})-(w^{},S_{n}) _{0}+_{1}D)^{2}(1/)}{ n^{2}}(_{ 2}^{2}_{0}D}{^{3}})^{}\]

_Remark 4.3_.: The lower bound on the excess error of any DP algorithm for the class of strongly convex functions [1, Thm. 5.5] implies that the achievable excess error in Theorem 4.2 is _optimal_ in terms of the dependence on \(d\), \(\), and \(n\). Also, the oracle complexity of our algorithm is an exponential improvement over the oracle complexity of first-order methods . \(\)

_Remark 4.4_.: The proof of Theorem 4.2 suggests that Algorithm 1 has two phases. First, while \(w_{t}\) is far from \(w^{}\), the convergence rate is \(1/T^{4}\). Second, when \(w_{t}\) is close to \(w^{}\), the algorithm exhibits the convergence rate of \(((-T))\). Notice that Algorithm 1 is agnostic to this transition in the sense that we do not have an explicit switching step in Algorithm 1 and Algorithm 2. It is also interesting to note that the transition happens when \(\|w_{t}-w^{}\| 3/4_{2}\). \(\)

_Remark 4.5_ (Comparison with ).: In [1, SS4], the authors propose a DP variant of Newton's method. Their main idea is to add independent noise _directly_ to the Hessian matrix and the gradient vector using the Gaussian mechanism. They also require that _the Hessian be a rank-1 matrix_. The issue with adding noise directly to a full-rank Hessian matrix is that the noise scales with the dimension \(d\), which can lead to a suboptimal excess loss. In contrast, our algorithm has a global convergence without placing restrictions on the rank of the Hessian matrix or the initialization. \(\)

_Remark 4.6_.: We showed in Theorem 4.2 that our algorithm has an exponentially smaller _oracle complexity_ than the first-order methods in terms of the dependence to \(n\). For the class of convex, smooth, Lipschitz, and strongly convex,  proposes a first-order algorithm with an oracle complexity of \(T_{1}=(_{1}}/+(n))\). It is important to note that the _constant_ term in \(T_{1}\) differs from our result, making a direct comparison challenging. It is an interesting question to develop a second-order DP algorithm with a smaller oracle complexity than both the algorithms proposed in  and ours in Algorithm 1. \(\)

_Remark 4.7_.: The cubic Newton method has a non-private convergence rate of \(T^{-2}\) for the class of convex (but not strongly convex) functions [12, Thm. 4]. We leave it as an open question whether there exists a \(\) such that Algorithm 1 achieves an optimal excess error and oracle complexity for convex functions. However, this can be achieved by a DP variant of the first-order accelerated Nesterov's method ; see Appendix A.2. \(\)

## 5 DP Logistic Regression using Second-Order Information

The main limitation of our cubic Newton's method (Algorithm 1) is that each iteration requires solving a nontrivial subproblem. So, despite low oracle complexity, it is computationally expensive. Moreover, many loss functions, such as logistic loss, are not strongly convex in the unconstrained setting. In this section, we aim to develop a fast second-order algorithm for unconstrained logistic regression avoiding this issue. In many real-world classification tasks, the logistic loss is the loss of choice. The logistic loss is a convex surrogate of the 0-1 loss, and satisfies many regularity conditions that give rise to various practical optimization algorithms . Also, note that our results in this section can readily be extended to the class of smooth and convex GLMs.

First, we recall the logistic loss function. Let \(d\) and \(=^{d}(1)\{-1,1\}\) be the dimension and data space, where \(^{d}(1)=\{x^{d}:\|x\| 1\}\) is the unit ball in \(^{d}\). Let \(f_{}:^{d}\) denote the logistic loss function defined as

\[f_{}(w,(x,y))=(1+(-y w,x)).\] (4)

The gradient and Hessian of \(f_{}\) are given by

\[_{w}f_{}(w,(x,y))\!=\!,\ \ _{w}^{2}f_{}(w,\!(x,\!y))\!=\!}{((-)\!+\!())^{2}}.\] (5)Newton's method [14, SS9.5] is based on successively minimizing a _local_ second-order Taylor approximation on the function. Newton's method does not guarantee a global convergence ; the reason is that the second-order Taylor approximation of the logistic loss can greatly underestimate the function. Next we show that it is possible to obtain a quadratic _global upper bound_ on the logistic loss function. We will use this to develop an algorithm that converges globally.

**Lemma 5.1**.: _For every \(v^{d}\), \(x^{d}\), \(w^{d}\), and \(y\{-1,+1\}\), we have_

\[f_{}(w,(x,y)) f_{}(v,(x,y))+ f_{ }(v,(x,y)),w-v+ H_{}(v,(x,y))(w-v),w-v,\]

_where \(H_{}(v,(x,y))}{{2}})}{2  x,v}xx^{}^{d d}\)._

_Remark 5.2_.: Since \(f_{}\) is \(\)-smooth, we can construct a simpler global quadratic upper-bound as follows [12, Thm. 2.1.5]: \(f_{}(w,(x,y)) f_{}(v,(x,y))+ f_{ }(v,(x,y)),w-v+\|w-v\|^{2}.\) Lemma 5.1 is tighter than this, since \(H_{}(v,(x,y))I_{d}\); see Appendix B.2. \(\)

_Remark 5.3_.: The second-order Taylor approximation and our upper bound in Lemma 5.1 both provide a quadratic approximation of the logistic loss. In the remainder of the paper, we write \(H(v,(x,y))\) to refer to both \(^{2}f_{}(v,(x,y))\) and \(H_{}(v,(x,y))\). We refer to \(H(v,(x,y))\) as the second-order information (SOI) and to \(H_{}\) as _quadratic upperbound_ SOI. Finally, notice both \(^{2}f_{}(v,(x,y))\) and \(H_{}(v,(x,y))\) are PSD rank-1 matrices, with maximum eigenvalue \(\|x\|^{2}\). \(\)

### Algorithm Description

We are given a dataset \(S_{n}=((x_{1},y_{1}),,(x_{n},y_{n}))(^{d}(1)\{-1,+1\})^ {n}\) and we aim to minimize \(_{}(w,S_{n})_{i[n]}f_{ }(w,(x_{i},y_{i}))\). Our algorithm iteratively minimizes a quadratic approximation of \(_{}(w,S_{n})\). Consider

\[q_{t}(w)_{}(w_{t},S_{n})+_{ }(w_{t},S_{n}),w-w_{t}+ H(w_{t },S_{n})(w-w_{t}),(w-w_{t}),\] (6)

where \(H(w_{t},S_{n})_{i[n]}H(w_{t},(x_{i},y_{i}))\). In the non-private setting the next iterate is set to \(w_{t+1}=_{w}q_{t}(w)=w_{t}-H(w_{t},S_{n})^{-1}_{}(w_{t},S_{n})\). To develop a private variant of Newton's method, we need to characterize the sensitivity of this update rule. Our key observation is that _the directions corresponding to small eigenvalues of \(H(w_{t},S_{n})\) are more_ sensitive _than the directions corresponding to large eigenvalues_. To overcome this issue, we modify the eigenvalues of \(H(w_{t},S_{n})\) to ensure a minimum eigenvalue \(_{0}\), where \(_{0}>0\) is a carefully chosen constant. We show how to _adaptively_ tune \(_{0}\) in Section 5.2. This procedure yields the desired stability with respect to neighbouring datasets. Formally, the modification operator is defined as follows:

**Definition 5.4**.: Let \(A^{d d}\) be a positive semi-definite (PSD) matrix and \(_{0} 0\). Define

\[_{_{0}}(A,)=_{i=1}^{d}\{_{0},_{i} \}u_{i}u_{i}^{},_{_{0}}(A,)=_{i=1}^{d}( _{i}+_{0})u_{i}u_{i}^{}=A+_{0}I_{d}.\]

where \(A=_{i=1}^{d}_{i}u_{i}u_{i}^{}\) is the eigendecomposition of \(A\) - i.e., \(0_{1}_{d}\) are the eigenvalues and \(u_{1},,u_{d}^{d}\) are the eigenvectors, which satisfy \( i j\|u_{i}\|=1 u_{i},u_{j}=0\).

Algorithm 3 describes our algorithm. First, we state the privacy guarantee of Algorithm 3 whose proof can be found in Appendices B.3 and B.4.

**Theorem 5.5**.: _Assume in Algorithm 3 we choose \(\) for the SOI modification. Then, for every training set \(S_{n}(^{d}\{-1,+1\})^{n}\), \(w_{0}\), \(_{0}>0\), \(T\), \(_{+}\), and \((0,1)\), by setting \(_{1}=}{n}\) and \(_{2}=}{(4n_{0}^{2}+_{0})}\), \(w_{T}\) satisfies \(\)-zCDP._

**Theorem 5.6**.: _Assume in Algorithm 3, we choose \(\) for the SOI modification. Then, for every training set \(S_{n}(^{d}\{-1,+1\})^{n}\), \(w_{0}\), \(_{0}>0\), \(T\), \(_{+}\), and \((0,1)\) such that \(n>}\), by setting \(_{1}=}{n}\) and \(_{2}=}{(4n_{0}^{2}-_{0})}\), \(w_{T}\) satisfies \(\)-zCDP._the logistic loss. Nevertheless, in Appendix B.6, we present a generalization of Algorithm 3 whose privacy guarantee holds for _every_ convex, doubly differentiable, Lipschitz, and smooth loss function _without any constraints on the rank of Hessian_. The main technical challenge for sensitivity analysis is proving the approximate Lipschitzness of \(\) in the operator norm (See Lemma B.7). This demonstrates that our algorithm is more general than objective perturbation  and the private damped Newton's method  which both require a low-rank Hessian. \(\)

### Private and Adaptive Selection of Minimum Eigenvalue

One of the hyperparameters of Algorithm 3 is the minimum eigenvalue \(_{0}\). There exists a tradeoff for choosing \(_{0}\). We ideally want the modification to be as small as possible, so that the SOI is preserved. However, decreasing \(_{0}\) increases \(_{2}\) and we add more noise. To deal with this problem, we propose a heuristic rule for an adaptive, private, and time-varying selection of the minimum eigenvalue. We wish to find \(_{0,t}\) that minimizes expected loss at the next iteration, for which we have the quadratic approximation (6). More formally, we compute \(_{0,t}\) as \(_{k}[q_{t}(w_{t}-_{}(H(w_{t},S_{n}), )_{t}+\|_{t}\|\,_{2}() )]\) where \(q_{t}\) is given in (6) and \((0,I_{d})\). We show in Appendix B.5 that an approximate minimizer is \(_{0,t}((H_{t}(w_{t},S_{n}))}{ n^{2}})^{}\). Note that \(_{0,t}\) depends on the data through \((H(w_{t},S_{n}))\), which has sensitivity \(1/4n\), so it can be estimated privately. In Appendix B.5, we provide the algorithmic description of a variant of Algorithm 3 with an adaptive and private minimum eigenvalue. In particular, we divide the privacy budget at each iteration into three parts: (1) privatizing the gradient; (2) estimating the trace of SOI; and (3) privatizing the direction. We use this variant for our numerical experiments in Section 6.

### Convergence Results for Algorithm 3

In this section, we provide data-dependent convergence guarantees for Algorithm 3. We express these guarantees in terms of the conditional expectation \(_{t}[]=[|\{w_{i}\}_{i[t]}]\) and they can be easily extended to obtain high probability bounds. Before presenting the results, we introduce a notation. For a dataset \(S_{n}=((x_{1},y_{1}),,(x_{n},y_{n}))(^{d}\{-1,+1\})^{n}\), let \(V^{d d}\) denote the _orthogonal projection matrix_ on the linear subspace spanned by \(\{x_{1},,x_{n}\}\). For every vector \(u^{d}\), define \(\|u\|_{V}Vu}\). This norm naturally arises since for every \(w^{d}\) we have \(_{}(w,S_{n})-_{}(w^{*},S_{n}) \|w-w^{*}\|_{V}^{2}\) where \(w^{*}=_{}(w,S_{n})\) (See Appendix B.7).

#### 5.3.1 Local Convergence Guarantee of \(clip}\) and \(add}\)

**Theorem 5.8**.: _Let \(S_{n}\) denote the dataset and \(\) denote the dimension of the linear subspace spanned by \(\{x_{1},,x_{n}\}\). Let \(_{,t}\) be the smallest non-zero eigenvalue of \(^{2}_{}(w_{t},S_{n})\) and \(\) be the privacy budget (in zCDP) per iteration. Then,_

\[_{t}[\|w_{t+1}-w^{}\|_{V}^{2}]_{1,t}^ {2}\|w_{t}-w^{}\|_{V}^{2}+2_{1,t}_{2,t}\|w_{t}-w^{ }\|_{V}^{3}+_{2,t}^{2}\|w_{t}-w^{}\|_{V}^{4}+,\]

_where the coefficients are given by_

\[_{1,t}=1-_{,t}}{_{0}}+}}{(4n_{0}^{2}-_{0})},_{2,t }=_{,t}},=O(}{(1-)n^{2}}_{,t})^{2}} ).\] (7)

_Here, \(_{,t}=\{_{,t}, _{0}\}&$-clip},\\ _{,t}+_{0}&$-add},\) depends on the modification procedure._

This type of convergence is known as _composite convergence_, as it is a combination of linear and quadratic rates, and has been observed in the convergence analysis of several quasi-Newton's methods .

_Remark 5.9_.: \(_{,t}\) is the smallest _non-zero_ eigenvalue of \(^{2}_{}(w_{t},S_{n})\). Therefore, for sufficiently large \(n\) we have \(0<_{1,t}<1\). It shows Algorithm 3 with Hessian as SOI is, in-expectation, a descent algorithm locally given \(\|w_{t}-w^{}\|\) is sufficiently larger than \(\). Roughly speaking, Theorem 5.8 guarantees a linear convergence to a ball around the optimum whose radius is given by \(\). We also observe the linear rate in Figure 3. Moreover, the error due to the privacy, i.e., \(\) in Equation (7), is proportional to the rank of the feature vectors which is always smaller than \(d\). These interesting properties is due to the convergence analysis with respect to \(\|\|_{V}\). \(\)

_Remark 5.10_.: The coefficients of the convergence in Equation (7) depend on the iteration step which is an undesirable aspect of the results. In Lemma B.11, we prove that \(\|_{,t}-_{}^{}\| 0.1 \|w_{t}-w^{}\|_{V}\) where \(_{}^{}\) is the smallest non-zero eigenvalue of \(^{2}_{}(w^{},S_{n})\). Therefore, the coefficients can be well-approximated by their analogous values evaluated at the optimum. \(\)

3.2 Global Convergence Guarantee of \(\)-\(\) and \(\)-\(\)

We also establish a global convergence guarantee for \(\)-\(\) and \(\)-\(\). Due to the space the formal statement and proof are deferred to Appendix B.9. Roughly speaking, under the assumption of _local strong convexity at the optimum_, \(\)-\(\) and \(\)-\(\) converge globally: this is intuitive since \(\)-\(\) and \(\)-\(\) are based on minimizing a global upper bound on the function.

## 6 Numerical Results

In this section, we evaluate the performance of our algorithm (Algorithm 3 with the adaptive minimum eigenvalue selection from Section 5.2) for the problem of _binary classification_ using _logistic regression_. For brevity, many of the details behind our implementation and more experimental results are deferred to Appendix C.

### Setup

The setup of the experiments is as follows: **Baseline1- DP-(S)GD**: The update rule is \(w_{t+1}=w_{t}-(w_{t},S_{n})+\) where \(\) is a Gaussian noise . Since the logistic loss is \(1\)-Lipschitz, we do not need gradient clipping. The Lipschitzness parameter controls the variance of the Gaussian random vector. To draw a fair comparison and show the advantage of using second-order information, we chose the stepsize to be equal to the inverse smoothness. **Baseline2-

Figure 2: Privacy-Utility tradeoff on different datasets.

**Approximate Objective Perturbation (AOP)**: AOP is built on objective perturbation . Objective perturbation consists of a two-stage process: (1) _perturbing_ the objective function by adding a random linear term and (2) outputting the minimum of the perturbed objective. Releasing such a minimum is sufficient for achieving DP guarantees , but only if we can find the exact minimum of the perturbed objective. AOP extends objective perturbation to permit using an _approximate_ minimum of the perturbed objective . Notice AOP is not an iterative optimization algorithm. **Baseline3- Damped Newton Method **: The algorithm in  is a variant of damped Newton's method with the assumption that the Hessian of loss function is rank-1, which holds for the logistic loss. Their algorithm is based on adding two i.i.d. noises to the Hessian and the gradient: \(w_{t+1}=w_{t}-_{t}H_{,t}(w_{t},S_{n})^{-1}_{t}\), where \(_{t}\) is the stepsize, \(H_{,t}(w_{t},S_{n})=^{2}_{}(w_{t},S_{n})+_{ t}\) and \(_{t}=_{}(w_{t},S_{n})+_{t}\). Here \(_{t}\) and \(_{t}\) are carefully chosen Gaussian noise. With \(_{t}=1\), our experiments show that their algorithm is not converging. We use the strategy suggested in [2, Page 22] and set \(_{t}=(1+_{t})/_{t}\) where \(_{t}=\|^{2}_{}(w_{t},S_{n})^{-1}_{ {LL}}(w_{t},S_{n})\|\). This stepsize selection makes the algorithm _non-private_, however, it serves as a good baseline. **Datasets:** We conducted experiments on six publicly available datasets: a1a, Adult, covertype, synthetic, fashion-MNIST, and protein (Appendix C includes fashion-MNIST and protein results). The synthetic dataset is generated as follows: Fix \(d\) and \(w^{}^{d}\). Then, (1) the feature vectors \(\{x_{i}^{d}\ :\ i[n]\}\) are independent and sampled uniformly at random from the unit sphere in \(^{d}\), (2) for the \(i\)-th datapoint the label is \(+1\) with probability \((1+(- x_{i},w^{}))^{-1}\) and \(-1\) otherwise. **Privacy Notion:** The privacy notion for our experiments is \((,=()^{-2})\)-DP. Next, we present the results.

### Privacy-Utility-Run Time Tradeoff

We study the tradeoff for our algorithm and compare it with other baselines for a broad range of \(\{0.01,,10\}\). We _non-privately tune_ the total number of iterations of the iterative algorithms and report the best achievable excess error in Figure 2. As can be seen our algorithm almost always achieves the best excess loss for a broad range of \(\). Also, Figure 2 shows that damped private Newton method of  achieves a low excess loss only for large \(\). Figure 2 indicates that DP-GD and our algorithm are the best in terms of excess loss. In Table 1, we compare the run time of DP-GD and our algorithm, i.e., the computational time in seconds for achieving the excess loss in Figure 2. As can be seen, for many challenging datasets, our algorithm is \(10\)-\(40\) faster than DP-GD. Our experiments are run on CPU. We also remark that each step of Algorithm 3, i.e., computing gradient and SOI, is heavily parallelizable implying that the run time of Algorithm 3 can be made much smaller by an efficient implementation. Also, the reported numbers in Figure 2 and Table 1 correspond to Hess-clip.

    & }^{}}{T_{}^{}}\)} & }^{}()\)} \\   & \(=0.01\) & \(=0.1\) & \(=1\) & \(=10\) & \((T_{}^{})\) (sec.) & \((T_{}^{})\) (sec.) \\  a1a & \(4.87\) & \(2.95\) & \(5.09\) & \(30.59\) & \(2.45\) & \(4.2\) \\ synthetic & \(2.90\) & \(2.90\) & \(5.19\) & \(11.61\) & \(0.18\) & \(0.21\) \\ adult & \(12.08\) & \(11.84\) & \(22.17\) & \(38.16\) & \(6.81\) & \(8.07\) \\ covertype & \(24.19\) & \(19.85\) & \(35.70\) & \(36.20\) & \(2.93\) & \(3.58\) \\   

Table 1: Comparison between the run time of our algorithm and DP-GD in terms of the ratio \(T_{}^{}/T_{}^{}\). The last two columns show the minimum and maximum run time of our algorithm.

Figure 3: Comparison with DP-GD Oracle where at each iteration the stepsize tuned non-privately.

### Second Order Information vs Optimal Stepsize

In non-private convex optimization, the key to the success of second-order optimization algorithms is that the second-order information acts as a preconditioner, and the same performance _cannot_ be attained by optimally tuning the stepsize for GD algorithm. To investigate whether the same holds for our algorithms, we consider the following variant of DP-GD. Let \(_{t}\) denote the perturbed gradient obtained by adding a Gaussian random vector to \(_{}(w_{t},S_{n})\). Instead of a constant stepsize, the stepsize at iteration \(t\) is chosen based on \(_{t}=_{ 0}_{}(w_{t}-_{t})\). Notice this variant is obviously _not DP_. We refer to this variant as _DP-GD-Oracle_. The comparison with DP-GD-Oracle lets us answer the following question: _Could we have just computed a single number, i.e., stepsize, to achieve the same performance as our second-order optimization algorithms which require computing a \(d d\) matrix?_ In Figure 3, we compare the convergence speed of our algorithms with DP-GD-Oracle in low- and high-privacy regimes. Figure 3 shows our algorithms converge faster than DP-GD-Oracle which is not even a DP algorithm. Figure 3 confirms the expectation that as the privacy budget increases the difference between our algorithms and DP-GD-Oracle increases since we can use more curvature information.

### Minibatch Variant of Our Algorithm and Comparison with DP-SGD

So far we have considered full-batch algorithms that compute first- and second-order information on the entire dataset. We extend Algorithm 3 to the minibatch setting, where, at each iteration, the gradient and SOI matrix are computed using a subsample of the data points. In Appendix C.1 we provide a formal algorithmic description of the minibatch version of Algorithm 3 along with its privacy proof. Then, we compare the convergence speed and excess loss with DP-SGD.

DP-SGD is faster than DP-GD, but to achieve good privacy and utility, we need large batches [PHKX+23, Fig. 2]. This is in stark contrast with non-private SGD, where larger batch sizes yield diminishing returns [ZLMM+19]. In particular, to achieve the best excess loss we need to select the batch size as large as possible. We select the batch size of DP-SGD so that the achievable excess loss will be close to the full batch versions. Specifically, we select \(}{} 0.02\) and tune the number of iterations of DP-SGD to obtain the best result. Figure 4 shows the progress of different algorithm versus run time. Obviously, for a fixed run time DP-SGD performs more iterations compared to our algorithms. Nevertheless, our algorithms achieve the same excess error as DP-GD with \(8\)-\(10\) faster run time over all the datasets while _the batch sizes of our algorithms are larger than that of DP-SGD_. We observe that the variations of our algorithms based on the adding operator performs better in the minibatch setting. This can be attributed to the smaller \(_{2}\) for the adding operator in Algorithm 3. In summary, the comparison between privacy-utility-wall time tradeoff of the subsampled variant of our algorithm and DP-SGD is similar to their full-batch counterparts.

## 7 Conclusion and Limitations

We showed that second-order methods can be used in the DP setting both for improving worst-case convergence guarantees and designing faster practical algorithms. We believe our results open up many directions: A limitation of our algorithms is that the cost of forming and inverting the Hessian can be prohibitive when \(d\) is large. In the non-private setting, a line of research tries to address this limitation by constructing an approximation to SOI such that the update is efficient, yet still provides sufficient SOI [15; 16; 17]. It would be interesting to investigate how the ideas developed in our paper could be incorporated into these methods.

Figure 4: Minibatch Variant of Our Algorithm and Comparison with DP-SGD