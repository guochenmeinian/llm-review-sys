# Fairness Without Harm:

An Influence-Guided Active Sampling Approach

 Jinlong Pang

UC Santa Cruz

jpang14@ucsc.edu

&Jialu Wang

UC Santa Cruz

falidct@ucsc.edu

&Zhaowei Zhu

UC Santa Cruz

zzw@docta.ai

&Yuanshun Yao

Meta GenAI

kevinyao@meta.com

&Chen Qian

UC Santa Cruz

cqian12@ucsc.edu

&Yang Liu

UC Santa Cruz

yangliu@ucsc.edu

Equal contributionCorresponding Author: Yang Liu

38th Conference on Neural Information Processing Systems (NeurIPS 2024).

###### Abstract

The pursuit of fairness in machine learning (ML), ensuring that the models do not exhibit biases toward protected demographic groups, typically results in a compromise scenario. This compromise can be explained by a Pareto frontier where given certain resources (e.g., data), reducing the fairness violations often comes at the cost of lowering the model accuracy. In this work, we aim to train models that mitigate group fairness disparity without causing harm to model accuracy. Intuitively, acquiring more data is a natural and promising approach to achieve this goal by reaching a better Pareto frontier of the fairness-accuracy tradeoff. The current data acquisition methods, such as fair active learning approaches, typically require annotating sensitive attributes. However, these sensitive attribute annotations should be protected due to privacy and safety concerns. In this paper, we propose a tractable active data sampling algorithm that does not rely on training group annotations, instead only requiring group annotations on a small validation set. Specifically, the algorithm first scores each new example by its influence on fairness and accuracy evaluated on the validation dataset, and then selects a certain number of examples for training. We theoretically analyze how acquiring more data can improve fairness without causing harm, and validate the possibility of our sampling approach in the context of risk disparity. We also provide the upper bound of generalization error and risk disparity as well as the corresponding connections. Extensive experiments on real-world data demonstrate the effectiveness of our proposed algorithm. Our code is available at github.com/UCSC-REAL/FairnessWithoutHarm.

## 1 Introduction

Machine Learning (ML) has dramatically impacted numerous optimization and decision-making processes across various domains, such as credit scoring  and demand forecasting . Algorithmic fairness embraces the principle, often enforced by law and regulations, that the decision-maker should not exhibit biases toward protected group membership , identified by characteristics such as race, gender, or disability. However, the pursuit of fairness unavoidably results in a compromise scenario where reducing the fairness violations usually leads to a degradation in accuracy, which has been observed and verified by numerous literature . Theoretically, the phenomenon can be understood through a Pareto frontier on the tradeoff between group fairness and accuracy[72; 51; 10; 74]. That is, as illustrated in Figure 1, given certain resources such as training data, when a model has reached a point on the Pareto frontier, without more data resources, it is impossible that one can improve fairness without worsening model accuracy.

One major source of unfairness and a major cause of the fairness-accuracy tradeoff is biased training data. If an unbiased and "fairer" dataset is available, we will be hopeful that unfairness can be alleviated without compromising accuracy. Furthermore, such a "fairer" dataset would allow for obtaining a fair and accurate model through the standard empirical risk minimization (ERM) with cross-entropy (CE) loss. The above observation points to a promising way to improve fairness via actively acquiring more informative data, aiming to shift towards a better Pareto frontier of the fairness-accuracy trade-off [1; 46]. However, existing approaches that seek more data, such as fair active learning , typically require annotating sensitive attributes for training data. In practice, these sensitive attribute information such as race and gender, should be protected due to privacy regulations [35; 5; 66]. In the normal active learning scenario, collecting more data with sensitive attributes heightens privacy and safety risks due to the increased probability of leaking sensitive information.

Therefore, we ask the following question: _When not disclosing more annotations of training sensitive attributes, how can we acquire more data to improve model fairness without sacrificing accuracy?_

In this paper, we propose a tractable active data sampling algorithm in a _training sensitive attributes-free_ way, which solely requires sensitive attributes on a _small validation set_. Particularly, the algorithm evaluates each example's influence on fairness and accuracy using the validation dataset for ranking and then selects a certain number of examples to supplement the training set for training. We name our solution Fair Influential Sampling (FIS). The core challenge is approximating the corresponding influences of each new example without accessing its sensitive attributes. Technically, we evaluate the importance (influences) of each new example by comparing its gradient to that derived from the entire validation set. This comparison helps quantify the hypothesized change of group fairness disparity metric when adding this example to the training set. As a result, the requirement of training sensitive attributes can be relaxed, as gradient derivation serves as a role of fairness constraints to measure the group fairness disparity. The main contributions of our work are summarized as follows.

* We develop a tractable active data sampling algorithm (Algorithm 1) that does not rely on training sensitive attributes. The algorithm scores each new example based on the combined influences of prediction and fairness and then opts for a certain number of examples for training. [Section 4]
* We theoretically analyze how acquiring more data can improve fairness without harm from a distribution shift perspective view, and validate the possibility of our sampling approach in the context of risk disparity. We also provide the upper bound of generalization error and risk disparity as well as the corresponding connections (Theorem 5.1 and Theorem 5.2). [Section 5]
* Empirical experiments on real-world datasets (CelebA, Adult, and COMPAS) substantiate our claims, indicating the effectiveness and potential of our proposed algorithm in achieving fairness for ML classifiers. [Section 6]

## 2 Related work

Fairness-accuracy tradeoffThere are numerous works that have been successful at mitigating fairness disparities [24; 32; 2; 78; 69]. However, these works typically rely on protected sensitive

Figure 1: We compare the Pareto frontiers between the model trained with scarce data and that trained with rich data. Acquiring more data is capable of shifting the Pareto frontier toward lower disparity and lower error rates. In consequence, we can reach a new trade-off point that offers improved fairness and accuracy simultaneously, surpassing the original trade-off point.

attributes of training examples to measure the fairness disparities across groups. Moreover, a fairness-accuracy tradeoff has been shown, meaning that enforcing fair constraints heavily degrades the model performance [52; 25; 81]. Notably, Chen et al.  characterized the change of the fairness violation when the data distribution is shifted. Except for training sensitive attributes, this paper does not work in the classical regime of the fairness-accuracy tradeoff. By properly collecting new data, we can improve both accuracy and fairness, which cannot be achieved by working on a static training dataset that naturally incurs such a tradeoff. Besides, compared to prior works [52; 56], our method does not require additional assumptions about the classifier and the characteristics of the training/testing datasets (e.g., distribution shifts). Relevant work  utilizes the influence function to reweight the data examples but requires re-training. Our method focuses on soliciting additional samples from an external dataset while  reweights the existing and fixed training dataset.

Active learningThe core idea of active learning is to rank unlabeled instances by developing specific measures, including uncertainty [41; 45], representativeness , inconsistency , variance , and error . A related line of work [48; 71; 31; 76] concentrates on ranking unlabeled instances based on the influence function. Compared to these studies with a focus on prediction performance, our work poses a distinct challenge taking into account fairness violations. Our approach is more closely with the _fair active learning_ approach . However, this framework still relies on training sensitive attributes and then unavoidably encounters the tradeoff between fairness and accuracy.

Fair classifiers without demographicsThere are various studies to achieve fairness without demographics. For example, Zhao et al.  explores the correlations between sensitive attributes and non-sensitive attributes to learn fair and accurate classifiers. Yan et al.  investigates the class imbalance problem with a KNN-based pre-processing method. Chai et al.  utilizes the soft labels from an overfitting teacher model to train a student model to avoid using demographics. A line of research establishes theoretical connections between features and attributes to avoid using demographic information, employing methods like causal graphs , correlation shifts , and demographic shifts . In contrast, our approach refrains from making assumptions. Another line of work utilizes distributionally robust optimization (DRO) to reduce fairness disparity without relying on training sensitive attributes [33; 38; 47; 40; 67; 64]. Although these works evaluate the worst-case group performance in the context of fairness, their approaches differ as they do not strive to equalize the loss across groups. Besides, in these studies, accuracy and worst-case accuracy are used as fairness metrics to showcase the efficacy of the proposed algorithms. However, these fairness metrics are restrictive and inconsistent with common definitions such as demographic parity (DP).

Fair classificationThe fairness-aware learning algorithms, in general, can be categorized into pre-processing, in-processing, and post-processing methods. Pre-processing methods typically reweigh or distort the data examples to mitigate the identified biases [7; 9; 65; 60; 15; 17; 80; 18; 68]. More relevant to this work is the _importance reweighting_, which assigns weights to training examples [37; 36; 23; 21; 57; 44]. Our algorithm bears similarity to a specific case of importance reweighting, particularly the 0-1 reweighting applied to newly added data. Other parallel studies utilize importance weighting to learn a complex generative model in a weakly supervised setting [23; 21], or to mitigate representation bias in training datasets . Post-processing methods typically enforce fairness on a learned model through calibration [26; 27; 32], but these work might not achieve the best fairness-accuracy tradeoff [75; 55]. In contrast, these post-processing works still require sensitive attributes during the inference phase. Recent work  develops a _bias score_ classifier that operates independently of sensitive attributes; however, it is constrained to binary classifications.

## 3 Preliminaries

Problem setupWe consider a standard \(K\)-class classification task whose training (test) data distribution is \(\) (\(\)). Let \(P:=\{z_{n}\}_{n=1}^{|P|}\) represent the _training dataset_ following distribution \(\), where \(|P|\) denotes the corresponding sample size. Each example, denoted as \(z_{n}:=(x_{n},y_{n})\), comprises two random variables: the _feature vector_\(x\) and the _label_\(y\). The model trained on \(P\) is evaluated by the _test dataset_\(Q:=\{z^{}\}_{n=1}^{|Q|}\), where \(()^{}\) denotes that the data follows distribution \(\), each example \(z_{n}^{}:=(x^{},y^{},s^{})\), and the sensitive group \(s_{n}^{}\) often refers to characteristics such as race, gender, etc. To align the fairness requirements on the test set \(Q\) with the model trained on \(P\), a popular way is to exploit the sensitive attributes \(s\)[24; 72] or their proxies  in \(P\) and use them to formulate Lagrangians during training. However, extending these approaches to the active learning setting would require disclosing more sensitive attributes during sampling and training , which _contradicts_ our goal. To avoid disclosing more sensitive attributes, we align the fairness requirements by a small hold-out _validation dataset_\(Q_{v}:=\{z_{n}^{}\}_{n=1}^{|Q_{v}|}\) that is independent and identically distributed (IID) with the test set \(Q\). We defer more technical details to Section 4.

Following the general active learning setting , we would acquire new examples from a large _unlabeled dataset_\(U:=\{x_{n}^{}\}_{n=1}^{|U|}\) within a limited labeling budget \(B(|U|)\)[48; 71; 31]. Denote the solicited example by \(z_{n}^{}:=(x_{n}^{},y_{n}^{})\), where \(y_{n}^{}\) is the ground-truth label. Note that the protected sensitive attributes from datasets \(P\) and \(U\) remain undiscolosed during sampling and training. In this paper, we aim to incrementally update a model that was initially trained on \(P\) using standard ERM, by incorporating newly solicited data \(z_{n}^{}\), such that the model can improve fairness without worsening model accuracy. Thus, the core challenge is efficiently determining new examples that induce a significantly better Pareto frontier. In the proceeding section 4, we shall delve into how to acquire new data from unlabeled set.

Fairness definitionNote that this work focuses solely on active sampling to build a fairer dataset, which is then used to train the model through standard ERM with Cross-Entropy (CE) loss. Without relying on additional assumptions about the model or training/testing dataset, an intuitive and natural approach is to analyze the expected risk. Therefore, we introduce the concept of _risk disparity_ as an intermediate-term for theoretical analysis of fairness.

r

**Definition 3.1** (Risk disparity [33; 79; 3]).: _Define \(Q_{k}\) as the sub-distribution of \(Q\) corresponding to group \(k\). Given the optimized model parameters \(^{P}\) trained on set \(P\), risk disparity is defined as: \(_{_{k}}(^{P})-_{}( ^{P})\), where \(_{}():=_{z}[( ,z)]\) denotes expected risk induced on target distribution \(\)._

Definition 3.1 naturally quantifies the discrepancy in a trained model's performance between a specific group set \(Q_{k}\) and the entire test set \(Q\). That is, a model can be deemed fair if it exhibits consistent performance for a group set \(Q_{k}\) as compared to the test dataset \(Q\). In settings such as face or speech recognition, this fairness definition implies the necessity for all demographic groups to receive the same quality service . For completeness, we also include two well-known definitions of fairness:

**Definition 3.2** (Demographic Parity (Dp)).: _A classifier \(f\) adheres to demographic parity concerning the sensitive attribute \(s\) if: \([f(,x)]=[f(,x)|s]\)._

**Definition 3.3** (Equalized Odds (Edd ).: _A classifier \(f\) meets the equalized odds with respect to the sensitive attribute \(s\) if: \([f(,x)|y]=[f(,x)|y,s]\)._

Even though there may be a general incompatibility between risk disparity and popular group fairness metrics like DP and EOd, under the criteria of the proposed fairness notion, these definitions could be encouraged [61; 33]. More details and proof can be found in the Appendix B.

**Proposition 3.1**.: _(Informal) Under appropriate conditions, the risk disparity can serve as a lower bound for fairness disparities based on common fairness definitions, such as DP and EOd._

**Remark 3.1** (Connections to other fairness definitions).: _Definition 3.1 targets group-level risk fairness, which has similar granularity to other fairness notions such as accuracy parity , device-level parity , small accuracy loss for groups [79; 10; 50; 33], and bounded group loss ._

## 4 Improving fairness without harm via data influential sampling

In this section, we first introduce how to measure the importance (influence) of each example on accuracy and fairness without using the corresponding sensitive attributes, respectively. Then, we propose an influence-guided sampling algorithm that actively acquires new data based on the influences for further training.

### Finding influential examples

To avoid using training sensitive attributes, our primary idea is to find newly acquired data that assists in creating a "fairer" dataset, which allows for training a fair and accurate model via standard ERM.

Initially, we explore whether newly acquired data enhances fairness by examining the training process, where the model is typically updated using gradient descent. The change of model parameters by performing one step gradient descent on newly acquired data \(z^{}\) is

\[_{t+1}=_{t}-_{_{t}}(_{t},z^{})\] (1)

where \(\) refers to the learning rate and \(()\) is the training loss function. It should be noted that before we solicit the true labels of samples \(z^{}\), we first use proxy labels. In the following subsection 4.2, we will present a strategy for proxy labels. Training on \(z^{}\) affects the model's prediction on validation data \(z^{}_{n}\) regarding both accuracy and fairness. If the updated model \(_{t+1}\) outperforms the previous one \(_{t}\) evaluated on the validation dataset in terms of fairness and accuracy, this acquired data \(z^{}\) helps to reduce the fairness disparity without worsening accuracy.

To separately measure the accuracy and fairness performance of the updated model on the validation set, we introduce two types of loss functions: **fairness loss**\((,z^{}_{n})\) and **accuracy loss**\((,z^{}_{n})\), where validation data \(z^{}_{n}=(x^{}_{n},y^{}_{n},s^{}_{n})\). Note that these loss functions are developed for sampling, not for training. Besides, training loss function \(()\) can be reused as the accuracy loss function due to the same update target. One can identify training loss \((,z^{})\) and accuracy loss \((,z^{}_{n})\) based on the input data used. Without loss of generality, we assume that \(()\) and \(()\) are differentiable w.r.t. \(\). Here, we do not restrict the generality of the fairness loss function; it can be any smoothed version of fairness metrics such as DP or EOd. Following this, we develop the influence of the accuracy and fairness components for finding the samples, respectively.

Influence of accuracy componentWhen model parameters are updated from \(_{t}\) to \(_{t+1}\) by adding a new example \(z^{}\), the influence of model's accuracy on one validation example \(z^{}_{n}\) is:

\[_{}(z^{},z^{}_{n};_{t},_{t+1}):=(_{t+1},z^{}_{n})-(_{t},z^{}_ {n}).\]

For ease of notation, we use \(_{}(z^{},z^{}_{n})\) to represent \(_{}(z^{},z^{}_{n};_{t},_{t+1})\). By applying first-order Taylor expansion, we obtain the following closed-form statement:

**Lemma 4.1**.: _The accuracy influence of new example \(z^{}\) on the validation dataset \(Q_{v}\) is:_

\[_{}(z^{}):=_{n|Q_{v}|}_{}(z^{},z^{}_{n})-_{n|Q_{v}|} _{_{t}}(_{t},z^{}),_{ _{t}}(_{t},z^{}_{n})\] (2)

Intuitively, the more negative \(_{}(z^{})\) is, the more positive the model accuracy (performance) that example \(z^{}\) can provide.

Influence of fairness componentWhen model parameters are updated from \(_{t}\) to \(_{t+1}\) by adding a new example \(z^{}\), the influence of model's fainess on one validation example \(z^{}_{n}\) is:

\[_{}(z^{},z^{}_{n};_{t},_{t+1}):=(_{t+1},z^{}_{n})-(_{t},z^{}_{n}).\] (3)

For simplicity, we write \(_{}(z^{},z^{}_{n};_{t},_{t+1})\) as \(_{}(z^{},z^{}_{n})\). Then, similarly, we have:

**Lemma 4.2**.: _The fairness influence of new example \(z^{}\) on the validation dataset \(Q_{v}\) is:_

\[_{}(z^{}):=_{n|Q_{v}|}_{}(z^{},z^{}_{n})-_{n|Q_{v}|} _{_{t}}(_{t},z^{}),_{ _{t}}(_{t},z^{}_{n})\] (4)

Similar to the accuracy component, the greater the negativity of \(_{}(z^{})\) is, the greater the positive impact that the example \(z^{}\) has on fairness.

IntuitionsThese two components evaluate the accuracy and fairness impact of each example by comparing the gradient originating from a single data sample with the gradient derived from the entire validation set, respectively. This comparison helps quantify the potential advantage of including this specific example in training. For instance, if the gradient obtained from one example has a similar direction to the gradient from the validation set, it indicates that incorporating this example contributes to enhancing the model's fairness or accuracy.

```
1:Input: training set \(P\), unlabeled set \(U\), validation set \(Q_{v}\), new acquired set \(S_{t}=\{\}, t[T]\) rounds, number of new selected examples in each round \(r\), tolerance \(\).
2:Warmup: Train a classifier \(f\) solely on \(P\) by minimizing the empirical risk \(R_{}\). Obtain model parameters \(_{1}\) and validation accuracy (on \(Q_{v}\)) \(_{0}\).
3:for\(t\) in \(\{1,2,,T\}\)do
4: Guess proxy label \(^{}\) for new examples \(^{}\) using Eq. (5).
5: Compute the influence of accuracy and fairness component using Eq. (2) and Eq. (4):
6: \[S_{}=\{_{}(^{}) _{}(^{}) 0,_{}(^{ }) 0,^{} U\}\]
7:while\(|S_{t}|<r\)do
8: Find top-\((r-|S_{t}|)\) annotated examples \(^{}_{n}\) based on the lowest fairness influence and then inquire about true labels \(y^{}\):
9: \[S_{t} S_{t}\{z^{}_{n}_{}(z^{ }_{n}) 0,_{}(z^{}_{n}) 0\}\]
10:\(U U S_{t}; S_{} S_{}  S_{t}\)
11:endwhile
12: Continue to train the model \(f\) on the set \(S_{t}\) via standard ERM. Obtain the updated model parameters \(_{t+1}\). If the model's validation accuracy (on \(Q_{v}\)) \(_{t}\) does not meet the desired threshold \(_{0}\), reject the updated model.
13:endfor
14:Output: models \(\{_{t}_{t}>_{0}-\}\) ```

**Algorithm 1** Fair influential sampling (FIS)

Training sensitive attributes are not disclosedOne can easily check that neither the influence of accuracy nor fairness components require the sensitive attributes of any example \(z^{}\), as the example \(z^{}\) only appears in the first-order gradient of the accuracy loss \(_{_{t}}(_{t},z^{})\). In the fairness component, calculating the \(_{_{t}}(_{t},z^{}_{n})\) only relies on validation example \(z^{}_{n}\)'s sensitive attributes. Here, we also validate how accurate the first-order estimation of the influence is in comparison to the real influence , and find that the estimated influences for most of the examples are very close to their actual influence values. We refer the readers to Appendix C.1 for more details.

Even without disclosing training sensitive attributes, the correlations between non-demographic features and demographic information may still lead to privacy leakage issues . To address this potential privacy concern, we provide further discussions and theoretical analysis using differential privacy in Appendix F.

### Algorithm: fair influential sampling (FIS)

Following Lemma 4.1 and Lemma 4.2, we can efficiently select those examples with the most negative fairness influence and negative accuracy influence. This sampling method aids in reducing fairness disparities without worsening model accuracy.

LabelingBefore presenting our sampling algorithm, it is necessary to address the problem of not accessing the true labels of new solicited examples. Lacking the label information for new examples poses a challenge in determining the corresponding influence on accuracy and fairness, a fact that is substantiated by Lemma 4.1 and Lemma 4.2. Intuitively, one can always recruit human annotators to get the ground-truth labels for those unlabeled examples. However, it is impractical due to the limited labeling budgets. To tackle this problem, another common approach is utilizing a model that has been effectively trained on dataset \(P\) to produce proxy labels, which approximate the calculation of influences for examples from a substantial unlabeled dataset \(U\). It's important to note that these proxy labels are exclusively used during the sampling phase. To maintain good model performance, we still need to inquire about the true labels of the selected data examples for subsequent training. Here, we propose to annotate the proxy labels with the model trained on the labeled set \(P\). In particular, we introduce a strategy that employs lowest-influence labels for annotating label \(^{}\) given \(x^{}\):

\[^{}=*{arg\,min}_{k\{1,,K\}}\ |_{}(x^{},k)|,\] (5)

Here, we denote \(^{}:=(x^{},^{})\) for the proxy labels.

Proposed algorithmThe full procedure is outlined in Algorithm 1. Note that the tolerance \(\) is applied to monitor the performance drop in validation accuracy. In Line 2, we initiate the process by training a classifier \(f\) solely on dataset \(P\), that is, performing a warm start. Subsequently, \(T\)-round sampling iterations are applied to acquire more examples to dataset \(P\). Following the iterative fashion, FIS guesses labels using Eq. (5) in Line 4. Then, we calculate the scores for proxy examples based on the accuracy and fairness influence using Eq. (2) and Eq. (4) respectively. In Lines 6-12, we would opt for \(r\) samples based on the influence scores, and inquire about the true labels of these examples. However, due to the gap between proxy labels \(^{}\) and true label \(y\), the accuracy and fairness influences of the top-\(r\) samples based on inquired true labels \(z^{}\) may not necessarily satisfy the same conditions (\(_{}(^{}) 0,_{}(^{}) 0\)). Therefore, we use a while loop to iteratively select the top-\((r-|S_{i}|)\) examples for labeling until we obtain \(r\) samples whose fairness influences based on true labels meet the conditions. Subsequently, in Line 11, we update sets \(U\) and \(S_{}\) to prevent duplicate sampling. In Line 13, we would continue training using new examples with true inquired labels from set \(S_{t}\). We save the model parameters at each round as checkpoints \(_{t}\). To avoid potential accuracy drops incurred by excessively large random perturbations, we exclusively choose and offer models for output whose validation accuracy exceeds the initial validation accuracy \(_{0}\). Although we propose a specific strategy for guessing labels, our algorithm is flexible and compatible with other labeling methods. A comparative analysis of computational costs is detailed in Appendix C.2.

## 5 How more data improves fairness without harm?

In general, acquiring new data to supplement the original training dataset would potentially raise the distribution shift problem, affecting both accuracy and fairness. In this section, from a distribution shift perspective view, we first present a generalization error bound (accuracy side, Theorem 5.1) and risk disparity bound (fairness side, Theorem 5.2). The theoretical results jointly provide a high-level key insight that controlling the negative impact of distribution shift on generalization error, which refers to the model accuracy, could allow for improving fairness without harm. This theoretical insight validates the possibility of our sampling approach.

Without loss of generality, we discretize the whole distribution space and suppose that the train/test distributions are both drawn from a series of component distributions \(\{_{1},,_{I}\}\). Then, the empirical risk \(_{P}()\) calculated over a training set \(P\) can be reformulated by splitting samples based on the component distributions:

\[_{P}():=_{z P}[(,z)]= _{i=1}^{I}p^{(P)}(=i)_{z_{i}}[(,z)].\]

where \(p^{(P)}(=i)\) represents the frequencies of examples in \(P\) drawn from component distribution \(_{i}\). Then, we can define the measure of probability distance between two sets or distributions as \((,):=_{i=1}^{I}|p^{()}(=i )-p^{()}(=i)|\). To reflect the implicit unfairness in the models, we introduce two basic assumptions in convergence analysis .

**Assumption 5.1** (\(L\)-Lipschitz Continuous).: _There exists a constant \(L>0\), for any \(,^{d}\), \(_{P}()_{P}()+ _{P}(),-+|| -||_{2}^{2}\)._

**Assumption 5.2** (Bounded Gradient on Random Sample).: _The stochastic gradients on any sample \(z\) are uniformly bounded, i.e., \([||_{P}(_{t},z)||^{2}] G^{2}\), and training epoch \(t[1,,T]\)._

Analogous to Assumption 5.2, we further make a mild assumption to bound the loss over the component distributions \(_{i}\) according to the corresponding model, that is, \(_{z_{i}}[(^{P},z)] G_{P}, i I\), where \(G_{P}\) is a bounding constant. For completeness, we first analyze the upper bound of generalization error, specifically from the standpoint of distribution shifts. Omitted proof can be found in Appendix D.

**Theorem 5.1** (Generalization error bound).: _Let \((,)\), \(G_{P}\) be defined therein. With probability at least \(1-\) with \((0,1)\), the generalization error bound of the model trained on dataset \(P\) is_

\[_{}(^{P})(,)}_{}+}+_{P}(^{P}).\]

Note that the generalization error bound is predominantly impacted by the shift in distribution, especially when we consider an overfitting model, i.e., the empirical risk \(_{P}(^{P}) 0\).

**Theorem 5.2** (Upper bound of risk disparity).: _Suppose \(_{}()\) follows Assumption 5.1. Let \((,)\), \(G_{P}\), \((_{k},_{k})\) and \((P_{k},P)\) be defined therein. The initial learning rate \(_{0}\) satisfies \(_{0}^{2}<}\), where \(T\) is the number of training epochs. Given model \(^{P}\) and \(^{k}\) trained exclusively on group \(k\)'s data \(P_{k}\), with probability at least \(1-\) with \((0,1)\), then the upper bound of risk disparity is_

\[_{_{k}}(^{P})-_{}( ^{P})(_{k},_{k})+G_{P}(,)}_{}+G^{2}(P_{k},P)^{2}}_{}+\] (6)

_where \(=}+|}}++_{k}\). Note that \(_{z_{i}}[(^{k},z)] G_{k}\), \(=_{P}(^{P})-_{}^{*}(^{})\) and \(_{k}=_{P_{k}}(^{k})-_{_{k}}^{*} (^{_{k}})\). \(\) and \(_{k}\) can be regarded as constants because \(_{P}(^{P})\) and \(_{P_{k}}(^{k})\) correspond to the empirical risks, \(_{}^{*}(^{})\) and \(_{_{k}}^{*}(^{_{k}})\) represent the ideal minimal empirical risk of model \(^{}\) trained on distribution \(\) and \(_{k}\), respectively._

Interpretations of Theorem 5.2Eq. (6) illustrates several aspects that induce unfairness. (1) _Group biased data_. For group-level fairness, the more balanced the data is, the smaller the risk disparity would be; (2) _distribution shift_. For source/target distribution, the closer the distributions are, the smaller the performance gap would be; (3) _Data size_. For training data size, a larger data size (potentially eliminating data bias across groups) would lead to a smaller performance gap.

Main observationTheorem 5.1 underscores how the generalization error is impacted by distribution shifts. Theorem 5.2 implies that the risk disparity is essentially influenced by the distribution shift and the inherent group gap term. In practice, approaches that mitigate the group gap, such as imposing fairness regularizers, acquiring new data, or reweight the training data samples , will inevitably incur additional distribution shifts between the training and test data. The incurred distribution shift further leads to a performance drop due to the generalization error in Theorem 5.1. Nonetheless, one theoretical insight is that if one can control the negative impacts of potential distribution shifts through generalization error while implementing fairness-enhancing strategies, it is possible to achieve the goal of improving fairness without causing harm. This high-level insight supports the effectiveness of our proposed sampling approach, in which we acquire new data to reduce the group gap through fairness components while preventing the potential adverse impacts of distribution shifts using the accuracy influence component.

## 6 Empirical results

In this section, we empirically demonstrate the disparate impact across groups and present the effectiveness of the proposed Fair Influential Sampling method to mitigate the disparity.

### Experimental setup

We evaluate the performance of our algorithm on three real-world datasets across three different modalities: CelebA , UCI Adult  and Compas . We implement the fairness loss \(()\) based on three common group fairness metrics: difference of demographic parity (DP), difference of equality of opportunity (EOp), and difference of equal odds (EOd). We compare our method with five baselines: 1) Base (ERM): directly train the model on the training dataset \(P\); 2) Random: train the model on dataset \(P\) and randomly sampled data from \(U\) with inquired true labels; 3) BALD : active sampling according to the mutual information; 4) ISAL : selects unlabeled examples based on the calculated influence in an active learning setting. We apply model predictions as pseudo-labels; 5) Just Train Twice (JTT) : reweighting those misclassified examples for re-training. Here, we examine a weight of 20 for misclassified examples, marked as JTT-20. Recall that we present the average result of the classifier \(_{t}\) outputs from Algorithm 1. The general term "_fairness violation_" is utilized to quantify the absolute differences based on fairness metrics, such as DP and EOd. More details on datasets and hyper-parameters are provided in Appendix E.1.

### Main results

Note that all the experimental results presented subsequently are from three independent trials, each conducted with distinct random seeds. We present the primary results as tuples in the form(test_accuracy, fairness_violation) to facilitate comparison of the fairness-accuracy tradeoff. Due to space limits, we provide a full version of the experimental results (tables) in Appendix E.2.

Results on image datasetsInitially, we train a vision transformer using a patch size of \((8,8)\) on the CelebA face attribute dataset . We select four binary classification targets, including Smiling, Attractive, Young, and Big Nose. The sensitive attribute is gender. 2% of the labeled data is allocated for training, while the remaining 98% is reserved for sampling purposes. Then, the test dataset is split into two independent portions: a new test set and a validation set, with 10% of the test data randomly designated as the hold-out validation set. For ease of computation, only the last two layers of the model are used to calculate the influence of accuracy and fairness components. For Figure 2, one main observation is that FIS outperforms baselines with a significant margin on three fairness metrics while maintaining the same accuracy level. This improvement, as indicated in Theorem 5.2, can be attributed to FIS assigning priority to new examples based on the fairness influence, then avoiding accuracy reduction via their accuracy influence.

Results on tabular datasetsNext, we work with multi-layer perceptron (MLP) with two layers trained on the Adult  and Compas dataset , respectively. We select age as the sensitive attribute for the Adult dataset and race for the Compas dataset. For two datasets, we resample the data to balance the class and group membership . The whole dataset is split into training and test sets at a 4:1 ratio. Then, we randomly re-select 20% of the training set for initial training and the remaining 80% for sampling. Also, 20% examples of the test set are selected to form a validation set. The MLP model is a two-layer ReLU network with a hidden size of 64. We utilize the whole model parameters to compute the influence of accuracy and fairness for examples. Figure 2 summarizes the main results of the Adult and Compas datasets. On the Adult dataset, we observe that our sampling method achieves the lowest violation for equality of opportunity and has a comparable performance for the DP metric. Besides, our algorithm achieves a much better accuracy-fairness trade-off than

Figure 2: Main results on CelebA, Adult and Compas datasets. The Y axis shows fairness_violation; X axis denotes test_accuracy. CelebA: Four binary targets: Smiling, Attractive, Young, and Big_Nose; Sensitive attribute: gender. Adult: Binary target: Income; Sensitive attribute: Age. Compas: Binary target: Recidivism; Sensitive attribute: Race. We select two fairness metrics DP and Epp to measure fairness violations for each setting. The vertical dotted line at the random baseline accuracy helps easily identify which results achieve fairness without sacrificing performance (accuracy).

other baselines on the Compas dataset. JTT-20 achieves a lower fairness violation with the price of a significant accuracy drop compared to other baselines.

### Ablation study

What is the impact of label budgets?Here, we examine how the varying label budgets \(r\) affect the trade-off between accuracy and fairness. For ease of comparison, we adhere to a consistent label budget per round to illustrate their respective impacts. As shown in Figure 3, our method consistently preserves a lower fairness violation than the BALD and ISAL baselines with a similar test accuracy. While we observe that the JTT-20 algorithm can achieve a near-zero fairness violation under a limited budget on the CelebA dataset, we argue that the model accuracy is rather uninformative (about 50%). More empirical results can be found in Appendix E.3.

How does the validation set size affect the performance?We further explore the impact of adjusting the validation set size on our algorithm's performance. We present the test accuracy and fairness violations across different validation set sizes on the CelebA dataset. Note that the default validation set size is set to 1% of the whole dataset size. In particular, the minimum scale of the validation set size is set to \(}{{5}}\) (nearly 400 CelebA images). The results in Figure 3 indicate that our algorithm still retains the test accuracy and fairness violation when we vary the validation set size. Additional results conducted on Adult and Compas datasets are provided in Appendix E.4.

## 7 Conclusions and limitations

In this work, we are interested in facilitating ML models that mitigate group fairness disparity without harming model accuracy. To achieve this, different from current active sampling methods, we propose a tractable fair influential sampling method FIS, which avoids the need for training group annotations during the sampling or training phase, thereby preventing the potential exposure of sensitive information. In particular, this algorithm acquires data samples from a large dataset for training based on the influence of fairness and accuracy evaluated on the auxiliary validation dataset. Empirical experiments on real-world data validate the efficacy of our proposed method.

Nonetheless, we recognize that our method has limitations. Although the proposed sampling algorithm does not require sensitive attribute information from the massive data, it relies on a clean and informative validation set that contains the sensitive attributes of data examples. We consider this as a reasonable requirement in practice, given the relatively modest size of the validation set. Besides, one potential concern is that the sampling strategy may become inefficient when the collected validation set is noisy. However, this practical issue can be heavily alleviated by using loss correction methods [54; 84] or noise-tolerant fairness loss functions to rectify the error terms [53; 73; 29]. In future work, we aim to address this limitation by developing more robust sampling strategies that can perform effectively even with noisy validation sets.

## Broader Impact

This paper presents work whose goal is to advance the field of fairness in machine learning. There are many potential societal consequences of our work. While the proposed algorithm does intend to infer sensitive attributes of data examples that may be protected by privacy regulations, it does not necessitate direct access to such sensitive information. On the other hand, our work can serve as an effective approach leading to mitigating the disparity with a limited annotation budget. We have thoroughly examined the potential ethical implications of our work and, based on our assessment, do not identify any issues that we deem necessary to emphasize here specifically.