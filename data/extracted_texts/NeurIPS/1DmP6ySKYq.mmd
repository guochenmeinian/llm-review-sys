# HeadSculpt: Crafting 3D Head Avatars with Text

Xiao Han\({}^{1,4}\)

Equal contributions \({}^{}\) Corresponding authors \({}^{}\) Webpage: https://brandonhan.uk/HeadSculpt

Yukang Cao\({}^{2}\)

Equal contributions \({}^{1}\)University of Surrey \({}^{2}\)The University of Hong Kong \({}^{3}\)Imperial College London

Kai Han\({}^{2}\)

Equal contributions \({}^{1}\)University of Surrey \({}^{2}\)The University of Hong Kong \({}^{3}\)Imperial College London

Xiatian Zhu\({}^{1,5}\)

Equal contributions \({}^{1}\)University of Surrey \({}^{2}\)The University of Hong Kong \({}^{3}\)Imperial College London

Jiankang Deng\({}^{3}\)

Equal contributions \({}^{1}\)University of Surrey \({}^{2}\)The University of Hong Kong \({}^{3}\)Imperial College London

Yi-Zhe Song\({}^{1,4}\)

Equal contributions \({}^{1}\)University of Surrey \({}^{2}\)The University of Hong Kong \({}^{3}\)Imperial College London

Tao Xiang\({}^{1,4}\)

Equal contributions \({}^{1}\)University of Surrey \({}^{2}\)The University of Hong Kong \({}^{3}\)Imperial College London

Kwan-Yee K. Wong\({}^{2}\)

Equal contributions \({}^{1}\)University of Surrey \({}^{2}\)The University of Hong Kong \({}^{3}\)Imperial College London

###### Abstract

Recently, text-guided 3D generative methods have made remarkable advancements in producing high-quality textures and geometry, capitalizing on the proliferation of large vision-language and image diffusion models. However, existing methods still struggle to create high-fidelity 3D head avatars in two aspects: (1) They rely mostly on a pre-trained text-to-image diffusion model whilst missing the necessary 3D awareness and head priors. This makes them prone to inconsistency and geometric distortions in the generated avatars. (2) They fall short in fine-grained editing. This is primarily due to the inherited limitations from the pre-trained 2D image diffusion models, which become more pronounced when it comes to 3D head avatars. In this work, we address these challenges by introducing a versatile coarse-to-fine pipeline dubbed **HeadSculpt** for crafting (_i.e._, generating and editing) 3D head avatars from textual prompts. Specifically, we first equip the diffusion model with 3D awareness by leveraging landmark-based control and a learned textual embedding representing the back view appearance of heads, enabling 3D-consistent head avatar generations. We further propose a novel identity-aware editing score distillation strategy to optimize a textured mesh with a high-resolution differentiable rendering technique. This enables identity preservation while following the editing instruction. We showcase HeadSculpt's superior fidelity and editing capabilities through comprehensive experiments and comparisons with existing methods. \({}^{}\)

## 1 Introduction

Modeling 3D head avatars underpins a wide range of emerging applications (_e.g._, digital telepresence, game character creation, and AR/VR). Historically, the creation of intricate and detailed 3D head avatars demanded considerable time and expertise in art and engineering. With the advent of deep learning, existing works  have shown promising results on the reconstruction of 3D human heads from monocular images or videos. However, these methods remain restricted to head appearance contained in their training data which is often limited in size, resulting in the inability to generalize to new appearance beyond the training data. This constraint calls for the need of more flexible and generalizable methods for 3D head modeling.

Recently, vision-language models (_e.g._, CLIP ) and diffusion models (_e.g._, Stable Diffusion ) have attracted increasing interest. These progresses have led to the emergence of text-to-3D generative models  which create 3D content in a self-supervised manner. Notably, DreamFusion  introduces a score distillation sampling (SDS) strategy that leverages a pre-trained image diffusion model to compute the noise-level loss from the textual description, unlocking the potential to optimize differentiable 3D scenes (_e.g._, neural radiance field , tetrahedral mesh , texture , or point cloud ) with 2D diffusion prior only. Subsequent research efforts  improve and extend DreamFusion from various perspectives (_e.g._, higher resolution  and better geometry ).

Considering the flexibility and versatility of natural languages, one might think that these SDS-based text-to-3D generative methods would be sufficient for generating diverse 3D avatars. However, it is noted that existing methods have two major drawbacks (see Fig. 6): _(1) Inconsistency and geometric distortions_: The 2D diffusion models used in these methods lack 3D awareness particularly regarding camera pose; without any remedy, existing text-to-3D methods inherited this limitation, leading to the multi-face _"Janus"_ problem in the generated head avatars. _(2) Fine-grained editing limitations_: Although previous methods propose to edit 3D models by naively fine-tuning trained models with modified prompts , we find that this approach is prone to biased outcomes, such as identity loss or inadequate editing. This problem arises from two causes: (a) inherent bias in prompt-based editing in image diffusion models, and (b) challenges with inconsistent gradient back-propagation at separate iterations when using SDS calculated from a vanilla image diffusion model.

In this paper, we introduce a new head-avatar-focused text-to-3D method, dubbed **HeadSculpt**, that supports high-fidelity generation and fine-grained editing. Our method comprises two novel

Figure 1: **Examples of generation and editing results obtained using the proposed HeadSculpt. It enables the creation and fine-grained editing of high-quality head avatars, featuring intricate geometry and texture, for any type of head avatar using simple descriptions or instructions. Symbols indicate the following prompt prefixes: "a head of [text]" and "a DSLR portrait of [text]". The captions in gray are the prompt suffixes while the blue ones are the editing instructions.**

components: _(1) Prior-driven score distillation:_ We first arm the pre-trained image diffusion model with 3D awareness by integrating a landmark-based ControlNet . Specifically, we adopt the parametric 3D head model, FLAME , as a prior to obtain a 2D landmark map [41; 31], which serves as an additional condition for the diffusion model, ensuring the consistency of generated head avatars across different views. Further, to remedy the front-view bias in the pre-trained diffusion model, we utilize an improved view-dependent prompt through textual inversion , by learning a specialized <back-view> token to emphasize back views of heads and capture their unique visual details. _(2) Identity-aware editing score distillation (IESD):_ To address the challenges of fine-grained editing for head avatars, we introduce a novel method called IESD. It blends two scores, one for editing and the other for identity preservation, both predicted by a ControlNet-based implementation of InstructPix2Pix . This approach maintains a controlled editing direction that respects both the original identity and the editing instructions. To further improve the fidelity of our method, we integrate these two novel components into a coarse-to-fine pipeline , utilizing NeRF  as the low-resolution coarse model and DMTet as the high-resolution fine model. As demonstrated in Fig. 1, our method can generate high-fidelity human-like and non-human-like head avatars while enabling fine-grained editing, including local changes, shape/texture modifications, and style transfers.

## 2 Related work

**Text-to-2D generation.** In recent years, groundbreaking vision-language technologies such as CLIP  and diffusion models [25; 13; 59; 68] have led to significant advancements in text-to-2D content generation [61; 57; 1; 69; 70]. Trained on extensive 2D multimodal datasets [63; 64], they are empowered with the capability to _"dream"_ from the prompt. Follow-up works endeavor to efficiently control the generated results [84; 85; 47], extend the diffusion model to video sequence [67; 3], accomplish image or video editing [23; 32; 81; 5; 77; 14; 22], enhance the performance for personalized subjects [60; 17], etc. Although significant progress has been made in generating 2D content from text, carefully crafting the prompt is crucial, and obtaining the desired outcome often requires multiple attempts. The inherent randomness remains a challenge, especially for editing tasks.

**Text-to-3D generation.** Advancements in text-to-2D generation have paved the way for text-to-3D techniques. Early efforts [82; 27; 44; 62; 34; 29; 11] propose to optimize the 3D neural radiance field (NeRF) or vertex-based meshes by employing the CLIP language model. However, these models encounter difficulties in generating expressive 3D content, primarily because of the limitations of CLIP in comprehending natural language. Fortunately, the development of image diffusion models [69; 1] has led to the emergence of DreamFusion . It proposes Score Distillation Sampling (SDS) based on a pre-trained 2D diffusion prior , showcasing promising generation results. Subsequent works  have endeavored to improve DreamFusion from various aspects: Magic3D  proposes a coarse-to-fine pipeline for high-resolution generations; Latent-NeRF  includes shape guidance for more robust generation on the latent space ; DreamAvatar  leverages SMPL  to generate 3D human full-body avatars under controllable shapes and poses; Guide3D  explores the usage of multi-view generated images to create 3D human avatars; Fantasia3D  disentangles the geometry and texture training with DMTet and PBR texture  as their 3D representation; 3DFuse  integrates depth control and semantic code sampling to stabilize the generation process. Despite notable progress, current text-to-3D generative models still face challenges in producing view-consistent 3D content, especially for intricate head avatars. This is primarily due to the absence of 3D awareness in text-to-2D diffusion models. Additionally, to the best of our knowledge, there is currently no approach that specifically focuses on editing the generated 3D content, especially addressing the intricate fine-grained editing needs of head avatars.

**3D head modeling and creation.** Statistical mesh-based models, such as FLAME [38; 15], enable the reconstruction of 3D head models from images. However, they struggle to capture fine details like hair and wrinkles. To overcome this issue, recent approaches [8; 71; 72; 51] employ Generative Adversarial Networks (GANs) [46; 20; 30] to train 3D-aware networks on 2D head datasets and produce 3D-consistent images through latent code manipulation. Furthermore, neural implicit methods [87; 16; 28; 88] introduce implicit and subject-oriented head models based on neural rendering fields [45; 48; 2]. Recently, text-to-3D generative methods have gained traction, generating high-quality 3D head avatars from natural language using vision-language models [55; 69]. Typically, T2P  predicts bone-driven parameters of head avatars via a game engine under the CLIP guidance . Rodin  proposes a roll-out diffusion network to perform 3D-aware diffusion. DreamFace  employs a selection strategy in the CLIP embedding space to generate coarse geometry and uses SDS  to optimize UV-texture. Despite producing promising results, all these methods require a large amount of data for supervised training and struggle to generalize well to non-human-like avatars. In contrast, our approach relies solely on pre-trained text-to-2D models, generalizes well to out-of-domain avatars, and is capable of performing fine-grained editing tasks.

## 3 Methodology

HeadSculpt is a 3D-aware text-to-3D approach that utilizes a pre-trained text-to-2D Stable Diffusion model [69; 59] to generate high-resolution head avatars and perform fine-grained editing tasks. As illustrated in Fig. 2, the generation pipeline has two stages: coarse generation via the neural radiance field (NeRF)  and refinement/editing using tetrahedron mesh (DMTet) . Next, we will first introduce the preliminaries that form the basis of our method in Sec. 3.1. We will then discuss the key components of our approach in Sec. 3.2 and Sec. 3.3, including (1) the prior-driven score distillation process via landmark-based ControlNet  and textual inversion , and (2) identity-aware editing score distillation accomplished in the fine stage using the ControlNet-based InstructPix2Pix .

### Preliminaries

**Score distillation sampling.** Recently, DreamFusion  proposed score distillation sampling (SDS) to self-optimize a text-consistent neural radiance field (NeRF) based a the pre-trained text-to-2D diffusion model . Due to the unavailability of the Imagen model  used by DreamFusion, we employ the latent diffusion model in  instead. Specifically, given a latent feature \(z\) encoded from an image \(x\), SDS introduces random noise \(\) to \(z\) to create a noisy latent variable \(z_{t}\) and then uses a pre-trained denoising function \(_{}(z_{t};y,t)\) to predict the added noise. The SDS loss is defined as the difference between predicted and added noise and its gradient is given by

\[_{}_{}(,g())=_{t, (0,1)}[w(t)(_{}(z_{t};y,t )-)],\] (1)

where \(y\) is the text embedding, \(w(t)\) weights the loss from noise level \(t\). With the expressive text-to-2D diffusion model and self-supervised SDS loss, we can back-propagate the gradients to optimize an implicit 3D scene \(g()\), eliminating the need for an expensive 3D dataset.

**3D scene optimization.** HeadSculpt explores the potential of two different 3D differentiable representations as the optimization basis for crafting 3D head avatars. Specifically, we employ NeRF  in the coarse stage due to its greater flexibility in geometry deformation, while utilizing DMTet in the fine stage for efficient high-resolution optimization.

Figure 2: **Overall architecture of HeadSculpt. We craft high-resolution 3D head avatars in a coarse-to-fine manner. (a) We optimize neural field representations for the coarse model. (b) We refine or edit the model using the extracted 3D mesh and apply identity-aware editing score distillation if editing is the target. (c) The core of our pipeline is the prior-driven score distillation, which incorporates landmark control, enhanced view-dependent prompts, and an InstructPix2Pix branch.**

**(1) _3D prior-based NeRF._ DreamAvatar  recently proposed a density-residual setup to enhance the robustness and controllability of the generated 3D NeRF. Given a point \(\) inside the 3D volume, we can derive its density and color value based on a prior-based density field \(\):

\[F(,)=F_{}(())+(( ),)(,),\] (2)

where \(()\) denotes a hash-grid frequency encoder , and \(\) and \(\) are the density and RGB color respectively. We can derive \(\) from the signed distance \(d()\) of a given 3D shape prior (_e.g._, a canonical FLAME model  by default in our implementation):

\[()=(0,^{-1}(()) ),\,()=(-d()/a), a=0.005.\] (3)

To obtain a 2D RGB image from the implicit volume defined above, we employ a volume rendering technique that involves casting a ray \(\) from the 2D pixel location into the 3D scene, sampling points \(_{i}\) along the ray, and calculating their density and color value using \(F\) in Eq. (2):

\[C()=_{i}W_{i}_{i}, W_{i}=_{i}_{j<i}(1 -_{j}),_{i}=1-e^{(-_{i}||_{i}- _{i+1}||)}.\] (4)

**(2) DMTet.** It discretizes a deformable tetrahedral grid (\(V_{T},T\)), where \(V_{T}\) denotes the vertices within grid \(T\), to model the 3D space. Every vertex \(_{i} V_{T}^{3}\) possesses a signed distance value \(s_{i}\), along with a position offset \(_{i}^{3}\) of the vertex relative to its initial canonical coordinates. Subsequently, the underlying mesh can be extracted based on \(s_{i}\) with the differentiable marching tetrahedra algorithm. In addition to the geometry, we adopt the Magic3D approach  to construct a neural color field. This involves re-utilizing the MLP trained in the coarse NeRF stage to predict the RGB color value for each 3D point. During optimization, we render this textured surface mesh into high-resolution images using a differentiable rasterizer .

### 3D-Prior-driven score distillation

Existing text-to-3D methods with SDS  assume that maximizing the likelihood of images rendered from various viewpoints of a scene model \(g()\) is equivalent to maximizing the overall likelihood of \(g()\). This assumption can result in inconsistencies and geometric distortions . A notable issue is the "_Janus problem_" characterized by multiple faces on a single object (see Fig. 6). There are two possible causes: (1) the randomness of the diffusion model which can cause inconsistencies among different views, and (2) the lack of 3D awareness in controlling the generation process, causing the model to struggle in determining the front view, back view, etc. To address these issues in generating head avatars, we integrate 3D head priors into the diffusion model.

**Landmark-based ControlNet.** In Section 3.1, we explain our adoption of FLAME  as the density guidance for our NeRF. Nevertheless, this guidance by itself is insufficient to have a direct impact on the SDS loss. What is missing is a link between the NeRF and the diffusion model, incorporating the same head priors. Such a link is key to improving the view consistency of the generated head avatars. To achieve this objective, as illustrated in Fig. 2, we propose the incorporation of 2D landmark maps as an additional condition for the diffusion model using ControlNet . Specifically, we employ a ControlNet \(\) trained on a large-scale 2D face dataset , using facial landmarks rendered from MediaPipe  as ground-truth data. When given a randomly sampled camera pose \(\), we first project the vertices of the FLAME model onto the image. Following that, we select and render some of these vertices into a landmark map \(_{}\) based on some predefined vertex indexes. The landmark map will be fed into ControlNet and its output features are added to the intermediate features within the diffusion U-Net. The gradient of our SDS loss can be re-written as

\[_{}_{}(,g())=_{t, (0,1),}[w(t)(_{}(z_{t};y, t,(_{}))-)].\] (5)

**Enhanced view-dependent prompt via textual inversion.** Although the landmark-based ControlNet can inject 3D awareness into the pre-trained diffusion models, it struggles to maintain back view head consistency. This is expected as the 2D image dataset used for training mostly contains only front or side face views. Consequently, when applied directly to back views, the model introduces ambiguity as front and back 3D landmark views can appear similar, as shown in Fig. 8. To address this issue, we propose a simple yet effective method. Our method is inspired by previous works  which found it beneficial to append view-dependent text (_e.g._, "front view", "side view" or "backview") to the provided input text based on the azimuth angle of the randomly sampled camera. We extend this idea by learning a special token <back-view> to replace the plain text "back view" in order to emphasize the rear appearance of heads. This is based on the assumption that a pre-trained Stable Diffusion does has the ability to "imagine" the back view of a head - it has seen some during training. The main problem is that a generic text embedding of "back view" is inadequate in telling the model what appearance it entails. A better embedding for "back view" is thus required. To this end, we first randomly download 34 images of the back view of human heads, without revealing any personal identities, to construct a tiny dataset \(\), and then we optimize the special token \(v\) (_i.e._, <back-view>) to better fit the collected images, similar to the textual inversion :

\[v_{*}=*{arg\,min}_{v}_{t,(0,1), z}[\|-_{}(z_{t};v,t) \|_{2}^{2}],\] (6)

which is achieved by employing the same training scheme as the original diffusion model, while keeping \(_{}\) fixed. This constitutes a reconstruction task, which we anticipate will encourage the learned embedding to capture the fine visual details of the back views of human heads. Notably, as we do not update the weights of \(_{}\), it stays compatible with the landmark-based ControlNet.

### Identity-aware editing score distillation

After generating avatars, editing them to fulfill particular requirements poses an additional challenge. Previous works [54; 39] have shown promising editing results by fine-tuning a trained scene model with a new target prompt. However, when applied to head avatars, these methods often suffer from identity loss or inadequate appearance modifications (see Fig. 10). This problem stems from the inherent constraint of the SDS loss, where the 3D models often sacrifice prominent features to preserve view consistency. Substituting Stable Diffusion with InstructPix2Pix [5; 21] might seem like a simple solution, but it also faces difficulties in maintaining facial identity during editing based only on instructions, as it lacks a well-defined anchor point.

To this end, we propose identity-aware editing score distillation (IESD) to regulate the editing direction by blending two predicted scores, _i.e._, one for editing instruction and another for the original description. Rather than using the original InstructPix2Pix , we employ a ControlNet-based InstructPix2Pix \(\) trained on the same dataset, ensuring compatibility with our landmark-based ControlNet \(\) and the learned <back-view> token. Formally, given an initial textual prompt \(y\) describing the avatar to be edited and an editing instruction \(\), we first input them separately into the same diffusion model equipped with two ControlNets, \(\) and \(\). This allows us to obtain two predicted noises, which are then combined using a predefined hyper-parameter \(_{e}\) like classifier-free diffusion guidance (CFG) :

\[_{}_{}(,g( ))&=_{t,(0,1),}[w (t)(_{}(z_{t};y,,t,(_ {}),(_{}))-)],\\ &_{e}_{}(z_{t};,t,( _{}),(_{}))+(1-_{e} )_{}(z_{t};y,t,(_{}),(_{}))\] (7)

where \(_{}\) and \(_{}\) represent the 2D landmark maps and the reference images rendered in the coarse stage, both being obtained under the sampled camera pose \(\). The parameter \(_{e}\) governs a trade-off between the original appearance and the desired editing, which defaults to \(0.6\) in our experiments.

## 4 Experiments

We will now assess the efficacy of our HeadSculpt across different scenarios, while also conducting a comparative analysis against state-of-the-art text-to-3D generation pipelines.

**Implementation details.** HeadSculpt builds upon Stable-DreamFusion  and Huggingface Diffusers [78; 53]. We utilize version 1.5 of Stable Diffusion  and version 1.1 of ControlNet [84; 12] in our implementation. In the coarse stage, we optimize our 3D model at \(64 64\) grid resolution, while using \(512 512\) grid resolution for the fine stage (refinement or editing). Typically, each text prompt requires approximately \(7,000\) iterations for the coarse stage and \(5,000\) iterations for the fine stage. It takes around 1 hour for each stage on a single Tesla V100 GPU with a default batch size of 4. We use Adam  optimizer with a fixed learning rate of \(0.001\). Additional implementation details can be found in the supplementary material.

**Baseline methods for generation evaluation.** We compare the generation results with five baselines: DreamFusion , Latent-NeRF , 3DFuse  (improved version of SJC ), Fantasia3D , and DreamFace . We do not directly compare with DreamAvatar  as it involves deformation fields for full-body-related tasks.

**Baseline methods for editing evaluation.** We assess IESD's efficacy for fine-grained 3D head avatar editing by comparing it with various alternatives since no dedicated method exists for this: **(B1)** One-step optimization on the coarse stage without initialization; **(B2)** Initialized from the coarse stage, followed by optimization of another coarse stage with an altered description; **(B3)** Initialized from the coarse stage, followed by optimization of a new fine stage with an altered description; **(B4)** Initialized from the coarse stage, followed by optimization of a new fine stage with an instruction based on the vanilla InstructPix2Pix ; **(B5)** Ours without edit scale (_i.e._, \(_{e}=1\)). Notably, B2 represents the editing method proposed in DreamFusion , while B3 has a similar performance as Magic3D , which employs a three-stage editing process (_i.e._, Coarse + Coarse + Fine).

### Qualitative evaluations

**Head avatar generation with various prompts.** In Fig. 1, we show a diverse array of 3D head avatars generated by our HeadSculpt, consistently demonstrating high-quality geometry and texture across various viewpoints. Our method's versatility is emphasized by its ability to create an assortment of avatars, including humans (both celebrities and ordinary individuals) as well as non-human characters like superheroes, comic/game characters, paintings, and more.

**Head avatar generation with different shapes.** HeadSculpt leverages shape-guided NeRF initialization and landmark-guided diffusion priors. This allows controlling geometry by varying the FLAME shape used for initialization. To demonstrate adjustability, Fig. 3 presents examples generated from diverse FLAME shapes. The results fit closely to the shape guidance, highlighting HeadSculpt's capacity for geometric variation when provided different initial shapes.

**Head avatar editing with various instructions.** As illustrated in Fig. 1 and Fig. 4, HeadSculpt's adaptability is also showcased through its ability to perform fine-grained editing, such as local changes (_e.g._, adding accessories, changing hairstyles, or altering expressions), shape and texture modifications, and style transfers.

**Head avatar editing with different edit scales.** In Fig. 5, we demonstrate the effectiveness of IESD with different \(_{e}\) values, highlighting its ability to control editing influence on the reference identity.

Figure 4: **More specific editing results.**\(\) Instruction prefix: _make his expression as_ [text].

Figure 3: **Generation results with various shapes.** The first row shows three randomly sampled FLAME models, while the second row presents our generated results (incl. normals) using these FLAME models as initialization. All results are under the same text prompt.

**Comparison with existing methods on generation results.** We provide qualitative comparisons with existing methods in Fig. 6. We employ the same FLAME model for Latent-NeRF  to compute their sketch-guided loss and for Fantasia3D  as the initial geometry. The following observations can be made: **(1)** All baselines tend to be more unstable during training than ours, often resulting in diverged training processes; **(2)** Latent-NeRF occasionally produces plausible results due to its use of the shape prior, but its textures are inferior to ours since optimization occurs solely in the latent space; **(3)** Despite 3DFuse's depth control to mitigate the Janus problem, it still struggles to generate 3D consistent head avatars; **(4)** While Fantasia3D can generate a mesh-based 3D avatar, its geometry is heavily distorted, as its disentangled geometry optimization might be insufficient for highly detailed head avatars; **(5)** Although DreamFace generates realistic human face textures, it falls short in generating (i) complete heads, (ii) intricate geometry, (iii) non-human-like appearance, and (iv) composite accessories. In comparison, our method consistently yields superior results in both geometry and texture with much better consistency for the given prompt. More comparisons can be found in the supplementary material.

Figure 5: **Impact of the edit scale \(_{e}\) in IESD.** It balances the preservation of the initial appearance and the extent of the desired editing, making the editing process more controllable and flexible.

Figure 6: **Comparison with existing text-to-3D methods.** Unlike other methods that struggle or fail to generate reasonable results, our approach consistently achieves high-quality geometry and texture, yielding superior results. *Non-official implementation. \(\) Generated from the online website demo.

### Quantitative evaluations

**User studies.** We conducted user studies comparing with four baselines [73; 74; 65; 43]. 42 volunteers ranked them from 1 (worst) to 5 (best) individually based on three dimensions: **(1)** consistency with the text, **(2)** texture quality, and **(3)** geometry quality. The results, shown in Fig. 7, indicate that our method achieved the highest rank in all three aspects by large margins.

**CLIP-based metrics.** Following DreamFusion , **(1)** We calculate the CLIP R-Precision (CLIP-R)  and CLIP-Score (CLIP-S)  metrics, which evaluate the correlation between the generated images and the input texts, for all methods using 30 text prompts. As indicated in Tab. 1, our approach significantly outperforms the competing methods according to both metrics. This outcome provides additional evidence for the subjective superiority observed in the user studies and qualitative results. **(2)** We employ the CLIP Directional Similarity (CLIP-DS) [5; 18], to evaluate the editing performance. This metric measures the alignment between changes in text captions and corresponding image modifications. Specifically, we encode a pair of images (the original and edited 3D models rendered from a specific viewpoint) along with a pair of text prompts describing the original and edited subjects, _e.g._, "a DSLR portrait of Saul Goodman" and "a DSLR portrait of Saul Goodman dressed like a clown". We compare our approach against B3, B4, and B5 by evaluating 10 edited examples. The results, presented in Tab. 1, highlight the superiority of our editing framework according to this metric, indicating improved editing fidelity and identity preservation compared to alternatives.

### Further analysis

**Effectiveness of prior-driven score distillation.** In Fig. 8, we conduct ablation studies to examine the impact of the proposed landmark control and textual inversion priors in our method. We demonstrate this on the coarse stage because the refinement and editing results heavily depend on this stage. The findings show that landmark control is essential for generating spatially consistent head avatars. Without it, the optimized 3D avatar faces challenges in maintaining consistent facial views, particularly for non-human-like characters. Moreover, textual inversion is shown to be another vital component in mitigating the Janus problem, specifically for the back view, as landmarks cannot exert control on the rear view. Overall, the combination of both components enables HeadSculpt to produce view-consistent avatars with high-quality geometry.

  
**Generation** & **Dream Fusion** & **Latent-Net** & **3DPE** & **Fantasia3D** & **Ours** \\ 
**CLIP-R** & 95.83 & 87.50 & 70.83 & 62.50 & **100.00** \\
**CLIP-S** & 26.06 & 26.30 & 23.41 & 23.26 & **29.52** \\ 
**Editing** & **B3** & **B4** & **B5** & **Ours** \\ 
**CLIP-DS** & 16.62 & 8.76 & 14.03 & **16.84** \\   

Table 1: **Objective evaluation with CLIP-based metrics.** All numbers are calculated with CLIP-L/14.

Figure 8: **Analysis of prior-driven score distillation. Figure 9: **Failure cases.**

**Effectiveness of IESD.** In Fig. 10, we present two common biased editing scenarios produced by the baseline methods: insufficient editing and loss of identity. With Stable Diffusion, specific terms like "Saul Goodman" and "skull" exert a more substantial influence on the text embeddings compared to other terms, such as "older" and "Vincent van Gogh". B1, B2, and B3, all based on vanilla Stable Diffusion, inherit such bias in their generated 3D avatars. Although B4 does not show such bias, it faces two other issues: **(1)** the Janus problem reemerges due to incompatibility between vanilla InstructPix2Pix and the proposed prior-driven score distillation; **(2)** it struggles to maintain facial identity during editing based solely on instructions, lacking a well-defined anchor point. In contrast, B5 employs ControlNet-based InstructPix2Pix  with the proposed prior score distillation, resulting in more view-consistent editing. Additionally, our IESD further uses the proposed edit scale to merge two predicted scores, leading to better identity preservation and more effective editing. This approach allows our method to overcome the limitations faced by the alternative solutions, producing high-quality 3D avatars with improved fine-grained editing results.

**Limitations and failure cases.** While setting a new state-of-the-art, we acknowledge HeadSculpt has limitations, as the failure cases in Fig. 9 demonstrate: **(1)** non-deformable results hinder further extensions and applications in audio or video-driven problems; **(2)** generated textures are highly saturated and less realistic, especially for characters with highly detailed appearances (_e.g._, Freddy Krueger); **(3)** some inherited biases from Stable Diffusion  still remain, such as inaccurate and stereotypical appearances of Asian characters (_e.g._, Sun Wukong and Japanese Geisha); and **(4)** limitations inherited from InstructPix2Pix , such as the inability to perform large spatial manipulations (_e.g._, remove his nose).

## 5 Conclusions

We have introduced HeadSculpt, a novel pipeline for generating high-resolution 3D human avatars and performing identity-aware editing tasks through text. We proposed to utilize a prior-driven score distillation that combines a landmark-based ControlNet and view-dependent textual inversion to address the Janus problem. We also introduced identity-aware editing score distillation that preserves both the original identity information and the editing instruction. Extensive evaluations demonstrated that our HeadSculpt produces high-fidelity results under various scenarios, outperforming state-of-the-art methods significantly.

**Societal impact.** The advancements in geometry and texture generation for human head avatars could be deployed in many AR/VR use cases but also raises concerns about their potential malicious use. We encourage responsible research and application, fostering open and transparent practices.

**Acknowledgment.** This work is partially supported by Hong Kong Research Grant Council - Early Career Scheme (Grant No. 27208022) and HKU Seed Fund for Basic Research. We also thank the anonymous reviewers for their constructive suggestions.

Figure 10: **Analysis of identity-aware editing score distillation.**