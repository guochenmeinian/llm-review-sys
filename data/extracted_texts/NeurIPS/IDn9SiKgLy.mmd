# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

A prevalent issue in this domain is the lack of both shared assumptions and theoretical guarantees, making fair comparisons challenging. Our community has yet to reach a consensus on acceptable assumptions, particularly in the following areas. **(a) The level of effectiveness of experts' knowledge:** assuming near oracle-like knowledge, e.g. in [11; 39; 12], collaborative settings can significantly surpass vanilla BO. However, if experts are entirely erroneous (yet confident)--which can happen [43; 50; 24; 7]--overreliance on experts' input cannot guarantee the global optimum convergence. **(b) Human interaction method:** ideally, humans prefer minimising interaction with machines for convenience. Minimising interaction leads to maximising the information at each query to human, which often ends up requesting error-free and quantitative information for humans [82; 11; 43; 42]. However, accurate knowledge elicitation remains a long-standing quest [79; 68; 58]. Inversely, when we assume human belief is also a black-box function and require the elicitation of the belief function through statistical modelling, e.g. [73; 34; 7], we will demand excessive queries of the experts.

**Contributions.** We propose an expert-advised algorithm with the contributions summarised below:

1. **Handover guarantee**: we model the expert's role as cognitively simple and qualitative--the expert serves as a black-box classifier, providing binary labels on the desirability of the next query location. Similar to the no-regret property, we establish a sublinear bound on the cumulative number of binary labels needed. Initially, multiple labels per query are needed, but the frequency of querying binary labels asymptotically converges to zero, thus saving both expert effort and computation time.
2. **No-harm guarantee**: we show that the convergence rate of our expert-advised algorithm will not be worse than that of vanilla BO (i.e. without expert advice), even if the advice from experts is adversarial. Our convergence is achieved through data-driven trust level adjustments, and is unlike existing methods that rely on hand-tuned user-defined functions.
3. **Real-world contribution**: empirically, our algorithm provides both fast convergence and resilience against erroneous inputs. It outperformed existing methods in both popular synthetic, and new real-world, tasks in designing lithium-ion batteries.

## 2 Problem Statement

We address the black-box optimization problem,

\[x^{}_{x}\;f(x)\;,\] (1)

while collaborating with an expert, where \(^{d}\) and \(d\) is the dimension.

**Expert labelling model.** We model an expert as a binary labeller (see Fig. 1). An expert labels a point \(x\) as either 'accept' or'reject'. An 'accept' label indicates that the point is worth sampling, while'reject' label indicates it is not. These labels are binary, with \(0\) for 'accept' and \(1\) for'reject'. In practice, the labelling process can be noisy, since humans may find some points hard to classify. Non-expert or incorrect belief may label the optimum \(x^{}\)'reject'. The distribution of the labels is determined by the expert's prior belief about the black-box function \(f\), and we model the expert's belief through another unknown black-box function \(g\).

**Assumption 2.1**.: The notation \(x_{g}0\) denotes the event where \(x\) is labelled as'reject', based on the expert's belief function \(g\). Additionally, the random indicator \(_{x_{g}0}\{0,1\}\) takes value \(1\) if \(x_{g}0\) and \(0\) otherwise. The probability distribution of \(_{x_{g}0}\{0,1\}\) follows the Bernoulli distribution with \((_{x_{g}0}=1)=p_{x_{g}0}=S(g(x))\), where \(S(u)=}{{(1+e^{-u})}}\) is the sigmoid function.

**Example 2.2**.: Let us define an example'synthetic' expert's labelling response as \(p_{x_{g}0}=S(a(f(x)))\), where \(a\) is the accuracy coefficient and \(\) is the linear scaling function from bound \([_{x}f(x),_{x}f(x)]\) to \([-3,3]\). When \(a=1\), \((f(x^{}))=-3,S(-3) 0.05\), resulting in a Bernoulli distribution that yields an acceptance label of 0 with a 95% chance at the global minimum \(x^{}\). In this case, the sharpness of the belief \(p_{x_{g}0}\) is influenced by both the shape of \(f(x)\) and \(a\); if \(f(x)\) is peaky or \(a 1\), the expert can nearly pinpoint \(x^{}\).

Figure 1: **BO-expert collaboration framework**: The algorithm (red) decides if an expertâ€™s (blue) label is necessary. If rejected, it generates a different candidate; otherwise, it directly queries.

However, in reality, the expert does not know the exact true \(f\) and therefore, we consider \(g\) to be a'subjective' belief function representing \(f\). This differs from a typical surrogate model \(\) of \(f\), which infers an 'objective' belief function from oracle queries. If \(g\) has better predictive ability than the surrogate model \(\), exploiting \(g\) can accelerate convergence; otherwise it may decelerate the process. In the optimisation process, \(g\) may act as a regularizer function in addition to the objective function \(f\). For simplicity, we use this Ex. 2.2 as synthetic human feedback. Readers interested in other examples are encouraged to refer to Appendix H.

**Assumption 2.3**.: \(\) is compact and non-empty.

Assumption 2.3 is reasonable because in many applications (e.g., continuous hyperparameter tuning) of BO, we are able to restrict the optimisation into certain ranges based on domain knowledge. Regarding the black-box function \(f\) and the function \(g\), we assume that,

**Assumption 2.4**.: \(f_{k_{f}},g_{k_{g}}\), where \(k:^{d}^{d}\), representing \(k_{f}\) or \(k_{g}\), is a symmetric, positive-semidefinite kernel function, and \(_{k}\) is its corresponding reproducing kernel Hilbert space (RKHS, see ). Furthermore, we assume \(\|f\|_{k_{f}} B_{f}\) and \(\|g\|_{k_{g}} B_{g}\), where \(\|\|_{k}\) is the norm induced by the inner product in the corresponding RKHS \(_{k}\). We use \(_{g}\) to denote the set \(\{_{k_{g}}\|\|_{k_{g}} B_{g}\}\).

Assumption 2.4 requires that the objective \(f\) and the function \(g\) are regular in the sense that they have bounded norms in the corresponding RKHS, which is a common assumption.

**Assumption 2.5**.: \(k(x,x^{}) 1,x,x^{}\), and \(k(x,x^{})\) is continuous on \(^{d}^{d}\).

**Assumption 2.6**.: At step \(t\), if query point \(x_{t}\) is evaluated, we get a noisy evaluation of \(f\) (we refer to an oracle query), \(y_{t}=f(x_{t})+_{t}\,,\) where \(_{t}\) is i.i.d \(\)-sub-Gaussian noise with fixed \(>0\).

**Notation**. We refer to \(_{}\) as data realisation of \(_{x_{}_{g}0}\) at step \(\). We denote the following sequences of steps: iterations as \([t]:=\{1,2,,t\}\), \(f\) queries as \(_{t}^{f}:=\{[t-1]f\}\), and expert queries as \(_{t}^{g}\), respectively \((t|_{t}^{g}|,t|_{t}^{f}|)\). We use capitals, e.g. \(X_{_{t}^{f}}\), for the set \((x_{})_{_{t}^{f}}\).

## 3 Confidence Set of the Surrogate Models

We introduce surrogate models for the objective \(f\) and the function \(g\). We opted for a Gaussian process (GP; [86; 100]) for \(f\) and the likelihood ratio model [67; 27] for \(g\).

### Surrogate Model of the Objective \(f\): Gaussian Process

**Definitions.** We employ a zero-mean GP regression model, with predictive posterior \(_{t} D_{t}^{f}(_{f_{t}},_{f_{t}}^{2})\),

\[_{f_{t}}(x) =k_{f}(X_{_{t}^{f}},x)^{}(K_{_{t}^ {f}}+rI)^{-1}Y_{_{t}^{f}},\] (2a) \[_{f_{t}}^{2}(x) =k_{f}(x,x)-k_{f}(X_{_{t}^{f}},x)^{} (K_{_{t}^{f}}+rI)^{-1}k_{f}(X_{_{t}^{f} },x),\] (2b)

where \(K_{_{t}^{f}}=(k_{f}(x_{_{1}},x_{_{2}}))_{_{1 },_{2}_{t}^{f}}\), \(D_{t}^{f}:=(X_{_{t}^{f}},Y_{_{t}^{f}})\), \(r\) is the regularisation term .2 The maximum information gain  for the objective \(f\) is,

\[_{|_{t}^{f}|}^{f}:=_{X;\,|X|=|_{t}^{f}|}|I+r^{-1}K_{f,X}|, K_{f,X}:=(k_{f}(x,x^{ }))_{x,x^{} X}.\] (3)

**Lemma 3.1** (Theorem 2, ).: _Let Assumptions 2.3, 2.4 and 2.6 hold. For any \((0,1)\), with probability at least \(1-/2\), the following holds for all \(x\) and \(1 t T\), \(T\),_

\[|_{f_{t}}(x)-f(x)|_{f_{t}}_{f_{t}}(x),_{f_{t}}:= (B_{f}+_{t-1}^{f}|}^{f}+1+(2/ ))}),\]

_where \(_{f_{t}}(x),_{f_{t}}(x)\) and \(_{|_{t-1}^{f}|}^{f}\) are as given in Eq. (2) and Eq. (3), and \(_{0}^{f}=0\)._For brevity, we denote the lower/upper confidence bound (LCB/UCB) functions \(_{t}(x)\) and \(_{t}(x)\) as,

\[_{t}(x)=_{f_{t}}(x)-_{f_{t}}_{f_{t}}(x)\,_{t}(x)=_{f_{t}}(x)+_{f_{t}} _{f_{t}}(x).\]

### Surrogate Model of the Expert Function \(g\): Likelihood Ratio Model

While a GP classifier  is a popular choice, we opted for likelihood ratio model [67; 27]. The combination of a Gaussian prior with a Bernoulli likelihood in GP models presents challenges in estimating the posterior confidence bound both theoretically and computationally. Moreover, GPs assume strong rankability [38; 23], presuming humans can rank their preferences accurately in all cases, which often leads to inconsistent results . To address these issues, we drew inspiration from classic expert elicitation methods using imprecise probability theory [10; 41]. Instead of estimating the predictive distribution, we estimate the 'interval' of the worst-case prediction only. This approach does not assume any distribution within the interval, thereby relaxing the rankability assumption . This method is particularly well-suited to the GP-UCB algorithm , which only requires a confidence interval. We developed a kernel-based method to provably estimate the predictive interval.

**Definitions.** First, we introduce the function, \(p_{}(x_{},_{}):=_{}S((x_{ }))+(1-_{})[1-S((x_{}))]\), which is the likelihood of \(\) over the event when \(_{x_{}>_{}}=_{}\) under the Assumption 2.1, and \(\) is an estimate function of \(g_{k_{g}}\) under the Assumption 2.4. We can then derive the likelihood function of a fixed function \(\) over the historical dataset \(_{t}^{g}:=\{(x_{},_{})\}_{_{t} ^{g}}\), which becomes the product, \(_{}((x_{},_{})_{_{t}^{g }}):=_{_{t}^{g}}p_{}(x_{},_{})\). The log-likelihood (LL) function,

\[_{t}():=_{}((x_{}, _{})_{_{t}^{g}}),\] (4)

\[_{t}()=_{_{t}^{g}}z_{ }_{}-_{_{t}^{g}}})},\] where \(z_{}=(x_{})\) (this equality can be checked as correct for either \(_{}=1\) or \(_{}=0\)). We then introduce the maximum likelihood estimator (MLE), \(_{t}^{}_{_{g}} _{}((x_{},_{})_{_{t}^{ g}})\). Similar to [54; 27; 107], the _confidence set_ can be derived as shown in Lemma 3.2.

**Lemma 3.2** (**Likelihood-based confidence set)**.: \(,>0\)_, let,_

\[_{g}^{t+1}:=\{_{g}_{t}()_{t}(_{t}^{})-_{1}(,,|_{t}^ {g}|,t)\},\]

_where \(_{1}(,,|_{t}^{g}|,t):=_{t} ^{g}|B_{g}^{2}}^{(_{ },\|.\|_{})}{6}}+2 t.\) We have,_

\[(g_{g}^{t+1}, t 1)  1-.\]

The proof is in Appendix A. As introduced in Assumption 2.4, while the function \(g\) was originally in a broader set of RKHS functions \(g_{g}\), it is now in a smaller set defined as \(g_{g}^{t+1}\) conditioned on the expert labels \(_{t}^{g}\). Intuitively, with limited data, the MLE may be imperfect. Hence, it is reasonable to suppose that \(_{q}^{t+1}\), bounded by LL values'slightly worse' than the MLE, contains the ground truth with high probability.

_Remark 3.3_ (**Choice of \(\)**).: In Lemma 3.2, \(_{1}(,,|_{t}^{g}|,t)\) depends on a small positive value \(\). It will be seen that \(\) can be selected to be \(}{{T}}\) in Appendix B, where \(T\) is the running horizon of the algorithm.

_Remark 3.4_ (**Confidence bound**).: By Lemma 3.2, we define the pointwise confidence bound for unknown \(g_{k_{g}}\), \(_{t}(x) g(x)_{t}(x)\), where \(_{t}(x):=_{_{g}^{t}}(x)\) and \(_{t}(x):=_{_{g}^{t}}(x)\).

_Remark 3.5_ (**Pointwise predictive interval estimation**).: At a given prediction point \(x\), the predictive interval \([_{t}(x),\ _{t}(x)]\) can be estimated through two individual finite-dimensional optimisation problems (See Appendix B.3 for details). Subsequently, applying the sigmoid function yields the predictive interval in probability space \([S(_{t}(x)),\ S(_{t}(x))]\) (see Fig. 2 for visualisation).

## 4 Algorithm and Theoretical Guarantees

### Mixing Two Surrogate Models \(f\) and \(g\) via Primal-Dual Method

**Primal dual**. We introduce the following primal-dual problem (5) as our acquisition policy,

\[:x_{t}^{c}_{x}_{t}(x)+ _{t}_{t}(x),\ :_{t+1}=[_{t}+_{t}(x_{t}^{c})]^{+},\] (5)where \(_{t}\) is the primal-dual weight at the \(t\)-th iteration and \(\) is the step size for dual update. See Fig. 2 for the intuition: we prioritise the sample in the expert-preferred region (i.e., the region with small \(_{t}(x)\)). The primal-dual method is a classical algorithm for constrained optimisation  and has recently been applied to, for example, the constrained bandit problem . In terms of constrained optimisation, Prob. (5) can be understood as solving \(_{x}(x)\) s.t. \(_{t}(x) 0\). Interestingly, the primal-dual approach is also roughly analogous to Bayesian inference . Just as the prior acts as a regulariser to the LL maximiser , expert belief \(_{t}(x)\) regularises the \(_{t}(x)\) minimiser. More specifically, the weight \(_{t+1}\) increases when \(_{t}(x_{t}^{c})>0\); otherwise, \(_{t+1}\) decreases. The condition \(_{t}(x_{t}^{c})>0\) indicates that the primal solution \(x_{t}^{c}\) is more likely to be rejected.3 Under such a risk of rejection, increasing the weights \(_{t+1}\) is natural because it more strongly regularises the \(_{t}\) minimiser to enhance feasibility in the next round, and vice versa.

**Level of trust.** Note that the primal-dual method is not the primary reason we achieve the no-harm guarantee. Indeed, its proof (detailed in Appendix B) does not rely on the primal-dual formulation. Therefore, technically speaking, our algorithm could employ a more aggressive exploitation of \(g_{t}\) (e.g., simply minimising \(g_{t}\)). Nevertheless, the primal-dual approach is our recommended policy for generating the expert-augmented candidate \(x_{t}^{c}\) to enhance resilience to erroneous inputs. The initial level of trust on \(g_{t}\) is determined by the initial weight \(_{0}\), where larger \(_{0}\) values correspond to greater trust in the expert. We compared the effect of \(_{0}\) in the Fig. 3 of the experimental section.

**Efficient computation.** Leveraging the representor theorem  due to the RKHS property, we further reformulate Prob. (5) to a \((|_{t}^{g}|+d+1)\)-dimensional, tractable optimisation problem (6).

\[_{Z_{_{t}^{g}}^{|_{t}^{g}| },\ z,\ x} _{t}(x)+_{t}z\] (6) subject to \[[Z_{_{t}^{g}}\\ z]^{}K_{_{t}^{g},x}^{-1}[Z_{_{t}^{g}}\\ z] B_{g}^{2},\] \[(Z_{_{t}^{g}}_{t}^{g})_{t }(_{t}^{})-_{1}(,,|_{t}^{g}|,t),\]

where \(K_{_{t}^{g},x}:=(k_{g}(,^{}))_{, ^{} X_{_{t}^{g}}\{x\}}\), and \((Z_{_{t}^{g}}_{t}^{g})=_{_{ t}^{g}}Z_{}_{}-_{_{t}^{g}}(1+e^{Z_{ }})\) is the LL value when \((x_{})=Z_{}\), \(_{t}^{g}\). We update \(_{t+1}=_{t}+ z^{}\), where \(z^{}=_{t}(x_{t}^{c})\) is the optimal \(z\) of Prob. (6).

**Key hyperparameter estimation.** A key hyperparameter in Prob. (6) is the norm bound \(B_{g}\) in the first constraint. Another hyperparameter, \(_{1}\), in the second constraint, also scales with \(_{g}\), (see Lemma 3.2). However, \(B_{g}\) may be unknown in practice, and its mis-specification leads to mis-calibrated uncertainty. We estimate \(B_{g}\) by starting with a small initial guess (e.g., 1) and doubling it when the following condition is met based on newly observed expert labels: \(_{1}(,,|_{t}^{g}|,t 2_{g})<_{t}(_{t|2 _{g}}^{})-_{t}(_{t|_{g}}^{})\), where \(_{g}\) is our current guess. Intuitively, if the new likelihood \(_{t}(_{t|2_{g}}^{})\) is

Figure 2: Visual explanation: While the vanilla LCB returns \(x_{t}^{u}\), a far point from global minimum \(x^{}\), expert-augmented LCB can successfully navigate to closer point \(x_{t}^{c}\) by mixing \(f_{t}\) and \(g_{t}\) with \(_{t}+_{t}_{t}\), where \(_{t}\) is the dual variable. In the figure, \(D_{t}^{f}\) is the set of the sample points of the objective function \(f\) and \(D_{t}^{g}\) is the set of human feedback.

significantly larger, then \(2_{g}\) is more likely a valid bound. We iterate this estimation online during optimisation and in pre-training with the initial dataset (see details in Appendix F).

### Algorithm and Theoretical Guarantee

**Algorithm.** Our algorithm in Alg. 1 generates two candidates: the vanilla LCB \(x_{t}^{u}\) and the expert-augmented LCB \(x_{t}^{c}\). (See App. I.3 on extension to other acquisition functions.) Always selecting the vanilla LCB guarantees no-harm but misses the chance to accelerate convergence using the expert's belief. Intuitively, this can be seen as a bandit problem regarding which arm to select. Line 8 corresponds to the _handover guarantee_, stating that our algorithm stops asking the expert once our model \(g\) becomes more confident than the predefined \(g_{}\). Line 6 outlines the conditions for achieving the _no-harm guarantee_ by assessing the reliability of the expert-augmented candidate \(x_{t}^{c}\). The first condition ensures \(x_{t}^{c}\) is at least possibly better than the worst-case estimation of the optimal value. The second condition acts as active learning of human belief, exploring uncertain points to avoid inaccurate yet confident expert beliefs. The hyperparameter \( 1\) represents the initial level of trust in the expert. A larger \(\) indicates greater priority in exploring expert-preferred regions.

**Theoretical guarantee.** For Alg. 1, we mainly care about two metrics: cumulative regret \(R_{_{T}^{f}}:=_{t_{T}^{f}}(f(x_{t})-f(x^{*}))\) and cumulative queries \(Q_{T}^{g}:=|_{T}^{g}|\). \(R_{_{T}^{f}}\) captures the cumulative regret over the query points to the black-box function. \(Q_{T}^{g}\) captures the number of queries to the expert. Since intuitively each query to the expert causes inconvenience, ideally, the frequency of query to an expert should be low (e.g., \(Q_{T}^{g}\) grows sublinearly in \(T\)).

**Theorem 4.1**.: _Under Assumptions 2.1 to 2.6, with probability at least \(1-\), Alg 1 satisfies,_

\[R_{_{T}^{f}}((2+)_{|_{T}^{ f}|}^{f}_{T}^{f}|}),\] (7a) \[Q_{T}^{g}((_{T}^{g})^{2}(_{g},}{{r}},\|\|_{})}{ }).\] (7b)

See Appendix B for the proof of Thm. 4.1. Intuitively, Eq. (7a) shows the **no-harm guarantee**, since it provides a cumulative regret bound independent of the latent function \(g\). Eq. (7b) shows the **handover guarantee**, since the bound on cumulative queries to the expert is sublinear for commonly-used kernel functions (See Table 1). This means that the frequency of querying the expert asymptotically converges to zero. We do not query human label for \(x_{t}^{u}\) to reduce human effort. Since \(_{T}^{g}_{T}^{f}=[T]\), \(|_{T}^{f}|\) grows linearly in \(T\). There is a trade-off in \(\) selection. A larger \(\) can accelerate convergence when feedback is informative, but it may also cause the worse convergence rate for adversarial feedback (see Appendix B, which includes an additional constant factor of \(}{{4}}\) compared to the original UCB). In practice, setting \(=3\) is sufficiently effective (see Figure 3).

By plugging in the maximum information gain bounds [84; 93] and covering number bounds [104; 105; 18; 109], we apply Thm. 4.1 to derive the kernel-specific bounds in Table 1. In practice, kernel choice and scalability to high dimensions are common challenges for BO. Existing generic techniques, such as decomposed kernels , can be applied in our algorithm to choose kernel functions and achieve scalability in high-dimensional spaces.

### Related Works

**Human-AI Collaborative BO.** There are two primary approaches: the first approach assumes that human experts can express their beliefs through _quantitative_ labels, such as well-defined distributions [69; 52; 82; 43; 24; 42] or pinpoint querying locations [11; 39; 50; 12; 70]. While this strong assumption is valid in specific cases, such as physics simulations , many experimental tasks--such as chemistry, which lacks the consensus on numerical representations of, e.g. molecules--require more relaxed assumptions [24; 46]. The _qualitative_ approach, on the other hand, involves human experts providing pairwise comparisons  or binary recommendations (ours). The algorithm trains a surrogate model from experts' labels, thereby expanding applicable scenarios. Ours is the _first-of-its-kind_ principled method with both no-harm and handover guarantee on a continuous domain.

**Related BO tasks.** Eliciting human preference from labels has been explored in preferential BO [28; 37; 59; 91; 9; 107]. However, this approach treats human preference as the main objective of BO, whereas our work uses experts' belief as an additional information source. Constrained BO [32; 35; 88; 87; 110; 106; 62; 44; 96; 57] is another line of research that investigates BO under unknown constraints, placing another surrogate model on the constraint inferred from queried labels. However, our approach does not treat human belief as a constraint that must be satisfied or a reward to maximise, given that expert knowledge can sometimes be unreliable (see details in Appendix G).

## 5 Experiments

We benchmarked the performance of the proposed algorithm against existing baselines in a collaborative setting with human experts. We employed an ARD RBF kernel for both \(f\) and \(g\). In each iteration of the optimisation loop, the inputs were rescaled to the unit cube \(^{d}\), and the outputs were standardised to have zero mean and unit variance. The initial datasets consisted of three random data points sampled uniformly from within the domain, and in each iteration, one data point was queried. Additionally, we collected initial expert labels by asking an expert to label 'accept' (= 0) or'reject' (= 1) for 10 uniformly random points. All experiments were repeated ten times with different initial datasets and random seeds. We tuned hyperparameters online at each iteration. The GP hyperparameters were tuned by maximising the marginal likelihood on observed datasets using a multi-start L-BFGS-B method  (the default BoTorch optimiser ). The key hyperparameters of the confidence set, \(B_{g}\) and \(_{1}\), were optimised via the online method in Appendix F. Other hyperparameters were set as \(=3\), \(_{0}=1\), and \(g_{}=0.1\) by default throughout the experiments, with their sensitivity discussed later in Fig. 3 (see also Appendix J.1). The constrained optimisation in Prob. (6) was solved using the interior-point nonlinear optimiser IPOPT , which is highly scalable for solving the primal problem, via the symbolic interface CasADi . The unconstrained optimisation (line 5) was solved using the default BoTorch optimiser . More details for reproducing results are available on GitHub.4 The models were implemented in GPTorch . All experiments were conducted on a laptop PC.5 Computational time is discussed in Appendix J. In addition to cumulative regret and queries, we also consider simple regret defined as \(_{t}:=_{_{t}^{f}}(f(x_{})-f(x^{}))\).

   Metric & Linear & Squared Exponential & Matern \\  \(R_{_{T}^{f}}\) & \((_{T}^{f}|}|_{T}^{f}|)\) & \((_{T}^{f}|(|_{T}^{f}|)^{d+1}})\) & \((|_{T}^{f}|^{}^{}(|_{T}^{f}|))\) \\ \(Q_{T}^{g}\) & \((( T)^{3})\) & \((( T)^{3(d+1)})\) & \((T^{}T^{}( T)^{3})\) \\   

Table 1: Kernel-specific bounds (fixed \(\) is hidden) where \(\) is the smoothness parameter of the Matern kernel that is assumed to satisfy \(>(3+d++14d+17})=(d^{2})\).

**Robustness and sensitivity.** First, we tested the robustness of our algorithm to the accuracy of the expert's labels using the 4-dimensional Ackley function . We modelled the synthetic agent response according to Example 2.2. In particular, we examine the impact of feedback accuracy, denoted as \(a\). Fig. 3 illustrates the robustness of our algorithm. When labels are informative (\(a=1,2\)), the convergence rate for both simple and cumulative regrets is accelerated in accordance with the accuracy. Even if the feedback is completely random (\(a=0\)) or adversarial (\(a=-1,-2\)), the no-harm guarantee ensures that the algorithm converges at a rate on par with vanilla LCB by adjusting the level of trust to be lower over iterations. Refer to Appendix J.2.3 for additional confirmation of the no-harm guarantee based on more extensive experimental results. Handover guarantee ensures that our algorithm stops seeking label feedback once sufficient information has been elicited, as indicated by the plateau in the cumulative queries \(Q_{T}^{g}\). We also tested the sensitivity to the optimisation parameters \(,_{0}\), and \(g_{}\). The change in convergence at those parameters were varied mostly within the standard error, indicating that our algorithm is insensitive to these hyperparameters and that feedback accuracy is more dominant. For the primal-dual weight, \(_{0}=0\) corresponds to starting optimisation without a primal-dual mixing objective, which performs worse than mixing cases (\(_{0}=1,2\)), demonstrating the efficacy of incorporating the primal-dual mixing objective.

**Synthetic dataset.** We compared our algorithm against five common synthetic functions  (see details in Appendix J.2), using simple baselines for an ablation study: random sampling, vanilla LCB (unconstrained optimisation), and expert sampling. Expert sampling involves direct sampling from the expert belief distribution \(p_{x_{g}0}\). We employ rejection sampling by generating a uniform random sample over the domain and then accepting it with the probability \(1-p_{x_{g}0}\). We fixed the feedback accuracy at \(a=1\) (as in Example 2.2.). The efficacy of expert labels is roughly estimated by how much faster expert sampling converges compared to random sampling. In all synthetic experiments, our algorithm outperformed the baselines. While expert sampling is at least more effective than

Figure 4: Ablation study on five common synthetic functions with synthetic expert labels (\(a=1\)).

Figure 3: Robustness and sensitivity analysis using the Ackley function. Lines and shaded areas denote mean \(\) 1 standard error. The no-harm guarantee ensures the convergence rate is on par with vanilla LCB even in adversarial cases. Handover guarantee ensures that \(_{t}^{g}\) plateau, allowing optimisation without expert intervention once sufficient information has been elicited.

random sampling, it is not always better than vanilla LCB. For functions with a very sharp global optimum, such as Rosenbrock , \(p_{x_{>}{}_{9}0}\) nearly pinpoints the global minimum. Still, our algorithm performs slightly better than expert sampling. See Appendix J.2.2 for computation time and query frequency. The overhead of our algorithm is comparable to that of other baselines.

**Real-world experiments with human experts.** We conducted real-world experiments in collaboration with four human experts who possess post-doctoral level knowledge on lithium-ion batteries. In this experiment, human labelling costs vary among experts but typically range from a few seconds to several minutes. In the real-world development of lithium-ion batteries, creating and testing a prototype cell requires at least a week, making the labelling cost negligible by comparison.

Lithium-ion batteries are crucial for realising a green society, a rapidly growing field where knowledge is continuously updated at an unprecedented rate. This field typically suffers from data scarcity  due to the ongoing development of new materials synthesised by chemists. Consequently, transfer learning approaches, e.g., [90; 101; 30; 21], are not effective in this setting. We prepared four cases for the experiments: the first is a standard task where we optimise the standard electrolyte composition [26; 36], and the second involves a slight modification of the first setup by changing one solvent material .6 We expect the experts to have informative knowledge on these two tasks. The remaining two cases involve emerging new categories of materials: one is a polymer-nanocomposite electrolyte , and the other is an ionic liquid . We anticipate that the experts' knowledge on these new materials will not be as effective as in the first two tasks (see more details in Appendix J.3). Given the scarcity of real experts, we conducted a pre-experimental step to elicit their knowledge for a fair baseline comparison. We asked them to label 50 random points uniformly from the domain, for all experiments before seeing the results. Then we fit the confidence set model to these results and used \(_{t}^{}\) as the _estimated_ human response. Additionally, we asked the participants to manually select the next query point without any assistance from BO, which we refer to as 'expert sampling' in the baseline. We also compared against state-of-the-art algorithms [11; 50; 7]. These methods have predefined levels of trust, roughly ranked from strong to weak: \(\)\(\). Ours can adjust the level of trust based on data, so we expect it to perform well in both effective and ineffective cases.

Fig. 5 summarises the results. For the first two tasks, our algorithm outperformed all baselines. Particularly in the second task, human sampling was better than vanilla LCB, indicating that we should trust their advice aggressively. Our algorithm can adapt to trust them over time, resulting in significantly accelerated convergence. On the other hand, expert sampling for the new materials tasks was, although unintentionally, worse than random, thereby discouraging trust. While trustful algorithms [11; 7] struggled to converge, the distrustful algorithm  was able to converge on par with vanilla LCB. Our no-harm guarantee worked in this situation, gradually equating to LCB, and showed identical performance to the distrustful algorithm . See also Appendix J.4.1 for the complete experimental results on the number of queries and computation time.

## 6 Discussion

**Feedback form.** Other forms of feedback, such as pairwise comparisons  or preferential rankings , can be incorporated into our algorithm with slight modifications. However, we empirically

Figure 5: Real-world experiments with four human experts of lithium-ion batteries.

found that the binary labelling approach performs best (see Fig. 5), and therefore, we recommend using binary feedback as the primary choice. For those interested in using alternative feedback forms, detailed instructions on how to adapt them to our algorithm are provided in Appendix H.

**Time-varying human knowledge.** We assume that expert knowledge is stationary, although it can be time-varying, e.g., experts' knowledge often evolves as more data is gathered. A simple extension to accommodate this is the use of windowing, where past queried data is forgotten. This can be easily implemented in our algorithm by removing old data beyond a predefined iteration window. However, our initial trials did not show significant performance gains from this approach, so it was not included in the main text. We suggest a dynamic model as a potential future direction, which is discussed in Appendix I.1 with additional experimental results. Similarly, we kept the trust weight \(\) fixed throughout the optimization process. Since human knowledge can improve over time, an adaptive \(\) could be employed to enhance both convergence and robustness. Nevertheless, our no-harm guarantee remains valid even without this adaptation. Further details are provided in Appendix I.2.

**Acceleration vs. Robustness.** One might seek to derive a theoretical guarantee on the acceleration of convergence when the feedback is helpful. However, we want to emphasize that theoretically guaranteeing both acceleration and robustness may be incompatible. From a theoretical perspective, they are in a trade-off relationship . This can be intuitively explained by the no-free-lunch theorem : if algorithm A outperforms B, it does so by exploiting 'biased' information. The 'bias' inherent in the acceleration is contradictory to robustness. Our setting is unbiased, meaning we do not have prior knowledge of helpful or adversarial human expert. Therefore, we must make a design choice between prioritizing robustness or acceleration as a theoretical contribution, depending on whether we assume that expert input can be adversarial (weak bias) or that it will always be helpful (strong bias). Indeed, there are lower bound results for the average-case regret of Bayesian optimization in the literature (e.g., see ). GP-UCB is already nearly rate-optimal in achieving this lower bound. This means theoretical acceleration is obtained in the price of worse robustness. In Appendix E, we present a slightly modified version, Algorithm 2, which offers an improvement guarantee based on strong bias. Our Algorithm 1 can be seen as a relaxed version of this algorithm (soft constraint), which helps explain the empirical success in accelerating convergence.

## 7 Conclusion

Our algorithm, with its data-driven adjustment of the level of trust, successfully accelerated convergence from effective advice while ensuring a no-harm guarantee from unreliable inputs. The handover guarantee also ensures that the BO can automate the optimisation process without assistance from human experts at a later stage. These features are particularly valuable for scientific applications, where researchers often face trial and error, making it challenging to determine the effectiveness of their prior knowledge before starting experiments. Our flexible and robust framework is also expected to be effective in collaboration with large language models (LLMs), which demonstrate remarkable sample-efficient performance by exploiting encoded priors [55; 75; 66], and can be regarded as 'expert knowledge'. Our safeguard features would be particularly effective for shared challenges, such as difficulty in eliciting knowledge [45; 16] and varying accuracy of advice due to hallucinations [81; 98; 85]. Although ours is the _first-of-its-kind_ algorithm with a general theoretical guarantee in the expert-collaborative setting, it is still based on the GP-UCB algorithm 7 and shares its limitations (e.g., high dimensionality). One future direction is combining our approach with the high-dimensional BO methods [97; 51]. Additionally, our current setting does not consider the batch setting, yet one can easily extend with existing approaches, e.g. [4; 6; 3; 5]. Multiple expert scenario is also a promising future extension. While a simple expert aggregation approach (e.g., majority vote, adding multiple experts \(g\)) could work without modifications to the current algorithm, more advanced methods, such as choice functions , present promising directions for future work. Explainability is also key.  showed that Shapley value-based explanations improve human feedback accuracy, and this can be easily integrated into our framework. Our method can positively influence human experts by empirically demonstrating the value of their expertise, even amidst concerns about job security in the AI era . On the negative side, more powerful LLMs may eventually replace the expert role in our algorithm in areas where data is sufficiently shared on websites or in papers, such as hyperparameter tuning .