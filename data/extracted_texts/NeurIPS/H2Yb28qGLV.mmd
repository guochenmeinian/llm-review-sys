# Simon Steshin

_Lo-Hi_: Practical ML Drug Discovery BenchmarkIndependent Researcher

simon.steshin@gmail.com

###### Abstract

Finding new drugs is getting harder and harder. One of the hopes of drug discovery is to use machine learning models to predict molecular properties. That is why models for molecular property prediction are being developed and tested on benchmarks such as MoleculeNet. However, existing benchmarks are unrealistic and are too different from applying the models in practice. We have created a new practical _Lo-Hi_ benchmark consisting of two tasks: Lead Optimization (Lo) and Hit Identification (Hi), corresponding to the real drug discovery process. For the Hi task, we designed a novel molecular splitting algorithm that solves the Balanced Vertex Minimum \(k\)-Cut problem. We tested state-of-the-art and classic ML models, revealing which works better under practical settings. We analyzed modern benchmarks and showed that they are unrealistic and overoptimistic.

Review: https://openreview.net/forum?id=H2Yb28qGLV

Lo-Hi benchmark: https://github.com/SteshinSS/lohi_neurips2023

Lo-Hi splitter library: https://github.com/SteshinSS/lohi_splitter

## 1 Introduction

Drug discovery is the process of identifying molecules with therapeutic properties . To serve as a drug, a molecule must possess multiple properties simultaneously . It must be stable , yet easily eliminated from the body , able to reach its target , non-toxic , cause minimal side effects , and therapeutically active on the target .

To identify such molecules, researchers develop Molecular Property Prediction (MPP) models. These models are used in a virtual screening, during which the models are used to make predictions for a large number of molecules, after which the molecules with the best estimated property are selected for experimental validation .

To enable comparisons between various architectures, the research community relies on standardized benchmarks to evaluate model performance . The prevailing assumption is that models with superior metrics on these benchmarks are more suitable for real-world applications.

We believe this assumption is false. Modern benchmarks test in non-realistic conditions on impractical tasks. In this paper, we introduce two new practical drug discovery ML tasks -- Hit Identification and Lead Optimization -- that are often encountered in most drug discovery campaigns. We demonstrate that none of the benchmarks assess models for these tasks, which is why we propose seven new practical datasets that better imitate real-life drug discovery scenarios. To prepare Hi datasets, we designed a novel molecular splitter algorithm that solves Balanced Vertex Minimum \(k\)-Cut problem using Integer Linear Programming with heuristics.

## 2 Practical drug discovery

### Hit Identification (Hi)

Drug discovery involves multiple stages. When the potential mechanism of action is known, an early step is Hit Identification. During this phase, chemists search for a "hit" - a molecule with the potential to become a drug . A viable hit must exhibit some level of activity towards the target (e.g., Ki less than \(10\)\(\)M) and possess novelty, meaning it is eligible for patent protection.

Clinical trials can incur costs in the hundreds of millions of dollars [25; 26], making it risky to pursue non-patentable molecules. Consequently, companies prioritize patentable molecules from the outset. Novelty is an essential aspect of this process. In practice, medicinal chemists often pre-filter their chemical libraries before the virtual screening, eliminating molecules that exhibit a Tanimoto similarity above a specific threshold to molecules with known activity [10; 11; 16; 27; 28; 29].

### Lead Optimization (Lo)

Following Hit Identification, the next stage is Lead Optimization. Once a novel, patentable molecule with activity towards the desired target is identified, its closest analogs are also likely to exhibit activity . In this phase, medicinal chemists often make minor modifications to the hit molecule to enhance its target activity, selectivity, and other properties [31; 32]. Unlike in Hit Identification, novelty usually is not a priority during Lead Optimization. Instead, the objective is to discover molecules that are similar to the original hit but possess improved characteristics.

This is related to the field of goal-directed molecular generation , which is occasionally formulated as the problem of searching for molecules with maximal activity within the \(\)-neighborhood of a known hit [34; 35], within some scaffold , or as an optimization process in the latent space . These works use predictive ML models to distinguish between minor chemical modifications . The effectiveness of these models in their ability to distinguish between molecules with small modifications remains unclear.

### Model Selection

In this context, Lead Optimization (Lo) and Hit Identification (Hi) represent contrasting tasks. Hi ML models are expected to demonstrate their generalization capabilities by predicting properties of molecules significantly different from the training set. Conversely, Lo ML models should predict properties of minor modifications of molecules with known activity.

For selecting the appropriate ML model or adjusting its hyperparameters, it is crucial to test the model under conditions similar to its intended application. In Hi, models must identify novel molecules markedly different from known active ones, implying the models should be tested on molecules distinctly different from the training set. Conversely, Lo applies ML models to molecules resembling known active ones. The null Structure-Activity Relationship hypothesis assumes thatsmall modifications will not alter the molecule's properties [30; 39]. Consequently, Lo ML models should be evaluated based on their ability to make predictions that surpass the null hypothesis.

We show that modern benchmarks mix these two scenarios, which is why it remains unclear how effectively models can generalize to truly novel molecules and how proficiently they can distinguish minor modifications, thereby guiding molecular optimization. Furthermore, we untangle these scenarios using a novel benchmark, demonstrating that **different architectures are better suited for different tasks**.

## 3 Novelty

But how to measure novelty? From a commercialization and regulatory perspective, a novel molecule is one that can be patented. Contemporary patents consist of human-written text and incorporate non-trivial substitutions, rendering the assessment of a molecule's patentability a complex task for legal professionals. To date, this process has not been automated. Additionally, during the Hit Identification, not only must the hit be patentable -- its neighborhood should be patentable as well to make Lead Optimization possible. When dealing with an extensive chemical library, these challenges make patentability an impractical criterion for determining novelty.

While chemists agree that minor substitutions likely do not alter a molecule's function, a consensus on the distinction between a new molecule and a modified version of an existing one remains elusive. One might hope to find a general similarity threshold that would separate similar molecules with presumably the same activity from distinct molecules. This hope led to the "0.85 myth" , which proved to be false due to the significant variability of such thresholds across different targets . About molecular similarity, it was said [42; 41], "Similarity is in the eye of the beholder." Nevertheless, a practical criterion exists.

In study  143 experts from regulatory authorities(FDA, FDA of Taiwan, EMA, PMDA) were shown 100 molecular pairs and asked if the molecules should be regarded as structurally similar. This study is especially notable because it simulates a real-life scenario in which a regulatory committee (EMA's Committee for Medicinal Products for Human Use) decides if a new drug is novel enough to grant it a beneficial status: orphan drug designation. The study shows that when two molecules have Tanimoto similarity \( 0.4\) with ECFP4 fingerprints , half of the experts regard them as dissimilar, which is sufficient to conclude the novelty of the drug. See Appendix G for our reproduction.

While Tanimoto similarity with ECFP4 fingerprints has its own disadvantages such as size bias , its simplicity makes it possible to quickly measure the similarity of molecules, so there are practical  and theoretical [46; 47] works that use "Tanimoto similarity < 0.4" as a novelty criterion. Because of the practical evaluation in real-life scenarios and efficiency, we assess novelty using Tanimoto similarity with ECFP4 fingerprints in our benchmark.

## 4 Our contribution

* We suggest two new practical drug discovery ML tasks -- Hit Identification and Lead Optimization -- that better imitate real-life scenarios;
* We designed a novel molecular splitting algorithm for the Hi task;
* We propose seven new practical datasets;
* We demonstrate that modern ML drug discovery benchmarks simulate impractical scenarios;
* We evaluate modern and classic ML algorithms on our benchmark.

## 5 Modern benchmarks test neither Lo nor Hi

To select a model or its hyperparameters, we must evaluate them under the conditions in which they will be used. We demonstrate that none of the modern benchmarks correspond to realistic conditions, thus raising questions about their suitability for evaluating practical machine learning models. Although it is impossible to examine every existing benchmark due to their sheer number, we have opted to assess a variety of benchmarks using different data, different preprocessing techniques, and originating from different authors. We will initially focus on standard benchmarks, while in the section 6, we will explore more exotic and specialized ones. We provide additional analysis in Appendix D.

### MoleculeNet: Esol

MoleculeNet  is a widely-used benchmark for ML drug discovery, consisting of 17 datasets, including the ESOL dataset with water solubility data. This dataset is frequently used in studies for model comparisons [48; 49; 50; 51; 52; 53]. While the authors recommend a random train-test split, we discovered that it leads to 76% of the test molecules having a neighbor in the train with Tanimoto similarity > 0.4, making ESOL unsuitable for Hi scenario (see Fig. 3). It is also unclear how well the dataset represents the Lo scenario, as the distinct 24% of the train contributes to the evaluation, and the RMSE metric can be significantly improved over the constant baseline, even without identifying minor chemical modifications.

### MoleculeNet: Hiv

In benchmarks, train and test sets are typically split using random, scaffold, or occasionally time splits when time stamps are available. It has been frequently observed that random splits can result in highly similar molecules in both train and test sets, leading to overly optimistic and impractical estimations. For example: "Random splitting, common in machine learning, is often not correct for chemical data" . Thus, scaffold splitting  is sometimes suggested as an alternative.

Scaffold splitting  is a method where each molecule is represented by a graph consisting of ring systems, linkers and side chains. A group of molecules may correspond to a single graph, in which case they are assigned to the same partition.

The MoleculeNet Hiv dataset (ubiquitous in evaluation [55; 56; 57; 58; 59]) recommends using scaffold splitting, as it is believed to better reflect the process of discovering new molecules: "As we are more interested in discovering new categories of HIV inhibitors, scaffold splitting [...] is recommended" . Although scaffold splitting makes the train set more distinct from the test set, it is still insufficient for the scenario of finding novel molecules. We discovered that 56% of Hiv test molecules have a train neighbor with a Tanimoto similarity > 0.4, indicating the dataset is unsuitable for the Hi scenario (see Fig. 3). Additionally, the dataset is unsuitable for the Lo scenario due to its binary label, rather than a continuous value.

### Therapeutic Data Commons: TDC.CYP2D6_Veith

The Therapeutic Data Commons  is a novel platform designed for evaluating machine learning models in drug discovery. We examined the largest dataset, TDC.CYP2D6_Veith, within the Single-Instance Learning Tasks category. Although a scaffold split is recommended, the dataset is deemed unsuitable for the Hi scenario, as 78% of test molecules possess a training neighbor exhibiting a Tanimoto similarity greater than 0.4 (see Fig. 3). Additionally, the dataset is unsuitable for the Lo scenario due to its binary label, rather than a continuous value.

Figure 3: Common benchmarks contain highly similar molecules across different splits.

### MolData

MolData is a recent benchmark for MPP based on PubChem data. The authors themselves analyzed the novelty of the molecules within the subset and found their scaffold split to be unrealistic: "[...] more than 44% of the molecules within the MolData dataset have at least one other similar molecule to them with a Tanimoto Coefficient of 0.7 or higher. This high percentage of the similarity can denote lack of diversity within this portion of the dataset."

The authors use scaffold split to increase the difference between the train and the test, but we found that at least 88% of the molecules in the test have a similar molecule in the train with a Tanimoto similarity > 0.4, making the dataset unsuitable for the Hi scenario (see Fig. 3). Additionally, the dataset is unsuitable for the Lo scenario due to its binary label, rather than a continuous value.

## 6 Related works

### Out-of-distribution MPP

In the Hi scenario, the training and test sets are significantly different to simulate the search for new molecules, making the Hi scenario interpretable as an Out-Of-Distribution (OOD) task. Although OOD benchmarks already exist for machine learning-based drug discovery, they do not align with practical drug discovery applications.

DrugOD is a drug discovery benchmark dedicated to Out-Of-Distribution prediction. The authors accurately observe that "In the field of AI-aided drug discovery, the problem of distribution shift [...] is ubiquitous", and therefore suggest assay-based, scaffold-based, or molecular size-based partitions to create a distribution shift. However, their benchmark does not correspond to practical drug discovery, as it involves predicting average activity for all ChEMBL targets simultaneously in ligand-based drug discovery datasets. This approach is impractical because, in reality, researchers are not interested in average activity across ChEMBL targets, but rather in activity on a specific target.

GOOD is a benchmark explicitly designed to separate covariate and concept shifts. We examined the GOOD-HIV dataset, as it is a MPP benchmark containing a significant amount of diverse data (see the #Circles in Appendix A). In all Out-Of-Distribution test partitions, more than 40% of molecules have a neighbor in the training set at a Tanimoto similarity more than 0.4, making the dataset unsuitable for the Hi scenario.

During the NeurIPS review period, Valence Labs and Laval University jointly published work -- independent from ours -- investigating Molecular-Out-Of-Distribution generalization . Their research is akin to our Hi-scenario, but the authors studied different data splits, employed different models, and investigated uncertainty calibration.

### Activity cliff

Activity cliff is a pair of structurally similar compounds that are active against the same bio-target but significantly different in binding potency [61; 22].

Recently, several remarkable studies have emerged, seeking to establish a standard benchmark for Activity Cliff Prediction [22; 23]. These works have inspired us to create our own benchmark. While Activity Cliff Prediction may be advantageous in the Lo scenario, it is inadequate for guiding molecule optimization. In the Lo scenario, it is essential not only to identify extreme activity cliffs with a 10x difference in activity but also to predict minor activity fluctuations. We propose alternative splits and metrics in the Lo scenario.

## 7 Results

### Hi-splitter

To simulate the Hi scenario, it is necessary to divide the dataset into training and testing subsets such that any pair of molecules from different partitions has a ECFP4 Tanimoto similarity of less than \(0.4\). Despite the fact that the scaffold splitter produces a more diverse division than random splitter, we observe that it is inadequate for an effective Hi scenario.

A greedy approach is to implement the conventional scaffold split and discard from the test set any molecules excessively similar to those in the training set. The issue with this method, however, is that data points are costly, and it is desirable to minimize the number of discarded molecules. Consequently, we have developed a novel algorithm for strict dataset splitting, which discards fewer molecules than the greedy algorithm.

Let us consider molecules \(X=\{x_{1},...,x_{n}\}\). We construct a neighborhood graph \(G=(V,E)\), where each molecule \(x_{i}\) corresponds to a vertex \(v_{i} V\). Two vertices are connected by an edge if and only if the associated molecules have a similarity to each other greater than threshold \(t\): \(e_{vu} E T(x_{v},x_{u})>t\), where \(T\) is Tanimoto Similarity with ECFP4 fingerprints. In such a graph, connected components can be assigned to the training or testing sets independently. However, in practice, 95% of the molecules belong to a single connectivity component. Our goal is to remove the minimum number of vertices such that the giant component breaks up into multiple components with size constraints, thereby enabling us to distribute them between the training and testing sets.

This problem is known as the Balanced Vertex Minimum \(k\)-Cut and has been extensively researched in literature . A review of similar problems can be found elsewhere . Similar to  we formulate the Integer Linear Programming formulation.

Let \(K\) denote the set of integers \(\{1,2, k\}\). Let \(G=(V,E)\) represent a simple connected graph that we are going to split:

\[V=_{i=1}^{k}V_{i} V_{0} i j V_{i} V_{j}=\]

For all vertices \(v V\) and for all integers \(i K\), let us associate a binary indicator \(y_{v}^{i}\) such that:

\[y_{v}^{i}=1,&v V_{i}\\ 0,&\]

Note that \(V_{0}\) denotes the set of removed vertices, so if \(_{i K}y_{v}^{i}=0\) then the vertex \(v\) is in \(V_{0}\).

Let \(b_{i}\) be a lower bound on the cardinality \(|V_{i}|\) of partition \(V_{i}\). We need it to get partitions with size constraints, e.g. 80% in train and 20% in test. Let \(w_{v}\) be the weights of the nodes. In simple formulation we take \(w_{v}=1\). We formulate the Balanced Vertex Minimum \(k\)-Cut as follows:

\[_{i K}_{v V}w_{v}y_{v}^{i}\] (1)

\[_{i K}y_{v}^{i} 1 v V\] (2)

\[y_{u}^{i}+y_{v}^{j} 1 i j K, e_{uv} E\] (3)

\[_{v V}y_{v}^{i} b_{i} i K\] (4)

\[y_{v}^{i}\{0,1\} i K, v V\] (5)

Equation (1) minimizes the weight of removed molecules \(V_{0}\). Equation (2) states that each vertex \(v\) should be in one partition maximum. Equation (3) ensures there is no connectivity between different partitions, so for each edge \(e_{uv} E\) if \(u\) is in \(V_{i}\) partition, then the other vertex \(v\) is either in \(V_{i}\) as well, or in \(V_{0}\), meaning it was removed. Equation (4) puts constraints on the size of the partitions \(V_{i}\).

While this formulation was effective on small-scale graphs (around 100 vertices), we found it too slow for a real DRD2 activity dataset with 6k molecules. To our knowledge, existing literature  typically involves small graphs from the standard benchmarks with a maximum node count of several hundred. To expedite computation, we implemented a graph coarsening approach. The basic idea is outlined here, while the formal algorithm is detailed in Appendix E.

We initially performed Butina clustering  on molecules and created a coarse graph wherein the vertices correspond not to individual molecules but to clusters of molecules. In the coarsened graph,each vertex is assigned a weight \(w_{v}\) equal to the number of molecules in the respective cluster. This approach accelerated computations and enabled us to partition the HIV dataset consisting of 40k molecules, removing fewer vertices than would be the case with the greedy approach (See Table 1).

### _Lo-Hi_ benchmark

We conducted an analysis of numerous drug discovery benchmarks, yet none seemed to align with the actual drug discovery process. Consequently, we prepared seven datasets and are now making them available to the community.

We selected datasets that represent realistic drug discovery problems, contain substantial amounts of qualitative data, and cover a diverse chemical space. Diversity was assessed using the recently introduced #Circles metric . As the source code was unavailable at the time of writing, we employed our own implementation of the greedy algorithm outlined in : Appendix H. We provide additional statistics for the original datasets in the Appendix A.

We propose three distinct train-test splits for each dataset. We advise adjusting hyperparameters solely on the first split, applying the same hyperparameters to train and assess models on splits #2 and #3, and comparing models by averaging metrics across the splits. Datasets are released under the MIT license.

#### 7.2.1 Hi

In the Hit Identification scenario, we aim to predict binary labels for new molecules that significantly differ from those in the training set. We prepared four datasets which we divided into training and testing sets, ensuring that the Tanimoto similarity between any molecule in the test set and those in the training set is less than 0.4. See Fig. 1. Additionally, we show that such a split predicts experimental outcomes better than the scaffold split (Appendix F).

DRD2-Hi involves predicting dopamine receptor inhibition, a GPCR target of therapeutic importance in schizophrenia [71; 72] and Parkinson's disease [73; 74]. To create this dataset, we obtained Ki data for DRD2 from ChEMBL30 , cleaned it (see Appendix A), and binarized it so that molecules with Ki < 10uM are considered active.

HIV-Hi is an HIV dataset from the Drug Therapeutics Program AIDS Antiviral Screen that measures the inhibition of HIV replication. We obtained the prepared dataset from MoleculeNet .

DRD2-Hi and HIV-Hi are large. However, real-life data often comes in limited quantities. To simulate this crucial scenario, we prepared a smaller challenging dataset, KDR-Hi. This dataset is based on the ChEMBL30 IC50 data associated with vascular endothelial growth factor receptor 2, a kinase target for cancer treatment . Its creation process was similar to that of DRD2-Hi, but we restricted the training folds to just 500 molecules.

The Sol-Hi dataset draws from a public solubility dataset at Biogen . We binarized this data such that molecules with a solubility of less than 10 ug/mL were assigned a positive label.

Each dataset was divided into distinct training and testing sets. We used the Hi-splitter with \(k=3\) to obtain three highly dissimilar subsets: \(\{F_{1},F_{2},F_{3}\}\). These subsets were then combined to create three distinct folds, each with a unique test set. For instance, for the first fold, the training set was \(train_{1}=\{F_{1},F_{2}\}\) and the test set was \(test_{1}=\{F_{3}\}\). This methodology enabled us to assess the variability in quality resulting from using different data with the same models.1

Hi MetricFor our benchmark, we have selected the PR AUC. As a simple binary classification metric without parameters, it is implemented in most libraries and normalized to a range of .

   Method & DRD2 & HIV \\  Greedy & 1066 (17.0\%) & 5851 (14.2\%) \\ Hi-Splitter & 97 (1.5\%) & 1598 (3.8\%) \\ 

Table 1: Number of removed molecules for 0.9:0.1 split The PR AUC favors early recognition models  and does not appeal to wrong intuition among readers in an unbalanced setting.

#### 7.2.2 Lo

In the Lead Optimization scenario, we aim to predict the activity of molecules that are highly similar to those in the training set. As similar molecules tend to exhibit similar activity, our focus is not on predicting binary labels, but rather on ranking, which indicates whether a modification increases the activity or not.

To simulate the Lead Optimization scenario, we isolated clusters of molecules with intracluster similarity \( 0.4\) and consisting of \( 5\) molecules. We included them in the test dataset. In practical Lead Optimization, the activity of a given hit is already known; thus, for each cluster, we retained exactly one molecule with a similarity \( 0.4\) to that cluster in the training set. See Fig. 2. We provide additional analysis in Appendix A.

In order to confirm the validity of the Lo benchmark, we ensured that the intracluster variation in activity exceeds the experimental noise (See Appendix B). This step is critical, as if the variance within a cluster of similar molecules is not significantly greater than the experimental noise, it would not make sense to test models on such molecules -- under these conditions, even an ideal model would struggle to make accurate predictions. The formal pseudocode can be found in Appendix C.

We assembled three datasets: DRD2-Lo, KCNH2-Lo and KDR-Lo. Both DRD2-Lo and KDR-Lo are based on the same data as their respective Hi counterparts but feature a different split between training and testing sets. KCNH2 is an ion channel that regulates heartbeat, and its inhibition can cause dangerous side effects . Consequently, bioassays for KCNH2 are used as a screening method for cardiotoxicity. We extracted IC50 data from ChEMBL30, cleaned it, and divided it into training and testing sets.

Each dataset was split three times using different random seeds, resulting in three folds. This method enables us to assess the variability in quality when using different data with the same models.2

Lo MetricOur goal is to determine whether the models can make better predictions than assuming "the modified molecule active in the same manner as the original hit." We chose Spearman's correlation coefficient as our metric, calculated within each cluster and averaged across clusters. This metric does not rely on intracluster variation, depends solely on the ranking of molecules within the cluster, and is normalized, between minus one (ideally wrong), zero (random) and one (ideal), rendering it easily interpretable.

### Evaluation

We evaluated both traditional and state-of-the-art ML models on our benchmark. We meticulously executed the hyperparameter search, following the procedure outlined in Appendix H. Results are presented in Table 2. The best scores for fingerprint and graph models are highlighted in bold. It should be noted that the mean and standard deviation were calculated not for random seeds, but within different folds.

We found that the best models varied for the Hi and Lo tasks. The most effective model for the Hi task was the Chemprop graph neural network , aligning with its real-world success [16; 28; 29]. The second-best were gradient boosting and KNN for the small KDR-Hi dataset.

Conversely, for the Lo task, the SVM model was found to be the most proficient, which corroborates previous works on activity cliffs . Only for the small KDR-Lo did Chemprop outperform SVM, but even then, the performance was not satisfactory. We further provide a per-cluster Spearman distribution for SVM in Appendix I. The limited inability of Chemprop to distinguish minor modifications may be due to the limited expressivity of graph neural networks. However, this was surprising to us, considering the limited expressivity of binary fingerprints as well.

[MISSING_PAGE_FAIL:9]

Future work

Our focus was on medium and large datasets, yet many small datasets contain fewer than 100 datapoints. It would be beneficial to have smaller datasets similar to  but tailored for Hi generalization.

While our emphasis was on ligand-based drug discovery, where the goal is to predict a molecule's property, there is also structure-based drug discovery. This approach not only involves predicting molecular properties but also incorporates protein information. Hence, it would be advantageous to have structure-based drug discovery datasets that are divided not just by protein (or pockets ) similarity but also with Hi generalization across ligands.

A major ongoing challenge in molecular generative models is ensuring synthesizability, meaning that generated molecules can be made in the real world. Hi splits can help test the generalizability of synthesizability models. But, it's important to remember that Lo/Hi splits assume similar molecules have similar properties. While this holds for physico-chemical attributes, this premise remains to be validated in the context of feasibility measures.

## 11 Potential harmful Consequences

One of the primary concerns arises from the ability to design molecules that are unfamiliar to medical chemists and experts in the field. While the novelty of these compounds can be advantageous for pushing the boundaries of current scientific knowledge, it also raises the potential risk of misuse, especially if malicious actors were to use the system to generate harmful or toxic compounds for hostile purposes. Given their unfamiliar nature, these molecules might not immediately raise flags upon review by experts or during synthesis orders at chemical laboratories. This could open the door for the creation and dissemination of harmful compounds.

## 12 Acknowledgements

This work was funded by Gleb Pobegailo.