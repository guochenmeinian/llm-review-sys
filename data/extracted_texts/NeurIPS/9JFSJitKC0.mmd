# Spectral-Risk Safe Reinforcement Learning

with Convergence Guarantees

Dohyeong Kim\({}^{1}\)   Taehyun Cho\({}^{1}\)   Seungyub Han\({}^{1}\)

**Hojun Chung\({}^{1}\)   Kyungjae Lee\({}^{2*}\)   Songhwai Oh\({}^{1*}\)**

\({}^{1}\)Dep. of Electrical and Computer Engineering, Seoul National University

\({}^{2}\)Dep. of Statistics, Korea University

Corresponding authors: kyungjae_lee@korea.ac.kr, songhwai@snu.ac.kr.

###### Abstract

The field of risk-constrained reinforcement learning (RCRL) has been developed to effectively reduce the likelihood of worst-case scenarios by explicitly handling risk-measure-based constraints. However, the nonlinearity of risk measures makes it challenging to achieve convergence and optimality. To overcome the difficulties posed by the nonlinearity, we propose a spectral risk measure-constrained RL algorithm, _spectral-risk-constrained policy optimization (SRCPO)_, a bilevel optimization approach that utilizes the duality of spectral risk measures. In the bilevel optimization structure, the outer problem involves optimizing dual variables derived from the risk measures, while the inner problem involves finding an optimal policy given these dual variables. The proposed method, to the best of our knowledge, is the first to guarantee convergence to an optimum in the tabular setting. Furthermore, the proposed method has been evaluated on continuous control tasks and showed the best performance among other RCRL algorithms satisfying the constraints. Our code is available at https://github.com/rllab-snu/Spectral-Risk-Constrained-RL.

## 1 Introduction

Safe reinforcement learning (Safe RL) has been extensively researched [2; 15; 25], particularly for its applications in safety-critical areas such as robotic manipulation [3; 17] and legged robot locomotion . In safe RL, constraints are often defined by the expectation of the discounted sum of costs [2; 25], where cost functions are defined to capture safety signals. These expectation-based constraints can leverage existing RL techniques since the value functions for costs follow the Bellman equation. However, they inadequately reduce the likelihood of significant costs, which can result in worst-case outcomes, because expectations do not capture information on the tail distribution. In real-world applications, avoiding such worst-case scenarios is particularly crucial, as they can lead to irrecoverable damage.

In order to avoid worst-case scenarios, various safe RL methods [10; 26] attempt to define constraints using risk measures, such as conditional value at risk (CVaR)  and its generalized form, spectral risk measures . Yang et al.  and Zhang and Weng  have employed a distributional critic to estimate the risk constraints and calculate policy gradients. Zhang et al.  and Chow et al.  have introduced auxiliary variables to estimate CVaR. These approaches have effectively prevented worst-case outcomes by implementing risk constraints. However, the nonlinearity of risk measures makes it difficult to develop methods that ensure optimality. To the best of our knowledge, even for well-known risk measures such as CVaR, existing methods only guarantee local convergence [10; 32]. This highlights the need for a method that ensures convergence to a global optimum.

In this paper, we introduce a spectral risk measure-constrained RL (RCRL) algorithm called _spectral-risk-constrained policy optimization (SRCPO)_. Spectral risk measures, including CVaR, have a dual form expressed as the expectation over a random variable . Leveraging this duality, we propose a bilevel optimization framework designed to alleviate the challenges caused by the nonlinearity of risk measures. This framework consists of two levels: the _outer problem_, which involves optimizing a dual variable that originates from the dual form of spectral risk measures, and the _inner problem_, which aims to find an optimal policy for a given dual variable. For the inner problem, we define novel risk value functions that exhibit linearity in performance difference and propose a policy gradient method that ensures convergence to an optimal policy in tabular settings. The outer problem is potentially non-convex, so gradient-based optimization techniques are not suitable for global convergence. Instead, we propose a method to model and update a distribution over the dual variable, which also ensures finding an optimal dual variable in tabular settings. As a result, to the best of our knowledge, we present the first algorithm that guarantees convergence to an optimum for the RCRL problem. Moreover, the proposed method has been evaluated on continuous control tasks with both single and multiple constraints, demonstrating the highest reward performance among other RCRL algorithms satisfying constraints in all tasks. Our contributions are summarized as follows:

* We propose a bilevel optimization framework for RCRL, effectively addressing the complexities caused by the nonlinearity of risk measures.
* The proposed method demonstrates convergence and optimality in tabular settings.
* The proposed method has been evaluated on continuous control tasks with both single and multiple constraints, consistently outperforming existing RCRL algorithms.

## 2 Related Work

RCRL algorithms can be classified according to how constraints are handled and how risks are estimated. There are two main approaches to handling constraints: the Lagrangian [26; 29; 32] and the primal [15; 30] approaches. The Lagrangian approach deals with dual problems by jointly updating dual variables and policies, which can be easily integrated into the existing RL framework. In contrast, the primal approach directly tackles the original problem by constructing a subproblem according to the current policy and solving it. Subsequently, risk estimation can be categorized into three types. The first type [29; 32] uses an auxiliary variable to precisely estimate CVaR through its dual form. Another one [26; 30] approximates risks as the expectation of the state-wise risks calculated using a distributional critic . The third one  approximates CVaR by assuming that cost returns follow Gaussians. Despite these diverse methods, they only guarantee convergence to local optima. In the field of risk-sensitive RL, Bauerle and Glauner  have proposed an algorithm for spectral risk measures that ensures convergence to an optimum. They introduced a bilevel optimization approach, like our method, but did not provide a method for solving the outer problem. Furthermore, their approach to the inner problem is value-based, making it complicated to extend to RCRL, which typically requires a policy gradient approach.

## 3 Backgrounds

### Constrained Markov Decision Processes

A constrained Markov decision process (CMDP) is defined as \( S,A,P,,,R,\{C_{i}\}_{i=1}^{N}\), where \(S\) is a state space, \(A\) is an action space, \(P\) is a transition model, \(\) is an initial state distribution, \(\) is a discount factor, \(R:S A S[-R_{},R_{}]\) is a reward function, \(C_{i}:S A S[0,C_{}]\) is a cost function, and \(N\) is the number of cost functions. A policy \(:S(A)\) is defined as a mapping from the state space to the action distribution. Then, the state distribution given \(\) is defined as \(d_{}^{}(s):=(1-)_{t=0}^{}^{t}(s_{t}=s)\), where \(s_{0}\), \(a_{t}(|s_{t})\), and \(s_{t+1} P(|s_{t},a_{t})\) for \( t\). The state-action return of \(\) is defined as:

\[Z_{R}^{}(s,a):=_{t=0}^{}^{t}R(s_{t},a_{t},s_{t+1})s _{0}=s,\ a_{0}=a,\ a_{t}(|s_{t}),\ s_{t+1} P(|s_{t},a_{t}) \  t.\]

The state return and the return are defined, respectively, as:

\[Y_{R}^{}(s):=Z_{R}^{}(s,a)a(|s), G_{R}^{}:=Y _{R}^{}(s)s.\]The returns for a cost, \(Z^{}_{C_{i}}\), \(Y^{}_{C_{i}}\), and \(G^{}_{C_{i}}\), are defined by replacing the reward with the \(i\)th cost. Then, a safe RL problem can be defined as maximizing the reward return while keeping the cost returns below predefined thresholds to ensure safety.

### Spectral Risk Measures

In order to reduce the likelihood of encountering the worst-case scenario, it is required to incorporate risk measures into constraints. Conditional value at risk (CVaR) is a widely used risk measure in finance , and it has been further developed into a spectral risk measure in , which provides a more comprehensive framework. A spectral risk measure with spectrum \(\) is defined as:

\[_{}(X):=_{0}^{1}F_{X}^{-1}(u)(u)du,\]

where \(\) is an increasing function, \( 0\), \(_{0}^{1}(u)du=1\), and \(F_{X}\) is the cumulative density function (CDF) of the random variable \(X\). By appropriately defining the function \(\), various risk measures can be established, and examples are as follows:

\[_{}:(u)=_{u}/(1-), _{}:(u)=u^{/(1-)}/(1-),\] (1)

where \([0,1)\) represents a risk level, and the measures become risk neutral when \(=0\). Furthermore, it is known that the spectral risk measure has the following dual form :

\[_{}(X)=_{g}[g(X)]+_{0}^{1}g^{*}((u))du,\] (2)

where \(g:\) is an increasing convex function, and \(g^{*}(y):=_{x}xy-g(x)\) is the convex conjugate function of \(g\). For example, \(\) is expressed as \(_{}(X)=_{}[(X-)_{+ }]+\), where \(g(x)=(x-)_{+}/(1-)\), and the integral part of the conjugate function becomes \(\). We define the following sub-risk measure corresponding to the function \(g\) as follows:

\[^{g}_{}(X):=[g(X)]+_{0}^{1}g^{*}((u))du.\] (3)

Then, \(_{}(X)=_{g}^{g}_{}(X)\). Note that the integral part is independent of \(X\) and only serves as a constant value of risk. As a result, the sub-risk measure can be expressed using the expectation, so it provides computational advantages over the original risk measure in many operations. We will utilize this advantage to show optimality of the proposed method.

### Augmented State Spaces

As mentioned by Zhang et al. , a risk-constrained RL problem is non-Markovian, so a history-conditioned policy is required to solve the problem. Also, Bastani et al.  showed that an optimal history-conditioned policy can be expressed as a Markov policy defined in a newly augmented state space, which incorporates the discounted sum of costs as part of the state. To follow these results, we define an augmented state as follows:

\[_{t}:=(s_{t},\{e_{i,t}\}_{i=1}^{N},b_{t}),e_{i,0}=0,\ e_{i,t+1}=(C_{i}(s_{t},a_{t},s_{t+1})+e_{i,t})/ ,\ b_{t}=^{t}.\] (4)

Then, the CMDP is modified as follows:

\[_{t+1}=(s_{t+1},\{(c_{i,t}+e_{i,t})/\}_{i=1}^{N}, b_{t}), s_{t+1} P(|s_{t},a_{t}),\ c_{i,t}=C_{i}(s_{t},a_{t},s_{t+1}),\]

and the policy \((|)\) is defined on the augmented state space.

## 4 Proposed Method

The risk measure-constrained RL (RCRL) problem is defined as follows:

\[_{}[G^{}_{R}]\ \ \ _{_{i}}(G^{ }_{C_{i}}) d_{i}\  i\{1,2,...,N\},\] (5)

where \(d_{i}\) is the threshold of the \(i\)th constraint. Due to the nonlinearity of the risk measure, achieving an optimal policy through policy gradient techniques can be challenging. To address this issue, we propose solving the RCRL problem by decomposing it into two separate problems. Using a feasibility function \(\),2 the RCRL problem can be rewritten as follows:

\[_{}[G_{R}^{}]-_{i=1}^{N}(_{_{i}}(G_{C_{i}}^{})-d_{i})=_{}[G_{R}^{ }]-_{i=1}^{N}_{g_{i}}(_{_{i}} ^{g_{i}}(G_{C_{i}}^{})-d_{i})\\ =\}_{i=1}^{N}}_{}[G_{R }^{}]-_{i=1}^{N}(_{_{i}}^{g_{i}}( G_{C_{i}}^{})-d_{i})}_{},\] (6)

where **(a)** is the inner problem, **(b)** is the outer problem, and \(\{g_{i}\}_{i=1}^{N}\) is denoted as a dual variable. It is known that if the cost functions are bounded and there exists a policy that satisfies the constraints of (5), there exists an optimal dual variable , which transforms the supremum into the maximum in (6). Then, the RCRL problem can be solved by **1)** finding optimal policies for various dual variables and **2)** selecting the dual variable corresponding to the policy that achieves the maximum expected reward return while satisfying the constraints.

The inner problem is then a safe RL problem with sub-risk measure constraints. However, although the sub-risk measure is expressed using the expectation as in (3), the function \(g\) causes nonlinearity, which makes difficult to develop a policy gradient method. To address this issue, we propose novel risk value functions that show linearity in the performance difference between two policies. Through these risk value functions, we propose a policy update rule with convergence guarantees for the inner problem in Section 5.2.

The search space of the outer problem is a function space, rendering it impractical. To address this challenge, we appropriately parameterize the function \(g_{i}\) using a parameter \(_{i} B^{M-1}\) in Section 6.1. Then, we can solve the outer problem through a brute-force approach by searching the parameter space. However, it is computationally prohibitive, requiring exponential time regarding to the number of constraints and the dimension of the parameter space. Consequently, it is necessary to develop an efficient method for solving the outer problem. Given the difficulty of directly identifying an optimal \(^{*}\), we instead model a distribution \(\) to estimate the probability of a given \(\) being optimal. We then propose a method that ensures convergence to an optimal distribution \(^{*}\), which deterministically samples an optimal \(^{*}\), in Section 6.2.

By integrating the proposed methods to solve the inner and outer problems, we finally introduce an RCRL algorithm called _spectral-risk-constrained policy optimization (SRCPO)_. We can obtain an optimal \(^{*}\) by solving the outer problem and a set of optimal policies \(\{_{}^{*} B\}\) by solving the inner problem, ultimately achieving the optimal policy \(_{^{*}}^{*}\) of (5). An overview of SRCPO is presented in Algorithm 1, and a detailed pseudo-code of the proposed method is described in Algorithm 2. In the subsequent sections, we will provide details of the proposed methods to solve both the inner and the outer problems, respectively.

``` Input: Spectrum function \(_{i}\) for \(i\{1,...,N\}\).  Parameterize \(g_{i}\) of \(_{i}\) defined in (3) using \(_{i} B^{M-1}\). // Parameterization, Section 6.1. for\(t=1\)to\(T\)do for\(=\{_{1},...,_{N}\} B^{N}\)do  Update \(_{,t}\) using the proposed policy gradient method. // Inner problem, Section 5.2. endfor  Calculate the target distribution of \(\) using \(_{,t}\), and update \(_{t}\). // Outer problem, Section 6.2. endfor Output:\(_{,T}\), where \(_{T}\). ```

**Algorithm 1** Overview of Spectral-Risk-Constrained Policy Optimization (SRCPO)

## 5 Inner Problem: Safe RL with Sub-Risk Measure

In this section, we propose a policy gradient method to solve the inner problem, defined as:

\[_{}[G_{R}^{}]\;\;\;_{_{i}} ^{g_{i}}(G_{C_{i}}^{}) d_{i}\; i,\] (7)where constraints consist of the sub-risk measures, defined in (3). In the remainder of this section, we first introduce risk value functions to calculate the policy gradient of the sub-risk measures, propose a policy update rule, and finally demonstrate its convergence to an optimum in tabular settings.

### Risk Value Functions

In order to derive the policy gradient of the sub-risk measure, we first define risk value functions. To this end, using the fact that \(e_{i,t}=_{j=0}^{t-1}^{j}c_{i,j}/^{t}\) and \(b_{t}=^{t}\), where \(\{e_{i,t}\}_{i=1}^{N}\) and \(b_{t}\) is defined in the augmented state \(_{t}\), we expand the sub-risk measure of the \(i\)th cost as follows:

\[^{g_{i}}_{_{i}}(G^{}_{C_{i}})-_{0}^{1}{g_ {i}}^{*}(_{i}(u))du=_{-}^{}g_{i}(z)f_{G^{}_{C_{i}}}(z) dz=_{_{0}}[_{-}^{}g_{i}(z)f_{Y^{}_{C_{i} }(_{0})}(z)dz]\] \[=}[_{-}^{}g_ {i}(z)f_{_{j=0}^{t-1}^{j}c_{i,j}+^{t}Y^{}_{C_{i}}(_{ t})}(z)dz]=}[_{-}^{}g_{i}(z) f_{b_{1}(e_{i,t}+Y^{}_{C_{i}}(_{t}))}(z)dz]\  t,\]

where \(f_{X}\) is the probability density function (pdf) of a random variable \(X\). To capture the value of the sub-risk measure, we can define risk value functions as follows:

\[V^{}_{i,g}():=_{-}^{}g(z)f_{b(e_{i}+Y^{}_{C_{i}}( ))}(z)dz,\ Q^{}_{i,g}(,a):=_{-}^{}g(z)f_{b(e_ {i}+Z^{}_{C_{i}}(,a))}(z)dz.\]

Using the convexity of \(g\), we present the following lemma on the bounds of the risk value functions:

**Lemma 5.1**.: _Assuming that a function \(g\) is differentiable, \(V^{}_{i,g}()\) and \(Q^{}_{i,g}(,a)\) are bounded by \([g(e_{i}b),g(e_{i}b+bC_{}/(1-))]\), and \(|Q^{}_{i,g}(,a)-V^{}_{i,g}()| bC\), where \(C=}{1-}g^{}(}{1-})\) and \(=(s,\{e_{i}\}_{i=1}^{N},b)\)._

The proof is provided in Appendix A. Lemma 5.1 means that the value of \(Q^{}_{i,g}(,a)-V^{}_{i,g}()\) is scaled by \(b\). To eliminate this scale, we define a risk advantage function as follows:

\[A^{}_{i,g}(,a):=(Q^{}_{i,g}(,a)-V^{}_{i,g}())/b.\]

Now, we introduce a theorem on the difference in the sub-risk measure between two policies.

**Theorem 5.2**.: _Given two policies, \(\) and \(^{}\), the difference in the sub-risk measure is:_

\[^{g_{i}}_{_{i}}(G^{^{}}_{C_{i}})-^{g_{i }}_{_{i}}(G^{}_{C_{i}})=_{d^{^{}}_{},^{ }}[A^{}_{i,g_{i}}(,a)]/(1-).\] (8)

The proof is provided in Appendix A. Since Theorem 5.2 also holds for the optimal policy, it is beneficial for showing optimality, as done in policy gradient algorithms of traditional RL . Additionally, to calculate the advantage function, we use a distributional critic instead of directly estimating the risk value functions. This process is described in detail in Section 7.

### Policy Update Rule

In this section, we derive the policy gradient of the sub-risk measure and introduce a policy update rule to solve the inner problem. Before that, we first parameterize the policy as \(_{}\), where \(\), and denote the objective function as \([G^{_{}}_{R}]=J_{R}()=J_{R}()\) and the \(i\)th constraint function as \(^{g_{i}}_{_{i}}(G^{_{}}_{C_{i}})=J_{C_{i}}()=J_{C_ {i}}()\). Then, using (8) and the fact that \(_{a(|)}[A^{}_{i,g}(,a)]=0\), the policy gradient of the \(i\)th constraint function is derived as follows:

\[_{}J_{C_{i}}()=_{d^{_{}}_{},_{ }}[A^{_{}}_{i,g_{i}}(,a)_{}(_{} (a|))]/(1-).\] (9)

Now, we propose a policy update rule as follows:

\[_{t+1}=_{t}+_{t}F^{}(_{t})_{ }(J_{R}()-_{t}_{i=1}^{N}_{t,i}J_{C_{i}}( ))_{=_{t}}&,\\ _{t}+_{t}F^{}(_{t})_{}(_{t}_{ t}J_{R}()-_{i=1}^{N}_{t,i}J_{C_{i}}())_{= _{t}}&,\]

where \(_{t}\) is a learning rate, \(F()\) is Fisher information matrix, \(A^{}\) is Moore-Penrose inverse matrix of \(A\), \(\{_{t,1},...,_{t,N},_{t}\}[0,_{}]\) are weights, and \(_{t,i}=0\) for \(i\{i|J_{C_{i}}() d_{i}\}\) and \(_{i}_{t,i}=1\) when the constraints are not satisfied. There are various possible strategies for determining \(_{t,i}\) and \(_{t}\), which makes the proposed method generalize many primal approach-based safe RL algorithms [2; 15; 25]. Several options for determining \(_{t,i}\) and \(_{t}\), as well as our strategy, are described in Appendix B.

### Convergence Analysis

We analyze the convergence of the proposed policy update rule in a tabular setting where both the augmented state space and action space are finite. To demonstrate convergence in this setting, we use the softmax policy parameterization  as follows:

\[_{}(a|):=(,a)( _{a^{}}(,a^{}))\ (,a) A.\]

Then, we can show that the policy converges to an optimal policy if updated by the proposed method.

**Theorem 5.3**.: _Assume that there exists a feasible policy \(_{f}\) such that \(J_{C_{i}}(_{f}) d_{i}-\) for \( i\), where \(>0\). Then, if a policy is updated by the proposed update rule with a learning rate \(_{t}\) that follows the Robbins-Monro condition , it will converge to an optimal policy of the inner problem._

The proof is provided in Appendix A.1, and the assumption of the existence of a feasible policy is commonly used in convergence analysis in safe RL [5; 12; 15]. Through this result, we can also demonstrate convergence of existing primal approach-based safe RL methods, such as CPO , PCPO , and P3O ,3 by identifying proper \(_{t,i}\) and \(_{t}\) strategies for these methods. We also provide an analysis of the convergence rate of the proposed method in Appendix A.2, employing a similar approach used in .

## 6 Outer Problem: Dual Optimization for Spectral Risk Measure

In this section, we propose a method for solving the outer problem that can be performed concurrently with the policy update of the inner problem. To this end, since the domain of the outer problem is a function space, we first introduce a procedure to parameterize the functions, allowing the problem to be practically solved. Subsequently, we propose a method to find an optimal solution for the parameterized outer problem.

### Parameterization

Searching the function space of \(g\) is challenging. To address this issue, we discretize the spectrum function as illustrated in Figure 1 since the discretization allows \(g\) to be expressed using a finite number of parameters. The discretized spectrum is then formulated as:

\[(u)(u):=_{1}+_{j=1}^{M-1}(_{j +1}-_{j})_{u_{j}},\] (10)

where \(0_{j}_{j+1}\), \(0_{j}_{j+1} 1\), and \(M\) is the number of discretizations. The proper values of \(_{j}\) and \(_{j}\) can be achieved by minimizing the norm of the function space, expressed as:

\[_{\{_{j}\}_{j=1}^{M},\{_{j}\}_{j=1}^{M-1}}_{0}^{1}|(u )-(u)|du\ _{0}^{1}(u)du=1.\] (11)

Now, we can show that the difference between the original risk measure and its discretized version is bounded by the inverse of the number of discretizations.

**Lemma 6.1** (Approximation Error).: _The difference between the original risk measure and its discretized version is:_

\[|_{}(X)-_{}(X)| C_{}( 1)/((1-)M).\]

The proof is provided in Appendix A. Lemma 6.1 means that as the number of discretizations increases, the risk measure converges to the original one. Note that the discretized version of the risk measure is also a spectral risk measure, indicating that the discretization process projects the original risk measure into a specific class of spectral risk measures. Now, we introduce a parameterization method as detailed in the following theorem:

Figure 1: Discretization of spectrum.

**Theorem 6.2**.: _Let us parameterize the function \(g\) using a parameter \( B^{M-1}\) as:_

\[g_{}(x):=_{1}x+_{j=1}^{M-1}(_{j+1}-_{j})(x- [j])_{+},\]

_where \([j][j+1]\) for \(j\{1,..,M-2\}\). Then, the following is satisfied:_

\[_{}(X)=_{}_{}^{}( X),_{}^{}(X):=[g_{}(X)]+_{0}^{1}g_{ }^{*}((u))du.\]

According to Remark 2.7 in , the infimum function in (2) exists and can be expressed as an integral of the spectrum function if the reward and cost functions are bounded. By using this fact, Theorem 6.2 can be proved, and details are provided in Appendix A. Through this parameterization, the RCRL problem is now expressed as follows:

\[_{\{_{i}\}_{i=1}^{N}}_{}[G_{R}^{}]- _{i=1}^{N}(_{_{i}}^{_{i}}(G_{C_{i}}^{ })-d_{i}),\] (12)

where \(_{i} B^{M-1}\) is a parameter of the \(i\)th constraint. In the remainder of the paper, we denote the constraint function \(_{_{i}}^{_{i}}(G_{C_{i}}^{})\) as \(J_{C_{i}}(;)\), where \(=\{_{i}\}_{i=1}^{N}\), and the policy for \(\) as \(_{}\).

### Optimization

To obtain the supremum of \(\) in (12), a brute-force search can be used, but it demands exponential time relative to the dimension of \(\). Alternatively, \(\) can be directly optimized via gradient descent; however, due to the difficulty in confirming the convexity of the outer problem, optimal convergence cannot be assured. To resolve this issue, we instead propose to find a distribution on \(\), called a _sampler_\((B)^{N}\), that outputs the likelihood that a given \(\) is optimal.

For detailed descriptions of the sampler \(\), the probability of sampling \(\) is expressed as \(()=_{i=1}^{N}[i](_{i})\), where \([i]\) is the \(i\)th element of \(\), and \(_{i}\) is \([i]\). The sampling process is denoted by \(\), where each component is sampled according to \(_{i}[i]\). Implementation of this sampling process is similar to the stick-breaking process  due to the condition \(_{i}[j]_{i}[j+1]\) for \(j\{1,...,M-2\}\). Initially, \(_{i}\) is sampled within \([0,C_{}/(1-)]\), and subsequent values \(_{i}[j+1]\) are sampled within \([_{i}[j],C_{}/(1-)]\). Then, our target is to find the following optimal distribution:

\[^{*}() 0&\{|J_{R} (_{}^{*})=J_{R}(_{}^{*})\},\\ =0&\] (13)

where \(^{*}\) is an optimal solution of the outer problem (12). Once we obtain the optimal distribution, \(^{*}\) can be achieved by sampling from \(^{*}\).

In order to obtain the optimal distribution, we propose a novel update process by parameterizing the sampler and building a loss function. First, we parameterize the sampler as \(_{}\), where \(\), and define the following function:

\[J(;):=J_{R}()-K_{i}(J_{C_{i}}(;)-d_ {i})_{+}.\]

It is known that for sufficiently large \(K>0\), the optimal policy of the inner problem, \(_{}^{*}\), is also the solution of \(_{}J(;)\), which means \(J_{R}(_{}^{*})=_{}J(;)\). Using this fact, we build a target distribution for the sampler as: \(_{t}()(J(_{,t};))\), where \(_{,t}\) is the policy for \(\) at time-step \(t\). Then, we define a loss function using the cross-entropy (\(H\)) as follows:

\[_{t}():=H(_{},_{t})=-_{ _{}}[_{t}()]=-_{_{ }}[J(_{,t};)],\]

\[_{}_{t}()=-_{_{}}[ _{}(_{}())J(_{,t};)].\]

Finally, we present an update rule along with its convergence property in the following theorem.

**Theorem 6.3**.: _Let us assume that the space of \(\) is finite and update the sampler according to the following equation:_

\[_{t+1}=_{t}- F^{}(_{t})_{}_{t}( )|_{=_{t}},\] (14)

_where \(\) is a learning rate, and \(F^{}()\) is the pseudo-inverse of Fisher information matrix of \(_{}\). Then, under a softmax parameterization, the sampler converges to an optimal distribution defined in (13)._

The proof is provided in Appendix A. As the loss function consists of the current policy \(_{,t}\), the sampler can be updated simultaneously with the policy update.

## 7 Practical Implementation

In order to calculate the policy gradient defined in (9), it is required to estimate the risk value functions. Instead of modeling them directly, we use quantile distributional critics \(Z_{C_{i},}(,a;): A B^{N}^{L}\), which approximate the pdf of \(Z_{C_{i}}^{}(,a)\) using \(L\) Dirac delta functions and are parameterized by \(\). Then, the risk value function can be approximated as:

\[Q_{i,g}^{}(,a)_{l=1}^{L}g(be_{i}+bZ_{C _{i},}(,a;)[l])/L,\]

and \(V_{i,g}^{}()\) can be achieved from \(_{a()}[Q_{i,g}^{}(,a)]\). The distributional critics are trained to minimize the quantile regression loss  and details are referred to Appendix C. Additionally, to implement \(_{}\), we use a truncated normal distribution \((,^{2},a,b)\), where \([a,b]\) is the sampling range. Then, the sampling process of \(_{}\) can be implemented as follows:

\[_{i}[j]=_{k=1}^{j}_{i}[k],_{i}[j](_{i,}[j],_{i,}^{2}[j],0,C_ {}/(1-))\  i j.\]

Additional details of the practical implementation are provided in Appendix C.

## 8 Experiments and Results

**Tasks.** The experiments are conducted on the Safety Gymnasium tasks  with a single constraint and the legged robot locomotion tasks  with multiple constraints. In the Safety Gymnasium, two robots--point and car--are used to perform two tasks: a goal task, which involves controlling the robot to reach a target location, and a button task, which involves controlling the robot to press a designated button. In these tasks, a cost is incurred when the robot collides with an obstacle. In the legged robot locomotion tasks, bipedal and quadrupedal robots are controlled to track a target velocity while satisfying three constraints related to body balance, body height, and foot contact timing. For more details, please refer to Appendix D.

**Baselines.** Many RCRL algorithms use CVaR to define risk constraints [15; 29; 32]. Accordingly, the proposed method is evaluated and compared with other algorithms under the CVaR constraints with \(=0.75\). Experiments on other risk measures are performed in Section 8.1. The baseline algorithms are categorized into three types based on their approach to estimating risk measures. First, CVaR-CPO  and CPPO  utilize auxiliary variables to estimate CVaR. Second, WCSAC-Dist  and SDPO  approximate the risk measure \(_{}(G_{C}^{})\) with an expected value \(_{s}[_{}(Y_{C}^{}(s))]\). Finally, SDAC  approximates \(G_{C}^{}\) as a Gaussian distribution and uses the mean and standard deviation of \(G_{C}^{}\) to estimate CVaR. The hyperparameters and network structure of each algorithm are detailed in Appendix D. Note that both CVaR-CPO and the proposed method, SRCPO, use the augmented state space. However, for a fair comparison with other algorithms, the policy and critic networks of these methods are modified to operate on the original state space.

**Results.** Figures 2 and 3 show the training curves for the locomotion tasks and the Safety Gymnasium tasks, respectively. The reward sum in the figures refers to the sum of rewards within an episode,

Figure 2: **Training curves of the legged robot locomotion tasks. The upper graph shows results for the quadrupedal robot, and the lower one is for the bipedal robot. The solid line in each graph represents the average of each metric, and the shaded area indicates the standard deviation scaled by \(0.5\). The results are obtained by training each algorithm with five random seeds.**while the cost rate is calculated as the sum of costs divided by the episode length. In all tasks, the proposed method achieves the highest rewards among methods whose cost rates are below the specified thresholds. These results are likely because only the proposed method guarantees optimality. Specifically in the locomotion tasks, an initial policy often struggles to stabilize the balance of the robot, resulting in high costs from the start. Given this challenge, it is more susceptible to falling into local optima compared to other tasks, which enables the proposed method to outperform other baseline methods. Note that WCSAC-Dist shows the highest rewards in the Safety Gymnasium tasks, but the cost rates exceed the specified thresholds. This issue seems to arise from the approach to estimating risk constraints. WCSAC-Dist estimates the constraints based on the expected risk for each state, but it is lower than the original risk measure, leading to constraint violations.

### Study on Various Risk Measures

In this section, we analyze the results when constraints are defined using various risk measures. To this end, we train policies in the point goal task under constraints based on the \(_{}\) and \(_{}\) risk measures defined in (2), as well as the Wang risk measure . Although the Wang risk measure is not a spectral but a distortion risk measure, our parameterization method introduced in Section 6.1 enables it to be approximated as a spectral risk measure, and the visualization of this process is provided in Appendix E. We conduct experiments with three risk levels for each risk measure and set the constraint threshold as \(0.025\). Evaluation results are presented in Figure 4, and training curves are provided in Appendix E. Figure 4 (Right) shows intuitive results that increasing the risk level effectively reduces the likelihood of incurring high costs. Similarly, Figure 4 (Left) presents the trend across all risk measures, indicating that higher risk levels correspond to lower cost rates and decreased reward performance. Finally, Figure 4 (Middle) exhibits the differences in how each risk measure addresses worst-case scenarios. In the spectrum formulation defined in (1), \(\) applies a

Figure 4: **(Left) A correlation graph between cost rate and reward sum for policies trained in the point goal task under various risk measure constraints. The results are achieved by training policies with five random seeds for each risk measure and risk level. The center and radius of each ellipse show the average and standard deviation of the results from the five seeds, respectively. (Middle) Distribution graphs of the cost rate under different risk measure constraints. Locations of several percentiles (from the \(50\)th to the \(99\)th) are marked on the plot. The risk level of each risk measure is selected to have a similar cost rate. After training a policy in the point goal task, cost distributions have been collected by rolling out the trained policy across 500 episodes. (Right) Distribution graphs of the cost rate with different risk levels, \(\), under the CVaR constraint.**

Figure 3: **Training curves of the Safety Gymnasium tasks. The results for each task are displayed in columns, titled with the task name. The solid line represents the average of each metric, and the shaded area indicates the standard deviation scaled by \(0.2\). The results are obtained by training each algorithm with five random seeds.**

uniform penalty to the tail of the cost distribution above a specified percentile, whereas measures like \(\) and \(\) impose a heavier penalty at higher percentiles. As a result, because \(\) imposes a relatively milder penalty on the worst-case outcomes compared to other risk measures, it shows the largest intervals between the \(50\)th and \(99\)th percentiles.

## 9 Conclusions and Limitations

In this work, we introduced a spectral-risk-constrained RL algorithm that ensures convergence and optimality in a tabular setting. Specifically, in the inner problem, we proposed a generalized policy update rule that can facilitate the development of a new safe RL algorithm with convergence guarantees. For the outer problem, we introduced a notion called _sampler_, which enhances training efficiency by concurrently training with the inner problem. Through experiments in continuous control tasks, we empirically demonstrated the superior performance of the proposed method and its capability to handle various risk measures. However, convergence to an optimum is shown only in a tabular setting, so future research may focus on extending these results to linear MDPs or function approximation settings. Furthermore, since our approach can be applied to risk-sensitive RL, future work can also implement the proposed method in this area.