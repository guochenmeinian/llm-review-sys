# Benchmarking Distribution Shift in Tabular Data

with TableShift

Josh Gardner\({}^{}\)

Zoran Popovic\({}^{}\)

Ludwig Schmidt\({}^{,}\)

\({}^{}\) University of Washington \({}^{}\) Allen Institute for AI

{jpgard, zoran, schmidt}@cs.washington.edu

###### Abstract

Robustness to distribution shift has become a growing concern for text and image models as they transition from research subjects to deployment in the real world. However, high-quality benchmarks for distribution shift in _tabular_ machine learning tasks are still lacking despite the widespread real-world use of tabular data and differences in the models used for tabular data in comparison to text and images. As a consequence, the robustness of tabular models to distribution shift is poorly understood. To address this issue, we introduce TableShift, a distribution shift benchmark for tabular data. TableShift contains 15 binary classification tasks in total, each with an associated shift, and includes a diverse set of data sources, prediction targets, and distribution shifts. The benchmark covers domains including finance, education, public policy, healthcare, and civic participation, and is accessible using only a few lines of Python code via the TableShift API. We conduct a large-scale study comparing several state-of-the-art tabular data models alongside robust learning and domain generalization methods on the benchmark tasks. Our study demonstrates (1) a linear trend between in-distribution (ID) and out-of-distribution (OOD) accuracy; (2) domain robustness methods can reduce shift gaps but at the cost of reduced ID accuracy; (3) a strong relationship between shift gap (difference between ID and OOD performance) and shifts in the label distribution.1

## 1 Introduction

Modern machine learning models have achieved near- or even super-human performance on many tasks. This has contributed to deployments of models across critical domains, including finance, public policy, and healthcare. However, in tandem with the growing deployment of machine learning models, researchers have also demonstrated concerning model performance drops under _distribution shift_ - when the test/deployment data are not drawn from the same distribution as the training data. Analyses of these performance drops have primarily been confined to the domains of vision and language modeling (e.g. [38; 50], where effective benchmarks for distribution shift exist. Despite the widespread use of tabular data, the impact of distribution shift on _tabular_ data has not been thoroughly investigated. While there are existing benchmarks for IID tabular classification, none of these focus on distribution shifts [36; 33].

This is particularly concerning in light of the known differences between tabular data and the modalities mentioned above (images, text, audio). First, in contrast to these modalities, where large neural models are the undisputed state-of-the-art, there is considerable debate about whether deeplearning models improve performance over non-neural baselines (e.g. XGBoost, LightGBM) for tabular data, even without the presence of distribution shift [46; 15; 78]. Second, tabular data tends to contain structured features extracted from raw data (e.g. counts of activities, coded responses to questions), as opposed to the raw signals (e.g. activity event streams, pixel values, audio of responses) where modern machine learning methods perform well and where previous studies of distribution shift have focused. Third, tabular data requires fundamentally different preprocessing procedures, and the impact of these decisions is not widely understood, despite being known to have empirical impact . Finally, high-quality tabular datasets can be difficult to access ; for example, due to the personal nature of many tabular datasets, tabular data cannot simply be scraped at Internet scale as many text and image datasets are. This makes finding high-quality tabular _distribution shift_ datasets particularly challenging.

Thus, the machine learning research community currently lacks not only (1) an empirical understanding of the impact of distribution shift on tabular data models, but also (2) a shared set of accessible and high-quality benchmarks to enable such investigations. We address both gaps in this work. Our main contributions are:

**TableShift Benchmark:** we introduce a curated, high-quality set of publicly-accessible benchmark tasks for (binary) tabular data classification under distribution shift. We describe the tasks in SS3.1 and the API in SS3.2. TableShift includes a set of real-world tabular datasets from domains including finance , public policy , healthcare [19; 47; 18; 74; 87], and civic participation . We select these datasets to ensure a diversity of tasks, distribution shifts, and dataset sizes.

**Large-scale empirical study of distribution shift in tabular data:** We conduct a large-scale study in SS4, including state-of-the-art tree-based tabular models, tabular neural networks, distributional robustness methods, domain generalization methods, and label shift robustness methods. Our findings show (1) a strong linear trend between in-distribution (ID) and out-of-distribution (OOD) accuracy across benchmark tasks and models that was not previously identified for tabular data; (2) that no model consistently outperforms baseline methods, and (3) a correlation between the shift gap and the shift in label distribution, which is not ameliorated by label shift robustness methods included in our study.

Figure 1: In-domain (ID) and out-of-domain (OOD) accuracy show a linear trend across 15 TableShift tasks and 19 model types (\(=0.81\)). ID accuracy (\(x\)-axis values) and change in the label distribution \(_{y}\) (color) together explain 99% of the variance in OOD accuracy (\(R^{2}=0.993\)). For exact results see Section E.3.

**Accessible TableShift API and baselines:** We release a Python API for constructing rich datasets directly from their raw public forms. The API provides built-in documentation of data types and feature codings, alongside standardized preprocessing and transformation pipelines, making the datasets accessible in multiple formats suitable for training tabular models (e.g. in scikit-learn and PyTorch). We also release the set of baseline model implementations (including both state of the art tabular data models, robust learning models, and domain generalization methods) and end-to-end training code in order to facilitate future research on distribution shift in tabular data.

## 2 Setup, Task, and Notation

### Task and Setting

Consider a dataset composed of examples \((x,y,d) P_{d}\) where \(x\) is the input, \(y\) is the prediction target, and \(d\) the domain from which that example is drawn. All examples drawn from \(P_{d}\) have domain label \(d\). We can view the overall data distribution as a mixture of domains \(=\{d_{1},,d_{D}\}\), where \(D 2\). Training examples are drawn from the training distribution \(P^{}=_{d D}q_{d}^{}P_{d}\), and testing examples from \(P^{}=_{d D}q_{d}^{}P_{d}\), with domain weights \(q_{d}\). We can define the training and testing domains as \(^{}=\{d:q_{d}^{}>0\}\) and \(^{}=d:q_{d}^{}>0\), respectively. We refer to cases where \(|^{}| 2\) as "domain generalization" tasks, because domain generalization models require at least two subdomains in the training data.

In a standard (IID) setting, our goal is to learn a classifier \(f_{}\) that accurately predicts \(y\) using examples from \(^{}\). A _distribution shift_ (or _domain shift_) occurs due to the fact that \(P^{} P^{}\). As a consequence of this shift, the joint distributions \(p^{}(x,y) p^{}(x,y)\) differ in training and testing. This difference can be composed of one or more changes to the underlying data generating process. This includes covariate shift, where \(p(x)\) changes; label shift, where \(p(y)\) changes, and concept shift, where \(p(y|x)\) changes. In almost all real-world scenarios, distribution shifts are composed of an unknown mixture of all three forms of shift2. For a fixed classifier \(f_{}\), we refer to

\[_{}=(f_{},^{})-(f_{},^{})\] (1)

as the "shift gap" (where both metrics are computed on examples not seen at training time). Note that the shift gap can be affected by changes in \(p(y)\), \(p(y|x)\), and \(p(x)\). While disentangling the effects of these forms of shift is not a focus of the current work, we provide initial exploratory results on the impact of changes in \(p(y)\), \(p(y|x)\), and \(p(x)\) over the benchmark tasks in Sections 5 and E.

In our setting, we assume that no information about the target \(^{}\) is available - i.e., there is no knowledge of the change in \(p(y)\), \(p(y|x)\), and \(p(x)\), and no unlabeled data from the target domain.

### Related Work

Here we provide a brief overview of related work necessary to contextualize our benchmark and main results. For a detailed overview of related work, see Section D.

Our work is closely related to the literature on distribution shift in machine learning. A series of recent works have demonstrated that even state-of-the-art models experience significant performance drops under distribution shift in tasks including vision, language modeling, and question answering [61; 62; 38; 50; 9]. This has led to the development of methods to mitigate susceptibility to such shifts [76; 53; 1; 6; 90; 89; 54]. High-quality benchmarks, specifically tailored to distribution shift, have been essential in both measuring these gaps and assessing progress toward closing them [38; 50]. The use of tabular data is widespread in practice [15; 46; 78], including the use of sensitive personal data (race, gender, age) and for important tasks (credit scoring, medical diagnosis). However, the impact of distribution shift in the tabular domain has received little attention in the research literature. In particular, benchmarks containing _tabular_ distribution shifts are lacking (one notable exception is Shifts  and Shifts 2.0 , a multimodal benchmark of five tasks, two of which are tabular; for a more detailed overview of domain shift benchmarks and a comparison to TableShift, see Section G).

Tableshift: A Distribution Shift Benchmark for Tabular Data

This work introduces the TableShift benchmark. TableShift contains a set of 15 curated tasks designed to be a rigorous, challenging, diverse, and reliable benchmarking suite for tabular data under distribution shift, and we encapsulate them within a Python API.

### TableShift Benchmark Tasks

To select tasks for TableShift, we identified datasets meeting the following formal criteria:

**Open source:** datasets must be publicly available, including data dictionaries documenting the source of the data (i.e. conditions of its collection), definitions of variables, and any preprocessing applied.

**Real-world:** does not contain simulated data.

  
**Task** & **Target** & **Shift** &  **Domain** \\ **Generalization** \\  &  **Baseline** \\ **Gap**\(_{}\) \\  & 
 **SE(\(_{}\))** \\  \\ 
**ASSISTments** & Next Answer Correct & School & ✓ & \(-34.49\) \% & \(0.011\) \\
**College** & Low Degree Completion & Institution & ✓ & \(-11.16\) \% & \(0.010\) \\
**Scorecard** & Rate & Type & & \\
**ICU Hospital** & ICU patient expires in hos- & Insurance & ✓ & \(-6.30\) \% & \(0.008\) \\
**Mortality** & pital during current visit & Type & & & \\
**Hospital** & 30-day readmission of diabetic hospital patients & Admission & ✓ & \(-5.94\) \% & \(0.002\) \\
**Headmission** & 
 Diabetes diagnosis \\  & Race & ✓ & \(-4.48\) \% & \(0.001\) \\
**ICU Length** & Length of stay = 3 hrs in & Insurance & ✓ & \(-3.39\) \% & \(0.015\) \\
**of Stay** & ICU & Type & & & \\
**Voting** & Voted in U.S. presidential election & Geographic & ✓ & \(-2.58\) \% & \(0.016\) \\
**Food Stamps** & Food stamp recipiency in past year for households with child & Geographic & ✓ & \(-2.39\) \% & \(0.002\) \\
**Unemployment** & Unemployment for non-social security-eligible adults & Education & ✓ & \(-1.28\) \% & \(0.001\) \\
**Income** & Income & Income = 56k for employed adults & Geographic & ✓ & \(-1.25\) \% & \(0.002\) \\
**FICO** & Repayment of Home Equity & Est. third-party risk level & & \(-22.58\) \% & \(0.029\) \\
**HELOC** &  Cooperative of non-Medicare \\ eligible low-income individuals \\  & 
 Disability Status \\  & & & \(-14.46\) \% & \(0.001\) \\
**Sepsis** & Sepsis onset within next & Length of Stay & & & \(-6.05\) \% & \(0.001\) \\
**Childhood** & Blood lead levels above & Poverty level & & \(-5.12\) \% & \(0.005\) \\
**Lead** & CDC Blood Level Reference Value & & & & \\
**Hypertension** & Hypertension diagnosis for high-risk age (50+) & BMI Category & & \(-4.36\) \% & \(0.003\) \\   

Table 1: Summary of TableShift tasks and their associated distribution shifts. For details on each task, see Section E. “Domain Generalization” indicates whether there are multiple training subdomains (\(|^{}| 2\)) and thus whether domain generalization models can be applied to this task. “Baseline gap” gives the “shift gap” \(_{}\) (difference between ID and OOD test accuracy, see Equation (1)) of the tuned XGBoost or LightGBM model with the best validation accuracy after following our hyperparameter tuning procedure (§4.2).

**Sufficient dimensionality and size:** contains at least three features (in all cases, our benchmark datasets contain many more than three features) and at least 1000 observations. In particular, having large test sets is critical for making reliable statistical comparisons between models.

**Heterogeneous:** contains features of mixed types.

**Binary Classification:** supports a meaningful binary classification task (regression tasks are not included).

**Shift Gap:** We explicitly select datasets where strong hyperparameter-tuned tabular baselines display a statistically significant shift gap (\(_{} 0\), see Eqn. (1)).

In addition to these criteria, we selected benchmark tasks and data sources that were _diverse_. TableShift includes tasks from many domains (finance, policy, civic participation, medical diagnosis) and from a variety of raw data sources (electronic health records, surveys/questionnaires, etc.) and with a diversity of shift gap (\(_{}\)) magnitudes.

A summary of the benchmark tasks is shown in Table 1. We give a detailed overview of each task, including background and motivation, information on the data source, and distribution shifts, in Section B. Datasets and each individual feature of each task are also documented in the Python package. One important aspect of TableShift's diversity, shown in Table 1, is that not all real-world tasks support domain generalization (i.e. not all tasks have multiple training subdomains, \(|^{} 2|\)). To reflect this, we include both types of tasks in the TableShift benchmark.

While the intended use of TableShift is for distribution shift, the package is also likely to be of high utility to all researchers studying tabular data modeling due to the data quality, detailed documentation, flexible preprocessing, and ease of use of the datasets in TableShift.

### TableShift API

Successful existing benchmarks for distribution/domain shift in machine learning (e.g. WILDS, DomainBed) not only include high-quality datasets, but also make the data _accessible_ by providing a high-quality API as an interface to the otherwise-disparate sources. This section describes the TableShift API. Providing this API for tabular data is particularly important, for several reasons.

First, the input and output of tabular data pipelines differ from other modalities: tabular datasets are stored in different formats from image and text datasets, and are used with a greater variety of machine learning tools (e.g. scikit-learn). Second, the preprocessing operations used in tabular data differ significantly from other data modalities. These preprocessing operations also require unique feature-level metadata such as data types (i.e. categorical vs. numeric; numeric values for categorical features are a common encoding scheme in practice) and codings for categorical variables. Finally, raw sources used to build tabular datasets can be difficult to access. Datasets are often scattered across hundreds or even thousands of files (e.g., the Sepsis task dataset is constructed from over \(40k\) data files; the Childhood Lead dataset is joined from nearly 100 files containing disjoint feature sets provided by the National Health and Nutrition Examination Survey (NHANES)).

The TableShift API addresses each of these issues. It defines a set of primitives which allow for the construction of data pipelines which go from raw data sources to preprocessed data of any TableShift benchmark task in a few lines of Python code3. The resulting data is documented - each feature in the benchmark includes metadata which describes the feature and any encodings. The API natively supports a set of common data transformations, including one-hot and label encoding for categorical data; scaling and binning of numeric data; and handling of missing values. TableShift provides native output in a variety of data formats, including PyTorch DataLoaders, Pandas DataFrames, and Ray Datasets. Finally, _any_ dataset in the TableShift benchmark can be loaded with default preprocessing parameters with an identical call to the API, providing a unified interface.

We provide a a detailed comparison between TableShift and related existing benchmarks in Section G. However, we emphasize that there is _no_ existing benchmark suite for distribution shift in tabular data, and existing distribution shift benchmarks are incompatible with the unique constraints of tabular data discussed above.

Experiment Setup

We conduct a set of experiments to demonstrate the potential insights to be gained from using TableShift. As previously mentioned, there has been considerable debate about whether tree-based models (XGBoost, LightGBM, etc.) or specialized deep learning-based models (i.e. ResNet- and Transformer-based architectures) are more effective for tabular data modeling. However, previous investigations have not explored how these models perform under _distribution shift_ in tabular data. Additionally, many methods have been proposed for robust learning and domain generalization but also not rigorously evaluated on tabular data. We present a series of experiments to evaluate 19 distinct methods using the TableShift benchmark.

### Tabular Data Classification Techniques in our Comparison

We train and evaluate a set of tabular data classifiers from several families. For each, we give additional details and description in Section F, and the full hyperparameter grids in Table 19. Implementations of these classifiers, including the hyperparameter tuning framework used to tune them, are available in the TableShift API. The classifiers compared in our experiments are:

**Baseline Models:** These models do not include any intervention for robustness to domain shift, but are generally effective for tabular data in the IID setting. We evaluate multilayer perceptrons (MLP), XGBoost , LightGBM , and CatBoost  as baseline methods. While we refer to these as "baselines" for convenience, we note that the methods based on gradient-boosted trees (XGBoost, LightGBM, CatBoost) are still considered state-of-the-art on many tasks .

**Tabular Neural Networks:** We also include a set of state-of-the-art methods for modeling tabular data. The models we use are SAINT , TabTransformer , NODE , FT-Transformer, and tabular ResNet (the latter two via ).

**Domain Robustness Models:** These models attempt to ensure good performance on distributions close to the training data. These models attempt to optimize an objective over a worst-case distribution with bounded distance from the training data. We evaluate distributionally robust optimization (DRO) with both \(^{2}\) and CVaR geometry , and group DRO (where the groups are domains) . Both the DRO and group DRO models are parameterized over MLPs, as in both original works.

**Label Shift Robustness Models:** These models attempt to ensure good performance when the label distribution \(P(y)\) changes. We evaluate Group DRO (where the groups are class labels) and the adversarial label robustness method of .

**Domain Generalization Models:** These are models designed with the goal of achieving low error rates on unseen test domains. In practice, this is achieved by achieving low error _disparity_ across the subdomains in \(^{}\). These methods require domain labels at training time, and training data drawn from multiple different domains (\(|^{}| 2\)). Domain generalization models in our study are: Domain-Adversarial Neural Networks (DANN) , Invariant Risk Minimization (IRM) , Domain MixUp , Risk Extrapolation (VReX) , DeepCORAL  and MMD .

We note that our goal of the current study is not to propose novel methods for distributionally robust learning; it is to conduct a comprehensive comparison of a large set of existing methods, many of which have not been previously compared to each other, on a high-quality benchmark. For example, while domain generalization models have been applied to image and text classification tasks (e.g. ), to our knowledge these methods have not been previously investigated for mitigating distribution shift in _tabular_ data in a large-scale benchmarking study. Indeed, we are aware of no prior applications of many of these domain generalization methods to tabular data. As a result, it is not clear _a priori_ how these methods might compare to existing robustness or baseline methods due to the aforementioned differences between tabular data and these other data modalities.

The experiments described above cover both model architectures (different functional forms for the predictor \(f_{}\)) and loss functions (different objective functions \(\) used to train the model by attempting to find \(_{}\,(f_{}(^{}))\)). In order to train a classifier with gradient-based training, both are required. Except where noted otherwise, any method requiring gradient-based training (MLP, Tabular Neural Networks, Domain Generalization Models) is trained with standard empirical risk minimization and cross-entropy loss. Similarly, any method which itself is a loss function (i.e. all variants of DRO) is trained with \(f\) parameterized as an MLP, as is standard in prior works implementing and comparing these methods (e.g. [53; 76; 33]).

### Methods

For each task, we conduct the following procedure.

First, we split the full dataset into \(^{}\) and \(^{}\). We summarize the domain splits in Tables 1,1 and describe the splitting for each task in detail, along with background and motivation for each task domain split, in Section B. Within each domain, we have both a validation and a test set. We use the same domain splits, data preprocessing, and train/validation/test splits for all models and training runs, except where explicitly noted.

Second, we then conduct a hyperparameter sweep for each model described in Section 4.1. We use HyperOpt  to sample from the model hyperparameter space, in accordance with previous works (e.g. [36; 46]) which largely use adaptive hyperparameter optimization due to the variability in effective hyperparameter settings between datasets. We only train on the training set, and use the in-domain validation accuracy for hyperparameter tuning. We give the complete grid for each model in SS1. Each model is tuned for 100 trials.

Finally, we evaluate the trained models on the test splits of each dataset. As recommended in , we use in-domain and out-of-domain _test_ accuracy (not in-domain train accuracy) to evaluate the models. For all results shown, we use the best model selected according to (in-domain) validation accuracy; this follows the selection procedure used to study domain generalization in the image domain in .

## 5 Empirical Results

**ID and OOD Accuracy are Correlated.** Our results show that, across all models and tasks, in-distribution (ID) and out-of-distribution (OOD) accuracy are correlated: as ID performance improves, OOD performance also tends to improve (see Figure 1; \(=0.81\)). This linear trend holds _across datasets and model classes_. We note that, while this is consistent with findings for image  and question answering  models, the relationship between ID accuracy and OOD accuracy on tabular data was previously unknown. This result suggests that, for a wide variety of tabular data tasks, improving models' ID performance is likely to improve their OOD performance.

**No Model Consistently Outperforms Baselines.** While many models have been proposed for both (a) improving general performance on tabular data tasks over established baselines such as XGBoost and LightGBM, and (b) improving robustness to distribution shift, our results show that no model consistently outperforms the standard tabular baselines of XGBoost, LightGBM, or CatBoost in either respect. Figure 3(a) shows that, on average across all datasets, no model consistently achieves better performance (as measured as a fraction of the maximum OOD accuracy achieved by any model) compared to baseline methods. This finding has not been previously demonstrated in tabular data due to the lack of an existing benchmark.

**No Method Eliminates Gaps.** We investigate the empirical performance of several methods designed to improve robustness to distribution shift (described in Section 4.1). Our results shows that, on the datasets where multiple training subdomains are available (and thus where domain generalization is viable), there is weak evidence that several techniques reduce gaps due to distribution shift, but no technique eliminates these gaps. However, it is important to note that this gap reduction comes at the cost of in-distribution performance: as Figure 3(b) shows, all robustness-enhancing models tend to shrink gaps by _reducing average ID performance, not by improving OOD performance_. This is shown in Figure 3(b) by the two parallel lines: one set of blue points representing baselines + tabular NNs, and another, shifted left, representing robustness-engancing and domain generalization models. Furthermore, we note that all domain generalization and domain robustness methods evaluated (excluding DRO) require additional information that is only present for some datasets - namely, the discrete variable over which a shift will occur (e.g. "race" for diabetes task) and data from at least 2 categories of this variable.

**Change in label distribution is correlated with shift gap.** We investigate the degree to which the three factors mentioned previously (\(p(x)\), \(p(y|x)\), \(p(y)\)) are related to model performance. Our results, in Figures 5 and 8, show that change in the label distribution \(_{y}\) is correlated with shift gap Figure 2: Results for baselines, robust learning, and domain generalization models across the 15 TableShift benchmark tasks. The \(y=x\) line indicates a model with no shift gap, \(_{}=0\) (see Equation 1). Clopper-Pearson confidence intervals at \(=0.05\) shown for all points. Note that domain generalization models are only used on domain generalization tasks (cf. Table 1). Results for the remaining TableShift tasks are shown in Figure 3. For exact results see Section E.3.

Figure 4: (a): Percentage of Maximum OOD Accuracy (PMA-OOD) across tasks (see Table 14 for exact values). *: domain generalization models and Group DRO can only be trained on the subset of 10 tasks with multiple training subdomains (see “Domain Generalization” column in Table 1). (b): Average ID and OOD accuracy by model across domain generalization tasks only. We only show domain generalization tasks in order to compare all models on the same set of tasks. See Figure 9 for results on all tasks. Exact values in Table 15.

Figure 3: Additional results (cf. Figure 2). For exact results see Section E.3.

\(_{}\) (Pearson correlation \(=0.71\)). This persists even after accounting for ID accuracy: a simple linear regression of OOD accuracy on [ID accuracy, \(_{y}\)] obtains \(R^{2}=0.996\). This suggests that the change in the label distribution is an important factor in understanding tabular shifts (for example, the outliers in Figure 1 are from the four tasks with largest label shift: Public Coverage, HELOC, ASSISTments, College Scorecard; see Figures 2, 3 and Table 3). Label shift robustness methods in our study _did not_ eliminate performance gaps under shift; in fact, label shift robustness methods often degraded both ID and OOD accuracy (e.g. Figure 3(b)). We provide similar analyses relating shift gap to \((i)\) covariate shift and \((ii)\) concept shift in Figure 8, but find that they are not clearly related.

**Changes in predictions are related to covariate shift.** As an exploratory finding, we find some evidence that changes in the predictions for OOD data are correlated with changes in \(p(x)\), shown in Figure 6(a) (\(=0.99\)). This suggests that shift gaps in the benchmark datasets not explained by the combination of ID accuracy and \(_{y}\) may be driven primarily by covariate shift (changes in \(p(x)\)) as opposed to concept shift (changes in \(p(y|x)\)). Further analysis is needed to confirm this exploratory finding. We note that relationships between other forms of shift showed much weaker correlation, roughly \(-0.2\) (see Figure 7).

## 6 Limitations

The conclusions in this study are limited to the specific datasets and models evaluated. While we intentionally selected a diverse suite of benchmark datasets along several axes (domain, distribution shift, dataset size, etc.), our conclusions can only be extended to other distribution shifts insofar as they are similar to the shifts in TableShift. More empirical validation is needed, including studies comparing our findings to other tabular shifts.

Our work does not exhaustively cover the space of all possible tabular data classifiers. In particular, "hybrid" methods combining some of the loss-based robustness interventions (i.e. Group DRO) with various tabular data-specific model architectures (e.g. FT-Transformer, ResNet) might lead to different results. Our initial exploratory evaluation of hybrid methods (see Section E.5), however, does not suggest that hybrid methods led to qualitative changes in our results, but these methods warrant a more extensive evaluation. Finally, our work does not establish _theoretical_ connections between the factors analyzed (ID accuracy, OOD accuracy, \(_{y}\)).

## 7 Conclusion

We introduce the TableShift benchmark for studying distribution shift in tabular data. TableShift presents a diverse set of tasks for reliable study and benchmarking of tabular data models under distribution shift. We provide a Python API to access the datasets, along with implementations of several models including baselines, distributionally robust learners, and domain generalization methods. Finally, we present empirical results which form the first large-scale study of tabular data modeling under distribution shift.

Our results suggest multiple potential avenues for future work: First, improvements to _in-distribution_ accuracy are likely to drive OOD accuracy gains. Second, improved robustness to _label shift_ may reduce shift gaps. Third, _hybrid methods_ which combine robustness-enhancing losses (such as Group DRO) with improved neural network architectures may be able to further improve OOD performance. Beyond these proposed directions, we hope that TableShift opens new research frontiers for tabular machine learning research beyond those addressed in the current work.

Figure 5: Label shift (\(_{y}\), measured via Equation (3)) and absolute shift gap \(_{}\) show moderate correlation across datasets and models (Pearson correlation \(=0.70\)). Exact \(_{y}\) values in Table 3.