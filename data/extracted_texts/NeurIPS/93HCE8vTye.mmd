# Transformers need glasses! 68

Information over-squashing in language tasks

 Federico Barbero

University of Oxford

federico.barbero@cs.ox.ac.uk &Andrea Banino

Google DeepMind

abanino@google.com &Steven Kapturowski

Google DeepMind

skapturowski@google.com &Dharshan Kumaran

Google DeepMind

dkumaran@google.com &Joao G.M. Araujo

Google DeepMind

joaogui@google.com &Alex Vitvitskyi

Google DeepMind

avlife@google.com &Razvan Pascanu

Google DeepMind

razp@google.com &Petar Velickovic

Google DeepMind

petarv@google.com

Work performed while at Google DeepMind.

###### Abstract

We study how information propagates in decoder-only Transformers, which are the architectural backbone of most existing frontier large language models (LLMs). We rely on a theoretical signal propagation analysis--specifically, we analyse the representations of the last token in the final layer of the Transformer, as this is the representation used for next-token prediction. Our analysis reveals a _representational collapse_ phenomenon: we prove that certain distinct sequences of inputs to the Transformer can yield arbitrarily close representations in the final token. This effect is exacerbated by the low-precision floating-point formats frequently used in modern LLMs. As a result, the model is provably unable to respond to these sequences in different ways--leading to errors in, e.g., tasks involving counting or copying. Further, we show that decoder-only Transformer language models can lose sensitivity to specific tokens in the input, which relates to the well-known phenomenon of _over-squashing_ in graph neural networks. We provide empirical evidence supporting our claims on contemporary LLMs. Our theory also points to simple solutions towards ameliorating these issues.

## 1 Introduction

In recent years the field of Natural Language Processing (NLP) has been revolutionised through the introduction of Transformer-based architectures . Large Transformers trained on some version of next-token prediction, known as _Large_ Language Models (LLMs), have demonstrated impressive performance across different tasks, including conversational agents , understanding multi-modal inputs , and code completion . Most contemporary LLMs specifically focus on the decoder part of the original Transformer architecture, and are commonly referred to as _decoder-only_ Transformers. Consequently, we focus primarily on such models in this paper.

However, despite the impressive performance of Transformers, recent works have uncovered surprising failures that may point to fundamental issues in their architecture. For instance,Transformer-based LLMs seem to be particularly challenged by seemingly simple tasks requiring counting  or copying elements along input sequences . We find it important to study such failure cases, as these operations are fundamental building blocks of computation, and they are often necessary for solving reasoning tasks. A common strategy to assist LLMs in solving such tasks is to supply them with 'tools' [e.g. 25]. We argue that, while tool use will certainly help, it is still important to improve base model capabilities in this regard, because oftentimes, even producing accurate inputs to a tool may require complex reasoning operations. Specifically, often we need to _copy_ some part of the Transformer's input into a tool--if the base model struggles with robust copying, even this operation can be in peril.

Accordingly, we find it important to explain why decoder-only Transformers do not perform well when it comes to such problems--not just as an intellectual endeavour, but also to help guide further practical improvements. While many works have studied the computational capabilities of Transformers , they often make assumptions which do not correspond to present practical limitations, such as infinite floating-point precision or 'hard attention', making their conclusions less directly practically applicable.

In this work, we take a different approach and study what information _can_ be contained in the representation of the last token at the last layer, as this is ultimately the information that will be used for next-token prediction -- the fundamental mechanism through which modern Transformer LLMs perform training and inference. In particular, we show that for certain distinct sequences, their last-token representations can become arbitrarily close to each other. This leads to a _representational collapse_, exacerbated by the lower-precision floating point types typically used by modern LLM stacks. As a result Transformers incorrectly produce the same tokens on these sequence pairs -- see Figure 1 (a).

Furthermore, we reveal that the computation graph employed by decoder-only Transformers, with its unidirectional causal mask, contributes to the observed representational collapse. This unidirectional flow of information, converging at the final token, is in fact likely to lead to a loss of information due to _over-squashing_, an effect that is well studied in graph neural networks (GNNs) , and related to vanishing gradients . We hope that this result will be of independent interest to the GNN community, as a practical application of over-squashing results at scale. Finally, we provide supporting empirical evidence that these issues are likely of practical interest, and propose simple solutions--directly stemming from our theoretical study--to help alleviate them.

In summary, our paper provides the following contributions:

* Theoretical analysis of decoder-only Transformer limitations: we formalise the concepts of'representational collapse' (Section 4) and 'over-squashing' (Section 5) in the context of Transformer-based architectures.

Figure 1: **(a) Representational Collapse** (Theorem 4.2). From top to bottom, we have a series of sequences given to Transformer architectures, each comprising repeated 1 tokens with a single 0 token at the end. The color and proximity of the curved lines illustrate how these representations converge as sequence length increases. **(b) Over-squashing** (Theorem 5.1). Due to the architecture of decoder-only Transformers, tokens that are earlier in their input sequence will have significantly more paths through which their data can reach the representation used for next-token prediction, leading to ‘over-squashing’. This effect is depicted here for an early token (blue) and later token (red) in a five-token sequence.

* Impact of floating point precision: we explore how low floating-point precision exacerbate the identified theoretical issues, causing them to manifest even in relatively short input sequences.
* Empirical validation of theoretical analysis: our theoretical findings are supported by real-world experiments conducted on contemporary LLMs, demonstrating practical implications of the limitations we identified.

## 2 Background

In this work, we study a class of Transformers which we believe forms the basis for a large number of current LLMs. We let \(,,^{n d}\) be the query, key, and value matrices respectively on \(n\) tokens and \(d\) dimensions. We denote with \(_{i},_{i},_{i}^{d}\) the \(d\)-dimensional query, key, and value vectors of the \(i\)-th token. We let \(_{ij}^{2e}\) be the \(2e\)-dimensional positional encoding information between tokens \(i\) and \(j\). We focus on the case in which the positional encodings are _bounded_, which is the case for the large majority of positional encodings used in practice . The Transformer model we consider computes the values, for a single head, of the \(i\)-th token at the \(\)-th Transformer layer \(_{i}^{()}\) as2

\[_{i}^{()} =_{j i}_{ij}^{()}_{1}^{( )}(_{i}^{()})+_{i}^{()},_{ij}^{()}=_{i}^{()},_{j}^{()},_{ij}))}{_{w i}(k( _{i}^{()},_{w}^{()},_{iw}))}\] \[_{i}^{(+1)} =^{()}(_{2}^{( )}(_{i}^{()}))+_{i}^{()}\]

for a function \(k:^{d}^{d}^{2e}\) mapping queries, key, and positional encoding information to a scalar value, an MLP \(:^{d}^{d}\), and normalization functions at the \(\)-th layer \(_{1}^{()}\) and \(_{2}^{()}\). This specific interleaving of components is often referred to as a Pre-LN Transformer . We can view the output of the \(\)-th layer of a Transformer as a sequence of \(d\)-dimensional vectors \(^{()}=(_{1}^{()},,_{n}^{()})\). Importantly, due to the causal attention mechanism, the vector \(_{j}^{()}\), will only depend on elements \(_{i}^{(-1)}\) for \(i j\). We can group the attention weights into an _attention matrix_ at the \(\)-th layer which we define element-wise as \(_{ij}^{()}=_{ij}^{()}\). This is a row-stochastic triangular matrix that can also be interpreted as a probabilistic directed graph. After the last transformer block a normalization is applied to the token representations:

\[_{i}=_{3}(_{i}^{(L)})\]

We note that the next-token prediction usually depends purely on \(_{n}\)--the final representation of the last token.

Existing theory on Transformers.The theoretical representational capacity of Transformers has become a popular area of study, providing interesting results on what classes of problems they are able to model. For instance, it has been pointed out that Transformers are not Turing-complete, but one can apply modifications which make Transformers Turing-complete under certain assumptions . Works have also shown that Transformers using 'hard attention' which replaces softmax with one-hot vectors alongside the use of infinite precision makes Transformers Turing-complete . This contrasts with our work, which focuses on the more standard setting of Transformers using soft-attention and finite precision, and shows the limitations imposed by it.

Works have also tried to study transformers capabilities through the sense of formal languages, such as Weiss et al. , which develops a computational model of what transformers can represent in an analogous way to how Recurrent Neural Networks are associated with finite automata, and then derive an implementable programming language that representsthat model. Following that, Deletang et al.  place transformers within the Chomsky Hierarchy, showing that they are quite limited and cannot learn the decision problem for simple languages, which prompted authors to show that Transformer LLMs can perform substantially better if they generate a number of decoding tokens linear in the problem input size, through scratch-pad, Chain-of-Thought (CoT) or similar . Finally, Peng et al.  show that the Transformer block with finite precision is fundamentally limited in its ability to represent compositional functions and solve simple problems that require it. Our work similarly analyses the Transformer's inability to solve simple computational tasks, and proves that even with techniques like Chain-of-Thought that inability persists as it is inherent to the combination of architecture, next-token prediction, and limited floating point precision.

Decay in attention mechanisms.Works have also studied the limitations of self-attention by showing that it can reach pathological states that limit what transformers are able to learn. For instance, it has been show how a great reduction in the attention entropy can lead to unstable training if occurring early, but even when occurring later in training it can still lead to significantly lower performance . Further, it has been shown that specific tokens can strongly concentrate attention, leading to transformers being unable to learn to process simple languages, like PARITY and DYCK . Our work will similarly focus on showing how Transformers end-up effectively ignoring many tokens in their input which leads them to fail to solve simple computational problems, studying such a phenomenon by directly analysing the representational capacity.

Over-squashing.Graph neural networks (GNNs) are neural networks designed to operate over graph structures. Importantly, Transformers, may be seen as types of attention-based GNNs operating over specific types of graphs. The difficulties of propagating information over a graph have been thoroughly analysed, with a notable phenomenon being that of _over-squashing_. Over-squashing refers to the fact that propagating information over certain graphs that exhbit 'bottlenecks' is likely to induce a'squashing' of information. This can be made more precise by studying this effect via the notion of a _commute time_ -- the expected number of steps that a random walk takes to travel from a node to another node and back. Information travelling between nodes with higher commute time will be squashed more.

A common way to measure over-squashing is by looking at how _sensitive_ the representation \(_{v}^{(L)}\) of a node \(v\) after \(L\) GNN layers is to the initial representation \(_{u}^{(0)}\) of another node \(u\). In particular, the partial derivative \(_{v}^{(L)}/_{u}^{(0)}\) may be shown to decay, especially for nodes with high commute times between them. Our work may be seen as acting as a bridge between the well-studied phenomenon of over-squashing in GNNs and the loss of information we analyse in decoder-only Transformers specifically for language tasks. Note that this type of derivation is typical in the study of _vanishing gradients_ for recurrent models as well .

## 3 Motivating Examples

This section presents a series of experiments focused on copying and counting tasks. These experiments reveal surprising failure cases in modern decoder-only Transformer architectures,

Figure 2: Results on simple copying tasks. (a). Gemini was prompted to predict the last token (diamond) of a sequences ‘1...10’ or the first token (square) of a sequence ‘01...1’. (b). Same as (a) but with hints (see 3.2 for details) (c). Same as (a) but the sequences have interleaved 0s and 1s. See C.1 for extra details

providing concrete evidence that motivates the theoretical analysis presented in the following sections.

We start by providing motivating examples that show surprisingly simple failure cases of frontier LLMs specifically on copying (Section 3.1) and counting (Section 3.2) tasks. By copying we specifically mean tasks that involve the 'copying' or'recalling' of a single or multiple tokens from the prompt. Instead, by _counting_, we mean the task of counting how many times a specific token appears in a sequence. We focus our evaluation on Gemini 1.5  as our frontier LLM (referred as Gemini) and later analyse the internal representations of the open-sourced Gemma model . The goal is to showcase intriguing failure cases which will motivate our signal propagation analysis.

### Copying

In this Section, we present surprising results on simple _copying_ tasks. In particular, we focus on tasks that involve the copying of a _single_ token -- i.e. what is the token occurring at a particular position? The copy of a single token is in principle the most straightforward type of copying task, but still requires the LLM to accurately identify the token based on a prompt and to then propagate its information correctly.

Importantly, we study cases in which the LLM is prompted to copy tokens either at the _start_ or at the _end_ of a sequence. We avoid tasks that involve the copy of tokens at the '\(n\)-th' position as most frontier LLMs do not have absolute positional information, making it very challenging for them to solve tasks that require absolute position. We focus on tasks that involve sequences of 'zeros' and 'ones' growing in length with specific patterns.

In Figure 2 (a), we prompt Gemini to copy the last element of a sequence '\(1 10\)' or the first element of a sequence '\(01 1\)'. The answer for both is zero, but we progressively grow the number of ones. We observe how the task seems considerably easier when asked to return the first rather than the last element. Surprisingly, already at a sequence length of only 300 elements, Gemini incorrectly starts to output 'one' when trying to copy the last element. In Figure 2 (b), we show that providing hints in the form of: " *Hint* It's not necessarily a 1, check carefully", helps significantly with the performance. Finally, in Figure 2 (c), we show that replacing the constant sequence of ones with alternating ones and zeros seems to also help. We refer to the Appendix (Section C.1) for further details on the experiments.

These three motivating experiments seem to point towards a type of vanishing of information, caused by the growing number of ones dominating the sequence. Interestingly, such a

Figure 3: Gemini 1.5 being prompted to sum \(1++1\) (Column 1), Count the number of ones in a sequence of 1s (Column 2), Count the number of ones in a sequence of ones and zeroes (the sequence is a Bernoulli sequence with probability of sampling a one being 0.7) (Column 3), and to counter the number of times a word appears in a sentence (Column 4).

vanishing of information effect (a) seems to depend on the position in the sequence, (b) seems to be affected by the prompting, and (c) by the items that make up the sequence. We will later argue how all three of such observations can explained by our theoretical analysis.

### Counting

We now turn our attention to _counting_ problems, i.e. tasks of the form -- given a specific sequence, how many times does a particular token appear? Such problems are related to copying in the sense that they also require careful consideration of individual tokens in the sequence as ignoring even a single token may potentially lead to an incorrect output.

We consider four different tasks: (i) Summing \(1++1\), (ii) Counting the number of ones in a sequence of ones, (iii) Counting the number of ones in a sequence of ones and zeros, with ones being sampled with 70% probability, and (iv) Counting the number of times a specific word appears in a sentence. We consider predictions of an LLM which (1) Is instructed to only output the answer, (2) Is prompted to break down the problem (CoT-no-shot), and (3) Is prompted to break down the problem with few-shot in-context examples (CoT-few-shot). We refer to the Appendix (Section C.1) for a more detailed description of the tasks.

Results are presented in Figure 3. It is clear that the performance rapidly deteriorates with the sequence length. It is also interesting to see that the error seems to increase with the sequence very rapidly. For instance in task (i), the LLM is quite likely to predict the value of '100' once the sequence reaches a size around or larger than 100. Such an observation provides motivating evidence for the argument that Transformers may not be in fact mechanically counting but rather perform a type of crude sublisting. This explains why arguably 'common' numbers such as 100 are much more likely to be outputted by the LLM and why in tasks such as (i) and (ii) the values near 100 have relatively lower error. This does not happen in task (iii) as the response should actually be around 70% of the sequence length due to the sequence sampling procedure, explaining why the absolute error actually seems to increase around a sequence length of 100. Figure 4 further showcases this issue, more clearly showing how 100 is by far the most common response.

## 4 Representational Collapse

We start our theoretical analysis by showcasing a type of loss of information which we call _representational collapse_. More precisely, we show that under certain conditions, we can find distinct sequences such that their final representations of the last token at the last layer _become arbitrarily close_ as the sequence length increases. As Transformer models operate over finite machine precision, this points to a fundamental representational incapacity of Transformers to distinguish certain prompts if the sequence is long enough.

The key intuition is that if two sequences are similar everywhere except at the last token, as the sequences get larger, their final representations will become closer and closer until they

Figure 4: Frequency of different outputted values for Gemini 1.5 for the counting tasks. The large density at 100 suggests that Gemini is likely not counting, but instead possibly performing some crude form of subitising.

reach a critical point which is below floating point precision. In other words, solving certain tasks would require infinite floating point precision. We will later show how this phenomenon is not only theoretical, but also occurs in practice on sequences of reasonable length. In the Appendix (Section 4), we relate representational collapse to the \(L_{1}\) distance - or total variation - between the softmax distributions of the two sequences. We start by presenting a result that shows that the \(L_{1}\) difference tends to \(0\) as the sequence length grows, under some assumption on the sequences. We point to the Appendix (Lemma B.2) for the complete statement.

**Lemma 4.1** (Informal).: _Consider two sequences \(,^{}^{n}\) such that \(_{n}|_{n}-_{n}^{}|=0\). Then, the \(L_{1}\) difference of their softmax tends to \(0\)._

We now show, using Lemma 4.1, that we can find distinct sequences that will have arbitrarily close final representations. In particular, as language models often operate in low floating regimes, i.e. bf16, this can practically become catastrophic. The result is summarised in Theorem 4.2, which describes what we call representational collapse in this work. The complete statement is reported in the Appendix (Theorem B.3).

**Theorem 4.2** (Representational Collapse - informal).: _Let \(^{(0)}^{n d}\) be a sequence and \(^{*(0)}^{(n+1) d}\) be another sequence equal to \(^{(0)}\) with the last token of \(^{(0)}\) repeated. Assume that the positional encoding information decays to \(0\) with the distance. Then, their representations become arbitrarily close as \(n\) increases._

Theorem 4.2 shows that it becomes increasingly challenging for a Transformer to distinguish two sequences that only differ via a repeated last token. We note that the repetition of the last token is a technical consideration to show this direct representational collapse. As we will later show in Section 5.1, it is particularly problematic _in general_ to depend on the last token due to a type of topological'squashing' present in decoder-only Transformers.

Measuring representational collapse.We report experiments showcasing representational collapse by measuring the internal representations of Gemma 7B . For two sequences \(^{(0)}\) and \(^{*(0)}\) we report their difference in representation at the last layer \(\|^{(L)}-^{*(L)}\|_{}\) averaged out over each head, alongside the minimum and maximum over each head. Figure 5 shows the collapse occuring on (a) prompting the model to count the number of ones in a sequence of ones, with one having an additional one, and (b) prompting the model to count the number of ones for a sequences with digits sampled uniformly ending with either a single one or two ones. The repeated digits seem to make the collapse occur much sooner with a sequence length of around \(50\) being near machine precision, while varying the digits seems to delay such a collapse, but a downward trend is maintained with respect to the sequence length.

Quantisation and Tokenisation.A common technique used to speedup the inference of an LLM is that of _quantisation_, a process that constructs an approximate version of an LLM that operates over lower precision datatypes. This helps drastically improve the inference speed of LLMs as modern accelerators produce significantly more FLOPs over lower precision datatypes. Of course quantisation usually comes at a cost. Our theoretical analysis points towards a potentially catastrophic loss in representation due to quantisation. In particular, a lower machine precision will mean that the convergence of representations in Theorem 4.2 will occur much sooner, and consequently the LLM will not be able to distinguish even shorter sequences.

In practice, the direct application of theoretical results is made more complicated due to tokenisation. In particular, a sequence of repeated tokens '11111' for instance may not be necessarily tokenised into 5 distinct '1' tokens. In principle, this should help alleviate the direct collapse of the representations. Tokenisation in general makes it more of a challenge to study such phenomena as it adds an additional layer of complexity to the experimental analysis. In our experiments, we took tokenisation into consideration and attempted to mitigate its effects.

A simple solution to representational collapse.An important consequence of Theorem 4.2 is that _it is challenging for a Transformer to deal with a long sequence of repeated tokens_. A practical solution is to this issue is to introduce additional tokens throughout the sequence to help keep the representations distant. We provide direct evidence of this in Figure 5 (c,d), where we prompt the model on a simple copying task of a long string of ones. While the representations collapse for the sequence of ones (c), adding commas every third digit (d) helps to keep the representations well-separated.

## 5 Over-squashing in Language Tasks

In this Section, we study a more general phenomenon related to representational collapse--over-squashing. In particular, we are interested in analysing how information from the input sequence affects the information contained within the representation of the last token in the final layer--the representation ultimately used for next-token prediction. For this reason, we study the quantity \(_{n}/_{i}^{(0)}\) which measures how sensitive is the final token to an input token at position \(i\).

In graph neural network theory, the decay of such a partial derivative is often associated with the'squashing' of information, leading to the phenomenon of _over-squashing_, a problem related to the well-known vanishing gradients problem in RNNs . The over-squashing analysis we carry out in this work is particularly challenging due to the flexible nature of the attention mechanism and the many components that are part of decoder-only Transformers. Consequently, we make two simplifying assumptions in our analysis: (i) We summarise the effect of layer normalisation via a constant \(_{i}\) for the \(i\)-th layer norm component, and (ii) the attention weights are treated as independent of the input. Such simplifications are not strictly necessary for our analysis, but they greatly simplify the resulting bound we derive and do not detract from the two key takeaways: **(1) the sensitivity to an input token depends on its position in the sequence and (2) the sensitivity to an input token depends on the attention weights**. The result is summarised in Theorem 5.1. The full statement is reported in the Appendix (Theorem B.5).

**Theorem 5.1** (Over-squashing in Transformers).: _Consider an input sequence \(_{1}^{(0)},,_{n}^{(0)}\). Let \(C>0\) be some constant and \(_{i,j}^{()}=^{()}}{_{2}}+_{i,j}\), then:_

\[\|_{n}}{_{i}^{(0)}} \| C_{k_{1} i}_{k_{L} k_{L-1}}_ {n,k_{L}}^{(L-1)}_{=2}^{L-1}_{k_{},k_{-1}}^{( -1)}_{k_{1},i}^{(0)}\] (1)

Figure 5: Representational collapse for counting (a, b) and copying (c, d) tasks.

Theorem 5.1 provides intuition on how information propagates in a decoder-only Transformer. In particular, there is a topological aspect present in the bound which is directly controlled by the attention mechanism. More concretely, the sensitivity depends on the sum of the weighted paths between the token \(i\) at the input and the final layer. _In other words, for tokens coming sooner in the sequence, there will be more opportunity for their information to be preserved._ This is clear for instance for the last token, which will only be preserved by attention mechanism if the attention \(n n\) is large at every layer \(L\), i.e. there is only one path. The paths instead grow very quickly for tokens coming sooner in the sequence. A related observation, in terms of path counting, was also made for deep RNNs . We note that such a bound explains the better performance when copying elements at the start of the sequence in Figure 2 (a), why hints help in Figure 2 (b), and why repeating the final elements within the sequence also helps in Figure 2 (c).

This analysis leads to an interesting limiting case described in Proposition 5.2, that shows a type of exponential vanishing that can occur in some degenerate cases in which \(_{n}\) depends only on the starting input token \(_{1}^{(0)}\). Fortunately, there are many mechanisms which prevent this from happening, but believe this to be an interesting limiting case which is a direct consequence of the topology of the causal attention mechanism. Further, it provides an interesting connection between the spectral theory of directed graphs and causal attention mechanisms. We report the formal statement in the Appendix (Proposition B.8).

**Proposition 5.2** (Informal).: _Under certain assumptions on the effect of the normalisation and on the attention weights, in the limit of layers \(L\) the output representation will only depend on the first input token._

U-shape effect.Theorem 5.1 in part also helps to explain the empirically observed _U-shape effect_--the observation that LLMs seem to perform better at retrieval tasks when the information to be retrieved is located either near the start or the end of the sequence. In fact, due to the topology of the causal mechanism, we find from Theorem 5.1 that tokens at the start of the sequence have more opportunity for the information to be maintained at the end. The final tokens being also easier instead can be explained from the recency bias that is learnt by the attention mechanism during training. In auto-regressive next-token prediction, it is in fact reasonable to assume that tokens that are closer to the end will be more important and this is likely a bias that is learnt during training by the LLM.

## 6 Counting

We finally highlight another representational problem that arises specifically in counting problems. Our analysis points to a fundamental difficulty that emerges from the normalisation of the softmax. In particular, the normalisation of the softmax makes it hard for a model to take into account the _length_ of a sequence. This is exacerbated by the fact that positional encodings are often normalised and thus relative, meaning that they also do not hold absolute positional information. Intuitively, counting is a problem that requires some notion of 'unboundedness' of the representations, whilst the normalisations used inside a Transformer work against this.

We start by showing that without causal masking and positional embeddings, a Transformer is immediately unable to count the number of tokens in a sequence, highlighting a pathological issue which stems directly from the softmax normalisation. We note that similar issues have been already pointed out [e.g. 22]. We show the result in Proposition 6.1 and report the full statement in the Appendix (Proposition B.9).

**Proposition 6.1**.: _A Transformer without positional encodings and a causal attention mechanism is immediately unable to count._

While causal mechanisms and positional encodings help to break such representational issues, they break the permutation invariance of the Transformer, meaning that the representations will be heavily miss-aligned with the task, something which has been shown to hinder performance . As permutations grow factorially with sequence length, this makes it practically very challenging for a decoder-only Transformer to learn such a property simply from the data. This explains the extreme incapacity of counting highlighted in Section3. Further, as a corollary of Theorem 4.2, we have that even if a model would be able to generalise perfectly, the problem of representational collapse points to an impossibility result in counting regardless. The result is summarised in Corollary 6.2, with the full statement in the Appendix (Corollary B.10).

**Corollary 6.2** (Informal).: _Counting in certain situations becomes impossible due to representational collapse and finite floating point precision._

Corollary 6.2 shows how our main result on representational collapse points to practical issues when it comes to certain styles of prompts. When paired with low floating point arithmetic precision, representation collapse becomes problematic.

## 7 Conclusion and Future Work

In this work, we first presented surprising failure cases of LLMs on simple copying and counting tasks. We then discussed how such failure cases can be explained by studying what _can_ be contained inside the representation \(_{n}\) and in particular how information may be lost. This lead to the unvealing of two phenomena : representational collapse and over-squashing. We showed how we can measure these phenomena in practice and proposed simple solutions to help alleviate such information loss.

We believe that this work uncovers an interesting framework which can be used to study failure cases of Transformers and LLMs more generally. We believe that our analysis could be extended in many practical different directions, for instance by understanding how to directly measure over-squashing or how to best use this newly-found understanding to improve current Transformer models. In our work, we focused on pointing out information-propagation issues in Transformer-based architectures, but we hope that the findings may help better understand and improve language models available today.

## 8 Acknowledgements

We would like to thank Wojciech Marian Czarnecki (Google DeepMind), Simon Osindero (Google DeepMind), and Timothy Nguyen (Google DeepMind) for the valuable comments and suggestions regarding this work. We also thank Constantin Kogler (University of Oxford) for providing useful insights on the theoretical aspects of the work.