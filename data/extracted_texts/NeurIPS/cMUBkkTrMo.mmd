# Variational Imbalanced Regression: Fair Uncertainty Quantification via Probabilistic Smoothing

Ziyan Wang

Georgia Institute of Technology

wzy@gatech.edu

&Hao Wang

Rutgers University

hw488@cs.rutgers.edu

###### Abstract

Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets show that our VIR can outperform state-of-the-art imbalanced regression models in terms of both accuracy and uncertainty estimation. Code will soon be available at https://github.com/Wang-ML-Lab/variational-imbalanced-regression.

## 1 Introduction

Deep regression models are currently the state of the art in making predictions in a continuous label space and have a wide range of successful applications in computer vision , natural language processing , healthcare , recommender systems , etc. However, these models fail however when the label distribution in training data is imbalanced. For example, in visual age estimation , where a model infers the age of a person given her visual appearance, models are typically trained on imbalanced datasets with overwhelmingly more images of younger adults, leading to poor regression accuracy for images of children or elderly people . Such unreliability in imbalanced regression settings motivates the need for both _improving performance for the minority_ in the presence of imbalanced data and, more importantly, _providing reasonable uncertainty estimation_ to inform practitioners on how reliable the predictions are (especially for the minority where accuracy is lower).

Existing methods for deep imbalanced regression (DIR) only focus on improving the accuracy of deep regression models by smoothing the label distribution and reweighting data with different labels . On the other hand, methods that provide uncertainty estimation for deep regression models operates under the balance-data assumption and therefore do not work well in the imbalanced setting .

To simultaneously cover these two desiderata, we propose a probabilistic deep imbalanced regression model, dubbed variational imbalanced regression (VIR). Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR assumes Neighboring and Identically Distributed (N.I.D.) and borrowsdata with similar regression labels to compute the latent representation's variational distribution. Specifically, VIR first encodes a data point into a probabilistic representation and then mix it with neighboring representations (i.e., representations from data with similar regression labels) to produce its final probabilistic representation; VIR is therefore particularly useful for minority data as it can borrow probabilistic representations from data with similar labels (and naturally weigh them using our probabilistic model) to counteract data sparsity. Furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire Normal Inverse Gamma (NIG) distributions and modulates the associated conjugate distributions by the importance weight computed from the smoothed label distribution to impose probabilistic reweighting on the imbalanced data. This allows the negative log likelihood to naturally put more focus on the minority data, thereby balancing the accuracy for data with different regression labels. Our VIR framework is compatible with any deep regression models and can be trained end to end.

We summarize our contributions as below:

* We identify the problem of probabilistic deep imbalanced regression as well as two desiderata, balanced accuracy and uncertainty estimation, for the problem.
* We propose VIR to simultaneously cover these two desiderata and achieve state-of-the-art performance compared to existing methods.
* As a byproduct, we also provide strong baselines for benchmarking high-quality uncertainty estimation and promising prediction performance on imbalanced datasets.

## 2 Related Work

Variational Autoencoder.Variational autoencoder (VAE)  is an unsupervised learning model that aims to infer probabilistic representations from data. However, as shown in Figure 1, VAE typically assumes I.I.D. representations, where a data point's representation is not directly affected by other data points. In contrast, our VIR borrows data with similar regression labels to compute the latent representation's variational distribution.

Imbalanced Regression.Imbalanced regression is under-explored in the machine learning community. Most existing methods for imbalanced regression are direct extensions of the SMOTE algorithm , a commonly used algorithm for imbalanced classification, where data from the minority classes is over-sampled. These algorithms usually synthesize augmented data for the minority regression labels by either interpolating both inputs and labels  or adding Gaussian noise [5; 6] (more discussion on augmentation-based methods in the Appendix).

Such algorithms fail to measure the distance in continuous label space and fall short in handling high-dimensional data (e.g., images and text). Recently, DIR  addresses these issues by applying kernel density estimation to smooth and reweight data on the continuous label distribution, achieving state-of-the-art performance. However, DIR only focuses on improving the accuracy, especially for the data with minority labels, and therefore does not provide uncertainty estimation, which is crucial to assess the predictions' reliability.  focuses on re-balancing the mean squared error (MSE) loss for imbalanced regression, and  introduces ranking similarity for improving deep imbalanced regression. In contrast, our VIR provides a principled probabilistic approach to simultaneously achieve these two desiderata, not only improving upon DIR in terms of performance but also producing reasonable uncertainty estimation as a much-needed byproduct to assess model reliability. There is also related work on imbalanced classification , which is related to our work but focusing on classification rather than regression.

Figure 1: Comparing inference networks of typical VAE  and our VIR. In VAE (**left**), a data point’s latent representation (i.e., **z**) is affected only by itself, while in VIR (**right**), neighbors participate to modulate the final representation.

**Uncertainty Estimation in Regression.** There has been renewed interest in uncertainty estimation in the context of deep regression models [1; 12; 18; 23; 26; 29; 36; 37; 38; 51]. Most existing methods directly predict the variance of the output distribution as the estimated uncertainty [1; 23; 52], rely on post-hoc confidence interval calibration [26; 37; 51], or using Bayesian neural networks [44; 46; 47]; there are also training-free approaches, such as Infer Noise and Infer Dropout , which produce multiple predictions from different perturbed neurons and compute their variance as uncertainty estimation. Closest to our work is Deep Evidential Regression (DER) , which attempts to estimate both aleatoric and epistemic uncertainty [20; 23] on regression tasks by training the neural networks to directly infer the parameters of the evidential distribution, thereby producing uncertainty measures. DER  is designed for the data-rich regime and therefore fails to reasonably estimate the uncertainty if the data is imbalanced; for data with minority labels, DER  tends produce unstable distribution parameters, leading to poor uncertainty estimation (as shown in Sec. 5). In contrast, our proposed VIR explicitly handles data imbalance in the continuous label space to avoid such instability; VIR does so by modulating both the representations and the output conjugate distribution parameters according to the imbalanced label distribution, allowing training/inference to proceed as if the data is balance and leading to better performance as well as uncertainty estimation (as shown in Sec. 5).

## 3 Method

In this section we introduce the notation and problem setting, provide an overview of our VIR, and then describe details on each of VIR's key components.

### Notation and Problem Setting

Assuming an imbalanced dataset in continuous space \(\{_{i},y_{i}\}_{i=1}^{N}\) where \(N\) is the total number of data points, \(_{i}^{d}\) is the input, and \(y_{i}\) is the corresponding label from a continuous label space \(\). In practice, \(\) is partitioned into B equal-interval bins \([y^{(0)},y^{(1)}),[y^{(1)},y^{(2)}),...,[y^{(B-1)},y^{(B)})\), with slight notation overload. To directly compare with baselines, we use the same grouping index for target value \(b\) as in . We denote representations as \(_{i}\), and use \((}_{i}^{},}_{i}^{})=q_{}( |_{i})\) to denote the probabilistic representations for input \(_{i}\) generated by a probabilistic encoder parameterized by \(\); furthermore, we denote \(}\) as the mean of representation \(_{i}\) in each bin, i.e., \(}=}_{i=1}^{N_{b}}_{i}\) for a bin with \(N_{b}\) data points. Similarly we use \((_{i},_{i})\) to denote the mean and variance of the predictive distribution generated by a probabilistic predictor \(p_{}(y_{i}|)\).

### Method Overview

In order to achieve both desiderata in probabilistic deep imbalanced regression (i.e., performance improvement and uncertainty estimation), our proposed variational imbalanced regression (VIR) operates on both the encoder \(q_{}(_{i}|\{_{i}\}_{i=1}^{N})\) and the predictor \(p_{}(y_{i}|_{i})\).

Typical VAE  lower-bounds input \(_{i}\)'s marginal likelihood; in contrast, VIR lower-bounds the marginal likelihood of input \(_{i}\) and labels \(y_{i}\):

\[ p_{}(_{i},y_{i})=_{}q_{ }(_{i}|\{_{i}\}_{i=1}^{N})||p_{}(_{i}| _{i},y_{i})+(,;_{i},y_{i}).\]

Note that our variational distribution \(q_{}(_{i}|\{_{i}\}_{i=1}^{N})\) (1) does not condition on labels \(y_{i}\), since the task is to predict \(y_{i}\) and (2) conditions on all (neighboring) inputs \(\{_{i}\}_{i=1}^{N}\) rather than just \(_{i}\). The second term \((,;_{i},y_{i})\) is VIR's evidence lower bound (ELBO), which is defined as:

\[(,;_{i},y_{i})=_{q}  p_{}(_{i}|_{i})}_{_{i}^{ }}+_{q} p_{}(y_{i}| _{i})}_{_{i}^{}}-_{ }(q_{}(_{i}|\{_{i}\}_{i=1}^{N})||p_{ }(_{i}))}_{_{i}^{}}.\] (1)

Figure 2: Overview of our VIR method. **Left:** The inference model infers the latent representations given input \(\)’s in the neighborhood. **Right:** The generative model reconstructs the input and predicts the label distribution (including the associated uncertainty) given the latent representation.

where the \(p_{}(_{i})\) is the standard Gaussian prior \((,)\), following typical VAE , and the expectation is taken over \(q_{}(_{i}|\{_{i}\}_{i=1}^{N})\), which infers \(_{i}\) by borrowing data with similar regression labels to produce the balanced probabilistic representations, which is beneficial especially for the minority (see Sec. 3.3 for details).

Different from typical regression models which produce only point estimates for \(y_{i}\), our VIR's predictor, \(p_{}(y_{i}|_{i})\), directly produces the parameters of the entire NIG distribution for \(y_{i}\) and further imposes probabilistic reweighting on the imbalanced data, thereby producing balanced predictive distributions (more details in Sec. 3.4).

### Constructing \(q(_{i}|\{_{i}\}_{i=1}^{N})\)

To cover both desiderata, one needs to (1) produce _balanced_ representations to improve performance for the data with minority labels and (2) produce _probabilistic_ representations to naturally obtain reasonable uncertainty estimation for each model prediction. To learn such _balanced probabilistic representations_, we construct the encoder of our VIR (i.e., \(q_{}(_{i}|\{_{i}\}_{i=1}^{N})\)) by (1) first encoding a data point into a **probabilistic representation**, (2) computing **probabilistic statistics** from neighboring representations (i.e., representations from data with similar regression labels), and (3) producing the final representations via **probabilistic whitening and recoloring** using the obtained statistics.

**Intuition on Using Probabilistic Representation.** DIR uses deterministic representations, with one vector as the final representation for each data point. In contrast, our VIR uses probabilistic representations, with one vector as the mean of the representation and another vector as the variance of the representation. Such dual representation is more robust to noise and therefore leads to better prediction performance. Therefore, We first encode each data point into a probabilistic representation. Note that this is in contrast to existing work  that uses deterministic representations. We assume that each encoding \(_{i}\) is a Gaussian distribution with parameters \(\{_{i}^{},_{i}^{}\}\), which are generated from the last layer in the deep neural network.

**From I.I.D. to Neighboring and Identically Distributed (N.I.D.).** Typical VAE  is an unsupervised learning model that aims to learn a variational representation from latent space to reconstruct the original inputs under the I.I.D. assumption; that is, in VAE, the latent value (i.e., \(_{i}\)) is generated from its own input \(_{i}\). This I.I.D. assumption works well for data with majority labels, but significantly harms performance for data with minority labels. To address this problem, we replace the I.I.D. assumption with the N.I.D. assumption; specifically, VIR's variational latent representations still follow Gaussian distributions (i.e., \((_{i}^{},_{i}^{})\), but these distributions will be first calibrated using data with neighboring labels. For a data point \((_{i},y_{i})\) where \(y_{i}\) is in the \(b^{}\)th bin, i.e., \(y_{i}[y^{(b-1)},y^{(b)})\), we compute \(q(_{i}|\{_{i}\}_{i=1}^{N})(_{i};}_{i}^{},}_{i}^{})\) with the following four steps.

**(1)** Mean and Covariance of Initial \(_{i}\):

\[_{i}^{},_{i}^{}=(_{i}),\]

**(2)** Statistics of Bin \(b\)'s Statistics:

\[_{b}^{},}_{b}^{},}_{b}^{},}_{b}^{}= (\{_{b}^{},_{b}^{},_ {b}^{},_{b}^{}\}_{b=1}^{N}),\]

**(3)** Smoothed Statistics of Bin \(b\)'s Statistics:

\[}_{b}^{},}_{b}^{}, }_{b}^{},}_{b}^{}= (\{_{b}^{},_{b}^{}, _{b}^{},_{b}^{}\}_{b=1}^{N}),\]

**(4)** Mean and Covariance of Final \(_{i}\):

\[}_{i}^{},}_{i}^{}= (_{i}^{},_{i}^{},_{b}^{ },_{b}^{},_{b}^{},}_{b}^{},}_{b}^{},}_{b}^{},}_{b}^{},}_{b}^{},}_{b}^{}),\]

where the details of functions \(()\), \(()\), \(()\), and \(()\) are described below.

**(1) Function \(()\): From Deterministic to Probabilistic Statistics.** Different from deterministic statistics in , our VIR's encoder uses _probabilistic statistics_, i.e., _statistics of statistics_. Specifically, VIR treats \(_{i}\) as a distribution with the mean and covariance \((_{i}^{},_{i}^{})=(_{i})\) rather than a deterministic vector.

As a result, all the deterministic statistics for bin \(b\), \(_{b}\), \(_{b}\), \(}_{b}\), and \(}_{b}\) are replaced by distributions with the means and covariances, \((_{b}^{},_{b}^{})\), \((_{b}^{},_{b}^{})\), \((}_{b}^{},}_{b}^{})\), and \((}_{b}^{},}_{b}^{})\), respectively (more details in the following three paragraphs on \(()\), \(()\), and \(()\)).

**(2) Function \(()\): Statistics of the Current Bin \(b\)'s Statistics.** In VIR, the _deterministic overall mean_ for bin \(b\) (with \(N_{b}\) data points), \(_{b}=}=}_{i=1}^{N_{b}}_{i}\), becomes the _probabilistic overall mean_, i.e., a distribution of \(_{b}\) with the mean \(_{b}^{}\) and covariance \(_{b}^{}\) (assuming diagonal covariance) as follows:

\[_{b}^{}[}]=} _{i=1}^{N_{b}}[_{i}]=} _{i=1}^{N_{b}}_{i}^{},\] \[_{b}^{}[}]= {1}{N_{b}^{}}_{i=1}^{N_{b}}[_{i}]= ^{}}_{i=1}^{N_{b}}_{i}^{}.\]

Similarly, the _deterministic overall covariance_ for bin \(b\), \(_{b}=}_{i=1}^{N_{b}}(_{i}- {})^{2}\), becomes the _probabilistic overall covariance_, i.e., a matrix-variate distribution  with the mean:

\[_{b}^{}[_{b}]=} _{i=1}^{N_{b}}[(_{i}-})^{2}] =}_{i=1}^{N_{b}}_{i}^{}+( _{i}^{})^{2}-_{b}^{}]_{i}+([_{b}^{}]_{i})^{2},\]

since \([}]=_{b}^{}\) and \([}]=_{b}^{}\). Note that the covariance of \(_{b}\), i.e., \(_{b}^{}[_{b}]\), involves computing the fourth-order moments, which is computationally prohibitive. Therefore in practice, we directly set \(_{b}^{}\) to zero for simplicity; empirically we observe that such simplified treatment already achieves promising performance improvement upon the state of the art. More discussions on the idea of the hierarchical structure of the statistics of statistics for smoothing are in the Appendix.

**(3) Function \(()\): Neighboring Data and Smoothed Statistics.** Next, we can borrow data from neighboring label bins \(b^{}\) to compute the smoothed statistics of the current bin \(b\) by applying a symmetric kernel \(k(,)\) (e.g., Gaussian, Laplacian, and Triangular kernels). Specifically, the _probabilistic smoothed mean and covariance_ are (assuming diagonal covariance):

\[}_{b}^{}=_{b^{}}k(y_{b },y_{b^{}})_{b^{}}^{},}_{b}^{ }=_{b^{}}k^{2}(y_{b},y_{b^{}}) _{b^{}}^{},}_{b}^{}= _{b^{}}k(y_{b},y_{b^{}})_{b^{ }}.\]

**(4) Function \(()\): Probabilistic Whitening and Recoloring.** We develop a probabilistic version of the whitening and re-coloring procedure in [39; 49]. Specifically, we produce the final probabilistic representation \(\{}_{i}^{},}_{i}^{}\}\) for each data point as:

\[}_{i}^{}=(_{i}^{}-_{b}^{}) }_{b}^{}}{}_{ b}^{}}}+}_{b}^{},}_{i}^{ }=(_{i}^{}+_{b}^{})}_{b}^{}}{}_{b}^{}}}+ {}_{b}^{}.\] (2)

During training, we keep updating the probabilistic overall statistics, \(\{_{b}^{},_{b}^{},_{b}^{}\}\), and the probabilistic smoothed statistics, \(\{}_{b}^{},}_{b}^{},}_{b}^{}\}\), across different epochs. The probabilistic representation \(\{}_{i}^{},}_{i}^{}\}\) are then re-parameterized  into the final representation \(_{i}\), and passed into the final layer (discussed in Sec. 3.4) to generate the prediction and uncertainty estimation. Note that the computation of statistics from multiple \(\)'s is only needed during training. During testing, VIR directly uses these statistics and therefore does not need to re-compute them.

### Constructing \(p(y_{i}|_{i})\)

Our VIR's predictor \(p(y_{i}|_{i})(y_{i};_{i},_{i})\) predicts both the mean and variance for \(y_{i}\) by first predicting the NIG distribution and then marginalizing out the latent variables. It is motivated by the following observations on label distribution smoothing (LDS) in  and deep evidential regression (DER) in , as well as intuitions on effective counts in conjugate distributions.

**LDS's Limitations in Our Probabilistic Imbalanced Regression Setting.** The motivation of LDS  is that the empirical label distribution can not reflect the real label distribution in an imbalanced dataset with a continuous label space; consequently, reweighting methods for imbalanced regression fail due to these inaccurate label densities. By applying a smoothing kernel on the empirical label distribution, LDS tries to recover the effective label distribution, with which reweighting methods can obtain 'better' weights to improve imbalanced regression. However, in our probabilistic imbalanced regression, one needs to consider both (1) prediction accuracy for the data with minority labels and (2) uncertainty estimation for each model. Unfortunately, LDS only focuses on improving the accuracy, especially for the data with minority labels, and therefore does not provide uncertainty estimation, which is crucial to assess the predictions' reliability.

**DER's Limitations in Our Probabilistic Imbalanced Regression Setting.** In DER , the predicted labels with their corresponding uncertainties are represented by the approximate posterior parameters \((,,,)\) of the NIG distribution \(NIG(,,,)\). A DER model is trained via minimizing the negative log-likelihood (NLL) of a Student-t distribution:

\[_{i}^{DER}=()+(+) ((y_{i}-)^{2}+)-()+()}),\] (3)where \(=2(1+)\). It is therefore nontrivial to properly incorporate a reweighting mechanism into the NLL. One straightforward approach is to directly reweight \(_{i}^{DER}\) for different data points \((_{i},y_{i})\). However, this contradicts the formulation of NIG and often leads to poor performance, as we verify in Sec. 5.

**Intuition of Pseudo-Counts for VIR.** To properly incorporate different reweighting methods, our VIR relies on the intuition of pseudo-counts (pseudo-observations) in conjugate distributions . Assuming Gaussian likelihood, the _conjugate distributions_ would be an NIG distribution , i.e., \((,) NIG(,,,)\), which means:

\[(,/),\;\;\;^{-1}(,),\]

where \(^{-1}(,)\) is an inverse gamma distribution. With an NIG prior distribution \(NIG(_{0},_{0},_{0},_{0})\), the posterior distribution of the NIG after observing \(n\) real data points \(\{u_{i}\}_{i=1}^{n}\) are:

\[_{n}=_{0}+n}{_{n}},_{n}=_{0}+n, _{n}=_{0}+,_{n}=_{0}+(_{0}^{2}_{0})+,\] (4)

where \(=\) and \(=(_{i}u_{i}^{2}-_{n}^{2}_{n})\). Here \(_{0}\) and \(_{0}\) can be interpreted as virtual observations, i.e., _pseudo-counts or pseudo-observations_ that contribute to the posterior distribution. Overall, the mean of posterior distribution above can be interpreted as an estimation from \((2_{0}+n)\) observations, with \(2_{0}\) virtual observations and \(n\) real observations. Similarly, the variance can be interpreted an estimation from \((_{0}+n)\) observations. This intuition is crucial in developing our VIR's predictor.

**From Pseudo-Counts to Balanced Predictive Distributions.** Based on the intuition above, we construct our predictor (i.e., \(p(y_{i}|_{i})\)) by (1) generating the parameters in the posterior distribution of NIG, (2) computing re-weighted parameters by imposing the importance weights obtained from LDS, and (3) producing the final prediction with corresponding uncertainty estimation.

Based on Eqn. 4, we feed the final representation \(\{_{i}\}_{i=1}^{N}\) generated from the Sec. 3.3 (Eqn. 2) into a linear layer to output the intermediate parameters \(n_{i},_{i},_{i}\) for data point \((_{i},y_{i})\):

\[n_{i},_{i},_{i}=(_{i}),_{i} q (_{i}|\{_{i}\}_{i=1}^{N})=(_{i}; }_{i}^{},}_{i}^{})\]

We then apply the importance weights \(_{b^{}}k(y_{b},y_{b^{}})p(y_{b^{}} )^{-1/2}\) calculated from the smoothed label distribution to the _pseudo-count_\(n_{i}\) to produce the re-weighted parameters of posterior distribution of NIG, where \(p(y)\) denotes the marginal distribution of \(y\). Along with the pre-defined prior parameters \((_{0},_{0},_{0},_{0})\), we are able to compute the parameters of posterior distribution \(NIG(_{i},_{i},_{i},_{i})\) for \((_{i},y_{i})\):

\[_{i}^{*} =_{0}+_{b^{}}k(y_{b},y_{b^{}})p(y_{b^{}})^{-1/2} n_{i}_{i}}{ _{n}^{*}}, _{i}^{*}=_{0}+_{b^{}}k(y_{b},y_{b^{ }})p(y_{b^{}})^{-1/2} n_{i},\] \[_{i}^{*} =_{0}+_{b^{}}k(y_{b},y_{b^{ }})p(y_{b^{}})^{-1/2}}{2}, _{i}^{*}=_{0}+(_{0}^{2}_{0})+_{i}.\]

Based on the NIG posterior distribution, we can then compute final prediction and uncertainty estimation as

\[_{i}=_{i}^{*},\;\;\;_{i}=^{*}}{ _{i}^{*}(_{i}^{*}-1)}.\]

We use an objective function similar to Eqn. 3, but with different definitions of \((,,,)\), to optimize our VIR model:

\[_{i}^{}=_{q_{0}(_{i}|\{_{i }\}_{i=1}^{N})}[(^{*}})+(_{i}^{*}+ )((y_{i}-_{i}^{*})^{2}_{n}^{*}+)-_{i}^{*} (_{i}^{*})+(^{*})}{(_{i}^{*}+ )})],\] (5)

where \(_{i}^{*}=2_{i}^{*}(1+_{i}^{*})\). Note that \(_{i}^{}\) is part of the ELBO in Eqn. 1. Similar to , we use an additional regularization term to achieve better accuracy :

\[_{i}^{}=(+2)|y_{i}-_{i}|.\]

\(_{i}^{}\) and \(_{i}^{}\) together constitute the objective function for learning the predictor \(p(_{i}|_{i})\).

### Final Objective Function

Putting together Sec. 3.3 and Sec. 3.4, our final objective function (to minimize) for VIR is:

\[^{}=_{i=1}^{N}_{i}^{},_{i}^{}=_{i}^{}- (,;_{i},y_{i})=_{i}^{}-_{i}^{}-_{i}^{}+_{i}^{ },\]

where \((,;_{i},y_{i})=_{i}^{}+ _{i}^{}-_{i}^{}\) is the ELBO in Eqn. 1. \(\) adjusts the importance of the additional regularizer and the ELBO, and thus lead to a better result both on accuracy and uncertainty estimation.

## 4 Theory

**Notation.** As mentioned in Sec. 3.1, we partitioned \(\{Y_{j}\}_{j=1}^{N}\) into \(||\) equal-interval bins (denote the set of bins as \(\)), and \(\{Y_{j}\}_{j=1}^{N}\) are sampled from the label space \(\). In addition, We use the binary set \(\{P_{i}\}_{i=1}^{||}\) to represent the label distribution (frequency) for each bin \(i\), i.e., \(P_{i}(Y_{j}_{i})\). We also use the binary set \(\{O_{i}\}_{j=1}^{N}\) to represent whether the data point \((_{j},y_{j})\) is observed (i.e., \(O_{j}(O_{j}=1) P_{B(Y_{j})}\), and \(_{O}[O_{j}] P_{B(Y_{j})}\)), where \(B(Y_{j})\) represents the bin which \((x_{j},y_{j})\) belongs to. For each bin \(i\), we denote the associated set of data points as

\[_{i}\{j:Y_{j}=i\}.\]

When the imbalanced dataset is partially observed, we denote the observation set as:

\[_{i}\{j:O_{j}=1~{}~{}\&~{}~{}B(Y_{j})=i\}.\]

**Definition 4.1** (**Expectation over Observability \(_{O}\)**).: _We define the expectation over the observability variable \(O\) as \(_{O}[]_{O_{j}(O_{j}=1)}[]\)._

**Definition 4.2** (**True Risk**).: _Based on the previous definitions, the true risk is defined as:_

\[R()=|}_{i=1}^{||}_{i}|}_{j_{i}}_{j}(Y,),\]

_where \(_{j}(Y,)\) refers to some loss function (e.g. MAE, MSE). In this paper, we assume the loss is upper bounded by \(\), i.e., \(0_{j}(Y,)\)._

Below we define the Naive Estimator.

**Definition 4.3** (**Naive Estimator**).: _Given the observation set, the Naive Estimator is defined as:_

\[_{}()=^{||}| _{i}|}_{i=1}^{||}~{}_{j_{i}} _{j}(Y,).\]

   Metrics &  &  \\  Shot & all & many & medium & few & all & many & medium & few \\  VANILA  & 7.77 & 6.62 & 9.55 & 13.67 & 5.05 & 4.23 & 7.01 & 10.75 \\ VANIL  & 7.63 & 6.58 & 9.21 & 13.45 & 48.16 & 6.61 & 10.24 \\ Deep Ens.  & 7.33 & 6.62 & 9.37 & 13.09 & 48.7 & 47.37 & 6.91 & 11.15 \\ Inte Noise  & 8.53 & 7.62 & 9.13 & 13.82 & 5.57 & 49.5 & 6.88 & 10.86 \\ Smooth  & 8.16 & 7.39 & 8.65 & 12.82 & 45.62 & 5.69 & 8.49 \\ SMOON  & 8.26 & 7.64 & 9.01 & 6.50 & 4.99 & 6.14 & 8.44 \\ SMOON  & 7.81 & 7.16 & 8.90 & 12.19 & 4.99 & 4.57 & 5.73 & 7.77 \\ DER  & 8.09 & 7.31 & 8.99 & 12.66 & 4.99 & 4.69 & 8.40 & 10.49 \\ LDS  & 7.67 & 6.98 & 8.86 & 10.89 & 4.85 & 4.39 & 5.8 & 7.45 \\ GPS  & 7.69 & 7.10 & 8.86 & 9.98 & 4.84 & 4.91 & 5.97 & 6.29 \\ LDS + FDS  & 7.55 & 7.01 & 8.24 & 10.99 & 4.22 & 4.36 & 5.48 & 6.79 \\ RANSIM  & 7.02 & 6.49 & 7.84 & 6.85 & 4.13 & 5.37 & 6.89 \\ LDS + FDS - DER  & 8.18 & 7.44 & 9.52 & 11.53 & 5.30 & 5.47 & 6.74 & 7.68 \\ VIR (Ours) & **6.99** & **6.39** & **7.47** & **9.51** & **4.41** & **4.97** & **5.08** & **6.23** \\ 
**Ours vs. VANILA** & **4.03** & **4.03** & **4.04** & **4.04** & **4.16** & **4.16** & **4.36** & **4.52** \\
**Ours vs. Traffic Noise  & **4.51** & **4.23** & **4.23** & **4.31** & **4.16** & **4.08** & **4.13** & **4.53** \\
**Ours vs. DER**  & **4.19** & **4.21** & **4.25** & **4.31** & **4.16** & **4.08** & **4.13** & **4.26** \\
**Ours vs. DER**  & **4.19** & **4.52** & **4.31** & **4.16** & **4.08** & **4.13** & **4.26** & **4.35** \\
**Ours vs. DER**  & **4.25** & **4.31** & **4.17** & **4.12** & **4.06** & **4.32** & **4.46** & **4.14** \\
**Ours vs. DER**  & **4.32** & **4.31** & **4.32** & **4.17** & **4.12** & **4.06** & **4.32** & **4.46** \\
**Ours vs. RANSIM  & **4.03** & **4.10** & **4.37** & **4.17** & **4.12** & **4.06** & **4.32** & **4.46** \\   

Table 1: Accuracy on ZOB-DIR.

   Metrics &  &  \\  Shot & all & many & medium & few & all & many & medium & few \\  VANILA  & 8.06 & 7.23 & 15.12 & 26.33 & 4.57 & 4.17 & 10.59 & 20.46 \\ VANIL  & 8.04 & 7.20 & 15.06 & 28.30 & 45.7 & 4.22 & 10.66 & 20.72 \\ Deep Ens.  & 8.08 & 7.31 & 5.09 & 26.47 & 4.99 & 4.92 & 10.61 & 21.13 \\ Deep Noise  & 8.11 & 7.36 & 15.23 & 26.29 & 4.68 & 4.33 & 10.65 & 20.31 \\ Smooth  & 8.14 & 7.44 & 24.15 & 21.58 & 24.64 & 4.30 & 9.05 & 19.46 \\ SMOON  & 8.03 & 7.

It is easy to verify that the expectation of this naive estimator is not equal to the true risk, as \(_{O}[_{}()] R()\).

Considering an imbalanced dataset as a subset of observations from a balanced one, we contrast it with the Inverse Propensity Score (IPS) estimator .

**Definition 4.4** (**Inverse Propensity Score Estimator**).: _The inverse propensity score (IPS) estimator (an unbiased estimator) is defined as_

\[_{}(|P)=|}_{i=1}^{| |}_{i}|}_{j_{i}}(Y,)}{P_{i}}.\]

The IPS estimator is an unbiased estimator, as we can verify by taking the expectation value over the observation set:

\[_{O}[_{}(|P)] =|}_{i=1}^{||}_{i}|}_{j_{i}}(Y,)}{P _{i}}_{O}[O_{j}]\] \[=|}_{i=1}^{||}_{i}|}_{j_{i}}_{j}(Y,)=R( ).\]

Finally, we define our VIR/DIR estimator below.

**Definition 4.5** (**VIR Estimator**).: _The VIR estimator, denoted by \(_{}(|)\), is defined as:_

\[_{}(|)=|} _{i=1}^{||}_{i}|}_{j_{i} }(Y,)}{_{i}},\] (6)

_where \(\{_{i}\}_{i=1}^{||}\) represents the smoothed label distribution used in our VIR's objective function. It is important to note that our VIR estimator is biased._

For multiple predictions, we select the "best" estimator according to the following definition.

**Definition 4.6** (**Empirical Risk Minimizer**).: _For a given hypothesis space \(\) of predictions \(\), the Empirical Risk Minimization (ERM) identifies the prediction \(\) as_

\[^{}=_{} _{}(|)}\]

With all the aforementioned definitions, we can derive the generalization bound for the VIR estimator.

**Theorem 4.1** (Generalization Bound of VIR).: _In imbalanced regression with bins \(\), for any finite hypothesis space of predictions \(=\{_{1},,_{}\}\), the transductive prediction error of the empirical risk minimizer \(^{ERM}\) using the VIR estimator with estimated propensities \(\) (\(P_{i}>0\)) and given training observations \(O\) from \(\) with independent Bernoulli propensities \(P\), is bounded by:_

\[R(^{ERM})_{}(^{ERM}|)+|}_{i=1}^{||}|1- }{_{i}}|}_{}}+|}|/)}{2}}^{||}_{i}^{2}}}}_{}.\] (7)

**Remark.** The naive estimator (i.e., Definition 4.3) has large bias and large variance. If one directly uses the original label distribution in the training objective (i.e., Definition 4.4), i.e., \(_{i}=P_{i}\), the "bias" term will be \(0\). However, the "variance" term will be extremely large for minority data because \(_{i}\) is very close to \(0\). In contrast, under VIR's N.I.D., \(_{i}\) used in the training objective function will be smoothed. Therefore, the minority data's label density \(_{i}\) will be smoothed out by its neighbors and becomes larger (compared to the original \(P_{i}\)), leading to smaller "variance" in the generalization error bound. Note that \(_{i} P_{i}\), VIR (with N.I.D.) essentially increases bias, but **significantly reduces** its variance in the imbalanced setting, thereby leading to a lower generalization error.

[MISSING_PAGE_FAIL:9]

upon the state-of-the-art method RankSim by \(9.6\%\) and \(7.9\%\) on AgeDB-DIR and IMDB-WIK-DIR, respectively, in terms of few-shot GM. This verifies the effectiveness of our methods in terms of overall performance. More accuracy results on different metrics are included in the Appendix. Besides the main results, we also include ablation studies for VIR in the Appendix, showing the effectiveness of VIR's encoder and predictor.

### Imbalanced Regression Uncertainty Estimation

Different from DIR  which only focuses on accuracy, we create a new benchmark for uncertainty estimation in imbalanced regression. Table 4, Table 5, and Table 6 show the results on uncertainty estimation for three datasets AgeDB-DIR, IMDB-WIK-DIR, and STS-B-DIR, respectively. Note that most baselines from Table 1, Table 2, and Table 3 are _deterministic_ methods (as opposed to probabilistic methods like ours) and _cannot provide uncertainty estimation_; therefore they are not applicable here. To show the superiority of our VIR model, we create a strongest baseline by concatenating the DIR variants (LDS + FDS) with the DER .

Results show that our VIR consistently outperforms all baselines across different metrics, especially in the few-shot metrics. Note that our proposed methods mainly focus on the imbalanced setting, and therefore naturally places more emphasis on the few-shot metrics. Notably, on AgeDB-DIR, IMDB-WIKI-DIR, and STS-B-DIR, our VIR improves upon the strongest baselines, by \(14.2\% 17.1\%\) in terms of few-shot AVSE.

### Limitations

Although our methods successfully improve both accuracy and uncertainty estimation on imbalanced regression, there are still several limitations. Exactly computing _variance of the variances_ in Sec. 3.3 is challenging; we therefore resort to fixed variance as an approximation. Developing more accurate and efficient approximations would also be interesting future work.

## 6 Conclusion

We identify the problem of probabilistic deep imbalanced regression, which aims to both improve accuracy and obtain reasonable uncertainty estimation in imbalanced regression. We propose VIR, which can use any deep regression models as backbone networks. VIR borrows data with similar regression labels to produce the probabilistic representations and modulates the conjugate distributions to impose probabilistic reweighting on imbalanced data. Furthermore, we create new benchmarks with strong baselines for uncertainty estimation on imbalanced regression. Experiments show that our methods outperform state-of-the-art imbalanced regression models in terms of both accuracy and uncertainty estimation. Future work may include (1) improving VIR by better approximating _variance of the variances_ in probability distributions, and (2) developing novel approaches that can achieve stable performance even on imbalanced data with limited sample size, and (3) exploring techniques such as mixture density networks  to enable multi-modality in the latent distribution, thereby further improving the performance.

   Metrics &  &  \\  Shot & All & Many & Medium & Few & All & Many & Medium & Few \\  Deep Exs.  & 3.913 & 3.911 & 4.223 & 4.106 & 0.709 & 0.621 & 0.676 & 0.663 \\ DIRR Jonge  & 3.748 & 3.753 & 3.753 & 3.568 & 0.673 & 0.631 & 0.644 & 0.639 \\ DER  & 2.667 & 2.661 & 3.013 & 2.401 & 0.682 & 0.583 & 0.613 & 0.624 \\ LDS + FDS + DER  & 2.561 & 2.514 & 2.880 & 2.398 & 0.672 & 0.581 & 0.609 & 0.615 \\
**VIR (Overs)** & **1.996** & **1.810** & **2.754** & **2.152** & **0.591** & **0.575** & **0.602** & **0.510** \\  
**Overs** \(\) DER & **4.671** & **4.791** & **4.259** & **4.204** & **4.091** & **4.098** & **4.091** & **4.011** \\
**Overs** \(\) LDS + FDS + DER & **4.556** & **4.704** & **4.125** & **4.206** & **4.091** & **4.098** & **4.097** & **4.105** \\   

Table 6: Uncertainty estimation on STS-B-DIR.

   Metrics &  &  \\  Shot & All & Many & Medium & Few & All & Many & Medium & Few \\  Deep Exs.  & 5.219 & 4.122 & 5.823 & 6.824 & 6.082 & 0.764 & 0.743 \\ DERs Note  & 4.162 & 4.143 & 4.243 & 5.338 & 0.262 & 0.732 & 0.671 & 0.746 \\ DER  & 3.595 & 3.699 & 4.997 & 4.957 & 0.541 & 0.631 & 0.630 & 0.634 \\ LDS + FDS + DRR  & 3.872 & 3.897 & 3.951 & 4.224 & 0.681 & 0.640 & 0.599 & 0.643 \\
**VIR (Overs)** & **3.598** & **3.506** & **4.082** & **4.040** & **4.042** & **4.041** & **4.047** \\  
**Overs** \(\) DER & **4.115** & **4.104** & **4.104** & **4.243** & **4.455** & **4.098** & **4.098** & **4.094** \\
**Overs** \(\) LDS + FDS + DER & **4.403** & **4.107** & **4.107** & **4.038** & **4.147** & **4.041** & **4.095** & **4.151** \\   

Table 5: Uncertainty estimation on IW-DIR.