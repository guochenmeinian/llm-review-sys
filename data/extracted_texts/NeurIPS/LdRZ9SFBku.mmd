# Yuyuan Li\({}^{3,4\dagger}\), Chaochao Chen\({}^{4}\), Kecheng Zheng\({}^{2}\), Yujun Shen\({}^{2}\), Deli Zhao\({}^{1}\),

UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training

 Biao Gong\({}^{1}\), Shuai Tan\({}^{2}\), Yutong Feng\({}^{1}\), Xiaoying Xie\({}^{1}\),

**Yuyuan Li\({}^{3,4}\), Chaochao Chen\({}^{4}\), Kecheng Zheng\({}^{2}\), Yujun Shen\({}^{2}\), Deli Zhao\({}^{1}\),**

\({}^{1}\)Alibaba Group, \({}^{2}\)Ant Group, \({}^{3}\)Hangzhou Dianzi University, \({}^{4}\)Zhejiang University

{a.biao.gong, tanshuai2001, fengyutong.fyt}@gmail.com souyu.xxy@alibaba-inc.com y2li@hdu.edu.cn zjuccc@zju.edu.cn {zkechengzk, shenyujun0302, zhaodeli}@gmail.com

###### Abstract

This work presents a unified knowledge protocol, called _UKnow_, which facilitates knowledge-based studies from the perspective of data. Particularly focusing on visual and linguistic modalities, we categorize data knowledge into five unit types, namely, in-image, in-text, cross-image, cross-text, and image-text, and set up an efficient pipeline to help construct the multimodal knowledge graph from any data collection. Thanks to the logical information naturally contained in knowledge graph, organizing datasets under _UKnow_ format opens up more possibilities of data usage compared to the commonly used image-text pairs. Following _UKnow_ protocol, we collect, from public international news, a large-scale multimodal knowledge graph dataset that consists of 1,388,568 nodes (with 571,791 vision-related ones) and 3,673,817 triplets. The dataset is also annotated with rich event tags, including 11 coarse labels and 9,185 fine labels. Experiments on 4 benchmarks demonstrate the potential of _UKnow_ in supporting common-sense reasoning and boosting vision-language pre-training with a single dataset, benefiting from its unified form of knowledge organization. See Appendix A to download the dataset.

+
Footnote †: dagger\) Corresponding Author.

## 1 Introduction

Recent efforts have been attracted to leverage the _multimodal knowledge graph_ for data-driven intelligence. Inspired by the human mastery knowledge network , we consider that the multimodal knowledge graph, which naturally accommodates heterogeneous data based on its format of complex network [93; 77], is well suited for constructing a unified knowledge criterion from the perspective of data. Driven by the multimodal knowledge graph, models can easily introduce external knowledge , discover long-range relations  and understand more logical semantics . However, existing datasets of the multimodal knowledge graph commonly focus on only one task like common-sense reasoning [81; 46] due to their limited scale and irregular data organization. Therefore, it is imperative to construct a well-organized multimodal knowledge graph dataset with large-scale and rich-logic, which enables delving into deeper foundational problems in lower layers, such as the knowledge based vision-language pre-training.

To this end, we propose _UKnow_, a **U**nified **K**nowledge protocol, which facilitates knowledge-based studies from data perspective. Particularly focusing on visual and linguistic modalities, we categorize data knowledge into five unit types, namely, in-image \(I_{in}\), in-text \(T_{in}\), cross-image \(I_{cross}\), cross-text \(T_{cross}\), and image-text \(IT_{cross}\). As shown in Fig. 1, these knowledge types are together named as _Knowledge-View_ which can be easily used to construct a multimodal knowledge graph (\(_{m}\)).

To verify that _UKnow_ can serve as a standard protocol, we further set up an efficient data processing pipeline, consisting of _Phase-1/2/3_, to reorganize existing datasets into _UKnow_'s format. Please note that, this pipeline is also able to automatically extend an existing image-text dataset like LAION-5B  with more useful information to build a new dataset. A brief description of each _Phase_ is as follows:

_Phase-1: Content Extraction_. We use pre-trained models to preprocess data and extract useful content. Note that pre-trained models can be replaced / added / disabled freely as needed.

_Phase-2: Information Symbolization_. Since the results obtained in _Phase-1_ (_e.g._, images and texts) cannot be used directly for graph construction, we adopt information symbolization strategy to arrange all of them into the index in this phase. This information symbolization strategy numbers all original or generated data by a certain rule, which links the nodes from _Phase-1_ to make a multimodal graph. _Phase-3: Knowledge Construction_. Two kinds of internal knowledge (\(I_{in},T_{in}\)) and three kinds of associative knowledge (\(I_{cross},T_{cross},IT_{cross}\)) are aggregated into one graph (\(_{m}\)) in this phase as shown in Fig. 1.

Following _UKnow_ protocol and above pipeline, we build a novel large-scale multimodal knowledge graph. Considering that a large-scale event dataset is of practical significance for real-world applications, such as information retrieval and public sentiment analysis, our data are collected from public international news. Overall, our dataset contains 1,388,568 nodes of which 571,791 are vision relevant (_i.e._, news images or visual objects). The number of triples in the entire graph is 3,673,817. To the best of our knowledge, this dataset has become the largest multimodal knowledge graph dataset of international news events. Moreover, to organize data in a more structured way and enhance dataset with more category labels, our dataset introduces a _hierarchical event annotation_ for each news, including _Event-11_ and _Event-9185_. Specifically, the former contains general event categories such as _"Sports, Ceremony,..."_, while the latter consists of real human activity in the history such as _"2019 NBA All-Star Game, 2019 Daytona 500,..."_. More details about the annotation are shown in Sec. 3.2, Fig 3, and Tab. 3.

In summary, our **contributions** are as follows:

* We propose _UKnow_ to introduce the multimodal knowledge graph into the vision field as a new standard of data organization, which features the relation inside data in addition to the original data format. Such a protocol opens up the possibilities of data usage such that more logic-rich downstream tasks can be expected in the future.
* We design an efficient data processing pipeline for constructing dataset following our _UKnow_ protocol, together with a large-scale multimodal knowledge graph dataset collected from public international news. We also equip the dataset with hierarchical event annotations, which can help models understand human activities and history. See Appendix A to download the dataset.
* We provide some examples of the usage of _UKnow_ in practical applications. Experiments on four benchmarks showcase the advantages of _UKnow_ in supporting common-sense reasoning and boosting vision-language pre-training with a unified form of data organization, making it possible to evaluate various tasks on a single dataset.

## 2 Related Work

### Existing Knowledge Representation Formats

In recent years, a growing abundance multi-modal data are disseminated, linking diverse information across various modalities such as text and image in a global data space. This interconnected web of heterogeneous data constitutes a vast repository of information termed as knowledge. With the development of large-scale models, the utilization of knowledge has seen a notable surge in

Figure 1: **Overview of _UKnow_ protocol**, consisting of five unit knowledge types, namely, image \(I_{in}\) (_e.g._, object), in-text \(T_{in}\) (_e.g._, entity), cross-image \(I_{cross}\) (_e.g._, image similarity), cross-text \(T_{cross}\) (_e.g._, text continuity), and image-text \(IT_{cross}\) (_e.g._, description).

exploration. Existing knowledge-based deep learning models are broadly divided into two aspects: (1) external knowledge introduction , (2) internal knowledge mining . The former leverages expert knowledge by introducing external data [44; 28; 4] or pre-trained models [76; 58; 11; 86]. The latter means constructing correlations of training data by similarity [48; 13; 17] or discovering favorable substructures of internal models [32; 7; 33; 78].

However, from the perspective of data organization, existing studies often claim to be knowledge-based only using one piece of them, which is actually incomplete and cannot be analogous to the complex knowledge network held by humans. In this work, we build a unified knowledge protocol based on the multimodal knowledge graph to define the unified knowledge on multimodal data.

### Multimodel Knowledge Graph Datasets

The Multimodal Knowledge Graph (MMKG) serves as a potent means to store and leverage multimodal knowledge explicitly, which bolsters and enhances model performances across diverse domains. In Tab. 1, we list mainstream multimodal knowledge graph datasets [72; 47; 26; 1; 92; 81; 38; 79; 83; 36; 6; 90; 74; 88; 29], constructed by texts and images with detailed information. In terms of data scale, VisualGenome  is a multimodal knowledge graph which contains 40,480 relations, 108,077 image nodes with objects. The ImageGraph  further pushed up the number of image nodes to 829,931 but missing the extraction of visual objects. Recently, VisualSem  implements a multimodal knowledge graph with \(938K\) image nodes and 89,896 entity nodes, but it only uses 15 types of relation to build the graph. On the route of increasing the number of entity nodes, while Multi-OpenEA  boasts 920,000 entity nodes, surpassing prior methods, our endeavor has achieved 1,388,568 nodes, establishing the largest graph thus far. Besides, most of existing multimodal knowledge graphs are more like a vision-similarity-based image library [40; 65] with image descriptions and meta information, it lacks the most valuable feature of the knowledge graph: "The Logical Connection". This logic refers to the additional association between two nodes that were originally unrelated, triggered by a news event involving these two nodes. For example, prior to the news event "Celebrity 1 visits Area 1," there was no relation between Celebrity 1 and the Area 1. The newly added "visit" relation in \(<\)("Celebrity1"), visit, ("Area1")\(>\) tuple exemplifies this logic, which is highly beneficial for downstream tasks.

Generally speaking, the above news refer to international news, which carries the most complex event logic as well as plentiful multimodal information . To completely exploit the advantages of multimodal knowledge graphs, building a dataset using event logic from international news is a natural approach. However, there is not yet a large multimodal knowledge graph of news events. RESIN  is a recently published multimodal knowledge graph containing 24 types of entities, 46 types of relations and 67 types of events. The larger and fresher CLIP-Event  is a event rich dataset with 106,875 images and 187 types of events extracted by a text information extraction system [92; 37]. Actually, CLIP-Event is not a knowledge graph and its definition of "event" is not a news event but an action. In summary, one of goals of our work is to build a large, and realistic news-event rich, multimodal knowledge graph dataset from international news.

**DATASET** & **YEAR** & **ULTIMODAL INFO.** & **SOURCE** & **NODE** & **IMAGE** & **TRIPLE** & **WEB** & **GIT** & **EVENT** \\  W93-MMG-TXT  & 2016 & ENT. & WINLS-ImageNet & 6,535 & 632,428 & 14,397 & & ✓ & ✓ & ✓ \\ ImageGraph  & 2017 & ENT.CONCEPT & FRUS & 14870 & 829,931 & 564,010 & & ✓ & ✓ & ✓ \\ VisualGenome  & 2017 & ENT & MSCOCO & 75,729 & 108,077 & 1,531,488 & ✓ & ✓ & ✓ \\ GAIA  & 2018 & ENT.CONCEPT & FRUS & Greensens & 457,000 & 38,000 & ✓ & ✓ & ✓ \\ MMG-FUS  & 2019 & ENTCONCEPT & FRUS & 184,951 & 1,3444 & 592,213 & ✓ & ✓ & ✓ & ✓ \\ MMG-DBS  & 2019 & ENTCONCEPT & DB13, Search Engine & 14,777 & 12,842 & 90,213 & ✓ & ✓ & ✓ \\ MMG-TAOGIS  & 2019 & ENTCONCEPT & YAGUOLS & Search Engine & 15,283 & 11,192 & 12,886 & ✓ & ✓ & ✓ \\ Heliophas  & 2020 & ENT/Rel.CONCEPT & Wibergts & 29,985 & 2,914,770 & 2,708,511 & ✓ & ✓ & ✓ & ✓ \\ VisualSem  & 2020 & ENT/Rel.CONCEPT & BabelNet & 89,896 & 3990,900 & 1,500,000 & ✓ & ✓ & ✓ \\ RESIN  & 2021 & ENT/Rel.CONCEPT & 51,542 & 6,399 & 150,220 & ✓ & ✓ & ✓ & ✓ \\ MCR-W  & 2022 & ENT.RCL-CONCEPT & Open EA , Search Engine & 15,5000 & 14,463 & & & & ✓ \\ MGG-Y  & 2021 & ENT/Rel.CONCEPT & Open EA, Search Engine & 12,842 & 1,2818 & - & & & ✓ \\  MMG-T  & 2023 & ENT/Rel.CONCEPT & Wildata, Search Engine & 11,292 & 76,764 & 34,420 & ✓ & ✓ & ✓ \\ Multi-OpenEA  & 2023 & ENT.CONCEPT & Open EA, Search Engine & 92,9000 & 2,705,688 & & ✓ & ✓ \\ UMM  & 2023 & ENT/ConCEPT & Dzephas, Multi-OpenEA & 328,203 & 107,671 & 982,626 & & ✓ & ✓ \\ AspectMLG-DBS  & 2023 & ENT/Rel.CONCEPT & Wapleach, Search Engine & 2,330 & 65,655 & 9,685 & & ✓ & ✓ \\ TVAKG  & 2023 & ENT/Rel.CONCEPT & Wapleach, Search Engine & 44,350 & 1,695,688 & 1,382,158 & ✓ & ✓ & ✓ \\ YYYG-C  & 2023 & ENT/ConCEPT & ConvergeNet, WordNet & 4,3267 & 461,307 & 111,491 & & ✓ & ✓ \\  _(Knower)_ & 2024 & ENT/Rel.CONCEPT & News, Wikipedia & **1,388,568** & **1,073,671** & **3,673,817** & ✓ & ✓ & ✓ \\  _(Knoter)_ & 2024 & ENT/Rel.CONCEPT & News, Wikipedia & **1,388,568** & **1,073,671** & **3,673,817** & ✓ & ✓ & ✓ \\ 

Table 1: **Statistics of various multimodal knowledge graph datasets. TRIPLE is the basic component of knowledge graph (Sec. 2.1), WEB and GIT indicate homepage and Github repository respectively. EVENT** indicates the news event.**

### Knowledge-based Downstream Tasks

Thanks to the innovative unified knowledge proposed by our _UKnow_ protocol, our dataset can readily accommodate a variety of downstream tasks. In this study, we opt for common-sense reasoning and vision-language pre-training as experimental domains to validate our dataset. Common-sense reasoning is an extremely popular task in the field of knowledge graph. Since our dataset is based on the knowledge graph, the performance validation on common-sense reasoning is indispensable. Moreover, the representations from Vision-Language Pre-training models are capable of diminishing the necessity for intricate task-specific architectures , which allows the knowledge to further flow into various downstream tasks. By incorporating these two tasks, we are able to maximize the assessment of the dataset's knowledge validity.

**Common-sense Reasoning.** Common-sense reasoning means answering queries by logic permutations. The specific task in this work is the link prediction. Various works [3; 70; 68; 60; 94; 53] achieve reasoning by embedding entities and relations in knowledge graph into low-dimensional vector space. Path-based methods [27; 82; 63; 51] start from anchor entities and determine the answer set by traversing the intermediate entities via relational path. There are also GCN  based methods [61; 16] pass message to iterate graph representation for reasoning.

**Vision-Language Pre-training** Vision-language pre-training (VLP) can be divided into three categories based on how they encode images : OD-based region features [5; 31; 34; 41; 66; 69], CNN-based grid feature [62; 19; 20] and ViT-based patch features [84; 30; 24]. Pre-training objectives are usually: masked language/image modeling (MLM/MIM) [2; 9; 39], image-text matching (ITM) [34; 19; 10], and image-text contrastive learning (ITC) [30; 50; 35].

## 3 UKnow

We commence by introducing the overall architecture of _UKnow_ in Sec. 3.1. Then the detailed exposition of the data collection process for the new dataset and statistics are presented in Sec. 3.2 and Sec. 3.3. In Sec. 4, we lastly provide the guidance to researchers on how to integrate the multimodal knowledge graph and effectively design a UKnow-based model.

Compared to previous libraries-like methods [40; 65] with simple descriptions and meta-information, which lack the logical connection, the most valuable feature of our data processing pipeline is to endow with more logical connections to achieve superior performance in various tasks. As shown in Fig. 2, particularly focusing on visual and linguistic modalities, we categorize data knowledge into five unit types. Then we devise an efficient data processing pipeline to help reorganize existing datasets or create a new one under _UKnow_ format. The construction process of _UKnow_ can be invoked separately for any multimodal data to standardize the knowledge. As shown in Fig. 3, the whole pipeline is mainly empowered by three parts: _content extraction_, _information symbolization_, and _knowledge construction_.

### Construction Pipeline for UKnow Protocol

**Phase-1: Content Extraction.**_Content Extraction_ is used to extract useful information from different fields by pre-trained deep learning models. The pre-processing functions are designed as \(=\{P_{1},P_{2},,P_{k}\}\). Note that \(\) can be replaced / added / disabled freely as needed. We choose pre-trained models with both global descriptions and semantic level granularity: where _Det. / Seg._ and _NER / POS_ refer to _Detection / Segmentation_ and _Named Entity Recognition / Part-of-Speech tagging_. Then we construct the \(N_{p}^{ori}=(I,T)\) (\(I\) is a RGB-image and \(T\) is a text) which contains a wealth of external knowledge. At this stage, all inputs concurrently go through the entire \(\). It also supports the combined use of pre-trained models such as \(P_{6} P_{2}\) (_e.g._, extracting the features of each object detected from the image). The final output of _Content Extraction_ can be formulated as \(N_{p}=Merge(N_{p}^{ori})\). \(Merge\) transforms theoriginal output \(N_{p}^{ori}\) into a K:V dictionary \(N_{p}\). The KEY of \(N_{p}\) are shown in top right corner of Fig. 3 (\(N_{p}\) [_Phase-I_]). \(N_{p}\) is also used as the attribute of each node in the final output multimodal knowledge graph \(_{m}\).

**Phase-2: Information Symbolization.** Since Images and texts cannot be used directly for graph construction, we design the _Phase-2_ to number all original or generated data by a certain rule, then _Phase-3_ links these nodes to make a multimodal graph. _Information Symbolization_ is used to subscript \(N_{p}\) to edge index \(_{e}\) or node index \(_{n}\): (1) The symbolization for edges \(_{e}\) is based on the category or visual / semantic similarity. For example, "\(\) title_title_clip" is a kind of parallelism edge which is constructed by the cosine similarity of clip features of news titles. (2) The symbolization for nodes \(_{n}\) is divided into three levels: [fact, image / text, object / entity]. As shown in Fig. 3, [\(L_{1}\).*] means fact-level which is an abstraction of a piece of news. The real index used in our multimodal knowledge graph would be \(\{L_{1}.0,L_{1}.1,L_{1}.2,...\}\). Similarly, [\(L_{2}\).*] means image / text-level which is the symbolization of images or texts from news, [\(L_{3}\).*] is the object in image or entity in text. The index for all nodes is eventually shuffled, that is, the real index would be \(\{L_{1}.0,L_{2}.1,L_{1}.2,L_{3}.3,L_{3}.4,...\}\).

We provide the clearer explanations about the motivation of Phase-2. As stated in Sec. 3.2, our data are collected from international news, which encompasses a wide variety of text and images. Although Phase-1 preprocesses the data like detection and segmentation, the resulting features are still a huge volume as it contains detailed information extracted from the news. While this detailed information is valuable for constructing a knowledge graph, the computational demands and complexity far exceed available resources. Thus, a common approach in knowledge graph construction is to store data and their relationships as indices, as done in the Phase-2 Information Symbolization stage. This means it has the following benefits: efficiency in storage and retrieval, fast lookup and traversal, uniqueness and consistency, scalability, and simplification of graph operations.

**Phase-3: Knowledge Construction.** We categorize data knowledge into five unit types, namely, in-text (\(T_{in}\)), in-image (\(I_{in}\)), inter-text (\(T_{cross}\)), inter-image (\(I_{cross}\)), and image-text (\(IT_{cross}\)) which are together called _Knowledge-View_ detailed in Fig. 2 and Fig. 2.

In this phase, we aggregate two kinds of internal knowledge (\(I_{in},T_{in}\)) and three kinds of associative knowledge (\(I_{cross},T_{cross},IT_{cross}\)) in one graph \(_{m}\), which are usually introduced independently in previous studies. _Knowledge Construction_ takes as input the edge index \(_{e}\) and node index \(_{n}\) numbered by _Phase-2_ and output the multimodal

 
**Phrase** & **Construction Method** & **View** & **Num.** \\   & Detection Category & \(I_{m}\) & 648,871 \\  & NER Category & \(T_{in}\) & 1,606,936 \\  Phrase-2 \\ Similarity\&Manual Annotation \\ Similarity\&Manual Annotation \\  } & \(I_{Cross}\) & 648,207 \\  & Similarity\&Manual Annotation & \(T_{cross,I_{cross}}\) & 140,133 \\   & Manual Event Annotation & - & 593,670 \\  

Table 2: **Edge (\(_{e}\)) construction and statistics.**

Figure 2: **Detailed data organization under _UKnow_ protocol**, which builds the multimodal (image \(\&\) text) graph \(_{m}\) based on the _Knowledge-View_ (\(I_{in}\), \(T_{in}\), \(I_{cross}\), \(T_{cross}\), and \(IT_{cross}\)). Each node owns up to 22 attributes shown as \(N_{p}\) in Fig. 3.**

knowledge graph \(_{m}\) (Fig. 2(c)). Since \(_{e}\) and \(_{n}\) are both isolated, we use four kinds of correlation methods including semantic similarity, visual similarity, annotations, and categories to make connections between \(_{n}\) by \(_{e}\) shown in Tab. 2.

### Dataset Collection

Following the proposed protocol and three phases, we collect a new dataset, a large-scale multimodal knowledge graph from public international news. Specifically, based on the Wikipedia API  and our crawler system, we grab all the data of "_Worldwide Current Events_" from Wikipedia. As demonstrated in top of Fig. 4, we propose two category sets of news event called: _Event-11_ and _Event-9185_, which is coarse-grained and fine-grained respectively. For example, _"Sports"_ is a kind of coarse-grained event label in _Event-11_ and _"2019 Daytona 500"_ is a fine-grained label in _Event-9185_, detailed in Tab. 3. Since Wikipedia only records the news URL (downward black arrow in Fig. 4) and the HTML of original news from different news platforms is inconsistent, it is difficult to design a uniform crawler to get the well-structured raw data of news. Thus, we manually read each news and collect the original data (rightward black arrow). By this way, each news in our dataset is marked with extremely clean **title**, **content**, **time**, **[image]**, **image description**, **event description**, [hierarchical] **event name** (_e.g._, _"Armed conflicts and attacks\(\)War in Donbass"_), and **event attribute** (location, date, _etc_). Subsequently, as shown in bottom right of Fig. 4, we apply the designed pipeline to sequentially undergo phases 1/2/3 to restructure the above extracted raw data, resulting in the knowledge graph under the _UKnow_ format.

Figure 3: **Pipeline of dataset construction following _UKnow_ protocol.** Phase-1: _Content Extraction_ (\(N_{p}\)), Phase-2: _Information Symbolization_ (\(_{n}\), \(_{e}\)), and Phase-3: _Knowledge Construction_ (\(_{m}\)). \(_{n}\) hides the real node index for easy understanding, the actual number is much more than \(_{e}\).

Furthermore, in addition to utilizing intricate annotation files (_e.g._, Fig. 4) as inputs mentioned above, another major advantage of the proposed conversion pipeline is its ability to accommodate common image-text pair annotations expressed in the format of "_[image description]_V_/_xxx_,_jpg_v_n_"), as the fundamental input. This design allows _UKnow_ to automatically construct a new dataset with more useful information from an existing image-text pair dataset. Taking LAION-5B  as an example, which solely comprises pairs of images and text, our pipeline can extract more features from them like objects, and thus expand LAION-5B into a larger and more practical dataset. However, given the absence of high-level event logic, this type of input does not lend itself to the creation of [\(L_{1}\).*] nodes and event-related edges.

  
**PARTITION** & \)} & \)} & \)} & \)} & \)} \\ 
**Number** & **CODE** & **CODE** & **NOPE** & **EDGE** & **NOPE** & **EDGE** & **NOPE** & **EDGE** & **CODE** & **EDGE** \\  Training Set & 448,691 & 8,030,331 & 501,564 & 979,287 & 250,858 & 396,200 & 699,911 & 412,628 & 765,654 & 382,827 \\ Validation Set & 37,488 & 100,230 & 12,126 & 12,121 & 69,535 & 57,162 & 15,532 & 97,272 & 9,764 & 4,882 \\ Testing Set & 37,685 & 100,375 & 12,182 & 12,261 & 69,286 & 55,464 & 15,336 & 99,930 & 9,622 & 4,811 \\  Pre-training Set & 228,339 & 435,659 & 343,458 & 325,755 & 101,880 & 314,918 & 47,017 & 271,305 & 278,058 & 139,029- \\ Fine-tuning Set & 75,924 & 82,350 & 65,809 & 61,850 & 19,185 & 59,832 & 8,880 & 52,772 & 52,522 & 26,261 \\ Testing Set & 34,422 & 28,219 & 22,809 & 22,278 & 6,633 & 21,360 & 3,074 & 17,754 & 18,186 & 9,093 \\   

Table 4: Partition of our dataset.

Figure 4: **Event category** labeled on web data and the **data flow diagram.

  
**Event Name (_Event-11_)** & **Visual** & **Texual** & **All** & **Event Name (10 examples of _Event-9185_)** & **Visual** & **Texual** & **All** \\  Armed conflicts and attacks & 87,346 & 90,137 & 177,503 & Sand Arabian-Iad intervention in Yemen & 555 & 258 & 813 \\ Arts and culture & 11,059 & 149,896 & 255,653 & A best carrying false-test experiment of the system cost of Mahyria & 46 & 19 & 65 \\ Business and economy & 12,598 & 25,565 & 38,163 & Travel restrictions related to the COVID-19 pandemic & 753 & 796 & 1,549 \\ Disasters and accidents & 28,062 & 247,475 & 75,521 & GameStop short sequence & 45 & 175 & 220 \\ Health and environment & 230,926 & 253,949 & 349,8275 & Composition to Brazil in the United Kingdom & 383 & 93 & 476 \\ International relations & 37,349 & 56,444 & 93,793 & Gretchon Whither kidnapping plot & 167 & 308 & 475 \\ Sports & 15,647 & 31,194 & 46,841 & Legality of enthusiasm & 185 & 455 & 640 \\ Law and crime & 69,953 & 86,541 & 156,087 & Ukraine International Affines Flight 752 (Air Crain) & 314 & 179 & 493 \\ Politics and elections & 74,477 & 72,114 & 147,911 & Manhattan blackout & 269 & 90 & 359 \\ Science and technology & 4,062 & 15,556 & 19,618 & 2019 Lagos school collapse & 524 & 119 & 643 \\ Others & 236 & 184 & 420 &... &... &... &... \\   

Table 3: **Details of the event category.**

[MISSING_PAGE_FAIL:8]

task adaptation. While these applications benefit from the structured nature of KGs, the underlying datasets may not be flexible enough to support a wide range of tasks, particularly those that require cross-modal reasoning or dynamic context adaptation.

In contrast, UKnow is inherently designed to handle multimodal data (e.g., images, text) in a unified structure. This allows for seamless integration and interaction between different types of data, making it particularly well-suited for tasks that require cross-modal reasoning, such as vision-language pre-training and complex event understanding. Besides, UKnow introduces a hierarchical structure that organizes nodes into levels and incorporates logical connections between them. The unified structure and logical richness of UKnow make it highly versatile for a wide range of downstream tasks. The ability to evaluate various tasks on a unified Knowledge Graph also reduces the complexity of model development and evaluation, leading to more efficient and effective AI solutions.

## 4 Usage of UKnow

### UKnow for Common-sense Reasoning

Since _UKnow_ is reasoning compatible, _i.e._, it naturally supports all KG-reasoning models, we directly implemented the commonly used KG-reasoning models (_e.g._, TransE , Q2B ) on _UKnow_. We propose a plug-in module which aggregates node features within a small sub-graph region to achieve a better central node features. We briefly introduce how to implement this module. Suppose \(N(e)\{e_{neib}|r(e_{neib},e) r(e,e_{neib}),r\}\) is the collection of neighbors of each central node \(e\). The calculation expression of the new representation \(}\) of \(\) is as follow:

\[}=(Flatten((_{n}( ^{}(,N_{e}{}^{}))+_{n})),\] (2)

where \(^{d}\) is the node feature before enhancement, \(}\) is the new feature, \(\) denotes a 2D convolution operation, \(_{n}\) is the filter, \(_{n}\) is the bias and the specification of \(\) is \(^{m_{1} m_{2}}^{d}\). The concat function \(^{}(,N_{e})^{m_{1} m_{2}}\) as \([;_{neib}{}^{1^{}};_{neib}{}^{2^{}} ;;_{neib}{}^{m}]\) where \(_{neib}{}^{i} N_{}^{}\).

### UKnow for Vision-Language Pre-training

Following the recent works , our work applies CLIP  as the pre-trained backbone benefit from its strong downstream performance. Specifically, the text encoder first tokenize the input text description into the word sequence, and then projects them into word embeddings \(_{0}=\{_{0}^{1},_{0}^{2},,_{0}^{ N}\}^{N d^{t}}\). \(_{0}\) is fed into a \(L\)-layer Transformer  with the architecture modifications described in BERT . And the final text embedding \(^{T}\) is obtained by projecting the last token, which corresponds to the [EOS] (the end of sequence) token, from the last layer of the text encoder, _i.e._, \(^{T}=(_{L}^{L}),^{T}^{d}\). As for the vision encoder, the input image \(I\) is first split into \(M\) non-overlapping patches, and projected into a sequence of patch tokens \(_{}^{M d^{v}}\). Then, \(_{}\) is fed into a \(L\)-layer Transformer-based architecture along with a learnable [CLS] token \(_{0}\). The final image embedding \(^{I}\) is obtained by projecting the [CLS] token from the last layer of the vision encoder, _i.e._, \(^{I}=(_{L}^{L},_{L}^{U})), ^{I}^{d}\). Since we have _Knowledge-View_, a new dimension \(^{k}\) which is used to represent knowledge is introduced:

\[^{k}=(I_{in}(^{I}),T_{in}(^{T}), I_{cross}(^{I}),T_{cross}(^{T})),\] (3)

where \(I_{in}()\) and \(T_{in}()\) mean to get the embedding of the [\(L_{3}\)\(\)] nodes (\(_{n}\)) from \(_{m}\) via \(_{e}\), \(I_{cross}()\) and \(T_{cross}()\) mean to get the embedding of [\(L_{2}\)\(\)] from \(_{m}\). Therefore, the similarity score between the image, text and knowledge can be calculated with the cosine similarity as follow:

\[s(T,I,k)=^{T}{}^{}^{I}}{\|^{T}\|\| ^{I}\|}+^{k}{}^{}^{I}}{\|^{ k}\|\|^{I}\|}+^{k}{}^{}^{T}}{\| ^{k}\|\|^{T}\|}.\] (4)

### UKnow Baseline

Upgrading AI from understanding objects (_e.g._, an apple) as in most current vision tasks to understanding complex human activities (_e.g._, an event), to understanding the logic between entities or objects, and to achieving higher-order intelligence, is always the thing we would like to pioneer. Thus, in this section, we naturally present a series of novel logic-rich downstream tasks as the baselinesfor our dataset. Specifically, Common-sense Reasoning is a conventional and fundamental task in our domain, aligning closely with our dataset. Then we perform multiple downstream tasks to verify the performance of the pretrained model trained with our dataset. For more details about task description/training setting/evaluation metric/analysis, please refer to Sec. C.

**Common-sense Reasoning.** We implement the Q2B\({}^{*}\) with our _UKnow_ based plug-in module based on Q2B  and BETAE\({}^{*}\) based on BETAE . As shown in Tab. 9, BETAE\({}^{*}\) achieves on average **21.64%** and **21.23%** MRR on the validation and testing set of our dataset. It indicates that our _UKnow_ based module can significantly improve the performance of existing methods.

**Multimodal Event Classification.** As shown in Tab. 10, TCL  achieves on **66.80%** and **55.87%** on ACC@1 when using the image-input on the _Event-11_ and _Event-9185_. respectively. We add a late-fusion module after the image/text encoder for all methods to support multimodal classification. Results show that TCL obtains gains of **1.89%** and **5.02%** compared with the singlemodal input, which demonstrates that multimodal pre-training is more helpful for downstream multimodal tasks.

**Single- & Cross-Modal Retrieval.** As shown in Tab. 11, TCL  achieves on **33.24%**, **43.37%** and **45.22%** R@1, R@5, R@10 on the zero-shot setting of image retrieval. The results are **58.89%**, **68.47%** and **73.91%** when fine-tuning the pre-trained parameters, which means the pre-training\(\)fine-tuning strategy is extremely beneficial for downstream retrieval.

**Visual Task Adaptation.** As shown in Tab. 12, our approach obtains gains of avg. **1.14%** compared with the origin CLIP when fairly using the same _UKnow_'s data for the upstream pre-training. It is essential to highlight that the image-text PAIR constitutes only one type of data in our protocol. By leveraging the capabilities of _UKnow_, our pre-trained CLIP model can effectively comprehend the inherent knowledge, resulting in superior performance than original CLIP model (Tab. 12, Row2).

### Practical Applications in Other Domains

UKnow is a general multimodal knowledge graph construction protocol that can be easily adapted to different domains by adjusting \(P\) in Phase-1 to the relevant processing modules required. Due to issues such as time and effort and difficulty of data acquisition, in this paper, we only use international news as an example, given its significance in the multimodal field and its ability to highlight UKnow's strengths in handling multimodal data. In the future, as we mentioned in Sec. D.3, we aim to diversify modalities by augmenting our dataset with a broader range of modalities (e.g., audio, video, 3D, etc.) to facilitate exploration across various downstream tasks. Here's an example of how to extend UKnow to the video domain and modality: (1) Phase-1: Replace \(P\) with operations like Video Captioning, Action Recognition, Video Summarization, or Object Detection and Tracking to process the video content. (2) Phase-2: Organize the processed video features into the node index and construct relationship edges with other modalities such as text, images, and audio. (3) Phase-3: Utilize Phase 3 to build the knowledge graph, which can then be applied to various knowledge-based downstream tasks.

## 5 Conclusion

This paper presents a unified knowledge protocol called _UKnow_ to establish the standard of knowledge from the perspective of data. Following this protocol, we collect a novel and the largest multimodal knowledge graph dataset from public international news with rich news event annotations, which can help intelligent machines understand human activities and history. The specific tasks addressed in this paper are the common-sense reasoning and vision-language pre-training. The former is a typical task in the knowledge graph field, and the latter brings knowledge to various downstream tasks. We also present a series of novel logic-rich downstream tasks to showcase the advantages of _UKnow_. In future work, we will continuously expand the data of different scales based on the _UKnow_ protocol.