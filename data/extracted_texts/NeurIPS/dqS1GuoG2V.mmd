# The Memory Perturbation Equation:

Understanding Model's Sensitivity to Data

 Peter Nickl

peter.nickel@riken.jp

&Lu Xu

lu.xu.sw@riken.jp

&Dharmesh Tailor

lu.xu.sw@riken.jp

&Thomas Mollenhoff

thomas.moellenhoff@riken.jp

&Mohammad Emtiyaz Khan

emtiyaz.khan@riken.jp

Equal contribution. Part of this work was carried out when Dharmesh Tailor was at RIKEN AIP.RIKEN Center for AI Project, Tokyo, Japan.University of Amsterdam, Amsterdam, Netherlands.Corresponding author.University of Amsterdam, Amsterdam, Netherlands.Corresponding author.

###### Abstract

Understanding model's sensitivity to its training data is crucial but can also be challenging and costly, especially during training. To simplify such issues, we present the Memory-Perturbation Equation (MPE) which relates model's sensitivity to perturbation in its training data. Derived using Bayesian principles, the MPE unifies existing sensitivity measures, generalizes them to a wide-variety of models and algorithms, and unravels useful properties regarding sensitivities. Our empirical results show that sensitivity estimates obtained during training can be used to faithfully predict generalization on unseen test data. The proposed equation is expected to be useful for future research on robust and adaptive learning.

## 1 Introduction

Understanding model's sensitivity to training data is important to handle issues related to quality, privacy, and security. For example, we can use it to understand (i) the effect of errors and biases in the data; (ii) model's dependence on private information to avoid data leakage; (iii) model's weakness to malicious manipulations. Despite their importance, sensitivity properties of machine learning (ML) models are not well understood in general. Sensitivity is often studied through empirical investigations, but conclusions drawn this way do not always generalize across models or algorithms. Such studies are also costly, sometimes requiring thousands of GPUs , which can quickly become infeasible if we need to repeat them every time the model is updated.

A cheaper solution is to use local perturbation methods , for instance, influence measures that study sensitivity of trained model to data removal (Fig. 1(a)) [8; 7]. Such methods too fall short of providing a clear understanding of sensitivity properties for generic cases. For instance, influence measures are useful to study trained models but are not suited to analyze training trajectories [14; 54]. Another challenge is in handling non-differentiable loss functions or discrete parameter spaces where a natural choice of perturbation mechanisms may not always be clear . The measures also do not directly reveal the causes of sensitivities for generic ML models and algorithms.

In this paper, we simplify these issues by proposing a new method to unify, generalize, and understand perturbation methods for sensitivity analysis. We present the Memory-Perturbation Equation (MPE) as a unifying equation to understand sensitivity properties of generic ML algorithms. The equation builds upon the Bayesian learning rule (BLR)  which unifies many popular algorithmsfrom various fields as specific instances of a _natural-gradient_ descent to solve a Bayesian learning problem. The MPE uses natural-gradients to understand sensitivity of all such algorithms. We use the MPE to show several new results regarding sensitivity of generic ML algorithms:

1. We show that sensitivity to a group of examples can be estimated by simply adding their natural-gradients; see Eq. 6. Larger natural-gradients imply higher sensitivity and just a few such examples can often account for most of the sensitivity. Such examples can be used to characterize the model's memory and memory-perturbation refers to the fact that the model can forget its essential knowledge when those examples are perturbed heavily.
2. We derive Influence Function [8; 31] as a special case of the MPE when natural-gradients with respect to Gaussian posterior are used. More importantly, we derive new measures that, unlike influence functions, can be applied _during_ training for all algorithms covered under the BLR (such as those used in deep learning and optimization). See Table 1.
3. Measures derived using Gaussian posteriors share a common property: sensitivity to an example depends on the product of its prediction error and variance (Eq. 12). That is, most sensitive data lies where the model makes the most mistakes and is also least confident. In many cases, such estimates are extremely cheap to compute.
4. We show that sensitivity of the training data can be used to accurately predict model generalization, even during training (Fig. 1(b)). This agrees with similar studies which also show effectiveness of sensitivity in predicting generalization [22; 12; 19; 4].

## 2 Understanding a Model's Sensitivity to Its Training Data

Understanding a model's sensitivity to its training data is important but is often done by a costly process of retraining the model multiple times. For example, consider a model with a parameter vector \(^{P}\) trained on data \(=\{_{1},_{2},,_{N}\}\) by using an algorithm \(_{t}\) that generates a sequence \(\{_{t}\}\) for iteration \(t\) that converges to a minimizer \(_{*}\). Formally, we write

\[_{t}_{t}(_{t-1},( ))\ \ \ \ ()=_{i=1}^{N}_{i}()+(),\] (1)

and we use the loss \(_{i}()\) for \(_{i}\) and a regularizer \(()\). Because \(_{t}\) are all functions of \(\) or its subsets, we can analyze their sensitivity by simply 'perturbing' the data. For example, we can remove a subset \(\) to get a perturbed dataset, denoted by \(^{}\), and retrain the model to get new iterates \(_{t}^{}\), converging to a minimizer \(_{*}^{}\). If the deviation \(_{i}^{}-_{t}\) is large for most \(t\), we may deem the model to be highly sensitive to the examples in \(\). This is a simple method for sensitive analysis but requires a costly brute-force retraining  which is often infeasible for long training trajectories, big models, and large datasets. More importantly, conclusions drawn from retraining are often empirical and may not hold across models or algorithms.

Figure 1: Our main goal is to estimate the sensitivity of the training trajectory when examples are perturbed or simply removed; see Panel (a). We present the MPE to estimate the sensitivity without any retraining and use them to faithfully predict the test performance from training data alone; see Panel (b). The test negative log-likelihood (gray line) for ResNet–20 on CIFAR10 shows similar trends to the leave-one-out (LOO) score computed on the training data (black line).

A cheaper alternative is to use local perturbation methods , for instance, influence measures that _estimate_ the sensitivity without retraining (illustrated in Fig. 1(a) by the dashed red arrow). The simplest result of this kind is for linear regression which dates back to the 70s . The method makes use of the stationarity condition to derive deviations in \(_{*}\) due to small perturbations to data. For linear regression, the deviations can be obtained in closed-form. Consider input-output pairs \((_{i},y_{i})\) and the loss \(_{i}()=(y_{i}-f_{i}())^{2}\) for \(f_{i}()=_{i}^{}\) and a regularizer \(()=\|\|^{2}/2\). We can obtain closed-form expressions of the deviation due to the removal of the \(i\)'th example as shown below (a proof is included in App. A),

\[_{*}^{ i}-_{*}=(_{*}^{  i})^{-1}_{i}e_{i}, f_{i}(_{*}^{ i})-f_{i}(_{*})=v_{i}^{  i}e_{i},\] (2)

where we denote \(_{*}^{ i}=_{*}-_{i}_{i}^{}\) defined using the Hessian \(_{*}=^{2}(_{*})\). We also denote the prediction error of \(_{*}\) by \(e_{i}=_{i}^{}_{*}-y_{i}\), and prediction variance of \(_{*}^{ i}\) by \(v_{i}^{ i}=_{i}^{}(_{*}^{ i})^{-1} _{i}\).

The expression shows that the influence is bi-linearly related to both prediction error and variance, that is, when examples with high error and variance are removed, the model is expected to change a lot. These ideas are generalized using _infinitesimal perturbation_. For example, influence functions [8; 32; 31] use a perturbation model \(_{*}^{ i}=_{}()- _{i}_{i}()\) with a scalar perturbation \(_{i}\). By using a quadratic approximation, we get the following influence function,

\[_{*}^{ i}}{_{i}} _{_{i}=0}=_{*}^{-1}_{i}(_{*}).\] (3)

This works for a generic differentiable loss function and is closely related to Eq. 2. We can choose other perturbation models, but they often exhibit bi-linear relationships; see App. A for details.

Despite their generality, there remain many open challenges with the local perturbation methods:

1. Influence functions are valid only at a stationary point \(_{*}\) where the gradient is assumed to be 0, and extending them to iterates \(_{t}\) generated by generic algorithmic-steps \(_{t}\) is non-trivial . This is even more important for deep learning where we may never reach such a stationary point, for example, due to stochastic training or early stopping [33; 53].
2. Applying influence functions to a non-differentiable loss or discrete parameter spaces is difficult. This is because the choice of perturbation model is not always obvious .
3. Finally, despite their generality, these measures do not directly reveal the causes of high influence. Does the bi-linear relationship in Eq. 2 hold more generally? If yes, under what conditions? Answers to such questions are currently unknown.

Studies to fix these issues are rare in ML, rather it is more common to simply use heuristics measures. Many such measures have been proposed in the recent years, for example, those using derivatives with respect to inputs [23; 2; 38], variations of Cook's distance , prediction error and/or gradients [3; 51; 42; 40], backtracking training trajectories , or simply by retraining . These works, although useful, do not directly address the issues. Many of these measures are derived without any direct connections to perturbation methods. They also appear to be unaware of bi-linear relationships such as those in Eq. 2. Our goal here is to address the issues by unifying and generalizing perturbation methods of sensitivity analysis.

## 3 The Memory-Perturbation Equation (MPE)

We propose the memory-perturbation equation (MPE) to unify, generalize, and understand sensitivity methods in machine learning. We derive the equation by using a property of _conjugate Bayesian models_ which enables us to derive a closed-form expression for the sensitivity. In a Bayesian setting, data examples can be removed by simply dividing their likelihoods from the posterior . For example, consider a model with prior \(p_{0}=p()\) and likelihood \(_{j}=p(_{j}|)\), giving rise to a posterior \(q_{*}=p(|) p_{0}_{1}_{2} _{N}\). To remove \(_{j}\), say for all \(j\), we simply divide \(q_{*}\) by those \(_{j}\). This is further simplified if we assume conjugate exponential-family form for \(p_{0}\) and \(_{j}\). Then, the division between two distributions is equivalent to a subtraction between their natural parameters. This property yields a closed-form expression for the exact deviation, as stated below.

**Theorem 1**.: _Assuming a conjugate exponential-family model, the posterior \(q_{*}^{}\) (with natural parameter \(_{*}^{}\)) can be written in terms of \(q_{*}\) (with natural parameter \(_{*}\)), as shown below:_

\[q_{*}^{}}{_{j} _{j}} e^{_{*}^{ },\,()}_{*},\, ()}}{_{j}e^{}_{j},\,()}} _{*}^{}=_{*}-_{j}}_{j}.\] (4)

_where all exponential families are defined by using inner-product \(,()\) with natural parameters \(\) and sufficient statistics \(()\). The natural parameter of \(_{j}\) is denoted by \(}_{j}\)._

The deviation \(_{*}^{}-_{*}\) is obtained by simply adding \(}_{j}\) for all \(j\). Further explanations and examples are given in App. B, along with some elementary facts about exponential families. We use this result to derive an equation that enables us to estimate the sensitivity of generic algorithms.

Our derivation builds on the Bayesian learning rule (BLR)  which unifies many algorithms by expressing their iterations as inference in conjugate Bayesian models . This is done by reformulating Eq. 1 in a Bayesian setting to find an exponential-family approximation \(q_{*} p(|) e^{-()}\). At every iteration \(t\), the BLR updates the natural parameter \(_{t}\) of an exponential-family \(q_{t}\) which can equivalently be expressed as the posterior of a conjugate model (shown on the right),

\[_{t}(1-)_{t-1}-_{j=0}^{N} }_{j}(_{t-1}) q_{t} {(q_{t-1})^{1-}\,(p_{0})^{}}_{}_{j=1}^{N}}_{j}(_{t-1}),\,())}}{ }\] (5)

where \(}_{j}()=()^{-1}_{ {}}_{q}[_{j}()]\) is the natural gradient with respect to \(\) defined using the Fisher Information Matrix \((_{t})\) of \(q_{t}\), and \(>0\) is the learning rate. For simplicity, we denote \(_{0}()=()=- p_{0}\), and assume \(p_{0}\) to be conjugate. The conjugate model on the right uses a prior and likelihood both of which, by construction, belong to the same exponential-family as \(q_{t}\). By choosing an appropriate form for \(q_{t}\) and making necessary approximations to \(}_{j}\), the BLR can recover many popular algorithms as special cases. For instance, using a Gaussian \(q_{t}\), we can recover stochastic gradient descent (SGD), Newton's method, RMSprop, Adam, etc. For such cases, the conjugate model at the right is often a linear model . These details, along with a summary of the BLR, are included in App. C. Our main idea is to study the sensitivity of all the algorithms covered under the BLR by using the conjugate model in Eq. 5.

Let \(q_{t}^{}\) be the posterior obtained with the BLR but without the data in \(\). We can estimate its natural parameter \(_{t}^{}\) in a similar fashion as Eq. 4, that is, by dividing \(q_{t}\) by the likelihood approximation at the current \(_{t}\). This gives us the following estimate of the deviation obtained by simply adding the natural-gradients for all examples in \(\),

\[}_{t}^{}-_{t}=_{j }}_{j}(_{t})\] (6)

where \(}_{t}^{}\) is an estimate of the true \(_{t}^{}\). We call this the memory-perturbation equation (MPE) due to a unique property of the equation: the deviation is estimated by a simple addition and characterized solely by the examples in \(\). Due to the additive nature of the estimate, examples with larger natural-gradients contribute more to it and so we expect most of the sensitivity to be explained by just a few examples with largest natural gradients. This is similar to the representer theorem where just a few support vectors are sufficient to characterize the decision boundary . Here, such examples can be seen as characterizing the model's memory because perturbing them can make the model forget its essential knowledge. The phrase memory-perturbation signifies this.

The equation can be easily adopted to handle an arbitrary perturbation. For instance, consider perturbation \(()-_{j}_{j}_{j}()\). To estimate its effect, we divide \(q_{t}\) by the likelihood approximations raised to \(_{i}\), giving us the following variant,

\[}_{t}^{_{}}-_{t}=_ {j}_{j}}_{j}(_{t}), .}_{t}^{_{ }}}{_{j}}|_{_{j}=0}=\,}_{j}(_{t}), j,\] (7)

where we denote all \(_{j}\) in \(\) by \(_{}\). Setting \(_{j}=1\) in the left reduces to Eq. 6 which corresponds to removal. The example demonstrates how to adopt the MPE to handle arbitrary perturbations.

[MISSING_PAGE_FAIL:5]

Plugging \(_{t}\) from the second equation into the first one, we can recover the following expressions,

\[}_{t}^{}-_{t}=}_{t}^{}^{-1}_{q_{t}} _{j}_{j}(),}_{t}^{_{i}}}{_{i}}_{ _{i}=0}=\,_{t}^{-1}_{q_{t}}[_{i }()]\] (10)

For the second equation, we omit the proof but it is similar to App. F, resulting in preconditioning with \(_{t}\). For computational ease, we will approximate \(}_{t}^{}_{t}\) even in the first equation. We will also approximate the expectation at a sample \(_{t} q_{t}\) or simply at the mean \(_{t}=_{t}\). Ultimately, the suggestion is to use \(_{t}^{-1}_{i}(_{t})\) as the sensitivity measure, or variations of it, for example, by using a Monte-Carlo average over multiple samples.

Based on this, a list of algorithms and their corresponding measures is given in Table 1. All of the algorithms can be derived as special instances of the BLR by making specific approximations (see App. C.3). The measures are obtained by applying the exact same approximations to Eq. 10. For example, Newton's method is obtained when \(_{t}=_{t}\), \(_{t}=_{t-1}\), and expectations are approximated by using the delta method at \(_{t}\) (similarly to Thm. 4). With these, we get

\[_{t}^{-1}_{q_{t}}[_{i}()] _{t-1}^{-1}_{i}(_{t}),\] (11)

which is the measure shown in the first row of the table. In a similar fashion, we can derive measures for other algorithms that use a slightly different approximations leading to a different preconditioner. The exact strategy to update the preconditioners is given in Eqs. 31 to 34 of App. C.3. For all, the sensitivity measure is simply an update step for the \(i\)'th example but in the opposite direction.

Table 1 shows an interplay between the training algorithm and sensitivity measures. For instance, it suggests that the measure \(_{t-1}^{-1}_{i}(_{t})\) is justifiable for Newton's method but might be inappropriate otherwise. In general, it is more appropriate to use the algorithm's own preconditioner (if they use one). The quality of preconditioner (and therefore the measure) is tied to the quality of the posterior approximation. For example, RMSprop's preconditioner is not a good estimator of the posterior covariance when minibatch size is large [27, Thm. 1], therefore we should not expect it to work well for large minibatches. In contrast, the ON method  explicitly builds a good estimate of \(_{t}\) during training and we expect it to give better (and more faithful) sensitivity estimates.

For SGD, our approach suggests using the gradient. This goes well with many existing approaches  but also gives a straightforward way to modify them when the training algorithm is changed. For instance, the TracIn approach  builds sensitivity estimates during SGD training by tracing \(_{j}(_{t})^{}_{i}(_{t})\) for many examples \(i\) and \(j\). When the algorithm is switched, say to the ON method, we simply need to trace \(_{j}(_{t})^{}_{t}^{-1}_{ i}(_{t})\). Such a modification is speculated in [42, Sec 3.2] and the MPE provides a way to accomplish exactly that. It is also possible to mix and match algorithms with different measures but caution is required. For example, to use the measure in Eq. 11, say within a first-order method, the algorithm must be modified to build a well-conditioned estimate of the Hessian. This can be tricky and can make the sensitivity measure fragile .

Extensions to non-differentiable loss functions and discontinuous parameter spaces is straightforward. For example, when using a Gaussian posterior, the measures in Eq. 10 can be modified to

  Algorithm & Update & Sensitivity \\  Newton’s method & \(_{t}_{t-1}-_{t-1}^{- 1}(_{t-1})\) & \(_{t-1}^{-1}_{i}(_{t})\) \\  Online Newton (ON)  & \(_{t}_{t-1}-\,_{t }^{-1}(_{t-1})\) & \(_{t}^{-1}_{i}(_{t})\) \\  ON (diagonal+minibatch)  & \(_{t}_{t-1}-\,_{t }^{-1}(_{t-1})\) & \(_{t}^{-1}_{i}(_{t})\) \\ iBLR (diagonal+minibatch)  & \(_{t}_{t-1}-\,_{t}^{-1}(_{t-1})\) & \(_{t}^{-1}_{i}(_{t})\) \\  RMSprop/Adam  & \(_{t}_{t-1}-\,_{t }^{-}(_{t-1})\) & \(_{t}^{-}_{i}(_{t})\) \\ SGD & \(_{t}_{t-1}-\, (_{t-1})\) & \(_{i}(_{t})\) \\  

Table 1: A list of algorithms and their sensitivity measures derived using Eq. 10. The second column gives the update, most of which use pre-conditioners that are either matrices \((_{t},_{t})\) or a vector \((_{t})\); see the full update equations in Eqs. 31 to 34 in App. C. The third column shows the associated sensitivity measure to perturbation in the \(i\)’th example which can be interpreted as a step for the \(i\) example but in the opposite direction. We denote the element-wise multiplication between vectors by “\(\)” and the minibatch gradients by \(\). For iBLR, \(_{t}\) is either \(_{t}\) or a sample from \(q_{t}\).

handle non-differentiable loss function by simply replacing \(_{q_{t}}[_{i}()]\) with \(_{}_{q_{t}}[_{i}()]\), which is a simple application of the Bonnet theorem  (see App. G). The resulting approach is more principled than  which uses an ad-hoc smoothing of the non-differentiable loss: the smoothing in our approach is automatically done by using the posterior distribution. Handling of discontinuous parameter spaces follows in a similar fashion. For example, binary variables can be handled by measuring the sensitivity through the parameter of the Bernoulli distribution (see App. D).

### Understanding the causes of high sensitivity estimates for the Gaussian case

The MPE can be used to understand the causes of high sensitivity-estimates. We will demonstrate this for Gaussian \(q\) but similar analysis can be done for other distributions. We find that sensitivity measures derived using Gaussian posteriors generally have two causes of high sensitivity.

To see this, consider a loss \(_{i}()=- p(y_{i}|(f_{i}()))\) where \(p(y_{i}|)\) is an exponential-family distribution with expectation parameter \(\), \(f_{i}()\) is the model output for the \(i\)'th example, and \(()\) is an activation function, for example, the softmax function. For such loss functions, the gradient takes a simple form: \(_{i}()= f_{i}()[(f_{i}())-y_{i}]\)[6, Eq. 4.124]. Using this, we can approximate the deviations in model outputs by using a first-order Taylor approximation,

\[(_{t}^{ i})-f_{i}(_{t})}_{ } f_{i}(_{t})^{}(_ {t}^{ i}-_{t})(_{t})^{}_{t-1}^{-1} f_{i}(_{t})}_{=v_{it },}(_{t}))-y_{i}]}_{=e_{ it},}.\] (12)

where we used \(_{t}^{ i}-_{t}_{t-1}^{-1} _{i}(_{t})\) which is based on the measure in the first row of Table 1. Similarly to Eq. 2, the deviation in the model output is equal to the product of the prediction error and (linearized) prediction variance of \(f_{i}(_{t})\)[25; 20]. The change in the model output is expected to be high, whenever examples with high prediction error and variance are removed.

We can write many such variants with a similar bi-linear relationship. For example, Eq. 12 can be extended to get deviations in predictions as follows:

\[(f_{i}(_{t}^{ i}))-(f_{i}(_{t})) ^{}(f_{i}(_{t})) f_{i}(_{t})^{ }(_{t}^{ i}-_{t})^{}( f_{i}(_{t}))v_{it}e_{it}.\] (13)

Eq. 12 estimates the deviation at one example and at a location \(_{t}\), but we could also write them for a _group_ of examples and evaluate them at the mean \(_{t}\) or at any sample \( q_{t}\). For example, to remove a group \(\) of size \(M\), we can write the deviation of the model-output vector \(()^{M}\),

\[(_{t}^{})-(_{t}) (_{t})^{}_{t}^{-1} (_{t})[((_{t})-],\] (14)

where \(\) is the vector of labels and we used the sensitivity measure in Eq. 10. An example for sparse Gaussian process is in App. H. The measure for SGD in Table 1 can also be used which gives \(f_{i}(_{t}^{ i})-f_{i}(_{t})\| f_ {i}()\|^{2}e_{it}\) which is similar to the scores used in . The list in Table 1 suggests that such scores can be improved by using \(_{t}\) or \(_{t}\), essentially, replacing the gradient norm by an estimate of the prediction variance. Additional benefit can be obtained by further employing samples from \(q_{t}\) instead of using a point estimate \(_{t}\) or \(_{t}\); see an example in App. H.

It is also clear that all of the deviations above can be obtained cheaply during training by using already computed quantities. The estimation does not add significant computational overhead and can be used to efficiently predict the generalization performance during training. For example, using Eq. 12, we can approximate the leave-one-out (LOO) cross-validation (CV) error as follows,

\[(_{t})=_{i=1}^{N}_{i}(_{t}^{ i })=-_{i=1}^{N} p(y_{i}|(f_{i}(_{t}^{ i}))) -_{i=1}^{N} p(y_{i}|(f_{i}(_{t})+v_{it}e_{it} )).\] (15)

The approximation eliminates the need to train \(N\) models to perform CV, rather just uses \(e_{it}\) and \(v_{it}\) which are extremely cheap to compute within algorithms such as ON, RMSprop, and SGD. Leave-group-out (LGO) estimates can also be built, for example, by using Eq. 14, which enables us to understand the effect of leaving out a big chunk of training data, for example, an entire class for classification. The LOO and LGO estimates are closely related to marginal likelihood and sharpness, both of which are useful to predict generalization performance [22; 12; 19]. Estimates similar to Eq. 15 have been proposed previously [43; 4] but none of them do so during training.

## 4 Experiments

We show experimental results to demonstrate the usefulness of the MPE to understand the sensitivity of deep-learning models. We show the following: (1) we verify that the estimated deviations (sensitivities) for data removal correlate with the truth; (2) we predict the effect of class removal on generalization error; (3) we estimate the cross-validation curve for hyperparameter tuning; (4) we predict generalization during training; and (5) we study evolution of sensitivities during training. All details of the experimental setup are included in App. I and the code is available at https://github.com/team-approx-bayes/memory-perturbation.

**Estimated deviations correlate with the truth:** Fig. 2 shows a good correlation between the true deviations \((f_{i}(_{*}^{_{i}}))-(f_{i}( {}_{*}))\) and their estimates \(^{}(f_{i}(_{*}))v_{i*}e_{i*}\), as shown in Eq. 13. We show results for three datasets, each using a different architecture but all trained using SGD. To estimate the Hessian \(_{*}\) and compute \(v_{i*}= f_{i}(_{*})^{}_{*}^{-1} f _{i}(_{*})\), we use a Kronecker-factored (K-FAC) approximation implemented in the laplace and ASDL packages. Each marker represents a data example. The estimate roughly maintains the ranking of examples according to their sensitivity. Below each panel, a histogram of true deviations is included to show that the majority of examples have extremely low sensitivity and most of the large sensitivities are attributed to a small fraction of data. The high-sensitivity examples often include interesting cases (possibly mislabeled or simply ambiguous), some of which are visualized in each panel along with some low-sensitivity examples to show the contrast. High-sensitivity examples characterize the model's memory because perturbing them leads to a large change in the model. Similar trends are observed for removal of _groups_ of examples in Fig. 6 of App. I.2.

**Predicting the effect of class removal on generalization:** Fig. 3(a) shows that the leave-group-out estimates can be used to faithfully predict the test performance even when a _whole class_ is removed. The x-axis shows the test negative log-likelihood (NLL) on a held-out test set, while the y-axis shows the following leave-one-class-out (LOCO) loss on the set \(\) of a left-out class,

\[_{}(_{*})=_{i} _{i}(_{*}^{})^{}-_{i } p(y_{i}|(f_{i}(_{*})+v_{i*}e_{i*})).\]

Figure 2: The estimated deviation for an example removal correlates well with the true deviations in predictions. Each marker represents an example. For each panel, the histogram at the bottom shows that the majority of examples have low sensitivity and most of the large sensitivities are attributed to a small fraction of data. We show a few images of high and low sensitivity examples from two randomly chosen classes, where we observe the high-sensitivity examples to be more interesting (possibly mislabeled or just ambiguous), while low-sensitivity examples appear more predictable.

The estimate uses an approximation: \(f_{i}(_{*}^{})-f_{i}(_{*}) f_{i}(_{*})^{}_{*}^{-1}_{j}_{j}(_{*}) v_{i*}e_{i*}\), which is similar to Eq. 12, but uses an additional approximation \(_{j}_{j}(_{*})_{i}( _{*})\) to reduce the computation due to matrix-vector multiplications (we rely on the same \(\)-FAC approximation used in the previous experiment). Results might improve when this approximation is relaxed. We show results for two models: MLP and LeNet. Each marker corresponds to a specific class whose names are indicated with the text. The dashed lines indicate the general trends, showing a good correlation between the truth and estimate. The classes _Shirt_, _Pullover_ are the most sensitive, while the classes _Bag_, _Trousers_ are least sensitive. A similar result for MNIST is in Fig. 11(d) of App. I.3.

**Predicting generalization for hyperparameter tuning:** We consider the tuning of the parameter \(\) for the \(L_{2}\)-regularizer of form \(\|\|^{2}/2\). Fig. 4 shows an almost perfect match between the test NLL and the estimated LOO-CV error of Eq. 15. Additional figures with the test errors visualized on top are included in Fig. 7 of App. I.4 where we again see a close match to the LOO-CV curves.

**Predicting generalization during training:** As discussed earlier, existing influence measures are not designed to analyze sensitivity during training and care needs to be taken when using ad-hoc strategies. We first show results for our proposed measure in Eq. 10 which gives reliable sensitivity estimates during training. We use the improved-BLR method  which estimates the mean \(_{t}\) and a vector preconditioner \(_{t}\) during training. We can derive an estimate for the LOO error at the mean \(_{t}\) following a derivation similar to Eqs. 14 and 15,

\[(_{t})-_{i=1}^{N} p(y_{i}|(f_{i}( _{t})+v_{it}e_{it}))\] (16)

Figure 4: The test NLL (gray) almost perfectly matches the estimated LOO-CV error of Eq. 15 (black). The x-axis shows different values of \(\) parameter of an \(L_{2}\)-regularization \(\|\|^{2}/2\).

Figure 3: Panel (a) shows, in the x-axis, the test NLL of trained models with a class removed. In the y-axis, we show the respective leave-one-class-out (LOCO) estimates. Each marker correspond to a specific class removed (text indicates class names). Results for two models on FMNIST are shown. Both show good correlation between the test NLL and LOCO estimates; see the dashed lines. Panel (b) shows the evolution of estimated sensitivities during training of LeNet5 on FMNIST. As training progresses, the model becomes more and more sensitive to a small fraction of data.

where \(v_{it}= f_{i}(_{t})^{}(_{t})^{-1} f _{i}(_{t})\) and \(e_{it}=(f_{i}(_{t}))-y_{i}\).

The first panel in Fig. 5 shows a good match between the above LOO estimate and test NLL. For comparison, in the next two panels, we show results for SGD training by using two ad-hoc measures obtained by plugging different Hessian approximations in Eq. 11. The first panel approximates \(_{t}\) with a diagonal Generalized Gauss-Newton (GGN) matrix, while the second panel uses a K-FAC approximation. We see that diagonal-GGN-LOO does not work well at all and, while K-FAC-LOO improves this, it is still not as good as the iBLR result despite using a non-diagonal Hessian approximation. Not to mention, the two measures require an additional pass through the data to compute the Hessian approximation, and also need a careful setting of a damping parameter.

A similar result for iBLR is shown in Fig. 1(b) where we use the larger ResNet-20 on CIFAR10, and more such results are included in Fig. 8 of App. I.5. We also find that both diagonal-GGN-LOO or K-FAC-LOO further deteriorate when the model overfits; see Fig. 9. Results for the Adam optimizer are included in Fig. 10, where we again see that using ad hoc measures may not always work. Overall, these results show the difficulty of estimating sensitivity during training and suggest to take caution when using measures that are not naturally suited to analyze the training algorithm.

**Evolution of sensitivities during training:** Fig. 3(b) shows the evolution of sensitivities of examples as the training progresses. We use the iBLR algorithm and approximate the deviation as \((f_{i}(_{t}^{}}))-(f_{i}(_{t} ))^{}(f_{i}(_{t}))v_{it}e_{it}\) where \(v_{it}\) and \(e_{it}\) are obtained similarly to Eq. 16. The x-axis corresponds to examples sorted from least sensitive to most sensitive examples at convergence. The y-axis shows the histogram of sensitivity estimates. We observe that, as the training progresses, the distribution concentrates around a small fraction of the data. At the top, we visualize a few examples with high and low sensitivity estimates, where the high-sensitivity examples included interesting cases (similarly to Fig. 2). The result suggests that the model concentrates more and more on a small fraction of high-sensitivity examples, and therefore such examples can be used to characterize the model's memory. Additional experiments of this kind are included in Fig. 11 of App. I.6, along with other experiment details.

## 5 Discussion

We present the memory-perturbation equation by building upon the BLR framework. The equation suggests to take a step in the direction of the natural gradient of the perturbed examples. Using the MPE framework, we unify existing influence measures, generalize them to a wide variety of problems, and unravel useful properties regarding sensitivity. We also show that sensitivity estimation can be done cheaply and use this to predict generalization performance. An interesting avenue for future research is to apply the method to larger models and real-world problems. We also need to understand how our generalization measure compares to other methods, such as those considered in . We would also like to understand the effect of various posterior approximations. Another interesting direction is to apply the method to non-Gaussian cases, for example, to study ensemble methods in deep learning with mixture models.

Figure 5: We compare faithfulness of LOO estimates during training to predict the test NLL. The first panel shows results for iBLR where a good match is obtained by using the LOO estimate of Eq. 16 which uses a diagonal preconditioner. The next two panels show results for SGD where we use the LOO estimate of Eq. 15 but with different Hessian approximations. Panel (b) uses a diagonal-GGN which does not work very well. Results are improved when K-FAC is used, but they are still not as good as the iBLR, despite using a non-diagonal Hessian approximation.