# Hellinger-UCB: A novel algorithm for stochastic multi-armed bandit problem

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

In this paper, we study the stochastic multi-armed bandit problem, where the reward is driven by an unknown random variable. We propose a new variant of the Upper Confidence Bound (UCB) algorithm called Hellinger-UCB, which leverages the squared Hellinger distance to build the upper confidence bound. We prove that the Hellinger-UCB reaches the theoretical lower bound(O(T)). As a real-world example, we apply the Hellinger-UCB algorithm to solve the cold-start problem for a content recommender system of a financial app. With reasonable assumption, the Hellinger-UCB algorithm has an important lower latency feature, closed-form UCB. The online experiment also illustrates that the Hellinger-UCB outperforms both KL-UCB and UCB1 in the sense of a higher click-through rate (CTR), 33% higher than the KL-UCB and almost 100% higher than the UCB1.

## 1 Introduction

### Stochastic multi-armed bandit problem

The stochastic multi-armed bandit problem(MAB)  is a sequential decision problem defined by a payoff function and a set of actions. At each step \(t\), an action \(A_{t}\) is chosen from the action set \(A=\{1,2,3,...K\}\) by the agent. And the associated reward \(r_{t}(A_{t})\), which is independent and identically distributed(i.i.d.), is obtained. The goal of the agent is to find the optimal strategy that maximizes the cumulative payoff obtained in a sequence of decisions

\[S_{A_{t}}(T)=_{s=1}^{T}r_{t}(A_{t}).\] (1)

The agent must come up with a strategy to maximize the cumulative payoff by dealing with the dilemma between exploitation and exploration. The pseudo-regret \(_{T}\) is introduced to evaluate the performance of a strategy. It is defined as the maximized expectation of the difference between the cumulative payoff of consistently choosing the best action and that of the strategy in the first \(T\) steps

\[_{T}=_{i=1,2,...,K}[_{s=1}^{T}r_{t}(i)-_{s=1}^{T}r _{t}(A_{t})]\] (2)

Lai and Robbins(1985) showed that if for all \(>0\) and \(,_{i}\) with \(()>(_{i})\), there exists \(>0\) such that \(|KL(,_{i})-KL(,_{j})|<\) whenever \((_{i})<(_{j})<(_{i})+\), and the following theorem is true. Let \(N_{i}(t)\) denote the number of times the agent selected action \(i\) in the first \(t\) steps,

**Theorem 1**: _If a policy has regret \(_{T}=o(T^{a})\) for all \(a>0\) as \(T 0\), the number of draws up to time \(t\), \(N_{i}(t)\) of any sub-optimal arm \(i\) is lower bounded_

\[_{T}(T)}{(T)};_{}>^{*}}KL(_{i},)}\] (3)

_Therefore, the regret is lower-bounded_

\[_{T}_{T}}{(T)}_{i: _{i} 0}}{_{_{i};_{ }>^{*}}KL(_{i},)}\] (4)

The stochastic multi-armed bandit problem has been extensively studied [4; 7; 9]. Under the parametric setting, a type of policy called upper-confidence bound (UCB) is proposed and proved to be promising. Agrawal introduced a family of index policies that is easier to compute. Auer, Nicolo and Paul proposed an online, horizon-free procedure which is called upper-confidence bound(UCB) and proved its efficiency. Audibert and Bubeck presented an improvement to the UCB1 called MOSS which is optimal for finite time. A variant of UCB which builds UCB based on the Kullback-Leibler divergence(KL) KL-UCB was presented by Garivier, Cappe.

## 2 Setup And Notations

We consider a stochastic multi-armed bandit problem with finite arms \(A=\{1,2,3,...K\}\). Each arm \(i\) is associated with a reward distribution \(p_{i}()\) over \(\). It is assumed that \(p_{i}()\) is from some one-parameter exponential family \(()\) with unknown expectation \(_{i}\).

It is also common to use a different formula for the pseudo-regret for a stochastic problem. Write \(^{*}=_{i=1,2,...,K}[_{i}]\) as the expected reward of the optimal action. Then \(_{i}=_{*}-_{i}\), and we have

\[_{T}=_{i=1}^{K}[N_{i}(T)]^{*}-[_{i=1}^{ K}N_{i}(T)_{i}]=_{i=1}^{K}_{i}[N_{i}(T)]\] (5)

## 3 The Hellinger-UCB Algorithm

The goal of the UCB algorithm is to make sequential decisions in the stochastic environment. The reward distribution of each arm is unknown. The only way to collect information and estimate the distribution is to pull the arm. But each trial comes with risk which is measured by regret. Hence exploration-exploitation trade-off is important in this case. The motivation of the UCB algorithm is being optimistic about the reward distributions as one always believes that the expected reward is the highest value within the confidence region. Hence the key in the UCB algorithm is constructing the confidence region.

The formula of the squared Hellinger distance makes it computationally efficient. With this property, we propose a new UCB type algorithm, the Hellinger-UCB which constructs the UCB based on the squared Hellinger distance. The new algorithm achieves the theoretical lower bound and has a closed-form UCB for some distributions, for example, binomial distribution. The latter property is favorable in some low-latency applications.

### Main algorithm

We briefly describe the process of Hellinger-UCB here. Let \(A=\{i\}_{i=1}^{K}\) be the action set where \(K\), the number of actions, is a positive integer. For each arm \(i A\), the reward distribution \(P_{_{i}}\) is in some one-parameter exponential family with expectation \(_{i}\). At the first \(|K|\) rounds, the agent chooses each arm once. After that, at each round \(t>|K|\), the agent makes a decision \(A_{t}=i\) based on the collected observations of each arm and gets the reward \(g_{t}(A_{t})\) from \(P_{A_{t}}\). The upper confidence bound for arm \(i\) is

\[U_{i}(t)=\{():H^{2}(P_{_{i,t-1}},P_{ }) 1-e^{-(t)}{N_{i}(t)}}\}\] (6)where \(P_{_{i,t-1}}\) is the estimated reward distribution based on the past observations and \(N_{i}(t)\) the number of pulls of arm \(i\). In the right hand side term, \(c(,]\) and usually achieves optimal performance with \(c\) slightly greater than \(\) in practice. This is a convex optimization problem and can be solved efficiently. The agent will choose the action \(i\) with the maximal \(U_{i}(t)\). Algorithm 1 shows the pseudo-code of the Hellinger-UCB algorithm.

```
1. Known Parameters: \(T\)(time horizon), \(K\)(action set), \(i K\)(action), \(r_{t}(i)\)(reward given action)
2. For \(t=1\) to \(|K|\): 1. \(A_{t}=i=t\%|K|\) 2. \(N_{i}(t)=1\) 3. \(S_{i}(t)=r_{t}(A_{t})\) end for
3. For \(t=|K|+1\) to \(T\): 1. \(A_{t}=\,max_{i}\{():H^{2}(P_{_ {i,t-1}},P_{}) 1-e^{-c(t)}}\}\), where \(P_{_{t-1}}\) is the maximum likelihood estimation(MLE) of the reward distribution based on the past observations. 2. \(r=r_{t}(A_{t})\) 3. \(N_{i}(t)+=\{A_{t}=i\}\) 4. \(S_{i}(t)+=r\) end for ```

**Algorithm 1** Hellinger-UCB

### Optimality of Hellinger-UCB

As a UCB-based algorithm for the stochastic multi-armed bandit problem, we are interested in whether the pseudo regret of the Hellinger-UCB algorithm is optimal. The following theorem guarantees the optimality of this algorithm. We first derive the upper bound of each sub-optimal arm's expected number of pulls.

**Theorem 2**: _Consider a multi-armed bandit problem with \(K\) arms and the associated payoffs are some distributions in a one-parameter exponential family. Let \(a^{*}\) denote the optimal arm with expectation \(^{*}\) and \(i\) denote some sub-optimal arm with expectation \(_{i}\) such that \(_{i}<^{*}\). For any \(T>0\), the number of picks of arm \(i\) by Hellinger-UCB is \(N_{i}(T)\). For any \(>0\)_

\[[N_{i}(T)]-(^{*},_ {i})}{1+})}+()}{T^{C_{2}()}}+_{t=1}^{T }}+(^{*},_{i})}}{1-e^{-2H^{2}(^{*}, _{i})}}\]

_where \(C_{1}()=-(^{*},_{i})}{1+})}>0\) and \(C_{2}()=-1)^{2}}{1+}>0\)._

_if \(c>\),_

\[[N_{i}(T)]-(^{*},_ {i})}{1+})}+()}{T^{C_{2}()}}+O(1)\]

**Proof:** See Appendix A for details of the proof

Then the upper bound of the pseudo-regret is just a direct result of Theorem 3.1

**Theorem 3**: _The regret of Hellinger-UCB satisfies:_

\[_{T}_{i:_{i}^{*}}_{i}[N_{i} (T)]\] (7)Numerical result

The long-run online experiment is conducted in the front page content recommendation business of JD Finance App. The recommendation system is designed to provide personalized multi-type content recommendations to the users. For each request, the cold start model is required to rank a set of articles and tweets, and then present the top-rank contents to the users. All three algorithms, UCB1, KL-UCB, and Hellinger-UCB, rank about ten thousand of contents(Financial articles) from the cold start pool with estimated CTR. CTR is modeled as the mean reward of a series of Bernoulli trials, which is the exact historical clicks and impression information. Three UCB algorithms share the whole traffic and the final impression is generated by randomly selecting one of three results uniformly. The system records 1 point as a reward to the corresponding algorithm when the user has any positive interaction (click/like/comment) with the content. Under this setting, the comparison of rewards among the three algorithms will give an insight into CTR for each algorithm. We show the upper confidence bound of UCB1, KL-UCB, and Hellinger-UCB in the appendix.

Figure 1 shows the cumulative reward plot of three algorithms in a two-month experiment from Oct. 2020 to Nov. 2020. It is very clear that Hellinger-UCB (blue line) significantly outperforms KL-UCB (orange line) and UCB1 algorithm (green line). In fact, the Hellinger-UCB algorithm achieves about \(33\%\) more clicks than the KL-UCB algorithm and almost \(100\%\) more clicks than the UCB1 algorithm. Hellinger-UCB algorithm obtains more clicks in the early period and then achieves even more clicks as the learning continues. This is an encouraging illustration of the potential power of Hellinger-UCB in real applications.

## 5 Conclusion

We presented the Hellinger-UCB algorithm for the stochastic multi-armed bandit problem. In the case that the reward is from an unknown exponential family, we provide the detailed formula of the algorithm and an optimal regret upper bound that achieves \(O((T))\). We present real numerical experiments that show significant improvement over other variants of UCB algorithms. The cumulative reward form the proposed algorithm is higher. We also show the algorithm has a closed-form UCB when the reward is a bernoulli distribution, which is a beneficial property for low-latency applications.

Figure 1: Cumulative reward plot of different UCB algorithms. The y-axis is reward points. The x-axis is a time stamp recorded as 9 digits integer.