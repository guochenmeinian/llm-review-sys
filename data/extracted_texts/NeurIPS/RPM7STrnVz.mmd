# VideoTetris:

Towards Compositional Text-to-Video Generation

 Ye Tian\({}^{1}\) Ling Yang\({}^{1}\)1 Haotian Yang\({}^{2}\) Yuan Gao\({}^{2}\) Yufan Deng\({}^{1}\) Jingmin Chen\({}^{1}\)

**Xintao Wang\({}^{2}\) Zhaochen Yu\({}^{1}\) Xin Tao\({}^{2}\) Pengfei Wan\({}^{2}\) Di Zhang\({}^{2}\) Bin Cui\({}^{1}\)\({}^{}\)**

Project: videotetris.github.io Code: https://github.com/YangLing0818/VideoTetris

###### Abstract

Diffusion models have demonstrated great success in text-to-video (T2V) generation. However, existing methods may face challenges when handling complex (long) video generation scenarios that involve multiple objects or dynamic changes in object numbers. To address these limitations, we propose VideoTetris, a novel framework that enables compositional T2V generation. Specifically, we propose _spatio-temporal compositional diffusion_ to precisely follow complex textual semantics by manipulating and composing the attention maps of denoising networks spatially and temporally. Moreover, we propose an enhanced video data preprocessing to enhance the training data regarding motion dynamics and prompt understanding, equipped with a new _reference frame attention_ mechanism to improve the consistency of auto-regressive video generation. Extensive experiments demonstrate that our VideoTetris achieves impressive qualitative and quantitative results in compositional T2V generation.

## 1 Introduction

With the significant development of diffusion models  recently, advanced text-to-video models  have emerged and demonstrated impressive results. However, these models often struggle with generating complex scenes following compositional prompts, such as "_A man on the left walking his dog on the right_", which requires the model to compose various objects spatially and temporally. Moreover, with a growing interest in generating long videos, existing methods  try to explore multi-prompt long video generation, which is typically limited to simple single-object scene changes. These methods fail to manage scenarios where the number of objects changes dynamically, often resulting in bizarre transformations that do not accurately follow the input text.

To overcome these challenges, we introduce VideoTetris, a novel and effective diffusion-based framework to enable compositional text-to-video generation. Firstly, we define compositional video generation as encompassing two primary tasks: **(i) Video Generation with Compositional Prompts**, which involves integrating objects with various attributes and relationships into a complex and coherent video; and **(ii) Long Video Generation with Progressive Compositional Prompts**, where 'progressive' refers to the continuous changes in the position, quantity, and presence of objects with different attributes and relationships. Then, we introduce a novel **Spatio-Temporal Compositional Diffusion**, which manipulates the cross-attention value of denoising network temporally and spatially, synthesizing videos that faithfully follow complex or progressive instructions. Subsequently, to enhance the ability of long video generation models to grasp complex semantics and generate intricate scenes encompassing various attributes and relationships, we propose an **Enhanced Video

## 1 Introduction

Figure 1: _(a)_: Comparison in Video Generation with Compositional Prompts. _(b)_: Comparision in Long Video Generation with Progressive Compositional Prompts. VideoTetris demonstrates superior performance, exhibiting **precise adherence to position information, diverse attributes, interaction, consistent scene transitions, and high motion dynamics** in compositional video generation.

**Data Preprocessing** pipeline, augmenting the training data with enhanced _motion dynamics_ and _prompt semantics_, enabling the model to perform more effectively in long video generation with progressive compositional generation. Finally, we propose a consistency regularization method, namely **Reference Frame Attention**, that maintains content consistency in a coherent representation space with latent noise while being capable of accepting arbitrary image inputs, ensuring the consistency of multiple objects across different frames and positions. fig. 1(a) showcases our VideoTetris's superior performance in compositional short video generation. We accurately compose two distinct objects with their own attributes while maintaining their respective "left" and "right" positions and ensuring natural interaction between multiple objects. As for long video generation comparisons in fig. 1(b), FreeNoise  either depicts characters appearing abruptly and inexplicably transforming a man into a woman or depicts a squirrel transforming from a hazelnut. StreamingT2V  fails to incorporate information about new characters altogether, ignoring quantity information and exhibits severe color distortion in later stages. In contrast, our VideoTetris excels in generating long videos with progressive compositional prompts, seamlessly integrating new characters into the video scenes while maintaining consistent and accurate positional and quantity information. Notably, in generating long videos of the same length, FreeNoise produces only minor variations within the same scene, whereas VideoTetris demonstrates significantly higher motion dynamics, resulting in outputs that more closely resemble long narrative videos.

Our contributions are summarized as follows: **1)**. We introduce a Spatio-Temporal Compositional Diffusion method for handling scenes with multiple objects and following progressive complex prompts. **2)**. We develop an Enhanced Video Data Preprocessing pipeline to enhance auto-regressive long video generation through motion dynamics and prompt semantics **3)**. We propose a consistency regularization method with Reference Frame Attention that maintains content coherence in compositional video generation. **4)**. Extensive experiments show that VideoTetris can generate state-of-the-art quality compositional videos, as well as produce high-quality long videos that align with progressive compositional prompts while maintaining the best consistency.

## 2 Related Work

Text-to-Video Diffusion ModelsThe field of text-to-video generation has seen significant advancements with the progress of diffusion models [1; 12; 13] and the development of large-scale video-text paired datasets [14; 15]. Early works such as LVDM  and ModelScope , adapted 2D image diffusion models by flattening the U-Net architecture to a 3D U-Net and training on extensive video datasets. Subsequently, methods like AnimatedDiff  have incorporated temporal attention modules into the existing 2D latent diffusion models, preserving the established efficacy of T2I models. More recently, several transformers-based diffusion methods [17; 18; 6] have enabled large-scale joint training of videos and images, leading to significant improvements in generation quality.

Long Video GenerationMost existing text-to-video diffusion models have been trained on fixed-size video datasets due to the increased computational complexity and resource constraints. Consequently, these models are often limited to generating a relatively small number of frames, leading to significant degradation in quality when tasked with generating longer videos. Several advancements [19; 8; 10; 20] have sought to overcome this limitation through various strategies. More recently, Vlogger  and SparseCtrl  employ a masked diffusion model for conditional frame input. Although these masked diffusion approaches facilitate longer video generation, they often encounter model inconsistencies and quality degradation due to domain shifts in input. StreamingT2V  proposes a new paradigm, utilizing a ControlNet -like conditioning scheme to enable auto-regressive video generation. However, due to the low quality of the training data, the final video outputs often exhibit inconsistent and low-quality artifacts.

Compositional Video GenerationWhile current video generation models can synthesize text-guided videos, they often face challenges in generating videos featuring multiple objects or adhering to multiple complex instructions, which requires the model to compose objects with diverse temporal and spatial relationships. In the realm of text-to-video diffusion models, exploration of such scenarios remains incomplete. Several text-to-image methods like RPG  leverage additional layout or regional information to facilitate more intricate image generation [24; 25; 26; 23]. Within video diffusion techniques, approaches like LVD  and VideoDirectorGPT  employ a layout-to-video generator to produce videos based on spatial configurations. However, these layout-based methods often offeronly rudimentary and suboptimal spatial guidance, struggling particularly with overlapping objects, thereby resulting in videos with unnatural content. In contrast, our method adopts a compositional region diffusion approach. By explicitly modeling the spatial positions of objects with cross attention maps, our approach allows the objects to naturally integrate and blend during the denoising process, resulting in more realistic and coherent video output.

## 3 Method

OverviewIn this section, we introduce our method VideoTetris for compositional text-to-video generation. Our goal is to develop an efficient approach that enables text-to-video models to handle scenes with multiple objects and follow sequential complex instructions. We first introduce Spatio-Temporal Compositional Region Diffusion in section 3.1, which allows different objects to naturally integrate and blend during the denoising process in a training-free manner. Furthermore, for the task of generating long videos with progressive complex prompts, we construct an auto-regressive model based on the ControlNet  architecture and introduced a Enhanced Video Data Preprocessing pipeline in section 3.2 to collect a high-quality video-text pair dataset to train our auto-regressive model for enhanced motion dynamics and prompt understanding. Combined with Spatio-Temporal Compositional Region Diffusion, our auto-regressive model can generate long videos with seamless transitions between diverse target scenes. Finally, we propose a consistency regularization with Reference Frame Attention in section 3.3 for better object appearance preserving.

### Spatio-Temporal Compositional Diffusion

MotivationTo achieve natural compositional generation, a straightforward approach is to use the layout as a condition to guide the generation process. However, this method presents several challenges: (i) Requiring large-scale training. Given the significant potential for improvement in layout-to-image models, training a layout-to-video model or training temporal convolution and attention layers for a layout-to-image model would require substantial computational resources and may struggle to keep pace with the latest advancements in text-to-video models. (ii) Layout-based generation models impose significant constraints on object bounding boxes. For long video duration, the need to continuously adjust the positions and sizes of these boxes to maintain coherent video content introduces a complex planning process, which adds complexity to the overall method. Therefore, instead of training a layout-to-video model, we utilize cross-attention for precise generation [30; 31; 32; 33; 34; 35] and propose a training-free approach that directly adjusts the cross-attention of different targets [36; 37; 23; 38; 39], as is shown in fig. 3. This approach aims to overcome the limitations of layout-based methods and leverage the potential of more flexible and efficient generation techniques.

Localizing Subobjects with Prompt DecompositionFor a given prompt \(p\), we first decompose it temporally into contents at different frames: \(p=\{p^{1},p^{2},,p^{t}\}\), where \(t\) denotes the total number of frames and \(p^{i}\) denotes the given text prompt at \(i\)-th frame. Subsequently, for the \(i\)-th

Figure 2: The overall pipeline of VideoTetris. We introduce Spatio-Temporal Compositional module for compositional video generation and Reference Frame Attention for consistency regularization. For longer video generation, a ControlNet -like branch can be adopted for auto-regressive generation.

frame, we decompose the original \(p^{i}\) spatially into different sub-objects: \(\{p^{i}_{0},p^{i}_{1},,p^{i}_{n}\}\) with their corresponding region masks \(M^{i}=\{M^{i}_{0},M^{i}_{1},,M^{i}_{n}\}\), where \(n\) denotes the number of different objects. In this way, we decompose a prompt list temporally and spatially to acquire each sub-object's corresponding region information in the video timeline. We then calculate the cross attention value for the \(j\)-th sub-object at \(i\)-th frame as follows:

\[^{i}_{j}=((K^{i}_{j})^{T}}{})V^ {i}_{j} M^{i}_{j}, K=W_{K}*(p^{i}_{j}),V=W_{V}*(p^{i}_{j})\] (1)

where \(Q^{i}\) represents the query for the latent frame features, \(W_{K},W_{V}\) are linear projections, \(\) denotes the text encoder, and \(d\) is the latent projection dimension of the latent frame features.

LLM-based Automatic Spatio-Temporal Decomposer (Optional)Alternatively, the spatio-temporal decomposition process can directly utilize a Large Language Model (LLM) to automate tasks, given the robust performance of LLMs in language comprehension, reasoning, summarization and region generation abilities . We employ the in-context learning (ICL) capability of LLMs and guide the model to use chain-of-thought (CoT)  reasoning. Concretely, we first guide the LLM to decompose the story temporally, generating frame-wise prompts, and reception each one of them with LLM for better semantic richness. Then we use another LLM to decompose each prompt spatially into multiple prompts corresponding to different objects, assigning a region mask to each sub-prompt. The specific prompt templates that include task rules (instructions), in-context examples (demonstrations) are detailed in table 4, table 5 and table 6 of appendix A.1.

Spatio-Temporal Subobjects CompositionAfter we decompose the original prompt list temporally and spatially, we then compose them together from spatial to temporal. To this end, we first compute the cross-attention value of all sub-objects \(^{i}_{region}\) at \(i\)-th frame with:

\[^{i}_{region}=_{j=0}^{n}^{i}_{j}\] (2)

Subsequently, to ensure a cohesive transition across the boundaries of distinct regions and a seamless integration between the background and the entities within each region, we employ the weighted sum

Figure 3: Illustration of Spatio-Temporal Compositional Diffusion. For a given story ”A little dolphin starts exploring an old city under the sea, she first found a green turtle at the bottom, then her huge father comes along to accompany her at the right side.”, we first decompose it temporally to Text Prompt #1, #2 and #3, then we decompose each of them spatially to compute each sub-region’s cross attention maps. Finally, we compose them spatio-temporally to form a natural story.

of the CrossAtt\({}_{region}\) and the CrossAtt\({}_{original}\) for the original compositional prompt \(p\) with :

\[^{i}_{original}&=((K^{i})^{T}}{})V^{i}, K=W_{K}*(p^{i}),V=W_{V}*(p^{i}) \\ ^{i}&=*^{i}_{original }+(1-)*^{i}_{region}.\] (3)

Here \(\) parameter is utilized to adjust the balance between global information and individual characteristics, aiming to achieve video content more aligned with human aesthetic perception. Finally, we naturally concatenate all the cross-attention values computed along the temporal dimension:

\[=(^{1},^{2},, ^{t})\] (4)

In this way, either for a pre-trained text-to-video model such as Modelscope , Animatediff , VideoCrafter2  and Latte , or an auto-regressive model for longer video generation like StreamingT2V, this approach can be directly applied in a training-free manner to obtain compositional, consistent and aesthetically pleasing results.

### Enhanced Video Data Preprocessing

Enhancement of Motion DynamicsFor auto-regressive video generation, we empirically find StreamingT2V  is the most effective in producing consistent content. However, there is a notable tendency for the occurrence of poor-quality cases and color degradation in the later stages of video generation. We attribute this issue to the suboptimal quality of the original training data. To enhance the motion consistency and stability of long video generation, it is imperative to filter the video data to retain high-quality content with consistent motion dynamics. Inspired by Stable Video Diffusion , we empirically observed a significant correlation between a video's optical flow  score its motion magnitude. Excessively low optical flow often corresponds to static video frames, while excessively high optical flow typically indicates frames with intense changes. To ensure the generation of smooth and suitable video data, we filter Panda-70M  by selecting videos with average optical flow scores computed by RAFT  falling within a specified range (\(s_{1}\) to \(s_{2}\)).

Enhancedment of Prompt SemanticsWhile the Panda-70M's videos exhibit the best visual quality, the paired prompts tend to be relatively brief, which conflicts with our objective of generating videos that adhere to intricate, detailed, and compositional prompts. Directly using such data for training can result in a video generation model that inadequately comprehends complex compositional prompts. Inspired by recent text-to-image research [23; 44; 45], it has been demonstrated that high-quality prompts significantly enhance the output quality of visual content. Therefore, after filtering the initial set of videos, we perform a recaptioning process on the selected samples to ensure they are better aligned with our objectives. We employ three multimodal LLMs to generate spatio-temporally intricate and detailed descriptions of each video, followed by a local LLM to consolidate these descriptions, extract common elements, and add further information. More details on this process can be found in appendix A.2.

### Consistency Regularization with Reference Frame Attention

Given our approach involves the addition and removal of different objects in long videos, maintaining the consistency of each object throughout the video is crucial for final outputs. Most consistent ID control methods, such as IP-Adapter , StreamingT2V , InstantID , and Vlogger , typically encode reference images using an image encoder, often CLIP , and then integrate the results into the cross-attention block. However, since CLIP is pre-trained on image-text pairs, its image embeddings are designed to align with text. Consistency control, on the other hand, focuses on ensuring that the feature information of the same object in different frames is similar, which does not involve text. We hypothesize that using CLIP for this purpose is an indirect approach and propose Reference Frame Attention to maintain the inter-frame consistency of object features.

Formally, we first directly encode the reference images, which are usually the initial frames where the object appears, using the same autoencoder as the pre-trained T2V model. This ensures that the computational target during latent denoising is spatially consistent with the reference target within the hidden representation space. We then train a 2D convolutional layer and projection layer that are structurally identical to those in the original pipeline. This process can be represented as:

\[x_{ref}=W(((f_{k:k+l}))),\] (5)where \(W,\) denote the projection layer and the 2D convolutional layer, \(f_{k:k+l}\) denotes the \(l\) frames from index \(k\) that are chosen for reference. After encoding, we insert a Reference Frame Attention block in each attention block that calculates the cross-attention between the current object and the reference object, supplementing the existing attention blocks:

\[=(}{})V, K=W_{K}*x_{ref},V=W _{V}*xref\] (6)

It is noteworthy that to ensure the consistency of different objects across various regions, we need to separately multiply the corresponding object's region mask with \(Q,K,\) and \(V\) during this computation process, and in practical applications, when a new object emerges in the auto-regressive long video, we precompute its corresponding \(x_{ref}\) in the relevant regions for further process.

## 4 Experiments

### Experimental Setups

We conducted our experiments in two specific scenarios: Video Generation with Compositional Prompts and Long Video Generation for progressive Compositional Prompts. For the first scenario, we directly applied our Spatio-Temporal Compositional Diffusion on VideoCrafter2  to generate videos with \(F=16\) frames. For the second scenario, we employed the core ControlNet -like branch from StreamingT2V  as the backbone and processed the Panda-70M  dataset using the Enhanced Video Data Preprocessing methods in section 3.2 as the training set. For both scenarios, we used ChatGPT3 to generate 100 different prompts/prompt lists as input to the models, generated 6 videos for each prompt, and randomly selected one for comparison. Additional model hyperparameters and implementation details of VideoTetris are provided in appendix A.5.

Figure 4: Qualitative Results of Video Generation with Compositional Prompts in Comparision with SOTA Text-to-Video Models

### Metrics

To evaluate compositional video generation, existing metrics, such as CLIPScore  and Frechet Video Distance (FVD) , assess coarse text-video and video-video similarity but do not capture detailed correspondences in object-level attributes and spatial relationships. Instead, we extended the T2I-CompBench  to the video domain and introduced the following metrics for **compositional text-to-video evaluation**: **VBILIP-VQA**: the average BLIP -VQA score averaged across all frames and **VUnidet**: the average Unidet  score averaged across all frames. In addition, we followed previous work  and used **CLIP-SIM** to measure the content consistency of generated videos by calculating the CLIP  similarity among adjacent frames of generated videos.

### Video Generation with Compositional Prompts

Qualitative ResultsWe compare our VideoTetris with several state-of-the-art text-to-video (T2V) models on their ability to generate videos based on complex compositional prompts. These models include open-source options like LVD , VideoCrafter2 , and Animatediff , as well as commercial models Gen-2  and Pika . Using VideoCrafter2 as a backbone, we directly evaluate our Spatio-Temporal Compositional Diffusion module's training-free performance. In fig. 4, we show text-to-video synthesis results. For the prompt, "A brave knight and a wise wizard are journeying through a forest," most models generate two similar characters, blending features and losing individual distinctions. This highlights challenges in semantic alignment and compositional modeling for open-source models. In contrast, our VideoTetris preserves the distinct characteristics of each object and integrates them seamlessly with the background without confining them to fixed regions. For the prompt, "A talking sponge on the left and a superhero baby on the right are having an adventure," models like AnimateDiff split the image, while Runaway Gen-2, Pika, and VideoCrafter2 produce misaligned characters. LVD produces entangled features, resulting in disordered representations. In contrast, our method accurately aligns objects to their specified positions while maintaining high video quality, outperforming other methods. Additional examples in fig. 8 demonstrate our model's capability to handle more complex prompts with multiple objects, maintaining high quality and adherence to compositional semantics.

Quantitative ResultsWe report our quantitative results in table 1. Our VideoTetris achieves the best VBILIP-VQA and VUnidet scores across all models, demonstrating our superiority for complex compositional generation. We also achieved a CLIP-SIM higher than the original backbone VideoCrafter 2 and comparable to commercial models thanks to accurate semantic understanding. This proves that better text-video alignment can benefit overall consistency.

User StudyFor further evaluation, we conducted a user study comparing our method with other video generation models, reported in appendix A.4. Using GPT-4, we collected 100 compositional prompts and generated 100 video samples across diverse scenes, styles, and objects. Users compared model pairs by selecting their preferred video from three options: method 1, method 2, and comparable results.

### Long Video Generation for Progressive Compositional Prompts

Qualitative ResultsWe compared our VideoTetris with state-of-the-art long video generation models FreeNoise  and StreamingT2V . FreeNoise inherently supports multi-prompts, and we provide StreamingT2V with different prompts at various frame indexes for multi-prompt video generation. We present our qualitative experimental results in fig. 1 and fig. 5. For the multi

   Method & VBILIP-VQA & VUnidet & CLIP-SIM \\  Animatediff  & 0.3834 & 0.1921 & 0.8676 \\ VideoCrafter2  & 0.4510 & 0.1719 & 0.9249 \\ Gen-2  (Commercial) & 0.4427 & 0.1503 & 0.9421 \\ Pika  (Commercial) & 0.4219 & 0.1782 & **0.9736** \\ LVD  & 0.4820 & 0.1934 & 0.8873 \\ VideoTetris (Ours) & **0.5563** & **0.2350** & 0.9312 \\   

Table 1: Quantitative Results of Video Generation with Compositional Prompts

[MISSING_PAGE_FAIL:9]

Effect of Reference Frame AttentionWe also conducted an ablation study about the Reference Frame Attention, as demonstrated in fig. 6 and table 3. We observe from the result that our Reference Frame Attention achieves more consistent outputs, and the frequency of color artifacts significantly decreases, resulting in a more uniform overall color. This highlights the benefit of aligning reference and noise information semantically in the latent space. We provide more ablation studies about compositional approaches in appendix A.3.

## 5 Conclusion and Discussion

ConclusionIn this study, we addressed the limitations of current video generation models incapable of generating compositional video content and introduced a novel VideoTetris framework that enables high-quality compositional generation. We propose an efficient Spatio-Temporal Compositional module that decomposes and composes semantic information temporally and spatially in the cross-attention space. Additionally, to further enhance consistency in auto-regressive long video generation, we introduced an Enhanced Video Data Preprocessing pipeline and designed a brand new Reference Frame Attention module. Extensive experiment results confirmed the superiority of our paradigm in extending the generative capabilities of video diffusion models.

LimitationsOur proposed method can generate both short and long compositional videos. For fixed text-to-video generation, we can directly enhance the compositional performance of existing models. However, for long videos, due to the current performance limitations of long video generation models, our method inevitably encounters some bad cases. Additionally, using ControlNet  for auto-regressive long video generation results in huge computation cost and overly strong control information, leading to an excessive number of transition frames.

Broader ImpactRecent notable progress in text-to-video diffusion models has opened up new possibilities in creative design, autonomous media, and other fields. However, the dual-use nature of this technology raises concerns about its societal impact. There is a significant risk of misuse of video diffusion models, particularly in the impersonation of individuals. It is essential to emphasize that our algorithm is designed to enhance the quality of video generation, and we do not support or provide means for malicious uses.