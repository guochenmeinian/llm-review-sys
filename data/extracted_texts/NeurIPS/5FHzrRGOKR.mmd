# Federated Behavioural Planes: Explaining the Evolution of Client Behaviour in Federated Learning

Dario Fenoglio

Universita della Svizzera italiana

Lugano, Switzerland

dario.fenoglio@usi.ch

&Gabriele Dominici

Universita della Svizzera italiana

Lugano, Switzerland

gabriele.dominici@usi.ch

&Pietro Barbiero

Universita della Svizzera italiana

Lugano, Switzerland

pietro.barbiero@usi.ch

&Alberto Tonda

INRAE

Paris, France

alberto.tonda@inrae.fr

&Martin Gjoreski

Universita della Svizzera italiana

Lugano, Switzerland

martin.gjoreski@usi.ch

&Marc Langheinrich

Universita della Svizzera italiana

Lugano, Switzerland

marc.langheinrich@usi.ch

###### Abstract

Federated Learning (FL), a privacy-aware approach in distributed deep learning environments, enables many clients to collaboratively train a model without sharing sensitive data, thereby reducing privacy risks. However, enabling human trust and control over FL systems requires understanding the evolving behaviour of clients, whether beneficial or detrimental for the training, which still represents a key challenge in the current literature. To address this challenge, we introduce _Federated Behavioural Planes_ (FBPs), a novel method to analyse, visualise, and explain the dynamics of FL systems, showing how clients behave under two different lenses: predictive performance (error behavioural space) and decision-making processes (counterfactual behavioural space). Our experiments demonstrate that FBPs provide informative trajectories describing the evolving states of clients and their contributions to the global model, thereby enabling the identification of clusters of clients with similar behaviours. Leveraging the patterns identified by FBPs, we propose a robust aggregation technique named _Federated Behavioural Shields_ to detect malicious or noisy client models, thereby enhancing security and surpassing the efficacy of existing state-of-the-art FL defense mechanisms. Our code is publicly available on GitHub1.

## 1 Introduction

Federated Learning (FL), a privacy-aware deep learning (DL) approach in distributed environments, is a dynamic system where many clients collaborate to train a model without sharing sensitive data, thus mitigating privacy risks . Analyzing the behaviour of FL systems is crucial to detect anomalies--such as distribution shifts , biased data , or adversarial clients --which may compromise the global model's predictive performance and introduce biases into its decisionmaking process. Various strategies have been developed to detect FL anomalies [10; 11; 12; 13; 14; 15]. However, existing techniques are not designed to track, visualise, and explain how client behaviours affect the performance of the global model, thus limiting human trust and control on FL dynamics.

Various studies have analysed models' behaviour in terms of predictive performance and decision-making processes independently. Predictive performance behaviour is primarily investigated in the context of non-linear optimisation using _behavioural spaces_[16; 17; 18]. This technique allows the visualisation of the predictive diversity of a set of regression models by considering the vector of errors that each model produces on a set of samples. The decision-making process is studied mainly in explainable AI (XAI) research using, for example, _counterfactual explanations_. Counterfactuals can be used to identify relevant input features used by a model to make predictions, thus describing the position and orientation of the model's decision boundaries. However, these techniques have not yet been applied to FL systems, resulting in a lack of insight into how the behaviour of individual clients affects the overall model's accuracy and decision-making capabilities, leading to inefficiency in the training process.

To bridge this gap, we introduce _Federated Behavioural Planes_ (FBPs), a method designed to visualise, explain, and give insights into the dynamics of FL systems. Our key innovation involves the creation of two behavioural planes for FL clients: one to highlight their predictive diversity and another to emphasise their decision-making process diversity via counterfactuals. Building on the client behaviour information provided by FBPs, to show their practical utility, we propose _Federated Behavioural Shields_, a robust aggregation mechanism that enhances security against malicious or noisy clients by accurately weighting the client models according to their constructive contributions during training. The results of our experiments demonstrate that: (i) counterfactual generators jointly trained with FL systems produce valid and client-specific counterfactual explanations which effectively describe clients' decision-making diversity; (ii) FBPs facilitate the identification of clusters of clients with similar behaviours (e.g., normal vs. outlier clients), allowing for tracking of their trajectories during the entire training; (iii) Federated Behavioural Shields surpasses existing state-of-the-art defense mechanisms, demonstrating that the information contained in FBPs provides valuable descriptors of client behaviour.

## 2 Background

Federated learning.FL systems [20; 3] involve a network of \(K\) clients, coordinated by a central server, which collaboratively train a DL model. Each client \(k\) possesses a local and private dataset characterised by a set of \(z\) features \(x^{(k)} X^{(k)}^{z}\) and a set of \(u\) class labels \(y^{(k)} Y^{(k)}\{0,1\}^{u}\). In each training round \(t\), each client trains a model \(f:X^{(k)} Y^{(k)}\) on local data to maximise the likelihood \((^{(k)} x^{(k)},y^{(k)})\). Once trained, clients send their local model's parameters \(^{(k)}(t)^{q},q\) to a central server which aggregates these parameters using a permutation-invariant aggregation \(:^{q K}^{q}\) (such as the mean or median). The server then

Figure 1: The Federated Behavioural Planes framework enables the visualization of client behaviour in FL from two perspectives: predictive performance (_Error Behavioural Plane_) and decision-making processes (_Counterfactuals Behavioural Plane_). It highlights client trajectories and similarities, offering insights into client interactions and supporting the introduction of a new and effective robust aggregation mechanism with performance that surpasses state-of-the-art baselines.

sends the aggregated model parameters \((t+1)\) back to the clients to start a new training round:

\[() ^{(k)}(t)=_{^{(k)}(t)}(^{(k)} (t) x^{(k)},y^{(k)})\] (1) \[() (t+1)=_{k K}^{(k)}(t)\] (2)

While FL is an efficient process to safeguard privacy, the inherent lack of direct control over each individual client makes FL systems particularly vulnerable to various poisoning attacks [6; 7; 8; 9; 21; 22; 14; 23; 24; 25; 26]. These can be categorised into _model_ and _data_ poisoning attacks. Model poisoning involves altering gradients on compromised devices before transmission to the server [9; 14; 23; 24], while data poisoning indirectly manipulates gradients by tampering with training datasets on malicious devices [25; 27; 21].

Counterfactual explanations.Counterfactual explanations  describe a model's decision-making process by identifying minimal and plausible changes to an observed input's features that lead to a desired model prediction. In explainable AI, finding counterfactual explanations is framed as an optimisation problem where the objective is to identify, for each sample \(x\), the nearest data point \(x^{}\) such that the classifier \(f(,x^{})\) assigns a desired class label \(y^{}\):

\[_{x^{}}||x-x^{}|| f(,x^{ })=y^{}\] (3)

As a result, the variations in the input's features between \(x\) and \(x^{}\) offer actionable insights into how the model's decisions can be altered, highlighting the most important features.

Semantic and behavioural spaces.Semantic spaces [16; 17] represent the semantics of a model by considering the error vector \(e=[(f(,x_{i}),y_{i})]_{i=1,,n}\) that a model produces on a set of \(n\) samples, where \(\) could be the Mean Squared Error, for instance. Given a set of \(K\) models, the semantic space contains \(n\)-dimensional data points \([e_{1},,e_{K}]^{K n}\). Behavioural spaces  summarise semantic spaces into lower-dimensional spaces applying a transformation \(_{n m}:^{n}^{m}\) with \(m n\) (typically \(m=2\) for most applications) i.e., \(_{n m}([e_{1},,e_{K}])\).

## 3 Federated Behavioural Planes

**Problem definition:** Given an FL system composed of a set of \(K\) clients with local data \((x^{(k)},y^{(k)})\) and models \(f(^{(k)})\), we aim to analyse the evolution of the system to understand how clients impact the global model's predictive performance and decision-making over time. In Section 3.1, we introduce what drives our method and formalise the problem. In Section 3.2, we introduce Federated Behavioural Planes (FBPs), describing its components in more detail: the Error Behavioural Plane (Section 3.3) and the Counterfactual Behavioural Plane (Section 3.4). Finally, in Section 3.5, we introduce Federated Behavioural Shields, a new robust aggregation mechanism to enhance security in FL systems, showing a practical application of FBPs.

### Dynamic behaviour of federated learning

FL is a dynamic process transitioning from a state where all participating clients behave randomly to a state where the behaviour of participating entities becomes coherent (the parameters of client models tend to converge to values which lead to high model accuracy). Thus, FL systems can be effectively analysed through the lens of dynamical systems using tools traditionally employed for such studies, such as differential equations. Defining \(b(^{(k)})\) as the evolving state of the behaviour of the client \(k\) at time \(t\), we introduce two primary forces that influence \(b(^{(k)})\): \(g(^{(k)},x^{(k)},y^{(k)})\), the local training dynamics, which drives the client towards its local optimum by leveraging information from the local dataset \((x^{(k)},y^{(k)})\); and \(b(_{k K}^{(k)})-b(^{(k)})\), a correction term that periodically aligns \(b(^{(k)})\) with the aggregated state of all clients within the federated system. These dynamics can be encapsulated in the following differential equation2, which describes how client behaviours evolve during training and are influenced by internal forces within the FL system:

\[b(^{(k)})}{t}=g(^{(k)},x^{(k) },y^{(k)})(1-_{T})+[b(_{k K}^{(k)} )-b(^{(k)})]_{T}\] (4)

Here, \(_{T}\) is characterised by a periodic Dirac delta function, defined as \(_{T}=_{r=0}^{}(t-r T)\), which triggers instantaneous adjustments at intervals determined by period \(T\).

### Federated Behavioural Planes (FBPs)

Instead of finding a general analytical solution for Equation 4 (which is not trivial and requires strong assumptions on its components), we aim to empirically analyse the phase space of a FL system by considering different descriptors of client behaviours. More specifically, we focus on investigating (i) the _predictive performance_, evaluating how well the model is solving the task, and (ii) the _decision-making process_, as it contains information on how the model is solving the task. During each round, client behaviours are assessed through their respective models on the server, utilising a server-owned dataset reserved for this evaluation phase. This methodology aligns with existing protocols [9; 14; 28; 29; 30; 31; 32]. Each client behaviour is visualised through a two-dimensional plane: Error Behavioural Plane representing predictive performance and the Counterfactual Behavioural Plane to illustrate decision-making processes, collectively referred to as _Federated Behavioural Planes_. However, this framework offers a general approach to visualise and monitor different descriptors of the client behaviours simultaneously, which can be customised through specific functions, enabling the creation of additional planes.

### Error Behavioural Plane (EBP)

To comprehensively evaluate each model from the predictive performance point of view, we analyse the errors made by the model on all samples, rather than relying solely on a simpler aggregate metric, such as loss or error . This approach enables a more detailed examination of the differences in the model's performance as observed by Mouret and Clune . Following the methodology proposed by Zhang et al. , we first construct a semantic error space for each model and then map it to a reduced space, called Error Behavioural Plane (EBP).

**Definition 3.1** (Error Behavioural Plane).: Given a model \(f\), parametrised with a set of weights \(^{(k)}(t)\), related to the client \(k\) at round \(t\), a dataset \((x^{()},y^{()})\), owned by the server, and a dimensionality reduction technique \(_{n 2}\), the representation \(e^{(k)}(t)^{2}\) in the EBP of the client \(k\) is the following:

\[e^{(k)}(t)=_{n 2}([f(^{(k)}(t),x^{()} _{i})-y^{()}_{i}]_{i=1,,n})\] (5)

It is worth noting that two clients, despite having similar accuracy or loss, may receive significantly different representations in the EBP if they produce errors on distinct subsets of samples. In contrast, clients whose trajectories in the EBP converge over time form clusters representing clients whose predictive performance is similar on the same set of samples. However, clusters and trajectories in the EBP do not explain the decision-making process that leads to a prediction. This information could be used to further distinguish different types of clients and can be analysed using counterfactuals.

### Counterfactual Behavioural Plane (CBP)

The analysis of a model's decision-making process is the main research objective of explainable AI. Counterfactual explanations represent one of the most effective techniques as they give insights concerning the position and orientation of decision boundaries. Similar counterfactual explanations indicate models having similar decision boundaries, i.e. models taking decisions using a similar decision-making process. To this end, most differentiable counterfactual generators are trained to model the training data distribution [34; 35], thus potentially providing insights on non identical distributions in clients' data. To produce these explanations, clients' predictive models should be concurrently trained with a counterfactual generator. Additional information on the optimisation objective are provided in Appendix A.2, A.3. To obtain the Counterfactual Behavioural Plane (CBP), we first compute the distances between the counterfactual distribution generated by the server using a model of the client \(k\) and the distribution of other clients. Then, we apply dimensionality reduction to obtain the CBP.

**Definition 3.2** (Counterfactual Behavioural Plane).: Given a model \(f\) with parameters \(^{(k)}(t)\), related to the client \(k\) at round \(t\), a set of samples \(x^{()}^{n z}\) owned by the server with \(n\) samples and \(z\) features, a distance function \(d:^{n z}^{+}\), such as Wasserstein distance, and a dimensionality reduction technique \(_{K 2}\), the representation \(c^{(k)}(t)^{2}\) in the Counterfactual Behavioural Plane of client \(k\) in an FL settings is the following:

\[a^{(k)}(t)=[_{x^{}_{i}}||x^{()}_{i}-x^{ }_{i}||f(^{(k)}(t),x^{}_{i}) f(^{(k)}(t),x^{()}_{i} )]_{i=1,,n}\] (6)

\[l^{(k)}(t)=[d(a^{(k)}(t),a^{(i)}(t))]_{i=1,,K}, c^{(k)} (t)=_{K 2}(l^{(k)}(t))\] (7)

CBP produces complementary information to EBP as clients which are similar in the CBP might be far away in the EBP (as discussed in Appendix B.4). Furthermore, as the purpose of CBP is to track clients' behaviour rather than explaining the model decision to a user, counterfactuals can be generated based on predictive models' embeddings, instead of input features, concealing sensitive information.

### Federated Behavioural Shields - FBPs as a defence mechanism

FBPs provide descriptors of client behaviours during training, enabling various applications. Notably, trajectories in behavioural planes converging over time form clusters representing clients with similar predictive performance and decision-making process. This property can be used in practice to identify anomalies, such as malicious clients attempting to compromise FL training. In particular, leveraging this detailed information on client behaviours, we propose _Federated Behavioural Shields_ (FBSs), a new class of robust aggregation strategies designed to enhance security in FL without requiring prior knowledge on the attack. This defensive mechanism generates a behavioural score in round \(t\) for a client \(k\), denoted as \(s^{(k)}(t)\), which is formulated through the composition of multiple scores \(s^{(k)}_{j}\) computed on \(S\) behavioural spaces, to guide the aggregation process in creating the next round's global model \((t+1)\), as outlined below:

\[s^{(k)}(t)=s^{(k)}_{j}(t)}{_{i K}_{j S}s^{ (i)}_{j}(t)},(t+1)=_{k K}s^{(k)}(t)^{(k)}(t)\] (8)

Specifically, based on the FBPs we previously defined, we can compute these scores as follows:

\[s^{(k)}(t)=_{}(t)s^{(k)}_{}(t)}{_{i K }s^{(i)}_{}(t)s^{(i)}_{}(t)} s^{(k)}_{ }(t)=1-min(||e^{(k)}(t)||,1) s^{(k)}_{}(t)= _{i K}t^{(k)}_{i}(t)}\]

The error score \(s^{(k)}_{}(t)\) measures the distance between the client \(k\) and the optimal point at the center of the plane, while the counterfactual score \(s^{(k)}_{}\) measure the average distance between the distribution of counterfactual generated by a client and all the other clients. In addition, considering that honest clients may occasionally deviate from the norm but generally contribute positively, we introduce a moving average mechanism to track client behaviours (see Appendix A.2 for details).

## 4 Experiments

The preliminary goal of our experiments is to assess whether counterfactual generators can provide insights in an FL context without compromising the performance of the predictor. We then visualise FBPs to verify that they can reveal information about the behaviour of various clients through the error and counterfactual behavioural planes (EBP and CBP). Lastly, we analyse the effectiveness of Federated Behavioural Shields as a robust aggregation mechanism, demonstrating the utility of the information provided by FBPs. Our experiments aim to answer the following questions:* **Counterfactuals in FL:** Does the integration of counterfactual generators impact clients' predictive performance? Do counterfactuals have the same quality in FL compared to centralised scenarios? Could counterfactual generators be adapted for each client? Answering these questions is an essential preliminary step to check whether counterfactuals can be used to generate FBPs.
* **Explaining FL training:** Can trajectories in FBPs describe the evolving client behaviours during the training phase? Is it possible to visually identify clusters of clients using FBPs?
* **Leveraging FBPs information:** Do FBPs provide sufficient detail to Federated Behavioural Shields to enhance the security of the FL training process against security attacks?

This section describes essential information about the experiments. Further details on model configuration, training setup, and computational cost are presented in Appendices A.2, A.4, and A.6, respectively.

### Data & task setup

In our experiments, we utilise four datasets: a _Synthetic_ dataset (tabular) we designed to have full control on clients' data distributions, and thus test our assumptions; the _Breast Cancer Wisconsin_ (tabular); the _Diabetes Health Indicator_ (tabular); _small-MNIST_ (image); and _small-CIFAR-10_ (image), reducing its size by 76% to increase task difficulty and highlight client differences in performance. For all the experiments with the small-MNIST dataset, our approach involves generating counterfactuals at a non-interpretable internal representation level of the model instead of the input space. To reflect the most realistic cross-silo scenario , we distribute the training data among various clients such as data are not independent and identically distributed (IID) . This represents the most challenging scenario to detect malicious clients due to the significant variations even among benign clients. Further details on the datasets and non-IID implementation are provided in Appendix A.1. Additional experiments analysing setup characteristics such as window length, local epochs, server validation set size, and differences between non-IID and IID scenarios can be found in Appendix B.

### Evaluation

Metrics.To determine the efficacy of counterfactuals generated through end-to-end training in FL, we measure several key metrics: task accuracy (\(\) - higher is better); counterfactual validity (\(\)) , which checks if the counterfactuals' labels align with user-provided labels; proximity (\(\)) , assessing the realism of counterfactuals by their closeness to the training data (distance between the counterfactual and the closest data point in the training set with the same label); and sparsity (\(\)) , which quantifies the changes made to the input to generate the counterfactuals (number of features changed between the initial sample and the counterfactual). The latter is quantified using Euclidean distance, as counting the number of changes provides less insight on the generated counterfactuals. To evaluate the effectiveness of client-specific adaptation, we analyse the relative change in client proximity between global and client-specific models, expressed as \((P_{global}-P_{local})/P_{global}\). Finally, we measure the task accuracy (\(\)) of the FL system under different attacks and defenses. All metrics are reported as the mean and standard error across five experimental runs with distinct parameter initialization.

Baselines.In our experiments, we compare Federated Behavioural Shields with the following state-of-the-art robust aggregation methods: _Median_, _Trimmed-mean_, _Krum_, and _RFA_. Further information is provided in the Appendix A.7.

Federated Attacks.We focus on attacks with realistic assumptions and where additional information outside the typical FL scenario are not available [10; 14; 23]. We test the following attacks: _Label-flipping (Data Poisoning)_[26; 43], changes each sample's label to \(1-y\) (performed in the context of binary classification); _Inverted-loss (Data Poisoning)_, creates an update that maximises the loss on the local dataset; _Crafted-noise (Model Poisoning)_, adds noise \((0,(w^{t}))\) to the previous global model \(w^{t}\), where \((w^{t})\) is the standard deviation of \(w^{t}\) and \(\) is a scale factor set to 1.2; and _Inverted-gradient (Model Poisoning)_[45; 24], inverts the gradient derived from the server's previous update, misaligning it with the true gradient. Further details are provided in Appendix A.8.

## 5 Key Findings & Results

### Counterfactuals in FL

**Integrating counterfactual generators in FL optimisation does not compromise predictive performance (Table 1).** We compared model accuracy across four settings: two centralised learning (CL) and two FL, using three different datasets under non-IID conditions. For FL experiments, we used the traditional FedAvg approach with two variations: predictor-only and predictor with CF generator. The results indicate that our model, which involves the concurrent training of both the predictor and the counterfactual generator, achieves performance comparable to that of the predictor alone in FL. In the context of our model, both Local CL--where each client trains a model on its local data--and FL, implemented across all clients, comply with privacy standards . For Local CL, we report the average accuracy of models trained independently by each client and evaluated on a common test set. However, only FL reaches the performance levels of the CL scenario, which assumes local access to all client data. This result indicates that incorporating counterfactuals during training does not compromise the predictor's performance, thus supporting their beneficial application without adverse effects on performance.

**Counterfactuals generated in FL have similar quality to those in CL (Table 1).** Unlike Local CL, FL leverages information from all clients, thereby producing counterfactuals that more accurately reflect non-IID clients' data. As shown in Table 1, FL achieves higher validity compared to the Local CL approach, indicating that the FL's counterfactuals better match ground-truth labels. Additionally, FL exhibits lower sparsity, which measures the number of modifications needed to achieve the counterfactuals. Table 1 shows these results using a counterfactual generator we adapted for this scenario (see Appendix A.3), however, similar conclusions are also obtained using out-of-the-box generators  (see Appendix B.2).

**Counterfactual generators can be adapted to specific clients (Figure 2).** In our study, we explored the impact of client adaptation on counterfactual generation within FL environments, as detailed in Section 4.2. Client-personalisation is key whenever we need to extract client-specific information. This adaptation can be achieved by training (or fine-tuning) a counterfactual generator on local client's data (which naturally happens at each round). The effectiveness of client-specific adaptation can be measured by the relative change in client proximity between global and client-specific models, as shown in Figure 2. The figure shows a marked reduction in relative proximity across the three datasets under non-IID conditions (up to 70% in the Synthetic dataset). This suggests that client-specific counterfactuals after adaptation are more representative of individual client datasets, providing unique descriptors of client-specific behaviours in the CBP.

  
**Metric** & **Dataset** & **Local CL** & **CL** & **FL** & **FL** \\  & & predictor + CF & predictor + CF & predictor + CF & Predictor \\   & Diabetes & 55.9 \(\) 0.5\% & 75.0 \(\) 0.2\% & 74.7 \(\) 0.1\% & 74.2 \(\) 0.1\% \\  & Breast Cancer & 86.9 \(\) 0.7\% & 97.7 \(\) 0.0\% & 98.4 \(\) 0.1\% & 97.7 \(\) 0.4\% \\  & Synthetic & 75.0 \(\) 2.0\% & 99.4 \(\) 0.2\% & 99.8 \(\) 0.1\% & 99.9 \(\) 0.1\% \\   & Diabetes & 87.6 \(\) 2.6\% & 99.9 \(\) 0.1\% & 99.9 \(\) 0.0\% & _N/A_ \\  & Breast Cancer & 100.0 \(\) 0.1\% & 100.0 \(\) 0.0\% & 100.0 \(\) 0.0\% & _N/A_ \\  & Synthetic & 97.1 \(\) 1.9\% & 100.0 \(\) 0.0\% & 100.0 \(\) 0.0\% & _N/A_ \\   & Diabetes & 45.4 \(\) 2.1 & 34.5 \(\) 1.7 & 37.1 \(\) 1.2 & _N/A_ \\  & Breast Cancer & 1459 \(\) 25 & 1325 \(\) 20 & 1448 \(\) 43 & _N/A_ \\  & Synthetic & 8.63 \(\) 0.15 & 6.24 \(\) 0.22 & 6.14 \(\) 0.07 & _N/A_ \\   & Diabetes & 8.91 \(\) 0.61 & 5.45 \(\) 0.40 & 6.23 \(\) 0.44 & _N/A_ \\  & Breast Cancer & 61.2 \(\) 2.1 & 70.1 \(\) 1.8 & 72.1 \(\) 5.5 & _N/A_ \\  & Synthetic & 0.142 \(\) 0.026 & 0.091 \(\) 0.003 & 0.089 \(\) 0.002 & _N/A_ \\   

Table 1: Performance comparison of our model, which includes a Predictor and Counterfactual Generator (CF), across various settings: Local Centralised (Local CL), Centralised Learning (CL), Federated Learning (FL), and FL with only the Predictor in a non-IID setting.

Figure 2: Relative variation of client-proximity across datasets.

### Explaining FL training

**The trajectories in FBPs enable the identification of different client behaviours during training (Figure 3).** Figure 3 illustrates FBPs' client trajectories over the last 15 rounds, where we introduced a different attack on each dataset (from left to right: Inverted-loss, Crafted-noise, and Inverted-gradient attacks). Each dataset was configured with five non-IID clients and one attacker (red). In the Synthetic dataset, to highlight the visualization of honest client clusters, the two largest clients were subdivided into three (Client 2,3,4) and two smaller clients (Client 6,7), respectively, forming distinct clusters. Specifically, on the EBP for the Synthetic dataset, the attacker (Client 9) significantly deviates from the trajectories of other clients, aiming to disrupt the server model (S). On the CBP, clusters with similar data distributions converge, indicating similarity in decision boundaries and client training data. In Breast Cancer and small-MNIST, FBPs are also able to give insights on the type of attack. On the Breast Cancer EBP, Client 6 exhibits a random trajectory around the server trajectory, which indicates a Crafted-noise attack. In small-MNIST, both the CBP and EBP reveal that at each round the trajectory of the attacker consistently moves in the opposite direction to that of the server, which indicates an Inverted-gradient attack. These insights might be useful for FL users as they explain the nature of clients' behaviours, allowing the identification of as various types of attacks. Additional visualizations are available in the Appendix B.3.

**FBPs allow the identification of clusters of clients (see Figure 3).** In the Synthetic dataset, CBPs's clusters reflect client-specific data distributions (Client 1,2-3-4,5,6-7,8 and Attacker 9). Clients sharing similar data distributions tend to cluster closely in the CBP, which in the Synthetic dataset is structured with adjacent slices in the feature space (for example, Client 1 is positioned between Client 8 and Client 2). Similarly, in Breast Cancer and small-MNIST, two primary clusters are identifiable in the CBP: honest clients and attackers. This clustering capability is crucial for users aiming to comprehend client characteristics without direct access to data or models' parameters, thereby informing strategic decisions in training a federated model. For instance, one might consider reducing the number of clients with identical data distributions to avoid redundancy and enhance training efficiency. Conversely, identifying distinct clusters in FBPs can be useful to maximise model performance (at the expense of the generalisation ability) by forming a Clustered FL system where independent federated models are trained using a subset of similar clients.

### Leveraging FBPs information

**FBPs offer detailed insights into client behaviours, enabling the Federated Behavioural Shields to outperform existing state-of-the-art defense mechanisms (Figure 4).** Our comparative analysis demonstrates that Federated Behavioural Shields generally outperform traditional methods such as Krum, Median, and Trimmed-mean across various datasets including Breast, Diabetes, and small

Figure 3: Client trajectories on Counterfactuals and Error Behavioural Planes for Synthetic, Breast Cancer, and small-MNIST datasets, corresponding to Inverted-loss, Crafted-noise, and Inverted-gradient attacks, respectively. The figure highlights the deviation of the malicious client (red) from honest clients, who tend to cluster together over time, along with the previous-round global model (S)MNIST. The only exception was in the scenario of the Inverted gradient on the small-MNIST dataset, where Trimmed-mean performed better. Overall, our method enhanced the performance up to 10 percentage points (pp) over Median--the most robust aggregation baseline--when the FL system is under Label-flipping attack and up to 16 pp when the system is not under attack on small-MNIST. The proposed approach surpasses even a FedAvg aggregation in absence of attackers on the Breast Cancer and on the small-MNIST datasets, under both normal and crafted-noise conditions. These results suggest that an aggregation strategy based on predictive performance or decision-making similarities is superior to methods that solely consider sample count. Unlike other baselines, the independence of our method from prior knowledge about attackers establishes it as a robust aggregation tool in both adversarial and non-adversarial settings. Contrary to previous studies [14; 28], we find that predictive performance alone does not reliably identify malicious clients in non-IID settings, as honest clients may also show consistently low performance, leading to potential misclassification in the aggregation process. Occasionally, this reliance on predictive performance can reduce our method's accuracy compared to scenarios where only counterfactual information is used. For instance, under no-attack conditions, counterfactual information alone often offers more effective behavioural descriptors than combining it with predictive metrics. Further detailed analyses in Appendix B.12 indicate that using both descriptors generally yields better results than relying solely on counterfactual behaviour. Furthermore, visualizations in Appendix B.5 show how client scores consistently identify malicious clients during training, while Appendix B.11 demonstrates the robustness of our Federated Behavioural Shields against varying attack intensities.

## 6 Discussion

### Related works

Client behaviours in FL have been analysed using integrated visual tools [31; 46; 47] or indirectly through methods such as robust aggregation [10; 11; 12; 13; 14; 28; 30; 48; 49; 50; 51; 52] and clustering-based aggregation [53; 54; 55; 56; 57; 58; 59]. These studies typically focus on similarities in model or gradient parameters [10; 11; 12; 13; 31; 46; 57; 58; 59; 40; 52; 55; 56; 57; 59], under the premise that distinct client data distributions manifest as unique model parameters. Nevertheless, these methods might overlook valuable information about how client models behave with the data, particularly in terms of predictive performance. To mitigate this issue, previous studies adopted evaluation methods that utilise a clean validation set on the server to delineate clients' behaviour. Common descriptors used in such cases are straightforward metrics such as accuracy, error rates, and loss [14; 31; 46; 47; 53; 54]. Despite their utility, these metrics might not be able to reveal behavioural patterns, such as those involved in the decision-making processes of models, which may suggest subtle similarities or manipulations. Notably, Wang et al.  introduced

Figure 4: Comparative analysis of Federated Behavioural Shields and its simpler version with only counterfactuals (cf) or predictive-performance (error) versus Krum, Median, and Trimmed-mean defenses across five attack types—No attack, Crafted-noise, Inverted-gradient, Label-flipping, Inverted-loss—on three distinct datasets. Red dashed lines represent the accuracy achieved using FedAvg without attackers.

a post-hoc explainable approach, Grad-CAM , to explain client model behaviours during training. However, Grad-CAM is limited to CNN models and provides primarily qualitative visual insights, which cannot be easily automated and thus still require human intervention. Similarly, SentiNet  uses Grad-CAM to detect potentially malicious regions in input images, yet client behaviour assessments continue to rely primarily on evaluating predictive performance on manipulated and unmanipulated images. To the best of our knowledge, this work represents the first systematic attempt to formalise the evolving dynamics of clients in FL, showing how behavioural client trajectories affect the predictive performance and decision-making processes of the global model.

### Limitations and future works

The primary constraint of our framework lies in its assumption that the server possesses a minimal validation set for querying client models. While this assumption is common across various methods, it can be mitigated, as demonstrated in Wang et al. , by generating synthetic data points. To this end, our approach might already integrate a potential mechanism as differentiable counterfactual methods can be used to generate synthetic data . Furthermore, as indicated by the promising results in Appendix B.6, utilizing validation-independent descriptors, such as counterfactuals, renders our defense method robust against biased or unfair validation sets. Given the significance of ensuring fairness across clients, developing additional validation-independent descriptors represents a promising direction for future research. Another consideration is the computational overhead introduced by counterfactual generators, which, although minimal compared to other baselines (see Appendix A.6), is higher than that of traditional FedAvg. However, this overhead can be mitigated by using a smaller network for the counterfactual generator, thereby reducing the number of neurons (e.g., 1.8% of predictor parameters) without compromising accuracy.

Future work could leverage the extensive information provided by FBPs to explore additional strategies for optimizing the learning process, such as the development of Clustered FL among clusters of clients and the fine-grained categorization of attack types. Incorporating additional behavioural planes may also enhance the specificity of the FBPs explanations. Lastly, since our method allows for the integration of privacy-enhancing techniques such as Local Differential Privacy  and Homomorphic Encryption , future studies could analyze their impact on the overall performance and computational efficiency of our system.

### Conclusions

In this work, we proposed Federated Behavioural Planes, a method to explain the dynamics of FL systems and client behaviours. This innovative method allows to visualise, track, and analyse client behaviours based on specific characteristics. Our focus was twofold: examining predictive performance by analysing prediction errors and investigating the decision-making process through counterfactual generation. The results of our experiments showed that Federated Behavioural Planes enable to track client behaviours over time, cluster similar clients, and identify clients' contributions to the global model with respect to a specific descriptor. Based on Federated Behavioural Planes' information, we introduced a novel robust aggregation mechanism that improves existing state-of-the-art methods by not requiring prior knowledge of the attacker. This work lays the foundation to explain the evolution of client behaviours, with the potential to enhance reliability and control over FL systems.