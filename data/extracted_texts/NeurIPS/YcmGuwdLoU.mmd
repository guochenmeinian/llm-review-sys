# Real-Time Motion Prediction via Heterogeneous

Polyline Transformer with Relative Pose Encoding

 Zhejun Zhang

Computer Vision Lab

ETH Zurich

Zurich, Switzerland

zhejun.zhang@vision.ee.ethz.ch &Alexander Liniger

Computer Vision Lab

ETH Zurich

Zurich, Switzerland

alex.liniger@vision.ee.ethz.ch &Christos Sakaridis

Computer Vision Lab

ETH Zurich

Zurich, Switzerland

csakarid@vision.ee.ethz.ch &Fisher Yu

Computer Vision Lab

ETH Zurich

Zurich, Switzerland

i@yf.io &Luc Van Gool

CVL, ETH Zurich, CH

PSI, KU Leuven, BE

INSAIT, Un. Sofia, BU

vangool@vision.ee.ethz.ch

###### Abstract

The real-world deployment of an autonomous driving system requires its components to run on-board and in real-time, including the motion prediction module that predicts the future trajectories of surrounding traffic participants. Existing agent-centric methods have demonstrated outstanding performance on public benchmarks. However, they suffer from high computational overhead and poor scalability as the number of agents to be predicted increases. To address this problem, we introduce the K-nearest neighbor attention with relative pose encoding (Knarpe), a novel attention mechanism allowing the pairwise-relative representation to be used by Transformers. Then, based on Knarpe we present the Heterogeneous Polyline Transformer with Relative pose encoding (HPTR), a hierarchical framework enabling asynchronous token update during the online inference. By sharing contexts among agents and reusing the unchanged contexts, our approach is as efficient as scene-centric methods, while performing on par with state-of-the-art agent-centric methods. Experiments on Waymo and Argoverse-2 datasets show that HPTR achieves superior performance among end-to-end methods that do not apply expensive post-processing or model ensembling. The code is available at https://github.com/zhejz/HPTR.

## 1 Introduction

Motion prediction is an important component of modular autonomous driving stack . As the downstream module of perception [22; 42; 48] and the upstream module of planning [3; 9; 39], the task of motion prediction [16; 60] is to predict the multi-modal future trajectories of other agents next to the self-driving vehicle (SDV) based on the heterogeneous observations, including for example high-definition (HD) maps, traffic lights and other vehicles, pedestrians and cyclists. This is an essential task because without accurate and real-time prediction results , the planning module cannot safely and comfortably navigate the SDV through highly interactive driving environments.

To achieve top performance on public motion prediction leaderboards [1; 58], state-of-the-art (SOTA) methods [36; 47; 52; 55] leverage agent-centric vectorized representations and Transformer-based network architectures. However, the good performance of these approaches comes at a cost of high computational overhead as illustrated in Figure 1. The most well-known problem of agent-centricapproaches is the poor scalability as the number of target agents grows in urban driving environments with dense traffic; for example the busy intersection in Figure 0(a). Although the agent-centric regions of interest (ROI) in Figure 0(b) are largely overlapping, the same context is transformed to the coordinate system of each target agent and independently saved and processed. This causes a huge waste of computational resources, which is often neglected by prior works because their experiments focus on offline inference that queries the prediction only once given a scenario from the dataset. In contrast to offline inference, the motion prediction module of a real-world SDV is continuously queried with streaming inputs during online inference. For example, in Figure 0(c), the prediction module is queried again shortly after Figure 0(a). Although most contexts remain unchanged after this short period of time, existing methods would start inference from scratch without reusing the encoded features of static contexts. Moreover, prior works often predict a massive number of redundant trajectories and ensemble the outputs of numerous models, such that the final output can be adjusted in favor of prediction diversity during the post-processing, as illustrated in Figure 0(d). Although these techniques significantly boost the performance on public benchmarks, they should be sparingly used on a real-world SDV where the computational resources are scarce and the raw predictions, rather than the heuristically aggregated ones, are preferred by the downstream planning module.

To address these problems, our first step is to represent everything as heterogeneous polylines . Then we adopt the pairwise-relative representation [11; 28] and separate the local attribute from the global pose of each polyline. The local attribute specifies what the polyline actually is and it will be shared or reused if possible. The global pose specifies where the polyline is and it will be used to derive the pairwise-relative pose before being processed by the neural networks. Considering polylines as tokens, our second step is to introduce the **K**-nearest **N**eighbor **A**ttention with **R**elative **P**ose **E**ncoding (Knarpe) module, which allows Transformers  to aggregate the local contexts for each token via the local attributes and relative poses. While Knarpe enables context sharing among agents, we further propose the **H**eterogeneous **P**olyline **T**ransformer with **R**elative pose encoding (HPTR), which emphasizes the heterogeneous nature of polylines in motion prediction tasks. Based on Knarpe, HPTR uses hierarchical Transformer encoders and decoders to separate the intra-class and inter-class attention, such that tokens can be updated asynchronously during online inference. More specifically, for static polylines, such as HD maps, their features will be reused, whereas for dynamic polylines, such as traffic lights and agents, their features will be updated on demand.

Our contributions are summarized as follows: (1) We introduce Knarpe, a novel attention mechanism that enables the pairwise-relative representation to be used by Transformer-based architectures. (2) Based on Knarpe we propose HPTR, a hierarchical framework that minimizes the computational overhead via context sharing among agents and asynchronous token update during online inference. (3) Compared to SOTA agent-centric methods, we achieve similar performance while reducing the memory consumption and the inference latency by 80%. By caching the static map features during online inference, HPTR can generate predictions for 64 agents in real time at 40 frames per second. Experiments on Waymo and Argoverse-2 datasets show that our approach compares favorably against other end-to-end methods which do not apply expensive post-processing and ensembling.

Figure 1: To efficiently predict the multi-modal future of numerous agents in dense traffic (0(a)), HPTR minimizes the computational overhead by: (0(b)) Sharing contexts among target agents. (0(c)) Reusing static contexts during online inference. (0(d)) Avoiding expensive post-processing and ensembling.

Related work

Motion predictionis a popular research topic because it is an essential component of modular autonomous driving stacks . The task of motion prediction has been formulated in different ways. In this paper, we focus on the most popular one: the marginal motion prediction [6; 8; 27; 32; 33] where the multi-modal future trajectories are predicted individually for each target agent [1; 38; 58]. In contrast to the marginal formulation, joint motion prediction [19; 20; 35; 49] requires the multi-modal futures to be simultaneously generated for all target agents from the same scenario [2; 57; 66]. Beyond the open-loop motion prediction tasks, behavior simulation [44; 50; 68] is formulated in a closed-loop fashion where the future agent trajectories are simulated by rolling out a learned policy [3; 59]. Numerous works have also investigated the possibility to combine motion prediction with other modules, such as perception and planning [7; 15; 23; 29], or to learn an end-to-end driving policy [24; 25; 67] mapping sensor measurements directly to the actions or motion plans of the SDV.

Vectorized representationproposed by VectorNet  is widely used in recent works because of its promising performance [47; 52; 55]. Depending on how the coordinate system is selected, the vectorized representation falls into three categories: agent-centric, scene-centric and pairwise-relative. The most popular one is the agent-centric representation [36; 47; 52; 55] that transforms all inputs to the local coordinates of each target agent. Despite its good performance, this approach suffers from poor scalability as the number of target agents grows. To reduce the computational overhead, scene-centric representation  shares the contexts among all target agents by transforming all inputs to a global coordinate system, which is not tied to any specific agent but to the whole scene. However, its performance is poor due to the lack of rotation and translation invariance. In this paper we use the pairwise-relative representation, which is less often discussed in prior works because it has not demonstrated any clear advantage. To the best of our knowledge, there are three works using the pairwise-relative representation: HDGT , GoRela  and HiVT . HDGT and GoRela are based on Graph Neural Networks (GNNs), and their implementation requires message passing and GNN libraries. However, these libraries are often less efficiently implemented on Graphics Processing Units (GPUs) when compared to the basic matrix operations that Transformers rely on. HiVT augments the agent-centric encoders with a pairwise-relative interaction decoder in order to realize multi-agent prediction. In contrast to HiVT which uses agent-centric vectors and the standard Transformer, we formulate all inputs as pairwise-relative polylines and introduce the Knarpe attention mechanism. As a result, our HPTR demonstrates clear advantages in terms of accuracy and efficiency. Replacing the vanilla attention with our Knarpe, we can borrow ideas from other Transformer-based methods and adapt them to the pairwise-relative representation. For example, the hierarchical architecture of our HPTR is inspired by Wayformer . More sophisticated architectures and techniques, such as goal-based decoding [21; 69], can also be incorporated into our framework to further boost the performance.

Rasterized representationwas widely used in early works on motion prediction [34; 40; 45; 51]. These methods use convolutional neural networks (CNNs) to process the rasterized image of agent-centric ROI. To improve the inference efficiency, some works [5; 6; 10; 65] pre-compute the static map features and use rotated ROI alignment  to retrieve the local contexts around the target agent. In this paper, our Knarpe realizes this operation for pure Transformer-based architectures.

Transformerswith attention mechanism  have achieved great success in natural language processing [13; 43] and computer vision tasks [4; 14; 64]. Inspired by the vision Transformers, we treat polylines as high-dimensional pixels; the local attribute corresponds to color channels, whereas the global pose corresponds to the pixel-wise location. We further extend the relative position encoding [12; 46] to higher dimensions and rename it to relative pose encoding (RPE) in this paper. Although the benefit of RPE is still controversial for other tasks [54; 61], its value for motion prediction is demonstrated in this paper. Instead of using RPE, previous motion prediction Transformers use everything as input, which is equivalent to concatenating the pixel-wise location to the RGB channels. This is a very uncommon practice from the perspective of vision Transformers.

## 3 Method

In Sec. 3.1 we introduce the pairwise-relative polyline representation which enjoys the advantages of both the agent-centric and the scene-centric representation. Then in Sec. 3.2 we present the **K**-nearest **N**eighbor **A**ttention with **R**elative **P**ose **E**ncoding (Knarpe) which enables the pairwise relative polyline representation to be used by Transformers. Based on Knarpe, we propose the **H**eterogeneous **P**olyline **T**ransformer with **R**elative pose encoding (HPTR) in Sec. 3.3. HPTR uses a hierarchical architecture to prevent redundant computations and to realize the asynchronous update of heterogeneous tokens. Finally in Sec. 3.4 we discuss our output representation and training strategies.

### Pairwise-relative polyline representation

Based on prior works [11; 28], we formulate the pairwise-relative polyline representation as the third type of input representation for motion prediction. As shown in Figure 1(a), all data relevant to motion prediction can be represented as an ordered list of consecutive vectors, i.e. polylines. By transforming the polyline to the coordinate frame of its global pose as done in Figure 1(b), a polyline is fully described by a pair of global pose and local attribute. The global pose specifies the location and heading of the polyline in the global coordinate system, whereas the local attribute describes the polyline in its local coordinates, for example it is a yellow solid lane, 8 meters long, slightly curved to the right. In practice, it does not matter where the lane is on the earth; we only care about where the lane is relative to us. Hence, we obtain the relative poses from the global poses as shown in Figure 1(c), before feeding them to the prediction network. Specifically, the global pose and local attribute of polyline \(i\) are denoted as \((_{i},_{i})\). The global pose \(_{i}\) has 3 degrees of freedom \((x,y,)\), i.e. the 2D position and the heading yaw angle. The local attribute \(_{i}\) is derived from \(_{i}\), the intrinsic characteristics of the polyline, and \(_{i}\), the polyline vectors represented in the local coordinate.

The task of marginal motion prediction is to predict the future 2D positions individually for each target agent based on the static HD map (MP), the history trajectories of all agents (AG) and the traffic lights (TL). For each scenario to be predicted, we consider a maximum number of \(N_{}\) map polylines, \(N_{}\) traffic lights and \(N_{}\) agents. We define \(t=0\) to be the current step, \(\{T_{}-1,,0\}\) to be the observed history steps and \(\{1,,T_{}\}\) to be the predicted future steps. The static HD map are spatial polylines \((_{i}^{},_{i}^{},_{i}^{ {MP}}),i\{1,,N_{}\}\) where \(_{i}^{}^{3}\) is its starting vector, \(_{i}^{}^{N_{} 4}\) are the 2D positions and directions of the \(N_{}\) segments normalized against \(_{i}^{}\), and \(_{i}^{}^{C_{}}\) is the one-hot encoding of \(C_{}\) different lane types. Polygonal elements, such as crosswalks, are converted to a group of parallel polylines across the polygon. Similar to the map, history trajectories of agents are represented as spatial-temporal polylines \((_{i}^{},_{i}^{},_{i}^{ {AG}}),i\{1,,N_{}\}\) where \(_{i}^{}^{3}\) is the last observed agent pose, \(_{i}^{}^{_{}(6+3)}\) contains the history 2D positions, directions and velocities normalized against \(_{i}^{}\) as well as the 1D speed, yaw rate and acceleration which do not need to be normalized, and \(_{i}^{}^{6}\) is the 3D agent size and the one-hot encoding of 3 agent types (vehicle, pedestrian and cyclist). Following VectorNet , we use PointNet  with masked max-pooling to aggregate \((_{i}^{},_{i}^{})\) into \(_{i}^{}^{D}\) and \((_{i}^{},_{i}^{})\) into \(_{i}^{}^{D}\), where \(D\) is the hidden dimension. Since current datasets contain only the detection results rather than the tracking results of traffic lights, we consider only the traffic lights observed at \(t=0\) and represent them as singular polylines \((_{i}^{},_{i}^{}),i\{1,,N_{ {TL}}\}\) where \(_{i}^{}^{3}\) is the pose of the stop point and \(_{i}^{}^{C_{}}\) is the one-hot encoding of \(C_{}\) different states of traffic lights. We use a multi-layer perceptron (MLP) to encode \(_{i}^{}\) into \(_{i}^{}^{D}\).

Because the local attributes are normalized and the global poses are used to compute the relative poses before being consumed by the network, the pairwise-relative representation preserves the viewpoint invariance of the agent-centric representation. Additionally, since the local attributes have higher dimensions compared to the 3-dimensional global poses, sharing the local attributes enables the pairwise-relative representation to maintain the good scalability of the scene-centric representation.

Figure 2: Pairwise-relative polyline representation. MP: Map. TL: Traffic lights. AG: Agents.

However, so far this representation has only be exploited by GNNs, which are not comparable to Transformers in terms of accuracy and efficiency on the motion prediction benchmarks .

### Knarpe: **K-nearest neighbors attention with relative pose encoding**

After encoding the local attributes via the polyline-level encoders, the scenario is now described as \((_{i},_{i}),i\{1,,N\}\) where \(N=N_{}+N_{}+N_{}\) is the total number of polylines. However, this representation cannot be handled by standard self-attention because on the one hand modeling the all-to-all attention is prohibitively expensive  due to the large \(N\), and on the other hand the performance deteriorates when the input is scene-centric . To address both problems, we introduce the **K**-nearest **N**eighbors **A**ttention with **R**elative **P**ose **E**ncoding (Knarpe). Similar to MTR , Knarpe limits the attention to the K-nearest neighbors of each token. Specifically, \(_{i}^{K}(d_{i1},,d_{iN})\{1,,N\}\) is a set that contains the indices of \(K\) tokens closest to token \(i\). For simplicity, we use the L2 distance to measure the distance \(d_{ij}\) between the global poses of token \(i\) and \(j\). Instead of directly processing global poses, Knarpe uses the relative pose encoding (RPE), i.e. the positional encoding for pairwise-relative poses. Denoting \(_{ij}=(x_{ij},y_{ij},_{ij})\) to be the global pose of token \(j\) represented in the coordinate of token \(i\), i.e. \(_{j}\) transformed to the coordinate of \(_{i}\), the RPE of \(_{ij}\) is computed using sinusoidal positional encoding (PE) and angular encoding (AE) ,

\[(_{ij}) =((x_{ij}),(y_{ij}),( _{ij})),\] (1) \[_{2i}(x) =(x^{}),\,_{2i+1}(x)=(x ^{}),\] (2) \[_{2i}() =((i+1)),\,_{2i+1}()= ((i+1)),\,i\{0,,D/2-1\},\] (3)

where \(\) is the base frequency. Following , the RPE is projected and added to the keys and values to obtain \(_{i}\), the output of letting token \(i\) attend to its \(K\) neighbors \(_{i}^{K}\),

\[_{i} =}(_{i},_{j},_ {ij} j_{i}^{K})=_{j_{i}^{K}}_{ij}( _{j}^{v}+^{v}+(_{ij})}^{v}+}^{v}),\] (4) \[_{ij} =)}{_{k_{i}^{K}}(e_{ik})}, e _{ij}=_{i}^{q}+^{q})(_{j} ^{k}+^{k}+(_{ij})}^{k} +}^{k})}{},\] (5)

where \(_{ij}\) are the attention weights, \(e_{ij}\) are the logits, \(W^{\{q,k,v\}},b^{\{q,k,v\}}\) are the learnable projection matrices and biases for query, key and value, and \(^{\{k,v\}},^{\{k,v\}}\) are the learnable projection matrices and biases for RPE. We do not apply RPE to query because doing this does not boost the performance in our experiments.

Efficient implementation of Knarpe can be achieved using basic matrix operations such as matrix indexing, summation and element-wise multiplication. See the appendix for more details. Knarpe allows the pairwise-relative representation to be used by pure Transformer-based architectures. Self-attention with Knarpe aggregates the local context for each token in the same way as CNN aggregates the context around each pixel via convolutional kernels. The pixel corresponds to the token, whereas the kernel size of a CNN corresponds to the number of neighbors of Knarpe. Cross-attention with Knarpe enables rotated ROI alignment of pre-computed features, e.g. the static map features. Previously, this was only possible for CNN-based  and GNN-based  methods.

### HPTR: Heterogeneous polyline transformer with relative pose encoding

By replacing the standard multi-head attention  with our Knarpe, we construct Transformer encoders and decoders which can model the interactions between heterogeneous polylines via relative poses and local attributes. To address the marginal motion prediction task involving HD map, traffic lights and agents, we propose the **H**eterogeneous **P**olyline **T**ransformer with **R**elative pose encoding (HPTR) as shown in Figure 3. Since the temporal dimension is eliminated by the polyline-level encoder , HPTR models only the spatial relationship between tokens from different classes.

Firstly in Figure 2(a), the intra-class Transformer encoders build a block diagonal attention matrix that models the interactions within each class of tokens. Then in Figure 2(b), the inter-class Transformer decoders enhance the traffic lights tokens by making them attend to the map tokens, whereas the agent tokens are enhanced by attending to both the traffic lights and map tokens. The intuition behind this is the hierarchical nature of traffic; first there is only the map, then the traffic lights are added and finally the agents join. Intuitively, the map influences the interpretation of traffic lights but not vice versa, whereas map and traffic lights together influence the behavior of agents but not vice versa. After the inter-class Transformer decoders, the all-to-all Transformer encoder in Figure 2(c) builds a full attention matrix allowing tokens to attend to each other irrespective of their class. Finally, each agent token is concatenated with \(N_{}\) learnable anchors, which are shared among each type of agent. Since the task is marginal motion prediction, we batch over agents and anchors, i.e. now the number of anchor tokens is \(N_{} N_{}\). Then the anchor-to-all Transformer decoder in Figure 2(d) lets each of these anchor tokens attend to all tokens so as to aggregate more contextual information. The final output is denoted as \(}_{i}^{},i\{1,,N_{} N_{}\}\), and it is used to generate the multi-modal future trajectories. We set the number of neighbors to \(K\) for the intra-class and all-to-all Transformers. This number \(K\) is multiplied by \(_{}\), \(_{}\) and \(_{}\) respectively for the enhance-TL, enhance-AG and AC-to-all Transformers, such that these Transformers can have a larger receptive field.

HPTR organizes the Transformers in a hierarchical way such that some of the intermediate results can be cached and reused during the online inference; for example the outputs of the intra-MP Transformer, i.e. the static map features. This allows tokens from different classes to be updated asynchronously, which significantly reduces the online inference latency. We can further improve the efficiency without sacrificing the performance by trimming the redundant attentions. The attention matrices shown in Figure 2(a), 2(b) and 2(c) are overlapping, which means some relationships are repeatedly modeled, such as the agent-to-agent self-attention. We propose to remove the intra-TL, intra-AG and all-to-all Transformer, while keeping the intra-MP, enhance-TL and enhance-AG Transformer. These three Transformers together build a lower triangular attention matrix which is necessary and sufficient to model the relationship between map, traffic lights and agents. Our approach can be seen as an extension to Wayformer  which does not introduce the inter-class Transformer decoders. We can trim Figure 3 differently and cast HPTR into the Wayformer. Specifically, Wayformer with late fusion corresponds to HPTR with diagonal attention matrix (only intra-class TF), Wayformer with early fusion corresponds to HPTR with full attention matrix (only all-to-all TF) and Wayformer with hierarchical fusion corresponds to HPTR with the diagonal followed by the full attention matrix.

### Output representation and training strategies

We follow the common practice [36; 47; 52] to represent the outputs as a mixture of Gaussians and train with hard assignment. The multi-modal future trajectories for each agent are generated by decoding \(}_{i}^{}\), \(i\{1,,N_{} N_{}\}\) via two MLPs; the confidence head and the trajectory head. The confidence head predicts a scalar confidence of each trajectory. Besides the Gaussian parameters of 2D positions \((_{x},_{y},_{x},_{y},)\) at each future time step, our trajectory head predicts also the yaw angles, speeds and 2D velocities. We use the cross entropy loss for confidences, negative log-likelihood loss for 2D positions, negative cosine loss for yaw angles and Huber loss for speeds and 2D velocities. The final training loss is the unweighted sum of all losses. Following the hard-assignment strategy, for each agent we optimize only the predicted trajectory that is closest to the ground truth in terms of 2D average displacement error. Please refer to the appendix for more details.

Figure 3: The hierarchical architecture of HPTR. Transformers are applied in sequential order from left to right, top to down. Some attentions are redundant and can be skipped for better efficiency. We propose the lower triangular attention matrix, which excludes the intra-TL, intra-AG and all-to-all self-attentions (the transparent parts). By removing the redundant attentions, this specific lower triangular architecture enables asynchronous token update during online inference. TF: Transformer. Attn: Attention. MP: Map. TL: Traffic lights. AG: Agents.

Experiments

### Experimental setup

Benchmarks.We benchmark our method on the two most popular datasets: the Waymo Open Motion Dataset (WOMD)  and the Argoverse-2 motion forecasting dataset (AV2) . Both datasets have an online leaderboard for marginal motion prediction. However, their task descriptions are slightly different. For each scenario, WOMD evaluates the predictions of up to 8 agents, whereas AV2 evaluates only one agent. Both datasets require exactly 6 futures to be predicted for each target agent. The sampling time is 0.1 seconds for both datasets. The history length is 11 steps for WOMD and 50 steps for AV2, whereas the future length is 80 steps for WOMD and 60 steps for AV2. We use the official evaluation tool of the leaderboards to compute the metrics. For the WOMD leaderboard , the ranking metric is soft mAP; for the AV2 leaderboard , it is brief-minFDE. Please refer to the leaderboard homepages for more details about the dataset and evaluation metrics.

Implementation details.We use Transformer with pre-layer normalization  for HPTR. For each episode, we consider \(N_{}=1024\) map polylines, \(N_{}=40\) traffic light stop points and \(N_{}=64\) agents. Each polyline contains up to \(N_{}=20\) one-meter-long segments. The base number of neighbors considered by Knarpe is \(K=36\). This number is multiplied by \(_{}=2\), \(_{}=4\) and \(_{}=10\) respectively for the enhance-TL, enhance-AG and AC-to-all Transformer. We set \(N_{}=6\) to predict exactly 6 futures as specified by the leaderboard. We do not apply ensembling or expensive post-processing such as trajectory aggregation. Our post-processing manipulates only the confidences. To improve the soft mAP, we use the non-maximum suppression of MPA  for the WOMD leaderboard. To improve the brier-minFDE, we set the softmax temperature to 0.5 for the AV2 leaderboard. More details are provided in the appendix.

Training details.Thanks to the viewpoint invariance of the pairwise-relative representation, no data augmentation or input permutation is needed for the training of HPTR. We use AdamW optimizer with an initial learning rate of 1e-4 and decaying by 0.5 every 25 epochs. We train with a total batch size of 12 episodes on 4 RTX 2080Ti GPUs. For WOMD, we randomly sample 25% from all training episodes at each epoch; for AV2 we use 50%. Our final models are trained for 120 epochs for WOMD and 150 epochs for AV2. The complete training takes 10 days. For WOMD, the SDV agents, agents of interest and agents to be predicted are used for optimization. For AV2, the SDV agents, scored agents and focal agents are used for optimization. To address the imbalance between agent types, for WOMD we additionally optimize for pedestrians and cyclists which are tracked for at least 4 seconds.

### Benchmark results

In Table 1 we benchmark our approach on the public leaderboards of WOMD and AV2. From the leaderboard we observe that using ensemble and predicting redundant trajectories can increase the prediction diversity and hence improve the soft mAP, brier-minFDE and miss rate. For example, MTR-Adv-ens aggregates the outputs of 7 ablation models, each predicting 64 futures. Besides the exaggerated large number of redundant predictions, it is also non-trivial to aggregate them into 6 predictions. Some works use K-means clustering while the others use non-maximum suppression; both involve heuristic parameter fine-tuning. Since our framework is dedicated to real-world autonomous driving, applying these computationally expensive techniques is contradictory to our motivation. For a fair comparison, we focus on end-to-end methods which do not apply these techniques. On the WOMD dataset, we achieve SOTA performance among the end-to-end methods, including MTR-e2e, the end-to-end version of MTR, and MPA, the end-to-end version of MultiPath++. Specifically, HPTR outperforms the pairwise-relative HDGT and the scene-centric SceneTransformer by a large margin in all metrics. We also show competitive performance on the AV2 leaderboard. We achieve better performance in all metrics except the brier-minFDE compared to GoRea, which uses GNNs to tackle the pairwise-relative representation. Since there exists a performance gap while adapting from one dataset to another and we choose WOMD to be our main benchmark, our performance on the AV2 leaderboard could be further improved by fine-tuning the hyper-parameters for the AV2 dataset.

### Ablation study

In Table 2 we ablate different input representations and hierarchical architectures. The scene-centric baseline, HPTR SC, uses the architecture of our HPTR and the input representation of SceneTransformer . The agent-centric baseline, WF baseline, is our reimplementation of the Wayformer  with multi-axis attention and early fusion. Both baselines achieve their expected performances compared to their original implementations. The large performance gap between HPTR SC and other models confirms the disadvantage of scene-centric representation. The agent-centric baseline requires more training iterations. After convergence, its performance is on par with our HPTR. To ablate the hierarchical architecture, we implement three variations of HPTR; each corresponds to a fusion strategy investigated by Wayformer. The full attention corresponds to the early fusion, diagonal corresponds to late fusion, and HPTR with diagonal followed by full attention corresponds to Wayformer with hierarchical fusion. While the early fusion performs the best for Wayformer, for HPTR the lower triangular attention we proposed outperforms both the early and the late fusion by a significant margin. The performance of hierarchical fusion is slightly worse than ours, but its inference latency is significantly longer because of the redundancy in its attention matrices.

### Efficiency analysis and qualitative results

In Figure 4 we compare the computational efficiency of the ablation models presented in Table 2. As discussed in prior works [36; 47], one of the major drawbacks of agent-centric approaches is the poor scalability, which is reflected by the large slope of the memory and latency curves of our WF baseline. On the RTX 2080Ti, it can handle only up to 48 agents while the inference latency is 140ms. On the contrary, the scene-centric baseline is extremely efficient, but it suffers from poor accuracy. Our HPTR takes the best of both approaches. In terms of efficiency, our HPTR is comparable to the scene-centric approaches, while our performance is on par with the agent-centric approaches. Compared to the scene-centric baseline, the GPU memory consumption and the inference latency of HPTR are slightly higher because for each new agent, we have to compute its relative pose to all existing tokens. Compared to the hierarchical architectures introduced by Wayformer, HPTR with our lower triangular attention achieves the best trade-off between accuracy and latency. By reusing the static map features during the online inference, our HPTR predicts the multi-modal futures for 64 agents in 37ms without the use of inference libraries. Narrowing the receptive field, for example by

   WOMD _test_ & repr. & _soft mAP_\(\) & mAP \(\) & minADE \(\) & minFDE \(\) & miss rate \(\) \\  \({}^{*}\)MTR-Adv-ens  & AC & 0.4594 & 0.4492 & 0.5640 & 1.1344 & 0.1160 \\ \({}^{*}\)Wayformer  & AC & 0.4335 & 0.4190 & 0.5454 & 1.1280 & 0.1228 \\ \({}^{*}\)MTR  & AC & 0.4216 & 0.4129 & 0.6050 & 1.2207 & 0.1351 \\ \({}^{*}\)MultiPath++  & AC & N/A & 0.4092 & 0.5557 & 1.1577 & 0.1340 \\ 
**HPTR (Ours)** & PR & 0.3968 & 0.3904 & 0.5565 & 1.1393 & 0.1434 \\ MPA  (MultiPath++) & AC & 0.3930 & 0.3866 & 0.5913 & 1.2507 & 0.1603 \\ HDGT  & PR & 0.3709 & 0.3577 & 0.7676 & 1.1077 & 0.1325 \\ Gnet  & AC & 0.3396 & 0.3259 & 0.6207 & 1.2391 & 0.1718 \\ SceneTransformer  & SC & N/A & 0.2788 & 0.6117 & 1.2116 & 0.1564 \\  WOMD _valid_ & repr. & _soft mAP_\(\) & mAP \(\) & minADE \(\) & minFDE \(\) & miss rate \(\) \\ 
**HPTR (Ours)** & PR & 0.4222 & 0.4150 & 0.5378 & 1.0923 & 0.1326 \\ MTR-e2e & AC & N/A & 0.3245 & 0.5160 & 1.0404 & 0.1234 \\   AV2 _test_ & repr. & _brier-minFDE\({}_{6}\)\(\)_ & minFDE\({}_{6}\)\(\) & minFDE\({}_{1}\)\(\) & minADE\({}_{6}\)\(\) & miss rate\({}_{6}\)\(\) \\  \({}^{*}\)ProphNet  & AC & 1.88 & 1.33 & 4.74 & 0.68 & 0.18 \\ \({}^{}\)Gnet  & AC & 1.90 & 1.34 & 4.40 & 0.69 & 0.18 \\ \({}^{}\)TENET  & AC & 1.90 & 1.38 & 4.69 & 0.70 & 0.19 \\ \({}^{*}\)MTR  & AC & 1.98 & 1.44 & 4.39 & 0.73 & 0.15 \\ GOReIA  & PR & 2.01 & 1.48 & 4.62 & 0.76 & 0.22 \\
**HPTR (Ours)** & PR & 2.03 & 1.43 & 4.61 & 0.73 & 0.19 \\ THOMAS  & AC & 2.16 & 1.51 & 4.71 & 0.88 & 0.20 \\ HDGT  & PR & 2.24 & 1.60 & 5.37 & 0.84 & 0.21 \\   

Table 1: Results on the marginal motion prediction leaderboards of WOMD and AV2. Both tables are sorted according to the ranking metric such that the best performing method is on the top. The ranking metric is _soft mAP_ for WOMD, and _brier-minFDE\({}_{6}\)_ for AV2. \(\) denotes ensemble. * denotes predicting more futures than required. SC: scene-centric. AC: agent-centric. PR: pairwise-relative.

reducing \(_{}\) from 10 to 8, can improve the inference speed but the accuracy might be affected in some cases. Using half precision at inference time can reduce the latency to 25ms (40 fps) without affecting the accuracy. Compared to the most efficient agent-centric method Wayformer, we reduce the memory consumption and the online inference latency by 80% without sacrificing the accuracy.

In Figure 5 we compare the computational efficiency of our method with the GNN-based pairwise-relative methods. Currently there are two such methods, GoRela  and HDGT . While HDGT does not match the prediction accuracy of GoRela or our method, it is worth noting that GoRela is not open-sourced, whereas HDGT is. Therefore, we use HDGT in this efficiency comparison. The left plot of Figure 5 confirms the good scalability of HDGT in terms of GPU memory. This is expected because it uses the pairwise-relative representation. In the middle plot, we can observe that HDGT is slower than both our HPTR and our agent-centric Wayformer baseline in terms of offline inference speed. To confirm that HDGT runs correctly on our setup, in the right plot we reproduce the inference time of HDGT on the complete WOMD validation split with different validation batch size and we compare the reproduce numbers with the reported number in the HDGT paper. The slow inference speed of GNN-based methods such as HDGT is mainly because GNN libraries cannot utilize the GPU as efficiently as the basic matrix operations do. Our KNARPE is implemented with the most basic matrix operations, hence it is better suited for real-time and on-board applications.

Figure 6 illustrates the qualitative results of our HPTR. For each type of agent, we select a successful case where the most confident prediction matches the ground truth, and a failure case to show the limitation of our method. In the successful cases, we observe the vehicle stops at the red light, the pedestrian walks along the road edge with another person, and the cyclist rides across the road via the

   WOMD _valid_ &  input \\ repr. \\  &  intra \\ MP \\  &  intra \\ TL/AG \\  &  enhance \\ TL/AG \\  & 
 all \\ 2all \\  & minFDE \(\) & _soft mAP \(\)_ \\ 
**HPTR (Ours)** & PR & ✓ & \(\) & ✓ & \(\) & \(1.145 0.016\) & \(0.399 0.010\) \\ WF baseline (100-epoch) & AC & \(\) & \(\) & \(\) & ✓ & \(1.161 0.006\) & \(0.397 0.007\) \\ HPTR diag+full (WF hier.) & PR & ✓ & ✓ & \(\) & ✓ & \(1.156 0.014\) & \(0.391 0.002\) \\ HPTR diag (WF late) & PR & ✓ & ✓ & \(\) & \(\) & \(1.169 0.013\) & \(0.387 0.012\) \\ HPTR full (WF early) & PR & \(\) & \(\) & \(\) & ✓ & \(1.158 0.093\) & \(0.386 0.041\) \\ WF baseline & AC & \(\) & \(\) & \(\) & ✓ & \(1.212 0.019\) & \(0.378 0.014\) \\ HPTR SC & SC & ✓ & \(\) & ✓ & \(\) & \(1.687 0.046\) & \(0.246 0.005\) \\   

Table 2: Ablation on the valid split of WOMD. The table is sorted according to the _soft mAP_ such that the best-performing method is on the top. Performances are reported as the mean plus-minus 3 standard deviations over 3 training seeds. Models are trained for 60 epochs if not otherwise mentioned. WF: Wayformer. SC: scene-centric, AC: agent-centric, PR: pairwise-relative.

Figure 4: HPTR is as efficient as scene-centric methods in terms of GPU memory consumption and inference latency, while being as accurate as agent-centric methods. We use standard Ubuntu, Python and Pytorch without optimizing for real-time deployment. We predict one scenario at each inference time on one 2080Ti, i.e. we batch over scenarios and the batch size is 1. The number of context agents is the same as the number of predicted agents for all experiments. During offline inference, every inference starts from scratch, while during online inference, we cache and reuse the static map features. Specifically, we repeat the inference of the same scenario for 100 times to simulate online inference, where the static map features are computed at the first step and reused for the next 99 steps.

crosswalk. Although in the failure cases the most confident prediction deviates from the ground truth, we are encouraged to see that our predictions are still reasonable. The failure cases can be addressed by improving the prediction diversity via goal-conditioning, which is orthogonal to our contributions.

## 5 Conclusion

In this paper we introduce a novel attention module, Knarpe, that allows the pairwise-relative representation to be used by Transformers. Based on Knarpe, we present a pure Transformer-based framework called HPTR, which uses hierarchical architecture to enable asynchronous token update and avoid redundant computations. While agent-centric methods suffer from poor scalability and scene-centric methods suffer from poor accuracy, our HPTR gets the best from both worlds. Experiments on the two most popular benchmarks show that our approach achieves superior performance, while satisfying the real-time and on-board requirements of real-world autonomous driving.

Limitations.The poses in this work reside on a 2D plane, which can be extended to the 3D space in the future. For simplicity, we use L2 distance to obtain the K-nearest neighbors. Future works can explore more sophisticated distances which involve map topology. Currently we use the most basic anchor-based decoder which has limited diversity. This can be improved by using more advanced decoding techniques, such as goal-conditioning. In this paper we focus on marginal motion prediction. It would be interesting to investigate the potential of our approach in other prediction tasks.

Figure 5: Efficiency comparison with HDGT. We run their official repository on our machine with a single 2080Ti. The left plot shows that HDGT achieves good scalability in terms of GPU memory consumption as expected. The middle plot shows that the offline inference latency (with batch size 1) of HDGT scales well, but it is significantly larger than that of other Transformer-based methods. The right plot shows the inference times for the complete WOMD validation split (64 agents per episode) with different validation batch sizes. It confirms the inference speed reported in the original HDGT paper is correctly reproduced on our setup. Our setup is faster because it has a more powerful CPU.

Figure 6: Qualitative results of HPTR. For each type of agent we show a successful case (top) and a failure case (bottom). The intersections are cluttered because we visualize traffic lights by overlaying the lanes they control with their color. The ground truth is in orange. The target agent and the predictions are in cyan. The most confident prediction has the least transparent color, the thickest line and the biggest cross. Please refer to the appendix for a detailed explanation of the visualization.