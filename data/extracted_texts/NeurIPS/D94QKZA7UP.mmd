# A One-Size-Fits-All Approach to Improving Randomness in Paper Assignment

Yixuan Even Xu\({}^{1}\) Steven Jecmen\({}^{2}\) Zimeng Song\({}^{3}\) Fei Fang\({}^{2}\)

\({}^{1}\)Tsinghua University

\({}^{2}\)Carnegie Mellon University

\({}^{3}\)Independent Researcher

xuyx20@mails.tsinghua.edu.cn

{sjecmen,feif}@cs.cmu.edu

zmsongzm@gmail.com

This work was done when Xu was a visiting intern at Carnegie Mellon University.

###### Abstract

The assignment of papers to reviewers is a crucial part of the peer review processes of large publication venues, where organizers (e.g., conference program chairs) rely on algorithms to perform automated paper assignment. As such, a major challenge for the organizers of these processes is to specify paper assignment algorithms that find appropriate assignments with respect to various desiderata. Although the main objective when choosing a good paper assignment is to maximize the expertise of each reviewer for their assigned papers, several other considerations make introducing randomization into the paper assignment desirable: robustness to malicious behavior, the ability to evaluate alternative paper assignments, reviewer diversity, and reviewer anonymity. However, it is unclear in what way one should randomize the paper assignment in order to best satisfy all of these considerations simultaneously. In this work, we present a practical, one-size-fits-all method for randomized paper assignment intended to perform well across different motivations for randomness. We show theoretically and experimentally that our method outperforms currently-deployed methods for randomized paper assignment on several intuitive randomness metrics, demonstrating that the randomized assignments produced by our method are general-purpose.

## 1 Introduction

Peer review is the process in which submissions (such as scientific papers) are evaluated by expert reviewers. It is considered a critical part of the scientific process and is commonly used to determine which papers get published in journals and conferences. For concreteness, we set this work in the academic conference setting, although the approach can be generalized to other settings, such as peer review for grant proposals and peer grading in classrooms. Due to the large scale of modern conferences like NeurIPS and AAAI, conference program chairs work closely with assignment algorithms to assign papers to reviewers automatically. Among many other challenges involved in managing huge numbers of reviewers and submissions, these organizers are faced with the difficult task of balancing various considerations for the paper assignment. Human-friendly automated paper assignment algorithms are thus crucial for helping them find desirable paper assignments.

In a standard paper assignment setting, a set \(\) of \(n_{p}\) papers need to be assigned to a set \(\) of \(n_{r}\) reviewers. To ensure each paper gets enough reviewers and no reviewer is overloaded with papers, each paper \(p\) in \(\) should be assigned to \(_{p}\) reviewers and each reviewer \(r\) in \(\) should receive no more than \(_{r}\) papers. An assignment is represented as a binary matrix \(\) in \(\{0,1\}^{n_{p} n_{r}}\), where \(x_{p,r}=1\) indicates that paper \(p\) is assigned to reviewer \(r\). The main objective of paper assignment is usually to maximize the predicted match quality between reviewers and papers . To characterize this,a similarity matrix \(\) in \(_{ 0}^{n_{p} n_{r}}\) is commonly assumed . Here, \(S_{p,r}\) represents the predicted quality of review from reviewer \(r\) for paper \(p\) and is generally computed from various sources : reviewer and paper subject areas, reviewer-selected bids, and textual similarity between the paper and the reviewer's past work . Then, the **quality** of an assignment can be defined as the total similarity of all assigned paper reviewer pairs, i.e., \(()=_{p,r}x_{p,r}S_{p,r}\). One standard approach for computing a paper assignment is to maximize quality  (which we will refer to as the maximum-quality assignment). Variants of this approach have been widely used by existing conferences, such as NeurIPS, AAAI, and ICML .

While the deterministic maximum-quality assignment is the most common, there are strong reasons to introduce randomness into paper assignment - that is, to determine a probability distribution over feasible deterministic assignments and sample one assignment from the distribution. Specifically, randomized paper assignments are beneficial due to the following motivations:

* **Motivation 1: Robustness to malicious behavior.** Several computer science conferences have uncovered "collusion rings" of reviewers and authors , in which the reviewers aim to get assigned to the authors' papers in order to give them good reviews without considering their merits. By manipulating their stated expertise and interest (e.g., in the "paper bidding" process), these reviewers can cause the assignment algorithm to believe that their match quality with the targeted papers is very high. In other cases, reviewers may target assignment to a paper with the aim of giving it an unfair negative review . Randomization can reduce the probability that a malicious reviewer achieves assignment to a target paper.
* **Motivation 2: Evaluation of alternative assignments.** Accurate reviewer-paper similarity scores are fundamental for automated paper assignment algorithms. Despite this, these scores are currently computed using various different methods by different conferences , with no obvious way to tell beforehand which method produces the best-quality reviews. However, after deploying an assignment, it is possible to examine the resulting reviews to counterfactually evaluate the review quality produced by another method of similarity computation that is not deployed. Specifically, using techniques for off-policy evaluation , the randomness of the deployed paper assignment can be utilized to estimate the review quality of another non-deployed assignment. The variance of the estimation depends on the overlap in assignment probability between the deployed (randomized) assignment and the non-deployed alternative assignments of interest. Such an evaluation can then inform program chairs on how similarities should be computed (or how other algorithmic choices should be made) in the future.
* **Motivation 3: Reviewer diversity.** As each paper is evaluated by multiple reviewers, it is often desirable to assign a set of reviewers with diverse perspectives or areas of expertise. However, since maximum-quality assignments compute only a holistic score to represent the expertise of each reviewer-paper pair, they do not consider this factor. Randomization can increase diversity by spreading out assignment probability among a larger set of high-expertise reviewers.
* **Motivation 4: Reviewer anonymity.** In peer review, reviewer identities are hidden from authors so that authors cannot retaliate for negative reviews. As a result, conferences are generally reluctant to release paper assignment data since authors may be able to deduce the identities of their reviewers (even if reviewer names and other information are hidden). By sufficiently randomizing the assignment, conferences can make it difficult for authors to identify any reviewer on their paper with high probability from the assignment data.

Despite the significance of randomness in paper assignment, there is very limited prior work that looks into computing randomized assignments. A notable exception is , which proposed an algorithm for computing randomized paper assignments: **Probability Limited Randomized Assignment (PLRA)**. Formally, it represents a randomized assignment as a matrix \(\) in \(^{n_{p} n_{r}}\), where \(x_{p,r}\) denotes the marginal probability that paper \(p\) is assigned to reviewer \(r\). PLRA computes a randomized assignment via the following linear program (LP), defined for a given parameter \(Q\) as:

\[&()=_{p,r}x_{p,r}S_{p,r}\\ &_{r}x_{p,r}=_{p}\\ &_{p}x_{p,r}_{r}\\ &0 x_{p,r} Q p,\\  r,\\  p,r,\] (PLRA)where \(()\) for a randomized assignment \(\) is the expected total similarity of the assignment. A deterministic assignment can then be sampled, using the fact that any feasible randomized assignment \(\) can be implemented as a distribution over feasible deterministic assignments [22; 23].

PLRA is primarily concerned with the first motivation for randomization: robustness to malicious behavior. By limiting each entry of \(\) to be at most \(Q\), PLRA guarantees that any malicious reviewer aiming to be assigned to a target paper has at most probability \(Q\) to succeed, even if the reviewer and paper are chosen adversarially. The hyperparameter \(Q\) can be adjusted to balance the loss in quality and level of randomization. PLRA has been deployed in multiple iterations of the AAAI conference  and is implemented at the popular conference management system OpenReview.net .

However, PLRA does not fully solve the problem of randomized paper assignment. In particular, PLRA is specific to one metric of randomness: the maximum assignment probability across all paper-reviewer pairs. As a result, it is not clear how well PLRA aligns with motivations for randomization other than robustness to malicious behavior. Moreover, PLRA does not distinguish between different solutions with the same quality and maximum assignment probability. This means that it often loses opportunities to add additional randomness since it neglects to consider non-maximum assignment probabilities. One way to remedy this issue is to allow different values of \(Q\) to be set for each pair \((p,r)\), as in the original formulation of . However, this level of flexibility makes it a significant burden for program chairs to manually choose appropriate \(Q\) values, hindering the usability of the algorithm. As a result, only the single-\(Q\) version stated above has been deployed in practice.

In this work, we address the problem of randomizing paper assignment by looking for a simple and practical method of achieving general-purpose randomized paper assignments. We consider various intuitive randomness metrics that are relevant to all above motivations but not overly specific to a particular problem formulation, and aim to provide a method that performs well across these metrics. In this way, we can provide a method for randomized paper assignment that conference program chairs can easily deploy without needing to precisely specify objectives or hyperparameters.

More specifically, we make the following contributions in this work. **(1).** We define several metrics to measure the extent to which the randomization in a randomized paper assignment satisfies the stated motivations (Section 3). **(2).** We propose Perturbed Maximization (PM), a practical algorithm for randomized paper assignment that does not rely on any specific formulation of the stated motivations (Section 4). While our algorithm can be implemented using a standard convex optimization solver, we additionally propose an approximate implementation that is computationally cheaper. **(3).** We provide theoretical results showing that PM provably outperforms PLRA on two classes of structured similarity scores (Section 5). **(4).** We extensively evaluate our algorithm via experiments using two realistic datasets of similarity scores from AAMAS 2015 and ICLR 2018 (Section 6). PM simultaneously performs well on all defined randomness metrics while sacrificing a small amount of quality as compared to the optimal non-randomized assignment. Additionally, our experiments show that PM achieves good performance when hyperparameters are set based only on the desired assignment quality, ensuring that it is simple for program chairs to deploy in practice.

## 2 Related Work

This work follows a recent area of research in computer science on paper assignment algorithms for peer review. Building on the standard approach of maximum-similarity assignment, algorithms have been proposed to handle various additional considerations in the paper assignment: the fairness across papers , the seniority of reviewers , review processes with multiple phases , strategyproofness [25; 26], and many others . We note here specifically two relevant lines of work.

One motivation for our randomized assignment algorithm is to provide robustness to malicious reviewers, a problem considered by several past works [27; 28]. Although our work most closely relates to the randomized paper assignment algorithm proposed in , other non-randomized approaches to the problem exist. Leyton-Brown et al.  describe the paper assignment process used at AAAI 2021, which included several additional soft constraints intended to curb the possibility of reviewer-author collusion rings. Wu et al.  propose fitting a model of reviewer bidding in order to smooth out irregular bids. Boehmer et al.  consider computing paper assignments without short-length cycles in order to prevent quid-pro-quo agreements between reviewers.

Randomization in the paper assignment process has also been used for evaluating different assignment policies. Traditionally, conferences will sometimes run randomized controlled experiments in order to test policy changes, where the randomization is incorporated in the assignment of reviewers to different experimental conditions. For example, WSDM 2017 randomly separated reviewers into single- and double-blind conditions in order to evaluate the benefits of double-blind reviewing ; other notable experiments include NeurIPS 2014 [32; 33], ICML 2020 , and NeurIPS 2021 . In contrast, recent work by Saveski et al.  proposes a method for evaluating alternative assignment policies by leveraging randomization in the paper assignment itself, such as the randomized paper assignments of . By introducing additional randomness at a low cost, our algorithm provides an improved basis for the methods of  and potential future counterfactual policy evaluation methods.

## 3 Metrics for Randomness and Problem Statement

In Section 1, we introduced several motivations for choosing randomized paper assignments: robustness to malicious behavior, evaluation of alternative assignments, reviewer diversity, and reviewer anonymity. These indicate that assignment quality is not the only objective that should be considered when choosing a paper assignment. In this section, we propose several metrics to characterize the extent to which the randomness in a paper assignment is practically useful.

One randomness metric considered by PLRA is **maximum probability**, defined as \(()=_{p,r}\{x_{p,r}\}\). PLRA controls \(\) in order to trade off between quality and randomness, and thus achieves the greatest possible quality for a fixed level of \(\). However, as the following example shows, this metric alone is not sufficient to fully characterize the randomness of a paper assignment since it ignores the structure of the assignment with non-maximum probability.

Fig. 1 depicts a mini-conference consisting of 2 subject areas. Subject area A contains 3 papers and 3 reviewers, whereas subject area B contains 2 of each. The similarities between reviewers and papers within the same subject area are 1 and similarities across subject areas are 0. The constraints are \(_{p}=_{r}=1\), i.e., one-to-one assignment. Intuitively, the best randomized assignment matches papers and reviewers uniformly at random within each subject area, like Fig. 0(a), since this does not sacrifice any quality. However, PLRA fails to give out this ideal assignment regardless of the hyperparameter choice. Specifically, \(Q 1/2\) is required for PLRA to get a maximum-quality assignment, but when \(Q 1/2\), PLRA considers Fig. 0(a) and Fig. 0(b) to be equivalent (in terms of objective value). In fact, infinitely many solutions with the same optimal quality are considered equivalent by PLRA, and current LP solvers tend to return Fig. 0(b) as it is a vertex solution when \(Q=1/2\). In essence, PLRA is leaving "free randomness on the table," with many practical implications.

Consider the problem of mitigating malicious behavior (Motivation 1), and compare the assignments in Figs. 0(a) and 0(b). Although they appear to have the same \(\), when we only look at subject area A, the \(\) of Fig. 0(a) is lower than that of Fig. 0(b). This indicates that Fig. 0(a) is more robust to malicious behavior within subject area A. Therefore, Fig. 0(a) is more desirable.

Moreover, as introduced in Motivation 2, randomness can also be leveraged to evaluate alternative paper assignments using observations of the review quality from a deployed, randomized assignment. These techniques rely on a "positivity" assumption: paper-reviewer pairs assigned in the alternative assignment must be given non-zero probability in the deployed assignment. Thus, if we spread assignment probability more uniformly among more reviewer-paper pairs, the resulting data can be

Figure 1: Example showing the limitations of PLRA. There are 2 subject areas with 5 papers and 5 reviewers. \(_{p}=_{r}=1\). The similarity of paper-reviewer pair is 1 if they are in the same area. The edge weights denote the assignment probability. (a) shows the ideal assignment with optimal quality. (b) shows an assignment by PLRA with \(Q=\), which is less randomized than (a) in subject area A.

used to evaluate more varied strategies with tighter bounds. In this sense, the assignment in Fig. 0(a) will allow us to better estimate the quality of different paper assignments within subject area A.

The failure of PLRA on such a simple example shows that \(\) alone is an inadequate metric. Thus, we need other metrics to distinguish between assignments like Fig. 0(a) and Fig. 0(b). We therefore propose, in addition to \(\), a set of new randomness metrics to capture the neglected low-probability structure of an assignment. Under each of these metrics, a uniform assignment is considered "more random" than any other assignment, thus distinguishing Fig. 0(a) from Fig. 0(b).

1. **Average maximum probability:**\(()=}_{p}_{r}\{x_{p,r}\}\). With respect to the motivation of preventing malicious behavior, this randomness metric corresponds to the case when a target paper is randomly chosen and the reviewer targeting assignment to that paper is adversarially chosen. By minimizing average maximum probability, we will limit the success probability of manipulation in that case.
2. **Support size:**\(()=_{p,r}[x_{p,r}>0]\). Support size directly relates to the "positivity" assumption introduced above. As there may be many alternative paper assignments of interest, maximizing the support size effectively maximizes the quality of estimation across them.
3. **Entropy:**\(()=-_{p,r}x_{p,r}(x_{p,r})\). In information theory, entropy characterizes the uncertainty of a random variable. By maximizing entropy, we maximize the uncertainty of our assignment, corresponding to the idea of maximizing randomness. Note that strictly speaking, assignment \(\) is not a probability distribution, so this definition is a generalization.
4. **L2 norm:**\(()=x_{p,r}^{2}}\). To prevent manipulation, PLRA limits the assigned probability of each pair to be at most a specified value \(Q\). L2 norm relaxes this constraint to a soft one: a higher probability results in a larger loss. Note that a uniformly random assignment will always have the smallest L2 norm, and so minimizing L2 norm pushes the assignment towards the uniform assignment.

The combination of these metrics more comprehensively captures the impact of randomness on the motivations from Section 1. Moreover, they do so without requiring a specific problem formulation for each motivation (e.g., an assumption on the behavior of malicious reviewers, a list of the alternative assignments of interest), which are impractical or infeasible to accurately specify in practice.

**Problem statement.** For an input instance \((n_{p},n_{r},_{p},_{r},)\), we want to find an algorithm achieving a good trade-off between quality and randomness. Specifically, let the maximum possible quality be \(M\). For a given lower bound of assignment quality \( M\) (\(\)), we want the algorithm to produce an assignment \(\) with lower \((),(),( )\) and higher \((),()\), i.e., a good Pareto-frontier of quality and randomness.

## 4 Perturbed Maximization

In this section, we present our proposed algorithm for randomized paper assignment. To describe it, we first present some definitions.

**Definition 4.1** (Perturbation Function).: _A function \(f:\) is a **perturbation function** if (i) \(f(0)=0\), (ii) \(f^{}\) exists, (iii) \(f\) is non-decreasing on \(\) and (iv) \(f\) is concave on \(\)._

**Definition 4.2** (Perturbed Quality).: _For an assignment \(\), its **perturbed quality** with respect to perturbation function \(f\) on instance \((n_{p},n_{r},_{p},_{r},)\) is \(_{f}()=_{p,r}S_{p,r} f(x_{p,r})\)._

The definition of perturbed quality incorporates the intuition from the motivating example in the previous section. As function \(f\) is concave, the marginal increase of \(_{f}()\) from increasing a specific entry \(x_{p,r}\) is diminishing as \(x_{p,r}\) grows. Consequently, the assignment of Fig. 0(a) will have a higher perturbed quality than that of Fig. 0(b). This naturally gives our new algorithm, **Perturbed Maximization (PM)**. For a given parameter \(Q\) and a perturbation function \(f\):

\[&_{f}()=_{p,r }S_{p,r} f(x_{p,r})&\\ &_{r}x_{p,r}=_{p}& p,\\ &_{p}x_{p,r}_{r}& r,\\ &0 x_{p,r} Q& p,r.\] (PM)Note that PM is a class of algorithms induced by different perturbation functions. When the perturbation function \(f\) is chosen to be a linear function, PM becomes PLRA. In the main experiments of this paper, two specific perturbation functions are considered: **(1). Exponential Perturbation Function:**\(f(x)=1-e^{- x}\) where \((0,+)\) and **(2). Quadratic Perturbation Function:**\(f(x)=x- x^{2}\) where \(\). Respectively, we will denote PM with \(f(x)=1-e^{- x}\) and \(f(x)=x- x^{2}\) as **PM-Exponential (PM-E)** and **PM-Quadratic (PM-Q)** in the rest of the paper.

In Section 6, we will empirically show that under our settings of hyperparameters, PM-E and PM-Q have almost identical performances on every metric we consider, which suggests that the specific form of the perturbation function has limited impact as long as it is strictly concave. Therefore, there is likely no need to consider many different types of perturbation functions.

To analyze PM, first notice that the concaveness of function \(f\) guarantees that the optimization program is concave. Therefore, we can use standard concave optimization methods like gradient ascent or the ellipsoid method to solve the program in polynomial time. In most of the experiments of this paper, we will use Gurobi , a well-known commercial solver, to solve PM. While solving PM as a general concave optimization problem is conceptually convenient, doing so also incurs a high time complexity as there are \(n_{p} n_{r}\) variables. To further speed up the execution of PM, we propose Algorithm 1, a network-flow-based approximation of PM.

**Algorithm 1**: Network-Flow-Based Approximation of PM

For a given parameter \(Q\), a perturbation function \(f\) and a precision \(w^{+}\):

1. Construct a graph \(G\) with a source \(s\), a sink \(t\).
2. For paper \(p\), add a vertex \(v_{p}\) and an edge \(s v_{p}\) with capacity \(_{p} w\) and cost \(0\).
3. For reviewer \(r\), add a vertex \(v_{r}\) and an edge \(v_{r} t\) with capacity \(_{r} w\) and cost \(0\).
4. For a reviewer-paper pair \((p,r)\), add \( Q w\) edges \(v_{p} v_{r}\). The \(i\)-th edge has capacity \(1\) and cost \(S_{p,r}[f()-f()]\). Let this set of edges be \(E_{p,r}\).
5. Run maximum cost maximum flow algorithm  on \(G\). Define the assignment \(\) such that \(x_{p,r}=[\)The total flow on edges in \(E_{p,r}]/w\).
6. Enumerate all pairs \((p,r)\) in any order and increase \(x_{p,r}\) by \(\{Q-x_{p,r},_{p}-_{r}x_{p,r},_{r}-_{p}x_{p,r}\}\). If \(_{p,r}x_{p,r}=n_{p}_{p}\), return \(\). Otherwise, return infeasible.

**Algorithm 1**: Network-Flow-Based Approximation of PM

At a high level, Algorithm 1 uses a piecewise linear function with \(w\) pieces to approximate the concave function \(f(x)\) and solves the approximated objective with maximum cost maximum flow. As we increase the precision \(w\), the approximation becomes more accurate, but the running time of the algorithm also scales up. Formally, we have the following Theorem 1.

**Theorem 1**.: _Let \(w^{+}\) be the precision in Algorithm 1, and \(\) be the optimal perturbed quality._

1. _Algorithm 1 runs in_ \(O(w_{p}}^{2} n_{r})\) _time._
2. _Algorithm 1 returns an assignment with perturbed quality_ \(-f()_{p,r}S_{p,r}\)_._

The proof of Theorem 1 is deferred to Appendix B.1.

Consider the running time of Algorithm 1 given in Theorem 1 (a). If we directly model PM as an optimization problem, the number of variables will be \(n=n_{p} n_{r}\). The state-of-the-art algorithm for solving a linear program with \(n\) variables to high accuracy has a time complexity of \(O^{*}(n^{2.38} n)\). In contrast, for fixed \(w\), Algorithm 1 works in \(O(n^{2})\) time since \(_{p} n_{r}\). Thus, Algorithm 1 has a better time complexity than directly solving PM as an optimization program even if the objective is linear. For non-linear perturbation functions, the time complexity of Algorithm 1 remains the same while the complexity of solving the optimization program increases. Moreover, by Theorem 1 (b), we can see that as \(w\) increases, the approximated perturbed quality \(\) approaches \(\), formalizing the intuition that as we increase the precision \(w\), the approximation becomes more accurate. In Section 6, we empirically evaluate the running time and the approximation accuracy of Algorithm 1. We find with \(w=10\), Algorithm 1 produces decently-accurate approximations on our datasets.

Theoretical Analysis

In this section, we provide two theorems showing that PM provably outperforms PLRA on a general class of input instances. We start with the simpler one inspired by the example in Fig. 1. Let \((Q)\) and \((Q,f)\) be the set of possible solutions of PLRA and PM respectively.

**Definition 5.1** (Blockwise Dominant Matrix).: _A similarity matrix \(_{ 0}^{n_{p} n_{r}}\) is **blockwise dominant** with **block identity \(_{ 0}^{k k}\) and **block sizes \(\{p_{1},,p_{k}\},\{r_{1},,r_{k}\}\)** if_

\[=A_{1,1}_{p_{1} r_{1}}&A_{1,2} _{p_{1} r_{2}}&&A_{1,k}_{p_{1} r _{k}}\\ A_{2,1}_{p_{2} r_{1}}&A_{2,2}_{p_{2} r _{2}}&&A_{2,k}_{p_{2} r_{k}}\\ &&&\\ A_{k,1}_{p_{k} r_{1}}&A_{k,2}_{p_{k} r _{2}}&&A_{k,k}_{p_{k} r_{k}},\]

_where \(A_{i,j}>A_{i,j}, i j\) and \(_{p r}\) is a \(p r\) matrix with all entries being \(1\). Moreover, define the **dominance factor** of \(\) as \(()=\{ A_{i,j} A_{i,j},  i j\}\)._

**Remark.** Like Fig. 1, a blockwise dominant similarity matrix models a conference with \(k\) subject areas where the \(i\)-th subject area has \(p_{i}\) papers and \(r_{i}\) reviewers. The similarity between a paper and a reviewer is determined only by their subject areas, and a paper has highest similarity with a reviewer in the same subject area. Note that \(()>1\) as \(A_{i,i}>A_{i,j}, i j\).

**Theorem 2**.: _For an input instance \((n_{p},n_{r},_{p},_{r},)\), where \(\) is blockwise dominant with block identity \(_{ 0}^{k k}\) and block sizes \(\{p_{1},,p_{k}\},\{r_{1},,r_{k}\}\), assume (i) \(\{r_{1},,r_{k}\}\) are not all equal, (ii) \(p_{i}_{p} r_{i}_{r}\)\( i\{1,,k\}\) and (iii) \(Q r_{i}_{p}\)\( i\{1,,k\}\). Let \(f\) be a strictly concave perturbation function and \(f^{}(0)<()f^{}(1)\). PM with \(f(x)\) as the perturbation function (weakly) dominates PLRA in quality and all randomness metrics. Formally,_

* \((Q,f)\)_,_ \((Q)\)_,_ \[()(),( )(),() (),\] \(()(),()(),()( ).\]
* \((Q,f)\)_,_ \((Q)\)_,_ \[()<(),()<(),()> ().\]

The proof of Theorem 2 follows the same intuition as the example in Fig. 1. The assumptions guarantee that an assignment like Fig. 1a, i.e., uniformly matching papers to reviewers within the same subject area, is feasible. Details of the proof are deferred to Appendix B.2. At a high level, Theorem 2 shows that with a slight restriction on the perturbation function, PM provably performs better than PLRA on blockwise dominant similarity matrices.

Theorem 2 requires a strict restriction on \(\)'s structure. In our next theorem, we remove the restriction, stating that PM also outperforms PLRA on a random similarity matrix with high probability.

**Theorem 3**.: _For an input instance \((n_{p},n_{r},_{p},_{r},)\), where each entry of \(\) is i.i.d. sampled from \(\{v_{1},,v_{k}\}\)\((0<v_{1}<<v_{k})\) uniformly, assume (i) \(k n_{p}\), (ii) \(_{r} c(n_{r})\), (iii) \(2 n_{p}_{p} n_{r}_{r}\) and (iv) \(Q(n_{r}-1)_{p}\). Let \(f\) be a strictly concave perturbation function and \(f^{}(0)<}{v_{i-1}}f^{}(1), i\{2,,k\}\). With probability \(1-e^{-(c)}\), PM with \(f(x)\) as the perturbation function (weakly) dominates PLRA in quality and all randomness metrics. Formally,_

* \((Q,f)\)_,_ \((Q)\)_,_ \[()(),( )(),() (),\] \(()(),()(),()( ).\]
* \((Q,f)\)_,_ \((Q)\)_,_ \[()>(),()<(),()> ().\]

The complete proof of Theorem 3 is deferred to Appendix B.3. To sketch the proof, we will first relate PLRA and PM to two simpler auxiliary algorithms using a concentration inequality and then prove the dominance between them. The assumptions (ii) and (iii) are used to connect PM and PLRA with the auxiliary algorithms, while (i) and (iv) are for proving the dominance.

For Theorem 3 to hold on general random similarity matrices, we have made a major assumption (i), which requires that there are not too many distinct levels of similarity scores. Assumption (i) is naturally satisfied in some cases. For example, when the similarities are derived purely from reviewers' bids, the number of levels becomes the number of discrete bid levels; discrete values can also result when computing similarities from the overlap between reviewer- and author-selected subject areas. When the similarity scores are instead computed from continuous values like TPMS scores , assumption (i) is usually not satisfied. In Section 6, we show experiments on both discrete and continuous similarities to evaluate the effect of assumption (i).

## 6 Experiments

**Datasets.** In this section, we test our algorithm on two realistic datasets. The first dataset is bidding data from the AAMAS 2015 conference . In this dataset, \(n_{p}=613,n_{r}=201\), and the bidding data has 4 discrete levels: "yes", "maybe", "no" and "conflict". We transform these 4 levels to similarities \(1\), \(\), \(\) and \(0\), as inspired by NeurIPS 2016, in which similarity scores were computed as \(2^{}(0.5s_{}+0.5s_{})\). The transformed dataset satisfies assumption (i) in Theorem 3 as there are only 4 levels. The second dataset contains text-similarity scores recreated from the ICLR 2018 conference with \(n_{p}=91,n_{r}=2435\). These scores were computed by comparing the text of each paper with the text of each reviewer's past work; we directly use them as the similarity matrix. Assumption (i) is not valid in this dataset as it is continuous-valued. The constraints are set as \(_{p}=3,_{r}=6\) for ICLR 2018 as was done in  and \(_{p}=3,_{r}=12\) for AAMAS 2015 for feasibility. In Appendix A.3, we also test our algorithm on four additional datasets from .

**Experiment and hyperparameter setting.** We implement PLRA and two versions of PM (PM-E and PM-Q) using commercial optimization solver Gurobi 10.0 . For each algorithm on each dataset, we use a principled method (Appendix A.2) to find 8 sets of hyperparameters that produce solutions with at least \(\{80\%,85\%,90\%,95\%,98\%,99\%,99.5\%,100\%\}\) of the maximum possible quality. When setting the hyperparameters, we choose a "slackness" value \(\) and allow PM to produce solutions with \(\) at most \(\) higher than the optimal \(\) (in exchange for better performance on other metrics). For AAMAS 2015, \(=0.00\) and for ICLR 2018, \(=0.02\). We refer to Appendix A.2 for more details. We do not consider assignments with lower than \(80\%\) of the maximum quality since such low-quality assignments are unlikely to be deployed in practice. We evaluate the produced assignments on different randomness metrics to draw Figs. 2 to 4. We also implemented Algorithm 1 and tested it with the set of parameters at \(95\%\) relative quality to create Table 1. All source code is released at https://github.com/YixuanEvenXu/perturbed-maximization, and all experiments are done on a server with 56 cores and 504G RAM, running Ubuntu 20.04.6.

**Comparing Gurobi and Algorithm 1.** Table 1 shows that on ICLR 2018, Gurobi takes less wall clock time than Algorithm 1 due to parallelization over the server's 112 cores. However, Algorithm 1 takes less total CPU time (system + user CPU time) and achieves similar performance with Gurobi, which shows that Algorithm 1 provides decently approximated solutions using fewer computation

  Implementation &  &  \\  Algorithm & PM-Q & PM-E & PM-Q & PM-E \\   Wall Clock Time & 86.85s & 368.13s & 563.24s & 554.07s \\  Total CPU Time & 988.11s & 3212.18s & 562.77s & 553.59s \\   Quality & 95\% & 95\% & 94\% & 94\% \\  \(\) & 606.55 & 305.41 & 603.34 & 303.68 \\  \(\) (\(\)) & 0.87 & 0.87 & 0.86 & 0.86 \\  Entropy (\(\)) & 1589.95 & 1630.80 & 1575.86 & 1607.20 \\  

Table 1: Comparison of different implementations on ICLR 2018. Downarrows (\(\)) mean the lower the better and uparrows (\(\)) mean the higher the better. Due to parallelization, Gurobi takes less wall clock time. However, the network-flow-based approximation uses fewer computation resources (total CPU time). The approximation has relatively accurate \(\), \(\) and randomness metrics.

resources. Due to the space limit, rows in Table 1 have been selected and only results on ICLR 2018 are present. The complete tables and analysis for both datasets can be found in Appendix A.1.

**Comparing randomness metrics of PM and PLRA on discrete-valued datasets.** As shown in Figs. 1(a) and 2, on AAMAS 2015, both versions of PM achieve exactly the same performance with PLRA on \(\) while improving significantly on the other randomness metrics. Recall that PLRA achieves the optimal quality for a given \(\). This demonstrates that as predicted by Theorem 3, PM produces solutions that are more generally random while preserving the optimality in \(\) on discrete-valued datasets where assumption (i) holds. In Appendix A.3, we also test our algorithm on four more datasets from  and observe similar results.

**Comparing randomness metrics of PM and PLRA on continuous-valued datasets.** The results on ICLR 2018 in Figs. 1(b) and 2 show that both versions of PM sacrifice some \(\) as compared to PLRA. The exact amount of this sacrifice is affected by the slackness \(\) described in Appendix A.2. However, the improved randomness is still significant, though to a lesser extent than on AAMAS 2015. Note that assumption (i) of Theorem 3 does not hold on ICLR 2018. This exhibits the empirical efficacy of PM beyond the theoretical guarantees provided in Section 5.

**Comparing randomness metrics of PM-E and PM-Q.** In all of Figs. 1(c) and 2, we can see that no matter what metric and dataset are used, the performances of PM-E and PM-Q are always similar.

Figure 3: The trade-offs between quality and four randomness metrics on AAMAS 2015. Downarrows (\(\)) indicate that lower is better and uparrows (\(\)) indicate that higher is better. The figure shows that PM significantly outperforms PLRA on all four metrics, as predicted by Theorem 3.

Figure 2: The trade-offs between quality and \(\) on both datasets. Downarrows (\(\)) indicate that lower is better. On AAMAS 2015 where assumption (i) of Theorem 3 holds, PM incurs no increase in \(\); on ICLR 2018 where it does not hold, PM causes a \(0.02\) increase.

This suggests that the specific type of the perturbation function has limited impact as long as it is strictly concave. Therefore, program chairs do not need to carefully consider this choice in practice.

**Additional experiments.** Due to the page limit on the main paper, we are not able to present all the experiments we run in this section. We refer to Appendix A for additional experimental results about the network-flow-based approximation (Appendix A.1), hyperparameter tuning (Appendix A.2) and experiments on more datasets (Appendix A.3).

## 7 Conclusion

We present a general-purpose, practical algorithm for use by conference program chairs in computing randomized paper assignments. We show both theoretically and experimentally that our algorithm significantly improves over the previously deployed randomized assignment algorithm, PLRA. In conferences that currently deploy PLRA, our algorithm can be simply plugged-in in place of it to find a paper assignment with the same quality but with an improved level of randomization, achieving various benefits.

In practice, various aspects of paper assignment other than quality and randomness are considered by program chairs. We refer to Appendix C for discussions about how to optimize a specific metric with our algorithm (Appendix C.1) and how to incorporate some additional constraints from  (Appendix C.2). That being said, we do not consider various other aspects and constraints that may be desired by program chairs: e.g., fairness-based objectives  or constraints on review cycles . Incorporating these aspects remains an interesting direction for future work.

**Broader Impacts.** We expect our algorithm to have a mostly positive social impact by assisting program chairs in mitigating malicious behavior, facilitating evaluation of assignments, and increasing reviewer diversity and reviewer anonymity. While deploying a randomized assignment may negatively impact the assignment quality as compared to a deterministic assignment, our work allows conferences to choose the level of randomization that they deem best for their goals.