# Handling Learnwares from Heterogeneous Feature Spaces with Explicit Label Exploitation

Peng Tan, Hai-Tian Liu, Zhi-Hao Tan, Zhi-Hua Zhou

National Key Laboratory for Novel Software Technology, Nanjing University, China

School of Artificial Intelligence, Nanjing University, China

{tanp,liuht,tanzh,zhouzh}@lamda.nju.edu.cn

###### Abstract

The learnware paradigm aims to help users leverage numerous existing high-performing models instead of starting from scratch, where a learnware consists of a well-trained model and the specification describing its capability. Numerous learners are accommodated by a learnware dock system. When users solve tasks with the system, models that fully match the task feature space are often rare or even unavailable. However, models with heterogeneous feature space can still be helpful. This paper finds that _label information_, particularly model outputs, is helpful yet previously less exploited in the accommodation of heterogeneous learnwares. We extend the specification to better leverage model pseudo-labels and subsequently enrich the unified embedding space for better specification evolvement. With label information, the learnware identification can also be improved by additionally comparing conditional distributions. Experiments demonstrate that, even without a model explicitly tailored to user tasks, the system can effectively handle tasks by leveraging models from diverse feature spaces.

## 1 Introduction

The current machine learning paradigm has achieved remarkable success across various domains. This success, however, hinges on several critical factors: access to abundant high-quality labeled data, expensive computational resources, and deep expertise in feature engineering and algorithm design. These requirements pose a significant challenge for ordinary individuals aiming to build high-quality models from scratch. Moreover, issues such as data privacy, the difficulty of model adaptation, and catastrophic forgetting complicate the reuse or adaptation of trained models across different users.

Indeed, most efforts have focused on these issues separately, paying less attention to the fact that these problems are entangled. To address these challenges simultaneously, the _learnware_ paradigm was proposed by Zhou (2016). The learnware paradigm (Zhou and Tan, 2024) aims to assist users in solving their tasks by leveraging existing high-performing models, through the establishment of a _learnware dock system_. One important purpose of learnware paradigm is to enable high-performing models, submitted by developers, to be used "beyond-what-was-submitted." This means that models can be repurposed to assist with tasks not originally targeted by developers. To achieve this, learnware is designed as a high-performing model with a _specification_ describing its capability and utility. The specification, a central component for learnware management and identification, can be implemented by sketching the data distribution in which the model is proficient (Zhou and Tan, 2024). Recently, to facilitate research on the learnware paradigm, the learnware dock system, Beimingwu, has been released (Tan et al., 2024).

Previous research (Liu et al., 2024; Xie et al., 2023; Zhang et al., 2021) focuses on the homogeneous case where models and user tasks share the same feature space. However, in real-world scenarios, the feature spaces of models often differ due to varied feature engineering. As an example, weconsider the widely used clinical database, the OMOP Common Data Model (Biedermann et al., 2021), as illustrated in Figure 1. This model manages healthcare data from various sources through several standardized tables, such as demographic information, diagnoses, laboratory results, and medications. Experts across different hospitals often use different tables for feature engineering, even when working on the same clinical task, leading to the development of heterogeneous models.

In order to manage and exploit models developed from heterogeneous feature spaces, it is essential to build connections between these different spaces. Existing related techniques for exploiting relationships between feature spaces either rely on raw data (Wang and Sun, 2022; Zhu et al., 2023) of the model or utilize additional co-occurrence data (Xu et al., 2013; Huang et al., 2023). However, with model specifications, the learnware dock system can determine the relationships through subspace learning without the need for raw data or extra auxiliary data (Tan et al., 2023). To effectively accommodate heterogeneous learnwares, a unified subspace is constructed based on specifications of all submitted models, which helps to evolve the specification to have the capabilities of meeting requirements across different feature spaces. This paper finds that, without label information, subspace learning tends to yield suboptimal results, causing embeddings with entangled class representations in the subspace, or even rendering them meaningless when feature spaces are only weakly correlated. Additionally, without exploiting label information, the system can only identify models with marginal distributions similar to the user's task, ignoring models' capabilities.

This paper explicitly leverages label information for managing and utilizing heterogeneous models. We extend the specification to better incorporate model pseudo-labels, enabling the transition from unsupervised to supervised subspace learning for better specification evolvement. The extended specification also allows for additional comparison of conditional distributions using label information, thereby improving the learnware identification. The contributions are as follows:

* This paper proposes to exploit the model outputs to evolve specifications into a unified space during heterogeneous learnware accommodation. Specifically, the unified space is constructed based on the specifications of all models. By exploiting model outputs encoded in the specification, the resulting subspace exhibits improved properties, with less entangled class representations and more coherent embeddings.
* This paper extends the specification implementation to more effectively leverage label information by encoding both marginal and conditional distributions. This extended specification provides more accurate label information during subspace learning to better evolve specifications. Additionally, it also allows for additional comparison of conditional distributions, thereby improving the learnware identification.
* Experiments demonstrate that, even without a model explicitly tailored to the user's task, the system can effectively handle the task by leveraging models from diverse feature spaces.

## 2 Preliminary

Specification is the central part of the learnware, capturing the model ability. This section briefly introduces the Reduced Kernel Mean Embedding (RKME) specification (Zhou and Tan, 2024), which sketches the joint distribution of task features and model outputs with kernel methods.

We start by introducing the Kernel Mean Embedding (KME) (Scholkopf and Smola, 2002), which offers a novel representation for distributions. KME transforms a distribution into a reproducing kernel Hilbert space (RKHS). Given a distribution \(\) defined over a space \(\), the KME is defined as \(_{k}():=_{}k(,)( )\), where \(k:\) is a symmetric and positive definite kernel function, and its associated RKHS is \(\). For a data set \(\{_{i}\}_{i=1}^{m}\) sampled from \(\), the empirical estimate of KME is given by \(_{k}():=_{i=1}^{m}k(_{i},)\).

KME is considered as a potential specification due to several favorable properties. Accessing the raw data, however, compromises the necessary privacy concerns of the specification. Based

Figure 1: Heterogeneous feature space models in real-world scenario.

on KME, the RKME specification is proposed to use a reduced set of minor weighted samples \(\{(_{j},_{j})\}_{j=1}^{n},n m\) to approximate the empirical KME of the original dataset with model pseudo-outputs \(\{_{i}\}_{i=1}^{m}=\{(_{i},_{i})\}_{i=1}^{m}\), where \(_{i}=f(_{i})\) is the model prediction. The reduced set is generated by:

\[_{,}\|_{i=1}^{m}k(_{i}, )-_{j=1}^{n}_{j}k(_{j},)\|_{ }^{2},\] (1)

with the non-negative coefficients \(\{_{j}\}_{j=1}^{n}\). The RKME \(()=_{j=1}^{n}_{j}k(_{j},)\) acts as the specification, and the RKHS \(\) is referred to as the specification space. This specification effectively captures the major information of the distribution \(\) without exposing raw data and explicitly encodes the model capability based on its outputs. Notably, in simple cases where the features are sufficient to represent the model capability, the sketch solely on the features \(\{_{i}\}_{i=1}^{m}\) can also be used as the model specification (Wu et al., 2023). In this paper, we further extend the specification generation process to more effectively encode the model's outputs.

## 3 Problem setup

This paper addresses the challenge of constructing a heterogeneous learnware dock system and leveraging it to assist users who have only limited labeled data such that training a model by themselves will lead to poor performance. Without loss of generality, we consider the underlying full feature space, denoted as \(_{}\), as a composite of \(Q\) distinct blocks, i.e., \(_{}=_{1}_{Q}\). The feature spaces for developers, \(^{}\), and for users, \(^{}\), are represented as Cartesian products of specific blocks \(_{i C}_{i}\), where \(C\) refers to block indices.

The overall procedure consists of two stages: the submission stage and the deployment stage. In the submission stage, the developer trains a well-performing model \(f_{i}\) on the dataset \(D_{i}:=\{(_{ij},y_{ij})\}_{j=1}^{n_{i}}\) and generates a _developer-level_ specification \(_{i}^{}\), which captures the model's performance without exposing raw data. After receiving all heterogeneous models and their developer-level specifications, the learnware dock system assigns a _system-level_ specification \(_{i}\) to each model \(f_{i}\), based on all submitted specifications \(\{_{i}^{}\}_{i=1}^{N}\). The heterogeneous learnware dock system is then constructed as \(\{(f_{i},_{i})\}_{i=1}^{N}\). In the deployment stage, the user has unlabeled data \(D_{0}^{u}=\{_{0i}\}_{i=1}^{n_{u}}\) and a limited amount of labeled data \(D_{0}^{l}=\{(}_{0i},y_{0i})\}_{i=1}^{n_{l}}\) (the unlabeled data cover labeled data features, i.e., \(\{}_{i}\}_{i}\{_{i}\}_{i}\)). The user generates a user-level task requirement \(_{0}^{}\) and submits it to the learnware dock system. The system then identifies the most helpful model(s) for reuse to tackle the user task.

## 4 Methodology

This section outlines our methodology for accommodating heterogeneous models under the learnware paradigm and assisting user tasks, emphasizing the importance and utilization of label information, which remains _unexplored_ in learnware paradigm when dealing with heterogeneous feature spaces.

### Improve managing heterogeneous models with label information

To handle learnwares with heterogeneous feature spaces, it is helpful to exploit the relationships between these spaces. A common approach is to learn a unified subspace. However, without label information, the resulting subspace may produce entangled embeddings of samples from different

Figure 2: An illustration of the learnware paradigm with heterogeneous feature spaces

classes, and when feature blocks are weakly correlated, the subspace may become meaningless (see Section B.2 for detailed discussion). Since subspace learning is based on all learnware specifications, incorporating label information into the model specification is highly beneficial.

We rewrite the RKME specification represented by \(R=(,T)=\{(_{j},_{j})\}_{j=1}^{m}\) to RKMEL represented by \(R_{L}=(,Z,Y)=\{(_{j},_{j},y_{j})\}_{j=1}^{m}\) by splitting the sample \(_{j}\) into the feature \(_{j}\) and the pseudo label \(y_{j}\), emphasizing label information. Given existing model specifications \(\{s_{i}^{}:=\{(_{ij},_{j},y_{j})\}_{j=1}^{m_{i}}\}_{i=1}^ {N}\), the learnware dock system can learn a unified subspace \(_{}\) with encoding functions \(\{h_{k}:_{k}_{}\}_{k=1}^{Q}\) and decoding functions \(\{g_{k}:_{}_{k}\}_{k=1}^{Q}\) by optimizing \(L=_{1}L_{}+_{2}L_{}+_{3}L_{ }\) over mapping functions \(\{h_{k},g_{k}\}_{k=1}^{Q}\). The objective function has three components: the reconstruction loss, which trains mapping functions \((h_{k},g_{k})\) to map and reconstruct data in \(_{k}\); the similarity loss, which makes embeddings of different slices of \(_{j}\) similar; and the supervised loss, which uses label information to improve subspace learning by making class embeddings more separable or aligning samples within the same class. After subspace learning, the mapping functions \(\{h_{k},g_{k}\}_{k=1}^{Q}\) can project data from any combination of feature space blocks to the subspace. When reusing heterogeneous models, these functions can also fill in missing parts needed for model predictions. Such a framework can be implemented by existing subspace learning methods, such as self-supervised learning (Ucar et al., 2021; Bahri et al., 2022), matrix factorization (Xu and Gong, 2004; Wang et al., 2016). When the system receives learnwares from unseen feature spaces after subspace generation, the system can update the subspace during idle time.

### Improve matching model and user task with label information

Matching with only marginal distribution \(P_{X}\) is not enough.We first review previous methodologies for matching a user's task with a model in the homogeneous case, where all models and user tasks share the same feature space (Wu et al., 2023; Zhang et al., 2021). These methods recommend the model with the most similar marginal distribution \(P_{X}\). To avoid exposing raw data, they use RKME to sketch the marginal distributions of the model task and user task, serving as the model specifications and user requirements. To illustrate the deficiency, we refer to Figure 3, which presents five tasks with uniform distributions. Among these tasks, four have circular support sets and one has a square support set. The two problems are: 1) Models with the same \(P_{X}\) but different \(P_{X|Y}\) are indistinguishable. In Figure 3, Models 1, 2 and 4 are all recommended, but model 2 is unsuitable for the user task. 2) Models with different \(P_{X}\) are rarely considered, despite their potential usefulness. Model 3 in Figure 3, though suitable, are excluded because its \(P_{X}\) is square instead of circle.

Enhance matching by incorporating the conditional distribution \(P_{X|Y}\).Matching solely on the marginal distribution is insufficient for model identification. To better recommend models to user tasks, we propose additionally considering the conditional distribution \(P_{X|Y}\), which helps exclude the model with dissimilar conditional distributions (Model 2) and include the model with similar ones (Model 3). While the user's task can estimate the conditional distribution from labeled data, a key question arises for the model task: _should we use true labels or model-generated pseudo labels?_ Using True labels results in comparing the user's task distribution \(P(X,Y)\) with the model's original task distribution, while pseudo labels results in comparing the model-generated joint distribution \(P(X,)\) with new tasks, allowing the model to be reused beyond its original purpose. As shown in Figure 3, Model 4 would be recommended using pseudo labels but not using true labels. In conclusion, considering both marginal and conditional distributions improves model identification, with model-generated pseudo labels being helpful for encoding model capabilities.

### Summary

To accommodate and identify models developed from heterogeneous feature spaces, it is advantageous to utilize pseudo-label information generated by the models. To better incorporate this information,

Figure 3: Label information is beneficial for matching.

we propose to integrate both marginal and conditional distributions into the model specification and user requirements, represented as \(\{(_{j},_{j},y_{j})\}_{j=1}^{m}\). By comparing these distributions, we improve learnware identification. The inclusion of label information enhances subspace learning, and the framework can be applied across various subspace learning methods.

## 5 Detailed solution

In this section, we provide the detailed procedure for the heterogeneous learnware problem based on the aforementioned methodology, which consists of model specification generation, heterogeneous learnwares accommodation by the system, and system exploitation for solving new user tasks.

### The developer generates the model specification

The model specification sketches task distribution and model capabilities with a reduced set. Instead of sketching the joint distribution of task features and outputs (Eq. (1)), we propose to generate feature and label part separately to balance label and feature information. This includes a unified mechanism for classification and regression, and a specialized mechanism for classification.

Unified mechanism for classification and regression tasks.Given a dataset \(D=\{(_{i},y_{i})\}_{i=1}^{n}\) and a model \(f\) trained on it, we first generate a reduced set \(\{(_{j},_{j})\}_{j=1}^{m}\) solely on \(\{_{i}\}_{i=1}^{n}\) based on RXME via Eq. (1) with \(_{i}=_{i}\), which sketches the marginal distribution of the task feature. To encode the model's ability, pseudo labels can be assigned to the reduced set using \(y_{j}=f(_{j})\), resulting in the labeled reduced set \(R_{L}=(,Z,Y)=\{(_{j},_{j},y_{j})\}_ {j=1}^{m}\), serving as the developer-level model specification \(^{}\).

Specialized mechanism for classification tasks.For classification problems, given the finite label space \(\), we propose the mechanism to directly sketch the model's capacity, characterized by the conditional distribution \(P(X|Y)\) of the model \(f\). We first obtain the model predictions \(\{_{i}\}_{i=1}^{n}\) on its "skilled" marginal distribution, i.e., its training data \(\{_{i}\}_{i=1}^{n}\). Then, the pseudo-labeled dataset \(\{(_{i},_{i})\}_{i=1}^{n}\), which encodes the model's conditional distribution, can be sketched by a labeled reduced set \(R_{L}=(,Z,Y)=\{(_{j},_{j},y_{j})\}_ {j=1}^{m}\) using the following objective:

\[\|_{i=1}^{n}k(_{i},)-_{j=1}^{m} _{j}k(_{j},)\|_{_{k}}^{2}+ _{c=1}^{C}\|_{i_{c}}k(_{i}, )-_{j_{c}^{}}_{j}k(_{j}, )\|_{_{k}}^{2},\] (2)

where \(_{c}\) and \(_{c}^{}\) represent the indices of samples \(_{i}\) and \(_{j}\) belonging to class \(c\), respectively. \(\) is the parameter used to balance the marginal distribution distance and conditional distribution distance. The labeled reduced set \(R_{L}\) should approximate both the marginal distribution \(_{i=1}^{n}_{_{i}}\) with \(_{j=1}^{m}_{j}_{_{j}}\) and the conditional distribution given the \(c\) class \(_{i_{c}}_{_{i}}\) with \(_{j_{c}^{}}_{j}_{_{j}}\) simultaneously. Here, \(()\) is the Dirichlet function, which describes the probability mass at a single point. The objective Eq. (2) can be optimized by alternating optimization, the details are showed in E.1. The optimized reduced set \(R_{L}\) is served as the developer-level model specification \(^{}\).

The first unified mechanism sketches the marginal distribution and then encodes the model information, while the second specialized mechanism directly sketches the model's conditional distribution.

### The system accommodates heterogeneous learnwares

After the developer-level specification \(^{}\) is generated, the developer submits the model \(f\) with specification \(^{}\) to the learnware dock system. The system exploits the relationship of different feature spaces and manages heterogeneous models by assigning system-level specification \(^{}\).

Subspace learningAfter the learnware dock system receives heterogeneous models with their developer-level specifications, it generates a unified subspace \(_{}\) to connect different feature blocks \(\{_{i}\}_{i=1}^{Q}\) based on all developer-level specifications \(\{_{i}^{}:=\{(_{ij},_{ij},y_{ij})\}_{j=1}^{m_{i}} \}_{i=1}^{N}\). During subspace learning, the learnware dock system learns \(2Q\) mapping functions:and \(\{g_{k}:_{}_{k}\}_{k=1}^{Q}\). For a particular sample \(_{ij}\), it can be split into several blocks \(\{_{ij}^{(k)}\}_{k C_{i}}\) according to the feature split \(_{}=_{1}_{Q}\). The encoding function \(h_{k}\) produces the embedding of sample slice \(_{ij}^{(k)}\) as \(_{ij}^{(k)}\), and the decoding function \(g_{k}\) reconstructs it to \(_{k}\) as \(}_{ij}^{(k)}\).

The loss for subspace learning is implemented as follows: **1)** The reconstruction loss, \(_{}=_{i=1}^{N}_{j=1}^{m_{i}}_{ij}\| }_{ij}^{(k)}-_{ij}^{(k)}\|_{}^{2}\), penalizes the difference between the original sample \(_{ij}^{(k)}\) and the reconstructed sample \(}_{ij}^{(k)}\), weighted by the sample importance \(_{ij}\). **2)** The supervised loss involves building a simple classifier on \(\{\{(_{ij},y_{ij})\}_{j=1}^{m_{i}}\}_{i=1}^{N}\), where \(_{ij}=(\{_{ij}^{(k)}\}_{k C_{i}})\), and calculating the prediction loss to make the embeddings of different classes more separable. **3)** The contrastive loss aims to make the embeddings \(\{_{ij}^{(k)}\}_{k C_{i}}\) of a single sample \(_{ij}\) similar, while ensuring that embeddings of different samples are dissimilar. The contrastive loss \(_{}=_{i=1}^{N}l_{i}\) includes \(N\) terms, each term being a weighted loss extended from the Self-VPCL loss , calculated on the embeddings \(\{\{_{ij}^{(k)}\}_{k C_{i}}\}_{i=1}^{m_{i}}\) of one specification \(_{i}^{}\): \(l_{i}=_{j=1}^{m_{i}}_{ij}_{k C_{i}}_{k^{} C_{i}, k^{} k}_{ij}^{(k)},_{ij}^{(k^{ })})}{_{t=1}^{m_{i}}_{t C_{i}}(_{ij}^ {(k)},_{it}^{(k)})}\), where \(\) is the cosine similarity function. In the logarithm term, the numerator represents the similarity of the positive pair, while the denominator is the sum of all pairs. The loss \(\) is optimized with gradient descent.

Heterogeneous learnware accommodation.After subspace learning, the learnware dock system builds a unified subspace and corresponding mapping functions \(\{g_{i},h_{i}\}_{i=1}^{Q}\). The dock system then assigns a system-level specification \(_{i}=\{(_{ij},_{ij},y_{ij})\}_{j=1}^{m_{i}}\) for each model based on its developer-level specification \(_{i}^{}:=\{(_{ij},_{ij},y_{ij})\}_{j=1}^{m_{i}}\). During the system-level specification generation, the sample \(_{ij}\) is projected to the unified subspace as \(_{ij}\), while the coefficient \(_{ij}\) and the label \(y_{ij}\) remain unchanged. The projection \(_{ij}\) is calculated as follows: \(_{ij}=|}_{k C_{i}}h_{k}(_{ij}^{(k)})\). The whole procedure of learnware dock system construction is described in Algorithm 3.

### The user exploits the learnware dock system

After the learnware dock system accommodates heterogeneous learnwares, users can submit their task requirements to receive recommended models and the toolkit used for feature transformation. They can then directly reuse the model or combine it with a self-training model.

User requirement generation.As described in Section 4.2, comparing both the marginal distribution \(P_{X}\) and conditional distribution \(P_{X|Y}\) based on the model specification and user requirement helps better learnware identification. Similar to \(_{}\) specification encoding both distributions, the \(_{}\) requirement of the user is generated similarly to reflect both. In details, given the unlabeled data \(D^{u}=\{_{i}\}_{i=1}^{n_{u}}\) and some labeled data \(D^{l}=\{(}_{i},y_{i})\}_{i=1}^{n_{l}}\) (the unlabeled data cover labeled data features, i.e., \(\{}_{i}\}_{i}\{_{i}\}_{i}\)), the user can generate \(_{}\) requirement presented by labeled reduced set \(R_{L}=\{(_{j},_{j},y_{j})\}_{j=1}^{m}\) to sketch the task distribution.

For the classification case, the reduced set \(R_{L}\) can be generated by minimizing the following distance:

\[\|_{i=1}^{n_{u}}}k(_{i},)-_{j =1}^{m}_{j}k(_{j},)\|_{_{k}}^{2}+ _{c=1}^{C}\|_{i_{c}}}k( }_{i},)-_{j_{c}^{}}_{j}k (_{j},)\|_{_{k}}^{2}\] (3)

This equation is similar to specification generation in Eq. (2), where the specification sketches the pseudo-labeled dataset, while the requirement sketches semi-supervised data. The first term calculates the distance between the marginal distributions of the unlabeled dataset \(_{i=1}^{n_{u}}}_{_{i}}\) and the reduced set \(_{j=1}^{m}_{j}_{_{j}}\). The second term calculates the distance between the conditional distributions of the labeled dataset \(_{i_{c}}}_{}_{i}}\) and the reduced set \(_{j_{c}^{}}_{j}_{_{j}}\), where \(_{c}\) and \(_{c}^{}\) denote the sample indices of the labeled dataset and the reduced set with label \(c\), respectively. The optimized reduced set \(R_{L}\) becomes the user-level requirement \(_{0}^{}\). The optimization is described in E.2

For regression, the requirement is generated by sketching the marginal distribution and applying a self-trained model for pseudo-labeling, similar to unified specification generation in Section 5.1.

Learnware identification.After the user submits the user-level task requirement \(_{0}^{}=\{(_{0k},_{0k},y_{0k})\}_{k=1}^{m_{0}}\), to the dock system, the dock system transforms it into the system-level task requirement \(_{0}=\{(_{0k},_{0k},y_{0k})\}_{k=1}^{m_{0}}\) by projecting \(_{0k}\) into the subspace as \(_{0k}\). The dock system then calculates the distance between the system-level specification \(_{i}=\{(_{ij},_{ij},y_{ij})\}_{j=1}^{m_{i}}\) and the system-level task requirement \(_{0}\) as follows:

\[\|_{k}^{m_{0}}_{0k}k(_{0k},)-_{j}^{m_{i}}_{ ij}k(_{ij},)\|+_{C}\|_{k_{0,C} }_{0k}k(_{0k},)-_{j_{i,C}}_{ij}k(_{ij},)\|,\] (4)

which measures both the conditional distribution distances and marginal distribution distances. Where \(_{i,C}\) represents the indices of \(_{ij}\) with the class c. The learnware dock system then recommends the learnware with a minimal distance.

Learnware reuse.Once the user receives the recommended model \(f_{i}\) and the dock system toolkit \(\{h_{k},g_{k}\}_{k=1}^{Q}\), they can apply the model to their task. The toolkit helps bridge the gap between different feature spaces. For example, if the user's task is on \(_{1}_{2}\) and the model is on \(_{2}_{3}_{4}\), the user can project their data using \(h_{1}\) and \(h_{2}\), then decode it to \(_{3}\) and \(_{4}\) with \(g_{3}\) and \(g_{4}\). The user can use the recommended model's predictions directly or ensemble them with a self-trained model.

### Overall procedure

In the submission stage, the dock system receives models with developer-level specifications that sketch model capabilities and assigns system-level specifications by a learned unified subspace. In the deployment stage, users submit task requirements detailing marginal and conditional distributions to receive recommended learnware. This learnware can be integrated with their self-trained models to significantly enhance performance. The overall process is summarized in Appendix D.1.

## 6 Experiments

### Experiment setup

Datasets.We tested our methods on 30 datasets from the Tabzilla benchmark (McElfresh et al., 2023), excluding tiny datasets. These include 23 classification tasks and 7 regression tasks. For classification tasks, the sample sizes range from 1,000 to 58,310, feature space dimensions from 7 to 7,200, and the number of classes from 2 to 10. For regression tasks, the sample sizes range from 418 to 108,000, and feature space dimensions from 8 to 128.

Compared methods.As the heterogeneous learnware problem, where the user has some labeled data, is a new problem, we first compare our approach with two basic methods that train models from scratch: lightgbm (Ke et al., 2017), a widely used tree-based method for tabular datasets, and TabPFN(Hollmann et al., 2023), a recently proposed prior-data fitted network capable of training and inference on small classification datasets in less than one second. When seeking assistance from the model repository, one simple but inefficient approach is to fetch all models, conduct heterogeneous transfer learning, and select the best one. Align_unlabel(Tan et al., 2024a) aligns the feature space and uses the aligned model directly, while Align_label(Tan et al., 2024a) goes a step further by fine-tuning through training a new model with augmented features that include aligned model predictions. Another method for reusing knowledge from heterogeneous tasks involves pre-training a unified tabular network on different tables and fine-tuning on the downstream user tasks: Transtab(Wang and Sun, 2022) and Xtab(Zhu et al., 2023). However, these methods require access to raw task data, whereas our method protects user privacy. Next, we compare with Hetero(Tan et al., 2023), an initial attempt to address the heterogeneous learnware problem without using label information. Finally, we substitute the specification in our method with the RKME specification from (Zhou and Tan, 2024) as Our_basic_ and conduct a comparison with the proposed method.

Experiment configuration.The feature space is randomly divided into four equal blocks, creating feature spaces from three-block combinations for developer tasks and two-block combinations for user tasks. For user tasks, 100 labeled data points are sampled from the training set. All experiments are repeated five times. For more details, please see Appendix F.1.

### Performance on user tasks

Tables 1 and 2 compare the performance of our proposed methods with other contenders on classification and regression tasks. Our\({}_{}\) refers to the performance of the overall procedure with the unified specification, while Our\({}_{}\) refers to the specialized specification designed for classification tasks. Our approach, Our\({}_{}\), outperforms the competitors in most cases. While Lightgbm and TabPFN use self-training, their performance is limited by the small amount of labeled data. It is showed that TabPFN performs better than Lightgbm under these conditions. This highlights the importance of leveraging well-trained models, even with heterogeneous feature spaces, to improve performance.

Examining Align\({}_{}\) shows that heterogeneous transfer learning with only aligning feature spaces without labels is less effective than self-training. However, further fine-tuning enables Align\({}_{}\) to outperform self-training methods. Nevertheless, without leveraging knowledge across different tasks, Align\({}_{}\) still performs worse than our approach. Transtab and Xtab attempt to create a unified backbone across different tables to leverage cross-task knowledge, but they fail to reuse the high-performing model on each developer task, leading to worse performance than ours. These methods also require training on raw developer data, whereas our method only accesses model specifications without exposing raw data.

Hetero performs worse than our methods due to its lack of modeling the conditional distribution of submitted models and its reliance on unsupervised subspace learning. Compared to Our\({}_{}\), our proposed specification outperforms the RKME specification, as it alleviates the issue of label information being overshadowed by feature information during specification generation and comparison. Notably, for classification tasks, our specialized model Our\({}_{}\) outperforms Our\({}_{}\) due to its ability to encode conditional distribution of the model more effectively.

  
**Dataset name** & **Lightgbm** & **TabPFN** & **Align\({}_{}\)** & **Align\({}_{}\)** & **Transstab** & **Xtab** & **Hetero** & **Our\({}_{}\)** & **Our\({}_{}\)** & **Our\({}_{}\)** \\  credit-g & \(67.4 1.1\) & \(70.5 0.44\) & \(58.3 2.46\) & \(69.1 1.68\) & \(69.6 1.02\) & \(70.3 6.48\) & \(67.5 0.00\) & \(71.2 0.55\) & \(71.0 0.48\) & \(71.4 0.87\) \\ semeno & \(61.9 4.09\) & \(63.4 1.6\) & \(68.1 5.9\) & \(59.6 5.21\) & \(25.4 4.27\) & \(27.1 0.44\) & \(62.0 5.32\) & \(59.2 0.54\) & \(59.9 1.42\) & \(63.7 0.89\) \\ mfet-kaanthren & \(66.1 1.02\) & \(62.6 1.78\) & \(71.2 1.32\) & \(73.3 0.17\) & \(67.2 1.04\) & \(36.0 1.95\) & \(59.4 7.10\) & \(71.2 1.35\) & \(73.5 0.76\) \\ splice & \(64.9 1.9\) & \(63.2 0.48\) & \(42.5 3.11\) & \(56.2 5.46\) & \(72.8 1.46\) & \(56.0 6.84\) & \(60.7 22.12\) & \(73.5 0.49\) & \(73.3 0.89\) \\ gina agnostic & \(75.5 15.6\) & \(76.6 1.51\) & \(45.1 7.45\) & \(74.8 1.42\) & \(57.8 1.23\) & \(62.0 8.00\) & \(88.9 8.73\) & \(87.3 0.89\) & \(89.9 0.39\) \\ Bioresponse & \(64.2 1.57\) & \(57.2 1.57\) & \(57.2 4.28\) & \(64.2 1.30\) & \(51.6 5.16\) & \(56.6 5.64\) & \(66.7 1.67\) & \(61.7 0.71\) & \(61.7 0.79\) & \(62.9 1.57\) & \(71.5 0.89\) \\ sylvine & \(68.6 1.9\) & \(71.4 4.73\) & \(47.2 4.52\) & \(69.2 1.92\) & \(69.6 0.90\) & \(66.6 1.72\) & \(75.1 0.00\) & \(74.8 1.73\) & \(74.5 1.15\) & \(76.9 0.88\) \\ charitine & \(61.7 6.5\) & \(65.4 1.35\) & \(46.6 6.76\) & \(66.9 4.56\) & \(61.8 5.09\) & \(61.8 1.12\) & \(72.2 0.70\) & \(72.6 2.00\) & \(72.6 2.00\) & \(72.7 0.00\) \\ theorem-proving & \(42.1 1.8\) & \(42.5 1.5\) & \(27.0 1.6\) & \(39.5 1.22\) & \(42.0 40.8\) & \(40.8 4.34\) & \(34.1 0.30\) & \(51.2 0.00\) & \(50.4 0.51\) & \(51.0 0.00\) \\ satimage & \(80.5 1.5\) & \(83.3 6.3\) & \(43.1 4.18\) & \(83.3 5.5\) & \(75.4 1.43\) & \(58.8 5.08\) & \(52.8 3.69\) & \(86.9 8.71\) & \(87.6 0.89\) \\ fabert & \(23.4 1.0\) & \(20.4 1.04\) & \(13.5 20.15\) & \(32.0 2.84\) & \(22.9 2.99\) & \(34.2 1.02\) & \(41.2 1.13\) & \(41.0 0.00\) & \(46.0 1.32\) \\ gesture segmentation & \(40.7 1.4\) & \(43.1 2.3\) & \(14.2 1.6\) & \(37.7 3.4\) & \(39.7 3.64\) & \(38.6 1.7\) & \(37.6 0.50\) & \(53.6 0.00\) & \(52.8 4.52\) & \(52.1 0.00\) \\ robert & \(22.5 1.5\) & \(24.3 1.0\) & \(43.0 1.4\) & \(26.4 1.6\) & \(12.0 1.0\) & \(21.0 1.0\) & \(13.4 4.5\) & \(45.5 4.08\) & \(44.8 4.53\) & \(45.3 0.42\) \\ artificial-characters & \(25.0 1.3\) & \(23.0 1.6\) & \(8.5 1.0\)

### Evaluation on users with different size of labeled data

In the previous section, we showed that using a single learnware with heterogeneous feature spaces outperforms training models from scratch when labeled data is limited. Now, we analyze how performance changes as users train models and ensemble their predictions with learnware across different amounts of labeled data. Figures 4 and 5 display these trends for classification and regression tasks. These figures indicate that ensemble methods consistently outperform self-training with 100 labeled data points. With 500 labeled data points, the ensemble method still performs better in nearly 80% of cases. Additionally, learnware continues to enhance performance even with 5000 labeled samples, improving 21% of classification cases and 50% of regression cases. For certain datasets like kin8nm in regression tasks, even when users use their entire training dataset, the recommended heterogeneous learnware can still significantly boost performance.

## 7 Conclusion

This paper evolves specifications to a unified subspace with explicit exploitation of model outputs under the heterogeneous learnware scenario. The specification is extended by additionally encoding conditional distribution to better encode the model capability, which can be further evolved by more effective subspace learning enriched by label information. The extended specification also improves learnware identification by additionally matching conditional distributions. We present the complete workflow of the learnware dock system accommodating heterogeneous learnwares and validate the effectiveness of the proposed methods through extensive experiments.

Figure 4: User performance curve for classification tasks.

Figure 5: User performance curve for regression tasks.