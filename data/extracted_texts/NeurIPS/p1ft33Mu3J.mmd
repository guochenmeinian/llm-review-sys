# Linear Transformers are Versatile In-Context Learners

Max Vladymyrov

Google Research

mxv@google.com &Johannes von Oswald

Google, Paradigms of Intelligence Team

jvoswald@google.com &Mark Sandler

Google Research

sandler@google.com &Rong Ge

Duke University

rongge@cs.duke.edu

###### Abstract

Recent research has demonstrated that transformers, particularly linear attention models, implicitly execute gradient-descent-like algorithms on data provided in-context during their forward inference step. However, their capability in handling more complex problems remains unexplored. In this paper, we prove that each layer of a linear transformer maintains a weight vector for an implicit linear regression problem and can be interpreted as performing a variant of preconditioned gradient descent. We also investigate the use of linear transformers in a challenging scenario where the training data is corrupted with different levels of noise. Remarkably, we demonstrate that for this problem linear transformers discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines. We analyze this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels. Our findings show that even linear transformers possess the surprising ability to discover sophisticated optimization strategies.

## 1 Introduction

The transformer architecture (Vaswani et al., 2017) has revolutionized the field of machine learning, driving breakthroughs across various domains and serving as a foundation for powerful models (Anil et al., 2023; Achiam et al., 2023; Team et al., 2023; Jiang et al., 2023). However, despite their widespread success, the mechanisms that drive their performance remain an active area of research. A key component of their success is attributed to in-context learning (ICL, Brown et al., 2020) - an emergent ability of transformers to make predictions based on information provided within the input sequence itself, without explicit parameter updates.

Recently, several papers (Garg et al., 2022; Akyurek et al., 2022; von Oswald et al., 2023) have suggested that ICL might be partially explained by an implicit meta-optimization of the transformers that happens on input context (aka mesa-optimization Hubinger et al., 2019). They have shown that transformers with linear self-attention layers (aka linear transformers) trained on linear regression tasks can internally implement gradient-based optimization.

Specifically, von Oswald et al. (2023) demonstrated that linear transformers can execute iterations of an algorithm similar to the gradient descent algorithm (which they call GD\({}^{++}\)), with each attention layer representing one step of the algorithm. Later, Ahn et al. (2023); Zhang et al. (2023) further characterized this behavior, showing that the learned solution is a form of preconditioned GD, and this solution is optimal for one-layer linear transformers.

In this paper, we continue to study linear transformers trained on linear regression problems. We prove that each layer of every linear transformer maintains a weight vector for an underlying linear regression problem. Under some restrictions, the algorithm it runs can be interpreted as a complex variant of preconditioned gradient descent with momentum-like behaviors.

While maintaining a linear regression model (regardless of the data) might seem restrictive, we show that linear transformers can discover powerful optimization algorithms. As a first example, we prove that in case of GD\({}^{++}\), the preconditioner results in a second order optimization algorithm.

Furthermore, we demonstrate that linear transformers can be trained to uncover even more powerful and intricate algorithms. We modified the problem formulation to consider mixed linear regression with varying noise levels1 (inspired by Bai et al., 2023). This is a harder and non-trivial problem with no obvious closed-form solution, since it needs to account for various levels of noise in the input.

Our experiments with two different noise variance distributions (uniform and categorical) demonstrate the remarkable flexibility of linear transformers. Training a linear transformer in these settings leads to an algorithm that outperforms GD\({}^{++}\) as well as various baselines derived from the exact closed-form solution of the ridge regression. We discover that this result holds even when training a linear transformer with diagonal weight matrices.

Through a detailed analysis, we reveal key distinctions from GD\({}^{++}\), including momentum-like term and adaptive rescaling based on the noise levels.

Our findings contribute to the growing body of research where novel, high-performing algorithms have been directly discovered through the reverse-engineering of transformer weights. This work expands our understanding of the implicit learning capabilities of attention-based models and highlights the remarkable versatility of even simple linear transformers as in-context learners. We demonstrate that transformers have the potential to discover effective algorithms that may advance the state-of-the-art in optimization and machine learning in general.

## 2 Preliminaries

In this section we introduce notations for linear transformers, data, and type of problems we consider.

### Linear transformers and in-context learning

Given input sequence \(e_{1},e_{2},...,e_{n}^{d+1}\), a single head in a linear self-attention layer is usually parameterized by four matrices, key \(W_{K}\), query \(W_{Q}\), value \(W_{V}\) and projection \(W_{P}\). The output of the non-causal layer at position \(i\) is \(e_{i}+ e_{i}\) where \( e_{i}\) is computed as

\[ e_{i}=W_{P}(_{j=1}^{n} W_{Q}e_{i},W_{K}e_{j} W_{ V}e_{j}).\] (1)

Equivalently, one can use parameters \(P=W_{P}W_{V}\) and \(Q=W_{K}^{}W_{Q}\), and the equation becomes

\[ e_{i}=_{j=1}^{n}(e_{j}^{}Qe_{i})Pe_{j}.\] (2)

Multiple heads \((P_{1},Q_{1}),(P_{2},Q_{2}),...,(P_{h},Q_{h})\) simply sum their effects

\[ e_{i}=_{k=1}^{H}_{j=1}^{n}(e_{j}^{}Q_{k}e_{i})P_{k}e_{j}.\] (3)

We define a _linear transformer_ as a multi-layer neural network composed of \(L\) linear self-attention layers parameterized by \(=\{Q_{k}^{l},P_{k}^{l}\}\) for \(k=1 H,l=1 L\). To isolate the core mechanisms, we consider a simplified decoder-only architecture, excluding MLPs and LayerNorm components. This architecture was also used in previous work (von Oswald et al., 2023; Ahn et al., 2023).

We consider two versions of linear transformers: Full with the transformer parameters represented by full matrices and Diag, where the parameters are restricted to diagonal matrices only.

Inspired by von Oswald et al. (2023), in this paper we consider regression data as the token sequence. Each token \(e_{i}=(x_{i},y_{i})^{d+1}\) consists of a feature vector \(x_{i}^{d}\) and its corresponding output \(y_{i}\). Additionally, we append a query token \(e_{n+1}=(x_{t},0)\) to the sequence, where \(x_{t}^{d}\) represents test data. The goal of in-context learning is to predict \(y_{t}\) for the test data \(x_{t}\). We constrain the attention to only focus on the first \(n\) tokens of the sequence so that it ignores the query token.

We use \((x_{i}^{l},y_{i}^{l})\) to denote the \(i\)-th token in the transformer's output at layer \(l\). The initial layer is simply the input: \((x_{i}^{l},y_{i}^{0})=(x_{i},y_{i})\). For a model with parameters \(\), we read out the prediction by taking the negative2 of the last coordinate of the final token in the last layer as \(_{}(\{e_{1},...,e_{n}\},e_{n+1})=-y_{n+1}^{L}\).

Let's also define the following notation to be used throughout the paper

\[ =_{i=1}^{n}x_{i}(x_{i})^{}; =_{i=1}^{n}y_{i}x_{i}; =_{i=1}^{n}(y_{i})^{2}\] \[^{l} =_{i=1}^{n}x_{i}^{l}(x_{i}^{l})^{}; ^{l}=_{i=1}^{n}y_{i}^{l}x_{i}^{l}; ^{l}=_{i=1}^{n}(y_{i}^{l})^{2}\]

### Noisy regression model

As a model problem, we consider data generated from a noisy linear regression model. For each input sequence \(\), we sample a ground-truth weight vector \(w_{} N(0,I)\), and generate \(n\) data points as \(x_{i} N(0,I)\) and \(y_{i}= w_{},x_{i}+_{i}\), with noise \(_{i} N(0,_{}^{2})\).

Note that each sequence can have different ground-truth weight vectors \(w_{}\), but every data point in the sequence shares the same \(w_{}\) and \(_{}\). The query is generated as \(x_{t} N(0,I)\) and \(y_{t}= w_{},x_{t}\) (since the noise is independent, whether we include noise in \(y_{q}\) will only be an additive constant to the final objective).

We further define an ordinary least square (OLS) loss as

\[L_{}(w)=_{i=1}^{n}(y_{i}- w,x_{i})^{2}.\] (4)

The OLS solution is \(w^{*}:=^{-1}\) with residuals \(r_{i}:=y_{i}- w^{*},x_{i}\).

In the presence of noise \(_{}\), \(w^{*}\) in general is not equal to the ground truth \(w_{}\). For a _known_ noise level \(_{}\), the best estimator for \(w_{}\) is provided by ridge regression:

\[L_{}(w)=_{i=1}^{n}(y_{i}- w,x_{i})^{2}+ _{}^{2}\|w\|^{2},\] (5)

with solution \(w_{^{2}}^{*}:=(+_{}^{2}I)^{-1}\). Of course, in reality the variance of the noise is not known and has to be estimated from the data.

### Fixed vs. mixed noise variance problems

We consider two different problems within the noisy linear regression framework.

Fixed noise variance.In this scenario, the variance \(_{}\) remains constant for all the training data. Here, the in-context loss is:

\[L()=}_{w_{} N(0,I)\\ x_{i} N(0,I)\\ _{i} N(0,_{}^{2})}[(_{}(\{e_ {1},...,e_{n}\},e_{n+1})-y_{t})^{2}],\] (6)

where \(e_{i}=(x_{i},y_{i})\) and \(y_{i}= w_{},x_{i}+_{i}\). This problem was initially explored by Garg et al. (2022). Later, von Oswald et al. (2023a) have demonstrated that a linear transformer (6) converges to a form of a gradient descent solution, which they called GD++. We define this in details later.

Footnote â€¡: For the case of a linear transformer, we use the following notation: \(=_{w_{} N(0,I)\\ x_{i} N(0,I)\\ _{i} N(0,_{}^{2})}[(_{}(\{e_ {1},...,e_{n}\},e_{n+1})-y_{t})^{2}].\)

Mixed noise variance.In this case, the noise variance \(_{}\) is drawn from some fixed distribution \(p(_{})\) for each sequence. The in-context learning loss becomes:

\[L()=}_{w_{} N(0,I)\\ x_{i} N(0,I)\\ _{i} N(0,_{}^{2})\\ _{} p(_{})}[(_{}(\{e_ {1},...,e_{n}\},e_{n+1})-y_{t})^{2}].\] (7)In other words, each training sequence \(\) has a fixed noise level \(_{}\), but different training sequences have different noise levels sampled from a specified distribution \(p(_{})\). This scenario adds complexity because the model must predict \(w_{}\) for changing noise distribution, and the optimal solution likely would involve some sort of noise estimation. We have found that empirically, GD\({}^{++}\) fails to model this noise variance and instead converges to a solution which can be interpreted as a single noise variance estimate across all input data.

## 3 Related work

In-context Learning as Gradient DescentOur work builds on research that frames in-context learning as (variants of) gradient descent (Akyurek et al., 2022; von Oswald et al., 2023). For 1-layer linear transformer, several works Zhang et al. (2023); Mahankali et al. (2023); Ahn et al. (2023) characterized the optimal parameters and training dynamics. More recent works extended the ideas to auto-regressive models (Li et al., 2023; von Oswald et al., 2023) and nonlinear models (Cheng et al., 2023). Fu et al. (2023) noticed that transformers perform similarly to second-order Newton methods on linear data, for which we give a plausible explanation in Theorem 5.1.

In-context Learning in LLMsThere are also many works that study how in-context learning works in pre-trained LLMs (Kossen et al., 2023; Wei et al., 2023; Hendel et al., 2023; Shen et al., 2023). Due to the complexity of such models, the exact mechanism for in-context learning is still a major open problem. Several works (Olsson et al., 2022; Chan et al., 2022; Akyurek et al., 2024) identified induction heads as a crucial mechanism for simple in-context learning tasks, such as copying, token translation and pattern matching.

Other theories for training transformersOther than the setting of linear models, several other works (Garg et al., 2022; Tarzanagh et al., 2023; Li et al., 2023; Huang et al., 2023; Tian et al., 2023, 2023) considered optimization of transformers under different data and model assumptions. Wen et al. (2023) showed that it can be difficult to interpret the "algorithm" performed by transformers without very strong restrictions.

Mixed Linear ModelsSeveral works observed that transformers can achieve good performance on a mixture of linear models (Bai et al., 2023; Pathak et al., 2023; Yadlowsky et al., 2023). While these works show that transformers can implement many variants of model-selection techniques, our result shows that linear transformers solve such problems by discovering interesting optimization algorithm with many hyperparameters tuned during the training process. Such a strategy is quite different from traditional ways of doing model selection. Transformers are also known to be able to implement strong algorithms in many different setups (Guo et al., 2023; Giannou et al., 2023).

Effectiveness of linear and kernel-like transformersA main constraint on transformer architecture is that it takes \(O(N^{2})\) time for a sequence of length \(N\), while for a linear transformer this can be improved to \(O(N)\). Mirchandani et al. (2023) showed that even linear transformers are quite powerful for many tasks. Other works (Katharopoulos et al., 2020; Wang et al., 2020; Schlag et al., 2021; Choromanski et al., 2020) uses ideas similar to kernel/random features to improve the running time to almost linear while not losing much performance.

## 4 Linear transformers maintain linear regression model at every layer

While large, nonlinear transformers can model complex relationship, we show that linear transformers are restricted to maintaining a linear regression model based on the input, in the sense that the \(l\)-th layer output is always a linear function of the input with latent (and possibly nonlinear) coefficients.

**Theorem 4.1**.: _Suppose the output of a linear transformer at 1-th layer is \((x_{1}^{l},y_{1}^{l}),(x_{2}^{l},y_{2}^{l}),...,(x_{n}^{l},y_{n}^{l}),(x_{t}^{l},y_ {t}^{l})\), then there exists matrices \(M^{l}\), vectors \(u^{l},w^{l}\) and scalars \(a^{l}\) such that_

\[x_{i}^{l+1} =M^{l}x_{i}+y_{i}u^{l}, x_{t}^{l+1} =M^{l}x_{t},\] \[y_{i}^{l+1} =a^{l}y_{i}- w^{l},x_{i}, y_{t}^{l+1} =- w^{l},x_{t}.\]

Note that \(M^{l}\), \(u^{l},w^{l}\) and \(a^{l}\) are not linear in the input, but this still poses restrictions on what the linear transformers can do. For example we show that it cannot represent a quadratic function:

**Theorem 4.2**.: _Suppose the input to a linear transformer is \((x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\) where \(x_{i} N(0,I)\) and \(y_{i}=w^{}x_{i}\), let the 1-th layer output be \((x_{1}^{l},y_{1}^{l}),(x_{2}^{l},y_{2}^{l}),...,(x_{n}^{l},y_{n}^{l})\) and let \(y^{l}=(y_{1}^{l},...,y_{n}^{l})\) and \(y^{*}=(x_{1}(1)^{2},x_{2}(1)^{2},...,x_{n}(1)^{2})\) (here \(x_{i}(1)\) is just the first coordinate of \(x_{i}\)), then when \(n d\) with high probability the cosine similarity of \(y^{*}\) and \(y^{l}\) is at most 0.1._

Theorem 4.1 implies that the output of linear transformer can always be explained as linear combinations of input with latent weights \(a^{l}\) and \(w^{l}\). The matrices \(M^{l}\), vectors \(u^{l},w^{l}\) and numbers \(a^{l}\) are not linear and can in fact be quite complex, which we characterize below:

**Lemma 4.3**.: _In the setup of Theorem 4.1, if we let_

\[(A^{l}&b^{l}\\ (c^{l})^{}&d^{l}):=\] \[_{k=1}^{h}[P_{k}^{l}_{j=1}^{n}(( []{c}x_{j}^{l}\\ y_{j}^{l})((x_{j}^{l})^{},y_{j}^{l}))Q_{k}^{l} ],\]

_then one can recursively compute matrices \(M^{l}\), vectors \(u^{l},w^{l}\) and numbers \(a^{l}\) for every layer using_

\[M^{l+1} =(I+A^{l})M^{l}+b^{l}(w^{l})^{}\] \[u^{l+1} =(I+A^{l})u^{l}+a^{l}b^{l}\] \[a^{l+1} =(1+d^{l})a^{l}+ c^{l},u^{l}\] \[w^{l+1} =(1+d^{l})w^{l}-(M^{l})^{}c^{l},\]

_with the init. condition \(a^{0}=1,w^{0}=0,M^{0}=I,u^{0}=0\)._

The updates to the parameters are complicated and nonlinear, allowing linear transformers to implement powerful algorithms, as we will later see in Section 5. In fact, even with diagonal \(P\) and \(Q\), they remain flexible. The updates in this case can be further simplified to a more familiar form:

**Lemma 4.4**.: _In the setup of Theorem 4.1 with diagonal parameters, \(u^{l},w^{l}\) are updated as_

\[u^{l+1} =(I-^{l})u^{l}+^{l}(a^{l}w^{*}-w^{l} );\] \[w^{l+1} =(1+s^{l})w^{l}-^{l}(a^{l}w^{*}-w^{l})-^{l}u^{l}.\]

_Here \(^{l},^{l},s^{l},^{l},^{l}\) are matrices and numbers that depend on \(M^{l},u^{l},a^{l},w^{l}\) in Lemma 4.3._

Note that \((a^{l}w^{*}-w^{l})\) is (proportional to) the gradient of a linear model \(f(w^{l})=_{i=1}^{n}(a^{l}y_{i}- w^{l},x_{i})^{2}\). This makes the updates similar to a gradient descent with momentum:

\[u^{l+1}=(1-)u^{l}+ f(w^{l});w^{l+1}=w^{l}- u^{l}.\]

Of course, the formula in Lemma 4.4 is still much more complicated with matrices in places of \(\) and \(\), and also including a gradient term for the update of \(w\).

## 5 Power of diagonal attention matrices

Although linear transformers are constrained, they can solve complex in-context learning problems. Empirically, we have found that they are able to very accurately solve linear regression with mixed noise variance (7), with final learned weights that are very diagonal heavy with some low-rank component (see Fig. 4). Surprisingly, the final loss remains remarkably consistent even when their \(Q\) and \(P\) matrices (3) are diagonal. Here we will analyze this special case and explain its effectiveness.

Since the elements of \(x\) are permutation invariant, a diagonal parameterization reduces each attention heads to just four parameters:

\[P_{k}^{l}=(p_{x,k}^{l}I&0\\ 0&p_{y,k}^{l}); Q_{k}^{l}=(q_{x,k}^ {l}I&0\\ 0&q_{y,k}^{l}).\] (8)

It would be useful to further reparametrize the linear transformer (3) using:

\[_{xx}^{l}=_{k=1}^{H}p_{x,k}^{l}q_{x,k}^{l},& _{xy}^{l}=_{k=1}^{H}p_{x,k}^{l}q_{y,k}^{l},\\ _{yx}^{l}=_{k=1}^{H}p_{y,k}^{l}q_{x,k}^{l},&_{yy}^{l}=_{k=1} ^{H}p_{y,k}^{l}q_{y,k}^{l}.\] (9)

This leads to the following diagonal layer updates:

\[x_{i}^{l+1}=x_{i}^{l}+_{xx}^{l}^{l}x_{i}^{l}+w_ {xy}^{l}y_{i}^{l}^{l}\\ x_{t}^{l+1}=x_{t}^{l}+_{xx}^{l}^{l}x_{t}^{l}+w_{xy}^{l}y_{t}^{l} ^{l}\\ y_{i}^{l+1}=y_{i}^{l}+_{yx}^{l}^{l},x_{i}^{l}+ _{yy}^{l}y_{i}^{l}^{l},\\ y_{t}^{l+1}=y_{t}^{l}+_{yx}^{l}^{l},x_{t}^{l}+ _{yy}^{l}y_{t}^{l}^{l}.\] (10)

Four variables \(_{xx}^{l}\), \(_{xy}^{l}\), \(_{yx}^{l}\), \(_{yy}^{l}\) represent information flow between the data and the labels across layers. For instance, the controlled by \(_{xx}^{l}\) measures information flow from \(x^{l}\) to \(x^{l+1}\), \(_{yx}^{l}\) measures the flow from \(x^{l}\) to \(y^{l+1}\) and so forth. Since the model can always be captured by these 4 variables, having many heads does not significantly increase its representation power. When there is only one head the equation \(_{xx}^{l}_{yy}^{l}=_{xy}^{l}_{yx}^{l}\) is always true, while models with more than one head do not have this limitation. However empirically even models with one head is quite powerful.

### GD\({}^{++}\) and least squares solver

GD\({}^{++}\), introduced in von Oswald et al. (2023a), represents a linear transformer that is trained on a fixed noise variance problem (6). It is a variant of a diagonal linear transformer, with all the heads satisfying \(q_{y,k}^{l}=0\). Dynamics are influenced only by \(_{xx}^{l}\) and \(_{yx}^{l}\), leading to simpler updates:

\[x_{i}^{l+1}=(I+_{xx}^{l}^{l})x_{i}^ {l}\\ y_{i}^{l+1}=y_{i}^{l}+_{yx}^{l}^{l},x_{i}^{l}.\] (11)

The update on \(x\) acts as preconditioning and the update on \(y\) performs gradient descent on the data.

While existing analysis by Ahn et al. (2023) has not yielded fast convergence rates for GD\({}^{++}\), we show here that it is actually a second-order optimization algorithm for the least squares problem (4):

**Theorem 5.1**.: _Given \((x_{1},y_{1}),...,(x_{n},y_{n}),(x_{t},0)\) where \(\) has eigenvalues in the range \([,]\) with a condition number \(=/\). Let \(w^{*}\) be the optimal solution to least squares problem (4), then there exists hyperparameters for GD\({}^{++}\) algorithm that outputs \(\) with accuracy \(|- x_{t},w^{*}|\|x_{t}\|\|w^{*}\|\) in \(l=O(+ 1/)\) steps. In particular that implies there exists an \(l\)-layer linear transformer that can solve this task._

The convergence rate of \(O( 1/)\) is typically achieved only by second-order algorithms such as Newton's method.

### Understanding \(_{yy}\): adaptive rescaling

If a layer only has \(_{yy}^{l} 0\), it has a rescaling effect. The amount of scaling is related to the amount of noise added in a model selection setting. The update rule for this layer is:

\[y_{i}^{l+1}=(1+_{yy}^{l}^{l})y_{i}^{l}.\]

This rescales every \(y\) by a factor that depends on \(^{l}\). When \(_{yy}^{l}<0\), this shrinks of the output based on the norm of \(y\) in the previous layer. This is useful for the mixed noise variance problem, as ridge regression solution scales the least squares solution by a factor that depends on the noise level.

Specifically, assuming \([]=nI\), the ridge regression solution becomes \(w_{^{2}}^{*}}w^{*}\), which is exactly a scaled version of the OLS solution. Further, when noise is larger, the scaled factor is smaller, which agrees with the behavior of a negative \(_{yy}\).

We can show that using adaptive scaling \(_{yy}\) even a 2-layer linear transformer can be enough to solve a simple example of categorical mixed noise variance problem \(_{}\{_{1},_{2}\}\) and \(n\):

**Theorem 5.2**.: _Suppose the input to the transformer is \((x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n}),(x_{q},0)\), where \(x_{i} N(0,I)\), \(y_{i}=w^{}x_{i}+_{i}\). Here \(_{i} N(0,^{2})\) is the noise whose noise level \(\) can take one of two values: \(_{1}\) or \(_{2}\). Then as \(n\) goes to \(+\), there exists a set of parameters for two-layer linear transformers such that the implicit \(w^{2}\) of the linear transformer converges to the optimal ridge regression results (and the output of the linear transformer is \(- w^{2},x_{q}\)). Further, the first layer only has \(_{yx}\) being nonzero and the second layer only has \(_{yy}\) being nonzero._

### Understanding \(_{xy}\): adapting step-sizes

The final term in the diagonal model, \(_{xy}\), has a more complicated effect. Since it changes only the \(x\)-coordinates, it does not have an immediate effect on \(y\). To understand how it influences the \(y\) we consider a simplified two-step process, where the first step only has \(_{xy} 0\) and the second step only has \(_{yx} 0\) (so the second step is just doing one step of gradient descent). In this case, the first layer will update the \(x_{i}\)'s as:

\[x_{i}^{1} =x_{i}+y_{i}_{xy}_{j=1}^{n}y_{j}x_{j}\] \[=x_{i}+_{xy}y_{i}_{j=1}^{n}( w^{*},x_{j} +r_{j})x_{j}\] \[=x_{i}+_{xy}y_{i} w^{*}\] \[=x_{i}+_{xy}( w^{*},x_{i}+r_{i}) w^{*}\] \[=(I+_{xy} w^{*}(w^{*})^{})x_{i}+_{xy}r_{i}  w^{*}.\]

There are two effects of the \(_{xy}\) term, one is a multiplicative effect on \(x_{i}\), and the other is an additive term that makes \(x\)-output related to the residual \(r_{i}\). The multiplicative step in \(x_{i}\) has an unknown preconditioning effect. For simplicity we assume the multiplicative term is small, that is:

\[x_{i}^{1} x_{i}+_{xy}r_{i} w^{*}; x_{t}^{1} x_ {t}.\]

The first layer does not change \(y\), so \(y_{t}^{1}=y_{t}\) and \(y_{i}^{1}=y_{i}\). For this set of \(x_{i}\), we can write down the output on \(y\) in the second layer as

\[y_{t}^{2} =y_{t}+_{yx}_{i=1}^{n}y_{i}(x_{i}^{1})^{}x_{t}\] \[ y_{t}+_{yx}[_{i=1}^{n}y_{i}x_{i}+_{xy} _{i=1}^{n}y_{i}r_{i} w^{*}]x_{t}\] \[=y_{t}+_{yx}(1+_{xy}_{i=1}^{n}r_{i}^{2})( w ^{*})^{}x_{t}.\]

Figure 1: In-context learning performance for noisy linear regression problem across models with different number of layers and \(_{max}\) for \(_{} U(0,_{max})\). Each marker corresponds to a separately trained model with a given number of layers. Models with diagonal attention weights (Diag) match those with full attention weights (Full). Models specialized on a fixed noise (GD\({}^{++}\)) perform poorly, similar to a Ridge Regression solution with a constant noise (ConstRR). Among the baselines, only tuned exact Ridge Regression solution (TunedRR) is comparable with linear transformers.

Here we used the properties of residual \(r_{i}\) (in particular \(_{i}y_{i}x_{i}= w^{*}\), and \(_{i}y_{i}r_{i}=_{i}r_{i}^{2}\)). Note that \(( w^{*})^{}x_{t}\) is basically what a gradient descent step on the original input should do. Therefore effectively, the two-layer network is doing gradient descent, but the step size is the product of \(-_{yx}\) and \((1+_{xy}_{i}r_{i}^{2})\). The factor \((1+_{xy}_{i}r_{i}^{2})\) depends on the level of noise, and when \(_{xy},_{yx}<0\), the effective step size is smaller when there is more noise. This is especially helpful in the model selection problem, because intuitively one would like to perform early-stopping (small step sizes) when the noise is high.

## 6 Experiments

In this section, we investigate the training dynamics of linear transformers when trained with a mixed noise variance problem (7). We evaluate three types of single-head linear transformer models:

* Full. Trains full parameter matrices.
* Diag. Trains diagonal parameter matrices (10).
* GD\({}^{++}\). An even more restricted diagonal variant defined in (11).

For each experiment, we train each linear transformer modifications with a varying number of layers (1 to 7) using using Adam optimizer for \(200\,000\) iterations with a learning rate of \(0.0001\) and a batch size of \(2\,048\). In some cases, especially for a large number of layers, we had to adjust the learning rate to prevent stability issues. We report the best result out of \(5\) runs with different training seeds. We used \(N=20\) in-context examples in \(D=10\) dimensions. We evaluated the algorithm using \(100\,000\) novel sequences. All the experiments were done on a single H100 GPU with 80GB of VRAM. It took on average 4-12 hours to train a single algorithm, however experimenting with different weight decay parameters, better optimizer and learning rate schedule will likely reduce this number dramatically.

We use _adjusted evaluation loss_ as our main performance metric. It is calculated by subtracting the oracle loss from the predictor's loss. The oracle loss is the closed-form solution of the ridge regression loss (5), assuming the noise variance \(_{}\) is known. The adjusted evaluation loss allows for direct model performance comparison across different noise variances. This is important because higher noise significantly degrades the model prediction. Our adjustment does not affect the model's optimization process, since it only modifies the loss by an additive constant.

Baseline estimates.We evaluated the linear transformer against a closed-form solution to the ridge regression problem (5). We estimated the noise variance \(_{}\) using the following methods:

* _Constant Ridge Regression_ (ConstRR). The noise variance is estimated using a single scalar value for all the sequences, tuned separately for each mixed variance problem.

Figure 2: Per-variance profile of models behavior for uniform noise variance \(_{} U(0,_{max})\). _Top two rows:_ 7-layer models with varying \(_{max}\). _Bottom row_: models with varying numbers of layers, fixed \(_{max}=5\). In-distribution noise is shaded gray.

* _Adaptive Ridge Regression (_AdaRR_). Estimate the noise variance via unbiased estimator (Cherkassky & Ma, 2003) \(_{}^{2}=_{j=1}^{n}(y_{j}-_{j})^{2}\), where \(_{j}\) represents the solution to the ordinary least squares (4), found in a closed-form.
* _Tuned Adaptive Ridge Regression (_TunedR_). Same as above, but after the noise is estimated, we tuned two additional parameters to minimize the evaluation loss: (1) a max. threshold value for the estimated variance, (2) a multiplicative adjustment to the noise estimator. These values are tuned separately for each problem.

Notice that all the baselines above are based on ridge regression, which is a closed-form, non-iterative solution. Thus, they have an algorithmic advantage over linear transformers that do not have access to matrix inversion. These baselines help us gauge the best possible performance, establishing an upper bound rather than a strictly equivalent comparison.

A more faithful comparison to our method would be an iterative version of the AdaRR that does not use matrix inversion. Instead, we can use gradient descent to estimate the noise and the solution to the ridge regression. However, in practice, this gradient descent estimator converges to AdaRR only after \( 100\) iterations. In contrast, linear transformers typically converge in fewer than \(10\) layers.

We consider two choices for the distribution of \(_{}\):

* _Uniform._\(_{} U(0,_{max})\) drawn from a uniform distribution bounded by \(_{max}\). We tried multiple scenarios with \(_{max}\) ranging from 0 to 7.
* _Categorical._\(_{} S\) chosen from a discrete set \(S\). We tested \(S=\{1,3\}\) and \(S=\{1,3,5\}\).

Our approach generalizes the problem studied by Bai et al. (2023), who considered only categorical variance selection and show experiments only with two \(_{}\) values.

Uniform noise variance.For the uniform noise variance, Fig. 1 shows that Full and Diag achieve comparable performance across different numbers of layers and different \(_{max}\). On the other hand, GD\({}^{++}\) converges to a higher value, closely approaching the performance of the ConstRR baseline.

As \(_{max}\) grows, linear transformers show a clear advantage over the baselines. With 4 layers, they outperform the closed-form solution AdaRR for \(_{max}=4\) and larger. Models with \(5\) or more layers match or exceed the performance of TunedRR.

The top of Fig. 2 offers a detailed perspective on performance of 7-layer models and the baselines. Here, we computed per-variance profiles across noise variance range from 0 to \(_{max}+1\). We can see that poor performance of GD\({}^{++}\) comes from its inability to estimate well across the full noise variance range. Its performance closely mirrors to ConstRR, suggesting that GD\({}^{++}\) under the hood might also be estimating a single constant variance for all the data.

AdaRR perfectly estimates problems with no noise, but struggles more as noise variance increases. TunedRR slightly improves estimation by incorporating \(_{max}\) into its tunable parameters, yet its

Figure 3: In-context learning performance for noisy linear regression across models with varying number of layers for conditional noise variance \(_{}\{1,3\}\) and \(_{}\{1,3,5\}\). _Top:_ loss for models with various number of layers and per-variance profile for models with 7 layers. _Bottom:_ Per-variance profile of the model across different numbers of layers. In-distribution noise is shaded gray.

prediction suffers in the mid-range. Full and Diag demonstrate comparable performance across all noise variances. While more research is needed to definitively confirm or deny their equivalence, we believe that these models are actually not identical despite their similar performance.

At the bottom of Fig. 2 we set the noise variance to \(_{max}=5\) and display a per-variance profile for models with varying layers. Two-layer models for Full and Diag behave akin to GD\({}^{++}\), modeling only a single noise variance in the middle. However, the results quickly improve across the entire noise spectrum for 3 or more layers. In contrast, GD\({}^{++}\) quickly converges to a suboptimal solution.

Categorical noise variance.Fig. 3 shows a notable difference between Diag and Full models for categorical noise variance \(_{}\{1,3\}\). This could stem from a bad local minima, or suggest a fundamental difference between the models for this problem. Interestingly, from per-variance profiling we see that Diag extrapolates better for variances not used for training, while Full, despite its lower in-distribution error, performs worse on unseen variances. Fig. 4 shows learned weights of the 4 layer linear transformer with Full parametrization. The weights are very diagonal heavy, potentially with some low-rank component.

For \(_{}\{1,3,5\}\), examining the per-variance profile at the bottom of Fig. 3 reveals differences in their behaviors. Full exhibits a more complex per-variance profile with more fluctuations than the diagonal model, suggesting greater representational capacity. Surprisingly, it did not translate to better loss results compared to Diag.

For easy comparison, we compile the results of all methods and baselines in Table 1 in the Appendix.

## 7 Conclusions

Our research reveals the surprising ability of linear transformers to tackle challenging in-context learning problems. We show that each layer maintains an implicit linear regression model, akin to a complex variant of preconditioned gradient descent with momentum-like behavior.

Remarkably, when trained on noisy linear regression problems with unknown noise variance, linear transformers not only outperform standard baselines but also uncover a sophisticated optimization algorithm that incorporates noise-aware step-size adjustments and rescaling. This discovery highlights the potential of linear transformers to automatically discover novel optimization algorithms when presented with the right problems, opening exciting avenues for future research, including automated algorithm discovery using transformers and generalization to other problem domains.

While our findings demonstrate the impressive capabilities of linear transformers in learning optimization algorithms, we acknowledge limitations in our work. These include the focus on simplified linear models, analysis of primarily diagonal attention matrices, and the need for further exploration into the optimality of discovered algorithms, generalization to complex function classes, scalability with larger datasets, and applicability to more complex transformer architectures. We believe these limitations present valuable directions for future research and emphasize the need for a deeper understanding of the implicit learning mechanisms within transformer architectures.

Figure 4: Weights for 4 layer linear transformer with Full parametrization trained with categorical noise \(_{}\{1,3\}\). _Top:_ weights for \(Q^{l}\) matrix, _bottom:_ weights for \(P^{l}\) matrix.

Acknowledgements

The authors would like to thank Nolan Miller and Andrey Zhmoginov for their valuable suggestions and feedback throughout the development of this project. Part of this work was done while Rong Ge was visiting Google Research. Rong Ge's research is supported in part by NSF Award DMS-2031849 and CCF-1845171 (CAREER).