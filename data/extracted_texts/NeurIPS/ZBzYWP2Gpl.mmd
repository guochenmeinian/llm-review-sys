# We make the following key contributions:

[MISSING_PAGE_FAIL:1]

_scan_: distance computation w.r.t all points in the retrieved cluster to find the nearest neighbor (NN). Standard IVF utilizes the same high-dimensional \(\) for both phases, which can be sub-optimal.

**Why the sub-optimality?** Imagine one needs to partition a dataset into \(k\) clusters for IVF and the dimensionality of the data is \(d-\) uses full \(d\) representation to partition into \(k\) clusters. However, suppose we have an alternate approach that somehow projects the data in \(d/2\) dimensions and learns \(2k\) clusters. Note that the storage and computation to find the nearest cluster remains the same in both cases, i.e., when we have \(k\) clusters of \(d\) dimensions or \(2k\) clusters of \(d/2\) dimensions. \(2k\) clusters can provide significantly more refined partitioning, but the distances computed between queries and clusters could be significantly more inaccurate after projection to \(d/2\) dimensions.

So, if we can find a mechanism to obtain a \(d/2\)-dimensional representation of points that can accurately approximate the topology/distances of \(d\)-dimensional representation, then we can potentially build significantly better ANNS structure that utilizes different capacity representations for the cluster mapping and linear scan phases of IVF. But how do we find such _adaptive representations?_ These desired adaptive representations should be cheap to obtain and still ensure distance preservation across dimensionality. Post-hoc dimensionality reduction techniques like SVD  and random projections  on high-dimensional \(\) are potential candidates, but our experiments indicate that in practice they are highly inaccurate and do not preserve distances well enough (Figure 2).

Instead, we identify that the recently proposed Matryoshka Representations (\(\))  satisfy the specifications for adaptive representations. Matryoshka representations pack information in a hierarchical nested manner, i.e., the first \(m\)-dimensions of the \(d\)-dimensional \(\) form an accurate low-dimensional representation while being aware of the information in the higher dimensions. This allows us to deploy \(\) in two major and novel ways as part of ANNS: (a) low-dimensional representations for accuracy-compute optimal clustering and quantization, and (b) high-dimensional representations for precise re-ranking when feasible.

To this effort, we introduce \(\), a novel design framework for semantic search that uses matryoshka representation-based _adaptive representations_ across different stages of ANNS to ensure significantly better accuracy-compute trade-off than the state-of-the-art baselines.

Typical ANNS systems have two key components: (a) search data structure to store datapoints, (b) distance computation to map a given query to points in the data structure. Through \(\), we address both these components and significantly improve their performance. In particular, we first propose \(\)-IVF (Section 4.1) which tackles the first component of ANNS systems. \(\)-IVF uses standard full-precision computations but uses adaptive representations for different IVF stages. On ImageNet 1-NN image retrieval (Figure 0(a)), \(\)-IVF is up to \(1.5\%\) more accurate for the compute budget and \(90\) cheaper in deployment for the same accuracy as IVF.

Figure 1: \(\) helps design search data structures and quantization methods with _better accuracy-compute trade-offs_ than the existing solutions. In particular, (a) \(\)-IVF improves on standard IVF by up to \(1.5\%\) in accuracy while being \(90\) faster in deployment and (b) \(\)-OPQ is as accurate as the baseline at _half the cost!_ Rigid-IVF and Rigid-OPQ are standard techniques that are built on rigid representations (\(\)) while \(\) uses matryoshka representations (\(\)) .

We then propose \(\)-OPQ (Section 4.2) which addresses the second component by using \(\)-based quantization (OPQ ) - here we use exhaustive search overall points. \(\)-OPQ is as accurate as the baseline OPQ on \(\) while being at least \(\) faster on Natural Questions  1-NN passage retrieval (Figure 0(b)). Finally, we combine the two techniques to obtain \(\)-IVFOPQ (Section 4.3) which is more accurate while being much cheaper - up to \(\) - than the traditional IVFOPQ  index. To demonstrate generality of our technique, we adapt \(\) to DiskANN  which provides interesting accuracy-compute tradeoff; see Table 1.

While \(\) already has multi-granular representations, careful integration with ANNS building blocks is critical to obtain a practical method and is _our main contribution_. In fact, Kusupati et al.  proposed a simple adaptive retrieval setup that uses smaller-dimensional \(\) for shortlisting in retrieval followed by precise re-ranking with a higher-dimensional \(\). Such techniques, unfortunately, cannot be scaled to industrial systems as they require forming a new index for every shortlisting provided by low-dimensional \(\). Ensuring that the method aligns well with the modern-day ANNS pipelines is important as they already have mechanisms to handle real-world constraints like load-balancing  and random access from disk . So, \(\) is a step towards making the abstraction of adaptive search and retrieval feasible at the web-scale.

Through extensive experimentation, we also show that \(\) generalizes across search data structures, distance approximations, modalities (text & image), and encoders (CNNs & Transformers) while still translating the theoretical gains to latency reductions in deployment. While we have mainly focused on IVF and OPQ-based ANNS in this work, \(\) also blends well with other ANNS pipelines. We also show that \(\) can enable compute-aware elastic search on prebuilt indices without making any modifications (Section 5.1); note that this is in contrast to \(\)-IVF that builds the index explicitly utilizing "adaptivity" in representations. Finally, we provide an extensive analysis on the alignment of matryoshka representation for better semantic search (Section 5.2).* We introduce \(\), a novel framework for semantic search that leverages matryoshka representations for designing ANNS systems with better accuracy-compute trade-offs.
* \(\) powered search data structure (\(\)-IVF) and quantization (\(\)-OPQ) show a significant improvement in accuracy-compute tradeoff compared to existing solutions.
* \(\) generalizes to modern-day composite ANNS indices and can also enable compute-aware elastic search during inference with no modifications.

## 2 Related Work

Approximate nearest neighbour search (ANNS) is a paradigm to come as close as possible  to retrieving the "true" nearest neighbor (NN) without the exorbitant search costs associated with exhaustive search [21; 52]. The "approximate" nature comes from data pruning as well as the cheaper distance computation that enable real-time web-scale search. In its naive form, NN-search has a complexity of \((dN)\); \(d\) is the data dimensionality used for distance computation and \(N\) is the size of the database. ANNS employs each of these approximations to reduce the linear dependence on the dimensionality (cheaper distance computation) and data points visited during search (data pruning).

**Cheaper distance computation.** From a bird's eye view, cheaper distance computation is always obtained through dimensionality reduction (quantization included). PCA and SVD [14; 26] can reduce dimensionality and preserve distances only to a limited extent without sacrificing accuracy. On the other hand, quantization-based techniques [6; 15] like (optimized) product quantization ((O)PQ) [13; 23] have proved extremely crucial for relatively accurate yet cheap distance computation and simultaneously reduce the memory overhead significantly. Another naive solution is to independently train the representation function with varying low-dimensional information bottlenecks  which is rarely used due to the costs of maintaining multiple models and databases.

**Data pruning.** Enabled by various data structures, data pruning reduces the number of data points visited as part of the search. This is often achieved through hashing [8; 46], trees [3; 12; 16; 48] and graphs [22; 38]. More recently there have been efforts towards end-to-end learning of the search data structures [17; 29; 30]. However, web-scale ANNS indices are often constructed on rigid \(d\)-dimensional real vectors using the aforementioned data structures that assist with the real-time search. For a more comprehensive review of ANNS structures please refer to [5; 34; 51].

**Composite indices.** ANNS pipelines often benefit from the complementary nature of various building blocks [24; 42]. In practice, often the data structures (coarse-quantizer) like IVF  and HNSW  are combined with cheaper distance alternatives like PQ  (fine-quantizer) for massive speed-ups in web-scale search. While the data structures are built on \(d\)-dimensional real vectors, past works consistently show that PQ can be safely used for distance computation during search time. As evident in modern web-scale ANNS systems like DiskANN , the data structures are built on \(d\)-dimensional real vectors but work with PQ vectors (\(32-64\)-byte) for fast distance computations.

**ANNS benchmark datasets.** Despite the Herculean advances in representation learning [19; 42], ANNS progress is often only benchmarked on fixed representation vectors provided for about a dozen million to billion scale datasets [1; 47] with limited access to the raw data. This resulted in the improvement of algorithmic design for rigid representations (\(\)) that are often not specifically designed for search. All the existing ANNS methods work with the assumption of using the provided \(d\)-dimensional representation which might not be Pareto-optimal for the accuracy-compute trade-off in the first place. Note that the lack of raw-image and text-based benchmarks led us to using ImageNet-1K  (1.3M images, 50K queries) and Natural Questions  (21M passages, 3.6K queries) for experimentation. While not billion-scale, the results observed on ImageNet often translate to real-world progress , and Natural Questions is one of the largest question answering datasets benchmarked for dense passage retrieval , making our results generalizable and widely applicable.

In this paper, we investigate the utility of adaptive representations - embeddings of different dimensionalities having similar semantic information - in improving the design of ANNS algorithms. This helps in transitioning out of restricted construction and inference on rigid representations for ANNS. To this end, we extensively use Matryoshka Representations (\(\))  which have desired adaptive properties in-built. To the best of our knowledge, this is the first work that improves accuracy-compute trade-off in ANNS by leveraging adaptive representations on different phases of construction and inference for ANNS data structures.

## 3 Problem Setup, Notation, and Preliminaries

The problem setup of approximate nearest neighbor search (ANNS)  consists of a database of \(N\) data points, \([x_{1},x_{2},,x_{N}]\), and a query, \(q\), where the goal is to "approximately" retrieve the nearest data point to the query. Both the database and query are embedded to \(^{d}\) using a representation function \(:^{d}\), often a neural network that can be learned through various representation learning paradigms [2; 19; 40; 42; 20].

Matryoshka Representations (\(\)).The \(d\)-dimensional representations from \(\) can have a nested structure like Matryoshka Representations (\(\))  in-built \(-^{(d)}\). Matryoshka Representation Learning (MRL) learns these nested representations with a simple strategy of optimizing the same training objective at varying dimensionalities. These granularities are ordered such that the lowest representation size forms a prefix for the higher-dimensional representations. So, high-dimensional \(\) inherently contains low-dimensional representations of varying granularities that can be accessed for free - first \(m\)-dimensions (\(m[d]\)) ie., \(^{(d)}[1:m]\) from the \(d\)-dimensional \(\) form an \(m\)-dimensional representation which is as accurate as its independently trained rigid representation (\(\)) counterpart - \(^{(m)}\). Training an encoder with MRL does not involve any overhead or hyperparameter tuning and works seamlessly across modalities, training objectives, and architectures.

Inverted File Index (IVF).IVF  is an ANNS data structure used in web-scale search systems  owing to its simplicity, minimal compute overhead, and high accuracy. IVF construction involves clustering (coarse quantization through k-means)  on \(d\)-dimensional representation that results in an inverted file list  of all the data points in each cluster. During search, \(d\)-dimensional query representation is assigned to the most relevant cluster (\(C_{i};i[k]\)) by finding the closest centroid (\(_{i}\)) using an appropriate distance metric (\(L_{2}\) or cosine). This is followed by an exhaustive linear search across all data points in the cluster which gives the closest NN (see Figure 5 in Appendix A for IVF overview). Lastly, IVF can scale to web-scale by utilizing a hierarchical IVF structure within each cluster . Table 2 in Appendix A describes the retrieval formula for multiple variants of IVF.

Optimized Product Quantization (OPQ).Product Quantization (PQ)  works by splitting a \(d\)-dimensional real vector into \(m\) sub-vectors and quantizing each sub-vector with an independentlength codebook across the database. After PQ, each \(d\)-dimensional vector can be represented by a compact \(m b\) bit vector; we make each vector \(m\) bytes long by fixing \(b=8\). During search time, distance computation between the query vector and PQ database is extremely efficient with only \(m\) codebook lookups. The generality of PQ encompasses scalar/vector quantization  as special cases. However, PQ can be further improved by rotating the \(d\)-dimensional space appropriately to maximize distance preservation after PQ. Optimized Product Quantization (OPQ)  achieves this by learning an orthonormal projection matrix \(R\) that rotates the \(d\)-dimensional space to be more amenable to PQ. OPQ shows consistent gains over PQ across a variety of ANNS tasks and has become the default choice in standard composite indices .

Datasets.We evaluate the ANNS algorithms while changing the representations used for the search thus making it impossible to evaluate on the usual benchmarks . Hence we experiment with two public datasets: (a) ImageNet-1K  dataset on the task of image retrieval - where the goal is to retrieve images from a database (1.3M image train set) belonging to the same class as the query image (50K image validation set) and (b) Natural Questions (NQ)  dataset on the task of question answering through dense passage retrieval - where the goal is to retrieve the relevant passage from a database (21M Wikipedia passages) for a query (3.6K questions).

MetricsPerformance of ANNS is often measured using recall score , \(k\)-recall@\(N\) - recall of the exact NN across search complexities which denotes the recall of \(k\) "true" NN when \(N\) data points are retrieved. However, the presence of labels allows us to compute 1-NN (top-1) accuracy. Top-1 accuracy is a harder and more fine-grained metric that correlates well with typical retrieval metrics like recall and mean average precision (mAP@\(k\)). Even though we report top-1 accuracy by default during experimentation, we discuss other metrics in Appendix C. Finally, we measure the compute overhead of ANNS using MFLOPS/query and also provide wall-clock times (see Appendix B.1).

Encoders.For ImageNet, we encode both the database and query set using a ResNet50 (\(_{I}\))  trained on ImageNet-1K. For NQ, we encode both the passages in the database and the questions in the query set using a BERT-Base (\(_{N}\))  model fine-tuned on NQ for dense passage retrieval .

We use the trained ResNet50 models with varying representation sizes (\(d=[8,16,,2048]\); default being \(2048\)) as suggested by Kusupati et al.  alongside the MRL-ResNet50 models trained with MRL for the same dimensionalities. The \(\) and \(\) models are trained to ensure the supervised one-vs-all classification accuracy across all data dimensionalities is nearly the same - 1-NN accuracy of \(2048\)-\(d\)\(\) and \(\) models are \(71.19\%\) and \(70.97\%\) respectively on ImageNet-1K. Independently trained models, \(_{I}^{(d)}\), output \(d=[8,16,2048]\) dimensional \(\)s while a single MRL-ResNet50 model, \(_{I}^{(d)}\), outputs a \(d=2048\)-dimensional \(\) that contains all the 9 granularities.

We also train BERT-Base models in a similar vein as the aforementioned ResNet50 models. The key difference is that we take a pre-trained BERT-Base model and fine-tune on NQ as suggested by Karpukhin et al.  with varying (5) representation sizes (bottlenecks) (\(d=[48,96,,768]\); default being \(768\)) to obtain \(_{N}^{(d)}\) that creates \(\)s for the NQ dataset. To get the MRL-BERT-Base model, we fine-tune a pre-trained BERT-Base encoder on the NQ train dataset using the MRL objective with the same granularities as \(\)s to obtain \(_{N}^{(d)}\) which contains all five granularities. Akin to ResNet50 models, the \(\) and \(\) BERT-Base models on NQ are built to have similar 1-NN accuracy for \(768\)-\(d\) of \(52.2\%\) and \(51.5\%\) respectively. More implementation details can be found in Appendix B and additional experiment-specific information is provided at the appropriate places.

## 4 \(\) - Adaptive ANNS

In this section, we present our proposed \(\) - framework that exploits the inherent flexibility of matryoshka representations to improve the accuracy-compute trade-off for semantic search components. Standard ANNS pipeline can be split into two key components: (a) search data structure that indexes and stores data points, (b) query-point computation method that outputs (approximate) distance between a given query and data point. For example, standard IVFOPQ  method uses an IVF structure to index points on full-precision vectors and then relies on OPQ for more efficient distance computation between the query and the data points during the linear scan.

Below, we show that \(\) can be applied to both the above-mentioned ANNS components and provides significant gains on the computation-accuracy tradeoff curve. In particular, we present \(\)-IVF which is \(\) version of the standard IVF index structure , and the closely related ScaNN structure . We also present \(\)-OPQ which introduces representation adaptivity in the OPQ, an industry-default quantization. Then, in Section 4.3 we further demonstrate the combination of the two techniques to get \(\)-IVFOPQ - an \(\) version of IVFOPQ  - and \(\)-DiskANN, a similar variant of DiskANN . Overall, our experiments show that \(\)-IVF is significantly more accuracy-compute optimal compared to the IVF indices built on \(\) and \(\)-OPQ is as accurate as the OPQ on \(\) while being significantly cheaper.

### \(\)-IVF

Recall from Section 1 that IVF has a clustering and a linear scan phase, where both phase use same dimensional rigid representation. Now, \(\)-IVF allows the clustering phase to use the first \(d_{c}\) dimensions of the given matyoshka representation (\(\)). Similarly, the linear scan within each cluster uses \(d_{s}\) dimensions, where again \(d_{s}\) represents top \(d_{s}\) coordinates from \(\). Note that setting \(d_{c}=d_{s}\) results in non-adaptive regular IVF. Intuitively, we would set \(d_{c} d_{s}\), so that instead of clustering with a high-dimensional representation, we can approximate it accurately with a low-dimensional embedding of size \(d_{c}\) followed by a linear scan with a higher \(d_{s}\)-dimensional representation. Intuitively, this helps in the smooth search of design space for state-of-the-art accuracy-compute trade-off. Furthermore, this can provide a precise operating point on accuracy-compute tradeoff curve which is critical in several practical settings.

Our experiments on regular IVF with \(\) and \(\) (IVF-\(\) & IVF-\(\)) of varying dimensionalities and IVF configurations (# clusters, # probes) show that (Figure 2) matryoshka representations result in a significantly better accuracy-compute trade-off. We further studied and found that learned lower-dimensional representations offer better accuracy-compute trade-offs for IVF than higher-dimensional embeddings (see Appendix E for more results).

\(\) utilizes \(d\)-dimensional matryoshka representation to get accurate \(d_{c}\) and \(d_{s}\) dimensional vectors at no extra compute cost. The resulting \(\)-IVF provides a much better accuracy-compute trade-off (Figure 2) on ImageNet-1K retrieval compared to IVF-\(\), IVF-\(\), and MG-IVF-\(\) - multi-granular IVF with rigid representations (akin to \(\) without \(\)) - a strong baseline that uses \(d_{c}\) and \(d_{s}\) dimensional \(\). Finally, we exhaustively search the design space of IVF by varying \(d_{c},d_{s}[8,16,,2048]\) and the number of clusters \(k[8,16,,2048]\). Please see Appendix E for more details. For IVF experiments on the NQ dataset, please refer to Appendix G.

Empirical results.Figure 2 shows that \(\)-IVF outperforms the baselines across all accuracy-compute settings for ImageNet-1K retrieval. \(\)-IVF results in \(10\) lower compute for the best accuracy of the extremely expensive MG-IVF-\(\) and non-adaptive IVF-\(\). Specifically, as shown in Figure 0(a), \(\)-IVF is up to \(1.5\%\) more accurate for the same compute and has up to \(100\) lesser FLOPS/query (\(90\) real-world speed-up!) than the status quo ANNS on rigid representations (IVF-\(\)). We filter out points for the sake of presentation and encourage the reader to check out Figure 8 in Appendix E for an expansive plot of all the configurations searched.

The advantage of \(\) for construction of search structures is evident from the improvements in IVF (\(\)-IVF) and can be easily extended to other ANNS structures like ScaNN  and

Figure 2: 1-NN accuracy on ImageNet retrieval shows that \(\)-IVF achieves near-optimal accuracy-compute trade-off compared across various rigid and adaptive baselines. Both adaptive variants of \(\) and \(\) significantly outperform their rigid counterparts (IVF-XX) while post-hoc compression on \(\) using SVD for adaptivity falls short.

HNSW . For example, HNSW consists of multiple layers with graphs of NSW graphs  of increasing complexity. \(\) can be adopted to HNSW, where the construction of each level can be powered by appropriate dimensionalities for an optimal accuracy-compute trade-off. In general, \(\) provides fine-grained control over compute overhead (storage, working memory, inference, and construction cost) during construction and inference while providing the best possible accuracy.

### \(\)-Opq

Standard Product Quantization (PQ) essentially performs block-wise vector quantization via clustering. For example, suppose we need \(32\)-byte PQ compressed vectors from the given \(2048\) dimensional representations. Then, we can chunk the representations in \(m=32\) equal blocks/sub-vectors of \(64\)-d each, and each sub-vector space is clustered into \(2^{8}=256\) partitions. That is, the representation of each point is essentially cluster-id for each block. Optimized PQ (OPQ)  further refines this idea, by first rotating the representations using a learned orthogonal matrix, and then applying PQ on top of the rotated representations. In ANNS, OPQ is used extensively to compress vectors and improves approximate distance computation primarily due to significantly lower memory overhead than storing full-precision data points IVF.

\(\)-OPQ utilizes \(\) representations to apply OPQ on lower-dimensional representations. That is, for a given quantization budget, \(\) allows using top \(d_{s} d\) dimensions from \(\) and then computing clusters with \(d_{s}/m\)-dimensional blocks where \(m\) is the number of blocks. Depending on \(d_{s}\) and \(m\), we have further flexibility of trading-off dimensionality/capacity for increasing the number of clusters to meet the given quantization budget. \(\)-OPQ tries multiple \(d_{s}\), \(m\), and number of clusters for a fixed quantization budget to obtain the best performing configuration.

We experimented with \(8-128\) byte OPQ budgets for both ImageNet and Natural Questions retrieval with an exhaustive search on the quantized vectors. We compare \(\)-OPQ which uses \(\) of varying granularities to the baseline OPQ built on the highest dimensional \(\). We also evaluate OPQ vectors obtained projection using SVD  on top of the highest-dimensional \(\).

Empirical results.Figures 3 and 1b show that \(\)-OPQ significantly outperforms - up to \(4\%\) accuracy gain - the baselines (OPQ on \(\)) across compute budgets on both ImageNet and NQ. In particular, \(\)-OPQ tends to match the accuracy of a \(64\)-byte (a typical choice in ANNS) OPQ baseline with only a \(32\)-byte budget. This results in a \(2\) reduction in both storage and compute FLOPS which translates to significant gains in real-world web-scale deployment (see Appendix D). We only report the best \(\)-OPQ for each budget typically obtained through a much lower-dimensional \(\) (\(128\) & \(192\); much faster to build as well) than the highest-dimensional \(\) (\(2048\) & \(768\)) for ImageNet and NQ respectively (see Appendix G for more details). At the same time, we note that building compressed OPQ vectors on projected \(\) using SVD to the smaller dimensions (or using low-dimensional \(\), see Appendix D) as the optimal \(\)-OPQ does not help in improving the accuracy. The significant gains we observe in \(\)-OPQ are purely due to better information packing in \(\) - we hypothesize that packing the most important information in the initial coordinates results in a better PQ quantization than \(\) where the information is uniformly distributed across all the dimensions . See Appendix D for more details and experiments.

### \(\) for Composite Indices

We now extend \(\) to composite indices  which put together two main ANNS building blocks - search structures and quantization - together to obtain efficient web-scale ANNS indices used in practice. A simple instantiation of a composite index would be the combination of IVF and OPQ - IVFOPQ - where the clustering in IVF happens with full-precision real vectors but the linear scan within each cluster is approximated using OPQ-compressed variants of the representation - since often the full-precision vectors of the database cannot fit in RAM. Contemporary ANNS indices like DiskANN  make this a default choice where they build the search graph with a full-precision vector and approximate the distance computations during search with an OPQ-compressed vector to obtain a very small shortlist of retrieved datapoints. In DiskANN, the shortlist of data points is then re-ranked to form the final list using their full-precision vectors fetched from the disk. \(\) is naturally suited to this shortlist-rerank framework: we use a low-\(d\)\(\) for forming index, where we could tune \(\) parameters according to the accuracy-compute trade-off of the graph and OPQ vectors. We then use a high-\(d\)\(\) for re-ranking.

Empirical results.Figure 4 shows that \(\)-IVFOPQ is \(1-4\%\) better than the baseline at all the PQ compute budgets. Furthermore, \(\)-IVFOPQ has the same accuracy as the baselines at \(8\) lower overhead. With DiskANN, \(\) accelerates shortlist generation by using low-dimensional representations and recoups the accuracy by re-ranking with the highest-dimensional \(\) at negligible cost. Table 1 shows that \(\)-DiskANN is more accurate than the baseline for both 1-NN and ranking performance at only \(half\) the cost. Using low-dimensional representations further speeds up inference in \(\)-DiskANN (see Appendix F).

These results show the generality of \(\) and its broad applicability across a variety of ANNS indices built on top of the base building blocks. Currently, \(\) piggybacks on typical ANNS pipelines for their inherent accounting of the real-world system constraints . However, we believe that \(\)'s flexibility and significantly better accuracy-compute trade-off can be further informed by real-world deployment constraints. We leave this high-potential line of work that requires extensive study to future research.

## 5 Further Analysis and Discussion

### Compute-aware Elastic Search During Inference

\(\) search structures cater to many specific large-scale use scenarios that need to satisfy precise resource constraints during construction as well as inference. However, in many cases, construction and storage of the indices are not the bottlenecks or the user is unable to search the design space. In these settings, \(\)-D enables adaptive inference through accurate yet cheaper distance computation using the low-dimensional prefix of matryoshka representation. Akin to composite indices (Section 4.3) that use PQ vectors for cheaper distance computation, we can use the low-dimensional \(\) for faster distance computation on ANNS structure built _non-adaptively_ with a high-dimensional \(\) without any modifications to the existing index.

Empirical results.Figure 2 shows that for a given compute budget using IVF on ImageNet-1K retrieval, \(\)-IVF is better than \(\)-IVF-D due to the explicit control during the building of the ANNS structure which is expected. However, the interesting observation is that \(\)-D _matches or outperforms_ the IVF indices built with \(\) of varying capacities for ImageNet retrieval.

However, these methods are applicable in specific scenarios of deployment. Obtaining optimal \(\) search structure (highly accurate) or even the best IVF-MR index relies on a relatively expensive design search but delivers indices that fit the storage, memory, compute, and accuracy constraints all at once. On the other hand \(\)-D does not require a precisely built ANNS index but can enable compute-aware search during inference. \(\)-D is a great choice for setups that can afford only one single database/index but need to cater to varying deployment constraints, e.g., one task requires 70% accuracy while another task has a compute budget of 1 MFLOPS/query.

### Why \(\) over \(\)?

Quite a few of the gains from \(\) are owing to the quality and capabilities of matryoshka representations. So, we conducted extensive analysis to understand why matryoshka representations seem to be more aligned for semantic search than the status-quo rigid representations.

**Difficulty of NN search.** Relative contrast (\(C_{r}\))  is inversely proportional to the difficulty of nearest neighbor search on a given database. On ImageNet-1K, Figure 14 shows that \(\) have better \(C_{r}\) than \(\) across dimensionalities, further supporting that matryoshka representations are more aligned (easier) for NN search than existing rigid representations for the same accuracy. More details and analysis about this experiment can be found in Appendix H.2.

**Clustering distributions.** We also investigate the potential deviation in clustering distributions for \(\) across dimensionalities compared to \(\). Unlike the \(\) where the information is uniformly diffused across dimensions , \(\) have hierarchical information packing. Figure 11 in Appendix E.3 shows that matryoshka representations result in clusters similar (measured by total variation distance ) to that of rigid representations and do not result in any unusual artifacts.

**Robustness.** Figure 9 in Appendix E shows that \(\) continue to be better than \(\) even for out-of-distribution (OOD) image queries (ImageNetV2 ) using ANNS. It also shows that the highest data dimensionality need not always be the most robust which is further supported by the higher recall using lower dimensions. Further details about this experiment can be found in Appendix E.1.

**Generality across encoders.** IVF-\(\) consistently has higher accuracy than IVF-\(\) across dimensionalities despite having similar accuracies with exact NN search (for ResNet50 on ImageNet and BERT-Base on NQ). We find that our observations on better alignment of \(\) for NN search hold across neural network architectures, ResNet18/34/101  and ConvNeXt-Tiny . Appendix H.3 delves deep into the experimentation done using various neural architectures on ImageNet-1K.

**Recall score analysis.** Analysis of recall score (see Appendix C) in Appendix H.1 shows that for a similar top-1 accuracy, lower-dimensional representations have better 1-Recall@1 across search complexities for IVF and HNSW on ImageNet-1K. Across the board, \(\) have higher recall scores and top-1 accuracy pointing to easier "searchability" and thus suitability of matryoshka representations for ANNS. Larger-scale experiments and further analysis can be found in Appendix H.

Through these analyses, we argue that matryoshka representations are better suited for semantic search than rigid representations, thus making them an ideal choice for \(\).

### Search for \(\) Hyperparameters

Choosing the optimal hyperparameters for \(\), such as \(d_{c}\), \(d_{s}\), \(m\), # clusters, # probes, is an interesting and open problem that requires more rigorous examination. As the ANNS index is formed _once_ and used for potentially billions of queries with massive implications for cost, latency and queries-per-second, a hyperparameter search for the best index is generally an acceptable industry practice [22; 38]. The Faiss library  provides guidelines2 to choose the appropriate index for a specific problem, including memory constraints, database size, and the need for exact results. There have been efforts at automating the search for optimal indexing parameters, such as Autofaiss3, which maximizes recall given compute constraints.

1. [leftmargin=*]
2. \(\)-IVF: Top-1 accuracy generally improves (with diminishing returns after a point) with increasing dimensionality of clustering (\(d_{c}\)) and search (\(d_{s}\)), as we show on ImageNet variants and with multiple encoders in the Appendix (Figures 9 and 15). Clustering with low-\(d\)\(\) matches the performance of high-\(d\)\(\) as they likely contain similar amounts of useful information, making the increased compute cost not worth the marginal gains. Increasing # probes naturally boosts performance (Appendix, Figure 9(a)). Lastly, it is generally accepted that a good starting point for the # clusters \(k\) is \(/2}\), where \(N_{D}\) is the number of indexable items . \(k=}\) is the optimal choice of \(k\) from a FLOPS computation perspective as can be seen in Appendix B.1.
3. [leftmargin=*]
4. \(\)-OPQ: we observe that for a fixed compute budget in bytes (\(m\)), the top-1 accuracy reaches a peak at \(d<d_{max}\) (Appendix, Table 4). We hypothesize that the better performance of \(\)-OPQ at \(d<d_{max}\) is due to the curse of dimensionality, i.e. it is easier to learn PQ codebooks on smaller embeddings with similar amounts of information. We find that using an \(\) with \(d=4 m\) is a good starting point on ImageNet and NQ. We also suggest using an 8-bit (256-length) codebook for OPQ as the default for each of the sub-block quantizer.
4. [leftmargin=*]
5. \(\)-DiskANN: Our observations with DiskANN are consistent with other indexing structures, i.e. the optimal graph construction dimensionality \(d<d_{max}\) (Appendix, Figure 12). A careful study of DiskANN on different datasets is required for more general guidelines to choose graph construction and OPQ dimensionality \(d\).

### Limitations

\(\)'s core focus is to improve the design of the existing ANNS pipelines. To use \(\) on a corpus, we need to back-fill  the \(\) of the data - a significant yet a one-time overhead. We also notice that high-dimensional \(\) start to degrade in performance when optimizing also for an extremely low-dimensional granularity (e.g., \(<24\)-d for NQ) - otherwise is it quite easy to have comparable accuracies with both \(\) and \(\). Lastly, the existing dense representations can only in theory be converted to \(\) with an auto-encoder-style non-linear transformation. We believe most of these limitations form excellent future work to improve \(\) further.

## 6 Conclusions

We proposed a novel framework, \(\), that leverages adaptive representations for different phases of ANNS pipelines to improve the accuracy-compute tradeoff. \(\) utilizes the inherent flexibility of matryoshka representations  to design better ANNS building blocks than the standard ones which use the rigid representation in each phase. \(\) achieves SOTA accuracy-compute trade-off for the two main ANNS building blocks: search data structures (\(\)-IVF) and quantization (\(\)-OPQ). The combination of \(\)-based building blocks leads to the construction of better real-world composite ANNS indices - with as much as \(8\) reduction in cost at the same accuracy as strong baselines - while also enabling compute-aware elastic search. Finally, we note that combining \(\) with elastic encoders  enables truly adaptive large-scale retrieval.