# GraspGF: Learning Score-based Grasping Primitive for Human-assisting Dexterous Grasping

Tianhao Wu \({}^{1,2,3*}\), Mingdong Wu \({}^{1,3*}\), Jiyao Zhang\({}^{1,2,3}\), Yunchong Gan\({}^{1}\), Hao Dong\({}^{1,3}\)

\({}^{1}\) Center on Frontiers of Computing Studies, School of Computer Science, Peking University

\({}^{2}\) Beijing Academy of Artificial Intelligence

\({}^{3}\) National Key Laboratory for Multimedia Information Processing,

School of Computer Science, Peking University

{thwu,jiyaozhang}@stu.pku.edu.cn, {wmingd,yunchong,hao.dong}@pku.edu.cn

###### Abstract

The use of anthropomorphic robotic hands for assisting individuals in situations where human hands may be unavailable or unsuitable has gained significant importance. In this paper, we propose a novel task called _human-assisting dexterous grasping_ that aims to train a policy for controlling a robotic hand's fingers to assist users in grasping objects. Unlike conventional dexterous grasping, this task presents a more complex challenge as the policy needs to adapt to diverse user intentions, in addition to the object's geometry. We address this challenge by proposing an approach consisting of two sub-modules: a hand-object-conditional grasping primitive called **Grasping**Gradient **F**ield (GraspGF), and a history-conditional residual policy. GraspGF learns 'how' to grasp by estimating the gradient from a success grasping example set, while the residual policy determines 'when' and at what speed the grasping action should be executed based on the trajectory history. Experimental results demonstrate the superiority of our proposed method compared to baselines, highlighting user awareness and practicality in real-world applications. The codes and demonstrations can be viewed at https://sites.google.com/view/graspgf.

## 1 Introduction

The significance of human hands in everyday life cannot be overstated. However, there are situations where they may not always be available, especially in scenarios where an individual may have upper limb loss or need to interact with hazardous objects. In such instances, utilising an anthropomorphic dexterous robotic hand for assistance can be a viable option . Such a dexterous hand possesses a high degree of freedom, allowing it to handle diverse daily tasks , given that many everyday objects are designed to match the structure of the human hand. This inspired us to propose a novel task called _human-assisting dexterous grasping_, in which a policy is trained to assist users with upper limb loss in grasping objects by controlling the robotic hand's fingers, as illustrated in Figure 0(a).

Traditional teleoperation methods  are unsuitable for assisting upper limb amputees in grasping because no information about the human fingers can be accessed. Compared to conventional dexterous

Figure 1: **a)** Demonstration of human-assisting dexterous grasping. **b)** Challenges of our setting.

grasping, human-assisting dexterous grasping poses a more complex challenge, as the policy must adapt to an exponentially growing number of pre-conditions. As shown in Figure 0(b), human users may grasp an object with various intentions, such as grasping different parts for different purposes or moving the hand and wrist at different speeds due to the complexity and diversity of human behaviour. Consequently, the conditional policy must be tailored not only to the object's geometry, as required by conventional dexterous grasping, but also to the user's intentions, requiring the policy to be _user-aware_. In this context, open-looped methods such as grasp pose generation  and classification-based methods  may fall short, as they do not factor in the user's intention. Reinforcement learning (RL)  presents a natural solution by enabling the training of a closed-loop human-object-conditioned policy. However, in human-assisting dexterous grasping, RL may encounter more severe generalisation issues, due to the need to generalise to diverse grasping pre-conditions. Prior RL-based approaches  have explored leveraging human-collected and engineering-heavy demonstrations to address this issue. However, collecting a large volume of diverse demonstrations that encompass different objects, grasping timings, and locations may not be feasible.

To address the challenges associated with achieving dexterous grasping for assisting humans, an effective policy needs to tackle the following two crucial questions:

_1) **How** should the robot grasp the object considering the current relative pose between the user and the object? 2) **When** and at what speed should the robot execute the grasping action based on the user movement trajectory history?_

In this paper, we present a novel approach that consists of two sub-modules designed to address the aforementioned questions individually: 1) a hand-object-conditional grasping primitive, and 2) a history-conditional residual policy. The grasping primitive, which we call _Grasping Gradient Field (GraspGF)_, is trained to learn '_how_ to grasp' by estimating the score function, _i.e._, the gradient of the log-density, of a success grasping example set. The GraspGF outputs a gradient that indicates the fastest direction to increase the 'grasping likelihood' conditioned on an object and the user's wrist. The gradient can be translated into primitive controls on each finger joint, enabling the fingers to reach an appropriate grasp pose iteratively. However, GraspGF is not capable of determining how fast the fingers should move along the gradient as it is history-agnostic. To determine '_when_ to grasp', we train a residual policy that outputs a'scaling action' that determines how fast the finger joints should move along with the primitive action, based on the history of the wrist's trajectory. Besides, as the primitive policy is not aware of the environment dynamics due to the offline training, we further require the residual policy to output a'residual action' to correct the primitive action.

Our proposed approach offers several conceptual advantages. GraspGF leverages the strong conditional generative modelling of score-based methods, as demonstrated in prior works , enabling it to output promising primitive actions conditioned on novel user's intentions. Additionally, the residual-learning design of GraspGF facilitates cold start exploration and enhances the efficiency of residual policy training. Compared to demonstration-based methods, our approach only requires a synthesised grasping example set and does not rely on exhaustive human labelling or extensive engineering effort, making it more practical for implementation in real-world applications.

In our experiments, we evaluate several methods on a dexterous grasping environment that assists humans in grasping over 4900+ on-table objects with up to 200 realistic human wrist movement patterns. Our comparative results demonstrate that our proposed method significantly outperforms the baselines across various metrics. Ablation studies further confirm the effectiveness of our proposed grasping gradient field and residual-learning design. Our analysis reveals that our method's superiority lies in its user-awareness, _i.e._, our trained policy is more tailored to the user's intentions. Additionally, we conduct real-world experiments to validate the practicality of our approach. Our results indicate that our trained model can generalise to the real world to some degree without fine-tuning.

Our contributions are summarized as follows:

* We introduce a novel and challenging human-assisting dexterous grasping task, which may potentially help social welfare.
* We propose a novel two-stage framework that decomposes the task into learning a primitive policy via score-matching and training a residual policy to complement the primitive policy via RL.
* We conduct experiments to demonstrate our method significantly outperforms the baselines and the effectiveness of our method deployed in the real world.

Human-assisting Dexterous Grasping

We study the _human-assisting dexterous grasping_, in which a policy is trained to assist users in grasping objects by controlling the robotic hand's fingers. We formulate the problem as follows:

**State and Action Spaces:** In this task, we consider a human-assisting grasping scenario involving a 28-DoF 5-fingered robotic hand. The 18-DoF joints of the fingers are denoted as \(^{18}\), the 4-DoF under-actuated joints of the fingers are denoted as \(^{u}^{4}\), and the 6-DoF pose of the wrist is represented by \(=[_{p},_{q}]\), where \(_{p}^{3}\) denotes the 3-D position and \(_{q}^{4}\) represents the 4-D quaternion. The action space \(^{18}\) corresponds to the 18-D relative changes applied to the hand joints. Unlike traditional dexterous grasping tasks, the action space does not include the 6-D relative changes for the wrist, since the wrist pose is controlled by a human user.

**Task Simulation:** To simulate the movement of the human user's wrist, we sample a wrist trajectory at the start of each episode \(_{}=\{_{1},_{2},...,_{T}\}\) (\(T\) denotes the horizon). At each time step \(t\), the wrist's pose is set to \(_{t}\). Specifically, we initially sample a target object \(O p_{O}(O)\) from an object prior distribution. Then, the wrist trajectory is sampled \(_{} p_{_{}}(_{}|O)\) conditioned on the target object \(O\). The terminal wrist state, \(_{T}\), is designed to be 'graspable'. In other words, there exists a feasible hand joint \(^{*}\) such that the grasp pose \([^{*},_{T}]\) can successfully grasp the object \(O\).

**Observations:** This task requires the agent to adapt to both the wrist trajectories \(_{}\) and different objects \(O\). Consequently, the policy \((|)\) should be conditioned on the finger joints \(\), visual observations \(o\), and the history of hand wrist poses \(H_{t}=[_{t-k},_{t-k+1},...,_{t}]\), where \(k\) is a hyper-parameter. In this work, we use the full point cloud of the target object as the visual observation \(o\) and consider the last five wrist states as the history \(H_{t}=[_{t-4},...,_{t}]\). Note that the RL-based policies used in all the experiments also take under-actuated joints \(^{u}^{4}\) as input, for simplicity, we omit this input in the following notations.

**Objective:** The objective is to find a policy \((|,o,H)\) that maximizes the expected grasping success rate over the initial distributions, i.e. \(O p_{O}(O)\) and \(_{} p_{_{}}(_{}|O)\):

\[^{*}=_{}_{O p_{O}(O),_{ } p_{_{}}(_{}|O),\\ a_{t}(|_{t},o_{t},H_{t})}\;[( )]\] (1)

Eq. 1 poses a challenging objective since the policy should generalize not only to different objects \(O p_{O}(O)\), as required in conventional dexterous grasping, but also to different hand wrist trajectories \(_{} p_{_{}}(_{}|O)\). In other words, the agent should be _user-aware_.

## 3 Method

**Overview:** A user-aware policy needs to tackle the following two crucial questions: 1) _How_ should the robot grasp the object considering the current relative pose between the user and the object? 2) _When_ and at what speed should the robot execute the grasping action based on the user movement trajectory history? As illustrated in Figure 2, our key idea is to partition the task into two stages that address these questions individually: 1) Learning a primitive policy \(^{}_{p}(^{p}_{t}|_{t},o_{t},_{t})\) that proposes a primitive action \(^{p}_{t}\) that can guide the fingers forming into a pre-grasp pose, from an success grasping pose example set. 2) Learning a residual policy \(^{}_{r}(^{s}_{t},^{r}_{t}|_{t},o_{t},H_{t})\) that outputs a scaling action \(_{t}\) to determine 'how fast' the joints should move with the primitive action and a residual action \(^{r}_{t}\) that further corrects the overall action, via RL. The combined policy \(^{,}(_{t}|_{t},o_{t},H_{t})\) is as follows:

\[&^{p}_{t}^{}_{p}(^{ p}_{t}|_{t},o_{t},_{t}),\;(^{s}_{t},^{r}_{t}) ^{}_{r}(^{s}_{t},^{r}_{t}|_{t},o_{t}, H_{t})\\ &^{,}(|_{t},o_{t},H_{t})=^ {p}_{t}^{s}_{t}+^{r}_{t}\] (2)

Initially, we employ the score-matching to train the primitive policy \(^{}_{p}\) from a grasping poses dataset. Subsequently, the combined policy, which is constructed from the residual policy \(^{}_{r}\) is combined with the frozen \(^{}_{p}\), and is trained under RL. In the following, we will introduce the motivations and the training procedures of the primitive policy (_i.e._, GraspGF) and the residual policy in Sec 3.1 and Sec 3.2, respectively. The implementation details of both policies are described in Appendix B.

### Learning GraspGF from Synthetic Examples

To address the first question above, we aim to search for a primitive policy \(^{}_{p}\) that outputs action to maximize the likelihood of success, given a set of static conditions \((_{t},o_{t},_{t})\). Inspired by , we can train such a policy by estimating the _score function_ (_i.e_., gradient of the log-density) of a conditional distribution \(p_{}(|o_{t},_{t})\):

\[^{p}=^{}_{p}(|,o,)=_{} p_{}(|o,)\] (3)

the \(p_{}(|o,)\) denotes a fingers-joints' distribution that can successfully grasp the objects, given the current observation \(o\) and the hand wrist \(\). By definition, the score function of the distribution \(_{} p_{}(|o,)\) indicates the fastest direction to increase the likelihood of the \(p_{}(|o,)\). Intuitively, if the fingers move in the direction of \(_{} p_{}(|o,)\), they will probably reach a feasible grasp-pose in the future with the success-likelihood \(p_{}(|o,)\) increases. Hence, we formulate learning the primitive policy \(^{}_{p}\) as estimating the gradient field of the log-success-likelihood \(_{} p_{}(|o,)\), namely _Grasping Gradient Field (GraspGF)_.

Thanks to the Denoising Score Matching (DSM) , we can obtain a guaranteed estimation of the GraspGF \(_{} p_{}(|o,)\) from a set of success examples \(_{}=\{(_{i}^{*},o_{i}^{*},_{i}^{*} )\}_{i=1}^{N}\) where the \(_{i}^{*}\) is the feasible finger joints for grasping conditioned on \((o_{i}^{*},_{i}^{*})\). In the following, we first revisit the preliminaries of DSM and then introduce how to employ DSM to estimate the GraspGF.

**Denoising Score-Matching** Given a set of data-points \(\{_{i} p()\}_{i=1}^{N}\) from an unknown data distribution \(p()\), the score-based generative model aims at estimating the _score function_ of a data distribution \(_{} p()\) via a _score network_\(_{}():^{||}^{| |}\). During inference, a new sample is generated by the Langevin Dynamics, which is out of our interest.

To estimate \(_{} p()\), the Denoising Score-Matching (DSM)  proposes a tractable objective by pre-specifying a noise distribution \(q_{}(}|)\), _e.g_., \((0,^{2}I)\), and train the score network to denoise the perturbed data samples:

\[()=_{} q_{}, p()}[||_{}(})- _{}} q_{}(}|)||_{2}^{2}]\] (4)

where \(_{}} q_{}(}|)=}(-})\) are tractable for the Gaussian kernel. DSM guarantees that the optimal score network holds \(_{}^{*}()=_{} p()\) for almost all \(\).

**Employing DSM to Estimate GraspGF** To estimate the GraspGF \(_{} p_{}(|o,)\), we employ the DSM in Eq 4, the training objective of the primitive policy \(^{}_{p}\) is derived as follows:

\[()=_{ q_{} (|^{*})\\ (^{*},^{*},^{*}) p_{}} [\|^{}_{p}(|},o^{*},^{* })-^{*}-}}{^{2}}\|_{2}^{2}]\] (5)

Figure 2: We decompose the human-assisting dexterous grasping into learning a primitive policy \(^{}_{p}\) that learns to form a pre-grasp pose and a residual policy \(^{}_{p}\) that learns to adjust the proceeding of the primitive action. **a)** The primitive policy \(^{}_{p}\) is trained on success grasping examples via score-matching objective. **b)** The residual policy \(^{}_{r}\) is trained to adjust the primitive policy via RL.

The Eq 5 is the L2 distance between the output of the primitive policy and the _denoising direction_\(^{*}-}}{^{2}}\), _i.e._, a direction pointing from the perturbed joints \(}\) to the original joints \(^{*}\). Intuitively, this objective is forcing the primitive policy to denoise the current joints to regions where the fingers are more likely to grasp the object.

### Training Residual Policy via Reinforcement Learning

The human-assisting dexterous grasping cannot be effectively addressed solely through the primitive policy \(_{p}^{}\). Although \(_{p}^{}\) predicts the fastest direction to form a pre-grasp pose, it fails to determine the appropriate'velocities' at which the joints should move in that direction. If the fingers close too quickly or too slowly, the agent may struggle to grasp the object. Additionally, due to offline training, the primitive policy lacks awareness of the dynamics of the environment, leading to potential violations of physical constraints.

To overcome these limitations, we propose the training of a residual policy \(_{r}^{}\), which complements the primitive policy. The role of \(_{r}^{}\) is twofold: firstly, it outputs a scaling action \(^{*}\) that adjusts the speed of the primitive action, and secondly, it produces a residual action \(^{r}\) that corrects the final output action. Eq 2 demonstrates that \(^{*}\) can be interpreted as 18-D 'pseudo-velocities' imposed on the finger joints. With outputs less than 1 for \(^{*}\), the residual policy can decelerate the primitive action, whereas values greater than 1 accelerate it.

To effectively control the proceeding of the primitive action, the residual policy \(_{r}^{}(_{t}^{r}|_{t},o_{t},H_{t})\) takes into account the history of the wrist \(H_{t}\) and the object's point cloud \(o\) as inputs. This enables the policy to infer the agent's speed of approach towards the object. Furthermore, the policy network incorporates the current joint state \(_{t}\) to infer how to correct the primitive action \(^{p}\).

We employ Proximal Policy Optimization (PPO)  to search for a final policy \(^{,}\) that maximises the following objective, where the primitive policy's parameters \(\) are frozen during training:

\[J()=_{O p_{O}(O),_{} p _{} p_{}(_{}|O),\\ _{t},(|_{t},o_{t},H_{t})}[ _{t=0}^{T}^{t}r_{t}]\] (6)

where \(>0\) denotes a discounted factor and \(r_{t}\) is the reward at time-step \(t\). To encourage the policy to successfully grasp the object while leveraging the primitive action \(_{t}^{p}\) for efficient exploration, we assign the following simple reward function for training:

\[ r_{t}&=(1-d_{t})(r_{}+r_{ })+d_{t}_{s},\;d_{t}=1(,\,t=T)\\ r_{}&=_{a}<^{p}}{|| ^{p}||_{2}},_{t}-_{t-1}>,r_{}= _{h} h\] (7)

where \(_{a},_{h},_{s}>0\) are hyperparameters and \( h\) represents the change in the height of the object's centre of gravity after lifting. The term \(d_{t}\) rewards the agent if the final grasp pose can successfully lift the target object. The term \(r_{}\) is an intrinsic reward that encourages the agent when the joints-change follows the direction of the primitive action \(_{t}^{p}\). We defer the PPO's hyperparameters to Appendix B.

### Implementation Details

**Primitive Policy Network** This module takes the hand joints \(^{18}\), object point cloud \(o^{3 1024}\) and the hand wrist \((3)\) as input, we first project the point cloud into the hand wrist's coordinate \(o_{}\). We encode \(o_{}\) into a 512-D global feature by PointNet++ . The hand joints \(\) and the noise-condition \(t\) are further encoded into 1024-D and 512-D features respectively. Concatenating the features together, we feed the concatenated feature into MLPs to obtain the 18-D output.

**Residual Policy Network** Taking the hand joints \(_{t}^{18}\), under-actuated joints \(_{t}^{u}^{4}\), visual observation \(o_{t}^{3 1024}\) and the hand wrists' trajectory \(H_{t}=[_{t},...,_{t-4}],\;_{i}(3)\) as input, this module use the PointNet to encoder the point cloud. Similarly, we first obtain the global feature \(f_{o_{}}^{512}\) of the projected point cloud. The primitive action \(_{t}^{p}=_{p}^{}(_{t},o_{t},_{t})\), hand joints \(\) and the history \(H\) are concatenated and further encoded into 512-D features. Concatenating the above features together, we feed the concatenated feature into MLPs to obtain the 36-D output.

Experiment Setups

### Task Simulation

**Environment setup**: We created a simulation environment based on the ShadowHand environment in Isaac Gym , using the ShadowHand model from IBS . The simulation environment enables parallel training on hundreds of environments. Each environment consists of an object placed on the ground and a hand that follows a pre-generated human wrist trajectory. The agent can only control the joints of the hand. The episode horizon is 50 steps, and each episode only terminates at the final step. Following a similar setting to IBS , when the episode terminates, part of the joints of each finger will automatically close by 0.1 radians, and then the wrist of the hand will be lifted by 1m.

**Grasping pose generation**: We created our success grasping pose based on the UniDexGrasp dataset . We filtered the data to match our ShadowHand model's degrees of freedom. The dataset was split into three sets: training instances (3127 objects, 363,479 grasps), seen category unseen instances (519 objects, 2595 grasps), and unseen category instances (1298 objects, 6490 grasps).

**Human grasping wrist patterns**: To mimic real human grasping patterns, we resampled 200 real human grasping wrist trajectories from HandoverSim! , from which we extracted 200 wrist movement patterns. These patterns were split into 150 training patterns and 50 testing patterns.

The details are deferred to Appendix A.

### Metrics

Following the DexVIP , we report the following three metrics: 1) **Success**: The object can be lifted more than 0.1m off the table, while the change in distance between the object and hand position after lifting is smaller than 0.05m. 2) **Posture**: The distance between the target human hand pose and the agent hand pose. It tells us how human-like the learned grasps are. 3) **Stability**: The translation and rotation changes of the object during the agent's grasping process. Translation is measured by the Euclidean distance between the final object position and the initial object position. Rotation is measured by the cosine distance between the x-axis of the initial object and the final object.

### Baselines

We compare our method with the following RL-based methods. Note that all baselines are re-trained by taking the latest 5-frame wrist states as input. 1) **IBS**: IBS explicitly compute rich contact information according to hand mesh and object mesh, which has good generalisation to objects and different contact situation. We modify the baseline to only output joint actions, and keep the reward the same as previous. 2) **PPO**: We adopt PPO as ours pure RL baseline. For pure RL, we use the reward of fingertip distance (distance of fingertip to the closest point of object point cloud), \(r_{}\) and success reward. 3) **PPO (Goal)**: We randomly sample the goal which corresponds to the current object as additional input to the agent. We add another goal pose matching reward compared to PPO baseline. 4) **ILAD**: We choose ILAD as our RL+Imitation learning baseline, which will first generate sub-optimal grasping trajectories according to example grasp pose, then use these demonstrations for imitation learning and RL. We regenerate the trajectories for ILAD training based on our grasp data. We use the same reward as the PPO baseline. The details are deferred to Appendix C.

Figure 3: Qualitative results of comparison with baselines and different trajectories. a): final grasp poses of different methods. b): final grasp poses of our method under different human trajectories

## 5 Experimental Results

### Comparative Results

As shown in Figure 4, _Ours_ achieves comparable training efficiency to _ILAD_, even without the use of extra trajectory demonstrations, _Ours_ can converge to a higher Success. _Ours_ also surpasses other baselines of Success for both unseen and unseen category instances. This demonstrates the robust generalisation capabilities of our method. _Ours_ also has the highest performance of Posture, which indicates that our method is better aware of the user's intentions. The qualitative results in Figure 3 b) demonstrate the successful grasping of objects across different user's intentions. As indicated in Table 1, _Ours_ also causes the least disturbance to the object.

_ILAD_ and _IBS_ are considered as the strongest baseline. _ILAD_ utilises additional trajectory demonstrations for RL training. However, _Ours_ still surpasses this baseline because _ILAD_ still relies on RL to generalise across diverse object categories. For _IBS_, we observed that _IBS_ tends to fail when the human wrist trajectory has the potential to collide with the ground. This could be attributed to the reward design of _IBS_, which needs to balance between colliding with the scene and grasping the object. Additionally, computing the _IBS_ representation is ten times more computationally expensive.

_PPO_ primarily fails because it attempts to learn a general policy that can grasp most objects, as shown in Figure 3 a), which is not suitable for diverse objects and grasp poses. _PPO (Goal)_ faces challenges due to its inability to adjust the goal during the approach phase. Since the wrist is continuously moving during the process, the goal set at the initial state may not be suitable.

### Ablation Studies and Analysis

We conduct the ablation studies to investigate: 1) _The effectiveness of decomposing the policy into primitive policy and residual policy. 2) The necessity of different action modules. 3) The impact of different action modules on final action._

The ablation results depicted in Figure 5 demonstrate the significance of our proposed approaches. _Ours w/o GF_ experiences a significant performance decline, indicating the significance of combining RL policy with primitive action. Similarly, _Ours w/o RL_ shows a substantial drop in performance by focusing solely on the 'how' of grasping, which results in collisions during the grasping process.

    &  &  \\  & Tran(cm) \(\) & Rot (rad) \(\) & Tran(cm) \(\) & Rot (rad) \(\) \\  PPO(Goal) & \(2.621_{ 0.15}\) & \(0.589_{ 0.038}\) & \(2.537_{ 0.296}\) & \(0.543_{ 0.040}\) \\ PPO & \(2.745_{ 0.168}\) & \(0.594_{ 0.045}\) & \(2.771_{ 0.254}\) & \(0.563_{ 0.039}\) \\ IBS & \(2.653_{ 0.030}\) & \(0.572_{ 0.002}\) & \(2.596_{ 0.119}\) & \(0.520_{ 0.011}\) \\ ILAD & \(2.443_{ 0.042}\) & \(0.548_{ 0.027}\) & \(2.534_{ 0.101}\) & \(0.515_{ 0.022}\) \\ Ours & \(_{ 0.138}\) & \(_{ 0.020}\) & \(_{ 0.165}\) & \(_{ 0.029}\) \\   

Table 1: Results of **Stability** on seen category unseen instances and unseen category instances. _Tran_: translation of objects from the initial position to the final position. _Rot_: rotation of objects from initial orientation to final orientation.

Figure 4: Quantitative comparative results. Left: training curve of different methods. Note that IBS takes 144 hours on V100 to reach 3.5 million agent steps, while ours only takes 15 hours to reach 10 million agent steps. Middle: Success and Posture of different methods on **seen category unseen instances**. Right: Success and Posture of different methods on **unseen category instances**.

Although _Ours w/o_\(^{r}\) and _Ours w/o_\(^{s}\) exhibit higher training efficiency, both methods eventually experience training collapse. During this collapse, we observe that the agent tends to either follow the primitive action more closely or deviate from it to a greater extent. This suggests that the combination of both action modules allows for a better policy learning process that effectively utilises the primitive action.

The results shown in Table 2 indicate that the primitive action, which does not consider collisions due to physical constraints in the grasping procedure, can achieve similar performance to _Ours_. This suggests that the primitive action module has a good understanding of how to grasp, but it moves quickly towards the target without considering potential collisions, as shown in Figure 6. The inclusion of the \(^{s}\) module slows down the progress of \(^{p}\) to avoid collisions, as illustrated in Appendix D.1. However, this conservative approach leads to a drop in success. Nevertheless, after further addition of \(^{r}\), the performance increases to \(56.50\%\), which is comparable to \(^{p}\) w/o coll. By further combining the \(^{r}\) module, the final action is corrected to achieve the 'how to grasp' knowledge learned by \(^{p}\) while also incorporating correct 'when to grasp' knowledge.

### Adaptability Results

To further demonstrate the adaptability of our method, we conduct both quantitative and qualitative experiments. Quantitatively, as shown in Table 3, we calculate the success rate for each object across five different grasp poses. For the objects that have been successfully grasped, _Ours_ excels at grasping various parts of the objects. Qualitatively, Figure 7 demonstrates that the grasp pose generated by _Ours_ closely resembles a human's intended grasp pose. Furthermore, we also show that GraspGF can adapt to wrist rotation; videos can be viewed at https://sites.google.com/view/graspgf.

### Real-world Results

In this section, we construct a real-world system to validate our method. As shown in Figure 8, the system consisted of calibrated multi-view RGB-D cameras (four Intel RealSense D415 sensors), a

   Action Type & Success \\  \(^{p}\) & 19.49 \% \\ \(^{p}\) w/o coll & 55.60 \% \\ \(^{p}^{s}\) & 5.08 \% \\ \(^{p}^{s}+^{r}\) & 56.50 \% \\   

Table 2: Success rate of different combinations of action modules.

Figure 5: Ablation Study on decomposing policy and different action modules.

Figure 6: Qualitative results of grasping procedure governed by the primitive policy. The yellow circle highlights the collision between the finger and the object caused by premature closure.

  
**Success Rate** & **1/5** & **2/5** & **3/5** & **4/5** & **5/5** \\ 
**Ours** & 13.67\% & 27.70\% & **31.41\%** & **20.82\%** & **6.40\%** \\ 
**ILAD** & 25.44\% & 32.53\% & 26.61\% & 12.53\% & 2.89\% \\
**IBS** & 27.70\% & 34.73\% & 25.67\% & 10.29\% & 1.61\% \\
**PPO** & 48.20\% & 35.49\% & 13.10\% & 3.01\% & 0.20\% \\
**PPO (Goal)** & 77.04\% & 18.96\% & 3.62\% & 0.30\% & 0.08\% \\   

Table 3: Percentages of objects with varying success rates of grasping under different methods. The results are averaged over five different random seeds.

Figure 7: Qualitative results of grasping different parts of shoes with the same shape but different scales.

[MISSING_PAGE_FAIL:9]

In the medical AI field, there are studies with a similar setting to ours, known as prosthetic dexterous grasping [11; 12; 13]. These studies aim to control a prosthetic hand to assist users in grasping and explore open-looped approaches that initially predict the grasp type and then execute predefined primitives for control. In contrast, we explore a closed-loop control policy that adaptively adjusts the grasp pose based on the current user-object relationship and the user's movement trajectory history.

### Score-based Generative Models

In the pursuit of estimating the gradient of the log-likelihood associated with given data distribution, the score-based generative model, originally introduced by , has garnered substantial attention in the research community [22; 34; 35; 36; 37; 19; 38]. The denoising score-matching (DSM), as proposed by , further introduces a tractable surrogate objective for score-matching. To enhance the scalability of score-based generative models,  introduces a sliced score-matching objective that projects the scores onto random vectors before comparing them. Song et al. also introduce annealed training for denoising score matching , along with corresponding improved training techniques . They also extend the discrete levels of annealed score matching to a continuous diffusion process and demonstrate promising results in image generation . Recent works further explore the design choices of the diffusion process , maximum likelihood training , and deployment on the Riemann manifold . These recent advances show promising results when applying score-based generative models in high-dimensional domains and promote wide applications in various fields, such as object rearrangement , medical imaging , point cloud generation , scene graph generation , point cloud denoising , depth completion , and human pose estimation . These works formulate perception problems into conditional generative modelling or inpainting, allowing the utilisation of score-based generative models to address these tasks.

In contrast, our focus lies in the application of score-based generative models for training low-level control policies. In this domain,  and  have proposed learning diffusion models from offline trajectories and leveraging these models for model predictive control. However, these approaches suffer from the drawback of requiring a substantial amount of offline data and inefficiency during test-time sampling. To the best of our knowledge, our method represents the first exploration of score-based generative models for learning closed-loop dexterous grasping policies.

## 7 Conclusion

In this work, we introduce _human-assisting dexterous grasping_, wherein a policy is trained to assist users in grasping objects by controlling the robotic hand's fingers. To search for a _user-aware_ policy, we propose a novel two-stage framework that decomposes the task into learning a primitive policy via score-matching and training a residual policy to complement the primitive policy via RL. In experiments, we introduce a human-assisting dexterous grasping environment that consists of 4900+ on-table objects with up to 200 realistic human wrist movement patterns. Results demonstrate that our proposed method significantly outperforms the baselines across various metrics. Our analysis reveals that our trained policy is more tailored to the user's intentions. Our real-world experiments indicate that our learned policy can generalise to the real world to some degree without fine-tuning.

**Limitations and Future works.** Our method takes the full point cloud as the visual observation, which is not accessible in the wild. In the future, we may leverage teacher-student learning to generalise our method to a partial observation setting.

**Ethics Statement.** Our method has the potential to develop home-assistant robots and assist individuals with hand disabilities, thus contributing to social welfare. We evaluate our method in simulated environments, which may introduce data bias. However, similar studies also have such general concerns. We do not see any possible major harm in our study.