# Learning Linear Causal Representations from Interventions under General Nonlinear Mixing

Simon Buchholz\({}^{1}\)

Equal Contribution

Goutham Rajendran\({}^{2}\)1

Elan Rosenfeld\({}^{2}\)

Bryon Aragam\({}^{3}\)

Bernhard Scholkopf\({}^{1}\)

Pradeep Ravikumar\({}^{2}\)

\({}^{1}\)Max Planck Institute for Intelligent Systems, Tubingen, Germany

\({}^{2}\)Carnegie Mellon University, Pittsburgh, USA

\({}^{3}\)University of Chicago, Chicago, USA

###### Abstract

We study the problem of learning causal representations from unknown, latent interventions in a general setting, where the latent distribution is Gaussian but the mixing function is completely general. We prove strong identifiability results given unknown single-node interventions, i.e., without having access to the intervention targets. This generalizes prior works which have focused on weaker classes, such as linear maps or paired counterfactual data. This is also the first instance of identifiability from non-paired interventions for deep neural network embeddings and general causal structures. Our proof relies on carefully uncovering the high-dimensional geometric structure present in the data distribution after a non-linear density transformation, which we capture by analyzing quadratic forms of precision matrices of the latent distributions. Finally, we propose a contrastive algorithm to identify the latent variables in practice and evaluate its performance on various tasks.

## 1 Introduction

Modern generative models such as GPT-4  or DDPMs  achieve tremendous performance for a wide variety of tasks . They do this by effectively learning high-level representations which map to raw data through complicated _non-linear maps_, such as transformers  or diffusion processes . However, we are unable to reason about the specific representations they learn, in particular they are not necessarily related to the true underlying data generating process. Besides their susceptibility to bias , they often fail to generalize to out-of-distribution settings , rendering them problematic in safety-critical domains. In order to gain a deeper understanding of what representations deep generative models learn, one line of work has pursued the goal of causal representation learning (CRL) . CRL aims to learn high-level representations of data while simultaneously recovering the rich causal structure embedded in them, allowing us to reason about them and use them for higher-level cognitive tasks such as systematic generalization and planning. This has been used effectively in many application domains including genomics  and robotics .

A crucial primitive in representation learning is the fundamental notion of identifiability , i.e., the question whether a unique (up to tolerable ambiguities) model can explain the observed data. It is well known that because of non-identifiability, CRL is impossible in general settings in the absence of inductive biases or additional information . In this work, we consider additional informationin the form of interventional data . It is common to have access to such data in many application domains such as genomics and robotics stated above (e.g. [17; 58; 57]). Moreover, there is a pressing need in such safety-critical domains to build reliable and trustworthy systems, making identifiable CRL particularly important. Therefore, it's important to study whether we can and also how to perform identifiable CRL from raw observational and interventional data. Here, identifiability opens the possibility to provably recover the true representations with formal guarantees. Meaningfully learning such representations with causal structure enables better interpretability, allows us to reason about fairness, and helps with performing high-level tasks such as planning and reasoning.

In this work, we study precisely this problem of causal representation learning in the presence of interventions. While prior work has studied simpler settings of linear or polynomial mixing [84; 88; 2; 71; 11], we allow for general non-linear mixing, which means our identifiability results apply to complex real-world systems and datasets which are used in practice. With our results, we make progress on fundamental questions on interventional learning raised by .

Concretely, we consider a general model with latent variables \(Z\) and observed data \(X\) generated as \(X=f(Z)\) where \(f:^{d}^{d^{}}\) is an arbitrary non-linear mixing function. We assume \(Z\) satisfies a Gaussian structural equation model (SEM) which is unknown and unobserved. A Gaussian prior is commonly used in practice (which implies a linear SEM over \(Z\)) and further, having a simple model for \(Z\) allows us to learn useful representations, generate data efficiently, and explore the latent causal relationships, while the non-linearity of \(f\) ensures universal approximability of the model. We additionally assume access to interventional data \(X^{(i)}=f(Z^{(i)})\) for \(i I\) where \(Z^{(i)}\) is the latent under an intervention on a single node \(Z_{t_{i}}\). Notably, we allow for various kinds of interventions, we don't require knowledge of the targets \(t_{i}\), and we don't require paired data, (i.e., we don't need counterfactual samples from the joint distribution \((X,X^{(i)})\), but only their marginals). Having targets or counterfactual data is unrealistic in many practical settings but many prior identifiability results require them, so eliminating this dependence is a crucial step towards CRL in the real world.

**Contributions** Our main contribution is a general solution to the identifiability problem that captures practical settings with non-linear mixing functions (e.g. deep neural networks) and unknown interventions (since the targets are latent). We study both perfect and imperfect interventions and additionally allow for shift interventions, where perfect interventions remove the dependence of the target variable from its parents, while imperfect interventions (also known as soft interventions) modify the dependencies. Below, we summarize our main contributions:

1. We show identifiability of the full latent variable model, given non-paired interventional data with unknown intervention targets. In particular, we learn the mixing, the targets and the latent causal structure.
2. Compared to prior works that have focused on linear/polynomial \(f\), we allow non-linear \(f\) which encompasses representations learned by e.g. deep neural networks and captures complex real-world datasets. Moreover, we study both imperfect (also called soft) and perfect interventions, and always allow shift interventions.
3. We construct illustrative counterexamples to probe tightness of our assumptions which suggest directions for future work.
4. We propose a novel algorithm based on contrastive learning to learn causal representations from interventional data, and we run experiments to validate our theory. Our experiments suggest that a contrastive approach, which so far has been unexplored in interventional CRL, is a promising technique going forward.

## 2 Related work

Causal representation learning [75; 74] has seen much recent progress and applications since it generalizes and connects the fields of Independent Component Analysis [13; 28; 30], causal inference [81; 61; 63; 64; 82] and latent variable modeling [40; 3; 78; 96; 32]. Fundamental to this approach is the notion of identifiability [36; 15; 93]. Due to non-identifiability in general settings without inductive biases [29; 50], prior works have approached this problem from various angles -- using additional auxiliary labels [36; 27; 6; 76]; by imposing sparsity [75; 42; 54; 103]; or by restricting the function classes [41; 9; 104; 22]. See also the survey . Moreover, a long line of works have proposed practical methods for CRL (which includes causal disentanglement as a special case),[19; 95; 16; 97; 44; 14] to name a few. It's worth noting that most of these approaches are essentially variants of the Variational Autoencoder framework [39; 68].

Of particular relevance to this work is the setting when interventional data is available. We first remark that the much simpler case of fully observed variables (i.e., no latent variables), has been studied in e.g. [23; 62; 83; 34; 18] (see also the survey ). In this work, we consider the more difficult setting of structure learning over latent variables, which have been explored in [46; 42; 6; 104; 1; 84; 88; 2; 71; 11; 72]. Among these,  assumes that the intervention targets are known,  specifically consider instance-level pre- and post- interventions for time-series data and [6; 104; 1] assumes access to paired counterfactual data. In contrast, we assume unknown targets and work in general settings with non-paired interventional data, which is important in various real-world applications [85; 99]. The work  require several graphical restrictions on the causal graph and also require \(2d\) interventions, while we make no graphical restrictions and only require \(d\) interventions (which we also show cannot be improved under our assumptions). [71; 11; 84; 88] consider linear mixing functions \(f\) whereas we study general non-linear \(f\). Finally,  consider polynomial mixing in the more restricted class of deterministic do-interventions. Several concurrent works study related settings [100; 35; 65; 45; 92]. For a more detailed comparison to the related works, we refer to Appendix C.

Our proposed algorithm is based on contrastive learning. Contrastive learning has been used in other contexts in this domain [31; 27; 55; 104; 91] (either in the setting of time-series data or paired counterfactual data), however the application to non-paired interventional settings is new to the best of our knowledge.

NotationIn this work, we will almost always work with vectors and matrices in \(d\) dimensions where \(d\) is the latent dimension, and we will disambiguate when necessary. For a vector \(v\), we denote by \(v_{i}\) its \(i\)-th entry. Let \(\) denote the \(d d\) identity matrix with columns as the standard basis vectors \(e_{1},,e_{d}\). We denote by \(N(,)\) the multivariate normal distribution with mean \(\) and covariance \(\). For a set \(C\), let \((C)\) denote the uniform distribution on \(C\). For two random variables \(X,X^{}\), we write \(X}}{{=}}X^{}\) if their distributions are the same. We denote the set \(\{1,,d\}\) by \([d]\). For permutation matrices we use the convention that \((P_{})_{ij}=_{j=(i)}\) for \( S_{d}\). We use standard directed graph terminology, e.g. edges, parents. When we use the term "non-linear mixing" in this work, we also include linear mixing as a special case.

## 3 Setting: Interventional Causal Representation Learning

We now formally introduce our main settings of interest and the required assumptions. We assume that there is a latent variable distribution \(Z\) on \(^{d}\) and an observational distribution \(X\) on \(^{d^{}}\) given by \(X=f(Z)\) where \(d d^{}\) and \(f\) is a non-linear mixing. This encapsulates the most general definition of a latent variable model. As the terminology suggests, we observe \(X\) in real-life, e.g. images of cats and \(Z\) encodes high-level latent information we wish to learn and model, e.g. \(Z\) could indicate orientation and size.

**Assumption 1** (Nonlinear \(f\)).: _The non-linear mixing \(f\) is injective, differentiable and embeds into \(^{d^{}}\)._

Figure 1: Illustration of an example latent variable model under interventions. \((a)\) No interventions. \((b)\) An imperfect intervention on node \(t_{i}=3\). Dashed edges indicate the weights could have potentially been modified. \((c)\) A perfect intervention on node \(t_{i}=3\).

Such an assumption is standard in the literature. Injectivity is needed in order to identify \(Z\) from \(X\) because otherwise we may have learning ambiguity if multiple \(Z\)s map to the same \(X\). Differentiability is needed to transfer densities. Note that Assumption 1 allows for a large class of complicated nonlinearities and we discuss in Appendix E why proving results for such a large class of mixing functions is important.

Next, we assume that the latent variables \(Z\) encode causal structure which can be expressed through a Structural Causal Model (SCM) on a Directed Acyclic Graph (DAG). We focus on linear SCMs with an underlying causal graph \(G\) on vertex set \([d]\), encoded by a matrix \(A\) through its non-zero entries, i.e., there is an edge \(i j\) in \(G\) iff \(A_{ji} 0\).

**Assumption 2** (Linear Gaussian SCM on Latent Variables).: _The latent variables \(Z\) follow a linear SCM with Gaussian noise, so_

\[Z=AZ+D^{1/2}\] (1)

_where \(D\) is a diagonal matrix with positive entries, \(A\) encodes a DAG \(G\) and \( N(0,)\)._

For an illustration of the model, see Fig. 0(a). It is convenient to encode the coefficients of linear SCMs through the matrix \(B=D^{-1/2}(-A)\). Then \(Z=B^{-1}\) and both \(D\) and \(A\) can be recovered from \(B\), indeed the diagonal entries of \(B\) agree with the entries of \(D^{-1/2}\). Note that we can and will always assume that the entries of \(D\) are positive since \(}}{{=}}-\).

Assuming that the latent variables follow a linear Gaussian SCM is certainly restrictive but nevertheless a reasonable approximation of the underlying data generating process in many settings. The Gaussian prior assumption is also standard in latent variable modeling and enables efficient inference for downstream tasks, among other advantages. Importantly, under the above assumptions, our model has infinite modeling capacity , so they're able to model complex datasets such as images and there's no loss in representational power.

The goal of representation learning is to learn the non-linear mixing \(f\) and the high-level latent distribution \(Z\) from observational data \(X\). In causal representation learning, we wish to go a step further and model \(Z\) as well by learning the parameters \(B\) which then lets us easily recover \(A,D\) and the causal graph \(G\). For general non-linear mixing, this model is not identifiable and hence we cannot hope to recover \(Z\), but in this work we use additional interventional data, similar to the setups in recent works .

**Assumption 3** (Single node interventions).: _We consider interventional distributions \(Z^{(i)}\) for \(i I\) which are single node interventions of the latent distribution \(Z\), i.e., for each intervention \(i\) there is a target node \(t_{i}\) and we change the assigning equation of \(Z^{(i)}_{t_{i}}\) to_

\[Z^{(i)}_{t_{i}}=(A^{(i)}Z^{(i)})_{t_{i}}+(D^{(i)})^{1/2}_{t_{i},t_{i}}( _{t_{i}}+^{(i)})\] (2)

_while leaving all other equations unchanged. We assume that \(A^{(i)}_{t_{i},k} 0\) only if \(k t_{i}\) in \(G\), i.e., no parents are added and \(^{(i)}\) denotes a shift of the mean._

For an illustration, see Fig. 0(b). An intervention on node \(t_{i}\) has no effect on the non-descendants of \(t_{i}\), but will have a downstream effect on the descendants of \(t_{i}\). In particular, \(A\) is modified so that the weight of the edge \(k t_{i}\) could potentially be changed if it exists already but no new incoming edges to \(t_{i}\) may be added. Also, the noise variable \(_{t_{i}}\) is also allowed to be modified via a scale intervention as well as a shift intervention. Note that  do not allow shift interventions, whereas we do.

Again, this can be written concisely as \(Z^{(i)}=(B^{(i)})^{-1}(+^{(i)}e_{t_{i}})\) where \(B^{(i)}=(D^{(i)})^{-1/2}(-A^{(i)})\). Let \(r^{(i)}\) denote the row \(t_{i}\) of \(B^{(i)}\), then we can write \(B^{(i)}=B-e_{t_{i}}(B^{}e_{t_{i}}-r^{(i)})^{}\). We assume that interventions are non-trivial, i.e., \(Z^{(i)}}}{{}}Z\). In our model, we observe interventional distributions \(X^{(i)}=f(Z^{(i)})\) for various interventions \(i I\).

**Definition 1** (Intervention Types).: _We call an intervention perfect (or stochastic hard) if \(A^{(i)}_{t_{i}}=0\), i.e., we remove all connections to former parents and potentially change variance and mean of the noise variable. Note that for nodes without parents, either \(D^{(i)}_{t_{i},t_{i}} D_{t_{i},t_{i}}\) or \(^{(i)} 0\) so that the intervention is non-trivial. We call an intervention a pure noise intervention if \(A^{(i)}_{t_{i}}=A_{t_{i}}\), and \(A_{t_{i}} 0\) (i.e., node \(t_{i}\) has at least one parent). In other words, a pure noise intervention targets a node with parents by changing only the noise distribution through a change of variance (encoded in \(D\)) or a mean shift (encoded in \(\))._

We call a single-node intervention imperfect if it is not perfect (some prior works call it a soft intervention). Note that perfect interventions are never of pure noise type because they necessarily change the relation to the parents. For an illustration of perfect and imperfect interventions, see Fig. 0(c), Fig. 0(b) respectively. Finally, we require an exhaustive set of interventions.

**Assumption 4** ((Coverage of Interventions)).: _All nodes are intervened upon by at least one intervention, i.e., \(\{t_{i}:\,i I\}=[d]\)_

This assumption was also made in the prior works . When the interventions don't cover all the nodes, we have non-identifiability as described in Section 4 and Appendix D. We also extensively discuss that none of our other assumptions can simply be dropped in these sections.

**Remark 1**.:
* _Our theory also readily extends to noisy observations, i.e.,_ \(X=f(Z)+\) _where_ \(\) _is independent noise. In this case, we first denoise via a standard deconvolution argument_ _[_36, 42_]_ _and then apply our theory._
* _Similar to the results in_ _[_84_]_ _we can assume completely unknown intervention targets, i.e., there might be multiple interventions targeting the same node, and we do not need to know the partition. We only require coverage of all nodes. In contrast to their work we assume that we know which dataset corresponds to the observational distribution, but we expect that this restriction can be removed._

To simplify the notation, it is convenient to use \(B^{(0)}=B\), \(^{(0)}=0\) for the observational distribution. We also define \(=I\{0\}\). Then, all information about the latent variable distributions \(Z^{(i)}\) and observed distributions \(X^{(i)}\) are contained in \(((B^{(i)},^{(i)},t_{i})_{i},f)\).

## 4 Main Results

We can now state our main results for the setting introduced above.

**Theorem 1**.: _Suppose we are given distributions \(X^{(i)}\) generated using a model \(((B^{(i)},^{(i)},t_{i})_{i},f)\) such that Assumptions 1-4 hold and such that all interventions \(i\) are perfect. Then the model is identifiable up to permutation and scaling, i.e., for any model \(((^{(i)},^{(i)},_{i})_{i I}, {f})\) that generates the same data, the latent dimension \(d\) agrees and there is a permutation \( S_{d}\) (and associated permutation matrix \(P_{}\)) and an invertible pointwise scaling matrix \((d)\) such that_

\[_{i}=(t_{i}),^{(i)}=P_{}^{}B^ {(i)}^{-1}P_{},=f^{-1}P_{}, ^{(i)}=^{(i)}.\] (3)

_This in particular implies that_

\[^{(i)}}}{{=}}P_{}^{ } Z^{(i)}\] (4)

_and we can identify the causal graph \(G\) up to permutation of the labels._

This result says that for the interventional model as described in the previous section, we can identify the non-linear map \(f\), the intervention targets \(t_{i}\), the parameter matrices \(B,D,A\) up to permutations \(P_{}\) and diagonal scaling \(\). Moreover, we can recover the shifts \(^{(i)}\) exactly and also the underlying causal graph \(G\) up to permutations.

**Remark 2**.: _Identifiability and recovery up to permutation and scaling is the best possible for our setting. This is because the latent variables \(Z\) are not actually observed, which means we cannot (and in fact don't need to) resolve permutation and scaling ambiguity without further information about \(Z\). See [84, Proposition 1] for the short proof._

When we drop the assumption that the interventions are perfect, we can still obtain a weaker identifiability result. Define \(_{G}\) to be the minimal partial order on \([d]\) such that \(i_{G}j\) if \((i,j)\) is an edge in \(G\), i.e., \(i_{G}j\) if and only if \(i\) is an ancestor of \(j\) in \(G\). Note that any topological ordering of \(G\) is compatible with the partial order \(_{G}\). Then, our next result shows that under imperfect interventions, we can still recover the partial order \(_{G}\).

**Theorem 2**.: _Suppose we are given the distributions \(X^{(i)}\) generated using a model \(((B^{(i)},^{(i)},t_{i})_{i},f)\) with causal graph \(G\) such that the Assumptions 1-4 hold and none of the interventions is a pure noise intervention. Then for any other model \(((^{(i)},^{(i)},_{i})_{i}, )\) with causal graph \(\) generating the same observations the latent dimension \(d\) agrees and there is a permutation \( S_{d}\) such that \(_{i}=(t_{i})\) and \(i_{}j\) iff \((i)_{G}(j)\), i.e., \(_{G}\) can be identified up to a permutation of the labels._

**Remark 3**.: _We emphasize that neither the full graph nor the coefficients \(B^{(i)}\) or the latent variables \(Z_{i}\) are identifiable in this setting, even for linear mixing functions._

Theorem 1 and Theorem 2 generalize the main results of  which assume linear \(f\) while we allow for non-linear \(f\). The key new ingredient of our work is the following theorem, which shows identifiability of \(f\) up to linear maps.

**Theorem 3**.: _Assume that \(X^{(i)}\) is generated according to a model \(((B^{(i)},^{(i)},t_{i})_{i},f)\) such that the Assumptions 1-4 hold. Then we have identifiability up to linear transformations, i.e., if \(((^{(i)},^{(i)},_{i}),)\) generates the same observed distributions \((^{(i)})}}{{=}}X ^{(i)}\), then their latent dimensions \(d\) agree and there is an invertible linear map \(T\) such that_

\[=f T^{-1},^{(i)}=TZ^{(i)}.\] (5)

The proof of this theorem is deferred to Appendix A. In Appendix B we then review the results of  and show how their results can be extended to obtain our main results, Theorem 1 and Theorem 2. Now we provide some intuition for the proofs of the main theorems.

Proof intuitionThe recent work  studies the special case when \(f\) is linear, and the proof is linear algebraic. In particular, they consider row spans of the precision matrices of \(X^{(i)}\), project them to certain linear subspaces and use those subspaces to construct a generalized RQ decomposition of the pseudoinverse of the linear mixing matrix \(f\). However, once we are in the setting of non-linear \(f\), such an approach is not feasible because we can no longer reason about row spans of the precision matrices of \(X\), which have been transformed non-linearly thereby losing all linear algebraic structure. Instead, we take a statistical approach and look at the log densities of the \(X^{(i)}\). By choosing a Gaussian prior, the log-odds \( p^{(i)}_{X}(x)- p^{(0)}_{X}(x)\) of \(X^{(i)}\) with respect to \(X^{(0)}\) can be written as a quadratic form of difference of precision matrices, evaluated at non-linear functions of \(x\). For simplicity of this exposition, ignore terms arising from shift interventions and determinants of covariance matrices. Then, the log-odds looks like \((x)=f^{-1}(x)^{}(^{(i)}-^{(0)})f^{-1}(x)\), where \(^{(i)}\) is the precision matrix of \(Z^{(i)}\).

At this stage, we again shift our viewpoint to geometric and observe that \(^{(i)}-^{(0)}\) has a certain structure. In particular, for single-node interventions, it has rank at most \(2\) and for source node targets, it has rank \(1\). This implies that the level set manifolds of the quadratic forms \((x)\) also have a certain geometric structure in them, i.e., the DAG leaves a geometric signature on the data likelihood. We exploit this carefully and proceed by induction on the topological ordering until we end up showing that \(f\) can be identified up to a linear transformation, which is our main Theorem 3. Here, the presence of shift interventions adds additional complexities, and we have to generalize all of our intermediate lemmas to handle these. Once we identify \(f\) up to a linear transformation, we can apply the results of  to conclude Theorems 1 and 2.

**On the optimality and limitations of the assumptions** We make a few brief remarks on our assumptions, deferring to Appendix D a full discussion and the technical construction of several illustrative counterexamples.

1. _Number of interventions:_ For our main results, Theorems 1 and 2, we assume that there are at least \(d\) interventions (Assumption 4). This cannot be weakened even for linear mixing functions . In addition, we also show in Fact 1 that for the linear identifiability proved in Theorem 3, \(d-2\) interventions are not sufficient. Thus, the required number of interventions is tight in Theorems 1, 2 and is tight up to at most one intervention in Theorem 3.
2. _Intervention type:_ In the setting of imperfect interventions, the weaker identifiability guarantees in Theorem 2 cannot be improved even when the mixing is linear [84, Appendix C]. We also show in Fact 2 that if we drop the condition that interventions are not pure noise interventions, we have non-identifiability. Concretely, we show that when all interventionsare of pure noise type, any causal graph is compatible with the observations. Finally, in contrast to the special case of linear mixing, we show in Lemma 8 that we need to exclude non-stochastic hard interventions (i.e., \((Z_{i}=z_{i})\)) for identifiability up to linear maps.
3. _Distributional assumptions:_ We assume Gaussian latent variables, while allowing for very flexible mixing \(f\). This model has universal approximation guarantees and is moreover used ubiquitously in practice. While the result can potentially be extended to more general latent distributions (e.g., exponential families), we additionally show in Lemma 9 that the result is not true when making no assumption on the distribution of \(\).

## 5 Experimental methodology

In this section, we explain our experimental methodology and the theoretical underpinning of our approach. Our main experiments for interventional causal representation learning focus on a method based on contrastive learning. We train a deep neural network to learn to distinguish observational samples \(x X^{(0)}\) from interventional samples \(x X^{(i)}\). Additionally, we design the last layer of the model to model the log-likelihood of a linear Gaussian SCM. Due to representational flexibility of deep neural networks, we will in principle learn the Bayes optimal classifier after optimal training, which we show is related to the underlying causal model parameters. Accordingly, with careful design of the last layer parametric form, we indirectly learn the parameters of the underlying causal model. Similar methods have been used for time-series data or multimodal data [27; 32] but to the best of our knowledge, the contrastive learning approach to interventional learning is novel.

Denote the probability density of \(x X^{(i)}\) (resp. \(z Z^{(i)}\)) by \(p_{X}^{(i)}(x)\) (resp. \(p_{Z}^{(i)}(z)\)). The next lemma describes the log-odds of a sample \(x\) coming from the interventional distribution \(X^{(i)}\) as opposed to the observational distribution \(X^{(0)}\). We focus on the identifiable case (see Theorem 1) and therefore only consider perfect interventions. As per the notation in Section 3 and Appendix A.1, let \(s^{(i)}\) denote the row \(t_{i}\) of \(B^{(0)}\) and let \(^{(i)},_{i}\) denote the magnitude of the shift and scale intervention respectively.

**Lemma 1**.: _When we have perfect interventions, the log-odds of a sample \(x\) coming from \(X^{(i)}\) over coming from \(X^{(0)}\) is given by_

\[ p_{X}^{(i)}(x)- p_{X}^{(0)}(x)=c_{i}-_{i}^{2}(((f^{-1 }(x))_{t_{i}})^{2}+^{(i)}_{i}(f^{-1}(x))_{t_{i}})+  f^{-1}(x),s^{(i)}^{2}\] (6)

_for a constant \(c_{i}\) independent of \(x\)._

The proof is deferred to Appendix F. The form of the log-odds suggests considering the following functions

\[g_{i}(x,_{i},_{i},_{i},w^{(i)},)=_{i}-_{i}h _{t_{i}}^{2}(x,)+_{i}h_{t_{i}}(x,)+ h(x,),w^{( i)}^{2}\] (7)

where \(h(,)\) denotes a neural net parametrized by \(\), parameters \(w^{(i)}\) are the rows of a matrix \(W\) and \(_{i}\), \(_{i}\), \(_{i}\) are learnable parameters. Note that the ground truth parameters minimize the following cross entropy loss

\[_{}^{(i)}=_{j(\{0,i\})}_{x X^{(j)}}(_{j=i},g_{i}(x))=-_{j (\{0,i\})}_{x X^{(j)}}(_{ j=i}g_{i}(x)}}{e^{g_{i}(x)}+1}).\] (8)

Note that (compare (6) and (7)) \(W\) should learn \(B=D^{-1/2}(-A)\) thus its off-diagonal entries should form a DAG. To enforce this we add the NOTEARS regularizer  given by \(_{NOTEARS}(W)=(W_{0} W_{0})-d\) (see Appendix F.3) where \(W_{0}\) equals \(W\) with the main diagonal zeroed out. We also promote sparsity by adding the \(l_{1}\) regularization term \(_{REG}(W)=\|W_{0}\|_{1}\). Thus, the total loss is given by

\[(,,,W,)=_{i I}_{ }^{(i)}+_{1}_{NOTEARS}(W)+_{2}_{REG}(W)\] (9)

for hyperparameters \(_{1}\) and \(_{2}\). Our identifiability result Theorem 1 implies that when we assume that the neural network has infinite capacity, the loss in (9) is minimized, \(_{1}\) is large and \(_{2}\) small, and we learn Gaussian latent variables \(h(X^{(i)},)\), then we recover the ground truth latent variables up to the tolerable ambiguities of labeling and scale, i.e., \(h\) recovers \(f^{-1}\) and \(W\) recovers \(B\) up to permutation and scale. Thus, we estimate the latent variables using \(=h(X,)\) and estimate the DAG using \(W_{0}\). Full details of the practical implementation of our approach are given in Appendix H.

Our experimental setup is similar to [84; 2], we consider \(d\) interventions with different targets (our theory holds in full generality) and therefore, we can arbitrarily assign \(t_{i}=i\) based on the intervention index \(i\) which removes the permutation ambiguity. We focus on non-zero shifts because the cross entropy loss together with the quadratic expression for the log-odds results in a non-convex output layer which makes it hard to find the global loss minimizer, as we will describe in more detail in Appendix G.1. We emphasize that this is no contradiction to the theoretical results stated above. Even when there is no shift intervention the latent variables are identifiable, but our algorithm often fails to find the global minimizer of the loss (9) due to the non-convex loss landscape. For the sake of exposition and to set the stage for future work, we also briefly describe an approach via Variational Autoencoders (VAE). VAEs have been widely used in causal representation learning and while feasible in interventional settings, they are accompanied by certain difficulties, which we detail and suggest how to overcome in Appendix F.4.

## 6 Experiments

In this section, we implement our approach on synthetic data and image data. Complete details (architectures, hyperparameters) are deferred to Appendix H. Additional experiments investigating the effect of varyortability  and the noise distribution can be found in Appendix G.

Data generationFor all our experiments we use Erdos-Renyi graphs, i.e., we add each undirected edge with equal probability \(p\) to the graph and then finally orient them according to some random order of the nodes. We write \((d,k)\) for the Erdos-Renyi graph distribution on \(d\) nodes with \(kd\) expected edges. For a given graph \(G\) we then sample edge weights from \(([0.25,1.0])\) and a scale matrix \(D\). For simplicity we assume that we have \(n\) samples from each environment \(i I\). We only consider the setting where each node is intervened upon once and thus the latent dimension is also known. We consider three types of mixing functions. First, we consider linear mixing functions where we sample all matrix entries i.i.d. from a Gaussian distribution. Then, we consider non-linear mixing functions that are parametrized by MLPs with three hidden layers which are randomly initialized, and have Leaky ReLU activations. Finally, we consider image data as described in . Pairs of latent variables \([z_{2i+1},z_{2i+2})\) describe the coordinates of balls in an image and the non-linearity \(f\) is the rendering of the image. The image generation is based on pygame . A sample image can be found in Figure 3.

Evaluation MetricsWe evaluate how well we can recover the ground truth latent variables and the underlying DAG. For the recovery of the ground truth latents we use the Mean Correlation Coefficient (MCC) [37; Appendix A.2]. For the evaluation of the learned graph structure we use the Structural Hamming Distance (SHD) where we follow the convention that we count the number of edge differences of the directed graphs (i.e., an edge with wrong orientation counts as two errors). Since the scale of the variables is not fixed the selection of the edge selection threshold is slightly delicate. Thus, we use the heuristic where we match the number of selected edges to the expected number

Figure 3: Sample image with 3 balls.

Figure 2: Dependence of performance metrics for \(ER(d,2)\) graphs with \(d^{}=100\) and nonlinear mixing \(f\) on the dimension \(d\).

of edges. As a metric that is independent of this thresholding procedure, we also report the Area Under the Receiver Operating Curve (AUROC) for the edge selection.

MethodsWe implement our contrastive algorithm as explained in Section F where we use an MLP (with Leaky ReLU nonlinearities) for the function \(h\) for the linear and nonlinear mixing functions and a very small convolutional network for the image dataset, which are termed "Contrastive". For linear mixing function we also consider a version of the contrastive algorithm where \(h\) is a linear function, termed "Contrastive Linear". As baselines, we consider a variational autoencoder with latent dimension \(d\) and also the algorithm for linear disentanglement introduced in . Since the variational autoencoder does not output a causal graph structure we report the result for the empty graph here which serves as a baseline.

**Results for linear \(f\)** First, for the sake of comparison, we replicate exactly the setting from , i.e., we consider initial noise variances sampled uniformly from \(\) and we consider perfect interventions where the new variance is sampled uniformly from \(\). We set \(d=5\), \(d^{}=10\), \(k=3/2\), and \(n=50000\). Results can be found in Table 1. The linear contrastive method identifies the ground truth latent variables up to scaling. The failure of the nonlinear method to recover the ground truth latents will be explained and further analyzed in Appendix G.1. Note that the nonlinear contrastive and the linear contrastive method recover the underlying graph better than the baseline.

**Results for nonlinear \(f\)** We sample all variances from \(()\) (initial variance and resampling for the perfect interventions) and the shift parameters \(^{(i)}\) of the interventions from \(()\). Results can be found in Table 2. We find that our contrastive method can recover the ground truth latents and the causal structure almost perfectly, while the baseline for linear disentanglement cannot recover the graph or the latent variables (which is not surprising as the mixing is highly nonlinear). Also, training a vanilla VAE does not recover the latent variables up to a linear map as indicated by the \(R^{2}\) scores. In Figure 2 we illustrate the dependence on the dimension \(d\) of our algorithm in this setting.

**Results for image data** Finally, we report the results for image data. Here, we generate the graph as before and consider variances sampled from \(^{2}([0.01,0.02])\) and shifts \(^{(i)}\) from \(([0.1,0.2])\) (i.e., the shifts are still of order \(\)). We exclude samples where one of the balls is not contained in the image which generates a slight model misspecification compared to our theory. Again we find that we recover the latent graph and the latent variables as detailed in Table 3.

   Method & SHD \(\) & AUROC \(\) & MCC \(\) & \(R^{2}\) \\ 
**Contrastive** & \(4.6 0.5\) & \(0.84 0.02\) & \(0.05 0.02\) & \(0.02 0.00\) \\
**Contrastive Linear** & \(5.4 1.6\) & \(0.80 0.07\) & \(0.90 0.03\) & \(1.00 0.00\) \\ Linear baseline & \(7.0 0.5\) & \(0.64 0.05\) & \(0.83 0.04\) & \(1.00 0.00\) \\   

Table 1: Results for linear \(f\) with \(d=5,d^{}=10\), \(k=3/2\), \(n=50000\).

   Setting & Method & SHD \(\) & AUROC \(\) & MCC \(\) & \(R^{2}\) \\  =20\)} & **Contrastive** & \(1.8 0.5\) & \(0.97 0.01\) & \(0.97 0.00\) & \(0.96 0.00\) \\  & VAE & \(10.0 0.0\) & \(0.50 0.00\) & \(0.48 0.03\) & \(0.57 0.07\) \\  & Linear baseline & \(10.6 1.9\) & \(0.48 0.11\) & \(0.32 0.03\) & \(0.34 0.06\) \\  =100\)} & **Contrastive** & \(1.0 0.0\) & \(1.00 0.00\) & \(0.99 0.00\) & \(0.98 0.00\) \\  & VAE & \(10.0 0.0\) & \(0.50 0.00\) & \(0.59 0.02\) & \(0.68 0.04\) \\  & Linear baseline & \(3.4 1.2\) & \(0.85 0.07\) & \(0.18 0.04\) & \(0.11 0.04\) \\  =20\)} & **Contrastive** & \(3.6 1.3\) & \(0.98 0.01\) & \(0.93 0.00\) & \(0.87 0.01\) \\  & VAE & \(18.6 0.9\) & \(0.50 0.00\) & \(0.59 0.02\) & \(0.72 0.02\) \\  & Linear baseline & \(29.6 2.5\) & \(0.49 0.02\) & \(0.44 0.02\) & \(0.51 0.02\) \\  =100\)} & **Contrastive** & \(1.6 0.5\) & \(1.00 0.00\) & \(0.98 0.00\) & \(0.97 0.00\) \\  & VAE & \(18.6 0.9\) & \(0.50 0.00\) & \(0.62 0.02\) & \(0.78 0.01\) \\  & Linear baseline & \(28.4 2.1\) & \(0.51 0.04\) & \(0.17 0.03\) & \(0.13 0.03\) \\   

Table 2: Results for nonlinear synthetic data with \(n=10000\).

## 7 Conclusion

In this work, we extend several prior works and show identifiability for a widely used class of linear latent variable models with non-linear mixing, from interventional data with unknown targets. Counterexamples show that our assumptions are tight in this setting and could only potentially be relaxed under other additional assumptions. We leave to future work to extend our results for other classes of priors, such as non-parametric distribution families, and also to study sample complexity and robustness of our results. We also proposed a contrastive approach to learn such models in practice and showed that it can recover the latent structure in various settings. Finally, we highlight that the results of our experiments are very promising and it would be interesting to scale up our algorithms to large-scale datasets such as .