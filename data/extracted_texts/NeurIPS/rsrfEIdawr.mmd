# DaRF: Boosting Radiance Fields from Sparse Inputs with Monocular Depth Adaptation

Jiuhn Song & Seonghoon Park & Honggyu An &Seokju Cho & Min-Seop Kwak & Sungjin Cho & Seungryong Kim &Korea University

Equal contributionCorresponding author

###### Abstract

Neural radiance field (NeRF) shows powerful performance in novel view synthesis and 3D geometry reconstruction, but it suffers from critical performance degradation when the number of known viewpoints is drastically reduced. Existing works attempt to overcome this problem by employing external priors, but their success is limited to certain types of scenes or datasets. Employing monocular depth estimation (MDE) networks, pretrained on large-scale RGB-D datasets, with powerful generalization capability would be a key to solving this problem: however, using MDE in conjunction with NeRF comes with a new set of challenges due to various ambiguity problems exhibited by monocular depths. In this light, we propose a novel framework, dubbed DaRF, that achieves robust NeRF reconstruction with a handful of real-world images by combining the strengths of NeRF and monocular depth estimation through online complementary training. Our framework imposes the MDE network's powerful geometry prior to NeRF representation at both seen and unseen viewpoints to enhance its robustness and coherence. In addition, we overcome the ambiguity problems of monocular depths through patch-wise scale-shift fitting and geometry distillation, which adapts the MDE network to produce depths aligned accurately with NeRF geometry. Experiments show our framework achieves state-of-the-art results both quantitatively and qualitatively, demonstrating consistent and reliable performance in both indoor and outdoor real-world datasets. Project page is available at https://ku-cvlab.github.io/DaRF/.

## 1 Introduction

Neural radiance field (NeRF)  has gained significant attention for its powerful performance in reconstructing 3D scenes and synthesizing novel views. However, despite its impressive performance, NeRF often comes with a considerable limitation in that its performance highly relies on the presence of densely well-calibrated input images which are difficult to acquire. As the number of input images is reduced, NeRF's novel view synthesis quality drops significantly, displaying failure cases such as erroneous overfitting to the input images , artifacts clouding empty spaces , or degenerate geometry that yields incomprehensible jumble when rendered at unseen viewpoints . These challenges derive from its under-constrained nature, causing it to have extreme difficulty mapping a pixel in input images to a correct 3D location. In addition, NeRF's volume rendering allows the model to map a pixel to multiple 3D locations , exacerbating this problem.

Previous _few-shot_ NeRF methods attempt to solve these issues by imposing geometric regularization  or exploiting external 3D priors  such as depth information extracted frominput images by COLMAP . However, these methods have weaknesses in that they use 3D priors extracted from a few input images only, which prevents such guidance from encompassing the entire scene. To effectively tackle all the issues mentioned above, pretrained monocular depth estimation (MDE) networks with strong generalization capability [35; 34; 4] could be used to inject an additional 3D prior into NeRF that facilitates robust geometric reconstruction. Specifically, geometry prediction by MDE can constrain NeRF into recovering smooth and coherent geometry, while their bias towards predicting smooth geometry helps to filter out fine-grained artifacts that clutter the scene. More importantly, NeRF's capability to render any unseen viewpoints enables fully exploiting the capability of the MDE, as MDE could provide depth prior to the numerous renderings of unseen viewpoints as well as the original input viewpoints. This allows injecting additional 3D prior to effectively covering the entire scene instead of being constrained to a few input images.

However, applying MDE to few-shot NeRF is not trivial, as there are ambiguity problems that hinder the monocular depth from serving as a good 3D prior. Primarily, relative depths predicted by MDEs are not multiview-consistent . Moreover, MDEs perform poorly in estimating depth differences between multiple objects: this prevents global scale-shift fitting [55; 26] from being a viable solution, as alignment to one region of the scene inevitably leads to misalignment in many other regions. There also exists a convexity problem , in which the MDE has difficulty determining whether the surface is planar, convex, or concave, are also present. To overcome these challenges, we introduce a novel method to adapt MDE to NeRF's absolute scaling and multiview consistency as NeRF is regularized by MDE's powerful 3D priors, creating a complementary cycle.

In this paper, we propose DaRF, short for Monocular **D**epth **A**daptation for boosting **R**adiance **F**ields from Sparse Input Views, which achieves robust optimization of few-shot NeRF through MDE's geometric prior, as well as MDE adaptation for alignment with NeRF through complementary training (see Fig. 1). We exploit MDE for robust geometry reconstruction and artifact removal in both _unseen_ and _seen_ viewpoints. In addition, we leverage NeRF to adapt MDE toward multiview-consistent geometry prediction and introduce novel patch-wise scale-shift fitting to more accurately map local depths to NeRF geometry. Combined with a confidence modeling technique for verifying accurate depth information, our method achieves state-of-the-art performance in few-shot NeRF optimization. We evaluate and compare our approach on real-world indoor and outdoor scene datasets, establishing new state-of-the-art results for the benchmarks.

## 2 Related Work

Neural radiance field.Neural radiance field (NeRF)  represents photo-realistic 3D scenes with MLP. Owing to its remarkable performance, there has been a variety of follow-up studies [2; 51; 25].

Figure 1: **Overview. DaRF shows robust optimization of few-shot NeRF through MDEâ€™s geometric prior, removing inherent ambiguity from MDE through novel patch-wise distillation loss and MDE adaptation. Unlike existing work  that distills depths by applying pretrained MDE to NeRF at seen view only, our DaRF fully exploits the ability of MDE by jointly optimizing NeRF and MDE at a specific scene, and distilling the monocular depth prior to NeRF at both seen and unseen views.**

These studies improve NeRF such as dynamic and deformable scenes [31; 44; 33; 1], real-time rendering [51; 36; 28], unbounded scene [3; 42; 48] and generative modeling [40; 30; 7]. However, these works still encounter challenges in synthesizing novel views with a limited number of images in a single scene, limiting their applicability in real-world scenarios.

Few-shot NeRF.Numerous _few-shot_ NeRF works attempted to address few-shot 3D reconstruction problem through various techniques, such as pretraining external priors [52; 9], meta-learning , regularization [15; 29; 16; 20] or off-the-shelf modules [15; 29]. Recent approaches [29; 16; 20] emphasize the importance of geometric consistency and apply geometric regularization at unknown viewpoints. However, these regularization methods show limitations due to their heavy reliance on geometry information recovered by NeRF. Other works such as DS-NeRF , DDP-NeRF  and SCADE  exploit additional geometric information, such as COLMAP  3D points or monocular depth estimation, for geometry supervision. However, these works have critical limitations of only being able to provide geometry information corresponding to existing input viewpoints. Unlike these works, our work demonstrates methods to provide geometric prior even at unknown viewpoints with MDE for more effective geometry reconstruction.

Monocular depth estimation.Monocular depth estimation (MDE) is a task that aims to predict a dense depth map given a single image. Early works on MDE used handcrafted methods such as MRF for depth estimation . After the advent of deep learning, learning-based approaches [14; 17; 21] were introduced to the field. In this direction, the models were trained on ground-truth depth maps acquired by RGB-D cameras or LiDAR sensors to predict absolute depth values [24; 23]. Other approaches trained the networks on large-scale diverse datasets [8; 22; 34; 35], which demonstrates better generalization power. These approaches struggle with depth ambiguity caused by ill-posed problem, so the following works LeRes  and ZoeDepth  opt to recover absolute depths using additional parameters.

Incorporating MDE into 3D representation.As both NeRF and monocular depth estimation are closely related, there have been some works that utilize MDE models to enhance NeRF's performance. NeuralLift , MonoSDF  and SCADE  leverage depths predicted by pretrained MDE for depth ordering and detailed surface reconstruction, respectively. Other works optimize scene-specific parameters, such as depth predictor utilizing depth recovered by COLMAP  or learnable scale-shift values for reconstruction in noisy pose setting . However, these previous approaches were limited in that MDEs were used to provide prior to only the input viewpoints, which constrains their effectiveness when input views are reduced, e.g., in the few-shot setting.

As a concurrent work, SCADE  utilizes MDE for sparse view inputs, by injecting uncertainty into MDE through additional pretraining so that canonical geometry can be estimated through probabilistic modeling between multiple modes of estimated depths. While the ultimate goal which is to overcome the ambiguity of MDE may be similar, our approach directly removes ambiguity present in MDE by finetuning with canonical geometry captured by NeRF, for effective suppression of artifacts and divergent behaviors of few-shot NeRF.

## 3 Preliminaries

NeRF  represents a scene as a continuous function \(_{}()\) represented by a neural network with parameters \(\). During optimization, 3D points are sampled along rays represented by \(\) coming from a set of input images \(=\{I_{i}\}\), whose ground truth camera poses are given, for evaluation by the neural network. For each sampled point, \(_{}()\) takes as input its coordinate \(^{3}\) and viewing direction \(^{2}\) with a positional encoding \(()\) that facilitates learning high-frequency details, and outputs a color \(^{3}\) and a density \(\) such that \(\{,\}=_{}((),( ))\). With a ray parameterized as \(_{}(t)=+t_{}\), starting from camera center \(\) along the direction \(_{}\), color and depth value at the pixel \(\) are rendered as follows:

\[()=_{t_{n}}^{t_{f}}T(t)(_{}(t) )(_{}(t))dt,\ \ ()=_{t_{n}}^{t_{f}}T(t)(_{}(t)) tdt,\] (1)

where \(()\) and \(()\) are rendered color and depth values at the pixel \(\) along the ray \(_{}(t)\) from \(t_{n}\) to \(t_{f}\), and \(T(t)\) denotes an accumulated transmittance along the ray from \(t_{n}\) to \(t\) as follows:

\[T(t)=(-_{t_{n}}^{t}(_{}(s))ds).\] (2)Based on this volume rendering, \(_{}()\) is optimized by the reconstruction loss \(_{}\) that compares rendered color \(()\) to corresponding ground-truth \(I()\), with \(\) as a set of pixels for training rays:

\[_{}=_{I_{i}}_{ } I_{i}()-_{i}()_{2}^{2}.\] (3)

Our work explores the setting of few-shot optimization with NeRF [16; 20]. Whereas the number of input viewpoints \(||\) is normally higher than one hundred in the standard NeRF setting , the task of few-shot NeRF considers scenarios when \(||\) is drastically reduced to a few viewpoints (e.g., \(||<20\)). With such a small number of input viewpoints, NeRF shows high divergent behaviors such as geometry breakdown, overfitting to input viewpoints, and generation of artifacts that cloud the empty space between the camera and object, which causes its performance to drop sharply [15; 16; 29]. To overcome this problem, existing few-shot NeRF frameworks applied regularization techniques at unknown viewpoints to constrain NeRF with additional 3D priors [37; 11] and enhance the robustness of geometry, but they showed limited performance.

## 4 Methodology

### Motivation and Overview

Our framework leverages the complementary benefits of few-shot NeRF and monocular depth estimation networks for the goal of robust 3D reconstruction. The benefits that pretrained MDE can provide to few-shot NeRF are clear and straightforward: because they predict dense geometry, they provide guidance for the NeRF to recover more smooth geometry. In cases where few-shot NeRF's geometry undergoes divergent behaviors, MDE provides strong constraints to prevent the global geometry from breaking down.

However, there are difficult challenges that must be overcome if the depths estimated by MDE are to be used as 3D prior to NeRF. These challenges, which can be summarized as depth ambiguity problems , stem from the inherent ill-posed nature of the monocular depth estimation. Most importantly, MDE networks only predict relative depth information inferred from an image, meaning it is initially not aligned to NeRF's absolute geometry . Global scaling and shifting may seem to be the answer, but this approach leads us to another depth ambiguity problem, as predicted scales and spacings of each instance are inconsistent with one another, as demonstrated in (b) of Fig. 2. Additionally, MDE's weakness in predicting the convexity of a surface, whether it is flat, convex, or concave - also poses a problem in using this depth for NeRF guidance.

In this light, we adapt a pre-trained monodepth network to a single NeRF scene so that its powerful 3D prior can be leveraged to its maximum capability in regularizing the few-shot NeRF. In the following, we first explain how to distill geometric prior from off-the-shelf MDE model  from both seen and unseen viewpoints (Sec. 4.2). We also provide a strategy for adapting the MDE model to handle ill-posed problems to a specific scene, while keeping its 3D prior knowledge (Sec. 4.3). Then, we demonstrate a method to handle inaccurate depths (Sec. 4.4). Fig. 1 shows an overview of our method, compared to previous works using MDE prior [45; 53].

Figure 2: **Effectiveness of patch-wise scale and shift adjustment: (a) input image, (b) monocular depth with image-level adjustment, and (c) monocular depth with patch-level adjustment. We visualize the error of adjusted monocular depth from input image compared to GT depth value. The proposed patch-level adjustment helps to minimize the errors caused by inconsistency in depth differences among objects.**

### Distilling Monocular Depth Prior into Neural Radiance Field

To prevent the degradation of reconstruction quality in few-shot NeRF, we propose to distill monocular depth prior to the neural radiance field during optimization. By exploiting pre-trained MDE networks , which have high generalization power, we enforce a dense geometric constraint on both _seen_ and _unseen_ viewpoints by using estimated monocular depth maps as pseudo ground truth depth for training few-shot NeRF. We describe the details of this process below.

Monocular depth regularization on seen views.We leverage a pre-trained MDE model, denoted as \(_{}()\) with parameters \(\), to predict pseudo depth map from given _seen_ view image \(I_{i}\) as \(D_{i}^{*}=_{}(I_{i})\). Since \(D_{i}^{*}\) is initially a relative depth map, it needs to be scaled and shifted into an absolute depth  and aligned with NeRF's rendered depth \(\) in order for it to be used as pseudo-depth \(D^{*}\). However, the scale and shift parameters inferred from the global statistic may undermine local statistic . For example, as shown in Fig. 2 (b), global scale fitting tends to favor dominant objects in the image, leading to ill-fitted depths in less dominant sections of the scene due to inconsistencies in predicted depth differences between the objects. Naively employing such inaccurately estimated depths for distillation can adversely impact the overall geometry of the NeRF.

To alleviate this issue, we propose a patch-wise adjustment of scale and shift parameters, reducing the impact of erroneous depth differences, as illustrated in Fig. 2 (c). The depth consistency loss is defined as follows:

\[_{}=_{I_{i}}_{ }\|(w_{i}(D_{i}^{*}())+q_{i})-_{i}()\|,\] (4)

where \(w_{i}\) and \(q_{i}\) denote the scale and shift parameters obtained by least square  between \(D_{i}^{*}\) and \(_{i}\), \(\) denotes a set of pixels within a patch, and \(()\) denotes stop-gradient operation. Thus patch-based approach also helps to overcome the computational bottleneck of full image rendering.

Monocular depth regularization on unseen views.We further propose to give supervision even at _unseen_ viewpoints. As NeRF has the ability to render any unseen viewpoint of the scene, we render color \(_{l}\) and depth \(_{l}\) from a sampled patch of \(l\)-th novel viewpoint. Sequentially, we extract a monocular depth map from the rendered image as \(_{l}^{*}=_{}(_{l})\). Then, we enforce consistency between our rendered depth \(_{l}\) and the monocular depth \(_{l}^{*}\) of \(l\)-th novel viewpoint as follows:

\[_{}=_{I_{i}}_{ }\|(w_{l}(_{l}^{*}())+q_{l})- _{l}()\|,\] (5)

where \(\) denotes a set of unseen view images, \(w_{l}\) and \(q_{l}\) denotes the scale and shift parameters used to align \(_{l}^{*}\) towards \(_{l}\), and \(\) denotes randomly sampled patch.

A valid concern regarding this approach is that monocular depth obtained from noisy NeRF rendering may be affected by fine-grained rendering artifacts that frequently appear in unseen viewpoints of few-shot NeRF, resulting in noisy and erroneous pseudo-depths. However, we demonstrate in

Figure 3: **Robustness of MDE model for multi-view scale ambiguity and artifacts: (a-b) color and depth of NeRF rendered in the early stage of the training, (c-d) monocular depths estimated from rendered image \(\) and input image \(I\). The results show that MDE model ignores the artifacts of rendered images by NeRF, enabling reliable supervision for seen and unseen viewpoint.**Fig. 3 that a strong geometric prior within the MDE model exhibits robustness against such artifacts, effectively filtering out the artifacts and thereby providing reliable supervision for the unseen views.

It should be noted that our strategy differs from previous methods [11; 37; 53; 45] that exploit monocular depth estimation  and external depth priors such as COLMAP . These methods only impose depth priors upon the input viewpoints, and thus their priors only influence the scene partially due to self-occlusions and sparsity of known views. Our method, on the other hand, enables external depth priors to be applied to any arbitrary viewpoint and thus allows guidance signals to thoroughly reach every location of the scene, leading to more robust and coherent NeRF optimization.

### Adaptation of MDE via Neural Radiance Field

Although the patch-wise distillation of monocular depth provides invariance to depth difference inconsistency in MDE, the ill-posed nature of monocular depth estimation often introduces additional ambiguities, such as the inability to distinguish whether the surface is concavity, convexity, or planar or difficulty in determining the orientation of flat surfaces . We argue that these ambiguities arise due to the MDE lacking awareness of the scene-specific absolute depth priors and multiview consistency. To address this issue, we propose providing the scene priors optimized NeRF to MDE, whose knowledge of canonical space and absolute geometry helps eliminate the ambiguities present within MDE. Therefore, we propose to adapt the MDE to the absolute scene geometry, formally written as:

\[_{}=_{I_{i}}_{ }\{\|(_{i}())-D_{i}^{*} ()\|+\|(w_{i}(_{i}())+q_{i})-D_ {i}^{*}()\|\}.\] (6)

In addition to the patch-wise loss in Eq. 4, we add an \(l\)-1 loss without scale-shift adjustment to adapt the MDE with absolute depth prior. We also introduce a regularization term to preserve the local smoothness of MDE, given by:

\[_{}=_{I_{i}}_{ }\|(w_{i}(D_{i}^{*,}()) +q_{i})-D_{i}^{*}()\|,\] (7)

where \(D_{i}^{*,}\) denotes monocular depth map of \(I_{i}\) extracted from MDE with initial pre-trained weight.

### Confidence Modeling

Our framework must take into account the errors present in both few-shot NeRF and estimated monocular depths, which will propagate  and intensify during the distillation process if left unchecked. To prevent this, we adopt confidence modeling [20; 41] inspired by self-training approaches , to verify the accuracy and reliability of each ray before the distillation process.

The homogeneous coordinates of a pixel \(\) in the seen viewpoint are transformed to \(^{}\) at the target viewpoint using the viewpoint difference \(R_{i l}\) and the camera intrinsic parameter \(K\), as follows:

\[^{} KR_{i l}D_{i}()K^{-1}.\] (8)

We generate the confidence map \(M_{i}\) by measuring the distance between rendered depth of the unseen viewpoint and MDE output of seen viewpoint such that

\[M_{i}()=\|(w_{i}D_{i}^{*}()+q_{i})-_{l}( ^{})\|<,\] (9)

where \(\) denotes threshold parameter, \([]\) is Iverson bracket, and \(D_{l}(^{})\) refers to depth value of the corresponding pixel at \(l\)-th unseen viewpoint for reprojected target pixel \(\) of \(i\)-th seen viewpoint. We fit \(D_{i}^{*}\) to absolute scale, where scale and shift parameters, \(w_{i}\) and \(q_{i}\), are obtained by least square  between \(D_{i}^{*}\) and \(_{i}\).

### Overall Training

With the incorporation of confidence modeling, the loss functions for both the radiance field and MDE can redefined. \(_{}\) and \(_{}\) can be redefined as:

\[_{}=_{I_{i}}_{}M_{i}()\|(w_{i}(D_{i}^{*}( ))+q_{i})-_{i}()\|,\] (10) \[_{}=_{I_{i}}_{ }M_{l}()\|(w_{i}( _{l}^{*}())+q_{l})-_{l}()\|.\] (11)In addition, the loss for the adaptation of the MDE module can be redefined considering \(M\):

\[_{}=_{I_{i}}_{} M_{i}()(\|(_{i}())-_{i}^{*}( )\|+\|(w_{i}(_{i}())+q_{i})- _{i}^{*}()\|).\] (12)

With these losses, we train both NeRF and MDE simultaneously, enhancing both models by complementing each other. MDE provides a strong geometric prior to NeRF while having the inherent limitation of obliviousness to the scene-specific prior, whereas NeRF provides it with its absolute geometry.

## 5 Experiments

### Experimental Settings

Implementation details.DARF is implemented based on \(K\)-planes  as NeRF. We use DPT-hybrid  as MDE model. We use Adam  as an optimizer, with a learning rate of \(1 10^{-2}\) for NeRF and \(1 10^{-5}\) for the MDE, along with a cosine warmup learning rate scheduling. See supplementary material for more details. The code and pre-trained weights will be made publicly available.

Datasets.We evaluate our method in real-world scenes captured at both indoor and outdoor locations. Following previous works [37; 45], we use a subset of sparse-view ScanNet data  comprised with three indoor scenes, each consisting of 18 to 20 training images and 8 test images. We also conduct evaluations on more challenging setting with 9 to 10 train images. For outdoor reconstruction, we further test on 5 challenging scenes from the Tanks and Temples dataset . The scenes are real-world outdoor dataset, with a wide variety of scene scales and lighting conditions. Note that these setups are extremely sparse compared to full image setups, where we use approximately 0.5 to 5 percent of the whole training inputs.

Baselines.We adopt the following six recently proposed methods as baselines: standard neural radiance field method: \(K\)-planes , few-shot NeRF method: RegNeRF , and depth prior based methods: NerfingMVS , DS-NeRF , DDP-NeRF , and SCADE .

Evaluation metrics.For quantitative comparison, we follow the NeRF  and report the PSNR, SSIM , LPIPS . We report standard evaluation metrics for depth estimation , absolute relative error (Abs Rel), squared relative error (SqRel), root mean squared error (RMSE), root mean

    &  &  &  \\   & &  &  &  \\  & & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\   NerfingMVS  & âœ“ & N/A & N/A & N/A & 16.29 & 0.626 & 0.502 & N/A & N/A & N/A \\ \(K\)-planes  & âœ— & 16.01 & 0.618 & 0.494 & 18.70 & 0.708 & 0.400 & 12.57 & 0.453 & 0.607 \\  RegNeRF  & âœ— & 16.38 & 0.624 & 0.493 & 18.93 & 0.676 & 0.450 & 14.12 & 0.469 & **0.580** \\ DS-NeRF  & âœ“ & N/A & N/A & N/A & 20.85 & 0.713 & 0.344 & N/A & N/A & N/A \\ DDP-NeRF  & âœ“ & N/A & N/A & N/A & 19.29 & 0.695 & 0.368 & N/A & N/A & N/A \\ SCADE  & âœ“ & **18.83** & 0.646 & **0.375** & 21.54 & 0.732 & **0.292** & 13.46 & 0.402 & 0.607 \\  DaRF (Ours) & âœ“ & 18.29 & **0.690** & 0.412 & **21.58** & **0.765** & 0.325 & **15.70** & **0.514** & 0.583 \\   

Table 1: **Quantitative comparison on ScanNet  and Tanks and Temples . The best results are highlighted in bold, while the second best results are underlined.**

   Methods & AbsRel \(\) & SqRel \(\) & RMSE \(\) & RMSE \(\) \\   LeRes  & 0.391 & 0.472 & 0.999 & 0.661 \\ MiDaS  & 0.152 & 0.095 & 0.452 & 0.183 \\ DPT  & 0.191 & 0.135 & 0.563 & 0.220 \\  DaRF (9 - 10 views) & 0.154 & 0.074 & 0.361 & 0.171 \\ DaRF (18 - 20 views) & **0.151** & **0.071** & **0.356** & **0.168** \\   

Table 2: **Evaluation of depth quality: (a) quantitative evaluation of the adapted MDE, compared with other monocular depth estimation models and (b) visualization of depth distributions. The adapted MDE by our method shows a similar distribution to that of the ground truth.**squared log error (RMSE log). To evaluate view consistency, we utilize a single scaling factor \(s\) for each scene, which is the median scaling  value averaged across all test views.

### Comparisons

Indoor scene reconstruction.We conducted experiments in two settings: (1) a standard few-shot setup as described in literature [37; 45], and (2) an extreme few-shot setup with approximately 0.5 percent of the full images. As shown in Tab. 1, our approach outperforms the baseline methods in both settings in most of the metrics. Additionally, we provide quantitative results of the adapted MDE model in ScanNet dataset in Tab. 2, and qualitative results in Fig. 4. As shown in Fig. 5 for the setting of standard few-shot, DS-NeRF  and DDP-NeRF  still show floating artifacts in the novel view and show limitation in capturing details in the chair, smoothing into nearby object. Our method shows better qualitative results compared to other baselines, showing better geometry understanding and detailed view synthesis in the small objects near the chair. In the extreme few-shot setup, we conducted a visual comparison between our method and a baseline  in Fig. 6. This is a more complex setting than standard, but our method outperforms most of the baselines, showing better geometric understanding. It should be noted that SCADE  fine-tunes its depth network on an indoor dataset, Taskonomy dataset ; whereas our model does not undergo any additional fine-tuning. More qualitative images are included in the supplementary material.

Outdoor scene reconstruction.We conduct the qualitative and quantitative comparisons on the Tanks and Temples dataset in Tab. 1 and Fig. 7. Since COLMAP  with sparse images is not available, we provide comparisons with baselines without explicit depth prior. The quantitative results show that our approach outperforms the baseline methods on this complex outdoor dataset in all

Figure 4: **Error map visualization.** MDE adaptation results in a reduction of errors.

Figure 5: **Qualitative results of on ScanNet  with 18 - 20 input views.**

Figure 6: **Qualitative results on ScanNet  with 9 - 10 input views.**

[MISSING_PAGE_FAIL:9]

Analysis of MDE Adaptation Loss.In Tab. 6, we further investigate the effectiveness of scale-shift loss and \(l1\) loss at MDE adaptation. Equation 6 revolves around the idea of adapting MDE toward predicting a scene-specific absolute geometry, which is achieved by the first addend term: this first term forces itself MDE to adapt towards multiview consistency so that its ill-posed nature is reduced and its initial global depth prediction grows to be more in accordance with the absolute geometry captured by NeRF. In contrast, the second addend term, which takes into account patch-wise scale-shift fitting, is designed to aid the modeling of fine, detailed, local geometry which the model has difficulty modeling without such local fitting. As shown in the results, when only one term is used for optimization (scale-shift or \(l1\)), it performs worse in every metric than in both are used in conjunction (ours). This justifies our strategy of using both losses as effective.

## 6 Conclusion

We propose DaRF, a novel method that addresses the limitations of NeRF in few-shot settings by fully leveraging the ability of monocular depth estimation networks. By integrating MDE's geometric priors, DaRF achieves robust optimization of few-shot NeRF, improving geometry reconstruction and artifact removal in both unseen and seen viewpoints. We further introduce patch-wise scale-shift fitting for accurate mapping of local depths to 3D space, and adapt MDE to NeRF's absolute scaling and multiview consistency, by distilling NeRF's absolute geometry to monocular depth estimation. Through complementary training, DaRF establishes a strong synergy between MDE and NeRF, leading to a state-of-the-art performance in few-shot NeRF. Extensive evaluations on real-world scene datasets demonstrate the effectiveness of DaRF.