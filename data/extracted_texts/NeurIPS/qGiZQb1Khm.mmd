# Watermarking Makes Language Models Radioactive

Tom Sander

Meta FAIR & Ecole polytechnique

&Pierre Fernandez

Meta FAIR & Inria Rennes

&Alain Durmus

Ecole polytechnique

&Matthijs Douze

Meta FAIR

&Teddy Furon

Inria Rennes

Equal Contribution. Correspondence at {tomsander,pfz}@meta.com

###### Abstract

We investigate the _radioactivity_ of text generated by large language models (LLM), i.e., whether it is possible to detect that such synthetic input was used to train a subsequent LLM. Current methods like membership inference or active IP protection either work only in settings where the suspected text is known or do not provide reliable statistical guarantees. We discover that, on the contrary, it is possible to reliably determine if a language model was trained on synthetic data if that data is output by a watermarked LLM. Our new methods, specialized for radioactivity, detects with a provable confidence weak residuals of the watermark signal in the fine-tuned LLM. We link the radioactivity contamination level to the following properties: the watermark robustness, its proportion in the training set, and the fine-tuning process. For instance, if the suspect model is open-weight, we demonstrate that training on watermarked instructions can be detected with high confidence (\(p\)-value \(<10^{-5}\)) even when as little as \(5\%\) of training text is watermarked. Radioactivity detection code is available at https://github.com/facebookresearch/radioactive-watermark

## 1 Introduction

Large Language Models (LLMs) are often instruction fine-tuned to align them with human prompts and improve their performance and generalization (Ouyang et al., 2022; Wei et al., 2022; Chung et al., 2022). Fine-tuning requires expert knowledge to balance diversity and quality in the instruction dataset and a costly collection of manual annotations, especially for alignment (OpenAI, 2023; Touvron et al., 2023; Gemini, 2023). To address the cost and the difficulties of fine-tuning, practitioners often train on synthetic data generated by a model that has already been instructed, such as Bard, ChatGPT, or Claude. For example, works by Wang et al. (2022), Honovich et al. (2022), Peng et al. (2023) created instruction data for many of the most recent LLMs (Taori et al., 2023; Xu et al., 2023;

Figure 1: Bob fine-tunes his LLM on data with a fraction coming from Alice’s LLM. This leaves traces in Bob’s model that Alice can detect reliably, provided that her text was watermarked. Thus, a side effect of Alice’s watermark, intended for machine-generated text detection, is to reveal what data Bob’s model was fine-tuned on.

Gunasekar et al., 2023; Mukherjee et al., 2023). This may also be unintentional when, for example, Turkers use ChatGPT to perform their tasks (Veselovsky et al., 2023). Such imitation raises questions about whether the fine-tuned model is a derivative work of the original model (Wallace et al., 2020). In this context, it is crucial to understand how to detect when LLM outputs are used as training data.

Meanwhile, recent AI regulations enforce the transparency of generative models. This is increasingly important in cases where the generated content may be used for malicious purposes (Weidinger et al., 2022; Crothers et al., 2022). One approach is _watermarking_. It embeds a secret trace in the synthetic content that can be detected to identify the generating model. In the context of LLMs, recent techniques make detection efficient with minimal degradation of the generated text quality by altering the sampling of next tokens (Aaronson and Kirchner, 2023; Kirchenbauer et al., 2023; Kirchenbauer et al., 2023).

Based on these two observations, this study addresses the following question:

_What occurs when watermarked text is employed as fine-tuning data?_

We explore the potential "radioactivity" - a term coined by Sablayrolles et al. (2020) - of LLM watermarking, which refers to the capacity of watermarked training data to contaminate a model.

We examine a model that has been fine-tuned on a corpus that may contain watermarked text (see Fig. 1). The baseline method for detecting radioactivity executes the original watermark detection on the outputs generated by this model. However, this approach proves ineffective because the residual of the watermark is a weak signal hardly detectable in plain output text. In this work, we are able to demonstrate that LLM watermarking is indeed radioactive thanks to our specific protocol designed for revealing weak contamination traces. Our contributions include:

* We design radioactivity detection methods for four scenarios based on model (_open / closed_) and training data (_supervised / unsupervised_) access. Notably, our open-model detection (Fig. 3) improves the performance by orders of magnitudes.
* We show how to obtain reliable \(p\)-values for watermark detection when scoring millions of tokens.
* We prove that watermarked text is radioactive in a real-world setting where an LLM is fine-tuned on slightly watermarked instruction data. For instance, in the open-model scenario, our tests detect radioactivity with a \(p\)-value of \(10^{-5}\) when only 5% of fine-tuning data is watermarked (Fig. 5).

## 2 Background

### Related work

Watermarking for LLMs.A recent branch of watermarking methods for decoder-only LLMs modifies either the probability distribution (Kirchenbauer et al., 2023) or the sampling method of the next token (Aaronson and Kirchner, 2023; Kuditipudi et al., 2023). Theoretical studies indicate that detectability depends on the entropy of generated text (Christ et al., 2023; Huang et al., 2023). Subsequent research suggests watermarking entropic passages, particularly in code (Lee et al., 2023), while other works focus on "semantic" watermarks that depend on an entire past text's semantic representation (Liu et al., 2023; Liu and Bu, 2024; Fu et al., 2024).

Gu et al. (2023) distill the methods used in this study within the model weights, allowing LLMs to generate watermarked logits natively, which is key for open-source models. In contrast, we focus on unintentional contamination: Alice and Bob in Fig. 1 are not collaborating, and Bob consumes only a small proportion of watermarked data.

Membership inference attacks(MIAs) aim to determine whether an arbitrary sample is included in a model's training data, with varying granularity on the adversary's knowledge (Nasr et al., 2019). Most of the time, the detection either build shadow models and observe a difference in their behavior (Shokri et al., 2017; 20; 20; Hisamoto et al., 2020; Mahloujifar et al., 2021) or directly observe the loss of the model (Yeom et al., 2018; Sablayrolles et al., 2019; Watson et al., 2021; Carlini et al., 2022). In the context of generative models, MIAs are intertwined with _dataset contamination_ where one detects that an entire dataset is part of the training data (Shi et al., 2023; Golchin and Surdeanu, 2023). MIAs can violate the confidentiality of sensitive training or reveal training on "forbidden" data, like copyrighted material or evaluation data (which undermines benchmark results).

MIA may be used for radioactivity detection, but with a strong limitation. Since it focuses on specific pieces of text, Alice in Fig. 1 has to record all the outputs of her LLM.

IP Protection.Watermarking can be used for intellectual property protection. For instance, He et al. (2022b, a), Li et al. (2023) use lexical properties like synonyms whereas Peng et al. (2023) rely on backdoors. Zhao et al. (2023) develop a watermark dissuading model theft via distillation. Yet its accuracy is empirical, it does not provide \(p\)-values that align with empirical false positive rates.

Radioactivity.Sablayrolles et al. (2020) introduce the concept of _radioactivity_: images are modified to leave a detectable trace in any classifier trained on them. Our work studies the radioactivity of decoding-based LLM watermarks, which are primarily used to detect AI-generated text with proven accuracy. We demonstrate that this form of radioactivity is reliably detectable across diverse settings. Appendix E.2 details why existing IP protections or MIAs do not offer similar capabilities.

### Technical background for LLM watermarking

This paper focuses on watermarking schemes that modify the LLM decoding by hashing a watermark window. In experiments we use (Aaronson and Kirchner, 2023; Kirchenbauer et al., 2023) due to their omnipresence in the literature, their performance, and their practicality. We briefly overview them and refer the reader to Appendix C for details (on the watermark detection tests especially).

We consider a decoder-only LLM that takes as input a context (a sequence of tokens \((x^{(-C)},...,x^{(-1)})^{C}\), \(\) being the vocabulary of the model) and outputs a vector of logits \(^{||}\). Vector \(\) is transformed into \(=()^{||}\), the probability distribution of the next token. The text is generated by sampling the next token \(x^{(0)}\) from this distribution with some procedure (top-k sampling (Fan et al., 2018; Radford et al., 2019), nucleus-sampling (Holtzman et al., 2019), etc.), then appending it to the context, and repeating the process.

The _watermark embedding_ alters the logit vector \(\) or the sampling procedure depending on a secret key. Usually, the output of a secret-key cryptographic function hashes \(k\) previous tokens \((x^{(-k)},,x^{(-1)})\) (the watermark window) and the secret-key \(\). It serves as a seed for a random number generator, that influences the choice of the next token \(x^{(0)}\). For instance, Kirchenbauer et al. (2023) create a pseudo-random "greenlist" of tokens with proportion \(\) of the entire vocabulary, whose logits are incremented by a quantity \(\), increasing their sampling probability.

The _watermark detection_ tokenizes a text, replays the seed generation and scores each token. The score function on a current token \(x^{(0)}\) may therefore be summed up as \(W_{}\) that also takes as input the watermark window \((x^{(-k)},,x^{(-1)})\), and depends on the hashing's secret-key \(\):

(1)

A statistical test is performed on the cumulative score \(S(X_{N})\) - which follows a known distribution in the absence of watermark, see App. C - where \(X_{N}:=[x_{1},,x_{N}]\) is a list of \(N\) tuples and:

\[S(X_{N}):=_{t=1}^{N}W_{}(x_{t}^{(0)};,(x_{t}^{(-i) })_{i=k}^{1}).\] (2)

## 3 Problem Formulation

_Alice_ owns a language model \(\), fine-tuned for specific tasks such as chatting, problem solving, or code generation, which is available through an API (Figure 1). _Bob_ owns another language model \(\). Alice suspects that Bob fine-tuned \(\) on some outputs from \(\). We denote by \(D\) the dataset used to fine-tune \(\), among which \(D^{} D\) is made of outputs from \(\), in proportion \(=|D^{}|/|D|\).

Access to Bob's data.We consider two settings for Alice's knowledge about Bob's training data:

* _supervised_: Bob queries \(\) and Alice retains all the content \(^{}\) that \(\) generated for Bob. Thus, Alice knows that \(D^{}^{}\). We define the _degree of supervision_\(d:=|D^{}|/|^{}|\),* _unsupervised_: Bob does not use any identifiable account or is hiding behind others such that \(|^{}||D^{}|\) and \(d 0\). This is the most realistic scenario.

Thus, \(\) is the proportion of Bob's fine-tuning data which originates from Alice's model while \(d\) quantifies Alice's knowledge regarding the dataset that Bob may have utilized (see Fig. 2).

Access to Bob's model.We consider two scenarios:

* Alice has an _open-model_ access to \(\). She can forward any inputs through \(\) and observe the output logits. This is the case if Bob open-sources \(\), or if Alice sought it via legitimate channels.
* Alice has a _closed-model_ access. She can only query \(\) through an API without logits access: Alice only observes the generated texts. This would be the case for most chatbots.

We then introduce two definitions of radioactivity:

**Definition 1** (Text Radioactivity).: _Dataset \(D\) is \(\)-radioactive for a statistical test \(T\) if "\(\) was not trained on \(D\)" \(_{0}\) and \(T\) is able to reject \(_{0}\) at a significance level (\(p\)-value) smaller than \(\)._

**Definition 2** (Model Radioactivity).: _Model \(\) is \(\)-radioactive for a statistical test \(T\) if "\(\) was not trained on outputs of \(\)" \(_{0}\) and \(T\) is able to reject \(_{0}\) at a significance level smaller than \(\)._

Thus, \(\) quantifies the radioactivity of a dataset or model. A low \(\), e.g. \(10^{-6}\), indicates strong radioactivity: the probability of observing a result as extreme as the one observed, assuming that Bob's model was not trained on Alice's outputs, is 1 out of one million. Conversely, \( 0.5\) means that the observed result is equally likely under both the null and alternative (radioactive) hypotheses.

## 4 Radioactivity Detection

We build radioactivity detectors for all settings as shown in Tab. 1. The outputs of \(\) are watermarked with a method \(W\) with Alice's secret key \(\) as described in Sec. 2.2. Note that, unlike watermark detection which takes text as input, the input for radioactivity detection is a model.

### Theoretical validity of statistical tests for radioactivity detection

In the following, we construct a radioactivity test on \(\) based on the detection score of Alice's watermarking method. We focus on Kirchenbauer et al. (2023a) with the notations of Sec. 2.2 for simplicity. Each \(x_{i}\) in \(X_{N}\) is a \((k+1)\)-tuple of tokens \((x_{i}^{(-k)},,x_{i}^{(0)})\) and \(x_{i}^{(0)}\) is generated by \(\) from the preceding tokens \((x_{i}^{(-C_{i})},,x_{i}^{(-k)},,x_{i}^{(-1)})\), where \((x_{i}^{(-C_{i})},,x_{i}^{(-k-1)}):=c_{i}\) is an optional context, e.g. prompt. Note that the contexts \((c_{1},,c_{N})\) are crafted by Alice to better detect radioactivity in \((x_{1},,x_{N})\). As we see in Sec. 5.3, this can imply that they are watermarked themselves. In this scenario, an accurate statistical test for detecting radioactivity can be performed through de-duplication. Let \(_{0}\) be "\(S(X_{N})\) follows a binomial distribution \(B(N,)\)". Then:

**Proposition 1**.: _"\(\) was not trained on Alice's watermarked data" \(_{0}\) if tokens are de-duplicated: (1) \((x_{i})_{i N}\) are pairwise distinct and (2) for each \(1 i N\), \(x_{i}\) is not in the context \(c_{i}\)._

The proof and a formal statement of this result are provided in App. D.2.1.

With this inclusion, we can thus use a statistical test \(T\) for hypothesis \(_{0}\) to test against radioactivity. Specifically, assuming conditions (1) and (2) are met, the \(p\)-value \(P(S(X_{N})>t_{0})\) can be

    &  &  &  \\   & Open & Closed & Open & Closed & Open & Closed \\ Supervised & ✓ & ✓ & ✓ & \(\) & ✓ & \(\) \\ Unsupervised & ✓ & ✓ & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Availability of radioactivity detection under the different settings. _Open / closed-model_ refers to the availability of Bob’s model, and _supervised / unsupervised_ to Alice’s knowledge of his data. Detection with watermarks is described in Sec. 4, and a baseline without WM relying on MIA in App. E.1. Intellectual Property Protection (IPP) refers to Zhao et al. (2023); see App. E.2.

accurately computed using the regularized incomplete beta function \(I_{}(t+1,N-t)\) detailed in App. C. The necessity of condition (1) to simulate i.i.d. scores is also used in classical watermark detection (Fernandez et al., 2023; Kirchenbauer et al., 2023a), but is more pronounced in our scenario, given the larger volume of tokens required to observe radioactivity. (2) however is not necessary in classical watermark detection, as the prompt used to generate text is assumed not to be watermarked. In our case, we show in Sec. 5 and App. D.2 that both are critical in order to get reliable \(p\)-values.

### Radioactivity detection in practice

Naive approach.Text radioactivity can be detected by running \(T\) on any text generated by \(\). However, this detector is weak because radioactivity can only be observed on an \(x_{i}\) if it was part of \(\)'s watermarked outputs in \(\)'s training data. This is because there is no correlation between the green lists of different watermark windows, since each partition is only a function of the watermark window through a hash function.

Overview.We use the detection test \(T\) detailed in Sec 4.1 on \(X_{N}\) generated through carefully crafted prompts, as our goal is to reduce noise by focusing on contexts likely to lead to radioactivity signals. To this end, we recreate contexts similar to the ones that generated the watermarked text by Alice. We ensure the accuracy of statistical tests through de-duplication of scored tokens.

Radioactivity detection in \(\).To amplify the signals of radioactivity, we employ two strategies. (1) In the supervised setting, we use watermarked text from \(^{}\). In the unsupervised setting, we use other watermarked text generated by \(\) that aligns with the suspected training distribution of \(\) (e.g., English dialogues), but that was not used by \(\). (2) We score up to millions of tokens, orders of magnitudes more than usual. The scoring depends on the access to \(\):

* _closed-model_: we use the prompts to generate new texts from \(\), and score these texts.
* _open-model_, "reading mode": instead of generating completions with \(\), we directly forward the text generated by \(\) through \(\), as depicted in Fig. 3. We then score next-token predictions with \(W_{}\) by using tokens from the input as watermark windows. Intuitively, for each green list token present in \(\)'s watermarked text, the context that led to its generation is reproduced. This allows Alice to focus her analysis on how \(\) responds to these specific contexts, which are more likely to lead to radioactivity signals than more generic ones.

Filter on scored \(k\)-grams.To further improve detection in the closed-model setting where the reading mode is not possible, we only score \((k+1)\)-tuples \(x_{i}\) output by \(\) for which the watermark window is often in \(\)'s watermarked outputs. We thus introduce a filter \(\), a set that contains these watermark windows. In the _supervised_ setting (\(0<d 1\)), \(\) is made of the \(k\)-grams present in \(^{}\) (refer to Fig. 2). In the _unsupervised_ setting, we focus on 'likely' contaminated \(k\)-grams, e.g., \(k\)-grams appearing in (new) watermarked text generated by \(\).

Figure 3: Radioactivity detection with closed or open model access (for simplicity, only (Kirchenbauer et al., 2023a) is illustrated). _(Left)_ New texts are generated from \(\) using prompts from \(\) and these texts are scored. The filter \(\) is used to focus the score computation on likely contaminated \(k\)-grams. _(Right)_ Text generated by \(\) are directly forwarded through \(\), and the next-token predictions are scored using tokens from the input as the watermark window. In both cases, the _tape_ ensures reliable \(p\)-values by de-duplicating scored tokens.

Token scoring and de-duplication.We score a token only if the same (\(k+1\))-tuple \(x_{i}\) has not been previously encountered. Moreover, in the closed-model setting, we only score watermark windows (\(k\)-tuple) that are not part of the (watermarked) prompt. In the open-model setting, tokens with watermarked windows previously present in the attention span are not scored. This is achieved by maintaining a _tape_ memory of all such \(k\)-grams combinations during detection. These adjustments ensure reliable \(p\)-values even when many tokens are analyzed. This is empirically validated in Sec. 5.4, and additional details on the correctness of our tests are provided in App. D.

## 5 Radioactivity in Instruction Datasets

This section considers a realistic scenario where a pre-trained LLM \(\) is instruction fine-tuned on instruction/answer pairs generated by \(\). It shows that watermarked instructions are radioactive and compares the confidence of our different detection methods.

### Experimental setup of the instruction tuning

Instruction data generation.We follow the Self-Instruct protocol (Wang et al., 2022) with \(\)=Llama-2-chat-7B (Touvron et al., 2023b) (results hold for bigger teachers, see App. F.3). We prompt the model with an instruction followed by three examples of instruction/answer pairs and ask it to generate the next \(20\) instruction/answer pairs. The sampling from the LLM logits is done with or without the watermarking method of Kirchenbauer et al. (2023a), at logit bias \(=3.0\), proportion of greenlist tokens \(=0.25\), and \(k=2\). In both cases, we use nucleus sampling (Holtzman et al., 2019) with \(p=0.95\) and \(T=0.8\). We post-process the generated data to remove unfinished answers and near-duplicate instructions. This yields a dataset of 100k instruction/answer pairs (\(\)14M tokens). Appendix F shows examples of these instructions and the watermark detection rates.

Finally, we create six mixed datasets with \(\) % of watermarked data (with \(\{0,1,5,10,50,100\}\)), filling the rest with non-watermarked instructions. Thus, the total number of instructions is fixed at around 14M tokens, but the proportion of watermarked ones (which represents \(\)'s outputs) varies.

Fine-tuning.We train \(\) on these six datasets, closely following the approach of Alpaca (Taori et al., 2023): we use AdamW (Loshchilov and Hutter, 2017a) for 3000 steps, with a batch size of 8, a learning rate of \(10^{-5}\) and a context size of 2048 tokens (which results in 3 training epochs). The learning rate follows a cosine annealing schedule (Loshchilov and Hutter, 2017b) with 100 warmup steps. We set \(\)=Llama-1-7B (Touvron et al., 2023a), a model trained on different datasets than \(\)=Llama-2, to avoid biases that could arise if the same base model were also used for fine-tuning.

### Quality inspection of the instruction tuning

Alice's watermarking hyperparameters aim at 1) generating high-quality instructions and 2) ensuring that the watermark can be detected even in small text segments: the watermark window size is \(k=2\), sufficiently wide to eliminate biases yet narrow enough to make the watermark robust to edits; \(=3\) yields high-quality text while ensuring that the watermark can be detected with a \(p\)-value of \(10^{-6}\) on approximately 100 tokens (full results in App. F.2).

We inspect the outputs of the fine-tuned model \(\) both qualitatively (see examples in Fig. 4 and App. F) and quantitatively in Tab. 2. We report 0-shot scores for an evaluation setup close to that of Llama: exact match score for Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017); 0-shot exact match score without majority voting for GSM8k (Cobbe et al., 2021); pass@1

    & NQ & TQA & GSM8k & H.Eval & Avg. & MMLU \\   \\ \(0\%\) & \(5.0\) & \(33.6\) & \(11.8\) & \(12.8\) & \(15.8\) & \(33.6\) \\ \(5\%\) & \(5.2\) & \(35.7\) & \(11.2\) & \(11.6\) & \(15.9\) & \(34.7\) \\ \(50\%\) & \(4.1\) & \(35.5\) & \(9.6\) & \(12.8\) & \(15.5\) & \(35.0\) \\ \(100\%\) & \(5.6\) & \(36.4\) & \(11.1\) & \(9.8\) & \(15.7\) & \(31.0\) \\  Base & \(3.2\) & \(36.2\) & \(10.5\) & \(12.8\) & \(15.7\) & \(28.4\) \\   

Table 2: Evaluation of Llama-7B fine-tuned with varying proportions of watermarked instruction data.

   Supervision degree \(d\) & \(_{10}(p)\) \\  \(0.1\%\) & \(-5.8 1.8\) \\ \(1\%\) & \(-6.5 0.9\) \\ \(5\%\) & \(-16.0 2.6\) \\ \(10\%\) & \(<-30\) \\   

Table 3: Detection confidence \(_{10}(p)\) with varying supervision \(d\) at \(=5\%\) of \(\)’s training data from \(\), in the _open_-model setting using the reading mode for detection (see Fig. 3).

for HumanEval (Chen et al., 2021); and accuracy on MMLU (Hendrycks et al., 2020). As expected, instruction-tuning does not affect most benchmarks while enhancing it for MMLU, as in (Dettmers et al., 2023). Thus, watermarking does not significantly impact the fine-tuned model performance.

### Experimental setup of the detection

In the **open-model** setting, we use a set of watermarked instructions generated by Alice's model \(\) to score \(N=225\)k tokens. For the supervised setting (\(d=1\)), we directly use all the \(\%\) watermarked texts among the 100k instructions used to train \(\); for the unsupervised setting (\(d=0\)), we use watermarked instructions unused by \(\). We use the "reading mode" for detection (see Sec. 4).

In the **closed-model** setting, \(\) is only accessible via an API. We prompt \(\) with instructions generated by \(\), concatenate all the answers, and score \(N=600\)k tokens, after filtering and de-duplicating the \(k\)-grams of \( 1.5\)M generated tokens. This represents around \(10^{4}\) queries if we assume an answer is \(100\) tokens. We score a token only if its previous \(k\)-gram is part of filter \(\). In the supervised setting (\(d>0\)), we collect the watermarked prompts/answers from \(^{}\), part of which were used for fine-tuning, and define \(\) as the set of all the \(k\)-grams present in it. In the unsupervised setting, we generate \(100\)k new watermarked instructions with \(\) and save all \(k\)-grams into \(\). In both cases, we run the detection \(10\) times on different chunks of text and report averaged results.

### Detection results

Proportion of watermarked data and access to Bob's model and data.Figure 5 first presents the \(p\)-values of our tests for different proportions \(\), under the 4 possible scenarios, and shows that the detection confidence increases with the proportion of watermarked data, and with Alice's knowledge.

The supervised setting (\(d=1\)) is straightforward: radioactivity is detected with a \(p\)-value smaller than \(10^{-30}\) (resp. \(10^{-10}\)) in the open-model (resp. closed-model) setting, even if only \(1\%\) of Bob's fine-tuning data originated from \(\). Indeed, with open-model access, we only score 1) \(k\)-grams that can actually be contaminated and 2) within a context that matches the one seen during fine-tuning. In the closed-model access, prompting \(\) with its training questions enhances radioactivity detection, as its answers are likely to reproduce the watermarked responses from \(\) used during training.

In the unsupervised setting (\(d=0\)), our open-model radioactivity detection test still yields \(p<10^{-5}\) when no more than \(5\%\) of the instructions used to fine-tune \(\) originate from Alice's model. The detection is done on a corpus of texts that does not contain samples seen by Bob at training time. However, it contains watermark windows that likely overlap with Bob's training data, on which radioactivity may be detected. In the closed-model setting, the detection is less powerful: it requires a higher proportion of watermarked data to be detected with a \(p\)-value smaller than \(10^{-5}\).

Influence of the degree of supervision.To study the intermediate regime of weak supervision, we fix \(=5\%\) in the open-model detection. We vary the supervision degree \(d\) by mixing the watermarked data used for fine-tuning with new watermarked data generated by \(\) in a ratio \(d\) to

Figure 4: Answers generated from Bob’s model \(\) (Llama-1), fine-tuned on instruction data generated by Alice’s model \(\) (Llama-2-chat) with different proportions \(\) of watermarked data. The quality of the instruction-tuning is not affected by the watermarking of the data (examples of training instruction/answer pairs are in Fig. 15).

simulate \(^{}\). We then run the radioactivity detection test with the reading mode on this corpus. Table 3 shows that the detection confidence increases with the supervision degree (from \(-5.8\) to \(<-30\) when \(d\) goes from \(0.1\%\) to \(100\%\)). Even for very weak supervision, the detection is still effective, with a \(p\)-value smaller than \(10^{-5}\) when \(d=0.1\%\). This is in stark contrast with MIA-based methods for which supervision is necessary (we detail this in Fig. 10 of App. E.1).

Influence of the filtering in the closed-model setting.Figure 6 compares detection with and without the \(\) filter when \(1\%\) of fine-tuning data is watermarked in the supervised closed-model setting (with \(d=1\)). We plot the \(_{10}(p)\) against the number of generated tokens. As expected, the detection confidence increases with the number of tokens. Moreover, filtering consistently brings improvements: after scoring \(75000\) tokens, the \(_{10}(p)\) equals \(-12\) with filter and \(-8\) without. Filtering appears particularly important to increase the detection confidence on the worst-case scenarios (see the largest \(p\)-value observed over the 10 runs in Fig. 12 of App. F).

Influence of the de-duplication on the correctness of radioactivity tests.For a statistical test to be valid, the \(p\)-value should be uniformly distributed between 0 and 1 under the null hypothesis \(_{0}\) (mean \(0.5\), standard deviation of \( 0.28\)). Accurate theoretical \(p\)-values are particularly important in the common case where obtaining samples of fine-tuned \(\) is expensive. This cost limits the sample size, reducing the power of empirical tests and compromising the confidence in their results.

We validate our tests and highlight the importance of de-duplication by observing the empirical \(p\)-value distribution after scoring 1M tokens when \(\) is not trained on watermarked data (\(_{0}\)). We first run 10 detection tests when scoring distinct {watermarked window + current token} combinations, as suggested by . This approach is insufficient on its own, as shown in Tab. 4 in the "without de-duplication" column. For instance, in the closed-model setting, the average \(p\)-value is inferior to \(10^{-30}\) even if the model does not output watermarked texts. This is a strong false alarm, i.e., incorrectly rejecting the null hypothesis. The additional de-duplication rules (Sec. 4) resolve this issue. We observe in the "with de-duplication" column that the empirical \(p\)-values match the theoretical ones, indicating that the test results are valid and reliable. These statistics, computed across various secret keys and seeds, are detailed in App. D.1. App. D.2 gives additional details, supported by additionnal experiments, on the importance of de-duplication.

### Intermediate summary & discussion

Our watermark-based radioactivity detection methods can identify \(10^{-5}\)-radioactivity in model \(\) across various setups. Even in the most realistic scenario - unsupervised access to Bob's data - our method holds true with only closed access to \(\), given that at least \(10\%\) of the data is watermarked. Open-model access further enhances the test's statistical significance, detecting radioactivity with a \(p\)-value smaller than \(10^{-10}\) when \(10\%\) of the fine-tuning data is watermarked (Fig. 5). Note that we deliberately score different numbers of tokens for the open and closed-model settings. It shows that

Figure 5: Radioactivity detection results. Average of \(_{10}(p)\) over 10 runs (\(\) is better). Bars indicate standard deviations. The detection methods are detailed in Sec. 4. In the supervised closed-model setting, our tests detect radioactivity (\(p<10^{-5}\)) when only \(1\%\) of training data is watermarked. In the absence of watermarked data, all tests output random \(p\)-values.

Figure 6: Influence of the filter on scored tokens. \(_{10}(p)\) as a function of the number of generated tokens in the supervised closed-model setting with \(=1\%\). We perform the watermark detection test on text generated by \(\) with prompts from \(^{}\). When filtering, we only score \(k\)-grams that were part of \(^{}\).

the open-model is more efficient since it requires fewer tokens to achieve lower \(p\)-values (see Fig. 5). Furthermore, since \(p\)-values plateau beyond a certain threshold (see Fig. 6), additional token scoring in the open-model setting does not significantly enhance detection.

**Other approaches.** The applicability of other approaches is summarized in Tab. 1: "\(\)" means that no method in the literature currently tackles this problem with LLMs, and "\(\)" means that methods that address the problem have strong technical issues (the statistical guarantees do not hold). Indeed, we show in App. E.1 that other passive methods like Membership Inference Attacks (MIAs) are effective only in the _supervised_ setting, where Alice has precise knowledge of the data used to train Bob's model and _open_ access to it. In that scenario, she can demonstrate \(10^{-30}\)-radioactivity, providing strong evidence that Bob has trained on her model. App. E.2 demonstrates that even state-of-the-art _active_ protection methods (Zhao et al., 2023) fall short in the unsupervised setting.

Besides, our tests provide reliable \(p\)-values thanks to meticulous de-duplication. When \(\) is not trained on watermarked data, the \(p\)-values are not overly low (Fig. 5, Tab. 4, App. D). App. E.2 shows that this is not the case for recent synonym replacement-based watermarks (He et al., 2022a,b).

## 6 Investigating Radioactivity

This section further studies what influences radioactivity from three angles: fine-tuning, watermarking algorithm, and data distribution. We also explore how Bob can try to remove radioactivity traces. Other scenarios such as multi-bit watermarking (Yoo et al., 2024) and other influencing factors, such as the size of model \(\) or mixes of different fine-tuning data, are studied in App. F.

### Fine-tuning

We first study the influence of fine-tuning on the same setup as Sec. 5, with regards to: (a) the learning rate, (b) the fine-tuning algorithm, (c) the number of epochs, (d) the model size. We fine-tune \(\) with the same dataset of \(=100\%\) watermarked instructions and the same parameters. We detect radioactivity in the _open-model / unsupervised_ setting. This is done on \(N=10\)k next-predictions, and where the texts that are fed to \(\) are watermarked instructions generated with \(\). Table 6 reports the results. The more the model fits the data, the easier its radioactivity is to detect. For instance, multiplying the learning rate by \(10\) almost doubles the average \(_{10}(p)\) of the test.

### Watermarking method & data distribution

To introduce more variety to the data under study, we now prompt \(\)=Llama-2-7B with the beginnings of Wikipedia articles in English and generate the next tokens with or without watermarking. We then fine-tune \(\)=Llama-1-7B on the natural prompts followed by the generated answers. The fine-tuning is done in 1000 steps, using batches \(8 2048\) tokens (similarly to Sec. 5). This section fine-tunes \(\) on \(=100\%\) English watermarked texts. We explore two aspects of the radioactivity.

Table 6: Influence of the model fine-tuning on the radioactivity. We report the \(_{10}(p)\) for \(10\)k scored observations (lower means more radioactive). Gray indicates values used in Sec. 5.

Table 4: Average \(p\)-values under \(_{0}\) (\(\)_not_ trained on watermarked data: \(p\) should be 0.5). In the open-model setting (resp. closed), we exclude a token if the same watermark window is already present in the attention span (resp. in the watermarked prompt). Without de-duplication, \(p\)-values are overly low: the test does not work.

Table 5: Influence of watermarking method and \(k\) on radioactivity. Average \(_{10}p\)-values. “Orig” denotes watermark detection of texts used for training (100 tokens); “Rad” denotes radioactivity detection in closed-model setting (\(N\)=30k, \(\)=\(100\%\)). Both KGW (Kirchenbauer et al., 2023b) and AK (Aaronson and Kirchner, 2023) behave the same way and lower \(k\) increases radioactivity.

Watermark window size.Table 5 highlights that the confidence of the detection decreases with \(k\) when fixing the \(p\)-value of the watermark detection of the training texts. There are two explanations. First, for lower \(k\), the chances that a (\(k+1\))-tuple repeats in the training data are higher, which increases its memorization. Second, the number of watermark windows is \(||^{k}\) and therefore increases with \(k\), while the number of watermarked tokens is fixed. Thus, at detection time, the proportion of radioactive tuples decreases with \(k\), diminishing the test's power. This experiment also demonstrates that the methods of Aaronson and Kirchner (2023) and Kirchenbauer et al. (2023) behave similarly.

Data distribution.We consider an unsupervised setting where Alice has no prior knowledge about \(D^{}\), the data generated with \(\) used to fine-tune \(\). As an example, Alice does not know the language of \(D^{}\), which could be Italian, French, Chinese, etc. We run the detection on text generated by \(\), with prompts from Wikipedia in different languages. The confidence of the test on another language - that might share very few \(k\)-grams with \(D^{}\) - can be low, as shown in Tab. 7.

Alice may, however, combine the \(p\)-values of each test with Fisher's method. This discriminates against \(_{0}\): "_none of the datasets are radioactive_", under which the statement "_Bob did not use any outputs of \(\)_" falls. Therefore, the test aligns with our definition of model radioactivity as per definition 2. From Tab. 7, Fisher's method gives a combined \(p\)-value of \(<10^{-50}\). Thus, even if Alice is unaware of the specific data distribution generated by \(\) that Bob may have used to train \(\) (e.g., problem-solving scenarios), she may still detect radioactivity by combining the significance levels.

### Possible defense: "Purification"

We investigate the impact of a second fine-tuning on human-generated data to remove the watermark traces. After having trained his model on a mix of watermarked and non-watermarked data as in Sec. 5, Bob fine-tunes his model a second time on text from OASST1 (Kopf et al., 2024), with the same fine-tuning setup.

Table 8 shows that the second-fine-tuning divides by \(2\) the significance level of the statistical test, but does not remove all traces. Other attempts to remove radioactivity could be to rephrase the watermarked instructions or use differentially private training. The logic is overall the same as previously pointed out in the fine-tuning ablations: if the original watermark is weaker or if the fine-tuning overfits less, then radioactivity will be weaker too. Therefore, the radioactivity detection will be less powerful, but given a sufficient amount of data, Alice may still be able to detect it.

## 7 Conclusion

This study formalizes the concept of "radioactivity" in language models. It introduces methods to detect traces that LLM-generated texts leave when used as training data. We show that this task is difficult for non-watermarked texts in the most realistic scenarios. Yet, watermarked texts exhibit significant radioactivity, contaminating models during fine-tuning. This makes it possible to identify with high confidence if outputs from a watermarked model have been used to fine-tune another one (although it may not be used to detect the use of the other model itself).