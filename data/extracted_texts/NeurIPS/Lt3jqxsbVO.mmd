# Sharp Spectral Rates for Koopman Operator Learning

Vladimir R. Kostic

Istituto Italiano di Tecnologia

University of Novi Sad

vladimir.kostic@iit.it

&Karim Lounici

CMAP-Ecole Polytechnique

karim.lounici@polytechnique.edu

&Pietro Novelli

Istituto Italiano di Tecnologia

pietro.novelli@iit.it

&Massimiliano Pontil

Istituto Italiano di Tecnologia

University College London

massimiliano.pontil@iit.it

###### Abstract

Nonlinear dynamical systems can be handily described by the associated Koopman operator, whose action evolves every observable of the system forward in time. Learning the Koopman operator and its spectral decomposition from data is enabled by a number of algorithms. In this work we present for the first time non-asymptotic learning bounds for the Koopman eigenvalues and eigenfunctions. We focus on time-reversal-invariant stochastic dynamical systems, including the important example of Langevin dynamics. We analyze two popular estimators: Extended Dynamic Mode Decomposition (EDMD) and Reduced Rank Regression (RRR). Our results critically hinge on novel minimax estimation bounds for the operator norm error, that may be of independent interest. Our spectral learning bounds are driven by the simultaneous control of the operator norm error and a novel metric distortion functional of the estimated eigenfunctions. The bounds indicates that both EDMD and RRR have similar variance, but EDMD suffers from a larger bias which might be detrimental to its learning rate. Our results shed new light on the emergence of spurious eigenvalues, an issue which is well known empirically. Numerical experiments illustrate the implications of the bounds in practice.

## 1 Introduction

Recently, researchers have emphasized the utmost importance of developing physically-informed machine learning models that prioritize interpretability and foster physical insight and intuition, see for example  and references therein. One technique highlighted in these works is the Koopman operator regression framework to learn and interpret nonlinear dynamical systems see, e.g.  and references therein. A key component of this approach is the Koopman Mode Decomposition (KMD), which decomposes complex dynamical systems into simpler, coherent structures. When ordinary least squares are used to learn Koopman operator from data, estimated KMD is known as the Dynamic Mode Decomposition (DMD) . Koopman operator estimators and their modal decomposition find many applications, including fluid dynamics, molecular kinetics and robotics .

The Koopman operator returns the expected value of observables of the system in the future given the present, and one relies on estimators of this operator to in turn estimate its spectral decomposition that leads to the estimation of KMD. Our goal is to study the statistical properties of the eigenvalues and eigenfunctions of the Koopman operator estimators via two mainstream algorithms: Principal Component Regression (PCR) and Reduced Rank Regression (RRR) studied in . PCR encompasses as particular cases the popular Extended Dynamic Mode Decomposition (EDMD), which is the de-facto estimator in the data-driven dynamical system literature [see 26, 45, and referencestherein]. Both PCR and RRR are kernel-based algorithms that, given a dataset of observations of the dynamical system, implement a strategy to approximate the action of the Koopman operator on a reproducing kernel Hilbert space (RKHS) [3; 38].

We present for the first time non-asymptotic learning bounds on the distance between the Koopman eigenvalues and eigenfunctions and those estimated by either PCR or RRR. We show that the eigenvalues produced by such algorithms are _biased_ estimators of the true Koopman eigenvalues, with PCR incurring larger bias. Our results critically hinge on novel estimation bounds for the operator norm error, that may be of independent interest, leading to minimax optimal bounds for finite-rank Koopman operators. Moreover, we introduce the novel notion of metric distortion, which characterize how the norm of eigenfunctions vary when moving from the RKHS in which learning takes place to the underlying ambient space where the Koopman operator is properly defined. We show that both the operator norm error and metric distortion are needed in order to estimate the operator spectra and our bounds can be used to explain the well-known spuriousness phenomena in eigenvalue estimation , namely, the scenario in which the estimated eigenvalues are not related to the true ones, despite small operator norm error.

**Contributions and Organization.** We make the following contributions: **i)** We introduce the notion of metric distortion and show that it has to be used alongside the operator norm error to derive Koopman spectra estimation error bounds (Theorem 1); **ii)** We establish the first sharp estimation bound for the operator norm error (Theorem 2); **iii)** We establish spectral learning rates (Thms. 3 and 4) for both PCR and RRR; **iv)** We propose how to use the results entailed by Theorem 4 to detect the presence of spurious eigenvalues from data.

The paper is organized as follows. In Section 2 we recall the notion of Koopman operator, its spectral decomposition, and review PCR and RRR estimators. Section 3 describes the estimation problem and outline our main results. Section 4 presents our approach to bound eigenvalue and eigenvector estimation errors. Section 5 gives sharp upper bounds for the operator norm error. Section 6 presents our spectral learning bounds. Finally, Section 7 illustrates the implications of the bounds in practice, and is designed to provide practitioners with the tools to benchmark the performance of algorithms in real scenarios.

## 2 Background

**Dynamical Systems and Koopman Operator.** In this work we study Markovian dynamical systems, that is collections of random variables \(\{X_{t} t\}\), where \(X_{t}\) represents the _state_ at time \(t\), taking values in some space \(\). We focus on time-homogeneous (i.e. autonomous) systems hosting an invariant measure \(\) for which the _Koopman operator_[24; 27]

\[(A_{}f)(x):=[f(X_{t+1})|X_{t}=x], x\] (1)

is a well defined bounded linear operator on \(L^{2}_{}()\), the space of square integrable functions on \(\) relative to measure \(\). In the field of stochastic processes, (1) is also known as the _transfer operator_ and returns the expected value of \(f\) in the future given the present. This operator is is self-adjoint (i.e. \(A_{}=A_{}^{*}\)) whenever dynamics is time-reversal invariant w.r.t. \(\), which is satisfied by many stochastic processes in the physical sciences.

**Example 1** (Langevin Dynamics).: _Let \(=^{d}\) and let \(>0\). The (overdamped) Langevin equation driven by a potential \(U:^{d}\) is given by \(dX_{t}=- U(X_{t})dt+}dW_{t}\), where \(W_{t}\) is a Wiener process. The invariant measure of this process is the Boltzman distribution \((dx) e^{- U(x)}dx\), and the associated Koopman operator is self-adjoint._

The Langevin equation models a wealth of phenomena, such as the evolution of chemical and biological systems at thermal equilibrium , the mechanism regulating cell size in bacteria , chemical reactions , the dynamics of synapses [11; 42], stock market fluctuations  and many more. Furthermore, when \(U(x)=\|x\|^{2}/2\) (\(>0\)), the Langevin equation reduces to the celebrated Ornstein-Uhlenbeck process [34; Chapter 6].

The operator (1) evolves every observable of the system forward in time. Since it is bounded and linear, it admits a _spectral decomposition_, which plays a central role in the analysis and interpretation of the dynamical system , as well as (nonlinear) control . As in , to study the spectral decomposition we further assume that \(A_{}\) is a _compact_ operator, which rules out the presence of continuous and residual spectrum components and leads to

\[A_{}=_{i}_{i}\,f_{i} f_{i},\] (2)

where \((_{i},f_{i})_{i} L^{2}_{}()\) are Koopman eigenpairs, i.e. \(A_{}f_{i}=_{i}\,f_{i}\). Moreover, \(_{i}_{i}=0\) and \(\{f_{i}\}_{i}\) form a complete orthonormal system of \(L^{2}_{}()\). In the context of molecular dynamics, the leading eigenvalues and their eigenfunctions are key in the study of long-term dynamics and so-called _meta-stable_ states (see, e.g., .

**Koopman Operator Regression in RKHS.** Throughout the paper we let \(\) be an RKHS and let \(k:\) be the associated kernel function. We let \(:\) be a _feature map_ such that \(k(x,x^{})=(x),(x^{})\) for all \(x,x^{}\). We consider RKHSs satisfying \( L^{2}_{}()\)[38, Chapter 4.3], so that PCR and RRR approximate \(A_{}:L^{2}_{}() L^{2}_{}()\) with an operator \(G:\). Notice that despite \( L^{2}_{}()\), the two spaces have different metric structures, that is for all \(f,g L^{2}_{}()\), one in general has \( f,g_{} f,g_{L^{2}_{}()}\). In order to handle this ambiguity, we introduce the _injection operator_\(S: L^{2}_{}()\) such that for all \(f\), the object \(Sf\) is the element of \(L^{2}_{}()\) which is pointwise equal to \(f\), but endowed with the appropriate \(L^{2}_{}()\) norm. With this in mind, the Koopman operator restricted to \(\) is simply \(A_{}S\), which is then estimated by \(SG\) for some \(G()\). We will measure the operator norm error, \(\|A_{}S-SG\|\). This is in contrast to the more frequently used Hilbert-Schmidt (HS) norm.

Koopman operator regression estimators are supervised learning algorithms to learn the Koopman operator, in which input and output data are consecutive states of the system \((X_{t},X_{t+1})\) for some \(t\). Since the Markov process is time-homogeneous and stationary, the joint probability distribution of \((X_{t},X_{t+1})\) is the same for every \(t\) and we denote it by \(\). Furthermore, stationarity also implies that \(X_{t}\) for all \(t\). Given a dataset1\(_{n}:=(x_{i},y_{i})_{i=1}^{n}\) of consecutive states, PCR and RRR are two different strategies to minimize, under a fixed-rank constraint, the mean square error

\[}(G):=_{i[n]}(y_{i})-G^{*} (x_{i})^{2},\] (3)

where \(G()\), the space of Hilbert-Schmidt operator acting on \(\). PCR and RRR estimators are expressed as functions of the _input_ and _cross_ empirical covariances, defined respectively as

\[=_{i[n]}(x_{i})(x_{i}),\ \ \ \ =_{i[n]}(x_{i})(y_{i}).\]

Likewise, the population risk is \((G)=_{(X,Y)}(Y)-G^{*}(X)^{2}\), and the population covariance and cross convariance are \(C=_{X}(X)(X)\), and \(T=_{(X,Y)}(X)(Y)\), respectively. We note that by the _reproducing kernel property_ one finds that \(C=S^{*}S\); see e.g. .

**Two Important Estimators.** We next briefly recall two operator regression estimators that we study in this paper. The _Principal Component Regression_ (PCR) estimator works by first projecting the input data into the \(r\)-dimensional principal subspace of the covariance matrix \(\), and then ordinary least squares are solved for such projected data, yielding the estimator (see e.g. 

\[^{}_{r,}=[^{-1}_{}]_{r}.\] (4)

Here \(_{}:=+ I_{}\) and \([\![]\!]_{r}\) denotes the \(r\)-truncated SVD. The population counterpart is \(G^{}_{r,}=[\![C^{-1}_{}]\!]_{r}T\), where \(C_{}:=C+ I_{}\). Note, however, that the empirical PCR estimator does _not_ minimize the empirical risk (3) under the low-rank constraint.

The _Reduced Rank Regression (RRR)_ algorithm, in contrast, is the _exact_ minimizer of (3) under fixed rank constraint. Specifically, RRR is defined as \(^{}_{r,}:=\{}(G)+ G^{2}_{ }:G_{r}()\}\), where the _regularization_ term \( G^{2}_{}\) is added to ensure stability, and \(_{r}()\) denotes the set of bounded operators on \(\) that have rank at most \(r\). The closed form solution of the empirical RRR estimator is 

\[^{}_{r,}=^{-1/2}_{}[\![^{-1/2}_{}]\!]_{r},\] (5)

while the population counterpart is given by \(G^{}_{r,}=C^{-1/2}_{}[\![C^{-1/2}_{}T]\!]_{r}\).

Once either the PCR or RRR estimators are fitted, their spectral decomposition is a proxy for the spectral decomposition of the Koopman operator \(A_{}\). Theorem 2 in  shows how such a decomposition can be calculated via the kernel trick for both \(^{}_{r,}\) and \(^{}_{r,}\).

## 3 The Problem and Main Result in a Nutshell

In this section we introduce the spectral estimation problem, outline our main results in a distilled form, and discuss some important implications. Recall the definition of Koopman operator (1) and its spectral decomposition (2). Given a rank \(r\) estimator \(\!\!_{r}()\) of \(A_{}\), we let \((_{i},_{i})_{i=1}^{r}\) be its spectral decomposition, satisfying \(_{i}\!=\!_{i}\,_{i}\). We aim to study how well a nonzero eigenvalue \(_{i}\) of \(\) estimates its _closest_ Koopman eigenvalue \(_{j(i)}\), where

\[j(i)=_{j}|_{i}-_{j}|.\] (6)

Moreover we wish to compare \(_{i}\) with the corresponding true Koopman eigenfunction. To this end, we embed \(_{i}\) in \(L^{2}_{}()\) by means of the operator \(S\) and define the normalized estimated eigenfunction

\[_{i}=S_{i}\,/\,\|S_{i}\|.\] (7)

One of the key quantities studied in this work is the eigenvalue estimation error \(|_{i}-_{j(i)}|,\;i[r]\). Recalling that \(A_{}\) is compact and self-adjoint, the classical Davis-Kahan result  implies that the eigenvalue estimation error \(|_{i}-_{j(i)}|\) also bounds the quality of the eigenfunction approximation as

\[\|_{i}-f_{j(i)}\|^{2}_{i}-_{j(i)}| }{[_{j(i)}(A_{})-|_{i}-_{j(i)}|]_{+}}\] (8)

where \(_{j}(A_{})\!\!=\!_{ j}\!|_{}\!-\!_{j}|\) is the distance between \(_{j}\) and its closest Koopman eigenvalue.

Let \(_{j}()\) denotes the \(j\)-th singular value of an operator. To give a flavour of our results, here we report spectral bounds for the Gaussian kernel. In this case, Theorem 3 below gives a high probability bound on the estimation error \(|_{i}-_{j(i)}|\), that is of order

\[\!((A_{}S)}{_{r}(A_{}S)}+ {1}{})\ \ \ _{r,}^{},\ \ \ \ ((S)}{[_{r}(A_{}S)-_{r+1}(S)]_ {+}}+})\ \ \ \ _{r,}^{}.\]

If the Koopman operator has finite rank then \(_{r+1}(A_{}S)\!=\!0\), the RRR estimator is unbiased, and its error goes to zero at the rate \(1/\). Otherwise, recalling that \(_{r+1}(S)\) is the square root of the \((r+1)\)-th eigenvalue of the kernel operator (38, Chapter 4.5), if \(\) is infinite dimensional \(_{r+1}(S)>0\), i.e. PCR has a strictly positive bias. In general, the presence of a bias in the estimated eigenvalues may result in the appearance of _spurious eigenvalues_. This phenomenon for PCR is well documented in practice, see e.g. [13; 14; 26; 28]. In Figure 1 we illustrate such an effect on a simple dynamical system discussed both in Example 3 and in Section 7.

## 4 Approach

The core of our analysis is Theorem 1. It reveals that in order to derive spectral estimation bounds for the Koopman operator, it is not enough to study the excess risk in the HS norm. Indeed, our spectral

Figure 1: PCR vs. RRR in estimating the largest eigenvalues of the 1D Ornsteinâ€“Uhlenbeck process with three different kernels over 50 independent trials. Vertical lines correspond to Koopman eigenvalues. The _good_ kernel is such that its \(\) corresponds to the leading eigenspace of the Koopman operator, while the other two are spans of scaled and permuted eigenfunctions for which the distortion with respect to the original metric structure of \(A_{}\) introduce slow (_bad_ kernel) and fast (_ugly_ kernel) spectral decay of the covariance.

bounds are determined by both the _operator norm error_ of the Koopman estimator

\[():=\|A_{}S-S\|,\ ( )\] (9)

and the _metric distortion_ between \(\) and \(L_{}^{2}()\),

\[(h):=\|h\|/\|Sh\|,\ \ h.\] (10)

Note that since \(Sh L_{}^{2}()\) is just an equivalence class of a function \(h\), \(\|Sh\|\) is simply \(L_{}^{2}()\)-norm of \(h\), and hence the metric distortion can be written, with a slight abuse of notation, as \((h):=\|h\|_{}/\|h\|_{L_{}^{2}()}\). While the (HS norm) error was studied before (see 29, and references therein), little is know about operator norm error bounds. Moreover, the metric distortion is, to the best of our knowledge, a novel quantity in the spectral analysis of Koopman operator.

**Theorem 1**.: _Let \(A_{}\) be a self-adjoint compact operator and let \(r\). Then, for every empirical estimator \(_{r}()\) and every \(i[r]\)_

\[|_{i}-_{j(i)}|(_{i})\,( ),\|_{i}-f_{j(i)}\|^{2}_{i})\,()}{[_{j(i)}(A_{ })-(_{i})\,()]_{+}}.\] (11)

Proof Sketch.: First, note that for compact self-adjoint operators \(|_{i}-_{j(i)}|\|(A_{}-_{i}\,I)^{ -1}\|^{-1}\). So, following the reasoning of [24, Theorem 1] and observing that \(\|(A_{}S-S)_{i}\|/\|S_{i}\| ()(_{i})\) gives the right hand side of the first equation in (11). Next, since additionally \(\|(A_{}S-S)_{i}\|/\|S_{i}\| ()(_{i})\), we can apply the Davis-Kahan spectral perturbation result for compact self-adjoint operators (Proposition 2, Appendix C) to bound \(()\), where \(_{i}:=(_{i},f_{j(i)})\). The claim then follows since \(\|_{i}-f_{j(i)}\|^{2} 2(1-(_{i})) 2( _{i})\). The full proof can be found in Appendix C. 

Note that the error (9), at least for universal kernels, can be made arbitrary small, see [24, Proposition 1]. Still, the metric distortion may dominate the error and, since the bound (11) is tight, one may have that the operator is well estimated in norm, but the estimated eigenpairs are far from the true ones. This phenomenon is at the origin of _spurious eigenvalues_. The proposed way to detect them for _deterministic_ systems in  is to check if eigenvalue equations are satisfied empirically, which, however, is not useful for _stochastic_ systems, see Rem. 4 of Appendix C.

Spuriousness may also originate from poor conditioning of the true eigenvalues, i.e. when the angle between true left and right eigenfunctions is small. Here, however, we assume \(A_{}=A_{}^{*}\), so that we restrict ourselves to the case in which the _only source of spuriousness is due to the learning method_.

While we defer the discussion of the operator norm error to the next section, the following result bound the metric distortion; the proof can be found in Appendix C.

**Proposition 1**.: _Let \(_{r}()\). For all \(i[r]\) the metric distortion of \(_{i}\) can be tightly bounded as_

\[1\ /\ \ (_{i})\ \ (|_{i}| (_{i}),\|\|)\ /_{}^{+}(S),\] (12)

_where \((_{i}):=\|_{i}\|\|_{i}\|/|_{i},_{i}|\) is the condition number of \(_{i}\), and \(_{i}\) is its left eigenfunction._

The upper bound (12) depends on the estimator's eigenvalues and their conditioning. Notice that while the true eigenvalues of \(A_{}\) have condition number one, the conditioning of the estimated ones depends on the choice of the kernel. Moreover, the upper bound can be controlled by tuning the estimator rank. Since the bound is tight (see Rem. 3 in Appendix C), the metric distortion can grow with the rank of the estimator, further motivating the use of low-rank estimators of \(A_{}\) in practice, see .

We end this section by introducing an empirical estimator of the metric distortion \((_{i})\), given by

\[_{i}:=\|_{i}\|\ /_{i},_{i}}.\] (13)

Proposition 4 of Appendix C shows that \(_{i}\) can be efficiently computed and report upper bounds for concentration around its mean. The empirical metric distortion (13), used in conjunction with the spectral bounds in Theorem 4 below, provides a proxy to assess the reliability of the PCR and RRR estimators and can be successfully used as novel model selection criterion.We refer the reader to the second and third experiment in Section 7 for concrete use-cases.

## 5 Controlling the Operator Norm Error

The HS norm error of the PCR estimator was already studied, either in the "well-specified" setting, i.e. when there exists \(G_{}()\) such that \(A_{}S=SG_{}\), i.e. \(G_{}\) is \(\)-a.e. Koopman operator [12, Theorem B.10]. On the other hand, KRR estimator is studied also in the "misspecified setting" . But, up to our knowledge, the operator norm error has not yet been studied. To analyse these learning rates, we make the following assumptions:

* _Regularity of_ \(A_{}\)_. For some_ \((0,2]\) _there exists_ \(a>0\) _such that_ \(TT^{*} a^{2}C^{1+}\)_;_
* _Boundedness._ _There exists_ \(c_{}>0\) _such that_ \(*{ess\,sup}_{x}(x)^{2} c_{}\)_, i.e._ \( L^{}_{}(,)\)_;_
* _Spectral Decay._ _There exists_ \((0,1]\) _and_ \(b>0\) _such that_ \(_{j}(C)\!\!b\,j^{-1/}\)_, for all_ \(j J\)_._

_While we keep assumptions (**BK**) and (**SD**) as in , assumption (**RC**) is, up to our knowledge, novel. The rationale behind it is that for \(=1\) **(RC**) is equivalent to \((A_{}S)(S)\), in which case there exists a bounded \(\)-a.e. Koopman operator \(G_{}\). On the other hand, as \( 0\) **(RC)** becomes closer to \((A_{}S)((S))\) which is always satisfied for universal kernels since \(((S))=L^{2}_{}()\)[38, Chapter 4]. Importantly, as the next example shows, **(RC)** is weaker condition than the usual regularity conditions; see Appendix D.1 for a detailed discussion._

**Example 2**.: _Let \(X\) be an \(\)-valued random variable with law \(\). Consider the Markov chain \((X_{t})_{t}\) such that \(X_{t}=X\) for all \(t\). Then \(\) is an invariant measure and \(A_{}=I_{L^{2}_{}()}\) is the identity map on \(L^{2}_{}()\). Clearly, (**RC**) holds for all \((0,1]\). On the other hand, since \(A_{}S=SG_{}\) for bounded operator \(G_{}=I_{}()\), HS-norm learning rates derived in  do not apply._

In order to study the error of any empirical finite rank estimator \(\) we rely on the error decomposition

\[()S-SG_{}}_{ }+-G)}_{}+)}_{},\] (14)

where \(G_{}:=C_{}^{-1}T\) is the minimizer of the full (i.e. without rank constraint), Tikhonov regularized, HS norm error, and \(G\) is the population version of the empirical estimator \(\).

While the last two terms in the r.h.s. of (14) depend of the estimator of choice, the first term depends only on the choice of \(\) and the regularity of \(A_{}\) w.r.t. \(\). In this work we focus on the classical kernel-based learning of the Koopman operator , where one chooses a universal kernel [38, Chapter 4] for which \((A_{}S)((S))\), and controls the regularization bias with a regularity condition. For details see Rem. 7 of Appendix D.2.

The second source of bias and the estimator's variance in our error decomposition depends on the choice of the low rank estimator. While throughout this section we consider **(RC)** for \(\), we discuss extensions of our results to \(<1\) in Appendix D.5.

**Theorem 2**.: _Assume the operator \(A_{}\) satisfies \(_{r}(A_{}S)>_{r+1}(A_{}S) 0\) for some \(r\). Let_ **(SD)** _and_ **(RC)** _hold for some \((0,1]\) and \(\), respectively, and let \(((S))=L^{2}_{}()\). Let_

\[ n^{-}\;\;\;\;^{*}_{n}  n^{-}.\] (15)

_Let \((0,1)\). Then, there exists a constant \(c>0\), depending only on \(\), such that for large enough \(n r\), with probability at least \(1-\) in the i.i.d. draw of \(_{n}\) from \(\)_

\[()\{ &_{r+1}(A_{}S)\!+\!c\, ^{*}_{n}\,^{-1}&\;\;= ^{}_{r,},\\ &_{r+1}(S)+c\,^{*}_{n}\,^{-1}& \;\;=^{}_{r,}\; \;_{r}(S)>_{r+1}(S)..\] (16a)

Proof Sketch.: The regularization bias is bounded by \(a\,^{}\) by Proposition 5 of Appendix D.2. For the RRR estimator, the _rank reduction bias_ is upper bounded by \(_{r+1}(A_{}S)\), while for PCR by \(_{r+1}(S)\). The bounds on the variance terms critically rely on the well-known perturbation result for spectral projectors reported in Proposition 3, Appendix A. This result is then chained to two versions of the Bernstein inequality in separable Hilbert spaces. The first one is Pinelis-Sakhanenko's inequality and the second is Minsker's inequality extended to self-adjoint HS-operators, Props. 9 and 11 in Appendix D.3.1, respectively. These inequalities provide high probability bounds for the normsof \(C_{}^{-1/2}(-C)\) and \(C_{}^{-1/2}(-C)C_{}^{-1/2}\), as well as \(C_{}^{-1/2}(-T)\) and \(C_{}^{-1/2}(-T)C_{}^{-1/2}\). Combining the bias due to regularization and variance terms, for both estimators we obtain the balancing equation \(^{}=^{-}\,n^{-}\,^{-1}\), which yields the optimal choice of \(\) and the rates. 

We stress that the number of samples in the previous theorem depends on the problem's complexity, expressed in the constants \(c_{}=}^{2}(A_{}S)-_{r+1}^{2} (A_{}S)},\,\) and \(c_{}=(S)-_{r+1}(S)}.\) Namely, the better the separation of singular values, the smaller number of needed samples. Furthermore, analyzing the bounds (16a) and (16b), we see that faster spectral decay is, in general, preferable. For example, for the Gaussian kernel \(\) can be chosen arbitrarily small, yielding the rate \(n^{-1/2}\). On the other hand, kernels with slow spectral decay for which \(=1\) can give slower rates between \(n^{-1/4}\) and \(n^{-1/3}\). Finally, from the variance bounds for RRR and PCR, c.f. Appendix D.3, one can specify constants. Namely, in the slower regime when \(_{n}^{}>n^{-1/2}\) we have that \(c=a+7.2(10)}}(1+ac_{}^{(-1)/2})( }}}{})\), while in fastest regime \(_{n}^{}=n^{-1/2}\), there is a significant difference between RRR and PCR since \(c\) should be multiplied with the constants \(c_{}\) and \(c_{}\), respectively.

As argued in Section 3, the bound (16a) indicates that for rank \(r\) Koopman operators the error converges to zero w.r.t. the number of training samples, while the bias of PCR is strictly positive. Hence, in order to ensure small error for PCR, high values of the rank parameter might be necessary. To theoretically explain this effect, in Theorem 6 of Appendix D.4 we give also lower bounds of operator norm error for the RRR and PCR estimators showing that \((_{r,}^{})\) always concentrates around \(_{r+1}(A_{}S)\), while the concentration of \((_{r,}^{})\) around \(_{r+1}(S)\) depends on the _irreducible risk_ of the learning problem. To illustrate the tightness of the error concentration bounds we present Example 3 (see also Appendix D.4).

**Example 3**.: _Let \(=\). Consider the 1D equidistant sampling of the Ornstein-Uhlenbeck process, obtained by integrating the Langevin equation of Example 1 with \(=1\) and \(U(x)=x^{2}/2\), given by \(X_{t}=e^{-1}\,X_{t-1}+}\,_{t},\) where \(\{_{t}\}_{t 1}\) are i.i.d. standard Gaussians. For this process it is well-known  that \(\) is \((0,1)\) and that \(A_{}\) admits a spectral decomposition \((_{i},f_{i})_{i}\) in terms of Hermite polynomials. We study the family of kernel functions \(k_{,}(x,x^{}):=_{i}_{(i)}^{2}f_{i}(x)f_{ i}(x^{})\), where \(\) is a permutation of the indices of the eigenvalues and \(\) is a scaling factor. The rationale behind this class of kernels is that by varying \(\) and \(\) one morphs the original metric structure of \(A_{}\) in a way which is harder and harder to revert when learning from finite sets of data. In particular, for any target rank \(r\), setting \(:=1/r^{2}\) and \(\) to the permutation such that \(i 2r-i+1\) (\(i r\)), \(i i-r\) (\(r+1 i 2r\)) and \(i i\) elsewhere, elementary algebra and our concentration bounds give_

\[|(_{r,}^{})-e^{-1/r}| n^{-1/2} ^{-1},|(_{r,}^{})-e^{- r}| n^{-1/2}^{-1}.\]

_We refer the reader to Figure 1 and to Section 7 for a numerical implementation of this example._

We conclude this section with remarks on the tightness of our statistical analysis of operator norm error. Since discussed results are not the main focus of the paper, we present them in Appendix D.5.

**Remark 1** (Lower bound).: _The rate \(_{n}^{}=n^{-}\) guaranteed by (16a) matches the minimax lower bound for the operator norm error when learning finite rank \(A_{}\). Formal statement and its proof is given in Theorem 7 of Appendix D.5._

**Remark 2** (Extension to misspecified setting).: _The optimal rates for HS-norm error of the KRR estimator are developed in  under a stronger condition than_ **(RC)**_. In Theorem 9 of Appendix D.6 we extended this analysis to PCR and RRR estimators, deriving the optimal operator norm rates that also cover cases when Koopman operator cannot be properly defined as bounded operator on the chosen RKHS space \(\)._

## 6 Spectral Learning Rates

Collecting all the previous results, we are now ready to present our spectral learning rates for the two estimators in a general form. For brevity, we focus on two different type of bounds in which (i) we analyse the uniform bound for the whole estimated spectra, and (ii) we express the estimators' bias in _empirical form_ to provide an insight into spuriousness of eigenvalues. Moreover, we present only eigenvalue estimation bounds, noting that the eigenfunction estimation bounds readily follow from (8). The complete results are presented in detail in Appendix E.

**Theorem 3**.: _Let \(A_{}\) be a compact self-adjoint operator. Under the assumptions of Theorem 2, there exists a constant \(c>0\), depending only on \(\), such that for every \((0,1)\), for every large enough \(n r\) and every \(i[r]\) with probability at least \(1-\) in the i.i.d. draw of \(_{n}\) from \(\)_

\[|_{i}-_{j(i)}|\{(A_{}S)}{_{r}(A_{}S)}+c\,_{n}^{}^{- 1}&\ \ =_{r,}^{},\\ (S)}{[_{r}(A_{}S)-_{r+1}^{}(S))_{+}}+c \,_{n}^{}^{-1}&\ \ =_{r,}^{}..\] (17)

The uniform eigenvalue learning rates for RRR and PCR estimators, differ in the estimator's bias. While the PCR bias has a factor \(_{r+1}(S)\) in the numerator, RRR has \(_{r+1}(A_{}S)_{r+1}(S)\). The striking difference happens when the Koopman operator is of finite rank. Then, assuming that \(r\) is properly chosen, RRR estimator has no bias, and \(c_{}\) is typically moderate. On the other hand, PCR's bias can be potentially large, depending of the choice of the kernel, and choosing higher rank increases \(c_{}\), thus requiring larger sample sizes. Therefore, even in well-conditioned problems (self-adjoint operator) the _spurious eigenvalues may arise purely from the learning method_. To facilitate detection of such occurrences, we further provide an empirical estimator of the bias of both methods and illustrate their use experimentally in Section 7.

**Theorem 4**.: _Under the assumptions of Theorems 2 and 3, there exists a constant \(c>0\), depending only on \(\), such that for large enough \(n r\) and every \(i[r]\) with probability at least \(1-\) in the i.i.d. draw of \(_{n}\) from \(\)_

\[|_{i}-_{j(i)}|\{_{i}\,_{r+1}(^{-1/2})+c\,_{n}^{} \,^{-1},&=_{r,}^{},\\ _{i}\,()}+c\,_{n}^{ }\,^{-1},&=_{r,}^{}. .\] (18)

We remark that when \(A_{}\) is of finite rank \(r\), the bound above for the RRR estimator reduces to

\[|_{i}-_{i}| c\,_{n}^{}\,^{- 1}\ \ \ \ \|_{i}-f_{i}\|^{2}^{}\, ^{-1}}{[_{i}(_{r,}^{})-3\,c\, _{n}^{}\,^{-1}]_{+}},\]

see Cor. 1 in Appendix E. Hence, in this case RRR algorithm can learn all the eigenvalues and eigenfunctions of \(A_{}\) with rate \(_{n}^{}=n^{-}\). On the other hand, even in this case, the bounds for the PCR estimator do not guarantee unbiased estimation of Koopman eigenvalues and eigenfunctions.

**Choosing \(\) and \(r\).** The bias term \(_{r+1}(A_{}S)/_{r}(A_{}S)\) appearing in (17) represents the theoretical limit when estimating eigenvalues using RRR. It reflects the capacity of the RKHS to detect the separation of the leading \(r\) Koopman eigenvalues from the rest of its spectra. If \(A_{}\) has infinite rank and slowly decaying eigenvalues, estimating the leading ones becomes challenging, since increasing \(r\) leads to smaller operator norm error, but larger bias. Luckily, in many practical problems there is a separation of time-scales in the dynamics and the above ratio can be controlled by choosing \(r\) appropriately. While we do not have access to \(A_{}S\), we can still choose \(r\) via the empirical operator \(^{-1/2}\), see Proposition 20 of Appendix D.4. Note also that the optimal \(\) depends on \(\) which is typically unknown. In practice, one can implement a standard grid-search CV procedure for time series to tune this parameter.

**Spectral Bias as a Tool for Model Selection.** In equation (18), the data dependent quantities \(_{i}(_{r,}^{}):=_{i}\, _{r+1}(^{-1/2})\) and \(_{i}(_{r,}^{}):=_{i}\, _{r+1}()\) represent the _empirical spectral biases_ of RRR and PCR estimators of the Koopman operator, respectively. When they are small enough, the spectral estimation error is dominated by the same variance term, which decreases as the number of samples grows. Therefore, given a number of different kernels, we propose to select the best one (w.r.t. spectral estimation) by choosing the smallest spectral bias. This is illustrated in the Alenine Dipeptide example of following section.

**Normal operators.** Since Davis-Kahan theorem  also holds for normal operators, the results in this section apply whenever \(A_{}A_{}^{}=A_{}^{}A_{}\). While in this case Koopman eigenfunctions remain orthogonal in \(L_{}^{2}()\), the eigenvalues are in general complex. On the other hand, extension beyond normal compact operators asks for involved spectral perturbation analysis and a new statistical learning theory.

## 7 Experiments

We illustrate various aspects of our theory with simple experiments. They have been implemented in Python using the library Kooplearn (available at https://github.com/CSML-IIT-UCL/kooplearn) to fit the PCR and RRR estimators. Full details are in Appendix F.

**Learning the Spectrum of the Ornstein-Uhlenbeck Process.** In this experiment we designed three different kernel functions (the "_good_", the "_bad_" and the "_ugly_") to illustrate how an unseemly kernel choice can induce catastrophic biases in the estimation of Koopman eigenvalues. We focus on the uniformly sampled Ornstein-Uhlenbeck (OU) process, discussed in Example 3, relying on the spectral decomposition of its Koopman operator \((_{i},f_{i})_{i}\) to design the three kernel functions. The _good_ kernel is just the sum of the leading \(T=53\) terms of the spectral decomposition of \(A_{}\), i.e. \(k_{}(x,y):=_{i=1}^{T}_{i}f_{i}(x)f_{i}(y)\). The associated RKHS coincides with the leading eigenspace of \(A_{}\), and _no deformation of the metric structure_ takes place, so that the injection map \(S L_{}^{2}()\) is a partial isometry. The _bad_ kernel is defined according to the construction presented in Example 3 for \(=1/r^{2}\) where \(r\) is the rank of the estimator. For this kernel, the introduced bias is innocuous for RRR, but lethal for PCR. Finally, the _ugly_ kernel corresponds to \(=r^{2}\), introducing large quotients \(_{r+1}(A_{}S)/_{r}(A_{}S)\) and \(_{r+1}(S)/_{r}(S)\), and, hence, an irreparable bias in both estimators.

Figure 1 depicts the distribution of the eigenvalues estimated by PCR and RRR over 50 independent simulations, against the ground truth. For both algorithms each simulation is comprised of \(20000\) training points, the regularization is \(=10^{-4}\) and the rank is \(r=3\). The three largest eigenvalues of \(A_{}\) are correctly estimated by both algorithms for \(k_{}\) and by RRR for \(k_{}\). On the contrary, the distribution of the eigenvalues for \(k_{}\) (and \(k_{}\) for PCR) does not concentrate around any true eigenvalue of \(A_{}\), signaling the presence of _spurious eigenvalues_ in the estimation.

Figure 3: Forecasting RMSE on the Alanine Dipeptide dataset for 19 different RRR estimators, each corresponding to a different kernel, which show how the best model, according to the empirical spectral bias metric, also attains the best forecasting performances by a large margin.

Figure 2: Estimated eigenfunctions \(_{i}\) of a Langevin dynamics vs. ground truth. The average empirical biases \(_{i}\), \(i\) are discussed at the end of Section 6. The results correspond to 50 independent estimations on 2000 training points each. PCR and RRR estimators were fitted with the same parameters: Gaussian kernel of length scale \(0.175\), \(=10^{-5}\) and \(r=4\).

A Realistic Example: Langevin DynamicsBecause of its ubiquitous use in modelling real systems, we now study a numerical implementation of the Langevin dynamics Example 1 with \(=1\) and a potential \(U(x)=4(x^{8}+0.8e^{-80x^{2}}+0.2e^{-80(x-0.5)^{2}}+0.5e^{-40(x+0.5)^{2}})\) that is a mixture of three Gaussians barriers at \(x\{-0.5,0,0.5\}\) and a smooth "bounding" term \( x^{8}\) constraining most of the equilibrium distribution in the interval \([-1,1]\), see . In Figure 2, for \(i[r]\), we compare the \(_{i}\) estimated by PCR and RRR against the ground truth \(f_{i}\). The visible difficulty of PCR compared to RRR in estimating eigenfunctions is nicely explained by larger values of the empirical bias for PCR, which we report in the upper part of the figure. The reference eigenpairs of \(A_{}\) have been obtained by diagonalizing a finely discretized approximation of the infinitesimal generator (see Appendix A).

**Spectral Bias and Model Selection: the Case of Alanine Dipeptide.** In this example we show that minimizing the first term on the r.h.s. of (18) over a validation dataset, is also a good criterion for Koopman model selection. We use a realistic simulation of the small molecule Alanine Dipeptide already discussed in [24; 44]. We trained 19 RRR estimators each corresponding to a different kernel and then we evaluated the forecasting RMSE on 2000 initial conditions drawn from a test dataset. In Figure 3 we report these errors, highlighting the model with the smallest average empirical spectral bias (18) evaluated on 5000 validation points.

## 8 Conclusion

We established minimax optimal rates for the operator norm error in the Koopman regression problem, which we then used to derive sharp estimation bounds for eigenvalues and eigenfunctions of the Koopman operator associated with a time-invariant Markov chain. We considered two important estimators that implement either principal component regression (PCR) or reduced rank regression (RRR) to learn a linear operator on a reproducing kernel Hilbert space. Our bounds indicate that RRR may be advantageous over PCR (also known as EDMD, the de-facto estimator in the data-driven dynamical system literature) which may exhibit a larger estimation bias. This ultimately depends on the choice of the kernel, which significantly impacts the rate. A bad choice of the kernel could also introduce spurious eigenvalues, a phenomena which has been observed in the literature and which is now explained by our theory. Finally, we proposed a method to detect spuriousness in practice, which can be used also as a kernel selection tool. A limitation of this work is that it applies to compact normal operators only. While many real dynamical systems involve such operators, in the future our analysis may be extended using more sophisticated spectral perturbation theory.

Acknowledgements.This work was supported in part from the PNRR MUR Project PE000013 CUP J53C22003010006 "Future Artificial Intelligence Research (FAIR)", funded by the European Union - NextGenerationEU, and EU Project ELIAS under grant agreement No. 101120237.