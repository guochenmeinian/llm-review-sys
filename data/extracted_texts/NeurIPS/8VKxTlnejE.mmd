# MambaAD: Exploring State Space Models for Multi-class Unsupervised Anomaly Detection

Haoyang He\({}^{1}\)1  Yuhu Bai\({}^{1}\)1  Jiangning Zhang\({}^{2}\)2  Qingdong He\({}^{2}\)  Hongxu Chen\({}^{1}\)

Zhenye Gan\({}^{2}\)  Chengjie Wang\({}^{2}\)  Xiangtai Li\({}^{3}\)  Guanzhong Tian\({}^{1}\)  Lei Xie\({}^{1}\)2

\({}^{1}\)Zhejiang University \({}^{2}\)Youtu Lab, Tencent \({}^{3}\)Nanyang Technological University

{haoyanghe,yhgui,186368,chenhongxu,gztian}@zju.edu.cn,

{yingcaihe,wingzygan,jasoncjwang}@tencent.com,

xiangtai94@gmail.com, leix@iipc.zju.edu.cn

Equal contributions.Corresponding author.

###### Abstract

Recent advancements in anomaly detection have seen the efficacy of CNN- and transformer-based approaches. However, CNNs struggle with long-range dependencies, while transformers are burdened by quadratic computational complexity. Mamba-based models, with their superior long-range modeling and linear efficiency, have garnered substantial attention. This study pioneers the application of Mamba to multi-class unsupervised anomaly detection, presenting MambaAD, which consists of a pre-trained encoder and a Mamba decoder featuring (Locality-Enhanced State Space) LSS modules at multi-scales. The proposed LSS module, integrating parallel cascaded (Hybrid State Space) HSS blocks and multi-kernel convolutions operations, effectively captures both long-range and local information. The HSS block, utilizing (Hybrid Scanning) HS encoders, encodes feature maps into five scanning methods and eight directions, thereby strengthening global connections through the (State Space Model) SSM. The use of Hilbert scanning and eight directions significantly improves feature sequence modeling. Comprehensive experiments on six diverse anomaly detection datasets and seven metrics demonstrate state-of-the-art performance, substantiating the method's effectiveness. The code and models are available at https://lewandofskee.github.io/projects/MambaAD.

## 1 Introduction

The advent of smart manufacturing has markedly increased the importance of industrial visual Anomaly Detection (AD) in production processes. This technology promises to enhance efficiency, diminish the costs of manual inspections, and elevate product quality along with the stability of production lines. Presently, most methods predominantly utilize a single-class setting [12; 30; 55], where a separate model is trained and tested for each class, leading to considerable increases in training and memory usage. Despite recent progress in introducing multi-class AD techniques [47; 18], there is still significant potential for advancement in terms of both accuracy and efficiency trade-off.

The current unsupervised anomaly detection algorithms can be broadly categorized into three approaches [50; 9; 51]: _Embedding-based_[36; 11; 4; 12; 8], _Synthesizing-based_[48; 24; 55; 30], and _Reconstruction-based_[27; 18; 6]. Despite the promising results of both Synthesizing and Embedding-based methods in AD, these approaches often require extensive design and inflexible frameworks. Reconstruction-based methods, such as RD4AD  and UniAD , exhibit superior performanceand better scalability. RD4AD, as depicted in Fig. 1 (a), employs a pre-trained teacher-student model, comparing anomalies across multi-scale feature levels. While CNN-based RD4AD captures local context effectively, _it lacks the ability to establish long-range dependencies_. UniAD, the first multi-class AD algorithm, relies on a pre-trained encoder and transformer decoder architecture as illustrated in Fig. 1 (b). Despite their superior global modeling capabilities, _transformers are hampered by quadratic computational complexity, which confines UniAD to anomaly detection on the smallest feature maps, potentially impacting its performance_.

Recently, Mamboa  has demonstrated exceptional performance in large language models, offering significantly lower linear complexity compared to transformers while maintaining comparable effectiveness. Numerous recent studies have incorporated Mamboa into the visual domain, sparking a surge of research [29; 57; 40; 22; 37; 43]. This paper pioneers the application of Mamboa into the anomaly detection area, introducing MamboAD, as illustrated in Fig. 1 (c). _MamboaAD combines global and local modeling capabilities, leveraging its linear complexity to compute anomaly maps across multiple scales_. Notably, it boasts a lower parameter count and computational demand, making it well-suited for practical applications.

Specifically, MamboAD employs a pyramid-structured auto-encoder to reconstruct multi-scale features, utilizing a pre-trained encoder and a novel decoder based on the Mamboa architecture. This Mamboa-based decoder consists of Locality-Enhanced State Space (LSS) modules at varying scales and quantities. Each LSS module comprises two components: a series of Hybrid State Space (HSS) blocks for global information capture and parallel multi-kernel convolution operations for establishing local connections. The resulting output features integrate the global modeling capabilities of the mamboa structure with the local correlation strengths of CNNs. The proposed HSS module investigates five distinct scanning methods and eight scanning directions, with the (Hybrid Scanning) HS encoder and decoder encoding and decoding feature maps into sequences of various scanning methods and directions, respectively. The HSS module enhances the global receptive field across multiple directions, and its use of the Hilbert scanning method [19; 23] is particularly suited to the central concentration of industrial product features. By computing and summing anomaly maps across different feature map scales, MamboAD achieves SoTA performance on several representative AD datasets with seven different metrics for both image- and pixel-level while maintaining a low model parameter count and computational complexity. Our contributions are as follows:

* We introduce MamboAD, which innovatively applies the Mamboa framework to address multi-class unsupervised anomaly detection tasks. This approach enables multi-scale training and inference with minimal model parameters and computational complexity.
* We design a Locality-Enhanced State Space (LSS) module, comprising cascaded Mamboa-based blocks and parallel multi-kernel convolutions, extracts both global feature correlations and local information associations, achieving a unified model of global and local patterns.
* We have explored a Hybrid State Space (HSS) block, encompassing five methods and eight multi-directional scans, to enhance the global modeling capabilities for complex anomaly detection images across various categories and morphologies.
* We demonstrate the superiority and efficiency of MamboAD in multi-class anomaly detection tasks, achieving SoTA results on _six_ distinct AD datasets with _seven_ metrics while maintaining remarkably low model parameters and computational complexity.

## 2 Related Work

### Unsupervised Anomaly Detection

**Unsupervised Anomaly Detection.** Existing AD methods can be mainly categorized into three types: _1) Embedding-based methods_ focus on encoding RGB images into multi-channel features [36; 11; 4; 12; 10; 38]. These methods typically employ networks per-trained on ImageNet , PatchCore  extracts nominal patch features with a memory bank for measuring the Mahalanobis distance.  is based on a student-teacher framework where student networks are trained to regress the output of a teacher network. However, the datasets used for these pre-trained models have a significant distribution gap compared to industrial images. _2) Synthesizing-based methods_ synthesize anomalies on normal images [48; 24; 39; 20]. The pseudo-anomalies in DREAM  are generatedutilizing Perlin noise and texture images. DREAM, taking anomaly mask as output, consists of a reconstruction network and a discriminative network. Despite the decent performance of such methods, the synthesized anomalies still have a certain gap compared to real-world anomalies. _3) Reconstruction-based methods_ typically focus on self-training encoders and decoders to reconstruct images, reducing reliance on pre-trained models. Autoencoder , Transformers, Generative Adversarial Networks (GANs)  and diffusion models  can serve as the backbone for reconstruction networks in anomaly detection. While the model's generalization can occasionally lead to inaccuracies in pinpointing anomalous areas.

**Multi-class Anomaly Detection.** Most current works are trained individually on separate categories, which leads to increased time and memory consumption as the number of categories grows, and they are uncongenial to situations with large intra-class diversity. Recently, to address these issues, multi-class unsupervised anomaly detection (MUAD) methods have attracted a lot of interest. UniAD  firstly crafts a unified reconstruction framework for anomaly detection. DiAD  investigates an anomaly detection framework based on diffusion models, introducing a semantic-guided network to ensure the consistency of reconstructed image semantics.ViTAD  further explores the effectiveness of vanilla Vision Transformer (ViT) on multi-class anomaly detection.

### State Space Models

State space models (SSMs)  have gained considerable attention due to their efficacy in handling long language sequence modeling. Specifically, structure state-space sequence (S4)  efficiently models long-range dependencies (LRDs) through parameterization with a diagonal structure, addressing computational bottlenecks encountered in previous works. Building upon S4, numerous models have been proposed, including S5 , H3 , and notably, Mamba . Mamba introduces a data-dependent selection mechanism into S4, which provides a novel paradigm distinct from CNNs or Transformers, maintaining linear scalability of long sequences processing.

The tremendous potential of Mamba has sparked a series of excellent works  in the vision domain. Vamba  proposes a cross-scan module (CSM) to tackle the direction sensitivity issue between non-causal 2D images and ordered 1D sequences. Moreover, Mamba has found extensive use in the domain of medical image segmentation , incorporating Mamba blocks to UNet-like architecture to achieve task-specific architecture. VL-Mamba  and Cobra  explore the potential of SSMs in multimodal large language models. Besides, ZigMa  addresses the spatial continuity in the scan strategy, and it incorporates Mamba into the Stochastic Interpolation framework .

In this work, we develop MambaAD to exploit Mamba's long-range modeling capacity and linear computational efficiency for multi-class unsupervised anomaly detection. This approach innovatively combines SSM's global modeling capabilities with CNNs' detailed local modeling prowess

Figure 1: Compared with (a) local CNN-based RD4AD  and (b) global Transformer-based UniAD , ours MambaAD with linear complexity is capable of integrating the advantages of both global and local modeling, and multi-scale features endow it with more refined prediction accuracy.

## 3 Method

### Preliminaries

State Space Models , inspired by control systems, map a one-dimensional stimulation \(x(t)^{L}\) to response \(y(t)^{L}\) through a hidden state \(h(t)^{N}\), which are formulated as linear ordinary differential equations (ODEs):

\[h^{}(t)=h(t)+x(t),\ \ \ y(t)=h(t),\] (1)

where the state transition matrix \(^{N N}\), \(^{N 1}\) and \(^{1 N}\) for a state size \(N\).

S4  and Mamba  utilize zero-order hold with a timescale parameter \(\) to transform the continuous parameters \(\) and \(\) from the continuous system into the discrete parameters \(}\) and \(}\):

\[}=(),\ \ \ }=( )^{-1}(()-) .\] (2)

After the discretization, the discretized model formulation can be represented as:

\[h_{t}=}h_{t-1}+}x_{t},\ \ \ y_{t}=h_{t}.\] (3)

At last, from the perspective of global convolution, the output can be defined as:

\[}=(},}},,}^{L-1} }),\ \ \ =},\] (4)

where \(\) represents convolution operation, \(L\) is the length of sequence \(x\), and \(}^{L}\) is a structured convolutional kernel.

### MambaAD

The MambaAD framework is proposed for multi-class anomaly detection as illustrated in Fig. 2(a). It consists of three main components: a pre-trained CNN-based encoder, a Half-FPN bottleneck, and a Mamba-based decoder. During training, the encoder extracts feature maps at three different

Figure 2: **Overview of the proposed MambaAD**, which employs pyramidal auto-encoder framework to reconstruct multi-scale features by the proposed efficient and effective Locality-Enhanced State Space (LSS) module. Specifically, each LSS consists of: _1)_ cascaded Hybrid State Space (HSS) blocks to capture global interaction; and _2)_ parallel multi-kernel convolution operations to replenish local information. Aggregated multi-scale reconstruction error serves as the anomaly map for inference.

scales and inputs them into the H-FPN bottleneck for fusion. The fused output is then fed into the Mamba Decoder with a depth configuration of . The final loss function is the sum of the mean squared error (MSE) computed across feature maps at three scales. Within the Mamba Decoder, we introduce the Locality-Enhanced State Space (LSS) module. The LSS can be configured with different stages \(M_{i}\), where each stage represents the number \(N\) of Hybrid State Space (HSS) blocks within the module. In this experiment, we employ LSS with \(M_{i}=3\) and \(M_{i}=2\) as the primary modules. The LSS module processes the input \(X_{i}\) through HSS blocks that capture global information and through two different scales of Depth-Wise Convolution (DWConv) layers that capture local information. The original input feature dimension is restored through concatenation and convolution operations. The proposed HSS block features a Hybrid Scanning (HS) Encoder and an HS Decoder, which accommodates five distinct scanning methods and eight scanning directions.

### Locality-Enhanced State Space Module

Transformers excel in global modeling and capturing long-range dependencies but tend to overlook local semantic information and exhibit high computational complexity when processing high-resolution features. Conversely, CNNs effectively model local semantics by capturing information from adjacent positions but lack long-range modeling capabilities. To address these limitations, we propose the LSS module in Fig. 2 (b), which incorporates both Mamba-based cascaded HSS blocks for global modeling and parallel multi-kernel depth-wise convolution operations for local information capture. Specifically, for an input feature \(X_{i}^{H W C}\), global features \(G_{i}^{H W C}\) enter the HSS blocks while local features \(L_{i}^{H W C}\) proceed through a convolutional network. The global features \(G_{i}\) pass through a series of \(N\) HSS blocks to obtain global information features \(G_{o}\).

\[G_{o}=}(...(}(}(G_{i})))),\] (5)

where \(n N\) is the number of HSS blocks. In this study, we primarily use \(N=2\) and \(N=3\), with further ablation experiments presented in Sec. 4.3.

Local features \(L_{i}\) are processed by two parallel DWConv blocks, each comprising a \(1 1\) Conv block, an \(k k\) DWConv block, and another \(1 1\) Conv block.

\[L_{m}=}(}(}(L_{i})),\] (6)

where \(k\) is the kernel size for the DWConv. \(k=5\) and \(k=7\) are used in this experiment with further ablations in Sec. 4.3. Each convolutional module includes a Conv 2D layer, an Instance Norm 2D layer, and a SiLU as illustrated in Fig. 2 (d). Local and global features are aggregated by concatenation along the channel dimension. The final output \(X_{o}\) of this block is obtained by a \(1 1\) 2D convolution to restore the channel count to match that of the input and a residual connection.

\[X_{o}=}((G_{o},L_{k_{5}},L_{k_{7}}))+X_ {i}.\] (7)

### Hybrid State Space Block

Following the method outlined in , the HSS block is designed for hybrid-method and hybrid-directional scanning and fusion, as depicted in Fig. 2 (c). The HSS block primarily comprises Layer

Figure 3: Hybrid Scanning directions and methods. _(a)_ The Hilbert scanning method with 8 scanning directions is used for HS Encoder and Decoder. _(b)_ The other four scanning methods for comparison.

Normalization (LN), Linear Layer, depth-wise convolution, SiLU activation, Hybrid Scanning (HS) encoder \(_{HS}\), State Space Models (SSMs), HS decoder \(_{HS}\), and residual connections.

\[ G_{i}^{}&=( _{HS}((_{HS}(((Linear(LN(G_{i}))))})))),\\ G_{i+1}&=(G_{i}^{} (((G_{i})))+G_{i}\] (8)

**Hybrid Scanning methods.** Inspired by space-filling curves [49; 53], as shown in Fig. 3, this study explores five different scanning methods: (I) Sweep, (II) Scan, (III) Z-order, (IV) Zigzag, and (V) Hilbert, to assess their impact on the SSM's modeling capabilities. The Hilbert scanning method is ultimately selected for its superior encoding and modeling of local and global information within feature sequences, particularly in mitigating the challenges of modeling long-range dependencies. Further experimental results will be presented in the ablation study. Assuming \(A\) is a matrix, \(A^{T}\) is the transpose of \(A\), \(A^{lr}\) is the left-right reversal of \(A\), \(A^{ud}\) is the up-down reversal of \(A\). The Hilbert curve can be obtained by an n-order Hilbert matrix:

\[H_{n+1}=\{(H_{n}&4^{n}E_{n}+H_ {n}^{T}\\ (4^{n+1}+1)E_{n}-H_{n}^{ud}&(3 4^{n}+1)E_{n}-(H_{n}^{lr})^{T} )&\\ H_{n}&(4^{n+1}+1)E_{n}-H_{n}^{lr}\\ 4^{n}E_{n}+H_{n}^{T}&(3 4^{n}+1)E_{n}-(H_{n}^{T})^{lr} )&.\] (9)

where \(H_{1}=(1&2\\ 4&3)\) and \(E_{n}\) is all-one matrix for n-order.

**Hybrid Scanning directions.** Following the setup of previous scanning directions, this study supports eight Hilbert-based scanning directions: (i) forward, (ii) reverse, (iii) width-height (wh) forward, (iv) wh reverse, (v) rotated 90 degrees forward, (vi) rotated 90 degrees reverse, (vii) wh rotated 90 degrees forward, and (viii) wh rotated 90 degrees reverse, as illustrated in Fig. 3 (a). Multiple scanning directions enhance the encoding and modeling capabilities of feature sequences, enabling the handling of various types of anomalous features with further ablations in Sec. 4.3.

The HS encoder aims to combine and encode input features according to different scanning methods and directions before feeding them into the SSM to enhance the global modeling capacity of the feature vectors. The HS decoder then decodes the feature vectors output by the SSM back to the original input feature orientation, with the final output obtained by summation.

## 4 Experiments

### Setups: Datasets, Metrics, and Details

**MVTec-AD** encompasses a diverse collection of 5 types of textures and 10 types of objects, 5,354 high-resolution images in total. 3,629 normal images are designated for training. The remaining 1,725 images are reserved for testing and include both normal and abnormal samples.

**VisA** features 12 different objects, incorporating three diverse types: complex structures, multiple instances, and single instances. It consists of a total of 10,821 images, of which 9,621 are normal samples, and 1,200 are anomaly samples.

**Real-IAD** includes objects from 30 distinct categories, with a collection of 150K high-resolution images, making it larger than previous anomaly detection datasets. It consists of 99,721 normal images and 51,329 anomaly images.

More results on MVTec-3D , as well as newly proposed Uni-Medical [50; 2] and COCO-AD  datasets, can be viewed in Appendix 5.

**Metrics.** For anomaly detection and segmentation, we report Area Under the Receiver Operating Characteristic Curve (AU-ROC), Average Precision  (AP) and F1-score-max  (F1_max). Additionally, for anomaly segmentation, we also report Area Under the Per-Region-Overlap  (AU-PRO). We further calculate the mean value of the above seven evaluation metrics (denoted as \(mAD\)) to represent a model's comprehensive capability .

**Implementation Details.** All input images are resized to a uniform size of \(256 256\) without additional augmentation for consistency. A pre-trained ResNet34 acts as the feature extractor, while a Mamba decoder of equivalent depth  to ResNet34 serves as the student model for training. In the Mamba decoder, the number of cascaded HSS blocks in the second LSS module is set to 2, while all other LSS modules employ 3 cascaded HSS blocks. This experiment employs the Hilbert scanning technique, utilizing eight distinct scanning directions. The AdamW optimizer is employed with a learning rate of \(0.005\) and a decay rate of \(1 10^{-4}\). The model undergoes a training period of 500 epochs for the multi-class setting, conducted on a single NVIDIA TESLA V100 32GB GPU. During training, the sum of MSE across different scales is employed as the loss function. In the testing phase, the sum of cosine similarities at various scales is utilized as the anomaly maps.

### Comparison with SoTAs on Different AD datasets

We compared our method with current SoTA methods on a range of datasets utilizing both image-level and pixel-level metrics (_c.f._, Sec. 4.1). This paper primarily compares with UniAD  and DiAD  dedicated to MUAD. In addition, we also compare our MambaAD with Reconstruction-based RD4AD  and Embedding-based DeSTSeg /SimpleNet .

**Quantitative Results.** As shown in Tab. 1, on MVTec-AD dates, our MambaAD outperforms all the comparative methods and reaches a new SoTA to **98.6/99.6/97.6** and **97.7/56.3/59.2/93.1** in multi-class anomaly detection and segmentation. Specifically, compared to DiAD , our proposed MambaAD shows an improvement of 1.4 \(\)/0.6 \(\)/1.3 \(\) at image-level and 0.9 \(\)/3.7 \(\)/3.7 \(\)/2.4 \(\) at pixel-level. Notably, for overall metric mAD of a model, our MambaAD improves by 2.0 \(\), compared with SoTA DiAD. The VisA dataset is more complex and challenging, yet our method still

    &  &  &  &  \\   & & AU-ROC & AP & F1\_max & AU-ROC & AP & F1\_max & AU-PRO & **mAD** \\   & RD4AD  & 94.6 & 96.5 & 95.2 & 96.1 & 48.6 & 53.8 & 91.1 & 82.3 \\  & UniAD  & 96.5 & 98.8 & 96.2 & 96.8 & 43.4 & 49.5 & 90.7 & 81.7 \\  & SimpleNet  & 95.3 & 98.4 & 95.8 & 96.9 & 45.9 & 49.7 & 86.5 & 81.2 \\  & DeSTSeg  & 89.2 & 95.5 & 91.6 & 93.1 & 54.3 & 50.9 & 64.8 & 77.1 \\  & DiAD  & 97.2 & 90.0 & 96.5 & 96.8 & 52.6 & 55.5 & 90.7 & 84.0 \\  & ManphAD (Ours) & **98.6 \(\) 0.3** & **99.6 \(\) 0.2** & **97.8 \(\) 0.4** & **97.7 \(\) 0.4** & **56.3 \(\) 0.7** & **59.2 \(\) 0.6** & **93.1 \(\) 0.3** & **86.0 \(\) 0.3** \\   & RD4AD  & 92.4 & 92.4 & **89.6** & 98.1 & 38.0 & 42.6 & **91.8** & 72.8 \\  & UniAD  & 88.8 & 90.8 & 85.8 & 98.3 & 33.7 & 39.0 & 85.5 & 74.6 \\  & SimpleNet  & 87.2 & 87.0 & 81.8 & 96.8 & 34.7 & 37.8 & 81.4 & 72.4 \\  & DiAD  & 86.8 & 83.7 & 85.1 & 96.0 & 26.1 & 33.0 & 75.2 & 70.1 \\  & MamphAD (Ours) & **93.3 \(\) 0.4** & **94.5 \(\) 0.5** & **99.4 \(\) 0.6** & **98.5 \(\) 0.3** & **39.4 \(\) 1.1** & **44.0 \(\) 1.3** & 91.0 \(\) 0.9** & **78.7 \(\) 0.5** \\   & RD4AD  & 82.4 & 79.0 & 73.9 & 97.3 & 25.0 & 32.7 & 89.6 & 68.6 \\  & UniAD  & 83.0 & 80.9 & 74.3 & 97.3 & 21.1 & 29.2 & 86.7 & 67.5 \\   & SimpleNet  & 57.2 & 53.4 & 61.5 & 75.7 & 2.8 & 6.5 & 39.0 & 42.3 \\   & DeSTSeg  & 82.3 & 79.2 & 73.2 & 94.6 & **37.9** & **41.7** & 40.6 & 64.2 \\   & DiAD  & 75.6 & 66.4 & **69.7** \(\) 0.3** & **98.0** & 2.9 & 7.1 & 58.1 & 52.6 \\   & MamphAD (Ours) & **86.3 \(\) 0.4** & **84.6 \(\) 0.3** & **77.0 \(\) 0.4** & **98.5 \(\) 0.1** & 33.0 \(\) 0.6 & 38.7 \(\) 0.6 & **90.5 \(\) 0.3** & **72.7 \(\) 0.4** \\   

Table 1: Quantitative Results on different AD datasets for multi-class setting.

Figure 4: Qualitative visualization for pixel-level anomaly segmentation on MVTec and VisA datasets.

demonstrates excellent performance. As shown in Tab. 1, our MamboAD exceeds the performance of DiAD  by 7.5 \(\)6.2 \(\)4.3 \(\) at image-level and by 2.5 \(\)/13.3 \(\)/11.0 \(\)/15.8 \(\). Meanwhile, we achieve an enhancement of \(8.7\) compared to advanced DiAD on the mAD metric. In addition, the SoTA results on Real-IAD datasets, shown in Tab. 1, illustrate the scalability, versatility, and efficacy of our method MamboAD. We also compared the results of MamboAD with state-of-the-art (SoTA) methods for single-class anomaly detection in Tabs. A13 to A16 in the Appendix. In the single-class tasks, we included a comparison with the PatchCore  method. On the MVTec-AD dataset, SimpleNet and PatchCore achieved the best results at the image level, while our method achieved the second-best results. At the pixel level, DeSTSeg achieved the best results in most metrics, whereas our method achieved the best results in AUPRO. For the VisA dataset, our method achieved the best results in two metrics each at both the image and pixel levels. This demonstrates that our multi-class anomaly detection method can also achieve optimal or near-optimal results in single-class tasks, indicating its effectiveness and robustness. In contrast, existing single-class SoTA methods like SimpleNet and DeSTSeg, although effective in single-class tasks, show a significant performance drop in multi-class tasks. The PatchCore method can only operate in single-class tasks; for multi-class tasks, it encounters issues such as GPU and memory overflow due to the need to store all features in the Memory Bank. In summary, MamboAD exhibits strong robustness and effectiveness. More detailed results for each category are presented in the Appendix.

**Qualitative Results.** We conducted qualitative experiments on MVTev-AD and VisA datasets that substantiated the accuracy of our method in anomaly segmentation. Fig. 4 demonstrates that our method possesses more precise anomaly segmentation capabilities. Compared to DiAD, our method delivers more accurate anomaly segmentation without significant anomaly segmentation bias.

### Ablation and Analysis

**Components IncrementalAblations.** The ablation experiments for the proposed components are summarized in Tab. 2. Using the most basic decoder purely based on Mambo and employing only the simplest two-directional sweep scanning method, we achieve an mAD score of 82.1 on the MVTec-AD dataset. Subsequently, by incorporating the proposed LSS module, which integrates the global modeling capabilities of Mambo with the local modeling capabilities of CNNs, the mAD score improves by +2.8%. Finally, replacing the original scanning directions and methods with HSS, which combines features from different scanning directions and employs the Hilbert scanning method, better aligns with the data distribution in most industrial scenarios where objects are centrally located in the image. This results in an additional +1.1% point improvement in the mAD score. Overall, the proposed MamboAD achieves an mAD score of 86.0 on the MVTec-AD dataset and 78.9 on the VisA dataset, reaching the SoTA performance.

**Local/Global Branches of the LSS Module.** We conducted three ablation experiments to verify the impact of branches in the LSS module as shown in Tab. 3. The Local branch represents the use of only the parallel CNN branch, without the Mambo-based HSS branch. The Global branch represents the use of only the Mambo-based HSS branch, without the parallel CNN branch, making the decoder in this structure purely Mambo-based. Finally, Global+Local represents the proposed LSS structure used in MamboAD, which combines the serial Mambo-based HSS with parallel CNN branches of different kernel sizes. The experimental results are shown in the table below. The Local branch, which uses only CNNs, has the lowest parameter count and FLOPs but also the lowest mAD metric, indicating high efficiency but suboptimal accuracy. The Global method, based on the pure Mambo structure, consumes more parameters and FLOPs than the Local method but shows a significant improvement in performance (+2.7%). Finally, the combined Global+Local method, which is the LSS module used in MamboAD, achieves the best performance with a notable improvement of (+1.6%) over the individual methods.

**Effectiveness comparison of different pre-trained backbone and Mambo decoder depth.** First, we compared various pre-trained feature extraction networks, focusing on the popular ResNet series

   Basic Mamba & LSS & HSS & MVTec-AD & VisA \\  ✓ & & & & 82.1 & 72.9 \\ ✓ & ✓ & & & 84.9 & 78.0 \\ ✓ & ✓ & ✓ & **86.0** & **78.9** \\   

Table 2: Incremental Ablations.

   Model & Params(M) & FLOPs(G) & MVTec-AD & VisA \\  Local & 13.0 & 5.0 & 81.7 & 72.5 \\ Global & 22.5 & 7.5 & 82.1 & 72.9 \\ Local + Global & 25.7 & 8.3 & **86.0** & **78.9** \\   

Table 3: Ablation Study on the LSS Module.

[MISSING_PAGE_FAIL:9]

we compare scenarios where \(M_{i}=1\), meaning each LSS module contains a single HSS block, and we experiment with different kernel sizes for DWConv alone and configurations flanked by \(1 1\) convolutions. Subsequently, we contrast the results without residual connections against those with identical settings but with \(M_{i}=1\). Finally, we examine the outcomes when \(M_{i}=2\) and \(M_{i}=3\) under otherwise consistent settings. Analysis reveals that with \(M_{i}=1\), regardless of the presence of residual connections, the results are inferior to those with \(M_{i}=2\) and \(M_{i}=3\). Moreover, using only DWConv blocks with \(M_{i}=1\), a comparison of parallel depth-wise convolutions with varying kernel sizes indicates that smaller kernels, such as \(k=1\), significantly degrade performance. Therefore, subsequent comparative experiments focus on larger convolution kernels. In the absence of residual connections, some metrics may surpass those with residual connections, but the longer training time and difficulty in convergence preclude their use in further experiments. In configurations with \(M_{i}=2\) and \(M_{i}=3\), we find that DWConv blocks augmented with \(1 1\) convolutions exhibit superior performance. Additionally, kernels sized \(k=5\) and \(k=7\) are more suitable for extracting local features and establishing local information associations. Consequently, in this study, we opt for a quantity of HSS blocks with \(M_{i}=2\) and \(M_{i}=3\), and we employ parallel DWConv blocks with kernels sized \(k=5\) and\(k=7\), complemented by \(1 1\) convolutions before and after.

## 5 Conclusion

This paper introduces MambaAD, the first application of the Mamba framework to AD. MambaAD consists of a pre-trained encoder and a Mamba decoder, with a novel LSS module employed at different scales and depths. The LSS module, composed of sequential HSS modules and parallel multi-core convolutional networks, combines Mamba's global modeling prowess with CNN-based local feature correlation. The HSS module employs HS encoders to encode input features into five scanning patterns and eight directions, which facilitate the modeling of feature sequences in industrial products at their central positions. Extensive experiments on six diverse AD datasets and seven evaluation metrics demonstrate the effectiveness of our approach in achieving SoTA performance.

**Limitations, Broader Impact and Social Impact.** The model is not efficient enough and more lightweight models need to be designed. This study marks our initial attempt to apply Mamba in AD, laying a foundation for future research. We hope it can inspire lightweight designs in AD. MambaAD exhibits significant practical implications in enhancing industrial production efficiency.