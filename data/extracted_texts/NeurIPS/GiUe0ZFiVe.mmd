# Bi-Level Offline Policy Optimization with Limited Exploration

Wenzhuo Zhou

Department of Statistics

University of California Irvine

wenzhuz3@uci.edu

###### Abstract

We study offline reinforcement learning (RL) which seeks to learn a good policy based on a fixed, pre-collected dataset. A fundamental challenge behind this task is the distributional shift due to the dataset lacking sufficient exploration, especially under function approximation. To tackle this issue, we propose a bi-level structured policy optimization algorithm that models a hierarchical interaction between the policy (upper-level) and the value function (lower-level). The lower level focuses on constructing a confidence set of value estimates that maintain sufficiently small weighted average Bellman errors, while controlling uncertainty arising from distribution mismatch. Subsequently, at the upper level, the policy aims to maximize a conservative value estimate from the confidence set formed at the lower level. This novel formulation preserves the maximum flexibility of the implicitly induced exploratory data distribution, enabling the power of model extrapolation. In practice, it can be solved through a computationally efficient, penalized adversarial estimation procedure. Our theoretical regret guarantees do not rely on any data-coverage and completeness-type assumptions, only requiring realizability. These guarantees also demonstrate that the learned policy represents the "best effort" among all policies, as no other policies can outperform it. We evaluate our model using a blend of synthetic, benchmark, and real-world datasets for offline RL, showing that it performs competitively with state-of-the-art methods.

## 1 Introduction

Offline reinforcement learning (RL) is a task to learn a good policy using only a pre-collected, fixed dataset, without further exploration with the environment. This distinctive characteristic positions offline RL as a promising approach for solving real-world sequential decision-making problems in healthcare , financial marketing , robotics  and education , as acquiring diverse or expert-quality data in these fields can be costly or practically unattainable.

Arguably, two of the biggest challenges in offline RL are the distributional shift between the data-generating distribution and those induced by candidate policies, and the stringent requirements on the properties of function approximation . It has been observed that, in practice, the distributional mismatch often results in unsatisfactory performance of many existing algorithms, and even amplifying with function approximation . Many prior works  crucially rely on a global data-coverage assumption and completeness-type function approximation condition in a technical sense. The former necessitates that the dataset to contain any state-action pair with a lower bounded probability so that the distributional shift can be well calibrated. The latter requires the function class to be closed under Bellman updates. Both assumptions are particularly strong and are likely to be violated in practice . Consequently, algorithms that depend on these assumptions may experience performance degradation and instability . Therefore, it is crucial to develop novel algorithms that relax these assumptions, offering robust and widely applicable solutions for real-world scenarios.

[MISSING_PAGE_FAIL:2]

effectively \(\) covers the visitation induced by \(\). The primary objective of offline policy optimization is to learn an optimal policy that maximizes the return, \(J()\), using the offline dataset. Under the function approximation setting, we assume access to two function classes \(:\) and \(:\), which are utilized to capture \(q^{}\) and \(_{d_{}/}\), respectively.

**Exploration and coverage.** In general, when saying an offline dataset is well-explored, it means that a well-designed behavior policy has been executed, allowing for comprehensive exploration of the MDP environment. As a result, the dataset is likely to contain possibly all state-action pairs. This implicitly requires \(\) has the global coverage [18; 49]. In this context, the global coverage means that the density ratio-based concentrability coefficient, \(_{s,a}\{d_{}(s,a)/(s,a)\}\), is upper-bounded by a constant \(c^{+}\) for all policies \(\), where \(\) is some policy class. This condition is frequently employed in offline RL [2; 8; 12]. However, in practice, this assumption may not hold true, as devising an exploratory policy is a challenging task for large-scale RL problems. Instead, our goal is to learn a good policy with strong theoretical guarantees that can compete against any arbitrarily covered comparator policy under much weaker conditions than the global coverage.

## 3 Bi-Level Offline Policy Optimization Algorithm

In this section, we introduce our bi-level offline policy optimization framework. The development of the framework consists of three major steps.

**Step 1: robust interval learning.** In this step, we aim to provide a robust off-policy interval evaluation. The major advantage of this interval formulation is its robustness to the model-misspecification of the importance-weight class \(\), and the encoding of distributional-shift information in the policy evaluation process. First, we define a detection function \(()\), which is used to measure the degree of the distributional-shift in terms of density ratio.

**Definition 3.1**.: _For \(x,c_{1},c_{2},C^{+}\) and \(C 1\), the detection function \(()\) satisfies the following conditions: (1) 1-minimum: \((1)=0\). (2) Non-negativity: \((x) 0\). (3) Boundedness on first-order derivative: \(|^{}(x)| c_{2}\) if \(x[0,C]\). (4) Boundedness on value: \(|(x)| c_{1}\) for \(x[0,C]\). (5) Strong convexity: \((x)\) is \(M\)-strongly convex with respect to \(x\)._

The family of Renyi entropy , Bhattacharyya distance , and simple quadratic form functions all satisfy the conditions outlined in Definition 3.1. Under this definition, it can easily observe that \(\) has a convex conjugate function , \(_{*}\) with \(_{*}(x_{*})=_{x}\{x x_{*}-(x)\}\), that satisfies \(_{*}(0)=0\). It follows from Bellman equation \(^{}q^{}(s,a)=q^{}(s,a)\) for any \(s,a\), then \(J()=q^{}(s^{0},)+_{}[_{*}((^ {}q^{}(s,a)-q^{}(s,a)/))/(1-)]\) for \( 0\). Applying Fenchel-Legendre transformation [36; 21], and model \(x\) in a restricted importance weight class \(\) for any \(s,a\), we obtain

\[J()= q^{}(s^{0},)+_{}[_{x}x(^{ }q^{}(s,a)-q^{}(s,a))-(x)]/(1-) \] \[ q^{}(s^{0},)+_{}[(s,a)(r(s,a)+ q^{ }(s^{},)-q^{}(s,a))-((s,a))]/(1-). \]

Suppose \(q^{}\) is well-specified, i.e., \(q^{}\), we can find a lower bound of (2), which is valid for any \(\), via replacing \(q^{}\) with \(_{q}\) as follows:

\[J()_{q} _{}[(s,a)(r(s,a )+ q(s^{},)-q(s,a))]+q(s^{0},)/(1- )}_{:=H(,q,)}\] \[-_{}[( (s,a))]}_{:=(,)}}.\]

After following a similar derivation, we can establish an upper bound for \(J()\) as well, and thus construct a value interval for \(J()\). This interval holds for any \(\) and is therefore robust against model-misspecification of \(\). In order to obtain a tighter interval, we can shrink the interval width by maximizing the lower bound and minimizing the upper bound, both with respect to \(\). This procedure can be interpreted as searching for some good \(\) to minimize the function approximation error.

\[J()[_{}_{q}H(,q,)- (,),\ _{}_{q}H(,q,)+ (,)], \]

While the interval offers a robust method for dealing with the bias introduced by function approximation when estimating \(J()\), it lacks a crucial and non-trivial step for handling statistical uncertainty.

**Step 2: uncertainty quantification.** In this step, we quantify the uncertainty of the interval (3), and establish a non-asymptotic confidence interval (CI) for \(J()\) which integrates bias and uncertainty quantifications in a single interval inspired by . Given offline data \(_{1:n}\), our formal result for quantifying sampling uncertainty in order to establish the CI for \(J()\).

**Theorem 3.1** (Non-asymptotic confidence interval).: _For a target policy \(\), the return \(J()\) is within a CI for any \(\) with probability at least \(1-\), i.e., \(J()[_{n}^{-}(;),_{n}^{+}(;)]\) for_

\[_{n}^{-}(;):= _{i=1}^{n}(s_{i},a_{i})}{1-}- _{q}_{n}(-q,)-_{n}(, )-_{n},\] \[_{n}^{+}(;):= _{i=1}^{n}(s_{i},a_{i})}{1-} +_{q}_{n}(q,)+_{n}(, )+_{n}, \]

_if the uncertainty deviation \(_{n}\) satisfies_

\[P_{}_{i=1}^{n}(s_ {i},a_{i})(r_{i}+ q^{}(s_{i}^{},)-q^{}(s_{i},a_{i}) )-_{n}(,)_{n} 1-,\]

_where \(_{n}(q,):=_{i=1}^{n}(s_{i},a_{i})( q(s_{i}^{ },)-q(s_{i},a_{i}))/(1-)n+q(s^{0},)\)._

Similar to the value interval, the CI \([_{n}^{-}(;),_{n}^{+}(;)]\) also holds for any \(\). Therefore, we can optimize the confidence lower and upper bounds in (4) over \(\) to tighten the CI, and obtain:

\[PJ()[_{}_{n}^{-}(;),_{ }_{n}^{+}(;)][_{n}^{-}(; ),_{n}^{+}(;) 1-.\]

**Step 3: bridge policy evaluation to policy optimization.** In this step, we aim to formulate a policy optimization based on the derived high-confidence policy evaluation from the previous steps. Given the consistent CI estimation of \(J()\), we can naturally incorporate the pessimism principle, i.e., using the CI lower bounds of \(J()\) as the value estimate of the policy evaluation of \(\). With such a procedure, our objective is to maximize these lower bounds over some family \(\) of policies:

\[_{}\{_{}_{n}^{-}(;) \}. \]

Although (5) is algorithmically feasible for obtaining a policy solver \(\), it lacks direct interpretation without taking advantage of the bi-level optimization structure in hindsight. Therefore, we propose to reformulate (5) via a _dual-to-prime conversion_ (shown in Theorem 3.2), which naturally lends itself to lower-upper optimization with guaranteed convergence. Specifically, we formulate (5) as a bi-level framework problem:

\[() _{}-q^{}(s^{0},), \] \[() s.t.\ }*{arg\,min}_{q _{_{n}}}q(s^{0},),\] (7) \[}: _{_{n}}=q:_{ _{_{n}}}n^{-1}_{i=1}^{n}( s_{i},a_{i})(r_{i}+ q(s_{i}^{},)-q(s_{i},a_{i})) _{n}},\] \[}: _{_{n}}=\{_{}/ _{_{}}\|_{}\|_{}_{}:_{n}( ,_{}))_{n}\}.\]

At the upper level, the learned policy \(\) attempts to maximize the value estimate of \(q^{}\) over some policy class \(\), while at the lower level, \(q^{}\) is to seek the \(q\)-function with the pessimistic policy evaluation value from the confidence set \(_{_{n}}}\) with consistency guarantee and uncertainty control. For _consistency_, whenever \(q^{}\) or its good approximator is included in \(\) (realizability for \(\) class is satisfied), the set \(_{_{n}}\) ensures the estimation consistency of \(q^{}\) in terms of "sufficently small" weighted average Bellman error. For _uncertainty control_, the constrained set \(_{_{n}}\) attempts to control the uncertainty arising from distributional shift via a user-specific thresholding hyperparameter \(_{n}\). The feasible (uncertainty controllable) candidates \(_{n}\) are used as weights for the average Bellman error, helping to construct the consistent set \(_{_{n}}\). Risk-averse users can specify a lower value for the thresholding hyperparameter or consider a higher \(_{n}\) to tolerate a larger distribution shift. In other words, the chosen value of \(_{n}\) depends on the degree of pessimism users want to incorporate in the policy optimization.

[MISSING_PAGE_FAIL:5]

_Here \(_{^{}-q^{}}(s,a)=}(s,a)= }(s,a)\) for \(}:=_{q_{_{a}}}q(s^{0},)\) and \(}:=_{q_{_{a}}}q(s^{0},)\). For Pollard's pseudo-dimensions \(D_{},D_{},D_{}\), \(()=(e^{D}\{D_{},D_{},D_{}\}+1 )^{3}((1 L)_{2}^{*})^{2D}\) with the effective pseudo dimension \(D=D_{}+D_{}+D_{}\), where \(L\) is Lipschitz constant of \(M\)-strongly convex function \(()\). Moreover, \(_{x}\) and \(}\) denote constant terms depending on \(x\), and big-Oh notation ignoring high-order terms, respectively._

In the upper bound of Theorem 4.1, we split the regret into four different parts: the on-support intrinsic uncertainty \(_{}\), the on-support bias \(_{b}\), the violation of realizability \(_{}\), and the off-support extrapolation error \(_{}\). Recall that we require \(q^{}\) as in Assumption 1, in fact, we can further relax the condition to requiring \(q^{}\) to be in the linear hull of \(\), which is more robust to the realizability error \(_{}\). In the following, we focus on investigating the roles of the on-support and off-support error terms in the regret bound.

**On-support errors: bias and uncertainty tradeoff.** The on-support error consists of two terms: \(_{b}\) and \(_{}\). The on-support uncertainty deviation, \(_{}\), is scaled by a weighted \(L_{2}\)-based concentrability coefficient \(_{2}^{*}:=/_{L_{2}()}\), which measures the distribution mismatch between the implicit exploratory data distribution and the baseline data distribution \(\). Meanwhile, \(_{b}\) depends on the probability mass of \((d_{}-)^{+}_{>0}\), and represents the bias weighted by the probability mass difference between \(d_{}\) and \(\) in the support region of \(\). In general, a small value of \(_{2}^{*}\) necessitates choice of the distribution \(\) to be closer to \(\) which reduces \(_{}\), reducing \(_{}\) but potentially increasing the on-support bias \(_{b}\) due to the possible mismatch between \(d_{}\) and \(\). Consequently, within the on-support region, there is a trade-off between \(_{}\) and \(_{b}\), which is adjusted through \(_{2}^{*}\).

**Off-support error: enhanced model extrapolation.** One of our main algorithmic contributions is that the off-support extrapolation error \(_{}\) can be minimized by selecting the best possible \(\)_without_ worrying about balancing the error trade-off, unlike the on-support scenario. This desirable property is essential for allowing the model to harness its extrapolation capabilities to minimize \(_{}\), while simultaneously achieving a good on-support estimation error. As a result, the model attains a small regret. Recall the bi-level formulation; at the lower level, (7) addresses uncertainty arising from the distributional shift using \(L_{2}()\) control rather than \(L_{}\) control. This plays an important role in enhancing the power of the model extrapolation. In particular, Specifically, there exists an implicit exploratory data distribution \(\) with on-support behavior (\(_{>0}\)) close to \(\), such that \(/_{L_{2}()}\) is small. On the other hand, its off-support behavior (\(_{=0}\)) can be arbitrarily flexible, ensuring that \(d_{}_{=0}\) is close to \(_{=0}\). Consequently, \((d_{}-)^{+}_{=0}\) is small, as is \(_{}\).

When a dataset with partial coverage, as indicated in , it is necessary to provide a guarantee: learn the policy with "best efforts" which is competitive to any policy as long as it is covered. Before we state the near-optimal regret guarantee of our algorithm, we formally define a notion of covered policies according to a newly-defined concentrability coefficient.

**Definition 4.1** (\(_{2}^{*}\)-covered policy class).: _Let \((_{2}^{*})\) denote the \(_{2}^{*}\)-covered policy class of \(\) for \(_{2}^{*} 1\), defined as_

\[(_{2}^{*}):=\{:(s,a) _{(s,a)>0}}{(s,a)}_{L_{2}()}_{2}^ {*}_{s,a}(s,a)_{(s,a)=0}}{(s,a)}<+ \}.\]

Note that this mixture density ratio concentrability coefficient is always bounded by the \(L_{}\)-based concentrability coefficient. Thus such single-policy concentrability assumption in terms of the mixture density ratio is weaker than the standard \(L_{}\) density ratio-based assumption.

**Corollary 4.1** (Near-optimal regret).: _Under Assumptions 1-3 with \(_{}[0,1)\), and we set \(_{n},_{n}\) as in Theorem 4.1, then for any good comparator policy \(^{}(_{2}^{*})\) (not necessary the optimal policy \(^{*}\)), w.p. \( 1-\), the output policy \(\) of (6) satisfies_

\[J(^{})-J()} _{2}^{*}(+L)( )/\}}{nM}}+_{}^{*}+_{}^ {*}/M)\,_{}}.\]

A close prior result to Corollary 4.1 is that of , which develops a pessimistic algorithm based on a nontrivial performance gap condition. Their regret guarantees only hold if the data covers the optimal policy \(^{*}\), in particular, requiring a bounded \(L_{}\) single-policy concentrability with respect to \(^{*}\). In comparison, our guarantee can still provide a meaningful guarantee even when \(^{*}\) is not covered by data. In the following, we include the sample complexity of our algorithm when \(_{}=0\).

**Corollary 4.2** (Polynomial sample complexity).: _Under the conditions in Corollary 4.1, the output policy \(\) of solving (6) satisfies \(J(^{})-J()\) w.p. \( 1-\), if_

\[n=_{2}^{}(+L)/ )^{2}}{^{2}(1-)^{2}}+_{2}^{}^{ 2}(+L)/M)^{0.67}}{^{1.33}(1-)^{1.33}}+_{}^{}(+L)}{(1-)}()}{}.\]

The sample complexity consists of three terms corresponding to the slow rate \((n^{-1/2})\) and the two faster rate \((n^{-1})\) and \((n^{-3/4})\) terms in Corollary 4.1. When \(_{2}^{}\) and \(_{}^{}\) are not too much larger than \(_{2}^{}\), the fast rate terms are dominated, and the sample complexity is of order \((1/^{2})\), which is much faster than \((1/^{6})\) in the close work of . It is worth noting that even in exploratory settings where the global coverage assumption holds, our sample complexity rate matches the fast rate in popular offline RL frameworks with general function approximation [8; 54; 12].

In addition to the near-optimal regret guarantee, in safety-critical applications, an offline RL algorithm should consistently improve upon the baseline (behavior) policies that collected the data [19; 26]. Our algorithm also achieves this improvement guarantee with respect to the baseline policy.

**Theorem 4.2** (Baseline policy improvement).: _Under Assumptions 1-3 with \(_{}=0\) and set \(_{n},_{n}\) as in Theorem 4.1. Suppose \(1\) and the baseline policy \(_{b}\) such that \(d_{_{b}}=\), then the regret \((1-)(J(_{b})-J())\) for the output policy \(\) of solving (6), w.p. \( 1-\), is upper bounded by_

\[+L)^{2}\{()/ \}}{nM}}+^{3}+^{2}L)}{M}}()/\}}{n}^{}++L) \{()/\}}{n}.\]

The aforementioned information-theoretic results enhance the understanding of the developed algorithm, in terms of the function approximation and coverage conditions, sample complexity, horizon dependency, and bound tightness. In practice, although the information-theoretic algorithm offers a feasible solution to the problem, it is not yet tractable and computationally efficient due to the need to solve constrained optimization. In the following section, we develop a practical algorithm as a computationally efficient counterpart for the information-theoretic algorithm.

## 5 Penalized Adversarial Estimation Algorithm

Although the information-theoretic algorithm offers a feasible solution to the problem, it is not yet tractable and computationally efficient due to the need to solve constrained optimization. In this section, we develop an adversarial estimation proximal-mapping algorithm that still adheres to the pessimism principle, but through penalization. Specifically, the adversarial estimation loss is constructed as follows: \(}(q,,,c^{},)\) for solving

\[q(s^{0},)+\{c^{}_{i=1}^{n}(s_ {i},a_{i})(q(s_{i},a_{i})-r_{i}- q(s^{}_{i},)) -_{i=1}^{n}((s_{i},a_{i}))\}.\]

We observe that the inner minimization for solving \(q\) is relatively straightforward, as we can obtain a closed-form global solver using the maximum mean discrepancy principle [20; 44]. In contrast, optimizing \(_{}\) is more involved, often requiring a sufficiently expressive non-linear function approximation class, e.g., neural networks. However, concavity typically does not hold for such a class of functions . From this perspective, our problem can be viewed as solving a non-concave maximization problem, conditional on the solved global optimizer \(:=_{g}(q,,,c^{},)\). At each iteration, we propose to update \(\) by solving the proximal mapping  using the Euclidean distance to reduce the computational burden. As a result, the pre-iteration computation is quite low.

```
1:Input observed data \(_{1:n}=\{(s_{i},a_{i},r_{i},s^{}_{i})\}_{i=1}^{n}\) and parameters \(q^{0},^{0},^{0},c^{}\), \(\) and \(\).
2:For\(k=1\) to \(\):
3: Update \(^{k}\) and \(q^{k}\) by solving \(}}(q,, ^{k-1},c^{},)\)
4: Update \(^{k}\) by solving \(^{k}(|s)=} q^{k}( ,s),(|s)-D_{}((|s), ^{k}(|s)).\)
5:Return the policy \(\), which randomly selects a policy from the set \(\{^{k}\}_{k=1}^{}\).
```

**Algorithm 1** Adversarial proximal-mapping algorithmOnce \(q\) and \(\) are solved, we apply mirror descent in terms of the negative entropy \(D_{}\). That is, given a stochastic gradient direction of \(\) we solve the prox-mapping in each iteration as outlined in step 4 of Algorithm 1. A detailed version of Algorithm 1 with extended discussions on convergence and complexity is provided in Appendix. In the following, we establish the regret guarantee for the policy output by Algorithm 1.

**Theorem 5.1**.: _Under Assumptions 1-3 with \(_{}=0\), we properly choose \(=(_{2}^{})\), i.e., \(\) well depends on \(_{2}^{}\), and \(c^{*}=}_{ 2}^{}\{(^{})/\}}\). After running \(||\) rounds of Algorithm 1 with the stepsize \(=|/(2V)}\), for any policy \(\), the output policy \(\) of the algorithm, w.p \( 1-\), satisfies,_

\[J()-J()\ }_{2}^{*}^{2}_{V,,L}^{1}\{(^{})/\}}{n}}+||}{}}\\ +_{k=1}^{}_{_{k}_{_{2}}}_{(d_{}-_{k})^{+}}_{=0}( ^{^{k}}q^{k}(s,a)-q^{k}(s,a))+_{>0}_{V,,L}^{2}\{(^{})/ \}}{n}},\]

_where \(_{_{2}^{*}}:=\{_{k}:\|}{}\|_{L_{2}()}< _{2}^{*}\}\), \(_{V,,L}^{1},_{V,,L}^{2}\) are some constant terms, and the function class complexity \((^{})=(e^{D}\{D_{},D_{},D_ {}\}+1)^{3}(\{1 L\}_{2}^{})^{2D}\) for \(D=D_{}+D_{}+D_{}\)._

**Trajectory-adaptive exploratory data distribution.** Similar to Theorem 4.1, the penalized algorithm also exhibits a desirable extrapolation property for minimizing extrapolation error while simultaneously preserving small on-support estimation errors. This is achieved through adaptations of the implicit exploratory data distributions, \(_{k}\) for \(k[]\). In contrast to the information-theoretic algorithm, the automatic splitting by \(_{k}\) now depends on the optimization trajectory. At each iteration \(k\), the penalized algorithm allows each implicit exploratory data distribution \(_{k}\) to adapt to the comparator policy \(\). This results in a more flexible adaptation than the one in the information-theoretic algorithm, either for balancing the trade-off between on-support bias and uncertainty incurred by the distributional mismatch between \(d_{}\) and \(_{k}\), or for selecting the best implicit exploratory to minimize model extrapolation error.

**Optimization error.** Blessed by the reparametrization in the proximal-mapping policy update, which projects the mixture policies into the parametric space \(_{}\), the complexity of the restricted policy class is independent of the class of \(\) and the horizon optimization trajectory \(\). As a result, the optimization error \((|/})\) can be reduced arbitrarily by increasing the maximum number of iterations, \(\), without sacrificing overall regret to balance statistical error and optimization error. This allows for the construction of tight regret bounds. This distinguishes our algorithm from API-style algorithms, which do not possess a policy class that is independent of \(\).

### An Application to Linear MDPs with Refined Concentrability Coefficient

In this section, we conduct a case study in linear MDPs with insufficient data coverage. The concept of the linear MDP is initially developed in the fully exploratory setting . Let \(:^{d}\) be a \(d\)-dimensional feature mapping. We assume throughout that these feature mappings are normalized, such that \(\|(s,a)\|_{L_{2}} 1\) uniformly for all \((s,a)\). We focus on action-value functions that are linear in \(\) and consider families of the following form: \(_{}:=\{(s,a)(s,a),\ |\ \|\|_{L_{2}}  c_{}\}\), where \(c_{}[0,]\). For stochastic policies, we consider the soft-max policy class \(_{}:=\{_{}(a|s) e^{(s,a),}\ | \ \|\|_{L_{2}} c_{}\},\) where \(c_{}(0,)\). Note that the softmax policy class is consistent with the implicit policy class produced by the mirror descent updates with negative entropy in Algorithm 1, where the exponentiated gradient update rule is applied in each iteration. For the importance-weight class, we also consider the following form: \(_{}:=\{(s,a)(s,a),\ |\ \|\|_{L_{2}}  c_{}\}\) where \(c_{}(0,)\). To simplify the analysis, we assume the realizability condition for \(_{}\) is exactly met. In this linear MDP setting, we further refine the density ratio to a relative condition number to characterize partial coverage. This concept is recently introduced in the policy gradient literature  and is consistently upper-bounded by the \(L_{}\)-based density ratio concentrability coefficient.

**Definition 5.1** (Relative condition number).: _For any policy \(_{}\) and behavior policy \(_{b}\) such that \(d_{_{b}}=\), the relative condition number is defined as \((d_{},)=_{x^{d}}_{d_{}}[ (s,a)(s,a)^{}]x}{x^{}_{}[(s,a)(s,a)^{ }]x}\)._

**Assumption 4** (Bounded relative condition number).: _For any \(_{}\), \((d_{},)<\)._Intuitively, this implies that as long as a high-quality comparator policy exists, which only visits the subspace defined by the feature mapping \(\) and is covered by the offline data, our algorithm can effectively compete against it . This partial coverage assumption, in terms of the relative condition number, is considerably weaker than density ratio-based assumptions. In the following, we present our main near-optimal guarantee in linear MDPs. In addition, we design and conduct numerical experiments to empirically validate Theorem 5.2 in terms of the regret rate of convergence.

**Theorem 5.2**.: _Under Assumption 4, if we set property choose \(=(c_{})\) and \(c^{*}=}\{(1+e(1 L)Vc_ {}c_{})/\}\), and suppose \(^{}\) is returned by Algorithm 1 with linear function approxiamiton after running \(||\) rounds, then for any policy in \(_{}(_{2}^{tr})\) for \(_{2}^{tr} 1\), w.p. \( 1-\), \(J()-J(^{})\) is bounded by_

\[}c_{}^{2}\{ _{2}^{tr}\}d^{2},(d_{},)d\}}}{1-}_{,,L}d\{(1+e(1 L)c_{} c_{})/\}}{n}},\]

_where \(=(_{}[(s,a)(s,a)^{}])\) and \(c_{}\{_{2}^{tr}\}=_{\{:\|(s,a)^{}\|_{L^{2}( )}=_{2}^{tr}\}}\|\|_{L_{}}\)._

To the best of our knowledge, this is the first result PAC guarantees for an offline model-free RL algorithm in linear MDPs, requiring only realizability and single-policy concentrability. The regret bound we obtain is at least linear and, at best, sub-linear with respect to the feature dimension \(d\). Our approach demonstrates a sample complexity improvement in terms of feature dimension compared to prior work by , with a complexity of \((d^{1/2})\) versus \((d)\). It is worth noting that  only establishes results that compete with the optimal policy, and when specialized to linear MDPs, assumes the offline data has global coverage. Another previous study by  achieves a similar sub-linear rate in \(d\) as our approach; however, their algorithm is computationally intractable, relying on a much stronger Bellman-completeness assumption and requiring a small action space.

## 6 Experiments

In this section, we evaluate the performance of our practical algorithm by comparing to the model-free offline RL baselines including CQL , BEAR , BCQ , OptiDICE , ATAC , IQL , and TD3+BC . We also compete with a popular model-based approach COMBO .

**Synthetic data.** We consider two synthetic environments: a synthetic CartPole environment from the OpenAI Gym  and a simulated environment. Detailed discussions on the experimental designs are deferred to the Appendix. In both settings, following , we first learn a sub-optimal policy using DQN  and then apply softmax to its \(q\)-function, divided by a temperature parameter \(\) to set the action probabilities to define a behavior policy \(_{b}\). A smaller \(\) implies \(_{b}\) is less explored, and thus the support of \(=d_{_{b}}\) is relatively small. We vary different values of \(\) for evaluating the algorithm performance in "low", "medium" and "relatively high" offline data exploration scenarios. We use \(=0.95\) with the sample-size \(n=1500\) in all experiments. Tuning parameter selection is an open problem in offline policy optimization. Fortunately, Theorem 5.2 suggests an offline selection rule for hyperparameters \(\) and \(c^{*}\). In the following experiments, we set the hyper-parameters satisfying the condition \((}{d()})\). Figure 1 shows that our algorithm almost consistently outperforms competing methods in different settings. This performance mainly benefits from the advantages exposed in our theoretical analysis, such as model extrapolation enhancement, relaxation of completeness-type assumptions on function approximation, etc. The only exception is the slightly poorer performance compared to COMBO in a high exploration setting, where COMBO may learn a good dynamic model with relatively sufficient exploration. We provide the experiment details in Appendix due to page limit.

**Benchmark data.** We evaluate our proposed approach on the D4RL benchmark of OpenAI Gym locomotion (walker2d, hopper, halfcheetah) and Maze2D tasks , which encompasses a variety

Figure 1: The boxplot of the discounted return over \(50\) repeated experiments.

of dataset settings and domains and positions our algorithm within the existing baselines. We take the results of COMBO, OptiDICE and ATAC from their original papers for Gym locomotion, and run COMBO and ATAC using author-provided implementations for Maze2D. The results of BCQ, BEAR methods from the D4RL original paper. In addition, CQL, IQL and TD3+BC are re-run to ensure a fair evaluation process for all tasks. As shown in Table 1, the proposed algorithm achieves the best performance in 7 tasks and is comparable to the baselines in the remaining tasks. In addition to the evaluation of the policy performance, we also conduct sensitivity analyses on the hyperparameter-tuning and study the regret rate of convergence.

**Real-world application.** The Ohio Type 1 Diabetes (OhioT1DM) dataset  comprises a cohort of patients with Type-1 diabetes, where each patient exhibits different dynamics and 8 weeks of life-event data, including health status measurements and insulin injection dosages. Clinicians aim to adjust insulin injection dose levels  based on a patient's health status in order to maintain glucose levels within a specific range for safe dose recommendations. The state variables consist of health status measurements, and the action space is a bounded insulin dose range. The glycemic index serves as a reward function to assess the quality of dose suggestions. Since the data-generating process is unknown, we follow  to utilize the Monte Carlo approximation of the estimated value function on the initial state of each trajectory to evaluate the performance of each method. The mean and standard deviation of the improvements on the Monto Carlo discounted returns are presented in Table 2. As a result, our algorithm achieves the best performance for almost all patients, except for Patient \(552\). The main reason for the desired performance in real data is from the enhanced model extrapolation and relaxed function approximation requirements and outperforms the competing methods. This finding is consistent with the results in the synthetic and benchmark datasets, demonstrating the potential applicability of the proposed algorithm in real-world environments.

## 7 Conclusion

We study offline RL with limited exploration in function approximation settings. We propose a bi-level policy optimization framework, which can be further solved by a computationally practical penalized adversarial estimation algorithm, offering strong theoretical and empirical guarantees. Regarding limitations and future work, while the penalized adversarial estimation is more computationally efficient than the previously constrained problem, it may still be more challenging to solve than single-stage optimization problems. Another future direction is to explore environments with unobservable confounders. It will be interesting to address these limitations in future works.

   Tasks & Proposed & COMBO & BCQ & BEAR & OptiDICE & ATAC & CQL & IQL & TD3+BC \\  walker2d-med & \(80.8 5.1\) & \(81.9 2.8\) & \(53.1\) & \(59.1\) & \(21.8 7.1\) & \(\) & \(77.2 4.2\) & \(78.3 4.3\) & \(81.7 2.3\) \\ hopper-med & \(94.9 4.3\) & \(97.2 2.2\) & \(54.5\) & \(52.1\) & \(94.1 3.7\) & \(85.6\) & \(74.3 5.8\) & \(66.3 6.4\) & \( 1.6\) \\ halfcheetah-med & \(\) & \(54.2 1.5\) & \(40.7\) & \(41.7\) & \(38.2 0.1\) & \(53.3\) & \(37.2 0.3\) & \(47.4 1.1\) & \(27.8 0.7\) \\  walker2d-med & \(\) & \(56.0 8.6\) & \(15.0\) & \(19.2\) & \(21.6 2.1\) & \(92.5\) & \(20.8 1.6\) & \(73.9 2.8\) & \(34.4 4.2\) \\ hopper-med-rep & \(\) & \(89.5 1.8\) & \(33.1\) & \(33.7\) & \(33.7\) & \(1.1\) & \(102.5\) & \(32.6 1.9\) & \(94.7 1.5\) & \(44.4 3.7\) \\ halfcheetah-med-rep & \(49.3 2.1\) & \(\) & \(38.2\) & \(38.6\) & \(39.8 0.3\) & \(48.0\) & \(41.9 1.1\) & \(44.2 2.5\) & \(48.3 0.7\) \\  walker2d-med & \(\) & \(103.3 5.6\) & \(57.5 0.1\) & \(74.8 9.2\) & \(114.2\) & \(103.8 6.90\) & \(109.6 7.0\) & \(100.5 8.9\) \\ hopper-med-exp & \(117.8 1.9\) & \(111.1 2.1\) & \(110.9\) & \(96.3\) & \(111.5 0.6\) & \(\) & \(111.4 12.9\) & \(91.5 2.2\) & \(112.4 0.3\) \\ halfcheetah-med-exp & \(\) & \(90.0 5.6\) & \(64.7\) & \(53.4\) & \(91.1 3.7\) & \(94.8\) & \(66.7 8.9\) & \(86.7 3.6\) & \(95.9 3.9\) \\  walker2d-random & \(\) & \(7.0 3.6\) & \(4.9\) & \(7.3\) & \(9

## 8 Acknowledgments

The author is grateful to the five anonymous reviewers and the area chair for their valuable comments and suggestions.