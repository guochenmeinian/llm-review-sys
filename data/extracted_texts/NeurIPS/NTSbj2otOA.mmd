# First- and Second-Order Bounds for Adversarial Linear Contextual Bandits

Julia Olkhovskaya

Department of Intelligent Systems, Delft University of Technology, Delft, The Netherlands

Jack Mayo

Korteweg-de Vries Institute for Mathematics, University of Amsterdam, Amsterdam, The Netherlands

Tim van Erven

Korteweg-de Vries Institute for Mathematics, University of Amsterdam, Amsterdam, The Netherlands

Gergely Neu

AI group, DTIC, Universitat Pompeu Fabra, Barcelona, Spain

Chen-Yu Wei

MIT Institute for Data, Systems, and Society, Massachusetts Institute of Technology, Cambridge, MA, USA

###### Abstract

We consider the adversarial linear contextual bandit setting, which allows for the loss functions associated with each of \(K\) arms to change over time without restriction. Assuming the \(d\)-dimensional contexts are drawn from a fixed known distribution, the worst-case expected regret over the course of \(T\) rounds is known to scale as \(()\). Under the additional assumption that the density of the contexts is log-concave, we obtain a second-order bound of order \((K})\) in terms of the cumulative second moment of the learner's losses \(V_{T}\), and a closely related first-order bound of order \((K^{*}})\) in terms of the cumulative loss of the best policy \(L_{T}^{*}\). Since \(V_{T}\) or \(L_{T}^{*}\) may be significantly smaller than \(T\), these improve over the worst-case regret whenever the environment is relatively benign. Our results are obtained using a truncated version of the continuous exponential weights algorithm over the probability simplex, which we analyse by exploiting a novel connection to the linear bandit setting without contexts.

## 1 Introduction

The contextual bandit problem is a generalization of the multi-armed bandit setting in which a learner observes relevant contextual information before choosing an arm. The goal of the learner is to minimize the excess cumulative loss of the chosen arms compared to the best fixed policy for mapping contexts to arms. This framework addresses a broad range of important real-world problems like sequential treatment allocation (Tewari and Murphy, 2017), online recommendation (Beygelzimer et al., 2011) or online advertising (Li et al., 2010), and is actively used in practice (Agarwal et al., 2016). Numerous variants of the setting have been studied, which differ in the assumptions they make about the losses and the contexts. In this paper, we focus on the recently introduced setting of Neu and Olkhovskaya (2020) where the contexts are finite-dimensional i.i.d. random vectors, and the losses are time-varying linear functions of the context that may potentially be generated by an adversary. In this setting, the worst-case rate for the expected regret is known to be \(()\) for time horizon \(T\)(Neu and Olkhovskaya, 2020).

Our main contribution is to replace the worst-case rate by adaptive bounds. Specifically, we obtain a bound of \((})\) in terms of a quadratic measure of variance \(V_{T}\) for the losses of the algorithm, and a bound of \((^{*}})\), where \(L_{T}^{*}\) is the cumulative loss incurred by the optimal policy. Such bounds in terms of \(L_{T}^{*}\) or \(V_{T}\) are generally referred to as _first-order_ and _second-order bounds_, respectively,and have been extensively studied in the bandit literature. They can lead to much stronger guarantees in the often realistic case when \(T\) is large, but the losses vary little or when there exists a policy with very low cumulative loss.

Worst-case guarantees in terms of \(T\) have first been proved for the contextual bandit problem with finite policy classes by Auer et al. (2002), with further improvements by Beygelzimer et al. (2011). These methods can deal with adversarial losses and contexts, but only work for finite policy classes and have run-time scaling linearly with the size of the class--which is generally unacceptable in practice. This latter challenge has been addressed by a line of work culminating in Agarwal et al. (2014), which only requires access to an optimization oracle over the policy class. Their results, however, remain restricted to i.i.d. contexts and losses. An alternative line of work has been initiated by Auer (2002); Chu et al. (2011); Abbasi-Yadkori et al. (2011), who studied the special case of i.i.d. _linear_ loss functions with changing decision sets. The case of i.i.d. contexts and adversarial linear losses has first been studied by Neu and Olkhovskaya (2020).

Improvements of worst-case guarantees of order \(\) to first-order bounds scaling with \(^{s}}\) have been known for a variety of bandit settings since the works of Stoltz (2005); Allenberg et al. (2006), and Neu (2015). Regarding contextual bandits, the COLT 2017 open problem of Agarwal et al. (2017) asks for efficient algorithms that achieve first-order bounds for large, but finite, policy classes, either when both contexts and losses are i.i.d. or when both are fully adversarial. First to answer the open problem were Allen-Zhu et al. (2018), who obtained an optimal first-order regret guarantee for adversarial losses and contexts, but with an algorithm that is inefficient for large policy classes. Foster and Krishnamurthy (2021) provide the first efficient algorithm for the non-adversarial setting where the loss function is fixed over time and one has access to an oracle that can solve various optimization tasks over the policy class. We improve on these works in terms of the computational efficiency of our algorithm and by allowing the loss function to vary adversarially over time, although we do rely on the extra assumption that the loss functions are linear.

Another relevant framework is the adversarial linear bandit setting (without contexts), where there also exist adaptive results (Bubeck et al., 2019; Lee et al., 2020; Ito et al., 2020). While conceptually related, an important distinction is that the linear bandit setting assumes a fixed decision set, whereas reducing the linear contextual bandit problem to a linear bandit problem requires the use of decision sets that change as a function of the contexts.

Main Contributions.We consider a \(K\)-armed linear contextual bandit problem with \(d\)-dimensional contexts over \(T\) rounds. The contexts are assumed to be drawn i.i.d., but the linear loss functions mapping contexts to losses for the arms are chosen by an adaptive adversary. The aim of the learner is to minimize their regret, which is the gap between the expected cumulative loss of the learner and the expected cumulative loss of the best fixed policy \(_{T}^{*}\) chosen in full knowledge of the sequence of losses. In this setting, \(_{T}^{*}\) is known to be a linear classifier, i.e. it chooses the arm with smallest predicted loss, where the predictions are fixed linear functions of the context (see Section 2). The goal is therefore to compete with all linear classifiers. We first obtain the following second-order bound on the expected regret

\[R_{T}=K},\] (1)

where \(V_{T}\) is defined in (5) as a measure of the cumulative second moments of the losses for the arms played by the algorithm. Following Ito et al. (2020), we allow these moments to be centered around optimistic estimates that can further improve the bound when available or can simply be set to zero when they are not. We further obtain a first order bound of the form

\[R_{T}(_{T}^{*})=K^{*}}.\] (2)

The second-order bound is obtained using a truncated version of the continuous exponential weights algorithm over the probability simplex, similar to the algorithm for linear non-contextual bandits of Ito et al. (2020), and the first-order bound may be obtained as a corollary. As discussed in Section 3.3, the computational complexity of this method is dominated by two steps that together require \((K^{5})+(d/)^{O(1)}\) per round for approximation up to precision \(>0\), which is computationally feasible for moderate \(K\) and \(\). Both results are not strict improvements on the worst-case rate of \(()\) by Neu and Olkhovskaya (2020): first, they have a slightly worse dependence on \(K\). We consider this a price worth paying for the first adaptive bounds in this setting. Second, they require the extra assumption that the distribution of the contexts is _log-concave_. Although log-concavity is weaker than assuming the contexts follow e.g. (truncated) Gaussian distributions, we conjecture that it may not be necessary to obtain a computationally efficient algorithm. This conjecture is based on the observation that there exists in fact an easy way to obtain at least the first-order bound (2) without the log-concavity assumption, but with an algorithm that has no hope of being efficiently implemented. As described in Section 2.2, this is possible by running the MYGA algorithm (Allen-Zhu et al., 2018) on \(O(})^{Kd}\) experts that cover the set of linear classifiers to sufficient precision. The run-time of this approach is prohibitive, because it scales linearly with the number of experts, which is a large polynomial in \(T\).

Techniques.The LinExp3 method of Neu and Olkhovskaya (2020) is based on an adaptation of the classic Exp3 algorithm for regular multi-armed bandits (Auer et al., 2002a). A natural approach would therefore be to replace the Exp3 component in LinExp3 by a method with first-order guarantees for the multi-armed bandit setting, but, as discussed in Section D, this leads to difficulties controlling the variance. Instead of building on Exp3, we therefore follow the perhaps surprising approach of building our algorithm on _continuous exponential weights_ over the probability simplex (van der Hoeven et al., 2018). In particular, our approach is based on a combination of the recently proposed techniques of Ito et al. (2020) for linear bandits with tools designed by Neu and Olkhovskaya (2020) to deal with the contextual case.

Outline.The rest of the paper is organized as follows. After describing the setting in the next section, we state a formal version of the simple first-order bound that can be obtained using the MYGA algorithm (Theorem 2.1). This is followed by Section 3, which states our main results corresponding to the regret bounds in Equations 1 and 2. Section 4 then gives a high-level overview of the proofs, with pointers provided to the details in the appendix. Finally, Section 5 concludes with discussion.

## 2 Preliminaries

NotationLet \(^{K}=\{w^{K}|w_{1} 0,,w_{K} 0,_{a=1}^{K}w_{a }=1\}\) denote the \((K-1)\)-dimensional probability simplex. For any positive semi-definite matrix \(M^{d d}\), \(\|v\|_{M}=Mv}\) denotes the corresponding Mahalanobis norm, and for any positive integer \(n\), we abbreviate \([n]=\{1,,n\}\).

### Setting

We consider the setting of (Neu and Olkhovskaya, 2020), in which there is an interaction between a learner and an unknown environment. This interaction proceeds in rounds indexed by \(t[T]\), such that for each \(t\):

1. The environment commits to \([K]\) parameter vectors \(_{t,1},,_{t,K}^{d}\) without revealing any to the learner.
2. A context vector \(X_{t}^{d}\) is drawn i.i.d. from some fixed distribution \(\) according to \(X_{t}\), and revealed to the learner.
3. The learner commits to an action \(A_{t}[K]\), and incurs the loss \(_{t}(X_{t},A_{t})\), where \(_{t}(X,a)= X,_{t,a}\).

The environment is allowed to randomize its choices of \(_{t,a}\). These must be independent from the context \(X_{t}\) in round \(t\), but they may depend on previous contexts \(X_{s}\) and actions \(A_{s}\) for \(s<t\).

We write \(_{t}(a|X_{t})\) for the policy of the learner in round \(t\) conditional on observing context \(X_{t}\), so that \(A_{t}_{t}(X_{t})\), and we use the following notation for the expected cumulative losses of the algorithm and policy \(\), respectively:

\[L_{T}=[_{t=1}^{T}_{t}(X_{t},A_{t})],L_{T}^{}= [_{t=1}^{T}_{t}(X_{t},(X_{t}))].\]Let \(\) be the set of all all stationary deterministic policies \(:^{d}[K]\), we define the optimal policy \(^{*}\) as \(^{*}=_{}L_{T}^{}\). Then the learner's goal is to compete with policy \(^{*}\), as measured by the expected regret:

\[R_{T}=L_{T}-L_{T}^{^{*}}=[_{t=1}^{T} X_{t}, _{t,A_{t}}-_{t,^{*}(X_{t})}],\]

where the expectation is taken over each \(X_{t}\), and any randomness applied by the learner or environment in their respective choices. Using the linearity of the loss functions it can be shown that the optimal policy is always a linear classifier (Neu and Olkhovskaya, 2020):

\[_{T}^{*}(x)=_{a} x,_{t=1}^{T}[ _{t,a}].\]

We may therefore restrict attention to competing with policies of the form

\[_{}(x)=_{a} x,_{a}( ^{K d}).\] (3)

For deriving our technical results, it will be useful to define the filtration \(_{t}=(\{X_{s},A_{s}:s t\})\), and the notations \(_{t}[]=[|\,_{t-1}]\) and \(_{t}[]=[|\,_{t-1}]\).

AssumptionsFollowing Neu and Olkhovskaya (2020), we assume that \(\|X_{t}\|\), \(\|_{t,a}\| R\) and \(_{t}(x,a)[-1,1]\) almost surely. In addition, the covariance matrix \(=[XX^{}]\) of the context distribution is assumed to be positive definite, with smallest eigenvalue \(_{}()>0\).

### An Inefficient Algorithm

A first order bound for our problem can be obtained by instantiating the MYGA algorithm of Allen-Zhu et al. (2018) for a set of \((d})^{Kd}\) experts that cover the parameter space of policies of the form (3), which is guaranteed to contain the optimal policy \(_{T}^{*}\):

**Theorem 2.1**.: _Suppose that \(0_{t}(a,X_{t}) 1\) almost surely for all \(a[K]\). Then, by instantiating MYGA with \((d})^{Kd}\) experts, it obtains the following first-order bound for the adversarial linear contextual bandit problem:_

\[R_{T}=O(K^{*} T}+K^{2}d T).\] (4)

Although this provides a quick way to see that first-order bounds are possible, the resulting algorithm is completely impractical, because its run-time is proportional to the number of experts, which grows as a large polynomial in \(T\). The proof, including a more detailed description of the experts, can be found in Appendix A.

## 3 First- and Second-Order Bounds

In this section we present an algorithm using a novel adaptation of a method developed for the adversarial linear bandit to be suitable for use in the adversarial linear contextual bandit setting. The method proposed is based on a form of continuous exponential weights that has been shown to lead to a first-order bound in the former (Ito et al., 2020). The algorithm allows for optimistic estimates \(m_{t,a}^{d}\) for the environment's choices \(_{t,a}\), which can always be set to \(0\) when they are not available. We show two types of guarantees. First, in Theorem 3.1, we obtain a second-order regret bound in terms of the cumulative squared error of the estimates \(m_{t,a}\):

\[V_{T}=[_{s=1}^{T} X_{s},_{s,A_{s}}-m_{s, A_{s}}^{2}].\] (5)

Taking \(m_{t,a}=0\), this provides a second-order regret bound in terms of the squared losses. Alternatively, \(m_{t,a}\) may be estimated using an online regression algorithm, as described by Ito et al. (2020). As our second result, we show in Theorem 3.2 that a first-order bound can be derived for the same algorithm with a different choice of hyperparameters and the assumption that the losses are non-negative.

### Algorithm Description

Our full algorithm is shown in Algorithm 1. As it is an adaptation of continuous exponential weights for the contextual bandits setting, we refer to it as ContextEW. It runs a two-stage sampling procedure: after observing context \(X_{t}\), the first stage of the algorithm samples a random policy \(_{t}^{K}\), and then the second stage consists of drawing an arm \(A_{t}\) randomly from \(_{t}\). The distribution of \(_{t}\) is constructed as follows: first we sample a different policy \(Q_{t}\) from the exponential weights distribution over the probability simplex with density proportional to

\[w_{t}(q|X_{t})=(-_{t}_{a=1}^{K}q_{a} X_{t},_{s=1}^{t-1} _{s,a}+m_{t,a}),\] (7)

where \(m_{s,a}\) is a function that is measurable with respect to \(_{s-1}\). The sum \(_{a=1}^{K}q_{a} X_{t},_{s=1}^{t-1}_{s,a}\) estimates the cumulative loss that the policy \(q\) would have incurred if it had been played in all previous rounds. It relies on estimates \(_{s,a}\) of the loss vectors \(_{s,a}\), which will be defined below, and a time-varying learning rate \(_{t}>0\), which is hyperparameter of the algorithm. The normalized density function corresponding to the weights in (7) is:

\[p_{t}(q|X_{t})=(q|X_{t})}{_{^{K}}w_{t}(q|X_{t})dq}.\] (8)

Following Ito et al. (2020), we then introduce a rejection sampling step (6) to reduce the variance, which is based on the following covariance matrices \(_{t,a}\) corresponding to \(Q_{t}\):

\[_{t,a}=_{t}[Q_{t,a}^{2}X_{t}X_{t}^{}],\] (9)

so that \(_{t}\) ends up being sampled according to the following truncated exponential weights density:

\[_{t}(q|X_{t})=(q|X_{t})\{_{a=1}^{K} \|q_{a}X_{t}\|_{_{t,a}^{-1}}^{2} dK^{2}\}}{_{t }[_{a=1}^{K}\|q_{a}X_{t}\|_{_{t,a}^{-1}}^{2} dK^{2}|X _{t}]},\] (10)

with truncation level hyperparameter \(>0\). We will show that all \(_{t,a}\) are invertible, as are their analogues in which \(Q_{t}\) is replaced by \(_{t}\):

\[_{t,a}=_{t}[_{t,a}^{2}X_{t}X_{t }^{}].\] (11)

It remains to specify our estimators for \(_{t,a}\), which are defined as follows:

\[_{t,a}=m_{t,a}+_{t,a}_{t,a}^{- 1}X_{t}( X_{t},_{t,a}- X_{t},m_{t,a} )\{A_{t}=a\}.\] (12)These estimates can be shown to be unbiased:

\[_{t}[_{t,a}] =m_{t,a}+_{t,a}^{-1}_{t}[ {Q}_{t,a}X_{t}X_{t}^{}\{A_{t}=a\}]( _{t,a}-m_{t,a})\] \[=m_{t,a}+_{t,a}^{-1}_{t}[ _{t,a}^{2}X_{t}X_{t}^{}](_{t,a}-m_{t,a})= _{t,a}.\]

### Results

We instantiate ContextEW with adaptive learning rates \(_{t}\). For our second-order result, these are defined in terms of the empirical counterpart to \(V_{t}\): \(_{t}=_{s=1}^{t} X_{s},_{s,A_{s}}-m_{s,A_{s}} ^{2},\) and we abbreviate \(G_{t}=8_{t-1}(2T^{2})+144^{2}T+176 T}.\) Then we set

\[_{t}=(100dK^{2}+d(_{t-1}+1+G_{t-1}))^{-1/2}.\] (13)

This leads to the following second-order bound:

**Theorem 3.1** (Second-Order).: _Suppose \(\) has a log-concave density. Then, for \(=4(10dKT)\), \(_{t}\) as in (13) and any \(_{t-1}\)-measurable estimates \(m_{t}\), the expected regret of ContextEW is at most \(R_{T}=(K_{T}}).\)_

To tune \(_{t}\) adaptively for our first-order bound, we define it using the algorithm's empirical cumulative loss \(_{t}=_{s=1}^{t}_{t}(X_{s},A_{s}),\) which acts as a self-confident empirical estimate of \(L_{T}^{*}\). We further abbreviate

\[H_{t}=8_{t} T+40^{2}T}+72 T,\] (14)

and then set

\[_{t}=(100d^{2}+dK(_{t-1}+1+H_{t-1}))^{-1/2}.\] (15)

This leads to the following first-order bound:

**Theorem 3.2** (First-Order).: _Suppose that \(\) has a log-concave density and that \(0_{t}(a,X_{t}) 1\) almost surely for all \(a[K]\). Then, for \(=4(10dKT)\), \(_{t}\) as in (15) and \(m_{t}=0\), the expected regret of ContextEW is at most \(R_{T}=(K^{*}}).\)_

### Computational Efficiency

The two computational bottlenecks in the algorithm are the cost of sampling from the output distribution \(p_{t}(q|X_{t})\) and computation of the covariance matrices \(_{t,a}\) in each round.

Due to the log-linearity of our method, there exists several practical methods of sampling. As mentioned in Ito et al. (2020), one can employ the methods of Lovasz and Vempala (2007), which was shown in Lovasz and Vempala (2006) to enjoy a bound of \(O(K^{4}(1/))\) (where \(\) is a bound on the total variation distance between the output distribution and the target), but this still requires knowledge of a density dominating the target distribution on all but a set with total starting mass \(/2\). In Narayanan and Rakhlin (2017), a method is developed for general log-concave distributions which, specialized to log-linear distributions (and without additional assumptions on the initial distribution) yields an \(O(K^{3}^{2}+(1/))\) method when the geometry admits a \(\)-self concordant barrier. Since there always exists a \(K\)-self-concordant barrier for a \(K\)-dimensional convex body, and thus the running time of this method for our problem is \(O(K^{5}+ T)\) up to a precision \(}\) for some \(>0\). As referred to in Ito et al. (2020), the covariance matrix \(_{t,a}\) is computable in \(((d/)^{O(1)})\) sampling steps drawing upon the results of Lovasz and Vempala (2007).

## 4 Analysis

In this section we provide the analysis of ContextEW from which Theorems 3.1 and 3.2 follow. Throughout the analysis, we will be extensively using the following property of log-concave distributions:

**Lemma 4.1**.: _If \(x\) follows a log-concave distribution \(p\) over \(^{d}\) and \([xx^{}] I\), we have, for any \( 0:\)_

\[[\|x\|_{2}^{2} d^{2}] d(1- ).\] (16)This result was proven in Lemma 1 in Ito et al. (2020), and also follows from Lemma 5.7 in Lovasz and Vempala (2007).

First, we need to introduce some notation which will be useful for the reduction to the linear bandit setting and for the accompanying proofs. We denote \(z_{a}(q,x)=q_{a}x\) and \(z(q,x)=(z_{1}(q,x),,z_{K}(q,x))^{}\). We also define \(_{t}=_{[K]}(_{a,t})\) as a block diagonal arrangement of the covariance matrices per arm. Using this notation, the distribution of the sampling algorithm (10) may be rewritten as

\[_{t}(q|x)=(q|x)\{\|z(q,x)\|_{ _{t}^{-1}}^{2} dK^{2}\}}{_{t}[\|z(q, x)\|_{_{t}^{-1}}^{2} dK^{2}]}.\] (17)

Let \(_{t}(x)_{t}(q|x)\), \(Q_{t}(x) p_{t}(q|x)\) and \(_{t}(x)=z(_{t}(x),x)\), \(Z_{t}(x)=z(Q_{t}(x),x)\), \(Z^{*}(x)=z(^{*}(x),x)\). And we denote the aggregated loss parameter \(_{t}=(_{1},,_{K})^{}\) and its estimate \(_{t}=(_{1},,_{K})^{}\). Then we can express the regret as follows:

\[R_{T}=[_{t=1}^{T}_{t}(X_{t},A_{t})-_{t}(X_{t},^{ *}(X_{t}))]=[_{t=1}^{T}_{t}(X_ {t})-Z^{*}(X_{t}),_{t}].\] (18)

The crucial observation is that the log-concavity of the distribution of \(Z_{t}(X_{t})\) follows from that of the distribution of \(X_{t}\):

**Lemma 4.2**.: _Suppose \(z(q,x)=_{a}q_{a}(x,a)\) for \((x,a)=(^{},,x^{},)\) such that \(x\) is on the \(da\)'th co-ordinate and \(Q(x) p_{t}(|x)\) for \(p_{t}(|x)\) defined in (8). If \(X p_{X}()\) and \(p_{X}()\) is log-concave and \(Z(x)=z(Q_{t}(x),x)\), then \(Z(X)\) also follows a log-concave distribution._

The proof of this result is a rather straightforward computation of the density of \(Z_{t}(X_{t})\) and can be found in Appendix C. To proceed, we write regret as a sum of two terms

\[R_{t}=[_{t=1}^{T}_{t}(X_{t})-Z_{t }(X_{t}),_{t}]+[_{t=1}^{T}  Z_{t}(X_{t})-Z^{*}(X_{t}),_{t}].\] (19)

Having shown that \(Z_{t}(X_{t})\) is log-concave, and since the log-concavity is preserved under linear transformations, for \(y=_{t}^{-1/2^{}}Z_{t}(X_{t})\) we can see that \([yy^{}]=I\), and thus by Lemma 4.1 it immediately follows that the probability that (6) is not satisfied is small for a proposed choice of \(=4(10dKT)\):

\[_{t}[\|Z_{t}(X_{t})\|_{_{t}^{-1}}^{2}>dK ^{2}] dK(1-) 3dK(-)}.\]

Using this observation, we show that the first term of (19) is just \((1)\), which is formally proved in Lemma C.2 in the appendix.

To control the second term of the regret decomposition (19), consider the reduction of the contextual bandit problem to a combination of auxiliary online learning problems that are defined separately for each context, as proposed in Neu and Olkhovskaya (2020), Lemma 3. More details and a full proof can be found in Appendix C.

**Lemma 4.3**.: _Let \(^{*}\) be any fixed stochastic policy and let \(X_{0}\) be a sample from the context distribution independent from \(_{T}\). Suppose that \(p_{t}_{t-1}\), such that \(p_{t}(|x)\) is a probability density with respect to Lebesgue measure with support \(^{K}\) and let \(Q_{t}(x) p_{t}(|x)\). Then,_

\[_{t}[ Z_{t}(X_{t})-Z^{*}(X_{t}),_{t} ]=_{t}[ Z_{t}(X_{0})-Z^{*}(X_{0}),_{t}].\] (20)

To see why this would be useful further in the proof, we interpret the right-hand side of (20) as follows. Consider the online learning problem for a fixed \(x\) with the decision set to be \(^{K}\) and losses \(_{t}(x,q)= z(q,x),_{t}\) and consider running a version of a contextual bandit problem with a fixed context \(x\), such that the probability of an action \(q\) defined as in Equation 8, so \(p_{t}(q|x)(-_{t}_{a=1}^{K}q_{a} x,_{s=1} ^{t-1}_{s,a})\). Then, the regret for the fixed \(x\) against \(^{*}(x)\) can be written as:

\[_{T}(x)=_{t=1}^{T}_{Q_{t}(x) p_{t}(|x)}[  z(Q_{t}(x),x)-z(^{*}(x),x),_{t} ].\]Then it is easy to see that the right-hand side of (20) is equal to \([_{T}(X_{0})]\). Thus, we first show a bound on \(_{T}(x)\) that holds almost surely for any \(x\) and then take an expectation with respect to \(X_{0}\). We control the regret \(_{T}(x)\) by following the general schema of the optimistic mirror descent analysis developed in (Rakhlin and Sridharan, 2013; Ito et al., 2020). With this analysis, we get the following bound for any \(x\) :

**Lemma 4.4**.: _Assume that \(_{t+1}_{t}\) for all \(t\), let \(q_{0}\) be a uniform distribution over \([K]\) and \((y)=(y)-y-1\). Then, the regret \(_{T}(x)\) of ContextEW almost surely satisfies_

\[_{T}(x) _{t=1}^{T} z(q_{0}-^{*}(x),x), \ _{t}+}\] \[+_{t=1}^{T}}_{Q_{t}(x) p_{t}( |x)}[(-_{t} z(Q_{t}(x),x), _{t}-m_{t})],\] (21)

_for \((y)=(y)-y-1\)._

We place the derivation of the this bound in the appendix. The crucial ingredient is to show that the square of the estimated loss can be bounded by the square of the true loss. Using the definition of \(_{t}\), denoting \(Var_{t}=(_{t}^{-1}Z_{t}(X_{0})Z_{t}(X_{0})^{ }_{t}^{-1}Z_{t}(X_{t})Z_{t}(X_{t})^{})\), we get

\[_{t}[(-_{t} Z_{t}(X_{0}), _{t}-m_{t})^{2}]=_{t}[ _{t}^{2}(_{t}(A_{t},X_{t})-X_{t}^{}m_{t,A_{t}})^ {2}Var_{t}],\] (22)

As additional corollary of the concentration result for log-concave random variables, we can show the following relation between matrices \(_{t}\) and \(_{t}\):

\[_{t}_{t}_{t},\] (23)

which we prove in Lemma C.2 in the appendix. Then we can show that, almost surely:

\[_{X_{0}}[Var_{t}]=_{X_{0}}[ (_{t}^{-1}Z_{t}(X_{0})Z_{t}(X_{0})^{ }_{t}^{-1}Z_{t}(X_{t})Z_{t}(X_{t})^{} )]\] \[=(_{t}^{-1}_{t}_{t}^{-1}Z_{t}(X_{t})Z_{t}(X_{t})^{}) (_{t}^{-1}_{t}_{t}^{-1}Z_{t}(X_{t})Z_{t}(X_{t})^{})\] \[=Z_{t}(X_{t})^{}_{t}^{-1}Z _{t}(X_{t}) Z_{t}(X_{t})^{}_{t}^{-1}Z_{t}(X_{t}) dK ^{2}.\] (24)

where the first inequality follows from (23) and the second inequality is immediate from (23) and the fact that for symmetric positive definite matrices \(A B\) follows from \(B^{-1} A^{-1}\). The last inequality follows from (6) in the ContextEW. So, from (22) and (35), we get

\[_{t}[(-_{t} Z_{t}(X_{0}), _{t}-m_{t})^{2}] dK^{2} _{t}[_{t}^{2}(_{t}(A_{t},X_{t})-X_{t}^{}m_ {t,A_{t}})^{2}],\]

which, as we stated above, is the key step to prove Theorem 3.1.

First-order regret boundTo prove result of Theorem 3.2, we show that the bound in the Theorem 3.1 can instantiated to obtain a first-order regret bound with a different choice of the learning rate \(_{t}\). Going along the same lines with regard to the concentration of \(_{t}\) as for \(_{t}\), by setting \(m_{t}=0\) and noticing that then \(V_{T} L_{T}\) we get

\[R_{T} 2dK^{2}[_{t=1}^{T}_{t}_{t}(A_{t},X_{t} )^{2}]+}(K}) 4K^{2} }+}(K}).\]

Since \(R_{T}=L_{t}-L_{T}^{*}\), by solving the quadratic inequality with respect to \(L_{T}^{*}\), we get that \(L_{T} L_{T}^{*}+}(K)\), yielding the final bound.

## 5 Discussion

In conclusion, by applying the approach of Ito et al. (2020) we have constructed the first scheme achieving \((K^{*}})\) regret with a runtime of \(((K^{5}+ T) g_{})\), where \(g_{}\) is the time taken to construct the covariance matrix per round - a potentially large polynomial improvement over the \((T^{Kd})\) runtime of MYGA. The application of linear bandit algorithms to the contextual bandit problem constitutes, to the best of our knowledge, a novel approach. In doing so we've found a number of positive aspects, including efficiency, but also the direct applicability of other properties enjoyed by the algorithm such as second order bounds Ito et al. (2020).

Our approach is based on reducing the linear contextual bandit problem to a linear bandit problem, as opposed to a multi-armed bandit problem as in Neu and Olkhovskaya (2020). While the specifics of this reduction heavily relied on the joint log-concavity of the context distributions and the exponential-weights posterior over the simplex of actions, we wonder if such approaches can be successfully applied to achieve other types of improvements for linear contextual bandits. In particular, it is curious to what extent other recent advances in the linear bandit problem can be translated to the linear contextual bandit setting. Note that, while the truncation step in Algorithm 1 has an insignificant computational cost as the condition is satisfied with probability \((1-1/T)\), it can be removed by paying a \((1/_{min}())\) multiplicative term in the regret by implementing additional exploration with probability \(1/T\). It is natural to ask whether or not approaches based on other instantiations of online mirror descent would also yield first-order bounds, and possibly improve the dependence on \(K\). The answer is not obvious: for an example of how a naive application of an instantiation of FTRL fails to achieve a first-order bound, see Appendix D.

A relevant question pertains to whether or not such an application of algorithms for linear bandits is necessary at all, but standard approaches such as direct adaptation of Exp3, and first-order adaptations thereof such as GREEN Allenberg et al. (2006) do not seem to give the desired result.In addition, thresholding the worst performing arms inevitably biases the loss estimator due to undersampling of those arms for which the threshold has been applied, and the resulting additional bias term picked up in the regret scales with \(1/_{}(_{t,a})\), which may be arbitrarily large. Another standard approach of finding an optimistic estimator yielded no fruit during the course of this study due to the lack of the existence of such an estimator without saving all previous losses explicitly.

Our algorithm achieves the regret bound \((K})\), while the worst case guarantee of LinExp3 of Neu and Olkhovskaya (2020) is \(()\). This discrepancy is not surprising as the Algorithm 1 of Ito et al. (2020) scales as \((n)\) (\(n\) being the dimension of the action space for the linear bandit), which arises from the deployment of continuous exponential weights. MYGA achieves the same \((K^{*}})\) bound due to the number of experts needed to cover the joint set of additive loss parameters. It is worth here emphasising that no known algorithm achieves a better dependence on \(K\) than \((K^{*}})\) for the linear adversarial contextual bandit problem. Meanwhile, if the linear bandit is played on the \(n\)-simplex, an improvement to \(\) is possible. For further discussion of this point, see Section 28.5 of Lattimore and Szepesvari (2020). It is thus still unclear whether or not the extra factor of \(\) is necessary if one aims for a first-order bound.

An additional point is that while the MYGA algorithm Allen-Zhu et al. (2018) allows for adversarially chosen contexts, the analysis of MYGA for our setting relies heavily on the assumption that contexts are drawn i.i.d. at each iteration. A natural question is then whether or not a similar result is achievable in the adversarial context case. It is known that achieving sub-linear regret is not possible even for full-information online learning of one-dimensional threshold classifiers when both contexts and losses are adversarial Ben-David et al. (2009); Syrgkanis et al. (2016), which renders sub-linear regret similarly impossible to guarantee for the even harder setting that we consider in this paper. However, we do conjecture that we could overcome the assumption that the distribution is known or that we can sample from it by employing a more elaborate algorithm to estimate the distribution from the data. Indeed, it is not obvious if the distributional assumption of a lower bound to the covariance matrix eigenvalues is entirely necessary, since the regret does not depend on this.

Lastly, it would be an interesting challenge to see if a high-probability regret bound could be obtained in the form stated in the COLT 2017 open problem Agarwal et al. (2017) for this setting, but since a high-probability \(O()\) has not yet been proved for the problem here considered, the latter may be more worthy of focus in the short term.

## 6 Acknowledgments

Tim van Erven and Jack Mayo were supported by the Netherlands Organization for Scientific Research (NWO) under grant number VI.Vidi.192.095. Gergely Neu was supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (Grant agreement No. 950180). Chen-Yu Wei would like to acknowledge the support from Simons-Berkeley Research Fellowship.