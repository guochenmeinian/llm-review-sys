# OKRidge: Scalable Optimal k-Sparse

Ridge Regression

 Jiachang Liu, Sam Rosen, Chudi Zhong, Cynthia Rudin

Duke University

{jiachang.liu, sam.rosen, chudi.zhong}@duke.edu,

cynthia@cs.duke.edu

###### Abstract

We consider an important problem in scientific discovery, namely identifying sparse governing equations for nonlinear dynamical systems. This involves solving sparse ridge regression problems to provable optimality in order to determine which terms drive the underlying dynamics. We propose a fast algorithm, OKRidge, for sparse ridge regression, using a novel lower bound calculation involving, first, a saddle point formulation, and from there, either solving (i) a linear system or (ii) using an ADMM-based approach, where the proximal operators can be efficiently evaluated by solving another linear system and an isotonic regression problem. We also propose a method to warm-start our solver, which leverages a beam search. Experimentally, our methods attain provable optimality with run times that are orders of magnitude faster than those of the existing MIP formulations solved by the commercial solver Gurobi.

## 1 Introduction

We are interested in identifying sparse and interpretable governing differential equations arising from nonlinear dynamical systems. These are scientific machine learning problems whose solution involves sparse linear regression. Specifically, these problems require the _exact_ solution of sparse regression problems, with the most basic being sparse ridge regression:

\[_{}-_{2}^{2}+_{2}_{2}^{2}\ \ _{0} k,\] (1)

where \(k\) specifies the number of nonzero coefficients for the model. This formulation is general, but in the case of nonlinear dynamical systems, the outcome \(y\) is a derivative (usually time or spatial) of each dimension \(x\). Here, we assume that the practitioner has included the true variables, along with many other possibilities, and is looking to determine which terms (which transformations of the variables) are real and which are not. This problem is NP-hard , and is more challenging in the presence of highly correlated features. Selection of correct features is vital in this context, as many solutions may give good results on training data, but will quickly deviate from the true dynamics when extrapolating past the observed data due to the chaotic nature of complex dynamical systems.

Both heuristic and optimal algorithms have been proposed to solve these problems. Heuristic methods include greedy sequential adding of features  or ensemble  methods. These methods are fast, but often get stuck in local minima, and there is no way to assess solution quality due to the lack of a lower bound on performance. Optimal methods provide an alternative, but are slow since they must prove optimality. MIOSR , a mixed-integer programming (MIP) approach, has been able to certify optimality of solutions given enough time. Slow solvers cause difficulty in performing cross-validation on \(_{2}\) (the \(_{2}\) regularization coefficient) and \(k\) (sparsity level).

We aim to solve sparse ridge regression to certifiable optimality, but in a fraction of the run time. We present a fast branch-and-bound (BnB) formulation, OKRidge. A crucial challenge is obtaining atight and feasible lower bound for each node in the BnB tree. It is possible to calculate the lower bound via the SOS1 , big-M , or the perspective formulations (also known as the rotated second-order cone constraints) [32; 4; 59]; the mixed-integer problems can then be solved by a MIP solver. However, these formulations do not consider the special mathematical structure of the regression problem. To calculate a lower bound more efficiently, we first propose a new saddle point formulation for the relaxed sparse ridge regression problem. Based on the new saddle point formulation, we propose two novel methods to calculate the lower bound. The first method is extremely efficient and relies on solving only a linear system of equations. The second method is based on ADMM and can tighten the lower bound given by the first method. Together, these methods give us a tight lower bound, used to prune nodes and provide a small optimality gap. Additionally, we propose a method based on beam-search  to get a near-optimal solution quickly, which can be a starting point for both our algorithm and other MIP formulations. Unlike previous methods, our method uses a dynamic programming approach so that previous solutions in the BnB tree can be used while exploring the current node, giving a massive speedup. In summary, our contributions are:

(1) We develop a highly efficient customized branch-and-bound framework for achieving optimality in \(k\)-sparse ridge regression, using a novel lower bound calculation and heuristic search.

(2) To compute the lower bound, we introduce a new saddle point formulation, from which we derive two efficient methods (one based on solving a linear system and the other on ADMM).

(3) Our warm-start method is based on beam-search and implemented in a dynamic programming fashion, avoiding redundant calculations. We prove that our warm-start method is an approximation algorithm with an exponential factor tighter than previous work.

On benchmarks, OKRidge certifies optimality orders of magnitude faster than the commercial solver Gurobi. For dynamical systems, our method outperforms the state-of-the-art certifiable method by finding superior solutions, particularly in high-dimensional feature spaces.

## 2 Preliminary: Dual Formulation via the Perspective Function

There is an extensive literature on this topic, and a longer review of related work is in Appendix A. If we ignore the constant term \(^{T}\), we can rewrite the loss objective in Equation (1) as:

\[_{}():=^{T} ^{T}-2^{T} +_{2}_{j=1}^{p}_{j}^{2},\] (2)

with \(p\) as the number of features. We are interested in the following optimization problem:

\[_{}_{}()\; \;\;\;(1-z_{j})_{j}=0,\;\;_{j=1}^{p}z_{j} k,\;\;z_{j} \{0,1\},\] (3)

where \(k\) is the number of nonzero coefficients. With the sparsity constraint, the problem is NP-hard. The constraint \((1-z_{j})_{j}\) in Problem (3) can be reformulated with the SOS1, big-M, or the perspective formulation (with quadratic cone constraints), which then can be solved by a MIP solver. Since commercial solvers do not exploit the special structure of the problem, we develop a customized branch-and-bound framework.

For any function \(f(a)\), the perspective function is \(g(a,b):=bf()\) for the domain \(b>0\)[32; 34; 26] and \(g(a,b)=0\) otherwise. Applying to \(f(a)=a^{2}\), we obtain another function \(g(a,b)=}{b}\). As shown by , replacing the loss term \(_{j}^{2}\) and constraint \((1-z_{j})_{j}=0\) with the perspective formula \(_{j}^{2}/z_{j}\) in Problem (3) would not change the optimal solution. By the Fenchel conjugate , \(g(,)\) can be rewritten as \(g(a,b)=_{c}ac-}{4}b\). If we define a new perspective loss as:

\[_{}^{}(,,):=^{T}^{T} -2^{T}+ _{2}_{j=1}^{p}(_{j}c_{j}-^{2}}{4}z_{j}),\] (4)

then we can reformulate Problem (3) as:

\[_{,}_{c}_{}^{ }(,,)\;\;\;\;_{j=1}^{p}z_{j} k,\;\;z_{j}\{0,1\}.\] (5)If we relax the binary constraint \(\{0,1\}\) to the interval \(\) and swap \(\) and \(\) (no duality gap, as pointed by ), we obtain the dual formulation for the convex relaxation of Problem (5):

\[_{}_{,}_{}^{ }(,,)\ \ _{j=1}^{p}z_{j} k,\ \ z_{j}.\] (6)

While  uses the perspective formulation for safe feature screening, we use it to calculate a lower bound for Problem (3). However, directly solving the maxmin problem is computationally challenging. In Section 3.1, we propose two methods to achieve this in an efficient way.

## 3 Methodology

We propose a custom BnB framework to solve Problem (3). We have 3 steps to process each node in the BnB tree. First, we calculate a lower bound of the node, using two algorithms proposed in the next subsection. If the lower bound exceeds or equals the current best solution, we have proven that it does not lead to any optimal solution, so we prune the node. Otherwise, we go to Step 2, where we perform beam-search to find a near-optimal solution. In Step 3, we use the solution from Step 2 and propose a branching strategy to create new nodes in the BnB tree. We continue until reaching the optimality gap tolerance. Below, we elaborate on each step. In Appendix E, we provide visual illustrations of BnB and beam search as well as the complete pseudocodes of our algorithms.

### Lower Bound Calculation

#### Tight Saddle Point Formulation

We first rewrite Equation (2) with a new hyperparameter \(\):

\[_{-}(,):=^{T}_{ }-2^{T}+(_{2}+)_{j=1 }^{p}_{j}^{2},\] (7)

where \(_{}:=^{T}-\). We restrict \([0,_{}(^{T})]\), where \(_{}()\) denotes the minimum eigenvalue of a matrix. We see that \(_{}\) is positive semidefinite, so the first term remains convex. This trick is related to the optimal perspective formulation [62; 28; 37], but we set the diagonal matrix \(()\) in  to be \(\). We call this trick the eigen-perspective formulation. The optimal perspective formulation requires solving semidefinite programming (SDP) problems, which have been shown not scalable to high dimensions , and MI-SDP is not supported by Gurobi.

Solving Problem (3) is equivalent to solving the following problem:

\[_{,}\ _{-}(, )\ \ (1-z_{j})_{j}=0,\ _{j=1}^{p}z_{j} k,\ \ z_{j}\{0,1\}.\] (8)

We get a continuous relaxation of Problem (3) if we relax \(\{0,1\}\) to \(\).

We can now define a new loss analogous to the loss defined in Equation (4):

\[_{-}^{}(,,): =^{T}_{}-2^{T}+( _{2}+)_{j=1}^{p}(_{j}c_{j}-^{2}}{4}z_{j}).\] (9)

Then, the dual formulation analogous to Problem (6) is:

\[_{}_{,}_{- }^{}(,,)\ \ _{j=1}^{p}z_{j} k,\ \ z_{j}.\] (10)

Solving Problem (10) provides us with a lower bound to Problem (8). More importantly, this lower bound becomes tighter as \(\) increases. This novel formulation is the starting point for our work.

We next propose a reparametrization trick to simplify the optimization problem above. For the inner optimization problem in Problem (10), given any \(\), the optimality condition for \(\) is (take the gradient with respect to \(\) and set the gradient to \(\)):

\[=+}(^{T}-_{}).\] (11)

Inspired by this optimality condition, we have the following theorem:

**Theorem 3.1**.: _If we reparameterize \(=+}(^{T}-_{})\) with a new parameter \(\), then Problem (10) is equivalent to the following saddle point optimization problem:_

\[_{}_{}^{}_{- }(,)\ \ _{j=1}^{p}z_{j} k,\ \ z_{j},\] (12)

_where \(^{}_{-}(,):=- ^{T}_{}-+}(^{T}-_{})^{T}diag()(^{T}-_{ }),\)_

_and \(diag()\) is a diagonal matrix with \(\) on the diagonal._

To our knowledge, this is the first time this formulation is given. Solving the saddle point formulation to optimality in Problem (12) gives us a tight lower bound. However, this is computationally hard.

Our insight is that we can solve Problem (12) approximately while still obtaining a feasible lower bound. Let us define a new function \(h()\) as short-hand for the inner minimization in Problem (12):

\[h()=_{}^{}_{- }(,)\ \ _{j=1}^{p}z_{j} k,\ \ z_{j}.\] (14)

For any arbitrary \(^{p}\), \(h()\) is a valid lower bound for Problem (3). We should choose \(\) such that this lower bound \(h()\) is tight. Below, we provide two efficient methods to calculate such a \(\).

#### Fast Lower Bound Calculation

First, we provide a fast way to choose \(\). The choice of \(\) is motivated by the following theorem:

**Theorem 3.2**.: _The function \(h()\) defined in Equation (14) is lower bounded by_

\[h()-^{T}_{}- +}\|^{T}-_{}\| _{2}^{2}.\] (15)

_Furthermore, the right-hand size of Equation (15) is maximized if \(=}=*{argmin}_{} _{}()\), where in this case, \(h()\) evaluated at \(}\) becomes_

\[h(})=_{}(})+(_{ 2}+)_{p-k}(\{_{j}^{2}\}),\] (16)

_where \(_{p-k}()\) denotes the summation of the smallest \(p-k\) terms of a given set._

Here we provide an intuitive explanation of why \(h(})\) is a valid lower bound. Note that the ridge regression loss is strongly convex. Assuming that the strongly convex parameter is \(\) (see Appendix B), by the strong convexity property, we have that for any \(^{p}\),

\[_{}() _{}(})+_{ }(})^{T}(-})+\|-}\|_{2}^{2}.\] (17)

Because \(}\) minimizes \(_{}()\), we have \(_{}(})=\). For the \(k\)-sparse vector \(\) with \(\|\|_{0} k\), the minimum for the right-hand side of Inequality (17) can be achieved if \(_{j}=_{j}\) for the top \(k\) terms of \(_{j}^{2}\)'s. This ensures the bound applies for all \(k\)-sparse \(\). Thus, the \(k\)-sparse ridge regression loss is lower-bounded by

\[_{}() _{}(})+_{p-k}(\{_{j}^{2}\})\]

for \(^{p}\) with \(\|\|_{0} k\). For ridge regression, the strong convexity \(\) parameter can be chosen from \([0,2(_{2}+_{}(^{T}))]\). If we let \(=2(_{2}+)\), we obtain \(h(})\) in Theorem 3.2.

The lower bound \(h(})\) can be calculated extremely efficiently by solving the ridge regression problem (solving the linear system \((^{T}+_{2})=^{T}\) for \(\)) and adding the extra \(p-k\) terms. However, this bound is not the tightest we can achieve. In the next subsection, we discuss how to apply ADMM to maximize \(h()\) further based on Equation (14).

#### Tight Lower Bound via ADMM

Let us define \(:=^{T}-_{}\). Starting from Problem (12), if we minimize \(\) in the inner optimization under the constraints \(_{j=1}^{p}z_{j} k\) and \(z_{j}\) for \( j\), we have \(z_{j}=1\) for the top \(k\) terms of \(p_{j}^{2}\) and \(z_{j}=0\) otherwise. Then, Problem (12) can be reformulated as follows:

\[-_{}(F()+G())\ \ \ \ _{}+=^{T},\] (18)

where \(F():=^{T}_{}\) and \(G():=+}_{k}(\{p_{j}^{2}\})\). The solution to this problem is a dense vector that can be used to provide a lower bound on the original \(k\)-sparse problem. This problem can be solved by the alternating direction method of multipliers (ADMM) . Here, we apply the iterative algorithm with the scaled dual variable \(\):

\[^{t+1} =*{argmin}_{}F()+\|_{}+^{t}-^{T}+^{t}\|_ {2}^{2}\] (19) \[^{t+1} =2_{}^{t+1}-(1-2)(^{t}-^{T})\] (20) \[^{t+1} =*{argmin}_{}G()+\|^{t+1}+-^{T}+^{t}\|_{2}^{2}\] (21) \[^{t+1} =^{t}+^{t+1}+^{t+1}-^{T},\] (22)

where \(\) is the relaxation factor, and \(\) is the step size.

It is known that ADMM suffers from slow convergence when the step size is not properly chosen. According to , to ensure the optimized linear convergence rate bound factor, we can pick \(=1\) and \(=(_{})_{>0}( {Q}_{})}}\)1, where \(_{}()\) denotes the largest eigenvalue of a matrix, and \(_{>0}()\) denotes the smallest positive eigenvalue of a matrix.

Having settled the choices for the relaxation factor \(\) and the step size \(\), we are left with the task of solving Equation (19) and Equation (21) (also known as evaluating the proximal operators ). Interestingly, Equation (19) can be evaluated by solving a linear system while Equation (21) can be evaluated by recasting the problem as an isotonic regression problem.

**Theorem 3.3**.: _Let \(F()=^{T}_{}\) and \(G()=+}_{k}(\{p_{j}^{2}\})\). Then the solution for the problem \(^{t+1}=*{argmin}_{}F()+\|_{}+^{t}-^{T}+^{ t}\|_{2}^{2}\) is_

\[^{t+1}=(+_{})^{-1} (^{T}-^{t}-^{t}).\] (23)

_Furthermore, let \(=^{T}-^{t+1}-^{t}\) and \(\) be the indices of the top \(k\) terms of \(\{|a_{j}|\}\). The solution for the problem \(^{t+1}=*{argmin}_{}G()+\|^{t+1}+-^{T}+^{t}\|_{2}^{2}\) is \(p_{j}^{t+1}=(a_{j})_{j}\), where_

\[} =*{argmin}_{}_{j=1}^{p}w_{j}(v_{j}-b_{j})^{ 2}\ \ \ \ v_{i} v_{l}\ \ \ \ |a_{i}||a_{l}|\] (24) \[w_{j} =1&\ j\\ 1++)}&\, b_{j}=|}{w_{j}}.\]

_Problem (24) is an isotonic regression problem and can be efficiently solved in linear time ._

### Beam-Search as a Heuristic

After finishing the lower bound calculation in Section 3.1, we next explain how to quickly reduce the upper bound in the BnB tree. We discuss how to add features, keep good solutions, and use dynamic programming to improve efficiency. Lastly, we give a theoretical guarantee on the quality of our solution.

Starting from the vector \(\), we add one coordinate at a time into our support until we reach a solution with support size \(k\). At each iteration, we pick the coordinate that results in the largest decrease in the ridge regression loss while keeping coefficients in the existing support fixed:

\[j^{*}*{argmin}_{j}\;_{}_{}( +_{j}) j ^{*}*{argmax}_{j}\;_{}())^{2}}{\|_{:j}\|_{2}^{2}+ _{2}},\] (25)

where \(_{:j}\) denotes the \(j\)-th column of \(\), and the right-hand side uses an analytical solution for the line-search for \(\). This is similar to the sparse-simplex algorithm . However, after adding a feature, we adjust the coefficients restricted on the new support by minimizing the ridge regression loss.

The above idea does not handle highly correlated features well. Once a feature is added, it cannot be removed . To alleviate this problem, we use beam-search [58; 43], keeping the best \(B\) solutions at each stage of support expansion:

\[j^{*}*{arg\,BottomB}_{j}(_{}_{}(+_{j})),\] (26)

where \(j^{*}*{arg\,BottomB}_{j}\) means \(j^{*}\) belongs to the set of solutions whose loss is one of the B smallest losses. Afterwards, we finetune the solution on the newly expanded support and choose the best B solutions for the next stage of support expansion. A visual illustration of beam search can be found in Figure 6 in Appendix E, which also contains the detailed algorithm.

Although many methods have been proposed for sparse ridge regression, none of them have been designed with the BnB tree structure in mind. Our approach is to take advantage of the search history of past nodes to speed up the search process for a current node. To achieve this, we follow a dynamic programming approach by saving the solutions of already explored support sets. Therefore, whenever we need to adjust coefficients on the new support during beam search, we can simply retrieve the coefficients from the history if a support has been explored in the past. Essentially, we trade memory space for computational efficiency.

#### 3.2.1 Provable Guarantee

Lastly, using similar methods to , we quantify the gap between our found heuristic solution \(}\) and the optimal solution \(^{*}\) in Theorem 3.4. Compared with Theorem 5 in , we improve the factor in the exponent from \(}{M_{2k}}\) to \(}{M_{1}}\) (since \(M_{1} M_{2k}\), where \(M_{1}\) and \(M_{2k}\) are defined in ).

**Theorem 3.4**.: _Let us define a \(k\)-sparse vector pair domain to be \(_{k}:=\{(,)^{p}^{p}:\|\|_{0} k,\|\|_{0} k,\| -\|_{0} k\}\). Any \(M_{1}\) satisfying \(f() f()+ f()^{T}( -)+}{2}\|- \|_{2}^{2}\)for all \((,)_{1}\) is called a restricted smooth parameter with support size 1, and any \(m_{2k}\) satisfying \(f() f()+ f()^{T}( -)+}{2}\|- \|_{2}^{2}\) for all \((,)_{2k}\) is called a restricted strongly convex parameter with support size \(2k\). If \(}\) is our heuristic solution by the beam-search method, and \(^{*}\) is the optimal solution, then:_

\[_{}(^{*})_{}(})(1-e^{-m_{2k}/M_{1}})_{}(^{*}).\] (27)

### Branching and Queuing

**Branching:** The most common branching techniques include most-infeasible branching and strong branching [2; 1; 15; 7]. However, these two techniques require having fractional values for the binary variables \(z_{j}\)'s, which we do not compute in our framework. Instead, we propose a new branching strategy based on our heuristic solution \(}\): we branch on the coordinate whose coefficient, if set to \(0\), would result in the largest increase in the ridge regression loss \(_{}\) (See Appendix E for details):

\[j^{*}=*{argmax}_{j}_{}(}-_{j}_{j}).\] (28)

The intuition is that the coordinate with the largest increase in \(_{}\) potentially plays a significant role, so we want to fix such a coordinate as early as possible in the BnB tree.

**Queuing:** Besides the branching strategy, we need a queue to pick a node to explore among newly created nodes. Here, we use a breadth-first approach, evaluating nodes in the order they are created.

## 4 Experiments

We test the effectiveness of our OKRidge on synthetic benchmarks and sparse identification of nonlinear dynamical systems (SINDy). Our main focus is: assessing how well our proposed lower bound calculation speeds up certification (Section 4.1), and evaluating solution quality of OKRidge on challenging applications (Section 4.2). Additional extensive experiments are in Appendix G and H. Our algorithms are written in Python. Any improvements we see over commercial MIP solvers, which are coded in C/C++, are solely due to our specialized algorithms.

Figure 1: Comparison of running time (top row) and optimality gap (bottom row) between our method and baselines, varying the number of features, for three correlation levels \(=0.1,0.5,0.9\) (\(n=100000,k=10\)). Time is on the log scale. Our method is generally orders of magnitude faster than other approaches. Our method achieves the smallest optimality gap, especially when the feature correlation \(\) is high.

Figure 2: Comparison of running time (top row) and optimality gap (bottom row) between our method and baselines, varying sample sizes, for three correlation levels \(=0.1,0.5,0.9\) (\(p=3000,k=10\)). Time is on the log scale. When \(=0.1\) and \(=0.5\), OKRidge is generally orders of magnitude faster than other approaches. In the case \(=0.9\), we achieve the smallest optimality gap as shown in the bottom row.

### Assessing How Well Our Proposed Lower Bound Calculation Speeds Up Certification

Here, we demonstrate the speed of OKRidge for certifying optimality compared to existing MIPs solved by Gurobi . We set a 1-hour time limit and an optimality gap of relative tolerance \(10^{-4}\).

We use a value of \(0.001\) for \(_{2}\). Our 4 baselines include MIPs with SOS1, big-M (\(M=50\) to prevent cutting off optimal solutions), perspective , and eigen-perspective formulations (\(=_{}(^{T})\)) . In the main text, we use plots to present the results. In Appendix G, we present the results in tables. Additionally, in Appendix G, we conduct perturbation studies on \(_{2}\) (\(_{2}=0.1\) and \(_{2}=10\)) and \(M\) (\(M=20\) and \(M=5\)). Finally, still in Appendix G, we also compare OKRidge with other MIPs including the MOSEK solver , SubsetSelectionCIO , and LOBNB .

Similar to the data generation process in [11; 48], we first sample \(x_{i}^{p}\) from a Gaussian distribution \((,)\) with mean 0 and covariance matrix \(\), where \(_{ij}=p^{|i-j|}\). Variable \(\) controls the feature correlation. Then, we create the coefficient vector \(^{}\) with \(k\) nonzero entries, where \(^{}_{j}=1\) if \(j(p/k)=0\). Next, we construct the prediction \(y_{i}=x_{i}^{T}^{}+_{i}\), where \(_{i}(0,^{}\|_{2}^{2}}{})\). SNR stands for signal-to-noise ratio (SNR), and we choose SNR to be \(5\) in all our experiments.

In the first setting, we fix the number of samples with \(n=100000\) and vary the number of features \(p\{100,500,1000,3000,5000\}\) and correlation levels \(\{0.1,0.5,0.9\}\) (See Appendix G for \(=0.3\) and \(=0.7\)). We warm-started the MIP solvers by our beam-search solutions. The results can be seen in Figure 1. From both figures, we see that **OKRidge outperforms all existing MIPs solved by Gurobi, usually by orders of magnitude.**

In the second setting, we fix the number of features to \(p=3000\) and vary the number of samples \(n\{3000,4000,5000,6000,7000\}\) and the correlation levels \(\{0.1,0.5,0.9\}\) (see Appendix G for \(=0.3\) and \(=0.7\)). As in the first setting, we also warm-started the MIP solvers by our beam-search solutions. The results are in Figure 2. When \(n\) is close to \(p\) or the correlation is high

Figure 3: Results on discovering sparse differential equations. On various metrics, OKRidge outperforms all other methods, including MIOSR which uses a commercial (proprietary) MIP solver.

(\(=0.9\)), no methods can finish within the 1-hour time limit, but **OKRidge prunes the search space well and achieves the smallest optimality gap. When \(n\) becomes larger in the case of \(=0.1\) and \(=0.5\), OKRidge runs orders of magnitude faster than all baselines**.

### Evaluating Solution Quality of OKRidge on Challenging Applications

On previous synthetic benchmarks, many heuristics (including our beam search method) can find the optimal solution without branch-and-bound. In this subsection, we work on more challenging scenarios (sparse identification of differential equations). We replicate the experiments in  using three dynamical systems from the PySINDy library : Lorenz System, Hopf Bifurcation, and magnetohydrodynamical (MHD) model . The Lorenz System is a 3-D system with the nonlinear differential equations:

\[dx/dt=- x+ y, 28.452756ptdy/dt= x-y-xz, 28.452756ptdz/dt =xy- z\]

where we use standard parameters \(=10,=8/3,=28\). The true sparsities for each dimension are \((2,3,2)\). The Hopf Bifurcation is a 2-D system with nonlinear differential equations:

\[dx/dt= x+ y-Ax^{3}-Axy^{2}, 28.452756ptdy/dt=- x+ y-Ax^ {2}y-Ay^{3}\]

where we use the standard parameters \(=-0.05,=1,A=1\). The true sparsities for each dimension are \((4,4)\). Finally, the MHD is a 6-D system with the nonlinear differential equations:

\[dV_{1}/dt=4V_{2}V_{3}-4B_{2}B_{3},&dV_{2}/dt=-7V_{1}V_{3}+ 7B_{1}B_{2},&dV_{3}/dt=3V_{1}V_{2}-3B_{1}B_{2},\\ dB_{1}/dt=2B_{3}V_{2}-2V_{3}B_{2},&dB_{2}/dt=5V_{3}B_{1}-5B_{3}V_{1},&dB_{3}/dt =9V_{1}B_{2}-9B_{1}V_{2}.\]

The true sparsities for each dimension are \((2,2,2,2,2)\).

We use all monomial features (candidate functions) up to 5th order interactions. This results in 56 functions for the Lorentz System, 21 for Hopf Bifurcation, and 462 for MHD. Due to the high-order interaction terms, features are highly correlated, resulting in poor performance of heuristic methods.

#### 4.2.1 Baselines and Experimental Setup

In addition to MIOSR (which relies on the SOS1 formulation), we also compare with three common baselines in the SINDy literature: STLSQ , SSR , and E-STLSQ . The baseline SR3  is not included since the previous literature  shows it performs poorly. We compare OKRidge with other baselines using the SINDy library . We follow the experimental setups in  for model selection, hyperparameter choices, and evaluation metrics (please see Appendix F for details). In Appendix H, we provide additional experiments on Gurobi with different MIP formulations and comparing with more heuristic baselines.

#### 4.2.2 Results

Figure 3 displays the results. **OKRidge (red curves) outperforms all baselines, including MIOSR (blue curves), across evaluation metrics.** On the Lorenz System, all methods recover the true feature support when the training trajectory is long enough. When the training trajectory length is short, i.e., the left part of each subplot, (or equivalently, when the number of samples is small), OKRidge performs uniformly better than all other baselines. On the Hopf Bifurcation, all heuristic methods fail to recover the true support, resulting in poor performance. On the final MHD, OKRidge maintains the top performance and outperforms MIOSR on the true positivity rate. This demonstrates the effectiveness of OKRidge, which incurs lower runtimes and yields better metric scores under high-dimensional settings. The highest runtimes are incurred for the MHD (with 462 candidate functions/features), which are shown in Figure 4.

Figure 4: Running time comparison between OKRidge and MIOSR on the MHD system with 462 candidate functions. OKRidge is significantly faster than the previous state of the art.

Limitations of OKRidgeWhen the feature dimension is low (under 100s), Gurobi can solve the problem to optimality faster than OKRidge. This is observed on the synthetic benchmarks (\(p=100\)) and also on the Hopf Bifurcation (\(p=21\)). Since Gurobi is a commercial proprietary solver, we cannot inspect the details of its sophisticated implementation. Gurobi may resort to an enumeration/brute-force approach, which could be faster than spending time to calculate lower bounds in the BnB tree. This being said, OKRidge is still competitive with Gurobi in the low-dimensional setting, and OKRidge scales favorably in high-dimensional settings.

## 5 Conclusion

We presented a method for optimal sparse ridge regression that leverages a novel tight lower bound on the objective. We showed that the method is both faster and more accurate than existing approaches for learning differential equations - a key problem in scientific discovery. This tool (unlike its main competitor) does not require proprietary software with expensive licenses and can have a significant impact on various regression applications.

## Code Availability

Implementations of OKRidge discussed in this paper are available at https://github.com/jiachangliu/OKRidge.