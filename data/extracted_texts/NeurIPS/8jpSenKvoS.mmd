# Fast Channel Simulation via

Error-Correcting Codes

Sharang M. Sriramu

School of ECE

Cornell University

Ithaca, NY 14853

sms579@cornell.edu

&Rochelle Barsz

School of ECE

Cornell University

Ithaca, NY 14853

rsb359@cornell.edu

&Elizabeth Polito

School of ECE

Cornell University

Ithaca, NY 14853

emp234@cornell.edu

&Aaron B. Wagner

School of ECE

Cornell University

Ithaca, NY 14853

wagner@cornell.edu

###### Abstract

We consider the design of practically-implementable schemes for the task of channel simulation. Existing methods do not scale with the number of simultaneous uses of the channel and are therefore unable to harness the amortization gains associated with simulating many uses of the channel at once. We show how techniques from the theory of error-correcting codes can be applied to achieve scalability and hence improved performance. As an exemplar, we focus on how polar codes can be used to efficiently simulate i.i.d. copies of a class of binary-output channels.

## 1 Introduction

_Channel simulation_ refers to the task in which Alice observes a realization \(x\) of a random variable \(X\) and sends a bit string to Bob. Bob, who shares common randomness with Alice, outputs a random variable \(Y\) using both the message and the common randomness. The goal is to minimize the length of the bit string subject to the constraint that \(Y\) should have a specified distribution \(P_{Y|X}(|x)\). This problem can be viewed as a "soft" or "stochastic" generalization of quantization. As with quantization, \(Y\) has a digital representation (through Alice's message), and it can be viewed as a degraded version of \(X\). The difference is that here the degrading process is stochastic in general. In fact, the quantization problem is subsumed by taking the channel \(P_{Y|X}\) to be deterministic.

This stochastic generalization of quantization arises in lossy compression of various types of sources including images (Flamich et al., 2020; Balle et al., 2020), models (Havasi et al., 2018), and gradients (Shah et al., 2022). In these applications, \(X\) often represents a vector of latent variables, model weights, or even an image consisting of millions of pixels (Theis et al., 2022). The channel of interest is therefore high dimensional, and it is usually independent across the dimensions. For conventional quantization, it has long been recognized that the optimum rate-distortion tradeoff is more favorable in higher dimensions (Cover and Thomas, 2006), a trend that one expects to generalize to channel simulation. Indeed, let \(n R_{n}\) be the minimum number of bits required to generate \(Y^{n}=(Y_{1},,Y_{n})\), when \(X^{n}\) is i.i.d. and \(Y^{n}\) is conditionally i.i.d. given \(X^{n}\). Thus \(R_{n}\) is the minimum number of bits per dimension when simulating the channel \(n\) times. The sequencecan be shown to be _subadditive_ and therefore satisfies (e.g., Liggett (1999, Thm. B22))

\[_{n}R_{n}=_{n}R_{n}.\] (1)

It is known that \(R_{1}\) satisfies (Li and El Gamal, 2018)

\[I(X;Y) R_{1} I(X;Y)+(I(X;Y)+1)+5,\] (2)

where \(I(X;Y)\) refers to conventional Shannon mutual information. Applying this to i.i.d. \((X^{n},Y^{n})\) and using the fact that \(I(X^{n};Y^{n})=nI(X;Y)\), we have

\[nI(X;Y) n R_{n} nI(X;Y)+(nI(X;Y)+1)+5,\] (3)

which shows that as \(n\), \(R_{n}\) approaches the lower bound \(I(X;Y)\). The challenge, for both quantization and channel simulation, is that the complexity of schemes tends to grow exponentially in \(n\). In fact, although many channel simulation schemes have been proposed (Harsha et al., 2007), (Li and El Gamal, 2018), (Flamich et al., 2022), (Flamich and Theis, 2023), (Flamich et al., 2024), none have complexity that scales subexponentially in \(n\), ignoring isolated examples for which \(R_{1}\) happens to equal \(I(X;Y)\)(Zamir and Feder, 1992), (Agustsson and Theis, 2020).

Vector quantization has long been recognized to be the dual, in a precise sense, of channel coding (Pradhan et al., 2003). In channel coding, the decoder maps an arbitrary point to an element of a finite set that is "close" in some channel-dependent sense. This is analogous to the role of the encoder in quantization. Likewise, the encoder in channel coding is analogous to the decoder in quantization: both map bit strings to elements of said discrete set. Thus new techniques for channel coding can often be applied to vector quantization (Goblick, 1963), (Viterbi and Omura, 1974) and vice versa (Laroia et al., 1994).

The goal of this paper is to demonstrate how ideas from coding theory can be applied to the channel simulation problem. We shall see that by adopting these techniques, we can develop schemes that significantly outperform state-of-the art methods, both in terms of their scalability and their rate performance. Specifically, we show how _polar codes_(Arikan, 2009) can be applied to the simulation problem using a method called PolarSim. Polar codes make for a good exemplar of this general proposal for five reasons. First, they have excellent channel coding performance, both theoretically (Mondelli et al., 2016) and practically (Egilmez et al., 2019). Second, their complexity scales as \(n n\). Third, they require no manual tuning. Fourth, they are simple to describe, requiring minimal background in coding theory. Finally, there exist highly optimized implementations of the encoding and decoding algorithms (e.g., (Pfister, 2023)). Their limitation is that, in their basic form, they can only be applied to symmetric binary-input channels (see (4) to follow). As we shall see, this means that PolarSim can only simulate symmetric binary-output channels. This class includes, for example, the binary symmetric channel, the (reverse) binary erasure channel, and channels of the form \(X(X+Z)\), where \(X\) and \(Z\) are real-valued, independent random variables with symmetric distributions. Note that the input to the channel need not be binary and may even be continuous.

For these channels, we show both theoretically and experimentally that, by scaling up the dimension, the rate of PolarSim can be made to approach the mutual information lower bound \(I(X;Y) R_{n}\) from (3). The superior scalability of PolarSim thus translates to a significant rate improvement over the state-of-the-art, since those schemes are not able to harness the amortization gain associated with letting \(n\) grow. Although PolarSim is restricted to binary-output channels, it is worth noting that there are currently no known schemes that simulate any nontrivial class of channels with even subexponential complexity in \(n\). Also, for compression applications, a binary output alphabet is not unreasonable. It should be emphasized that the binary-output restriction is particular to the basic form of polar codes, not error-correction methods in general. In the supplementary materials we discuss how a different coding technique, trellis coded modulation, can be applied to closely simulate a Gaussian channel. See also the discussion in the Concluding Remarks section on non-binary polar codes.

One lesson from the coding theory literature, especially with the advent of modern coding theory in the 1990s (Richardson and Urbanke, 2008), is that it is advantageous to prioritize scalability with the dimension \(n\) over achieving optimal performance for particular \(n\). The reason is that performance naturally improves with increasing \(n\) (as in (1)-(3) above), and this improvement can overcome suboptimality at any given value of \(n\). For state-of-the-art channel codes, the decoder typically does not implement the optimal (maximum likelihood) decision rule. Instead, it implements a scalable approximation to it. This design strategy of favoring scalability over optimality is now well established in coding theory and vector quantization, and our goal here is to show how it can be profitably applied to channel simulation.

The rest of the paper is organized as follows. We provide the necessary background on polar codes in Section 2. Section 3 describes the PolarSim scheme, with Section 3.3 containing the theoretical result and Section 3.4 contains simulation results. Some concluding remarks are offered in Section 5.

### Terminology and Notation

We follow the standard convention of denoting the dimensionality of vectors by their superscript. We will also denote compound i.i.d. channels by superscripts: \(p^{ n}\) denotes \(n\) i.i.d. copies of a distribution \(p\). The number of copies here is referred to as the _block length_ or _dimension_ of the channel. This is to be distinguished from the notation \(F^{ n}\) which denotes a \(n\)-fold self-Kronecker product of a tensor \(F\). All logarithms mentioned in this paper are base \(2\). The _binary entropy function_\(h_{B}:[0,]\) is defined as \(h_{B}(p)=-p p-(1-p)(1-p)\). We will also refer to its inverse \(h_{B}^{-1}:[0,]\) defined such that \(h_{B}^{-1}(h_{B}(p))=p\).

## 2 Background

### The Channel Simulation Problem

Consider a joint probability measure \(p_{XY}\) on the set \(\). Alice receives a sequence of \(n\) symbols from the input alphabet \(\) drawn according to \(p_{X}^{ n}\) and encodes it into a binary string that she transmits to Bob. Upon receiving the message from Alice, Bob then decodes it to generate a sample from the channel \(p_{Y|X}^{ n}\). The objective is to find coding schemes that minimize the average _rate_-- i.e., the average amortized length of the bit string transmitted by Alice. We refer to \(n\) as the _block length_ or the _dimension_. We require that the set of strings that Alice can transmit to Bob to form a _prefix-free_ set, meaning that no string in the set is a prefix of any other. Alice's message is thus self-terminating, and schemes for block length \(m\) and \(n\) can be combined to obtain a scheme for block length \(m+n\) by concatenation. If \(R_{n}\) denotes the minimum average rate, i.e., the minimum average length of Alice's string over all schemes, normalized by \(n\), then \(nR_{n}\) is subadditive in \(n\), as noted earlier.

Both Alice and Bob are permitted to use randomized strategies and are assumed to share a source of common randomness. Under this assumption, Li and El Gamal (2018) prove the performance bounds (2) and (3) above (see also Harsha et al. (2007)). For large \(n\), Sriramu and Wagner (2024) improve upon this result for i.i.d. discrete memoryless channels, showing that the logarithmic redundancy term can be halved for some channels and eliminated for all others. While these schemes are nearly rate-optimal, their complexity scales exponentially in \(n\). Other practical schemes have been proposed, although none have even subexponential scaling in \(n\) outside the small class of channels for which the lower bound in (3) is tight for all \(n\).

### Background on Polar Codes

Modern coding theory has focused on the search for codes whose rates approach the theoretical limit while having a low encoding and decoding complexity. _Polar codes_ are among the crowning achievements of this search. In their basic form, they are capacity-achieving linear codes for binary input channels \(W_{X|Y}(|)\) satisfying the following symmetry condition: there exists a bijection \(:\) such that \(^{-1}=\) and

\[W(x|0)=W((x)|1)x.\] (4)

As a linear code, the encoding procedure for polar codes is defined by a generator matrix \(G_{n}\) that maps an input \(U^{n}\{0,1\}^{n}\) to the channel input \(Y^{n}\) as \(Y^{n}=U^{n}G_{n}\) with all operations performed over \(_{2}\). The generator matrix of size \(n=2^{m}\) can be constructed recursively from a kernel matrix \(F=[1&0\\ 1&1]\) and the "bit-reversal" permutation matrix \(B_{n}\)--see Arikan (2009). The generator matrix is then defined as \(G_{n}=B_{n}F^{ n}\). The structure of the generator matrix allows for encoding in \(n n\) time.

Decoding proceeds by sequentially guessing each bit of \(U^{n}\) in order. For this, one considers the _subchannel_\(U_{i}(U^{i-1},X^{n})\). Specifically, given the realization of the output \(x^{n}\) and the decodingdecisions for the prior bits \(^{i-1}\), one computes the likelihood ratio

\[=1|X^{n}=x^{n},U^{i-1}=^{i-1})}{(U_{i}=0|X^{n}=x^{n},U^{i -1}=^{i-1})},\] (5)

and selects \(_{i}\) accordingly. This decoding rule is suboptimal but scalable in that the likelihood ratio can be computed in \( n\) steps using the recursive structure of \(G_{n}\)(Arikan, 2009), resulting in decoding complexity of \(n n\).

The suboptimality turns out to be acceptable because as \(n\) grows, the subchannels _polarize_, meaning that for most \(i\) the likelihood ratio in (5) is close to \(0\), \(1\) or \(\) with high probability. Equivalently, the mutual information \(I(U_{i};X^{n},U^{i-1})\) is close to zero or one. Figure 1 illustrates this phenomenon for a _binary symmetric channel_ (\(\) ) which is defined by \(X=Y Z\), where \(Z(p)\) for some crossover probability \(p\).

For communication, the encoder uses the "clean" subchannels with mutual information close to \(1\) to send information bits. The inputs to the remaining noisy channels, called the _frozen bits_, are fixed ahead of time and known to both the encoder and decoder. The data rate is thus the fraction of subchannels that are clean, which can be shown to approach capacity (Arikan, 2009).

Implementing the code requires determining which indices correspond to the clean subchannels. This can be done using the dimensionality reduction technique of Tal and Vardy (2013), by exploiting the recursive structure of the polar transform (Zhang et al., 2014; Arikan, 2009), or by Monte Carlo simulation.

## 3 Simulating Binary Output Channels

Previously, we described polar codes for constructing channel codes for binary _input_ channels. Based on the duality between channel coding and channel simulation, we will see that this naturally leads to a scheme for simulating binary _output_ channels.

We begin by describing a "toy scheme". This is not a rate-efficient scheme in its own right, but it serves as a foundation for PolarSim.

### Toy Scheme for Binary Output Channel Simulation

Consider a joint distribution \(p_{XY}\) where \(p_{Y}\) is \(()\). Then, the following algorithm simulates \(p_{Y|X}\) exactly:

1. Use the common randomness to generate \(Z(0,1)\) and \(V=(Z>)\) at both the encoder and decoder.
2. At the encoder, having observed an input realization \(x\), compute the output bit \(Y=(Z>p_{Y|X}(0|x))\) and the correction bit \(=Y V\).
3. Transmit \(\) to the decoder after lossless compression.
4. Recover \(Y= V\) at the decoder.

In Appendix A, we show that the rate associated with repeated application of this scheme is upper bounded by \(h_{B}(-h_{B}^{-1}(1-I(X;Y)))\). As we can see in Figure 2, this is highly suboptimal in general. However, we note that for the special case in which the mutual information is _polarized_, i.e., where \(I(X;Y) 0\) or \(I(X;Y) 1\), the toy scheme is close to optimal.

This observation is crucial as it suggests a path forward: If we can transform a given channel simulation problem to the problem of simulating polarized channels, it can be solved rate-efficiently using the toy scheme. Polar codes provide us with the means to achieve such a transformation.

### Channel Simulation using Polar Codes

As in the toy scheme, we shall consider a joint distribution \(p_{XY}\) where \(p_{Y}\) is \(()\). We will additionally assume that \(p_{X|Y}\) satisfies the symmetry condition described in (4). Let us then examine Figure 2: The upper bound on the rate of the toy scheme () described in section 3.1 is plotted against the mutual information lower bound ().

Figure 1: Channel polarization for a BSC with crossover probability 0.2 and block lengths \(n=2^{12}\) (**top**) and \(n=2^{15}\) (**bottom**). The scatter plots on the **left** show the subchannel capacities \(I(U_{i};X^{n},U^{i-1})\) for each index \(i\). In the curves () on the **right**, these indices are sorted in the increasing order of their subchannel capacities for better visualization. The area under these curves is the mutual information lower bounds at their respective block length. The **vertical dotted line** marks the ideal polarized channel, i.e., the fraction of indices to its right is equal to the mutual information of the channel. We see that the sorted subchannel capacity curve approaches this line as the block length is increased. Finally, we also plot the theoretical upper bound (see (38)) on the rate of our proposed scheme, PolarSim, for block lengths \(=2^{12}\) and \(=n=2^{15}\). The area under these curves is an upper bound on the rate of PolarSim. The shaded area in between is therefore an upper bound on the redundancy of PolarSim, which vanishes as \(n\) due to the polarization phenomenon.

the problem of simulating two independent realizations of the channel \(p_{Y|X}\), \(X^{2} Y^{2}\). Consider the following bijection applied to the output \((Y_{1},Y_{2})\):

\[U_{1} =Y_{1} Y_{2},\] \[U_{2} =Y_{2}.\] (6)

It is clear that simulating the original pair of i.i.d. channels \(X_{1} Y_{1}\) and \(X_{2} Y_{2}\) is equivalent to simulating the transformed pair of channels \((X_{1},X_{2}) U_{1}\) and \((X_{1},X_{2},U_{1}) U_{2}\).

The mutual information of each of the original i.i.d. channels are equal to \(I(X;Y)\). However, the two transformed channels differ in terms of mutual information: \(I(X_{1},X_{2},U_{1};U_{2})>I(X;Y)\) because \(U_{2}\) is observed through two different channels. This necessitates that the other channel has lower mutual information: \(I(X_{1},X_{2};U_{1})<I(X;Y)\). Therefore, the linear transformation (6) we applied to the output had the effect of _polarizing_ the target channel.

For block lengths that are larger powers of two, we can apply the transform inductively, resulting in the relationship we saw in section 2: \(U^{n}=Y^{n}G_{n}^{-1}\). Similar to the two-dimensional (\(n=2\)) case, this transforms the original simulation problem \(X^{n} Y^{n}\) into the problem of simulating the _subchannels_\(X^{n} U_{1},(X^{n},U_{1}) U_{2},(X^{n},U_{1},U_{2}) U_{3},,(X ^{n},U^{n-1}) U_{n}\). Arikan  shows that these subchannels are polarized for large \(n\): for each \(i\), \(I(U_{i};X^{n},U^{i-1}) 0\) or \(I(U_{i};X^{n},U^{i-1}) 1\). This allows us to simulate them using the toy algorithm described in the previous section.

Algorithms 1 and 2 describe the complete scheme.

``` Input :Block length \(n\)  Random bit string \(z^{n}\ (^{n})\)  Probability table \(^{n}^{n}\)  Source string \(x^{n}^{n}\) Output :String \(b\{0,1\}^{*}\) for\(i=1,,n\)do if\(z_{i}>(x^{n},u^{i-1})\)then\(u_{i} 1\)else\(u_{i} 0\) if\(z_{i}>1/2\)then\(v_{i} 1\)else\(v_{i} 0\) \(_{i} u_{i}+v_{i}\) \(b(^{n},^{n})\) return\(b\) ```

**Algorithm 1**Encoder for simulating a channel using polar codes.

The encoder input \(_{i}\) refers to the subchannel parameter

\[_{i}=h_{B}^{-1}(H(U_{i}|U^{i-1},X^{n})).\] (7)

This can be calculated offline using the techniques used to compute subchannel quality for communication described at the end of Section 2. The SoftPolarDec\((u^{i-1},x^{n})\) subroutine outputs

\[(U_{i}=0|U^{i-1}=u^{i-1},X^{n}=x^{n}),\] (8)which can be calculated with \(O(n n)\) complexity using a recursion given by Arikan (2009). The \((u^{n})\) subroutine simply multiplies by the generator matrix in: \(y^{n}=u^{n}G_{n}\). This can be implemented in \(O(n n)\) by exploiting the recursive structure, as noted earlier. The \((^{n},^{n})\) and \((b,^{n})\) routines can be any prefix-free lossless compressor/decompressor pair that uses at most

\[c+_{i=1}^{n}[(_{i}=1)_{i}}+ (_{i}=0)_{i}}]\] (9)

bits to send \(^{n}\), where \(c\) is some constant independent of \(n\) and \(^{n}\). Arithmetic coding (Rissanen (1976)) is a widely-used scheme that achieves this guarantee with \(c=2\).

This defines PolarSim for \(n\) that is a power of two. An arbitrary \(n\) can be handled by partitioning \(\{1,,n\}\) into subsets, each of which has a cardinality that is a power of two. One then applies PolarSim to each subset separately. Since the coding is prefix-free, the encoder outputs can simply be concatenated together, as noted in Sec. 2.1.

### Theoretical Guarantees

If one uses a linear complexity algorithm for Compress and Decompress, then the overall complexity of PolarSim is \(O(n n)\) for both encoding an decoding. Using the fact that polar codes achieve capacity for channels satisfying the symmetry condition in (4), we can show that PolarSim is rate optimal in the large-\(n\) limit, making it currently the only scheme with subexponential complexity in \(n\) with a comparable guarantee.

**Theorem 1**.: _Consider a joint distribution \(P_{XY}\) in which \(Y\) is binary and uniform and the reverse channel \(P_{X|Y}\) satisfies the symmetry condition in (4). Suppose \(\) and \(\) achieve the guarantee in (9)._

1. _(_Correctness:_) Algorithms_ 1 _and_ 2 _simulate the channel_ \(P^{ n}(Y|X)\) _exactly: If_ \(Z^{n}\) _is i.i.d._ \(\)_, and_ \(_{i}=h_{B}^{-1}(H(U_{i}|U^{i-1},X^{n}))\)_, then the conditional probability that Algorithm_ 2 _outputs_ \(y^{n}\) _given that_ \(x^{n}\) _is the input to Algorithm_ 1 _is_ \[_{i=1}^{n}P_{Y|X}(y_{i}|x_{i}).\] (10)
2. _(_Optimality:_) Algorithms_ 1 _and_ 2 _are asymptotically rate optimal:_ \[_{n}E[(b)] I(X;Y),\] (11) _where_ \(b\) _is the output of the encoder._

The proof is provided in Appendix D.

### Experimental Results

We run PolarSim on the reverse \((P_{Y|X})\) version of three channels: (1) the \(\)  with a uniform input (2) the binary erasure channel, \(X=Z Y\), where \(Y\) is uniform over \(\{-1,1\}\) and \(Z\) is Bernoulli(\(\)), and (3) the binary Gaussian channel \(X=Y+Z\), where \(Y\) is again uniform over \(\{-1,1\}\) and \(Z\) is \((0,^{2})\). Note that the reverse of the \(\) with a uniform input is the \(\) itself.

Fig. 3 and Fig. 4 show the rate performance of these simulations. Even at a block length of \(2^{12}\), the performance is already close to the mutual information lower bound across all channels and rates. Performance improves with \(n\) as expected, with both the average rate and the variance in the rate decreasing.

We also compare our scheme to the state-of-the-art scheme for channel simulation, Greedy Poisson Rejection Sampling (GPRS) (Flamich, 2024) (see Appendix E for the implementation details). Fig. 5 shows that PolarSim  significantly outperforms GPRS in terms of the communication rate, even when the latter is optimized for the channel at hand. In Table 1, we can see that its computational efficiency is also significantly better, by several orders of magnitude. This is due to the exponential computational complexity of GPRS in \(n\) compared to the pseudolinear complexity of PolarSim.

Figure 4: The redundancy of PolarSim is plotted for certain fixed channels (**Top:**BSC with \(p=0.05\), **Middle:** Reverse binary Gaussian channel with \(=0.5\), **Bottom:** Reverse binary erasure channel with \(=0.2\)) as the block length \(n\) is varied. The plotted curve () is the median redundancy over 200 simulations, with the boundaries of the shaded region showing the bootstrapped \(95\)% confidence interval around the sample median. The redundancy is defined as the gap between the achieved rate and the mutual information lower bound. For comparison, the theoretical maximum redundancy of PFRL () is also plotted for the respective channels (see 3 ). We see that for large block lengths, PolarSim has a higher redundancy, which is consistent with known results from channel coding .

Figure 3: Rates achieved by PolarSim at different block lengths — \(n=2^{12}\) (left), \(n=2^{17}\) (top-right and middle-right), \(n=2^{14}\) (bottom-right) for different noise levels across different channels, compared against the theoretical lower bound \(I(X;Y)\). **Top:**BSC\({}_{p}\) for \(p(0,)\), **Middle:** Gaussian for \((0,3)\), **Bottom:** Erasure for \((0,1)\). The lines represent the median values, and the boundaries of shaded regions represent the \(5^{}\) to \(95^{}\) percentile rates over \(200\) simulation runs.

## 4 Related Work

### Performance Bounds

Arguably the first to consider the channel simulation problem was Wyner (1975), who studied the problem without common randomness. The problem with common randomness was first introduced in the context of quantum information theory literature by Bennett et al. (2002) and Winter (2002). Harsha et al. (2007) proposed a greedy one-shot channel simulation algorithm that achieved a logarithmic rate redundancy with respect to the lower bound for discrete channels. Li and El Gamal (2018) showed a similar achievability result that generalizes to more general channels using their strong Poisson functional representation lemma. Sriramu and Wagner (2024) showed via a two-stage rejection sampling scheme that an even lower redundancy was achievable in the i.i.d. case. Li and Anantharam (2021) extended the Poisson representation to obtain a generalised method for deriving one-shot achievability in different settings (see also Phan et al. (2024)).

### Practical Schemes

The achievability proof in Li and El Gamal (2018) inspired several practical schemes that exploit properties of the Poisson process and perform well at short block lengths (Flamich et al., 2022; Flamich, 2024). Similarly, the rejection sampler proposed by Harsha et al. (2007) has been generalized by Flamich et al. (2024) to work for arbitrary probability spaces. None of these schemes exhibit subexponential complexity with \(n\) when applied to product channels, however. If one restricts attention

  \(\) &  &  &  \\   & **Median** & **p5** & **p95** & **Median** & **p5** & **p95** & \\ 
0.00 & 0.009 & 0.008 & 0.01 & 192.0 & 161.4 & 205.8 & 21333 \\
0.25 & 0.009 & 0.008 & 0.01 & 246.1 & 180.7 & 415.7 & 27347 \\
0.49 & 0.009 & 0.008 & 0.01 & 227.7 & 176.1 & 318.7 & 25305 \\  

Table 1: Execution time comparison between PolarSim and GPRS, for simulating BSC’s with block length \(n=2^{12}\). The reported statistics are computed over 1000 trials for each value of the crossover probability \(p\). GPRS cannot directly simulate such large block lengths. Therefore, the GPRS runtimes are obtained by scaling up the runtime for \(n=8\) blocks. This is justified by subadditivity (see (1)). The column \(\) computes the ratio between the medians of the two schemes. For our chosen block length, PolarSim performs over four orders of magnitude faster than GPRS.

Figure 5: Comparison of schemes for BSC simulation: Average rates for PolarSim with \(n=2^{12}\) and \(n=2^{17}\) compared against \(\) GPRS with \(n=8\) and the theoretical lower bound \(\)\(I(X;Y)\) over \(1000\) simulation runs.

to \(n=1\) and unimodal distributions, then improved schemes are possible (Flamich et al., 2024; Hegazy and Li, 2022), although by their nature such schemes cannot harness the amortization gain associated with increasing \(n\).

Chou et al. (2018) addresses the problem of total variation approximate channel simulation and proposes a fixed rate scheme based on a soft covering argument that uses polar codes.

As noted in the introduction, the primary application of the channel simulation task is learned compression. Lei et al. (2024) and Li et al. (2020) consider how vector quantization methods can be directly applied to the compression task without explicitly simulating a channel. Although their methods do not simulate an i.i.d. channel, their use of ideas from error-correcting codes and vector quantization makes them the closest prior works to the present paper, along with Chou et al. (2018).

## 5 Concluding Remarks

PolarSim shows how polar codes can be used to simulate channels with favorable scalability in the dimension. By harnessing the attending amortization gain, the codes are able to realize significantly improved performance compared with existing schemes. Our focus has been on how the original form of polar codes can simulate symmetric, binary output channels. Subsequent extensions of polar codes (Sasoglu et al., 2009) could potentially be used to simulate arbitrary channels. This would be an interesting topic of future research.

The aim of the paper, however, is not to show that polar codes are useful for simulation _per se_. Rather, we seek to make the larger point that ideas from the field of error-correcting codes are useful for the simulation problem. We have used polar codes as an exemplar, but one could potentially apply turbo codes (Berrou et al. (1993)), low-density parity-check codes LDPCs (Gallager (1962)), algebraic codes (Blahut, 2003), trellis-coded modulation/quantization (Taubman et al. (2002)), sparse superposition codes (Joseph and Barron (2012)), or other codes (e.g., Caire et al. (1998)) instead. The PolarSim method is not expected to generalize to these other families. Polar codes are linear, however, and one can transform them into a simulator for the BSC using a different approach that relies on linearity alone. This is discussed in the supplementary materials, where we find that this alternate scheme achieves comparable performance to PolarSim. This alternate approach is directly applicable to other linear codes such as LDPCs.

In learned compression applications, the channels of interest are generally continuous. Error-correcting schemes that operate directly on continuous channels, such as trellis-coded modulation/quantization, are therefore of particular interest. To illustrate the potential of these codes, in the supplementary materials we show how trellis-coded quantization can be used to approximately simulate a Gaussian channel. The simulation is not exact, but in practical applications, exact simulation may be unnecessary.

## 6 Acknowledgement

This research was supported by the US National Science Foundation under grant CCF-2306278 and by a gift from Google. The authors wish to thank Lucas Theis and Jona Balle for helpful discussions. They would also like to thank the anonymous reviewers for the feedback and suggestions they provided.