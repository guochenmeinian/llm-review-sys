# Linguistic Collapse:

**Neural Collapse in (Large) Language Models**

**Robert Wu**

University of Toronto, Vector Institute

rupert@cs.toronto.edu

**Vardan Papyan**

University of Toronto, Vector Institute

vardan.papyan@utoronto.ca

###### Abstract

Neural collapse (\(\)) is a phenomenon observed in classification tasks where top-layer representations collapse into their class means, which become equinorm, equiangular and aligned with the classifiers. These behaviours -- associated with generalization and robustness -- would manifest under specific conditions: models are trained towards zero loss, with noise-free labels belonging to balanced classes, which do not outnumber the model's hidden dimension. Recent studies have explored \(\) in the absence of one or more of these conditions to extend and capitalize on the associated benefits of ideal geometries. Language modelling presents a curious frontier, as _training by token prediction_ constitutes a classification task where none of the conditions exist: the vocabulary is imbalanced and exceeds the embedding dimension; different tokens might correspond to similar contextual embeddings; and large language models (LLMs) in particular are typically only trained for a few epochs. This paper empirically investigates the impact of scaling the architectures and training of causal language models (CLMs) on their progression towards \(\). We find that \(\) properties that develop with scale (and regularization) are linked to generalization. Moreover, there is evidence of some relationship between \(\) and generalization independent of scale. Our work thereby underscores the generality of \(\) as it extends to the novel and more challenging setting of language modelling. Downstream, we seek to inspire further research on the phenomenon to deepen our understanding of LLMs -- and neural networks at large -- and improve existing architectures based on \(\)-related properties. Our code is hosted on GitHub: https://github.com/rhubarbuu/linguistic-collapse.

Figure 1: Simultaneous development of the four _neural collapse_ (\(\))  properties in 230 causal language models trained on TinyStories , alongside improvement in generalization (i.e. validation performance). Left to right: \(_{1}\)) within-class (representation) variability collapse; \(_{2}\)) hyper-spherical uniformity of class means; \(_{3}\)) uniform duality between class means and corresponding classifiers; and \(_{4}\)) agreement between token (maximum a prior) classifiers and implicit nearest-class centre classifiers. Coloured by model size and annotated with coefficient of determination (\(R^{2}\)).

Introduction

### Neural Collapse

A learning phenomenon known as _neural collapse_ (\(\)) emerges during the terminal phase of training (TPT) deep neural networks with cross-entropy (CE) loss for classification tasks. It was originally characterized as the co-occurrence of the following properties in a model's top-layer (also known as last-layer) representations (also known as features or embeddings) and linear classifier weights:

* **Within-class variability collapse:** Top-layer representations collapse to their class means.
* **Convergence to a simplex ETF:** Class means tend towards equinorm and equiangular vectors when centred about the global average. The resulting geometry -- known as a simplex _equiangular tight frame_ (ETF) -- maximizes pairwise angles and distances.
* **Convergence to self-duality:** Linear classifier weight vectors converge to their corresponding top-layer class mean vectors, and thus also form a simplex ETF.
* **Nearest decision rule:** Linear classifiers approximate a nearest-class centre (NCC) classifier: top-layer representations predict the class with the closest mean (implied by \(\)1-3).

These behaviours, often associated with improved generalization and robustness  (among other benefits, such as those discussed in SS1.4), traditionally manifest under the following conditions:

* **Few classes:** The number of classes is upper-bounded by the embedding dimension plus one: \(C d+1\); this is required to construct a perfect simplex ETF.
* **Balanced classes:** The number of samples is equal across classes: \(N_{c}=N_{c^{}}, c c^{}\).
* **Noise-free labels:** Identical (or very similar) embeddings should belong to the same class.
* **Sufficient training (TPT):** The model is trained past zero error, towards zero loss.

Absent these conditions, one does not typically anticipate \(\). Since then, follow-up studies have extended beyond and proposed techniques of quantifying or achieving \(\) (discussed in Section 2).

### (Large) Language Models

\(\) is a phenomenon observed specifically in classification tasks. While not traditionally thought of as classifiers, language models -- including large language models (LLMs) -- learn to model aleatoric uncertainty, which can be viewed as stochastic token prediction . For instance, masked language models (MLMs) such as BERT  predict one or several masked tokens within an input sequence based on the surrounding _context_. Likewise, autoregressive or causal language models (CLMs) such as GPT  perform next-token prediction (NTP) in a sequence given the _context_ of previous tokens. Most of these models are essentially _(pre-)trained_ by token classification on their vocabularies. This parallel -- also drawn by  -- raises a few natural questions:

1. Does the pre-training stage of a language model exhibit \(\)?
2. How do scaling and other training configurations influence \(\) in (L)LMs?
3. To what extent is \(\) in (L)LMs correlated with their generalization abilities?
4. Do such correlations, between \(\) and improved generalization, persist independent of the (potential confounders of) model size and training?

To address these, we first examine the specific settings of training CLMs as they are opposed (\(\)) to the traditional prerequisites (R1-4, SS1.1) for \(\) to manifest.

* **Many classes:** The unique tokens found in language modelling vocabularies are vast, usually numbering in the tens of thousands and far exceeding the hidden dimension .
* **Imbalanced classes:** The distribution of tokens in natural language is typically very imbalanced , as is the case in TinyStories , the dataset we use (Appendix Figure 4). It has an average of 16K samples per class but a standard deviation of over 32K.

* **Ambiguous contexts:** There may exist very similar or even identical contexts followed by different tokens in the natural language data . For instance, over half of the sequences in TinyStories  lead with "Once upon a time", but only three-quarters of these follow with a comma ("Once upon a time,"). In other words, there is almost certainly some _ambiguity_ to contend with in our context embeddings.
* **Dudertraining:** Most practical language models (including LLMs) are not trained for more than a few epochs [14; 15]. Further optimization typically renders diminishing returns in improving evaluation performance  long before any possible TPT.

### Contributions

We train a suite of Transformer-based  CLMs1 across a grid of model widths, depths, and training epochs on the TinyStories dataset  to assess the degrees to which \(\) properties develop and how they relate to generalization performance. Our findings (summarized in Figure 1) reveal:

* **Emergence of \(\) with scale:** As we scale model size and training, the properties of \(\) emerge; within-class variability (\(_{1}\)) and interference are reduced while hyperpsherical uniformity (\(_{2}\)), uniform duality (\(_{3}\)), and classifier agreement (\(_{4}\)) improve.
* **Progression towards hyperspherical uniformity:** Class means, while unable to form a simplex ETF (\(_{2}\)), nonetheless tend towards uniform dispersion on a hypersphere, a geometry theorized by  and formalized by  as _hyperspherical uniformity_ (\(2\)).
* which we term _uniform duality_ (\(_{3}\))
- may be more cohesive with \(\).
* **Correlation between \(\) and generalization:** The developments of \(\) properties are correlated with improved validation performance. We show these correlations to persist even when fixing the (potential confounders of) model architecture and training by simply varying the random seeds for initialization and data shuffling. This suggests that \(\) is not simply a side-effect of training, but possibly a factor of generalization in its own right.

### Significance

Recently, methods building on \(\) have found use in deep learning at large. We highlight areas such as federated learning , graph neural networks , incremental/continual learning [22; 23; 24], meta-learning [24; 25], out-of-distribution detection [26; 27; 28], privacy [29; 30], learning theory [31; 32; 33; 34; 35; 36] transfer learning [37; 3; 3; 38; 39; 40], and even LLM prompting . With our results, we aim to extend insights from such contributions and other related works to the autoregressive language modelling domain and ultimately assist researchers in improving and interpreting their (L)LMs.

## 2 Related Works

\(\) was initially observed in image classification tasks such as on CIFAR-10/100  and ImageNet . Since then, the phenomenon has been further studied both theoretically and empirically [5; 18; 44; 45; 46; 5; 47; 48; 5; 49; 60; 5; 44; 46; 5; 48; 5; 5; 50; 51; 52; 53; 54; 55; 56; 57; 58; 59; 61; 62; 63; 64; 65; 66], with several works venturing into settings without some of the traditional prerequisites (~Rq1-4, SS1.2) and proposing adaptations of the analysis framework or optimization procedures:

A large number of classes (~Rq1)\(\) established the simplex ETF as an optimal configuration. However, a perfect simplex ETF (\(_{2}\)) requires that the number of classes \(C\) not exceed \(d+1\) where \(d\) is the embedding dimension. This requirement that \(d\) be sufficiently large is impractical when the classes number beyond the thousands.2 For instance, GPT-2  and Llama 3.1  have vocabularies of around 50K and 128K tokens, respectively.

In such a scenario, one might still expect class means to be uniformly distributed on a \(d\)-dimensional hypersphere .  formalize this as _hyperspherical uniformity_ within a _generalized neural _collapse_ (\(\)) framework, which  then empirically demonstrate. These latter two works mention language modelling as applicable for \(\);  even framed token prediction as a classification task, just as we do. We remark however that both earlier works _simulate_ a large number of classes by drastically shrinking the embedding dimension. In contrast, we study realistic NTP, using the full class space (vocabulary) with an imbalanced token distribution and ambiguous samples.

Class imbalance (-Rq2)\(\) traditionally assumed that classes were sample-balanced. Since then, follow-up works have investigated the effect of class imbalance on the prevalence of \(\).  studied the _layer-peeled model_ (LPM) and discovered that _minority collapse_ occurs when classes are imbalanced across two equally-sized groups of classes; a threshold for minority collapse was later characterized by .  showed that \(\) still occurs in such an LPM when the classifier is initialized as a fixed ETF.  introduced _simplex-encoded-label interpolation_ (SELI) but noted that more severe imbalance worsens even this geometry. Recently, feature regularization has been employed to induce \(\) and improve generalization in class-imbalanced settings [58; 61; 62].

Multi-label classification (-Rq3)In some problems, one might encounter mixed or multi-label samples, be they natural or due to noise or augmentation [71; 72]. \(\) was also recently studied for such data by , who observed that multi-label class means arrive at the average of their labels' respective means. They also devise an augmented CE loss function to accommodate such samples.

Likewise, most of our ambiguous samples could be considered multi- or soft-label: identical (or very similar) context samples with different hard token labels (-Rq3). Under popular CLM pre-training (teacher-forcing with CE loss), this effectively precludes the prospect of achieving zero classification error and potentially introduces irreducible noise.

A recent study showed that memorization of noisy labels likely leads to degradation of \(\).

Early stages of training (-Rq4) studied \(\) in small ResNet  models that had not yet converged (similar to most LLMs). They show that within-class variability drops and "plateaus" (\(_{1}\)) earlier than other \(\) metrics, a result that we also observe (SS4.1, Figures 6, 7).

Natural language processing (NLP)An earlier study reported that the ratio of within-class to between-class covariances of word embeddings increases with model depth [75; 76], seemingly at odds with \(_{1}\). It can, however, be distinguished from literature studying layer-wise \(\) in that it does not centre the mean representation vectors (i.e. subtract the global mean).

 fine-tuned BERT  using their _hyperspherical uniformity gap_ (HUG) objective on binary classification tasks from the GLUE benchmark .  conducted a tangentially-related investigation of convolutional neural networks for few-class semi-supervised clustering in which they identify \(\). Our work is distinct from these in several ways: a) our class space far exceeds our embedding dimension (\(C d+1\)) because we classify on the full token vocabulary; b) we analyze embeddings at a token-level granularity rather than at the sequence level; and c) our NTP task is causal (context-dependent) as opposed to the per-sample independence of their few-category classification.

A more related work studied _feature collapse_ in individual word representations , but the authors note that their analysis is limited to shallow NLP modelling on more rigid and tabular text data.

## 3 Methodology

Below we describe the training setup for our CLMs (SS3.1), procedures3 for collecting top-layer embeddings (SS3.2, 3.7), and measurements of \(\) and generalization (SS3.4, 3.5, 3.6, 3.7, 3.8).

### Dataset and Language Models

TinyStories  is a synthetic4 dataset generated by GPT-3.5 and GPT-4 using around 1500 English words a child might use. NTP is performed by sampling from the token vocabulary \(= 1,29233\)which for our purposes can therefore be framed as classification among \(C=29,\!233\) classes.5 Following the GPT-style preprocessing regime , raw sequences are packed into \(S\) chunks of size \(T\), providing \(N=S(T-1)\) token samples for training.6 Details are listed in Appendix A.

We use 30 CLM architectures based on GPT Neo , configured similarly to . They vary in width (embedding dimension) \(d\{64,128,256,512,768,1024\}\) and depth (number of self-attention layers) \(L\{1,2,4,8,12\}\). Our models were trained by teacher-forcing7 using CE loss. For each architecture, we trained multiple models for 1, 3, and 10 epochs ablated over weight decay factors \(=0.0005\) and \(=0.1\). Further details are listed in Appendices B, C.

### Context Embeddings

Base CLMs perform next-token prediction: given a sequence of tokens \(_{1:t}^{t}\), a top-layer context embedding \((_{1:t})^{d}\) is used to predict the next token \(x^{}_{t+1}\) where \(1 t T\). A classifier for class \(c\) with weights \(_{c}\) and bias8\(b_{c}\) would make maximum a prior (MAP) predictions as

\[x^{}_{t+1}:=*{argmax}_{c}_{ c},(_{1:t})+b_{c}.\] (1)

Class embedding meansFor each token class \(c\), we are interested in accumulating the mean embedding \(_{c}^{d}\) across sequences \(s\) and their contexts \(^{(s)}_{1:t}\), where the next token \(x^{(s)}_{t+1}\) is ground-truth (\(t<T\)) and equal to \(c\):

\[_{c}:=}_{s=1}^{S}_{t=1}^{T-1}( ^{(s)}_{1:t})(x^{(s)}_{t+1}=c),\] (2)

where \(N_{c}\) is the number of samples of class \(c\) and \(\) is the (binary) indicator function. We also utilize their unweighted9 average \(}:=_{c}[_{c}]\), and subsequently the globally-centred means \(}_{c}=_{c}-}}{\|_{c}-}\|_{2}}\).

Class embedding variancesIn a second pass, we accumulate the sample variance norms:10

\[_{c}^{2}:=}_{s=1}^{S}_{t=1}^{T-1}\| (^{(s)}_{1:t})-_{c}\|_{2}^{2}(x ^{(s)}_{t+1}=c).\] (3)

### Homogeneity and Variability

For some collapse measures (such as \(()_{2}\) and \(_{3}\)), we are primarily interested in the _variation_ rather than the average of pairwise relationships. To that end, we also include in our analysis the _coefficient of variation_ (CoV) of several measurements, which is the ratio of their standard deviations to their means: \(()/()\). We can interpret this as a normalized measure of variability.

### Signal-to-Noise Ratio -- \(1\)

The ability to disambiguate between classes depends on the ratio of within-class to between-class variabilities. Building upon foundual works , \(\) originally measured variability through an inverse _signal-to-no ratio_ (SNR), whose minimization constitutes _within-class variability collapse_ (\(_{1}\)). We instead employ a _class-distance normalized variance_ (CDNV) similar to :

\[_{c,c^{}}:=_{c}-_{c^{}}\|_ {2}^{k}}^{2}+_{c^{}}^{2}}{2\|_{c}- {}_{c^{}}\|_{2}^{2}}, c c^{}.\] (4)

Our metric differs in that we divide by an additional power \(\|_{c}-_{c^{}}\|_{2}^{k}\) of the mean distance norm. This further downweights the CDNV within well-separated class pairs in favour of emphasizing more mutually noisy pairs. We found this augmented CDNV with \(k=2\) to be especially useful in our setting of many imbalanced and variably confusable token classes.

These pairwise measures constitute the off-diagonal11 entries of a symmetric matrix in \(^{C C}\), whose average we use as an inverse SNR. Within-class variability collapse is then re-characterized by the minimization of this quantity: \(_{c,c^{}} 0, c c^{}\). This alternative convergence is empirically faithful to \(_{1}\) but more robust and numerically stable .

### Geometric Structures -- \(()2\)

The separability of our representations also depends on the geometric structures found in our embeddings.  characterize \(_{2}\) as convergence to a _simplex equiangular tight frame_ (ETF) [87; 88].

EquinormnessSuch a near-orthonormal configuration firstly implies that class means are equinorm:

\[\|_{c}-}\|_{2}-\| _{c^{}}-}\|_{2} 0, c c^{}.\] (5)

We measure CoV in the _logarithms_ of class mean norms to assess the degree of "equinormness".

Equangularity\(_{2}\) also entails that class means are equiangular about their centre \(}\): pairwise distances and angles between their class means should be maximized and similar. Following , we measure _interference_ (sometimes known as similarity or _coherence_[89; 90]). Its minimization,

\[}_{c},}_{c^{}} , c c^{},\] (6)

together with equinormness (Equation 5) constitute convergence to a simplex ETF. Although this geometry is not ultimately attainable since there are too many classes (\(C>d+1\)), it can still be meaningful to measure a model's tendency towards one. As with CDNV noise (Equation 4), pairwise interferences form off-diagonal12 entries in a symmetric matrix in \(^{C C}\). The minimization of CoV in interferences therefore expresses the degree of "equinangularity".

Hyperspherical uniformityA relaxation from the ETF is _hyperspherical uniformity_ (\(_{2}\)), with equinorm (Eq. 5) means \(_{c}\) uniformly distributed on the \(d\)-dimensional hypersphere [18; 19]. We likewise gauge the angular uniformity with pairwise interactions through some distance kernel \(K\):13

\[_{c c^{}}K(}_{c},}_{c^{}})_{}_{1},, }_{C}}_{c c^{}}K(}_{c},}_{c^{}}).\] (7)

### Alignment Between Classifiers and Class Embedding Means -- \(()3\)

The linear classifiers \(\{_{c}\}_{c=1}^{C}\) lie in a dual vector space to that of the class means \(\{_{c}\}_{c=1}^{C}\). While convergence to self-duality (\(_{3}\)) was initially measured as distances  between class means and classifiers (Equation 11), we follow  to inspect class-wise cosine similarities:14

\[_{c}}{\|_{c}\|_{2}}, }_{c} 1, c.\] (8)

For intuition analogous to that for equinormness and equiangularity (SS3.5), we also measure _uniform duality_ (\(_{3}\)) as the minimization of the CoV of these similarities (Appendices N, O).

### Agreement of the Classifiers -- \(4\)

Finally, \(_{4}\) is described as the simplification (or approximation) of the linear classifier's MAP prediction behaviour (Equation 1) to that of the implicit _nearest-class centre_ (NCC) classifier:

\[*{argmax}_{c}_{c}, +b_{c}*{argmin}_{c }\|-_{c}\|_{2}, .\] (9)We calculate15 agreement as the proportion of validation samples on which the classifiers agree: 16

\[}}_{s=1}^{S_{}}_{t=1}^{T_{}-1 }({x^{}_{t+1}}^{(s)}=*{argmin}_{c}\|(x^{(s)}_{1:t})-_{c}\|_{2} ).\] (10)

### Probing a Relationship Between \(\) and Generalization

To isolate the effect of \(\) on generalization independent of model scaling and training (if it exists), we selected a two-layer \(768\)-wide architecture of which to train twenty more instances with weight decay \(=0.0005\), each with a different data shuffling seed. We then followed the remainder of the pipeline described above to collect and analyze embeddings with respect to \(\). Finally, we performed a permutation test with \(10^{4}\) trials to determine the statistical significance of any correlation between \(\) and generalization that remains when we hold constant all factors but shuffling seeds.

## 4 Experimental Results

In this section, we present the results from our empirical study on scaling and generalization:

* Within-class variability is reduced across model scale (more so by width than depth) and training (up to 6 epochs), and is tightly correlated with validation performance.
* Equinormness/equiangularity shows some improvement with scale, training, and performance. Hyperspherical uniformity (\(_{2}\)) also improves but more clearly and consistently.
* Our models fail to achieve self-duality: class means do not align with classifiers. However, uniform duality (\(3\)) is correlated with model width, training, and performance.
* Larger or more trained models exhibit closer agreement between their linear and implicit NCC classifiers. Agreement is also associated with validation performance.

### Within-Class Variability Collapse -- \(1\)

Scaling our models dramatically reduces normalized variance, which is further aided by more training epochs and stronger weight decay (Appendix Figs. 6, 7). These noise reductions tightly associate with generalization (Fig. 1, left, "\(_{1}\)"). The relationship is most apparent at scale.

### Geometric Structures -- \(()2\)

EquinormnessLogarithmic class mean norms grow with model width and training (Appendix Fig. 8), and subtly with depth (Appendix Fig. 9). Meanwhile, the variation of these (logarithmic) norms consistently decreases (improving equinormness) with scale (Appendix Figs. 10, 11). Both trends correlate with improved generalization (Appendix Fig. 20).

Figure 2: Validation loss is correlated with all three measurements: **(left)** equinormness (\(2\)) expressed as variation in logarithmic norms; **(centre)** equiangularity (\(2\)) as variation in interference; **(right)** hyperspherical uniformity (\(2\)) as variation in logarithmic pairwise distances.

EquiangularityScaling model dimensions reduces average interference (Appendix Figs. 12, 13) down to an empirical plateau of approximately \(10^{-3}\), which is more apparent in less-trained models. However, the variation of interference rises and persists when scaling (Appendix Figs. 14, 15), suggesting that interference becomes more concentrated between some pairs of classes. These results could be due to various factors, including but not limited to unfriendly conditions of language modelling (SS1.2) or the impossibility of a perfect simplex ETF (SS3.5).

Appendix Figure 21 shows only a rough performance correlation with average interference (i.e. coherence) and a more definite -- albeit still noisy -- one with the variation of interference (i.e. equiangularity). In other words, the limited trends we observed toward a simplex ETF suggest that the association of \(2\) with generalization begins to break down when \(C>d+1\).

Hyperspherical uniformityLogarithmic distances drop more gradually and consistently with scale (Appendix Figs. 16, 17), implying this quantity is more robust or may not face the same geometric barriers seen in conventional interference (Appendix Figs. 14, 15). Variation of these logarithmic distances is also consistently reduced with scale (Appendix Fig. 18, 19).

And finally, generalization has much stronger correlations with logarithmic distances than it has with regular interference (Fig. 2), validating the applicability of \(\) when \(C>d+1\).

### Classifier (Mis)alignment and Duality -- \(()3\)

Model width (\(d\)) is correlated faintly with the average similarity between class means and their respective classifiers (Appendix Fig. 23), but strongly with variation in similarity (Appendix Fig. 25). The relationships to generalization follow the same pattern (Fig. 3), suggesting that uniform duality (\(_{3}\)) _might_ serve as a better \(\) property than self-duality (\(_{3}\)) overall; we discuss this in SS5.1.

### Classifier Agreement -- \(4\)

The linear and NCC classifiers agree on far more examples than random chance, and model scaling encourages this agreement (Appendix Figs. 29, 30). Increasing width for certain depths happens to plateau or even regress the agreement rate, but this limitation is overcome with further training. And finally, agreement is a strong indicator of generalization (Fig. 1, right, \(_{4}\)).

## 5 Analysis

We find that \(\) is generally promoted by model size and training and correlated with generalization (validation performance). We also discern some of this correlation independent of scale (SS5.1).

Figure 3: Validation loss shows a negligible relationship with self-duality (\(_{3}\), left) and some correlation with uniform duality (\(_{3}\), right). In other words, \(_{3}\) develops with scale and correlates with generalization much better than \(_{3}\).

### Neural Collapse's Relationship with Generalization

Table 1 presents the correlation scores of \(\) metrics with generalization alongside their associated p-values from the permutation tests described in SS3.8. The majority of the correlations are statistically significant (\(p<0.05\)) independent of scale, affirming that \(\) is correlated with generalization.

### Duality of Duality

The dichotomy of self-duality (\(_{3}\)) and uniform duality (\(_{3}\)) is rather conspicuous. Our main experiments find that \(_{3}\) does not consistently develop with scale while \(_{3}\) does (Fig. 3). However, within a fixed scale, the opposite is true, implying that \(_{3}\) may be confounded by model capacity while \(_{3}\) is a subtle and fine-grained indicator of generalization.

### The Effect of Weight Regularization

Our models trained with either weight decay factor exhibited very similar patterns in the emergence of \(\) (or lack thereof), but the more aggressive factor \(=0.1\) resulted in stronger development of \(\) properties than with \(=0.0005\) (Appendices E, F, G, H, I, J, K, M, N, P). These findings empirically affirm \(=0.1\) weight decay as sensible for CLM pre-training , and concur with  on the pivotal role that appropriate regularization plays in the emergence of \(\).

## 6 Limitations

Neural collapseWhile to the best of our knowledge, no previous work has studied realistic stochastic token prediction, it is possible that the quantities that we measure are not perfectly suited for \(\) in language modelling. As we described in SS1.1, the \(\) framework does not translate neatly to the language modelling space due to many adverse conditions, so full convergence to \(\) in the TPT was highly improbable. This paper leaves much room for future work to better adapt \(\) for next-token prediction, which we discuss further in Section 7.

Language modellingOur work focused on autoregressive pre-training in its most basic form. We did not conduct experiments into encoder, multi-modal, or instruction-tuned models. Post-training techniques such as supervised fine-tuning, reinforcement learning with human feedback  or direct preference optimization  are also out-of-scope. This paper uses validation CE loss as the sole indicator of performance, leaving out any downstream task evaluations.

Confounder of model scaleThe models that we used in our permutation test (SS5.1, Table 1) are only of a single small architecture trained for one epoch with relatively weak weight regularization (\(=0.0005\)). Therefore, our experimental results on scale-independent links between \(\) and generalization may not necessarily translate to larger models. Further investigation on (foundation) LLMs orders of magnitude larger than our CLMs trained with modern NLP methods would provide more robust insight into any direct correlations.

   Property & Measurement & \(R^{2}\) Correlation (\(\)) & \(p\)-value (\(\)) \\  \(_{1}\) & Within-Class Variability Collapse & \(0.192011\) & \(\) \\ \(_{2}\) & Equinormness & \(0.026174\) & \(0.4870\) \\ \(_{2}\) & Equiangularity & \(0.218574\) & \(\) \\ \(_{2}\) & Hyperspherical Uniformity & \(0.487935\) & \(\) \\ \(_{3}\) & Self-Duality & \(0.322210\) & \(\) \\ \(_{3}\) & Uniform Duality & \(0.000036\) & \(0.9784\) \\ \(_{4}\) & Classifier Agreement & \(0.490278\) & \(\) \\   

Table 1: Permutation test of \(\) measurements with respect to validation loss. Twenty-one (21) identical two-layer \(768\)-wide models were trained with different data shuffling seeds and permuted with \(10^{4}\) trials. The \(p\)-values below \(0.05\) (bolded) show those properties to be statistically significant.

## 7 Discussion

Layer/depth-wise neural collapsePast works have established that properties resembling \(\) evolve as a function of model depth . Layer-wise \(\) -- sometimes dubbed _deep neural collapse_ (\(\))  -- and related phenomena at intermediate layers remain an interesting subtopic. We leave their study and induction in CLMs (like ) as future work.

Learning to collapseGiven the evidence for the development of \(\) and associated generalization under various loss functions  in other domains, NLP researchers may still benefit from analyzing, adapting or training towards \(\). As alluded to earlier, the simplex ETF and even the CE loss may not be truly optimal for this problem setting, so we anticipate future works to both construct more amenable geometries with better-suited objectives and then capitalize on their benefits downstream. As discussed in SS2, there is an abundance of literature in \(\), some of which could potentially adapt \(\) to be useful for NLP; we hope to inspire more such investigations.

InterpretabilityAt a high level, the number and density of clusters for a token can reflect its learned meanings and uses. This would be particularly useful as LLMs adapt to ever-evolving language and further expansion into non-English domains. Our formulae (Section 3) and results (Section 4) expose the pairwise token class interactions in noise, interference, classifier duality, and classifier agreement in the top-level features. Similarly to works in other domains , these \(\) metrics can serve as a form of low-level interpretability to aid understanding certain behaviours of (L)LMs. Between tokens, one can often discern how related or interchangeable words are based on their pair-wise interactions, or how antithetical or unrelated they are based on orthogonality. For example, we present a rudimentary analysis of homonyms and English first names in Appendix Q.

FairnessFoundation models are ubiquitous for their comprehensive capabilities and adaptability. As previous work discussed class imbalance , our work may extend these strategies to measure and perhaps promote fairness in foundation LLMs, some of which are designed to be multilingual or multicultural. For example,  contemporarily explores the use of \(3\) to mitigate biases in BERT-based  models.

While \(\) itself would not lead to unfairness, its potential interpretability may, in theory, enable an (adversarial) agent to measure and optimize for (un)fairness as they manipulate an LLM.

LLM EvaluationsResearchers in NLP and multimodal settings are ultimately interested in measuring model performance on practical tasks; notable benchmarks include GLUE , MMLU , and BIG-bench . However, several contemporaries  have demonstrated that models' downstream capabilities are roughly correlated with their abilities to effectively compress their pre-training data. Based on their findings, our application of the \(\) framework to the pre-training stage of CLMs against validation CE loss should be an appropriate first step in this intersection. Looking forward, we anticipate exciting analysis for language modelling tasks or benchmarks, especially on creativity and retrieval for natural language understanding and generation.

Conversely, some form of \(\) could be an alternative evaluation. Although it would be prohibitively expensive to measure \(\) on the vast and sometimes obscure pre-training data of most frontier production LLMs, doing so on a small set of in-distribution data (i.e. test set) would be realistic.

## 8 Conclusion

In this paper, we apply the _neural collapse_ (\(\)) framework to the next-token prediction problem, where models are undertrained and next-tokens are variably drawn from numerous and imbalanced token classes. We leverage canonical and more recent metrics to demonstrate that \(\) emerges as we scale the size and training of hundreds of causal language models. Our results show a correlation between \(\) and generalization, a relationship that persists even when the model scale is fixed.

In the short term, our work presents rudimentary techniques to analyze and interpret token-level properties of (L)LMs. We anticipate future work to suitably adapt \(\) (and related frameworks) to the still-fresh frontier of autoregressive language modelling. Researchers could then effectively capitalize on previous learnings from \(\) to better understand and improve the pre/post-training processes of increasingly complex and large language (multimodal) models.