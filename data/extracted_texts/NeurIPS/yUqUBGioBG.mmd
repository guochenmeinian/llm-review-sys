# Class Distribution Shifts in Zero-Shot Learning:

Learning Robust Representations

 Yuli Slavutsky

Department of Statistics and Data Science

The Hebrew University of Jerusalem

Jerusalem, Israel

yuli.slavutsky@mail.huji.ac.il &Yuval Benjamini

Department of Statistics and Data Science

The Hebrew University of Jerusalem

Jerusalem, Israel

yuval.benjamini@mail.huji.ac.il

###### Abstract

Zero-shot learning methods typically assume that the new, unseen classes encountered during deployment come from the same distribution as the the classes in the training set. However, real-world scenarios often involve class distribution shifts (e.g., in age or gender for person identification), posing challenges for zero-shot classifiers that rely on learned representations from training classes. In this work, we propose and analyze a model that assumes that the attribute responsible for the shift is unknown in advance. We show that in this setting, standard training may lead to non-robust representations. To mitigate this, we develop an algorithm for learning robust representations in which (a) synthetic data environments are constructed via hierarchical sampling, and (b) environment balancing penalization, inspired by out-of-distribution problems, is applied. We show that our algorithm improves generalization to diverse class distributions in both simulations and experiments on real-world datasets.

## 1 Introduction

Zero-shot learning systems [14; 27] are designed to classify instances of new, previously unseen classes at deployment, a scenario known as _open-world_ classification. These systems are widely applied in extreme multi-class applications, such as face or voice recognition  for matching observations of the same individual, and more generally, for learning data representations .

Class distribution shifts typically refer to changes in the prevalence of a fixed set of classes between training and testing. In zero-shot learning, however, a different challenge arises: the appearance of entirely new classes at test time. This raises a critical question - are these new classes drawn from the same distribution as the training classes? Most zero-shot methods assume that they are, an assumption that not only shapes the design of test sets [57; 16] but also plays an explicit role in assessing the generalization capabilities of zero-shot classifiers [59; 48].

In practice, training classes are often chosen based on convenience and accessibility during data collection. Even when data is carefully collected, the distribution of classes may shift over time, leading to a different distribution. For instance, this could occur when a face recognition system is deployed in a building located in a neighborhood undergoing demographic changes.

Class distribution shifts pose significant challenges to zero-shot classifiers, since they rely on learning data representations from the training classes to distinguish new, unseen ones. Typically, these classifiers are trained by minimizing the loss on the training set to effectively separate the training classes. However, this approach may result in poor performance when confronted with data from distributions that differ significantly from the class distribution in the training data. Notably, in person re-identification, this concern gained attention from a fairness perspective with respect to gender, age , and racial  bias. In all these studies the variable (i.e., gender, age, race) expected to cause the distribution shift was known in advance.

In contrast, in real-world scenarios, the attribute responsible for a future distribution shift is usually unknown during training. In such cases, existing approaches based on collecting balanced datasets or re-weighting training examples  are inapplicable. Furthermore, while class distribution shifts have been extensively studied in the standard setting of supervised learning (see Appendix A), previous research assumed a _closed-world_ setting that does not account for new classes at test time. Instead, it only addressed changes in the prevalence of fixed classes between training and testing. Consequently, class distribution shifts in zero-shot learning remain largely unaddressed.

In this paper we first address these limitations by examining the effects of class distribution shifts on constrastive zero-shot learning, by proposing and analyzing a parametric model (SS3). We identify conditions where minimizing loss in this model leads to representations that perform poorly when a distribution shift has occurred.

We then use the insights gained from this model to present our second contribution (SS4): an algorithm for learning representations that are robust against class distribution shifts in zero-shot classification. In our proposed approach, _artificial data environments_ with diverse attribute distributions are constructed using hierarchical subsampling, and an _environment balancing_ criterion inspired by out-of-distribution (OOD) methods is applied. We assess our method's effectiveness in both simulations and experiments on real-world datasets, demonstrating its enhanced robustness in SS5.

### Problem Setup

Let \(\{z_{i},c_{i}\}_{i=1}^{N_{z}}\) be a labeled set of training data points \(z\) and classes \(c\), such that \(c_{i}\) is the class of \(z_{i}\).

In this work, we focus on verification algorithms that enable _open-world_ classification by determining whether two data points \(x_{ij}(z_{i},z_{j})\) belong to the same class. For instance, in person re-identification the task is to identify whether two data points (e.g., face images or voice recordings) belong to the same person. We denote this by \(y_{ij}\), where \(y_{ij}=1\) if \(c_{i}=c_{j}\) and \(y_{ij}=0\) otherwise. When the identity of each data point in the pair is not important, a single index is used for simplicity, namely \((x_{k},y_{k})\).

We assume that each class \(c\) is characterized by some attribute \(A\). We further assume that the training classes are sampled from \(P_{C}(c)\), the test classes are sampled according to \(Q_{C}(c)\), and the two distributions differ solely due to a shift in the distribution of an attribute \(A\):

\[P_{C}(c)= P_{C|A}(c|a)_{A}(a)\;da, Q_{C}(c)= P_{C|A}(c|a) _{A}(a)\;da.\] (1)

Importantly, we assume that the attribute \(A\) is unknown, and that both during training and testing, data points \(z\) for each class are sampled according to \(P_{Z|C}(z|c)\). For instance, revisit the person identification example where each person is a class. If the attribute \(A\) is binary (e.g., \(a_{1}\) is blond and \(a_{2}\) is dark-haired), then \(P(C|A=a_{1})\) represents the distribution of people with blond hair, and \(P(C|A=a_{2})\) of individuals with other hair colors. The training classes might be predominantly sampled from the blond population \(P(A=a_{1})=_{}=0.8\), while test classes are predominantly sampled from \(Q(A=a_{1})=_{}=0.1\).

We focus on verification techniques based on _deep metric learning_ methods (for surveys see ) such as contrastive-learning , Siamese neural networks , triplet networks , and other more recent variations . These methods learn a representation function that maps data points to a representation space \(g:}\), so that examples from the same class are close (in a predefined distance function \(d(,)\)), while those from different classes are farther apart.

We assume that \(g\) is a neural network trained by optimizing a deep-metric-learning loss, such as the contrastive loss :

\[(z_{i},z_{j},y_{ij};d_{g}) y_{ij}d_{g}^{2}(z_{i},z_{j})+(1-y_{ij} )\{0,m-d_{g}(z_{i},z_{j})\}^{2}\] (2)

where \(m 0\) is a predefined margin, and \(d_{g}(z_{i},z_{j}) d(g(z_{i}),g(z_{j}))\) is the distance between the representations of the datapoints \(z_{i},z_{j}\). In our theoretical analysis, we examine the no-hinge

[MISSING_PAGE_FAIL:3]

\(s_{i}=\{s g(z_{i}),1-s g(z_{i})\}\) be the _confidence_ on the \(i\)-th data point. Denote the _correctness_ as \(b_{i}=\{|c_{i}-s_{i}|<\}\), and let \(K:\) be a universal kernel. Let \(Z^{e}\) denote the training data in the environment \(e\). The authors proposed using the MMCE as a penalty in an objective that takes the form of Equation 5 with \(R^{e}_{}(s,g_{})=_{z_{i},z_{j} Z ^{e}}}(b_{i}-s_{i})(b_{j}-s_{j})K(s_{i},s_{j}).\)

**Variance Risk Extrapolation (VarREx)** proposed by Krueger et al. , is based on the observation that reducing differences in loss (risk) across training domains can reduce a model's sensitivity to a wide range of distribution shifts. The authors found that using the variance of losses as a regularizer is stabler and more effective compared to other penalties. Therefore, they propose the following regularization term for \(n\) training environments: \(R_{}(g_{},E_{})=( ^{e_{1}}(g_{}),,^{e_{n}}(g_{})).\)

While simple and intuitive, this approach assumes that losses across different environments accurately reflect the classifier's performance. However, as discussed in SS4, this is often not true for deep metric learning losses, where significant changes in loss may correspond to only minor variations in performance.

## 3 Parametric Model of Class Distribution Shifts in Zero-Shot Learning

In this section, we introduce a parametric model of class distribution shifts. Our model shows that in zero-shot learning, even if the conditional distribution of data given the class \(P(z|c)\) remains the same between training and testing, a shift in the class distribution from \(P(c)\) to \(Q(c)\) can cause poor performance on newly encountered classes sampled from the shifted distribution \(Q(c)\).

Assume that for all classes, the data points \(z_{i}^{d}\) are sampled from \(z_{i}|c_{i}(c_{i},_{z})\), where \(_{z}=_{z} I_{d}\), and \(I_{d}\) is the identity matrix. Let the attribute \(A\) indicate the type of a class \(c\), with two possible types: \(a_{1}\) and \(a_{2}\). Assume that the classes \(c_{i}\) are drawn according to \(c_{i}(0,_{a})\) for \(a\{a_{1},a_{2}\}\). Finally, assume that in training, \(a_{1}\) is the majority type with \(P(a_{1})=_{} 0.5\), whereas at test time, \(a_{2}\) is the majority type with \(Q(a_{1})=_{} 0.5\).

We construct the model such that differences between the training class distribution \(P(c)\) and the test distribution \(Q(c)\) stem solely from a shift in the mixing probabilities of an unknown attribute \(A\) (see Equation 1). Therefore, we define \(_{a}\) as a diagonal matrix with replicates of three distinct values on its diagonal: \(_{0},^{+},^{-}\). Let \(0<^{-}<_{z}_{0}<^{+}\). Then, in the coordinates corresponding to \(_{0}\) and \(^{+}\) data points from different classes are well separated, whereas in the coordinates corresponding to \(^{-}\) they are not. Assume the coordinates corresponding to \(_{0}\) are shared by both types, but \(^{+}\) and \(^{-}\) are swapped:

\[_{a_{1}} =\,,,_{0}, \,}^{d_{0}},,,^{+}}^{d_{1}},, ,^{-}}^{d_{2}},\] \[_{a_{2}} =\,_{0},,_{0}\,,^{-}, ,^{-},\,^{+},,^{+}\,.\]

An illustration with one replicate of each value is shown in Figure 1.

The following proposition shows that if the number of dimensions \(d_{1}\) that allow good separation for classes of type \(a_{1}\) is relatively similar to the number of dimensions \(d_{2}\) that enable good separation for classes of type \(a_{2}\), specifically if \(h_{l}(,_{z},_{0},_{1},_{2})<+2}{d_{1}+2}< h_{u}(,_{z},_{0},_{1},_{2})\), then the optimal solution for the training distribution prioritizes the components (features) corresponding to \(^{+}\) for classes of type \(a_{1}\). Thus, the prioritized features allow good separation for classes from the majority type in training, but offer poor separation for the shifted test distribution, where most classes are of type \(a_{2}\). Note that if \(d_{2}\) is large, when combined, the corresponding components may still provide reasonable separation. We define \(h_{l}\) and \(h_{u}\) in Equation 3 and provide the proof of Proposition 1 in Appendix B.2.

**Proposition 1**.: _Consider a weight representation \(g(z)=Wz\), where \(W^{d d}\)is a diagonal matrix, and the squared Euclidean distance \(d_{g}(z_{i},z_{j})=\|W(z_{i}-z_{j})\|^{2}\). Let \(W^{*}=(w^{*})_{W}[ {}(,,;d_{g})]\). Denote \(_{1}^{*2}=}_{k=d_{0}+1}^{d_{1}}w_{k}^{*}\) and \(_{2}^{*2}=}_{k=d_{1}+1}^{d}w_{k}^{*2}\). Then, for all \(>\) and \(_{z},_{0},_{1},_{2},d_{1},d_{2}\) satisfying \(h_{l}(,_{z},_{0},_{1},_{2})<+2}{d_{1}+2}< h_{u}(,_{z},_{0},_{1},_{2})\) it holds that \(d_{2}_{2}^{*2} d_{1}_{1}^{*2}\)._Note that the conditions outlined in Proposition 1 are sufficient but not necessary. Accordingly, in Appendix B.3, we provide the complete analytical solution for \(w^{*}\) that minimizes the expected loss \([(,,;d_{g})]\) for the weight representation \(g(z)=Wz\), using the squared Euclidean distance. According to Proposition 1, larger \(d_{2}\) values favor \(^{-}\) for better aggregated separation. Increasing \(_{0}/^{+}\) leads to increased differences between \(w_{1}^{*2}\) and \(w_{2}^{*2}\), and vice versa for \(_{0}/^{-}\).

These relationships in the optimal solution are illustrated in Figure 2, showcasing different scenarios. The top row shows that when \(d_{1}=d_{2}=10\) dimensions favoring classes of type \(a_{1}\) are prioritized for \(>0.5\), while those favoring type \(a_{2}\) are prioritized for \(<0.5\). When \(d_{1}=10\) while \(d_{2}=5\), dimensions favoring type \(a_{1}\) are prioritized for all values of \(\), and vice versa when \(d_{2}\) is significantly larger than \(d_{1}\). The middle and the bottom row further explore the \(d_{1}=d_{2}\) case, showing how differences in separability between shared dimensions (\(_{0}\)) and type-favoring dimensions impact weight allocation.

Since components corresponding to \(^{+}\) for classes of type \(a_{1}\) align with \(^{-}\) for classes of type \(a_{2}\), the optimal representation for the training distribution results in poor separation for the shifted test distribution. Therefore, a robust representation should prioritize dimensions that provide effective separation for both class types, corresponding to \(_{0}\).

This aligns with a common principle in the OOD generalization field, where robust representations are those that rely on features shared across environments (see SS2). This principle is often referred to as _invariance_.

## 4 Proposed Approach

Motivated by our analysis of the parametric model, we propose a new approach for tackling class distribution shifts in zero-shot learning. Our approach revolves around two key ideas: (i) during training, different mixtures of the attribute \(A\) can be produced by sampling small subsets of the classes, forming artificial environments, and (ii) penalizing for differences in performance across these environments is likely to increase robustness to the class mixture encountered at test time.

### Synthetic Environments

Standard ERM training involves sampling pairs of data points \((z_{i},z_{j})\) uniformly at random from all \(N_{c}\) classes available during training. However, as discussed in SS3, this is prone to overfitting to the attribute distribution of the training data. Since the identity of the attribute is unknown, weighted sampling (and similar approaches) cannot be used to create environments with different attribute mixtures.

Yet, our goal is to design artificial environments with diverse compositions of the (unknown) attribute of interest. To do so, we leverage the variability in small samples: while class subsets of similar size to \(N_{c}\) maintain attribute mixtures similar to the overall training set, smaller subsets with \(k N_{c}\) classes are likely to exhibit distinct attribute mixtures. Therefore, we propose creating multiple environments, composed of examples from few sampled classes.

This results in a hierarchical sampling scheme for the data pairs: first, sample a subset of \(k\) classes, \(S=\{c_{1},,c_{k}\}\). Then, for each \(c S\) sample \(2r\) pairs of data points as follows: \(r\) pairs from within the class \(c\), \(\{z_{i};c_{i}=c\}\), uniformly at random (positive pairs); and \(r\) negative pairs, where one point is sampled uniformly at random from \(c\), and the other from all other data points in \(S\), \(\{z_{i};c_{i} c,c_{i} S\}\).1

Across multiple class subsets \(S=\{S_{1},,S_{n}\}\), this hierarchical sampling results in diverse mixtures of any unknown attribute (see Figure 3). In particular, in some of the class subsets, classes from the overall minority type constitute the majority in the environment.

### Environment Balancing Algorithm for Class Distribution Shifts

Our goal is to learn data representations that will allow separation between classes without knowing which attribute is expected to change and how significantly. Therefore, we require the learned data representation to perform similarly well on all mixtures obtained on the synthetic environments.

To achieve this, inspired by OOD performance balancing methods (see SS2), we optimize a penalized objective:

\[_{}_{l=1}^{n}^{S_{l}}(g_{})+ R(S_{1},,S_{n})\] (6)

where \(R(S_{1},,S_{n})\) is any balancing term between the constructed synthetic environments.

Note that computing \(R(S_{1},,S_{n})\) often involves evaluating some value on each environment separately. For a general balancing term, we denote the value in the \(l\)-th environment as \(f(S_{l})\) and accordingly express \(R(S_{1},,S_{n})=(f(S_{1}),,f(S_{n}))\), where \(\) represents the corresponding aggregation function. Our approach2 for balancing performance across synthetic environments of class subsets, is outlined in Algorithm 1.

### Balancing Performance Instead of Loss

In multiple OOD penalties (e.g., IRM and VarREx), \(f\) represents the loss in each environment, which, in deep metric learning algorithms, is based on distance. This presents a challenge in zero

Figure 3: Illustration of the proposed hierarchical sampling. Top: \(N_{c}=6\) classes, with 2 minority-type classes D, F (in purple). Middle: synthetic environments formed by sampling small (\(k=3\)) class subsets; in \(}{{5}}\) of the environments, minority-type classes become the majority constituting \(}{{3}}\) of the classes. Bottom: sampling \(r=1\) positive and \(r=1\) negative pairs for each class in the environment.

shot verification, where sampled tuples often include numerous easy negative examples, leading to performance plateau early in the learning process, although the distances themselves still exhibit considerable variations. Strategies like selecting the most difficult tuples  were proposed to address this issue, however these methods have been found to generate noisy gradients and loss values .

We therefore propose to balance performance directly instead of relying on the losses in the training environments. Denote the set of negative pairs in a synthetic environment by \(D_{l}^{0}=\{x_{ij}=(z_{i},z_{j}):c_{i},c_{j} S_{l},y_{ij}=0\}\) and the set of positive pairs by \(D_{l}^{1}=\{x_{ij}=(z_{i},z_{j}):c_{i},c_{j} S_{l},y_{ij}=1\}\). An unbiased estimator of the AUC on a given synthetic environment \(S_{l}\) is given by

\[}(S_{l};d_{g})=^{0}| |D_{l}^{1}|}_{x_{ij}}_{x_{uv}}[d_{g}(x_{ij})<d_{g}(x_{ uv})]\] (7)

for \(x_{ij} D_{l}^{1}\) and \(x_{uv} D_{l}^{0}\). Since this estimator is non-differentiable and therefore cannot be used in gradient-descent-based optimization, we use _soft-AUC_ as an approximation 

\[}(S_{l};d_{g})^{0}| |D_{l}^{1}|}_{x_{ij}}_{x_{uv}}_{}(d_{g}(x_{ uv})-d_{g}(x_{ij}))\] (8)

where a sigmoid \(_{}(t)=}\) approximates the step function. Note that when \(\), \(_{}\) converges pointwise to the step function. Consequently, we propose the penalty:

\[R_{}(S_{1},,S_{n};g_{d})=} (}(S_{1};g,d),, }(S_{n};g,d)).\] (9)

### How Many Environments Are Needed?

The proposed hierarchical sampling scheme allows for the construction of many synthetic environments with various attribute mixtures, influenced by the number of classes in each environment. As shown in the analysis below, this ensures that with high probability there will be at least one environment with a pair of minority type classes, thereby supporting learning to separate negative pairs within the minority type.

In each training iteration, we consider \(n\) class subsets (environments) of size \(k\). Our goal is to achieve robustness to all attribute values \(a\) that are associated with at least \(_{}(0,1)\) of the training classes. Note that \(_{}\) is specified by the practitioner without knowledge of the true attribute that may cause the shift or its true prevalence \(\) in the training set.

We compute the number of synthetic environments \(n\), such that with high probability of \((1-)\), \(S_{1},,S_{n}\) will include at least one subset with at least two classes associated with \(a\) (otherwise none of the subsets would contain negative pairs with the attribute \(a\)). Denote the probability of a given subset not to contain any class associated with \(a\) by \(_{0}=})_{n}_{c}}{k}\)/\(}{k}\) and the probability of a given subset to contain exactly one such class by \(_{1}=_{}N_{c}})_{n}_{c }}{k}\)/\(}{k}\). Therefore, the required number of environments needed to ensure that at least two minority-type classes appear together in the same environment is

\[n}{+_{1})}}.\] (10)

Note that that \(n\) is typically much smaller than \(}{k}\).

## 5 Empirical Results

Our method enhances standard training with two components: a hierarchical sampling scheme and a balancing term for synthetic environments. To the best of our knowledge, this is the first work addressing OOD generalization for class distribution in zero-shot learning. We therefore benchmark our algorithm against the ERM baseline (uniform random sampling with an unpenalized score) and a hierarchical sampling baseline (hierarchical sampling with unpenalized score). Additionally, we tested standard regularization techniques including dropout and the \(L_{2}\) norm, which did not yield notable improvements in the distribution shift scenario, and are therefore not shown.

To ensure a comprehensive comparison, in addition to the proposed VarAUC penalty, we evaluate variants of our algorithm in which the IRM, CLOvE, and VarREx penalties are used instead. While we show that VarAUC consistently outperforms other penalties, the crucial improvement lies in its performance compared to the ERM baseline: application of existing OOD penalties is enabled by the construction of synthetic environments in our algorithm. As discussed in Appendix C, this construction facilitates the formulation of class distribution shifts in zero-shot learning within the OOD setting.

In all of the experiments performed, we trained the network with contrastive loss (Equation 2) and the normalized cosine distance: \(d_{g}(z_{1},z_{2})=(1-) g(z_{2})}{\|g(z_{1}) \|\|g(z_{2})\|})\). The specific setups are detailed below (additional details can be found in Appendix F), and code to reproduce our results is available at https://github.com/YuliSl/Zero_Shot_Robust_Representations.

### Simulations: Revisiting the Parametric Model

We now revisit the parametric model presented in SS3. To increase the complexity of the problem, we add dimensions where classes from both types are not well separated. That is, \(_{a}\) includes additional dimensions set to zero.

**Setup** We used 68 subsets in each training iteration, each consisting of two classes. This corresponds to choosing \(_{}=0.1\) (desired sensitivity, regardless of the true unknown parameter \(\{0.05,0.1,0.3\}\)), with a low \(\) value of 0.5, resulting in the construction of fewer environments according to Equation 10. For each class, we sampled \(2r=10\) pairs of data points. The representation was defined as \(g(z)=wz\) for \(w^{d p}\)3. Here we focus on the case of \(p=16\), \(_{z}=_{0}=1\)

Figure 4: Average AUC over 10 simulation repetitions for majority attribute proportion \(=0.9\) in training (and 0.1 in test). Solid lines: distribution-shift. Dashed lines: in-distribution. Our method improves robustness for shifts, without compromising training distribution results.

\(^{-}=0.1\), \(_{+}=2\), \(d_{0}=5\), \(d_{1}=d_{2}=10\). The results for additional representation sizes \(p\), noise ratios \(}{^{-}}\) and varying proportions of positive and negative examples are presented in Appendix D.1. To assess the importance assigned to each dimension, we examine weight values relative to other weights: \(_{i}=|^{p}w_{ij}}{_{i^{}=1}^{p} _{j^{}=1}^{p}w_{i^{}j^{}}}|.(11)\)

**Results** In Figure 5 we examine the learned representation. The analysis indicates that ERM prioritizes dimensions 5-15, providing good separation for \(a_{1}\), the dominant type in training, but leading to poor separation after the shift. ERM assigns low weights to dimensions beneficial for both types (0-5) and those suitable for \(a_{2}\) (15-25). In contrast, our algorithm, particularly with the two variance-based penalties, assigns the lowest weights to dimensions corresponding to \(a_{1}\) and higher weights to shared dimensions and those that effectively separate \(a_{2}\) classes.

In Figure 4, the learning progress is depicted for \(=0.9\) (a similar analysis for \(=0.95\) and \(=0.7\) can be found in Appendix D). Performance on the same distribution as the training data is similar for ERM and our algorithm, suggesting that applying our algorithm does not negatively impact performance when no distribution shift occurs. However, when there is a distribution shift our algorithm achieves much better results. The VarREx penalty achieves high AUC values more quickly than the VarAUC penalty, but the VarAUC penalty attains higher overall accuracy. IRM shows noisier convergence, since it is applied directly on the gradients, which have been shown to be noisy in contrastive learning due to high variance in data-pair samples . Means and standard deviations are reported in Appendix D.1, as well as the results for additional data dimensions, positive proportions, and variance ratios.

### Experiments on Real Data

**Experiment 1 - Species Recognition** We used the ETHEC dataset  which contains 47,978 butterfly images from six families and 561 species (example of the images are provided in Appendix D). We filtered out species with less than five images and focused on images of butterflies from the Lycaenidae and Nymphalidae families. In the training set, \(10\%\) of the species were from the Nymphalidae family, while at test time, \(90\%\) of the species were from the Nymphalidae family. For each class we sampled \(2r=20\) pairs.

**Experiment 2 - Face Recognition** We used the _CelebA_ dataset  which contains 202,599 images of 10,177 celebrities. We filtered out people for which the dataset contains less than three images. Following Vinyals et al. , we implemented \(g\) as a convolutional neural network which has four modules with \(3 3\) convolutions and 64 filters, followed by batch normalization, a ReLU activation, \(2 2\) max-pooling, and a fully connected layer of size 32. We used the attribute _blond hair_ for the class distribution shift: for training, we mainly sampled people without blond hair (\(95\%\)), while at test time, most people (\(95\%\))

Figure 5: Average feature importance for \(=0.9\), 10 repetitions. Our VarAUC penalty favors shared features (blocks 1 and 3), while depriitizing majority features (block 2). All methods assign low weight to noise features (block 4).

Figure 6: Average percentage changes of our method compared to ERM across 10 repetitions are shown for the ETHEC (top) and CelebA (bottom) datasets. Error bars represent \(\) one std-dev.

had blond hair. Each training iteration had 150 synthetic environments of two classes and \(2r=20\) data points per class.

We trained the models on 200 synthetic environments at a time, each of two classes. We implemented \(g\) as a fully connected neural network with layers of sizes 128, 64, 32 and 16, and ReLU activations between them.

**Experimental Results** As can be seen in Figure 6, while all versions of our algorithm show some improvement over ERM, the best results are achieved with the VarAUC penalty (exact means and standard deviations are reported in Table 3 in Appendix D). One-sided paired t-tests show that the improvement over ERM achieved by our algorithm with the VarAUC penalty is statistically significant, with p-values of \(<0.04\) on both datasets; p-values for other penalties are reported in Table 4. All p-values were adjusted with FDR  correction.

In Appendix D we also provide additional analysis confirming that the main improvement of our algorithm over the ERM baseline stems from improved performance on negative minority pairs.

## 6 Discussion

In this study, we examined class distribution shifts in zero-shot learning, with a focus on shifts induced by unknown attributes. Such shifts pose significant challenges in zero-shot learning where new classes emerge in testing, causing standard techniques trained via ERM to fail on shifted class distributions, even when the conditional distribution of the data given class remains the same.

Previous research (see Appendix A) assumes closed-world classification or a known cause, making these methods unsuitable for zero-shot learning or shifts caused by unknown attributes. In response, we introduced a framework and the first algorithm to address class distribution shifts in zero-shot learning using OOD environment balancing methods.

In the causal terminology of closed-world OOD generalization, our framework employs synthetic environments to intervene on attribute mixtures by sampling small class subsets, thereby manipulating the class distribution. This facilitates the creation of diverse environments with varied attribute mixtures, enhancing the distinction between negative examples. A further comparison of our framework with OOD environment balancing methods is provided in Appendix C. Additionally, our proposed VarAUC penalty, designed for metric losses, enhances the separation of negative examples.

Our results demonstrate improvements compared to the ERM baseline on shifted distributions, without compromising performance on unshifted distributions, enabling the learning of more robust representations for zero-shot tasks and ensuring reliable performance.

While the proposed framework is general, our current experiments address shifts in a binary attribute. We defer exploration of additional scenarios, such as those involving shifts in multiple correlated attributes, to future work. An additional promising direction for future work is the consideration of shifts where the responsible attribute is strongly correlated with additional attributes or covariates. This opens up possibilities to explore structured constructions of synthetic environments that leverage such correlations.