# Energy-Based Learning Algorithms

for Analog Computing:

A Comparative Study

 Benjamin Scellier

Rain AI

benjamin@rain.ai

&Maxence Ernoult

Rain AI

maxence@rain.ai

&Jack Kendall

Rain AI

jack@rain.ai

&Suhas Kumar

Rain AI

suhas@rain.ai

###### Abstract

Energy-based learning algorithms have recently gained a surge of interest due to their compatibility with analog (post-digital) hardware. Existing algorithms include contrastive learning (CL), equilibrium propagation (EP) and coupled learning (CpL), all consisting in contrasting two states, and differing in the type of perturbation used to obtain the second state from the first one. However, these algorithms have never been explicitly compared on equal footing with same models and datasets, making it difficult to assess their scalability and decide which one to select in practice. In this work, we carry out a comparison of seven learning algorithms, namely CL and different variants of EP and CpL depending on the signs of the perturbations. Specifically, using these learning algorithms, we train deep convolutional Hopfield networks (DCHNs) on five vision tasks (MNIST, F-MNIST, SVHN, CIFAR-10 and CIFAR-100). We find that, while all algorithms yield comparable performance on MNIST, important differences in performance arise as the difficulty of the task increases. Our key findings reveal that negative perturbations are better than positive ones, and highlight the centered variant of EP (which uses two perturbations of opposite sign) as the best-performing algorithm. We also endorse these findings with theoretical arguments. Additionally, we establish new SOTA results with DCHNs on all five datasets, both in performance and speed. In particular, our DCHN simulations are 13.5 times faster with respect to Laborieux et al. (2021), which we achieve thanks to the use of a novel energy minimisation algorithm based on asynchronous updates, combined with reduced precision (16 bits).

## 1 Introduction

Prior to the dominance of backpropagation-based machine learning, Hopfield, Hinton and others proposed an alternative 'energy-based' learning (EBL) approach (Hopfield, 1984; Hinton et al., 1984). In this approach, the learning model is described by a state variable whose dynamics is governed by an energy function. An EBL model can be used to compute as follows: 1) clamp the model input, 2) let the model settle to equilibrium (that is, the configuration of lowest energy), and 3) read the model output. The objective is to modify the weights of the model so that it computes a desired input-to-output function. This is achieved thanks to an EBL algorithm. One of the earliest EBL algorithms was constrastive learning (CL)(Movellan, 1991; Baldi and Pineda, 1991), which adjusts the model weights by contrasting two states: a 'free' state where the model outputs are free, and a perturbed (or 'clamped') state where the model outputs are clamped to desired values. In the machine learning literature, interest in EBL algorithms has remained limited due to the widespread success of backpropagation running on graphics processing units (GPUs). However, EBL algorithms have more recently revived interest as a promising learning framework for _analog_ learning machines (Kendallet al., 2020; Stern et al., 2021). The benefit of these algorithms is that they use a single computational circuit or network for both inference and training, and they rely on local learning rules (i.e. the weight updates are local). The locality of computation and learning makes these algorithms attractive for training adaptive physical systems in general (Stern and Murugan, 2023), and for building energy-efficient analog AI in particular (Kendall et al., 2020). Small-scale EBL-trained variable resistor networks have already been built (Dillavou et al., 2022, 2023; Yi et al., 2023), projecting a possible 10,000\(\) improvement in energy efficiency compared to GPU-based training of deep neural networks (Yi et al., 2023).

In recent years, various EBL algorithms have been proposed, such as equilibrium propagation (EP) (Scellier and Bengio, 2017), the centered variant of EP (Laborieux et al., 2021) and coupled learning (CpL) (Stern et al., 2021). These algorithms, which are variants of CL with modified perturbation methods, are often evaluated on different models and different datasets without being compared to CL and to one another.1 Due to the lack of explicit comparison between these algorithms, and since the algorithmic differences between them are small, they are often gathered under the 'contrastive learning' (or 'contrastive Hebbian learning') umbrella name (Dillavou et al., 2022; Stern and Murugan, 2023; Peterson and Lavin, 2022; Lillicrap et al., 2020; Luczak et al., 2022; Hoier et al., 2023). Consequently, many recent follow-up works in energy-based learning indifferently pick one of these algorithms without considering alternatives (Dillavou et al., 2022; Wycoff et al., 2022; Stern et al., 2022; Kiraz et al., 2022; Watfa et al., 2023; Yi et al., 2023; Dillavou et al., 2023; Altman et al., 2023). The main contribution of the present work is to provide an explicit comparison of the above-mentioned EBL algorithms and highlight the important differences arising when the difficulty of the task increases. Nonetheless, comparing these algorithms comes with another challenge: simulations are typically very slow. Due to this slowness, EBL algorithms have often been used to train small networks on small datasets (by deep learning standards). 2\({}^{,}\)3 Likewise, experimental realizations of EBL algorithms on analog hardware ("physical learning machines") have thus far been performed on small systems only. 4

In this work, we conduct a study to compare seven EBL algorithms, including the four above-mentioned and three new ones. Depending on the sign of the perturbation, we distinguish between 'positively-perturbed' (P-), 'negatively-perturbed' (N-) and 'centered' (C-) algorithms. To avoid the problem of slow simulations of analog circuits, we conduct our comparative study on deep convolutional Hopfield networks (DCHNs)(Ernoult et al., 2019), an energy-based network that has been emulated on Ising machines (Laydevant et al., 2023), and for which simulations are faster and previously demonstrated to scale to tasks such as CIFAR-10 (Laborieux et al., 2021) and Imagenet 32x32 (Laborieux and Zenke, 2022). Our contributions include the following:

* We train DCHNs with each of the seven EBL algorithms on five vision tasks: MNIST, Fashion-MNIST, SVHN, CIFAR-10 and CIFAR-100. We find that all these algorithms perform well on MNIST, but as the difficulty of the task increases, important behavioural differences start to emerge. Perhaps counter-intuitively, we find that N-type algorithms outperform P-type algorithms by a large margin on most tasks. The C-EP algorithm emerges as the best performing one, outperforming the other six algorithms on the three hardest tasks (SVHN, CIFAR-10 and CIFAR-100).
* We state novel theoretical results for EP, adapted from those of 'agnostic EP' (Scellier et al., 2022), that support our empirical findings. While EP is often presented as an algorithm that approximates gradient descent on the cost function (Scellier and Bengio, 2017; Laborieux et al., 2021), we provide a more precise and stronger statement: EP performs (exact) gradient descent on a surrogate function that approximates the (true) cost function. The surrogate function of N-EP is an upper bound of the cost function, whereas the one of P-EP is a lower bound (Theorem 2). Moreover, the surrogate function of C-EP approximates the true cost function at the second order in the perturbation strength (Theorem 3), whereas the ones of P-EP and N-EP are first-order approximations.
* to be compared with 11.4% reported in Laborieux and Zenke (2022). Our simulation speedup is enabled thanks to the use of a novel energy minimization procedure for DCHNs based on asynchronous updates and the use of 16 bit precision.

We note that our work also bears similarities with the line of works on energy-based models (Grathowhl et al., 2019; Nijkamp et al., 2019; Du and Mordatch, 2019; Geng et al., 2021). Like our approach, these studies involve the minimization of an energy function within the activation space (or input space) of a network. However, unlike our approach, they do not typically exclude the use of the backpropagation algorithm, employing it not only to compute the parameter gradients, but also to execute gradient descent within the network's activation space (or input space). In contrast, the primary motivation of our work is to eliminate the need for backpropagation, and to perform inference and learning by leveraging locally computed quantities, with the long term goal of building energy-efficient processors dedicated to model optimization. Hence, our motivation diverges significantly from traditional 'energy-based models'.

Figure 1: Cartoon illustrating the seven energy-based learning (EBL) algorithms: contrastive learning (CL), positively-perturbed algorithms (P-), negatively-perturbed algorithms (N-) and centered algorithms (C-). EP and Cpl stand for equilibrium propagation and coupled learning, respectively. The desired output is \(y\). The model prediction is \(o_{*}\) i.e. the output configuration minimizing the energy function. The strength of the perturbation is \(\). A positive perturbation pulls the model output (\(o_{}\)) towards \(y\). A negative perturbation pushes the model output (\(o_{-}\)) away from \(y\). Arrows indicate the weight update: green (resp. red) arrows decrease (resp. increase) the energy value of the corresponding configuration.

Energy-based learning algorithms

This section introduces the concepts and notations, presents the seven energy-based learning algorithms and states the theoretical results.

We consider the setting of image classification. In this setting, an energy-based learning (EBL) model is composed of an input variable (\(x\)), a parameter variable (\(\)) a hidden variable (\(h\)) and an output variable (\(o\)). A scalar function \(E\) called _energy function_ assigns to each tuple \((,x,h,o)\) a real number \(E(,x,h,o)\). Given \(\) and \(x\), among all possible configurations \((h,o)\), the effective configuration of the model is the equilibrium state (or steady state), denoted \((h_{},o_{})\) which is _implicitly_ defined as a minimum of the energy function,

\[(h_{},o_{}):=*{arg\,min}_{(h,o)}\,E(,x,h,o). \]

The equilibrium value (or steady state value) of output variables, \(o_{}\), represents a prediction of the model, which we also denote \(o(,x)=o_{}\) to emphasize that it depends on the input \(x\) and the model parameter \(\).

The goal of training an EBL model is to adjust \(\) so that, for any input \(x\), the output \(o(,x)\) coincides with a desired output \(y\) (the label associated to \(x\)). We refer to an algorithm for training an EBL model as an _EBL algorithm_.

**Remark.** In the terminology of Stern and Murugan (2023), \(h\) and \(o\) are the 'physical degrees of freedom', \(\) is the set of 'learning degrees of freedom' and \(E\) is the 'physical cost function'.

### Contrastive learning

Contrastive learning (CL) is the earliest EBL algorithm (Movellan, 1991). The CL algorithm proceeds in two phases. In the first phase, input variables \(x\) are clamped, while the hidden and output variables are free to stabilize to the energy minimum \((h_{},o_{})\) as in (1). In the second phase, the output variables \(o\) are now also clamped to the desired output \(y\), and the hidden variables \(h\) are free to stabilize to a second energy minimum, denoted \(h_{}^{}\), characterized by

\[h_{}^{}:=*{arg\,min}_{h}\,E(,x,h,y). \]

The contrastive learning rule for the model parameters reads

\[^{}=(( ,x,h_{},o_{})-( ,x,h_{}^{},y)), \]

where \(\) is a learning rate. The CL rule is illustrated in Figure 1.

### Equilibrium propagation

Equilibrium propagation (EP) is another EBL algorithm (Scellier and Bengio, 2017). One notable difference in EP is that one explicitly introduces a cost function \(C(o,y)\) that represents the discrepancy between the output \(o\) and desired output \(y\). In its original formulation, EP is a variant of CL which also consists of contrasting two states. In the first phase, similar to CL, input variables are clamped while hidden and output variables are free to settle to the free state \((h_{},o_{})\) characterized by (1). In the second phase, in contrast with CL, EP proceeds by only perturbing (or _nudging_) the output variables rather than clamping them. This is achieved by augmenting the model's energy by a term \( C(o,y)\), where \(\) is a scalar - the _nudging parameter_. The model settles to another equilibrium state, the perturbed state, characterized by

\[(h_{}^{},o_{}^{})=*{arg\,min}_{(h,o)}\,[E(,x,h,o)+ C(o,y)]. \]

In its classic form, the learning rule of EP is similar to CL:

\[^{}=((,x,h_{},o_{})-(,x,h_{}^{},o_{}^{}) ). \]

The EP learning rule (5) comes in two variants depending on the sign of \(\). In Scellier and Bengio (2017), EP is introduced in the variant with \(>0\), which we call here _positively-perturbed_ EP (P-EP).

In this work, we introduce the variant with \(<0\), which we call _negatively-perturbed_ EP (N-EP). 5 We also consider the variant of EP introduced by Laborieux et al. (2021) whose learning rule reads

\[^{}=((,x,h^{}_{-},o^{}_{- })-(,x,h^{}_{ },o^{}_{})), \]

which we call _centered_ EP (C-EP). The EP learning rules are illustrated in Figure 1.

### Coupled learning

Coupled learning (CpL) is another variant of contrastive learning (Stern et al., 2021). In the first phase, similar to CL and EP, input variables are clamped, while hidden and output variables are free to settle to their free state value \((h_{},o_{})\) characterized by (1). In the second phase, output variables are clamped to a weighted mean of \(o_{}\) and \(y\), and the hidden variables are allowed to settle to their new equilibrium value. Mathematically, the new equilibrium state is characterized by the following formulas, where the weighted mean output (\(o^{}_{}\)) is parameterized by a factor \(\{0\}\):

\[h^{}_{}:=_{h}\,E(,x,h,o^{}_{}), o^{}_{}:=(1-)o_{}+ y. \]

Similarly to CL and EP, the learning rule of CpL reads

\[^{}=((,x,h_{},o_{})-(,x,h^{}_{},o^{}_{ })). \]

In particular, for \(=1\), one recovers the contrastive learning algorithm (CL). In their original formulation, Stern et al. (2021) use \(>0\) ; here we refer to this algorithm as _positively-perturbed_ CpL (P-CpL). Similarly to EP, we also introduce _negatively-perturbed_ CpL (N-CpL, with \(<0\)) as well as _centered_ CpL (C-CpL):

\[^{}=((,x,h^{}_{-},o^{}_{- })-(,x,h^{ }_{},o^{}_{})). \]

Figure 1 also depicts P-CpL, N-CpL and C-CpL.

### Theoretical results

The EBL algorithms presented above have different theoretical properties. We start with CL.

**Theorem 1** (Contrastive learning).: _The contrastive learning rule (3) performs one step of gradient descent on the so-called contrastive function \(_{}\),_

\[^{}=-_{}}{ }(,x,y),_{}(,x,y):=E (,x,h^{}_{},y)-E(,x,h_{},o_{ }). \]

Theorem 1 is proved in Movellan (1991). However, it is not clear that the contrastive function \(_{}\) has the desirable properties of an objective function from a machine learning perspective.

The equilibrium propagation (EP) learning rules have better theoretical properties. EP is often presented as an algorithm that approximates the gradient of the cost function, e.g. in Scellier and Bengio (2017), Laborieux et al. (2021). In this work, we provide a more precise and stronger statement: EP computes the exact gradient of a surrogate function that approximates the cost function. Moreover, in N-EP, the surrogate function is an upper bound of the cost function (Theorem 2), and in C-EP, the surrogate function approximates the cost function at the second order in \(\) (Theorem 3). Theorems 2 and 3 are adapted from Scellier et al. (2022)6 - see Appendix A for proofs.

**Theorem 2** (Equilibrium propagation).: _There exists some function \(_{}^{}\) such that the learning rule (5) performs one step of gradient descent on \(_{}^{}\):_

\[^{}=-_{}^{} }{}(,x,y). \]

_The function \(_{}^{}\) is a lower bound of the 'true' cost function if \(>0\) (P-EP), and an upper bound if \(<0\) (N-EP), i.e._

\[_{}^{}(,x,y) C(o(,x),y) _{-}^{}(,x,y),>0, \]

_where \(o(,x)=o_{}\) is the free equilibrium value of output variables given \(\) and \(x\), as in (1). Furthermore, \(_{}^{}\) approximates the 'true' cost function up to \(O()\) when \( 0\),_

\[_{}^{}(,x,y)=C(o(,x),y)+O( ). \]

**Theorem 3** (Centered EP).: _There exists some function \(_{-;+}^{}\) such that the learning rule (6) performs one step of gradient descent on \(_{-;+}^{}\):_

\[^{}=-_{-;+}^{ }}{}(,x,y). \]

_Moreover, the function \(_{-;+}^{}\) approximates the 'true' cost function up to \(O(^{2})\) when \( 0\),_

\[_{-;+}^{}(,x,y)=C(o(,x),y )+O(^{2}). \]

Finally, the analysis of the coupled learning rules (P-CpL, N-CpL and C-CpL) is more complicated. Stern et al. (2021) introduce the following function as a candidate loss function for coupled learning:

\[_{}^{(1)}(,x,y):=(y-o(,x))^{} G}{ o^{2}}(,x,o(,x))(y-o(,x) ), G(,x,o):=_{h}E(,x,h,o). \]

Stern et al. (2021) argue that the P-CpL rule (8) optimizes both \(_{}\) and the squared error \(_{}(,x,y):=(o(,x)-y)^{2}\). Besides, Stern et al. (2022, 2023) argue that the learning rule (8) performs gradient descent on

\[_{}^{(2)}:=(E(,x,h_{}^{ },o_{}^{})-E(,x,h_{},o_{})). \]

In Appendix B, we demonstrate that the coupled learning rules (8) and (9) do not optimize\(_{}^{(1)}\) or \(_{}\), and do not perform gradient descent on \(_{}^{(2)}\).

## 3 Deep convolutional Hopfield networks

To compare the EBL algorithms presented in section 2, we consider the EBL model of Ernoult et al. (2019), which we call _deep convolutional Hopfield network_ (DCHN). We consider specifically the network architecture of Laborieux et al. (2021).

Network architecture.The network has an input layer, four hidden layers, and an output layer. Since we consider classification tasks, the output layer has \(M\) units, where \(M\) is the number of categories for the task. Successive layers are interconnected by convolutional interactions with kernel size \(3 3\), padding \(1\), and max pooling. Except for the last hidden layer and the output layer, which are interconnected by a dense interaction.

Energy function.We denote the state of the network \(s=(s_{0},s_{1},s_{2},s_{3},s_{4},s_{5})\), where \(x=s_{0}\) is the input layer, \(h=(s_{1},s_{2},s_{3},s_{4})\) is the hidden variable and \(o=s_{5}\) is the output variable. The energy function of the network is

\[E(,s):=_{k=1}^{5}\|s_{k}\|^{2}+_{k=1}^{4}E_{k}^{ }(w_{k},s_{k-1},s_{k})+E_{5}^{}(w_{5},s_{4},s_{5})+ _{k=1}^{5}E_{k}^{}(b_{k},s_{k}), \]where \(=\{w_{k},b_{k} 1 k 5\}\) is the set of model parameters, and \(E_{k}^{ conv}\), \(E_{k}^{ dense}\) and \(E_{k}^{ bias}\) are energy terms defined as

\[E_{k}^{ conv}:=-s_{k}(w_{k} s_{k-1}), E _{k}^{ dense}:=-s_{k}^{}w_{k}s_{k-1}, E_{k}^{ bias}:=-b_{k}^{ }s_{k}. \]

Specifically, \(E_{k}^{ conv}\) is the energy function of a convolutional interaction between layers \(k-1\) and \(k\), parameterized by the kernel \(w_{k}\) (the weights), where \(\) is the convolution operation, \(\) is the max pooling operation, and \(\) is the scalar product for pairs of tensors. \(E_{k}^{ dense}\) is the energy of a dense interaction between layers \(k-1\) and \(k\), parameterized by the \((s_{k-1})(s_{k})\) matrix \(w_{k}\). Finally, \(E_{k}^{ bias}\) is the energy term of \(b_{k}\), the bias of layer \(k\).

Cost function.For the equilibrium propagation learning rules, we use the squared error cost function \(C(o,y)=\|o-y\|^{2}\), where \(o=s_{5}\) is the output layer and \(y\) is the one-hot code of the label (associated to image \(x\)).

State space.Given an input \(x\), the steady state of the model is \((h_{},o_{})=*{arg\,min}_{(h,o)}E(,x,h,o)\), where \(\) is the state space, i.e. the space of configurations for the hidden and output variables. We use \(=_{k=1}^{4}^{(s_{k})}^{(s_{5})}\), where \(\) is the closed interval of \(\) with bounds \(0\) and \(1\), and \((s_{k})\) is the number of units in layer \(k\).

Energy minimization via asynchronous updates.In order to compute the steady states (free state and perturbed states) of the learning algorithms presented in section 2, we use a novel procedure for DCHNs using 'asynchronous updates' for the state variables, as opposed to the'synchronous updates' used in other works (Ernould et al., 2019; Laborieux et al., 2021; Laydevant et al., 2021; Laborieux and Zenke, 2022). This asynchronous procedure proceeds as follows: at every iteration, we first update the layers of even indices in parallel (the first half of the layers) and then we update the layers of odd indices (the other half of the layers). Updating all the layers once (first the even layers, then the odd layers) constitutes one 'iteration'. We repeat as many iterations as necessary until convergence to the steady state. See Appendix C for a detailed explanation of the asynchronous procedure, and for a comparison with the synchronous procedure used in other works. Although we do not have a proof of convergence of this asynchronous procedure for DCHNs when \(\) is the max pooling operation, we find experimentally that it converges faster than the synchronous procedure (for which there is no proof of convergence anyway).

## 4 Simulations

### Comparative study of EBL algorithms

Setup.We compare with simulations the seven energy-based learning (EBL) algorithms of section 2 on the deep convolutional Hopfield network (DCHN) of section 3. To do this, we train a DCHN on MNIST, Fashion-MNIST, SVHN, CIFAR-10 and CIFAR-100 using each of the seven EBL algorithms. We also compare the performance of these algorithms to two baselines: recurrent backpropagation (RBP) and truncated backpropagation (TBP), detailed below. For each simulation, the DCHN is trained for 100 epochs. Each run is performed on a single A100 GPU. A run on MNIST and FashionMNIST takes 3 hours 30 minutes ; a run on SVHN takes 4 hours 45 minutes ; and a run on CIFAR-10 and CIFAR-100 takes 3 hours. All these simulations are performed with the same network using the same initialization scheme and the same hyperparameters. Details of the training experiments are provided in Appendix D and the results are reported in Table 1. 7

First baseline: recurrent backpropagation (RBP).As the equilibrium state \(o_{}\) of an EBL model is defined _implicitly_ as the minimum of an energy function, the gradient of the cost \(C(o_{},y)\) can be computed via implicit differentiation, or more specifically via an algorithm called _recurrent backpropagation_(Almeida, 1987; Pineda, 1987).

Second baseline: truncated backpropagation (TBP).Our second baseline uses automatic differentiation (i.e. backpropagation) as follows. First, we compute the free state \((h_{},o_{})\). Then, starting from the free state \((h_{0},o_{0})=(h_{*},o_{*})\) we perform \(K\) iterations of the fixed point dynamics ; we arrive at \((h_{K},o_{K})\), which is also equal to the free state. We then compute the gradient of \(C(o_{K},y)\) with respect to the parameters (\(\)), without backpropagating through \((h_{0},o_{0})\). This is a truncated version of backpropagation, where we only backpropagate through the last \(K\) iterations but not through the minimization of the energy function. This baseline is also used by Ernoult et al. (2019).

We draw several lessons from Table 1.

Algorithms perform alike on MNIST.Little difference is observed in the test performance of the algorithms on MNIST, ranging from 0.42% to 0.66% test error rate for six of the seven EBL algorithms. This result confirms the current trend in the literature, often skewed towards simple tasks, which sometimes goes to treat all EBL algorithms as one and the same.

Weak positive perturbations perform worse than strong ones.P-EP fails on most datasets (Fashion-MNIST, SVHN, CIFAR-10 and CIFAR-100). P-CpL (the other weakly positively-perturbed algorithm) performs better than P-EP on all tasks, but also fails on CIFAR-100 (91% training error). It is noteworthy that CL, which employs full clamping to the desired output (i.e. a strong positive perturbation), performs better than these two algorithms on _all tasks_, sometimes by a very large margin (Fashion-MNIST and SVHN).

Negative perturbations yield better results than positive ones.On Fashion-MNIST, CIFAR-10 and CIFAR-100, algorithms employing a negative perturbation (N-EP and N-CpL) perform significantly better than those employing a positive perturbation (CL, P-EP and P-CpL). Theorem 2 sheds light on why N-EP performs better than P-EP: N-EP optimizes an upper bound of the cost function, whereas P-EP optimizes a lower bound. We note however that on SVHN, the results obtained with N-EP and N-CpL are much worse than CL - in Appendix E we perform additional simulations where we change the weight initialization of the network ; we find that N-EP and N-CpL generally perform much better than CL, P-EP and P-CpL, supporting our conclusion.

Two-sided perturbations (i.e. centered algorithms) yield better results than one-sided perturbations.While little difference in performance between the centered (C-EP and C-CpL) and negatively-perturbed (N-EP and N-CpL) algorithms is observed on MNIST, FashionMNIST and CIFAR-10, the centered algorithms unlock training on SVHN and significantly improve the test error rate on CIFAR-100 (by \( 5.4\%\)). Theorem 3 sheds light on why C-EP performs better than N-EP: the loss function \(^{}_{-;+}\) of C-EP better approximates the cost function (up to \(O(^{2})\)) than the loss function \(^{}_{-}\) of N-EP (up to \(O()\)).

    &  &  &  &  &  \\   & Test & Train & Test & Train & Test & Train & Test & Train & Test & Train \\  TBP & 0.42 & 0.23 & 6.12 & 3.09 & 3.76 & 2.37 & 10.1 & 3.1 & 33.4 & 17.2 \\ RBP & 0.44 & 0.33 & 6.28 & 3.70 & 3.87 & 3.43 & 10.7 & 5.2 & 34.4 & 18.2 \\  CL & 0.61 & 2.39 & 10.10 & 15.49 & 6.1 & 15.8 & 31.4 & 45.2 & 71.4 & 88.6 \\ P-EP & 1.66 & 2.29 & 90.00 & 89.99 & 83.9 & 81.9 & 72.6 & 79.5 & 89.4 & 95.5 \\ N-EP & **0.42** & 0.19 & **6.22** & 3.87 & 80.4 & 81.1 & 11.9 & 6.2 & 44.7 & 40.1 \\ C-EP & 0.44 & 0.20 & 6.47 & 4.01 & **3.51** & 3.01 & **11.1** & 5.6 & **37.0** & 26.0 \\ P-CpL & 0.66 & 0.59 & 64.70 & 65.31 & 40.1 & 50.8 & 46.9 & 57.7 & 77.9 & 91.0 \\ N-CpL & 0.50 & 0.88 & 6.86 & 6.27 & 80.4 & 81.1 & 13.5 & 10.2 & 51.9 & 50.6 \\ C-CpL & 0.44 & 0.38 & 6.91 & 5.29 & 4.23 & 5.05 & 14.9 & 14.6 & 46.5 & 37.9 \\   

Table 1: Results obtained by training the deep convolutional Hopfield network (DCHN) of Section 3 with the EBL algorithms of Section 2: contrastive learning (CL), equilibrium propagation (EP) and coupled learning (CpL). EP and CpL are tested in their positively-perturbed (P-), negatively-perturbed (N-) and centered (C-) variants. We also report two baselines: truncated backpropagation (TBP) and recurrent backpropagation (RBP). Train and Test refer to the training and test error rates, in %. For each of these 45 experiments, we perform three runs and report the mean values. See Appendix D for the complete results with std values. The hyperparameters used for this study are reported in Table 5.

The EP perturbation method yields better results than the one of CpL.While little difference in performance is observed between C-EP and C-CpL on MNIST and FashionMNIST, C-EP significantly outperforms C-CpL on CIFAR-10 and CIFAR-100, both in terms of training error rate (\( 11.9\%\) difference) and test error rate (\( 3.8\%\) difference). The same observations hold between N-EP and N-CpL. We also note that the CpL learning rules seem to be less theoretically grounded than the EP learning rules, as detailed in Appendix B.

### State-of-the-art DCHN simulations (performance and speed)

The comparative study conducted in the previous subsection highlights C-EP as the best EBL algorithm among the seven algorithms considered in this work. Using C-EP, we then perform additional simulations on MNIST, CIFAR-10 and CIFAR-100, where we optimize the hyperparameters of training (weight initialization, initial learning rates, number of iterations, value of the nudging parameter and weight decay) to yield the best performance. We use the hyperparameters reported in Table 5 (Appendix D). We report in Table 2 our fastest simulations (100 epochs) as well as the ones that yield the lowest test error rate (300 epochs), averaged over three runs each. We also compare our results with existing works on deep convolutional Hopfield networks (DCHNs).

Table 2 shows that we achieve better simulation results than existing works on DCHNs on all three datasets, both in terms of performance and speed. For instance, our 100-epoch simulations on CIFAR-10 take 3 hours 18 minutes, which is 7 times faster than those reported in (Laborieux and Zenke, 2022) (1 day), and 36 times faster than those reported in Laydevant et al. (2021) (5 days), and our 300-epoch simulations on CIFAR-10 yield 9.70% test error rate, which is significantly lower than (Laborieux and Zenke, 2022) (11.4%). On CIFAR-100, it is interesting to note that our 300-epoch simulations yield a significantly better Top-1 error rate than Laborieux and Zenke (2022) (31.6% vs 38.4%), but a slightly worse Top-5 error rate (14.1% vs 14.0%) ; we speculate that this might be due to our choice of using the mean-squared error instead of the cross-entropy loss used in Laborieux and Zenke (2022). Since no earlier work on DCHNs has performed simulations on Fashion-MNIST and SVHN, our results reported in Table 1 are state-of-the-art on these datasets as well.

Our important speedup comes from our novel energy-minimization procedure based on "asynchronous updates", combined with 60 iterations at inference (free phase) and the use of 16-bit precision. In comparison, Laborieux et al. (2021) used "synchronous updates" with 250 iterations and 32-bit precision. We show in Appendix C that these changes result in a 13.5x speedup on the same device (a A100 GPU) without degrading the performance (test error rate).

We also stress that there still exists an important gap between our SOTA DCHN results and SOTA computer vision models. For example, Dosovitskiy et al. (2020) report 0.5% test error rate on CIFAR-10, and 5.45% top-1 test error rate on CIFAR-100.

    &  &  &  \\   & Top1 & WCT & Top1 & WCT & Top1 & Top5 & WCT \\  Ernoult et al. (2019) & 1.02 & 8:58 & & & & & \\ Laborieux et al. (2021) & 11.68 & N.A. & 13.78 & \(\) 120:00 & & & \\ Laydevant et al. (2021) & 0.85 & N.A. & 20.03 & N.A. & & & \\ Laborieux and Zenke (2022) & & & 11.4 & \(\) 24:00 & 38.4 & **14.0** & \(\) 24:00 \\  This work (100 epochs) & **0.44** & **3:30** & 10.40 & **3:18** & 34.2 & 14.2 & **3:02** \\ This work (300 epochs) & & & **9.70** & 9:54 & **31.6** & 14.1 & 9:04 \\   

Table 2: We achieve state-of-the-art results (both in terms of speed and accuracy) with C-EP-trained DCHNs on MNIST, CIFAR-10 and CIFAR-100. The results are averaged over 3 runs, and compared with the existing literature on DCHNs. Top1 (resp. Top5) refers to the test error rate for the Top1 (resp. Top5) classification task. Wall-clock time (WCT) is reported as HH:MM. The hyperparameters used for these simulations are reported in Table 5 (Appendix D).

Conclusion

Our comparative study of energy-based learning (EBL) algorithms delivers a few key take-aways: 1) simple tasks such as MNIST may suggest that all EBL algorithms work equally well, but more difficult tasks such as CIFAR-100 magnify small algorithmic differences, 2) negative perturbations yield better results than positive ones, 3) two-sided (centered) perturbations perform better than one-sided perturbations, and 4) the perturbation technique of equilibrium propagation yields better results than the one of coupled learning. These findings highlight the centered variant of equilibrium propagation (C-EP) as the best EBL algorithm among those considered in the present work, outperforming the second-best algorithm on CIFAR-100 (N-EP) by a significant margin. Our results also challenge some common views. First, it is noteworthy that CL (which uses clamping to the desired output, i.e. a strong positive perturbation) yields better results than P-Cpl and P-EP (which use weak positive perturbations): this is contrary to the prescription of Stern et al. (2021) to use small positive perturbations, and opens up questions in the strong perturbation regime (Meulemans et al., 2022). Second, it is interesting to note that CL (Movellan, 1991; Baldi and Pineda, 1991), EP (Scellier and Bengio, 2017) and Cpl (Stern et al., 2021) were originally introduced around the same intuition of bringing the model output values closer to the desired outputs ; the fact that the four best-performing algorithms of our study (C-EP, N-EP, C-Cpl and N-Cpl) all require a negative perturbation suggests on the contrary that the crucial part in EBL algorithms is not to _pull_ the model outputs towards the desired outputs, but to _push away_ from the desired outputs. Third, Laborieux et al. (2021) explained the very poor performance of P-EP by it approximating the gradient of the cost function up to \(O()\), but our observation that N-EP considerably outperforms P-EP while also possessing a bias of order \(O()\) shows that this explanation is incomplete ; Theorem 2 provides a complementary explanation: P-EP optimizes a lower bound of the cost function, whereas N-EP optimizes an upper bound.

Our work also establishes new state-of-the-art results for deep convolutional Hopfield networks (DCHNs) on all five datasets, both in terms of performance (accuracy) and speed. In particular, thanks to the use of a novel "asynchronous" energy-minimization procedure for DCHNs, we manage to reduce the number of iterations required to converge to equilibrium to 60 - compared to 250 iterations used in Laborieux et al. (2021). Combined with the use of 16-bit precision (instead of 32-bit), this leads our simulations to be 13.5 times faster than those of Laborieux et al. (2021) when run on the same hardware (a A100 GPU).

While our theoretical and simulation results seem conclusive, we also stress some of the limiting aspects of our study. First, our comparative study of EBL algorithms was conducted only on the DCHN model ; further studies will be required to confirm whether or not the conclusions that we have drawn extend to a broader class of models such as resistive networks (Kendall et al., 2020) and elastic networks (Stern et al., 2021). Second, in our simulations of DCHNs, the performance of the different EBL methods depends on various hyperparameters such as the perturbation strength (\(\)), the number of iterations performed to converge to steady state, the weight initialization scheme, the learning rates, the weight decay, etc. Since a different choice of these hyperparameters can lead to different performance, it is not excluded that a deeper empirical study could lead to slightly different conclusions.

Ultimately, while our work is concerned with _simulations_, EBL algorithms are most promising for training actual analog machines ; we believe that our findings can inform the design of such hardware as well (Dillavou et al., 2022, 2023; Yi et al., 2023; Altman et al., 2023). Finally, we contend that our theoretical insights and experimental findings may also help better understand or improve novel EBL algorithms (Anisetti et al., 2022; Laborieux and Zenke, 2022; Hexner, 2023; Anisetti et al., 2023), and more generally guide the design of better learning algorithms for bi-level optimization problems (Zucchet and Sacramento, 2022), including the training of Lagrangian systems (Kendall, 2021; Scellier, 2021), meta-learning (Zucchet et al., 2022; Park and Simeone, 2022), direct loss minimization (Song et al., 2016) and predictive coding (Whittington and Bogacz, 2019; Millidge et al., 2022).