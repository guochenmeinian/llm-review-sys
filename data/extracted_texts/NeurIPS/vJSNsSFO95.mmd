# Flaws can be Applause: Unleashing Potential of Segmenting Ambiguous Objects in SAM

Chenxin Li1, Yuzhi Huang2, Wuyang Li1, Hengyu Liu1,

**Xinyu Liu1, Qing Xu3, Zhen Chen4, Yue Huang2, Yixuan Yuan1**

1The Chinese University of Hong Kong 2Xiamen University

3University of Nottingham Ningbo China 4Yale University

Equal Contribution: {chenxinli@link.cuhk.edu.hk, yzhuang13@stu.xmu.edu.cn}.Corresponding Author: {yxyuan@ee.cuhk.edu.hk}.

###### Abstract

As the vision foundation models like the Segment Anything Model (SAM) demonstrate potent universality, they also present challenges in giving ambiguous and uncertain predictions. Significant variations in the model output and granularity can occur with simply subtle changes in the prompt, contradicting the consensus requirement for the robustness of a model. While some established works have been dedicated to stabilizing and fortifying the prediction of SAM, this paper takes a unique path to explore how this flaw can be inverted into an advantage when modeling inherently ambiguous data distributions. We introduce an optimization framework based on a conditional variational autoencoder, which jointly models the prompt and the granularity of the object with a latent probability distribution. This approach enables the model to adaptively perceive and represent the real ambiguous label distribution, taming SAM to produce a series of diverse, convincing, and reasonable segmentation outputs controllably. Extensive experiments on several practical deployment scenarios involving ambiguity demonstrates the exceptional performance of our framework. Project page: https://a-sa-m.github.io/.

## 1 Introduction

The advent of Visual Foundational Models (VFMs) such as the Segment Anything Model (SAM)  has been unprecedented, largely due to the availability of vast datasets and computational resources. These models have exhibited remarkable generalization capabilities in zero-shot scenarios and the capacity to interact with human feedback. SAM, in particular, employs a specialized data engine to manage 11 million image masks, using a unique prompt-based segmentation framework to generate accurate masks for any object within a visual context, largely extending the capacity and generality of segmentation . Such successes have been widely extended to various domains, such as medical imaging analysis , remote sensing , etc.

However, it has been observed that SAM suffers from severe _predictive ambiguity_ to segment desired concepts  in practical scenarios. To uncover the factual basis, we delve into this ambiguity and detail it into two flaws according to experimental insight. Specifically, the first flaw lies in that SAM prediction is sensitive to slightly different prompt variants. As shown in Fig. 1 (a), we ground SAM into a realistic clinical scenario to segment the lesion in CT images. Even though three medical experts uniformly give rational box prompts covering the lesion, SAM, unfortunately, makes enormous differences among them, even including one wrong case. Further, we provide detailed statistics regarding IoU over the small perturbation (only 5 pixels) of box prompts, as shown in Fig. 1(b). We can observe a significant IoU fluctuation over small prompt differences, which reveals that SAM prediction is highly sensitive to the prompt variants caused by such small perturbation3.

The second flaw lies in the susceptibility of SAM output to the inherent structural granularity of the object. Despite the fact that VFMs like SAM gain generalizable knowledge and abilities from extensive datasets, they frequently forfeit the capacity to segment specific visual concepts as they are class agnostic, making the model unable to discern the difference between objects with different levels of semantic granularity. Consequently, for targets that are challenging to define, particularly those with rich internal hierarchical granularity, they are inclined to produce multiple candidate results at different granularities and amalgamate them, rather than directly outputting a definitive result. As depicted in Fig. 1 (c), we also discern that the multiple candidate results captured by SAM often exhibit significant differences, and the segmentation precision of outputs at different granularities diverges greatly when compared to the final integrated SAM output that incorporates multiple candidates.

Nonetheless, every coin has two sides. While these observations reveal SAM's faults regarding the output sensitivity, we explore a different perspective: _could this sensitivity flaw become an advantage in other cases, such as ambiguous segmentation, which requires the model to learn from a set of ambiguous object annotations caused by imaging noise, ambiguous tissue boundaries, and different annotator preferences?_ Specifically, we are particularly interested in the following:

Given the segmentation result's sensitivity to prompts, can we probabilistically model this prompt variation to tame prompt-sensitive SAM to controllably yield multiple likely results close to the actual fuzzy distribution?

Considering the segmentation result's sensitivity to object structure, can we probabilistically model this granularity variation to tame granularity-sensitive SAM to controllably yield multiple likely results close to the actual ambiguous distribution?

Driven by these questions, we propose an innovative strategy, which flips the inherent category-agnostic ambiguity induced in SAM into a controlled ability to generate a range of feasible results for ambiguous segmentation tasks. Specifically, to simulate segmentation ambiguity under different prompts, we introduce context-aware prompt ambiguity modelling. This method probabilistically models the uncertainty of the prompts inputted into SAM using a latent learnable distribution, which adaptively perceives the specific ambiguity of different contexts. Furthermore, to simulate ambiguity caused by complex object structures at different granularities, we introduce granularity-aware object ambiguity modelling. This method introduces and enhances the visual ambiguity of objects at different granularities into SAM's original image embedding via a learnable embedding distribution. While establishing these two levels of ambiguity modelling, we introduce an efficient optimization strategy based on posterior constraints, allowing the model to mimic models that can perceive the actual ambiguous distribution. Our contributions can be summarized as follows:

Figure 1: **Analysis of Inherent Ambiguity in SAM. (a)**: Feeding SAM with slightly different prompts from multiple experts for a single image can significantly alter the segmentation output. **(b)(c)**: We evaluate SAM using canonical box prompts and various perturbed versions, measuring the mean and variance of segmentation IoU on LIDC. Perturbations involve shifting the box five pixels in different directions and employing various granular outputs within SAM. Results highlight SAMâ€™s sensitivity to prompt variations and granularity.

* We explore the inherent ambiguity in SAM to flip this commonly seen disadvantage in deterministic segmentation tasks into an advantage for more practical ambiguous segmentation tasks that allow multiple possible outputs.
* We propose a \(\)-_SAM_ framework that employs a learnable latent distribution to encapsulate the ambiguity at two strata, from prompts and object granularity.
* We introduce an optimization architecture based on variational autoencoders, which effectively represents ambiguity by constraining the sample embedding to align with those from a series of practically feasible annotations.
* Rigurous benchmarked experiments across a wide range of potential scenarios demonstrate that our method produces more accurate, diverse, and reasonable segmentation outputs.

## 2 Related Work

**Prompting Foundational Models for Segmentation.** There has been a surge in the advancement of large-scale vision models for image segmentation, drawing inspiration from language foundation models [62; 4; 25; 39]. Segmentation Foundation Models (SFMs), such as the Segment Anything Model (SAM)  and SEEM , have delivered significant segmentation results across various downstream datasets [17; 38; 54]. SAM, leveraging a data engine incorporating model-in-the-loop annotation, learns a promotable segmentation framework that generalizes to downstream scenarios in a zero-shot manner. Meanwhile, other models like Painter  and SegGPT  introduce a robust in-context learning paradigm that enables segmenting any images given an image-mask prompt. On the contrary, SEEM  presents a general segmentation model prompted by multi-modal references, such as language and audio, thereby incorporating a wide range of semantic knowledge. These advancements in SFMs, driven by _promptable segmentation_ design, involve two types of prompts: semantic prompts (e.g., free-form texts) and spatial prompts (e.g., points or bounding boxes) [22; 57; 40; 41; 43; 26].

Recently, the practice of adapting vision foundation models such as SAM  for application in medical image segmentation is garnering increasing interest [46; 10; 12; 44; 33]. A prevalent and cost-effective approach involves adapter techniques, which necessitate the inclusion of bottleneck modules with a finite number of parameters within the model. By fine-tuning these diminutive adapters, SAM can bridge the domain discrepancy between medical and natural images while preserving stellar performance. For instance, models like MSA , SAM-Med2D , and SAM-adapter  utilize adapter strategies to transfigure SAM for medical imaging, thereby achieving significant segmentation outcomes. Despite these advancements, acquiring suitable prompts for SFMs remains largely under-explored. This work aims to explore generating effective prompts for SAM, focusing on harnessing pre-training knowledge to complete ambiguous image segmentation.

**Ambiguous Image Segmentation.** Ambiguous image segmentation aims to model a range of, rather than single, segmentation labels [31; 65; 45]. A wealth of existing research has proposed various techniques to quantify uncertainty. Initial research focused on enhancing a traditional U-Net[51; 16; 29; 5] with a probabilistic component to generate multiple predictions for the same image. This was typically achieved by incorporating a conditional variational autoencoder (cVAE) , with the low-dimensional latent space encoding potential segmentation variations. Subsequent work further extended this setup to a hierarchical variant [3; 24; 64; 15]. Other research has utilized normalizing flows to allow for distribution in the cVAE [52; 55] to represent a discrete latent space  or incorporated variational dropout and directly used inter-grader variability as a training target. Several other methods  do not rely on the Probabilistic U-Net [47; 21; 32; 11; 60]. Monteiro _et al._ proposed a network utilizing a low-rank multivariate normal distribution to model the logit distribution. Kassapis _et al._ leveraged adversarial training to learn potential label maps based on the logits of a trained segmentation network. Zhang _et al._ employed an autoregressive PixelCNN to model the conditional distribution between pixels [37; 36]. Finally, Gao _et al._ used a mixture of stochastic experts, where each expert network estimates a mode of uncertainty, and a gating network predicts the probabilities of an input image being segmented by one of the experts. Unlike previous efforts [1; 42; 59; 7], our approach signifies the first exploration of leveraging the inherent properties in vision foundation models for ambiguous image segmentation.

## 3 Method

### Revisiting SAM by Probabilistic Perspective

**SAM: Segment Anything Model.** Given an image \(I\) and a set of user-given prompts \(P\), which could be a point, a box, or a rough mask, the Segment Anything Model (SAM)  employs a vision transformer-based image encoder \(_{I}\) to extract salient image feature \(F_{I}\) and deploy a prompt encoder \(_{P}\) with length \(k\) to encode prompt embeddings \(T_{P}\), which are denoted as follows,

\[F_{I}=_{I}(I),\ \ T_{P}=_{P}(P),\] (1)

where \(F_{I}^{h w c}\) and \(T_{P}^{k c}\), where the resolution of the image feature map is represented by \(h\), \(w\), and the feature dimension is denoted by \(c\). Subsequently, the encoded image and prompts are introduced into the decoder \(_{M}\) for interaction based on attention mechanisms. SAM constructs the decoder's input tokens by concatenating several learnable mask tokens \(T_{M}\) as prefixes to the prompt tokens \(T_{P}\). These mask tokens are accountable for generating the mask output \(M\)

\[M=_{M}(F_{I},\ T_{P},\ T_{M}),\] (2)

\(\)**-SAM: Lifting SAM to Distributional Space.** Unlike the one-to-one deterministic mapping in SAM, we formulate a probabilistic latent distribution to enable the one-to-many ambiguous mapping named \(\)-_SAM_, with each observation being a sample from this hidden distribution. To this end, the prompt and image embedding can be probabilistically formulated as a distribution:

\[}_{P}(),\ \ \ }_{I}(),\] (3)

where \(_{P}\) and \(_{I}\) denote a latent distribution for prompt embedding and image embedding, respectively. \(}\) and \(}\) denotes a prompt at one sampling from the defined latent distribution at one time. Formally, by implementing multiple rounds of sampling, we can construct a distributional mapping of segmentation outputs with respect to their prompts, formulated as the format of expectation,

\[=_{M}(},\ },\ T_{M}),\ \ \ \ st.\ }_{P}(),}_{I}( ),\] (4)

where \(\) denotes the SAM output corresponding to one prompt sampling, which can also be interpreted as the sampling from a virtual distribution \(_{M}()\) for the segmentation results obeying parameters \(\). As a result, we can construct an optimized probability distribution \(}_{P}()\) and \(}_{I}()\) by narrowing the gap between \(_{M}()\) and ground-truth distribution.

\(\)**-SAM: Inference Stage.** After the training, \(\)_-SAM_ can model two types of latent distribution, representing the ambiguity of the prompt variation and the varied object granularity, respectively.

Figure 2: \(\)**-_SAM_ Training Pipeline.** We probabilistically model the prompt and object-level ambiguity by jointly probabilities the SAM embeddings with PGN and IGN, respectively.

Based on this formulation, each latent sample drawn from the distribution represents a segmentation candidate. Concretely, to predict a set of \(m\) segmentations, we apply the network \(m\) times to the same input image. In each iteration \(i\{1,,m\}\), we draw a random sample regarding prompt embedding \(}^{N_{P}}\) and image embedding \(}^{N_{I}}\) respectively from \(_{P}()\) and \(_{I}()\). The final prediction maps \(_{M}()\) can be obtained by Eq. 4. In what follows, we will primarily focus on the methodology of training our overall ambiguous segmentation framework.

### Context-aware Prompt Ambiguity Modeling

Distributional Prompt Representation.To model the distribution of prompt embedding, it is imperative to estimate the parameters \(\) of this distribution. We adopt an axisymmetric Gaussian distribution to characterize the prompt embedding, which is dictated by two crucial parameters, including mean \(\) and standard deviation \(\). Then, we can sample a prompt embedding from the given Gaussian distribution, which is shown as

\[}_{P}()=(_{P},( _{P})),\] (5)

where \(_{P}\) and \(_{P}\) denotes the parameters characterized for prompt \(P\). \(_{P}\) and \(_{P}\) respectively denote the mean and standard deviation of the Axis Gaussian Distribution generated by the network, where \(_{P},_{P}^{N_{P}}\). This simple yet effective formulation enables the discrete prompt to be continuously represented in the probabilistic latent space, making the uncertainty estimation available.

Context-aware Prompt Embedding Generation.To parameterize the latent prompt distribution, we propose a prompt generation network (PGN) to effectively model the aforementioned Gaussian related to prompt embedding. This network is simply designed to include several convolution blocks. Considering the variation in salient regions within the image context, the required prompt positions and sizes should also be varied. Therefore, we incorporate image context as prior knowledge into PGN during the forward inference process. By integrating this prior knowledge, the network can customize a unique prompt-associated axial Gaussian distribution for each image \(I\), thereby achieving adaptive and infinite sampling in the latent prompt distribution:

\[[_{P},_{P}]=F_{PGN}(T_{P},F_{I};),\] (6)

where the parameters of the prompt generation network \(F_{PGN}\) are modeled by the parameters \(\) of our desired distribution, as each set of generated mean and variance uniquely specifies a distribution. Previous research indicates that allowing the model to conditionally perceive the ground truth label distribution during the training phase enhances training stability for such tasks that exhibit significant uncertainty. Thus, a posterior version for prompt generation network \(F_{PGN}^{post}\), parameterized by \(^{}\), is further introduced during training, learning to generate the effective distribution for prompt embedding when accessing the ground-truth label distribution:

\[[_{P}^{},_{P}^{}]=F_{PGN}^{post}(T_{P},F_{I}, GT;^{})\] (7)

We only employ this posterior network during training and guide the standard network, which cannot perceive the true labels during testing, to achieve viable performance via a KL loss.

\[_{PKL}=D_{}(_{P}^{}, (_{P}^{}))(_{P},(_{P}))\] (8)

### Granularity-aware Object Ambiguity Modeling

Distributional Object Representation.The model integrates object ambiguity from a probabilistic perspective, enhancing its ability to solve problems in innovative ways. We instantiate various segmentation labels to represent ambiguity levels and incorporate them as priors in visual feature extraction, influencing object-related feature modeling for the input image \(I\),

\[}=(F_{I},}),} _{I}()=(_{I},(_{I})),\] (9)

where \(_{I}\) and \(_{I}\) respectively denote the mean and standard deviation of the Axis Gaussian Distribution generated by the network, where \(_{I},_{I}^{N_{I}}\). This approach enhances our understanding and depiction of object diversity and uncertainty in a more profound and lucid manner, enabling better adaptability and handling of variation and uncertainty.

**Granularity-aware Image Embedding Generation.** SAM generates multiple candidate segmentation masks for the same object at different granularities and levels, demonstrating the inherent ambiguity of SAM related to object granularity. This inspires us to leverage this aspect to enhance the model's perception of ambiguous objects. We subsequently introduce an image generation network (IGN) for object embedding to model this distribution:

\[[_{I},_{I}]=F_{IGN}(F_{I};),\] (10)

where the parameters of the image generation network \(F_{IGN}\) are modeled by the parameters \(\) of our desired distribution, as each set of generated mean and variance uniquely specifies a distribution. Similar to the previous introduction of a posterior network to enhance learning in modeling prompt ambiguity, we introduce a posterior version of IGN, denoted as \(F_{IGN}^{post}\), for perceptible labels.

\[[_{I}^{},_{I}^{}]=F_{IGN}^{post}(F_{I},GT;^ {})\] (11)

We only employ this posterior network during training and guide the standard network, which cannot perceive the true labels during testing, to achieve viable performance via a KL loss.

\[_{IKL}=D_{}(_{I}^{}, (_{I}^{}))(_{I},(_{I}))\] (12)

### Overall Optimization

In line with current SAM techniques that generate segmentation masks for the same object at different granularity levels, the present approach utilizes this feature to enhance the model's capture of multi-level object concepts through a common ensemble strategy. Specifically, given the multiple candidate outputs from SAM, represented as \(\{^{1},^{2}...,^{n}\}\), where \(n\) is the number of scales. By introducing a set of learnable mask weights \(=\{w_{1},w_{2}...,w_{n}\}^{n}\), the final mask output can be fine-tuned and obtained through a weighted sum calculation:

\[=_{i=1}^{n}w_{i}^{i},\] (13)

where \(w_{1},w_{2},...,w_{n}\) are initialized to \(\) and subsequently fine-tuned to enable the model effectively being aware of the object scales. By adaptively integrating multiple scale masks, the model's perception and modeling capabilities for complex target diversity are further enhanced.

When the representation from SAM is combined with the ground-truth segmentation \(GT\) from the training samples, a guide-providing teacher prediction segmentation \(^{}\) is created. A cross-entropy loss \(CE(,)\) is employed to penalize the discrepancies between the distribution of \(^{}\) and \(GT\), i.e., \(_{M}()\) and \(_{GT}\), where the distribution of GT is a constant value that does not need to be parameterised, as: \(_{Seg}=CE(GT,)\). Additionally, the \(KL\) losses introduced to regularize training in prompt ambiguity and image ambiguity, respectively, in Eq. 8 and Eq. 12, are amalgamated into a weighted sum with weight coefficient of \(_{P}\) and \(_{I}\):

\[_{All}=_{Seg}+_{P}_{PKL}+_{I }_{IKL}.\] (14)

The model is trained from scratch using randomly initialized weights. Parameters requiring update include the prompt encoder and image encoder within SAM as well as PGN, posterior PGN, IGN, and posterior IGN. The KL loss during training aligns the distribution of perceptually true segmentation labels (encoded segmentation variants) with the distribution that is imperceptible at inference time. Adhering to this training objective, the eventual distribution is adjusted to encompass all segmentation variants for a specific input image.

## 4 Experiment

### Experimental Setup

**Dataset.** Four datasets are utilized for comparison. The LIDC-IDRI dataset  is used for lung lesion segmentation, consisting of lung computed tomography scans from 1010 subjects with annotations from four domain experts. This dataset accurately captures the typical ambiguity found in CT imaging. The BraTS 2017 dataset  is used for 3D brain tumor segmentation, comprising 285 cases of 3D MRI images. Each image includes 155 slices and four modes (T1, T1ce, T2, and Flair). These slicesare annotated into four classes: Background, Non-enhancing/Necrotic Tumor Core, Edema, and Enhancing Tumor Core. The ISBI 2016 dataset  contains 900 dermoscopic images for training and 379 images for testing, all annotated by an expert with the lesion area. The images are resized and padded to maintain a uniform scale. The SIM 10k dataset  consists of 10,000 images rendered by the gaming engine Grand Theft Auto, providing bounding boxes of 58,701 cars in training images.

**Implementation Details.** For the LIDC dataset, we use the included four expert annotations to represent different ambiguous segmentation labels. In the case of the BraTS dataset, we amalgamate annotations from different categories into a binary mask, creating multiple segmentation masks to mimic real-world ambiguous segmentation scenarios. For the ISBI dataset, we use the single label provided. All three datasets are optimized using the Adam optimizer, with a learning rate of 1e-4, over 100 epochs. For the SIM 10k dataset, we select images where pixels from two instances overlap, creating three potential masks. Optimization for this dataset is carried out with Adam optimizer over 500 epochs, with a learning rate of 1e-4. The trade-off coefficients are set as \(_{P}=_{I}=1\).

**Evaluation Metrics.** Four metrics are used for evaluation: Generalized Energy Distance (GED), Hungarian-Matched Intersection over Union (HM-IoU), Maximum Dice Matching (\(D_{max}\)), and Average Dice Matching (\(D_{mean}\)). GED is a metric used in ambiguous image segmentation tasks that compares the distribution of segmentations. It leverages the distance between observations, where lower energy signifies a better match between prediction and the ground truth. HM-IoU calculates the optimal match of Intersection over Union (IoU) between annotations and predictions using Hungarian algorithm, providing an accurate representation of sample fidelity. \(D_{max}\) and \(D_{mean}\) represent the best and average Dice scores between each prediction result and each ground truth, respectively.

### Comparison to Prompted Segmentation Models

Tab. 1 presents the quantitative results on four datasets, offering a comparison with the current state-of-the-art prompting-based segmentation models adapted for ambiguous segmentation tasks. Specifically, we have adapted SegGPT , a SAM-like prompt-based segmentation approach that

   Metric & GED \(\) & HM-IOU \(\) & D\({}_{max}\) & D\({}_{mean}\) & GED \(\) & HM-IOU \(\) & D\({}_{max}\) & D\({}_{mean}\) \\  Method &  &  \\  SegGPT w/ Point shift & 0.462 & 0.280 & 0.573 & 0.153 & 0.451 & 0.032 & 0.144 & 0.046 \\ SegGPT w/ Box shift & 0.392 & 0.354 & 0.638 & 0.325 & 0.348 & 0.082 & 0.224 & 0.146 \\ SEEM w/ Mask shift & 0.381 & 0.401 & 0.692 & 0.272 & 0.210 & 0.228 & 0.281 & 0.194 \\  SAM w/ Point shift & 0.377 & 0.365 & 0.650 & 0.337 & 0.252 & 0.169 & 0.334 & 0.238 \\ SAM w/ Box shift & 0.361 & 0.380 & 0.673 & 0.253 & 0.239 & 0.242 & 0.344 & 0.246 \\ \(\)-_SAM_ (Ours) & 0.228 & 0.717 & 0.948 & 0.356 & 0.193 & 0.610 & 0.864 & 0.423 \\   Method &  &  \\  SegGPT w/ Point shift & 0.649 & 0.662 & 0.659 & 0.323 & 0.259 & 0.128 & 0.151 & 0.127 \\ SegGPT w/ Box shift & 0.527 & 0.772 & 0.874 & 0.536 & 0.272 & 0.152 & 0.220 & 0.183 \\ SEEM w/ Mask shift & 0.522 & 0.821 & 0.908 & 0.760 & 0.238 & 0.271 & 0.344 & 0.246 \\  SAM w/ Point shift & 0.513 & 0.782 & 0.886 & 0.681 & 0.265 & 0.155 & 0.229 & 0.189 \\ SAM w/ Box shift & 0.491 & 0.792 & 0.896 & 0.685 & 0.255 & 0.160 & 0.239 & 0.199 \\ \(\)-_SAM_ (Ours) & 0.226 & 0.835 & 0.926 & 0.904 & 0.233 & 0.637 & 0.851 & 0.327 \\   

Table 1: Comparison with prompted segmentation models adapted for ambiguous segmentation.

Figure 3: Qualitative comparison with prompted segmentation models adapted for ambiguous segmentation. Examples include three ground-truth expert labels and sampled segmentation masks.

[MISSING_PAGE_FAIL:8]

mentation of their official code. The methods compared encompass recent ambiguous segmentation methodologies that amalgamate a conditional variational autoencoder and UNet: Probabilistic U-Net , Hierarchical Probabilistic U-Net (HProb UNet) , PhiSeg , Stochastic Segmentation Networks (SSN) , PixelSeg , and the ambiguous segmentation endeavor of Calibrated Adversarial Refinement (CAR)  and Collective Intelligence Medical Diffusion (CIMD)  that integrate generative models. It also includes ensemble-based techniques utilizing a Mixture-of-expert like the Mix of Stochastic Experts (Mose) . We observe that the _\(\)-SAM_ transcends state-of-the-art methodologies that amalgamate a conditional variational autoencoder and UNet in terms of diversity and precision. Additionally, the _\(\)-SAM_ outstrips segmentation paradigms based on generative models or ensembles in all dimensions. This suggests that our strategy accurately apprehends the ambiguous characteristics inherent in varied images and objects, effectively attaining equilibrium between diversity and precision in the ambiguous segmentation task. Fig. 4 further elucidates the qualitative outcomes of our methodology juxtaposed with extant technologies. In contrast to other technologies, the segmentations engendered by our proposed _\(\)-SAM_ preserve a higher degree of exact object detail, particularly boundary details, and provide a distinctive visual representation of potential diversity.

### Further Empirical Results

**Ablation Study.** Tab. 6 delineates the consequences of eliminating various principal strategies of the _\(\)-SAM_. _No Ambiguity Modeling_ precludes all ambiguous modeling, including both prompt embedding and image embedding levels. At this stage, we employ the smallest box encompassing all segmentation labels within each image mask as the standard prompt. This standard prompt is then randomly perturbed numerous times by scaling [0.8, 1.2] and shifting up, down, left, or right by [-8, 8] pixels, yielding different ambiguous segmentation outcomes. _No Object Ambiguity_ and _No Prompt Ambiguity_ either eradicates the ambiguity related to the object or the prompt, that is, it alters its object embedding or prompt embedding to a regular deterministic rather than an ambiguous characteristic. _No Posterior Distillation_ eliminates a process of network training guided by a teacher network that can perceive the actual labels. We discover that when any component is excised, the performance correspondingly deteriorates, which underscores the effectiveness of our proposed several strategies.

**Robustness Analysis.** Fig. 5 reports the IoU performance of _\(\)-SAM_ under a variety of instantaneous perturbations. The blue and red solid lines respectively illustrate the performance changes of the SAM model and our method when prompted by different disturbances, while the dashed lines depict the performance of both models under standard prompts, serving as an upper bound. We selected both light and severe degrees of perturbation. Specifically, 'Shift' indicates a random offset of the box by  pixels, 'Scale' represents a random scaling of the box by [0.85,1.15], 'Shift+' denotes a random offset of the box by  pixels, and 'Scale+' implies a random scaling of the box by [0.7,1.3]. Compared to the vanilla SAM baseline, _\(\)-SAM_ demonstrates robustness against various instantaneous perturbations.

## 5 Conclusion

The continued evolution of vision foundation models like the Segment Anything Model (SAM) demonstrates impactful universality, while also posing challenges in producing ambiguous and uncertain predictions. Minor changes in the prompt can cause significant variations in the model's output, challenging its required robustness. While many works aim to stabilize SAM's prediction capabilities,

   No Key Components & GED \(\) & HM-IOU \(\) & D\({}_{max}\)\(\) & D\({}_{mean}\)\(\) \\  No Ambiguity Modeling & 0.361 & 0.380 & 0.673 & 0.253 \\ No Object Ambiguity & 0.370 & 0.389 & 0.691 & 0.230 \\ No Prompt Ambiguity & 0.308 & 0.674 & 0.930 & 0.336 \\ No Posterior Distillation & 0.266 & 0.385 & 0.805 & 0.341 \\ _\(\)-SAM_ (Ours) & 0.228 & 0.717 & 0.948 & 0.356 \\   

Table 6: Ablation study on the proposed key components.

Figure 5: Robustness analysis of our _\(\)-SAM_ framework over the SAM baseline against prompt perturbation.

this paper uniquely explores leveraging this perceived flaw to advantageously model inherently ambiguous data distributions. We introduce an innovative optimization framework grounded in a conditional variational autoencoder, which cohesively models the prompt and the object granularity with a latent probability distribution. This approach endows the model with the capacity to adaptively perceive and represent the genuine ambiguous label distribution, thereby enabling SAM to generate a controlled series of diverse, persuasive, and reasonable segmentation outputs. Our comprehensive experiments across multiple practical deployment scenarios involving ambiguity underscore the exceptional performance of our framework, thereby illuminating the need for increased focus on addressing related challenges and opportunities.

Acknowledgement.This work was supported by the Hong Kong Research Grants Council (RGC) General Research Fund under Grant 11211221, 14204321. This work was also supported in part by the National Natural Science Foundation of China under Grant 82172033, Grant 82272071 and in part by the Dreams Foundation of Jianghuai Advance Technology Center, and in part by the Open Fund of the National Key Laboratory of Infrared Detection Technologies.