# Conformalized Credal Set Predictors

Alireza Javanmardi

LMU Munich, MCML

Munich, Germany

alireza.javanmardi@ifi.lmu.de &David Stutz

Max Planck Institute for Informatics

Saarbrucken, Germany

david.stutz@mpi-inf.mpg.de &Eyke Hullermeier

LMU Munich, MCML

Munich, Germany

eyke@ifi.lmu.de

###### Abstract

Credal sets are sets of probability distributions that are considered as candidates for an imprecisely known ground-truth distribution. In machine learning, they have recently attracted attention as an appealing formalism for uncertainty representation, in particular, due to their ability to represent both the aleatoric and epistemic uncertainty in a prediction. However, the design of methods for learning credal set predictors remains a challenging problem. In this paper, we make use of conformal prediction for this purpose. More specifically, we propose a method for predicting credal sets in the classification task, given training data labeled by probability distributions. Since our method inherits the coverage guarantees of conformal prediction, our conformal credal sets are guaranteed to be valid with high probability (without any assumptions on model or distribution). We demonstrate the applicability of our method on ambiguous classification tasks for uncertainty quantification.

## 1 Introduction

Representing and quantifying uncertainty is becoming increasingly important in machine learning (ML), particularly as ML models are employed in safety-critical application domains such as medicine or autonomous driving. In such domains, a distinction between so-called _aleatoric uncertainty_ (AU) and _epistemic uncertainty_ (EU) is often useful . Broadly speaking, aleatoric uncertainty is due to the inherent randomness of the data-generating process, whereas epistemic uncertainty stems from the learner's lack of knowledge about the best predictive model. Thus, while the former is irreducible, the latter can, in principle, be reduced through additional information, e.g., by gathering additional data to learn from.

Representation of aleatoric and epistemic uncertainty requires formalism more expressive than standard probability distributions . One such formalism which prevails in the recent ML literature is second-order probability distributions. Essentially, in a classification setting, these are distributions over distributions over classes. Models producing second-order distributions as predictions can be learned in a classical Bayesian way [16; 25] or using more recent approaches such as evidential deep learning . Yet, approaches of that kind are not unproblematic and have been subject to criticism [8; 9]. Specifically, such approaches have been shown to misrepresent epistemic uncertainty. Another formalism suitable for representing both types of uncertainty is the concept of a _credal set_, which is well-established in the field of imprecise probability theory  and meanwhile also attracted attention in ML [23; 46]. Credal sets are (convex) sets of probability distributions that can be considered as candidates for an imprecisely known ground-truth distribution.

Figure 1 shows examples of credal sets in a three-class scenario, where the space of distributions can be visualized by the two-dimensional probability simplex. Broadly speaking, the larger the credal set, the higher the epistemic uncertainty, and the more "in the middle" the set is located, i.e., the closer it is to the uniform distribution, the higher the aleatoric uncertainty.

Learning to predict second-order representations, such as credal sets or second-order distributions, from standard "zero-order" data -- training instances together with observed class labels -- has been shown to be difficult in that it typically provides biased estimates of epistemic uncertainty [8; 9]. To alleviate this problem, we assume "first-order" training data, i.e., instances associated with probability (frequency) distributions over the class labels. In other words, instances are labeled probabilistically instead of being assigned a deterministic class label. This type of data is becoming increasingly available in practice, for example, in the form of aggregations over multiple annotations per data instance [10; 31; 35; 63], and hence increasingly relevant in many applications [52; 53]. Moreover, such data facilitates second-order learning.

In this paper, we leverage conformal prediction (CP), a non-parametric approach for set-valued prediction rooted in classical frequentist statistics , to construct credal sets. With relatively mild assumptions similar to those in CP, this approach inherits the so-called marginal coverage from CP: Predicted sets are guaranteed to cover the true target with high probability. For us, this means that we can use any first-order or second-order predictive model to construct unbiased credal sets equipped with such a coverage guarantee. Specifically, we propose various nonconformity functions applicable to first- and second-order predictors to construct conformal credal sets. We also study the case where only a noisy version of first-order distributions is available, demonstrating that the coverage guarantee holds under a bounded noise assumption. On ChaosNLI , an ambiguous natural language inference task with multiple annotations per example, we show that our conformal credal sets are valid, i.e., covering the true ground-truth distribution with high probability while comparing the efficiency of different nonconformity functions. Together with experiments on CIFAR10-H --a variant of the CIFAR10 test set containing class distributions from human annotations--we demonstrate how these credal sets enable practical quantification of aleatoric and epistemic uncertainty. We further complement this study with controlled experiments on synthetic data, specifically investigating the performance of credal set prediction in the presence of noisy data.

## 2 Background

### Supervised Learning and Predictive Uncertainty

We consider the setting of (polychotomous) classification with label space \(=\{1,,K\}\) and an instance space \(\). As usual, we assume an underlying data-generating process in the form of a probability distribution \(P\) on \(\), so that observations \((X,Y)\) are i.i.d. samples from \(P\). We denote by \(^{}=(_{1}^{},, _{K}^{})^{}^{}\) the conditional probability distribution \(P(\,|\,X=)\), which we also consider as an element of the \((K-1)\)-simplex \(^{K}\). Thus, the probability to observe \(Y=k\) as an outcome for \(\) is given by \(_{k}^{}\).

Figure 1: For the three-class classification setting, the space of probability distributions can be illustrated by a two-dimensional simplex: each point in the simplex corresponds to a probability distribution so that credal sets can be depicted as regions. The left case corresponds to the special case of a singleton (credal) set, i.e., a precise probability distribution, signifying aleatoric but no epistemic uncertainty. The case in the middle represents partial knowledge with a certain degree of (epistemic) uncertainty about the true distribution, and the right one corresponds to the case of complete ignorance, where nothing is known about the distribution.

Since the dependency between instances \(X\) and outcomes \(Y\) is non-deterministic, the prediction of \(Y\) given \(X=\) is necessarily afflicted with uncertainty, even if the ground-truth distribution \(^{}\) is known. As already said, this uncertainty is commonly referred to as _aleatoric_. Intuitively, the closer \(^{}\) to the uniform distribution \(_{}=(1/K,,1/K)^{}\), the higher the uncertainty, and the closer it is to a degenerate (Dirac) distribution assigning all probability mass to a single class (a corner point in \(^{K}\)), the lower the uncertainty.

Instead of assuming \(^{}\) to be known, suppose now that only a prediction \(}^{}\) of this distribution is available. _Epistemic uncertainty_ refers to the uncertainty about how well the latter approximates the former, and hence to the additional uncertainty in the prediction of outcome \(Y\) that is caused by the discrepancy between \(}^{}\) and the ground-truth \(^{}\). We seek to capture this discrepancy by means of credal sets \(Q_{K}^{K}\,,\) with the idea that \(Q^{}\) holds with high probability. Typically, credal sets are assumed to be convex, and further restrictions might be imposed on \(_{K}\) for practical and computational reasons, for example, a restriction to convex polygons (with a finite number of extreme points).

### Conformal Prediction

Conformal prediction provides a general framework for producing set-valued predictions with a certain guarantee of validity. In a supervised setting, consider data points of the form \(Z=(X,U)\), and the task is to predict \(U\) given \(X=\). We assume the space \(=\) to be equipped with a nonconformity measure \(f:\,\) that quantifies the "strangeness" of \(\), i.e., the higher \(f()\), the less normal or expected the data point. Let \(_{}\) be a (randomly generated) set of data points, called _calibration data_, and \(Z\) another data point that remains unobserved. Under the assumption of exchangeability, i.e., that the calibration data and the query point \(Z\) have been generated by an exchangeable process, we seek a so-called confidence set \(C\) that guarantees coverage:

\[(U C) 1-.\] (1)

By a simple combinatorial argument , the confidence set \(C\) can be constructed as

\[C():=\,u\,|\,f(,u) q(, ^{})\,}\,,\] (2)

where \(:=\{f():_{}\}\) is the set of nonconformity scores, \(^{}=||^{-1}(1+||)(1-)\), and \(q(;^{})\) denotes the \(^{}\)-quantile of \(\). Importantly, the guarantee (1) holds regardless of the nonconformity function \(f()\), which, however, has an influence on the _efficiency_ of the prediction: The more appropriate the function, the smaller the prediction set \(C\) tends to be. Normally, \(f()\) is not predefined but constructed in a data-driven way using training data \(_{}\). For example, a common approach is to train a predictor \(:\,\) and then define \(f(,u)\) in terms of \(d(u,())\), where \(d(,)\) is an appropriate distance function on \(\). Replacing the point-prediction \(()\) by the prediction set \(C()\) can then be seen as "conformalizing" the predictor \(\).

## 3 Conformal Credal Set Prediction

Our goal is to learn a credal set predictor \(h:\,_{K}\), that is, a model that makes predictions in the form of credal sets, thereby representing both aleatoric and epistemic uncertainty. To this end, we assume access to first-order data, i.e., probabilistic training data of the form

\[=_{1},^{_{1}},, _{N},^{_{N}}} ^{K}\,.\] (3)

The model \(h\) should be able to predict the (probabilistic) outcomes for new query instances in a reliable way. More specifically, suppose that \(_{}\) is a new query instance (following the same distribution as the training data) for which a prediction is sought. To correctly represent epistemic and aleatoric uncertainty, we want the credal prediction \(Q=h(_{})\) to be valid, meaning \(Q^{_{}}\) with high probability, while at the same time being informative such that the (epistemic) uncertainty reflected by \(Q\) is as small as possible.

We aim to construct the credal set predictor \(h\) by means of conformal prediction. Following the conformalization recipe outlined in Section 2.2, we partition \(\) into \(_{}\) and \(_{}\), using the former for model training and the latter for calibration.

Regarding the training step, we explore two learning strategies connected with two ways of defining a nonconformity function, which is pivotal in the calibration step. The first approach is based on training a standard (_first-order_) probability predictor, i.e., a probabilistic classifier \(g:\,^{K}\) that maps instances to the (first-order) probability distribution on \(\). This can be achieved, for example, by minimizing the cross-entropy loss between the ground truth and the predicted distributions, i.e.,

\[g=*{argmin}_{g}\,_{(_{i},^{ _{i}})_{}}-_{k=1}^{K}_{k}^{_ {i}}((_{i})_{k}),\] (4)

where \(\) is a hypothesis space. Given a predictor \(g()\) of this kind, nonconformity is naturally defined in terms of a distance:

\[f_{1}(,^{}):=d(^{},g())\,,\] (5)

where \(d(,)\) is a suitable distance function on \(^{K}\), such as total variation, Wasserstein distance, etc.

An alternative approach is motivated by recent work on (epistemic) uncertainty representation via _second-order_ probability distributions [16; 25; 44]. A second-order learner \(G:\,(^{K})\) maps each input \(\) to a distribution over \(^{K}\). Given the training data of the form (3), meaningful learning in this context can be accomplished, for instance, by parameterizing the second-order distributions using Dirichlet distributions. Specifically, one can assume that each \(\) is associated with a Dirichlet distribution characterized by the parameter vector \(^{}_{ 1}^{K}\) with the probability density function

\[P(\,|\,^{})=^{})} _{k=1}^{K}_{k}^{^{}_{k}-1}\,,\] (6)

where \(B()\) is the multivariate beta function. This way, \(^{}\) can be thought of as a sample from that distribution, i.e., \(^{}(^{})\). The model aims to find parameter vectors \(^{}\)s that minimize the negative log-likelihood loss

\[_{(_{i},^{_{i}})_{}} (B(^{_{i}}))-_{k=1}^{K}(_{k}^{_{ i}}-1)(_{k}^{_{i}}).\] (7)

Given a second-order predictor \(^{}\), nonconformity can be defined as a decreasing function of likelihood, e.g., as 1 minus relative likelihood:

\[f_{2}(,^{})=1-^{}\,|\,^{})}{_{^{K}}P(\,|\,^{})}.\] (8)

Using the nonconformity function \(f_{i}()\) (\(i\{1,2\}\)), we obtain the set of nonconformity scores by

\[_{i}:=f_{i}(_{j},^{_{j}})\,|\,(_{j},^{_{j}})_{}}.\] (9)

Accordingly, the credal set can be defined as

\[h_{i}(_{}):=\,^{K}\,|\,f_{i}( {x}_{},) q(_{i},^{})\,}\,.\] (10)

Algorithm 1 outlines a summary of the proposed methods. In the following theorem, we state the validity of the predicted set, that is, the restatement of the conformal coverage guarantee  adjusted to our setting.

**Theorem 3.1** (Validity Guarantee).: _Let \(\) denote the joint probability distribution on \((X,)^{K}\). If data points in \(_{}\) and \((_{},^{_{}})\) are drawn exchangeably from \(\), then the conformal credal sets in (10) are valid, i.e.,_

\[^{_{}} h_{i}(_{}) 1-\,,\ i\{1,2\}.\]

It is worth mentioning that both predictors can also be trained using zero-order data. A first-order predictor can be achieved through standard training with a cross-entropy loss function, while a second-order predictor can be achieved, for instance, through the means of evidential learning . Hence, it is sufficient to have probabilistic calibration data in order to have a reasonable judgment of the predictors' performances and be able to conformalize them.

Moreover, while these two approaches can be compared in various ways, one immediate observation is that training a second-order predictor poses greater challenges. However, credal sets constructed using the second-order predictor exhibit a natural and superior adaptivity compared to those constructed by the first-order predictor. This is because, with the first-order predictor, once the quantile is determined during calibration, all distributions within a certain distance are included in the set for any given point at prediction time. On the other hand, with the second-order predictor, the resulting set depends on the calculated quantile as well as the skewness of the predicted distribution.

### Generalization to Imprecise First-Order Data

So far, we (implicitly) assumed that ground-truth probability distributions \(^{_{i}}\) will be provided as calibration (and training) data. Needless to say, this assumption will rarely hold true in practice. Instead, observations will rather be noisy versions \(}^{_{i}}\) of the true probabilities. Notably, such datasets emerge in scenarios where each data instance \(\) is annotated by multiple human experts, which recently have attracted a lot of attention in the context of machine learning [10; 31; 35; 43; 63] and also conformal prediction [24; 52]. In this context, \(}^{}\) denotes the distribution derived from aggregating annotator disagreements concerning the label of instance \(\). Of course, conformal prediction can still be applied to noisy data of that kind, but the coverage guarantee will then only hold w.r.t. noisy labeling, i.e., \((}^{_{}} h(_{})) 1-\).

Practically, one may expect that the guarantees will hold for the ground truth as well, simply because calibration on noisy instead of clean data will tend to make prediction regions larger and hence more conservative. Moreover, since nonconformity is derived from a predictive model \(g()\) that seeks to recover ground-truth probabilities, the latter should conform at least as well as noisy distributions. Of course, this intuition is not a formal guarantee. In order to provide such a guarantee for the ground-truth probabilities, one obviously needs to make some assumptions. Concretely, let us make the following _bounded noise_ assumption for the labeling process: The labeling noise is (stochastically) bounded in the sense that, given the nonconformity function \(f\) and a (small) probability \(>0\), there exists a tolerance \(>0\) such that the following holds all \(\):

\[(|f(,^{})-f(,}^{})|<) 1-.\] (11)

**Theorem 3.2**.: _Let \(>0\) be any miscoverage rate, and suppose the bounded noise assumption holds. Let \(q=q(,)\) be the critical threshold on the noisy calibration data \(_{}\) for miscoverage rate \(=(-)/(1-)\). Then, for any new query \(_{}\),_

\[f(_{},^{_{}})<q+  1-\,.\]

The proof is deferred to Appendix B. As a consequence of this result, a conformal predictor learned on the noisy data with modified miscoverage rate \(\) can be turned into a valid predictor (with miscoverage rate \(\)) for the ground-truth data by increasing the learned rejection threshold by \(\), provided the bounded noise property (11) can be ascertained. Thus, if we denote the corresponding credal set predictor by \(h_{}\), we can guarantee that \(^{_{}} h_{}(_{ }) 1-\,.\) For a comprehensive study of handling noisy data in conformal prediction, we refer to .

## 4 Experiments

For the sake of comparison, we examine different nonconformity functions in our experiments. When utilizing a first-order predictor, besides total variation (**TV**) and the First Wasserstein (**WS**)distance, we also investigate the Kullback-Leibler (**KL**) divergence and 1 minus the inner product (**Inner**) as nonconformity functions. For the second-order predictor, we consider 1 minus the relative likelihood (**SO**) as defined in (8). Furthermore, we demonstrate how our credal sets allow uncertainty quantification and the disentanglement of total uncertainty into epistemic and aleatoric components. We focus on a measure proposed by Abellan et al. , which regards the upper Shannon entropy \(H^{1}\) of a given credal set \(Q\) as total uncertainty, the lower Shannon entropy as aleatoric, and the difference between upper and lower entropy as epistemic:

\[(Q)}_{}=(Q)}_{}+(Q)-H_{*}(Q))}_{}\ \ \ H^{*}(Q):=_{ Q}H(),\ H_{*}(Q):=_ { Q}H().\] (12)

We refer to  for other measures, along with a discussion of their corresponding strengths and weaknesses. Interestingly enough, the interval \([H_{*}(Q),H^{*}(Q)]\) can be seen as an alternative characterization of the credal set \(Q\), which helps address the challenge of visualization for \(K>3\).

In this section, we focus on experiments on two real-world datasets. Further information on these datasets and details about the learning models can be found in Appendix C. Additional experiments on synthetic data, including an illustrative example showing how the resulting credal sets change as epistemic uncertainty decreases, and experiments on the impact of imprecise first-order data, are provided in Appendix E. All implementations and experiments can be found on our GitHub repository.2

ChaosNLI Dataset.We start our experiments with a highly ambiguous dataset, ChaosNLI , where the task is to classify the textual entailment of a premise-hypothesis pair into three classes: entailment, contradiction, and neutral. We train both first-order and second-order predictors using this data and construct credal sets with all five nonconformity functions to facilitate a comprehensive comparison of these methods. In Figure 2, we compare the resulting credal sets of different nonconformity functions for three specific instances. As expected, even though CP should work regardless of the choice of nonconformity score function, this choice affects the size and geometry of the prediction set. To compare the prediction set size, aka _efficiency_, across different nonconformity functions, we discretize the simplex using a fine grid. The efficiency is gauged by considering the fraction of all distributions that lie within the predicted credal sets. We perform training, calibration, and testing using 10 different random seeds and depict the average coverage and efficiency results of each credal set predictor under different miscoverage rates (\(\)) in Figure 3. Notably, the mean of the

Figure 2: Various credal sets obtained for three instances from ChaosNLI dataset. The ground truth distributions are denoted by orange squares. Black circles indicate model predictions in cases employing a first-order learner (first four columns). For the last column, utilizing a second-order learner, the predicted second-order distributions are represented through contour plots. The miscoverage rate is \(=0.2\), and the efficiency of each credal set is depicted below it.

average coverage over the test data across various random seeds aligns with or exceeds the nominal value, consistent with the conformal prediction guarantee.

We calculate the uncertainty intervals as in (12) for all (test) instances. Figure 4 provides scatter plots of quantified AU vs. EU for different credal set predictors, with colors indicating the entropy of the ground-truth distribution. These plots serve as an evaluation of uncertainty quantification performance. Generally, given access to the first-order distribution, we expect the following: if the EU is low, the AU should align closely with the entropy of the ground-truth first-order distribution. However, when the EU is high, the quantified AU may vary, appearing either close to or far from this entropy. In contrast, with only standard zero-order data available, such direct evaluation isn't feasible, which is why indirect evaluation methods like OOD detection or accuracy-rejection curves are preferred.

Figure 4: Scatter plots of aleatoric versus epistemic uncertainty using various credal set predictors for the ChaosNLI dataset with \(=0.2\). The colors indicate the entropy of the ground truth distribution. The three cases numbered 1 to 3, correspond to the first to third rows of instances shown in Figure 5.

Figure 5: Different uncertainty situations given the predicted credal sets generated by **SO** method for the ChaosNLI dataset with \(=0.2\).

Figure 3: Coverage and efficiency results of different nonconformity functions applied on the ChaosNLI dataset. The horizontal dashed lines indicate the nominal coverage levels.

For the **SO** method, three uncertainty cases are chosen from the results: high EU, low EU with high AU, and low EU with low AU as shown in Figure 5. When visualizing the uncertainty intervals, we display the entropy of the ground truth distribution in orange and the predicted entropy for the first-order predictor or the mean of the Dirichlet distribution for the second-order predictor in black. However, note that covering the entropy of a distribution with the uncertainty interval does not equate to covering the distribution itself with the credal set.

CIFAR-10H Dataset.We also apply our methods to the CIFAR-10H dataset , which contains the distributions on the classes for the CIFAR-10 test set derived from human annotations. This time, we simply use a first-order predictor pre-trained on the CIFAR-10 training set (i.e., standard zero-order training data) and only use CIFAR-10H for calibration and testing. Since the visualization of credal sets is no longer possible for this dataset, we limit our attention to uncertainty quantification analysis. Figure 6 illustrates scatter plots of quantified AU versus EU for different credal set predictors. Three uncertainty cases are chosen from the results and shown in Figure 7. For instance, in the first row, the model's predicted distribution (\(g()\)) heavily favors "cat" over "dog", while human-derived distributions assign nearly equal probabilities to both classes. This relatively high epistemic uncertainty is effectively captured by the uncertainty interval.

Further Discussions.Given a model and a data point, different methods may exhibit different behaviors in terms of uncertainty representation, stemming from their distinct approaches to generating the sets. For instance, it has been observed that **KL** tends to limit the set's expansion towards high entropy regions compared to other methods. The **Inner** method, on the other hand, tends to construct

Figure 6: Scatter plots of aleatoric versus epistemic uncertainty using various credal set predictors for the CIFAR10-H dataset with \(=0.2\). The colors indicate the entropy of the ground truth distribution. The three cases numbered 1 to 3, correspond to the first to third rows of instances shown in Figure 7.

Figure 7: Different uncertainty situations given the predicted credal sets generated by **KL** method for the CIFAR10-H dataset with \(=0.2\).

credal sets by incorporating corners (degenerate distributions), thereby setting the lower entropy to zero. Consequently, it may fail to effectively represent different uncertainty components in a reliable manner. Moreover, this method has the potential to yield empty credal sets, meaning that even the prediction of the first-order predictor may be excluded from the credal set.

From an uncertainty quantification perspective and considering the measure in (12), it becomes apparent that set size may not always accurately reflect epistemic uncertainty. In fact, a credal set of a certain size positioned in the middle of simplex may contain less epistemic uncertainty than the same set located around a corner.

The observed dependency between quantified aleatoric and epistemic uncertainty in Figures 4 and 6 is partly due to the chosen measure in (12): Generally, higher epistemic uncertainty implies a larger uncertainty interval, resulting in a lower value for aleatoric uncertainty. Thus, this dependency is inevitable in the high EU region. Another unintended behavior observed in the **TV**, **KL**, and **WS** approaches is that low epistemic uncertainty coincides _solely_ with high aleatoric uncertainty. This issue arises mainly from the lack of adaptivity in the credal sets constructed by first-order predictors. In Appendix D, we propose a method to enhance the adaptivity of these approaches using the concept of normalized nonconformity functions .

## 5 Limitations

The methods we propose are promising but still subject to certain limitations. For instance, for our methods to perform effectively, we require first-order data, at least for calibration. While such data is becoming increasingly available in practice, it is not accessible for all datasets and domains. Besides, for our generalization to the case of imprecise first-order data, practical implementation depends on a meaningful choice of the hyperparameters \(\) and \(\) to ensure inference that is both valid and efficient. When labels are based on relative frequencies (as in the case of multiple annotators), classical statistical methods might apply. However, determining an appropriate choice of \(\) and \(\) for broader practical problems remains an open issue. Another challenge lies in representing credal sets as subsets of the probability simplex. Although the credal sets can always be precisely described by equation (10), for the nonconformity functions used in this work, there are no closed-form equations for the resulting credal sets. Instead, the sets are represented implicitly through the nonconformity threshold. Numerical approximation is feasible but generally limited to scenarios with a small number of classes. The representation issue is also connected to computing uncertainty measures for quantifying epistemic and aleatoric uncertainty in a credal set that involves the computation of specific set characteristics [26; 41].

## 6 Conclusion and Future Work

Conformal credal set prediction connects machine learning with imprecise probability theory and offers a novel data-driven approach to constructing predictions that effectively capture both aleatoric and epistemic uncertainty. Thereby, it provides the basis of a new approach to reliable, uncertainty-aware machine learning. Leveraging the inherent validity of the conformal prediction framework, our conformalized credal sets are assured to cover the ground truth distributions with high probability. We have explored different nonconformity functions within this novel setting and evaluated their performance through numerical experiments.

There are several promising directions for future work. One avenue is to extend our method to standard (zero-order) data. While it has been shown that learning a second-order predictor from such data is challenging [8; 9], whether similar problems apply to credal predictors constructed from such data is not yet fully clear. Additionally, exploring other nonconformity functions that lead to closed-form solutions for credal sets or enhance efficiency and reduce uncertainty is worth considering. Finally, constructing a set of labels from our proposed credal sets presents an intriguing opportunity for further research, especially as such sets can provide more information compared to standard conformal prediction sets.

**The broader impact** of our work is the advancement of Machine Learning models towards better uncertainty-aware predictions, and we do not foresee any negative societal impacts.