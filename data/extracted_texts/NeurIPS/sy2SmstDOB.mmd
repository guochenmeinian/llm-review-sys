# UniFL: Improve Latent Diffusion Model via Unified Feedback Learning

Jiacheng Zhang\({}^{1,2,}\) &Jie Wu\({}^{2,,,}\)\({}^{,*}\) &Yuxi Ren\({}^{2}\) &Xin Xia\({}^{2}\) &Huafeng Kuang\({}^{2}\) &Pan Xie\({}^{2}\) &Jiashi Li\({}^{2}\) &Xuefeng Xiao\({}^{2}\) &Weilin Huang\({}^{2}\) &Shilei Wen\({}^{2}\) &Lean Fu\({}^{2}\) &Guanbin Li\({}^{1,3}\)\({}^{*}\)

\({}^{1}\)Sun Yat-sen University \({}^{2}\)Bytedance Inc \({}^{3}\)Peng Cheng Laboratory

Project Page: https://uni-fl.github.io/

Corresponding authors: wujie10558@gmail.com,liguanbin@mail.sysu.edu.cn ; \(\): project lead ; \(\): Equal Contribution. Work done during an internship at ByteDance.

###### Abstract

Latent diffusion models (LDM) have revolutionized text-to-image generation, leading to the proliferation of various advanced models and diverse downstream applications. However, despite these significant advancements, current diffusion models still suffer from several limitations, including inferior visual quality, inadequate aesthetic appeal, and inefficient inference, without a comprehensive solution in sight. To address these challenges, we present **UniFL**, a unified framework that leverages feedback learning to enhance diffusion models comprehensively. UniFL stands out as a universal, effective, and generalizable solution applicable to various diffusion models, such as SD1.5 and SDXL. Notably, UniFL consists of three key components: perceptual feedback learning, which enhances visual quality; decoupled feedback learning, which improves aesthetic appeal; and adversarial feedback learning, which accelerates inference. In-depth experiments and extensive user studies validate the superior performance of our method in enhancing generation quality and inference acceleration. For instance, UniFL surpasses ImageReward by 17\(\%\) user preference in terms of generation quality and outperforms LCM and SDXL Turbo by 57\(\%\) and 20\(\%\) general preference with 4-step inference.

Figure 1: Generated samples with 20 steps inference from stable-diffusion-xl-base-1.0 optimized by Unified Feedback Learning (UniFL). The last three images of the third row are generated with 4 steps.

Introduction

The emergence of diffusion models has led to remarkable advances in the field of text-to-image (T2I) generation, marked by notable milestones like DALLE-3 , Imagen , Midjourney , etc, elevating the generation quality of images to an unprecedented level. Particularly, the introduction of open-source image generation models, exemplified by latent diffusion model (LDM) , has inaugurated a transformative era of text-to-image generation, triggering numerous downstream applications such as T2I personalization [5; 6; 7; 8], controllable generation [9; 10; 11] and text-to-video (T2V) generation [12; 13; 14]. Nevertheless, despite these advancements achieved thus far, current latent diffusion-based image generation models still exhibit certain limitations. i) Inferior visual quality: The generated images still suffer from poor visual quality and lack authenticity. Examples include characters with incomplete limbs or distorted body parts, as well as limited fidelity in terms of style representation. ii) Inadequate aesthetic appeal: The generated image tends to lack aesthetic appeal and often fails to align with human preferences, especially in the abstract aesthetic concepts aspects such as color, lighting, atmosphere, etc. iii) Slow inference speed: The iterative denoising process employed by diffusion models led to inefficiencies during inference that significantly impede generation speed, thereby limiting the practicality of these models in various application scenarios. Recently, numerous works have endeavored to address the aforementioned challenges. For instance, RAPHAEL  resorts to the techniques of Mixture of Experts) [16; 17; 18] boost the generation performance via stacking the space MoE and time MoE block. Works [19; 20; 21; 22; 23] represented by ImageReward  propose incorporating human preference feedback to guide diffusion models toward aligning with human preferences. SDXL Turbo , PGD , and LCM [26; 27], on the other hand, targets on achieve inference acceleration through techniques like distillation and consistency models . However, these methods primarily concentrate on tackling individual problems through specialized designs, which poses a significant challenge to the elegant integration of these techniques. For example, MoE significantly complicates the pipeline, making the acceleration method infeasible to apply, and the consistency models  alter the denoising process of the diffusion model, making it arduous to directly apply the ReFL preference tuning framework proposed by ImageReward . Therefore, a natural question arises: _Can we devise a more effective approach that comprehensively enhances diffusion models in terms of image quality, aesthetic appearance, and generation speed?_

To tackle this issue, we present UniFL, a solution that offers a comprehensive improvement to latent diffusion models through unified feedback learning formulation. UniFL aims to boost the visual generation quality, enhance aesthetic attractiveness, and accelerate the inference process. To achieve these objectives, UniFL features three novel designs upon the unified formulation of feedback learning. Firstly, we introduce a pioneering perceptual feedback learning (PeFL) framework that effectively harnesses the extensive knowledge embedded within diverse existing perceptual models to provide more precise and targeted feedback on the potential visual defects of the generated results. Secondly, we employ decoupled aesthetic feedback learning to boost the visual appeal, which breaks down the coarse aesthetic concept into distinct aspects such as color, atmosphere, and texture, simplifying the challenge of abstract aesthetic optimization. Furthermore, an active prompt selection strategy is also introduced to choose the more informative and diverse prompt to facilitate more efficient aesthetics preference learning. Lastly, UniFL develops adversarial feedback learning to achieve inference acceleration by incorporating the adversarial objective in feedback tuning. We instantiate UniFL with a two-stage training pipeline and validate its effectiveness with SD1.5 and SDXL, yielding impressive improvements in generation quality and acceleration. Our contributions are summarized as follows:

* **New Insight**: Our proposed method, UniFL, introduces a unified framework of feedback learning to optimize the visual quality, aesthetics, and inference speed of diffusion models. To the best of our knowledge, UniFL offers the first attempt to address both generation quality and speed simultaneously, offering a fresh perspective in the field.
* **Novelty and Pioneering**: In our work, we shed light on the untapped potential of leveraging existing perceptual models in feedback learning for diffusion models. We highlight the significance of decoupled reward models and elucidate the underlying acceleration mechanism through adversarial training.
* **High Effectiveness**: Through extensive experiments, we demonstrate the substantial improvements achieved by UniFL across various types of diffusion models, including SD1.5 and SDXL, in terms of generation quality and inference acceleration.

Related Works

**Text-to-Image Diffusion Models.** Text-to-image generation has gained unprecedented attention over other traditional tasks [29; 30; 31; 32; 33]. Recently, diffusion models have gained substantial attention and emerged as the _de facto_ mainstream method for text-to-image generation, surpassing traditional image generative models like GAN  and VAE . Numerous related works have been proposed, including GLIDE , DALL-E2 , Imagen , CogView  etc. Among these, Latent Diffusion Models (LDM)  extend the diffusion process to the latent space and significantly improve the training and inference efficiency of the diffusion models, opening the door to diverse applications such as controllable generation [9; 10], image editing [11; 38; 39], and image personalization [5; 7; 6] and so on. Even though, current text-to-image diffusion models still have limitations in inferior visual generation quality, deviations from human aesthetic preferences, and inefficient inference. The target of this work is to offer a comprehensive solution to address these issues.

**Improvements on Text-to-Image Diffusion Models.** Given the aforementioned limitations, researchers have proposed various methods to tackle these issues. Notably, [40; 15; 41] focuses on improving generation quality through more advanced training strategies. Inspired by the success of reinforcement learning with human feedback (RLHF) [42; 43] in the field of LLM, [20; 21; 44; 23; 45] explore the incorporation of human feedback to improve image aesthetic quality. On the other hand, [25; 24; 28; 27; 26] concentrate on acceleration techniques, such as distillation and consistency models  to achieve inference acceleration. While these methods have demonstrated their effectiveness in addressing specific challenges, their independent nature makes it challenging to combine them for comprehensive improvements. In contrast, our study unifies the objective of enhancing visual quality, aligning with human aesthetic preferences, and acceleration through the feedback learning framework.

## 3 Preliminaries

**Latent Diffusion Model.** Text-to-image latent diffusion models leverage diffusion modeling to generate high-quality images based on textual prompts, which generate images from Gaussian noise through a gradual denoising process. During pre-training, a sampled image \(x\) is first processed by a pre-trained VAE encoder to derive its latent representation \(z\). Subsequently, random noise is injected into the latent representation through a forward diffusion process, following a predefined schedule \(\{_{t}\}^{T}\). This process can be formulated as \(z_{t}=_{t}}z+_{t}}\), where \((0,1)\) is the random noise with identical dimension to \(z\), \(_{t}=_{s=1}^{t}_{s}\) and \(_{t}=1-_{t}\). To achieve the denoising process, a UNet \(_{}\) is trained to predict the added noise in the forward diffusion process, conditioned on the noised latent and the text prompt \(c\). Formally, the optimization objective of the UNet is:

\[()=_{z,c,c,t}[||-_{}(_{t}}z+_{t}},c,t)||_{2}^{2}]\] (1)

**Reward Feedback Learning.** Reward feedback learning (ReFL)  is a preference fine-tuning framework that aims to improve the diffusion model via human preference feedback. It consists of two phases: (1) Reward Model Training and (2) Preference Fine-tuning. In the Reward Model Training phase, human preference data is collected to train a human preference reward model, which serves as a proxy to provide human preferences. More specifically, considering two candidate generations, denoted as \(x_{w}\) (preferred generation) and \(x_{l}\) (unpreferred one), the loss function for training the human preference reward model \(r_{}\) can be formulated as follows:

\[_{}()=-_{(c,x_{w},x_{l}) }[log((r_{}(c,x_{w})-r_{}(c,x_{l})))]\] (2)

where \(\) denotes the collected feedback data, \(()\) represents the sigmoid function, and \(c\) corresponds to the text prompt. The reward model \(r\) is optimized to produce a reward score that aligns with human preferences. In the Preference Fine-tuning phase, ReFL begins with an input prompt \(c\), initializing a random latent variable \(x_{T}\). The latent variable is then progressively denoised until reaching a randomly selected timestep \(t\). Then, the denoised image \(x_{0}^{}\) is directly predicted from \(x_{t}\). The reward model obtained from the previous phase is applied to this denoised image, generating the expected preference score \(r_{}(c,x_{0}^{})\). ReFL maximizes such preference scores to make the diffusion model generate images that align more closely with human preferences:

\[_{}()=_{c p(c)}_{x_{0}^{ } p(x_{0}^{}|c)}[-r(x_{0}^{},c)]\] (3)

Our method follows a similar learning framework to ReFL but devises several novel components to enable comprehensive improvements.

## 4 UniFL: Unified Feedback Learning

Our proposed method, UniFL, aims to improve the latent diffusion models in various aspects, including visual generation quality, human aesthetic quality, and inference efficiency. our method takes a unified feedback learning perspective, offering a comprehensive and streamlined solution. An overview of UniFL is illustrated in Fig.2. In the following subsections, we delve into the details of three key components: perceptual feedback learning to enhance visual generation quality (section 4.1); decoupled feedback learning to improve aesthetic appeal (section 4.2); and adversarial feedback learning to facilitate inference acceleration (section 4.3).

### Perceptual Feedback Learning

Current diffusion models exhibit limitations in achieving high-fidelity visual generation, for example, object structure distortion. These limitations stem from the reliance on reconstruction loss(MSE loss) solely in the latent space, which lacks structural supervision on the high-level visual quality. To address this issue, we propose perceptual feedback learning (PeFL). Our key insight is that various visual perception models already embed rich visual priors, which can be exploited to provide feedback for visual generation and fine-tune the diffusion model. The complete PeFL process is summarized in Algorithm 1. In contrast to ReFL, which starts from a randomly initialized latent representation and only considers the text prompt as a condition, PeFL incorporates image content as an additional visual condition for perceptual guidance. Specifically, given a text-image pair, \((c,x)\), we first select a forward step \(T_{a}\) and inject noise into the ground truth image to obtain a conditional latent \(x_{0} x_{T_{a}}\). Subsequently, we randomly select a denoising time step \(t\) and denoising from \(x_{T_{a}}\), yielding \(x_{T_{a}} x_{T_{a}-1}... x_{t}\). Next, we directly predict \(x_{t} x_{0}^{{}^{}}\). By incorporating the visual condition input, the denoised image is expected to restore the same high-level visual characteristics, such as object structure, and style, which existing perception models can capture. For instance, in the case of object structure, the instance segmentation model can serve as a valuable resource as it provides essential descriptions of object structure through instance masks. Consequently, the feedback on the generation of such visual characteristics on \(x_{0}^{{}^{}}\) can be obtained by comparing it with the ground truth segmentation mask via:

\[_{}^{}()=_{x_{0},x_{0}^{{}^{}} G(x_{t_{0}})}_{}(m_{I}(x_{0}^{{}^{ }}),(x_{0}))\] (4)

where \(m_{I}\) is the instance segmentation model, \((x_{0})\) is the ground truth instance segmentation mask and \(_{}\) is the instance segmentation loss. Note that our PeFL differs from ReFL as indicated by the red font in Algorithm 1. With the visual condition input and perception model, the diffusion model is allowed to get a detailed and focused feedback signal on a specific aspect, instead of the general quality feedback offered by ReFL. Moreover, the flexibility of PeFL allows us to leverage various existing visual perceptual models, more examples can be found in the Appendix A.

### Decoupled Feedback Learning

**Decoupled Aesthetic Fine-tuning.** Existing text-to-image diffusion models exhibit shortcomings in images that satisfy human aesthetic preferences. While PeFL prioritizes objective visual quality,

Figure 2: **Overview of _UniFL_. We leverage a unified feedback learning framework to enhance the model performance and inference speed comprehensively. The training process of UniFL is divided into two stages, the first stage aims to improve visual quality and aesthetics, and the second stage speeds up model inference.**aesthetic quality is inherently subjective and abstract, requiring human aesthetic feedback to steer the generation process. Despite ImageNetward's attempt to incorporate human aesthetic preferences through a reward model, its performance is hindered by oversimplified modeling that fails to capture the multidimensional nature of human aesthetic preferences. Generally, humans consider the aesthetic attractiveness of an image from various aspects, such as color, lighting, etc, and conflating these aspects without distinguishing during preference tuning would encounter optimization conflicts as evidenced in . To address this issue, we follow  to achieve aesthetic preference tuning but suggest decoupling the various aesthetic aspects when constructing preference reward models. Specifically, we decomposed the general aesthetic concept into representative dimensions and collected the corresponding annotated data, respectively. These dimensions include color, layout, lighting, and detail. Subsequently, we train a separate aesthetic preference reward model for each annotated data according to Eq.2. Finally, we leveraged these reward models for aesthetic preference tuning:

\[_{}()=_{d}^{K}_{c p(c)}_{x_{0}^{} p(x_{0}^{}|c)}[(_{d}-r_{d}(x_{0} ^{},c))]\] (5)

\(r_{d}\) is the aesthetic reward model on \(d\) dimension, \(d\{,,,\}\), \(_{d}\) is the dimension-aware hinge coefficient, and \(K\) is the number of fine-grained aesthetic dimension.

```
1:Dataset: Captioned perceptual text-image dataset with \(=\{(_{1},_{1}),...(_{n},_{n })\}\)
2:Input: LDM with pre-trained parameters \(w_{0}\), perceptual model \(m.\), perceptual loss function \(\), loss weight \(\)
3:Initialization: The number of noise scheduler time steps \(T\), ad noise timestep \(T_{a}\), denoising time step \(t\).
4:for perceptual data point \((_{i},_{i})\)do
5:\(x_{0}\)VaeEnc(img\({}_{i}\)) // From image to latent
6:\(x_{T_{a}}\)AddNoise(\(x_{0}\)) // Add noise to latent
7:for\(j=T_{a},...,t+1\)do
8:no grad:\(x_{j-1}\)LDM\({}_{w_{i}}\{x_{j}\}\)
9:endfor
10:with grad:\(x_{t-1}\) LDM\({}_{w_{i}}\{x_{t}\}\)
11:\(x_{0}^{{}^{}} x_{t-1}\) // Predict the denoised latent
12:img\({}_{i}^{{}^{}}\)VaeDec(\(x_{0}^{}\)) // From latent to image
13:\(_{}(m(_{i}^{{}^{}}), (_{i})\) // PeFL loss by perceptual model
14:\(w_{i+1} w_{i}\) // Update LDM\({}_{w_{i}}\) using PeFL loss
15:endfor ```

**Algorithm 1** Perceptual Feedback Learning (PeFL)

### Adversarial Feedback Learning

The inherent iterative denoising process of diffusion models significantly hinders their inference speed. To address this limitation, we introduce adversarial feedback learning to reduce the denoising steps during inference. Specifically, to achieve inference acceleration, we exploit a general reward model \(r_{a}()\) to improve the generation quality of fewer denoising steps. However, as studied in , the samples under low inference steps tend to be too noisy to obtain the correct rewarding scores. To tackle this problem, rather than freeze the reward model during fine-tuning, we incorporate an extra adversarial optimization objective by treating \(r_{a}()\) as a **discriminator** and update it together with the diffusion model. Concretely, we follow a similar way with PeFL to take an image as input and execute the diffusion and denoising consecutively. Afterward, in addition to maximizing the reward score of the denoised image, we also update the reward model in an adversarial manner. The optimization objective is formulated as:

\[^{G}()=_{c p(c)}_{x_{0}^ {} p(x_{0}^{}|c)}[-r_{a}(x_{0}^{},c)],\] (6) \[^{D}()=-_{(x_{0},x_{0}^{},c) _{},t[1,T]}[(r_{a}(x_{0}))+(1- (r_{a}(x_{0}^{})))].\]

where \(\) and \(\) are the parameters of the diffusion model and discriminator. With the adversarial objective, the reward model is always aligned with the distribution of the denoised images with various denoised steps, enabling the reward model to function well across all the timesteps. Note that our method is distinct from the existing adversarial diffusion methods like SDXL-Turbo . Thesemethods take the _adversarial distillation_ manner to accelerate the inference, which tends to require another LDM as the teacher model to realize distillation, incurring considerable memory costs. By contrast, we follow the _reward feedback learning_ formulation, which integrates adversarial training with the reward tuning and achieves the adversarial reward feedback tuning via the lightweight reward model.

### Training Objective

We employ a two-stage training pipeline to implement UniFL. The first stage focuses on improving generation quality, leveraging perceptual feedback learning and decoupled feedback learning to boost visual fidelity and aesthetic appeal. In the second stage, we apply adversarial feedback learning to accelerate the diffusion inference speed. To prevent potential degradation, we also include decoupled feedback learning to maintain aesthetics. The training objectives of each stage are summarized as follows:

\[^{1}()=_{}()+_{ }();^{2}(,)=^{G}( )+^{D}()+_{}()\] (7)

## 5 Experiments

### Implementation Details and Metrics

**Dataset.** We utilized the COCO2017  train split dataset with instance annotations and captions for structure optimization with PeFL. Additionally, we collected the human preference dataset for the decoupled aesthetic feedback learning from diverse aspects (such as color, layout, detail, and lighting). 100,000 prompts are selected for aesthetic optimization from DiffusionDB  via active prompt selection. During the adversarial feedback learning, we use data from the aesthetic subset of LAION  with image aesthetic scores above 5.

**Training Setting.** We utilize the SOLO  as the instance segmentation model. We utilize the DDIM  scheduler with a total of 20 inference steps. \(T_{a}=10\) and the optimization steps \(t\) during PeFL training. For adversarial feedback learning, we initialize the adversarial reward model with the weight of the aesthetic preference reward model of details. During adversarial training, the optimization step is set to \(t\) encompassing the entire diffusion process. Our training per stage costs around 200 A100 GPU hours.

**Baseline Models.** We choose two representative text-to-image diffusion models with distinct generation capacities to comprehensively evaluate the effectiveness of UniFL, including (i) SD1.5 ; (ii) SDXL . Based on these models, we pick up several state-of-the-art methods(i.e. ImageNetward , Dreamshaper , and DPO  for generation quality enhancement, LCM , SDXL-Turbo , and SDXL-Lightning  for inference acceleration) to compare the effectiveness of quality improvement and acceleration. All results of these methods are reimplemented with the official code provided by the authors.

**Evaluation Metrics.** We generate the 5K image with the prompt from the COCO2017 validation split to report the Frechet Inception Distance (FID)  as the overall visual quality metric. We also report the CLIP score with ViT-B-32  and the aesthetic score with LAION aesthetic predictor to evaluate the text-to-image alignment and aesthetic quality of the generated images, respectively. Given the subjective nature of quality evaluations, we further conducted comprehensive user studies to obtain a more accurate evaluation.

### Main Results

**Quantitative Comparison.** Tab.1 summarize the quantitative comparisons with competitive approaches on SD1.5 and SDXL. Generally, UniFL exhibits consistent performance improvement on both architectures and surpasses the existing methods of focus on improving generation quality or acceleration. Specifically, for the generation quality, UniFL surpasses both DreamShaper (DS) and ImageReward (IR) across all metrics, where the former relies on high-quality training images while the latter exploits the human preference for fine-tuning. It is also the case when compared with the recently proposed preference tuning method DPO. In terms of acceleration, UniFL also exhibits notable performance advantages, surpassing the LCM with the same 4-step inference on both SD1.5 and SDXL. Surprisingly, we found that UniFL sometimes obtained even better aesthetic quality with fewer inference steps. For example, when applied to SD1.5, the aesthetic score is first boosted from 5.26 to 5.54 without acceleration, and then further improved to 5.88 after being optimized by adversarial feedback learning. This demonstrates the superiority of our method in acceleration. We also compared the two latest acceleration methods on SDXL, including the SDXL Turbo and SDXL Lightning. Although retaining the high text-to-image alignment, we found that the image generated by SDXL Turbo tends to lack fidelity, leading to an inferior FID score. SDXL Lightning achieves the most balanced performance in all of these aspects and reaches impressive aesthetic quality in 4-step inference. However, UniFL still obtains slightly better performance on these metrics.

**User Study.** We conducted a comprehensive user study using SDXL to evaluate the effectiveness of our method in enhancing generation quality and acceleration. As illustrated in Fig.3, our method significantly improves the original SDXL in terms of generation quality with a 68% preference rate and outperforms DreamShaper and DPO by 36% and 25% preference rate, respectively. Thanks to PeFL and decoupled aesthetic feedback learning, our method exhibits improvement even when compared to the competitive ImageNet, and is preferred by 17% additional people. In terms of acceleration, our method surpasses the widely used LCM by a substantial margin of 57% with 4-step inference. Even when compared to the latest acceleration methods like SDXL-Turbo and SDXL-Lightning, UniFL still demonstrates superiority and obtains more preference. This highlights the effectiveness of adversarial feedback learning in achieving acceleration.

**Qualitative Comparison.** As shown in Fig.4, UniFL achieves superior generation results compared with other methods. For example, when compared to ImageNet, UniFL generates images that exhibit a more coherent object structure (e.g., the horse), and a more captivating aesthetic quality (e.g., the cocktail). Notably, even with fewer inference steps, UniFL consistently showcases higher generation quality, outperforming other methods. It is worth noting that SDXL-Turbo, due to its modification of the diffusion hypothesis, tends to produce images with a distinct style.

  
**Model** & **Step** & **FID\(\)** & **CLIP Score\(\)** & **Aes Score\(\)** \\  SD1-5-Base & 20 & 37.99 & 0.308 & 5.26 \\ SD1-5-IR  & 20 & 23.31 & 0.312 & 5.37 \\ SD1-5-D8  & 20 & 34.21 & 0.313 & 5.44 \\ SD1-5-DPO  & 20 & 32.83 & 0.308 & 5.22 \\
**SD1-5-UnifL** & 20 & **31.14** & **0.318** & **5.54** \\ SD1-5-Base & 4 & 42.91 & 0.279 & 5.16 \\ SD1-5-ICM  & 4 & 42.65 & 0.314 & 5.71 \\ SD1-5-DSL ICM  & 4 & 35.48 & 0.314 & 5.58 \\
**SD1-5-UnifL** & 4 & **33.54** & **0.316** & **5.88** \\  SDXL-Base & 25 & 27.92 & 0.321 & 5.65 \\ SDXL-IR  & 25 & 26.71 & 0.319 & 5.81 \\ SDXL-D  & 28 & 28.53 & 0.321 & 5.65 \\ SDXL-DPO  & 25 & 35.30 & 0.325 & 5.64 \\
**SDXL-UnifL** & 25 & **25.54** & **0.328** & **5.98** \\ SDXL-Base & 4 & 15.89 & 0.256 & 5.18 \\ SDXL-LQM  & 24 & 27.23 & 0.322 & 5.48 \\ SDXL-Turbo  & 4 & 30.43 & 0.325 & 5.60 \\ SDXL-Lightning  & 4 & 28.48 & 0.323 & 5.66 \\
**SDXL-UnifL** & 4 & **26.25** & **0.325** & **5.87** \\   

Table 1: **Quantitative comparison** between our method and other methods on SD1.5 and SDXL architecture. The best performance is highlighted with bold font, and the second-best is underlined.

Figure 4: **Qualitive comparison** of the generation results of different methods based on SDXL.

Figure 3: **User study** about UniFL and other methods with 10 users on the generation of 500 prompts in generation quality (left) and inference acceleration (right).

### Ablation Study

To validate the effectiveness of our design, we systematically remove one component at a time and conduct a user study. The results are summarized in Fig.6 (a). In the subsequent sections, we will further analyze each component. More results are presented in the Appendix.

**Superiority of PeFL.** As depicted in Fig.5 (a), PeFL leverages the instance segmentation model to capture the overall structure of the generated object effectively. By identifying structural defects, such as the distorted limbs of the little girl, the broken elephant, and the missing skateboard, PeFL provides more precise feedback signals for diffusion models. Such fine-grained flaws can not be recognized well with ReFL due to its global and coarse preference feedback, instead, the exploited professional visual perception provides more detailed and targeted feedback. As presented in Fig.5 (b), the PeFL significantly boosts the object structure generation (e.g. the woman's glasses, ballet dancer's legs). It is also demonstrated by the notable performance drop (71.9% vs 28.1%) when disabling the PeFL.

**Multiple Aspects Optimization with PeFL.** PeFL exploits various perceptual models to improve some particular visual aspects of the diffusion model and can easily be extended to multi-aspect optimization. As illustrated in Fig.8, the simultaneous incorporation of two distinct optimization objectives (style and structure optimization) does not compromise the effectiveness of each other. Take the prompt a baby Swan, graffiti as an example, integrating the style optimization via PeFL upon the base model successfully aligns the image with the target style. Further integrating the structure optimization objective preserves the intended style while enhancing the overall structural details (e.g. the feet of the Swan).

**Necessity of Decoupling Design.** We conducted an experiment that finetuned the SD1.5 using the same prompt set but a global aesthetic reward model trained with all dimensions' collected aesthetic preference data. As depicted in Fig.6 (b), the generated images are more harmonious and have an artistic atmosphere with the decoupled aesthetic reward tuning and are preferred by more 17%

Figure 5: (a) Illustration of PeFL with instance segmentation model (SOLO). (b) Effect of PeFL on structure optimization.

Figure 6: (a) Design components ablation of UniFL. (b) Visualization of decoupled and non-decoupled aesthetic feedback learning results.

individuals than the non-decoupled counterpart. This can be attributed to the ease of abstract aesthetic learning with the decoupling design. Moreover, it also can be found that aesthetic feedback learning with actively selected prompts leads to a higher preference rate (54.6% vs 45.4%) compared with the random prompts. Further analysis of the prompt selection can be found in the Appendix.B.2.

**Selection of Hinge Coefficient \(_{d}\).** We select the hinge coefficient for each aesthetic reward model based on their reward distributions on the validation set. As illustrated in Fig.7 (left). there are clear margins in the reward scores between preferred and unpreferred samples. Moreover, such margin varies across these dimensions, emphasizing the necessity of the decoupled design. Empirically, we set \(_{d}\) to the average reward scores of the preferred samples to encourage the diffusion model to prioritize generating samples with higher reward scores. Fig.7 (right) demonstrates that setting a small hinge for the "color" reward resulted in only minor improvement, while substantial coefficients led to image oversaturation. Optimal results were achieved by selecting a coefficient close to the average reward score of the preferred samples. A similar trend was observed for layout and lighting aesthetics from our experiments, except for the "detail" dimension. Interestingly, a slightly lower coefficient sufficed for satisfactory detail optimization, as a higher coefficient introduced more background noise. This could be attributed to the significant reward score difference between preferred and unpreferred samples, where a high coefficient could excessively guide the model toward the target reward dimension.

**Analysis on Adversarial Feedback Learning.** We analyzed the mechanism for the acceleration behind our adversarial feedback learning and found that (i) Adversarial training enables the reward model to provide guidance continuously. As shown in Fig.9 (a), the diffusion model offer suffers rapid overfitting when frozen reward models, known as reward hacks. By employing adversarial feedback learning, the trainable reward model (acting as the discriminator) can swiftly adapt to the distribution shift of the diffusion model output, significantly mitigating the over-optimization phenomenon, and allowing the reward to provide effective guidance for a longer duration. (ii) Adversarial training expands the time step of feedback learning optimization. The adversarial objective poses a strong constraint to force high-noise timesteps to generate clearer images, which allows the samples across all denoising steps to be rewarded properly. As presented in Fig.9 (b), when disabling the adversarial objective while retaining the full optimization timesteps during rewarding, the reward model fails to provide effective guidance for samples under fewer denoising steps due to the high-level noise, which

Figure 8: Incorporating the style and structure optimization objectives simultaneously with PeFL results in _no effectiveness degeneration of each other_.

Figure 7: **Analysis on the \(_{d}\). Left: reward scores distribution on 5k validation preference image pairs with our final chosen values highlighted. Right: ablation on the \(_{d}\) on _color_ and _detail_ reward.**

results in poor generation results. With these two benefits, adversarial feedback learning significantly improves the generation quality of samples in lower inference steps and achieves superior acceleration performance ultimately. Notably, as shown in Fig.6 (a), the image generated with 4-step inference retains similar visual quality with 20-step inference (52.5% vs 47.4%) after going through the second stage training, which demonstrates the superiority of UniFL in acceleration.

**Ablation on Acceleration Steps.** We examine the acceleration capacity of UniFL under various inference steps, ranging from 1 to 8 as illustrated in Fig.10. Generally, UniFL performs exceptionally well with 2 to 8 inference steps with superior text-to-image alignment and higher aesthetic quality. The LCM method is prone to generate blurred images when using fewer inference steps and requires more steps (e.g., 8 steps) to produce satisfied images. However, both UniFL and LCM struggle to generate high-fidelity images with just 1-step inference, exhibiting a noticeable gap compared to SDXL-Turbo (e.g., the Labradoodle), which is intentionally designed and optimized for an extremely low-step inference regime. Therefore, there is still room for further exploration to enhance the acceleration capabilities of UniFL towards 1-step inference.

## 6 Conclusion

We propose UniFL, a framework that enhances visual quality, aesthetic appeal, and inference efficiency for latent diffusion models from the unified feedback learning perspective. By incorporating perceptual, decoupled, and adversarial feedback learning, UniFL can be applied to various latent diffusion models, such as SD1.5 and SDXL, and exceeds existing methods in terms of both generation quality enhancement and inference acceleration.

Figure 10: **Ablation on different inference steps of UniFL.**

Figure 9: **Analysis of the benefits of adversarial training. (a) It enables a longer optimization time for the reward model. (b) It enables the image under low denoising steps to be rewarded correctly. The red rectangle means incorporating the adversarial training objective.**