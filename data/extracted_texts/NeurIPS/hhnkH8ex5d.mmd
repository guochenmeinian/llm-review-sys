# Precipitation Downscaling

with Spatiotemporal Video Diffusion

 Prakhar Srivastava\({}^{1}\) Ruihan Yang\({}^{1}\) Gavin Kerrigan\({}^{1}\) Gideon Dresdner\({}^{2}\) Jeremy McGibbon\({}^{2}\) Christopher Bretherton\({}^{2}\) Stephan Mandt\({}^{1}\)

\({}^{1}\)University of California, Irvine \({}^{2}\)Allen Institute for AI, Seattle

{prakhs2,ruihan.yang,gavin.k,mandt}@uci.edu

{gideond,jeremym,christopherb}@allenai.org

###### Abstract

In climate science and meteorology, high-resolution local precipitation (rain and snowfall) predictions are limited by the computational costs of simulation-based methods. Statistical downscaling, or super-resolution, is a common workaround where a low-resolution prediction is improved using statistical approaches. Unlike traditional computer vision tasks, weather and climate applications require capturing the accurate conditional distribution of high-resolution given low-resolution patterns to assure reliable ensemble averages and unbiased estimates of extreme events, such as heavy rain. This work extends recent video diffusion models to precipitation super-resolution, employing a deterministic downscaler followed by a temporally-conditioned diffusion model to capture noise characteristics and high-frequency patterns. We test our approach on FV3GFS output, an established large-scale global atmosphere model, and compare it against six state-of-the-art baselines. Our analysis, capturing CRPS, MSE, precipitation distributions, and qualitative aspects using California and the Himalayas as examples, establishes our method as a new standard for data-driven precipitation downscaling.

## 1 Introduction

Precipitation patterns are central to human and natural life. In a rapidly warming climate, reliable simulations of changing precipitation patterns can help adapt to climate change. However, these simulations are challenging due to the multi-scale variability of weather systems and the influence of complex surface features (like mountains and coastlines) on precipitation trends and extremes

Figure 1: Static snapshot from the Spatiotemporal Video Diffusion (STVD) model, illustrating input (left) and output (right) precipitation frames. The input panel displays simulated coarse-resolution precipitation (rain, snow) fields (Section 3), super-resolved into the high-resolution output shown in the right panel. Both frames use Robinson projection and cover six tiles of the cubed-sphere grid, providing a detailed global view (optimal viewing with zoom). For dynamics, see Fig. 3.

. For many purposes, such as estimating flood hazards, precipitation must be estimated at spatial resolutions of only a few kilometers. Fluid-dynamical models of the global atmosphere are too expensive to run routinely at such fine scales , so the climate adaptation community relies on "downscaling"1 of coarse-grid simulations to a finer grid. Traditional downscaling methods are either dynamical (running a fine-grid fluid-dynamical model limited to the region of interest, which requires specialized knowledge and computational resources) or statistical (typically restricted to simple univariate methods) . Our work builds on vision based super-resolution methods to improve statistical downscaling and is a natural follow-up to recent deep-learning-based weather/climate prediction methods, which have revolutionized data-driven forecasting. These approaches boast improvements of orders of magnitude in runtime without sacrificing accuracy [46; 29].

We address the downscaling problem for a sequence. Our objective is to transform a sequence ("video") of low-resolution precipitation frames into a sequence of high-resolution frames. Despite differences from natural videos, precipitation's hourly temporal continuity allows us to use video super-resolution techniques to leverage multiple context frames for stochastic downscaling [53; 38].

Recent efforts to enhance the resolution of climate states like precipitation have relied on deterministic regression methods using convolutions or transformers. However, super-resolution is a one-to-many mapping with a continuum of "correct" answers. Supervised learning for these problems often leads to visual artifacts from _mode averaging_, where the network predicts an average of incompatible solutions, causing blurriness in visual data [30; 76]. Besides visual artifacts, mode averaging can have even more dramatic implications in climate and weather modeling, such as the underestimation of extreme precipitation , which is mainly induced by regional weather patterns on the unresolved scale. A natural alternative to supervised super-resolution methods [12; 27; 75; 11; 23] to prevent mode averaging is conditional _generative modeling_, which captures multimodal conditional distributions.

To that end, recent works propose using generative adversarial networks (GANs) for precipitation downscaling. These methods often face challenges, tending to converge on specific modes of the data distribution and occasionally fixating on isolated points in extreme cases. Despite their perceptual appeal, the scientific utility of super-resolution requires accurate modeling of the statistical _distribution_ of high-resolution data given low-resolution input, which GANs typically fail to capture.

We propose SpatioTemporal Video Diffusion (STVD)2 for precipitation downscaling. We use a deterministic regression model ("downscaler") for a coarse prediction, refined by a conditional video diffusion model that captures the residual error for adding fine-grained details. Both modules rely on spatio-temporal factorized attention to process the input sequence. Diffusion models are well-suited for precipitation downscaling as they successfully capture high dimensional and multimodal distributions, alleviating a key drawback of GAN-based methods for climate science applications.

Figure 2: Our modelâ€™s training and inference pipelines: Blue blocks apply to both phases, red blocks to training only, and green blocks to inference only. It deterministically downscales a low-resolution precipitation sequence using spatio-temporal factorized attention and models residuals with conditional diffusion (with factorized attention). Here, \(T\) denotes sequence length and \(N\) denotes diffusion steps. The parameters (\(=,\)) are optimized jointly during training. See A.1 for details.

This study highlights the capability of conditional diffusion models to meet the specific needs of statistical precipitation downscaling, with our key contributions being:

1. We introduce a novel framework for temporal precipitation downscaling using diffusion models. Our model combines a deterministic downscaling module with a diffusion-based residual module. It leverages spatio-temporal factorized attention to process information from multiple low-resolution frames.
2. Our model outperforms six strong super-resolution baselines across multiple criteria, including MSE and several distributional metrics. We compare against two image super-resolution models and four video super-resolution models using the FV3GFS global atmosphere simulation dataset [77; 10].
3. Our approach captures key characteristics of precipitation, including extreme precipitation probabilities and spatial patterns of annual precipitation in mountainous regions, which are crucial for domain science applications.

Our paper is structured as follows: we first describe our method (Sec. 2), followed by our experimental findings (Sec. 3). Finally, we discuss relevant literature (Sec. 4) and its connection to our work. Fig. 1 shows a global view of the input and predicted precipitation of our model. The code for our model is available at https://github.com/mandt-lab/STVD.

## 2 Downscaling via Spatiotemporal Video Diffusion

Problem StatementAt training time, we assume access to a collection of high-resolution precipitation frame sequences \(^{0:T}\) and their corresponding low-resolution precipitation frame sequences \(^{0:T}\). Such a low-resolution sequence can be obtained through area-weighted coarsening  of the corresponding high-resolution sequence. The dataset is discussed extensively in Sec. 3. Frame indices are represented by superscripts, where we assume that each sequence consists of \(T+1\) frames for simplicity. While it may be possible to roll out predictions for multiple sequences autoregressively using techniques such as reconstruction guidance , we leave this exploration for future work. Our objective is to train a model to effectively _downscale_, or _super-resolve_ a given sequence \(^{0:T}\) with \(^{0:T}\) serving as the target. We use "downscaling" and "super-resolution" interchangeably.

More formally, let \(^{t}^{C H W}\) and \(^{t}^{1 sH sW}\) represent individual low-resolution and high-resolution frames. Here, \(s\) denotes the downscaling factor, \(C\) is the number of channels (quantities used as input to the model to characterize the atmospheric state in each low-resolution grid cell so as to add skill to the precipitation prediction), and \(H,W\) indicate the height and width of the low-resolution frame. For our study, we adopt a downscaling factor of \(s=8\) and have \(C=12\) total low-resolution channels. In addition to the low-resolution precipitation state, we provide eleven channels of information to the model, such as topography, wind velocity, and surface temperature; see A.2 for details.

Solution SketchOur approach treats the downscaling problem as a conditional generative modeling task. We devise a model to learn the conditional distribution of high-resolution precipitation frames, incorporating contextual information from the low-resolution precipitation frame sequence.

Our proposed solution, **SpatioTemporal Video Diffusion (STVD)** (Fig. 2), relies on two modules: a deterministic downscaler and a stochastic component based on conditional diffusion models [20; 63], both using spatio-temporal factorized attention. The first module uses a UNet with factorized attention to integrate information from a low-resolution frame sequence, resulting in an initial prediction frame sequence \(}^{0:T}\). The second module is a conditional diffusion model that stochastically generates a sequence of additive residual frames \(^{0:T}\) which serves to add fine-grained details to the initial prediction. Together, these two modules produce a high-resolution frame sequence \(}^{0:T}=}^{0:T}+^{0:T}\). Both modules are trained end-to-end.

Decomposing the prediction into a deterministic mean and a stochastic residual is inspired by predictive-coding-based video decompression. This approach aims to predict a sequence of video frames while compressing the sparse residuals [2; 74] which are easier to model than dense frames. Similarly, it is easier to generate residuals than dense images when using diffusion models [73; 41].

In what follows, we first describe our overall probabilistic framework for downscaling. Then, we discuss the deterministic module along with spatio-temporal attention, followed by the remaining residual prediction module based on diffusion generative modeling. See A.1 for architecture details.

### Probabilistic Modeling of Downscaling

Given a sequence of low-resolution frames \(^{0:T}\) and the corresponding high-resolution frames \(^{0:T}\), we aim to learn a parametric approximation \(p_{}\) of the conditional distribution \(p(^{0:T}^{0:T}) p_{}( ^{0:T}^{0:T})\). Importantly, we do not assume independence across time; each generated frame \(^{t}\) can depend on all other generated frames. The generated high-resolution frame sequence is conditioned on the entire low-resolution frame sequence, capturing long-range temporal correlations and enhancing the fidelity and cohesion of the high-resolution reconstruction.

As noted earlier, the likelihood \(p_{}(^{0:T}^{0:T})\) is modeled using a deterministic downscaler and a residual diffusion model. We will discuss how the model parameters \(=(,)\) decompose into those for a downscaler (\(\)) and a diffusion model (\(\)).

#### 2.1.1 Deterministic Downscaling

Our first module is a deterministic downscaler that predicts an initial high-resolution frame sequence \(}^{0:T}=_{}(^{0:T})\) where \(_{}\) is a network generating a deterministic high-resolution prediction with parameters \(\). We perform bicubic interpolation on each frame of \(^{0:T}\) before passing the sequence through the network \(_{}\). Since the diffusion network operates on high-resolution inputs (i.e. denoising the high-resolution residuals), this choice allows us to use the same UNet  architecture (with different weights) for both the downscaling module \(_{}\) and the residual diffusion module. This enables us to easily share features across the modules via concatenation. See A.1 for further details.

Importantly, \(_{}\) incorporates a temporal attention mechanism that allows any frame at time \(t\), or its corresponding feature map, to attend to all context frames from \(0\) to \(T\). This architecture enables the concurrent inference of all frames within the sequence \(}^{0:T}\). The attention weights differ for each frame, allowing for the flexible incorporation of information across time.

Figure 3: A qualitative comparison between our proposed model and top baseline for a precipitation event associated with a cold front impinging on the Northern California coast and then the Sierra mountain range (coastline marked in hazy white). Fig. 6 plots the regional topography. The time interval between adjacent frames is 3 hours; the plotted region is \(1000 1000\) km. Our model resolves the fine-grid precipitation structure better than the considered baselines. See A.3 for full-page high quality samples from Himalayas and Sierra.

#### 2.1.2 Stochastic Residual Modeling via Diffusion

After computing the initial prediction \(}^{0:T}\), finer details are modeled by residuals learned from a conditional diffusion model. Our final stochastic high-resolution frame sequence \(}^{0:T}\) is generated by sampling an additive residual sequence \(^{0:T}\) from this model: \(}^{0:T}=}^{0:T}+^{0:T}\). Thus, we seek to model the residuals \(^{0:T}=^{0:T}-}^{0:T}\). Our diffusion model generates the entire residual sequence \(^{0:T}\) concurrently, with the generation of each residual \(^{t}\) dependent on the others. This is achieved via a UNet architecture with spatio-temporal attention, similar to the mechanism used for the deterministic downscaling module. See A.1 for further details.

To model the distribution of \(^{0:T}\), we use DDPM . To that end, we introduce a collection of latent variables \(^{0:T}_{0:N}\), where the lower subscripts indicate the denoising diffusion step. In the _forward process_, the latent variable \(^{0:T}_{n}\) is created from \(^{0:T}_{n-1}\) via additive noise. In the _reverse process_ for generation, a denoising model (with parameters \(\)) is trained to predict \(^{0:T}_{j-1}\) from \(^{0:T}_{j}\). \(N\) denotes the total number of denoising steps. Note that \(^{0:T}=^{0:T}_{0}\), i.e. the first diffusion step corresponds to the true residual. Additionally, \(^{0:T}_{0}\) implicitly depends on the downscaler parameters \(\), allowing us to simultaneously optimize all model parameters \(=(,)\) within the context of diffusion modeling.

As is standard in diffusion models , we parameterize the reverse process via a Gaussian distribution with a mean determined by a neural network \(M_{}\),

\[p_{}(^{0:T}_{n-1}|^{0:T}_{n},)= (^{0:T}_{n-1}|M_{}(^{0:T}_{n},n, ),),\] (1)

where \(M_{}\) is a denoising network and \(\) is a hyperparameter for variance. The diffusion model directly accesses the context \(=(^{0:T},}^{0:T})\), and is implicitly conditioned on \(^{0:T}\) via concatenation of feature maps from the downscaler module. As in the downscaler, we bicubically upsample \(^{0:T}\) before channel-wise concatenation with \(}^{0:T}\) to match the dimensions when forming \(\).

#### 2.1.3 Loss Function

To train our model, we use the angular parametrization suggested by . Specifically, this results in the diffusion loss of the form

\[L(,)=_{^{0:T},^{0:T},n, }_{t=0}^{T}\|-M_{}(^{0:T}_{n},n, )\|^{2}\] (2)

where \((0,I)\), \(n\) is sampled uniformly from \(\{1,2,,N\}\), and the sequences \(^{0:T}\), \(^{0:T}\) are sampled from the training distribution. Here, \(=(^{0:T},}^{0:T})\) where \(}^{0:T}=_{}(^{0:T})\). The scalars \(_{n}^{2}=_{i=1}^{n}(1-_{i})\) and \(_{n}^{2}=1-_{n}^{2}\) are used to define \(_{n}-_{n}^{0:T}_{0}\). Training and inference are concurrent across multiple frames due to spatio-temporal attention. Alg. 1 and 2 demonstrate the training and sampling strategy under the angular parametrization. We use DDIM sampling  to generate frame residuals with fewer diffusion steps.

#### 2.1.4 Network Architecture

Both the downscaler and the conditional diffusion model employ a UNet backbone with similar architectures and key adaptations to the attention mechanism (see A.1). The downscaler takes the multi-channel input frames (\(^{0:T}\)), yielding an initial estimate (\(}^{0:T}\)). The diffusion UNet conditions on diffusion step \(n\) and concatenates feature maps from the downscaler with its own. The concatenated input to the diffusion UNet (\(^{0:T}\), \(^{0:T}\), and \(^{0:T}_{n}\)), along with the conditioning variables (diffusion step \(n\) and the feature maps from downscaler), yields the output \(\).

Computing full attention for temporal coherence across the entire video data cube is very expensive for processing long sequences or high-resolution inputs. To optimize efficiency, we decouple attention between spatial and temporal dimensions, use a linear variant of self-attention  for non-bottleneck layers (where the effective number of "tokens" for attention is relatively large), focus spatial attention on localized patches (instead of the entire feature map, which could be wasteful), and calculate per-channel temporal attention in large spatial dimensions (namely, the ultimate and penultimate expansion and contraction layers of UNet). These modifications dramatically reduce the time complexity and memory footprint of these transformer blocks.

## 3 Experiments

We conduct a comprehensive evaluation of our proposed method, SpatioTemporal Video Diffusion (STVD), against six contemporary state-of-the-art baselines. The first two baselines are image super-resolution models based on the Swin Vision Transformer (Swin-IR) and its residual diffusion variant (Swin-IR-Diff). The next two baselines are video super-resolution models grounded in vision transformer architecture (VRT)  and its recurrent variant (RVRT) . The latter incorporates guided deformable attention for clip alignment, enhancing its temporal modeling capabilities. We compare against another video-super-resolution baseline (PSRT)  which also relies on the transformer architecture but uses multi-frame attention groups. Finally, we compare against a video diffusion baseline (VDM) . Fig. 1 shows a global view of the input and predicted precipitation.

We perform ablation studies in three configurations. In the first two, we experiment with the input sequence length. While our proposed model uses a context length of 5 frames, we also conduct experiments with 3 frames and 1 frame (STVD-3 and STVD-1). Note that using a single context frame ablates for the temporal attention block as well. The third ablation (STVD-Single) involves removing the additional input channels (i.e. only providing the model with the low-resolution precipitation sequence) to assess their impact on performance metrics. In summary, our experiments demonstrate that our method outperforms all baselines across all metrics considered. Additionally, our ablation studies highlight the importance of temporal context and additional climate inputs.

DatasetOur dataset derives from an 11-member initial condition ensemble of 13-month simulations using a global atmosphere model, FV3GFS, run at 25 km resolution and forced by climatological sea surface temperatures and sea ice. The first month of each simulation is discarded to allow the simulations to spin up and meteorologically diverge, effectively providing 11 years of reference data (of which first 10 years are used for training and the last year for validation). FV3GFS, developed by the National Oceanic and Atmospheric Administration (NOAA), is a version of NOAA's operational global weather forecast model ([77; 10]).

Three-hourly average data were saved from this entire simulation, which used a 25 km horizontal "fine grid". We further coarsened the selected fields by a factor of 8 to create a 200 km horizontal "coarse grid", resulting in paired data (\(x_{t},y_{t}\)), where \(x_{t}\) is the coarse-grid global state and \(y_{t}\) is the corresponding fine-grid global state. Our goal is to apply video downscaling to the coarse-grid precipitation field to obtain temporally smooth fine-grid precipitation estimates that are statistically similar to the true data. This approach is attractive because many fine-grid precipitation features, such as cold fronts and tropical cyclones, are poorly resolved on the coarse grid but are temporally coherent across periods much longer than 3 hours. We use 12 coarse-grid input fields, including precipitation, topography, and horizontal vector wind at various levels. See A.2 for the list of included atmospheric variables. FV3GFS uses a cubed-sphere grid, where the surface of the globe is divided into six tiles, each of which is covered by an \(S S\) array of points. Our data fields reflect this structure with \(S=48\) for the 200 km coarse grid and \(S=384\) for the 25 km fine grid.

Figure 4: Tradeoff between mean square error and percentile error (see Sec. 3). Inference at Himalayan region (see Figs. 6 and 12).

The application presented here serves as a pilot for broader uses of our methodology. Fine-grid simulations are significantly more computationally expensive than coarse-grid simulations (an 8-fold reduction in grid spacing requires almost 1000x more computation), so a coarse-grid simulation with super-resolved details in desired regions could be highly cost-effective for many applications.

During training, our model randomly selects data from one of the six tiles. This strategy ensures that the model learns from the diverse spatial contexts and weather regimes that produce precipitation worldwide. Post-training, for localized analysis, we selectively sample super-resolved precipitation channels from regions with complex terrain, such as California (Fig. 6). These regions can systematically pattern the precipitation on fine scales. This analysis helps us to see how well the super-resolution can learn the time-mean spatial patterns (e.g. precipitation enhancement on the windward side of mountain ranges and lee rain shadows) in the fine-grid reference data.

Training and Testing DetailsWe downscale a sequence of precipitation frames from FV3GFS output by a factor of 8. Our approach (STVD) trains on 5 consecutive frames that are downscaled jointly. We optimize our model end-to-end with a single diffusion loss using Adam  with an initial learning rate of \(1 10^{-4}\), decaying to \(5 10^{-7}\) with cosine annealing during training, executed on an NVidia RTX A6000 GPU. The diffusion model is trained using v-parametrization , with a fixed diffusion depth (N = 1400). Random tiles extracted from the cube-sphere representation of Earth, with dimensions 384 in high-resolution and 48 in low-resolution, are used during training. We train for one million steps, requiring approximately 7 days on a single node (slightly less for ablations). We use a batch size of one, apply a logarithmic transformation to precipitation states, and normalize to the range \([-1,1]\). During testing, we employ DDIM sampling with 30 steps on an Exponential Moving Average (EMA) variant of our model (for full frame size), with a decay rate of 0.995.

Baseline ModelsWe compare our generative setup against several recent high-performing transformer-based video super-resolution models. These models are trained deterministically. The first, Video Restoration Transformer (VRT) , allows for parallel frame prediction and long-range temporal dependency modeling. The second, recurrent VRT (RVRT) , incorporates guided deformable attention for effective clip alignment, enhancing its temporal modeling capabilities. The third, PSRT , removes the alignment module and modifies the attention window. We also compare against the recent Video Diffusion Model (VDM)  which employs global quadratic attention.

To assess the benefits of multi-frame downscaling, we compare with Swin-IR , a popular image super-resolution model that harnesses Swin Transformer blocks. However, Swin-IR is trained in a supervised fashion. Thus, as a generative baseline, we compare to Swin-IR-Diff. This model generates a deterministic prediction using Swin-IR , followed by modeling a stochastic residual

    & CRPS & MSE & EMD & PE & SAE \\  & \((10^{-5})\) & \((10^{-8})\) & \((10^{-6})\) & \((10^{-3})\) & \((10^{-6})\) \\ 
**STVD (ours)** & **1.85** & **0.59** & **2.49** & **1.2** & **4.00** \\ SRT (60) & 2.15 & 0.66 & 4.21 & 3.8 & 6.24 \\ RVRT (16) & 3.55 & 1.73 & 4.33 & 3.6 & 7.39 \\ VRT (34) & 3.58 & 1.74 & 4.61 & 4.0 & 7.39 \\ Swin-IR (2017) & 2.29 & 1.94 & 6.38 & 4.4 & 7.70 \\ Swin-IR (21) & 2.21 & 0.73 & 12.70 & 6.4 & 8.84 \\ Swin-IR (35) & 2.36 & 2.29 & 17.40 & 23.40 & 18.9 \\   

Table 1: Quantitative comparison between our method and other competitive baselines. EMD represents the Earth-Mover Distance, PE denotes the 99.999th percentile error and SAE is the spatial-autocorrelation error. Overall, our proposed method (STVD) outperforms the baselines across all metrics. In our ablation study, the exclusion of additional side information (STVD-single) or decrement in context length (STVD-3 and STVD-1) appreciably degrades performance.

Figure 5: Distributions of the fine-grid three-hourly average precipitation, for all gridpoints around the globe. The Swin-IR baseline over-estimates large precipitation events, whereas all other baselines underestimate key extreme and VRT .

Figure 6: Distributions of the fine-grid three-hourly average precipitation, for all gridpoints around the globe. The Swin-IR baseline over-estimates large precipitation events, whereas all other baselines underestimate key extreme and VRT .

using diffusion. This baseline is inspired by concurrent work on single-image radar-reflectivity downscaling , where a UNet is used instead of Swin-IR. See A.2 for details.

Evaluation MetricsWe evaluate our model differently from standard vision tasks. In addition to the Mean Square Error (MSE), which measures the average squared difference between predicted and actual values but lacks full distributional information, we use several distribution-level metrics for a more meaningful comparison. One such metric is the Continuous Ranked Probability Score (CRPS) [8; 66], which assesses the discrepancy between the predicted cumulative distribution function and the observed data. We compute CRPS over 10 stochastic realizations of our predictions. Fig. 7 visualizes several of these samples.

Furthermore, given the distinctive light-tailed exponential distribution of the precipitation climate state, it is crucial to ensure that downscaling does not significantly alter the distribution of precipitation rates. This necessitates two additional metrics. First, we compute the Earth Mover (or 1-Wasserstein) Distance  to quantify the agreement between the target and predicted global precipitation distributions, which are strongly affected by high-resolution details. Second, we focus on tail events and extreme precipitation by considering the 99.999th percentile error (PE), providing a nuanced understanding of the model's performance on rare and extreme precipitation events.

To further assess the spatial fidelity of our downscaling approach, we use the Spatial Autocorrelation Error (SAE) . This metric calculates the mean absolute error between the spatial autocorrelation of the predictions and ground truth. Low SAE ensures that the spatial patterns and the fine structure in precipitation data are preserved during downscaling, which is critical for accurate climate modeling.

Qualitative and Quantitative AnalysisTab. 1 provides a quantitative evaluation comparing our method with state-of-the-art baselines and ablations. Our model (STVD) performs strongly across all metrics, outperforming all baselines. We highlight the distributional characteristics in Fig. 5. Swin-IR overestimates precipitation, while all other baselines underestimate it. This discrepancy is undesirable, as poor performance on rare and extreme precipitation events can negatively imapct disaster mitigation policies. In contrast, our method closely matches the precipitation distribution, as measured by PE and EMD.

Using only precipitation as an input (STVD-single) results in slightly worse performance across all metrics, indicating the predictive value of additional inputs. In contrast, our ablation model STVD-1, which lacks full sequence information, performs significantly worse, highlighting the importance of temporal attention in our approach (which decays as a function of time lag as shown in Fig. 14).

Figs. 3, 11 and 12 depict the performance of our model compared to other baselines on examples of a precipitation feature interacting with mountainous terrain. Our model generates high quality results

Figure 6: Precipitation over two regions (left: Himalayas; right: Northern California coast, same region as Fig. 3), averaged across a year, for our STVD model and the ground-truth. For each half, the topography of the region is shown in the corresponding top-left whereas the predicted annual average is shown in the corresponding bottom-right. Annually-averaged precipitation is an important indicator of water availability in a region. STVD successfully captures many details of the precipitation that are tied to local topography and are too fine to be resolved the coarse-grid data.

which preserves most patterns with a high degree of similarity. PSBT and RVRT produce slightly more diffuse precipitation features, while Swin-IR produces slightly more pixelated features.

Fig. 6 shows annually-averaged precipitation from the patches in Figs. 3 and 12. Accurately capturing the fine-grid structure of time-mean precipitation is crucial for assessing long-term water availability. Our method (which includes fine-grid topography as a training input) effectively replicates the ground truth. This includes the strength and narrow spatial structure of high precipitation bands along the Northern California coastal mountains and the Sierras. These features are not resolved by the coarse-grid inputs to the super-resolution. See A.3 for full-page high-resolution samples and spectral analysis. Fig. 13 reveals that the spectra for baselines decay more rapidly than for STVD.

Realism-Distortion TradeoffDistortion metrics such as MSE often conflict with perceptual quality, where reducing distortion typically degrades perceptual realism . In our context, this tradeoff translates to balancing MSE and PE. While MSE captures the average accuracy of predictions, PE represents the model's ability to reproduce extreme events, thereby serving as a proxy for realism. Realism in climate modeling refers to the accurate representation of extreme weather patterns, which are crucial for applications like flood forecasting and disaster mitigation. PE is a distributional criterion that effectively captures these tail events, offering a robust measure of realism. Fig. 4 illustrates this tradeoff, with darker colors corresponding to fewer STVD sampling steps. As the number of diffusion sampling steps increases, MSE tends to rise slightly, but PE decreases significantly. Depending on the application, this tradeoff may potentially be exploited by practitioners. Essentially, the conditional mean minimizes MSE, so any deviation from it increases MSE--even if the deviation appears more realistic. As for sampling steps, fewer steps correspond to larger time increments in the diffusion process. At one extreme, a single step predicts the conditional mean, minimizing MSE. Conversely, more sampling steps more accurately simulate diffusion, generating diverse, realistic samples that increase MSE while reducing PE.

## 4 Related Work

Diffusion ModelsDiffusion models [61; 20; 64; 43; 45; 40] are a class of generative models based on an iterative denoising process. Closely related to our work are diffusion models for video. Recent

Figure 7: A visualization of the stochastic samples predicted by STVD for a given coarse-grid data. The precipitation event is the same as depicted in Fig. 3. Additionally, we also plot a variance map over the set of these samples to analyze the stochasticity better. Red regions correspond to high variance whereas blue regions correspond to low variance. Model stochasticity seems to be meaningful since the variance is large where mean precipitation is large.

models  generate deterministic next-frame predictions autoregressively with additional residuals generated by a diffusion model, or generate videos directly in pixel space [18; 69; 21] or in a latent space [6; 5]. While some works on video diffusion [6; 21] employ video super-resolution as a step in the overall modeling process, our work focuses exclusively on the video super-resolution task, particularly within the context of precipitation downscaling.

Super-ResolutionWithin the computer vision community, the paradigm for single image super-resolution has shifted from classical approaches [4; 13] to deep learning based methods . Generative approaches, like cascaded diffusion [49; 55], SR3 , and DiffPIR  employ diffusion models for image super-resolution. However, these are unable to leverage temporal context. On the other hand, many approaches for video super-resolution have been proposed [9; 14; 22]. For a more comprehensive overview, see . Recent models of note include the transformer-based models P8RT  and VRT , as well as the recurrent variant RVRT , which focuses on parallel decoding and guided clip alignment. We emphasize that these state-of-the-art approaches are deterministic, where our approach is generative. This allows us to preventing mode averaging and to produce more realistic samples, which is particularly critical in the context of precipitation modeling.

Data-driven weather and climate modelingRecent years have seen advancements in data-driven climate and weather modeling [51; 42], with models like GraphCast , GenCast , and FourCastNet  providing forecasts that are competitive with meteorological methods while being significantly faster. Rather than replacing numerical forecasting methods, our approach seeks to augment their capabilities by downscaling coarse-grid predictions.

While downscaling for climate and weather has been approached using techniques based on domain knowledge [25; 24], we focus here on data-driven approaches.  draw inspiration from FRVSR , adopting an iterative approach that uses the high-resolution frame estimated in the previous step as input for subsequent iterations.  employed Fourier neural operators for downscaling at arbitrary resolutions.  generate physically consistent downscaled climate states, using a softmax layer to enforce conservation laws. These approaches, though, are deterministic and trained by minimizing the MSE, thus lacking the realism and uncertainty quantification provided by a generative approach.

In terms of generative approaches, concurrent work  employs diffusion models for downscaling climate states. The use of GANs has also been pervasive in downscaling and precipitation prediction [32; 47; 17; 50; 15; 70]. However, these GAN-based approaches inherit the mode collapse and training difficulties present in all GAN-based models . Here, we highlight that these approaches are applied at each frame and cannot incorporate temporal information as is done in our model.

Beyond downscaling,  demonstrate the diffusion model's efficacy in synthesizing full rain density from vorticity inputs. Additionally,  uses a diffusion model for downscaling solar irradiance. These models are also used for probabilistic weather forecasting and nowcasting [31; 33].

## 5 Conclusion

We propose a video super-resolution method for probabilistic precipitation downscaling. Our model, STVD, deterministically super-resolves a given low-resolution frame sequence and then stochastically models the residual details via diffusion. Our model effectively resolves how fine-grid precipitation features, generated as weather systems, interact with complex topography based on temporally coherent coarse-grid information. Our method outperforms several competitive baselines on a range of quantitative metrics. This is an important step towards designing effective statistical downscaling methods, providing highly localized information for planning against extreme weather events, such as floods or hurricanes in a warming climate, using tractable coarse-grid atmospheric models.

Limitations and Broader ImpactsA limitation of our approach is the necessity of paired low-resolution and high-resolution images for training. While this can be done once prior to training, designing methods that only require the low-resolution states is an interesting challenge. In terms of broader impacts, our approach could potentially have harmful consequences if adopted blindly to a new dataset, where distribution shift could cause the model performance to degrade, potentially leading to underestimation of extreme weather risks such as droughts or floods. To mitigate these risks, the model should be re-trained and rigorously evaluated on the dataset of interest.