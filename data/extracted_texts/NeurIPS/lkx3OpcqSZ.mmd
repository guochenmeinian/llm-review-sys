# Compressing Large Language Models using Low Rank and Low Precision Decomposition

Rajarshi Saha

Stanford University

&Naomi Sagan

Stanford University

&Varun Srivastava

Stanford University

Andrea J. Goldsmith

Princeton University

&Mert Pilanci

Stanford University

###### Abstract

The prohibitive sizes of Large Language Models (LLMs) today make it difficult to deploy them on memory-constrained edge devices. This work introduces CALDERA - a new post-training LLM compression algorithm that harnesses the inherent low-rank structure of a weight matrix \(\) by approximating it via a low-rank, low-precision decomposition as \(+\). Here, \(\) and \(\) are low rank factors, and the entries of \(\), \(\) and \(\) are quantized. The model is compressed by substituting each layer with its \(+\) decomposition, and the zero-shot performance of the compressed model is evaluated. Additionally, \(\) and \(\) are readily amenable to low-rank adaptation, consequently enhancing the zero-shot performance. CALDERA obtains this decomposition by formulating it as an optimization problem \(_{,,}\|(+- )^{}\|_{}^{2}\), where \(\) is the calibration data, and \(,,\) are constrained to be representable using low-precision formats. Theoretical upper bounds on the approximation error of CALDERA are established using a rank-constrained regression framework, and the tradeoff between compression ratio and model performance is studied by analyzing the impact of target rank and quantization bit budget. Results illustrate that compressing LlaMa-\(2\)\(7\)B/\(13\)B/\(70\)B and LlaMa-\(3\)\(8\)B models using CALDERA outperforms existing post-training LLM compression techniques in the regime of less than \(2.5\) bits per parameter. The implementation is available at: https://github.com/pilancilab/caldera.

## 1 Introduction

Large Language Models (LLMs) stand out due to their remarkable ability to generate human-like text, thereby supporting a diverse range of applications ranging from writing assistance to code generation. These models leverage vast datasets and significant computational resources to achieve their impressive functionality. The architecture of LLMs typically includes multiple layers, each with weight matrices essential for encoding various aspects of the training data - from simple syntactic patterns to complex semantic relationships. However, the substantial size of these trained models leads to high computational costs and considerable energy consumption during inference, which can be challenging for deployment in resource-constrained environments. As LLMs continue to expand in scale, compression techniques to reduce the memory and computational requirements of the models are becoming crucial to ensure their broad accessibility.

Due to the correlated nature of language syntax and semantics learned during training, often, the weight matrices of LLMs exhibit redundancy, which manifests as a low-rank structure. This redundancy suggests the potential for compression without substantial loss in performance. This work introduces CALDERA: Calibration **A**ware **L**ow-Precision **DE**composition with Low-**R**ank **A**daptation, which compresses LLMs by leveraging the approximate low rank structure inherent inthese weight matrices. Given a matrix \(^{n d}\), CALDERA approximates it as \(+\), where \(^{n d}\), \(^{n k}\) and \(^{k d}\). Here, the _left_ and _right_ low rank factors, respectively \(\) and \(\), are tall and wide matrices, and \(k\) is the target rank. Furthermore, the entries of \(\), \(\) and \(\) are represented using low-precision formats with \(_{}\), \(_{}\) and \(_{}\) bits per entry, respectively.

Since the singular value profile (aka spectrum) of the weight matrices of an LLM follow a decaying profile as shown in Fig. 2, the low-rank factors \(\) and \(\) capture the effect of the large singular components of \(\) with high fidelity. Moreover, the backbone \(\), which is quantized aggressively - for instance, using \(_{}=2\) bits, coarsely captures the essence of the moderately decaying and low singular components of \(\). CALDERA substitutes each weight matrix \(\) in an LLM, with its approximate low-precision and low-rank decomposition \(+\), resulting in a post-training quantization strategy that delivers state-of-the-art zero-shot performance. In addition, since usually \(k\{n,d\}\), implying that the total number of parameters in \(\) is much smaller compared to the number of entries in \(\) (i.e., \(k(n+d) nd\)), CALDERA can readily fine-tune (or _"adapt"_) the low rank factors \(\) and \(\) in order to boost the zero-shot results.

### Significance and Related Works

Recent efforts have explored various avenues for compression, including but not limited to weight pruning, quantization, and the use of parameter-efficient training methods - each approach offering distinct advantages and tradeoffs. This section briefly reviews the current methodologies, highlighting the contributions and limitations of some studies closely related to this work.

**LLM Compression and Outlier Mitigation**: Recent studies like SmoothQuant , OPTQ , QuIP , AQLM , and QuIP#  consider the challenging regime of sub-\(4\) bit post-training LLM quantization. These works collectively emphasize the need to manage the impact of outliers, i.e., weights with unusually high magnitudes. Accommodating outliers necessitates choosing the dynamic range (or scale) of a quantizer to be high, consequently increasing the quantization error. QuIP equalizes (and reduces) the weight matrices by using a randomized matrix transform, and subsequently, QuIP# employs E8 lattice to make the weights more amenable to vector quantization. Both QuIP and QuIP# use a refined variant of the column-wise quantization method proposed in OPTQ, wherein error feedback from previously quantized columns of a matrix is used to compensate for the error incurred while quantizing subsequent columns. CALDERA utilizes this diverse arsenal of strategies and builds on top of QuIP#, while capitalizing on the approximate low-rank structure of LLM weight matrices. While it is possible to obtain even more aggressively compressed LLMs , this approach requires training from scratch, which is computationally demanding.

**Parameter Efficient Fine-Tuning (PEFT)**: In a related yet distinct vein of work, PEFT methods have gained significant momentum, aiming to adapt LLMs to specific tasks without extensive computational overhead. Recent studies such as QLoRA , LoftQ , and LQ-LoRA  have explored the intersection of PEFT and quantization, demonstrating that fine-tuning through low-rank updates, as originally proposed in LoRA , can mitigate the performance losses due to quantization. Given that CALDERA yields a decomposition \(+\), the low-rank components are particularly suitable for fine-tuning with any existing PEFT methods, thereby enhancing the zero-shot capabilities.

**Low Rank Approximation**: The rank-\(k\) approximation of a matrix \(^{n d}\) can be represented by the factorization \(\), with \(^{n k}\) and \(^{k d}\), where \(k\{n,d\}\). Known as the Burer-Monteiro factorization, this method substantially decreases the number of parameters,thus reducing computational demands. Recent studies such as LoRD , ASVD , FWSVD , LASER , LQER , and ZeroQuant-V2  have explored the efficacy of low-rank structures in LLM weights, treating low-rank factorization and quantization independently. In contrast, LPLR  approaches this by uniquely formulating a joint optimization problem for generic matrices, while simultaneously leveraging the equalization property of randomized transforms, as in . CALDERA formally leverages this inherent low-rank structure for LLM compression alongside existing frameworks such as QuIP#  and LoftQ , providing additional flexibility for compression. Furthermore, rigorous theoretical guarantees are derived using a rank-constrained regression framework for obtaining a low precision and low-rank decomposition, thereby also analytically demonstrating its superiority over rank-agnostic strategies.

## 2 Problem Formulation

In a neural network layer, a weight matrix \(\) transforms an input activation \(\) into an output activation given by \(\). This transformation can be succinctly described using the matrix's singular value decomposition (SVD). For any matrix \(^{n d}\), the SVD is \(=_{i}_{i}_{i}_{i}\), where \(_{i},_{i},_{i}\) are the \(i^{}\) singular value and the corresponding left and right singular vectors, respectively. The impact of each singular component \(_{i}_{i}\) on the matrix's transformation is determined by the magnitude of \(_{i}\). Given that weight matrices exhibit a decaying singular value profile (Fig. 1), indicating an approximate low-rank structure, lesser contributing singular components can be pruned with minimal impact on the functionality of the matrix, ensuring minimal distortion in the output activations.

CALDERA approximates the weight matrix of a neural network, \(\), as a low-precision, low-rank decomposition, \(+\), with all components \(,,\) in low-precision format. Unlike previous works such as [13; 22; 44; 45], which represent the low-rank factors \(\) and \(\) in high-precision (16 or 32-bit floating point), this work extends their representation to low-precision. This further reduces the memory footprint while preserving performance. Alternatively, for the same memory footprint, it allows the target rank \(k\) to be higher, thereby capturing the low rank structure with higher fidelity by including more of the higher singular value components. The following paragraph formalizes this as a constrained optimization problem.

For a given quantizer, let \(\) denote the set of discrete quantization points in \(\). For \(\)-bit quantization, the cardinality of \(\) satisfies \(_{2}[]\). Consider a matrix \(^{n d}\). The goal of this work is to obtain an decomposition \(+\) by approximately solving the minimization problem

\[_{,,}\|(+-)^{}\|_{}^{2}_{}^{n d},\; _{}^{n k},\;\;\;_ {}^{k d}.\] (1)

Here, \(_{}\), \(_{}\) and \(_{}\) denote the lattice codebooks used to quantize \(\), \(\) and \(\), using \(_{}\), \(_{}\) and \(_{}\) bits, respectively. Furthermore, \(^{m d}\) is a calibration matrix that aims to preserve the Frobenius norm error of the compressed layer output activations. If \(\) is the first layer's weight matrix, \(\) includes input embeddings from a calibration dataset, such as a subset of RedPajama , with the \(i^{}\) row representing the \(i^{}\) datapoint. For intermediate layers, \(\) contains the input activations, which are the output activations of the preceding layer.

Using the Frobenius norm of the output of a layer as a proxy objective for quantizing the weight matrices of an LLM is a popular strategy, and was used in prior work of Nagel et al. . This proxy objective function is particularly useful for post-training quantization of LLMs because their large size makes it difficult to apply sophisticated compression methods.

## 3 Proposed Algorithm: Calibration-Aware Low-Precision Decomposition with Low Rank Adaptation

This section introduces CALDERA to approximately solve (1) and get a \(+\) decomposition of a weight matrix \(\) using the calibration matrix \(\). The pseudocode is provided in Alg. 1. It consists of a nested loop for alternately optimizing the variables \(,\) and \(\). Suppose \(_{}\), \(_{}\) and \(_{}\), respectively, denote quantizers used for quantizing \(\), \(\) and \(\). For instance, they can refer to uniformly dithered scalar quantizers, as described in App. G.2. Initially, the low-rank factors are set to \(\), and \(\) is quantized using the LDLQ quantizer proposed in [3; SS3.1]. LDLQ is an adaptive quantization method that iteratively quantizes  each column of \(\) using \(_{}\) to get \(\) as

\[^{(k)}=_{}(^{(k)}+(^{(1:k-1)}- ^{(1:k-1)})_{k}),\] (2)

where \(^{(k)},^{(k)}\) denote the \(k^{}\) column, \(^{(1:k-1)}\) denotes the first \(k\) columns, \(_{}\) has a bit-budget \(_{}\), and \(_{k}^{k-1}\) is a learnable sequence of vectors. Update Eq. (2) incorporates linear feedback from already quantized columns, it can be seen that \(\) satisfies \(=_{}(+(-) )\), where the feedback matrix \(\) is a strictly upper triangular matrix with columns \(_{k}\). Defining \(^{}\) to be the (scaled) Hessian of the least squares objective in (1),  show that the optimal feedback matrix is the \(\) obtained from the LDL decomposition of \(m\), given by \(m=(+)(+)^{}\).

Subsequently, \(\) is fixed and the Low-Precision Low-Rank (LPLR) factorization of the residual, \((-)\), is computed. This is done by the \(\) submodule (Alg. 2), which is a refined version of the LPLR algorithm proposed in . For a given matrix \(\), Alg. 2 minimizes

\[_{,}\|(-) ^{}\|_{}^{2} _{}^{n k},_{}^{k  d},\] (3)

where \(_{}\) and \(_{}\) use \(_{}\) and \(_{}\) bits, respectively. In contrast to , the objective in (3) is calibration-data aware. Therefore, the update equations are derived using a rank-constrained regression framework, as described in App. B. Moreover, lines \(7\) to \(14\) in \(\) iteratively refine the estimates of \(\) and \(\), and can only yield a smaller Frobenius norm error. The left and right low-rank factor update equations are described as follows.

**Initialization**: In the absence of quantization constraints, a globally optimal solution to the optimization problem (3) can be found as described later in lemma 4.2. Consequently, the low-rank factors are initialized using rank-constrained regression in lines \(2-4\). Since subsequent quantization disrupts optimality of the solution, the factors are iteratively updated to minimize this distortion.

**Updating**\(\): To update the left factor \(\), lines \(5\) and \(9\) of Alg. 2 solves \(_{}\|(-)^{}\|_{ }^{2}\). For a fixed \(\), this is a least squares minimization, whose solution is available is closed form as \(}=(^{})( ^{})^{}=^{}( ^{})^{-1}\), as derived in App. C.1.

**Updating**\(\): Line \(8\) of Alg. 2, updates the right factor \(\) by keeping \(\) fixed and solving \(_{}\|(-)^{}\|_{ }^{2}\). As this is an under-determined linear system, there exist multiple solutions for \(\), all attaining the same objective function value. It is shown in App. C.1 that \(}=^{}^{}\) is a solution. The corresponding error is also obtained, which is used in the derivation of Thm. 4.1.

**Computational Complexity**: A high-level calculation is provided here, and detailed discussions can be found in App. D. It is worthwhile to note that the closed form expressions of \(}\) and \(}\), which are iteratively quantized, are functions of the Hessian \(=^{}\). Therefore, \(\) can be computed offline initially, per LLM, by doing a single forward pass, and subsequently used for all model quantization experiments. For each layer, this pre-processing includes computing \(\) and its LDL decomposition, along with computing \(^{}\), requiring a total of \((md^{2}+2d^{3})\) multiplications. Each outer iteration involves an LDLQ quantization. Quantizing the \(k^{}\) column has complexity \((nk)\), since feedback from \(k\) already quantized columns need to be incorporated. Hence, quantizing a matrix in \(^{n d}\) entails \((n^{2}+3n)\) complexity. Moreover, \(\) requires \((m^{2}(n+d))\) to initialize, and subsequently, each inner iteration entails \((ndk)\). Assuming \(n,d m k\), and keeping only the dominant terms, the total complexity of CALDERA, not including the complexity of the pre-processing discussed earlier, is \((_{}(n^{2}+m^{2}(n+d)+ndk\ _{ }))\).

**Fine tuning via Low-Rank Adaptation**: Once the weight matrices of each layer are replaced by its \(+\) approximation, the zero-shot performance of (post-training) quantized model can be evaluated. \(@sectionsign\)5 shows that \(\) quantized models outperform existing strategies. Additionally, if desired, the low-rank factors \(\) and \(\) can be further fine-tuned using low-rank adaptation  on a small task-specific dataset. While the initialization of the fine-tuning step has quantized \(,\) and \(\), the fine-tuned factors are represented using \(16\)-bits (\(16\) format). Although this leads to a slight increase in the memory footprint, the performance gains from fine-tuning are substantial.

## 4 Approximation Error Analysis

The approximation error upper bounds are derived via a rank-constrained regression framework. Thm. 4.1 below (formally stated and proved in App. C.4) is an informal version of the main theoretical result of this paper, and provides an upper bound on the Frobenius norm error when \(\)approximates a weight matrix \(\) is as \(+\) by solving the optimization problem (1) using Alg. 1. For convenience of analysis, it is assumed that the dynamic range of \(_{}\), denoted as \(\), is chosen to be high enough, ensuring it remains unsaturated. Consequently, for a scalar input the quantization error from \(_{}\) has zero mean and bounded variance, given by \(}{4}=^{2}}{(2^{_{}}-1)^{2}}\).

**Theorem 4.1**.: **Approximation error of caldera (Informal)** _Given \(^{n d}\) and \(^{m d}\) with \(m d\), let \(\) be obtained from the LDL decomposition \(^{}=m=(+)( +)^{}\), and \(_{}\), \(_{}\) denote the max and min eigenvalues of \(\). Additionally, let \((,_{})\), where \(_{}\) has dynamic range \(\) and bit-budget \(_{}\), the quantization error be \(_{}(+(- ))-(+(-))\), and \(_{1}_{k}\) be the singular values of \((-)^{}\). If the target rank \(k\) satisfies \(0.25_{}^{1/2}(m_{1})^{-1}_{}^{-3/2}_{i>k} _{i}^{2} k m\), and the dynamic ranges of \(_{}\) and \(_{}\) are set as \(_{}=}{_{k}}}\) and \(_{}=_{1}\), then \(,\) and \(\) returned by Alg. 1 satisfy_

\[\ \|(+-) ^{}\|_{}^{2}_{i>k} _{i}(^{})+ ^{2}}{(2^{_{ }}-1)^{2}}(1-)^{2}+,\]

_while utilizing an average budget of \(_{2}(^{3}}{m_{k}}}{_{}})\) bits per parameter for the low-rank factors \(\) and \(\), when \(n d\). Here, the expectation is over the stochasticity of the quantizers._

An informal version of the main result is provided here, and the formal version including specific constant values, along with the derivation, can be found in App. C.4. The requirement \(m d\) is not restrictive, because when \(\) is positive definite, (1) can be rewritten as \(\|(+-)^{}\|_{ }^{2}=\|(+-)^{1/2}\|_{ }^{2}\), ensuring \(m=d\). This is detailed further in App. C.5. The approximation error upper bound given by Thm. 4.1 can be directly compared with the result of Chee et al. [3, Thm. 1], which states that for vanilla LDLQ without \(\),

\[\|(-)^{}\|_{ }^{2}[( ^{})]=_{i=1}^{n}_{i}( ^{}).\] (4)

Evidently, Alg. 1 yields a smaller error provided, \(_{i>k}_{i}(^ {})<_{i=1}^{k}_{i}( ^{})-\), where \(\) can be chosen to be arbitrarily small. Furthermore, since the expression in Thm. 4.1 consists of two terms, namely, the rank-constrained regression error, which depends on the target rank \(k\), and the additive quantization error of \(\), which is dictated by the bit-budgets used for \(\) and \(\), this upper bound can be made arbitrarily small by ensuring that the two terms are approximately equal, i.e., \(\|(+-)^{}\|_{ }^{2}\) is upper bounded by \(2\). This is apparent in the following regimes:

1. [label=()]
2. \(k n\): In this regime, \(k\) is treated as a constant as \(n\) grows. Then, if the bit-budget satisfies \(_{}_{2}(2()^{-1/2} }+1),\|( +-)^{}\|_{}^{2} 2\).
3. \(k=(n)\): For a fixed \(_{}\), if \(k\) is allowed to grow with dimension \(n\), then choosing \(k\) to satisfy \(k 2n-(2^{_{}}-1)^{-1}()^{1/2}(md _{}^{-1/2})\|( +-)^{}\|_{}^{2} 2\).

This implies that the upper bound can be made arbitrarily small by either \(\) increasing the bit-budget of the backbone, i.e., \(_{}\), for a fixed rank \(k\), or \(\) increasing the rank \(k\) for a fixed \(_{}\), for example, \(_{}=2\). Alternatively stated, this provides a tunable knob for controlling the error by trading off the allocated bit-budget between the backbone \(\) and the low-rank factors \(,\).

### Analysis Outline

In this section, a brief proof sketch is presented, highlighting the major challenges in the proof and how they are addressed. For analysis, \(\) is assumed to be updated prior to \(,\) in Alg. 1. However, in practice, the update order is inconsequential, and can be swapped, depending on whichever yields a smaller error. The complete derivation of the approximation error is provided in App. C. A key ingredient of the proof is the solution of the rank-constrained regression problem, which is defined as,

\[_{() k}\|- \|_{}^{2}.\] (5)

Although this problem is non-convex, it can be solved to global optimality via two SVDs . The following lemma characterizes the solution to the optimization problem in (5).

``` Input: Matrix: \(^{n d}\), Target rank: \(k\), Calibration matrix: \(^{m d}\), Iterations: \(_{}\), Quantizers: \(_{}\), \(_{}\) Output: Low precision Low Rank factors: \(^{n k}\), \(^{k d}\) s.t. \(^{}^{}\)
1Initialize: Iteration counter: \(i 0\)
2 Compute SVD of \(\) as \(}^{}\).
3 Compute SVD of \(^{}^{}\) as \(}^{}\).
4 Get right low-rank factor: \(_{0}_{}(_{k}^{}}^{})\)
5 Get left low-rank factor: \(_{0}_{}(}_{0})\), where \(}_{0}=_{^{k d}}\|( _{0}-)^{}\|_{}^{2}\)
6\(_{}_{0},_{}_{0}\), \(\|(_{0}_{0}-) ^{}\|_{}^{2}\).
7while\(i<_{}\)do
8 Update right: \(_{i+1}_{}(}_{i+1})\), where \(}_{i+1}=_{^{k d}}\|( _{i}-)\,^{}\|_{}^{2}\)
9 Update left: \(_{i+1}_{}(}_{i+1})\), where \(}_{i+1}=_{^{n k}}\|( _{i}-)\,^{}\|_{}^{2}\)
10if\(\|(_{i+1}_{i+1}-)^{}\|_{ }^{2}<\)then
11\(_{}_{i+1}\), \(_{}_{i+1}\), \(\|(_{i+1}_{i+1}-) ^{}\|_{}^{2}\)
12 end if
13\(i i+1\)
14 end while return\(_{},_{}\) ```

**Algorithm 2**\((,k,,_{},_{ },_{})\): LPLR factorization submodule

**Lemma 4.2**.: _Given \(^{m n}\), and full rank \(^{m d}\), where \(m d\). Let \(=}^{}\) and \(^{}=}}^{}\) denote full SVDs of \(\) and \(^{}\). Then, for \(k m\), the solution of (5) is given by_

\[_{*}*{arg\;min}_{()  k}\|-\|_{}^{2}=( _{m}^{-1}}_{k} )(_{k}^{}}}^{} ),\]_where \(:=}_{m}^{m m}\) is a diagonal matrix consisting of the non-zero singular values of \(\). Moreover, denoting the non-zero singular values of \(\) as \(\{_{i}()\}_{i=1}^{m}\), the optimal value of (7) is_

\[_{() k}\|- \|_{}^{2}=\|_{*}-\|_ {}^{2}=_{i=k+1}^{m}_{i}^{2}().\] (6)

The complete lemma (with the case \(m>d\)), and the derivation, are provided in App. B. Using lemma 4.2, the approximation error of LPLRFactorize is analyzed in App. C.3. Specifically, lemma C.3 shows that for any input matrix \(\), Alg. 2 with suitably chosen \(_{}\) and \(_{}\), ensures that \(\|(-)^{}\|_ {}^{2}\), as in (3), can be upper bounded by twice the sum of squared trailing singular values, (ref. (6)). While proving lemma C.3, it is assumed that if \(_{}\) or \(_{}\) gets saturated, a trivial output of \(=,=\) is returned. Therefore, lemmas C.1 and C.2 specify choosing the dynamic ranges \(_{}\) and \(_{}\) to be sufficiently high so that saturation happens with a very low probability. The proof of Thm. 4.1 is completed by using the LDL decomposition of \(m\) as proposed in , along with an application of Marchenko-Pastur approximation to bound the expected eigenvalues of the quantization error, i.e., \(_{i}(^{})\), yielding the final inequality._

## 5 Numerical Simulations

The efficacy of CALDERA is assessed by using it to compress four popular open source LLMs from Meta AI, namely, LLaMa-2 7B, LLaMa-2 13B, LLaMa-2 70B  and LLaMa-3 8B . The framework is built in PyTorch on top of the QuIP#  and LoftQ , and is available at https://github.com/pilancilab/caldera.

**Baselines**. The full-rank matrix \(\), also referred to as the backbone, is quantized to 2-bits using the LDLQ procedure from QuIP , employing an E8 lattice quantizer . For CALDERA, which allows even the low-rank factors, \(\) and \(\), to be represented in low-precision, the quantization is also performed with an E8 lattice. Prior to running Alg. 1, a randomized Hadamard transform (RHT) is applied to the left and the right of the input weight matrix, as the incoherence pre-processing step, to equalize the magnitude of the entries making them more robust to quantization. In other words, CALDERA decomposition is performed on \(}_{}^{} _{}\), where \(_{}\) and \(_{}\) are Hadamard matrices, right-multiplied by a diagonal matrix with i.i.id. \(\{ 1\}\) entries. In addition, the Hessian matrix obtained from the calibration data is substituted by \(}_{}^{} _{}\). As described in , this improves the quantization error incurred by LDLQ. Further details are provided in App. E.2.

**Metrics**. The performance of CALDERA is evaluated using perplexity on the test splits of the Wikitext2  and C4  datasets, as well as task-specific goodness-of-fit metrics such as zero-shot accuracy for sequence classification. Specifically, zero-shot accuracy was measured on the Winogrande , RTE , PiQA , ARC-Easy, and ARC-Challenge  tasks. App. E.3 provides more details regarding these benchmarks. Perplexity was measured using a sequence length equal to the model's maximum context length, i.e., 4096 for LLaMa-2, and 8192 for LLaMa-3. Zero-shot experiments were performed using EleutherAI's Language Model Evaluation Harness .

### Zero-shot Results

Tables 1 and 2 report the perplexities and accuracies for CALDERA with varying target rank \((k)\) of \(\) and \(\). A smaller value is better for perplexity, which is defined as the \(()\) of the training objective, while zero-shot accuracies are reported as percentages. Per-parameter bit budgets range from \(2.1\) (e.g., rank-\(64\) factors in \(4\)-bit precision) to \(2.4\) bits (e.g., rank-\(64\) factors in half precision or rank-\(256\) factors in 4-bit precision). For comparison, the \(+\) decomposition of weight matrices found in the QuIP# codebase was performed on each model. For the sake of direct comparison, fine-tuning of the diagonal matrices in RHT was omitted. As QuIP# does not support quantized factors, \(\) and \(\) are rank-64 in order to ensure that the per-parameter bit-budget remains in the \(2-2.4\) range. As another baseline comparison, each model is quantized using QuIP# without any low-rank factors. Results for the unquantized models are also provided.

For all models, the rank-\(256\) CALDERA decomposition with \(4\)-bit factors had the lowest perplexity and generally had the highest accuracies. As CALDERA supports quantizing low-rank factors with minimal performance loss, more singular components can be captured compared to using half-precision factors while employing the same number of bits. Consequently, the low-rank factorscan regain the performance that was compromised when the backbone \(\) was quantized to \(2\) bits. Since zero-shot experiments have some inherent randomness and low-rank regularization effects , the zero-shot accuracies reported here are not as directly indicative of quantization performance as the perplexity results. In addition, SS5.3, demonstrates that degradation in zero-shot accuracy can be recovered via LoRA fine-tuning. It is worthwhile to note these results substantiate the claims of , which report that low-bit quantization of LLaMa-\(3\)\(8\)B, significantly deteriorates model performance across various post-training quantization techniques, more so than with the LLaMa-\(2\) series.

### Fine-tuning of Randomized Hadamard Transform (RHT) Parameters

As CALDERA presents a general optimization framework for matrix decompositions of the form \(+\), it can easily be extended with additional heuristics to improve performance. This section serves as a proof of concept, by examining one such heuristic: Fine-tuning of randomized Hadamard transform parameters. This technique, proposed in QuIP# , involves fine-tuning the diagonal Rademacher matrices with \( 1\) entries in the RHT to minimize the cross-entropy loss between the output of the original and quantized models on the calibration dataset. Subsequently, RHT fine-tuning is performed on the models quantized using CALDERA in SS5.1.1 Details on specific fine-tuning hyperparameters can be found in App. E.4.

Perplexity and zero-shot results in Tables 3 and 4 match the trends in SS5.1, i.e., CALDERA with rank-\(256\) factors typically performs best, with the exception of RTE. In addition, perplexities are substantially lower than without the fine-tuning of randomized Hadamard transform parameters.

   Method & Rank & \((=B_{R})}\) & Avg Bits & Wiki\(2\) & \(\) & Wino \(\) & RTE \(\) & PiQA \(\) & ArcE \(\) & ArcC \(\) \\  CALDERA (7B) & 64 & 16 & 2.4 & 7.36 & 9.47 & 64.6 & 66.4 & 73.7 & 60.8 & 31.7 \\ CALDERA (7B) & 64 & 4 & 2.1 & 7.37 & 9.74 & 63.7 & 62.1 & 72.3 & 60.9 & 31.7 \\ CALDERA (7B) & 128 & 4 & 2.2 & 6.76 & 8.83 & 63.8 & 59.9 & 75.1 & **65.1** & **34.6** \\ CALDERA (7B) & 256 & 4 & 2.4 & **6.19** & **8.14** & **66.0** & 60.6 & **75.6** & 63.6 & 34.0 \\ QuIP\# (7B, No FT) & 64 & 16 & 2.4 & 7.73 & 10.0 & 63.1 & **66.8** & 71.7 & 63.2 & 31.7 \\ QuIP\# (7B, No FT) & 0 & — & 2 & 8.23 & 10.8 & 61.7 & 57.8 & 69.6 & 61.2 & 29.9 \\  CALDERA (13B) & 64 & 4 & 2.08 & 6.04 & 7.98 & 66.9 & 61.0 & 76.0 & 69.5 & 37.2 \\ CALDERA (13B) & 128 & 4 & 2.16 & 5.72 & 7.66 & **67.9** & 58.5 & 76.0 & 68.5 & 38.7 \\ CALDERA (13B) & 256 & 4 & 2.32 & **5.41** & **7.21** & 66.9 & 62.1 & **76.2** & **70.3** & **40.4** \\ QuIP\# (13B, No FT) & 0 & — & 2 & 6.06 & 8.07 & 63.6 & 54.5 & 74.2 & 68.7 & 36.2 \\  CALDERA (70B) & 128 & 4 & 2.1 & 4.11 & 5.95 & 75.5 & 69.3 & **79.8** & 76.9 & 47.7 \\ CALDERA (70B) & 256 & 4 & 2.2 & **3.98** & **5.76** & **77.6** & **71.5** & **79.8** & **79.5** & 47.4 \\ QuIP\# (70B, No FT) & 0 & — & 2 & 4.16 & 6.01 & 74.2 & 70.0 & 78.8 & 77.9 & **48.6** \\  Unquantized (7B) & 0 & — & 16 & 5.12 & 6.63 & 67.3 & 63.2 & 78.5 & 69.3 & 40.0 \\ Unquantized (13B) & 0 & — & 16 & 4.57 & 6.05 & 69.5 & 61.7 & 78.8 & 73.2 & 45.6 \\ Unquantized (70B) & 0 & — & 16 & 3.12 & 4.97 & 77.0 & 67.9 & 81.1 & 77.7 & 51.1 \\   

Table 1: Zero-shot perplexities (denoted by \(\)) and accuracies (\(\)) for LLaMa-\(2\). \(}=2\) bits throughout.

   Method & Rank & \((=B_{R})}\) & Avg Bits & Wiki\(2\) & \(\) & Wino \(\) & RTE \(\) & PiQA \(\) & ArcE \(\) & ArcC \(\) \\  CALDERA & 64 & 16 & 2.4 & 9.22 & 10.5 & 68.9 & 63.9 & 72.9 & 69.9 & 36.5 \\ CALDERA & 64 & 4 & 2.1 & 10.6 & 11.8 & 66.9 & 58.5 & 71.8 & 68.2 & 34.3 \\ CALDERA & 128 & 4 & 2.2 & 9.21 & 10.5 & 67.6 & **69.7** & 74.4 & 71.8 & 36.3 \\ CALDERA & 256 & 4 & 2.4 & **8.22** & **9.56** & **69.7** & 65.0 & **75.1** & **73.2** & **40.0** \\  QuIP\# (No FT) & 64 & 16 & 2.4 & 10.9 & 11.8 & 66.5 & 57.0 & 69.6 & 63.8 & 31.0 \\ QuIP\# (No FT) & 0 & — & 2 & 13.8 & 15.6 & 63.2 & 52.7 & 67.6 & 57.6 & 28.2 \\  Unquantized & 0 & — & 16 & 5.54 & 7.01 & 73.5 & 68.6 & 79.7 & 80.1 & 50.2 \\   

Table 2: Zero-shot perplexities (denoted by \(\)) and accuracies (\(\)) for LLaMa-\(3\)\(8\)B. \(}=2\) bits throughout.

### Low Rank Adaptation (LoRA) Fine-tuning Results

In addition to RHT FT as described above, once the \(+\) decomposition with target rank \(k\) is obtained, and \(k\) takes values \(64,128\) and \(256\), fine-tuning the top \(r\) (\( k\)) singular components on a specific downstream datasets can recover the performance lost due to quantization. We consider three such tasks - (i) language modeling on Wikitext (Wiki2), (ii) recognizing textual entailment (RTE), and (iii) commonsense reasoning (WinoGrande). Throughout all experiments in Table 5, \(r=64\) is chosen and those singular components are fine-tuned to \(16\)-bit precision, i.e., \(\) format. The tasks (ii) and (iii) are sequence classification tasks, and the pre-trained LLaMa model is augmented with a linear classification head, which is fine-tuned along with the low-rank factors . In other words, the approximation is written as \(+_{1}_{1}+_{2} _{2}\), where \(_{1}^{n r}\), \(_{2}^{n(k-r)}\), \(_{1}^{r d}\), \(_{2}^{(k-r) d}\), \(=[_{1}_{2}]\), \(^{}=[_{1}^{}_{2}^{}]\). The value of \(r\) is set to \(64\) and \(_{2},_{2}\) are fined-tuned to \(_{},_{}\) using low-rank adaptation similar to [13; 16; 22]. Doing this significantly on a small task-specific dataset like WikiText2, RTE, or Winogrande, can noticeably boost the zero-shot accuracy, as can be seen from Table 5.2

Experimental details can be found in App. E.4. For each dataset, ten checkpoints are saved during the course of fine-tuning, and the best test performance is reported in Table 5. For datasets where test labels are not available, evaluation performance is reported instead.

For comparison, results from the LoftQ  and LQ-LoRA  papers are also reported, where available. As these papers were published before the release of LLaMa-3, only LLaMa-2 results are available.3 In each case, CALDERA achieves better performance at a lower bit budget.

### Autoregressive Generation Throughput

The low-rank \(()\) component in CALDERA can recover some of the accuracy lost due to the aggressive \(2\)-bit quantization of \(\). However, CALDERA also needs to dequantize and multiply the low-rank factors, which results in a slight (albeit, acceptable) throughput degradation compared to QuIP# (shown in Table 6). Nevertheless, CALDERA's throughput is significantly higher than that of the unquantized model. This is because compressing weight matrices results in a smaller volume of data transfer from and to the GPU's SRAM, speeding up forward passes. It is worthwhile to note that

   Method & Rank & \(_{}(=_{})\) & Avg Bits & Wiki2 \(\) & C4 \(\) & Wino \(\) & RTE \(\) & PiQA \(\) & ArcE \(\) & ArcC \(\) \\  CALDERA & 64 & 16 & 2.4 & 7.63 & 8.9 & **70.3** & **70.8** & 75.4 & **72.4** & 39.0 \\ CALDERA & 64 & 4 & 2.1 & 8.06 & 9.34 & 69.5 & 64.3 & 76.0 & 71.5 & 40.0 \\ CALDERA & 128 & 4 & 2.2 & 7.76 & 9.02 & 69.4 & 63.9 & 76.0 & **73.7** & 41.8 \\ CALDERA & 256 & 4 & 2.4 & **7.34** & **8.68** & **70.3** & 70.4 & **76.5** & **73.6** & **42.3** \\  QuIP\({}^{*}\) & 64 & 16 & 2.4 & 7.92 & 9.15 & 68.4 & 58.1 & 74.9 & 72.3 & 40.4 \\ QuIP\({}^{*}\) & 0 & — & 2 & 8.44 & 9.75 & 67.5 & 57.8 & 72.9 & 67.6 & 37.3 \\   

Table 4: Zero-shot perplexities and accuracies for LLaMa-3 8B, with end-to-end fine-tuning of randomized Hadamard transform parameters. \(_{}=2\) bits throughout. \({}^{*}\)See Footnote 1.

   Method & Rank & \(_{}(=_{})\) & Avg Bits & Wiki2 \(\) & C4 \(\) & Wino \(\) & RTE \(\) & PiQA \(\) & ArcE \(\) & ArcC \(\) \\  CALDERA & 64 & 16 & 2.4 & 6.22 & 8.23 & 64.2 & 63.2 & 76.1 & 63.4 & 34.7 \\ CALDERA & 64 & 4 & 2.1 & 6.30 & 8.32 & 64.6 & 65.7 & 75.4 & 63.3 & 35.4 \\ CALDERA & 128 & 4 & 2.2 & 6.09 & 8.06 & 65.1 & 61.0 & **76.5** & **65.1** & 35.6 \\ CALDERA & 256 & 4 & 2.4 & **5.84** & **7.75** & **65.7** & 60.6 & **76.5** & 64.6 & **35.9** \\  QuIP\({}^{*}\) & 64 & 16 & 2.4 & 6.32 & 8.31 & 64.9 & **66.4** & 75.0 & **65.2** & 34.5 \\ QuIP\({}^{*}\) & 0 & — & 2 & 6.58 & 8.62 & 64.4 & 53.4 & 75.0 & 64.8 & 34.0 \\   

Table 3: Zero-shot perplexities and accuracies for LLaMa-2 7B, with end-to-end fine-tuning of randomized Hadamard transform parameters. \(_{}=2\) bits throughout. \({}^{*}\)See Footnote 1.

Llama-2 70B runs into out-of-memory (OOM) as an A10G GPU only has \(24\) GiB of VRAM, which is not enough for 70B parameters in FP16 format (which approximately requires 140 GiB).

Notably, the throughput is higher when the \(\) factors are in \(16\)-bit compared to when they are in \(4\)-bit. This is because CALDERA used QuIP#'s lattice dequantizers for the low-rank factors as well, adding to the compute overhead. Moreover, QuIP# also used fused kernels, and CALDERA's throughput can be improved by leveraging such optimizations. Since this work is primarily motivated with the goal of closing the gap with respect to uncompressed models in the \(2\) to \(2.5\) bits per parameter regime, throughput improvement using custom kernels, paged attention, etc., is left for future work. We discuss the broader impacts of our work along with limitations in App. H.

## 6 Conclusions

In this work, the problem of obtaining a low-precision and low-rank decomposition of an LLM weight matrix was considered. A \(+\) decomposition efficiently captures the high singular components of the weight matrix with sufficient fidelity, while coarsely compressing the less significant moderate-to-low singular components. An optimization-theoretically motivated algorithm was proposed to obtain this decomposition, which iteratively optimized the quantized backbone \(\) and the low-rank factors \(,\). Additionally, it was shown that \(\) and \(\) can be efficiently fine-tuned using low-rank adaptation to boost the zero-shot performance of the quantized model. By utilizing a rank-constrained regression framework, an upper bound was established on the approximation error of the algorithm, and it was shown that this upper bound can be significantly smaller than prior bounds in the literature. Finally, the proposed method was empirically evaluated by compressing the LLaMA family of LLMs in the challenging sub-\(2.5\) bits per parameter regime. The proposed approach can also be used to complement existing compression strategies; thereby making it efficient to distribute compressed LLMs and deploy them on regular consumer hardware, making them more accessible to researchers.

   Method & Rank & \((=B_{R})}\) & Throughput (tok/s) \\  Uncompressed (7B, FP16) & — & — & 31.75 \\  CALDERA (7B) & 64 & 16 & 61.68 \\ CALDERA (7B) & 64 & 4 & 46.29 \\ CALDERA (7B) & 128 & 4 & 46.19 \\ CALDERA (7B) & 256 & 4 & 45.89 \\ QuIP\# (7B) & 0 & — & 87.74 \\  Uncompressed (70B, FP16) & – & – & OOM \\  CALDERA(70B) & 128 & 4 & 5.33 \\ CALDERA (70B) & 256 & 4 & 4.66 \\ QuIP\# (70B) & – & – & 18.18 \\   

Table 6: Throughputs for meta-llama/Llama-2-{7,70}b-hf on an NVIDIA A10G GPU for a batch size and sequence length of \(1\) (\(}=2\) for all rows)