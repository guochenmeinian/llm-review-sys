# Rethinking the Backward Propagation for

Adversarial Transferability

 Xiaosen Wang\({}^{1}\), Kangheng Tong\({}^{2}\), Kun He\({}^{2}\)

\({}^{1}\)Huawei Singular Security Lab

\({}^{2}\)School of Computer Science and Technology, Huazhong University of Science and Technology

{xiaosen,tongkangheng,brooklet60}@hust.edu.cn

The first two authors contribute equally.Corresponding author.

###### Abstract

Transfer-based attacks generate adversarial examples on the surrogate model, which can mislead other black-box models without access, making it promising to attack real-world applications. Recently, several works have been proposed to boost adversarial transferability, in which the surrogate model is usually overlooked. In this work, we identify that non-linear layers (_e.g_. ReLU, max-pooling, _etc_.) truncate the gradient during backward propagation, making the gradient _w.r.t_. input image imprecise to the loss function. We hypothesize and empirically validate that such truncation undermines the transferability of adversarial examples. Based on these findings, we propose a novel method called Backward Propagation Attack (BPA) to increase the relevance between the gradient _w.r.t_. input image and loss function so as to generate adversarial examples with higher transferability. Specifically, BPA adopts a non-monotonic function as the derivative of ReLU and incorporates softmax with temperature to smooth the derivative of max-pooling, thereby mitigating the information loss during the backward propagation of gradients. Empirical results on the ImageNet dataset demonstrate that not only does our method substantially boost the adversarial transferability, but it is also general to existing transfer-based attacks. Code is available at https://github.com/Trustworthy-AI-Group/RPA.

## 1 Introduction

Deep Neural Networks (DNNs) have been widely applied in various domains, such as image recognition , object detection , face verification , _etc_. However, their susceptibility to adversarial examples , which are carefully crafted by adding imperceptible perturbations to natural examples, has raised significant concerns regarding their security. In recent years, the generation of adversarial examples, _aka_ adversarial attacks, has garnered increasing attention  in the research community. Notably, there has been a significant advancement in the efficiency and applicability of adversarial attacks , making them increasingly viable in real-world scenarios.

By exploiting the transferability of adversarial examples across different models , transfer-based attacks generate adversarial examples on the surrogate model to fool the target models . Unlike other types of attacks , transfer-based attacks do not require direct access to the victim models, making them particularly applicable for attacking online interfaces. Consequently, transfer-based attacks have emerged as a prominent branch of adversarial attacks. However, it is worth noting that the early white-box attacks  often exhibit poor transferability despite demonstrating superior performance within the white-box setting.

To this end, different techniques have been proposed to enhance adversarial transferability, such as momentum-based attacks [6; 27; 43; 46; 9; 59], input transformations [57; 7; 45; 30; 8; 48; 42], advanced objective functions [17; 54; 49], and model-related attacks [23; 53; 12; 50]. Among these techniques, model-related attacks are particularly valuable due to their ability to exploit the characteristics of surrogate models. Model-related attacks offer a unique perspective on adversarial attacks by leveraging the knowledge gained from surrogate models, which can also shed new light on the design of more robust models. In despite of their potential significance, model-related attacks have been somewhat overlooked compared to other types of transfer-based attacks.

Since transfer-based attacks mainly design various gradient ascend methods to generate adversarial examples on the surrogate model, in this work, we first revisit the backward propagation procedure. We find that non-linear layers (_e.g._, activation function, max-pooling, _etc._) often truncate the gradient of loss _w.r.t._ the feature map, which diminishes the relevance of the gradient between the loss and input image. We assume and empirically validate that such gradient truncation undermines the adversarial transferability. Based on this finding, we propose Backward Propagation Attack (BPA), which modifies the calculation for the derivative of ReLU activation function and max-pooling layers during the backward propagation process. With these modifications, BPA mitigates the negative impact of gradient truncation and improves the transferability of adversarial attacks.

Our main contribution can be summarized as follows:

* To our knowledge, this is the first work that proposes and empirically validates the detrimental effect of gradient truncation on adversarial transferability. This finding sheds new light on improving adversarial transferability and might provide new directions to boost the model robustness.
* We propose a model-related attack called BPA, that adopts a non-monotonic function as the derivative of the ReLU activation function and incorporates softmax with temperature to calculate the derivative of max-pooling. With these modifications, BPA mitigates the negative impact of gradient truncation and enhances the relevance of gradient between the loss function and the input.
* Extensive experiments on ImageNet dataset demonstrate that BPA could significantly boost various untargeted and targeted transfer-based attacks and outperform the baselines with a substantial margin, emphasizing the effectiveness and superiority of our proposed approach.

## 2 Related Work

In this section, we provide a brief overview of the existing adversarial attacks and defenses.

### Adversarial Attacks

Existing adversarial attacks can be categorized into two groups based on the access to target model, namely white-box attacks and black-box attacks. In the white-box setting [10; 33; 31; 2], attackers have complete access to the structure and parameters of the target model. In the black-box setting, the attacker has limited or no information about the target model, making it applicable in the physical world. Black-box attacks can be further grouped into three classes, _i.e._, score-based attacks [4; 19], query-based attacks [3; 22; 47], and transfer-based attacks [6; 57; 43]. Among the three types of black-box attacks, transfer-based attacks generate adversarial examples on the surrogate model without accessing the target model, drawing increasing interest recently.

Since MI-FGSM  integrates momentum into I-FGSM  to stabilize the update direction and achieve improved transferability, various momentum-based attacks have been proposed to generate transferable adversarial examples. For instance, NI-FGSM  leverages Nesterov Accelerated Gradient for better transferability. VMI-FGSM  refines the current gradient using the gradient variance from the previous iteration, resulting in more stable updates. EMI-FGSM  enhances the momentum by averaging the gradient of several data points sampled in the previous gradient direction.

On the other hand, input transformations that modify the input image prior to gradient calculation have proven highly effective in enhancing adversarial transferability, such as DIM , TIM , SIM , Admix , SSA  and so on. Among these attacks, Admix introduces a small segment of an image from different categories, while SSA applies frequency domain transformations to the input image, both of which have demonstrated superior performance in generating transferable adversarial examples.

Several studies have explored the utilization of more sophisticated objective functions to enhance transferability in adversarial attacks. ILA  employs fine-tuning techniques to increase the similarity of feature differences between the original or current adversarial example and a benign sample. ATA  maximizes the disparity of attention maps between a benign sample and an adversarial example. FIA  minimizes a weighted feature map in an intermediate layer to disrupt significant object-aware features.

A few works have emphasized the significance of the surrogate model in generating highly transferable adversarial examples. Ghost network  attacks a set of ghost networks generated by densely applying dropout at the intermediate features. On the other hand, another line of work focuses on the gradient during backward propagation. SGM  adjusts the decay factor to incorporate more gradients from the skip connections of ResNet to generate more transferable adversarial examples. LinBP  performs backward propagation in a more linear fashion by setting the gradient of ReLU as a constant of 1 and scaling the gradient of residual blocks. In this work, we find that the gradient truncation introduced by non-linear layers undermines the transferability and modify the backward propagation so as to generate more transferable adversarial examples.

### Adversarial Defenses

The existence of adversarial examples poses a significant security threat to deep neural networks (DNNs). To mitigate this impact, researchers have proposed various methods, among which adversarial training has emerged as a widely used and effective approach [10; 21; 31]. By augmenting the training data with adversarial examples, this method enhances the robustness of trained models against adversarial attacks. For instance, Tramer et al.  introduce ensemble adversarial training, a technique that generates adversarial examples using multiple models simultaneously, which shows superior performance against transfer-based attacks.

Although adversarial training is effective, it comes with high training costs, particularly for large-scale datasets and complex networks. Consequently, researchers have proposed innovative defense methods as alternatives. Guo et al.  utilize various input transformations such as JPEG compression and total variance minimization to eliminate adversarial perturbations from input images. Xie et al.  mitigate adversarial effects through random resizing and padding of input images. Liao et al.  propose training a high-level representation denoiser (HGD) specifically designed to purify input images. Nasser  introduce a neural representation purifier (NRP) by a self-supervised adversarial training mechanism to purify the input sample. Various certified defenses aim to provide a verified guarantee in a specific radius, such as randomized smoothing (RS) .

## 3 Methodology

In this section, we analyze the backward propagation procedure and identify that the gradient truncation introduced by non-linear layers undermines the adversarial transferability. Based on this finding, we propose Backward Propagation Attack (BPA) to mitigate such negative effect and gain more transferable adversarial examples.

### Backward Propagation for Adversarial Transferability

Given an input image \(x\) with ground-truth label \(y\), a classifier \(f\) with \(l\) successive layers (_e.g._, \(z_{i+1}=_{i}(f_{i}(z_{i}))\), \(z_{0}=x\)) predicts the label \(f(x)=f_{l}(z_{l})=y\) with high probability. Here \(()\) is a non-linear activation function (_e.g._, ReLU) or identity function if there is no activation function after \(i\)-th layer \(f_{i}\). The attacker aims to find an adversarial example \(x^{adv}\) adhering the constraint of \(\|x^{adv}-x\|_{p}\), but resulting in \(f(x^{adv}) f(x)=y\) for untargeted attack and \(f(x^{adv})=y_{t}\) for targeted attack. Here \(\) is the maximum perturbation magnitude, \(y_{t}\) is the target label, and \(\|\|_{p}\) denotes the \(p\)-norm distance. For brevity, the following description will focus on non-targeted attacks with \(p=\). Let \(J(x,y;)\) denote the loss function of classifier \(f\) (_e.g._, the cross-entropy loss). Existing white-box attacks often solve the following constrained maximization problem using the gradient \(_{x}J(x,y;)\):

\[x^{adv}=*{argmax}_{\|x^{}-x\|_{p}}J(x^{ },y;).\] (1)Based on the chain rule, we can calculate the gradient as follows:

\[_{x}J(x,y;)=(z_{l})}( _{i=k+1}^{l}(f_{i}(z_{i}))}{ z_{i}}) }{ z_{k}}}{ x},\] (2)

where \(0<k<l\) is the index of an arbitrary layer. Without loss of generality, we explore the backward propagation when passing the \(k\)-th layer as follows:

* **A fully connected or convolutional layer followed by a non-linear activation function**. Taking ReLU activation (_i.e._, \(_{k}\)) for example, the \(j\)-th element in the gradient _w.r.t._ the \(k\)-th feature, \([}{ z_{k}}]_{j}\), will be one if \(z_{k,j}>0\) and otherwise, \([}{ z_{k}}]_{j}\) will be zero. These zero gradients in \(}{ z_{k}}\) can lead to the truncation of gradient of the loss function \(}\)_w.r.t._ the input image. As a result, the gradient is effectively limited or weakened to some extent.
* **Max-pooling layer**. As shown in Fig. 1, max-pooling calculates the maximum value (orange block) within a specific patch. Hence, the derivative \(}{ z_{k}}\) will be a binary matrix, containing only ones at locations corresponding to the orange blocks. In this case, approximately \(3/4\) of the elements in the given sample will be zeros. This means that max-pooling tends to discard a significant portion of the gradient information contained in \(}{ x}\), resulting in a truncated gradient.

The truncation of gradient caused by non-linear layers (_e.g._, activation function, max-pooling) can limit or dampen the flow of gradients during backward propagation, which decays the relevance among the gradient between the loss and input. Considering that many existing attacks rely on maximizing the loss by leveraging the gradient information, we make the following assumption:

**Assumption 1**: _The truncation of gradient \(_{x}J(x,y;)\) introduced by non-linear layers in the backward propagation process decays the adversarial transferability._

To validate Assumption 1, we conduct several experiments using FGSM, I-FGSM and MI-FGSM. The detailed experimental settings are summarized in Sec. 4.1.

* **Randomly mask the gradient**. To investigate the impact of gradient truncation on adversarial transferability, we introduce a random masking operation to increase the probability of gradient truncation between stage 3 and stage 2 of ResNet-50. Fig. 1(a) illustrates the attack performance with various mask probabilities. As the mask probability increases, more zeros appear in the derivative, indicating a higher degree of gradient truncation. Consequently, the larger truncation probability renders the gradient less relevant to the loss function, decreasing the attack performance of the three evaluated methods. These findings validate our hypothesis that the truncation of gradient negatively impacts adversarial transferability and highlight the importance of preserving gradient information to maintain the effectiveness of adversarial attacks across various models.
* **Recover the gradient of ReLU or max-pooling layers**. In contrast, it is expected that mitigating the truncation of gradient can improve the adversarial transferability. To explore this, we randomly replaced the zeros in the derivative of ReLU or max-pooling operations with ones, using various replacement probabilities. a) In Fig. 1(b), as the probability of replacement increases, fewer gradients are truncated across ReLU, resulting in improved adversarial transferability on all the three attacks. Notably, these attacks achieve their best performance when the derivative consists entirely of ones, which aligns with LinBP . b) As illustrated in Fig. 1(c), when the ratio of ones in the derivative of max-pooling increases (_i.e._, the replacement probability increases), the attack performance initially improves, reaching a peak around \(0.3\). Subsequently, the attack performance gradually decreases but remains superior to vanilla backward propagation. This highlights that we need a more suitable approximation for the derivative calculation of max-pooling, which is detailed in Sec. 3.2. These results suggest that decreasing the probability of gradient truncation in max-pooling is beneficial for enhancing adversarial transferability.

Overall, these findings validate Assumption 1 that the truncation of gradients negatively impacts adversarial transferability. By preserving gradient information and carefully adjusting the replacement probabilities, it is possible to improve the effectiveness of adversarial attacks across different models.

Figure 1: A max-pooling layer with \(2 2\) kernel size and stride \(s=2\) on a \(4 4\) feature map in the forward propagation.

### Mitigating the Negative Impact of Gradient Truncation

In Sec. 3.1, we demonstrate that reducing the probability of gradient truncation in non-linear layers can enhance adversarial transferability. However, setting all elements in the corresponding derivative to one is not optimal for generating transferable adversarial examples. Here we investigate how to modify the backward propagation process of non-linear layers to further enhance the transferability.

Within the standard backward propagation procedure, the elements comprising the derivative depend on the magnitudes of the associated feature map. This observation provides an impetus for considering the intrinsic characteristics of the underlying features when diminishing the probability of gradient truncation. To this end, we modify the gradient calculation for the ReLU activation function and max-pooling in the backward propagation procedure as follows:

* **Gradient calculation for ReLU**. To ensure precise gradient calculation, it is important to exclude extreme values from consideration when calculating the gradient, while still maintaining the relationship between the elements in the derivative and the magnitude of the feature map. Among the family of ReLU activation functions, SiLU  provides a smooth and continuous gradient across the entire input range and is less susceptible to gradient saturation issues. Hence, we propose using the derivative of SiLU to calculate the gradient of ReLU during the backward propagation process, _i.e._, \(}{ z_{i}}=(z_{i})(1+z_{i}(1- (z_{i})))\), where \(()\) is the Sigmoid function. This formulation allows our gradient calculation to reflect the input magnitude mainly within the input range around \([-5,5]\), while closely resembling the behavior of ReLU when the input is outside this range. As shown in Fig. 3, our proposed gradient calculation method demonstrates improved alignment with the input's magnitude compared to both the original derivative of ReLU and the derivative used in LinBP. By leveraging the smoothness and non-monotonicity of SiLU, we can obtain more accurate and reliable gradient information for ReLU.
* **Gradient calculation for max-pooling**. Similar to the gradient calculation for ReLU, it is essential to exclude extreme values and ensure that the gradient remains connected to the magnitude of the feature map. Furthermore, in the case of max-pooling, the summation of gradients within each window should remain at one to minimize modifications to the gradient. To address these considerations, we propose using the softmax function to calculate the gradient within each window \(w\) of the max-pooling operation: \[[}{ z_{k}}]_{i,j,w}=}}{_{v w}e^{t v}},\] (3) where \(t\) is the temperature coefficient to adjust the smoothness of the gradient. If the feature \(z_{k,i,j}\) is related to multiple windows (_i.e._, the stride is smaller than the size of max-pooling), we sum its gradient calculated by Eq. 3 in each window as the final gradient.

Figure 3: Various candidate derivatives of ReLU function.

Figure 2: Average untargeted attack success rates (%) of FGSM, I-FGSM and MI-FGSM when we randomly mask the gradient, recover the gradient of ReLU or max-pooling layers, respectively. The adversarial examples are generated on ResNet-50 and tested on all the nine victim models illustrated in Sec. 4.1. Raw data is provided in Appendix A.1.

In practice, we adopt the above two strategies to calculate the gradient of ReLU and max-pooling during the backward propagation process. This approach allows us to circumvent the issue of gradient truncation introduced by these non-linear layers. We refer to this modified backward propagation technique as Backward Propagation Attack (BPA), which can be applied to existing CNNs to adapt to various transfer-based attack methods.

## 4 Experiments

In this section, we conduct extensive experiments on standard ImageNet dataset  to validate the effectiveness of the proposed BPA. We first specify our experimental setup, then we conduct a series of experiments to compare BPA with existing state-of-the-art attacks under different settings. Additionally, we provide ablation studies to further investigate the performance and behavior of BPA.

### Experimental Setup

**Dataset.** Following LinBP , we randomly sample 5,000 images pertaining to the 1,000 categories from ILSVRC 2012 validation set , which could be classified correctly by all the victim models.

**Models.** We select ResNet-50  and VGG-19  as our surrogate model for generating adversarial examples. As for the victim models, we consider six standardly trained networks, _i.e_., Inception-v3 (Inc-v3) , Inception-Resnet-v2 (IncRes-v2) , DenseNet , MobileNet-v2 , PNAS-Net , and SENet . Additionally, we adopt three ensemble adversarially trained models, namely ens3-adv-Inception-v3 (Inc-v3\({}_{ens3}\)), ens4-Inception-v3 (Inc-v3\({}_{ens4}\)), and ens-adv-Inception-ResNet-v2 (IncRes-v2\({}_{ens}\)) . To address the issue of different input shapes required by these models, we adhere to the official pre-processing pipeline, including resizing and cropping techniques.

**Baselines.** We adopt three model-related methods as our baselines, _i.e_., SGM , LinBP  and Ghost , and evaluate their performance to boost adversarial transferability of iterative attacks (PGD ), momentum-based attacks (MI-FGSM , VMI-FGSM ), advanced objective functions (ILA ) and input transformation-based attacks (SSA ).

**Hyper-parameters.** We adopt the maximum magnitude of perturbation \(=8/255\) to align with existing works. We run the attacks in \(T=10\) iterations with step size \(=1.6/255\) for untargeted attacks and \(T=300\) iterations with step size \(=1/255\) for targeted attacks. We set the momentum decay factor \(=1.0\) and sample \(20\) examples for VMI-FGSM. The number of spectrum transformations and tuning factor is set to \(N=20\) and \(=0.5\), respectively. The decay factor for SGM is

   Attacker & Method & Inc-v3 & IncRes-v2 & DenseNet & MobileNet & PNASNet & SENet & Inc-v3\({}_{ens3}\) & Inc-v3\({}_{ens4}\) & IncRes-v2\({}_{ens}\) \\   & N/A & 16.34 & 13.38 & 36.86 & 36.12 & 13.46 & 17.14 & 10.24 & 9.46 & 5.52 \\  & SGM & 23.68 & 19.82 & 51.66 & 55.44 & 22.12 & 30.34 & 13.78 & 12.38 & 7.90 \\  & LinBP & 27.22 & 23.04 & 59.34 & 59.74 & 22.68 & 33.72 & 16.24 & 13.58 & 7.88 \\  & Ghost & 17.74 & 13.68 & 42.36 & 41.06 & 13.92 & 19.10 & 11.60 & 10.34 & 6.04 \\  & BPA & **35.36** & **30.12** & **70.70** & **68.90** & **32.52** & **42.02** & **22.72** & **19.28** & **12.40** \\   & N/A & 26.20 & 21.50 & 51.50 & 49.68 & 22.92 & 30.12 & 16.22 & 14.58 & 9.00 \\  & SGM & 33.78 & 28.84 & 63.06 & 65.84 & 31.90 & 41.54 & 19.56 & 17.48 & 10.98 \\  & LinBP & 35.92 & 29.82 & 68.66 & 69.72 & 30.24 & 41.68 & 19.98 & 16.58 & 9.94 \\  & Ghost & 29.76 & 23.68 & 57.28 & 56.10 & 25.00 & 34.76 & 17.10 & 14.76 & 9.50 \\  & BPA & **47.58** & **41.22** & **80.54** & **79.40** & **44.70** & **54.28** & **32.06** & **25.98** & **17.46** \\   & N/A & 42.68 & 36.86 & 88.82 & 66.68 & 40.78 & 46.34 & 27.36 & 24.20 & 17.18 \\  & SGM & 50.04 & 44.28 & 77.56 & 79.34 & 48.58 & 56.86 & 32.22 & 27.72 & 19.66 \\   & LinBP & 47.70 & 40.40 & 77.44 & 78.76 & 41.48 & 52.10 & 28.58 & 24.06 & 16.60 \\   & Ghost & 47.82 & 41.42 & 75.98 & 73.40 & 44.84 & 52.78 & 30.84 & 27.18 & 19.08 \\   & BPA & **55.00** & **48.72** & **85.44** & **83.64** & **52.02** & **60.88** & **38.76** & **33.70** & **23.78** \\   & N/A & 29.10 & 26.08 & 58.02 & 59.10 & 27.60 & 39.16 & 15.12 & 12.30 & 7.86 \\  & SGM & 35.64 & 32.34 & 65.20 & 72.12 & 34.20 & 46.72 & 17.10 & 13.86 & 9.08 \\   & LinBP & 37.36 & 34.24 & 71.98 & 72.84 & 35.12 & 48.80 & 19.38 & 14.10 & 9.28 \\   & Ghost & 30.06 & 26.50 & 60.52 & 61.74 & 28.68 & 40.46 & 14.84 & 12.54 & 7.90 \\   & BPA & **47.62** & **43.50** & **81.74** & **80.88** & **47.88** & **60.64** & **27.94** & **20.64** & **14.76** \\   & N/A & 35.78 & 29.58 & 60.46 & 64.70 & 25.66 & 34.18 & 20.64 & 17.30 & 11.44 \\   & SGM & 45.22 & 38.98 & 70.22 & 78.44 & 35.30 & 46.06 & 26.28 & 21.64 & 14.50 \\   & LinBP & 48.48 & 41.90 & 75.02 & 78.30 & 36.66 & 49.58 & 28.76 & 23.64 & 15.46 \\   & Ghost & 36.44 & 28.62 & 61.12 & 66.06 & 24.90 & 33.98 & 20.58 & 16.84 & 10.82 \\   & BPA & **51.36** & **44.70** & **76.24** & **79.66** & **39.38** & **50.00** & **32.10** & **26.44** & **18.20** \\   

Table 1: Untargeted attack success rates (%) of various adversarial attacks on nine models when generating the adversarial examples on ResNet-50 w/wo various model-related methods.

\(=0.5\) and the random range of Ghost network is \(=0.22\). We follow the setting of LinBP to modify the backward propagation of ReLU in the last eight residual blocks of ResNet-50. We set the temperature coefficient \(t=10\) for ResNet-50 and \(t=1\) for VGG-19.

### Evaluation on Untargeted Attacks

To validate the effectiveness of our proposed method, we compare BPA with several other model-related methods (_i.e._, SGM, LinBP, Ghost) on ResNet-50 to boost various adversarial attacks, namely PGD, MI-FGSM, VMI-FGSM, ILA and SSA. Here we adopt ResNet-50 as the surrogate model since SGM is specific to ResNets. However, it is worth noting that BPA is general to various surrogate models with non-linear layers and we also report the results on VGG-19 in Appendix A.2. To further validate the effectiveness of BPA, we also consider more input transformation based attacks, different perturbation budgets and conduct evaluations on CIFAR-10 dataset  in Appendix A.3-A.5. We measure the attack success rates by evaluating the misclassification rates of the nine different target models on the generated adversarial examples.

**Evaluations on the single baseline**. We can observe from Table 1 that the model-related strategies can consistently boost performance of the five typical attacks on nine models. Among the baseline methods, LinBP generally achieves the best performance, except for VMI-FGSM where SGM surpasses LinBP. By addressing the issue of gradient truncation, BPA consistently improves the performance of all the five attack methods and achieves the best overall performance. On average, BPA outperforms the runner-up attack by a significant margin of \(7.84\%\), \(11.19\%\), \(5.08\%\), \(9.17\%\), \(2.25\%\), respectively. These results highlight the effectiveness and generality of BPA in generating transferable adversarial examples compared with existing model-related strategies. The performance improvement achieved by BPA on SGM and LinBP, which also modify the backward propagation, validates our hypothesis that reducing the gradient truncation introduced by non-linear layers is beneficial for enhancing the adversarial transferability. This emphasizes the importance of carefully considering the backward propagation procedure when generating transferable adversarial examples.

**Evaluations by combining BPA with the baselines**. The primary objective of BPA is to mitigate the negative impact of gradient truncation on adversarial transferability, which is not considered by the baselines. Hence, it is expected that BPA can also boost the performance of these baselines. For validation, we integrate BPA with the baseline methods to enhance the performance of PGD and MI-FGSM attacks. The results of these combinations are presented in Table 2. We can observe that BPA can effectively boost the adversarial transferability of various baselines. On average, BPA can boost the best baseline (_i.e._, LinBP) with a remarkable margin of \(13.23\%\) and \(20.94\%\) for PGD and MI-FGSM, highlighting

   Attacker & Method & Inc-v3 & IncRes-v2 & DenseNet & MobileNet & PNASNet & SENet & Inc-v3\({}_{max}\) & NC-v3\({}_{min}\) & Inc-v2\({}_{max}\) \\   & SGM & 23.68 & 19.82 & 51.66 & 55.44 & 22.12 & 30.34 & 13.78 & 12.38 & 7.90 \\  & SGM+BP & **43.44** & **38.14** & **77.66** & **81.50** & **41.24** & **53.56** & **27.20** & **22.58** & **14.70** \\  & LinBP & 27.22 & 23.04 & 59.34 & 59.74 & 22.68 & 33.72 & 16.24 & 13.58 & 7.88 \\  & LinBP+BPA & **39.08** & **34.50** & **77.80** & **76.86** & **40.50** & **50.26** & **25.66** & **22.46** & **15.10** \\   & Ghost & 17.74 & 13.68 & 42.36 & 41.06 & 13.92 & 19.10 & 11.60 & 10.34 & 6.04 \\   & Ghost+BPA & **34.62** & **29.28** & **69.48** & **69.20** & **29.98** & **41.60** & **22.68** & **18.88** & **11.48** \\   & SGM & 33.78 & 28.84 & 63.06 & 65.84 & 31.90 & 41.54 & 19.56 & 17.48 & 10.98 \\  & SGM+BPA & **56.04** & **49.10** & **85.32** & **88.08** & **52.96** & **63.50** & **36.10** & **29.78** & **20.98** \\   & LinBP & 15.92 & 29.82 & 68.66 & 69.72 & 30.74 & 41.68 & 19.98 & 16.58 & 9.94 \\   & LinBP+BPA & **48.74** & **43.96** & **83.30** & **83.52** & **50.00** & **59.22** & **32.60** & **28.42** & **20.32** \\   & Ghost & 29.76 & 23.68 & 57.28 & 56.10 & 25.00 & 34.76 & 17.10 & 14.76 & 9.50 \\   & Ghost+BPA & **50.42** & **42.84** & **83.02** & **81.24** & **44.70** & **56.50** & **32.46** & **26.82** & **18.34** \\   

Table 2: Untargeted attack success rates (%) of various baselines combined with our method using PGD and MI-FGSM. The adversarial examples are generated on ResNet-50.

   Attacker & Method & HGD & R\&P & NIPS-v3 & JPEG & RS & NRP \\   & N/A & 9.34 & 5.00 & 6.00 & 11.04 & 8.50 & 11.96 \\  & SGM & 16.80 & 7.50 & 9.44 & 13.96 & 10.50 & 12.76 \\  & LinBP & 16.80 & 7.68 & 10.08 & 15.76 & 10.50 & 13.14 \\  & Ghost & 9.60 & 5.06 & 6.42 & 11.92 & 9.50 & 12.06 \\  & BPA & **23.96** & **12.02** & **15.60** & **22.52** & **14.00** & **14.08** \\   & N/A & 16.64 & 8.04 & 9.92 & 16.68 & 13.00 & 13.32 \\  & SGM & 24.80 & 11.02 & 13.16 & 20.26 & 14.00 & 14.38 \\   & LinBP & 21.98 & 10.32 & 13.26 & 20.56 & 12.50 & 13.22 \\   & Ghost & 17.98 & 8.88 & 10.64 & 18.52 & 13.50 & 13.84 \\   & BPA & **34.30** & **17.84** & **22.04** & **30.86** & **17.50** & **15.96** \\   

Table 3: Untargeted attack success rates (%) of several attacks on six defenses when generating the adversarial examples on ResNet-50 w/wo various model-related methods.

the high effectiveness and superiority of BPA. Such high performance also validates its excellent generality to various architectures and supports our hypothesis about gradient truncation.

**Evaluations on defense methods**. To further evaluate the effectiveness of BPA, we also assess its performance on six defense methods using PGD and MI-FGSM, namely HGD , R&P , NIPS-r33, JPEG , RS  and NRP . The results are presented in Table 3. We can observe that our BPA method successfully enhances both the PGD and MI-FGSM attacks, leading to higher attack performance against the defense methods. The results suggest that BPA can effectively enhance adversarial attacks against a range of defense techniques, reinforcing its potential as a powerful tool for generating transferable adversarial examples.

In summary, BPA exhibits superior transferability compared to various baseline methods when evaluated using a range of transfer-based attacks. It also exhibits good generality to further boost existing model-related approaches and achieves remarkable performance on several defense models, highlighting its effectiveness and versatility in generating highly transferable adversarial examples.

### Evaluation on Targeted Attacks

To further evaluate the effectiveness of BPA, we also investigate its performance in boosting targeted attacks. Zhao _et al_.  identified that logit loss can yield better results than most resource-intensive attacks regarding targeted attacks. Here we adopt PGD and MI-FGSM to optimize the logit loss on ResNet-50 w/wo various model-related methods. The results are summarized in Table 4. Without the model-related methods, both PGD and MI-FGSM exhibit poor attack performance. However, when these methods are applied, the attack performance improves significantly. Notably, our BPA method achieves the best attack performance among all the baselines. This highlights the high effectiveness and excellent versatility of our proposed method in boosting targeted attacks and exhibits its potential to improve adversarial attacks in a wide range of scenarios. We also provide the results on VGG-19 in Appendix A.6.

### Ablation Study

To gain further insights into the effectiveness of BPA, we perform parameter studies on two crucial aspects: the position of the first ReLU layer to be modified and the temperature coefficient \(t\) for max-pooling. Additionally, we conduct ablation studies to investigate the impact of diminishing the gradient truncation of ReLU and max-pooling separately. We also provide more discussions about BPA in Appendix A.7-A.9.

**On the position of the first ReLU layer to be modified**. ReLU activation functions are densely applied in existing neural networks. For instance, there are totally \(17\) ReLU activation functions in ResNet-50. Intuitively, the truncation in the latter layers has a greater impact on gradient relevance compared to the earlier layers. As BPA aims to recover the truncated gradients by injecting imprecise gradients into the backward propagation, it is essential to focus on the more critical layers. To identify these important layers and evaluate their impact on transferability, we conduct the BPA attack using MI-FGSM by modifying the ReLU layers starting from the \(i\)-th layer, where \(1 i 17\). As shown in Fig. 3(a), modifying the last ReLU layer alone significantly improves the transferability of the attack, showing its high effectiveness. As we modify more ReLU layers, the transferability further

   Attacker & Method & Inc-v3 & IncRes-v2 & DenseNet & MobileNet & PNASNet & SENet & Inc-v3\({}_{}\) & Inc-v3\({}_{}\) & IncRes-v2\({}_{}\) \\   & N/A & 0.54 & 0.80 & 4.48 & 2.04 & 1.62 & 2.26 & 0.18 & 0.08 & 0.02 \\  & SGM & 2.56 & 3.12 & 15.08 & 8.68 & 5.78 & 9.84 & 0.62 & 0.18 & 0.04 \\  & InBP & 5.30 & 4.84 & 16.08 & 8.48 & 7.26 & 7.94 & 1.50 & 0.54 & 0.28 \\  & Ghost & 1.34 & 2.14 & 10.24 & 4.74 & 3.90 & 6.64 & 0.36 & 0.16 & 0.10 \\  & BPA & **8.76** & **9.74** & **23.76** & **13.42** & **14.66** & **13.76** & **2.52** & **1.02** & **0.72** \\   & N/A & 0.16 & 0.26 & 2.06 & 0.90 & 0.42 & 1.22 & 0.00 & 0.02 & 0.02 \\  & SGM & 0.74 & 0.76 & 5.84 & 3.24 & 1.66 & 3.70 & 0.00 & 0.02 & 0.00 \\   & LinBP & 3.30 & 3.00 & 13.44 & 6.26 & 5.50 & 7.18 & 0.30 & 0.10 & 0.02 \\   & Ghost & 0.66 & 0.76 & 5.48 & 2.14 & 1.58 & 3.38 & 0.08 & 0.02 & 0.00 \\   & BPA & **5.68** & **7.30** & **23.34** & **12.16** & **12.50** & **14.56** & **0.60** & **0.12** & **0.06** \\   

Table 4: Targeted attack success rates (%) of various attackers on nine models when generating adversarial examples on ResNet-50 w/wo model-related methods using PGD and MI-FGSM.

improves and remains consistently high for most models. However, for a few models (_e.g._, PNASNet), modifying more ReLU layers leads to a slight decay on performance. To maintain a high level of performance across all nine models, we modify the ReLU layers starting from \(3\)-\(0\) ReLU layer, which is also adopted in LinBP .

**On the temperature coefficient \(t\) for max-pooling**. The temperature coefficient \(t\) plays a crucial role in determining the distribution of relative gradient magnitudes within each window. For example, when \(t=0\), the gradient distribution becomes a normalized uniform distribution. To find an appropriate temperature coefficient, we conduct the BPA attack using MI-FGSM with various temperatures. As shown in Fig. 3(b), when \(t=0\), the attack exhibits the poorest performance but still outperforms the vanilla MI-FGSM. As we increase the value of \(t\), the attack's performance consistently improves and reaches a high level of performance after \(t=10\). By selecting a suitable temperature coefficient, we ensure that the gradient distribution within each window is well-balanced and contributes effectively to the adversarial perturbation. Thus, we adopt \(t=10\) in our experiments.

**Ablation studies on ReLU and max-pooling**. As stated in Sec. 3.1, we hypothesize that the gradient truncation caused by non-linear layers, such as ReLU and max-pooling in ResNet-50, has a detrimental effect on adversarial transferability. To further validate this hypothesis, we conduct ablation studies by comparing the performance of PGD and MI-FGSM attacks using the vallina backward propagation, the backward propagation modified by either ReLU or max-pooling, and both modifications combined. As shown in Table 5, adopting the modified backward propagation with either ReLU or max-pooling results in a significant improvement in adversarial transferability for both PGD and MI-FGSM attacks. Considering the presence of only one max-pooling layer in ResNet-50, the average performance improvement of \(4.07\%\) and \(7.58\%\) for PGD and MI-FGSM highlights the high effectiveness of BPA and underscores the efficacy of BPA in addressing the issue of gradient truncation. Furthermore, when both ReLU and max-pooling layers are modified in backward propagation, PGD and MI-FGSM exhibit the best performance. This finding supports the rational design of BPA and highlights the importance of mitigating gradient truncation in both ReLU and max-pooling layers to achieve optimal adversarial transferability.

Figure 4: Hyper-parameter studies on the position of the first ReLu layer to be modified and the temperature coefficient \(t\) for the max-pooling layer.

   Attacker & ReLU & Max-pooling & Inc-v3 & IncRes-v2 & DenseNet & MobileNet & PNASNet & SENet & Inc-v3\({}_{max}\) & Inc-v3\({}_{max}\) & IncRes-v2\({}_{max}\) \\   & ✗ & ✗ & 16.34 & 13.38 & 36.86 & 36.12 & 13.46 & 17.40 & 10.24 & 9.46 & 5.52 \\  & ✓ & ✗ & 29.38 & 24.00 & 62.80 & 61.82 & 24.98 & 34.96 & 17.52 & 14.38 & 8.90 \\  & ✗ & ✓ & 20.26 & 16.16 & 44.66 & 42.82 & 17.12 & 21.52 & 13.20 & 11.88 & 7.74 \\  & ✓ & ✓ & **30.36** & **30.12** & **70.70** & **68.90** & **32.52** & **42.02** & **22.72** & **19.28** & **12.40** \\   & ✗ & ✗ & 26.20 & 21.50 & 51.50 & 49.68 & 22.92 & 30.12 & 16.22 & 14.58 & 9.00 \\  & ✓ & ✗ & 41.50 & 34.42 & 74.96 & 74.42 & 35.96 & 47.58 & 23.34 & 18.22 & 10.94 \\   & ✗ & ✓ & 34.16 & 29.02 & 61.38 & 59.42 & 32.24 & 37.32 & 21.74 & 19.96 & 14.70 \\   & ✓ & ✓ & **47.58** & **41.22** & **80.54** & **79.40** & **44.70** & **54.28** & **32.06** & **25.98** & **17.46** \\   

Table 5: Untargeted attack success rates (%) of PGD and MI-FGSM when generating adversarial examples on ResNet-50 w/wo modifying the backward propagation of ReLU or max-pooling.

Conclusion

In this work, we analyzed the backward propagation procedure and identified that non-linear layers (_e.g._, ReLU and max-pooling) introduce gradient truncation, which undermined the adversarial transferability. Based on this finding, we proposed a novel attack called Backward Propagation Attack (BPA) to mitigate the gradient truncation for more transferable adversarial examples. In particular, BPA addressed gradient truncation by introducing a non-monotonic function as the derivative of the ReLU activation function and incorporating softmax with temperature to calculate the derivative of max-pooling. These modifications helped to preserve the gradient information and prevented significant truncation during the backward propagation process. Empirical evaluations on ImageNet dataset demonstrated that BPA can significantly enhance existing untargeted and targeted attacks and outperformed the baselines by a remarkable margin. Our findings identified the vulnerability of model architectures and raised a new challenge in designing secure deep neural network architectures.

## 6 Limitation

Our proposed BPA modifies backpropagation process for gradient calculation, making it only suitable for gradient-based attacks. Besides, BPA modifies the derivatives of non-linear layers, such as ReLU and max-pooling. Consequently, it may not be directly applicable to models lacking these specific components, such as transformers. In the future, we will investigate how to generalize our BPA to such transformers by refining the derivatives of some components, _e.g._, softmax. This endeavor to enhance the generality and versatility of BPA will be an essential aspect of ongoing research, paving the way for the broader applicability of the proposed method and facilitating its adoption in various deep learning models beyond those with ReLU and max-pooling layers.

## 7 Acknowledgement

This work is supported by National Natural Science Foundation (U22B2017, 62076105) and International Cooperation Foundation of Hubei Province, China (2021EHB011).