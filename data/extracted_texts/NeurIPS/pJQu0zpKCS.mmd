# Optimal Exploration for Model-Based RL

in Nonlinear Systems

 Andrew Wagenmaker

Paul G. Allen School of Computer Science & Engineering

University of Washington

Seattle, WA 98195

ajwagen@cs.washington.edu

&Guanya Shi

Robotics Institute

Carnegie Mellon University

Pittsburgh, PA 15213

&Kevin Jamieson

Paul G. Allen School of Computer Science & Engineering

University of Washington

Seattle, WA 98195

###### Abstract

Learning to control unknown nonlinear dynamical systems is a fundamental problem in reinforcement learning and control theory. A commonly applied approach is to first explore the environment (exploration), learn an accurate model of it (system identification), and then compute an optimal controller with the minimum cost on this estimated system (policy optimization). While existing work has shown that it is possible to learn a uniformly good model of the system , in practice, if we aim to learn a good controller with a low cost on the actual system, certain system parameters may be significantly more critical than others, and we therefore ought to focus our exploration on learning such parameters.

In this work, we consider the setting of nonlinear dynamical systems and seek to formally quantify, in such settings, (a) which parameters are most relevant to learning a good controller, and (b) how we can best explore so as to minimize uncertainty in such parameters. Inspired by recent work in linear systems , we show that minimizing the controller loss in nonlinear systems translates to estimating the system parameters in a particular, task-dependent metric. Motivated by this, we develop an algorithm able to efficiently explore the system to reduce uncertainty in this metric, and prove a lower bound showing that our approach learns a controller at a near-instance-optimal rate. Our algorithm relies on a general reduction from policy optimization to optimal experiment design in arbitrary systems, and may be of independent interest. We conclude with experiments demonstrating the effectiveness of our method in realistic nonlinear robotic systems1.

## 1 Introduction

Controlling nonlinear dynamical systems is a core problem in robotics, cyber-physical systems, and beyond, and a significant body of work in both the control theory and reinforcement learning communities has sought to address this challenge . In many real-world scenarios , the dynamics of the system of interest is unknown, or only a coarse model of them is available, whichsignificantly increases the challenge of control--not only must we control such systems, we must _learn_ to control them. While a variety of methods exist to address this challenge, a commonly applied approach is to first perform _system identification_, learning an accurate model of the system's dynamics, and then use this model to obtain a controller. Despite its promising potential, there are still several fundamental questions that must be answered to make this approach practically effective.

_Which parameters are most relevant to learning a good controller?_ Beyond some special cases, little work has been done characterizing how the estimation error from system identification translates to end-to-end suboptimality in the resulting controller of our nonlinear systems. In particular, certain parameters of the system or regions of the state space may be irrelevant to learning a good controller, and coarse estimates of these parameters would suffice, while other parameters may be critical to learning a good controller, and we must therefore estimate these parameters very accurately in order to effectively control the system. In the context of this work, where nonlinearities are considered, the heterogeneity of the parameters is further accentuated. For instance, around a point of equilibrium, some system parameters might be completely inactive, having no impact on the dynamics (see the example in Section 1.1 for an illustration of this).

_How can we best explore so as to minimize uncertainty in relevant parameters?_ Even if we are able to determine which parameters are most important for obtaining a good controller on the true system, it is not obvious how to use this information. How can we direct our system identification phase in order to focus on learning these parameters as quickly as possible, without spending time estimating the parameters of the system less critical for control? This is fundamentally a question of _exploration_. While it is known in linear systems that random excitation will efficiently explore , exploration in nonlinear systems is significantly more challenging since, in order to excite all parameters of interest, non-trivial planning may be required to ensure all relevant states are reached (as is the case in the example considered in Section 1.1).

We address both these questions in a particular class of nonlinear systems parameterized as:

\[_{h+1}=A_{}(_{h},_{h})+_{h}.\] (1.1)

Here \(_{h}^{d_{}}\) denotes the state of the system, \(_{h}^{d_{}}\) the input, \(_{h}(0,_{}^{2} I)\) random noise, \((,)^{d_{}}\) a (possibly nonlinear, known) feature map, and \(A_{}^{d_{} d_{}}\) the (unknown) system parameter. Systems of this form are able to model a variety of real-world settings 2, and have been the subject of recent attention in the reinforcement learning community , yet the aforementioned questions have remained unanswered. Towards addressing this, in this work we make the following contributions:

1. For systems of the form (1.1), given some cost of interest which we wish to find a controller to minimize, we (a) formally characterize how estimation error translates into suboptimality in the learned controller, under the _certainty equivalent_ control rule and (b) provide a lower bound on the loss of _any_ (sufficiently regular) control rule learned from \(T\) rounds of interaction with (1.1).
2. Motivated by this characterization, we present an algorithm which achieves the _instance-optimal_ rate, with controller loss matching our lower bound. To the best of our knowledge, this is the first statistically optimal algorithm in the setting of nonlinear dynamical systems. Our algorithm relies on a generic reduction from policy optimization to optimal exploration in _arbitrary_ dynamical systems (not necessarily of the form (1.1)), which may be of independent interest.
3. We present numerical experiments on several realistic nonlinear systems which illustrate that our approach--efficiently exploring to reduce uncertainty in parameters most relevant to learning a controller--yields significant gains in practice.

To further motivate our approach, we consider the following example.

### Motivating Example

To motivate the need for effective exploration, we consider a simple 1-D system with nonlinear dynamics given by:

\[_{h+1}=a_{1}_{h}+a_{2}_{h}+_{i=1}^{10}a_{i+2}_ {i}(_{h})+_{h}\]where \(_{i}()=\{1-100(-c_{i})^{2},0\}\) for some \(c_{i}\). We choose \(a_{1}=0.8,a_{2}=1\), and \(a_{3}==a_{12}=-3\). We assume \(a_{1:12}\) are unknown, \((_{i})_{i=1}^{10}\) is known, and set

\[(,)=(-c_{1})^{2}+100^{-1}^{2}.\]

With this choice of cost, the optimal controller will attempt to direct the state \(\) to the equilibrium point \(c_{1}\) and maintain this position. Note that, with our choice of \(_{i}\), \(_{i}() 0\) only when \(\) is very close to \(c_{i}\). This renders the parameters \(a_{4:12}\) irrelevant to learning the optimal controller, since \(_{2},,_{10}\) will be inactive if we are playing optimally, but learning \(a_{1:3}\) is critical to performing optimally. In particular, the coefficient of the first nonlinearity, \(a_{3}\), must be learned, as its value significantly changes the dynamics at the goal state.

We illustrate the result of running on this system in Figure 1, comparing our proposed approach (Task-Driven Exploration, Algorithm 1) to the approach which chooses \(_{h}(0,_{}^{2})\) (Random Exploration), and the approach proposed in  (Uniform Exploration) which seeks to explore so as to estimate \(a_{1:12}\) uniformly well. As can be seen, neither of these latter two approaches are able to learn a good controller, while our approach easily finds a near-optimal controller. The failure modes of each of these approaches is somewhat different. Here Random Exploration fails since the chance of reaching the point \(_{h} c_{1}\) is extremely small if the input is random noise--reaching \(c_{1}\) requires playing a particular sequence of actions which are very unlikely to be played if \(_{h}\) is chosen randomly. The Uniform Exploration approach does, in contrast, plan and, given enough time, is guaranteed to estimate all parameters accurately. However, as it aims to estimate all parameters uniformly well, it will attempt to estimate \(a_{4:12}\) accurately despite their irrelevance to control, which will slow down the rate at which it is able to estimate \(a_{3}\). Only our approach, which both plans and takes into account the cost while exploring, is able to reach \(a_{3}\) enough times to efficiently estimate it, and learn a good controller.

This example illustrates that it is critical both to explore efficiently, and also to let the objective--learning a good controller--guide this exploration. We emphasize that the behavior in this example is only exhibited in nonlinear systems--though taking into account the task while exploring in linear systems is known to yield provable improvements , even playing random noise allows every direction to be learned in such systems. In nonlinear systems, however, this is not the case--one may fail to learn completely unless careful planning is performed.

## 2 Related Work

Learning for control.Recently, there has been increased interest in studying control problems from a learning-theoretic perspective, largely for linear system settings such as online LQR or LQG with unknown dynamics [18; 19; 20; 21; 22; 23; 24; 25; 26]. In the nonlinear setting, [27; 28; 29] provide formal guarantees on system identification in several different classes of nonlinear systems, yet they only consider noiseless systems, or systems that are significantly easier to excite than (1.1).  study systems of the form (1.1), but consider only the regret minimization problem. While their bounds would yield a polynomial complexity via an online-to-batch conversion, the resulting guarantee would scale at a \((1/)\) rate, significantly slower than our \((1/T)\) rate. Several additional works in reinforcement with general function approximation encompass nonlinear systems of the form (1.1) [30; 31; 32], yet these results also achieve a slow \((1/)\) rate. The most relevant work  proposes an active learning approach to identify unknown parameters in (1.1), with the goal of minimizing the Euclidean distance in the parameter space. However, they do not provide end-to-end guarantees on learning controllers and, as shown in Section 1.1, this approach could be significantly worse than learning a model with the goal task in mind. Also very related to our work is , which seeks to answer a similar set of questions: performing system identification in order to learn a good controller. This work is restricted to the setting of linear dynamics, however, and does not address the additional complexities of nonlinear systems.

System identification, dual control, and iterative learning control.There is a large body of classical work in system identification , and our work can be seen as an instance of _active_ system

Figure 1: Performance on Motivating Example

identification. While a variety of approaches have been proposed for active system identification [33; 34; 35; 36; 37; 38; 39; 40], these tend to only consider linear systems, or lack rigorous theoretical guarantees. Recently deep learning approaches have also been applied in system identification [6; 8; 9; 41; 42]. In these works, the system identification phase is separate from the downstream controller design. Instead, in the control community, estimating parameters while simultaneously optimizing for performance has been formulated as a dual or iterative learning control problem [43; 44; 45], yet these settings focus on stability or asymptotic convergence whereas our work quantifies the end-to-end suboptimality gap.

**Model-based reinforcement learning.** This paper falls into the broad category of model-based reinforcement learning (MBRL), where an agent explores the environment to learn a model and then computes an optimal policy using the learned model. On the empirical side, deep MBRL has made exciting progress in many domains [46; 47; 48], and several task-aware methods have been designed to improve MBRL's performance [47; 48; 49], yet these works lack formal guarantees. On the theoretical side, a variety of different model-based approaches exist [50; 51; 52; 53; 14]; however, the majority of these consider restricted settings such as tabular or linear MDPs. Of particular interest is the work of  which presents a result in systems of the form (1.1). While they show that polynomial sample complexity is possible, their guarantee scales at a \((1/T^{1/6})\) rate compared to our much faster \((1/T)\) rate.

**Adaptive nonlinear control.** Adaptive nonlinear control also seeks to control an unknown nonlinear system with parametric uncertainties [3; 4]. In particular, the key idea of model-reference adaptive control (MRAC) bears affinity to this paper, in that the adaptation law in MRAC adapts unknown parameters in a task-aware manner. There are two main differences between MRAC and our work. First, adaptive control does not explicitly optimize a cost function--the objective is typically tracking error convergence and Lyapunov stability, whereas our framework allows general cost functions--and the focus is typically on asymptotic convergence, while we give non-asymptotic optimality guarantees. Second, adaptive control has by and large been limited to specific system classes (e.g., fully-actuated systems [4; 15]) and policy classes (e.g., policy to directly cancel out the matched uncertainty [12; 13]), whereas our framework allows more general systems and policy classes.

## 3 Preliminaries

**Notation.**\(\|\|_{}\) denotes the operator norm (matrix 2-norm), \(\|\|_{}\) the Frobenius norm, and \(\|\|_{M}\) the Mahalanobis norm, defined as \(\|\|_{M}:=^{}M}\) for \(M 0\). \((A)\) denotes the vectorization of matrix \(A\). \(_{p}(A;r):=\{A^{}:\|A-A^{}\|_{p} r\}\). \([H]=\{1,2,,H\}\). We let \(_{A}[]\) denote the expectation over trajectories induced on system with parameter \(A\), and \(_{A,}[]\) the expectation induced when policy \(\) is played. \(()\) denotes some term that is polynomial in its arguments, with exponents absolute constants. We use \(\) informally to highlight key parameters in an inequality.

**Setting.** In this work, we are interested in systems of the form (1.1). We consider the episodic setting, where episodes are of length \(H\), and assume that each episodes starts from a given state \(_{1}\). We also assume \(\|A_{}\|_{} B_{A}\) for some known \(B_{A}>0\). We note that the setting considered here encompasses many real-world systems of interest in robotics and control (e.g., [12; 14; 15; 11] and Section 6).

The goal of the learner is to find a policy (controller) \(=(_{h})_{h=1}^{H}\) which achieves minimal cost on (1.1), for the cost defined by some (known) function \((_{h}(,))_{h=1}^{H}\), with \(_{h}:^{d_{}}_{+}\). For a given policy \(\), we define the expected cost on system \(A\) as

\[(;A):=_{A,}[_{h=1}^{H}_{h}( _{h},_{h})].\]

We consider the following interaction protocol:

1. Learner interacts with (1.1) for \(T\) episodes, at each episode playing a policy \(_{}_{}\).
2. After \(T\) episodes, the learner proposes a policy \(_{T}^{}\).
3. The learner suffers cost \((_{T};A_{})\).

The goal of the learner is therefore first to explore and, after \(T\) episodes of exploration, to propose its best guess at the optimal controller for (1.1), \(_{T}\). Here we take \(_{}\) to be a (known) set of admissible exploration policies (for example, policies with bounded input power), and \(^{}\) a (known) set of admissible control policies. We assume that policies in \(^{}\) are deterministic, but allow for randomized policies in \(_{}\). Policies may be either open- or closed-loop.

System Notation.We let \(\) denote the space of all possible state trajectories, \(^{d_{}(H+1)}\), and, for any \(\), let \(_{1:h}\) denote the first \(h\) states and inputs in \(\). For any policy \(\), we denote

\[_{A,}:=_{A,}[_{h=1}^{H}(_{h },_{h})(_{h},_{h})^{}]\]

the expected covariance induced by playing \(\) on system \(A\), \(_{}:=_{A_{*},}\), and \(}:=I_{d_{}}\) the Kronecker product of \(I_{d_{}}\) and \(\). Finally, we let \(\) denote the convex hull of covariance matrices induced by \(_{}\), \(:=\{_{}[_{}]\ :\ _{_{}}\}\), for \(_{_{}}\) the set of distributions over \(_{}\).

### Regularity Assumptions

In order to make learning in (1.1) tractable, we need several regularity assumptions.

**Assumption 1** (Bounded Features).: _For all \(^{d_{}}\) and \(\), we have \(\|(,)\|_{2} B_{}\)._

**Assumption 2** (Bounded Cost).: _There exists some \(r_{}(A_{})>0\) such that, for all \(A_{}(A_{};r_{}(A_{}))\) and all \(^{}\), we have \(_{A,}[(_{h=1}^{H}_{h}(_{h},_{h}))^{2 }] L_{}\)._

**Assumption 3** (Uniform Feature Excitation).: _There exists \(_{_{}}\) such that \(_{}(_{_{}}[_{_{ }}])_{}^{}\) for some \(_{}^{}>0\)._

We remark that these assumptions have appeared before in work on systems of the form (1.1) . In particular, Assumption 3 implies that every direction of \(A_{}\) can, in principle, be excited, allowing it to be learned. In order to precisely quantify the optimal rates of learning, we require that our system satisfy certain smoothness assumptions. First, we require that \((,)\) is differentiable in its second argument.

**Assumption 4** (Smooth Nonlinearity).: _For all \(^{d_{}}\) and \(\), \((,)\) is four-times differentiable in \(\). Furthermore, \(\|_{}^{(i)}(,)\|_{} L_{}\), \( i\{1,2,3,4\}\), \(^{d_{}}\), and \(\)._

We also require that the class of admissible control policies, \(^{}\), has a parametric form, \(^{}=\{^{}\ :\ ^{d_{}}\}\), and that the parameterization is smooth in the following sense.

**Assumption 5** (Smooth Controller Class).: \(_{h}^{}(_{1:h})\) _is four-times differentiable in \(\) for all \(\) and \(h[H]\). Furthermore, \(\|_{}^{(i)}_{h}^{}(_{1:h})\|_{} L_{}\) for \( i\{1,2,3,4\}\), \(^{d_{}}\), and \(\)._

Assumption 5 is satisfied for commonly considered classes of controllers, such as linear controllers, but is also satisfied by more complex classes such as neural network controllers. While the learner may propose any \(_{T}^{}\), we are particularly interested in the _certainty equivalence_ decision rule (i.e., the learner decides \(_{T}\) as if the estimated system is the actual one), defined as:

\[_{}(A):=^{_{}(A)}_{ }(A):=_{^{d_{}}}( ^{};A).\] (3.1)

To ensure that \(_{}(A)\) is well-defined and sufficiently regular, we make the following assumption.

**Assumption 6** (Unique Optimal Controller).: _We assume that the global minimum of \((^{};A_{})\), \(_{}(A_{})\), is unique, and that \(_{}^{2}(^{};A_{})|_{ =_{}(A_{})} 0\)._

In general, the policy optimization problem in (3.1) may not be computationally tractable. As we show in Appendix D, the globally optimal decision rule of (3.1) can be replaced with a locally optimal decision rule (i.e. \(_{}(A)\) a local minimum of \((;A)\)). Furthermore, Assumption 6 can be replaced by assuming the differentiability of \(_{}(A)\) with respect to \(A\) for \(A\) near \(A_{}\). For ease of exposition, in the main text we assume that Assumption 6 holds and that \(_{}(A)\) is defined as in (3.1). With these definitions and under Assumptions 1, 2, 4 and 5, we can show that \((^{};A_{})\) is differentiable in \(\) and, combined with Assumption 6, that \(_{}(A)\) is differentiable in \(A\), for \(A_{}(A_{};r_{}(A_{}))\) and some \(r_{}(A_{})>0\). We let \(L_{_{}}\) denote an upper bound on the norm of the derivatives of \(_{}(A)\).

## 4 Optimal Exploration in Nonlinear Systems

In this work, we are interested in characterizing the instance-optimal rates of learning a controller \(^{}\) which minimizes the loss \((;A_{})\). The following result, a generalization of Proposition 8.2 of  to nonlinear systems, is the starting point of our analysis.

**Proposition 1** (Informal).: _Under Assumptions 1, 2 and 4 to 6 and on the system (1.1), we have_

\[(_{}();A_{})-(_{ }(A_{});A_{})=\|(A_{}-)\|_{(A_{})}^{2}+^{}(\|A_{}-\|_{}^{3})\]

_for_

\[(A_{}):=_{A}^{2}(_{}(A); A_{})|_{A=A_{}}\]

_and where \(^{}()\) hides factors polynomial in the regularity parameters of Assumptions 1 to 6._

The quantity \((A_{}):=_{A}^{2}(_{}(A);A_{})|_{ A=A_{}}\), referred to as the _model-task Hessian_ in , corresponds to the _curvature_ of the loss of the certainty-equivalence controller \(_{}(A)\) around \(A A_{}\). It precisely quantifies how estimation error in each coordinate of \(A_{}\) translates into suboptimality of the controller--providing an answer to our question of which parameters are most relevant to learning a good controller--and reduces the problem of minimizing the controller loss to estimating \(A_{}\) in a particular norm. The following result gives a bound on this estimation error, \(\|(A_{}-)\|_{(A_{})}^{2}\).

**Proposition 2** (Informal).: _Consider interacting with (1.1) for \(T\) episodes, and let \(_{T}=_{t=1}^{T}_{h=1}^{H}( _{h}^{t},_{h}^{t})(_{h}^{t},_{h}^{t})^{}\) denote the observed covariates and_

\[=_{A}_{t=1}^{T}\!_{h=1}^{H}\!\| _{h+1}^{t}-A(_{h}^{t}, _{h}^{t})\|_{2}^{2}\]

_the least-squares estimate of \(A_{}\). Recalling that \(}_{T}=I_{d_{}} _{T}\), we have, with high probability:_

\[\|(A_{}-)\|_{(A_{}) }^{2}_{}^{2}((A_{ })}_{T}^{-1}).\]

### Algorithm and Upper Bound

Proposition 2 motivates our algorithmic approach: explore to collect covariates \(_{T}\) minimizing \(((A_{})}_{T}^{-1})\). There are two primary challenges to achieving this: we do not know \((A_{})\), as it depends on the (unknown) parameter \(A_{}\) and, even if we did know \((A_{})\), it is not clear how to explore so as to collect data minimizing \(((A_{})}_{T}^{-1})\). We address both of these challenges in Algorithm 1.

```
1:inputs: episodes \(T\), \((_{h})_{h=1}^{H}\), confidence \(\), control policies \(^{}\), exploration policies \(_{}\)
2:\(^{1} 0\), \(_{T}_{2}T/8\), \(T_{} 2^{}\)
3:for\(=1,2,3,,_{T}\)do
4: Compute estimate of model-task Hessian: \(_{}(^{})\)
5: Run DynamicOED on \(_{}()(_{} ^{-1})\) to learn exploration policies \(_{}_{}\)
6: Rerun each policy in \(_{}\)\(N_{}= T_{}/\|_{}\) times, denote collected data \(_{}\)
7: Estimate \(A_{}\): \(^{+1}=_{A}_{h=1}^{H}_{(_{h+1}, _{h},_{h})_{}}\|_{h+1}-A(_{h},_{h})\|_{2}^{2}\)
8:return\(_{T}_{}(^{+1})^{}\) ```

**Algorithm 1** Optimal Exploration in Nonlinear Systems (informal)

Algorithm 1 proceeds in epochs of exponentially increasing length. At each epoch it first approximates \((A_{})\) by computing the model-task Hessian of the estimated system, \(^{}\). Using this approximatiom of \((A_{})\), it seeks to explore to minimize \(((^{})}_{T}^{-1})\). This exploration routine is encapsulated in the DynamicOED (dynamic optimal experiment design) function, an adaptive experiment-design routine inspired by recent work in reinforcement learning  and described in more detail in Section 5. DynamicOED returns a set of exploration policies, \(_{}\), which we run to collect data \(_{}\). As we will show, the collected covariates, \(_{}:=_{h=1}^{H}_{(_{h}, _{h})_{}}(_{h}, _{h})(_{h},_{h})^{}\), satisfy

\[((^{})}_{}^{ -1}) T_{}^{-1}_{}((^{})}^{ -1}),\]

which implies that DynamicOED collects data minimizing \(((^{})}_{}^{ -1})\) at a near-optimal rate. Given the data \(_{}\), we form the least-squares estimate of \(A_{}\), \(^{+1}\), and the process repeats. After running for \(T\) episodes, the certainty-equivalence controller on the last estimate obtained, \(_{T}=_{}(^{}x^{+1})\), is returned. The following result bounds the suboptimality of \(_{T}\) as compared to \(_{}(A_{})\)

**Theorem 1**.: _Under Assumptions 1 to 6, if \(T C_{}\{1,r_{}(A_{})^{-2},r_{}(A_{})^{-2}\}\), then with probability at least \(1-\), Algorithm 1 explores with policies in \(_{}\) at every episode, runs for at most \(T\) episodes, and returns \(_{T}^{}\) satisfying:_

\[(_{T};A_{})-(_{}(A_{});A _{})}^{2}}{T}_{ }((A_{})}^{-1}) C }d_{}}{}+}}{T^{3/2}}\]

_where we recall \(\) is the set of possible expected covariates on (1.1), \(C\) is a universal constant, and_

\[C_{}=(d_{},d_{},H,B_{A},B_{ },L_{},L_{},L_{},L_{_{}},_{ {w}},_{}^{-1},},).\]

Theorem 1 shows that Algorithm 1 is able to explore so as to optimally minimize the exploration loss \(((A_{})}_{T}^{-1})\), up to a lower-order term scaling as \(T^{-3/2}\) and polynomially in system parameters. While Propositions 1 and 2 show that collecting data which minimizes \(((A_{})}_{T}^{-1})\) is in some sense fundamental, it is not clear it is necessary. We next show that it is indeed necessary.

### Lower Bounds on Learning Controllers

Our goal is to show that, up to constants and lower-order terms, the bound given in Theorem 1 is not improvable, regardless of which controller estimate we use. To obtain such lower bounds, we need several additional assumptions. In particular, we require that the loss \((^{};A)\) grows quadratically in the distance \(\) is from \(_{}(A)\), and strengthen Assumption 3 to ensure (1.1) is sufficiently easy to excite. Formal statements of these conditions are given in Appendix F. Our lower bound is as follows.

**Theorem 2** (Informal).: _Under Assumptions 1 to 6 and the additional regularity assumptions mentioned above, as long as \(T C_{}\), for any \(_{}_{_{}}\), we have_

\[_{}_{A_{T}}_{_{T}  A,_{}}[((_{T});A) -(_{}(A);A)]}^{2}}{3T}_{ }((A_{})}^{-1})-}}{T^{5/4}}\]

_for \(_{T}:=_{F}(A_{};(T^{-5/6}))\), \(_{_{T} A,_{}}[]=_{ _{}_{}}[_{_{T}  A,_{}}[]]\) the expectation over trajectories generated by running policies \(_{}\) on system \(A\) for \(T\) episodes, \(\) any mapping from observations to policies in \(^{}\), and \(C_{}\) some value scaling polynomially in problem parameters._

Note that this lower bound holds for _any_\(A_{}\) and mapping \(\), as long as our assumptions are met. Up to constants and lower-order terms, the scaling of Theorem 2 matches that of Theorem 1--both scale with \(_{}((A_{})}^{-1})\)--which implies that Algorithm 1 is indeed optimal (under certain additional regularity conditions). To the best of our knowledge, this is the first result characterizing the optimal statistical rates for learning in nonlinear dynamical systems. We emphasize that Theorem 2 holds for _any_ decision rule \(\)--it does not require that we use the certainty equivalence decision rule. As Algorithm 1 does rely on certainty equivalence, this result also implies that the certainty equivalence decision rule is optimal for (certain classes of) nonlinear dynamical systems.

The proof of Theorem 2 builds on the work , which shows a similar result for linear dynamical systems. It critically relies on our quadratic decomposition of the controller loss in Proposition 1, which reduces the problem of obtaining a lower bound on controller loss to a lower bound on estimating \(A_{}\) in the \((A_{})\) norm. Given this, the result can be obtained by applying lower bounds on regression in general norms.

## 5 Optimal Experiment Design in Arbitrary Dynamical Systems

We turn now to the DynamicOED routine, which is the key algorithmic tool we use to prove Theorem 1. DynamicOED is a general reduction from policy optimization to optimal experiment design in arbitrary dynamical systems, and is an extension of a recently proposed approach for experiment design in linear MDPs . This section may be of independent interest.

To illustrate the generality of this reduction, in this section we consider the following system:

\[_{h+1}=f_{h}(_{h},_{h},_{h}), h=1,2,,H,\] (5.1)

where \(_{h}^{d_{}}\) denotes the state, \(_{h}^{d_{}}\) the input, and \(_{h}^{d_{}}\) the noise. We take the dynamics \((f_{h})_{h=1}^{H}\) to be unknown and arbitrary. We assume there is some known featurization ofour system that is of interest, \((,)^{d_{}}\), and an experiment design object on this featurization, \(:^{d_{} d_{}}\). Our goal is to collect some set of trajectories \(\{_{t}\}_{t=1}^{T}\) which minimizes \(\):

\[_{t=1}^{T}_{h=1}^{H}(_{h}^ {t},_{h}^{t})(_{h}^{t},_{h}^{t})^{}.\]

As an example, if \(()=()\), this reduces to \(D\)-optimal design, and if \(()=(^{-1})\), the setting considered in Section 4, this reduces to weighted \(A\)-optimal design. As before, we assume we have access to some set of exploration policies \(_{}\), and define \(_{}\) and \(\) as in Section 3, but with respect to this new feature map \(\) and system (5.1). We also define \(}\) to be the space of all possible covariance matrices:

\[}:=_{h=1}^{H}(_{h},_{h}) (_{h},_{h})^{}\ :\ _{h},_{h},  h[H]}.\]

To facilitate efficient experiment design in this setting, we will make the following assumption on \(\).

**Assumption 7** (Regularity of \(\)).: \(\) _is regular in the following sense:_

1. \(\) _is convex, differentiable, and_ \(\)_-smooth in the norm_ \(\|\|\) _(with dual-norm_ \(\|\|_{}\)_):_ \[\|_{}()-_{^{}}( ^{})\|_{}\|-^{ }\|,,}.\]
2. _There exists some_ \(M<\) _satisfying_ \(_{}}_{, }|(,)^{}_{}( {})(,)| M\)_._

The key algorithmic assumption we make is access to a regret minimization oracle on (5.1).

**Assumption 8** (Regret Minimization Oracle).: _Let \(_{h}(,)=(,)^{}Q_{h} {}(,)\) for some \(Q_{h}^{d_{} d_{}}\) such that \(|_{h}_{h}(_{h},_{h})| 1\) for all \(_{h},_{h}\). We assume we have access to some learner \(_{}\) which is able to achieve low regret on costs \(\{_{h}(,)\}_{h=1}^{H}\) with respect to policy class \(_{}\). That is, with probability at least \(1-\):_

\[_{t=1}^{T}_{f,_{t}}[_{h=1}^{H}_{h}( {x}_{h}^{t},_{h}^{t})]-T_{_{}}_{f,}[ _{h=1}^{H}_{h}(_{h},_{h})] C_{}^{p_{}} T^{}\]

_for some \(C_{}>0\), \(p_{}>0\), and \((0,1)\), and where \(_{t}\) is the policy \(_{}\) plays at episode \(t\)._

Note that the regret minimization algorithm satisfying Assumption 8 may be arbitrary. For example, for linear systems, we could apply provably efficient algorithms for the Linear Quadratic Regulator ; for nonlinear systems of the form (1.1) we could apply the LC\({}^{3}\) algorithm of ; for more general settings of reinforcement learning with function approximation, algorithms such as BiLin-UCB  or E2D  could be applied. In practice, though they may not formally satisfy the guarantee of Assumption 8, deep RL approaches could be used. We have the following result.

**Theorem 3**.: _Fix \(T>0\) and denote \(R:=_{,^{}}}\|-^{}\|\). Under Assumption 7, and assuming we have access to a learner \(_{}\) satisfying Assumption 8 with \(=1/2\), DynamicOED runs for \(T\) episodes on (5.1), and with probability at least \(1-\) collects data \(\{(_{h}^{t},_{h}^{t})\}_{h[H],t[T]}\) satisfying_

\[_{t=1}^{T}_{h=1}^{H}_{h}^{t}(_{h}^{t})^{}-_{}( ) T+HM(C_{}^{p_{}}+3^{1/2})}{T^{1/3}}\]

_where \(_{h}^{t}:=(_{h}^{t},_{h}^{t})\), and we recall \(\) is the set of possible expected covariates on (5.1)._

Theorem 3 shows that, given access only to a regret minimization oracle, it is possible to solve experiment design problems on arbitrary dynamical systems. The requirement that \(=1/2\) is for expositional purposes only--we generalize this result to arbitrary \(\) (and more general feature maps) in Appendix C. Under certain conditions, it can be shown that, if the exploration policies DynamicOED runs to collect \(\) are _rerun_, the newly collected data satisfies a similar guarantee as Theorem 3. This lets us run DynamicOED to learn an approximate solution of \(_{}()\), and then rerun the learned policies as many times as desired to collect additional data approximately minimizing \(\).

### Overview of DynamicOED Algorithm

DynamicOED is inspired by recent work on experiment design in reinforcement learning , and can be seen as an extension of the FWRegret algorithm of  to arbitrary systems. Werefer the reader to  for a more in-depth discussion of the FWRegret algorithm, and briefly sketch its extension to arbitrary systems here (see Appendix C and Algorithm 4 for precise definitions).

```
1:input: objective \(\), episodes \(T\), confidence \(\), regret algorithm \(_{}\), exploration policies \(_{}\)
2:Set \(K(T^{2/3}),N(T^{1/3})\), \(_{n}\)
3:// \(_{h}^{k,n}:=(_{h}^{k,n},_{h}^ {k,n})\) for \((_{h}^{k,n},_{h}^{k,n})\) the state-input at step \(h\) of episode \(k\) of iteration \(n\)
4:Play any \(_{}_{}\) for \(K\) episodes, set \(_{0}_{k=1}^{K}_{h=1}^{H} _{h}^{k,0}(_{h}^{k,0})^{}\)
5:for\(n=1,2,,N\)do
6: Compute derivative of \((_{n-1})\), \(_{n}_{}()|_{ =_{n-1}}\)
7:Run \(_{}\) on cost \(^{n}_{h}(,) (,)^{}(_{n}) (,)\) for \(K\) episodes
8:\(_{n}(1-_{n})_{n-1}+ }{K}_{k=1}^{K}_{h=1}^{H}_{h}^{k,n}(_{h}^{k,n})^{}\)
9:return\(_{n=0}^{N}_{k=1}^{K}_{h=1}^{H}_{h}^{k,n}(_{h}^{k,n})^{}\) ```

**Algorithm 2** Dynamic Optimal Experiment Design (DynamicOED, Informal)

Conceptually, DynamicOED runs a variant of conditional gradient descent on the objective \(()\). At each iteration, \(n\), it computes the gradient of the loss at the current iterate, \(_{n}_{}()|_{ =_{n-1}}\). To run a standard gradient descent algorithm on this objective, we would simply update \(_{n-1}\) by taking a step in the direction \(_{n}\). However, our objective is to minimize \(\) over the constraint set, \(\). Thus, rather than taking a step in the direction \(_{n}\), we wish to take a step in the direction of steepest descent _within the constraint set_.

The challenge is that the constraint set in our setting, \(\), is _unknown_, as it depends on the expectation over trajectories induced on the unknown dynamics \((f_{h})_{h=1}^{H}\), and therefore we cannot directly compute this steepest descent direction. The key observation is that the computation of this steepest descent direction is equivalent to solving:

\[_{_{}_{}}_{f,_{}}[_{h=1}^{H} (_{h},_{h})^{}(_{n}) (_{h},_{h})].\]

This is simply a policy optimization problem, however, and can be solved approximately by \(_{}\) under Assumption 8. Thus, in the call to \(_{}\) on Line 6, we approximate the steepest descent direction, and on Line 7 update \(_{n-1}\) in this direction. Convergence of this procedure to the optimal value, \(_{}()\), can then be shown by the standard analysis of conditional gradient descent. We remark that, under Assumption 7 and Assumption 8, this argument is completely generic and does not require that our system, (5.1), exhibit any additional properties.

### From Theorem 3 to Theorem 1

In Algorithm 1, our goal is to collect covariates, \(}_{T_{}}^{-1}\), on (1.1) such that \(((^{})}_{T_{ }}^{-1})\) is as small as possible. To achieve this, we apply DynamicOED to the dynamics (1.1) and objective \(_{}()=((^{} )}^{-1})\), with Assumption 8 instantiated by the LC\({}^{3}\) algorithm of . By the guarantee given in Theorem 3, after running for a number of episodes \(N\) which scales polynomially in problem parameters, DynamicOED will collect covariates \(_{N}\) such that \(_{}(_{N}) 2_{ }_{}()\), which implies \(((^{})}_{N} ^{-1})_{} ((^{})}^{-1})\). By rerunning the policies DynamicOED used to collect this \(_{N}\) for \(T_{}/N\) additional times, we can ensure \(((^{})}_{T_{ }}^{-1})}_{ }((^{})}^{-1})\) as desired.

We remark that the LC\({}^{3}\) algorithm requires access to a computation oracle. As the focus of this work is primarily statistical, we leave addressing this computational challenge for future work. Furthermore, as we show in the following section, computationally efficient, sampling-based implementations of our approach are very effective in practice. We remark as well that the objective we ultimately care about minimizing is \(((A_{*})}^{-1})\). As we show, by including a small amount of uniform exploration, we can ensure that the suboptimality incurred optimizing \(((^{})}^{-1})\) instead of \(((A_{*})}^{-1})\) only contributes to the lower-order terms of the final guarantee in Theorem 1.

## 6 Experimental Results

Finally, we demonstrate the effectiveness of our proposed approach (Algorithm 1, the Task-Driven Exploration method in Figures 1 to 3) on several systems motivated by robotic applications. Wecompare Algorithm 1 with an approach that plays \(_{h}(0,_{}^{2} I)\) (Gaussian Exploration), and an approach inspired by  (Uniform Exploration), which seeks to estimate \(A_{}\) uniformly well, playing inputs that reduce \(\|-A_{}\|_{}\).

To benchmark the performance of these approaches, we consider an affine system with dynamics corresponding to that of a simplified 3-D drone (i.e., 3-D double integrator with a gravity term), and a nonlinear system with dynamics corresponding to that of a 2-D car. For both systems, we choose \(H=50\), and plot the value of \((_{t};A_{})-(_{}(A_{});A_ {})\) for \(_{t}\) the certainty-equivalence controller computed on the estimate of the system obtained at time \(t\). For the drone, we let \(^{}\) be the class of linear-affine feedback controllers, and for the car, \(^{}\) is a set of nonlinear controllers with dimension 4. While the optimal controller for the drone can be computed in closed-form, for the car we rely on a sampling-based routine to find an approximately optimal controller. The model-task hessian \((^{})\) is computed via automatic differentiation. For the exploration policies of Task-Driven Exploration and Uniform Exploration, \(_{}\), we rely on MPC-style sampling based methods. For all approaches, we require that \(_{A,_{}}[_{h=1}^{H}\|_{h}\|_{2}^{2}] ^{2}\) for some \(^{2}>0\) and all \(_{}_{}\). On all examples, we implement DynamicOED with \(_{}\) a posterior sampling-inspired version of the LC\({}^{3}\) of . Figures 1 and 3 shows performance averaged over 100 trials, and Figure 2 over 200 trials. Additional experimental details can be found in Appendix G.

As illustrated in Figures 1 to 3, our approach yields a non-trivial gain over existing approaches on all systems. In particular, in Figures 2 and 3 it improves on the sample complexity of existing approaches by roughly a factor of 2--for example, in the drone system, reaching excess controller cost of \(10\) after less than \(20\) episodes, as compared to over \(40\) episodes for existing approaches.

Our implementation is very modular, and any piece (for example, the parameterization of \(_{}\) and \(^{}\), the policy optimizer, or the exploration routine) can be easily replaced with other procedures. Our results highlight that, even when using, for example, a possibly suboptimal policy optimizer, exploring so as to minimize uncertainty in the model-task hessian yields a non-trivial gain. We expect that this would hold true regardless of the policy optimizer used--the model-task hessian will adapt to the structure of the policy optimizer. Integration of our approach with deep model-based RL approaches is an interesting direction for future work, but we believe the approach will scale to these settings as well.

## 7 Conclusion

In this work, we have characterized the instance-optimal rate of learning controllers in nonlinear dynamical systems. To the best of our knowledge, this is the first work to obtain an optimal sample complexity for learning in nonlinear dynamical systems. Furthermore, our experimental results demonstrate the effectiveness of our proposed algorithm in realistic nonlinear systems. This work opens the door for several interesting directions for future work. First, it is not clear that the assumption on uniform feature excitation, Assumption 3, is necessary. Can this be removed with a more refined analysis (or shown to be necessary)? Second, while in many settings \(\) is known or can be effectively represented by random features, in some settings it is helpful to learn it . Can we obtain end-to-end guarantees on learning both \(\) and \(A_{}\)? Finally, it is of much interest to extend our experimental results to larger-scale systems for real-world deployment.

Figure 2: Performance on Drone Figure 3: Performance on Car