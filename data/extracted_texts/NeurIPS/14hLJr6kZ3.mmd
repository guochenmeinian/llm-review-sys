# Enhancing Domain Adaptation through

Prompt Gradient Alignment

 Hoang Phan

New York University

hvp2011@nyu.edu

&Lam Tran

VinAI Research

lamtt12@vinai.io

&Quyen Tran

VinAI Research

quyentt15@vinai.io

Equal contributions.

###### Abstract

Prior Unsupervised Domain Adaptation (UDA) methods often aim to train a domain-invariant feature extractor, which may hinder the model from learning sufficiently discriminative features. To tackle this, a line of works based on prompt learning leverages the power of large-scale pre-trained vision-language models to learn both domain-invariant and specific features through a set of domain-agnostic and domain-specific learnable prompts. Those studies typically enforce invariant constraints on representation, output, or prompt space to learn such prompts. Differently, we cast UDA as a multiple-objective optimization problem in which each objective is represented by a domain loss. Under this new framework, we propose aligning per-objective gradients to foster consensus between them. Additionally, to prevent potential overfitting when fine-tuning this deep learning architecture, we penalize the norm of these gradients. To achieve these goals, we devise a practical gradient update procedure that can work under both single-source and multi-source UDA. Empirically, our method consistently surpasses other vision language model adaptation methods by a large margin on a wide range of benchmarks. The implementation is available at https://github.com/VietHoang1512/PGA.

## 1 Introduction

Deep learning has significantly advanced the field of computer vision, achieving remarkable performance in tasks such as image classification [1; 2; 3; 4; 5], object detection [6; 7; 8; 9], and semantic segmentation [10; 11; 12; 13]. However, the effectiveness of these deep learning models heavily relies on large amounts of labeled training data, which is often labor-intensive and expensive to collect. Moreover, the discrepancy between training data and real-world testing data can lead to substantial performance drops when models are deployed in practical settings [14; 15; 16].

To address these challenges, Unsupervised Domain Adaptation (UDA) has emerged as a pivotal solution. UDA aims to transfer knowledge from a labeled source domain to an unlabeled target domain in the presence of a domain shift, thereby enabling models to generalize well across different domains without requiring extensive labeled data for the target domain. This is often achieved by optimizing objective function on source domains and other auxiliary terms that encourage learning domain-invariant feature representations [17; 18; 19; 20] or enhance model robustness [21; 22; 23; 24], which mitigates the domain shift and improve the performance on unseen data. Nevertheless, aligning representations could potentially hurt the model performance due to the loss of discriminative features [25; 26].

Conceptually, our proposed method is orthogonal to these invariant feature learning methods, and they could complement each other.

Recent works leveraging pre-trained models like CLIP  for UDA can significantly bridge domain gaps and improve generalization by utilizing rich semantic information and robust visual representations through extensive pre-training on diverse image-text datasets. Following this vein, DAPL  first introduces domain-specific and domain-agnostic prompts to efficiently adapt pre-trained vision-language models without fine-tuning the entire model. Furthermore, MPA  aligns multiple prompts from different sources using an auto-encoder. While these methods could obtain superior performance on different benchmarks, we find that the main improvement comes from the strong zero-shot performance and self-training mechanism. In particular, prior works often generate pseudo-label for unlabeled images and then train the model on those samples. Consequently, finetuning a pretrained CLIP model on this dataset alone without leveraging source datasets can help refine model prediction significantly, boosting the performance from \(88.1\%\) to \(90.1\%\), yielding a competitive result compared against MPA, as presented in Table 1.

Motivated by this observation, we directly optimize the main objective function not only on source domains but also on the target data, instead of only using them for auxiliary objectives as in previous work . We thus cast the original UDA problem as a multi-objective optimization (MOO) problem. Specifically, we minimize a vector-valued loss function, which includes the objectives of multiple source domains and the target domain. This formulation allows us to apply existing results from MOO literature for finding Pareto solutions, from which we can not optimize an objective without hurting another  or encourage positive inter-task transfer between objectives . Note that in the context of UDA, we focus more on learning the target task, thus motivating us to apply prioritized MOO algorithms  or to incorporate predefined preferences . While those methods allow practitioners to focus more or less on the objectives at hand, they come with the cost of extensive hyperparameter tuning. Besides, recent works  argue that simple loss functions reweighting can match the performance of gradient-based MOO methods . Those findings suggest we focus more on the inherent conflict nature of per-objective gradients instead of attempting to remove the conflict between them .

In this paper, we propose casting the problem of UDA as a multi-objective optimization by leveraging powerful pre-trained models. However, while obtaining impressive results on various downstream tasks, over-parameterization is still a crucial problem for transformer-based models , which potentially causes overfitting  more severely than small-parameter architectures , especially in the multi-task learning context. For that reason, we propose to (i) fine-tune pre-trained model via prompt learning, which is known for being more robust  and especially more light-weight than full fine-tuning, (ii) and to leverage the gradient norm penalty to encourage model generalization . Furthermore, we introduce a gradient alignment algorithm to foster inherent consensus between per-objective gradients without modifying the gradient itself. Our proposed method, termed Prompt Gradient Alignment (PGA), and its variant for multi-source UDA, Multi-Prompt Gradient Alignment (MPGA), achieve state-of-the-art performance on different UDA benchmarks. As shown in Figure 1, PGA and MPGA outperform traditional UDA methods like MFSAN  and recent prompt-based UDA methods such as MPA  and DAMP  while requiring fewer trainable parameters. We also provide a generalization bound for UDA and show how theoretical insights motivate the design of our proposed method.

   Dataset & \(\) & \(\) & \(\) & **Avg** \\  Zero-shot & 87.9 & 88.2 & 78.7 & 88.1 \\ Simple Prompt & 93.6 & 90.6 & 80.9 & 88.4 \\ Self-training & 92.9 & 94.3 & 83.2 & 90.1 \\ MPA & 97.2 & 96.2 & 80.4 & 91.3 \\   

Table 1: Self-training on pseudo-labeled target data is already a strong baseline. Figure 1: Baselines performance on Office-HomeRelated work

**Unsupervised Domain Adaptation.** A dominant approach to solving the UDA problem is to reduce the distribution shift between source and target domains. Following the foundational theory outlined in , one group of methods seeks to minimize the H-divergence between the marginal distributions of these domains [65; 66; 67]. Alternatively, other methods aim to align the moments of these distributions, as suggested in [68; 69; 70]. Additionally, adversarial learning has been employed to learn domain-invariant features. For instance, methods such as those in [19; 71] use a domain discriminator to differentiate between source and target samples, training a feature extractor to deceive this discriminator. However, as  highlights, these methods often struggle with a trade-off between domain alignment and classification performance, particularly in multi-source scenarios where only a single model is used.

**Prompt learning-based domain adaptation** is a novel approach introduced in , leverages the generalization capabilities of CLIP to learn both domain-agnostic and domain-specific prompts. This method effectively addresses the trade-off between domain alignment and classification performance by employing a contrastive loss. This loss aligns the representation of an image with the prompt corresponding to its ground truth class and domain, thereby encouraging the learning of domain-invariant features. Building on this foundation, MPA  advances the concept of multi-source UDA. It adapts the prompt learning strategy to each source-target domain pair. The prompts are aligned through a denoising auto-encoder using Euclidean distance. However, prompting is known as a brittle process where a small shift to the prompt can cause large variations in the model predictions [72; 73; 74]. Therefore, in this work, we propose to intervene in the training on the gradient space as it offers a more interpretable and controllable effect during training. Furthermore, PGA is trained in an end-to-end fashion, avoiding the sequential training for each source-target pair as in MPA.

**Gradient-based multi-task learning**. Due to the multi-objective nature of the multi-source domain adaptation problem, one can leverage recent methods from the multi-task learning literature [34; 35; 75] to derive an optimization procedure that benefits the learning across domains or put more weight on some specific domains via incorporating preference [32; 41; 42]. While those techniques are readily applicable in our context, we directly re-weight per-task gradients, similar to scalarization, instead of adopting multi-task learning methods for simplicity. Furthermore, our work is orthogonal to those gradient-based multi-task learning methods where we encourage the consensus among objects instead of directly manipulating their gradients to remove inherent conflicts among them.

**Gradient matching** is commonly used in continual learning [76; 77; 78] to measure conflict and transferability between tasks. A positive dot product between two tasks' gradients implies updating the models with one task can benefit the other. This principle is also applied in domain generalization [79; 80] to focus on invariant features. However, our approach aligns in the space of prompt gradient, a significantly smaller parameter set than the full model gradients used in previous works. Besides, to avoid the computation of costly second-order derivatives,  linearly approximate the inner product between gradients, which underperforms on datasets with a larger number of domains due to cumulative approximation error. Meanwhile, our method does not face this problem since we implicitly compute this term without using any approximation. More works sharing the same intuition of gradient alignment are provided in Appendix D.

## 3 Background

### Unsupervised Domain Adaptation

Given a set of \(N 1\) source domains \(\{D_{S,i}\}_{i=1}^{N}\) each of which is a collection of data-label pairs of domain \(i\), i.e. \(D_{S,i}=\{_{j},y_{j}\}_{j=1}^{N_{S,i}}\), and one unlabelled target domain \(D_{T}=\{_{j}\}_{j=1}^{N_{T}}\), where \(N_{S,i}\) and \(N_{T}\) are respectively the number of data points in source domain \(i\) and target domain \(T\), the goal is to learn a model that can perform well on the unlabelled target domain. In this paper, we focus on classification problems and denote \(K\) as the number of categories.

### Prompt Learning on CLIP-based models

CLIP  is a vision-language model that consists of an image encoder \(f_{i}\) and a text encoder \(f_{t}\), which is trained to align the visual representation \(f_{i}()\) of an image \(\) with the textual representation \(f_{t}(y)\) of the corresponding label. The textual representation is derived from a manually crafted prompt \(_{k}\) in the form "A photo of a \([]_{k}\)", where \([]_{k}\) is the class \(k\)'s name. With great generalization capability, pre-trained CLIP models are often used for a variety of downstream tasks through prompt learning.

For zero-shot inference, \(K\) class names are forwarded through the text encoder, and the one with the highest representation similarity with the image is the predicted class:

\[y_{}=_{k}P(y=k|),P(y=k|)=(),f_{t}(_{k})/)}{_{k^{}=1}^{K} ( f_{i}(),f_{t}(_{k^{}})/)},\] (1)

and \(.,.\) measures the cosine similarity and \(\) is the temperature.

For fine-tuning, a set of learnable class-shared prompts are added to the class token to form \(_{k}=[_{1}|_{2}||_{M}][]_{k}\), where \(_{i}\) is a vector with the same size as the word embedding, and \(M\) is the number of added prompts. These prompts are learnt by maximizing log-likelihood on downstream data, i.e. \(_{i} P(y=y_{i}|_{i},)\). Note that in this predictive probability, we abuse symbol \(\) to refer to the learnable tokens \(_{i}\), and when we drop the symbol as in 1, we refer to the zero-shot prediction using CLIP. As a result, additional information about the downstream task can be encoded in the prompts, and this design will enable knowledge transfer from pre-trained datasets.

## 4 Proposed method

In this section, we describe our proposed prompt gradient alignment method. Motivated by the lightweight and effective nature of prompt learning in adapting pre-trained knowledge to downstream tasks, we cast UDA as a multi-objective optimization (MOO) problem, from which we propose aligning gradients of different objectives and minimizing their norms simultaneously. Additionally, we derive a UDA generalization bound to justify the intuition of our method. The full details of our proposed method in the generalized case where we have more than one source domain are provided in Appendix B.

### Prompt design

A common assumption in domain adaptation literature is that each domain can be represented by domain-specific features and those that are shared with others. To reflect this, we employ two sets of prompts for each domain: domain-agnostic prompt (or shared prompt, interchangeably) \(_{sh}\), and domain-specific prompts \(_{S,i}\) and \(_{T}\). Here, \(_{S,i}\) refers to prompt used for source domain \(i\), and \(_{T}\) is that for target one. In particular, following DAPL, we use \(K M_{1}\) tokens to construct \(_{sh}=[_{sh}^{k}]_{k=1}^{K}\), where \(_{sh}^{k}=[_{1}^{k}|_{2}^{k}||_{M_{1}}^{k}]\) is class-specific shared tokens. For source- and target-specific prompts, we use \(M_{2}\) tokens: \(_{S,i}=[_{1}^{S,i}|_{2}^{S,i}||_{M_{2}}^{S,i}]\), \(_{T}=[_{1}^{T}|_{2}^{T}||_{M_{2}}^{T}]\). And denote \(=[_{sh},\{_{S,i}\}_{i=1}^{N},_{T}]\) as the whole prompts used in our method. Based on this, we use a prompt of the form \([_{sh}^{k}][_{S,i}][]_{k}\) to compute the predictive distribution of a source \(i\) sample belonging to class \(k\), and similarly \([_{sh}^{k}][_{T}][]_{k}\) for a target sample.

### Empirical risk minimization: a simple baseline

As we introduced, to learn those prompts, we consider the cross-entropy losses applied to source data and target data with pseudo labels as a set of objectives to optimize simultaneously:

\[_{total}():=[_{S,i}()]_{i=1}^{N}, _{T}()=[_{S,i}(_{sh},_{ S,i})]_{i=1}^{N},_{T}(_{sh},_{T}),\]

\[_{S,i}(_{sh},_{S,i}) =(_{sh},_{S,i};_{S,i},Y_{S,i})=-}_{j=1}^{N_{S,i}} P(y=y_{j}|_{j},_{sh},_{S, i}),\] (2) \[_{T}(_{sh},_{T}) =_{}(_{sh},_{T};_{T},Y_{T})\] \[=-}_{j=1}^{N_{T}}(P(y=_{j}| _{j})) P(y=}|_{j},_{sh},_{T}),\] (3) \[_{j} =_{k}P(y=k|_{j}).\] (4)In summary, the total loss consists of \(N+1\) objectives. The target objective is applied to target samples whose zero-shot predictions made by CLIP are larger than a threshold \(\).

Given these objectives, source- and target-specific prompts can be updated by minimizing source and target losses, respectively. Regarding domain-agnostic prompt, one can put a weighting term on the signal from source losses to compute the gradient. Formally, for \( i=1 N\), we have:

\[_{sh,i},_{S,i} =_{}_{S,i}(_{sh},_{S,i}), _{sh,T},_{T}=_{}_{T}(_{sh},_{T}),\] \[_{S,i} =_{S,i}-_{S,i},_{T}= _{T}-_{T},\] (5) \[_{sh} =_{sh}-(_{sh,T}+_{i}_{sh,i}),\] (6)

where \(\) is the learning rate, and \(\) is the weighting term to control how much emphasis we want to put on the target domain. Note that we treat gradient signals from source domains equally as we assume no prior preference knowledge about them. Nevertheless, one can measure the domain similarity between each source and target domain to devise a better way to reweight source domains' objectives. However, as will be shown in the experiments, taking the average is simple yet yields superior results, hence we will leave this for future work.

### Prompt gradient alignment for UDA

For simplicity, we first consider the single-source UDA setting and will present the extension to the multi-source one later in Appendix B. One problem with the method above is we ignored the potential inherent gradient conflict between objectives when updating the shared prompt. To mitigate this, one can follow gradient-based methods, such as [35; 47] to manipulate the gradients so that conflict is reduced. However, it has been shown in [44; 45; 46] that comparable performance can be obtained without such complex manipulations, but with simple re-weighting the loss functions. Therefore, to encourage consensus between these gradients without modifying them, we propose aligning gradients between source and target domains during training. Specifically, we aim to maximize their cosine similarity, \(_{sh,S},_{sh,T}\), If this goal is achieved, one can expect the shared prompt to capture useful features for classes regardless of domains. Indeed, \(-_{sh,S}\) denotes the direction that moves the shared prompt towards low-loss region of source data, and similar for \(-_{sh,T}\). Hence, when they point to the same direction, i.e., \(_{sh,S},_{sh,T}>0\), updating the shared prompt as in Eq. 6 can reduce loss of both domains, because the aggregated gradient \(_{sh}=_{sh,S}+_{sh,T}\) will create acute angles with both \(_{sh,S}\) and \(_{sh,T}\). As a result, the shared prompt can learn knowledge that benefits both domains, which is its ultimate goal.

However, there remain two important questions when implementing this gradient alignment constrain: (i) How to incorporate the cosine similarity maximization term as a regularization in the framework described in Sec. 4.2?; and (ii) How to reduce training time and space when explicitly maximizing it, as it involves the computation of Hessian matrix w.r.t the shared prompt? Our method will address these two concerns.

Consider the following loss applied on target data with \(||.||\) denoting \(l_{2}\)-norm of a vector:

\[_{T}^{}() :=_{T}(_{sh}-_{sh,S}}{\|_{ sh,S}\|.\|_{sh,T}\|},_{T})\] \[_{T}(_{sh},_{T})-_ {sh,S})^{}._{_{sh}}_{T}(_{sh},_{T}) }{\|_{sh,S}\|.\|_{sh,T}\|}\] \[=_{T}(_{sh},_{T})-_{sh,S},_{sh,T},\] (7)

where Eq. 7 is obtained by applying first-order Taylor expansion with \(\) is a small value, and \(\) is the vector transpose. It can be seen that minimizing this loss also maximizes cosine similarity between gradients of the two domains. In order to achieve this, let denote \(=_{sh,S}}{\|_{sh,S}\|.\|_{sh,T}\|}\), and consider the loss's gradient w.r.t \(_{sh}\):

\[_{sh,T}^{} :=_{_{sh}}_{T}(_{sh}-,_{T})\] \[=._{sh}-)}{d(_{sh})}_{ _{sh}}_{T}(_{sh},_{T})|_{_{sh}= _{sh}-}\] \[._{_{sh}}_{T}(_{sh},_{T})|_{_{sh}=_{sh}-}.\] (8)In the approximation of Eq. 8, we avoid the Hessian computation by dropping the derivative of \(\) w.r.t \(_{sh}\). Now we can practically apply deep learning optimizers, such as SGD, to minimize \(_{T}^{}()\). Specifically, we first compute gradients of the source and target losses w.r.t the shared prompt to get vector \(\), then move the current shared prompt to the new stage: \(_{sh}=_{sh}-\). Finally, at this new stage, re-compute the loss on target data then calculate the new gradient.

In a similar way, we can derive \(_{S}^{}()\) on source data and then compute its new gradient w.r.t the shared prompt, i.e. \(_{sh,S}^{}\). Given these two new gradients, we can combine them to get the final update direction of the shared prompt, which will navigate it to common low-valued regions in the loss landscapes of both domains.

\[=_{sh,T}}{\|_{sh,S}\|.\|_{sh,T}\|},_{sh,S}^{} _{_{sh}}_{S}(_{sh},_{S })|_{_{sh}=_{sh}-}\,,\] \[_{sh}^{} =_{sh,T}^{}+_{sh,S}^{}.\]

### Prompt gradient-norm penalization for UDA

So far, we have proposed casting each domain loss as an objective in a multiple-objective optimization framework, and have suggested maximizing congruence between gradients of these objectives to reduce their inherent conflict. However, the domain loss is in the empirical form, which has been shown to be easily stuck in sharp minima and thus limiting generalization ability [81; 82], especially under distribution shifts . Therefore, we argue that explicit control over the generalization of these prompts can be beneficial. Moreover, inspired by the finding in  that gradient norm penalization can help model favor flat minima, and by the effectiveness of such minima in the context of multi-task learning , we propose minimizing prompt gradient norm of each objective to enhance prompt generalization.

By following the same analysis as in Eq. 7, we can seamlessly fuse the gradient norm penalty term with the cosine similarity maximization with the loss below:

\[_{T}^{}() :=_{T}(_{sh}-_{ga}_{sh,S}}{\| _{sh,S}\|.\|_{sh,T}\|}+_{gn}_{sh,T}}{\|_{sh,T}\|},_{T}+_{gn}_{T}}{\|_{T}\|})\] \[_{T}(_{sh},_{T})-_{ga}_{sh,S})^{}._{_{sh}}_{T}(_{sh}, _{T})}{\|_{sh,S}\|.\|_{sh,T}\|}+_{gn}(\|_{sh,T}\| +\|_{T}\|)\] \[=_{T}(_{sh},_{T})-_{ga}_ {sh,S},_{sh,T}+_{gn}(\|_{sh,T}\|+\|_{T}\|),\]

where \(_{T}\) is the gradient of the target loss w.r.t target-specific \(_{T}\), and \(gn\) stands for gradient norm.

We then follow the derivation of Eq. 8 to come up with a practical approximation of the gradient of \(_{T}^{}()\)

\[_{sh,T}^{},_{T}^{} :=_{}_{T}^{}()\] \[_{}_{T}(_{sh},_{T})|_{ _{sh}=_{sh}-_{ga}+_{gn}_{sh,T}}{\|_{sh,T}\|},_{T}=_{T}+_{gn}_{T}}{\|_{T}\|}}\,.\]

Similarly, we obtain the gradient of the source objective

\[_{sh,S}^{},_{S}^{} _{}_{S}(_{sh},_{S})|_{ _{sh}=_{sh}-_{ga}+_{gn}_{sh,S}}{\|_{sh,S}\|},_{S}=_{S}+_{gn}_{S}}{\|_{S}\|}}\,.\]

Following the update rules in Eq. 5 and Eq. 6, the prompts can be learnt to achieve both of our two goals: inter-domain gradient alignment and flat minima enforcement, which can lead to improved performance for UDA. We will recap this with a generalization bound in the next part, and provide details for the final loss function in Appendix B.

### Theoretical Analysis of PGA

We informally present an information-theoretic bound to explain why PGA works. Refer to Appendix A for the formal version. For simplicity, we will consider the single-source UDA setting and abuse \(N\) as the number of source samples. Let \(,\) be the input-label space and prompt space (or hypothesis space), respectively. Assume the loss function \(:_{0}^{+}\) is R-subgaussian * Denote \(,^{}\) as the two underlying distributions from which the source and target data is sampled, and \(KL(.||.)\) as the KL-divergence. The generalization error is defined as the difference between the target population loss and the source empirical loss

Footnote *: A random variable \(X\) is R-subgaussian if for any \(,((X-X))^{2}R^{2}/2\).

\[Err:=_{,D_{S},D_{}}[R_{^{}}()-R_{D_{ S}}()]=_{,D_{S},D_{}}[_{^{ }^{}}[(,^{})]-_{i=1}^ {N}(,_{i})].\]

**Theorem 4.1**.: _Under the assumption R-subgaussianity, the generalization error can be upper-bounded by:_

\[|Err|}{N}_{t=1}^{}_{t}^{2} _{_{t-1},D_{S},D_{}}[||_{t}^{src}||^{2}+|| _{t}^{tgt}||^{2}+||_{t}^{src}-_{t}^{tgt}||^{2}]}+(||^{})},\]

_where \(\) is the total number of training iterations, \(_{t}\) is the learning rate at iteration \(t\) scaled by a scalar; \(_{t}^{src}=_{}_{S}(_{t-1})\), \(_{t}^{tgt}=_{}_{T}(_{t-1})\) are the gradients w.r.t \(_{t-1}\) of source loss Eq. 2 and target loss Eq 3 where \(_{t}\) is the prompt at iteration \(t\)._

As our method tries to minimize source empirical loss, gradient norms and gradient mis-alignment, from the first term in the R.H.S of Eq. 4.1, its benefit to the performance on target domain can be justified. Furthermore, the second term shows that the generalization error can be reduced by bridging the gap between the two domain distributions, which is the core of many UDA methods, such as . However, as stated earlier, our work is orthogonal to this line of method as we do not explicitly attempt to close such gap. Hence, an interesting future development could be taking the second term into account. Refer to Appendix A.5 for more discussion about this bound.

## 5 Experiments

In this section, we evaluate the efficacy of our proposed method on different UDA benchmarks, following the same protocol of recent prompt-based UDA studies . Before that, we start with a simple multi-objective-optimization setup to derive insights into the effectiveness of our proposed method compared to conventional empirical risk minimization (ERM).

### Illustrative example

Let \(\{-1,1\}\) be the true label, \(\) be the environmental feature and \(\) be Gaussian noise, \(^{300}\), and \((0,1),C>1\) be predefined scalar constants. The data-generating process is given by:

\[\{-1,1\},\{p_{}(=y=y)=\\ p_{}(=-y=y)=(1-)., (0,^{298}),=[C*,,]\]

The environmental feature \(\) correlates with the true label \(\) according to \(\). Similar to , we set \(=0.9\) for the training and validation set (in-distribution) and \(=0.1\) for the test set

Figure 2: Performance of ERM and PGA on the in-domain data (validation set) and out-of-distribution data (test set). Average results and shaded standard errors are obtained from \(10\) random seeds.

(out-of-distribution). Figure 2 presents the performance of three linear classifiers trained by ERM, our gradient alignment method only and PGA. In summary, while ERM learns non-predictive features and fails to generalize beyond in-distribution data, our gradient alignment algorithm can leverage the gradient information from multiple environments to learn the core feature that helps perform well on OOD data. Besides, incorporating the gradient norm penalty term further enhances stability and robustness at convergence.

### Experimental setup

**Datasets**. We conduct experiments using three well-established UDA datasets of varying scales: ImageCLEF , Office-Home , and DomainNet , respectively. Detailed descriptions of these datasets are available in Appendix C.1.

**Metrics**. We evaluate our model by reporting the top-1 accuracy for each target domain and the average accuracy across all domains. To further validate the effectiveness of our proposed method, we conduct experiments in two distinct settings: a source-combined setting, where data from all source domains are merged, and a multi-source setting, which utilizes individual domain identifications. Additionally, we provide pair-wise single-source domain adaptation results for the Office-Home dataset.

**Baselines.** Regarding prompt-based baselines, we compare our method with MPA , DAPL , Simple Prompt , and Zero-shot CLIP . To ensure a comprehensive evaluation, we also include comparisons with various non-prompt methods such as DCTN , MDDA , MFSAN , T-SVDNet  and PFSA ... As we follow the same settings as in  and , results for baselines are taken from those studies for the consistency. Note that while DAPL , MPA  and our methods employ CoOp  with text-end soft-prompt, other methods finetune the transformer block  or both image and text-end soft-prompts  or the whole encoders [94; 95]. Since those methods typically fine-tune many more parameters, we thus do not include them in the experimental results for the sake of fair comparison.

### Experimental results

Table 2 presents the results for the ImageCLEF and Office-Home datasets. Under the source-combined scenario, PGA significantly outperforms nearly all other baseline methods on both datasets, with the exception of its own multi-source variant, MPGA. For instance, PGA surpasses the second

    &  &  &  \\    & \(\)**C** & \(\)**I** & \(\)**P** & **Avg** & \(\)**Ar** & \(\)**Cl** & \(\)**Pr** & \(\)**Rw** & **Avg** \\ 
**Zero-Shot** & & & & & & & & & & \\ CLIP  & 87.9 & 88.2 & 78.7 & 88.1 & 71.2 & 50.4 & 81.4 & 82.6 & 71.4 \\ 
**Source Combined** & & & & & & & & & & \\ DAN  & 93.3 & 92.2 & 77.6 & 87.7 & 68.5 & 59.4 & 79.0 & 82.5 & 72.4 \\ DANN  & 93.7 & 91.8 & 77.9 & 87.8 & 68.4 & 59.1 & 79.5 & 82.7 & 72.4 \\ D-CORAL  & 93.6 & 91.7 & 77.1 & 87.5 & 68.1 & 58.6 & 79.5 & 82.7 & 72.2 \\ DAPL  & 96.0 & 89.2 & 76.0 & 87.1 & 72.8 & 51.9 & 82.6 & 83.7 & 72.8 \\ Simple Prompt  & 93.6 & 90.6 & 80.9 & 88.4 & 70.7 & 52.9 & 82.9 & 83.9 & 72.4 \\
**PGA** (Ours) & 96.8 & 95.7 & 84.6 & 92.4 & 75.2 & 59.7 & 86.2 & 86.2 & 76.8 \\ 
**Multi-Source** & & & & & & & & & \\ DCTN  & 95.7 & 90.3 & 75.0 & 87.0 & N.A. & N.A. & N.A. & N.A. & N.A. \\ MDDA  & N.A. & N.A. & N.A. & 86.7 & 62.3 & 79.5 & 79.6 & 71.0 \\ SIMplDA  & 93.3 & 91.0 & 77.5 & 87.3 & 70.8 & 56.3 & 80.2 & 81.5 & 72.2 \\ MFSAN  & 95.4 & 93.6 & 79.1 & 89.4 & 72.1 & 62.0 & 80.3 & 81.8 & 74.1 \\ MPA  & 97.2 & 96.2 & 80.4 & 91.3 & 74.8 & 54.9 & 86.2 & 85.7 & 75.4 \\
**MPGA** (Ours) & **97.4** & **96.5** & **84.7** & **92.9** & **76.3** & **63.8** & **90.0** & **87.4** & **79.4** \\   

Table 2: Accuracy (%) on ImageCLEF and Office-Home. We use **bold** to denote the best method overall and underscore to denote the best method using source combined. Overall, PGA and MPGA consistently obtain the best results among source combined and multi-source scenarios, respectively.

best source combined method by a notable \(4\%\) in average accuracy and exceeds MPA by over \(1\%\). Notably, in the Office-Home domain Clipart, while two prompt-based baselines, DAPL and Simple Prompt, lag behind their non-prompt counterparts, PGA still manages to achieve slightly better results than these non-prompt methods. In the multi-source setting, MPGA consistently delivers the highest performance across all domains, notably outperforming MPA, the state-of-the-art (SOTA) prompt-based method for multi-source UDA, by a substantial margin of \(4\%\) on Office-Home.

On DomainNet, as Table 3 presents, our method still obtains superior average accuracy under both source combined and multi-source, higher than the runner-up by 3.4% and 2.1%, respectively. Overall, in the domain where CLIP brings significant results compared with non-prompt baselines, our method leads to better performance, except for the difficult QuickDraw domain, as remarked by a relatively low zero-shot accuracy for CLIP-based methods, where it seems that prompt learning fails to beat non-prompt counterparts. Even though, both PGA and MPGA still outperform other prompt-based counterparts while fine-tuning fewer parameters (e.g. \(500\)k versus \(2\)M of MPA).

In addition, we also demonstrate our method's effectiveness under 12 pair-wise source-target settings on Office-Home in Table 4. Again, PGA acquires the highest average score and consistently beats DAPL under 12 settings while using the same parameter-efficient-finetuning method .

   Method & Ar\(\)Cl & Ar\(\)Pr & Ar\(\)Re & Cl\(\)Ar & Cl\(\)Pr & Cl\(\)Re & Pr\(\)Ar & Pr\(\)Cl & Pr\(\)Re & Rw\(\)Ar & Rw\(\)Cl & Rw\(\)Pr & Avg \\  ResNet-50 & 34.9 & 50.0 & 58.0 & 37.4 & 41.9 & 46.2 & 38.5 & 31.2 & 60.4 & 53.9 & 41.2 & 59.9 & 46.1 \\ DANN  & 45.6 & 59.3 & 70.1 & 47.0 & 58.5 & 60.9 & 46.1 & 43.7 & 68.5 & 63.2 & 51.8 & 76.8 & 57.6 \\ JAN  & 45.9 & 61.2 & 68.9 & 50.4 & 59.7 & 61.0 & 45.8 & 43.4 & 70.3 & 63.9 & 52.4 & 76.8 & 58.3 \\ CDANe  & 50.7 & 70.6 & 76.0 & 57.6 & 70.0 & 70.0 & 57.4 & 50.9 & 77.3 & 70.9 & 56.7 & 81.6 & 65.8 \\ By+CDAN  & 52.0 & 68.6 & 76.1 & 58.0 & 70.3 & 70.2 & 58.6 & 50.2 & 77.6 & 72.2 & 59.3 & 81.9 & 66.3 \\ SymsNets  & 47.7 & 72.9 & 78.5 & 64.2 & 71.3 & 74.2 & 61.6 & 47.6 & 79.4 & 73.8 & 50.8 & 82.6 & 67.2 \\ ETH  & 51.3 & 71.9 & 85.7 & 57.6 & 69.2 & 73.7 & 57.8 & 51.2 & 79.3 & 70.2 & 57.5 & 82.1 & 67.3 \\ BAN  & 52.3 & 73.9 & 80.0 & 63.3 & 72.9 & 74.9 & 61.7 & 49.5 & 79.7 & 70.5 & 53.6 & 82.2 & 67.9 \\ GSDL  & 64.3 & 76.1 & 94.4 & 65.4 & 73.3 & 74.3 & 65.0 & 53.2 & 80.0 & 72.2 & **60.6** & 83.1 & 70.3 \\ GVB-GD  & 57.0 & 74.7 & 79.8 & 64.6 & 74.1 & 74.6 & 65.2 & 55.1 & 81.0 & 74.6 & 59.7 & 84.3 & 70.4 \\ BADA-MSYN  & 51.3 & 77.7 & 81.3 & 66.4 & 74.0 & 76.5 & 67.9 & 53.0 & 82.0 & 75.8 & 57.8 & 58.4 & 70.9 \\ SPL  & 54.5 & 77.8 & 81.9 & 65.1 & 78.0 & 81.1 & 66.0 & 53.1 & 82.8 & 69.9 & 55.3 & **86.0** & 71.0 \\ SRBC  & 52.3 & 76.3 & 81.0 & 69.5 & 76.2 & 78.0 & 68.7 & 53.8 & 81.7 & **76.3** & 57.1 & 85.0 & 71.3 \\ DeMexlerDA  & 58.8 & 77.0 & 80.8 & 67.0 & 74.6 & 77.1 & 65.9 & **56.3** & 81.4 & 74.2 & 60.5 & 83.6 & 71.4 \\  CLIP  & 51.6 & 81.9 & 82.6 & 71.9 & 81.9 & 82.6 & 71.9 & 51.6 & 82.6 & 71.9 & 51.6 & 81.9 & 72.0 \\ DAPL  & 54.1 & 84.3 & 84.8 & 74.4 & 83.7 & 85.0 & 74.5 & 54.6 & 84.8 & 75.2 & 54.7 & 83.8 & 24.5 \\
**PGA** (Ours) & 56.1 & **85.5** & **86.0** & **75.5** & **85.2** & **85.8** & **75.2** & 55.7 & **86.1** & 75.4 & 56.7 & 85.8 & **75.8** \\   

Table 4: Accuracy (%) on Office-Home for unsupervised domain adaptation (ResNet-50). The best accuracy is indicated in **bold**.

    &  \\   & \(\)**Clp** & \(\)**Inf** & \(\)**Put** & \(\)**Qdr** & \(\)**Rel** & \(\)**Skt** & **Avg** \\ 
**Zero-Shot** & & & & & & & \\ CLIP  & 61.3 & 42.0 & 56.1 & 10.3 & 79.3 & 54.1 & 50.5 \\ 
**Source Combined** & & & & & & & \\ DANN  & 45.5 & 13.1 & 37.0 & 13.2 & 48.9 & 31.8 & 32.6 \\ MCD  & 54.3 & 22.1 & 45.7 & 7.6 & 58.4 & 43.5 & 38.5 \\ DAPL  & 62.4 & 43.8 & 59.3 & 10.6 & 81.5 & 54.6 & 52.0 \\ Simple Prompt  & 63.1 & 41.2 & 57.7 & 10.0 & 75.8 & 55.8 & 50.6 \\
**PGA** (Ours) & 66.3 & 49.2 & 63.3 & 11.1 & 81.8 & 60.6 & 55.4 \\ 
**Multi-Source** & & & & & & & \\ M9SDA-\(\) & 58.6 & 26.0 & 52.3 & 6.3 & 62.7 & 49.5 & 42.6 \\ SImpAI101  & 66.4 & 26.5 & 56.6 & **18.9** & 68.0 & 55.5 & 48.6 \\ LUC-MSDA  & 63.1 & 28.7 & 56.1 & 16.3 & 66.1 & 53.8 & 47.4 \\ T-SVDNet  & 66.1 & 25.0 & 54.3 & 16.5 & 65.4 & 54.6 & 47.0 \\ PFSA  & 64.5 & 29.2 & 57.6 & 17.2 & 67.2 & 55.1 & 48.5 \\ PTMDA  & 66.0 & 28.5 & 58.4 & 13.0 & 63.0 & 54.1 & 47.2 \\ MPA  & 65.2 & 47.3 & 62.0 & 10.2 & 82.0 & 57.9 & 54.1 \\
**MPGA** (Ours) & **67.9** & **50.5** & **63.8** & 11.6 & **82.2** & **61.0** & **56.2** \\   

Table 3: Accuracy (%) on DomainNet. We use **bold** to denote the best method overall and underscore to denote the best method using source combine.

### Ablation study

From Table 5, we can see that (i) learning prompts using solely the target loss, the accuracy across all settings already surpasses that of Zero-shot CLIP. This confirms the reliability of pseudo labels generated by CLIP. (ii) When adding source loss and grad-norm penalization, the results improve slightly. (iii) Importantly, adding gradient alignment, the scores increase more clearly. These observations verify each of our contributions.

Furthermore, to show that gradient alignment indeed increases consensus between gradients, we plot cosine similarity along the training process with three different values of \(_{ga}\) in Figure 3. First, during early training stages, there seems to be less agreement between gradients when no alignment is enforced, c.f. \(_{ga}=0\). When \(_{ga}>0\), we can see the similarity increase. Noticeably, there exists a point where similarity starts plummeting. This is reasonable when the model starts to converge to a Pareto solution where source and target gradients cancel each other. This is depicted more clearly in Figure 4 in the appendix where the closer the model is to the Pareto front, the more conflict the gradients are.

## 6 Conclusion

In this work, we have proposed a framework for UDA inspired by Multi-objective optimization thanks to the generalizability of CLIP and the lightweight nature of prompt learning. We have then devised a practical method to align per-objective gradients, which aims to encourage inherent consensus between objectives. We have further fused gradient norm penalization into the method to enhance prompt generalization. Finally, a UDA generalization bound is presented to justify the benefits of our method.