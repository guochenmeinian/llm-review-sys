# Preventing Model Collapse in Deep Canonical Correlation Analysis by Noise Regularization

Junlin He

The Hong Kong Polytechnic University

Hong Kong SAR, China

junlinspeed.he@connect.polyu.hk

&Jinxiao Du

The Hong Kong Polytechnic University

Hong Kong SAR, China

jinxiao.du@connect.polyu.hk

&Susu Xu

Johns Hopkins University

Maryland, USA

sxu83@jhu.edu

&Wei Ma

The Hong Kong Polytechnic University

Hong Kong SAR, China

wei.w.ma@polyu.edu.hk

Corresponding author.

###### Abstract

Multi-View Representation Learning (MVRL) aims to learn a unified representation of an object from multi-view data. Deep Canonical Correlation Analysis (DCCA) and its variants share simple formulations and demonstrate state-of-the-art performance. However, with extensive experiments, we observe the issue of model collapse, _i.e._, the performance of DCCA-based methods will drop drastically when training proceeds. The model collapse issue could significantly hinder the wide adoption of DCCA-based methods because it is challenging to decide when to early stop. To this end, we develop NR-DCCA, which is equipped with a novel noise regularization approach to prevent model collapse. Theoretical analysis shows that the Correlation Invariant Property is the key to preventing model collapse, and our noise regularization forces the neural network to possess such a property. A framework to construct synthetic data with different common and complementary information is also developed to compare MVRL methods comprehensively. The developed NR-DCCA outperforms baselines stably and consistently in both synthetic and real-world datasets, and the proposed noise regularization approach can also be generalized to other DCCA-based methods such as DGCCA. Our code will be released at https://github.com/Umaruchain/NR-DCCA.git.

## 1 Introduction

In recent years, multi-view representation learning (MVRL) has emerged as a core technology for learning from multi-source data and providing readily useful representations to downstream tasks (Sun et al., 2023; Yan et al., 2021), and it has achieved tremendous success in various applications, such as video surveillance (Guo et al., 2015; Feichtenhofer et al., 2016; Deepak et al., 2021), medical diagnosis (Wei et al., 2019; Xu et al., 2020) and social media (Srivastava and Salakhutdinov, 2012; Karpathy and Fei-Fei, 2015; Mao et al., 2014; Fan et al., 2020). Specifically, multi-source data can be collected from the same object, and each data source can be regarded as one view of the object. For instance, an object can be described simultaneously through texts, videos, and audio, which contain both common and complementary information of the object (Yan et al., 2021; Zhang et al., 2019; Hwang et al., 2021; Geng et al., 2021), and the MVRL aims to learn a unified representation of the object from the multi-view data.

The key challenge of MVRL is to learn the intricate relationships of different views. The Canonical Correlation Analysis (CCA), which is one of the early and representative methods for MVRL, transforms all the views into a unified space by maximizing their correlations (Hotelling 1992, Horst 1961, Hardoon et al. 2004, Lahat et al. 2015, Yan et al. 2023, Sun et al. 2023). Through correlation maximization, CCA can identify the common information between different views and extract them to form the representation of the object. On top of CCA, Linear CCA, and DCCA maximize the correlation defined by CCA through gradient descent, while the former uses an affine transformation and the latter uses Deep Neural Networks (DNNs). (Andrew et al. 2013). Indeed, there are quite a few variants of DCCA, such as DGCCA (Benton et al. 2017), DCCAE (Wang et al. 2015), DVCCA (Wang et al. 2016), DTCCA (Wong et al. 2021) and DCCA_GHA (Chapman et al. 2022).

However, extensive experimentation reveals that **DCCA-based methods typically excel during the initial stages of training but suffer a significant decline in performance as training progresses**. This phenomenon is defined as model collapse within the context of DCCA. Notably, our definition is grounded in the performance of the learned representations on downstream tasks. Previous studies found that the representations (i.e., final output) of both Linear CCA and DCCA are full-rank (Andrew et al. 2013, De Bie & De Moor 2003). Nevertheless, they did not further explore whether merely guaranteeing that the full-rank representations can guarantee that the weight matrices are full-rank.

Though early stopping could be adopted to prevent model collapse (Prechelt 1998, Yao et al. 2007), it remains challenging when to stop. The model collapse issue of DCCA-based methods prevents the adoption in large models, and currently, many applications still use simple concatenation to combine different views (Yan et al. 2021, Zheng et al. 2020, Nie et al. 2017). Therefore, how to develop a DCCA-based MVRL method free of model collapse remains an interesting and open question.

In this work, we demonstrate that both representations and weight matrices of Linear CCA are full-rank whereas DCCA only guarantees that representations are full-rank but not for the weight matrices. Considering that Linear CCA does not show the model collapse while DCCA does, we conjecture that the root cause of the model collapse in DCCA is that the weight matrices in DNNs tend to be low-rank. A wealth of research supports this assertion, both theoretically and empirically, demonstrating that over-parameterized DNNs are predisposed to discovering low-rank solutions (Jing et al. 2021, Saxe et al. 2019, Soudry et al. 2018, Dwibedi et al. 2021). If the weight matrices in DNNs tend to be low-rank, it means that the weight matrices are highly self-related and redundant, which limits the expressiveness of DNNs and thus affects the quality of representations.

Therefore, this paper develops NR-DCCA, a DCCA-based method equipped with a generalized noise regularization (NR) approach. The NR approach ensures that the correlation with random data is invariant before and after the transformation, which we define as the Correlation Invariant Property (CIP). It is also verified that the NR approach can be applied to other DCCA-based methods. Comprehensive experiments using both synthetic datasets and real-world datasets demonstrate the consistent outperformance and stability of the developed NR-DCCA method.

From a theoretical perspective, we derive the equivalent conditions between the full-rank property and CIP of the weight matrix. By forcing DNNs to possess CIP and thus mimicking the behavior of Linear CCA, we introduce random data to constrain the weight matrices in DNNs and expect to avoid them being redundant and thus prevent model collapse.

In summary, our contributions are four-fold:

* The model collapse issue in DCCA-based methods for MVRL is identified, demonstrated, and explained.
* A simple yet effective noise regularization approach is proposed and NR-DCCA is developed to prevent model collapse. Comprehensive experiments using both synthetic datasets and real-world datasets demonstrate the consistent outperformance and stability of the developed NR-DCCA.
* Rigorous proofs are provided to demonstrate that CIP is the equal condition of the full-rank weight matrix, which justifies the developed NR approach from a theoretical perspective.
* A novel framework is proposed to construct synthetic data with different common and complementary information for comprehensively evaluating MVRL methods.

Related Works

### Multi-view representation learning

MVRL aims to uncover relationships among multi-view data in an unsupervised manner, thereby obtaining semantically rich representations that can be utilized for various downstream tasks (Sun et al., 2023; Yan et al., 2021). Several works have been proposed to deal with MVRL from different aspects. DMF-MVC (Zhao et al., 2017) utilizes deep matrix factorization to extract a shared representation from multiple views. MDcR (Zhang, Fu, Hu, Zhu & Cao, 2016) maps each view to a lower-dimensional space and applies kernel matching to enforce dependencies across the views. CPM-Nets (Zhang, Han, Fu, Zhou, Hu et al., 2019) formalizes the concept of partial MVRL and many works have been proposed for such issue (Zhang et al., 2020; Tao et al., 2019; Li et al., 2022; Yin & Sun, 2021). AE\({}^{2}\)-Nets (Zhang, Liu & Fu, 2019) utilizes a two-level autoencoder framework to obtain a comprehensive representation of multi-view data. DUA-Nets (Geng et al., 2021) takes a generative modeling perspective and dynamically estimates the weights for different views. MVT-CAE (Hwang et al., 2021) explores MVRL from an information-theoretic perspective, which can capture the shared and view-specific factors of variation by maximizing or minimizing specific total correlation. Our work focuses on CCA as a simple, classic, and theoretically sound approach as it can still achieve state-of-the-art performance consistently.

### CCA and its variants

Canonical Correlation Analysis (CCA) projects the multi-view data into a unified space by maximizing their correlations (Hotelling, 1992; Horst, 1961; Hardoon et al., 2004; Lahat et al., 2015; Yan et al., 2023; Sun et al., 2023). It has been widely applied in various scenarios that involve multi-view data, including dimension reduction (Zhang, Zhang, Pan & Zhang, 2016; Sun, Ceran & Ye, 2010; Avron et al., 2013), classification (Kim et al., 2007; Sun, Ji & Ye, 2010), and clustering (Fern et al., 2005; Chang & Lin, 2011). To further enhance the nonlinear transformability of CCA, Kernel CCA (KCCA) uses kernel methods, while Deep CCA (DCCA) employs DNNs. Since DNNs is parametric and can take advantage of large amounts of data for training, numerous DCCA-based methods have been proposed. Benton et al. (2017) utilizes DNNs to optimize the objective of Generalized CCA, to reveal connections between multiple views more effectively. To better preserve view-specific information, Wang et al. (2015) introduces the reconstruction errors of autoencoders to DCCA. Going a step further, Wang et al. (2016) proposes Variational CCA and utilizes dropout and private autoencoders to project common and view-specific information into two distinct spaces. Furthermore, many studies are exploring efficient methods for computing the correlations between multi-view data when dealing with more than two views such as MCCA, GCCA, and TCCA (Horst, 1961; Nielsen, 2002; Kettenring, 1971; Hwang et al., 2021). Some research focuses on improving the efficiency of computing CCA by avoiding the need for singular value decomposition (SVD) (Chang et al., 2018; Chapman et al., 2022). However, the model collapse issue of DCCA-based methods has not been explored and addressed.

### Noise Regularization

Noise regularization is a pluggable approach to regularize the neural networks during training (Bishop, 1995; An, 1996; Sietsma & Dow, 1991; Gong et al., 2020). In supervised tasks, Sietsma & Dow (1991) might be the first to propose that, by adding noise to the train data, the model will generalize well on new unseen data. Moreover, Bishop (1995), Gong et al. (2020) analyze the mechanism of the noise regularization, and He et al. (2019), Gong et al. (2020) indicate that noise regularization can also be used for adversarial training to improve the generalization of the network. In unsupervised tasks, Poole et al. (2014) systematically explores the role of noise injection at different layers in autoencoders, and distinct positions of noise perform specific regularization tasks. However, how to make use of noise regularization for DCCA-based methods, especially for preventing model collapse, has not been studied.

Preliminaries

In this section, we will explain the objectives of the MVRL and then introduce Linear CCA and DCCA as representatives of the CCA-based methods and DCCA-based methods, respectively. Lastly, the model collapse issue in DCCA is demonstrated.

### Settings for MVRL

Suppose the set of datasets from \(K\) different sources that describe the same object is represented by \(X\), and we define \(X=\{X_{1},,X_{k},,X_{K}\},X_{k}^{d_{k} n}\), where \(x_{k}\) represents the \(k\)-th view (\(k\)-th data source), \(n\) is the sample size, and \(d_{k}\) represents the feature dimension for the \(k\)-th view. And we use \(X_{k}^{}\) to denote the transpose of \(X_{k}\). We take the Caltech101 dataset as an example and the training set has 6400 images. One image has been fed to three different feature extractors producing three features: a 1984-d HOG feature, a 512-d GIST feature, and a 928-d SIFT feature. Then for this dataset, we have \(X_{1}^{1984 6400}\), \(X_{2}^{512 6400}\), \(X_{3}^{928 6400}\).

The objective of MVRL is to learn a transformation function \(\) that projects the multi-view data \(X\) to a unified representation \(Z^{m n}\), where \(m\) represents the dimension of the representation space, as shown below:

\[Z=(X)=(X_{1},,X_{k},,X_{K}).\] (1)

After applying \(\) for representation learning, we expect that the performance of using \(Z\) would be better than directly using \(X\) for various downstream tasks.

### Canonical Correlation Analysis

Among various MVRL methods, CCA projects the multi-view data into a common space by maximizing their correlations. We first define the correlation between the two views as follows:

\[(W_{1}X_{1},W_{2}X_{2})=((_{11}^{-1/2}_{12} _{22}^{-1/2})^{}_{11}^{-1/2}_{12}_{22}^{-1/2})^{ 1/2}\] (2)

where tr denotes the matrix trace, \(_{11}\), \(_{22}\) represent the self-covariance matrices of the projected views, and \(_{12}\) is the cross-covariance matrix between the projected views (D'Agostini 1994, Andrew et al. 2013). The correlation between the two projected views can be regarded as the sum of all singular values of the normalized cross-covariance (Hotelling 1992, Anderson et al. 1958).

For multiple views, their correlation is defined as the summation of all the pairwise correlations (Nielsen 2002, Kettenring 1971), which is shown as follows:

\[(W_{1}X_{1},,W_{k}X_{k},,W_{K}X_{K})=_{k<j}(W_{k}X_{k},W_{j}X_{j}).\] (3)

Essentially, Linear CCA searches for the linear transformation matrices \(\{W_{k}\}_{k}\) that maximize correlation among all the views. Mathematically, it can be represented as follows (Wang et al. 2015):

\[\{W_{k}^{*}\}_{k}=_{\{W_{k}\}_{k}}(W_{1}X_{1},,W_{k} X_{k},,W_{K}X_{K}).\] (4)

Once \(W_{k}^{*}\) is obtained by backpropagation, the multi-view data are projected into a unified space. Lastly, all projected data are concatenated to obtain \(Z=[W_{1}^{*}X_{1};;W_{k}^{*}X_{k};;W_{K}^{*}X_{K}]\) for downstream tasks.

As an extension of linear CCA, DCCA employs neural networks to capture the nonlinear relationship among multi-view data. The only difference between DCCA and Linear CCA is that the linear transformation matrix \(W_{k}\) is replaced by multi-layer perceptrons (MLP). Specifically, each \(W_{k}\) is replaced by a neural network \(f_{k}\), which can be viewed as a nonlinear transformation. Similar to Linear CCA, the goal of DCCA is to solve the following optimization problem:

\[\{f_{k}^{*}\}_{k}=_{\{f_{k}\}_{k}}(f_{1}(X_{1}), ,f_{k}(X_{k}),,f_{K}(X_{K})).\] (5)

The parameters in Linear CCA and DCCA are both updated through backpropagation (Andrew et al. 2013, Wang et al. 2015). Again, the unified representation is obtained by \(Z=[f_{1}^{*}(X_{1});;f_{k}^{*}(X_{k});;f_{K}^{*}(X_{K})]\) for downstream tasks.

Model Collapse of DCCA

Despite exhibiting promising performance, DCCA shows a significant decline in performance as the training proceeds. We define this decline-in-performance phenomenon as the model collapse of DCCA.

Previous studies found that the representations (i.e., final output) of both Linear CCA and DCCA are full-rank (Andrew et al., 2013; De Bie and De Moor, 2003). However, we further demonstrate that both representations and weight matrices of Linear CCA are full-rank whereas DCCA only guarantees that representations are full-rank but not for the weight matrices. Given that Linear CCA has only a single layer of linear transformation \(W_{k}\) and the representations \(W_{k}X_{k}\) are constrained to be full-rank by the loss function, \(W_{k}\) in Linear CCA is full-rank (referred to Lemma 4 and assume that \(W_{k}\) is a square matrix and \(X_{k}\) is full-rank). As for DCCA, we consider a simple case when \(f_{k}(X_{k})=Relu(W_{k}X_{k})\), and \(f_{k}\) is a single-layer network and uses an element-wise Relu activation function. Only the representations \(Relu(W_{k}X_{k})\) are constrained to be full-rank, and hence we cannot guarantee that \(W_{k}X_{k}\) is full-rank. For example, when \(Relu(W_{k}X_{k})=(1,\ 0\\ 0,\ 1)\), it is clear that this is a matrix of rank 2, but in fact \(W_{k}X_{k}\) can be \((1,\ 1\\ -1,\ 1)\), and this is not full-rank. This reveals that the neural network \(f_{k}\) is overfitted on \(X_{k}\), i.e., making representations \(Relu(W_{k}X_{k})\) to be full-rank with the constraint of its loss function, rather than \(W_{k}\) itself being full-rank (verified in Appendix A.5.1).

Thus, we hypothesize that model collapse in DCCA arises primarily due to the low-rank nature of the DNN weight matrices. To investigate this, we analyze the eigenvalue distributions of the first linear layer's weight matrices in both DCCA and NR-DCCA across various training epochs on synthetic datasets. Figure 1 illustrates that during the initial training phase (100th epoch), the eigenvalues decay slowly for both DCCA and NR-DCCA. However, by the 1200th epoch, DCCA exhibits a markedly faster decay in eigenvalues compared to NR-DCCA. This observation suggests a synchronization between model collapse in DCCA and increased redundancy of the weight matrices. For more details on the experimental setup and results, please refer to Section 6.2.

Figure 1: Eigenvalue distributions of the first linear layer’s weight matrices in the encoder of \(1\)-st view.

DCCA with Noise Regularization (NR-DCCA)

### Method

Based on the discussions in previous sections, we present NR-DCCA, which makes use of the noise regularization approach to prevent model collapse in DCCA. Indeed, the developed noise regularization approach can be applied to variants of DCCA methods, such as Deep Generalized CCA (DGCCA) (Benton et al. 2017). An overview of the NR-DCCA framework is presented in Figure 2.

The key idea in NR-DCCA is to generate a set of i.i.d Gaussian white noise, denoted as \(A=\{A_{1},,A_{k},,A_{K}\},A_{k}^{d_{k} n}\), with the same shape as the multi-view data \(X_{k}\). In Linear CCA, the correlation with noise is invariant to the linear transformation \(W_{k}\): \((X_{k},A_{k})=(W_{k}X_{k},W_{k}A_{k})\) (rigorous proof provided in Theorem 1). However, for DCCA, \((X_{k},A_{k})\) might not equal \((f_{k}(X_{k}),f_{k}(A_{k}))\) because the powerful neural networks \(f_{k}\) have overfitted to the maximization program in DCCA and the weight matrices have been highly self-related. Therefore, we enforce the DCCA to mimic the behavior of Linear CCA by adding an NR loss \(_{k}=|Corr(f_{k}(X_{k}),f_{k}(A_{k}))-Corr(X_{k},A_{k})|\), and hence the formulation of NR-DCCA is:

\[\{f_{k}^{*}\}_{k}=_{\{f_{k}\}_{k}}(f_{1}(X_{1}), ,f_{K}(X_{K}))-_{k=1}^{K}_{k}.\] (6)

where \(\) is the hyper-parameter weighing the NR loss. NR-DCCA can be trained through backpropagation with the randomly generated \(A\) in each epoch, and the unified representation is obtained directly using \(\{f_{k}^{*}\}_{k}\) in the same manner as DCCA.

### Theoretical Analysis

In this section, we provide the rationale for why the developed noise regularization can help to prevent the weight matrices from being low-rank and thus model collapse. Moreover, we prove the effect of full-rank weight matrices on the representations, which provides a tool to empirically verify the full-rank property of weight matrices by the quality of representations.

Utilizing a new Moore-Penrose Inverse (MPI)-based (Petersen et al. 2008) form of \(Corr\) in CCA, we discover that the full-rank property of \(W_{k}\) is equal to CIP:

**Theorem 1** (Correlation Invariant Property (CIP) of \(}\)): _Given \(W_{k}\) is a square matrix for any \(k\) and \(_{k}=|Corr(W_{k}X_{k},W_{k}A_{k})-Corr(X_{k},A_{k})|\), we have \(_{k}=0\) (i.e. CIP) \( W_{k}\) is full-rank._

Figure 2: Illustration of NR-DCCA. We take the CUB dataset as an example: similar to DCCA, the \(k\)-th view \(X_{k}\) is transformed using \(f_{k}\) to obtain new representation \(f_{k}(X_{k})\) and then maximize the correlation between new representations. Additionally, for the \(k\)-th view, we incorporate the proposed NR loss to regularize \(f_{k}\).

Similarly, we say \(f_{k}\) possess CIP if \(_{k}=0\). Under Linear CCA, it is redundant to introduce the NR approach and force \(W_{k}\) to possess CIP, since forcing \(W_{k}X_{k}\) to be full-rank is sufficient to ensure that \(W_{k}\) is full-rank. However, in DCCA, \(f_{k}\) is overfitted on \(X_{k}\), i.e., making representations \(f_{k}(X_{k})\) to be full-rank, rather than weight matrices in \(f_{k}\) being full-rank. By forcing \(f_{k}\) to possess CIP and thus mimicking the behavior of Linear CCA, the NR approach constrains the weight matrices to be full-rank and less redundant and thus prevents model collapse.

Next, we show that full-rank weight matrices (i.e., CIP) can greatly affect the quality of representations.

**Theorem 2** (Effects of CIP on the obtained representations): _For any \(k\), if \(W_{k}\) is a square matrix and CIP holds for \(W_{k}\) (i.e. \(W_{k}\) is full-rank), \(W_{k}X_{k}\) holds that:_

\[_{P_{k}}\|P_{k}W_{k}X_{k}-X_{k}\|_{F}=0\] (7)

\[_{Q_{k}}\|Q_{k}W_{k}(X_{k}+A_{k})-W_{k}X_{k}\|_{F}\|W_{k}A_{k} \|_{F},E(\|W_{K}A_{k}\|_{F}^{2})=\|W_{k}\|_{F}^{2}\] (8)

_where \(\|\|_{F}\) denotes the Frobenius norm and \(\) is a small positive threshold. \(P_{k}\) and \(Q_{k}\) are searched weight matrices of \(k\)-th view to recover the input and discard noise, respectively. And we refer \(\|P_{k}W_{k}X_{k}-X_{k}\|_{F}\) and \(\|Q_{k}W_{k}(X_{k}+A_{k})-W_{k}X_{k}\|_{F}\) as reconstruction loss and denosing loss._

Theorem 2 suggests that the obtained representation is of low reconstruction loss and denoising loss. Low reconstruction loss suggests that the representations can be linearly reconstructed to the inputs. This implies that \(W_{k}\) preserves distinct and essential features of the input data, which is a desirable property to avoid model collapse since it ensures that the model captures and retains the whole modality of data (Zhang, Liu & Fu 2019, Tschannen et al. 2018, Tian & Zhang 2022). Low denoising loss implies that the model's representation is robust to noise, which means that small perturbations in the input do not lead to significant changes in the output. This condition can be seen as a form of regularization that prevents overfitting the noise in the data (Zhou & Paffenroth 2017, Yan et al. 2023, Staerman et al. 2023). Additionally, the theorem also suggests that the rank of weight matrices is a good indicator to assess the quality of representations, which coincides with existing literature (Kornblith et al. 2019, Raghu et al. 2021, Garrido et al. 2023, Nguyen et al. 2020, Agrawal et al. 2022).

## 6 Numerical Experiments

We conduct extensive experiments on both synthetic and real-world datasets to answer the following research questions:

* **RQ1:** How can we construct synthetic datasets to evaluate the MVRL methods comprehensively?
* **RQ2:** Does NR-DCCA avoid model collapse across all synthetic MVRL datasets?
* **RQ3:** Does NR-DCCA perform consistently in real-world datasets?

We follow the protocol described in Hwang et al. (2021) for evaluating the MVRL methods. For each dataset, we construct a training dataset and a test dataset. The encoders of all MVRL methods are trained on the training dataset. Subsequently, we encode the test dataset to obtain the representation, which will be evaluated in downstream tasks. We employ Ridge Regression (Hoerl & Kennard 1970) for the regression task and use \(R2\) as the evaluation metric. For the classification task, we use a Support Vector Classifier (SVC) (Chang & Lin 2011) and report the average F1 scores. All tasks are evaluated using 5-fold cross-validation, and the reported results correspond to the average values of the respective metrics.

For a fair comparison, we use the same architectures of MLPs for all D(G)CCA methods. To be specific, for the synthetic dataset, which is simple, we employ only one hidden layer with a dimension of 256. For the real-world dataset, we use MLPs with three hidden layers, and the dimension of the middle hidden layer is 1024. We further demonstrate that increasing the depth of MLPs further accelerates the mod collapse of DCCA, while NR-DCCA maintains a stable performance in Appendix A.7.

Baseline methods include **CONCAT**, **PRCCA**(Tuzhilina et al., 2023), **KCCA**(Akaho, 2006), **Linear CCA**Wang et al. (2015),**Linear GCCA**,**DCCA**(Andrew et al., 2013),**DCCA**.**EY**, **DCCA**.**GHA**(Chapman et al., 2022), **DGCCA**(Benton et al., 2017), **DCCAE**/**DGCCAE**(Wang et al., 2015), **DCCA**.**PRIVATE**/**DGCCA**.**PRIVATE**(Wang et al., 2016), and **MVTCAE**(Hwang et al., 2021).

It is important to note that our proposed NR approach requires the noise matrix employed to be full-rank, which is compatible with several common continuous noise distributions. In our primary experiments, we utilize Gaussian white noise. Additionally, as demonstrated in Appendix A.6, uniformly distributed noise is also effective in our NR approach.

Details of the experiment settings including datasets and baselines are presented in Appendix A.3. Hyper-parameter settings, including ridge regularization of DCCA, \(\) of NR, are discussed in Appendix A.5. We also analyze the computational complexity of different DCCA-based methods in Appendix A.12 and the learned representations are visualized in Appendix A.9. In the main paper, we mainly compare Linear CCA, DCCA-based methods, and NR-DCCA while other MVRL methods are discussed in Appendix A.11. The results related to DGCCA and are similar and presented in Appendix A.10.

### Construction of synthetic datasets (RQ1)

We construct synthetic datasets to assess the performance of MVRL methods, and the framework is illustrated in Figure 3. We believe that the multi-view data describes the same object, which is represented by a high-dimensional embedding \(G^{d n}\), where \(d\) is the feature dimension and \(n\) is the size of the data, and we call it God Embedding. Each view of data is regarded as a non-linear transformation of part (or all) of \(G\). For example, we choose \(K=2,d=100\), and then \(X_{1}=_{1}(G[0:50+/2,:]),X_{2}=_{x}(G[50-/2:100],:)\), where \(_{1}\) and \(_{2}\) are non-linear transformations, and \(CR\) is referred to as common rate. The common rate is defined as follows:

**Definition 1** (Common Rate): _For two view data \(X=\{X_{1},X_{2}\}\), common rate is defined as the percentage overlap of the features in \(X_{1}\) and \(X_{2}\) that originate from \(G\)._

One can see that the common rate ranges from \(0\%\) to \(100\%\). The larger the value, the greater the correlation between the two views, and a value of \(0\) indicates that the two views do not share any common dimensions in \(G\). Additionally, we construct the downstream tasks by directly transforming the God Embedding \(G\). Each task \(T_{j}=_{j}(G)\), where \(_{j}\) is a transformation, and \(T_{j}\) represents the \(j\)-th task. By setting different \(G\), common rates, \(_{k}\), and \(_{j}\), we can create various synthetic datasets to evaluate the MVRL methods. Finally, \(X_{k}\) are observable to the MVRL methods for learning the representation, and the learned representation will be used to classify/regress \(T_{j}\) to examine the performance of each method. Detailed implementation is given in Appendix A.4.

### Performance on Synthetic Datasets (RQ2)

We generate the synthetic datasets with different common rates, and the proposed NR-DCCA and other baseline methods are compared. As shown in Figure 4, one can see that the DCCA-based

Figure 3: Construction of a synthetic dataset. This example consists of \(2\) views and \(n\) objects, and the common rate is \(0\%\).

methods (e.g. DCCA, DCCAE, DCCA_PRIVATE) will encounter model collapse during training, and the variance of accuracy also increases. Linear CCA demonstrates stable performance, while the best accuracy is not as good as DCCA-based methods. Our proposed NR-DCCA achieves state-of-the-art performance as well as training stability to prevent model collapse. The results at the final epoch for all common rates are also presented in Table 3 in Appendix A.11.

Considering that we believe that the low-rank property (i.e. highly self-related and redundant) of the weight matrices is the root cause of the model collapse, we utilize NESum to measure the correlation among filters in the weight matrices ( defined in A.8). Higher NESum represents lower redundancy in weight matrices. As shown in (b) of Figure 4, our findings demonstrate that the NR approach effectively reduces filter redundancy, thereby preventing the emergence of low-rank weight matrices and thus averting model collapse.

Figure 4: (a) Mean and standard deviation of the (D)CCA-based method performance across synthetic datasets in different training epochs. (b) The mean correlation between noise and real data after transformation varies with epochs. (c) Average NESum across all weights within the trained encoders. (d,e) The mean of reconstruction and denoising loss on the test set.

Moreover, according to our analysis, the correlation should be invariant if neural networks have CIP. Therefore, after training DCCA, DCCAE, and NR-DCCA, we utilize the trained encoders to project the corresponding view data and randomly generated Gaussian white noise and then compute their correlation, as shown in (c) of Figure 4. It can be observed that, except for our method (NR-DCCA), as training progresses, other methods increase the correlation between unrelated data. It should be noted that this phenomenon always occurs under any common rates.

Given that the full-rank weight matrix not only produces features that are linearly reconstructed but also discriminates noise in the inputs, we also present the mean value pf Reconstruction and Denoising Loss across different common rates in (d) of Figure 4. Notably, NR-DCCA achieves a markedly lower loss, comparable to that observed with Linear CCA, whereas alternative DCCA-based approaches generally lose the above properties.

### Consistent Performance on Real-world Datasets (RQ3)

We further conduct experiments on three real-world datasets: **PolyMnist**(Sutter et al., 2021), **CUB**(Wah et al., 2011), **Caltech**(Deng et al., 2018). Additionally, we use a different number of views in PolyMnist. The results are presented in Figure 5, and the performance of the final epoch in the figure is presented in Table 3 in the Appendix A.11. Generally, the proposed NR-DCCA demonstrates a competitive and stable performance. Different from the synthetic data, the DCCA-based methods exhibit varying degrees of collapse on various datasets, which might be due to the complex nature of the real-world views.

## 7 Conclusions

We propose a novel noise regularization approach for DCCA in the context of MVRL, and it can prevent model collapse during the training, which is an issue observed and analyzed in this paper for the first time. Specifically, we theoretically analyze the CIP in Linear CCA and demonstrate that it is the key to preventing model collapse. To this end, we develop a novel NR approach to equip DCCA with such a property (NR-DCCA). Additionally, synthetic datasets with different common rates are generated and tested, which provide a benchmark for fair and comprehensive comparisons of different MVRL methods. The NR-DCCA developed in the paper inherits the merits of both Linear CCA and DCCA to achieve stable and consistent outperformance in both synthetic and real-world datasets. More importantly, the proposed noise regularization approach can also be generalized to other DCCA-based methods (_e.g._, DGCCA).

In future studies, we wish to explore the potential of noise regularization in other representation learning tasks, such as contrastive learning and generative models. It is also interesting to further investigate the difference between our developed NR and other neural network regularization approaches, such as orthogonality regularization (Bansal et al., 2018; Huang et al., 2020) and weight decay (Loshchilov and Hutter, 2017; Zhang et al., 2018; Krogh and Hertz, 1991). Our ultimate goal is to make the developed noise regularization a pluggable and useful module for neural network regularization.

Figure 5: Performance of different methods in real-world datasets. Each column represents the performance on a specific dataset. The number of views in the dataset is denoted in the parentheses next to the dataset name.