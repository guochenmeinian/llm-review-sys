# Stochastic Gradient MCMC for Gaussian Process Inference on Massive Geostatistical Data

Mohamed A. Abba\({}^{1}\)   Brian J. Reich\({}^{1}\)   Reetam Majumder\({}^{2*}\)   Brandon Feng\({}^{1}\)

\({}^{1}\)Department of Statistics, North Carolina State University

\({}^{2}\)Department of Mathematical Sciences, University of Arkansas

\({}^{*}\)reetamm@uark.edu

###### Abstract

Gaussian processes (GPs) are the workhorses of spatial data analyses, but are difficult to scale to large spatial datasets. The Vecchia approximation induces sparsity in the dependence structure and is one of several methods proposed to scale GP inference. We develop a stochastic gradient Markov chain Monte Carlo framework for efficient computation in GPs for spatial data. At each step, the algorithm subsamples a minibatch of locations and subsequently updates process parameters through stochastic gradient Riemannian Langevin dynamics (SGRLD) on a Vecchia-approximated GP likelihood. We are able to conduct full Bayesian analysis for GPs with up to 100,000 locations using our spatial SGRLD, and demonstrate its efficacy through numerical studies and an application using ocean temperature data.

## 1 Introduction

Gaussian process (GP) modeling is a powerful statistical and machine learning tool used to tackle a variety of tasks including regression, classification, and optimization. Within spatial statistics, in particular, GPs have become the primary tool for inference , with their main advantage being the ability to provide predictions at unobserved locations along with uncertainty quantification. However, handling large datasets with GPs poses computational challenges due to the cubic time complexity and quadratic memory requirements to evaluate the joint likelihood. This is compounded in the course of Bayesian inference, where thousands of Markov chain Monte Carlo (MCMC) iterations are needed to accurately approximate the posterior distribution. Scalable computation for GPs is therefore necessary for inference on large spatial datasets.

Stochastic gradient (SG) based optimization, where gradient information is used to sample the posterior efficiently, has emerged as an attractive alternative to regular MCMC for scalable computation. Instead of computing a costly gradient based on the full dataset, SG methods only need an unbiased and possibly noisy estimate using a subsample of the data. Although SGMCMC is widely used for _iid_ data , a naive application in the correlated setting would overlook critical dependencies in the data during subsampling. Moreover, the gradient estimate from the subsamples are not guaranteed to be unbiased. SGMCMC has been used for certain classes of dependent data , but to the best of our knowledge, subsampling methods for spatial data that result in unbiased gradient estimates have not been explored. In this work, we develop an SGMCMC algorithm based on Langevin dynamics (SGLD) for large spatial datasets, assumed to have a Matern correlation structure . We extend the SGLD method to the case of non-_iid_ data using the Vecchia approximation that substantially reduces the computational cost to provide a method that takes account of the local curvature to improve convergence.

## 2 Methodology

### The Matern GP and the Vecchia approximation

Let \(Y_{i}\) for \(i\{1,...,n\}\) be the observation at a spatial location \(_{i}=(s_{i1},s_{i2})\), and let \(_{i}=(X_{i1},...,X_{ip})\) be a corresponding vector of covariates. The data-generation model for GP regression in the case of Gaussian data is

\[Y_{i}=_{i}+Z_{i}+_{i}, \]

with covariate effects \(\), spatial process \(Z_{i} Z(_{i})\), and measurement error \(_{i}}{{}}(0,^{2})\) with a nugget \(^{2}\). The process \(Z()\) is an isotropic spatial Gaussian process with mean \(\{Z()\}=0\), spatial variance \(\{Z()\}=^{2}\) and spatial correlation \(\{Z_{i},_{j}\}=(d_{ij})\) for distance \(d_{ij}=||_{i}-_{j}||\). Specifically, we assume that the process has a Matern correlation function  with range \(\) and smoothness \(\):

\[(d)=}()^{} _{}(), \]

where \(_{}\) is the modified Bessel function of the second kind. Let \(=(^{2},,,^{2})\) be the collection of covariance parameters. The marginal distribution (over \(Z\)) of \(=\{Y_{1},,Y_{n}\}\) is multivariate normal with mean \([]=_{i}\) for \(^{n p}\) covariate matrix with the i\({}^{}\) row \(_{i}\), and covariance matrix \([(-)(- )^{}]=()\) with

\[() =^{2}+^{2}_{n}, \] \[_{i,j} =(d_{ij}).\]

The full log-likelihood for the process is given by:

\[_{}(,)=-(2 )-()-(- )^{}()^{-1}( -). \]

Evaluating the full likelihood of the process involves computing the determinant and inverse of \(()\) which generally requires \(O(n^{3})\) operations, and becomes prohibitive for large spatial datasets. To alleviate this, we write the joint distribution of \(\) as a product of univariate conditional distributions, which can then be approximated by a Vecchia approximation [29; 28; 8; 16]:

\[f(Y_{1},...,Y_{n})=_{i=1}^{n}f(Y_{i}|Y_{1},...,Y_{i-1})_{i=1 }^{n}f_{i}(Y_{i}|Y_{(i)}), \]

for \(Y_{(i)}=\{Y_{j};j_{i}\}\) and conditioning set \(_{i}\{1,...,i-1\}\), e.g., the indices of the \(m_{i} m\) locations in \(_{i}\) that are closest to \(_{i}\) according to some ordering of the data. Conditioning on \(_{i}\) leads to substantial computational savings when \(m\) is small, _i.e._, \(m<<n\). Let \(p(,)\) be the prior distribution on the regression and covariance parameters. Using (5) we can write the posterior \(p(,)\) (ignoring a constant that does not depend on the parameters) as:

\[(,) =_{i=1}^{n} f(Y_{i} Y_{(i)},, ),\] \[ p(,) =(,)+ p( ,). \]

The log-likelihood and log-posterior of the parameters \(\{,\}\) can consequently be written as a sum of conditional normal log-densities, where the conditioning set is at most of size \(m\).

### SGLD and SGRLD for spatial data

The Vecchia approximation reduces the computational cost for evaluating the full likelihood and the posterior from \(O(n^{3})\) to \(O(nm^{3})\); however, this can still pose challenges for very large \(n\). We can further reduce the cost of Bayesian inference by using subsampling strategies. Note that sampling the summands of (6) with equal probability and without replacement leads to an unbiased estimate of the gradient. Let \(\{1,,n\}\) be a subsample, _i.e._, a minibatch index set of size \(n_{}\), and let

\[_{}(,)=}}_{i} f(Y_{i} Y_{(i)},,). \]

**Theorem 1**.: _The gradient of \(_{}\) is an unbiased estimator of the gradient of the Vecchia likelihood \((,)\)._

Proof.: \[_{}[_{}(, )] =\,_{}[}} _{i=1}^{n} f(Y_{i} Y_{(i)},,)_{i }]\] \[=_{i=1}^{n} f(Y_{i} Y_{(i)},,)\] \[=(,).\] (8)

Using (8), we can construct an unbiased estimate of the gradient of the Vecchia log-posterior based on a minibatch of the data:

\[_{}(,)=_{ }(,)+ p(,), \]

reducing the cost of learning iterations to be linear in \(n_{}\) instead of \(n\), _i.e._, \(O(m^{3}n_{})\).

SGMCMC proceeds by simulating continuous dynamics of a potential energy, namely the negative log-posterior, \(- p(,)\), in a manner that generates samples from the posterior distribution. Let \(=(^{},^{})^{}\) be the vector of all parameters for the GP regression model. The Langevin diffusion over \( p()\) is given by the stochastic differential equation

\[d(_{t})= p(_{t})dt+ dW_{t}, \]

where \(dW_{t}\) is Brownian motion and the index \(t\) represents time. The distribution of samples \(_{t}\) converges to the true posterior as \(t\). Since simulating a continuous time process is infeasible in practice, we use the Euler discretization method to approximate the Langevin dynamics:

\[_{t+1}=_{t}+h_{t} p(_{t} )+}e_{t}, \]

where \(h_{t}\) is the step size at time \(t\), \(_{t}\) the current value of the parameter, and \(e_{t}\) is random white noise. This recursive sampling approach is known as the Langevin Monte Carlo algorithm. Often, a Metropolis-Hastings (MH) correction step is added to account for the discretization error.

Computing the gradient of the log-posterior for large \(n\) represents a computational bottleneck. To overcome this problem, the key idea of stochastic gradient Langevin dynamics (SGLD) is to replace \( p()\) with an unbiased gradient estimate, _i.e._, \(_{}()\) in (9), that is computationally cheaper to compute and uses a decreasing step size \(h_{t}\) to avoid the costly MH correction steps,

\[:_{t+1}=_{t}+h_{t}_{ }(_{t})+}e_{t}, \]

for positive step sizes that satisfy the Robbins-Monro conditions . Note that (12) updates all parameters using the same step size, which can cause slow mixing when different parameters have different curvature or scales. Stochastic gradient Reimannian Langevin dynamics (SGRLD) accounts for differences in curvature and scale by using an appropriate Riemannian metric \(G()\) and preconditioning the unbiased gradient and noise in (12) using \(G^{-1}()\). Commonly used metrics for \(G()\) include the Fisher information matrix and estimates of the Hessian of the log-posterior. Given a preconditioning matrix \(G()\), the SGRLD step is

\[:_{t+1}=_{t}+h_{t}(G^{-1}(_{t})_{}(_{t})+(_{t}))\;+ }G^{-1/2}(_{t})e_{t}, \]

where the term \((_{t})\) represents the drift term that describes how the preconditioner \(G(_{t})\) changes with respect to \(_{t}\). The drift term is given by

\[(_{t})_{i}=_{j}_{t}) _{ij}^{-1}}{_{tj}}. \]

The drift term vanishes in the SGLD step since the preconditioner is assumed to be the identity matrix. The SGRLD algorithm in (13) takes steps in the steepest ascent on the manifold defined by the metric \(G(_{t})\). While the Fisher information matrix is often intractable, our use of the Vecchia approximation facilitates computation of the Fisher information and its inverse without incurring a high computational cost; derivations of the expressions are provided in Appendix A. Code for our approach is available in the form of an R package on GitHub .

## 3 Results

We tested the efficacy of our proposed SGRLD method in (13) using a numerical study and assessed its performance against four state-of-the-art Bayesian methods. The first three are SG methods with adaptive drifts. The last method is the nearest neighbor Gaussian process (NNGP)  that uses the full dataset to sample the posterior distribution using the Vecchia approximation. The methods were compared for datasets with \(n=\{10^{4},10^{5},10^{6}\}\) locations. Study details are provided in the Appendix B.1. SGRLD outperformed the competing methods with very low MSE across parameters. Additionally, all SG methods outperformed NNGP. SGRLD had consistently high coverage for \(95\%\) credible intervals and overall the highest expected sample size per minute (ESS/min) among all methods.

We also applied the proposed method to the ocean temperature data provided by the Argo Program  made available through the GpGP package in R. Each of the \(n=32,436\) observations are taken on buoys in the spring of 2016, and measures ocean temperature (C) at depths of roughly 100, 150, and 200 meters. The data are plotted in Figure 1 for a depth of 100 meters. As an illustrative example, the mean function is taken to be quadratic in latitude and longitude. All prior distributions and MCMC settings are the same as in the numerical study in Section B.1.

We set aside \(20\%\) of all observations as the testing set, and train the models using \(8000\) and \(40000\) MCMC iterations for the NNGP and SGRLD methods respectively. Table 1 gives the MSE and coverage rate on the testing set, and total training time respectively. SGRLD results in less than a quarter of the MSE of NNGP while also requiring less than a twentieth of the time. For the coverage of the \(95\%\) prediction intervals, the NNGP method's average coverage on the testing set is significantly lower than the nominal value, while our proposed method achieves \(93\%\) coverage.

Table 2 gives the posterior mean, 95% interval and the effective sample size [(ESS), 14] per minute for the covariance parameters for SGRLD and NNGP. The posterior means and credible intervals for \(\), and to a lesser extent \(^{2}\), vary substantially across methods. The range estimates from SGRLD are almost three orders of magnitude higher than the NNGP estimate. Given the prediction results in Table 1, this indicates that the NNGP is underestimating \(\). Furthermore, for NNGP, the credible interval for \(\) has a total width of \(10^{-2}\), perhaps indicating poor convergence. We also see from Table 2 that our SGRLD method allows fast exploration of the posterior and leads to massively higher ESS

  & MSE & Coverage & \(R^{2}\) & Time (in minutes) \\  NNGP & \(6.41\) & \(0.88\) & \(0.89\) & \(218.55\) \\ SGRLD & \(1.47\) & \(0.93\) & \(0.94\) & \(7.01\) \\ 

Table 1: Prediction MSE, squared correlation between predicted and observed (\(R^{2}\)) and coverage rate of the \(95\%\) predictive credible intervals on the test set and the correlation between the predicted temperatures and true observed values. The last column gives the total training time in minutes. We take \(8000\) and \(40000\) samples using the NNGP and SGRLD method respectively.

Figure 1: Argo ocean temperature measurements at a depth of 100 meters.

per minute, while giving reasonable convergence (Figure 2). Additional plots and results, including a sensitivity study for the hyperparameters \(m\) and \(n_{}\), are provided in the Appendix B.2.

## 4 Discussion

SG methods offer considerable speed-ups when the data size is very large. This enables fast exploration of the posterior in significantly less time. GPs however fall within the correlated setting case where SGMCMC methods have received limited attention. Spatial correlation is a critical component of GPs and naive subsampling during parameter estimation would lead to random divisions of the spatial domain at each iteration. By leveraging the form of the Vecchia approximation, we derive unbiased gradient estimates based on minibatches of the data. We developed a new stochastic gradient based MCMC algorithm for scalable Bayesian inference in large spatial data settings. Without the Vecchia approximation, subsampling strategies would always lead to biased gradient estimates. The proposed method also uses the exact Fisher information to speed up convergence and explore the parameter space efficiently. Our work contributes to the literature on scalable methods for Gaussian process, and can be extended to non Gaussian models, e.g., to classification problems.