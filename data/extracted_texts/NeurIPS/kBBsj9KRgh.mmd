# SAME: Uncovering GNN Black Box with Structure-aware Shapley-based Multipiece Explanation

Ziyuan Ye\({}^{1,2,*}\), Rihan Huang\({}^{1,3,*}\), Qilin Wu\({}^{1,4}\), Quanying Liu\({}^{1,}\)

\({}^{1}\)Southern University of Science and Technology, \({}^{2}\)The Hong Kong Polytechnic University,

\({}^{3}\)King Abdullah University of Science and Technology, \({}^{4}\)Carnegie Mellon University

{ziyuanye9801, rihanhuang.work, kyrinwu}@gmail.com, liuqy@sustech.edu.cn

Equal contribution, co-first author.Corresponding author.

###### Abstract

Post-hoc explanation techniques on graph neural networks (GNNs) provide economical solutions for opening the black-box graph models without model retraining. Many GNN explanation variants have achieved state-of-the-art explaining results on a diverse set of benchmarks, while they rarely provide theoretical analysis for their inherent properties and explanatory capability. In this work, we propose Structure-Aware Shapley-based Multipiece Explanation (SAME) method to address the structure-aware feature interactions challenges for GNNs explanation. Specifically, SAME leverages an expansion-based Monte Carlo tree search to explore the multi-grained structure-aware connected substructure. Afterward, the explanation results are encouraged to be informative of the graph properties by optimizing the combination of distinct single substructures. With the consideration of fair feature interactions in the process of investigating multiple connected important substructures, the explanation provided by SAME has the potential to be as explainable as the theoretically optimal explanation obtained by the Shapley value within polynomial time. Extensive experiments on real-world and synthetic benchmarks show that SAME improves the previous state-of-the-art fidelity performance by 12.9% on BBBP, 7.01% on MUTAG, 42.3% on Graph-SST2, 38.9% on Graph-SST5, 11.3% on BA-2Motifs and 18.2% on BA-Shapes under the same testing condition. Code is available at [https://github.com/same2023heurips/samc](https://github.com/same2023heurips/samc)

## 1 Introduction

Graph neural networks (GNNs) have demonstrated a powerful representation learning ability to deal with data in non-Euclidean space. However, the explanation techniques for deep learning models on images and text cannot directly apply to understand GNNs, . There is a gap in the understanding of how GNNs work, which largely limits GNNs' application in many fields.

Many GNN explanation techniques aim to examine the extent to which GNNs depend on individual nodes and edges of the graph . However, graph features within nodes and edges contribute different amounts of information when considered individually than contextualized with topology . Therefore, discovering the most important explanation with one or more connected components given an input graph and a well-trained GNN raises the additional challenge of handling structure-aware feature interactions. Recently, a number of studies, including GNN-LRP , SubgraphX  and GStarX  have endeavored to address this issue to some extent. Although many of the current GNN explanation techniques have empirically achieved state-of-the-artexplainability performance, the design of new GNN explanation techniques mainly relies upon empirical intuition, iterative experiments, and heuristics principles. To the best of our knowledge, the explanatory capability, potential limitations and inherent properties of GNN explanation techniques have not yet been thoroughly studied from a theoretical perspective.

In this work, we propose a novel Structure-Aware Shapley-based Multipiece Explanation (SAME) technique for fairly considering the multi-level structure-aware feature interactions over the graph by introducing expansion-based Monte Carlo tree search (MCTS). Our construction is inspired by the recently proposed perturbation-based GNN explanation methods , which have proven effective for providing explainability from a cooperative game perspective. We summarize the main differences between SAME and previous work in Table**The main contributions and novelties** of this work include the following. (1) _Theoretical aspect:_ i) We review the characteristics of previous methods  and highlight several desired properties (see Table 1 that can be considered by explanation methods for GNNs. ii) We provide the loss bound of the MCTS-based explanation techniques and further verify the superiority of our expansion-based MCTS in SAME compared to previous work  in an intuitive manner (Sec. 3.2). (2) _Empirical aspect:_ Our experiments cover both real-world and synthetic datasets. The results show that i) SAME outperforms previous SOTA with fidelity and harmonic fidelity metrics under the same testing condition (Sec. 5.1). ii) SAME qualitatively achieves a more human-intuitive explanation compared to previous methods across multiple datasets (Sec. 5.2).

## 2 Related Work

Improving the explainability of GNN models in a post-hoc fashion has been a theme in deep graph learning. An intuitive way to explain a well-trained GNN is to trace the gradient in the models , where the larger gradient indicates the higher importance of node or edge of the graph. Previous work has also studied the decomposition-based methods  which decompose the final prediction into several terms and mark them as the important scores for input features. Another line of GNN explanation techniques lies in perturbation-based methods  which usually obtain a mask for the input graph in various ways to identify the important input feature. SubgraphX , as one of a perturbation-based method, samples the subgraphs from the input graph by the pruning-based MCTS and finds the most important one via the Shapley value. However, the pruning-based MCTS in SubgraphX leads to a much larger search space and thus causes higher computational costs. Moreover, SubgraphX can only provide a single connected explanation for each graph, which limits its explanatory power in many scenarios that require multipiece explanation. Most recently, GStarX  scores node importance based on the Hamiache-Navarro (HN) value. Although GStarX also fairly considers the structure-aware feature interactions, it fails to account for the multi-grained importance, which might result in a suboptimal explanation. For other categories in GNN explanation techniques, including surrogate methods , generation-based methods , and counterfactual-based methods , we refer readers to a recent survey .

   &  Grad \\ CAM \\  } &  &  &  &  &  &  \\  & & & & & & &  &  \\  & & & & & &  &  &  & \\  Graph-level tasks & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\ Node-level tasks & & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\  Feature interactions & & & & & & ✓ & ✓ & ✓ \\ Structure awareness & & & & & ✓ & ✓ & ✓ & ✓ \\ Multipiece explanation & ✓ & ✓ & ✓ & ✓ & & ✓ & ✓ \\  Node-wise importance & ✓ & & & & & ✓ & ✓ \\ Substructure-wise importance & & & & ✓ & ✓ & & ✓ \\ Composite-wise importance & & ✓ & ✓ & ✓ & ✓ & & ✓ \\ Priority-based integration & & ✓ & ✓ & & ✓ & & ✓ \\ Redundancy consideration & & ✓ & ✓ & & ✓ & ✓ \\   Note: _Feature interactions_ and _structure awareness_ are discussed in Sec. 3.1_Multipiece explanation_ is provided in Sec. 3.2_Multi-grained importance (node / substructure / composite), priority-based integration_ and _redundancy consideration_ are presented in Sec. 4 Detailed mathematical definitions are provided in Appendix B.1.

Table 1: Comparison of the properties of different GNN explanation techniques.

Theoretical Motivation

**Notation and Preliminaries**. The well-trained target GNN to be explained can be formulated as \(f:\), where \(\) denotes the space of input graphs and \(\) refers to the related label space. A graph can be denoted as \(G=(V,X,E)\), where \(V^{n n}\) represents node set, \(X^{n d}\) is the node feature set and \(E^{n n}\) denotes edge set. Given a well-trained GNN model \(f()\) and an input graph \(G\), the goal of GNN explanation is to find the most important explanation \(G^{*}_{ex}\) from \(G\). Formally, this can be defined as an optimization problem that maximizes the importance of the explanation \(G_{ex}\) for a given graph \(G\) with \(n\) nodes using an importance scoring function \(I(f(),G_{ex},G)\):

\[G^{*}_{ex}=*{arg\,max}_{G_{ex} G}\;I(f(),G_{ex},G), \]

where each explanation \(G^{i}_{ex}\) has \(n_{i}\) nodes, and the other nodes not in the explanation can be expressed as \(\{G G^{i}_{ex}\}=\{v_{j}\}_{j=n_{i}+1}^{n}\). It is noteworthy that each explanation \(G^{i}_{ex}\) might contain one or more substructures (_i.e._, connected components).

### Structure-aware Shapley-based Explanations Satisfies Fairness Axioms

Unlike grid-like images or sequence-like texts, graphs have more complex and abstract structures. The importance scoring function of explanations determines the reliability and explanatory power of the GNN explanation method. Therefore, we present the idea that a comprehensive assessment of the importance of an explanation should consider the feature interactions under the constraints of the input graph's topology.

Shapley value , originating from cooperative game theory, is the unique credit allocation scheme that satisfies the fairness axioms. This concept is similar to the importance scoring function for explanation with the consideration of feature interactions. Some previous work have brought Shapley value into deep learning explanation methods . In our study, the importance assessment of explanation is treated as a cooperative game, where the explanation \(G^{i}_{ex}\) and all nodes not in the explanation \(\{G G^{i}_{ex}\}\) are the players in the game. Therefore, when scoring the importance of any explanation \(G^{i}_{ex}\), a set of players participating in the game can be denoted as:

\[P_{i}=\{G^{i}_{ex},+1},v_{n_{i}+2},,v_{n}}_{\{G  G^{i}_{ex}\}}\}.\]

Inspired by the close connection between _feature interactions_ and Shapley value, we define several desirable properties of importance scoring function for explanation according to fairness axioms:

**Property 1**.: _(Efficiency). The sum importance of all players \(p_{j}\) in \(P_{i}\) is the same as the improvement of GNN \(f()\) on \(P_{i}\) over an empty set, \(_{j=1}^{|P_{i}|}I(f(),p_{j},G)=f(P_{i})-f()\)._

**Property 2**.: _(Symmetry). For any explanation \(G^{i}_{ex}\), if there exist two other players \(p_{j},p_{k}\{P_{i}/G^{i}_{ex}\}\) that satisfy \(f(G^{i}_{ex} p_{j})=f(G^{i}_{ex} p_{j})\), then \(I(f(),p_{j},G)=I(f(),p_{k},G)\)._

**Property 3**.: _(Dummy). If a player \(p_{j}\) makes no contribution to GNN \(f()\), i.e. \(f(G^{i}_{ex} p_{j})=f(G^{i}_{ex})\) holds for any \(G^{i}_{ex}\), then \(I(f(),p_{j},G)=0\)._

**Property 4**.: _(Monotonicity). Consider two well-trained GNN models \(f_{1}()\) and \(f_{2}()\), given an explanation \(G^{i}_{ex}\), if for any player \(p_{j}\), \(f_{1}(G^{i}_{ex} p_{j})-f_{1}(p_{j}) f_{2}(G^{i}_{ex} p_{j})-f_{2} (p_{j})\) always holds, then \(I(f_{1}(),G^{i}_{ex},G) I(f_{2}(),G^{i}_{ex},G)\)._

Shapley value can well satisfy the above four fairness Properties However, the Shapley-based importance scoring function attempts to take all nodes in the graph except explanation \(G^{i}_{ex}\) into the cooperation game, which not only ignores the topological information of the input graph but also brings huge computational costs. Fortunately,  alleviate this issue by modifying the Shapley-based importance scoring function as 'k-hop Shapley', which also makes it _structure-aware_:

\[I(f(),G^{i}_{ex},G)=_{p_{i}\{P_{i,khop} G^{i}_{ex} \}}|(|P_{i,khop}|-|p_{i}|-1)!}{|P_{i,khop}|!}(f(p_{i} G^{i}_{ ex})-f(p_{i})), \]where \(I(f(),G^{i}_{ex},)\) denotes the weighted sum of the marginal contribution of explanation \(G^{i}_{ex}\), and \(P_{i,khop}=\{G^{i}_{ex},v_{n,i+1},v_{n,i+2},,v_{n+i+k_{i}}\}\) includes the nodes within the \(G^{i}_{ex}\) as well as the nodes in the k-hop neighbors of \(G^{i}_{ex}\).

### Structure-aware Shapley-based Multipiece Explanation Provide Strong Explainability

The characteristics and properties within a graph or node tend to be _jointly_ influenced by more than one high-order connected community of the graph. This implies that the appropriate explanation within this context requires the GNN explanation method to provide the multiple connected substructures simultaneously.

To solve the above challenge, we first define the mathematical formalization of the search processes on graphs by utilizing the MCTS-based GNN explanation methods. Then we propose a mathematically coherent framework to explore the explanatory power of MCTS-based GNN explanation methods using computational methods for the multi-armed bandit problem in the MCTS algorithm .

Our mathematical framework is based on the hierarchical partitioning of the MCTS search space \(\). More precisely, the smoothness of the MCTS search space \(\) can be defined by the inequality \(|f(x_{i})-f(x_{j})| l(x_{i},x_{j})\), where \(l(x_{i},x_{j})\) refers to the Lipschitz continuity between any two substructures \(x_{i}\) and \(x_{j}\) in \(\). It serves as a critical metric to ascertain the boundedness of the change in the explanation method \(f()\) concerning the change in input substructures. Assuming that the function \(f()\) is Lipschitz continuous and the \(l\) is given, an evaluation of the function \(f()\) at any point \(x_{t}\) enables us to define an upper bounding function \(B_{t}(x)\) for \(f()\). This upper bounding function can be refined after each evaluation of \(f()\):

\[ x,f(x) B_{t}(x)^{def} _{1 s t}[f(x_{s})+l(x,x_{s})], \]

where metric \(l\) satisfies the following Assumptions . In the context of computational uncertainties associated with MCTS, the evaluation strategy in  offers the potential to describe specific numerical estimates within an undefinable space.

**Assumption 1**.: _(Local smoothness). There exists at least one stage-optimal substructure \(x^{}\) of \(f()\) (i.e. \(f(x^{})=sup_{x}f(x)\)) and \( x,f(x^{})-f(x) l(x,x^{})\) holds._

**Assumption 2**.: _(Decreasing diameters). There exists a decreasing sequence \((h)>0\), such that for any depth \(h 0\) and for any cell \(_{h,i}\) of depth \(h\), \(_{_{h,i}}\)\(l(,_{h,i})(h)\) holds._

**Assumption 3**.: _(Well-shaped cells). There exists \(>0\) such that for any depth \(h 0\), any cell \(_{h,i}\) contains a \(l\)-ball of radius \((h)\) centered in \(x_{h,i}\)._

With the given assumptions, the search space \(\) can be partitioned into \(K^{H}\) subsets (_i.e._, cells) \(_{h,i}\) using a \(K\)-ary tree of depth \(H\), where \(0 h H,0 i K^{h-1}\). Based on this partitioning method, the MCTS process can be treated as the expansion of this \(K\)-ary tree. The root of \(K\)-ary tree (_i.e._, cell \(_{0,0}\)) corresponds to the whole search space \(\). Each cell \(_{h,i}\) corresponds to a node \((h,i)\) of the tree, where \(h\) denotes the depth of the tree and \(i\) refers to the index. Each node \((h,i)\) possesses \(K\) children nodes \(\{(h+1,i_{k})\}_{1 k K}\) s.t. the associated cells \(\{_{h+1,i_{k}},1 k K\}\) form a partition of the parent's cell \(_{h,i}\). Consequently, expanding one node requires adding one of its \(K\) children to the current tree, which corresponds to subdividing the cell \(_{h,j}\) into \(K\) children cells \(\{_{h+1,j_{1}},,_{h+1,j_{K}}\}\). Assume that there exist a decreasing sequence \((h) 0\) that satisfies \(_{_{h,i}}\)\(l(,_{h,i})(h)\) for any \(_{h,i}\). The decreasing sequence \((h)\) ensures that each cell size reduces with increasing depth.

The search space \(\) can be divided through the above partitioning method according to \((h)\), with respect to \(l\)-open balls. Let \(_{t}\) denote nodes of the current tree, and \(_{t}\) denotes the incoming leaves of \(_{t}\) to be expanded at round \(t\). Recalling our earlier definition of \(B_{t}(h)\) which was derived from the Lipschitz continuity, we can now generalize it to a new representation that connects to \((h)\). Formally, it is expressed as:

\[b_{h,i}=f(x_{h,i})+(h). \]Based on this, we now consider which nodes will be expanded during the search. Note that Assumption 2 implies that the b-value of any cell contains \(x^{}\) upper bounds \(f^{}\). In other words, for any cell \(_{h,i}\) such that \(x^{}_{h,i}\),

\[b_{h,i}=f(x_{h,i})+(h) f(x_{h,i})+l(x_{h,i},x^{}) f^{}. \]

This means that a leaf \((h,i)\) of a \(K\)-ary tree will never be expanded if \(f(x_{h,i})+(h)<f^{}\). Therefore, under this partitioning strategy, the only set of nodes that will be expanded could be defined as \(I}{{=}}_{h 0}I_{h}\), which could be stated as

\[I_{h}}{{=}}\{nodes\{h,i\}f(x_{h,i})+ (h) f^{}\}. \]

In order to derive a loss bound, we now define a measure of the quantity of near-optimal states, called _near-optimality dimension_. For any \(>0\), the set of \(\)-optimal states can be defined as

\[_{}:=\{x,f(x) f^{}-\}. \]

**Definition 1**.: _(\(\)-near-optimality dimension). The \(\)-near-optimality dimension is the smallest \(d 0\) such that there exists \(C 0\), for all \(>0\), the maximum number of disjoint \(l\)-balls of radius \(\) with centers in \(_{}\) is less than \(C^{-d}\)._

Definition 1 represents the number of near-optimal states for a function \(f()\) around its optimal solution. It is important to note that \(d\) is not an intrinsic property of \(f()\) as we are packing near-optimal states using \(l\)-balls. Instead, it characterizes both \(f()\) and \(l\) and depends on the constant. In order to relate this measure to the algorithmic details, we also need to correlate it with the characteristics of the partitioning, specifically the shape of the cells. That is, the near-optimality dimension \(d\) is dependent on a particular constant, which will be determined in accordance with the parameter \(\) as defined in Assumption 2

**Lemma 1**.: _Let \(d\) be the \(\)-near-optimality dimension, and \(C\) the corresponding constant. Then \(|I_{h}| C(h)^{-d}\)._

Building upon Lemma 1 we further analyze the loss of the MCTS-based GNN explanation methods across \(n\) iterations in Theorem 1

**Theorem 1**.: _Let us write \(h(n)\) the smallest integer \(h\) such that \(C_{l=0}^{h}(l)^{(-d)} n\), then the loss of the MCTS-based GNN explanation methods is bounded as:_

\[r_{n}(h(n)) \]

The loss \(r_{n}\) reflects the gap between the obtained and optimal explanations over \(n\) iterations. We aim to bound this loss by \((h(n))\), illustrating that refining the partition of the search space \(\) reduces the loss, thus better approximating the optimal explanation. We provide a complete description of the mathematical properties and theorem proofs of the framework in Appendix B.2.

Leveraging the mathematical underpinnings provided above, as demonstrated in Figure 1 we employ an example to contextualize the theoretical insights within the GNN explanation. The ground truth

Figure 1: Illustration of ground truth explanation and the possible sub-optimal explanations provided by pruning-based MCTS explanation techniques.

explanation is highlighted in yellow which includes two connected components. When searching for explanations starting from any node in the two components through the pruning-based MCTS, _the nodes in other components_ are accessible only via the unimportant node which is highlighted in blue. In this situation, given sparsity constraints, the pruning-based MCTS can only generate the suboptimal explanation. As a consequence, regardless of the search trajectory adopted, the diameter of the \(l\)-ball remains unyielding, failing to converge to the \(\)-near optimality numerical solution. Therefore, it is necessary to design an explanation method that can accurately retain important nodes while avoiding irrelevant nodes, thus increasing the likelihood of discovering the optimal explanation. This ambition resonates with previously proposed loss bound \(r_{n}(h(n))\), emphasizing the need for advanced exploration to reduce losses and approximate the optimal explanation with higher precision.

## 4 Structure-aware Shapley-based Multipiece Explanation Method

As we discussed in Section 3 the structure-aware Shapley-based multipiece explanation provides a potential way to effectively uncover the GNN black box. In order to approximate the optimal Shapley-based explanation, we propose a two-phase framework, Structure-aware Shapley-based Multipiece Explanation (SAME) method, which is composed of (1) an _important substructure initialization_ phase and (2) an _explanation exploration_ phase, as shown in Figure. 2

In the first phase (Section4.1), we extend an expansion-based Monte Carlo tree search as an important substructure initializer to generate connected components not only of high importance but also of multi-grained diversity. In the second phase (Section4.2), we apply the important substructure set as an action set in another expansion-based Monte Carlo tree search to explore potential explanations.

### Important Substructure Initialization

In this section, we propose an expansion-based MCTS approach for important substructure initialization. It is intuitive that a favorable initialization should not exclude important substructures at any scale and should not include redundant substructures. Formally, these can be defined as:

**Property 5**.: _(Node-wise importance). Given an input graph \(G\) to be explained, for any node \(v_{i} G\), its \(I(f(),v_{i},G)\) importance will be considered._

**Property 6**.: _(Substructure-wise importance). Given an input graph \(G\) to be explained, for any substructure \(G_{sub_{i}} G\), its importance \(I(f(),G_{sub_{i}},G)\) will be considered._

Figure 2: **Overview of Structure-A**ware Shapley-based **M**ultipiece **E**xplanation (SAME) method. (a) _Important substructure initialization phase_ aims at searching the single connected important substructure. (b) _Explanation exploration phase_ provides a candidate set of explanations by optimizing the combination of different important substructures. (c) The comparison of the final explanation with the highest importance score from the candidate set with the optimal explanation.

**Property 7**.: _(Composite-wise importance). Given an explanation \(G^{i}_{com} G\) consisting of one or more substructures, its importance \(I(f(),G^{i}_{com},G)\) will be considered._

**Property 8**.: _(Priority-based integration). Given an explanation \(G^{i}_{ex}\) with any size, the node \(v_{i}\{G G^{j}_{ex}\}\) will be added on \(G^{j}_{ex}\) to get a new explanation \(G^{i}_{ex}\), if and only if for any \(v_{l}\{G(G^{j}_{ex} v_{i})\}\), \(I(f(),G^{j}_{ex} v_{i},G)>I(f(),G^{j}_{ex} v_{l},G)\) holds._

**Property 9**.: _(Redundancy consideration). Given an explanation \(G_{ex} G\), if \(I(f(),G_{ex}\{i\},G)>I(f(),G_{ex},G)\) holds, the new explanation \(G^{}_{ex}=G_{ex}\{i\}\) will be chosen._

Taking the above properties into account, we propose our expansion-based Monte Carlo tree search for important substructure initialization which aims at providing important connected components of the graph. The detailed Algorithm is presented in Appendix C. Given a graph \(G\) to be explained, the node in the MCTS is defined as \(N_{i}\) which contains the following variables:

\[N_{i}:\{G^{i}_{sub},T_{i},R_{i},A_{i},C_{i},W_{i}\}\]

where \(G^{i}_{sub}\) denotes the corresponding substructure of graph \(G\) for node \(N_{i}\) in the search tree. \(T_{i}\) is the visiting time of node \(N_{i}\) in the search tree, \(R_{i}\) refers to the the importance (reward) of substructure \(G^{i}_{sub}\). \(A_{i}\) represents the action set of node \(N_{i}\). \(C_{i}=\{N_{j}\}_{j=1}^{|A_{i}|}\) represents a set of children with respect to node \(N_{i}\), and we denote \(N_{j}\) as one of the child nodes obtained through the action \(a_{j} A_{i}\) at parent node \(N_{i}\). \(W_{i}\) means the sum of the children's rewards.

The expansion-based MCTS is initialized with an empty set, \(N_{0}=\). At the beginning of each iteration, our method will randomly choose an unvisited node from the graph, or choose the node with the highest reward if all nodes have been visited. Then, the tree will be iteratively expanded according to the _node-wise tree expansion_ until it reaches a leaf node. Specifically, given an MCTS node \(N_{i}\), it will only choose the child node within 1-hop neighbors of the associate \(G^{i}_{sub}\) to expand, which means that the action set is topology dependent. Therefore, the substructure of any nodes in the search tree will be a connected component. During the child node selection, the reward of each child node \(R_{j}\) of \(N_{i}\) will be calculated following Equation 1. Finally, the chosen action is decided following the child selection strategy:

\[a^{}=*{arg\,max}_{a_{j}}\ }{T_{j}}+ R_{j} }T_{k}}}{1+T_{j}} \]

where \(\) is a hyperparameter for balancing exploration-exploitation trade-off 1. We define the maximum substructure size as \(\), and for any \(G^{i}_{sub} G\), \(|G^{i}_{sub}|\) always holds. After reaching the maximum substructure size, the reward of associated leaf nodes is backpropagated up the tree along the search path, updating all the information stored in each node of the path. The important substructure set includes all nodes in MCTS after performing \(M_{1}\) times iterations.

### Explanation Exploration

In this section, we propose another expansion-based MCTS for exploring high-explainable explanations. The detailed algorithm is provided in Appendix C. Different from the previous expansion-based MCTS provided in Section 4.1 we propose a slight modification on it such that we change the action set from node level to substructure level. As Section 3.2discussed, this can be useful not only to obtain more flexible explanation results on real-world cases but also to provide the higher potential to approximate the theoretically optimal Shapley-based explanation.

The action set of MCTS in this phase is built upon the substructure set derived from the important substructure initialization phase. Similar to the previous MCTS, at the beginning of each iteration, it will randomly select an unvisited substructure from the set, or choose the substructure with the highest reward to expand if all substructures have been visited. Afterward, the _substructure-wise tree expansion_ also follows the Equation 1 and 2 to develop the tree and provide the explanation candidate. The action set corresponding to each node of MCTS in the phase is the whole substructure set. To further accelerate the exploration, we filter the unimportant substructures by only keeping the top K important substructure in the action set.

Notice that calculating all possible combinations of either substructures or nodes can definitely obtain the optimal Shapley-based explanation. Nevertheless, such node-wise or substructure-wise brute force methods lead to \(O(2^{|V|})\) computational complexity, which is an NP-hard problem. We provide a feasible solution to approximate the optimal Shapley-based explanation in polynomial time \((M_{1}|V|^{2}+M_{1}|V||E|+M_{2}Kt_{s})\), where \(M_{1}\) and \(M_{2}\) denote maximum number of iterations of the first phase and second phase respectively, \(\) is the maximum substructure size, \(K\) refers to the size of the important substructure set and \(t_{s}\) is the time budget of MCTS in the second phase. We leave the derivation of the time complexity in Appendix B.4.

## 5 Experiments

Our objective of the experiments is to understand the following two questions. 1) Are the explanations provided by SAME more informative and faithful compared to other methods under the same test conditions? 2) Can SAME provide a more human-intuitive explanation than others? To this end, we perform extensive quantitative and qualitative analysis to evaluate the explanatory power of SAME, following previous literature . The SAME is compared with various competitive baselines and shows state-of-the-art (SOTA) results in all the cases.

**Dataset**. The experiments are conducted on six datasets with diverse categories, including molecular graphs (e.g., BBBP  and MUTAG ), sentiment graphs (e.g., Graph-SST2 and Graph-SST5 ) and synthetic Barabasi-Albert graphs (e.g., BA-2Motifs  and BA-Shapes ). We conduct a node classification task for the BA-Shapes dataset, and graph classification tasks for the rest five datasets. More detailed descriptions of datasets are provided in Appendix D.

**Metrics**. In this work, we use several criteria  to evaluate our approach: (1) _Sparsity_ quantifies how compact are the explanations, and further facilitates fair comparison by restricting the different explanations to the same size. (2) _Fidelity_ determines how informative and faithful are the explanations by removing the selected nodes. (3) _Inv-Fidelity_ measures the explanations from the same aspect as fidelity while it keeps the selected nodes. (4) _Harmonic fidelity_ normalizes fidelity by sparsity and takes a harmonic mean to make different explanations comparable with a single metric. We leave detailed mathematical definitions of the above metrics in Appendix E.

**Experimental setup**. In the important substructure initialization phase, we set the MCTS iteration number \(M_{1}\) to 20. The exploration-exploitation trade-off \(\) is set to 5 for BBBP and 10 for other datasets. The substructure size \(\) has different settings in different datasets. In the explanation exploration phase, we set the hyperparameter \(K=7\) for important substructure filtering, \(M_{2}=10\) for the MCTS iteration number, and the other hyperparameters of MCTS remain the same as the previous phase. We follow  to set other baselines hyperparameters. All methods are implemented in PyTorch  and PyG . Our experiments are conducted on a single Nvidia V100 GPU with an Intel Xeon Gold 5218 CPU. We leave the detailed settings in Appendix F.

    &  & Node classif. \\   &  &  &  \\   & BBBP & MUTAG & Graph-SST2 & Graph-SST5 & BA-2Motifs & BA-Shapes \\  Grad-CAM  & 0.226\(\)0.036 & 0.261\(\)0.018 & 0.257\(\)0.056 & 0.229\(\)0.042 & 0.472\(\)0.010 & - \\ GNNExplainer  & 0.148\(\)0.041 & 0.188\(\)0.031 & 0.143\(\)0.041 & 0.170\(\)0.046 & 0.442\(\)0.026 & 0.154\(\)0.000 \\ PGExplainer  & 0.197\(\)0.043 & 0.156\(\)0.004 & 0.219\(\)0.040 & 0.207\(\)0.036 & 0.431\(\)0.011 & 0.135\(\)0.020 \\ GNN-LRP  & 0.111\(\)0.040 & 0.253\(\)0.030 & 0.103\(\)0.042 & 0.131\(\)0.057 & 0.146\(\)0.010 & 0.155\(\)0.000 \\ SubgraphX  & 0.433\(\)0.073 & 0.379\(\)0.030 & 0.262\(\)0.027 & 0.283\(\)0.042 & 0.493\(\)0.003 & 0.181\(\)0.005 \\ GStarX  & 0.117\(\)0.043 & 0.656\(\)0.096 & 0.183\(\)0.050 & 0.186\(\)0.050 & 0.476\(\)0.014 & - \\ 
**SAME** & **0.489\(\)0.034** & **0.702\(\)0.125** & **0.373\(\)0.042** & **0.393\(\)0.02** & **0.549\(\)0.004** & **0.214\(\)0.000** \\
**Relative Improve** & **12.9\%\(\)** & **7.01\%\(\)** & **42.3\%\(\)** & **38.9\%\(\)** & **11.3\%\(\)** & **18.2\%\(\)** \\    Note: The fidelity results are averaged across different sparsity from 0.5 to 0.8. The quantitative results are presented in the form of mean \(\) std. The previous SOTA results on different datasets are marked with an underline. _Relative Improve_ denotes the relative improvement of our SAME method over the SOTA methods.

Table 2: Comparison of our SAME and other baseline using fidelity.

### Quantitative Analysis

To validate the overall explainability performance, we compare the proposed SAME with a series of competitive baselines under different metrics. Table 2 shows the averaged fidelity under different sparsity (_i.e._, sparsity=[0.5,0.6,0.7,0.8]). The proposed SAME significantly outperforms the previous state-of-the-art on both real-world and synthetic datasets. Specifically, the performance improvement of SAME is 12.9% on BBBP, 7.01% on MUTAG, 42.3% on Graph-SST2, 38.9% on Graph-SST5, 11.3% on BA-2Motifs and 18.2% on BA-Shapes. Notably, we also demonstrate reliable improvements of SAME over previous SOTA methods in terms of harmonic fidelity at different sparsities, with an average improvement of 1.92% on the graph classification task. This result implies SAME has a higher ability to discover important components in the graph than all baseline methods. As for the inv-fidelity metric, the explanatory power of SAME is competitive compared with the previous SOTA. Detailed results for harmonic fidelity and inv-fidelity are in Appendix G.1. Since the proposed SAME is a model-agnostic method, it works well with GAT and GIN. More comparisons with GraphSVX  and OrphicX  are provided in Appendix G.3.

The computational cost of SAME and other baselines on different datasets are summarized in Table 3. We show that SAME consistently achieves much lower computational cost compared to GStarX and SubgraphX, reflecting its efficiency and robustness. This further verifies that expansion-based MCTS in SAME can work effectively in various scenarios.

### Qualitative Analysis

Figure 4 presents the visualization comparison of the explanations on sentiment graphs. The nodes of adjectives and adverbs are considered to be important for they reveal the attitude of a sentence. Thus, the optimal explanation here is therefore the word or phrase with a positive meaning. In this sense,

Figure 3: Comparison of the explanations on Graph-SST2 with GCN classifier.

   MethodsDataset & BBBP & MUTAG & Graph-SST2 & Graph-SST5 & BA-2Motifs & BA-Shapes \\  Grad-CAM  & 0.16 & 0.23 & 0.39 & 0.44 & 0.14 & - \\ GNNExplainer  & 7.56 & 1.96 & 7.64 & 19.39 & 1.89 & 2.72 \\ PGExplainer  & 0.15 & 0.21 & 0.35 & 0.43 & 0.12 & 0.13 \\ GNN-LRP  & 2.37 & 1.97 & 5.84 & 5.47 & 3.30 & 51.77 \\ SubgraphX  & 26.72 & 151.75 & 36.48 & 71.32 & 85.50 & 162.80 \\ GStarX  & 84.54 & 25.24 & 30.64 & 54.49 & 77.99 & - \\ 
**SAME** & 7.86 & 5.67 & 6.06 & 8.83 & 8.19 & 14.08 \\    Note: The PGExplainer needs training before inferring the explanation.

Table 3: Comparison of inference time (in seconds) on different datasets.

SAME can better capture the adjectives-or-adverbs-like graph structures than other baselines. For instance, SubgraphX focuses on adjectives and adverbs but fails to capture the "but" word which bears significant weight under the contrasting relationship. Intuitively, without the "but", the contribution of "tortured" and "unsettling" should be negative. GStarX achieves to identify words that are consistent with the opinion such as "carefully" and "alive," yet overlooks the crucial transitional relationship between "alive" and "unsettling". Figure 1 shows the visualization of explanations on molecular graphs. The ground truth explanations (_i.e._, functional group \(-}\)) of MUTAG are labelled by human experts. We see that SAME is able to provide the explanations the same as the ground truth. We also illustrate the explanation of the synthetic graph in Figure 1. The ground-truth label of all the graphs in BA-2Motifs is a 5-node-house-structure motif. Results show that SAME exactly finds the ground-truth explanation compared to other baselines. We leave more comparisons in Appendix G.2.

## 6 Conclusion

Structure-aware Shapley-based Multipiece Explanation provides strong explainability over GNN models, while this ability is limited by only using the single connected substructure. Moving forward from the theoretical deduction, we propose the SAME method for explaining GNN, a novel perturbation-based method that is aware of input graph structure, feature interactions, and multi-grained importance. Experimental results demonstrate that our SAME consistently outperforms SOTA methods on multiple benchmarks by a large margin with various metrics and provides a more understandable explanation.

**Limitations.** In the implementation of SAME, the Shapley value is obtained through approximation following . Under this approximation, the fairness axioms discussed in Section [3.1] no longer hold. This is also identified as an unsolved issue for the Shapley value in machine learning by . In addition, the scalability of SAME on large graphs can also become a potential challenge. As the time complexity shown in Section  the time overhead caused by the increase in the number of nodes will become very expensive when scaling up the size of the input graph.

Figure 4: Comparison of the explanations on MUTAG with GIN classifier.

Figure 5: Comparison of the explanations on BA-2Motifs with GCN classifier.