# An Examination of AI-Generated Text Detectors Across Multiple Domains and Models

Brian Tufts

Carnegie Mellon University

btwfts@cs.cmu.edu

&Xuandong Zhao

UC Berkeley

xuandongzhao@berkeley.edu

&Lei Li

Carnegie Mellon University

leili@cs.cmu.edu

###### Abstract

The proliferation of large language models has raised growing concerns about their misuse, particularly in cases where AI-generated text is falsely attributed to human authors. Machine-generated content detectors claim to effectively identify such text under various conditions and from any language model. This paper critically evaluates these claims by assessing several popular detectors (RADAR, Wild, T5Sentinel, Fast-DetectGPT, GPTID, LogRank) on a range of domains, datasets, and models that these detectors have not previously encountered. We employ various prompting strategies to simulate adversarial attacks, demonstrating that even moderate efforts can significantly evade detection. We emphasize the importance of the true positive rate at a specific false positive rate (TPR@FPR) metric and demonstrate that these detectors perform poorly in certain settings, with TPR@.01 as low as 0%. Our findings suggest that both trained and zero-shot detectors struggle to maintain high sensitivity while achieving a reasonable true positive rate.1

## 1 Introduction

Large language models (LLMs) are becoming increasingly accessible and powerful, leading to numerous beneficial applications (Touvron et al., 2023; Achiam et al., 2023). However, they also pose risks if used maliciously, such as generating fake news articles or facilitating academic plagiarism (Feng et al., 2024; Zellers et al., 2019; Perkins, 2023). The potential for misuse of LLMs has become a significant concern for major tech corporations, particularly in light of the upcoming 2024 elections. At the Munich Security Conference on February 16th, 2024, these companies pledged to combat misleading machine-generated content, acknowledging the potential of AI to deceptively influence electoral outcomes (Accord, 2024). As a result, there is a growing need to develop reliable methods for differentiating between LLM-generated and human-written content. To ensure the effectiveness and accountability of LLM detection methods, continuous evaluation of popular techniques is crucial.

Many methods have been released recently that claim to have a strong ability to detect the difference between AI-generated and human-generated texts. These detectors primarily fall into three categories: trained detectors, zero-shot detectors, and watermarking techniques (Yang et al., 2023; Ghosal et al., 2023; Tang et al., 2023). _Trained detectors_ utilize datasets of human and AI-generated texts and train a binary classification model to detect the source of a text (Zellers et al., 2019; Hovy, 2016; Hu et al., 2023; Tian & Cui, 2023; Verma et al., 2023). _Zero-shot detection_ utilizes a language model's inherent traits to identify text in generates, without explicit training for detection tasks (Gehrmann et al., 2019; Mitchell et al., 2023; Bao et al., 2024; Yang et al., 2023; Venkatraman et al., 2023). _Watermarking_ is another technique in which the model owner embeds a specific probabilistic pattern into the text to make it detectable Kirchenbauer et al. (2023). However, watermarking requires the model owner to add the signal, and its design has theoretical guarantees; we do not evaluate watermarking models in this study.

In this paper, we test the robustness of these detection methods to unseen models, data sources, and adversarial prompting. To do this, we treat all model-generated text as a black box generation. That is, none of the detectors know the source of the text or have access to the model generating the text.

This presents the most realistic scenario where the user is presented with text and wants to know if it is AI-generated or not. Specifically, we contribute:

* We conduct a thorough evaluation of AI-generated text detectors on unseen models and tasks, providing insights into their effectiveness in real-world settings.
* We analyze the performance of various detectors under adversarial prompting, exploring the extent to which prompting can be used to evade detection.
* We demonstrate that high AUROC scores, which are often used as a measure of performance in classification tasks, do not necessarily translate to practical usage for machine-generated text detection. Instead, we motivate using the metric of true positive rate (TPR) at a 1% false positive rate (FPR) threshold as a more reliable indicator of a detector's effectiveness in practice.

Due to space limitations, the discussion of related work and background is deferred to Appendix A.

## 2 Benchmarking Procedure

Our benchmarking method involves compiling datasets that have not been encountered by any of the detectors during their training or evaluation phases. This approach ensures that the datasets represent new, unseen data and prevents the possibility of data leakage. For zero-shot detectors, this methodology eliminates the risk of using cherry-picked datasets that may bias the evaluation. For trained detectors this reduces the risk of data leakage and tests on out of domain data. Furthermore, we assess the model's performance across a diverse range of domains that the detectors may not have been previously evaluated against. This comprehensive evaluation strategy allows for a more robust assessment of the detectors' generalization capabilities. Additionally, we evaluate the detectors on a variety of language models that they have not encountered before. This approach enables us to examine the detectors' performance on unfamiliar language models, providing a more comprehensive understanding of their effectiveness and adaptability.

### Datasets

We evaluate each of the detectors on seven different tasks with three of the tasks, question answering, summarization, and dialogue writing, including multilingual results. The datasets chosen for each domain are as follows:

* **Question Answering:** The MFAQ dataset (De Bruyn et al., 2021) was used for this domain. It contains over one million question-answer pairs in various languages. We used the English, Spanish, French, and Chinese subsets.
* **Summarization:** We used the MTG summarization dataset (Chen et al., 2021) for this task. The complete multilingual dataset comprises roughly 200k summarizations. We utilized the English, Spanish, French, and Chinese subsets.
* **Dialogue Writing:** For this task, we utilized the MSAMSum dataset, a translated version of the SAMSum dataset(Feng et al., 2022; Gliwa et al., 2019). This dataset consists of over 16k dialogues with summaries in six languages. We utilized English, Spanish, French, and Chinese for consistency with the other multilingual domains.
* **Code:** We used the APPS dataset (Hendrycks et al., 2021), which contains 10k code questions and solutions. The subset used was randomly selected from all the data included in APPS.
* **Abstract Writing:** For this task, we utilized the Arxiv section of the scientific papers dataset (Cohan et al., 2018) to avoid potential bias, as some detectors have previously been exposed to PubMed data. Additionally, we only selected papers published in 2020 or earlier to remove potential LLM influence.
* **Review Writing:** The PeerRead dataset was used for the review writing task (Kang et al., 2018). PeerRead contains over 10k peer reviviews written by experts corresponding to the paper that they were written for.
* **Translation:** We used the Par3 dataset (Karpinska et al., 2022), which provides paragraph level translations from public-domain foreign language novels. Each paragraph includes at least 2 human translations of which we selected only one to represent human translation.

[MISSING_PAGE_FAIL:3]

## 3 Experiment

**Dataset Processing.** Each dataset undergoes additional processing to prepare it for detection tasks. Research indicates that detectors of machine-generated text are more effective with longer content (Yang et al., 2023). To leverage this, we aimed to use human samples of maximum possible length. However, the minimum length needed to obtain sufficient samples varied by task. We randomly selected 500 samples of human text from filtered subsets with the following lengths: 500 tokens for question answering, 400 tokens for code3, 150 tokens for summarization, 275 tokens for dialogue, 500 tokens for reviews, 500 tokens for abstracts, and 500 tokens for translation (Table 1). These 500 samples served as human examples. From them, prompts from the first 100 samples were chosen for use in the generator model, using the input given to the human author as the model prompt. This resulted in a dataset of 500 human examples and 100 machine-generated examples per model for a total of 400 machine-generated examples for each task. This slight data imbalance is intentional to ensure a more accurate TPR@FPR metric.

Detection methods show improved performance with longer text sequences (Wu et al., 2023) so we show the statistics of the text in Table 1. Our primary focus was on detectors' ability to identify AI-generated text while maintaining a low FPR. The longer length of human-generated text is likely to enhance the TPR@FPR by making it easier to detect as human. We considered the AI-generated text sufficiently long for two reasons. First, Li et al. (2024) reports an average AI generation length of 279.99, which is much lower than our average token lengths. Their extensive training and evaluation data support the adequacy of this length for AI content. Second, our models, with a maximum generation length of 512 tokens 4, produced responses indicative of real-world lengths.

  
**Task** & **AI Avg AI Min** & **Human Avg** \\  Code & 502.16 & 15 & 4496.88 \\ QA & 508.19 & 19 & 1052.37 \\ Summ & 411.57 & 16 & 191.00 \\ Dialogue & 378.26 & 15 & 402.13 \\ Reviews & 549.51 & 24 & 796.06 \\ Abstract & 425.89 & 32 & 2081.88 \\ Translation & 525.43 & 9 & 772.75 \\   

Table 1: Average and minimum token counts of machine-generated and human-generated text for each task, tokenized using the Llama2-13B tokenizer (Touvron et al., 2023). Minimum token counts for human-generated text are omitted as they were previously described.

**Text Generation and Detection Process.** Once the prompt samples were selected, we needed to generate positive examples. The process for this can be seen in Figure 1. We employ three different strategies for prompting the models. The first strategy involves using a basic prompt for each domain that explains the goal of the model and the desired output format. The second strategy consists of requesting that the model be as human as possible. The third strategy requests that the model rewrite and improve upon the human written response 5. The first strategy aims to simulate a basic system prompt that would generally be in place on a model someone is using to generate content. The second strategy simulates the case where a user might try to get the model to generate content that closely resembles human-generated content. The third strategy simulates a scenario where the user writes their own response and simply wants the model to clean it up or make it easier to understand. The outputs of the models were taken as is with no editing. After generating the positive examples, we passed all of the machine-generated and human-generated examples through the detectors. RADAR, Fastdetectgpt, Wild, and T5Sentinel all return a percentage probability for each class, and GPTID and LogRank return a value representing their score. We do not use any thresholds and take the scores as is for AUROC and TPR@FPR metrics.

## 4 Results and Analysis

Table 2 shows the overall performance of each detector across the entire dataset. In this section, we break down the performance of each detector across tasks, languages, and prompt techniques.

### Plain Prompting

We evaluate the AUROC and TPR at 0.01 FPR for machine-generated texts from direct prompting using identical prompts as human written texts. A simple prompt was employed to ensure the generated text was in the correct format and language for the multilingual tasks.

Figures 1(a) and 1(b) show the results for the multilingual tasks and 2(a) and show the results for the only English tasks. A significant difference is observed in detector performance across languages and tasks, particularly in the multilingual setting. Fastdetectgpt consistently performs well overall but encounters challenges in summarization tasks, especially in languages other than English. Other

 
**Detector** & **TPR@@0.01** & **TPR@0.05** & **TPR@0.1** & **AUROC** \\ 
**Radar** & 0.06 & 0.17 & 0.29 & 0.6085 \\
**Fast-Detectgpt** & 0.48 & 0.60 & 0.67 & 0.8376 \\
**Wild** & 0.13 & 0.20 & 0.30 & 0.6888 \\
**PITD** & 0.00 & 0.02 & 0.05 & 0.3204 \\
**LegRank** & 0.00 & 0.01 & 0.02 & 0.2235 \\
**T5Sentinel** & 0.02 & 0.06 & 0.12 & 0.4798 \\  

Table 2: Performance of different detectors across the entire dataset

Figure 1: Pipeline for prompting and evaluation. Adversarial prompting and rewriting are applied to the LLMs. After collecting machine-generated text, AUROC and TPR@FPR are measured for each detector.

detectors show similar patterns: while they achieve strong results in English tasks, their performance becomes more inconsistent with non-English tasks. The AUROC graph suggests robust performance for Fastdetectgpt, but when examining the TPR@0.01 graph, we observe that it struggles to maintain low false positive rates, particularly in summarization tasks where it falls below 0.25 for most languages, except for English dialogue and French question-answering, where it exceeds 0.8.

Figure 3: Comparison of AUROC and TPR@0.01 results for English tasks across all detectors using different prompting styles (normal, template, and rewrite).

Figure 2: Comparison of AUROC and TPR@0.01 results for multilingual tasks across all detectors using different prompting styles (normal, template, and rewrite).

For the English-only tasks, most detectors show improved performance. In these tasks, Wild and Radar demonstrate competitive performance with Fastdetectgpt, which struggles in the translation domain. Despite expectations that the translation domain would be the most challenging due to lower entropy in translated texts, detectors performed reasonably well. While the AUROC graph indicates promise, the TPR@@0.01 graph highlights ongoing challenges in maintaining low false positive rates. Additionally, while Radar and Wild outperform Fastdetectgpt in the review domain based on AUROC, they fall short in TPR@0.01 compared to Fastdetectgpt's performance.

### Template Prompting

Figures 1(c) and 1(d) show the results on the multilingual tasks where the model was instructed to be "as human as possible." Interestingly, this request had little effect on performance. In the few instances where changes occurred, scores generally increased, suggesting that asking the model to "sound human" may have made its output easier to detect. This aligns with expectations, as large language models are already trained on predominantly human-written texts, and generating more conversational output can make detection more straightforward, as evidenced in dialogue generation tasks.

On the English tasks, as shown in figures 2(c) and 2(d), the results were similarly unaffected by the human-like request, with some slight score increases where changes were observed. This is especially expected in domains such as reviews, code, and abstracts, which follow specific writing conventions, while tasks like question answering and dialogue generation exhibit more variability and creativity.

### Rewriting

Finally, we show the results for the rewriting prompt for the multilingual tasks in figure 1(e) and 1(f) and for the English tasks in figures 2(e) and 2(f). We observe a notable decrease in AUROC performance for detectors that previously performed well, such as Fastdetectgpt, Radar, and Wild, while Phd and LogRank see an increase in performance, with T5Sentinel remaining largely unaffected. This performance decline is even more pronounced in TPR@0.01, where none of the detectors show improvement. Despite these shifts, the relative performance across tasks remains consistent, indicating an inherent variability in detectability based on the type of task and language.

### TPR@FPR vs AUROC

In this paper, we utilize both the AUROC and TPR@FPR metrics. However, we also argue that TPR at a low FPR is a much more important metric for this detection task. Figure 4 shows the correlation between TRP scores at various FPR rates and the AUROC score for all tasks, detectors, and models used in this research. The AUROC correlates much higher with FPR rates in the 0.4 to 0.6 range and much lower with FPR rates at the edges, less than 0.2 and greater than 0.8. While the 0.75 is still a reasonable correlation value, the AUROC is still much more representative of the middle FPR's while we are really concerned with the lower FPR's for this task. This is why we report the TPR@0.01, which is much more representative of the applicability of a detector than the AUROC.

### Output Quality and Detection

Measuring the quality of LLM outputs, especially in creative tasks, remains challenging, making it difficult to determine if higher-quality outputs are harder to detect. Table 3 compares various models' performance scores and rankings from Chatbot Arena (Chiang et al., 2024), allowing us to explore if output quality affects detectability. The data shows little difference in detectability across models of

Figure 4: Correlations between various FPR rates and the overall AUROC score. AUROC score is much more representative of the middle FPR rates, while this detection task is much more concerned with the lower end of FPR.

varying quality, with AUROC and TPR@0.01 scores remaining consistent. This suggests that output quality does not significantly impact the difficulty of detection, though further research is needed for a fuller understanding.

## 5 Conclusion

This study evaluates six advanced detectors across seven tasks and four languages, revealing notable inconsistencies in their detection capabilities. We also examined three different prompting strategies and their impact on detectability, finding that requests for more "human-like" output do not make the text harder to detect, while rewritten human content proves more difficult to identify.

Additionally, this research highlights the limitations of relying on the AUROC metric for assessing machine-generated content detectors. Our findings emphasize the need for robust evaluation methods to develop more reliable detection techniques. The study underscores the challenges in detecting machine-generated text, particularly when human written text was only modified by a language model, and advocates for TPR@FPR as the preferred evaluation metric to better capture detector performance.

## 6 Limitations

A limitation of this method is the settings in which the human data was collected may vary from the settings in which these detectors will be used. Additionally, some of the datasets we used had collected their data from the internet which raises a concern that some of that data is not completely human generated. This is a challenge that all future detectors will also struggle with when training and evaluating. These results pose the risk of emboldening users to use AI generated content when they otherwise should not because they know detectors cannot be confidently trusted. However, acknowledging this is important to encouraging research into new detection methods and improving current methods.