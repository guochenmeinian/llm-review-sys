# Sparse Parameterization for Epitomic

Dataset Distillation

 Xing Wei1 Anjia Cao1 Funing Yang1 Zhiheng Ma2

1School of Software Engineering, Xi'an Jiaotong University

2Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences

weixing@mail.xjtu.edu.cn zh.ma@siat.ac.cn

{caoanjia7, moolink}@stu.xjtu.edu.cn

Zhiheng Ma is the corresponding author.

###### Abstract

The success of deep learning relies heavily on large and diverse datasets, but the storage, preprocessing, and training of such data present significant challenges. To address these challenges, dataset distillation techniques have been proposed to obtain smaller synthetic datasets that capture the essential information of the originals. In this paper, we introduce a Sparse Parameterization for Epitomic datasEt Distillation (SPEED) framework, which leverages the concept of dictionary learning and sparse coding to distill epitomes that represent pivotal information of the dataset. SPEED prioritizes proper parameterization of the synthetic dataset and introduces techniques to capture spatial redundancy within and between synthetic images. We propose Spatial-Agnostic Epitomic Tokens (SAETs) and Sparse Coding Matrices (SCMs) to efficiently represent and select significant features. Additionally, we build a Feature-Recurrent Network (FREeNet) to generate hierarchical features with high compression and storage efficiency. Experimental results demonstrate the superiority of SPEED in handling high-resolution datasets, achieving state-of-the-art performance on multiple benchmarks and downstream applications. Our framework is compatible with a variety of dataset matching approaches, generally enhancing their performance. This work highlights the importance of proper parameterization in epitomic dataset distillation and opens avenues for efficient representation learning. Source code is available at https://github.com/MIV-XITU/SPEED.

## 1 Introduction

Deep learning has achieved remarkable success across diverse domains, thanks to its ability to extract insightful representations from large and diverse datasets . Nevertheless, the storage, preprocessing, and training of these massive datasets introduce significant challenges that strain storage and computational resources. In response to these challenges, dataset distillation techniques have arisen as a means to distill a more compact synthetic dataset that encapsulates the pivotal information of the original dataset. The central concept of dataset distillation is the extraction of an epitomic representation, capturing the core characteristics and patterns of the original dataset while minimizing storage demands. By doing so, deep learning models trained on the distilled dataset can attain performance levels similar to those trained on the original dataset but with significantly reduced storage and training costs. This approach opens new horizons for applications requiring cost-effective storage solutions and expedited training times, such as neural architecture search  and continual learning .

The success of dataset distillation heavily relies on two essential factors: proper parameterization of the synthetic dataset and an effective design of the matching objective to align it with the originaldataset. While many dataset distillation methods have primarily focused on optimizing the matching objective , the critical aspect of synthetic dataset parameterization has often been overlooked. Typically, these methods employ a naive image-independent parameterization approach, where each learnable parameter basis (synthetic image) is optimized independently. Although some studies have recognized the inefficiency of image-independent parameterization and explored the mutual coherence and relationship between different synthetic images to improve compression efficiency , none of the previous methods have fully considered the spatial redundancy that exists within individual images and between different images.

In this paper, we present an efficient parameterization framework, named Sparse Parameterization for Epitomic dataEt Distillation (SPEED), which addresses the aforementioned limitations. SPEED leverages principles from representation learning paradigms, including convolutional neural network (CNN) , vision transformer (ViT) , dictionary learning , and sparse coding .

SPEED introduces Spatial-Agnostic Epitomic Tokens (SAETs) that are shared among all synthetic image patches, and employs the Sparse Coding Matrices (SCMs) to select the most significant tokens. Subsequently, these selected tokens are assembled sequentially to form higher-level representations that facilitate the reconstruction of synthetic patches via non-linear mapping. To further minimize storage requirements, we propose a Feature-Recurrent Network (FReNet) that utilizes recurrent blocks to generate hierarchical features, with SAETs and SCMs shared by all blocks, while leveraging a multi-head mechanism to enhance feature diversity.

In comparison to previous methods, our approach demonstrates significant advantages in handling high-resolution real-world datasets with substantial spatial redundancy. Notably, it achieves outstanding performance on ImageNet subsets, surpassing most of the previous state-of-the-art methods  by achieving an average improvement of \(11.2\%\) with 1 image per class storage space. Additionally, our sparse parameterization approach exhibits superior performance on unseen network architectures, outperforming previous state-of-the-art approaches , and even surpassing our own full-element baseline, highlighting the potential of sparse representation for storage efficiency and improved generalization abilities.

Our method demonstrates competitive results across three standard dataset distillation benchmarks, such as surpassing the previous state-of-the-art  on CIFAR100 by \(6.0\%\) and on TinyImageNet by \(10.9\%\) when using 1 image per class storage space. It also exhibits strong performance on downstream applications, such as continual learning. Furthermore, our framework is compatible with multiple existing matching objectives , generally enhancing their performance through the use of our sparse parameterization strategy.

In summary, our work highlights the importance of proper parameterization in epitomic dataset distillation and introduces the SPEED framework as a solution. We showcase its superiority in handling high-resolution datasets, achieving exceptional performance on benchmarks and downstream

Figure 1: **SPEED Overview. We take spatial-agnostic epitomic tokens as the shared dictionary of the dataset and perform multi-head sparse combinations to synthesize instance-specific features. Subsequently, we utilize feature-recurrent blocks to generate hierarchical representations for non-linear synthesis of image patches, while reusing the sparse features. In this way, we sparsely parameterize the dataset, alleviating the storage burden and producing highly representative synthetic images.**

applications. Our framework not only enhances storage efficiency but also improves generalization capabilities, opening new avenues for efficient representation learning in deep learning applications.

## 2 Method

The purpose of dataset distillation  is to learn a synthetic dataset \(=\{(_{i},y_{i})\}_{i=1}^{N}\) that is much smaller in size than the original dataset \(=\{(X_{i},y_{i})\}_{i=1}^{M}\), _i.e._, \(N M\), while minimizing the loss of information. Formally, previous methods optimize the synthetic dataset by minimizing various matching objectives, all of which can be expressed using the following formulation:

\[^{*}=*{arg\,min}_{}_{ }(,),( ,),\] (1)

where \(\) represents the distribution used for initializing the network parameters, \(\) parameterize the training network, \((,)\) is the dataset matching metric, \(()\) maps the dataset to other informative spaces (e.g. gradient , feature [17; 18], and parameter spaces ).

However, rather than exclusively focusing on matching objectives, this paper introduces a universal parameterization method for synthetic datasets that can be seamlessly integrated with most existing matching objectives. The naive parameterization method, which optimizes each synthetic image \(\) independently [12; 13; 14; 16; 17], fails to leverage shared information between images, resulting in unnecessary redundancy. In the following sections, we will present our parameterization framework, which decomposes the synthetic dataset into Spatial-Agnostic Epitomic Tokens, Sparse Coding Matrices, and a Feature-Recurrent Network. This framework significantly mitigates redundancy within and between images, irrespective of the spatial locations of features.

### Spatial-Agnostic Recurrent Parameterization

Taking inspiration from the Vision Transformer (ViT) , we propose a more efficient parameterization approach applied at the patch level, utilizing cascade non-linear combinations to reduce complex and fine-level redundancy. To further minimize the storage footprint, we share Sparse Coding Matrices (SCMs) and Spatial-Agnostic Epitomic Tokens (SAETs) across all recurrent blocks. The main approach can be formulated as follows:

\[_{i}=_{}(E,A_{i}).\] (2)

Here \(_{i}=[_{i}^{(1)},_{i}^{(2)},...,_{i}^{(J)} ]^{L J}\), and \(_{i}^{j}^{L}\) represents the \(j\)-th patch of \(_{i}\), which is flatten into a vector. \(L\) equals to the product of the patch height, width, and number of channels, and \(J\) is the total patch number. Similar to ViT, we divide the synthetic image into non-overlapping rectangular patches. \(E=[e_{1},e_{2},...,e_{K}]^{D K}\) is the Spatial-Agnostic Epitomic Tokens (SAETs) shared by all synthetic patches, where \(D\) is the feature dimension, and \(K\) is the total number of tokens. \(A_{i}=[a_{i}^{(1)},a_{i}^{(2)},...,a_{i}^{(J)}]^{K J}\) is the Sparse Coding Matrix (SCM) for the \(i\)-th synthetic image, where \(a_{i}^{(j)}^{K}\) is the specific coding vector for patch \(_{i}^{(j)}\). \(A_{i}\) will be further sparsified and saved in a storage-efficient format. \(_{}()\) is a non-linear recurrent transformer-style network that maps SAETs and the SCM to a synthetic image. Its learnable parameters are denoted as \(\). This network is referred to as the Feature-Recurrent Network (FReeNet), which is described in detail below.

Feature-Recurrent Network.In the Feature-Recurrent Network (FReeNet), each recurrent block shares the same SAETs and SCMs for the sake of parameter efficiency. However, this shared approach can lead to a lack of diversity in the resulting representations, as a single pool of SAETs must model multi-scale features. Drawing inspiration from the multi-head mechanism introduced in the transformer architecture, we introduce the concept of "multi-head" SAETs, aiming to strike a balance between storage efficiency and feature diversity. Initially, the original SAETs are split along the feature dimension to create multiple SAET pools, denoted as \(\{E^{h}\}_{h=1}^{H}\), where \(E^{h}^{ K}\). Additionally, each pool is assigned an independent SCM, denoted as \(\{A_{i}^{h}\}_{h=1}^{H}\), where \(A_{i}^{h}^{K J}\). The coding matrix \(A_{i}^{h}\) will undergo further sparsification to select the most significant tokens. We refer to the mechanism that combines the multi-head SAETs and SCMs as Multi-Head Sparse Coding (MHSC), which can be formulated as follows.

\[_{r}(\{E^{h}\}_{h=1}^{H},\{A_{i}^{h}\}_{h=1}^{H})=W_{r} [E^{1}A_{i}^{1},E^{2}A_{i}^{2},...,E^{H}A_{i}^{H}]+b_{r},\] (3)where \(W_{r}^{D D}\) is a linear projection and \(b_{r}\) is the bias, both of which are specific to each recurrent block and not shared across blocks. Using MHSC as the central component, we can construct the FReeNet in a recurrent manner, with SAETs and SCMs shared across different scales:

\[Z_{r}^{} =_{r}^{1}(_{r}(\{E^{h}\}_{h=1}^{H},\{ A_{i}^{h}\}_{h=1}^{H}))+Z_{r-1}), r=1,2,...,R,\] \[Z_{r} =_{r}^{2}(_{r}(Z_{r}^{})+Z_{r}^{ }), r=1,2,...,R,\] (4) \[_{i} =WZ_{R}+b,\]

where \(R\) is the total number of recurrent blocks, and \(Z_{R}^{D J}\) is the output of the last block. \(Z_{0}\) is initialized with a zero matrix. \(W^{L D}\) and \(b\) make up the final linear projection layer, and \(_{i}^{L J}\) is the output synthetic image, which is then rearranged into its original shape. MLP stands for the multi-layer perceptron. In our implementation, we set the MLP to have one hidden layer with the same dimension as the input, and incorporate layer normalization (LN)  and residual connection  in each block. It is worth noting that the parameters other than the SAETs and SCMs are not shared between different blocks to ensure that each block processes a different scale of features. Despite the use of shared SCMs across different blocks, our analysis demonstrates that SCMs still occupy the majority of the stored parameters. Therefore, we introduce a sparsification method to further enhance the storage efficiency of SCMs, motivated by the theories and techniques of sparse coding . In the subsequent section, we will reuse \(_{i}=_{}(\{E^{h}\}_{h=1}^{H},\{A_{i}^{h}\}_{h=1}^{H})\) to refer to the multi-head implementation of the FReeNet, without any ambiguity.

### Training Objective and Feature Sparsification

Since the \(_{0}\) norm is not differentiable and difficult to optimize , we instead adopt the \(_{1}\) norm as the sparsity penalty function. By promoting sparsity in solutions, it can effectively remove redundant features . Our optimization objective can be expressed as follows:

\[=\{(_{}(\{E^{h}\}_{h=1}^{H},\{A_{i}^{h}\}_{h=1}^{H}),y _{i})\}_{i=1}^{N},\] (5)

where \(||||_{1}\) is the \(_{1}\) norm of a matrix, \(\) controls the amount of regularization. Using this approach, we decompose synthetic images into a multi-head SAET \(\{E^{h}\}_{h=1}^{H}\) and network parameters \(\) that are shared by all synthetic images, and a multi-head SCM \(\{A_{i}^{h}\}_{h=1}^{H}\) for each synthetic image.

Feature Sparsification with Global Semantic Preservation.Sparse codes allow for the ranking of features , with higher coefficients indicating more important features. Therefore, we can select the most influential sub-parameters of SCM to achieve the desired sparsity \(||A_{i}^{h}||_{0} k\). This process reduces storage inefficiency while preserving the global semantics of synthetic images. Moreover, all existing matching objectives  optimize synthetic images using a specific training network, but these synthetic images are then used to train various agnostic network architectures. By pruning unnecessary features of synthetic images, we can further enhance their generalization ability on unseen network architectures. We experimentally validate this claim in Sec. 3.3. Specifically, given a learned SCM \(A^{K J}\), we assign a binary mask \(B^{K J}\) to it:

\[B[i,j]=1,&A[i,j]((A))\\ 0,&,\ =B A,\] (6)

where \(()\) obtains the largest \(k\) elements, and \(()\) takes the absolute value of each element of the input matrix. We operate a Hadamard product on the two to preserve the top-\(k\) efficient elements of the SCM, _i.e._, select the most critical epitomic features. By operating on each learned SCM, we receive \(\{\{_{i}^{h}\}_{h=1}^{H}\}_{i=1}^{N}\), which can be directly applied to synthesize images.

For the compressed storage of SCMs, we adopt the widely-used coordinate (COO) format, utilizing the uint8 data type to store the row and column coordinates of non-zero elements as the size of the sparse matrix is always smaller than or equal to \(256 256\) in our implementation. Consequently, the storage needed for each non-zero element is \(1.5\) times that of a single float32 tensor. In this way, the storage complexity of SCMs can be greatly reduced from \(O(NHKJ)\) to \(O(NHk)\), where \(k KJ\). The algorithm of our approach is summarized in Alg. 1.

[MISSING_PAGE_FAIL:5]

ImageNet into 6 subsets, namely, ImageNet, ImageWool, ImageNetFruit, ImageMeow, ImageSquawk, and ImageYellow, each consisting of 10 classes with resolutions of \(128 128\). And a 5-layer ConvNet is employed as the model for both training and evaluation.

Notably, our results achieved with IPC 1 storage space are highly competitive with the previous state-of-the-art results [21; 37] obtained with IPC 10. Specifically, we only need to employ \(10\%\) of their parameters to achieve similar or even better performance. Compared to the previous state-of-the-art  using the same IPC 1 storage space, our approach exhibits an average improvement of \(11.2\%\) across all subsets. Moreover, we maintain a substantial lead for IPC 10 storage space. For instance, we achieve \(71.8\%\) accuracy on ImageSquawk, which is a \(15.0\%\) improvement over the previous state-of-the-art . These outstanding outcomes are attributed to our design of sharing SAETs among patches, which enables SPEED to be more effective in reducing spatial redundancy as data resolution increases.

Continual Learning.Owing to its expressive representations and finer-grained image construction, SPEED has the ability to synthesize an informative dataset. Synthetic images of each class are of exceptional quality, thereby contributing to the dataset's overall richness and relevance. Following the DM  setup based on GDumb , we conduct continual learning experiments on CIFAR100 with IPC 20 storage space and use the default ConvNet and ResNet18 for evaluation. We randomly divide the 100 classes into 5 learning steps, that is, 20 classes per step. As illustrated in Fig. 2, SPEED maintains the highest test accuracy at all steps for both evaluation networks.

### Generalization

Universality to Matching Objectives.SPEED exhibits adaptability to multiple existing matching objectives, including those presented in [13; 16; 17], and can be directly integrated with them. In line with , we evaluate a diverse set of architectures, including the default ConvNet, ResNet , MLP, and ViT . The results and comparisons to corresponding baselines on CIFAR10 are presented in Tab. 3. For instance, when using MLP for evaluation under the IPC 1 budget, SPEED yields a

    & Dataset &  &  &  \\  & IPC & 1 & 10 & 50 & 1 & 10 & 50 & 1 & 10 & 50 \\   **ConvNet** \\ **F** \\  } & Random & 14.4\(\)2.0 & 26.0\(\)1.2 & 43.4\(\)1.0 & 4.2\(\)3.0 & 14.6\(\)0.5 & 30.0\(\)0.4 & 1.4\(\)0.1 & 5.0\(\)0.2 & 15.0\(\)0.4 \\  & Herding & 21.5\(\)1.3 & 31.6\(\)0.7 & 40.4\(\)0.6 & 8.4\(\)0.3 & 17.3\(\)0.3 & **33.7\(\)0.5** & **2.8\(\)0.2** & 6.3\(\)0.2 & **16.7\(\)0.3** \\  & K-Center & **23.3\(\)0.9** & **36.4\(\)0.6** & **48.7\(\)0.3** & **8.6\(\)0.3** & **20.7\(\)0.2** & 33.6\(\)0.4 & 2.7\(\)0.2 & **7.8\(\)0.4** & 16.7\(\)0.4 \\  & Forgetting & 13.5\(\)1.2 & 23.3\(\)1.0 & 23.3\(\)1.1 & 4.5\(\)0.3 & 9.8\(\)0.2 & - & 1.6\(\)0.1 & 5.1\(\)0.2 & 15.0\(\)0.3 \\   **ConvNet** \\ **F** \\  } & DC  & 28.3\(\)0.5 & 44.9\(\)0.5 & 53.9\(\)0.5 & 12.8\(\)0.3 & 25.2\(\)0.3 & 32.1\(\)0.3 & - & - & - \\  & DSA  & 28.8\(\)0.7 & 52.1\(\)0.5 & 60.6\(\)0.5 & 13.9\(\)0.3 & 32.3\(\)0.3 & 42.8\(\)0.4 & - & - & - \\  & KIP  & **49.9\(\)0.2** & 62.7\(\)0.3 & 68.6\(\)0.2 & 15.7\(\)0.2 & 28.3\(\)0.1 & - & - & - \\  & DM  & 26.0\(\)0.8 & 48.9\(\)0.6 & 63.0\(\)0.4 & 11.4\(\)0.3 & 3.9\(\)0.3 & 43.6\(\)0.4 & 3.9\(\)0.2 & 12.9\(\)0.4 & 24.1\(\)0.3 \\  & TM  & 46.3\(\)0.8 & 65.3\(\)0.7 & 71.6\(\)0.2 & 2.3\(\)0.3 & 40.1\(\)0.4 & **47.7\(\)0.2** & 8.8\(\)0.3 & 23.2\(\)0.2 & **28.0\(\)0.3** \\  & FRPo  & 46.8\(\)0.7 & **65.5\(\)0.4** & **71.7\(\)0.2** & **28.7\(\)0.1** & **42.5\(\)0.2** & 44.3\(\)0.2 & **15.4\(\)0.3** & **25.4\(\)0.2** & - \\   **ConvNet** \\ **F** \\  } & Parameters / Class & 3,072 & 30,720 & 153,600 & 3,072 & 30,720 & 153,600 & 12,288 & 122,880 & 614,400 \\    & IDC  & 50.0\(\)0.4 & 67.5\(\)0.5 & 74.5\(\)0.1 & - & 44.8\(\)0.2 & - & - & - & - \\  & HaBa  & 48.3\(\)0.8 & 69.9\(\)0.4 & 74.0\(\)0.2 & 33.4\(\)0.4 & 40.2\(\)0.2 & 47.0\(\)0.2 & - & - & - \\  & RTP  & **66.4\(\)0.4** & 71.2\(\)0.4 & 73.6\(\)0.5 & 34.0\(\)0.4 & 42.9\(\)0.7 & - & 16.0\(\)0.7 & - & - \\  & SPEED (Ours) & 63.2\(\)0.1 & **73.5\(\)0.2** & **77.7\(\)0.4** & **40.0\(\)0.4** & **45.9\(\)0.3** & **49.1\(\)0.2** & **26.9\(\)0.3** & **28.8\(\)0.2** & **30.1\(\)0.3** \\   Whole Dataset \\  } &  &  &  \\   

Table 1: Comparisons with previous dataset distillation and coreset selection methods on standard benchmarks. ”Matching” refers to dataset distillation methods with specific matching objectives. ”Param.” refers to dataset distillation methods with synthetic data parameterization. The bold **numbers** represent the highest accuracy achieved in each category.

   Dataset &  &  &  &  &  &  \\   & IPC & 1 & 10 & 10 & 1 & 10 & 1 & 10 & 10 & 1 & 10 & 10 \\  TM  & 47.7\(\)0.9 & 63.0\(\)1.3 & 28.6\(\)0.8 & 35.8\(\)1.8 & 26.6\(\)0.8 & 40.3\(\)1.3 & 30.7\(\)1.6 & 40.4\(\)2.2 & 39.4\(\)1.5 & 52.3\(\)1.0 & 45.2\(\)0.8 & 60.0\(\)1.5 \\  & FRPo  & **48.1\(\)0.7** & **66.5\(\)0.8** & **29.7\(\)0.6significant improvement of \(24.8\%\) and \(23.7\%\) in accuracy compared to the distribution  and trajectory  matching baselines, respectively. Our experiments reveal that the improvements in cross-architecture accuracy tend to be more significant than those on the ConvNet. For instance, with an IPC 10 budget for trajectory matching , we achieve a \(24.3\%\) gain on ResNet18. This outcome further showcases the strengths of our sparse parameterization approach in terms of generalization.

Cross-Architecture Performance.The main purpose of dataset distillation is to distill a synthetic dataset that is effective on various even unseen architectures. In this study, we evaluate the cross-architecture performance of our method by comparing it with previous synthetic data parameterization approaches [19; 21; 22] on CIFAR10, using IPC 10 storage space. The results presented in Tab. 4 demonstrate that SPEED continues to outperform other methods significantly in terms of generalization across unseen architectures. As an illustration, our method achieves an accuracy of \(51.5\%\) on ViT, which represents a \(3.6\%\) improvement over the previous state-of-the-art . Although various existing dataset distillation matching objectives tend to overfit the training network, we address this challenge by pruning unnecessary features through sparse parameterization.

Robustness to Corruption.To explore the out-of-domain generalization of our synthetic dataset, we conduct experiments on CIFAR100-C . In detail, we evaluate on ConvNet and ResNet18, using the synthetic dataset trained under the IPC 1 budget. Fig. 3 shows the average accuracy of 14 types of corruption under 5 levels respectively. Compared with the previous methods, we achieve better performance under all kinds of corruption. Especially on ResNet18, SPEED outperforms previous methods significantly, achieving almost _double_ the test accuracy under every corruption scenario. This demonstrates the generalization and robustness benefits brought by sparse parameterization.

### Ablation Study

Size of \(k\) for Feature Sparsification.Our total storage parameters are positively correlated with the size of \(k\), and the representation ability of the SCM is also related to this \(k\)-winner. Therefore, we

   Method & ConvNet & MLP & ResNet18 & ViT \\  IDC  & 67.5\(\)0.5 & 41.4\(\)0.2 & 62.9\(\)0.6 & 47.9\(\)0.8 \\ HaBa  & 69.9\(\)0.4 & 35.4\(\)0.4 & 60.2\(\)0.9 & 42.2\(\)0.6 \\ RTP  & 71.2\(\)0.4 & 27.2\(\)0.2 & 67.5\(\)0.1 & 35.7\(\)0.4 \\  SPEED & **73.5\(\)0.2** & **44.4\(\)0.4** & **69.5\(\)0.4** & **51.5\(\)0.3** \\   

Table 4: Comparision of cross-architecture generalization evaluation. SPEED exhibits outstanding leads on all architectures.

[MISSING_PAGE_FAIL:8]

and outperforms the previous state-of-the-art  in multiple settings, except when the number of synthetic images is too small. Tab. 6 studies the depth of FReNet, we conduct ablation experiments on TinyImageNet with IPC 1 storage space. Although using only one block can maximize the number of synthetic images per class (\(N/c\)), the lack of hierarchical features severely compromises the model's representational capacity, resulting in poor image quality. However, having too many blocks can result in increased reconstruction overhead. We find that FReNet with two blocks already achieves a great compromise between performance and computation cost.

Effects of Increasing Image Resolution.To study the effectiveness of our method in higher resolution scenarios, we performed experimental investigations on both the distribution matching  baseline and our method, using the ImageNette dataset with image sizes of 128\(\)128 and 256\(\)256. In line with the practices of previous methods that handle higher resolution images with deeper networks, we increased the depth of the ConvNet to 6 for the 256\(\)256 image size.

As shown in Tab. 10, when the resolution increases from 128\(\)128 to 256\(\)256, the gain brought by SPEED to the baseline also amplifies from \(24.9\%\) to \(28.2\%\). The results demonstrate that our method achieves more substantial improvements when applied to higher-resolution images.

## 4 Related Work

Dataset Distillation.Dataset distillation, proposed by Wang _et al._, aims to learn a smaller synthetic dataset so that the test performance of the model on the synthetic dataset is similar to that of the original dataset. For better matching objectives, Zhao _et al._ present a single-step matching framework, encouraging the result gradient of the synthetic dataset and the original dataset to be similar, further extended by [15; 54; 55; 19]. Subsequently, Cazenavette _et al._ introduce TM  to alleviate the cumulative error problem of single-step matching, which inspires a series of work [56; 57; 58]. To avoid the expensive computational overhead brought by complex second-level optimization, Zhao _et al._ suggest DM, a distribution matching approach, and Wang _et al._ explicitly align the synthetic and real distributions in the feature space of a downstream network. There are also some methods based on kernel ridge regression [59; 47; 60; 37], which can bring out a closed-form solution for the linear model, avoiding extensive inner loop training.

In terms of synthetic data parameterization, Kim _et al._ introduce IDC , using downsampling strategies to synthesize more images under the same storage budget. Zhao _et al._ propose to synthesize informative data via GAN [61; 62]. Deng _et al._ have explored how to compress datasets into bases and recall them by linear combinations. Liu _et al._ propose a dataset factorization approach, utilizing image bases and hallucinators for image synthesis. Lee _et al._ further factorize the dataset into latent codes and decoders. These parameterization methods are designed to find shareable or low-resolution image bases. Still, they either only consider the connections between synthetic images [63; 21; 22], or do not reduce the redundancy inside the synthetic image thoroughly . So the storage space of their image bases is still proportional to the dataset resolution, bringing out unsatisfactory performance on high-resolution datasets. SPEED is a universal synthetic data parameterization framework, in which the distillation and construction are spatial-agnostic. It allows for joint modeling correlations between and within the synthetic images, leading to lower storage redundancy and finer-grained image synthesis, thus performing competently on high-resolution datasets.

Sparse Coding.Sparse coding calls for constructing efficient representations of data as a combination of a few high-level patterns [31; 32; 33; 34; 35; 36]. It has been proven to be an effective approach in the field of image reconstruction and image classification [64; 65; 66; 67; 68; 69; 70]. Typically, a dictionary of basis functions (e.g. wavelets  or curvelets ) is used to decompose the image patches into coefficient vectors. By imposing sparsity constraints on the coefficient vectors, efficient sparse representations of the image patches can be obtained. In addition, the dictionary is not necessarily fixed, it can also be learned to adapt to different tasks [25; 26; 27; 28], and the penalty function can also be varied [73; 33; 74]. Prior sparse coding research has primarily focused on compressing individual images [75; 76], with an emphasis on achieving high compression ratios while minimizing perceptual distortion. However, there has been limited exploration of how to apply sparse coding to compress entire datasets in a

    & 128\(\)128 & 256\(\)256 & Gain \\  DM  & 28.6\(\)0.6 & 29.5\(\)1.1 & +0.9 \\ w.SPEED & 53.5\(\)1.2 & 57.7\(\)0.9 & +42.2 \\ Gain & **+24.9** & **+28.2** & **+3.3** \\   

Table 10: Effects of increasing image resolution on ImageNette.

way that enhances the training of downstream neural networks. Our work demonstrates that theories and techniques in sparse coding can provide valuable inspiration for developing dataset distillation methods.

Coreset Selection.Coreset selection identifies a representative subset of the original dataset . Its objective is in line with the goals of dataset distillation and can be utilized to tackle challenges such as continual learning  and active learning tasks . This technique typically performs better when the storage budget is relatively large, while dataset distillation demonstrates superior performance under extremely limited storage budgets .

## 5 Conclusion and Limitations

In this paper, we introduce SPEED, an efficient and generalizable solution for dataset distillation that offers the following merits: First, the spatial-agnostic epitomic tokens distilled by our method are not only shared between the different classes but also shared among patches of every synthetic image, regardless of their spatial locations. Such efficient modeling enables us to perform well on high-resolution datasets with much less spatial redundancy. Second, the proposed feature-recurrent network promotes hierarchical representation in an efficient recurrent manner, resulting in more informative synthetic data. Finally, the proposed feature sparsification mechanism improves both the storage efficiency and the generalization ability. SPEED achieves outstanding performance on various datasets and architectures through extensive experiments.

Limitations.Similar to the previous parameterization methods , decomposing the original synthetic datasets into different components will slightly increase distilling costs and incur reconstruction overhead. We alleviate this issue by designing the network to be as lightweight as possible. While the dataset synthesized by SPEED offers better privacy protection for users compared to the original dataset, there remains a possibility of privacy leakage.