# A Partially Supervised Reinforcement Learning Framework for Visual Active Search

Anindya Sarkar Nathan Jacobs Yevgeniy Vorobeychik

{anindya, jacbsn, yvorobeychik}@wustl.edu,

Department of Computer Science and Engineering

Washington University in St. Louis

###### Abstract

Visual active search (VAS) has been proposed as a modeling framework in which visual cues are used to guide exploration, with the goal of identifying regions of interest in a large geospatial area. Its potential applications include identifying hot spots of rare wildlife poaching activity, search-and-rescue scenarios, identifying illegal trafficking of weapons, drugs, or people, and many others. State of the art approaches to VAS include applications of deep reinforcement learning (DRL), which yield end-to-end search policies, and traditional active search, which combines predictions with custom algorithmic approaches. While the DRL framework has been shown to greatly outperform traditional active search in such domains, its end-to-end nature does not make full use of supervised information attained either during training, or during actual search, a significant limitation if search tasks differ significantly from those in the training distribution. We propose an approach that combines the strength of both DRL and conventional active search by decomposing the search policy into a prediction module, which produces a geospatial distribution of regions of interest based on task embedding and search history, and a search module, which takes the predictions and search history as input and outputs the search distribution. We develop a novel meta-learning approach for jointly learning the resulting combined policy that can make effective use of supervised information obtained both at training and decision time. Our extensive experiments demonstrate that the proposed representation and meta-learning frameworks significantly outperform state of the art in visual active search on several problem domains.

## 1 Introduction

Consider a scenario where a child is abducted and law enforcement needs to scan across hundreds of potential regions from a helicopter for a particular vehicle. An important strategy in such a search and rescue portfolio is to obtain aerial imagery using drones that helps detect a target object of interest (e.g., the abductor's car) [1; 2; 3; 4; 5]. The quality of the resulting photographs, however, is generally somewhat poor, making the detection problem extremely difficult. Moreover, security officers can only inspect relatively few small regions to confirm search and rescue activity, doing so sequentially.

We can distill some key generalizable structure from this scenario: given a broad area image (often with a relatively low resolution), sequentially query small areas within it (e.g., by sending security officers to the associated regions, on the ground) to identify as many target objects as possible. The number of queries we can make is typically limited, for example, by budget or resource constraints. Moreover, query results (e.g., detected search and rescue activity in a particular region) are _highly informative about the locations of target objects in other regions_, for example, due to spatial correlation. We refer to this general modeling framework as _visual active search (VAS)_. Numerous other scenarios share this broad structure, such as identification of drug or human trafficking sites, anti-poaching enforcement activities, identifying landmarks, and many others. Sarkar et al. recently proposed a visual active search (VAS) framework for geospatial exploration, as well as a deep reinforcement learning (DRL) approach for learning a search policy. However, the efficacy of this DRL approach remains limited by its inability to adapt to search tasks that differ significantly from those in the training data, with the DRL approach struggling in such settings even when combined with test-time adaptation methods.

To gain intuition about the importance of domain adaptivity in VAS, consider Figure 1. Suppose, we pre-train a policy by leveraging a fully annotated search tasks with _large vehicle_ as a target class to learn a search policy. Now the challenge in visual active search problem is how to utilize such a policy for our current task, that is to search for _small car_ given a broad aerial image. As depicted in Figure 1, an adaptive policy (right) initially makes a mistake but then quickly adapts to the current task by efficiently leveraging information obtained in response to queries to learning from its mistakes. In contrast, a non-adaptive policy (left) keeps repeating its mistakes and ultimately fails to find the region containing the target object.

Indeed, traditional active search approaches have been designed precisely with such adaptivity in mind [7; 8; 9; 10] by combining an explicit machine learning model that predicts labels over inputs with custom algorithmic approaches aiming to balance exploration (which improves prediction efficacy) and exploitation (to identify as many actual objects of interest as possible within a limited budget). However, as Sarkar et al.  have shown, such approaches perform poorly in VAS settings compared to DRL, despite the lack of effective domain adaptivity of the latter.

We combine the best of traditional active search and the state-of-the-art DRL-based VAS approach by developing a partially-supervised reinforcement learning framework for VAS (PSVAS). A PSVAS policy architecture is comprised of two module: (i) A _task-specific prediction module_ that learns to predict the locations of the target object based on the task aerial image and labels resulting from queries during the search process, and (ii) a _task-agnostic search module_ that learns a search policy given the predictions provided by the prediction module and prior search results. The key advantage of this decomposition is that it enables us to use supervised information _observed at decision time_ to update the parameters of the prediction module, without changing the search module. Furthermore, to learn search policies that are effective in the context of evolving predictions during the search, we propose a novel meta-learning approach to jointly learning the search module with the _initialization_ parameters of the prediction module (used at the beginning of each task). Finally, we generalize the original VAS framework to allow for multiple simultaneous queries (a common setting in practice), and develop the PSVAS framework and a meta-learning approach for such settings.

In summary, we make the following contributions:

* A novel partially supervised reinforcement learning (PSVAS) framework that enables effective adaptation of VAS search policies to out-of-distribution search tasks.
* A novel meta-learning approach (MPS-VAS) to learn initialization parameters of the prediction module jointly with a search policy that is robust to the evolving predictions during a search task.
* A generalization of the VAS problem to domains in which we can make multiple simultaneous queries, and a variant of MPS-VAS that learns how to choose a subset of queries to make in each search iteration.
* An extensive experimental evaluation on two publicly available satellite imagery datasets, xView and DOTA, in a variety of unknown target settings, demonstrating that the proposed approaches significantly outperform all baselines. Our code is publicly available at this link.

## 2 Preliminaries

We consider a generalization of the _visual active search (VAS)_ problem proposed by Sarkar et al. . The basic building block of VAS is a _task_, which centers around an aerial image \(x\) divided into

Figure 1: Comparative search strategy of non-adaptive and adaptive policy with _small car_ as a target.

\(N\) grid cells, so that \(x=(x^{(1)},x^{(2)},...,x^{(N)})\), with each grid cell a subimage. Broadly, the goal is to identify as many target objects of interest through iterative exploration of these grid cells as possible, subject to a budget constraint \(\). To this end, we represent the subset of grids containing the target object by associating each grid cell \(j\) with a binary label \(y^{(j)}\{0,1\}\), where \(y^{(j)}=1\) if the grid cell \(j\) contains the target object, and 0 otherwise. The complete label vector associated with the task is \(y=(y^{(1)},y^{(2)},...,y^{(N)})\). When we are faced with the task at decision time, we have no direct information about \(y\), but when we query a grid cell \(j\), we obtain the ground truth label \(y^{(j)}\) for this cell. Moreover, if \(y^{(j)}=1\), we also accrue utility from exploring \(j\). In the original variant of VAS, we can make a single query \(j\) in each time step. Here, we consider a natural generalization where we have \(R\) query resources (for example, \(R<N\) patrol units identifying traps in a wildlife conservation setting), so that we can make \(R\) queries in each time step. We assume that \(R\) is constant for convenience; it is straightforward to generalize our approach below if \(R\) is time-varying.

Let \(c(j,k)\) be the cost of querying grid cell \(k\) if we start in grid cell \(j\). For the very first query, we can define a dummy initial grid cell \(d\), so that cost function \(c(d,k)\) captures the initial query cost. Let \(q_{t}^{r}\) denote the set of queries performed in step \(t\) by a query resource \(r\). Our ultimate goal is to solve the following optimization problem:

\[&_{\{q_{t}^{r}\}}U(x;\{q_{t}^{r}\})_{t }y^{(q_{t}^{r})}\\ &:_{t 0}_{r=1}^{R}cq_{t-1}^{r},q_{t}^{ r}.\] (1)

Finally, we assume that we possess a collection of tasks (in this case, aerial images) for which we have annotated whether each grid cell contains the target object or not. This collection, which we will refer to as \(=\{(x_{i},y_{i})\}\), is comprised of images \(x_{i}\) with corresponding grid cell labels \(y_{i}\), where each \(x_{i}\) is composed of \(N\) elements \((x_{i}^{(1)},x_{i}^{(2)},,x_{i}^{(N)})\) representing the cells in the image, and each \(y_{i}\) contains \(N\) corresponding labels \((y_{i}^{(1)},y_{i}^{(2)},,y_{i}^{(N)})\).

The central technical goal in VAS is to learn an effective search policy that maximizes the total number of targets discovered, on average, for a sequence of tasks \(x\) on which we have no prior direct experience, given a set of resources \(R\), exploration cost function \(c(j,k)\), and total exploration budget \(\). Sarkar et al.  proposed a deep reinforcement learning (DRL) approach in which they learned a search policy \((x,o,B)\) that outputs at each time step \(t\) the grid we should explore at the next step \(t+1\), given the task \(x\), remaining budget \(B\), and information produced from the sequence of previous queries encoded into an observation vector \(o\) with \(o^{(j)}=2y^{(j)}-1\) if \(j\) has been explored, and \(o^{(j)}=0\) otherwise. The reward function for this DRL approach is naturally captured by \(R(x,o,j)=y^{(j)}\).

Note that active search (including VAS) is qualitatively distinct from active learning [11; 12; 13]: in the latter, the goal is solely to learn to predict well, so that the entire query process serves the goal of improving predictions. In active search, in contrast, we aim to learn a search policy that balances exploration (improving our ability to predict where target objects are) and exploitation (actually finding such objects) within a limited budget. Indeed, it has been shown that active learning approaches are not competitive in the VAS context .

## 3 Partially-Supervised Reinforcement Learning Approach for VAS

The DRL approach for VAS proposed by Sarkar et al.  is end-to-end, producing a policy that is only partly adaptive to observations \(o\) made during exploration for each task. In particular, this end-to-end policy aims to capture the tension between exploration and exploitation fundamental in active search [7; 9], without explicitly representing the central aim of exploration, which is to improve our ability to _predict_ which grid cells in fact contain the target object. In conventional active search, in contrast, this is directly represented by learning a prediction function \(f(x^{(j)})\) that is updated each time a grid \(j\) is explored during the search process, so that exploration directly impacts our ability to predict target locations, and thereby make better query choices in the future. While the end-to-end approach implicitly learns this, it would only do so effectively so long as tasks \(x\) we face at prediction time are closely related to those used in training. However, it does not take advantage of the _supervised_ information we obtain about which grid cells actually contain the target object, either at training or test time, potentially reducing the efficacy of learning as well as ability to adapt when task distribution changes.

To address this issue, we propose a novel _partially-supervised reinforcement learning_ approach for visual active search, which we refer to as PSVAS. In PSVAS, the search policy we obtain is a composition of two modules: 1) the task-specific prediction module \(f_{}(x,o)\) and 2) the task-agnostic search module \(g_{}(p,o,B)\), where \(\) and \(\) are trainable parameters, where \(p=f_{}(x,o)\) is the vector of predicted probabilities with \(p^{(j)}\) the predicted probability of a target in grid cell \(j\) (see Figure 2, and Supplement Section A for further details about the policy network architecture). Conceptually, \(f_{}\) makes predictions based solely on the task \(x\) and the prediction-relevant information gathered during the search \(o\), while \(g\) relies _solely_ on the information relevant to the search itself: predicted locations of objects \(p\), observations acquired during the search \(o\), and remaining budget \(B\). The search policy is then the composition of these modules, \((x,o,B)=g_{}(f_{}(x,o),o,B)\).

Although in principle we can still train the policy \(\) above end-to-end (effectively collapsing the composition), a key advantage of PSVAS is that it enables us to directly make use of supervised information about true labels \(y\) as they are observed either at training or decision time, using these to update \(f_{}\) in both cases. In prior work that relies solely on end-to-end policy representation, there was no straightforward way to take advantage of this information. Specifically, we train the policy using the following objective function:

\[_{}=_{RL}+_{}},\]

where \(\) is a hyperparameter. We represent \(_{RL}\) loss as follows:

\[_{RL}=-_{i=1}^{M}_{t=1}^{T_{i}}_{_{t =0}\,c(q_{t-1},q_{t})}_{(,)}(a_{i}^{t}|x_ {i},o_{i}^{t},B_{i}^{t})_{i}^{t}\]

Where \(M\) is the number of example search task seen during training and \(_{i}^{t}\) is the discounted cumulative reward defined as \(_{i}^{t}=_{k=t}^{T}^{k-t}R_{i}^{k}\) with a discount factor \(\). Note that the RL loss in this approach _also updates the parameters of the prediction module_, \(\), in an end-to-end fashion. We also represents \(_{}}\) as follows: \(_{}}=_{i=1}^{M}-(y_{i}(p_{i})+(1-y_{i})(1-p_ {i}))\).

The PSVAS algorithm is more formally presented in Algorithm 1. The combined RL and supervised loss yields a balance between using supervised information to improve the quality of initial predictions at the beginning of the episode and ensuring that these serve the goal of producing the best episode-level policies. However, it is still crucial to note that \(f_{}\) serves solely an _instrumental_ role in the process, with learning a search policy that _effectively adapts to each task_ the primary goal. Consequently, we ultimately wish to jointly learn \(g_{}\) and \(f_{}\) in such a way that \(f_{}\) facilitates adaptive search as it is updated during a search task at decision time. To address this, we propose a meta-learning approach, MPS-VAS, that learns \(g_{}\) along with an initial parametrization \(_{0}\) of \(f_{}\) for each task. At the beginning of each task, then, \(\) is initialized to \(_{0}\), and then updated as labels are observed after each query.

At the high level, MPS-VAS trains over a series of _episodes_, where each episode corresponds to a fully labeled training task \((x_{i},y_{i})\) and budget constraint \(\) (we vary the budget constraints during training). We begin an episode \(i\) with the current prediction function parameters \(_{i}=\) and search policy parameters \(\). We fix \(g_{}\) over the length of the episode, and use it to generate a sequence of queries (since the policy is stochastic, it naturally induces exploration). After observing the label \(y^{(j)}\)

Figure 2: The PSVAS policy network architecture.

for each queried grid cell \(j\) during the episode, we update \(f_{_{i}}\) using a standard supervised (binary cross-entropy) loss. At the completion of the episode (once we have exhausted the search budget \(\)), we update the policy parameters, as well as the _initialization_ prediction function parameters \(\) by combining RL and supervised loss. For the search module, we use the accumulated sum of rewards \(R_{i}=_{j}y^{(j)}\) over grids \(j\) explored during the episode, with the RL loss \(_{RL}\) based on the _REINFORCE_ algorithm . For the prediction module, we use the collected labels \(y^{(j)}\) during the episode and the standard binary cross-entropy loss. Finally, the MPS-VAS loss also explicitly trades off the RL and supervised loss: \(_{}=_{RL}+_{BCE} \). _The proposed meta-learning approach thus explicitly trains the policy to account for the evolution of the prediction during the episode_. The full MPS-VAS is presented more formally in Algorithm 2.

```
0: A search task instance \((x_{i},y_{i})\); budget constraint \(\); Prediction function (\(f\)) with current parameters \(_{i}\), i.e., \(f_{_{i}}\); Search policy (\(g\)) with current parameters \(_{i}\), i.e., \(g_{_{i}}\);
1:Initialize\(o^{0}=[0...0]\); \(B^{0}=\); step \(t=0\)
2:while\(B^{t}>0\)do
3:\(=f_{_{i}}(x_{i},o^{t})\)
4:\(j_{j\{\}}[g_{_{i}}( ,o^{t},B^{t})]\)
5: Query grid cell with index \(j\) and observe true label \(y^{(j)}\).
6: Obtain reward \(R^{t}=y^{(j)}\), Update \(o^{t}\) to \(o^{t+1}\) with \(o^{(j)}=2y^{(j)}-1\), and update \(B^{t}\) to \(B^{t+1}\) with \(B^{t+1}=B^{t}-c(k,j)\) (assuming we query \(k\)th grid at \((t-1)\)).
7: Collect transition tuple (\(\)) at step t, i.e., \(^{t}=\) ( state = \((x_{i},o^{t},B^{t})\), action = \(j\), reward = \(R^{t}\), next state = \((x_{i},o^{t+1},B^{t+1})\) ).
8:\(t t+1\)
9:endwhile
10: Update the prediction and search policy parameters, i.e., \(_{i+1}\) and \(_{i+1}\) respectively. ```

**Algorithm 1** The PSVAS algorithm.

The search policy \(\) produces a probability distribution over grid cells. However, since in our setting no advantage can be gained by querying previously queried grid cells, we simply renormalize the probability distribution induced by \(\) over the remaining grid cells, both during training and at decision time. Note that during inference, in both PSVAS and MPS-VAS framework, we freeze the parameters of the search module \(()\) and update the parameters of the prediction module \(()\) after observing query outcomes at each step using \(_{}\) loss between predicted label and pseudo label as shown in step \(()\) of Algorithm 2.

The discussion thus far effectively assumed that \(R=1\), that is, we make only a single query in each search time step. Next, we describe the generalization of our approach when we have multiple query resources \(R>1\). First, note that the prediction module \(f_{}\) is unaffected, since the number of query resources only pertain to the actual search strategy \(g_{}\). One way to handle \(R\) queries is to simply sample the search module (which is stochastic) iteratively \(R\) times without replacement during training, and to greedily choose the most probable \(R\) grid cells to query at decision time. We refer to this as MPS-VAS-topk. However, since the underlying problem is now combinatorial (the choice of \(R\) queries out of \(N\) grid cells), such a greedy policy may fail to capture important interdependencies among such search decisions.

To address this, we propose a novel policy architecture which is designed to learn how to optimize such a heuristic greedy approach for combinatorial grid cell selection in a way that is non-myopic and accounts for the interdependent effects of sequential greedy choices. Specifically, let \(\) be a vector corresponding to the grid cells, with \(_{j}=1\) if grid cell \(j\) has either been queried in the past (during previous search steps), or has been already chosen to be queried in the current search step, and \(_{j}=0\) otherwise. Thus, \(\) encodes the choices that have already been made, and enables the policy to learn the best next sequential choice to make using a greedy procedure through the same RL framework that we described above. We refer to this approach as MPS-VAS-MQ (in reference to multiple queries).

## 4 Experiments

### Experiment Setup

Since the goal of active search is to maximize the number of target objects identified, we use _average number of targets_ identified through exploration (**ANT**) as our evaluation metric.

We consider two ways of generating query costs: (i) \(c(i,j)=1\) for all \(i,j\), where \(\) is just the number of queries, and (ii) \(c(i,j)\) is based on Manhattan distance between \(i\) and \(j\). Most of the results we present in the main paper reflect the Manhattan distance based setting; we also report the results for uniform query costs in the Supplement. We use \(=0.1\) in all the experiments. We present the details of policy architecture and hyper-parameter details for each different experimental settings in the Supplement.

BaselinesWe compare the proposed PSVAS and MPS-VAS policy learning framework to the following baselines.

* _Random Search (RS)_, in which unexplored grid cells are selected uniformly at random.
* _Conventional Active Search (AS)_ proposed by Jiang et. al. , using a low-dimensional feature representation for each image grid from the same feature extraction network as in our approach.
* _Greedy Classification (GC)_, in which we train a classifier \(_{GC}\) to determine whether a grid contains a target object. We then prioritize the search in grids with the highest probability of containing the target object until the search budget is depleted.
* _Active Learning (AL)_, in which the first grid is selected randomly for querying, and the remaining grids are chosen using a state-of-the-art active learning approach by Yoo et al.  until the search budget is saturated.
* _Greedy Selection (GS)_, proposed by Uzkent et al. , that trains a policy \(_{GS}\) to assign a probability of zooming into each grid cell \(j\). We use this policy to select grids greedily until the search budget \(\) is exhausted.
* _End-to-end visual active search (E2EVAS)_, the state-of-the-art approach for VAS proposed by Sarkar et al. .

When dealing with a multi-query scenario, we compare the effectiveness of MPS-VAS-topk and MPS-VAS-MQ.

DatasetsWe evaluate the proposed approach using two datasets: xView  and DOTA . xView is a satellite imagery dataset which consists of large satellite images representing 60 categories, with approximately 3000 pixels in each dimensions. We use \(67\%\) and \(33\%\) of the large satellite images to train and test the policy network respectively. DOTA is also a satellite imagery dataset. We rescale the original \(3000 3000px\) images to \(1200 1200px\).

### Single Query Setting

We begin by considering a setting with a single query resource, as in most prior work. We first evaluate the proposed method on the xView dataset with varying search budget \(\{25,50,75\}\) and the number of equal sized grid cells \(N=49\). We train the policy with _small car_ as target and evaluate the performance of the policy with the following target classes : _Small Car_ (SC), _Helicopter_, _Sail Boat_ (SB), _Construction Cite_ (CC), _Building_, and _Helipad_. As the dataset contains variable size images, we take random crops of \(3500 3500\) for \(N=49\), ensuring equal grid cell sizes. We present the results with different values of \(N\) in the Supplement. The results with \(N=49\) are presented in Table 1. We observe significant improvements in performance of the proposed PSVAS approach compared to all baselines in each different target setting, ranging from \(3\) to \(25\%\) improvement relative to the most competitive E2EVAS method. The significance of utilizing supervised information of true labels \(y\), which are observed after each query at inference time, is supported by the obtained results. This highlights the effectiveness of the PSVAS framework, which enables us to update task-specific prediction module \(f\) by leveraging such crucial information in an efficient manner. The experimental outcomes also indicates two consistent trends. In each target setting, the overall search performance improves as \(\) increases, and the relative advantage of MPS-VAS over PSVAS increases, as it is better able to exploit the observed outcomes and in turn improve the policy further for the larger search budget \(\). We also observe that the extent of improvement in performance is greater when there is a greater difference between the target class used in training and the one used during inference. For example, when the target class is a _sail boat_, the improvement in performance of MPS-VAS in comparison to PSVAS ranges between \(~{}15\%\) to \(35\%\). However, if the target class is a _small car_, the improvement in performance of MPS-VAS is only between \(~{}1\%\) to \(2\%\). Considering all different target settings, performance improvement of MPS-VAS in comparison to PSVAS, ranges from \(1\%\) to \(35\%\). The results demonstrate the efficacy of the MPS-VAS framework in learning a search policy that enables adaptive search. Figure 3 demonstrates the exploration strategies of different policies that are trained on small car as the target class and test on sail boat as the target. The figure showcases the different exploration behaviors exhibited by each policy in response to the target class, highlighting the impact of the proposed _adaptive search_ framework on the resulting exploration strategies. In Table 2, we present similar results on the DOTA dataset with \(N\) as 64. Here, we train the policy with _Large Vehicle_ as target and evaluate the policy with the following target classes: _Ship_, _Large Vehicle_ (LV), _Harbor_, _Helicopter_, _Plane_, and _Roundabout_ (RB). We observe that MPS-VAS approach significantly outperforms all other baseline methods across different target scenarios. This shows the importance of MPS-VAS framework for deploying visual active search when search tasks differ significantly from those that are used for training.

    &  &  \\  Method & \(=25\) & \(=50\) & \(=75\) & \(=25\) & \(=50\) & \(=75\) & \(=25\) & \(=50\) & \(=75\) \\  RS & 0.16 & 0.39 & 0.72 & 0.35 & 0.71 & 1.02 & 2.41 & 3.56 & 4.62 \\ GC & 0.29 & 0.55 & 0.93 & 0.52 & 0.95 & 1.21 & 3.94 & 5.14 & 6.61 \\ GS  & 0.41 & 0.68 & 1.08 & 0.61 & 1.03 & 1.26 & 4.51 & 5.80 & 6.82 \\ AL  & 0.27 & 0.54 & 0.92 & 0.52 & 0.93 & 1.18 & 3.92 & 5.12 & 6.60 \\ AS  & 0.25 & 0.46 & 0.83 & 0.51 & 0.95 & 1.20 & 3.79 & 5.01 & 6.34 \\ E2EVAS  & 0.53 & 0.83 & 1.25 & 0.67 & 1.10 & 1.30 & 5.85 & 9.26 & 11.96 \\ OnlineTTA & 0.54 & 0.84 & 1.26 & 0.68 & 1.10 & 1.32 & 5.86 & 9.26 & 11.98 \\ 
**PSVAS** & **0.87** & **1.08** & **1.28** & **0.93** & **1.23** & **1.66** & **6.81** & **10.53** & **13.44** \\
**MPS-VAS** & **0.92** & **1.13** & **1.38** & **1.07** & **1.67** & **2.10** & **6.83** & **10.59** & **13.64** \\    &  &  \\  Method & \(=25\) & \(=50\) & \(=75\) & \(=25\) & \(=50\) & \(=75\) & \(=25\) & \(=50\) & \(=75\) \\  RS & 0.57 & 1.18 & 1.72 & 1.80 & 3.40 & 5.11 & 0.19 & 0.42 & 0.72 \\ GC & 1.05 & 1.81 & 2.14 & 2.64 & 4.88 & 7.04 & 0.41 & 0.78 & 1.04 \\ GS  & 1.12 & 1.97 & 2.48 & 3.35 & 5.39 & 7.52 & 0.54 & 0.93 & 1.12 \\ AL  & 1.04 & 1.77 & 2.12 & 2.62 & 4.88 & 7.03 & 0.40 & 0.76 & 1.03 \\ AS  & 1.02 & 1.61 & 2.03 & 2.43 & 4.61 & 6.95 & 0.38 & 0.75 & 0.98 \\ E2EVAS  & 1.43 & 2.31 & 2.98 & 4.73 & 7.43 & 9.59 & 0.81 & 1.20 & 1.46 \\ OnlineTTA & 1.43 & 2.33 & 2.99 & 4.75 & 7.44 & 9.59 & 0.83 & 1.21 & 1.46 \\ 
**PSVAS** & **1.62** & **2.49** & **3.14** & **5.51** & **8.33** & **10.52** & **0.91** & **1.22** & **1.47** \\
**MPS-VAS** & **1.74** & **2.64** & **3.47** & **5.55** & **8.40** & **10.69** & **0.96** & **1.30** & **1.63** \\   

Table 1: **ANT** comparisons when trained with _small car_ as target on xView in single-query setting.

### Multi Query Setting

Next, we evaluate the proposed MPS-VAS-MQ approach on the xView and DOTA dataset in multi query setting. In Table 3, we present the results with varying search budget \(=\{25,50,75\}\) and the number of equal sized grid cells \(N=49\). We consider \(K=3\) in all the experiments we perform in different target settings. We observe two general consistent trends across different target settings. First, the search performance of MPS-VAS in single query setting is always better than the performance of MPS-VAS-MQ in multi query settings, due to the fact that in single query setting the task-specific prediction module is updated \(K\) times more frequently than in the multi-query setting. Second, across various target settings, the performance improvement of MPS-VAS-MQ over MPS-VAS-topk ranges from \(0.08\%\) to \(3.5\%\). In Table 4, we present similar result with the number of grid cells \(N\) = \(64\) and train the policy with _large vehicle_ as the target class. We report the results with different values of \(N\) in the Supplement. Here the improvement of MPS-VAS-MQ over MPS-VAS-topk is up to \(3.5\%\) across different target settings, suggesting that there is added value from learning to capture interdependence in greedy search decisions.

### Effect of \(\) on Search Performance

We perform experiments with different choices of \(\) and found \(\) = 0.1 to be the best choice across all different experimental setup. For comparison, here we report the results in the case when we train the policy with different values of \(\) using _small car_ as a target class and test the policy with

    &  &  \\  Method & \(=25\) & \(=50\) & \(=75\) & \(=25\) & \(=50\) & \(=75\) & \(=25\) & \(=50\) & \(=75\) \\  Random & 1.08 & 2.11 & 3.06 & 1.48 & 2.96 & 3.91 & 1.41 & 2.72 & 3.90 \\ GC & 1.52 & 3.04 & 4.19 & 2.59 & 3.77 & 5.48 & 1.90 & 3.77 & 5.02 \\ GS & 1.79 & 3.56 & 4.47 & 2.72 & 4.10 & 5.77 & 2.31 & 4.14 & 5.87 \\ AL & 1.51 & 3.02 & 4.18 & 2.57 & 3.74 & 5.47 & 1.89 & 3.74 & 5.01 \\ AS & 1.43 & 2.87 & 4.01 & 1.64 & 3.15 & 4.23 & 1.73 & 3.45 & 4.68 \\ VAS & 2.45 & 4.37 & 5.87 & 5.33 & 8.47 & 10.51 & 3.12 & 5.04 & 6.82 \\ OnlineTTA & 2.46 & 4.38 & 5.89 & 5.33 & 8.47 & 10.52 & 3.12 & 5.06 & 6.83 \\ 
**PSVAS** & **2.46** & **4.41** & **6.00** & **5.33** & **8.52** & **10.59** & **3.15** & **5.24** & **6.94** \\
**MPS-VAS** & **3.08** & **5.25** & **7.13** & **5.34** & **8.53** & **10.63** & **3.82** & **6.77** & **9.00** \\    &  &  \\  Method & \(=25\) & \(=50\) & \(=75\) & \(=25\) & \(=50\) & \(=75\) & \(=25\) & \(=50\) & \(=75\) \\  Random & 0.27 & 0.46 & 0.71 & 1.37 & 2.55 & 3.62 & 1.47 & 2.53 & 3.81 \\ GC & 0.38 & 0.59 & 0.91 & 1.96 & 3.14 & 4.02 & 1.62 & 2.86 & 4.23 \\ GS & 0.42 & 0.66 & 1.01 & 2.26 & 3.56 & 4.71 & 1.91 & 3.24 & 4.68 \\ AL & 0.37 & 0.58 & 0.89 & 1.94 & 3.14 & 3.99 & 1.61 & 2.82 & 4.21 \\ AS & 0.34 & 0.53 & 0.82 & 1.89 & 3.06 & 3.92 & 1.55 & 2.71 & 4.06 \\ E2EVAS & 0.47 & 0.72 & 1.05 & 3.07 & 4.87 & 6.34 & 3.05 & 4.94 & 6.34 \\ OnlineTTA & 0.48 & 0.72 & 1.06 & 3.08 & 4.89 & 6.34 & 3.05 & 4.95 & 6.36 \\ 
**PSVAS** & **0.50** & **0.73** & **1.10** & **3.09** & **4.96** & **6.38** & **3.09** & **4.96** & **6.39** \\
**MPS-VAS** & **0.63** & **1.03** & **1.60** & **3.46** & **5.57** & **7.71** & **3.46** & **5.57** & **7.71** \\   

Table 2: **ANT** comparisons when trained with _large vehicle_ as target on DOTA in single-query setting.

Figure 3: Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using E2EVAS (top row), PSVAS (middle row), and MPS-VAS (bottom row).

[MISSING_PAGE_EMPTY:9]

## 5 Related Work

**RL for Visual Navigation** RL has found broad applicability in visual navigation tasks [18; 19; 20; 21]. While these tasks share some similarities at a high level, such as requiring a sequence of visual navigation steps based on a local view of the environment, they often do not involve search budget constraints and rely on a predetermined kinematic model of motion. In contrast, our approach involves observing the full environment, albeit potentially at a lower resolution, and sequentially determining which regions to query without being constrained to a particular kinematic model. This highlights the distinctive nature of our approach compared to traditional visual navigation tasks and demonstrates the potential value of active search strategies in addressing budget-constrained settings.

**Active Search** Active Search was first introduced by Garnett et al.  as a means to discover members of valuable and rare classes rather than solely focusing on learning an accurate model as in Active Learning . Subsequently, Jiang et al. [9; 22] proposed efficient nonmyopic active search techniques and incorporated search cost into the problem. Sarkar et al.  demonstrate that prior active search techniques do not scale well in high-dimensional visual space, and instead propose a DRL based visual active search framework for geo-spatial broad area search. However, the efficacy of the proposed approach by Sarkar et al.  can be limited when the search task varies between training and testing, which is often the case in real-world applications. In this work, we propose a novel framework that enables efficient and adaptive search in any previously unseen search task.

**Meta Learning** The concept of meta-learning, has consistently attracted attention in the field of machine learning [23; 24; 25; 26]. Finn et al.  present Model Agnostic Meta-Learning, a technique that utilizes SGD updates to rapidly adapt to new tasks. This approach, based on gradient-based meta-learning, can be seen as learning an effective parameter initialization, enabling the network to achieve good performance with just a few gradient updates. Wortsman et al.  introduces a self-adaptive visual navigation approach, which has the ability to learn and adapt to novel environments without the need for explicit supervision. Our work is significantly different than all these prior works as we observe true label at each step during search and hence the main challenge is how to leverage the supervised information in order to learn an efficient adaptive search policy.

**Foveated Processing of Large Images** Several studies have investigated the utilization of low-resolution images for guiding the selection of high-resolution regions to process [28; 29; 30; 31; 32; 33; 34; 35], with some employing reinforcement learning techniques [36; 15] to improve this process. However, our work differs significantly, as we focus on selecting a sequence of regions to query, where each query provides the true label, instead of a higher resolution image region. These labels are crucial for guiding further search and serving as an ultimate objective. As such, our approach tackles a unique challenge that differs from existing methods that rely on low-resolution imagery.

## 6 Conclusions

We present a novel approach for visual active search in which we decompose the search policy into a prediction and search modules. This decomposition enables us to combine supervised and reinforcement learning for training, and make use of supervised learning even during execution. Moreover, we propose a novel meta-learning framework to jointly learn a policy and initialization parameters for the supervised prediction module. Our findings demonstrate the significance of the proposed frameworks for conducting efficient search, particularly in real-world situations where search tasks may differ significantly from those utilized during policy training. We hope our framework will find its applicability in many practical scenarios ranging from human trafficking to animal poaching.

    &  &  &  \\  Method & \(=25\) & \(=50\) & \(=75\) & \(=25\) & \(=50\) & \(=75\) & \(=25\) & \(=50\) & \(=75\) \\  USVAS & 4.77 & 7.46 & 9.61 & 5.86 & 9.37 & 12.05 & 0.64 & 1.08 & 1.27 \\ PSVAS & 5.51 & 8.33 & 10.52 & 6.81 & 10.53 & 13.44 & 0.93 & 1.23 & 1.66 \\ MPS-VAS & **5.55** & **8.40** & **10.69** & **6.83** & **10.59** & **13.64** & **1.07** & **1.67** & **2.10** \\    &  &  &  \\  Method & \(=25\) & \(=50\) & \(=75\) & \(=25\) & \(=50\) & \(=75\) & \(=25\) & \(=50\) & \(=75\) \\  USVAS & 0.53 & 0.84 & 1.19 & 1.44 & 2.27 & 2.99 & 0.80 & 1.16 & 1.42 \\ PSVAS & 0.87 & 1.08 & 1.28 & 1.62 & 2.49 & 3.14 & 0.91 & 1.22 & 1.47 \\ MPS-VAS & **0.92** & **1.13** & **1.38** & **1.74** & **2.64** & **3.47** & **0.96** & **1.30** & **1.63** \\   

Table 7: **ANT** comparisons when trained with _small car_ as target on xView.