# RMLR: Extending Multinomial Logistic Regression

into General Geometries

 Ziheng Chen\({}^{1}\), Yue Song\({}^{2}\)1, Rui Wang\({}^{3}\), Xiao-Jun Wu\({}^{3}\), Nicu Sebe\({}^{1}\)

\({}^{1}\) University of Trento, \({}^{2}\) Caltech, \({}^{3}\) Jiangnan University

ziheng_ch@163.com, yuesong@caltech.edu

###### Abstract

Riemannian neural networks, which extend deep learning techniques to Riemannian spaces, have gained significant attention in machine learning. To better classify the manifold-valued features, researchers have started extending Euclidean multinomial logistic regression (MLR) into Riemannian manifolds. However, existing approaches suffer from limited applicability due to their strong reliance on specific geometric properties. This paper proposes a framework for designing Riemannian MLR over general geometries, referred to as RMLR. Our framework only requires minimal geometric properties, thus exhibiting broad applicability and enabling its use with a wide range of geometries. Specifically, we showcase our framework on the Symmetric Positive Definite (SPD) manifold and special orthogonal group \((n)\), _i.e._, the set of rotation matrices in \(^{n}\). On the SPD manifold, we develop five families of SPD MLRs under five types of power-deformed metrics. On \((n)\), we propose Lie MLR based on the popular bi-invariant metric. Extensive experiments on different Riemannian backbone networks validate the effectiveness of our framework. The code is available at https://github.com/GitZH-Chen/RMLR.

## 1 Introduction

In recent years, significant advancements have been achieved in Deep Neural Networks (DNNs), enabling them to effectively analyze complex patterns from various types of data, including images, videos, and speech . However, most existing models have primarily assumed the underlying data with a Euclidean structure. Recently, a growing body of research has emerged, recognizing that the latent spaces of many applications exhibit non-Euclidean geometries, such as Riemannian geometries . Various frequently-encountered manifolds in machine learning have posed interesting challenges and opportunities, including special orthogonal groups \((n)\), symmetric positive definite (SPD) , Gaussian , Grassmannian  spherical , and hyperbolic manifolds . These manifolds share an important Riemannian property -- their Riemannian operators, including geodesics, exponential & logarithmic maps, and parallel transportation, often possess closed-form expressions. Leveraging these Riemannian operators, researchers have successfully generalized different types of DNNs into manifolds, dubbed _Riemannian neural networks_.

Although Riemannian networks demonstrated success in many applications, most approaches still rely on Euclidean spaces for classification, such as tangent spaces , ambient Euclidean spaces , or coordinate systems . However, these strategies distort the intrinsic geometry of the manifold, undermining the effectiveness of Riemannian networks. Researchers have recently started directly developing Riemannian Multinomial Logistic Regression (RMLR) on manifolds. Inspired by the idea of hyperplane margin , Ganea et al.  developed a hyperbolic MLR in the Poincare ball for Hyperbolic Neural Networks (HNNs). Motivated by HNNs, Nguyen and Yang  developed three kinds of gyro SPD MLRs based on three distinct gyrostructures of the SPD manifold. In parallel, Chen et al.  proposed a framework for building SPD MLRs induced by the flat metrics on the SPD manifold. Nguyen et al.  proposed gyro MLRs for the Symmetric Positive Semi-definite (SPSD) manifold based on the product of gyro spaces. However, these classifiers often rely on specific Riemannian properties, limiting their generalizability to other geometries. For instance, the hyperbolic MLR  relies on the generalized law of sine, while the gyro MLRs [50; 51] rely on the gyro structures.

This paper presents a framework of RMLR over general geometries. In contrast to previous works, our framework only requires the explicit expression of the Riemannian logarithm, which is the minimal requirement in extending the Euclidean MLR into manifolds. Since this property is satisfied by many commonly encountered manifolds in machine learning, our framework can be broadly applied to various types of manifolds. Empirically, we showcase our framework on the SPD manifold and rotation matrices. On the SPD manifold, we systematically propose SPD MLRs under five families of power-deformed metrics. We also present a complete theoretical discussion on the geometric properties of these metrics. In the Lie group of \((n)\), we propose Lie MLR based on the widely used bi-invariant metric to build the Lie MLR. Our work is the first to extend the Euclidean MLR into Lie groups. Besides, our framework incorporates several previous Riemannian MLRs, including gyro SPD MLRs in , SPD MLRs in , and gyro SPSD MLRs in .

Our SPD MLRs are validated on four SPD backbone networks, including SPDNet  on the radar and human action recognition tasks and TSMNet  on the electroencephalography (EEG) classification tasks for the Riemannian feedforward network, RResNet  on the human action recognition task for the Riemannian residual network, and SPDGCN  on the node classification for the Riemannian graph neural network. Our Lie MLR is validated on the classic LieNet  backbone for the human action recognition task. Compared with previous non-intrinsic classifiers, our MLRs achieve consistent performance gains. Especially, our SPD MLRs outperform the previous classifiers by **14.23%** on SPDNet and **13.72%** on RResNet for human action recognition, and **4.46%** on TSMNet for EEG inter-subject classification. Furthermore, our Lie MLR can improve both the training stability and performance. In summary, our **main theoretical contributions** are the following: **(a)** We develop a general framework for designing Riemannian MLR over general geometries, incorporating several previous Riemannian MLRs on different geometries. **(b)** We systematically propose 5 families of SPD MLRs based on different geometries of the SPD manifold. **(c)** We propose a novel Lie MLR for deep neural networks on \((n)\).

**Main theoretical results:** We solve the Riemannian margin distance to the hyperplane in Thm. 3.2 and present our RMLR framework in Thm. 3.3. As shown in Tab. 1, our RMLR incorporates several existing MLRs on different geometries. Thm. 4.2 showcases our RMLR on the SPD manifold under five families of metrics summarized in Tab. 2. To remedy the numerical instability of BWM geometry on the SPD manifold, we also propose a backpropagation-friendly solver for the SPD MLR under BWM in App. F.2.2. Thm. 5.2 proposes the Lie MLR for the Lie group \((n)\). Due to the page limits, we put all the proofs in App. H.

## 2 Preliminaries

This section provides a brief review of the basic geometries of SPD manifolds and special orthogonal groups. Detailed review and notations are left in Apps. B and B.1.

**SPD manifolds:** The set of \(n n\) symmetric positive definite (SPD) matrices is an open submanifold of the Euclidean space \(^{n}\) of symmetric matrices, referred to as the SPD manifold \(^{n}_{++}\). There are five kinds of popular Riemannian metrics on \(^{n}_{++}\): Affine-Invariant Metric (AIM) , Log-Euclidean Metric (LEM) , Power-Euclidean Metrics (PEM) , Log-Cholesky Metric (LCM) , and Bures-Wasserstein Metric (BWM) . Note that, when power equals 1, the PEM is reduced to the Euclidean Metric (EM). Thawerds and Pennec  generalized AIM, LEM, and EM into two-parameters families of \((n)\)-invariant metrics, _i.e._,\((,)\)-AIM, \((,)\)-LEM, and \((,)\)-EM, with \((,+n)>0\). We denote the metric tensor of \((,)\)-AIM, \((,)\)-LEM, \((,)\)-EM, LCM, and BWM as \(g^{(,)}\), \(g^{(,)}\), \(g^{(,)}\), \(g^{}\), and \(g^{}\), respectively.

**Rotation matrices:** The special orthogonal group \((n)\) is the set of \(n n\) orthogonal matrices with unit determinant, the elements of which are also referred to as rotation matrices. As shown in , \((n)\) forms a Lie group. We adopt the widely used bi-invariant Riemannian metric .

## 3 Riemannian multinomial logistic regression

Inspired by , Ganea et al. , Nguyen and Yang , Chen et al. , Nguyen et al.  extended the Euclidean MLR into hyperbolic, SPD, and SPSD manifolds. However, these classifiers rely on specific Riemannian properties, such as the generalized law of sines, gyro structures, and flat metrics, which limits their generality. In this section, we first revisit several existing MLRs and then propose our Riemannian classifiers with minimal geometric requirements.

### Revisiting existing multinomial logistic regressions

Given \(C\) classes, the Euclidean MLR computes the multinomial probability of each class:

\[ k\{1,,C\}, p(y=k x)( a_{k},x -b_{k}),\] (1)

where \(b_{k}\), and \(x,a_{k}^{n}\{\}\). As shown in , the Euclidean MLR can be reformulated by the margin distance to the hyperplane:

\[p(y=k x)(( a_{k},x-p_{k} )\|a_{k}\|d(x,H_{a_{k},p_{k}})),\] (2) \[H_{a_{k},p_{k}}=\{x^{n}: a_{k},x-p_{k} =0\},\] (3)

where \( a_{k},p_{k}=b_{k}\), and \(H_{a_{k},p_{k}}\) is a hyperplane.

Eqs. (2) and (3) can be naturally extended into manifolds \(\) by Riemannian operators:

\[p(y=k S) ((_{k}, _{P_{k}}(S)_{P_{k}})\|_{k}\|_{P_{k}}(S,_{ _{k},P_{k}})),\] (4) \[_{_{k},P_{k}} =\{S:g_{P_{k}}(_{P_{k}}\,S,_{k })=0\},\] (5)

where \(P_{k},_{k} T_{P_{k}}\{\}\), \(g_{P_{k}}\) is the Riemannian metric at \(P_{k}\), and \(_{P_{k}}\) is the Riemannian logarithm at \(P_{k}\). The margin distance is defined as an infimum:

\[(S,_{_{k},P_{k}}))=_{Q_{_ {k},P_{k}}}d(S,Q).\] (6)

The MLRs proposed in [39; 23; 50; 16] can be viewed as different implementations of Eq. (4)-Eq. (6). To calculate the MLR in Eq. (4), one has to compute the associated Riemannian metrics, logarithmic maps, and margin distance. The associated Riemannian metrics and logarithmic maps often have closed-form expressions on the frequently-encounter manifolds in machine learning. However, the computation of the margin distance can be challenging. On the Poincare ball of hyperbolic manifolds, the generalized law of sines simplifies the calculation of Eq. (6) . However, the generalized law of sines is not universally guaranteed on other manifolds. Additionally, Chen et al.  developed a closed-form solution of margin distance on the SPD manifold under any metric pulled back from Euclidean spaces. For curved manifolds, solving Eq. (6) would become a non-convex optimization problem. To address this challenge, Nguyen and Yang  defined gyro structures on the SPD manifold and proposed a pseudo-gyrodistance to calculate the margin distance. Similarly, Nguyen et al.  proposed a pseudo-gyrodistance on the SPSD manifold based on the gyro product space. However, gyro structures do not necessarily exist in general geometries. _In summary, the aforementioned methods often rely on specific properties of their associated Riemannian metrics, which usually do not generalize to general geometries._

### Riemannian multinomial logistic regression

Recalling Eqs. (4) and (5), the least requirement of extending Euclidean MLR into manifolds is the well-definedness of \(_{P_{k}}(S)\) for each \(k\). In this subsection, we will develop Riemannian MLR, which depends solely on the Riemannian logarithm, without additional requirements, such as gyro structures and generalized law of sines. In the following, we always assume the well-definedness of the Riemannian logarithm. We start by reformulating the Euclidean margin distance to the hyperplane from a trigonometry perspective and then present our Riemannian MLR.

As we discussed before, obtaining the margin distance of Eq. (6) could be challenging. Inspired by , we resort to the perspective of trigonometry to reinterpret Euclidean margin distance. In Euclidean space, the margin distance is equivalent to

\[d(x,H_{a,p}))=( xpy^{*})d(x,p),y^{*}=*{ arg\,max}_{y H_{a,p}\{p\}}( xpy).\] (7)

We extend Eq. (7) to manifolds by the Riemannian trigonometry and geodesic distance, the counterparts of Euclidean trigonometry and distance.

**Definition 3.1** (Riemannian Margin Distance).: Let \(_{,P}\) be a Riemannian hyperplane defined in Eq. (5), and \(S\). The Riemannian margin distance from \(S\) to \(_{,P}\) is defined as

\[d(S,_{,P})=( SPY^{*})d(S,P),\] (8)

where \(d(S,P)\) is the geodesic distance, and \(Y^{*}=*{argmax}( SPY)\) with \(Y_{,P}\{P\}\). The initial velocities of geodesics define \( SPY\):

\[ SPY=_{P}Y,_{P}S _{P}}{\|_{P}Y\|_{P},\|_{P}S\|_{P}},\] (9)

where \(,_{P}\) is the Riemannian metric at \(P\), and \(\|\|_{P}\) is the associated norm.

The Riemannian margin distance in Def. 3.1 has a closed-form expression.

**Theorem 3.2**.: [\(\)] _The Riemannian margin distance defined in Def. 3.1 is given as_

\[d(S,_{,P})=_{P}S, _{P}|}{\|\|_{P}}.\] (10)

Putting the Eq. (10) into Eq. (4), we can a closed-form expression for Riemannian MLR.

**Theorem 3.3** (RMLR).: [\(\)] _Given a Riemannian manifold \(\{,g\}\), the Riemannian MLR induced by \(g\) is_

\[p(y=k S)(_{P_{k}}S,_{k}_{P_{k}}),\] (11)

_where \(P_{k}\), \(_{k} T_{P_{k}}\{\}\), and \(\) is the Riemannian logarithm._

\(_{k}\) in Eq. (11) can not be directly viewed as a Euclidean parameter, as \(_{k} T_{P_{k}}\) depends on \(P_{k}\) and \(P_{k}\) varies during the training. However, the tangent vector \(_{k}\) can be generated from a tangent space at a fixed point. Several tricks can be used, such as Riemannian parallel transportation , vector transportation , the differential of Lie group or gyrogroup translation [64; 65]. Following previous work [23; 16; 50], we focus on parallel transportation and Lie group translation:

\[_{k}=_{Q P_{k}}A_{k},\] (12) \[_{k}=L_{P_{k} Q_{}^{-1}*,Q}A_{k},\] (13)

where \(Q\) is a fixed point, \(A_{k} T_{Q}\{0\}\), \(\) is the parallel transportation along geodesic connecting \(Q\) and \(P_{k}\), and \(L_{P_{k} Q_{}^{-1}*,Q}\) denotes the differential map at \(Q\) of left translation \(L_{P_{k} Q_{}^{-1}}\) with \(P_{k} Q_{}^{-1}\) denoting Lie group product and inverse. In this way, \(A_{k}\) lies in a fixed tangent space and, therefore, can be optimized by a Euclidean optimizer.

_Remark 3.4_.: We make the following remarks w.r.t. our Riemannian MLR.

(a). The reformulation of Eq. (7) in gyro MLR [50; 51] and ours are different. Gyro MLR adopts gyro trigonometry and gyro distance to reformulate Eq. (7), while our method directly uses Riemannian trigonometry and geodesic distance.

(b). Compared with the MLRs on hyperbolic, SPD, or SPSD manifolds in [23; 50; 16; 51], our framework enjoys broader applicability, as our framework only requires the Riemannian logarithm. This property is commonly satisfied by most manifolds encountered in machine learning, such as the five metrics on SPD manifolds mentioned in Sec. 2, the invariant metric on \((n)\), and hyperbolic & spherical manifolds [11; 56]. Besides, several existing MLRs on different geometries are special cases of our Riemannian MLR, which are detailed in Tab. 1.

(c). The well-definedness of the Riemannian logarithm is a much weaker requirement compared to the existence of the gyro structure. The gyro structure not only requires the Riemannian logarithm but also implicitly requires geodesic completeness [50, Eqs. (1-2)]. For instance, on SPD manifolds, EM and BWM  are incomplete, undermining the well-definedness of gyro operations.

## 4 SPD multinomial logistic regressions

This section showcases our RMLR framework on the SPD manifold. We first systematically discuss the power-deformed geometries of SPD manifolds. Based on these metrics, we will develop five families of deformed SPD MLRs.

### Deformed geometries of SPD manifolds

As discussed in Sec. 2, there are five popular Riemannian metrics on SPD manifolds. These metrics can be all extended into power-deformed metrics. For a metric \(g\) on \(^{n}_{++}\), the power-deformed metric is defined as

\[_{P}(V,W)=}g_{P^{}}((_{ })_{*,P}(V),(_{})_{*,P}(W)), P^{n}_ {++},V,W T_{P}^{n}_{++},\] (14)

where \(_{}(P)=P^{}\) is the matrix power, and \((_{})_{*,P}\) is the differential map. The deformed metric \(\) can interpolate between a LEM-like metric (\( 0\)) and \(g\) (\(=1\)) . Previous work has extended \((,)\)-AIM, \((,)\)-LEM, LCM, and BWM into power-deformed metrics and \((,)\)-LEM is proven to be invariant under the power deformation . We denote these metrics as \((,,)\)-AIM , \((,)\)-LEM , \(2\)-BWM , and \(\)-LCM , respectively. The deformation of these metrics is discussed in App. E.1. We further define the power-deformed metric of \((,)\)-EM by Eq. (14), denoted as \((,,)\)-EM. We have the following for the deformation of \((,,)\)-EM.

**Proposition 4.1**.: \([](,,)\)_-EM interpolates between \((,)\)-LEM (\( 0\)) and \((,)\)-EM (\(=1\))._

So far, all five popular Riemannian metrics on SPD manifolds have been generalized into power-deformed families of metrics. We summarize their associated properties in Tab. 2 and present their theoretical relation in Fig. 1. We leave technical details in App. E.2.

### Five families of SPD multinomial logistic regressions

This subsection presents five families of specific SPD MLRs by our general framework in Thm. 3.3 and metrics discussed in Sec. 4.1. We focus on generating \(}\) by parallel transportation from the identity matrix, except for \(2\)-BWM. Since the parallel transportation under \(2\)-BWM would undermine numerical stability (please refer to App. F.2.1 for more details), we resort to a newly

    & Properties \\  \((,,)\)-LEM & Bi-Invariance, \((n)\)-Invariance, Geodesically Completeness \\  \((,,)\)-AIM & Lie Group Left-Invariance, \((n)\)-Invariance, Geodesically Completeness \\  \((,,)\)-EM & \((n)\)-Invariance \\  \(\)-LCM & Lie Group Bi-Invariance, Geodesically Completeness \\  \(2\)-BWM & \((n)\)-Invariance \\   

Table 2: Properties of deformed metrics on SPD manifolds (\( 0\) and \((,+n)>0\)).

    &  & Requirements & Incorporated \\  & & by Our MLR \\  Euclidean MLR (Eq. (1)) & Euclidean geometry & N/A & ✓(App. C) \\  Gyro SPD MLRs  & AIM, LEM \& LCM on \(^{n}_{++}\) & Gyro structures & ✓(Rem. 4.3) \\  Gyro SPSD MLRs  & SPSD product gyro spaces & Gyro structures & ✓(App. D) \\  Flat SPD MLRs  & \((,)\)-LEM \& \(()\)-LCM on \(^{n}_{++}\) & Pullback metrics from the Euclidean space & ✓(Rem. 4.3) \\  Ours & General Geometries & Riemannian logarithm & N/A \\   

Table 1: Several MLRs on different geometries are special cases of our MLR.

developed Lie group operation :

\[S_{1} S_{2}=L_{1}S_{2}L_{1}^{T}, S_{1},S_{2}_{++}^{n}.\] (15)

where \(L_{1}=(S_{1})\) is the Cholesky decomposition.

**Theorem 4.2** (SPD MLRs).: \(\) _By abuse of notation, we omit the subscripts \(k\) of \(A_{k}\) and \(P_{k}\). Given SPD feature \(S\), the SPD MLRs, \(p(y=k S_{++}^{n})\), are proportional to_

\[(,): [(S)-(P),A^{(,)}],\] (16) \[(,,): [(P^{-}S^{ }P^{-}),A^{(,)}],\] (17) \[(,,): [ S^{}-P^{},A ^{(,)}],\] (18) \[: [- +[(())-(())], A+(A) ],\] (19) \[2: [(P^{2}S^{2})^{}+(S^{2}P^{2})^{}-2P^{2},_{P^{2 }}(A^{})],\] (20)

_where \(A T_{I}_{++}^{n}\{0\}\) is a symmetric matrix, \(()\) is the matrix logarithm, \(_{P}(V)\) is the solution to the matrix linear system \(_{P}[V]P+P_{P}[V]=V\), known as the Lyapunov operator, \(()\) is the diagonal element-wise logarithm, \(\) is the strictly lower part of a square matrix, and \(()\) is a diagonal matrix with diagonal elements of a square matrix. Besides, \(_{*,P}\) is the differential maps at \(P\), \(=(S^{})\), \(=(P^{})\), and \(=(P^{2})\)._

The Lyapunov operator in Eq. (20) requires the eigendecomposition. However, the backpropagation of eigendecomposition involves \(}{{(_{1}-_{j})}}\), undermining the numerical stability. Therefore, we propose a numerically stable backpropagation for the Lyapunov operator, detailed in App. F.2.2.

As \(2 2\) SPD matrices can be embedded into \(^{3}\) as an open cone , we illustrate SPD hyperplanes induced by five families of metrics in Fig. 2.

_Remark 4.3_.: Our SPD MLRs extend the existing SPD MLRs . The pseudo-gyrodistance to a SPD hyperplane in [50, Thms. 2.23-2.25] is incorporated by our Thm. 3.2, while the flat SPD MLRs under \((,)\)-LEM and \(\)-LCM in [16, Cor. 4.1] are special cases of our Thm. 4.2. Furthermore, our approach extends the scope of prior work as neither  nor  explored SPD MLRs based on \((,,)\)-EM and \(2\)-BWM. The gyro operations in [50, Eq. (1)] implicitly requires geodesic completeness, whereas \((,,)\)-EM and \(2\)-BWM are incomplete. As neither \((,,)\)-EM nor \(2\)-BWM belong to pullback Euclidean metrics, the framework presented in  cannot be applied to these metrics. To the best of our knowledge, our work is the **first** to apply PEM and BWM to establish Riemannian neural networks, opening up new possibilities for utilizing these metrics in machine learning applications. Besides, neither Nguyen and Yang  nor Chen et al.  explore the deformed metrics for building SPD MLRs.

Figure 3: Conceptual illustration of a Lie hyperplane. Each pair of antipodal black dots corresponds to a rotation matrix with an Euler angle of \(\), while the green dots denote a Lie hyperplane.

Figure 2: Conceptual illustration of SPD hyperplanes induced by five families of Riemannian metrics. The black dots denote the boundary of \(_{++}^{2}\).

## 5 Lie multinomial logistic regression

This section introduces our Lie MLR on \((n)\) based on the general RMLR framework in Thm. 3.3. The Riemannian metric on \((n)\) is assumed to be the invariant metric in Tab. 13.

The two ways to generate \(_{k}\) in RMLR, _i.e._,Eqs. (12) and (13), are equivalent on \((n)\).

**Lemma 5.1**.: __

\[_{Q P}=L_{PQ^{-1}*,Q}, P,Q(n).\] (21)

Similar with SPD MLRs, we set \(Q=I\). The Lie MLR on \((n)\) is presented in the following.

**Theorem 5.2**.:  _The Lie MLR on \((n)\) is given as_

\[p(y=k R(n))(P_{k}^{}S),A_{k},\] (22)

_where \(P_{k}(n)\) and \(A_{k}(n)\)._

We refer to the Riemannian hyperplanes (Eq. (5)) on \((n)\) as Lie hyperplanes. As \((3)\) is homeomorphic to 3-dimensional real projective space \(^{3}\), Fig. 3 illustrates Lie hyperplanes in the closed ball in \(^{3}\) of radius \(\).

## 6 Experiments

We first validate our SPD MLRs on four SPD neural networks: SPDNet  and TSMNet  for Riemannian feedforward networks, RResNet  for Riemannian residual networks, and SPDGCN  for Riemannian graph neural networks. Then, we proceed with experiments of our Lie MLR under the classic LieNet architecture . The classifier in all the above networks is the LogEig MLR (matrix logarithm + FC + softmax), a Euclidean MLR on the tangent space at the identity matrix. We substitute the original non-intrinsic LogEig MLR in each baseline model with our RMLRs. Notably, the gyro SPD MLRs  are special cases of our SPD MLRs under the standard AIM, LEM, and LCM (\((,,)=(1,1,0)\)), while flat SPD MLRs  are incorporated by our SPD MLRs under \((,)\)-LEM and \(\)-LCM. More implementation details are presented in App. G.

### Experiments on the proposed SPD MLRs

In the following, we abbreviate _SPD MLR-metric_ as _metric_. For instance, \((,,)\)-AIM denotes the baseline endowed with the SPD MLR induced by \((,,)\)-AIM and (1,1,0) as the value of \((,,)\)

   &  &  &  &  &  \\   & LogEig MLR & & (1,1,0) & (0.5,1,0.05) & (1,1,0) & (1,1,0) & (0.5) & (1) & (1,5) \\  Balanced Acc. & 53.8349.77 & 53.3649.92 & **55.2748.68** & **54.849.21** & 53.51e140.02 & **55.5447.45** & 55.7148.57 & **56.438.79** \\  

Table 6: Inter-subject experiments of TSMNet with different MLRs on the Hinss2021 dataset.

   &  &  &  &  &  \\   & LogEig MLR & & (1,1,0) & (1,1,0) & (0.5,1,0.1/\(_{0}\)) & (1,1,0) & (0.5) & (0.25) & (1) & (0.5) \\ 
2-Block & 92.881.10 & **94.53a.05.95** & 94.240.50 & **94.93a.60.60** & 93.551.21 & **95.440.83** & 92.22.03 & **94.949.47** & 93.491.25 & **94.599.02** \\
2-Block & 93.749.45 & **94.32a.94** & **95.11e10.52** & 95.01e10.84 & 94.604.70 & **95.87.58** & 95.690.66 & **94.844.60.65** & 93.93a.90.98 & **95.166.07** \\  

Table 3: Comparison of SPDNet with LogEig against SPD MLRs on the Radar dataset.

   &  &  &  &  &  \\   & LogEig MLR & & (1,1,0) & (1,1,0) & (0.5,1,0.1/\(_{0}\)) & (1,1,0) & (0.5) & (0.25) & (1) & (0.5) \\ 
1-Block & 57.42e1.31 & 58.0740.64 & 66.3240.63 & **71.65a.08** & 56.97e0.61 & **70.2440.92** & 63.84e1.31 & **65.6640.73** \\
2-Block & 60.6940.66 & 60.7240.62 & 66.4040.87 & **70.566.39** & 60.69e1.02 & **70.4660.71** & 62.61e1.46 & **65.79e0.63** \\
3-Block & 60.7640.80 & 61.1440.94 & 66.7041.26 & **70.22e0.81** & 60.2840.91 & **70.204.91** & 62.33e1.25 & **65.7140.75** \\  

Table 5: Inter-session experiments of TSMNet with different MLRs on the Hinss2021 dataset.

[MISSING_PAGE_FAIL:8]

SPDNet and TSMNet to model features into the SPD manifold and directly use LogEig or our SPD MLRs for classification. The average results are presented in Tab. 9. The hyperparameters \((,,)\) are borrowed from Tabs. 3 to 6. Our SPD MLRs consistently outperform the vanilla LogEig MLR. Particularly on the HDM05 dataset, **the highest performance improvement by our SPD MLRs is 18.34%**, surpassing the non-intrinsic LogEig MLR by a large margin. Ablations on model efficiency are also discussed in App. G.1.5.

### Experiments on the proposed Lie MLR

We apply our Lie MLR into the classic \((n)\) network, _i.e.,_LieNet , where features are on the Lie group of \((3)(3)\). Following LieNet , we use G3D  and HDM05  datasets. We also extend the Riemannian optimization package geopopt  into \((3)\), allowing for Riemannian optimization. We find that Riemannian SGD performs best for LieNet. Tab. 10 presents the 10-fold average results of LieNet with or without Lie MLR. Note that on the HDM05 datasets, the LieNet might fail to converge, fluctuating between the validation accuracy of 70% - 75%. Therefore, we select 10-fold best performance out of 20-fold experiments. It can be observed that our Lie MLR can improve the performance of LieNet. Besides, our Lie MLR can also improve the training stability. On the HDM05 dataset, LieNet fails to converge in 8 out of 20 folds. However, when endowed with our Lie MLR, LieNet+LieMLR only encounters convergence failures in 2 folds.

## 7 Conclusions

This paper presents a novel and versatile framework for designing RMLR for general geometries, with a specific focus on SPD manifolds and \((n)\). On the SPD manifold, we systematically explore five families of Riemannian metrics and utilize them to construct five families of deformed SPD MLRs. On \((n)\), we develop the Lie MLR for classifying rotation matrices. Extensive experiments demonstrate the superiority of our intrinsic classifiers. We expect that our work could present a promising direction for designing intrinsic classifiers on diverse geometries.

    &  &  \\   & Mean\(\)STD & Max & Mean\(\)STD & Max \\  LogEig MLR & 87.91\(\)0.90 & 89.73 & 76.92\(\)1.27 & 79.11 \\ Lie MLR & **89.13\(\)1.7** & **92.12** & **78.24\(\)1.03** & **80.25** \\   

Table 10: Results of LogEig MLR against Lie MLR under the LieNet architecture.

    &  &  &  \\   & Mean\(\)STD & Max & Mean\(\)STD & Max & Mean\(\)STD & Max \\  LogEig MLR & 90.55 \(\) 4.83 & 96.85 & 78.04 \(\) 1.27 & 79.6 & 70.99 \(\) 5.12 & 77.6 \\ \((,,)\)-AM & 94.84 \(\) 2.27 & 98.43 & 79.79 \(\) 1.44 & 81.6 & 77.83 \(\) 1.08 & **80** \\ \((,,)\)-EM & 90.87 \(\) 5.14 & 98.03 & 79.05 \(\) 1.23 & 81 & 78.16 \(\) 2.41 & 79.5 \\ \((,)\)-LEM & **96.33 \(\) 2.19** & **98.82** & **79.89 \(\) 0.99** & 81.8 & **78.16 \(\) 2.41** & 79.5 \\ \(2\)-BWM & 91.93 \(\) 3.64 & 96.85 & 73.46 \(\) 2.18 & 77.7 & 73.22 \(\) 4.06 & 78.1 \\ \(\)-LCM & 93.01 \(\) 2.14 & 98.43 & 77.59 \(\) 1.20 & 80.1 & 74.46 \(\) 5.81 & 78.9 \\   

Table 8: Comparison of LogEig against SPD MLRs under the SPDGCN architecture.

    &  & HDM05 &  \\  & & & Inter-session & Inter-subject \\  LogEig MLR & 91.93 \(\) 1.30 & 48.43 \(\) 1.25 & 39.76 \(\) 7.60 & 44.66 \(\) 7.17 \\  \((,,)\)-AIM & 95.21 \(\) 0.81 & 49.17 \(\) 1.08 & 41.14 \(\) 7.26 & 45.89 \(\) 6.52 \\ \((,,)\)-EM & 92.25 \(\) 1.20 & 61.60 \(\) 0.69 & **45.78 \(\) 8.51 (\(\) 6.02)** & 45.84 \(\) 4.75 \\ \((,)\)-LEM & 95.09 \(\) 0.57 & 49.05 \(\) 0.91 & 40.88 \(\) 7.46 & **46.02 \(\) 5.96 (\(\) 1.36)** \\ \(2\)-BWM & 94.89 \(\) 0.41 & **66.77 \(\) 1.34 (\(\) 18.34)** & 44.84 \(\) 8.00 & 45.21 \(\) 7.44 \\ \(\)-LCM & **95.67 \(\) 0.61 (\(\) 3.74)** & 58.66 \(\) 0.51 & 43.17 \(\) 6.21 & 45.10 \(\) 6.20 \\   

Table 9: Comparison of LogEig against SPD MLRs for direct classification.