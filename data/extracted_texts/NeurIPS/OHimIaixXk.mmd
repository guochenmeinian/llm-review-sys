# Building the Bridge of Schrodinger:

A Continuous Entropic Optimal Transport Benchmark

 Nikita Gushchin

Skoltech

Moscow, Russia

n.gushchin@skoltech.ru

&Alexander Kolesov

Skoltech1

Moscow, Russia

a.kolesov@skoltech.ru

&Petr Mokrov

Skoltech1

Moscow, Russia

petr.mokrov@skoltech.ru

Polina Karpikova

Skoltech1

Moscow, Russia

polina.karpikova@skoltech.ru

&Andrey Spiridonov

Skoltech1

Moscow, Russia

andrew.spiridonov@skoltech.ru

&Evgeny Burnaev

Skoltech1

AIRI2

Moscow, Russia

e.burnaev@skoltech.ru

&Alexander Korotin

Skoltech1

AIRI2

Moscow, Russia

a.korotin@skoltech.ru

###### Abstract

Over the last several years, there has been significant progress in developing neural solvers for the Schrodinger Bridge (SB) problem and applying them to generative modelling. This new research field is justifiably fruitful as it is interconnected with the practically well-performing diffusion models and theoretically grounded entropic optimal transport (EOT). Still, the area lacks non-trivial tests allowing a researcher to understand how well the methods solve SB or its equivalent continuous EOT problem. We fill this gap and propose a novel way to create pairs of probability distributions for which the ground truth OT solution is known by the construction. Our methodology is generic and works for a wide range of OT formulations, in particular, it covers the EOT which is equivalent to SB (the main interest of our study). This development allows us to create continuous benchmark distributions with the known EOT and SB solutions on high-dimensional spaces such as spaces of images. As an illustration, we use these benchmark pairs to test how well existing neural EOT/SB solvers actually compute the EOT solution. Our code for constructing benchmark pairs under different setups is available at:

https://github.com/ngushchin/EntropicOTBenchmark

Diffusion models are a powerful tool to solve image synthesis [25; 45] and image-to-image translation [50; 47] tasks. Still, they suffer from the time-consuming inference which requires modeling thousands of diffusion steps. Recently, the **Schrodinger Bridge** (SB) has arisen as a promising framework to cope with this issue [15; 9; 54]. Informally, SB is a special diffusion which has rather _straight trajectories_ and _finite time horizon_. Thus, it may require fewer discretization steps to infer the diffusion.

In addition to promising practical features, SB is known to have good and well-studied theoretical properties. Namely, it is _equivalent_ to the **Entropic Optimal Transport** problem (EOT, ) about moving the mass of one probability distribution to the other in the most efficient way. This problem has gained a genuine interest in the machine learning community thanks to its nice sample complexity properties, convenient dual form and a wide range of applications .

Expectiedly, recent **neural EOT/SB** solvers start showing promising performance in various tasks . However, it remains unclear to which extent this success is actually attributed to the fact that these methods properly solve EOT/SB problem rather than to a good choice of parameterization, regularization, tricks, etc. This ambiguity exists because of the **lack of ways to evaluate the performance of solvers qualitatively** in solving EOT/SB. Specifically, the class of continuous distributions with the analytically known EOT/SB solution is narrow (Gaussians ) and these solutions have been obtained only recently. Hence, although papers in the field of neural EOT/SB frequently appear, we never know how well they actually solve EOT/SB.

**Contributions**. We develop a generic methodology for evaluating continuous EOT/SB solvers.

1. We propose a generic method to create continuous pairs of probability distributions with analytically known (by our construction) EOT solution between them (SS3.1, SS3.2).
2. We use log-sum-exp of quadratic functions (SS3.3, SS3.4) to construct pairs of distributions (SS4) that we use as a benchmark with analytically-known EOT/SB solution for the quadratic cost.
3. We use these **benchmark pairs** to evaluate (SS5) many popular neural EOT/SB solvers (SS2) in high-dimensional spaces, including the space of \(64 64\) celebrity faces.

In the field of neural OT, there already exist several _benchmarks_ for the Wasserstein-2 , the Wasserstein-1  and the Wasserstein-2 barycenter  OT tasks. Their benchmark construction methodologies work **only** for specific OT formulations and **do not** generalize to EOT which we study.

## 1 Background: Optimal Transport and Schrodinger Bridges Theory

We work in Euclidean space \(==^{D}\) equipped with the standard Euclidean norm \(\|\|\). We use \(()=()=(^{D})\) to denote the sets of Borel probability distributions on \(,\), respectively.

**Classic (Kantorovich) OT formulation**. For two distributions \(_{0}()\), \(_{1}()\) and a cost function \(c:\), consider the following problem (Fig. 0(a)):

\[_{c}(_{0},_{1})}}{{=}}_{(_{0},_{1})}_{ }c(x,y)d(x,y),\] (1)

where the optimization is performed over the set \((_{0},_{1})\) of transport plans, i.e., joint distributions on \(\) with marginals \(_{0}\), \(_{1}\), respectively. The set \((_{0},_{1})\) is non-empty as it always contains the trivial plan \(_{0}_{1}\). With mild assumptions, a minimizer \(^{*}\) of (1) exists and is called an _OT plan_. Typical examples of \(c\) are powers of Euclidean norms, i.e., \(c(x,y)=\|x-y\|^{q}\), \(q 1\).

**Weak OT formulation**. Let \(C:()\{+\}\) be a weak cost which takes a point \(x\) and a distribution of \(y\) as inputs. The weak OT cost between \(_{0}\), \(_{1}\) is (Fig. 0(b))

\[_{C}(_{0},_{1})}}{{=}}_{(_{0},_{1})}_{}C(x,( |x))d_{0}(x)=_{(_{0},_{1})}_{ }C(x,(|x))d_{0}(x),\] (2)

where \((|x)\) denotes the conditional distribution of \(y\) given \(x\) and \(_{0}\) is the projection of \(\) to \(\) which equals \(_{0}\) since \((_{0},_{1})\). Weak OT formulation (2) generalizes classic OT (1): it suffices to pick \(Cx,(|x)=_{}c(x,y)d(y|x)\) to obtain (1) from (2). A more general case of a

Figure 1: Classic (Kantorovich’s) and weak OT formulations.

weak cost is \(Cx,(|x)=_{}c(x,y)d(y|x)+(|x),\) where \(>0\) and \(:()\) is some functional (a.k.a. regularizer), e.g., variance , kernel variance  or entropy . With mild assumptions on the weak cost function \(C\), an OT plan \(^{*}\) in (2) exists. We say that the family of its conditional distributions \(\{^{*}(|x)\}_{x}\) is the _conditional OT plan_.

**Entropic OT formulation**. It is common to consider entropy-based regularizers for (1):

\[\{ &^{(1)}_{c,c}(_{0}, _{1})\\ &^{(2)}_{c,c}(_{0},_{1})\\ &_{c,c}(_{0},_{1}) .}}{{=}}_{(_{0},_{1})}_{}c(x,y)(x,y)+ \{ &+(\|_{0} _{1}),\\ &- H(),\\ &-_{}\!H(|x)d _{0}(x)..\] (3)

Here KL is the Kullback-Leibler divergence and \(H\) is the differential entropy, i.e., the minus KL divergence with the Lebesgue measure. Since \((_{0},_{1})\), it holds that \((\|_{0}_{1})=H(_{0 })-_{}H(y|x)d_{0}(x)=-H()+H( _{0})+H(_{1})\), i.e., these formulations are equal up to an additive constant when \(_{0}_{ac}()\) and \(_{1}_{ac}()\) and have finite entropy. Here we introduce "\(ac\)" subscript to indicate the subset of absolutely continuous distributions. With mild assumptions on \(c,_{0},_{1}\), the minimizer \(^{*}\) exists, it is **unique** and called the entropic OT plan. It is important to note that _entropic OT_ (5) _is a case of weak OT_ (2). Indeed, for the weak cost

\[C_{c,}(x,(|x))}}{{=}}_{ }c(x,y)d(y|x)- H(|x),\] (4)

formulation (2) immediately turns to (5). This allows us to apply the theory of weak OT to EOT.

**Dual OT formulation.** There exists a wide range of dual formulations of OT , WOT  and EOT . We only recall the particular dual form for WOT from  which serves as the main theoretical ingredient for our paper. For technical reasons, from now on we consider only \(_{1}_{p}()()\) for some \(p 1\), where subscript "\(p\)" indicates distributions with a finite \(p\)-th moment. We also assume that the weak cost \(C:_{p}()\{+\}\) is lower bounded, _convex_ in the second argument and jointly lower-semicontinuous in \(_{p}()\). In this case, a minimizer \(^{*}\) of WOT (2) exists [3, Theorem 3.2] and the following dual formulation holds [3, Eq. 3.3]:

\[_{C}(_{0},_{1})=_{f}_{}f^{C}(x)d_{0}(x)+_{}f(y)d_{1}(y) },\] (5)

where \(f_{p}()}}{{=}}\{f: \) continuous s.t. \(,:\;|f()|\|\|^{p}+\}\) and \(f^{C}\) is the so-called _weak \(C\)-transform_ of \(f\) which is defined by

\[f^{C}(x)}}{{=}}_{_{p}( )}\{C(x,)-_{}f(y)d(y)\}.\] (6)

Function \(f\) in (5) is typically called the dual variable or the _Kantorovich potential_.

**SB problem with Wiener prior**. Let \(\) be the space of \(^{D}\)-valued functions of time \(t\) describing trajectories in \(^{D}\), which start at time \(t=0\) and end at time \(t=1\). We use \(()\) to denote the set of probability distributions on \(\), i.e., stochastic processes.

Consider two distributions \(_{0}_{2,ac}()\) and \(_{1}_{2,ac}()\) with finite entropy. Let \((_{0},_{1})()\) be the subset of processes which have marginals \(_{0}\) and \(_{1}\) at times \(t=0\) and \(t=1\), respectively. Let \(dW_{t}\) be the differential of the standard \(^{D}\)-valued Wiener process. Let \(W^{}()\) be the Wiener process with the variance \(>0\) which starts at \(_{0}\) at time \(t=0\). It can be represented via the following stochastic differential equation (SDE): \(dX_{t}=dW_{t}\) with \(X_{0}_{0}\).

The Schrodinger Bridge problem with the **Wiener prior** is the following:

\[_{T(_{0},_{1})}(T\|W^{ }).\] (7)

The \(\) is attained uniquely at some process \(T^{*}\)[38, Proposition 4.1]. This process turns out to be a **diffusion** process and can be (uniquely) represented as the following SDE:

\[T^{*} : dX_{t}=v^{*}(X_{t},t)dt+dW_{t},\] (8)

Figure 2: The bridge of Schrödinger.

where \(v^{*}:^{D}^{D}\) is its drift function which we call the _optimal drift_. Hence, in (9), one may consider only diffusion processes \((_{0},_{1})\) with the volatility \(\) coinciding with the volatility of the Wiener prior \(W^{}\). In turn, solving SB can be viewed as finding the optimal drift \(v^{*}\).

**Link between SB and EOT problem.** The process \(T^{*}\) solving SB (9) is related to the solution \(^{*}\) of EOT problem (5) _with the quadratic cost function \(c(x,y)=\|x-y\|^{2}\)_. We start with some notations. For a process \(T()\), denote the joint distribution at time moments \(t=0,1\) by \(^{T}()\). Let \(T_{|x,y}\) be the distribution of \(T\) for \(t(0,1)\) conditioned on \(T\)'s values \(x,y\) at \(t=0,1\).

For the solution \(T^{*}\) of SB (9), it holds that \(^{T^{*}}=^{*}\), where \(^{*}\) is the EOT plan solving (5).

Moreover, \(T^{*}_{|x,y}=W^{}_{|x,y}\), i.e., informally, the _"inner"_ part of \(T^{*}\) matches that of the prior \(W^{}\).

Conditional process \(W^{}_{|x,y}\) is well-known as the **Brownian Bridge**. Due to this, given \(x,y\), simulating the trajectories of \(W^{}_{|x,y}\) is rather straightforward. Thanks to this aspect, SB and EOT can be treated as _nearly_ equivalent problems. Still EOT solution \(^{*}\) does not directly yield the optimal drift \(v^{*}\). However, it is known that the density \((x,y)}{d(x,y)}\) of \(^{*}\) has the specific form [38, Theorem 2.8], namely, \((x,y)}{d(x,y)}=^{*}(x)(y|x,  I)^{*}(y)\), where functions \(^{*},^{*}:^{D}\) are called the _Schrodinger potential_. From this equality one gets the expression for \(^{*}()\) and the density of \(^{*}(|x)\):

\[(y|x)}{dy}(y|x, I)^{*}(y) ^{*}(y)(y|x)}{dy} (y|x, I)^{-1}\] (11)

up to multiplicative constants. One may recover the optimal drift \(v^{*}\) via [38, Proposition 4.1]

\[v^{*}(x,t)=_{^{D}}(y|x,(1-t)  I_{D})^{*}(y)dy.\] (12)

Here the normalization constant vanishes when one computes \(()\). Thus, technically, knowing the (unnormalized) density of \(^{*}\), one may recover the optimal drift \(v^{*}\) for SB (9).

## 2 Background: Solving Continuous OT and SB Problems

Although OT (2), EOT (5) and SB (9) problems are well-studied in theory, solving them in practice is challenging. Existing OT solvers are of two main types: _discrete_ and _continuous_. **Our benchmark is designed for continuous EOT solvers**; discrete OT/EOT is out of the scope of the paper.

Continuous OT assumes that distributions \(_{0}\) and \(_{1}\) are continuous and accessible only via their random samples \(X=\{x_{1},,x_{N}\}_{0}\) and \(Y=\{y_{1},,y_{M}\}_{1}\). The goal is to recover an OT plan \(^{*}\) between _entire_\(_{0}\) and \(_{1}\) but using only \(X\) and \(Y\). Most continuous OT solvers do this via employing neural networks to implicitly learn the conditional distributions \((|x)^{*}(|x)\). In turn, SB solvers learn the optimal drift \( v^{*}\) but it is anyway used to produce samples \(y(|x)\) via solving SDE \(dX_{t}=(x,t)dt+dW_{t}\) starting from \(X_{0}=x\) (sampled from \(_{0}\)) at time \(t=0\).

After training on available samples \(X\) and \(Y\), continuous solvers may produce \(y(|x_{})\) for previously unseen samples \(x_{}_{0}\). This is usually called the out-of-sample estimation. It allows applying continuous OT solver to generative modelling problems such as the **image synthesis** (_noise-to-data_) and **translation** (_data-to-data_). In both these cases, \(_{1}\) is a data distribution, and \(_{0}\) is either a noise (in synthesis) or some other data distribution (in translation). Many recent OT solvers achieve competitive performance in synthesis [12; 15; 46] and translation [35; 34] tasks.

Continuous OT/SB solvers are usually referred to as **neural OT/SB** because they employ neural networks. There exist a lot of neural OT solvers for classic OT (1) [46; 17; 56; 40; 30; 19], see also [32; 31] for **surveys**, weak OT (2) [34; 35; 1], entropic OT (5) [49; 14; 42] and SB (9) [52; 15; 9; 23]. Providing a concise but still explanatory overview of them is nearly impossible as the _underlying principles of many of them are rather different and non-trivial_. We list only EOT/SB solvers which are relevant to our benchmark and provide a brief summary of them in Table 1. In SS5, we test all these solvers on our continuous benchmark distributions which we construct in subsequent SS3.

**Approaches to evaluate solvers.** As seen from Table 1, each paper usually tests its solver on a restricted set of examples which rarely intersects with those from the other papers. In particular, some papers consider _data\(\)data_ tasks, while the others focus on _noise\(\)data_. Due to this, there is no clear understanding of the superiority of one solver over the other. Importantly, in many cases, the _quantitative_ evaluation is done exclusively via the metrics of the downstream task. For example,  consider image generation or translation tasks and test the quality of generated images via FID . That is, they compare generated _marginal_ distribution \(_{1}\) with target \(_{1}\). This allows to access the generative performance of solvers but gives **no hint whether they actually learn the true EOT/SB solution**. Works  do a step toward opening the veil of secrecy and test their solvers in the Gaussian case. Unfortunately, it is rather trivial and may not be representative.

## 3 Constructing Benchmark Pairs for OT and SB: Theory

In this section, we present our theoretical results allowing us to construct pairs of distributions with the EOT/SB solution known by the construction. We provide proofs in Appendix A.

### Generic Optimal Transport Benchmark Idea

For a given distribution \(_{0}()\), we want to construct a distribution \(_{1}_{p}()\) such that some OT plan \(^{*}(_{0},_{1})\) for a given weak OT cost function \(C\) between them is known by the construction. That is, \(^{*}_{0}=_{0}\), \(^{*}_{1}=_{1}\) and \(^{*}\) minimizes (2). In this case, \((_{0},_{1})\) may be used as a **benchmark pair** with a known OT solution. Our following main theorem provides a way to do so.

**Theorem 3.1** (Optimal transport benchmark constructor).: _Let \(_{0}()\) be a given distribution, \(f^{*}_{p}()\) be a given function and \(C:_{p}()\) be a given jointly lower semi-continuous, convex in the second argument and lower bounded weak cost. Let \(^{*}()\) be a distribution for which \(^{*}_{0}=_{0}\) and for all \(x\) it holds that_

\[^{*}(|x)*{argmin}_{_{p}( )}\{C(x,)-_{}f^{*}(y)d(y)\}.\] (13)

_Let \(_{1}}}{{=}}^{*}_{1}\) be the second marginal of \(^{*}\) and assume that \(_{1}_{p}()\). Then \(^{*}\) is an **OT plan** between \(_{0}\) and \(_{1}\) (it minimizes (2)) and \(f^{*}\) is an **optimal dual potential** (it maximizes (7))._

Thanks to our theorem, given a pair \((_{0},f^{*})()_{p}( )\) of a distribution and a potential, one may produce a distribution \(_{1}\) for which an OT plan between them is known by the construction. This may be done by picking \(^{*}()\) whose conditionals \(^{*}(|x)\) minimize (13).

While our theorem works for rather general costs \(C\), it may be non-trivial to compute a minimizer \(^{*}(|x)\) in the weak \(C\)-transform (8), e.g., to sample from it or to estimate its density. Also, we note that our theorem states that \(^{*}\) is optimal but does not claim that it is the unique OT plan. These aspects may complicate the usage of the theorem for constructing the benchmark pairs \((_{0},_{1})\) for general costs \(C\). Fortunately, both these issues vanish when we consider EOT, see below.

### Entropic Optimal Transport Benchmark Idea

For \(C=C_{c,}\) (6) with \(>0\), the characterization of minimizers \(^{*}(|x)\) in (13) is almost explicit.

    & **Solver** &  & **Evaluated as** &  **tested in generation** \\ _(noise -data)_ \\  &  **Tested in translation** \\ _(data -data)_ \\  \\   &  & Solves classic dual EOT [20, \(\).1] with 2 NNs. &  \(\) \\  } &  &  MNIST\(\)USPS(16x16), \\ USPS\(\)MNIST (16x16), \\ SVHN\(\)MNIST (3x32x32) \\  } \\    &  & Combines LSOT’s potentials with a score model for &  &  \(\) \\  } & &  CelebA Upscale (**3x6x64**) \\  } \\    & & & & & & \\    &  & Solves max-min reformulation of weak OT &  \(\) This is a generic neural solver for weak OT \\ but it has **not** been tested with the **entropic** cost function. \\  } \\    & & dual (7) with 2 NNs (transport map and potential). & & &  \(\) This is a generic neural solver for weak OT \\ but it has **not** been tested with the **entropic** cost function. \\  } \\    &  & Employs energy-based modeling (EBM ) & & & & \\    & & to solve weak EOT dual (7); non-minimax; 1NN. & & & & \\   &  & Solves max-min reformulation of SB with 2 NNs (potential and SDE drift). &  &  CelebA Upscale (**3x6x64**), \\ Colored MNIST 2\(\)3 (3x32x32) \\  } \\    &  Alletime solving of two half Bridle (HB) problems. \\ HB is solved via full estimation with GP . \\  } & & & & \\     &  & Iterative Mean-Matching Proportional Fitting &  CelebA (32x32) \\ CelebA (32x32) \\ CIFAR-10 (3x32x32) \\  } \\     & &  & Likelihood training of SB &  MNIST (32x32), \\ CelebA (3x32x32), \\ CIFAR-10 (3x32x32) \\  } \\     & & & & & \\  

Table 1: Table of existing continuous (neural) solvers for EOT/SB.

**Theorem 3.2** (Entropic optimal transport benchmark constructor).: _Let \(_{0}()\) be a given distribution and \(f^{*}_{p}()\) be a given potential. Assume that \(c:\) is lower bounded and \((x,)_{}c(x,y)d(y)\) is lower semi-continuous in \(_{p}()\). Furthermore, assume that there exists \(M_{+}\) such that for all \(x\) it holds that \(M_{x}}}{{=}}_{}- dy M\). Assume that for all \(x\) value \(Z_{x}}}{{=}}_{} (y)-c(x,y)}{}dy\) is finite. Consider the joint distribution \(^{*}()\) whose first marginal distribution satisfies \(^{*}_{0}=_{0}\) and for all \(x\) it holds that_

\[(y|x)}{dy}=}(y)-c(x,y)}{ }\] (14)

_and \(^{*}(|x)\!\!_{p}()\). Then if \(_{1}}}{{=}}^{*}_{1}\) belongs to \(_{p}()\), the distribution \(^{*}\) is an **EOT plan** for \(_{0},_{1}\) and cost \(C_{c,}\). Moreover, if \(_{}C_{c,}x,^{*}(|x)d_{0 }(x)\!<\!\), then \(^{*}\) is the **unique EOT plan**._

Our result above requires some technical assumptions on \(c\) and \(f^{*}\) but a reader should not worry as they are easy to satisfy in popular cases such as the quadratic cost \(c(x,y)=\|x-y\|^{2}\) (SS3.3). The important thing is that our result allows **sampling**\(y^{*}(|x)\) from the conditional EOT plan by using MCMC methods [5, SS11.2] since (14) provides the **unnormalized density** of \(^{*}(y|x)\). Such sampling may be time-consuming, which is why we provide a clever approach to avoid MCMC below.

### Fast Sampling With LogSumExp Quadratic Potentials.

In what follows, we propose a way to overcome the challenging sampling problem by considering the case \(c(x,y)=}{2}\) and the special family of functions \(f^{*}\). For brevity, for a matrix \(A^{D D}\) and \(b^{D}\), we introduce \((y|b,A)\!}}{{=}}\!- (y-b)^{T}A(y-b)\). Henceforth, we choose the potential \(f^{*}\) to be a weighted log-sum-exp (LSE) of \(N\) quadratic functions:

\[f^{*}(y)\!}}{{=}}\!_{n=1}^{ N}w_{n}(y|b_{n},^{-1}A_{n})\] (15)

Here \(w_{n} 0\) and we put \(A_{n}\) to be a symmetric matrix with eigenvalues in range \((-1,+)\). We say that such potentials \(f^{*}\) are **appropriate**. One may also check that \(f^{*}_{2}()\) as it is just the LSE smoothing of quadratic functions. Importantly, for this potential \(f\) and the quadratic cost, \(^{*}(|x)\) is a Gaussian mixture, from which one can efficiently sample **without using MCMC methods**.

**Proposition 3.3** (Entropic OT solution for LSE potentials).: _Let \(f^{*}\) be a given appropriate LSE potential (15) and let \(_{0}\!\!_{2}()\!\!( )\). Consider the plan \(d^{*}(x,y)=d^{*}(y|x)d_{0}(x)\), where_

\[(y|x)}{dy} =_{n=1}^{N}_{n}\,(y|_{n}(x),_{n}) { with }_{n}}}{{=}}(A_{n}+I)^{-1} {, }_{n}(x)}}{{=}}(A_{n}+I)^{-1}(A_{n}b_{n}+x),\] \[_{n}}}{{=}} _{n}/_{n=1}^{N}_{n},_{n}}}{{=}}w_{n}(2)^{})}( x|b_{n},I-}_{n}).\]

_Then it holds that \(_{1}}}{{=}}^{*}_{1}\) belongs to \(_{2}()\) and the joint distribution \(^{*}\) is the **unique EOT plan** between \(_{0}\) and \(_{1}\) for cost \(C_{c,}\) with \(c(x,y)=\|x-y\|^{2}\)._

We emphasize that although each conditional distribution \(^{*}(|x)\) is a Gaussian mixture, in general, this **does not** mean that \(^{*}\) or \(_{1}=^{*}_{1}\) is a Gaussian mixture, even when \(_{0}\) is Gaussian. This aspect does not matter for our construction, and we mention it only for the completeness of the exposition.

### Schrodinger Bridge Benchmark Idea

Since there is a link between EOT and SB, our approach allows us to immediately obtain a solution to the Schrodinger Bridge between \(_{0}\) and \(_{1}\) (constructed with an LSE potential \(f^{*}\)).

**Corollary 3.4** (Solution for SB between \(_{0}\) and constructed \(_{1}\)).: _In the context of Theorem 3.2, let \(p=2\) and consider \(c(x,y)=\|x-y\|^{2}\). Assume that \(_{0}_{2,ac}()()\) and both \(_{0}\) and \(_{1}\) (constructed with a given \(f^{*}\)) have finite entropy. Then it holds that \(^{*}(y)}}{{=}}(y) }{}\) is a Schrodinger potential providing the **optimal drift**\(v^{*}\) for SB via formula (12)._

Although the drift is given in the closed form, its computation may be challenging, especially in high dimensions. Fortunately, as well as for EOT, for the quadratic cost \(c(x,y)=}{2}\) and our LSE (15) potentials \(f^{*}\), we can derive the optimal drift explicitly.

**Corollary 3.5** (SB solution for LSE potentials).: _Let \(f^{*}\) be a given appropriate LSE potential (15) and consider a distribution \(_{0}_{2,}()\) with finite entropy. Let \(_{1}\) be the one constructed in Proposition 3.3. Then it holds that \(_{1}\) has finite entropy, belongs to \(_{2,}()\) and_

\[v^{*}(x,t)=_{x}_{n=1}^{N}w_{n}^{t})} (x|b_{n},I-(1-t)} _{n}^{t})\] (16)

_is the optimal drift for the SB between \(_{0}\) and \(_{1}\). Here \(A_{n}^{t}}}{{=}}(1-t)A_{n}\) and \(_{n}^{t}}}{{=}}(A_{n}^{t}+I)^ {-1}\)._

## 4 Constructing Benchmark Pairs for OT and SB: Implementation

Our benchmark is implemented using PyTorch framework and is publicly available at

https://github.com/ngushchin/EntropicOTBenchmark

It provides code to sample from our constructed continuous benchmark pairs \((_{0},_{1})\) for various \(\), see SS4.1 for details of these **pairs**. In SS4.2, we explain the **intended usage** of these pairs.

### Constructed Benchmark Pairs

We construct various pairs in dimensions \(D\) up to \(12288\) and \(\{0.1,1,10\}\). Our mixtures pairs simulate _noise\(\)data_ setup and images pairs simulate _data\(\)data_ case.

**Mixtures benchmark pairs.** We consider EOT with \(\{0.1,1,10\}\) in space \(^{D}\) with dimension \(D\{2,16,64,128\}\). We use a centered Gaussian as \(_{0}\) and we use LSE function (15) with \(N=5\) for constructing \(_{1}\) (Proposition 3.3). In this case, the constructed distribution \(_{1}\) has \(5\) modes (Fig. 2(a), 2(f)). Details of particular parameters (\(A_{n},b_{n},w_{n}\), etc.) are given in Appendix B.

**Images benchmark pairs.** We consider EOT with \(\{0.1,1,10\}\). As distribution \(_{0}\), we use the approximation of the distribution of \(64 64\) RGB images (\(D=12288\)) of CelebA faces dataset . Namely, we train a normalizing flow with Glow architecture . It is absolutely continuous by the construction and allows straightforward sampling from \(_{0}\). For constructing distribution \(_{1}\), we also use LSE function \(f^{*}\). We fix \(N=100\) random samples from \(_{0}\) for \(b_{n}\) and choose all \(A_{n} I\). Details of \(w_{n}\) are given in Appendix C. For these parameters, samples from \(_{1}\) look like noised samples from \(_{0}\) which are shifted to one of \(_{n}\) (Fig. 4).

By the construction of distribution \(_{1}\), obtaining the conditional EOT plan \(^{*}(y|x)\) between \((_{0},_{1})\) may be viewed as learning the _noising_ model. From the practical perspective, this looks less interesting than learning the _de-noising_ model \(^{*}(x|y)\). Due to this, working with images in SS5, we always test EOT solvers in \(_{1}_{0}\) direction, i.e., recovering \(^{*}(x|y)\) and generating clean samples \(x\) form noised \(y\). The reverse conditional OT plans \(^{*}(x|y)\) are not as tractable as \(^{*}(y|x)\). We overcome this issue with MCMC in the latent space of the normalizing flow (Appendix C).

### Intended Usage of the Benchmark Pairs

The EOT/SB solvers (SS2) provide an approximation of the conditional OT plan \((|x)^{*}(|x)\) from which one can sample (given \(x_{0}\)). In particular, SB solvers recover the approximation of the optimal drift \( v^{*}\); it is anyway used to produce samples \(y(|x)\) via solving SDE \(dX_{t}=(x,t)dt+dW_{t}\) starting from \(X_{0}=x\) at time \(t=0\). Therefore, **the main goal of our benchmark** is to provide a way to compare such approximations \(,\) with the ground truth. _Prior to our work, this was not possible_ due to the lack of non-trivial pairs \((_{0},_{1})\) with known \(^{*},v^{*}\).

   &  &  &  \\   & & **LSOT** & **SCONES** & **NOT** & **EgNOT** & **[ENOT]** & **[MLE-SB]** & **[DiffSB]** & **[**FB-SDE-A**] & **[**FB-SDE-J**] \\   & _{2}^{2}\)-UVP} &  &  &  &  &  &  &  &  \\  & & & & do not work for small \(\) & & & & & & \\  &  & due to numerical instability &  &  &  &  &  &  \\   &  &  &  &  &  &  &  &  \\   &  &  &  &  &  &  &  &  &  \\  &  &  &  &  &  &  &  &  &  &  \\   &  &  &  &  &  &  &  &  &  &  &  \\   &  &  &  &  &  &  &  &  &  &  &  &  \\  

Table 2: The summary of EOT/SB solvers’ quantitative performance in \(_{2}^{2}\)-UVP and \(_{2}^{2}\)-UVP metrics on our mixtures pairs. Detailed evaluation and coloring principles are given in Appendix B.

For each of the constructed pairs \((_{0},_{1})\), we provide the code to do 5 main things: **(a)** sample \(x_{0}\); **(b)** sample \(y^{*}(|x)\) for any given \(x\); **(c)** sample pairs \((x,y)^{*}\) from the EOT plan; **(d)** sample \(y_{1}\); **(e)** compute the optimal drift \(v^{*}(x,t)\). Function **(c)** is just a combination of **(a)** and **(b)**. For images pairs we implement the extra functionality **(f)** to sample \(x^{*}(|y)\) using MCMC. Sampling **(d)** is implemented via discarding \(x\) (not returning it to a user) in \((x,y)\) in **(c)**.

When **training** a neural EOT/SB solver, one should use random batches from **(a,d)**. Everything coming from **(b,c,e,f)** should be considered as **test** information and used only for evaluation purposes. Also, for each of the benchmark pairs (mixtures and images), we provide a hold-out **test** for evaluation.

## 5 Experiments: Testing EOT and SB Solvers on Our Benchmark Pairs

Now we train various existing EOT/SB solvers from Table 1 on our benchmark pairs \((_{0},_{1})\) to showcase how well they capture the ground truth EOT plan. For solvers' details, see Appendix D.

**Mixtures benchmark pairs**. For quantitative analysis, we propose the following metric:

\[_{2}^{2},^{*} }}{{=}}( _{1})}_{}_{2}^{2}(|x),^{*}(|x)d_{0}(x).\] (17)

For each \(x\) we compare **conditional** distributions \((|x)\) and \(^{*}(|x)\) with each other by using the Bures-Wasserstein metric , i.e., the Wasserstein-2 distance between Gaussian approximations of distributions. Then we average this metric w.r.t. \(x_{0}\). The final normalization \((_{1})\) is chosen so that the trivial baseline which maps the entire \(_{0}\) to the mean of \(_{1}\) provides \(100\%\) error. Metric (17) is a modification of the standard \(_{2}^{2}\)-UVP  for the conditional setting. For completeness, we also report the **standard**\(_{2}^{2}\)-UVP to check how well \(_{1}\) matches \(_{1}^{*}\).

**Disclaimer.** We found that most solvers' performance **significantly** depends on the selected hyper-parameters. We neither have deep knowledge of many solvers nor have the resources to tune them to achieve the best performance on our benchmark pairs. Thus, _we kindly invite the interested authors of solvers to improve the results for their solvers._ Meanwhile, we report the results of solvers with their default configs and/or with limited tuning. Nevertheless, we present a hyperparameter study in Appendix E to show that the chosen hyperparameters are a reasonable fit for the considered tasks. Our goal here is to find out and explain the issues of the methods which are due to their principle rather than non-optimal hyperparameter selection.

The detailed results are in Appendix B. Here we give their concise summary (Table 2) and give a qualitative example (Fig. 3) of solvers' performance on our mixtures pair with \((D,)=(16,1)\).

**EOT solvers.**[**LSOT**] and [**SCONES**] solvers work only for medium/large \(=1,10\). \(}\) learns only the barycentric projection [49, Def. 1] hence naturally experiences large errors and even collapses (Fig. 3b). \(}\) solver works better but recovers the plan with a large error. We think

Figure 3: Qualitative results of EOT/SB solvers on our mixtures benchmark pair with \((D,)=(16,1)\). The distributions are visualized using \(2\) PCA components of target distribution \(_{1}\). Additional examples of performance on pairs with other \(\{0.1,10\}\) are given in Appendix B.

this is due to using the Langevin dynamic [14, Alg. 2] during the inference which gets stuck in modes plus the imprecise target density approximation which we employed (Appendix D). \(\) solver also employs Langevin dynamic and possibly experiences the same issue (Fig. 2(e)). Interestingly, our evaluation shows that it provides a better metric in matching the target distribution \(_{1}\). \(\) was originally not designed for EOT because it is non-trivial to estimate entropy from samples. To fix this issue, we modify the authors' code for EOT by employing _conditional normalizing flow_ (CNF) as the generator. This allows us to estimate the entropy from samples and hence apply the solver to the EOT case. It scores good results despite the restrictiveness of the used architecture (Fig. 2(d)).

**SB solvers.** For \(\), the original authors' implementation uses Gaussian processes as parametric approximators instead of neural nets . Since other SB solvers \(,\)\(\) and \()\) use neural nets, after discussion with the authors of \(\), we decided to use neural nets in their solver as well. All SB solvers work reasonably well, but their performance drops as the \(\) increases. This is because it becomes more difficult to model the entire diffusion (with volatility \(\)); these solvers may require more discretization steps. Still, the case of large \(\) is not very interesting since, in this case, the EOT is almost equal to the trivial independent plan \(_{0}_{1}\).

**Images benchmark pairs.** There are only two solvers which have been tested by the authors in their papers in such a large-scale _data\(\)data_ setup (\(64 64\) RGB images), see Table 1. Namely, these are \(\) and \(.\) Unfortunately, we found that \(\) yields unstable training on our benchmark pairs, probably due to too small \(\) for it, see [14, SS5.1]. Therefore, we only report the results of \(\) solver. For completeness, we tried to run \(,\)\(\) solvers with their configs from _noise\(\)data_ generative modelling setups but they diverged. We also tried \(\) with convolutional CNF as the generator but it also did not converge. We leave adapting these solvers for high-dimensional _data\(\)data_ setups for future studies. Hence, here we test only \(\).

In Fig. 4, we qualitatively see that \(\) solver _only for small \(\)_ properly learns the EOT plan \(^{*}\) and sufficiently well restores images from the input noised inputs. As there is anyway the lack of baselines in the field of neural EOT/SB, we plan to release these \(\) checkpoints and expect them to become a **baseline for future works** in the field. Meanwhile, in Appendix C, we discuss possible metrics which we recommend to use to compare with these baselines.

## 6 Discussion

**Potential Impact.** Despite the considerable growth of the field of EOT/SB, there is still no standard way to test existing neural (continuous) solvers. In our work, we fill this gap. Namely, _we make a step towards bringing clarity and healthy competition to this research area by proposing the first-ever theoretically-grounded EOT/SB benchmark_. We hope that our constructed benchmark pairs will

Figure 4: Qualitative comparison of ground truth samples \(x^{*}(|y)\) with samples produced by \(\). With the increase of \(\), the diversity increases but the precision of image restoration drops.

become the standard playground for testing continuous EOT/SB solvers as part of the ongoing effort to advance computational OT/SB, in particular, in its application to generative modelling.

**Limitations (benchmark).** We employ LSE quadratic functions (15) as optimal Kantorovich potentials to construct benchmark pairs. It is unclear whether our benchmark sufficiently reflects the practical scenarios in which the EOT/SB solvers are used. Nevertheless, our methodology is generic and can be used to construct new benchmark pairs but may require MCMC to sample from them.

To show that the family of EOT plans which can be produced with LSE potentials is rich enough, we provide a heuristic recipe on how to construct benchmark pairs simulating given real-world datasets, see Appendix H. As we show there, the recipe works on several non-trivial single-cell datasets . Thus, we conjecture that LSE potentials may be sufficient to represent any complex distribution just like the well-celebrated Gaussian mixtures are capable of approximating any density . We leave this inspiring theoretical question open for future studies.

For completeness, we note that our images benchmark pairs use LSE potentials and do not require MCMC for sampling from marginals \(_{0},_{1}\), i.e., to get clean and noisy images, respectively. However, for computing the test conditional FID (Appendix C) of EOT/SB solvers, MCMC is needed to sample clean images \(x^{*}(|y)\) conditioned on noisy inputs \(y\). This may introduce extra sources of error.

**Limitations (evaluation).** We employ \(_{2}^{2}\)-UVP for the quantitative evaluation (SS5) as it is popular in OT field . However, it may not capture the full picture as it only compares the 1st and 2nd moments of distributions. We point to developing of novel evaluation metrics for neural OT/SB solvers as an important and helpful future research direction.

Following our disclaimer in SS5, we acknowledge one more time, that the hyper-parameters tuning of the solvers which we test on our proposed benchmark is not absolutely comprehensive. It is possible that we might have missed something and did not manage to achieve the best possible performance in each particular case. At the same time, our Appendix E shows the extensive empirical study of the key hyper-parameters and it seems that the metrics reported are reasonably close to the optimal ones.

## 7 Acknowledgements

This work was partially supported by the Skoltech NGP Program (Skoltech-MIT joint project).