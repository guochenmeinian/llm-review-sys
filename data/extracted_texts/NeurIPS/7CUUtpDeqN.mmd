# Part 1: Specifying \(Q_{MC}(M,X,Y)\)

Analytically deriving Partial Information Decomposition for affine systems of stable and convolution-closed distributions

 Chaitanya Goswami

Electrical & Computer Engineering,

Carnegie Mellon University,

Pittsburgh, PA-15213.

cgoswami@andrew.cmu.edu

&Amanda Merkley

Electrical & Computer Engineering,

Carnegie Mellon University,

Pittsburgh, PA-15213.

amerkley@andrew.cmu.edu

###### Abstract

Bivariate partial information decomposition (PID) has emerged as a promising tool for analyzing interactions in complex systems, particularly in neuroscience. PID achieves this by decomposing the information that two sources (e.g., different brain regions) have about a target (e.g., a stimulus) into unique, redundant, and synergistic terms. However, the computation of PID remains a challenging problem, often involving optimization over distributions. While several works have been proposed to compute PID terms _numerically_, there is a surprising dearth of work on computing PID terms _analytically_. The only known analytical PID result is for jointly Gaussian distributions. In this work, we present two theoretical advances that enable analytical calculation of the PID terms for numerous well-known distributions, including distributions relevant to neuroscience, such as Poisson, Cauchy, and binomial. Our first result generalizes the analytical Gaussian PID result to the much larger class of stable distributions. We also discover a theoretical link between PID and the emerging fields of data thinning and data fission. Our second result utilizes this link to derive analytical PID terms for two more classes of distributions: convolution-closed distributions and a sub-class of the exponential family. Furthermore, we provide an analytical upper bound for approximately computing PID for convolution-closed distributions, whose tightness we demonstrate in simulation.

## 1 Introduction

Bivariate partial information decomposition1 (PID) is an information-theoretic framework developed for answering a central inquiry in many neuroscientific and machine learning studies: _how do two sources, \(X\) and \(Y\), jointly process information about a target \(M\)?_ PID answers this question by quantifying the \(M\)-specific information contained in different interactions between \(X\) and \(Y\). Specifically, it decomposes the total information \(X\) and \(Y\) have about \(M\) into _four_ components: (i) the information about \(M\) contained uniquely in \(X\), (ii) the information about \(M\) contained uniquely in \(Y\), (iii) the redundant information about \(M\) contained in both \(X\) and \(Y\), and (iv) the synergistic information about \(M\) which arises from the interaction between \(X\) and \(Y\).

The PID terms offer novel insights for understanding interactions within complex systems, particularly in neuroscience. For instance, PID has been used to understand the firing patterns of grid cells [1; 2] and to study the flow of information in the visual cortex . The following list of works [1; 3; 4; 5; 6; 7; 8] demonstrate the application of PID in studying diverse neuroscientific questions.

Beyond neuroscience, PID has found applications in multimodal learning for interpreting model predictions [9; 10], fair machine learning for defining and quantifying bias [11; 12; 13; 14], and understanding financial markets .

However, the primary constraint hindering broader adoption of PID is the difficulty of computing and estimating the PID terms. BROJA-PID , a widely applied PID framework, requires solving a constrained minimization problem over a set of probability distributions (see Sec. 2). This minimization problem can pose a considerable challenge, particularly when the underlying distributions are continuous. As a result, several recent works have been dedicated to providing _numerical algorithms_ that solve the aforementioned minimization problem exactly or approximately [3; 9; 17; 18; 19; 20], with more emphasis on the case of discrete distributions.

Despite significant progress in _numerically_ calculating PID (typically through discrete approximations), very few works exist on _analytically2_ calculating PID. As of this writing, the only known analytical PID expressions exist for jointly Gaussian \(M\), \(X\), and \(Y\)[21; 22]. A fundamental property of the Gaussian system is that one of the unique information (UI) terms in its PID is guaranteed to be _zero_. This property greatly simplifies the computation of PID for Gaussian systems, as the rest of the PID terms can be easily derived by solving desirable linear equations specified in many PID frameworks (see Sec 2), bypassing the need for optimizing over distributions (see Sec. 3).

In this work, we show that numerous systems of random variables \(M\), \(X\), and \(Y\) expressing a particular "affine dependence structure" also exhibit the same fundamental property that at least one of the UI terms is zero. Consequently, we expand significantly on the existing Gaussian PID result by using this fundamental property to analytically compute PID for various systems of \(M\), \(X\), and \(Y\) employing well-known distributions such as Poisson, exponential, gamma, beta, negative binomial, multinomial, Cauchy, Levy-stable and more. The main contributions of this work are:

1. We extend the Gaussian PID result to a much larger class of distributions, known as the stable distribution family [23; 24] in Sec. 4. These results provide the first known analytical PID for fat-tailed distributions.
2. We highlight a theoretical link between PID calculation and the fields of data thinning  and data fission  in Sec. 5. We utilize this link to derive analytical PID terms for two more distribution families: convolution-closed distributions  and certain exponential family distributions .
3. For convolution-closed distributions, we further derive an analytical upper bound on the objective of the minimization used for computing BROJA-PID. We use this upper bound to approximately compute PID for systems of \(M\), \(X\), and \(Y\) having a non-affine dependence structure. We show the goodness of our approximation by a simulation study in Sec. 6.

## 2 Background

**Notation**: We denote the set of all natural numbers, real numbers, and positive real numbers as \(\), \(\), and \(^{+}\), respectively. Vectors are denoted by bold-faced font and an arrow, and matrices are denoted by bold-faced font. Define \(_{0}=\{0\}\). Let \(_{d}\) be the identity matrix of size \(d d\) and \([d]=\{1,,d\}\ \ d\). We denote \(}_{d}\) and \(}_{d}\) as \(d\)-dimensional vectors having all elements as \(1\) and \(0\), respectively. For brevity, the probability notations of the form \(P(A|B)\) and \(P(A)\) are always understood to be as \(P(A=a|B=b)\) and \(P(A=a)\), respectively. The general term 'distribution' is used to refer to both probability density function (p.d.f.) and probability mass function (p.m.f.) (whichever is appropriate depending upon the context). The symbol \(A\!\!\! B|C\) denotes that \(A\) and \(B\) are conditionally independent given \(C\), and similarly, \(A\!\!\! B\) implies \(A\) and \(B\) are independent. The \(L_{p}\)-norm is denoted by \(\|()\|_{p}\ \ p[0,)\), and \(|()|\) denotes the absolute function.

**PID Background**: Suppose \(M,X,Y\) are random variables with joint distribution \(P(M,X,Y)\). According to [16; 27], three desirable equalities for a bivariate PID are as follows:

\[I_{P}(M;[X,Y])=UI(M;X Y)+UI(M;Y X)+RI(M;X,Y)+ SI(M;X,Y),\] \[I_{P}(M;X)\!=\!UI(M;X Y)\!+\!RI(M;X,Y),\] \[I_{P}(M;Y)\!=\!UI(M;Y X)\!+\!RI(M;X,Y).\] (1)Here, \(I_{P}(M;X)\) and \(I_{P}(M;Y)\) denote the mutual information  between \(M\) and \(X\), and \(M\) and \(Y\), respectively, under the distribution \(P(M,X,Y)\). Similarly, \(I_{P}(M;[X,Y])\) denotes the joint mutual information between \(M\) and \([X,Y]\) under the distribution \(P(M,X,Y)\). \(UI(M;X|Y)\) and \(UI(M;Y X)\) are the unique information about \(M\) in \(X\) and \(Y\), respectively. \(SI(M;X,Y)\) and \(RI(M;X,Y)\) denote the respective synergistic and redundant information shared between \(X\) and \(Y\) about \(M\). The linear system defined in (1) contains four variables in three equations. Hence, only one of \(UI,RI\), or \(SI\) need be defined to evaluate all four PID terms. Proposing a suitable definition of PID is the focus of much research  and all our theoretical results are applicable for any Blackwellian PID  or a PID definition satisfying the assumption (\(*\)) of Bertschinger et al.  (see Appx. B for a formal justification). For example, our results are applicable for the PID definitions proposed in . In this work, we discuss our results in the context of the BROJA-PID , which is the definition of PID adopted in previous works :

\[UI(M;X Y)=_{Q_{P}}I_{Q}(M;X|Y)UI(M;Y  X)=_{Q_{P}}I_{Q}(M;Y|X),\] (2)

where \(_{P}\!=\!\{Q(M,X,Y)\!:\!Q(M,X)\!=\!P(M,X),Q(M,Y)=\!P(M,Y)\}\) and \(I_{Q}(M;X|Y)\) is the conditional mutual information under the distribution \(Q(M,X,Y)\). Note that the minimizing distributions for both problems shown in (2) are the same , and consequently, both minimization problems are equivalent. We refer the reader to  for a review on PID. In this work, we define "analytically calculating PID" as analytically solving (2) by providing an explicit construction of the minimizing distribution \(Q^{*}(M,X,Y)\) that minimizes (2). Note that only the distributions \(Q^{*}(M,X,Y)\) and \(P(M,X,Y)\) are needed to compute the BROJA-PID terms.

We briefly describe two distribution families used in Sec. 4 and Sec. 5 (see Appx. N for more details).

**Stable distribution family**: Stable distributions are a family of distributions that naturally arise in the context of generalized central limit theorems. Some well-known members of this family are the Gaussian, Cauchy, Poisson, and Levy distributions. A defining feature of stable distributions is that the sum of two independent copies of a random variable \(X\), denoted as \(X_{1}\) and \(X_{2}\), has the same distribution as a translated and scaled version of \(X\). In this work, we consider five sub-classes of stable distributions:

(i) _Continuous stable distributions_ are parameterized through four parameters: stability parameter \((0,2]\), skewness parameter \([-1,1]\), scale parameter \((0,)\), and location parameter \(\). We denote its p.d.f. as \(p_{CS}(,,,)\). Note that all continuous stable distribution (except Gaussian) are fat-tailed.

(ii) _Independent component multivariate stable distributions_ describe the distribution of \(}\) consisting of \(d\) independent random variables \(\{X_{j}\}_{j=1}^{d}\), such that each \(X_{j} p_{CS}(,_{j},_{j},_{j})\). We denote its p.d.f. as \(p_{CS-IC}(,},},})\).

(iii) _Elliptically-contoured multivariate stable distributions_ are the distributions of continuous stable random vectors whose p.d.f. has elliptical contours, e.g., the multivariate Gaussian distribution. We denote its p.d.f. as \(p_{CS-EC}(,,})\). Here, \(\) is a positive definite matrix, \(}^{d}\), and \((0,2]\).

(iv) _Discrete stable distributions_ are the discrete analogues of the continuous stable distributions. The p.m.f. of discrete stable distributions, denoted as \(P_{DS}(,)\), are parameterized through two parameters: rate parameter \(>0\) and exponent \(0< 1\). Discrete stable distributions do not have well-known multivariate generalizations except the Poisson distribution.

(v) _Multivariate Poisson distribution_: We use the multivariate Poisson distribution proposed in , denoted as Poisson\((d,d^{},})\), which represents each random variable in the random vector \(}\) as a sum of independent Poisson random variables. Formally, \(}(d,d^{},})\) if \(}=}^{g}\), where \(=[_{1}_{d^{}}]\). Here, \(_{i}\) denotes a \(d\) submatrix having no duplicate columns, where each of its columns contain exactly \(i\) ones and \((d-i)\) zeros . The vector

\[}^{g}=[N_{1}^{g} N_{d}^{g}\ N_{12}^{g}\ N_{d-(d^{ }-1) d}^{g}]^{T}\]

with mutually independent \(N_{i_{1} i_{j}}^{g}(_{i_{1} i_{j}})\ \ (i_{1},,i_{j})_{j}^{d},j[d^{}]\), where \(_{i}^{d}=\{(j_{1},,j_{i})\!:\!j_{1}<j_{2}<<j_{i},j_{1},,j_{i}[d]\}\), e.g., \(_{2}^{3}=\{(1,2),(1,3),(2,3)\}\). Note that \(d^{} d\), and \(}=_{1}\ \ _{d-(d^{}-1) d}^{T}\).

We refer the reader to [23; 24; 40; 42; 44; 45; 46; 47] for more details on stable distributions.

**Convolution-closed distributions**: Convolution-closed distributions form a large class of distributions that are closed under convolution in some parameter \(\). Formally, we define the convolution-closed distribution as follows: Let \(_{}\) denote a family of distributions, where each member distribution \(f()_{}\) is indexed by a parameter \(\). Consider \(X_{1} f(_{1})\), \(X_{2} f(_{2})\), and \(X_{1}\!\!\! X_{2}\) for some \(_{1},_{2}\). Then, \(_{}\) is convolution-closed in the parameter \(\) if

\[X_{1}+X_{2} f(_{1})*f(_{2})=f(_{1}+_{2})\; \;_{1},_{2}_{1}+_{2} ,\] (3)

where \(*\) denotes the convolution operator. Many well-known distributions can be considered convolution-closed in some parameter , such as the Poisson distribution, Gaussian distribution, gamma distribution, etc. Table 1 in  lists various examples of convolution-closed distributions.

## 3 A sufficient condition for computing PID terms analytically

The main focus of this work is to analyze the cases where (2) is analytically solvable. A sufficient condition for solving (2) is to show that \(_{P}\) contains a distribution \(Q_{MC}(M,X,Y)\) with the Markovian structure \(M X Y\) or \(M Y X\). We briefly discuss the argument justifying this sufficient condition. Consider the case where \(Q_{MC}(M,X,Y)_{P}\) and has the Markovian structure \(M X Y\). First, we note that \(UI(M;Y X)=_{Q_{P}}I_{Q}(M;Y|X) 0\) due to the non-negativity of conditional mutual information . Second, \(Y\!\!\! M|X\) due to the Markovian structure of \(Q_{MC}(M,X,Y)\), which implies that \(I_{Q_{MC}}(M;Y|X)=0\) (conditionally independent random variables have zero conditional mutual information ). Hence, \(Q_{MC}(M,X,Y)\) achieves the lower bound of zero for the minimization problem \(_{Q_{P}}I_{Q}(M;Y|X)\), showing that \(Q_{MC}(M,X,Y)\) indeed minimizes (2). A similar argument can be made for the case when \(Q_{MC}(M,X,Y)\) has the Markovian structure \(M Y X\). Proposition 1 in Appx. A formalizes the above argument.

The above sufficient condition provides an easy way to analytically calculate the PID terms, as it ensures one of the unique information terms is always zero. Consequently, the remaining PID terms can be calculated by substituting zero for the appropriate unique information term in (1) and solving the resultant linear system. Hence, if applicable, the above sufficient condition considerably simplifies the calculation of PID terms by circumventing the need for optimizing over a set of distributions. Surprisingly, many well-known distribution families allow intuitive constructions of \(P(M,X,Y)\), for which the above sufficient condition is applicable. In the following sections, we provide theorems that specify sufficient conditions under which the existence of these Markov chains in \(_{P}\) can be guaranteed for these \(P(M,X,Y)\).

## 4 Computing PID for stable distributions

In this section, we extend existing results for analytical PID computation of jointly Gaussian \(M\), \(X\), and \(Y\)[21; 22] to the much larger class of stable distributions. Our results utilize two key observations to provide the generalization of the Gaussian results: (i) We first identify that the analytical computation of PID for jointly Gaussian systems is due to their particular "affine dependence" structure; (ii) We show that these particular affine dependence structures are not unique to Gaussian systems, but rather extend to many members of the stable distribution family.

For jointly Gaussian \(M\), \(X\), and \(Y\), the conditional distributions \(P(X|M){=}(aM+b,_{X}^{2})\) and \(P(Y|M){=}(cM+d,_{Y}^{2})\) are also Gaussian distributions, where their means are an affine function of \(M\) and their variances are fixed with respect to \(M\). This particular affine dependence of \(X\) and \(Y\) on \(M\) is the key to the analytical calculation of their PID, as it guarantees existence of a Markov chain \(Q_{MC}(M,X,Y)_{P}\). Thus, we can apply the sufficient condition described in Sec. 3 to compute the PID terms. We illustrate through an example: consider \(P(X|M){=}(M,_{X}^{2})\) and \(P(Y|M){=}(M,_{Y}^{2})\) where \(_{X}^{2}<_{Y}^{2}\), and \(M P(M)\) for some appropriate \(P(M)\). Then, we can explicitly construct \(Q_{MC}(M,X,Y)_{P}\) with the Markovian structure \(M X Y\) as follows: Choose \(Q_{MC}(M,X,Y){=}P(M)P(X|M)Q_{MC}(Y|X)\) where \(Q_{MC}(Y|X)\) is specified through the addition of independent Gaussian noise, i.e., \(Y{=}X+\). Here, \((0,_{Y}^{2}-_{X}^{2})\) and \(\!\!\!(M,X)\). It is easy to verify that \(Q_{MC}(Y|M){=}(M,_{Y}^{2})\), which implies that \(Q_{MC}(M,Y){=}P(M,Y)\). By construction, \(Q_{MC}(M,X){=}P(M,X)\), and hence, \(Q_{MC}(M,X,Y)_{P}\).

The above example is a special case of a well-known result where, for a scalar \(M\), one can always construct a lower signal-to-noise ratio (SNR) "Gaussian channel", i.e. \(P(Y|M)\) in our example, by adding independent Gaussian noise to a higher SNR "Gaussian channel" [22; 48], i.e. \(P(X|M)\) in our example. Surprisingly, the technique of adding independent noise to construct Markov chains contained in \(_{P}\) can be extended to \(P(X|M)\) and \(P(Y|M)\) as members of stable distributions. Theorems 1 and 4 generalize the above construction for \(P(X|M)\) and \(P(Y|M)\) as members of univariate continuous and univariate discrete stable distributions, respectively. Theorems 2, 3, and 5 consider the case of multivariate stable distributions (see Sec. 2). The key technique for proving these theorems is that a Markov chain \(Q_{MC}(M,X,Y)_{P}\) can always be constructed by adding appropriate independent noise to a higher SNR \(P(X|M)\) to obtain a lower SNR \(P(Y|M)\), similar to our above example.

### PID of univariate affine continuous stable system

Theorem 1 can be viewed as a direct generalization of Barrett's Gaussian PID result  to stable distributions, showing one of the UI terms is always zero. We begin by formally describing the univariate affine continuous stable system that generalizes the Gaussian system of Barrett's . Let the joint distribution of \(M,X,\) and \(Y\), denoted as \(P(M,X,Y)\), satisfy the following properties: (i) \(M P(M)\) with support set \(\); (ii) \(P(X|M)\) and \(P(Y|M)\) are univariate continuous stable distributions with an affine dependence on \(M\), i.e., \(P(X|M)\)\(=p_{CS}(,_{X},_{X},aM+b)\) and \(P(Y|M)\)\(=p_{CS}(,_{Y}(ac),_{Y},cM+d)\), where \(a,b,c,d\).

**Theorem 1**.: _Let \(M,X,\) and \(Y\) be random variables whose joint distribution \(P(M,X,Y)\) is described by a univariate affine continuous stable system. Without the loss of generality, assume \(}{{_{X}}}}{{_{Y}}}\). If \(1-_{Y}|c|/_{Y}|a|)}^{}} {(1-_{X})}\) and \(1+_{Y}|c|/_{Y}|a|)}^{}} {(1+_{X})}\), then \(_{P}\) contains a Markov chain of the form \(M X Y\) and \(UI(M;Y X)=0\)._

Proof.: See Appx. F for the proof. Here, \(}{{_{X}}}\) and \(}{{_{Y}}}\) are the SNR analogues. 

### PID of multivariate affine continuous stable systems

We analyze two multivariate generalizations of Theorem 1 in Theorems 2 and 3. Namely, we consider independent component multivariate stable distribution (where all the components of the random vector are independent) and elliptically-contoured multivariate stable distribution (where the p.d.f. is elliptically contoured, similar to multivariate Gaussian distributions). We construct two systems employing these two sub-classes of multivariate continuous stable distributions and show that one of the UI terms is always zero for these systems. For both cases, denote the joint distribution of \(M\), \(}\), and \(}\) as \(P(M,},})\), where the support set of \(M\) is \(\). The dimensions of \(M\), \(}\), and \(}\) are \(1\), \(d_{X}\), and \(d_{Y}\), respectively.

System 1: The random vectors \(}\) and \(}\) satisfy the following equations:

\[}=}_{X}M+_{X}}_{X}+ }_{X}}=}_{Y}M+_{Y}}_{Y}+ _{Y}},\] (4)

where \(}_{X} p_{CS-IC}(,}_{d_{X}},}_{d_{X}},}_{d_{X}})\), \(}_{Y} p_{CS-IC}(,}_{d_{Y}},}_{d_{Y}},}_{d_{Y}})\), \(_{X}\) and \(_{Y}\) are invertible matrices, \(}_{X},}_{X}^{d_{X}}\), and \(}_{Y},}_{Y}^{d_{Y}}\).

**Theorem 2**.: _Let System 1 describe the joint distribution \(P(M,},})\) of \(M\), \(}\), and \(}\). Without the loss of generality, assume \(\|_{Y}^{-1}}_{Y}\|_{}\|_{X}^{-1 }}_{X}\|_{}\), where \(=}{{-1}}\ \ (1,2]\) and \(=\ \ (0,1]\). Then, \(_{P}\) contains a Markov chain of the form \(M}}\) and \(UI(M;}})=0\)._

Proof.: See Appx. G for the proof. Here, \(\|_{Y}^{-1}}_{Y}\|_{}\) and \(\|_{X}^{-1}}_{X}\|_{}\) are the SNR analogues. 

System 2: The conditional distribution of \(}\) and \(}\) conditioned on \(M\) are as follows: \(P(}|M)=p_{CS-EC}(,_{X},}_{X} M+}_{X})\) and \(P(}|M)=p_{CS-EC}(,_{Y},}_{Y} M+}_{Y})\), where \(_{X}\) and \(_{Y}\) are positive definite matrices, \(}_{X},}_{X}^{d_{X}}\), and \(}_{Y},}_{Y}^{d_{Y}}\).

**Theorem 3**.: _Let System 2 describe the joint distribution \(P(M,},})\) of \(M\), \(}\), and \(}\). Define \(_{X}^{-}{{2}}}\) and \(_{Y}^{-}{{2}}}\) as the respective inverses of the matrices \(_{X}^{}{{2}}}\) and \(_{Y}^{}{{2}}}\) which satisfy: \((_{X}^{}{{2}}})^{T}_{X}^{}{{2}}}=_{X}\), and \((_{Y}^{}{{2}}})^{T}_{Y}^{}{{2}}}=_{Y}\). Without the loss of generality, assume \(\|_{Y}^{-}{{2}}}}_{Y}\|_{2}\| _{X}^{-}{{2}}}}_{X}\|_{2}\). Then, \(_{P}\) contains a Markov chain of the form \(M}}\) and \(UI(M;}})=0\)._

Proof.: See Appx. H for the proof. Here, \(\|_{Y}^{}{{2}}}}_{Y}\|_{2}\) and \(\|_{X}^{}{{2}}}}_{X}\|_{2}\) are the SNR analogues. 

### PID of univariate affine discrete stable system

The univariate affine discrete stable system is the discrete counterpart of the univariate affine continuous stable system described in Sec. 4.1. The formal description of the univariate affine discrete stable system is as follows. Let the joint distribution of \(M,X,\) and \(Y\), denoted as \(P(M,X,Y)\), satisfy the following properties: (i) \(M P(M)\) with support set \((0,)\); (ii) \(P(X|M)\) and \(P(Y|M)\) are univariate discrete stable distributions with an affine dependence on \(M\), i.e., \(P(X|M\!=\!m)\!=\!P_{DS}(,am+b)\) and \(P(Y|M\!=\!m)\!=\!P_{DS}(,cm+d)\), where \(a,b,c,d(0,)\).

**Theorem 4**.: _Let \(M,X,\) and \(Y\) be random variables whose joint distribution \(P(M,X,Y)\) is described by a univariate affine discrete stable system. Without the loss of generality, assume \(a c\). If \(a^{}{{b}}}}{{d}}\), then \(_{P}\) contains a Markov chain of the form \(M X Y\) and \(UI(M;Y X)=0\)._

Proof.: See Appx. I for the proof. Here, \(a\) and \(c\) are the SNR analogues. 

### PID of multivariate linear Poisson system

The Poisson distribution is the only the discrete stable distribution with a well-known multivariate extension. Hence, we analyze vector-generalizations of Theorem 4 only for the Poisson distribution. We now describe the multivariate linear Poisson system: Let the joint distribution \(P(M,},})\) of \(M,},\) and \(}\) satisfy the following properties: (i) \(M P(M)\) with support set \((0,)\); (ii) \(P(}|M)\!=\!(d_{X},d_{X}^{},}_ {X})\) and \(P(}|M)\!=\!(d_{Y},d_{Y}^{},}_ {Y})\), with:

\[}_{X}\!=\![_{1}^{X}\ \ _{d_{X}-(d_{X}^{ }-1) d_{X}}^{X}]^{T},_{i_{1} i_{j}}^{X}=_ {i_{1} i_{j}}^{X}M^{j}\ \ j[d_{X}]\ \ (i_{1},,i_{j}) _{j}^{d_{X}},\] \[}_{Y}=[_{1}^{Y}\ \ _{d_{Y}-(d_{Y}^{ }-1) d_{Y}}^{Y}]^{T},_{i_{1} i_{j}}^{Y}=_ {i_{1} i_{j}}^{Y}M^{j}\ \ j[d_{Y}]\ \ (i_{1},,i_{j}) _{j}^{d_{Y}}.\] (5)

**Theorem 5**.: _Let the joint distribution \(P(M,},})\) of \(M\), \(}\), and \(}\) be described by the multivariate linear Poisson system defined above. Without the loss of generality, assume \(d_{X}^{} d_{Y}^{}\). If \(_{(i_{1},,i_{j}) A_{j}^{d_{X}}}_{i_{1} i_{j}}^{X} _{(i_{1},,i_{j}) A_{j}^{d_{Y}}}_{i_{1} i_{j}}^{Y}\ \ j[d_{Y}^{}]\), then \(_{P}\) contains a Markov chain of the form \(M}}\) and \(UI(M;}})=0\)._

Proof.: See Appx. J for the proof. Here, \(_{1}^{X},,_{d_{X}-(d_{X}^{}-1) d_{X}}^{X}, _{1}^{Y},,_{d_{Y}-(d_{Y}^{}-1) d_{Y}}^{Y}^{+}\) are the SNR analogues. 

## 5 Computing PID using data thinning and data fission strategies

Data thinning and data fission are emerging fields in machine learning and statistics dedicated to studying the procedures of splitting a random variable \(X\) into \(N\) different component random variables. These splitting procedures provide an attractive alternative to the standard splitting of datasets into training, validation, and test splits for performing cross-validation to select statistical model parameters, as they enable users to perform cross-validation even for the extreme case of a dataset containing a _single_ datapoint. We refer the reader to the recent works of Neufeld et al.  and Leiner et al.  for a more comprehensive discussion on data thinning and fission, respectively.

The fields of data thinning and data fission share an important theoretical link with analytically calculating PID terms. Specifically, the tools developed for splitting random variables for data thinning and data fission can be readily used to calculate PID terms analytically. To give an intuition, suppose \(X\) contains more information about \(M\) than \(Y\). Then, we can employ data fission and thinning strategies to decompose \(X\) into two components, \(f_{1}(X)\) and \(f_{2}(X)\), such that \(f_{2}(X)\) follows the same distribution as \(Y\). Then, \(f_{2}(X)\) and \(Y\) convey the same information about \(M\) as theyare identically distributed and represent the redundant component of the PID terms. Similarly, \(f_{1}(X)\) contains the information uniquely contained in \(X\) about \(M\). Thus, \(f_{1}(X)\) represents the unique information term. Theorems 6 and 7 utilize the data thinning and data fission proposed in [25; 26] to construct a Markov chain \(Q_{MC}(M,X,Y)_{P}\) for several systems of random variables \(M\), \(X\), and \(Y\). This allows us to use the sufficient condition discussed in Sec. 3 to compute their PID.

### PID for convolution-closed distribution based on data thinning strategies of Neufeld et al.

Neufeld et al.  introduces data thinning for a large family of distributions known as convolution-closed distributions (see Sec. 2). An attractive property of convolution-closed distributions is that they provide a natural way to define a dilation/thinning operation. Formally, let \(X f()\), then we define \(X_{}\) as the \(\)-dilated version of \(X\) if \(X_{} f()\) for some \((0,1)\) such that \(\). Furthermore, if we assume \((1-)\), then \(P(X_{}|X)\!=\!P(X_{}|X_{}+X_{1-})\), where \(X_{1-} f((1-))\) and \(X_{}\!\!\! X_{1-}\) (see lemma 19 for a formal justification). We denote \(P(X_{}|X=x)=P(X_{}|X_{}+X_{1-}=x)\) as \(G(,(1-),x)\). This dilation operation forms the basis of data thinning, as it enables \(X\) to be split into its dilated components \(X_{}\) and \(X_{1-}\), such that \(X=X_{}+X_{1-}\), where \(X_{}\!\!\! X_{1-}\). We utilize this dilation operation for analytically calculating the PID terms of the following linear convolution-closed system:

**Linear convolution-closed system**: Let \(_{}\) be a convolution-closed distribution family as described in Sec. 2. The joint distribution \(P(M,X,Y)\) of the random variables \(M\), \(X\), and \(Y\) describes a _linear convolution-closed system_ if the distributions \(P(X|M)\) and \(P(Y|M)\) are defined as follows:

\[P(X|M\!=\!m)=f(_{m}^{X})P(Y|M\!=\!m)=f(_{m}^{Y}) _{m}^{X},_{m}^{Y}\ \ m,\] (6)

where \(\) is the support of \(M\). Furthermore, we assume \(_{m}^{X}=_{m}^{Y}\ \ m\) for some \(^{+}\).

**Theorem 6**.: _Let the joint density \(P(M,X,Y)\) of random variables \(M\), \(X\), and \(Y\) be described by a linear convolution-closed system. Without the loss of generality, assume \( 1\). If_

1. \((1-)_{m}^{Y}\ \ m\)_,_
2. \(P(X_{}|X_{}+X_{1-}\!=\!x,M\!=\!m)=G(_{m}^{X},(1- )_{m}^{X},x)\) _does not depend on_ \(m\)_, where_ \(P(X_{}|M)=f(_{m}^{X})\)_,_ \(P(X_{1-}|M)=f((1-)_{m}^{X})\) _and_ \(X_{}\!\!\! X_{1-}|M\)_,_

_then \(_{P}\) contains a Markov chain of the form \(M X Y\) and \(UI(M;Y X)=0\)._

The proof of Theorem 6 is provided in Appx. K. Theorem 6 enables PID calculation for several well-known distributions such as gamma, Poisson, beta etc. (see Appx. C for a (non-exhaustive) list).

### PID for distributions based on data fission strategies of Leiner et al.

Leiner et al.  propose a "conjugate-prior reversal" strategy for splitting a random variable \(X\) into two components, \(f(X)\) and \(g(X)\), for certain exponential family distributions. The distributions proposed for performing conjugate-prior reversal provide natural descriptions of \(P(M,X,Y)\) (specified in theorem 7) for which PID can be calculated analytically. We briefly describe the distributions used in the conjugate-prior reversal strategy. Let \(X p_{exp1}(X)\), where

\[p_{exp1}(X=x;_{1},_{2})=H(_{1},_{2})(_{1}^{T}x -_{2}^{T}A(x)),\] (7)

for some appropriately defined \(H(,),A(),_{1}\) and \(_{2}\). Furthermore, define a random variable \(Y\) through its conditional density \(p(Y|X=x)\):

\[p(Y=y|X=x;_{3})=h(y)(x^{T}T(y)-_{3}^{T}A(x)),\] (8)

for some \(h()\), \(T()\), and \(_{3}\), such that \(p(Y=y|X=x;_{1},_{2})\) is a well-defined distribution. Then, the decomposition terms \(f(X)\) and \(g(X)\) are \(Y\) and \(X\), respectively, in the conjugate-prior reversal strategy. Furthermore, the marginal distribution of \(Y\) is expressed as:

\[p_{exp2}(Y=y;_{1},_{2},_{3})=h(y)H(_{1},_{2})/H( _{1}+T(y),_{2}+_{3})]}).}\]

**Theorem 7**.: _Let \(M,X,\) and \(Y\) be random variables having the joint distribution \(P(M,X,Y)\). Furthermore, the conditional distribution of \(X\) and \(Y\) conditioned on \(M\) are as follows: \(P(X|M\!=\!m)\!=\!p_{exp1}(X;_{1}(m),_{2}(m))\) and \(P(Y|M\!=\!m)\!=\!p_{exp2}(Y;_{1}(m),_{2}(m),_{3})\). Then, \(_{P}\) contains a Markov chain of the form \(M X Y\) and \(UI(M;Y X)=0\)._The proof of Theorem 7 can be found in Appx. L. The proof essentially stems from observing that the joint distribution \(Q_{MC}(M,X,Y)\!=\!P(M)P(X|M)Q_{MC}(Y|X)\) lies in \(_{P}\), where \(Q_{MC}(Y|X)\!=\!h(y)x^{T}T(y)-_{3}^{T}A(x)\). A (non-exhaustive) list of well-known distributions for which theorem 7 is applicable is provided in Appx. D. Leiner et al.  also discuss some more strategies for performing data fission that do not follow the conjugate-prior reversal strategy. We provide the corresponding results for computing PID for these remaining data fission strategies and for additional miscellaneous distributions in Appx. E.

## 6 Upper bound for convolution-closed distributions

Several numerical methods, such as , approximately solve (2) by considering a smaller constraint set or minimizing an appropriate upper bound for calculating the PID terms. These numerical methods employ general approximations that rely on weak assumptions on the underlying distributions for solving (2), as their goal is to estimate PID for a large class of distributions. However, for specific applications, it is possible to make stronger assumptions on the underlying distributions (e.g., assuming the Poisson distribution for modeling neural spikes). This section illustrates how our theoretical analysis can benefit these numerical algorithms by providing more refined approximations for solving (2) that harness these stronger assumptions. Specifically, we construct an upper bound for the objective of (2) for convolution-closed distributions and show that, under certain assumptions, the upper bound can be _analytically minimized_ over \(_{P}\) in Sec. 6.1. Note that these upper bounds are applicable for more general cases than our theoretical results, as they do not require the sources \(X\) and \(Y\) to have an affine dependence on \(M\). Consequently, our upper bound is also applicable in cases where both of the UI terms in the PID are non-zero, unlike our results in Sec. 4 and 5 We demonstrate the tightness of our upper bound through a simulation study in Sec. 6.2.

**Notations and assumptions**: We consider the minimization problem \(_{Q_{P}}I_{Q}(M;[X,Y])\) for deriving the upper bound, as it was shown to be equivalent to (2) in . The distribution \(P(M,X,Y)\) for which we will construct our upper bound is specified as follows. The random variable \(M\) has support over \(\), the conditional distributions \(P(X|M)\) and \(P(Y|M)\) are members of some convolution-closed distribution family \(_{}\), and there exists some \(_{bias}^{X},_{bias}^{Y}\) such that

\[P(X|M\!=\!m)\!=\!f(_{m}^{X})P(Y|M\!=\!m)\!=\!f (_{m}^{Y})_{m}^{X},_{m}^{Y}\;\;m ,\] \[(_{m}^{Y}-_{bias}^{Y}),(_{m}^{X}-_{bias}^ {X}),(_{m}^{X}-_{m}^{Y}-(_{bias}^{X}-_{bias}^{Y})) _{m}^{X}-_{bias}^{X}\!=\! _{m}^{(1)}(_{m}^{Y}\!-\!_{bias}^{Y}),\] \[_{m}^{Y}=_{m}^{(2)}(_{m}^{Y}-_{bias}^ {Y}),_{m}^{X}=_{m}^{(3)}(_{m}^{X}-_{bias}^{X})_{m}^{(1)},_{m}^{(2)},_{m}^{(3)}\;\;m .\] (9)

The above assumptions ensure that \(X\) and \(Y\) can always be decomposed into new random variables \(X^{},Y^{},n_{X},Y^{}\), and \(n_{Y}\) (see (10)). We utilize these decomposed random variables to construct our upper bound. Hence, our upper bound is only applicable for systems satisfying (9). The above assumptions describe a large class of systems. For example, in the Poisson case, our upper bound is applicable for any \(P(M,X,Y)\) having \(P(X|M)=(f_{1}(M))\) and \(P(Y|M)=(f_{2}(M))\), as long as \(f_{1}(M) f_{2}(M)\) over \(\). In Appx. C, we provide numerous examples of systems for which (9) holds to illustrate the assumptions of (9).

Figure 1: **a** and **b**, respectively, show the box plot of the difference \(I_{Q_{A}}(M,X,Y)-I_{Q_{N}}(M,X,Y)\) and the corresponding values of \(I_{Q_{N}}(M,X,Y)\) for the \(20\) different function pairs across the \(75\) different \(P(M)\) distributions. The light-blue dots show the corresponding data points used for making the box plots. **c** shows the ratio of the median difference \(I_{Q_{A}}(M,X,Y)-I_{Q_{N}}(M,X,Y)\) and the median value of \(I_{Q_{N}}(M,X,Y)\) in percentage, for each function pair.

### Upper bound construction

First, we consider an arbitrary distribution \(Q(M,X,Y)_{P}\). Therefore, we know \(Q(X|M\!=\!m)\!=\!P(X|M\!=\!m)\!=\!f(_{m}^{X})\) and \(Q(Y|M\!=\!m)\!=\!P(Y|M\!=\!m)\!=\!f(_{m}^{Y})\). We use the dilation properties of convolution-closed distributions (Sec. 5.1) to decompose \(X\) and \(Y\) into their respective dilated versions: \((X^{},Y^{},n_{X})\) and \((Y^{},n_{Y})\). From the results of Appx. M.1, we know that \(X=X^{}+Y^{}+n_{X}\) and \(Y=Y^{}+n_{Y}\). Furthermore, \((X^{},Y^{},n_{X})\) are mutually conditionally independent given \(M\) and \(Y^{} n_{Y}|M\). Hence, we can construct the following Markov chain for any arbitrary \(Q(M,X,Y)_{P}\):

\[M[X^{} Y^{} Y^{} n_{X} n _{Y}]^{T}[X^{}+Y^{}+n_{X} Y^{}+n _{Y}]^{T}=[X Y]^{T}\,.\] (10)

We denote the joint distribution of \((M,X^{},Y^{},Y^{},n_{X},n_{Y})\) as \((M,X^{},Y^{},Y^{},n_{X},n_{Y})\). We appropriately choose the dilation amounts for \(X\) and \(Y\) such that the respective conditional distributions of \((X^{},Y^{},Y^{},n_{X},n_{Y})\) are as follows:

\[(X^{}|M\!=\!m)\!=\!f(_{m}^{X}\!-\!_{m}^{Y }-(_{bias}^{X}\!-\!_{bias}^{Y})),\ \ (Y^{}|M\!=\!m)=f(_{m}^{Y}-_{bias}^{Y}),\] \[(Y^{}|M\!=\!m)\!=\!f(_{m}^{Y}-_{bias}^{ Y}),\ \ (n_{X}|M)=f(_{bias}^{X}),\ \ (n_{Y}|M)=f(_{bias}^{Y}).\] (11)

The distributions in (11) are well-defined due to (9). Appx. M.1 formally shows that a \(\), defined as above, exists for each \(Q_{P}\). We use data-processing inequality and (10) to conclude:

\[I_{}(M;[X^{},Y^{},n_{X},Y^{},n_{Y} ]) I_{Q}(M;[X,Y])\ \ Q(M,X,Y)_{P}.\] (12)

Hence, (12) provides us the desired upper bound for our objective \(I_{Q}(M;[X,Y])\). Furthermore, the minimization problem \(_{Q_{P}}I_{}(M;[X^{},Y^{},n_{X},Y^{ },n_{Y}])\) is analytically solvable, and the minimizing distribution \(^{*}\) has the structure:

\[Y^{}=Y^{}\ \ (n_{X},n_{Y})\!\!(M,X^{ },Y^{}).\]

The corresponding distribution of \((M,X,Y)\), denoted as \(Q_{A}(M,X,Y)\), can be found by appropriately manipulating \(^{*}\), as \(X=X^{}+Y^{}+n_{X}\) and \(Y=Y^{}+n_{Y}\). The distribution \(Q_{A}\) serves as an approximate solution for the problem \(_{Q_{P}}I_{Q}(M;[X,Y])\) and, consequently, (2). Note that if multiple \(_{bias}^{X}\) and \(_{bias}^{Y}\) exist satisfying (9), we optimize over the pairs \((_{bias}^{X},_{bias}^{Y})\) to further refine our approximate solution of (2).

### Simulation study for numerically validating the upper bound

We illustrate the tightness of our upper bound through a simulation study on the Poisson distribution (a convolution-closed distribution). The reason for choosing the Poisson distribution is two-fold: (i) Poisson distribution is easily approximated as a discrete distribution over finite support, enabling calculation of ground-truth PID terms through numerical solvers such as [10; 17; 18], which are not readily available for continuous distributions; (ii) Many practical applications of PID have been in neuroscience, and the Poisson distribution is frequently used for modeling neural spikes in neuroscientific studies .

We compare the performance of our analytical estimate \(Q_{A}(M,X,Y)\) with the numerical ground-truth estimate \(Q_{N}(M,X,Y)\) for the Poisson distribution. The simulation setup is as follows: We choose a \(P(M,X,Y)\) such that \(P(X|M)=(f_{1}(M))\) and \(P(Y|M)=(f_{2}(M))\). We chose \(20\) different pairs of non-linear \(f_{1}()\) and \(f_{2}()\) (enumerated in Table 1 in Appx. M), such that the assumptions in (9) are satisfied. The distribution \(P(M)\) is chosen to be a discrete distribution. For each function pair, we compare \(Q_{A}(M,X,Y)\) and \(Q_{N}(M,X,Y)\) over \(75\) different distributions of \(P(M)\) as shown in Fig. 1a. The \(75\) distributions are further sub-divided into three groups of \(25\) based on the number of the outcomes of \(M\), which are \(2\), \(4\), and \(8\). For each of the \(75\) distributions, the values of \(M\) are randomly sampled from \(\) and the values of \(P(M)\) are randomly sampled from a simplex of appropriate dimensions. We compare the mutual information \(I_{Q_{A}}(M;[X,Y])\) calculated using \(Q_{A}\) with the numerical ground-truth \(I_{Q_{N}}(M;[X,Y])\) calculated using numerical solvers (Fig. 1b). For calculating \(I_{Q_{N}}(M;[X,Y])\), we approximate the Poisson distribution as a finite discrete distribution by terminating its support at the smallest integer where its cumulative distribution is greater than \(0.99\). We use the code provided in  (under Apache 2.0-license) to numerically solve (2) using this discrete approximation of the Poisson distribution. Fig. 1c demonstrates that the analytical upper bound provided by \(Q_{A}\) is very tight for the tested function pairs (within \(<1\%\) ofthe numerical ground-truth for \(16\) function pairs) and serves as a good approximation for solving (2). Furthermore, the tightness of our analytical upper bound suggests that \(Q_{A}\) might be an analytical solution of (2) for a larger class of systems of \(M\), \(X\), and \(\) having non-affine dependence on \(M\). Similar results demonstrating the tightness of upper-bound for negative-binomial and binomial distributions are presented in Appx. M.4.

## 7 Discussion and Limitations

In this work, we analytically compute PID for large classes of distributions, greatly expanding upon the analytical result for the Gaussian system. We provide the first known analytical PID for systems employing Poisson, gamma, exponential, Cauchy, beta, Dirichlet, Levy-stable, binomial, multinomial, negative binomial, and uniform distributions. Furthermore, we generalize the previous Gaussian PID result  in an additional way by showing the target \(M\) need not be Gaussian. Our stable distribution results provide the first known analytical computation of PID for fat-tailed distributions (all continuous stable distributions have infinite variance except the Gaussian distribution). A practical utility of our analytical results is that they provide a large test bed in which the performance of numerical PID estimators can be compared and evaluated. This test bed may benefit future works on numerical PID estimation by enabling more comprehensive testing of PID estimators.

Our results on the Poisson, Cauchy, and binomial are of particular relevance in the neuroscientific context. Poisson and Cauchy distributions are widely used to model neural spikes  and network dynamics in the brain . Binomial thinning is a frequently-used operator in neuroscience . Our generalization of the Gaussian result could be helpful in refining approximations already used in computing PID for neural data , e.g., by relaxing the assumption of joint Gaussianity. As continuous stable distributions have been shown to better model Magnetic Resonance Imaging data [52; 53], our results may also be helpful in computing PID in this application.

We also connect the fields of data thinning and data fission with PID by using their decomposition strategies for analytically computing the PID of systems based on convolution-closed distributions (see Sec. 5). Conversely, our PID results also suggest decomposition strategies for data thinning/fission purposes, e.g., our stable distribution results suggest that stable random variables can be decomposed by adding independent noise (similar to the Gaussian case discussed in ). Convolution-closed distributions are particularly promising for studying PID as they allow intuitive construction of upper bounds that can be analytically minimized (see Sec. 6). These upper bounds can complement the work on numerical estimation of PID by providing more refined approximations. Another promising avenue is to combine our upper bounds with lower bounds (e.g., from ) to create branch and bound algorithms  for solving (2). Overall, our analytical results greatly facilitate the computation of PID, either by directly using the analytical expressions or by providing refined approximations for numerical methods.

**Limitations and Future Work**: We study PID for univariate \(M\), \(X\), and \(Y\) and provide some vector extensions. More vector extensions of our results are a promising direction for subsequent works. Most of our analytical results require \(P(X|M)\) and \(P(Y|M)\) to depend on some affine functions of \(M\). The existence of analytical solutions for the cases where \(P(X|M)\) and \(P(Y|M)\) depend on non-affine functions of \(M\) remains an open question. The upper bound discussed in Sec. 6 can be further refined with more careful analysis, and more rigorous testing of these upper bounds is required to understand in which cases the upper bound is a good approximation for solving (2). We defer the testing and refinement of these upper bounds for subsequent works, as our primary goal in this work is to study analytical solutions of PID. Niu & Quinn  also propose a duality result between the synergistic and redundant components in the Gaussian broadcast and multiple-access channels utilizing the analytical PID expressions of the Gaussian system. It may be possible to derive similar duality results for appropriately defined broadcast and multiple-access channels employing other distributions (e.g., Poisson or Cauchy) with our theoretical results.

**Broader Impact**: Due to their theoretical nature, our results' negative impact primarily depends on how they are used and interpreted. Our theoretical results are applicable only under the specific assumptions outlined in this work, and using these results without ensuring that the theorem's assumptions are satisfied can lead to incorrect scientific conclusions. We also caution reader against naively using PID to draw causal inferences, as PID is inherently a correlational quantity.