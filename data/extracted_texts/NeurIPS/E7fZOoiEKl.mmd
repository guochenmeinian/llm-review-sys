# FuseFL: One-Shot Federated Learning through the Lens of Causality with Progressive Model Fusion

Zhenheng Tang\({}^{}\) Yonggang Zhang\({}^{}\) Peijie Dong\({}^{}\) Yiu-ming Cheung\({}^{}\)

**Amelie Chi Zhou\({}^{}\) Bo Han\({}^{}\) Xiaowen Chu\({}^{,}\)**

\({}^{}\) Department of Computer Science, Hong Kong Baptist University

\({}^{}\) DSA Thrust, The Hong Kong University of Science and Technology (Guangzhou)

{zhtang, ygzhang, ymc, amelieczhou, bhannl}@comp.hkbu.edu.hk

{pdong212, xwchu}@connect.hkust-gz.edu.cn

This work is partially done during the visiting in The Hong Kong University of Science and Technology (Guangzhou).

###### Abstract

One-shot Federated Learning (OFL) significantly reduces communication costs in FL by aggregating trained models only once. However, the performance of advanced OFL methods is far behind the normal FL. In this work, we provide a causal view to find that this performance drop of OFL methods comes from the isolation problem, which means that locally isolatedly trained models in OFL may easily fit to spurious correlations due to data heterogeneity. From the causal perspective, we observe that the spurious fitting can be alleviated by augmenting intermediate features from other clients. Built upon our observation, we propose a novel learning approach to endow OFL with superb performance and low communication and storage costs, termed as FuseFL. Specifically, FuseFL decomposes neural networks into several blocks and progressively trains and fuses each block following a bottom-up manner for feature augmentation, introducing no additional communication costs. Comprehensive experiments demonstrate that FuseFL outperforms existing OFL and ensemble FL by a significant margin. We conduct comprehensive experiments to show that FuseFL supports high scalability of clients, heterogeneous model training, and low memory costs. Our work is the first attempt using causality to analyze and alleviate data heterogeneity of OFL2.

## 1 Introduction

Federated learning (FL) [96; 67] has become a popular paradigm that enables collaborative model training without sharing private datasets from clients. Two typical characteristics of FL limit its performance: (1) FL normally has non-IID (Independently and Identically Distributed) data, also called _data heterogeneity_, which causes unstable slow convergence [68; 146; 124; 135] and poor model performance [157; 94; 134; 162; 150]; (2) The extremely low bandwidth, e.g. \(1 10\) MB/s of FL in Internet environments [67; 133; 129; 128; 132; 129], leads to high communication time of a large neural network. For example, communicating once ResNet-50  with 25.56M parameters (102.24MB) or GPT-3  with 175B parameters (700GB) will consume around 102.24 seconds or 194 hours, respectively. Current FL methods alleviate this problem by skipping the gradient synchronization of traditional distributed training to save communication costs [96; 67]. But the required hundreds or thousands of communication rounds still make the communication time unacceptable.

To reduce the communication costs at extreme, one-shot FL (OFL) [159; 38; 81; 164; 25; 23] only communicates the local trained model once. Thus, the communication cost is the model size \(S\) for each client, less than FedAvg-style algorithms for hundreds or thousands of times. However, averaging for only once cannot guarantee the convergence of FedAvg. Thus, the direct idea is to aggregate client models on the server and conduct inference as ensemble learning does. Some advanced works also consider better model averaging [63; 112; 90; 6], neurons matching [5; 141], selective ensemble learning [26; 52; 143], model distillation [81; 164; 25; 23]. These methods may be impractical due to the requirements of additional datasets with privacy concerns, and the extra large storage or computation costs. Most importantly, there still exists a large performance gap between OFL and the normal FL or the ensemble learning. This motivates the following question:

_How to improve FL performance under extremely low communication costs with almost no extra computational and storage costs?_

In this work, we provide a _causal view_[109; 110; 3; 119] to analyze the performance drop of OFL. We firstly construct a causal graph to model the data generation process in FL, where the spurious features build up the data heterogeneity between clients, and invariant features of the same class remain constant in each client (domain) [3; 119; 22; 151; 161]. Then, we show the performance drop comes from the _isolation problem_, which means that locally isolatedly trained models in OFL may easily fit to spurious correlations like adversarial shortcuts [40; 35; 53], instead of learning invariant features [3; 119], causing a performance drop of OFL on the test dataset. Consider a real-world example, Alice takes photos of birds in the forests, while Bob near the sea. Now, the isolated models will mistakenly identify birds according to the forests or the sea [53; 35]. Based on the causal graph, we intuitively and empirically show that such spurious fitting can be alleviated by augmenting intermediate features from other clients (Section 3).

Built upon this observation, we propose a simple yet effective learning approach to realizing OFL with superb performance and extremely low communication and storage costs, termed as FuseFL, which builds up the global model through bottom-up training and fusion to improve OFL performance (Section 4). Specifically, we split the whole model into multiple blocks 3 (The "block" means a single or some continuous layers in a DNN.). For each block, clients first train and share their local blocks with others; then, these trained local blocks are assembled together, and the features outputted from these blocks are fused together and fed into the next local blocks. This process is repeated until the whole model is trained. Through this bottom-up training-and-fusion method, local models can learn better feature extractors that learn more invariant features from other clients to avoid the isolation problem. To avoid the large storage cost, given the number of clients \(M\), we assign each local client with a small model with reduced hidden dimension with ratio \(\), to ensure the final learned global model has the same size \(S\) as the original model.

Our main contributions can be summarized as follows:

* We provide a causal view to understand the gap between multi-round FL and OFL, showing that augmenting intermediate features from other clients contributes helps improve OFL. As far as we know, this is the first work using causality to analyze the data heterogeneity of OFL.
* To leverage causality to improve OFL, we design FuseFL, which decomposes models into several modules and transmits one module for feature augmentation at each communication round.
* We conduct comprehensive experiments to show how FuseFL significantly promotes the performance of OFL without _no_ additional communication and computation cost.

## 2 Preliminary

### Federated Learning

In FL, a set of clients \(=\{m|m 1,2,...,M\}\) have their own dataset \(_{m}\). Given \(C\) classes indexed by \([C]\), a sample in \(_{m}\) is denoted by \((x,y)[C]\), where \(x\) is the input in the space \(\) and \(y\) is its corresponding label. These clients cooperatively learn a model \(F(,x):^{}\) that is parameterized as \(^{d}\). Formally, the global optimization problem can be formulated as :

\[_{}L()_{m=1}^{M}p_{m}L_{m}()=_{m=1}^{M}p_ {m}_{(x,y)_{m}}(F;x,y),\]

where the local objective function of \(m\)th-client \((F;x,y) CE(,y)\) with \( F(;x)\), \(CE\) denotes the cross-entropy loss, \(p_{m}>0\) and \(_{m=1}^{M}p_{m}=1\). Usually, \(p_{m} n_{m}/N\), where \(n_{m}\) denotes the number of samples on client \(m\) (\(n_{m}=|_{m}|\)) and \(N=_{m=1}^{M}n_{m}\).

The classic FL algorithm is FedAvg . In each communication round \(t\), the central server randomly samples a part of clients \(^{t}\) and broadcasts the model \(^{t}\) to all selected clients, and then each \(m\)-th client performs multiple local updates. After local training, all selected clients send the optimized \(^{t}_{m,E}\) to the server, and the server aggregates and averages local models to obtain a global model. Such a multi-round communication introduces large communication costs .

### Ensembled FL

The FedAvg requires multiple communication rounds \(T\) for convergence [146; 83], which might be extremely large . Given the model size \(S\), FedAvg-style FL methods introduce communication costs as \(T S\). As shown in Table 1, the current lowest communication cost of FL is reduced as \(S\) in OFL, making FL possibly deployable in low communication bandwidth scenarios [133; 129]. Thus, we analyze what causes the performance drop of OFL and how to improve it. As the performance of average-based and model distillation OFL methods is upper bounded by ensemble learning [159; 38; 81; 164; 23], we mainly focus on analyzing ensemble learning and differentiating FuseFL from it. The output of the ensemble learning can be formalized as: \(F_{}(x)_{m}F_{m}^{}(_{m};x)\), in which the local model \(F_{m}^{}\) parameterized with \(_{m}\) is isolatedly trained with minimizing empirical risk minimization (ERM) objective [152; 3; 22] function \((F(,x),y),(x,y)_{m}\) by SGD.

## 3 Federated Learning: A Causal View

### The Sequential Structure of Neural Networks

A neural network can be decomposed into the sequential module-wise structure as shown in Figure 1. Formally, it can be defined as:

\[F= H^{K} H^{K-1} H^{1},\ 1 k K,\] (1)

where \(\) is the final classifier, and \(H_{k}\) is the module that may consist of single or multiple blocks. The \(\) and each \(H_{k}\) are parameterized by \(^{}^{d_{}}\) and \(^{k}^{d_{k}}\). The \(H^{i} H^{j}()\) means \(H^{i}(H^{j}())\). Thus, the parameter \(\) of \(F\) are concatenated by the \(^{}\) and \(\{^{k}|k 1,2, K\}\), and \(d_{}+_{k=1}^{K}d_{k}=d\).

As Figure 1 illustrates, each module \(H^{k}\) receives the output of module \(H^{k-1}\), and the final classifier receives the output of the final hidden module \(H^{K}\) and makes predictions \(=f(x)\) on the input \(x\). We call the output from each module, \(h^{k}=H^{k}(h^{k-1})\) and \(h^{1}=H^{1}(x)\), as the feature for simplicity.

### Structure Equation Model of FL

Inspired from the analysis of out-of-distribution (OOD) generalization  through the lens of mutual information [122; 2] and structure equation

Figure 1: Structure Equation Model  of FL.

model (SEM) in causality [110; 3; 19; 161], we define the data generation SEM of FL as shown in Figure 1. For local training dataset \(_{m}\) at client \(m\), the SEM is \(Y_{m} R^{}_{m} X_{m} R^{}_{m}\), where \(R^{}_{m}\) and \(R^{}_{m}\) are invariant and spurious features, \(Y_{m}\) and \(X_{m}\) are label and input data respectively. Here, the dataset \(_{m}\) is a subset of the whole dataset \(\). The \(R^{}_{m}\) is actually the nuisance at a global level (respect to \(Y\)), being independent of \(Y\), but dependent on \(X_{m}\).

**Non-IID data and causality.** For a groundtruth label \(Y\), its corresponding invariant features \(R^{}_{m}\) do not change across clients [3; 119; 22; 151]. However, the spurious features \(R^{}_{m}\) are other factors that occasionally exist in data and do not have a relationship to \(Y\), which means that the heterogeneous features of data (non-iid) with the same class come from the spurious features \(R^{}_{m}\) (concept shift ). For example, in photos of birds in the forests or the sea, pixels of birds are \(R^{}_{m}\) while the forests and the sea are \(R^{}_{m}\). Considering the test dataset includes all client data distribution and even OOD data, the \(Y_{}\) is largely dependent on \(R^{}_{}\): \(P(Y_{}|X_{},R^{}_{}) P(Y_{ {test}}|X_{},R^{}_{})\)4.

**Non-IID scenarios.** This SEM model considers the label shift (\(p_{i}(y) p_{j}(y)\)) and concept shift (\(p_{i}(x|y) p_{j}(x|y)\)) scenarios [67; 80], or both of them appear simultaneously. When the support5\(_{m}\) of \(Y_{m}\) is different or partly overlapped between clients \(m=1,...,M\), this would be the severe non-IID scenario . And it is obvious that spurious features \(R^{}_{m}\) relate to the concept shift.

**Spurious fitting.** By conducting isolated local training on local dataset \(_{m}\) at client \(m\), the model \(F^{}_{m}\) is prone to learn to predict \(Y_{m}\) based on spurious features \(R^{}_{m}\), i.e. low distance \(d^{}_{,m}=d(P(Y_{m}|X_{m},R^{}_{m}),P(F^{}_{m}|X_{m},R^{}_{m}))\) but high distance \(d(P(Y_{m}|X_{m},R^{}_{m}),P(F^{}_{m}|X_{m},R^{}_{ m}))\), in which the distance \(d\) could be \(CE\) loss or \(KL\) divergence. The reason for the spurious fitting by isolated training is that the invariant features \(R^{}_{i m}\) from other clients are not observed by client \(m\), while the \(R^{}_{m}\) frequently appears in the local dataset \(_{m}\) like the adversarial attacks or shortcuts [40; 35; 53]. This guarantees low error on the training dataset \(_{m}\), because it has much less data than \(_{}\) and \(_{1,...,M}\), thus introducing high probability \(P(Y_{m}|X_{m},R^{}_{m})\). However, on test dataset \(_{}\), the low \(d^{}_{,m}\) of model \(F^{}_{m}\) but high \(d^{}_{,m}\) of model \(F^{}_{m}\) leads to high test error. Different from isolated training, FedAvg alleviates this problem by multiple times of averaging models to find those common features, including more \(R^{}_{1,...,M}\) and removing \(R^{}_{1,...,M}\).

**Feature augmentation.** Through the above analysis, the key to improve OFL performance is to endow OFL with the ability of training to see invariant features across all clients. It has been found that training on noised datasets with SGD to optimize ERM can still result in some feature representations consisting of both spurious and invariant features; exploiting the invariant features is the key to helping improve OOD performance [158; 3; 8]. In light of this, we introduce augmenting features by fusing client models block by block. Concisely speaking, in FuseFL, each local model can conduct local training with the view from other clients \(H_{i m}(X_{m})\), which helps filter out \(R^{}_{m}\) but retain \(R^{}_{m}\), as other clients cannot see \(R^{}_{m}\) in their dataset \(_{i m}\). This method can be seen as a kind of invariant feature augmentation . The details of FuseFL are shown in Section 4.

### Mutual Information

The goal of FL is to obtain a model that performs well on all client datasets . Thus, here we consider the random variable \(X,Y\) sampled from the global dataset \(\). In this section, we also write \(H^{k}\) as the features that output from \(H^{k}(H^{k-1}(H^{1}(X)))\) for simplicity.

Given the probabilistic graph model \((R^{},Y) X H^{1} H^{k} F(X)\) (Eq. 1), where \(R^{}\) are ignored for simplicity, the MI between \(Y\) and subsequent transformations \(H^{k}\) on \(X\) satisfies a decreasing trend: \(I(X;Y) I(H^{1};Y) I(H^{K};Y)\); the MI between \(X\) and subsequent transformations on \(X\) satisfies a decreasing trend: \(Entropy(X) I(H^{1};X) I(H^{K};X)\). If \(I(H^{K};R^{})=0\) and \(H^{K}\) can predict labels, \(H^{K}\) is called invariant features so that the final classifier will not overfit to spurious correlations between \(R^{}\) and \(Y\). The previous works  show that achieving the following minimal sufficient statistic provides good generalization:

**Sufficient statistic**: \[I(X;Y)=I(H(X);Y),\] (2)
**Minimal statistic**: \[H(X)=_{(X)}I((X);X).\] (3)

**Lemma 3.1** (Invariance and minimality ).: _Given spurious feature \(R^{}\) for the label \(Y\), and probabilistic graph model \((R^{},Y) X H(X)\), then,_

\[I(H(X);R^{}) I(H(X);X)-I(X;Y).\]

_There is a nuisance \(R^{}\) such that equality holds up to a residual \(\):_

\[I(H(X);R^{})=I(H(X);X)-I(X;Y)-,\]

_where \( I(H(X);Y|R^{})-I(X;Y)\). The sufficient statistic \(H(X)\) (satisfying Eq. 3) is invariant to \(R^{}\) if and only if it is minimal (satisfying Eq. 2)._

_Remark 3.1_.: Based on Lemma 3.1, we can study how \(I(H(X);X)\) and \(I((H);Y)\) changes to study to what degree the \(H(X)\) contains spurious features.

**Empirical study.** We empirically estimate the MI \(I(H^{k}_{},X)\) and \(I(H^{k}_{},Y)\) of isolated local trained features, and the \(I(H^{k}_{},X)\) and \(I(H^{k}_{},Y)\) of augmented features. As the deep neural networks (DNNs) show layer-wise feature enhancements , we also measure the linear separability [7; 101] of features \(H^{k}_{}\) and \(H^{k}_{}\) to see how they change. Details of MI estimation and linear separability are shown in Appendix D.3 and D.4. The experiments are conducted by training ResNet-18 with CIFAR-10  partitioned across \(M=5\) clients. Figure 2 shows the local features \(H^{k}_{}\) have significantly higher \(I(H^{k},X)\) but lower \(I(H^{k},Y)\) than augmented features \(H^{k}_{}\). With the increased non-IID degree (lower \(a\)), the \(I(H^{k},Y)\) decreased further, demonstrating that the local feature \(H^{k}_{}\) fits on a more anti-causal relationship between \(R^{}_{}\) and \(Y_{m}\). The fused high-level features show better linear separability. And \(H^{k}_{}\) is more robust to \(R^{}_{m}\).

Except for the natural spurious features that exist in CIFAR-10, we also study the effect of spurious features by handcraft. Specifically, we inject backdoored data samples [10; 97] of \(1\) out of \(5\) clients as \(^{}_{1}\), in which the images have handcrafted textures generated according to the labels as a strong anti-causal relation. Details of backdoored datasets are introduced in Appendix D.5. Figure 3 shows that the backdoor features lead to information loss in \(X\). And the backdoored data samples further aggravate the information loss of label \(Y\). The isolated local trained features retain significantly less \(I(H^{k},Y)\) than augmented features.

Figure 3: Estimated MI and separability of trained models with non-IID backdoored datasets.

Figure 2: Estimated MI and separability of trained models with non-IID datasets.

## 4 FuseFL: Progressive FL Model Fusion

Motivated by analysis in Section 3, we propose augmenting local intermediate features \(H_{m}^{1,,K}\) on client \(m\), which helps reduce fitting to a spurious correlation between \(Y_{m}\) and \(R_{m}^{}\). However, direct fusing features together to make predictions faces the following problems.

**Altered feature distribution.** During local training, the local subsequent model \(_{m}^{k+1}_{m} H_{m}^{K} H_{m}^{k+1}\) after block \(k\) on client \(m\) is trained based on local features \(H_{m}^{k}\). After feature fusion, changed local features lead to feature drifts [75; 134].

**Mismatched semantics.** Each local feature \(H_{m}^{k}\) has totally different distributions, scales, or even dimensions; thus, directly averaging may cause useful features to be overwhelmed or confused by noisy features.

### Train, Fuse, Freeze, and Re-Train

As Figure 4 and Algorithm 1 (Appendix 1) show, the main loop of FuseFL including training, fusing, and freezing, which is repeated for all \(K\) split blocks following a progressive manner. Note that for \(H_{m}^{1}\), there is no layer fusion, which is isolatedly trained.

**Fuse.** After each local training step, clients share their \(H_{m}^{k}\) with other clients, and fuse them following Eq. 4. An adaptor \(A\) is stitched before \(H_{m}^{k+1}\) (Section 4.2). Then the local model becomes as \(F_{}^{k,m}\) following Eq. 5.

**Freeze and re-train.** To address the altered feature distribution problem (\(A(H_{}^{k}(x)) H_{m}^{k}(x)\)), for each step \(k\) on client \(m\), the subsequent layers \(_{m}^{k+1}\) will be trained again based on \(_{m}\). Thus, the \(_{m}^{k+1}\) can learn from all low-level features of all clients. Note that we do not need to train each block \(k\) with the same epochs in isolated training, because the DNNs naturally follow the layer-wise convergence [113; 48].

Figure 4: (a) Initially, all layers are isolated training. Note that the layer here does not only mean one or Conv layer, but generally refers to a neural network block that can consist of multiple layers. (b) Then, all first blocks (**L1**) of different clients are communicated, shared and frozen among clients. Then, the adaptors are added behind the fused block, to fuse features outputted from the concatenated local blocks. (c) Train the third blocks (**L3**) follow the similar process in (b). (d) inference process of FuseFL. The larger squares represent the original training block in local models. The smaller squares are adaptors that fuse features from previous modules together, which are \(1 1\) Conv kernels or simple average operations with little or no memory costs. Note that (a) also represents local training in ensemble FL, where different clients train models on local datasets.

\[H_{}^{k}(x) =[H_{1}^{k}(x),...,H_{M}^{k}(x)].\] (4) \[F_{}^{k,m} =_{m} H_{m}^{K}... H_{m}^{k+1} H_{ {fus}}^{k}... H_{}^{1}.\] (5) \[F_{} = H_{}^{K} H_{}^{K-1}...  H_{}^{1}.\] (6) \[A_{} =(H_{}^{k}(x)).\] (7) \[A_{} =((H_{}^{k}(x))).\] (8)

By freezing fused blocks and retraining high-level models, the another benefit is to enforce the SGD to use previous features from other clients to continue tuning the high-level model. The previous local trained high-level models may overfit on shortcut features from the noisy data. This insight is also utilized in defensing adversarial attacks [139; 107].

### Feature Adaptation

To address the mismatched semantics problem, the intuitive approach is to preserve the original feature structures through the concatenation of all features to the next block. However, this leads to new problems: (1) requiring modification of subsequent modules; (2) feature size explosion of subsequent blocks by \(O(M^{K})\). To address these two problems, we introduce an adaptor stitched before local modules (\(k>1\)), and training together with \(_{m}^{k}\). As an initial trial to operationalize \(\), we utilize conv1\(\)1 as the adapter as Eq. 8. We also verify the use of average as an adapter (Eq. 7) in experiments (Section 6).

### Benefits of \(\) Design

**Mitigating fitting on spurious correlations.** During the local training on datasets with spurious features, the final learned representations with ERM still contain some invariant features [3; 22]. Thus, some work proposes to finetune the classifier based on data samples with invariant features to let the classifier make predictions based on invariant features [70; 61; 106]. Similar to this motivation, we hope to incorporate other client modules as auxiliary feature extractors to generate more invariant features of local data during training subsequent layers. Figure 1 describes mechanism that using other local models \(H_{i m}(X_{m})\) help to filter out spurious features \(R_{m}^{}\), but retain \(F_{m}^{}\). As other clients \(\{i m\}\) cannot see \(R_{m}^{}\) in their dataset \(_{i m}\) during local training, only invariant features can pass through \(H_{i m}(X_{m})\). This method can be seen as the invariant feature augmentation .

**Saving storage and communication costs than ensemble FL.** Similar to ensemble learning, directly collecting and fusing local models together will enlarge the total model size from \(S\) to \(S M\). Note that \(\) actually builds up a global model with blocks fused together, with the hidden dimensions (channels) enlarged from \(n_{s} n_{s} M\). Thus, intuitively, we can reduce the hidden dimension of the local model \(n_{f}\) to reduce the memory requirements. Interestingly, with a scaling ratio \(\), when scaling all local linear or convolutional layers, each matrix should be scaled on _both input and output_ dimension as \(n_{f}= n_{s}\). The ratio of memory costs between \(\) and the original single model is \(r_{m}=M n_{s}^{2}/( n_{s})^{2}\). To obtain \(r_{m}=1\), we obtain the scaling ratio \(=\), which means that \(\) can keep similar memory requirements with the original model size \(S\) with reducing hidden dimensions as ratio \(\), demonstrating good theoretical scalability to the \(M\). We will verify this in experiments (Section 6.2).

**Privacy concerns.**\(\) only shares layers between clients, which aligns with other classic and advanced FL methods in all directions mentioned in Section 5.

**Support of heterogeneous models.** The block in \(\) does not mean a single linear or convolution layer, but a general module that can consist of any DNN, thus supporting FL with heterogeneous models (see experiments 6.2). The adaptor can be designed to transform features of different shapes to align with the input of the next local block.

**Layer-wise training to reduce training epochs.** Because each communication round means multiple local training epochs. To keep the total training epochs the same as the one-shot FedAVG (represented as \(E\)), we assign the local training epochs of the \(\) as \(E/K\). Thus, the number of total training epochs of \(\) is the same as other OFL methods. The core insight of this design can be referred to as the progressive freezing during training DNNs .

Related Works

### Data Heterogeneity in FL

The notorious non-IID data distribution in FL severely harms the convergence rate and model performance of FL. The **model regularization** proposes to add a penalty of distances between local and global models [117; 1]. **Feature calibration** aligns feature representations of different clients in similar spaces [29; 134; 60; 135]. FedMA  exploits a layer-wise communication and averaging methods, in which the aggregation is conducted on fine-grained layer. Thus, its linear dependence of computation and communication on the network's depth, is not suitable for deeper models, which introduces large computation costs in re-training and more communication rounds [19; 58]. Unlike FedMA, FuseFL introduces block-wise communication and aggregation with much less communication rounds and computation costs. Furthermore, due to the matching and averaging aggregation, FedMA only supports linear or Conv layers, which severely limits its practical usage. By viewing the separated block as a black box and concatenating output features, FuseFL can successfully support merging any kind of neural layer. Some works on fairness analysis in FL also relate to this work in perspective of local and global characteristics [42; 32].

### One-shot FL

One-shot FL [159; 38; 81; 164; 25] reduces communication costs from \(T S\) to \(S\) by communicating with only one round. **Average-based** methods focus on better averaging client models, like Fisher information [63; 112], bayesian optimization [90; 6] or matching neurons [5; 141]. However, the non-linear structure of DNNs makes it difficult to obtain a comparable global model through averaging. **Ensemble-based** methods make prediction based on all or selected client models [26; 52; 143], but requires additional datasets with privacy concerns. And they have low scalability of the number of clients due to the storage of client models. **Model distillation** uses the public  or synthesized datasets [164; 25] to distill a new model based on ensemble models [38; 81]. These methods may be impractical in data-sensitive scenarios, such as medical and education, or continuous learning scenarios [28; 27]. Furthermore, there exists a large performance gap between these methods and the ensemble learning.

Due to the limited space, we leave the detailed reviews in Table 8 and Appendix C. Table 1 concisely demystifies different FL methods in terms of communication cost, storage cost, model performance, and whether supporting model heterogeneity or requiring extra data.

## 6 Experiments

### Experiment Setup

**Federated Datasets and Models.** In order to validate the efficacy of FuseFL, we conduct comprehensive experiments with commonly used datasets in FL, including MNIST , CIFAR-10 , FMNIST , SVHN , CIFAR-100  and Tiny-Imagenet . For studying the non-IID problem in FL, we partitioned the datasets through a widely-used non-IID partition method, namely Latent Dirichlet Sampling [56; 67; 114; 80], in which the coefficient \(a\) represents the non-IID degree. Lower \(a\) generates more non-IID datasets, and vice versa. Consistent with established practices in the field [159; 114; 93; 80], each dataset was divided with three distinct degrees of non-IID with \(a\{0.1,0.3,0.5\}\). If there is no additional explanation, the non-IID degree \(a\) is set to \(0.5\) by default.

Following other classic and advanced FL works studying non-IID problems and communication-efficient FL [125; 52; 93; 159], we train ResNet-18  on all datasets in main experiments. And we reduce and increase the number of layers as ResNet-10 and ResNet-26 to verify the effect of FuseFL

   &  &  Supercond. \\ Interoperancy \\  } &  Net regime \\ extra data \\  } \\  FedAvg & \(T S\) & \(S\) & ✓ & ✓ \\  Average-based OPL & \(S\) & \(S\) & ✗ & ✗ \\ Ensemble-based OPL & \(S\) & \(S M\) & ✓ & ✗ \\ Model distillation OPL & \(S\) & \(S\) & ✓ & ✗ \\  FuseFL & \(S\) & \(S\) & ✓ & ✓ \\  

Table 1: Demystifying different FL algorithms. \(T\) represents communication rounds, \(S\) the model size, \(M\) the number of clients. The “Centralized” means training the model with all datasets aggregated with SGD. “Comm.” means communication.

in model-heterogeneity FL. The number of clients is set as \(M=5\) by default. Moreover, we study the scalability of our methods with different \(M\{5,10,20,50\}\).

We use SGD optimizer with momentum coefficient as 0.9, and the batch size is 128. The number of local training epochs \(E=200\). We search learning rates in \(\{0.0001,0.001,0.01,0.1\}\) and report the best results. The detailed hyper-parameters of different settings are shown in Table 9 of Appendix D.

**Baselines.** Except the classic baseline FedAvg  and advanced OFL method DENSE , we apply two prevailing data-free KD methods DAFL  and ADI  into OFL. We choose FedDF  as its high efficiency in few-round FL. We conduct ensemble FL as it is the upper bound across ensemble-and-distillation based methods, yet impractical in real-world scenarios. The communication round for all baseline methods is only 1. For our method FuseFL, the number of communication rounds is equal to the number of splitted blocks \(K\). However, the actual communication cost is as same as one-shot FL. Because FuseFL only communicates a part of the model. After all rounds, the total communication cost is \(SM\), where \(S\) is the model size, \(M\) the number of clients.

### Experimental Results

**Main Results.** Table 2 shows that FuseFL generally outperforms all other baselines except for ensemble FL. All ensemble-and-distillation baselines have lower performance than ensemble FL. Nevertheless, by the insights from causality (Section 3) and our innovative design (Section 4), FuseFL can significantly outperform ensemble FL for almost all cases except for CIFAR-100 with \(a=0.3,0.5\) and Tiny-Imagenet. We suppose the reason is that CIFAR-100 and Tiny-Imagenet has much more data divergence between different classes, thus the overlap between \(R^{}_{}\) is much less than other datasets. Recall that the benefits of FuseFL come from fusing sub models training on other clients, thus filtering out \(R^{}_{}\) and collecting \(R^{}_{}\) of the same class to improve the generalization performance. The large data divergence in CIFAR-100 and Tiny-Imagenet limits benefits of FuseFL.

**Support of heterogeneous model design.** Table 3 shows training heterogeneous model using FuseFL. In all \(M=5\) clients, 2 clients train ResNet-10 and other 2 clients train ResNet-26, the left 1 client trains ResNet-18. We set \(K=4\) for FuseFL. Results show that training with heterogeneous models has similar or even better results than homogeneous models, demonstrating that FuseFL supports training heterogeneous model well. This will be very useful in heterogeneous computation environments.

   Dataset &  &  &  &  &  &  \\  Method & 0.01 & 0.03 & 0.03 & 0.05 & 0.01 & 0.03 & 0.05 & 0.01 & 0.03 & 0.05 & 0.01 & 0.03 & 0.05 & 0.01 & 0.03 & 0.05 & 0.01 & 0.01 & 0.03 & 0.03 & 0.05 \\  FedAvg & 62.42 & 72.94 & 90.55 & 41.69 & 82.96 & 83.72 & 23.93 & 27.22 & 43.67 & 31.68 & 61.51 & 56.09 & 4.58 & 11.61 & 12.11 & 3.12 & 10.46 & 11.89 \\ FedDF & 60.35 & 74.04 & 92.18 & 43.58 & 80.07 & 84.53 & 46.78 & 43.58 & 83.56 & 84.13 & 73.54 & 73.98 & 28.71 & 32.82 & 36.53 & 35.34 & 18.22 & 27.43 \\ FedDAPH & 64.35 & 74.18 & 81.37 & 43.89 & 84.90 & 84.47 & 35.59 & 85.99 & 83.23 & 76.66 & 70.80 & 28.39 & 34.59 & 83.19 & 18.38 & 21.88 & 22.22 \\ FedDAPH & 64.35 & 75.00 & 55.94 & 89.48 & 81.15 & 84.95 & 84.96 & 59.54 & 55.54 & 77.55 & 85.53 & 31.53 & 35.13 & 35.13 & 35.13 & 35.12 \\ DENSE & 66.64 & 76.48 & 95.52 & 50.20 & 83.96 & 85.94 & 50.26 & 62.19 & 53.54 & 79.59 & 99.00 & 37.32 & 42.97 & 24.28 & 31.42 & 32.34 \\  Ensemble & 86.51 & 96.76 & 97.22 & 62.77 & 87.75 & 82.92 & 80.94 & 57.55 & 77.93 & 79.51 & 85.29 & 88.11 & 85.75 & 35.69 & 49.41 & 53.99 & 30.55 & **90.43** & 45.85 \\  FuseFL \(k=2\) & 97.63 & **94.04** & **95.43** & 81.38 & **93.94** & 97.07 & 85.14 & 81.44 & 75.52 & **91.01** & **93.07** & **84.17** & **45.21** & **45.21** & **42.12** & **20.53** & **31.11** & **34.34** \\ FuseFL \(k=3\) & 97.19 & 98.34 & 98.29 & 83.05 & 84.58 & **90.70** & **77.59** & **84.81** & 75.08 & 89.63 & 89.34 & **86.66** & 47.79 & **48.32** & 27.63 & **33.04** & 34.25 \\ FuseFL \(k=4\) & 96.66 & 98.35 & 98.16 & **85.32** & 85.87 & 88.24 & 70.46 & 80.70 & 74.99 & **80.30** & 88.88 & 89.94 & 34.97 & 39.08 & 40.73 & 25.11 & 32.59 & 33.82 \\   

Table 2: Accuracy of different methods across \(=\{0.1,0.3,0.5\}\) on different datasets. Ensemble means ensemble learning with local trained models, which is an upper bound of all previous methods but impractical in FL due to the large memory costs and the weak scalability of clients. Thus, we highlight the best results in **bold font** except Ensemble.

   non-IID degree & \(a=0.1\) & \(a=0.3\) & \(a=0.5\) \\  Ensemble & 57.5 & 77.35 & 79.91 \\  FuseFL & **73.79** & **84.58** & 81.15 \\ FuseFL (Avg) & 68.08 & 71.49 & 80.35 \\ FuseFL-Littero & 75.33 & 81.71 & 82.71 \\ FuseFL (Avg)-Hetero & 68.31 & 76.27 & 79.74 \\   

Table 3: Accuracy with FuseFL with Table 4: Memory Occupation. For different number conv1\(\)1 or averaging to support heterogeneous-of clients, the number of basic channels in ResNet-18 of FuseFL is set as 32, 20, 14, 9 with \(M\{5,10,20,50\}\), respectively. Other OFLs refer to FedAvg, FedDF, Fed-DAFL, Fed-ADI, DENSE.

[MISSING_PAGE_FAIL:10]