# The Learnability of In-Context Learning

Noam Wies, Yoav Levine & Amnon Shashua

The Hebrew University of Jerusalem

{noam.wies,yoav.levine,shashua}@cs.huji.ac.il

###### Abstract

In-context learning is a surprising and important phenomenon that emerged when modern language models were scaled to billions of learned parameters. Without modifying a large language model's weights, it can be tuned to perform various downstream natural language tasks simply by including concatenated training examples of these tasks in its input. Though disruptive for many practical applications of large language models, this emergent learning paradigm is not well understood from a theoretical perspective. In this paper, we propose a first-of-its-kind PAC based framework for in-context learnability, and use it to provide the first finite sample complexity results for the in-context learning setup. Our framework includes an initial pretraining phase, which fits a function to the pretraining distribution, and then a second in-context learning phase, which keeps this function constant and concatenates training examples of the downstream task in its input. We use our framework in order to prove that, under mild assumptions, when the pretraining distribution is a mixture of latent tasks (a model often considered for natural language pretraining), these tasks can be _efficiently_ learned via in-context learning, even though the model's weights are unchanged and the input significantly diverges from the pretraining distribution. Our theoretical analysis reveals that in this setting, in-context learning is more about identifying the task than about learning it, a result which is in line with a series of recent empirical findings. We hope that the in-context learnability framework presented in this paper will facilitate future progress towards a deeper understanding of this important new learning paradigm.

## 1 Introduction

The practice of pretraining language models (LMs) over massive general purpose text corpora has revolutionized natural language processing in recent years (Radford et al., 2019; Devlin et al., 2019; Brown et al., 2020). After an LM has been pretrained, the common approach for applying it to a specific downstream natural language task is to further train it on task-specific data, in a procedure referred to as fine-tuning (Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019). In their influential GPT3 paper, Brown et al. (2020) showed that a non-trivial alternative to fine-tuning emerges when the LM is large enough: an LM can be specialized to a downstream natural language task by simply receiving in its input a string composed of concatenated training examples of this task. Importantly, while the LM's weights are _unchanged_ in this procedure, some form of learning evidently takes place; the performance on the downstream task was shown to significantly improve with the number of concatenated training examples, for a disparate variety of natural language tasks.

This phenomenon, referred to as _in-context learning_, has had a profound practical impact on the applicability of large LMs: one does not need to have any access to the model weights in order to specialize the model for a certain task. Instead, a string of training examples provided even via API access to the model is enough, and often not many examples are required (Brown et al., 2020). However, despite its growing popularity in a multitude of use-cases (Bommasani et al., 2021), the reasons for the effectiveness of in-context learning in pretrained LMs are not well understood froma theoretical perspective. In particular, this new learning paradigm still lacks a formal definition of learning.

In this paper, we propose a PAC learning (Valiant, 1984) definition for in-context learning, along with the first finite sample-complexity learning results for in-context learning. Our results shed light on the mysterious question of why in-context learning works even though a string of concatenated input-output pairs does not resemble the natural distribution of pretraining examples. Our framework is based on an interpretation of pretraining as unsupervised multi-task learning of many natural language tasks, which dates back to the GPT2 paper (Radford et al., 2019), one of the instigators of the LM pretraining era. This view, which has since been widely adopted, was supported by experiments that revealed non-trivial zero-shot capabilities of pretrained LMs on a variety of natural language tasks. In other words, an LM pretrained only to maximize the likelihood of unlabeled text from the web, was able to perform reasonably well on a variety of downstream natural language tasks without learning from any additional training examples post pretraining, implying that many different natural tasks are directly learned during pretraining.

Following this interpretation, we prove our in-context learning results in a setting in which the pretraining distribution is a mixture of downstream tasks. Importantly, the pretraining examples are not explicitly associated with downstream tasks, but rather they are drawn from a mixture of tasks and the association between examples and tasks is latent. We prove that under mild assumptions, in-context learning is _guaranteed_ to happen for a model trained by such multi-task pretraining. We show that the in-context learning mechanism of concatenating input/output pairs of a certain task allows the pretrained LM to uncover the latent task and improve its performance _without modifying its weights_. Our assumptions are quite general, namely that there is a lower bound on the probability of any mixture component and of any token, and that the downstream tasks are both distinguishable and have sufficient margin between different labels. See Section 3.1 for a formal definition of our assumptions.

The interpretation of in-context learning as uncovering tasks that were already learned during pretraining is also supported by empirical evidence. For example, both Min et al. (2022) and Lyu et al. (2023) showed that replacing the labels provided in the in-context training examples with random task labels barely affects the performance of in-context learning, implying that the in-context learning mechanism is more about identifying the task than about learning it. Similarly, both Webson and Pavlick (2022) and Lampinen et al. (2022) studied whether LMs truly understand the text of the in-context examples, and found that irrelevant text that mimic the task can have a similar effect as learning with true training examples.

Finally, the latent task inference perspective of in-context learning was also examined theoretically. Xie et al. (2022) study a language-inspired toy distribution for pretraining. Specifically, they analyze a pretraining distribution consisting of a mixture of Hidden Markov Models (HMMs), where each HMM factor corresponds to a single task. They showed that in this setting in-context learning was guaranteed, however, unlike our work, their analysis is confined only to the infinite limit of the number of in-context examples. We provide polynomial sample complexity guarantees for in-context learning, which are much more relevant to its efficiency in practice (often referred to as "few-shot"). Furthermore, Xie et al. (2022) limit their analysis only to a certain class of mixture of HMMs as pretraining distribution, while our results are for any mixture distribution under mild assumptions (see above). Lastly, their analysis assumes perfect learning of the pretraining distribution while our PAC approach captures imperfect pretraining. Beyond Xie et al. (2022), several recent papers (Akyurek et al., 2023; Dai et al., 2022; von Oswald et al., 2022) showed that from an expressivity point of view, self-attention architectures can implement a gradient descent algorithm with in-context examples. However, these works do not justify why pretraining on natural data will converge to weights that implement gradient decent, nor do they provide finite sample-complexity results for in-context learning. Overall, our presented PAC framework for in-context learning (Section 2) allows us to prove the first finite in-context learning sample-complexity results, in a quite general setting (Section 3). We hope that this framework will facilitated a further expansions of the understanding of in-context learning as a new learning paradigm.

A PAC Learnability Framework for In-Context Learning

In this section, we define a Probably Approximately Correct (PAC) learnability framework (Valiant, 1984) for in-context learning. Conceptually, we aim to adjust the PAC learnability framework, in order to capture the in-context few-shot learning capabilities of large language models (Brown et al., 2020; Wei et al., 2022; Sanh et al., 2022; Ouyang et al., 2022).

We begin by describing the in-context learning paradigm. Let \(f_{}\) be a function fitted to a massive general purpose pretraining distribution. In order to apply \(f_{}\) for a specific downstream task, it is common to further train \(f_{}\) on pairs of the task's inputs \(x_{1},,x_{k}\) with their corresponding labels \(y_{1},,y_{k}\), in a procedure referred to as fine-tuning. In-context learning is an alternative, simpler technique, which "teaches" \(f_{}\) the task at hand by constructing a _prompt_ comprised of concatenated training examples. Specifically, denote the string concatenation operator by \(\), and let "\( n\)" be a special delimiter token. Then, the in-context learning technique is to construct a prompt \(p\) by concatenating pairs of the task's inputs along with theirs labels:

\[p x_{1} y_{1} n  x_{k} y_{k} n\] (1)

Given this prompt, the prediction is made by choosing the label \(y\) that is the most likely continuation for the prefix \(p x\) according to the function \(f_{}\). In other words, by predicting the label \(y\) that maximizes \(f_{}(p x y)\). Notably, the prefix \(p x\) does not resemble inputs that \(f_{}\) has trained on. Note that we used the newline symbol "\( n\)" for clarity. However, in practice the delimiter token might depend on implementation and could, for instance, be two successive newlines "\( n n\)" as done in the few-shot learning evaluation framework Evaluation Harness (Gao et al., 2021).

We now present our in-context learning learnability definition. Let \(\) be a pretraining distribution over strings of characters from an alphabet \(\). And let \(}\) be a downstream task's distribution over pairs of strings \(x^{}\) and theirs corresponding labels \(y^{}\). We aim to define distribution-dependent learnability in the in-context learning setup, for a frozen probabilistic model \(f_{}\) that originally trained to maximize the likelihood of the pretraining distribution \(\). Then, given a prompt \(p}^{k}\) which consist of \(k\) pairs of inputs and theirs corresponding labels, each independently sampled from \(}\), the model \(f_{}\) is tested on the zero-one loss of the in-context predictor:

\[L_{,}_{x,y}}[l_{0-1}(*{argmax}_{y^{}}f_{}(p  x y^{}),y)]\] (2)

Since we are interested in frozen models as opposed to models that are fine-tuned, we will require that the same \(f_{}\) model achieves a low \(L_{,}}\) loss for multiple downstream tasks simultaneously. The following is our PAC learning definition for in-context learning of a model that was pretrained to maximize the likelihood of the pretraining distribution.

**Definition 1**.: _Let \(\) and \(}\) be hypothesis classes of pretraining distributions and downstream distributions respectively. We say that \(}\) is in-context learnable after pretraining on a pretraining distribution \(\), if there exist a pretraining algorithm \(\) and a pair of functions \(m_{},m_{}}:(0,1)^{2}\) with the following property: For every \(,>0\) and every \(}}\), if the number of pretraining examples n is greater than \(m_{}}(,)\), and if the number of the downstream tasks' examples k is greater than \(m_{}}(,)\), then with probability of at least \(1-\) (over the choice of the \(n+k\) examples) the following holds:_

\[L_{,}}-\] (3)

In addition, we will say that a collection of downstream tasks \(}\) is **efficiently** in-context learnable after pretraining on \(\), if both \(m_{}\) and \(m_{}}\) in the above definition are polynomial in \(^{-1}\), \(^{-1}\) and \(|}|\).

In order to focus on in-context learning questions that are relevant for the natural language processing setup, and avoid complications that are not necessary for answering them, we will assume that the pretraining distribution \(\) belongs to some hypothesis class \(\) that is learnable by some pretraining algorithm:

**Assumption 1**.: _There exists a learning algorithm \(\), and a sample complexity function \(m_{}:(0,1)^{2}\), with the following property: for any pretraining distribution \(\) and any \(,>0\), if the number of pretraining examples \(n\) is greater than \(m_{}(,)\), then with probability at least \(1-\)_over the pretraining examples, for any \(T 1\), the total variation of the conditional distributions of the \(T\) 'th token is at most \(\):_

\[_{o_{1} o_{T}}|_{}(o_{T}|o_{1}  o_{T-1})-f_{}(o_{T}|o_{1} o_{T-1})|<\] (4)

_where \(f_{}\) denotes the model yielded by the pretraining algorithm \(\) and \(f_{}(o_{T}|o_{1} o_{T-1})\) denotes for the conditional likelihood \((o_{1} o_{T-1}o_{T})}{f_{}(o_{1}  o_{T-1})}\). In addition, \(m_{}\) is polynomial in both \(^{-1}\) and \(^{-1}\)._

With the above formal definition of in-context learning, we aim to shed some light on the mysterious in-context learning abilities of large LMs. Specifically, in the next section, we will shed light on the following major question: How do _frozen_ pretrained models learn from prompts that do not resemble their pretraining distribution?

## 3 Guarantees on In-Context Learning

In this section, we will demonstrate the use of the above learnability framework for in-context learning and analyze a setting in which pretraining a simple language model provably leads to in-context learning capabilities. For doing so, we follow previous work and view the language modeling task as an implicit multi-task setup. Indeed, creating human-like text involves many different skills, from grammar to world knowledge, so learning a language model inevitably develops a variety of skills (Gulordava et al., 2018; Zhang and Bowman, 2018; Weber et al., 2021). The implicit unsupervised multi-task view of language modeling can be traced to the GPT2 paper (Radford et al., 2019), which revealed that pretrained LMs are capable of a wide variety of natural language tasks without the need for further training. Since then, this view has been reinforced for example by Hendrycks et al. (2021). They showed that on a diverse massive set of 57 real world text understanding tasks, the largest GPT-3 (Brown et al., 2020) language model improves over random chance by almost 13 percents points on average. Importantly, these results were obtained in a zero-shot learning setting, _i.e._ in a setting where the language model had only been pretrained to maximize the likelihood of unlabeled text. Accordingly, these results suggest that many different natural tasks are directly learned during pretraining.

We reflect the above implicit multi-task nature of language modeling by assuming that the pretraining distribution contains a latent variable that represents the task at hand. We show below that for such pretraining distributions, adding training examples to the in-context learning prompt implicitly reveals the already learned latent task. In Subsection 3.1, we present a multi-task pretraining hypothesis class, for which Subsection 3.2 shows that in-context learning reveals what task is currently being performed.

### The analyzed latent concept hypothesis class

In this subsection, we describe the analyzed multi-task pretraining hypothesis class, as well as the corresponding downstream task hypothesis class that can be learned in-context after pretraining. To begin, we define the pretraining distribution as a mixture of multiple downstream tasks. Importantly, during pretraining the downstream task of each example is unknown, _i.e._ it is a latent variable, and thus pretraining is not equivalent to fine-tuning on the task since the model cannot simply ignore pretraining examples of irrelevant tasks. Specifically, we generate a length-\(T\) pretraining example \(x_{1},,x_{T}\) from the pretraining distribution \(\) by first sampling a concept \(\) from a family of concepts \(\) according to a prior \(()\). We then sample the tokens according to the concept's specific distribution \(_{}(x_{1},,x_{T})\).

Moving to the downstream tasks, we will prove in-context learnability results for tasks where the underlying inputs distribution is a component \(\) in the pretraining mixture distribution \(\). Formally, we generate a length \(T\) downstream task example \(x\) and the corresponding label \(y\) from \(}\) by first sampling \(T\) tokens \(o_{1},,o_{T}\) according to the downstream task distribution \(_{}(o_{1},,o_{T})\). Then we assemble \(x\) using all tokens except the last one, and set the label to be that token _i.e._ we set that \(x_{t}=o_{t}\) for any \(t<T\) and that \(y=o_{T}\). Note that in principle the concatenation of independent examples in \(p\) causes a distribution drift from the pretraining distribution1. In this sense, the analyzedmodel captures the fact that few-shot prompts are unnatural since they are not encountered during pretraining.

Now we describe assumptions about the pretraining distributions, for which we will prove our in-context learnability results. Our first assumption requires that given a delimiter token "\( n\)", two successive strings \(s_{1}\) and \(s_{2}\) that are concatenated with "\( n\)" are approximately independent according to \(_{}\):

**Assumption 2**.: _There exists a constant \(0<c_{1} 1\) such that for any two strings \(s_{1},s_{2}\) in \(^{}\) and any concept \(\) the following holds:_

\[c_{1}_{}(s_{1} n)_{}(s_{2})}{_{}(s_ {1} n s_{2})}}\] (5)

Note that when the two successive strings \(s_{1}\) and \(s_{2}\) are exactly independent, the probability ratio in Equation 5 is equal to one, and the constant \(c_{1}\) quantifies the deviation from this situation. While this assumption might sound restrictive, it is reasonable to assume that two consecutive paragraphs are not highly dependent according to the distributions in the pretraining mixture. Intuitively, we will use this assumption in order to apply concentration inequalities to the likelihood of the in-context prompt, and we will deduce that the role of the in-context prompt is to reweight the prior regarding the different mixture components. It is important to note that the approximate independence assumption for any component in the mixture does not imply approximate independence of the mixture distribution itself. Hence this reweighting is possible, since the assumption does not imply that the in-context prompt is ignored.

Beyond this approximate independence, we will also require that there exist a lower bound on the conditional probability of any single token:

**Assumption 3**.: _There exist a constant \(c_{2}>0\) such that for any string \(s\) in \(^{}\), any character \(\), and any concept \(\) the following holds:_

\[_{}(\,|\,s)>c_{2}\] (6)

Basically, we need this assumption in order to avoid the harm of zero likelihood due to the unnatural concatenation of input and output pairs in the prompt \(p\) (where the prompt \(p\) is defined in Section 2). Note that without such an assumption, in-context learning is impossible since the probability of a prompt \(p\) might be zero and hence the prediction of the model in such cases becomes meaningless. Finally, we assume that the prior distribution is strictly positive. In other words, we will assume that there is a lower bound higher than zero on the likelihood of any concept appearing in the pretraining distribution.

**Assumption 4**.: _There exist a constant \(c_{3}>0\) such that for the prior \(_{}()\) of any concept \(\), is at least \(c_{3}\)._

Clearly, without such an assumption, the prior of the downstream task can be arbitrarily low, which means it will be nearly impossible to recognize the task. In the next subsection, we will use the above assumptions in order to provide in-context learning guarantees via the mechanism of task recognition.

### Guarantees on In-Context Learning via Latent Concept Inference

In this subsection, we analyze the prediction of an in-context learning model in the setting described in Subsection 3.1. We show that in this setting, there is a polynomial sample complexity that guarantees in-context learning is **P**robably **A**pproximately **C**orrect. At a high level, since the pretraining is not precise and can only approximate the pretraining distribution \(\) up to some error \(_{}>0\) (see Assumption 1), we will split the in-context prediction analysis into two parts. The first part will involve the simpler case of test examples \(x\) and corresponding label candidates \(y\) and \(\) for which the margin between the conditional likelihoods of the labels \(_{}}(y\,|\,x)\) and \(_{}}(\,|\,x)\) is large enough. In this scenario, we show that both the deviation due to imperfect pretraining and due to imperfect task recognition is negligible. Therefore, we will conclude that such deviations do not have any impact on the loss of in-context learning. In the second scenario, when the difference between ground-truth likelihoods is small, the error rate of the Bayes optimal classifier must be high. Accordingly, even though the in-context predictor might confuse between labels, the loss in any case will be small because we only compare it to the error rate of the Bayes optimal classifier.

Starting with the first scenario, where the margin between label candidates is sufficiently large, we will prove that as more examples are added to the prompt, the in-context predictions converge to the correct label. As a preliminary step, we prove a lemma regarding the ratio of the prompt likelihoods \(_{}(p)\) (where the prompt \(p\) is defined in Section 2) according to the ground-truth task versus the other mixture components. In other words, we ask how likely it is that the prompt of concatenated examples was sampled according to one of the tasks distributions versus the other tasks distributions. We will use this lemma in order to estimate the effect of the in-context prompt, on the prior regarding the different mixture components. Specifically, we denote by \(_{}\) the minimum Kullback-Leibler divergence between the ground-truth component, and the other mixture components. Then, we prove that the ratio of the prompt probabilities according to the ground-truth components converge to zero, with rate that is exponential in both the number of in-context examples \(k\), and in the minimal Kullback-Leibler divergence \(_{}\). Intuitively, the exponential rate with regard to the number of examples comes naturally from the fact that each example in the prompt is sampled independently of the others. As a result, their effect is a multiplicative one. Additionally, the Kullback-Leibler divergence between the ground truth component and the other mixture components measures log probabilities, while we are interested in the probabilities themselves. Hence the rate is also exponential in the above Kullback-Leibler divergence. Formally, we have that:

**Lemma 1**.: _Let \(\) be a pretraining distribution for which assumptions 2,3 hold, and let \(^{}\) be a downstream task mixture component from \(\) such that \(_{}>8 c_{2}}\). Then, there exists \(m_{}}:(0,1)^{2}\) with the following property: for any \(,>0\) and any \(^{}\), if the number of in-context examples \(k\) is at least \(m_{}}(,)\), then, \(_{}(p)}{_{^{}}(p)}<\) with probability of at least \(1-\) (over the choice of the \(k\) in-context examples). Moreover, the above still holds when the labels in \(p\) are randomly flipped, and \(m_{}}\) can be chosen such that it will be polynomial in \(,, c_{2}}, }}\) and \(T\)._

Proof.: In essence, we prove this lemma by using the approximate independence assumption in order to apply concentration inequalities. In addition, we use Assumption 3 for bounding the distribution drift that is caused by the artificially inserted newline token. In particular, the log of the ratio of prompt probabilities according to different mixture components is concentrated around its expectation. Importantly, this expectation is equal to minus one times the Kullback-Leibler divergence between the components, plus a term that is caused by the mentioned distribution drift. Thus, we conclude that the ratio of the prompt probabilities according to the ground-truth task distribution and the other pretraining components converges to zero. Moreover, the rate of that convergence is exponential in both the number of in-context demonstrations, and the minimal Kullback-Leibler divergence between different mixture components. See full details in Section A of the appendix. 

Now we will use the above Lemma 1 to analyze the in-context predictions, namely the labels \(\) that maximize the likelihood of the concatenation of the in-context prompt \(p\), with the example \(x\) according to the pretrained model \(f_{}\). Essentially, we will aim to understand when these predictions are identical to the Bayes Optimal Classifier predictions. That it, when these predictions align with the labels \(y\) that maximize the likelihood of the example \(x y\) as determined by downstream task distribution \(_{}}\). Since we are in the first scenario, where the margin between label candidates is large enough. We will prove that in this case, for large enough \(k\), the ground truth in-context predictor also has margin that is at least half of the original margin. Hence, we will conclude that for such \(k\) the ground truth in-context predictor is equal to the Bayes Optimal Classifier. Moreover, we will prove a lower bound on the margin of the in-context predictions, so the above still holds for any distribution that approximates the pretraining distribution sufficiently well.

Formally, for a test example input \(x\), we define the margin \((x,y,)\) between two label candidates \(y\) and \(\) as the difference between their conditional likelihoods \(_{}}(y\,|\,x)\) and \(_{}}(\,|\,x)\) according to the downstream ground-truth distribution. Similarly, for a test example input \(x\) and in-context prompt \(p\), we define the margin \((p,x,y,)\) between two label candidates \(y\) and \(\) as the difference between their conditional likelihoods \(_{}(y\,|\,p x)\) and \(_{}(\,|\,p x)\) according to the pretraining distribution conditioned on the prompt \(p\). Using these definitions, we consider triplets of test example input \(x\) and two labels candidates \(y,\) with margin at least two times the pretraining error \(_{}\) as the first scenario. In this case we will prove that for large enough \(k\) the ground truth in-context predictor also has margin of at least the pretraining error \(_{}\). This means that deviations arising from imperfect task recognition will be negligible in this situation. Moreover, the margin will remain larger than the pretraining error, which means the pretrained model \(f_{}\) handles this case well.

**Theorem 1**.: _Let \(\) and \(}\) be a pair of pretraining distribution and downstream task, for which Assumption 4 as well as the assumptions in Lemma 1 upholds. Then, there exists \(m_{}}:(0,1)^{2}\) with the following property: for every test example \(x\) and two label candidates \(y,\) with positive margin \((x,y,)>0\) and \(>0\), if the number of in-context examples \(k\) is at least \(m_{}}()}}{ 2},)\), then \((p,x,y,)> )}}{2}+c_{1}^{2}-1\) with probability of at least \(1-\) (over the choice of the \(k\) in-context examples). Moreover, the above still holds when the labels in \(p\) are randomly flipped2, and \(m_{}}\) can be chosen such that it will be polynomial in \(,, c_{2}  c_{3}},}\) and \(T\)._

Proof.: We begin by writing the difference between the ground-truth label likelihood, and the likelihood of other another label \(\) explicitly. Specifically, by the definition of conditional probabilities we have that:

\[_{}(y\,|\,p x)-_{} (\,|\,p x)=_{} ()[_{}(p x y)- _{}(p x)]}{_{} _{}()_{}(p x )}\] (7)

Now, denote by \(^{}\) the mixture component of \(}\), then Assumption 2 assures us that for each component in the mixture, the textual prompt is approximately independent of the test example. So we can use Lemma 1 and get that in both the numerator and the denominator, the \(^{}\) term is the dominant term in the sum. Thus, we will get that the difference of the labels' likelihoods is approximately the fraction of the \(^{}\) terms, which is equal to the original downstream task margin. So we will conclude that the margin of the in-context predictor is at least half of the downstream task original margin. See full details in Section B of the appendix. 

We denote by \(_{}}\) the minimal margin of the Bayes Optimal Classifier predictions in downstream task \(}\)_i.e._ the minimal margin between the Bayes Optimal Classifier prediction and another labels. With the above definitions and theorem, we can combine the scenario of large margins, which are preserved by the in-context predictor, with the scenario of small margins, in which the loss is minimally affected by the wrong prediction, and prove our main in-context learnability results:

**Theorem 2**.: _Let \(}\) be hypothesis classes of downstream distributions, and denote by \(\) a mixture distribution on \(}\) for which assumptions 1,2,3 and 4 uphold. Further assume that the margin \(_{}}\) of any downstream task \(}}\) is at least \(4(1-c_{1}^{2})\), and the minimal Kullback-Leibler divergence between different distributions is greater than \( c_{2}}\). Then \(}\) is efficiently in-context learnable after pretraining on \(\) (see Definition 1)._

Proof.: Assumption 1 assures us the existence of the pretraining algorithm with a polynomial sample complexity. So we will prove the theorem with pretraining sample complexity that is derived from this algorithm, where the accuracy required from pretraining is \(\) times the accuracy required from in-context learning (see analysis below). In addition, Theorem 1 assures us that for large enough \(k\) the ground truth in-context predictions are equal to the Bayes Optimal Classifier predictions, so we will prove the theorem with downstream sample complexity that is derived from Theorem 1.

Let \(,>0\) and denote by \(f_{}\) the model that was learned during the pretraining process. We will prove that the contribution of any \(x\) to the loss \(L_{,}}\) (see Equation 2) is at most \(\) and hence complete the proof. Given \(x\), let \(y\) be the Bayes Optimal Classifier prediction for that \(x\). Then, we will split the analysis into two cases.

The first case is of examples \(x,y\) such that theirs margin \((x,y,)\) from any alternative label candidate \(\) is at least \(8_{}\). In this case, Theorem 1 assures us that for large enough \(k\) the ground truth in-context predictor also has margin \((p,x,y,)\) that is greater than \((x,y,)+c_{1}^{2}-1\). Now since \((x,y,)_{}}>4 (1-c_{1}^{2})\) we have that \((p,x,y,)\) is at least \(2_{}\). Thus, we conclude that in this case, the predictions of the ground truth in-context prediction and \(f_{}\) are the same. Furthermore both of them are identical to the Bayes Optimal Classifier prediction, and hence \(x\) does not contribute to the loss.

Moving to the second case, and denote by \(\) the in-context prediction of \(f_{}\). Then, the arguments from the previous paragraph assure us that \(_{^{*}}(y\,|\,x)-_{^{*}}( \,|\,x)<8_{}\), since otherwise we will get that \(f_{}(y\,|\,x)>f_{}(\,|\,x)\). Finally, we will choose that \(_{}=\), and hence get that also in the second case, the contribution of \(x\) to the loss is less than \(\). 

## 4 Related Work

Since the Probably Approximately Correct (PAC) learning framework was introduced in Valiant (1984), a rich line of works has extended the framework to distribution-dependent bounds (Benedek and Itai, 1991; Vayatis and Azencott, 1999; Sabato et al., 2013)_inter alia_. These works relax PAC's adversarial requirement of generalization for all input distributions, and only consider distributions that satisfy certain statistical properties. Consequently, these papers provide more realistic sample complexity bounds. In this paper we present a framework for in-context learning learnability that uses self-supervised pretraining to assist in solving downstream tasks. In this framework, we follow the above line of distribution-specific works with the distinct feature of a self-supervised pretraining phase, and of learning with frozen models through their input context. The advantages of such pretraining have been studied from a theoretical perspective before. For example, Saunshi et al. (2021) and Wei et al. (2021) demonstrate that language modeling can benefit downstream tasks either by prompt tuning or head tuning. However, unlike our work, in their analysis some weights are learned. In contrast, we focus on frozen models that can learn only from their input context, which includes the concatenation of few inputs and outputs pairs.

Returning to in-context learning, a recent line of work study learning paradigms that are based on concatenating several inputs into a single input context. For example, Garg et al. (2022) show that a Transformer architecture is able to discover in-context learning algorithms for simple function classes, such as two-layer neural networks, and decision trees. Similarly, Akyurek et al. (2023) show that a Transformer architecture is able to discover efficient Bayes optimal least square learning algorithms, and Laskin et al. (2022) show that a Transformer architecture is able to discover efficient in-context reinforcement learning algorithms. Additionally, Levine et al. (2022) studied the inductive bias of in-context learning, and proved that Transformer based language models can model much stronger dependencies between text segments that appeared in the same training example. Finally, Li et al. (2023) proved a multi-task generalization bound for in-context learning with the Transformer architecture. However, unlike our work, the pretraining distribution in all of the above works designed to include input context that includes few-shot demonstrations explicitly. As a consequence, these methods do not answer the mysterious question of how frozen pretrained models can learn from in-context prompts that do not resemble their pretraining distribution.

Another recent line of work investigates possible mechanisms that enable in-context learning. For example, Olsson et al. (2022) provide indirect evidence that _induction heads_ might constitute the mechanism for in-context learning in large transformer models. In addition, several recent papers (Akyurek et al., 2023; Dai et al., 2022; von Oswald et al., 2022) showed that from an expressivity point of view, self-attention architectures can implement a gradient descent algorithm with in-context examples. Finally, Chan et al. (2022) provided empirical evidence that some data distributional properties encourage in-context learning even when the frozen models do not see prompts during pretraining. However, unlike our work, all of the above works does not provide theoretical guarantees of in-context learnability.

Finally, Xie et al. (2022) are the closest to our work. They studied a language-inspired toy distribution for pretraining and analyzed a pretraining distribution consisting of a mixture of Hidden Markov Models (HMMs). Unlike their results, our results are applicable to any mixture distribution that meets our mild assumption. In addition, unlike our polynomial sample complexity guarantees, their analysis guarantees in-context learning only for an infinite number of in-context examples. Lastly, their analysis assumes perfect pretraining distribution learning, but our approach captures imperfect pretraining.

## 5 Conclusion

The discovery of in-context learning in large LMs, made by Brown et al. (2020), was surprising to many in our field. A model that was pretrained to maximize the likelihood of natural text was able to make use of _concatenated_ training examples of downstream natural language tasks--inputs that do not resemble its pretraining distribution, and moreover these inputs improved the model's ability to perform the task. Our theoretical results, based on a common latent multitask framework for the pretraining phase, shed light on the above surprising mysteries. With our PAC-based framework, we were able to provide sample complexity guarantees for in-context learning in such pretrained models, which are not only the first finite sample complexity results for this framework but they also indicate efficient (polynomial) in-context learning, which reflect the behavior of this setting in practice.

We hope that our framework can be used to deepen the understanding of the in-context learning phenomenon. In particular, we mark the connection between model size and the in-context learning efficiency as an interesting open question (Wei et al., 2022b). Additionally, in-context learning has shown to be capable of learning new tasks not included in the pre-training distribution (Wei et al., 2023). The extensions of our results to such situations, as well as input-dependent bounds that capture the sensitivity of few-shot in-context learning to the order of the few-shot examples is an interesting open questions.

Limitations:While we assume that the pre-training distribution perfectly matches a mixture of downstream tasks, we acknowledge that the pre-training data in real-world LLMs is often noisy and imperfect. Our results represent an idealized scenario with perfect pre-training data. Extending our analysis to account for limitations of real-world pre-training data remains an important direction for future work.