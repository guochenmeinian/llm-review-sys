# The Contextual Lasso:

Sparse Linear Models via Deep Neural Networks

 Ryan Thompson

University of New South Wales

CSIRO's Data61

&Amir Dezfouli

BIMLOGIQ

&Robert Kohn

University of New South Wales

Corresponding author. Email: ryan.thompson1@unsw.edu.auPart of this work was carried out while the author was at CSIRO's Data61.

###### Abstract

Sparse linear models are one of several core tools for interpretable machine learning, a field of emerging importance as predictive models permeate decision-making in many domains. Unfortunately, sparse linear models are far less flexible as functions of their input features than black-box models like deep neural networks. With this capability gap in mind, we study a not-uncommon situation where the input features dichotomize into two groups: explanatory features, which are candidates for inclusion as variables in an interpretable model, and contextual features, which select from the candidate variables and determine their effects. This dichotomy leads us to the contextual lasso, a new statistical estimator that fits a sparse linear model to the explanatory features such that the sparsity pattern and coefficients vary as a function of the contextual features. The fitting process learns this function nonparametrically via a deep neural network. To attain sparse coefficients, we train the network with a novel lasso regularizer in the form of a projection layer that maps the network's output onto the space of \(_{1}\)-constrained linear models. An extensive suite of experiments on real and synthetic data suggests that the learned models, which remain highly transparent, can be sparser than the regular lasso without sacrificing the predictive power of a standard deep neural network.

## 1 Introduction

Sparse linear models--linear predictive functions in a small subset of features--have a long history in statistics, dating back at least to the 1960s (Garside, 1965). Nowadays, against the backdrop of elaborate, black-box models such as deep neural networks, the appeal of sparse linear models is largely their transparency and intelligibility. These qualities are sought in decision-making settings (e.g., consumer finance and criminal justice) and constitute the foundation of interpretable machine learning, a topic that has recently received significant attention (Murdoch et al., 2019; Molnar et al., 2020; Rudin et al., 2022; Marcinkevics and Vogt, 2023). Interpretability, however, comes at a price when the underlying phenomenon cannot be predicted accurately without a more expressive model capable of well-approximating complex functions, such as a neural network. Unfortunately, one must forgo direct interpretation of expressive models and instead resort to post hoc explanations (Ribeiro et al., 2016; Lundberg and Lee, 2017), which have their own flaws (Laugel et al., 2019; Rudin, 2019).

Motivated by a desire for interpretability and expressivity, this paper focuses on a setting where sparse linear models and neural networks can collaborate together. The setting is characterized by a not-uncommon situation where the input features dichotomize into two groups, which we call explanatory features and contextual features. Explanatory features are features whose effects are of primary interest. They should be modeled via a low-complexity function such as a sparse linear model for interpretability. Meanwhile, contextual features describe the broader predictive context,e.g., the location of the prediction in time or space (see the house pricing example below). These inform which explanatory features are relevant and, for those that are, their exact low-complexity effects. Given this role, contextual features are best modeled via an expressive function class.

The explanatory-contextual feature dichotomy described above leads to the seemingly previously unstudied contextually sparse linear model:

\[g([y\,|\,,])=_{j S() }x_{j}_{j}(). \]

To parse the notation, \(y\) is a response variable, \(=(x_{1},,x_{p})^{}^{p}\) are explanatory features, \(=(z_{1},,z_{m})^{}^{m}\) are contextual features, and \(g\) is a link function (e.g., identity for regression or logit for classification).3 Via the contextual features, the set-valued function \(S()\) encodes the indices of the relevant explanatory features (typically, a small set of \(j\)s), while the coefficient functions \(_{j}()\) encode the effects of those relevant features. The model (1) draws inspiration from the varying-coefficient model (Hastie and Tibshirani, 1993; Fan and Zhang, 2008; Park et al., 2015), a special case that assumes all explanatory features are always relevant, i.e., \(S()=\{1,,p\}\) for all \(^{m}\). We show that this new model is more powerful for various problems, including energy forecasting and disease prediction. For these tasks, sparsity patterns can be strongly context-dependent.

The main contribution of our paper is a new statistical estimator for (1) called the contextual lasso. The new estimator is inspired by the lasso (Tibshirani, 1996), a classic sparse learning tool with excellent properties (Hastie et al., 2015). We focus on tabular datasets as these are the most common use case for the lasso and its cousins. Whereas the lasso fits a sparse linear model that fixes the relevant features and their coefficients once and for all (i.e., \(S()\) and \(_{j}()\) are constant), the contextual lasso fits a contextually sparse linear model that allows the relevant explanatory features and coefficients to change according to the prediction context. To learn the map from contextual feature vector to sparse coefficient vector, we use the expressive power of neural networks. Specifically, we train a feedforward neural network to output a vector of linear model coefficients sparsified via a novel lasso regularizer. In contrast to the lasso, which constraints the coefficients' \(_{1}\)-norm, our regularizer constraints the _expectation_ of the coefficients' \(_{1}\)-norm with respect to \(\). To implement this new regularizer, we include a novel projection layer at the bottom of the network that maps the network's output onto the space of \(_{1}\)-constrained linear models by solving a convex optimization problem.

To briefly illustrate our proposal, we consider data on property sales in Beijing, China, studied in Zhou and Hooker (2022). We use the contextual lasso to learn a pricing model with longitude and latitude as contextual features. The response is price per square meter. Figure 1 plots the fitted coefficient functions of three property attributes (explanatory features) and an intercept. The relevance and effect of these attributes can vary greatly with location. The elevator indicator, e.g., is irrelevant throughout inner Beijing, where buildings tend to be older and typically do not have elevators. The absence of elevators also makes it difficult to access higher floors, hence the negative effect of floor on price. Beyond the inner city, the floor is mostly irrelevant. Naturally, renovations are valuable everywhere, but more so for older buildings in the inner city than elsewhere. The flexibility of the contextual lasso to add or remove attributes by location, and simultaneously determine their coefficients, equips sellers with personalized interpretable models containing only the attributes most relevant to them. At the same time, these models outpredict both the lasso and a deep neural network; see Appendix A.

Figure 1: Fitted coefficient functions from the contextual lasso for the house pricing dataset. Colored points indicate coefficient values at different locations. Grey points indicate coefficients equal to zero.

The rest of paper is organized as follows. Section 2 introduces the contextual lasso and describes techniques for its computation. Section 3 discusses connections with earlier related work. Section 4 reports experiments on synthetic and real data. Section 5 closes the paper with a discussion.

## 2 Contextual lasso

This section describes our estimator. To facilitate exposition, we first rewrite the contextually sparse linear model (1) more concisely:

\[g([y\,|\,,])=^{}().\]

The notation \(():=(_{1}(),,_{p}())^{}\) represents a vector coefficient function which is sparse over its codomain. That is, for different values of \(\), the output of \(()\) contains zeros at different positions. The function \(S()\), which encodes the set of active explanatory features in (1), is recoverable as \(S():=\{j:_{j}() 0\}\).

### Problem formulation

At the population level, the contextual lasso comprises a minimization of the expectation of a loss function subject to an inequality on the expectation of a constraint function:

\[_{}[l(^{} (),y)] [\|()\|_{1}], \]

where the set \(\) is a class of functions that constitute feasible solutions and \(l:^{2}\) is the loss function, e.g., square loss \(l(,y)=(y-)^{2}\) for regression or logistic loss \(l(,y)=-y()-(1-y)(1-)\) for classification. Here, the expectations are taken with respect to the random variables \(y\), \(\), and \(\). The parameter \( 0\) controls the level of regularization. Smaller values of \(\) encourage \(()\) towards zero over more of its codomain. Larger values have the opposite effect. The contextual lasso thus generalizes the lasso, which learns \(()\) as a constant:

\[_{}[l(^{},y )]\|\|_{1}.\]

To reiterate the difference: the lasso coaxes the _fixed coefficients_\(\) towards zero, while the contextual lasso coaxes the _expectation of the function_\(()\) to zero. The result for the latter is coefficients that can change in value and sparsity with \(\), provided the function class \(\) is suitably chosen.

Given a sample \((y_{i},_{i},_{i})_{i=1}^{n}\), the data version of the population problem (2) replaces the unknown expectations with their sample counterparts:

\[_{}_{i=1}^{n}l(_{i}^{}(_{i}),y_{i}) _{i=1}^{n}\|(_{i})\|_{1}. \]

The set of feasible solutions to optimization problem (3) are coefficient functions that lie in the \(_{1}\)-ball of radius \(\) when averaged over the observed data.4 To operationalize this estimator, we take \(=\{_{}():\}\), where \(_{}()\) is a certain architecture of feedforward neural network (described shortly) parameterized by weights \(\). This choice leads to our core proposal:

\[_{}_{i=1}^{n}l(_{i}^{} _{}(_{i}),y_{i}) _{i=1}^{n}\|_{}(_{i})\|_{1} . \]

Configuring a feedforward neural network such that its outputs are sparse and satisfy the \(_{1}\)-constraint is not trivial. We introduce a novel network architecture to address this challenge.

### Network architecture

The neural network architecture--depicted in Figure 2--involves two key components. The first and most straightforward component is a vanilla feedforward network \(():^{m}^{p}\). The purpose of the network is to capture the nonlinear effects of the contextual features on the explanatory features.

Since the network involves only hidden layers with standard affine transformations and nonlinear maps (e.g., rectified linear activation functions), the coefficients they produce generally do not satisfy the contextual lasso constraint and are not sparse. To enforce the constraint and attain sparsity, we employ a novel projection layer as the second main component of our network architecture.

The projection layer takes the dense coefficients \(()\) from the network and maps them to sparse coefficients \(()\) by performing a projection onto the \(_{1}\)-ball. Because the contextual lasso does not constrain each coefficient vector to the \(_{1}\)-ball, but rather constrains the _average_ coefficient vector, we project all \(n\) coefficient vectors \((_{1}),,(_{n})\) together. That is, we take the final sparse coefficients \((_{1}),,(_{n})\) as the minimizing arguments of a convex optimization problem:

\[(_{1}),,(_{n}):= *{arg\,min}_{_{1},,_{n}: _{i=1}^{n}\|_{i}\|_{1}}_{i=1}^{n }\|(_{i})-_{i}\|_{2}^{2}. \]

The minimizers of this optimization problem are typically sparse thanks to the geometry of the \(_{1}\)-ball. The idea of including optimization as a layer in a neural network is explored in previous works (see, e.g., Amos and Kolter, 2017; Agrawal et al., 2019). Yet, to our knowledge, no previous work has studied optimization layers (also known as implicit layers) for inducing sparsity in a neural network.

The optimization problem (5) does not admit an analytical solution, though it is solvable by general purpose convex solvers (see, e.g., Boyd and Vandenberghe, 2004). However, because (5) is a highly structured problem, it is also amenable to more specialized algorithms. Such algorithms facilitate the type of scalable computation necessary for deep learning. Duchi et al. (2008) provide a low-complexity algorithm for solving (5) when \(n=1\). Algorithm 1 below is an extension to \(n 1\). The algorithm consists of two main steps: (1) computing a thresholding parameter \(\) and (2) soft-thresholding the inputs using the computed \(\). Critically, the operations comprising Algorithm 1 are suitable for computation on a GPU, meaning the model can be trained end-to-end at scale.

```
input Dense coefficients \(_{1},,_{n}\) and radius \(\)  Set \(=(|_{1}^{}|,,|_{n}^{}|)^{}\)  Sort \(\) in decreasing order: \(_{i}_{j}\) for all \(i<j\)  Set \(k_{}=\{k:_{k}>(_{l=1}^{k}_{l}-n )/k\}\)  Set \(=(_{k=1}^{k_{}}_{k}-n)/k_{}\)  Compute \(_{1},,_{n}\) as \(_{ij}=*{sign}(_{ij})(|_{ij}|-,0)\) for \(i=1,,n\) and \(j=1,,p\) output Sparse coefficients \(_{1},,_{n}\)
```

**Algorithm 1** Projection onto \(_{1}\)-ball

Although the projection algorithm involves some nondifferentiable operations, most deep learning libraries provide gradients for these operations, e.g., sort is differentiated by permuting the gradients and abs is differentiated by taking the subgradient zero at zero. The gradient obtained by automatically differentiating through Algorithm 1 is the same as that from implicitly differentiating through a convex solver (e.g., using cvxpylayers of Agrawal et al., 2019), though the latter is slower. In subsequent work, Thompson et al. (2023) derive analytical gradients for a projection layer that maps matrices onto an \(_{1}\)-ball. Their result readily adapts to the vector case dealt with here.

Computation of the thresholding parameter is performed only during training. For inference, the estimate \(\) from the training set is used for soft-thresholding. That is, rather than using Algorithm 1

Figure 2: Network architecture. The contextual features \(\) pass through a series of hidden layers. The resulting dense coefficients \(_{1},,_{p}\) then enter a projection layer to produce sparse coefficients \(_{1},,_{p}\). Here, the last coefficient is gray to illustrate that it is zeroed-out by the projection layer.

as an activation function when performing inference, we use \(T(x):=*{sign}(x)(|x|-,0)\). The purpose of using the estimate \(\) rather than recomputing \(\) via the algorithm is because the \(_{1}\)-constraint applies to the _expected_ coefficient vector. It need not be the case that every coefficient vector produced at inference time lies in the \(_{1}\)-ball, which would occur if the algorithm is rerun.

Algorithm 1 computes the thresholding parameter using the full batch of \(n\) observations. The algorithm can also be applied to mini-batches during training. Once training is complete, the estimate \(\) for inference can be obtained via a single forward pass of the full batch through the network.

### Grouped explanatory features

In certain settings, the explanatory features may be organized into groups such that all the features in a group should be selected together. These groups may emerge naturally (e.g., genes in the same biological path) or be constructed for a statistical task (e.g., basis expansions for nonparametric regression). The prevalence of such problems has led to the development of sparse estimators capable of handling groups, one of the most well-known being the group lasso (Yuan and Lin, 2006; Meier et al., 2008). Perhaps unsurprisingly, the contextual lasso extends gracefully to grouped selection.

Let \(_{1},,_{g}\{1,,p\}\) be a set of \(g\) nonoverlapping groups, and let \(_{k}()\) and \(_{k}\) be the coefficient function and explanatory features restricted to group \(_{k}\). In the noncontextual setting, the group lasso replaces the \(_{1}\)-norm \(\|\|_{1}\) of the lasso with a sum of group-wise \(_{2}\)-norms \(_{k=1}^{g}\|_{k}\|_{2}\). To define the _contextual group lasso_, we make the analogous modification to (4):

\[_{}_{i=1}^{n}l(_{k=1}^{g} _{ik}^{}_{k,}(_{i}),y_{i}) *{s.t.}_{i=1}^{n}_{k=1}^{g}\|_ {k,}(_{i})\|_{2}.\]

Similar to how the absolute values of the \(_{1}\)-norm are nondifferentiable at zero, which causes individual explanatory features to be selected, the \(_{2}\)-norm is nondifferentiable at the zero vector, causing grouped explanatory features to be selected together. To realize the grouped estimator, we adopt the same architecture as before but replace the previous (ungrouped) projection layer with its grouped counterpart. This change demands a different projection algorithm, presented in Appendix B.

### Side constraints

Besides the contextual (group) lasso constraint, our architecture readily accommodates side constraints on \(()\) via modifications to the projection. For instance, we follow Zhou and Hooker (2022) in the house pricing example (Figure 1) and constrain the coefficients on the elevator and renovation features to be nonnegative. Such sign constraints reflect domain knowledge that these features should not impact price negatively. Appendix C presents the details and proofs of this extension.

### Pathwise optimization

The lasso regularization parameter \(\), controlling the size of the \(_{1}\)-ball and thus the sparsity level, is typically treated as a tuning parameter. For this reason, algorithms for the lasso usually provide multiple models over a grid of varying \(\), which can then be compared (Friedman et al., 2010). Towards this end, it can be computationally efficient to compute the models pathwise by sequentially warm-starting the optimizer. As Friedman et al. (2007) point out, pathwise computation for many \(\) can be as fast as for a single \(\). For the contextual lasso, warm starts also reduce run time compared with initializing at random weights. More importantly, however, pathwise optimization improves the training quality. This last advantage is a consequence of the network's nonconvex optimization surface. Building up a sophisticated network from a simple one helps the optimizer navigate this surface. Appendix D presents our pathwise algorithm and an approach for setting the \(\) grid.

### Relaxed fit

A possible drawback to the contextual lasso, and indeed all lasso estimators, is bias of the linear model coefficients towards zero. This bias, which is a consequence of shrinkage from the \(_{1}\)-norm, can help or hinder depending on the data. Typically, bias is beneficial when the number of observations is low or the level of noise is high, while the opposite is true in the converse situation (see, e.g., Hastie et al., 2020). This consideration motivates a relaxation of the contextual lasso that unwinds some, or all, of the bias imparted by the \(_{1}\)-norm. We describe an approach in Appendix E that extends the proposal of Hastie et al. (2020) for relaxing the lasso. Their relaxation, which simplifies an earlier proposal by Meinshausen (2007), involves a convex combination of the lasso's coefficients and "polished" coefficients from an unregularized least squares fit on the lasso's selected features. We extend this idea from the lasso's fixed coefficients to the contextual lasso's varying coefficients. The benefits of the relaxation are demonstrated empirically in Appendix E, where we present an ablation study.

### Computational complexity

A forward or backward pass through the vanilla feedforward component of the network takes \(O(md+hd^{2}+pd)\) time, where \(h\) is the number of hidden layers, \(d\) is the number of nodes per layer, \(m\) is the number of contextual features, and \(p\) is the number of explanatory features. A forward or backward pass through the projection algorithm takes \(O(p)\) time (Duchi et al., 2008). The time complexity for a pass through the full network over \(n\) observations is thus \(O(nd(m+hd+p))\). This result suggests that the training time is linear in the sample size \(n\) and number of features \(m\) and \(p\). Actual training times are reported in Appendix F that demonstrate linear complexity empirically.

### Package

We implement the contextual lasso as described in this section in the Julia(Bezanson et al., 2017) package ContextualLasso. For training the neural network, we use the deep learning library Flux(Innes et al., 2018). Though the experiments throughout this paper involve square or logistic loss functions, our package supports _any_ differentiable loss function, e.g., those in the family of generalized linear models (Nelder and Wedderburn, 1972). ContextualLasso is available at

[https://github.com/ryan-thompson/ContextualLasso.jl](https://github.com/ryan-thompson/ContextualLasso.jl).

## 3 Related work

The contextual explanation networks in Al-Shedivat et al. (2020) are cousins of the contextual lasso. These neural networks input contextual features and output interpretable models for explanatory features. They include the (nonsparse) contextual linear model, a special case of the contextual lasso where \(=\). In their terminology, the contextual linear model is a "linear explanation" model with a "deterministic encoding" function. They also explore a "constrained deterministic encoding" function that involves a weighted combination of individual fixed linear models with weights determined by the contextual features. To avoid overfitting, they apply \(_{1}\)-regularization to the individual models. However, they have no mechanism that encourages the network to combine these sparse models such that the result is sparse. In contrast, the contextual lasso directly regularizes the sparsity of its outputs.

The contextual lasso is also related to several estimators that allow varying sparsity patterns. Yamada et al. (2017) devise the first of these--the localized lasso--which fits a linear model with a different coefficient vector for each observation. The coefficients are sparsified using a lasso regularizer that relies on the availability of graph information to link the observations. Yang et al. (2022) and Yoshikawa and Iwata (2022) follow with LLSPIN and NGSLL, neural networks that produce linear models with varying sparsity patterns via gating mechanisms. These approaches are distinct from our own, however. First, they do not dichotomize into \(\) and \(\), making the resulting model \(^{}()\) difficult to interpret. Second, the sparsity level (NGSSL) or nonzero coefficients (LLSPIN) are fixed across observations, making them unsuitable for the contextual setting where both may vary.

Hallac et al. (2015) introduce the network lasso which has a different coefficient vector per observation, clustered using a lasso-style regularizer. They consider problems similar to ours, for which contextual information is available, but do not impose sparsity on the coefficients. Deleu and Bengio (2021) induce structured sparsity over neural network weights to obtain smaller, pruned networks that admit efficient computation. In our work, we leave the weights as dense and instead induce sparsity over the network's output for interpretability. Wang et al. (2020) propose a network quantization scheme with activation functions that output zeros and ones. Though our approach involves an activation that outputs zeros, we also allow a continuous output. Moreover, their end goal differs from ours; whereas they pursue sparsity to reduce computational complexity, we pursue sparsity for interpretability.

Our work also advances the broader literature at the intersection of feature sparsity and neural networks, an area that has gained momentum over the last few years. See, e.g., the lassonet of LEMhadri et al. (2021, 2021) which selects features in a residual neural network using an \(_{1}\)-regularizer on the skip connection. This regularizer is combined with constraints that force a feature's weights on the first hidden layer to zero whenever its skip connection is zero. See also Scardapane et al. (2017) and Feng and Simon (2019) for earlier ideas based on the group lasso, and Chen et al. (2021) for another approach. Though related, these methods differ from the contextual lasso in that they involve uninterpretable neural networks with fixed sparsity patterns. The underlying optimization problems also differ--whereas these methods regularize the network's weights, ours regularizes its outputs.

## 4 Experiments

The contextual lasso is evaluated here via experimentation on synthetic and real data. As benchmark methods, we consider the (nonsparse) contextual linear model, which uses no projection layer, and a deep neural network, which receives all explanatory and contextual features as inputs.5 We further include the lasso, lassonet, and LLSPIN, which also receive all features. The localized lasso does not scale to the experiments that follow, so we instead compare it with the contextual lasso on smaller experiments in Appendix G. Appendix H provides the implementation details of all methods.

### Synthetic data

We consider three different settings of increasing complexity: (1) \(p=10\) and \(m=2\), (2) \(p=50\) and \(m=2\), and (3) \(p=50\) and \(m=5\). Within each setting, the sample size ranges from \(n=10^{2}\) to \(n=10^{5}\). The full simulation design is detailed in Appendix I. As a prediction metric, we report the square or logistic loss relative to the intercept-only model. As an interpretability metric, we report the proportion of nonzero features. As a selection metric, we report the F1-score of the selected features; a value of one indicates all true positives recovered and no false positives.6 All three metrics are evaluated on a testing set with tuning on a validation set, both constructed by drawing \(n\) observations independently of the training set. Figure 3 reports the results for regression (i.e., continuous response).

The contextual lasso performs comparably with most of its competitors when the sample size is small. On the other hand, the contextual linear model (the contextual lasso's unregularized counterpart) can perform poorly here (its relative loss had to be omitted from some plots to maintain the aspect ratio). As \(n\) increases, the contextual lasso begins to outperform other methods in prediction, interpretability, and selection. Eventually, it learns the correct map from contextual features to relevant explanatory features, recovering only the true nonzeros. Though its unregularized counterpart performs nearly as well in terms of prediction for large \(n\), it remains much less interpretable, using all explanatory features. In contrast, the contextual lasso uses just 10% of the explanatory features on average.

The deep neural network's performance is underwhelming for most \(n\). Only for large sample sizes does it begin to approach the prediction performance of the contextual lasso. The lassonet often performs somewhere between the two. These three methods should predict equally well for large enough \(n\), though the function learned by the deep neural network and lassonet will remain opaque. The lasso makes some gains with increasing sample size, but lacks the expressive power of the contextual lasso needed to adapt to the complex sparsity pattern of the true model. LLSPIN--the only other method to allow for varying sparsity patterns--is the second best feature selector for \(p=10\), though its mediocre performance more generally is likely due to it not exploiting the explanatory-contextual feature dichotomy and not allowing its nonzero coefficients to change.

### Energy consumption data

We consider a real dataset containing energy readings for a home in Mons, Belgium (Candanedo et al., 2017). Besides this continuous response feature, the dataset also contains \(p=25\) explanatory features in the form of temperature and humidity readings in different rooms of the house and local weather data. We define several contextual features from the time stamp to capture seasonality: month of year, day of week, hour of day, and a weekend indicator. To reflect their cyclical nature, the first three contextual features are transformed using a sine and cosine, leading to \(m=7\) contextual features.

The dataset, containing \(n=19,375\) observations, is randomly split into training, validation, and testing sets in 0.6-0.2-0.2 proportions. We repeat this random split 10 times, each time recording performance on the testing set, and report the aggregate results in Table 1. As performance metrics, we consider the relative loss and average sparsity level (i.e., average number of selected explanatory features).

Among all methods, the contextual lasso leads to the lowest test loss, outperforming even the deep neural network and lassonet.7 Importantly, this excellent prediction performance is achieved while maintaining a high level of interpretability. In contrast to most other methods, which use all (or nearly all) available explanatory features, the predictions from the contextual lasso arise from linear models containing just 2.8 explanatory features on average! These linear models are also much simpler than those from the lasso, which typically involve more than four times as many features.

    & Relative loss & Avg. sparsity \\  Deep neural network & \(0.433 0.004\) & \(25.0 0.0\) \\ Contextual linear model & \(0.387 0.003\) & \(25.0 0.0\) \\ Lasso & \(0.690 0.002\) & \(11.6 0.4\) \\ Lassonet & \(0.423 0.003\) & \(25.0 0.0\) \\ LLSPIN & \(0.639 0.005\) & \(24.5 0.1\) \\ Contextual lasso & \(0.356 0.003\) & \(2.8 0.4\) \\   

Table 1: Comparisons on the energy consumption data. Metrics are aggregated over 10 random splits of the data. Averages and standard errors are reported.

Figure 3: Comparisons on synthetic regression data. Metrics are aggregated over 10 synthetic datasets. Solid points are averages and error bars are standard errors. Dashed horizontal lines in the middle row indicate the true sparsity level.

The good predictive performance of the contextual lasso suggests a seasonal pattern of sparsity. To investigate this phenomenon, we apply the fitted model to a randomly sampled testing set and plot the resulting sparsity levels as a function of the hour of day in Figure 4. The model is typically highly sparse in the late evening and early morning. Between 10 pm and 6 am, the median proportion of nonzero coefficients is 0%. There is likely little or no activity inside the house at these times, so sensor readings from within the house--which constitute the majority of the explanatory features--are irrelevant. The number of active explanatory features rises later in the day, peaking around lunchtime and dinnertime. Overall, a major benefit of the contextual lasso, besides its good predictions, is the ability to identify a parsimonious set of factors driving energy use at any given time of day.

### Parkinson's telemonitoring data

We illustrate the contextual lasso on _grouped explanatory features_ using data from a study on the progression of Parkinson's disease in 42 patients (Tsanas et al., 2009). The task is to predict disease progression (a continuous variable) using 16 vocal characteristics of the patients as measured at different times throughout the study. As Tsanas et al. (2009) point out, these vocal characteristics can relate nonlinearly to disease progression. To account for these effects, we compute a five-term cubic regression spline per explanatory feature (\(p=16 5=80\)). Each spline forms a single group of explanatory features (\(g=16\)). The contextual features are the age and sex of the patients (\(m=2\)).

The dataset of \(n=5,875\) observations is again partitioned into training, validation, and testing sets in the same proportions as before. As a new benchmark, we evaluate the group lasso, which is applied to splines of all explanatory and contextual features. The deep neural network and lassonet are applied to the original (nonspline) features.8 The lasso is also applied to the original features to serve as a linear benchmark. The results are reported in Table 2.

The purely linear estimator--the lasso--performs worst overall. The group lasso improves over the lasso, supporting claims of nonlinearity in the data. The contextual group lasso is, however, the star of the show. Its models predict nearly three-times better than the next best competitor (lassonet) and are sparser than those from any other method.

Setting aside predictive accuracy, a major benefit of the contextual group lasso (compared with the deep neural network and lassonet) is that it remains highly interpretable. To illustrate, we consider the fitted spline function (i.e., the spline multiplied by its coefficients from the contextual group lasso) on the detrended fluctuation analysis (DFA) feature, which characterizes turbulent noise in speech. Figure 5 plots the function at three different ages of patient. For 70-year-olds, the function is zero, indicating DFA is not yet a good predictor of Parkinson's. At 75, the function becomes nonzero,

    & Relative loss & Avg. sparsity \\  Deep neural network & \(0.367 0.015\) & \(16.0 0.0\) \\ Lasso & \(0.885 0.005\) & \(3.1 0.1\) \\ Group lasso & \(0.710 0.006\) & \(4.2 0.4\) \\ Lassonet & \(0.263 0.007\) & \(15.5 0.2\) \\ Contextual group lasso & \(0.113 0.006\) & \(1.6 0.3\) \\   

Table 2: Comparisons on the Parkinson’s telemonitoring data. Metrics are aggregated over 10 random splits of the data. Averages and standard errors are reported.

Figure 4: Explanatory feature sparsity as a function of hour of day for the estimated energy consumption model. The sparsity level varies within each hour because the other contextual features vary.

taking on a concave shape. It becomes even more concave and negative 80. The models reported in Tsanas et al. (2009) also had this coefficient negative, but fixed across all ages. In contrast, the contextual lasso identifies DFA and other features as relevant only for patients of certain ages and sex.

### Additional experiments

Experiments for classification on synthetic and real data are available in Appendix J. In Appendix K, we report high-dimensional experiments with \(p=1,000\) and fixed coefficient experiments (i.e., the lasso's home court). Appendix L investigates the stability of the contextual lasso with respect to the random initialization. Appendix M provides hyperlinks to the datasets used throughout the paper.

## 5 Discussion

Contextual sparsity is an important extension of the classical notion of feature sparsity. Rather than fix the relevant features once and for all, contextual sparsity allows feature relevance to depend on the prediction context. To tackle this intricate statistical learning problem, we devise the contextual lasso. This new estimator utilizes the expressive power of deep neural networks to learn interpretable sparse linear models with sparsity patterns that vary with the contextual features. The optimization problem that defines the contextual lasso is solvable at scale using modern deep learning frameworks. Grouped explanatory features and side constraints are readily accommodated by the contextual lasso's neural network architecture. An extensive experimental analysis of the new estimator illustrates its good prediction, interpretation, and selection properties in various settings. To the best of our knowledge, the contextual lasso is the only tool currently available for handling the contextually sparse setting.

The problem of deciding the explanatory-contextual feature split is the same as that faced with varying-coefficient models. Though the literature on varying-coefficient models is extensive, there are no definitive rules for partitioning the features in general. In the housing and energy examples, the contextual features are spatial or temporal effects, which are distinct from the remaining features. In the telemonitoring example, the patient attributes (age and sex) differ fundamentally from the vocal characteristics. Ultimately, the partition for any given application should be guided by domain expertise with consideration to the end goal. If one needs to interpret the exact effect of a feature, that feature should be an explanatory feature. If a feature's effect is of secondary interest, or it is suspected that the feature influences the structural form of the model, that feature should be a contextual feature. If the user determines there are no contextual features, the ordinary lasso is a more appropriate tool.

It remains an important avenue of future research to establish a solid theoretical foundation for the contextual lasso. The statistical properties of the lasso in terms of estimation, prediction, and selection are now well-established in theory (Bunea et al., 2007; Raskutti et al., 2011; Shen et al., 2013; Zhang et al., 2014). The synthetic experiments in our paper suggest that the contextual lasso satisfies similar properties, though theoretically establishing these results is challenging. Statistical convergence results for vanilla feedforward neural networks (e.g., Schmidt-Hieber, 2020) do not apply in our setting due to the projection layer. Moreover, to our knowledge, no statistical guarantees exist for neural networks configured with convex optimization layers that otherwise might apply here. It is also important to understand when the contextual lasso's performance is matched by a deep neural network, since both should predict well for large samples in the contextually sparse linear regime.

Figure 5: Fitted spline function from the contextual lasso for the detrended fluctuation analysis (DFA) explanatory feature. The age explanatory feature is varied while the sex feature is set to female.