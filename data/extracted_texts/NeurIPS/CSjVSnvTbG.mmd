# Bisimulation Metrics are Optimal Transport Distances, and Can be Computed Efficiently

Sergio Calo Anders Jonsson  Gergely Neu  Ludovic Schwartz  Javier Segovia-Aguas

Universitat Pompeu Fabra, Barcelona, Spain

{sergio.calo,anders.jonsson,gergely.neu,ludovic.schwartz,javier.segovia}@upf.edu

###### Abstract

We propose a new framework for formulating optimal transport distances between Markov chains. Previously known formulations studied couplings between the entire joint distribution induced by the chains, and derived solutions via a reduction to dynamic programming (DP) in an appropriately defined Markov decision process. This formulation has, however, not led to particularly efficient algorithms so far, since computing the associated DP operators requires fully solving a static optimal transport problem, and these operators need to be applied numerous times during the overall optimization process. In this work, we develop an alternative perspective by considering couplings between a "flattened" version of the joint distributions that we call discounted occupancy couplings, and show that calculating optimal transport distances in the full space of joint distributions can be equivalently formulated as solving a linear program (LP) in this reduced space. This LP formulation allows us to port several algorithmic ideas from other areas of optimal transport theory. In particular, our formulation makes it possible to introduce an appropriate notion of entropy regularization into the optimization problem, which in turn enables us to directly calculate optimal transport distances via a Sinkhorn-like method we call Sinkhorn Value Iteration (SVI). We show both theoretically and empirically that this method converges quickly to an optimal coupling, essentially at the same computational cost of running vanilla Sinkhorn in each pair of states. Along the way, we point out that our optimal transport distance exactly matches the common notion of bisimulation metrics between Markov chains, and thus our results also apply to computing such metrics, and in fact our algorithm turns out to be significantly more efficient than the best known methods developed so far for this purpose.

## 1 Introduction

Measuring distances between structured objects and sequences is an important problem in a variety of areas of science. The more structured the objects become, the harder it gets to define appropriate notions of distances, as good notions of proximity need to take into account the possibly complex relationships between the constituent parts of each object. The possibility that the objects in question may be random further complicates the picture, and in such cases it becomes more natural to measure distances between the underlying joint probability distributions. Within the specific context of comparing stochastic processes, two natural notions of distance have emerged over the past decades: the notion of _probabilistic bisimulation metrics_ that takes its root in modal logic and theoretical computer science (Sangiorgi, 2009; Abate, 2013), and the notion of _optimal-transport distances_ that originates from probability theory (Villani, 2009; Peyre and Cuturi, 2019). In this paper, we show that bisimulation metrics are in fact optimal-transport distances, and we make use of this observation to derive efficient algorithms for computing distances between stochastic processes.

The two distance notions have found strikingly different applications. Bisimulation emerged within the area of theoretical computer science as one of the most important important concepts in concurrency theory and formal verification of computer systems (Park, 1981; Milner, 1989), and has been extended to probabilistic transition systems by Larsen and Skou (1989). Within machine learning, bisimulation metrics have become especially popular in the context of reinforcement learning (RL) due to the work of Ferns et al. (2004), and have become one of the few standard tools of representation learning (Jiang, 2018, 2024). In particular, the work of Ferns et al. (2004) advocates for using bisimilarity as a basis for state aggregation, measuring similarities of states in terms of similarities of two chains \(M_{}\) and \(M_{}\) that only differ in their initial state. While this approach has inspired numerous follow-up works (Gelada et al., 2019; Castro, 2020; Agarwal et al., 2021; Zhang et al., 2021; Hansen-Estruch et al., 2022; Castro et al., 2022), ultimately this line of work has failed to discover efficient algorithms for computing bisimulation metrics and has largely resorted to heuristics for computing similarity metrics.

On the other hand, optimal transport (OT) has found numerous applications in areas as diverse as economics (Galichon, 2016), signal processing (Kolouri et al., 2017), or genomics (Schiebinger et al., 2019). Within machine learning, it has been used for the similarly diverse areas of domain adaptation (Courty et al., 2016), generative modeling (Arjovsky et al., 2017; Song et al., 2020; Shi et al., 2024), representation learning (Courty et al., 2018), and, perhaps most relevant to our work, as a way of measuring distances between graphs (Titouan et al., 2019; Chen et al., 2022; Chuang and Jegelka, 2022). The recent works of Yi et al. (2021); Brugere et al. (2024) propose to define graph distances via studying the behavior of random walks defined on the graph, thus reducing the problem of comparing graphs to comparing stochastic processes--exactly the subject of the present paper. Other applications of OT between stochastic processes include generative modeling for sequential data (Xu et al., 2020), pricing and hedging in mathematical finance (Backhoff-Veraguas et al., 2017), and analyzing multistage stochastic optimization problems (Pflug, 2010; Bartl and Wiesel, 2022). It appears however that the literature on optimal transport for stochastic processes has apparently not yet discovered connections with bisimulation metrics and the rich intellectual history behind it. Also, applications of optimal transport for representation learning within the context of reinforcement learning appear to be nonexistent.

In this paper we observe that, despite their apparent differences, bisimulation metrics and optimal transport distances are one and the same. Furthermore, we provide a new perspective on both OT distances and bisimulation metrics by formulating the distance metric as the solution of a linear program (LP) in the space of "occupancy couplings", a finite-dimensional projection of the infinite-dimensional process laws. Building on tools from computational optimal transport (Peyre and Cuturi, 2019) and entropy-regularized Markov decision processes (Neu et al., 2017; Geist et al., 2019), we design an algorithm that effectively combines Sinkhorn's algorithm (Sinkhorn and Knopp, 1967; Cuturi, 2013) with an entropy-regularized version of the classic Value Iteration algorithm (Bellman, 1957; Neu et al., 2017). Building on recent work on computational optimal transport (Altschuler et al., 2017; Ballu and Berthet, 2023), we provide theoretical guarantees for the resulting algorithm (called Sinkhorn Value Iteration) and perform numerical studies that demonstrate its effectiveness for computing distances between Markov chains.

Notations.For a finite set \(\), we use \(_{}\) to denote the set of all probability distributions over \(\). We will denote infinite sequences by \(=(x_{0},x_{1},)\) and the corresponding subsequences as \(_{n}=(x_{0},x_{1},,x_{n})\). For two sets \(\) and \(\), we will often write \(\) to abbreviate the direct-product notation \(\), and for two indices \(x\) and \(y\) and a function \(f:\), we will often write \(f(xy)\) instead of \(f(x,y)\) to save space. Also, we will denote scalar products by \(,\) and use \(_{p}\) to denote the \(_{p}\)-norm.

## 2 Preliminaries

We study the problem of measuring distances between pairs of finite Markov chains. Specifically, we consider two stationary Markov processes \(M_{}=(,P_{},_{0,})\) and \(M_{}=(,P_{},_{0,})\), where

* \(\) and \(\) are the finite state spaces with cardinalities \(m=||\) and \(n=|Y|\),
* \(P_{}:_{}\) and \(P_{}:_{}\) are the transition kernels that determine the evolution of the states as \(P_{}(x^{}|x)=[\,X_{t+1}\!=\!x^{}|\,X_{t} \!=\!x]\) and \(P_{}(y^{}|y)=[\,Y_{t+1}\!=\!y^{}|\,Y_{t} \!=\!y]\) for all \(t\), and
* \(_{0,}\) and \(_{0,}\) are the initial-state distributions with \(X_{0}_{0,}\) and \(Y_{0}_{0,}\).

Without significant loss of generality, we will suppose that the initial states are fixed almost surely as \(X_{0}=x_{0}\) and \(Y_{0}=y_{0}\), and refer to their corresponding joint distribution as \(_{0}=_{x_{0},y_{0}}\). These objects together define a sequence of joint distributions \([(X_{0},X_{1},,X_{n})=(x_{0},x_{1},,x_{n})]\) and \([(Y_{0},Y_{1},,Y_{n})=(y_{0},y_{1},,y_{n})]\) for each \(n\), which together define respective the laws of the infinite sequences \(=(X_{1},X_{2},)\) and \(=(Y_{1},Y_{2},)\) via Kolmogorov's extension theorem. With a slight abuse of notation, we will use \(M_{}\) and \(M_{}\) to denote the corresponding measures that satisfy \(M_{}(_{n})=[_{n}= {x}_{n}]\) and \(M_{}(_{n})=[_{n}= {y}_{n}]\) for any \(^{}\), \(^{}\) and \(n\). The corresponding conditional distributions are denoted as \(M_{}(x_{n}|_{n-1})=[X_{n}=x_{n}| _{n-1}=_{n-1}]\) and \(M_{}(y_{n}|_{n-1})=[Y_{n}=y_{n}| _{n-1}=_{n-1}]\).

### Optimal transport between Markov chains

Our main object of interest in this work is a notion of optimal transport distance between infinite-horizon Markov chains. Several previous works have studied such distances (which are discussed in detail in Appendix A), and our precise definition we give below is closest to Moulos (2021); O'Connor et al. (2022) and Brugere et al. (2024). We consider Markov chains on state spaces where a "ground metric" (or "ground cost") \(c:^{+}\) is available to measure distances between any two individual states \(x\) and \(y\), with the distance denoted as \(c(x,y)\). For any two sequences \(=(x_{1},x_{2},)\) and \(=(y_{1},y_{2},)\), we define the discounted total cost

\[c_{}(,)=_{t=0}^{}^{t}c(x_{t},y _{t}),\] (1)

where \((0,1)\) is the _discount factor_ that expresses the preference that two sequences be considered further apart if they exhibit differences at earlier times in terms of the ground cost \(c\). Following the optimal-transport literature, we will consider distances between the stochastic processes \(M_{}\) and \(M_{}\) via the notion of _couplings_. To this end, we define a coupling of \(M_{}\) and \(M_{}\) as a stochastic process evolving on the joint space \(\), with its law defined for all \(n\) as \(M_{}(_{n}_{n})= _{n}=_{n},_{n}=_{n}\), required to satisfy \(_{_{n}^{n}}M_{}( {x}_{n}_{n})=M_{}(_{n})\) and \(_{_{n}^{n}}M_{}( {x}_{n}_{n})=M_{}(_{n})\). We will define the set of all such couplings as \(\).

The notion of couplings defined above does not respect the temporal structure of the Markov chains \(M_{}\) and \(M_{}\) appropriately: while by definition the distribution of state \(X_{n}\) may only be causally influenced by past states \(X_{k}\) with \(k<n\), the general notion of coupling above allows the state \(X_{n}\) to be influenced by future states \(Y_{k}\) with \(k n\) as well. To rule out this possibility (and following past works mentioned in the introduction), we will introduce the notion of _bicausal couplings_. A coupling \(M_{}\) is called bicausal if and only if it satisfies

\[_{y_{n}}M_{}(x_{n}y_{n}|_{n-1}_{n-1})=M_{}(x_{n}|_{n-1})\ \ \ \ _{x_{n}}M_{}(x_{n}y_{n}|_{n-1}_{n-1})=M_{}(y_{n}|_{n-1})\]

for all sequences \(,^{}^{}\) and all \(n\). Denoting the set of all bicausal couplings by \(_{}\), we define our optimal transport distance as

\[_{}(M_{},M_{};c,x_{0},y_{0})=_{M_{ }_{}} c_{}(, )\,M_{}(,),\] (2)

where we emphasize the dependence of the distance on \(x_{0},y_{0}\) explicitly with our notation.

By noticing that the optimization problem outlined above can be reformulated as a Markov decision process (MDP), Moulos (2021) has shown that the infimum in (2) is achieved within the family of Markovian couplings that satisfy

\[_{y_{n}}M_{}(x_{n}y_{n}|_{n-1}_{n-1})=P_{}(x_{n}|x_{n-1})\ \ \ \ _{x_{n}}M_{}(x_{n}y_{n}|_{n-1}_{n-1})=P_{}(y_{n}|y_{n-1})\]

for all sequences of state pairs and all values of \(n\). Furthermore, it can be seen that Markovian couplings are fully specified in terms of _transition couplings_ of the form \(:_{}\), with \((x^{}y^{}|xy)\) standing for \([(X_{t+1},Y_{t+1})=(x^{},y^{})](X_ {t},Y_{t})=(x_{t},y_{t})\)] under the law induced by the coupling. We say that a transition coupling is _valid_ if it satisfies the marginal constraints \(_{y^{}}(x^{}y^{}|xy)=P_{}(x^{}|x)\) and \(_{x^{}}(x^{}y^{}|xy)=P_{}(y^{}|y)\). Defining the set of such valid transition couplings in state pair \(xy\) by \(P_{}(y^{}|y)\), Moulos (2021) introduces an MDP \(\) with an infinite action set corresponding to picking the joint next-state couplings in \(_{xy}\). An optimal transition coupling can be found by solving the following Bellman optimality equations of the MDP \(\):

\[V^{*}(xy)=c(xy)+_{p_{xy}}_{x^{}y^{}}p(x^{ }y^{})V^{*}(x^{}y^{}).\] (3)

The infimum on the right-hand side is achieved by an optimal transition coupling \(^{*}(|xy)=*{arg\,min}_{p_{xy}}_{x^{}y^{ }}p(x^{}y^{})V^{*}(x^{}y^{})\). The solution \(V^{*}\) is unique and can be shown to satisfy \(V^{*}(xy)=_{}(M_{},M_{};c,x,y)\) for all \(xy\). For completeness, we include the precise definition of the MDP \(\) and the proofs of these results in Appendix B.

### Bisimulation metrics

The notion of _bisimulation metrics_ has been introduced by Desharnais et al. (1999, 2004) and van Breugel and Worrell (2001), with the purpose of defining distances between Markov chains, using a methodology rooted in modal logic that at first may appear entirely different from the optimal-transport framework described above. We only give a very high-level overview of the classic logic-based characterization here (as the fine details are irrelevant to the final conclusion that this section is headed to), and refer the reader to the additional discussion in Appendix A for further reading. Desharnais et al. (1999, 2004) considered _labeled_ Markov chains where a labeling function \(r:\) assigns labels to each state, and defined bisimulation metrics via

\[d_{}(M_{},M_{};r,x_{0},y_{0})=_{f_{}}|f_{M_{}}(x_{0})-f_{M_{}}(y_{0})|\,,\] (4)

where \(_{}\) is a family of functional expressions generated by a certain grammar, and \(f_{M_{}}:\) and \(f_{M_{}}:\) are the respective "interpretations" of each \(f\) on the Markov chains \(M_{}\) and \(M_{}\). Building on this formulation, van Breugel and Worrell (2001) have shown that the distance metric can be equivalently characterized by the solution of a fixed-point equation, whose expression was subsequently used by Ferns et al. (2004) to define bisimulation metrics for Markov _decision_ processes where \(r\) takes the role of a reward function, and the evolution of states may be influenced by actions. The case of having no actions available corresponds to our setting, where their definition of a bisimulation metric simplifies to the solution of the fixed-point equation

\[U^{*}(xy)=(1-)|r(x)-r(y)|+_{p_{xy}}_{x^{}y^{ }}p(x^{}y^{})U^{*}(x^{}y^{}).\] (5)

The solution to this system is unique and satisfies \(U^{*}(xy)=d_{}(M_{},M_{};r,x,y)\). Putting this result side-by-side with Equation (3), one can immediately realize that _bisimulation metrics and our notion of optimal transport distances coincide when picking the ground cost function \(c(xy)=(1-)|r(x)-r(y)|\)_. To our knowledge, this remarkable observation has not been publicly made anywhere in either the optimal-transport or the bisimulation-metric literature. This connection has several important implications, which we have already discussed at some length in Section 1. We relegate further discussion of these metrics in the light of this observation to Appendix A.

## 3 Optimal transport between Markov chains as a linear program

Optimal transport problems can typically be formulated as linear programs (LPs), since couplings can be characterized as joint distributions satisfying a set of easily-expressed linear constraints (see, e.g., Chapter 3 in Peyre and Cuturi 2019). Our problem is no exception, and in fact the original problem statement of Equation 2 can be expressed in this form: the constraints defining \(_{}\) are all linear in \(M_{}\). However, \(M_{}\) is an infinite-dimensional object and thus this formulation is not instructive for developing computationally tractable algorithms. We address this problem in this section, where we define an equivalent LP formulation that replaces the infinite-dimensional optimization variable with an appropriate low-dimensional projection. Our framework builds on the classic LP formulation of optimal control in MDPs first proposed in the 1960's (Manne, 1960; de Ghellinck, 1960; d'Epenoux, 1963; Denardo, 1970), and covered thoroughly in several standard textbooks (e.g., Section 6.9 of Puterman 1994).

In order to set things up, we need to start with some important definitions. We say that a transition coupling \(:_{}\) generates a trajectory \((X_{0},Y_{0},X_{1},Y_{1},)\) if \((X_{0},Y_{0})_{0}\) and the subsequent state-pairs are drawn independently from the transition coupling as \((X_{t+1},Y_{t+1})(|X_{t},Y_{t})\) for all \(t\). Then, we define the _occupancy coupling_\(^{}\) associated with this process as a distribution over \(\), with each of its entries \(xy,x^{}y^{}\) defined as

\[^{}(xy,x^{}y^{})=(1-)_{}[_{t=0}^ {}^{t}_{\{(X_{t},Y_{t})=(x,y),(X_{t+1},Y_{t+1})=(x^{ },y^{})\}}],\]

where \(_{}[]\) emphasizes that the trajectory of state-pairs has been generated by \(\). In words, \(^{}(xy,x^{}y^{})\) is the discounted number of times that the quadruple \((xy,x^{}y^{})\) is visited by the process. With this definition, it is easy to notice that the objective optimized in Equation (2) can be rewritten as a linear function of \(^{}\). Indeed, suppose that \(M_{}\) is the law of the process generated by \(\) as described above, so that we can write

\[ c_{}(,)\,M_{ }(,)=_{}[_ {t=0}^{}^{t}c(X_{t},Y_{t})]=_{}[_{t=0} ^{}^{t}_{xy}_{\{(X_{t},Y_{t})=(x,y)\}}c(xy)]\] \[=_{xy}_{}[_{t=0}^{}^{ t}_{\{(X_{t},Y_{t})=(x,y)\}}]c(xy)=_{ xy,x^{}y^{}}^{}(xy,x^{}y^{})c(xy)=,c }{1-}.\]

We say that an occupancy coupling \(\) is _valid_ if it is generated by a valid transition coupling. It is easy to verify that every valid occupancy coupling \(_{}\) satisfies the following three constraints:

\[_{x^{}y^{}}(xy,x^{}y^{}) =_{x^{}y^{}}(x^{}y^{},xy)+(1- )_{0}(xy)( xy),\] (6) \[_{y^{}}(xy,x^{}y^{}) =_{x^{},y^{}}(xy,x^{}y ^{})P_{}(x^{}|x)( x,x^{} ),\] (7) \[_{x^{}}(xy,x^{}y^{}) =_{x^{},y^{}}(xy,x^{}y ^{})P_{}(y^{}|y)( y,y^{} ).\] (8)

We refer to the first equality constraint as the _flow constraint_ and the second and third ones as the _transition coherence constraints_ for \(M_{}\) and \(M_{}\), respectively1. We show in the following lemma that the above conditions uniquely characterize the set of valid occupancy couplings.

**Lemma 1**.: _A distribution \(_{}\) is a valid occupancy coupling associated with some transition coupling \(:_{}\) if and only if it satisfies Equations (6)-(8)._

One important consequence of this result is that for each transition coupling, one can associate a transition coupling \(_{}\) with \(^{_{}}=\), with entries satisfying \(_{x^{}y^{}}(xy,x^{}y^{}) _{}(x^{}y^{}|xy)=(xy,x^{}y^{})\). A proof is provided in Appendix B.2. Having established in the previous section that stationary occupancy couplings are sufficient to achieve the supremum in the definition of the OT distance of Equation (2), this result immediately implies the following.

**Theorem 1**.: _Let \(\), \(_{}\) and \(_{}\) respectively stand for the sets of all distributions \(_{}\) that satisfy equations (6), (7), and (8). Then,_

\[_{}(M_{},M_{};c,x_{0},y_{0})=_{_{}_{}},c.\]

Since the constraints on \(\) in the above reformulation are all linear, the optimization problem stated above is obviously a linear program. Notably, the resulting LP is _not_ the standard dual to the LP associated with the MDP formulation introduced in Section 2.1, which would result in an infinite-dimensional LP with one constraint associated with each of the continuously-valued actions. Such an infinite-dimensional reformulation was previously considered by Chen et al.  in the context of computing bisimulation metrics, who used it as a tool for analysis rather than algorithm design. Notably, our formulation results in a finite-dimensional LP with \(||^{2}||^{2}\) variables and \(||^{2}+||||+ ||^{2}\) constraints. Building on the celebrated result of Ye  (and similarly to the work of Chen et al., 2012 mentioned above), one can show that our LP can be solved in strongly polynomial time via an appropriate adaptation of the simplex method or the classic policy iteration method of Howard . We propose an alternative methodology in the next section.

## 4 Sinkhorn Value Iteration

Solving optimal-transport problems via standard LP solvers (like variations of the simplex method or interior-point methods) is known to be empirically hard, and thus we seek alternatives to this approach towards optimizing our own LP defined in the previous section. In computational optimal transport, a paradigm shift was initiated by Cuturi (2013) who successfully applied entropic regularization to the classic LP objective of optimal transport, and solved the resulting optimization method through an iterative algorithm called the Sinkhorn-Knopp method (sometimes simply called Sinkhorn's algorithm, due to Sinkhorn and Knopp, 1967). This method is based on finding a feasible point in a two-constraint problem by alternately satisfying one and the other, and has resulted in practical algorithms that were orders of magnitude faster than all previously studied methods. Entropy regularization and Sinkhorn's algorithm have thus became the most important cornerstones of computational optimal transport. Drawing on the same principles as well as the theory of entropy-regularized Markov decision processes (Neu et al., 2017; Geist et al., 2019), we develop a computationally effective algorithm for computing optimal transport distances between Markov chains below.

### Formal definition: Mirror Sinkhorn on the space of occupancy couplings

Our method is an adaptation of a version of Sinkhorn's algorithm called Mirror Sinkhorn, first proposed and analyzed by Ballu and Berthet (2023). This method combines mirror-descent-style updates (Nemirovski and Yudin, 1983; Beck and Teboulle, 2003) with alternating projections to two convex sets whose intersection corresponds to the feasible set we seek to optimize over. In our adaptation, we choose the two sets as

\[_{}=\{:\ _{y^{}}(xy,x^{}y^{ })=(_{x^{}y^{}}(x^{ }y^{},xy)+(1-)_{0}(xy))P_{}(x^{ }|x)( xy,x^{})\},\]

that can be seen to be the set of distributions \(\) that satisfy both Equations (6) and (7), and

\[_{}=\{:\ _{x^{}}(xy,x^{}y^{ })=(_{x^{}y^{}}(x^{ }y^{},xy)+(1-)_{0}(xy))P_{}(y^{ }|y)( xy,y^{})\}\]

which is the set of distributions \(\) that satisfy both Equations (6) and (8). Naturally, the intersection of the two sets corresponds to valid occupancy couplings. It remains to define an appropriate notion of entropy for the purpose of regularization. Following Neu et al. (2017), we will use the _conditional relative entropy_ defined between two joint distributions \(,^{}_{}\) as

\[(\|^{}) =_{xy,x^{}y^{}}(xy,x^{}y^{}) y^{})/_{x^{}y^{}} (xy,x^{}y^{})}{^{}(xy,x^{}y^{}) /_{x^{}y^{}}^{}(xy,x^{}y^{ })}\] \[=_{xy,x^{}y^{}}(xy,x^{}y^{}) (x^{}y^{}|xy)}{_{^{}}(x^{}y^{ }|xy)}.\]

It is an easy exercise to show that \(\) is a Bregman divergence that is convex in its first argument (see, e.g., Appendix A.1 of Neu et al., 2017). Note however that \((\|^{})\) can be zero even if \(^{}\) and thus it is not strongly convex in \(\).

With these ingredients, we symbolically define our algorithm as calculating the sequence of updates

\[_{k+1}=*{}_{_{k}}\{ ,c+(\|_{k})\}\] (9)

for each \(k=1,,K-1\), where \(_{1}\) is the occupancy coupling associated to the trivial coupling \(_{1}(|xy)=P_{}(|x) P_{}(|y)\) for each state pair \(xy\), \(>0\) is a stepsize (or learning-rate) parameter and \(_{k}\) is chosen to be \(_{}\) in odd rounds and \(_{}\) in even rounds. By adapting tools from the theory of entropy-regularized Markov decision processes, the updates can be computed in closed form by solving a system of equations closely resembling the regularized Bellman equations. In particular, we define the _Bellman-Sinkhorn operators_ for a given transition coupling \(\) as the operators\(^{}_{}:^{ X} ^{ X}\) and \(^{}_{}:^{ X} ^{}\) acting on functions \(V_{}^{ X}\) and \(V_{}^{}\) respectively as

\[(^{}_{}V_{})(xy,x^{})=- _{y^{}}y^{}|xy)}{P_{ }(x^{}|x)}(-(c(xy)+_{x^{ }}P_{}(x^{}|x^{})V_{}(x^{ }y^{},x^{}))),\]

and

\[(^{}_{}V_{})(xy,y^{})=- _{x^{}}y^{}|xy)}{P_{ }(y^{}|y)}(-(c(xy)+_{y^{ }}P_{}(y^{}|y^{})V_{}(x^{ }y^{},y^{}))).\]

Then, for odd rounds, the updates can be calculated by solving the fixed-point equations \(V_{k}=^{_{k}}_{}V_{k}\), defining the shorthand \(Q_{k}(xy,x^{}y^{})=c(xy)+_{x^{}}P_{}(x^{}|x^{})V_{k}(x^{}y^{},x^{ })\), and subsequently updating the transition coupling \(_{k}\) multiplicatively as

\[_{k+1}(x^{}y^{}|xy)=(x^{}y^{}|xy) (- Q_{k}(xy,x^{}y^{}))}{_{y^{}} _{k}(x^{}y^{}|xy)(- Q_{k}(xy,x^{}y^{ }))}P_{}(x^{}|x).\] (10)

It is easy to verify that this transition coupling satisfies \(_{y^{}}_{k+1}(x^{}y^{}|xy)=P(x^{}|x)\). The updates for even rounds are computed analogously with the roles of \(\) and \(\) swapped. We respectively refer to the fixed-point equations \(V_{k}=^{_{k}}_{}V_{k}\) and \(V_{k}=^{_{k}}_{}V_{k}\) as the _Bellman-Sinkhorn equations_ for \(M_{}\) and \(M_{}\), and the functions \(V_{k}\) and \(Q_{k}\) as _value functions_. The following proposition (proved in Appendix E.1) formally establishes the equivalence between the two update rules.

**Proposition 1**.: _Let \(_{k+1}\) and \(_{k+1}\) be specified for each \(k\) as in Equations (9) and (10), respectively. Then, \(_{k+1}=^{_{k+1}}\) holds for all \(k\)._

### Practical implementation

The algorithm described above can be seen as performing online Mirror Sinkhorn updates in each state pair \(xy\) with a sequence of cost functions \(Q_{k}\), which are computed via solving the Bellman-Sinkhorn equations. Since \(^{}_{}\) and \(^{}_{}\) are easily seen to be contractive with respect to the supremum norm with contraction factor \(\) (as shown by a standard calculation included in Appendix E.3), these equations can be solved by an adaptation of the classic Value Iteration method of Bellman (1957). Concretely, we repeatedly apply the Bellman-Sinkhorn operators until the fixed point is reached up to sufficient precision (controlled by the number of update steps \(m\)). We call the resulting method _Sinkhorn Value Iteration_ (SVI), and provide its pseudocode as Algorithm 1.

Notably, while SVI is defined in its abstract form as a sequence of updates in the space of occupancy couplings \(_{k}\), its implementation only works with transition couplings \(_{k}\). The final output of SVI is a transition coupling \(_{}\), obtained by computing the average \(_{K}=_{k=1}^{K}^{_{k}}\) of all occupancy couplings, computing \(_{_{K}}\) and then rounding the result to a valid transition coupling. In particular we apply a simple rounding procedure due to Altschuler et al. (2017) individually on \(_{_{K}}(|xy)\) for each state-pair \(xy\)--for the full details, see Appendix E.2. Besides \(_{}\), SVI also outputs an estimate of \(V^{*}\) in the form of the _value function_\(V^{_{}}\), as defined in Equation (11) in Appendix B.1. This function can be computed efficiently by solving the linear system of Bellman equations \(V^{_{}}(xy)=c(xy)+_{x^{}y^{}}_{} (x^{}y^{}|xy)V^{_{}}(x^{}y^{})\).

A number of small simplifying steps can be made to make the algorithm easier to implement. First, instead of obtaining \(_{_{K}}\) via the computationally expensive procedure described above, one can simply run the rounding procedure on the final transition coupling \(_{K}\) and output the result. Second,while theoretical analysis suggests setting \(m=\) in order to make sure that all projection steps are perfect, such exact computation may be unnecessary and inefficient in practice, and thus (much) smaller values can be used instead. Third, for small values of \(\) the softmax function used in the definition of the Bellman-Sinkhorn operator can be accurately approximated by an average with respect to \(_{k}(x^{}y^{}|xy)/P_{}(x^{}|x)\), which suggests a simple alternative to the projection steps. This approximates SWI similarly as to how the Mirror Descent Modified Policy Iteration method of Geist et al. (2019) approximates the mirror descent method of Neu et al. (2017) (see also Azar et al., 2012). The resulting method (that we refer to as _Sinkhorn Policy Iteration_, or SPI) is presented in detail along with its theoretical analysis in Appendix D. We study effects of these implementation choices via a sequence of experiments in Section 5.

### Convergence guarantees

The following theorem establishes a guarantee on the number of iterations necessary for \(V^{_{}}(xy)\) be an \(\)-accurate approximation of the transport cost \(_{}(M_{},M_{};c,x,y)\) for any \(x,y\).

**Theorem 2**.: _Suppose that Sinkhorn Value Iteration is run for \(K\) steps with regularization parameter \(=}| |||}{K}}\), and initialized with the uniform coupling defined for each \(xy,x^{}y^{}\) as \(_{1}(x^{}y^{}|xy)=|||}\). Then, for any \(x_{0}y_{0}\), the output satisfies \(V^{_{}}(x_{0}y_{0})_{}(M_{},M_{ };c,x_{0},y_{0})+\) if the number of iterations is at least_

\[K^{2}||||}{(1- )^{5}^{2}}.\]

The proof is relegated to Section C, and we present a similar performance guarantee for SPI in Appendix D. Importantly, these guarantees technically only hold when setting \(m=\), which is a limitation we discuss in more detail in Section 6. The condition that \(_{1}\) is chosen as the uniform coupling is not necessary and simply made to make the statement easier to state. A more detailed statement of the bound is provided in Appendix C.4.

## 5 Experiments

We have conducted a range of experiments on some simple environments with the purpose of illustrating the numerical properties of our algorithms and some aspects of the distance metrics we studied. Due to space restrictions, we only report a very limited subsample of the results below, and refer the reader to Appendix F for the complete suite2.

One set of experiments we report here addresses the biggest open question left behind our theory: the effect of the choice of \(m\) on the quality of the updates. For this experiment, we use the classic "4-rooms" environment first studied by Sutton et al. (1999), and run both SVI (Algorithm 1) and SPI (Algorithm 2) for a range of different choices of \(m\), and a fixed \(=0.95\). The results of this study are shown in Figure 1. The plots indicate that the estimates produced by both algorithms converge towards the true distance at a rate that is basically unaffected by \(m\), and in particular even a value of \(m=1\) remains competitive. This observation is consistent across all of our experiments. Also, the output of SPI appears to converge slightly more slowly towards the optimum in this experiment, but this observation is not entirely consistent and can be likely ascribed to the fact that the learning rate was not optimized to favor either algorithm in this experiment. In most experiments, the two algorithms performed very similarly, up to some small occasional differences.

We have also conducted a number of experiments to illustrate the potential of optimal-transport distances for comparing Markov chains of different sizes and transition functions. In the experiment we show here, we compare two Markov chains illustrated in Figure 2. The first Markov chain \(M_{}\) is a simple, nine-state "gridworld" environment, which has its initial state in the upper left corner (denoted as \(s_{0}\)) and a reward of \(+1\) in the lower left corner (shown in blue). The second, \(M_{}\), is an instance of the 4-rooms environment, where each room is a rotation of the aforementioned small grid. The transition kernels in both environments are uniform distributions over the adjacent cells in the four principal directions. The plot shows the distances between the two chains as a function of the initial state of \(M_{}\), revealing an intuitive pattern of similarities that captures the symmetries of \(M_{}\).

## 6 Discussion

We discuss some further aspects of our framework and results below.

Representation learning for reinforcement learning.Among the numerous applications listed in Section 1 and Appendix A, the most interesting for us is using our metrics for representation learning in RL. As mentioned earlier, bisimulation metrics have been extensively used for this purpose in the past. In particular, almost all such work uses bisimulation metrics to compare states within the same MDP and use the resulting similarity metrics for merging states that are at low distance (an approach called "state aggregation"). As our results highlight, this is a rather narrow view of what bisimulation metrics are capable of: they can define similarity metrics between processes that live on potentially different state spaces, which in particular can be used to select representations by minimizing the distance between a high-dimensional process and a set of low-dimensional representations. Curiously, our LP formulation may allow differentiating the distances with respect to the transition kernels, which we believe will be an important property for future developments in representation learning for RL.

Limitations of the theory.In their current form, our theoretical guarantees in Theorems 2 and 6 only apply to perfect projection and evaluation steps, corresponding to setting \(m=\). We conjecture that this limitation can be addressed with a more careful analysis, and results similar to those of Theorems 2 and 6 can be shown, potentially at the price of a worse dependence on the effective horizon \(1/(1-)\)(Scherrer et al., 2012, 2015), by making use of the techniques of Geist et al. (2019) and Moulin and Neu (2023) for analyzing regularized dynamic-programming algorithms.

From dynamic programming to learning from data.This paper focuses on computing distances between known Markov processes via dynamic-programming-style methods. In the most interesting applications however, the transition kernels are unknown, which requires the development of new tools. We are confident that our framework can serve as a solid basis for such developments, and in particular that one can port many ideas from the field of reinforcement learning that is essentially all about turning dynamic-programming methods into algorithms that can learn from interaction data. Additionally, we believe that our LP formulation in Section 3 makes it much easier to import further ideas from computational optimal transport, and in particular that stochastic optimization methods like those of Genevay et al. (2016) can be adapted to solving our linear programs.

Figure 1: Estimated transport cost as a function \(k\), for various choices of \(m\) and \(=1\).

Figure 2: Visual representation of the distances computed between the chains \(M_{}\) and \(M_{}\).