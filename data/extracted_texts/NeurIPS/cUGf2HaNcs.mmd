# Learning Truncated Causal History Model

for Video Restoration

 Amirhosein Ghasemabadi

ECE Department, University of Alberta

ghasemab@ualberta.ca Muhammad Kamran Janjua

Huawei Technologies, Canada

kamran.janjua@huawei.com Mohammad Salameh

Huawei Technologies, Canada

mohammad.salameh@huawei.com Di Niu

ECE Department, University of Alberta

dniu@ualberta.ca

indicates equal contribution.

###### Abstract

One key challenge to video restoration is to model the transition dynamics of video frames governed by motion. In this work, we propose Turtle to learn the **Tr**U**nca**T**ed causaL history modEl for efficient and high-performing video restoration. Unlike traditional methods that process a range of contextual frames in parallel, Turtle enhances efficiency by storing and summarizing a truncated history of the input frame latent representation into an evolving historical state. This is achieved through a sophisticated similarity-based retrieval mechanism that implicitly accounts for inter-frame motion and alignment. The causal design in Turtle enables recurrence in inference through state-memorized historical features while allowing parallel training by sampling truncated video clips. We report new state-of-the-art results on a multitude of video restoration benchmark tasks, including video desnowing, nighttime video deraining, video raindrops and rain streak removal, video super-resolution, real-world and synthetic video deblurring, and blind video denoising while reducing the computational cost compared to existing best contextual methods on all these tasks.

https://kjanjua26.github.io/turtle/

## 1 Introduction

Video restoration aims to restore degraded low-quality videos. Degradation in videos occurs due to noise during the acquisition process, camera sensor faults, or external factors such as weather or motion blur . Several methods in the literature process the entire video either in parallel or with recurrence in design. In the former case, multiple contextual frames are processed simultaneously to facilitate information fusion and flow, which leads to increased memory consumption and inference cost as the context size increases . Methods with recurrence in design reuse the same network to process new frame sequentially based on previously refined ones . Such sequential processing approaches often result in cumulative errors, leading to information loss in long-range temporal dependency modeling  and limiting parallelization capabilities.

Recently, methods based on state space models (SSMs) have seen applications across several machine vision tasks, including image restoration , and video understanding . While Video-Mamba  proposes a state space model for video understanding, the learned state space does not reason at the pixel level and, hence, can suffer from information collapse in restoration tasks .

Additionally, the state evolves over time with respect to motion that affects the entire trajectory non-uniformly  at the pixel level. Therefore, it is pertinent to learn a model capable of summarizing the history1 of the input as it operates on the spatiotemporal structure of the input video.

In this work, we present "turtle", a new video restoration framework to learn the **Tr**run**c**a**T**ed causa**L history model of a video. turtle employs the proposed Causal History Model (CHM) to align and borrow information from previously processed frames, maximizing feature utilization and efficiency by leveraging the frame history to enhance restoration quality. We outline our contributions.

* turtle's encoder processes each frame individually, while its decoder, based on the proposed Causal History Model (CHM), reuses features from previously restored frames. This structure dynamically propagates features and compensates for lost or obscured information by conditioning the decoder on the frame history. CHM models the evolving state and compensates the history for motion relative to the input. Further, it learns to control the effect of history frames by scoring and aggregating motion-compensated features according to their relevance to the restoration of the current frame.
* turtle facilitates training parallelism by sampling short clips from the entire video sequence. In inference, turtle's recurrent view implicitly maintains the entire trajectory ensuring effective frame restoration.
* turtle sets new state-of-the-art results on several benchmark datasets and video restoration tasks, including video desnowing, nighttime video deraining, video raindrops and rain streak removal, video super-resolution, real and synthetic video deblurring, and achieves competitive results on the blind video denoising task.

## 2 Related Work

Video restoration is studied from several facets, mainly distributed in how the motion is estimated and compensated for in the learning procedure and how the frames are processed. Additional literature review is deferred to appendix G.

Motion Compensation in Video Restoration.Motion estimation and compensation are crucial for correcting camera and object movements in video restoration. Several methods employ optical flow to explicitly estimate motion and devise a compensation strategy as part of the learning procedure, such as deformable convolutions [33; 34], or flow refinement . However, optical flow can struggle with degraded inputs [84; 3; 20], often requiring several refinement stages to achieve precise flow estimation. On the other end, methods also rely on the implicit learning of correspondences in the latent space across the temporal resolution of the video; a few techniques include temporal shift modules , non-local search [64; 32; 85], or deformable convolutions [69; 13; 80].

Video Processing Methods.There is a similar distinction in how a video is processed, with several methods opt for either recurrence in design or restoring several frames simultaneously. Parallel methods, also known as sliding window methods, process multiple frames simultaneously. This sliding window approach can lead to inefficiencies in feature utilization and increased computational costs [63; 4; 69; 86; 58; 28; 26; 5; 34; 62; 9]. Although effective in learning joint features from the entire input context, their size and computational demands often render them unsuitable for resource-constrained devices. Conversely, recurrent methods restore frames sequentially, using multiple stages to propagate latent features [87; 81; 82]. These methods are prone to information loss . Furthermore, while typical video restoration methods in the literature often rely on context from both past and future neighboring frames [34; 33; 29], Turtle is causal in design, focuses on using only past frames. This approach allows Turtle to apply in scenarios like streaming and online video restoration, where future frames are unavailable.

## 3 Methodology

Consider a low-quality video \(^{}^{T H W C}\), where \(T\), \(H\), \(W\), \(C\) denote the temporal resolution, height, width, and number of channels, respectively, that has been degraded with some degradation \(d\). The goal of video restoration is to learn a model \(_{}\) parameterized by \(\) to restore high-quality video \(}^{}^{T sH sW C}\), where \(s\) is the scale factor (where \(s>1\) for video super-resolution). To this end, we propose Turtle, a U-Net style  architecture, to process, and restore a single frame at any given timestep conditioned on the truncated history of the given frame. Turtle's encoder focuses only on a single frame input and does not consider the broader temporal context of the video sequence. In contrast, the decoder, however, utilizes features from previously restored frames. This setup facilitates a dynamic propagation of features through time, effectively compensating for information that may be lost or obscured in the input frame. More specifically, we condition a decoder block at the different U-Net stages on the history of the frames. Given a frame at timestep \(t\), each block learns to model the causal relationship \(p(_{t}|_{t},_{t})\), where \(_{t}\) is the output of a decoder block, \(_{t}\) is the input feature map of the decoder block, and \(_{t}\) is the history of corresponding features maps from the previous frames at the same block. We train the architecture with the standard \(_{1}\) loss function: \(=_{i=1}^{N}||^{}-^{ }||_{1}\) for all the restoration tasks. We present the visual illustration of turtle's architecture in Figure 1.

### Architecture Design

Given a model \(_{}\), let \(_{t}^{[l]}\) denote the feature map of a frame at timestep \(t\), taken from \(_{}\) at layer \(l\). We, then, utilize \(_{t}^{[l]}\) to construct the causal history states denoted as \(_{t}^{[l]}^{ h^{l} w^{l} c^{l}}\), where \(\) is the truncation factor (or length of the history), \(h,w\) denote spatial resolution of the history, and \(c\) denotes the channels. More specifically, \(_{t}^{[l]}=\{_{t-}^{[l]}_{t-+1}^ {[l]}_{t-1}^{[l]}\}^{ h^{l } w^{l} c^{l}}\), where \(\) is the concatenation operation. We denote the motion-compensated history at timestep \(t\) as \(}_{t}^{[l]}\), which is compensated for motion with respect to the input frame features \(_{t}^{[l]}\). In this work, the state refers to the representation of a frame of the video. Further, history states (or causal history states) refers to a set of certain frame features previous to the input at some timestep.

Turtle's encoder learns a representation of each frame by downsampling the spatial resolution, while inflating the channel dimensions by a factor of \(2\). At each stage of the encoder, we opt for several stacked convolutional feedforward blocks, termed as Historyless FFN,2. The learned representation at the last encoder stage onwards is fed to a running history queue \(\) of length \(\).3 We empirically set \(=5\) for all the tasks, and consider sequence of \(5\) frames. The entire video sequence is reshaped

Figure 1: **Turtleâ€™s Architecture.** The overall architecture diagram of the proposed method. Turtle is a U-Net  style architecture, wherein the encoder blocks are historyless feedforward blocks, while the decoder couples the causal history model (CHM) to condition the restoration procedure on truncated history of the input. We also present assorted restoration examples on the rightâ€“frame taken from video raindrops and rain streak removal , night deraining , and video deblurring  tasks, respectively.

into \(^{}{7} H W C}\) thereby allowing parallelism in training while maintaining a dense representation of history states to condition the reconstruction procedure on.

The decoder takes the feature map of the current frame, \(_{t}^{[l]}\), and the history states \(_{t}^{[l]}\). We propose a motion compensation module that operates on the feature space to implicitly align history states with respect to the input frame. Next, a dynamic router learns to control the effect of history frames by scoring and aggregating motion-compensated features based on their relevance to the restoration of the current frame. Such a procedure accentuates the aligned history such that the following stages of the decoder can learn to reconstruct the high-quality frame appropriately. Both of these procedures combine to form the Causal History Model \((_{t}^{[l]},_{t}^{[l]})\), detailed in section 3.2. Further, multiple \(\) are stacked as black box layers at different stages to construct the decoder of Turtle.

### Causal History Model

\(\) learns to align the history states with respect to the input feature map. Further, there could still exist potential degradation differences at the same feature locations along the entire sequence in the motion-compensated history states. To this end, \(\) re-weights the sequence along the temporal dimension to accentuate significant features and suppress irrelevant ones. Let \(}_{t}^{[l]}^{(+1) h^{l} w^{l} c ^{l}}\) denote the motion-compensated causal history states, and let input feature map be \(_{t}^{[l]}^{h^{l} w^{l} c^{l}}\). Let the transformation on the history states to align the features be denoted by \(_{t}\), and let \(_{t}\) denote the re-weighting scheme. If the output is given by \(_{t}^{[l]}^{h^{l} w^{l} c^{l}}\), we then, formalize the Causal History Model (\(\)) as,

\[}_{t}^{[l]} =_{t}(_{t}^{[l]},_{t}^{[l]})_{t}(_{t}^{[l]}),\] (1) \[_{t}^{[l]} =_{t}(}_{t}^{[l]},_{t}^{[l]})+ _{t}(_{t}^{[l]}).\] (2)

In eq. (1), \(_{t}\) denotes transformation on the input, and \(\) denotes the skip connection, while \(\) is the concatenation operation. In practice, we learn \(_{t}\), and the input transformation matrix \(\) following the procedure described in _State Align Block_, while \(_{t}\) is detailed in _Frame History Router_. We present a visual illustration of Causal History Model (\(\)) in fig. 2. We also present a special case of (\(\)) in appendix D, wherein we consider optimally compensated motion in videos.

State Align Block (\(\)).State Align Block (\(\)) implicitly tracks and aligns the corresponding regions defined as groups of pixels (or patches) (\(p_{1} p_{2}\))--across each frame in the history. State Align Block computes attention scores through a dot product between any given patch from the current frame and

Figure 2: **Causal History Model.** The diagrammatic illustration of the proposed Causal History Model (\(\)) detailing the internal function. In the initial phase, for each patch in the current frame (denoted by the stars), we identify and implicitly align the _top-k_ similar patches in the history. In the subsequent phase, we score and aggregate features from this aligned history to create a refined output that blends the input frame features with pertinent history data. We visualize frames in this diagram for exposition, but in practice the procedure operates on the feature maps.

all the patches from the history. Given the input feature map of a frame \(_{t}^{[l]}^{h^{l} w^{l} c^{l}}\), we calculate the patched projections as, \(_{_{t}}^{[l]},_{_{t}}^{[l]},_{ _{t}}^{[l]}^{}{p_{1}}}{p_{2} }(cp_{1}p_{2})^{l}}\), i.e., \(_{_{t}}^{[l]},_{_{t}}^{[l]}, _{_{t}}^{[l]}_{t}^{[l]}W^{_{t}^{[l]}}\) where \(W^{_{t}^{[l]}}\) is a learnable parameter matrix. For exposition, let the dimensions of projections be \(^{n_{h}^{l} n_{w}^{l} c^{l}}\), we subsequently rearrange the patches to \(^{(n_{h}n_{w})^{l} d^{l}}\). Here, \(n_{h}^{l}=}{p_{1}}\), and \(n_{w}^{l}=}{p_{2}}\) denote the number of patches along the height and width dimension, and \(d^{l}=(cp_{1}p_{2})^{l}\) represents the dimension of each patch. Formally, we define the history states \(_{t}^{[l]}\) as a set of keys and values to facilitate the attention mechanism as,

\[_{t}^{[l]}=\{_{_{t}}^{[l]},_{ _{t}}^{[l]}\},\] (3)

where \(_{_{t}}^{[l]}\), and \(_{_{t}}^{[l]}\) are formally written as \(_{_{t}}^{[l]}=\{_{_{t-r}}^{[l]}, _{_{t-r+1}}^{[l]},,_{_{t-1}}^{[ l]}\}^{ n_{h}n_{w} d}\), and \(_{_{t}}^{[l]}=\{_{_{t-r}}^{[l]}, _{_{t-r+1}}^{[l]},,_{_{t-1}}^{[ l]}\}^{ n_{h}n_{w} d}\).

We, then, compute the attention, and limit it to the _top-k_ most similar patches in the key vector for each patch in the query vector, and, hence, focus solely on those that align closely. This prevents the inclusion of unrelated patches, which can, potentially, introduce irrelevant correlations, and obscure principal features. We, then, formalize the _top-k_ selection procedure as,

\[_{t}^{[l]} =(_{_{t}}^{[l]}_{_{ t}}^{[l]})/^{(n_{h}^{l}n_{w}^{l})(n_{h}^{l}n_{w} ^{l})},\] (4) \[^{*}_{t}^{[l]} =x,&_{i(n_{h}^{l}n_{w}^{l})}( _{(,:,:)}|_{t}^{[l]},k)$},\\ -,&\] (5)

where \(\) is a learnable parameter to scale the dot product, and \(_{(,:,i)}\) denotes the \(i^{}\) patch along the second dimension. \(^{*}_{t}^{[l]}\) masks the non _top-k_ scores, and replaces with \(-\) to allow for softmax computation. In other words, each patch is compensated for with respect to its _top-k_ similar, and salient patches across the trajectory. Such a procedure allows for soft alignment, and encourages each patch to borrow information from its most similar temporal neighbors, i.e., a one-to-_top-k_ temporal correspondence is learned. Given the _top-k_ scores, we compute the motion-compensated history states \(}_{t}^{[l]}\) as follows,

\[}_{t}^{[l]}=[(^{*}_{t}{}^{[l]})_{ _{t}}^{[l]}]W^{}_{t}^{[l]}}_{ t}(_{t}^{[l]}),\] (6)

where \(\) is the softmax operator, \(\) is the concatenation operator, \(W^{}_{t}^{[l]}}\) is the parameter matrix learned with gradient descent, and \(\) is a transformation on the input \(_{t}^{[l]}\) realized through self-attention along the spatial dimensions . In eq. (6), \(_{t}(_{t}^{[l]},_{t}^{[l]})=[(^{ *}_{t}{}^{[l]})_{_{t}}^{[l]}]W^{}_{ t}^{[l]}}\) which follows from eq. (1).

Frame History Router (\(\)).Given the motion-compensated history states \(}_{t}^{[l]}^{(+1) h^{l} w^{l}  c^{l}}\) and the input features \(_{t}^{[l]}^{h^{l} w^{l} c^{l}}\), Frame History Router (\(\)) learns to route and aggregate critical features for the restoration of the input frame. To this end, we compute the query vector from \(_{t}^{[l]}\) through the transformation matrix \(W^{_{t}^{[l]}}\), resulting in \(_{t}^{[l]}_{t}^{[l]}W^{_{t}^{[l]}}\). Similarly, the key and value vectors are derived from \(}_{t}^{[l]}\), and are parameterized \(W^{}_{t}^{[l]}}\), i.e., \(_{t}^{[l]},_{t}^{[l]}}_{t}^{[l]}W^{ }_{t}^{[l]}}\).

This configuration enables cross-frame channel attention, where the query from \(_{t}^{[l]}\) attends to channels from both \(}_{t}^{[l]}\) and \(_{t}^{[l]}\), and accentuates temporal history states as necessary in order to restore the given frame. The cross-channel attention map \(^{(+1)c^{l} c^{l}}\) is then computed through the dot product, i.e., \(_{t}^{[l]}=(_{t}^{[l]}_{t}^{[l]})/ ^{(+1)c^{l} c^{l}}\), where \(\) is the scale factor to control the dot product magnitude. Note that, we overload the notation \(_{t}^{[l]}\) for exposition. The output feature map, \(_{t}^{[l]}\) takes the shape \(^{h^{l} w^{l} c^{l}}\) since the attention matrix takes the shape \(^{(+1)c^{l} c^{l}}\), while \(_{t}^{[l]}\) is \(^{(+1)c^{l} h^{l} w^{l}}\).4 If \(\) denotes the softmax operator, and \(\) is the skip connection, we then compute the output, \(_{t}^{[l]}\), as,

\[_{t}^{[l]}=[(_{t}^{[l]})_{t}^{[l]} ]W^{}_{t}^{[l]}}+_{t}(_{t}^{[l]}) ^{h^{l} w^{l} c^{l}}.\] (7)

In eq. (7), \(_{t}(}_{t}^{[l]},_{t}^{[l]})=[(_{t}^{[l]})_{t}^{[l]}]W^{}_{t}^{[l]}}\) which follows from eq. (2).

## 4 Experiments

We follow the standard training setting of architectures in the restoration literature [29; 79; 15] with Adam optimizer  (\(_{1}=0.9,_{2}=0.999\)). The initial learning rate is set to \(4e^{-4}\), and is decayed to \(1e^{-7}\) throughout training following the cosine annealing strategy . All of our models are implemented in the PyTorch library, and are trained on \(8\) NVIDIA Tesla v100 PCIe 32 GB GPUs for \(250\)k iterations. Each training video is sampled into clips of \(=5\) frames, and turtle restores frames of each clip with recurrence. The training videos are cropped to \(192 192\) sized patches at random locations, maintaining temporal consistency, while the evaluation is done on the full frames during inference. We assume no prior knowledge of the degradation process for all the tasks. Further, we apply basic data augmentation techniques, including horizontal-vertical flips and \(90\)-degree rotations. Following the video restoration literature, we use Peak Signal-to-Noise

  
**Method** & **PSNR\(\)** & **SSIM\(\)** \\  FDM  & \(23.49\) & \(0.7657\) \\ DSTFM  & \(17.82\) & \(0.6486\) \\ WeatherDiff  & \(20.98\) & \(0.6697\) \\ RMED  & \(16.18\) & \(0.6402\) \\ DLF  & \(15.17\) & \(0.6307\) \\ HRIR  & \(16.83\) & \(0.6481\) \\ MetaRain (Meta)  & \(23.49\) & \(0.7171\) \\ MetaRain (Scrt)  & \(22.21\) & \(0.6723\) \\ NightRain  & \(26.73\) & \(0.8647\) \\ 
**Turtle** & **29.26** & **0.9250** \\   

Table 1: **Night Video Deraining Results.**

  
**Method** & **PSNR\(\)** & **SSIM\(\)** \\  TransWeather  & \(23.11\) & \(0.8543\) \\ SnowFormer  & \(24.01\) & \(0.8939\) \\ S2VD  & \(22.95\) & \(0.8590\) \\ RDDNet  & \(22.97\) & \(0.8742\) \\ EDVR  & \(17.93\) & \(0.5790\) \\ BasicSVR  & \(22.46\) & \(0.8473\) \\ IconvSR  & \(22.35\) & \(0.8482\) \\ BasicVSR++  & \(22.64\) & \(0.8618\) \\ RVRT  & \(20.90\) & \(0.7974\) \\ SVDNet  & \(25.06\) & \(0.9210\) \\ 
**Turtle** & **26.02** & **0.9230** \\   

Table 2: **Video Desnowing Results.**

Figure 3: **Visual Results on Video Desnowing and Nighttime Video Deraining.** We compare video desnowing results with the best published method in literature, SVDNet . The video frame has both snow, and haze. While SVDNet  removes snow flakes, turtle can remove haze, and snow flakes, and hence is more faithful to the ground truth. In nighttime deraining, we compare turtle to MetaRain . turtle maintains color consistency in the restored result.

Ratio (PSNR) and Structural Similarity Index (SSIM)  distortion metrics to report quantitative performance. For qualitative evaluation, we present visual outputs for each task and compare them with the results obtained from previous best methods in the literature.

### Night Video Deraining

SynNightRain  is a synthetic video deraining dataset focusing on nighttime videos wherein rain streaks get mixed in with significant noise in low-light regions. Therefore, nighttime deraining with heavy rain is generally a harder restoration task than other daytime video deraining. We follow the train/test protocol outlined in [47; 35], and train turtle on \(10\) videos from scratch, and evaluate on a held-out test set of \(20\) videos. We report distortion metrics, PSNR and SSIM, in table 1, and compare them with previous restoration methods. turtle achieves a PSNR of \(\) dB, which is a notable improvement of **+2.53** dB over the next best result, NightRain . Further, we present visual results in fig. 3, and in fig. 12. Our method, turtle, maintains color consistency in the restored results.

### Video Desnowing

Realistic Video Desnowing Dataset (RVSD)  is a video-first desnowing dataset simulating realistic physical characteristics of snow and haze. The dataset comprises a variety of scenes, and the videos are captured from various angles to capture realistic scenes with different intensities. In total, the dataset includes \(110\) videos, of which \(80\) are used for training, while \(30\) are held-out test set to measure desnowing performance. We follow the proposed train/test split in the original work  and train turtle on the video desnowing dataset. Our scores, \(\) dB in PSNR, are reported in table 2, and compared to previous methods, turtle significantly improves the performance by **+0.96** dB in PSNR. Notably, turtle is prior-free, unlike the previous best result SVDNet , which exploits snow-type priors. We present visual results in fig. 3, and in fig. 11 comparing turtle to SVDNet . Our method not only removes snowflakes but also removes haze, and the restored frame is visually pleasing.

### Real Video Deblurring

The work done in [83; 82] introduced a real-world deblurring dataset (BSD) using the Beam-Splitter apparatus. The dataset introduced contains three different variants depending on the blur intensity settings. Each of the three variants has a total of \(11,000\) blurry/sharp pairs with a resolution of \(640 480\). We employ the variant of BSD with the most blur exposure time, i.e., \(3\)ms-\(24\)ms.5 We follow the standard train/test split introduced in  with \(60\) training videos, and \(20\) test videos. We report the scores in table 3 on the \(3\)ms-\(24\)ms variant of BSD and compare with previously published methods. turtle scores \(\) dB in PSNR on the task, observing an increase of **+2.0** dB compared to the previous best methods, CDVD-TSP , and ESTRNN [83; 82]. We present visual results in fig. 13.

  
**Method** & **PSNR\(\)** & **SSIM\(\)** \\  STRONN  & \(29.42\) & \(0.893\) \\ DBN  & \(31.21\) & \(0.922\) \\ SRN  & \(28.92\) & \(0.882\) \\ IFI-RNN  & \(30.89\) & \(0.917\) \\ STAN  & \(29.47\) & \(0.872\) \\ CDVD-TSP  & \(31.58\) & \(0.926\) \\ PVDNet  & \(31.35\) & \(0.923\) \\ ESTRNN  & \(31.39\) & \(0.926\) \\ 
**TURLE** & **33.58** & **0.954** \\   

Table 3: **Real-World Video Deblurring. Quantitative results (PSNR, and SSIM) on the \(3\)ms-\(24\)ms BSD dataset  comparing state-of-the-art methods.**

  
**Method** & **PSNR\(\)** & **SSIM\(\)** \\  STRONN  & \(29.42\) & \(0.893\) \\ DBN  & \(31.21\) & \(0.922\) \\ SRN  & \(28.92\) & \(0.882\) \\ IFI-RNN  & \(30.89\) & \(0.917\) \\ STAN  & \(29.47\) & \(0.872\) \\ CDVD-TSP  & \(31.58\) & \(0.926\) \\ PVDNet  & \(31.35\) & \(0.923\) \\ ESTRNN  & \(31.39\) & \(0.926\) \\ 
**TURLE** & **33.58** & **0.954** \\   

Table 4: **Stribtic Video Deblurring Results. Quantitative results (PSNR, and SSIM) on the VGPro dataset  comparing state-of-the-art methods.**

  
**Method** & **PSNR\(\)** & **SSIM\(\)** \\  S2VD  & \(18.95\) & \(0.6630\) \\ EDVR  & \(19.19\) & \(0.6363\) \\ BasicVSR  & \(28.35\) & \(0.8990\) \\ WRT  & \(27.77\) & \(0.8856\) \\ TTVSR  & \(28.05\) & \(0.8998\) \\ RVRT  & \(28.24\) & \(0.8857\) \\ RDNet  & \(28.38\) & \(0.9096\) \\ BasicVSR++  & \(29.75\) & \(0.9171\) \\ ViMPNet  & \(31.02\) & \(0.9283\) \\ 
**TURLE** & **34.50** & **0.9720** \\ 
**TURLE** & **32.01** & **0.9590** \\   

Table 5: **Video Raindrop and Rain Streak Removal. Quantitative results (PSNR, and SSIM) on the VRDS dataset  comparing state-of-the-art methods.**

[MISSING_PAGE_FAIL:8]

### Video Super-Resolution

MVSR\(4\) is a real-world paired video super-resolution dataset  collected by mobile phone's dual cameras. We train turtle following the dataset split in the official work  and test on the provided held-out test set. We report distortion metrics in table 7 and compare it with several methods in the literature. turtle scores \(\) dB in PSNR on the task, with a significant increase of **+1.36** dB compared to the previous best method, EAVSR+ . We present visual results on the task in fig. 5. Other methods such as TTVSR , BasicVSR , or EAVSR  tend to introduce blur in up-scaled results, while turtle's restored results are sharper.

### Blind Video Denoising

We assume no degradation prior, and consider blind video denoising task [49; 55]. We train our model on DAVIS  dataset, and test on DAVIS held-out testset, and a generalization set Set8 . We add white Gaussian noise to the dataset with noise level \(\) to train turtle, and test on two noise levels \(=30\), and \(=50\); scores are reported in table 6. turtle observes a gain of **+0.31** dB on \(=30\), and **+0.34** dB on \(=50\) on Set8 testset, scoring \(\) dB, and \(\) dB, respectively, while it observes an average drop of \(-0.3\) dB to BSVD-64  on the DAVIS testset. Further, we present qualitative results in fig. 5 comparing turtle, and previous best method BSVD .

### Computational Cost Comparison

In table 8, we compare turtle with previous methods in the literature in terms of multiply-accumulate operations (MACs). The results are computed for the input size \(256 256\). We measure the performance on the number of frames the original works utilized7 to report their performance, as

Figure 5: **Blind Video Denoising and Video Super-Resolution Visual Results. Qualitative comparison of previous methods with turtle on a test frame from Set8 dataset for blind video denoising (\(=50\)), and MVSR\(4\) dataset  for video super resolution. In video denoising, turtle restores details, while BSVD-64  smudges textures (text and the dinosaur on the bikerâ€™s jacket). In VSR, previous methods such as TTVSR , BasicVSR++ , or EAVSR  tend to introduce blur in results, while turtleâ€™s restored results are sharper, and crisper.**

  
**Method** & **Venue** & **Task** & **MACs (G) \(\)** \\  RVRT  & NeurIPSâ€™22 & Restoration & \(1182.16\) \\ VRT  & TIPâ€™24 & Restoration & \(1631.67\) \\ RDDNet  & ECCVâ€™22 & Deraining & \(362.36\) \\ DSTNet  & CVPRâ€™23 & Deblurring & \(720.28\) \\ EDVR  & CVPRâ€™19 & Deraining & \(527.5\) \\ BasicVSR [6; 7] & CVPRâ€™21 & Super Resolution & \(240.17\) \\
**turtle** & NeurIPSâ€™24 & Restoration & **181.06** \\   

Table 8: **MACs (G) Comparison. We report MACs (G) of turtle, and compare with previous methods in literature. We also extensively profile turtle with varying input resolutions on a single GPU, and compare it with previous restoration methods in appendix F.**reported in their manuscript or code bases. In turtle's case, we report MACs (G) on a single frame since turtle only considers a single frame at a time but adjust for history features utilized in CHM as part of turtle's decoder. In comparison to parallel methods, EDVR , VRT , turtle is computationally efficient, as it is lower in MACs (G). Although the MACs are approximately similar to recurrent methods, BasicVSR , turtle scores significantly higher in PSNR/SSIM metrics (see table 7, and table 5). In comparison to contemporary methods such as RVRT , which combines recurrence and parallelism in design, turtle is significantly lower on MACs (G) and performs better (see table 5, and table 2) thanks to its ability to memorize previous frames.

## 5 Ablation Study

We ablate turtle to understand what components necessitate efficiency and performance gains. All experiments are conducted on synthetic video deblurring task, GoPro dataset , using a smaller variant of our model. Our smaller models operate within a computational budget of approximately \(5\) MACs (G), while the remaining settings are the same as those of the main model. In all the cases, the combinations we adopt for turtle are highlighted. Additional ablation studies are deferred to appendix A, and we discuss the limitations of the proposed method in appendix C.

Block Configuration.We ablate the Causal History Model (CHM) to understand if learning from history benefits the restoration performance. We compare turtle with two settings: baseline (no CHM block) and turtle without State Align Block (\(\)). In baseline (no CHM), no history states are considered, and two frames are concatenated and fed to the network directly. Further, in No \(\), the state align block is removed from CHM. We detail the results in table 9, and find that both State Align Block, and CHM are important to the observed performance gains.

Truncation Factor \(\).We evaluate context lengths of \(=1\), \(3\), and \(5\) past frames and found no PSNR improvement when increasing the context length beyond three frames. Results in table 10 confirm that extending beyond three frames does not benefit performance. This is because, as in most cases, the missing information in the current frame is typically covered within the three-frame span, and additional explicit frame information fails to provide additional relevant details.

Value of \(k\) in _topk_.We investigate the effects of different \(k\) values in _topk_ attention. Our experiments, detailed in table 11, show that \(k\) crucially affects restoration quality. Utilizing a larger number of patches, \(k=20\), leads to an accumulation of irrelevant information, negatively impacting performance by adding unnecessary noise. Further, selecting only 1 patch is also sub-optimal as the degraded nature of inputs can lead to inaccuracies in identifying the most similar patch, missing vital contextual information. The optimal balance was found empirically with \(k=5\), which effectively minimizes noise while ensuring the inclusion of key information.

## 6 Conclusion

In this work, we introduced a novel framework, turtle, for video restoration. turtle learns to restore any given frame by conditioning the restoration procedure on the frame history. Further, it compensates the history for motion with respect to the input and accentuates key information to benefit from temporal redundancies in the sequence. turtle enjoys training parallelism and maintains the entire frame history implicitly during inference. We evaluated the effectiveness of the proposed method and reported state-of-the-art results on seven video restoration tasks.