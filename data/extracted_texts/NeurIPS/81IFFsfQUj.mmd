# DMPlug: A Plug-in Method for Solving Inverse Problems with Diffusion Models

Hengkang Wang\({}^{1}\)   Xu Zhang\({}^{2}\)1   Taihui Li\({}^{1}\)   Yuxiang Wan\({}^{1}\)   Tiancong Chen\({}^{1}\)   Ju Sun\({}^{1}\)

\({}^{1}\)Department of Computer Science and Engineering, University of Minnesota

{wang9881,lixx5027,wan01530,chen6271,jusun}@umn.edu

\({}^{2}\)Amazon.com Inc., spongezhang@gmail.com

###### Abstract

Pretrained diffusion models (DMs) have recently been popularly used in solving inverse problems (IPs). The existing methods mostly interleave iterative steps in the reverse diffusion process and iterative steps to bring the iterates closer to satisfying the measurement constraint. However, such interleaving methods struggle to produce final results that look like natural objects of interest (i.e., manifold feasibility) and fit the measurement (i.e., measurement feasibility), especially for nonlinear IPs. Moreover, their capabilities to deal with noisy IPs with unknown types and levels of measurement noise are unknown. In this paper, we advocate viewing the reverse process in DMs as a function and propose a novel plug-in method for solving IPs using pretrained DMs, dubbed DMPlug. DMPlug addresses the issues of manifold feasibility and measurement feasibility in a principled manner, and also shows great potential for being robust to unknown types and levels of noise. Through extensive experiments across various IP tasks, including two linear and three nonlinear IPs, we demonstrate that DMPlug consistently outperforms state-of-the-art methods, often by large margins especially for nonlinear IPs. The code is available at https://github.com/sun-umn/DMPlug.

## 1 Introduction

Inverse problems (IPs) are prevalent in numerous fields, such as computer vision, medical imaging, remote sensing, and autonomous driving . The goal of IPs is to recover an unknown object \(\) from noisy measurements \(=()+\), where \(\) is a (possibly nonlinear) forward model and \(\) denotes the measurement noise. IPs are often ill-posed: typically, even if \(\) is noiseless, \(\) cannot be uniquely determined from \(\) and \(\). Hence, incorporating prior knowledge on \(\) is necessary to obtain a reliable estimate of the underlying \(\).

Traditionally, IPs are solved in the regularized data-fitting framework, often motivated as performing the Maximum a Posterior (MAP) inference:

\[_{}\ (,())+() (,())()\] (1)

Here, minimizing the data-fitting loss promotes \(()\), and the regularizer encodes prior knowledge on \(\). Recently, the advent of deep learning (DL) has brought about a few new algorithmic ideas to solve IPs. For example, given a training set of measurement-object pairs, i.e., \(\{(_{i},_{i})\}_{i=1,,N}\), one can hope to train a DL model that directly predicts \(\) for a given \(\). However, such hopes can be shattered by practical challenges in collecting **massive and realistic** paired training sets, especially for complex IPs . Even if such challenges can be tackled, one may need to collect a new training set and train a new DL model for every new IP , overlooking potential shared priors on \(\) across IPs. An attractive alternative family of ideas combine pretrained priors on \(\) andregularized data-fitting in Eq. (1). For example, they first model the distribution of \(\) using deep generative models, such as generative adversarial networks (GANs) and diffusion models (DMs), based on training sets of the form \(\{_{i}\}_{i=1,,N}\), and then encode these pretrained generative priors when solving Eq. (1). In this way, pretrained priors on \(\) can be reused in an off-the-shelf manner in different IPs about the same family of structured objects.

**In this paper, we focus on solving IPs with pretrained DMs**. DMs have recently emerged as a dominant family of deep generative models due to their relative stability during training (e.g., vs. GANs) and their strong capabilities to generate photorealistic images (and, depending on applications, other structured objects) once trained . These strengths of DMs have motivated ideas to use pretrained DMs to solve IPs, such as denoising, super-resolution, inpainting, deblurring, and phase retrieval . Most of these ideas interleave iterative reverse diffusion steps and iterative steps (sometimes projection steps, especially for linear IPs) to move closer to the feasibility set \(\{|=()\}\) (see Fig. 3 and Fig. 4 (1)). However, they typically cannot guarantee the final convergence of the iteration sequence to either the feasible set (i.e., **measurement feasibility**), or the object manifold \(\) captured by pretrained DMs (i.e., **manifold feasibility**), as they have modified both iterative processes (see detailed arguments in Section 2).

Fig. 1 shows visible artifacts produced by these ideas on several IPs, highlighting **(Issue 1) insufficient manifold feasibility**. To quickly confirm **(Issue 2) insufficient measurement feasibility**,

Figure 1: Visualization of sample results from our DMPlug method (**Ours**) and main competing methods (**DPS** and **Resample** for super-resolution, inpainting, and nonlinear deblurring; **BlindDPS** and **Stripformer** for blind image deblurring (BID) and BID with turbulence) on IPs we focus on in this paper. All measurements contain Gaussian noise with \(=0.01\).

Figure 2: **Evolution of the data-fitting loss \(\|-()\|_{2}^{2}\) of our DMPlug method vs. SOTA methods over percentage progress**, for noiseless nonlinear deblurring on the CelebA dataset. Here, the percentage progress is calculated with respect to the total number of iterations taken by each method. The shadow regions indicate the ranges of the loss over \(50\) instances.

we experiment on **noiseless** instances of the nonlinear deblurring problem (a nonlinear IP) following [20; 19], and find that state-of-the-art (SOTA) methods fail to find an \(\) that satisfies \(=()\), as shown in Fig. 2. Furthermore, most SOTA DM-based methods for IPs assume known noise types (e.g., often Gaussian) and known, often very low, noise levels, casting doubts on their performance when faced with unknown noise types and levels, i.e., (**Issue 3**) robustness to unknown noise types and levels, as we confirm in Table 4.

Our contributionsIn this paper, **we propose a novel plug-in method, dubbed DMPlug, to solve IPs with pretrained DMs to mitigate all of the above three issues**. DMPlug departs from the popular and dominant interleaving line of ideas by viewing the whole reverse diffusion process as a function \(()\), consisting of multiblock stacked DL models, mapping from the seed space to the object manifold \(\). This novel perspective allows us to naturally parameterize the object to be recovered as \(=()\) and then plug this reparametrization into Eq. (1), leading to a unified optimization formulation **with respect to the seed \(\)**. Conceptually, since \(()\) probably produces a feasible point on the object manifold \(\) and global optimization of the unified formulation encourages \((())\), the optimized \(\) could lead to an \(=()\) that **enjoys both manifold and measurement feasibility**, i.e., (**tackling Issues 1 & 2**). Fig. 3 and Fig. 4 schematically illustrate the dramatic difference between interleaving methods and our plug-in method (DMPlug), and Figs. 1 and 2 confirms DMPlug's **strong capability** in finding feasible solutions. For noisy IPs with unknown noise types and levels, we observe that our DMPlug enjoys a benign "early-learning-then-overfitting" (ELTO) property: the quality of the estimated object climbs first to a peak and then degrades once the noise is picked up, as shown in Fig. 6 (2). This benign property, in combination with appropriate early-stopping (ES) methods that stop the estimate sequence near the peak, allows our method to solve IPs without the exact noise information, i.e., (**tackling Issue 3**), as shown in Table 4). Fortunately, we find that an ES method, ES-WMV  (co-developed by a subset of the current authors), works well for this purpose (see Table 4).

Our contributions can be summarized as follows. **(1)** In Section 3.1, we **pioneer a novel plug-in method, DMPlug**, which is significantly different from the prevailing interleaving methods, to solve IPs with pretrained DMs. Then we **make the proposed method practical** in terms of computational and memory expenses by leveraging one key observation; **(2)** In Section 4 and Appendix E, we perform extensive experiments on various linear and nonlinear IPs and show that **our method outperforms SOTA methods**, both qualitatively and quantitatively--often by large margins, especially on nonlinear IPs. For example, measured in PSNR, our method can lead SOTA methods by about \(2\)dB and \(3 6\)dB for linear and nonlinear IPs, respectively. Moreover, our method demonstrates flexibility in employing different priors and optimizers, as explored in Section 4.3; **(3)** In Section 3.3, we **observe an early-learning-then-overfitting (ELTO) property**, i.e., that our DMPlug tends to recover the desirable object first and then overfit to the potential noise. By taking advantage of this benign property and integrating the ES method ES-WMV , our method is **the first to achieve robustness to unknown noise types and levels**, leading SOTA methods by about \(1\)dB and \(3.5\)dB in terms of PSNR for linear and nonlinear IPs, respectively.

## 2 Background and related work

Diffusion models (DMs)The denoising diffusion probabilistic model (**DDPM**)  is a seminal DM for unconditional image generation. It gradually transforms \(_{0} p_{}\) into total noise \(_{T}(,)\) (i.e., forward diffusion process) and then learns to gradually recover \(_{0}\) from \(_{T}\) through

Figure 3: Interleaving methods (left) vs. our DMPlug method (right) for solving IPs using pretrained DMs. While interleaving methods cannot ensure the feasibility of the final estimate for either the object manifold \(\) or the feasible set \(\{|=()\}\), our DMPlug method ensures the manifold feasibility while promoting \(()\) through global optimization.

incremental denoising (i.e., reverse diffusion process). The forward process can be described by a stochastic differential equation (SDE), \(d=-_{t}/2dt+}d\), where \(_{t}\) is the noise schedule and \(\) is the standard Wiener process. The corresponding reverse process is described by

\[()\ d=-_{t}[/2+ _{} p_{t}()]dt+}d}.\] (2)

Here, \(}\) is the time-reversed standard Wiener process, \(p_{t}()\) is the probability density at time \(t\), and \(_{} p_{t}()\) is the (Stein) score function, which is approximated by a DL model \(_{}^{(t)}()\) via score matching methods during DM training . For discrete settings, given time steps \(t\{1,,T\}\), a variance schedule \(_{1},,_{T}\), \(_{t} 1-_{t}\) with \(_{T} 0\), and \(_{t}_{s=1}^{t}_{s}\), the DDPM has the forward process \(_{t}=}_{t-1}+}\) and the reverse process

\[()\ _{t-1}=1/}(_{t}- _{t}/_{t}}_{}^{(t)}( _{t}))+},\] (3)

where \((,)\). As the DDPM has a slow reverse/sampling process,  proposes the denoising diffusion implicit model (**DDIM**) to mitigate this issue. With the same notation as that of the DDPM, the DDIM makes a crucial change to the DDPM: relaxing the forward process to be non-Markovian by making \(_{t}\) depend on both \(_{0}\) and \(_{t-1}\). This simple change allows skipping iterative steps in the reverse process, without retraining the DDPM. This leads to much smaller numbers of reverse steps, and hence substantial speedup in sampling. The reverse process is now defined as

\[()\ _{t-1}\!=\!_{t-1}}}_{0} (_{t})\!+\!_{t-1}}_{}^{(t )}(_{t}),\] (4)

where \(}_{0}(_{t})[_{t}-_{t}} _{}^{(t)}(_{t})]/_{t}}\) is the predicted \(_{0}\) with \(_{t}\).

Pretrained DMs for solving IPsIdeas for solving IPs with DMs can be classified into two categories: supervised and zero-shot . The former trains DM-based IP solvers based on paired training sets of the form \(\{(_{i},_{i})\}\) and is hence not our focus here (see our arguments in Section 1). The latter makes use of pretrained DMs as data-driven priors: **(I)** Most of the work in this category considers modeling \(p_{t}(|)\) directly and replaces the (unconditional) score function \(_{} p_{t}()\) in Eq. (2) by the conditional score function \(_{} p_{t}(|)=_{} p_{t}()+ _{} p_{t}(|)\), leading to the conditional reverse SDE

\[d=[-_{t}/2-_{t}(_{}  p_{t}()+_{} p_{t}(|))]dt+}d}.\] (5)

Here, while \(_{} p_{t}()\) can be naturally approximated by the pretrained score function \(_{}^{(t)}()\), \(_{} p_{t}(|)\) is intractable as \(\) does not directly depend on \((t)\)2. Ideas to circumvent this difficulty include approximating \(p_{t}(|(t))\) by \(p_{t}(|}(0)|(t)|)\), where \(}(0)[(t)]\) is implemented as \(}_{0}(_{t})\) of Eq. (4) in discretization , and interleaving unconditional reverse steps of Eq. (3) or Eq. (4) and (approximate) projections onto the feasible set \(\{|=()\}\) to bypass the likelihood \(p_{t}(|)\); **(II)** An interesting alternative is to recall that the MAP framework (see also Eq. (1)) involves \(_{} p(|)+ p()\), and first-order methods, especially proximal-gradient style methods, to optimize the MAP formulation typically only need to access \(p()\) through \(_{} p()\), i.e., the score function--the central object in DMs! So, one can derive IP solvers by wrapping pretrained DMs around first-order methods for (approximately) optimizing the MAP formulation, see, e.g., .

Despite the disparate conceptual ways of utilizing pretrained DM priors, most of the methods under **(I) and (II)** proposed in the literature so far follow a single algorithmic template, i.e., Algorithm 1. There, Lines 3-5 are simply a reverse iterative step in the DDIM (Eq. (4); obviously, one could replace this by a reverse iterative step in other appropriate pretrained DMs. Line 6 helps move the iterate closer or onto the feasible set \(\{|=()\}\). In other words, these methods interleave iterative steps to move toward the data manifold \(\) defined by the pretrained DM and iterative steps to move toward the feasible set \(\{|=()\}\), i.e., as illustrated in Fig. 3 (left). However, it is unclear, a priori, whether such interleaving iterative sequence will converge to either, leading to concerns about **(Issue 1) insufficient manifold feasibility** and **(Issue 2) insufficient measurement feasibility**. In fact, our Figs. 1 and 2 confirm these issues empirically, echoing observations made in several prior papers . In principle, Issue 2 can be mitigated by ensuring that Line 6 finds a feasible \(_{i-1}\) for \(\{|=()\}\) in each iteration [39; 30; 33; 32; 40; 41; 20]. However, this is possible only for easy IPs (e.g., linear IPs where we can perform a closed-form projection) and is difficult for typical nonlinear IPs as hard nonconvex problems are entailed. Moreover, although most of these methods have considered noisy IPs alongside noiseless ones [39; 30; 33; 32; 20; 28; 19; 21; 29], their common assumption about known noise types (often Gaussian) and levels (often low) is unrealistic; there are many types of measurement noise in practice--often hybrid from multiple sources , and the noise levels are usually unknown and hard to estimate , raising concerns about **(Issue 3) the robustness of these methods to unknown noise types and levels**; see Section 4.2.

## 3 Method

In this section, we propose a simple plug-in method, DMPlug, to solve IPs with pretrained DMs to address Issues 1 & 2 in Section 3.1, discuss its connection to and difference from several familiar ideas in Section 3.2, and finally explain how to integrate an early-stopping strategy, ES-WMV , into our plug-in method to address Issue 3 in Section 3.3, leading to an algorithm, DMPlug+ES-WMV, summarized in Algorithm 3, that solves IPs with pretrained DMs under the regularized data-fitting framework Eq. (1), even in the presence of unknown noise.

```
0: # Diffusion steps \(T\), measurement \(\)
1:\(_{T}(,)\)
2:for\(i=T-1\) to \(0\)do
3:\(}_{}^{(i)}(_{i})\)
4:\(}_{0}}}_{i}-_{i}}}\)
5:\(^{}_{i-1}\) reverse with \(}_{0}\) and \(}\)
6:\(_{i-1}\)
7:endfor
8: Update \(^{e+1}_{T}\) from \(^{e}_{T}\) via a gradient update for Eq. (7)
9:endfor
10: Recovered object \(_{0}\) ```

**Algorithm 1** Template for interleaving methods

### Our plug-in method: DMPlug

Reverse process as a function and our plug-in methodInterleaving methods discussed above can have trouble satisfying both manifold feasibility and measurement feasibility because they interleave and hence modify both processes. Although projection-style modifications can improve measurement feasibility [39; 30; 33; 32; 40; 41; 20], their application to nonlinear IPs seems tricky, and a single "projection" step could be as difficult and expensive as solving the original problem due to the typical nonconvexity induced by the nonlinearity in \(\)[28; 20]. In contrast, we propose **viewing the whole reverse process as a function**\(()\) that maps from the seed space to the object space (or the object manifold \(\)). Mathematically, if we write a single inverse step as a function \(g\) that depends on \(_{}^{(i)}\), i.e., \(g_{_{}^{(i)}}\) for the \(i\)-th reverse step that maps \(_{i+1}\) to \(_{i}\), then

\[=g_{_{}^{(0)}} g_{_ {}^{(1)}} g_{_{}^{(T-2)} } g_{_{}^{(T-1)}}.()\] (6)

Note that for the DDPM, and those DMs based on SDEs in general, \(\) is a stochastic function due to noise injection in each step. To reduce technicality, we focus on DMs based on ordinary differential equations (ODEs), in particular, the DDIM, due to their increasing popularity [45; 46], resulting in deterministic \(\)'s. This conceptual leap allows us to reparametrize our object of interest as \(=()\) and plug this reparametrization into the traditional regularized data-fitting framework in Eq. (1), yielding the following unified optimization formulation:

\[(})\ ^{*}*{arg\,min}_{}\ (, (()))+(()),^{*} =(^{*}).\] (7)

We stress that here the optimization is with respect to the seed variable \(\), given the pretrained reverse process \(\). Since we never modify the reverse diffusion process \(\), we expect \(()\) to

Figure 4: Comparison of prevailing interleaving methods and our plug-in method

produce an object on the object manifold \(\), i.e., enforcing manifold feasibility to **address Issue 1**. Moreover, optimizing the unified formulation Eq. (7) is expected to promote \(((^{*}))\), inherent in the regularized data-fitting framework, i.e., promoting data feasibility to **address Issue 2**.

When there are multiple objects of interest in the IP under consideration, i.e., \(=(_{1},,_{k})+\) with \(k\) objects (e.g., in blind image deblurring \(=(,)+\), both the blur kernel \(\) and sharp image \(\) are objects of interest, where \((,)=*\); see also Appendix C.4), different objects may have different priors that should be encoded and treated differently. Our unified optimization formulation Eq. (7) facilitates the natural integration of multiple priors. Another bonus feature of our unified formulation lies in the flexibility in choosing numerical optimization solvers. We briefly explore both aspects in Section 4.3.

Fast samplers for memory and computational efficiencyWhen implementing DMPlug with typical gradient-based solvers, e.g., _ADAM_, the gradient calculation in each iterative step requires a forward and a backward pass through the entire \(\), i.e., \(T\) blocks of the basic DL model in the given DM. For high-quality image generation [23; 24; 25], \(T\) is typically tens or hundreds, resulting in prohibitive memory and computational burdens. To resolve this, we use the DDIM, which allows skipping reverse sampling steps thanks to its non-Markovian property, as the sampler \(\). We observe, _to our surprise and also in our favor_, that a very small number of reverse steps, such as \(3\), is sufficient for our method to beat SOTA methods on all IPs we evaluate (see Fig. 5 and Section 4), and further increasing the number does not substantially improve the performance (actually even slightly degrades it perhaps due to the numerical difficulty caused by vanishing gradients as more steps are included). So, **we default the number of reverse steps to \(3\) unless otherwise stated**. This number is not sufficient for generating high-quality photorealistic images with existing DMs, but is good enough for our method. The discrepancy suggests a fundamental difference between image generation and image "regression" involved in solving IPs--we leave this for future work.

### Seemingly similar ideas

(A) GAN inversion for IPsOur plug-in method is reminiscent of GAN inversion to solve IPs [48; 49]: for a pretrained GAN generator \(G_{}\), GAN inversion performs a similar reparametrization \(=G_{}()\) and plugs it into Eq. (1). The remaining task is also to minimize the resulting formulation with respect to the trainable seed \(\), and produce an \(=G_{}(^{*})\) through a solution \(^{*}\). However, there are numerous signs that GAN inversion does not work well for IPs. For example,  also finetunes the generator \(G_{}\) alongside the trainable seed to boost performance, and [33; 32] report superior performance of DM-based methods compared to GAN-inversion-based ones for solving IPs; (B) **Diffusion inversion (DI)** Given an object \(\), DI aims to find a seed \(\) so that \(()\) reproduces \(\), an important algorithmic component for DM-based image and text editing [51; 52; 53; 24; 54; 55]. A popular choice for DI is to modify DDIM [53; 24; 54; 55]. Although we cannot use DI to solve general IPs, DI can be considered as an IP and solved through \(_{}~{}(,())\) using our plug-in method. (C) **Algorithm unrolling (AU)** The multiblock structure in \(\) also resembles those DL models used in AU, a popular family of supervised methods for solving IPs [56; 16]. In AU, the trainable DL model also consists of multiple blocks of basic DL models, induced by unfolded iterative steps to solve Eq. (1). While in AU the weights in these DL blocks are trainable, the weights in our DL blocks are fixed. More importantly, as a supervised approach, AU requires paired training sets of the form \(\{(_{i},_{i})\}_{i=1,,N}\), in contrast to the zero-shot nature of our method here.

### Achieving robustness to unknown noise

The early-learning-then-overfitting (ELTO) phenomenonWhen \(\) contains noise, solving Eq. (7) can promote measurement feasibility, i.e., \(((^{*}))\), but \(((^{*}))\) may also learn the noise, i.e., **overfitting to noise**. Interestingly, **our method seems to favor desirable content and resist noise**: (A) our method converges much faster when used to regress clean natural images than random

Figure 5: **PSNR (dB) vs. per-iteration wall-clock time (s) running on an _NVIDIA A100_, for various reverse steps in \(\). Experiments on CelebA for \(4\) super-resolution; solver: _ADAM_; maximum iterations: \(6,000\)

noise (Fig. 6 (1), suggesting that our method shows high resistance to noise and low resistance to structured content, and **(B)** when performing regression against a noisy image \(=_{0}+\), our method, although powerful enough to overfit the noisy image \(\) ultimately, picks up the desired image content first and then learns the noise, leading to a hallmark "early-learning-then-overfitting" (**ELTO**) phenomenon so that **the recovery quality climbs to a peak before the potential degradation due to noise** (Fig. 6 (2)). We stress that similar ELTO phenomena have been widely reported in the literature on using deep image priors (DIPs) to solve IPs [57; 58; 59; 60; 61], although **this is the first time this phenomenon has been reported for DM-based methods**. Inspired by related studies in DIP, we also perform a spectral analysis of intermediate recovery and reveal that our method has **a spectral bias toward low-frequency components during learning**, similar to DIP methods; see Appendix B.

Achieving robustness via early stopping (ES)One may wonder how widely this ELTO phenomenon occurs when using our DMPlug to solve IPs. Besides extensive additional visual confirmation (see Appendix F), in Table 4 we show that our method, without using the noise information, leads the SOTA methods in terms of peak performance during iteration--particularly, by large margins on non-linear deblurring. The quantitative results suggest that the ELTO phenomenon is likely widespread, especially across noise types and levels. Hence, if we can perform proper early stopping (ES) to locate the peak performance, we **tackle Issue 3**. Deriving an effective ES strategy here is nontrivial, as in practice we do not have groundtruth images to compute any reference-based performance metrics such as PSNR. Fortunately, in the DIP literature,  discovers that for DIP-based methods for IPs, the valleys of the running-variance curves of the intermediate reconstructions are well aligned with the performance peaks. Based on this crucial observation, they propose an ES strategy, ES-WMV, that can accurately detect performance peaks with small performance loss on various IP tasks. Inspired by their success, we integrate the ES-WMV strategy into our plug-in method and find that ES-WMV is highly synergetic with our method and performs reliable ES with negligible performance loss (see Table 4). Details of the entire algorithm can be found in Algorithm 3.

## 4 Experiments

In this section, we evaluate our plug-in method, DMPlug, and compare it with other SOTA methods on two linear IPs, including **super-resolution** and **inpainting**, and three nonlinear IPs, including **nonlinear deblurring**, **blind image deblurring** (**BID**), and **BID with turbulence**. Following , we construct the evaluation sets by sampling \(100\) images from CelebA , FFHQ  and LSUN-bedroom , respectively, and resizing all images to \(256 256\); we measure recovery quality using three standard metrics for image restoration, including peak signal-to-noise-ratio (PSNR), structural similarity index (SSIM) and Learned Perceptual Image Patch Similarity (LPIPS)  with the default backbone. We describe in detail the five IPs tested and the formulations we use for them in Appendix C; we provide implementation details of our method and the competing methods in

Figure 6: The recovery performance of our method on image regression with (1): (a) a clean natural image, (b) the same image with pixels randomly shuffled, (c) random noise iid sampled from \((0,1)\), and (2) a noisy natural image with Gaussian noise at \(=0.08\). Here, image regression means, given any image \(\), performing \(_{}~{}(,())\).

### IP tasks and experimental results

Linear IPsDue to space constraints, our experimental results on **Super-resolution & inpainting** are included in Appendix E.2. Our DMPlug can lead the best SOTA methods by about \(2\) in PSNR and \(0.02\) in SSIM on average, respectively.

Nonlinear IPsNonlinear deblurringWe use the learned blurring operators from  with a known Gaussian-shaped kernel and Gaussian additive noise with \(=0.01\), following . We compare our DMPlug against several strong baselines: Blur Kernel Space (BKS)-styleGAN2  based on GAN priors, BKS-generic  based on Hyper-Laplacian priors , and DM-based methods that can handle nonlinear IPs, including MCG, ILVR, DPS and Res-Sample. Despite the advancements made by the recent ReSample  to enhance the DPS method, from Table 1, it is evident that our DMPlug can still significantly outperform the SOTA ReSample by substantial margins. Specifically, our method improves LPIPS, PSNR, and SSIM by \(0.05\), \(4.5\), and \(0.07\), respectively, across the three datasets on average. In addition, our DMPlug delivers a more faithful and precise restoration of details, as shown in Fig. 1.

**BID & BID with turbulence** BID is about recovering a sharp image \(\) (and kernel \(\)) from \(=*+\) where \(*\) denotes the linear convolution and the spatially invariant blur kernel \(\) is unknown; for BID with turbulence which often arises in long-range imaging through atmospheric turbulence, we model the forward process as a simplified "tilt-then-blur" process, following : \(=*_{}()+\), where \(_{}()\) is the tilt operation parameterized by unknown \(\) (see more details in Appendix C). We mainly compare our DMPlug to two DM-based models, including ILVR and BlindDPS , which is an extension of DPS. In addition, we choose two classical MAP-based methods, i.e., Pan-Dark Channel Prior (Pan-DCP)  and Pan-\(_{0}\), DIP-based SelfDeblur , and four methods that are based on supervised training on paired datasets, including DeBlurGANv2 , Stripformer , MPRNet  and TSR-WGAN . It is important to mention that **BlindDPS  use pretrained DMs not only for images but also for blur kernels (and tilt maps), tending it an unfair advantage over other methods**. Although our method is flexible in working with multiple DM priors, as shown in Section 4.3, we opt to only use the pretrained DMs for images to ensure a fair comparison. Tables 2 and 3 show that our method, despite using fewer priors than BlindDPS, can surpass the best competing methods by approximately \(0.03\), \(4.5\), and \(0.1\) in terms of LPIPS, PSNR, and SSIM, respectively, on average. In Fig. 1, the reconstructions of our method look sharper and more precise than those of the main competitors.

    &  &  &  \\   & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) \\  BKS-styleGAN  & 1.047 & 22.82 & 0.653 & 1.051 & 22.07 & 0.620 & 0.987 & 20.90 & 0.538 \\ BKS-generic  & 1.051 & 21.04 & 0.591 & 1.056 & 20.76 & 0.583 & 0.994 & 18.55 & 0.481 \\ MCG  & 0.705 & 13.18 & 0.135 & 0.675 & 13.71 & 0.167 & 0.698 & 14.28 & 0.188 \\ ILVR  & 0.335 & 21.08 & 0.586 & 0.374 & 20.40 & 0.556 & 0.482 & 18.76 & 0.444 \\ DPS  & 0.149 & 24.57 & 0.723 & 0.130 & 25.00 & 0.759 & 0.244 & 23.46 & 0.684 \\ ReSample  & 0.104 & 28.52 & 0.839 & 0.104 & 27.02 & 0.834 & 0.143 & 26.03 & 0.803 \\
**DMTlug (ours)** & **0.073** & **31.61** & **0.882** & **0.057** & **32.83** & **0.907** & **0.083** & **30.74** & **0.882** \\
**Ours vs. Best compe.** & \(-0.031\) & \(+3.09\) & \(+0.043\) & \(-0.047\) & \(+5.79\) & \(+0.073\) & \(-0.060\) & \(+4.71\) & \(+0.079\) \\   

Table 1: (Nonlinear IP) **Nonlinear deblurring** with additive Gaussian noise (\(=0.01\)). (**Bold**: best, _under_: second best, _green_: performance increase, red: performance decrease)

    &  &  \\   & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) \\  DeBlurGANv2  & 0.356 & 24.17 & 0.739 & 0.379 & 23.52 & 0.723 \\ Stripformer  & 0.318 & 24.97 & 0.745 & 0.343 & 24.30 & 0.725 \\ MPPRNet  & 0.379 & 22.64 & 0.696 & 0.399 & 22.28 & 0.682 \\ TSR-WGAN  & 0.304 & 23.09 & 0.732 & 0.333 & 22.56 & 0.715 \\ ILVR  & 0.337 & 21.25 & 0.589 & 0.375 & 20.24 & 0.554 \\ BlindDPS  & **0.137** & 25.45 & 0.730 & 0.165 & 24.40 & 0.712 \\
**DMPLug (ours)** & 0.146 & **28.34** & **0.790** & **0.164** & **27.91** & **0.812** \\
**Ours vs. Best compe.** & \(-0.009\) & \(+2.89\) & \(+0.045\) & \(-0.001\) & \(+3.51\) & \(+0.087\) \\   

Table 2: (Nonlinear IP) **BID with turbulence** with additive Gaussian noise (\(=0.01\)). (**Bold**: best, _under_: second best, _green_: performance increase, red: performance decrease)

### Robustness to unknown noise

For robustness experiments, we choose super-resolution and nonlinear deblurring to represent linear and nonlinear IPs, respectively. To simulate scenarios involving unknown noise, we generate measurements with four types of noise: Gaussian, impulse, shot, and speckle noise, and across two different noise levels: low (level-1) and high (level-2), following  (see details in Appendix D.1), but we use the same formulation and code for each IP designed for mild Gaussian noise, regardless of the actual noise type and level. Table 4 and Fig. 7 clearly show that (1) **most current IP solvers, except for DPS, suffer from the robustness issue**, corroborating the hypotheses made in Section 2, and (2) the peak performance of our DMPlug can lead SOTA methods by around \(1\)dB and \(3.5\)dB in PSNR for the two tasks, respectively. To check the compatibility of our method with ES-WMV , we measure the detection performance via PSNR gaps, i.e., the absolute PSNR difference between the peak and the detected ES point following . Table 4 indicates that the detection gaps in the two exemplary tasks are nearly negligible, with PSNR gaps smaller than \(0.5\)dB and \(0.2\)dB, respectively. This suggests that **the proposed method is highly synergetic with ES-WMV**.

### Ablation studies

We conduct two ablation studies to demonstrate the flexibility of our method in terms of using multiple and non-DDIM DM priors and using alternative optimizers. (**Flexibility of using DM priors**) First, we explore the possibility of using different types of DMs. For super-resolution, we show in Table 5 that latent diffusion models (LDMs)  are also synergetic with our method. Next, we study the potential of using multiple DMs together, taking BID with turbulence as an example. Using extra pretrained DMs for blur kernels and tilt maps from , our method can achieve even better reconstruction results. (**Flexibility of using alternative optimizers**) Here, we test the built-in _ADAM_ and _L-BFGS_ optimizers in _PyTorch_, with several different learning rates to solve Eq. (7). As shown in Table 6, the best _ADAM_ and _L-BFGS_ combinations can lead to comparable performance for our DMPlug. We choose _ADAM_ as the default optimizer because in _PyTorch_, optimizing multiple groups of variables with different learning rates--as for the case of BID (with turbulence)--is easy to program with _ADAM_ but tricky for _L-BFGS_.

    &  &  \\   &  &  &  &  \\   &  & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) \\  SelfDeblur  & 0.568 & 16.59 & 0.417 & 0.579 & 16.55 & 0.423 & 0.628 & 16.33 & 0.408 & 0.604 & 16.22 & 0.410 \\ DeBlurGANv2  & 0.313 & 20.56 & 0.613 & 0.350 & 24.29 & 0.743 & 0.353 & 19.67 & 0.581 & 0.374 & 23.58 & 0.726 \\ St√ºpformer  & 0.287 & 22.06 & 0.644 & 0.316 & 25.03 & 0.747 & 0.324 & 21.31 & 0.613 & 0.339 & 24.34 & 0.728 \\ MPRNet  & 0.332 & 20.53 & 0.620 & 0.375 & 22.72 & 0.698 & 0.373 & 19.70 & 0.590 & 0.394 & 22.33 & 0.685 \\ Pan-DCP  & 0.606 & 15.83 & 0.483 & 0.653 & 20.57 & 0.701 & 0.616 & 15.59 & 0.464 & 0.667 & 20.69 & 0.698 \\ Pan-\(_{3}\) & 0.631 & 15.16 & 0.470 & 0.654 & 20.49 & 0.675 & 0.642 & 14.43 & 0.443 & 0.669 & 20.34 & 0.671 \\ ILVR  & 0.398 & 19.23 & 0.520 & 0.338 & 21.20 & 0.588 & 0.445 & 18.33 & 0.484 & 0.375 & 20.45 & 0.555 \\ BlindDPS  & 0.164 & 23.60 & 0.682 & 0.173 & 25.15 & 0.721 & 0.185 & 21.77 & 0.630 & 0.193 & 23.83 & 0.693 \\
**DMPlug (ours)** & **0.104** & **29.61** & **0.825** & **0.140** & **28.84** & **0.795** & **0.135** & **27.99** & **0.794** & **0.169** & **28.26** & **0.811** \\
**Ours vs. Best compe.** & \(-0.060\) & \(+6.01\) & \(+0.143\) & \(-0.033\) & \(+3.69\) & \(+0.048\) & \(-0.050\) & \(+6.22\) & \(+0.164\) & \(-0.024\) & \(+3.92\) & \(+0.083\) \\   

Table 3: (Nonlinear IP) **BID** with additive Gaussian noise (\(=0.01\)). (**Bold**: best, under: second best, green: performance increase, red: performance decrease)

Figure 7: (**Robustness**) Visualization of sample results from our DMPlug and main competing methods for \(4\) super-resolution (**top**) and nonlinear deblurring (**bottom**) with low-level Gaussian noise.

## 5 Discussion

In this paper, we focus on solving IPs with pretrained DMs. To deal with **(Issue 1)** insufficient manifold feasibility and **(Issue 2)** insufficient measurement feasibility of the prevailing interleaving methods, we **pioneer a novel plug-in method**, DMPlug, and make it practical in terms of computation and memory requirements. Taking advantage of a benign ELTO property and integrating an ES method ES-WMV , our method is **the first to achieve robustness to unknown noise (Issue 3)**. Extensive experiment results demonstrate that our method can lead SOTA methods, both qualitatively and quantitatively--often by large margins, particularly for nonlinear IPs. As for limitations, our empirical results in Section 3.1 suggest a fundamental gap between image generation and regression using pretrained DMs, that we have not managed to nail down. Also, our work is mostly empirical, and we leave a solid theoretical understanding for future work.